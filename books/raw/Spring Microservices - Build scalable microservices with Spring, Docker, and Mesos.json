{
  "metadata": {
    "title": "Spring Microservices - Build scalable microservices with Spring, Docker, and Mesos",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 437,
    "conversion_date": "2025-12-19T17:45:31.737681",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Spring Microservices - Build scalable microservices with Spring, Docker, and Mesos.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "[ 3 ]",
      "start_page": 30,
      "end_page": 79,
      "detection_method": "regex_chapter",
      "content": "Chapter 1\n[ 3 ]\nMicroservices provide an approach for developing quick and agile \napplications, resulting in less overall cost.\nToday, for instance, airlines or financial institutions do not invest in rebuilding \ntheir core mainframe systems as another monolithic monster. Retailers and other \nindustries do not rebuild heavyweight supply chain management applications, such \nas their traditional ERPs. Focus has shifted to building quick-win point solutions that \ncater to specific needs of the business in the most agile way possible.\nLet's take an example of an online retailer running with a legacy monolithic \napplication. If the retailer wants to innovate his/her sales by offering their products \npersonalized to a customer based on the customer's past shopping, preferences, and \nso on and also wants to enlighten customers by offering products based on their \npropensity to buy them, they will quickly develop a personalization engine or offers \nbased on their immediate needs and plug them into their legacy application.\nAs shown in the preceding diagram, rather than investing in rebuilding the core \nlegacy system, this will be either done by passing the responses through the new \nfunctions, as shown in the diagram marked A, or by modifying the core legacy \nsystem to call out these functions as part of the processing, as shown in the diagram \nmarked B. These functions are typically written as microservices.\nThis approach gives organizations a plethora of opportunities to quickly try out new \nfunctions with lesser cost in an experimental mode. Businesses can later validate key \nperformance indicators and alter or replace these implementations if required.\nwww.allitebooks.com\n\n\nDemystifying Microservices\n[ 4 ]\nModern architectures are expected to maximize the ability to replace \ntheir parts and minimize the cost of replacing their parts. The \nmicroservices approach is a means to achieving this.\nTechnology as a catalyst for the \nmicroservices evolution\nEmerging technologies have also made us rethink the way we build software \nsystems. For example, a few decades back, we couldn't even imagine a distributed \napplication without a two-phase commit. Later, NoSQL databases made us think \ndifferently.\nSimilarly, these kinds of paradigm shifts in technology have reshaped all the layers \nof the software architecture.\nThe emergence of HTML 5 and CSS3 and the advancement of mobile applications \nrepositioned user interfaces. Client-side JavaScript frameworks such as Angular, \nEmber, React, Backbone, and so on are immensely popular due to their client-side \nrendering and responsive designs.\nWith cloud adoptions steamed into the mainstream, Platform as a Services \n(PaaS) providers such as Pivotal CF, AWS, Salesforce.com, IBMs Bluemix, RedHat \nOpenShift, and so on made us rethink the way we build middleware components. \nThe container revolution created by Docker radically influenced the infrastructure \nspace. These days, an infrastructure is treated as a commodity service.\nThe integration landscape has also changed with Integration Platform as a Service \n(iPaaS), which is emerging. Platforms such as Dell Boomi, Informatica, MuleSoft, \nand so on are examples of iPaaS. These tools helped organizations stretch integration \nboundaries beyond the traditional enterprise.\nNoSQLs have revolutionized the databases space. A few years ago, we had only a \nfew popular databases, all based on relational data modeling principles. We have  \na long list of databases today: Hadoop, Cassandra, CouchDB, and Neo 4j to name  \na few. Each of these databases addresses certain specific architectural problems.\nImperative architecture evolution\nApplication architecture has always been evolving alongside demanding business \nrequirements and the evolution of technologies. Architectures have gone through  \nthe evolution of age-old mainframe systems to fully abstract cloud services such  \nas AWS Lambda.\n\n\nChapter 1\n[ 5 ]\nUsing AWS Lambda, developers can now drop their \"functions\" into \na fully managed compute service.\nRead more about Lambda at: https://aws.amazon.com/\ndocumentation/lambda/\nDifferent architecture approaches and styles such as mainframes, client server, \nN-tier, and service-oriented were popular at different timeframes. Irrespective of \nthe choice of architecture styles, we always used to build one or the other forms \nof monolithic architectures. The microservices architecture evolved as a result \nof modern business demands such as agility and speed of delivery, emerging \ntechnologies, and learning from previous generations of architectures.\nMicroservices help us break the boundaries of monolithic applications and build a \nlogically independent smaller system of systems, as shown in the preceding diagram.\nIf we consider monolithic applications as a set of logical \nsubsystems encompassed with a physical boundary, microservices \nare a set of independent subsystems with no enclosing physical \nboundary.\nWhat are microservices?\nMicroservices are an architecture style used by many organizations today as a game \nchanger to achieve a high degree of agility, speed of delivery, and scale. Microservices \ngive us a way to develop more physically separated modular applications.\nMicroservices are not invented. Many organizations such as Netflix, Amazon, and \neBay successfully used the divide-and-conquer technique to functionally partition \ntheir monolithic applications into smaller atomic units, each performing a single \nfunction. These organizations solved a number of prevailing issues they were \nexperiencing with their monolithic applications. \n\n\nDemystifying Microservices\n[ 6 ]\nFollowing the success of these organizations, many other organizations started \nadopting this as a common pattern to refactor their monolithic applications. Later, \nevangelists termed this pattern as the microservices architecture.\nMicroservices originated from the idea of hexagonal architecture coined by Alistair \nCockburn. Hexagonal architecture is also known as the Ports and Adapters pattern.\nRead more about hexagonal architecture at http://alistair.\ncockburn.us/Hexagonal+architecture.\nMicroservices are an architectural style or an approach to building IT systems as a set \nof business capabilities that are autonomous, self-contained, and loosely coupled:\nThe preceding diagram depicts a traditional N-tier application architecture having \na presentation layer, business layer, and database layer. The modules A, B, and C \nrepresent three different business capabilities. The layers in the diagram represent a \nseparation of architecture concerns. Each layer holds all three business capabilities \npertaining to this layer. The presentation layer has web components of all the three \nmodules, the business layer has business components of all the three modules, and \nthe database hosts tables of all the three modules. In most cases, layers are physically \nspreadable, whereas modules within a layer are hardwired.\n\n\nChapter 1\n[ 7 ]\nLet's now examine a microservices-based architecture.\nAs we can note in the preceding diagram, the boundaries are inversed in the \nmicroservices architecture. Each vertical slice represents a microservice. Each \nmicroservice has its own presentation layer, business layer, and database layer. \nMicroservices are aligned towards business capabilities. By doing so, changes  \nto one microservice do not impact others.\nThere is no standard for communication or transport mechanisms for microservices. \nIn general, microservices communicate with each other using widely adopted \nlightweight protocols, such as HTTP and REST, or messaging protocols, such as \nJMS or AMQP. In specific cases, one might choose more optimized communication \nprotocols, such as Thrift, ZeroMQ, Protocol Buffers, or Avro.\nAs microservices are more aligned to business capabilities and have independently \nmanageable life cycles, they are the ideal choice for enterprises embarking on \nDevOps and cloud. DevOps and cloud are two facets of microservices.\nDevOps is an IT realignment to narrow the gap between traditional IT \ndevelopment and operations for better efficiency.\nRead more about DevOps:\nhttp://dev2ops.org/2010/02/what-is-devops/\n\n\nDemystifying Microservices\n[ 8 ]\nMicroservices – the honeycomb analogy\nThe honeycomb is an ideal analogy for representing the evolutionary microservices \narchitecture.\nIn the real world, bees build a honeycomb by aligning hexagonal wax cells. They \nstart small, using different materials to build the cells. Construction is based on \nwhat is available at the time of building. Repetitive cells form a pattern and result \nin a strong fabric structure. Each cell in the honeycomb is independent but also \nintegrated with other cells. By adding new cells, the honeycomb grows organically \nto a big, solid structure. The content inside each cell is abstracted and not visible \noutside. Damage to one cell does not damage other cells, and bees can reconstruct \nthese cells without impacting the overall honeycomb.\nPrinciples of microservices\nIn this section, we will examine some of the principles of the microservices \narchitecture. These principles are a \"must have\" when designing and developing \nmicroservices.\nSingle responsibility per service\nThe single responsibility principle is one of the principles defined as part of the \nSOLID design pattern. It states that a unit should only have one responsibility.\nRead more about the SOLID design pattern at:\nhttp://c2.com/cgi/wiki?PrinciplesOfObjectOrientedDes\nign\n\n\nChapter 1\n[ 9 ]\nThis implies that a unit, either a class, a function, or a service, should have only one \nresponsibility. At no point should two units share one responsibility or one unit have \nmore than one responsibility. A unit with more than one responsibility indicates \ntight coupling.\nAs shown in the preceding diagram, Customer, Product, and Order are different \nfunctions of an e-commerce application. Rather than building all of them into one \napplication, it is better to have three different services, each responsible for exactly \none business function, so that changes to one responsibility will not impair others. \nIn the preceding scenario, Customer, Product, and Order will be treated as three \nindependent microservices.\nMicroservices are autonomous\nMicroservices are self-contained, independently deployable, and autonomous \nservices that take full responsibility of a business capability and its execution. \nThey bundle all dependencies, including library dependencies, and execution \nenvironments such as web servers and containers or virtual machines that abstract \nphysical resources.\nOne of the major differences between microservices and SOA is in their level of \nautonomy. While most SOA implementations provide service-level abstraction, \nmicroservices go further and abstract the realization and execution environment.\n\n\nDemystifying Microservices\n[ 10 ]\nIn traditional application developments, we build a WAR or an EAR, then deploy \nit into a JEE application server, such as with JBoss, WebLogic, WebSphere, and \nso on. We may deploy multiple applications into the same JEE container. In the \nmicroservices approach, each microservice will be built as a fat Jar, embedding all \ndependencies and run as a standalone Java process.\nMicroservices may also get their own containers for execution, as shown in the \npreceding diagram. Containers are portable, independently manageable, lightweight \nruntime environments. Container technologies, such as Docker, are an ideal choice \nfor microservices deployment.\nCharacteristics of microservices\nThe microservices definition discussed earlier in this chapter is arbitrary. Evangelists \nand practitioners have strong but sometimes different opinions on microservices. \nThere is no single, concrete, and universally accepted definition for microservices. \nHowever, all successful microservices implementations exhibit a number of common \ncharacteristics. Therefore, it is important to understand these characteristics rather \nthan sticking to theoretical definitions. Some of the common characteristics are \ndetailed in this section.\n\n\nChapter 1\n[ 11 ]\nServices are first-class citizens\nIn the microservices world, services are first-class citizens. Microservices expose \nservice endpoints as APIs and abstract all their realization details. The internal \nimplementation logic, architecture, and technologies (including programming \nlanguage, database, quality of services mechanisms, and so on) are completely \nhidden behind the service API.\nMoreover, in the microservices architecture, there is no more application \ndevelopment; instead, organizations focus on service development. In most \nenterprises, this requires a major cultural shift in the way that applications are built.\nIn a Customer Profile microservice, internals such as the data structure, technologies, \nbusiness logic, and so on are hidden. They aren't exposed or visible to any external \nentities. Access is restricted through the service endpoints or APIs. For instance, \nCustomer Profile microservices may expose Register Customer and Get Customer \nas two APIs for others to interact with.\nCharacteristics of services in a microservice\nAs microservices are more or less like a flavor of SOA, many of the service \ncharacteristics defined in the SOA are applicable to microservices as well.\nThe following are some of the characteristics of services that are applicable to \nmicroservices as well:\n•\t\nService contract: Similar to SOA, microservices are described through  \nwell-defined service contracts. In the microservices world, JSON and REST \nare universally accepted for service communication. In the case of JSON/\nREST, there are many techniques used to define service contracts. JSON \nSchema, WADL, Swagger, and RAML are a few examples.\n•\t\nLoose coupling: Microservices are independent and loosely coupled. In \nmost cases, microservices accept an event as input and respond with another \nevent. Messaging, HTTP, and REST are commonly used for interaction \nbetween microservices. Message-based endpoints provide higher levels  \nof decoupling.\n•\t\nService abstraction: In microservices, service abstraction is not just an \nabstraction of service realization, but it also provides a complete abstraction \nof all libraries and environment details, as discussed earlier.\n•\t\nService reuse: Microservices are course-grained reusable business  \nservices. These are accessed by mobile devices and desktop channels,  \nother microservices, or even other systems.\n\n\nDemystifying Microservices\n[ 12 ]\n•\t\nStatelessness: Well-designed microservices are stateless and share nothing \nwith no shared state or conversational state maintained by the services.  \nIn case there is a requirement to maintain state, they are maintained in  \na database, perhaps in memory.\n•\t\nServices are discoverable: Microservices are discoverable. In a typical \nmicroservices environment, microservices self-advertise their existence \nand make themselves available for discovery. When services die, they \nautomatically take themselves out from the microservices ecosystem.\n•\t\nService interoperability: Services are interoperable as they use standard \nprotocols and message exchange standards. Messaging, HTTP, and so on are \nused as transport mechanisms. REST/JSON is the most popular method for \ndeveloping interoperable services in the microservices world. In cases where \nfurther optimization is required on communications, other protocols such as \nProtocol Buffers, Thrift, Avro, or Zero MQ could be used. However, the use \nof these protocols may limit the overall interoperability of the services.\n•\t\nService composeability: Microservices are composeable. Service \ncomposeability is achieved either through service orchestration or  \nservice choreography.\nMore detail on SOA principles can be found at:\nhttp://serviceorientation.com/serviceorientation/index\nMicroservices are lightweight\nWell-designed microservices are aligned to a single business capability, so they \nperform only one function. As a result, one of the common characteristics we see  \nin most of the implementations are microservices with smaller footprints.\nWhen selecting supporting technologies, such as web containers, we will have \nto ensure that they are also lightweight so that the overall footprint remains \nmanageable. For example, Jetty or Tomcat are better choices as application containers \nfor microservices compared to more complex traditional application servers such as \nWebLogic or WebSphere.\nContainer technologies such as Docker also help us keep the infrastructure footprint \nas minimal as possible compared to hypervisors such as VMWare or Hyper-V.\n\n\nChapter 1\n[ 13 ]\nAs shown in the preceding diagram, microservices are typically deployed in Docker \ncontainers, which encapsulate the business logic and needed libraries. This help us \nquickly replicate the entire setup on a new machine or on a completely different \nhosting environment or even to move across different cloud providers. As there is no \nphysical infrastructure dependency, containerized microservices are easily portable.\nMicroservices with polyglot architecture\nAs microservices are autonomous and abstract everything behind service APIs, it is \npossible to have different architectures for different microservices. A few common \ncharacteristics that we see in microservices implementations are:\n•\t\nDifferent services use different versions of the same technologies. One \nmicroservice may be written on Java 1.7, and another one could be on Java 1.8.\n•\t\nDifferent languages are used to develop different microservices, such as one \nmicroservice is developed in Java and another one in Scala.\n•\t\nDifferent architectures are used, such as one microservice using the Redis \ncache to serve data, while another microservice could use MySQL as a \npersistent data store.\n\n\nDemystifying Microservices\n[ 14 ]\nIn the preceding example, as Hotel Search is expected to have high transaction \nvolumes with stringent performance requirements, it is implemented using Erlang. \nIn order to support predictive searching, Elasticsearch is used as the data store. \nAt the same time, Hotel Booking needs more ACID transactional characteristics. \nTherefore, it is implemented using MySQL and Java. The internal implementations \nare hidden behind service endpoints defined as REST/JSON over HTTP.\nAutomation in a microservices environment\nMost of the microservices implementations are automated to a maximum from \ndevelopment to production.\nAs microservices break monolithic applications into a number of smaller services, \nlarge enterprises may see a proliferation of microservices. A large number of \nmicroservices is hard to manage until and unless automation is in place. The smaller \nfootprint of microservices also helps us automate the microservices development to \nthe deployment life cycle. In general, microservices are automated end to end—for \nexample, automated builds, automated testing, automated deployment, and elastic \nscaling.\nAs indicated in the preceding diagram, automations are typically applied during the \ndevelopment, test, release, and deployment phases:\n•\t\nThe development phase is automated using version control tools such as Git \ntogether with Continuous Integration (CI) tools such as Jenkins, Travis CI, \nand so on. This may also include code quality checks and automation of unit \ntesting. Automation of a full build on every code check-in is also achievable \nwith microservices.\n•\t\nThe testing phase will be automated using testing tools such as Selenium, \nCucumber, and other AB testing strategies. As microservices are aligned to \nbusiness capabilities, the number of test cases to automate is fewer compared \nto monolithic applications, hence regression testing on every build also \nbecomes possible.\n•\t\nInfrastructure provisioning is done through container technologies such as \nDocker, together with release management tools such as Chef or Puppet, and \nconfiguration management tools such as Ansible. Automated deployments are \nhandled using tools such as Spring Cloud, Kubernetes, Mesos, and Marathon.\n\n\nChapter 1\n[ 15 ]\nMicroservices with a supporting ecosystem\nMost of the large-scale microservices implementations have a supporting ecosystem \nin place. The ecosystem capabilities include DevOps processes, centralized log \nmanagement, service registry, API gateways, extensive monitoring, service routing, \nand flow control mechanisms.\nMicroservices work well when supporting capabilities are in place, as represented in \nthe preceding diagram.\nMicroservices are distributed and dynamic\nSuccessful microservices implementations encapsulate logic and data within the \nservice. This results in two unconventional situations: distributed data and logic  \nand decentralized governance.\nCompared to traditional applications, which consolidate all logic and data into \none application boundary, microservices decentralize data and logic. Each service, \naligned to a specific business capability, owns its data and logic.\nLogical System Boundary (as in monolithic)\nMicroservice A\nA\nData\nA\nLogic\nMicroservice B\nB\nData\nB\nLogic\nMicroservice C\nC\nData\nC\nLogic\nThe dotted line in the preceding diagram implies the logical monolithic application \nboundary. When we migrate this to microservices, each microservice A, B, and C \ncreates its own physical boundaries.\n\n\nDemystifying Microservices\n[ 16 ]\nMicroservices don't typically use centralized governance mechanisms the way \nthey are used in SOA. One of the common characteristics of microservices \nimplementations is that they do not relay on heavyweight enterprise-level products, \nsuch as Enterprise Service Bus (ESB). Instead, the business logic and intelligence are \nembedded as a part of the services themselves.\nA typical SOA implementation is shown in the preceding diagram. Shopping logic is \nfully implemented in ESB by orchestrating different services exposed by Customer, \nOrder, and Product. In the microservices approach, on the other hand, Shopping \nitself will run as a separate microservice, which interacts with Customer, Product, \nand Order in a fairly decoupled way.\nSOA implementations heavily relay on static registry and repository configurations \nto manage services and other artifacts. Microservices bring a more dynamic nature \ninto this. Hence, a static governance approach is seen as an overhead in maintaining \nup-to-date information. This is why most of the microservices implementations use \nautomated mechanisms to build registry information dynamically from the runtime \ntopologies.\nAntifragility, fail fast, and self-healing\nAntifragility is a technique successfully experimented at Netflix. It is one of the most \npowerful approaches to building fail-safe systems in modern software development.\nThe antifragility concept is introduced by Nassim Nicholas Taleb in his \nbook Antifragile: Things That Gain from Disorder.\nIn the antifragility practice, software systems are consistently challenged. Software \nsystems evolve through these challenges and, over a period of time, get better and \nbetter at withstanding these challenges. Amazon's GameDay exercise and Netflix' \nSimian Army are good examples of such antifragility experiments.\n\n\nChapter 1\n[ 17 ]\nFail fast is another concept used to build fault-tolerant, resilient systems. This \nphilosophy advocates systems that expect failures versus building systems that never \nfail. Importance should be given to how quickly the system can fail and if it fails, \nhow quickly it can recover from this failure. With this approach, the focus is shifted \nfrom Mean Time Between Failures (MTBF) to Mean Time To Recover (MTTR). \nA key advantage of this approach is that if something goes wrong, it kills itself, and \ndownstream functions aren't stressed.\nSelf-healing is commonly used in microservices deployments, where the system \nautomatically learns from failures and adjusts itself. These systems also prevent \nfuture failures.\nMicroservices examples\nThere is no \"one size fits all\" approach when implementing microservices. In this \nsection, different examples are analyzed to crystalize the microservices concept.\nAn example of a holiday portal\nIn the first example, we will review a holiday portal, Fly By Points. Fly By Points \ncollects points that are accumulated when a customer books a hotel, flight, or car \nthrough the online website. When the customer logs in to the Fly By Points website, \nhe/she is able to see the points accumulated, personalized offers that can be availed \nof by redeeming the points, and upcoming trips if any.\n\n\nDemystifying Microservices\n[ 18 ]\nLet's assume that the preceding page is the home page after login. There are two \nupcoming trips for Jeo, four personalized offers, and 21,123 loyalty points. When  \nthe user clicks on each of the boxes, the details are queried and displayed.\nThe holiday portal has a Java Spring-based traditional monolithic application \narchitecture, as shown in the following:\nAs shown in the preceding diagram, the holiday portal's architecture is web-based \nand modular, with a clear separation between layers. Following the usual practice, \nthe holiday portal is also deployed as a single WAR file on a web server such as \nTomcat. Data is stored on an all-encompassing backing relational database. This is a \ngood fit for the purpose architecture when the complexities are few. As the business \ngrows, the user base expands, and the complexity also increases. This results in a \nproportional increase in transaction volumes. At this point, enterprises should look \nto rearchitecting the monolithic application to microservices for better speed of \ndelivery, agility, and manageability.\n\n\nChapter 1\n[ 19 ]\nExamining the simple microservices version of this application, we can immediately \nnote a few things in this architecture:\n•\t\nEach subsystem has now become an independent system by itself, a \nmicroservice. There are three microservices representing three business \nfunctions: Trips, Offers, and Points. Each one has its internal data store  \nand middle layer. The internal structure of each service remains the same.\n•\t\nEach service encapsulates its own database as well as its own HTTP listener. \nAs opposed to the previous model, there is no web server or WAR. Instead, \neach service has its own embedded HTTP listener, such as Jetty, Tomcat,  \nand so on.\n•\t\nEach microservice exposes a REST service to manipulate the resources/entity \nthat belong to this service.\nIt is assumed that the presentation layer is developed using a client-side JavaScript \nMVC framework such as Angular JS. These client-side frameworks are capable of \ninvoking REST calls directly.\nWhen the web page is loaded, all the three boxes, Trips, Offers, and Points will be \ndisplayed with details such as points, the number of offers, and the number of trips. \nThis will be done by each box independently making asynchronous calls to the \nrespective backend microservices using REST. There is no dependency between the \nservices at the service layer. When the user clicks on any of the boxes, the screen will \nbe transitioned and will load the details of the item clicked on. This will be done by \nmaking another call to the respective microservice.\n\n\nDemystifying Microservices\n[ 20 ]\nA microservice-based order management \nsystem\nLet's examine another microservices example: an online retail website. In this \ncase, we will focus more on the backend services, such as the Order Service which \nprocesses the Order Event generated when a customer places an order through  \nthe website:\nThis microservices system is completely designed based on reactive programming \npractices.\nRead more on reactive programming at:\nhttp://www.reactivemanifesto.org\nWhen an event is published, a number of microservices are ready to kick-start upon \nreceiving the event. Each one of them is independent and does not rely on other \nmicroservices. The advantage of this model is that we can keep adding or replacing \nmicroservices to achieve specific needs.\n\n\nChapter 1\n[ 21 ]\nIn the preceding diagram, there are eight microservices shown. The following \nactivities take place upon the arrival of Order Event:\n1.\t Order Service kicks off when Order Event is received. Order Service creates \nan order and saves the details to its own database.\n2.\t If the order is successfully saved, Order Successful Event is created by Order \nService and published.\n3.\t A series of actions take place when Order Successful Event arrives.\n4.\t Delivery Service accepts the event and places Delivery Record to deliver the \norder to the customer. This, in turn, generates Delivery Event and publishes \nthe event.\n5.\t Trucking Service picks up Delivery Event and processes it. For instance, \nTrucking Service creates a trucking plan.\n6.\t Customer Notification Service sends a notification to the customer informing \nthe customer that an order is placed.\n7.\t Inventory Cache Service updates the inventory cache with the available \nproduct count.\n8.\t Stock Reorder Service checks whether the stock limits are adequate and \ngenerates Replenish Event if required.\n9.\t Customer Points Service recalculates the customer's loyalty points based  \non this purchase.\n10.\t Customer Account Service updates the order history in the customer's \naccount.\nIn this approach, each service is responsible for only one function. Services \naccept and generate events. Each service is independent and is not aware of its \nneighborhood. Hence, the neighborhood can organically grow as mentioned in the \nhoneycomb analogy. New services can be added as and when necessary. Adding  \na new service does not impact any of the existing services.\n\n\nDemystifying Microservices\n[ 22 ]\nAn example of a travel agent portal\nThis third example is a simple travel agent portal application. In this example, we \nwill see both synchronous REST calls as well as asynchronous events.\nIn this case, the portal is just a container application with multiple menu items or \nlinks in the portal. When specific pages are requested—for example, when the menu \nor a link is clicked on—they will be loaded from the specific microservices.\nWhen a customer requests a booking, the following events take place internally:\n1.\t The travel agent opens the flight UI, searches for a flight, and identifies the \nright flight for the customer. Behind the scenes, the flight UI is loaded from \nthe Flight microservice. The flight UI only interacts with its own backend \nAPIs within the Flight microservice. In this case, it makes a REST call to the \nFlight microservice to load the flights to be displayed.\n2.\t The travel agent then queries the customer details by accessing the customer \nUI. Similar to the flight UI, the customer UI is loaded from the Customer \nmicroservice. Actions in the customer UI will invoke REST calls on the \nCustomer microservice. In this case, customer details are loaded by  \ninvoking appropriate APIs on the Customer microservice.\n3.\t Then, the travel agent checks the visa details for the customer's eligibility \nto travel to the selected country. This also follows the same pattern as \nmentioned in the previous two points.\n\n\nChapter 1\n[ 23 ]\n4.\t Next, the travel agent makes a booking using the booking UI from the \nBooking microservice, which again follows the same pattern.\n5.\t The payment pages are loaded from the Payment microservice. In general, \nthe payment service has additional constraints such as PCIDSS compliance \n(protecting and encrypting data in motion and data at rest). The advantage \nof the microservices approach is that none of the other microservices need \nto be considered under the purview of PCIDSS as opposed to the monolithic \napplication, where the complete application comes under the governing rules \nof PCIDSS. Payment also follows the same pattern as described earlier.\n6.\t Once the booking is submitted, the Booking microservice calls the flight \nservice to validate and update the flight booking. This orchestration is \ndefined as part of the Booking microservice. Intelligence to make a booking is \nalso held within the Booking microservice. As part of the booking process, it \nalso validates, retrieves, and updates the Customer microservice.\n7.\t Finally, the Booking microservice sends the Booking Event, which the \nNotification service picks up and sends a notification of to the customer.\nThe interesting factor here is that we can change the user interface, logic, and data  \nof a microservice without impacting any other microservices.\nThis is a clean and neat approach. A number of portal applications can be built by \ncomposing different screens from different microservices, especially for different \nuser communities. The overall behavior and navigation will be controlled by the \nportal application.\nThe approach has a number of challenges unless the pages are designed with this \napproach in mind. Note that the site layouts and static content will be loaded by the \nContent Management System (CMS) as layout templates. Alternately, this could be \nstored in a web server. The site layout may have fragments of UIs that will be loaded \nfrom the microservices at runtime.\nMicroservices benefits\nMicroservices offer a number of benefits over the traditional multitier, monolithic \narchitectures. This section explains some key benefits of the microservices \narchitecture approach.\nSupports polyglot architecture\nWith microservices, architects and developers can choose fit for purpose \narchitectures and technologies for each microservice. This gives the flexibility  \nto design better-fit solutions in a more cost-effective way.\nwww.allitebooks.com\n\n\nDemystifying Microservices\n[ 24 ]\nAs microservices are autonomous and independent, each service can run with its \nown architecture or technology or different versions of technologies.\nThe following shows a simple, practical example of a polyglot architecture with \nmicroservices.\nThere is a requirement to audit all system transactions and record transaction details \nsuch as request and response data, the user who initiated the transaction, the service \ninvoked, and so on.\nAs shown in the preceding diagram, while core services such as the Order and \nProducts microservices use a relational data store, the Audit microservice persists \ndata in Hadoop File System (HDFS). A relational data store is neither ideal nor \ncost effective in storing large data volumes such as in the case of audit data. In the \nmonolithic approach, the application generally uses a shared, single database that \nstores Order, Products, and Audit data.\nIn this example, the audit service is a technical microservice using a different \narchitecture. Similarly, different functional services could also use different \narchitectures.\nIn another example, there could be a Reservation microservice running on Java \n7, while a Search microservice could be running on Java 8. Similarly, an Order \nmicroservice could be written on Erlang, whereas a Delivery microservice could be \non the Go language. None of these are possible with a monolithic architecture.\nEnabling experimentation and innovation\nModern enterprises are thriving towards quick wins. Microservices are one of the \nkey enablers for enterprises to do disruptive innovation by offering the ability to \nexperiment and fail fast.\n\n\nChapter 1\n[ 25 ]\nAs services are fairly simple and smaller in size, enterprises can afford to experiment \nnew processes, algorithms, business logics, and so on. With large monolithic \napplications, experimentation was not easy; nor was it straightforward or cost \neffective. Businesses had to spend huge money to build or change an application \nto try out something new. With microservices, it is possible to write a small \nmicroservice to achieve the targeted functionality and plug it into the system in a \nreactive style. One can then experiment with the new function for a few months, and \nif the new microservice does not work as expected, we can change or replace it with \nanother one. The cost of change will be considerably less compared to that of the \nmonolithic approach.\nIn another example of an airline booking website, the airline wants to show \npersonalized hotel recommendations in their booking page. The recommendations \nmust be displayed on the booking confirmation page.\nAs shown in the preceding diagram, it is convenient to write a microservice that can \nbe plugged into the monolithic applications booking flow rather than incorporating \nthis requirement in the monolithic application itself. The airline may choose to start \nwith a simple recommendation service and keep replacing it with newer versions till \nit meets the required accuracy.\nElastically and selectively scalable\nAs microservices are smaller units of work, they enable us to implement selective \nscalability. \nScalability requirements may be different for different functions in an application. \nA monolithic application, packaged as a single WAR or an EAR, can only be scaled \nas a whole. An I/O-intensive function when streamed with high velocity data could \neasily bring down the service levels of the entire application.\n\n\nDemystifying Microservices\n[ 26 ]\nIn the case of microservices, each service could be independently scaled up or \ndown. As scalability can be selectively applied at each service, the cost of scaling is \ncomparatively less with the microservices approach.\nIn practice, there are many different ways available to scale an application and \nis largely subject to the architecture and behavior of the application. Scale Cube \ndefines primarily three approaches to scaling an application:\n•\t\nScaling the x axis by horizontally cloning the application\n•\t\nScaling the y axis by splitting different functionality\n•\t\nScaling the z axis by partitioning or sharding the data\nRead more about Scale Cube in the following site:\nhttp://theartofscalability.com/\nWhen y axis scaling is applied to monolithic applications, it breaks the monolithic \nto smaller units aligned with business functions. Many organizations successfully \napplied this technique to move away from monolithic applications. In principle, the \nresulting units of functions are in line with the microservices characteristics.\nFor instance, in a typical airline website, statistics indicate that the ratio of flight \nsearching to flight booking could be as high as 500:1. This means one booking \ntransaction for every 500 search transactions. In this scenario, the search needs \n500 times more scalability than the booking function. This is an ideal use case for \nselective scaling.\n\n\nChapter 1\n[ 27 ]\nThe solution is to treat search requests and booking requests differently. With \na monolithic architecture, this is only possible with z scaling in the scale cube. \nHowever, this approach is expensive because in the z scale, the entire code base  \nis replicated.\nIn the preceding diagram, Search and Booking are designed as different microservices \nso that Search can be scaled differently from Booking. In the diagram, Search has \nthree instances, and Booking has two instances. Selective scalability is not limited \nto the number of instances, as shown in the diagram, but also in the way in which \nthe microservices are architected. In the case of Search, an in-memory data grid \n(IMDG) such as Hazelcast can be used as the data store. This will further increase the \nperformance and scalability of Search. When a new Search microservice instance is \ninstantiated, an additional IMDG node is added to the IMDG cluster. Booking does \nnot require the same level of scalability. In the case of Booking, both instances of the \nBooking microservice are connected to the same instance of the database.\nAllowing substitution\nMicroservices are self-contained, independent deployment modules enabling the \nsubstitution of one microservice with another similar microservice.\nMany large enterprises follow buy-versus-build policies to implement software \nsystems. A common scenario is to build most of the functions in house and buy \ncertain niche capabilities from specialists outside. This poses challenges in traditional \nmonolithic applications as these application components are highly cohesive. \nAttempting to plug in third-party solutions to the monolithic applications results in \ncomplex integrations. With microservices, this is not an afterthought. Architecturally, \na microservice can be easily replaced by another microservice developed either  \nin-house or even extended by a microservice from a third party.\n\n\nDemystifying Microservices\n[ 28 ]\nA pricing engine in the airline business is complex. Fares for different routes are \ncalculated using complex mathematical formulas known as the pricing logic. Airlines \nmay choose to buy a pricing engine from the market instead of building the product \nin house. In the monolithic architecture, Pricing is a function of Fares and Booking. In \nmost cases Pricing, Fares, and Booking are hardwired, making it almost impossible \nto detach.\nIn a well-designed microservices system, Booking, Fares, and Pricing would \nbe independent microservices. Replacing the Pricing microservice will have \nonly a minimal impact on any other services as they are all loosely coupled and \nindependent. Today, it could be a third-party service; tomorrow, it could be easily \nsubstituted by another third-party or home-grown service.\nEnabling to build organic systems\nMicroservices help us build systems that are organic in nature. This is significantly \nimportant when migrating monolithic systems gradually to microservices.\nOrganic systems are systems that grow laterally over a period of time by adding \nmore and more functions to it. In practice, an application grows unimaginably \nlarge in its lifespan, and in most cases, the manageability of the application reduces \ndramatically over this same period of time.\nMicroservices are all about independently manageable services. This enable us to \nkeep adding more and more services as the need arises with minimal impact on the \nexisting services. Building such systems does not need huge capital investments. \nHence, businesses can keep building as part of their operational expenditure.\nA loyalty system in an airline was built years ago, targeting individual passengers. \nEverything was fine until the airline started offering loyalty benefits to their \ncorporate customers. Corporate customers are individuals grouped under \ncorporations. As the current systems core data model is flat, targeting individuals, \nthe corporate environment needs a fundamental change in the core data model,  \nand hence huge reworking, to incorporate this requirement.\n\n\nChapter 1\n[ 29 ]\nAs shown in the preceding diagram, in a microservices-based architecture, customer \ninformation would be managed by the Customer microservice and loyalty by the \nLoyalty Points microservice.\nIn this situation, it is easy to add a new Corporate Customer microservice to manage \ncorporate customers. When a corporation is registered, individual members will \nbe pushed to the Customer microservice to manage them as usual. The Corporate \nCustomer microservice provides a corporate view by aggregating data from the \nCustomer microservice. It will also provide services to support corporate-specific \nbusiness rules. With this approach, adding new services will have only a minimal \nimpact on the existing services.\nHelping reducing technology debt\nAs microservices are smaller in size and have minimal dependencies, they allow the \nmigration of services that use end-of-life technologies with minimal cost.\nTechnology changes are one of the barriers in software development. In many \ntraditional monolithic applications, due to the fast changes in technologies, today's \nnext-generation applications could easily become legacy even before their release \nto production. Architects and developers tend to add a lot of protection against \ntechnology changes by adding layers of abstractions. However, in reality, this \napproach does not solve the issue but, instead, results in over-engineered systems. \nAs technology upgrades are often risky and expensive with no direct returns to \nbusiness, the business may not be happy to invest in reducing the technology  \ndebt of the applications.\nWith microservices, it is possible to change or upgrade technology for each service \nindividually rather than upgrading an entire application.\nUpgrading an application with, for instance, five million lines written on EJB 1.1 and \nHibernate to the Spring, JPA, and REST services is almost similar to rewriting the \nentire application. In the microservices world, this could be done incrementally.\n\n\nDemystifying Microservices\n[ 30 ]\nAs shown in the preceding diagram, while older versions of the services are running \non old versions of technologies, new service developments can leverage the latest \ntechnologies. The cost of migrating microservices with end-of-life technologies is \nconsiderably less compared to enhancing monolithic applications.\nAllowing the coexistence of different versions\nAs microservices package the service runtime environment along with the service \nitself, this enables having multiple versions of the service to coexist in the same \nenvironment.\nThere will be situations where we will have to run multiple versions of the same \nservice at the same time. Zero downtime promote, where one has to gracefully \nswitch over from one version to another, is one example of a such a scenario as \nthere will be a time window where both services will have to be up and running \nsimultaneously. With monolithic applications, this is a complex procedure because \nupgrading new services in one node of the cluster is cumbersome as, for instance, \nthis could lead to class loading issues. A canary release, where a new version is \nonly released to a few users to validate the new service, is another example where \nmultiple versions of the services have to coexist.\nWith microservices, both these scenarios are easily manageable. As each microservice \nuses independent environments, including service listeners such as Tomcat or Jetty \nembedded, multiple versions can be released and gracefully transitioned without \nmany issues. When consumers look up services, they look for specific versions of \nservices. For example, in a canary release, a new user interface is released to user \nA. When user A sends a request to the microservice, it looks up the canary release \nversion, whereas all other users will continue to look up the last production version.\nCare needs to be taken at the database level to ensure the database design is always \nbackward compatible to avoid breaking the changes.\n\n\nChapter 1\n[ 31 ]\nAs shown in the preceding diagram, version 1 and 2 of the Customer service can \ncoexist as they are not interfering with each other, given their respective deployment \nenvironments. Routing rules can be set at the gateway to divert traffic to specific \ninstances, as shown in the diagram. Alternatively, clients can request specific \nversions as part of the request itself. In the diagram, the gateway selects the version \nbased on the region from which the request is originated.\nSupporting the building of self-organizing \nsystems\nMicroservices help us build self-organizing systems. A self-organizing system \nsupport will automate deployment, be resilient, and exhibit self-healing and self-\nlearning capabilities.\nIn a well-architected microservices system, a service is unaware of other services. It \naccepts a message from a selected queue and processes it. At the end of the process, it \nmay send out another message, which triggers other services. This allows us to drop \nany service into the ecosystem without analyzing the impact on the overall system. \nBased on the input and output, the service will self-organize into the ecosystem. No \nadditional code changes or service orchestration is required. There is no central brain \nto control and coordinate the processes.\nImagine an existing notification service that listens to an INPUT queue and sends \nnotifications to an SMTP server, as shown in the following figure:\nLet's assume, later, a personalization engine, responsible for changing the language \nof the message to the customer's native language, needs to be introduced to \npersonalize messages before sending them to the customer, the personalization \nengine is responsible for changing the language of the message to the customer's \nnative language.\n\n\nDemystifying Microservices\n[ 32 ]\nWith microservices, a new personalization microservice will be created to do this \njob. The input queue will be configured as INPUT in an external configuration \nserver, and the personalization service will pick up the messages from the INPUT \nqueue (earlier, this was used by the notification service) and send the messages \nto the OUTPUT queue after completing process. The notification services input \nqueue will then send to OUTPUT. From the very next moment onward, the system \nautomatically adopts this new message flow.\nSupporting event-driven architecture\nMicroservices enable us to develop transparent software systems. Traditional \nsystems communicate with each other through native protocols and hence behave \nlike a black box application. Business events and system events, unless published \nexplicitly, are hard to understand and analyze. Modern applications require data  \nfor business analysis, to understand dynamic system behaviors, and analyze market  \ntrends, and they also need to respond to real-time events. Events are useful \nmechanisms for data extraction.\nA well-architected microservice always works with events for both input and output. \nThese events can be tapped by any service. Once extracted, events can be used for  \na variety of use cases.\nFor example, the business wants to see the velocity of orders categorized by product \ntype in real time. In a monolithic system, we need to think about how to extract these \nevents. This may impose changes in the system.\nIn the microservices world, Order Event is already published whenever an order is \ncreated. This means that it is just a matter of adding a new service to subscribe to the \nsame topic, extract the event, perform the requested aggregations, and push another \nevent for the dashboard to consume.\n\n\nChapter 1\n[ 33 ]\nEnabling DevOps\nMicroservices are one of the key enablers of DevOps. DevOps is widely adopted  \nas a practice in many enterprises, primarily to increase the speed of delivery and \nagility. A successful adoption of DevOps requires cultural changes, process changes, \nas well as architectural changes. DevOps advocates to have agile development,  \nhigh-velocity release cycles, automatic testing, automatic infrastructure provisioning, \nand automated deployment. \nAutomating all these processes is extremely hard to achieve with traditional \nmonolithic applications. Microservices are not the ultimate answer, but microservices \nare at the center stage in many DevOps implementations. Many DevOps tools and \ntechniques are also evolving around the use of microservices.\nConsider a monolithic application that takes hours to complete a full build and 20 \nto 30 minutes to start the application; one can see that this kind of application is not \nideal for DevOps automation. It is hard to automate continuous integration on every \ncommit. As large, monolithic applications are not automation friendly, continuous \ntesting and deployments are also hard to achieve.\nOn the other hand, small footprint microservices are more automation-friendly and \ntherefore can more easily support these requirements.\nMicroservices also enable smaller, focused agile teams for development. Teams will \nbe organized based on the boundaries of microservices.\nRelationship with other architecture \nstyles\nNow that we have seen the characteristics and benefits of microservices, in this \nsection, we will explore the relationship of microservices with other closely related \narchitecture styles such as SOA and Twelve-Factor Apps.\nRelations with SOA\nSOA and microservices follow similar concepts. Earlier in this chapter, we discussed \nthat microservices are evolved from SOA, and many service characteristics are \ncommon in both approaches.\nHowever, are they the same or are they different?\nAs microservices are evolved from SOA, many characteristics of microservices are \nsimilar to SOA. Let's first examine the definition of SOA.\n\n\nDemystifying Microservices\n[ 34 ]\nThe definition of SOA from The Open Group consortium is as follows:\n\"Service-Oriented Architecture (SOA) is an architectural style that supports \nservice orientation. Service orientation is a way of thinking in terms of services  \nand service-based development and the outcomes of services.\nA service:\nIs a logical representation of a repeatable business activity that has a specified \noutcome (e.g., check customer credit, provide weather data, consolidate drilling \nreports)\nIt is self-contained.\nIt may be composed of other services.\nIt is a \"black box\" to consumers of the service.\"\nWe observed similar aspects in microservices as well. So, in what way are \nmicroservices different? The answer is: it depends.\nThe answer to the previous question could be yes or no, depending upon the \norganization and its adoption of SOA. SOA is a broader term, and different \norganizations approached SOA differently to solve different organizational \nproblems. The difference between microservices and SOA is in a way based  \non how an organization approaches SOA.\nIn order to get clarity, a few cases will be examined.\nService-oriented integration\nService-oriented integration refers to a service-based integration approach used by \nmany organizations.\n\n\nChapter 1\n[ 35 ]\nMany organizations would have used SOA primarily to solve their integration \ncomplexities, also known as integration spaghetti. Generally, this is termed as \nService-Oriented Integration (SOI). In such cases, applications communicate \nwith each other through a common integration layer using standard protocols and \nmessage formats such as SOAP/XML-based web services over HTTP or JMS. These \ntypes of organizations focus on Enterprise Integration Patterns (EIP) to model their \nintegration requirements. This approach strongly relies on heavyweight ESB such \nas TIBCO Business Works, WebSphere ESB, Oracle ESB, and the likes. Most ESB \nvendors also packed a set of related products such as rules engines, business process \nmanagement engines, and so on as an SOA suite. Such organizations' integrations are \ndeeply rooted into their products. They either write heavy orchestration logic in the \nESB layer or the business logic itself in the service bus. In both cases, all enterprise \nservices are deployed and accessed via ESB. These services are managed through an \nenterprise governance model. For such organizations, microservices are altogether \ndifferent from SOA.\nLegacy modernization\nSOA is also used to build service layers on top of legacy applications.\nAnother category of organizations would use SOA in transformation projects or \nlegacy modernization projects. In such cases, the services are built and deployed \nin the ESB layer connecting to backend systems using ESB adapters. For these \norganizations, microservices are different from SOA.\n\n\nDemystifying Microservices\n[ 36 ]\nService-oriented application\nSome organizations adopt SOA at an application level.\nIn this approach, lightweight integration frameworks, such as Apache Camel or Spring \nIntegration, are embedded within applications to handle service-related cross-cutting \ncapabilities such as protocol mediation, parallel execution, orchestration, and service \nintegration. As some of the lightweight integration frameworks have native Java object \nsupport, such applications would even use native Plain Old Java Objects (POJO) \nservices for integration and data exchange between services. As a result, all services \nhave to be packaged as one monolithic web archive. Such organizations could see \nmicroservices as the next logical step of their SOA.\nMonolithic migration using SOA\nThe last possibility is transforming a monolithic application into smaller units \nafter hitting the breaking point with the monolithic system. They would break the \napplication into smaller, physically deployable subsystems, similar to the y axis \nscaling approach explained earlier, and deploy them as web archives on web servers \nor as JARs deployed on some home-grown containers. These subsystems as service \nwould use web services or other lightweight protocols to exchange data between \nservices. They would also use SOA and service design principles to achieve this. For \nsuch organizations, they may tend to think that microservices are the same old wine \nin a new bottle.\n\n\nChapter 1\n[ 37 ]\nRelations with Twelve-Factor apps\nCloud computing is one of the rapidly evolving technologies. Cloud computing \npromises many benefits, such as cost advantage, speed, agility, flexibility, and \nelasticity. There are many cloud providers offering different services. They lower the \ncost models to make it more attractive to the enterprises. Different cloud providers \nsuch as AWS, Microsoft, Rackspace, IBM, Google, and so on use different tools, \ntechnologies, and services. On the other hand, enterprises are aware of this evolving \nbattlefield and, therefore, they are looking for options for de-risking from lockdown \nto a single vendor.\nMany organizations do lift and shift their applications to the cloud. In such cases, \nthe applications may not realize all the benefits promised by cloud platforms. Some \napplications need to undergo overhaul, whereas some may need minor tweaking \nbefore moving to cloud. This by and large depends upon how the application is \narchitected and developed.\nFor example, if the application has its production database server URLs hardcoded \nas part of the applications WAR, it needs to be modified before moving the \napplication to cloud. In the cloud, the infrastructure is transparent to the application, \nand especially, the physical IP addresses cannot be assumed.\nHow do we ensure that an application, or even microservices, can run seamlessly \nacross multiple cloud providers and take advantages of cloud services such as \nelasticity?\nIt is important to follow certain principles while developing cloud native applications.\nCloud native is a term used for developing applications that can work \nefficiently in a cloud environment, understanding and utilizing cloud \nbehaviors such as elasticity, utilization based charging, fail aware, and \nso on.\nTwelve-Factor App, forwarded by Heroku, is a methodology describing the \ncharacteristics expected from modern cloud-ready applications. Twelve-Factor App \nis equally applicable for microservices as well. Hence, it is important to understand \nTwelve-Factor App.\n\n\nDemystifying Microservices\n[ 38 ]\nA single code base\nThe code base principle advises that each application has a single code base. There \ncan be multiple instances of deployment of the same code base, such as development, \ntesting, and production. Code is typically managed in a source control system such \nas Git, Subversion, and so on.\nExtending the same philosophy for microservices, each microservice should have its \nown code base, and this code base is not shared with any other microservice. It also \nmeans that one microservice has exactly one code base.\nBundling dependencies\nAs per this principle, all applications should bundle their dependencies along with \nthe application bundle. With build tools such as Maven and Gradle, we explicitly \nmanage dependencies in a pom.xml or the .gradle file and link them using a central \nbuild artifact repository such as Nexus or Archiva. This ensures that the versions \nare managed correctly. The final executables will be packaged as a WAR file or an \nexecutable JAR file, embedding all the dependencies.\nIn the context of microservices, this is one of the fundamental principles to be followed. \nEach microservice should bundle all the required dependencies and execution libraries \nsuch as the HTTP listener and so on in the final executable bundle.\n\n\nChapter 1\n[ 39 ]\nExternalizing configurations\nThis principle advises the externalization of all configuration parameters from the \ncode. An application's configuration parameters vary between environments, such as \nsupport to the e-mail IDs or URL of an external system, username, passwords, queue \nname, and so on. These will be different for development, testing, and production. \nAll service configurations should be externalized.\nThe same principle is obvious for microservices as well. The microservices \nconfiguration parameters should be loaded from an external source. This will also \nhelp to automate the release and deployment process as the only difference between \nthese environments is the configuration parameters.\nBacking services are addressable\nAll backing services should be accessible through an addressable URL. All services \nneed to talk to some external resources during the life cycle of their execution. \nFor example, they could be listening or sending messages to a messaging system, \nsending an e-mail, persisting data to database, and so on. All these services should \nbe reachable through a URL without complex communication requirements.\n\n\nDemystifying Microservices\n[ 40 ]\nIn the microservices world, microservices either talk to a messaging system to send \nor receive messages, or they could accept or send messages to other service APIs. In \na regular case, these are either HTTP endpoints using REST and JSON or TCP- or \nHTTP-based messaging endpoints.\nIsolation between build, release, and run\nThis principle advocates a strong isolation between the build, release, and run stages. \nThe build stage refers to compiling and producing binaries by including all the \nassets required. The release stage refers to combining binaries with environment-\nspecific configuration parameters. The run stage refers to running application on a \nspecific execution environment. The pipeline is unidirectional, so it is not possible \nto propagate changes from the run stages back to the build stage. Essentially, it also \nmeans that it is not recommended to do specific builds for production; rather, it has \nto go through the pipeline.\nIn microservices, the build will create executable JAR files, including the service \nruntime such as an HTTP listener. During the release phase, these executables will be \ncombined with release configurations such as production URLs and so on and create \na release version, most probably as a container similar to Docker. In the run stage, \nthese containers will be deployed on production via a container scheduler.\nStateless, shared nothing processes\nThis principle suggests that processes should be stateless and share nothing. If the \napplication is stateless, then it is fault tolerant and can be scaled out easily.\nAll microservices should be designed as stateless functions. If there is any \nrequirement to store a state, it should be done with a backing database or  \nin an in-memory cache.\n\n\nChapter 1\n[ 41 ]\nExposing services through port bindings\nA Twelve-Factor application is expected to be self-contained. Traditionally, \napplications are deployed to a server: a web server or an application server such as \nApache Tomcat or JBoss. A Twelve-Factor application does not rely on an external \nweb server. HTTP listeners such as Tomcat or Jetty have to be embedded in the \nservice itself.\nPort binding is one of the fundamental requirements for microservices to be \nautonomous and self-contained. Microservices embed service listeners as a part  \nof the service itself.\nConcurrency to scale out\nThis principle states that processes should be designed to scale out by replicating the \nprocesses. This is in addition to the use of threads within the process.\nIn the microservices world, services are designed to scale out rather than scale up. \nThe x axis scaling technique is primarily used for a scaling service by spinning up \nanother identical service instance. The services can be elastically scaled or shrunk \nbased on the traffic flow. Further to this, microservices may make use of parallel \nprocessing and concurrency frameworks to further speed up or scale up the \ntransaction processing.\nDisposability with minimal overhead\nThis principle advocates building applications with minimal startup and shutdown \ntimes with graceful shutdown support. In an automated deployment environment, \nwe should be able bring up or bring down instances as quick as possible. If the \napplication's startup or shutdown takes considerable time, it will have an adverse \neffect on automation. The startup time is proportionally related to the size of the \napplication. In a cloud environment targeting auto-scaling, we should be able to  \nspin up new instance quickly. This is also applicable when promoting new versions \nof services.\nIn the microservices context, in order to achieve full automation, it is extremely \nimportant to keep the size of the application as thin as possible, with minimal startup \nand shutdown time. Microservices also should consider a lazy loading of objects  \nand data.\n\n\nDemystifying Microservices\n[ 42 ]\nDevelopment and production parity\nThis principle states the importance of keeping development and production \nenvironments as identical as possible. For example, let's consider an application with \nmultiple services or processes, such as a job scheduler service, cache services, and \none or more application services. In a development environment, we tend to run all \nof them on a single machine, whereas in production, we will facilitate independent \nmachines to run each of these processes. This is primarily to manage the cost \nof infrastructure. The downside is that if production fails, there is no identical \nenvironment to re-produce and fix the issues.\nNot only is this principle valid for microservices, but it is also applicable to any \napplication development.\nExternalizing logs\nA Twelve-Factor application never attempts to store or ship log files. In a cloud, it is \nbetter to avoid local I/Os. If the I/Os are not fast enough in a given infrastructure, \nit could create a bottleneck. The solution to this is to use a centralized logging \nframework. Splunk, Greylog, Logstash, Logplex, and Loggly are some examples  \nof log shipping and analysis tools. The recommended approach is to ship logs to  \na central repository by tapping the logback appenders and write to one of the \nshippers' endpoints.\nIn a microservices ecosystem, this is very important as we are breaking a system  \ninto a number of smaller services, which could result in decentralized logging. If they \nstore logs in a local storage, it would be extremely difficult to correlate logs between \nservices.\nIn development, the microservice may direct the log stream to stdout, whereas in \nproduction, these streams will be captured by the log shippers and sent to a central \nlog service for storage and analysis.\n\n\nChapter 1\n[ 43 ]\nPackage admin processes\nApart from application services, most applications provide admin tasks as well. This \nprinciple advises to use the same release bundle as well as an identical environment \nfor both application services and admin tasks. Admin code should also be packaged \nalong with the application code.\nNot only is this principle valid for microservices, but also it is applicable to any \napplication development.\nMicroservice use cases\nA microservice is not a silver bullet and will not solve all the architectural challenges \nof today's world. There is no hard-and-fast rule or rigid guideline on when to use \nmicroservices.\nMicroservices may not fit in each and every use case. The success of microservices \nlargely depends on the selection of use cases. The first and the foremost activity is \nto do a litmus test of the use case against the microservices' benefits. The litmus test \nmust cover all the microservices' benefits we discussed earlier in this chapter. For a \ngiven use case, if there are no quantifiable benefits or the cost outweighs the benefits, \nthen the use case may not be the right choice for microservices.\nLet's discuss some commonly used scenarios that are suitable candidates for a \nmicroservices architecture:\n•\t\nMigrating a monolithic application due to improvements required in \nscalability, manageability, agility, or speed of delivery. Another similar \nscenario is rewriting an end-of-life heavily used legacy application. In \nboth cases, microservices present an opportunity. Using a microservices \narchitecture, it is possible to replatform a legacy application by slowly \ntransforming functions to microservices. There are benefits in this approach. \nThere is no humongous upfront investment required, no major disruption \nto business, and no severe business risks. As the service dependencies are \nknown, the microservices dependencies can be well managed.\n•\t\nUtility computing scenarios such as integrating an optimization service, \nforecasting service, price calculation service, prediction service, offer service, \nrecommendation service, and so on are good candidates for microservices. \nThese are independent stateless computing units that accept certain data, \napply algorithms, and return the results. Independent technical services such \nas the communication service, the encryption service, authentication services, \nand so on are also good candidates for microservices.\n\n\nDemystifying Microservices\n[ 44 ]\n•\t\nIn many cases, we can build headless business applications or services that \nare autonomous in nature—for instance, the payment service, login service, \nflight search service, customer profile service, notification service, and so on. \nThese are normally reused across multiple channels and, hence, are good \ncandidates for building them as microservices.\n•\t\nThere could be micro or macro applications that serve a single purpose and \nperforming a single responsibility. A simple time tracking application is an \nexample of this category. All it does is capture the time, duration, and task \nperformed. Common-use enterprise applications are also candidates for \nmicroservices.\n•\t\nBackend services of a well-architected, responsive client-side MVC web \napplication (the Backend as a Service (BaaS) scenario) load data on demand \nin response to the user navigation. In most of these scenarios, data could be \ncoming from multiple logically different data sources as described in the Fly \nBy Points example mentioned earlier.\n•\t\nHighly agile applications, applications demanding speed of delivery or time \nto market, innovation pilots, applications selected for DevOps, applications \nof the System of Innovation type, and so on could also be considered as \npotential candidates for the microservices architecture.\n•\t\nApplications that we could anticipate getting benefits from microservices \nsuch as polyglot requirements, applications that require Command Query \nResponsibility segregations (CQRS), and so on are also potential candidates \nof the microservices architecture.\nIf the use case falls into any of these categories, it is a potential candidate for the \nmicroservices architecture.\nThere are few scenarios in which we should consider avoiding microservices:\n•\t\nIf the organization's policies are forced to use centrally managed \nheavyweight components such as ESB to host a business logic or if the \norganization has any other policies that hinder the fundamental principles \nof microservices, then microservices are not the right solution unless the \norganizational process is relaxed.\n•\t\nIf the organization's culture, processes, and so on are based on the \ntraditional waterfall delivery model, lengthy release cycles, matrix teams, \nmanual deployments and cumbersome release processes, no infrastructure \nprovisioning, and so on, then microservices may not be the right fit. This \nis underpinned by Conway's Law. This states that there is a strong link \nbetween the organizational structure and software it creates.\n\n\nChapter 1\n[ 45 ]\nRead more about the Conway's Law at:\nhttp://www.melconway.com/Home/Conways_Law.html\nMicroservices early adopters\nMany organizations have already successfully embarked on their journey to the \nmicroservices world. In this section, we will examine some of the frontrunners on the \nmicroservices space to analyze why they did what they did and how they did it. We \nwill conduct some analysis at the end to draw some conclusions:\n•\t\nNetflix (www.netflix.com): Netflix, an international on-demand media \nstreaming company, is a pioneer in the microservices space. Netflix \ntransformed their large pool of developers developing traditional monolithic \ncode to smaller development teams producing microservices. These \nmicroservices work together to stream digital media to millions of Netflix \ncustomers. At Netflix, engineers started with monolithic, went through \nthe pain, and then broke the application into smaller units that are loosely \ncoupled and aligned to the business capability.\n•\t\nUber (www.uber.com): Uber, an international transportation network \ncompany, began in 2008 with a monolithic architecture with a single code \nbase. All services were embedded into the monolithic application. When \nUber expanded their business from one city to multiple cities, the challenges \nstarted. Uber then moved to SOA-based architecture by breaking the system \ninto smaller independent units. Each module was given to different teams and \nempowered them to choose their language, framework, and database. Uber \nhas many microservices deployed in their ecosystem using RPC and REST.\n•\t\nAirbnb (www.airbnb.com): Airbnb, a world leader providing a trusted \nmarketplace for accommodation, started with a monolithic application that \nperformed all the required functions of the business. Airbnb faced scalability \nissues with increased traffic. A single code base became too complicated to \nmanage, resulted in a poor separation of concerns, and ran into performance \nissues. Airbnb broke their monolithic application into smaller pieces with \nseparate code bases running on separate machines with separate deployment \ncycles. Airbnb developed their own microservices or SOA ecosystem around \nthese services.\n\n\nDemystifying Microservices\n[ 46 ]\n•\t\nOrbitz (www.orbitz.com): Orbitz, an online travel portal, started with a \nmonolithic architecture in the 2000s with a web layer, a business layer, and a \ndatabase layer. As Orbitz expanded their business, they faced manageability \nand scalability issues with monolithic-tiered architecture. Orbitz then went \nthrough continuous architecture changes. Later, Orbitz broke down their \nmonolithic to many smaller applications.\n•\t\neBay (www.ebay.com): eBay, one of the largest online retailers, started \nin the late 1990s with a monolithic Perl application and FreeBSD as the \ndatabase. eBay went through scaling issues as the business grew. It was \nconsistently investing in improving its architecture. In the mid 2000s, eBay \nmoved to smaller decomposed systems based on Java and web services. \nThey employed database partitions and functional segregation to meet the \nrequired scalability.\n•\t\nAmazon (www.amazon.com): Amazon, one of the largest online retailer \nwebsites, was run on a big monolithic application written on C++ \nin 2001. The well-architected monolithic application was based on a \ntiered architecture with many modular components. However, all these \ncomponents were tightly coupled. As a result, Amazon was not able to speed \nup their development cycle by splitting teams into smaller groups. Amazon \nthen separated out the code as independent functional services, wrapped \nwith web services, and eventually advanced to microservices.\n•\t\nGilt (www.gilt.com): Gilt, an online shopping website, began in 2007 with \na tiered monolithic Rails application and a Postgres database at the back. \nSimilarly to many other applications, as traffic volumes increased, the web \napplication was not able to provide the required resiliency. Gilt went through \nan architecture overhaul by introducing Java and polyglot persistence. Later, \nGilt moved to many smaller applications using the microservices concept.\n•\t\nTwitter (www.twitter.com): Twitter, one of the largest social websites, \nbegan with a three-tiered monolithic rails application in the mid 2000s. Later, \nwhen Twitter experienced growth in its user base, they went through an \narchitecture-refactoring cycle. With this refactoring, Twitter moved away \nfrom a typical web application to an API-based even driven core. Twitter \nuses Scala and Java to develop microservices with polyglot persistence.\n•\t\nNike (www.nike.com): Nike, the world leader in apparel and footwear, \ntransformed their monolithic applications to microservices. Similarly to many \nother organizations, Nike too was run with age-old legacy applications that \nwere hardly stable. In their journey, Nike moved to heavyweight commercial \nproducts with an objective to stabilize legacy applications but ended up in \nmonolithic applications that were expensive to scale, had long release cycles, \nand needed too much manual work to deploy and manage applications. \nLater, Nike moved to a microservices-based architecture that brought down \nthe development cycle considerably.\n\n\nChapter 1\n[ 47 ]\nThe common theme is monolithic migrations\nWhen we analyze the preceding enterprises, there is one common theme. All these \nenterprises started with monolithic applications and transitioned to a microservices \narchitecture by applying learning and pain points from their previous editions.\nEven today, many start-ups begin with monolith as it is easy to start, conceptualize, \nand then slowly move to microservices when the demand arises. Monolithic to \nmicroservices migration scenarios have an added advantage: they have all the \ninformation upfront, readily available for refactoring.\nThough, for all these enterprises, it is monolithic transformation, the catalysts were \ndifferent for different organizations. Some of the common motivations are a lack \nof scalability, long development cycles, process automation, manageability, and \nchanges in the business models.\nWhile monolithic migrations are no-brainers, there are opportunities to build \nmicroservices from the ground up. More than building ground-up systems, look \nfor opportunities to build smaller services that are quick wins for business—for \nexample, adding a trucking service to an airline's end-to-end cargo management \nsystem or adding a customer scoring service to a retailer's loyalty system. These \ncould be implemented as independent microservices exchanging messages with  \ntheir respective monolithic applications.\nAnother point is that many organizations use microservices only for their business-\ncritical customer engagement applications, leaving the rest of the legacy monolithic \napplications to take their own trajectory.\nAnother important observation is that most of the organizations examined \npreviously are at different levels of maturity in their microservices journey. When \neBay transitioned from a monolithic application in the early 2000s, they functionally \nsplit the application into smaller, independent, and deployable units. These logically \ndivided units are wrapped with web services. While single responsibility and \nautonomy are their underpinning principles, the architectures are limited to the \ntechnologies and tools available at that point in time. Organizations such as Netflix \nand Airbnb built capabilities of their own to solve the specific challenges they faced. \nTo summarize, all of these are not truly microservices, but are small, business-\naligned services following the same characteristics.\nThere is no state called \"definite or ultimate microservices\". It is a journey and is \nevolving and maturing day by day. The mantra for architects and developers is the \nreplaceability principle; build an architecture that maximizes the ability to replace its \nparts and minimizes the cost of replacing its parts. The bottom line is that enterprises \nshouldn't attempt to develop microservices by just following the hype.\n\n\nDemystifying Microservices\n[ 48 ]\nSummary\nIn this chapter, you learned about the fundamentals of microservices with the help of \na few examples.\nWe explored the evolution of microservices from traditional monolithic applications. \nWe examined some of the principles and the mind shift required for modern \napplication architectures. We also took a look at the characteristics and benefits \nof microservices and use cases. In this chapter, we established the microservices' \nrelationship with service-oriented architecture and Twelve-Factor Apps. Lastly, we \nanalyzed examples of a few enterprises from different industries.\nWe will develop a few sample microservices in the next chapter to bring more clarity \nto our learnings in this chapter.\n\n\n[ 49 ]\nBuilding Microservices with \nSpring Boot\nDeveloping microservices is not so tedious anymore thanks to the powerful \nSpring Boot framework. Spring Boot is a framework to develop production-ready \nmicroservices in Java.\nThis chapter will move from the microservices theory explained in the previous \nchapter to hands-on practice by reviewing code samples. This chapter will introduce \nthe Spring Boot framework and explain how Spring Boot can help build RESTful \nmicroservices in line with the principles and characteristics discussed in the previous \nchapter. Finally, some of the features offered by Spring Boot to make microservices \nproduction-ready will be reviewed.\nBy the end of this chapter, you will have learned about:\n•\t\nSetting up the latest Spring development environment\n•\t\nDeveloping RESTful services using the Spring framework\n•\t\nUsing Spring Boot to build fully qualified microservices\n•\t\nUseful Spring Boot features to build production-ready microservices\nSetting up a development environment\nTo crystalize microservices concepts, a couple of microservices will be built. For this, \nit is assumed that the following components are installed:\n•\t\nJDK 1.8: http://www.oracle.com/technetwork/java/javase/\ndownloads/jdk8-downloads-2133151.html\n•\t\nSpring Tool Suite 3.7.2 (STS): https://spring.io/tools/sts/all\n•\t\nMaven 3.3.1: https://maven.apache.org/download.cgi\n\n\nBuilding Microservices with Spring Boot\n[ 50 ]\nAlternately, other IDEs such as IntelliJ IDEA, NetBeans, or Eclipse could be used. \nSimilarly, alternate build tools such as Gradle can be used. It is assumed that the \nMaven repository, class path, and other path variables are set properly to run STS \nand Maven projects.\nThis chapter is based on the following versions of Spring libraries:\n•\t\nSpring Framework 4.2.6.RELEASE\n•\t\nSpring Boot 1.3.5.RELEASE\nDetailed steps to download the code bundle are mentioned in the \nPreface of this book. Have a look.\nThe code bundle for the book is also hosted on GitHub at \nhttps://github.com/PacktPublishing/Spring-\nMicroservices. We also have other code bundles from our rich \ncatalog of books and videos available at https://github.com/\nPacktPublishing/. Check them out!\nDeveloping a RESTful service – the \nlegacy approach\nThis example will review the traditional RESTful service development before \njumping deep into Spring Boot.\nSTS will be used to develop this REST/JSON service.\nThe full source code of this example is available as the \nlegacyrest project in the code files of this book.\n\n\nChapter 2\n[ 51 ]\nThe following are the steps to develop the first RESTful service:\n1.\t Start STS and set a workspace of choice for this project.\n2.\t Navigate to File | New | Project.\n3.\t Select Spring Legacy Project as shown in the following screenshot and click \non Next:\n\n\nBuilding Microservices with Spring Boot\n[ 52 ]\n4.\t Select Spring MVC Project as shown in the following diagram and click  \non Next:\n5.\t Select a top-level package name of choice. This example uses org.rvslab.\nchapter2.legacyrest as the top-level package.\n6.\t Then, click on Finish.\n7.\t This will create a project in the STS workspace with the name legacyrest.\nBefore proceeding further, pom.xml needs editing.\n",
      "page_number": 30
    },
    {
      "number": 2,
      "title": "[ 53 ]",
      "start_page": 80,
      "end_page": 133,
      "detection_method": "regex_chapter",
      "content": "Chapter 2\n[ 53 ]\n8.\t Change the Spring version to 4.2.6.RELEASE, as follows:\n<org.springframework-version>4.2.6.RELEASE</org.springframework-\nversion>\n9.\t Add Jackson dependencies in the pom.xml file for JSON-to-POJO and \nPOJO-to-JSON conversions. Note that the 2.*.* version is used to ensure \ncompatibility with Spring 4.\n<dependency>\n    <groupId>com.fasterxml.jackson.core</groupId>\n    <artifactId>jackson-databind</artifactId>\n    <version>2.6.4</version>\n</dependency>\n10.\t Some Java code needs to be added. In Java Resources, under legacyrest, \nexpand the package and open the default HomeController.java file:\n11.\t The default implementation is targeted more towards the MVC project. \nRewriting HomeController.java to return a JSON value in response to the \nREST call will do the trick. The resulting HomeController.java file will look \nsimilar to the following:\n@RestController\npublic class HomeController {\n  @RequestMapping(\"/\")\n  public Greet sayHello(){\n    return new Greet(\"Hello World!\");\n  }\n}\n\n\nBuilding Microservices with Spring Boot\n[ 54 ]\nclass Greet { \n  private String message;\n  public Greet(String message) {\n    this.message = message;\n  }\n  //add getter and setter\n}\nExamining the code, there are now two classes:\n°°\nGreet: This is a simple Java class with getters and setters to represent \na data object. There is only one attribute in the Greet class, which is \nmessage.\n°°\nHomeController.java: This is nothing but a Spring controller REST \nendpoint to handle HTTP requests.\nNote that the annotation used in HomeController is @RestController, \nwhich automatically injects @Controller and @ResponseBody and has the \nsame effect as the following code:\n@Controller\n@ResponseBody\npublic class HomeController { }\n12.\t The project can now be run by right-clicking on legacyrest, navigating to \nRun As | Run On Server, and then selecting the default server (Pivotal tc \nServer Developer Edition v3.1) that comes along with STS.\nThis should automatically start the server and deploy the web application on \nthe TC server.\nIf the server started properly, the following message will appear in the \nconsole:\nINFO : org.springframework.web.servlet.DispatcherServlet - \nFrameworkServlet 'appServlet': initialization completed in 906 ms\nMay 08, 2016 8:22:48 PM org.apache.catalina.startup.Catalina start\nINFO: Server startup in 2289 ms\n\n\nChapter 2\n[ 55 ]\n13.\t If everything is fine, STS will open a browser window to http://\nlocalhost:8080/legacyrest/ and display the JSON object as shown in \nthe browser. Right-click on and navigate to legacyrest | Properties | Web \nProject Settings and review Context Root to identify the context root of the \nweb application:\nThe alternate build option is to use Maven. Right-click on the project and navigate \nto Run As | Maven install. This will generate chapter2-1.0.0-BUILD-SNAPSHOT.\nwar under the target folder. This war is deployable in any servlet container such as \nTomcat, JBoss, and so on.\nMoving from traditional web applications \nto microservices\nCarefully examining the preceding RESTful service will reveal whether this really \nconstitutes a microservice. At first glance, the preceding RESTful service is a fully \nqualified interoperable REST/JSON service. However, it is not fully autonomous \nin nature. This is primarily because the service relies on an underlying application \nserver or web container. In the preceding example, a war was explicitly created and \ndeployed on a Tomcat server.\nThis is a traditional approach to developing RESTful services as a web application. \nHowever, from the microservices point of view, one needs a mechanism to develop \nservices as executables, self-contained JAR files with an embedded HTTP listener.\nSpring Boot is a tool that allows easy development of such kinds of services. \nDropwizard and WildFly Swarm are alternate server-less RESTful stacks.\n\n\nBuilding Microservices with Spring Boot\n[ 56 ]\nUsing Spring Boot to build RESTful \nmicroservices\nSpring Boot is a utility framework from the Spring team to bootstrap Spring-\nbased applications and microservices quickly and easily. The framework uses an \nopinionated approach over configurations for decision making, thereby reducing the \neffort required in writing a lot of boilerplate code and configurations. Using the 80-20 \nprinciple, developers should be able to kickstart a variety of Spring applications with \nmany default values. Spring Boot further presents opportunities for the developers \nto customize applications by overriding the autoconfigured values.\nSpring Boot not only increases the speed of development but also provides a set \nof production-ready ops features such as health checks and metrics collection. As \nSpring Boot masks many configuration parameters and abstracts many lower-level \nimplementations, it minimizes the chance of error to a certain extent. Spring Boot \nrecognizes the nature of the application based on the libraries available in the class \npath and runs the autoconfiguration classes packaged in these libraries.\nOften, many developers mistakenly see Spring Boot as a code generator, but in \nreality, it is not. Spring Boot only autoconfigures build files—for example, POM files \nin the case of Maven. It also sets properties, such as data source properties, based on \ncertain opinionated defaults. Take a look at the following code:\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-jpa</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.hsqldb</groupId>\n    <artifactId>hsqldb</artifactId>\n    <scope>runtime</scope>\n</dependency>\nFor instance, in the preceding case, Spring Boot understands that the project is set to \nuse the Spring Data JPA and HSQL databases. It automatically configures the driver \nclass and other connection parameters.\nOne of the great outcomes of Spring Boot is that it almost eliminates the need to have \ntraditional XML configurations. Spring Boot also enables microservices' development \nby packaging all the required runtime dependencies in a fat executable JAR file.\n\n\nChapter 2\n[ 57 ]\nGetting started with Spring Boot\nThere are different ways that Spring Boot-based application development can be \nstarted:\n•\t\nUsing the Spring Boot CLI as a command-line tool\n•\t\nUsing IDEs such as STS to provide Spring Boot, which are supported out of \nthe box\n•\t\nUsing the Spring Initializr project at http://start.spring.io\nAll these three options will be explored in this chapter, developing a variety of \nsample services.\nDeveloping the Spring Boot microservice \nusing the CLI\nThe easiest way to develop and demonstrate Spring Boot's capabilities is using the \nSpring Boot CLI, a command-line tool. Perform the following steps:\n1.\t Install the Spring Boot command-line tool by downloading the spring-\nboot-cli-1.3.5.RELEASE-bin.zip file from http://repo.spring.io/\nrelease/org/springframework/boot/spring-boot-cli/1.3.5.RELEASE/\nspring-boot-cli-1.3.5.RELEASE-bin.zip.\n2.\t Unzip the file into a directory of your choice. Open a terminal window and \nchange the terminal prompt to the bin folder.\nEnsure that the bin folder is added to the system path so that Spring Boot \ncan be run from any location.\n3.\t Verify the installation with the following command. If successful, the Spring \nCLI version will be printed in the console:\n$spring –-version\nSpring CLI v1.3.5.RELEASE\n\n\nBuilding Microservices with Spring Boot\n[ 58 ]\n4.\t As the next step, a quick REST service will be developed in Groovy, which \nis supported out of the box in Spring Boot. To do so, copy and paste the \nfollowing code using any editor of choice and save it as myfirstapp.groovy \nin any folder:\n@RestController\nclass HelloworldController {\n    @RequestMapping(\"/\")\n    String sayHello() {\n        \"Hello World!\"\n    }\n}\n5.\t In order to run this Groovy application, go to the folder where myfirstapp.\ngroovy is saved and execute the following command. The last few lines of \nthe server start-up log will be similar to the following:\n$spring run myfirstapp.groovy \n2016-05-09 18:13:55.351  INFO 35861 --- [nio-8080-exec-1] \no.s.web.servlet.DispatcherServlet        : FrameworkServlet \n'dispatcherServlet': initialization started\n2016-05-09 18:13:55.375  INFO 35861 --- [nio-8080-exec-1] \no.s.web.servlet.DispatcherServlet        : FrameworkServlet \n'dispatcherServlet': initialization completed in 24 ms\n6.\t Open a browser window and go to http://localhost:8080; the browser \nwill display the following message:\nHello World!\nThere is no war file created, and no Tomcat server was run. Spring Boot \nautomatically picked up Tomcat as the webserver and embedded it into the \napplication. This is a very basic, minimal microservice. The @RestController \nannotation, used in the previous code, will be examined in detail in the next example.\nDeveloping the Spring Boot Java \nmicroservice using STS\nIn this section, developing another Java-based REST/JSON Spring Boot service using \nSTS will be demonstrated.\n\n\nChapter 2\n[ 59 ]\nThe full source code of this example is available as the  \nchapter2.bootrest project in the code files of this book.\n1.\t Open STS, right-click within the Project Explorer window, navigate to \nNew | Project, and select Spring Starter Project, as shown in the following \nscreenshot, and click on Next:\nSpring Starter Project is a basic template wizard that provides a number of \nother starter libraries to select from.\n2.\t Type the project name as chapter2.bootrest or any other name of your \nchoice. It is important to choose the packaging as JAR. In traditional web \napplications, a war file is created and then deployed to a servlet container, \nwhereas Spring Boot packages all the dependencies to a self-contained, \nautonomous JAR file with an embedded HTTP listener.\n\n\nBuilding Microservices with Spring Boot\n[ 60 ]\n3.\t Select 1.8 under Java Version. Java 1.8 is recommended for Spring 4 \napplications. Change the other Maven properties such as Group, Artifact, \nand Package, as shown in the following screenshot:\n4.\t Once completed, click on Next.\n\n\nChapter 2\n[ 61 ]\n5.\t The wizard will show the library options. In this case, as the REST service is \ndeveloped, select Web under Web. This is an interesting step that tells Spring \nBoot that a Spring MVC web application is being developed so that Spring \nBoot can include the necessary libraries, including Tomcat as the HTTP \nlistener and other configurations, as required:\n\n\nBuilding Microservices with Spring Boot\n[ 62 ]\n6.\t Click on Finish.\nThis will generate a project named chapter2.bootrest in Project Explorer \nin STS:\n7.\t Take a moment to examine the generated application. Files that are of  \ninterest are:\n°°\npom.xml\n°°\nApplication.java\n°°\nApplication.properties\n°°\nApplicationTests.java\nExamining the POM file\nThe parent element is one of the interesting aspects in the pom.xml file. Take a look at \nthe following:\n<parent>\n  <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>1.3.4.RELEASE</version>\n</parent>\n\n\nChapter 2\n[ 63 ]\nThe spring-boot-starter-parent pattern is a bill of materials (BOM), a pattern \nused by Maven's dependency management. BOM is a special kind of POM file used \nto manage different library versions required for a project. The advantage of using \nthe spring-boot-starter-parent POM file is that developers need not worry about \nfinding the right compatible versions of different libraries such as Spring, Jersey, JUnit, \nLogback, Hibernate, Jackson, and so on. For instance, in our first legacy example, \na specific version of the Jackson library was added to work with Spring 4. In this \nexample, these are taken care of by the spring-boot-starter-parent pattern.\nThe starter POM file has a list of Boot dependencies, sensible resource filtering, and \nsensible plug-in configurations required for the Maven builds.\nRefer to https://github.com/spring-projects/\nspring-boot/blob/1.3.x/spring-boot-\ndependencies/pom.xml to take a look at the different \ndependencies provided in the starter parent (version 1.3.x). \nAll these dependencies can be overridden if required.\nThe starter POM file itself does not add JAR dependencies to the project. Instead,  \nit will only add library versions. Subsequently, when dependencies are added to  \nthe POM file, they refer to the library versions from this POM file. A snapshot of  \nsome of the properties are as shown as follows:\n<spring-boot.version>1.3.5.BUILD-SNAPSHOT</spring-boot.version>\n<hibernate.version>4.3.11.Final</hibernate.version>\n<jackson.version>2.6.6</jackson.version>\n<jersey.version>2.22.2</jersey.version>\n<logback.version>1.1.7</logback.version>\n<spring.version>4.2.6.RELEASE</spring.version>\n<spring-data-releasetrain.version>Gosling-SR4</spring-data-\nreleasetrain.version>\n<tomcat.version>8.0.33</tomcat.version>\nReviewing the dependency section, one can see that this is a clean and neat POM file \nwith only two dependencies, as follows:\n<dependencies>\n   <dependency>\n  <groupId>org.springframework.boot</groupId>\n  <artifactId>spring-boot-starter-web</artifactId>\n   </dependency>\n\n\nBuilding Microservices with Spring Boot\n[ 64 ]\n    \n   <dependency>\n  <groupId>org.springframework.boot</groupId>\n  <artifactId>spring-boot-starter-test</artifactId>\n  <scope>test</scope>\n   </dependency>\n</dependencies>\nAs web is selected, spring-boot-starter-web adds all dependencies required for \na Spring MVC project. It also includes dependencies to Tomcat as an embedded \nHTTP listener. This provides an effective way to get all the dependencies required as \na single bundle. Individual dependencies could be replaced with other libraries, for \nexample replacing Tomcat with Jetty.\nSimilar to web, Spring Boot comes up with a number of spring-boot-starter-* \nlibraries, such as amqp, aop, batch, data-jpa, thymeleaf, and so on.\nThe last thing to be reviewed in the pom.xml file is the Java 8 property. By default, \nthe parent POM file adds Java 6. It is recommended to override the Java version to 8 \nfor Spring:\n<java.version>1.8</java.version>\nExamining Application.java\nSpring Boot, by default, generated a org.rvslab.chapter2.Application.java \nclass under src/main/java to bootstrap, as follows:\n@SpringBootApplication\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n}\nThere is only a main method in Application, which will be invoked at startup as \nper the Java convention. The main method bootstraps the Spring Boot application by \ncalling the run method on SpringApplication. Application.class is passed as a \nparameter to tell Spring Boot that this is the primary component.\n\n\nChapter 2\n[ 65 ]\nMore importantly, the magic is done by the @SpringBootApplication annotation. \nThe @SpringBootApplication annotation is a top-level annotation that encapsulates \nthree other annotations, as shown in the following code snippet:\n@Configuration\n@EnableAutoConfiguration\n@ComponentScan\npublic class Application {\nThe @Configuration annotation hints that the contained class declares one or  \nmore @Bean definitions. The @Configuration annotation is meta-annotated with  \n@Component; therefore, it is a candidate for component scanning.\nThe @EnableAutoConfiguration annotation tells Spring Boot to automatically \nconfigure the Spring application based on the dependencies available in the class path.\nExamining application.properties\nA default application.properties file is placed under src/main/resources. \nIt is an important file to configure any required properties for the Spring Boot \napplication. At the moment, this file is kept empty and will be revisited with some \ntest cases later in this chapter.\nExamining ApplicationTests.java\nThe last file to be examined is ApplicationTests.java under src/test/java.  \nThis is a placeholder to write test cases against the Spring Boot application.\nTo implement the first RESTful service, add a REST endpoint, as follows:\n1.\t One can edit Application.java under src/main/java and add a RESTful \nservice implementation. The RESTful service is exactly the same as what was \ndone in the previous project. Append the following code at the end of the \nApplication.java file:\n@RestController\nclass GreetingController{\n  @RequestMapping(\"/\")\n  Greet greet(){\n    return new Greet(\"Hello World!\");\n  }\n\n\nBuilding Microservices with Spring Boot\n[ 66 ]\n}\nclass Greet {\n  private String message;\npublic Greet() {}\n  public Greet(String message) {\n    this.message = message;\n  }\n//add getter and setter\n}\n2.\t To run, navigate to Run As | Spring Boot App. Tomcat will be started on the \n8080 port:\nWe can notice from the log that:\n°°\nSpring Boot get its own process ID (in this case, it is 41130)\n°°\nSpring Boot is automatically started with the Tomcat server at the \nlocalhost, port 8080.\n3.\t Next, open a browser and point to http://localhost:8080. This will show \nthe JSON response as shown in the following screenshot:\nA key difference between the legacy service and this one is that the Spring Boot \nservice is self-contained. To make this clearer, run the Spring Boot application \noutside STS. Open a terminal window, go to the project folder, and run Maven,  \nas follows:\n$ maven install\n\n\nChapter 2\n[ 67 ]\nThis will generate a fat JAR file under the target folder of the project. Running the \napplication from the command line shows:\n$java -jar target/bootrest-0.0.1-SNAPSHOT.jar\nAs one can see, bootrest-0.0.1-SNAPSHOT.jar is self-contained and could be run \nas a standalone application. At this point, the JAR is as thin as 13 MB. Even though \nthe application is no more than just \"Hello World\", the Spring Boot service just \ndeveloped, practically follows the principles of microservices.\nTesting the Spring Boot microservice\nThere are multiple ways to test REST/JSON Spring Boot microservices. The easiest \nway is to use a web browser or a curl command pointing to the URL, as follows:\ncurl http://localhost:8080\nThere are number of tools available to test RESTful services, such as Postman, \nAdvanced REST client, SOAP UI, Paw, and so on.\nIn this example, to test the service, the default test class generated by Spring Boot \nwill be used.\nAdding a new test case to ApplicatonTests.java results in:\n@RunWith(SpringJUnit4ClassRunner.class)\n@SpringApplicationConfiguration(classes = Application.class)\n@WebIntegrationTest\npublic class ApplicationTests {\n  @Test\n  public void testVanillaService() {\n    RestTemplate restTemplate = new RestTemplate();\n    Greet greet = restTemplate.getForObject \n      (\"http://localhost:8080\", Greet.class);\n    Assert.assertEquals(\"Hello World!\", greet.getMessage());\n  }\n}\nNote that @WebIntegrationTest is added and @WebAppConfiguration removed \nat the class level. The @WebIntegrationTest annotation is a handy annotation that \nensures that the tests are fired against a fully up-and-running server. Alternately,  \na combination of @WebAppConfiguration and @IntegrationTest will give the  \nsame result.\n\n\nBuilding Microservices with Spring Boot\n[ 68 ]\nAlso note that RestTemplate is used to call the RESTful service. RestTemplate is a \nutility class that abstracts the lower-level details of the HTTP client.\nTo test this, one can open a terminal window, go to the project folder, and run  \nmvn install.\nDeveloping the Spring Boot microservice \nusing Spring Initializr – the HATEOAS \nexample\nIn the next example, Spring Initializr will be used to create a Spring Boot project. \nSpring Initializr is a drop-in replacement for the STS project wizard and provides \na web UI to configure and generate a Spring Boot project. One of the advantages of \nSpring Initializr is that it can generate a project through the website that then can be \nimported into any IDE.\nIn this example, the concept of HATEOAS (short for Hypertext As The Engine Of \nApplication State) for REST-based services and the HAL (Hypertext Application \nLanguage) browser will be examined.\nHATEOAS is a REST service pattern in which navigation links are provided as part \nof the payload metadata. The client application determines the state and follows the \ntransition URLs provided as part of the state. This methodology is particularly useful \nin responsive mobile and web applications in which the client downloads additional \ndata based on user navigation patterns.\nThe HAL browser is a handy API browser for hal+json data. HAL is a format based \non JSON that establishes conventions to represent hyperlinks between resources. \nHAL helps APIs be more explorable and discoverable.\nThe full source code of this example is available as the  \nchapter2.boothateoas project in the code files of this book.\n\n\nChapter 2\n[ 69 ]\nHere are the concrete steps to develop a HATEOAS sample using Spring Initilizr:\n1.\t In order to use Spring Initilizr, go to https://start.spring.io:\n2.\t Fill the details, such as whether it is a Maven project, Spring Boot version, \ngroup, and artifact ID, as shown earlier, and click on Switch to the full \nversion link under the Generate Project button. Select Web, HATEOAS,  \nand Rest Repositories HAL Browser. Make sure that the Java version is 8 \nand the package type is selected as JAR:\n\n\nBuilding Microservices with Spring Boot\n[ 70 ]\n3.\t Once selected, hit the Generate Project button. This will generate a Maven \nproject and download the project as a ZIP file into the download directory of \nthe browser.\n4.\t Unzip the file and save it to a directory of your choice.\n5.\t Open STS, go to the File menu and click on Import:\n6.\t Navigate to Maven | Existing Maven Projects and click on Next.\n7.\t Click on Browse next to Root Directory and select the unzipped folder.  \nClick on Finish. This will load the generated Maven project into STS'  \nProject Explorer.\n\n\nChapter 2\n[ 71 ]\n8.\t Edit the Application.java file to add a new REST endpoint, as follows:\n@RequestMapping(\"/greeting\")\n@ResponseBody\npublic HttpEntity<Greet> greeting(@RequestParam(value = \"name\", \nrequired = false, defaultValue = \"HATEOAS\") String name) {\n       Greet greet = new Greet(\"Hello \" + name);\n       greet.add(linkTo(methodOn(GreetingController. \n         class).greeting(name)).withSelfRel());\n       return new ResponseEntity<Greet>(greet,  \n         HttpStatus.OK);\n}\n9.\t Note that this is the same GreetingController class as in the previous \nexample. However, a method was added this time named greeting. In \nthis new method, an additional optional request parameter is defined and \ndefaulted to HATEOAS. The following code adds a link to the resulting JSON \ncode. In this case, it adds the link to the same API:\ngreet.add(linkTo(methodOn(GreetingController.class).\ngreeting(name)).withSelfRel());\nIn order to do this, we need to extend the Greet class from \nResourceSupport, as shown here. The rest of the code remains the same:\nclass Greet extends ResourceSupport{\n10.\t The add method is a method in ResourceSupport. The linkTo and \nmethodOn methods are static methods of ControllerLinkBuilder, a utility \nclass for creating links on controller classes. The methodOn method will do \na dummy method invocation, and linkTo will create a link to the controller \nclass. In this case, we will use withSelfRel to point it to itself.\n11.\t This will essentially produce a link, /greeting?name=HATEOAS, by default.  \nA client can read the link and initiate another call.\n12.\t Run this as a Spring Boot app. Once the server startup is complete, point the \nbrowser to http://localhost:8080.\n\n\nBuilding Microservices with Spring Boot\n[ 72 ]\n13.\t This will open the HAL browser window. In the Explorer field,  \ntype /greeting?name=World! and click on the Go button. If everything \nis fine, the HAL browser will show the response details as shown in the \nfollowing screenshot:\nAs shown in the screenshot, the Response Body section has the result with a link \nwith href pointing back to the same service. This is because we pointed the reference \nto itself. Also, review the Links section. The little green box against self is the \nnavigable link.\nIt does not make much sense in this simple example, but this could be handy in \nlarger applications with many related entities. Using the links provided, the client \ncan easily navigate back and forth between these entities with ease.\nWhat's next?\nA number of basic Spring Boot examples have been reviewed so far. The rest of this \nchapter will examine some of the Spring Boot features that are important from a \nmicroservices development perspective. In the upcoming sections, we will take a \nlook at how to work with dynamically configurable properties, change the default \nembedded web server, add security to the microservices, and implement cross-origin \nbehavior when dealing with microservices.\nThe full source code of this example is available as the  \nchapter2.boot-advanced project in the code files of this book.\n\n\nChapter 2\n[ 73 ]\nThe Spring Boot configuration\nIn this section, the focus will be on the configuration aspects of Spring Boot. The \nchapter2.bootrest project, already developed, will be modified in this section \nto showcase configuration capabilities. Copy and paste chapter2.bootrest and \nrename the project as chapter2.boot-advanced.\nUnderstanding the Spring Boot \nautoconfiguration\nSpring Boot uses convention over configuration by scanning the dependent \nlibraries available in the class path. For each spring-boot-starter-* dependency \nin the POM file, Spring Boot executes a default AutoConfiguration class. \nAutoConfiguration classes use the *AutoConfiguration lexical pattern, where * \nrepresents the library. For example, the autoconfiguration of JPA repositories is done \nthrough JpaRepositoriesAutoConfiguration.\nRun the application with --debug to see the autoconfiguration report. The following \ncommand shows the autoconfiguration report for the chapter2.boot-advanced \nproject:\n$java -jar target/bootadvanced-0.0.1-SNAPSHOT.jar --debug\nHere are some examples of the autoconfiguration classes:\n•\t\nServerPropertiesAutoConfiguration\n•\t\nRepositoryRestMvcAutoConfiguration\n•\t\nJpaRepositoriesAutoConfiguration\n•\t\nJmsAutoConfiguration\nIt is possible to exclude the autoconfiguration of certain libraries if the application \nhas special requirements and you want to get full control of the configurations.  \nThe following is an example of excluding DataSourceAutoConfiguration:\n@EnableAutoConfiguration(exclude={DataSourceAutoConfiguration.class})\nwww.allitebooks.com\n\n\nBuilding Microservices with Spring Boot\n[ 74 ]\nOverriding default configuration values\nIt is also possible to override default configuration values using the application.\nproperties file. STS provides an easy-to-autocomplete, contextual help on \napplication.properties, as shown in the following screenshot:\nIn the preceding screenshot, server.port is edited to be set as 9090. Running this \napplication again will start the server on port 9090.\nChanging the location of the configuration file\nIn order to align with the Twelve-Factor app, configuration parameters need to \nbe externalized from the code. Spring Boot externalizes all configurations into \napplication.properties. However, it is still part of the application's build. \nFurthermore, properties can be read from outside the package by setting the \nfollowing properties: \nspring.config.name= # config file name  \nspring.config.location= # location of config file\nHere, spring.config.location could be a local file location.\nThe following command starts the Spring Boot application with an externally \nprovided configuration file:\n$java -jar target/bootadvanced-0.0.1-SNAPSHOT.jar --spring.config.\nname=bootrest.properties\n\n\nChapter 2\n[ 75 ]\nReading custom properties\nAt startup, SpringApplication loads all the properties and adds them to the Spring \nEnvironment class. Add a custom property to the application.properties file. \nIn this case, the custom property is named bootrest.customproperty. Autowire \nthe Spring Environment class into the GreetingController class. Edit the \nGreetingController class to read the custom property from Environment  \nand add a log statement to print the custom property to the console.\nPerform the following steps to do this:\n1.\t Add the following property to the application.properties file:\nbootrest.customproperty=hello\n2.\t Then, edit the GreetingController class as follows:\n@Autowired\nEnvironment env;\nGreet greet(){\n    logger.info(\"bootrest.customproperty \"+  \n      env.getProperty(\"bootrest.customproperty\"));\n    return new Greet(\"Hello World!\");\n}\n3.\t Rerun the application. The log statement prints the custom variable in the \nconsole, as follows:\norg.rvslab.chapter2.GreetingController   : bootrest.customproperty \nhello\nUsing a .yaml file for configuration\nAs an alternate to application.properties, one may use a .yaml file. YAML \nprovides a JSON-like structured configuration compared to the flat properties file.\nTo see this in action, simply replace application.properties with application.\nyaml and add the following property:\nserver\n  port: 9080\nRerun the application to see the port printed in the console.\n\n\nBuilding Microservices with Spring Boot\n[ 76 ]\nUsing multiple configuration profiles\nFurthermore, it is possible to have different profiles such as development, testing, \nstaging, production, and so on. These are logical names. Using these, one can \nconfigure different values for the same properties for different environments. \nThis is quite handy when running the Spring Boot application against different \nenvironments. In such cases, there is no rebuild required when moving from one \nenvironment to another.\nUpdate the .yaml file as follows. The Spring Boot group profiles properties based on \nthe dotted separator:\nspring:\n    profiles: development\nserver:\n      port: 9090\n---\nspring:\n    profiles: production\nserver:\n      port: 8080\nRun the Spring Boot application as follows to see the use of profiles:\nmvn -Dspring.profiles.active=production install\nmvn -Dspring.profiles.active=development install\nActive profiles can be specified programmatically using the @ActiveProfiles \nannotation, which is especially useful when running test cases, as follows:\n@ActiveProfiles(\"test\")\nOther options to read properties\nThe properties can be loaded in a number of ways, such as the following:\n•\t\nCommand-line parameters (-Dhost.port =9090)\n•\t\nOperating system environment variables\n•\t\nJNDI (java:comp/env)\n\n\nChapter 2\n[ 77 ]\nChanging the default embedded web \nserver\nEmbedded HTTP listeners can easily be customized as follows. By default, Spring \nBoot supports Tomcat, Jetty, and Undertow. In the following example, Tomcat is \nreplaced with Undertow:\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-web</artifactId>\n    <exclusions>\n        <exclusion>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-tomcat</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-undertow</artifactId>\n</dependency>\nImplementing Spring Boot security\nIt is important to secure microservices. In this section, some basic measures to \nsecure Spring Boot microservices will be reviewed using chapter2.bootrest to \ndemonstrate the security features.\nSecuring microservices with basic security\nAdding basic authentication to Spring Boot is pretty simple. Add the following \ndependency to pom.xml. This will include the necessary Spring security library files:\n<dependency>\n  <groupId>org.springframework.boot</groupId> \n  <artifactId>spring-boot-starter-security</artifactId>\n</dependency>\n\n\nBuilding Microservices with Spring Boot\n[ 78 ]\nOpen Application.java and add @EnableGlobalMethodSecurity to the \nApplication class. This annotation will enable method-level security:\n@EnableGlobalMethodSecurity\n@SpringBootApplication\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n}\nThe default basic authentication assumes the user as being user. The default \npassword will be printed in the console at startup. Alternately, the username  \nand password can be added in application.properties, as shown here:\nsecurity.user.name=guest\nsecurity.user.password=guest123\nAdd a new test case in ApplicationTests to test the secure service results,  \nas in the following:\n  @Test\n  public void testSecureService() {  \n    String plainCreds = \"guest:guest123\";\n    HttpHeaders headers = new HttpHeaders();\n    headers.add(\"Authorization\", \"Basic \" + new String(Base64.\nencode(plainCreds.getBytes())));\n    HttpEntity<String> request = new HttpEntity<String>(headers);\n    RestTemplate restTemplate = new RestTemplate();\n    \n    ResponseEntity<Greet> response = restTemplate.exchange(\"http://\nlocalhost:8080\", HttpMethod.GET, request, Greet.class);\n    Assert.assertEquals(\"Hello World!\", response.getBody().\ngetMessage());\n  }\nAs shown in the code, a new Authorization request header with Base64 encoding \nthe username-password string is created.\nRerun the application using Maven. Note that the new test case passed, but the old \ntest case failed with an exception. The earlier test case now runs without credentials, \nand as a result, the server rejected the request with the following message:\norg.springframework.web.client.HttpClientErrorException: 401 Unauthorized\n\n\nChapter 2\n[ 79 ]\nSecuring a microservice with OAuth2\nIn this section, we will take a look at the basic Spring Boot configuration for OAuth2. \nWhen a client application requires access to a protected resource, the client sends \na request to an authorization server. The authorization server validates the request \nand provides an access token. This access token is validated for every client-to-server \nrequest. The request and response sent back and forth depends on the grant type.\nRead more about OAuth and grant types at http://oauth.net.\nThe resource owner password credentials grant approach will be used in this \nexample:\nIn this case, as shown in the preceding diagram, the resource owner provides the \nclient with a username and password. The client then sends a token request to the \nauthorization server by providing the credential information. The authorization \nserver authorizes the client and returns with an access token. On every subsequent \nrequest, the server validates the client token.\n\n\nBuilding Microservices with Spring Boot\n[ 80 ]\nTo implement OAuth2 in our example, perform the following steps:\n1.\t As a first step, update pom.xml with the OAuth2 dependency, as follows:\n<dependency>\n  <groupId>org.springframework.security.oauth</groupId>\n  <artifactId>spring-security-oauth2</artifactId>\n  <version>2.0.9.RELEASE</version>\n</dependency>\n2.\t Next, add two new annotations, @EnableAuthorizationServer  \nand @EnableResourceServer, to the Application.java file. The  \n@EnableAuthorizationServer annotation creates an authorization server \nwith an in-memory repository to store client tokens and provide clients with \na username, password, client ID, and secret. The @EnableResourceServer \nannotation is used to access the tokens. This enables a spring security filter \nthat is authenticated via an incoming OAuth2 token.\nIn our example, both the authorization server and resource server are the \nsame. However, in practice, these two will run separately. Take a look at the \nfollowing code:\n@EnableResourceServer\n@EnableAuthorizationServer\n@SpringBootApplication\npublic class Application {\n3.\t Add the following properties to the application.properties file:\nsecurity.user.name=guest\nsecurity.user.password=guest123\nsecurity.oauth2.client.clientId: trustedclient\nsecurity.oauth2.client.clientSecret: trustedclient123\nsecurity.oauth2.client.authorized-grant-types: authorization_\ncode,refresh_token,password\nsecurity.oauth2.client.scope: openid\n\n\nChapter 2\n[ 81 ]\n4.\t Then, add another test case to test OAuth2, as follows:\n  @Test\n  public void testOAuthService() {\n        ResourceOwnerPasswordResourceDetails resource = new \nResourceOwnerPasswordResourceDetails();\n        resource.setUsername(\"guest\");\n        resource.setPassword(\"guest123\");\n          resource.setAccessTokenUri(\"http://localhost:8080/oauth/\ntoken\");\n        resource.setClientId(\"trustedclient\");\n        resource.setClientSecret(\"trustedclient123\");\n        resource.setGrantType(\"password\");\n  \n        DefaultOAuth2ClientContext clientContext = new \nDefaultOAuth2ClientContext();\n        OAuth2RestTemplate restTemplate = new \nOAuth2RestTemplate(resource, clientContext);\n \n        Greet greet = restTemplate.getForObject(\"http://\nlocalhost:8080\", Greet.class);\n        Assert.assertEquals(\"Hello World!\", greet.getMessage());\n  }\nAs shown in the preceding code, a special REST template, \nOAuth2RestTemplate, is created by passing the resource details \nencapsulated in a resource details object. This REST template handles the \nOAuth2 processes underneath. The access token URI is the endpoint for  \nthe token access.\n5.\t Rerun the application using mvn install. The first two test cases will \nfail, and the new one will succeed. This is because the server only accepts \nOAuth2-enabled requests.\nThese are quick configurations provided by Spring Boot out of the box but \nare not good enough to be production grade. We may need to customize \nResourceServerConfigurer and AuthorizationServerConfigurer to make  \nthem production-ready. This notwithstanding, the approach remains the same.\n\n\nBuilding Microservices with Spring Boot\n[ 82 ]\nEnabling cross-origin access for \nmicroservices\nBrowsers are generally restricted when client-side web applications running from \none origin request data from another origin. Enabling cross-origin access is generally \ntermed as CORS (Cross-Origin Resource Sharing).\nThis example shows how to enable cross-origin requests. With microservices, as each \nservice runs with its own origin, it will easily get into the issue of a client-side web \napplication consuming data from multiple origins. For instance, a scenario where \na browser client accessing Customer from the Customer microservice and Order \nHistory from the Order microservices is very common in the microservices world.\nSpring Boot provides a simple declarative approach to enabling cross-origin \nrequests. The following example shows how to enable a microservice to enable cross-\norigin requests:\n@RestController\nclass GreetingController{\n  @CrossOrigin\n  @RequestMapping(\"/\")\n  Greet greet(){\n    return new Greet(\"Hello World!\");\n  }\n}\n\n\nChapter 2\n[ 83 ]\nBy default, all the origins and headers are accepted. We can further customize  \nthe cross-origin annotations by giving access to specific origins, as follows. The  \n@CrossOrigin annotation enables a method or class to accept cross-origin requests:\n@CrossOrigin(\"http://mytrustedorigin.com\")\nGlobal CORS can be enabled using the WebMvcConfigurer bean and customizing  \nthe addCorsMappings(CorsRegistry registry) method.\nImplementing Spring Boot messaging\nIn an ideal case, all microservice interactions are expected to happen asynchronously \nusing publish-subscribe semantics. Spring Boot provides a hassle-free mechanism to \nconfigure messaging solutions:\nIn this example, we will create a Spring Boot application with a sender and receiver, \nboth connected though an external queue. Perform the following steps:\nThe full source code of this example is available as the  \nchapter2.bootmessaging project in the code files of this book.\n\n\nBuilding Microservices with Spring Boot\n[ 84 ]\n1.\t Create a new project using STS to demonstrate this capability. In this \nexample, instead of selecting Web, select AMQP under I/O:\n2.\t Rabbit MQ will also be needed for this example. Download and install the \nlatest version of Rabbit MQ from https://www.rabbitmq.com/download.\nhtml.\nRabbit MQ 3.5.6 is used in this book.\n3.\t Follow the installation steps documented on the site. Once ready, start the \nRabbitMQ server via the following command:\n$./rabbitmq-server\n4.\t Make the configuration changes to the application.properties file to \nreflect the RabbitMQ configuration. The following configuration uses the \ndefault port, username, and password of RabbitMQ:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\n\n\nChapter 2\n[ 85 ]\n5.\t Add a message sender component and a queue named TestQ of the org.\nspringframework.amqp.core.Queue type to the Application.java file \nunder src/main/java. RabbitMessagingTemplate is a convenient way to \nsend messages, which will abstract all the messaging semantics. Spring Boot \nprovides all boilerplate configurations to send messages:\n@Component \nclass Sender {\n  @Autowired\n  RabbitMessagingTemplate template;\n  @Bean\n  Queue queue() {\n    return new Queue(\"TestQ\", false);\n  }\n  public void send(String message){\n    template.convertAndSend(\"TestQ\", message);\n  }\n}\n6.\t To receive the message, all that needs to be used is a @RabbitListener \nannotation. Spring Boot autoconfigures all the required boilerplate \nconfigurations:\n@Component\nclass Receiver {\n    @RabbitListener(queues = \"TestQ\")\n    public void processMessage(String content) {\n       System.out.println(content);\n    }\n}\n7.\t The last piece of this exercise is to wire the sender to our main application \nand implement the run method of CommandLineRunner to initiate the \nmessage sending. When the application is initialized, it invokes the run \nmethod of CommandLineRunner, as follows:\n@SpringBootApplication\npublic class Application implements CommandLineRunner{\n  @Autowired\n  Sender sender;\n  \n    public static void main(String[] args) {\n\n\nBuilding Microservices with Spring Boot\n[ 86 ]\n        SpringApplication.run(Application.class, args);\n    }\n    \n    @Override\n    public void run(String... args) throws Exception {\n      sender.send(\"Hello Messaging..!!!\");\n    }\n}\n8.\t Run the application as a Spring Boot application and verify the output.  \nThe following message will be printed in the console:\nHello Messaging..!!!\nDeveloping a comprehensive \nmicroservice example\nSo far, the examples we have considered are no more than just a simple \"Hello \nworld.\" Putting together what we have learned, this section demonstrates an \nend-to-end Customer Profile microservice implementation. The Customer Profile \nmicroservices will demonstrate interaction between different microservices. It also \ndemonstrates microservices with business logic and primitive data stores.\nIn this example, two microservices, the Customer Profile and Customer Notification \nservices, will be developed:\nAs shown in the diagram, the Customer Profile microservice exposes methods to \ncreate, read, update, and delete (CRUD) a customer and a registration service to \nregister a customer. The registration process applies certain business logic, saves the \ncustomer profile, and sends a message to the Customer Notification microservice. \nThe Customer Notification microservice accepts the message sent by the registration \nservice and sends an e-mail message to the customer using an SMTP server. \nAsynchronous messaging is used to integrate Customer Profile with the Customer \nNotification service.\n\n\nChapter 2\n[ 87 ]\nThe Customer microservices class domain model diagram is as shown here:\nCustomerController in the diagram is the REST endpoint, which invokes a \ncomponent class, CustomerComponent. The component class/bean handles all the \nbusiness logic. CustomerRepository is a Spring data JPA repository defined to \nhandle the persistence of the Customer entity.\nThe full source code of this example is available as the  \nchapter2.bootcustomer and chapter2.\nbootcustomernotification projects in the code files of this book.\n1.\t Create a new Spring Boot project and call it chapter2.bootcustomer, the \nsame way as earlier. Select the options as in the following screenshot in the \nstarter module selection screen:\n\n\nBuilding Microservices with Spring Boot\n[ 88 ]\nThis will create a web project with JPA, the REST repository, and H2  \nas a database. H2 is a tiny in-memory embedded database with which it is \neasy to demonstrate database features. In the real world, it is recommended \nto use an appropriate enterprise-grade database. This example uses JPA to \ndefine persistence entities and the REST repository to expose REST-based \nrepository services.\nThe project structure will be similar to the following screenshot:\n2.\t Start building the application by adding an Entity class named Customer. For \nsimplicity, there are only three fields added to the Customer Entity class: the \nautogenerated id field, name, and email. Take a look at the following code:\n@Entity\nclass Customer {\n  @Id\n  @GeneratedValue(strategy = GenerationType.AUTO)\n  private Long id;\n  private String name;\n  private String email;\n3.\t Add a repository class to handle the persistence handling of Customer. \nCustomerRepository extends the standard JPA repository. This means \nthat all CRUD methods and default finder methods are automatically \nimplemented by the Spring Data JPA repository, as follows:\n@RepositoryRestResource\ninterface CustomerRespository extends JpaRepository \n<Customer,Long>{\n  Optional<Customer> findByName(@Param(\"name\") String name);\n}\n\n\nChapter 2\n[ 89 ]\nIn this example, we added a new method to the repository class, findByName, \nwhich essentially searches the customer based on the customer name and \nreturns a Customer object if there is a matching name.\n4.\t The @RepositoryRestResource annotation enables the repository  \naccess through RESTful services. This will also enable HATEOAS and HAL \nby default. As for CRUD methods there is no additional business logic \nrequired, we will leave it as it is without controller or component classes. \nUsing HATEOAS will help us navigate through Customer Repository \nmethods effortlessly.\nNote that there is no configuration added anywhere to point to any database. \nAs H2 libraries are in the class path, all the configuration is done by default \nby Spring Boot based on the H2 autoconfiguration.\n5.\t Update the Application.java file by adding CommandLineRunner to \ninitialize the repository with some customer records, as follows:\n@SpringBootApplication\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n    @Bean\n  CommandLineRunner init(CustomerRespository repo) {\n  return (evt) ->  {\n    repo.save(new Customer(\"Adam\",\"adam@boot.com\"));\n    repo.save(new Customer(\"John\",\"john@boot.com\"));\n  repo.save(new Customer(\"Smith\",\"smith@boot.com\"));\n    repo.save(new Customer(\"Edgar\",\"edgar@boot.com\"));\n    repo.save(new Customer(\"Martin\",\"martin@boot.com\"));\n    repo.save(new Customer(\"Tom\",\"tom@boot.com\"));\n    repo.save(new Customer(\"Sean\",\"sean@boot.com\"));\n  };\n  }\n}\n6.\t CommandLineRunner, defined as a bean, indicates that it should run when \nit is contained in SpringApplication. This will insert six sample customer \nrecords into the database at startup.\n7.\t At this point, run the application as Spring Boot App. Open the HAL \nbrowser and point the browser to http://localhost:8080.\n\n\nBuilding Microservices with Spring Boot\n[ 90 ]\n8.\t In the Explorer section, point to http://localhost:8080/customers and \nclick on Go. This will list all the customers in the Response Body section of \nthe HAL browser.\n9.\t In the Explorer section, enter http://localhost:8080/customers?size=2\n&page=1&sort=name and click on Go. This will automatically execute paging \nand sorting on the repository and return the result.\nAs the page size is set to 2 and the first page is requested, it will come back \nwith two records in a sorted order.\n10.\t Review the Links section. As shown in the following screenshot, it will \nfacilitate navigating first, next, prev, and last. These are done using the \nHATEOAS links automatically generated by the repository browser:\n11.\t Also, one can explore the details of a customer by selecting the appropriate \nlink, such as http://localhost:8080/customers/2.\n\n\nChapter 2\n[ 91 ]\n12.\t As the next step, add a controller class, CustomerController, to handle \nservice endpoints. There is only one endpoint in this class, /register, which \nis used to register a customer. If successful, it returns the Customer object as \nthe response, as follows:\n@RestController\nclass CustomerController{\n  \n  @Autowired\n  CustomerRegistrar customerRegistrar;\n  \n  @RequestMapping( path=\"/register\", method = RequestMethod.POST)\n  Customer register(@RequestBody Customer customer){\n    return customerRegistrar.register(customer);\n  }\n}\n13.\t A CustomerRegistrar component is added to handle the business logic. \nIn this case, there is only minimal business logic added to the component. \nIn this component class, while registering a customer, we will just check \nwhether the customer name already exists in the database or not. If it does \nnot exist, then we will insert a new record, and otherwise, we will send an \nerror message back, as follows:\n@Component \nclass CustomerRegistrar {\n  \n  CustomerRespository customerRespository;\n  \n  @Autowired\n  CustomerRegistrar(CustomerRespository customerRespository){\n    this.customerRespository = customerRespository;\n  }\n  \n  Customer register(Customer customer){\n    Optional<Customer> existingCustomer = customerRespository.\nfindByName(customer.getName());\n    if (existingCustomer.isPresent()){\n      throw new RuntimeException(\"is already exists\");\n    } else {\n      customerRespository.save(customer); \n    }\n    return customer;\n  }\n}\n\n\nBuilding Microservices with Spring Boot\n[ 92 ]\n14.\t Restart the Boot application and test using the HAL browser via the URL \nhttp://localhost:8080.\n15.\t Point the Explorer field to http://localhost:8080/customers. Review the \nresults in the Links section:\n16.\t Click on the NON-GET option against self. This will open a form to create a \nnew customer:\n17.\t Fill the form and change the Action as shown in the diagram. Click on the \nMake Request button. This will call the register service and register the \ncustomer. Try giving a duplicate name to test the negative case.\n\n\nChapter 2\n[ 93 ]\n18.\t Let's complete the last part in the example by integrating the Customer \nNotification service to notify the customer. When registration is successful, \nsend an e-mail to the customer by asynchronously calling the Customer \nNotification microservice.\n19.\t First update CustomerRegistrar to call the second service. This is done \nthrough messaging. In this case, we injected a Sender component to send a \nnotification to the customer by passing the customer's e-mail address to the \nsender, as follows:\n@Component \n@Lazy\nclass CustomerRegistrar {\n  \n  CustomerRespository customerRespository;\n  Sender sender;\n  \n  @Autowired\n  CustomerRegistrar(CustomerRespository customerRespository, \nSender sender){\n    this.customerRespository = customerRespository;\n    this.sender = sender;\n  }\n  \n  Customer register(Customer customer){\n    Optional<Customer> existingCustomer = customerRespository.\nfindByName(customer.getName());\n    if (existingCustomer.isPresent()){\n      throw new RuntimeException(\"is already exists\");\n    } else {\n      customerRespository.save(customer); \n      sender.send(customer.getEmail());\n    } \n    return customer;\n  }\n} \n20.\t The sender component will be based on RabbitMQ and AMQP. In this \nexample, RabbitMessagingTemplate is used as explored in the last \nmessaging example; take a look at the following:\n@Component \n@Lazy\nclass Sender {\n  \n  @Autowired\n\n\nBuilding Microservices with Spring Boot\n[ 94 ]\n  RabbitMessagingTemplate template;\n  @Bean\n  Queue queue() {\n    return new Queue(\"CustomerQ\", false);\n  }\n  \n  public void send(String message){\n    template.convertAndSend(\"CustomerQ\", message);\n  }\n}\nThe @Lazy annotation is a useful one and it helps to increase the boot startup \ntime. These beans will be initialized only when the need arises.\n21.\t We will also update the application.property file to include Rabbit MQ-\nrelated properties, as follows:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\n22.\t We are ready to send the message. To consume the message and send \ne-mails, we will create a notification service. For this, let's create another \nSpring Boot service, chapter2.bootcustomernotification. Make sure that \nthe AMQP and Mail starter libraries are selected when creating the Spring \nBoot service. Both AMQP and Mail are under I/O.\n23.\t The package structure of the chapter2.bootcustomernotification project \nis as shown here:\n\n\nChapter 2\n[ 95 ]\n24.\t Add a Receiver class. The Receiver class waits for a message on customer. \nThis will receive a message sent by the Customer Profile service. On the \narrival of a message, it sends an e-mail, as follows:\n@Component\nclass Receiver {  \n  @Autowired\n  Mailer mailer;\n  \n  @Bean\n  Queue queue() {\n    return new Queue(\"CustomerQ\", false);\n  }\n  @RabbitListener(queues = \"CustomerQ\")\n    public void processMessage(String email) {\n       System.out.println(email);\n       mailer.sendMail(email);\n    }\n}\n25.\t Add another component to send an e-mail to the customer. We will use \nJavaMailSender to send an e-mail via the following code:\n@Component \nclass Mailer {\n  @Autowired\n  private  JavaMailSender  javaMailService;\n    public void sendMail(String email){\n      SimpleMailMessage mailMessage=new  \n        SimpleMailMessage();\n      mailMessage.setTo(email);\n      mailMessage.setSubject(\"Registration\");\n      mailMessage.setText(\"Successfully Registered\");\n      javaMailService.send(mailMessage);\n    }\n}\nBehind the scenes, Spring Boot automatically configures all the parameters \nrequired by JavaMailSender.\n26.\t To test SMTP, a test setup for SMTP is required to ensure that the mails \nare going out. In this example, FakeSMTP will be used. You can download \nFakeSMTP from http://nilhcem.github.io/FakeSMTP.\n\n\nBuilding Microservices with Spring Boot\n[ 96 ]\n27.\t Once you download fakeSMTP-2.0.jar, run the SMTP server by executing \nthe following command:\n$ java -jar fakeSMTP-2.0.jar\nThis will open a GUI to monitor e-mail messages. Click on the Start Server \nbutton next to the listening port textbox.\n28.\t Update application.properties with the following configuration \nparameters to connect to RabbitMQ as well as to the mail server:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\nspring.mail.host=localhost\nspring.mail.port=2525\n29.\t We are ready to test our microservices end to end. Start both the Spring Boot \napps. Open the browser and repeat the customer creation steps through the \nHAL browser. In this case, immediately after submitting the request, we will \nbe able to see the e-mail in the SMTP GUI.\nInternally, the Customer Profile service asynchronously calls the  \nCustomer Notification service, which, in turn, sends the e-mail message  \nto the SMTP server:\n\n\nChapter 2\n[ 97 ]\nSpring Boot actuators\nThe previous sections explored most of the Spring Boot features required to develop \na microservice. In this section, some of the production-ready operational aspects of \nSpring Boot will be explored.\nSpring Boot actuators provide an excellent out-of-the-box mechanism to monitor  \nand manage Spring Boot applications in production:\nThe full source code of this example is available as the  \nchapter2.bootactuator project in the code files of this book.\n1.\t Create another Spring Starter Project and name it chapter2.bootactuator. \nThis time, select Web and Actuators under Ops. Similar to the chapter2.\nbootrest project, add a GreeterController endpoint with the  \ngreet method.\n2.\t Start the application as Spring Boot app.\n3.\t Point the browser to localhost:8080/actuator. This will open the HAL \nbrowser. Then, review the Links section.\nA number of links are available under the Links section. These are \nautomatically exposed by the Spring Boot actuator:\n\n\nBuilding Microservices with Spring Boot\n[ 98 ]\nSome of the important links are listed as follows:\n•\t\ndump: This performs a thread dump and displays the result\n•\t\nmappings: This lists all the HTTP request mappings\n•\t\ninfo: This displays information about the application\n•\t\nhealth: This displays the application's health conditions\n•\t\nautoconfig: This displays the autoconfiguration report\n•\t\nmetrics: This shows different metrics collected from the application\nMonitoring using JConsole\nAlternately, we can use the JMX console to see the Spring Boot information. Connect \nto the remote Spring Boot instance from JConsole. The Boot information will be \nshown as follows:\n\n\nChapter 2\n[ 99 ]\nMonitoring using SSH\nSpring Boot provides remote access to the Boot application using SSH. The following \ncommand connects to the Spring Boot application from a terminal window:\n$ ssh -p 2000 user@localhost\nThe password can be customized by adding the shell.auth.simple.user.\npassword property in the application.properties file. The updated \napplication.properties file will look similar to the following:\nshell.auth.simple.user.password=admin\nWhen connected with the preceding command, similar actuator information can be \naccessed. Here is an example of the metrics information accessed through the CLI:\n•\t\nhelp: This lists out all the options available\n•\t\ndashboard: This is one interesting feature that shows a lot of system-level \ninformation\nConfiguring application information\nThe following properties can be set in application.properties to customize \napplication-related information. After adding, restart the server and visit the /info \nendpoint of the actuator to take a look at the updated information, as follows:\ninfo.app.name=Boot actuator\ninfo.app.description= My Greetings Service\ninfo.app.version=1.0.0\nAdding a custom health module\nAdding a new custom module to the Spring Boot application is not so complex.  \nTo demonstrate this feature, assume that if a service gets more than two transactions \nin a minute, then the server status will be set as Out of Service.\nIn order to customize this, we have to implement the HealthIndicator interface \nand override the health method. The following is a quick and dirty implementation \nto do the job:\nclass TPSCounter {\n  LongAdder count;\n  int threshold = 2;\n\n\nBuilding Microservices with Spring Boot\n[ 100 ]\n  Calendar expiry = null; \n  TPSCounter(){\n    this.count = new LongAdder();\n    this.expiry = Calendar.getInstance();\n    this.expiry.add(Calendar.MINUTE, 1);\n  }\n  \n  boolean isExpired(){\n    return Calendar.getInstance().after(expiry);\n  }\n  \n  boolean isWeak(){\n    return (count.intValue() > threshold);\n  }\n  \n  void increment(){\n     count.increment();\n  }\n}\nThe preceding class is a simple POJO class that maintains the transaction counts in the \nwindow. The isWeak method checks whether the transaction in a particular window \nreached its threshold. The isExpired method checks whether the current window is \nexpired or not. The increment method simply increases the counter value.\nFor the next step, implement our custom health indicator class, TPSHealth. This is \ndone by extending HealthIndicator, as follows:\n@Component\nclass TPSHealth implements HealthIndicator {\n  TPSCounter counter;\n@Override\n    public Health health() {\n        boolean health = counter.isWeak(); // perform some specific \nhealth check\n        if (health) {\n            return Health.outOfService().withDetail(\"Too many \nrequests\", \"OutofService\").build();\n        }\n        return Health.up().build();\n    }\n    \n\n\nChapter 2\n[ 101 ]\n    void updateTx(){\n    if(counter == null || counter.isExpired()){\n      counter = new TPSCounter();\n      \n    }\n    counter.increment();\n    }\n}\nThe health method checks whether the counter is weak or not. A weak counter \nmeans the service is handling more transactions than it can handle. If it is weak,  \nit marks the instance as Out of Service.\nFinally, we will autowire TPSHealth into the GreetingController class and then \ncall health.updateTx() in the greet method, as follows:\n  Greet greet(){\n    logger.info(\"Serving Request....!!!\");\n    health.updateTx(); \n    return new Greet(\"Hello World!\");\n  }\nGo to the /health end point in the HAL browser and take a look at the status  \nof the server.\nNow, open another browser, point to http://localhost:8080, and fire the  \nservice twice or thrice. Go back to the /health endpoint and refresh to see the  \nstatus. It should be changed to Out of Service.\nIn this example, as there is no action taken other than collecting the health status, \neven though the status is Out of Service, new service calls will still go through. \nHowever, in the real world, a program should read the /health endpoint and block \nfurther requests from going to this instance.\nBuilding custom metrics\nSimilar to health, customization of the metrics is also possible. The following example \nshows how to add counter service and gauge service, just for demonstration purposes:\n  @Autowired   \n  CounterService counterService;\n  @Autowired\n  GaugeService gaugeService;\n\n\nBuilding Microservices with Spring Boot\n[ 102 ]\nAdd the following methods in the greet method:\n  this.counterService.increment(\"greet.txnCount\");\n  this.gaugeService.submit(\"greet.customgauge\", 1.0);\nRestart the server and go to /metrics to see the new gauge and counter added \nalready reflected there.\nDocumenting microservices\nThe traditional approach of API documentation is either by writing service \nspecification documents or using static service registries. With a large number of \nmicroservices, it would be hard to keep the documentation of APIs in sync.\nMicroservices can be documented in many ways. This section will explore how \nmicroservices can be documented using the popular Swagger framework. The \nfollowing example will use Springfox libraries to generate REST API documentation. \nSpringfox is a set of Java- and Spring-friendly libraries.\nCreate a new Spring Starter Project and select Web in the library selection window. \nName the project chapter2.swagger.\nThe full source code of this example is available as the  \nchapter2.swagger project in the code files of this book.\nAs Springfox libraries are not part of the Spring suite, edit pom.xml and add \nSpringfox Swagger library dependencies. Add the following dependencies to  \nthe project:\n<dependency>\n    <groupId>io.springfox</groupId>\n    <artifactId>springfox-swagger2</artifactId>\n    <version>2.3.1</version>\n</dependency>  \n<dependency>\n    <groupId>io.springfox</groupId>\n    <artifactId>springfox-swagger-ui</artifactId>\n    <version>2.3.1</version>\n</dependency>\n\n\nChapter 2\n[ 103 ]\nCreate a REST service similar to the services created earlier, but also add the  \n@EnableSwagger2 annotation, as follows:\n@SpringBootApplication\n@EnableSwagger2\npublic class Application {\nThis is all that's required for a basic Swagger documentation. Start the application \nand point the browser to http://localhost:8080/swagger-ui.html. This will \nopen the Swagger API documentation page:\nAs shown in the diagram, the Swagger lists out the possible operations on Greet \nController. Click on the GET operation. This expands the GET row, which provides \nan option to try out the operation.\n\n\nBuilding Microservices with Spring Boot\n[ 104 ]\nSummary\nIn this chapter, you learned about Spring Boot and its key features to build \nproduction-ready applications.\nWe explored the previous-generation web applications and then how Spring Boot \nmakes developers' lives easier to develop fully qualified microservices. We also \ndiscussed the asynchronous message-based interaction between services. Further, \nwe explored how to achieve some of the key capabilities required for microservices, \nsuch as security, HATEOAS, cross-origin, configurations, and so on with practical \nexamples. We also took a look at how Spring Boot actuators help the operations \nteams and also how we can customize it to our needs. Finally, documenting \nmicroservices APIs was also explored.\nIn the next chapter, we will take a deeper look at some of the practical issues that \nmay arise when implementing microservices. We will also discuss a capability \nmodel that essentially helps organizations when dealing with large microservices \nimplementations.\n\n\n[ 105 ]\nApplying Microservices \nConcepts\nMicroservices are good, but can also be an evil if they are not properly conceived. \nWrong microservice interpretations could lead to irrecoverable failures.\nThis chapter will examine the technical challenges around practical implementations \nof microservices. It will also provide guidelines around critical design decisions for \nsuccessful microservices development. The solutions and patterns for a number of \ncommonly raised concerns around microservices will also be examined. This chapter \nwill also review the challenges in enterprise scale microservices development, and \nhow to overcome those challenges. More importantly, a capability model for a \nmicroservices ecosystem will be established at the end.\nIn this chapter you will learn about the following: \n•\t\nTrade-offs between different design choices and patterns to be considered \nwhen developing microservices\n•\t\nChallenges and anti-patterns in developing enterprise grade microservices\n•\t\nA capability model for a microservices ecosystem\nPatterns and common design decisions\nMicroservices have gained enormous popularity in recent years. They have evolved \nas the preferred choice of architects, putting SOA into the backyards. While \nacknowledging the fact that microservices are a vehicle for developing scalable cloud \nnative systems, successful microservices need to be carefully designed to avoid \ncatastrophes. Microservices are not the one-size-fits-all, universal solution for all \narchitecture problems.\n\n\nApplying Microservices Concepts\n[ 106 ]\nGenerally speaking, microservices are a great choice for building a lightweight, \nmodular, scalable, and distributed system of systems. Over-engineering, wrong \nuse cases, and misinterpretations could easily turn the system into a disaster. While \nselecting the right use cases is paramount in developing a successful microservice, it \nis equally important to take the right design decisions by carrying out an appropriate \ntrade-off analysis. A number of factors are to be considered when designing \nmicroservices, as detailed in the following sections.\nEstablishing appropriate microservice \nboundaries\nOne of the most common questions relating to microservices is regarding the size of \nthe service. How big (mini-monolithic) or how small (nano service) can a microservice \nbe, or is there anything like right-sized services? Does size really matter?\nA quick answer could be \"one REST endpoint per microservice\", or \"less than 300 \nlines of code\", or \"a component that performs a single responsibility\". But before we \npick up any of these answers, there is lot more analysis to be done to understand the \nboundaries for our services.\nDomain-driven design (DDD) defines the concept of a bounded context. A \nbounded context is a subdomain or a subsystem of a larger domain or system  \nthat is responsible for performing a particular function.\nRead more about DDD at http://domainlanguage.com/ddd/.\nThe following diagram is an example of the domain model:\nIn a finance back office, system invoices, accounting, billing, and the like represent \ndifferent bounded contexts. These bounded contexts are strongly isolated domains \nthat are closely aligned with business capabilities. In the financial domain, the \ninvoices, accounting, and billing are different business capabilities often handled  \nby different subunits under the finance department.\n",
      "page_number": 80
    },
    {
      "number": 3,
      "title": "[ 107 ]",
      "start_page": 134,
      "end_page": 179,
      "detection_method": "regex_chapter",
      "content": "Chapter 3\n[ 107 ]\nA bounded context is a good way to determine the boundaries of microservices. \nEach bounded context could be mapped to a single microservice. In the real world, \ncommunication between bounded contexts are typically less coupled, and often, \ndisconnected.\nEven though real world organizational boundaries are the simplest mechanisms \nfor establishing a bounded context, these may prove wrong in some cases due to \ninherent problems within the organization's structures. For example, a business \ncapability may be delivered through different channels such as front offices, online, \nroaming agents, and so on. In many organizations, the business units may be \norganized based on delivery channels rather than the actual underlying business \ncapabilities. In such cases, organization boundaries may not provide accurate service \nboundaries.\nA top-down domain decomposition could be another way to establish the right \nbounded contexts.\nThere is no silver bullet to establish microservices boundaries, and often, this \nis quite challenging. Establishing boundaries is much easier in the scenario of \nmonolithic application to microservices migration, as the service boundaries and \ndependencies are known from the existing system. On the other hand, in a green \nfield microservices development, the dependencies are hard to establish upfront.\nThe most pragmatic way to design microservices boundaries is to run the scenario \nat hand through a number of possible options, just like a service litmus test. Keep in \nmind that there may be multiple conditions matching a given scenario that will lead \nto a trade-off analysis.\nThe following scenarios could help in defining the microservice boundaries.\nAutonomous functions\nIf the function under review is autonomous by nature, then it can be taken as \na microservices boundary. Autonomous services typically would have fewer \ndependencies on external functions. They accept input, use its internal logic and \ndata for computation, and return a result. All utility functions such as an encryption \nengine or a notification engine are straightforward candidates.\nA delivery service that accepts an order, processes it, and then informs the trucking \nservice is another example of an autonomous service. An online flight search based \non cached seat availability information is yet another example of an autonomous \nfunction.\n\n\nApplying Microservices Concepts\n[ 108 ]\nSize of a deployable unit\nMost of the microservices ecosystems will take advantage of automation, such as \nautomatic integration, delivery, deployment, and scaling. Microservices covering \nbroader functions result in larger deployment units. Large deployment units pose \nchallenges in automatic file copy, file download, deployment, and start up times.  \nFor instance, the size of a service increases with the density of the functions that  \nit implements.\nA good microservice ensures that the size of its deployable units remains manageable.\nMost appropriate function or subdomain\nIt is important to analyze what would be the most useful component to detach from \nthe monolithic application. This is particularly applicable when breaking monolithic \napplications into microservices. This could be based on parameters such as resource-\nintensiveness, cost of ownership, business benefits, or flexibility.\nIn a typical hotel booking system, approximately 50-60% of the requests are search-\nbased. In this case, moving out the search function could immediately bring in \nflexibility, business benefits, cost reduction, resource free up, and so on.\nPolyglot architecture\nOne of the key characteristics of microservices is its support for polyglot architecture. \nIn order to meet different non-functional and functional requirements, components \nmay require different treatments. It could be different architectures, different \ntechnologies, different deployment topologies, and so on. When components are \nidentified, review them against the requirement for polyglot architectures.\nIn the hotel booking scenario mentioned earlier, a Booking microservice may need \ntransactional integrity, whereas a Search microservice may not. In this case, the \nBooking microservice may use an ACID compliance database such as MySQL, \nwhereas the Search microservice may use an eventual consistent database such  \nas Cassandra.\nSelective scaling\nSelective scaling is related to the previously discussed polyglot architecture. In \nthis context, all functional modules may not require the same level of scalability. \nSometimes, it may be appropriate to determine boundaries based on scalability \nrequirements.\n\n\nChapter 3\n[ 109 ]\nFor example, in the hotel booking scenario, the Search microservice has to scale \nconsiderably more than many of the other services such as the Booking microservice \nor the Notification microservice due to the higher velocity of search requests. In this \ncase, a separate Search microservice could run on top of an Elasticsearch or an  \nin-memory data grid for better response.\nSmall, agile teams\nMicroservices enable Agile development with small, focused teams developing \ndifferent parts of the pie. There could be scenarios where parts of the systems  \nare built by different organizations, or even across different geographies, or by  \nteams with varying skill sets. This approach is a common practice, for example,  \nin manufacturing industries.\nIn the microservices world, each of these teams builds different microservices, and \nthen assembles them together. Though this is not the desired way to break down the \nsystem, organizations may end up in such situations. Hence, this approach cannot  \nbe completely ruled out.\nIn an online product search scenario, a service could provide personalized options \nbased on what the customer is looking for. This may require complex machine \nlearning algorithms, and hence need a specialist team. In this scenario, this function \ncould be built as a microservice by a separate specialist team.\nSingle responsibility\nIn theory, the single responsibility principle could be applied at a method, at a class, \nor at a service. However, in the context of microservices, it does not necessarily map \nto a single service or endpoint.\nA more practical approach could be to translate single responsibility into single \nbusiness capability or a single technical capability. As per the single responsibility \nprinciple, one responsibility cannot be shared by multiple microservices. Similarly, \none microservice should not perform multiple responsibilities.\nThere could, however, be special cases where a single business capability is divided \nacross multiple services. One of such cases is managing the customer profile, \nwhere there could be situations where you may use two different microservices for \nmanaging reads and writes using a Command Query Responsibility Segregation \n(CQRS) approach to achieve some of the quality attributes.\n\n\nApplying Microservices Concepts\n[ 110 ]\nReplicability or changeability\nInnovation and speed are of the utmost importance in IT delivery. Microservices \nboundaries should be identified in such a way that each microservice is easily \ndetachable from the overall system, with minimal cost of re-writing. If part of  \nthe system is just an experiment, it should ideally be isolated as a microservice.\nAn organization may develop a recommendation engine or a customer ranking \nengine as an experiment. If the business value is not realized, then throw away  \nthat service, or replace it with another one.\nMany organizations follow the startup model, where importance is given to meeting \nfunctions and quick delivery. These organizations may not worry too much about the \narchitecture and technologies. Instead, the focus will be on what tools or technologies \ncan deliver solutions faster. Organizations increasingly choose the approach of \ndeveloping Minimum Viable Products (MVPs) by putting together a few services, \nand allowing the system to evolve. Microservices play a vital role in such cases where \nthe system evolves, and services gradually get rewritten or replaced.\nCoupling and cohesion\nCoupling and cohesion are two of the most important parameters for deciding \nservice boundaries. Dependencies between microservices have to be evaluated \ncarefully to avoid highly coupled interfaces. A functional decomposition, together \nwith a modeled dependency tree, could help in establishing a microservices \nboundary. Avoiding too chatty services, too many synchronous request-response \ncalls, and cyclic synchronous dependencies are three key points, as these could \neasily break the system. A successful equation is to keep high cohesion within a \nmicroservice, and loose coupling between microservices. In addition to this, ensure \nthat transaction boundaries are not stretched across microservices. A first class \nmicroservice will react upon receiving an event as an input, execute a number \nof internal functions, and finally send out another event. As part of the compute \nfunction, it may read and write data to its own local store.\nThink microservice as a product\nDDD also recommends mapping a bounded context to a product. As per DDD, each \nbounded context is an ideal candidate for a product. Think about a microservice \nas a product by itself. When microservice boundaries are established, assess them \nfrom a product's point of view to see whether they really stack up as product. It is \nmuch easier for business users to think boundaries from a product point of view. \nA product boundary may have many parameters, such as a targeted community, \nflexibility in deployment, sell-ability, reusability, and so on.\n\n\nChapter 3\n[ 111 ]\nDesigning communication styles\nCommunication between microservices can be designed either in synchronous \n(request-response) or asynchronous (fire and forget) styles.\nSynchronous style communication\nThe following diagram shows an example request/response style service:\nIn synchronous communication, there is no shared state or object. When a caller \nrequests a service, it passes the required information and waits for a response.  \nThis approach has a number of advantages.\nAn application is stateless, and from a high availability standpoint, many \nactive instances can be up and running, accepting traffic. Since there are no \nother infrastructure dependencies such as a shared messaging server, there are \nmanagement fewer overheads. In case of an error at any stage, the error will be \npropagated back to the caller immediately, leaving the system in a consistent state, \nwithout compromising data integrity.\nThe downside in a synchronous request-response communication is that the user \nor the caller has to wait until the requested process gets completed. As a result, the \ncalling thread will wait for a response, and hence, this style could limit the scalability \nof the system.\nA synchronous style adds hard dependencies between microservices. If one service \nin the service chain fails, then the entire service chain will fail. In order for a service \nto succeed, all dependent services have to be up and running. Many of the failure \nscenarios have to be handled using timeouts and loops.\n\n\nApplying Microservices Concepts\n[ 112 ]\nAsynchronous style communication\nThe following diagram is a service designed to accept an asynchronous message as \ninput, and send the response asynchronously for others to consume:\nThe asynchronous style is based on reactive event loop semantics which decouple \nmicroservices. This approach provides higher levels of scalability, because services \nare independent, and can internally spawn threads to handle an increase in \nload. When overloaded, messages will be queued in a messaging server for later \nprocessing. That means that if there is a slowdown in one of the services, it will not \nimpact the entire chain. This provides higher levels of decoupling between services, \nand therefore maintenance and testing will be simpler.\nThe downside is that it has a dependency to an external messaging server. It is \ncomplex to handle the fault tolerance of a messaging server. Messaging typically \nworks with an active/passive semantics. Hence, handling continuous availability of \nmessaging systems is harder to achieve. Since messaging typically uses persistence,  \na higher level of I/O handling and tuning is required.\nHow to decide which style to choose?\nBoth approaches have their own merits and constraints. It is not possible to develop \na system with just one approach. A combination of both approaches is required \nbased on the use cases. In principle, the asynchronous approach is great for building \ntrue, scalable microservice systems. However, attempting to model everything as \nasynchronous leads to complex system designs.\nHow does the following example look in the context where an end user clicks on a  \nUI to get profile details?\n\n\nChapter 3\n[ 113 ]\nThis is perhaps a simple query to the backend system to get a result in a request-\nresponse model. This can also be modeled in an asynchronous style by pushing \na message to an input queue, and waiting for a response in an output queue till \na response is received for the given correlation ID. However, though we use \nasynchronous messaging, the user is still blocked for the entire duration of the query.\nAnother use case is that of a user clicking on a UI to search hotels, which is depicted \nin the following diagram:\nThis is very similar to the previous scenario. However, in this case, we assume that \nthis business function triggers a number of activities internally before returning \nthe list of hotels back to the user. For example, when the system receives this \nrequest, it calculates the customer ranking, gets offers based on the destination, gets \nrecommendations based on customer preferences, optimizes the prices based on \ncustomer values and revenue factors, and so on. In this case, we have an opportunity \nto do many of these activities in parallel so that we can aggregate all these results \nbefore presenting them to the customer. As shown in the preceding diagram, \nvirtually any computational logic could be plugged in to the search pipeline  \nlistening to the IN queue. \nAn effective approach in this case is to start with a synchronous request response,  \nand refactor later to introduce an asynchronous style when there is value in doing that.\n\n\nApplying Microservices Concepts\n[ 114 ]\nThe following example shows a fully asynchronous style of service interactions:\nThe service is triggered when the user clicks on the booking function. It is again, by \nnature, a synchronous style communication. When booking is successful, it sends \na message to the customer's e-mail address, sends a message to the hotel's booking \nsystem, updates the cached inventory, updates the loyalty points system, prepares an \ninvoice, and perhaps more. Instead of pushing the user into a long wait state, a better \napproach is to break the service into pieces. Let the user wait till a booking record  \nis created by the Booking Service. On successful completion, a booking event will  \nbe published, and return a confirmation message back to the user. Subsequently,  \nall other activities will happen in parallel, asynchronously.\nIn all three examples, the user has to wait for a response. With the new web \napplication frameworks, it is possible to send requests asynchronously, and define \nthe callback method, or set an observer for getting a response. Therefore, the users \nwon't be fully blocked from executing other activities.\nIn general, an asynchronous style is always better in the microservices world, but \nidentifying the right pattern should be purely based on merits. If there are no merits \nin modeling a transaction in an asynchronous style, then use the synchronous style \ntill you find an appealing case. Use reactive programming frameworks to avoid \ncomplexity when modeling user-driven requests, modeled in an asynchronous style.\n\n\nChapter 3\n[ 115 ]\nOrchestration of microservices\nComposability is one of the service design principles. This leads to confusion \naround who is responsible for the composing services. In the SOA world, ESBs are \nresponsible for composing a set of finely-grained services. In some organizations, \nESBs play the role of a proxy, and service providers themselves compose and expose \ncoarse-grained services. In the SOA world, there are two approaches for handling \nsuch situations.\nThe first approach is orchestration, which is depicted in the following diagram:\nIn the orchestration approach, multiple services are stitched together to get a \ncomplete function. A central brain acts as the orchestrator. As shown in the diagram, \nthe order service is a composite service that will orchestrate other services. There \ncould be sequential as well as parallel branches from the master process. Each task \nwill be fulfilled by an atomic task service, typically a web service. In the SOA world, \nESBs play the role of orchestration. The orchestrated service will be exposed by ESBs \nas a composite service.\nThe second approach is choreography, which is shown in the following diagram:\n\n\nApplying Microservices Concepts\n[ 116 ]\nIn the choreography approach, there is no central brain. An event, a booking event in \nthis case, is published by a producer, a number of consumers wait for the event, and \nindependently apply different logics on the incoming event. Sometimes, events could \neven be nested where the consumers can send another event which will be consumed \nby another service. In the SOA world, the caller pushes a message to the ESB, and the \ndownstream flow will be automatically determined by the consuming services.\nMicroservices are autonomous. This essentially means that in an ideal situation, \nall required components to complete their function should be within the service. \nThis includes the database, orchestration of its internal services, intrinsic state \nmanagement, and so on. The service endpoints provide coarse-grained APIs. This \nis perfectly fine as long as there are no external touch points required. But in reality, \nmicroservices may need to talk to other microservices to fulfil their function.\nIn such cases, choreography is the preferred approach for connecting multiple \nmicroservices together. Following the autonomy principle, a component sitting \noutside a microservice and controlling the flow is not the desired option. If the use \ncase can be modeled in choreographic style, that would be the best possible way to \nhandle the situation.\nBut it may not be possible to model choreography in all cases. This is depicted in the \nfollowing diagram:\nIn the preceding example, Reservation and Customer are two microservices, with \nclearly segregated functional responsibilities. A case could arise when Reservation \nwould want to get Customer preferences while creating a reservation. These are \nquite normal scenarios when developing complex systems.\nCan we move Customer to Reservation so that Reservation will be complete by \nitself? If Customer and Reservation are identified as two microservices based on \nvarious factors, it may not be a good idea to move Customer to Reservation.  \nIn such a case, we will meet another monolithic application sooner or later.\n\n\nChapter 3\n[ 117 ]\nCan we make the Reservation to Customer call asynchronous? This example is \nshown in the following diagram:\nCustomer preference is required for Reservation to progress, and hence, it may \nrequire a synchronous blocking call to Customer. Retrofitting this by modeling \nasynchronously does not really make sense.\nCan we take out just the orchestration bit, and create another composite \nmicroservice, which then composes Reservation and Customer?\nThis is acceptable in the approach for composing multiple components within a \nmicroservice. But creating a composite microservice may not be a good idea. We will \nend up creating many microservices with no business alignment, which would not \nbe autonomous, and could result in many fine-grained microservices.\n\n\nApplying Microservices Concepts\n[ 118 ]\nCan we duplicate customer preference by keeping a slave copy of the preference data \ninto Reservation?\nChanges will be propagated whenever there is a change in the master. In this \ncase, Reservation can use customer preference without fanning out a call. It is a \nvalid thought, but we need to carefully analyze this. Today we replicate customer \npreference, but in another scenario, we may want to reach out to customer service \nto see whether the customer is black-listed from reserving. We have to be extremely \ncareful in deciding what data to duplicate. This could add to the complexity.\nHow many endpoints in a microservice?\nIn many situations, developers are confused with the number of endpoints per \nmicroservice. The question really is whether to limit each microservice with one \nendpoint or multiple endpoints:\n\n\nChapter 3\n[ 119 ]\nThe number of endpoints is not really a decision point. In some cases, there may \nbe only one endpoint, whereas in some other cases, there could be more than one \nendpoint in a microservice. For instance, consider a sensor data service which \ncollects sensor information, and has two logical endpoints: create and read. But in \norder to handle CQRS, we may create two separate physical microservices as shown \nin the case of Booking in the preceding diagram. Polyglot architecture could be \nanother scenario where we may split endpoints into different microservices.\nConsidering a notification engine, notifications will be send out in response to an \nevent. The process of notification such as preparation of data, identification of a \nperson, and delivery mechanisms, are different for different events. Moreover, we \nmay want to scale each of these processes differently at different time windows. In \nsuch situations, we may decide to break each notification endpoint in to a separate \nmicroservice.\nIn yet another example, a Loyalty Points microservice may have multiple services \nsuch as accrue, redeem, transfer, and balance. We may not want to treat each of these \nservices differently. All of these services are connected and use the points table for \ndata. If we go with one endpoint per service, we will end up in a situation where \nmany fine-grained services access data from the same data store or replicated  \ncopies of the same data store.\nIn short, the number of endpoints is not a design decision. One microservice  \nmay host one or more endpoints. Designing appropriate bounded context for  \na microservice is more important.\nOne microservice per VM or multiple?\nOne microservice could be deployed in multiple Virtual Machines (VMs) by \nreplicating the deployment for scalability and availability. This is a no brainer. \nThe question is whether multiple microservices could be deployed in one virtual \nmachine? There are pros and cons for this approach. This question typically arises \nwhen the services are simple, and the traffic volume is less.\nConsider an example where we have a couple of microservices, and the overall \ntransaction per minute is less than 10. Also assume that the smallest possible VM size \navailable is 2-core 8 GB RAM. A further assumption is that in such cases, a 2-core 8 \nGB VM can handle 10-15 transactions per minute without any performance concerns. \nIf we use different VMs for each microservice, it may not be cost effective, and we \nwill end up paying more for infrastructure and license, since many vendors charge \nbased on the number of cores.\n\n\nApplying Microservices Concepts\n[ 120 ]\nThe simplest way to approach this problem is to ask a few questions:\n•\t\nDoes the VM have enough capacity to run both services under peak usage?\n•\t\nDo we want to treat these services differently to achieve SLAs (selective \nscaling)? For example, for scalability, if we have an all-in-one VM, we will \nhave to replicate VMs which replicate all services.\n•\t\nAre there any conflicting resource requirements? For example, different OS \nversions, JDK versions, and others.\nIf all your answers are No, then perhaps we can start with collocated deployment, \nuntil we encounter a scenario to change the deployment topology. However, we  \nwill have to ensure that these services are not sharing anything, and are running  \nas independent OS processes.\nHaving said that, in an organization with matured virtualized infrastructure or cloud \ninfrastructure, this may not be a huge concern. In such environments, the developers \nneed not worry about where the services are running. Developers may not even think \nabout capacity planning. Services will be deployed in a compute cloud. Based on the \ninfrastructure availability, SLAs and the nature of the service, the infrastructure self-\nmanages deployments. AWS Lambda is a good example of such a service.\nRules engine – shared or embedded?\nRules are an essential part of any system. For example, an offer eligibility service \nmay execute a number of rules before making a yes or no decision. Either we hand \ncode rules, or we may use a rules engine. Many enterprises manage rules centrally \nin a rules repository as well as execute them centrally. These enterprise rule engines \nare primarily used for providing the business an opportunity to author and manage \nrules as well as reuse rules from the central repository. Drools is one of the popular \nopen source rules engines. IBM, FICO, and Bosch are some of the pioneers in the \ncommercial space. These rule engines improve productivity, enable reuse of rules, \nfacts, vocabularies, and provide faster rule execution using the rete algorithm.\nIn the context of microservices, a central rules engine means fanning out calls from \nmicroservices to the central rules engine. This also means that the service logic is now \nin two places, some within the service, and some external to the service. Nevertheless, \nthe objective in the context of microservices is to reduce external dependencies:\n\n\nChapter 3\n[ 121 ]\nIf the rules are simple enough, few in numbers, only used within the boundaries of \na service, and not exposed to business users for authoring, then it may be better to \nhand-code business rules than rely on an enterprise rule engine:\nIf the rules are complex, limited to a service context, and not given to business users, \nthen it is better to use an embedded rules engine within the service:\nIf the rules are managed and authored by business, or if the rules are complex, or if \nwe are reusing rules from other service domains, then a central authoring repository \nwith a locally embedded execution engine could be a better choice.\nNote that this has to be carefully evaluated since all vendors may not support the \nlocal rule execution approach, and there could be technology dependencies such as \nrunning rules only within a specific application server, and so on.\nRole of BPM and workflows\nBusiness Process Management (BPM) and Intelligent Business Process \nManagement (iBPM) are tool suites for designing, executing, and monitoring \nbusiness processes.\nTypical use cases for BPM are:\n•\t\nCoordinating a long-running business process, where some processes are \nrealized by existing assets, whereas some other areas may be niche, and there \nis no concrete implementation of the processes being in place. BPM allows \ncomposing both types, and provides an end-to-end automated process. This \noften involves systems and human interactions.\n\n\nApplying Microservices Concepts\n[ 122 ]\n•\t\nProcess-centric organizations, such as those that have implemented Six \nSigma, want to monitor their processes for continuous improvement  \non efficiency.\n•\t\nProcess re-engineering with a top-down approach by redefining the business \nprocess of an organization.\nThere could be two scenarios where BPM fits in the microservices world:\nThe first scenario is business process re-engineering, or threading an end-to-end long \nrunning business process, as stated earlier. In this case, BPM operates at a higher level, \nwhere it may automate a cross-functional, long-running business process by stitching \na number of coarse-grained microservices, existing legacy connectors, and human \ninteractions. As shown in the preceding diagram, the loan approval BPM invokes \nmicroservices as well as legacy application services. It also integrates human tasks.\nIn this case, microservices are headless services that implement a subprocess. From \nthe microservices' perspective, BPM is just another consumer. Care needs to be taken \nin this approach to avoid accepting a shared state from a BPM process as well as \nmoving business logic to BPM:\n\n\nChapter 3\n[ 123 ]\nThe second scenario is monitoring processes, and optimizing them for efficiency. \nThis goes hand in hand with a completely automated, asynchronously \nchoreographed microservices ecosystem. In this case, microservices and BPM work \nas independent ecosystems. Microservices send events at various timeframes such \nas the start of a process, state changes, end of a process, and so on. These events are \nused by the BPM engine to plot and monitor process states. We may not require \na full-fledged BPM solution for this, as we are only mocking a business process \nto monitor its efficiency. In this case, the order delivery process is not a BPM \nimplementation, but it is more of a monitoring dashboard that captures and displays \nthe progress of the process.\nTo summarize, BPM could still be used at a higher level for composing multiple \nmicroservices in situations where end-to-end cross-functional business processes \nare modeled by automating systems and human interactions. A better and simpler \napproach is to have a business process dashboard to which microservices feed state \nchange events as mentioned in the second scenario.\nCan microservices share data stores?\nIn principle, microservices should abstract presentation, business logic, and data \nstores. If the services are broken as per the guidelines, each microservice logically \ncould use an independent database:\nIn the preceding diagram, both Product and Order microservices share one \ndatabase and one data model. Shared data models, shared schema, and shared \ntables are recipes for disasters when developing microservices. This may be good \nat the beginning, but when developing complex microservices, we tend to add \nrelationships between data models, add join queries, and so on. This can result in \ntightly coupled physical data models.\n\n\nApplying Microservices Concepts\n[ 124 ]\nIf the services have only a few tables, it may not be worth investing a full instance of \na database like an Oracle database instance. In such cases, a schema level segregation \nis good enough to start with:\nThere could be scenarios where we tend to think of using a shared database for \nmultiple services. Taking an example of a customer data repository or master \ndata managed at the enterprise level, the customer registration and customer \nsegmentation microservices logically share the same customer data repository:\nAs shown in the preceding diagram, an alternate approach in this scenario is to \nseparate the transactional data store for microservices from the enterprise data \nrepository by adding a local transactional data store for these services. This will \nhelp the services to have flexibility in remodeling the local data store optimized for \nits purpose. The enterprise customer repository sends change events when there is \nany change in the customer data repository. Similarly, if there is any change in any \nof the transactional data stores, the changes have to be sent to the central customer \nrepository.\n\n\nChapter 3\n[ 125 ]\nSetting up transaction boundaries\nTransactions in operational systems are used to maintain the consistency of data \nstored in an RDBMS by grouping a number of operations together into one atomic \nblock. They either commit or rollback the entire operation. Distributed systems \nfollow the concept of distributed transactions with a two-phase commit. This is \nparticularly required if heterogeneous components such as an RPC service, JMS,  \nand so on participate in a transaction.\nIs there a place for transactions in microservices? Transactions are not bad, but one \nshould use transactions carefully, by analyzing what we are trying do.\nFor a given microservice, an RDBMS like MySQL may be selected as a backing store \nto ensure 100% data integrity, for example, a stock or inventory management service \nwhere data integrity is key. It is appropriate to define transaction boundaries within \nthe microsystem using local transactions. However, distributed global transactions \nshould be avoided in the microservices context. Proper dependency analysis is \nrequired to ensure that transaction boundaries do not span across two different \nmicroservices as much as possible.\nAltering use cases to simplify transactional \nrequirements\nEventual consistency is a better option than distributed transactions that span \nacross multiple microservices. Eventual consistency reduces a lot of overheads, \nbut application developers may need to re-think the way they write application \ncode. This could include remodeling functions, sequencing operations to minimize \nfailures, batching insert and modify operations, remodeling data structure, and \nfinally, compensating operations that negate the effect.\nA classical problem is that of the last room selling scenario in a hotel booking use \ncase. What if there is only one room left, and there are multiple customers booking \nthis singe available room? A business model change sometimes makes this scenario \nless impactful. We could set an \"under booking profile\", where the actual number of \nbookable rooms can go below the actual number of rooms (bookable = available - 3) in \nanticipation of some cancellations. Anything in this range will be accepted as \"subject \nto confirmation\", and customers will be charged only if payment is confirmed. \nBookings will be confirmed in a set time window.\n\n\nApplying Microservices Concepts\n[ 126 ]\nNow consider the scenario where we are creating customer profiles in a NoSQL \ndatabase like CouchDB. In more traditional approaches with RDBMS, we insert \na customer first, and then insert the customer's address, profile details, then \npreferences, all in one transaction. When using NoSQL, we may not do the same \nsteps. Instead, we may prepare a JSON object with all the details, and insert this  \ninto CouchDB in one go. In this second case, no explicit transaction boundaries  \nare required.\nDistributed transaction scenarios\nThe ideal scenario is to use local transactions within a microservice if required, \nand completely avoid distributed transactions. There could be scenarios where at \nthe end of the execution of one service, we may want to send a message to another \nmicroservice. For example, say a tour reservation has a wheelchair request. Once the \nreservation is successful, we will have to send a message for the wheelchair booking \nto another microservice that handles ancillary bookings. The reservation request \nitself will run on a local transaction. If sending this message fails, we are still in the \ntransaction boundary, and we can roll back the entire transaction. What if we create \na reservation and send the message, but after sending the message, we encounter \nan error in the reservation, the reservation transaction fails, and subsequently, \nthe reservation record is rolled back? Now we end up in a situation where we've \nunnecessarily created an orphan wheelchair booking:\nThere are a couple of ways we can address this scenario. The first approach is to \ndelay sending the message till the end. This ensures that there are less chances \nfor any failure after sending the message. Still, if failure occurs after sending the \nmessage, then the exception handling routine is run, that is, we send a compensating \nmessage to reverse the wheelchair booking.\n\n\nChapter 3\n[ 127 ]\nService endpoint design consideration\nOne of the important aspects of microservices is service design. Service design has \ntwo key elements: contract design and protocol selection.\nContract design\nThe first and foremost principle of service design is simplicity. The services should \nbe designed for consumers to consume. A complex service contract reduces the \nusability of the service. The KISS (Keep It Simple Stupid) principle helps us \nto build better quality services faster, and reduces the cost of maintenance and \nreplacement. The YAGNI (You Ain't Gonna Need It) is another principle supporting \nthis idea. Predicting future requirements and building systems are, in reality, not \nfuture-proofed. This results in large upfront investment as well as higher cost of \nmaintenance.\nEvolutionary design is a great concept. Do just enough design to satisfy today's \nwants, and keep changing and refactoring the design to accommodate new features \nas and when they are required. Having said that, this may not be simple unless there \nis a strong governance in place.\nConsumer Driven Contracts (CDC) is a great idea that supports evolutionary \ndesign. In many cases, when the service contract gets changed, all consuming \napplications have to undergo testing. This makes change difficult. CDC helps in \nbuilding confidence in consumer applications. CDC advocates each consumer to \nprovide their expectation to the provider in the form of test cases so that the provider \nuses them as integration tests whenever the service contract is changed.\nPostel's law is also relevant in this scenario. Postel's law primarily addresses TCP \ncommunications; however, this is also equally applicable to service design. When \nit comes to service design, service providers should be as flexible as possible when \naccepting consumer requests, whereas service consumers should stick to the contract \nas agreed with the provider.\nProtocol selection\nIn the SOA world, HTTP/SOAP, and messaging were kinds of default service \nprotocols for service interactions. Microservices follow the same design principles for \nservice interaction. Loose coupling is one of the core principles in the microservices \nworld too.\nMicroservices fragment applications into many physically independent deployable \nservices. This not only increases the communication cost, it is also susceptible to \nnetwork failures. This could also result in poor performance of services.\n\n\nApplying Microservices Concepts\n[ 128 ]\nMessage-oriented services\nIf we choose an asynchronous style of communication, the user is disconnected, \nand therefore, response times are not directly impacted. In such cases, we may \nuse standard JMS or AMQP protocols for communication with JSON as payload. \nMessaging over HTTP is also popular, as it reduces complexity. Many new entrants \nin messaging services support HTTP-based communication. Asynchronous REST is \nalso possible, and is handy when calling long-running services.\nHTTP and REST endpoints\nCommunication over HTTP is always better for interoperability, protocol handling, \ntraffic routing, load balancing, security systems, and the like. Since HTTP is stateless, \nit is more compatible for handling stateless services with no affinity. Most of the \ndevelopment frameworks, testing tools, runtime containers, security systems, and so \non are friendlier towards HTTP.\nWith the popularity and acceptance of REST and JSON, it is the default choice for \nmicroservice developers. The HTTP/REST/JSON protocol stack makes building \ninteroperable systems very easy and friendly. HATEOAS is one of the design \npatterns emerging for designing progressive rendering and self-service navigations. \nAs discussed in the previous chapter, HATEOAS provides a mechanism to  \nlink resources together so that the consumer can navigate between resources.  \nRFC 5988 – Web Linking is another upcoming standard.\nOptimized communication protocols\nIf the service response times are stringent, then we need to pay special attention to \nthe communication aspects. In such cases, we may choose alternate protocols such \nas Avro, Protocol Buffers, or Thrift for communicating between services. But this \nlimits the interoperability of services. The trade-off is between performance and \ninteroperability requirements. Custom binary protocols need careful evaluation as \nthey bind native objects on both sides—consumer and producer. This could run into \nrelease management issues such as class version mismatch in Java-based RPC style \ncommunications.\nAPI documentations\nLast thing: a good API is not only simple, but should also have enough \ndocumentation for the consumers. There are many tools available today for \ndocumenting REST-based services like Swagger, RAML, and API Blueprint.\n\n\nChapter 3\n[ 129 ]\nHandling shared libraries\nThe principle behind microservices is that they should be autonomous and  \nself-contained. In order to adhere to this principle, there may be situations where  \nwe will have to duplicate code and libraries. These could be either technical libraries \nor functional components.\nFor example, the eligibility for a flight upgrade will be checked at the time of \ncheck-in as well as when boarding. If check-in and boarding are two different \nmicroservices, we may have to duplicate the eligibility rules in both the services.  \nThis was the trade-off between adding a dependency versus code duplication.\nIt may be easy to embed code as compared to adding an additional dependency,  \nas it enables better release management and performance. But this is against the  \nDRY principle.\nDRY principle\nEvery piece of knowledge must have a single, unambiguous, \nauthoritative representation within a system.\nThe downside of this approach is that in case of a bug or an enhancement on the \nshared library, it has to be upgraded in more than one place. This may not be a \nsevere setback as each service can contain a different version of the shared library:\nAn alternative option of developing the shared library as another microservice \nitself needs careful analysis. If it is not qualified as a microservice from the business \ncapability point of view, then it may add more complexity than its usefulness. The \ntrade-off analysis is between overheads in communication versus duplicating the \nlibraries in multiple services.\n\n\nApplying Microservices Concepts\n[ 130 ]\nUser interfaces in microservices\nThe microservices principle advocates a microservice as a vertical slice from the \ndatabase to presentation:\nIn reality, we get requirements to build quick UI and mobile applications mashing \nup the existing APIs. This is not uncommon in the modern scenario, where a \nbusiness wants quick turnaround time from IT:\nPenetration of mobile applications is one of the causes of this approach. In many \norganizations, there will be mobile development teams sitting close to the business \nteam, developing rapid mobile applications by combining and mashing up APIs \nfrom multiple sources, both internal and external. In such situations, we may just \nexpose services, and leave it for the mobile teams to realize in the way the business \nwants. In this case, we will build headless microservices, and leave it to the mobile \nteams to build a presentation layer.\n\n\nChapter 3\n[ 131 ]\nAnother category of problem is that the business may want to build consolidated \nweb applications targeted to communities:\nFor example, the business may want to develop a departure control application \ntargeting airport users. A departure control web application may have functions \nsuch as check-in, lounge management, boarding, and so on. These may be designed \nas independent microservices. But from the business standpoint, it all needs to be \nclubbed into a single web application. In such cases, we will have to build web \napplications by mashing up services from the backend.\nOne approach is to build a container web application or a placeholder web \napplication, which links to multiple microservices at the backend. In this case, \nwe develop full stack microservices, but the screens coming out of this could be \nembedded in to another placeholder web application. One of the advantages of \nthis approach is that you can have multiple placeholder web applications targeting \ndifferent user communities, as shown in the preceding diagram. We may use an API \ngateway to avoid those crisscross connections. We will explore the API gateway in \nthe next section.\nUse of API gateways in microservices\nWith the advancement of client-side JavaScript frameworks like AngularJS, the \nserver is expected to expose RESTful services. This could lead to two issues. The first \nissue is the mismatch in contract expectations. The second issue is multiple calls to \nthe server to render a page.\n\n\nApplying Microservices Concepts\n[ 132 ]\nWe start with the contract mismatch case. For example, GetCustomer may return  \na JSON with many fields:\nCustomer {\n  Name: \n  Address: \n  Contact: \n}\nIn the preceding case, Name, Address, and Contact are nested JSON objects. But a \nmobile client may expect only basic information such as first name, and last name. \nIn the SOA world, an ESB or a mobile middleware did this job of transformation of \ndata for the client. The default approach in microservices is to get all the elements of \nCustomer, and then the client takes up the responsibility to filter the elements. In this \ncase, the overhead is on the network.\nThere are several approaches we can think about to solve this case:\nCustomer {\n  Id: 1\n  Name: /customer/name/1\n  Address: /customer/address/1\n  Contact: /customer/contact/1\n}\nIn the first approach, minimal information is sent with links as explained in the \nsection on HATEOAS. In the preceding case, for customer ID 1, there are three links, \nwhich will help the client to access specific data elements. The example is a simple \nlogical representation, not the actual JSON. The mobile client in this case will get \nbasic customer information. The client further uses the links to get the additional \nrequired information.\nThe second approach is used when the client makes the REST call; it also sends the \nrequired fields as part of the query string. In this scenario, the client sends a request \nwith firstname and lastname as the query string to indicate that the client only \nrequires these two fields. The downside is that it ends up in complex server-side \nlogic as it has to filter based on the fields. The server has to send different elements \nbased on the incoming query.\nThe third approach is to introduce a level of indirection. In this, a gateway component \nsits between the client and the server, and transforms data as per the consumer's \nspecification. This is a better approach as we do not compromise on the backend \nservice contract. This leads to what is called UI services. In many cases, the API \ngateway acts as a proxy to the backend, exposing a set of consumer-specific APIs:\n\n\nChapter 3\n[ 133 ]\nThere are two ways we can deploy an API gateway. The first one is one API gateway \nper microservice as shown in diagram A. The second approach (diagram B) is to \nhave a common API gateway for multiple services. The choice really depends on \nwhat we are looking for. If we are using an API gateway as a reverse proxy, then \noff-the-shelf gateways such as Apigee, Mashery, and the like could be used as a \nshared platform. If we need fine-grained control over traffic shaping and complex \ntransformations, then per service custom API gateways may be more useful.\nA related problem is that we will have to make many calls from the client to the server. \nIf we refer to our holiday example in Chapter 1, Demystifying Microservices, you know \nthat for rendering each widget, we had to make a call to the server. Though we transfer \nonly data, it can still add a significant overhead on the network. This approach is not \nfully wrong, as in many cases, we use responsive design and progressive design. The \ndata will be loaded on demand, based on user navigations. In order to do this, each \nwidget in the client should make independent calls to the server in a lazy mode. If \nbandwidth is an issue, then an API gateway is the solution. An API gateway acts as a \nmiddleman to compose and transform APIs from multiple microservices.\nUse of ESB and iPaaS with microservices\nTheoretically, SOA is not all about ESBs, but the reality is that ESBs have always been \nat the center of many SOA implementations. What would be the role of an ESB in  \nthe microservices world?\nIn general, microservices are fully cloud native systems with smaller footprints. \nThe lightweight characteristics of microservices enable automation of deployments, \nscaling, and so on. On the contrary, enterprise ESBs are heavyweight in nature, and \nmost of the commercial ESBs are not cloud friendly. The key features of an ESB are \nprotocol mediation, transformation, orchestration, and application adaptors. In a \ntypical microservices ecosystem, we may not need any of these features.\n\n\nApplying Microservices Concepts\n[ 134 ]\nThe limited ESB capabilities that are relevant for microservices are already available \nwith more lightweight tools such as an API gateway. Orchestration is moved from \nthe central bus to the microservices themselves. Therefore, there is no centralized \norchestration capability expected in the case of microservices. Since the services are \nset up to accept more universal message exchange styles using REST/JSON calls, \nno protocol mediation is required. The last piece of capability that we get from ESBs \nare the adaptors to connect back to the legacy systems. In the case of microservices, \nthe service itself provides a concrete implementation, and hence, there are no legacy \nconnectors required. For these reasons, there is no natural space for ESBs in the \nmicroservices world.\nMany organizations established ESBs as the backbone for their application \nintegrations (EAI). Enterprise architecture policies in such organizations are built \naround ESBs. There could be a number of enterprise-level policies such as auditing, \nlogging, security, validation, and so on that would have been in place when \nintegrating using ESB. Microservices, however, advocate a more decentralized \ngovernance. ESBs will be an overkill if integrated with microservices.\nNot all services are microservices. Enterprises have legacy applications, vendor \napplications, and so on. Legacy services use ESBs to connect with microservices. \nESBs still hold their place for legacy integration and vendor applications to integrate \nat the enterprise level.\nWith the advancement of clouds, the capabilities of ESBs are not sufficient to manage \nintegration between clouds, cloud to on-premise, and so on. Integration Platform as \na Service (iPaaS) is evolving as the next generation application integration platform, \nwhich further reduces the role of ESBs. In typical deployments, iPaaS invokes API \ngateways to access microservices.\nService versioning considerations\nWhen we allow services to evolve, one of the important aspect to consider is \nservice versioning. Service versioning should be considered upfront, and not as \nan afterthought. Versioning helps us to release new services without breaking the \nexisting consumers. Both the old version and the new version will be deployed  \nside by side.\nSemantic versions are widely used for service versioning. A semantic version has \nthree components: major, minor, and patch. Major is used when there is a breaking \nchange, minor is used when there is a backward compatible change, and patch is \nused when there is a backward compatible bug fix.\n\n\nChapter 3\n[ 135 ]\nVersioning could get complicated when there is more than one service in a \nmicroservice. It is always simple to version services at the service level compared \nto the operations level. If there is a change in one of the operations, the service is \nupgraded and deployed to V2. The version change is applicable to all operations  \nin the service. This is the notion of immutable services.\nThere are three different ways in which we can version REST services:\n•\t\nURI versioning\n•\t\nMedia type versioning\n•\t\nCustom header\nIn URI versioning, the version number is included in the URL itself. In this case, \nwe just need to be worried about the major versions only. Hence, if there is a minor \nversion change or a patch, the consumers do not need to worry about the changes.  \nIt is a good practice to alias the latest version to a non-versioned URI, which is done \nas follows:\n/api/v3/customer/1234\n/api/customer/1234  - aliased to v3.\n@RestController(\"CustomerControllerV3\")\n@RequestMapping(\"api/v3/customer\")\npublic class CustomerController {\n}\nA slightly different approach is to use the version number as part of the URL \nparameter:\napi/customer/100?v=1.5\nIn case of media type versioning, the version is set by the client on the HTTP Accept \nheader as follows:\nAccept:  application/vnd.company.customer-v3+json\nA less effective approach for versioning is to set the version in the custom header:\n@RequestMapping(value = \"/{id}\", method = RequestMethod.GET, headers = \n{\"version=3\"})\npublic Customer getCustomer(@PathVariable(\"id\") long id) {\n     //other code goes here.\n}\n\n\nApplying Microservices Concepts\n[ 136 ]\nIn the URI approach, it is simple for the clients to consume services. But this has \nsome inherent issues such as the fact that versioning-nested URI resources could \nbe complex. Indeed, migrating clients is slightly complex as compared to media \ntype approaches, with caching issues for multiple versions of the services, and \nothers. However, these issues are not significant enough for us to not go with a URI \napproach. Most of the big Internet players such as Google, Twitter, LinkedIn, and \nSalesforce are following the URI approach.\nDesign for cross origin\nWith microservices, there is no guarantee that the services will run from the \nsame host or same domain. Composite UI web applications may call multiple \nmicroservices for accomplishing a task, and these could come from different  \ndomains and hosts.\nCORS allows browser clients to send requests to services hosted on different \ndomains. This is essential in a microservices-based architecture.\nOne approach is to enable all microservices to allow cross origin requests from other \ntrusted domains. The second approach is to use an API gateway as a single trusted \ndomain for the clients.\nHandling shared reference data\nWhen breaking large applications, one of the common issues which we see is the \nmanagement of master data or reference data. Reference data is more like shared \ndata required between different microservices. City master, country master, and so \non will be used in many services such as flight schedules, reservations, and others.\nThere are a few ways in which we can solve this. For instance, in the case of \nrelatively static, never changing data, then every service can hardcode this data \nwithin all the microservices themselves:\n\n\nChapter 3\n[ 137 ]\nAnother approach, as shown in the preceding diagram, is to build it as another \nmicroservice. This is good, clean, and neat, but the downside is that every service \nmay need to call the master data multiple times. As shown in the diagram for the \nSearch and Booking example, there are transactional microservices, which use the \nGeography microservice to access shared data:\nAnother option is to replicate the data with every microservice. There is no single \nowner, but each service has its required master data. When there is an update, all \nthe services are updated. This is extremely performance friendly, but one has to \nduplicate the code in all the services. It is also complex to keep data in sync across all \nmicroservices. This approach makes sense if the code base and data is simple or the \ndata is more static.\nYet another approach is similar to the first approach, but each service has a local near \ncache of the required data, which will be loaded incrementally. A local embedded \ncache such as Ehcache or data grids like Hazelcast or Infinispan could also be used \nbased on the data volumes. This is the most preferred approach for a large number  \nof microservices that have dependency on the master data.\n\n\nApplying Microservices Concepts\n[ 138 ]\nMicroservices and bulk operations\nSince we have broken monolithic applications into smaller, focused services, it is no \nlonger possible to use join queries across microservice data stores. This could lead to \nsituations where one service may need many records from other services to perform \nits function.\nFor example, a monthly billing function needs the invoices of many customers \nto process the billing. To make it a bit more complicated, invoices may have \nmany orders. When we break billing, invoices, and orders into three different \nmicroservices, the challenge that arises is that the Billing service has to query the \nInvoices service for each customer to get all the invoices, and then for each invoice, \ncall the Order service for getting the orders. This is not a good solution, as the \nnumber of calls that goes to other microservices are high:\nThere are two ways we can think about for solving this. The first approach is to \npre-aggregate data as and when it is created. When an order is created, an event \nis sent out. Upon receiving the event, the Billing microservice keeps aggregating \ndata internally for monthly processing. In this case, there is no need for the Billing \nmicroservice to go out for processing. The downside of this approach is that there is \nduplication of data.\n\n\nChapter 3\n[ 139 ]\nA second approach, when pre-aggregation is not possible, is to use batch APIs. In \nsuch cases, we call GetAllInvoices, then we use multiple batches, and each batch \nfurther uses parallel threads to get orders. Spring Batch is useful in these situations.\nMicroservices challenges\nIn the previous section, you learned about the right design decisions to be taken, and \nthe trade-offs to be applied. In this section, we will review some of the challenges with \nmicroservices, and how to address them for a successful microservice development.\nData islands\nMicroservices abstract their own local transactional store, which is used for their own \ntransactional purposes. The type of store and the data structure will be optimized for \nthe services offered by the microservice.\nFor example, if we want to develop a customer relationship graph, we may use a \ngraph database like Neo4j, OrientDB, and the like. A predictive text search to find \nout a customer based on any related information such as passport number, address, \ne-mail, phone, and so on could be best realized using an indexed search database  \nlike Elasticsearch or Solr.\nThis will place us into a unique situation of fragmenting data into heterogeneous \ndata islands. For example, Customer, Loyalty Points, Reservations, and others are \ndifferent microservices, and hence, use different databases. What if we want to do a \nnear real-time analysis of all high value customers by combining data from all three \ndata stores? This was easy with a monolithic application, because all the data was \npresent in a single database:\n\n\nApplying Microservices Concepts\n[ 140 ]\nIn order to satisfy this requirement, a data warehouse or a data lake is required. \nTraditional data warehouses like Oracle, Teradata, and others are used primarily \nfor batch reporting. But with NoSQL databases (like Hadoop) and microbatching \ntechniques, near real-time analytics is possible with the concept of data lakes. Unlike \nthe traditional warehouses that are purpose-built for batch reporting, data lakes \nstore raw data without assuming how the data is going to be used. Now the question \nreally is how to port the data from microservices into data lakes.\nData porting from microservices to a data lake or a data warehouse can be done in \nmany ways. Traditional ETL could be one of the options. Since we allow backdoor \nentry with ETL, and break the abstraction, this is not considered an effective way for \ndata movement. A better approach is to send events from microservices as and when \nthey occur, for example, customer registration, customer update events, and so on. \nData ingestion tools consume these events, and propagate the state change to the \ndata lake appropriately. The data ingestion tools are highly scalable platforms such \nas Spring Cloud Data Flow, Kafka, Flume, and so on.\nLogging and monitoring\nLog files are a good piece of information for analysis and debugging. Since each \nmicroservice is deployed independently, they emit separate logs, maybe to a local disk. \nThis results in fragmented logs. When we scale services across multiple machines, each \nservice instance could produce separate log files. This makes it extremely difficult to \ndebug and understand the behavior of the services through log mining.\nExamining Order, Delivery, and Notification as three different microservices, we \nfind no way to correlate a customer transaction that runs across all three of them:\n\n\nChapter 3\n[ 141 ]\nWhen implementing microservices, we need a capability to ship logs from each \nservice to a centrally managed log repository. With this approach, services do not \nhave to rely on the local disk or local I/Os. A second advantage is that the log files \nare centrally managed, and are available for all sorts of analysis such as historical, \nreal time, and trending. By introducing a correlation ID, end-to-end transactions  \ncan be easily tracked.\nWith a large number of microservices, and with multiple versions and service \ninstances, it would be difficult to find out which service is running on which server, \nwhat's the health of these services, the service dependencies, and so on. This was \nmuch easier with monolithic applications that are tagged against a specific or a  \nfixed set of servers.\nApart from understanding the deployment topology and health, it also poses a \nchallenge in identifying service behaviors, debugging, and identifying hotspots. \nStrong monitoring capabilities are required to manage such an infrastructure.\nWe will cover the logging and monitoring aspects in Chapter 7, Logging and Monitoring \nMicroservices.\nDependency management\nDependency management is one of the key issues in large microservice deployments. \nHow do we identify and reduce the impact of a change? How do we know whether \nall the dependent services are up and running? How will the service behave if one  \nof the dependent services is not available?\nToo many dependencies could raise challenges in microservices. Four important \ndesign aspects are stated as follows:\n•\t\nReducing dependencies by properly designing service boundaries.\n•\t\nReducing impacts by designing dependencies as loosely coupled as possible. \nAlso, designing service interactions through asynchronous communication \nstyles.\n•\t\nTackling dependency issues using patterns such as circuit breakers.\n•\t\nMonitoring dependencies using visual dependency graphs.\n\n\nApplying Microservices Concepts\n[ 142 ]\nOrganization culture\nOne of the biggest challenges in microservices implementation is the organization \nculture. To harness the speed of delivery of microservices, the organization should \nadopt Agile development processes, continuous integration, automated QA checks, \nautomated delivery pipelines, automated deployments, and automatic infrastructure \nprovisioning.\nOrganizations following a waterfall development or heavyweight release management \nprocesses with infrequent release cycles are a challenge for microservices development. \nInsufficient automation is also a challenge for microservices deployment.\nIn short, Cloud and DevOps are supporting facets of microservice development. \nThese are essential for successful microservices implementation.\nGovernance challenges\nMicroservices impose decentralized governance, and this is quite in contrast with  \nthe traditional SOA governance. Organizations may find it hard to come up with  \nthis change, and that could negatively impact the microservices development.\nThere are number of challenges that comes with a decentralized governance model. \nHow do we understand who is consuming a service? How do we ensure service \nreuse? How do we define which services are available in the organization? How  \ndo we ensure enforcement of enterprise polices?\nThe first thing is to have a set of standards, best practices, and guidelines on how to \nimplement better services. These should be available to the organization in the form \nof standard libraries, tools, and techniques. This ensures that the services developed \nare top quality, and that they are developed in a consistent manner.\nThe second important consideration is to have a place where all stakeholders  \ncan not only see all the services, but also their documentations, contracts, and \nservice-level agreements. Swagger and API Blueprint are commonly used for \nhandling these requirements.\nOperation overheads\nMicroservices deployment generally increases the number of deployable units and \nvirtual machines (or containers). This adds significant management overheads and \nincreases the cost of operations.\n\n\nChapter 3\n[ 143 ]\nWith a single application, a dedicated number of containers or virtual machines in \nan on-premise data center may not make much sense unless the business benefit is \nvery high. The cost generally goes down with economies of scale. A large number of \nmicroservices that are deployed in a shared infrastructure which is fully automated \nmakes more sense, since these microservices are not tagged against any specific \nVMs or containers. Capabilities around infrastructure automation, provisioning, \ncontainerized deployment, and so on are essential for large scale microservices \ndeployments. Without this automation, it would result in a significant operation \noverhead and increased cost.\nWith many microservices, the number of configurable items (CIs) becomes too \nhigh, and the number of servers in which these CIs are deployed might also be \nunpredictable. This makes it extremely difficult to manage data in a traditional \nConfiguration Management Database (CMDB). In many cases, it is more useful \nto dynamically discover the current running topology than a statically configured \nCMDB-style deployment topology.\nTesting microservices\nMicroservices also pose a challenge for the testability of services. In order to achieve \na full-service functionality, one service may rely on another service, and that, in turn, \non another service—either synchronously or asynchronously. The issue is how do we \ntest an end-to-end service to evaluate its behavior? The dependent services may or \nmay not be available at the time of testing.\nService virtualization or service mocking is one of the techniques used for testing \nservices without actual dependencies. In testing environments, when the services \nare not available, mock services can simulate the behavior of the actual service. The \nmicroservices ecosystem needs service virtualization capabilities. However, this may \nnot give full confidence, as there may by many corner cases that mock services do \nnot simulate, especially when there are deep dependencies.\nAnother approach, as discussed earlier, is to use a consumer driven contract.  \nThe translated integration test cases can cover more or less all corner cases of the  \nservice invocation.\nTest automation, appropriate performance testing, and continuous delivery \napproaches such as A/B testing, future flags, canary testing, blue-green deployments, \nand red-black deployments, all reduce the risks of production releases.\n\n\nApplying Microservices Concepts\n[ 144 ]\nInfrastructure provisioning\nAs briefly touched on under operation overheads, manual deployment could \nseverely challenge the microservices rollouts. If a deployment has manual elements, \nthe deployer or operational administrators should know the running topology, \nmanually reroute traffic, and then deploy the application one by one till all services \nare upgraded. With many server instances running, this could lead to significant \noperational overheads. Moreover, the chances of errors are high in this manual \napproach.\nMicroservices require a supporting elastic cloud-like infrastructure which can \nautomatically provision VMs or containers, automatically deploy applications,  \nadjust traffic flows, replicate new version to all instances, and gracefully phase  \nout older versions. The automation also takes care of scaling up elastically by  \nadding containers or VMs on demand, and scaling down when the load falls  \nbelow threshold.\nIn a large deployment environment with many microservices, we may also need \nadditional tools to manage VMs or containers that can further initiate or destroy \nservices automatically.\nThe microservices capability model\nBefore we conclude this chapter, we will review a capability model for microservices \nbased on the design guidelines and common pattern and solutions described in  \nthis chapter.\nThe following diagram depicts the microservices capability model:\n\n\nChapter 3\n[ 145 ]\nThe capability model is broadly classified in to four areas:\n•\t\nCore capabilities: These are part of the microservices themselves\n•\t\nSupporting capabilities: These are software solutions supporting core \nmicroservice implementations\n•\t\nInfrastructure capabilities: These are infrastructure level expectations for  \na successful microservices implementation\n•\t\nGovernance capabilities: These are more of process, people, and reference \ninformation\nCore capabilities\nThe core capabilities are explained as follows:\n•\t\nService listeners (HTTP/messaging): If microservices are enabled for a \nHTTP-based service endpoint, then the HTTP listener is embedded within \nthe microservices, thereby eliminating the need to have any external \napplication server requirement. The HTTP listener is started at the time \nof the application startup. If the microservice is based on asynchronous \ncommunication, then instead of an HTTP listener, a message listener is \nstarted. Optionally, other protocols could also be considered. There may not \nbe any listeners if the microservice is a scheduled service. Spring Boot and \nSpring Cloud Streams provide this capability.\n•\t\nStorage capability: The microservices have some kind of storage mechanisms \nto store state or transactional data pertaining to the business capability. This \nis optional, depending on the capabilities that are implemented. The storage \ncould be either a physical storage (RDBMS such as MySQL; NoSQL such  \nas Hadoop, Cassandra, Neo 4J, Elasticsearch, and so on), or it could be an  \nin-memory store (cache like Ehcache, data grids like Hazelcast, Infinispan, \nand so on)\n•\t\nBusiness capability definition: This is the core of microservices, where  \nthe business logic is implemented. This could be implemented in any \napplicable language such as Java, Scala, Conjure, Erlang, and so on. All \nrequired business logic to fulfill the function will be embedded within  \nthe microservices themselves.\n•\t\nEvent sourcing: Microservices send out state changes to the external world \nwithout really worrying about the targeted consumers of these events. \nThese events could be consumed by other microservices, audit services, \nreplication services, or external applications, and the like. This allows other \nmicroservices and applications to respond to state changes.\n\n\nApplying Microservices Concepts\n[ 146 ]\n•\t\nService endpoints and communication protocols: These define the APIs \nfor external consumers to consume. These could be synchronous endpoints \nor asynchronous endpoints. Synchronous endpoints could be based on \nREST/JSON or any other protocols such as Avro, Thrift, Protocol Buffers, \nand so on. Asynchronous endpoints are through Spring Cloud Streams \nbacked by RabbitMQ, other messaging servers, or other messaging style \nimplementations such as ZeroMQ.\n•\t\nAPI gateway: The API gateway provides a level of indirection by either \nproxying service endpoints or composing multiple service endpoints. The \nAPI gateway is also useful for policy enforcements. It may also provide real \ntime load balancing capabilities. There are many API gateways available \nin the market. Spring Cloud Zuul, Mashery, Apigee, and 3scale are some \nexamples of the API gateway providers.\n•\t\nUser interfaces: Generally, user interfaces are also part of microservices for \nusers to interact with the business capabilities realized by the microservices. \nThese could be implemented in any technology, and are channel- and  \ndevice-agnostic.\nInfrastructure capabilities\nCertain infrastructure capabilities are required for a successful deployment, and \nmanaging large scale microservices. When deploying microservices at scale, not \nhaving proper infrastructure capabilities can be challenging, and can lead to failures:\n•\t\nCloud: Microservices implementation is difficult in a traditional data center \nenvironment with long lead times to provision infrastructures. Even a large \nnumber of infrastructures dedicated per microservice may not be very cost \neffective. Managing them internally in a data center may increase the cost  \nof ownership and cost of operations. A cloud-like infrastructure is better  \nfor microservices deployment.\n•\t\nContainers or virtual machines: Managing large physical machines is not \ncost effective, and they are also hard to manage. With physical machines, it \nis also hard to handle automatic fault tolerance. Virtualization is adopted by \nmany organizations because of its ability to provide optimal use of physical \nresources. It also provides resource isolation. It also reduces the overheads in \nmanaging large physical infrastructure components. Containers are the next \ngeneration of virtual machines. VMWare, Citrix, and so on provide virtual \nmachine technologies. Docker, Drawbridge, Rocket, and LXD are some of  \nthe containerizer technologies.\n\n\nChapter 3\n[ 147 ]\n•\t\nCluster control and provisioning: Once we have a large number of \ncontainers or virtual machines, it is hard to manage and maintain \nthem automatically. Cluster control tools provide a uniform operating \nenvironment on top of the containers, and share the available capacity across \nmultiple services. Apache Mesos and Kubernetes are examples of cluster \ncontrol systems.\n•\t\nApplication lifecycle management: Application lifecycle management \ntools help to invoke applications when a new container is launched, or \nkill the application when the container shuts down. Application life cycle \nmanagement allows for script application deployments and releases. It \nautomatically detects failure scenario, and responds to those failures thereby \nensuring the availability of the application. This works in conjunction with \nthe cluster control software. Marathon partially addresses this capability.\nSupporting capabilities\nSupporting capabilities are not directly linked to microservices, but they are essential \nfor large scale microservices development:\n•\t\nSoftware defined load balancer: The load balancer should be smart enough \nto understand the changes in the deployment topology, and respond \naccordingly. This moves away from the traditional approach of configuring \nstatic IP addresses, domain aliases, or cluster addresses in the load balancer. \nWhen new servers are added to the environment, it should automatically \ndetect this, and include them in the logical cluster by avoiding any manual \ninteractions. Similarly, if a service instance is unavailable, it should take it out \nfrom the load balancer. A combination of Ribbon, Eureka, and Zuul provide \nthis capability in Spring Cloud Netflix.\n•\t\nCentral log management: As explored earlier in this chapter, a capability is \nrequired to centralize all logs emitted by service instances with the correlation \nIDs. This helps in debugging, identifying performance bottlenecks, and \npredictive analysis. The result of this is fed back into the life cycle manager  \nto take corrective actions.\n•\t\nService registry: A service registry provides a runtime environment for \nservices to automatically publish their availability at runtime. A registry \nwill be a good source of information to understand the services topology at \nany point. Eureka from Spring Cloud, Zookeeper, and Etcd are some of the \nservice registry tools available.\n\n\nApplying Microservices Concepts\n[ 148 ]\n•\t\nSecurity service: A distributed microservices ecosystem requires a central \nserver for managing service security. This includes service authentication \nand token services. OAuth2-based services are widely used for microservices \nsecurity. Spring Security and Spring Security OAuth are good candidates for \nbuilding this capability.\n•\t\nService configuration: All service configurations should be externalized as \ndiscussed in the Twelve-Factor application principles. A central service for \nall configurations is a good choice. Spring Cloud Config server, and Archaius \nare out-of-the-box configuration servers.\n•\t\nTesting tools (anti-fragile, RUM, and so on): Netflix uses Simian Army for \nanti-fragile testing. Matured services need consistent challenges to see the \nreliability of the services, and how good fallback mechanisms are. Simian \nArmy components create various error scenarios to explore the behavior of \nthe system under failure scenarios.\n•\t\nMonitoring and dashboards: Microservices also require a strong monitoring \nmechanism. This is not just at the infrastructure-level monitoring but also \nat the service level. Spring Cloud Netflix Turbine, Hysterix Dashboard, and \nthe like provide service level information. End-to-end monitoring tools like \nAppDynamic, New Relic, Dynatrace, and other tools like statd, Sensu, and \nSpigo could add value to microservices monitoring.\n•\t\nDependency and CI management: We also need tools to discover runtime \ntopologies, service dependencies, and to manage configurable items. A \ngraph-based CMDB is the most obvious tool to manage these scenarios.\n•\t\nData lake: As discussed earlier in this chapter, we need a mechanism to \ncombine data stored in different microservices, and perform near real-time \nanalytics. A data lake is a good choice for achieving this. Data ingestion tools \nlike Spring Cloud Data Flow, Flume, and Kafka are used to consume data. \nHDFS, Cassandra, and the like are used for storing data.\n•\t\nReliable messaging: If the communication is asynchronous, we may need \na reliable messaging infrastructure service such as RabbitMQ or any other \nreliable messaging service. Cloud messaging or messaging as a service is a \npopular choice in Internet scale message-based service endpoints.\nProcess and governance capabilities\nThe last piece in the puzzle is the process and governance capabilities that are \nrequired for microservices:\n•\t\nDevOps: The key to successful implementation of microservices is to adopt \nDevOps. DevOps compliment microservices development by supporting \nAgile development, high velocity delivery, automation, and better change \nmanagement.\n\n\nChapter 3\n[ 149 ]\n•\t\nDevOps tools: DevOps tools for Agile development, continuous integration, \ncontinuous delivery, and continuous deployment are essential for successful \ndelivery of microservices. A lot of emphasis is required on automated \nfunctioning, real user testing, synthetic testing, integration, release, and \nperformance testing.\n•\t\nMicroservices repository: A microservices repository is where the versioned \nbinaries of microservices are placed. These could be a simple Nexus \nrepository or a container repository such as a Docker registry.\n•\t\nMicroservice documentation: It is important to have all microservices \nproperly documented. Swagger or API Blueprint are helpful in achieving \ngood microservices documentation.\n•\t\nReference architecture and libraries: The reference architecture provides a \nblueprint at the organization level to ensure that the services are developed \naccording to certain standards and guidelines in a consistent manner. Many \nof these could then be translated to a number of reusable libraries that \nenforce service development philosophies.\nSummary\nIn this chapter, you learned about handling practical scenarios that will arise in \nmicroservices development.\nYou learned various solution options and patterns that could be applied to \nsolve common microservices problems. We reviewed a number of challenges \nwhen developing large scale microservices, and how to address those challenges \neffectively.\nWe also built a capability reference model for a microservices-based ecosystem. \nThe capability model helps in addressing gaps when building Internet scale \nmicroservices. The capability model learned in this chapter will be the backbone  \nfor this book. The remaining chapters will deep dive into the capability model.\nIn the next chapter, we will take a real-world problem and model it using the \nmicroservices architecture to see how to translate our learnings into practice.\n\n\n[ 151 ]\nMicroservices Evolution – A \nCase Study\nLike SOA, a microservices architecture can be interpreted differently by different \norganizations, based on the problem in hand. Unless a sizable, real world problem  \nis examined in detail, microservices concepts are hard to understand.\nThis chapter will introduce BrownField Airline (BF), a fictitious budget airline, and \ntheir journey from a monolithic Passenger Sales and Service (PSS) application \nto a next generation microservices architecture. This chapter examines the PSS \napplication in detail, and explains the challenges, approach, and transformation \nsteps of a monolithic system to a microservices-based architecture, adhering to the \nprinciples and practices that were explained in the previous chapter.\nThe intention of this case study is to get us as close as possible to a live scenario so \nthat the architecture concepts can be set in stone.\nBy the end of this chapter, you will have learned about the following:\n•\t\nA real world case for migrating monolithic systems to microservices-based \nones, with the BrownField Airline's PSS application as an example\n•\t\nVarious approaches and transition strategies for migrating a monolithic \napplication to microservices\n•\t\nDesigning a new futuristic microservices system to replace the PSS \napplication using Spring Framework components\n\n\nMicroservices Evolution – A Case Study\n[ 152 ]\nReviewing the microservices capability \nmodel\nThe examples in this chapter explore the following microservices capabilities  \nfrom the microservices capability model discussed in Chapter 3, Applying \nMicroservices Concepts:\n•\t\nHTTP Listener\n•\t\nMessage Listener\n•\t\nStorage Capabilities (Physical/In-Memory)\n•\t\nBusiness Capability Definitions\n•\t\nService Endpoints & Communication Protocols\n•\t\nUser Interfaces\n•\t\nSecurity Service\n•\t\nMicroservice Documentation\n\n\nChapter 4\n[ 153 ]\nIn Chapter 2, Building Microservices with Spring Boot, we explored all these capabilities \nin isolation including how to secure Spring Boot microservices. This chapter will \nbuild a comprehensive microservices example based on a real world case study.\nThe full source code of this chapter is available under the \nChapter 4 projects in the code files.\nUnderstanding the PSS application\nBrownField Airline is one of the fastest growing low-cost, regional airlines,  \nflying directly to more than 100 destinations from its hub. As a start-up airline, \nBrownField Airline started its operations with few destinations and few aircrafts. \nBrownField developed its home-grown PSS application to handle their passenger \nsales and services.\nBusiness process view\nThis use case is considerably simplified for discussion purposes. The process view \nin the following diagram shows BrownField Airline's end-to-end passenger services \noperations covered by the current PSS solution:\nThe current solution is automating certain customer-facing functions as well as \ncertain internally facing functions. There are two internally facing functions,  \nPre-flight and Post-flight. Pre-flight functions include the planning phase, used for \npreparing flight schedules, plans, aircrafts, and so on. Post-flight functions are used \nby the back office for revenue management, accounting, and so on. The Search and \nReserve functions are part of the online seat reservation process, and the Check-in \nfunction is the process of accepting passengers at the airport. The Check-in function \nis also accessible to the end users over the Internet for online check-in.\n",
      "page_number": 134
    },
    {
      "number": 4,
      "title": "[ 153 ]",
      "start_page": 180,
      "end_page": 231,
      "detection_method": "regex_chapter",
      "content": "Microservices Evolution – A Case Study\n[ 154 ]\nThe cross marks at the beginning of the arrows in the preceding diagram indicate \nthat they are disconnected, and occur at different timelines. For example, passengers \nare allowed to book 360 days in advance, whereas the check-in generally happens 24 \nhours before flight departure.\nFunctional view\nThe following diagram shows the functional building blocks of BrownField Airline's \nPSS landscape. Each business process and its related subfunctions are represented in \na row:\nEach subfunction shown in the preceding diagram explains its role in the overall \nbusiness process. Some subfunctions participate in more than one business process. \nFor example, inventory is used in both search as well as in booking. To avoid any \ncomplication, this is not shown in the diagram. Data management and cross-cutting \nsubfunctions are used across many business functions.\nArchitectural view\nIn order to effectively manage the end-to-end passenger operations, BrownField had \ndeveloped an in-house PSS application, almost ten years back. This well-architected \napplication was developed using Java and JEE technologies combined with the best-\nof-the-breed open source technologies available at the time.\n\n\nChapter 4\n[ 155 ]\nThe overall architecture and technologies are shown in the following diagram:\nThe architecture has well-defined boundaries. Also, different concerns are separated \ninto different layers. The web application was developed as an N-tier, component-\nbased modular system. The functions interact with each other through well-defined \nservice contracts defined in the form of EJB endpoints.\nDesign view\nThe application has many logical functional groupings or subsystems. Further, each \nsubsystem has many components organized as depicted in the next diagram:\nSubsystems interact with each other through remote EJB calls using the IIOP \nprotocol. The transactional boundaries span across subsystems. Components \nwithin the subsystems communicate with each other through local EJB component \ninterfaces. In theory, since subsystems use remote EJB endpoints, they could run on \ndifferent physically separated application servers. This was one of the design goals.\n\n\nMicroservices Evolution – A Case Study\n[ 156 ]\nImplementation view\nThe implementation view in the following diagram showcases the internal \norganization of a subsystem and its components. The purpose of the diagram is also \nto show the different types of artifacts:\nIn the preceding diagram, the gray-shaded boxes are treated as different Maven \nprojects, and translate into physical artifacts. Subsystems and components are \ndesigned adhering to the program to an interface principle. Interfaces are packaged \nas separate JAR files so that clients are abstracted from the implementations. The \ncomplexity of the business logic is buried in the domain model. Local EJBs are used \nas component interfaces. Finally, all subsystems are packaged into a single all-in-one \nEAR, and deployed in the application server.\n\n\nChapter 4\n[ 157 ]\nDeployment view\nThe application's initial deployment was simple and straightforward as shown in  \nthe next diagram:\nThe web modules and business modules were deployed into separate application \nserver clusters. The application was scaled horizontally by adding more and more \napplication servers to the cluster.\nZero downtime deployments were handled by creating a standby cluster, and \ngracefully diverting the traffic to that cluster. The standby cluster is destroyed once \nthe primary cluster is patched with the new version and brought back to service. \nMost of the database changes were designed for backward compatibility, but \nbreaking changes were promoted with application outages.\n\n\nMicroservices Evolution – A Case Study\n[ 158 ]\nDeath of the monolith\nThe PSS application was performing well, successfully supporting all business \nrequirements as well as the expected service levels. The system had no issues in \nscaling with the organic growth of the business in the initial years.\nThe business has seen tremendous growth over a period of time. The fleet size \nincreased significantly, and new destinations got added to the network. As a result of \nthis rapid growth, the number of bookings has gone up, resulting in a steep increase \nin transaction volumes, up to 200 - to 500 - fold of what was originally estimated.\nPain points\nThe rapid growth of the business eventually put the application under pressure. Odd \nstability issues and performance issues surfaced. New application releases started \nbreaking the working code. Moreover, the cost of change and the speed of delivery \nstarted impacting the business operations profoundly.\nAn end-to-end architecture review was ordered, and it exposed the weaknesses of \nthe system as well as the root causes of many failures, which were as follows:\n•\t\nStability: The stability issues are primarily due to stuck threads, which limit \nthe application server's capability to accept more transactions. The stuck \nthreads are mainly due to database table locks. Memory issues are another \ncontributor to the stability issues. There were also issues in certain resource \nintensive operations that were impacting the whole application.\n•\t\nOutages: The outage window increased largely because of the increase in \nserver startup time. The root cause of this issue boiled down to the large size \nof the EAR. Message pile up during any outage windows causes heavy usage \nof the application immediately after an outage window. Since everything \nis packaged in a single EAR, any small application code change resulted in \nfull redeployment. The complexity of the zero downtime deployment model \ndescribed earlier, together with the server startup times increased both the \nnumber of outages and their duration.\n•\t\nAgility: The complexity of the code also increased considerably over time, \npartially due to the lack of discipline in implementing the changes. As a \nresult, changes became harder to implement. Also, the impact analysis \nbecame too complex to perform. As a result, inaccurate impact analysis often \nled to fixes that broke the working code. The application build time went \nup severely, from a few minutes to hours, causing unacceptable drops in \ndevelopment productivity. The increase in build time also led to difficulty in \nbuild automation, and eventually stopped continuous integration (CI) and \nunit testing.\n\n\nChapter 4\n[ 159 ]\nStop gap fix\nPerformance issues were partially addressed by applying the Y-axis scale method \nin the scale cube, as described in Chapter 1, Demystifying Microservices. The all-\nencompassing EAR is deployed into multiple disjoint clusters. A software proxy \nwas installed to selectively route the traffic to designated clusters as shown in the \nfollowing diagram:\nThis helped BrownField's IT to scale the application servers. Therefore, the stability \nissues were controlled. However, this soon resulted in a bottleneck at the database \nlevel. Oracle's Real Application Cluster (RAC) was implemented as a solution to \nthis problem at the database layer.\nThis new scaling model reduced the stability issues, but at a premium of increased \ncomplexity and cost of ownership. The technology debt also increased over a period \nof time, leading to a state where a complete rewrite was the only option for reducing \nthis technology debt.\n\n\nMicroservices Evolution – A Case Study\n[ 160 ]\nRetrospection\nAlthough the application was well-architected, there was a clear segregation between \nthe functional components. They were loosely coupled, programmed to interfaces, \nwith access through standards-based interfaces, and had a rich domain model.\nThe obvious question is, how come such a well-architected application failed to live \nup to the expectations? What else could the architects have done?\nIt is important to understand what went wrong over a period of time. In the context \nof this book, it is also important to understand how microservices can avoid the \nrecurrence of these scenarios. We will examine some of these scenarios in the \nsubsequent sections.\nShared data\nAlmost all functional modules require reference data such as the airline's details, \nairplane details, a list of airports and cities, countries, currencies, and so on. For \nexample, fare is calculated based on the point of origin (city), a flight is between an \norigin and a destination (airports), check-in is at the origin airport (airport), and so \non. In some functions, the reference data is a part of the information model, whereas \nin some other functions, it is used for validation purposes.\nMuch of this reference data is neither fully static nor fully dynamic. Addition of \na country, city, airport, or the like could happen when the airline introduces new \nroutes. Aircraft reference data could change when the airline purchases a new \naircraft, or changes an existing airplane's seat configuration.\nOne of the common usage scenarios of reference data is to filter the operational data \nbased on certain reference data. For instance, say a user wishes to see all the flights to \na country. In this case, the flow of events could be as follows: find all the cities in the \nselected country, then all airports in the cities, and then fire a request to get all the \nflights to the list of resulting airports identified in that country.\nThe architects considered multiple approaches when designing the system. \nSeparating the reference data as an independent subsystem like other subsystems \nwas one of the options considered, but this could lead to performance issues. The \nteam took the decision to follow an exception approach for handling reference data \nas compared to other transactions. Considering the nature of the query patterns \ndiscussed earlier, the approach was to use the reference data as a shared library. \n\n\nChapter 4\n[ 161 ]\nIn this case, the subsystems were allowed to access the reference data directly using \npass-by-reference semantic data instead of going through the EJB interfaces. This also \nmeant that irrespective of the subsystems, hibernate entities could use the reference \ndata as a part of their entity relationships:\nAs depicted in the preceding diagram, the Booking entity in the reservation \nsubsystem is allowed to use the reference data entities, in this case Airport,  \nas part of their relationships.\nSingle database\nThough enough segregation was enforced at the middle tier, all functions pointed to \na single database, even to the same database schema. The single schema approach \nopened a plethora of issues.\nNative queries\nThe Hibernate framework provides a good abstraction over the underlying \ndatabases. It generates efficient SQL statements, in most of the cases targeting the \ndatabase using specific dialects. However, sometimes, writing native JDBC SQLs \noffers better performance and resource efficiency. In some cases, using native \ndatabase functions gives an even better performance.\nThe single database approach worked well at the beginning. But over a period of \ntime, it opened up a loophole for the developers by connecting database tables \nowned by different subsystems. Native JDBC SQL was a good vehicle for doing this.\n\n\nMicroservices Evolution – A Case Study\n[ 162 ]\nThe following diagram shows an example of connecting two tables owned by two \nsubsystems using a native JDBC SQL:\nAs shown in the preceding diagram, the Accounting component requires all \nbooking records for a day, for a given city, from the Booking component to process \nthe day-end billing. The subsystem-based design enforces Accounting to make \na service call to Booking to get all booking records for a given city. Assume this \nresults in N booking records. Now, for each booking record, Accounting has to \nexecute a database call to find the applicable rules based on the fare code attached \nto each booking record. This could result in N+1 JDBC calls, which is inefficient. \nWorkarounds, such as batch queries or parallel and batch executions, are available, \nbut this would lead to increased coding efforts and higher complexity. The \ndevelopers tackled this issue with a native JDBC query as an easy-to-implement \nshortcut. Essentially, this approach could reduce the number of calls from N+1 to a \nsingle database call, with minimal coding efforts.\nThis habit continued with many JDBC native queries connecting tables across \nmultiple components and subsystems. This resulted not only in tightly coupled \ncomponents, but also led to undocumented, hard-to-detect code.\n\n\nChapter 4\n[ 163 ]\nStored procedures\nAnother issue that surfaced as a result of the use of a single database was the use of \ncomplex stored procedures. Some of the complex data-centric logic written at the \nmiddle layer was not performing well, causing slow response, memory issues, and \nthread-blocking issues.\nIn order to address this problem, the developers took the decision to move some of \nthe complex business logic from the middle tier to the database tier by implementing \nthe logic directly within the stored procedures. This decision resulted in better \nperformance of some of the transactions, and removed some of the stability issues. \nMore and more procedures were added over a period of time. However, this \neventually broke the application's modularity.\nDomain boundaries\nThough the domain boundaries were well established, all the components were \npackaged as a single EAR file. Since all the components were set to run on a single \ncontainer, there was no stopping the developers referencing objects across these \nboundaries. Over a period of time, the project teams changed, delivery pressure \nincreased, and the complexity grew tremendously. The developers started looking \nfor quick solutions rather than the right ones. Slowly, but steadily, the modular \nnature of the application went away.\nAs depicted in the following diagram, hibernate relationships were created across \nsubsystem boundaries:\n\n\nMicroservices Evolution – A Case Study\n[ 164 ]\nMicroservices to the rescue\nThere are not many improvement opportunities left to support the growing demand \nof BrownField Airline's business. BrownField Airline was looking to re-platform the \nsystem with an evolutionary approach rather than a revolutionary model.\nMicroservices is an ideal choice in these situations—for transforming a legacy \nmonolithic application with minimal disruption to the business:\nAs shown in the preceding diagram, the objective is to move to a microservices-\nbased architecture aligned to the business capabilities. Each microservice will hold \nthe data store, the business logic, and the presentation layer.\nThe approach taken by BrownField Airline is to build a number of web portal \napplications targeting specific user communities such as customer facing, front office, \nand back office. The advantage of this approach lies in the flexibility for modeling, \nand also in the possibility to treat different communities differently. For example, \nthe policies, architecture, and testing approaches for the Internet facing layer are \ndifferent from the intranet-facing web application. Internet-facing applications may \ntake advantage of CDNs (Content Delivery Networks) to move pages as close to the \ncustomer as possible, whereas intranet applications could serve pages directly from \nthe data center.\n\n\nChapter 4\n[ 165 ]\nThe business case\nWhen building business cases for migration, one of the commonly asked questions \nis \"how does the microservices architecture avoid resurfacing of the same issues in \nanother five years' time?\"\nMicroservices offers a full list of benefits, which you learned in Chapter 1, \nDemystifying Microservices, but it is important to list a few here that are critical  \nin this situation:\n•\t\nService dependencies: While migrating from monolithic applications to \nmicroservices, the dependencies are better known, and therefore the architects \nand developers are much better placed to avoid breaking dependencies and \nto future-proof dependency issues. Lessons from the monolithic application \nhelps architects and developers to design a better system.\n•\t\nPhysical boundaries: Microservices enforce physical boundaries in all areas \nincluding the data store, the business logic, and the presentation layer. \nAccess across subsystems or microservices are truly restricted due to their \nphysical isolation. Beyond the physical boundaries, they could even run on \ndifferent technologies.\n•\t\nSelective scaling: Selective scale out is possible in microservices architecture. \nThis provides a much more cost-effective scaling mechanism compared to \nthe Y-scale approach used in the monolithic scenario.\n•\t\nTechnology obsolescence: Technology migrations could be applied at a \nmicroservices level rather than at the overall application level. Therefore,  \nit does not require a humongous investment.\nPlan the evolution\nIt is not simple to break an application that has millions of lines of code, especially if \nthe code has complex dependencies. How do we break it? More importantly, where \ndo we start, and how do we approach this problem?\n\n\nMicroservices Evolution – A Case Study\n[ 166 ]\nEvolutionary approach\nThe best way to address this problem is to establish a transition plan, and gradually \nmigrate the functions as microservices. At every step, a microservice will be created \noutside of the monolithic application, and traffic will be diverted to the new service \nas shown in the following diagram:\nIn order to run this migration successfully, a number of key questions need to be \nanswered from the transition point of view:\n•\t\nIdentification of microservices' boundaries\n•\t\nPrioritizing microservices for migration\n•\t\nHandling data synchronization during the transition phase\n•\t\nHandling user interface integration, working with old and new user \ninterfaces\n\n\nChapter 4\n[ 167 ]\n•\t\nHandling of reference data in the new system\n•\t\nTesting strategy to ensure the business capabilities are intact and correctly \nreproduced\n•\t\nIdentification of any prerequisites for microservice development such as \nmicroservices capabilities, frameworks, processes, and so on \nIdentification of microservices boundaries\nThe first and foremost activity is to identify the microservices' boundaries. This \nis the most interesting part of the problem, and the most difficult part as well. If \nidentification of the boundaries is not done properly, the migration could lead to \nmore complex manageability issues.\nLike in SOA, a service decomposition is the best way to identify services. However, \nit is important to note that decomposition stops at a business capability or bounded \ncontext. In SOA, service decomposition goes further into an atomic, granular service \nlevel.\nA top-down approach is typically used for domain decomposition. The bottom-up \napproach is also useful in the case of breaking an existing system, as it can utilize \na lot of practical knowledge, functions, and behaviors of the existing monolithic \napplication.\nThe previous decomposition step will give a potential list of microservices. It is \nimportant to note that this isn't the final list of microservices, but it serves as a good \nstarting point. We will run through a number of filtering mechanisms to get to a \nfinal list. The first cut of functional decomposition will, in this case, be similar to the \ndiagram shown under the functional view introduced earlier in this chapter.\nAnalyze dependencies\nThe next step is to analyze the dependencies between the initial set of candidate \nmicroservices that we created in the previous section. At the end of this activity,  \na dependency graph will be produced.\nA team of architects, business analysts, developers, release \nmanagement and support staff is required for this exercise.\n\n\nMicroservices Evolution – A Case Study\n[ 168 ]\nOne way to produce a dependency graph is to list out all the components of the \nlegacy system and overlay dependencies. This could be done by combining one or \nmore of the approaches listed as follows:\n•\t\nAnalyzing the manual code and regenerating dependencies.\n•\t\nUsing the experience of the development team to regenerate dependencies.\n•\t\nUsing a Maven dependency graph. There are a number of tools we could use \nto regenerate the dependency graph, such as PomExplorer, PomParser, and \nso on.\n•\t\nUsing performance engineering tools such as AppDynamics to identify the \ncall stack and roll up dependencies.\nLet us assume that we reproduce the functions and their dependencies as shown in \nthe following diagram:\n\n\nChapter 4\n[ 169 ]\nThere are many dependencies going back and forth between different modules.  \nThe bottom layer shows cross-cutting capabilities that are used across multiple \nmodules. At this point, the modules are more like spaghetti than autonomous units.\nThe next step is to analyze these dependencies, and come up with a better, simplified \ndependency map.\nEvents as opposed to query \nDependencies could be query-based or event-based. Event-based is better for \nscalable systems. Sometimes, it is possible to convert query-based communications \nto event-based ones. In many cases, these dependencies exist because either the \nbusiness organizations are managed like that, or by virtue of the way the old system \nhandled the business scenario.\nFrom the previous diagram, we can extract the Revenue Management and the  \nFares services:\nRevenue Management is a module used for calculating optimal fare values, based \non the booking demand forecast. In case of a fare change between an origin and a \ndestination, Update Fare on the Fare module is called by Revenue Management to \nupdate the respective fares in the Fare module.\nAn alternate way of thinking is that the Fare module is subscribed to Revenue \nManagement for any changes in fares, and Revenue Management publishes \nwhenever there is a fare change. This reactive programming approach gives an \nadded flexibility by which the Fares and the Revenue Management modules could \nstay independent, and connect them through a reliable messaging system. This same \npattern could be applied in many other scenarios from Check-In to the Loyalty and \nBoarding modules.\nNext, examine the scenario of CRM and Booking:\n\n\nMicroservices Evolution – A Case Study\n[ 170 ]\nThis scenario is slightly different from the previously explained scenario. The CRM \nmodule is used to manage passenger complaints. When CRM receives a complaint, \nit retrieves the corresponding passenger's Booking data. In reality, the number of \ncomplaints are negligibly small when compared to the number of bookings. If we \nblindly apply the previous pattern where CRM subscribes to all bookings, we will \nfind that it is not cost effective:\nExamine another scenario between the Check-in and Booking modules. Instead of \nCheck-in calling the Get Bookings service on Booking, can Check-in listen to booking \nevents? This is possible, but the challenge here is that a booking can happen 360 \ndays in advance, whereas Check-in generally starts only 24 hours before the fight \ndeparture. Duplicating all bookings and booking changes in the Check-in module \n360 days in advance would not be a wise decision as Check-in does not require this \ndata until 24 hours before the flight departure.\nAn alternate option is that when check-in opens for a flight (24 hours before \ndeparture), Check-in calls a service on the Booking module to get a snapshot of the \nbookings for a given flight. Once this is done, Check-in could subscribe for booking \nevents specifically for that flight. In this case, a combination of query-based as well as \nevent-based approaches is used. By doing so, we reduce the unnecessary events and \nstorage apart from reducing the number of queries between these two services.\nIn short, there is no single policy that rules all scenarios. Each scenario requires \nlogical thinking, and then the most appropriate pattern is applied.\nEvents as opposed to synchronous updates\nApart from the query model, a dependency could be an update transaction as well. \nConsider the scenario between Revenue Management and Booking:\n\n\nChapter 4\n[ 171 ]\nIn order to do a forecast and analysis of the current demand, Revenue Management \nrequires all bookings across all flights. The current approach, as depicted in the \ndependency graph, is that Revenue Management has a schedule job that calls Get \nBooking on Booking to get all incremental bookings (new and changed) since the last \nsynchronization.\nAn alternative approach is to send new bookings and the changes in bookings \nas soon as they take place in the Booking module as an asynchronous push. The \nsame pattern could be applied in many other scenarios such as from Booking to \nAccounting, from Flight to Inventory, and also from Flight to Booking. In this \napproach, the source service publishes all state-change events to a topic. All \ninterested parties could subscribe to this event stream and store locally. This \napproach removes many hard wirings, and keeps the systems loosely coupled.\nThe dependency is depicted in the next diagram:\nIn this case depicted in the preceding diagram, we changed both dependencies and \nconverted them to asynchronous events.\nOne last case to analyze is the Update Inventory call from the Booking module to the \nInventory module:\n\n\nMicroservices Evolution – A Case Study\n[ 172 ]\nWhen a booking is completed, the inventory status is updated by depleting the \ninventory stored in the Inventory service. For example, when there are 10 economy \nclass seats available, at the end of the booking, we have to reduce it to 9. In the \ncurrent system, booking and updating inventory are executed within the same \ntransaction boundaries. This is to handle a scenario in which there is only one seat \nleft, and multiple customers are trying to book. In the new design, if we apply the \nsame event-driven pattern, sending the inventory update as an event to Inventory \nmay leave the system in an inconsistent state. This needs further analysis, which we \nwill address later in this chapter.\nChallenge requirements\nIn many cases, the targeted state could be achieved by taking another look at the \nrequirements:\nThere are two Validate Flight calls, one from Booking and another one from the \nSearch module. The Validate Flight call is to validate the input flight data coming \nfrom different channels. The end objective is to avoid incorrect data stored or serviced. \nWhen a customer does a flight search, say \"BF100\", the system validates this flight to \nsee the following things:\n•\t\nWhether this is a valid flight?\n•\t\nWhether the flight exists on that particular date?\n•\t\nAre there any booking restrictions set on this flight?\n\n\nChapter 4\n[ 173 ]\nAn alternate way of solving this is to adjust the inventory of the flight based on \nthese given conditions. For example, if there is a restriction on the flight, update the \ninventory as zero. In this case, the intelligence will remain with Flight, and it keeps \nupdating the inventory. As far as Search and Booking are concerned, both just look \nup the inventory instead of validating flights for every request. This approach is \nmore efficient as compared to the original approach.\nNext we will review the Payment use case. Payment is typically a disconnected \nfunction due to the nature of security constraints such as PCIDSS-like standards. The \nmost obvious way to capture a payment is to redirect a browser to a payment page \nhosted in the Payment service. Since card handling applications come under the \npurview of PCIDSS, it is wise to remove any direct dependencies from the Payment \nservice. Therefore, we can remove the Booking-to-Payment direct dependency, and \nopt for a UI-level integration.\nChallenge service boundaries\nIn this section, we will review some of the service boundaries based on the \nrequirements and dependency graph, considering Check-in and its dependencies to \nSeating and Baggage.\nThe Seating function runs a few algorithms based on the current state of the seat \nallocation in the airplane, and finds out the best way to position the next passenger \nso that the weight and balance requirements can be met. This is based on a number \nof predefined business rules. However, other than Check-in, no other module is \ninterested in the Seating function. From a business capability perspective, Seating is \njust a function of Check-in, not a business capability by itself. Therefore, it is better to \nembed this logic inside Check-in itself.\nThe same is applicable to Baggage as well. BrownField has a separate baggage \nhandling system. The Baggage function in the PSS context is to print the baggage tag \nas well as store the baggage data against the Check-in records. There is no business \ncapability associated with this particular functionality. Therefore, it is ideal to move \nthis function to Check-in itself.\n\n\nMicroservices Evolution – A Case Study\n[ 174 ]\nThe Book, Search, and Inventory functions, after redesigning, are shown in the \nfollowing diagram:\nSimilarly, Inventory and Search are more supporting functions of the Booking \nmodule. They are not aligned with any of the business capabilities as such. Similar to \nthe previous judgement, it is ideal to move both the Search and Inventory functions \nto Booking. Assume, for the time being, that Search, Inventory, and Booking are \nmoved to a single microservice named Reservation.\nAs per the statistics of BrownField, search transactions are 10 times more  \nfrequent than the booking transactions. Moreover, search is not a revenue-generating \ntransaction when compared to booking. Due to these reasons, we need different \nscalability models for search and booking. Booking should not get impacted if \nthere is a sudden surge of transactions in search. From the business point of view, \ndropping a search transaction in favor of saving a valid booking transaction is  \nmore acceptable.\n\n\nChapter 4\n[ 175 ]\nThis is an example of a polyglot requirement, which overrules the business capability \nalignment. In this case, it makes more sense to have Search as a service separate \nfrom the Booking service. Let us assume that we remove Search. Only Inventory and \nBooking remain under Reservation. Now Search has to hit back to Reservation to \nperform inventory searches. This could impact the booking transactions:\nA better approach is to keep Inventory along with the Booking module, and keep a \nread-only copy of the inventory under Search, while continuously synchronizing the \ninventory data over a reliable messaging system. Since both Inventory and Booking \nare collocated, this will also solve the need to have two-phase commits. Since both of \nthem are local, they could work well with local transactions.\nLet us now challenge the Fare module design. When a customer searches for a \nflight between A and B for a given date, we would like to show the flights and fares \ntogether. That means that our read-only copy of inventory can also combine both \nfares as well as inventory. Search will then subscribe to Fare for any fare change \nevents. The intelligence still stays with the Fare service, but it keeps sending fare \nupdates to the cached fare data under Search.\n\n\nMicroservices Evolution – A Case Study\n[ 176 ]\nFinal dependency graph\nThere are still a few synchronized calls, which, for the time being, we will keep as \nthey are.\nBy applying all these changes, the final dependency diagram will look like the \nfollowing one:\nNow we can safely consider each box in the preceding diagram as a microservice.  \nWe have nailed down many dependencies, and modeled many of them as \nasynchronous as well. The overall system is more or less designed in the reactive style. \nThere are still some synchronized calls shown in the diagram with bold lines, such as \nGet Bulk from Check-In, Get Booking from CRM, and Get Fare from Booking. These \nsynchronous calls are essentially required as per the trade-off analysis.\n\n\nChapter 4\n[ 177 ]\nPrioritizing microservices for migration\nWe have identified a first-cut version of our microservices-based architecture. As the \nnext step, we will analyze the priorities, and identify the order of migration. This \ncould be done by considering multiple factors explained as follows:\n•\t\nDependency: One of the parameters for deciding the priority is the \ndependency graph. From the service dependency graph, services with less \ndependency or no dependency at all are easy to migrate, whereas complex \ndependencies are way harder. Services with complex dependencies will also \nneed dependent modules to be migrated along with them.\nAccounting, Loyalty, CRM, and Boarding have less dependencies as \ncompared to Booking and Check-in. Modules with high dependencies will \nalso have higher risks in their migration.\n•\t\nTransaction volume: Another parameter that can be applied is analyzing the \ntransaction volumes. Migrating services with the highest transaction volumes \nwill relieve the load on the existing system. This will have more value from \nan IT support and maintenance perspective. However, the downside of this \napproach is the higher risk factor.\nAs stated earlier, Search requests are ten times higher in volume as compared \nto Booking requests. Requests for Check-in are the third-highest in volume \ntransaction after Search and Booking.\n•\t\nResource utilization: Resource utilization is measured based on the current \nutilizations such as CPU, memory, connection pools, thread pools, and so on. \nMigrating resource intensive services out of the legacy system provides relief \nto other services. This helps the remaining modules to function better.\nFlight, Revenue Management, and Accounting are resource-intensive \nservices, as they involve data-intensive transactions such as forecasting, \nbilling, flight schedule changes, and so on.\n•\t\nComplexity: Complexity is perhaps measured in terms of the business logic \nassociated with a service such as function points, lines of code, number of \ntables, number of services, and others. Less complex modules are easy to \nmigrate as compared to the more complex ones.\nBooking is extremely complex as compared to the Boarding, Search, and \nCheck-in services.\n\n\nMicroservices Evolution – A Case Study\n[ 178 ]\n•\t\nBusiness criticality: The business criticality could be either based on  \nrevenue or customer experience. Highly critical modules deliver higher \nbusiness value.\nBooking is the most revenue-generating service from the business stand \npoint, whereas Check-in is business critical as it could lead to flight departure \ndelays, which could lead to revenue loss as well as customer dissatisfaction.\n•\t\nVelocity of changes: Velocity of change indicates the number of change \nrequests targeting a function in a short time frame. This translates to speed \nand agility of delivery. Services with high velocity of change requests are \nbetter candidates for migration as compared to stable modules.\nStatistics show that Search, Booking, and Fares go through frequent changes, \nwhereas Check-in is the most stable function.\n•\t\nInnovation: Services that are part of a disruptive innovative process need \nto get priority over back office functions that are based on more established \nbusiness processes. Innovations in legacy systems are harder to achieve as \ncompared to applying innovations in the microservices world.\nMost of the innovations are around Search, Booking, Fares, Revenue \nManagement, and Check-in as compared to back office Accounting.\nBased on BrownField's analysis, Search has the highest priority, as it requires \ninnovation, has high velocity of changes, is less business critical, and gives better \nrelief for both business and IT. The Search service has minimal dependency with no \nrequirements to synchronize data back to the legacy system.\nData synchronization during migration\nDuring the transition phase, the legacy system and the new microservices will  \nrun in parallel. Therefore, it is important to keep the data synchronized between  \nthe two systems.\nThe simplest option is to synchronize the data between the two systems at  \nthe database level by using any data synchronization tool. This approach works  \nwell when both the old and the new systems are built on the same data store \ntechnologies. The complexity will be higher if the data store technologies are \ndifferent. The second problem with this approach is that we allow a backdoor entry, \nhence exposing the microservices' internal data store outside. This is against the \nprinciple of microservices.\n\n\nChapter 4\n[ 179 ]\nLet us take this on a case-by-case basis before we can conclude with a generic \nsolution. The following diagram shows the data migration and synchronization \naspect once Search is taken out:\nLet us assume that we use a NoSQL database for keeping inventory and fares under \nthe Search service. In this particular case, all we need is the legacy system to supply \ndata to the new service using asynchronous events. We will have to make some \nchanges in the existing system to send the fare changes or any inventory changes as \nevents. The Search service then accepts these events, and stores them locally into the \nlocal NoSQL store.\nThis is a bit more tedious in the case of the complex Booking service.\n\n\nMicroservices Evolution – A Case Study\n[ 180 ]\nIn this case, the new Booking microservice sends the inventory change events to the \nSearch service. In addition to this, the legacy application also has to send the fare \nchange events to Search. Booking will then store the new Booking service in its My \nSQL data store.\nThe most complex piece, the Booking service, has to send the booking events  \nand the inventory events back to the legacy system. This is to ensure that the \nfunctions in the legacy system continue to work as before. The simplest approach  \nis to write an update component which accepts the events and updates the old \nbooking records table so that there are no changes required in the other legacy \nmodules. We will continue this until none of the legacy components are referring \nthe booking and inventory data. This will help us minimize changes in the legacy \nsystem, and therefore, reduce the risk of failures.\nIn short, a single approach may not be sufficient. A multi-pronged approach based \non different patterns is required.\n\n\nChapter 4\n[ 181 ]\nManaging reference data\nOne of the biggest challenges in migrating monolithic applications to microservices \nis managing reference data. A simple approach is to build the reference data as \nanother microservice itself as shown in the following diagram:\nIn this case, whoever needs reference data should access it through the microservice \nendpoints. This is a well-structured approach, but could lead to performance issues \nas encountered in the original legacy system.\nAn alternate approach is to have reference data as a microservice service for all the \nadmin and CRUD functions. A near cache will then be created under each service to \nincrementally cache data from the master services. A thin reference data access proxy \nlibrary will be embedded in each of these services. The reference data access proxy \nabstracts whether the data is coming from cache or from a remote service.\n\n\nMicroservices Evolution – A Case Study\n[ 182 ]\nThis is depicted in the next diagram. The master node in the given diagram is the \nactual reference data microservice:\nThe challenge is to synchronize the data between the master and the slave. A \nsubscription mechanism is required for those data caches that change frequently.\nA better approach is to replace the local cache with an in-memory data grid, as \nshown in the following diagram:\n\n\nChapter 4\n[ 183 ]\nThe reference data microservice will write to the data grid, whereas the proxy \nlibraries embedded in other services will have read-only APIs. This eliminates the \nrequirement to have subscription of data, and is much more efficient and consistent.\nUser interfaces and web applications\nDuring the transition phase, we have to keep both the old and new user interfaces \ntogether. There are three general approaches usually taken in this scenario.\nThe first approach is to have the old and new user interfaces as separate user \napplications with no link between them, as depicted in the following diagram:\nA user signs in to the new application as well as into the old application, much \nlike two different applications, with no single sign-on (SSO) between them. This \napproach is simple, and there is no overhead. In most of the cases, this may not be \nacceptable to the business unless it is targeted at two different user communities.\nThe second approach is to use the legacy user interface as the primary application, \nand then transfer page controls to the new user interfaces when the user requests \npages of the new application:\n\n\nMicroservices Evolution – A Case Study\n[ 184 ]\nIn this case, since the old and the new applications are web-based applications \nrunning in a web browser window, users will get a seamless experience. SSO has to \nbe implemented between the old and the new user interfaces.\nThe third approach is to integrate the existing legacy user interface directly to the \nnew microservices backend, as shown in the next diagram:\nIn this case, the new microservices are built as headless applications with no \npresentation layer. This could be challenging, as it may require many changes  \nin the old user interface such as introducing service calls, data model conversions, \nand so on.\nAnother issue in the last two cases is how to handle the authentication of resources \nand services.\nSession handling and security\nAssume that the new services are written based on Spring Security with a  \ntoken-based authorization strategy, whereas the old application uses a  \ncustom-built authentication with its local identity store.\n\n\nChapter 4\n[ 185 ]\nThe following diagram shows how to integrate between the old and the new \nservices:\nThe simplest approach, as shown in the preceding diagram, is to build a new  \nidentity store with an authentication service as a new microservice using Spring \nSecurity. This will be used for all our future resource and service protections,  \nfor all microservices.\nThe existing user interface application authenticates itself against the new \nauthentication service, and secures a token. This token will be passed to the new user \ninterface or new microservice. In both cases, the user interface or microservice will \nmake a call to the authentication service to validate the given token. If the token is \nvalid, then the UI or microservice accepts the call.\nThe catch here is that the legacy identity store has to be synchronized with the  \nnew one.\n\n\nMicroservices Evolution – A Case Study\n[ 186 ]\nTest strategy\nOne important question to answer from a testing point of view is how can we ensure \nthat all functions work in the same way as before the migration?\nIntegration test cases should be written for the services that are getting migrated \nbefore the migration or refactoring. This ensures that once migrated, we get the same \nexpected result, and the behavior of the system remains the same. An automated \nregression test pack has to be in place, and has to be executed every time we make a \nchange in the new or old system.\nIn the following diagram, for each service we need one test against the EJB endpoint, \nand another one against the microservices endpoint:\n\n\nChapter 4\n[ 187 ]\nBuilding ecosystem capabilities\nBefore we embark on actual migration, we have to build all of the microservice's \ncapabilities mentioned under the capability model, as documented in Chapter 3,  \nApplying Microservices Concepts. These are the prerequisites for developing \nmicroservices-based systems.\nIn addition to these capabilities, certain application functions are also required \nto be built upfront such as reference data, security and SSO, and Customer and \nNotification. A data warehouse or a data lake is also required as a prerequisite. An \neffective approach is to build these capabilities in an incremental fashion, delaying \ndevelopment until it is really required.\nMigrate modules only if required\nIn the previous chapters, we have examined approaches and steps for transforming \nfrom a monolithic application to microservices. It is important to understand that it is \nnot necessary to migrate all modules to the new microservices architecture, unless it \nis really required. A major reason is that these migrations incur cost.\nWe will review a few such scenarios here. BrownField has already taken a decision \nto use an external revenue management system in place of the PSS revenue \nmanagement function. BrownField is also in the process of centralizing their \naccounting functions, and therefore, need not migrate the accounting function from \nthe legacy system. Migration of CRM does not add much value at this point to the \nbusiness. Therefore, it is decided to keep the CRM in the legacy system itself. The \nbusiness has plans to move to a SaaS-based CRM solution as part of their cloud \nstrategy. Also note that stalling the migration halfway through could seriously \nimpact the complexity of the system.\n\n\nMicroservices Evolution – A Case Study\n[ 188 ]\nTarget architecture\nThe architecture blueprint shown in the following diagram consolidates earlier \ndiscussions into an architectural view. Each block in the diagram represents a \nmicroservice. The shaded boxes are core microservices, and the others are supporting \nmicroservices. The diagram also shows the internal capabilities of each microservice. \nUser management is moved under security in the target architecture:\n\n\nChapter 4\n[ 189 ]\nEach service has its own architecture, typically consisting of a presentation layer, \none or more service endpoints, business logic, business rules, and database. As \nwe can see, we use different selections of databases that are more suitable for each \nmicroservice. Each one is autonomous with minimal orchestration between the \nservices. Most of the services interact with each other using the service endpoints.\nInternal layering of microservices\nIn this section, we will further explore the internal structure of microservices. There \nis no standard to be followed for the internal architecture of a microservice. The rule \nof thumb is to abstract realizations behind simple service endpoints.\nA typical structure would look like the one shown in the following diagram:\nThe UI accesses REST services through a service gateway. The API gateway may be \none per microservice or one for many microservices—it depends on what we want \nto do with the API gateway. There could be one or more rest endpoints exposed by \nmicroservices. These endpoints, in turn, connect to one of the business components \nwithin the service. Business components then execute all the business functions with \nthe help of domain entities. A repository component is used for interacting with the \nbackend data store.\n\n\nMicroservices Evolution – A Case Study\n[ 190 ]\nOrchestrating microservices\nThe logic of the booking orchestration and the execution of rules sits within the \nBooking service. The brain is still inside the Booking service in the form of one or \nmore booking business components. Internally, business components orchestrate \nprivate APIs exposed by other business components or even external services:\nAs shown in the preceding diagram, the booking service internally calls to update \nthe inventory of its own component other than calling the Fare service.\nIs there any orchestration engine required for this activity? It depends on the \nrequirements. In complex scenarios, we may have to do a number of things in \nparallel. For example, creating a booking internally applies a number of booking \nrules, it validates the fare, and it validates the inventory before creating a \nbooking. We may want to execute them in parallel. In such cases, we may use Java \nconcurrency APIs or reactive Java libraries.\nIn extremely complex situations, we may opt for an integration framework such as \nSpring Integration or Apache Camel in embedded mode.\n\n\nChapter 4\n[ 191 ]\nIntegration with other systems\nIn the microservices world, we use an API gateway or a reliable message bus for \nintegrating with other non-microservices.\nLet us assume that there is another system in BrownField that needs booking data. \nUnfortunately, the system is not capable of subscribing to the booking events that \nthe Booking microservice publishes. In such cases, an Enterprise Application \nintegration (EAI) solution could be employed, which listens to our booking events, \nand then uses a native adaptor to update the database.\nManaging shared libraries\nCertain business logic is used in more than one microservice. Search and \nReservation, in this case, use inventory rules. In such cases, these shared libraries will \nbe duplicated in both the microservices.\nHandling exceptions\nExamine the booking scenario to understand the different exception handling \napproaches. In the following service sequence diagram, there are three lines marked \nwith a cross mark. These are the potential areas where exceptions could occur:\n\n\nMicroservices Evolution – A Case Study\n[ 192 ]\nThere is a synchronous communication between Booking and Fare. What if the Fare \nservice is not available? If the Fare service is not available, throwing an error back \nto the user may cause revenue loss. An alternate thought is to trust the fare which \ncomes as part of the incoming request. When we serve search, the search results will \nhave the fare as well. When the user selects a flight and submits, the request will \nhave the selected fare. In case the Fare service is not available, we trust the incoming \nrequest, and accept the Booking. We will use a circuit breaker and a fallback service \nwhich simply creates the booking with a special status, and queues the booking for \nmanual action or a system retry.\nWhat if creating the booking fails? If creating a booking fails unexpectedly, a better \noption is to throw a message back to the user. We could try alternative options, but \nthat could increase the overall complexity of the system. The same is applicable for \ninventory updates.\nIn the case of creating a booking and updating the inventory, we avoid a situation \nwhere a booking is created, and an inventory update somehow fails. As the \ninventory is critical, it is better to have both, create booking and update inventory, \nto be in a local transaction. This is possible as both components are under the same \nsubsystem.\nIf we consider the Check-in scenario, Check-in sends an event to Boarding and \nBooking as shown in the next diagram:\n\n\nChapter 4\n[ 193 ]\nConsider a scenario where the Check-in services fail immediately after the Check-in \nComplete event is sent out. The other consumers processed this event, but the actual \ncheck-in is rolled back. This is because we are not using a two-phase commit. In this \ncase, we need a mechanism for reverting that event. This could be done by catching \nthe exception, and sending another Check-in Cancelled event.\nIn this case, note that to minimize the use of compensating transactions, sending the \nCheck-in event is moved towards the end of the Check-in transaction. This reduces \nthe chance of failure after sending out the event.\n\n\nMicroservices Evolution – A Case Study\n[ 194 ]\nOn the other hand, what if the check-in is successful, but sending the event failed? \nWe could think of two approaches. The first approach would be to invoke a fallback \nservice to store it locally, and then use another sweep-and-scan program to send \nthe event at a later time. It could even retry multiple times. This could add more \ncomplexity and may not be efficient in all cases. An alternate approach is to throw \nthe exception back to the user so that the user can retry. However, this might not \nalways be good from a customer engagement standpoint. On the other hand, the \nearlier option is better for the system's health. A trade-off analysis is required to find \nout the best solution for the given situation.\nTarget implementation view\nThe next diagram represents the implementation view of the BrownField PSS \nmicroservices system:\nAs shown in the preceding diagram, we are implementing four microservices as an \nexample: Search, Fare, Booking, and Check-in. In order to test the application, there \nis a website application developed using Spring MVC with Thymeleaf templates. \nThe asynchronous messaging is implemented with the help of RabbitMQ. In this \nsample implementation, the default H2 database is used as the in-memory store for \ndemonstration purposes.\nThe code in this section demonstrates all the capabilities highlighted in the Reviewing \nthe microservices capability model section of this chapter.\n\n\nChapter 4\n[ 195 ]\nImplementation projects\nThe basic implementation of the BrownField Airline's PSS microservices system has \nfive core projects as summarized in the following table. The table also shows the port \nrange used for these projects to ensure consistency throughout the book:\nMicroservice \nProjects\nPort Range\nBook microservice\nchapter4.book\n8060-8069\nCheck-in microservice\nchapter4.checkin\n8070-8079\nFare microservice\nchapter4.fares\n8080-8089\nSearch microservice\nchapter4.search\n8090-8099\nWebsite\nchapter4.website\n8001\nThe website is the UI application for testing the PSS microservices.\nAll microservice projects in this example follow the same pattern for package \nstructure as shown in the following screenshot:\n\n\nMicroservices Evolution – A Case Study\n[ 196 ]\nThe different packages and their purposes are explained as follows:\n•\t\nThe root folder (com.brownfield.pss.book) contains the default Spring \nBoot application.\n•\t\nThe component package hosts all the service components where the business \nlogic is implemented.\n•\t\nThe controller package hosts the REST endpoints and the messaging \nendpoints. Controller classes internally utilize the component classes for \nexecution.\n•\t\nThe entity package contains the JPA entity classes for mapping to the \ndatabase tables.\n•\t\nRepository classes are packaged inside the repository package, and are \nbased on Spring Data JPA.\nRunning and testing the project\nFollow the steps listed next to build and test the microservices developed in this \nchapter:\n1.\t Build each of the projects using Maven. Ensure that the test flag is switched \noff. The test programs assume other dependent services are up and running. \nIt fails if the dependent services are not available. In our example, Booking \nand Fare have direct dependencies. We will learn how to circumvent this \ndependency in Chapter 7, Logging and Monitoring Microservices:\nmvn -Dmaven.test.skip=true install\n2.\t Run the RabbitMQ server:\nrabbitmq_server-3.5.6/sbin$ ./rabbitmq-server\n3.\t Run the following commands in separate terminal windows:\njava -jar target/fares-1.0.jar\njava -jar target/search-1.0.jar\njava -jar target/checkin-1.0.jar\njava -jar target/book-1.0.jar\njava -jar target/website-1.0.jar\n\n\nChapter 4\n[ 197 ]\n4.\t The website project has a CommandLineRunner, which executes all the test \ncases at startup. Once all the services are successfully started, open http://\nlocalhost:8001 in a browser.\n5.\t The browser asks for basic security credentials. Use guest or guest123 as \nthe credentials. This example only shows the website security with a basic \nauthentication mechanism. As explained in Chapter 2, Building Microservices \nwith Spring Boot, service-level security can be achieved using OAuth2.\n6.\t Entering the correct security credentials displays the following screen. This is \nthe home screen of our BrownField PSS application:\n7.\t The SUBMIT button invokes the Search microservice to fetch the available \nflights that meet the conditions mentioned on the screen. A few flights are \npre-populated at the startup of the Search microservice. Edit the Search \nmicroservice code to feed in additional flights, if required.\n\n\nMicroservices Evolution – A Case Study\n[ 198 ]\n8.\t The output screen with a list of flights is shown in the next screenshot.  \nThe Book link will take us to the booking screen for the selected flight:\n9.\t The following screenshot shows the booking screen. The user can enter the \npassenger details, and create a booking by clicking on the CONFIRM button. \nThis invokes the Booking microservice, and internally, the Fare service as \nwell. It also sends a message back to the Search microservice:\n\n\nChapter 4\n[ 199 ]\n10.\t If booking is successful, the next confirmation screen is displayed with a \nbooking reference number:\n11.\t Let us test the Check-in microservice. This can be done by clicking on \nCheckIn in the menu at the top of the screen. Use the booking reference \nnumber obtained in the previous step to test Check-in. This is shown in the \nfollowing screenshot:\n\n\nMicroservices Evolution – A Case Study\n[ 200 ]\n12.\t Clicking on the SEARCH button in the previous screen invokes the Booking \nmicroservice, and retrieves the booking information. Click on the CheckIn \nlink to perform the check-in. This invokes the Check-in microservice:\n13.\t If check-in is successful, it displays the confirmation message, as shown in \nthe next screenshot, with a confirmation number. This is done by calling the \nCheck-in service internally. The Check-in service sends a message to Booking \nto update the check-in status:\n\n\nChapter 4\n[ 201 ]\nSummary\nIn this chapter, we implemented and tested the BrownField PSS microservice with \nbasic Spring Boot capabilities. We learned how to approach a real use case with a \nmicroservices architecture. \nWe examined the various stages of a real-world evolution towards microservices \nfrom a monolithic application. We also evaluated the pros and cons of multiple \napproaches, and the obstacles encountered when migrating a monolithic application. \nFinally, we explained the end-to-end microservices design for the use case \nthat we examined. Design and implementation of a fully-fledged microservice \nimplementation was also validated.\nIn the next chapter, we will see how the Spring Cloud project helps us to transform \nthe developed BrownField PSS microservices to an Internet-scale deployment.\n\n\n[ 203 ]\nScaling Microservices with \nSpring Cloud\nIn order to manage Internet-scale microservices, one requires more capabilities than \nwhat are offered by the Spring Boot framework. The Spring Cloud project has a suite \nof purpose-built components to achieve these additional capabilities effortlessly.\nThis chapter will provide a deep insight into the various components of the Spring \nCloud project such as Eureka, Zuul, Ribbon, and Spring Config by positioning \nthem against the microservices capability model discussed in Chapter 3, Applying \nMicroservices Concepts. It will demonstrate how the Spring Cloud components help  \nto scale the BrownField Airline's PSS microservices system, developed in the \nprevious chapter.\nBy the end of this chapter, you will learn about the following:\n•\t\nThe Spring Config server for externalizing configuration\n•\t\nThe Eureka server for service registration and discovery\n•\t\nThe relevance of Zuul as a service proxy and gateway\n•\t\nThe implementation of automatic microservice registration and  \nservice discovery\n•\t\nSpring Cloud messaging for asynchronous microservice composition\n\n\nScaling Microservices with Spring Cloud\n[ 204 ]\nReviewing microservices capabilities\nThe examples in this chapter explore the following microservices capabilities from \nthe microservices capability model discussed in Chapter 3, Applying Microservices \nConcepts:\n•\t\nSoftware Defined Load Balancer\n•\t\nService Registry\n•\t\nConfiguration Service\n•\t\nReliable Cloud Messaging\n•\t\nAPI Gateways\nReviewing BrownField's PSS \nimplementation\nIn Chapter 4, Microservices Evolution – A Case Study, we designed and developed a \nmicroservice-based PSS system for BrownField Airlines using the Spring framework \nand Spring Boot. The implementation is satisfactory from the development point of \nview, and it serves the purpose for low volume transactions. However, this is not \ngood enough for deploying large, enterprise-scale deployments with hundreds or \neven thousands of microservices.\n\n\nChapter 5\n[ 205 ]\nIn Chapter 4, Microservices Evolution – A Case Study, we developed four microservices: \nSearch, Booking, Fares, and Check-in. We also developed a website to test the \nmicroservices.\nWe have accomplished the following items in our microservice implementation  \nso far:\n•\t\nEach microservice exposes a set of REST/JSON endpoints for accessing \nbusiness capabilities\n•\t\nEach microservice implements certain business functions using the  \nSpring framework.\n•\t\nEach microservice stores its own persistent data using H2, an in-memory \ndatabase\n•\t\nMicroservices are built with Spring Boot, which has an embedded Tomcat \nserver as the HTTP listener\n•\t\nRabbitMQ is used as an external messaging service. Search, Booking, and \nCheck-in interact with each other through asynchronous messaging\n•\t\nSwagger is integrated with all microservices for documenting the REST APIs.\n•\t\nAn OAuth2-based security mechanism is developed to protect the \nmicroservices\nWhat is Spring Cloud?\nThe Spring Cloud project is an umbrella project from the Spring team that implements \na set of common patterns required by distributed systems, as a set of easy-to-use Java \nSpring libraries. Despite its name, Spring Cloud by itself is not a cloud solution. Rather, \nit provides a number of capabilities that are essential when developing applications \ntargeting cloud deployments that adhere to the Twelve-Factor application principles. \nBy using Spring Cloud, developers just need to focus on building business capabilities \nusing Spring Boot, and leverage the distributed, fault-tolerant, and  \nself-healing capabilities available out of the box from Spring Cloud.\nThe Spring Cloud solutions are agnostic to the deployment environment, and can \nbe developed and deployed in a desktop PC or in an elastic cloud. The cloud-ready \nsolutions that are developed using Spring Cloud are also agnostic and portable across \nmany cloud providers such as Cloud Foundry, AWS, Heroku, and so on. When not \nusing Spring Cloud, developers will end up using services natively provided by the \ncloud vendors, resulting in deep coupling with the PaaS providers. An alternate option \nfor developers is to write quite a lot of boilerplate code to build these services. Spring \nCloud also provides simple, easy-to-use Spring-friendly APIs, which abstract the cloud \nprovider's service APIs such as those APIs coming with AWS Notification Service.\n\n\nScaling Microservices with Spring Cloud\n[ 206 ]\nBuilt on Spring's \"convention over configuration\" approach, Spring Cloud defaults \nall configurations, and helps the developers get off to a quick start. Spring Cloud \nalso hides the complexities, and provides simple declarative configurations to build \nsystems. The smaller footprints of the Spring Cloud components make it developer \nfriendly, and also make it easy to develop cloud-native applications.\nSpring Cloud offers many choices of solutions for developers based on their \nrequirements. For example, the service registry can be implemented using popular \noptions such as Eureka, ZooKeeper, or Consul. The components of Spring Cloud  \nare fairly decoupled, hence, developers get the flexibility to pick and choose what  \nis required.\nWhat is the difference between Spring Cloud and Cloud Foundry?\nSpring Cloud is a developer kit for developing Internet-scale Spring Boot \napplications, whereas Cloud Foundry is an open-source Platform as a \nService for building, deploying, and scaling applications.\nSpring Cloud releases\nThe Spring Cloud project is an overarching Spring project that includes  \na combination of different components. The versions of these components  \nare defined in the spring-cloud-starter-parent BOM.\nIn this book, we are relying on the Brixton.RELEASE version of the Spring Cloud:\n  <dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-dependencies</artifactId>\n    <version>Brixton.RELEASE</version>\n    <type>pom</type>\n    <scope>import</scope>\n  </dependency>\nThe spring-cloud-starter-parent defines different versions of its subcomponents \nas follows:\n<spring-cloud-aws.version>1.1.0.RELEASE</spring-cloud-aws.version>\n<spring-cloud-bus.version>1.1.0.RELEASE</spring-cloud-bus.version>\n<spring-cloud-cloudfoundry.version>1.0.0.RELEASE</spring-cloud-\ncloudfoundry.version>\n<spring-cloud-commons.version>1.1.0.RELEASE</spring-cloud-commons.\nversion>\n<spring-cloud-config.version>1.1.0.RELEASE</spring-cloud-config.\nversion>\n<spring-cloud-netflix.version>1.1.0.RELEASE</spring-cloud-netflix.\nversion>\n",
      "page_number": 180
    },
    {
      "number": 5,
      "title": "[ 205 ]",
      "start_page": 232,
      "end_page": 287,
      "detection_method": "regex_chapter",
      "content": "Chapter 5\n[ 207 ]\n<spring-cloud-security.version>1.1.0.RELEASE</spring-cloud-security.\nversion>\n<spring-cloud-cluster.version>1.0.0.RELEASE</spring-cloud-cluster.\nversion>\n<spring-cloud-consul.version>1.0.0.RELEASE</spring-cloud-consul.\nversion>\n<spring-cloud-sleuth.version>1.0.0.RELEASE</spring-cloud-sleuth.\nversion>\n<spring-cloud-stream.version>1.0.0.RELEASE</spring-cloud-stream.\nversion>\n<spring-cloud-zookeeper.version>1.0.0.RELEASE </spring-cloud-\nzookeeper.version>\nThe names of the Spring Cloud releases are in an alphabetic sequence, \nstarting with A, following the names of the London Tube stations. Angel \nwas the first release, and Brixton is the second release.\nComponents of Spring Cloud\nEach Spring Cloud component specifically addresses certain distributed system \ncapabilities. The grayed-out boxes at the bottom of the following diagram show the \ncapabilities, and the boxes placed on top of these capabilities showcase the Spring \nCloud subprojects addressing these capabilities:\n\n\nScaling Microservices with Spring Cloud\n[ 208 ]\nThe Spring Cloud capabilities are explained as follows:\n•\t\nDistributed configuration: Configuration properties are hard to manage when \nthere are many microservice instances running under different profiles such as \ndevelopment, test, production, and so on. It is, therefore, important to manage \nthem centrally, in a controlled way. The distributed configuration management \nmodule is to externalize and centralize microservice configuration parameters. \nSpring Cloud Config is an externalized configuration server with Git or SVN \nas the backing repository. Spring Cloud Bus provides support for propagating \nconfiguration changes to multiple subscribers, generally a microservice \ninstance. Alternately, ZooKeeper or HashiCorp's Consul can also be used  \nfor distributed configuration management.\n•\t\nRouting: Routing is an API gateway component, primarily used similar  \nto a reverse proxy that forwards requests from consumers to service \nproviders. The gateway component can also perform software-based  \nrouting and filtering. Zuul is a lightweight API gateway solution that  \noffers fine-grained controls to developers for traffic shaping and request/\nresponse transformations.\n•\t\nLoad balancing: The load balancer capability requires a software-defined \nload balancer module which can route requests to available servers using a \nvariety of load balancing algorithms. Ribbon is a Spring Cloud subproject \nwhich supports this capability. Ribbon can work as a standalone component, \nor integrate and work seamlessly with Zuul for traffic routing.\n•\t\nService registration and discovery: The service registration and discovery \nmodule enables services to programmatically register with a repository when \na service is available and ready to accept traffic. The microservices advertise \ntheir existence, and make them discoverable. The consumers can then look \nup the registry to get a view of the service availability and the endpoint \nlocations. The registry, in many cases, is more or less a dump. But the \ncomponents around the registry make the ecosystem intelligent. There are \nmany subprojects existing under Spring Cloud which support registry and \ndiscovery capability. Eureka, ZooKeeper, and Consul are three subprojects \nimplementing the registry capability.\n•\t\nService-to-service calls: The Spring Cloud Feign subproject under Spring \nCloud offers a declarative approach for making RESTful service-to-service \ncalls in a synchronous way. The declarative approach allows applications \nto work with POJO (Plain Old Java Object) interfaces instead of low-level \nHTTP client APIs. Feign internally uses reactive libraries for communication.\n\n\nChapter 5\n[ 209 ]\n•\t\nCircuit breaker: The circuit breaker subproject implements the circuit \nbreaker pattern. The circuit breaker breaks the circuit when it encounters \nfailures in the primary service by diverting traffic to another temporary \nfallback service. It also automatically reconnects back to the primary \nservice when the service is back to normal. It finally provides a monitoring \ndashboard for monitoring the service state changes. The Spring Cloud \nHystrix project and Hystrix Dashboard implement the circuit breaker  \nand the dashboard respectively.\n•\t\nGlobal locks, leadership election and cluster state: This capability is \nrequired for cluster management and coordination when dealing with \nlarge deployments. It also offers global locks for various purposes such as \nsequence generation. The Spring Cloud Cluster project implements these \ncapabilities using Redis, ZooKeeper, and Consul.\n•\t\nSecurity: Security capability is required for building security for cloud-\nnative distributed systems using externalized authorization providers such \nas OAuth2. The Spring Cloud Security project implements this capability \nusing customizable authorization and resource servers. It also offers SSO \ncapabilities, which are essential when dealing with many microservices.\n•\t\nBig data support: The big data support capability is a capability that is \nrequired for data services and data flows in connection with big data \nsolutions. The Spring Cloud Streams and the Spring Cloud Data Flow \nprojects implement these capabilities. The Spring Cloud Data Flow is  \nthe re-engineered version of Spring XD.\n•\t\nDistributed tracing: The distributed tracing capability helps to thread and \ncorrelate transitions that are spanned across multiple microservice instances. \nSpring Cloud Sleuth implements this by providing an abstraction on top of \nvarious distributed tracing mechanisms such as Zipkin and HTrace with the \nsupport of a 64-bit ID.\n•\t\nDistributed messaging: Spring Cloud Stream provides declarative \nmessaging integration on top of reliable messaging solutions such as Kafka, \nRedis, and RabbitMQ.\n•\t\nCloud support: Spring Cloud also provides a set of capabilities that offers \nvarious connectors, integration mechanisms, and abstraction on top of \ndifferent cloud providers such as the Cloud Foundry and AWS.\n\n\nScaling Microservices with Spring Cloud\n[ 210 ]\nSpring Cloud and Netflix OSS\nMany of the Spring Cloud components which are critical for microservices' \ndeployment came from the Netflix Open Source Software (Netflix OSS) center. \nNetflix is one of the pioneers and early adaptors in the microservices space. In \norder to manage large scale microservices, engineers at Netflix came up with a \nnumber of homegrown tools and techniques for managing their microservices. \nThese are fundamentally crafted to fill some of the software gaps recognized in \nthe AWS platform for managing Netflix services. Later, Netflix open-sourced these \ncomponents, and made them available under the Netflix OSS platform for public use. \nThese components are extensively used in production systems, and are battle-tested \nwith large scale microservice deployments at Netflix.\nSpring Cloud offers higher levels of abstraction for these Netflix OSS components, \nmaking it more Spring developer friendly. It also provides a declarative mechanism, \nwell-integrated and aligned with Spring Boot and the Spring framework.\nSetting up the environment for \nBrownField PSS\nIn this chapter, we will amend the BrownField PSS microservices developed in \nChapter 4, Microservices Evolution – A Case Study, using Spring Cloud capabilities.  \nWe will also examine how to make these services enterprise grade using Spring \nCloud components.\nSubsequent sections of this chapter will explore how to scale the microservices \ndeveloped in the previous chapter for cloud scale deployments, using some out-of-\nthe-box capabilities provided by the Spring Cloud project. The rest of this chapter \nwill explore Spring Cloud capabilities such as configuration using the Spring Config \nserver, Ribbon-based service load balancing, service discovery using Eureka, Zuul \nfor API gateway, and finally, Spring Cloud messaging for message-based service \ninteractions. We will demonstrate the capabilities by modifying the BrownField PSS \nmicroservices developed in Chapter 4, Microservices Evolution – A Case Study.\nIn order to prepare the environment for this chapter, import and rename \n(chapter4.* to chapter5.*) projects into a new STS workspace.\nThe full source code of this chapter is available under the Chapter 5 \nprojects in the code files.\n\n\nChapter 5\n[ 211 ]\nSpring Cloud Config\nThe Spring Cloud Config server is an externalized configuration server in which \napplications and services can deposit, access, and manage all runtime configuration \nproperties. The Spring Config server also supports version control of the \nconfiguration properties.\nIn the earlier examples with Spring Boot, all configuration parameters were read \nfrom a property file packaged inside the project, either application.properties or \napplication.yaml. This approach is good, since all properties are moved out of code \nto a property file. However, when microservices are moved from one environment \nto another, these properties need to undergo changes, which require an application \nre-build. This is violation of one of the Twelve-Factor application principles, which \nadvocate one-time build and moving of the binaries across environments.\nA better approach is to use the concept of profiles. Profiles, as discussed in Chapter \n2, Building Microservices with Spring Boot, is used for partitioning different properties \nfor different environments. The profile-specific configuration will be named \napplication-{profile}.properties. For example, application-development.\nproperties represents a property file targeted for the development environment.\nHowever, the disadvantage of this approach is that the configurations are statically \npackaged along with the application. Any changes in the configuration properties \nrequire the application to be rebuilt.\nThere are alternate ways to externalize the configuration properties from the \napplication deployment package. Configurable properties can also be read  \nfrom an external source in a number of ways:\n•\t\nFrom an external JNDI server using JNDI namespace (java:comp/env)\n•\t\nUsing the Java system properties (System.getProperties()) or using  \nthe –D command line option\n•\t\nUsing the PropertySource configuration:\n@PropertySource(\"file:${CONF_DIR}/application.properties\")\n  public class ApplicationConfig {\n}\n•\t\nUsing a command-line parameter pointing a file to an external location:\njava -jar myproject.jar --spring.config.location=\n\n\nScaling Microservices with Spring Cloud\n[ 212 ]\nJNDI operations are expensive, lack flexibility, have difficulties in replication, and \nare not version controlled. System.properties is not flexible enough for large-scale \ndeployments. The last two options rely on a local or a shared filesystem mounted  \non the server.\nFor large scale deployments, a simple yet powerful centralized configuration \nmanagement solution is required:\nAs shown in the preceding diagram, all microservices point to a central server to get \nthe required configuration parameters. The microservices then locally cache these \nparameters to improve performance. The Config server propagates the configuration \nstate changes to all subscribed microservices so that the local cache's state can be \nupdated with the latest changes. The Config server also uses profiles to resolve \nvalues specific to an environment.\nAs shown in the following screenshot, there are multiple options available under \nthe Spring Cloud project for building the configuration server. Config Server, \nZookeeper Configuration, and Consul Configuration are available as options. \nHowever, this chapter will only focus on the Spring Config server implementation:\n\n\nChapter 5\n[ 213 ]\nThe Spring Config server stores properties in a version-controlled repository such as \nGit or SVN. The Git repository can be local or remote. A highly available remote Git \nserver is preferred for large scale distributed microservice deployments.\nThe Spring Cloud Config server architecture is shown in the following diagram:\nAs shown in the preceding diagram, the Config client embedded in the Spring Boot \nmicroservices does a configuration lookup from a central configuration server using \na simple declarative mechanism, and stores properties into the Spring environment. \nThe configuration properties can be application-level configurations such as \ntrade limit per day, or infrastructure-related configurations such as server URLs, \ncredentials, and so on.\nUnlike Spring Boot, Spring Cloud uses a bootstrap context, which is a parent context \nof the main application. Bootstrap context is responsible for loading configuration \nproperties from the Config server. The bootstrap context looks for bootstrap.yaml \nor bootstrap.properties for loading initial configuration properties. To make this \nwork in a Spring Boot application, rename the application.* file to bootstrap.*.\n\n\nScaling Microservices with Spring Cloud\n[ 214 ]\nWhat's next?\nThe next few sections demonstrate how to use the Config server in a real-world \nscenario. In order to do this, we will modify our search microservice (chapter5.\nsearch) to use the Config server. The following diagram depicts the scenario:\n \nIn this example, the Search service will read the Config server at startup by passing \nthe service name. In this case, the service name of the search service will be search-\nservice. The properties configured for the search-service include the RabbitMQ \nproperties as well as a custom property.\nThe full source code of this section is available under the \nchapter5.configserver project in the code files.\nSetting up the Config server\nThe following steps need to be followed to create a new Config server using STS:\n1.\t Create a new Spring Starter Project, and select Config Server and Actuator \nas shown in the following diagram:\n\n\nChapter 5\n[ 215 ]\n2.\t Set up a Git repository. This can be done by pointing to a remote Git \nconfiguration repository like the one at https://github.com/spring-cloud-\nsamples/config-repo. This URL is an indicative one, a Git repository used \nby the Spring Cloud examples. We will have to use our own Git repository \ninstead.\n3.\t Alternately, a local filesystem-based Git repository can be used. In a real \nproduction scenario, an external Git is recommended. The Config server in \nthis chapter will use a local filesystem-based Git repository for demonstration \npurposes.\n4.\t Enter the commands listed next to set up a local Git repository:\n$ cd $HOME\n$ mkdir config-repo\n$ cd config-repo\n$ git init .\n$ echo message : helloworld > application.properties\n$ git add -A .\n$ git commit -m \"Added sample application.properties\"\nThis code snippet creates a new Git repository on the local filesystem. A \nproperty file named application.properties with a message property  \nand value helloworld is also created.\n\n\nScaling Microservices with Spring Cloud\n[ 216 ]\nThe file application.properties is created for demonstration purposes.  \nWe will change this in the subsequent sections.\n5.\t The next step is to change the configuration in the Config server to use the \nGit repository created in the previous step. In order to do this, rename the  \nfile application.properties to bootstrap.properties:\n6.\t Edit the contents of the new bootstrap.properties file to match  \nthe following:\nserver.port=8888\nspring.cloud.config.server.git.uri: file://${user.home}/config-\nrepo\nPort 8888 is the default port for the Config server. Even without configuring \nserver.port, the Config server should bind to 8888. In the Windows \nenvironment, an extra / is required in the file URL.\n7.\t Optionally, rename the default package of the auto-generated Application.\njava from com.example to com.brownfield.configserver. Add  \n@EnableConfigServer in Application.java:\n@EnableConfigServer\n@SpringBootApplication\npublic class ConfigserverApplication {\n8.\t Run the Config server by right-clicking on the project, and running it as a \nSpring Boot app.\n9.\t Visit http://localhost:8888/env to see whether the server is running.  \nIf everything is fine, this will list all environment configurations. Note that  \n/env is an actuator endpoint.\n\n\nChapter 5\n[ 217 ]\n10.\t Check http://localhost:8888/application/default/master to see \nthe properties specific to application.properties, which were added \nin the earlier step. The browser will display the properties configured in \napplication.properties. The browser should display contents similar to \nthe following:\n{\"name\":\"application\",\"profiles\":[\"default\"],\"label\":\"master\",\"ver\nsion\":\"6046fd2ff4fa09d3843767660d963866ffcc7d28\",\"propertySources\"\n:[{\"name\":\"file:///Users/rvlabs /config-repo /application.properti\nes\",\"source\":{\"message\":\"helloworld\"}}]}\nUnderstanding the Config server URL\nIn the previous section, we used http://localhost:8888/application/default/\nmaster to explore the properties. How do we interpret this URL?\nThe first element in the URL is the application name. In the given example, the \napplication name should be application. The application name is a logical \nname given to the application, using the spring.application.name property in \nbootstrap.properties of the Spring Boot application. Each application must \nhave a unique name. The Config server will use the name to resolve and pick up \nappropriate properties from the Config server repository. The application name is \nalso sometimes referred to as service ID. If there is an application with the name \nmyapp, then there should be a myapp.properties in the configuration repository  \nto store all the properties related to that application.\nThe second part of the URL represents the profile. There can be more than one  \nprofile configured within the repository for an application. The profiles can be \nused in various scenarios. The two common scenarios are segregating different \nenvironments such as Dev, Test, Stage, Prod, and the like, or segregating server \nconfigurations such as Primary, Secondary, and so on. The first one represents \ndifferent environments of an application, whereas the second one represents \ndifferent servers where an application is deployed.\nThe profile names are logical names that will be used for matching the file name in \nthe repository. The default profile is named default. To configure properties for \ndifferent environments, we have to configure different files as given in the following \nexample. In this example, the first file is for the development environment whereas \nthe second is for the production environment:\napplication-development.properties\napplication-production.properties\n\n\nScaling Microservices with Spring Cloud\n[ 218 ]\nThese are accessible using the following URLs respectively:\n•\t\nhttp://localhost:8888/application/development\n•\t\nhttp://localhost:8888/application/production\nThe last part of the URL is the label, and is named master by default. The label is an \noptional Git label that can be used, if required.\nIn short, the URL is based on the following pattern: http://localhost:8888/\n{name}/{profile}/{label}.\nThe configuration can also be accessed by ignoring the profile. In the preceding \nexample, all the following three URLs point to the same configuration:\n•\t\nhttp://localhost:8888/application/default\n•\t\nhttp://localhost:8888/application/master\n•\t\nhttp://localhost:8888/application/default/master\nThere is an option to have different Git repositories for different profiles. This  \nmakes sense for production systems, since the access to different repositories  \ncould be different.\nAccessing the Config Server from clients\nIn the previous section, a Config server is set up and accessed using a web browser. \nIn this section, the Search microservice will be modified to use the Config server.  \nThe Search microservice will act as a Config client.\nFollow these steps to use the Config server instead of reading properties from the \napplication.properties file:\n1.\t Add the Spring Cloud Config dependency and the actuator (if the actuator \nis not already in place) to the pom.xml file. The actuator is mandatory for \nrefreshing the configuration properties:\n  <dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-config</artifactId>\n  </dependency>\n2.\t Since we are modifying the Spring Boot Search microservice from the earlier \nchapter, we will have to add the following to include the Spring Cloud \ndependencies. This is not required if the project is created from scratch:\n  <dependencyManagement>\n    <dependencies>\n      <dependency>\n\n\nChapter 5\n[ 219 ]\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-dependencies</artifactId>\n        <version>Brixton.RELEASE</version>\n        <type>pom</type>\n        <scope>import</scope>\n      </dependency>\n    </dependencies>\n  </dependencyManagement>\n3.\t The next screenshot shows the Cloud starter library selection screen. If the \napplication is built from the ground up, select the libraries as shown in the \nfollowing screenshot:\n \n4.\t Rename application.properties to bootstrap.properties, and add an \napplication name and a configuration server URL. The configuration server \nURL is not mandatory if the Config server is running on the default port \n(8888) on the local host:\nThe new bootstrap.properties file will look as follows:\nspring.application.name=search-service \nspring.cloud.config.uri=http://localhost:8888\nserver.port=8090\n\n\nScaling Microservices with Spring Cloud\n[ 220 ]\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\nsearch-service is a logical name given to the Search microservice. This will \nbe treated as service ID. The Config server will look for search-service.\nproperties in the repository to resolve the properties.\n5.\t Create a new configuration file for search-service. Create a new  \nsearch-service.properties under the config-repo folder where the Git \nrepository is created. Note that search-service is the service ID given to the \nSearch microservice in the bootstrap.properties file. Move service-specific \nproperties from bootstrap.properties to the new search-service.\nproperties file. The following properties will be removed from bootstrap.\nproperties, and added to search-service.properties:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\n6.\t In order to demonstrate the centralized configuration of properties and \npropagation of changes, add a new application-specific property to the \nproperty file. We will add originairports.shutdown to temporarily take \nout an airport from the search. Users will not get any flights when searching \nfor an airport mentioned in the shutdown list:\noriginairports.shutdown=SEA\nIn this example, we will not return any flights when searching with SEA  \nas origin.\n7.\t Commit this new file into the Git repository by executing the following \ncommands:\ngit add –A .\ngit commit –m \"adding new configuration\"  \n8.\t The final search-service.properties file should look as follows:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\noriginairports.shutdown:SEA\n\n\nChapter 5\n[ 221 ]\n9.\t The chapter5.search project's bootstrap.properties should look like  \nthe following:\nspring.application.name=search-service\nserver.port=8090\nspring.cloud.config.uri=http://localhost:8888\n10.\t Modify the Search microservice code to use the configured parameter, \noriginairports.shutdown. A RefreshScope annotation has to be added at \nthe class level to allow properties to be refreshed when there is a change. In \nthis case, we are adding a refresh scope to the SearchRestController class:\n@RefreshScope\n11.\t Add the following instance variable as a place holder for the new property \nthat is just added in the Config server. The property names in the search-\nservice.properties file must match:\n  @Value(\"${originairports.shutdown}\")\n  private String originAirportShutdownList;\n12.\t Change the application code to use this property. This is done by modifying \nthe search method as follows:\n  @RequestMapping(value=\"/get\", method =  \n    RequestMethod.POST)\n  List<Flight> search(@RequestBody SearchQuery query){\n    logger.info(\"Input : \"+ query);\n  if(Arrays.asList(originAirportShutdownList.split(\",\")) \n    .contains(query.getOrigin())){\n    logger.info(\"The origin airport is in shutdown state\");\n    return new ArrayList<Flight>();\n  }\n  return searchComponent.search(query);\n  }\nThe search method is modified to read the parameter \noriginAirportShutdownList and see whether the requested origin is in the \nshutdown list. If there is a match, then instead of proceeding with the actual \nsearch, the search method will return an empty flight list.\n13.\t Start the Config server. Then start the Search microservice. Make sure that \nthe RabbitMQ server is running.\n14.\t Modify the chapter5.website project to match the bootstrap.properties \ncontent as follows to utilize the Config server:\nspring.application.name=test-client\nserver.port=8001\nspring.cloud.config.uri=http://localhost:8888\n\n\nScaling Microservices with Spring Cloud\n[ 222 ]\n15.\t Change the run method of CommandLineRunner in Application.java to \nquery SEA as the origin airport:\nSearchQuery = new SearchQuery(\"SEA\",\"SFO\",\"22-JAN-16\");\n16.\t Run the chapter5.website project. The CommandLineRunner will now \nreturn an empty flight list. The following message will be printed in  \nthe server:\nThe origin airport is in shutdown state\nHandling configuration changes\nThis section will demonstrate how to propagate configuration properties when there \nis a change:\n1.\t Change the property in the search-service.properties file to the following:\noriginairports.shutdown:NYC\nCommit the change in the Git repository. Refresh the Config server URL \n(http://localhost:8888/search-service/default) for this service and \nsee whether the property change is reflected. If everything is fine, we will see \nthe property change. The preceding request will force the Config server to \nread the property file again from the repository.\n2.\t Rerun the website project again, and observe the CommandLineRunner \nexecution. Note that in this case, we are not restarting the Search \nmicroservice nor the Config server. The service returns an empty  \nflight list as earlier, and still complains as follows:\nThe origin airport is in shutdown state\nThis means the change is not reflected in the Search service, and the service is \nstill working with an old copy of the configuration properties.\n3.\t In order to force reloading of the configuration properties, call the /refresh \nendpoint of the Search microservice. This is actually the actuator's refresh \nendpoint. The following command will send an empty POST to the /refresh \nendpoint:\ncurl –d {} localhost:8090/refresh\n4.\t Rerun the website project, and observe the CommandLineRunner execution. \nThis should return the list of flights that we have requested from SEA. Note \nthat the website project may fail if the Booking service is not up and running.\nThe /refresh endpoint will refresh the locally cached configuration \nproperties, and reload fresh values from the Config server.\n\n\nChapter 5\n[ 223 ]\nSpring Cloud Bus for propagating \nconfiguration changes\nWith the preceding approach, configuration parameters can be changed without \nrestarting the microservices. This is good when there are only one or two instances \nof the services running. What happens if there are many instances? For example, if \nthere are five instances, then we have to hit /refresh against each service instance. \nThis is definitely a cumbersome activity:\nThe Spring Cloud Bus provides a mechanism to refresh configurations across \nmultiple instances without knowing how many instances there are, or their locations. \nThis is particularly handy when there are many service instances of a microservice \nrunning or when there are many microservices of different types running. This is \ndone by connecting all service instances through a single message broker. Each \ninstance subscribes for change events, and refreshes its local configuration when \nrequired. This refresh is triggered by making a call to any one instance by hitting the \n/bus/refresh endpoint, which then propagates the changes through the cloud bus \nand the common message broker.\nIn this example, RabbitMQ is used as the AMQP message broker. Implement this by \nfollowing the steps documented as follows:\n1.\t Add a new dependency in the chapter5.search project's pom.xml file to \nintroduce the Cloud Bus dependency:\n<dependency>\n   <groupId>org.springframework.cloud</groupId>\n   <artifactId>spring-cloud-starter-bus-amqp</artifactId>\n</dependency>\n\n\nScaling Microservices with Spring Cloud\n[ 224 ]\n2.\t The Search microservice also needs connectivity to the RabbitMQ, but this is \nalready provided in search-service.properties.\n3.\t Rebuild and restart the Search microservice. In this case, we will run two \ninstances of the Search microservice from a command line, as follows:\njava -jar -Dserver.port=8090  search-1.0.jar \njava -jar -Dserver.port=8091  search-1.0.jar\nThe two instances of the Search service will be now running, one on port \n8090 and another one on 8091.\n4.\t Rerun the website project. This is just to make sure that everything is \nworking. The Search service should return one flight at this point.\n5.\t Now, update search-service.properties with the following value,  \nand commit to Git:\noriginairports.shutdown:SEA\n6.\t Run the following command to /bus/refresh. Note that we are running  \na new bus endpoint against one of the instances, 8090 in this case:\ncurl –d {} localhost:8090/bus/refresh\n7.\t Immediately, we will see the following message for both instances:\nReceived remote refresh request. Keys refreshed [originairports.\nshutdown]\nThe bus endpoint sends a message to the message broker internally, which is \neventually consumed by all instances, reloading their property files. Changes \ncan also be applied to a specific application by specifying the application \nname like so:\n/bus/refresh?destination=search-service:**\nWe can also refresh specific properties by setting the property name as a parameter.\nSetting up high availability for the Config \nserver\nThe previous sections explored how to set up the Config server, allowing real-time \nrefresh of configuration properties. However, the Config server is a single point of \nfailure in this architecture.\nThere are three single points of failure in the default architecture that was established \nin the previous section. One of them is the availability of the Config server itself, the \nsecond one is the Git repository, and the third one is the RabbitMQ server.\n\n\nChapter 5\n[ 225 ]\nThe following diagram shows a high availability architecture for the Config server:\nThe architecture mechanisms and rationale are explained as follows:\nThe Config server requires high availability, since the services won't be able to \nbootstrap if the Config server is not available. Hence, redundant Config servers \nare required for high availability. However, the applications can continue to run \nif the Config server is unavailable after the services are bootstrapped. In this case, \nservices will run with the last known configuration state. Hence, the Config server \navailability is not at the same critical level as the microservices availability.\nIn order to make the Config server highly available, we need multiple instances \nof the Config servers. Since the Config server is a stateless HTTP service, multiple \ninstances of configuration servers can be run in parallel. Based on the load on the \nconfiguration server, a number of instances have to be adjusted. The bootstrap.\nproperties file is not capable of handling more than one server address. Hence, \nmultiple configuration servers should be configured to run behind a load balancer or \nbehind a local DNS with failover and fallback capabilities. The load balancer or DNS \nserver URL will be configured in the microservices' bootstrap.properties file. \nThis is with the assumption that the DNS or the load balancer is highly available  \nand capable of handling failovers.\nIn a production scenario, it is not recommended to use a local file-based Git \nrepository. The configuration server should be typically backed with a highly \navailable Git service. This is possible by either using an external highly available Git \nservice or a highly available internal Git service. SVN can also be considered.\n\n\nScaling Microservices with Spring Cloud\n[ 226 ]\nHaving said that, an already bootstrapped Config server is always capable of \nworking with a local copy of the configuration. Hence, we need a highly available Git \nonly when the Config server needs to be scaled. Therefore, this too is not as critical  \nas the microservices availability or the Config server availability.\nThe GitLab example for setting up high availability is available at \nhttps://about.gitlab.com/high-availability/.\nRabbitMQ also has to be configured for high availability. The high availability for \nRabbitMQ is needed only to push configuration changes dynamically to all instances. \nSince this is more of an offline controlled activity, it does not really require the same \nhigh availability as required by the components.\nRabbitMQ high availability can be achieved by either using a cloud service or a \nlocally configured highly available RabbitMQ service.\nSetting up high availability for Rabbit MQ is documented at \nhttps://www.rabbitmq.com/ha.html.\nMonitoring the Config server health\nThe Config server is nothing but a Spring Boot application, and is, by default, \nconfigured with an actuator. Hence, all actuator endpoints are applicable for  \nthe Config server. The health of the server can be monitored using the following  \nactuator URL: http://localhost:8888/health.\nConfig server for configuration files\nWe may run into scenarios where we need a complete configuration file such \nas logback.xml to be externalized. The Config server provides a mechanism to \nconfigure and store such files. This is achievable by using the URL format as follows: \n/{name}/{profile}/{label}/{path}.\nThe name, profile, and label have the same meanings as explained earlier. The path \nindicates the file name such as logback.xml.\n\n\nChapter 5\n[ 227 ]\nCompleting changes to use the Config server\nIn order to build this capability to complete BrownField Airline's PSS, we have \nto make use of the configuration server for all services. All microservices in the \nexamples given in chapter5.* need to make similar changes to look to the Config \nserver for getting the configuration parameters.\nThe following are a few key change considerations:\n•\t\nThe Fare service URL in the booking component will also be externalized:\nprivate static final String FareURL = \"/fares\";\n  \n@Value(\"${fares-service.url}\")\nprivate String fareServiceUrl;\nFare = restTemplate.getForObject(fareServiceUrl+FareURL +\"/\nget?flightNumber=\"+record.getFlightNumber()+\"&flightDate=\"+record.\ngetFlightDate(),Fare.class);\nAs shown in the preceding code snippet, the Fare service URL is fetched \nthrough a new property: fares-service.url.\n•\t\nWe are not externalizing the queue names used in the Search, Booking, and \nCheck-in services at the moment. Later in this chapter, these will be changed \nto use Spring Cloud Streams.\nFeign as a declarative REST client\nIn the Booking microservice, there is a synchronous call to Fare. RestTemplate \nis used for making the synchronous call. When using RestTemplate, the URL \nparameter is constructed programmatically, and data is sent across to the other \nservice. In more complex scenarios, we will have to get to the details of the HTTP \nAPIs provided by RestTemplate or even to APIs at a much lower level.\nFeign is a Spring Cloud Netflix library for providing a higher level of abstraction \nover REST-based service calls. Spring Cloud Feign works on a declarative principle. \nWhen using Feign, we write declarative REST service interfaces at the client, and \nuse those interfaces to program the client. The developer need not worry about the \nimplementation of this interface. This will be dynamically provisioned by Spring at \nruntime. With this declarative approach, developers need not get into the details of \nthe HTTP level APIs provided by RestTemplate.\n\n\nScaling Microservices with Spring Cloud\n[ 228 ]\nThe following code snippet is the existing code in the Booking microservice for \ncalling the Fare service: \nFare fare = restTemplate.getForObject(FareURL +\"/\nget?flightNumber=\"+record.getFlightNumber()+\"&flightDate=\"+record.\ngetFlightDate(),Fare.class);\nIn order to use Feign, first we need to change the pom.xml file to include the Feign \ndependency as follows:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-feign</artifactId>\n</dependency>\nFor a new Spring Starter project, Feign can be selected from the starter library \nselection screen, or from http://start.spring.io/. This is available under  \nCloud Routing as shown in the following screenshot:\nThe next step is to create a new FareServiceProxy interface. This will act as a proxy \ninterface of the actual Fare service:\n@FeignClient(name=\"fares-proxy\", url=\"localhost:8080/fares\")\npublic interface FareServiceProxy {\n  @RequestMapping(value = \"/get\", method=RequestMethod.GET)\n  Fare getFare(@RequestParam(value=\"flightNumber\") String  \n    flightNumber, @RequestParam(value=\"flightDate\") String  \n    flightDate);\n}\n\n\nChapter 5\n[ 229 ]\nThe FareServiceProxy interface has a @FeignClient annotation. This annotation \ntells Spring to create a REST client based on the interface provided. The value could \nbe a service ID or a logical name. The url indicates the actual URL where the target \nservice is running. Either name or value is mandatory. In this case, since we have \nurl, the name attribute is irrelevant.\nUse this service proxy to call the Fare service. In the Booking microservice, we have \nto tell Spring that Feign clients exist in the Spring Boot application, which are to be \nscanned and discovered. This will be done by adding @EnableFeignClients at the \nclass level of BookingComponent. Optionally, we can also give the package names  \nto scan.\nChange BookingComponent, and make changes to the calling part. This is as simple \nas calling another Java interface:\nFare = fareServiceProxy.getFare(record.getFlightNumber(), record.\ngetFlightDate());\nRerun the Booking microservice to see the effect.\nThe URL of the Fare service in the FareServiceProxy interface is hardcoded: \nurl=\"localhost:8080/fares\".\nFor the time being, we will keep it like this, but we are going to change this later in \nthis chapter.\nRibbon for load balancing\nIn the previous setup, we were always running with a single instance of the \nmicroservice. The URL is hardcoded both in client as well as in the service-to-service \ncalls. In the real world, this is not a recommended approach, since there could be more \nthan one service instance. If there are multiple instances, then ideally, we should use \na load balancer or a local DNS server to abstract the actual instance locations, and \nconfigure an alias name or the load balancer address in the clients. The load balancer \nthen receives the alias name, and resolves it with one of the available instances. With \nthis approach, we can configure as many instances behind a load balancer. It also helps \nus to handle server failures transparent to the client.\n\n\nScaling Microservices with Spring Cloud\n[ 230 ]\nThis is achievable with Spring Cloud Netflix Ribbon. Ribbon is a client-side load \nbalancer which can do round-robin load balancing across a set of servers. There \ncould be other load balancing algorithms possible with the Ribbon library. Spring \nCloud offers a declarative way to configure and use the Ribbon client.\nAs shown in the preceding diagram, the Ribbon client looks for the Config server to \nget the list of available microservice instances, and, by default, applies a round-robin \nload balancing algorithm.\nIn order to use the Ribbon client, we will have to add the following dependency  \nto the pom.xml file:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-ribbon</artifactId>\n</dependency>\nIn case of development from ground up, this can be selected from the Spring  \nStarter libraries, or from http://start.spring.io/. Ribbon is available under \nCloud Routing:\n\n\nChapter 5\n[ 231 ]\nUpdate the Booking microservice configuration file, booking-service.properties, \nto include a new property to keep the list of the Fare microservices:\nfares-proxy.ribbon.listOfServers=localhost:8080,localhost:8081\nGoing back and editing the FareServiceProxy class created in the previous section \nto use the Ribbon client, we note that the value of the @RequestMapping annotations \nis changed from /get to /fares/get so that we can move the host name and port to \nthe configuration easily:\n@FeignClient(name=\"fares-proxy\")\n@RibbonClient(name=\"fares\")\npublic interface FareServiceProxy {\n  @RequestMapping(value = \"fares/get\", method=RequestMethod.GET)\nWe can now run two instances of the Fares microservices. Start one of them on 8080, \nand the other one on 8081:\njava -jar -Dserver.port=8080 fares-1.0.jar\njava -jar -Dserver.port=8081 fares-1.0.jar\nRun the Booking microservice. When the Booking microservice is bootstrapped, the \nCommandLineRunner automatically inserts one booking record. This will go to the \nfirst server.\nWhen running the website project, it calls the Booking service. This request will go  \nto the second server.\nOn the Booking service, we see the following trace, which says there are two servers \nenlisted:\nDynamicServerListLoadBalancer:{NFLoadBalancer:name=fares-proxy,current \nlist of Servers=[localhost:8080, localhost:8081],Load balancer stats=Zone \nstats: {unknown=[Zone:unknown;  Instance count:2;  Active connections \ncount: 0;  Circuit breaker tripped count: 0;  Active connections per \nserver: 0.0;]\n}, \n\n\nScaling Microservices with Spring Cloud\n[ 232 ]\nEureka for registration and discovery\nSo far, we have achieved externalizing configuration parameters as well as load \nbalancing across many service instances.\nRibbon-based load balancing is sufficient for most of the microservices requirements. \nHowever, this approach falls short in a couple of scenarios:\n•\t\nIf there is a large number of microservices, and if we want to optimize \ninfrastructure utilization, we will have to dynamically change the number \nof service instances and the associated servers. It is not easy to predict and \npreconfigure the server URLs in a configuration file.\n•\t\nWhen targeting cloud deployments for highly scalable microservices, static \nregistration and discovery is not a good solution considering the elastic \nnature of the cloud environment.\n•\t\nIn the cloud deployment scenarios, IP addresses are not predictable, and \nwill be difficult to statically configure in a file. We will have to update the \nconfiguration file every time there is a change in address.\nThe Ribbon approach partially addresses this issue. With Ribbon, we can \ndynamically change the service instances, but whenever we add new service \ninstances or shut down instances, we will have to manually update the Config \nserver. Though the configuration changes will be automatically propagated to all \nrequired instances, the manual configuration changes will not work with large scale \ndeployments. When managing large deployments, automation, wherever possible,  \nis paramount.\nTo fix this gap, the microservices should self-manage their life cycle by dynamically \nregistering service availability, and provision automated discovery for consumers.\nUnderstanding dynamic service registration \nand discovery\nDynamic registration is primarily from the service provider's point of view. With \ndynamic registration, when a new service is started, it automatically enlists its \navailability in a central service registry. Similarly, when a service goes out of service, \nit is automatically delisted from the service registry. The registry always keeps  \nup-to-date information of the services available, as well as their metadata.\n\n\nChapter 5\n[ 233 ]\nDynamic discovery is applicable from the service consumer's point of view. Dynamic \ndiscovery is where clients look for the service registry to get the current state of  \nthe services topology, and then invoke the services accordingly. In this approach, \ninstead of statically configuring the service URLs, the URLs are picked up from the \nservice registry.\nThe clients may keep a local cache of the registry data for faster access. Some registry \nimplementations allow clients to keep a watch on the items they are interested in. \nIn this approach, the state changes in the registry server will be propagated to the \ninterested parties to avoid using stale data.\nThere are a number of options available for dynamic service registration and \ndiscovery. Netflix Eureka, ZooKeeper, and Consul are available as part of Spring \nCloud, as shown in the http://start.spring.io/ screenshot given next. Etcd \nis another service registry available outside of Spring Cloud to achieve dynamic \nservice registration and discovery. In this chapter, we will focus on the Eureka \nimplementation:\n\n\nScaling Microservices with Spring Cloud\n[ 234 ]\nUnderstanding Eureka\nSpring Cloud Eureka also comes from Netflix OSS. The Spring Cloud project \nprovides a Spring-friendly declarative approach for integrating Eureka with  \nSpring-based applications. Eureka is primarily used for self-registration, dynamic \ndiscovery, and load balancing. Eureka uses Ribbon for load balancing internally:\nAs shown in the preceding diagram, Eureka consists of a server component \nand a client-side component. The server component is the registry in which all \nmicroservices register their availability. The registration typically includes service \nidentity and its URLs. The microservices use the Eureka client for registering \ntheir availability. The consuming components will also use the Eureka client for \ndiscovering the service instances.\nWhen a microservice is bootstrapped, it reaches out to the Eureka server, and \nadvertises its existence with the binding information. Once registered, the service \nendpoint sends ping requests to the registry every 30 seconds to renew its lease. If a \nservice endpoint cannot renew its lease in a few attempts, that service endpoint will \nbe taken out of the service registry. The registry information will be replicated to \nall Eureka clients so that the clients have to go to the remote Eureka server for each \nand every request. Eureka clients fetch the registry information from the server, and \ncache it locally. After that, the clients use that information to find other services. This \ninformation is updated periodically (every 30 seconds) by getting the delta updates \nbetween the last fetch cycle and the current one.\n\n\nChapter 5\n[ 235 ]\nWhen a client wants to contact a microservice endpoint, the Eureka client provides \na list of currently available services based on the requested service ID. The Eureka \nserver is zone aware. Zone information can also be supplied when registering a \nservice. When a client requests for a services instance, the Eureka service tries to find \nthe service running in the same zone. The Ribbon client then load balances across \nthese available service instances supplied by the Eureka client. The communication \nbetween the Eureka client and the server is done using REST and JSON.\nSetting up the Eureka server\nIn this section, we will run through the steps required for setting up the  \nEureka server.\nThe full source code of this section is available under the \nchapter5.eurekaserver project in the code files. Note that the \nEureka server registration and refresh cycles take up to 30 seconds. \nHence, when running services and clients, wait for 40-50 seconds.\n1.\t Start a new Spring Starter project, and select Config Client, Eureka Server, \nand Actuator:\n\n\nScaling Microservices with Spring Cloud\n[ 236 ]\nThe project structure of the Eureka server is shown in the following image:\nNote that the main application is named EurekaserverApplication.java.\n2.\t Rename application.properties to bootstrap.properties since this is \nusing the Config server. As we did earlier, configure the details of the Config \nserver in the bootsratp.properties file so that it can locate the Config \nserver instance. The bootstrap.properties file will look as follows:\nspring.application.name=eureka-server1\nserver.port:8761\nspring.cloud.config.uri=http://localhost:8888\nThe Eureka server can be set up in a standalone mode or in a clustered \nmode. We will start with the standalone mode. By default, the Eureka server \nitself is another Eureka client. This is particularly useful when there are \nmultiple Eureka servers running for high availability. The client component \nis responsible for synchronizing state from the other Eureka servers. The \nEureka client is taken to its peers by configuring the eureka.client.\nserviceUrl.defaultZone property.\nIn the standalone mode, we point eureka.client.serviceUrl.\ndefaultZone back to the same standalone instance. Later we will  \nsee how we can run Eureka servers in a clustered mode.\n\n\nChapter 5\n[ 237 ]\n3.\t Create a eureka-server1.properties file, and update it in the Git \nrepository. eureka-server1 is the name of the application given in the \napplication's bootstrap.properties file in the previous step. As shown \nin the following code, serviceUrl points back to the same server. Once the \nfollowing properties are added, commit the file to the Git repository:\nspring.application.name=eureka-server1\neureka.client.serviceUrl.defaultZone:http://localhost:8761/eureka/\neureka.client.registerWithEureka:false\neureka.client.fetchRegistry:false\n4.\t Change the default Application.java. In this example, the package is \nalso renamed as com.brownfield.pss.eurekaserver, and the class name \nchanged to EurekaserverApplication. In EurekaserverApplication,  \nadd @EnableEurekaServer:\n@EnableEurekaServer\n@SpringBootApplication\npublic class EurekaserverApplication {\n5.\t We are now ready to start the Eureka server. Ensure that the Config server is \nalso started. Right-click on the application and then choose Run As | Spring \nBoot App. Once the application is started, open http://localhost:8761 in \na browser to see the Eureka console.\n6.\t In the console, note that there is no instance registered under Instances \ncurrently registered with Eureka. Since no services have been started with \nthe Eureka client enabled, the list is empty at this point.\n\n\nScaling Microservices with Spring Cloud\n[ 238 ]\n7.\t Making a few changes to our microservice will enable dynamic registration \nand discovery using the Eureka service. To do this, first we have to add the \nEureka dependencies to the pom.xml file. If the services are being built up \nfresh using the Spring Starter project, then select Config Client, Actuator, \nWeb as well as Eureka discovery client as follows:\n8.\t Since we are modifying our microservices, add the following additional \ndependency to all microservices in their pom.xml files:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-eureka</artifactId>\n</dependency>\n9.\t The following property has to be added to all microservices in their \nrespective configuration files under config-repo. This will help the \nmicroservices to connect to the Eureka server. Commit to Git once updates \nare completed:\neureka.client.serviceUrl.defaultZone: http://localhost:8761/\neureka/\n\n\nChapter 5\n[ 239 ]\n10.\t Add @EnableDiscoveryClient to all microservices in their respective \nSpring Boot main classes. This asks Spring Boot to register these services at \nstart up to advertise their availability.\n11.\t Start all servers except Booking. Since we are using the Ribbon client on the \nBooking service, the behavior could be different when we add the Eureka \nclient in the class path. We will fix this soon.\n12.\t Going to the Eureka URL (http://localhost:8761), you can see that all \nthree instances are up and running:\nTime to fix the issue with Booking. We will remove our earlier Ribbon client, \nand use Eureka instead. Eureka internally uses Ribbon for load balancing. \nHence, the load balancing behavior will not change.\n13.\t Remove the following dependency:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-ribbon</artifactId>\n</dependency>\n14.\t Also remove the @RibbonClient(name=\"fares\") annotation from the \nFareServiceProxy class.\n15.\t Update @FeignClient(name=\"fares-service\") to match the actual \nFare microservices' service ID. In this case, fare-service is the service ID \nconfigured in the Fare microservices' bootstrap.properties. This is the \nname that the Eureka discovery client sends to the Eureka server. The service \nID will be used as a key for the services registered in the Eureka server.\n16.\t Also remove the list of servers from the booking-service.properties  \nfile. With Eureka, we are going to dynamically discover this list from the \nEureka server:\nfares-proxy.ribbon.listOfServers=localhost:8080, localhost:8081\n\n\nScaling Microservices with Spring Cloud\n[ 240 ]\n17.\t Start the Booking service. You will see that CommandLineRunner successfully \ncreated a booking, which involves calling the Fare services using the Eureka \ndiscovery mechanism. Go back to the URL to see all the registered services:\n18.\t Change the website project's bootstrap.properties file to make use of \nEureka rather than connecting directly to the service instances. We will not \nuse the Feign client in this case. Instead, for demonstration purposes, we  \nwill use the load balanced RestTemplate. Commit these changes to the  \nGit repository:\nspring.application.name=test-client\neureka.client.serviceUrl.defaultZone: http://localhost:8761/\neureka/\n19.\t Add @EnableDiscoveryClient to the Application class to make the client \nEureka-aware.\n20.\t Edit both Application.java as well as BrownFieldSiteController.\njava. Add three RestTemplate instances. This time, we annotate them with \n@Loadbalanced to ensure that we use the load balancing features using \nEureka and Ribbon. RestTemplate cannot be automatically injected.  \nHence, we have to provide a configuration entry as follows:\n@Configuration\nclass AppConfiguration {\n    @LoadBalanced\n    @Bean\n    RestTemplate restTemplate() {\n        return new RestTemplate();\n    }\n}\n@Autowired\nRestTemplate searchClient;\n   \n@Autowired\nRestTemplate bookingClient;\n  \n@Autowired\nRestTemplate checkInClient;\n\n\nChapter 5\n[ 241 ]\n21.\t We use these RestTemplate instances to call the microservices. Replace the \nhardcoded URLs with service IDs that are registered in the Eureka server. \nIn the following code, we use the service names search-service, book-\nservice, and checkin-service instead of explicit host names and ports:\nFlight[] flights = searchClient.postForObject(\"http://search-\nservice/search/get\", searchQuery, Flight[].class);\nlong bookingId = bookingClient.postForObject(\"http://book-service/\nbooking/create\", booking, long.class);\n  \nlong checkinId = checkInClient.postForObject(\"http://checkin-\nservice/checkin/create\", checkIn, long.class);\n22.\t We are now ready to run the client. Run the website project. If everything \nis fine, the website project's CommandLineRunner will successfully perform \nsearch, booking, and check-in. The same can also be tested using the browser \nby pointing the browser to http://localhost:8001.\nHigh availability for Eureka\nIn the previous example, there was only one Eureka server in standalone mode.  \nThis is not good enough for a real production system.\nThe Eureka client connects to the server, fetches registry information, and stores it \nlocally in a cache. The client always works with this local cache. The Eureka client \nchecks the server periodically for any state changes. In the case of a state change, it \ndownloads the changes from the server, and updates the cache. If the Eureka server \nis not reachable, then the Eureka clients can still work with the last-known state of \nthe servers based on the data available in the client cache. However, this could lead \nto stale state issues quickly.\n\n\nScaling Microservices with Spring Cloud\n[ 242 ]\nThis section will explore the high availability for the Eureka server. The high \navailability architecture is shown in the following diagram:\nThe Eureka server is built with a peer-to-peer data synchronization mechanism. \nThe runtime state information is not stored in a database, but managed using an in-\nmemory cache. The high availability implementation favors availability and partition \ntolerance in the CAP theorem, leaving out consistency. Since the Eureka server \ninstances are synchronized with each other using an asynchronous mechanism, \nthe states may not always match between server instances. The peer-to-peer \nsynchronization is done by pointing serviceUrls to each other. If there is more than \none Eureka server, each one has to be connected to at least one of the peer servers. \nSince the state is replicated across all peers, Eureka clients can connect to any one of \nthe available Eureka servers.\nThe best way to achieve high availability for Eureka is to cluster multiple Eureka \nservers, and run them behind a load balancer or a local DNS. The clients always \nconnect to the server using the DNS/load balancer. At runtime, the load balancer \ntakes care of selecting the appropriate servers. This load balancer address will be \nprovided to the Eureka clients.\nThis section will showcase how to run two Eureka servers in a cluster for high \navailability. For this, define two property files: eureka-server1 and eureka-\nserver2. These are peer servers; if one fails, the other one will take over. Each of \nthese servers will also act as a client for the other so that they can sync their states. \nTwo property files are defined in the following snippet. Upload and commit these \nproperties to the Git repository. \n\n\nChapter 5\n[ 243 ]\nThe client URLs point to each other, forming a peer network as shown in the \nfollowing configuration:\neureka-server1.properties\neureka.client.serviceUrl.defaultZone:http://localhost:8762/eureka/\neureka.client.registerWithEureka:false\neureka.client.fetchRegistry:false\neureka-server2.properties\neureka.client.serviceUrl.defaultZone:http://localhost:8761/eureka/\neureka.client.registerWithEureka:false\neureka.client.fetchRegistry:false\nUpdate the bootstrap.properties file of Eureka, and change the application name \nto eureka. Since we are using two profiles, based on the active profile supplied at \nstartup, the Config server will look for either eureka-server1 or eureka-server2:\nspring.application.name=eureka\nspring.cloud.config.uri=http://localhost:8888\nStart two instances of the Eureka servers, server1 on 8761 and server2 on 8762:\njava -jar –Dserver.port=8761 -Dspring.profiles.active=server1 demo-0.0.1-\nSNAPSHOT.jar\njava -jar –Dserver.port=8762 -Dspring.profiles.active=server2 demo-0.0.1-\nSNAPSHOT.jar\nAll our services still point to the first server, server1. Open both the browser \nwindows: http://localhost:8761 and http://localhost:8762.\nStart all microservices. The one which opened 8761 will immediately reflect the \nchanges, whereas the other one will take 30 seconds for reflecting the states. Since \nboth the servers are in a cluster, the state is synchronized between these two servers. \nIf we keep these servers behind a load balancer/DNS, then the client will always \nconnect to one of the available servers.\nAfter completing this exercise, switch back to the standalone mode for the remaining \nexercises.\n\n\nScaling Microservices with Spring Cloud\n[ 244 ]\nZuul proxy as the API gateway\nIn most microservice implementations, internal microservice endpoints are not \nexposed outside. They are kept as private services. A set of public services will be \nexposed to the clients using an API gateway. There are many reasons to do this:\n•\t\nOnly a selected set of microservices are required by the clients.\n•\t\nIf there are client-specific policies to be applied, it is easy to apply them in  \na single place rather than in multiple places. An example of such a scenario  \nis the cross-origin access policy.\n•\t\nIt is hard to implement client-specific transformations at the service endpoint.\n•\t\nIf there is data aggregation required, especially to avoid multiple client calls \nin a bandwidth-restricted environment, then a gateway is required in the \nmiddle.\nZuul is a simple gateway service or edge service that suits these situations well. \nZuul also comes from the Netflix family of microservice products. Unlike many \nenterprise API gateway products, Zuul provides complete control for the developers \nto configure or program based on specific requirements:\nThe Zuul proxy internally uses the Eureka server for service discovery, and Ribbon \nfor load balancing between service instances.\nThe Zuul proxy is also capable of routing, monitoring, managing resiliency, security, \nand so on. In simple terms, we can consider Zuul a reverse proxy service. With Zuul, \nwe can even change the behaviors of the underlying services by overriding them at \nthe API layer.\n\n\nChapter 5\n[ 245 ]\nSetting up Zuul\nUnlike the Eureka server and the Config server, in typical deployments, Zuul \nis specific to a microservice. However, there are deployments in which one API \ngateway covers many microservices. In this case, we are going to add Zuul for each \nof our microservices: Search, Booking, Fare, and Check-in:\nThe full source code of this section is available under the chapter5.*-\napigateway project in the code files.\n1.\t Convert the microservices one by one. Start with Search API Gateway.  \nCreate a new Spring Starter project, and select Zuul, Config Client, Actuator, \nand Eureka Discovery:\n\n\nScaling Microservices with Spring Cloud\n[ 246 ]\nThe project structure for search-apigateway is shown in the following \ndiagram:\n2.\t The next step is to integrate the API gateway with Eureka and the Config \nserver. Create a search-apigateway.property file with the contents given \nnext, and commit to the Git repository.\nThis configuration also sets a rule on how to forward traffic. In this case, any \nrequest coming on the /api endpoint of the API gateway should be sent to \nsearch-service:\nspring.application.name=search-apigateway\nzuul.routes.search-apigateway.serviceId=search-service\nzuul.routes.search-apigateway.path=/api/**\neureka.client.serviceUrl.defaultZone:http://localhost:8761/eureka/\nsearch-service is the service ID of the Search service, and it will be \nresolved using the Eureka server.\n3.\t Update the bootstrap.properties file of search-apigateway as follows. \nThere is nothing new in this configuration—a name to the service, the port, \nand the Config server URL:\nspring.application.name=search-apigateway\nserver.port=8095\nspring.cloud.config.uri=http://localhost:8888\n\n\nChapter 5\n[ 247 ]\n4.\t Edit Application.java. In this case, the package name and the class \nname are also changed to com.brownfield.pss.search.apigateway and \nSearchApiGateway respectively. Also add @EnableZuulProxy to tell Spring \nBoot that this is a Zuul proxy:\n@EnableZuulProxy\n@EnableDiscoveryClient\n@SpringBootApplication\npublic class SearchApiGateway {\n5.\t Run this as a Spring Boot app. Before that, ensure that the Config server, the \nEureka server, and the Search microservice are running.\n6.\t Change the website project's CommandLineRunner as well as \nBrownFieldSiteController to make use of the API gateway:\nFlight[] flights = searchClient.postForObject(\"http://search-\napigateway/api/search/get\", searchQuery, Flight[].class); \nIn this case, the Zuul proxy acts as a reverse proxy which proxies all microservice \nendpoints to consumers. In the preceding example, the Zuul proxy does not add \nmuch value, as we just pass through the incoming requests to the corresponding \nbackend service.\nZuul is particularly useful when we have one or more requirements like the \nfollowing:\n•\t\nEnforcing authentication and other security policies at the gateway instead of \ndoing that on every microservice endpoint. The gateway can handle security \npolicies, token handling, and so on before passing the request to the relevant \nservices behind. It can also do basic rejections based on some business \npolicies such as blocking requests coming from certain black-listed users.\n•\t\nBusiness insights and monitoring can be implemented at the gateway \nlevel. Collect real-time statistical data, and push it to an external system \nfor analysis. This will be handy as we can do this at one place rather than \napplying it across many microservices.\n•\t\nAPI gateways are useful in scenarios where dynamic routing is required \nbased on fine-grained controls. For example, send requests to different \nservice instances based on business specific values such as \"origin country\". \nAnother example is all requests coming from a region to be sent to one group \nof service instances. Yet another example is all requests requesting for a \nparticular product have to be routed to a group of service instances.\n\n\nScaling Microservices with Spring Cloud\n[ 248 ]\n•\t\nHandling the load shredding and throttling requirements is another scenario \nwhere API gateways are useful. This is when we have to control load based \non set thresholds such as number of requests in a day. For example, control \nrequests coming from a low-value third party online channel.\n•\t\nThe Zuul gateway is useful for fine-grained load balancing scenarios. The \nZuul, Eureka client, and Ribbon together provide fine-grained controls over \nthe load balancing requirements. Since the Zuul implementation is nothing \nbut another Spring Boot application, the developer has full control over the \nload balancing.\n•\t\nThe Zuul gateway is also useful in scenarios where data aggregation \nrequirements are in place. If the consumer wants higher level coarse-grained \nservices, then the gateway can internally aggregate data by calling more than \none service on behalf of the client. This is particularly applicable when the \nclients are working in low bandwidth environments.\nZuul also provides a number of filters. These filters are classified as pre filters, \nrouting filters, post filters, and error filters. As the names indicate, these are applied \nat different stages of the life cycle of a service call. Zuul also provides an option for \ndevelopers to write custom filters. In order to write a custom filter, extend from the \nabstract ZuulFilter, and implement the following methods:\npublic class CustomZuulFilter extends ZuulFilter{\npublic Object run(){}\npublic boolean shouldFilter(){}\npublic int filterOrder(){}\npublic String filterType(){}\nOnce a custom filter is implemented, add that class to the main context. In our \nexample case, add this to the SearchApiGateway class as follows:\n@Bean\npublic CustomZuulFilter customFilter() {\n    return new CustomZuulFilter();\n}\nAs mentioned earlier, the Zuul proxy is a Spring Boot service. We can customize the \ngateway programmatically in the way we want. As shown in the following code, \nwe can add custom endpoints to the gateway, which, in turn, can call the backend \nservices:\n@RestController \nclass SearchAPIGatewayController {\n  @RequestMapping(\"/\")\n  String greet(HttpServletRequest req){\n\n\nChapter 5\n[ 249 ]\n    return \"<H1>Search Gateway Powered By Zuul</H1>\";\n  }\n}\nIn the preceding case, it just adds a new endpoint, and returns a value from the \ngateway. We can further use @Loadbalanced RestTemplate to call a backend \nservice. Since we have full control, we can do transformations, data aggregation, \nand so on. We can also use the Eureka APIs to get the server list, and implement \ncompletely independent load-balancing or traffic-shaping mechanisms instead  \nof the out-of-the-box load balancing features provided by Ribbon.\nHigh availability of Zuul\nZuul is just a stateless service with an HTTP endpoint, hence, we can have as many \nZuul instances as we need. There is no affinity or stickiness required. However, \nthe availability of Zuul is extremely critical as all traffic from the consumer to the \nprovider flows through the Zuul proxy. However, the elastic scaling requirements \nare not as critical as the backend microservices where all the heavy lifting happens.\nThe high availability architecture of Zuul is determined by the scenario in which we \nare using Zuul. The typical usage scenarios are:\n•\t\nWhen a client-side JavaScript MVC such as AngularJS accesses Zuul services \nfrom a remote browser.\n•\t\nAnother microservice or non-microservice accesses services via Zuul\nIn some cases, the client may not have the capabilities to use the Eureka client \nlibraries, for example, a legacy application written on PL/SQL. In some cases, \norganization policies do not allow Internet clients to handle client-side load \nbalancing. In the case of browser-based clients, there are third-party Eureka \nJavaScript libraries available.\nIt all boils down to whether the client is using Eureka client libraries or not. Based on \nthis, there are two ways we can set up Zuul for high availability.\n\n\nScaling Microservices with Spring Cloud\n[ 250 ]\nHigh availability of Zuul when the client is also a \nEureka client\nIn this case, since the client is also another Eureka client, Zuul can be configured just \nlike other microservices. Zuul registers itself to Eureka with a service ID. The clients \nthen use Eureka and the service ID to resolve Zuul instances:\nAs shown in the preceding diagram, Zuul services register themselves with Eureka \nwith a service ID, search-apigateway in our case. The Eureka client asks for the \nserver list with the ID search-apigateway. The Eureka server returns the list of \nservers based on the current Zuul topology. The Eureka client, based on this list \npicks up one of the servers, and initiates the call.\nAs we saw earlier, the client uses the service ID to resolve the Zuul instance. In the \nfollowing case, search-apigateway is the Zuul instance ID registered with Eureka:\nFlight[] flights = searchClient.postForObject(\"http://search-\napigateway/api/search/get\", searchQuery, Flight[].class); \n\n\nChapter 5\n[ 251 ]\nHigh availability when the client is not a Eureka \nclient\nIn this case, the client is not capable of handling load balancing by using the Eureka \nserver. As shown in the following diagram, the client sends the request to a load \nbalancer, which in turn identifies the right Zuul service instance. The Zuul instance, \nin this case, will be running behind a load balancer such as HAProxy or a hardware \nload balancer like NetScaler:\nThe microservices will still be load balanced by Zuul using the Eureka server.\nCompleting Zuul for all other services\nIn order to complete this exercise, add API gateway projects (name them as \n*-apigateway) for all our microservices. The following steps are required to achieve \nthis task:\n1.\t Create new property files per service, and check in to the Git repositories.\n2.\t Change application.properties to bootstrap.properties, and add the \nrequired configurations.\n3.\t Add @EnableZuulProxy to Application.java in each of the *-apigateway \nprojects.\n\n\nScaling Microservices with Spring Cloud\n[ 252 ]\n4.\t Add @EnableDiscoveryClient in all the Application.java files under \neach of the *-apigateway projects.\n5.\t Optionally, change the package names and file names generated by default.\nIn the end, we will have the following API gateway projects:\n•\t\nchapter5.fares-apigateway\n•\t\nchapter5.search-apigateway\n•\t\nchapter5.checkin-apigateway\n•\t\nchapter5.book-apigateway\nStreams for reactive microservices\nSpring Cloud Stream provides an abstraction over the messaging infrastructure. The \nunderlying messaging implementation can be RabbitMQ, Redis, or Kafka. Spring \nCloud Stream provides a declarative approach for sending and receiving messages:\nAs shown in the preceding diagram, Cloud Stream works on the concept of a source \nand a sink. The source represents the sender perspective of the messaging, and sink \nrepresents the receiver perspective of the messaging.\nIn the example shown in the diagram, the sender defines a logical queue called \nSource.OUTPUT to which the sender sends messages. The receiver defines a logical \nqueue called Sink.INPUT from which the receiver retrieves messages. The physical \nbinding of OUTPUT to INPUT is managed through the configuration. In this case, \nboth link to the same physical queue—MyQueue on RabbitMQ. So, while at one end, \nSource.OUTPUT points to MyQueue, on the other end, Sink.INPUT points to the  \nsame MyQueue.\n\n\nChapter 5\n[ 253 ]\nSpring Cloud offers the flexibility to use multiple messaging providers in one \napplication such as connecting an input stream from Kafka to a Redis output stream, \nwithout managing the complexities. Spring Cloud Stream is the basis for message-\nbased integration. The Cloud Stream Modules subproject is another Spring Cloud \nlibrary that provides many endpoint implementations.\nAs the next step, rebuild the inter-microservice messaging communication with \nthe Cloud Streams. As shown in the next diagram, we will define a SearchSink \nconnected to InventoryQ under the Search microservice. Booking will define a \nBookingSource for sending inventory change messages connected to InventoryQ. \nSimilarly, Check-in defines a CheckinSource for sending the check-in messages. \nBooking defines a sink, BookingSink, for receiving messages, both bound to the \nCheckinQ queue on the RabbitMQ:\nIn this example, we will use RabbitMQ as the message broker:\n1.\t Add the following Maven dependency to Booking, Search, and Check-in, as \nthese are the three modules using messaging:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-stream-rabbit \n    </artifactId>\n</dependency>\n\n\nScaling Microservices with Spring Cloud\n[ 254 ]\n2.\t Add the following two properties to booking-service.properties. These \nproperties bind the logical queue inventoryQ to physical inventoryQ, and \nthe logical checkinQ to the physical checkinQ:\nspring.cloud.stream.bindings.inventoryQ.destination=inventoryQ\nspring.cloud.stream.bindings.checkInQ.destination=checkInQ\n3.\t Add the following property to search-service.properties. This property \nbinds the logical queue inventoryQ to the physical inventoryQ:\nspring.cloud.stream.bindings.inventoryQ.destination=inventoryQ\n4.\t Add the following property to checkin-service.properties. This \nproperty binds the logical queue checkinQ to the physical checkinQ:\nspring.cloud.stream.bindings.checkInQ.destination=checkInQ\n5.\t Commit all files to the Git repository.\n6.\t The next step is to edit the code. The Search microservice consumes a \nmessage from the Booking microservice. In this case, Booking is the source \nand Search is the sink.\nAdd @EnableBinding to the Sender class of the Booking service. This \nenables the Cloud Stream to work on autoconfigurations based on the \nmessage broker library available in the class path. In our case, it is RabbitMQ. \nThe parameter BookingSource defines the logical channels to be used for \nthis configuration:\n@EnableBinding(BookingSource.class)\npublic class Sender {\n7.\t In this case, BookingSource defines a message channel called inventoryQ, \nwhich is physically bound to RabbitMQ's inventoryQ, as configured in the \nconfiguration. BookingSource uses an annotation, @Output, to indicate that \nthis is of the output type—a message that is outgoing from a module. This \ninformation will be used for autoconfiguration of the message channel:\ninterface BookingSource {\n    public static String InventoryQ=\"inventoryQ\"; \n    @Output(\"inventoryQ\")\n    public MessageChannel inventoryQ();      \n}\n8.\t Instead of defining a custom class, we can also use the default Source class \nthat comes with Spring Cloud Stream if the service has only one source  \nand sink:\npublic interface Source {\n  @Output(\"output\")\n\n\nChapter 5\n[ 255 ]\n  MessageChannel output();\n}\n9.\t Define a message channel in the sender, based on BookingSource. The \nfollowing code will inject an output message channel with the name \ninventory, which is already configured in BookingSource:\n  @Output (BookingSource.InventoryQ)\n  @Autowired\n  private MessageChannel;\n10.\t Reimplement the send message method in BookingSender:\npublic void send(Object message){\n  messageChannel.\n    send(MessageBuilder.withPayload(message).\n    build());\n}\n11.\t Now add the following to the SearchReceiver class the same way we did \nfor the Booking service:\n@EnableBinding(SearchSink.class)\npublic class Receiver {\n12.\t In this case, the SearchSink interface will look like the following. This will \ndefine the logical sink queue it is connected with. The message channel in \nthis case is defined as @Input to indicate that this message channel is to \naccept messages:\ninterface SearchSink {\n    public static String INVENTORYQ=\"inventoryQ\"; \n    @Input(\"inventoryQ\")\n    public MessageChannel inventoryQ();\n}\n13.\t Amend the Search service to accept this message:\n@ServiceActivator(inputChannel = SearchSink.INVENTORYQ)\npublic void accept(Map<String,Object> fare){\n        searchComponent.updateInventory((String)fare.\n        get(\"FLIGHT_NUMBER\"),(String)fare.\n        get(\"FLIGHT_DATE\"),(int)fare.\n        get(\"NEW_INVENTORY\"));\n}\n\n\nScaling Microservices with Spring Cloud\n[ 256 ]\n14.\t We will still need the RabbitMQ configurations that we have in our \nconfiguration files to connect to the message broker:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\nserver.port=8090\n15.\t Run all services, and run the website project. If everything is fine, the  \nwebsite project successfully executes the Search, Booking, and Check-in \nfunctions. The same can also be tested using the browser by pointing  \nto http://localhost:8001.\nSummarizing the BrownField PSS \narchitecture\nThe following diagram shows the overall architecture that we have created with \nthe Config server, Eureka, Feign, Zuul, and Cloud Streams. The architecture also \nincludes the high availability of all components. In this case, we assume that the \nclient uses the Eureka client libraries:\n\n\nChapter 5\n[ 257 ]\nThe summary of the projects and the port they are listening on is given in the \nfollowing table:\nMicroservice \nProjects\nPort\nBook microservice\nchapter5.book\n8060 to 8064\nCheck-in microservice\nchapter5.checkin\n8070 to 8074\nFare microservice\nchapter5.fares\n8080 to 8084\nSearch microservice\nchapter5.search\n8090 to 8094\nWebsite client\nchapter5.website\n8001\nSpring Cloud Config server\nchapter5.configserver\n8888/8889\nSpring Cloud Eureka server\nchapter5.eurekaserver\n8761/8762\nBook API gateway\nchapter5.book-\napigateway\n8095 to 8099\nCheck-in API gateway\nchapter5.checkin-\napigateway\n8075 to 8079\nFares API gateway\nchapter5.fares-\napigateway\n8085 to 8089\nSearch API gateway\nchapter5.search-\napigateway\n8065 to 8069\nFollow these steps to do a final run:\n1.\t Run RabbitMQ.\n2.\t Build all projects using pom.xml at the root level:\nmvn –Dmaven.test.skip=true clean install \n3.\t Run the following projects from their respective folders. Remember to wait \nfor 40 to 50 seconds before starting the next service. This will ensure that  \nthe dependent services are registered and are available before we start a  \nnew service:\njava -jar target/fares-1.0.jar\njava -jar target/search-1.0.jar\njava -jar target/checkin-1.0.jar\njava -jar target/book-1.0.jar\njava –jar target/fares-apigateway-1.0.jar\njava –jar target/search-apigateway-1.0.jar\njava –jar target/checkin-apigateway-1.0.jar\njava –jar target/book-apigateway-1.0.jar\njava -jar target/website-1.0.jar\n\n\nScaling Microservices with Spring Cloud\n[ 258 ]\n4.\t Open the browser window, and point to http://localhost:8001. Follow \nthe steps mentioned in the Running and testing the project section in Chapter 4, \nMicroservices Evolution – A Case Study.\nSummary\nIn this chapter, you learned how to scale a Twelve-Factor Spring Boot microservice \nusing the Spring Cloud project. What you learned was then applied to the \nBrownField Airline's PSS microservice that we developed in the previous chapter.\nWe then explored the Spring Config server for externalizing the microservices' \nconfiguration, and the way to deploy the Config server for high availability. \nWe also discussed the declarative service calls using Feign, examined the use of \nRibbon and Eureka for load balancing, dynamic service registration, and discovery. \nImplementation of an API gateway was examined by implementing Zuul. Finally, \nwe concluded with a reactive style integration of microservices using Spring  \nCloud Stream.\nBrownField Airline's PSS microservices are now deployable on the Internet scale. \nOther Spring Cloud components such as Hyterix, Sleuth, and so on will be covered \nin Chapter 7, Logging and Monitoring Microservices. The next chapter will demonstrate \nautoscaling features, extending the BrownField PSS implementation.\n\n\n[ 259 ]\nAutoscaling Microservices\nSpring Cloud provides the support essential for the deployment of microservices at \nscale. In order to get the full power of a cloud-like environment, the microservices \ninstances should also be capable of scaling out and shrinking automatically based  \non traffic patterns.\nThis chapter will detail out how to make microservices elastically grow and shrink \nby effectively using the actuator data collected from Spring Boot microservices to \ncontrol the deployment topology by implementing a simple life cycle manager.\nBy the end of this chapter, you will learn about the following topics:\n•\t\nThe basic concept of autoscaling and different approaches for autoscaling\n•\t\nThe importance and capabilities of a life cycle manager in the context  \nof microservices\n•\t\nExamining the custom life cycle manager to achieve autoscaling\n•\t\nProgrammatically collecting statistics from the Spring Boot actuator and \nusing it to control and shape incoming traffic\n\n\nAutoscaling Microservices\n[ 260 ]\nReviewing the microservice capability \nmodel\nThis chapter will cover the Application Lifecycle Management capability in the \nmicroservices capability model discussed in Chapter 3, Applying Microservices \nConcepts, highlighted in the following diagram:\nWe will see a basic version of the life cycle manager in this chapter, which will be \nenhanced in later chapters.\nScaling microservices with Spring Cloud\nIn Chapter 5, Scaling Microservices with Spring Cloud, you learned how to scale Spring \nBoot microservices using Spring Cloud components. The two key concepts of Spring \nCloud that we implemented are self-registration and self-discovery. These two \ncapabilities enable automated microservices deployments. With self-registration, \nmicroservices can automatically advertise the service availability by registering \nservice metadata to a central service registry as soon as the instances are ready \nto accept traffic. Once the microservices are registered, consumers can consume \nthe newly registered services from the very next moment by discovering service \ninstances using the registry service. Registry is at the heart of this automation.\n\n\nChapter 6\n[ 261 ]\nThis is quite different from the traditional clustering approach employed by the \ntraditional JEE application servers. In the case of JEE application servers, the server \ninstances' IP addresses are more or less statically configured in a load balancer. \nTherefore, the cluster approach is not the best solution for automatic scaling in \nInternet-scale deployments. Also, clusters impose other challenges, such as they  \nhave to have exactly the same version of binaries on all cluster nodes. It is also \npossible that the failure of one cluster node can poison other nodes due to the  \ntight dependency between nodes.\nThe registry approach decouples the service instances. It also eliminates the need to \nmanually maintain service addresses in the load balancer or configure virtual IPs:\nAs shown in the diagram, there are three key components in our automated \nmicroservices deployment topology:\n•\t\nEureka is the central registry component for microservice registration and \ndiscovery. REST APIs are used by both consumers as well as providers to \naccess the registry. The registry also holds the service metadata such as the \nservice identity, host, port, health status, and so on.\n•\t\nThe Eureka client, together with the Ribbon client, provide client-side \ndynamic load balancing. Consumers use the Eureka client to look up the \nEureka server to identify the available instances of a target service. The \nRibbon client uses this server list to load-balance between the available \nmicroservice instances. In a similar way, if the service instance goes out of \nservice, these instances will be taken out of the Eureka registry. The load \nbalancer automatically reacts to these dynamic topology changes.\n•\t\nThe third component is the microservices instances developed using Spring \nBoot with the actuator endpoints enabled.\n\n\nAutoscaling Microservices\n[ 262 ]\nHowever, there is one gap in this approach. When there is need for an additional \nmicroservice instance, a manual task is required to kick off a new instance. In an \nideal scenario, the starting and stopping of microservice instances also require \nautomation.\nFor example, when there is a requirement to add another Search microservice \ninstance to handle the increase in traffic volumes or a load burst scenario, the \nadministrator has to manually bring up a new instance. Also, when the Search \ninstance is idle for some time, it needs to be manually taken out of service to  \nhave optimal infrastructure usage. This is especially relevant when services  \nrun on a pay-as-per-usage cloud environment.\nUnderstanding the concept of \nautoscaling\nAutoscaling is an approach to automatically scaling out instances based on the \nresource usage to meet the SLAs by replicating the services to be scaled.\nThe system automatically detects an increase in traffic, spins up additional instances, \nand makes them available for traffic handling. Similarly, when the traffic volumes \ngo down, the system automatically detects and reduces the number of instances by \ntaking active instances back from the service:\nAs shown in the preceding diagram, autoscaling is done, generally, using a set of \nreserve machines.\n",
      "page_number": 232
    },
    {
      "number": 6,
      "title": "[ 261 ]",
      "start_page": 288,
      "end_page": 311,
      "detection_method": "regex_chapter",
      "content": "Chapter 6\n[ 263 ]\nAs many of the cloud subscriptions are based on a pay-as-you-go model, this is an \nessential capability when targeting cloud deployments. This approach is often called \nelasticity. It is also called dynamic resource provisioning and deprovisioning. \nAutoscaling is an effective approach specifically for microservices with varying \ntraffic patterns. For example, an Accounting service would have high traffic during \nmonth ends and year ends. There is no point in permanently provisioning instances \nto handle these seasonal loads.\nIn the autoscaling approach, there is often a resource pool with a number of spare \ninstances. Based on the demand, instances will be moved from the resource pool to \nthe active state to meet the surplus demand. These instances are not pretagged for \nany particular microservices or prepackaged with any of the microservice binaries.  \nIn advanced deployments, the Spring Boot binaries are downloaded on demand \nfrom an artifact repository such as Nexus or Artifactory.\nThe benefits of autoscaling\nThere are many benefits in implementing the autoscaling mechanism. In traditional \ndeployments, administrators reserve a set of servers against each application. With \nautoscaling, this preallocation is no longer required. This prefixed server allocation \nmay result in underutilized servers. In this case, idle servers cannot be utilized even \nwhen neighboring services struggle for additional resources.\nWith hundreds of microservice instances, preallocating a fixed number of servers to \neach of the microservices is not cost effective. A better approach is to reserve a number \nof server instances for a group of microservices without preallocating or tagging them \nagainst a microservice. Instead, based on the demand, a group of services can share a \nset of available resources. By doing so, microservices can be dynamically moved across \nthe available server instances by optimally using the resources:\n\n\nAutoscaling Microservices\n[ 264 ]\nAs shown in the preceding diagram, there are three instances of the M1 microservice, \none instance of M2, and one instance of M3 up and running. There is another server \nkept unallocated. Based on the demand, the unallocated server can be used for \nany of the microservices: M1, M2, or M3. If M1 has more service requests, then the \nunallocated instance will be used for M1. When the service usage goes down, the \nserver instance will be freed up and moved back to the pool. Later, if the M2 demand \nincreases, the same server instance can be activated using M2.\nSome of the key benefits of autoscaling are:\n•\t\nIt has high availability and is fault tolerant: As there are multiple service \ninstances, even if one fails, another instance can take over and continue \nserving clients. This failover will be transparent to the consumers. If no other \ninstance of this service is available, the autoscaling service will recognize this \nsituation and bring up another server with the service instance. As the whole \nprocess of bringing up or bringing down instances is automatic, the overall \navailability of the services will be higher than the systems implemented \nwithout autoscaling. The systems without autoscaling require manual \nintervention to add or remove service instances, which will be hard to \nmanage in large deployments.\nFor example, assume that two of instances of the Booking service are \nrunning. If there is an increase in the traffic flow, in a normal scenario,  \nthe existing instance might become overloaded. In most of the scenarios,  \nthe entire set of services will be jammed, resulting in service unavailability. \nIn the case of autoscaling, a new Booking service instance can be brought  \nup quickly. This will balance the load and ensure service availability.\n•\t\nIt increases scalability: One of the key benefits of autoscaling is horizontal \nscalability. Autoscaling allows us to selectively scale up or scale down \nservices automatically based on traffic patterns.\n•\t\nIt has optimal usage and is cost saving: In a pay-as-you-go subscription \nmodel, billing is based on actual resource utilization. With the autoscaling \napproach, instances will be started and shut down based on the demand. \nHence, resources are optimally utilized, thereby saving cost.\n\n\nChapter 6\n[ 265 ]\n•\t\nIt gives priority to certain services or group of services: With autoscaling, \nit is possible to give priority to certain critical transactions over low-value \ntransactions. This will be done by removing an instance from a low-value \nservice and reallocating it to a high-value service. This will also eliminate \nsituations where a low-priority transaction heavily utilizes resources when \nhigh-value transactions are cramped up for resources.\nFor instance, the Booking and Reports services run with two instances, as \nshown in the preceding diagram. Let's assume that the Booking service is a \nrevenue generation service and therefore has a higher value than the Reports \nservice. If there are more demands for the Booking service, then one can set \npolicies to take one Reports service out of the service and release this server \nfor the Booking service.\nDifferent autoscaling models\nAutoscaling can be applied at the application level or at the infrastructure level.  \nIn a nutshell, application scaling is scaling by replicating application binaries only, \nwhereas infrastructure scaling is replicating the entire virtual machine, including \napplication binaries.\n\n\nAutoscaling Microservices\n[ 266 ]\nAutoscaling an application\nIn this scenario, scaling is done by replicating the microservices, not the underlying \ninfrastructure, such as virtual machines. The assumption is that there is a pool of \nVMs or physical infrastructures available to scale up microservices. These VMs have \nthe basic image fused with any dependencies, such as JRE. It is also assumed that \nmicroservices are homogeneous in nature. This gives flexibility in reusing the same \nvirtual or physical machines for different services:\nAs shown in the preceding diagram, in scenario A, VM3 is used for Service 1, \nwhereas in scenario B, the same VM3 is used for Service 2. In this case, we only \nswapped the application library and not the underlying infrastructure.\nThis approach gives faster instantiation as we are only handling the application \nbinaries and not the underlying VMs. The switching is easier and faster as the \nbinaries are smaller in size and there is no OS boot required either. However, the \ndownside of this approach is that if certain microservices require OS-level tuning  \nor use polyglot technologies, then dynamically swapping microservices will not  \nbe effective.\nAutoscaling the infrastructure\nIn contrast to the previous approach, in this case, the infrastructure is also provisioned \nautomatically. In most cases, this will create a new VM on the fly or destroy the VMs \nbased on the demand:\n\n\nChapter 6\n[ 267 ]\nAs shown in the preceding diagram, the reserve instances are created as VM images \nwith predefined service instances. When there is demand for Service 1, VM3 is \nmoved to an active state. When there is a demand for Service 2, VM4 is moved  \nto the active state.\nThis approach is efficient if the applications depend upon the parameters and \nlibraries at the infrastructure level, such as the operating system. Also, this approach \nis better for polyglot microservices. The downside is the heavy nature of VM images \nand the time required to spin up a new VM. Lightweight containers such as Dockers \nare preferred in such cases instead of traditional heavyweight virtual machines.\nAutoscaling in the cloud\nElasticity or autoscaling is one of the fundamental features of most cloud providers. \nCloud providers use infrastructure scaling patterns, as discussed in the previous \nsection. These are typically based on a set of pooled machines.\nFor example, in AWS, these are based on introducing new EC2 instances with a \npredefined AMI. AWS supports autoscaling with the help of autoscaling groups. \nEach group is set with a minimum and maximum number of instances. AWS  \nensures that the instances are scaled on demand within these bounds. In case of \npredictable traffic patterns, provisioning can be configured based on timelines.  \nAWS also provides ability for applications to customize autoscaling policies.\nMicrosoft Azure also supports autoscaling based on the utilization of resources such \nas the CPU, message queue length, and so on. IBM Bluemix supports autoscaling \nbased on resources such as CPU usage.\nOther PaaS platforms, such as CloudBees and OpenShift, also support autoscaling \nfor Java applications. Pivotal Cloud Foundry supports autoscaling with the help of \nPivotal Autoscale. Scaling policies are generally based on resource utilization, such \nas the CPU and memory thresholds.\n\n\nAutoscaling Microservices\n[ 268 ]\nThere are components that run on top of the cloud and provide fine-grained controls \nto handle autoscaling. Netflix Fenzo, Eucalyptus, Boxfuse, and Mesosphere are some \nof the components in this category.\nAutoscaling approaches\nAutoscaling is handled by considering different parameters and thresholds. In this \nsection, we will discuss the different approaches and policies that are typically \napplied to take decisions on when to scale up or down.\nScaling with resource constraints\nThis approach is based on real-time service metrics collected through monitoring \nmechanisms. Generally, the resource-scaling approach takes decisions based on \nthe CPU, memory, or the disk of machines. This can also be done by looking at the \nstatistics collected on the service instances themselves, such as heap memory usage.\nA typical policy may be spinning up another instance when the CPU utilization \nof the machine goes beyond 60%. Similarly, if the heap size goes beyond a certain \nthreshold, we can add a new instance. The same applies to downsizing the compute \ncapacity when the resource utilization goes below a set threshold. This is done by \ngradually shutting down servers:\nIn typical production scenarios, the creation of additional services is not done on the \nfirst occurrence of a threshold breach. The most appropriate approach is to define a \nsliding window or a waiting period.\n\n\nChapter 6\n[ 269 ]\nThe following are some of the examples:\n•\t\nAn example of a response sliding window is if 60% of the response time of \na particular transaction is consistently more than the set threshold value in a \n60-second sampling window, increase service instances\n•\t\nIn a CPU sliding window, if the CPU utilization is consistently beyond 70% \nin a 5 minutes sliding window, then a new instance is created\n•\t\nAn example of the exception sliding window is if 80% of the transactions \nin a sliding window of 60 seconds or 10 consecutive executions result in a \nparticular system exception, such as a connection timeout due to exhausting \nthe thread pool, then a new service instance is created\nIn many cases, we will set a lower threshold than the actual expected thresholds. \nFor example, instead of setting the CPU utilization threshold at 80%, set it at 60% so \nthat the system gets enough time to spin up an instance before it stops responding. \nSimilarly, when scaling down, we use a lower threshold than the actual. For example, \nwe will use 40% CPU utilization to scale down instead of 60%. This allows us to have  \na cool-down period so that there will not be any resource struggle when shutting \ndown instances.\nResource-based scaling is also applicable to service-level parameters such as the \nthroughput of the service, latency, applications thread pool, connection pool, and \nso on. These can also be at the application level, such as the number of sales orders \nprocessing in a service instance, based on internal benchmarking.\nScaling during specific time periods\nTime-based scaling is an approach to scaling services based on certain periods of  \nthe day, month, or year to handle seasonal or business peaks. For example, some \nservices may experience a higher number of transactions during office hours and  \na considerably low number of transactions outside office hours. In this case, during \nthe day, services autoscale to meet the demand and automatically downsize during \nthe non-office hours:\n\n\nAutoscaling Microservices\n[ 270 ]\nMany airports worldwide impose restrictions on night-time landing. As a result, \nthe number of passengers checking in at the airports during the night time is less \ncompared to the day time. Hence, it is cost effective to reduce the number of instances \nduring the night time.\nScaling based on the message queue length\nThis is particularly useful when the microservices are based on asynchronous \nmessaging. In this approach, new consumers are automatically added when the \nmessages in the queue go beyond certain limits:\nThis approach is based on the competing consumer pattern. In this case, a pool \nof instances is used to consume messages. Based on the message threshold, new \ninstances are added to consume additional messages.\nScaling based on business parameters\nIn this case, adding instances is based on certain business parameters—for example, \nspinning up a new instance just before handling sales closing transactions. As soon \nas the monitoring service receives a preconfigured business event (such as sales \nclosing minus 1 hour), a new instance will be brought up in anticipation of large \nvolumes of transactions. This will provide fine-grained control on scaling based  \non business rules:\n\n\nChapter 6\n[ 271 ]\nPredictive autoscaling\nPredictive scaling is a new paradigm of autoscaling that is different from the \ntraditional real-time metrics-based autoscaling. A prediction engine will take \nmultiple inputs, such as historical information, current trends, and so on, to predict \npossible traffic patterns. Autoscaling is done based on these predictions. Predictive \nautoscaling helps avoid hardcoded rules and time windows. Instead, the system \ncan automatically predict such time windows. In more sophisticated deployments, \npredictive analysis may use cognitive computing mechanisms to predict autoscaling.\nIn the cases of sudden traffic spikes, traditional autoscaling may not help. Before \nthe autoscaling component can react to the situation, the spike would have hit and \ndamaged the system. The predictive system can understand these scenarios and \npredict them before their actual occurrence. An example will be handling a flood  \nof requests immediately after a planned outage.\nNetflix Scryer is an example of such a system that can predict resource requirements \nin advance.\n\n\nAutoscaling Microservices\n[ 272 ]\nAutoscaling BrownField PSS \nmicroservices\nIn this section, we will examine how to enhance microservices developed in  \nChapter 5, Scaling Microservices with Spring Cloud, for autoscaling. We need a \ncomponent to monitor certain performance metrics and trigger autoscaling.  \nWe will call this component the life cycle manager.\nThe service life cycle manager, or the application life cycle manager, is responsible  \nfor detecting scaling requirements and adjusting the number of instances accordingly. \nIt is responsible for starting and shutting down instances dynamically.\nIn this section, we will take a look at a primitive autoscaling system to understand \nthe basic concepts, which will be enhanced in later chapters.\nThe capabilities required for an autoscaling \nsystem\nA typical autoscaling system has capabilities as shown in the following diagram:\n\n\nChapter 6\n[ 273 ]\nThe components involved in the autoscaling ecosystem in the context of microservices \nare explained as follows:\n•\t\nMicroservices: These are sets of the up-and-running microservice instances \nthat keep sending health and metrics information. Alternately, these services \nexpose actuator endpoints for metrics collection. In the preceding diagram, \nthese are represented as Microservice 1 through Microservice 4.\n•\t\nService Registry: A service registry keeps track of all the services, their \nhealth states, their metadata, and their endpoint URI.\n•\t\nLoad Balancer: This is a client-side load balancer that looks up the service \nregistry to get up-to-date information about the available service instances.\n•\t\nLifecycle Manager: The life cycle manger is responsible for autoscaling, \nwhich has the following subcomponents:\n°°\nMetrics Collector: A metrics collection unit is responsible for \ncollecting metrics from all service instances. The life cycle manager \nwill aggregate the metrics. It may also keep a sliding time window. \nThe metrics could be infrastructure-level metrics, such as CPU usage, \nor application-level metrics, such as transactions per minute.\n°°\nScaling policies: Scaling policies are nothing but sets of rules \nindicating when to scale up and scale down microservices—for \nexample, 90% of CPU usage above 60% in a sliding window of 5 \nminutes.\n°°\nDecision Engine: A decision engine is responsible for making \ndecisions to scale up and scale down based on the aggregated  \nmetrics and scaling policies.\n°°\nDeployment Rules: The deployment engine uses deployment rules \nto decide which parameters to consider when deploying services.  \nFor example, a service deployment constraint may say that the \ninstance must be distributed across multiple availability regions  \nor a 4 GB minimum of memory required for the service.\n°°\nDeployment Engine: The deployment engine, based on the  \ndecisions of the decision engine, can start or stop microservice \ninstances or update the registry by altering the health states of \nservices. For example, it sets the health status as \"out of service\"  \nto take out a service temporarily.\n\n\nAutoscaling Microservices\n[ 274 ]\nImplementing a custom life cycle manager \nusing Spring Boot\nThe life cycle manager introduced in this section is a minimal implementation \nto understand autoscaling capabilities. In later chapters, we will enhance this \nimplementation with containers and cluster management solutions. Ansible, \nMarathon, and Kubernetes are some of the tools useful in building this capability.\nIn this section, we will implement an application-level autoscaling component  \nusing Spring Boot for the services developed in Chapter 5, Scaling Microservices  \nwith Spring Cloud.\nUnderstanding the deployment topology\nThe following diagram shows a sample deployment topology of BrownField  \nPSS microservices:\nAs shown in the diagram, there are four physical machines. Eight VMs are created \nfrom four physical machines. Each physical machine is capable of hosting two VMs, \nand each VM is capable of running two Spring Boot instances, assuming that all \nservices have the same resource requirements.\n\n\nChapter 6\n[ 275 ]\nFour VMs, VM1 through VM4, are active and are used to handle traffic. VM5 to \nVM8 are kept as reserve VMs to handle scalability. VM5 and VM6 can be used for \nany of the microservices and can also be switched between microservices based \non scaling demands. Redundant services use VMs created from different physical \nmachines to improve fault tolerance.\nOur objective is to scale out any services when there is increase in traffic flow using \nfour VMs, VM5 through VM8, and scale down when there is not enough load. The \narchitecture of our solution is as follows.\nUnderstanding the execution flow\nHave a look at the following flowchart:\nAs shown in the preceding diagram, the following activities are important for us:\n•\t\nThe Spring Boot service represents microservices such as Search, Book, Fares, \nand Check-in. Services at startup automatically register endpoint details \nto the Eureka registry. These services are actuator-enabled, so the life cycle \nmanager can collect metrics from the actuator endpoints.\n\n\nAutoscaling Microservices\n[ 276 ]\n•\t\nThe life cycle manager service is nothing but another Spring Boot application. \nThe life cycle manager has a metrics collector that runs a background job, \nperiodically polls the Eureka server, and gets details of all the service \ninstances. The metrics collector then invokes the actuator endpoints of each \nmicroservice registered in the Eureka registry to get the health and metrics \ninformation. In a real production scenario, a subscription approach for data \ncollection is better.\n•\t\nWith the collected metrics information, the life cycle manager executes a \nlist of policies and derives decisions on whether to scale up or scale down \ninstances. These decisions are either to start a new service instance of a \nparticular type on a particular VM or to shut down a particular instance.\n•\t\nIn the case of shutdown, it connects to the server using an actuator endpoint \nand calls the shutdown service to gracefully shut down an instance.\n•\t\nIn the case of starting a new instance, the deployment engine of the life cycle \nmanager uses the scaling rules and decides where to start the new instance \nand what parameters are to be used when starting the instance. Then, it \nconnects to the respective VMs using SSH. Once connected, it executes a \npreinstalled script (or passes this script as a part of the execution) by passing \nthe required constraints as a parameter. This script fetches the application \nlibrary from a central Nexus repository in which the production binaries \nare kept and initiates it as a Spring Boot application. The port number is \nparameterized by the life cycle manager. SSH needs to be enabled on the \ntarget machines.\nIn this example, we will use TPM (Transactions Per Minute) or RPM (Requests Per \nMinute) as sampler metrics for decision making. If the Search service has more than \n10 TPM, then it will spin up a new Search service instance. Similarly, if the TPM is \nbelow 2, one of the instances will be shut down and released back to the pool.\nWhen starting a new instance, the following policies will be applied:\n•\t\nThe number of service instances at any point should be a minimum of 1 and a \nmaximum of 4. This also means that at least one service instance will always  \nbe up and running.\n•\t\nA scaling group is defined in such a way that a new instance is created on a \nVM that is on a different physical machine. This will ensure that the services \nrun across different physical machines.\nThese policies could be further enhanced. The life cycle manager ideally provides \noptions to customize these rules through REST APIs or Groovy scripts.\n\n\nChapter 6\n[ 277 ]\nA walkthrough of the life cycle manager code\nWe will take a look at how a simple life cycle manager is implemented. This section \nwill be a walkthrough of the code to understand the different components of the life \ncycle manager.\nThe full source code is available under the Chapter 6 project \nin the code files. The chapter5.configserver, chapter5.\neurekaserver, chapter5.search, and chapter5.search-\napigateway are copied and renamed as chapter6.*, respectively.\nPerform the following steps to implement the custom life cycle manager:\n1.\t Create a new Spring Boot application and name it chapter6.\nlifecyclemanager. The project structure is shown in the following diagram:\n\n\nAutoscaling Microservices\n[ 278 ]\nThe flowchart for this example is as shown in the following diagram:\nThe components of this diagram are explained in details here.\n2.\t Create a MetricsCollector class with the following method. At the \nstartup of the Spring Boot application, this method will be invoked using \nCommandLineRunner, as follows:\npublic void start(){\n  while(true){ \n    eurekaClient.getServices().forEach(service -> {        System.\nout.println(\"discovered service \"+ service);\n      Map metrics = restTemplate.getForObject(\"http://\"+service+\"/\nmetrics\",Map.class);\n      decisionEngine.execute(service, metrics);\n    });  \n  }    \n}\nThe preceding method looks for the services registered in the Eureka  \nserver and gets all the instances. In the real world, rather than polling, \nthe instances should publish metrics to a common place, where metrics \naggregation will happen.\n\n\nChapter 6\n[ 279 ]\n3.\t The following DecisionEngine code accepts the metric and applies certain \nscaling policies to determine whether the service requires scaling up or not:\n  public boolean execute(String serviceId, Map metrics){\n  if(scalingPolicies.getPolicy(serviceId). \n    execute(serviceId, metrics)){    \n      return deploymentEngine.scaleUp(deploymentRules.\ngetDeploymentRules(serviceId), serviceId);  \n    }\n    return false;\n  }\n4.\t Based on the service ID, the policies that are related to the services will \nbe picked up and applied. In this case, a minimal TPM scaling policy is \nimplemented in TpmScalingPolicy, as follows:\npublic class TpmScalingPolicy implements ScalingPolicy {\n  public boolean execute(String serviceId, Map metrics){\n    if(metrics.containsKey(\"gauge.servo.tpm\")){\n      Double tpm = (Double) metrics.get(\"gauge.servo.tpm\");\n      System.out.println(\"gauge.servo.tpm \" + tpm);\n      return (tpm > 10);\n    }\n    return false;\n  }\n}\n5.\t If the policy returns true, DecisionEngine then invokes \nDeploymentEngine to spin up another instance. DeploymentEngine  \nmakes use of DeploymentRules to decide how to execute scaling. The  \nrules can enforce the number of min and max instances, in which region  \nor machine the new instance has to be started, the resources required for \nthe new instance, and so on. DummyDeploymentRule simply makes sure  \nthe max instance is not more than 2.\n6.\t\nDeploymentEngine, in this case, uses the JSch (Java Secure Channel) library \nfrom JCraft to SSH to the destination server and start the service. This requires \nthe following additional Maven dependency:\n<dependency>\n    <groupId>com.jcraft</groupId>\n    <artifactId>jsch</artifactId>\n    <version>0.1.53</version>\n</dependency>\n\n\nAutoscaling Microservices\n[ 280 ]\n7.\t The current SSH implementation is kept simple enough as we will change \nthis in future chapters. In this example, DeploymentEngine sends the \nfollowing command over the SSH library on the target machine:\n String command =\"java -jar -Dserver.port=8091 ./work/codebox/\nchapter6/chapter6.search/target/search-1.0.jar\";\nIntegration with Nexus happens from the target machine using Linux scripts \nwith Nexus CLI or using curl. In this example, we will not explore Nexus.\n8.\t The next step is to change the Search microservice to expose a new gauge for \nTPM. We have to change all the microservices developed earlier to submit \nthis additional metric.\nWe will only examine Search in this chapter, but in order to complete it, \nall the services have to be updated. In order to get the gauge.servo.tpm \nmetrics, we have to add TPMCounter to all the microservices.\nThe following code counts the transactions over a sliding window of 1 \nminute:\nclass TPMCounter {\n  LongAdder count;\n  Calendar expiry = null; \n  TPMCounter(){\n    reset();\n  }  \n  void reset (){\n    count = new LongAdder();\n    expiry = Calendar.getInstance();\n    expiry.add(Calendar.MINUTE, 1);\n  }\n  boolean isExpired(){\n    return Calendar.getInstance().after(expiry);\n  }\n  void increment(){\n     if(isExpired()){\n       reset();\n     }\n     count.increment();\n  }\n}\n\n\nChapter 6\n[ 281 ]\n9.\t The following code needs to be added to SearchController to set the  \ntpm value:\nclass SearchRestController {\n  TPMCounter tpm = new TPMCounter();\n  @Autowired\n  GaugeService gaugeService;\n   //other code \n10.\t The following code is from the get REST endpoint (the search method) of \nSearchRestController, which submits the tpm value as a gauge to the \nactuator endpoint:\ntpm.increment();\ngaugeService.submit(\"tpm\", tpm.count.intValue()); \nRunning the life cycle manager\nPerform the following steps to run the life cycle manager developed in the  \nprevious section:\n1.\t Edit DeploymentEngine.java and update the password to reflect the \nmachine's password, as follows. This is required for the SSH connection:\nsession.setPassword(\"rajeshrv\");\n2.\t Build all the projects by running Maven from the root folder (Chapter 6)  \nvia the following command:\nmvn -Dmaven.test.skip=true clean install\n3.\t Then, run RabbitMQ, as follows:\n./rabbitmq-server\n4.\t Ensure that the Config server is pointing to the right configuration \nrepository. We need to add a property file for the life cycle manager.\n5.\t Run the following commands from the respective project folders:\njava -jar target/config-server-0.0.1-SNAPSHOT.jar\njava -jar target/eureka-server-0.0.1-SNAPSHOT.jar\njava -jar target/lifecycle-manager-0.0.1-SNAPSHOT.jar\njava -jar target/search-1.0.jar\njava -jar target/search-apigateway-1.0.jar\njava -jar target/website-1.0.jar\n\n\nAutoscaling Microservices\n[ 282 ]\n6.\t Once all the services are started, open a browser window and load  \nhttp://localhost:8001.\n7.\t Execute the flight search 11 times, one after the other, within a minute.  \nThis will trigger the decision engine to instantiate another instance  \nof the Search microservice.\n8.\t Open the Eureka console (http://localhost:8761) and watch for a  \nsecond SEARCH-SERVICE. Once the server is started, the instances  \nwill appear as shown here:\nSummary\nIn this chapter, you learned the importance of autoscaling when deploying  \nlarge-scale microservices.\nWe also explored the concept of autoscaling and the different models of  \nand approaches to autoscaling, such as the time-based, resource-based, queue \nlength-based, and predictive ones. We then reviewed the role of a life cycle \nmanager in the context of microservices and reviewed its capabilities. Finally,  \nwe ended this chapter by reviewing a sample implementation of a simple  \ncustom life cycle manager in the context of BrownField PSS microservices.\nAutoscaling is an important supporting capability required when dealing with  \nlarge-scale microservices. We will discuss a more mature implementation of  \nthe life cycle manager in Chapter 9, Managing Dockerized Microservices with  \nMesos and Marathon.\nThe next chapter will explore the logging and monitoring capabilities that are \nindispensable for successful microservice deployments.\n\n\n[ 283 ]\nLogging and Monitoring \nMicroservices\nOne of the biggest challenges due to the very distributed nature of Internet-scale \nmicroservices deployment is the logging and monitoring of individual microservices. \nIt is difficult to trace end-to-end transactions by correlating logs emitted by different \nmicroservices. As with monolithic applications, there is no single pane of glass to \nmonitor microservices.\nThis chapter will cover the necessity and importance of logging and monitoring in \nmicroservice deployments. This chapter will further examine the challenges and \nsolutions to address logging and monitoring with a number of potential architectures \nand technologies.\nBy the end of this chapter, you will learn about:\n•\t\nThe different options, tools, and technologies for log management\n•\t\nThe use of Spring Cloud Sleuth in tracing microservices\n•\t\nThe different tools for end-to-end monitoring of microservices\n•\t\nThe use of Spring Cloud Hystrix and Turbine for circuit monitoring\n•\t\nThe use of data lakes in enabling business data analysis\n\n\nLogging and Monitoring Microservices\n[ 284 ]\nReviewing the microservice capability \nmodel\nIn this chapter, we will explore the following microservice capabilities from the \nmicroservices capability model discussed in Chapter 3, Applying Microservices Concepts:\n•\t\nCentral Log Management\n•\t\nMonitoring and Dashboards\n•\t\nDependency Management (part of Monitoring and Dashboards)\n•\t\nData Lake\nUnderstanding log management \nchallenges\nLogs are nothing but streams of events coming from a running process. For \ntraditional JEE applications, a number of frameworks and libraries are available to \nlog. Java Logging (JUL) is an option off the shelf from Java itself. Log4j, Logback, \nand SLF4J are some of the other popular logging frameworks available. These \nframeworks support both UDP as well as TCP protocols for logging. Applications \nsend log entries to the console or to the filesystem. File recycling techniques are \ngenerally employed to avoid logs filling up all the disk space.\n\n\nChapter 7\n[ 285 ]\nOne of the best practices of log handling is to switch off most of the log entries in \nproduction due to the high cost of disk IOs. Not only do disk IOs slow down the \napplication, but they can also severely impact scalability. Writing logs into the disk \nalso requires high disk capacity. An out-of-disk-space scenario can bring down the \napplication. Logging frameworks provide options to control logging at runtime to \nrestrict what is to be printed and what not. Most of these frameworks provide fine-\ngrained control over the logging controls. They also provide options to change these \nconfigurations at runtime.\nOn the other hand, logs may contain important information and have high value if \nproperly analyzed. Therefore, restricting log entries essentially limits our ability to \nunderstand the application's behavior.\nWhen moved from traditional to cloud deployment, applications are no longer \nlocked to a particular, predefined machine. Virtual machines and containers are not \nhardwired with an application. The machines used for deployment can change from \ntime to time. Moreover, containers such as Docker are ephemeral. This essentially \nmeans that one cannot rely on the persistent state of the disk. Logs written to the disk \nare lost once the container is stopped and restarted. Therefore, we cannot rely on the \nlocal machine's disk to write log files.\nAs we discussed in Chapter 1, Demystifying Microservices, one of the principles of the \nTwelve-Factor app is to avoid routing or storing log files by the application itself. In \nthe context of microservices, they will run on isolated physical or virtual machines, \nresulting in fragmented log files. In this case, it is almost impossible to trace end-to-\nend transactions that span multiple microservices:\n\n\nLogging and Monitoring Microservices\n[ 286 ]\nAs shown in the diagram, each microservice emits logs to a local filesystem. In \nthis case, microservice M1 calls M3. These services write their logs to their own \nlocal filesystems. This makes it harder to correlate and understand the end-to-end \ntransaction flow. Also, as shown in the diagram, there are two instances of M1 and \ntwo instances of M2 running on two different machines. In this case, log aggregation \nat the service level is hard to achieve.\nA centralized logging solution\nIn order to address the challenges stated earlier, traditional logging solutions \nrequire serious rethinking. The new logging solution, in addition to addressing the \npreceding challenges, is also expected to support the capabilities summarized here:\n•\t\nThe ability to collect all log messages and run analytics on top of the log \nmessages\n•\t\nThe ability to correlate and track transactions end to end\n•\t\nThe ability to keep log information for longer time periods for trending and \nforecasting\n•\t\nThe ability to eliminate dependency on the local disk system\n•\t\nThe ability to aggregate log information coming from multiple sources such \nas network devices, operating system, microservices, and so on\nThe solution to these problems is to centrally store and analyze all log messages, \nirrespective of the source of log. The fundamental principle employed in the new \nlogging solution is to detach log storage and processing from service execution \nenvironments. Big data solutions are better suited to storing and processing large \nnumbers of log messages more effectively than storing and processing them in \nmicroservice execution environments.\nIn the centralized logging solution, log messages will be shipped from the execution \nenvironment to a central big data store. Log analysis and processing will be handled \nusing big data solutions:\n",
      "page_number": 288
    },
    {
      "number": 7,
      "title": "[ 285 ]",
      "start_page": 312,
      "end_page": 341,
      "detection_method": "regex_chapter",
      "content": "Chapter 7\n[ 287 ]\nAs shown in the preceding logical diagram, there are a number of components in the \ncentralized logging solution, as follows:\n•\t\nLog streams: These are streams of log messages coming out of source \nsystems. The source system can be microservices, other applications, or \neven network devices. In typical Java-based systems, these are equivalent to \nstreaming Log4j log messages.\n•\t\nLog shippers: Log shippers are responsible for collecting the log messages \ncoming from different sources or endpoints. The log shippers then send these \nmessages to another set of endpoints, such as writing to a database, pushing \nto a dashboard, or sending it to stream-processing endpoint for further real-\ntime processing.\n•\t\nLog store: A log store is the place where all log messages are stored for real-\ntime analysis, trending, and so on. Typically, a log store is a NoSQL database, \nsuch as HDFS, capable of handling large data volumes.\n•\t\nLog stream processor: The log stream processor is capable of analyzing real-\ntime log events for quick decision making. A stream processor takes actions \nsuch as sending information to a dashboard, sending alerts, and so on. In \nthe case of self-healing systems, stream processors can even take actions to \ncorrect the problems.\n•\t\nLog dashboard: A dashboard is a single pane of glass used to display log \nanalysis results such as graphs and charts. These dashboards are meant for \nthe operational and management staff.\nThe benefit of this centralized approach is that there is no local I/O or blocking \ndisk writes. It also does not use the local machine's disk space. This architecture is \nfundamentally similar to the lambda architecture for big data processing.\nTo read more on the Lambda architecture, go to http://lambda-\narchitecture.net.\nIt is important to have in each log message a context, message, and correlation ID. \nThe context typically has the timestamp, IP address, user information, process details \n(such as service, class, and functions), log type, classification, and so on. The message \nwill be plain and simple free text information. The correlation ID is used to establish \nthe link between service calls so that calls spanning microservices can be traced.\n\n\nLogging and Monitoring Microservices\n[ 288 ]\nThe selection of logging solutions\nThere are a number of options available to implement a centralized logging solution. \nThese solutions use different approaches, architectures, and technologies. It is \nimportant to understand the capabilities required and select the right solution that \nmeets the needs.\nCloud services\nThere are a number of cloud logging services available, such as the SaaS solution.\nLoggly is one of the most popular cloud-based logging services. Spring Boot \nmicroservices can use Loggly's Log4j and Logback appenders to directly stream log \nmessages into the Loggly service.\nIf the application or service is deployed in AWS, AWS CloudTrail can be integrated \nwith Loggly for log analysis.\nPapertrial, Logsene, Sumo Logic, Google Cloud Logging, and Logentries are \nexamples of other cloud-based logging solutions.\nThe cloud logging services take away the overhead of managing complex \ninfrastructures and large storage solutions by providing them as simple-to-integrate \nservices. However, latency is one of the key factors to be considered when selecting \ncloud logging as a service.\nOff-the-shelf solutions\nThere are many purpose-built tools to provide end-to-end log management \ncapabilities that are installable locally in an on-premises data center or in the cloud.\nGraylog is one of the popular open source log management solutions. Graylog uses \nElasticsearch for log storage and MongoDB as a metadata store. Graylog also uses \nGELF libraries for Log4j log streaming.\nSplunk is one of the popular commercial tools available for log management and \nanalysis. Splunk uses the log file shipping approach, compared to log streaming used \nby other solutions to collect logs.\nBest-of-breed integration\nThe last approach is to pick and choose best-of-breed components and build a \ncustom logging solution.\n\n\nChapter 7\n[ 289 ]\nLog shippers\nThere are log shippers that can be combined with other tools to build an end-to-end \nlog management solution. The capabilities differ between different log shipping \ntools.\nLogstash is a powerful data pipeline tool that can be used to collect and ship log \nfiles. Logstash acts as a broker that provides a mechanism to accept streaming \ndata from different sources and sync them to different destinations. Log4j and \nLogback appenders can also be used to send log messages directly from Spring Boot \nmicroservices to Logstash. The other end of Logstash is connected to Elasticsearch, \nHDFS, or any other database.\nFluentd is another tool that is very similar to Logstash, as is Logspout, but the latter \nis more appropriate in a Docker container-based environment.\nLog stream processors\nStream-processing technologies are optionally used to process log streams on the \nfly. For example, if a 404 error is continuously occurring as a response to a particular \nservice call, it means there is something wrong with the service. Such situations have \nto be handled as soon as possible. Stream processors are pretty handy in such cases \nas they are capable of reacting to certain streams of events that a traditional reactive \nanalysis can't.\nA typical architecture used for stream processing is a combination of Flume and \nKafka together with either Storm or Spark Streaming. Log4j has Flume appenders, \nwhich are useful to collect log messages. These messages are pushed into distributed \nKafka message queues. The stream processors collect data from Kafka and process \nthem on the fly before sending it to Elasticsearch and other log stores.\nSpring Cloud Stream, Spring Cloud Stream Modules, and Spring Cloud Data Flow \ncan also be used to build the log stream processing.\nLog storage\nReal-time log messages are typically stored in Elasticsearch. Elasticsearch allows \nclients to query based on text-based indexes. Apart from Elasticsearch, HDFS is also \ncommonly used to store archived log messages. MongoDB or Cassandra is used to \nstore summary data, such as monthly aggregated transaction counts. Offline log \nprocessing can be done using Hadoop's MapReduce programs.\n\n\nLogging and Monitoring Microservices\n[ 290 ]\nDashboards\nThe last piece required in the central logging solution is a dashboard. The most \ncommonly used dashboard for log analysis is Kibana on top of an Elasticsearch data \nstore. Graphite and Grafana are also used to display log analysis reports.\nA custom logging implementation\nThe tools mentioned before can be leveraged to build a custom end-to-end logging \nsolution. The most commonly used architecture for custom log management is a \ncombination of Logstash, Elasticsearch, and Kibana, also known as the ELK stack.\nThe full source code of this chapter is available under the Chapter \n7 project in the code files. Copy chapter5.configserver, \nchapter5.eurekaserver, chapter5.search, chapter5.\nsearch-apigateway, and chapter5.website into a new STS \nworkspace and rename them chapter7.*.\nThe following diagram shows the log monitoring flow:\nIn this section, a simple implementation of a custom logging solution using the ELK \nstack will be examined.\nFollow these steps to implement the ELK stack for logging:\n1.\t Download and install Elasticsearch, Kibana, and Logstash from https://\nwww.elastic.co.\n2.\t Update the Search microservice (chapter7.search). Review and ensure that \nthere are some log statements in the Search microservice. The log statements \nare nothing special but simple log statements using slf4j, as follows:\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n  //other code goes here\n  private static final Logger logger = LoggerFactory. \n    getLogger(SearchRestController.class);\n//other code goes here\n  \n\n\nChapter 7\n[ 291 ]\nlogger.info(\"Looking to load flights...\");\nfor (Flight flight : flightRepository. \n  findByOriginAndDestinationAndFlightDate \n  (\"NYC\", \"SFO\", \"22-JAN-16\")) {\n      logger.info(flight.toString());\n}\n3.\t Add the logstash dependency to integrate logback to Logstash in the \nSearch service's pom.xml file, as follows:\n<dependency>\n  <groupId>net.logstash.logback</groupId>\n  <artifactId>logstash-logback-encoder</artifactId>\n  <version>4.6</version>\n</dependency>\n4.\t Also, downgrade the logback version to be compatible with Spring \n1.3.5.RELEASE via the following line:\n<logback.version>1.1.6</logback.version>\n5.\t Override the default Logback configuration. This can be done by adding a \nnew logback.xml file under src/main/resources, as follows:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n    <include resource=\"org/springframework/boot/logging/logback/\ndefaults.xml\"/>\n  <include resource=\"org/springframework/boot/logging/logback/\nconsole-appender.xml\" />\n    <appender name=\"stash\" class=\"net.logstash.logback. \n      appender.LogstashTcpSocketAppender\">\n        <destination>localhost:4560</destination>\n        <!-- encoder is required -->\n        <encoder class=\"net.logstash.logback.encoder. \n          LogstashEncoder\" />\n    </appender>\n  <root level=\"INFO\">\n    <appender-ref ref=\"CONSOLE\" />\n    <appender-ref ref=\"stash\" />\n  </root>\n</configuration>\nThe preceding configuration overrides the default Logback configuration by \nadding a new TCP socket appender, which streams all the log messages to \na Logstash service, which is listening on port 4560. It is important to add an \nencoder, as mentioned in the previous configuration.\n\n\nLogging and Monitoring Microservices\n[ 292 ]\n6.\t Create a configuration as shown in the following code and store it in a \nlogstash.conf file. The location of this file is irrelevant as it will be passed \nas an argument when starting Logstash. This configuration will take input \nfrom the socket listening on 4560 and send the output to Elasticsearch \nrunning on 9200. The stdout is optional and is set to debug:\ninput {\n  tcp {\n     port => 4560\n     host => localhost\n  }\n}\noutput {\nelasticsearch { hosts => [\"localhost:9200\"] }\n  stdout { codec => rubydebug }\n}\n7.\t Run Logstash, Elasticsearch, and Kibana from their respective installation \nfolders, as follows:\n./bin/logstash -f logstash.conf\n./bin/elasticsearch\n./bin/kibana\n8.\t Run the Search microservice. This will invoke the unit test cases and result in \nprinting the log statements mentioned before.\n9.\t Go to a browser and access Kibana, at http://localhost:5601.\n10.\t Go to Settings | Configure an index pattern, as shown here:\n\n\nChapter 7\n[ 293 ]\n11.\t Go to the Discover menu to see the logs. If everything is successful, we \nwill see the Kibana screenshot as follows. Note that the log messages are \ndisplayed in the Kibana screen.\nKibana provides out-of-the-box features to build summary charts and graphs \nusing log messages:\nDistributed tracing with Spring Cloud Sleuth\nThe previous section addressed microservices' distributed and fragmented logging \nissue by centralizing the log data. With the central logging solution, we can have all \nthe logs in a central storage. However, it is still almost impossible to trace end-to-end \ntransactions. In order to do end-to-end tracking, transactions spanning microservices \nneed to have a correlation ID.\nTwitter's Zipkin, Cloudera's HTrace, and Google's Dapper systems are examples of \ndistributed tracing systems. Spring Cloud provides a wrapper component on top of \nthese using the Spring Cloud Sleuth library.\n\n\nLogging and Monitoring Microservices\n[ 294 ]\nDistributed tracing works with the concepts of span and trace. The span is a unit of \nwork; for example, calling a service is identified by a 64-bit span ID. A set of spans \nform a tree-like structure is called a trace. Using the trace ID, the call can be tracked \nend to end:\nAs shown in the diagram, Microservice 1 calls Microservice 2, and Microservice \n2 calls Microservice 3. In this case, as shown in the diagram, the same trace ID is \npassed across all microservices, which can be used to track transactions end to end.\nIn order to demonstrate this, we will use the Search API Gateway and Search \nmicroservices. A new endpoint has to be added in Search API Gateway (chapter7.\nsearch-apigateway) that internally calls the Search service to return data. Without \nthe trace ID, it is almost impossible to trace or link calls coming from the Website to \nSearch API Gateway to Search microservice. In this case, it only involves two to three \nservices, whereas in a complex environment, there could be many interdependent \nservices.\nFollow these steps to create the example using Sleuth:\n1.\t Update Search and Search API Gateway. Before this, the Sleuth dependency \nneeds to be added to the respective POM files, which can be done via the \nfollowing code:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-sleuth</artifactId>\n</dependency>\n2.\t In the case of building a new service, select Sleuth and Web, as shown here:\n\n\nChapter 7\n[ 295 ]\n3.\t Add the Logstash dependency to the Search service as well as the Logback \nconfiguration, as in the previous example.\n4.\t The next step is to add two more properties in the Logback configuration,  \nas follows:\n<property name=\"spring.application.name\" value=\"search-service\"/>\n<property name=\"CONSOLE_LOG_PATTERN\" value=\"%d{yyyy-MM-dd \nHH:mm:ss.SSS} [${spring.application.name}] [trace=%X{X-Trace-Id:-\n},span=%X{X-Span-Id:-}] [%15.15t] %-40.40logger{39}: %m%n\"/>\nThe first property is the name of the application. The names given in this are \nthe service IDs: search-service and search-apigateway in Search and \nSearch API Gateway, respectively. The second property is an optional pattern \nused to print the console log messages with a trace ID and span ID. The \npreceding change needs to be applied to both the services.\n5.\t Add the following piece of code to advise Sleuth when to start a new span ID \nin the Spring Boot Application class. In this case, AlwaysSampler is used to \nindicate that the span ID has to be created every time a call hits the service. \nThis change needs to be applied in both the services:\n  @Bean\n    public AlwaysSampler defaultSampler() {\n      return new AlwaysSampler();\n    }\n\n\nLogging and Monitoring Microservices\n[ 296 ]\n6.\t Add a new endpoint to Search API Gateway, which will call the Search \nservice as follows. This is to demonstrate the propagation of the trace ID \nacross multiple microservices. This new method in the gateway returns the \noperating hub of the airport by calling the Search service, as follows:\n  @RequestMapping(\"/hubongw\")\n  String getHub(HttpServletRequest req){\n    logger.info(\"Search Request in API gateway for getting Hub, \nforwarding to search-service \");\n    String hub = restTemplate.getForObject(\"http://search-service/\nsearch/hub\", String.class);\n    logger.info(\"Response for hub received,  Hub \"+ hub);\n    return hub; \n  }\n7.\t Add another endpoint in the Search service, as follows:\n  @RequestMapping(\"/hub\")\n  String getHub(){\n    logger.info(\"Searching for Hub, received from search-\napigateway \");\n    return \"SFO\"; \n  }\n8.\t Once added, run both the services. Hit the gateway's new hub on the \ngateway (/hubongw) endpoint using a browser ( http://localhost:8095/\nhubongw).\nAs mentioned earlier, the Search API Gateway service is running on 8095 \nand the Search service is running on 8090.\n9.\t Look at the console logs to see the trace ID and span IDs printed. The first \nprint is from Search API Gateway, and the second one came from the Search \nservice. Note that the trace IDs are the same in both the cases, as follows:\n2016-04-02 17:24:37.624 [search-apigateway] [trace=8a7e278f-7b2b-\n43e3-a45c-69d3ca66d663,span=8a7e278f-7b2b-43e3-a45c-69d3ca66d663] \n[io-8095-exec-10] c.b.p.s.a.SearchAPIGatewayController    : \nResponse for hub received,  Hub DXB\n2016-04-02 17:24:37.612 [search-service] [trace=8a7e278f-7b2b-\n43e3-a45c-69d3ca66d663,span=fd309bba-5b4d-447f-a5e1-7faaab90cfb1] \n[nio-8090-exec-1] c.b.p.search.component.SearchComponent  : \nSearching for Hub, received from search-apigateway\n\n\nChapter 7\n[ 297 ]\n10.\t Open the Kibana console and search for the trace ID using this trace ID \nprinted in the console. In this case, it is 8a7e278f-7b2b-43e3-a45c-\n69d3ca66d663. As shown in the following screenshot, with a trace ID,  \none can trace service calls that span multiple services:\nMonitoring microservices\nMicroservices are truly distributed systems with a fluid deployment topology. \nWithout sophisticated monitoring in place, operations teams may run into trouble \nmanaging large-scale microservices. Traditional monolithic application deployments \nare limited to a number of known services, instances, machines, and so on. This \nis easier to manage compared to the large number of microservices instances \npotentially running across different machines. To add more complication, these \nservices dynamically change their topologies. A centralized logging capability only \naddresses part of the issue. It is important for operations teams to understand the \nruntime deployment topology and also the behavior of the systems. This demands \nmore than a centralized logging can offer.\nIn general application, monitoring is more a collection of metrics, aggregation, and \ntheir validation against certain baseline values. If there is a service-level breach, \nthen monitoring tools generate alerts and send them to administrators. With \nhundreds and thousands of interconnected microservices, traditional monitoring \ndoes not really offer true value. The one-size-fits-all approach to monitoring or \nmonitoring everything with a single pane of glass is not easy to achieve in large-scale \nmicroservices.\n\n\nLogging and Monitoring Microservices\n[ 298 ]\nOne of the main objectives of microservice monitoring is to understand the behavior \nof the system from a user experience point of view. This will ensure that the end-to-\nend behavior is consistent and is in line with what is expected by the users.\nMonitoring challenges\nSimilar to the fragmented logging issue, the key challenge in monitoring \nmicroservices is that there are many moving parts in a microservice ecosystem.\nThe typical issues are summarized here:\n•\t\nThe statistics and metrics are fragmented across many services, instances, \nand machines.\n•\t\nHeterogeneous technologies may be used to implement microservices, which \nmakes things even more complex. A single monitoring tool may not give all \nthe required monitoring options.\n•\t\nMicroservices deployment topologies are dynamic, making it impossible to \npreconfigure servers, instances, and monitoring parameters.\nMany of the traditional monitoring tools are good to monitor monolithic applications \nbut fall short in monitoring large-scale, distributed, interlinked microservice systems. \nMany of the traditional monitoring systems are agent-based preinstall agents on the \ntarget machines or application instances. This poses two challenges:\n•\t\nIf the agents require deep integration with the services or operating systems, \nthen this will be hard to manage in a dynamic environment\n•\t\nIf these tools impose overheads when monitoring or instrumenting the \napplication, it may lead to performance issues\nMany traditional tools need baseline metrics. Such systems work with preset rules, \nsuch as if the CPU utilization goes above 60% and remains at this level for 2 minutes, \nthen an alert should be sent to the administrator. It is extremely hard to preconfigure \nthese values in large, Internet-scale deployments.\nNew-generation monitoring applications learn the application's behavior by \nthemselves and set automatic threshold values. This frees up administrators from \ndoing this mundane task. Automated baselines are sometimes more accurate than \nhuman forecasts:\n\n\nChapter 7\n[ 299 ]\nAs shown in the diagram, the key areas of microservices monitoring are:\n•\t\nMetrics sources and data collectors: Metrics collection at the source is done \neither by the server pushing metrics information to a central collector or \nby embedding lightweight agents to collect information. Data collectors \ncollect monitoring metrics from different sources, such as network, physical \nmachines, containers, software components, applications, and so on. The \nchallenge is to collect this data using autodiscovery mechanisms instead of \nstatic configurations.\nThis is done by either running agents on the source machines, streaming data \nfrom the sources, or polling at regular intervals.\n•\t\nAggregation and correlation of metrics: Aggregation capability is required \nfor aggregating metrics collected from different sources, such as user \ntransaction, service, infrastructure, network, and so on. Aggregation can be \nchallenging as it requires some level of understanding of the application's \nbehavior, such as service dependencies, service grouping, and so on. In many \ncases, these are automatically formulated based on the metadata provided by \nthe sources.\nGenerally, this is done by an intermediary that accept the metrics.\n•\t\nProcessing metrics and actionable insights: Once data is aggregated, the \nnext step is to do the measurement. Measurements are typically done using \nset thresholds. In the new-generation monitoring systems, these thresholds \nare automatically discovered. Monitoring tools then analyze the data and \nprovide actionable insights.\nThese tools may use big data and stream analytics solutions.\n•\t\nAlerting, actions, and dashboards: As soon as issues are detected, they have \nto be notified to the relevant people or systems. Unlike traditional systems, \nthe microservices monitoring systems should be capable of taking actions on \na real-time basis. Proactive monitoring is essential to achieving self-healing. \nDashboards are used to display SLAs, KPIs, and so on.\nDashboards and alerting tools are capable of handling these requirements.\n\n\nLogging and Monitoring Microservices\n[ 300 ]\nMicroservice monitoring is typically done with three approaches. A combination of \nthese is really required for effective monitoring:\n•\t\nApplication performance monitoring (APM): This is more of a traditional \napproach to system metrics collection, processing, alerting, and dashboard \nrendering. These are more from the system's point of view. Application \ntopology discovery and visualization are new capabilities implemented \nby many of the APM tools. The capabilities vary between different APM \nproviders.\n•\t\nSynthetic monitoring: This is a technique that is used to monitor the \nsystem's behavior using end-to-end transactions with a number of test \nscenarios in a production or production-like environment. Data is collected to \nvalidate the system's behavior and potential hotspots. Synthetic monitoring \nhelps understand the system dependencies as well.\n•\t\nReal user monitoring (RUM) or user experience monitoring: This is \ntypically a browser-based software that records real user statistics, such as \nresponse time, availability, and service levels. With microservices, with more \nfrequent release cycle and dynamic topology, user experience monitoring is \nmore important.\nMonitoring tools\nThere are many tools available to monitor microservices. There are also overlaps \nbetween many of these tools. The selection of monitoring tools really depends upon \nthe ecosystem that needs to be monitored. In most cases, more than one tool is \nrequired to monitor the overall microservice ecosystem.\nThe objective of this section is to familiarize ourselves with a number of common \nmicroservices-friendly monitoring tools:\n•\t\nAppDynamics, Dynatrace, and New Relic are top commercial vendors \nin the APM space, as per Gartner Magic Quadrant 2015. These tools are \nmicroservice friendly and support microservice monitoring effectively \nin a single console. Ruxit, Datadog, and Dataloop are other commercial \nofferings that are purpose-built for distributed systems that are essentially \nmicroservices friendly. Multiple monitoring tools can feed data to Datadog \nusing plugins.\n•\t\nCloud vendors come with their own monitoring tools, but in many \ncases, these monitoring tools alone may not be sufficient for large-scale \nmicroservice monitoring. For instance, AWS uses CloudWatch and Google \nCloud Platform uses Cloud Monitoring to collect information from various \nsources.\n\n\nChapter 7\n[ 301 ]\n•\t\nSome of the data collecting libraries, such as Zabbix, statd, collectd, jmxtrans, \nand so on operate at a lower level in collecting runtime statistics, metrics, \ngauges, and counters. Typically, this information is fed into data collectors \nand processors such as Riemann, Datadog, and Librato, or dashboards such \nas Graphite.\n•\t\nSpring Boot Actuator is one of the good vehicles to collect microservices \nmetrics, gauges, and counters, as we discussed in Chapter 2, Building \nMicroservices with Spring Boot. Netflix Servo, a metric collector similar to \nActuator, and the QBit and Dropwizard metrics also fall in the same category \nof metric collectors. All these metrics collectors need an aggregator and \ndashboard to facilitate full-sized monitoring.\n•\t\nMonitoring through logging is popular but a less effective approach \nin microservices monitoring. In this approach, as discussed in the \nprevious section, log messages are shipped from various sources, such as \nmicroservices, containers, networks, and so on to a central location. Then, \nwe can use the logs files to trace transactions, identify hotspots, and so on. \nLoggly, ELK, Splunk, and Trace are candidates in this space.\n•\t\nSensu is a popular choice for microservice monitoring from the open source \ncommunity. Weave Scope is another tool, primarily targeting containerized \ndeployments. Spigo is one of the purpose-built microservices monitoring \nsystems closely aligned with the Netflix stack.\n•\t\nPingdom, New Relic Synthetics, Runscope, Catchpoint, and so on provide \noptions for synthetic transaction monitoring and user experience monitoring \non live systems.\n•\t\nCirconus is classified more as a DevOps monitoring tool but can also do \nmicroservices monitoring. Nagios is a popular open source monitoring tool \nbut falls more into the traditional monitoring system.\n•\t\nPrometheus provides a time series database and visualization GUI useful in \nbuilding custom monitoring tools.\nMonitoring microservice dependencies\nWhen there are a large number of microservices with dependencies, it is important \nto have a monitoring tool that can show the dependencies among microservices. It is \nnot a scalable approach to statically configure and manage these dependencies. There \nare many tools that are useful in monitoring microservice dependencies, as follows:\n•\t\nMentoring tools such as AppDynamics, Dynatrace, and New Relic can draw \ndependencies among microservices. End-to-end transaction monitoring can \nalso trace transaction dependencies. Other monitoring tools, such as Spigo, \nare also useful for microservices dependency management.\n\n\nLogging and Monitoring Microservices\n[ 302 ]\n•\t\nCMDB tools such as Device42 or purpose-built tools such as Accordance are \nuseful in managing the dependency of microservices. Veritas Risk Advisor \n(VRA) is also useful for infrastructure discovery.\n•\t\nA custom implementation with a Graph database, such as Neo4j, is also \nuseful. In this case, a microservice has to preconfigure its direct and indirect \ndependencies. At the startup of the service, it publishes and cross-checks its \ndependencies with a Neo4j database.\nSpring Cloud Hystrix for fault-tolerant \nmicroservices\nThis section will explore Spring Cloud Hystrix as a library for a fault-tolerant and \nlatency-tolerant microservice implementation. Hystrix is based on the fail fast and \nrapid recovery principles. If there is an issue with a service, Hystrix helps isolate it. It \nhelps to recover quickly by falling back to another preconfigured fallback service. \nHystrix is another battle-tested library from Netflix. Hystrix is based on the circuit \nbreaker pattern.\nRead more about the circuit breaker pattern at https://msdn.\nmicrosoft.com/en-us/library/dn589784.aspx.\nIn this section, we will build a circuit breaker with Spring Cloud Hystrix. Perform \nthe following steps to change the Search API Gateway service to integrate it with \nHystrix:\n1.\t Update the Search API Gateway service. Add the Hystrix dependency to the \nservice. If developing from scratch, select the following libraries:\n\n\nChapter 7\n[ 303 ]\n2.\t In the Spring Boot Application class, add @EnableCircuitBreaker.  \nThis command will tell Spring Cloud Hystrix to enable a circuit breaker  \nfor this application. It also exposes the /hystrix.stream endpoint for \nmetrics collection.\n3.\t Add a component class to the Search API Gateway service with a method; \nin this case, this is getHub annotated with @HystrixCommand. This tells \nSpring that this method is prone to failure. Spring Cloud libraries wrap these \nmethods to handle fault tolerance and latency tolerance by enabling circuit \nbreaker. The Hystrix command typically follows with a fallback method. In \ncase of failure, Hystrix automatically enables the fallback method mentioned \nand diverts traffic to the fallback method. As shown in the following code, in \nthis case, getHub will fall back to getDefaultHub:\n@Component  \nclass SearchAPIGatewayComponent { \n  @LoadBalanced\n  @Autowired \n  RestTemplate restTemplate;\n\n\nLogging and Monitoring Microservices\n[ 304 ]\n  @HystrixCommand(fallbackMethod = \"getDefaultHub\")\n  public String getHub(){\n    String hub = restTemplate.getForObject(\"http://search-service/\nsearch/hub\", String.class);\n    return hub;\n  }\n  public String getDefaultHub(){\n    return \"Possibily SFO\";\n  }\n}\n4.\t The getHub method of SearchAPIGatewayController calls the getHub \nmethod of SearchAPIGatewayComponent, as follows:\n@RequestMapping(\"/hubongw\") \nString getHub(){\n  logger.info(\"Search Request in API gateway for getting Hub, \nforwarding to search-service \");\n  return component.getHub(); \n} \n5.\t The last part of this exercise is to build a Hystrix Dashboard. For this, build \nanother Spring Boot application. Include Hystrix, Hystrix Dashboard, and \nActuator when building this application.\n6.\t In the Spring Boot Application class, add the @EnableHystrixDashboard \nannotation.\n7.\t Start the Search service, Search API Gateway, and Hystrix Dashboard \napplications. Point the browser to the Hystrix Dashboard application's URL. \nIn this example, the Hystrix Dashboard is started on port 9999. So, open the \nURL http://localhost:9999/hystrix.\n8.\t A screen similar to the following screenshot will be displayed. In the Hystrix \nDashboard, enter the URL of the service to be monitored.\nIn this case, Search API Gateway is running on port 8095. Hence, the \nhystrix.stream URL will be http://localhost:8095/hytrix.stream,  \nas shown:\n\n\nChapter 7\n[ 305 ]\n9.\t The Hystrix Dashboard will be displayed as follows:\n\n\nLogging and Monitoring Microservices\n[ 306 ]\nNote that at least one transaction has to be executed to see the display. \nThis can be done by hitting http://localhost:8095/hubongw.\n10.\t Create a failure scenario by shutting down the Search service. Note \nthat the fallback method will be called when hitting the URL http://\nlocalhost:8095/hubongw.\n11.\t If there are continuous failures, then the circuit status will be changed to \nopen. This can be done by hitting the preceding URL a number of times. In \nthe open state, the original service will no longer be checked. The Hystrix \nDashboard will show the status of the circuit as Open, as shown in the \nfollowing screenshot. Once a circuit is opened, periodically, the system will \ncheck for the original service status for recovery. When the original service \nis back, the circuit breaker will fall back to the original service and the status \nwill be set to Closed:\nTo know the meaning of each of these parameters, visit the Hystrix wiki \nat https://github.com/Netflix/Hystrix/wiki/Dashboard.\n\n\nChapter 7\n[ 307 ]\nAggregating Hystrix streams with Turbine\nIn the previous example, the /hystrix.stream endpoint of our microservice was \ngiven in the Hystrix Dashboard. The Hystrix Dashboard can only monitor one \nmicroservice at a time. If there are many microservices, then the Hystrix Dashboard \npointing to the service has to be changed every time we switch the microservices to \nmonitor. Looking into one instance at a time is tedious, especially when there are \nmany instances of a microservice or multiple microservices.\nWe have to have a mechanism to aggregate data coming from multiple /hystrix.\nstream instances and consolidate it into a single dashboard view. Turbine does \nexactly the same thing. Turbine is another server that collects Hystrix streams from \nmultiple instances and consolidates them into one /turbine.stream instance. \nNow, the Hystrix Dashboard can point to /turbine.stream to get the consolidated \ninformation:\nTurbine currently works only with different hostnames. Each instance \nhas to be run on separate hosts. If you are testing multiple services \nlocally on the same host, then update the host file (/etc/hosts) to \nsimulate multiple hosts. Once done, bootstrap.properties has to \nbe configured as follows:\neureka.instance.hostname: localdomain2\nThis example showcases how to use Turbine to monitor circuit breakers across \nmultiple instances and services. We will use the Search service and Search API \nGateway in this example. Turbine internally uses Eureka to resolve service IDs  \nthat are configured for monitoring.\n\n\nLogging and Monitoring Microservices\n[ 308 ]\nPerform the following steps to build and execute this example:\n1.\t The Turbine server can be created as just another Spring Boot application \nusing Spring Boot Starter. Select Turbine to include the Turbine libraries.\n2.\t Once the application is created, add @EnableTurbine to the main Spring \nBoot Application class. In this example, both Turbine and Hystrix Dashboard \nare configured to be run on the same Spring Boot application. This is \npossible by adding the following annotations to the newly created Turbine \napplication:\n@EnableTurbine\n@EnableHystrixDashboard\n@SpringBootApplication\npublic class TurbineServerApplication {\n3.\t Add the following configuration to the .yaml or property file to point to the \ninstances that we are interested in monitoring:\nspring:\n   application:\n     name : turbineserver\nturbine:\n   clusterNameExpression: new String('default')\n   appConfig : search-service,search-apigateway\nserver:\n  port: 9090\neureka:\n  client:\n    serviceUrl:\n       defaultZone: http://localhost:8761/eureka/\n4.\t The preceding configuration instructs the Turbine server to look up the \nEureka server to resolve the search-service and search-apigateway \nservices. The search-service and search-apigateways service IDs are \nused to register services with Eureka. Turbine uses these names to resolve the \nactual service host and port by checking with the Eureka server. It will then \nuse this information to read /hystrix.stream from each of these instances. \nTurbine will then read all the individual Hystrix streams, aggregate all of \nthem, and expose them under the Turbine server's /turbine.stream URL.\n5.\t The cluster name expression is pointing to the default cluster as there is \nno explicit cluster configuration done in this example. If the clusters are \nmanually configured, then the following configuration has to be used:\nturbine:\n  aggregator:\n    clusterConfig: [comma separated clusternames]\n\n\nChapter 7\n[ 309 ]\n6.\t Change the Search service's SearchComponent to add another circuit breaker, \nas follows:\n  @HystrixCommand(fallbackMethod = \"searchFallback\")\n  public List<Flight> search(SearchQuery query){\n7.\t Also, add @EnableCircuitBreaker to the main Application class in the \nSearch service.\n8.\t Add the following configuration to bootstrap.properties of the Search \nservice. This is required because all the services are running on the same \nhost:\nEureka.instance.hostname: localdomain1\n9.\t Similarly, add the following in bootstrap.properties of the Search API \nGateway service. This is to make sure that both the services use different \nhostnames:\neureka.instance.hostname: localdomain2\n10.\t In this example, we will run two instances of search-apigateway: one on \nlocaldomain1:8095 and another one on localdomain2:8096. We will also \nrun one instance of search-service on localdomain1:8090.\n11.\t Run the microservices with command-line overrides to manage different host \naddresses, as follows:\njava -jar -Dserver.port=8096 -Deureka.instance.\nhostname=localdomain2 -Dserver.address=localdomain2 target/\nchapter7.search-apigateway-1.0.jar\njava -jar -Dserver.port=8095 -Deureka.instance.\nhostname=localdomain1 -Dserver.address=localdomain1 target/\nchapter7.search-apigateway-1.0.jar\njava -jar -Dserver.port=8090 -Deureka.instance.\nhostname=localdomain1 -Dserver.address=localdomain1 target/\nchapter7.search-1.0.jar\n12.\t Open Hystrix Dashboard by pointing the browser to http://\nlocalhost:9090/hystrix.\n13.\t Instead of giving /hystrix.stream, this time, we will point to /turbine.\nstream. In this example, the Turbine stream is running on 9090. Hence, the \nURL to be given in the Hystrix Dashboard is http://localhost:9090/\nturbine.stream.\n14.\t Fire a few transactions by opening a browser window and hitting the \nfollowing two URLs: http://localhost:8095/hubongw and http://\nlocalhost:8096/hubongw.\nOnce this is done, the dashboard page will show the getHub service.\n\n\nLogging and Monitoring Microservices\n[ 310 ]\n15.\t Run chapter7.website. Execute the search transaction using the website \nhttp://localhost:8001.\nAfter executing the preceding search, the dashboard page will show  \nsearch-service as well. This is shown in the following screenshot:\nAs we can see in the dashboard, search-service is coming from the Search \nmicroservice, and getHub is coming from Search API Gateway. As we have two \ninstances of Search API Gateway, getHub is coming from two hosts, indicated by \nHosts 2.\nData analysis using data lakes\nSimilarly to the scenario of fragmented logs and monitoring, fragmented data is \nanother challenge in the microservice architecture. Fragmented data poses challenges \nin data analytics. This data may be used for simple business event monitoring, data \nauditing, or even deriving business intelligence out of the data.\nA data lake or data hub is an ideal solution to handling such scenarios. An event-\nsourced architecture pattern is generally used to share the state and state changes \nas events with an external data store. When there is a state change, microservices \npublish the state change as events. Interested parties may subscribe to these events \nand process them based on their requirements. A central event store may also \nsubscribe to these events and store them in a big data store for further analysis.\n\n\nChapter 7\n[ 311 ]\nOne of the commonly followed architectures for such data handling is shown in the \nfollowing diagram:\nState change events generated from the microservice—in our case, the Search, \nBooking, and Check-In events—are pushed to a distributed high-performance \nmessaging system, such as Kafka. A data ingestion service, such as Flume, can \nsubscribe to these events and update them to an HDFS cluster. In some cases, these \nmessages will be processed in real time by Spark Streaming. To handle heterogeneous \nsources of events, Flume can also be used between event sources and Kafka.\nSpring Cloud Streams, Spring Cloud Streams modules, and Spring Data Flow are \nalso useful as alternatives for high-velocity data ingestion.\nSummary\nIn this chapter, you learned about the challenges around logging and monitoring \nwhen dealing with Internet-scale microservices.\nWe explored the various solutions for centralized logging. You also learned about \nhow to implement a custom centralized logging using Elasticsearch, Logstash, and \nKibana (ELK). In order to understand distributed tracing, we upgraded BrownField \nmicroservices using Spring Cloud Sleuth.\nIn the second half of this chapter, we went deeper into the capabilities required \nfor microservices monitoring solutions and different approaches to monitoring. \nSubsequently, we examined a number of tools available for microservices \nmonitoring.\n\n\nLogging and Monitoring Microservices\n[ 312 ]\nThe BrownField microservices are further enhanced with Spring Cloud Hystrix \nand Turbine to monitor latencies and failures in inter-service communications. The \nexamples also demonstrated how to use the circuit breaker pattern to fall back to \nanother service in case of failures.\nFinally, we also touched upon the importance of data lakes and how to integrate a \ndata lake architecture in a microservice context.\nMicroservice management is another important challenge we need to tackle when \ndealing with large-scale microservice deployments. The next chapter will explore \nhow containers can help in simplifying microservice management.\n\n\n[ 313 ]\nContainerizing Microservices \nwith Docker\nIn the context of microservices, containerized deployment is the icing on the cake. \nIt helps microservices be more autonomous by self-containing the underlying \ninfrastructure, thereby making the microservices cloud neutral.\nThis chapter will introduce the concepts and relevance of virtual machine images \nand the containerized deployment of microservices. Then, this chapter will further \nfamiliarize readers with building Docker images for the BrownField PSS microservices \ndeveloped with Spring Boot and Spring Cloud. Finally, this chapter will also touch \nbase on how to manage, maintain, and deploy Docker images in a production-like \nenvironment.\nBy the end of this chapter, you will learn about:\n•\t\nThe concept of containerization and its relevance in the context  \nof microservices\n•\t\nBuilding and deploying microservices as Docker images and containers\n•\t\nUsing AWS as an example of cloud-based Docker deployments\n\n\nContainerizing Microservices with Docker\n[ 314 ]\nReviewing the microservice capability \nmodel\nIn this chapter, we will explore the following microservice capabilities from the \nmicroservice capability model discussed in Chapter 3, Applying Microservices Concepts:\n•\t\nContainers and virtual machines\n•\t\nThe private/public cloud\n•\t\nThe microservices repository\nThe model is shown in the following diagram:\nUnderstanding the gaps in BrownField \nPSS microservices\nIn Chapter 5, Scaling Microservices with Spring Cloud, BrownField PSS microservices \nwere developed using Spring Boot and Spring Cloud. These microservices \nare deployed as versioned fat JAR files on bare metals, specifically on a local \ndevelopment machine.\n\n\nChapter 8\n[ 315 ]\nIn Chapter 6, Autoscaling Microservices, the autoscaling capability was added with the \nhelp of a custom life cycle manager. In Chapter 7, Logging and Monitoring Microservices, \nchallenges around logging and monitoring were addressed using centralized logging \nand monitoring solutions.\nThere are still a few gaps in our BrownField PSS implementation. So far, the \nimplementation has not used any cloud infrastructure. Dedicated machines, as \nin traditional monolithic application deployments, are not the best solution for \ndeploying microservices. Automation such as automatic provisioning, the ability to \nscale on demand, self-service, and payment based on usage are essential capabilities \nrequired to manage large-scale microservice deployments efficiently. In general, a \ncloud infrastructure provides all these essential capabilities. Therefore, a private or \npublic cloud with the capabilities mentioned earlier is better suited to deploying \nInternet-scale microservices.\nAlso, running one microservice instance per bare metal is not cost effective. \nTherefore, in most cases, enterprises end up deploying multiple microservices \non a single bare metal server. Running multiple microservices on a single bare \nmetal could lead to a \"noisy neighbor\" problem. There is no isolation between the \nmicroservice instances running on the same machine. As a result, services deployed \non a single machine may eat up others' space, thus impacting their performance.\nAn alternate approach is to run the microservices on VMs. However, VMs are \nheavyweight in nature. Therefore, running many smaller VMs on a physical \nmachine is not resource efficient. This generally results in resource wastage.  \nIn the case of sharing a VM to deploy multiple services, we would end up  \nfacing the same issues of sharing the bare metal, as explained earlier.\nIn the case of Java-based microservices, sharing a VM or bare metal to deploy \nmultiple microservices also results in sharing JRE among microservices. This is \nbecause the fat JARs created in our BrownField PSS abstract only application code \nand its dependencies but not JREs. Any update on JRE installed on the machine \nwill have an impact on all the microservices deployed on this machine. Similarly, \nif there are OS-level parameters, libraries, or tunings that are required for specific \nmicroservices, then it will be hard to manage them on a shared environment.\nOne microservice principle insists that it should be self-contained and autonomous \nby fully encapsulating its end-to-end runtime environment. In order to align with \nthis principle, all components, such as the OS, JRE, and microservice binaries, \nhave to be self-contained and isolated. The only option to achieve this is to follow \nthe approach of deploying one microservice per VM. However, this will result in \nunderutilized virtual machines, and in many cases, extra cost due to this can nullify \nbenefits of microservices.\n\n\nContainerizing Microservices with Docker\n[ 316 ]\nWhat are containers?\nContainers are not revolutionary, ground-breaking concepts. They have been in \naction for quite a while. However, the world is witnessing the re-entry of containers, \nmainly due to the wide adoption of cloud computing. The shortcomings of traditional \nvirtual machines in the cloud computing space also accelerated the use of containers. \nContainer providers such as Docker simplified container technologies to a great extent, \nwhich also enabled a large adoption of container technologies in today's world. The \nrecent popularity of DevOps and microservices also acted as a catalyst for the rebirth \nof container technologies.\nSo, what are containers? Containers provide private spaces on top of the operating \nsystem. This technique is also called operating system virtualization. In this \napproach, the kernel of the operating system provides isolated virtual spaces. Each \nof these virtual spaces is called a container or virtual engine (VE). Containers allow \nprocesses to run on an isolated environment on top of the host operating system. A \nrepresentation of multiple containers running on the same host is shown as follows:\nContainers are easy mechanisms to build, ship, and run compartmentalized \nsoftware components. Generally, containers package all the binaries and libraries \nthat are essential for running an application. Containers reserve their own \nfilesystem, IP address, network interfaces, internal processes, namespaces, OS \nlibraries, application binaries, dependencies, and other application configurations.\nThere are billions of containers used by organizations. Moreover, there are many \nlarge organizations heavily investing in container technologies. Docker is far ahead \nof the competition, supported by many large operating system vendors and cloud \nproviders. Lmctfy, SystemdNspawn, Rocket, Drawbridge, LXD, Kurma, and Calico \nare some of the other containerization solutions. Open container specification is also \nunder development.\n",
      "page_number": 312
    },
    {
      "number": 8,
      "title": "[ 315 ]",
      "start_page": 342,
      "end_page": 365,
      "detection_method": "regex_chapter",
      "content": "Chapter 8\n[ 317 ]\nThe difference between VMs and \ncontainers\nVMs such as Hyper-V, VMWare, and Zen were popular choices for data center \nvirtualization a few years ago. Enterprises experienced a cost saving by implementing \nvirtualization over the traditional bare metal usage. It has also helped many enterprises \nutilize their existing infrastructure in a much more optimized manner. As VMs support \nautomation, many enterprises experienced that they had to make lesser management \neffort with virtual machines. Virtual machines also helped organizations get isolated \nenvironments for applications to run in.\nPrima facie, both virtualization and containerization exhibit exactly the same \ncharacteristics. However, in a nutshell, containers and virtual machines are not the \nsame. Therefore, it is unfair to make an apple-to-apple comparison between VMs  \nand containers. Virtual machines and containers are two different techniques and \naddress different problems of virtualization. This difference is evident from the \nfollowing diagram:\nVirtual machines operate at a much lower level compared to containers. VMs provide \nhardware virtualization, such as that of CPUs, motherboards, memory, and so on. \nA VM is an isolated unit with an embedded operating system, generally called a \nGuest OS. VMs replicate the whole operating system and run it within the VM with \nno dependency on the host operating system environment. As VMs embed the full \noperating system environment, these are heavyweight in nature. This is an advantage \nas well as a disadvantage. The advantage is that VMs offer complete isolation to the \nprocesses running on VMs. The disadvantage is that it limits the number of VMs one \ncan spin up in a bare metal due to the resource requirements of VMs.\n\n\nContainerizing Microservices with Docker\n[ 318 ]\nThe size of a VM has a direct impact on the time to start and stop it. As starting a VM \nin turn boots the OS, the start time for VMs is generally high. VMs are more friendly \nwith infrastructure teams as it requires a low level of infrastructure competency to \nmanage VMs.\nIn the container world, containers do not emulate the entire hardware or operating \nsystem. Unlike VMs, containers share certain parts of the host kernel and operating \nsystem. There is no concept of guest OS in the case of containers. Containers provide \nan isolated execution environment directly on top of the host operating system. This \nis its advantage as well as disadvantage. The advantage is that it is lighter as well \nas faster. As containers on the same machine share the host operating system, the \noverall resource utilization of containers is fairly small. As a result, many smaller \ncontainers can be run on the same machine, as compared to heavyweight VMs. As \ncontainers on the same host share the host operating system, there are limitations as \nwell. For example, it is not possible to set iptables firewall rules inside a container. \nProcesses inside the container are completely independent from the processes on \ndifferent containers running on the same host.\nUnlike VMs, container images are publically available on community portals. This \nmakes developers' lives much easier as they don't have to build the images from \nscratch; instead, they can now take a base image from certified sources and add \nadditional layers of software components on top of the downloaded base image.\nThe lightweight nature of the containers is also opening up a plethora of opportunities, \nsuch as automated build, publishing, downloading, copying, and so on. The ability to \ndownload, build, ship, and run containers with a few commands or to use REST APIs \nmakes containers more developer friendly. Building a new container does not take \nmore than a few seconds. Containers are now part and parcel of continuous delivery \npipelines as well.\nIn summary, containers have many advantages over VMs, but VMs have their own \nexclusive strengths. Many organizations use both containers and VMs, such as by \nrunning containers on top of VMs.\n\n\nChapter 8\n[ 319 ]\nThe benefits of containers\nWe have already considered the many benefits of containers over VMs. This section \nwill explain the overall benefits of containers beyond the benefits of VMs:\n•\t\nSelf-contained: Containers package the essential application binaries and their \ndependencies together to make sure that there is no disparity between different \nenvironments such as development, testing, or production. This promotes \nthe concept of Twelve-Factor applications and that of immutable containers. \nSpring Boot microservices bundle all the required application dependencies. \nContainers stretch this boundary further by embedding JRE and other \noperating system-level libraries, configurations, and so on, if there are any.\n•\t\nLightweight: Containers, in general, are smaller in size with a lighter \nfootprint. The smallest container, Alpine, has a size of less than 5 MB. The \nsimplest Spring Boot microservice packaged with an Alpine container with \nJava 8 would only come to around 170 MB in size. Though the size is still on \nthe higher side, it is much less than the VM image size, which is generally in \nGBs. The smaller footprint of containers not only helps spin new containers \nquickly but also makes building, shipping, and storing easier.\n•\t\nScalable: As container images are smaller in size and there is no OS booting \nat startup, containers are generally faster to spin up and shut down. This \nmakes containers the popular choice for cloud-friendly elastic applications.\n•\t\nPortable: Containers provide portability across machines and cloud \nproviders. Once the containers are built with all the dependencies, they \ncan be ported across multiple machines or across multiple cloud providers \nwithout relying on the underlying machines. Containers are portable from \ndesktops to different cloud environments.\n•\t\nLower license cost: Many software license terms are based on the physical \ncore. As containers share the operating system and are not virtualized at the \nphysical resources level, there is an advantage in terms of the license cost.\n•\t\nDevOps: The lightweight footprint of containers makes it easy to automate \nbuilds and publish and download containers from remote repositories. This \nmakes it easy to use in Agile and DevOps environments by integrating with \nautomated delivery pipelines. Containers also support the concept of build \nonce by creating immutable containers at build time and moving them across \nmultiple environments. As containers are not deep into the infrastructure, \nmultidisciplinary DevOps teams can manage containers as part of their  \nday-to-day life.\n•\t\nVersion controlled: Containers support versions by default. This helps build \nversioned artifacts, just as with versioned archive files.\n\n\nContainerizing Microservices with Docker\n[ 320 ]\n•\t\nReusable: Container images are reusable artifacts. If an image is built  \nby assembling a number of libraries for a purpose, it can be reused in  \nsimilar situations.\n•\t\nImmutable containers: In this concept, containers are created and disposed \nof after usage. They are never updated or patched. Immutable containers are \nused in many environments to avoid complexities in patching deployment \nunits. Patching results in a lack of traceability and an inability to recreate \nenvironments consistently.\nMicroservices and containers\nThere is no direct relationship between microservices and containers. Microservices \ncan run without containers, and containers can run monolithic applications. However, \nthere is a sweet spot between microservices and containers.\nContainers are good for monolithic applications, but the complexities and the size of \nthe monolith application may kill some of the benefits of the containers. For example, \nspinning new containers quickly may not be easy with monolithic applications. \nIn addition to this, monolithic applications generally have local environment \ndependencies, such as the local disk, stovepipe dependencies with other systems,  \nand so on. Such applications are difficult to manage with container technologies.  \nThis is where microservices go hand in hand with containers.\nThe following diagram shows three polyglot microservices running on the same \nhost machine and sharing the same operating system but abstracting the runtime \nenvironment:\n\n\nChapter 8\n[ 321 ]\nThe real advantage of containers can be seen when managing many polyglot \nmicroservices—for instance, one microservice in Java and another one in Erlang or \nsome other language. Containers help developers package microservices written in any \nlanguage or technology in a platform- and technology-agnostic fashion and uniformly \ndistribute them across multiple environments. Containers eliminate the need to have \ndifferent deployment management tools to handle polyglot microservices. Containers \nnot only abstract the execution environment but also how to access the services. \nIrrespective of the technologies used, containerized microservices expose REST APIs. \nOnce the container is up and running, it binds to certain ports and exposes its APIs. \nAs containers are self-contained and provide full stack isolation among services, in \na single VM or bare metal, one can run multiple heterogeneous microservices and \nhandle them in a uniform way.\nIntroduction to Docker\nThe previous sections talked about containers and their benefits. Containers have \nbeen in the business for years, but the popularity of Docker has given containers  \na new outlook. As a result, many container definitions and perspectives emerged \nfrom the Docker architecture. Docker is so popular that even containerization is \nreferred to as dockerization.\nDocker is a platform to build, ship, and run lightweight containers based on Linux \nkernels. Docker has default support for Linux platforms. It also has support for  \nMac and Windows using Boot2Docker, which runs on top of Virtual Box.\nAmazon EC2 Container Service (ECS) has out-of-the-box support for Docker on \nAWS EC2 instances. Docker can be installed on bare metals and also on traditional \nvirtual machines such as VMWare or Hyper-V.\n\n\nContainerizing Microservices with Docker\n[ 322 ]\nThe key components of Docker\nA Docker installation has two key components: a Docker daemon and a Docker \nclient. Both the Docker daemon and Docker client are distributed as a single binary.\nThe following diagram shows the key components of a Docker installation:\nThe Docker daemon\nThe Docker daemon is a server-side component that runs on the host machine \nresponsible for building, running, and distributing Docker containers. The Docker \ndaemon exposes APIs for the Docker client to interact with the daemon. These APIs \nare primarily REST-based endpoints. One can imagine that the Docker daemon as a \ncontroller service running on the host machine. Developers can programmatically \nuse these APIs to build custom clients as well.\nThe Docker client\nThe Docker client is a remote command-line program that interacts with the Docker \ndaemon through either a socket or REST APIs. The CLI can run on the same host as \nthe daemon is running on or it can run on a completely different host and connect \nto the daemon remotely. Docker users use the CLI to build, ship, and run Docker \ncontainers.\n\n\nChapter 8\n[ 323 ]\nDocker concepts\nThe Docker architecture is built around a few concepts: images, containers, the \nregistry, and the Dockerfile.\nDocker images\nOne of the key concepts of Docker is the image. A Docker image is the read-only \ncopy of the operating system libraries, the application, and its libraries. Once an \nimage is created, it is guaranteed to run on any Docker platform without alterations.\nIn Spring Boot microservices, a Docker image packages operating systems such as \nUbuntu, Alpine, JRE, and the Spring Boot fat application JAR file. It also includes \ninstructions to run the application and expose the services:\nAs shown in the diagram, Docker images are based on a layered architecture in \nwhich the base image is one of the flavors of Linux. Each layer, as shown in the \npreceding diagram, gets added to the base image layer with the previous image  \nas the parent layer. Docker uses the concept of a union filesystem to combine all \nthese layers into a single image, forming a single filesystem.\nIn typical cases, developers do not build Docker images from scratch. Images  \nof an operating system, or other common libraries, such as Java 8 images,  \nare publicly available from trusted sources. Developers can start building on top  \nof these base images. The base image in Spring microservices can be JRE 8 rather \nthan starting from a Linux distribution image such as Ubuntu.\n\n\nContainerizing Microservices with Docker\n[ 324 ]\nEvery time we rebuild the application, only the changed layer gets rebuilt, and the \nremaining layers are kept intact. All the intermediate layers are cached, and hence, \nif there is no change, Docker uses the previously cached layer and builds it on \ntop. Multiple containers running on the same machine with the same type of base \nimages would reuse the base image, thus reducing the size of the deployment. For \ninstance, in a host, if there are multiple containers running with Ubuntu as the base \nimage, they all reuse the same base image. This is applicable when publishing or \ndownloading images as well:\nAs shown in the diagram, the first layer in the image is a boot filesystem called \nbootfs, which is similar to the Linux kernel and the boot loader. The boot filesystem \nacts as a virtual filesystem for all images.\nOn top of the boot filesystem, the operating system filesystem is placed, which \nis called rootfs. The root filesystem adds the typical operating system directory \nstructure to the container. Unlike in the Linux systems, rootfs, in the case of  \nDocker, is on a read-only mode.\nOn top of rootfs, other required images are placed as per the requirements. In \nour case, these are JRE and the Spring Boot microservice JARs. When a container \nis initiated, a writable filesystem is placed on top of all the other filesystems for the \nprocesses to run. Any changes made by the process to the underlying filesystem \nare not reflected in the actual container. Instead, these are written to the writable \nfilesystem. This writable filesystem is volatile. Hence, the data is lost once the \ncontainer is stopped. Due to this reason, Docker containers are ephemeral in nature.\n\n\nChapter 8\n[ 325 ]\nThe base operating system packaged inside Docker is generally a minimal copy of \njust the OS filesystem. In reality the process running on top may not use the entire \nOS services. In a Spring Boot microservice, in many cases, the container just initiates \na CMD and JVM and then invokes the Spring Boot fat JAR.\nDocker containers\nDocker containers are the running instances of a Docker image. Containers use the \nkernel of the host operating system when running. Hence, they share the host kernel \nwith other containers running on the same host. The Docker runtime ensures that \nthe container processes are allocated with their own isolated process space using \nkernel features such as cgroups and the kernel namespace of the operating system. \nIn addition to the resource fencing, containers get their own filesystem and network \nconfigurations as well.\nThe containers, when instantiated, can have specific resource allocations, such as \nthe memory and CPU. Containers, when initiated from the same image, can have \ndifferent resource allocations. The Docker container, by default, gets an isolated \nsubnet and gateway to the network. The network has three modes.\nThe Docker registry\nThe Docker registry is a central place where Docker images are published and \ndownloaded from. The URL https://hub.docker.com is the central registry \nprovided by Docker. The Docker registry has public images that one can download \nand use as the base registry. Docker also has private images that are specific to the \naccounts created in the Docker registry. The Docker registry screenshot is shown  \nas follows:\n\n\nContainerizing Microservices with Docker\n[ 326 ]\nDocker also offers Docker Trusted Registry, which can be used to set up registries \nlocally on premises.\nDockerfile\nA Dockerfile is a build or scripting file that contains instructions to build a Docker \nimage. There can be multiple steps documented in the Dockerfile, starting from \ngetting a base image. A Dockerfile is a text file that is generally named Dockerfile.  \nThe docker build command looks up Dockerfile for instructions to build.  \nOne can compare a Dockerfile to a pom.xml file used in a Maven build.\nDeploying microservices in Docker\nThis section will operationalize our learning by showcasing how to build containers \nfor our BrownField PSS microservices.\nThe full source code of this chapter is available under the Chapter \n8 project in the code files. Copy chapter7.configserver, \nchapter7.eurekaserver, chapter7.search, chapter7.\nsearch-apigateway, and chapter7.website into a new STS \nworkspace and rename them chapter8.*.\nPerform the following steps to build Docker containers for BrownField  \nPSS microservices:\n1.\t Install Docker from the official Docker site at https://www.docker.com.\nFollow the Get Started link for the download and installation instructions \nbased on the operating system of choice. Once installed, use the following \ncommand to verify the installation:\n$docker –version\nDocker version 1.10.1, build 9e83765\n2.\t In this section, we will take a look at how to dockerize the Search \n(chapter8.search) microservice, the Search API Gateway (chapter8.\nsearch-apigateway) microservice, and the Website (chapter8.website) \nSpring Boot application.\n3.\t Before we make any changes, we need to edit bootstrap.properties to \nchange the config server URL from localhost to the IP address as localhost is \nnot resolvable from within the Docker containers. In the real world, this will \npoint to a DNS or load balancer, as follows:\nspring.cloud.config.uri=http://192.168.0.105:8888\n\n\nChapter 8\n[ 327 ]\nReplace the IP address with the IP address of your machine.\n4.\t Similarly, edit search-service.properties on the Git repository and \nchange localhost to the IP address. This is applicable for the Eureka URL  \nas well as the RabbitMQ URL. Commit back to Git after updating. You can  \ndo this via the following code:\nspring.application.name=search-service\nspring.rabbitmq.host=192.168.0.105\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\norginairports.shutdown:JFK\neureka.client.serviceUrl.defaultZone: http://192.168.0.105:8761/\neureka/\nspring.cloud.stream.bindings.inventoryQ=inventoryQ\n5.\t Change the RabbitMQ configuration file rabbitmq.config by uncommenting \nthe following line to provide access to guest. By default, guest is restricted to be \naccessed from localhost only:\n    {loopback_users, []}\nThe location of rabbitmq.config will be different for different  \noperating systems.\n6.\t Create a Dockerfile under the root directory of the Search microservice,  \nas follows:\nFROM frolvlad/alpine-oraclejdk8\nVOLUME /tmp\nADD  target/search-1.0.jar search.jar\nEXPOSE 8090\nENTRYPOINT [\"java\",\"-jar\",\"/search.jar\"]\nThe following is a quick examination of the contents of the Dockerfile:\n°°\nFROM frolvlad/alpine-oraclejdk8: This tells the Docker build to \nuse a specific alpine-oraclejdk8 version as the basic image for this \nbuild. The frolvlad indicates the repository to locate the alpine-\noraclejdk8 image. In this case, it is an image built with Alpine Linux \nand Oracle JDK 8. This will help layer our application on top of the base \nimage without setting up Java libraries ourselves. In this case, as this \nimage is not available on our local image store, the Docker build will go \nahead and download this image from the remote Docker Hub registry.\n\n\nContainerizing Microservices with Docker\n[ 328 ]\n°°\nVOLUME /tmp: This enables access from the container to the \ndirectory specified in the host machine. In our case, this points \nto the tmp directory in which the Spring Boot application creates \nworking directories for Tomcat. The tmp directory is a logical \none for the container, which indirectly points to one of the local \ndirectories of the host.\n°°\nADD target/search-1.0.jar search.jar: This adds the \napplication binary file to the container with the destination filename \nspecified. In this case, the Docker build copies target/search-\n1.0.jar to the container as search.jar.\n°°\nEXPOSE 8090: This is to tell the container how to do port mapping. \nThis associates 8090 with external port binding for the internal \nSpring Boot service.\n°°\nENTRYPOINT [\"java\",\"-jar\", \"/search.jar\"]: This tells the \ncontainer which default application to run when a container is \nstarted. In this case, we are pointing to the Java process and the \nSpring Boot fat JAR file to initiate the service.\n7.\t The next step is to run docker build from the folder in which the Dockerfile \nis stored. This will download the base image and run the entries in the \nDockerfile one after the other, as follows:\ndocker build –t search:1.0 .\nThe output of this command will be as follows:\n\n\nChapter 8\n[ 329 ]\n8.\t Repeat the same steps for Search API Gateway and Website.\n9.\t Once the images are created, they can be verified by typing the following \ncommand. This command will list out the images and their details,  \nincluding the size of image files:\ndocker images\nThe output will be as follows:\n10.\t The next thing to do is run the Docker container. This can be done with the \ndocker run command. This command will load and run the container. \nOn starting, the container calls the Spring Boot executable JAR to start the \nmicroservice.\nBefore starting the containers, ensure that the Config and the Eureka servers \nare running:\ndocker run --net host -p 8090:8090 -t search:1.0\ndocker run --net host -p 8095:8095 -t search-apigateway:1.0\ndocker run --net host -p 8001:8001 -t website:1.0\nThe preceding command starts the Search and Search API Gateway \nmicroservices and Website.\nIn this example, we are using the host network (--net host) instead of  \nthe bridge network to avoid Eureka registering with the Docker container \nname. This can be corrected by overriding EurekaInstanceConfigBean.  \nThe host option is less isolated compared to the bridge option from the \nnetwork perspective. The advantage and disadvantage of host versus  \nbridge depends on the project.\n11.\t Once all the services are fully started, verify with the docker ps command,  \nas shown in the following screenshot:\n\n\nContainerizing Microservices with Docker\n[ 330 ]\n12.\t The next step is to point the browser to http://192.168.99.100:8001.  \nThis will open the BrownField PSS website.\nNote the IP address. This is the IP address of the Docker machine if you are \nrunning with Boot2Docker on Mac or Windows. In Mac or Windows, if the \nIP address is not known, then type the following command to find out the \nDocker machine's IP address for the default machine:\ndocker-machine ip default\nIf Docker is running on Linux, then this is the host IP address.\nApply the same changes to Booking, Fares, Check-in, and their respective  \ngateway microservices.\nRunning RabbitMQ on Docker\nAs our example also uses RabbitMQ, let's explore how to set up RabbitMQ as  \na Docker container. The following command pulls the RabbitMQ image from  \nDocker Hub and starts RabbitMQ:\ndocker run –net host rabbitmq3\nEnsure that the URL in *-service.properties is changed to the Docker host's  \nIP address. Apply the earlier rule to find out the IP address in the case of Mac  \nor Windows.\nUsing the Docker registry\nThe Docker Hub provides a central location to store all the Docker images. The \nimages can be stored as public as well as private. In many cases, organizations \ndeploy their own private registries on premises due to security-related concerns.\nPerform the following steps to set up and run a local registry:\n1.\t The following command will start a registry, which will bind the registry  \non port 5000:\ndocker run -d -p 5000:5000 --restart=always --name registry \nregistry:2\n2.\t Tag search:1.0 to the registry, as follows:\ndocker tag search:1.0 localhost:5000/search:1.0\n\n\nChapter 8\n[ 331 ]\n3.\t Then, push the image to the registry via the following command:\ndocker push localhost:5000/search:1.0\n4.\t Pull the image back from the registry, as follows:\ndocker pull localhost:5000/search:1.0\nSetting up the Docker Hub\nIn the previous chapter, we played with a local Docker registry. This section will \nshow how to set up and use the Docker Hub to publish the Docker containers. This \nis a convenient mechanism to globally access Docker images. Later in this chapter, \nDocker images will be published to the Docker Hub from the local machine and \ndownloaded from the EC2 instances.\nIn order to do this, create a public Docker Hub account and a repository.  \nFor Mac, follow the steps as per the following URL: https://docs.docker.com/\nmac/step_five/.\nIn this example, the Docker Hub account is created using the brownfield username.\nThe registry, in this case, acts as the microservices repository in which all the \ndockerized microservices will be stored and accessed. This is one of the capabilities \nexplained in the microservices capability model.\nPublishing microservices to the Docker Hub\nIn order to push dockerized services to the Docker Hub, follow these steps. The first \ncommand tags the Docker image, and the second one pushes the Docker image to  \nthe Docker Hub repository:\ndocker tag search:1.0brownfield/search:1.0\ndocker push brownfield/search:1.0\nTo verify whether the container images are published, go to the Docker Hub repository \nat https://hub.docker.com/u/brownfield.\nRepeat this step for all the other BrownField microservices as well. At the end of this \nstep, all the services will be published to the Docker Hub.\n\n\nContainerizing Microservices with Docker\n[ 332 ]\nMicroservices on the cloud\nOne of the capabilities mentioned in the microservices capability model is the use of \nthe cloud infrastructure for microservices. Earlier in this chapter, we also explored \nthe necessity of using the cloud for microservices deployments. So far, we have not \ndeployed anything to the cloud. As we have eight microservices in total—Config-\nserver, Eureka-server, Turbine, RabbitMQ, Elasticsearch, Kibana, and Logstash—\nin our overall BrownField PSS microservices ecosystem, it is hard to run all of them \non the local machine.\nIn the rest of this book, we will operate using AWS as the cloud platform to deploy \nBrownField PSS microservices.\nInstalling Docker on AWS EC2\nIn this section, we will install Docker on the EC2 instance.\nThis example assumes that readers are familiar with AWS and an account is already \ncreated on AWS.\nPerform the following steps to set up Docker on EC2:\n1.\t Launch a new EC2 instance. In this case, if we have to run all the instances \ntogether, we may need a large instance. The example uses t2.large.\nIn this example, the following Ubuntu AMI image is used: ubuntu-trusty-\n14.04-amd64-server-20160114.5 (ami-fce3c696).\n2.\t Connect to the EC2 instance and run the following commands:\nsudo apt-get update \nsudo apt-get install docker.io\n3.\t The preceding command will install Docker on an EC2 instance. Verify the \ninstallation with the following command:\ndocker version\nRunning BrownField services on EC2\nIn this section, we will set up BrownField microservices on the EC2 instances created. \nIn this case, the build is set up in the local desktop machine, and the binaries will be \ndeployed to AWS.\n\n\nChapter 8\n[ 333 ]\nPerform the following steps to set up services on an EC2 instance:\n1.\t Install Git via the following command:\nsudo apt-get install git\n2.\t Create a Git repository on any folder of your choice.\n3.\t Change the Config server's bootstrap.properties to point to the \nappropriate Git repository created for this example.\n4.\t Change the bootstrap.properties of all the microservices to point to the \nconfig-server using the private IP address of the EC2 instance.\n5.\t Copy all *.properties from the local Git repository to the EC2 Git \nrepository and perform a commit.\n6.\t Change the Eureka server URLs and RabbitMQ URLs in the *.properties \nfile to match the EC2 private IP address. Commit the changes to Git once \nthey have been completed.\n7.\t On the local machine, recompile all the projects and create Docker images  \nfor the search, search-apigateway, and website microservices. Push all  \nof them to the Docker Hub registry.\n8.\t Copy the config-server and the Eureka-server binaries from the local machine \nto the EC2 instance.\n9.\t Set up Java 8 on the EC2 instance.\n10.\t Then, execute the following commands in sequence:\njava –jar config-server.jar \njava –jar eureka-server.jar \ndocker run –net host rabbitmq:3\ndocker run --net host -p 8090:8090 rajeshrv/search:1.0\ndocker run --net host -p 8095:8095 rajeshrv/search-apigateway:1.0\ndocker run --net host -p 8001:8001 rajeshrv/website:1.0\n11.\t Check whether all the services are working by opening the URL of the website \nand executing a search. Note that we will use the public IP address in this case: \nhttp://54.165.128.23:8001.\n\n\nContainerizing Microservices with Docker\n[ 334 ]\nUpdating the life cycle manager\nIn Chapter 6, Autoscaling Microservices, we considered a life cycle manager to \nautomatically start and stop instances. We used SSH and executed a Unix script to \nstart the Spring Boot microservices on the target machine. With Docker, we no longer \nneed SSH connections as the Docker daemon provides REST-based APIs to start and \nstop instances. This greatly simplifies the complexities of the deployment engine \ncomponent of the life cycle manager.\nIn this section, we will not rewrite the life cycle manager. By and large, we will \nreplace the life cycle manager in the next chapter.\nThe future of containerization – \nunikernels and hardened security\nContainerization is still evolving, but the number of organizations adopting \ncontainerization techniques has gone up in recent times. While many organizations \nare aggressively adopting Docker and other container technologies, the downside  \nof these techniques is still in the size of the containers and security concerns.\nCurrently, Docker images are generally heavy. In an elastic automated environment, \nwhere containers are created and destroyed quite frequently, size is still an issue. \nA larger size indicates more code, and more code means that it is more prone to \nsecurity vulnerabilities.\nThe future is definitely in small footprint containers. Docker is working on \nunikernels, lightweight kernels that can run Docker even on low-powered IoT \ndevices. Unikernels are not full-fledged operating systems, but they provide the  \nbasic necessary libraries to support the deployed applications.\nThe security issues of containers are much discussed and debated. The key security \nissues are around the user namespace segregation or user ID isolation. If the container \nis on root, then it can by default gain the root privilege of the host. Using container \nimages from untrusted sources is another security concern. Docker is bridging these \ngaps as quickly as possible, but there are many organizations that use a combination  \nof VMs and Docker to circumvent some of the security concerns.\n\n\nChapter 8\n[ 335 ]\nSummary\nIn this chapter, you learned about the need to have a cloud environment when \ndealing with Internet-scale microservices.\nWe explored the concept of containers and compared them with traditional virtual \nmachines. You also learned the basics of Docker, and we explained the concepts of \nDocker images, containers, and registries. The importance and benefits of containers \nwere explained in the context of microservices.\nThis chapter then switched to a hands-on example by dockerizing the BrownField \nmicroservice. We demonstrated how to deploy the Spring Boot microservice \ndeveloped earlier on Docker. You learned the concept of registries by exploring a  \nlocal registry as well as the Docker Hub to push and pull dockerized microservices.\nAs the last step, we explored how to deploy a dockerized BrownField microservice in \nthe AWS cloud environment.\n\n\n[ 337 ]\nManaging Dockerized \nMicroservices with Mesos \nand Marathon\nIn an Internet-scale microservices deployment, it is not easy to manage thousands \nof dockerized microservices. It is essential to have an infrastructure abstraction \nlayer and a strong cluster control platform to successfully manage Internet-scale \nmicroservice deployments.\nThis chapter will explain the need and use of Mesos and Marathon as an infrastructure \nabstraction layer and a cluster control system, respectively, to achieve optimized \nresource usage in a cloud-like environment when deploying microservices at scale. \nThis chapter will also provide a step-by-step approach to setting up Mesos and \nMarathon in a cloud environment. Finally, this chapter will demonstrate how to \nmanage dockerized microservices in the Mesos and Marathon environment.\nBy the end of this chapter, you will have learned about:\n•\t\nThe need to have an abstraction layer and cluster control software\n•\t\nMesos and Marathon from the context of microservices\n•\t\nManaging dockerized BrownField Airline's PSS microservices with Mesos \nand Marathon\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 338 ]\nReviewing the microservice capability \nmodel\nIn this chapter, we will explore the Cluster Control & Provisioning microservices \ncapability from the microservices capability model discussed in Chapter 3, Applying \nMicroservices Concepts:\nThe missing pieces\nIn Chapter 8, Containerizing Microservices with Docker, we discussed how to \ndockerize BrownField Airline's PSS microservices. Docker helped package the \nJVM runtime and OS parameters along with the application so that there is no \nspecial consideration required when moving dockerized microservices from one \nenvironment to another. The REST APIs provided by Docker have simplified the life \ncycle manager's interaction with the target machine in starting and stopping artifacts.\n\n\nChapter 9\n[ 339 ]\nIn a large-scale deployment, with hundreds and thousands of Docker containers, we \nneed to ensure that Docker containers run with their own resource constraints, such \nas memory, CPU, and so on. In addition to this, there may be rules set for Docker \ndeployments, such as replicated copies of the container should not be run on the \nsame machine. Also, a mechanism needs to be in place to optimally use the server \ninfrastructure to avoid incurring extra cost.\nThere are organizations that deal with billions of containers. Managing them manually \nis next to impossible. In the context of large-scale Docker deployments, some of the key \nquestions to be answered are:\n•\t\nHow do we manage thousands of containers?\n•\t\nHow do we monitor them?\n•\t\nHow do we apply rules and constraints when deploying artifacts?\n•\t\nHow do we ensure that we utilize containers properly to gain  \nresource efficiency?\n•\t\nHow do we ensure that at least a certain number of minimal instances  \nare running at any point in time?\n•\t\nHow do we ensure dependent services are up and running?\n•\t\nHow do we do rolling upgrades and graceful migrations?\n•\t\nHow do we roll back faulty deployments?\nAll these questions point to the need to have a solution to address two key \ncapabilities, which are as follows:\n•\t\nA cluster abstraction layer that provides a uniform abstraction over many \nphysical or virtual machines\n•\t\nA cluster control and init system to manage deployments intelligently on  \ntop of the cluster abstraction\nThe life cycle manager is ideally placed to deal with these situations. One can add \nenough intelligence to the life cycle manager to solve these issues. However, before \nattempting to modify the life cycle manager, it is important to understand the role  \nof cluster management solutions a bit more.\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 340 ]\nWhy cluster management is important\nAs microservices break applications into different micro-applications, many \ndevelopers request more server nodes for deployment. In order to manage \nmicroservices properly, developers tend to deploy one microservice per VM,  \nwhich further drives down the resource utilization. In many cases, this results  \nin an overallocation of CPUs and memory.\nIn many deployments, the high-availability requirements of microservices force \nengineers to add more and more service instances for redundancy. In reality,  \nthough it provides the required high availability, this will result in underutilized \nserver instances.\nIn general, microservice deployment requires more infrastructure compared to \nmonolithic application deployments. Due to the increase in cost of the infrastructure, \nmany organizations fail to see the value of microservices:\nIn order to address the issue stated before, we need a tool that is capable of  \nthe following:\n•\t\nAutomating a number of activities, such as the allocation of containers  \nto the infrastructure efficiently and keeping it transparent to developers  \nand administrators\n•\t\nProviding a layer of abstraction for the developers so that they can deploy \ntheir application against a data center without knowing which machine is  \nto be used to host their applications\n•\t\nSetting rules or constraints against deployment artifacts\n•\t\nOffering higher levels of agility with minimal management overheads for \ndevelopers and administrators, perhaps with minimal human interaction\n•\t\nBuilding, deploying, and managing the application's cost effectively by \ndriving a maximum utilization of the available resources\nContainers solve an important issue in this context. Any tool that we select with \nthese capabilities can handle containers in a uniform way, irrespective of the \nunderlying microservice technologies.\n\n\nChapter 9\n[ 341 ]\nWhat does cluster management do?\nTypical cluster management tools help virtualize a set of machines and manage \nthem as a single cluster. Cluster management tools also help move the workload or \ncontainers across machines while being transparent to the consumer. Technology \nevangelists and practitioners use different terminologies, such as cluster orchestration, \ncluster management, data center virtualization, container schedulers, or container life \ncycle management, container orchestration, data center operating system, and so on.\nMany of these tools currently support both Docker-based containers as well as \nnoncontainerized binary artifact deployments, such as a standalone Spring Boot \napplication. The fundamental function of these cluster management tools is to abstract \nthe actual server instance from the application developers and administrators.\nCluster management tools help the self-service and provisioning of infrastructure \nrather than requesting the infrastructure teams to allocate the required machines \nwith a predefined specification. In this automated cluster management approach, \nmachines are no longer provisioned upfront and preallocated to the applications. \nSome of the cluster management tools also help virtualize data centers across \nmany heterogeneous machines or even across data centers, and create an elastic, \nprivate cloud-like infrastructure. There is no standard reference model for cluster \nmanagement tools. Therefore, the capabilities vary between vendors.\nSome of the key capabilities of cluster management software are summarized  \nas follows:\n•\t\nCluster management: It manages a cluster of VMs and physical machines  \nas a single large machine. These machines could be heterogeneous in terms  \nof resource capabilities, but they are, by and large, machines with Linux as \nthe operating system. These virtual clusters can be formed on the cloud,  \non-premises, or a combination of both.\n•\t\nDeployments: It handles the automatic deployment of applications and \ncontainers with a large set of machines. It supports multiple versions of the \napplication containers and also rolling upgrades across a large number of \ncluster machines. These tools are also capable of handling the rollback of \nfaulty promotes.\n•\t\nScalability: It handles the automatic and manual scalability of application \ninstances as and when required, with optimized utilization as the primary \ngoal.\n•\t\nHealth: It manages the health of the cluster, nodes, and applications.  \nIt removes faulty machines and application instances from the cluster.\n",
      "page_number": 342
    },
    {
      "number": 9,
      "title": "[ 339 ]",
      "start_page": 366,
      "end_page": 399,
      "detection_method": "regex_chapter",
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 342 ]\n•\t\nInfrastructure abstraction: It abstracts the developers from the actual \nmachine on which the applications are deployed. The developers need not \nworry about the machine, its capacity, and so on. It is entirely the cluster \nmanagement software's decision to decide how to schedule and run the \napplications. These tools also abstract machine details, their capacity, \nutilization, and location from the developers. For application owners, these \nare equivalent to a single large machine with almost unlimited capacity.\n•\t\nResource optimization: The inherent behavior of these tools is to allocate \ncontainer workloads across a set of available machines in an efficient way, \nthereby reducing the cost of ownership. Simple to extremely complicated \nalgorithms can be used effectively to improve utilization.\n•\t\nResource allocation: It allocates servers based on resource availability and \nthe constraints set by application developers. Resource allocation is based on \nthese constraints, affinity rules, port requirements, application dependencies, \nhealth, and so on.\n•\t\nService availability: It ensures that the services are up and running \nsomewhere in the cluster. In case of a machine failure, cluster control tools \nautomatically handle failures by restarting these services on some other \nmachine in the cluster.\n•\t\nAgility: These tools are capable of quickly allocating workloads to the \navailable resources or moving the workload across machines if there is  \nchange in resource requirements. Also, constraints can be set to realign  \nthe resources based on business criticality, business priority, and so on.\n•\t\nIsolation: Some of these tools provide resource isolation out of the box. \nHence, even if the application is not containerized, resource isolation can  \nbe still achieved.\nA variety of algorithms are used for resource allocation, ranging from simple \nalgorithms to complex algorithms, with machine learning and artificial intelligence. \nThe common algorithms used are random, bin packing, and spread. Constraints \nset against applications will override the default algorithms based on resource \navailability:\n\n\nChapter 9\n[ 343 ]\nThe preceding diagram shows how these algorithms fill the available machines with \ndeployments. In this case, it is demonstrated with two machines:\n•\t\nSpread: This algorithm performs the allocation of workload equally across \nthe available machines. This is showed in diagram A.\n•\t\nBin packing: This algorithm tries to fill in data machine by machine and \nensures the maximum utilization of machines. Bin packing is especially  \ngood when using cloud services in a pay-as-you-use style. This is shown  \nin diagram B.\n•\t\nRandom: This algorithm randomly chooses machines and deploys containers \non randomly selected machines. This is showed in diagram C.\nThere is a possibility of using cognitive computing algorithms such as machine \nlearning and collaborative filtering to improve efficiency. Techniques such as \noversubscription allow a better utilization of resources by allocating underutilized \nresources for high-priority tasks—for example, revenue-generating services for  \nbest-effort tasks such as analytics, video, image processing, and so on.\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 344 ]\nRelationship with microservices\nThe infrastructure of microservices, if not properly provisioned, can easily result in \noversized infrastructures and, essentially, a higher cost of ownership. As discussed \nin the previous sections, a cloud-like environment with a cluster management tool is \nessential to realize cost benefits when dealing with large-scale microservices.\nThe Spring Boot microservices turbocharged with the Spring Cloud project is the \nideal candidate workload to leverage cluster management tools. As Spring Cloud-\nbased microservices are location unaware, these services can be deployed anywhere \nin the cluster. Whenever services come up, they automatically register to the service \nregistry and advertise their availability. On the other hand, consumers always look \nfor the registry to discover the available service instances. This way, the application \nsupports a full fluid structure without preassuming a deployment topology. With \nDocker, we were able to abstract the runtime so that the services could run on any \nLinux-based environments.\nRelationship with virtualization\nCluster management solutions are different from server virtualization solutions \nin many aspects. Cluster management solutions run on top of VMs or physical \nmachines as an application component.\nCluster management solutions\nThere are many cluster management software tools available. It is unfair to do an \napple-to-apple comparison between them. Even though there are no one-to-one \ncomponents, there are many areas of overlap in capabilities between them. In many \nsituations, organizations use a combination of one or more of these tools to fulfill \ntheir requirements.\n\n\nChapter 9\n[ 345 ]\nThe following diagram shows the position of cluster management tools from the \nmicroservices context:\nIn this section, we will explore some of the popular cluster management solutions \navailable on the market.\nDocker Swarm\nDocker Swarm is Docker's native cluster management solution. Swarm provides a \nnative and deeper integration with Docker and exposes APIs that are compatible \nwith Docker's remote APIs. Docker Swarm logically groups a pool of Docker hosts \nand manages them as a single large Docker virtual host. Instead of application \nadministrators and developers deciding on which host the container is to be deployed \nin, this decision making will be delegated to Docker Swarm. Docker Swarm will decide \nwhich host to be used based on the bin packing and spread algorithms.\nAs Docker Swarm is based on Docker's remote APIs, its learning curve for those \nalready using Docker is narrower compared to any other container orchestration \ntools. However, Docker Swarm is a relatively new product on the market, and it  \nonly supports Docker containers.\nDocker Swarm works with the concepts of manager and nodes. A manager is the \nsingle point for administrations to interact and schedule the Docker containers for \nexecution. Nodes are where Docker containers are deployed and run.\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 346 ]\nKubernetes\nKubernetes (k8s) comes from Google's engineering, is written in the Go language, \nand is battle-tested for large-scale deployments at Google. Similar to Swarm, \nKubernetes helps manage containerized applications across a cluster of nodes. \nKubernetes helps automate container deployments, scheduling, and the scalability  \nof containers. Kubernetes supports a number of useful features out of the box, such \nas automatic progressive rollouts, versioned deployments, and container resiliency  \nif containers fail due to some reason.\nThe Kubernetes architecture has the concepts of master, nodes, and pods. The master \nand nodes together form a Kubernetes cluster. The master node is responsible for \nallocating and managing workload across a number of nodes. Nodes are nothing but \na VM or a physical machine. Nodes are further subsegmented as pods. A node can \nhost multiple pods. One or more containers are grouped and executed inside a pod. \nPods are also helpful in managing and deploying co-located services for efficiency. \nKubernetes also supports the concept of labels as key-value pairs to query and find \ncontainers. Labels are user-defined parameters to tag certain types of nodes that \nexecute a common type of workloads, such as frontend web servers. The services \ndeployed on a cluster get a single IP/DNS to access the service.\nKubernetes has out-of-the-box support for Docker; however, the Kubernetes learning \ncurve is steeper compared to Docker Swarm. RedHat offers commercial support for \nKubernetes as part of its OpenShift platform.\nApache Mesos\nMesos is an open source framework originally developed by the University of \nCalifornia at Berkeley and is used by Twitter at scale. Twitter uses Mesos primarily \nto manage the large Hadoop ecosystem.\nMesos is slightly different from the previous solutions. Mesos is more of a resource \nmanager that relays on other frameworks to manage workload execution. Mesos  \nsits between the operating system and the application, providing a logical cluster  \nof machines.\nMesos is a distributed system kernel that logically groups and virtualizes many \ncomputers to a single large machine. Mesos is capable of grouping a number of \nheterogeneous resources to a uniform resource cluster on which applications can  \nbe deployed. For these reasons, Mesos is also known as a tool to build a private \ncloud in a data center.\n\n\nChapter 9\n[ 347 ]\nMesos has the concepts of the master and slave nodes. Similar to the earlier solutions, \nmaster nodes are responsible for managing the cluster, whereas slaves run the \nworkload. Mesos internally uses ZooKeeper for cluster coordination and storage. \nMesos supports the concept of frameworks. These frameworks are responsible for \nscheduling and running noncontainerized applications and containers. Marathon, \nChronos, and Aurora are popular frameworks for the scheduling and execution of \napplications. Netflix Fenzo is another open source Mesos framework. Interestingly, \nKubernetes also can be used as a Mesos framework.\nMarathon supports the Docker container as well as noncontainerized applications. \nSpring Boot can be directly configured in Marathon. Marathon provides a number  \nof capabilities out of the box, such as supporting application dependencies, grouping \napplications to scale and upgrade services, starting and shutting down healthy and \nunhealthy instances, rolling out promotes, rolling back failed promotes, and so on.\nMesosphere offers commercial support for Mesos and Marathon as part of its  \nDCOS platform.\nNomad\nNomad from HashiCorp is another cluster management software. Nomad is a cluster \nmanagement system that abstracts lower-level machine details and their locations. \nNomad has a simpler architecture compared to the other solutions explored earlier. \nNomad is also lightweight. Similar to other cluster management solutions, Nomad \ntakes care of resource allocation and the execution of applications. Nomad also \naccepts user-specific constraints and allocates resources based on this.\nNomad has the concept of servers, in which all jobs are managed. One server acts  \nas the leader, and others act as followers. Nomad has the concept of tasks, which \nis the smallest unit of work. Tasks are grouped into task groups. A task group has \ntasks that are to be executed in the same location. One or more task groups or tasks  \nare managed as jobs.\nNomad supports many workloads, including Docker, out of the box. Nomad also \nsupports deployments across data centers and is region and data center aware.\nFleet\nFleet is a cluster management system from CoreOS. It runs on a lower level and \nworks on top of systemd. Fleet can manage application dependencies and make sure \nthat all the required services are running somewhere in the cluster. If a service fails, \nit restarts the service on another host. Affinity and constraint rules are possible to \nsupply when allocating resources.\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 348 ]\nFleet has the concepts of engine and agents. There is only one engine at any point \nin the cluster with multiple agents. Tasks are submitted to the engine and agent run \nthese tasks on a cluster machine.\nFleet also supports Docker out of the box.\nCluster management with Mesos and \nMarathon\nAs we discussed in the previous section, there are many cluster management \nsolutions or container orchestration tools available. Different organizations  \nchoose different solutions to address problems based on their environment.  \nMany organizations choose Kubernetes or Mesos with a framework such as \nMarathon. In most cases, Docker is used as a default containerization method  \nto package and deploy workloads.\nFor the rest of this chapter, we will show how Mesos works with Marathon to \nprovide the required cluster management capability. Mesos is used by many \norganizations, including Twitter, Airbnb, Apple, eBay, Netflix, PayPal, Uber,  \nYelp, and many others.\nDiving deep into Mesos\nMesos can be treated as a data center kernel. DCOS is the commercial version \nof Mesos supported by Mesosphere. In order to run multiple tasks on one node, \nMesos uses resource isolation concepts. Mesos relies on the Linux kernel's cgroups \nto achieve resource isolation similar to the container approach. It also supports \ncontainerized isolation using Docker. Mesos supports both batch workload as  \nwell as the OLTP kind of workloads:\n\n\nChapter 9\n[ 349 ]\nMesos is an open source top-level Apache project under the Apache license. Mesos \nabstracts lower-level computing resources such as CPU, memory, and storage from \nlower-level physical or virtual machines.\nBefore we examine why we need both Mesos and Marathon, let's understand the \nMesos architecture.\nThe Mesos architecture\nThe following diagram shows the simplest architectural representation of Mesos. \nThe key components of Mesos includes a Mesos master node, a set of slave nodes, \na ZooKeeper service, and a Mesos framework. The Mesos framework is further \nsubdivided into two components: a scheduler and an executor:\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 350 ]\nThe boxes in the preceding diagram are explained as follows:\n•\t\nMaster: The Mesos master is responsible for managing all the Mesos slaves. \nThe Mesos master gets information on the resource availability from all slave \nnodes and take the responsibility of filling the resources appropriately based \non certain resource policies and constraints. The Mesos master preempts \navailable resources from all slave machines and pools them as a single \nlarge machine. The master offers resources to frameworks running on slave \nmachines based on this resource pool.\nFor high availability, the Mesos master is supported by the Mesos master's \nstandby components. Even if the master is not available, the existing tasks \ncan still be executed. However, new tasks cannot be scheduled in the absence \nof a master node. The master standby nodes are nodes that wait for the \nfailure of the active master and take over the master's role in the case of  \na failure. It uses ZooKeeper for the master leader election. A minimum \nquorum requirement must be met for leader election.\n•\t\nSlave: Mesos slaves are responsible for hosting task execution frameworks. \nTasks are executed on the slave nodes. Mesos slaves can be started with \nattributes as key-value pairs, such as data center = X. This is used for constraint \nevaluations when deploying workloads. Slave machines share resource \navailability with the Mesos master.\n•\t\nZooKeeper: ZooKeeper is a centralized coordination server used in Mesos \nto coordinate activities across the Mesos cluster. Mesos uses ZooKeeper for \nleader election in case of a Mesos master failure.\n•\t\nFramework: The Mesos framework is responsible for understanding the \napplication's constraints, accepting resource offers from the master, and \nfinally running tasks on the slave resources offered by the master. The Mesos \nframework consists of two components: the framework scheduler and the \nframework executor:\n°°\nThe scheduler is responsible for registering to Mesos and handling \nresource offers\n°°\nThe executor runs the actual program on Mesos slave nodes\nThe framework is also responsible for enforcing certain policies and \nconstraints. For example, a constraint can be, let's say, that a minimum  \nof 500 MB of RAM is available for execution.\n\n\nChapter 9\n[ 351 ]\nFrameworks are pluggable components and are replaceable with another \nframework. The framework workflow is depicted in the following diagram:\nThe steps denoted in the preceding workflow diagram are elaborated as follows:\n1.\t The framework registers with the Mesos master and waits for resource \noffers. The scheduler may have many tasks in its queue to be executed \nwith different resource constraints (tasks A to D, in this example). A task, \nin this case, is a unit of work that is scheduled—for example, a Spring Boot \nmicroservice.\n2.\t The Mesos slave offers the available resources to the Mesos master.  \nFor example, the slave advertises the CPU and memory available with  \nthe slave machine.\n3.\t The Mesos master then creates a resource offer based on the allocation \npolicies set and offers it to the scheduler component of the framework. \nAllocation policies determine which framework the resources are to be \noffered to and how many resources are to be offered. The default policies  \ncan be customized by plugging additional allocation policies.\n4.\t The scheduler framework component, based on the constraints, capabilities, \nand policies, may accept or reject the resource offering. For example,  \na framework rejects the resource offer if the resources are insufficient  \nas per the constraints and policies set.\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 352 ]\n5.\t If the scheduler component accepts the resource offer, it submits the details \nof one more task to the Mesos master with resource constraints per task.  \nLet's say, in this example, that it is ready to submit tasks A to D.\n6.\t The Mesos master sends this list of tasks to the slave where the resources \nare available. The framework executor component installed on the slave \nmachines picks up and runs these tasks.\nMesos supports a number of frameworks, such as:\n•\t\nMarathon and Aurora for long-running processes, such as web applications\n•\t\nHadoop, Spark, and Storm for big data processing\n•\t\nChronos and Jenkins for batch scheduling\n•\t\nCassandra and Elasticsearch for data management\nIn this chapter, we will use Marathon to run dockerized microservices.\nMarathon\nMarathon is one of the Mesos framework implementations that can run both \ncontainer as well as noncontainer execution. Marathon is particularly designed for \nlong-running applications, such as a web server. Marathon ensures that the service \nstarted with Marathon continues to be available even if the Mesos slave it is hosted \non fails. This will be done by starting another instance.\nMarathon is written in Scala and is highly scalable. Marathon offers a UI as well as \nREST APIs to interact with Marathon, such as the start, stop, scale, and monitoring \napplications.\nSimilar to Mesos, Marathon's high availability is achieved by running multiple \nMarathon instances pointing to a ZooKeeper instance. One of the Marathon instances \nacts as a leader, and others are in standby mode. In case the leading master fails, a \nleader election will take place, and the next active master will be determined.\nSome of the basic features of Marathon include:\n•\t\nSetting resource constraints\n•\t\nScaling up, scaling down, and the instance management of applications\n•\t\nApplication version management\n•\t\nStarting and killing applications\n\n\nChapter 9\n[ 353 ]\nSome of the advanced features of Marathon include:\n•\t\nRolling upgrades, rolling restarts, and rollbacks\n•\t\nBlue-green deployments\nImplementing Mesos and Marathon for \nBrownField microservices\nIn this section, the dockerized Brownfield microservice developed in Chapter 8, \nContainerizing Microservices with Docker, will be deployed into the AWS cloud and \nmanaged with Mesos and Marathon.\nFor the purposes of demonstration, only three of the services (Search, Search API \nGateway, and Website) are covered in the explanations:\nThe logical architecture of the target state implementation is shown in the preceding \ndiagram. The implementation uses multiple Mesos slaves to execute dockerized \nmicroservices with a single Mesos master. The Marathon scheduler component is \nused to schedule dockerized microservices. Dockerized microservices are hosted on \nthe Docker Hub registry. Dockerized microservices are implemented using Spring \nBoot and Spring Cloud.\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 354 ]\nThe following diagram shows the physical deployment architecture:\nAs shown in the preceding diagram, in this example, we will use four EC2 instances:\n•\t\nEC2-M1: This hosts the Mesos master, ZooKeeper, the Marathon scheduler, \nand one Mesos slave instance\n•\t\nEC2-M2: This hosts one Mesos slave instance\n•\t\nEC2-M3: This hosts another Mesos slave instance\n•\t\nEC2-M4: This hosts Eureka, Config server, and RabbitMQ\nFor a real production setup, multiple Mesos masters as well as multiple instances  \nof Marathon are required for fault tolerance.\nSetting up AWS\nLaunch the four t2.micro EC2 instances that will be used for this deployment.  \nAll four instances have to be on the same security group so that the instances  \ncan see each other using their local IP addresses.\n\n\nChapter 9\n[ 355 ]\nThe following tables show the machine details and IP addresses for indicative \npurposes and to link subsequent instructions:\nInstance ID \nPrivate DNS/IP\nPublic DNS/IP\ni-06100786\nip-172-31-54-69.ec2.\ninternal\n172.31.54.69\nec2-54-85-107-37.compute-1.\namazonaws.com\n54.85.107.37\ni-2404e5a7\nip-172-31-62-44.ec2.\ninternal\n172.31.62.44\nec2-52-205-251-150.compute-1.\namazonaws.com\n52.205.251.150\ni-a7df2b3a\nip-172-31-49-55.ec2.\ninternal\n172.31.49.55\nec2-54-172-213-51.compute-1.\namazonaws.com\n54.172.213.51\ni-b0eb1f2d\nip-172-31-53-109.ec2.\ninternal\n172.31.53.109\nec2-54-86-31-240.compute-1.\namazonaws.com\n54.86.31.240\nReplace the IP and DNS addresses based on your AWS EC2 configuration.\nInstalling ZooKeeper, Mesos, and Marathon\nThe following software versions will be used for the deployment. The deployment  \nin this section follows the physical deployment architecture explained in the  \nearlier section:\n•\t\nMesos version 0.27.1\n•\t\nDocker version 1.6.2, build 7c8fca2\n•\t\nMarathon version 0.15.3\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 356 ]\nThe detailed instructions to set up ZooKeeper, Mesos, and \nMarathon are available at https://open.mesosphere.\ncom/getting-started/install/.\nPerform the following steps for a minimal installation of ZooKeeper, Mesos, and \nMarathon to deploy the BrownField microservice:\n1.\t As a prerequisite, JRE 8 must be installed on all the machines. Execute the \nfollowing command:\nsudo apt-get -y install oracle-java8-installer\n2.\t Install Docker on all machines earmarked for the Mesos slave via the \nfollowing command:\nsudo apt-get install docker\n3.\t Open a terminal window and execute the following commands.  \nThese commands set up the repository for installation:\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv \nE56151BF\nDISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]')\nCODENAME=$(lsb_release -cs)\n# Add the repository\necho \"deb http://repos.mesosphere.com/${DISTRO} ${CODENAME} main\" \n| \\\n  sudo tee /etc/apt/sources.list.d/mesosphere.list\nsudo apt-get -y update\n4.\t Execute the following command to install Mesos and Marathon. This will \nalso install Zookeeper as a dependency:\nsudo apt-get -y install mesos marathon\nRepeat the preceding steps on all the three EC2 instances reserved for the Mesos \nslave execution. As the next step, ZooKeeper and Mesos have to be configured on  \nthe machine identified for the Mesos master.\nConfiguring ZooKeeper\nConnect to the machine reserved for the Mesos master and Marathon scheduler.  \nIn this case, 172.31.54.69 will be used to set up ZooKeeper, the Mesos master,  \nand Marathon.\n\n\nChapter 9\n[ 357 ]\nThere are two configuration changes required in ZooKeeper, as follows:\n1.\t The first step is to set /etc/zookeeper/conf/myid to a unique integer \nbetween 1 and 255, as follows:\nOpen vi /etc/zookeeper/conf/myid and set 1. \n2.\t The next step is to edit /etc/zookeeper/conf/zoo.cfg. Update the file to \nreflect the following changes:\n# specify all zookeeper servers\n# The first port is used by followers to connect to the leader\n# The second one is used for leader election\nserver.1= 172.31.54.69:2888:3888\n#server.2=zookeeper2:2888:3888\n#server.3=zookeeper3:2888:3888\nReplace the IP addresses with the relevant private IP address. In this case,  \nwe will use only one ZooKeeper server, but in a production scenario, \nmultiple servers are required for high availability.\nConfiguring Mesos\nMake changes to the Mesos configuration to point to ZooKeeper, set up a quorum, \nand enable Docker support via the following steps:\n1.\t Edit /etc/mesos/zk to set the following value. This is to point Mesos to a \nZooKeeper instance for quorum and leader election:\nzk:// 172.31.54.69:2181/mesos \n2.\t Edit the /etc/mesos-master/quorum file and set the value as 1. In a \nproduction scenario, we may need a minimum quorum of three:\nvi /etc/mesos-master/quorum\n3.\t The default Mesos installation does not support Docker on Mesos slaves. In \norder to enable Docker, update the following mesos-slave configuration:\necho 'docker,mesos' > /etc/mesos-slave/containerizers\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 358 ]\nRunning Mesos, Marathon, and ZooKeeper  \nas services\nAll the required configuration changes are implemented. The easiest way to start \nMesos, Marathon, and Zookeeper is to run them as services, as follows:\n•\t\nThe following commands start services. The services need to be started in  \nthe following order:\nsudo service zookeeper start\nsudo service mesos-master start\nsudo service mesos-slave start\nsudo service marathon start\n•\t\nAt any point, the following commands can be used to stop these services:\nsudo service zookeeper stop\nsudo service mesos-master stop\nsudo service mesos-slave stop\nsudo service marathon stop\n•\t\nOnce the services are up and running, use a terminal window to verify \nwhether the services are running:\n\n\nChapter 9\n[ 359 ]\nRunning the Mesos slave in the command line\nIn this example, instead of using the Mesos slave service, we will use a command-\nline version to invoke the Mesos slave to showcase additional input parameters. Stop \nthe Mesos slave and use the command line as mentioned here to start the slave again:\n$sudo service mesos-slave stop\n$sudo /usr/sbin/mesos-slave  --master=172.31.54.69:5050 --log_dir=/var/\nlog/mesos --work_dir=/var/lib/mesos --containerizers=mesos,docker --resou\nrces=\"ports(*):[8000-9000, 31000-32000]\"\nThe command-line parameters used are explained as follows:\n•\t\n--master=172.31.54.69:5050: This parameter is to tell the Mesos  \nslave to connect to the correct Mesos master. In this case, there is only  \none master running at 172.31.54.69:5050. All the slaves connect to  \nthe same Mesos master.\n•\t\n--containerizers=mesos,docker: This parameter is to enable support  \nfor Docker container execution as well as noncontainerized executions on  \nthe Mesos slave instances.\n•\t\n--resources=\"ports(*):[8000-9000, 31000-32000]: This parameter \nindicates that the slave can offer both ranges of ports when binding resources. \n31000 to 32000 is the default range. As we are using port numbers starting \nwith 8000, it is important to tell the Mesos slave to allow exposing ports \nstarting from 8000 as well.\nPerform the following steps to verify the installation of Mesos and Marathon:\n1.\t Execute the command mentioned in the previous step to start the Mesos slave \non all the three instances designated for the slave. The same command can be \nused across all three instances as all of them connect to the same master.\n2.\t If the Mesos slave is successfully started, a message similar to the following \nwill appear in the console:\nI0411 18:11:39.684809 16665 slave.cpp:1030] Forwarding total \noversubscribed resources\nThe preceding message indicates that the Mesos slave started sending the \ncurrent state of resource availability periodically to the Mesos master.\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 360 ]\n3.\t Open http://54.85.107.37:8080 to inspect the Marathon UI. Replace the \nIP address with the public IP address of the EC2 instance:\nAs there are no applications deployed so far, the Applications section of the \nUI is empty.\n4.\t Open the Mesos UI, which runs on port 5050, by going to \nhttp://54.85.107.37:5050:\nThe Slaves section of the console shows that there are three activated Mesos \nslaves available for execution. It also indicates that there is no active task.\n\n\nChapter 9\n[ 361 ]\nPreparing BrownField PSS services\nIn the previous section, we successfully set up Mesos and Marathon. In this section, \nwe will take a look at how to deploy the BrownField PSS application previously \ndeveloped using Mesos and Marathon.\nThe full source code of this chapter is available under the Chapter \n9 project in the code files. Copy chapter8.configserver, \nchapter8.eurekaserver, chapter8.search, chapter8.\nsearch-apigateway, and chapter8.website into a new STS \nworkspace and rename them chapter9.*.\n1.\t Before we deploy any application, we have to set up the Config server, \nEureka server, and RabbitMQ in one of the servers. Follow the steps \ndescribed in the Running BrownField services on EC2 section in Chapter 8, \nContainerizing Microservices with Docker. Alternately, we can use the same \ninstance as used in the previous chapter for this purpose.\n2.\t Change all bootstrap.properties files to reflect the Config server  \nIP address.\n3.\t Before we deploy our services, there are a few specific changes required  \non the microservices. When running dockerized microservices with the \nBRIDGE mode on, we need to tell the Eureka client the hostname to be  \nused to bind. By default, Eureka uses the instance ID to register. However, \nthis is not helpful as Eureka clients won't be able to look up these services \nusing the instance ID. In the previous chapter, the HOST mode was used \ninstead of the BRIDGE mode.\nThe hostname setup can be done using the eureka.instance.hostname \nproperty. However, when running on AWS specifically, an alternate \napproach is to define a bean in the microservices to pick up AWS-specific \ninformation, as follows:\n@Configuration\nclass EurekaConfig { \n@Bean\n    public EurekaInstanceConfigBean eurekaInstanceConfigBean() {\n    EurekaInstanceConfigBean config = new \nEurekaInstanceConfigBean(new InetUtils(new \nInetUtilsProperties()));\nAmazonInfo info = AmazonInfo.Builder.newBuilder().\nautoBuild(\"eureka\");\n        config.setDataCenterInfo(info);\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 362 ]\n        info.getMetadata().put(AmazonInfo.MetaDataKey.\npublicHostname.getName(), info.get(AmazonInfo.MetaDataKey.\npublicIpv4));\n        config.setHostname(info.get(AmazonInfo.MetaDataKey.\nlocalHostname));       \nconfig.setNonSecurePortEnabled(true);\nconfig.setNonSecurePort(PORT); \nconfig.getMetadataMap().put(\"instanceId\",  info.get(AmazonInfo.\nMetaDataKey.localHostname));\nreturn config;\n}\nThe preceding code provides a custom Eureka server configuration using \nthe Amazon host information using Netflix APIs. The code overrides the \nhostname and instance ID with the private DNS. The port is read from \nthe Config server. This code also assumes one host per service so that the \nport number stays constant across multiple deployments. This can also be \noverridden by dynamically reading the port binding information at runtime.\nThe previous code has to be applied in all microservices.\n4.\t Rebuild all the microservices using Maven. Build and push the Docker \nimages to the Docker Hub. The steps for the three services are shown \nas follows. Repeat the same steps for all the other services. The working \ndirectory needs to be switched to the respective directories before executing \nthese commands:\ndocker build -t search-service:1.0 .\ndocker tag search-service:1.0 rajeshrv/search-service:1.0\ndocker push rajeshrv/search-service:1.0\ndocker build -t search-apigateway:1.0 .\ndocker tag search-apigateway:1.0 rajeshrv/search-apigateway:1.0\ndocker push rajeshrv/search-apigateway:1.0\ndocker build -t website:1.0 .\ndocker tag website:1.0 rajeshrv/website:1.0\ndocker push rajeshrv/website:1.0\n\n\nChapter 9\n[ 363 ]\nDeploying BrownField PSS services\nThe Docker images are now published to the Docker Hub registry. Perform the \nfollowing steps to deploy and run BrownField PSS services:\n1.\t Start the Config server, Eureka server, and RabbitMQ on its dedicated instance.\n2.\t Make sure that the Mesos server and Marathon are running on the machine \nwhere the Mesos master is configured.\n3.\t Run the Mesos slave on all the machines as described earlier using the \ncommand line.\n4.\t At this point, the Mesos Marathon cluster is up and running and is ready to \naccept deployments. The deployment can be done by creating one JSON file \nper service, as shown here:\n{\n  \"id\": \"search-service-1.0\",\n  \"cpus\": 0.5,\n  \"mem\": 256.0,\n  \"instances\": 1,\n  \"container\": {\n   \"docker\": {\n    \"type\": \"DOCKER\",\n      \"image\": \"rajeshrv/search-service:1.0\",\n       \"network\": \"BRIDGE\",\n       \"portMappings\": [\n        {  \"containerPort\": 0, \"hostPort\": 8090 }\n      ]\n    }\n  }\n}\nThe preceding JSON code will be stored in the search.json file. Similarly, \ncreate a JSON file for other services as well.\nThe JSON structure is explained as follows:\n°°\nid: This is the unique ID of the application. This can be a logical name.\n°°\ncpus and mem: This sets the resource constraints for this application. If \nthe resource offer does not satisfy this resource constraint, Marathon \nwill reject this resource offer from the Mesos master.\n°°\ninstances: This decides how many instances of this application to \nstart with. In the preceding configuration, by default, it starts one \ninstance as soon as it gets deployed. Marathon maintains the number \nof instances mentioned at any point.\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 364 ]\n°°\ncontainer: This parameter tells the Marathon executor to use a \nDocker container for execution.\n°°\nimage: This tells the Marathon scheduler which Docker image has to \nbe used for deployment. In this case, this will download the search-\nservice:1.0 image from the Docker Hub repository rajeshrv.\n°°\nnetwork: This value is used for Docker runtime to advise on the \nnetwork mode to be used when starting the new docker container. \nThis can be BRIDGE or HOST. In this case, the BRIDGE mode will  \nbe used.\n°°\nportMappings: The port mapping provides information on how to \nmap the internal and external ports. In the preceding configuration, \nthe host port is set as 8090, which tells the Marathon executor to use \n8090 when starting the service. As the container port is set as 0, the \nsame host port will be assigned to the container. Marathon picks up \nrandom ports if the host port value is 0.\n5.\t Additional health checks are also possible with the JSON descriptor, as \nshown here:\n\"healthChecks\": [\n    {\n      \"protocol\": \"HTTP\",\n      \"portIndex\": 0,\n      \"path\": \"/admin/health\",\n      \"gracePeriodSeconds\": 100,\n      \"intervalSeconds\": 30,\n      \"maxConsecutiveFailures\": 5\n    }\n  ]\n6.\t Once this JSON code is created and saved, deploy it to Marathon using the \nMarathon REST APIs as follows:\ncurl -X POST http://54.85.107.37:8080/v2/apps -d @search.json -H \n\"Content-type: application/json\"\nRepeat this step for all the other services as well.\nThe preceding step will automatically deploy the Docker container to the \nMesos cluster and start one instance of the service.\n\n\nChapter 9\n[ 365 ]\nReviewing the deployment\nThe steps for this are as follows:\n1.\t Open the Marathon UI. As shown in the following screenshot, the UI shows \nthat all the three applications are deployed and are in the Running state.  \nIt also indicates that 1 of 1 instance is in the Running state:\n2.\t Visit the Mesos UI. As shown in the following screenshot, there are three \nActive Tasks, all of them in the Running state. It also shows the host in \nwhich these services run:\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 366 ]\n3.\t In the Marathon UI, click on a running application. The following screenshot \nshows the search-apigateway-1.0 application. In the Instances tab, the IP \naddress and port in which the service is bound is indicated:\nThe Scale Application button allows administrators to specify how many \ninstances of the service are required. This can be used to scale up as well as \nscale down instances.\n4.\t Open the Eureka server console to take a look at how the services are bound. \nAs shown in the screenshot, AMIs and Availability Zones are reflected \nwhen services are registered. Follow http://52.205.251.150:8761:\n\n\nChapter 9\n[ 367 ]\n5.\t Open http://54.172.213.51:8001 in a browser to verify the  \nWebsite application.\nA place for the life cycle manager\nThe life cycle manager introduced in Chapter 6, Autoscaling Microservices, has the \ncapability of autoscaling up or down instances based on demand. It also has the \nability to take decisions on where to deploy and how to deploy applications on \na cluster of machines based on polices and constraints. The life cycle manager's \ncapabilities are shown in the following figure:\nMarathon has the capability to manage clusters and deployments to clusters based \non policies and constraints. The number of instances can be altered using the \nMarathon UI.\nThere are redundant capabilities between our life cycle manager and Marathon. \nWith Marathon in place, SSH work or machine-level scripting is no longer required. \nMoreover, deployment policies and constraints can be delegated to Marathon. The \nREST APIs exposed by Marathon can be used to initiate scaling functions.\nMarathon autoscale is a proof-of-concept project from Mesosphere for autoscaling. \nThe Marathon autoscale provides basic autoscale features such as the CPU, memory, \nand rate of request.\n\n\nManaging Dockerized Microservices with Mesos and Marathon\n[ 368 ]\nRewriting the life cycle manager with Mesos \nand Marathon\nWe still need a custom life cycle manager to collect metrics from the Spring Boot \nactuator endpoints. A custom life cycle manager is also handy if the scaling rules  \nare beyond the CPU, memory, and rate of scaling.\nThe following diagram shows the updated life cycle manager using the  \nMarathon framework:\nThe life cycle manager, in this case, collects actuator metrics from different Spring \nBoot applications, combines them with other metrics, and checks for certain \nthresholds. Based on the scaling policies, the decision engine informs the scaling \nengine to either scale down or scale up. In this case, the scaling engine is nothing \nbut a Marathon REST client. This approach is cleaner and neater than our earlier \nprimitive life cycle manager implementation using SSH and Unix scripts.\nThe technology metamodel\nWe have covered a lot of ground on microservices with the BrownField PSS \nmicroservices. The following diagram sums it up by bringing together all the \ntechnologies used into a technology metamodel:\n\n\nChapter 9\n[ 369 ]\nSummary\nIn this chapter, you learned the importance of a cluster management and init system \nto efficiently manage dockerized microservices at scale.\nWe explored the different cluster control or cluster orchestration tools before diving \ndeep into Mesos and Marathon. We also implemented Mesos and Marathon in the \nAWS cloud environment to demonstrate how to manage dockerized microservices \ndeveloped for BrownField PSS.\nAt the end of this chapter, we also explored the position of the life cycle manager  \nin conjunction with Mesos and Marathon. Finally, we concluded this chapter with a \ntechnology metamodel based on the BrownField PSS microservices implementation.\nSo far, we have discussed all the core and supporting technology capabilities \nrequired for a successful microservices implementation. A successful microservice \nimplementation also requires processes and practices beyond technology. The next \nchapter, the last in the book, will cover the process and practice perspectives of \nmicroservices.\n\n\n[ 371 ]\nThe Microservices \nDevelopment Life Cycle\nSimilar to the software development life cycle (SDLC), it is important to understand \nthe aspects of the microservice development life cycle processes for a successful \nimplementation of the microservices architecture.\nThis final chapter will focus on the development process and practice of \nmicroservices with the help of BrownField Airline's PSS microservices example. \nFurthermore, this chapter will describe best practices in structuring development \nteams, development methodologies, automated testing, and continuous delivery \nof microservices in line with DevOps practices. Finally, this chapter will conclude \nby shedding light on the importance of the reference architecture in a decentralized \ngovernance approach to microservices.\nBy the end of this chapter, you will learn about the following topics:\n•\t\nReviewing DevOps in the context of microservices development\n•\t\nDefining the microservices life cycle and related processes\n•\t\nBest practices around the development, testing, and deployment of  \nInternet-scale microservices\n\n\nThe Microservices Development Life Cycle\n[ 372 ]\nReviewing the microservice capability \nmodel\nThis chapter will cover the following microservices capabilities from the microservices \ncapability model discussed in Chapter 3, Applying Microservices Concepts:\n•\t\nDevOps\n•\t\nDevOps Tools\n•\t\nReference Architecture & Libraries\n•\t\nTesting Tools (Anti-Fragile, RUM etc)\nThe new mantra of lean IT – DevOps\nWe discussed the definition of DevOps in Chapter 2, Building Microservices with Spring \nBoot. Here is a quick recap of the DevOps definition.\nGartner defines DevOps as follows:\n\"DevOps represents a change in IT culture, focusing on rapid IT service delivery \nthrough the adoption of agile, lean practices in the context of a system-oriented \napproach. DevOps emphasizes people (and culture), and seeks to improve \ncollaboration between operations and development teams. DevOps implementations \nutilize technology — especially automation tools that can leverage an increasingly \nprogrammable and dynamic infrastructure from a life cycle perspective.\"\n\n\nChapter 10\n[ 373 ]\nDevOps and microservices evolved independently. Chapter 1, Demystifying \nMicroservices, explored the evolution of microservices. In this section, we will  \nreview the evolution of DevOps and then take a look at how DevOps supports \nmicroservices adoption.\nIn the era of digital disruption and in order to support modern business, IT \norganizations have to master two key areas: speed of delivery and value-driven \ndelivery. This is obviously apart from being expert in leading technologies.\nMany IT organizations failed to master this change, causing frustration to business \nusers. To overcome this situation, many business departments started their own \nshadow IT or stealth IT under their control. Some smart IT organizations then \nadopted a lean IT model to respond to these situations.\nHowever, many organizations still struggle with this transformation due to the large \nbaggage of legacy systems and processes. Gartner coined the concept of a pace-\nlayered application strategy. Gartner's view is that high speed is required only for \ncertain types of applications or certain business areas. Gartner termed this a system \nof innovation. A system of innovation requires rapid innovations compared to a \nsystem of records. As a system of innovations needs rapid innovation, a lean IT \ndelivery model is essential for such applications. Practitioners evangelized the lean \nIT model as DevOps.\nThere are two key strategies used by organizations to adopt DevOps.\nSome organizations positioned DevOps as a process to fill the gaps in their existing \nprocesses. Such organizations adopted an incremental strategy for their DevOps \njourney. The adoption path starts with Agile development, then incrementally \nadopts continuous integration, automated testing, and release to production and \nthen all DevOps practices. The challenge in such organizations is the time to realize \nthe full benefits as well as the mixed culture of people due to legacy processes.\nMany organizations, therefore, take a disruptive approach to adopt DevOps. This \nwill be achieved by partitioning IT into two layers or even as two different IT units. \nThe high-speed layer of IT uses DevOps-style practices to dramatically change the \nculture of the organization with no connection to the legacy processes and practices. \nA selective application cluster will be identified and moved to the new IT based on \nthe business value:\n\n\nThe Microservices Development Life Cycle\n[ 374 ]\nThe intention of DevOps is not just to reduce cost. It also enables the business \nto disrupt competitors by quickly moving ideas to production. DevOps attacks \ntraditional IT issues in multiple ways, as explained here.\nReducing wastage\nDevOps processes and practices essentially speed up deliveries which improves \nquality. The speed of delivery is achieved by cutting IT wastage. This is achieved by \navoiding work that adds no value to the business nor to desired business outcomes. \nIT wastage includes software defects, productivity issues, process overheads, \ntime lag in decision making, time spent in reporting layers, internal governance, \noverestimation, and so on. By reducing these wastages, organizations can radically \nimprove the speed of delivery. The wastage is reduced by primarily adopting Agile \nprocesses, tools, and techniques.\nAutomating every possible step\nBy automating the manually executed tasks, one can dramatically improve the \nspeed of delivery as well as the quality of deliverables. The scope of automation \ngoes from planning to customer feedback. Automation reduces the time to move \nbusiness ideas to production. This also reduces a number of manual gate checks, \nbureaucratic decision making, and so on. Automated monitoring mechanisms and \nfeedback go back to the development factory, which gets it fixed and quickly moved \nto production.\nValue-driven delivery\nDevOps reduces the gap between IT and business through value-driven delivery. \nValue-driven delivery closely aligns IT to business by understanding true business \nvalues and helps the business by quickly delivering these values, which can give \na competitive advantage. This is similar to the shadow IT concept, in which IT is \ncollocated with the business and delivers business needs quickly, rather than  \nwaiting for heavy project investment-delivery cycles.\nTraditionally, IT is partially disconnected from the business and works with IT \nKPIs, such as the number of successful project deliveries, whereas in the new model, \nIT shares business KPIs. As an example, a new IT KPI could be that IT helped \nbusiness to achieve a 10% increase in sales orders or led to 20% increase in customer \nacquisition. This will shift IT's organizational position from merely a support \norganization to a business partner. \n\n\nChapter 10\n[ 375 ]\nBridging development and operations\nTraditionally, IT has different teams for development and operations. In many cases, \nthey are differentiated with logical barriers. DevOps reduces the gap between the \ndevelopment and operations teams so that it can potentially reduce wastage and \nimprove quality. Multidisciplinary teams work together to address problems at  \nhand rather than throwing mud across the wall.\nWith DevOps, operations teams will have a fairly good understanding about the \nservices and applications developed by development teams. Similarly, development \nteams will have a good handle on the infrastructure components and configurations \nused by the applications. As a result, operations teams can make decisions based \nexactly on service behaviors rather than enforcing standard organizational policies \nand rules when designing infrastructure components. This would eventually help \nthe IT organization to improve the quality of the product as well as the time to \nresolve incidents and problem management.\nIn the DevOps world, speed of delivery is achieved through the automation of  \nhigh-velocity changes, and quality is achieved through automation and people. \nBusiness values are achieved through efficiency, speed of delivery, quality, and the \nability to innovate. Cost reduction is achieved through automation, productivity,  \nand reducing wastage.\nMeeting the trio – microservices, \nDevOps, and cloud\nThe trio—cloud, microservices, and DevOps—targets a set of common objectives: \nspeed of delivery, business value, and cost benefit. All three can stay and evolve \nindependently, but they complement each other to achieve the desired common \ngoals. Organizations embarking on any of these naturally tend to consider the other \ntwo as they are closely linked together:\n\n\nThe Microservices Development Life Cycle\n[ 376 ]\nMany organizations start their journey with DevOps as an organizational practice \nto achieve high-velocity release cycles but eventually move to the microservices \narchitecture and cloud. It is not mandatory to have microservices and cloud support \nDevOps. However, automating the release cycles of large monolithic applications \ndoes not make much sense, and in many cases, it would be impossible to achieve. \nIn such scenarios, the microservices architecture and cloud will be handy when \nimplementing DevOps.\nIf we flip a coin, cloud does not need a microservices architecture to achieve its \nbenefits. However, to effectively implement microservices, both cloud and DevOps \nare essential.\nIn summary, if the objective of an organization is to achieve a high speed of delivery \nand quality in a cost-effective way, the trio together can bring tremendous success.\nCloud as the self-service infrastructure for \nMicroservices\nThe main driver for cloud is to improve agility and reduce cost. By reducing the time \nto provision the infrastructure, the speed of delivery can be increased. By optimally \nutilizing the infrastructure, one can bring down the cost. Therefore, cloud directly \nhelps achieve both speed of delivery and cost.\nAs discussed in Chapter 9, Managing Dockerized Microservices with Mesos and Marathon, \nwithout having a cloud infrastructure with cluster management software, it would \nbe hard to control the infrastructure cost when deploying microservices. Hence, the \ncloud with self-service capabilities is essential for microservices to achieve their full \npotential benefits. In the microservices context, the cloud not only helps abstract the \nphysical infrastructure but also provides software APIs for dynamic provisioning \nand automatic deployments. This is referred to as infrastructure as code or  \nsoftware-defined infrastructure.\nDevOps as the practice and process for \nmicroservices\nMicroservice is an architecture style that enables quick delivery. However, \nmicroservices cannot provide the desired benefits by themselves. A microservices-\nbased project with a delivery cycle of 6 months does not give the targeted speed of \ndelivery or business agility. Microservices need a set of supporting delivery practices \nand processes to effectively achieve their goal.\n",
      "page_number": 366
    },
    {
      "number": 10,
      "title": "[ 373 ]",
      "start_page": 400,
      "end_page": 436,
      "detection_method": "regex_chapter",
      "content": "Chapter 10\n[ 377 ]\nDevOps is the ideal candidate for the underpinning process and practices \nfor microservice delivery. DevOps processes and practices gel well with the \nmicroservices architecture's philosophies.\nPractice points for microservices \ndevelopment\nFor a successful microservice delivery, a number of development-to-delivery \npractices need to be considered, including the DevOps philosophy. In the previous \nchapters, you learned the different architecture capabilities of microservices. In this \nsection, we will explore the nonarchitectural aspects of microservice developments.\nUnderstanding business motivation and value\nMicroservices should not be used for the sake of implementing a niche architecture \nstyle. It is extremely important to understand the business value and business KPIs \nbefore selecting microservices as an architectural solution for a given problem. A \ngood understanding of business motivation and business value will help engineers \nfocus on achieving these goals in a cost-effective way.\nBusiness motivation and value should justify the selection of microservices. Also, \nusing microservices, the business value should be realizable from a business point \nof view. This will avoid situations where IT invests in microservices but there is no \nappetite from the business to leverage any of the benefits that microservices can \nbring to the table. In such cases, a microservices-based development would be an \noverhead to the enterprise.\nChanging the mindset from project to product \ndevelopment\nAs discussed in Chapter 1, Demystifying Microservices, microservices are more aligned \nto product development. Business capabilities that are delivered using microservices \nshould be treated as products. This is in line with the DevOps philosophy as well.\nThe mindset for project development and product development is different. The \nproduct team will always have a sense of ownership and take responsibility for what \nthey produce. As a result, product teams always try to improve the quality of the \nproduct. The product team is responsible not only for delivering the software but \nalso for production support and maintenance of the product.\n\n\nThe Microservices Development Life Cycle\n[ 378 ]\nProduct teams are generally linked directly to a business department for which \nthey are developing the product. In general, product teams have both an IT and a \nbusiness representative. As a result, product thinking is closely aligned with actual \nbusiness goals. At every moment, product teams understand the value they are \nadding to the business to achieve business goals. The success of the product directly \nlies with the business value being gained out of the product.\nBecause of the high-velocity release cycles, product teams always get a sense of \nsatisfaction in their delivery, and they always try to improve on it. This brings  \na lot more positive dynamics within the team.\nIn many cases, typical product teams are funded for the long term and remain intact. \nAs a result, product teams become more cohesive in nature. As they are small in size, \nsuch teams focus on improving their process from their day-to-day learnings.\nOne common pitfall in product development is that IT people represent the business \nin the product team. These IT representatives may not fully understand the business \nvision. Also, they may not be empowered to take decisions on behalf of the business. \nSuch cases can result in a misalignment with the business and lead to failure  \nquite rapidly.\nIt is also important to consider a collocation of teams where business and IT \nrepresentatives reside at the same place. Collocation adds more binding between  \nIT and business teams and reduces communication overheads.\nChoosing a development philosophy\nDifferent organizations take different approaches to developing microservices, be it a \nmigration or a new development. It is important to choose an approach that suits the \norganization. There is a wide verity of approaches available, out of which a few are \nexplained in this section.\nDesign thinking\nDesign thinking is an approach primarily used for innovation-centric development. \nIt is an approach that explores the system from an end user point of view: what the \ncustomers see and how they experience the solution. A story is then built based on \nobservations, patterns, intuition, and interviews.\nDesign thinking then quickly devises solutions through solution-focused thinking \nby employing a number of theories, logical reasoning, and assumptions around the \nproblem. The concepts are expanded through brainstorming before arriving at a \nconverged solution.\n\n\nChapter 10\n[ 379 ]\nOnce the solution is identified, a quick prototype is built to consider how the \ncustomer responds to it, and then the solution is adjusted accordingly. When the \nteam gets satisfactory results, the next step is taken to scale the product. Note that \nthe prototype may or may not be in the form of code.\nDesign thinking uses human-centric thinking with feelings, empathy, intuition, and \nimagination at its core. In this approach, solutions will be up for rethinking even for \nknown problems to find innovative and better solutions.\nThe start-up model\nMore and more organizations are following the start-up philosophy to deliver \nsolutions. Organizations create internal start-up teams with the mission to deliver \nspecific solutions. Such teams stay away from day-to-day organizational activities \nand focus on delivering their mission.\nMany start-ups kick off with a small, focused team—a highly cohesive unit. The unit \nis not worried about how they achieve things; rather, the focus is on what they want \nto achieve. Once they have a product in place, the team thinks about the right way  \nto build and scale it.\nThis approach addresses quick delivery through production-first thinking. The \nadvantage with this approach is that teams are not disturbed by organizational \ngovernance and political challenges. The team is empowered to think out of the box, \nbe innovative, and deliver things. Generally, a higher level of ownership is seen in \nsuch teams, which is one of the key catalysts for success. Such teams employ just \nenough processes and disciplines to take the solution forward. They also follow  \na fail fast approach and course correct sooner than later.\nThe Agile practice\nThe most commonly used approach is the Agile methodology for development. \nIn this approach, software is delivered in an incremental, iterative way using the \nprinciples put forth in the Agile manifesto. This type of development uses an Agile \nmethod such as Scrum or XP. The Agile manifesto defines four key points that Agile \nsoftware development teams should focus on:\n•\t\nIndividuals and interaction over processes and tools\n•\t\nWorking software over comprehensive documentation\n•\t\nCustomer collaboration over contract negotiation\n•\t\nResponding to change over following a plan\n\n\nThe Microservices Development Life Cycle\n[ 380 ]\nThe 12 principles of Agile software development can be found at \nhttp://www.agilemanifesto.org/principles.html.\nUsing the concept of Minimum Viable Product\nIrrespective of the development philosophy explained earlier, it is essential  \nto identify a Minimum Viable Product (MVP) when developing microservice \nsystems for speed and agility.\nEric Ries, while pioneering the lean start-up movement, defined MVP as:\n\"A Minimum Viable Product is that version of a new product which allows a team \nto collect the maximum amount of validated learning about customers with the \nleast effort.\"\nThe objective of the MVP approach is to quickly build a piece of software that \nshowcases the most important aspects of the software. The MVP approach realizes \nthe core concept of an idea and perhaps chooses those features that add maximum \nvalue to the business. It helps get early feedback and then course corrects as \nnecessary before building a heavy product.\nThe MVP may be a full-fledged service addressing limited user groups or partial \nservices addressing wider user groups. Feedback from customers is extremely \nimportant in the MVP approach. Therefore, it is important to release the MVP  \nto the real users.\nOvercoming the legacy hotspot\nIt is important to understand the environmental and political challenges in an \norganization before embarking on microservices development.\nIt is common in microservices to have dependencies on other legacy applications, \ndirectly or indirectly. A common issue with direct legacy integration is the slow \ndevelopment cycle of the legacy application. An example would be an innovative \nrailway reservation system relaying on an age-old transaction processing facility \n(TPF) for some of the core backend features, such as reservation. This is especially \ncommon when migrating legacy monolithic applications to microservices. In many \ncases, legacy systems continue to undergo development in a non-Agile way with \nlarger release cycles. In such cases, microservices development teams may not be \nable to move so quickly because of the coupling with legacy systems. Integration \npoints might drag the microservices developments heavily. Organizational political \nchallenges make things even worse.\n\n\nChapter 10\n[ 381 ]\nThere is no silver bullet to solve this issue. The cultural and process differences could \nbe an ongoing issue. Many enterprises ring-fence such legacy systems with focused \nattention and investments to support fast-moving microservices. Targeted C-level \ninterventions on these legacy platforms could reduce the overheads.\nAddressing challenges around databases\nAutomation is key in microservices development. Automating databases is one  \nof the key challenges in many microservice developments.\nIn many organizations, DBAs play a critical role in database management, and they \nlike to treat the databases under their control differently. Confidentiality and access \ncontrol on data is also cited as a reason for DBAs to centrally manage all data.\nMany automation tools focus on the application logic. As a result, many development \nteams completely ignore database automation. Ignoring database automation can \nseverely impact the overall benefits and can derail microservices development.\nIn order to avoid such situations, the database has to be treated in the same way \nas applications with appropriate source controls and change management. When \nselecting a database, it is also important to consider automation as one of the  \nkey aspects.\nDatabase automation is much easier in the case of NoSQL databases but is hard \nto manage with traditional RDBMs. Database Lifecycle Management (DLM) as a \nconcept is popular in the DevOps world, particularly to handle database automation. \nTools such as DBmaestro, Redgate DLM, Datical DB, and Delphix support database \nautomation.\nEstablishing self-organizing teams\nOne of the most important activities in microservices development is to establish the \nright teams for development. As recommended in many DevOps processes, a small, \nfocused team always delivers the best results.\n\n\nThe Microservices Development Life Cycle\n[ 382 ]\nAs microservices are aligned with business capabilities and are fairly loosely coupled \nproducts, it is ideal to have a dedicated team per microservice. There could be cases \nwhere the same team owns multiple microservices from the same business area \nrepresenting related capabilities. These are generally decided by the coupling and \nsize of the microservices.\nTeam size is an important aspect in setting up effective teams for microservices \ndevelopment. The general notion is that the team size should not exceed 10 people. \nThe recommended size for optimal delivery is between 4 and 7. The founder of \nAmazon.com, Jeff Bezos, coined the theory of two-pizza teams. Jeff's theory says the \nteam will face communication issues if the size gets bigger. Larger teams work with \nconsensus, which results in increased wastage. Large teams also lose ownership  \nand accountability. A yardstick is that the product owner should get enough time  \nto speak to individuals in the team to make them understand the value of what they \nare delivering.\nTeams are expected to take full ownership in ideating for, analyzing, developing, \nand supporting services. Werner Vogels from Amazon.com calls this you build it  \nand you run it. As per Werner's theory, developers pay more attention to develop \nquality code to avoid unexpected support calls. The members in the team consist  \nof fullstack developers and operational engineers. Such a team is fully aware of all \nthe areas. Developers understand operations as well as operations teams understand \napplications. This not only reduces the changes of throwing mud across teams but \nalso improves quality.\nTeams should have multidisciplinary skills to satisfy all the capabilities required to \ndeliver a service. Ideally, the team should not rely on external teams to deliver the \ncomponents of the service. Instead, the team should be self-sufficient. However, in \nmost organizations, the challenge is on specialized skills that are rare. For example, \nthere may not be many experts on a graph database in the organization. One common \nsolution to this problem is to use the concept of consultants. Consultants are SMEs \nand are engaged to gain expertise on specific problems faced by the team. Some \norganizations also use shared or platform teams to deliver some common capabilities.\nTeam members should have a complete understanding of the products, not only \nfrom the technical standpoint but also from the business case and the business KPIs. \nThe team should have collective ownership in delivering the product as well as in \nachieving business goals together.\n\n\nChapter 10\n[ 383 ]\nAgile software development also encourages having self-organizing teams. Self-\norganizing teams act as a cohesive unit and find ways to achieve their goals as a \nteam. The team automatically align themselves and distribute the responsibilities. \nThe members in the team are self-managed and empowered to make decisions in \ntheir day-to-day work. The team's communication and transparency are extremely \nimportant in such teams. This emphasizes the need for collocation and collaboration, \nwith a high bandwidth for communication:\nIn the preceding diagram, both Microservice A and Microservice B represent related \nbusiness capabilities. Self-organizing teams treat everyone in the team equally, \nwithout too many hierarchies and management overheads within the team. The \nmanagement would be thin in such cases. There won't be many designated vertical \nskills in the team, such as team lead, UX manager, development manager, testing \nmanager, and so on. In a typical microservice development, a shared product \nmanager, shared architect, and a shared people manager are good enough to manage \nthe different microservice teams. In some organizations, architects also take up \nresponsibility for delivery.\nSelf-organizing teams have some level of autonomy and are empowered to take \ndecisions in a quick and Agile mode rather than having to wait for long-running \nbureaucratic decision-making processes that exist in many enterprises. In many \nof these cases, enterprise architecture and security are seen as an afterthought. \nHowever, it is important to have them on board from the beginning. While \nempowering the teams with maximum freedom for developers in decision-making \ncapabilities, it is equally important to have fully automated QA and compliance so  \nas to ensure that deviations are captured at the earliest.\n\n\nThe Microservices Development Life Cycle\n[ 384 ]\nCommunication between teams is important. However, in an ideal world, it should \nbe limited to interfaces between microservices. Integrations between teams ideally \nhas to be handled through consumer-driven contracts in the form of test scripts \nrather than having large interface documents describing various scenarios. Teams \nshould use mock service implementations when the services are not available.\nBuilding a self-service cloud\nOne of the key aspects that one should consider before embarking on microservices \nis to build a cloud environment. When there are only a few services, it is easy  \nto manage them by manually assigning them to a certain predesignated set of  \nvirtual machines.\nHowever, what microservice developers need is more than just an IaaS cloud \nplatform. Neither the developers nor the operations engineers in the team should \nworry about where the application is deployed and how optimally it is deployed. \nThey also should not worry about how the capacity is managed.\nThis level of sophistication requires a cloud platform with self-service capabilities, \nsuch as what we discussed in Chapter 9, Managing Dockerized Microservices with \nMesos and Marathon, with the Mesos and Marathon cluster management solutions. \nContainerized deployment discussed in Chapter 8, Containerizing Microservices with \nDocker, is also important in managing and end to-end-automation. Building this  \nself-service cloud ecosystem is a prerequisite for microservice development.\nBuilding a microservices ecosystem\nAs we discussed in the capability model in Chapter 3, Applying Microservices Concepts, \nmicroservices require a number of other capabilities. All these capabilities should be \nin place before implementing microservices at scale.\nThese capabilities include service registration, discovery, API gateways, and an \nexternalized configuration service. All are provided by the Spring Cloud project. \nCapabilities such as centralized logging, monitoring, and so on are also required  \nas a prerequisite for microservices development.\n\n\nChapter 10\n[ 385 ]\nDefining a DevOps-style microservice life \ncycle process\nDevOps is the best-suited practice for microservices development. Organizations \nalready practicing DevOps do not need another practice for microservices \ndevelopment.\nIn this section, we will explore the life cycle of microservices development. Rather \nthan reinventing a process for microservices, we will explore DevOps processes  \nand practices from the microservice perspective.\nBefore we explore DevOps processes, let's iron out some of the common \nterminologies used in the DevOps world:\n•\t\nContinuous integration (CI): This automates the application build and \nquality checks continuously in a designated environment, either in a  \ntime-triggered manner or on developer commits. CI also publishes code \nmetrics to a central dashboard as well as binary artifacts to a central \nrepository. CI is popular in Agile development practices.\n•\t\nContinuous delivery (CD): This automates the end-to-end software delivery \npractice from idea to production. In a non-DevOps model, this used to be \nknown as Application Lifecycle Management (ALM). One of the common \ninterpretations of CD is that it is the next evolution of CI, which adds QA \ncycles into the integration pipeline and makes the software ready to release \nto production. A manual action is required to move it to production.\n•\t\nContinuous deployment: This is an approach to automating the deployment \nof application binaries to one or more environments by managing binary \nmovement and associated configuration parameters. Continuous deployment \nis also considered as the next evolution of CD by integrating automatic \nrelease processes into the CD pipeline.\n•\t\nApplication Release Automation (ARA): ARA tools help monitor and \nmanage end-to-end delivery pipelines. ARA tools use CI and CD tools and \nmanage the additional steps of release management approvals. ARA tools \nare also capable of rolling out releases to different environments and rolling \nthem back in case of a failed deployment. ARA provides a fully orchestrated \nworkflow pipeline, implementing delivery life cycles by integrating many \nspecialized tools for repository management, quality assurance, deployment, \nand so on. XL Deploy and Automic are some of the ARA tools.\n\n\nThe Microservices Development Life Cycle\n[ 386 ]\nThe following diagram shows the DevOps process for microservices development:\nLet's now further explore these life cycle stages of microservices development.\nValue-driven planning\nValue-driven planning is a term used in Agile development practices. Value-driven \nplanning is extremely important in microservices development. In value-driven \nplanning, we will identify which microservices to develop. The most important \naspect is to identify those requirements that have the highest value to business and \nthose that have the lowest risks. The MVP philosophy is used when developing \nmicroservices from the ground up. In the case of monolithic to microservices \nmigration, we will use the guidelines provided in Chapter 3, Applying Microservices \nConcepts, to identify which services have to be taken first. The selected microservices \nare expected to precisely deliver the expected value to the business. Business KPIs  \nto measure this value have to be identified as part of value-driven planning.\nAgile development\nOnce the microservices are identified, development must be carried out in an Agile \napproach following the Agile manifesto principles. The scrum methodology is used \nby most of the organizations for microservices development.\nContinuous integration\nThe continuous integration steps should be in place to automatically build the source \ncode produced by various team members and generate binaries. It is important to \nbuild only once and then move the binary across the subsequent phases. Continuous \nintegration also executes various QAs as part of the build pipeline, such as code \ncoverage, security checks, design guidelines, and unit test cases. CI typically delivers \nbinary artefacts to a binary artefact repository and also deploys the binary artefacts \ninto one or more environments. Part of the functional testing also happens as part  \nof CI.\n\n\nChapter 10\n[ 387 ]\nContinuous testing\nOnce continuous integration generates the binaries, they are moved to the testing \nphase. A fully automated testing cycle is kicked off in this phase. It is also important \nto automate security testing as part of the testing phase. Automated testing \nhelps improve the quality of deliverables. The testing may happen in multiple \nenvironments based on the type of testing. This could range from the integration  \ntest environment to the production environment to test in production.\nContinuous release\nContinuous release to production takes care of actual deployment, infrastructure \nprovisioning, and rollout. The binaries are automatically shipped and deployed to \nproduction by applying certain rules. Many organizations stop automation with the \nstaging environment and make use of manual approval steps to move to production.\nContinuous monitoring and feedback\nThe continuous monitoring and feedback phase is the most important phase in Agile \nmicroservices development. In an MVP scenario, this phase gives feedback on the \ninitial acceptance of the MVP and also evaluates the value of the service developed. \nIn a feature addition scenario, this further gives insight into how this new feature \nis accepted by users. Based on the feedback, the services are adjusted and the same \ncycle is then repeated.\nAutomating the continuous delivery pipeline\nIn the previous section, we discussed the life cycle of microservices development. \nThe life cycle stages can be altered by organizations based on their organizational \nneeds but also based on the nature of the application. In this section, we will take \na look at a sample continuous delivery pipeline as well as toolsets to implement a \nsample pipeline.\nThere are many tools available to build end-to-end pipelines, both in the open  \nsource and commercial space. Organizations can select the products of their  \nchoice to connect pipeline tasks.\nRefer to the XebiaLabs periodic table for a tool reference to build \ncontinuous delivery pipelines. It is available at https://xebialabs.\ncom/periodic-table-of-devops-tools/.\n\n\nThe Microservices Development Life Cycle\n[ 388 ]\nThe pipelines may initially be expensive to set up as they require many toolsets \nand environments. Organizations may not realize an immediate cost benefit in \nimplementing the delivery pipeline. Also, building a pipeline needs high-power \nresources. Large build pipelines may involve hundreds of machines. It also takes \nhours to move changes through the pipeline from one end to the other. Hence, it is \nimportant to have different pipelines for different microservices. This will also help \ndecoupling between the releases of different microservices.\nWithin a pipeline, parallelism should be employed to execute tests on different \nenvironments. It is also important to parallelize the execution of test cases as much \nas possible. Hence, designing the pipeline based on the nature of the application is \nimportant. There is no one size fits all scenario.\nThe key focus in the pipeline is on end-to-end automation, from development  \nto production, and on failing fast if something goes wrong.\nThe following pipeline is an indicative one for microservices and explores the \ndifferent capabilities that one should consider when developing a microservices \npipeline:\nThe continuous delivery pipeline stages are explained in the following sections.\n\n\nChapter 10\n[ 389 ]\nDevelopment\nThe development stage has the following activities from a development perspective. \nThis section also indicates some of the tools that can be used in the development \nstage. These tools are in addition to the planning, tracking, and communication tools \nsuch as Agile JIRA, Slack, and others used by Agile development teams. Take a look \nat the following:\n•\t\nSource code: The development team requires an IDE or a development \nenvironment to cut source code. In most organizations, developers get the \nfreedom to choose the IDEs they want. Having said this, the IDEs can be \nintegrated with a number of tools to detect violations against guidelines. \nGenerally, Eclipse IDEs have plugins for static code analysis and code \nmatrices. SonarQube is one example that integrates other plugins such as \nCheckstyle for code conventions, PMD to detect bad practices, FindBugs \nto detect potential bugs, and Cobertura for code coverage. It is also \nrecommended to use Eclipse plugins such as ESVD, Find Security Bugs, \nSonarQube Security Rules, and so on to detect security vulnerabilities.\n•\t\nUnit test cases: The development team also produces unit test cases \nusing JUnit, NUnit, TestNG, and so on. Unit test cases are written against \ncomponents, repositories, services, and so on. These unit test cases are \nintegrated with the local Maven builds. The unit test cases targeting the \nmicroservice endpoints (service tests) serve as the regression test pack.  \nWeb UI, if written in AngularJS, can be tested using Karma.\n•\t\nConsumer-driven contracts: Developers also write CDCs to test integration \npoints with other microservices. Contract test cases are generally written \nas JUnit, NUnit, TestNG, and so on and are added to the service tests pack \nmentioned in the earlier steps.\n•\t\nMock testing: Developers also write mocks to simulate the integration \nendpoints to execute unit test cases. Mockito, PowerMock, and others are \ngenerally used for mock testing. It is good practice to deploy a mock service \nbased on the contract as soon as the service contract is identified. This acts as \na simple mechanism for service virtualization for the subsequent phases.\n•\t\nBehavior driven design (BDD): The Agile team also writes BDD scenarios \nusing a BDD tool, such as Cucumber. Typically, these scenarios are targeted \nagainst the microservices contract or the user interface that is exposed by a \nmicroservice-based web application. Cucumber with JUnit and Cucumber \nwith Selenium WebDriver, respectively, are used in these scenarios. Different \nscenarios are used for functional testing, user journey testing, as well as \nacceptance testing.\n\n\nThe Microservices Development Life Cycle\n[ 390 ]\n•\t\nSource code repository: A source control repository is a part and parcel of \ndevelopment. Developers check-in their code to a central repository, mostly \nwith the help of IDE plugins. One microservice per repository is a common \npattern used by many organizations. This disallows other microservice \ndevelopers from modifying other microservices or writing code based on  \nthe internal representations of other microservices. Git and Subversion are \nthe popular choices to be used as source code repositories.\n•\t\nBuild tools: A build tool such as Maven or Gradle is used to manage \ndependencies and build target artifacts—in this case, Spring Boot services. \nThere are many cases, such as basic quality checks, security checks and unit \ntest cases, code coverage, and so on, that are integrated as part of the build \nitself. These are similar to the IDE, especially when IDEs are not used by \ndevelopers. The tools that we examined as part of the IDEs are also available \nas Maven plugins. The development team does not use containers such as \nDocker until the CI phase of the project. All the artifacts have to be versioned \nproperly for every change.\n•\t\nArtifact repository: The artifact repository plays a pivotal role in the \ndevelopment process. The artifact repository is where all build artifacts  \nare stored. The artifact repository could be Artifactory, Nexus, or any  \nsimilar product.\n•\t\nDatabase schemas: Liquibase and Flyway are commonly used to manage, \ntrack, and apply database changes. Maven plugins allow interaction with \nthe Liquibase or Flyway libraries. The schema changes are versioned and \nmaintained, just like source code.\nContinuous integration\nOnce the code is committed to the repository, the next phase, continuous integration, \nautomatically starts. This is done by configuring a CI pipeline. This phase builds the \nsource code with a repository snapshot and generates deployable artifacts. Different \norganizations use different events to kickstart the build. A CI start event may be on \nevery developer commit or may be based on a time window, such as daily, weekly, \nand so on.\nThe CI workflow is the key aspect of this phase. Continuous integration tools such as \nJenkins, Bamboo, and others play the central role of orchestrating the build pipeline. \nThe tool is configured with a workflow of activities to be invoked. The workflow \nautomatically executes configured steps such as build, deploy, and QA. On the \ndeveloper commit or on a set frequency, the CI kickstarts the workflow.\n\n\nChapter 10\n[ 391 ]\nThe following activities take place in a continuous integration workflow:\n1.\t Build and QA: The workflow listens to Git webhooks for commits. Once it \ndetects a change, the first activity is to download the source code from the \nrepository. A build is executed on the downloaded snapshot source code. \nAs part of the build, a number of QA checks are automatically performed, \nsimilarly to QA executed in the development environment. These include \ncode quality checks, security checks, and code coverage. Many of the QAs \nare done with tools such as SonarQube, with the plugins mentioned earlier. It \nalso collects code metrics such as code coverage and more and publishes it to \na central database for analysis. Additional security checks are executed using \nOWASP ZAP Jenkins' plugins. As part of the build, it also executes JUnit or \nsimilar tools used to write test cases. If the web application supports Karma \nfor UI testing, Jenkins is also capable of running web tests written in Karma. \nIf the build or QA fails, it sends out alarms as configured in the system.\n2.\t Packaging: Once build and QA are passed, the CI creates a deployable \npackage. In our microservices case, it generates the Spring Boot standalone \nJAR. It is recommended to build Docker images as part of the integration \nbuild. This is the one and only place where we build binary artifacts. Once \nthe build is complete, it pushes the immutable Docker images to a Docker \nregistry. This could be on Docker Hub or a private Docker registry. It is \nimportant to properly version control the containers at this stage itself.\n3.\t Integration tests: The Docker image is moved to the integration environment \nwhere regression tests (service tests) and the like are executed. This \nenvironment has other dependent microservices capabilities, such as \nSpring Cloud, logging, and so on, in place. All dependent microservices \nare also present in this environment. If an actual dependent service is not \nyet deployed, service virtualization tools such as MockServer are used. \nAlternately, a base version of the service is pushed to Git by the respective \ndevelopment teams. Once successfully deployed, Jenkins triggers service tests \n(JUnits against services), a set of end-to-end sanity tests written in Selenium \nWebDriver (in the case of web) and security tests with OWASP ZAP.\nAutomated testing\nThere are many types of testing to be executed as part of the automated delivery \nprocess before declaring the build ready for production. The testing may happen \nby moving the application across multiple environments. Each environment is \ndesignated for a particular kind of testing, such as acceptance testing, performance \ntesting, and so on. These environments are adequately monitored to gather the \nrespective metrics.\n\n\nThe Microservices Development Life Cycle\n[ 392 ]\nIn a complex microservices environment, testing should not be seen as a last-minute \ngate check; rather, testing should be considered as a way to improve software quality \nas well as to avoid last-minute failures. Shift left testing is an approach of shifting \ntests as early as possible in the release cycle. Automated testing turns software \ndevelopment to every-day development and every-day testing mode. By automating \ntest cases, we will avoid manual errors as well as the effort required to complete \ntesting.\nCI or ARA tools are used to move Docker images across multiple test environments. \nOnce deployed in an environment, test cases are executed based on the purpose \nof the environment. By default, a set of sanity tests are executed to verify the test \nenvironment.\nIn this section, we will cover all the types of tests that are required in the automated \ndelivery pipeline, irrespective of the environment. We have already considered  \nsome types of tests as part of the development and integration environment. Later  \nin this section, we will also map test cases against the environments in which they \nare executed.\nDifferent candidate tests for automation\nIn this section, we will explore different types of tests that are candidates for \nautomation when designing an end-to-end delivery pipeline. The key testing  \ntypes are described as follows.\nAutomated sanity tests\nWhen moving from one environment to another, it is advisable to run a few \nsanity tests to make sure that all the basic things are working. This is created as \na test pack using JUnit service tests, Selenium WebDriver, or a similar tool. It is \nimportant to carefully identify and script all the critical service calls. Especially if the \nmicroservices are integrated using synchronous dependencies, it is better to consider \nthese scenarios to ensure that all dependent services are also up and running.\nRegression testing\nRegression tests ensure that changes in software don't break the system. In a \nmicroservices context, the regression tests could be at the service level (Rest API or \nmessage endpoints) and written using JUnit or a similar framework, as explained \nearlier. Service virtualizations are used when dependent services are not available. \nKarma and Jasmine can be used for web UI testing.\n\n\nChapter 10\n[ 393 ]\nIn cases where microservices are used behind web applications, Selenium WebDriver \nor a similar tool is used to prepare regression test packs, and tests are conducted at \nthe UI level rather than focusing on the service endpoints. Alternatively, BDD tools, \nsuch as Cucumber with JUnit or Cucumber with Selenium WebDriver, can also be \nused to prepare regression test packs. CI tools such as Jenkins or ARA are used  \nto automatically trigger regression test packs. There are other commercial tools,  \nsuch as TestComplete, that can also be used to build regression test packs.\nAutomated functional testing\nFunctional test cases are generally targeted at the UIs that consume the \nmicroservices. These are business scenarios based on user stories or features.  \nThese functional tests are executed on every build to ensure that the microservice  \nis performing as expected.\nBDD is generally used in developing functional test cases. Typically in BDD, \nbusiness analysts write test cases in a domain-specific language but in plain English. \nDevelopers then add scripts to execute these scenarios. Automated web testing tools \nsuch as Selenium WebDriver are useful in such scenarios, together with BDD tools \nsuch as Cucumber, JBehave, SpecFlow, and so on. JUnit test cases are used in the \ncase of headless microservices. There are pipelines that combine both regression \ntesting and functional testing as one step with the same set of test cases.\nAutomated acceptance testing\nThis is much similar to the preceding functional test cases. In many cases, automated \nacceptance tests generally use the screenplay or journey pattern and are applied at \nthe web application level. The customer perspective is used in building the test cases \nrather than features or functions. These tests mimic user flows.\nBDD tools such as Cucumber, JBehave, and SpecFlow are generally used in these \nscenarios together with JUnit or Selenium WebDriver, as discussed in the previous \nscenario. The nature of the test cases is different in functional testing and acceptance \ntesting. Automation of acceptance test packs is achieved by integrating them \nwith Jenkins. There are many other specialized automatic acceptance testing tools \navailable on the market. FitNesse is one such tool.\n\n\nThe Microservices Development Life Cycle\n[ 394 ]\nPerformance testing\nIt is important to automate performance testing as part of the delivery pipeline. This \npositions performance testing from a gate check model to an integral part of the \ndelivery pipeline. By doing so, bottlenecks can be identified at very early stages of \nbuild cycles. In some organizations, performance tests are conducted only for major \nreleases, but in others, performance tests are part of the pipeline. There are multiple \noptions for performance testing. Tools such as JMeter, Gatling, Grinder, and so on \ncan be used for load testing. These tools can be integrated into the Jenkins workflow \nfor automation. Tools such as BlazeMeter can then be used for test reporting.\nApplication Performance Management tools such as AppDynamics, New Relic, \nDynatrace, and so on provide quality metrics as part of the delivery pipeline. This \ncan be done using these tools as part of the performance testing environment. In \nsome pipelines, these are integrated into the functional testing environment to get \nbetter coverage. Jenkins has plugins in to fetch measurements.\nReal user flow simulation or journey testing\nThis is another form of test typically used in staging and production environments. \nThese tests continuously run in staging and production environments to ensure \nthat all the critical transactions perform as expected. This is much more useful \nthan a typical URL ping monitoring mechanism. Generally, similar to automated \nacceptance testing, these test cases simulate user journeys as they happen in the real \nworld. These are also useful to check whether the dependent microservices are up \nand running. These test cases could be a carved-out subset of acceptance test cases  \nor test packs created using Selenium WebDriver.\nAutomated security testing\nIt is extremely important to make sure that the automation does not violate the \nsecurity policies of the organization. Security is the most important thing, and \ncompromising security for speed is not desirable. Hence, it is important to integrate \nsecurity testing as part of the delivery pipeline. Some security evaluations are \nalready integrated in the local build environment as well as in the integration \nenvironment, such as SonarQube, Find Security Bugs, and so on. Some security \naspects are covered as part of the functional test cases. Tools such as BDD-Security, \nMittn, and Gauntlt are other security test automation tools following the BDD \napproach. VAPT can be done using tools such as ImmuniWeb. OWASP ZAP and \nBurp Suite are other useful tools in security testing.\n\n\nChapter 10\n[ 395 ]\nExploratory testing\nExploratory testing is a manual testing approach taken by testers or business users \nto validate the specific scenarios that they think automated tools may not capture. \nTesters interact with the system in any manner they want without prejudgment. \nThey use their intellect to identify the scenarios that they think some special users \nmay explore. They also do exploratory testing by simulating certain user behavior.\nA/B testing, canary testing, and blue-green deployments\nWhen moving applications to production, A/B testing, blue-green deployments,  \nand canary testing are generally applied. A/B testing is primarily used to review  \nthe effectiveness of a change and how the market reacts to the change. New features \nare rolled out to a certain set of users. Canary release is moving a new product or \nfeature to a certain community before fully rolling out to all customers. Blue-green is \na deployment strategy from an IT point of view to test the new version of a service. \nIn this model, both blue and green versions are up and running at some point of  \ntime and then gracefully migrate from one to the other.\nOther nonfunctional tests\nHigh availability and antifragility testing (failure injection tests) are also important \nto execute before production. This helps developers unearth unknown errors that \nmay occur in a real production scenario. This is generally done by breaking the \ncomponents of the system to understand their failover behavior. This is also helpful \nto test circuit breakers and fallback services in the system. Tools such as Simian \nArmy are useful in these scenarios.\nTesting in production\nTesting in Production (TiP) is as important as all the other environments as we can \nonly simulate to a certain extend. There are two types of tests generally executed \nagainst production. The first approach is running real user flows or journey tests in a \ncontinuous manner, simulating various user actions. This is automated using one of \nthe Real User Monitoring (RUM) tools, such as AppDynamics. The second approach \nis to wiretap messages from production, execute them in a staging environment, and \nthen compare the results in production with those in the staging environment.\n\n\nThe Microservices Development Life Cycle\n[ 396 ]\nAntifragility testing\nAntifragility testing is generally conducted in a preproduction environment identical \nto production or even in the production environment by creating chaos in the \nenvironment to take a look at how the application responds and recovers from these \nsituations. Over a period of time, the application gains the ability to automatically \nrecover from most of these failures. Simian Army is one such tool from Netflix. \nSimian Army is a suite of products built for the AWS environment. Simian Army is \nfor disruptive testing using a set of autonomous monkeys that can create chaos in  \nthe preproduction or production environments. Chaos Monkey, Janitor Monkey,  \nand Conformity Monkey are some of the components of Simian Army.\nTarget test environments\nThe different test environments and the types of tests targeted on these environments \nfor execution are as follows:\n•\t\nDevelopment environment: The development environment is used to test \nthe coding style checks, bad practices, potential bugs, unit tests, and basic \nsecurity scanning.\n•\t\nIntegration test environment: Integration environment is used for unit \ntesting and regression tests that span across multiple microservices.  \nSome basic security-related tests are also executed in the integration  \ntest environment.\n•\t\nPerformance and diagnostics: Performance tests are executed in the \nperformance test environment. Application performance testing tools  \nare deployed in these environments to collect performance metrics and \nidentify bottlenecks.\n•\t\nFunctional test environment: The functional test environment is used to \nexecute a sanity test and functional test packs.\n•\t\nUAT environment: The UAT environment has sanity tests, automated \nacceptance test packs, and user journey simulations.\n•\t\nStaging: The preproduction environment is used primarily for sanity tests, \nsecurity, antifragility, network tests, and so on. It is also used for user journey \nsimulations and exploratory testing.\n•\t\nProduction: User journey simulations and RUM tests are continuously \nexecuted in the production environment.\nMaking proper data available across multiple environments to support test cases is \nthe biggest challenge. Delphix is a useful tool to consider when dealing with test data \nacross multiple environments in an effective way.\n\n\nChapter 10\n[ 397 ]\nContinuous deployment\nContinuous deployment is the process of deploying applications to one or more \nenvironments and configuring and provisioning these environments accordingly. As \ndiscussed in Chapter 9, Managing Dockerized Microservices with Mesos and Marathon, \ninfrastructure provisioning and automation tools facilitate deployment automation.\nFrom the deployment perspective, the released Docker images are moved to \nproduction automatically once all the quality checks are successfully completed. \nThe production environment, in this case, has to be cloud based with a cluster \nmanagement tool such as Mesos or Marathon. A self-service cloud environment  \nwith monitoring capabilities is mandatory.\nCluster management and application deployment tools ensure that application \ndependencies are properly deployed. This automatically deploys all the \ndependencies that are required in case any are missing. It also ensures that a \nminimum number of instances are running at any point in time. In case of failure, it \nautomatically rolls back the deployments. It also takes care of rolling back upgrades \nin a graceful manner.\nAnsible, Chef, or Puppet are tools useful in moving configurations and binaries to \nproduction. The Ansible playbook concepts can be used to launch a Mesos cluster \nwith Marathon and Docker support.\nMonitoring and feedback\nOnce an application is deployed in production, monitoring tools continuously \nmonitor its services. Monitoring and log management tools collect and analyze \ninformation. Based on the feedback and corrective actions needed, information is \nfed to the development teams to take corrective actions, and the changes are pushed \nback to production through the pipeline. Tools such as APM, Open Web Analytics, \nGoogle Analytics, Webalizer, and so on are useful tools to monitor web applications. \nReal user monitoring should provide end-to-end monitoring. QuBit, Boxever, \nChannel Site, MaxTraffic, and so on are also useful in analyzing customer behavior.\nAutomated configuration management\nConfiguration management also has to be rethought from a microservices and \nDevOps perspective. Use new methods for configuration management rather than \nusing a traditional statically configured CMDB. The manual maintenance of CMDB \nis no longer an option. Statically managed CMDB requires a lot of mundane tasks \nto maintain entries. At the same time, due to the dynamic nature of the deployment \ntopology, it is extremely hard to maintain data in a consistent way.\n\n\nThe Microservices Development Life Cycle\n[ 398 ]\nThe new styles of CMDB automatically create CI configurations based on an \noperational topology. These should be discovery based to get up-to-date information. \nThe new CMDB should be capable of managing bare metals, virtual machines,  \nand containers.\nMicroservices development governance, \nreference architectures, and libraries\nIt is important to have an overall enterprise reference architecture and a standard \nset of tools for microservices development to ensure that development is done in a \nconsistent manner. This helps individual microservices teams to adhere to certain \nbest practices. Each team may identify specialized technologies and tools that are \nsuitable for their development. In a polyglot microservices development, there are \nobviously multiple technologies used by different teams. However, they have to \nadhere to the arching principles and practices.\nFor quick wins and to take advantage of timelines, microservices development \nteams may deviate from these practices in some cases. This is acceptable as long as \nthe teams add refactoring tasks in their backlogs. In many organizations, although \nthe teams make attempts to reuse something from the enterprise, reuse and \nstandardization generally come as an afterthought.\nIt is important to make sure that the services are catalogued and visible in the \nenterprise. This improves the reuse opportunities of microservices.\nSummary\nIn this chapter, you learned about the relationship between microservices and \nDevOps. We also examined a number of practice points when developing \nmicroservices. Most importantly, you learned the microservices development  \nlife cycle.\nLater in this chapter, we also examined how to automate the microservices delivery \npipeline from development to production. As part of this, we examined a number of \ntools and technologies that are helpful when automating the microservices delivery \npipeline. Finally, we touched base with the importance of reference architectures in \nmicroservices governance.\nPutting together the concepts of microservices, challenges, best practices, and various \ncapabilities covered in this book makes a perfect recipe for developing successful \nmicroservices at scale.\n\n\n[ 399 ]\nIndex\nA\nagents  348\nAgile software development\nURL  380\nAirbnb\nabout  45\nURL  45\nAmazon\nabout  46\nURL  46\nAmazon EC2 Container Service (ECS)  321\nAngel  207\nApache Mesos  346, 347\napplication\nautoscaling  266\napplication information\nconfiguring  99\nappropriate microservice boundaries\nestablishing  106\narchitecture, Mesos\nabout  349\nframework  350\nmaster  350\nslave  350\nZooKeeper  350\nautoconfig  98\nautoscaling\nabout  262, 263\nbenefits  263-265\ndifferent autoscaling models  265\nin cloud  267\nautoscaling approaches\nabout  268\npredictive autoscaling  271\nscaling, based on business parameters  270\nscaling, based on message  \nqueue length  270\nscaling, during specific time periods  269\nscaling, with resource constraints  268\nautoscaling ecosystem\nabout  273\nlife cycle manager  273\nload balancer  273\nMicroservices  273\nservice registry  273\nAWS\nsetting up  354, 355\nAWS Lambda\nreference  5\nB\nBackend as a Service (BaaS)  44\nbenefits, microservices\nabout  23\nbuild organic systems, enabling  28\ncoexistence of different versions,  \nallowing  30\nDevOps, enabling  33\nelastically scalable  25-27\nevent-driven architecture, supporting  32\nexperimentation, enabling  24\ninnovation, enabling  25\npolyglot architecture support  23, 24\nselectively scalable  26, 27\nself-organizing systems, building  31\nsubstitution, allowing  27, 28\ntechnology debt, reducing  29\nbest-of-the-breed components\ndashboards  290\nlog shippers  289\n\n\n[ 400 ]\nlog storage  289\nbill of materials (BOM)  63\nBin packing algorithm  343\nBoot2Docker  321\nbounded context  106, 107\nBrixton  207\nBrownField PSS\nenvironment, setting up for  210\nBrownField PSS architecture\ndefining  256, 257\nBrownField PSS microservices\nlimitations  315\nBrownField PSS microservices, autoscaling\nabout  272\ncapabilities  272\ncustom life cycle manager, implementing \nwith Spring Boot  274\ndeployment topology  274, 275\nexecution flow  275, 276\nlife cycle manager code  277-280\nlife cycle manager, running  281, 282\nBrownField PSS services\ndeploying  363, 364\npreparing  361, 362\nreviewing  365, 366\nBrownField services\nrunning, on EC2  332, 333\nbusiness case\ndefining  165\nBusiness Process Management (BPM)\nabout  121\nuse cases  121\nC\nCalico  316\ncandidate tests, for automation\nabout  392\nA/B testing  395\nantifragility testing  396\nautomated acceptance testing  393\nautomated functional testing  393\nautomated sanity tests  392\nautomated security testing  394\nblue-green deployments  395\ncanary testing  395\nexploratory testing  395\nnonfunctional tests  395\nperformance testing  394\nreal user flow simulation or  \njourney testing  394\nregression testing  392\nTesting in Production (TiP)  395\nCDNs (Content Delivery Networks)  164\ncentralized logging solution\nabout  286\nlog dashboard  287\nlog shippers  287\nlog store  287\nlog stream processor  287\nlog streams  287\ncgroups  325, 348\ncharacteristics, of microservices\nabout  10\nantifragility  16\nautomation  14\ncharacteristics of services  11, 12\ndistributed and dynamic  15, 16\nfail fast  17\nlightweight  12, 13\npolyglot architecture  13, 14\nself-healing  17\nservices are first-class citizens  11\nsupporting ecosystem  15\ncharacteristics of services, microservices\nloose coupling  11\nservice abstraction  11\nservice composeability  12\nservice contract  11\nservice interoperability  12\nservice reuse  11\nservices are discoverable  12\nstatelessness  12\ncircuit breaker pattern\nreference  302\ncloud\nas self-service infrastructure, for  \nmicroservices  376\ncluster management\nabout  340\nfeatures  340\nfunctioning  341\nrelationship with microservices  344\n\n\n[ 401 ]\nrelationship with virtualization  344\nwith Mesos and Marathon  348\ncluster management solutions\nabout  344, 345\nApache Mesos  346, 347\nDocker Swarm  345\nFleet  347\nKubernetes  346\nNomad  347\nCommand Query Responsibility  \nSegregation (CQRS)  109\ncommunication styles, designing\nasynchronous style communication  112\nstyle, selecting  112-114\nsynchronous style communication  111\ncomponents, Spring Cloud\nNetflix OSS  210\ncomprehensive microservice example\ndeveloping  86-96\nConfig server URL\nConfig Server, accessing  \nfrom clients  218-222\ndefining  217\nreferences  218\nconfigurable items (CIs)  143\nConfiguration Management Database \n(CMDB)  143\ncontainerization\nabout  334\nfuture  334\nsecurity issues  334\nunikernels  334\ncontainers\nabout  316\nbenefits  319\nDevOps  319\nimmutable containers  320\nlightweight  319\nlower license cost  319\nportable  319\nreusable  320\nscalable  319\nself-contained  319\nversion controlled  319\nversus, virtual machines (VMs)  317, 318\nContent Management System (CMS)  23\ncontinuous delivery pipeline, automating\nabout  387\nautomated testing  391, 392\ncontinuous deployment  397\ncontinuous integration  390\ndevelopment stage  389\nmonitoring and feedback  397\nContinuous Integration (CI) tools  14\ncontinuous integration workflow\nabout  391\nbuild and QA  391\nintegration tests  391\npackaging  391\nConway's Law\nreference  45\ncore capabilities, microservices  \ncapability model\nabout  145\nAPI gateway  146\nbusiness capability definition  145\nevent sourcing  145\nservice endpoints and communication \nprotocols  146\nservice listeners  145\nstorage capability  145\nuser interfaces  146\nCorporate Customer microservice  29\ncreate, read, update, and delete (CRUD)  86\ncross-origin\nenabling, for microservices  82, 83\nCustomer microservices  29\nCustomer Profile microservice\nabout  11\nGet Customer  11\nRegister Customer  11\nCustomer service  31\ncustom health module\nadding  99-101\ncustom metrics, building  101, 102\nD\ndata analysis\ndata lakes, used  310, 311\nDatabase Lifecycle Management (DLM)  381\ndata lake  310\n\n\n[ 402 ]\ndefault embedded web server\nchanging  77\ndependencies, microservice boundaries\ndependency graph  176\nevents, as opposed to query  169, 170\nevents, as opposed to synchronous  \nupdates  170-172\nrequirements, challenging  172, 173\nservice boundaries, challenging  173-175\ndevelopment environment\nsetting up  49, 50\ndevelopment philosophy\nAgile practice  379\ndesign thinking  378\nselecting  378\nstart-up model  379\ndevelopment stage\nabout  389\nartifact repository  390\nbehavior driven design (BDD)  389\nbuild tool  390\nconsumer-driven contracts  389\ndatabase schemas  390\nmock testing  389\nsource code  389\nsource code repository  390\nunit test cases  389\nDevOps\nabout  7\nas practice and process, for  \nmicroservices  376\ndefining  372-374\ndevelopment and operations, bridging  375\ntasks, automating  374\nURL  7\nvalue-driven delivery  374\nwastage, reducing  374\nDevOps-style microservice life cycle process\nAgile development  386\nApplication Release Automation  \n(ARA)  385\ncontinuous delivery (CD)  385\ncontinuous deployment  385\ncontinuous integration (CI)  385, 386\ncontinuous monitoring and feedback  387\ncontinuous release  387\ncontinuous testing  387\ndefining  385\nvalue-driven planning  386\nDocker\nabout  316, 321\nconcepts  323\nkey components  322\nmicroservices, deploying  326-330\nRabbitMQ, running on  330\nURL  326\nDocker client  322\nDocker concepts\nDocker containers  325\nDockerfile  326\nDocker images  323, 324\nDocker registry  325, 326\nDocker daemon  322\nDocker file\ncontents  327, 328\nDocker Hub account\nreference  331\nDockerization  321\nDocker registry\nDocker hub, setting up  331\nmicroservices, publishing to  \nDocker hub  331\nusing  330\nDocker Swarm  345\ndomain-driven design (DDD)\nabout  106\nURL  106\nDrawbridge  316\nDrools  120\ndump  98\ndynamic discovery\ndefining  232\ndynamic resource provisioning and  \ndeprovisioning  263\ndynamic service registration\ndefining  232\nE\neBay\nabout  46\nURL  46\n\n\n[ 403 ]\necosystem capabilities\nbuilding  187\nelasticity  263\nElasticsearch\nURL  290\nengine  348\nEnterprise Application integration  \n(EAI)  191\nEnterprise Integration Patterns (EIP)  35\nEnterprise Service Bus (ESB)  16\nEureka\ndefining  234, 235\nhigh availability  241-243\nserver, setting up  235-241\nevolution\nplanning  165\nevolution, microservices\nabout  1\nbusiness demand, as catalyst  2, 3\nimperative architecture evolution  4, 5\ntechnology, as catalyst  4\nexamples, microservices\nabout  17\nholiday portal  17-19\nmicroservice-based order management \nsystem  20, 21\ntravel agent portal  22, 23\nF\nfactors\ndefining  177, 178\nFakeSMTP\nreference  95\nFeign\ndefining  227-229\nFleet  347\nFly By Points  17\nfollowers  347\nG\ngateway  325\nGilt\nabout  46\nURL  46\nGuest OS  317\nH\nHAL (Hypertext Application Language) \nbrowser  68\nHATEOAS example  68\nhealth  98\nhexagonal architecture, microservices\nreference  6\nhigh availability\nURL  226\nhigh availability, of Zuul\nabout  249\nEureka client  250\nnot Eureka client  251\nhoneycomb analogy  8\nHypertext As The Engine Of Application \nState. See  HATEOAS example\nI\ninfo  98\ninfrastructure\nautoscaling  266\ninfrastructure capabilities, microservices \ncapability model\nabout  146\napplication life cycle management  147\ncloud  146\ncluster control and provisioning  147\ncontainers or virtual machines  146\nin-memory data grid (IMDG)  27\nINPUT queue  31\ninstance ID  361\nIntegration Platform as a Service (iPaaS)  4\nIntelligent Business Process Management \n(iBPM)  121\nJ\nJDK 1.8\nreference  49\njobs  347\nJSch (Java Secure Channel)  279\nJSON structure\ncontainer  364\ncpus  363\nid  363\nimage  364\n\n\n[ 404 ]\ninstances  363\nmem  363\nnetwork  364\nportMappings  364\nK\nkey capabilities, cluster management  \nsoftware\nabout  341\nagility  342\ncluster management  341\ndeployments  341\nhealth  341\ninfrastructure abstraction  342\nisolation  342\nresource allocation  342\nresource optimization  342\nscalability  341\nservice availability  342\nkey components, automated microservices \ndeployment topology\nabout  261\nEureka client  261\nRibbon client  261\nkey components, Docker\nabout  322\nDocker client  322\nDocker daemon  322\nKubernetes  346\nL\nLambda architecture\nreference  287\nleader  347\nlibraries  398\nlife cycle manager  \nabout  272, 273, 367\ndecision engine  273\ndeployment engine  273\ndeployment rules  273\nmetrics collection  273\nrewriting, with Mesos and Marathon  368\nscaling policies  273\nupdating  334\nLmctfy  316\nlogging solutions\nbest-of-the-breed integration  288\ncloud services  288\ncustom logging implementation  290-292\ndistributed tracing, with Spring Cloud \nSleuth  293-295\noff-the-shelf solutions  288\nselecting  288\nlog management challenges  284, 285\nLoyalty Points microservice  29\nLXD  316\nM\nmanager  345\nmappings  98\nMarathon\nabout  352\nfeatures  352\nimplementing, for BrownField  \nmicroservices  353, 354\ninstalling  356\nreference  356\nrunning as services  358\nMarathon autoscale  367\nmaster  346\nMaven 3.3.1\nreference  50\nMean Time Between Failures (MTBF)  17\nMean Time To Recover (MTTR)  17\nMesos\nabout  348, 349\narchitecture  349, 350\nconfiguring  357\nimplementing, for BrownField  \nmicroservices  353, 354\ninstalling  356\nreference  356\nrunning as services  358\nworkflow diagram  351\nMesos slave\nrunning,  in command line  359, 360\nmetrics  98\nMicroservice A  383\nMicroservice B  383\nmicroservice boundaries\nagile teams  109\n\n\n[ 405 ]\nautonomous functions  107\nchangeability  110\ncoupling  110\ndependencies, analyzing  167-169\nidentifying  167\nmicroservice, as product  110\nmost appropriate function  108\npolyglot architecture  108\nreplicability  110\nselective scaling  108\nsingle responsibility  109\nsize, of deployable unit  108\nsubdomain  108\nmicroservice capability model\nreviewing  260\nmicroservice monitoring\nabout  297\nactions  299\naggregation and correlation of metrics  299\nalerts  299\nApplication Performance Monitoring \n(APM) approach  300\nchallenges  298-300\ndashboards  299\nHystrix streams, aggregating  \nwith Turbine  307-310\nmetrics sources and data collectors  299\nmicroservice dependencies,  \nmonitoring  301, 302\nobjectives  298\nprocessing metrics and actionable  \ninsights  299\nreal user monitoring (RUM)  300\nSpring Cloud Hystrix, for fault-tolerant \nmicroservices  302-306\nsynthetic monitoring  300\ntools  300, 301\nuser experience monitoring  300\nmicroservices\nabout  1, 5\nand containers  320, 321\narchitecture  6\nbenefits  23, 165\ncharacteristics  10\nCustomer  9\ndata synchronization, during  \nmigration  178-180\ndefining  164\ndeploying, in Docker  326-330\ndocumenting  102, 103\nevolution  1\nexamples  17\nhexagonal architecture  6\nhoneycomb analogy  8\nOrder  9\nprinciples  8\nprioritizing, for migration  177, 178\nProduct  9\nreference data, managing  181-183\nrelationship, with other architecture   33\nscaling, with Spring Cloud  260-262\nuse cases  43\nmicroservices-based architecture\nexamining  7\nmicroservices capabilities\nreviewing  204\nmicroservices capability model\nabout  144, 145\ncore capabilities  145\ngovernance capabilities  145\ninfrastructure capabilities  145, 146\nprocess and governance capabilities  148\nreviewing  152, 153\nsupporting capabilities  145-147\nmicroservices challenges\nabout  139\ndata islands  139, 140\ndependency management  141\ngovernance challenges  142\ninfrastructure provisioning  144\nlogging  140, 141\nmicroservices, testing  143\nmonitoring  140, 141\noperation overheads  142\norganization culture  142\nmicroservices development\npractice points  377\nmicroservices development teams  398\nmicroservices, on cloud\nabout  332\nDocker, installing on AWS EC2  332\nMinimum Viable Product (MVP)\nabout  380\nusing  380\n\n\n[ 406 ]\nmodules\nmigrating  187\nN\nNetflix\nabout  45, 210\nURL  45\nNetflix Open Source Software  \n(Netflix OSS)  210\nNike\nabout  46\nURL  46\nnodes  345, 346\nNomad  347\nO\nOAuth\nreference  79\nOrbitz\nabout  46\nURL  46\nOrder Event  32\nOUTPUT queue  32\noversubscription  343\nP\nPassenger Sales and Service (PSS)  151\npatterns and common design decisions\nabout  105, 106\nAPI gateways, using in  \nmicroservices  131-133\nappropriate microservice boundaries,  \nestablishing  106\nbulk operations, in microservices  138\ncommunication styles, designing  111\ndata store, sharing  123, 124\ndesign, for cross origin  136\nESB and iPaaS, using with  \nmicroservices  133, 134\nmicroservice, on  multiple VMs  119\nmicroservice, on VM  119\nnumber of endpoints  118, 119\norchestration  115-118\nrole, of BPM and workflows  121-123\nrules engine  120, 121\nservice endpoint design consideration  127\nservice versioning considerations  134-136\nshared libraries, handling  129\nshared reference data, handling  136, 137\ntransaction boundaries, setting up  125\nuser interfaces, in microservices  130, 131\nPlatform as a Services (PaaS)  4\npods  346\nPOJO (Plain Old Java Object)  208\npom file\nreference  63\npractice points, microservices development\nabout  377\nautomated configuration management  397\nbusiness motivation and value  377\nchallenges around databases,  \naddressing  381\ncontinuous delivery pipeline,  \nautomating  387, 388\ndevelopment philosophy, selecting  378\nDevOps-style microservice life cycle  \nprocess, defining  385, 386\nlegacy hotspot  380\nmicroservices ecosystem, building  384\nmindset, changing from project to product \ndevelopment  377\nMinimum Viable Product (MVP), using  380\nself-organizing teams, establishing  381-383\nself-service cloud, building  384\nprinciples, of microservices\nabout  8\nautonomous services  9, 10\nsingle responsibility principle  8, 9\nprocess and governance capabilities,  \nmicroservices capability model\nabout  148\nDevOps  148\nDevOps tools  149\nmicroservice documentation  149\nmicroservices repositor   149\nreference architecture and libraries  149\nprotocol selection\nabout  127\nAPI documentations  128\nHTTP and REST endpoints  128\nmessage-oriented services  128\noptimized communication protocols  128\n\n\n[ 407 ]\nPSS application\nabout  158\narchitectural view  154, 155\nbusiness process view  153\ndefining  153\ndeployment view  157\ndesign view  155\ndomain boundaries  163\nfunctional view  154\nimplementation view  156\nlimitation  158\nshared data  160, 161\nsingle database  161\nstop gap fix  159\nPSS Implementation, BrownField\nreviewing  204, 205\nR\nRabbitMQ\nrunning, on Docker  330\nURL  226\nRandom algorithm  343\nreactive programming\nreference  20\nReal Application Cluster (RAC)  159\nReal User Monitoring (RUM) tools  395\nreference architectures  398\nrelationship, microservices\nabout  33\nrelations, with SOA  33, 34\nrelations, with Twelve-Factor Apps  37\nrelationship, microservices with SOA\nabout  33, 34\nlegacy modernization  35\nmonolithic application  36\nservice-oriented application  36\nservice-oriented integration  34, 35\nrelationship, microservices with  \nTwelve-Factor Apps\nabout  37\nbacking services  39, 40\nconcurrency, scaling out  41\nconfigurations, externalizing  39\ndependencies, bundling  38\ndevelopment and production parity  42\ndisposability, with minimal overhead  41\nlogs, externalizing  42\npackage admin processes  43\nservices, exposing through  \nport bindings  41\nsingle codebase  38\nRESTful microservices\nbuilding, Spring Boot used  56\nRESTful service\ndeveloping  50-55\nRibbon\nabout  230\ndefining, for load balancing  229-231\nRocket  316\nRPM (Requests Per Minute)  276\nS\nsales closing transactions  270\nScale Cube\nabout  26\nreference  26\nSearch API Gateway microservice  326\nservers  347\nservice availability\nand discovery  232\nregistering  232\nservice endpoint design consideration\nabout  127\ncontract design  127\nprotocol selection  127\nService Oriented Architecture (SOA)  1\nService-Oriented Integration (SOI)  35\nsession handling\nand security  184, 185\nshopping logic  16\nsingle database\nnative queries  161, 162\nstored procedures  163\nsingle sign-on (SSO)  183\nSMTP server  31\nSOA principles\nreference  12\nsoftware-defined infrastructure  376\nsoftware development life cycle (SDLC)  371\nSOLID design pattern\nreference  8\nspan  294\n\n\n[ 408 ]\nSpread algorithm  343\nSpring Boot\nabout  49, 57\nused, for building RESTful  \nmicroservices  56\nSpring Boot actuators\nabout  97\nmonitoring, JConsole used  98\nmonitoring, SSH used  99\nSpring Boot command-line tool\nreference  57\nSpring Boot configuration\n.yaml file, using  75\nabout  73\nautoconfiguration  73\nconfiguration file location, changing  74\ncustom properties, reading  75\ndefault configuration values, overriding  74\nmultiple configuration profiles, using  76\nproperties, reading  76\nSpring Boot Java microservice\nApplication.java, examining  64, 65\napplication.properties, examining  65\nApplicationTests.java, examining  65, 66\ndeveloping, STS used  58-62\npom file, examining  62-64\ntesting  67\nSpring Boot messaging\nimplementing  83-86\nSpring Boot microservice\ndeveloping, CLI used  57, 58\ndeveloping, Spring Initializr used  68-72\nSpring Boot security\nimplementing  77\nmicroservice, securing with  \nbasic security  77, 78\nmicroservice, securing with OAuth2  79-81\nSpring Cloud\nand Cloud Foundry, comparing  206\ncapabilities  208\ncomponents  207-209\ndefining  205, 206\nfor scaling microservices  260, 261\nreleases  206, 207\nSpring Cloud Config\nchanges, completing for Config  \nserver usage  227\nConfig server, for configuration files  226\nConfig server health, monitoring  226\nConfig server, setting up  214-216\nConfig server URL, defining  217\nConfig server, using  214\nconfiguration changes, handling  222\ndefining  211-213\nhigh availability, setting up for Config \nserver  224-226\nSpring Cloud Bus, for propagating  \nconfiguration changes  223, 224\nSpring Cloud examples\nreferences  215\nSpring Initializr project\nreference  57\nSpring Starter project\nURL  228\nSpring Tool Suite 3.7.2 (STS)\nreference  49\nstreams\nfor reactive microservices  252-256\nsubnet  325\nsupporting capabilities, microservices  \ncapability model\nabout  147\ncentral log management  147\ndata lake  148\ndependency and CI management  148\nmonitoring and dashboards  148\nreliable messaging  148\nsecurity service  148\nservice configuration  148\nservice registry  147\nsoftware defined load balancer  147\ntesting tools  148\nSystemdNspawn  316\nT\nt2.large  332\nt2.micro EC2 instances  354\ntarget architecture\nabout  188, 189\nexceptions, handling  191-194\nintegration, with other systems  191\ninternal layering, of microservices  189\nmicroservices, orchestrating  190\n\n\n[ 409 ]\nshared libraries, managing  191\ntarget implementation view\nabout  194\nimplementation projects  195, 196\nproject, running  196-200\nproject, testing  196-200\ntarget test environments\nabout  396\ndevelopment environment  396\nfunctional test environment  396\nintegration test environment  396\nperformance and diagnostics  396\nproduction  396\nstaging  396\nUAT environment  396\ntask groups  347\ntasks  347\ntechnology metamodel  368\nTesting in Production (TiP)  395\ntest strategy  186\nTPM (Transactions Per Minute)  276\ntrace  294\ntraditional web application\nmoving, to microservices  55\ntransaction boundaries\ndistributed transaction scenarios  126\nsetting up  125\nuse cases, altering  125, 126\ntransaction processing facility (TPF)  380\ntransition plan\nestablishing  166\ntransition point of view\nqueries  166\nTwitter\nabout  46\nURL  46\nU\nUber\nabout  45\nURL  45\nuse cases, microservices\nabout  43, 44\nmicroservices early adopters  45\nmonolithic migrations  47\nuser interfaces\nand web applications  183, 184\nV\nVeritas Risk Advisor (VRA)  302\nvirtual engine (VE)  316\nvirtual machines (VMs)\nHyper-V  317\nversus containers  317, 318\nZen  317\nW\nweb applications\nand user interfaces  183, 184\nX\nXebiaLabs periodic table\nreference  387\nZ\nZooKeeper\nconfiguring  356\ninstalling  355, 356\nreference  356\nrunning as services  358\nZuul\ncompleting, for all services  251\nhigh availability  249\nsetting up  245-249\nusing  247, 248\nZuul proxy\ndefining, as API gateway  244\n",
      "page_number": 400
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "www.allitebooks.com\n",
      "content_length": 20,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": "Spring Microservices\nBuild scalable microservices with Spring, Docker,  \nand Mesos\nRajesh RV\nBIRMINGHAM - MUMBAI\nwww.allitebooks.com\n",
      "content_length": 133,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Spring Microservices\nCopyright © 2016 Packt Publishing\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval \nsystem, or transmitted in any form or by any means, without the prior written \npermission of the publisher, except in the case of brief quotations embedded in \ncritical articles or reviews.\nEvery effort has been made in the preparation of this book to ensure the accuracy \nof the information presented. However, the information contained in this book is \nsold without warranty, either express or implied. Neither the author, nor Packt \nPublishing, and its dealers and distributors will be held liable for any damages \ncaused or alleged to be caused directly or indirectly by this book.\nPackt Publishing has endeavored to provide trademark information about all of the \ncompanies and products mentioned in this book by the appropriate use of capitals. \nHowever, Packt Publishing cannot guarantee the accuracy of this information.\nFirst published: June 2016\nProduction reference: 1200616\nPublished by Packt Publishing Ltd.\nLivery Place\n35 Livery Street\nBirmingham B3 2PB, UK.\nISBN 978-1-78646-668-6\nwww.packtpub.com\nwww.allitebooks.com\n",
      "content_length": 1173,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "Credits\nAuthor\nRajesh RV\nReviewer\nYogendra Sharma\nCommissioning Editor\nEdward Gordon\nAcquisition Editor\nRahul Nair\nContent Development Editor\nAnish Sukumaran\nTechnical Editors\nTaabish Khan\nKunal Chaudhari\nCopy Editors\nShruti Iyer\nSonia Mathur\nProject Coordinator\nIzzat Contractor\nProofreader\nSafis Editing\nIndexer\nHemangini Bari\nGraphics\nJason Monteiro\nProduction Coordinator\nMelwyn D'sa\nCover Work\nMelwyn D'sa\nwww.allitebooks.com\n",
      "content_length": 431,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "About the Author\nRajesh RV is a seasoned IT architect with extensive experience in diversified \ntechnologies and more than 16 years of airline IT experience.\nRajesh received a degree in computer engineering from University of Cochin, \nIndia. He joined the JEE community Spring during the early days of EJB. During \nthe course, as an architect, he worked on many large-scale, mission-critical projects, \nincluding New Generation Airline Passenger Reservation System (iFlyRes) and Next \nGeneration Airline Cargo Reservation systems (Skychain and CROAMIS) in the \nairlines domain.\nAt present, working as chief architect at Emirates (http://www.emirates.com/), \nRajesh is handling the solution architecture portfolio, which is spread across various \narchitecture capabilities, such as JEE, SOA, NoSQL, IoT, mobile, UI, integration, \nand more. At Emirates, Open Travel Platform (OTP) architected by Rajesh earned \nthe group the prestigious 2011 RedHat Innovation Award in the Carved Out \nCosts category. In 2011, he introduced the innovative concept of the Honeycomb \narchitecture based on the hexagonal architecture pattern used to transform the legacy \nmainframe system.\nRajesh has a deep passion for technology and architecture. He also holds several \ncertifications, such as BEA Certified WebLogic Administrator, Sun Certified Java \nEnterprise Architect, Open Group Certified TOGAF Practitioner, Licensed ZapThink \nArchitect in SOA, and IASA global CITA-A Certified Architecture Specialist.\nPreviously, Rajesh reviewed the book Service Oriented Java Business Integration,  \nPackt Publishing by Binildas A. Christudas.\nRajesh's social profile is available at https://www.linkedin.com/in/rajeshrv.\nwww.allitebooks.com\n",
      "content_length": 1715,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "Acknowledgments\nI would like to thank everyone I worked with closely at Packt Publishing to make my \ndream come true. A special thanks to the reviewers; your in-depth reviews helped \nimprove the quality of this book.\nThis book would have never been possible without the encouragement from my \nexcellent colleagues at Emirates. A very special thanks goes to Neetan Chopra,  \nSenior Vice President, and Thomas Benjamin, Vice President, for their constant  \nsupport and help.\nI would like to extend my thanks to Daniel Oor, who works as an independent \nenterprise architect, for his quality input and inspiration throughout the \ndevelopment of this book.\nA heartfelt thanks goes to my wife, Saritha, for her tireless and unconditional \nsupport that helped me focus on this book. I would like to thank my kids, Nikhil and \nAditya; I took away a lot of their playing hours to author this book. A huge thanks \nis due to my father, Ramachandran Nair, and mother, Vasanthakumari, for their \nselfless support that helped me reach where I am today.\nwww.allitebooks.com\n",
      "content_length": 1059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "About the Reviewer\nYogendra Sharma is a Java developer with Python background and with \nexperience mainly in backend development. He has completed his bachelors of \ntechnology in computer science.\nYogendra is currently working in Pune at Siemens Industry Software Pvt. Ltd as a \nproduct development engineer. He is constantly exploring technical novelties and is \nopen-minded and eager to learn about new technologies and frameworks.\nYogendra was also the technical reviewer of Mastering Python Design Patterns, Sakis \nKasampalis, and Test-Driven Development with Django, Kevin Harvey, both by Packt \nPublishing.\nHis LinkedIn profile is available at http://in.linkedin.com/in/\nyogendra0sharma. He blogs at http://TechiesEyes.com.\nI would like to thank my parents for allowing me to learn all that \nI did. I would also like to thank my friends for their support and \nencouragement.\nwww.allitebooks.com\n",
      "content_length": 901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "www.PacktPub.com\neBooks, discount offers, and more\nDid you know that Packt offers eBook versions of every book published, with PDF \nand ePub files available? You can upgrade to the eBook version at www.PacktPub.\ncom and as a print book customer, you are entitled to a discount on the eBook copy. \nGet in touch with us at customercare@packtpub.com for more details.\nAt www.PacktPub.com, you can also read a collection of free technical articles, sign \nup for a range of free newsletters and receive exclusive discounts and offers on Packt \nbooks and eBooks.\nhttps://www2.packtpub.com/books/subscription/packtlib\nDo you need instant solutions to your IT questions? PacktLib is Packt's online digital \nbook library. Here, you can search, access, and read Packt's entire library of books.\nWhy subscribe?\n•\t\nFully searchable across every book published by Packt\n•\t\nCopy and paste, print, and bookmark content\n•\t\nOn demand and accessible via a web browser\nwww.allitebooks.com\n",
      "content_length": 970,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "www.allitebooks.com\n",
      "content_length": 20,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "[ i ]\nTable of Contents\nPreface\t\nxi\nChapter 1: Demystifying Microservices\t\n1\nThe evolution of microservices\t\n1\nBusiness demand as a catalyst for microservices evolution\t\n2\nTechnology as a catalyst for the microservices evolution\t\n4\nImperative architecture evolution\t\n4\nWhat are microservices?\t\n5\nMicroservices – the honeycomb analogy\t\n8\nPrinciples of microservices\t\n8\nSingle responsibility per service\t\n8\nMicroservices are autonomous\t\n9\nCharacteristics of microservices\t\n10\nServices are first-class citizens\t\n11\nCharacteristics of services in a microservice\t\n11\nMicroservices are lightweight\t\n12\nMicroservices with polyglot architecture\t\n13\nAutomation in a microservices environment\t\n14\nMicroservices with a supporting ecosystem\t\n15\nMicroservices are distributed and dynamic\t\n15\nAntifragility, fail fast, and self-healing\t\n16\nMicroservices examples\t\n17\nAn example of a holiday portal\t\n17\nA microservice-based order management system\t\n20\nAn example of a travel agent portal\t\n22\nMicroservices benefits\t\n23\nSupports polyglot architecture\t\n23\nEnabling experimentation and innovation\t\n24\nElastically and selectively scalable\t\n25\nwww.allitebooks.com\n",
      "content_length": 1144,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "Table of Contents\n[ ii ]\nAllowing substitution\t\n27\nEnabling to build organic systems\t\n28\nHelping reducing technology debt\t\n29\nAllowing the coexistence of different versions\t\n30\nSupporting the building of self-organizing systems\t\n31\nSupporting event-driven architecture\t\n32\nEnabling DevOps\t\n33\nRelationship with other architecture styles\t\n33\nRelations with SOA\t\n33\nService-oriented integration\t\n34\nLegacy modernization\t\n35\nService-oriented application\t\n36\nMonolithic migration using SOA\t\n36\nRelations with Twelve-Factor apps\t\n37\nA single code base\t\n38\nBundling dependencies\t\n38\nExternalizing configurations\t\n39\nBacking services are addressable\t\n39\nIsolation between build, release, and run\t\n40\nStateless, shared nothing processes\t\n40\nExposing services through port bindings\t\n41\nConcurrency to scale out\t\n41\nDisposability with minimal overhead\t\n41\nDevelopment and production parity\t\n42\nExternalizing logs\t\n42\nPackage admin processes\t\n43\nMicroservice use cases\t\n43\nMicroservices early adopters\t\n45\nThe common theme is monolithic migrations\t\n47\nSummary\t\n48\nChapter 2: Building Microservices with Spring Boot\t\n49\nSetting up a development environment\t\n49\nDeveloping a RESTful service – the legacy approach\t\n50\nMoving from traditional web applications to microservices\t\n55\nUsing Spring Boot to build RESTful microservices\t\n56\nGetting started with Spring Boot\t\n57\nDeveloping the Spring Boot microservice using the CLI\t\n57\nDeveloping the Spring Boot Java microservice using STS\t\n58\nExamining the POM file\t\n62\nExamining Application.java\t\n64\nExamining application.properties\t\n65\nExamining ApplicationTests.java\t\n65\nTesting the Spring Boot microservice\t\n67\n",
      "content_length": 1645,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "Table of Contents\n[ iii ]\nDeveloping the Spring Boot microservice using  \nSpring Initializr – the HATEOAS example\t\n68\nWhat's next?\t\n72\nThe Spring Boot configuration\t\n73\nUnderstanding the Spring Boot autoconfiguration\t\n73\nOverriding default configuration values\t\n74\nChanging the location of the configuration file\t\n74\nReading custom properties\t\n75\nUsing a .yaml file for configuration\t\n75\nUsing multiple configuration profiles\t\n76\nOther options to read properties\t\n76\nChanging the default embedded web server\t\n77\nImplementing Spring Boot security\t\n77\nSecuring microservices with basic security\t\n77\nSecuring a microservice with OAuth2\t\n79\nEnabling cross-origin access for microservices\t\n82\nImplementing Spring Boot messaging\t\n83\nDeveloping a comprehensive microservice example\t\n86\nSpring Boot actuators\t\n97\nMonitoring using JConsole\t\n98\nMonitoring using SSH\t\n99\nConfiguring application information\t\n99\nAdding a custom health module\t\n99\nBuilding custom metrics\t\n101\nDocumenting microservices\t\n102\nSummary\t\n104\nChapter 3: Applying Microservices Concepts\t\n105\nPatterns and common design decisions\t\n105\nEstablishing appropriate microservice boundaries\t\n106\nAutonomous functions\t\n107\nSize of a deployable unit\t\n108\nMost appropriate function or subdomain\t\n108\nPolyglot architecture\t\n108\nSelective scaling\t\n108\nSmall, agile teams\t\n109\nSingle responsibility\t\n109\nReplicability or changeability\t\n110\nCoupling and cohesion\t\n110\nThink microservice as a product\t\n110\nDesigning communication styles\t\n111\nSynchronous style communication\t\n111\nAsynchronous style communication\t\n112\nHow to decide which style to choose?\t\n112\n",
      "content_length": 1606,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "Table of Contents\n[ iv ]\nOrchestration of microservices\t\n115\nHow many endpoints in a microservice?\t\n118\nOne microservice per VM or multiple?\t\n119\nRules engine – shared or embedded?\t\n120\nRole of BPM and workflows\t\n121\nCan microservices share data stores?\t\n123\nSetting up transaction boundaries\t\n125\nAltering use cases to simplify transactional requirements\t\n125\nDistributed transaction scenarios\t\n126\nService endpoint design consideration\t\n127\nContract design\t\n127\nProtocol selection\t\n127\nHandling shared libraries\t\n129\nUser interfaces in microservices\t\n130\nUse of API gateways in microservices\t\n131\nUse of ESB and iPaaS with microservices\t\n133\nService versioning considerations\t\n134\nDesign for cross origin\t\n136\nHandling shared reference data\t\n136\nMicroservices and bulk operations\t\n138\nMicroservices challenges\t\n139\nData islands\t\n139\nLogging and monitoring\t\n140\nDependency management\t\n141\nOrganization culture\t\n142\nGovernance challenges\t\n142\nOperation overheads\t\n142\nTesting microservices\t\n143\nInfrastructure provisioning\t\n144\nThe microservices capability model\t\n144\nCore capabilities\t\n145\nInfrastructure capabilities\t\n146\nSupporting capabilities\t\n147\nProcess and governance capabilities\t\n148\nSummary\t\n149\nChapter 4: Microservices Evolution – A Case Study\t\n151\nReviewing the microservices capability model\t\n152\nUnderstanding the PSS application\t\n153\nBusiness process view\t\n153\nFunctional view\t\n154\nArchitectural view\t\n154\n",
      "content_length": 1423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "Table of Contents\n[ v ]\nDesign view\t\n155\nImplementation view\t\n156\nDeployment view\t\n157\nDeath of the monolith\t\n158\nPain points\t\n158\nStop gap fix\t\n159\nRetrospection\t\n160\nShared data\t\n160\nSingle database\t\n161\nDomain boundaries\t\n163\nMicroservices to the rescue\t\n164\nThe business case\t\n165\nPlan the evolution\t\n165\nEvolutionary approach\t\n166\nIdentification of microservices boundaries\t\n167\nAnalyze dependencies\t\n167\nPrioritizing microservices for migration\t\n177\nData synchronization during migration\t\n178\nManaging reference data\t\n181\nUser interfaces and web applications\t\n183\nSession handling and security\t\n184\nTest strategy\t\n186\nBuilding ecosystem capabilities\t\n187\nMigrate modules only if required\t\n187\nTarget architecture\t\n188\nInternal layering of microservices\t\n189\nOrchestrating microservices\t\n190\nIntegration with other systems\t\n191\nManaging shared libraries\t\n191\nHandling exceptions\t\n191\nTarget implementation view\t\n194\nImplementation projects\t\n195\nRunning and testing the project\t\n196\nSummary\t\n201\nChapter 5: Scaling Microservices with Spring Cloud\t\n203\nReviewing microservices capabilities\t\n204\nReviewing BrownField's PSS implementation\t\n204\nWhat is Spring Cloud?\t\n205\nSpring Cloud releases\t\n206\nComponents of Spring Cloud\t\n207\nSpring Cloud and Netflix OSS\t\n210\n",
      "content_length": 1265,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "Table of Contents\n[ vi ]\nSetting up the environment for BrownField PSS\t\n210\nSpring Cloud Config\t\n211\nWhat's next?\t\n214\nSetting up the Config server\t\n214\nUnderstanding the Config server URL\t\n217\nAccessing the Config Server from clients\t\n218\nHandling configuration changes\t\n222\nSpring Cloud Bus for propagating configuration changes\t\n223\nSetting up high availability for the Config server\t\n224\nMonitoring the Config server health\t\n226\nConfig server for configuration files\t\n226\nCompleting changes to use the Config server\t\n227\nFeign as a declarative REST client\t\n227\nRibbon for load balancing\t\n229\nEureka for registration and discovery\t\n232\nUnderstanding dynamic service registration and discovery\t\n232\nUnderstanding Eureka\t\n234\nSetting up the Eureka server\t\n235\nHigh availability for Eureka\t\n241\nZuul proxy as the API gateway\t\n244\nSetting up Zuul\t\n245\nHigh availability of Zuul\t\n249\nHigh availability of Zuul when the client is also a Eureka client\t\n250\nHigh availability when the client is not a Eureka client\t\n251\nCompleting Zuul for all other services\t\n251\nStreams for reactive microservices\t\n252\nSummarizing the BrownField PSS architecture\t\n256\nSummary\t\n258\nChapter 6: Autoscaling Microservices\t\n259\nReviewing the microservice capability model\t\n260\nScaling microservices with Spring Cloud\t\n260\nUnderstanding the concept of autoscaling\t\n262\nThe benefits of autoscaling\t\n263\nDifferent autoscaling models\t\n265\nAutoscaling an application\t\n266\nAutoscaling the infrastructure\t\n266\nAutoscaling in the cloud\t\n267\nAutoscaling approaches\t\n268\nScaling with resource constraints\t\n268\nScaling during specific time periods\t\n269\nScaling based on the message queue length\t\n270\n",
      "content_length": 1664,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "Table of Contents\n[ vii ]\nScaling based on business parameters\t\n270\nPredictive autoscaling\t\n271\nAutoscaling BrownField PSS microservices\t\n272\nThe capabilities required for an autoscaling system\t\n272\nImplementing a custom life cycle manager using Spring Boot\t\n274\nUnderstanding the deployment topology\t\n274\nUnderstanding the execution flow\t\n275\nA walkthrough of the life cycle manager code\t\n277\nRunning the life cycle manager\t\n281\nSummary\t\n282\nChapter 7: Logging and Monitoring Microservices\t\n283\nReviewing the microservice capability model\t\n284\nUnderstanding log management challenges\t\n284\nA centralized logging solution\t\n286\nThe selection of logging solutions\t\n288\nCloud services\t\n288\nOff-the-shelf solutions\t\n288\nBest-of-breed integration\t\n288\nLog shippers\t\n289\nLog stream processors\t\n289\nLog storage\t\n289\nDashboards\t\n290\nA custom logging implementation\t\n290\nDistributed tracing with Spring Cloud Sleuth\t\n293\nMonitoring microservices\t\n297\nMonitoring challenges\t\n298\nMonitoring tools\t\n300\nMonitoring microservice dependencies\t\n301\nSpring Cloud Hystrix for fault-tolerant microservices\t\n302\nAggregating Hystrix streams with Turbine\t\n307\nData analysis using data lakes\t\n310\nSummary\t\n311\nChapter 8: Containerizing Microservices with Docker\t\n313\nReviewing the microservice capability model\t\n314\nUnderstanding the gaps in BrownField PSS microservices\t\n314\nWhat are containers?\t\n316\nThe difference between VMs and containers\t\n317\nThe benefits of containers\t\n319\nMicroservices and containers\t\n320\n",
      "content_length": 1491,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "Table of Contents\n[ viii ]\nIntroduction to Docker\t\n321\nThe key components of Docker\t\n322\nThe Docker daemon\t\n322\nThe Docker client\t\n322\nDocker concepts\t\n323\nDocker images\t\n323\nDocker containers\t\n325\nThe Docker registry\t\n325\nDockerfile\t\n326\nDeploying microservices in Docker\t\n326\nRunning RabbitMQ on Docker\t\n330\nUsing the Docker registry\t\n330\nSetting up the Docker Hub\t\n331\nPublishing microservices to the Docker Hub\t\n331\nMicroservices on the cloud\t\n332\nInstalling Docker on AWS EC2\t\n332\nRunning BrownField services on EC2\t\n332\nUpdating the life cycle manager\t\n334\nThe future of containerization – unikernels and hardened security\t\n334\nSummary\t\n335\nChapter 9: Managing Dockerized Microservices with Mesos  \nand Marathon\t\n337\nReviewing the microservice capability model\t\n338\nThe missing pieces\t\n338\nWhy cluster management is important\t\n340\nWhat does cluster management do?\t\n341\nRelationship with microservices\t\n344\nRelationship with virtualization\t\n344\nCluster management solutions\t\n344\nDocker Swarm\t\n345\nKubernetes\t\n346\nApache Mesos\t\n346\nNomad\t\n347\nFleet\t\n347\nCluster management with Mesos and Marathon\t\n348\nDiving deep into Mesos\t\n348\nThe Mesos architecture\t\n349\nMarathon\t\n352\nImplementing Mesos and Marathon for BrownField microservices\t\n353\nSetting up AWS\t\n354\nInstalling ZooKeeper, Mesos, and Marathon\t\n355\nConfiguring ZooKeeper\t\n356\n",
      "content_length": 1336,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "Table of Contents\n[ ix ]\nConfiguring Mesos\t\n357\nRunning Mesos, Marathon, and ZooKeeper as services\t\n358\nPreparing BrownField PSS services\t\n361\nDeploying BrownField PSS services\t\n363\nReviewing the deployment\t\n365\nA place for the life cycle manager\t\n367\nRewriting the life cycle manager with Mesos and Marathon\t\n368\nThe technology metamodel\t\n368\nSummary\t\n369\nChapter 10: The Microservices Development Life Cycle\t\n371\nReviewing the microservice capability model\t\n372\nThe new mantra of lean IT – DevOps\t\n372\nReducing wastage\t\n374\nAutomating every possible step\t\n374\nValue-driven delivery\t\n374\nBridging development and operations\t\n375\nMeeting the trio – microservices, DevOps, and cloud\t\n375\nCloud as the self-service infrastructure for Microservices\t\n376\nDevOps as the practice and process for microservices\t\n376\nPractice points for microservices development\t\n377\nUnderstanding business motivation and value\t\n377\nChanging the mindset from project to product development\t\n377\nChoosing a development philosophy\t\n378\nDesign thinking\t\n378\nThe start-up model\t\n379\nThe Agile practice\t\n379\nUsing the concept of Minimum Viable Product\t\n380\nOvercoming the legacy hotspot\t\n380\nAddressing challenges around databases\t\n381\nEstablishing self-organizing teams\t\n381\nBuilding a self-service cloud\t\n384\nBuilding a microservices ecosystem\t\n384\nDefining a DevOps-style microservice life cycle process\t\n385\nValue-driven planning\t\n386\nAgile development\t\n386\nContinuous integration\t\n386\nContinuous testing\t\n387\nContinuous release\t\n387\nContinuous monitoring and feedback\t\n387\nAutomating the continuous delivery pipeline\t\n387\nDevelopment\t\n389\nContinuous integration\t\n390\nAutomated testing\t\n391\n",
      "content_length": 1666,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "Table of Contents\n[ x ]\nContinuous deployment\t\n397\nMonitoring and feedback\t\n397\nAutomated configuration management\t\n397\nMicroservices development governance, reference architectures,  \nand libraries\t\n398\nSummary\t\n398\nIndex\t\n399\n",
      "content_length": 228,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "[ xi ]\nPreface\nMicroservice is an architecture style and pattern in which complex systems are \ndecomposed into smaller services that work together to form larger business services. \nMicroservices are services that are autonomous, self-contained, and independently \ndeployable. In today's world, many enterprises use microservices as the default \nstandard for building large, service-oriented enterprise applications.\nThe Spring framework is a popular programming framework with the developer \ncommunity for many years. Spring Boot removed the need to have a heavyweight \napplication container and provided a means to deploy lightweight, server-less \napplications. Spring Cloud combines many Netflix OSS components and provides an \necosystem to run and manage large-scale microservices. It provides capabilities such \nas load balancing, service registry, monitoring, service gateway, and so on.\nHowever, microservices come with their own challenges, such as monitoring, \nmanaging, distributing, scaling, discovering, and so on, especially when deploying \nat scale. Adopting microservices without addressing the common microservices \nchallenges would lead to catastrophic results. The most important part of this book \nis a technology-agnostic microservice capability model that helps address all the \ncommon microservice challenges.\nThe goal of this book is to enlighten readers with a pragmatic approach and \nguidelines for implementing responsive microservices at scale. This book will \ntake readers on a deep dive into Spring Boot, Spring Cloud, Docker, Mesos, and \nMarathon. Readers of this book will understand how Spring Boot is used to deploy \nautonomous services server-less by removing the need to have a heavyweight \napplication server. Readers will learn different Spring Cloud capabilities and also \nrealize the use of Docker for containerization and of Mesos and Marathon for \ncompute resource abstraction and cluster-wide control, respectively.\n",
      "content_length": 1958,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "Preface\n[ xii ]\nI am sure readers will enjoy each and every section of this book. Also, I honestly \nbelieve that this book adds tremendous value by successfully conceiving microservices \nin your business. Throughout this book, I have used practical aspects of microservices \nimplementation by providing a number of examples, including a case study from \nthe travel domain. In the end, you will have learned how to implement microservice \narchitectures using the Spring framework, Spring Boot, and Spring Cloud. These are \nbattle-tested, robust tools to develop and deploy scalable microservices. Written to \nthe latest specifications of Spring, with the help of this book, you'll be able to build \nmodern, Internet-scale Java applications in no time.\nWhat this book covers\nChapter 1, Demystifying Microservices, gives you an introduction to microservices. \nThis chapter covers the fundamental concepts of microservices, their evolution, and \ntheir relationship with service-oriented architecture, as well as the concepts of cloud \nnative and Twelve-Factor applications.\nChapter 2, Building Microservices with Spring Boot, introduces building REST- and \nmessage-based microservices using the Spring framework and how to wrap them \nwith Spring Boot. In addition, we will also explore some core capabilities of Spring \nBoot.\nChapter 3, Applying Microservices Concepts, explains the practical aspects of \nmicroservices implementation by detailing out the challenges that developers face \nwith enterprise-grade microservices. This will also summarize the capabilities \nrequired to successfully manage a microservices ecosystem.\nChapter 4, Microservices Evolution – A Case Study, takes the readers into a real-world \ncase study of microservices evolution by introducing BrownField Airline. Using the \ncase study, this chapter explains how to apply the microservices concepts learned in \nprevious chapters.\nChapter 5, Scaling Microservices with Spring Cloud, shows how to scale the previous \nexample using Spring Cloud stack capabilities. It details out the architecture and \ndifferent components of Spring Cloud and how they integrate together.\nChapter 6, Autoscaling Microservices, demonstrates the use of a simple life cycle \nmanager to attain elasticity and the self-management of microservices by \norchestrating services with service gateways. It explains how, in the real world,  \none can add intelligence to service gateways.\n",
      "content_length": 2426,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "Preface\n[ xiii ]\nChapter 7, Logging and Monitoring Microservices, covers the importance of logging and \nmonitoring aspects when developing microservices. Here, we will go into the details \nof some of the best practices when using microservices such as centralized logging \nand monitoring capabilities using open source tools and how to integrate them with \nSpring projects.\nChapter 8, Containerizing Microservices with Docker, explains containerization \nconcepts in the context of microservices. Using Mesos and Marathon, this chapter \ndemonstrates a next-level implementation to replace a custom life cycle manager for \nlarge deployments.\nChapter 9, Managing Dockerized Microservices with Mesos and Marathon, explains the \nautoprovisioning and deployment of microservices. Here, you will also learn how to \nuse Docker containers in the previous example for large-scale deployments.\nChapter 10, The Microservices Development Life Cycle, covers the process and practices \nof microservices development. The importance of DevOps and continuous delivery \npipelines is also explained in this chapter.\nWhat you need for this book\nChapter 2, Building Microservices with Spring Boot, introduces Spring Boot, which \nrequires the following software components to test the code:\n•\t\nJDK 1.8\n•\t\nSpring Tool Suite 3.7.2 (STS)\n•\t\nMaven 3.3.1\n•\t\nSpring Framework 4.2.6.RELEASE\n•\t\nSpring Boot 1.3.5.RELEASE\n•\t\nspring-boot-cli-1.3.5.RELEASE-bin.zip\n•\t\nRabbitMQ 3.5.6\n•\t\nFakeSMTP\nIn Chapter 5, Scaling Microservices with Spring Cloud, you will learn about the Spring \nCloud project. This requires the following software components in addition to the \npreviously mentioned ones:\n•\t\nSpring Cloud Brixton.RELEASE\n",
      "content_length": 1691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "Preface\n[ xiv ]\nIn Chapter 7, Logging and Monitoring Microservices, we will take a look at how \ncentralized logging can be implemented for microservices. This requires the \nfollowing software stack:\n•\t\nElasticsearch 1.5.2\n•\t\nkibana-4.0.2-darwin-x64\n•\t\nLogstash 2.1.2\nIn Chapter 8, Containerizing Microservices with Docker, we will demonstrate how we \ncan use Docker for microservices deployments. This requires the following software \ncomponents:\n•\t\nDocker version 1.10.1\n•\t\nDocker Hub\nChapter 9, Managing Dockerized Microservices with Mesos and Marathon, uses Mesos \nand Marathon to deploy dockerized microservices into an autoscalable cloud. The \nfollowing software components are required for this purpose:\n•\t\nMesos version 0.27.1\n•\t\nDocker version 1.6.2\n•\t\nMarathon version 0.15.3\nWho this book is for\nThis book is primarily for Spring developers who are looking to build cloud-\nready Internet-scale applications to meet modern business demands. The book \nwill help developers to understand what exactly microservices are and why they \nare important in today's world by examining a number of real-world use cases \nand hands-on code samples. Developers will understand how to build simple \nRESTful services and organically grow them to truly enterprise-grade microservices \necosystem.\nThis book will be interesting to architects who are seeking help on designing robust \nInternet-scale microservices using the Spring framework, Spring Boot, and Spring \nCloud and managing them using Docker, Mesos, and Marathon. The capability \nmodel will help architects devise solutions even beyond the tools and technologies \ndiscussed in this book.\n",
      "content_length": 1639,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "Preface\n[ xv ]\nConventions\nIn this book, you will find a number of text styles that distinguish between different \nkinds of information. Here are some examples of these styles and an explanation of \ntheir meaning.\nCode words in text, database table names, folder names, filenames, file extensions, \npathnames, dummy URLs, user input, and Twitter handles are shown as follows: \n\"The following properties can be set in application.properties to customize \napplication-related information.\"\nA block of code is set as follows:\n<parent>\n  <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>1.3.4.RELEASE</version>\n</parent>\n When we wish to draw your attention to a particular part of a code block,  \nthe relevant lines or items are set in bold:\neureka-server2.properties\neureka.client.serviceUrl.defaultZone:http://localhost:8761/eureka/\neureka.client.registerWithEureka:false\neureka.client.fetchRegistry:false\n Any command-line input or output is written as follows:\n$ java -jar fakeSMTP-2.0.jar\nNew terms and important words are shown in bold. Words that you see on  \nthe screen, for example, in menus or dialog boxes, appear in the text like this:  \n\"Click on the Make Request button.\"\nWarnings or important notes appear in a box like this.\nTips and tricks appear like this.\n",
      "content_length": 1336,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "Reader feedback\nFeedback from our readers is always welcome. Let us know what you think about \nthis book—what you liked or disliked. Reader feedback is important for us as it helps \nus develop titles that you will really get the most out of.\nTo send us general feedback, simply e-mail feedback@packtpub.com, and mention \nthe book's title in the subject of your message.\nIf there is a topic that you have expertise in and you are interested in either writing \nor contributing to a book, see our author guide at www.packtpub.com/authors.\nCustomer support\nNow that you are the proud owner of a Packt book, we have a number of things to \nhelp you to get the most from your purchase.\nDownloading the example code\nYou can download the example code files for this book from your account at \nhttp://www.packtpub.com. If you purchased this book elsewhere, you can visit \nhttp://www.packtpub.com/support and register to have the files e-mailed directly \nto you.\nYou can download the code files by following these steps:\n1.\t Log in or register to our website using your e-mail address and password.\n2.\t Hover the mouse pointer on the SUPPORT tab at the top.\n3.\t Click on Code Downloads & Errata.\n4.\t Enter the name of the book in the Search box.\n5.\t Select the book for which you're looking to download the code files.\n6.\t Choose from the drop-down menu where you purchased this book from.\n7.\t Click on Code Download.\nYou can also download the code files by clicking on the Code Files button on the \nbook's webpage at the Packt Publishing website. This page can be accessed by entering \nthe book's name in the Search box. Please note that you need to be logged in to your \nPackt account.\n",
      "content_length": 1677,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "Preface\n[ xvii ]\nOnce the file is downloaded, please make sure that you unzip or extract the folder \nusing the latest version of:\n•\t\nWinRAR / 7-Zip for Windows\n•\t\nZipeg / iZip / UnRarX for Mac\n•\t\n7-Zip / PeaZip for Linux\nThe code bundle for the book is also hosted on GitHub at https://github.com/\nPacktPublishing/Spring-Microservices. We also have other code bundles \nfrom our rich catalog of books and videos available at https://github.com/\nPacktPublishing/. Check them out!\nErrata\nAlthough we have taken every care to ensure the accuracy of our content, mistakes \ndo happen. If you find a mistake in one of our books—maybe a mistake in the text or \nthe code—we would be grateful if you could report this to us. By doing so, you can \nsave other readers from frustration and help us improve subsequent versions of this \nbook. If you find any errata, please report them by visiting http://www.packtpub.\ncom/submit-errata, selecting your book, clicking on the Errata Submission Form \nlink, and entering the details of your errata. Once your errata are verified, your \nsubmission will be accepted and the errata will be uploaded to our website or added \nto any list of existing errata under the Errata section of that title.\nTo view the previously submitted errata, go to https://www.packtpub.com/books/\ncontent/support and enter the name of the book in the search field. The required \ninformation will appear under the Errata section.\nPiracy\nPiracy of copyrighted material on the Internet is an ongoing problem across all \nmedia. At Packt, we take the protection of our copyright and licenses very seriously. \nIf you come across any illegal copies of our works in any form on the Internet, please \nprovide us with the location address or website name immediately so that we can \npursue a remedy.\nPlease contact us at copyright@packtpub.com with a link to the suspected pirated \nmaterial.\nWe appreciate your help in protecting our authors and our ability to bring you \nvaluable content.\n",
      "content_length": 1986,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "Preface\n[ xviii ]\nQuestions\nIf you have a problem with any aspect of this book, you can contact us at \nquestions@packtpub.com, and we will do our best to address the problem.\n",
      "content_length": 175,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "[ 1 ]\nDemystifying Microservices\nMicroservices are an architecture style and an approach for software development to \nsatisfy modern business demands. Microservices are not invented; they are more of \nan evolution from the previous architecture styles.\nWe will start the chapter by taking a closer look at the evolution of the microservices \narchitecture from the traditional monolithic architectures. We will also examine the \ndefinition, concepts, and characteristics of microservices. Finally, we will analyze \ntypical use cases of microservices and establish the similarities and relationships \nbetween microservices and other architecture approaches such as Service Oriented \nArchitecture (SOA) and Twelve-Factor Apps. Twelve-Factor Apps defines a set of \nsoftware engineering principles of developing applications targeting the cloud.\nIn this chapter you, will learn about:\n•\t\nThe evolution of microservices\n•\t\nThe definition of the microservices architecture with examples\n•\t\nConcepts and characteristics of the microservices architecture\n•\t\nTypical use cases of the microservices architecture\n•\t\nThe relationship of microservices with SOA and Twelve-Factor Apps\nThe evolution of microservices\nMicroservices are one of the increasingly popular architecture patterns next to \nSOA, complemented by DevOps and cloud. The microservices evolution is greatly \ninfluenced by the disruptive digital innovation trends in modern business and the \nevolution of technologies in the last few years. We will examine these two factors  \nin this section.\n",
      "content_length": 1546,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "Demystifying Microservices\n[ 2 ]\nBusiness demand as a catalyst for \nmicroservices evolution\nIn this era of digital transformation, enterprises increasingly adopt technologies as \none of the key enablers for radically increasing their revenue and customer base. \nEnterprises primarily use social media, mobile, cloud, big data, and Internet of \nThings as vehicles to achieve the disruptive innovations. Using these technologies, \nenterprises find new ways to quickly penetrate the market, which severely pose \nchallenges to the traditional IT delivery mechanisms.\nThe following graph shows the state of traditional development and microservices \nagainst the new enterprise challenges such as agility, speed of delivery, and scale.\nMicroservices promise more agility, speed of delivery, and scale \ncompared to traditional monolithic applications.\nGone are the days when businesses invested in large application developments \nwith the turnaround time of a few years. Enterprises are no longer interested in \ndeveloping consolidated applications to manage their end-to-end business functions \nas they did a few years ago.\nThe following graph shows the state of traditional monolithic applications and \nmicroservices in comparison with the turnaround time and cost.\n",
      "content_length": 1261,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "Chapter 1\n[ 3 ]\nMicroservices provide an approach for developing quick and agile \napplications, resulting in less overall cost.\nToday, for instance, airlines or financial institutions do not invest in rebuilding \ntheir core mainframe systems as another monolithic monster. Retailers and other \nindustries do not rebuild heavyweight supply chain management applications, such \nas their traditional ERPs. Focus has shifted to building quick-win point solutions that \ncater to specific needs of the business in the most agile way possible.\nLet's take an example of an online retailer running with a legacy monolithic \napplication. If the retailer wants to innovate his/her sales by offering their products \npersonalized to a customer based on the customer's past shopping, preferences, and \nso on and also wants to enlighten customers by offering products based on their \npropensity to buy them, they will quickly develop a personalization engine or offers \nbased on their immediate needs and plug them into their legacy application.\nAs shown in the preceding diagram, rather than investing in rebuilding the core \nlegacy system, this will be either done by passing the responses through the new \nfunctions, as shown in the diagram marked A, or by modifying the core legacy \nsystem to call out these functions as part of the processing, as shown in the diagram \nmarked B. These functions are typically written as microservices.\nThis approach gives organizations a plethora of opportunities to quickly try out new \nfunctions with lesser cost in an experimental mode. Businesses can later validate key \nperformance indicators and alter or replace these implementations if required.\nwww.allitebooks.com\n",
      "content_length": 1697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "Demystifying Microservices\n[ 4 ]\nModern architectures are expected to maximize the ability to replace \ntheir parts and minimize the cost of replacing their parts. The \nmicroservices approach is a means to achieving this.\nTechnology as a catalyst for the \nmicroservices evolution\nEmerging technologies have also made us rethink the way we build software \nsystems. For example, a few decades back, we couldn't even imagine a distributed \napplication without a two-phase commit. Later, NoSQL databases made us think \ndifferently.\nSimilarly, these kinds of paradigm shifts in technology have reshaped all the layers \nof the software architecture.\nThe emergence of HTML 5 and CSS3 and the advancement of mobile applications \nrepositioned user interfaces. Client-side JavaScript frameworks such as Angular, \nEmber, React, Backbone, and so on are immensely popular due to their client-side \nrendering and responsive designs.\nWith cloud adoptions steamed into the mainstream, Platform as a Services \n(PaaS) providers such as Pivotal CF, AWS, Salesforce.com, IBMs Bluemix, RedHat \nOpenShift, and so on made us rethink the way we build middleware components. \nThe container revolution created by Docker radically influenced the infrastructure \nspace. These days, an infrastructure is treated as a commodity service.\nThe integration landscape has also changed with Integration Platform as a Service \n(iPaaS), which is emerging. Platforms such as Dell Boomi, Informatica, MuleSoft, \nand so on are examples of iPaaS. These tools helped organizations stretch integration \nboundaries beyond the traditional enterprise.\nNoSQLs have revolutionized the databases space. A few years ago, we had only a \nfew popular databases, all based on relational data modeling principles. We have  \na long list of databases today: Hadoop, Cassandra, CouchDB, and Neo 4j to name  \na few. Each of these databases addresses certain specific architectural problems.\nImperative architecture evolution\nApplication architecture has always been evolving alongside demanding business \nrequirements and the evolution of technologies. Architectures have gone through  \nthe evolution of age-old mainframe systems to fully abstract cloud services such  \nas AWS Lambda.\n",
      "content_length": 2224,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "Chapter 1\n[ 5 ]\nUsing AWS Lambda, developers can now drop their \"functions\" into \na fully managed compute service.\nRead more about Lambda at: https://aws.amazon.com/\ndocumentation/lambda/\nDifferent architecture approaches and styles such as mainframes, client server, \nN-tier, and service-oriented were popular at different timeframes. Irrespective of \nthe choice of architecture styles, we always used to build one or the other forms \nof monolithic architectures. The microservices architecture evolved as a result \nof modern business demands such as agility and speed of delivery, emerging \ntechnologies, and learning from previous generations of architectures.\nMicroservices help us break the boundaries of monolithic applications and build a \nlogically independent smaller system of systems, as shown in the preceding diagram.\nIf we consider monolithic applications as a set of logical \nsubsystems encompassed with a physical boundary, microservices \nare a set of independent subsystems with no enclosing physical \nboundary.\nWhat are microservices?\nMicroservices are an architecture style used by many organizations today as a game \nchanger to achieve a high degree of agility, speed of delivery, and scale. Microservices \ngive us a way to develop more physically separated modular applications.\nMicroservices are not invented. Many organizations such as Netflix, Amazon, and \neBay successfully used the divide-and-conquer technique to functionally partition \ntheir monolithic applications into smaller atomic units, each performing a single \nfunction. These organizations solved a number of prevailing issues they were \nexperiencing with their monolithic applications. \n",
      "content_length": 1675,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "Demystifying Microservices\n[ 6 ]\nFollowing the success of these organizations, many other organizations started \nadopting this as a common pattern to refactor their monolithic applications. Later, \nevangelists termed this pattern as the microservices architecture.\nMicroservices originated from the idea of hexagonal architecture coined by Alistair \nCockburn. Hexagonal architecture is also known as the Ports and Adapters pattern.\nRead more about hexagonal architecture at http://alistair.\ncockburn.us/Hexagonal+architecture.\nMicroservices are an architectural style or an approach to building IT systems as a set \nof business capabilities that are autonomous, self-contained, and loosely coupled:\nThe preceding diagram depicts a traditional N-tier application architecture having \na presentation layer, business layer, and database layer. The modules A, B, and C \nrepresent three different business capabilities. The layers in the diagram represent a \nseparation of architecture concerns. Each layer holds all three business capabilities \npertaining to this layer. The presentation layer has web components of all the three \nmodules, the business layer has business components of all the three modules, and \nthe database hosts tables of all the three modules. In most cases, layers are physically \nspreadable, whereas modules within a layer are hardwired.\n",
      "content_length": 1358,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "Chapter 1\n[ 7 ]\nLet's now examine a microservices-based architecture.\nAs we can note in the preceding diagram, the boundaries are inversed in the \nmicroservices architecture. Each vertical slice represents a microservice. Each \nmicroservice has its own presentation layer, business layer, and database layer. \nMicroservices are aligned towards business capabilities. By doing so, changes  \nto one microservice do not impact others.\nThere is no standard for communication or transport mechanisms for microservices. \nIn general, microservices communicate with each other using widely adopted \nlightweight protocols, such as HTTP and REST, or messaging protocols, such as \nJMS or AMQP. In specific cases, one might choose more optimized communication \nprotocols, such as Thrift, ZeroMQ, Protocol Buffers, or Avro.\nAs microservices are more aligned to business capabilities and have independently \nmanageable life cycles, they are the ideal choice for enterprises embarking on \nDevOps and cloud. DevOps and cloud are two facets of microservices.\nDevOps is an IT realignment to narrow the gap between traditional IT \ndevelopment and operations for better efficiency.\nRead more about DevOps:\nhttp://dev2ops.org/2010/02/what-is-devops/\n",
      "content_length": 1229,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "Demystifying Microservices\n[ 8 ]\nMicroservices – the honeycomb analogy\nThe honeycomb is an ideal analogy for representing the evolutionary microservices \narchitecture.\nIn the real world, bees build a honeycomb by aligning hexagonal wax cells. They \nstart small, using different materials to build the cells. Construction is based on \nwhat is available at the time of building. Repetitive cells form a pattern and result \nin a strong fabric structure. Each cell in the honeycomb is independent but also \nintegrated with other cells. By adding new cells, the honeycomb grows organically \nto a big, solid structure. The content inside each cell is abstracted and not visible \noutside. Damage to one cell does not damage other cells, and bees can reconstruct \nthese cells without impacting the overall honeycomb.\nPrinciples of microservices\nIn this section, we will examine some of the principles of the microservices \narchitecture. These principles are a \"must have\" when designing and developing \nmicroservices.\nSingle responsibility per service\nThe single responsibility principle is one of the principles defined as part of the \nSOLID design pattern. It states that a unit should only have one responsibility.\nRead more about the SOLID design pattern at:\nhttp://c2.com/cgi/wiki?PrinciplesOfObjectOrientedDes\nign\n",
      "content_length": 1312,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "Chapter 1\n[ 9 ]\nThis implies that a unit, either a class, a function, or a service, should have only one \nresponsibility. At no point should two units share one responsibility or one unit have \nmore than one responsibility. A unit with more than one responsibility indicates \ntight coupling.\nAs shown in the preceding diagram, Customer, Product, and Order are different \nfunctions of an e-commerce application. Rather than building all of them into one \napplication, it is better to have three different services, each responsible for exactly \none business function, so that changes to one responsibility will not impair others. \nIn the preceding scenario, Customer, Product, and Order will be treated as three \nindependent microservices.\nMicroservices are autonomous\nMicroservices are self-contained, independently deployable, and autonomous \nservices that take full responsibility of a business capability and its execution. \nThey bundle all dependencies, including library dependencies, and execution \nenvironments such as web servers and containers or virtual machines that abstract \nphysical resources.\nOne of the major differences between microservices and SOA is in their level of \nautonomy. While most SOA implementations provide service-level abstraction, \nmicroservices go further and abstract the realization and execution environment.\n",
      "content_length": 1347,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "Demystifying Microservices\n[ 10 ]\nIn traditional application developments, we build a WAR or an EAR, then deploy \nit into a JEE application server, such as with JBoss, WebLogic, WebSphere, and \nso on. We may deploy multiple applications into the same JEE container. In the \nmicroservices approach, each microservice will be built as a fat Jar, embedding all \ndependencies and run as a standalone Java process.\nMicroservices may also get their own containers for execution, as shown in the \npreceding diagram. Containers are portable, independently manageable, lightweight \nruntime environments. Container technologies, such as Docker, are an ideal choice \nfor microservices deployment.\nCharacteristics of microservices\nThe microservices definition discussed earlier in this chapter is arbitrary. Evangelists \nand practitioners have strong but sometimes different opinions on microservices. \nThere is no single, concrete, and universally accepted definition for microservices. \nHowever, all successful microservices implementations exhibit a number of common \ncharacteristics. Therefore, it is important to understand these characteristics rather \nthan sticking to theoretical definitions. Some of the common characteristics are \ndetailed in this section.\n",
      "content_length": 1255,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "Chapter 1\n[ 11 ]\nServices are first-class citizens\nIn the microservices world, services are first-class citizens. Microservices expose \nservice endpoints as APIs and abstract all their realization details. The internal \nimplementation logic, architecture, and technologies (including programming \nlanguage, database, quality of services mechanisms, and so on) are completely \nhidden behind the service API.\nMoreover, in the microservices architecture, there is no more application \ndevelopment; instead, organizations focus on service development. In most \nenterprises, this requires a major cultural shift in the way that applications are built.\nIn a Customer Profile microservice, internals such as the data structure, technologies, \nbusiness logic, and so on are hidden. They aren't exposed or visible to any external \nentities. Access is restricted through the service endpoints or APIs. For instance, \nCustomer Profile microservices may expose Register Customer and Get Customer \nas two APIs for others to interact with.\nCharacteristics of services in a microservice\nAs microservices are more or less like a flavor of SOA, many of the service \ncharacteristics defined in the SOA are applicable to microservices as well.\nThe following are some of the characteristics of services that are applicable to \nmicroservices as well:\n•\t\nService contract: Similar to SOA, microservices are described through  \nwell-defined service contracts. In the microservices world, JSON and REST \nare universally accepted for service communication. In the case of JSON/\nREST, there are many techniques used to define service contracts. JSON \nSchema, WADL, Swagger, and RAML are a few examples.\n•\t\nLoose coupling: Microservices are independent and loosely coupled. In \nmost cases, microservices accept an event as input and respond with another \nevent. Messaging, HTTP, and REST are commonly used for interaction \nbetween microservices. Message-based endpoints provide higher levels  \nof decoupling.\n•\t\nService abstraction: In microservices, service abstraction is not just an \nabstraction of service realization, but it also provides a complete abstraction \nof all libraries and environment details, as discussed earlier.\n•\t\nService reuse: Microservices are course-grained reusable business  \nservices. These are accessed by mobile devices and desktop channels,  \nother microservices, or even other systems.\n",
      "content_length": 2391,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "Demystifying Microservices\n[ 12 ]\n•\t\nStatelessness: Well-designed microservices are stateless and share nothing \nwith no shared state or conversational state maintained by the services.  \nIn case there is a requirement to maintain state, they are maintained in  \na database, perhaps in memory.\n•\t\nServices are discoverable: Microservices are discoverable. In a typical \nmicroservices environment, microservices self-advertise their existence \nand make themselves available for discovery. When services die, they \nautomatically take themselves out from the microservices ecosystem.\n•\t\nService interoperability: Services are interoperable as they use standard \nprotocols and message exchange standards. Messaging, HTTP, and so on are \nused as transport mechanisms. REST/JSON is the most popular method for \ndeveloping interoperable services in the microservices world. In cases where \nfurther optimization is required on communications, other protocols such as \nProtocol Buffers, Thrift, Avro, or Zero MQ could be used. However, the use \nof these protocols may limit the overall interoperability of the services.\n•\t\nService composeability: Microservices are composeable. Service \ncomposeability is achieved either through service orchestration or  \nservice choreography.\nMore detail on SOA principles can be found at:\nhttp://serviceorientation.com/serviceorientation/index\nMicroservices are lightweight\nWell-designed microservices are aligned to a single business capability, so they \nperform only one function. As a result, one of the common characteristics we see  \nin most of the implementations are microservices with smaller footprints.\nWhen selecting supporting technologies, such as web containers, we will have \nto ensure that they are also lightweight so that the overall footprint remains \nmanageable. For example, Jetty or Tomcat are better choices as application containers \nfor microservices compared to more complex traditional application servers such as \nWebLogic or WebSphere.\nContainer technologies such as Docker also help us keep the infrastructure footprint \nas minimal as possible compared to hypervisors such as VMWare or Hyper-V.\n",
      "content_length": 2152,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "Chapter 1\n[ 13 ]\nAs shown in the preceding diagram, microservices are typically deployed in Docker \ncontainers, which encapsulate the business logic and needed libraries. This help us \nquickly replicate the entire setup on a new machine or on a completely different \nhosting environment or even to move across different cloud providers. As there is no \nphysical infrastructure dependency, containerized microservices are easily portable.\nMicroservices with polyglot architecture\nAs microservices are autonomous and abstract everything behind service APIs, it is \npossible to have different architectures for different microservices. A few common \ncharacteristics that we see in microservices implementations are:\n•\t\nDifferent services use different versions of the same technologies. One \nmicroservice may be written on Java 1.7, and another one could be on Java 1.8.\n•\t\nDifferent languages are used to develop different microservices, such as one \nmicroservice is developed in Java and another one in Scala.\n•\t\nDifferent architectures are used, such as one microservice using the Redis \ncache to serve data, while another microservice could use MySQL as a \npersistent data store.\n",
      "content_length": 1181,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "Demystifying Microservices\n[ 14 ]\nIn the preceding example, as Hotel Search is expected to have high transaction \nvolumes with stringent performance requirements, it is implemented using Erlang. \nIn order to support predictive searching, Elasticsearch is used as the data store. \nAt the same time, Hotel Booking needs more ACID transactional characteristics. \nTherefore, it is implemented using MySQL and Java. The internal implementations \nare hidden behind service endpoints defined as REST/JSON over HTTP.\nAutomation in a microservices environment\nMost of the microservices implementations are automated to a maximum from \ndevelopment to production.\nAs microservices break monolithic applications into a number of smaller services, \nlarge enterprises may see a proliferation of microservices. A large number of \nmicroservices is hard to manage until and unless automation is in place. The smaller \nfootprint of microservices also helps us automate the microservices development to \nthe deployment life cycle. In general, microservices are automated end to end—for \nexample, automated builds, automated testing, automated deployment, and elastic \nscaling.\nAs indicated in the preceding diagram, automations are typically applied during the \ndevelopment, test, release, and deployment phases:\n•\t\nThe development phase is automated using version control tools such as Git \ntogether with Continuous Integration (CI) tools such as Jenkins, Travis CI, \nand so on. This may also include code quality checks and automation of unit \ntesting. Automation of a full build on every code check-in is also achievable \nwith microservices.\n•\t\nThe testing phase will be automated using testing tools such as Selenium, \nCucumber, and other AB testing strategies. As microservices are aligned to \nbusiness capabilities, the number of test cases to automate is fewer compared \nto monolithic applications, hence regression testing on every build also \nbecomes possible.\n•\t\nInfrastructure provisioning is done through container technologies such as \nDocker, together with release management tools such as Chef or Puppet, and \nconfiguration management tools such as Ansible. Automated deployments are \nhandled using tools such as Spring Cloud, Kubernetes, Mesos, and Marathon.\n",
      "content_length": 2256,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "Chapter 1\n[ 15 ]\nMicroservices with a supporting ecosystem\nMost of the large-scale microservices implementations have a supporting ecosystem \nin place. The ecosystem capabilities include DevOps processes, centralized log \nmanagement, service registry, API gateways, extensive monitoring, service routing, \nand flow control mechanisms.\nMicroservices work well when supporting capabilities are in place, as represented in \nthe preceding diagram.\nMicroservices are distributed and dynamic\nSuccessful microservices implementations encapsulate logic and data within the \nservice. This results in two unconventional situations: distributed data and logic  \nand decentralized governance.\nCompared to traditional applications, which consolidate all logic and data into \none application boundary, microservices decentralize data and logic. Each service, \naligned to a specific business capability, owns its data and logic.\nLogical System Boundary (as in monolithic)\nMicroservice A\nA\nData\nA\nLogic\nMicroservice B\nB\nData\nB\nLogic\nMicroservice C\nC\nData\nC\nLogic\nThe dotted line in the preceding diagram implies the logical monolithic application \nboundary. When we migrate this to microservices, each microservice A, B, and C \ncreates its own physical boundaries.\n",
      "content_length": 1249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "Demystifying Microservices\n[ 16 ]\nMicroservices don't typically use centralized governance mechanisms the way \nthey are used in SOA. One of the common characteristics of microservices \nimplementations is that they do not relay on heavyweight enterprise-level products, \nsuch as Enterprise Service Bus (ESB). Instead, the business logic and intelligence are \nembedded as a part of the services themselves.\nA typical SOA implementation is shown in the preceding diagram. Shopping logic is \nfully implemented in ESB by orchestrating different services exposed by Customer, \nOrder, and Product. In the microservices approach, on the other hand, Shopping \nitself will run as a separate microservice, which interacts with Customer, Product, \nand Order in a fairly decoupled way.\nSOA implementations heavily relay on static registry and repository configurations \nto manage services and other artifacts. Microservices bring a more dynamic nature \ninto this. Hence, a static governance approach is seen as an overhead in maintaining \nup-to-date information. This is why most of the microservices implementations use \nautomated mechanisms to build registry information dynamically from the runtime \ntopologies.\nAntifragility, fail fast, and self-healing\nAntifragility is a technique successfully experimented at Netflix. It is one of the most \npowerful approaches to building fail-safe systems in modern software development.\nThe antifragility concept is introduced by Nassim Nicholas Taleb in his \nbook Antifragile: Things That Gain from Disorder.\nIn the antifragility practice, software systems are consistently challenged. Software \nsystems evolve through these challenges and, over a period of time, get better and \nbetter at withstanding these challenges. Amazon's GameDay exercise and Netflix' \nSimian Army are good examples of such antifragility experiments.\n",
      "content_length": 1857,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "Chapter 1\n[ 17 ]\nFail fast is another concept used to build fault-tolerant, resilient systems. This \nphilosophy advocates systems that expect failures versus building systems that never \nfail. Importance should be given to how quickly the system can fail and if it fails, \nhow quickly it can recover from this failure. With this approach, the focus is shifted \nfrom Mean Time Between Failures (MTBF) to Mean Time To Recover (MTTR). \nA key advantage of this approach is that if something goes wrong, it kills itself, and \ndownstream functions aren't stressed.\nSelf-healing is commonly used in microservices deployments, where the system \nautomatically learns from failures and adjusts itself. These systems also prevent \nfuture failures.\nMicroservices examples\nThere is no \"one size fits all\" approach when implementing microservices. In this \nsection, different examples are analyzed to crystalize the microservices concept.\nAn example of a holiday portal\nIn the first example, we will review a holiday portal, Fly By Points. Fly By Points \ncollects points that are accumulated when a customer books a hotel, flight, or car \nthrough the online website. When the customer logs in to the Fly By Points website, \nhe/she is able to see the points accumulated, personalized offers that can be availed \nof by redeeming the points, and upcoming trips if any.\n",
      "content_length": 1352,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "Demystifying Microservices\n[ 18 ]\nLet's assume that the preceding page is the home page after login. There are two \nupcoming trips for Jeo, four personalized offers, and 21,123 loyalty points. When  \nthe user clicks on each of the boxes, the details are queried and displayed.\nThe holiday portal has a Java Spring-based traditional monolithic application \narchitecture, as shown in the following:\nAs shown in the preceding diagram, the holiday portal's architecture is web-based \nand modular, with a clear separation between layers. Following the usual practice, \nthe holiday portal is also deployed as a single WAR file on a web server such as \nTomcat. Data is stored on an all-encompassing backing relational database. This is a \ngood fit for the purpose architecture when the complexities are few. As the business \ngrows, the user base expands, and the complexity also increases. This results in a \nproportional increase in transaction volumes. At this point, enterprises should look \nto rearchitecting the monolithic application to microservices for better speed of \ndelivery, agility, and manageability.\n",
      "content_length": 1109,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "Chapter 1\n[ 19 ]\nExamining the simple microservices version of this application, we can immediately \nnote a few things in this architecture:\n•\t\nEach subsystem has now become an independent system by itself, a \nmicroservice. There are three microservices representing three business \nfunctions: Trips, Offers, and Points. Each one has its internal data store  \nand middle layer. The internal structure of each service remains the same.\n•\t\nEach service encapsulates its own database as well as its own HTTP listener. \nAs opposed to the previous model, there is no web server or WAR. Instead, \neach service has its own embedded HTTP listener, such as Jetty, Tomcat,  \nand so on.\n•\t\nEach microservice exposes a REST service to manipulate the resources/entity \nthat belong to this service.\nIt is assumed that the presentation layer is developed using a client-side JavaScript \nMVC framework such as Angular JS. These client-side frameworks are capable of \ninvoking REST calls directly.\nWhen the web page is loaded, all the three boxes, Trips, Offers, and Points will be \ndisplayed with details such as points, the number of offers, and the number of trips. \nThis will be done by each box independently making asynchronous calls to the \nrespective backend microservices using REST. There is no dependency between the \nservices at the service layer. When the user clicks on any of the boxes, the screen will \nbe transitioned and will load the details of the item clicked on. This will be done by \nmaking another call to the respective microservice.\n",
      "content_length": 1542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "Demystifying Microservices\n[ 20 ]\nA microservice-based order management \nsystem\nLet's examine another microservices example: an online retail website. In this \ncase, we will focus more on the backend services, such as the Order Service which \nprocesses the Order Event generated when a customer places an order through  \nthe website:\nThis microservices system is completely designed based on reactive programming \npractices.\nRead more on reactive programming at:\nhttp://www.reactivemanifesto.org\nWhen an event is published, a number of microservices are ready to kick-start upon \nreceiving the event. Each one of them is independent and does not rely on other \nmicroservices. The advantage of this model is that we can keep adding or replacing \nmicroservices to achieve specific needs.\n",
      "content_length": 786,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "Chapter 1\n[ 21 ]\nIn the preceding diagram, there are eight microservices shown. The following \nactivities take place upon the arrival of Order Event:\n1.\t Order Service kicks off when Order Event is received. Order Service creates \nan order and saves the details to its own database.\n2.\t If the order is successfully saved, Order Successful Event is created by Order \nService and published.\n3.\t A series of actions take place when Order Successful Event arrives.\n4.\t Delivery Service accepts the event and places Delivery Record to deliver the \norder to the customer. This, in turn, generates Delivery Event and publishes \nthe event.\n5.\t Trucking Service picks up Delivery Event and processes it. For instance, \nTrucking Service creates a trucking plan.\n6.\t Customer Notification Service sends a notification to the customer informing \nthe customer that an order is placed.\n7.\t Inventory Cache Service updates the inventory cache with the available \nproduct count.\n8.\t Stock Reorder Service checks whether the stock limits are adequate and \ngenerates Replenish Event if required.\n9.\t Customer Points Service recalculates the customer's loyalty points based  \non this purchase.\n10.\t Customer Account Service updates the order history in the customer's \naccount.\nIn this approach, each service is responsible for only one function. Services \naccept and generate events. Each service is independent and is not aware of its \nneighborhood. Hence, the neighborhood can organically grow as mentioned in the \nhoneycomb analogy. New services can be added as and when necessary. Adding  \na new service does not impact any of the existing services.\n",
      "content_length": 1637,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "Demystifying Microservices\n[ 22 ]\nAn example of a travel agent portal\nThis third example is a simple travel agent portal application. In this example, we \nwill see both synchronous REST calls as well as asynchronous events.\nIn this case, the portal is just a container application with multiple menu items or \nlinks in the portal. When specific pages are requested—for example, when the menu \nor a link is clicked on—they will be loaded from the specific microservices.\nWhen a customer requests a booking, the following events take place internally:\n1.\t The travel agent opens the flight UI, searches for a flight, and identifies the \nright flight for the customer. Behind the scenes, the flight UI is loaded from \nthe Flight microservice. The flight UI only interacts with its own backend \nAPIs within the Flight microservice. In this case, it makes a REST call to the \nFlight microservice to load the flights to be displayed.\n2.\t The travel agent then queries the customer details by accessing the customer \nUI. Similar to the flight UI, the customer UI is loaded from the Customer \nmicroservice. Actions in the customer UI will invoke REST calls on the \nCustomer microservice. In this case, customer details are loaded by  \ninvoking appropriate APIs on the Customer microservice.\n3.\t Then, the travel agent checks the visa details for the customer's eligibility \nto travel to the selected country. This also follows the same pattern as \nmentioned in the previous two points.\n",
      "content_length": 1478,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "Chapter 1\n[ 23 ]\n4.\t Next, the travel agent makes a booking using the booking UI from the \nBooking microservice, which again follows the same pattern.\n5.\t The payment pages are loaded from the Payment microservice. In general, \nthe payment service has additional constraints such as PCIDSS compliance \n(protecting and encrypting data in motion and data at rest). The advantage \nof the microservices approach is that none of the other microservices need \nto be considered under the purview of PCIDSS as opposed to the monolithic \napplication, where the complete application comes under the governing rules \nof PCIDSS. Payment also follows the same pattern as described earlier.\n6.\t Once the booking is submitted, the Booking microservice calls the flight \nservice to validate and update the flight booking. This orchestration is \ndefined as part of the Booking microservice. Intelligence to make a booking is \nalso held within the Booking microservice. As part of the booking process, it \nalso validates, retrieves, and updates the Customer microservice.\n7.\t Finally, the Booking microservice sends the Booking Event, which the \nNotification service picks up and sends a notification of to the customer.\nThe interesting factor here is that we can change the user interface, logic, and data  \nof a microservice without impacting any other microservices.\nThis is a clean and neat approach. A number of portal applications can be built by \ncomposing different screens from different microservices, especially for different \nuser communities. The overall behavior and navigation will be controlled by the \nportal application.\nThe approach has a number of challenges unless the pages are designed with this \napproach in mind. Note that the site layouts and static content will be loaded by the \nContent Management System (CMS) as layout templates. Alternately, this could be \nstored in a web server. The site layout may have fragments of UIs that will be loaded \nfrom the microservices at runtime.\nMicroservices benefits\nMicroservices offer a number of benefits over the traditional multitier, monolithic \narchitectures. This section explains some key benefits of the microservices \narchitecture approach.\nSupports polyglot architecture\nWith microservices, architects and developers can choose fit for purpose \narchitectures and technologies for each microservice. This gives the flexibility  \nto design better-fit solutions in a more cost-effective way.\nwww.allitebooks.com\n",
      "content_length": 2469,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "Demystifying Microservices\n[ 24 ]\nAs microservices are autonomous and independent, each service can run with its \nown architecture or technology or different versions of technologies.\nThe following shows a simple, practical example of a polyglot architecture with \nmicroservices.\nThere is a requirement to audit all system transactions and record transaction details \nsuch as request and response data, the user who initiated the transaction, the service \ninvoked, and so on.\nAs shown in the preceding diagram, while core services such as the Order and \nProducts microservices use a relational data store, the Audit microservice persists \ndata in Hadoop File System (HDFS). A relational data store is neither ideal nor \ncost effective in storing large data volumes such as in the case of audit data. In the \nmonolithic approach, the application generally uses a shared, single database that \nstores Order, Products, and Audit data.\nIn this example, the audit service is a technical microservice using a different \narchitecture. Similarly, different functional services could also use different \narchitectures.\nIn another example, there could be a Reservation microservice running on Java \n7, while a Search microservice could be running on Java 8. Similarly, an Order \nmicroservice could be written on Erlang, whereas a Delivery microservice could be \non the Go language. None of these are possible with a monolithic architecture.\nEnabling experimentation and innovation\nModern enterprises are thriving towards quick wins. Microservices are one of the \nkey enablers for enterprises to do disruptive innovation by offering the ability to \nexperiment and fail fast.\n",
      "content_length": 1664,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "Chapter 1\n[ 25 ]\nAs services are fairly simple and smaller in size, enterprises can afford to experiment \nnew processes, algorithms, business logics, and so on. With large monolithic \napplications, experimentation was not easy; nor was it straightforward or cost \neffective. Businesses had to spend huge money to build or change an application \nto try out something new. With microservices, it is possible to write a small \nmicroservice to achieve the targeted functionality and plug it into the system in a \nreactive style. One can then experiment with the new function for a few months, and \nif the new microservice does not work as expected, we can change or replace it with \nanother one. The cost of change will be considerably less compared to that of the \nmonolithic approach.\nIn another example of an airline booking website, the airline wants to show \npersonalized hotel recommendations in their booking page. The recommendations \nmust be displayed on the booking confirmation page.\nAs shown in the preceding diagram, it is convenient to write a microservice that can \nbe plugged into the monolithic applications booking flow rather than incorporating \nthis requirement in the monolithic application itself. The airline may choose to start \nwith a simple recommendation service and keep replacing it with newer versions till \nit meets the required accuracy.\nElastically and selectively scalable\nAs microservices are smaller units of work, they enable us to implement selective \nscalability. \nScalability requirements may be different for different functions in an application. \nA monolithic application, packaged as a single WAR or an EAR, can only be scaled \nas a whole. An I/O-intensive function when streamed with high velocity data could \neasily bring down the service levels of the entire application.\n",
      "content_length": 1815,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "Demystifying Microservices\n[ 26 ]\nIn the case of microservices, each service could be independently scaled up or \ndown. As scalability can be selectively applied at each service, the cost of scaling is \ncomparatively less with the microservices approach.\nIn practice, there are many different ways available to scale an application and \nis largely subject to the architecture and behavior of the application. Scale Cube \ndefines primarily three approaches to scaling an application:\n•\t\nScaling the x axis by horizontally cloning the application\n•\t\nScaling the y axis by splitting different functionality\n•\t\nScaling the z axis by partitioning or sharding the data\nRead more about Scale Cube in the following site:\nhttp://theartofscalability.com/\nWhen y axis scaling is applied to monolithic applications, it breaks the monolithic \nto smaller units aligned with business functions. Many organizations successfully \napplied this technique to move away from monolithic applications. In principle, the \nresulting units of functions are in line with the microservices characteristics.\nFor instance, in a typical airline website, statistics indicate that the ratio of flight \nsearching to flight booking could be as high as 500:1. This means one booking \ntransaction for every 500 search transactions. In this scenario, the search needs \n500 times more scalability than the booking function. This is an ideal use case for \nselective scaling.\n",
      "content_length": 1435,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "Chapter 1\n[ 27 ]\nThe solution is to treat search requests and booking requests differently. With \na monolithic architecture, this is only possible with z scaling in the scale cube. \nHowever, this approach is expensive because in the z scale, the entire code base  \nis replicated.\nIn the preceding diagram, Search and Booking are designed as different microservices \nso that Search can be scaled differently from Booking. In the diagram, Search has \nthree instances, and Booking has two instances. Selective scalability is not limited \nto the number of instances, as shown in the diagram, but also in the way in which \nthe microservices are architected. In the case of Search, an in-memory data grid \n(IMDG) such as Hazelcast can be used as the data store. This will further increase the \nperformance and scalability of Search. When a new Search microservice instance is \ninstantiated, an additional IMDG node is added to the IMDG cluster. Booking does \nnot require the same level of scalability. In the case of Booking, both instances of the \nBooking microservice are connected to the same instance of the database.\nAllowing substitution\nMicroservices are self-contained, independent deployment modules enabling the \nsubstitution of one microservice with another similar microservice.\nMany large enterprises follow buy-versus-build policies to implement software \nsystems. A common scenario is to build most of the functions in house and buy \ncertain niche capabilities from specialists outside. This poses challenges in traditional \nmonolithic applications as these application components are highly cohesive. \nAttempting to plug in third-party solutions to the monolithic applications results in \ncomplex integrations. With microservices, this is not an afterthought. Architecturally, \na microservice can be easily replaced by another microservice developed either  \nin-house or even extended by a microservice from a third party.\n",
      "content_length": 1933,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "Demystifying Microservices\n[ 28 ]\nA pricing engine in the airline business is complex. Fares for different routes are \ncalculated using complex mathematical formulas known as the pricing logic. Airlines \nmay choose to buy a pricing engine from the market instead of building the product \nin house. In the monolithic architecture, Pricing is a function of Fares and Booking. In \nmost cases Pricing, Fares, and Booking are hardwired, making it almost impossible \nto detach.\nIn a well-designed microservices system, Booking, Fares, and Pricing would \nbe independent microservices. Replacing the Pricing microservice will have \nonly a minimal impact on any other services as they are all loosely coupled and \nindependent. Today, it could be a third-party service; tomorrow, it could be easily \nsubstituted by another third-party or home-grown service.\nEnabling to build organic systems\nMicroservices help us build systems that are organic in nature. This is significantly \nimportant when migrating monolithic systems gradually to microservices.\nOrganic systems are systems that grow laterally over a period of time by adding \nmore and more functions to it. In practice, an application grows unimaginably \nlarge in its lifespan, and in most cases, the manageability of the application reduces \ndramatically over this same period of time.\nMicroservices are all about independently manageable services. This enable us to \nkeep adding more and more services as the need arises with minimal impact on the \nexisting services. Building such systems does not need huge capital investments. \nHence, businesses can keep building as part of their operational expenditure.\nA loyalty system in an airline was built years ago, targeting individual passengers. \nEverything was fine until the airline started offering loyalty benefits to their \ncorporate customers. Corporate customers are individuals grouped under \ncorporations. As the current systems core data model is flat, targeting individuals, \nthe corporate environment needs a fundamental change in the core data model,  \nand hence huge reworking, to incorporate this requirement.\n",
      "content_length": 2121,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "Chapter 1\n[ 29 ]\nAs shown in the preceding diagram, in a microservices-based architecture, customer \ninformation would be managed by the Customer microservice and loyalty by the \nLoyalty Points microservice.\nIn this situation, it is easy to add a new Corporate Customer microservice to manage \ncorporate customers. When a corporation is registered, individual members will \nbe pushed to the Customer microservice to manage them as usual. The Corporate \nCustomer microservice provides a corporate view by aggregating data from the \nCustomer microservice. It will also provide services to support corporate-specific \nbusiness rules. With this approach, adding new services will have only a minimal \nimpact on the existing services.\nHelping reducing technology debt\nAs microservices are smaller in size and have minimal dependencies, they allow the \nmigration of services that use end-of-life technologies with minimal cost.\nTechnology changes are one of the barriers in software development. In many \ntraditional monolithic applications, due to the fast changes in technologies, today's \nnext-generation applications could easily become legacy even before their release \nto production. Architects and developers tend to add a lot of protection against \ntechnology changes by adding layers of abstractions. However, in reality, this \napproach does not solve the issue but, instead, results in over-engineered systems. \nAs technology upgrades are often risky and expensive with no direct returns to \nbusiness, the business may not be happy to invest in reducing the technology  \ndebt of the applications.\nWith microservices, it is possible to change or upgrade technology for each service \nindividually rather than upgrading an entire application.\nUpgrading an application with, for instance, five million lines written on EJB 1.1 and \nHibernate to the Spring, JPA, and REST services is almost similar to rewriting the \nentire application. In the microservices world, this could be done incrementally.\n",
      "content_length": 1998,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "Demystifying Microservices\n[ 30 ]\nAs shown in the preceding diagram, while older versions of the services are running \non old versions of technologies, new service developments can leverage the latest \ntechnologies. The cost of migrating microservices with end-of-life technologies is \nconsiderably less compared to enhancing monolithic applications.\nAllowing the coexistence of different versions\nAs microservices package the service runtime environment along with the service \nitself, this enables having multiple versions of the service to coexist in the same \nenvironment.\nThere will be situations where we will have to run multiple versions of the same \nservice at the same time. Zero downtime promote, where one has to gracefully \nswitch over from one version to another, is one example of a such a scenario as \nthere will be a time window where both services will have to be up and running \nsimultaneously. With monolithic applications, this is a complex procedure because \nupgrading new services in one node of the cluster is cumbersome as, for instance, \nthis could lead to class loading issues. A canary release, where a new version is \nonly released to a few users to validate the new service, is another example where \nmultiple versions of the services have to coexist.\nWith microservices, both these scenarios are easily manageable. As each microservice \nuses independent environments, including service listeners such as Tomcat or Jetty \nembedded, multiple versions can be released and gracefully transitioned without \nmany issues. When consumers look up services, they look for specific versions of \nservices. For example, in a canary release, a new user interface is released to user \nA. When user A sends a request to the microservice, it looks up the canary release \nversion, whereas all other users will continue to look up the last production version.\nCare needs to be taken at the database level to ensure the database design is always \nbackward compatible to avoid breaking the changes.\n",
      "content_length": 2009,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "Chapter 1\n[ 31 ]\nAs shown in the preceding diagram, version 1 and 2 of the Customer service can \ncoexist as they are not interfering with each other, given their respective deployment \nenvironments. Routing rules can be set at the gateway to divert traffic to specific \ninstances, as shown in the diagram. Alternatively, clients can request specific \nversions as part of the request itself. In the diagram, the gateway selects the version \nbased on the region from which the request is originated.\nSupporting the building of self-organizing \nsystems\nMicroservices help us build self-organizing systems. A self-organizing system \nsupport will automate deployment, be resilient, and exhibit self-healing and self-\nlearning capabilities.\nIn a well-architected microservices system, a service is unaware of other services. It \naccepts a message from a selected queue and processes it. At the end of the process, it \nmay send out another message, which triggers other services. This allows us to drop \nany service into the ecosystem without analyzing the impact on the overall system. \nBased on the input and output, the service will self-organize into the ecosystem. No \nadditional code changes or service orchestration is required. There is no central brain \nto control and coordinate the processes.\nImagine an existing notification service that listens to an INPUT queue and sends \nnotifications to an SMTP server, as shown in the following figure:\nLet's assume, later, a personalization engine, responsible for changing the language \nof the message to the customer's native language, needs to be introduced to \npersonalize messages before sending them to the customer, the personalization \nengine is responsible for changing the language of the message to the customer's \nnative language.\n",
      "content_length": 1788,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "Demystifying Microservices\n[ 32 ]\nWith microservices, a new personalization microservice will be created to do this \njob. The input queue will be configured as INPUT in an external configuration \nserver, and the personalization service will pick up the messages from the INPUT \nqueue (earlier, this was used by the notification service) and send the messages \nto the OUTPUT queue after completing process. The notification services input \nqueue will then send to OUTPUT. From the very next moment onward, the system \nautomatically adopts this new message flow.\nSupporting event-driven architecture\nMicroservices enable us to develop transparent software systems. Traditional \nsystems communicate with each other through native protocols and hence behave \nlike a black box application. Business events and system events, unless published \nexplicitly, are hard to understand and analyze. Modern applications require data  \nfor business analysis, to understand dynamic system behaviors, and analyze market  \ntrends, and they also need to respond to real-time events. Events are useful \nmechanisms for data extraction.\nA well-architected microservice always works with events for both input and output. \nThese events can be tapped by any service. Once extracted, events can be used for  \na variety of use cases.\nFor example, the business wants to see the velocity of orders categorized by product \ntype in real time. In a monolithic system, we need to think about how to extract these \nevents. This may impose changes in the system.\nIn the microservices world, Order Event is already published whenever an order is \ncreated. This means that it is just a matter of adding a new service to subscribe to the \nsame topic, extract the event, perform the requested aggregations, and push another \nevent for the dashboard to consume.\n",
      "content_length": 1823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "Chapter 1\n[ 33 ]\nEnabling DevOps\nMicroservices are one of the key enablers of DevOps. DevOps is widely adopted  \nas a practice in many enterprises, primarily to increase the speed of delivery and \nagility. A successful adoption of DevOps requires cultural changes, process changes, \nas well as architectural changes. DevOps advocates to have agile development,  \nhigh-velocity release cycles, automatic testing, automatic infrastructure provisioning, \nand automated deployment. \nAutomating all these processes is extremely hard to achieve with traditional \nmonolithic applications. Microservices are not the ultimate answer, but microservices \nare at the center stage in many DevOps implementations. Many DevOps tools and \ntechniques are also evolving around the use of microservices.\nConsider a monolithic application that takes hours to complete a full build and 20 \nto 30 minutes to start the application; one can see that this kind of application is not \nideal for DevOps automation. It is hard to automate continuous integration on every \ncommit. As large, monolithic applications are not automation friendly, continuous \ntesting and deployments are also hard to achieve.\nOn the other hand, small footprint microservices are more automation-friendly and \ntherefore can more easily support these requirements.\nMicroservices also enable smaller, focused agile teams for development. Teams will \nbe organized based on the boundaries of microservices.\nRelationship with other architecture \nstyles\nNow that we have seen the characteristics and benefits of microservices, in this \nsection, we will explore the relationship of microservices with other closely related \narchitecture styles such as SOA and Twelve-Factor Apps.\nRelations with SOA\nSOA and microservices follow similar concepts. Earlier in this chapter, we discussed \nthat microservices are evolved from SOA, and many service characteristics are \ncommon in both approaches.\nHowever, are they the same or are they different?\nAs microservices are evolved from SOA, many characteristics of microservices are \nsimilar to SOA. Let's first examine the definition of SOA.\n",
      "content_length": 2125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "Demystifying Microservices\n[ 34 ]\nThe definition of SOA from The Open Group consortium is as follows:\n\"Service-Oriented Architecture (SOA) is an architectural style that supports \nservice orientation. Service orientation is a way of thinking in terms of services  \nand service-based development and the outcomes of services.\nA service:\nIs a logical representation of a repeatable business activity that has a specified \noutcome (e.g., check customer credit, provide weather data, consolidate drilling \nreports)\nIt is self-contained.\nIt may be composed of other services.\nIt is a \"black box\" to consumers of the service.\"\nWe observed similar aspects in microservices as well. So, in what way are \nmicroservices different? The answer is: it depends.\nThe answer to the previous question could be yes or no, depending upon the \norganization and its adoption of SOA. SOA is a broader term, and different \norganizations approached SOA differently to solve different organizational \nproblems. The difference between microservices and SOA is in a way based  \non how an organization approaches SOA.\nIn order to get clarity, a few cases will be examined.\nService-oriented integration\nService-oriented integration refers to a service-based integration approach used by \nmany organizations.\n",
      "content_length": 1279,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "Chapter 1\n[ 35 ]\nMany organizations would have used SOA primarily to solve their integration \ncomplexities, also known as integration spaghetti. Generally, this is termed as \nService-Oriented Integration (SOI). In such cases, applications communicate \nwith each other through a common integration layer using standard protocols and \nmessage formats such as SOAP/XML-based web services over HTTP or JMS. These \ntypes of organizations focus on Enterprise Integration Patterns (EIP) to model their \nintegration requirements. This approach strongly relies on heavyweight ESB such \nas TIBCO Business Works, WebSphere ESB, Oracle ESB, and the likes. Most ESB \nvendors also packed a set of related products such as rules engines, business process \nmanagement engines, and so on as an SOA suite. Such organizations' integrations are \ndeeply rooted into their products. They either write heavy orchestration logic in the \nESB layer or the business logic itself in the service bus. In both cases, all enterprise \nservices are deployed and accessed via ESB. These services are managed through an \nenterprise governance model. For such organizations, microservices are altogether \ndifferent from SOA.\nLegacy modernization\nSOA is also used to build service layers on top of legacy applications.\nAnother category of organizations would use SOA in transformation projects or \nlegacy modernization projects. In such cases, the services are built and deployed \nin the ESB layer connecting to backend systems using ESB adapters. For these \norganizations, microservices are different from SOA.\n",
      "content_length": 1575,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "Demystifying Microservices\n[ 36 ]\nService-oriented application\nSome organizations adopt SOA at an application level.\nIn this approach, lightweight integration frameworks, such as Apache Camel or Spring \nIntegration, are embedded within applications to handle service-related cross-cutting \ncapabilities such as protocol mediation, parallel execution, orchestration, and service \nintegration. As some of the lightweight integration frameworks have native Java object \nsupport, such applications would even use native Plain Old Java Objects (POJO) \nservices for integration and data exchange between services. As a result, all services \nhave to be packaged as one monolithic web archive. Such organizations could see \nmicroservices as the next logical step of their SOA.\nMonolithic migration using SOA\nThe last possibility is transforming a monolithic application into smaller units \nafter hitting the breaking point with the monolithic system. They would break the \napplication into smaller, physically deployable subsystems, similar to the y axis \nscaling approach explained earlier, and deploy them as web archives on web servers \nor as JARs deployed on some home-grown containers. These subsystems as service \nwould use web services or other lightweight protocols to exchange data between \nservices. They would also use SOA and service design principles to achieve this. For \nsuch organizations, they may tend to think that microservices are the same old wine \nin a new bottle.\n",
      "content_length": 1480,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "Chapter 1\n[ 37 ]\nRelations with Twelve-Factor apps\nCloud computing is one of the rapidly evolving technologies. Cloud computing \npromises many benefits, such as cost advantage, speed, agility, flexibility, and \nelasticity. There are many cloud providers offering different services. They lower the \ncost models to make it more attractive to the enterprises. Different cloud providers \nsuch as AWS, Microsoft, Rackspace, IBM, Google, and so on use different tools, \ntechnologies, and services. On the other hand, enterprises are aware of this evolving \nbattlefield and, therefore, they are looking for options for de-risking from lockdown \nto a single vendor.\nMany organizations do lift and shift their applications to the cloud. In such cases, \nthe applications may not realize all the benefits promised by cloud platforms. Some \napplications need to undergo overhaul, whereas some may need minor tweaking \nbefore moving to cloud. This by and large depends upon how the application is \narchitected and developed.\nFor example, if the application has its production database server URLs hardcoded \nas part of the applications WAR, it needs to be modified before moving the \napplication to cloud. In the cloud, the infrastructure is transparent to the application, \nand especially, the physical IP addresses cannot be assumed.\nHow do we ensure that an application, or even microservices, can run seamlessly \nacross multiple cloud providers and take advantages of cloud services such as \nelasticity?\nIt is important to follow certain principles while developing cloud native applications.\nCloud native is a term used for developing applications that can work \nefficiently in a cloud environment, understanding and utilizing cloud \nbehaviors such as elasticity, utilization based charging, fail aware, and \nso on.\nTwelve-Factor App, forwarded by Heroku, is a methodology describing the \ncharacteristics expected from modern cloud-ready applications. Twelve-Factor App \nis equally applicable for microservices as well. Hence, it is important to understand \nTwelve-Factor App.\n",
      "content_length": 2070,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "Demystifying Microservices\n[ 38 ]\nA single code base\nThe code base principle advises that each application has a single code base. There \ncan be multiple instances of deployment of the same code base, such as development, \ntesting, and production. Code is typically managed in a source control system such \nas Git, Subversion, and so on.\nExtending the same philosophy for microservices, each microservice should have its \nown code base, and this code base is not shared with any other microservice. It also \nmeans that one microservice has exactly one code base.\nBundling dependencies\nAs per this principle, all applications should bundle their dependencies along with \nthe application bundle. With build tools such as Maven and Gradle, we explicitly \nmanage dependencies in a pom.xml or the .gradle file and link them using a central \nbuild artifact repository such as Nexus or Archiva. This ensures that the versions \nare managed correctly. The final executables will be packaged as a WAR file or an \nexecutable JAR file, embedding all the dependencies.\nIn the context of microservices, this is one of the fundamental principles to be followed. \nEach microservice should bundle all the required dependencies and execution libraries \nsuch as the HTTP listener and so on in the final executable bundle.\n",
      "content_length": 1303,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "Chapter 1\n[ 39 ]\nExternalizing configurations\nThis principle advises the externalization of all configuration parameters from the \ncode. An application's configuration parameters vary between environments, such as \nsupport to the e-mail IDs or URL of an external system, username, passwords, queue \nname, and so on. These will be different for development, testing, and production. \nAll service configurations should be externalized.\nThe same principle is obvious for microservices as well. The microservices \nconfiguration parameters should be loaded from an external source. This will also \nhelp to automate the release and deployment process as the only difference between \nthese environments is the configuration parameters.\nBacking services are addressable\nAll backing services should be accessible through an addressable URL. All services \nneed to talk to some external resources during the life cycle of their execution. \nFor example, they could be listening or sending messages to a messaging system, \nsending an e-mail, persisting data to database, and so on. All these services should \nbe reachable through a URL without complex communication requirements.\n",
      "content_length": 1167,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "Demystifying Microservices\n[ 40 ]\nIn the microservices world, microservices either talk to a messaging system to send \nor receive messages, or they could accept or send messages to other service APIs. In \na regular case, these are either HTTP endpoints using REST and JSON or TCP- or \nHTTP-based messaging endpoints.\nIsolation between build, release, and run\nThis principle advocates a strong isolation between the build, release, and run stages. \nThe build stage refers to compiling and producing binaries by including all the \nassets required. The release stage refers to combining binaries with environment-\nspecific configuration parameters. The run stage refers to running application on a \nspecific execution environment. The pipeline is unidirectional, so it is not possible \nto propagate changes from the run stages back to the build stage. Essentially, it also \nmeans that it is not recommended to do specific builds for production; rather, it has \nto go through the pipeline.\nIn microservices, the build will create executable JAR files, including the service \nruntime such as an HTTP listener. During the release phase, these executables will be \ncombined with release configurations such as production URLs and so on and create \na release version, most probably as a container similar to Docker. In the run stage, \nthese containers will be deployed on production via a container scheduler.\nStateless, shared nothing processes\nThis principle suggests that processes should be stateless and share nothing. If the \napplication is stateless, then it is fault tolerant and can be scaled out easily.\nAll microservices should be designed as stateless functions. If there is any \nrequirement to store a state, it should be done with a backing database or  \nin an in-memory cache.\n",
      "content_length": 1784,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "Chapter 1\n[ 41 ]\nExposing services through port bindings\nA Twelve-Factor application is expected to be self-contained. Traditionally, \napplications are deployed to a server: a web server or an application server such as \nApache Tomcat or JBoss. A Twelve-Factor application does not rely on an external \nweb server. HTTP listeners such as Tomcat or Jetty have to be embedded in the \nservice itself.\nPort binding is one of the fundamental requirements for microservices to be \nautonomous and self-contained. Microservices embed service listeners as a part  \nof the service itself.\nConcurrency to scale out\nThis principle states that processes should be designed to scale out by replicating the \nprocesses. This is in addition to the use of threads within the process.\nIn the microservices world, services are designed to scale out rather than scale up. \nThe x axis scaling technique is primarily used for a scaling service by spinning up \nanother identical service instance. The services can be elastically scaled or shrunk \nbased on the traffic flow. Further to this, microservices may make use of parallel \nprocessing and concurrency frameworks to further speed up or scale up the \ntransaction processing.\nDisposability with minimal overhead\nThis principle advocates building applications with minimal startup and shutdown \ntimes with graceful shutdown support. In an automated deployment environment, \nwe should be able bring up or bring down instances as quick as possible. If the \napplication's startup or shutdown takes considerable time, it will have an adverse \neffect on automation. The startup time is proportionally related to the size of the \napplication. In a cloud environment targeting auto-scaling, we should be able to  \nspin up new instance quickly. This is also applicable when promoting new versions \nof services.\nIn the microservices context, in order to achieve full automation, it is extremely \nimportant to keep the size of the application as thin as possible, with minimal startup \nand shutdown time. Microservices also should consider a lazy loading of objects  \nand data.\n",
      "content_length": 2097,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "Demystifying Microservices\n[ 42 ]\nDevelopment and production parity\nThis principle states the importance of keeping development and production \nenvironments as identical as possible. For example, let's consider an application with \nmultiple services or processes, such as a job scheduler service, cache services, and \none or more application services. In a development environment, we tend to run all \nof them on a single machine, whereas in production, we will facilitate independent \nmachines to run each of these processes. This is primarily to manage the cost \nof infrastructure. The downside is that if production fails, there is no identical \nenvironment to re-produce and fix the issues.\nNot only is this principle valid for microservices, but it is also applicable to any \napplication development.\nExternalizing logs\nA Twelve-Factor application never attempts to store or ship log files. In a cloud, it is \nbetter to avoid local I/Os. If the I/Os are not fast enough in a given infrastructure, \nit could create a bottleneck. The solution to this is to use a centralized logging \nframework. Splunk, Greylog, Logstash, Logplex, and Loggly are some examples  \nof log shipping and analysis tools. The recommended approach is to ship logs to  \na central repository by tapping the logback appenders and write to one of the \nshippers' endpoints.\nIn a microservices ecosystem, this is very important as we are breaking a system  \ninto a number of smaller services, which could result in decentralized logging. If they \nstore logs in a local storage, it would be extremely difficult to correlate logs between \nservices.\nIn development, the microservice may direct the log stream to stdout, whereas in \nproduction, these streams will be captured by the log shippers and sent to a central \nlog service for storage and analysis.\n",
      "content_length": 1825,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "Chapter 1\n[ 43 ]\nPackage admin processes\nApart from application services, most applications provide admin tasks as well. This \nprinciple advises to use the same release bundle as well as an identical environment \nfor both application services and admin tasks. Admin code should also be packaged \nalong with the application code.\nNot only is this principle valid for microservices, but also it is applicable to any \napplication development.\nMicroservice use cases\nA microservice is not a silver bullet and will not solve all the architectural challenges \nof today's world. There is no hard-and-fast rule or rigid guideline on when to use \nmicroservices.\nMicroservices may not fit in each and every use case. The success of microservices \nlargely depends on the selection of use cases. The first and the foremost activity is \nto do a litmus test of the use case against the microservices' benefits. The litmus test \nmust cover all the microservices' benefits we discussed earlier in this chapter. For a \ngiven use case, if there are no quantifiable benefits or the cost outweighs the benefits, \nthen the use case may not be the right choice for microservices.\nLet's discuss some commonly used scenarios that are suitable candidates for a \nmicroservices architecture:\n•\t\nMigrating a monolithic application due to improvements required in \nscalability, manageability, agility, or speed of delivery. Another similar \nscenario is rewriting an end-of-life heavily used legacy application. In \nboth cases, microservices present an opportunity. Using a microservices \narchitecture, it is possible to replatform a legacy application by slowly \ntransforming functions to microservices. There are benefits in this approach. \nThere is no humongous upfront investment required, no major disruption \nto business, and no severe business risks. As the service dependencies are \nknown, the microservices dependencies can be well managed.\n•\t\nUtility computing scenarios such as integrating an optimization service, \nforecasting service, price calculation service, prediction service, offer service, \nrecommendation service, and so on are good candidates for microservices. \nThese are independent stateless computing units that accept certain data, \napply algorithms, and return the results. Independent technical services such \nas the communication service, the encryption service, authentication services, \nand so on are also good candidates for microservices.\n",
      "content_length": 2443,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "Demystifying Microservices\n[ 44 ]\n•\t\nIn many cases, we can build headless business applications or services that \nare autonomous in nature—for instance, the payment service, login service, \nflight search service, customer profile service, notification service, and so on. \nThese are normally reused across multiple channels and, hence, are good \ncandidates for building them as microservices.\n•\t\nThere could be micro or macro applications that serve a single purpose and \nperforming a single responsibility. A simple time tracking application is an \nexample of this category. All it does is capture the time, duration, and task \nperformed. Common-use enterprise applications are also candidates for \nmicroservices.\n•\t\nBackend services of a well-architected, responsive client-side MVC web \napplication (the Backend as a Service (BaaS) scenario) load data on demand \nin response to the user navigation. In most of these scenarios, data could be \ncoming from multiple logically different data sources as described in the Fly \nBy Points example mentioned earlier.\n•\t\nHighly agile applications, applications demanding speed of delivery or time \nto market, innovation pilots, applications selected for DevOps, applications \nof the System of Innovation type, and so on could also be considered as \npotential candidates for the microservices architecture.\n•\t\nApplications that we could anticipate getting benefits from microservices \nsuch as polyglot requirements, applications that require Command Query \nResponsibility segregations (CQRS), and so on are also potential candidates \nof the microservices architecture.\nIf the use case falls into any of these categories, it is a potential candidate for the \nmicroservices architecture.\nThere are few scenarios in which we should consider avoiding microservices:\n•\t\nIf the organization's policies are forced to use centrally managed \nheavyweight components such as ESB to host a business logic or if the \norganization has any other policies that hinder the fundamental principles \nof microservices, then microservices are not the right solution unless the \norganizational process is relaxed.\n•\t\nIf the organization's culture, processes, and so on are based on the \ntraditional waterfall delivery model, lengthy release cycles, matrix teams, \nmanual deployments and cumbersome release processes, no infrastructure \nprovisioning, and so on, then microservices may not be the right fit. This \nis underpinned by Conway's Law. This states that there is a strong link \nbetween the organizational structure and software it creates.\n",
      "content_length": 2566,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "Chapter 1\n[ 45 ]\nRead more about the Conway's Law at:\nhttp://www.melconway.com/Home/Conways_Law.html\nMicroservices early adopters\nMany organizations have already successfully embarked on their journey to the \nmicroservices world. In this section, we will examine some of the frontrunners on the \nmicroservices space to analyze why they did what they did and how they did it. We \nwill conduct some analysis at the end to draw some conclusions:\n•\t\nNetflix (www.netflix.com): Netflix, an international on-demand media \nstreaming company, is a pioneer in the microservices space. Netflix \ntransformed their large pool of developers developing traditional monolithic \ncode to smaller development teams producing microservices. These \nmicroservices work together to stream digital media to millions of Netflix \ncustomers. At Netflix, engineers started with monolithic, went through \nthe pain, and then broke the application into smaller units that are loosely \ncoupled and aligned to the business capability.\n•\t\nUber (www.uber.com): Uber, an international transportation network \ncompany, began in 2008 with a monolithic architecture with a single code \nbase. All services were embedded into the monolithic application. When \nUber expanded their business from one city to multiple cities, the challenges \nstarted. Uber then moved to SOA-based architecture by breaking the system \ninto smaller independent units. Each module was given to different teams and \nempowered them to choose their language, framework, and database. Uber \nhas many microservices deployed in their ecosystem using RPC and REST.\n•\t\nAirbnb (www.airbnb.com): Airbnb, a world leader providing a trusted \nmarketplace for accommodation, started with a monolithic application that \nperformed all the required functions of the business. Airbnb faced scalability \nissues with increased traffic. A single code base became too complicated to \nmanage, resulted in a poor separation of concerns, and ran into performance \nissues. Airbnb broke their monolithic application into smaller pieces with \nseparate code bases running on separate machines with separate deployment \ncycles. Airbnb developed their own microservices or SOA ecosystem around \nthese services.\n",
      "content_length": 2217,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "Demystifying Microservices\n[ 46 ]\n•\t\nOrbitz (www.orbitz.com): Orbitz, an online travel portal, started with a \nmonolithic architecture in the 2000s with a web layer, a business layer, and a \ndatabase layer. As Orbitz expanded their business, they faced manageability \nand scalability issues with monolithic-tiered architecture. Orbitz then went \nthrough continuous architecture changes. Later, Orbitz broke down their \nmonolithic to many smaller applications.\n•\t\neBay (www.ebay.com): eBay, one of the largest online retailers, started \nin the late 1990s with a monolithic Perl application and FreeBSD as the \ndatabase. eBay went through scaling issues as the business grew. It was \nconsistently investing in improving its architecture. In the mid 2000s, eBay \nmoved to smaller decomposed systems based on Java and web services. \nThey employed database partitions and functional segregation to meet the \nrequired scalability.\n•\t\nAmazon (www.amazon.com): Amazon, one of the largest online retailer \nwebsites, was run on a big monolithic application written on C++ \nin 2001. The well-architected monolithic application was based on a \ntiered architecture with many modular components. However, all these \ncomponents were tightly coupled. As a result, Amazon was not able to speed \nup their development cycle by splitting teams into smaller groups. Amazon \nthen separated out the code as independent functional services, wrapped \nwith web services, and eventually advanced to microservices.\n•\t\nGilt (www.gilt.com): Gilt, an online shopping website, began in 2007 with \na tiered monolithic Rails application and a Postgres database at the back. \nSimilarly to many other applications, as traffic volumes increased, the web \napplication was not able to provide the required resiliency. Gilt went through \nan architecture overhaul by introducing Java and polyglot persistence. Later, \nGilt moved to many smaller applications using the microservices concept.\n•\t\nTwitter (www.twitter.com): Twitter, one of the largest social websites, \nbegan with a three-tiered monolithic rails application in the mid 2000s. Later, \nwhen Twitter experienced growth in its user base, they went through an \narchitecture-refactoring cycle. With this refactoring, Twitter moved away \nfrom a typical web application to an API-based even driven core. Twitter \nuses Scala and Java to develop microservices with polyglot persistence.\n•\t\nNike (www.nike.com): Nike, the world leader in apparel and footwear, \ntransformed their monolithic applications to microservices. Similarly to many \nother organizations, Nike too was run with age-old legacy applications that \nwere hardly stable. In their journey, Nike moved to heavyweight commercial \nproducts with an objective to stabilize legacy applications but ended up in \nmonolithic applications that were expensive to scale, had long release cycles, \nand needed too much manual work to deploy and manage applications. \nLater, Nike moved to a microservices-based architecture that brought down \nthe development cycle considerably.\n",
      "content_length": 3041,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "Chapter 1\n[ 47 ]\nThe common theme is monolithic migrations\nWhen we analyze the preceding enterprises, there is one common theme. All these \nenterprises started with monolithic applications and transitioned to a microservices \narchitecture by applying learning and pain points from their previous editions.\nEven today, many start-ups begin with monolith as it is easy to start, conceptualize, \nand then slowly move to microservices when the demand arises. Monolithic to \nmicroservices migration scenarios have an added advantage: they have all the \ninformation upfront, readily available for refactoring.\nThough, for all these enterprises, it is monolithic transformation, the catalysts were \ndifferent for different organizations. Some of the common motivations are a lack \nof scalability, long development cycles, process automation, manageability, and \nchanges in the business models.\nWhile monolithic migrations are no-brainers, there are opportunities to build \nmicroservices from the ground up. More than building ground-up systems, look \nfor opportunities to build smaller services that are quick wins for business—for \nexample, adding a trucking service to an airline's end-to-end cargo management \nsystem or adding a customer scoring service to a retailer's loyalty system. These \ncould be implemented as independent microservices exchanging messages with  \ntheir respective monolithic applications.\nAnother point is that many organizations use microservices only for their business-\ncritical customer engagement applications, leaving the rest of the legacy monolithic \napplications to take their own trajectory.\nAnother important observation is that most of the organizations examined \npreviously are at different levels of maturity in their microservices journey. When \neBay transitioned from a monolithic application in the early 2000s, they functionally \nsplit the application into smaller, independent, and deployable units. These logically \ndivided units are wrapped with web services. While single responsibility and \nautonomy are their underpinning principles, the architectures are limited to the \ntechnologies and tools available at that point in time. Organizations such as Netflix \nand Airbnb built capabilities of their own to solve the specific challenges they faced. \nTo summarize, all of these are not truly microservices, but are small, business-\naligned services following the same characteristics.\nThere is no state called \"definite or ultimate microservices\". It is a journey and is \nevolving and maturing day by day. The mantra for architects and developers is the \nreplaceability principle; build an architecture that maximizes the ability to replace its \nparts and minimizes the cost of replacing its parts. The bottom line is that enterprises \nshouldn't attempt to develop microservices by just following the hype.\n",
      "content_length": 2847,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "Demystifying Microservices\n[ 48 ]\nSummary\nIn this chapter, you learned about the fundamentals of microservices with the help of \na few examples.\nWe explored the evolution of microservices from traditional monolithic applications. \nWe examined some of the principles and the mind shift required for modern \napplication architectures. We also took a look at the characteristics and benefits \nof microservices and use cases. In this chapter, we established the microservices' \nrelationship with service-oriented architecture and Twelve-Factor Apps. Lastly, we \nanalyzed examples of a few enterprises from different industries.\nWe will develop a few sample microservices in the next chapter to bring more clarity \nto our learnings in this chapter.\n",
      "content_length": 744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "[ 49 ]\nBuilding Microservices with \nSpring Boot\nDeveloping microservices is not so tedious anymore thanks to the powerful \nSpring Boot framework. Spring Boot is a framework to develop production-ready \nmicroservices in Java.\nThis chapter will move from the microservices theory explained in the previous \nchapter to hands-on practice by reviewing code samples. This chapter will introduce \nthe Spring Boot framework and explain how Spring Boot can help build RESTful \nmicroservices in line with the principles and characteristics discussed in the previous \nchapter. Finally, some of the features offered by Spring Boot to make microservices \nproduction-ready will be reviewed.\nBy the end of this chapter, you will have learned about:\n•\t\nSetting up the latest Spring development environment\n•\t\nDeveloping RESTful services using the Spring framework\n•\t\nUsing Spring Boot to build fully qualified microservices\n•\t\nUseful Spring Boot features to build production-ready microservices\nSetting up a development environment\nTo crystalize microservices concepts, a couple of microservices will be built. For this, \nit is assumed that the following components are installed:\n•\t\nJDK 1.8: http://www.oracle.com/technetwork/java/javase/\ndownloads/jdk8-downloads-2133151.html\n•\t\nSpring Tool Suite 3.7.2 (STS): https://spring.io/tools/sts/all\n•\t\nMaven 3.3.1: https://maven.apache.org/download.cgi\n",
      "content_length": 1382,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "Building Microservices with Spring Boot\n[ 50 ]\nAlternately, other IDEs such as IntelliJ IDEA, NetBeans, or Eclipse could be used. \nSimilarly, alternate build tools such as Gradle can be used. It is assumed that the \nMaven repository, class path, and other path variables are set properly to run STS \nand Maven projects.\nThis chapter is based on the following versions of Spring libraries:\n•\t\nSpring Framework 4.2.6.RELEASE\n•\t\nSpring Boot 1.3.5.RELEASE\nDetailed steps to download the code bundle are mentioned in the \nPreface of this book. Have a look.\nThe code bundle for the book is also hosted on GitHub at \nhttps://github.com/PacktPublishing/Spring-\nMicroservices. We also have other code bundles from our rich \ncatalog of books and videos available at https://github.com/\nPacktPublishing/. Check them out!\nDeveloping a RESTful service – the \nlegacy approach\nThis example will review the traditional RESTful service development before \njumping deep into Spring Boot.\nSTS will be used to develop this REST/JSON service.\nThe full source code of this example is available as the \nlegacyrest project in the code files of this book.\n",
      "content_length": 1131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "Chapter 2\n[ 51 ]\nThe following are the steps to develop the first RESTful service:\n1.\t Start STS and set a workspace of choice for this project.\n2.\t Navigate to File | New | Project.\n3.\t Select Spring Legacy Project as shown in the following screenshot and click \non Next:\n",
      "content_length": 273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "Building Microservices with Spring Boot\n[ 52 ]\n4.\t Select Spring MVC Project as shown in the following diagram and click  \non Next:\n5.\t Select a top-level package name of choice. This example uses org.rvslab.\nchapter2.legacyrest as the top-level package.\n6.\t Then, click on Finish.\n7.\t This will create a project in the STS workspace with the name legacyrest.\nBefore proceeding further, pom.xml needs editing.\n",
      "content_length": 410,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "Chapter 2\n[ 53 ]\n8.\t Change the Spring version to 4.2.6.RELEASE, as follows:\n<org.springframework-version>4.2.6.RELEASE</org.springframework-\nversion>\n9.\t Add Jackson dependencies in the pom.xml file for JSON-to-POJO and \nPOJO-to-JSON conversions. Note that the 2.*.* version is used to ensure \ncompatibility with Spring 4.\n<dependency>\n    <groupId>com.fasterxml.jackson.core</groupId>\n    <artifactId>jackson-databind</artifactId>\n    <version>2.6.4</version>\n</dependency>\n10.\t Some Java code needs to be added. In Java Resources, under legacyrest, \nexpand the package and open the default HomeController.java file:\n11.\t The default implementation is targeted more towards the MVC project. \nRewriting HomeController.java to return a JSON value in response to the \nREST call will do the trick. The resulting HomeController.java file will look \nsimilar to the following:\n@RestController\npublic class HomeController {\n  @RequestMapping(\"/\")\n  public Greet sayHello(){\n    return new Greet(\"Hello World!\");\n  }\n}\n",
      "content_length": 1012,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "Building Microservices with Spring Boot\n[ 54 ]\nclass Greet { \n  private String message;\n  public Greet(String message) {\n    this.message = message;\n  }\n  //add getter and setter\n}\nExamining the code, there are now two classes:\n°°\nGreet: This is a simple Java class with getters and setters to represent \na data object. There is only one attribute in the Greet class, which is \nmessage.\n°°\nHomeController.java: This is nothing but a Spring controller REST \nendpoint to handle HTTP requests.\nNote that the annotation used in HomeController is @RestController, \nwhich automatically injects @Controller and @ResponseBody and has the \nsame effect as the following code:\n@Controller\n@ResponseBody\npublic class HomeController { }\n12.\t The project can now be run by right-clicking on legacyrest, navigating to \nRun As | Run On Server, and then selecting the default server (Pivotal tc \nServer Developer Edition v3.1) that comes along with STS.\nThis should automatically start the server and deploy the web application on \nthe TC server.\nIf the server started properly, the following message will appear in the \nconsole:\nINFO : org.springframework.web.servlet.DispatcherServlet - \nFrameworkServlet 'appServlet': initialization completed in 906 ms\nMay 08, 2016 8:22:48 PM org.apache.catalina.startup.Catalina start\nINFO: Server startup in 2289 ms\n",
      "content_length": 1338,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "Chapter 2\n[ 55 ]\n13.\t If everything is fine, STS will open a browser window to http://\nlocalhost:8080/legacyrest/ and display the JSON object as shown in \nthe browser. Right-click on and navigate to legacyrest | Properties | Web \nProject Settings and review Context Root to identify the context root of the \nweb application:\nThe alternate build option is to use Maven. Right-click on the project and navigate \nto Run As | Maven install. This will generate chapter2-1.0.0-BUILD-SNAPSHOT.\nwar under the target folder. This war is deployable in any servlet container such as \nTomcat, JBoss, and so on.\nMoving from traditional web applications \nto microservices\nCarefully examining the preceding RESTful service will reveal whether this really \nconstitutes a microservice. At first glance, the preceding RESTful service is a fully \nqualified interoperable REST/JSON service. However, it is not fully autonomous \nin nature. This is primarily because the service relies on an underlying application \nserver or web container. In the preceding example, a war was explicitly created and \ndeployed on a Tomcat server.\nThis is a traditional approach to developing RESTful services as a web application. \nHowever, from the microservices point of view, one needs a mechanism to develop \nservices as executables, self-contained JAR files with an embedded HTTP listener.\nSpring Boot is a tool that allows easy development of such kinds of services. \nDropwizard and WildFly Swarm are alternate server-less RESTful stacks.\n",
      "content_length": 1506,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "Building Microservices with Spring Boot\n[ 56 ]\nUsing Spring Boot to build RESTful \nmicroservices\nSpring Boot is a utility framework from the Spring team to bootstrap Spring-\nbased applications and microservices quickly and easily. The framework uses an \nopinionated approach over configurations for decision making, thereby reducing the \neffort required in writing a lot of boilerplate code and configurations. Using the 80-20 \nprinciple, developers should be able to kickstart a variety of Spring applications with \nmany default values. Spring Boot further presents opportunities for the developers \nto customize applications by overriding the autoconfigured values.\nSpring Boot not only increases the speed of development but also provides a set \nof production-ready ops features such as health checks and metrics collection. As \nSpring Boot masks many configuration parameters and abstracts many lower-level \nimplementations, it minimizes the chance of error to a certain extent. Spring Boot \nrecognizes the nature of the application based on the libraries available in the class \npath and runs the autoconfiguration classes packaged in these libraries.\nOften, many developers mistakenly see Spring Boot as a code generator, but in \nreality, it is not. Spring Boot only autoconfigures build files—for example, POM files \nin the case of Maven. It also sets properties, such as data source properties, based on \ncertain opinionated defaults. Take a look at the following code:\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-jpa</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.hsqldb</groupId>\n    <artifactId>hsqldb</artifactId>\n    <scope>runtime</scope>\n</dependency>\nFor instance, in the preceding case, Spring Boot understands that the project is set to \nuse the Spring Data JPA and HSQL databases. It automatically configures the driver \nclass and other connection parameters.\nOne of the great outcomes of Spring Boot is that it almost eliminates the need to have \ntraditional XML configurations. Spring Boot also enables microservices' development \nby packaging all the required runtime dependencies in a fat executable JAR file.\n",
      "content_length": 2201,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "Chapter 2\n[ 57 ]\nGetting started with Spring Boot\nThere are different ways that Spring Boot-based application development can be \nstarted:\n•\t\nUsing the Spring Boot CLI as a command-line tool\n•\t\nUsing IDEs such as STS to provide Spring Boot, which are supported out of \nthe box\n•\t\nUsing the Spring Initializr project at http://start.spring.io\nAll these three options will be explored in this chapter, developing a variety of \nsample services.\nDeveloping the Spring Boot microservice \nusing the CLI\nThe easiest way to develop and demonstrate Spring Boot's capabilities is using the \nSpring Boot CLI, a command-line tool. Perform the following steps:\n1.\t Install the Spring Boot command-line tool by downloading the spring-\nboot-cli-1.3.5.RELEASE-bin.zip file from http://repo.spring.io/\nrelease/org/springframework/boot/spring-boot-cli/1.3.5.RELEASE/\nspring-boot-cli-1.3.5.RELEASE-bin.zip.\n2.\t Unzip the file into a directory of your choice. Open a terminal window and \nchange the terminal prompt to the bin folder.\nEnsure that the bin folder is added to the system path so that Spring Boot \ncan be run from any location.\n3.\t Verify the installation with the following command. If successful, the Spring \nCLI version will be printed in the console:\n$spring –-version\nSpring CLI v1.3.5.RELEASE\n",
      "content_length": 1291,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "Building Microservices with Spring Boot\n[ 58 ]\n4.\t As the next step, a quick REST service will be developed in Groovy, which \nis supported out of the box in Spring Boot. To do so, copy and paste the \nfollowing code using any editor of choice and save it as myfirstapp.groovy \nin any folder:\n@RestController\nclass HelloworldController {\n    @RequestMapping(\"/\")\n    String sayHello() {\n        \"Hello World!\"\n    }\n}\n5.\t In order to run this Groovy application, go to the folder where myfirstapp.\ngroovy is saved and execute the following command. The last few lines of \nthe server start-up log will be similar to the following:\n$spring run myfirstapp.groovy \n2016-05-09 18:13:55.351  INFO 35861 --- [nio-8080-exec-1] \no.s.web.servlet.DispatcherServlet        : FrameworkServlet \n'dispatcherServlet': initialization started\n2016-05-09 18:13:55.375  INFO 35861 --- [nio-8080-exec-1] \no.s.web.servlet.DispatcherServlet        : FrameworkServlet \n'dispatcherServlet': initialization completed in 24 ms\n6.\t Open a browser window and go to http://localhost:8080; the browser \nwill display the following message:\nHello World!\nThere is no war file created, and no Tomcat server was run. Spring Boot \nautomatically picked up Tomcat as the webserver and embedded it into the \napplication. This is a very basic, minimal microservice. The @RestController \nannotation, used in the previous code, will be examined in detail in the next example.\nDeveloping the Spring Boot Java \nmicroservice using STS\nIn this section, developing another Java-based REST/JSON Spring Boot service using \nSTS will be demonstrated.\n",
      "content_length": 1597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "Chapter 2\n[ 59 ]\nThe full source code of this example is available as the  \nchapter2.bootrest project in the code files of this book.\n1.\t Open STS, right-click within the Project Explorer window, navigate to \nNew | Project, and select Spring Starter Project, as shown in the following \nscreenshot, and click on Next:\nSpring Starter Project is a basic template wizard that provides a number of \nother starter libraries to select from.\n2.\t Type the project name as chapter2.bootrest or any other name of your \nchoice. It is important to choose the packaging as JAR. In traditional web \napplications, a war file is created and then deployed to a servlet container, \nwhereas Spring Boot packages all the dependencies to a self-contained, \nautonomous JAR file with an embedded HTTP listener.\n",
      "content_length": 787,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "Building Microservices with Spring Boot\n[ 60 ]\n3.\t Select 1.8 under Java Version. Java 1.8 is recommended for Spring 4 \napplications. Change the other Maven properties such as Group, Artifact, \nand Package, as shown in the following screenshot:\n4.\t Once completed, click on Next.\n",
      "content_length": 280,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "Chapter 2\n[ 61 ]\n5.\t The wizard will show the library options. In this case, as the REST service is \ndeveloped, select Web under Web. This is an interesting step that tells Spring \nBoot that a Spring MVC web application is being developed so that Spring \nBoot can include the necessary libraries, including Tomcat as the HTTP \nlistener and other configurations, as required:\n",
      "content_length": 375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "Building Microservices with Spring Boot\n[ 62 ]\n6.\t Click on Finish.\nThis will generate a project named chapter2.bootrest in Project Explorer \nin STS:\n7.\t Take a moment to examine the generated application. Files that are of  \ninterest are:\n°°\npom.xml\n°°\nApplication.java\n°°\nApplication.properties\n°°\nApplicationTests.java\nExamining the POM file\nThe parent element is one of the interesting aspects in the pom.xml file. Take a look at \nthe following:\n<parent>\n  <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-parent</artifactId>\n    <version>1.3.4.RELEASE</version>\n</parent>\n",
      "content_length": 608,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "Chapter 2\n[ 63 ]\nThe spring-boot-starter-parent pattern is a bill of materials (BOM), a pattern \nused by Maven's dependency management. BOM is a special kind of POM file used \nto manage different library versions required for a project. The advantage of using \nthe spring-boot-starter-parent POM file is that developers need not worry about \nfinding the right compatible versions of different libraries such as Spring, Jersey, JUnit, \nLogback, Hibernate, Jackson, and so on. For instance, in our first legacy example, \na specific version of the Jackson library was added to work with Spring 4. In this \nexample, these are taken care of by the spring-boot-starter-parent pattern.\nThe starter POM file has a list of Boot dependencies, sensible resource filtering, and \nsensible plug-in configurations required for the Maven builds.\nRefer to https://github.com/spring-projects/\nspring-boot/blob/1.3.x/spring-boot-\ndependencies/pom.xml to take a look at the different \ndependencies provided in the starter parent (version 1.3.x). \nAll these dependencies can be overridden if required.\nThe starter POM file itself does not add JAR dependencies to the project. Instead,  \nit will only add library versions. Subsequently, when dependencies are added to  \nthe POM file, they refer to the library versions from this POM file. A snapshot of  \nsome of the properties are as shown as follows:\n<spring-boot.version>1.3.5.BUILD-SNAPSHOT</spring-boot.version>\n<hibernate.version>4.3.11.Final</hibernate.version>\n<jackson.version>2.6.6</jackson.version>\n<jersey.version>2.22.2</jersey.version>\n<logback.version>1.1.7</logback.version>\n<spring.version>4.2.6.RELEASE</spring.version>\n<spring-data-releasetrain.version>Gosling-SR4</spring-data-\nreleasetrain.version>\n<tomcat.version>8.0.33</tomcat.version>\nReviewing the dependency section, one can see that this is a clean and neat POM file \nwith only two dependencies, as follows:\n<dependencies>\n   <dependency>\n  <groupId>org.springframework.boot</groupId>\n  <artifactId>spring-boot-starter-web</artifactId>\n   </dependency>\n",
      "content_length": 2059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "Building Microservices with Spring Boot\n[ 64 ]\n    \n   <dependency>\n  <groupId>org.springframework.boot</groupId>\n  <artifactId>spring-boot-starter-test</artifactId>\n  <scope>test</scope>\n   </dependency>\n</dependencies>\nAs web is selected, spring-boot-starter-web adds all dependencies required for \na Spring MVC project. It also includes dependencies to Tomcat as an embedded \nHTTP listener. This provides an effective way to get all the dependencies required as \na single bundle. Individual dependencies could be replaced with other libraries, for \nexample replacing Tomcat with Jetty.\nSimilar to web, Spring Boot comes up with a number of spring-boot-starter-* \nlibraries, such as amqp, aop, batch, data-jpa, thymeleaf, and so on.\nThe last thing to be reviewed in the pom.xml file is the Java 8 property. By default, \nthe parent POM file adds Java 6. It is recommended to override the Java version to 8 \nfor Spring:\n<java.version>1.8</java.version>\nExamining Application.java\nSpring Boot, by default, generated a org.rvslab.chapter2.Application.java \nclass under src/main/java to bootstrap, as follows:\n@SpringBootApplication\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n}\nThere is only a main method in Application, which will be invoked at startup as \nper the Java convention. The main method bootstraps the Spring Boot application by \ncalling the run method on SpringApplication. Application.class is passed as a \nparameter to tell Spring Boot that this is the primary component.\n",
      "content_length": 1576,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "Chapter 2\n[ 65 ]\nMore importantly, the magic is done by the @SpringBootApplication annotation. \nThe @SpringBootApplication annotation is a top-level annotation that encapsulates \nthree other annotations, as shown in the following code snippet:\n@Configuration\n@EnableAutoConfiguration\n@ComponentScan\npublic class Application {\nThe @Configuration annotation hints that the contained class declares one or  \nmore @Bean definitions. The @Configuration annotation is meta-annotated with  \n@Component; therefore, it is a candidate for component scanning.\nThe @EnableAutoConfiguration annotation tells Spring Boot to automatically \nconfigure the Spring application based on the dependencies available in the class path.\nExamining application.properties\nA default application.properties file is placed under src/main/resources. \nIt is an important file to configure any required properties for the Spring Boot \napplication. At the moment, this file is kept empty and will be revisited with some \ntest cases later in this chapter.\nExamining ApplicationTests.java\nThe last file to be examined is ApplicationTests.java under src/test/java.  \nThis is a placeholder to write test cases against the Spring Boot application.\nTo implement the first RESTful service, add a REST endpoint, as follows:\n1.\t One can edit Application.java under src/main/java and add a RESTful \nservice implementation. The RESTful service is exactly the same as what was \ndone in the previous project. Append the following code at the end of the \nApplication.java file:\n@RestController\nclass GreetingController{\n  @RequestMapping(\"/\")\n  Greet greet(){\n    return new Greet(\"Hello World!\");\n  }\n",
      "content_length": 1655,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "Building Microservices with Spring Boot\n[ 66 ]\n}\nclass Greet {\n  private String message;\npublic Greet() {}\n  public Greet(String message) {\n    this.message = message;\n  }\n//add getter and setter\n}\n2.\t To run, navigate to Run As | Spring Boot App. Tomcat will be started on the \n8080 port:\nWe can notice from the log that:\n°°\nSpring Boot get its own process ID (in this case, it is 41130)\n°°\nSpring Boot is automatically started with the Tomcat server at the \nlocalhost, port 8080.\n3.\t Next, open a browser and point to http://localhost:8080. This will show \nthe JSON response as shown in the following screenshot:\nA key difference between the legacy service and this one is that the Spring Boot \nservice is self-contained. To make this clearer, run the Spring Boot application \noutside STS. Open a terminal window, go to the project folder, and run Maven,  \nas follows:\n$ maven install\n",
      "content_length": 887,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "Chapter 2\n[ 67 ]\nThis will generate a fat JAR file under the target folder of the project. Running the \napplication from the command line shows:\n$java -jar target/bootrest-0.0.1-SNAPSHOT.jar\nAs one can see, bootrest-0.0.1-SNAPSHOT.jar is self-contained and could be run \nas a standalone application. At this point, the JAR is as thin as 13 MB. Even though \nthe application is no more than just \"Hello World\", the Spring Boot service just \ndeveloped, practically follows the principles of microservices.\nTesting the Spring Boot microservice\nThere are multiple ways to test REST/JSON Spring Boot microservices. The easiest \nway is to use a web browser or a curl command pointing to the URL, as follows:\ncurl http://localhost:8080\nThere are number of tools available to test RESTful services, such as Postman, \nAdvanced REST client, SOAP UI, Paw, and so on.\nIn this example, to test the service, the default test class generated by Spring Boot \nwill be used.\nAdding a new test case to ApplicatonTests.java results in:\n@RunWith(SpringJUnit4ClassRunner.class)\n@SpringApplicationConfiguration(classes = Application.class)\n@WebIntegrationTest\npublic class ApplicationTests {\n  @Test\n  public void testVanillaService() {\n    RestTemplate restTemplate = new RestTemplate();\n    Greet greet = restTemplate.getForObject \n      (\"http://localhost:8080\", Greet.class);\n    Assert.assertEquals(\"Hello World!\", greet.getMessage());\n  }\n}\nNote that @WebIntegrationTest is added and @WebAppConfiguration removed \nat the class level. The @WebIntegrationTest annotation is a handy annotation that \nensures that the tests are fired against a fully up-and-running server. Alternately,  \na combination of @WebAppConfiguration and @IntegrationTest will give the  \nsame result.\n",
      "content_length": 1754,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "Building Microservices with Spring Boot\n[ 68 ]\nAlso note that RestTemplate is used to call the RESTful service. RestTemplate is a \nutility class that abstracts the lower-level details of the HTTP client.\nTo test this, one can open a terminal window, go to the project folder, and run  \nmvn install.\nDeveloping the Spring Boot microservice \nusing Spring Initializr – the HATEOAS \nexample\nIn the next example, Spring Initializr will be used to create a Spring Boot project. \nSpring Initializr is a drop-in replacement for the STS project wizard and provides \na web UI to configure and generate a Spring Boot project. One of the advantages of \nSpring Initializr is that it can generate a project through the website that then can be \nimported into any IDE.\nIn this example, the concept of HATEOAS (short for Hypertext As The Engine Of \nApplication State) for REST-based services and the HAL (Hypertext Application \nLanguage) browser will be examined.\nHATEOAS is a REST service pattern in which navigation links are provided as part \nof the payload metadata. The client application determines the state and follows the \ntransition URLs provided as part of the state. This methodology is particularly useful \nin responsive mobile and web applications in which the client downloads additional \ndata based on user navigation patterns.\nThe HAL browser is a handy API browser for hal+json data. HAL is a format based \non JSON that establishes conventions to represent hyperlinks between resources. \nHAL helps APIs be more explorable and discoverable.\nThe full source code of this example is available as the  \nchapter2.boothateoas project in the code files of this book.\n",
      "content_length": 1662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "Chapter 2\n[ 69 ]\nHere are the concrete steps to develop a HATEOAS sample using Spring Initilizr:\n1.\t In order to use Spring Initilizr, go to https://start.spring.io:\n2.\t Fill the details, such as whether it is a Maven project, Spring Boot version, \ngroup, and artifact ID, as shown earlier, and click on Switch to the full \nversion link under the Generate Project button. Select Web, HATEOAS,  \nand Rest Repositories HAL Browser. Make sure that the Java version is 8 \nand the package type is selected as JAR:\n",
      "content_length": 509,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "Building Microservices with Spring Boot\n[ 70 ]\n3.\t Once selected, hit the Generate Project button. This will generate a Maven \nproject and download the project as a ZIP file into the download directory of \nthe browser.\n4.\t Unzip the file and save it to a directory of your choice.\n5.\t Open STS, go to the File menu and click on Import:\n6.\t Navigate to Maven | Existing Maven Projects and click on Next.\n7.\t Click on Browse next to Root Directory and select the unzipped folder.  \nClick on Finish. This will load the generated Maven project into STS'  \nProject Explorer.\n",
      "content_length": 570,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "Chapter 2\n[ 71 ]\n8.\t Edit the Application.java file to add a new REST endpoint, as follows:\n@RequestMapping(\"/greeting\")\n@ResponseBody\npublic HttpEntity<Greet> greeting(@RequestParam(value = \"name\", \nrequired = false, defaultValue = \"HATEOAS\") String name) {\n       Greet greet = new Greet(\"Hello \" + name);\n       greet.add(linkTo(methodOn(GreetingController. \n         class).greeting(name)).withSelfRel());\n       return new ResponseEntity<Greet>(greet,  \n         HttpStatus.OK);\n}\n9.\t Note that this is the same GreetingController class as in the previous \nexample. However, a method was added this time named greeting. In \nthis new method, an additional optional request parameter is defined and \ndefaulted to HATEOAS. The following code adds a link to the resulting JSON \ncode. In this case, it adds the link to the same API:\ngreet.add(linkTo(methodOn(GreetingController.class).\ngreeting(name)).withSelfRel());\nIn order to do this, we need to extend the Greet class from \nResourceSupport, as shown here. The rest of the code remains the same:\nclass Greet extends ResourceSupport{\n10.\t The add method is a method in ResourceSupport. The linkTo and \nmethodOn methods are static methods of ControllerLinkBuilder, a utility \nclass for creating links on controller classes. The methodOn method will do \na dummy method invocation, and linkTo will create a link to the controller \nclass. In this case, we will use withSelfRel to point it to itself.\n11.\t This will essentially produce a link, /greeting?name=HATEOAS, by default.  \nA client can read the link and initiate another call.\n12.\t Run this as a Spring Boot app. Once the server startup is complete, point the \nbrowser to http://localhost:8080.\n",
      "content_length": 1702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "Building Microservices with Spring Boot\n[ 72 ]\n13.\t This will open the HAL browser window. In the Explorer field,  \ntype /greeting?name=World! and click on the Go button. If everything \nis fine, the HAL browser will show the response details as shown in the \nfollowing screenshot:\nAs shown in the screenshot, the Response Body section has the result with a link \nwith href pointing back to the same service. This is because we pointed the reference \nto itself. Also, review the Links section. The little green box against self is the \nnavigable link.\nIt does not make much sense in this simple example, but this could be handy in \nlarger applications with many related entities. Using the links provided, the client \ncan easily navigate back and forth between these entities with ease.\nWhat's next?\nA number of basic Spring Boot examples have been reviewed so far. The rest of this \nchapter will examine some of the Spring Boot features that are important from a \nmicroservices development perspective. In the upcoming sections, we will take a \nlook at how to work with dynamically configurable properties, change the default \nembedded web server, add security to the microservices, and implement cross-origin \nbehavior when dealing with microservices.\nThe full source code of this example is available as the  \nchapter2.boot-advanced project in the code files of this book.\n",
      "content_length": 1375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "Chapter 2\n[ 73 ]\nThe Spring Boot configuration\nIn this section, the focus will be on the configuration aspects of Spring Boot. The \nchapter2.bootrest project, already developed, will be modified in this section \nto showcase configuration capabilities. Copy and paste chapter2.bootrest and \nrename the project as chapter2.boot-advanced.\nUnderstanding the Spring Boot \nautoconfiguration\nSpring Boot uses convention over configuration by scanning the dependent \nlibraries available in the class path. For each spring-boot-starter-* dependency \nin the POM file, Spring Boot executes a default AutoConfiguration class. \nAutoConfiguration classes use the *AutoConfiguration lexical pattern, where * \nrepresents the library. For example, the autoconfiguration of JPA repositories is done \nthrough JpaRepositoriesAutoConfiguration.\nRun the application with --debug to see the autoconfiguration report. The following \ncommand shows the autoconfiguration report for the chapter2.boot-advanced \nproject:\n$java -jar target/bootadvanced-0.0.1-SNAPSHOT.jar --debug\nHere are some examples of the autoconfiguration classes:\n•\t\nServerPropertiesAutoConfiguration\n•\t\nRepositoryRestMvcAutoConfiguration\n•\t\nJpaRepositoriesAutoConfiguration\n•\t\nJmsAutoConfiguration\nIt is possible to exclude the autoconfiguration of certain libraries if the application \nhas special requirements and you want to get full control of the configurations.  \nThe following is an example of excluding DataSourceAutoConfiguration:\n@EnableAutoConfiguration(exclude={DataSourceAutoConfiguration.class})\nwww.allitebooks.com\n",
      "content_length": 1575,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "Building Microservices with Spring Boot\n[ 74 ]\nOverriding default configuration values\nIt is also possible to override default configuration values using the application.\nproperties file. STS provides an easy-to-autocomplete, contextual help on \napplication.properties, as shown in the following screenshot:\nIn the preceding screenshot, server.port is edited to be set as 9090. Running this \napplication again will start the server on port 9090.\nChanging the location of the configuration file\nIn order to align with the Twelve-Factor app, configuration parameters need to \nbe externalized from the code. Spring Boot externalizes all configurations into \napplication.properties. However, it is still part of the application's build. \nFurthermore, properties can be read from outside the package by setting the \nfollowing properties: \nspring.config.name= # config file name  \nspring.config.location= # location of config file\nHere, spring.config.location could be a local file location.\nThe following command starts the Spring Boot application with an externally \nprovided configuration file:\n$java -jar target/bootadvanced-0.0.1-SNAPSHOT.jar --spring.config.\nname=bootrest.properties\n",
      "content_length": 1184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "Chapter 2\n[ 75 ]\nReading custom properties\nAt startup, SpringApplication loads all the properties and adds them to the Spring \nEnvironment class. Add a custom property to the application.properties file. \nIn this case, the custom property is named bootrest.customproperty. Autowire \nthe Spring Environment class into the GreetingController class. Edit the \nGreetingController class to read the custom property from Environment  \nand add a log statement to print the custom property to the console.\nPerform the following steps to do this:\n1.\t Add the following property to the application.properties file:\nbootrest.customproperty=hello\n2.\t Then, edit the GreetingController class as follows:\n@Autowired\nEnvironment env;\nGreet greet(){\n    logger.info(\"bootrest.customproperty \"+  \n      env.getProperty(\"bootrest.customproperty\"));\n    return new Greet(\"Hello World!\");\n}\n3.\t Rerun the application. The log statement prints the custom variable in the \nconsole, as follows:\norg.rvslab.chapter2.GreetingController   : bootrest.customproperty \nhello\nUsing a .yaml file for configuration\nAs an alternate to application.properties, one may use a .yaml file. YAML \nprovides a JSON-like structured configuration compared to the flat properties file.\nTo see this in action, simply replace application.properties with application.\nyaml and add the following property:\nserver\n  port: 9080\nRerun the application to see the port printed in the console.\n",
      "content_length": 1440,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "Building Microservices with Spring Boot\n[ 76 ]\nUsing multiple configuration profiles\nFurthermore, it is possible to have different profiles such as development, testing, \nstaging, production, and so on. These are logical names. Using these, one can \nconfigure different values for the same properties for different environments. \nThis is quite handy when running the Spring Boot application against different \nenvironments. In such cases, there is no rebuild required when moving from one \nenvironment to another.\nUpdate the .yaml file as follows. The Spring Boot group profiles properties based on \nthe dotted separator:\nspring:\n    profiles: development\nserver:\n      port: 9090\n---\nspring:\n    profiles: production\nserver:\n      port: 8080\nRun the Spring Boot application as follows to see the use of profiles:\nmvn -Dspring.profiles.active=production install\nmvn -Dspring.profiles.active=development install\nActive profiles can be specified programmatically using the @ActiveProfiles \nannotation, which is especially useful when running test cases, as follows:\n@ActiveProfiles(\"test\")\nOther options to read properties\nThe properties can be loaded in a number of ways, such as the following:\n•\t\nCommand-line parameters (-Dhost.port =9090)\n•\t\nOperating system environment variables\n•\t\nJNDI (java:comp/env)\n",
      "content_length": 1307,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "Chapter 2\n[ 77 ]\nChanging the default embedded web \nserver\nEmbedded HTTP listeners can easily be customized as follows. By default, Spring \nBoot supports Tomcat, Jetty, and Undertow. In the following example, Tomcat is \nreplaced with Undertow:\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-web</artifactId>\n    <exclusions>\n        <exclusion>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-tomcat</artifactId>\n        </exclusion>\n    </exclusions>\n</dependency>\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-undertow</artifactId>\n</dependency>\nImplementing Spring Boot security\nIt is important to secure microservices. In this section, some basic measures to \nsecure Spring Boot microservices will be reviewed using chapter2.bootrest to \ndemonstrate the security features.\nSecuring microservices with basic security\nAdding basic authentication to Spring Boot is pretty simple. Add the following \ndependency to pom.xml. This will include the necessary Spring security library files:\n<dependency>\n  <groupId>org.springframework.boot</groupId> \n  <artifactId>spring-boot-starter-security</artifactId>\n</dependency>\n",
      "content_length": 1269,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "Building Microservices with Spring Boot\n[ 78 ]\nOpen Application.java and add @EnableGlobalMethodSecurity to the \nApplication class. This annotation will enable method-level security:\n@EnableGlobalMethodSecurity\n@SpringBootApplication\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n}\nThe default basic authentication assumes the user as being user. The default \npassword will be printed in the console at startup. Alternately, the username  \nand password can be added in application.properties, as shown here:\nsecurity.user.name=guest\nsecurity.user.password=guest123\nAdd a new test case in ApplicationTests to test the secure service results,  \nas in the following:\n  @Test\n  public void testSecureService() {  \n    String plainCreds = \"guest:guest123\";\n    HttpHeaders headers = new HttpHeaders();\n    headers.add(\"Authorization\", \"Basic \" + new String(Base64.\nencode(plainCreds.getBytes())));\n    HttpEntity<String> request = new HttpEntity<String>(headers);\n    RestTemplate restTemplate = new RestTemplate();\n    \n    ResponseEntity<Greet> response = restTemplate.exchange(\"http://\nlocalhost:8080\", HttpMethod.GET, request, Greet.class);\n    Assert.assertEquals(\"Hello World!\", response.getBody().\ngetMessage());\n  }\nAs shown in the code, a new Authorization request header with Base64 encoding \nthe username-password string is created.\nRerun the application using Maven. Note that the new test case passed, but the old \ntest case failed with an exception. The earlier test case now runs without credentials, \nand as a result, the server rejected the request with the following message:\norg.springframework.web.client.HttpClientErrorException: 401 Unauthorized\n",
      "content_length": 1751,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "Chapter 2\n[ 79 ]\nSecuring a microservice with OAuth2\nIn this section, we will take a look at the basic Spring Boot configuration for OAuth2. \nWhen a client application requires access to a protected resource, the client sends \na request to an authorization server. The authorization server validates the request \nand provides an access token. This access token is validated for every client-to-server \nrequest. The request and response sent back and forth depends on the grant type.\nRead more about OAuth and grant types at http://oauth.net.\nThe resource owner password credentials grant approach will be used in this \nexample:\nIn this case, as shown in the preceding diagram, the resource owner provides the \nclient with a username and password. The client then sends a token request to the \nauthorization server by providing the credential information. The authorization \nserver authorizes the client and returns with an access token. On every subsequent \nrequest, the server validates the client token.\n",
      "content_length": 1006,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "Building Microservices with Spring Boot\n[ 80 ]\nTo implement OAuth2 in our example, perform the following steps:\n1.\t As a first step, update pom.xml with the OAuth2 dependency, as follows:\n<dependency>\n  <groupId>org.springframework.security.oauth</groupId>\n  <artifactId>spring-security-oauth2</artifactId>\n  <version>2.0.9.RELEASE</version>\n</dependency>\n2.\t Next, add two new annotations, @EnableAuthorizationServer  \nand @EnableResourceServer, to the Application.java file. The  \n@EnableAuthorizationServer annotation creates an authorization server \nwith an in-memory repository to store client tokens and provide clients with \na username, password, client ID, and secret. The @EnableResourceServer \nannotation is used to access the tokens. This enables a spring security filter \nthat is authenticated via an incoming OAuth2 token.\nIn our example, both the authorization server and resource server are the \nsame. However, in practice, these two will run separately. Take a look at the \nfollowing code:\n@EnableResourceServer\n@EnableAuthorizationServer\n@SpringBootApplication\npublic class Application {\n3.\t Add the following properties to the application.properties file:\nsecurity.user.name=guest\nsecurity.user.password=guest123\nsecurity.oauth2.client.clientId: trustedclient\nsecurity.oauth2.client.clientSecret: trustedclient123\nsecurity.oauth2.client.authorized-grant-types: authorization_\ncode,refresh_token,password\nsecurity.oauth2.client.scope: openid\n",
      "content_length": 1459,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "Chapter 2\n[ 81 ]\n4.\t Then, add another test case to test OAuth2, as follows:\n  @Test\n  public void testOAuthService() {\n        ResourceOwnerPasswordResourceDetails resource = new \nResourceOwnerPasswordResourceDetails();\n        resource.setUsername(\"guest\");\n        resource.setPassword(\"guest123\");\n          resource.setAccessTokenUri(\"http://localhost:8080/oauth/\ntoken\");\n        resource.setClientId(\"trustedclient\");\n        resource.setClientSecret(\"trustedclient123\");\n        resource.setGrantType(\"password\");\n  \n        DefaultOAuth2ClientContext clientContext = new \nDefaultOAuth2ClientContext();\n        OAuth2RestTemplate restTemplate = new \nOAuth2RestTemplate(resource, clientContext);\n \n        Greet greet = restTemplate.getForObject(\"http://\nlocalhost:8080\", Greet.class);\n        Assert.assertEquals(\"Hello World!\", greet.getMessage());\n  }\nAs shown in the preceding code, a special REST template, \nOAuth2RestTemplate, is created by passing the resource details \nencapsulated in a resource details object. This REST template handles the \nOAuth2 processes underneath. The access token URI is the endpoint for  \nthe token access.\n5.\t Rerun the application using mvn install. The first two test cases will \nfail, and the new one will succeed. This is because the server only accepts \nOAuth2-enabled requests.\nThese are quick configurations provided by Spring Boot out of the box but \nare not good enough to be production grade. We may need to customize \nResourceServerConfigurer and AuthorizationServerConfigurer to make  \nthem production-ready. This notwithstanding, the approach remains the same.\n",
      "content_length": 1617,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "Building Microservices with Spring Boot\n[ 82 ]\nEnabling cross-origin access for \nmicroservices\nBrowsers are generally restricted when client-side web applications running from \none origin request data from another origin. Enabling cross-origin access is generally \ntermed as CORS (Cross-Origin Resource Sharing).\nThis example shows how to enable cross-origin requests. With microservices, as each \nservice runs with its own origin, it will easily get into the issue of a client-side web \napplication consuming data from multiple origins. For instance, a scenario where \na browser client accessing Customer from the Customer microservice and Order \nHistory from the Order microservices is very common in the microservices world.\nSpring Boot provides a simple declarative approach to enabling cross-origin \nrequests. The following example shows how to enable a microservice to enable cross-\norigin requests:\n@RestController\nclass GreetingController{\n  @CrossOrigin\n  @RequestMapping(\"/\")\n  Greet greet(){\n    return new Greet(\"Hello World!\");\n  }\n}\n",
      "content_length": 1047,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "Chapter 2\n[ 83 ]\nBy default, all the origins and headers are accepted. We can further customize  \nthe cross-origin annotations by giving access to specific origins, as follows. The  \n@CrossOrigin annotation enables a method or class to accept cross-origin requests:\n@CrossOrigin(\"http://mytrustedorigin.com\")\nGlobal CORS can be enabled using the WebMvcConfigurer bean and customizing  \nthe addCorsMappings(CorsRegistry registry) method.\nImplementing Spring Boot messaging\nIn an ideal case, all microservice interactions are expected to happen asynchronously \nusing publish-subscribe semantics. Spring Boot provides a hassle-free mechanism to \nconfigure messaging solutions:\nIn this example, we will create a Spring Boot application with a sender and receiver, \nboth connected though an external queue. Perform the following steps:\nThe full source code of this example is available as the  \nchapter2.bootmessaging project in the code files of this book.\n",
      "content_length": 953,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "Building Microservices with Spring Boot\n[ 84 ]\n1.\t Create a new project using STS to demonstrate this capability. In this \nexample, instead of selecting Web, select AMQP under I/O:\n2.\t Rabbit MQ will also be needed for this example. Download and install the \nlatest version of Rabbit MQ from https://www.rabbitmq.com/download.\nhtml.\nRabbit MQ 3.5.6 is used in this book.\n3.\t Follow the installation steps documented on the site. Once ready, start the \nRabbitMQ server via the following command:\n$./rabbitmq-server\n4.\t Make the configuration changes to the application.properties file to \nreflect the RabbitMQ configuration. The following configuration uses the \ndefault port, username, and password of RabbitMQ:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\n",
      "content_length": 831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "Chapter 2\n[ 85 ]\n5.\t Add a message sender component and a queue named TestQ of the org.\nspringframework.amqp.core.Queue type to the Application.java file \nunder src/main/java. RabbitMessagingTemplate is a convenient way to \nsend messages, which will abstract all the messaging semantics. Spring Boot \nprovides all boilerplate configurations to send messages:\n@Component \nclass Sender {\n  @Autowired\n  RabbitMessagingTemplate template;\n  @Bean\n  Queue queue() {\n    return new Queue(\"TestQ\", false);\n  }\n  public void send(String message){\n    template.convertAndSend(\"TestQ\", message);\n  }\n}\n6.\t To receive the message, all that needs to be used is a @RabbitListener \nannotation. Spring Boot autoconfigures all the required boilerplate \nconfigurations:\n@Component\nclass Receiver {\n    @RabbitListener(queues = \"TestQ\")\n    public void processMessage(String content) {\n       System.out.println(content);\n    }\n}\n7.\t The last piece of this exercise is to wire the sender to our main application \nand implement the run method of CommandLineRunner to initiate the \nmessage sending. When the application is initialized, it invokes the run \nmethod of CommandLineRunner, as follows:\n@SpringBootApplication\npublic class Application implements CommandLineRunner{\n  @Autowired\n  Sender sender;\n  \n    public static void main(String[] args) {\n",
      "content_length": 1333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "Building Microservices with Spring Boot\n[ 86 ]\n        SpringApplication.run(Application.class, args);\n    }\n    \n    @Override\n    public void run(String... args) throws Exception {\n      sender.send(\"Hello Messaging..!!!\");\n    }\n}\n8.\t Run the application as a Spring Boot application and verify the output.  \nThe following message will be printed in the console:\nHello Messaging..!!!\nDeveloping a comprehensive \nmicroservice example\nSo far, the examples we have considered are no more than just a simple \"Hello \nworld.\" Putting together what we have learned, this section demonstrates an \nend-to-end Customer Profile microservice implementation. The Customer Profile \nmicroservices will demonstrate interaction between different microservices. It also \ndemonstrates microservices with business logic and primitive data stores.\nIn this example, two microservices, the Customer Profile and Customer Notification \nservices, will be developed:\nAs shown in the diagram, the Customer Profile microservice exposes methods to \ncreate, read, update, and delete (CRUD) a customer and a registration service to \nregister a customer. The registration process applies certain business logic, saves the \ncustomer profile, and sends a message to the Customer Notification microservice. \nThe Customer Notification microservice accepts the message sent by the registration \nservice and sends an e-mail message to the customer using an SMTP server. \nAsynchronous messaging is used to integrate Customer Profile with the Customer \nNotification service.\n",
      "content_length": 1537,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "Chapter 2\n[ 87 ]\nThe Customer microservices class domain model diagram is as shown here:\nCustomerController in the diagram is the REST endpoint, which invokes a \ncomponent class, CustomerComponent. The component class/bean handles all the \nbusiness logic. CustomerRepository is a Spring data JPA repository defined to \nhandle the persistence of the Customer entity.\nThe full source code of this example is available as the  \nchapter2.bootcustomer and chapter2.\nbootcustomernotification projects in the code files of this book.\n1.\t Create a new Spring Boot project and call it chapter2.bootcustomer, the \nsame way as earlier. Select the options as in the following screenshot in the \nstarter module selection screen:\n",
      "content_length": 716,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "Building Microservices with Spring Boot\n[ 88 ]\nThis will create a web project with JPA, the REST repository, and H2  \nas a database. H2 is a tiny in-memory embedded database with which it is \neasy to demonstrate database features. In the real world, it is recommended \nto use an appropriate enterprise-grade database. This example uses JPA to \ndefine persistence entities and the REST repository to expose REST-based \nrepository services.\nThe project structure will be similar to the following screenshot:\n2.\t Start building the application by adding an Entity class named Customer. For \nsimplicity, there are only three fields added to the Customer Entity class: the \nautogenerated id field, name, and email. Take a look at the following code:\n@Entity\nclass Customer {\n  @Id\n  @GeneratedValue(strategy = GenerationType.AUTO)\n  private Long id;\n  private String name;\n  private String email;\n3.\t Add a repository class to handle the persistence handling of Customer. \nCustomerRepository extends the standard JPA repository. This means \nthat all CRUD methods and default finder methods are automatically \nimplemented by the Spring Data JPA repository, as follows:\n@RepositoryRestResource\ninterface CustomerRespository extends JpaRepository \n<Customer,Long>{\n  Optional<Customer> findByName(@Param(\"name\") String name);\n}\n",
      "content_length": 1320,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "Chapter 2\n[ 89 ]\nIn this example, we added a new method to the repository class, findByName, \nwhich essentially searches the customer based on the customer name and \nreturns a Customer object if there is a matching name.\n4.\t The @RepositoryRestResource annotation enables the repository  \naccess through RESTful services. This will also enable HATEOAS and HAL \nby default. As for CRUD methods there is no additional business logic \nrequired, we will leave it as it is without controller or component classes. \nUsing HATEOAS will help us navigate through Customer Repository \nmethods effortlessly.\nNote that there is no configuration added anywhere to point to any database. \nAs H2 libraries are in the class path, all the configuration is done by default \nby Spring Boot based on the H2 autoconfiguration.\n5.\t Update the Application.java file by adding CommandLineRunner to \ninitialize the repository with some customer records, as follows:\n@SpringBootApplication\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n    @Bean\n  CommandLineRunner init(CustomerRespository repo) {\n  return (evt) ->  {\n    repo.save(new Customer(\"Adam\",\"adam@boot.com\"));\n    repo.save(new Customer(\"John\",\"john@boot.com\"));\n  repo.save(new Customer(\"Smith\",\"smith@boot.com\"));\n    repo.save(new Customer(\"Edgar\",\"edgar@boot.com\"));\n    repo.save(new Customer(\"Martin\",\"martin@boot.com\"));\n    repo.save(new Customer(\"Tom\",\"tom@boot.com\"));\n    repo.save(new Customer(\"Sean\",\"sean@boot.com\"));\n  };\n  }\n}\n6.\t CommandLineRunner, defined as a bean, indicates that it should run when \nit is contained in SpringApplication. This will insert six sample customer \nrecords into the database at startup.\n7.\t At this point, run the application as Spring Boot App. Open the HAL \nbrowser and point the browser to http://localhost:8080.\n",
      "content_length": 1888,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "Building Microservices with Spring Boot\n[ 90 ]\n8.\t In the Explorer section, point to http://localhost:8080/customers and \nclick on Go. This will list all the customers in the Response Body section of \nthe HAL browser.\n9.\t In the Explorer section, enter http://localhost:8080/customers?size=2\n&page=1&sort=name and click on Go. This will automatically execute paging \nand sorting on the repository and return the result.\nAs the page size is set to 2 and the first page is requested, it will come back \nwith two records in a sorted order.\n10.\t Review the Links section. As shown in the following screenshot, it will \nfacilitate navigating first, next, prev, and last. These are done using the \nHATEOAS links automatically generated by the repository browser:\n11.\t Also, one can explore the details of a customer by selecting the appropriate \nlink, such as http://localhost:8080/customers/2.\n",
      "content_length": 889,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "Chapter 2\n[ 91 ]\n12.\t As the next step, add a controller class, CustomerController, to handle \nservice endpoints. There is only one endpoint in this class, /register, which \nis used to register a customer. If successful, it returns the Customer object as \nthe response, as follows:\n@RestController\nclass CustomerController{\n  \n  @Autowired\n  CustomerRegistrar customerRegistrar;\n  \n  @RequestMapping( path=\"/register\", method = RequestMethod.POST)\n  Customer register(@RequestBody Customer customer){\n    return customerRegistrar.register(customer);\n  }\n}\n13.\t A CustomerRegistrar component is added to handle the business logic. \nIn this case, there is only minimal business logic added to the component. \nIn this component class, while registering a customer, we will just check \nwhether the customer name already exists in the database or not. If it does \nnot exist, then we will insert a new record, and otherwise, we will send an \nerror message back, as follows:\n@Component \nclass CustomerRegistrar {\n  \n  CustomerRespository customerRespository;\n  \n  @Autowired\n  CustomerRegistrar(CustomerRespository customerRespository){\n    this.customerRespository = customerRespository;\n  }\n  \n  Customer register(Customer customer){\n    Optional<Customer> existingCustomer = customerRespository.\nfindByName(customer.getName());\n    if (existingCustomer.isPresent()){\n      throw new RuntimeException(\"is already exists\");\n    } else {\n      customerRespository.save(customer); \n    }\n    return customer;\n  }\n}\n",
      "content_length": 1507,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "Building Microservices with Spring Boot\n[ 92 ]\n14.\t Restart the Boot application and test using the HAL browser via the URL \nhttp://localhost:8080.\n15.\t Point the Explorer field to http://localhost:8080/customers. Review the \nresults in the Links section:\n16.\t Click on the NON-GET option against self. This will open a form to create a \nnew customer:\n17.\t Fill the form and change the Action as shown in the diagram. Click on the \nMake Request button. This will call the register service and register the \ncustomer. Try giving a duplicate name to test the negative case.\n",
      "content_length": 572,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "Chapter 2\n[ 93 ]\n18.\t Let's complete the last part in the example by integrating the Customer \nNotification service to notify the customer. When registration is successful, \nsend an e-mail to the customer by asynchronously calling the Customer \nNotification microservice.\n19.\t First update CustomerRegistrar to call the second service. This is done \nthrough messaging. In this case, we injected a Sender component to send a \nnotification to the customer by passing the customer's e-mail address to the \nsender, as follows:\n@Component \n@Lazy\nclass CustomerRegistrar {\n  \n  CustomerRespository customerRespository;\n  Sender sender;\n  \n  @Autowired\n  CustomerRegistrar(CustomerRespository customerRespository, \nSender sender){\n    this.customerRespository = customerRespository;\n    this.sender = sender;\n  }\n  \n  Customer register(Customer customer){\n    Optional<Customer> existingCustomer = customerRespository.\nfindByName(customer.getName());\n    if (existingCustomer.isPresent()){\n      throw new RuntimeException(\"is already exists\");\n    } else {\n      customerRespository.save(customer); \n      sender.send(customer.getEmail());\n    } \n    return customer;\n  }\n} \n20.\t The sender component will be based on RabbitMQ and AMQP. In this \nexample, RabbitMessagingTemplate is used as explored in the last \nmessaging example; take a look at the following:\n@Component \n@Lazy\nclass Sender {\n  \n  @Autowired\n",
      "content_length": 1404,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "Building Microservices with Spring Boot\n[ 94 ]\n  RabbitMessagingTemplate template;\n  @Bean\n  Queue queue() {\n    return new Queue(\"CustomerQ\", false);\n  }\n  \n  public void send(String message){\n    template.convertAndSend(\"CustomerQ\", message);\n  }\n}\nThe @Lazy annotation is a useful one and it helps to increase the boot startup \ntime. These beans will be initialized only when the need arises.\n21.\t We will also update the application.property file to include Rabbit MQ-\nrelated properties, as follows:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\n22.\t We are ready to send the message. To consume the message and send \ne-mails, we will create a notification service. For this, let's create another \nSpring Boot service, chapter2.bootcustomernotification. Make sure that \nthe AMQP and Mail starter libraries are selected when creating the Spring \nBoot service. Both AMQP and Mail are under I/O.\n23.\t The package structure of the chapter2.bootcustomernotification project \nis as shown here:\n",
      "content_length": 1066,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "Chapter 2\n[ 95 ]\n24.\t Add a Receiver class. The Receiver class waits for a message on customer. \nThis will receive a message sent by the Customer Profile service. On the \narrival of a message, it sends an e-mail, as follows:\n@Component\nclass Receiver {  \n  @Autowired\n  Mailer mailer;\n  \n  @Bean\n  Queue queue() {\n    return new Queue(\"CustomerQ\", false);\n  }\n  @RabbitListener(queues = \"CustomerQ\")\n    public void processMessage(String email) {\n       System.out.println(email);\n       mailer.sendMail(email);\n    }\n}\n25.\t Add another component to send an e-mail to the customer. We will use \nJavaMailSender to send an e-mail via the following code:\n@Component \nclass Mailer {\n  @Autowired\n  private  JavaMailSender  javaMailService;\n    public void sendMail(String email){\n      SimpleMailMessage mailMessage=new  \n        SimpleMailMessage();\n      mailMessage.setTo(email);\n      mailMessage.setSubject(\"Registration\");\n      mailMessage.setText(\"Successfully Registered\");\n      javaMailService.send(mailMessage);\n    }\n}\nBehind the scenes, Spring Boot automatically configures all the parameters \nrequired by JavaMailSender.\n26.\t To test SMTP, a test setup for SMTP is required to ensure that the mails \nare going out. In this example, FakeSMTP will be used. You can download \nFakeSMTP from http://nilhcem.github.io/FakeSMTP.\n",
      "content_length": 1333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "Building Microservices with Spring Boot\n[ 96 ]\n27.\t Once you download fakeSMTP-2.0.jar, run the SMTP server by executing \nthe following command:\n$ java -jar fakeSMTP-2.0.jar\nThis will open a GUI to monitor e-mail messages. Click on the Start Server \nbutton next to the listening port textbox.\n28.\t Update application.properties with the following configuration \nparameters to connect to RabbitMQ as well as to the mail server:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\nspring.mail.host=localhost\nspring.mail.port=2525\n29.\t We are ready to test our microservices end to end. Start both the Spring Boot \napps. Open the browser and repeat the customer creation steps through the \nHAL browser. In this case, immediately after submitting the request, we will \nbe able to see the e-mail in the SMTP GUI.\nInternally, the Customer Profile service asynchronously calls the  \nCustomer Notification service, which, in turn, sends the e-mail message  \nto the SMTP server:\n",
      "content_length": 1037,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "Chapter 2\n[ 97 ]\nSpring Boot actuators\nThe previous sections explored most of the Spring Boot features required to develop \na microservice. In this section, some of the production-ready operational aspects of \nSpring Boot will be explored.\nSpring Boot actuators provide an excellent out-of-the-box mechanism to monitor  \nand manage Spring Boot applications in production:\nThe full source code of this example is available as the  \nchapter2.bootactuator project in the code files of this book.\n1.\t Create another Spring Starter Project and name it chapter2.bootactuator. \nThis time, select Web and Actuators under Ops. Similar to the chapter2.\nbootrest project, add a GreeterController endpoint with the  \ngreet method.\n2.\t Start the application as Spring Boot app.\n3.\t Point the browser to localhost:8080/actuator. This will open the HAL \nbrowser. Then, review the Links section.\nA number of links are available under the Links section. These are \nautomatically exposed by the Spring Boot actuator:\n",
      "content_length": 999,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "Building Microservices with Spring Boot\n[ 98 ]\nSome of the important links are listed as follows:\n•\t\ndump: This performs a thread dump and displays the result\n•\t\nmappings: This lists all the HTTP request mappings\n•\t\ninfo: This displays information about the application\n•\t\nhealth: This displays the application's health conditions\n•\t\nautoconfig: This displays the autoconfiguration report\n•\t\nmetrics: This shows different metrics collected from the application\nMonitoring using JConsole\nAlternately, we can use the JMX console to see the Spring Boot information. Connect \nto the remote Spring Boot instance from JConsole. The Boot information will be \nshown as follows:\n",
      "content_length": 670,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "Chapter 2\n[ 99 ]\nMonitoring using SSH\nSpring Boot provides remote access to the Boot application using SSH. The following \ncommand connects to the Spring Boot application from a terminal window:\n$ ssh -p 2000 user@localhost\nThe password can be customized by adding the shell.auth.simple.user.\npassword property in the application.properties file. The updated \napplication.properties file will look similar to the following:\nshell.auth.simple.user.password=admin\nWhen connected with the preceding command, similar actuator information can be \naccessed. Here is an example of the metrics information accessed through the CLI:\n•\t\nhelp: This lists out all the options available\n•\t\ndashboard: This is one interesting feature that shows a lot of system-level \ninformation\nConfiguring application information\nThe following properties can be set in application.properties to customize \napplication-related information. After adding, restart the server and visit the /info \nendpoint of the actuator to take a look at the updated information, as follows:\ninfo.app.name=Boot actuator\ninfo.app.description= My Greetings Service\ninfo.app.version=1.0.0\nAdding a custom health module\nAdding a new custom module to the Spring Boot application is not so complex.  \nTo demonstrate this feature, assume that if a service gets more than two transactions \nin a minute, then the server status will be set as Out of Service.\nIn order to customize this, we have to implement the HealthIndicator interface \nand override the health method. The following is a quick and dirty implementation \nto do the job:\nclass TPSCounter {\n  LongAdder count;\n  int threshold = 2;\n",
      "content_length": 1639,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "Building Microservices with Spring Boot\n[ 100 ]\n  Calendar expiry = null; \n  TPSCounter(){\n    this.count = new LongAdder();\n    this.expiry = Calendar.getInstance();\n    this.expiry.add(Calendar.MINUTE, 1);\n  }\n  \n  boolean isExpired(){\n    return Calendar.getInstance().after(expiry);\n  }\n  \n  boolean isWeak(){\n    return (count.intValue() > threshold);\n  }\n  \n  void increment(){\n     count.increment();\n  }\n}\nThe preceding class is a simple POJO class that maintains the transaction counts in the \nwindow. The isWeak method checks whether the transaction in a particular window \nreached its threshold. The isExpired method checks whether the current window is \nexpired or not. The increment method simply increases the counter value.\nFor the next step, implement our custom health indicator class, TPSHealth. This is \ndone by extending HealthIndicator, as follows:\n@Component\nclass TPSHealth implements HealthIndicator {\n  TPSCounter counter;\n@Override\n    public Health health() {\n        boolean health = counter.isWeak(); // perform some specific \nhealth check\n        if (health) {\n            return Health.outOfService().withDetail(\"Too many \nrequests\", \"OutofService\").build();\n        }\n        return Health.up().build();\n    }\n    \n",
      "content_length": 1247,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "Chapter 2\n[ 101 ]\n    void updateTx(){\n    if(counter == null || counter.isExpired()){\n      counter = new TPSCounter();\n      \n    }\n    counter.increment();\n    }\n}\nThe health method checks whether the counter is weak or not. A weak counter \nmeans the service is handling more transactions than it can handle. If it is weak,  \nit marks the instance as Out of Service.\nFinally, we will autowire TPSHealth into the GreetingController class and then \ncall health.updateTx() in the greet method, as follows:\n  Greet greet(){\n    logger.info(\"Serving Request....!!!\");\n    health.updateTx(); \n    return new Greet(\"Hello World!\");\n  }\nGo to the /health end point in the HAL browser and take a look at the status  \nof the server.\nNow, open another browser, point to http://localhost:8080, and fire the  \nservice twice or thrice. Go back to the /health endpoint and refresh to see the  \nstatus. It should be changed to Out of Service.\nIn this example, as there is no action taken other than collecting the health status, \neven though the status is Out of Service, new service calls will still go through. \nHowever, in the real world, a program should read the /health endpoint and block \nfurther requests from going to this instance.\nBuilding custom metrics\nSimilar to health, customization of the metrics is also possible. The following example \nshows how to add counter service and gauge service, just for demonstration purposes:\n  @Autowired   \n  CounterService counterService;\n  @Autowired\n  GaugeService gaugeService;\n",
      "content_length": 1518,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "Building Microservices with Spring Boot\n[ 102 ]\nAdd the following methods in the greet method:\n  this.counterService.increment(\"greet.txnCount\");\n  this.gaugeService.submit(\"greet.customgauge\", 1.0);\nRestart the server and go to /metrics to see the new gauge and counter added \nalready reflected there.\nDocumenting microservices\nThe traditional approach of API documentation is either by writing service \nspecification documents or using static service registries. With a large number of \nmicroservices, it would be hard to keep the documentation of APIs in sync.\nMicroservices can be documented in many ways. This section will explore how \nmicroservices can be documented using the popular Swagger framework. The \nfollowing example will use Springfox libraries to generate REST API documentation. \nSpringfox is a set of Java- and Spring-friendly libraries.\nCreate a new Spring Starter Project and select Web in the library selection window. \nName the project chapter2.swagger.\nThe full source code of this example is available as the  \nchapter2.swagger project in the code files of this book.\nAs Springfox libraries are not part of the Spring suite, edit pom.xml and add \nSpringfox Swagger library dependencies. Add the following dependencies to  \nthe project:\n<dependency>\n    <groupId>io.springfox</groupId>\n    <artifactId>springfox-swagger2</artifactId>\n    <version>2.3.1</version>\n</dependency>  \n<dependency>\n    <groupId>io.springfox</groupId>\n    <artifactId>springfox-swagger-ui</artifactId>\n    <version>2.3.1</version>\n</dependency>\n",
      "content_length": 1546,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "Chapter 2\n[ 103 ]\nCreate a REST service similar to the services created earlier, but also add the  \n@EnableSwagger2 annotation, as follows:\n@SpringBootApplication\n@EnableSwagger2\npublic class Application {\nThis is all that's required for a basic Swagger documentation. Start the application \nand point the browser to http://localhost:8080/swagger-ui.html. This will \nopen the Swagger API documentation page:\nAs shown in the diagram, the Swagger lists out the possible operations on Greet \nController. Click on the GET operation. This expands the GET row, which provides \nan option to try out the operation.\n",
      "content_length": 607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "Building Microservices with Spring Boot\n[ 104 ]\nSummary\nIn this chapter, you learned about Spring Boot and its key features to build \nproduction-ready applications.\nWe explored the previous-generation web applications and then how Spring Boot \nmakes developers' lives easier to develop fully qualified microservices. We also \ndiscussed the asynchronous message-based interaction between services. Further, \nwe explored how to achieve some of the key capabilities required for microservices, \nsuch as security, HATEOAS, cross-origin, configurations, and so on with practical \nexamples. We also took a look at how Spring Boot actuators help the operations \nteams and also how we can customize it to our needs. Finally, documenting \nmicroservices APIs was also explored.\nIn the next chapter, we will take a deeper look at some of the practical issues that \nmay arise when implementing microservices. We will also discuss a capability \nmodel that essentially helps organizations when dealing with large microservices \nimplementations.\n",
      "content_length": 1031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "[ 105 ]\nApplying Microservices \nConcepts\nMicroservices are good, but can also be an evil if they are not properly conceived. \nWrong microservice interpretations could lead to irrecoverable failures.\nThis chapter will examine the technical challenges around practical implementations \nof microservices. It will also provide guidelines around critical design decisions for \nsuccessful microservices development. The solutions and patterns for a number of \ncommonly raised concerns around microservices will also be examined. This chapter \nwill also review the challenges in enterprise scale microservices development, and \nhow to overcome those challenges. More importantly, a capability model for a \nmicroservices ecosystem will be established at the end.\nIn this chapter you will learn about the following: \n•\t\nTrade-offs between different design choices and patterns to be considered \nwhen developing microservices\n•\t\nChallenges and anti-patterns in developing enterprise grade microservices\n•\t\nA capability model for a microservices ecosystem\nPatterns and common design decisions\nMicroservices have gained enormous popularity in recent years. They have evolved \nas the preferred choice of architects, putting SOA into the backyards. While \nacknowledging the fact that microservices are a vehicle for developing scalable cloud \nnative systems, successful microservices need to be carefully designed to avoid \ncatastrophes. Microservices are not the one-size-fits-all, universal solution for all \narchitecture problems.\n",
      "content_length": 1520,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "Applying Microservices Concepts\n[ 106 ]\nGenerally speaking, microservices are a great choice for building a lightweight, \nmodular, scalable, and distributed system of systems. Over-engineering, wrong \nuse cases, and misinterpretations could easily turn the system into a disaster. While \nselecting the right use cases is paramount in developing a successful microservice, it \nis equally important to take the right design decisions by carrying out an appropriate \ntrade-off analysis. A number of factors are to be considered when designing \nmicroservices, as detailed in the following sections.\nEstablishing appropriate microservice \nboundaries\nOne of the most common questions relating to microservices is regarding the size of \nthe service. How big (mini-monolithic) or how small (nano service) can a microservice \nbe, or is there anything like right-sized services? Does size really matter?\nA quick answer could be \"one REST endpoint per microservice\", or \"less than 300 \nlines of code\", or \"a component that performs a single responsibility\". But before we \npick up any of these answers, there is lot more analysis to be done to understand the \nboundaries for our services.\nDomain-driven design (DDD) defines the concept of a bounded context. A \nbounded context is a subdomain or a subsystem of a larger domain or system  \nthat is responsible for performing a particular function.\nRead more about DDD at http://domainlanguage.com/ddd/.\nThe following diagram is an example of the domain model:\nIn a finance back office, system invoices, accounting, billing, and the like represent \ndifferent bounded contexts. These bounded contexts are strongly isolated domains \nthat are closely aligned with business capabilities. In the financial domain, the \ninvoices, accounting, and billing are different business capabilities often handled  \nby different subunits under the finance department.\n",
      "content_length": 1888,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "Chapter 3\n[ 107 ]\nA bounded context is a good way to determine the boundaries of microservices. \nEach bounded context could be mapped to a single microservice. In the real world, \ncommunication between bounded contexts are typically less coupled, and often, \ndisconnected.\nEven though real world organizational boundaries are the simplest mechanisms \nfor establishing a bounded context, these may prove wrong in some cases due to \ninherent problems within the organization's structures. For example, a business \ncapability may be delivered through different channels such as front offices, online, \nroaming agents, and so on. In many organizations, the business units may be \norganized based on delivery channels rather than the actual underlying business \ncapabilities. In such cases, organization boundaries may not provide accurate service \nboundaries.\nA top-down domain decomposition could be another way to establish the right \nbounded contexts.\nThere is no silver bullet to establish microservices boundaries, and often, this \nis quite challenging. Establishing boundaries is much easier in the scenario of \nmonolithic application to microservices migration, as the service boundaries and \ndependencies are known from the existing system. On the other hand, in a green \nfield microservices development, the dependencies are hard to establish upfront.\nThe most pragmatic way to design microservices boundaries is to run the scenario \nat hand through a number of possible options, just like a service litmus test. Keep in \nmind that there may be multiple conditions matching a given scenario that will lead \nto a trade-off analysis.\nThe following scenarios could help in defining the microservice boundaries.\nAutonomous functions\nIf the function under review is autonomous by nature, then it can be taken as \na microservices boundary. Autonomous services typically would have fewer \ndependencies on external functions. They accept input, use its internal logic and \ndata for computation, and return a result. All utility functions such as an encryption \nengine or a notification engine are straightforward candidates.\nA delivery service that accepts an order, processes it, and then informs the trucking \nservice is another example of an autonomous service. An online flight search based \non cached seat availability information is yet another example of an autonomous \nfunction.\n",
      "content_length": 2384,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "Applying Microservices Concepts\n[ 108 ]\nSize of a deployable unit\nMost of the microservices ecosystems will take advantage of automation, such as \nautomatic integration, delivery, deployment, and scaling. Microservices covering \nbroader functions result in larger deployment units. Large deployment units pose \nchallenges in automatic file copy, file download, deployment, and start up times.  \nFor instance, the size of a service increases with the density of the functions that  \nit implements.\nA good microservice ensures that the size of its deployable units remains manageable.\nMost appropriate function or subdomain\nIt is important to analyze what would be the most useful component to detach from \nthe monolithic application. This is particularly applicable when breaking monolithic \napplications into microservices. This could be based on parameters such as resource-\nintensiveness, cost of ownership, business benefits, or flexibility.\nIn a typical hotel booking system, approximately 50-60% of the requests are search-\nbased. In this case, moving out the search function could immediately bring in \nflexibility, business benefits, cost reduction, resource free up, and so on.\nPolyglot architecture\nOne of the key characteristics of microservices is its support for polyglot architecture. \nIn order to meet different non-functional and functional requirements, components \nmay require different treatments. It could be different architectures, different \ntechnologies, different deployment topologies, and so on. When components are \nidentified, review them against the requirement for polyglot architectures.\nIn the hotel booking scenario mentioned earlier, a Booking microservice may need \ntransactional integrity, whereas a Search microservice may not. In this case, the \nBooking microservice may use an ACID compliance database such as MySQL, \nwhereas the Search microservice may use an eventual consistent database such  \nas Cassandra.\nSelective scaling\nSelective scaling is related to the previously discussed polyglot architecture. In \nthis context, all functional modules may not require the same level of scalability. \nSometimes, it may be appropriate to determine boundaries based on scalability \nrequirements.\n",
      "content_length": 2230,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "Chapter 3\n[ 109 ]\nFor example, in the hotel booking scenario, the Search microservice has to scale \nconsiderably more than many of the other services such as the Booking microservice \nor the Notification microservice due to the higher velocity of search requests. In this \ncase, a separate Search microservice could run on top of an Elasticsearch or an  \nin-memory data grid for better response.\nSmall, agile teams\nMicroservices enable Agile development with small, focused teams developing \ndifferent parts of the pie. There could be scenarios where parts of the systems  \nare built by different organizations, or even across different geographies, or by  \nteams with varying skill sets. This approach is a common practice, for example,  \nin manufacturing industries.\nIn the microservices world, each of these teams builds different microservices, and \nthen assembles them together. Though this is not the desired way to break down the \nsystem, organizations may end up in such situations. Hence, this approach cannot  \nbe completely ruled out.\nIn an online product search scenario, a service could provide personalized options \nbased on what the customer is looking for. This may require complex machine \nlearning algorithms, and hence need a specialist team. In this scenario, this function \ncould be built as a microservice by a separate specialist team.\nSingle responsibility\nIn theory, the single responsibility principle could be applied at a method, at a class, \nor at a service. However, in the context of microservices, it does not necessarily map \nto a single service or endpoint.\nA more practical approach could be to translate single responsibility into single \nbusiness capability or a single technical capability. As per the single responsibility \nprinciple, one responsibility cannot be shared by multiple microservices. Similarly, \none microservice should not perform multiple responsibilities.\nThere could, however, be special cases where a single business capability is divided \nacross multiple services. One of such cases is managing the customer profile, \nwhere there could be situations where you may use two different microservices for \nmanaging reads and writes using a Command Query Responsibility Segregation \n(CQRS) approach to achieve some of the quality attributes.\n",
      "content_length": 2295,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "Applying Microservices Concepts\n[ 110 ]\nReplicability or changeability\nInnovation and speed are of the utmost importance in IT delivery. Microservices \nboundaries should be identified in such a way that each microservice is easily \ndetachable from the overall system, with minimal cost of re-writing. If part of  \nthe system is just an experiment, it should ideally be isolated as a microservice.\nAn organization may develop a recommendation engine or a customer ranking \nengine as an experiment. If the business value is not realized, then throw away  \nthat service, or replace it with another one.\nMany organizations follow the startup model, where importance is given to meeting \nfunctions and quick delivery. These organizations may not worry too much about the \narchitecture and technologies. Instead, the focus will be on what tools or technologies \ncan deliver solutions faster. Organizations increasingly choose the approach of \ndeveloping Minimum Viable Products (MVPs) by putting together a few services, \nand allowing the system to evolve. Microservices play a vital role in such cases where \nthe system evolves, and services gradually get rewritten or replaced.\nCoupling and cohesion\nCoupling and cohesion are two of the most important parameters for deciding \nservice boundaries. Dependencies between microservices have to be evaluated \ncarefully to avoid highly coupled interfaces. A functional decomposition, together \nwith a modeled dependency tree, could help in establishing a microservices \nboundary. Avoiding too chatty services, too many synchronous request-response \ncalls, and cyclic synchronous dependencies are three key points, as these could \neasily break the system. A successful equation is to keep high cohesion within a \nmicroservice, and loose coupling between microservices. In addition to this, ensure \nthat transaction boundaries are not stretched across microservices. A first class \nmicroservice will react upon receiving an event as an input, execute a number \nof internal functions, and finally send out another event. As part of the compute \nfunction, it may read and write data to its own local store.\nThink microservice as a product\nDDD also recommends mapping a bounded context to a product. As per DDD, each \nbounded context is an ideal candidate for a product. Think about a microservice \nas a product by itself. When microservice boundaries are established, assess them \nfrom a product's point of view to see whether they really stack up as product. It is \nmuch easier for business users to think boundaries from a product point of view. \nA product boundary may have many parameters, such as a targeted community, \nflexibility in deployment, sell-ability, reusability, and so on.\n",
      "content_length": 2726,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "Chapter 3\n[ 111 ]\nDesigning communication styles\nCommunication between microservices can be designed either in synchronous \n(request-response) or asynchronous (fire and forget) styles.\nSynchronous style communication\nThe following diagram shows an example request/response style service:\nIn synchronous communication, there is no shared state or object. When a caller \nrequests a service, it passes the required information and waits for a response.  \nThis approach has a number of advantages.\nAn application is stateless, and from a high availability standpoint, many \nactive instances can be up and running, accepting traffic. Since there are no \nother infrastructure dependencies such as a shared messaging server, there are \nmanagement fewer overheads. In case of an error at any stage, the error will be \npropagated back to the caller immediately, leaving the system in a consistent state, \nwithout compromising data integrity.\nThe downside in a synchronous request-response communication is that the user \nor the caller has to wait until the requested process gets completed. As a result, the \ncalling thread will wait for a response, and hence, this style could limit the scalability \nof the system.\nA synchronous style adds hard dependencies between microservices. If one service \nin the service chain fails, then the entire service chain will fail. In order for a service \nto succeed, all dependent services have to be up and running. Many of the failure \nscenarios have to be handled using timeouts and loops.\n",
      "content_length": 1520,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "Applying Microservices Concepts\n[ 112 ]\nAsynchronous style communication\nThe following diagram is a service designed to accept an asynchronous message as \ninput, and send the response asynchronously for others to consume:\nThe asynchronous style is based on reactive event loop semantics which decouple \nmicroservices. This approach provides higher levels of scalability, because services \nare independent, and can internally spawn threads to handle an increase in \nload. When overloaded, messages will be queued in a messaging server for later \nprocessing. That means that if there is a slowdown in one of the services, it will not \nimpact the entire chain. This provides higher levels of decoupling between services, \nand therefore maintenance and testing will be simpler.\nThe downside is that it has a dependency to an external messaging server. It is \ncomplex to handle the fault tolerance of a messaging server. Messaging typically \nworks with an active/passive semantics. Hence, handling continuous availability of \nmessaging systems is harder to achieve. Since messaging typically uses persistence,  \na higher level of I/O handling and tuning is required.\nHow to decide which style to choose?\nBoth approaches have their own merits and constraints. It is not possible to develop \na system with just one approach. A combination of both approaches is required \nbased on the use cases. In principle, the asynchronous approach is great for building \ntrue, scalable microservice systems. However, attempting to model everything as \nasynchronous leads to complex system designs.\nHow does the following example look in the context where an end user clicks on a  \nUI to get profile details?\n",
      "content_length": 1688,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "Chapter 3\n[ 113 ]\nThis is perhaps a simple query to the backend system to get a result in a request-\nresponse model. This can also be modeled in an asynchronous style by pushing \na message to an input queue, and waiting for a response in an output queue till \na response is received for the given correlation ID. However, though we use \nasynchronous messaging, the user is still blocked for the entire duration of the query.\nAnother use case is that of a user clicking on a UI to search hotels, which is depicted \nin the following diagram:\nThis is very similar to the previous scenario. However, in this case, we assume that \nthis business function triggers a number of activities internally before returning \nthe list of hotels back to the user. For example, when the system receives this \nrequest, it calculates the customer ranking, gets offers based on the destination, gets \nrecommendations based on customer preferences, optimizes the prices based on \ncustomer values and revenue factors, and so on. In this case, we have an opportunity \nto do many of these activities in parallel so that we can aggregate all these results \nbefore presenting them to the customer. As shown in the preceding diagram, \nvirtually any computational logic could be plugged in to the search pipeline  \nlistening to the IN queue. \nAn effective approach in this case is to start with a synchronous request response,  \nand refactor later to introduce an asynchronous style when there is value in doing that.\n",
      "content_length": 1489,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "Applying Microservices Concepts\n[ 114 ]\nThe following example shows a fully asynchronous style of service interactions:\nThe service is triggered when the user clicks on the booking function. It is again, by \nnature, a synchronous style communication. When booking is successful, it sends \na message to the customer's e-mail address, sends a message to the hotel's booking \nsystem, updates the cached inventory, updates the loyalty points system, prepares an \ninvoice, and perhaps more. Instead of pushing the user into a long wait state, a better \napproach is to break the service into pieces. Let the user wait till a booking record  \nis created by the Booking Service. On successful completion, a booking event will  \nbe published, and return a confirmation message back to the user. Subsequently,  \nall other activities will happen in parallel, asynchronously.\nIn all three examples, the user has to wait for a response. With the new web \napplication frameworks, it is possible to send requests asynchronously, and define \nthe callback method, or set an observer for getting a response. Therefore, the users \nwon't be fully blocked from executing other activities.\nIn general, an asynchronous style is always better in the microservices world, but \nidentifying the right pattern should be purely based on merits. If there are no merits \nin modeling a transaction in an asynchronous style, then use the synchronous style \ntill you find an appealing case. Use reactive programming frameworks to avoid \ncomplexity when modeling user-driven requests, modeled in an asynchronous style.\n",
      "content_length": 1584,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "Chapter 3\n[ 115 ]\nOrchestration of microservices\nComposability is one of the service design principles. This leads to confusion \naround who is responsible for the composing services. In the SOA world, ESBs are \nresponsible for composing a set of finely-grained services. In some organizations, \nESBs play the role of a proxy, and service providers themselves compose and expose \ncoarse-grained services. In the SOA world, there are two approaches for handling \nsuch situations.\nThe first approach is orchestration, which is depicted in the following diagram:\nIn the orchestration approach, multiple services are stitched together to get a \ncomplete function. A central brain acts as the orchestrator. As shown in the diagram, \nthe order service is a composite service that will orchestrate other services. There \ncould be sequential as well as parallel branches from the master process. Each task \nwill be fulfilled by an atomic task service, typically a web service. In the SOA world, \nESBs play the role of orchestration. The orchestrated service will be exposed by ESBs \nas a composite service.\nThe second approach is choreography, which is shown in the following diagram:\n",
      "content_length": 1176,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "Applying Microservices Concepts\n[ 116 ]\nIn the choreography approach, there is no central brain. An event, a booking event in \nthis case, is published by a producer, a number of consumers wait for the event, and \nindependently apply different logics on the incoming event. Sometimes, events could \neven be nested where the consumers can send another event which will be consumed \nby another service. In the SOA world, the caller pushes a message to the ESB, and the \ndownstream flow will be automatically determined by the consuming services.\nMicroservices are autonomous. This essentially means that in an ideal situation, \nall required components to complete their function should be within the service. \nThis includes the database, orchestration of its internal services, intrinsic state \nmanagement, and so on. The service endpoints provide coarse-grained APIs. This \nis perfectly fine as long as there are no external touch points required. But in reality, \nmicroservices may need to talk to other microservices to fulfil their function.\nIn such cases, choreography is the preferred approach for connecting multiple \nmicroservices together. Following the autonomy principle, a component sitting \noutside a microservice and controlling the flow is not the desired option. If the use \ncase can be modeled in choreographic style, that would be the best possible way to \nhandle the situation.\nBut it may not be possible to model choreography in all cases. This is depicted in the \nfollowing diagram:\nIn the preceding example, Reservation and Customer are two microservices, with \nclearly segregated functional responsibilities. A case could arise when Reservation \nwould want to get Customer preferences while creating a reservation. These are \nquite normal scenarios when developing complex systems.\nCan we move Customer to Reservation so that Reservation will be complete by \nitself? If Customer and Reservation are identified as two microservices based on \nvarious factors, it may not be a good idea to move Customer to Reservation.  \nIn such a case, we will meet another monolithic application sooner or later.\n",
      "content_length": 2116,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "Chapter 3\n[ 117 ]\nCan we make the Reservation to Customer call asynchronous? This example is \nshown in the following diagram:\nCustomer preference is required for Reservation to progress, and hence, it may \nrequire a synchronous blocking call to Customer. Retrofitting this by modeling \nasynchronously does not really make sense.\nCan we take out just the orchestration bit, and create another composite \nmicroservice, which then composes Reservation and Customer?\nThis is acceptable in the approach for composing multiple components within a \nmicroservice. But creating a composite microservice may not be a good idea. We will \nend up creating many microservices with no business alignment, which would not \nbe autonomous, and could result in many fine-grained microservices.\n",
      "content_length": 775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "Applying Microservices Concepts\n[ 118 ]\nCan we duplicate customer preference by keeping a slave copy of the preference data \ninto Reservation?\nChanges will be propagated whenever there is a change in the master. In this \ncase, Reservation can use customer preference without fanning out a call. It is a \nvalid thought, but we need to carefully analyze this. Today we replicate customer \npreference, but in another scenario, we may want to reach out to customer service \nto see whether the customer is black-listed from reserving. We have to be extremely \ncareful in deciding what data to duplicate. This could add to the complexity.\nHow many endpoints in a microservice?\nIn many situations, developers are confused with the number of endpoints per \nmicroservice. The question really is whether to limit each microservice with one \nendpoint or multiple endpoints:\n",
      "content_length": 863,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "Chapter 3\n[ 119 ]\nThe number of endpoints is not really a decision point. In some cases, there may \nbe only one endpoint, whereas in some other cases, there could be more than one \nendpoint in a microservice. For instance, consider a sensor data service which \ncollects sensor information, and has two logical endpoints: create and read. But in \norder to handle CQRS, we may create two separate physical microservices as shown \nin the case of Booking in the preceding diagram. Polyglot architecture could be \nanother scenario where we may split endpoints into different microservices.\nConsidering a notification engine, notifications will be send out in response to an \nevent. The process of notification such as preparation of data, identification of a \nperson, and delivery mechanisms, are different for different events. Moreover, we \nmay want to scale each of these processes differently at different time windows. In \nsuch situations, we may decide to break each notification endpoint in to a separate \nmicroservice.\nIn yet another example, a Loyalty Points microservice may have multiple services \nsuch as accrue, redeem, transfer, and balance. We may not want to treat each of these \nservices differently. All of these services are connected and use the points table for \ndata. If we go with one endpoint per service, we will end up in a situation where \nmany fine-grained services access data from the same data store or replicated  \ncopies of the same data store.\nIn short, the number of endpoints is not a design decision. One microservice  \nmay host one or more endpoints. Designing appropriate bounded context for  \na microservice is more important.\nOne microservice per VM or multiple?\nOne microservice could be deployed in multiple Virtual Machines (VMs) by \nreplicating the deployment for scalability and availability. This is a no brainer. \nThe question is whether multiple microservices could be deployed in one virtual \nmachine? There are pros and cons for this approach. This question typically arises \nwhen the services are simple, and the traffic volume is less.\nConsider an example where we have a couple of microservices, and the overall \ntransaction per minute is less than 10. Also assume that the smallest possible VM size \navailable is 2-core 8 GB RAM. A further assumption is that in such cases, a 2-core 8 \nGB VM can handle 10-15 transactions per minute without any performance concerns. \nIf we use different VMs for each microservice, it may not be cost effective, and we \nwill end up paying more for infrastructure and license, since many vendors charge \nbased on the number of cores.\n",
      "content_length": 2616,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "Applying Microservices Concepts\n[ 120 ]\nThe simplest way to approach this problem is to ask a few questions:\n•\t\nDoes the VM have enough capacity to run both services under peak usage?\n•\t\nDo we want to treat these services differently to achieve SLAs (selective \nscaling)? For example, for scalability, if we have an all-in-one VM, we will \nhave to replicate VMs which replicate all services.\n•\t\nAre there any conflicting resource requirements? For example, different OS \nversions, JDK versions, and others.\nIf all your answers are No, then perhaps we can start with collocated deployment, \nuntil we encounter a scenario to change the deployment topology. However, we  \nwill have to ensure that these services are not sharing anything, and are running  \nas independent OS processes.\nHaving said that, in an organization with matured virtualized infrastructure or cloud \ninfrastructure, this may not be a huge concern. In such environments, the developers \nneed not worry about where the services are running. Developers may not even think \nabout capacity planning. Services will be deployed in a compute cloud. Based on the \ninfrastructure availability, SLAs and the nature of the service, the infrastructure self-\nmanages deployments. AWS Lambda is a good example of such a service.\nRules engine – shared or embedded?\nRules are an essential part of any system. For example, an offer eligibility service \nmay execute a number of rules before making a yes or no decision. Either we hand \ncode rules, or we may use a rules engine. Many enterprises manage rules centrally \nin a rules repository as well as execute them centrally. These enterprise rule engines \nare primarily used for providing the business an opportunity to author and manage \nrules as well as reuse rules from the central repository. Drools is one of the popular \nopen source rules engines. IBM, FICO, and Bosch are some of the pioneers in the \ncommercial space. These rule engines improve productivity, enable reuse of rules, \nfacts, vocabularies, and provide faster rule execution using the rete algorithm.\nIn the context of microservices, a central rules engine means fanning out calls from \nmicroservices to the central rules engine. This also means that the service logic is now \nin two places, some within the service, and some external to the service. Nevertheless, \nthe objective in the context of microservices is to reduce external dependencies:\n",
      "content_length": 2420,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "Chapter 3\n[ 121 ]\nIf the rules are simple enough, few in numbers, only used within the boundaries of \na service, and not exposed to business users for authoring, then it may be better to \nhand-code business rules than rely on an enterprise rule engine:\nIf the rules are complex, limited to a service context, and not given to business users, \nthen it is better to use an embedded rules engine within the service:\nIf the rules are managed and authored by business, or if the rules are complex, or if \nwe are reusing rules from other service domains, then a central authoring repository \nwith a locally embedded execution engine could be a better choice.\nNote that this has to be carefully evaluated since all vendors may not support the \nlocal rule execution approach, and there could be technology dependencies such as \nrunning rules only within a specific application server, and so on.\nRole of BPM and workflows\nBusiness Process Management (BPM) and Intelligent Business Process \nManagement (iBPM) are tool suites for designing, executing, and monitoring \nbusiness processes.\nTypical use cases for BPM are:\n•\t\nCoordinating a long-running business process, where some processes are \nrealized by existing assets, whereas some other areas may be niche, and there \nis no concrete implementation of the processes being in place. BPM allows \ncomposing both types, and provides an end-to-end automated process. This \noften involves systems and human interactions.\n",
      "content_length": 1459,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "Applying Microservices Concepts\n[ 122 ]\n•\t\nProcess-centric organizations, such as those that have implemented Six \nSigma, want to monitor their processes for continuous improvement  \non efficiency.\n•\t\nProcess re-engineering with a top-down approach by redefining the business \nprocess of an organization.\nThere could be two scenarios where BPM fits in the microservices world:\nThe first scenario is business process re-engineering, or threading an end-to-end long \nrunning business process, as stated earlier. In this case, BPM operates at a higher level, \nwhere it may automate a cross-functional, long-running business process by stitching \na number of coarse-grained microservices, existing legacy connectors, and human \ninteractions. As shown in the preceding diagram, the loan approval BPM invokes \nmicroservices as well as legacy application services. It also integrates human tasks.\nIn this case, microservices are headless services that implement a subprocess. From \nthe microservices' perspective, BPM is just another consumer. Care needs to be taken \nin this approach to avoid accepting a shared state from a BPM process as well as \nmoving business logic to BPM:\n",
      "content_length": 1173,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "Chapter 3\n[ 123 ]\nThe second scenario is monitoring processes, and optimizing them for efficiency. \nThis goes hand in hand with a completely automated, asynchronously \nchoreographed microservices ecosystem. In this case, microservices and BPM work \nas independent ecosystems. Microservices send events at various timeframes such \nas the start of a process, state changes, end of a process, and so on. These events are \nused by the BPM engine to plot and monitor process states. We may not require \na full-fledged BPM solution for this, as we are only mocking a business process \nto monitor its efficiency. In this case, the order delivery process is not a BPM \nimplementation, but it is more of a monitoring dashboard that captures and displays \nthe progress of the process.\nTo summarize, BPM could still be used at a higher level for composing multiple \nmicroservices in situations where end-to-end cross-functional business processes \nare modeled by automating systems and human interactions. A better and simpler \napproach is to have a business process dashboard to which microservices feed state \nchange events as mentioned in the second scenario.\nCan microservices share data stores?\nIn principle, microservices should abstract presentation, business logic, and data \nstores. If the services are broken as per the guidelines, each microservice logically \ncould use an independent database:\nIn the preceding diagram, both Product and Order microservices share one \ndatabase and one data model. Shared data models, shared schema, and shared \ntables are recipes for disasters when developing microservices. This may be good \nat the beginning, but when developing complex microservices, we tend to add \nrelationships between data models, add join queries, and so on. This can result in \ntightly coupled physical data models.\n",
      "content_length": 1826,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "Applying Microservices Concepts\n[ 124 ]\nIf the services have only a few tables, it may not be worth investing a full instance of \na database like an Oracle database instance. In such cases, a schema level segregation \nis good enough to start with:\nThere could be scenarios where we tend to think of using a shared database for \nmultiple services. Taking an example of a customer data repository or master \ndata managed at the enterprise level, the customer registration and customer \nsegmentation microservices logically share the same customer data repository:\nAs shown in the preceding diagram, an alternate approach in this scenario is to \nseparate the transactional data store for microservices from the enterprise data \nrepository by adding a local transactional data store for these services. This will \nhelp the services to have flexibility in remodeling the local data store optimized for \nits purpose. The enterprise customer repository sends change events when there is \nany change in the customer data repository. Similarly, if there is any change in any \nof the transactional data stores, the changes have to be sent to the central customer \nrepository.\n",
      "content_length": 1166,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "Chapter 3\n[ 125 ]\nSetting up transaction boundaries\nTransactions in operational systems are used to maintain the consistency of data \nstored in an RDBMS by grouping a number of operations together into one atomic \nblock. They either commit or rollback the entire operation. Distributed systems \nfollow the concept of distributed transactions with a two-phase commit. This is \nparticularly required if heterogeneous components such as an RPC service, JMS,  \nand so on participate in a transaction.\nIs there a place for transactions in microservices? Transactions are not bad, but one \nshould use transactions carefully, by analyzing what we are trying do.\nFor a given microservice, an RDBMS like MySQL may be selected as a backing store \nto ensure 100% data integrity, for example, a stock or inventory management service \nwhere data integrity is key. It is appropriate to define transaction boundaries within \nthe microsystem using local transactions. However, distributed global transactions \nshould be avoided in the microservices context. Proper dependency analysis is \nrequired to ensure that transaction boundaries do not span across two different \nmicroservices as much as possible.\nAltering use cases to simplify transactional \nrequirements\nEventual consistency is a better option than distributed transactions that span \nacross multiple microservices. Eventual consistency reduces a lot of overheads, \nbut application developers may need to re-think the way they write application \ncode. This could include remodeling functions, sequencing operations to minimize \nfailures, batching insert and modify operations, remodeling data structure, and \nfinally, compensating operations that negate the effect.\nA classical problem is that of the last room selling scenario in a hotel booking use \ncase. What if there is only one room left, and there are multiple customers booking \nthis singe available room? A business model change sometimes makes this scenario \nless impactful. We could set an \"under booking profile\", where the actual number of \nbookable rooms can go below the actual number of rooms (bookable = available - 3) in \nanticipation of some cancellations. Anything in this range will be accepted as \"subject \nto confirmation\", and customers will be charged only if payment is confirmed. \nBookings will be confirmed in a set time window.\n",
      "content_length": 2351,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "Applying Microservices Concepts\n[ 126 ]\nNow consider the scenario where we are creating customer profiles in a NoSQL \ndatabase like CouchDB. In more traditional approaches with RDBMS, we insert \na customer first, and then insert the customer's address, profile details, then \npreferences, all in one transaction. When using NoSQL, we may not do the same \nsteps. Instead, we may prepare a JSON object with all the details, and insert this  \ninto CouchDB in one go. In this second case, no explicit transaction boundaries  \nare required.\nDistributed transaction scenarios\nThe ideal scenario is to use local transactions within a microservice if required, \nand completely avoid distributed transactions. There could be scenarios where at \nthe end of the execution of one service, we may want to send a message to another \nmicroservice. For example, say a tour reservation has a wheelchair request. Once the \nreservation is successful, we will have to send a message for the wheelchair booking \nto another microservice that handles ancillary bookings. The reservation request \nitself will run on a local transaction. If sending this message fails, we are still in the \ntransaction boundary, and we can roll back the entire transaction. What if we create \na reservation and send the message, but after sending the message, we encounter \nan error in the reservation, the reservation transaction fails, and subsequently, \nthe reservation record is rolled back? Now we end up in a situation where we've \nunnecessarily created an orphan wheelchair booking:\nThere are a couple of ways we can address this scenario. The first approach is to \ndelay sending the message till the end. This ensures that there are less chances \nfor any failure after sending the message. Still, if failure occurs after sending the \nmessage, then the exception handling routine is run, that is, we send a compensating \nmessage to reverse the wheelchair booking.\n",
      "content_length": 1929,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "Chapter 3\n[ 127 ]\nService endpoint design consideration\nOne of the important aspects of microservices is service design. Service design has \ntwo key elements: contract design and protocol selection.\nContract design\nThe first and foremost principle of service design is simplicity. The services should \nbe designed for consumers to consume. A complex service contract reduces the \nusability of the service. The KISS (Keep It Simple Stupid) principle helps us \nto build better quality services faster, and reduces the cost of maintenance and \nreplacement. The YAGNI (You Ain't Gonna Need It) is another principle supporting \nthis idea. Predicting future requirements and building systems are, in reality, not \nfuture-proofed. This results in large upfront investment as well as higher cost of \nmaintenance.\nEvolutionary design is a great concept. Do just enough design to satisfy today's \nwants, and keep changing and refactoring the design to accommodate new features \nas and when they are required. Having said that, this may not be simple unless there \nis a strong governance in place.\nConsumer Driven Contracts (CDC) is a great idea that supports evolutionary \ndesign. In many cases, when the service contract gets changed, all consuming \napplications have to undergo testing. This makes change difficult. CDC helps in \nbuilding confidence in consumer applications. CDC advocates each consumer to \nprovide their expectation to the provider in the form of test cases so that the provider \nuses them as integration tests whenever the service contract is changed.\nPostel's law is also relevant in this scenario. Postel's law primarily addresses TCP \ncommunications; however, this is also equally applicable to service design. When \nit comes to service design, service providers should be as flexible as possible when \naccepting consumer requests, whereas service consumers should stick to the contract \nas agreed with the provider.\nProtocol selection\nIn the SOA world, HTTP/SOAP, and messaging were kinds of default service \nprotocols for service interactions. Microservices follow the same design principles for \nservice interaction. Loose coupling is one of the core principles in the microservices \nworld too.\nMicroservices fragment applications into many physically independent deployable \nservices. This not only increases the communication cost, it is also susceptible to \nnetwork failures. This could also result in poor performance of services.\n",
      "content_length": 2452,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "Applying Microservices Concepts\n[ 128 ]\nMessage-oriented services\nIf we choose an asynchronous style of communication, the user is disconnected, \nand therefore, response times are not directly impacted. In such cases, we may \nuse standard JMS or AMQP protocols for communication with JSON as payload. \nMessaging over HTTP is also popular, as it reduces complexity. Many new entrants \nin messaging services support HTTP-based communication. Asynchronous REST is \nalso possible, and is handy when calling long-running services.\nHTTP and REST endpoints\nCommunication over HTTP is always better for interoperability, protocol handling, \ntraffic routing, load balancing, security systems, and the like. Since HTTP is stateless, \nit is more compatible for handling stateless services with no affinity. Most of the \ndevelopment frameworks, testing tools, runtime containers, security systems, and so \non are friendlier towards HTTP.\nWith the popularity and acceptance of REST and JSON, it is the default choice for \nmicroservice developers. The HTTP/REST/JSON protocol stack makes building \ninteroperable systems very easy and friendly. HATEOAS is one of the design \npatterns emerging for designing progressive rendering and self-service navigations. \nAs discussed in the previous chapter, HATEOAS provides a mechanism to  \nlink resources together so that the consumer can navigate between resources.  \nRFC 5988 – Web Linking is another upcoming standard.\nOptimized communication protocols\nIf the service response times are stringent, then we need to pay special attention to \nthe communication aspects. In such cases, we may choose alternate protocols such \nas Avro, Protocol Buffers, or Thrift for communicating between services. But this \nlimits the interoperability of services. The trade-off is between performance and \ninteroperability requirements. Custom binary protocols need careful evaluation as \nthey bind native objects on both sides—consumer and producer. This could run into \nrelease management issues such as class version mismatch in Java-based RPC style \ncommunications.\nAPI documentations\nLast thing: a good API is not only simple, but should also have enough \ndocumentation for the consumers. There are many tools available today for \ndocumenting REST-based services like Swagger, RAML, and API Blueprint.\n",
      "content_length": 2319,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "Chapter 3\n[ 129 ]\nHandling shared libraries\nThe principle behind microservices is that they should be autonomous and  \nself-contained. In order to adhere to this principle, there may be situations where  \nwe will have to duplicate code and libraries. These could be either technical libraries \nor functional components.\nFor example, the eligibility for a flight upgrade will be checked at the time of \ncheck-in as well as when boarding. If check-in and boarding are two different \nmicroservices, we may have to duplicate the eligibility rules in both the services.  \nThis was the trade-off between adding a dependency versus code duplication.\nIt may be easy to embed code as compared to adding an additional dependency,  \nas it enables better release management and performance. But this is against the  \nDRY principle.\nDRY principle\nEvery piece of knowledge must have a single, unambiguous, \nauthoritative representation within a system.\nThe downside of this approach is that in case of a bug or an enhancement on the \nshared library, it has to be upgraded in more than one place. This may not be a \nsevere setback as each service can contain a different version of the shared library:\nAn alternative option of developing the shared library as another microservice \nitself needs careful analysis. If it is not qualified as a microservice from the business \ncapability point of view, then it may add more complexity than its usefulness. The \ntrade-off analysis is between overheads in communication versus duplicating the \nlibraries in multiple services.\n",
      "content_length": 1555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "Applying Microservices Concepts\n[ 130 ]\nUser interfaces in microservices\nThe microservices principle advocates a microservice as a vertical slice from the \ndatabase to presentation:\nIn reality, we get requirements to build quick UI and mobile applications mashing \nup the existing APIs. This is not uncommon in the modern scenario, where a \nbusiness wants quick turnaround time from IT:\nPenetration of mobile applications is one of the causes of this approach. In many \norganizations, there will be mobile development teams sitting close to the business \nteam, developing rapid mobile applications by combining and mashing up APIs \nfrom multiple sources, both internal and external. In such situations, we may just \nexpose services, and leave it for the mobile teams to realize in the way the business \nwants. In this case, we will build headless microservices, and leave it to the mobile \nteams to build a presentation layer.\n",
      "content_length": 927,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "Chapter 3\n[ 131 ]\nAnother category of problem is that the business may want to build consolidated \nweb applications targeted to communities:\nFor example, the business may want to develop a departure control application \ntargeting airport users. A departure control web application may have functions \nsuch as check-in, lounge management, boarding, and so on. These may be designed \nas independent microservices. But from the business standpoint, it all needs to be \nclubbed into a single web application. In such cases, we will have to build web \napplications by mashing up services from the backend.\nOne approach is to build a container web application or a placeholder web \napplication, which links to multiple microservices at the backend. In this case, \nwe develop full stack microservices, but the screens coming out of this could be \nembedded in to another placeholder web application. One of the advantages of \nthis approach is that you can have multiple placeholder web applications targeting \ndifferent user communities, as shown in the preceding diagram. We may use an API \ngateway to avoid those crisscross connections. We will explore the API gateway in \nthe next section.\nUse of API gateways in microservices\nWith the advancement of client-side JavaScript frameworks like AngularJS, the \nserver is expected to expose RESTful services. This could lead to two issues. The first \nissue is the mismatch in contract expectations. The second issue is multiple calls to \nthe server to render a page.\n",
      "content_length": 1506,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "Applying Microservices Concepts\n[ 132 ]\nWe start with the contract mismatch case. For example, GetCustomer may return  \na JSON with many fields:\nCustomer {\n  Name: \n  Address: \n  Contact: \n}\nIn the preceding case, Name, Address, and Contact are nested JSON objects. But a \nmobile client may expect only basic information such as first name, and last name. \nIn the SOA world, an ESB or a mobile middleware did this job of transformation of \ndata for the client. The default approach in microservices is to get all the elements of \nCustomer, and then the client takes up the responsibility to filter the elements. In this \ncase, the overhead is on the network.\nThere are several approaches we can think about to solve this case:\nCustomer {\n  Id: 1\n  Name: /customer/name/1\n  Address: /customer/address/1\n  Contact: /customer/contact/1\n}\nIn the first approach, minimal information is sent with links as explained in the \nsection on HATEOAS. In the preceding case, for customer ID 1, there are three links, \nwhich will help the client to access specific data elements. The example is a simple \nlogical representation, not the actual JSON. The mobile client in this case will get \nbasic customer information. The client further uses the links to get the additional \nrequired information.\nThe second approach is used when the client makes the REST call; it also sends the \nrequired fields as part of the query string. In this scenario, the client sends a request \nwith firstname and lastname as the query string to indicate that the client only \nrequires these two fields. The downside is that it ends up in complex server-side \nlogic as it has to filter based on the fields. The server has to send different elements \nbased on the incoming query.\nThe third approach is to introduce a level of indirection. In this, a gateway component \nsits between the client and the server, and transforms data as per the consumer's \nspecification. This is a better approach as we do not compromise on the backend \nservice contract. This leads to what is called UI services. In many cases, the API \ngateway acts as a proxy to the backend, exposing a set of consumer-specific APIs:\n",
      "content_length": 2161,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "Chapter 3\n[ 133 ]\nThere are two ways we can deploy an API gateway. The first one is one API gateway \nper microservice as shown in diagram A. The second approach (diagram B) is to \nhave a common API gateway for multiple services. The choice really depends on \nwhat we are looking for. If we are using an API gateway as a reverse proxy, then \noff-the-shelf gateways such as Apigee, Mashery, and the like could be used as a \nshared platform. If we need fine-grained control over traffic shaping and complex \ntransformations, then per service custom API gateways may be more useful.\nA related problem is that we will have to make many calls from the client to the server. \nIf we refer to our holiday example in Chapter 1, Demystifying Microservices, you know \nthat for rendering each widget, we had to make a call to the server. Though we transfer \nonly data, it can still add a significant overhead on the network. This approach is not \nfully wrong, as in many cases, we use responsive design and progressive design. The \ndata will be loaded on demand, based on user navigations. In order to do this, each \nwidget in the client should make independent calls to the server in a lazy mode. If \nbandwidth is an issue, then an API gateway is the solution. An API gateway acts as a \nmiddleman to compose and transform APIs from multiple microservices.\nUse of ESB and iPaaS with microservices\nTheoretically, SOA is not all about ESBs, but the reality is that ESBs have always been \nat the center of many SOA implementations. What would be the role of an ESB in  \nthe microservices world?\nIn general, microservices are fully cloud native systems with smaller footprints. \nThe lightweight characteristics of microservices enable automation of deployments, \nscaling, and so on. On the contrary, enterprise ESBs are heavyweight in nature, and \nmost of the commercial ESBs are not cloud friendly. The key features of an ESB are \nprotocol mediation, transformation, orchestration, and application adaptors. In a \ntypical microservices ecosystem, we may not need any of these features.\n",
      "content_length": 2070,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "Applying Microservices Concepts\n[ 134 ]\nThe limited ESB capabilities that are relevant for microservices are already available \nwith more lightweight tools such as an API gateway. Orchestration is moved from \nthe central bus to the microservices themselves. Therefore, there is no centralized \norchestration capability expected in the case of microservices. Since the services are \nset up to accept more universal message exchange styles using REST/JSON calls, \nno protocol mediation is required. The last piece of capability that we get from ESBs \nare the adaptors to connect back to the legacy systems. In the case of microservices, \nthe service itself provides a concrete implementation, and hence, there are no legacy \nconnectors required. For these reasons, there is no natural space for ESBs in the \nmicroservices world.\nMany organizations established ESBs as the backbone for their application \nintegrations (EAI). Enterprise architecture policies in such organizations are built \naround ESBs. There could be a number of enterprise-level policies such as auditing, \nlogging, security, validation, and so on that would have been in place when \nintegrating using ESB. Microservices, however, advocate a more decentralized \ngovernance. ESBs will be an overkill if integrated with microservices.\nNot all services are microservices. Enterprises have legacy applications, vendor \napplications, and so on. Legacy services use ESBs to connect with microservices. \nESBs still hold their place for legacy integration and vendor applications to integrate \nat the enterprise level.\nWith the advancement of clouds, the capabilities of ESBs are not sufficient to manage \nintegration between clouds, cloud to on-premise, and so on. Integration Platform as \na Service (iPaaS) is evolving as the next generation application integration platform, \nwhich further reduces the role of ESBs. In typical deployments, iPaaS invokes API \ngateways to access microservices.\nService versioning considerations\nWhen we allow services to evolve, one of the important aspect to consider is \nservice versioning. Service versioning should be considered upfront, and not as \nan afterthought. Versioning helps us to release new services without breaking the \nexisting consumers. Both the old version and the new version will be deployed  \nside by side.\nSemantic versions are widely used for service versioning. A semantic version has \nthree components: major, minor, and patch. Major is used when there is a breaking \nchange, minor is used when there is a backward compatible change, and patch is \nused when there is a backward compatible bug fix.\n",
      "content_length": 2619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "Chapter 3\n[ 135 ]\nVersioning could get complicated when there is more than one service in a \nmicroservice. It is always simple to version services at the service level compared \nto the operations level. If there is a change in one of the operations, the service is \nupgraded and deployed to V2. The version change is applicable to all operations  \nin the service. This is the notion of immutable services.\nThere are three different ways in which we can version REST services:\n•\t\nURI versioning\n•\t\nMedia type versioning\n•\t\nCustom header\nIn URI versioning, the version number is included in the URL itself. In this case, \nwe just need to be worried about the major versions only. Hence, if there is a minor \nversion change or a patch, the consumers do not need to worry about the changes.  \nIt is a good practice to alias the latest version to a non-versioned URI, which is done \nas follows:\n/api/v3/customer/1234\n/api/customer/1234  - aliased to v3.\n@RestController(\"CustomerControllerV3\")\n@RequestMapping(\"api/v3/customer\")\npublic class CustomerController {\n}\nA slightly different approach is to use the version number as part of the URL \nparameter:\napi/customer/100?v=1.5\nIn case of media type versioning, the version is set by the client on the HTTP Accept \nheader as follows:\nAccept:  application/vnd.company.customer-v3+json\nA less effective approach for versioning is to set the version in the custom header:\n@RequestMapping(value = \"/{id}\", method = RequestMethod.GET, headers = \n{\"version=3\"})\npublic Customer getCustomer(@PathVariable(\"id\") long id) {\n     //other code goes here.\n}\n",
      "content_length": 1591,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "Applying Microservices Concepts\n[ 136 ]\nIn the URI approach, it is simple for the clients to consume services. But this has \nsome inherent issues such as the fact that versioning-nested URI resources could \nbe complex. Indeed, migrating clients is slightly complex as compared to media \ntype approaches, with caching issues for multiple versions of the services, and \nothers. However, these issues are not significant enough for us to not go with a URI \napproach. Most of the big Internet players such as Google, Twitter, LinkedIn, and \nSalesforce are following the URI approach.\nDesign for cross origin\nWith microservices, there is no guarantee that the services will run from the \nsame host or same domain. Composite UI web applications may call multiple \nmicroservices for accomplishing a task, and these could come from different  \ndomains and hosts.\nCORS allows browser clients to send requests to services hosted on different \ndomains. This is essential in a microservices-based architecture.\nOne approach is to enable all microservices to allow cross origin requests from other \ntrusted domains. The second approach is to use an API gateway as a single trusted \ndomain for the clients.\nHandling shared reference data\nWhen breaking large applications, one of the common issues which we see is the \nmanagement of master data or reference data. Reference data is more like shared \ndata required between different microservices. City master, country master, and so \non will be used in many services such as flight schedules, reservations, and others.\nThere are a few ways in which we can solve this. For instance, in the case of \nrelatively static, never changing data, then every service can hardcode this data \nwithin all the microservices themselves:\n",
      "content_length": 1757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "Chapter 3\n[ 137 ]\nAnother approach, as shown in the preceding diagram, is to build it as another \nmicroservice. This is good, clean, and neat, but the downside is that every service \nmay need to call the master data multiple times. As shown in the diagram for the \nSearch and Booking example, there are transactional microservices, which use the \nGeography microservice to access shared data:\nAnother option is to replicate the data with every microservice. There is no single \nowner, but each service has its required master data. When there is an update, all \nthe services are updated. This is extremely performance friendly, but one has to \nduplicate the code in all the services. It is also complex to keep data in sync across all \nmicroservices. This approach makes sense if the code base and data is simple or the \ndata is more static.\nYet another approach is similar to the first approach, but each service has a local near \ncache of the required data, which will be loaded incrementally. A local embedded \ncache such as Ehcache or data grids like Hazelcast or Infinispan could also be used \nbased on the data volumes. This is the most preferred approach for a large number  \nof microservices that have dependency on the master data.\n",
      "content_length": 1241,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "Applying Microservices Concepts\n[ 138 ]\nMicroservices and bulk operations\nSince we have broken monolithic applications into smaller, focused services, it is no \nlonger possible to use join queries across microservice data stores. This could lead to \nsituations where one service may need many records from other services to perform \nits function.\nFor example, a monthly billing function needs the invoices of many customers \nto process the billing. To make it a bit more complicated, invoices may have \nmany orders. When we break billing, invoices, and orders into three different \nmicroservices, the challenge that arises is that the Billing service has to query the \nInvoices service for each customer to get all the invoices, and then for each invoice, \ncall the Order service for getting the orders. This is not a good solution, as the \nnumber of calls that goes to other microservices are high:\nThere are two ways we can think about for solving this. The first approach is to \npre-aggregate data as and when it is created. When an order is created, an event \nis sent out. Upon receiving the event, the Billing microservice keeps aggregating \ndata internally for monthly processing. In this case, there is no need for the Billing \nmicroservice to go out for processing. The downside of this approach is that there is \nduplication of data.\n",
      "content_length": 1343,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "Chapter 3\n[ 139 ]\nA second approach, when pre-aggregation is not possible, is to use batch APIs. In \nsuch cases, we call GetAllInvoices, then we use multiple batches, and each batch \nfurther uses parallel threads to get orders. Spring Batch is useful in these situations.\nMicroservices challenges\nIn the previous section, you learned about the right design decisions to be taken, and \nthe trade-offs to be applied. In this section, we will review some of the challenges with \nmicroservices, and how to address them for a successful microservice development.\nData islands\nMicroservices abstract their own local transactional store, which is used for their own \ntransactional purposes. The type of store and the data structure will be optimized for \nthe services offered by the microservice.\nFor example, if we want to develop a customer relationship graph, we may use a \ngraph database like Neo4j, OrientDB, and the like. A predictive text search to find \nout a customer based on any related information such as passport number, address, \ne-mail, phone, and so on could be best realized using an indexed search database  \nlike Elasticsearch or Solr.\nThis will place us into a unique situation of fragmenting data into heterogeneous \ndata islands. For example, Customer, Loyalty Points, Reservations, and others are \ndifferent microservices, and hence, use different databases. What if we want to do a \nnear real-time analysis of all high value customers by combining data from all three \ndata stores? This was easy with a monolithic application, because all the data was \npresent in a single database:\n",
      "content_length": 1601,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "Applying Microservices Concepts\n[ 140 ]\nIn order to satisfy this requirement, a data warehouse or a data lake is required. \nTraditional data warehouses like Oracle, Teradata, and others are used primarily \nfor batch reporting. But with NoSQL databases (like Hadoop) and microbatching \ntechniques, near real-time analytics is possible with the concept of data lakes. Unlike \nthe traditional warehouses that are purpose-built for batch reporting, data lakes \nstore raw data without assuming how the data is going to be used. Now the question \nreally is how to port the data from microservices into data lakes.\nData porting from microservices to a data lake or a data warehouse can be done in \nmany ways. Traditional ETL could be one of the options. Since we allow backdoor \nentry with ETL, and break the abstraction, this is not considered an effective way for \ndata movement. A better approach is to send events from microservices as and when \nthey occur, for example, customer registration, customer update events, and so on. \nData ingestion tools consume these events, and propagate the state change to the \ndata lake appropriately. The data ingestion tools are highly scalable platforms such \nas Spring Cloud Data Flow, Kafka, Flume, and so on.\nLogging and monitoring\nLog files are a good piece of information for analysis and debugging. Since each \nmicroservice is deployed independently, they emit separate logs, maybe to a local disk. \nThis results in fragmented logs. When we scale services across multiple machines, each \nservice instance could produce separate log files. This makes it extremely difficult to \ndebug and understand the behavior of the services through log mining.\nExamining Order, Delivery, and Notification as three different microservices, we \nfind no way to correlate a customer transaction that runs across all three of them:\n",
      "content_length": 1854,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "Chapter 3\n[ 141 ]\nWhen implementing microservices, we need a capability to ship logs from each \nservice to a centrally managed log repository. With this approach, services do not \nhave to rely on the local disk or local I/Os. A second advantage is that the log files \nare centrally managed, and are available for all sorts of analysis such as historical, \nreal time, and trending. By introducing a correlation ID, end-to-end transactions  \ncan be easily tracked.\nWith a large number of microservices, and with multiple versions and service \ninstances, it would be difficult to find out which service is running on which server, \nwhat's the health of these services, the service dependencies, and so on. This was \nmuch easier with monolithic applications that are tagged against a specific or a  \nfixed set of servers.\nApart from understanding the deployment topology and health, it also poses a \nchallenge in identifying service behaviors, debugging, and identifying hotspots. \nStrong monitoring capabilities are required to manage such an infrastructure.\nWe will cover the logging and monitoring aspects in Chapter 7, Logging and Monitoring \nMicroservices.\nDependency management\nDependency management is one of the key issues in large microservice deployments. \nHow do we identify and reduce the impact of a change? How do we know whether \nall the dependent services are up and running? How will the service behave if one  \nof the dependent services is not available?\nToo many dependencies could raise challenges in microservices. Four important \ndesign aspects are stated as follows:\n•\t\nReducing dependencies by properly designing service boundaries.\n•\t\nReducing impacts by designing dependencies as loosely coupled as possible. \nAlso, designing service interactions through asynchronous communication \nstyles.\n•\t\nTackling dependency issues using patterns such as circuit breakers.\n•\t\nMonitoring dependencies using visual dependency graphs.\n",
      "content_length": 1943,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "Applying Microservices Concepts\n[ 142 ]\nOrganization culture\nOne of the biggest challenges in microservices implementation is the organization \nculture. To harness the speed of delivery of microservices, the organization should \nadopt Agile development processes, continuous integration, automated QA checks, \nautomated delivery pipelines, automated deployments, and automatic infrastructure \nprovisioning.\nOrganizations following a waterfall development or heavyweight release management \nprocesses with infrequent release cycles are a challenge for microservices development. \nInsufficient automation is also a challenge for microservices deployment.\nIn short, Cloud and DevOps are supporting facets of microservice development. \nThese are essential for successful microservices implementation.\nGovernance challenges\nMicroservices impose decentralized governance, and this is quite in contrast with  \nthe traditional SOA governance. Organizations may find it hard to come up with  \nthis change, and that could negatively impact the microservices development.\nThere are number of challenges that comes with a decentralized governance model. \nHow do we understand who is consuming a service? How do we ensure service \nreuse? How do we define which services are available in the organization? How  \ndo we ensure enforcement of enterprise polices?\nThe first thing is to have a set of standards, best practices, and guidelines on how to \nimplement better services. These should be available to the organization in the form \nof standard libraries, tools, and techniques. This ensures that the services developed \nare top quality, and that they are developed in a consistent manner.\nThe second important consideration is to have a place where all stakeholders  \ncan not only see all the services, but also their documentations, contracts, and \nservice-level agreements. Swagger and API Blueprint are commonly used for \nhandling these requirements.\nOperation overheads\nMicroservices deployment generally increases the number of deployable units and \nvirtual machines (or containers). This adds significant management overheads and \nincreases the cost of operations.\n",
      "content_length": 2160,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "Chapter 3\n[ 143 ]\nWith a single application, a dedicated number of containers or virtual machines in \nan on-premise data center may not make much sense unless the business benefit is \nvery high. The cost generally goes down with economies of scale. A large number of \nmicroservices that are deployed in a shared infrastructure which is fully automated \nmakes more sense, since these microservices are not tagged against any specific \nVMs or containers. Capabilities around infrastructure automation, provisioning, \ncontainerized deployment, and so on are essential for large scale microservices \ndeployments. Without this automation, it would result in a significant operation \noverhead and increased cost.\nWith many microservices, the number of configurable items (CIs) becomes too \nhigh, and the number of servers in which these CIs are deployed might also be \nunpredictable. This makes it extremely difficult to manage data in a traditional \nConfiguration Management Database (CMDB). In many cases, it is more useful \nto dynamically discover the current running topology than a statically configured \nCMDB-style deployment topology.\nTesting microservices\nMicroservices also pose a challenge for the testability of services. In order to achieve \na full-service functionality, one service may rely on another service, and that, in turn, \non another service—either synchronously or asynchronously. The issue is how do we \ntest an end-to-end service to evaluate its behavior? The dependent services may or \nmay not be available at the time of testing.\nService virtualization or service mocking is one of the techniques used for testing \nservices without actual dependencies. In testing environments, when the services \nare not available, mock services can simulate the behavior of the actual service. The \nmicroservices ecosystem needs service virtualization capabilities. However, this may \nnot give full confidence, as there may by many corner cases that mock services do \nnot simulate, especially when there are deep dependencies.\nAnother approach, as discussed earlier, is to use a consumer driven contract.  \nThe translated integration test cases can cover more or less all corner cases of the  \nservice invocation.\nTest automation, appropriate performance testing, and continuous delivery \napproaches such as A/B testing, future flags, canary testing, blue-green deployments, \nand red-black deployments, all reduce the risks of production releases.\n",
      "content_length": 2454,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "Applying Microservices Concepts\n[ 144 ]\nInfrastructure provisioning\nAs briefly touched on under operation overheads, manual deployment could \nseverely challenge the microservices rollouts. If a deployment has manual elements, \nthe deployer or operational administrators should know the running topology, \nmanually reroute traffic, and then deploy the application one by one till all services \nare upgraded. With many server instances running, this could lead to significant \noperational overheads. Moreover, the chances of errors are high in this manual \napproach.\nMicroservices require a supporting elastic cloud-like infrastructure which can \nautomatically provision VMs or containers, automatically deploy applications,  \nadjust traffic flows, replicate new version to all instances, and gracefully phase  \nout older versions. The automation also takes care of scaling up elastically by  \nadding containers or VMs on demand, and scaling down when the load falls  \nbelow threshold.\nIn a large deployment environment with many microservices, we may also need \nadditional tools to manage VMs or containers that can further initiate or destroy \nservices automatically.\nThe microservices capability model\nBefore we conclude this chapter, we will review a capability model for microservices \nbased on the design guidelines and common pattern and solutions described in  \nthis chapter.\nThe following diagram depicts the microservices capability model:\n",
      "content_length": 1448,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "Chapter 3\n[ 145 ]\nThe capability model is broadly classified in to four areas:\n•\t\nCore capabilities: These are part of the microservices themselves\n•\t\nSupporting capabilities: These are software solutions supporting core \nmicroservice implementations\n•\t\nInfrastructure capabilities: These are infrastructure level expectations for  \na successful microservices implementation\n•\t\nGovernance capabilities: These are more of process, people, and reference \ninformation\nCore capabilities\nThe core capabilities are explained as follows:\n•\t\nService listeners (HTTP/messaging): If microservices are enabled for a \nHTTP-based service endpoint, then the HTTP listener is embedded within \nthe microservices, thereby eliminating the need to have any external \napplication server requirement. The HTTP listener is started at the time \nof the application startup. If the microservice is based on asynchronous \ncommunication, then instead of an HTTP listener, a message listener is \nstarted. Optionally, other protocols could also be considered. There may not \nbe any listeners if the microservice is a scheduled service. Spring Boot and \nSpring Cloud Streams provide this capability.\n•\t\nStorage capability: The microservices have some kind of storage mechanisms \nto store state or transactional data pertaining to the business capability. This \nis optional, depending on the capabilities that are implemented. The storage \ncould be either a physical storage (RDBMS such as MySQL; NoSQL such  \nas Hadoop, Cassandra, Neo 4J, Elasticsearch, and so on), or it could be an  \nin-memory store (cache like Ehcache, data grids like Hazelcast, Infinispan, \nand so on)\n•\t\nBusiness capability definition: This is the core of microservices, where  \nthe business logic is implemented. This could be implemented in any \napplicable language such as Java, Scala, Conjure, Erlang, and so on. All \nrequired business logic to fulfill the function will be embedded within  \nthe microservices themselves.\n•\t\nEvent sourcing: Microservices send out state changes to the external world \nwithout really worrying about the targeted consumers of these events. \nThese events could be consumed by other microservices, audit services, \nreplication services, or external applications, and the like. This allows other \nmicroservices and applications to respond to state changes.\n",
      "content_length": 2332,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "Applying Microservices Concepts\n[ 146 ]\n•\t\nService endpoints and communication protocols: These define the APIs \nfor external consumers to consume. These could be synchronous endpoints \nor asynchronous endpoints. Synchronous endpoints could be based on \nREST/JSON or any other protocols such as Avro, Thrift, Protocol Buffers, \nand so on. Asynchronous endpoints are through Spring Cloud Streams \nbacked by RabbitMQ, other messaging servers, or other messaging style \nimplementations such as ZeroMQ.\n•\t\nAPI gateway: The API gateway provides a level of indirection by either \nproxying service endpoints or composing multiple service endpoints. The \nAPI gateway is also useful for policy enforcements. It may also provide real \ntime load balancing capabilities. There are many API gateways available \nin the market. Spring Cloud Zuul, Mashery, Apigee, and 3scale are some \nexamples of the API gateway providers.\n•\t\nUser interfaces: Generally, user interfaces are also part of microservices for \nusers to interact with the business capabilities realized by the microservices. \nThese could be implemented in any technology, and are channel- and  \ndevice-agnostic.\nInfrastructure capabilities\nCertain infrastructure capabilities are required for a successful deployment, and \nmanaging large scale microservices. When deploying microservices at scale, not \nhaving proper infrastructure capabilities can be challenging, and can lead to failures:\n•\t\nCloud: Microservices implementation is difficult in a traditional data center \nenvironment with long lead times to provision infrastructures. Even a large \nnumber of infrastructures dedicated per microservice may not be very cost \neffective. Managing them internally in a data center may increase the cost  \nof ownership and cost of operations. A cloud-like infrastructure is better  \nfor microservices deployment.\n•\t\nContainers or virtual machines: Managing large physical machines is not \ncost effective, and they are also hard to manage. With physical machines, it \nis also hard to handle automatic fault tolerance. Virtualization is adopted by \nmany organizations because of its ability to provide optimal use of physical \nresources. It also provides resource isolation. It also reduces the overheads in \nmanaging large physical infrastructure components. Containers are the next \ngeneration of virtual machines. VMWare, Citrix, and so on provide virtual \nmachine technologies. Docker, Drawbridge, Rocket, and LXD are some of  \nthe containerizer technologies.\n",
      "content_length": 2505,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "Chapter 3\n[ 147 ]\n•\t\nCluster control and provisioning: Once we have a large number of \ncontainers or virtual machines, it is hard to manage and maintain \nthem automatically. Cluster control tools provide a uniform operating \nenvironment on top of the containers, and share the available capacity across \nmultiple services. Apache Mesos and Kubernetes are examples of cluster \ncontrol systems.\n•\t\nApplication lifecycle management: Application lifecycle management \ntools help to invoke applications when a new container is launched, or \nkill the application when the container shuts down. Application life cycle \nmanagement allows for script application deployments and releases. It \nautomatically detects failure scenario, and responds to those failures thereby \nensuring the availability of the application. This works in conjunction with \nthe cluster control software. Marathon partially addresses this capability.\nSupporting capabilities\nSupporting capabilities are not directly linked to microservices, but they are essential \nfor large scale microservices development:\n•\t\nSoftware defined load balancer: The load balancer should be smart enough \nto understand the changes in the deployment topology, and respond \naccordingly. This moves away from the traditional approach of configuring \nstatic IP addresses, domain aliases, or cluster addresses in the load balancer. \nWhen new servers are added to the environment, it should automatically \ndetect this, and include them in the logical cluster by avoiding any manual \ninteractions. Similarly, if a service instance is unavailable, it should take it out \nfrom the load balancer. A combination of Ribbon, Eureka, and Zuul provide \nthis capability in Spring Cloud Netflix.\n•\t\nCentral log management: As explored earlier in this chapter, a capability is \nrequired to centralize all logs emitted by service instances with the correlation \nIDs. This helps in debugging, identifying performance bottlenecks, and \npredictive analysis. The result of this is fed back into the life cycle manager  \nto take corrective actions.\n•\t\nService registry: A service registry provides a runtime environment for \nservices to automatically publish their availability at runtime. A registry \nwill be a good source of information to understand the services topology at \nany point. Eureka from Spring Cloud, Zookeeper, and Etcd are some of the \nservice registry tools available.\n",
      "content_length": 2409,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "Applying Microservices Concepts\n[ 148 ]\n•\t\nSecurity service: A distributed microservices ecosystem requires a central \nserver for managing service security. This includes service authentication \nand token services. OAuth2-based services are widely used for microservices \nsecurity. Spring Security and Spring Security OAuth are good candidates for \nbuilding this capability.\n•\t\nService configuration: All service configurations should be externalized as \ndiscussed in the Twelve-Factor application principles. A central service for \nall configurations is a good choice. Spring Cloud Config server, and Archaius \nare out-of-the-box configuration servers.\n•\t\nTesting tools (anti-fragile, RUM, and so on): Netflix uses Simian Army for \nanti-fragile testing. Matured services need consistent challenges to see the \nreliability of the services, and how good fallback mechanisms are. Simian \nArmy components create various error scenarios to explore the behavior of \nthe system under failure scenarios.\n•\t\nMonitoring and dashboards: Microservices also require a strong monitoring \nmechanism. This is not just at the infrastructure-level monitoring but also \nat the service level. Spring Cloud Netflix Turbine, Hysterix Dashboard, and \nthe like provide service level information. End-to-end monitoring tools like \nAppDynamic, New Relic, Dynatrace, and other tools like statd, Sensu, and \nSpigo could add value to microservices monitoring.\n•\t\nDependency and CI management: We also need tools to discover runtime \ntopologies, service dependencies, and to manage configurable items. A \ngraph-based CMDB is the most obvious tool to manage these scenarios.\n•\t\nData lake: As discussed earlier in this chapter, we need a mechanism to \ncombine data stored in different microservices, and perform near real-time \nanalytics. A data lake is a good choice for achieving this. Data ingestion tools \nlike Spring Cloud Data Flow, Flume, and Kafka are used to consume data. \nHDFS, Cassandra, and the like are used for storing data.\n•\t\nReliable messaging: If the communication is asynchronous, we may need \na reliable messaging infrastructure service such as RabbitMQ or any other \nreliable messaging service. Cloud messaging or messaging as a service is a \npopular choice in Internet scale message-based service endpoints.\nProcess and governance capabilities\nThe last piece in the puzzle is the process and governance capabilities that are \nrequired for microservices:\n•\t\nDevOps: The key to successful implementation of microservices is to adopt \nDevOps. DevOps compliment microservices development by supporting \nAgile development, high velocity delivery, automation, and better change \nmanagement.\n",
      "content_length": 2677,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "Chapter 3\n[ 149 ]\n•\t\nDevOps tools: DevOps tools for Agile development, continuous integration, \ncontinuous delivery, and continuous deployment are essential for successful \ndelivery of microservices. A lot of emphasis is required on automated \nfunctioning, real user testing, synthetic testing, integration, release, and \nperformance testing.\n•\t\nMicroservices repository: A microservices repository is where the versioned \nbinaries of microservices are placed. These could be a simple Nexus \nrepository or a container repository such as a Docker registry.\n•\t\nMicroservice documentation: It is important to have all microservices \nproperly documented. Swagger or API Blueprint are helpful in achieving \ngood microservices documentation.\n•\t\nReference architecture and libraries: The reference architecture provides a \nblueprint at the organization level to ensure that the services are developed \naccording to certain standards and guidelines in a consistent manner. Many \nof these could then be translated to a number of reusable libraries that \nenforce service development philosophies.\nSummary\nIn this chapter, you learned about handling practical scenarios that will arise in \nmicroservices development.\nYou learned various solution options and patterns that could be applied to \nsolve common microservices problems. We reviewed a number of challenges \nwhen developing large scale microservices, and how to address those challenges \neffectively.\nWe also built a capability reference model for a microservices-based ecosystem. \nThe capability model helps in addressing gaps when building Internet scale \nmicroservices. The capability model learned in this chapter will be the backbone  \nfor this book. The remaining chapters will deep dive into the capability model.\nIn the next chapter, we will take a real-world problem and model it using the \nmicroservices architecture to see how to translate our learnings into practice.\n",
      "content_length": 1927,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "[ 151 ]\nMicroservices Evolution – A \nCase Study\nLike SOA, a microservices architecture can be interpreted differently by different \norganizations, based on the problem in hand. Unless a sizable, real world problem  \nis examined in detail, microservices concepts are hard to understand.\nThis chapter will introduce BrownField Airline (BF), a fictitious budget airline, and \ntheir journey from a monolithic Passenger Sales and Service (PSS) application \nto a next generation microservices architecture. This chapter examines the PSS \napplication in detail, and explains the challenges, approach, and transformation \nsteps of a monolithic system to a microservices-based architecture, adhering to the \nprinciples and practices that were explained in the previous chapter.\nThe intention of this case study is to get us as close as possible to a live scenario so \nthat the architecture concepts can be set in stone.\nBy the end of this chapter, you will have learned about the following:\n•\t\nA real world case for migrating monolithic systems to microservices-based \nones, with the BrownField Airline's PSS application as an example\n•\t\nVarious approaches and transition strategies for migrating a monolithic \napplication to microservices\n•\t\nDesigning a new futuristic microservices system to replace the PSS \napplication using Spring Framework components\n",
      "content_length": 1348,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "Microservices Evolution – A Case Study\n[ 152 ]\nReviewing the microservices capability \nmodel\nThe examples in this chapter explore the following microservices capabilities  \nfrom the microservices capability model discussed in Chapter 3, Applying \nMicroservices Concepts:\n•\t\nHTTP Listener\n•\t\nMessage Listener\n•\t\nStorage Capabilities (Physical/In-Memory)\n•\t\nBusiness Capability Definitions\n•\t\nService Endpoints & Communication Protocols\n•\t\nUser Interfaces\n•\t\nSecurity Service\n•\t\nMicroservice Documentation\n",
      "content_length": 504,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "Chapter 4\n[ 153 ]\nIn Chapter 2, Building Microservices with Spring Boot, we explored all these capabilities \nin isolation including how to secure Spring Boot microservices. This chapter will \nbuild a comprehensive microservices example based on a real world case study.\nThe full source code of this chapter is available under the \nChapter 4 projects in the code files.\nUnderstanding the PSS application\nBrownField Airline is one of the fastest growing low-cost, regional airlines,  \nflying directly to more than 100 destinations from its hub. As a start-up airline, \nBrownField Airline started its operations with few destinations and few aircrafts. \nBrownField developed its home-grown PSS application to handle their passenger \nsales and services.\nBusiness process view\nThis use case is considerably simplified for discussion purposes. The process view \nin the following diagram shows BrownField Airline's end-to-end passenger services \noperations covered by the current PSS solution:\nThe current solution is automating certain customer-facing functions as well as \ncertain internally facing functions. There are two internally facing functions,  \nPre-flight and Post-flight. Pre-flight functions include the planning phase, used for \npreparing flight schedules, plans, aircrafts, and so on. Post-flight functions are used \nby the back office for revenue management, accounting, and so on. The Search and \nReserve functions are part of the online seat reservation process, and the Check-in \nfunction is the process of accepting passengers at the airport. The Check-in function \nis also accessible to the end users over the Internet for online check-in.\n",
      "content_length": 1655,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "Microservices Evolution – A Case Study\n[ 154 ]\nThe cross marks at the beginning of the arrows in the preceding diagram indicate \nthat they are disconnected, and occur at different timelines. For example, passengers \nare allowed to book 360 days in advance, whereas the check-in generally happens 24 \nhours before flight departure.\nFunctional view\nThe following diagram shows the functional building blocks of BrownField Airline's \nPSS landscape. Each business process and its related subfunctions are represented in \na row:\nEach subfunction shown in the preceding diagram explains its role in the overall \nbusiness process. Some subfunctions participate in more than one business process. \nFor example, inventory is used in both search as well as in booking. To avoid any \ncomplication, this is not shown in the diagram. Data management and cross-cutting \nsubfunctions are used across many business functions.\nArchitectural view\nIn order to effectively manage the end-to-end passenger operations, BrownField had \ndeveloped an in-house PSS application, almost ten years back. This well-architected \napplication was developed using Java and JEE technologies combined with the best-\nof-the-breed open source technologies available at the time.\n",
      "content_length": 1241,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "Chapter 4\n[ 155 ]\nThe overall architecture and technologies are shown in the following diagram:\nThe architecture has well-defined boundaries. Also, different concerns are separated \ninto different layers. The web application was developed as an N-tier, component-\nbased modular system. The functions interact with each other through well-defined \nservice contracts defined in the form of EJB endpoints.\nDesign view\nThe application has many logical functional groupings or subsystems. Further, each \nsubsystem has many components organized as depicted in the next diagram:\nSubsystems interact with each other through remote EJB calls using the IIOP \nprotocol. The transactional boundaries span across subsystems. Components \nwithin the subsystems communicate with each other through local EJB component \ninterfaces. In theory, since subsystems use remote EJB endpoints, they could run on \ndifferent physically separated application servers. This was one of the design goals.\n",
      "content_length": 974,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "Microservices Evolution – A Case Study\n[ 156 ]\nImplementation view\nThe implementation view in the following diagram showcases the internal \norganization of a subsystem and its components. The purpose of the diagram is also \nto show the different types of artifacts:\nIn the preceding diagram, the gray-shaded boxes are treated as different Maven \nprojects, and translate into physical artifacts. Subsystems and components are \ndesigned adhering to the program to an interface principle. Interfaces are packaged \nas separate JAR files so that clients are abstracted from the implementations. The \ncomplexity of the business logic is buried in the domain model. Local EJBs are used \nas component interfaces. Finally, all subsystems are packaged into a single all-in-one \nEAR, and deployed in the application server.\n",
      "content_length": 813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "Chapter 4\n[ 157 ]\nDeployment view\nThe application's initial deployment was simple and straightforward as shown in  \nthe next diagram:\nThe web modules and business modules were deployed into separate application \nserver clusters. The application was scaled horizontally by adding more and more \napplication servers to the cluster.\nZero downtime deployments were handled by creating a standby cluster, and \ngracefully diverting the traffic to that cluster. The standby cluster is destroyed once \nthe primary cluster is patched with the new version and brought back to service. \nMost of the database changes were designed for backward compatibility, but \nbreaking changes were promoted with application outages.\n",
      "content_length": 709,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "Microservices Evolution – A Case Study\n[ 158 ]\nDeath of the monolith\nThe PSS application was performing well, successfully supporting all business \nrequirements as well as the expected service levels. The system had no issues in \nscaling with the organic growth of the business in the initial years.\nThe business has seen tremendous growth over a period of time. The fleet size \nincreased significantly, and new destinations got added to the network. As a result of \nthis rapid growth, the number of bookings has gone up, resulting in a steep increase \nin transaction volumes, up to 200 - to 500 - fold of what was originally estimated.\nPain points\nThe rapid growth of the business eventually put the application under pressure. Odd \nstability issues and performance issues surfaced. New application releases started \nbreaking the working code. Moreover, the cost of change and the speed of delivery \nstarted impacting the business operations profoundly.\nAn end-to-end architecture review was ordered, and it exposed the weaknesses of \nthe system as well as the root causes of many failures, which were as follows:\n•\t\nStability: The stability issues are primarily due to stuck threads, which limit \nthe application server's capability to accept more transactions. The stuck \nthreads are mainly due to database table locks. Memory issues are another \ncontributor to the stability issues. There were also issues in certain resource \nintensive operations that were impacting the whole application.\n•\t\nOutages: The outage window increased largely because of the increase in \nserver startup time. The root cause of this issue boiled down to the large size \nof the EAR. Message pile up during any outage windows causes heavy usage \nof the application immediately after an outage window. Since everything \nis packaged in a single EAR, any small application code change resulted in \nfull redeployment. The complexity of the zero downtime deployment model \ndescribed earlier, together with the server startup times increased both the \nnumber of outages and their duration.\n•\t\nAgility: The complexity of the code also increased considerably over time, \npartially due to the lack of discipline in implementing the changes. As a \nresult, changes became harder to implement. Also, the impact analysis \nbecame too complex to perform. As a result, inaccurate impact analysis often \nled to fixes that broke the working code. The application build time went \nup severely, from a few minutes to hours, causing unacceptable drops in \ndevelopment productivity. The increase in build time also led to difficulty in \nbuild automation, and eventually stopped continuous integration (CI) and \nunit testing.\n",
      "content_length": 2683,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "Chapter 4\n[ 159 ]\nStop gap fix\nPerformance issues were partially addressed by applying the Y-axis scale method \nin the scale cube, as described in Chapter 1, Demystifying Microservices. The all-\nencompassing EAR is deployed into multiple disjoint clusters. A software proxy \nwas installed to selectively route the traffic to designated clusters as shown in the \nfollowing diagram:\nThis helped BrownField's IT to scale the application servers. Therefore, the stability \nissues were controlled. However, this soon resulted in a bottleneck at the database \nlevel. Oracle's Real Application Cluster (RAC) was implemented as a solution to \nthis problem at the database layer.\nThis new scaling model reduced the stability issues, but at a premium of increased \ncomplexity and cost of ownership. The technology debt also increased over a period \nof time, leading to a state where a complete rewrite was the only option for reducing \nthis technology debt.\n",
      "content_length": 948,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "Microservices Evolution – A Case Study\n[ 160 ]\nRetrospection\nAlthough the application was well-architected, there was a clear segregation between \nthe functional components. They were loosely coupled, programmed to interfaces, \nwith access through standards-based interfaces, and had a rich domain model.\nThe obvious question is, how come such a well-architected application failed to live \nup to the expectations? What else could the architects have done?\nIt is important to understand what went wrong over a period of time. In the context \nof this book, it is also important to understand how microservices can avoid the \nrecurrence of these scenarios. We will examine some of these scenarios in the \nsubsequent sections.\nShared data\nAlmost all functional modules require reference data such as the airline's details, \nairplane details, a list of airports and cities, countries, currencies, and so on. For \nexample, fare is calculated based on the point of origin (city), a flight is between an \norigin and a destination (airports), check-in is at the origin airport (airport), and so \non. In some functions, the reference data is a part of the information model, whereas \nin some other functions, it is used for validation purposes.\nMuch of this reference data is neither fully static nor fully dynamic. Addition of \na country, city, airport, or the like could happen when the airline introduces new \nroutes. Aircraft reference data could change when the airline purchases a new \naircraft, or changes an existing airplane's seat configuration.\nOne of the common usage scenarios of reference data is to filter the operational data \nbased on certain reference data. For instance, say a user wishes to see all the flights to \na country. In this case, the flow of events could be as follows: find all the cities in the \nselected country, then all airports in the cities, and then fire a request to get all the \nflights to the list of resulting airports identified in that country.\nThe architects considered multiple approaches when designing the system. \nSeparating the reference data as an independent subsystem like other subsystems \nwas one of the options considered, but this could lead to performance issues. The \nteam took the decision to follow an exception approach for handling reference data \nas compared to other transactions. Considering the nature of the query patterns \ndiscussed earlier, the approach was to use the reference data as a shared library. \n",
      "content_length": 2467,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "Chapter 4\n[ 161 ]\nIn this case, the subsystems were allowed to access the reference data directly using \npass-by-reference semantic data instead of going through the EJB interfaces. This also \nmeant that irrespective of the subsystems, hibernate entities could use the reference \ndata as a part of their entity relationships:\nAs depicted in the preceding diagram, the Booking entity in the reservation \nsubsystem is allowed to use the reference data entities, in this case Airport,  \nas part of their relationships.\nSingle database\nThough enough segregation was enforced at the middle tier, all functions pointed to \na single database, even to the same database schema. The single schema approach \nopened a plethora of issues.\nNative queries\nThe Hibernate framework provides a good abstraction over the underlying \ndatabases. It generates efficient SQL statements, in most of the cases targeting the \ndatabase using specific dialects. However, sometimes, writing native JDBC SQLs \noffers better performance and resource efficiency. In some cases, using native \ndatabase functions gives an even better performance.\nThe single database approach worked well at the beginning. But over a period of \ntime, it opened up a loophole for the developers by connecting database tables \nowned by different subsystems. Native JDBC SQL was a good vehicle for doing this.\n",
      "content_length": 1357,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "Microservices Evolution – A Case Study\n[ 162 ]\nThe following diagram shows an example of connecting two tables owned by two \nsubsystems using a native JDBC SQL:\nAs shown in the preceding diagram, the Accounting component requires all \nbooking records for a day, for a given city, from the Booking component to process \nthe day-end billing. The subsystem-based design enforces Accounting to make \na service call to Booking to get all booking records for a given city. Assume this \nresults in N booking records. Now, for each booking record, Accounting has to \nexecute a database call to find the applicable rules based on the fare code attached \nto each booking record. This could result in N+1 JDBC calls, which is inefficient. \nWorkarounds, such as batch queries or parallel and batch executions, are available, \nbut this would lead to increased coding efforts and higher complexity. The \ndevelopers tackled this issue with a native JDBC query as an easy-to-implement \nshortcut. Essentially, this approach could reduce the number of calls from N+1 to a \nsingle database call, with minimal coding efforts.\nThis habit continued with many JDBC native queries connecting tables across \nmultiple components and subsystems. This resulted not only in tightly coupled \ncomponents, but also led to undocumented, hard-to-detect code.\n",
      "content_length": 1325,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "Chapter 4\n[ 163 ]\nStored procedures\nAnother issue that surfaced as a result of the use of a single database was the use of \ncomplex stored procedures. Some of the complex data-centric logic written at the \nmiddle layer was not performing well, causing slow response, memory issues, and \nthread-blocking issues.\nIn order to address this problem, the developers took the decision to move some of \nthe complex business logic from the middle tier to the database tier by implementing \nthe logic directly within the stored procedures. This decision resulted in better \nperformance of some of the transactions, and removed some of the stability issues. \nMore and more procedures were added over a period of time. However, this \neventually broke the application's modularity.\nDomain boundaries\nThough the domain boundaries were well established, all the components were \npackaged as a single EAR file. Since all the components were set to run on a single \ncontainer, there was no stopping the developers referencing objects across these \nboundaries. Over a period of time, the project teams changed, delivery pressure \nincreased, and the complexity grew tremendously. The developers started looking \nfor quick solutions rather than the right ones. Slowly, but steadily, the modular \nnature of the application went away.\nAs depicted in the following diagram, hibernate relationships were created across \nsubsystem boundaries:\n",
      "content_length": 1418,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "Microservices Evolution – A Case Study\n[ 164 ]\nMicroservices to the rescue\nThere are not many improvement opportunities left to support the growing demand \nof BrownField Airline's business. BrownField Airline was looking to re-platform the \nsystem with an evolutionary approach rather than a revolutionary model.\nMicroservices is an ideal choice in these situations—for transforming a legacy \nmonolithic application with minimal disruption to the business:\nAs shown in the preceding diagram, the objective is to move to a microservices-\nbased architecture aligned to the business capabilities. Each microservice will hold \nthe data store, the business logic, and the presentation layer.\nThe approach taken by BrownField Airline is to build a number of web portal \napplications targeting specific user communities such as customer facing, front office, \nand back office. The advantage of this approach lies in the flexibility for modeling, \nand also in the possibility to treat different communities differently. For example, \nthe policies, architecture, and testing approaches for the Internet facing layer are \ndifferent from the intranet-facing web application. Internet-facing applications may \ntake advantage of CDNs (Content Delivery Networks) to move pages as close to the \ncustomer as possible, whereas intranet applications could serve pages directly from \nthe data center.\n",
      "content_length": 1382,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "Chapter 4\n[ 165 ]\nThe business case\nWhen building business cases for migration, one of the commonly asked questions \nis \"how does the microservices architecture avoid resurfacing of the same issues in \nanother five years' time?\"\nMicroservices offers a full list of benefits, which you learned in Chapter 1, \nDemystifying Microservices, but it is important to list a few here that are critical  \nin this situation:\n•\t\nService dependencies: While migrating from monolithic applications to \nmicroservices, the dependencies are better known, and therefore the architects \nand developers are much better placed to avoid breaking dependencies and \nto future-proof dependency issues. Lessons from the monolithic application \nhelps architects and developers to design a better system.\n•\t\nPhysical boundaries: Microservices enforce physical boundaries in all areas \nincluding the data store, the business logic, and the presentation layer. \nAccess across subsystems or microservices are truly restricted due to their \nphysical isolation. Beyond the physical boundaries, they could even run on \ndifferent technologies.\n•\t\nSelective scaling: Selective scale out is possible in microservices architecture. \nThis provides a much more cost-effective scaling mechanism compared to \nthe Y-scale approach used in the monolithic scenario.\n•\t\nTechnology obsolescence: Technology migrations could be applied at a \nmicroservices level rather than at the overall application level. Therefore,  \nit does not require a humongous investment.\nPlan the evolution\nIt is not simple to break an application that has millions of lines of code, especially if \nthe code has complex dependencies. How do we break it? More importantly, where \ndo we start, and how do we approach this problem?\n",
      "content_length": 1758,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "Microservices Evolution – A Case Study\n[ 166 ]\nEvolutionary approach\nThe best way to address this problem is to establish a transition plan, and gradually \nmigrate the functions as microservices. At every step, a microservice will be created \noutside of the monolithic application, and traffic will be diverted to the new service \nas shown in the following diagram:\nIn order to run this migration successfully, a number of key questions need to be \nanswered from the transition point of view:\n•\t\nIdentification of microservices' boundaries\n•\t\nPrioritizing microservices for migration\n•\t\nHandling data synchronization during the transition phase\n•\t\nHandling user interface integration, working with old and new user \ninterfaces\n",
      "content_length": 727,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "Chapter 4\n[ 167 ]\n•\t\nHandling of reference data in the new system\n•\t\nTesting strategy to ensure the business capabilities are intact and correctly \nreproduced\n•\t\nIdentification of any prerequisites for microservice development such as \nmicroservices capabilities, frameworks, processes, and so on \nIdentification of microservices boundaries\nThe first and foremost activity is to identify the microservices' boundaries. This \nis the most interesting part of the problem, and the most difficult part as well. If \nidentification of the boundaries is not done properly, the migration could lead to \nmore complex manageability issues.\nLike in SOA, a service decomposition is the best way to identify services. However, \nit is important to note that decomposition stops at a business capability or bounded \ncontext. In SOA, service decomposition goes further into an atomic, granular service \nlevel.\nA top-down approach is typically used for domain decomposition. The bottom-up \napproach is also useful in the case of breaking an existing system, as it can utilize \na lot of practical knowledge, functions, and behaviors of the existing monolithic \napplication.\nThe previous decomposition step will give a potential list of microservices. It is \nimportant to note that this isn't the final list of microservices, but it serves as a good \nstarting point. We will run through a number of filtering mechanisms to get to a \nfinal list. The first cut of functional decomposition will, in this case, be similar to the \ndiagram shown under the functional view introduced earlier in this chapter.\nAnalyze dependencies\nThe next step is to analyze the dependencies between the initial set of candidate \nmicroservices that we created in the previous section. At the end of this activity,  \na dependency graph will be produced.\nA team of architects, business analysts, developers, release \nmanagement and support staff is required for this exercise.\n",
      "content_length": 1932,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "Microservices Evolution – A Case Study\n[ 168 ]\nOne way to produce a dependency graph is to list out all the components of the \nlegacy system and overlay dependencies. This could be done by combining one or \nmore of the approaches listed as follows:\n•\t\nAnalyzing the manual code and regenerating dependencies.\n•\t\nUsing the experience of the development team to regenerate dependencies.\n•\t\nUsing a Maven dependency graph. There are a number of tools we could use \nto regenerate the dependency graph, such as PomExplorer, PomParser, and \nso on.\n•\t\nUsing performance engineering tools such as AppDynamics to identify the \ncall stack and roll up dependencies.\nLet us assume that we reproduce the functions and their dependencies as shown in \nthe following diagram:\n",
      "content_length": 760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "Chapter 4\n[ 169 ]\nThere are many dependencies going back and forth between different modules.  \nThe bottom layer shows cross-cutting capabilities that are used across multiple \nmodules. At this point, the modules are more like spaghetti than autonomous units.\nThe next step is to analyze these dependencies, and come up with a better, simplified \ndependency map.\nEvents as opposed to query \nDependencies could be query-based or event-based. Event-based is better for \nscalable systems. Sometimes, it is possible to convert query-based communications \nto event-based ones. In many cases, these dependencies exist because either the \nbusiness organizations are managed like that, or by virtue of the way the old system \nhandled the business scenario.\nFrom the previous diagram, we can extract the Revenue Management and the  \nFares services:\nRevenue Management is a module used for calculating optimal fare values, based \non the booking demand forecast. In case of a fare change between an origin and a \ndestination, Update Fare on the Fare module is called by Revenue Management to \nupdate the respective fares in the Fare module.\nAn alternate way of thinking is that the Fare module is subscribed to Revenue \nManagement for any changes in fares, and Revenue Management publishes \nwhenever there is a fare change. This reactive programming approach gives an \nadded flexibility by which the Fares and the Revenue Management modules could \nstay independent, and connect them through a reliable messaging system. This same \npattern could be applied in many other scenarios from Check-In to the Loyalty and \nBoarding modules.\nNext, examine the scenario of CRM and Booking:\n",
      "content_length": 1668,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "Microservices Evolution – A Case Study\n[ 170 ]\nThis scenario is slightly different from the previously explained scenario. The CRM \nmodule is used to manage passenger complaints. When CRM receives a complaint, \nit retrieves the corresponding passenger's Booking data. In reality, the number of \ncomplaints are negligibly small when compared to the number of bookings. If we \nblindly apply the previous pattern where CRM subscribes to all bookings, we will \nfind that it is not cost effective:\nExamine another scenario between the Check-in and Booking modules. Instead of \nCheck-in calling the Get Bookings service on Booking, can Check-in listen to booking \nevents? This is possible, but the challenge here is that a booking can happen 360 \ndays in advance, whereas Check-in generally starts only 24 hours before the fight \ndeparture. Duplicating all bookings and booking changes in the Check-in module \n360 days in advance would not be a wise decision as Check-in does not require this \ndata until 24 hours before the flight departure.\nAn alternate option is that when check-in opens for a flight (24 hours before \ndeparture), Check-in calls a service on the Booking module to get a snapshot of the \nbookings for a given flight. Once this is done, Check-in could subscribe for booking \nevents specifically for that flight. In this case, a combination of query-based as well as \nevent-based approaches is used. By doing so, we reduce the unnecessary events and \nstorage apart from reducing the number of queries between these two services.\nIn short, there is no single policy that rules all scenarios. Each scenario requires \nlogical thinking, and then the most appropriate pattern is applied.\nEvents as opposed to synchronous updates\nApart from the query model, a dependency could be an update transaction as well. \nConsider the scenario between Revenue Management and Booking:\n",
      "content_length": 1879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "Chapter 4\n[ 171 ]\nIn order to do a forecast and analysis of the current demand, Revenue Management \nrequires all bookings across all flights. The current approach, as depicted in the \ndependency graph, is that Revenue Management has a schedule job that calls Get \nBooking on Booking to get all incremental bookings (new and changed) since the last \nsynchronization.\nAn alternative approach is to send new bookings and the changes in bookings \nas soon as they take place in the Booking module as an asynchronous push. The \nsame pattern could be applied in many other scenarios such as from Booking to \nAccounting, from Flight to Inventory, and also from Flight to Booking. In this \napproach, the source service publishes all state-change events to a topic. All \ninterested parties could subscribe to this event stream and store locally. This \napproach removes many hard wirings, and keeps the systems loosely coupled.\nThe dependency is depicted in the next diagram:\nIn this case depicted in the preceding diagram, we changed both dependencies and \nconverted them to asynchronous events.\nOne last case to analyze is the Update Inventory call from the Booking module to the \nInventory module:\n",
      "content_length": 1190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "Microservices Evolution – A Case Study\n[ 172 ]\nWhen a booking is completed, the inventory status is updated by depleting the \ninventory stored in the Inventory service. For example, when there are 10 economy \nclass seats available, at the end of the booking, we have to reduce it to 9. In the \ncurrent system, booking and updating inventory are executed within the same \ntransaction boundaries. This is to handle a scenario in which there is only one seat \nleft, and multiple customers are trying to book. In the new design, if we apply the \nsame event-driven pattern, sending the inventory update as an event to Inventory \nmay leave the system in an inconsistent state. This needs further analysis, which we \nwill address later in this chapter.\nChallenge requirements\nIn many cases, the targeted state could be achieved by taking another look at the \nrequirements:\nThere are two Validate Flight calls, one from Booking and another one from the \nSearch module. The Validate Flight call is to validate the input flight data coming \nfrom different channels. The end objective is to avoid incorrect data stored or serviced. \nWhen a customer does a flight search, say \"BF100\", the system validates this flight to \nsee the following things:\n•\t\nWhether this is a valid flight?\n•\t\nWhether the flight exists on that particular date?\n•\t\nAre there any booking restrictions set on this flight?\n",
      "content_length": 1383,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "Chapter 4\n[ 173 ]\nAn alternate way of solving this is to adjust the inventory of the flight based on \nthese given conditions. For example, if there is a restriction on the flight, update the \ninventory as zero. In this case, the intelligence will remain with Flight, and it keeps \nupdating the inventory. As far as Search and Booking are concerned, both just look \nup the inventory instead of validating flights for every request. This approach is \nmore efficient as compared to the original approach.\nNext we will review the Payment use case. Payment is typically a disconnected \nfunction due to the nature of security constraints such as PCIDSS-like standards. The \nmost obvious way to capture a payment is to redirect a browser to a payment page \nhosted in the Payment service. Since card handling applications come under the \npurview of PCIDSS, it is wise to remove any direct dependencies from the Payment \nservice. Therefore, we can remove the Booking-to-Payment direct dependency, and \nopt for a UI-level integration.\nChallenge service boundaries\nIn this section, we will review some of the service boundaries based on the \nrequirements and dependency graph, considering Check-in and its dependencies to \nSeating and Baggage.\nThe Seating function runs a few algorithms based on the current state of the seat \nallocation in the airplane, and finds out the best way to position the next passenger \nso that the weight and balance requirements can be met. This is based on a number \nof predefined business rules. However, other than Check-in, no other module is \ninterested in the Seating function. From a business capability perspective, Seating is \njust a function of Check-in, not a business capability by itself. Therefore, it is better to \nembed this logic inside Check-in itself.\nThe same is applicable to Baggage as well. BrownField has a separate baggage \nhandling system. The Baggage function in the PSS context is to print the baggage tag \nas well as store the baggage data against the Check-in records. There is no business \ncapability associated with this particular functionality. Therefore, it is ideal to move \nthis function to Check-in itself.\n",
      "content_length": 2163,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "Microservices Evolution – A Case Study\n[ 174 ]\nThe Book, Search, and Inventory functions, after redesigning, are shown in the \nfollowing diagram:\nSimilarly, Inventory and Search are more supporting functions of the Booking \nmodule. They are not aligned with any of the business capabilities as such. Similar to \nthe previous judgement, it is ideal to move both the Search and Inventory functions \nto Booking. Assume, for the time being, that Search, Inventory, and Booking are \nmoved to a single microservice named Reservation.\nAs per the statistics of BrownField, search transactions are 10 times more  \nfrequent than the booking transactions. Moreover, search is not a revenue-generating \ntransaction when compared to booking. Due to these reasons, we need different \nscalability models for search and booking. Booking should not get impacted if \nthere is a sudden surge of transactions in search. From the business point of view, \ndropping a search transaction in favor of saving a valid booking transaction is  \nmore acceptable.\n",
      "content_length": 1033,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "Chapter 4\n[ 175 ]\nThis is an example of a polyglot requirement, which overrules the business capability \nalignment. In this case, it makes more sense to have Search as a service separate \nfrom the Booking service. Let us assume that we remove Search. Only Inventory and \nBooking remain under Reservation. Now Search has to hit back to Reservation to \nperform inventory searches. This could impact the booking transactions:\nA better approach is to keep Inventory along with the Booking module, and keep a \nread-only copy of the inventory under Search, while continuously synchronizing the \ninventory data over a reliable messaging system. Since both Inventory and Booking \nare collocated, this will also solve the need to have two-phase commits. Since both of \nthem are local, they could work well with local transactions.\nLet us now challenge the Fare module design. When a customer searches for a \nflight between A and B for a given date, we would like to show the flights and fares \ntogether. That means that our read-only copy of inventory can also combine both \nfares as well as inventory. Search will then subscribe to Fare for any fare change \nevents. The intelligence still stays with the Fare service, but it keeps sending fare \nupdates to the cached fare data under Search.\n",
      "content_length": 1283,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "Microservices Evolution – A Case Study\n[ 176 ]\nFinal dependency graph\nThere are still a few synchronized calls, which, for the time being, we will keep as \nthey are.\nBy applying all these changes, the final dependency diagram will look like the \nfollowing one:\nNow we can safely consider each box in the preceding diagram as a microservice.  \nWe have nailed down many dependencies, and modeled many of them as \nasynchronous as well. The overall system is more or less designed in the reactive style. \nThere are still some synchronized calls shown in the diagram with bold lines, such as \nGet Bulk from Check-In, Get Booking from CRM, and Get Fare from Booking. These \nsynchronous calls are essentially required as per the trade-off analysis.\n",
      "content_length": 742,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "Chapter 4\n[ 177 ]\nPrioritizing microservices for migration\nWe have identified a first-cut version of our microservices-based architecture. As the \nnext step, we will analyze the priorities, and identify the order of migration. This \ncould be done by considering multiple factors explained as follows:\n•\t\nDependency: One of the parameters for deciding the priority is the \ndependency graph. From the service dependency graph, services with less \ndependency or no dependency at all are easy to migrate, whereas complex \ndependencies are way harder. Services with complex dependencies will also \nneed dependent modules to be migrated along with them.\nAccounting, Loyalty, CRM, and Boarding have less dependencies as \ncompared to Booking and Check-in. Modules with high dependencies will \nalso have higher risks in their migration.\n•\t\nTransaction volume: Another parameter that can be applied is analyzing the \ntransaction volumes. Migrating services with the highest transaction volumes \nwill relieve the load on the existing system. This will have more value from \nan IT support and maintenance perspective. However, the downside of this \napproach is the higher risk factor.\nAs stated earlier, Search requests are ten times higher in volume as compared \nto Booking requests. Requests for Check-in are the third-highest in volume \ntransaction after Search and Booking.\n•\t\nResource utilization: Resource utilization is measured based on the current \nutilizations such as CPU, memory, connection pools, thread pools, and so on. \nMigrating resource intensive services out of the legacy system provides relief \nto other services. This helps the remaining modules to function better.\nFlight, Revenue Management, and Accounting are resource-intensive \nservices, as they involve data-intensive transactions such as forecasting, \nbilling, flight schedule changes, and so on.\n•\t\nComplexity: Complexity is perhaps measured in terms of the business logic \nassociated with a service such as function points, lines of code, number of \ntables, number of services, and others. Less complex modules are easy to \nmigrate as compared to the more complex ones.\nBooking is extremely complex as compared to the Boarding, Search, and \nCheck-in services.\n",
      "content_length": 2229,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "Microservices Evolution – A Case Study\n[ 178 ]\n•\t\nBusiness criticality: The business criticality could be either based on  \nrevenue or customer experience. Highly critical modules deliver higher \nbusiness value.\nBooking is the most revenue-generating service from the business stand \npoint, whereas Check-in is business critical as it could lead to flight departure \ndelays, which could lead to revenue loss as well as customer dissatisfaction.\n•\t\nVelocity of changes: Velocity of change indicates the number of change \nrequests targeting a function in a short time frame. This translates to speed \nand agility of delivery. Services with high velocity of change requests are \nbetter candidates for migration as compared to stable modules.\nStatistics show that Search, Booking, and Fares go through frequent changes, \nwhereas Check-in is the most stable function.\n•\t\nInnovation: Services that are part of a disruptive innovative process need \nto get priority over back office functions that are based on more established \nbusiness processes. Innovations in legacy systems are harder to achieve as \ncompared to applying innovations in the microservices world.\nMost of the innovations are around Search, Booking, Fares, Revenue \nManagement, and Check-in as compared to back office Accounting.\nBased on BrownField's analysis, Search has the highest priority, as it requires \ninnovation, has high velocity of changes, is less business critical, and gives better \nrelief for both business and IT. The Search service has minimal dependency with no \nrequirements to synchronize data back to the legacy system.\nData synchronization during migration\nDuring the transition phase, the legacy system and the new microservices will  \nrun in parallel. Therefore, it is important to keep the data synchronized between  \nthe two systems.\nThe simplest option is to synchronize the data between the two systems at  \nthe database level by using any data synchronization tool. This approach works  \nwell when both the old and the new systems are built on the same data store \ntechnologies. The complexity will be higher if the data store technologies are \ndifferent. The second problem with this approach is that we allow a backdoor entry, \nhence exposing the microservices' internal data store outside. This is against the \nprinciple of microservices.\n",
      "content_length": 2332,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "Chapter 4\n[ 179 ]\nLet us take this on a case-by-case basis before we can conclude with a generic \nsolution. The following diagram shows the data migration and synchronization \naspect once Search is taken out:\nLet us assume that we use a NoSQL database for keeping inventory and fares under \nthe Search service. In this particular case, all we need is the legacy system to supply \ndata to the new service using asynchronous events. We will have to make some \nchanges in the existing system to send the fare changes or any inventory changes as \nevents. The Search service then accepts these events, and stores them locally into the \nlocal NoSQL store.\nThis is a bit more tedious in the case of the complex Booking service.\n",
      "content_length": 721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "Microservices Evolution – A Case Study\n[ 180 ]\nIn this case, the new Booking microservice sends the inventory change events to the \nSearch service. In addition to this, the legacy application also has to send the fare \nchange events to Search. Booking will then store the new Booking service in its My \nSQL data store.\nThe most complex piece, the Booking service, has to send the booking events  \nand the inventory events back to the legacy system. This is to ensure that the \nfunctions in the legacy system continue to work as before. The simplest approach  \nis to write an update component which accepts the events and updates the old \nbooking records table so that there are no changes required in the other legacy \nmodules. We will continue this until none of the legacy components are referring \nthe booking and inventory data. This will help us minimize changes in the legacy \nsystem, and therefore, reduce the risk of failures.\nIn short, a single approach may not be sufficient. A multi-pronged approach based \non different patterns is required.\n",
      "content_length": 1053,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "Chapter 4\n[ 181 ]\nManaging reference data\nOne of the biggest challenges in migrating monolithic applications to microservices \nis managing reference data. A simple approach is to build the reference data as \nanother microservice itself as shown in the following diagram:\nIn this case, whoever needs reference data should access it through the microservice \nendpoints. This is a well-structured approach, but could lead to performance issues \nas encountered in the original legacy system.\nAn alternate approach is to have reference data as a microservice service for all the \nadmin and CRUD functions. A near cache will then be created under each service to \nincrementally cache data from the master services. A thin reference data access proxy \nlibrary will be embedded in each of these services. The reference data access proxy \nabstracts whether the data is coming from cache or from a remote service.\n",
      "content_length": 904,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "Microservices Evolution – A Case Study\n[ 182 ]\nThis is depicted in the next diagram. The master node in the given diagram is the \nactual reference data microservice:\nThe challenge is to synchronize the data between the master and the slave. A \nsubscription mechanism is required for those data caches that change frequently.\nA better approach is to replace the local cache with an in-memory data grid, as \nshown in the following diagram:\n",
      "content_length": 438,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "Chapter 4\n[ 183 ]\nThe reference data microservice will write to the data grid, whereas the proxy \nlibraries embedded in other services will have read-only APIs. This eliminates the \nrequirement to have subscription of data, and is much more efficient and consistent.\nUser interfaces and web applications\nDuring the transition phase, we have to keep both the old and new user interfaces \ntogether. There are three general approaches usually taken in this scenario.\nThe first approach is to have the old and new user interfaces as separate user \napplications with no link between them, as depicted in the following diagram:\nA user signs in to the new application as well as into the old application, much \nlike two different applications, with no single sign-on (SSO) between them. This \napproach is simple, and there is no overhead. In most of the cases, this may not be \nacceptable to the business unless it is targeted at two different user communities.\nThe second approach is to use the legacy user interface as the primary application, \nand then transfer page controls to the new user interfaces when the user requests \npages of the new application:\n",
      "content_length": 1153,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "Microservices Evolution – A Case Study\n[ 184 ]\nIn this case, since the old and the new applications are web-based applications \nrunning in a web browser window, users will get a seamless experience. SSO has to \nbe implemented between the old and the new user interfaces.\nThe third approach is to integrate the existing legacy user interface directly to the \nnew microservices backend, as shown in the next diagram:\nIn this case, the new microservices are built as headless applications with no \npresentation layer. This could be challenging, as it may require many changes  \nin the old user interface such as introducing service calls, data model conversions, \nand so on.\nAnother issue in the last two cases is how to handle the authentication of resources \nand services.\nSession handling and security\nAssume that the new services are written based on Spring Security with a  \ntoken-based authorization strategy, whereas the old application uses a  \ncustom-built authentication with its local identity store.\n",
      "content_length": 1009,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "Chapter 4\n[ 185 ]\nThe following diagram shows how to integrate between the old and the new \nservices:\nThe simplest approach, as shown in the preceding diagram, is to build a new  \nidentity store with an authentication service as a new microservice using Spring \nSecurity. This will be used for all our future resource and service protections,  \nfor all microservices.\nThe existing user interface application authenticates itself against the new \nauthentication service, and secures a token. This token will be passed to the new user \ninterface or new microservice. In both cases, the user interface or microservice will \nmake a call to the authentication service to validate the given token. If the token is \nvalid, then the UI or microservice accepts the call.\nThe catch here is that the legacy identity store has to be synchronized with the  \nnew one.\n",
      "content_length": 854,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "Microservices Evolution – A Case Study\n[ 186 ]\nTest strategy\nOne important question to answer from a testing point of view is how can we ensure \nthat all functions work in the same way as before the migration?\nIntegration test cases should be written for the services that are getting migrated \nbefore the migration or refactoring. This ensures that once migrated, we get the same \nexpected result, and the behavior of the system remains the same. An automated \nregression test pack has to be in place, and has to be executed every time we make a \nchange in the new or old system.\nIn the following diagram, for each service we need one test against the EJB endpoint, \nand another one against the microservices endpoint:\n",
      "content_length": 720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "Chapter 4\n[ 187 ]\nBuilding ecosystem capabilities\nBefore we embark on actual migration, we have to build all of the microservice's \ncapabilities mentioned under the capability model, as documented in Chapter 3,  \nApplying Microservices Concepts. These are the prerequisites for developing \nmicroservices-based systems.\nIn addition to these capabilities, certain application functions are also required \nto be built upfront such as reference data, security and SSO, and Customer and \nNotification. A data warehouse or a data lake is also required as a prerequisite. An \neffective approach is to build these capabilities in an incremental fashion, delaying \ndevelopment until it is really required.\nMigrate modules only if required\nIn the previous chapters, we have examined approaches and steps for transforming \nfrom a monolithic application to microservices. It is important to understand that it is \nnot necessary to migrate all modules to the new microservices architecture, unless it \nis really required. A major reason is that these migrations incur cost.\nWe will review a few such scenarios here. BrownField has already taken a decision \nto use an external revenue management system in place of the PSS revenue \nmanagement function. BrownField is also in the process of centralizing their \naccounting functions, and therefore, need not migrate the accounting function from \nthe legacy system. Migration of CRM does not add much value at this point to the \nbusiness. Therefore, it is decided to keep the CRM in the legacy system itself. The \nbusiness has plans to move to a SaaS-based CRM solution as part of their cloud \nstrategy. Also note that stalling the migration halfway through could seriously \nimpact the complexity of the system.\n",
      "content_length": 1745,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "Microservices Evolution – A Case Study\n[ 188 ]\nTarget architecture\nThe architecture blueprint shown in the following diagram consolidates earlier \ndiscussions into an architectural view. Each block in the diagram represents a \nmicroservice. The shaded boxes are core microservices, and the others are supporting \nmicroservices. The diagram also shows the internal capabilities of each microservice. \nUser management is moved under security in the target architecture:\n",
      "content_length": 468,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "Chapter 4\n[ 189 ]\nEach service has its own architecture, typically consisting of a presentation layer, \none or more service endpoints, business logic, business rules, and database. As \nwe can see, we use different selections of databases that are more suitable for each \nmicroservice. Each one is autonomous with minimal orchestration between the \nservices. Most of the services interact with each other using the service endpoints.\nInternal layering of microservices\nIn this section, we will further explore the internal structure of microservices. There \nis no standard to be followed for the internal architecture of a microservice. The rule \nof thumb is to abstract realizations behind simple service endpoints.\nA typical structure would look like the one shown in the following diagram:\nThe UI accesses REST services through a service gateway. The API gateway may be \none per microservice or one for many microservices—it depends on what we want \nto do with the API gateway. There could be one or more rest endpoints exposed by \nmicroservices. These endpoints, in turn, connect to one of the business components \nwithin the service. Business components then execute all the business functions with \nthe help of domain entities. A repository component is used for interacting with the \nbackend data store.\n",
      "content_length": 1310,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "Microservices Evolution – A Case Study\n[ 190 ]\nOrchestrating microservices\nThe logic of the booking orchestration and the execution of rules sits within the \nBooking service. The brain is still inside the Booking service in the form of one or \nmore booking business components. Internally, business components orchestrate \nprivate APIs exposed by other business components or even external services:\nAs shown in the preceding diagram, the booking service internally calls to update \nthe inventory of its own component other than calling the Fare service.\nIs there any orchestration engine required for this activity? It depends on the \nrequirements. In complex scenarios, we may have to do a number of things in \nparallel. For example, creating a booking internally applies a number of booking \nrules, it validates the fare, and it validates the inventory before creating a \nbooking. We may want to execute them in parallel. In such cases, we may use Java \nconcurrency APIs or reactive Java libraries.\nIn extremely complex situations, we may opt for an integration framework such as \nSpring Integration or Apache Camel in embedded mode.\n",
      "content_length": 1137,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "Chapter 4\n[ 191 ]\nIntegration with other systems\nIn the microservices world, we use an API gateway or a reliable message bus for \nintegrating with other non-microservices.\nLet us assume that there is another system in BrownField that needs booking data. \nUnfortunately, the system is not capable of subscribing to the booking events that \nthe Booking microservice publishes. In such cases, an Enterprise Application \nintegration (EAI) solution could be employed, which listens to our booking events, \nand then uses a native adaptor to update the database.\nManaging shared libraries\nCertain business logic is used in more than one microservice. Search and \nReservation, in this case, use inventory rules. In such cases, these shared libraries will \nbe duplicated in both the microservices.\nHandling exceptions\nExamine the booking scenario to understand the different exception handling \napproaches. In the following service sequence diagram, there are three lines marked \nwith a cross mark. These are the potential areas where exceptions could occur:\n",
      "content_length": 1050,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "Microservices Evolution – A Case Study\n[ 192 ]\nThere is a synchronous communication between Booking and Fare. What if the Fare \nservice is not available? If the Fare service is not available, throwing an error back \nto the user may cause revenue loss. An alternate thought is to trust the fare which \ncomes as part of the incoming request. When we serve search, the search results will \nhave the fare as well. When the user selects a flight and submits, the request will \nhave the selected fare. In case the Fare service is not available, we trust the incoming \nrequest, and accept the Booking. We will use a circuit breaker and a fallback service \nwhich simply creates the booking with a special status, and queues the booking for \nmanual action or a system retry.\nWhat if creating the booking fails? If creating a booking fails unexpectedly, a better \noption is to throw a message back to the user. We could try alternative options, but \nthat could increase the overall complexity of the system. The same is applicable for \ninventory updates.\nIn the case of creating a booking and updating the inventory, we avoid a situation \nwhere a booking is created, and an inventory update somehow fails. As the \ninventory is critical, it is better to have both, create booking and update inventory, \nto be in a local transaction. This is possible as both components are under the same \nsubsystem.\nIf we consider the Check-in scenario, Check-in sends an event to Boarding and \nBooking as shown in the next diagram:\n",
      "content_length": 1506,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "Chapter 4\n[ 193 ]\nConsider a scenario where the Check-in services fail immediately after the Check-in \nComplete event is sent out. The other consumers processed this event, but the actual \ncheck-in is rolled back. This is because we are not using a two-phase commit. In this \ncase, we need a mechanism for reverting that event. This could be done by catching \nthe exception, and sending another Check-in Cancelled event.\nIn this case, note that to minimize the use of compensating transactions, sending the \nCheck-in event is moved towards the end of the Check-in transaction. This reduces \nthe chance of failure after sending out the event.\n",
      "content_length": 642,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "Microservices Evolution – A Case Study\n[ 194 ]\nOn the other hand, what if the check-in is successful, but sending the event failed? \nWe could think of two approaches. The first approach would be to invoke a fallback \nservice to store it locally, and then use another sweep-and-scan program to send \nthe event at a later time. It could even retry multiple times. This could add more \ncomplexity and may not be efficient in all cases. An alternate approach is to throw \nthe exception back to the user so that the user can retry. However, this might not \nalways be good from a customer engagement standpoint. On the other hand, the \nearlier option is better for the system's health. A trade-off analysis is required to find \nout the best solution for the given situation.\nTarget implementation view\nThe next diagram represents the implementation view of the BrownField PSS \nmicroservices system:\nAs shown in the preceding diagram, we are implementing four microservices as an \nexample: Search, Fare, Booking, and Check-in. In order to test the application, there \nis a website application developed using Spring MVC with Thymeleaf templates. \nThe asynchronous messaging is implemented with the help of RabbitMQ. In this \nsample implementation, the default H2 database is used as the in-memory store for \ndemonstration purposes.\nThe code in this section demonstrates all the capabilities highlighted in the Reviewing \nthe microservices capability model section of this chapter.\n",
      "content_length": 1474,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "Chapter 4\n[ 195 ]\nImplementation projects\nThe basic implementation of the BrownField Airline's PSS microservices system has \nfive core projects as summarized in the following table. The table also shows the port \nrange used for these projects to ensure consistency throughout the book:\nMicroservice \nProjects\nPort Range\nBook microservice\nchapter4.book\n8060-8069\nCheck-in microservice\nchapter4.checkin\n8070-8079\nFare microservice\nchapter4.fares\n8080-8089\nSearch microservice\nchapter4.search\n8090-8099\nWebsite\nchapter4.website\n8001\nThe website is the UI application for testing the PSS microservices.\nAll microservice projects in this example follow the same pattern for package \nstructure as shown in the following screenshot:\n",
      "content_length": 726,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "Microservices Evolution – A Case Study\n[ 196 ]\nThe different packages and their purposes are explained as follows:\n•\t\nThe root folder (com.brownfield.pss.book) contains the default Spring \nBoot application.\n•\t\nThe component package hosts all the service components where the business \nlogic is implemented.\n•\t\nThe controller package hosts the REST endpoints and the messaging \nendpoints. Controller classes internally utilize the component classes for \nexecution.\n•\t\nThe entity package contains the JPA entity classes for mapping to the \ndatabase tables.\n•\t\nRepository classes are packaged inside the repository package, and are \nbased on Spring Data JPA.\nRunning and testing the project\nFollow the steps listed next to build and test the microservices developed in this \nchapter:\n1.\t Build each of the projects using Maven. Ensure that the test flag is switched \noff. The test programs assume other dependent services are up and running. \nIt fails if the dependent services are not available. In our example, Booking \nand Fare have direct dependencies. We will learn how to circumvent this \ndependency in Chapter 7, Logging and Monitoring Microservices:\nmvn -Dmaven.test.skip=true install\n2.\t Run the RabbitMQ server:\nrabbitmq_server-3.5.6/sbin$ ./rabbitmq-server\n3.\t Run the following commands in separate terminal windows:\njava -jar target/fares-1.0.jar\njava -jar target/search-1.0.jar\njava -jar target/checkin-1.0.jar\njava -jar target/book-1.0.jar\njava -jar target/website-1.0.jar\n",
      "content_length": 1485,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "Chapter 4\n[ 197 ]\n4.\t The website project has a CommandLineRunner, which executes all the test \ncases at startup. Once all the services are successfully started, open http://\nlocalhost:8001 in a browser.\n5.\t The browser asks for basic security credentials. Use guest or guest123 as \nthe credentials. This example only shows the website security with a basic \nauthentication mechanism. As explained in Chapter 2, Building Microservices \nwith Spring Boot, service-level security can be achieved using OAuth2.\n6.\t Entering the correct security credentials displays the following screen. This is \nthe home screen of our BrownField PSS application:\n7.\t The SUBMIT button invokes the Search microservice to fetch the available \nflights that meet the conditions mentioned on the screen. A few flights are \npre-populated at the startup of the Search microservice. Edit the Search \nmicroservice code to feed in additional flights, if required.\n",
      "content_length": 935,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "Microservices Evolution – A Case Study\n[ 198 ]\n8.\t The output screen with a list of flights is shown in the next screenshot.  \nThe Book link will take us to the booking screen for the selected flight:\n9.\t The following screenshot shows the booking screen. The user can enter the \npassenger details, and create a booking by clicking on the CONFIRM button. \nThis invokes the Booking microservice, and internally, the Fare service as \nwell. It also sends a message back to the Search microservice:\n",
      "content_length": 495,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "Chapter 4\n[ 199 ]\n10.\t If booking is successful, the next confirmation screen is displayed with a \nbooking reference number:\n11.\t Let us test the Check-in microservice. This can be done by clicking on \nCheckIn in the menu at the top of the screen. Use the booking reference \nnumber obtained in the previous step to test Check-in. This is shown in the \nfollowing screenshot:\n",
      "content_length": 374,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "Microservices Evolution – A Case Study\n[ 200 ]\n12.\t Clicking on the SEARCH button in the previous screen invokes the Booking \nmicroservice, and retrieves the booking information. Click on the CheckIn \nlink to perform the check-in. This invokes the Check-in microservice:\n13.\t If check-in is successful, it displays the confirmation message, as shown in \nthe next screenshot, with a confirmation number. This is done by calling the \nCheck-in service internally. The Check-in service sends a message to Booking \nto update the check-in status:\n",
      "content_length": 541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "Chapter 4\n[ 201 ]\nSummary\nIn this chapter, we implemented and tested the BrownField PSS microservice with \nbasic Spring Boot capabilities. We learned how to approach a real use case with a \nmicroservices architecture. \nWe examined the various stages of a real-world evolution towards microservices \nfrom a monolithic application. We also evaluated the pros and cons of multiple \napproaches, and the obstacles encountered when migrating a monolithic application. \nFinally, we explained the end-to-end microservices design for the use case \nthat we examined. Design and implementation of a fully-fledged microservice \nimplementation was also validated.\nIn the next chapter, we will see how the Spring Cloud project helps us to transform \nthe developed BrownField PSS microservices to an Internet-scale deployment.\n",
      "content_length": 812,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "[ 203 ]\nScaling Microservices with \nSpring Cloud\nIn order to manage Internet-scale microservices, one requires more capabilities than \nwhat are offered by the Spring Boot framework. The Spring Cloud project has a suite \nof purpose-built components to achieve these additional capabilities effortlessly.\nThis chapter will provide a deep insight into the various components of the Spring \nCloud project such as Eureka, Zuul, Ribbon, and Spring Config by positioning \nthem against the microservices capability model discussed in Chapter 3, Applying \nMicroservices Concepts. It will demonstrate how the Spring Cloud components help  \nto scale the BrownField Airline's PSS microservices system, developed in the \nprevious chapter.\nBy the end of this chapter, you will learn about the following:\n•\t\nThe Spring Config server for externalizing configuration\n•\t\nThe Eureka server for service registration and discovery\n•\t\nThe relevance of Zuul as a service proxy and gateway\n•\t\nThe implementation of automatic microservice registration and  \nservice discovery\n•\t\nSpring Cloud messaging for asynchronous microservice composition\n",
      "content_length": 1119,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "Scaling Microservices with Spring Cloud\n[ 204 ]\nReviewing microservices capabilities\nThe examples in this chapter explore the following microservices capabilities from \nthe microservices capability model discussed in Chapter 3, Applying Microservices \nConcepts:\n•\t\nSoftware Defined Load Balancer\n•\t\nService Registry\n•\t\nConfiguration Service\n•\t\nReliable Cloud Messaging\n•\t\nAPI Gateways\nReviewing BrownField's PSS \nimplementation\nIn Chapter 4, Microservices Evolution – A Case Study, we designed and developed a \nmicroservice-based PSS system for BrownField Airlines using the Spring framework \nand Spring Boot. The implementation is satisfactory from the development point of \nview, and it serves the purpose for low volume transactions. However, this is not \ngood enough for deploying large, enterprise-scale deployments with hundreds or \neven thousands of microservices.\n",
      "content_length": 872,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "Chapter 5\n[ 205 ]\nIn Chapter 4, Microservices Evolution – A Case Study, we developed four microservices: \nSearch, Booking, Fares, and Check-in. We also developed a website to test the \nmicroservices.\nWe have accomplished the following items in our microservice implementation  \nso far:\n•\t\nEach microservice exposes a set of REST/JSON endpoints for accessing \nbusiness capabilities\n•\t\nEach microservice implements certain business functions using the  \nSpring framework.\n•\t\nEach microservice stores its own persistent data using H2, an in-memory \ndatabase\n•\t\nMicroservices are built with Spring Boot, which has an embedded Tomcat \nserver as the HTTP listener\n•\t\nRabbitMQ is used as an external messaging service. Search, Booking, and \nCheck-in interact with each other through asynchronous messaging\n•\t\nSwagger is integrated with all microservices for documenting the REST APIs.\n•\t\nAn OAuth2-based security mechanism is developed to protect the \nmicroservices\nWhat is Spring Cloud?\nThe Spring Cloud project is an umbrella project from the Spring team that implements \na set of common patterns required by distributed systems, as a set of easy-to-use Java \nSpring libraries. Despite its name, Spring Cloud by itself is not a cloud solution. Rather, \nit provides a number of capabilities that are essential when developing applications \ntargeting cloud deployments that adhere to the Twelve-Factor application principles. \nBy using Spring Cloud, developers just need to focus on building business capabilities \nusing Spring Boot, and leverage the distributed, fault-tolerant, and  \nself-healing capabilities available out of the box from Spring Cloud.\nThe Spring Cloud solutions are agnostic to the deployment environment, and can \nbe developed and deployed in a desktop PC or in an elastic cloud. The cloud-ready \nsolutions that are developed using Spring Cloud are also agnostic and portable across \nmany cloud providers such as Cloud Foundry, AWS, Heroku, and so on. When not \nusing Spring Cloud, developers will end up using services natively provided by the \ncloud vendors, resulting in deep coupling with the PaaS providers. An alternate option \nfor developers is to write quite a lot of boilerplate code to build these services. Spring \nCloud also provides simple, easy-to-use Spring-friendly APIs, which abstract the cloud \nprovider's service APIs such as those APIs coming with AWS Notification Service.\n",
      "content_length": 2410,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "Scaling Microservices with Spring Cloud\n[ 206 ]\nBuilt on Spring's \"convention over configuration\" approach, Spring Cloud defaults \nall configurations, and helps the developers get off to a quick start. Spring Cloud \nalso hides the complexities, and provides simple declarative configurations to build \nsystems. The smaller footprints of the Spring Cloud components make it developer \nfriendly, and also make it easy to develop cloud-native applications.\nSpring Cloud offers many choices of solutions for developers based on their \nrequirements. For example, the service registry can be implemented using popular \noptions such as Eureka, ZooKeeper, or Consul. The components of Spring Cloud  \nare fairly decoupled, hence, developers get the flexibility to pick and choose what  \nis required.\nWhat is the difference between Spring Cloud and Cloud Foundry?\nSpring Cloud is a developer kit for developing Internet-scale Spring Boot \napplications, whereas Cloud Foundry is an open-source Platform as a \nService for building, deploying, and scaling applications.\nSpring Cloud releases\nThe Spring Cloud project is an overarching Spring project that includes  \na combination of different components. The versions of these components  \nare defined in the spring-cloud-starter-parent BOM.\nIn this book, we are relying on the Brixton.RELEASE version of the Spring Cloud:\n  <dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-dependencies</artifactId>\n    <version>Brixton.RELEASE</version>\n    <type>pom</type>\n    <scope>import</scope>\n  </dependency>\nThe spring-cloud-starter-parent defines different versions of its subcomponents \nas follows:\n<spring-cloud-aws.version>1.1.0.RELEASE</spring-cloud-aws.version>\n<spring-cloud-bus.version>1.1.0.RELEASE</spring-cloud-bus.version>\n<spring-cloud-cloudfoundry.version>1.0.0.RELEASE</spring-cloud-\ncloudfoundry.version>\n<spring-cloud-commons.version>1.1.0.RELEASE</spring-cloud-commons.\nversion>\n<spring-cloud-config.version>1.1.0.RELEASE</spring-cloud-config.\nversion>\n<spring-cloud-netflix.version>1.1.0.RELEASE</spring-cloud-netflix.\nversion>\n",
      "content_length": 2120,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "Chapter 5\n[ 207 ]\n<spring-cloud-security.version>1.1.0.RELEASE</spring-cloud-security.\nversion>\n<spring-cloud-cluster.version>1.0.0.RELEASE</spring-cloud-cluster.\nversion>\n<spring-cloud-consul.version>1.0.0.RELEASE</spring-cloud-consul.\nversion>\n<spring-cloud-sleuth.version>1.0.0.RELEASE</spring-cloud-sleuth.\nversion>\n<spring-cloud-stream.version>1.0.0.RELEASE</spring-cloud-stream.\nversion>\n<spring-cloud-zookeeper.version>1.0.0.RELEASE </spring-cloud-\nzookeeper.version>\nThe names of the Spring Cloud releases are in an alphabetic sequence, \nstarting with A, following the names of the London Tube stations. Angel \nwas the first release, and Brixton is the second release.\nComponents of Spring Cloud\nEach Spring Cloud component specifically addresses certain distributed system \ncapabilities. The grayed-out boxes at the bottom of the following diagram show the \ncapabilities, and the boxes placed on top of these capabilities showcase the Spring \nCloud subprojects addressing these capabilities:\n",
      "content_length": 1001,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "Scaling Microservices with Spring Cloud\n[ 208 ]\nThe Spring Cloud capabilities are explained as follows:\n•\t\nDistributed configuration: Configuration properties are hard to manage when \nthere are many microservice instances running under different profiles such as \ndevelopment, test, production, and so on. It is, therefore, important to manage \nthem centrally, in a controlled way. The distributed configuration management \nmodule is to externalize and centralize microservice configuration parameters. \nSpring Cloud Config is an externalized configuration server with Git or SVN \nas the backing repository. Spring Cloud Bus provides support for propagating \nconfiguration changes to multiple subscribers, generally a microservice \ninstance. Alternately, ZooKeeper or HashiCorp's Consul can also be used  \nfor distributed configuration management.\n•\t\nRouting: Routing is an API gateway component, primarily used similar  \nto a reverse proxy that forwards requests from consumers to service \nproviders. The gateway component can also perform software-based  \nrouting and filtering. Zuul is a lightweight API gateway solution that  \noffers fine-grained controls to developers for traffic shaping and request/\nresponse transformations.\n•\t\nLoad balancing: The load balancer capability requires a software-defined \nload balancer module which can route requests to available servers using a \nvariety of load balancing algorithms. Ribbon is a Spring Cloud subproject \nwhich supports this capability. Ribbon can work as a standalone component, \nor integrate and work seamlessly with Zuul for traffic routing.\n•\t\nService registration and discovery: The service registration and discovery \nmodule enables services to programmatically register with a repository when \na service is available and ready to accept traffic. The microservices advertise \ntheir existence, and make them discoverable. The consumers can then look \nup the registry to get a view of the service availability and the endpoint \nlocations. The registry, in many cases, is more or less a dump. But the \ncomponents around the registry make the ecosystem intelligent. There are \nmany subprojects existing under Spring Cloud which support registry and \ndiscovery capability. Eureka, ZooKeeper, and Consul are three subprojects \nimplementing the registry capability.\n•\t\nService-to-service calls: The Spring Cloud Feign subproject under Spring \nCloud offers a declarative approach for making RESTful service-to-service \ncalls in a synchronous way. The declarative approach allows applications \nto work with POJO (Plain Old Java Object) interfaces instead of low-level \nHTTP client APIs. Feign internally uses reactive libraries for communication.\n",
      "content_length": 2700,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "Chapter 5\n[ 209 ]\n•\t\nCircuit breaker: The circuit breaker subproject implements the circuit \nbreaker pattern. The circuit breaker breaks the circuit when it encounters \nfailures in the primary service by diverting traffic to another temporary \nfallback service. It also automatically reconnects back to the primary \nservice when the service is back to normal. It finally provides a monitoring \ndashboard for monitoring the service state changes. The Spring Cloud \nHystrix project and Hystrix Dashboard implement the circuit breaker  \nand the dashboard respectively.\n•\t\nGlobal locks, leadership election and cluster state: This capability is \nrequired for cluster management and coordination when dealing with \nlarge deployments. It also offers global locks for various purposes such as \nsequence generation. The Spring Cloud Cluster project implements these \ncapabilities using Redis, ZooKeeper, and Consul.\n•\t\nSecurity: Security capability is required for building security for cloud-\nnative distributed systems using externalized authorization providers such \nas OAuth2. The Spring Cloud Security project implements this capability \nusing customizable authorization and resource servers. It also offers SSO \ncapabilities, which are essential when dealing with many microservices.\n•\t\nBig data support: The big data support capability is a capability that is \nrequired for data services and data flows in connection with big data \nsolutions. The Spring Cloud Streams and the Spring Cloud Data Flow \nprojects implement these capabilities. The Spring Cloud Data Flow is  \nthe re-engineered version of Spring XD.\n•\t\nDistributed tracing: The distributed tracing capability helps to thread and \ncorrelate transitions that are spanned across multiple microservice instances. \nSpring Cloud Sleuth implements this by providing an abstraction on top of \nvarious distributed tracing mechanisms such as Zipkin and HTrace with the \nsupport of a 64-bit ID.\n•\t\nDistributed messaging: Spring Cloud Stream provides declarative \nmessaging integration on top of reliable messaging solutions such as Kafka, \nRedis, and RabbitMQ.\n•\t\nCloud support: Spring Cloud also provides a set of capabilities that offers \nvarious connectors, integration mechanisms, and abstraction on top of \ndifferent cloud providers such as the Cloud Foundry and AWS.\n",
      "content_length": 2322,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "Scaling Microservices with Spring Cloud\n[ 210 ]\nSpring Cloud and Netflix OSS\nMany of the Spring Cloud components which are critical for microservices' \ndeployment came from the Netflix Open Source Software (Netflix OSS) center. \nNetflix is one of the pioneers and early adaptors in the microservices space. In \norder to manage large scale microservices, engineers at Netflix came up with a \nnumber of homegrown tools and techniques for managing their microservices. \nThese are fundamentally crafted to fill some of the software gaps recognized in \nthe AWS platform for managing Netflix services. Later, Netflix open-sourced these \ncomponents, and made them available under the Netflix OSS platform for public use. \nThese components are extensively used in production systems, and are battle-tested \nwith large scale microservice deployments at Netflix.\nSpring Cloud offers higher levels of abstraction for these Netflix OSS components, \nmaking it more Spring developer friendly. It also provides a declarative mechanism, \nwell-integrated and aligned with Spring Boot and the Spring framework.\nSetting up the environment for \nBrownField PSS\nIn this chapter, we will amend the BrownField PSS microservices developed in \nChapter 4, Microservices Evolution – A Case Study, using Spring Cloud capabilities.  \nWe will also examine how to make these services enterprise grade using Spring \nCloud components.\nSubsequent sections of this chapter will explore how to scale the microservices \ndeveloped in the previous chapter for cloud scale deployments, using some out-of-\nthe-box capabilities provided by the Spring Cloud project. The rest of this chapter \nwill explore Spring Cloud capabilities such as configuration using the Spring Config \nserver, Ribbon-based service load balancing, service discovery using Eureka, Zuul \nfor API gateway, and finally, Spring Cloud messaging for message-based service \ninteractions. We will demonstrate the capabilities by modifying the BrownField PSS \nmicroservices developed in Chapter 4, Microservices Evolution – A Case Study.\nIn order to prepare the environment for this chapter, import and rename \n(chapter4.* to chapter5.*) projects into a new STS workspace.\nThe full source code of this chapter is available under the Chapter 5 \nprojects in the code files.\n",
      "content_length": 2294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "Chapter 5\n[ 211 ]\nSpring Cloud Config\nThe Spring Cloud Config server is an externalized configuration server in which \napplications and services can deposit, access, and manage all runtime configuration \nproperties. The Spring Config server also supports version control of the \nconfiguration properties.\nIn the earlier examples with Spring Boot, all configuration parameters were read \nfrom a property file packaged inside the project, either application.properties or \napplication.yaml. This approach is good, since all properties are moved out of code \nto a property file. However, when microservices are moved from one environment \nto another, these properties need to undergo changes, which require an application \nre-build. This is violation of one of the Twelve-Factor application principles, which \nadvocate one-time build and moving of the binaries across environments.\nA better approach is to use the concept of profiles. Profiles, as discussed in Chapter \n2, Building Microservices with Spring Boot, is used for partitioning different properties \nfor different environments. The profile-specific configuration will be named \napplication-{profile}.properties. For example, application-development.\nproperties represents a property file targeted for the development environment.\nHowever, the disadvantage of this approach is that the configurations are statically \npackaged along with the application. Any changes in the configuration properties \nrequire the application to be rebuilt.\nThere are alternate ways to externalize the configuration properties from the \napplication deployment package. Configurable properties can also be read  \nfrom an external source in a number of ways:\n•\t\nFrom an external JNDI server using JNDI namespace (java:comp/env)\n•\t\nUsing the Java system properties (System.getProperties()) or using  \nthe –D command line option\n•\t\nUsing the PropertySource configuration:\n@PropertySource(\"file:${CONF_DIR}/application.properties\")\n  public class ApplicationConfig {\n}\n•\t\nUsing a command-line parameter pointing a file to an external location:\njava -jar myproject.jar --spring.config.location=\n",
      "content_length": 2126,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "Scaling Microservices with Spring Cloud\n[ 212 ]\nJNDI operations are expensive, lack flexibility, have difficulties in replication, and \nare not version controlled. System.properties is not flexible enough for large-scale \ndeployments. The last two options rely on a local or a shared filesystem mounted  \non the server.\nFor large scale deployments, a simple yet powerful centralized configuration \nmanagement solution is required:\nAs shown in the preceding diagram, all microservices point to a central server to get \nthe required configuration parameters. The microservices then locally cache these \nparameters to improve performance. The Config server propagates the configuration \nstate changes to all subscribed microservices so that the local cache's state can be \nupdated with the latest changes. The Config server also uses profiles to resolve \nvalues specific to an environment.\nAs shown in the following screenshot, there are multiple options available under \nthe Spring Cloud project for building the configuration server. Config Server, \nZookeeper Configuration, and Consul Configuration are available as options. \nHowever, this chapter will only focus on the Spring Config server implementation:\n",
      "content_length": 1208,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "Chapter 5\n[ 213 ]\nThe Spring Config server stores properties in a version-controlled repository such as \nGit or SVN. The Git repository can be local or remote. A highly available remote Git \nserver is preferred for large scale distributed microservice deployments.\nThe Spring Cloud Config server architecture is shown in the following diagram:\nAs shown in the preceding diagram, the Config client embedded in the Spring Boot \nmicroservices does a configuration lookup from a central configuration server using \na simple declarative mechanism, and stores properties into the Spring environment. \nThe configuration properties can be application-level configurations such as \ntrade limit per day, or infrastructure-related configurations such as server URLs, \ncredentials, and so on.\nUnlike Spring Boot, Spring Cloud uses a bootstrap context, which is a parent context \nof the main application. Bootstrap context is responsible for loading configuration \nproperties from the Config server. The bootstrap context looks for bootstrap.yaml \nor bootstrap.properties for loading initial configuration properties. To make this \nwork in a Spring Boot application, rename the application.* file to bootstrap.*.\n",
      "content_length": 1200,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "Scaling Microservices with Spring Cloud\n[ 214 ]\nWhat's next?\nThe next few sections demonstrate how to use the Config server in a real-world \nscenario. In order to do this, we will modify our search microservice (chapter5.\nsearch) to use the Config server. The following diagram depicts the scenario:\n \nIn this example, the Search service will read the Config server at startup by passing \nthe service name. In this case, the service name of the search service will be search-\nservice. The properties configured for the search-service include the RabbitMQ \nproperties as well as a custom property.\nThe full source code of this section is available under the \nchapter5.configserver project in the code files.\nSetting up the Config server\nThe following steps need to be followed to create a new Config server using STS:\n1.\t Create a new Spring Starter Project, and select Config Server and Actuator \nas shown in the following diagram:\n",
      "content_length": 932,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "Chapter 5\n[ 215 ]\n2.\t Set up a Git repository. This can be done by pointing to a remote Git \nconfiguration repository like the one at https://github.com/spring-cloud-\nsamples/config-repo. This URL is an indicative one, a Git repository used \nby the Spring Cloud examples. We will have to use our own Git repository \ninstead.\n3.\t Alternately, a local filesystem-based Git repository can be used. In a real \nproduction scenario, an external Git is recommended. The Config server in \nthis chapter will use a local filesystem-based Git repository for demonstration \npurposes.\n4.\t Enter the commands listed next to set up a local Git repository:\n$ cd $HOME\n$ mkdir config-repo\n$ cd config-repo\n$ git init .\n$ echo message : helloworld > application.properties\n$ git add -A .\n$ git commit -m \"Added sample application.properties\"\nThis code snippet creates a new Git repository on the local filesystem. A \nproperty file named application.properties with a message property  \nand value helloworld is also created.\n",
      "content_length": 1006,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "Scaling Microservices with Spring Cloud\n[ 216 ]\nThe file application.properties is created for demonstration purposes.  \nWe will change this in the subsequent sections.\n5.\t The next step is to change the configuration in the Config server to use the \nGit repository created in the previous step. In order to do this, rename the  \nfile application.properties to bootstrap.properties:\n6.\t Edit the contents of the new bootstrap.properties file to match  \nthe following:\nserver.port=8888\nspring.cloud.config.server.git.uri: file://${user.home}/config-\nrepo\nPort 8888 is the default port for the Config server. Even without configuring \nserver.port, the Config server should bind to 8888. In the Windows \nenvironment, an extra / is required in the file URL.\n7.\t Optionally, rename the default package of the auto-generated Application.\njava from com.example to com.brownfield.configserver. Add  \n@EnableConfigServer in Application.java:\n@EnableConfigServer\n@SpringBootApplication\npublic class ConfigserverApplication {\n8.\t Run the Config server by right-clicking on the project, and running it as a \nSpring Boot app.\n9.\t Visit http://localhost:8888/env to see whether the server is running.  \nIf everything is fine, this will list all environment configurations. Note that  \n/env is an actuator endpoint.\n",
      "content_length": 1301,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "Chapter 5\n[ 217 ]\n10.\t Check http://localhost:8888/application/default/master to see \nthe properties specific to application.properties, which were added \nin the earlier step. The browser will display the properties configured in \napplication.properties. The browser should display contents similar to \nthe following:\n{\"name\":\"application\",\"profiles\":[\"default\"],\"label\":\"master\",\"ver\nsion\":\"6046fd2ff4fa09d3843767660d963866ffcc7d28\",\"propertySources\"\n:[{\"name\":\"file:///Users/rvlabs /config-repo /application.properti\nes\",\"source\":{\"message\":\"helloworld\"}}]}\nUnderstanding the Config server URL\nIn the previous section, we used http://localhost:8888/application/default/\nmaster to explore the properties. How do we interpret this URL?\nThe first element in the URL is the application name. In the given example, the \napplication name should be application. The application name is a logical \nname given to the application, using the spring.application.name property in \nbootstrap.properties of the Spring Boot application. Each application must \nhave a unique name. The Config server will use the name to resolve and pick up \nappropriate properties from the Config server repository. The application name is \nalso sometimes referred to as service ID. If there is an application with the name \nmyapp, then there should be a myapp.properties in the configuration repository  \nto store all the properties related to that application.\nThe second part of the URL represents the profile. There can be more than one  \nprofile configured within the repository for an application. The profiles can be \nused in various scenarios. The two common scenarios are segregating different \nenvironments such as Dev, Test, Stage, Prod, and the like, or segregating server \nconfigurations such as Primary, Secondary, and so on. The first one represents \ndifferent environments of an application, whereas the second one represents \ndifferent servers where an application is deployed.\nThe profile names are logical names that will be used for matching the file name in \nthe repository. The default profile is named default. To configure properties for \ndifferent environments, we have to configure different files as given in the following \nexample. In this example, the first file is for the development environment whereas \nthe second is for the production environment:\napplication-development.properties\napplication-production.properties\n",
      "content_length": 2419,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "Scaling Microservices with Spring Cloud\n[ 218 ]\nThese are accessible using the following URLs respectively:\n•\t\nhttp://localhost:8888/application/development\n•\t\nhttp://localhost:8888/application/production\nThe last part of the URL is the label, and is named master by default. The label is an \noptional Git label that can be used, if required.\nIn short, the URL is based on the following pattern: http://localhost:8888/\n{name}/{profile}/{label}.\nThe configuration can also be accessed by ignoring the profile. In the preceding \nexample, all the following three URLs point to the same configuration:\n•\t\nhttp://localhost:8888/application/default\n•\t\nhttp://localhost:8888/application/master\n•\t\nhttp://localhost:8888/application/default/master\nThere is an option to have different Git repositories for different profiles. This  \nmakes sense for production systems, since the access to different repositories  \ncould be different.\nAccessing the Config Server from clients\nIn the previous section, a Config server is set up and accessed using a web browser. \nIn this section, the Search microservice will be modified to use the Config server.  \nThe Search microservice will act as a Config client.\nFollow these steps to use the Config server instead of reading properties from the \napplication.properties file:\n1.\t Add the Spring Cloud Config dependency and the actuator (if the actuator \nis not already in place) to the pom.xml file. The actuator is mandatory for \nrefreshing the configuration properties:\n  <dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-config</artifactId>\n  </dependency>\n2.\t Since we are modifying the Spring Boot Search microservice from the earlier \nchapter, we will have to add the following to include the Spring Cloud \ndependencies. This is not required if the project is created from scratch:\n  <dependencyManagement>\n    <dependencies>\n      <dependency>\n",
      "content_length": 1928,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "Chapter 5\n[ 219 ]\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-dependencies</artifactId>\n        <version>Brixton.RELEASE</version>\n        <type>pom</type>\n        <scope>import</scope>\n      </dependency>\n    </dependencies>\n  </dependencyManagement>\n3.\t The next screenshot shows the Cloud starter library selection screen. If the \napplication is built from the ground up, select the libraries as shown in the \nfollowing screenshot:\n \n4.\t Rename application.properties to bootstrap.properties, and add an \napplication name and a configuration server URL. The configuration server \nURL is not mandatory if the Config server is running on the default port \n(8888) on the local host:\nThe new bootstrap.properties file will look as follows:\nspring.application.name=search-service \nspring.cloud.config.uri=http://localhost:8888\nserver.port=8090\n",
      "content_length": 884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "Scaling Microservices with Spring Cloud\n[ 220 ]\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\nsearch-service is a logical name given to the Search microservice. This will \nbe treated as service ID. The Config server will look for search-service.\nproperties in the repository to resolve the properties.\n5.\t Create a new configuration file for search-service. Create a new  \nsearch-service.properties under the config-repo folder where the Git \nrepository is created. Note that search-service is the service ID given to the \nSearch microservice in the bootstrap.properties file. Move service-specific \nproperties from bootstrap.properties to the new search-service.\nproperties file. The following properties will be removed from bootstrap.\nproperties, and added to search-service.properties:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\n6.\t In order to demonstrate the centralized configuration of properties and \npropagation of changes, add a new application-specific property to the \nproperty file. We will add originairports.shutdown to temporarily take \nout an airport from the search. Users will not get any flights when searching \nfor an airport mentioned in the shutdown list:\noriginairports.shutdown=SEA\nIn this example, we will not return any flights when searching with SEA  \nas origin.\n7.\t Commit this new file into the Git repository by executing the following \ncommands:\ngit add –A .\ngit commit –m \"adding new configuration\"  \n8.\t The final search-service.properties file should look as follows:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\noriginairports.shutdown:SEA\n",
      "content_length": 1801,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "Chapter 5\n[ 221 ]\n9.\t The chapter5.search project's bootstrap.properties should look like  \nthe following:\nspring.application.name=search-service\nserver.port=8090\nspring.cloud.config.uri=http://localhost:8888\n10.\t Modify the Search microservice code to use the configured parameter, \noriginairports.shutdown. A RefreshScope annotation has to be added at \nthe class level to allow properties to be refreshed when there is a change. In \nthis case, we are adding a refresh scope to the SearchRestController class:\n@RefreshScope\n11.\t Add the following instance variable as a place holder for the new property \nthat is just added in the Config server. The property names in the search-\nservice.properties file must match:\n  @Value(\"${originairports.shutdown}\")\n  private String originAirportShutdownList;\n12.\t Change the application code to use this property. This is done by modifying \nthe search method as follows:\n  @RequestMapping(value=\"/get\", method =  \n    RequestMethod.POST)\n  List<Flight> search(@RequestBody SearchQuery query){\n    logger.info(\"Input : \"+ query);\n  if(Arrays.asList(originAirportShutdownList.split(\",\")) \n    .contains(query.getOrigin())){\n    logger.info(\"The origin airport is in shutdown state\");\n    return new ArrayList<Flight>();\n  }\n  return searchComponent.search(query);\n  }\nThe search method is modified to read the parameter \noriginAirportShutdownList and see whether the requested origin is in the \nshutdown list. If there is a match, then instead of proceeding with the actual \nsearch, the search method will return an empty flight list.\n13.\t Start the Config server. Then start the Search microservice. Make sure that \nthe RabbitMQ server is running.\n14.\t Modify the chapter5.website project to match the bootstrap.properties \ncontent as follows to utilize the Config server:\nspring.application.name=test-client\nserver.port=8001\nspring.cloud.config.uri=http://localhost:8888\n",
      "content_length": 1912,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "Scaling Microservices with Spring Cloud\n[ 222 ]\n15.\t Change the run method of CommandLineRunner in Application.java to \nquery SEA as the origin airport:\nSearchQuery = new SearchQuery(\"SEA\",\"SFO\",\"22-JAN-16\");\n16.\t Run the chapter5.website project. The CommandLineRunner will now \nreturn an empty flight list. The following message will be printed in  \nthe server:\nThe origin airport is in shutdown state\nHandling configuration changes\nThis section will demonstrate how to propagate configuration properties when there \nis a change:\n1.\t Change the property in the search-service.properties file to the following:\noriginairports.shutdown:NYC\nCommit the change in the Git repository. Refresh the Config server URL \n(http://localhost:8888/search-service/default) for this service and \nsee whether the property change is reflected. If everything is fine, we will see \nthe property change. The preceding request will force the Config server to \nread the property file again from the repository.\n2.\t Rerun the website project again, and observe the CommandLineRunner \nexecution. Note that in this case, we are not restarting the Search \nmicroservice nor the Config server. The service returns an empty  \nflight list as earlier, and still complains as follows:\nThe origin airport is in shutdown state\nThis means the change is not reflected in the Search service, and the service is \nstill working with an old copy of the configuration properties.\n3.\t In order to force reloading of the configuration properties, call the /refresh \nendpoint of the Search microservice. This is actually the actuator's refresh \nendpoint. The following command will send an empty POST to the /refresh \nendpoint:\ncurl –d {} localhost:8090/refresh\n4.\t Rerun the website project, and observe the CommandLineRunner execution. \nThis should return the list of flights that we have requested from SEA. Note \nthat the website project may fail if the Booking service is not up and running.\nThe /refresh endpoint will refresh the locally cached configuration \nproperties, and reload fresh values from the Config server.\n",
      "content_length": 2082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "Chapter 5\n[ 223 ]\nSpring Cloud Bus for propagating \nconfiguration changes\nWith the preceding approach, configuration parameters can be changed without \nrestarting the microservices. This is good when there are only one or two instances \nof the services running. What happens if there are many instances? For example, if \nthere are five instances, then we have to hit /refresh against each service instance. \nThis is definitely a cumbersome activity:\nThe Spring Cloud Bus provides a mechanism to refresh configurations across \nmultiple instances without knowing how many instances there are, or their locations. \nThis is particularly handy when there are many service instances of a microservice \nrunning or when there are many microservices of different types running. This is \ndone by connecting all service instances through a single message broker. Each \ninstance subscribes for change events, and refreshes its local configuration when \nrequired. This refresh is triggered by making a call to any one instance by hitting the \n/bus/refresh endpoint, which then propagates the changes through the cloud bus \nand the common message broker.\nIn this example, RabbitMQ is used as the AMQP message broker. Implement this by \nfollowing the steps documented as follows:\n1.\t Add a new dependency in the chapter5.search project's pom.xml file to \nintroduce the Cloud Bus dependency:\n<dependency>\n   <groupId>org.springframework.cloud</groupId>\n   <artifactId>spring-cloud-starter-bus-amqp</artifactId>\n</dependency>\n",
      "content_length": 1509,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "Scaling Microservices with Spring Cloud\n[ 224 ]\n2.\t The Search microservice also needs connectivity to the RabbitMQ, but this is \nalready provided in search-service.properties.\n3.\t Rebuild and restart the Search microservice. In this case, we will run two \ninstances of the Search microservice from a command line, as follows:\njava -jar -Dserver.port=8090  search-1.0.jar \njava -jar -Dserver.port=8091  search-1.0.jar\nThe two instances of the Search service will be now running, one on port \n8090 and another one on 8091.\n4.\t Rerun the website project. This is just to make sure that everything is \nworking. The Search service should return one flight at this point.\n5.\t Now, update search-service.properties with the following value,  \nand commit to Git:\noriginairports.shutdown:SEA\n6.\t Run the following command to /bus/refresh. Note that we are running  \na new bus endpoint against one of the instances, 8090 in this case:\ncurl –d {} localhost:8090/bus/refresh\n7.\t Immediately, we will see the following message for both instances:\nReceived remote refresh request. Keys refreshed [originairports.\nshutdown]\nThe bus endpoint sends a message to the message broker internally, which is \neventually consumed by all instances, reloading their property files. Changes \ncan also be applied to a specific application by specifying the application \nname like so:\n/bus/refresh?destination=search-service:**\nWe can also refresh specific properties by setting the property name as a parameter.\nSetting up high availability for the Config \nserver\nThe previous sections explored how to set up the Config server, allowing real-time \nrefresh of configuration properties. However, the Config server is a single point of \nfailure in this architecture.\nThere are three single points of failure in the default architecture that was established \nin the previous section. One of them is the availability of the Config server itself, the \nsecond one is the Git repository, and the third one is the RabbitMQ server.\n",
      "content_length": 1995,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "Chapter 5\n[ 225 ]\nThe following diagram shows a high availability architecture for the Config server:\nThe architecture mechanisms and rationale are explained as follows:\nThe Config server requires high availability, since the services won't be able to \nbootstrap if the Config server is not available. Hence, redundant Config servers \nare required for high availability. However, the applications can continue to run \nif the Config server is unavailable after the services are bootstrapped. In this case, \nservices will run with the last known configuration state. Hence, the Config server \navailability is not at the same critical level as the microservices availability.\nIn order to make the Config server highly available, we need multiple instances \nof the Config servers. Since the Config server is a stateless HTTP service, multiple \ninstances of configuration servers can be run in parallel. Based on the load on the \nconfiguration server, a number of instances have to be adjusted. The bootstrap.\nproperties file is not capable of handling more than one server address. Hence, \nmultiple configuration servers should be configured to run behind a load balancer or \nbehind a local DNS with failover and fallback capabilities. The load balancer or DNS \nserver URL will be configured in the microservices' bootstrap.properties file. \nThis is with the assumption that the DNS or the load balancer is highly available  \nand capable of handling failovers.\nIn a production scenario, it is not recommended to use a local file-based Git \nrepository. The configuration server should be typically backed with a highly \navailable Git service. This is possible by either using an external highly available Git \nservice or a highly available internal Git service. SVN can also be considered.\n",
      "content_length": 1785,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "Scaling Microservices with Spring Cloud\n[ 226 ]\nHaving said that, an already bootstrapped Config server is always capable of \nworking with a local copy of the configuration. Hence, we need a highly available Git \nonly when the Config server needs to be scaled. Therefore, this too is not as critical  \nas the microservices availability or the Config server availability.\nThe GitLab example for setting up high availability is available at \nhttps://about.gitlab.com/high-availability/.\nRabbitMQ also has to be configured for high availability. The high availability for \nRabbitMQ is needed only to push configuration changes dynamically to all instances. \nSince this is more of an offline controlled activity, it does not really require the same \nhigh availability as required by the components.\nRabbitMQ high availability can be achieved by either using a cloud service or a \nlocally configured highly available RabbitMQ service.\nSetting up high availability for Rabbit MQ is documented at \nhttps://www.rabbitmq.com/ha.html.\nMonitoring the Config server health\nThe Config server is nothing but a Spring Boot application, and is, by default, \nconfigured with an actuator. Hence, all actuator endpoints are applicable for  \nthe Config server. The health of the server can be monitored using the following  \nactuator URL: http://localhost:8888/health.\nConfig server for configuration files\nWe may run into scenarios where we need a complete configuration file such \nas logback.xml to be externalized. The Config server provides a mechanism to \nconfigure and store such files. This is achievable by using the URL format as follows: \n/{name}/{profile}/{label}/{path}.\nThe name, profile, and label have the same meanings as explained earlier. The path \nindicates the file name such as logback.xml.\n",
      "content_length": 1792,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "Chapter 5\n[ 227 ]\nCompleting changes to use the Config server\nIn order to build this capability to complete BrownField Airline's PSS, we have \nto make use of the configuration server for all services. All microservices in the \nexamples given in chapter5.* need to make similar changes to look to the Config \nserver for getting the configuration parameters.\nThe following are a few key change considerations:\n•\t\nThe Fare service URL in the booking component will also be externalized:\nprivate static final String FareURL = \"/fares\";\n  \n@Value(\"${fares-service.url}\")\nprivate String fareServiceUrl;\nFare = restTemplate.getForObject(fareServiceUrl+FareURL +\"/\nget?flightNumber=\"+record.getFlightNumber()+\"&flightDate=\"+record.\ngetFlightDate(),Fare.class);\nAs shown in the preceding code snippet, the Fare service URL is fetched \nthrough a new property: fares-service.url.\n•\t\nWe are not externalizing the queue names used in the Search, Booking, and \nCheck-in services at the moment. Later in this chapter, these will be changed \nto use Spring Cloud Streams.\nFeign as a declarative REST client\nIn the Booking microservice, there is a synchronous call to Fare. RestTemplate \nis used for making the synchronous call. When using RestTemplate, the URL \nparameter is constructed programmatically, and data is sent across to the other \nservice. In more complex scenarios, we will have to get to the details of the HTTP \nAPIs provided by RestTemplate or even to APIs at a much lower level.\nFeign is a Spring Cloud Netflix library for providing a higher level of abstraction \nover REST-based service calls. Spring Cloud Feign works on a declarative principle. \nWhen using Feign, we write declarative REST service interfaces at the client, and \nuse those interfaces to program the client. The developer need not worry about the \nimplementation of this interface. This will be dynamically provisioned by Spring at \nruntime. With this declarative approach, developers need not get into the details of \nthe HTTP level APIs provided by RestTemplate.\n",
      "content_length": 2033,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "Scaling Microservices with Spring Cloud\n[ 228 ]\nThe following code snippet is the existing code in the Booking microservice for \ncalling the Fare service: \nFare fare = restTemplate.getForObject(FareURL +\"/\nget?flightNumber=\"+record.getFlightNumber()+\"&flightDate=\"+record.\ngetFlightDate(),Fare.class);\nIn order to use Feign, first we need to change the pom.xml file to include the Feign \ndependency as follows:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-feign</artifactId>\n</dependency>\nFor a new Spring Starter project, Feign can be selected from the starter library \nselection screen, or from http://start.spring.io/. This is available under  \nCloud Routing as shown in the following screenshot:\nThe next step is to create a new FareServiceProxy interface. This will act as a proxy \ninterface of the actual Fare service:\n@FeignClient(name=\"fares-proxy\", url=\"localhost:8080/fares\")\npublic interface FareServiceProxy {\n  @RequestMapping(value = \"/get\", method=RequestMethod.GET)\n  Fare getFare(@RequestParam(value=\"flightNumber\") String  \n    flightNumber, @RequestParam(value=\"flightDate\") String  \n    flightDate);\n}\n",
      "content_length": 1172,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "Chapter 5\n[ 229 ]\nThe FareServiceProxy interface has a @FeignClient annotation. This annotation \ntells Spring to create a REST client based on the interface provided. The value could \nbe a service ID or a logical name. The url indicates the actual URL where the target \nservice is running. Either name or value is mandatory. In this case, since we have \nurl, the name attribute is irrelevant.\nUse this service proxy to call the Fare service. In the Booking microservice, we have \nto tell Spring that Feign clients exist in the Spring Boot application, which are to be \nscanned and discovered. This will be done by adding @EnableFeignClients at the \nclass level of BookingComponent. Optionally, we can also give the package names  \nto scan.\nChange BookingComponent, and make changes to the calling part. This is as simple \nas calling another Java interface:\nFare = fareServiceProxy.getFare(record.getFlightNumber(), record.\ngetFlightDate());\nRerun the Booking microservice to see the effect.\nThe URL of the Fare service in the FareServiceProxy interface is hardcoded: \nurl=\"localhost:8080/fares\".\nFor the time being, we will keep it like this, but we are going to change this later in \nthis chapter.\nRibbon for load balancing\nIn the previous setup, we were always running with a single instance of the \nmicroservice. The URL is hardcoded both in client as well as in the service-to-service \ncalls. In the real world, this is not a recommended approach, since there could be more \nthan one service instance. If there are multiple instances, then ideally, we should use \na load balancer or a local DNS server to abstract the actual instance locations, and \nconfigure an alias name or the load balancer address in the clients. The load balancer \nthen receives the alias name, and resolves it with one of the available instances. With \nthis approach, we can configure as many instances behind a load balancer. It also helps \nus to handle server failures transparent to the client.\n",
      "content_length": 1976,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "Scaling Microservices with Spring Cloud\n[ 230 ]\nThis is achievable with Spring Cloud Netflix Ribbon. Ribbon is a client-side load \nbalancer which can do round-robin load balancing across a set of servers. There \ncould be other load balancing algorithms possible with the Ribbon library. Spring \nCloud offers a declarative way to configure and use the Ribbon client.\nAs shown in the preceding diagram, the Ribbon client looks for the Config server to \nget the list of available microservice instances, and, by default, applies a round-robin \nload balancing algorithm.\nIn order to use the Ribbon client, we will have to add the following dependency  \nto the pom.xml file:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-ribbon</artifactId>\n</dependency>\nIn case of development from ground up, this can be selected from the Spring  \nStarter libraries, or from http://start.spring.io/. Ribbon is available under \nCloud Routing:\n",
      "content_length": 971,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "Chapter 5\n[ 231 ]\nUpdate the Booking microservice configuration file, booking-service.properties, \nto include a new property to keep the list of the Fare microservices:\nfares-proxy.ribbon.listOfServers=localhost:8080,localhost:8081\nGoing back and editing the FareServiceProxy class created in the previous section \nto use the Ribbon client, we note that the value of the @RequestMapping annotations \nis changed from /get to /fares/get so that we can move the host name and port to \nthe configuration easily:\n@FeignClient(name=\"fares-proxy\")\n@RibbonClient(name=\"fares\")\npublic interface FareServiceProxy {\n  @RequestMapping(value = \"fares/get\", method=RequestMethod.GET)\nWe can now run two instances of the Fares microservices. Start one of them on 8080, \nand the other one on 8081:\njava -jar -Dserver.port=8080 fares-1.0.jar\njava -jar -Dserver.port=8081 fares-1.0.jar\nRun the Booking microservice. When the Booking microservice is bootstrapped, the \nCommandLineRunner automatically inserts one booking record. This will go to the \nfirst server.\nWhen running the website project, it calls the Booking service. This request will go  \nto the second server.\nOn the Booking service, we see the following trace, which says there are two servers \nenlisted:\nDynamicServerListLoadBalancer:{NFLoadBalancer:name=fares-proxy,current \nlist of Servers=[localhost:8080, localhost:8081],Load balancer stats=Zone \nstats: {unknown=[Zone:unknown;  Instance count:2;  Active connections \ncount: 0;  Circuit breaker tripped count: 0;  Active connections per \nserver: 0.0;]\n}, \n",
      "content_length": 1556,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "Scaling Microservices with Spring Cloud\n[ 232 ]\nEureka for registration and discovery\nSo far, we have achieved externalizing configuration parameters as well as load \nbalancing across many service instances.\nRibbon-based load balancing is sufficient for most of the microservices requirements. \nHowever, this approach falls short in a couple of scenarios:\n•\t\nIf there is a large number of microservices, and if we want to optimize \ninfrastructure utilization, we will have to dynamically change the number \nof service instances and the associated servers. It is not easy to predict and \npreconfigure the server URLs in a configuration file.\n•\t\nWhen targeting cloud deployments for highly scalable microservices, static \nregistration and discovery is not a good solution considering the elastic \nnature of the cloud environment.\n•\t\nIn the cloud deployment scenarios, IP addresses are not predictable, and \nwill be difficult to statically configure in a file. We will have to update the \nconfiguration file every time there is a change in address.\nThe Ribbon approach partially addresses this issue. With Ribbon, we can \ndynamically change the service instances, but whenever we add new service \ninstances or shut down instances, we will have to manually update the Config \nserver. Though the configuration changes will be automatically propagated to all \nrequired instances, the manual configuration changes will not work with large scale \ndeployments. When managing large deployments, automation, wherever possible,  \nis paramount.\nTo fix this gap, the microservices should self-manage their life cycle by dynamically \nregistering service availability, and provision automated discovery for consumers.\nUnderstanding dynamic service registration \nand discovery\nDynamic registration is primarily from the service provider's point of view. With \ndynamic registration, when a new service is started, it automatically enlists its \navailability in a central service registry. Similarly, when a service goes out of service, \nit is automatically delisted from the service registry. The registry always keeps  \nup-to-date information of the services available, as well as their metadata.\n",
      "content_length": 2179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "Chapter 5\n[ 233 ]\nDynamic discovery is applicable from the service consumer's point of view. Dynamic \ndiscovery is where clients look for the service registry to get the current state of  \nthe services topology, and then invoke the services accordingly. In this approach, \ninstead of statically configuring the service URLs, the URLs are picked up from the \nservice registry.\nThe clients may keep a local cache of the registry data for faster access. Some registry \nimplementations allow clients to keep a watch on the items they are interested in. \nIn this approach, the state changes in the registry server will be propagated to the \ninterested parties to avoid using stale data.\nThere are a number of options available for dynamic service registration and \ndiscovery. Netflix Eureka, ZooKeeper, and Consul are available as part of Spring \nCloud, as shown in the http://start.spring.io/ screenshot given next. Etcd \nis another service registry available outside of Spring Cloud to achieve dynamic \nservice registration and discovery. In this chapter, we will focus on the Eureka \nimplementation:\n",
      "content_length": 1098,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "Scaling Microservices with Spring Cloud\n[ 234 ]\nUnderstanding Eureka\nSpring Cloud Eureka also comes from Netflix OSS. The Spring Cloud project \nprovides a Spring-friendly declarative approach for integrating Eureka with  \nSpring-based applications. Eureka is primarily used for self-registration, dynamic \ndiscovery, and load balancing. Eureka uses Ribbon for load balancing internally:\nAs shown in the preceding diagram, Eureka consists of a server component \nand a client-side component. The server component is the registry in which all \nmicroservices register their availability. The registration typically includes service \nidentity and its URLs. The microservices use the Eureka client for registering \ntheir availability. The consuming components will also use the Eureka client for \ndiscovering the service instances.\nWhen a microservice is bootstrapped, it reaches out to the Eureka server, and \nadvertises its existence with the binding information. Once registered, the service \nendpoint sends ping requests to the registry every 30 seconds to renew its lease. If a \nservice endpoint cannot renew its lease in a few attempts, that service endpoint will \nbe taken out of the service registry. The registry information will be replicated to \nall Eureka clients so that the clients have to go to the remote Eureka server for each \nand every request. Eureka clients fetch the registry information from the server, and \ncache it locally. After that, the clients use that information to find other services. This \ninformation is updated periodically (every 30 seconds) by getting the delta updates \nbetween the last fetch cycle and the current one.\n",
      "content_length": 1654,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "Chapter 5\n[ 235 ]\nWhen a client wants to contact a microservice endpoint, the Eureka client provides \na list of currently available services based on the requested service ID. The Eureka \nserver is zone aware. Zone information can also be supplied when registering a \nservice. When a client requests for a services instance, the Eureka service tries to find \nthe service running in the same zone. The Ribbon client then load balances across \nthese available service instances supplied by the Eureka client. The communication \nbetween the Eureka client and the server is done using REST and JSON.\nSetting up the Eureka server\nIn this section, we will run through the steps required for setting up the  \nEureka server.\nThe full source code of this section is available under the \nchapter5.eurekaserver project in the code files. Note that the \nEureka server registration and refresh cycles take up to 30 seconds. \nHence, when running services and clients, wait for 40-50 seconds.\n1.\t Start a new Spring Starter project, and select Config Client, Eureka Server, \nand Actuator:\n",
      "content_length": 1074,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "Scaling Microservices with Spring Cloud\n[ 236 ]\nThe project structure of the Eureka server is shown in the following image:\nNote that the main application is named EurekaserverApplication.java.\n2.\t Rename application.properties to bootstrap.properties since this is \nusing the Config server. As we did earlier, configure the details of the Config \nserver in the bootsratp.properties file so that it can locate the Config \nserver instance. The bootstrap.properties file will look as follows:\nspring.application.name=eureka-server1\nserver.port:8761\nspring.cloud.config.uri=http://localhost:8888\nThe Eureka server can be set up in a standalone mode or in a clustered \nmode. We will start with the standalone mode. By default, the Eureka server \nitself is another Eureka client. This is particularly useful when there are \nmultiple Eureka servers running for high availability. The client component \nis responsible for synchronizing state from the other Eureka servers. The \nEureka client is taken to its peers by configuring the eureka.client.\nserviceUrl.defaultZone property.\nIn the standalone mode, we point eureka.client.serviceUrl.\ndefaultZone back to the same standalone instance. Later we will  \nsee how we can run Eureka servers in a clustered mode.\n",
      "content_length": 1254,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "Chapter 5\n[ 237 ]\n3.\t Create a eureka-server1.properties file, and update it in the Git \nrepository. eureka-server1 is the name of the application given in the \napplication's bootstrap.properties file in the previous step. As shown \nin the following code, serviceUrl points back to the same server. Once the \nfollowing properties are added, commit the file to the Git repository:\nspring.application.name=eureka-server1\neureka.client.serviceUrl.defaultZone:http://localhost:8761/eureka/\neureka.client.registerWithEureka:false\neureka.client.fetchRegistry:false\n4.\t Change the default Application.java. In this example, the package is \nalso renamed as com.brownfield.pss.eurekaserver, and the class name \nchanged to EurekaserverApplication. In EurekaserverApplication,  \nadd @EnableEurekaServer:\n@EnableEurekaServer\n@SpringBootApplication\npublic class EurekaserverApplication {\n5.\t We are now ready to start the Eureka server. Ensure that the Config server is \nalso started. Right-click on the application and then choose Run As | Spring \nBoot App. Once the application is started, open http://localhost:8761 in \na browser to see the Eureka console.\n6.\t In the console, note that there is no instance registered under Instances \ncurrently registered with Eureka. Since no services have been started with \nthe Eureka client enabled, the list is empty at this point.\n",
      "content_length": 1362,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "Scaling Microservices with Spring Cloud\n[ 238 ]\n7.\t Making a few changes to our microservice will enable dynamic registration \nand discovery using the Eureka service. To do this, first we have to add the \nEureka dependencies to the pom.xml file. If the services are being built up \nfresh using the Spring Starter project, then select Config Client, Actuator, \nWeb as well as Eureka discovery client as follows:\n8.\t Since we are modifying our microservices, add the following additional \ndependency to all microservices in their pom.xml files:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-eureka</artifactId>\n</dependency>\n9.\t The following property has to be added to all microservices in their \nrespective configuration files under config-repo. This will help the \nmicroservices to connect to the Eureka server. Commit to Git once updates \nare completed:\neureka.client.serviceUrl.defaultZone: http://localhost:8761/\neureka/\n",
      "content_length": 975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "Chapter 5\n[ 239 ]\n10.\t Add @EnableDiscoveryClient to all microservices in their respective \nSpring Boot main classes. This asks Spring Boot to register these services at \nstart up to advertise their availability.\n11.\t Start all servers except Booking. Since we are using the Ribbon client on the \nBooking service, the behavior could be different when we add the Eureka \nclient in the class path. We will fix this soon.\n12.\t Going to the Eureka URL (http://localhost:8761), you can see that all \nthree instances are up and running:\nTime to fix the issue with Booking. We will remove our earlier Ribbon client, \nand use Eureka instead. Eureka internally uses Ribbon for load balancing. \nHence, the load balancing behavior will not change.\n13.\t Remove the following dependency:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-ribbon</artifactId>\n</dependency>\n14.\t Also remove the @RibbonClient(name=\"fares\") annotation from the \nFareServiceProxy class.\n15.\t Update @FeignClient(name=\"fares-service\") to match the actual \nFare microservices' service ID. In this case, fare-service is the service ID \nconfigured in the Fare microservices' bootstrap.properties. This is the \nname that the Eureka discovery client sends to the Eureka server. The service \nID will be used as a key for the services registered in the Eureka server.\n16.\t Also remove the list of servers from the booking-service.properties  \nfile. With Eureka, we are going to dynamically discover this list from the \nEureka server:\nfares-proxy.ribbon.listOfServers=localhost:8080, localhost:8081\n",
      "content_length": 1601,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "Scaling Microservices with Spring Cloud\n[ 240 ]\n17.\t Start the Booking service. You will see that CommandLineRunner successfully \ncreated a booking, which involves calling the Fare services using the Eureka \ndiscovery mechanism. Go back to the URL to see all the registered services:\n18.\t Change the website project's bootstrap.properties file to make use of \nEureka rather than connecting directly to the service instances. We will not \nuse the Feign client in this case. Instead, for demonstration purposes, we  \nwill use the load balanced RestTemplate. Commit these changes to the  \nGit repository:\nspring.application.name=test-client\neureka.client.serviceUrl.defaultZone: http://localhost:8761/\neureka/\n19.\t Add @EnableDiscoveryClient to the Application class to make the client \nEureka-aware.\n20.\t Edit both Application.java as well as BrownFieldSiteController.\njava. Add three RestTemplate instances. This time, we annotate them with \n@Loadbalanced to ensure that we use the load balancing features using \nEureka and Ribbon. RestTemplate cannot be automatically injected.  \nHence, we have to provide a configuration entry as follows:\n@Configuration\nclass AppConfiguration {\n    @LoadBalanced\n    @Bean\n    RestTemplate restTemplate() {\n        return new RestTemplate();\n    }\n}\n@Autowired\nRestTemplate searchClient;\n   \n@Autowired\nRestTemplate bookingClient;\n  \n@Autowired\nRestTemplate checkInClient;\n",
      "content_length": 1408,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "Chapter 5\n[ 241 ]\n21.\t We use these RestTemplate instances to call the microservices. Replace the \nhardcoded URLs with service IDs that are registered in the Eureka server. \nIn the following code, we use the service names search-service, book-\nservice, and checkin-service instead of explicit host names and ports:\nFlight[] flights = searchClient.postForObject(\"http://search-\nservice/search/get\", searchQuery, Flight[].class);\nlong bookingId = bookingClient.postForObject(\"http://book-service/\nbooking/create\", booking, long.class);\n  \nlong checkinId = checkInClient.postForObject(\"http://checkin-\nservice/checkin/create\", checkIn, long.class);\n22.\t We are now ready to run the client. Run the website project. If everything \nis fine, the website project's CommandLineRunner will successfully perform \nsearch, booking, and check-in. The same can also be tested using the browser \nby pointing the browser to http://localhost:8001.\nHigh availability for Eureka\nIn the previous example, there was only one Eureka server in standalone mode.  \nThis is not good enough for a real production system.\nThe Eureka client connects to the server, fetches registry information, and stores it \nlocally in a cache. The client always works with this local cache. The Eureka client \nchecks the server periodically for any state changes. In the case of a state change, it \ndownloads the changes from the server, and updates the cache. If the Eureka server \nis not reachable, then the Eureka clients can still work with the last-known state of \nthe servers based on the data available in the client cache. However, this could lead \nto stale state issues quickly.\n",
      "content_length": 1645,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "Scaling Microservices with Spring Cloud\n[ 242 ]\nThis section will explore the high availability for the Eureka server. The high \navailability architecture is shown in the following diagram:\nThe Eureka server is built with a peer-to-peer data synchronization mechanism. \nThe runtime state information is not stored in a database, but managed using an in-\nmemory cache. The high availability implementation favors availability and partition \ntolerance in the CAP theorem, leaving out consistency. Since the Eureka server \ninstances are synchronized with each other using an asynchronous mechanism, \nthe states may not always match between server instances. The peer-to-peer \nsynchronization is done by pointing serviceUrls to each other. If there is more than \none Eureka server, each one has to be connected to at least one of the peer servers. \nSince the state is replicated across all peers, Eureka clients can connect to any one of \nthe available Eureka servers.\nThe best way to achieve high availability for Eureka is to cluster multiple Eureka \nservers, and run them behind a load balancer or a local DNS. The clients always \nconnect to the server using the DNS/load balancer. At runtime, the load balancer \ntakes care of selecting the appropriate servers. This load balancer address will be \nprovided to the Eureka clients.\nThis section will showcase how to run two Eureka servers in a cluster for high \navailability. For this, define two property files: eureka-server1 and eureka-\nserver2. These are peer servers; if one fails, the other one will take over. Each of \nthese servers will also act as a client for the other so that they can sync their states. \nTwo property files are defined in the following snippet. Upload and commit these \nproperties to the Git repository. \n",
      "content_length": 1781,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "Chapter 5\n[ 243 ]\nThe client URLs point to each other, forming a peer network as shown in the \nfollowing configuration:\neureka-server1.properties\neureka.client.serviceUrl.defaultZone:http://localhost:8762/eureka/\neureka.client.registerWithEureka:false\neureka.client.fetchRegistry:false\neureka-server2.properties\neureka.client.serviceUrl.defaultZone:http://localhost:8761/eureka/\neureka.client.registerWithEureka:false\neureka.client.fetchRegistry:false\nUpdate the bootstrap.properties file of Eureka, and change the application name \nto eureka. Since we are using two profiles, based on the active profile supplied at \nstartup, the Config server will look for either eureka-server1 or eureka-server2:\nspring.application.name=eureka\nspring.cloud.config.uri=http://localhost:8888\nStart two instances of the Eureka servers, server1 on 8761 and server2 on 8762:\njava -jar –Dserver.port=8761 -Dspring.profiles.active=server1 demo-0.0.1-\nSNAPSHOT.jar\njava -jar –Dserver.port=8762 -Dspring.profiles.active=server2 demo-0.0.1-\nSNAPSHOT.jar\nAll our services still point to the first server, server1. Open both the browser \nwindows: http://localhost:8761 and http://localhost:8762.\nStart all microservices. The one which opened 8761 will immediately reflect the \nchanges, whereas the other one will take 30 seconds for reflecting the states. Since \nboth the servers are in a cluster, the state is synchronized between these two servers. \nIf we keep these servers behind a load balancer/DNS, then the client will always \nconnect to one of the available servers.\nAfter completing this exercise, switch back to the standalone mode for the remaining \nexercises.\n",
      "content_length": 1647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "Scaling Microservices with Spring Cloud\n[ 244 ]\nZuul proxy as the API gateway\nIn most microservice implementations, internal microservice endpoints are not \nexposed outside. They are kept as private services. A set of public services will be \nexposed to the clients using an API gateway. There are many reasons to do this:\n•\t\nOnly a selected set of microservices are required by the clients.\n•\t\nIf there are client-specific policies to be applied, it is easy to apply them in  \na single place rather than in multiple places. An example of such a scenario  \nis the cross-origin access policy.\n•\t\nIt is hard to implement client-specific transformations at the service endpoint.\n•\t\nIf there is data aggregation required, especially to avoid multiple client calls \nin a bandwidth-restricted environment, then a gateway is required in the \nmiddle.\nZuul is a simple gateway service or edge service that suits these situations well. \nZuul also comes from the Netflix family of microservice products. Unlike many \nenterprise API gateway products, Zuul provides complete control for the developers \nto configure or program based on specific requirements:\nThe Zuul proxy internally uses the Eureka server for service discovery, and Ribbon \nfor load balancing between service instances.\nThe Zuul proxy is also capable of routing, monitoring, managing resiliency, security, \nand so on. In simple terms, we can consider Zuul a reverse proxy service. With Zuul, \nwe can even change the behaviors of the underlying services by overriding them at \nthe API layer.\n",
      "content_length": 1547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "Chapter 5\n[ 245 ]\nSetting up Zuul\nUnlike the Eureka server and the Config server, in typical deployments, Zuul \nis specific to a microservice. However, there are deployments in which one API \ngateway covers many microservices. In this case, we are going to add Zuul for each \nof our microservices: Search, Booking, Fare, and Check-in:\nThe full source code of this section is available under the chapter5.*-\napigateway project in the code files.\n1.\t Convert the microservices one by one. Start with Search API Gateway.  \nCreate a new Spring Starter project, and select Zuul, Config Client, Actuator, \nand Eureka Discovery:\n",
      "content_length": 622,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "Scaling Microservices with Spring Cloud\n[ 246 ]\nThe project structure for search-apigateway is shown in the following \ndiagram:\n2.\t The next step is to integrate the API gateway with Eureka and the Config \nserver. Create a search-apigateway.property file with the contents given \nnext, and commit to the Git repository.\nThis configuration also sets a rule on how to forward traffic. In this case, any \nrequest coming on the /api endpoint of the API gateway should be sent to \nsearch-service:\nspring.application.name=search-apigateway\nzuul.routes.search-apigateway.serviceId=search-service\nzuul.routes.search-apigateway.path=/api/**\neureka.client.serviceUrl.defaultZone:http://localhost:8761/eureka/\nsearch-service is the service ID of the Search service, and it will be \nresolved using the Eureka server.\n3.\t Update the bootstrap.properties file of search-apigateway as follows. \nThere is nothing new in this configuration—a name to the service, the port, \nand the Config server URL:\nspring.application.name=search-apigateway\nserver.port=8095\nspring.cloud.config.uri=http://localhost:8888\n",
      "content_length": 1089,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "Chapter 5\n[ 247 ]\n4.\t Edit Application.java. In this case, the package name and the class \nname are also changed to com.brownfield.pss.search.apigateway and \nSearchApiGateway respectively. Also add @EnableZuulProxy to tell Spring \nBoot that this is a Zuul proxy:\n@EnableZuulProxy\n@EnableDiscoveryClient\n@SpringBootApplication\npublic class SearchApiGateway {\n5.\t Run this as a Spring Boot app. Before that, ensure that the Config server, the \nEureka server, and the Search microservice are running.\n6.\t Change the website project's CommandLineRunner as well as \nBrownFieldSiteController to make use of the API gateway:\nFlight[] flights = searchClient.postForObject(\"http://search-\napigateway/api/search/get\", searchQuery, Flight[].class); \nIn this case, the Zuul proxy acts as a reverse proxy which proxies all microservice \nendpoints to consumers. In the preceding example, the Zuul proxy does not add \nmuch value, as we just pass through the incoming requests to the corresponding \nbackend service.\nZuul is particularly useful when we have one or more requirements like the \nfollowing:\n•\t\nEnforcing authentication and other security policies at the gateway instead of \ndoing that on every microservice endpoint. The gateway can handle security \npolicies, token handling, and so on before passing the request to the relevant \nservices behind. It can also do basic rejections based on some business \npolicies such as blocking requests coming from certain black-listed users.\n•\t\nBusiness insights and monitoring can be implemented at the gateway \nlevel. Collect real-time statistical data, and push it to an external system \nfor analysis. This will be handy as we can do this at one place rather than \napplying it across many microservices.\n•\t\nAPI gateways are useful in scenarios where dynamic routing is required \nbased on fine-grained controls. For example, send requests to different \nservice instances based on business specific values such as \"origin country\". \nAnother example is all requests coming from a region to be sent to one group \nof service instances. Yet another example is all requests requesting for a \nparticular product have to be routed to a group of service instances.\n",
      "content_length": 2190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "Scaling Microservices with Spring Cloud\n[ 248 ]\n•\t\nHandling the load shredding and throttling requirements is another scenario \nwhere API gateways are useful. This is when we have to control load based \non set thresholds such as number of requests in a day. For example, control \nrequests coming from a low-value third party online channel.\n•\t\nThe Zuul gateway is useful for fine-grained load balancing scenarios. The \nZuul, Eureka client, and Ribbon together provide fine-grained controls over \nthe load balancing requirements. Since the Zuul implementation is nothing \nbut another Spring Boot application, the developer has full control over the \nload balancing.\n•\t\nThe Zuul gateway is also useful in scenarios where data aggregation \nrequirements are in place. If the consumer wants higher level coarse-grained \nservices, then the gateway can internally aggregate data by calling more than \none service on behalf of the client. This is particularly applicable when the \nclients are working in low bandwidth environments.\nZuul also provides a number of filters. These filters are classified as pre filters, \nrouting filters, post filters, and error filters. As the names indicate, these are applied \nat different stages of the life cycle of a service call. Zuul also provides an option for \ndevelopers to write custom filters. In order to write a custom filter, extend from the \nabstract ZuulFilter, and implement the following methods:\npublic class CustomZuulFilter extends ZuulFilter{\npublic Object run(){}\npublic boolean shouldFilter(){}\npublic int filterOrder(){}\npublic String filterType(){}\nOnce a custom filter is implemented, add that class to the main context. In our \nexample case, add this to the SearchApiGateway class as follows:\n@Bean\npublic CustomZuulFilter customFilter() {\n    return new CustomZuulFilter();\n}\nAs mentioned earlier, the Zuul proxy is a Spring Boot service. We can customize the \ngateway programmatically in the way we want. As shown in the following code, \nwe can add custom endpoints to the gateway, which, in turn, can call the backend \nservices:\n@RestController \nclass SearchAPIGatewayController {\n  @RequestMapping(\"/\")\n  String greet(HttpServletRequest req){\n",
      "content_length": 2199,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "Chapter 5\n[ 249 ]\n    return \"<H1>Search Gateway Powered By Zuul</H1>\";\n  }\n}\nIn the preceding case, it just adds a new endpoint, and returns a value from the \ngateway. We can further use @Loadbalanced RestTemplate to call a backend \nservice. Since we have full control, we can do transformations, data aggregation, \nand so on. We can also use the Eureka APIs to get the server list, and implement \ncompletely independent load-balancing or traffic-shaping mechanisms instead  \nof the out-of-the-box load balancing features provided by Ribbon.\nHigh availability of Zuul\nZuul is just a stateless service with an HTTP endpoint, hence, we can have as many \nZuul instances as we need. There is no affinity or stickiness required. However, \nthe availability of Zuul is extremely critical as all traffic from the consumer to the \nprovider flows through the Zuul proxy. However, the elastic scaling requirements \nare not as critical as the backend microservices where all the heavy lifting happens.\nThe high availability architecture of Zuul is determined by the scenario in which we \nare using Zuul. The typical usage scenarios are:\n•\t\nWhen a client-side JavaScript MVC such as AngularJS accesses Zuul services \nfrom a remote browser.\n•\t\nAnother microservice or non-microservice accesses services via Zuul\nIn some cases, the client may not have the capabilities to use the Eureka client \nlibraries, for example, a legacy application written on PL/SQL. In some cases, \norganization policies do not allow Internet clients to handle client-side load \nbalancing. In the case of browser-based clients, there are third-party Eureka \nJavaScript libraries available.\nIt all boils down to whether the client is using Eureka client libraries or not. Based on \nthis, there are two ways we can set up Zuul for high availability.\n",
      "content_length": 1810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "Scaling Microservices with Spring Cloud\n[ 250 ]\nHigh availability of Zuul when the client is also a \nEureka client\nIn this case, since the client is also another Eureka client, Zuul can be configured just \nlike other microservices. Zuul registers itself to Eureka with a service ID. The clients \nthen use Eureka and the service ID to resolve Zuul instances:\nAs shown in the preceding diagram, Zuul services register themselves with Eureka \nwith a service ID, search-apigateway in our case. The Eureka client asks for the \nserver list with the ID search-apigateway. The Eureka server returns the list of \nservers based on the current Zuul topology. The Eureka client, based on this list \npicks up one of the servers, and initiates the call.\nAs we saw earlier, the client uses the service ID to resolve the Zuul instance. In the \nfollowing case, search-apigateway is the Zuul instance ID registered with Eureka:\nFlight[] flights = searchClient.postForObject(\"http://search-\napigateway/api/search/get\", searchQuery, Flight[].class); \n",
      "content_length": 1031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "Chapter 5\n[ 251 ]\nHigh availability when the client is not a Eureka \nclient\nIn this case, the client is not capable of handling load balancing by using the Eureka \nserver. As shown in the following diagram, the client sends the request to a load \nbalancer, which in turn identifies the right Zuul service instance. The Zuul instance, \nin this case, will be running behind a load balancer such as HAProxy or a hardware \nload balancer like NetScaler:\nThe microservices will still be load balanced by Zuul using the Eureka server.\nCompleting Zuul for all other services\nIn order to complete this exercise, add API gateway projects (name them as \n*-apigateway) for all our microservices. The following steps are required to achieve \nthis task:\n1.\t Create new property files per service, and check in to the Git repositories.\n2.\t Change application.properties to bootstrap.properties, and add the \nrequired configurations.\n3.\t Add @EnableZuulProxy to Application.java in each of the *-apigateway \nprojects.\n",
      "content_length": 1002,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "Scaling Microservices with Spring Cloud\n[ 252 ]\n4.\t Add @EnableDiscoveryClient in all the Application.java files under \neach of the *-apigateway projects.\n5.\t Optionally, change the package names and file names generated by default.\nIn the end, we will have the following API gateway projects:\n•\t\nchapter5.fares-apigateway\n•\t\nchapter5.search-apigateway\n•\t\nchapter5.checkin-apigateway\n•\t\nchapter5.book-apigateway\nStreams for reactive microservices\nSpring Cloud Stream provides an abstraction over the messaging infrastructure. The \nunderlying messaging implementation can be RabbitMQ, Redis, or Kafka. Spring \nCloud Stream provides a declarative approach for sending and receiving messages:\nAs shown in the preceding diagram, Cloud Stream works on the concept of a source \nand a sink. The source represents the sender perspective of the messaging, and sink \nrepresents the receiver perspective of the messaging.\nIn the example shown in the diagram, the sender defines a logical queue called \nSource.OUTPUT to which the sender sends messages. The receiver defines a logical \nqueue called Sink.INPUT from which the receiver retrieves messages. The physical \nbinding of OUTPUT to INPUT is managed through the configuration. In this case, \nboth link to the same physical queue—MyQueue on RabbitMQ. So, while at one end, \nSource.OUTPUT points to MyQueue, on the other end, Sink.INPUT points to the  \nsame MyQueue.\n",
      "content_length": 1408,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "Chapter 5\n[ 253 ]\nSpring Cloud offers the flexibility to use multiple messaging providers in one \napplication such as connecting an input stream from Kafka to a Redis output stream, \nwithout managing the complexities. Spring Cloud Stream is the basis for message-\nbased integration. The Cloud Stream Modules subproject is another Spring Cloud \nlibrary that provides many endpoint implementations.\nAs the next step, rebuild the inter-microservice messaging communication with \nthe Cloud Streams. As shown in the next diagram, we will define a SearchSink \nconnected to InventoryQ under the Search microservice. Booking will define a \nBookingSource for sending inventory change messages connected to InventoryQ. \nSimilarly, Check-in defines a CheckinSource for sending the check-in messages. \nBooking defines a sink, BookingSink, for receiving messages, both bound to the \nCheckinQ queue on the RabbitMQ:\nIn this example, we will use RabbitMQ as the message broker:\n1.\t Add the following Maven dependency to Booking, Search, and Check-in, as \nthese are the three modules using messaging:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-stream-rabbit \n    </artifactId>\n</dependency>\n",
      "content_length": 1227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "Scaling Microservices with Spring Cloud\n[ 254 ]\n2.\t Add the following two properties to booking-service.properties. These \nproperties bind the logical queue inventoryQ to physical inventoryQ, and \nthe logical checkinQ to the physical checkinQ:\nspring.cloud.stream.bindings.inventoryQ.destination=inventoryQ\nspring.cloud.stream.bindings.checkInQ.destination=checkInQ\n3.\t Add the following property to search-service.properties. This property \nbinds the logical queue inventoryQ to the physical inventoryQ:\nspring.cloud.stream.bindings.inventoryQ.destination=inventoryQ\n4.\t Add the following property to checkin-service.properties. This \nproperty binds the logical queue checkinQ to the physical checkinQ:\nspring.cloud.stream.bindings.checkInQ.destination=checkInQ\n5.\t Commit all files to the Git repository.\n6.\t The next step is to edit the code. The Search microservice consumes a \nmessage from the Booking microservice. In this case, Booking is the source \nand Search is the sink.\nAdd @EnableBinding to the Sender class of the Booking service. This \nenables the Cloud Stream to work on autoconfigurations based on the \nmessage broker library available in the class path. In our case, it is RabbitMQ. \nThe parameter BookingSource defines the logical channels to be used for \nthis configuration:\n@EnableBinding(BookingSource.class)\npublic class Sender {\n7.\t In this case, BookingSource defines a message channel called inventoryQ, \nwhich is physically bound to RabbitMQ's inventoryQ, as configured in the \nconfiguration. BookingSource uses an annotation, @Output, to indicate that \nthis is of the output type—a message that is outgoing from a module. This \ninformation will be used for autoconfiguration of the message channel:\ninterface BookingSource {\n    public static String InventoryQ=\"inventoryQ\"; \n    @Output(\"inventoryQ\")\n    public MessageChannel inventoryQ();      \n}\n8.\t Instead of defining a custom class, we can also use the default Source class \nthat comes with Spring Cloud Stream if the service has only one source  \nand sink:\npublic interface Source {\n  @Output(\"output\")\n",
      "content_length": 2089,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "Chapter 5\n[ 255 ]\n  MessageChannel output();\n}\n9.\t Define a message channel in the sender, based on BookingSource. The \nfollowing code will inject an output message channel with the name \ninventory, which is already configured in BookingSource:\n  @Output (BookingSource.InventoryQ)\n  @Autowired\n  private MessageChannel;\n10.\t Reimplement the send message method in BookingSender:\npublic void send(Object message){\n  messageChannel.\n    send(MessageBuilder.withPayload(message).\n    build());\n}\n11.\t Now add the following to the SearchReceiver class the same way we did \nfor the Booking service:\n@EnableBinding(SearchSink.class)\npublic class Receiver {\n12.\t In this case, the SearchSink interface will look like the following. This will \ndefine the logical sink queue it is connected with. The message channel in \nthis case is defined as @Input to indicate that this message channel is to \naccept messages:\ninterface SearchSink {\n    public static String INVENTORYQ=\"inventoryQ\"; \n    @Input(\"inventoryQ\")\n    public MessageChannel inventoryQ();\n}\n13.\t Amend the Search service to accept this message:\n@ServiceActivator(inputChannel = SearchSink.INVENTORYQ)\npublic void accept(Map<String,Object> fare){\n        searchComponent.updateInventory((String)fare.\n        get(\"FLIGHT_NUMBER\"),(String)fare.\n        get(\"FLIGHT_DATE\"),(int)fare.\n        get(\"NEW_INVENTORY\"));\n}\n",
      "content_length": 1370,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "Scaling Microservices with Spring Cloud\n[ 256 ]\n14.\t We will still need the RabbitMQ configurations that we have in our \nconfiguration files to connect to the message broker:\nspring.rabbitmq.host=localhost\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\nserver.port=8090\n15.\t Run all services, and run the website project. If everything is fine, the  \nwebsite project successfully executes the Search, Booking, and Check-in \nfunctions. The same can also be tested using the browser by pointing  \nto http://localhost:8001.\nSummarizing the BrownField PSS \narchitecture\nThe following diagram shows the overall architecture that we have created with \nthe Config server, Eureka, Feign, Zuul, and Cloud Streams. The architecture also \nincludes the high availability of all components. In this case, we assume that the \nclient uses the Eureka client libraries:\n",
      "content_length": 894,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "Chapter 5\n[ 257 ]\nThe summary of the projects and the port they are listening on is given in the \nfollowing table:\nMicroservice \nProjects\nPort\nBook microservice\nchapter5.book\n8060 to 8064\nCheck-in microservice\nchapter5.checkin\n8070 to 8074\nFare microservice\nchapter5.fares\n8080 to 8084\nSearch microservice\nchapter5.search\n8090 to 8094\nWebsite client\nchapter5.website\n8001\nSpring Cloud Config server\nchapter5.configserver\n8888/8889\nSpring Cloud Eureka server\nchapter5.eurekaserver\n8761/8762\nBook API gateway\nchapter5.book-\napigateway\n8095 to 8099\nCheck-in API gateway\nchapter5.checkin-\napigateway\n8075 to 8079\nFares API gateway\nchapter5.fares-\napigateway\n8085 to 8089\nSearch API gateway\nchapter5.search-\napigateway\n8065 to 8069\nFollow these steps to do a final run:\n1.\t Run RabbitMQ.\n2.\t Build all projects using pom.xml at the root level:\nmvn –Dmaven.test.skip=true clean install \n3.\t Run the following projects from their respective folders. Remember to wait \nfor 40 to 50 seconds before starting the next service. This will ensure that  \nthe dependent services are registered and are available before we start a  \nnew service:\njava -jar target/fares-1.0.jar\njava -jar target/search-1.0.jar\njava -jar target/checkin-1.0.jar\njava -jar target/book-1.0.jar\njava –jar target/fares-apigateway-1.0.jar\njava –jar target/search-apigateway-1.0.jar\njava –jar target/checkin-apigateway-1.0.jar\njava –jar target/book-apigateway-1.0.jar\njava -jar target/website-1.0.jar\n",
      "content_length": 1458,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "Scaling Microservices with Spring Cloud\n[ 258 ]\n4.\t Open the browser window, and point to http://localhost:8001. Follow \nthe steps mentioned in the Running and testing the project section in Chapter 4, \nMicroservices Evolution – A Case Study.\nSummary\nIn this chapter, you learned how to scale a Twelve-Factor Spring Boot microservice \nusing the Spring Cloud project. What you learned was then applied to the \nBrownField Airline's PSS microservice that we developed in the previous chapter.\nWe then explored the Spring Config server for externalizing the microservices' \nconfiguration, and the way to deploy the Config server for high availability. \nWe also discussed the declarative service calls using Feign, examined the use of \nRibbon and Eureka for load balancing, dynamic service registration, and discovery. \nImplementation of an API gateway was examined by implementing Zuul. Finally, \nwe concluded with a reactive style integration of microservices using Spring  \nCloud Stream.\nBrownField Airline's PSS microservices are now deployable on the Internet scale. \nOther Spring Cloud components such as Hyterix, Sleuth, and so on will be covered \nin Chapter 7, Logging and Monitoring Microservices. The next chapter will demonstrate \nautoscaling features, extending the BrownField PSS implementation.\n",
      "content_length": 1304,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "[ 259 ]\nAutoscaling Microservices\nSpring Cloud provides the support essential for the deployment of microservices at \nscale. In order to get the full power of a cloud-like environment, the microservices \ninstances should also be capable of scaling out and shrinking automatically based  \non traffic patterns.\nThis chapter will detail out how to make microservices elastically grow and shrink \nby effectively using the actuator data collected from Spring Boot microservices to \ncontrol the deployment topology by implementing a simple life cycle manager.\nBy the end of this chapter, you will learn about the following topics:\n•\t\nThe basic concept of autoscaling and different approaches for autoscaling\n•\t\nThe importance and capabilities of a life cycle manager in the context  \nof microservices\n•\t\nExamining the custom life cycle manager to achieve autoscaling\n•\t\nProgrammatically collecting statistics from the Spring Boot actuator and \nusing it to control and shape incoming traffic\n",
      "content_length": 985,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "Autoscaling Microservices\n[ 260 ]\nReviewing the microservice capability \nmodel\nThis chapter will cover the Application Lifecycle Management capability in the \nmicroservices capability model discussed in Chapter 3, Applying Microservices \nConcepts, highlighted in the following diagram:\nWe will see a basic version of the life cycle manager in this chapter, which will be \nenhanced in later chapters.\nScaling microservices with Spring Cloud\nIn Chapter 5, Scaling Microservices with Spring Cloud, you learned how to scale Spring \nBoot microservices using Spring Cloud components. The two key concepts of Spring \nCloud that we implemented are self-registration and self-discovery. These two \ncapabilities enable automated microservices deployments. With self-registration, \nmicroservices can automatically advertise the service availability by registering \nservice metadata to a central service registry as soon as the instances are ready \nto accept traffic. Once the microservices are registered, consumers can consume \nthe newly registered services from the very next moment by discovering service \ninstances using the registry service. Registry is at the heart of this automation.\n",
      "content_length": 1181,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "Chapter 6\n[ 261 ]\nThis is quite different from the traditional clustering approach employed by the \ntraditional JEE application servers. In the case of JEE application servers, the server \ninstances' IP addresses are more or less statically configured in a load balancer. \nTherefore, the cluster approach is not the best solution for automatic scaling in \nInternet-scale deployments. Also, clusters impose other challenges, such as they  \nhave to have exactly the same version of binaries on all cluster nodes. It is also \npossible that the failure of one cluster node can poison other nodes due to the  \ntight dependency between nodes.\nThe registry approach decouples the service instances. It also eliminates the need to \nmanually maintain service addresses in the load balancer or configure virtual IPs:\nAs shown in the diagram, there are three key components in our automated \nmicroservices deployment topology:\n•\t\nEureka is the central registry component for microservice registration and \ndiscovery. REST APIs are used by both consumers as well as providers to \naccess the registry. The registry also holds the service metadata such as the \nservice identity, host, port, health status, and so on.\n•\t\nThe Eureka client, together with the Ribbon client, provide client-side \ndynamic load balancing. Consumers use the Eureka client to look up the \nEureka server to identify the available instances of a target service. The \nRibbon client uses this server list to load-balance between the available \nmicroservice instances. In a similar way, if the service instance goes out of \nservice, these instances will be taken out of the Eureka registry. The load \nbalancer automatically reacts to these dynamic topology changes.\n•\t\nThe third component is the microservices instances developed using Spring \nBoot with the actuator endpoints enabled.\n",
      "content_length": 1843,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "Autoscaling Microservices\n[ 262 ]\nHowever, there is one gap in this approach. When there is need for an additional \nmicroservice instance, a manual task is required to kick off a new instance. In an \nideal scenario, the starting and stopping of microservice instances also require \nautomation.\nFor example, when there is a requirement to add another Search microservice \ninstance to handle the increase in traffic volumes or a load burst scenario, the \nadministrator has to manually bring up a new instance. Also, when the Search \ninstance is idle for some time, it needs to be manually taken out of service to  \nhave optimal infrastructure usage. This is especially relevant when services  \nrun on a pay-as-per-usage cloud environment.\nUnderstanding the concept of \nautoscaling\nAutoscaling is an approach to automatically scaling out instances based on the \nresource usage to meet the SLAs by replicating the services to be scaled.\nThe system automatically detects an increase in traffic, spins up additional instances, \nand makes them available for traffic handling. Similarly, when the traffic volumes \ngo down, the system automatically detects and reduces the number of instances by \ntaking active instances back from the service:\nAs shown in the preceding diagram, autoscaling is done, generally, using a set of \nreserve machines.\n",
      "content_length": 1336,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "Chapter 6\n[ 263 ]\nAs many of the cloud subscriptions are based on a pay-as-you-go model, this is an \nessential capability when targeting cloud deployments. This approach is often called \nelasticity. It is also called dynamic resource provisioning and deprovisioning. \nAutoscaling is an effective approach specifically for microservices with varying \ntraffic patterns. For example, an Accounting service would have high traffic during \nmonth ends and year ends. There is no point in permanently provisioning instances \nto handle these seasonal loads.\nIn the autoscaling approach, there is often a resource pool with a number of spare \ninstances. Based on the demand, instances will be moved from the resource pool to \nthe active state to meet the surplus demand. These instances are not pretagged for \nany particular microservices or prepackaged with any of the microservice binaries.  \nIn advanced deployments, the Spring Boot binaries are downloaded on demand \nfrom an artifact repository such as Nexus or Artifactory.\nThe benefits of autoscaling\nThere are many benefits in implementing the autoscaling mechanism. In traditional \ndeployments, administrators reserve a set of servers against each application. With \nautoscaling, this preallocation is no longer required. This prefixed server allocation \nmay result in underutilized servers. In this case, idle servers cannot be utilized even \nwhen neighboring services struggle for additional resources.\nWith hundreds of microservice instances, preallocating a fixed number of servers to \neach of the microservices is not cost effective. A better approach is to reserve a number \nof server instances for a group of microservices without preallocating or tagging them \nagainst a microservice. Instead, based on the demand, a group of services can share a \nset of available resources. By doing so, microservices can be dynamically moved across \nthe available server instances by optimally using the resources:\n",
      "content_length": 1958,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "Autoscaling Microservices\n[ 264 ]\nAs shown in the preceding diagram, there are three instances of the M1 microservice, \none instance of M2, and one instance of M3 up and running. There is another server \nkept unallocated. Based on the demand, the unallocated server can be used for \nany of the microservices: M1, M2, or M3. If M1 has more service requests, then the \nunallocated instance will be used for M1. When the service usage goes down, the \nserver instance will be freed up and moved back to the pool. Later, if the M2 demand \nincreases, the same server instance can be activated using M2.\nSome of the key benefits of autoscaling are:\n•\t\nIt has high availability and is fault tolerant: As there are multiple service \ninstances, even if one fails, another instance can take over and continue \nserving clients. This failover will be transparent to the consumers. If no other \ninstance of this service is available, the autoscaling service will recognize this \nsituation and bring up another server with the service instance. As the whole \nprocess of bringing up or bringing down instances is automatic, the overall \navailability of the services will be higher than the systems implemented \nwithout autoscaling. The systems without autoscaling require manual \nintervention to add or remove service instances, which will be hard to \nmanage in large deployments.\nFor example, assume that two of instances of the Booking service are \nrunning. If there is an increase in the traffic flow, in a normal scenario,  \nthe existing instance might become overloaded. In most of the scenarios,  \nthe entire set of services will be jammed, resulting in service unavailability. \nIn the case of autoscaling, a new Booking service instance can be brought  \nup quickly. This will balance the load and ensure service availability.\n•\t\nIt increases scalability: One of the key benefits of autoscaling is horizontal \nscalability. Autoscaling allows us to selectively scale up or scale down \nservices automatically based on traffic patterns.\n•\t\nIt has optimal usage and is cost saving: In a pay-as-you-go subscription \nmodel, billing is based on actual resource utilization. With the autoscaling \napproach, instances will be started and shut down based on the demand. \nHence, resources are optimally utilized, thereby saving cost.\n",
      "content_length": 2313,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "Chapter 6\n[ 265 ]\n•\t\nIt gives priority to certain services or group of services: With autoscaling, \nit is possible to give priority to certain critical transactions over low-value \ntransactions. This will be done by removing an instance from a low-value \nservice and reallocating it to a high-value service. This will also eliminate \nsituations where a low-priority transaction heavily utilizes resources when \nhigh-value transactions are cramped up for resources.\nFor instance, the Booking and Reports services run with two instances, as \nshown in the preceding diagram. Let's assume that the Booking service is a \nrevenue generation service and therefore has a higher value than the Reports \nservice. If there are more demands for the Booking service, then one can set \npolicies to take one Reports service out of the service and release this server \nfor the Booking service.\nDifferent autoscaling models\nAutoscaling can be applied at the application level or at the infrastructure level.  \nIn a nutshell, application scaling is scaling by replicating application binaries only, \nwhereas infrastructure scaling is replicating the entire virtual machine, including \napplication binaries.\n",
      "content_length": 1189,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "Autoscaling Microservices\n[ 266 ]\nAutoscaling an application\nIn this scenario, scaling is done by replicating the microservices, not the underlying \ninfrastructure, such as virtual machines. The assumption is that there is a pool of \nVMs or physical infrastructures available to scale up microservices. These VMs have \nthe basic image fused with any dependencies, such as JRE. It is also assumed that \nmicroservices are homogeneous in nature. This gives flexibility in reusing the same \nvirtual or physical machines for different services:\nAs shown in the preceding diagram, in scenario A, VM3 is used for Service 1, \nwhereas in scenario B, the same VM3 is used for Service 2. In this case, we only \nswapped the application library and not the underlying infrastructure.\nThis approach gives faster instantiation as we are only handling the application \nbinaries and not the underlying VMs. The switching is easier and faster as the \nbinaries are smaller in size and there is no OS boot required either. However, the \ndownside of this approach is that if certain microservices require OS-level tuning  \nor use polyglot technologies, then dynamically swapping microservices will not  \nbe effective.\nAutoscaling the infrastructure\nIn contrast to the previous approach, in this case, the infrastructure is also provisioned \nautomatically. In most cases, this will create a new VM on the fly or destroy the VMs \nbased on the demand:\n",
      "content_length": 1428,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "Chapter 6\n[ 267 ]\nAs shown in the preceding diagram, the reserve instances are created as VM images \nwith predefined service instances. When there is demand for Service 1, VM3 is \nmoved to an active state. When there is a demand for Service 2, VM4 is moved  \nto the active state.\nThis approach is efficient if the applications depend upon the parameters and \nlibraries at the infrastructure level, such as the operating system. Also, this approach \nis better for polyglot microservices. The downside is the heavy nature of VM images \nand the time required to spin up a new VM. Lightweight containers such as Dockers \nare preferred in such cases instead of traditional heavyweight virtual machines.\nAutoscaling in the cloud\nElasticity or autoscaling is one of the fundamental features of most cloud providers. \nCloud providers use infrastructure scaling patterns, as discussed in the previous \nsection. These are typically based on a set of pooled machines.\nFor example, in AWS, these are based on introducing new EC2 instances with a \npredefined AMI. AWS supports autoscaling with the help of autoscaling groups. \nEach group is set with a minimum and maximum number of instances. AWS  \nensures that the instances are scaled on demand within these bounds. In case of \npredictable traffic patterns, provisioning can be configured based on timelines.  \nAWS also provides ability for applications to customize autoscaling policies.\nMicrosoft Azure also supports autoscaling based on the utilization of resources such \nas the CPU, message queue length, and so on. IBM Bluemix supports autoscaling \nbased on resources such as CPU usage.\nOther PaaS platforms, such as CloudBees and OpenShift, also support autoscaling \nfor Java applications. Pivotal Cloud Foundry supports autoscaling with the help of \nPivotal Autoscale. Scaling policies are generally based on resource utilization, such \nas the CPU and memory thresholds.\n",
      "content_length": 1917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "Autoscaling Microservices\n[ 268 ]\nThere are components that run on top of the cloud and provide fine-grained controls \nto handle autoscaling. Netflix Fenzo, Eucalyptus, Boxfuse, and Mesosphere are some \nof the components in this category.\nAutoscaling approaches\nAutoscaling is handled by considering different parameters and thresholds. In this \nsection, we will discuss the different approaches and policies that are typically \napplied to take decisions on when to scale up or down.\nScaling with resource constraints\nThis approach is based on real-time service metrics collected through monitoring \nmechanisms. Generally, the resource-scaling approach takes decisions based on \nthe CPU, memory, or the disk of machines. This can also be done by looking at the \nstatistics collected on the service instances themselves, such as heap memory usage.\nA typical policy may be spinning up another instance when the CPU utilization \nof the machine goes beyond 60%. Similarly, if the heap size goes beyond a certain \nthreshold, we can add a new instance. The same applies to downsizing the compute \ncapacity when the resource utilization goes below a set threshold. This is done by \ngradually shutting down servers:\nIn typical production scenarios, the creation of additional services is not done on the \nfirst occurrence of a threshold breach. The most appropriate approach is to define a \nsliding window or a waiting period.\n",
      "content_length": 1419,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "Chapter 6\n[ 269 ]\nThe following are some of the examples:\n•\t\nAn example of a response sliding window is if 60% of the response time of \na particular transaction is consistently more than the set threshold value in a \n60-second sampling window, increase service instances\n•\t\nIn a CPU sliding window, if the CPU utilization is consistently beyond 70% \nin a 5 minutes sliding window, then a new instance is created\n•\t\nAn example of the exception sliding window is if 80% of the transactions \nin a sliding window of 60 seconds or 10 consecutive executions result in a \nparticular system exception, such as a connection timeout due to exhausting \nthe thread pool, then a new service instance is created\nIn many cases, we will set a lower threshold than the actual expected thresholds. \nFor example, instead of setting the CPU utilization threshold at 80%, set it at 60% so \nthat the system gets enough time to spin up an instance before it stops responding. \nSimilarly, when scaling down, we use a lower threshold than the actual. For example, \nwe will use 40% CPU utilization to scale down instead of 60%. This allows us to have  \na cool-down period so that there will not be any resource struggle when shutting \ndown instances.\nResource-based scaling is also applicable to service-level parameters such as the \nthroughput of the service, latency, applications thread pool, connection pool, and \nso on. These can also be at the application level, such as the number of sales orders \nprocessing in a service instance, based on internal benchmarking.\nScaling during specific time periods\nTime-based scaling is an approach to scaling services based on certain periods of  \nthe day, month, or year to handle seasonal or business peaks. For example, some \nservices may experience a higher number of transactions during office hours and  \na considerably low number of transactions outside office hours. In this case, during \nthe day, services autoscale to meet the demand and automatically downsize during \nthe non-office hours:\n",
      "content_length": 2019,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "Autoscaling Microservices\n[ 270 ]\nMany airports worldwide impose restrictions on night-time landing. As a result, \nthe number of passengers checking in at the airports during the night time is less \ncompared to the day time. Hence, it is cost effective to reduce the number of instances \nduring the night time.\nScaling based on the message queue length\nThis is particularly useful when the microservices are based on asynchronous \nmessaging. In this approach, new consumers are automatically added when the \nmessages in the queue go beyond certain limits:\nThis approach is based on the competing consumer pattern. In this case, a pool \nof instances is used to consume messages. Based on the message threshold, new \ninstances are added to consume additional messages.\nScaling based on business parameters\nIn this case, adding instances is based on certain business parameters—for example, \nspinning up a new instance just before handling sales closing transactions. As soon \nas the monitoring service receives a preconfigured business event (such as sales \nclosing minus 1 hour), a new instance will be brought up in anticipation of large \nvolumes of transactions. This will provide fine-grained control on scaling based  \non business rules:\n",
      "content_length": 1241,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "Chapter 6\n[ 271 ]\nPredictive autoscaling\nPredictive scaling is a new paradigm of autoscaling that is different from the \ntraditional real-time metrics-based autoscaling. A prediction engine will take \nmultiple inputs, such as historical information, current trends, and so on, to predict \npossible traffic patterns. Autoscaling is done based on these predictions. Predictive \nautoscaling helps avoid hardcoded rules and time windows. Instead, the system \ncan automatically predict such time windows. In more sophisticated deployments, \npredictive analysis may use cognitive computing mechanisms to predict autoscaling.\nIn the cases of sudden traffic spikes, traditional autoscaling may not help. Before \nthe autoscaling component can react to the situation, the spike would have hit and \ndamaged the system. The predictive system can understand these scenarios and \npredict them before their actual occurrence. An example will be handling a flood  \nof requests immediately after a planned outage.\nNetflix Scryer is an example of such a system that can predict resource requirements \nin advance.\n",
      "content_length": 1095,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "Autoscaling Microservices\n[ 272 ]\nAutoscaling BrownField PSS \nmicroservices\nIn this section, we will examine how to enhance microservices developed in  \nChapter 5, Scaling Microservices with Spring Cloud, for autoscaling. We need a \ncomponent to monitor certain performance metrics and trigger autoscaling.  \nWe will call this component the life cycle manager.\nThe service life cycle manager, or the application life cycle manager, is responsible  \nfor detecting scaling requirements and adjusting the number of instances accordingly. \nIt is responsible for starting and shutting down instances dynamically.\nIn this section, we will take a look at a primitive autoscaling system to understand \nthe basic concepts, which will be enhanced in later chapters.\nThe capabilities required for an autoscaling \nsystem\nA typical autoscaling system has capabilities as shown in the following diagram:\n",
      "content_length": 890,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "Chapter 6\n[ 273 ]\nThe components involved in the autoscaling ecosystem in the context of microservices \nare explained as follows:\n•\t\nMicroservices: These are sets of the up-and-running microservice instances \nthat keep sending health and metrics information. Alternately, these services \nexpose actuator endpoints for metrics collection. In the preceding diagram, \nthese are represented as Microservice 1 through Microservice 4.\n•\t\nService Registry: A service registry keeps track of all the services, their \nhealth states, their metadata, and their endpoint URI.\n•\t\nLoad Balancer: This is a client-side load balancer that looks up the service \nregistry to get up-to-date information about the available service instances.\n•\t\nLifecycle Manager: The life cycle manger is responsible for autoscaling, \nwhich has the following subcomponents:\n°°\nMetrics Collector: A metrics collection unit is responsible for \ncollecting metrics from all service instances. The life cycle manager \nwill aggregate the metrics. It may also keep a sliding time window. \nThe metrics could be infrastructure-level metrics, such as CPU usage, \nor application-level metrics, such as transactions per minute.\n°°\nScaling policies: Scaling policies are nothing but sets of rules \nindicating when to scale up and scale down microservices—for \nexample, 90% of CPU usage above 60% in a sliding window of 5 \nminutes.\n°°\nDecision Engine: A decision engine is responsible for making \ndecisions to scale up and scale down based on the aggregated  \nmetrics and scaling policies.\n°°\nDeployment Rules: The deployment engine uses deployment rules \nto decide which parameters to consider when deploying services.  \nFor example, a service deployment constraint may say that the \ninstance must be distributed across multiple availability regions  \nor a 4 GB minimum of memory required for the service.\n°°\nDeployment Engine: The deployment engine, based on the  \ndecisions of the decision engine, can start or stop microservice \ninstances or update the registry by altering the health states of \nservices. For example, it sets the health status as \"out of service\"  \nto take out a service temporarily.\n",
      "content_length": 2157,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "Autoscaling Microservices\n[ 274 ]\nImplementing a custom life cycle manager \nusing Spring Boot\nThe life cycle manager introduced in this section is a minimal implementation \nto understand autoscaling capabilities. In later chapters, we will enhance this \nimplementation with containers and cluster management solutions. Ansible, \nMarathon, and Kubernetes are some of the tools useful in building this capability.\nIn this section, we will implement an application-level autoscaling component  \nusing Spring Boot for the services developed in Chapter 5, Scaling Microservices  \nwith Spring Cloud.\nUnderstanding the deployment topology\nThe following diagram shows a sample deployment topology of BrownField  \nPSS microservices:\nAs shown in the diagram, there are four physical machines. Eight VMs are created \nfrom four physical machines. Each physical machine is capable of hosting two VMs, \nand each VM is capable of running two Spring Boot instances, assuming that all \nservices have the same resource requirements.\n",
      "content_length": 1015,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "Chapter 6\n[ 275 ]\nFour VMs, VM1 through VM4, are active and are used to handle traffic. VM5 to \nVM8 are kept as reserve VMs to handle scalability. VM5 and VM6 can be used for \nany of the microservices and can also be switched between microservices based \non scaling demands. Redundant services use VMs created from different physical \nmachines to improve fault tolerance.\nOur objective is to scale out any services when there is increase in traffic flow using \nfour VMs, VM5 through VM8, and scale down when there is not enough load. The \narchitecture of our solution is as follows.\nUnderstanding the execution flow\nHave a look at the following flowchart:\nAs shown in the preceding diagram, the following activities are important for us:\n•\t\nThe Spring Boot service represents microservices such as Search, Book, Fares, \nand Check-in. Services at startup automatically register endpoint details \nto the Eureka registry. These services are actuator-enabled, so the life cycle \nmanager can collect metrics from the actuator endpoints.\n",
      "content_length": 1032,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "Autoscaling Microservices\n[ 276 ]\n•\t\nThe life cycle manager service is nothing but another Spring Boot application. \nThe life cycle manager has a metrics collector that runs a background job, \nperiodically polls the Eureka server, and gets details of all the service \ninstances. The metrics collector then invokes the actuator endpoints of each \nmicroservice registered in the Eureka registry to get the health and metrics \ninformation. In a real production scenario, a subscription approach for data \ncollection is better.\n•\t\nWith the collected metrics information, the life cycle manager executes a \nlist of policies and derives decisions on whether to scale up or scale down \ninstances. These decisions are either to start a new service instance of a \nparticular type on a particular VM or to shut down a particular instance.\n•\t\nIn the case of shutdown, it connects to the server using an actuator endpoint \nand calls the shutdown service to gracefully shut down an instance.\n•\t\nIn the case of starting a new instance, the deployment engine of the life cycle \nmanager uses the scaling rules and decides where to start the new instance \nand what parameters are to be used when starting the instance. Then, it \nconnects to the respective VMs using SSH. Once connected, it executes a \npreinstalled script (or passes this script as a part of the execution) by passing \nthe required constraints as a parameter. This script fetches the application \nlibrary from a central Nexus repository in which the production binaries \nare kept and initiates it as a Spring Boot application. The port number is \nparameterized by the life cycle manager. SSH needs to be enabled on the \ntarget machines.\nIn this example, we will use TPM (Transactions Per Minute) or RPM (Requests Per \nMinute) as sampler metrics for decision making. If the Search service has more than \n10 TPM, then it will spin up a new Search service instance. Similarly, if the TPM is \nbelow 2, one of the instances will be shut down and released back to the pool.\nWhen starting a new instance, the following policies will be applied:\n•\t\nThe number of service instances at any point should be a minimum of 1 and a \nmaximum of 4. This also means that at least one service instance will always  \nbe up and running.\n•\t\nA scaling group is defined in such a way that a new instance is created on a \nVM that is on a different physical machine. This will ensure that the services \nrun across different physical machines.\nThese policies could be further enhanced. The life cycle manager ideally provides \noptions to customize these rules through REST APIs or Groovy scripts.\n",
      "content_length": 2619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "Chapter 6\n[ 277 ]\nA walkthrough of the life cycle manager code\nWe will take a look at how a simple life cycle manager is implemented. This section \nwill be a walkthrough of the code to understand the different components of the life \ncycle manager.\nThe full source code is available under the Chapter 6 project \nin the code files. The chapter5.configserver, chapter5.\neurekaserver, chapter5.search, and chapter5.search-\napigateway are copied and renamed as chapter6.*, respectively.\nPerform the following steps to implement the custom life cycle manager:\n1.\t Create a new Spring Boot application and name it chapter6.\nlifecyclemanager. The project structure is shown in the following diagram:\n",
      "content_length": 693,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "Autoscaling Microservices\n[ 278 ]\nThe flowchart for this example is as shown in the following diagram:\nThe components of this diagram are explained in details here.\n2.\t Create a MetricsCollector class with the following method. At the \nstartup of the Spring Boot application, this method will be invoked using \nCommandLineRunner, as follows:\npublic void start(){\n  while(true){ \n    eurekaClient.getServices().forEach(service -> {        System.\nout.println(\"discovered service \"+ service);\n      Map metrics = restTemplate.getForObject(\"http://\"+service+\"/\nmetrics\",Map.class);\n      decisionEngine.execute(service, metrics);\n    });  \n  }    \n}\nThe preceding method looks for the services registered in the Eureka  \nserver and gets all the instances. In the real world, rather than polling, \nthe instances should publish metrics to a common place, where metrics \naggregation will happen.\n",
      "content_length": 890,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "Chapter 6\n[ 279 ]\n3.\t The following DecisionEngine code accepts the metric and applies certain \nscaling policies to determine whether the service requires scaling up or not:\n  public boolean execute(String serviceId, Map metrics){\n  if(scalingPolicies.getPolicy(serviceId). \n    execute(serviceId, metrics)){    \n      return deploymentEngine.scaleUp(deploymentRules.\ngetDeploymentRules(serviceId), serviceId);  \n    }\n    return false;\n  }\n4.\t Based on the service ID, the policies that are related to the services will \nbe picked up and applied. In this case, a minimal TPM scaling policy is \nimplemented in TpmScalingPolicy, as follows:\npublic class TpmScalingPolicy implements ScalingPolicy {\n  public boolean execute(String serviceId, Map metrics){\n    if(metrics.containsKey(\"gauge.servo.tpm\")){\n      Double tpm = (Double) metrics.get(\"gauge.servo.tpm\");\n      System.out.println(\"gauge.servo.tpm \" + tpm);\n      return (tpm > 10);\n    }\n    return false;\n  }\n}\n5.\t If the policy returns true, DecisionEngine then invokes \nDeploymentEngine to spin up another instance. DeploymentEngine  \nmakes use of DeploymentRules to decide how to execute scaling. The  \nrules can enforce the number of min and max instances, in which region  \nor machine the new instance has to be started, the resources required for \nthe new instance, and so on. DummyDeploymentRule simply makes sure  \nthe max instance is not more than 2.\n6.\t\nDeploymentEngine, in this case, uses the JSch (Java Secure Channel) library \nfrom JCraft to SSH to the destination server and start the service. This requires \nthe following additional Maven dependency:\n<dependency>\n    <groupId>com.jcraft</groupId>\n    <artifactId>jsch</artifactId>\n    <version>0.1.53</version>\n</dependency>\n",
      "content_length": 1750,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "Autoscaling Microservices\n[ 280 ]\n7.\t The current SSH implementation is kept simple enough as we will change \nthis in future chapters. In this example, DeploymentEngine sends the \nfollowing command over the SSH library on the target machine:\n String command =\"java -jar -Dserver.port=8091 ./work/codebox/\nchapter6/chapter6.search/target/search-1.0.jar\";\nIntegration with Nexus happens from the target machine using Linux scripts \nwith Nexus CLI or using curl. In this example, we will not explore Nexus.\n8.\t The next step is to change the Search microservice to expose a new gauge for \nTPM. We have to change all the microservices developed earlier to submit \nthis additional metric.\nWe will only examine Search in this chapter, but in order to complete it, \nall the services have to be updated. In order to get the gauge.servo.tpm \nmetrics, we have to add TPMCounter to all the microservices.\nThe following code counts the transactions over a sliding window of 1 \nminute:\nclass TPMCounter {\n  LongAdder count;\n  Calendar expiry = null; \n  TPMCounter(){\n    reset();\n  }  \n  void reset (){\n    count = new LongAdder();\n    expiry = Calendar.getInstance();\n    expiry.add(Calendar.MINUTE, 1);\n  }\n  boolean isExpired(){\n    return Calendar.getInstance().after(expiry);\n  }\n  void increment(){\n     if(isExpired()){\n       reset();\n     }\n     count.increment();\n  }\n}\n",
      "content_length": 1367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "Chapter 6\n[ 281 ]\n9.\t The following code needs to be added to SearchController to set the  \ntpm value:\nclass SearchRestController {\n  TPMCounter tpm = new TPMCounter();\n  @Autowired\n  GaugeService gaugeService;\n   //other code \n10.\t The following code is from the get REST endpoint (the search method) of \nSearchRestController, which submits the tpm value as a gauge to the \nactuator endpoint:\ntpm.increment();\ngaugeService.submit(\"tpm\", tpm.count.intValue()); \nRunning the life cycle manager\nPerform the following steps to run the life cycle manager developed in the  \nprevious section:\n1.\t Edit DeploymentEngine.java and update the password to reflect the \nmachine's password, as follows. This is required for the SSH connection:\nsession.setPassword(\"rajeshrv\");\n2.\t Build all the projects by running Maven from the root folder (Chapter 6)  \nvia the following command:\nmvn -Dmaven.test.skip=true clean install\n3.\t Then, run RabbitMQ, as follows:\n./rabbitmq-server\n4.\t Ensure that the Config server is pointing to the right configuration \nrepository. We need to add a property file for the life cycle manager.\n5.\t Run the following commands from the respective project folders:\njava -jar target/config-server-0.0.1-SNAPSHOT.jar\njava -jar target/eureka-server-0.0.1-SNAPSHOT.jar\njava -jar target/lifecycle-manager-0.0.1-SNAPSHOT.jar\njava -jar target/search-1.0.jar\njava -jar target/search-apigateway-1.0.jar\njava -jar target/website-1.0.jar\n",
      "content_length": 1441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "Autoscaling Microservices\n[ 282 ]\n6.\t Once all the services are started, open a browser window and load  \nhttp://localhost:8001.\n7.\t Execute the flight search 11 times, one after the other, within a minute.  \nThis will trigger the decision engine to instantiate another instance  \nof the Search microservice.\n8.\t Open the Eureka console (http://localhost:8761) and watch for a  \nsecond SEARCH-SERVICE. Once the server is started, the instances  \nwill appear as shown here:\nSummary\nIn this chapter, you learned the importance of autoscaling when deploying  \nlarge-scale microservices.\nWe also explored the concept of autoscaling and the different models of  \nand approaches to autoscaling, such as the time-based, resource-based, queue \nlength-based, and predictive ones. We then reviewed the role of a life cycle \nmanager in the context of microservices and reviewed its capabilities. Finally,  \nwe ended this chapter by reviewing a sample implementation of a simple  \ncustom life cycle manager in the context of BrownField PSS microservices.\nAutoscaling is an important supporting capability required when dealing with  \nlarge-scale microservices. We will discuss a more mature implementation of  \nthe life cycle manager in Chapter 9, Managing Dockerized Microservices with  \nMesos and Marathon.\nThe next chapter will explore the logging and monitoring capabilities that are \nindispensable for successful microservice deployments.\n",
      "content_length": 1432,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "[ 283 ]\nLogging and Monitoring \nMicroservices\nOne of the biggest challenges due to the very distributed nature of Internet-scale \nmicroservices deployment is the logging and monitoring of individual microservices. \nIt is difficult to trace end-to-end transactions by correlating logs emitted by different \nmicroservices. As with monolithic applications, there is no single pane of glass to \nmonitor microservices.\nThis chapter will cover the necessity and importance of logging and monitoring in \nmicroservice deployments. This chapter will further examine the challenges and \nsolutions to address logging and monitoring with a number of potential architectures \nand technologies.\nBy the end of this chapter, you will learn about:\n•\t\nThe different options, tools, and technologies for log management\n•\t\nThe use of Spring Cloud Sleuth in tracing microservices\n•\t\nThe different tools for end-to-end monitoring of microservices\n•\t\nThe use of Spring Cloud Hystrix and Turbine for circuit monitoring\n•\t\nThe use of data lakes in enabling business data analysis\n",
      "content_length": 1055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "Logging and Monitoring Microservices\n[ 284 ]\nReviewing the microservice capability \nmodel\nIn this chapter, we will explore the following microservice capabilities from the \nmicroservices capability model discussed in Chapter 3, Applying Microservices Concepts:\n•\t\nCentral Log Management\n•\t\nMonitoring and Dashboards\n•\t\nDependency Management (part of Monitoring and Dashboards)\n•\t\nData Lake\nUnderstanding log management \nchallenges\nLogs are nothing but streams of events coming from a running process. For \ntraditional JEE applications, a number of frameworks and libraries are available to \nlog. Java Logging (JUL) is an option off the shelf from Java itself. Log4j, Logback, \nand SLF4J are some of the other popular logging frameworks available. These \nframeworks support both UDP as well as TCP protocols for logging. Applications \nsend log entries to the console or to the filesystem. File recycling techniques are \ngenerally employed to avoid logs filling up all the disk space.\n",
      "content_length": 983,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "Chapter 7\n[ 285 ]\nOne of the best practices of log handling is to switch off most of the log entries in \nproduction due to the high cost of disk IOs. Not only do disk IOs slow down the \napplication, but they can also severely impact scalability. Writing logs into the disk \nalso requires high disk capacity. An out-of-disk-space scenario can bring down the \napplication. Logging frameworks provide options to control logging at runtime to \nrestrict what is to be printed and what not. Most of these frameworks provide fine-\ngrained control over the logging controls. They also provide options to change these \nconfigurations at runtime.\nOn the other hand, logs may contain important information and have high value if \nproperly analyzed. Therefore, restricting log entries essentially limits our ability to \nunderstand the application's behavior.\nWhen moved from traditional to cloud deployment, applications are no longer \nlocked to a particular, predefined machine. Virtual machines and containers are not \nhardwired with an application. The machines used for deployment can change from \ntime to time. Moreover, containers such as Docker are ephemeral. This essentially \nmeans that one cannot rely on the persistent state of the disk. Logs written to the disk \nare lost once the container is stopped and restarted. Therefore, we cannot rely on the \nlocal machine's disk to write log files.\nAs we discussed in Chapter 1, Demystifying Microservices, one of the principles of the \nTwelve-Factor app is to avoid routing or storing log files by the application itself. In \nthe context of microservices, they will run on isolated physical or virtual machines, \nresulting in fragmented log files. In this case, it is almost impossible to trace end-to-\nend transactions that span multiple microservices:\n",
      "content_length": 1798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "Logging and Monitoring Microservices\n[ 286 ]\nAs shown in the diagram, each microservice emits logs to a local filesystem. In \nthis case, microservice M1 calls M3. These services write their logs to their own \nlocal filesystems. This makes it harder to correlate and understand the end-to-end \ntransaction flow. Also, as shown in the diagram, there are two instances of M1 and \ntwo instances of M2 running on two different machines. In this case, log aggregation \nat the service level is hard to achieve.\nA centralized logging solution\nIn order to address the challenges stated earlier, traditional logging solutions \nrequire serious rethinking. The new logging solution, in addition to addressing the \npreceding challenges, is also expected to support the capabilities summarized here:\n•\t\nThe ability to collect all log messages and run analytics on top of the log \nmessages\n•\t\nThe ability to correlate and track transactions end to end\n•\t\nThe ability to keep log information for longer time periods for trending and \nforecasting\n•\t\nThe ability to eliminate dependency on the local disk system\n•\t\nThe ability to aggregate log information coming from multiple sources such \nas network devices, operating system, microservices, and so on\nThe solution to these problems is to centrally store and analyze all log messages, \nirrespective of the source of log. The fundamental principle employed in the new \nlogging solution is to detach log storage and processing from service execution \nenvironments. Big data solutions are better suited to storing and processing large \nnumbers of log messages more effectively than storing and processing them in \nmicroservice execution environments.\nIn the centralized logging solution, log messages will be shipped from the execution \nenvironment to a central big data store. Log analysis and processing will be handled \nusing big data solutions:\n",
      "content_length": 1880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "Chapter 7\n[ 287 ]\nAs shown in the preceding logical diagram, there are a number of components in the \ncentralized logging solution, as follows:\n•\t\nLog streams: These are streams of log messages coming out of source \nsystems. The source system can be microservices, other applications, or \neven network devices. In typical Java-based systems, these are equivalent to \nstreaming Log4j log messages.\n•\t\nLog shippers: Log shippers are responsible for collecting the log messages \ncoming from different sources or endpoints. The log shippers then send these \nmessages to another set of endpoints, such as writing to a database, pushing \nto a dashboard, or sending it to stream-processing endpoint for further real-\ntime processing.\n•\t\nLog store: A log store is the place where all log messages are stored for real-\ntime analysis, trending, and so on. Typically, a log store is a NoSQL database, \nsuch as HDFS, capable of handling large data volumes.\n•\t\nLog stream processor: The log stream processor is capable of analyzing real-\ntime log events for quick decision making. A stream processor takes actions \nsuch as sending information to a dashboard, sending alerts, and so on. In \nthe case of self-healing systems, stream processors can even take actions to \ncorrect the problems.\n•\t\nLog dashboard: A dashboard is a single pane of glass used to display log \nanalysis results such as graphs and charts. These dashboards are meant for \nthe operational and management staff.\nThe benefit of this centralized approach is that there is no local I/O or blocking \ndisk writes. It also does not use the local machine's disk space. This architecture is \nfundamentally similar to the lambda architecture for big data processing.\nTo read more on the Lambda architecture, go to http://lambda-\narchitecture.net.\nIt is important to have in each log message a context, message, and correlation ID. \nThe context typically has the timestamp, IP address, user information, process details \n(such as service, class, and functions), log type, classification, and so on. The message \nwill be plain and simple free text information. The correlation ID is used to establish \nthe link between service calls so that calls spanning microservices can be traced.\n",
      "content_length": 2230,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "Logging and Monitoring Microservices\n[ 288 ]\nThe selection of logging solutions\nThere are a number of options available to implement a centralized logging solution. \nThese solutions use different approaches, architectures, and technologies. It is \nimportant to understand the capabilities required and select the right solution that \nmeets the needs.\nCloud services\nThere are a number of cloud logging services available, such as the SaaS solution.\nLoggly is one of the most popular cloud-based logging services. Spring Boot \nmicroservices can use Loggly's Log4j and Logback appenders to directly stream log \nmessages into the Loggly service.\nIf the application or service is deployed in AWS, AWS CloudTrail can be integrated \nwith Loggly for log analysis.\nPapertrial, Logsene, Sumo Logic, Google Cloud Logging, and Logentries are \nexamples of other cloud-based logging solutions.\nThe cloud logging services take away the overhead of managing complex \ninfrastructures and large storage solutions by providing them as simple-to-integrate \nservices. However, latency is one of the key factors to be considered when selecting \ncloud logging as a service.\nOff-the-shelf solutions\nThere are many purpose-built tools to provide end-to-end log management \ncapabilities that are installable locally in an on-premises data center or in the cloud.\nGraylog is one of the popular open source log management solutions. Graylog uses \nElasticsearch for log storage and MongoDB as a metadata store. Graylog also uses \nGELF libraries for Log4j log streaming.\nSplunk is one of the popular commercial tools available for log management and \nanalysis. Splunk uses the log file shipping approach, compared to log streaming used \nby other solutions to collect logs.\nBest-of-breed integration\nThe last approach is to pick and choose best-of-breed components and build a \ncustom logging solution.\n",
      "content_length": 1873,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "Chapter 7\n[ 289 ]\nLog shippers\nThere are log shippers that can be combined with other tools to build an end-to-end \nlog management solution. The capabilities differ between different log shipping \ntools.\nLogstash is a powerful data pipeline tool that can be used to collect and ship log \nfiles. Logstash acts as a broker that provides a mechanism to accept streaming \ndata from different sources and sync them to different destinations. Log4j and \nLogback appenders can also be used to send log messages directly from Spring Boot \nmicroservices to Logstash. The other end of Logstash is connected to Elasticsearch, \nHDFS, or any other database.\nFluentd is another tool that is very similar to Logstash, as is Logspout, but the latter \nis more appropriate in a Docker container-based environment.\nLog stream processors\nStream-processing technologies are optionally used to process log streams on the \nfly. For example, if a 404 error is continuously occurring as a response to a particular \nservice call, it means there is something wrong with the service. Such situations have \nto be handled as soon as possible. Stream processors are pretty handy in such cases \nas they are capable of reacting to certain streams of events that a traditional reactive \nanalysis can't.\nA typical architecture used for stream processing is a combination of Flume and \nKafka together with either Storm or Spark Streaming. Log4j has Flume appenders, \nwhich are useful to collect log messages. These messages are pushed into distributed \nKafka message queues. The stream processors collect data from Kafka and process \nthem on the fly before sending it to Elasticsearch and other log stores.\nSpring Cloud Stream, Spring Cloud Stream Modules, and Spring Cloud Data Flow \ncan also be used to build the log stream processing.\nLog storage\nReal-time log messages are typically stored in Elasticsearch. Elasticsearch allows \nclients to query based on text-based indexes. Apart from Elasticsearch, HDFS is also \ncommonly used to store archived log messages. MongoDB or Cassandra is used to \nstore summary data, such as monthly aggregated transaction counts. Offline log \nprocessing can be done using Hadoop's MapReduce programs.\n",
      "content_length": 2201,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "Logging and Monitoring Microservices\n[ 290 ]\nDashboards\nThe last piece required in the central logging solution is a dashboard. The most \ncommonly used dashboard for log analysis is Kibana on top of an Elasticsearch data \nstore. Graphite and Grafana are also used to display log analysis reports.\nA custom logging implementation\nThe tools mentioned before can be leveraged to build a custom end-to-end logging \nsolution. The most commonly used architecture for custom log management is a \ncombination of Logstash, Elasticsearch, and Kibana, also known as the ELK stack.\nThe full source code of this chapter is available under the Chapter \n7 project in the code files. Copy chapter5.configserver, \nchapter5.eurekaserver, chapter5.search, chapter5.\nsearch-apigateway, and chapter5.website into a new STS \nworkspace and rename them chapter7.*.\nThe following diagram shows the log monitoring flow:\nIn this section, a simple implementation of a custom logging solution using the ELK \nstack will be examined.\nFollow these steps to implement the ELK stack for logging:\n1.\t Download and install Elasticsearch, Kibana, and Logstash from https://\nwww.elastic.co.\n2.\t Update the Search microservice (chapter7.search). Review and ensure that \nthere are some log statements in the Search microservice. The log statements \nare nothing special but simple log statements using slf4j, as follows:\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n  //other code goes here\n  private static final Logger logger = LoggerFactory. \n    getLogger(SearchRestController.class);\n//other code goes here\n  \n",
      "content_length": 1586,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "Chapter 7\n[ 291 ]\nlogger.info(\"Looking to load flights...\");\nfor (Flight flight : flightRepository. \n  findByOriginAndDestinationAndFlightDate \n  (\"NYC\", \"SFO\", \"22-JAN-16\")) {\n      logger.info(flight.toString());\n}\n3.\t Add the logstash dependency to integrate logback to Logstash in the \nSearch service's pom.xml file, as follows:\n<dependency>\n  <groupId>net.logstash.logback</groupId>\n  <artifactId>logstash-logback-encoder</artifactId>\n  <version>4.6</version>\n</dependency>\n4.\t Also, downgrade the logback version to be compatible with Spring \n1.3.5.RELEASE via the following line:\n<logback.version>1.1.6</logback.version>\n5.\t Override the default Logback configuration. This can be done by adding a \nnew logback.xml file under src/main/resources, as follows:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n    <include resource=\"org/springframework/boot/logging/logback/\ndefaults.xml\"/>\n  <include resource=\"org/springframework/boot/logging/logback/\nconsole-appender.xml\" />\n    <appender name=\"stash\" class=\"net.logstash.logback. \n      appender.LogstashTcpSocketAppender\">\n        <destination>localhost:4560</destination>\n        <!-- encoder is required -->\n        <encoder class=\"net.logstash.logback.encoder. \n          LogstashEncoder\" />\n    </appender>\n  <root level=\"INFO\">\n    <appender-ref ref=\"CONSOLE\" />\n    <appender-ref ref=\"stash\" />\n  </root>\n</configuration>\nThe preceding configuration overrides the default Logback configuration by \nadding a new TCP socket appender, which streams all the log messages to \na Logstash service, which is listening on port 4560. It is important to add an \nencoder, as mentioned in the previous configuration.\n",
      "content_length": 1676,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "Logging and Monitoring Microservices\n[ 292 ]\n6.\t Create a configuration as shown in the following code and store it in a \nlogstash.conf file. The location of this file is irrelevant as it will be passed \nas an argument when starting Logstash. This configuration will take input \nfrom the socket listening on 4560 and send the output to Elasticsearch \nrunning on 9200. The stdout is optional and is set to debug:\ninput {\n  tcp {\n     port => 4560\n     host => localhost\n  }\n}\noutput {\nelasticsearch { hosts => [\"localhost:9200\"] }\n  stdout { codec => rubydebug }\n}\n7.\t Run Logstash, Elasticsearch, and Kibana from their respective installation \nfolders, as follows:\n./bin/logstash -f logstash.conf\n./bin/elasticsearch\n./bin/kibana\n8.\t Run the Search microservice. This will invoke the unit test cases and result in \nprinting the log statements mentioned before.\n9.\t Go to a browser and access Kibana, at http://localhost:5601.\n10.\t Go to Settings | Configure an index pattern, as shown here:\n",
      "content_length": 991,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "Chapter 7\n[ 293 ]\n11.\t Go to the Discover menu to see the logs. If everything is successful, we \nwill see the Kibana screenshot as follows. Note that the log messages are \ndisplayed in the Kibana screen.\nKibana provides out-of-the-box features to build summary charts and graphs \nusing log messages:\nDistributed tracing with Spring Cloud Sleuth\nThe previous section addressed microservices' distributed and fragmented logging \nissue by centralizing the log data. With the central logging solution, we can have all \nthe logs in a central storage. However, it is still almost impossible to trace end-to-end \ntransactions. In order to do end-to-end tracking, transactions spanning microservices \nneed to have a correlation ID.\nTwitter's Zipkin, Cloudera's HTrace, and Google's Dapper systems are examples of \ndistributed tracing systems. Spring Cloud provides a wrapper component on top of \nthese using the Spring Cloud Sleuth library.\n",
      "content_length": 933,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "Logging and Monitoring Microservices\n[ 294 ]\nDistributed tracing works with the concepts of span and trace. The span is a unit of \nwork; for example, calling a service is identified by a 64-bit span ID. A set of spans \nform a tree-like structure is called a trace. Using the trace ID, the call can be tracked \nend to end:\nAs shown in the diagram, Microservice 1 calls Microservice 2, and Microservice \n2 calls Microservice 3. In this case, as shown in the diagram, the same trace ID is \npassed across all microservices, which can be used to track transactions end to end.\nIn order to demonstrate this, we will use the Search API Gateway and Search \nmicroservices. A new endpoint has to be added in Search API Gateway (chapter7.\nsearch-apigateway) that internally calls the Search service to return data. Without \nthe trace ID, it is almost impossible to trace or link calls coming from the Website to \nSearch API Gateway to Search microservice. In this case, it only involves two to three \nservices, whereas in a complex environment, there could be many interdependent \nservices.\nFollow these steps to create the example using Sleuth:\n1.\t Update Search and Search API Gateway. Before this, the Sleuth dependency \nneeds to be added to the respective POM files, which can be done via the \nfollowing code:\n<dependency>\n  <groupId>org.springframework.cloud</groupId>\n  <artifactId>spring-cloud-starter-sleuth</artifactId>\n</dependency>\n2.\t In the case of building a new service, select Sleuth and Web, as shown here:\n",
      "content_length": 1513,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "Chapter 7\n[ 295 ]\n3.\t Add the Logstash dependency to the Search service as well as the Logback \nconfiguration, as in the previous example.\n4.\t The next step is to add two more properties in the Logback configuration,  \nas follows:\n<property name=\"spring.application.name\" value=\"search-service\"/>\n<property name=\"CONSOLE_LOG_PATTERN\" value=\"%d{yyyy-MM-dd \nHH:mm:ss.SSS} [${spring.application.name}] [trace=%X{X-Trace-Id:-\n},span=%X{X-Span-Id:-}] [%15.15t] %-40.40logger{39}: %m%n\"/>\nThe first property is the name of the application. The names given in this are \nthe service IDs: search-service and search-apigateway in Search and \nSearch API Gateway, respectively. The second property is an optional pattern \nused to print the console log messages with a trace ID and span ID. The \npreceding change needs to be applied to both the services.\n5.\t Add the following piece of code to advise Sleuth when to start a new span ID \nin the Spring Boot Application class. In this case, AlwaysSampler is used to \nindicate that the span ID has to be created every time a call hits the service. \nThis change needs to be applied in both the services:\n  @Bean\n    public AlwaysSampler defaultSampler() {\n      return new AlwaysSampler();\n    }\n",
      "content_length": 1229,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "Logging and Monitoring Microservices\n[ 296 ]\n6.\t Add a new endpoint to Search API Gateway, which will call the Search \nservice as follows. This is to demonstrate the propagation of the trace ID \nacross multiple microservices. This new method in the gateway returns the \noperating hub of the airport by calling the Search service, as follows:\n  @RequestMapping(\"/hubongw\")\n  String getHub(HttpServletRequest req){\n    logger.info(\"Search Request in API gateway for getting Hub, \nforwarding to search-service \");\n    String hub = restTemplate.getForObject(\"http://search-service/\nsearch/hub\", String.class);\n    logger.info(\"Response for hub received,  Hub \"+ hub);\n    return hub; \n  }\n7.\t Add another endpoint in the Search service, as follows:\n  @RequestMapping(\"/hub\")\n  String getHub(){\n    logger.info(\"Searching for Hub, received from search-\napigateway \");\n    return \"SFO\"; \n  }\n8.\t Once added, run both the services. Hit the gateway's new hub on the \ngateway (/hubongw) endpoint using a browser ( http://localhost:8095/\nhubongw).\nAs mentioned earlier, the Search API Gateway service is running on 8095 \nand the Search service is running on 8090.\n9.\t Look at the console logs to see the trace ID and span IDs printed. The first \nprint is from Search API Gateway, and the second one came from the Search \nservice. Note that the trace IDs are the same in both the cases, as follows:\n2016-04-02 17:24:37.624 [search-apigateway] [trace=8a7e278f-7b2b-\n43e3-a45c-69d3ca66d663,span=8a7e278f-7b2b-43e3-a45c-69d3ca66d663] \n[io-8095-exec-10] c.b.p.s.a.SearchAPIGatewayController    : \nResponse for hub received,  Hub DXB\n2016-04-02 17:24:37.612 [search-service] [trace=8a7e278f-7b2b-\n43e3-a45c-69d3ca66d663,span=fd309bba-5b4d-447f-a5e1-7faaab90cfb1] \n[nio-8090-exec-1] c.b.p.search.component.SearchComponent  : \nSearching for Hub, received from search-apigateway\n",
      "content_length": 1860,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "Chapter 7\n[ 297 ]\n10.\t Open the Kibana console and search for the trace ID using this trace ID \nprinted in the console. In this case, it is 8a7e278f-7b2b-43e3-a45c-\n69d3ca66d663. As shown in the following screenshot, with a trace ID,  \none can trace service calls that span multiple services:\nMonitoring microservices\nMicroservices are truly distributed systems with a fluid deployment topology. \nWithout sophisticated monitoring in place, operations teams may run into trouble \nmanaging large-scale microservices. Traditional monolithic application deployments \nare limited to a number of known services, instances, machines, and so on. This \nis easier to manage compared to the large number of microservices instances \npotentially running across different machines. To add more complication, these \nservices dynamically change their topologies. A centralized logging capability only \naddresses part of the issue. It is important for operations teams to understand the \nruntime deployment topology and also the behavior of the systems. This demands \nmore than a centralized logging can offer.\nIn general application, monitoring is more a collection of metrics, aggregation, and \ntheir validation against certain baseline values. If there is a service-level breach, \nthen monitoring tools generate alerts and send them to administrators. With \nhundreds and thousands of interconnected microservices, traditional monitoring \ndoes not really offer true value. The one-size-fits-all approach to monitoring or \nmonitoring everything with a single pane of glass is not easy to achieve in large-scale \nmicroservices.\n",
      "content_length": 1611,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "Logging and Monitoring Microservices\n[ 298 ]\nOne of the main objectives of microservice monitoring is to understand the behavior \nof the system from a user experience point of view. This will ensure that the end-to-\nend behavior is consistent and is in line with what is expected by the users.\nMonitoring challenges\nSimilar to the fragmented logging issue, the key challenge in monitoring \nmicroservices is that there are many moving parts in a microservice ecosystem.\nThe typical issues are summarized here:\n•\t\nThe statistics and metrics are fragmented across many services, instances, \nand machines.\n•\t\nHeterogeneous technologies may be used to implement microservices, which \nmakes things even more complex. A single monitoring tool may not give all \nthe required monitoring options.\n•\t\nMicroservices deployment topologies are dynamic, making it impossible to \npreconfigure servers, instances, and monitoring parameters.\nMany of the traditional monitoring tools are good to monitor monolithic applications \nbut fall short in monitoring large-scale, distributed, interlinked microservice systems. \nMany of the traditional monitoring systems are agent-based preinstall agents on the \ntarget machines or application instances. This poses two challenges:\n•\t\nIf the agents require deep integration with the services or operating systems, \nthen this will be hard to manage in a dynamic environment\n•\t\nIf these tools impose overheads when monitoring or instrumenting the \napplication, it may lead to performance issues\nMany traditional tools need baseline metrics. Such systems work with preset rules, \nsuch as if the CPU utilization goes above 60% and remains at this level for 2 minutes, \nthen an alert should be sent to the administrator. It is extremely hard to preconfigure \nthese values in large, Internet-scale deployments.\nNew-generation monitoring applications learn the application's behavior by \nthemselves and set automatic threshold values. This frees up administrators from \ndoing this mundane task. Automated baselines are sometimes more accurate than \nhuman forecasts:\n",
      "content_length": 2081,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "Chapter 7\n[ 299 ]\nAs shown in the diagram, the key areas of microservices monitoring are:\n•\t\nMetrics sources and data collectors: Metrics collection at the source is done \neither by the server pushing metrics information to a central collector or \nby embedding lightweight agents to collect information. Data collectors \ncollect monitoring metrics from different sources, such as network, physical \nmachines, containers, software components, applications, and so on. The \nchallenge is to collect this data using autodiscovery mechanisms instead of \nstatic configurations.\nThis is done by either running agents on the source machines, streaming data \nfrom the sources, or polling at regular intervals.\n•\t\nAggregation and correlation of metrics: Aggregation capability is required \nfor aggregating metrics collected from different sources, such as user \ntransaction, service, infrastructure, network, and so on. Aggregation can be \nchallenging as it requires some level of understanding of the application's \nbehavior, such as service dependencies, service grouping, and so on. In many \ncases, these are automatically formulated based on the metadata provided by \nthe sources.\nGenerally, this is done by an intermediary that accept the metrics.\n•\t\nProcessing metrics and actionable insights: Once data is aggregated, the \nnext step is to do the measurement. Measurements are typically done using \nset thresholds. In the new-generation monitoring systems, these thresholds \nare automatically discovered. Monitoring tools then analyze the data and \nprovide actionable insights.\nThese tools may use big data and stream analytics solutions.\n•\t\nAlerting, actions, and dashboards: As soon as issues are detected, they have \nto be notified to the relevant people or systems. Unlike traditional systems, \nthe microservices monitoring systems should be capable of taking actions on \na real-time basis. Proactive monitoring is essential to achieving self-healing. \nDashboards are used to display SLAs, KPIs, and so on.\nDashboards and alerting tools are capable of handling these requirements.\n",
      "content_length": 2081,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "Logging and Monitoring Microservices\n[ 300 ]\nMicroservice monitoring is typically done with three approaches. A combination of \nthese is really required for effective monitoring:\n•\t\nApplication performance monitoring (APM): This is more of a traditional \napproach to system metrics collection, processing, alerting, and dashboard \nrendering. These are more from the system's point of view. Application \ntopology discovery and visualization are new capabilities implemented \nby many of the APM tools. The capabilities vary between different APM \nproviders.\n•\t\nSynthetic monitoring: This is a technique that is used to monitor the \nsystem's behavior using end-to-end transactions with a number of test \nscenarios in a production or production-like environment. Data is collected to \nvalidate the system's behavior and potential hotspots. Synthetic monitoring \nhelps understand the system dependencies as well.\n•\t\nReal user monitoring (RUM) or user experience monitoring: This is \ntypically a browser-based software that records real user statistics, such as \nresponse time, availability, and service levels. With microservices, with more \nfrequent release cycle and dynamic topology, user experience monitoring is \nmore important.\nMonitoring tools\nThere are many tools available to monitor microservices. There are also overlaps \nbetween many of these tools. The selection of monitoring tools really depends upon \nthe ecosystem that needs to be monitored. In most cases, more than one tool is \nrequired to monitor the overall microservice ecosystem.\nThe objective of this section is to familiarize ourselves with a number of common \nmicroservices-friendly monitoring tools:\n•\t\nAppDynamics, Dynatrace, and New Relic are top commercial vendors \nin the APM space, as per Gartner Magic Quadrant 2015. These tools are \nmicroservice friendly and support microservice monitoring effectively \nin a single console. Ruxit, Datadog, and Dataloop are other commercial \nofferings that are purpose-built for distributed systems that are essentially \nmicroservices friendly. Multiple monitoring tools can feed data to Datadog \nusing plugins.\n•\t\nCloud vendors come with their own monitoring tools, but in many \ncases, these monitoring tools alone may not be sufficient for large-scale \nmicroservice monitoring. For instance, AWS uses CloudWatch and Google \nCloud Platform uses Cloud Monitoring to collect information from various \nsources.\n",
      "content_length": 2422,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "Chapter 7\n[ 301 ]\n•\t\nSome of the data collecting libraries, such as Zabbix, statd, collectd, jmxtrans, \nand so on operate at a lower level in collecting runtime statistics, metrics, \ngauges, and counters. Typically, this information is fed into data collectors \nand processors such as Riemann, Datadog, and Librato, or dashboards such \nas Graphite.\n•\t\nSpring Boot Actuator is one of the good vehicles to collect microservices \nmetrics, gauges, and counters, as we discussed in Chapter 2, Building \nMicroservices with Spring Boot. Netflix Servo, a metric collector similar to \nActuator, and the QBit and Dropwizard metrics also fall in the same category \nof metric collectors. All these metrics collectors need an aggregator and \ndashboard to facilitate full-sized monitoring.\n•\t\nMonitoring through logging is popular but a less effective approach \nin microservices monitoring. In this approach, as discussed in the \nprevious section, log messages are shipped from various sources, such as \nmicroservices, containers, networks, and so on to a central location. Then, \nwe can use the logs files to trace transactions, identify hotspots, and so on. \nLoggly, ELK, Splunk, and Trace are candidates in this space.\n•\t\nSensu is a popular choice for microservice monitoring from the open source \ncommunity. Weave Scope is another tool, primarily targeting containerized \ndeployments. Spigo is one of the purpose-built microservices monitoring \nsystems closely aligned with the Netflix stack.\n•\t\nPingdom, New Relic Synthetics, Runscope, Catchpoint, and so on provide \noptions for synthetic transaction monitoring and user experience monitoring \non live systems.\n•\t\nCirconus is classified more as a DevOps monitoring tool but can also do \nmicroservices monitoring. Nagios is a popular open source monitoring tool \nbut falls more into the traditional monitoring system.\n•\t\nPrometheus provides a time series database and visualization GUI useful in \nbuilding custom monitoring tools.\nMonitoring microservice dependencies\nWhen there are a large number of microservices with dependencies, it is important \nto have a monitoring tool that can show the dependencies among microservices. It is \nnot a scalable approach to statically configure and manage these dependencies. There \nare many tools that are useful in monitoring microservice dependencies, as follows:\n•\t\nMentoring tools such as AppDynamics, Dynatrace, and New Relic can draw \ndependencies among microservices. End-to-end transaction monitoring can \nalso trace transaction dependencies. Other monitoring tools, such as Spigo, \nare also useful for microservices dependency management.\n",
      "content_length": 2628,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "Logging and Monitoring Microservices\n[ 302 ]\n•\t\nCMDB tools such as Device42 or purpose-built tools such as Accordance are \nuseful in managing the dependency of microservices. Veritas Risk Advisor \n(VRA) is also useful for infrastructure discovery.\n•\t\nA custom implementation with a Graph database, such as Neo4j, is also \nuseful. In this case, a microservice has to preconfigure its direct and indirect \ndependencies. At the startup of the service, it publishes and cross-checks its \ndependencies with a Neo4j database.\nSpring Cloud Hystrix for fault-tolerant \nmicroservices\nThis section will explore Spring Cloud Hystrix as a library for a fault-tolerant and \nlatency-tolerant microservice implementation. Hystrix is based on the fail fast and \nrapid recovery principles. If there is an issue with a service, Hystrix helps isolate it. It \nhelps to recover quickly by falling back to another preconfigured fallback service. \nHystrix is another battle-tested library from Netflix. Hystrix is based on the circuit \nbreaker pattern.\nRead more about the circuit breaker pattern at https://msdn.\nmicrosoft.com/en-us/library/dn589784.aspx.\nIn this section, we will build a circuit breaker with Spring Cloud Hystrix. Perform \nthe following steps to change the Search API Gateway service to integrate it with \nHystrix:\n1.\t Update the Search API Gateway service. Add the Hystrix dependency to the \nservice. If developing from scratch, select the following libraries:\n",
      "content_length": 1458,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "Chapter 7\n[ 303 ]\n2.\t In the Spring Boot Application class, add @EnableCircuitBreaker.  \nThis command will tell Spring Cloud Hystrix to enable a circuit breaker  \nfor this application. It also exposes the /hystrix.stream endpoint for \nmetrics collection.\n3.\t Add a component class to the Search API Gateway service with a method; \nin this case, this is getHub annotated with @HystrixCommand. This tells \nSpring that this method is prone to failure. Spring Cloud libraries wrap these \nmethods to handle fault tolerance and latency tolerance by enabling circuit \nbreaker. The Hystrix command typically follows with a fallback method. In \ncase of failure, Hystrix automatically enables the fallback method mentioned \nand diverts traffic to the fallback method. As shown in the following code, in \nthis case, getHub will fall back to getDefaultHub:\n@Component  \nclass SearchAPIGatewayComponent { \n  @LoadBalanced\n  @Autowired \n  RestTemplate restTemplate;\n",
      "content_length": 952,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "Logging and Monitoring Microservices\n[ 304 ]\n  @HystrixCommand(fallbackMethod = \"getDefaultHub\")\n  public String getHub(){\n    String hub = restTemplate.getForObject(\"http://search-service/\nsearch/hub\", String.class);\n    return hub;\n  }\n  public String getDefaultHub(){\n    return \"Possibily SFO\";\n  }\n}\n4.\t The getHub method of SearchAPIGatewayController calls the getHub \nmethod of SearchAPIGatewayComponent, as follows:\n@RequestMapping(\"/hubongw\") \nString getHub(){\n  logger.info(\"Search Request in API gateway for getting Hub, \nforwarding to search-service \");\n  return component.getHub(); \n} \n5.\t The last part of this exercise is to build a Hystrix Dashboard. For this, build \nanother Spring Boot application. Include Hystrix, Hystrix Dashboard, and \nActuator when building this application.\n6.\t In the Spring Boot Application class, add the @EnableHystrixDashboard \nannotation.\n7.\t Start the Search service, Search API Gateway, and Hystrix Dashboard \napplications. Point the browser to the Hystrix Dashboard application's URL. \nIn this example, the Hystrix Dashboard is started on port 9999. So, open the \nURL http://localhost:9999/hystrix.\n8.\t A screen similar to the following screenshot will be displayed. In the Hystrix \nDashboard, enter the URL of the service to be monitored.\nIn this case, Search API Gateway is running on port 8095. Hence, the \nhystrix.stream URL will be http://localhost:8095/hytrix.stream,  \nas shown:\n",
      "content_length": 1436,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "Chapter 7\n[ 305 ]\n9.\t The Hystrix Dashboard will be displayed as follows:\n",
      "content_length": 74,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "Logging and Monitoring Microservices\n[ 306 ]\nNote that at least one transaction has to be executed to see the display. \nThis can be done by hitting http://localhost:8095/hubongw.\n10.\t Create a failure scenario by shutting down the Search service. Note \nthat the fallback method will be called when hitting the URL http://\nlocalhost:8095/hubongw.\n11.\t If there are continuous failures, then the circuit status will be changed to \nopen. This can be done by hitting the preceding URL a number of times. In \nthe open state, the original service will no longer be checked. The Hystrix \nDashboard will show the status of the circuit as Open, as shown in the \nfollowing screenshot. Once a circuit is opened, periodically, the system will \ncheck for the original service status for recovery. When the original service \nis back, the circuit breaker will fall back to the original service and the status \nwill be set to Closed:\nTo know the meaning of each of these parameters, visit the Hystrix wiki \nat https://github.com/Netflix/Hystrix/wiki/Dashboard.\n",
      "content_length": 1045,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "Chapter 7\n[ 307 ]\nAggregating Hystrix streams with Turbine\nIn the previous example, the /hystrix.stream endpoint of our microservice was \ngiven in the Hystrix Dashboard. The Hystrix Dashboard can only monitor one \nmicroservice at a time. If there are many microservices, then the Hystrix Dashboard \npointing to the service has to be changed every time we switch the microservices to \nmonitor. Looking into one instance at a time is tedious, especially when there are \nmany instances of a microservice or multiple microservices.\nWe have to have a mechanism to aggregate data coming from multiple /hystrix.\nstream instances and consolidate it into a single dashboard view. Turbine does \nexactly the same thing. Turbine is another server that collects Hystrix streams from \nmultiple instances and consolidates them into one /turbine.stream instance. \nNow, the Hystrix Dashboard can point to /turbine.stream to get the consolidated \ninformation:\nTurbine currently works only with different hostnames. Each instance \nhas to be run on separate hosts. If you are testing multiple services \nlocally on the same host, then update the host file (/etc/hosts) to \nsimulate multiple hosts. Once done, bootstrap.properties has to \nbe configured as follows:\neureka.instance.hostname: localdomain2\nThis example showcases how to use Turbine to monitor circuit breakers across \nmultiple instances and services. We will use the Search service and Search API \nGateway in this example. Turbine internally uses Eureka to resolve service IDs  \nthat are configured for monitoring.\n",
      "content_length": 1557,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "Logging and Monitoring Microservices\n[ 308 ]\nPerform the following steps to build and execute this example:\n1.\t The Turbine server can be created as just another Spring Boot application \nusing Spring Boot Starter. Select Turbine to include the Turbine libraries.\n2.\t Once the application is created, add @EnableTurbine to the main Spring \nBoot Application class. In this example, both Turbine and Hystrix Dashboard \nare configured to be run on the same Spring Boot application. This is \npossible by adding the following annotations to the newly created Turbine \napplication:\n@EnableTurbine\n@EnableHystrixDashboard\n@SpringBootApplication\npublic class TurbineServerApplication {\n3.\t Add the following configuration to the .yaml or property file to point to the \ninstances that we are interested in monitoring:\nspring:\n   application:\n     name : turbineserver\nturbine:\n   clusterNameExpression: new String('default')\n   appConfig : search-service,search-apigateway\nserver:\n  port: 9090\neureka:\n  client:\n    serviceUrl:\n       defaultZone: http://localhost:8761/eureka/\n4.\t The preceding configuration instructs the Turbine server to look up the \nEureka server to resolve the search-service and search-apigateway \nservices. The search-service and search-apigateways service IDs are \nused to register services with Eureka. Turbine uses these names to resolve the \nactual service host and port by checking with the Eureka server. It will then \nuse this information to read /hystrix.stream from each of these instances. \nTurbine will then read all the individual Hystrix streams, aggregate all of \nthem, and expose them under the Turbine server's /turbine.stream URL.\n5.\t The cluster name expression is pointing to the default cluster as there is \nno explicit cluster configuration done in this example. If the clusters are \nmanually configured, then the following configuration has to be used:\nturbine:\n  aggregator:\n    clusterConfig: [comma separated clusternames]\n",
      "content_length": 1963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "Chapter 7\n[ 309 ]\n6.\t Change the Search service's SearchComponent to add another circuit breaker, \nas follows:\n  @HystrixCommand(fallbackMethod = \"searchFallback\")\n  public List<Flight> search(SearchQuery query){\n7.\t Also, add @EnableCircuitBreaker to the main Application class in the \nSearch service.\n8.\t Add the following configuration to bootstrap.properties of the Search \nservice. This is required because all the services are running on the same \nhost:\nEureka.instance.hostname: localdomain1\n9.\t Similarly, add the following in bootstrap.properties of the Search API \nGateway service. This is to make sure that both the services use different \nhostnames:\neureka.instance.hostname: localdomain2\n10.\t In this example, we will run two instances of search-apigateway: one on \nlocaldomain1:8095 and another one on localdomain2:8096. We will also \nrun one instance of search-service on localdomain1:8090.\n11.\t Run the microservices with command-line overrides to manage different host \naddresses, as follows:\njava -jar -Dserver.port=8096 -Deureka.instance.\nhostname=localdomain2 -Dserver.address=localdomain2 target/\nchapter7.search-apigateway-1.0.jar\njava -jar -Dserver.port=8095 -Deureka.instance.\nhostname=localdomain1 -Dserver.address=localdomain1 target/\nchapter7.search-apigateway-1.0.jar\njava -jar -Dserver.port=8090 -Deureka.instance.\nhostname=localdomain1 -Dserver.address=localdomain1 target/\nchapter7.search-1.0.jar\n12.\t Open Hystrix Dashboard by pointing the browser to http://\nlocalhost:9090/hystrix.\n13.\t Instead of giving /hystrix.stream, this time, we will point to /turbine.\nstream. In this example, the Turbine stream is running on 9090. Hence, the \nURL to be given in the Hystrix Dashboard is http://localhost:9090/\nturbine.stream.\n14.\t Fire a few transactions by opening a browser window and hitting the \nfollowing two URLs: http://localhost:8095/hubongw and http://\nlocalhost:8096/hubongw.\nOnce this is done, the dashboard page will show the getHub service.\n",
      "content_length": 1980,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "Logging and Monitoring Microservices\n[ 310 ]\n15.\t Run chapter7.website. Execute the search transaction using the website \nhttp://localhost:8001.\nAfter executing the preceding search, the dashboard page will show  \nsearch-service as well. This is shown in the following screenshot:\nAs we can see in the dashboard, search-service is coming from the Search \nmicroservice, and getHub is coming from Search API Gateway. As we have two \ninstances of Search API Gateway, getHub is coming from two hosts, indicated by \nHosts 2.\nData analysis using data lakes\nSimilarly to the scenario of fragmented logs and monitoring, fragmented data is \nanother challenge in the microservice architecture. Fragmented data poses challenges \nin data analytics. This data may be used for simple business event monitoring, data \nauditing, or even deriving business intelligence out of the data.\nA data lake or data hub is an ideal solution to handling such scenarios. An event-\nsourced architecture pattern is generally used to share the state and state changes \nas events with an external data store. When there is a state change, microservices \npublish the state change as events. Interested parties may subscribe to these events \nand process them based on their requirements. A central event store may also \nsubscribe to these events and store them in a big data store for further analysis.\n",
      "content_length": 1368,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "Chapter 7\n[ 311 ]\nOne of the commonly followed architectures for such data handling is shown in the \nfollowing diagram:\nState change events generated from the microservice—in our case, the Search, \nBooking, and Check-In events—are pushed to a distributed high-performance \nmessaging system, such as Kafka. A data ingestion service, such as Flume, can \nsubscribe to these events and update them to an HDFS cluster. In some cases, these \nmessages will be processed in real time by Spark Streaming. To handle heterogeneous \nsources of events, Flume can also be used between event sources and Kafka.\nSpring Cloud Streams, Spring Cloud Streams modules, and Spring Data Flow are \nalso useful as alternatives for high-velocity data ingestion.\nSummary\nIn this chapter, you learned about the challenges around logging and monitoring \nwhen dealing with Internet-scale microservices.\nWe explored the various solutions for centralized logging. You also learned about \nhow to implement a custom centralized logging using Elasticsearch, Logstash, and \nKibana (ELK). In order to understand distributed tracing, we upgraded BrownField \nmicroservices using Spring Cloud Sleuth.\nIn the second half of this chapter, we went deeper into the capabilities required \nfor microservices monitoring solutions and different approaches to monitoring. \nSubsequently, we examined a number of tools available for microservices \nmonitoring.\n",
      "content_length": 1409,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "Logging and Monitoring Microservices\n[ 312 ]\nThe BrownField microservices are further enhanced with Spring Cloud Hystrix \nand Turbine to monitor latencies and failures in inter-service communications. The \nexamples also demonstrated how to use the circuit breaker pattern to fall back to \nanother service in case of failures.\nFinally, we also touched upon the importance of data lakes and how to integrate a \ndata lake architecture in a microservice context.\nMicroservice management is another important challenge we need to tackle when \ndealing with large-scale microservice deployments. The next chapter will explore \nhow containers can help in simplifying microservice management.\n",
      "content_length": 684,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": "[ 313 ]\nContainerizing Microservices \nwith Docker\nIn the context of microservices, containerized deployment is the icing on the cake. \nIt helps microservices be more autonomous by self-containing the underlying \ninfrastructure, thereby making the microservices cloud neutral.\nThis chapter will introduce the concepts and relevance of virtual machine images \nand the containerized deployment of microservices. Then, this chapter will further \nfamiliarize readers with building Docker images for the BrownField PSS microservices \ndeveloped with Spring Boot and Spring Cloud. Finally, this chapter will also touch \nbase on how to manage, maintain, and deploy Docker images in a production-like \nenvironment.\nBy the end of this chapter, you will learn about:\n•\t\nThe concept of containerization and its relevance in the context  \nof microservices\n•\t\nBuilding and deploying microservices as Docker images and containers\n•\t\nUsing AWS as an example of cloud-based Docker deployments\n",
      "content_length": 975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "Containerizing Microservices with Docker\n[ 314 ]\nReviewing the microservice capability \nmodel\nIn this chapter, we will explore the following microservice capabilities from the \nmicroservice capability model discussed in Chapter 3, Applying Microservices Concepts:\n•\t\nContainers and virtual machines\n•\t\nThe private/public cloud\n•\t\nThe microservices repository\nThe model is shown in the following diagram:\nUnderstanding the gaps in BrownField \nPSS microservices\nIn Chapter 5, Scaling Microservices with Spring Cloud, BrownField PSS microservices \nwere developed using Spring Boot and Spring Cloud. These microservices \nare deployed as versioned fat JAR files on bare metals, specifically on a local \ndevelopment machine.\n",
      "content_length": 719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": "Chapter 8\n[ 315 ]\nIn Chapter 6, Autoscaling Microservices, the autoscaling capability was added with the \nhelp of a custom life cycle manager. In Chapter 7, Logging and Monitoring Microservices, \nchallenges around logging and monitoring were addressed using centralized logging \nand monitoring solutions.\nThere are still a few gaps in our BrownField PSS implementation. So far, the \nimplementation has not used any cloud infrastructure. Dedicated machines, as \nin traditional monolithic application deployments, are not the best solution for \ndeploying microservices. Automation such as automatic provisioning, the ability to \nscale on demand, self-service, and payment based on usage are essential capabilities \nrequired to manage large-scale microservice deployments efficiently. In general, a \ncloud infrastructure provides all these essential capabilities. Therefore, a private or \npublic cloud with the capabilities mentioned earlier is better suited to deploying \nInternet-scale microservices.\nAlso, running one microservice instance per bare metal is not cost effective. \nTherefore, in most cases, enterprises end up deploying multiple microservices \non a single bare metal server. Running multiple microservices on a single bare \nmetal could lead to a \"noisy neighbor\" problem. There is no isolation between the \nmicroservice instances running on the same machine. As a result, services deployed \non a single machine may eat up others' space, thus impacting their performance.\nAn alternate approach is to run the microservices on VMs. However, VMs are \nheavyweight in nature. Therefore, running many smaller VMs on a physical \nmachine is not resource efficient. This generally results in resource wastage.  \nIn the case of sharing a VM to deploy multiple services, we would end up  \nfacing the same issues of sharing the bare metal, as explained earlier.\nIn the case of Java-based microservices, sharing a VM or bare metal to deploy \nmultiple microservices also results in sharing JRE among microservices. This is \nbecause the fat JARs created in our BrownField PSS abstract only application code \nand its dependencies but not JREs. Any update on JRE installed on the machine \nwill have an impact on all the microservices deployed on this machine. Similarly, \nif there are OS-level parameters, libraries, or tunings that are required for specific \nmicroservices, then it will be hard to manage them on a shared environment.\nOne microservice principle insists that it should be self-contained and autonomous \nby fully encapsulating its end-to-end runtime environment. In order to align with \nthis principle, all components, such as the OS, JRE, and microservice binaries, \nhave to be self-contained and isolated. The only option to achieve this is to follow \nthe approach of deploying one microservice per VM. However, this will result in \nunderutilized virtual machines, and in many cases, extra cost due to this can nullify \nbenefits of microservices.\n",
      "content_length": 2961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "Containerizing Microservices with Docker\n[ 316 ]\nWhat are containers?\nContainers are not revolutionary, ground-breaking concepts. They have been in \naction for quite a while. However, the world is witnessing the re-entry of containers, \nmainly due to the wide adoption of cloud computing. The shortcomings of traditional \nvirtual machines in the cloud computing space also accelerated the use of containers. \nContainer providers such as Docker simplified container technologies to a great extent, \nwhich also enabled a large adoption of container technologies in today's world. The \nrecent popularity of DevOps and microservices also acted as a catalyst for the rebirth \nof container technologies.\nSo, what are containers? Containers provide private spaces on top of the operating \nsystem. This technique is also called operating system virtualization. In this \napproach, the kernel of the operating system provides isolated virtual spaces. Each \nof these virtual spaces is called a container or virtual engine (VE). Containers allow \nprocesses to run on an isolated environment on top of the host operating system. A \nrepresentation of multiple containers running on the same host is shown as follows:\nContainers are easy mechanisms to build, ship, and run compartmentalized \nsoftware components. Generally, containers package all the binaries and libraries \nthat are essential for running an application. Containers reserve their own \nfilesystem, IP address, network interfaces, internal processes, namespaces, OS \nlibraries, application binaries, dependencies, and other application configurations.\nThere are billions of containers used by organizations. Moreover, there are many \nlarge organizations heavily investing in container technologies. Docker is far ahead \nof the competition, supported by many large operating system vendors and cloud \nproviders. Lmctfy, SystemdNspawn, Rocket, Drawbridge, LXD, Kurma, and Calico \nare some of the other containerization solutions. Open container specification is also \nunder development.\n",
      "content_length": 2035,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "Chapter 8\n[ 317 ]\nThe difference between VMs and \ncontainers\nVMs such as Hyper-V, VMWare, and Zen were popular choices for data center \nvirtualization a few years ago. Enterprises experienced a cost saving by implementing \nvirtualization over the traditional bare metal usage. It has also helped many enterprises \nutilize their existing infrastructure in a much more optimized manner. As VMs support \nautomation, many enterprises experienced that they had to make lesser management \neffort with virtual machines. Virtual machines also helped organizations get isolated \nenvironments for applications to run in.\nPrima facie, both virtualization and containerization exhibit exactly the same \ncharacteristics. However, in a nutshell, containers and virtual machines are not the \nsame. Therefore, it is unfair to make an apple-to-apple comparison between VMs  \nand containers. Virtual machines and containers are two different techniques and \naddress different problems of virtualization. This difference is evident from the \nfollowing diagram:\nVirtual machines operate at a much lower level compared to containers. VMs provide \nhardware virtualization, such as that of CPUs, motherboards, memory, and so on. \nA VM is an isolated unit with an embedded operating system, generally called a \nGuest OS. VMs replicate the whole operating system and run it within the VM with \nno dependency on the host operating system environment. As VMs embed the full \noperating system environment, these are heavyweight in nature. This is an advantage \nas well as a disadvantage. The advantage is that VMs offer complete isolation to the \nprocesses running on VMs. The disadvantage is that it limits the number of VMs one \ncan spin up in a bare metal due to the resource requirements of VMs.\n",
      "content_length": 1772,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "Containerizing Microservices with Docker\n[ 318 ]\nThe size of a VM has a direct impact on the time to start and stop it. As starting a VM \nin turn boots the OS, the start time for VMs is generally high. VMs are more friendly \nwith infrastructure teams as it requires a low level of infrastructure competency to \nmanage VMs.\nIn the container world, containers do not emulate the entire hardware or operating \nsystem. Unlike VMs, containers share certain parts of the host kernel and operating \nsystem. There is no concept of guest OS in the case of containers. Containers provide \nan isolated execution environment directly on top of the host operating system. This \nis its advantage as well as disadvantage. The advantage is that it is lighter as well \nas faster. As containers on the same machine share the host operating system, the \noverall resource utilization of containers is fairly small. As a result, many smaller \ncontainers can be run on the same machine, as compared to heavyweight VMs. As \ncontainers on the same host share the host operating system, there are limitations as \nwell. For example, it is not possible to set iptables firewall rules inside a container. \nProcesses inside the container are completely independent from the processes on \ndifferent containers running on the same host.\nUnlike VMs, container images are publically available on community portals. This \nmakes developers' lives much easier as they don't have to build the images from \nscratch; instead, they can now take a base image from certified sources and add \nadditional layers of software components on top of the downloaded base image.\nThe lightweight nature of the containers is also opening up a plethora of opportunities, \nsuch as automated build, publishing, downloading, copying, and so on. The ability to \ndownload, build, ship, and run containers with a few commands or to use REST APIs \nmakes containers more developer friendly. Building a new container does not take \nmore than a few seconds. Containers are now part and parcel of continuous delivery \npipelines as well.\nIn summary, containers have many advantages over VMs, but VMs have their own \nexclusive strengths. Many organizations use both containers and VMs, such as by \nrunning containers on top of VMs.\n",
      "content_length": 2265,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": "Chapter 8\n[ 319 ]\nThe benefits of containers\nWe have already considered the many benefits of containers over VMs. This section \nwill explain the overall benefits of containers beyond the benefits of VMs:\n•\t\nSelf-contained: Containers package the essential application binaries and their \ndependencies together to make sure that there is no disparity between different \nenvironments such as development, testing, or production. This promotes \nthe concept of Twelve-Factor applications and that of immutable containers. \nSpring Boot microservices bundle all the required application dependencies. \nContainers stretch this boundary further by embedding JRE and other \noperating system-level libraries, configurations, and so on, if there are any.\n•\t\nLightweight: Containers, in general, are smaller in size with a lighter \nfootprint. The smallest container, Alpine, has a size of less than 5 MB. The \nsimplest Spring Boot microservice packaged with an Alpine container with \nJava 8 would only come to around 170 MB in size. Though the size is still on \nthe higher side, it is much less than the VM image size, which is generally in \nGBs. The smaller footprint of containers not only helps spin new containers \nquickly but also makes building, shipping, and storing easier.\n•\t\nScalable: As container images are smaller in size and there is no OS booting \nat startup, containers are generally faster to spin up and shut down. This \nmakes containers the popular choice for cloud-friendly elastic applications.\n•\t\nPortable: Containers provide portability across machines and cloud \nproviders. Once the containers are built with all the dependencies, they \ncan be ported across multiple machines or across multiple cloud providers \nwithout relying on the underlying machines. Containers are portable from \ndesktops to different cloud environments.\n•\t\nLower license cost: Many software license terms are based on the physical \ncore. As containers share the operating system and are not virtualized at the \nphysical resources level, there is an advantage in terms of the license cost.\n•\t\nDevOps: The lightweight footprint of containers makes it easy to automate \nbuilds and publish and download containers from remote repositories. This \nmakes it easy to use in Agile and DevOps environments by integrating with \nautomated delivery pipelines. Containers also support the concept of build \nonce by creating immutable containers at build time and moving them across \nmultiple environments. As containers are not deep into the infrastructure, \nmultidisciplinary DevOps teams can manage containers as part of their  \nday-to-day life.\n•\t\nVersion controlled: Containers support versions by default. This helps build \nversioned artifacts, just as with versioned archive files.\n",
      "content_length": 2760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "Containerizing Microservices with Docker\n[ 320 ]\n•\t\nReusable: Container images are reusable artifacts. If an image is built  \nby assembling a number of libraries for a purpose, it can be reused in  \nsimilar situations.\n•\t\nImmutable containers: In this concept, containers are created and disposed \nof after usage. They are never updated or patched. Immutable containers are \nused in many environments to avoid complexities in patching deployment \nunits. Patching results in a lack of traceability and an inability to recreate \nenvironments consistently.\nMicroservices and containers\nThere is no direct relationship between microservices and containers. Microservices \ncan run without containers, and containers can run monolithic applications. However, \nthere is a sweet spot between microservices and containers.\nContainers are good for monolithic applications, but the complexities and the size of \nthe monolith application may kill some of the benefits of the containers. For example, \nspinning new containers quickly may not be easy with monolithic applications. \nIn addition to this, monolithic applications generally have local environment \ndependencies, such as the local disk, stovepipe dependencies with other systems,  \nand so on. Such applications are difficult to manage with container technologies.  \nThis is where microservices go hand in hand with containers.\nThe following diagram shows three polyglot microservices running on the same \nhost machine and sharing the same operating system but abstracting the runtime \nenvironment:\n",
      "content_length": 1546,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": "Chapter 8\n[ 321 ]\nThe real advantage of containers can be seen when managing many polyglot \nmicroservices—for instance, one microservice in Java and another one in Erlang or \nsome other language. Containers help developers package microservices written in any \nlanguage or technology in a platform- and technology-agnostic fashion and uniformly \ndistribute them across multiple environments. Containers eliminate the need to have \ndifferent deployment management tools to handle polyglot microservices. Containers \nnot only abstract the execution environment but also how to access the services. \nIrrespective of the technologies used, containerized microservices expose REST APIs. \nOnce the container is up and running, it binds to certain ports and exposes its APIs. \nAs containers are self-contained and provide full stack isolation among services, in \na single VM or bare metal, one can run multiple heterogeneous microservices and \nhandle them in a uniform way.\nIntroduction to Docker\nThe previous sections talked about containers and their benefits. Containers have \nbeen in the business for years, but the popularity of Docker has given containers  \na new outlook. As a result, many container definitions and perspectives emerged \nfrom the Docker architecture. Docker is so popular that even containerization is \nreferred to as dockerization.\nDocker is a platform to build, ship, and run lightweight containers based on Linux \nkernels. Docker has default support for Linux platforms. It also has support for  \nMac and Windows using Boot2Docker, which runs on top of Virtual Box.\nAmazon EC2 Container Service (ECS) has out-of-the-box support for Docker on \nAWS EC2 instances. Docker can be installed on bare metals and also on traditional \nvirtual machines such as VMWare or Hyper-V.\n",
      "content_length": 1790,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "Containerizing Microservices with Docker\n[ 322 ]\nThe key components of Docker\nA Docker installation has two key components: a Docker daemon and a Docker \nclient. Both the Docker daemon and Docker client are distributed as a single binary.\nThe following diagram shows the key components of a Docker installation:\nThe Docker daemon\nThe Docker daemon is a server-side component that runs on the host machine \nresponsible for building, running, and distributing Docker containers. The Docker \ndaemon exposes APIs for the Docker client to interact with the daemon. These APIs \nare primarily REST-based endpoints. One can imagine that the Docker daemon as a \ncontroller service running on the host machine. Developers can programmatically \nuse these APIs to build custom clients as well.\nThe Docker client\nThe Docker client is a remote command-line program that interacts with the Docker \ndaemon through either a socket or REST APIs. The CLI can run on the same host as \nthe daemon is running on or it can run on a completely different host and connect \nto the daemon remotely. Docker users use the CLI to build, ship, and run Docker \ncontainers.\n",
      "content_length": 1141,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": "Chapter 8\n[ 323 ]\nDocker concepts\nThe Docker architecture is built around a few concepts: images, containers, the \nregistry, and the Dockerfile.\nDocker images\nOne of the key concepts of Docker is the image. A Docker image is the read-only \ncopy of the operating system libraries, the application, and its libraries. Once an \nimage is created, it is guaranteed to run on any Docker platform without alterations.\nIn Spring Boot microservices, a Docker image packages operating systems such as \nUbuntu, Alpine, JRE, and the Spring Boot fat application JAR file. It also includes \ninstructions to run the application and expose the services:\nAs shown in the diagram, Docker images are based on a layered architecture in \nwhich the base image is one of the flavors of Linux. Each layer, as shown in the \npreceding diagram, gets added to the base image layer with the previous image  \nas the parent layer. Docker uses the concept of a union filesystem to combine all \nthese layers into a single image, forming a single filesystem.\nIn typical cases, developers do not build Docker images from scratch. Images  \nof an operating system, or other common libraries, such as Java 8 images,  \nare publicly available from trusted sources. Developers can start building on top  \nof these base images. The base image in Spring microservices can be JRE 8 rather \nthan starting from a Linux distribution image such as Ubuntu.\n",
      "content_length": 1408,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "Containerizing Microservices with Docker\n[ 324 ]\nEvery time we rebuild the application, only the changed layer gets rebuilt, and the \nremaining layers are kept intact. All the intermediate layers are cached, and hence, \nif there is no change, Docker uses the previously cached layer and builds it on \ntop. Multiple containers running on the same machine with the same type of base \nimages would reuse the base image, thus reducing the size of the deployment. For \ninstance, in a host, if there are multiple containers running with Ubuntu as the base \nimage, they all reuse the same base image. This is applicable when publishing or \ndownloading images as well:\nAs shown in the diagram, the first layer in the image is a boot filesystem called \nbootfs, which is similar to the Linux kernel and the boot loader. The boot filesystem \nacts as a virtual filesystem for all images.\nOn top of the boot filesystem, the operating system filesystem is placed, which \nis called rootfs. The root filesystem adds the typical operating system directory \nstructure to the container. Unlike in the Linux systems, rootfs, in the case of  \nDocker, is on a read-only mode.\nOn top of rootfs, other required images are placed as per the requirements. In \nour case, these are JRE and the Spring Boot microservice JARs. When a container \nis initiated, a writable filesystem is placed on top of all the other filesystems for the \nprocesses to run. Any changes made by the process to the underlying filesystem \nare not reflected in the actual container. Instead, these are written to the writable \nfilesystem. This writable filesystem is volatile. Hence, the data is lost once the \ncontainer is stopped. Due to this reason, Docker containers are ephemeral in nature.\n",
      "content_length": 1742,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "Chapter 8\n[ 325 ]\nThe base operating system packaged inside Docker is generally a minimal copy of \njust the OS filesystem. In reality the process running on top may not use the entire \nOS services. In a Spring Boot microservice, in many cases, the container just initiates \na CMD and JVM and then invokes the Spring Boot fat JAR.\nDocker containers\nDocker containers are the running instances of a Docker image. Containers use the \nkernel of the host operating system when running. Hence, they share the host kernel \nwith other containers running on the same host. The Docker runtime ensures that \nthe container processes are allocated with their own isolated process space using \nkernel features such as cgroups and the kernel namespace of the operating system. \nIn addition to the resource fencing, containers get their own filesystem and network \nconfigurations as well.\nThe containers, when instantiated, can have specific resource allocations, such as \nthe memory and CPU. Containers, when initiated from the same image, can have \ndifferent resource allocations. The Docker container, by default, gets an isolated \nsubnet and gateway to the network. The network has three modes.\nThe Docker registry\nThe Docker registry is a central place where Docker images are published and \ndownloaded from. The URL https://hub.docker.com is the central registry \nprovided by Docker. The Docker registry has public images that one can download \nand use as the base registry. Docker also has private images that are specific to the \naccounts created in the Docker registry. The Docker registry screenshot is shown  \nas follows:\n",
      "content_length": 1617,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "Containerizing Microservices with Docker\n[ 326 ]\nDocker also offers Docker Trusted Registry, which can be used to set up registries \nlocally on premises.\nDockerfile\nA Dockerfile is a build or scripting file that contains instructions to build a Docker \nimage. There can be multiple steps documented in the Dockerfile, starting from \ngetting a base image. A Dockerfile is a text file that is generally named Dockerfile.  \nThe docker build command looks up Dockerfile for instructions to build.  \nOne can compare a Dockerfile to a pom.xml file used in a Maven build.\nDeploying microservices in Docker\nThis section will operationalize our learning by showcasing how to build containers \nfor our BrownField PSS microservices.\nThe full source code of this chapter is available under the Chapter \n8 project in the code files. Copy chapter7.configserver, \nchapter7.eurekaserver, chapter7.search, chapter7.\nsearch-apigateway, and chapter7.website into a new STS \nworkspace and rename them chapter8.*.\nPerform the following steps to build Docker containers for BrownField  \nPSS microservices:\n1.\t Install Docker from the official Docker site at https://www.docker.com.\nFollow the Get Started link for the download and installation instructions \nbased on the operating system of choice. Once installed, use the following \ncommand to verify the installation:\n$docker –version\nDocker version 1.10.1, build 9e83765\n2.\t In this section, we will take a look at how to dockerize the Search \n(chapter8.search) microservice, the Search API Gateway (chapter8.\nsearch-apigateway) microservice, and the Website (chapter8.website) \nSpring Boot application.\n3.\t Before we make any changes, we need to edit bootstrap.properties to \nchange the config server URL from localhost to the IP address as localhost is \nnot resolvable from within the Docker containers. In the real world, this will \npoint to a DNS or load balancer, as follows:\nspring.cloud.config.uri=http://192.168.0.105:8888\n",
      "content_length": 1962,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "Chapter 8\n[ 327 ]\nReplace the IP address with the IP address of your machine.\n4.\t Similarly, edit search-service.properties on the Git repository and \nchange localhost to the IP address. This is applicable for the Eureka URL  \nas well as the RabbitMQ URL. Commit back to Git after updating. You can  \ndo this via the following code:\nspring.application.name=search-service\nspring.rabbitmq.host=192.168.0.105\nspring.rabbitmq.port=5672\nspring.rabbitmq.username=guest\nspring.rabbitmq.password=guest\norginairports.shutdown:JFK\neureka.client.serviceUrl.defaultZone: http://192.168.0.105:8761/\neureka/\nspring.cloud.stream.bindings.inventoryQ=inventoryQ\n5.\t Change the RabbitMQ configuration file rabbitmq.config by uncommenting \nthe following line to provide access to guest. By default, guest is restricted to be \naccessed from localhost only:\n    {loopback_users, []}\nThe location of rabbitmq.config will be different for different  \noperating systems.\n6.\t Create a Dockerfile under the root directory of the Search microservice,  \nas follows:\nFROM frolvlad/alpine-oraclejdk8\nVOLUME /tmp\nADD  target/search-1.0.jar search.jar\nEXPOSE 8090\nENTRYPOINT [\"java\",\"-jar\",\"/search.jar\"]\nThe following is a quick examination of the contents of the Dockerfile:\n°°\nFROM frolvlad/alpine-oraclejdk8: This tells the Docker build to \nuse a specific alpine-oraclejdk8 version as the basic image for this \nbuild. The frolvlad indicates the repository to locate the alpine-\noraclejdk8 image. In this case, it is an image built with Alpine Linux \nand Oracle JDK 8. This will help layer our application on top of the base \nimage without setting up Java libraries ourselves. In this case, as this \nimage is not available on our local image store, the Docker build will go \nahead and download this image from the remote Docker Hub registry.\n",
      "content_length": 1814,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "Containerizing Microservices with Docker\n[ 328 ]\n°°\nVOLUME /tmp: This enables access from the container to the \ndirectory specified in the host machine. In our case, this points \nto the tmp directory in which the Spring Boot application creates \nworking directories for Tomcat. The tmp directory is a logical \none for the container, which indirectly points to one of the local \ndirectories of the host.\n°°\nADD target/search-1.0.jar search.jar: This adds the \napplication binary file to the container with the destination filename \nspecified. In this case, the Docker build copies target/search-\n1.0.jar to the container as search.jar.\n°°\nEXPOSE 8090: This is to tell the container how to do port mapping. \nThis associates 8090 with external port binding for the internal \nSpring Boot service.\n°°\nENTRYPOINT [\"java\",\"-jar\", \"/search.jar\"]: This tells the \ncontainer which default application to run when a container is \nstarted. In this case, we are pointing to the Java process and the \nSpring Boot fat JAR file to initiate the service.\n7.\t The next step is to run docker build from the folder in which the Dockerfile \nis stored. This will download the base image and run the entries in the \nDockerfile one after the other, as follows:\ndocker build –t search:1.0 .\nThe output of this command will be as follows:\n",
      "content_length": 1312,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "Chapter 8\n[ 329 ]\n8.\t Repeat the same steps for Search API Gateway and Website.\n9.\t Once the images are created, they can be verified by typing the following \ncommand. This command will list out the images and their details,  \nincluding the size of image files:\ndocker images\nThe output will be as follows:\n10.\t The next thing to do is run the Docker container. This can be done with the \ndocker run command. This command will load and run the container. \nOn starting, the container calls the Spring Boot executable JAR to start the \nmicroservice.\nBefore starting the containers, ensure that the Config and the Eureka servers \nare running:\ndocker run --net host -p 8090:8090 -t search:1.0\ndocker run --net host -p 8095:8095 -t search-apigateway:1.0\ndocker run --net host -p 8001:8001 -t website:1.0\nThe preceding command starts the Search and Search API Gateway \nmicroservices and Website.\nIn this example, we are using the host network (--net host) instead of  \nthe bridge network to avoid Eureka registering with the Docker container \nname. This can be corrected by overriding EurekaInstanceConfigBean.  \nThe host option is less isolated compared to the bridge option from the \nnetwork perspective. The advantage and disadvantage of host versus  \nbridge depends on the project.\n11.\t Once all the services are fully started, verify with the docker ps command,  \nas shown in the following screenshot:\n",
      "content_length": 1401,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "Containerizing Microservices with Docker\n[ 330 ]\n12.\t The next step is to point the browser to http://192.168.99.100:8001.  \nThis will open the BrownField PSS website.\nNote the IP address. This is the IP address of the Docker machine if you are \nrunning with Boot2Docker on Mac or Windows. In Mac or Windows, if the \nIP address is not known, then type the following command to find out the \nDocker machine's IP address for the default machine:\ndocker-machine ip default\nIf Docker is running on Linux, then this is the host IP address.\nApply the same changes to Booking, Fares, Check-in, and their respective  \ngateway microservices.\nRunning RabbitMQ on Docker\nAs our example also uses RabbitMQ, let's explore how to set up RabbitMQ as  \na Docker container. The following command pulls the RabbitMQ image from  \nDocker Hub and starts RabbitMQ:\ndocker run –net host rabbitmq3\nEnsure that the URL in *-service.properties is changed to the Docker host's  \nIP address. Apply the earlier rule to find out the IP address in the case of Mac  \nor Windows.\nUsing the Docker registry\nThe Docker Hub provides a central location to store all the Docker images. The \nimages can be stored as public as well as private. In many cases, organizations \ndeploy their own private registries on premises due to security-related concerns.\nPerform the following steps to set up and run a local registry:\n1.\t The following command will start a registry, which will bind the registry  \non port 5000:\ndocker run -d -p 5000:5000 --restart=always --name registry \nregistry:2\n2.\t Tag search:1.0 to the registry, as follows:\ndocker tag search:1.0 localhost:5000/search:1.0\n",
      "content_length": 1642,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": "Chapter 8\n[ 331 ]\n3.\t Then, push the image to the registry via the following command:\ndocker push localhost:5000/search:1.0\n4.\t Pull the image back from the registry, as follows:\ndocker pull localhost:5000/search:1.0\nSetting up the Docker Hub\nIn the previous chapter, we played with a local Docker registry. This section will \nshow how to set up and use the Docker Hub to publish the Docker containers. This \nis a convenient mechanism to globally access Docker images. Later in this chapter, \nDocker images will be published to the Docker Hub from the local machine and \ndownloaded from the EC2 instances.\nIn order to do this, create a public Docker Hub account and a repository.  \nFor Mac, follow the steps as per the following URL: https://docs.docker.com/\nmac/step_five/.\nIn this example, the Docker Hub account is created using the brownfield username.\nThe registry, in this case, acts as the microservices repository in which all the \ndockerized microservices will be stored and accessed. This is one of the capabilities \nexplained in the microservices capability model.\nPublishing microservices to the Docker Hub\nIn order to push dockerized services to the Docker Hub, follow these steps. The first \ncommand tags the Docker image, and the second one pushes the Docker image to  \nthe Docker Hub repository:\ndocker tag search:1.0brownfield/search:1.0\ndocker push brownfield/search:1.0\nTo verify whether the container images are published, go to the Docker Hub repository \nat https://hub.docker.com/u/brownfield.\nRepeat this step for all the other BrownField microservices as well. At the end of this \nstep, all the services will be published to the Docker Hub.\n",
      "content_length": 1665,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "Containerizing Microservices with Docker\n[ 332 ]\nMicroservices on the cloud\nOne of the capabilities mentioned in the microservices capability model is the use of \nthe cloud infrastructure for microservices. Earlier in this chapter, we also explored \nthe necessity of using the cloud for microservices deployments. So far, we have not \ndeployed anything to the cloud. As we have eight microservices in total—Config-\nserver, Eureka-server, Turbine, RabbitMQ, Elasticsearch, Kibana, and Logstash—\nin our overall BrownField PSS microservices ecosystem, it is hard to run all of them \non the local machine.\nIn the rest of this book, we will operate using AWS as the cloud platform to deploy \nBrownField PSS microservices.\nInstalling Docker on AWS EC2\nIn this section, we will install Docker on the EC2 instance.\nThis example assumes that readers are familiar with AWS and an account is already \ncreated on AWS.\nPerform the following steps to set up Docker on EC2:\n1.\t Launch a new EC2 instance. In this case, if we have to run all the instances \ntogether, we may need a large instance. The example uses t2.large.\nIn this example, the following Ubuntu AMI image is used: ubuntu-trusty-\n14.04-amd64-server-20160114.5 (ami-fce3c696).\n2.\t Connect to the EC2 instance and run the following commands:\nsudo apt-get update \nsudo apt-get install docker.io\n3.\t The preceding command will install Docker on an EC2 instance. Verify the \ninstallation with the following command:\ndocker version\nRunning BrownField services on EC2\nIn this section, we will set up BrownField microservices on the EC2 instances created. \nIn this case, the build is set up in the local desktop machine, and the binaries will be \ndeployed to AWS.\n",
      "content_length": 1706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": "Chapter 8\n[ 333 ]\nPerform the following steps to set up services on an EC2 instance:\n1.\t Install Git via the following command:\nsudo apt-get install git\n2.\t Create a Git repository on any folder of your choice.\n3.\t Change the Config server's bootstrap.properties to point to the \nappropriate Git repository created for this example.\n4.\t Change the bootstrap.properties of all the microservices to point to the \nconfig-server using the private IP address of the EC2 instance.\n5.\t Copy all *.properties from the local Git repository to the EC2 Git \nrepository and perform a commit.\n6.\t Change the Eureka server URLs and RabbitMQ URLs in the *.properties \nfile to match the EC2 private IP address. Commit the changes to Git once \nthey have been completed.\n7.\t On the local machine, recompile all the projects and create Docker images  \nfor the search, search-apigateway, and website microservices. Push all  \nof them to the Docker Hub registry.\n8.\t Copy the config-server and the Eureka-server binaries from the local machine \nto the EC2 instance.\n9.\t Set up Java 8 on the EC2 instance.\n10.\t Then, execute the following commands in sequence:\njava –jar config-server.jar \njava –jar eureka-server.jar \ndocker run –net host rabbitmq:3\ndocker run --net host -p 8090:8090 rajeshrv/search:1.0\ndocker run --net host -p 8095:8095 rajeshrv/search-apigateway:1.0\ndocker run --net host -p 8001:8001 rajeshrv/website:1.0\n11.\t Check whether all the services are working by opening the URL of the website \nand executing a search. Note that we will use the public IP address in this case: \nhttp://54.165.128.23:8001.\n",
      "content_length": 1599,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "Containerizing Microservices with Docker\n[ 334 ]\nUpdating the life cycle manager\nIn Chapter 6, Autoscaling Microservices, we considered a life cycle manager to \nautomatically start and stop instances. We used SSH and executed a Unix script to \nstart the Spring Boot microservices on the target machine. With Docker, we no longer \nneed SSH connections as the Docker daemon provides REST-based APIs to start and \nstop instances. This greatly simplifies the complexities of the deployment engine \ncomponent of the life cycle manager.\nIn this section, we will not rewrite the life cycle manager. By and large, we will \nreplace the life cycle manager in the next chapter.\nThe future of containerization – \nunikernels and hardened security\nContainerization is still evolving, but the number of organizations adopting \ncontainerization techniques has gone up in recent times. While many organizations \nare aggressively adopting Docker and other container technologies, the downside  \nof these techniques is still in the size of the containers and security concerns.\nCurrently, Docker images are generally heavy. In an elastic automated environment, \nwhere containers are created and destroyed quite frequently, size is still an issue. \nA larger size indicates more code, and more code means that it is more prone to \nsecurity vulnerabilities.\nThe future is definitely in small footprint containers. Docker is working on \nunikernels, lightweight kernels that can run Docker even on low-powered IoT \ndevices. Unikernels are not full-fledged operating systems, but they provide the  \nbasic necessary libraries to support the deployed applications.\nThe security issues of containers are much discussed and debated. The key security \nissues are around the user namespace segregation or user ID isolation. If the container \nis on root, then it can by default gain the root privilege of the host. Using container \nimages from untrusted sources is another security concern. Docker is bridging these \ngaps as quickly as possible, but there are many organizations that use a combination  \nof VMs and Docker to circumvent some of the security concerns.\n",
      "content_length": 2135,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "Chapter 8\n[ 335 ]\nSummary\nIn this chapter, you learned about the need to have a cloud environment when \ndealing with Internet-scale microservices.\nWe explored the concept of containers and compared them with traditional virtual \nmachines. You also learned the basics of Docker, and we explained the concepts of \nDocker images, containers, and registries. The importance and benefits of containers \nwere explained in the context of microservices.\nThis chapter then switched to a hands-on example by dockerizing the BrownField \nmicroservice. We demonstrated how to deploy the Spring Boot microservice \ndeveloped earlier on Docker. You learned the concept of registries by exploring a  \nlocal registry as well as the Docker Hub to push and pull dockerized microservices.\nAs the last step, we explored how to deploy a dockerized BrownField microservice in \nthe AWS cloud environment.\n",
      "content_length": 880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "[ 337 ]\nManaging Dockerized \nMicroservices with Mesos \nand Marathon\nIn an Internet-scale microservices deployment, it is not easy to manage thousands \nof dockerized microservices. It is essential to have an infrastructure abstraction \nlayer and a strong cluster control platform to successfully manage Internet-scale \nmicroservice deployments.\nThis chapter will explain the need and use of Mesos and Marathon as an infrastructure \nabstraction layer and a cluster control system, respectively, to achieve optimized \nresource usage in a cloud-like environment when deploying microservices at scale. \nThis chapter will also provide a step-by-step approach to setting up Mesos and \nMarathon in a cloud environment. Finally, this chapter will demonstrate how to \nmanage dockerized microservices in the Mesos and Marathon environment.\nBy the end of this chapter, you will have learned about:\n•\t\nThe need to have an abstraction layer and cluster control software\n•\t\nMesos and Marathon from the context of microservices\n•\t\nManaging dockerized BrownField Airline's PSS microservices with Mesos \nand Marathon\n",
      "content_length": 1099,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 338 ]\nReviewing the microservice capability \nmodel\nIn this chapter, we will explore the Cluster Control & Provisioning microservices \ncapability from the microservices capability model discussed in Chapter 3, Applying \nMicroservices Concepts:\nThe missing pieces\nIn Chapter 8, Containerizing Microservices with Docker, we discussed how to \ndockerize BrownField Airline's PSS microservices. Docker helped package the \nJVM runtime and OS parameters along with the application so that there is no \nspecial consideration required when moving dockerized microservices from one \nenvironment to another. The REST APIs provided by Docker have simplified the life \ncycle manager's interaction with the target machine in starting and stopping artifacts.\n",
      "content_length": 803,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "Chapter 9\n[ 339 ]\nIn a large-scale deployment, with hundreds and thousands of Docker containers, we \nneed to ensure that Docker containers run with their own resource constraints, such \nas memory, CPU, and so on. In addition to this, there may be rules set for Docker \ndeployments, such as replicated copies of the container should not be run on the \nsame machine. Also, a mechanism needs to be in place to optimally use the server \ninfrastructure to avoid incurring extra cost.\nThere are organizations that deal with billions of containers. Managing them manually \nis next to impossible. In the context of large-scale Docker deployments, some of the key \nquestions to be answered are:\n•\t\nHow do we manage thousands of containers?\n•\t\nHow do we monitor them?\n•\t\nHow do we apply rules and constraints when deploying artifacts?\n•\t\nHow do we ensure that we utilize containers properly to gain  \nresource efficiency?\n•\t\nHow do we ensure that at least a certain number of minimal instances  \nare running at any point in time?\n•\t\nHow do we ensure dependent services are up and running?\n•\t\nHow do we do rolling upgrades and graceful migrations?\n•\t\nHow do we roll back faulty deployments?\nAll these questions point to the need to have a solution to address two key \ncapabilities, which are as follows:\n•\t\nA cluster abstraction layer that provides a uniform abstraction over many \nphysical or virtual machines\n•\t\nA cluster control and init system to manage deployments intelligently on  \ntop of the cluster abstraction\nThe life cycle manager is ideally placed to deal with these situations. One can add \nenough intelligence to the life cycle manager to solve these issues. However, before \nattempting to modify the life cycle manager, it is important to understand the role  \nof cluster management solutions a bit more.\n",
      "content_length": 1810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 340 ]\nWhy cluster management is important\nAs microservices break applications into different micro-applications, many \ndevelopers request more server nodes for deployment. In order to manage \nmicroservices properly, developers tend to deploy one microservice per VM,  \nwhich further drives down the resource utilization. In many cases, this results  \nin an overallocation of CPUs and memory.\nIn many deployments, the high-availability requirements of microservices force \nengineers to add more and more service instances for redundancy. In reality,  \nthough it provides the required high availability, this will result in underutilized \nserver instances.\nIn general, microservice deployment requires more infrastructure compared to \nmonolithic application deployments. Due to the increase in cost of the infrastructure, \nmany organizations fail to see the value of microservices:\nIn order to address the issue stated before, we need a tool that is capable of  \nthe following:\n•\t\nAutomating a number of activities, such as the allocation of containers  \nto the infrastructure efficiently and keeping it transparent to developers  \nand administrators\n•\t\nProviding a layer of abstraction for the developers so that they can deploy \ntheir application against a data center without knowing which machine is  \nto be used to host their applications\n•\t\nSetting rules or constraints against deployment artifacts\n•\t\nOffering higher levels of agility with minimal management overheads for \ndevelopers and administrators, perhaps with minimal human interaction\n•\t\nBuilding, deploying, and managing the application's cost effectively by \ndriving a maximum utilization of the available resources\nContainers solve an important issue in this context. Any tool that we select with \nthese capabilities can handle containers in a uniform way, irrespective of the \nunderlying microservice technologies.\n",
      "content_length": 1943,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 368,
      "content": "Chapter 9\n[ 341 ]\nWhat does cluster management do?\nTypical cluster management tools help virtualize a set of machines and manage \nthem as a single cluster. Cluster management tools also help move the workload or \ncontainers across machines while being transparent to the consumer. Technology \nevangelists and practitioners use different terminologies, such as cluster orchestration, \ncluster management, data center virtualization, container schedulers, or container life \ncycle management, container orchestration, data center operating system, and so on.\nMany of these tools currently support both Docker-based containers as well as \nnoncontainerized binary artifact deployments, such as a standalone Spring Boot \napplication. The fundamental function of these cluster management tools is to abstract \nthe actual server instance from the application developers and administrators.\nCluster management tools help the self-service and provisioning of infrastructure \nrather than requesting the infrastructure teams to allocate the required machines \nwith a predefined specification. In this automated cluster management approach, \nmachines are no longer provisioned upfront and preallocated to the applications. \nSome of the cluster management tools also help virtualize data centers across \nmany heterogeneous machines or even across data centers, and create an elastic, \nprivate cloud-like infrastructure. There is no standard reference model for cluster \nmanagement tools. Therefore, the capabilities vary between vendors.\nSome of the key capabilities of cluster management software are summarized  \nas follows:\n•\t\nCluster management: It manages a cluster of VMs and physical machines  \nas a single large machine. These machines could be heterogeneous in terms  \nof resource capabilities, but they are, by and large, machines with Linux as \nthe operating system. These virtual clusters can be formed on the cloud,  \non-premises, or a combination of both.\n•\t\nDeployments: It handles the automatic deployment of applications and \ncontainers with a large set of machines. It supports multiple versions of the \napplication containers and also rolling upgrades across a large number of \ncluster machines. These tools are also capable of handling the rollback of \nfaulty promotes.\n•\t\nScalability: It handles the automatic and manual scalability of application \ninstances as and when required, with optimized utilization as the primary \ngoal.\n•\t\nHealth: It manages the health of the cluster, nodes, and applications.  \nIt removes faulty machines and application instances from the cluster.\n",
      "content_length": 2585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 369,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 342 ]\n•\t\nInfrastructure abstraction: It abstracts the developers from the actual \nmachine on which the applications are deployed. The developers need not \nworry about the machine, its capacity, and so on. It is entirely the cluster \nmanagement software's decision to decide how to schedule and run the \napplications. These tools also abstract machine details, their capacity, \nutilization, and location from the developers. For application owners, these \nare equivalent to a single large machine with almost unlimited capacity.\n•\t\nResource optimization: The inherent behavior of these tools is to allocate \ncontainer workloads across a set of available machines in an efficient way, \nthereby reducing the cost of ownership. Simple to extremely complicated \nalgorithms can be used effectively to improve utilization.\n•\t\nResource allocation: It allocates servers based on resource availability and \nthe constraints set by application developers. Resource allocation is based on \nthese constraints, affinity rules, port requirements, application dependencies, \nhealth, and so on.\n•\t\nService availability: It ensures that the services are up and running \nsomewhere in the cluster. In case of a machine failure, cluster control tools \nautomatically handle failures by restarting these services on some other \nmachine in the cluster.\n•\t\nAgility: These tools are capable of quickly allocating workloads to the \navailable resources or moving the workload across machines if there is  \nchange in resource requirements. Also, constraints can be set to realign  \nthe resources based on business criticality, business priority, and so on.\n•\t\nIsolation: Some of these tools provide resource isolation out of the box. \nHence, even if the application is not containerized, resource isolation can  \nbe still achieved.\nA variety of algorithms are used for resource allocation, ranging from simple \nalgorithms to complex algorithms, with machine learning and artificial intelligence. \nThe common algorithms used are random, bin packing, and spread. Constraints \nset against applications will override the default algorithms based on resource \navailability:\n",
      "content_length": 2199,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 370,
      "content": "Chapter 9\n[ 343 ]\nThe preceding diagram shows how these algorithms fill the available machines with \ndeployments. In this case, it is demonstrated with two machines:\n•\t\nSpread: This algorithm performs the allocation of workload equally across \nthe available machines. This is showed in diagram A.\n•\t\nBin packing: This algorithm tries to fill in data machine by machine and \nensures the maximum utilization of machines. Bin packing is especially  \ngood when using cloud services in a pay-as-you-use style. This is shown  \nin diagram B.\n•\t\nRandom: This algorithm randomly chooses machines and deploys containers \non randomly selected machines. This is showed in diagram C.\nThere is a possibility of using cognitive computing algorithms such as machine \nlearning and collaborative filtering to improve efficiency. Techniques such as \noversubscription allow a better utilization of resources by allocating underutilized \nresources for high-priority tasks—for example, revenue-generating services for  \nbest-effort tasks such as analytics, video, image processing, and so on.\n",
      "content_length": 1071,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 371,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 344 ]\nRelationship with microservices\nThe infrastructure of microservices, if not properly provisioned, can easily result in \noversized infrastructures and, essentially, a higher cost of ownership. As discussed \nin the previous sections, a cloud-like environment with a cluster management tool is \nessential to realize cost benefits when dealing with large-scale microservices.\nThe Spring Boot microservices turbocharged with the Spring Cloud project is the \nideal candidate workload to leverage cluster management tools. As Spring Cloud-\nbased microservices are location unaware, these services can be deployed anywhere \nin the cluster. Whenever services come up, they automatically register to the service \nregistry and advertise their availability. On the other hand, consumers always look \nfor the registry to discover the available service instances. This way, the application \nsupports a full fluid structure without preassuming a deployment topology. With \nDocker, we were able to abstract the runtime so that the services could run on any \nLinux-based environments.\nRelationship with virtualization\nCluster management solutions are different from server virtualization solutions \nin many aspects. Cluster management solutions run on top of VMs or physical \nmachines as an application component.\nCluster management solutions\nThere are many cluster management software tools available. It is unfair to do an \napple-to-apple comparison between them. Even though there are no one-to-one \ncomponents, there are many areas of overlap in capabilities between them. In many \nsituations, organizations use a combination of one or more of these tools to fulfill \ntheir requirements.\n",
      "content_length": 1741,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 372,
      "content": "Chapter 9\n[ 345 ]\nThe following diagram shows the position of cluster management tools from the \nmicroservices context:\nIn this section, we will explore some of the popular cluster management solutions \navailable on the market.\nDocker Swarm\nDocker Swarm is Docker's native cluster management solution. Swarm provides a \nnative and deeper integration with Docker and exposes APIs that are compatible \nwith Docker's remote APIs. Docker Swarm logically groups a pool of Docker hosts \nand manages them as a single large Docker virtual host. Instead of application \nadministrators and developers deciding on which host the container is to be deployed \nin, this decision making will be delegated to Docker Swarm. Docker Swarm will decide \nwhich host to be used based on the bin packing and spread algorithms.\nAs Docker Swarm is based on Docker's remote APIs, its learning curve for those \nalready using Docker is narrower compared to any other container orchestration \ntools. However, Docker Swarm is a relatively new product on the market, and it  \nonly supports Docker containers.\nDocker Swarm works with the concepts of manager and nodes. A manager is the \nsingle point for administrations to interact and schedule the Docker containers for \nexecution. Nodes are where Docker containers are deployed and run.\n",
      "content_length": 1306,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 373,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 346 ]\nKubernetes\nKubernetes (k8s) comes from Google's engineering, is written in the Go language, \nand is battle-tested for large-scale deployments at Google. Similar to Swarm, \nKubernetes helps manage containerized applications across a cluster of nodes. \nKubernetes helps automate container deployments, scheduling, and the scalability  \nof containers. Kubernetes supports a number of useful features out of the box, such \nas automatic progressive rollouts, versioned deployments, and container resiliency  \nif containers fail due to some reason.\nThe Kubernetes architecture has the concepts of master, nodes, and pods. The master \nand nodes together form a Kubernetes cluster. The master node is responsible for \nallocating and managing workload across a number of nodes. Nodes are nothing but \na VM or a physical machine. Nodes are further subsegmented as pods. A node can \nhost multiple pods. One or more containers are grouped and executed inside a pod. \nPods are also helpful in managing and deploying co-located services for efficiency. \nKubernetes also supports the concept of labels as key-value pairs to query and find \ncontainers. Labels are user-defined parameters to tag certain types of nodes that \nexecute a common type of workloads, such as frontend web servers. The services \ndeployed on a cluster get a single IP/DNS to access the service.\nKubernetes has out-of-the-box support for Docker; however, the Kubernetes learning \ncurve is steeper compared to Docker Swarm. RedHat offers commercial support for \nKubernetes as part of its OpenShift platform.\nApache Mesos\nMesos is an open source framework originally developed by the University of \nCalifornia at Berkeley and is used by Twitter at scale. Twitter uses Mesos primarily \nto manage the large Hadoop ecosystem.\nMesos is slightly different from the previous solutions. Mesos is more of a resource \nmanager that relays on other frameworks to manage workload execution. Mesos  \nsits between the operating system and the application, providing a logical cluster  \nof machines.\nMesos is a distributed system kernel that logically groups and virtualizes many \ncomputers to a single large machine. Mesos is capable of grouping a number of \nheterogeneous resources to a uniform resource cluster on which applications can  \nbe deployed. For these reasons, Mesos is also known as a tool to build a private \ncloud in a data center.\n",
      "content_length": 2454,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 374,
      "content": "Chapter 9\n[ 347 ]\nMesos has the concepts of the master and slave nodes. Similar to the earlier solutions, \nmaster nodes are responsible for managing the cluster, whereas slaves run the \nworkload. Mesos internally uses ZooKeeper for cluster coordination and storage. \nMesos supports the concept of frameworks. These frameworks are responsible for \nscheduling and running noncontainerized applications and containers. Marathon, \nChronos, and Aurora are popular frameworks for the scheduling and execution of \napplications. Netflix Fenzo is another open source Mesos framework. Interestingly, \nKubernetes also can be used as a Mesos framework.\nMarathon supports the Docker container as well as noncontainerized applications. \nSpring Boot can be directly configured in Marathon. Marathon provides a number  \nof capabilities out of the box, such as supporting application dependencies, grouping \napplications to scale and upgrade services, starting and shutting down healthy and \nunhealthy instances, rolling out promotes, rolling back failed promotes, and so on.\nMesosphere offers commercial support for Mesos and Marathon as part of its  \nDCOS platform.\nNomad\nNomad from HashiCorp is another cluster management software. Nomad is a cluster \nmanagement system that abstracts lower-level machine details and their locations. \nNomad has a simpler architecture compared to the other solutions explored earlier. \nNomad is also lightweight. Similar to other cluster management solutions, Nomad \ntakes care of resource allocation and the execution of applications. Nomad also \naccepts user-specific constraints and allocates resources based on this.\nNomad has the concept of servers, in which all jobs are managed. One server acts  \nas the leader, and others act as followers. Nomad has the concept of tasks, which \nis the smallest unit of work. Tasks are grouped into task groups. A task group has \ntasks that are to be executed in the same location. One or more task groups or tasks  \nare managed as jobs.\nNomad supports many workloads, including Docker, out of the box. Nomad also \nsupports deployments across data centers and is region and data center aware.\nFleet\nFleet is a cluster management system from CoreOS. It runs on a lower level and \nworks on top of systemd. Fleet can manage application dependencies and make sure \nthat all the required services are running somewhere in the cluster. If a service fails, \nit restarts the service on another host. Affinity and constraint rules are possible to \nsupply when allocating resources.\n",
      "content_length": 2533,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 375,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 348 ]\nFleet has the concepts of engine and agents. There is only one engine at any point \nin the cluster with multiple agents. Tasks are submitted to the engine and agent run \nthese tasks on a cluster machine.\nFleet also supports Docker out of the box.\nCluster management with Mesos and \nMarathon\nAs we discussed in the previous section, there are many cluster management \nsolutions or container orchestration tools available. Different organizations  \nchoose different solutions to address problems based on their environment.  \nMany organizations choose Kubernetes or Mesos with a framework such as \nMarathon. In most cases, Docker is used as a default containerization method  \nto package and deploy workloads.\nFor the rest of this chapter, we will show how Mesos works with Marathon to \nprovide the required cluster management capability. Mesos is used by many \norganizations, including Twitter, Airbnb, Apple, eBay, Netflix, PayPal, Uber,  \nYelp, and many others.\nDiving deep into Mesos\nMesos can be treated as a data center kernel. DCOS is the commercial version \nof Mesos supported by Mesosphere. In order to run multiple tasks on one node, \nMesos uses resource isolation concepts. Mesos relies on the Linux kernel's cgroups \nto achieve resource isolation similar to the container approach. It also supports \ncontainerized isolation using Docker. Mesos supports both batch workload as  \nwell as the OLTP kind of workloads:\n",
      "content_length": 1490,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 376,
      "content": "Chapter 9\n[ 349 ]\nMesos is an open source top-level Apache project under the Apache license. Mesos \nabstracts lower-level computing resources such as CPU, memory, and storage from \nlower-level physical or virtual machines.\nBefore we examine why we need both Mesos and Marathon, let's understand the \nMesos architecture.\nThe Mesos architecture\nThe following diagram shows the simplest architectural representation of Mesos. \nThe key components of Mesos includes a Mesos master node, a set of slave nodes, \na ZooKeeper service, and a Mesos framework. The Mesos framework is further \nsubdivided into two components: a scheduler and an executor:\n",
      "content_length": 642,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 377,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 350 ]\nThe boxes in the preceding diagram are explained as follows:\n•\t\nMaster: The Mesos master is responsible for managing all the Mesos slaves. \nThe Mesos master gets information on the resource availability from all slave \nnodes and take the responsibility of filling the resources appropriately based \non certain resource policies and constraints. The Mesos master preempts \navailable resources from all slave machines and pools them as a single \nlarge machine. The master offers resources to frameworks running on slave \nmachines based on this resource pool.\nFor high availability, the Mesos master is supported by the Mesos master's \nstandby components. Even if the master is not available, the existing tasks \ncan still be executed. However, new tasks cannot be scheduled in the absence \nof a master node. The master standby nodes are nodes that wait for the \nfailure of the active master and take over the master's role in the case of  \na failure. It uses ZooKeeper for the master leader election. A minimum \nquorum requirement must be met for leader election.\n•\t\nSlave: Mesos slaves are responsible for hosting task execution frameworks. \nTasks are executed on the slave nodes. Mesos slaves can be started with \nattributes as key-value pairs, such as data center = X. This is used for constraint \nevaluations when deploying workloads. Slave machines share resource \navailability with the Mesos master.\n•\t\nZooKeeper: ZooKeeper is a centralized coordination server used in Mesos \nto coordinate activities across the Mesos cluster. Mesos uses ZooKeeper for \nleader election in case of a Mesos master failure.\n•\t\nFramework: The Mesos framework is responsible for understanding the \napplication's constraints, accepting resource offers from the master, and \nfinally running tasks on the slave resources offered by the master. The Mesos \nframework consists of two components: the framework scheduler and the \nframework executor:\n°°\nThe scheduler is responsible for registering to Mesos and handling \nresource offers\n°°\nThe executor runs the actual program on Mesos slave nodes\nThe framework is also responsible for enforcing certain policies and \nconstraints. For example, a constraint can be, let's say, that a minimum  \nof 500 MB of RAM is available for execution.\n",
      "content_length": 2329,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 378,
      "content": "Chapter 9\n[ 351 ]\nFrameworks are pluggable components and are replaceable with another \nframework. The framework workflow is depicted in the following diagram:\nThe steps denoted in the preceding workflow diagram are elaborated as follows:\n1.\t The framework registers with the Mesos master and waits for resource \noffers. The scheduler may have many tasks in its queue to be executed \nwith different resource constraints (tasks A to D, in this example). A task, \nin this case, is a unit of work that is scheduled—for example, a Spring Boot \nmicroservice.\n2.\t The Mesos slave offers the available resources to the Mesos master.  \nFor example, the slave advertises the CPU and memory available with  \nthe slave machine.\n3.\t The Mesos master then creates a resource offer based on the allocation \npolicies set and offers it to the scheduler component of the framework. \nAllocation policies determine which framework the resources are to be \noffered to and how many resources are to be offered. The default policies  \ncan be customized by plugging additional allocation policies.\n4.\t The scheduler framework component, based on the constraints, capabilities, \nand policies, may accept or reject the resource offering. For example,  \na framework rejects the resource offer if the resources are insufficient  \nas per the constraints and policies set.\n",
      "content_length": 1344,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 379,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 352 ]\n5.\t If the scheduler component accepts the resource offer, it submits the details \nof one more task to the Mesos master with resource constraints per task.  \nLet's say, in this example, that it is ready to submit tasks A to D.\n6.\t The Mesos master sends this list of tasks to the slave where the resources \nare available. The framework executor component installed on the slave \nmachines picks up and runs these tasks.\nMesos supports a number of frameworks, such as:\n•\t\nMarathon and Aurora for long-running processes, such as web applications\n•\t\nHadoop, Spark, and Storm for big data processing\n•\t\nChronos and Jenkins for batch scheduling\n•\t\nCassandra and Elasticsearch for data management\nIn this chapter, we will use Marathon to run dockerized microservices.\nMarathon\nMarathon is one of the Mesos framework implementations that can run both \ncontainer as well as noncontainer execution. Marathon is particularly designed for \nlong-running applications, such as a web server. Marathon ensures that the service \nstarted with Marathon continues to be available even if the Mesos slave it is hosted \non fails. This will be done by starting another instance.\nMarathon is written in Scala and is highly scalable. Marathon offers a UI as well as \nREST APIs to interact with Marathon, such as the start, stop, scale, and monitoring \napplications.\nSimilar to Mesos, Marathon's high availability is achieved by running multiple \nMarathon instances pointing to a ZooKeeper instance. One of the Marathon instances \nacts as a leader, and others are in standby mode. In case the leading master fails, a \nleader election will take place, and the next active master will be determined.\nSome of the basic features of Marathon include:\n•\t\nSetting resource constraints\n•\t\nScaling up, scaling down, and the instance management of applications\n•\t\nApplication version management\n•\t\nStarting and killing applications\n",
      "content_length": 1962,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 380,
      "content": "Chapter 9\n[ 353 ]\nSome of the advanced features of Marathon include:\n•\t\nRolling upgrades, rolling restarts, and rollbacks\n•\t\nBlue-green deployments\nImplementing Mesos and Marathon for \nBrownField microservices\nIn this section, the dockerized Brownfield microservice developed in Chapter 8, \nContainerizing Microservices with Docker, will be deployed into the AWS cloud and \nmanaged with Mesos and Marathon.\nFor the purposes of demonstration, only three of the services (Search, Search API \nGateway, and Website) are covered in the explanations:\nThe logical architecture of the target state implementation is shown in the preceding \ndiagram. The implementation uses multiple Mesos slaves to execute dockerized \nmicroservices with a single Mesos master. The Marathon scheduler component is \nused to schedule dockerized microservices. Dockerized microservices are hosted on \nthe Docker Hub registry. Dockerized microservices are implemented using Spring \nBoot and Spring Cloud.\n",
      "content_length": 975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 381,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 354 ]\nThe following diagram shows the physical deployment architecture:\nAs shown in the preceding diagram, in this example, we will use four EC2 instances:\n•\t\nEC2-M1: This hosts the Mesos master, ZooKeeper, the Marathon scheduler, \nand one Mesos slave instance\n•\t\nEC2-M2: This hosts one Mesos slave instance\n•\t\nEC2-M3: This hosts another Mesos slave instance\n•\t\nEC2-M4: This hosts Eureka, Config server, and RabbitMQ\nFor a real production setup, multiple Mesos masters as well as multiple instances  \nof Marathon are required for fault tolerance.\nSetting up AWS\nLaunch the four t2.micro EC2 instances that will be used for this deployment.  \nAll four instances have to be on the same security group so that the instances  \ncan see each other using their local IP addresses.\n",
      "content_length": 834,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 382,
      "content": "Chapter 9\n[ 355 ]\nThe following tables show the machine details and IP addresses for indicative \npurposes and to link subsequent instructions:\nInstance ID \nPrivate DNS/IP\nPublic DNS/IP\ni-06100786\nip-172-31-54-69.ec2.\ninternal\n172.31.54.69\nec2-54-85-107-37.compute-1.\namazonaws.com\n54.85.107.37\ni-2404e5a7\nip-172-31-62-44.ec2.\ninternal\n172.31.62.44\nec2-52-205-251-150.compute-1.\namazonaws.com\n52.205.251.150\ni-a7df2b3a\nip-172-31-49-55.ec2.\ninternal\n172.31.49.55\nec2-54-172-213-51.compute-1.\namazonaws.com\n54.172.213.51\ni-b0eb1f2d\nip-172-31-53-109.ec2.\ninternal\n172.31.53.109\nec2-54-86-31-240.compute-1.\namazonaws.com\n54.86.31.240\nReplace the IP and DNS addresses based on your AWS EC2 configuration.\nInstalling ZooKeeper, Mesos, and Marathon\nThe following software versions will be used for the deployment. The deployment  \nin this section follows the physical deployment architecture explained in the  \nearlier section:\n•\t\nMesos version 0.27.1\n•\t\nDocker version 1.6.2, build 7c8fca2\n•\t\nMarathon version 0.15.3\n",
      "content_length": 1010,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 383,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 356 ]\nThe detailed instructions to set up ZooKeeper, Mesos, and \nMarathon are available at https://open.mesosphere.\ncom/getting-started/install/.\nPerform the following steps for a minimal installation of ZooKeeper, Mesos, and \nMarathon to deploy the BrownField microservice:\n1.\t As a prerequisite, JRE 8 must be installed on all the machines. Execute the \nfollowing command:\nsudo apt-get -y install oracle-java8-installer\n2.\t Install Docker on all machines earmarked for the Mesos slave via the \nfollowing command:\nsudo apt-get install docker\n3.\t Open a terminal window and execute the following commands.  \nThese commands set up the repository for installation:\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv \nE56151BF\nDISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]')\nCODENAME=$(lsb_release -cs)\n# Add the repository\necho \"deb http://repos.mesosphere.com/${DISTRO} ${CODENAME} main\" \n| \\\n  sudo tee /etc/apt/sources.list.d/mesosphere.list\nsudo apt-get -y update\n4.\t Execute the following command to install Mesos and Marathon. This will \nalso install Zookeeper as a dependency:\nsudo apt-get -y install mesos marathon\nRepeat the preceding steps on all the three EC2 instances reserved for the Mesos \nslave execution. As the next step, ZooKeeper and Mesos have to be configured on  \nthe machine identified for the Mesos master.\nConfiguring ZooKeeper\nConnect to the machine reserved for the Mesos master and Marathon scheduler.  \nIn this case, 172.31.54.69 will be used to set up ZooKeeper, the Mesos master,  \nand Marathon.\n",
      "content_length": 1608,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 384,
      "content": "Chapter 9\n[ 357 ]\nThere are two configuration changes required in ZooKeeper, as follows:\n1.\t The first step is to set /etc/zookeeper/conf/myid to a unique integer \nbetween 1 and 255, as follows:\nOpen vi /etc/zookeeper/conf/myid and set 1. \n2.\t The next step is to edit /etc/zookeeper/conf/zoo.cfg. Update the file to \nreflect the following changes:\n# specify all zookeeper servers\n# The first port is used by followers to connect to the leader\n# The second one is used for leader election\nserver.1= 172.31.54.69:2888:3888\n#server.2=zookeeper2:2888:3888\n#server.3=zookeeper3:2888:3888\nReplace the IP addresses with the relevant private IP address. In this case,  \nwe will use only one ZooKeeper server, but in a production scenario, \nmultiple servers are required for high availability.\nConfiguring Mesos\nMake changes to the Mesos configuration to point to ZooKeeper, set up a quorum, \nand enable Docker support via the following steps:\n1.\t Edit /etc/mesos/zk to set the following value. This is to point Mesos to a \nZooKeeper instance for quorum and leader election:\nzk:// 172.31.54.69:2181/mesos \n2.\t Edit the /etc/mesos-master/quorum file and set the value as 1. In a \nproduction scenario, we may need a minimum quorum of three:\nvi /etc/mesos-master/quorum\n3.\t The default Mesos installation does not support Docker on Mesos slaves. In \norder to enable Docker, update the following mesos-slave configuration:\necho 'docker,mesos' > /etc/mesos-slave/containerizers\n",
      "content_length": 1465,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 385,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 358 ]\nRunning Mesos, Marathon, and ZooKeeper  \nas services\nAll the required configuration changes are implemented. The easiest way to start \nMesos, Marathon, and Zookeeper is to run them as services, as follows:\n•\t\nThe following commands start services. The services need to be started in  \nthe following order:\nsudo service zookeeper start\nsudo service mesos-master start\nsudo service mesos-slave start\nsudo service marathon start\n•\t\nAt any point, the following commands can be used to stop these services:\nsudo service zookeeper stop\nsudo service mesos-master stop\nsudo service mesos-slave stop\nsudo service marathon stop\n•\t\nOnce the services are up and running, use a terminal window to verify \nwhether the services are running:\n",
      "content_length": 792,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 386,
      "content": "Chapter 9\n[ 359 ]\nRunning the Mesos slave in the command line\nIn this example, instead of using the Mesos slave service, we will use a command-\nline version to invoke the Mesos slave to showcase additional input parameters. Stop \nthe Mesos slave and use the command line as mentioned here to start the slave again:\n$sudo service mesos-slave stop\n$sudo /usr/sbin/mesos-slave  --master=172.31.54.69:5050 --log_dir=/var/\nlog/mesos --work_dir=/var/lib/mesos --containerizers=mesos,docker --resou\nrces=\"ports(*):[8000-9000, 31000-32000]\"\nThe command-line parameters used are explained as follows:\n•\t\n--master=172.31.54.69:5050: This parameter is to tell the Mesos  \nslave to connect to the correct Mesos master. In this case, there is only  \none master running at 172.31.54.69:5050. All the slaves connect to  \nthe same Mesos master.\n•\t\n--containerizers=mesos,docker: This parameter is to enable support  \nfor Docker container execution as well as noncontainerized executions on  \nthe Mesos slave instances.\n•\t\n--resources=\"ports(*):[8000-9000, 31000-32000]: This parameter \nindicates that the slave can offer both ranges of ports when binding resources. \n31000 to 32000 is the default range. As we are using port numbers starting \nwith 8000, it is important to tell the Mesos slave to allow exposing ports \nstarting from 8000 as well.\nPerform the following steps to verify the installation of Mesos and Marathon:\n1.\t Execute the command mentioned in the previous step to start the Mesos slave \non all the three instances designated for the slave. The same command can be \nused across all three instances as all of them connect to the same master.\n2.\t If the Mesos slave is successfully started, a message similar to the following \nwill appear in the console:\nI0411 18:11:39.684809 16665 slave.cpp:1030] Forwarding total \noversubscribed resources\nThe preceding message indicates that the Mesos slave started sending the \ncurrent state of resource availability periodically to the Mesos master.\n",
      "content_length": 1989,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 387,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 360 ]\n3.\t Open http://54.85.107.37:8080 to inspect the Marathon UI. Replace the \nIP address with the public IP address of the EC2 instance:\nAs there are no applications deployed so far, the Applications section of the \nUI is empty.\n4.\t Open the Mesos UI, which runs on port 5050, by going to \nhttp://54.85.107.37:5050:\nThe Slaves section of the console shows that there are three activated Mesos \nslaves available for execution. It also indicates that there is no active task.\n",
      "content_length": 537,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 388,
      "content": "Chapter 9\n[ 361 ]\nPreparing BrownField PSS services\nIn the previous section, we successfully set up Mesos and Marathon. In this section, \nwe will take a look at how to deploy the BrownField PSS application previously \ndeveloped using Mesos and Marathon.\nThe full source code of this chapter is available under the Chapter \n9 project in the code files. Copy chapter8.configserver, \nchapter8.eurekaserver, chapter8.search, chapter8.\nsearch-apigateway, and chapter8.website into a new STS \nworkspace and rename them chapter9.*.\n1.\t Before we deploy any application, we have to set up the Config server, \nEureka server, and RabbitMQ in one of the servers. Follow the steps \ndescribed in the Running BrownField services on EC2 section in Chapter 8, \nContainerizing Microservices with Docker. Alternately, we can use the same \ninstance as used in the previous chapter for this purpose.\n2.\t Change all bootstrap.properties files to reflect the Config server  \nIP address.\n3.\t Before we deploy our services, there are a few specific changes required  \non the microservices. When running dockerized microservices with the \nBRIDGE mode on, we need to tell the Eureka client the hostname to be  \nused to bind. By default, Eureka uses the instance ID to register. However, \nthis is not helpful as Eureka clients won't be able to look up these services \nusing the instance ID. In the previous chapter, the HOST mode was used \ninstead of the BRIDGE mode.\nThe hostname setup can be done using the eureka.instance.hostname \nproperty. However, when running on AWS specifically, an alternate \napproach is to define a bean in the microservices to pick up AWS-specific \ninformation, as follows:\n@Configuration\nclass EurekaConfig { \n@Bean\n    public EurekaInstanceConfigBean eurekaInstanceConfigBean() {\n    EurekaInstanceConfigBean config = new \nEurekaInstanceConfigBean(new InetUtils(new \nInetUtilsProperties()));\nAmazonInfo info = AmazonInfo.Builder.newBuilder().\nautoBuild(\"eureka\");\n        config.setDataCenterInfo(info);\n",
      "content_length": 2007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 389,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 362 ]\n        info.getMetadata().put(AmazonInfo.MetaDataKey.\npublicHostname.getName(), info.get(AmazonInfo.MetaDataKey.\npublicIpv4));\n        config.setHostname(info.get(AmazonInfo.MetaDataKey.\nlocalHostname));       \nconfig.setNonSecurePortEnabled(true);\nconfig.setNonSecurePort(PORT); \nconfig.getMetadataMap().put(\"instanceId\",  info.get(AmazonInfo.\nMetaDataKey.localHostname));\nreturn config;\n}\nThe preceding code provides a custom Eureka server configuration using \nthe Amazon host information using Netflix APIs. The code overrides the \nhostname and instance ID with the private DNS. The port is read from \nthe Config server. This code also assumes one host per service so that the \nport number stays constant across multiple deployments. This can also be \noverridden by dynamically reading the port binding information at runtime.\nThe previous code has to be applied in all microservices.\n4.\t Rebuild all the microservices using Maven. Build and push the Docker \nimages to the Docker Hub. The steps for the three services are shown \nas follows. Repeat the same steps for all the other services. The working \ndirectory needs to be switched to the respective directories before executing \nthese commands:\ndocker build -t search-service:1.0 .\ndocker tag search-service:1.0 rajeshrv/search-service:1.0\ndocker push rajeshrv/search-service:1.0\ndocker build -t search-apigateway:1.0 .\ndocker tag search-apigateway:1.0 rajeshrv/search-apigateway:1.0\ndocker push rajeshrv/search-apigateway:1.0\ndocker build -t website:1.0 .\ndocker tag website:1.0 rajeshrv/website:1.0\ndocker push rajeshrv/website:1.0\n",
      "content_length": 1658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 390,
      "content": "Chapter 9\n[ 363 ]\nDeploying BrownField PSS services\nThe Docker images are now published to the Docker Hub registry. Perform the \nfollowing steps to deploy and run BrownField PSS services:\n1.\t Start the Config server, Eureka server, and RabbitMQ on its dedicated instance.\n2.\t Make sure that the Mesos server and Marathon are running on the machine \nwhere the Mesos master is configured.\n3.\t Run the Mesos slave on all the machines as described earlier using the \ncommand line.\n4.\t At this point, the Mesos Marathon cluster is up and running and is ready to \naccept deployments. The deployment can be done by creating one JSON file \nper service, as shown here:\n{\n  \"id\": \"search-service-1.0\",\n  \"cpus\": 0.5,\n  \"mem\": 256.0,\n  \"instances\": 1,\n  \"container\": {\n   \"docker\": {\n    \"type\": \"DOCKER\",\n      \"image\": \"rajeshrv/search-service:1.0\",\n       \"network\": \"BRIDGE\",\n       \"portMappings\": [\n        {  \"containerPort\": 0, \"hostPort\": 8090 }\n      ]\n    }\n  }\n}\nThe preceding JSON code will be stored in the search.json file. Similarly, \ncreate a JSON file for other services as well.\nThe JSON structure is explained as follows:\n°°\nid: This is the unique ID of the application. This can be a logical name.\n°°\ncpus and mem: This sets the resource constraints for this application. If \nthe resource offer does not satisfy this resource constraint, Marathon \nwill reject this resource offer from the Mesos master.\n°°\ninstances: This decides how many instances of this application to \nstart with. In the preceding configuration, by default, it starts one \ninstance as soon as it gets deployed. Marathon maintains the number \nof instances mentioned at any point.\n",
      "content_length": 1660,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 391,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 364 ]\n°°\ncontainer: This parameter tells the Marathon executor to use a \nDocker container for execution.\n°°\nimage: This tells the Marathon scheduler which Docker image has to \nbe used for deployment. In this case, this will download the search-\nservice:1.0 image from the Docker Hub repository rajeshrv.\n°°\nnetwork: This value is used for Docker runtime to advise on the \nnetwork mode to be used when starting the new docker container. \nThis can be BRIDGE or HOST. In this case, the BRIDGE mode will  \nbe used.\n°°\nportMappings: The port mapping provides information on how to \nmap the internal and external ports. In the preceding configuration, \nthe host port is set as 8090, which tells the Marathon executor to use \n8090 when starting the service. As the container port is set as 0, the \nsame host port will be assigned to the container. Marathon picks up \nrandom ports if the host port value is 0.\n5.\t Additional health checks are also possible with the JSON descriptor, as \nshown here:\n\"healthChecks\": [\n    {\n      \"protocol\": \"HTTP\",\n      \"portIndex\": 0,\n      \"path\": \"/admin/health\",\n      \"gracePeriodSeconds\": 100,\n      \"intervalSeconds\": 30,\n      \"maxConsecutiveFailures\": 5\n    }\n  ]\n6.\t Once this JSON code is created and saved, deploy it to Marathon using the \nMarathon REST APIs as follows:\ncurl -X POST http://54.85.107.37:8080/v2/apps -d @search.json -H \n\"Content-type: application/json\"\nRepeat this step for all the other services as well.\nThe preceding step will automatically deploy the Docker container to the \nMesos cluster and start one instance of the service.\n",
      "content_length": 1649,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 392,
      "content": "Chapter 9\n[ 365 ]\nReviewing the deployment\nThe steps for this are as follows:\n1.\t Open the Marathon UI. As shown in the following screenshot, the UI shows \nthat all the three applications are deployed and are in the Running state.  \nIt also indicates that 1 of 1 instance is in the Running state:\n2.\t Visit the Mesos UI. As shown in the following screenshot, there are three \nActive Tasks, all of them in the Running state. It also shows the host in \nwhich these services run:\n",
      "content_length": 477,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 393,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 366 ]\n3.\t In the Marathon UI, click on a running application. The following screenshot \nshows the search-apigateway-1.0 application. In the Instances tab, the IP \naddress and port in which the service is bound is indicated:\nThe Scale Application button allows administrators to specify how many \ninstances of the service are required. This can be used to scale up as well as \nscale down instances.\n4.\t Open the Eureka server console to take a look at how the services are bound. \nAs shown in the screenshot, AMIs and Availability Zones are reflected \nwhen services are registered. Follow http://52.205.251.150:8761:\n",
      "content_length": 676,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 394,
      "content": "Chapter 9\n[ 367 ]\n5.\t Open http://54.172.213.51:8001 in a browser to verify the  \nWebsite application.\nA place for the life cycle manager\nThe life cycle manager introduced in Chapter 6, Autoscaling Microservices, has the \ncapability of autoscaling up or down instances based on demand. It also has the \nability to take decisions on where to deploy and how to deploy applications on \na cluster of machines based on polices and constraints. The life cycle manager's \ncapabilities are shown in the following figure:\nMarathon has the capability to manage clusters and deployments to clusters based \non policies and constraints. The number of instances can be altered using the \nMarathon UI.\nThere are redundant capabilities between our life cycle manager and Marathon. \nWith Marathon in place, SSH work or machine-level scripting is no longer required. \nMoreover, deployment policies and constraints can be delegated to Marathon. The \nREST APIs exposed by Marathon can be used to initiate scaling functions.\nMarathon autoscale is a proof-of-concept project from Mesosphere for autoscaling. \nThe Marathon autoscale provides basic autoscale features such as the CPU, memory, \nand rate of request.\n",
      "content_length": 1191,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 395,
      "content": "Managing Dockerized Microservices with Mesos and Marathon\n[ 368 ]\nRewriting the life cycle manager with Mesos \nand Marathon\nWe still need a custom life cycle manager to collect metrics from the Spring Boot \nactuator endpoints. A custom life cycle manager is also handy if the scaling rules  \nare beyond the CPU, memory, and rate of scaling.\nThe following diagram shows the updated life cycle manager using the  \nMarathon framework:\nThe life cycle manager, in this case, collects actuator metrics from different Spring \nBoot applications, combines them with other metrics, and checks for certain \nthresholds. Based on the scaling policies, the decision engine informs the scaling \nengine to either scale down or scale up. In this case, the scaling engine is nothing \nbut a Marathon REST client. This approach is cleaner and neater than our earlier \nprimitive life cycle manager implementation using SSH and Unix scripts.\nThe technology metamodel\nWe have covered a lot of ground on microservices with the BrownField PSS \nmicroservices. The following diagram sums it up by bringing together all the \ntechnologies used into a technology metamodel:\n",
      "content_length": 1144,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 396,
      "content": "Chapter 9\n[ 369 ]\nSummary\nIn this chapter, you learned the importance of a cluster management and init system \nto efficiently manage dockerized microservices at scale.\nWe explored the different cluster control or cluster orchestration tools before diving \ndeep into Mesos and Marathon. We also implemented Mesos and Marathon in the \nAWS cloud environment to demonstrate how to manage dockerized microservices \ndeveloped for BrownField PSS.\nAt the end of this chapter, we also explored the position of the life cycle manager  \nin conjunction with Mesos and Marathon. Finally, we concluded this chapter with a \ntechnology metamodel based on the BrownField PSS microservices implementation.\nSo far, we have discussed all the core and supporting technology capabilities \nrequired for a successful microservices implementation. A successful microservice \nimplementation also requires processes and practices beyond technology. The next \nchapter, the last in the book, will cover the process and practice perspectives of \nmicroservices.\n",
      "content_length": 1031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 398,
      "content": "[ 371 ]\nThe Microservices \nDevelopment Life Cycle\nSimilar to the software development life cycle (SDLC), it is important to understand \nthe aspects of the microservice development life cycle processes for a successful \nimplementation of the microservices architecture.\nThis final chapter will focus on the development process and practice of \nmicroservices with the help of BrownField Airline's PSS microservices example. \nFurthermore, this chapter will describe best practices in structuring development \nteams, development methodologies, automated testing, and continuous delivery \nof microservices in line with DevOps practices. Finally, this chapter will conclude \nby shedding light on the importance of the reference architecture in a decentralized \ngovernance approach to microservices.\nBy the end of this chapter, you will learn about the following topics:\n•\t\nReviewing DevOps in the context of microservices development\n•\t\nDefining the microservices life cycle and related processes\n•\t\nBest practices around the development, testing, and deployment of  \nInternet-scale microservices\n",
      "content_length": 1091,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 399,
      "content": "The Microservices Development Life Cycle\n[ 372 ]\nReviewing the microservice capability \nmodel\nThis chapter will cover the following microservices capabilities from the microservices \ncapability model discussed in Chapter 3, Applying Microservices Concepts:\n•\t\nDevOps\n•\t\nDevOps Tools\n•\t\nReference Architecture & Libraries\n•\t\nTesting Tools (Anti-Fragile, RUM etc)\nThe new mantra of lean IT – DevOps\nWe discussed the definition of DevOps in Chapter 2, Building Microservices with Spring \nBoot. Here is a quick recap of the DevOps definition.\nGartner defines DevOps as follows:\n\"DevOps represents a change in IT culture, focusing on rapid IT service delivery \nthrough the adoption of agile, lean practices in the context of a system-oriented \napproach. DevOps emphasizes people (and culture), and seeks to improve \ncollaboration between operations and development teams. DevOps implementations \nutilize technology — especially automation tools that can leverage an increasingly \nprogrammable and dynamic infrastructure from a life cycle perspective.\"\n",
      "content_length": 1047,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 400,
      "content": "Chapter 10\n[ 373 ]\nDevOps and microservices evolved independently. Chapter 1, Demystifying \nMicroservices, explored the evolution of microservices. In this section, we will  \nreview the evolution of DevOps and then take a look at how DevOps supports \nmicroservices adoption.\nIn the era of digital disruption and in order to support modern business, IT \norganizations have to master two key areas: speed of delivery and value-driven \ndelivery. This is obviously apart from being expert in leading technologies.\nMany IT organizations failed to master this change, causing frustration to business \nusers. To overcome this situation, many business departments started their own \nshadow IT or stealth IT under their control. Some smart IT organizations then \nadopted a lean IT model to respond to these situations.\nHowever, many organizations still struggle with this transformation due to the large \nbaggage of legacy systems and processes. Gartner coined the concept of a pace-\nlayered application strategy. Gartner's view is that high speed is required only for \ncertain types of applications or certain business areas. Gartner termed this a system \nof innovation. A system of innovation requires rapid innovations compared to a \nsystem of records. As a system of innovations needs rapid innovation, a lean IT \ndelivery model is essential for such applications. Practitioners evangelized the lean \nIT model as DevOps.\nThere are two key strategies used by organizations to adopt DevOps.\nSome organizations positioned DevOps as a process to fill the gaps in their existing \nprocesses. Such organizations adopted an incremental strategy for their DevOps \njourney. The adoption path starts with Agile development, then incrementally \nadopts continuous integration, automated testing, and release to production and \nthen all DevOps practices. The challenge in such organizations is the time to realize \nthe full benefits as well as the mixed culture of people due to legacy processes.\nMany organizations, therefore, take a disruptive approach to adopt DevOps. This \nwill be achieved by partitioning IT into two layers or even as two different IT units. \nThe high-speed layer of IT uses DevOps-style practices to dramatically change the \nculture of the organization with no connection to the legacy processes and practices. \nA selective application cluster will be identified and moved to the new IT based on \nthe business value:\n",
      "content_length": 2422,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 401,
      "content": "The Microservices Development Life Cycle\n[ 374 ]\nThe intention of DevOps is not just to reduce cost. It also enables the business \nto disrupt competitors by quickly moving ideas to production. DevOps attacks \ntraditional IT issues in multiple ways, as explained here.\nReducing wastage\nDevOps processes and practices essentially speed up deliveries which improves \nquality. The speed of delivery is achieved by cutting IT wastage. This is achieved by \navoiding work that adds no value to the business nor to desired business outcomes. \nIT wastage includes software defects, productivity issues, process overheads, \ntime lag in decision making, time spent in reporting layers, internal governance, \noverestimation, and so on. By reducing these wastages, organizations can radically \nimprove the speed of delivery. The wastage is reduced by primarily adopting Agile \nprocesses, tools, and techniques.\nAutomating every possible step\nBy automating the manually executed tasks, one can dramatically improve the \nspeed of delivery as well as the quality of deliverables. The scope of automation \ngoes from planning to customer feedback. Automation reduces the time to move \nbusiness ideas to production. This also reduces a number of manual gate checks, \nbureaucratic decision making, and so on. Automated monitoring mechanisms and \nfeedback go back to the development factory, which gets it fixed and quickly moved \nto production.\nValue-driven delivery\nDevOps reduces the gap between IT and business through value-driven delivery. \nValue-driven delivery closely aligns IT to business by understanding true business \nvalues and helps the business by quickly delivering these values, which can give \na competitive advantage. This is similar to the shadow IT concept, in which IT is \ncollocated with the business and delivers business needs quickly, rather than  \nwaiting for heavy project investment-delivery cycles.\nTraditionally, IT is partially disconnected from the business and works with IT \nKPIs, such as the number of successful project deliveries, whereas in the new model, \nIT shares business KPIs. As an example, a new IT KPI could be that IT helped \nbusiness to achieve a 10% increase in sales orders or led to 20% increase in customer \nacquisition. This will shift IT's organizational position from merely a support \norganization to a business partner. \n",
      "content_length": 2359,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 402,
      "content": "Chapter 10\n[ 375 ]\nBridging development and operations\nTraditionally, IT has different teams for development and operations. In many cases, \nthey are differentiated with logical barriers. DevOps reduces the gap between the \ndevelopment and operations teams so that it can potentially reduce wastage and \nimprove quality. Multidisciplinary teams work together to address problems at  \nhand rather than throwing mud across the wall.\nWith DevOps, operations teams will have a fairly good understanding about the \nservices and applications developed by development teams. Similarly, development \nteams will have a good handle on the infrastructure components and configurations \nused by the applications. As a result, operations teams can make decisions based \nexactly on service behaviors rather than enforcing standard organizational policies \nand rules when designing infrastructure components. This would eventually help \nthe IT organization to improve the quality of the product as well as the time to \nresolve incidents and problem management.\nIn the DevOps world, speed of delivery is achieved through the automation of  \nhigh-velocity changes, and quality is achieved through automation and people. \nBusiness values are achieved through efficiency, speed of delivery, quality, and the \nability to innovate. Cost reduction is achieved through automation, productivity,  \nand reducing wastage.\nMeeting the trio – microservices, \nDevOps, and cloud\nThe trio—cloud, microservices, and DevOps—targets a set of common objectives: \nspeed of delivery, business value, and cost benefit. All three can stay and evolve \nindependently, but they complement each other to achieve the desired common \ngoals. Organizations embarking on any of these naturally tend to consider the other \ntwo as they are closely linked together:\n",
      "content_length": 1815,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 403,
      "content": "The Microservices Development Life Cycle\n[ 376 ]\nMany organizations start their journey with DevOps as an organizational practice \nto achieve high-velocity release cycles but eventually move to the microservices \narchitecture and cloud. It is not mandatory to have microservices and cloud support \nDevOps. However, automating the release cycles of large monolithic applications \ndoes not make much sense, and in many cases, it would be impossible to achieve. \nIn such scenarios, the microservices architecture and cloud will be handy when \nimplementing DevOps.\nIf we flip a coin, cloud does not need a microservices architecture to achieve its \nbenefits. However, to effectively implement microservices, both cloud and DevOps \nare essential.\nIn summary, if the objective of an organization is to achieve a high speed of delivery \nand quality in a cost-effective way, the trio together can bring tremendous success.\nCloud as the self-service infrastructure for \nMicroservices\nThe main driver for cloud is to improve agility and reduce cost. By reducing the time \nto provision the infrastructure, the speed of delivery can be increased. By optimally \nutilizing the infrastructure, one can bring down the cost. Therefore, cloud directly \nhelps achieve both speed of delivery and cost.\nAs discussed in Chapter 9, Managing Dockerized Microservices with Mesos and Marathon, \nwithout having a cloud infrastructure with cluster management software, it would \nbe hard to control the infrastructure cost when deploying microservices. Hence, the \ncloud with self-service capabilities is essential for microservices to achieve their full \npotential benefits. In the microservices context, the cloud not only helps abstract the \nphysical infrastructure but also provides software APIs for dynamic provisioning \nand automatic deployments. This is referred to as infrastructure as code or  \nsoftware-defined infrastructure.\nDevOps as the practice and process for \nmicroservices\nMicroservice is an architecture style that enables quick delivery. However, \nmicroservices cannot provide the desired benefits by themselves. A microservices-\nbased project with a delivery cycle of 6 months does not give the targeted speed of \ndelivery or business agility. Microservices need a set of supporting delivery practices \nand processes to effectively achieve their goal.\n",
      "content_length": 2345,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 404,
      "content": "Chapter 10\n[ 377 ]\nDevOps is the ideal candidate for the underpinning process and practices \nfor microservice delivery. DevOps processes and practices gel well with the \nmicroservices architecture's philosophies.\nPractice points for microservices \ndevelopment\nFor a successful microservice delivery, a number of development-to-delivery \npractices need to be considered, including the DevOps philosophy. In the previous \nchapters, you learned the different architecture capabilities of microservices. In this \nsection, we will explore the nonarchitectural aspects of microservice developments.\nUnderstanding business motivation and value\nMicroservices should not be used for the sake of implementing a niche architecture \nstyle. It is extremely important to understand the business value and business KPIs \nbefore selecting microservices as an architectural solution for a given problem. A \ngood understanding of business motivation and business value will help engineers \nfocus on achieving these goals in a cost-effective way.\nBusiness motivation and value should justify the selection of microservices. Also, \nusing microservices, the business value should be realizable from a business point \nof view. This will avoid situations where IT invests in microservices but there is no \nappetite from the business to leverage any of the benefits that microservices can \nbring to the table. In such cases, a microservices-based development would be an \noverhead to the enterprise.\nChanging the mindset from project to product \ndevelopment\nAs discussed in Chapter 1, Demystifying Microservices, microservices are more aligned \nto product development. Business capabilities that are delivered using microservices \nshould be treated as products. This is in line with the DevOps philosophy as well.\nThe mindset for project development and product development is different. The \nproduct team will always have a sense of ownership and take responsibility for what \nthey produce. As a result, product teams always try to improve the quality of the \nproduct. The product team is responsible not only for delivering the software but \nalso for production support and maintenance of the product.\n",
      "content_length": 2180,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 405,
      "content": "The Microservices Development Life Cycle\n[ 378 ]\nProduct teams are generally linked directly to a business department for which \nthey are developing the product. In general, product teams have both an IT and a \nbusiness representative. As a result, product thinking is closely aligned with actual \nbusiness goals. At every moment, product teams understand the value they are \nadding to the business to achieve business goals. The success of the product directly \nlies with the business value being gained out of the product.\nBecause of the high-velocity release cycles, product teams always get a sense of \nsatisfaction in their delivery, and they always try to improve on it. This brings  \na lot more positive dynamics within the team.\nIn many cases, typical product teams are funded for the long term and remain intact. \nAs a result, product teams become more cohesive in nature. As they are small in size, \nsuch teams focus on improving their process from their day-to-day learnings.\nOne common pitfall in product development is that IT people represent the business \nin the product team. These IT representatives may not fully understand the business \nvision. Also, they may not be empowered to take decisions on behalf of the business. \nSuch cases can result in a misalignment with the business and lead to failure  \nquite rapidly.\nIt is also important to consider a collocation of teams where business and IT \nrepresentatives reside at the same place. Collocation adds more binding between  \nIT and business teams and reduces communication overheads.\nChoosing a development philosophy\nDifferent organizations take different approaches to developing microservices, be it a \nmigration or a new development. It is important to choose an approach that suits the \norganization. There is a wide verity of approaches available, out of which a few are \nexplained in this section.\nDesign thinking\nDesign thinking is an approach primarily used for innovation-centric development. \nIt is an approach that explores the system from an end user point of view: what the \ncustomers see and how they experience the solution. A story is then built based on \nobservations, patterns, intuition, and interviews.\nDesign thinking then quickly devises solutions through solution-focused thinking \nby employing a number of theories, logical reasoning, and assumptions around the \nproblem. The concepts are expanded through brainstorming before arriving at a \nconverged solution.\n",
      "content_length": 2460,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 406,
      "content": "Chapter 10\n[ 379 ]\nOnce the solution is identified, a quick prototype is built to consider how the \ncustomer responds to it, and then the solution is adjusted accordingly. When the \nteam gets satisfactory results, the next step is taken to scale the product. Note that \nthe prototype may or may not be in the form of code.\nDesign thinking uses human-centric thinking with feelings, empathy, intuition, and \nimagination at its core. In this approach, solutions will be up for rethinking even for \nknown problems to find innovative and better solutions.\nThe start-up model\nMore and more organizations are following the start-up philosophy to deliver \nsolutions. Organizations create internal start-up teams with the mission to deliver \nspecific solutions. Such teams stay away from day-to-day organizational activities \nand focus on delivering their mission.\nMany start-ups kick off with a small, focused team—a highly cohesive unit. The unit \nis not worried about how they achieve things; rather, the focus is on what they want \nto achieve. Once they have a product in place, the team thinks about the right way  \nto build and scale it.\nThis approach addresses quick delivery through production-first thinking. The \nadvantage with this approach is that teams are not disturbed by organizational \ngovernance and political challenges. The team is empowered to think out of the box, \nbe innovative, and deliver things. Generally, a higher level of ownership is seen in \nsuch teams, which is one of the key catalysts for success. Such teams employ just \nenough processes and disciplines to take the solution forward. They also follow  \na fail fast approach and course correct sooner than later.\nThe Agile practice\nThe most commonly used approach is the Agile methodology for development. \nIn this approach, software is delivered in an incremental, iterative way using the \nprinciples put forth in the Agile manifesto. This type of development uses an Agile \nmethod such as Scrum or XP. The Agile manifesto defines four key points that Agile \nsoftware development teams should focus on:\n•\t\nIndividuals and interaction over processes and tools\n•\t\nWorking software over comprehensive documentation\n•\t\nCustomer collaboration over contract negotiation\n•\t\nResponding to change over following a plan\n",
      "content_length": 2288,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 407,
      "content": "The Microservices Development Life Cycle\n[ 380 ]\nThe 12 principles of Agile software development can be found at \nhttp://www.agilemanifesto.org/principles.html.\nUsing the concept of Minimum Viable Product\nIrrespective of the development philosophy explained earlier, it is essential  \nto identify a Minimum Viable Product (MVP) when developing microservice \nsystems for speed and agility.\nEric Ries, while pioneering the lean start-up movement, defined MVP as:\n\"A Minimum Viable Product is that version of a new product which allows a team \nto collect the maximum amount of validated learning about customers with the \nleast effort.\"\nThe objective of the MVP approach is to quickly build a piece of software that \nshowcases the most important aspects of the software. The MVP approach realizes \nthe core concept of an idea and perhaps chooses those features that add maximum \nvalue to the business. It helps get early feedback and then course corrects as \nnecessary before building a heavy product.\nThe MVP may be a full-fledged service addressing limited user groups or partial \nservices addressing wider user groups. Feedback from customers is extremely \nimportant in the MVP approach. Therefore, it is important to release the MVP  \nto the real users.\nOvercoming the legacy hotspot\nIt is important to understand the environmental and political challenges in an \norganization before embarking on microservices development.\nIt is common in microservices to have dependencies on other legacy applications, \ndirectly or indirectly. A common issue with direct legacy integration is the slow \ndevelopment cycle of the legacy application. An example would be an innovative \nrailway reservation system relaying on an age-old transaction processing facility \n(TPF) for some of the core backend features, such as reservation. This is especially \ncommon when migrating legacy monolithic applications to microservices. In many \ncases, legacy systems continue to undergo development in a non-Agile way with \nlarger release cycles. In such cases, microservices development teams may not be \nable to move so quickly because of the coupling with legacy systems. Integration \npoints might drag the microservices developments heavily. Organizational political \nchallenges make things even worse.\n",
      "content_length": 2281,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 408,
      "content": "Chapter 10\n[ 381 ]\nThere is no silver bullet to solve this issue. The cultural and process differences could \nbe an ongoing issue. Many enterprises ring-fence such legacy systems with focused \nattention and investments to support fast-moving microservices. Targeted C-level \ninterventions on these legacy platforms could reduce the overheads.\nAddressing challenges around databases\nAutomation is key in microservices development. Automating databases is one  \nof the key challenges in many microservice developments.\nIn many organizations, DBAs play a critical role in database management, and they \nlike to treat the databases under their control differently. Confidentiality and access \ncontrol on data is also cited as a reason for DBAs to centrally manage all data.\nMany automation tools focus on the application logic. As a result, many development \nteams completely ignore database automation. Ignoring database automation can \nseverely impact the overall benefits and can derail microservices development.\nIn order to avoid such situations, the database has to be treated in the same way \nas applications with appropriate source controls and change management. When \nselecting a database, it is also important to consider automation as one of the  \nkey aspects.\nDatabase automation is much easier in the case of NoSQL databases but is hard \nto manage with traditional RDBMs. Database Lifecycle Management (DLM) as a \nconcept is popular in the DevOps world, particularly to handle database automation. \nTools such as DBmaestro, Redgate DLM, Datical DB, and Delphix support database \nautomation.\nEstablishing self-organizing teams\nOne of the most important activities in microservices development is to establish the \nright teams for development. As recommended in many DevOps processes, a small, \nfocused team always delivers the best results.\n",
      "content_length": 1850,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 409,
      "content": "The Microservices Development Life Cycle\n[ 382 ]\nAs microservices are aligned with business capabilities and are fairly loosely coupled \nproducts, it is ideal to have a dedicated team per microservice. There could be cases \nwhere the same team owns multiple microservices from the same business area \nrepresenting related capabilities. These are generally decided by the coupling and \nsize of the microservices.\nTeam size is an important aspect in setting up effective teams for microservices \ndevelopment. The general notion is that the team size should not exceed 10 people. \nThe recommended size for optimal delivery is between 4 and 7. The founder of \nAmazon.com, Jeff Bezos, coined the theory of two-pizza teams. Jeff's theory says the \nteam will face communication issues if the size gets bigger. Larger teams work with \nconsensus, which results in increased wastage. Large teams also lose ownership  \nand accountability. A yardstick is that the product owner should get enough time  \nto speak to individuals in the team to make them understand the value of what they \nare delivering.\nTeams are expected to take full ownership in ideating for, analyzing, developing, \nand supporting services. Werner Vogels from Amazon.com calls this you build it  \nand you run it. As per Werner's theory, developers pay more attention to develop \nquality code to avoid unexpected support calls. The members in the team consist  \nof fullstack developers and operational engineers. Such a team is fully aware of all \nthe areas. Developers understand operations as well as operations teams understand \napplications. This not only reduces the changes of throwing mud across teams but \nalso improves quality.\nTeams should have multidisciplinary skills to satisfy all the capabilities required to \ndeliver a service. Ideally, the team should not rely on external teams to deliver the \ncomponents of the service. Instead, the team should be self-sufficient. However, in \nmost organizations, the challenge is on specialized skills that are rare. For example, \nthere may not be many experts on a graph database in the organization. One common \nsolution to this problem is to use the concept of consultants. Consultants are SMEs \nand are engaged to gain expertise on specific problems faced by the team. Some \norganizations also use shared or platform teams to deliver some common capabilities.\nTeam members should have a complete understanding of the products, not only \nfrom the technical standpoint but also from the business case and the business KPIs. \nThe team should have collective ownership in delivering the product as well as in \nachieving business goals together.\n",
      "content_length": 2656,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 410,
      "content": "Chapter 10\n[ 383 ]\nAgile software development also encourages having self-organizing teams. Self-\norganizing teams act as a cohesive unit and find ways to achieve their goals as a \nteam. The team automatically align themselves and distribute the responsibilities. \nThe members in the team are self-managed and empowered to make decisions in \ntheir day-to-day work. The team's communication and transparency are extremely \nimportant in such teams. This emphasizes the need for collocation and collaboration, \nwith a high bandwidth for communication:\nIn the preceding diagram, both Microservice A and Microservice B represent related \nbusiness capabilities. Self-organizing teams treat everyone in the team equally, \nwithout too many hierarchies and management overheads within the team. The \nmanagement would be thin in such cases. There won't be many designated vertical \nskills in the team, such as team lead, UX manager, development manager, testing \nmanager, and so on. In a typical microservice development, a shared product \nmanager, shared architect, and a shared people manager are good enough to manage \nthe different microservice teams. In some organizations, architects also take up \nresponsibility for delivery.\nSelf-organizing teams have some level of autonomy and are empowered to take \ndecisions in a quick and Agile mode rather than having to wait for long-running \nbureaucratic decision-making processes that exist in many enterprises. In many \nof these cases, enterprise architecture and security are seen as an afterthought. \nHowever, it is important to have them on board from the beginning. While \nempowering the teams with maximum freedom for developers in decision-making \ncapabilities, it is equally important to have fully automated QA and compliance so  \nas to ensure that deviations are captured at the earliest.\n",
      "content_length": 1839,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 411,
      "content": "The Microservices Development Life Cycle\n[ 384 ]\nCommunication between teams is important. However, in an ideal world, it should \nbe limited to interfaces between microservices. Integrations between teams ideally \nhas to be handled through consumer-driven contracts in the form of test scripts \nrather than having large interface documents describing various scenarios. Teams \nshould use mock service implementations when the services are not available.\nBuilding a self-service cloud\nOne of the key aspects that one should consider before embarking on microservices \nis to build a cloud environment. When there are only a few services, it is easy  \nto manage them by manually assigning them to a certain predesignated set of  \nvirtual machines.\nHowever, what microservice developers need is more than just an IaaS cloud \nplatform. Neither the developers nor the operations engineers in the team should \nworry about where the application is deployed and how optimally it is deployed. \nThey also should not worry about how the capacity is managed.\nThis level of sophistication requires a cloud platform with self-service capabilities, \nsuch as what we discussed in Chapter 9, Managing Dockerized Microservices with \nMesos and Marathon, with the Mesos and Marathon cluster management solutions. \nContainerized deployment discussed in Chapter 8, Containerizing Microservices with \nDocker, is also important in managing and end to-end-automation. Building this  \nself-service cloud ecosystem is a prerequisite for microservice development.\nBuilding a microservices ecosystem\nAs we discussed in the capability model in Chapter 3, Applying Microservices Concepts, \nmicroservices require a number of other capabilities. All these capabilities should be \nin place before implementing microservices at scale.\nThese capabilities include service registration, discovery, API gateways, and an \nexternalized configuration service. All are provided by the Spring Cloud project. \nCapabilities such as centralized logging, monitoring, and so on are also required  \nas a prerequisite for microservices development.\n",
      "content_length": 2097,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 412,
      "content": "Chapter 10\n[ 385 ]\nDefining a DevOps-style microservice life \ncycle process\nDevOps is the best-suited practice for microservices development. Organizations \nalready practicing DevOps do not need another practice for microservices \ndevelopment.\nIn this section, we will explore the life cycle of microservices development. Rather \nthan reinventing a process for microservices, we will explore DevOps processes  \nand practices from the microservice perspective.\nBefore we explore DevOps processes, let's iron out some of the common \nterminologies used in the DevOps world:\n•\t\nContinuous integration (CI): This automates the application build and \nquality checks continuously in a designated environment, either in a  \ntime-triggered manner or on developer commits. CI also publishes code \nmetrics to a central dashboard as well as binary artifacts to a central \nrepository. CI is popular in Agile development practices.\n•\t\nContinuous delivery (CD): This automates the end-to-end software delivery \npractice from idea to production. In a non-DevOps model, this used to be \nknown as Application Lifecycle Management (ALM). One of the common \ninterpretations of CD is that it is the next evolution of CI, which adds QA \ncycles into the integration pipeline and makes the software ready to release \nto production. A manual action is required to move it to production.\n•\t\nContinuous deployment: This is an approach to automating the deployment \nof application binaries to one or more environments by managing binary \nmovement and associated configuration parameters. Continuous deployment \nis also considered as the next evolution of CD by integrating automatic \nrelease processes into the CD pipeline.\n•\t\nApplication Release Automation (ARA): ARA tools help monitor and \nmanage end-to-end delivery pipelines. ARA tools use CI and CD tools and \nmanage the additional steps of release management approvals. ARA tools \nare also capable of rolling out releases to different environments and rolling \nthem back in case of a failed deployment. ARA provides a fully orchestrated \nworkflow pipeline, implementing delivery life cycles by integrating many \nspecialized tools for repository management, quality assurance, deployment, \nand so on. XL Deploy and Automic are some of the ARA tools.\n",
      "content_length": 2278,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 413,
      "content": "The Microservices Development Life Cycle\n[ 386 ]\nThe following diagram shows the DevOps process for microservices development:\nLet's now further explore these life cycle stages of microservices development.\nValue-driven planning\nValue-driven planning is a term used in Agile development practices. Value-driven \nplanning is extremely important in microservices development. In value-driven \nplanning, we will identify which microservices to develop. The most important \naspect is to identify those requirements that have the highest value to business and \nthose that have the lowest risks. The MVP philosophy is used when developing \nmicroservices from the ground up. In the case of monolithic to microservices \nmigration, we will use the guidelines provided in Chapter 3, Applying Microservices \nConcepts, to identify which services have to be taken first. The selected microservices \nare expected to precisely deliver the expected value to the business. Business KPIs  \nto measure this value have to be identified as part of value-driven planning.\nAgile development\nOnce the microservices are identified, development must be carried out in an Agile \napproach following the Agile manifesto principles. The scrum methodology is used \nby most of the organizations for microservices development.\nContinuous integration\nThe continuous integration steps should be in place to automatically build the source \ncode produced by various team members and generate binaries. It is important to \nbuild only once and then move the binary across the subsequent phases. Continuous \nintegration also executes various QAs as part of the build pipeline, such as code \ncoverage, security checks, design guidelines, and unit test cases. CI typically delivers \nbinary artefacts to a binary artefact repository and also deploys the binary artefacts \ninto one or more environments. Part of the functional testing also happens as part  \nof CI.\n",
      "content_length": 1921,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 414,
      "content": "Chapter 10\n[ 387 ]\nContinuous testing\nOnce continuous integration generates the binaries, they are moved to the testing \nphase. A fully automated testing cycle is kicked off in this phase. It is also important \nto automate security testing as part of the testing phase. Automated testing \nhelps improve the quality of deliverables. The testing may happen in multiple \nenvironments based on the type of testing. This could range from the integration  \ntest environment to the production environment to test in production.\nContinuous release\nContinuous release to production takes care of actual deployment, infrastructure \nprovisioning, and rollout. The binaries are automatically shipped and deployed to \nproduction by applying certain rules. Many organizations stop automation with the \nstaging environment and make use of manual approval steps to move to production.\nContinuous monitoring and feedback\nThe continuous monitoring and feedback phase is the most important phase in Agile \nmicroservices development. In an MVP scenario, this phase gives feedback on the \ninitial acceptance of the MVP and also evaluates the value of the service developed. \nIn a feature addition scenario, this further gives insight into how this new feature \nis accepted by users. Based on the feedback, the services are adjusted and the same \ncycle is then repeated.\nAutomating the continuous delivery pipeline\nIn the previous section, we discussed the life cycle of microservices development. \nThe life cycle stages can be altered by organizations based on their organizational \nneeds but also based on the nature of the application. In this section, we will take \na look at a sample continuous delivery pipeline as well as toolsets to implement a \nsample pipeline.\nThere are many tools available to build end-to-end pipelines, both in the open  \nsource and commercial space. Organizations can select the products of their  \nchoice to connect pipeline tasks.\nRefer to the XebiaLabs periodic table for a tool reference to build \ncontinuous delivery pipelines. It is available at https://xebialabs.\ncom/periodic-table-of-devops-tools/.\n",
      "content_length": 2117,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 415,
      "content": "The Microservices Development Life Cycle\n[ 388 ]\nThe pipelines may initially be expensive to set up as they require many toolsets \nand environments. Organizations may not realize an immediate cost benefit in \nimplementing the delivery pipeline. Also, building a pipeline needs high-power \nresources. Large build pipelines may involve hundreds of machines. It also takes \nhours to move changes through the pipeline from one end to the other. Hence, it is \nimportant to have different pipelines for different microservices. This will also help \ndecoupling between the releases of different microservices.\nWithin a pipeline, parallelism should be employed to execute tests on different \nenvironments. It is also important to parallelize the execution of test cases as much \nas possible. Hence, designing the pipeline based on the nature of the application is \nimportant. There is no one size fits all scenario.\nThe key focus in the pipeline is on end-to-end automation, from development  \nto production, and on failing fast if something goes wrong.\nThe following pipeline is an indicative one for microservices and explores the \ndifferent capabilities that one should consider when developing a microservices \npipeline:\nThe continuous delivery pipeline stages are explained in the following sections.\n",
      "content_length": 1298,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 416,
      "content": "Chapter 10\n[ 389 ]\nDevelopment\nThe development stage has the following activities from a development perspective. \nThis section also indicates some of the tools that can be used in the development \nstage. These tools are in addition to the planning, tracking, and communication tools \nsuch as Agile JIRA, Slack, and others used by Agile development teams. Take a look \nat the following:\n•\t\nSource code: The development team requires an IDE or a development \nenvironment to cut source code. In most organizations, developers get the \nfreedom to choose the IDEs they want. Having said this, the IDEs can be \nintegrated with a number of tools to detect violations against guidelines. \nGenerally, Eclipse IDEs have plugins for static code analysis and code \nmatrices. SonarQube is one example that integrates other plugins such as \nCheckstyle for code conventions, PMD to detect bad practices, FindBugs \nto detect potential bugs, and Cobertura for code coverage. It is also \nrecommended to use Eclipse plugins such as ESVD, Find Security Bugs, \nSonarQube Security Rules, and so on to detect security vulnerabilities.\n•\t\nUnit test cases: The development team also produces unit test cases \nusing JUnit, NUnit, TestNG, and so on. Unit test cases are written against \ncomponents, repositories, services, and so on. These unit test cases are \nintegrated with the local Maven builds. The unit test cases targeting the \nmicroservice endpoints (service tests) serve as the regression test pack.  \nWeb UI, if written in AngularJS, can be tested using Karma.\n•\t\nConsumer-driven contracts: Developers also write CDCs to test integration \npoints with other microservices. Contract test cases are generally written \nas JUnit, NUnit, TestNG, and so on and are added to the service tests pack \nmentioned in the earlier steps.\n•\t\nMock testing: Developers also write mocks to simulate the integration \nendpoints to execute unit test cases. Mockito, PowerMock, and others are \ngenerally used for mock testing. It is good practice to deploy a mock service \nbased on the contract as soon as the service contract is identified. This acts as \na simple mechanism for service virtualization for the subsequent phases.\n•\t\nBehavior driven design (BDD): The Agile team also writes BDD scenarios \nusing a BDD tool, such as Cucumber. Typically, these scenarios are targeted \nagainst the microservices contract or the user interface that is exposed by a \nmicroservice-based web application. Cucumber with JUnit and Cucumber \nwith Selenium WebDriver, respectively, are used in these scenarios. Different \nscenarios are used for functional testing, user journey testing, as well as \nacceptance testing.\n",
      "content_length": 2668,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 417,
      "content": "The Microservices Development Life Cycle\n[ 390 ]\n•\t\nSource code repository: A source control repository is a part and parcel of \ndevelopment. Developers check-in their code to a central repository, mostly \nwith the help of IDE plugins. One microservice per repository is a common \npattern used by many organizations. This disallows other microservice \ndevelopers from modifying other microservices or writing code based on  \nthe internal representations of other microservices. Git and Subversion are \nthe popular choices to be used as source code repositories.\n•\t\nBuild tools: A build tool such as Maven or Gradle is used to manage \ndependencies and build target artifacts—in this case, Spring Boot services. \nThere are many cases, such as basic quality checks, security checks and unit \ntest cases, code coverage, and so on, that are integrated as part of the build \nitself. These are similar to the IDE, especially when IDEs are not used by \ndevelopers. The tools that we examined as part of the IDEs are also available \nas Maven plugins. The development team does not use containers such as \nDocker until the CI phase of the project. All the artifacts have to be versioned \nproperly for every change.\n•\t\nArtifact repository: The artifact repository plays a pivotal role in the \ndevelopment process. The artifact repository is where all build artifacts  \nare stored. The artifact repository could be Artifactory, Nexus, or any  \nsimilar product.\n•\t\nDatabase schemas: Liquibase and Flyway are commonly used to manage, \ntrack, and apply database changes. Maven plugins allow interaction with \nthe Liquibase or Flyway libraries. The schema changes are versioned and \nmaintained, just like source code.\nContinuous integration\nOnce the code is committed to the repository, the next phase, continuous integration, \nautomatically starts. This is done by configuring a CI pipeline. This phase builds the \nsource code with a repository snapshot and generates deployable artifacts. Different \norganizations use different events to kickstart the build. A CI start event may be on \nevery developer commit or may be based on a time window, such as daily, weekly, \nand so on.\nThe CI workflow is the key aspect of this phase. Continuous integration tools such as \nJenkins, Bamboo, and others play the central role of orchestrating the build pipeline. \nThe tool is configured with a workflow of activities to be invoked. The workflow \nautomatically executes configured steps such as build, deploy, and QA. On the \ndeveloper commit or on a set frequency, the CI kickstarts the workflow.\n",
      "content_length": 2573,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 418,
      "content": "Chapter 10\n[ 391 ]\nThe following activities take place in a continuous integration workflow:\n1.\t Build and QA: The workflow listens to Git webhooks for commits. Once it \ndetects a change, the first activity is to download the source code from the \nrepository. A build is executed on the downloaded snapshot source code. \nAs part of the build, a number of QA checks are automatically performed, \nsimilarly to QA executed in the development environment. These include \ncode quality checks, security checks, and code coverage. Many of the QAs \nare done with tools such as SonarQube, with the plugins mentioned earlier. It \nalso collects code metrics such as code coverage and more and publishes it to \na central database for analysis. Additional security checks are executed using \nOWASP ZAP Jenkins' plugins. As part of the build, it also executes JUnit or \nsimilar tools used to write test cases. If the web application supports Karma \nfor UI testing, Jenkins is also capable of running web tests written in Karma. \nIf the build or QA fails, it sends out alarms as configured in the system.\n2.\t Packaging: Once build and QA are passed, the CI creates a deployable \npackage. In our microservices case, it generates the Spring Boot standalone \nJAR. It is recommended to build Docker images as part of the integration \nbuild. This is the one and only place where we build binary artifacts. Once \nthe build is complete, it pushes the immutable Docker images to a Docker \nregistry. This could be on Docker Hub or a private Docker registry. It is \nimportant to properly version control the containers at this stage itself.\n3.\t Integration tests: The Docker image is moved to the integration environment \nwhere regression tests (service tests) and the like are executed. This \nenvironment has other dependent microservices capabilities, such as \nSpring Cloud, logging, and so on, in place. All dependent microservices \nare also present in this environment. If an actual dependent service is not \nyet deployed, service virtualization tools such as MockServer are used. \nAlternately, a base version of the service is pushed to Git by the respective \ndevelopment teams. Once successfully deployed, Jenkins triggers service tests \n(JUnits against services), a set of end-to-end sanity tests written in Selenium \nWebDriver (in the case of web) and security tests with OWASP ZAP.\nAutomated testing\nThere are many types of testing to be executed as part of the automated delivery \nprocess before declaring the build ready for production. The testing may happen \nby moving the application across multiple environments. Each environment is \ndesignated for a particular kind of testing, such as acceptance testing, performance \ntesting, and so on. These environments are adequately monitored to gather the \nrespective metrics.\n",
      "content_length": 2809,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 419,
      "content": "The Microservices Development Life Cycle\n[ 392 ]\nIn a complex microservices environment, testing should not be seen as a last-minute \ngate check; rather, testing should be considered as a way to improve software quality \nas well as to avoid last-minute failures. Shift left testing is an approach of shifting \ntests as early as possible in the release cycle. Automated testing turns software \ndevelopment to every-day development and every-day testing mode. By automating \ntest cases, we will avoid manual errors as well as the effort required to complete \ntesting.\nCI or ARA tools are used to move Docker images across multiple test environments. \nOnce deployed in an environment, test cases are executed based on the purpose \nof the environment. By default, a set of sanity tests are executed to verify the test \nenvironment.\nIn this section, we will cover all the types of tests that are required in the automated \ndelivery pipeline, irrespective of the environment. We have already considered  \nsome types of tests as part of the development and integration environment. Later  \nin this section, we will also map test cases against the environments in which they \nare executed.\nDifferent candidate tests for automation\nIn this section, we will explore different types of tests that are candidates for \nautomation when designing an end-to-end delivery pipeline. The key testing  \ntypes are described as follows.\nAutomated sanity tests\nWhen moving from one environment to another, it is advisable to run a few \nsanity tests to make sure that all the basic things are working. This is created as \na test pack using JUnit service tests, Selenium WebDriver, or a similar tool. It is \nimportant to carefully identify and script all the critical service calls. Especially if the \nmicroservices are integrated using synchronous dependencies, it is better to consider \nthese scenarios to ensure that all dependent services are also up and running.\nRegression testing\nRegression tests ensure that changes in software don't break the system. In a \nmicroservices context, the regression tests could be at the service level (Rest API or \nmessage endpoints) and written using JUnit or a similar framework, as explained \nearlier. Service virtualizations are used when dependent services are not available. \nKarma and Jasmine can be used for web UI testing.\n",
      "content_length": 2346,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 420,
      "content": "Chapter 10\n[ 393 ]\nIn cases where microservices are used behind web applications, Selenium WebDriver \nor a similar tool is used to prepare regression test packs, and tests are conducted at \nthe UI level rather than focusing on the service endpoints. Alternatively, BDD tools, \nsuch as Cucumber with JUnit or Cucumber with Selenium WebDriver, can also be \nused to prepare regression test packs. CI tools such as Jenkins or ARA are used  \nto automatically trigger regression test packs. There are other commercial tools,  \nsuch as TestComplete, that can also be used to build regression test packs.\nAutomated functional testing\nFunctional test cases are generally targeted at the UIs that consume the \nmicroservices. These are business scenarios based on user stories or features.  \nThese functional tests are executed on every build to ensure that the microservice  \nis performing as expected.\nBDD is generally used in developing functional test cases. Typically in BDD, \nbusiness analysts write test cases in a domain-specific language but in plain English. \nDevelopers then add scripts to execute these scenarios. Automated web testing tools \nsuch as Selenium WebDriver are useful in such scenarios, together with BDD tools \nsuch as Cucumber, JBehave, SpecFlow, and so on. JUnit test cases are used in the \ncase of headless microservices. There are pipelines that combine both regression \ntesting and functional testing as one step with the same set of test cases.\nAutomated acceptance testing\nThis is much similar to the preceding functional test cases. In many cases, automated \nacceptance tests generally use the screenplay or journey pattern and are applied at \nthe web application level. The customer perspective is used in building the test cases \nrather than features or functions. These tests mimic user flows.\nBDD tools such as Cucumber, JBehave, and SpecFlow are generally used in these \nscenarios together with JUnit or Selenium WebDriver, as discussed in the previous \nscenario. The nature of the test cases is different in functional testing and acceptance \ntesting. Automation of acceptance test packs is achieved by integrating them \nwith Jenkins. There are many other specialized automatic acceptance testing tools \navailable on the market. FitNesse is one such tool.\n",
      "content_length": 2285,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 421,
      "content": "The Microservices Development Life Cycle\n[ 394 ]\nPerformance testing\nIt is important to automate performance testing as part of the delivery pipeline. This \npositions performance testing from a gate check model to an integral part of the \ndelivery pipeline. By doing so, bottlenecks can be identified at very early stages of \nbuild cycles. In some organizations, performance tests are conducted only for major \nreleases, but in others, performance tests are part of the pipeline. There are multiple \noptions for performance testing. Tools such as JMeter, Gatling, Grinder, and so on \ncan be used for load testing. These tools can be integrated into the Jenkins workflow \nfor automation. Tools such as BlazeMeter can then be used for test reporting.\nApplication Performance Management tools such as AppDynamics, New Relic, \nDynatrace, and so on provide quality metrics as part of the delivery pipeline. This \ncan be done using these tools as part of the performance testing environment. In \nsome pipelines, these are integrated into the functional testing environment to get \nbetter coverage. Jenkins has plugins in to fetch measurements.\nReal user flow simulation or journey testing\nThis is another form of test typically used in staging and production environments. \nThese tests continuously run in staging and production environments to ensure \nthat all the critical transactions perform as expected. This is much more useful \nthan a typical URL ping monitoring mechanism. Generally, similar to automated \nacceptance testing, these test cases simulate user journeys as they happen in the real \nworld. These are also useful to check whether the dependent microservices are up \nand running. These test cases could be a carved-out subset of acceptance test cases  \nor test packs created using Selenium WebDriver.\nAutomated security testing\nIt is extremely important to make sure that the automation does not violate the \nsecurity policies of the organization. Security is the most important thing, and \ncompromising security for speed is not desirable. Hence, it is important to integrate \nsecurity testing as part of the delivery pipeline. Some security evaluations are \nalready integrated in the local build environment as well as in the integration \nenvironment, such as SonarQube, Find Security Bugs, and so on. Some security \naspects are covered as part of the functional test cases. Tools such as BDD-Security, \nMittn, and Gauntlt are other security test automation tools following the BDD \napproach. VAPT can be done using tools such as ImmuniWeb. OWASP ZAP and \nBurp Suite are other useful tools in security testing.\n",
      "content_length": 2624,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 422,
      "content": "Chapter 10\n[ 395 ]\nExploratory testing\nExploratory testing is a manual testing approach taken by testers or business users \nto validate the specific scenarios that they think automated tools may not capture. \nTesters interact with the system in any manner they want without prejudgment. \nThey use their intellect to identify the scenarios that they think some special users \nmay explore. They also do exploratory testing by simulating certain user behavior.\nA/B testing, canary testing, and blue-green deployments\nWhen moving applications to production, A/B testing, blue-green deployments,  \nand canary testing are generally applied. A/B testing is primarily used to review  \nthe effectiveness of a change and how the market reacts to the change. New features \nare rolled out to a certain set of users. Canary release is moving a new product or \nfeature to a certain community before fully rolling out to all customers. Blue-green is \na deployment strategy from an IT point of view to test the new version of a service. \nIn this model, both blue and green versions are up and running at some point of  \ntime and then gracefully migrate from one to the other.\nOther nonfunctional tests\nHigh availability and antifragility testing (failure injection tests) are also important \nto execute before production. This helps developers unearth unknown errors that \nmay occur in a real production scenario. This is generally done by breaking the \ncomponents of the system to understand their failover behavior. This is also helpful \nto test circuit breakers and fallback services in the system. Tools such as Simian \nArmy are useful in these scenarios.\nTesting in production\nTesting in Production (TiP) is as important as all the other environments as we can \nonly simulate to a certain extend. There are two types of tests generally executed \nagainst production. The first approach is running real user flows or journey tests in a \ncontinuous manner, simulating various user actions. This is automated using one of \nthe Real User Monitoring (RUM) tools, such as AppDynamics. The second approach \nis to wiretap messages from production, execute them in a staging environment, and \nthen compare the results in production with those in the staging environment.\n",
      "content_length": 2250,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 423,
      "content": "The Microservices Development Life Cycle\n[ 396 ]\nAntifragility testing\nAntifragility testing is generally conducted in a preproduction environment identical \nto production or even in the production environment by creating chaos in the \nenvironment to take a look at how the application responds and recovers from these \nsituations. Over a period of time, the application gains the ability to automatically \nrecover from most of these failures. Simian Army is one such tool from Netflix. \nSimian Army is a suite of products built for the AWS environment. Simian Army is \nfor disruptive testing using a set of autonomous monkeys that can create chaos in  \nthe preproduction or production environments. Chaos Monkey, Janitor Monkey,  \nand Conformity Monkey are some of the components of Simian Army.\nTarget test environments\nThe different test environments and the types of tests targeted on these environments \nfor execution are as follows:\n•\t\nDevelopment environment: The development environment is used to test \nthe coding style checks, bad practices, potential bugs, unit tests, and basic \nsecurity scanning.\n•\t\nIntegration test environment: Integration environment is used for unit \ntesting and regression tests that span across multiple microservices.  \nSome basic security-related tests are also executed in the integration  \ntest environment.\n•\t\nPerformance and diagnostics: Performance tests are executed in the \nperformance test environment. Application performance testing tools  \nare deployed in these environments to collect performance metrics and \nidentify bottlenecks.\n•\t\nFunctional test environment: The functional test environment is used to \nexecute a sanity test and functional test packs.\n•\t\nUAT environment: The UAT environment has sanity tests, automated \nacceptance test packs, and user journey simulations.\n•\t\nStaging: The preproduction environment is used primarily for sanity tests, \nsecurity, antifragility, network tests, and so on. It is also used for user journey \nsimulations and exploratory testing.\n•\t\nProduction: User journey simulations and RUM tests are continuously \nexecuted in the production environment.\nMaking proper data available across multiple environments to support test cases is \nthe biggest challenge. Delphix is a useful tool to consider when dealing with test data \nacross multiple environments in an effective way.\n",
      "content_length": 2365,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 424,
      "content": "Chapter 10\n[ 397 ]\nContinuous deployment\nContinuous deployment is the process of deploying applications to one or more \nenvironments and configuring and provisioning these environments accordingly. As \ndiscussed in Chapter 9, Managing Dockerized Microservices with Mesos and Marathon, \ninfrastructure provisioning and automation tools facilitate deployment automation.\nFrom the deployment perspective, the released Docker images are moved to \nproduction automatically once all the quality checks are successfully completed. \nThe production environment, in this case, has to be cloud based with a cluster \nmanagement tool such as Mesos or Marathon. A self-service cloud environment  \nwith monitoring capabilities is mandatory.\nCluster management and application deployment tools ensure that application \ndependencies are properly deployed. This automatically deploys all the \ndependencies that are required in case any are missing. It also ensures that a \nminimum number of instances are running at any point in time. In case of failure, it \nautomatically rolls back the deployments. It also takes care of rolling back upgrades \nin a graceful manner.\nAnsible, Chef, or Puppet are tools useful in moving configurations and binaries to \nproduction. The Ansible playbook concepts can be used to launch a Mesos cluster \nwith Marathon and Docker support.\nMonitoring and feedback\nOnce an application is deployed in production, monitoring tools continuously \nmonitor its services. Monitoring and log management tools collect and analyze \ninformation. Based on the feedback and corrective actions needed, information is \nfed to the development teams to take corrective actions, and the changes are pushed \nback to production through the pipeline. Tools such as APM, Open Web Analytics, \nGoogle Analytics, Webalizer, and so on are useful tools to monitor web applications. \nReal user monitoring should provide end-to-end monitoring. QuBit, Boxever, \nChannel Site, MaxTraffic, and so on are also useful in analyzing customer behavior.\nAutomated configuration management\nConfiguration management also has to be rethought from a microservices and \nDevOps perspective. Use new methods for configuration management rather than \nusing a traditional statically configured CMDB. The manual maintenance of CMDB \nis no longer an option. Statically managed CMDB requires a lot of mundane tasks \nto maintain entries. At the same time, due to the dynamic nature of the deployment \ntopology, it is extremely hard to maintain data in a consistent way.\n",
      "content_length": 2527,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 425,
      "content": "The Microservices Development Life Cycle\n[ 398 ]\nThe new styles of CMDB automatically create CI configurations based on an \noperational topology. These should be discovery based to get up-to-date information. \nThe new CMDB should be capable of managing bare metals, virtual machines,  \nand containers.\nMicroservices development governance, \nreference architectures, and libraries\nIt is important to have an overall enterprise reference architecture and a standard \nset of tools for microservices development to ensure that development is done in a \nconsistent manner. This helps individual microservices teams to adhere to certain \nbest practices. Each team may identify specialized technologies and tools that are \nsuitable for their development. In a polyglot microservices development, there are \nobviously multiple technologies used by different teams. However, they have to \nadhere to the arching principles and practices.\nFor quick wins and to take advantage of timelines, microservices development \nteams may deviate from these practices in some cases. This is acceptable as long as \nthe teams add refactoring tasks in their backlogs. In many organizations, although \nthe teams make attempts to reuse something from the enterprise, reuse and \nstandardization generally come as an afterthought.\nIt is important to make sure that the services are catalogued and visible in the \nenterprise. This improves the reuse opportunities of microservices.\nSummary\nIn this chapter, you learned about the relationship between microservices and \nDevOps. We also examined a number of practice points when developing \nmicroservices. Most importantly, you learned the microservices development  \nlife cycle.\nLater in this chapter, we also examined how to automate the microservices delivery \npipeline from development to production. As part of this, we examined a number of \ntools and technologies that are helpful when automating the microservices delivery \npipeline. Finally, we touched base with the importance of reference architectures in \nmicroservices governance.\nPutting together the concepts of microservices, challenges, best practices, and various \ncapabilities covered in this book makes a perfect recipe for developing successful \nmicroservices at scale.\n",
      "content_length": 2257,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 426,
      "content": "[ 399 ]\nIndex\nA\nagents  348\nAgile software development\nURL  380\nAirbnb\nabout  45\nURL  45\nAmazon\nabout  46\nURL  46\nAmazon EC2 Container Service (ECS)  321\nAngel  207\nApache Mesos  346, 347\napplication\nautoscaling  266\napplication information\nconfiguring  99\nappropriate microservice boundaries\nestablishing  106\narchitecture, Mesos\nabout  349\nframework  350\nmaster  350\nslave  350\nZooKeeper  350\nautoconfig  98\nautoscaling\nabout  262, 263\nbenefits  263-265\ndifferent autoscaling models  265\nin cloud  267\nautoscaling approaches\nabout  268\npredictive autoscaling  271\nscaling, based on business parameters  270\nscaling, based on message  \nqueue length  270\nscaling, during specific time periods  269\nscaling, with resource constraints  268\nautoscaling ecosystem\nabout  273\nlife cycle manager  273\nload balancer  273\nMicroservices  273\nservice registry  273\nAWS\nsetting up  354, 355\nAWS Lambda\nreference  5\nB\nBackend as a Service (BaaS)  44\nbenefits, microservices\nabout  23\nbuild organic systems, enabling  28\ncoexistence of different versions,  \nallowing  30\nDevOps, enabling  33\nelastically scalable  25-27\nevent-driven architecture, supporting  32\nexperimentation, enabling  24\ninnovation, enabling  25\npolyglot architecture support  23, 24\nselectively scalable  26, 27\nself-organizing systems, building  31\nsubstitution, allowing  27, 28\ntechnology debt, reducing  29\nbest-of-the-breed components\ndashboards  290\nlog shippers  289\n",
      "content_length": 1433,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 427,
      "content": "[ 400 ]\nlog storage  289\nbill of materials (BOM)  63\nBin packing algorithm  343\nBoot2Docker  321\nbounded context  106, 107\nBrixton  207\nBrownField PSS\nenvironment, setting up for  210\nBrownField PSS architecture\ndefining  256, 257\nBrownField PSS microservices\nlimitations  315\nBrownField PSS microservices, autoscaling\nabout  272\ncapabilities  272\ncustom life cycle manager, implementing \nwith Spring Boot  274\ndeployment topology  274, 275\nexecution flow  275, 276\nlife cycle manager code  277-280\nlife cycle manager, running  281, 282\nBrownField PSS services\ndeploying  363, 364\npreparing  361, 362\nreviewing  365, 366\nBrownField services\nrunning, on EC2  332, 333\nbusiness case\ndefining  165\nBusiness Process Management (BPM)\nabout  121\nuse cases  121\nC\nCalico  316\ncandidate tests, for automation\nabout  392\nA/B testing  395\nantifragility testing  396\nautomated acceptance testing  393\nautomated functional testing  393\nautomated sanity tests  392\nautomated security testing  394\nblue-green deployments  395\ncanary testing  395\nexploratory testing  395\nnonfunctional tests  395\nperformance testing  394\nreal user flow simulation or  \njourney testing  394\nregression testing  392\nTesting in Production (TiP)  395\nCDNs (Content Delivery Networks)  164\ncentralized logging solution\nabout  286\nlog dashboard  287\nlog shippers  287\nlog store  287\nlog stream processor  287\nlog streams  287\ncgroups  325, 348\ncharacteristics, of microservices\nabout  10\nantifragility  16\nautomation  14\ncharacteristics of services  11, 12\ndistributed and dynamic  15, 16\nfail fast  17\nlightweight  12, 13\npolyglot architecture  13, 14\nself-healing  17\nservices are first-class citizens  11\nsupporting ecosystem  15\ncharacteristics of services, microservices\nloose coupling  11\nservice abstraction  11\nservice composeability  12\nservice contract  11\nservice interoperability  12\nservice reuse  11\nservices are discoverable  12\nstatelessness  12\ncircuit breaker pattern\nreference  302\ncloud\nas self-service infrastructure, for  \nmicroservices  376\ncluster management\nabout  340\nfeatures  340\nfunctioning  341\nrelationship with microservices  344\n",
      "content_length": 2125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 428,
      "content": "[ 401 ]\nrelationship with virtualization  344\nwith Mesos and Marathon  348\ncluster management solutions\nabout  344, 345\nApache Mesos  346, 347\nDocker Swarm  345\nFleet  347\nKubernetes  346\nNomad  347\nCommand Query Responsibility  \nSegregation (CQRS)  109\ncommunication styles, designing\nasynchronous style communication  112\nstyle, selecting  112-114\nsynchronous style communication  111\ncomponents, Spring Cloud\nNetflix OSS  210\ncomprehensive microservice example\ndeveloping  86-96\nConfig server URL\nConfig Server, accessing  \nfrom clients  218-222\ndefining  217\nreferences  218\nconfigurable items (CIs)  143\nConfiguration Management Database \n(CMDB)  143\ncontainerization\nabout  334\nfuture  334\nsecurity issues  334\nunikernels  334\ncontainers\nabout  316\nbenefits  319\nDevOps  319\nimmutable containers  320\nlightweight  319\nlower license cost  319\nportable  319\nreusable  320\nscalable  319\nself-contained  319\nversion controlled  319\nversus, virtual machines (VMs)  317, 318\nContent Management System (CMS)  23\ncontinuous delivery pipeline, automating\nabout  387\nautomated testing  391, 392\ncontinuous deployment  397\ncontinuous integration  390\ndevelopment stage  389\nmonitoring and feedback  397\nContinuous Integration (CI) tools  14\ncontinuous integration workflow\nabout  391\nbuild and QA  391\nintegration tests  391\npackaging  391\nConway's Law\nreference  45\ncore capabilities, microservices  \ncapability model\nabout  145\nAPI gateway  146\nbusiness capability definition  145\nevent sourcing  145\nservice endpoints and communication \nprotocols  146\nservice listeners  145\nstorage capability  145\nuser interfaces  146\nCorporate Customer microservice  29\ncreate, read, update, and delete (CRUD)  86\ncross-origin\nenabling, for microservices  82, 83\nCustomer microservices  29\nCustomer Profile microservice\nabout  11\nGet Customer  11\nRegister Customer  11\nCustomer service  31\ncustom health module\nadding  99-101\ncustom metrics, building  101, 102\nD\ndata analysis\ndata lakes, used  310, 311\nDatabase Lifecycle Management (DLM)  381\ndata lake  310\n",
      "content_length": 2044,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 429,
      "content": "[ 402 ]\ndefault embedded web server\nchanging  77\ndependencies, microservice boundaries\ndependency graph  176\nevents, as opposed to query  169, 170\nevents, as opposed to synchronous  \nupdates  170-172\nrequirements, challenging  172, 173\nservice boundaries, challenging  173-175\ndevelopment environment\nsetting up  49, 50\ndevelopment philosophy\nAgile practice  379\ndesign thinking  378\nselecting  378\nstart-up model  379\ndevelopment stage\nabout  389\nartifact repository  390\nbehavior driven design (BDD)  389\nbuild tool  390\nconsumer-driven contracts  389\ndatabase schemas  390\nmock testing  389\nsource code  389\nsource code repository  390\nunit test cases  389\nDevOps\nabout  7\nas practice and process, for  \nmicroservices  376\ndefining  372-374\ndevelopment and operations, bridging  375\ntasks, automating  374\nURL  7\nvalue-driven delivery  374\nwastage, reducing  374\nDevOps-style microservice life cycle process\nAgile development  386\nApplication Release Automation  \n(ARA)  385\ncontinuous delivery (CD)  385\ncontinuous deployment  385\ncontinuous integration (CI)  385, 386\ncontinuous monitoring and feedback  387\ncontinuous release  387\ncontinuous testing  387\ndefining  385\nvalue-driven planning  386\nDocker\nabout  316, 321\nconcepts  323\nkey components  322\nmicroservices, deploying  326-330\nRabbitMQ, running on  330\nURL  326\nDocker client  322\nDocker concepts\nDocker containers  325\nDockerfile  326\nDocker images  323, 324\nDocker registry  325, 326\nDocker daemon  322\nDocker file\ncontents  327, 328\nDocker Hub account\nreference  331\nDockerization  321\nDocker registry\nDocker hub, setting up  331\nmicroservices, publishing to  \nDocker hub  331\nusing  330\nDocker Swarm  345\ndomain-driven design (DDD)\nabout  106\nURL  106\nDrawbridge  316\nDrools  120\ndump  98\ndynamic discovery\ndefining  232\ndynamic resource provisioning and  \ndeprovisioning  263\ndynamic service registration\ndefining  232\nE\neBay\nabout  46\nURL  46\n",
      "content_length": 1915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 430,
      "content": "[ 403 ]\necosystem capabilities\nbuilding  187\nelasticity  263\nElasticsearch\nURL  290\nengine  348\nEnterprise Application integration  \n(EAI)  191\nEnterprise Integration Patterns (EIP)  35\nEnterprise Service Bus (ESB)  16\nEureka\ndefining  234, 235\nhigh availability  241-243\nserver, setting up  235-241\nevolution\nplanning  165\nevolution, microservices\nabout  1\nbusiness demand, as catalyst  2, 3\nimperative architecture evolution  4, 5\ntechnology, as catalyst  4\nexamples, microservices\nabout  17\nholiday portal  17-19\nmicroservice-based order management \nsystem  20, 21\ntravel agent portal  22, 23\nF\nfactors\ndefining  177, 178\nFakeSMTP\nreference  95\nFeign\ndefining  227-229\nFleet  347\nFly By Points  17\nfollowers  347\nG\ngateway  325\nGilt\nabout  46\nURL  46\nGuest OS  317\nH\nHAL (Hypertext Application Language) \nbrowser  68\nHATEOAS example  68\nhealth  98\nhexagonal architecture, microservices\nreference  6\nhigh availability\nURL  226\nhigh availability, of Zuul\nabout  249\nEureka client  250\nnot Eureka client  251\nhoneycomb analogy  8\nHypertext As The Engine Of Application \nState. See  HATEOAS example\nI\ninfo  98\ninfrastructure\nautoscaling  266\ninfrastructure capabilities, microservices \ncapability model\nabout  146\napplication life cycle management  147\ncloud  146\ncluster control and provisioning  147\ncontainers or virtual machines  146\nin-memory data grid (IMDG)  27\nINPUT queue  31\ninstance ID  361\nIntegration Platform as a Service (iPaaS)  4\nIntelligent Business Process Management \n(iBPM)  121\nJ\nJDK 1.8\nreference  49\njobs  347\nJSch (Java Secure Channel)  279\nJSON structure\ncontainer  364\ncpus  363\nid  363\nimage  364\n",
      "content_length": 1624,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 431,
      "content": "[ 404 ]\ninstances  363\nmem  363\nnetwork  364\nportMappings  364\nK\nkey capabilities, cluster management  \nsoftware\nabout  341\nagility  342\ncluster management  341\ndeployments  341\nhealth  341\ninfrastructure abstraction  342\nisolation  342\nresource allocation  342\nresource optimization  342\nscalability  341\nservice availability  342\nkey components, automated microservices \ndeployment topology\nabout  261\nEureka client  261\nRibbon client  261\nkey components, Docker\nabout  322\nDocker client  322\nDocker daemon  322\nKubernetes  346\nL\nLambda architecture\nreference  287\nleader  347\nlibraries  398\nlife cycle manager  \nabout  272, 273, 367\ndecision engine  273\ndeployment engine  273\ndeployment rules  273\nmetrics collection  273\nrewriting, with Mesos and Marathon  368\nscaling policies  273\nupdating  334\nLmctfy  316\nlogging solutions\nbest-of-the-breed integration  288\ncloud services  288\ncustom logging implementation  290-292\ndistributed tracing, with Spring Cloud \nSleuth  293-295\noff-the-shelf solutions  288\nselecting  288\nlog management challenges  284, 285\nLoyalty Points microservice  29\nLXD  316\nM\nmanager  345\nmappings  98\nMarathon\nabout  352\nfeatures  352\nimplementing, for BrownField  \nmicroservices  353, 354\ninstalling  356\nreference  356\nrunning as services  358\nMarathon autoscale  367\nmaster  346\nMaven 3.3.1\nreference  50\nMean Time Between Failures (MTBF)  17\nMean Time To Recover (MTTR)  17\nMesos\nabout  348, 349\narchitecture  349, 350\nconfiguring  357\nimplementing, for BrownField  \nmicroservices  353, 354\ninstalling  356\nreference  356\nrunning as services  358\nworkflow diagram  351\nMesos slave\nrunning,  in command line  359, 360\nmetrics  98\nMicroservice A  383\nMicroservice B  383\nmicroservice boundaries\nagile teams  109\n",
      "content_length": 1744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 432,
      "content": "[ 405 ]\nautonomous functions  107\nchangeability  110\ncoupling  110\ndependencies, analyzing  167-169\nidentifying  167\nmicroservice, as product  110\nmost appropriate function  108\npolyglot architecture  108\nreplicability  110\nselective scaling  108\nsingle responsibility  109\nsize, of deployable unit  108\nsubdomain  108\nmicroservice capability model\nreviewing  260\nmicroservice monitoring\nabout  297\nactions  299\naggregation and correlation of metrics  299\nalerts  299\nApplication Performance Monitoring \n(APM) approach  300\nchallenges  298-300\ndashboards  299\nHystrix streams, aggregating  \nwith Turbine  307-310\nmetrics sources and data collectors  299\nmicroservice dependencies,  \nmonitoring  301, 302\nobjectives  298\nprocessing metrics and actionable  \ninsights  299\nreal user monitoring (RUM)  300\nSpring Cloud Hystrix, for fault-tolerant \nmicroservices  302-306\nsynthetic monitoring  300\ntools  300, 301\nuser experience monitoring  300\nmicroservices\nabout  1, 5\nand containers  320, 321\narchitecture  6\nbenefits  23, 165\ncharacteristics  10\nCustomer  9\ndata synchronization, during  \nmigration  178-180\ndefining  164\ndeploying, in Docker  326-330\ndocumenting  102, 103\nevolution  1\nexamples  17\nhexagonal architecture  6\nhoneycomb analogy  8\nOrder  9\nprinciples  8\nprioritizing, for migration  177, 178\nProduct  9\nreference data, managing  181-183\nrelationship, with other architecture   33\nscaling, with Spring Cloud  260-262\nuse cases  43\nmicroservices-based architecture\nexamining  7\nmicroservices capabilities\nreviewing  204\nmicroservices capability model\nabout  144, 145\ncore capabilities  145\ngovernance capabilities  145\ninfrastructure capabilities  145, 146\nprocess and governance capabilities  148\nreviewing  152, 153\nsupporting capabilities  145-147\nmicroservices challenges\nabout  139\ndata islands  139, 140\ndependency management  141\ngovernance challenges  142\ninfrastructure provisioning  144\nlogging  140, 141\nmicroservices, testing  143\nmonitoring  140, 141\noperation overheads  142\norganization culture  142\nmicroservices development\npractice points  377\nmicroservices development teams  398\nmicroservices, on cloud\nabout  332\nDocker, installing on AWS EC2  332\nMinimum Viable Product (MVP)\nabout  380\nusing  380\n",
      "content_length": 2234,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 433,
      "content": "[ 406 ]\nmodules\nmigrating  187\nN\nNetflix\nabout  45, 210\nURL  45\nNetflix Open Source Software  \n(Netflix OSS)  210\nNike\nabout  46\nURL  46\nnodes  345, 346\nNomad  347\nO\nOAuth\nreference  79\nOrbitz\nabout  46\nURL  46\nOrder Event  32\nOUTPUT queue  32\noversubscription  343\nP\nPassenger Sales and Service (PSS)  151\npatterns and common design decisions\nabout  105, 106\nAPI gateways, using in  \nmicroservices  131-133\nappropriate microservice boundaries,  \nestablishing  106\nbulk operations, in microservices  138\ncommunication styles, designing  111\ndata store, sharing  123, 124\ndesign, for cross origin  136\nESB and iPaaS, using with  \nmicroservices  133, 134\nmicroservice, on  multiple VMs  119\nmicroservice, on VM  119\nnumber of endpoints  118, 119\norchestration  115-118\nrole, of BPM and workflows  121-123\nrules engine  120, 121\nservice endpoint design consideration  127\nservice versioning considerations  134-136\nshared libraries, handling  129\nshared reference data, handling  136, 137\ntransaction boundaries, setting up  125\nuser interfaces, in microservices  130, 131\nPlatform as a Services (PaaS)  4\npods  346\nPOJO (Plain Old Java Object)  208\npom file\nreference  63\npractice points, microservices development\nabout  377\nautomated configuration management  397\nbusiness motivation and value  377\nchallenges around databases,  \naddressing  381\ncontinuous delivery pipeline,  \nautomating  387, 388\ndevelopment philosophy, selecting  378\nDevOps-style microservice life cycle  \nprocess, defining  385, 386\nlegacy hotspot  380\nmicroservices ecosystem, building  384\nmindset, changing from project to product \ndevelopment  377\nMinimum Viable Product (MVP), using  380\nself-organizing teams, establishing  381-383\nself-service cloud, building  384\nprinciples, of microservices\nabout  8\nautonomous services  9, 10\nsingle responsibility principle  8, 9\nprocess and governance capabilities,  \nmicroservices capability model\nabout  148\nDevOps  148\nDevOps tools  149\nmicroservice documentation  149\nmicroservices repositor   149\nreference architecture and libraries  149\nprotocol selection\nabout  127\nAPI documentations  128\nHTTP and REST endpoints  128\nmessage-oriented services  128\noptimized communication protocols  128\n",
      "content_length": 2215,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 434,
      "content": "[ 407 ]\nPSS application\nabout  158\narchitectural view  154, 155\nbusiness process view  153\ndefining  153\ndeployment view  157\ndesign view  155\ndomain boundaries  163\nfunctional view  154\nimplementation view  156\nlimitation  158\nshared data  160, 161\nsingle database  161\nstop gap fix  159\nPSS Implementation, BrownField\nreviewing  204, 205\nR\nRabbitMQ\nrunning, on Docker  330\nURL  226\nRandom algorithm  343\nreactive programming\nreference  20\nReal Application Cluster (RAC)  159\nReal User Monitoring (RUM) tools  395\nreference architectures  398\nrelationship, microservices\nabout  33\nrelations, with SOA  33, 34\nrelations, with Twelve-Factor Apps  37\nrelationship, microservices with SOA\nabout  33, 34\nlegacy modernization  35\nmonolithic application  36\nservice-oriented application  36\nservice-oriented integration  34, 35\nrelationship, microservices with  \nTwelve-Factor Apps\nabout  37\nbacking services  39, 40\nconcurrency, scaling out  41\nconfigurations, externalizing  39\ndependencies, bundling  38\ndevelopment and production parity  42\ndisposability, with minimal overhead  41\nlogs, externalizing  42\npackage admin processes  43\nservices, exposing through  \nport bindings  41\nsingle codebase  38\nRESTful microservices\nbuilding, Spring Boot used  56\nRESTful service\ndeveloping  50-55\nRibbon\nabout  230\ndefining, for load balancing  229-231\nRocket  316\nRPM (Requests Per Minute)  276\nS\nsales closing transactions  270\nScale Cube\nabout  26\nreference  26\nSearch API Gateway microservice  326\nservers  347\nservice availability\nand discovery  232\nregistering  232\nservice endpoint design consideration\nabout  127\ncontract design  127\nprotocol selection  127\nService Oriented Architecture (SOA)  1\nService-Oriented Integration (SOI)  35\nsession handling\nand security  184, 185\nshopping logic  16\nsingle database\nnative queries  161, 162\nstored procedures  163\nsingle sign-on (SSO)  183\nSMTP server  31\nSOA principles\nreference  12\nsoftware-defined infrastructure  376\nsoftware development life cycle (SDLC)  371\nSOLID design pattern\nreference  8\nspan  294\n",
      "content_length": 2052,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 435,
      "content": "[ 408 ]\nSpread algorithm  343\nSpring Boot\nabout  49, 57\nused, for building RESTful  \nmicroservices  56\nSpring Boot actuators\nabout  97\nmonitoring, JConsole used  98\nmonitoring, SSH used  99\nSpring Boot command-line tool\nreference  57\nSpring Boot configuration\n.yaml file, using  75\nabout  73\nautoconfiguration  73\nconfiguration file location, changing  74\ncustom properties, reading  75\ndefault configuration values, overriding  74\nmultiple configuration profiles, using  76\nproperties, reading  76\nSpring Boot Java microservice\nApplication.java, examining  64, 65\napplication.properties, examining  65\nApplicationTests.java, examining  65, 66\ndeveloping, STS used  58-62\npom file, examining  62-64\ntesting  67\nSpring Boot messaging\nimplementing  83-86\nSpring Boot microservice\ndeveloping, CLI used  57, 58\ndeveloping, Spring Initializr used  68-72\nSpring Boot security\nimplementing  77\nmicroservice, securing with  \nbasic security  77, 78\nmicroservice, securing with OAuth2  79-81\nSpring Cloud\nand Cloud Foundry, comparing  206\ncapabilities  208\ncomponents  207-209\ndefining  205, 206\nfor scaling microservices  260, 261\nreleases  206, 207\nSpring Cloud Config\nchanges, completing for Config  \nserver usage  227\nConfig server, for configuration files  226\nConfig server health, monitoring  226\nConfig server, setting up  214-216\nConfig server URL, defining  217\nConfig server, using  214\nconfiguration changes, handling  222\ndefining  211-213\nhigh availability, setting up for Config \nserver  224-226\nSpring Cloud Bus, for propagating  \nconfiguration changes  223, 224\nSpring Cloud examples\nreferences  215\nSpring Initializr project\nreference  57\nSpring Starter project\nURL  228\nSpring Tool Suite 3.7.2 (STS)\nreference  49\nstreams\nfor reactive microservices  252-256\nsubnet  325\nsupporting capabilities, microservices  \ncapability model\nabout  147\ncentral log management  147\ndata lake  148\ndependency and CI management  148\nmonitoring and dashboards  148\nreliable messaging  148\nsecurity service  148\nservice configuration  148\nservice registry  147\nsoftware defined load balancer  147\ntesting tools  148\nSystemdNspawn  316\nT\nt2.large  332\nt2.micro EC2 instances  354\ntarget architecture\nabout  188, 189\nexceptions, handling  191-194\nintegration, with other systems  191\ninternal layering, of microservices  189\nmicroservices, orchestrating  190\n",
      "content_length": 2347,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 436,
      "content": "[ 409 ]\nshared libraries, managing  191\ntarget implementation view\nabout  194\nimplementation projects  195, 196\nproject, running  196-200\nproject, testing  196-200\ntarget test environments\nabout  396\ndevelopment environment  396\nfunctional test environment  396\nintegration test environment  396\nperformance and diagnostics  396\nproduction  396\nstaging  396\nUAT environment  396\ntask groups  347\ntasks  347\ntechnology metamodel  368\nTesting in Production (TiP)  395\ntest strategy  186\nTPM (Transactions Per Minute)  276\ntrace  294\ntraditional web application\nmoving, to microservices  55\ntransaction boundaries\ndistributed transaction scenarios  126\nsetting up  125\nuse cases, altering  125, 126\ntransaction processing facility (TPF)  380\ntransition plan\nestablishing  166\ntransition point of view\nqueries  166\nTwitter\nabout  46\nURL  46\nU\nUber\nabout  45\nURL  45\nuse cases, microservices\nabout  43, 44\nmicroservices early adopters  45\nmonolithic migrations  47\nuser interfaces\nand web applications  183, 184\nV\nVeritas Risk Advisor (VRA)  302\nvirtual engine (VE)  316\nvirtual machines (VMs)\nHyper-V  317\nversus containers  317, 318\nZen  317\nW\nweb applications\nand user interfaces  183, 184\nX\nXebiaLabs periodic table\nreference  387\nZ\nZooKeeper\nconfiguring  356\ninstalling  355, 356\nreference  356\nrunning as services  358\nZuul\ncompleting, for all services  251\nhigh availability  249\nsetting up  245-249\nusing  247, 248\nZuul proxy\ndefining, as API gateway  244\n",
      "content_length": 1459,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}