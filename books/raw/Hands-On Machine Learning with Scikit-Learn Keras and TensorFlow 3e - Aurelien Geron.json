{
  "metadata": {
    "title": "Hands-On Machine Learning with Scikit-Learn Keras and TensorFlow 3e - Aurelien Geron",
    "author": "Aurélien Géron",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 718,
    "conversion_date": "2025-12-19T17:30:07.035278",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Hands-On Machine Learning with Scikit-Learn Keras and TensorFlow 3e - Aurelien Geron.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "The\tMachine\tLearning\tLandscape",
      "start_page": 19,
      "end_page": 55,
      "detection_method": "regex_chapter_title",
      "content": "What\tIs\tMachine\tLearning? Machine\tLearning\tis\tthe\tscience\t(and\tart)\tof\tprogramming\tcomputers\tso\tthey\tcan\tlearn\tfrom\tdata.\n\nHere\tis\ta\tslightly\tmore\tgeneral\tdefinition:\n\n[Machine\tLearning\tis\tthe]\tfield\tof\tstudy\tthat\tgives\tcomputers\tthe\tability\tto\tlearn\twithout\tbeing explicitly\tprogrammed. Arthur\tSamuel,\t1959\n\nAnd\ta\tmore\tengineering-oriented\tone:\n\nA\tcomputer\tprogram\tis\tsaid\tto\tlearn\tfrom\texperience\tE\twith\trespect\tto\tsome\ttask\tT\tand\tsome performance\tmeasure\tP,\tif\tits\tperformance\ton\tT,\tas\tmeasured\tby\tP,\timproves\twith\texperience\tE. Tom\tMitchell,\t1997\n\nFor\texample,\tyour\tspam\tfilter\tis\ta\tMachine\tLearning\tprogram\tthat\tcan\tlearn\tto\tflag\tspam\tgiven\texamples of\tspam\temails\t(e.g.,\tflagged\tby\tusers)\tand\texamples\tof\tregular\t(nonspam,\talso\tcalled\t“ham”)\temails.\tThe examples\tthat\tthe\tsystem\tuses\tto\tlearn\tare\tcalled\tthe\ttraining\tset.\tEach\ttraining\texample\tis\tcalled\ta training\tinstance\t(or\tsample).\tIn\tthis\tcase,\tthe\ttask\tT\tis\tto\tflag\tspam\tfor\tnew\temails,\tthe\texperience\tE\tis the\ttraining\tdata,\tand\tthe\tperformance\tmeasure\tP\tneeds\tto\tbe\tdefined;\tfor\texample,\tyou\tcan\tuse\tthe\tratio of\tcorrectly\tclassified\temails.\tThis\tparticular\tperformance\tmeasure\tis\tcalled\taccuracy\tand\tit\tis\toften used\tin\tclassification\ttasks.\n\nIf\tyou\tjust\tdownload\ta\tcopy\tof\tWikipedia,\tyour\tcomputer\thas\ta\tlot\tmore\tdata,\tbut\tit\tis\tnot\tsuddenly\tbetter at\tany\ttask.\tThus,\tit\tis\tnot\tMachine\tLearning.\n\nWhy\tUse\tMachine\tLearning? Consider\thow\tyou\twould\twrite\ta\tspam\tfilter\tusing\ttraditional\tprogramming\ttechniques\t(Figure\t1-1):\n\n1.\t First\tyou\twould\tlook\tat\twhat\tspam\ttypically\tlooks\tlike.\tYou\tmight\tnotice\tthat\tsome\twords\tor\tphrases (such\tas\t“4U,”\t“credit\tcard,”\t“free,”\tand\t“amazing”)\ttend\tto\tcome\tup\ta\tlot\tin\tthe\tsubject.\tPerhaps you\twould\talso\tnotice\ta\tfew\tother\tpatterns\tin\tthe\tsender’s\tname,\tthe\temail’s\tbody,\tand\tso\ton.\n\n2.\t You\twould\twrite\ta\tdetection\talgorithm\tfor\teach\tof\tthe\tpatterns\tthat\tyou\tnoticed,\tand\tyour\tprogram would\tflag\temails\tas\tspam\tif\ta\tnumber\tof\tthese\tpatterns\tare\tdetected.\n\n3.\t You\twould\ttest\tyour\tprogram,\tand\trepeat\tsteps\t1\tand\t2\tuntil\tit\tis\tgood\tenough.\n\nFigure\t1-1.\tThe\ttraditional\tapproach\n\nSince\tthe\tproblem\tis\tnot\ttrivial,\tyour\tprogram\twill\tlikely\tbecome\ta\tlong\tlist\tof\tcomplex\trules\t—\tpretty hard\tto\tmaintain.\n\nIn\tcontrast,\ta\tspam\tfilter\tbased\ton\tMachine\tLearning\ttechniques\tautomatically\tlearns\twhich\twords\tand phrases\tare\tgood\tpredictors\tof\tspam\tby\tdetecting\tunusually\tfrequent\tpatterns\tof\twords\tin\tthe\tspam examples\tcompared\tto\tthe\tham\texamples\t(Figure\t1-2).\tThe\tprogram\tis\tmuch\tshorter,\teasier\tto\tmaintain, and\tmost\tlikely\tmore\taccurate.\n\nFigure\t1-2.\tMachine\tLearning\tapproach\n\nMoreover,\tif\tspammers\tnotice\tthat\tall\ttheir\temails\tcontaining\t“4U”\tare\tblocked,\tthey\tmight\tstart\twriting “For\tU”\tinstead.\tA\tspam\tfilter\tusing\ttraditional\tprogramming\ttechniques\twould\tneed\tto\tbe\tupdated\tto\tflag “For\tU”\temails.\tIf\tspammers\tkeep\tworking\taround\tyour\tspam\tfilter,\tyou\twill\tneed\tto\tkeep\twriting\tnew rules\tforever.\n\nIn\tcontrast,\ta\tspam\tfilter\tbased\ton\tMachine\tLearning\ttechniques\tautomatically\tnotices\tthat\t“For\tU”\thas become\tunusually\tfrequent\tin\tspam\tflagged\tby\tusers,\tand\tit\tstarts\tflagging\tthem\twithout\tyour\tintervention (Figure\t1-3).\n\nFigure\t1-3.\tAutomatically\tadapting\tto\tchange\n\nAnother\tarea\twhere\tMachine\tLearning\tshines\tis\tfor\tproblems\tthat\teither\tare\ttoo\tcomplex\tfor\ttraditional approaches\tor\thave\tno\tknown\talgorithm.\tFor\texample,\tconsider\tspeech\trecognition:\tsay\tyou\twant\tto\tstart simple\tand\twrite\ta\tprogram\tcapable\tof\tdistinguishing\tthe\twords\t“one”\tand\t“two.”\tYou\tmight\tnotice\tthat the\tword\t“two”\tstarts\twith\ta\thigh-pitch\tsound\t(“T”),\tso\tyou\tcould\thardcode\tan\talgorithm\tthat\tmeasures high-pitch\tsound\tintensity\tand\tuse\tthat\tto\tdistinguish\tones\tand\ttwos.\tObviously\tthis\ttechnique\twill\tnot scale\tto\tthousands\tof\twords\tspoken\tby\tmillions\tof\tvery\tdifferent\tpeople\tin\tnoisy\tenvironments\tand\tin\n\ndozens\tof\tlanguages.\tThe\tbest\tsolution\t(at\tleast\ttoday)\tis\tto\twrite\tan\talgorithm\tthat\tlearns\tby\titself,\tgiven many\texample\trecordings\tfor\teach\tword.\n\nFinally,\tMachine\tLearning\tcan\thelp\thumans\tlearn\t(Figure\t1-4):\tML\talgorithms\tcan\tbe\tinspected\tto\tsee what\tthey\thave\tlearned\t(although\tfor\tsome\talgorithms\tthis\tcan\tbe\ttricky).\tFor\tinstance,\tonce\tthe\tspam filter\thas\tbeen\ttrained\ton\tenough\tspam,\tit\tcan\teasily\tbe\tinspected\tto\treveal\tthe\tlist\tof\twords\tand combinations\tof\twords\tthat\tit\tbelieves\tare\tthe\tbest\tpredictors\tof\tspam.\tSometimes\tthis\twill\treveal unsuspected\tcorrelations\tor\tnew\ttrends,\tand\tthereby\tlead\tto\ta\tbetter\tunderstanding\tof\tthe\tproblem.\n\nApplying\tML\ttechniques\tto\tdig\tinto\tlarge\tamounts\tof\tdata\tcan\thelp\tdiscover\tpatterns\tthat\twere\tnot immediately\tapparent.\tThis\tis\tcalled\tdata\tmining.\n\nFigure\t1-4.\tMachine\tLearning\tcan\thelp\thumans\tlearn\n\nTo\tsummarize,\tMachine\tLearning\tis\tgreat\tfor:\n\nProblems\tfor\twhich\texisting\tsolutions\trequire\ta\tlot\tof\thand-tuning\tor\tlong\tlists\tof\trules:\tone\tMachine Learning\talgorithm\tcan\toften\tsimplify\tcode\tand\tperform\tbetter.\n\nComplex\tproblems\tfor\twhich\tthere\tis\tno\tgood\tsolution\tat\tall\tusing\ta\ttraditional\tapproach:\tthe\tbest Machine\tLearning\ttechniques\tcan\tfind\ta\tsolution.\n\nFluctuating\tenvironments:\ta\tMachine\tLearning\tsystem\tcan\tadapt\tto\tnew\tdata.\n\nGetting\tinsights\tabout\tcomplex\tproblems\tand\tlarge\tamounts\tof\tdata.\n\nTypes\tof\tMachine\tLearning\tSystems There\tare\tso\tmany\tdifferent\ttypes\tof\tMachine\tLearning\tsystems\tthat\tit\tis\tuseful\tto\tclassify\tthem\tin\tbroad categories\tbased\ton:\n\nWhether\tor\tnot\tthey\tare\ttrained\twith\thuman\tsupervision\t(supervised,\tunsupervised,\tsemisupervised, and\tReinforcement\tLearning)\n\nWhether\tor\tnot\tthey\tcan\tlearn\tincrementally\ton\tthe\tfly\t(online\tversus\tbatch\tlearning)\n\nWhether\tthey\twork\tby\tsimply\tcomparing\tnew\tdata\tpoints\tto\tknown\tdata\tpoints,\tor\tinstead\tdetect patterns\tin\tthe\ttraining\tdata\tand\tbuild\ta\tpredictive\tmodel,\tmuch\tlike\tscientists\tdo\t(instance-based versus\tmodel-based\tlearning)\n\nThese\tcriteria\tare\tnot\texclusive;\tyou\tcan\tcombine\tthem\tin\tany\tway\tyou\tlike.\tFor\texample,\ta\tstate-of-the- art\tspam\tfilter\tmay\tlearn\ton\tthe\tfly\tusing\ta\tdeep\tneural\tnetwork\tmodel\ttrained\tusing\texamples\tof\tspam\tand ham;\tthis\tmakes\tit\tan\tonline,\tmodel-based,\tsupervised\tlearning\tsystem.\n\nLet’s\tlook\tat\teach\tof\tthese\tcriteria\ta\tbit\tmore\tclosely.\n\nSupervised/Unsupervised\tLearning Machine\tLearning\tsystems\tcan\tbe\tclassified\taccording\tto\tthe\tamount\tand\ttype\tof\tsupervision\tthey\tget during\ttraining.\tThere\tare\tfour\tmajor\tcategories:\tsupervised\tlearning,\tunsupervised\tlearning, semisupervised\tlearning,\tand\tReinforcement\tLearning.\n\nSupervised\tlearning\n\nIn\tsupervised\tlearning,\tthe\ttraining\tdata\tyou\tfeed\tto\tthe\talgorithm\tincludes\tthe\tdesired\tsolutions,\tcalled labels\t(Figure\t1-5).\n\nFigure\t1-5.\tA\tlabeled\ttraining\tset\tfor\tsupervised\tlearning\t(e.g.,\tspam\tclassification)\n\nA\ttypical\tsupervised\tlearning\ttask\tis\tclassification.\tThe\tspam\tfilter\tis\ta\tgood\texample\tof\tthis:\tit\tis\ttrained with\tmany\texample\temails\talong\twith\ttheir\tclass\t(spam\tor\tham),\tand\tit\tmust\tlearn\thow\tto\tclassify\tnew emails.\n\nAnother\ttypical\ttask\tis\tto\tpredict\ta\ttarget\tnumeric\tvalue,\tsuch\tas\tthe\tprice\tof\ta\tcar,\tgiven\ta\tset\tof\tfeatures (mileage,\tage,\tbrand,\tetc.)\tcalled\tpredictors.\tThis\tsort\tof\ttask\tis\tcalled\tregression\t(Figure\t1-6).1\tTo\ttrain the\tsystem,\tyou\tneed\tto\tgive\tit\tmany\texamples\tof\tcars,\tincluding\tboth\ttheir\tpredictors\tand\ttheir\tlabels (i.e.,\ttheir\tprices).\n\nNOTE\n\nIn\tMachine\tLearning\tan\tattribute\tis\ta\tdata\ttype\t(e.g.,\t“Mileage”),\twhile\ta\tfeature\thas\tseveral\tmeanings\tdepending\ton\tthe context,\tbut\tgenerally\tmeans\tan\tattribute\tplus\tits\tvalue\t(e.g.,\t“Mileage\t=\t15,000”).\tMany\tpeople\tuse\tthe\twords\tattribute\tand feature\tinterchangeably,\tthough.\n\nFigure\t1-6.\tRegression\n\nNote\tthat\tsome\tregression\talgorithms\tcan\tbe\tused\tfor\tclassification\tas\twell,\tand\tvice\tversa.\tFor\texample, Logistic\tRegression\tis\tcommonly\tused\tfor\tclassification,\tas\tit\tcan\toutput\ta\tvalue\tthat\tcorresponds\tto\tthe probability\tof\tbelonging\tto\ta\tgiven\tclass\t(e.g.,\t20%\tchance\tof\tbeing\tspam).\n\nHere\tare\tsome\tof\tthe\tmost\timportant\tsupervised\tlearning\talgorithms\t(covered\tin\tthis\tbook):\n\nk-Nearest\tNeighbors\n\nLinear\tRegression\n\nLogistic\tRegression\n\nSupport\tVector\tMachines\t(SVMs)\n\nDecision\tTrees\tand\tRandom\tForests\n\nNeural\tnetworks2\n\nUnsupervised\tlearning\n\nIn\tunsupervised\tlearning,\tas\tyou\tmight\tguess,\tthe\ttraining\tdata\tis\tunlabeled\t(Figure\t1-7).\tThe\tsystem\ttries to\tlearn\twithout\ta\tteacher.\n\nFigure\t1-7.\tAn\tunlabeled\ttraining\tset\tfor\tunsupervised\tlearning\n\nHere\tare\tsome\tof\tthe\tmost\timportant\tunsupervised\tlearning\talgorithms\t(we\twill\tcover\tdimensionality reduction\tin\tChapter\t8):\n\nClustering\n\nk-Means\n\nHierarchical\tCluster\tAnalysis\t(HCA)\n\nExpectation\tMaximization\n\nVisualization\tand\tdimensionality\treduction\n\nPrincipal\tComponent\tAnalysis\t(PCA)\n\nKernel\tPCA\n\nLocally-Linear\tEmbedding\t(LLE)\n\nt-distributed\tStochastic\tNeighbor\tEmbedding\t(t-SNE)\n\nAssociation\trule\tlearning\n\nApriori\n\nEclat\n\nFor\texample,\tsay\tyou\thave\ta\tlot\tof\tdata\tabout\tyour\tblog’s\tvisitors.\tYou\tmay\twant\tto\trun\ta\tclustering algorithm\tto\ttry\tto\tdetect\tgroups\tof\tsimilar\tvisitors\t(Figure\t1-8).\tAt\tno\tpoint\tdo\tyou\ttell\tthe\talgorithm which\tgroup\ta\tvisitor\tbelongs\tto:\tit\tfinds\tthose\tconnections\twithout\tyour\thelp.\tFor\texample,\tit\tmight notice\tthat\t40%\tof\tyour\tvisitors\tare\tmales\twho\tlove\tcomic\tbooks\tand\tgenerally\tread\tyour\tblog\tin\tthe evening,\twhile\t20%\tare\tyoung\tsci-fi\tlovers\twho\tvisit\tduring\tthe\tweekends,\tand\tso\ton.\tIf\tyou\tuse\ta hierarchical\tclustering\talgorithm,\tit\tmay\talso\tsubdivide\teach\tgroup\tinto\tsmaller\tgroups.\tThis\tmay\thelp you\ttarget\tyour\tposts\tfor\teach\tgroup.\n\nFigure\t1-8.\tClustering\n\nVisualization\talgorithms\tare\talso\tgood\texamples\tof\tunsupervised\tlearning\talgorithms:\tyou\tfeed\tthem\ta\tlot of\tcomplex\tand\tunlabeled\tdata,\tand\tthey\toutput\ta\t2D\tor\t3D\trepresentation\tof\tyour\tdata\tthat\tcan\teasily\tbe plotted\t(Figure\t1-9).\tThese\talgorithms\ttry\tto\tpreserve\tas\tmuch\tstructure\tas\tthey\tcan\t(e.g.,\ttrying\tto\tkeep separate\tclusters\tin\tthe\tinput\tspace\tfrom\toverlapping\tin\tthe\tvisualization),\tso\tyou\tcan\tunderstand\thow\tthe data\tis\torganized\tand\tperhaps\tidentify\tunsuspected\tpatterns.\n\nFigure\t1-9.\tExample\tof\ta\tt-SNE\tvisualization\thighlighting\tsemantic\tclusters3\n\nA\trelated\ttask\tis\tdimensionality\treduction,\tin\twhich\tthe\tgoal\tis\tto\tsimplify\tthe\tdata\twithout\tlosing\ttoo much\tinformation.\tOne\tway\tto\tdo\tthis\tis\tto\tmerge\tseveral\tcorrelated\tfeatures\tinto\tone.\tFor\texample,\ta car’s\tmileage\tmay\tbe\tvery\tcorrelated\twith\tits\tage,\tso\tthe\tdimensionality\treduction\talgorithm\twill\tmerge them\tinto\tone\tfeature\tthat\trepresents\tthe\tcar’s\twear\tand\ttear.\tThis\tis\tcalled\tfeature\textraction.\n\nTIP\n\nIt\tis\toften\ta\tgood\tidea\tto\ttry\tto\treduce\tthe\tdimension\tof\tyour\ttraining\tdata\tusing\ta\tdimensionality\treduction\talgorithm\tbefore\tyou feed\tit\tto\tanother\tMachine\tLearning\talgorithm\t(such\tas\ta\tsupervised\tlearning\talgorithm).\tIt\twill\trun\tmuch\tfaster,\tthe\tdata\twill\ttake up\tless\tdisk\tand\tmemory\tspace,\tand\tin\tsome\tcases\tit\tmay\talso\tperform\tbetter.\n\nYet\tanother\timportant\tunsupervised\ttask\tis\tanomaly\tdetection\t—\tfor\texample,\tdetecting\tunusual\tcredit card\ttransactions\tto\tprevent\tfraud,\tcatching\tmanufacturing\tdefects,\tor\tautomatically\tremoving\toutliers from\ta\tdataset\tbefore\tfeeding\tit\tto\tanother\tlearning\talgorithm.\tThe\tsystem\tis\ttrained\twith\tnormal instances,\tand\twhen\tit\tsees\ta\tnew\tinstance\tit\tcan\ttell\twhether\tit\tlooks\tlike\ta\tnormal\tone\tor\twhether\tit\tis likely\tan\tanomaly\t(see\tFigure\t1-10).\n\nFigure\t1-10.\tAnomaly\tdetection\n\nFinally,\tanother\tcommon\tunsupervised\ttask\tis\tassociation\trule\tlearning,\tin\twhich\tthe\tgoal\tis\tto\tdig\tinto large\tamounts\tof\tdata\tand\tdiscover\tinteresting\trelations\tbetween\tattributes.\tFor\texample,\tsuppose\tyou own\ta\tsupermarket.\tRunning\tan\tassociation\trule\ton\tyour\tsales\tlogs\tmay\treveal\tthat\tpeople\twho\tpurchase barbecue\tsauce\tand\tpotato\tchips\talso\ttend\tto\tbuy\tsteak.\tThus,\tyou\tmay\twant\tto\tplace\tthese\titems\tclose\tto each\tother.\n\nSemisupervised\tlearning\n\nSome\talgorithms\tcan\tdeal\twith\tpartially\tlabeled\ttraining\tdata,\tusually\ta\tlot\tof\tunlabeled\tdata\tand\ta\tlittle bit\tof\tlabeled\tdata.\tThis\tis\tcalled\tsemisupervised\tlearning\t(Figure\t1-11).\n\nSome\tphoto-hosting\tservices,\tsuch\tas\tGoogle\tPhotos,\tare\tgood\texamples\tof\tthis.\tOnce\tyou\tupload\tall\tyour family\tphotos\tto\tthe\tservice,\tit\tautomatically\trecognizes\tthat\tthe\tsame\tperson\tA\tshows\tup\tin\tphotos\t1,\t5, and\t11,\twhile\tanother\tperson\tB\tshows\tup\tin\tphotos\t2,\t5,\tand\t7.\tThis\tis\tthe\tunsupervised\tpart\tof\tthe algorithm\t(clustering).\tNow\tall\tthe\tsystem\tneeds\tis\tfor\tyou\tto\ttell\tit\twho\tthese\tpeople\tare.\tJust\tone\tlabel per\tperson,4\tand\tit\tis\table\tto\tname\teveryone\tin\tevery\tphoto,\twhich\tis\tuseful\tfor\tsearching\tphotos.\n\nFigure\t1-11.\tSemisupervised\tlearning\n\nMost\tsemisupervised\tlearning\talgorithms\tare\tcombinations\tof\tunsupervised\tand\tsupervised\talgorithms. For\texample,\tdeep\tbelief\tnetworks\t(DBNs)\tare\tbased\ton\tunsupervised\tcomponents\tcalled\trestricted Boltzmann\tmachines\t(RBMs)\tstacked\ton\ttop\tof\tone\tanother.\tRBMs\tare\ttrained\tsequentially\tin\tan unsupervised\tmanner,\tand\tthen\tthe\twhole\tsystem\tis\tfine-tuned\tusing\tsupervised\tlearning\ttechniques.\n\nReinforcement\tLearning\n\nReinforcement\tLearning\tis\ta\tvery\tdifferent\tbeast.\tThe\tlearning\tsystem,\tcalled\tan\tagent\tin\tthis\tcontext, can\tobserve\tthe\tenvironment,\tselect\tand\tperform\tactions,\tand\tget\trewards\tin\treturn\t(or\tpenalties\tin\tthe form\tof\tnegative\trewards,\tas\tin\tFigure\t1-12).\tIt\tmust\tthen\tlearn\tby\titself\twhat\tis\tthe\tbest\tstrategy,\tcalled\ta policy,\tto\tget\tthe\tmost\treward\tover\ttime.\tA\tpolicy\tdefines\twhat\taction\tthe\tagent\tshould\tchoose\twhen\tit\tis in\ta\tgiven\tsituation.\n\nFigure\t1-12.\tReinforcement\tLearning\n\nFor\texample,\tmany\trobots\timplement\tReinforcement\tLearning\talgorithms\tto\tlearn\thow\tto\twalk. DeepMind’s\tAlphaGo\tprogram\tis\talso\ta\tgood\texample\tof\tReinforcement\tLearning:\tit\tmade\tthe\theadlines in\tMarch\t2016\twhen\tit\tbeat\tthe\tworld\tchampion\tLee\tSedol\tat\tthe\tgame\tof\tGo.\tIt\tlearned\tits\twinning policy\tby\tanalyzing\tmillions\tof\tgames,\tand\tthen\tplaying\tmany\tgames\tagainst\titself.\tNote\tthat\tlearning\twas turned\toff\tduring\tthe\tgames\tagainst\tthe\tchampion;\tAlphaGo\twas\tjust\tapplying\tthe\tpolicy\tit\thad\tlearned.\n\nBatch\tand\tOnline\tLearning Another\tcriterion\tused\tto\tclassify\tMachine\tLearning\tsystems\tis\twhether\tor\tnot\tthe\tsystem\tcan\tlearn incrementally\tfrom\ta\tstream\tof\tincoming\tdata.\n\nBatch\tlearning\n\nIn\tbatch\tlearning,\tthe\tsystem\tis\tincapable\tof\tlearning\tincrementally:\tit\tmust\tbe\ttrained\tusing\tall\tthe available\tdata.\tThis\twill\tgenerally\ttake\ta\tlot\tof\ttime\tand\tcomputing\tresources,\tso\tit\tis\ttypically\tdone offline.\tFirst\tthe\tsystem\tis\ttrained,\tand\tthen\tit\tis\tlaunched\tinto\tproduction\tand\truns\twithout\tlearning anymore;\tit\tjust\tapplies\twhat\tit\thas\tlearned.\tThis\tis\tcalled\toffline\tlearning.\n\nIf\tyou\twant\ta\tbatch\tlearning\tsystem\tto\tknow\tabout\tnew\tdata\t(such\tas\ta\tnew\ttype\tof\tspam),\tyou\tneed\tto train\ta\tnew\tversion\tof\tthe\tsystem\tfrom\tscratch\ton\tthe\tfull\tdataset\t(not\tjust\tthe\tnew\tdata,\tbut\talso\tthe\told data),\tthen\tstop\tthe\told\tsystem\tand\treplace\tit\twith\tthe\tnew\tone.\n\nFortunately,\tthe\twhole\tprocess\tof\ttraining,\tevaluating,\tand\tlaunching\ta\tMachine\tLearning\tsystem\tcan\tbe automated\tfairly\teasily\t(as\tshown\tin\tFigure\t1-3),\tso\teven\ta\tbatch\tlearning\tsystem\tcan\tadapt\tto\tchange. Simply\tupdate\tthe\tdata\tand\ttrain\ta\tnew\tversion\tof\tthe\tsystem\tfrom\tscratch\tas\toften\tas\tneeded.\n\nThis\tsolution\tis\tsimple\tand\toften\tworks\tfine,\tbut\ttraining\tusing\tthe\tfull\tset\tof\tdata\tcan\ttake\tmany\thours,\tso you\twould\ttypically\ttrain\ta\tnew\tsystem\tonly\tevery\t24\thours\tor\teven\tjust\tweekly.\tIf\tyour\tsystem\tneeds\tto adapt\tto\trapidly\tchanging\tdata\t(e.g.,\tto\tpredict\tstock\tprices),\tthen\tyou\tneed\ta\tmore\treactive\tsolution.\n\nAlso,\ttraining\ton\tthe\tfull\tset\tof\tdata\trequires\ta\tlot\tof\tcomputing\tresources\t(CPU,\tmemory\tspace,\tdisk space,\tdisk\tI/O,\tnetwork\tI/O,\tetc.).\tIf\tyou\thave\ta\tlot\tof\tdata\tand\tyou\tautomate\tyour\tsystem\tto\ttrain\tfrom scratch\tevery\tday,\tit\twill\tend\tup\tcosting\tyou\ta\tlot\tof\tmoney.\tIf\tthe\tamount\tof\tdata\tis\thuge,\tit\tmay\teven\tbe impossible\tto\tuse\ta\tbatch\tlearning\talgorithm.\n\nFinally,\tif\tyour\tsystem\tneeds\tto\tbe\table\tto\tlearn\tautonomously\tand\tit\thas\tlimited\tresources\t(e.g.,\ta smartphone\tapplication\tor\ta\trover\ton\tMars),\tthen\tcarrying\taround\tlarge\tamounts\tof\ttraining\tdata\tand taking\tup\ta\tlot\tof\tresources\tto\ttrain\tfor\thours\tevery\tday\tis\ta\tshowstopper.\n\nFortunately,\ta\tbetter\toption\tin\tall\tthese\tcases\tis\tto\tuse\talgorithms\tthat\tare\tcapable\tof\tlearning incrementally.\n\nOnline\tlearning\n\nIn\tonline\tlearning,\tyou\ttrain\tthe\tsystem\tincrementally\tby\tfeeding\tit\tdata\tinstances\tsequentially,\teither individually\tor\tby\tsmall\tgroups\tcalled\tmini-batches.\tEach\tlearning\tstep\tis\tfast\tand\tcheap,\tso\tthe\tsystem can\tlearn\tabout\tnew\tdata\ton\tthe\tfly,\tas\tit\tarrives\t(see\tFigure\t1-13).\n\nFigure\t1-13.\tOnline\tlearning\n\nOnline\tlearning\tis\tgreat\tfor\tsystems\tthat\treceive\tdata\tas\ta\tcontinuous\tflow\t(e.g.,\tstock\tprices)\tand\tneed\tto adapt\tto\tchange\trapidly\tor\tautonomously.\tIt\tis\talso\ta\tgood\toption\tif\tyou\thave\tlimited\tcomputing\tresources: once\tan\tonline\tlearning\tsystem\thas\tlearned\tabout\tnew\tdata\tinstances,\tit\tdoes\tnot\tneed\tthem\tanymore,\tso you\tcan\tdiscard\tthem\t(unless\tyou\twant\tto\tbe\table\tto\troll\tback\tto\ta\tprevious\tstate\tand\t“replay”\tthe\tdata). This\tcan\tsave\ta\thuge\tamount\tof\tspace.\n\nOnline\tlearning\talgorithms\tcan\talso\tbe\tused\tto\ttrain\tsystems\ton\thuge\tdatasets\tthat\tcannot\tfit\tin\tone machine’s\tmain\tmemory\t(this\tis\tcalled\tout-of-core\tlearning).\tThe\talgorithm\tloads\tpart\tof\tthe\tdata,\truns\ta training\tstep\ton\tthat\tdata,\tand\trepeats\tthe\tprocess\tuntil\tit\thas\trun\ton\tall\tof\tthe\tdata\t(see\tFigure\t1-14).\n\nWARNING\n\nThis\twhole\tprocess\tis\tusually\tdone\toffline\t(i.e.,\tnot\ton\tthe\tlive\tsystem),\tso\tonline\tlearning\tcan\tbe\ta\tconfusing\tname.\tThink\tof\tit as\tincremental\tlearning.\n\nFigure\t1-14.\tUsing\tonline\tlearning\tto\thandle\thuge\tdatasets\n\nOne\timportant\tparameter\tof\tonline\tlearning\tsystems\tis\thow\tfast\tthey\tshould\tadapt\tto\tchanging\tdata:\tthis\tis called\tthe\tlearning\trate.\tIf\tyou\tset\ta\thigh\tlearning\trate,\tthen\tyour\tsystem\twill\trapidly\tadapt\tto\tnew\tdata, but\tit\twill\talso\ttend\tto\tquickly\tforget\tthe\told\tdata\t(you\tdon’t\twant\ta\tspam\tfilter\tto\tflag\tonly\tthe\tlatest\tkinds of\tspam\tit\twas\tshown).\tConversely,\tif\tyou\tset\ta\tlow\tlearning\trate,\tthe\tsystem\twill\thave\tmore\tinertia;\tthat is,\tit\twill\tlearn\tmore\tslowly,\tbut\tit\twill\talso\tbe\tless\tsensitive\tto\tnoise\tin\tthe\tnew\tdata\tor\tto\tsequences\tof nonrepresentative\tdata\tpoints.\n\nA\tbig\tchallenge\twith\tonline\tlearning\tis\tthat\tif\tbad\tdata\tis\tfed\tto\tthe\tsystem,\tthe\tsystem’s\tperformance\twill gradually\tdecline.\tIf\twe\tare\ttalking\tabout\ta\tlive\tsystem,\tyour\tclients\twill\tnotice.\tFor\texample,\tbad\tdata could\tcome\tfrom\ta\tmalfunctioning\tsensor\ton\ta\trobot,\tor\tfrom\tsomeone\tspamming\ta\tsearch\tengine\tto\ttry\tto rank\thigh\tin\tsearch\tresults.\tTo\treduce\tthis\trisk,\tyou\tneed\tto\tmonitor\tyour\tsystem\tclosely\tand\tpromptly switch\tlearning\toff\t(and\tpossibly\trevert\tto\ta\tpreviously\tworking\tstate)\tif\tyou\tdetect\ta\tdrop\tin performance.\tYou\tmay\talso\twant\tto\tmonitor\tthe\tinput\tdata\tand\treact\tto\tabnormal\tdata\t(e.g.,\tusing\tan anomaly\tdetection\talgorithm).\n\nInstance-Based\tVersus\tModel-Based\tLearning One\tmore\tway\tto\tcategorize\tMachine\tLearning\tsystems\tis\tby\thow\tthey\tgeneralize.\tMost\tMachine Learning\ttasks\tare\tabout\tmaking\tpredictions.\tThis\tmeans\tthat\tgiven\ta\tnumber\tof\ttraining\texamples,\tthe system\tneeds\tto\tbe\table\tto\tgeneralize\tto\texamples\tit\thas\tnever\tseen\tbefore.\tHaving\ta\tgood\tperformance measure\ton\tthe\ttraining\tdata\tis\tgood,\tbut\tinsufficient;\tthe\ttrue\tgoal\tis\tto\tperform\twell\ton\tnew\tinstances.\n\nThere\tare\ttwo\tmain\tapproaches\tto\tgeneralization:\tinstance-based\tlearning\tand\tmodel-based\tlearning.\n\nInstance-based\tlearning\n\nPossibly\tthe\tmost\ttrivial\tform\tof\tlearning\tis\tsimply\tto\tlearn\tby\theart.\tIf\tyou\twere\tto\tcreate\ta\tspam\tfilter this\tway,\tit\twould\tjust\tflag\tall\temails\tthat\tare\tidentical\tto\temails\tthat\thave\talready\tbeen\tflagged\tby\tusers —\tnot\tthe\tworst\tsolution,\tbut\tcertainly\tnot\tthe\tbest.\n\nInstead\tof\tjust\tflagging\temails\tthat\tare\tidentical\tto\tknown\tspam\temails,\tyour\tspam\tfilter\tcould\tbe programmed\tto\talso\tflag\temails\tthat\tare\tvery\tsimilar\tto\tknown\tspam\temails.\tThis\trequires\ta\tmeasure\tof similarity\tbetween\ttwo\temails.\tA\t(very\tbasic)\tsimilarity\tmeasure\tbetween\ttwo\temails\tcould\tbe\tto\tcount the\tnumber\tof\twords\tthey\thave\tin\tcommon.\tThe\tsystem\twould\tflag\tan\temail\tas\tspam\tif\tit\thas\tmany\twords in\tcommon\twith\ta\tknown\tspam\temail.\n\nThis\tis\tcalled\tinstance-based\tlearning:\tthe\tsystem\tlearns\tthe\texamples\tby\theart,\tthen\tgeneralizes\tto\tnew cases\tusing\ta\tsimilarity\tmeasure\t(Figure\t1-15).\n\nFigure\t1-15.\tInstance-based\tlearning\n\nModel-based\tlearning\n\nAnother\tway\tto\tgeneralize\tfrom\ta\tset\tof\texamples\tis\tto\tbuild\ta\tmodel\tof\tthese\texamples,\tthen\tuse\tthat model\tto\tmake\tpredictions.\tThis\tis\tcalled\tmodel-based\tlearning\t(Figure\t1-16).\n\nFigure\t1-16.\tModel-based\tlearning\n\nFor\texample,\tsuppose\tyou\twant\tto\tknow\tif\tmoney\tmakes\tpeople\thappy,\tso\tyou\tdownload\tthe\tBetter\tLife Index\tdata\tfrom\tthe\tOECD’s\twebsite\tas\twell\tas\tstats\tabout\tGDP\tper\tcapita\tfrom\tthe\tIMF’s\twebsite.\tThen you\tjoin\tthe\ttables\tand\tsort\tby\tGDP\tper\tcapita.\tTable\t1-1\tshows\tan\texcerpt\tof\twhat\tyou\tget.\n\nTable\t1-1.\tDoes\tmoney\tmake\tpeople happier?\n\nCountry\n\nGDP\tper\tcapita\t(USD) Life\tsatisfaction\n\nHungary\n\n12,240\n\n4.9\n\nKorea\n\n27,195\n\n5.8\n\nFrance\n\n37,675\n\n6.5\n\nAustralia\n\n50,962\n\n7.3\n\nUnited\tStates 55,805\n\n7.2\n\nLet’s\tplot\tthe\tdata\tfor\ta\tfew\trandom\tcountries\t(Figure\t1-17).\n\nFigure\t1-17.\tDo\tyou\tsee\ta\ttrend\there?\n\nThere\tdoes\tseem\tto\tbe\ta\ttrend\there!\tAlthough\tthe\tdata\tis\tnoisy\t(i.e.,\tpartly\trandom),\tit\tlooks\tlike\tlife satisfaction\tgoes\tup\tmore\tor\tless\tlinearly\tas\tthe\tcountry’s\tGDP\tper\tcapita\tincreases.\tSo\tyou\tdecide\tto model\tlife\tsatisfaction\tas\ta\tlinear\tfunction\tof\tGDP\tper\tcapita.\tThis\tstep\tis\tcalled\tmodel\tselection:\tyou selected\ta\tlinear\tmodel\tof\tlife\tsatisfaction\twith\tjust\tone\tattribute,\tGDP\tper\tcapita\t(Equation\t1-1).\n\nEquation\t1-1.\tA\tsimple\tlinear\tmodel\n\nThis\tmodel\thas\ttwo\tmodel\tparameters,\tθ0\tand\tθ1.5\tBy\ttweaking\tthese\tparameters,\tyou\tcan\tmake\tyour model\trepresent\tany\tlinear\tfunction,\tas\tshown\tin\tFigure\t1-18.\n\nFigure\t1-18.\tA\tfew\tpossible\tlinear\tmodels\n\nBefore\tyou\tcan\tuse\tyour\tmodel,\tyou\tneed\tto\tdefine\tthe\tparameter\tvalues\tθ0\tand\tθ1.\tHow\tcan\tyou\tknow\n\nwhich\tvalues\twill\tmake\tyour\tmodel\tperform\tbest?\tTo\tanswer\tthis\tquestion,\tyou\tneed\tto\tspecify\ta performance\tmeasure.\tYou\tcan\teither\tdefine\ta\tutility\tfunction\t(or\tfitness\tfunction)\tthat\tmeasures\thow good\tyour\tmodel\tis,\tor\tyou\tcan\tdefine\ta\tcost\tfunction\tthat\tmeasures\thow\tbad\tit\tis.\tFor\tlinear\tregression problems,\tpeople\ttypically\tuse\ta\tcost\tfunction\tthat\tmeasures\tthe\tdistance\tbetween\tthe\tlinear\tmodel’s predictions\tand\tthe\ttraining\texamples;\tthe\tobjective\tis\tto\tminimize\tthis\tdistance.\n\nThis\tis\twhere\tthe\tLinear\tRegression\talgorithm\tcomes\tin:\tyou\tfeed\tit\tyour\ttraining\texamples\tand\tit\tfinds the\tparameters\tthat\tmake\tthe\tlinear\tmodel\tfit\tbest\tto\tyour\tdata.\tThis\tis\tcalled\ttraining\tthe\tmodel.\tIn\tour case\tthe\talgorithm\tfinds\tthat\tthe\toptimal\tparameter\tvalues\tare\tθ0\t=\t4.85\tand\tθ1\t=\t4.91\t×\t10–5. Now\tthe\tmodel\tfits\tthe\ttraining\tdata\tas\tclosely\tas\tpossible\t(for\ta\tlinear\tmodel),\tas\tyou\tcan\tsee\tin Figure\t1-19.\n\nFigure\t1-19.\tThe\tlinear\tmodel\tthat\tfits\tthe\ttraining\tdata\tbest\n\nYou\tare\tfinally\tready\tto\trun\tthe\tmodel\tto\tmake\tpredictions.\tFor\texample,\tsay\tyou\twant\tto\tknow\thow happy\tCypriots\tare,\tand\tthe\tOECD\tdata\tdoes\tnot\thave\tthe\tanswer.\tFortunately,\tyou\tcan\tuse\tyour\tmodel\tto make\ta\tgood\tprediction:\tyou\tlook\tup\tCyprus’s\tGDP\tper\tcapita,\tfind\t$22,587,\tand\tthen\tapply\tyour\tmodel and\tfind\tthat\tlife\tsatisfaction\tis\tlikely\tto\tbe\tsomewhere\taround\t4.85\t+\t22,587\t×\t4.91\t×\t10-5\t=\t5.96. To\twhet\tyour\tappetite,\tExample\t1-1\tshows\tthe\tPython\tcode\tthat\tloads\tthe\tdata,\tprepares\tit,6\tcreates\ta scatterplot\tfor\tvisualization,\tand\tthen\ttrains\ta\tlinear\tmodel\tand\tmakes\ta\tprediction.7\n\nExample\t1-1.\tTraining\tand\trunning\ta\tlinear\tmodel\tusing\tScikit-Learn import\tmatplotlib import\tmatplotlib.pyplot\tas\tplt import\tnumpy\tas\tnp import\tpandas\tas\tpd import\tsklearn\n\n#\tLoad\tthe\tdata oecd_bli\t=\tpd.read_csv(\"oecd_bli_2015.csv\",\tthousands=',') gdp_per_capita\t=\tpd.read_csv(\"gdp_per_capita.csv\",thousands=',',delimiter='\\t', \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tencoding='latin1',\tna_values=\"n/a\")\n\n#\tPrepare\tthe\tdata country_stats\t=\tprepare_country_stats(oecd_bli,\tgdp_per_capita) X\t=\tnp.c_[country_stats[\"GDP\tper\tcapita\"]]\n\ny\t=\tnp.c_[country_stats[\"Life\tsatisfaction\"]]\n\n#\tVisualize\tthe\tdata country_stats.plot(kind='scatter',\tx=\"GDP\tper\tcapita\",\ty='Life\tsatisfaction') plt.show()\n\n#\tSelect\ta\tlinear\tmodel model\t=\tsklearn.linear_model.LinearRegression()\n\n#\tTrain\tthe\tmodel model.fit(X,\ty)\n\n#\tMake\ta\tprediction\tfor\tCyprus X_new\t=\t[[22587]]\t\t#\tCyprus'\tGDP\tper\tcapita print(model.predict(X_new))\t#\toutputs\t[[\t5.96242338]]\n\nNOTE\n\nIf\tyou\thad\tused\tan\tinstance-based\tlearning\talgorithm\tinstead,\tyou\twould\thave\tfound\tthat\tSlovenia\thas\tthe\tclosest\tGDP\tper\tcapita to\tthat\tof\tCyprus\t($20,732),\tand\tsince\tthe\tOECD\tdata\ttells\tus\tthat\tSlovenians’\tlife\tsatisfaction\tis\t5.7,\tyou\twould\thave\tpredicted\ta life\tsatisfaction\tof\t5.7\tfor\tCyprus.\tIf\tyou\tzoom\tout\ta\tbit\tand\tlook\tat\tthe\ttwo\tnext\tclosest\tcountries,\tyou\twill\tfind\tPortugal\tand Spain\twith\tlife\tsatisfactions\tof\t5.1\tand\t6.5,\trespectively.\tAveraging\tthese\tthree\tvalues,\tyou\tget\t5.77,\twhich\tis\tpretty\tclose\tto\tyour model-based\tprediction.\tThis\tsimple\talgorithm\tis\tcalled\tk-Nearest\tNeighbors\tregression\t(in\tthis\texample,\tk\t=\t3).\n\nReplacing\tthe\tLinear\tRegression\tmodel\twith\tk-Nearest\tNeighbors\tregression\tin\tthe\tprevious\tcode\tis\tas\tsimple\tas\treplacing\tthis line:\n\nmodel\t=\tsklearn.linear_model.LinearRegression()\n\nwith\tthis\tone:\n\nmodel\t=\tsklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\n\nIf\tall\twent\twell,\tyour\tmodel\twill\tmake\tgood\tpredictions.\tIf\tnot,\tyou\tmay\tneed\tto\tuse\tmore\tattributes (employment\trate,\thealth,\tair\tpollution,\tetc.),\tget\tmore\tor\tbetter\tquality\ttraining\tdata,\tor\tperhaps\tselect\ta more\tpowerful\tmodel\t(e.g.,\ta\tPolynomial\tRegression\tmodel).\n\nIn\tsummary:\n\nYou\tstudied\tthe\tdata.\n\nYou\tselected\ta\tmodel.\n\nYou\ttrained\tit\ton\tthe\ttraining\tdata\t(i.e.,\tthe\tlearning\talgorithm\tsearched\tfor\tthe\tmodel\tparameter values\tthat\tminimize\ta\tcost\tfunction).\n\nFinally,\tyou\tapplied\tthe\tmodel\tto\tmake\tpredictions\ton\tnew\tcases\t(this\tis\tcalled\tinference),\thoping that\tthis\tmodel\twill\tgeneralize\twell.\n\nThis\tis\twhat\ta\ttypical\tMachine\tLearning\tproject\tlooks\tlike.\tIn\tChapter\t2\tyou\twill\texperience\tthis\tfirst- hand\tby\tgoing\tthrough\tan\tend-to-end\tproject.\n\nWe\thave\tcovered\ta\tlot\tof\tground\tso\tfar:\tyou\tnow\tknow\twhat\tMachine\tLearning\tis\treally\tabout,\twhy\tit\tis useful,\twhat\tsome\tof\tthe\tmost\tcommon\tcategories\tof\tML\tsystems\tare,\tand\twhat\ta\ttypical\tproject\tworkflow looks\tlike.\tNow\tlet’s\tlook\tat\twhat\tcan\tgo\twrong\tin\tlearning\tand\tprevent\tyou\tfrom\tmaking\taccurate\n\npredictions.\n\nMain\tChallenges\tof\tMachine\tLearning In\tshort,\tsince\tyour\tmain\ttask\tis\tto\tselect\ta\tlearning\talgorithm\tand\ttrain\tit\ton\tsome\tdata,\tthe\ttwo\tthings\tthat can\tgo\twrong\tare\t“bad\talgorithm”\tand\t“bad\tdata.”\tLet’s\tstart\twith\texamples\tof\tbad\tdata.\n\nInsufficient\tQuantity\tof\tTraining\tData For\ta\ttoddler\tto\tlearn\twhat\tan\tapple\tis,\tall\tit\ttakes\tis\tfor\tyou\tto\tpoint\tto\tan\tapple\tand\tsay\t“apple” (possibly\trepeating\tthis\tprocedure\ta\tfew\ttimes).\tNow\tthe\tchild\tis\table\tto\trecognize\tapples\tin\tall\tsorts\tof colors\tand\tshapes.\tGenius.\n\nMachine\tLearning\tis\tnot\tquite\tthere\tyet;\tit\ttakes\ta\tlot\tof\tdata\tfor\tmost\tMachine\tLearning\talgorithms\tto work\tproperly.\tEven\tfor\tvery\tsimple\tproblems\tyou\ttypically\tneed\tthousands\tof\texamples,\tand\tfor complex\tproblems\tsuch\tas\timage\tor\tspeech\trecognition\tyou\tmay\tneed\tmillions\tof\texamples\t(unless\tyou can\treuse\tparts\tof\tan\texisting\tmodel).\n\nTHE\tUNREASONABLE\tEFFECTIVENESS\tOF\tDATA\n\nIn\ta\tfamous\tpaper\tpublished\tin\t2001,\tMicrosoft\tresearchers\tMichele\tBanko\tand\tEric\tBrill\tshowed\tthat\tvery\tdifferent\tMachine\tLearning algorithms,\tincluding\tfairly\tsimple\tones,\tperformed\talmost\tidentically\twell\ton\ta\tcomplex\tproblem\tof\tnatural\tlanguage\tdisambiguation8\tonce they\twere\tgiven\tenough\tdata\t(as\tyou\tcan\tsee\tin\tFigure\t1-20).\n\nFigure\t1-20.\tThe\timportance\tof\tdata\tversus\talgorithms9\n\nAs\tthe\tauthors\tput\tit:\t“these\tresults\tsuggest\tthat\twe\tmay\twant\tto\treconsider\tthe\ttrade-off\tbetween\tspending\ttime\tand\tmoney\ton\talgorithm development\tversus\tspending\tit\ton\tcorpus\tdevelopment.”\n\nThe\tidea\tthat\tdata\tmatters\tmore\tthan\talgorithms\tfor\tcomplex\tproblems\twas\tfurther\tpopularized\tby\tPeter\tNorvig\tet\tal.\tin\ta\tpaper\ttitled “The\tUnreasonable\tEffectiveness\tof\tData”\tpublished\tin\t2009.10\tIt\tshould\tbe\tnoted,\thowever,\tthat\tsmall-\tand\tmedium-sized\tdatasets\tare still\tvery\tcommon,\tand\tit\tis\tnot\talways\teasy\tor\tcheap\tto\tget\textra\ttraining\tdata,\tso\tdon’t\tabandon\talgorithms\tjust\tyet.\n\nNonrepresentative\tTraining\tData In\torder\tto\tgeneralize\twell,\tit\tis\tcrucial\tthat\tyour\ttraining\tdata\tbe\trepresentative\tof\tthe\tnew\tcases\tyou want\tto\tgeneralize\tto.\tThis\tis\ttrue\twhether\tyou\tuse\tinstance-based\tlearning\tor\tmodel-based\tlearning.\n\nFor\texample,\tthe\tset\tof\tcountries\twe\tused\tearlier\tfor\ttraining\tthe\tlinear\tmodel\twas\tnot\tperfectly representative;\ta\tfew\tcountries\twere\tmissing.\tFigure\t1-21\tshows\twhat\tthe\tdata\tlooks\tlike\twhen\tyou\tadd the\tmissing\tcountries.\n\nFigure\t1-21.\tA\tmore\trepresentative\ttraining\tsample\n\nIf\tyou\ttrain\ta\tlinear\tmodel\ton\tthis\tdata,\tyou\tget\tthe\tsolid\tline,\twhile\tthe\told\tmodel\tis\trepresented\tby\tthe dotted\tline.\tAs\tyou\tcan\tsee,\tnot\tonly\tdoes\tadding\ta\tfew\tmissing\tcountries\tsignificantly\talter\tthe\tmodel,\tbut it\tmakes\tit\tclear\tthat\tsuch\ta\tsimple\tlinear\tmodel\tis\tprobably\tnever\tgoing\tto\twork\twell.\tIt\tseems\tthat\tvery rich\tcountries\tare\tnot\thappier\tthan\tmoderately\trich\tcountries\t(in\tfact\tthey\tseem\tunhappier),\tand conversely\tsome\tpoor\tcountries\tseem\thappier\tthan\tmany\trich\tcountries.\n\nBy\tusing\ta\tnonrepresentative\ttraining\tset,\twe\ttrained\ta\tmodel\tthat\tis\tunlikely\tto\tmake\taccurate predictions,\tespecially\tfor\tvery\tpoor\tand\tvery\trich\tcountries.\n\nIt\tis\tcrucial\tto\tuse\ta\ttraining\tset\tthat\tis\trepresentative\tof\tthe\tcases\tyou\twant\tto\tgeneralize\tto.\tThis\tis\toften harder\tthan\tit\tsounds:\tif\tthe\tsample\tis\ttoo\tsmall,\tyou\twill\thave\tsampling\tnoise\t(i.e.,\tnonrepresentative data\tas\ta\tresult\tof\tchance),\tbut\teven\tvery\tlarge\tsamples\tcan\tbe\tnonrepresentative\tif\tthe\tsampling\tmethod is\tflawed.\tThis\tis\tcalled\tsampling\tbias.\n\nA\tFAMOUS\tEXAMPLE\tOF\tSAMPLING\tBIAS\n\nPerhaps\tthe\tmost\tfamous\texample\tof\tsampling\tbias\thappened\tduring\tthe\tUS\tpresidential\telection\tin\t1936,\twhich\tpitted\tLandon\tagainst Roosevelt:\tthe\tLiterary\tDigest\tconducted\ta\tvery\tlarge\tpoll,\tsending\tmail\tto\tabout\t10\tmillion\tpeople.\tIt\tgot\t2.4\tmillion\tanswers,\tand predicted\twith\thigh\tconfidence\tthat\tLandon\twould\tget\t57%\tof\tthe\tvotes.\tInstead,\tRoosevelt\twon\twith\t62%\tof\tthe\tvotes.\tThe\tflaw\twas\tin the\tLiterary\tDigest’s\tsampling\tmethod:\n\nFirst,\tto\tobtain\tthe\taddresses\tto\tsend\tthe\tpolls\tto,\tthe\tLiterary\tDigest\tused\ttelephone\tdirectories,\tlists\tof\tmagazine\tsubscribers,\tclub membership\tlists,\tand\tthe\tlike.\tAll\tof\tthese\tlists\ttend\tto\tfavor\twealthier\tpeople,\twho\tare\tmore\tlikely\tto\tvote\tRepublican\t(hence Landon).\n\nSecond,\tless\tthan\t25%\tof\tthe\tpeople\twho\treceived\tthe\tpoll\tanswered.\tAgain,\tthis\tintroduces\ta\tsampling\tbias,\tby\truling\tout\tpeople who\tdon’t\tcare\tmuch\tabout\tpolitics,\tpeople\twho\tdon’t\tlike\tthe\tLiterary\tDigest,\tand\tother\tkey\tgroups.\tThis\tis\ta\tspecial\ttype\tof sampling\tbias\tcalled\tnonresponse\tbias.\n\nHere\tis\tanother\texample:\tsay\tyou\twant\tto\tbuild\ta\tsystem\tto\trecognize\tfunk\tmusic\tvideos.\tOne\tway\tto\tbuild\tyour\ttraining\tset\tis\tto\tsearch “funk\tmusic”\ton\tYouTube\tand\tuse\tthe\tresulting\tvideos.\tBut\tthis\tassumes\tthat\tYouTube’s\tsearch\tengine\treturns\ta\tset\tof\tvideos\tthat\tare representative\tof\tall\tthe\tfunk\tmusic\tvideos\ton\tYouTube.\tIn\treality,\tthe\tsearch\tresults\tare\tlikely\tto\tbe\tbiased\ttoward\tpopular\tartists\t(and\tif you\tlive\tin\tBrazil\tyou\twill\tget\ta\tlot\tof\t“funk\tcarioca”\tvideos,\twhich\tsound\tnothing\tlike\tJames\tBrown).\tOn\tthe\tother\thand,\thow\telse\tcan you\tget\ta\tlarge\ttraining\tset?\n\nPoor-Quality\tData Obviously,\tif\tyour\ttraining\tdata\tis\tfull\tof\terrors,\toutliers,\tand\tnoise\t(e.g.,\tdue\tto\tpoor-quality measurements),\tit\twill\tmake\tit\tharder\tfor\tthe\tsystem\tto\tdetect\tthe\tunderlying\tpatterns,\tso\tyour\tsystem\tis less\tlikely\tto\tperform\twell.\tIt\tis\toften\twell\tworth\tthe\teffort\tto\tspend\ttime\tcleaning\tup\tyour\ttraining\tdata. The\ttruth\tis,\tmost\tdata\tscientists\tspend\ta\tsignificant\tpart\tof\ttheir\ttime\tdoing\tjust\tthat.\tFor\texample:\n\nIf\tsome\tinstances\tare\tclearly\toutliers,\tit\tmay\thelp\tto\tsimply\tdiscard\tthem\tor\ttry\tto\tfix\tthe\terrors manually.\n\nIf\tsome\tinstances\tare\tmissing\ta\tfew\tfeatures\t(e.g.,\t5%\tof\tyour\tcustomers\tdid\tnot\tspecify\ttheir\tage), you\tmust\tdecide\twhether\tyou\twant\tto\tignore\tthis\tattribute\taltogether,\tignore\tthese\tinstances,\tfill\tin\tthe missing\tvalues\t(e.g.,\twith\tthe\tmedian\tage),\tor\ttrain\tone\tmodel\twith\tthe\tfeature\tand\tone\tmodel\twithout it,\tand\tso\ton.\n\nIrrelevant\tFeatures As\tthe\tsaying\tgoes:\tgarbage\tin,\tgarbage\tout.\tYour\tsystem\twill\tonly\tbe\tcapable\tof\tlearning\tif\tthe\ttraining data\tcontains\tenough\trelevant\tfeatures\tand\tnot\ttoo\tmany\tirrelevant\tones.\tA\tcritical\tpart\tof\tthe\tsuccess\tof\ta Machine\tLearning\tproject\tis\tcoming\tup\twith\ta\tgood\tset\tof\tfeatures\tto\ttrain\ton.\tThis\tprocess,\tcalled feature\tengineering,\tinvolves:\n\nFeature\tselection:\tselecting\tthe\tmost\tuseful\tfeatures\tto\ttrain\ton\tamong\texisting\tfeatures.\n\nFeature\textraction:\tcombining\texisting\tfeatures\tto\tproduce\ta\tmore\tuseful\tone\t(as\twe\tsaw\tearlier, dimensionality\treduction\talgorithms\tcan\thelp).\n\nCreating\tnew\tfeatures\tby\tgathering\tnew\tdata.\n\nNow\tthat\twe\thave\tlooked\tat\tmany\texamples\tof\tbad\tdata,\tlet’s\tlook\tat\ta\tcouple\tof\texamples\tof\tbad algorithms.\n\nOverfitting\tthe\tTraining\tData Say\tyou\tare\tvisiting\ta\tforeign\tcountry\tand\tthe\ttaxi\tdriver\trips\tyou\toff.\tYou\tmight\tbe\ttempted\tto\tsay\tthat\tall taxi\tdrivers\tin\tthat\tcountry\tare\tthieves.\tOvergeneralizing\tis\tsomething\tthat\twe\thumans\tdo\tall\ttoo\toften,\tand unfortunately\tmachines\tcan\tfall\tinto\tthe\tsame\ttrap\tif\twe\tare\tnot\tcareful.\tIn\tMachine\tLearning\tthis\tis\tcalled overfitting:\tit\tmeans\tthat\tthe\tmodel\tperforms\twell\ton\tthe\ttraining\tdata,\tbut\tit\tdoes\tnot\tgeneralize\twell.\n\nFigure\t1-22\tshows\tan\texample\tof\ta\thigh-degree\tpolynomial\tlife\tsatisfaction\tmodel\tthat\tstrongly\toverfits the\ttraining\tdata.\tEven\tthough\tit\tperforms\tmuch\tbetter\ton\tthe\ttraining\tdata\tthan\tthe\tsimple\tlinear\tmodel, would\tyou\treally\ttrust\tits\tpredictions?\n\nFigure\t1-22.\tOverfitting\tthe\ttraining\tdata\n\nComplex\tmodels\tsuch\tas\tdeep\tneural\tnetworks\tcan\tdetect\tsubtle\tpatterns\tin\tthe\tdata,\tbut\tif\tthe\ttraining\tset is\tnoisy,\tor\tif\tit\tis\ttoo\tsmall\t(which\tintroduces\tsampling\tnoise),\tthen\tthe\tmodel\tis\tlikely\tto\tdetect\tpatterns in\tthe\tnoise\titself.\tObviously\tthese\tpatterns\twill\tnot\tgeneralize\tto\tnew\tinstances.\tFor\texample,\tsay\tyou feed\tyour\tlife\tsatisfaction\tmodel\tmany\tmore\tattributes,\tincluding\tuninformative\tones\tsuch\tas\tthe\tcountry’s name.\tIn\tthat\tcase,\ta\tcomplex\tmodel\tmay\tdetect\tpatterns\tlike\tthe\tfact\tthat\tall\tcountries\tin\tthe\ttraining\tdata with\ta\tw\tin\ttheir\tname\thave\ta\tlife\tsatisfaction\tgreater\tthan\t7:\tNew\tZealand\t(7.3),\tNorway\t(7.4),\tSweden (7.2),\tand\tSwitzerland\t(7.5).\tHow\tconfident\tare\tyou\tthat\tthe\tW-satisfaction\trule\tgeneralizes\tto\tRwanda\tor Zimbabwe?\tObviously\tthis\tpattern\toccurred\tin\tthe\ttraining\tdata\tby\tpure\tchance,\tbut\tthe\tmodel\thas\tno\tway to\ttell\twhether\ta\tpattern\tis\treal\tor\tsimply\tthe\tresult\tof\tnoise\tin\tthe\tdata.\n\nWARNING\n\nOverfitting\thappens\twhen\tthe\tmodel\tis\ttoo\tcomplex\trelative\tto\tthe\tamount\tand\tnoisiness\tof\tthe\ttraining\tdata.\tThe\tpossible solutions\tare:\n\nTo\tsimplify\tthe\tmodel\tby\tselecting\tone\twith\tfewer\tparameters\t(e.g.,\ta\tlinear\tmodel\trather\tthan\ta\thigh-degree\tpolynomial model),\tby\treducing\tthe\tnumber\tof\tattributes\tin\tthe\ttraining\tdata\tor\tby\tconstraining\tthe\tmodel\n\nTo\tgather\tmore\ttraining\tdata\n\nTo\treduce\tthe\tnoise\tin\tthe\ttraining\tdata\t(e.g.,\tfix\tdata\terrors\tand\tremove\toutliers)\n\nConstraining\ta\tmodel\tto\tmake\tit\tsimpler\tand\treduce\tthe\trisk\tof\toverfitting\tis\tcalled\tregularization.\tFor\n\nexample,\tthe\tlinear\tmodel\twe\tdefined\tearlier\thas\ttwo\tparameters,\tθ0\tand\tθ1.\tThis\tgives\tthe\tlearning algorithm\ttwo\tdegrees\tof\tfreedom\tto\tadapt\tthe\tmodel\tto\tthe\ttraining\tdata:\tit\tcan\ttweak\tboth\tthe\theight\t(θ0) and\tthe\tslope\t(θ1)\tof\tthe\tline.\tIf\twe\tforced\tθ1\t=\t0,\tthe\talgorithm\twould\thave\tonly\tone\tdegree\tof\tfreedom and\twould\thave\ta\tmuch\tharder\ttime\tfitting\tthe\tdata\tproperly:\tall\tit\tcould\tdo\tis\tmove\tthe\tline\tup\tor\tdown to\tget\tas\tclose\tas\tpossible\tto\tthe\ttraining\tinstances,\tso\tit\twould\tend\tup\taround\tthe\tmean.\tA\tvery\tsimple model\tindeed!\tIf\twe\tallow\tthe\talgorithm\tto\tmodify\tθ1\tbut\twe\tforce\tit\tto\tkeep\tit\tsmall,\tthen\tthe\tlearning algorithm\twill\teffectively\thave\tsomewhere\tin\tbetween\tone\tand\ttwo\tdegrees\tof\tfreedom.\tIt\twill\tproduce\ta simpler\tmodel\tthan\twith\ttwo\tdegrees\tof\tfreedom,\tbut\tmore\tcomplex\tthan\twith\tjust\tone.\tYou\twant\tto\tfind the\tright\tbalance\tbetween\tfitting\tthe\tdata\tperfectly\tand\tkeeping\tthe\tmodel\tsimple\tenough\tto\tensure\tthat\tit will\tgeneralize\twell.\n\nFigure\t1-23\tshows\tthree\tmodels:\tthe\tdotted\tline\trepresents\tthe\toriginal\tmodel\tthat\twas\ttrained\twith\ta\tfew countries\tmissing,\tthe\tdashed\tline\tis\tour\tsecond\tmodel\ttrained\twith\tall\tcountries,\tand\tthe\tsolid\tline\tis\ta linear\tmodel\ttrained\twith\tthe\tsame\tdata\tas\tthe\tfirst\tmodel\tbut\twith\ta\tregularization\tconstraint.\tYou\tcan\tsee that\tregularization\tforced\tthe\tmodel\tto\thave\ta\tsmaller\tslope,\twhich\tfits\ta\tbit\tless\tthe\ttraining\tdata\tthat\tthe model\twas\ttrained\ton,\tbut\tactually\tallows\tit\tto\tgeneralize\tbetter\tto\tnew\texamples.\n\nFigure\t1-23.\tRegularization\treduces\tthe\trisk\tof\toverfitting\n\nThe\tamount\tof\tregularization\tto\tapply\tduring\tlearning\tcan\tbe\tcontrolled\tby\ta\thyperparameter.\tA hyperparameter\tis\ta\tparameter\tof\ta\tlearning\talgorithm\t(not\tof\tthe\tmodel).\tAs\tsuch,\tit\tis\tnot\taffected\tby\tthe learning\talgorithm\titself;\tit\tmust\tbe\tset\tprior\tto\ttraining\tand\tremains\tconstant\tduring\ttraining.\tIf\tyou\tset\tthe regularization\thyperparameter\tto\ta\tvery\tlarge\tvalue,\tyou\twill\tget\tan\talmost\tflat\tmodel\t(a\tslope\tclose\tto zero);\tthe\tlearning\talgorithm\twill\talmost\tcertainly\tnot\toverfit\tthe\ttraining\tdata,\tbut\tit\twill\tbe\tless\tlikely\tto find\ta\tgood\tsolution.\tTuning\thyperparameters\tis\tan\timportant\tpart\tof\tbuilding\ta\tMachine\tLearning\tsystem (you\twill\tsee\ta\tdetailed\texample\tin\tthe\tnext\tchapter).\n\nUnderfitting\tthe\tTraining\tData As\tyou\tmight\tguess,\tunderfitting\tis\tthe\topposite\tof\toverfitting:\tit\toccurs\twhen\tyour\tmodel\tis\ttoo\tsimple\tto learn\tthe\tunderlying\tstructure\tof\tthe\tdata.\tFor\texample,\ta\tlinear\tmodel\tof\tlife\tsatisfaction\tis\tprone\tto underfit;\treality\tis\tjust\tmore\tcomplex\tthan\tthe\tmodel,\tso\tits\tpredictions\tare\tbound\tto\tbe\tinaccurate,\teven on\tthe\ttraining\texamples.\n\nThe\tmain\toptions\tto\tfix\tthis\tproblem\tare:\n\nSelecting\ta\tmore\tpowerful\tmodel,\twith\tmore\tparameters\n\nFeeding\tbetter\tfeatures\tto\tthe\tlearning\talgorithm\t(feature\tengineering)\n\nReducing\tthe\tconstraints\ton\tthe\tmodel\t(e.g.,\treducing\tthe\tregularization\thyperparameter)\n\nStepping\tBack By\tnow\tyou\talready\tknow\ta\tlot\tabout\tMachine\tLearning.\tHowever,\twe\twent\tthrough\tso\tmany\tconcepts that\tyou\tmay\tbe\tfeeling\ta\tlittle\tlost,\tso\tlet’s\tstep\tback\tand\tlook\tat\tthe\tbig\tpicture:\n\nMachine\tLearning\tis\tabout\tmaking\tmachines\tget\tbetter\tat\tsome\ttask\tby\tlearning\tfrom\tdata,\tinstead\tof having\tto\texplicitly\tcode\trules.\n\nThere\tare\tmany\tdifferent\ttypes\tof\tML\tsystems:\tsupervised\tor\tnot,\tbatch\tor\tonline,\tinstance-based\tor model-based,\tand\tso\ton.\n\nIn\ta\tML\tproject\tyou\tgather\tdata\tin\ta\ttraining\tset,\tand\tyou\tfeed\tthe\ttraining\tset\tto\ta\tlearning\talgorithm. If\tthe\talgorithm\tis\tmodel-based\tit\ttunes\tsome\tparameters\tto\tfit\tthe\tmodel\tto\tthe\ttraining\tset\t(i.e.,\tto make\tgood\tpredictions\ton\tthe\ttraining\tset\titself),\tand\tthen\thopefully\tit\twill\tbe\table\tto\tmake\tgood predictions\ton\tnew\tcases\tas\twell.\tIf\tthe\talgorithm\tis\tinstance-based,\tit\tjust\tlearns\tthe\texamples\tby heart\tand\tuses\ta\tsimilarity\tmeasure\tto\tgeneralize\tto\tnew\tinstances.\n\nThe\tsystem\twill\tnot\tperform\twell\tif\tyour\ttraining\tset\tis\ttoo\tsmall,\tor\tif\tthe\tdata\tis\tnot\trepresentative, noisy,\tor\tpolluted\twith\tirrelevant\tfeatures\t(garbage\tin,\tgarbage\tout).\tLastly,\tyour\tmodel\tneeds\tto\tbe neither\ttoo\tsimple\t(in\twhich\tcase\tit\twill\tunderfit)\tnor\ttoo\tcomplex\t(in\twhich\tcase\tit\twill\toverfit).\n\nThere’s\tjust\tone\tlast\timportant\ttopic\tto\tcover:\tonce\tyou\thave\ttrained\ta\tmodel,\tyou\tdon’t\twant\tto\tjust “hope”\tit\tgeneralizes\tto\tnew\tcases.\tYou\twant\tto\tevaluate\tit,\tand\tfine-tune\tit\tif\tnecessary.\tLet’s\tsee\thow.\n\nTesting\tand\tValidating The\tonly\tway\tto\tknow\thow\twell\ta\tmodel\twill\tgeneralize\tto\tnew\tcases\tis\tto\tactually\ttry\tit\tout\ton\tnew cases.\tOne\tway\tto\tdo\tthat\tis\tto\tput\tyour\tmodel\tin\tproduction\tand\tmonitor\thow\twell\tit\tperforms.\tThis works\twell,\tbut\tif\tyour\tmodel\tis\thorribly\tbad,\tyour\tusers\twill\tcomplain\t—\tnot\tthe\tbest\tidea.\n\nA\tbetter\toption\tis\tto\tsplit\tyour\tdata\tinto\ttwo\tsets:\tthe\ttraining\tset\tand\tthe\ttest\tset.\tAs\tthese\tnames\timply, you\ttrain\tyour\tmodel\tusing\tthe\ttraining\tset,\tand\tyou\ttest\tit\tusing\tthe\ttest\tset.\tThe\terror\trate\ton\tnew\tcases\tis called\tthe\tgeneralization\terror\t(or\tout-of-sample\terror),\tand\tby\tevaluating\tyour\tmodel\ton\tthe\ttest\tset, you\tget\tan\testimation\tof\tthis\terror.\tThis\tvalue\ttells\tyou\thow\twell\tyour\tmodel\twill\tperform\ton\tinstances\tit has\tnever\tseen\tbefore.\n\nIf\tthe\ttraining\terror\tis\tlow\t(i.e.,\tyour\tmodel\tmakes\tfew\tmistakes\ton\tthe\ttraining\tset)\tbut\tthe\tgeneralization error\tis\thigh,\tit\tmeans\tthat\tyour\tmodel\tis\toverfitting\tthe\ttraining\tdata.\n\nTIP\n\nIt\tis\tcommon\tto\tuse\t80%\tof\tthe\tdata\tfor\ttraining\tand\thold\tout\t20%\tfor\ttesting.\n\nSo\tevaluating\ta\tmodel\tis\tsimple\tenough:\tjust\tuse\ta\ttest\tset.\tNow\tsuppose\tyou\tare\thesitating\tbetween\ttwo models\t(say\ta\tlinear\tmodel\tand\ta\tpolynomial\tmodel):\thow\tcan\tyou\tdecide?\tOne\toption\tis\tto\ttrain\tboth and\tcompare\thow\twell\tthey\tgeneralize\tusing\tthe\ttest\tset.\n\nNow\tsuppose\tthat\tthe\tlinear\tmodel\tgeneralizes\tbetter,\tbut\tyou\twant\tto\tapply\tsome\tregularization\tto\tavoid overfitting.\tThe\tquestion\tis:\thow\tdo\tyou\tchoose\tthe\tvalue\tof\tthe\tregularization\thyperparameter?\tOne option\tis\tto\ttrain\t100\tdifferent\tmodels\tusing\t100\tdifferent\tvalues\tfor\tthis\thyperparameter.\tSuppose\tyou find\tthe\tbest\thyperparameter\tvalue\tthat\tproduces\ta\tmodel\twith\tthe\tlowest\tgeneralization\terror,\tsay\tjust 5%\terror.\n\nSo\tyou\tlaunch\tthis\tmodel\tinto\tproduction,\tbut\tunfortunately\tit\tdoes\tnot\tperform\tas\twell\tas\texpected\tand produces\t15%\terrors.\tWhat\tjust\thappened?\n\nThe\tproblem\tis\tthat\tyou\tmeasured\tthe\tgeneralization\terror\tmultiple\ttimes\ton\tthe\ttest\tset,\tand\tyou\tadapted the\tmodel\tand\thyperparameters\tto\tproduce\tthe\tbest\tmodel\tfor\tthat\tset.\tThis\tmeans\tthat\tthe\tmodel\tis unlikely\tto\tperform\tas\twell\ton\tnew\tdata.\n\nA\tcommon\tsolution\tto\tthis\tproblem\tis\tto\thave\ta\tsecond\tholdout\tset\tcalled\tthe\tvalidation\tset.\tYou\ttrain multiple\tmodels\twith\tvarious\thyperparameters\tusing\tthe\ttraining\tset,\tyou\tselect\tthe\tmodel\tand hyperparameters\tthat\tperform\tbest\ton\tthe\tvalidation\tset,\tand\twhen\tyou’re\thappy\twith\tyour\tmodel\tyou\trun a\tsingle\tfinal\ttest\tagainst\tthe\ttest\tset\tto\tget\tan\testimate\tof\tthe\tgeneralization\terror.\n\nTo\tavoid\t“wasting”\ttoo\tmuch\ttraining\tdata\tin\tvalidation\tsets,\ta\tcommon\ttechnique\tis\tto\tuse\tcross- validation:\tthe\ttraining\tset\tis\tsplit\tinto\tcomplementary\tsubsets,\tand\teach\tmodel\tis\ttrained\tagainst\ta different\tcombination\tof\tthese\tsubsets\tand\tvalidated\tagainst\tthe\tremaining\tparts.\tOnce\tthe\tmodel\ttype\tand hyperparameters\thave\tbeen\tselected,\ta\tfinal\tmodel\tis\ttrained\tusing\tthese\thyperparameters\ton\tthe\tfull training\tset,\tand\tthe\tgeneralized\terror\tis\tmeasured\ton\tthe\ttest\tset.\n\nNO\tFREE\tLUNCH\tTHEOREM\n\nA\tmodel\tis\ta\tsimplified\tversion\tof\tthe\tobservations.\tThe\tsimplifications\tare\tmeant\tto\tdiscard\tthe\tsuperfluous\tdetails\tthat\tare\tunlikely\tto generalize\tto\tnew\tinstances.\tHowever,\tto\tdecide\twhat\tdata\tto\tdiscard\tand\twhat\tdata\tto\tkeep,\tyou\tmust\tmake\tassumptions.\tFor\texample, a\tlinear\tmodel\tmakes\tthe\tassumption\tthat\tthe\tdata\tis\tfundamentally\tlinear\tand\tthat\tthe\tdistance\tbetween\tthe\tinstances\tand\tthe\tstraight\tline is\tjust\tnoise,\twhich\tcan\tsafely\tbe\tignored.\n\nIn\ta\tfamous\t1996\tpaper,11\tDavid\tWolpert\tdemonstrated\tthat\tif\tyou\tmake\tabsolutely\tno\tassumption\tabout\tthe\tdata,\tthen\tthere\tis\tno\treason to\tprefer\tone\tmodel\tover\tany\tother.\tThis\tis\tcalled\tthe\tNo\tFree\tLunch\t(NFL)\ttheorem.\tFor\tsome\tdatasets\tthe\tbest\tmodel\tis\ta\tlinear model,\twhile\tfor\tother\tdatasets\tit\tis\ta\tneural\tnetwork.\tThere\tis\tno\tmodel\tthat\tis\ta\tpriori\tguaranteed\tto\twork\tbetter\t(hence\tthe\tname\tof the\ttheorem).\tThe\tonly\tway\tto\tknow\tfor\tsure\twhich\tmodel\tis\tbest\tis\tto\tevaluate\tthem\tall.\tSince\tthis\tis\tnot\tpossible,\tin\tpractice\tyou\tmake some\treasonable\tassumptions\tabout\tthe\tdata\tand\tyou\tevaluate\tonly\ta\tfew\treasonable\tmodels.\tFor\texample,\tfor\tsimple\ttasks\tyou\tmay evaluate\tlinear\tmodels\twith\tvarious\tlevels\tof\tregularization,\tand\tfor\ta\tcomplex\tproblem\tyou\tmay\tevaluate\tvarious\tneural\tnetworks.\n\nExercises In\tthis\tchapter\twe\thave\tcovered\tsome\tof\tthe\tmost\timportant\tconcepts\tin\tMachine\tLearning.\tIn\tthe\tnext chapters\twe\twill\tdive\tdeeper\tand\twrite\tmore\tcode,\tbut\tbefore\twe\tdo,\tmake\tsure\tyou\tknow\thow\tto\tanswer the\tfollowing\tquestions:\n\n1.\t How\twould\tyou\tdefine\tMachine\tLearning?\n\n2.\t Can\tyou\tname\tfour\ttypes\tof\tproblems\twhere\tit\tshines?\n\n3.\t What\tis\ta\tlabeled\ttraining\tset?\n\n4.\t What\tare\tthe\ttwo\tmost\tcommon\tsupervised\ttasks?\n\n5.\t Can\tyou\tname\tfour\tcommon\tunsupervised\ttasks?\n\n6.\t What\ttype\tof\tMachine\tLearning\talgorithm\twould\tyou\tuse\tto\tallow\ta\trobot\tto\twalk\tin\tvarious unknown\tterrains?\n\n7.\t What\ttype\tof\talgorithm\twould\tyou\tuse\tto\tsegment\tyour\tcustomers\tinto\tmultiple\tgroups?\n\n8.\t Would\tyou\tframe\tthe\tproblem\tof\tspam\tdetection\tas\ta\tsupervised\tlearning\tproblem\tor\tan unsupervised\tlearning\tproblem?\n\n9.\t What\tis\tan\tonline\tlearning\tsystem?\n\n10.\t What\tis\tout-of-core\tlearning?\n\n11.\t What\ttype\tof\tlearning\talgorithm\trelies\ton\ta\tsimilarity\tmeasure\tto\tmake\tpredictions?\n\n12.\t What\tis\tthe\tdifference\tbetween\ta\tmodel\tparameter\tand\ta\tlearning\talgorithm’s\thyperparameter?\n\n13.\t What\tdo\tmodel-based\tlearning\talgorithms\tsearch\tfor?\tWhat\tis\tthe\tmost\tcommon\tstrategy\tthey\tuse\tto succeed?\tHow\tdo\tthey\tmake\tpredictions?\n\n14.\t Can\tyou\tname\tfour\tof\tthe\tmain\tchallenges\tin\tMachine\tLearning?\n\n15.\t If\tyour\tmodel\tperforms\tgreat\ton\tthe\ttraining\tdata\tbut\tgeneralizes\tpoorly\tto\tnew\tinstances,\twhat\tis happening?\tCan\tyou\tname\tthree\tpossible\tsolutions?\n\n16.\t What\tis\ta\ttest\tset\tand\twhy\twould\tyou\twant\tto\tuse\tit?\n\n17.\t What\tis\tthe\tpurpose\tof\ta\tvalidation\tset?\n\n18.\t What\tcan\tgo\twrong\tif\tyou\ttune\thyperparameters\tusing\tthe\ttest\tset?\n\n19.\t What\tis\tcross-validation\tand\twhy\twould\tyou\tprefer\tit\tto\ta\tvalidation\tset?\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nFun\tfact:\tthis\todd-sounding\tname\tis\ta\tstatistics\tterm\tintroduced\tby\tFrancis\tGalton\twhile\the\twas\tstudying\tthe\tfact\tthat\tthe\tchildren\tof\ttall people\ttend\tto\tbe\tshorter\tthan\ttheir\tparents.\tSince\tchildren\twere\tshorter,\the\tcalled\tthis\tregression\tto\tthe\tmean.\tThis\tname\twas\tthen applied\tto\tthe\tmethods\the\tused\tto\tanalyze\tcorrelations\tbetween\tvariables.\n\n2\n\nSome\tneural\tnetwork\tarchitectures\tcan\tbe\tunsupervised,\tsuch\tas\tautoencoders\tand\trestricted\tBoltzmann\tmachines.\tThey\tcan\talso\tbe semisupervised,\tsuch\tas\tin\tdeep\tbelief\tnetworks\tand\tunsupervised\tpretraining.\n\n3\n\nNotice\thow\tanimals\tare\trather\twell\tseparated\tfrom\tvehicles,\thow\thorses\tare\tclose\tto\tdeer\tbut\tfar\tfrom\tbirds,\tand\tso\ton.\tFigure\treproduced with\tpermission\tfrom\tSocher,\tGanjoo,\tManning,\tand\tNg\t(2013),\t“T-SNE\tvisualization\tof\tthe\tsemantic\tword\tspace.”\n\n4\n\nThat’s\twhen\tthe\tsystem\tworks\tperfectly.\tIn\tpractice\tit\toften\tcreates\ta\tfew\tclusters\tper\tperson,\tand\tsometimes\tmixes\tup\ttwo\tpeople\twho look\talike,\tso\tyou\tneed\tto\tprovide\ta\tfew\tlabels\tper\tperson\tand\tmanually\tclean\tup\tsome\tclusters.\n\n5\n\nBy\tconvention,\tthe\tGreek\tletter\tθ\t(theta)\tis\tfrequently\tused\tto\trepresent\tmodel\tparameters.\n\n6\n\nThe\tcode\tassumes\tthat\tprepare_country_stats()\tis\talready\tdefined:\tit\tmerges\tthe\tGDP\tand\tlife\tsatisfaction\tdata\tinto\ta\tsingle\tPandas dataframe.\n\n7\n\nIt’s\tokay\tif\tyou\tdon’t\tunderstand\tall\tthe\tcode\tyet;\twe\twill\tpresent\tScikit-Learn\tin\tthe\tfollowing\tchapters.\n\n8\n\nFor\texample,\tknowing\twhether\tto\twrite\t“to,”\t“two,”\tor\t“too”\tdepending\ton\tthe\tcontext.\n\n9\n\nFigure\treproduced\twith\tpermission\tfrom\tBanko\tand\tBrill\t(2001),\t“Learning\tCurves\tfor\tConfusion\tSet\tDisambiguation.”\n\n10\n\n“The\tUnreasonable\tEffectiveness\tof\tData,”\tPeter\tNorvig\tet\tal.\t(2009).\n\n11\n\n“The\tLack\tof\tA\tPriori\tDistinctions\tBetween\tLearning\tAlgorithms,”\tD.\tWolperts\t(1996).\n\nChapter\t2.\tEnd-to-End\tMachine\tLearning Project\n\nIn\tthis\tchapter,\tyou\twill\tgo\tthrough\tan\texample\tproject\tend\tto\tend,\tpretending\tto\tbe\ta\trecently\thired\tdata scientist\tin\ta\treal\testate\tcompany.1\tHere\tare\tthe\tmain\tsteps\tyou\twill\tgo\tthrough:\n\n1.\t Look\tat\tthe\tbig\tpicture.\n\n2.\t Get\tthe\tdata.\n\n3.\t Discover\tand\tvisualize\tthe\tdata\tto\tgain\tinsights.\n\n4.\t Prepare\tthe\tdata\tfor\tMachine\tLearning\talgorithms.\n\n5.\t Select\ta\tmodel\tand\ttrain\tit.\n\n6.\t Fine-tune\tyour\tmodel.\n\n7.\t Present\tyour\tsolution.\n\n8.\t Launch,\tmonitor,\tand\tmaintain\tyour\tsystem.",
      "page_number": 19
    },
    {
      "number": 2,
      "title": "End-to-End\tMachine\tLearning Project",
      "start_page": 56,
      "end_page": 111,
      "detection_method": "regex_chapter_title",
      "content": "Working\twith\tReal\tData When\tyou\tare\tlearning\tabout\tMachine\tLearning\tit\tis\tbest\tto\tactually\texperiment\twith\treal-world\tdata,\tnot just\tartificial\tdatasets.\tFortunately,\tthere\tare\tthousands\tof\topen\tdatasets\tto\tchoose\tfrom,\tranging\tacross\tall sorts\tof\tdomains.\tHere\tare\ta\tfew\tplaces\tyou\tcan\tlook\tto\tget\tdata:\n\nPopular\topen\tdata\trepositories:\n\nUC\tIrvine\tMachine\tLearning\tRepository\n\nKaggle\tdatasets\n\nAmazon’s\tAWS\tdatasets\n\nMeta\tportals\t(they\tlist\topen\tdata\trepositories):\n\nhttp://dataportals.org/\n\nhttp://opendatamonitor.eu/\n\nhttp://quandl.com/\n\nOther\tpages\tlisting\tmany\tpopular\topen\tdata\trepositories:\n\nWikipedia’s\tlist\tof\tMachine\tLearning\tdatasets\n\nQuora.com\tquestion\n\nDatasets\tsubreddit\n\nIn\tthis\tchapter\twe\tchose\tthe\tCalifornia\tHousing\tPrices\tdataset\tfrom\tthe\tStatLib\trepository2\t(see\tFigure\t2- 1).\tThis\tdataset\twas\tbased\ton\tdata\tfrom\tthe\t1990\tCalifornia\tcensus.\tIt\tis\tnot\texactly\trecent\t(you\tcould still\tafford\ta\tnice\thouse\tin\tthe\tBay\tArea\tat\tthe\ttime),\tbut\tit\thas\tmany\tqualities\tfor\tlearning,\tso\twe\twill pretend\tit\tis\trecent\tdata.\tWe\talso\tadded\ta\tcategorical\tattribute\tand\tremoved\ta\tfew\tfeatures\tfor\tteaching purposes.\n\nFigure\t2-1.\tCalifornia\thousing\tprices\n\nLook\tat\tthe\tBig\tPicture Welcome\tto\tMachine\tLearning\tHousing\tCorporation!\tThe\tfirst\ttask\tyou\tare\tasked\tto\tperform\tis\tto\tbuild\ta model\tof\thousing\tprices\tin\tCalifornia\tusing\tthe\tCalifornia\tcensus\tdata.\tThis\tdata\thas\tmetrics\tsuch\tas\tthe population,\tmedian\tincome,\tmedian\thousing\tprice,\tand\tso\ton\tfor\teach\tblock\tgroup\tin\tCalifornia.\tBlock groups\tare\tthe\tsmallest\tgeographical\tunit\tfor\twhich\tthe\tUS\tCensus\tBureau\tpublishes\tsample\tdata\t(a\tblock group\ttypically\thas\ta\tpopulation\tof\t600\tto\t3,000\tpeople).\tWe\twill\tjust\tcall\tthem\t“districts”\tfor\tshort.\n\nYour\tmodel\tshould\tlearn\tfrom\tthis\tdata\tand\tbe\table\tto\tpredict\tthe\tmedian\thousing\tprice\tin\tany\tdistrict, given\tall\tthe\tother\tmetrics.\n\nTIP\n\nSince\tyou\tare\ta\twell-organized\tdata\tscientist,\tthe\tfirst\tthing\tyou\tdo\tis\tto\tpull\tout\tyour\tMachine\tLearning\tproject\tchecklist.\tYou\tcan start\twith\tthe\tone\tin\tAppendix\tB;\tit\tshould\twork\treasonably\twell\tfor\tmost\tMachine\tLearning\tprojects\tbut\tmake\tsure\tto\tadapt\tit\tto your\tneeds.\tIn\tthis\tchapter\twe\twill\tgo\tthrough\tmany\tchecklist\titems,\tbut\twe\twill\talso\tskip\ta\tfew,\teither\tbecause\tthey\tare\tself- explanatory\tor\tbecause\tthey\twill\tbe\tdiscussed\tin\tlater\tchapters.\n\nFrame\tthe\tProblem The\tfirst\tquestion\tto\task\tyour\tboss\tis\twhat\texactly\tis\tthe\tbusiness\tobjective;\tbuilding\ta\tmodel\tis\tprobably not\tthe\tend\tgoal.\tHow\tdoes\tthe\tcompany\texpect\tto\tuse\tand\tbenefit\tfrom\tthis\tmodel?\tThis\tis\timportant because\tit\twill\tdetermine\thow\tyou\tframe\tthe\tproblem,\twhat\talgorithms\tyou\twill\tselect,\twhat\tperformance measure\tyou\twill\tuse\tto\tevaluate\tyour\tmodel,\tand\thow\tmuch\teffort\tyou\tshould\tspend\ttweaking\tit.\n\nYour\tboss\tanswers\tthat\tyour\tmodel’s\toutput\t(a\tprediction\tof\ta\tdistrict’s\tmedian\thousing\tprice)\twill\tbe\tfed to\tanother\tMachine\tLearning\tsystem\t(see\tFigure\t2-2),\talong\twith\tmany\tother\tsignals.3\tThis\tdownstream system\twill\tdetermine\twhether\tit\tis\tworth\tinvesting\tin\ta\tgiven\tarea\tor\tnot.\tGetting\tthis\tright\tis\tcritical,\tas it\tdirectly\taffects\trevenue.\n\nFigure\t2-2.\tA\tMachine\tLearning\tpipeline\tfor\treal\testate\tinvestments\n\nPIPELINES\n\nA\tsequence\tof\tdata\tprocessing\tcomponents\tis\tcalled\ta\tdata\tpipeline.\tPipelines\tare\tvery\tcommon\tin\tMachine\tLearning\tsystems,\tsince there\tis\ta\tlot\tof\tdata\tto\tmanipulate\tand\tmany\tdata\ttransformations\tto\tapply.\n\nComponents\ttypically\trun\tasynchronously.\tEach\tcomponent\tpulls\tin\ta\tlarge\tamount\tof\tdata,\tprocesses\tit,\tand\tspits\tout\tthe\tresult\tin\tanother data\tstore,\tand\tthen\tsome\ttime\tlater\tthe\tnext\tcomponent\tin\tthe\tpipeline\tpulls\tthis\tdata\tand\tspits\tout\tits\town\toutput,\tand\tso\ton.\tEach component\tis\tfairly\tself-contained:\tthe\tinterface\tbetween\tcomponents\tis\tsimply\tthe\tdata\tstore.\tThis\tmakes\tthe\tsystem\tquite\tsimple\tto grasp\t(with\tthe\thelp\tof\ta\tdata\tflow\tgraph),\tand\tdifferent\tteams\tcan\tfocus\ton\tdifferent\tcomponents.\tMoreover,\tif\ta\tcomponent\tbreaks down,\tthe\tdownstream\tcomponents\tcan\toften\tcontinue\tto\trun\tnormally\t(at\tleast\tfor\ta\twhile)\tby\tjust\tusing\tthe\tlast\toutput\tfrom\tthe\tbroken component.\tThis\tmakes\tthe\tarchitecture\tquite\trobust.\n\nOn\tthe\tother\thand,\ta\tbroken\tcomponent\tcan\tgo\tunnoticed\tfor\tsome\ttime\tif\tproper\tmonitoring\tis\tnot\timplemented.\tThe\tdata\tgets\tstale\tand the\toverall\tsystem’s\tperformance\tdrops.\n\nThe\tnext\tquestion\tto\task\tis\twhat\tthe\tcurrent\tsolution\tlooks\tlike\t(if\tany).\tIt\twill\toften\tgive\tyou\ta\treference performance,\tas\twell\tas\tinsights\ton\thow\tto\tsolve\tthe\tproblem.\tYour\tboss\tanswers\tthat\tthe\tdistrict\thousing prices\tare\tcurrently\testimated\tmanually\tby\texperts:\ta\tteam\tgathers\tup-to-date\tinformation\tabout\ta\tdistrict, and\twhen\tthey\tcannot\tget\tthe\tmedian\thousing\tprice,\tthey\testimate\tit\tusing\tcomplex\trules.\n\nThis\tis\tcostly\tand\ttime-consuming,\tand\ttheir\testimates\tare\tnot\tgreat;\tin\tcases\twhere\tthey\tmanage\tto\tfind out\tthe\tactual\tmedian\thousing\tprice,\tthey\toften\trealize\tthat\ttheir\testimates\twere\toff\tby\tmore\tthan\t10%. This\tis\twhy\tthe\tcompany\tthinks\tthat\tit\twould\tbe\tuseful\tto\ttrain\ta\tmodel\tto\tpredict\ta\tdistrict’s\tmedian housing\tprice\tgiven\tother\tdata\tabout\tthat\tdistrict.\tThe\tcensus\tdata\tlooks\tlike\ta\tgreat\tdataset\tto\texploit\tfor this\tpurpose,\tsince\tit\tincludes\tthe\tmedian\thousing\tprices\tof\tthousands\tof\tdistricts,\tas\twell\tas\tother\tdata.\n\nOkay,\twith\tall\tthis\tinformation\tyou\tare\tnow\tready\tto\tstart\tdesigning\tyour\tsystem.\tFirst,\tyou\tneed\tto\tframe\n\nthe\tproblem:\tis\tit\tsupervised,\tunsupervised,\tor\tReinforcement\tLearning?\tIs\tit\ta\tclassification\ttask,\ta regression\ttask,\tor\tsomething\telse?\tShould\tyou\tuse\tbatch\tlearning\tor\tonline\tlearning\ttechniques?\tBefore you\tread\ton,\tpause\tand\ttry\tto\tanswer\tthese\tquestions\tfor\tyourself.\n\nHave\tyou\tfound\tthe\tanswers?\tLet’s\tsee:\tit\tis\tclearly\ta\ttypical\tsupervised\tlearning\ttask\tsince\tyou\tare\tgiven labeled\ttraining\texamples\t(each\tinstance\tcomes\twith\tthe\texpected\toutput,\ti.e.,\tthe\tdistrict’s\tmedian housing\tprice).\tMoreover,\tit\tis\talso\ta\ttypical\tregression\ttask,\tsince\tyou\tare\tasked\tto\tpredict\ta\tvalue.\tMore specifically,\tthis\tis\ta\tmultivariate\tregression\tproblem\tsince\tthe\tsystem\twill\tuse\tmultiple\tfeatures\tto\tmake a\tprediction\t(it\twill\tuse\tthe\tdistrict’s\tpopulation,\tthe\tmedian\tincome,\tetc.).\tIn\tthe\tfirst\tchapter,\tyou predicted\tlife\tsatisfaction\tbased\ton\tjust\tone\tfeature,\tthe\tGDP\tper\tcapita,\tso\tit\twas\ta\tunivariate\tregression problem.\tFinally,\tthere\tis\tno\tcontinuous\tflow\tof\tdata\tcoming\tin\tthe\tsystem,\tthere\tis\tno\tparticular\tneed\tto adjust\tto\tchanging\tdata\trapidly,\tand\tthe\tdata\tis\tsmall\tenough\tto\tfit\tin\tmemory,\tso\tplain\tbatch\tlearning should\tdo\tjust\tfine.\n\nTIP\n\nIf\tthe\tdata\twas\thuge,\tyou\tcould\teither\tsplit\tyour\tbatch\tlearning\twork\tacross\tmultiple\tservers\t(using\tthe\tMapReduce\ttechnique,\tas we\twill\tsee\tlater),\tor\tyou\tcould\tuse\tan\tonline\tlearning\ttechnique\tinstead.\n\nSelect\ta\tPerformance\tMeasure Your\tnext\tstep\tis\tto\tselect\ta\tperformance\tmeasure.\tA\ttypical\tperformance\tmeasure\tfor\tregression problems\tis\tthe\tRoot\tMean\tSquare\tError\t(RMSE).\tIt\tgives\tan\tidea\tof\thow\tmuch\terror\tthe\tsystem\ttypically makes\tin\tits\tpredictions,\twith\ta\thigher\tweight\tfor\tlarge\terrors.\tEquation\t2-1\tshows\tthe\tmathematical formula\tto\tcompute\tthe\tRMSE.\n\nEquation\t2-1.\tRoot\tMean\tSquare\tError\t(RMSE)\n\nNOTATIONS\n\nThis\tequation\tintroduces\tseveral\tvery\tcommon\tMachine\tLearning\tnotations\tthat\twe\twill\tuse\tthroughout\tthis\tbook:\n\nm\tis\tthe\tnumber\tof\tinstances\tin\tthe\tdataset\tyou\tare\tmeasuring\tthe\tRMSE\ton.\n\nFor\texample,\tif\tyou\tare\tevaluating\tthe\tRMSE\ton\ta\tvalidation\tset\tof\t2,000\tdistricts,\tthen\tm\t=\t2,000.\n\nx(i)\tis\ta\tvector\tof\tall\tthe\tfeature\tvalues\t(excluding\tthe\tlabel)\tof\tthe\tith\tinstance\tin\tthe\tdataset,\tand\ty(i)\tis\tits\tlabel\t(the\tdesired output\tvalue\tfor\tthat\tinstance).\n\nFor\texample,\tif\tthe\tfirst\tdistrict\tin\tthe\tdataset\tis\tlocated\tat\tlongitude\t–118.29°,\tlatitude\t33.91°,\tand\tit\thas\t1,416\tinhabitants with\ta\tmedian\tincome\tof\t$38,372,\tand\tthe\tmedian\thouse\tvalue\tis\t$156,400\t(ignoring\tthe\tother\tfeatures\tfor\tnow),\tthen:\n\nand:\n\nX\tis\ta\tmatrix\tcontaining\tall\tthe\tfeature\tvalues\t(excluding\tlabels)\tof\tall\tinstances\tin\tthe\tdataset.\tThere\tis\tone\trow\tper\tinstance\tand the\tith\trow\tis\tequal\tto\tthe\ttranspose\tof\tx(i),\tnoted\t(x(i))T.4\n\nFor\texample,\tif\tthe\tfirst\tdistrict\tis\tas\tjust\tdescribed,\tthen\tthe\tmatrix\tX\tlooks\tlike\tthis:\n\nh\tis\tyour\tsystem’s\tprediction\tfunction,\talso\tcalled\ta\thypothesis.\tWhen\tyour\tsystem\tis\tgiven\tan\tinstance’s\tfeature\tvector\tx(i),\tit outputs\ta\tpredicted\tvalue\tŷ(i)\t=\th(x(i))\tfor\tthat\tinstance\t(ŷ\tis\tpronounced\t“y-hat”).\n\nFor\texample,\tif\tyour\tsystem\tpredicts\tthat\tthe\tmedian\thousing\tprice\tin\tthe\tfirst\tdistrict\tis\t$158,400,\tthen\tŷ(1)\t=\th(x(1))\t= 158,400.\tThe\tprediction\terror\tfor\tthis\tdistrict\tis\tŷ(1)\t–\ty(1)\t=\t2,000.\n\nRMSE(X,h)\tis\tthe\tcost\tfunction\tmeasured\ton\tthe\tset\tof\texamples\tusing\tyour\thypothesis\th.\n\nWe\tuse\tlowercase\titalic\tfont\tfor\tscalar\tvalues\t(such\tas\tm\tor\ty(i))\tand\tfunction\tnames\t(such\tas\th),\tlowercase\tbold\tfont\tfor\tvectors\t(such as\tx(i)),\tand\tuppercase\tbold\tfont\tfor\tmatrices\t(such\tas\tX).\n\nEven\tthough\tthe\tRMSE\tis\tgenerally\tthe\tpreferred\tperformance\tmeasure\tfor\tregression\ttasks,\tin\tsome contexts\tyou\tmay\tprefer\tto\tuse\tanother\tfunction.\tFor\texample,\tsuppose\tthat\tthere\tare\tmany\toutlier\tdistricts. In\tthat\tcase,\tyou\tmay\tconsider\tusing\tthe\tMean\tAbsolute\tError\t(also\tcalled\tthe\tAverage\tAbsolute Deviation;\tsee\tEquation\t2-2):\n\nEquation\t2-2.\tMean\tAbsolute\tError\n\nBoth\tthe\tRMSE\tand\tthe\tMAE\tare\tways\tto\tmeasure\tthe\tdistance\tbetween\ttwo\tvectors:\tthe\tvector\tof predictions\tand\tthe\tvector\tof\ttarget\tvalues.\tVarious\tdistance\tmeasures,\tor\tnorms,\tare\tpossible:\n\nComputing\tthe\troot\tof\ta\tsum\tof\tsquares\t(RMSE)\tcorresponds\tto\tthe\tEuclidian\tnorm:\tit\tis\tthe\tnotion of\tdistance\tyou\tare\tfamiliar\twith.\tIt\tis\talso\tcalled\tthe\tℓ2\tnorm,\tnoted\t\t·\n\n2\t(or\tjust\t\t·\t).\n\nComputing\tthe\tsum\tof\tabsolutes\t(MAE)\tcorresponds\tto\tthe\tℓ1\tnorm,\tnoted\t\t·\t called\tthe\tManhattan\tnorm\tbecause\tit\tmeasures\tthe\tdistance\tbetween\ttwo\tpoints\tin\ta\tcity\tif\tyou\tcan only\ttravel\talong\torthogonal\tcity\tblocks.\n\n1.\tIt\tis\tsometimes\n\nMore\tgenerally,\tthe\tℓk\tnorm\tof\ta\tvector\tv\tcontaining\tn\telements\tis\tdefined\tas\n\n.\tℓ0\tjust\tgives\tthe\tnumber\tof\tnon-zero\telements\tin\tthe\tvector,\n\nand\tℓ∞\tgives\tthe\tmaximum\tabsolute\tvalue\tin\tthe\tvector.\n\nThe\thigher\tthe\tnorm\tindex,\tthe\tmore\tit\tfocuses\ton\tlarge\tvalues\tand\tneglects\tsmall\tones.\tThis\tis\twhy the\tRMSE\tis\tmore\tsensitive\tto\toutliers\tthan\tthe\tMAE.\tBut\twhen\toutliers\tare\texponentially\trare\t(like in\ta\tbell-shaped\tcurve),\tthe\tRMSE\tperforms\tvery\twell\tand\tis\tgenerally\tpreferred.\n\nCheck\tthe\tAssumptions Lastly,\tit\tis\tgood\tpractice\tto\tlist\tand\tverify\tthe\tassumptions\tthat\twere\tmade\tso\tfar\t(by\tyou\tor\tothers);\tthis can\tcatch\tserious\tissues\tearly\ton.\tFor\texample,\tthe\tdistrict\tprices\tthat\tyour\tsystem\toutputs\tare\tgoing\tto\tbe fed\tinto\ta\tdownstream\tMachine\tLearning\tsystem,\tand\twe\tassume\tthat\tthese\tprices\tare\tgoing\tto\tbe\tused\tas such.\tBut\twhat\tif\tthe\tdownstream\tsystem\tactually\tconverts\tthe\tprices\tinto\tcategories\t(e.g.,\t“cheap,” “medium,”\tor\t“expensive”)\tand\tthen\tuses\tthose\tcategories\tinstead\tof\tthe\tprices\tthemselves?\tIn\tthis\tcase, getting\tthe\tprice\tperfectly\tright\tis\tnot\timportant\tat\tall;\tyour\tsystem\tjust\tneeds\tto\tget\tthe\tcategory\tright.\tIf that’s\tso,\tthen\tthe\tproblem\tshould\thave\tbeen\tframed\tas\ta\tclassification\ttask,\tnot\ta\tregression\ttask.\tYou don’t\twant\tto\tfind\tthis\tout\tafter\tworking\ton\ta\tregression\tsystem\tfor\tmonths.\n\nFortunately,\tafter\ttalking\twith\tthe\tteam\tin\tcharge\tof\tthe\tdownstream\tsystem,\tyou\tare\tconfident\tthat\tthey\tdo indeed\tneed\tthe\tactual\tprices,\tnot\tjust\tcategories.\tGreat!\tYou’re\tall\tset,\tthe\tlights\tare\tgreen,\tand\tyou\tcan start\tcoding\tnow!\n\nGet\tthe\tData It’s\ttime\tto\tget\tyour\thands\tdirty.\tDon’t\thesitate\tto\tpick\tup\tyour\tlaptop\tand\twalk\tthrough\tthe\tfollowing\tcode examples\tin\ta\tJupyter\tnotebook.\tThe\tfull\tJupyter\tnotebook\tis\tavailable\tat https://github.com/ageron/handson-ml.\n\nCreate\tthe\tWorkspace First\tyou\twill\tneed\tto\thave\tPython\tinstalled.\tIt\tis\tprobably\talready\tinstalled\ton\tyour\tsystem.\tIf\tnot,\tyou can\tget\tit\tat\thttps://www.python.org/.5\n\nNext\tyou\tneed\tto\tcreate\ta\tworkspace\tdirectory\tfor\tyour\tMachine\tLearning\tcode\tand\tdatasets.\tOpen\ta terminal\tand\ttype\tthe\tfollowing\tcommands\t(after\tthe\t$\tprompts):\n\n$\texport\tML_PATH=\"$HOME/ml\"\t\t\t\t\t\t#\tYou\tcan\tchange\tthe\tpath\tif\tyou\tprefer $\tmkdir\t-p\t$ML_PATH\n\nYou\twill\tneed\ta\tnumber\tof\tPython\tmodules:\tJupyter,\tNumPy,\tPandas,\tMatplotlib,\tand\tScikit-Learn.\tIf\tyou already\thave\tJupyter\trunning\twith\tall\tthese\tmodules\tinstalled,\tyou\tcan\tsafely\tskip\tto\t“Download\tthe Data”.\tIf\tyou\tdon’t\thave\tthem\tyet,\tthere\tare\tmany\tways\tto\tinstall\tthem\t(and\ttheir\tdependencies).\tYou\tcan use\tyour\tsystem’s\tpackaging\tsystem\t(e.g.,\tapt-get\ton\tUbuntu,\tor\tMacPorts\tor\tHomeBrew\ton\tmacOS), install\ta\tScientific\tPython\tdistribution\tsuch\tas\tAnaconda\tand\tuse\tits\tpackaging\tsystem,\tor\tjust\tuse Python’s\town\tpackaging\tsystem,\tpip,\twhich\tis\tincluded\tby\tdefault\twith\tthe\tPython\tbinary\tinstallers\t(since Python\t2.7.9).6\tYou\tcan\tcheck\tto\tsee\tif\tpip\tis\tinstalled\tby\ttyping\tthe\tfollowing\tcommand:\n\n$\tpip3\t--version pip\t9.0.1\tfrom\t[...]/lib/python3.5/site-packages\t(python\t3.5)\n\nYou\tshould\tmake\tsure\tyou\thave\ta\trecent\tversion\tof\tpip\tinstalled,\tat\tthe\tvery\tleast\t>1.4\tto\tsupport\tbinary module\tinstallation\t(a.k.a.\twheels).\tTo\tupgrade\tthe\tpip\tmodule,\ttype:7\n\n$\tpip3\tinstall\t--upgrade\tpip Collecting\tpip [...] Successfully\tinstalled\tpip-9.0.1\n\nCREATING\tAN\tISOLATED\tENVIRONMENT\n\nIf\tyou\twould\tlike\tto\twork\tin\tan\tisolated\tenvironment\t(which\tis\tstrongly\trecommended\tso\tyou\tcan\twork\ton\tdifferent\tprojects\twithout having\tconflicting\tlibrary\tversions),\tinstall\tvirtualenv\tby\trunning\tthe\tfollowing\tpip\tcommand:\n\n$\tpip3\tinstall\t--user\t--upgrade\tvirtualenv Collecting\tvirtualenv [...] Successfully\tinstalled\tvirtualenv\n\nNow\tyou\tcan\tcreate\tan\tisolated\tPython\tenvironment\tby\ttyping:\n\n$\tcd\t$ML_PATH $\tvirtualenv\tenv Using\tbase\tprefix\t'[...]' New\tpython\texecutable\tin\t[...]/ml/env/bin/python3.5 Also\tcreating\texecutable\tin\t[...]/ml/env/bin/python Installing\tsetuptools,\tpip,\twheel...done.\n\nNow\tevery\ttime\tyou\twant\tto\tactivate\tthis\tenvironment,\tjust\topen\ta\tterminal\tand\ttype:\n\n$\tcd\t$ML_PATH $\tsource\tenv/bin/activate\n\nWhile\tthe\tenvironment\tis\tactive,\tany\tpackage\tyou\tinstall\tusing\tpip\twill\tbe\tinstalled\tin\tthis\tisolated\tenvironment,\tand\tPython\twill\tonly\thave access\tto\tthese\tpackages\t(if\tyou\talso\twant\taccess\tto\tthe\tsystem’s\tsite\tpackages,\tyou\tshould\tcreate\tthe\tenvironment\tusing\tvirtualenv’s\t-- system-site-packages\toption).\tCheck\tout\tvirtualenv’s\tdocumentation\tfor\tmore\tinformation.\n\nNow\tyou\tcan\tinstall\tall\tthe\trequired\tmodules\tand\ttheir\tdependencies\tusing\tthis\tsimple\tpip\tcommand:\n\n$\tpip3\tinstall\t--upgrade\tjupyter\tmatplotlib\tnumpy\tpandas\tscipy\tscikit-learn Collecting\tjupyter \t\tDownloading\tjupyter-1.0.0-py2.py3-none-any.whl Collecting\tmatplotlib \t\t[...]\n\nTo\tcheck\tyour\tinstallation,\ttry\tto\timport\tevery\tmodule\tlike\tthis:\n\n$\tpython3\t-c\t\"import\tjupyter,\tmatplotlib,\tnumpy,\tpandas,\tscipy,\tsklearn\"\n\nThere\tshould\tbe\tno\toutput\tand\tno\terror.\tNow\tyou\tcan\tfire\tup\tJupyter\tby\ttyping:\n\n$\tjupyter\tnotebook [I\t15:24\tNotebookApp]\tServing\tnotebooks\tfrom\tlocal\tdirectory:\t[...]/ml [I\t15:24\tNotebookApp]\t0\tactive\tkernels [I\t15:24\tNotebookApp]\tThe\tJupyter\tNotebook\tis\trunning\tat:\thttp://localhost:8888/ [I\t15:24\tNotebookApp]\tUse\tControl-C\tto\tstop\tthis\tserver\tand\tshut\tdown\tall kernels\t(twice\tto\tskip\tconfirmation).\n\nA\tJupyter\tserver\tis\tnow\trunning\tin\tyour\tterminal,\tlistening\tto\tport\t8888.\tYou\tcan\tvisit\tthis\tserver\tby opening\tyour\tweb\tbrowser\tto\thttp://localhost:8888/\t(this\tusually\thappens\tautomatically\twhen\tthe\tserver starts).\tYou\tshould\tsee\tyour\tempty\tworkspace\tdirectory\t(containing\tonly\tthe\tenv\tdirectory\tif\tyou\tfollowed the\tpreceding\tvirtualenv\tinstructions).\n\nNow\tcreate\ta\tnew\tPython\tnotebook\tby\tclicking\ton\tthe\tNew\tbutton\tand\tselecting\tthe\tappropriate\tPython version8\t(see\tFigure\t2-3).\n\nThis\tdoes\tthree\tthings:\tfirst,\tit\tcreates\ta\tnew\tnotebook\tfile\tcalled\tUntitled.ipynb\tin\tyour\tworkspace; second,\tit\tstarts\ta\tJupyter\tPython\tkernel\tto\trun\tthis\tnotebook;\tand\tthird,\tit\topens\tthis\tnotebook\tin\ta\tnew tab.\tYou\tshould\tstart\tby\trenaming\tthis\tnotebook\tto\t“Housing”\t(this\twill\tautomatically\trename\tthe\tfile\tto Housing.ipynb)\tby\tclicking\tUntitled\tand\ttyping\tthe\tnew\tname.\n\nFigure\t2-3.\tYour\tworkspace\tin\tJupyter\n\nA\tnotebook\tcontains\ta\tlist\tof\tcells.\tEach\tcell\tcan\tcontain\texecutable\tcode\tor\tformatted\ttext.\tRight\tnow\tthe notebook\tcontains\tonly\tone\tempty\tcode\tcell,\tlabeled\t“In\t[1]:”.\tTry\ttyping\tprint(\"Hello\tworld!\")\tin the\tcell,\tand\tclick\ton\tthe\tplay\tbutton\t(see\tFigure\t2-4)\tor\tpress\tShift-Enter.\tThis\tsends\tthe\tcurrent\tcell\tto this\tnotebook’s\tPython\tkernel,\twhich\truns\tit\tand\treturns\tthe\toutput.\tThe\tresult\tis\tdisplayed\tbelow\tthe\tcell, and\tsince\twe\treached\tthe\tend\tof\tthe\tnotebook,\ta\tnew\tcell\tis\tautomatically\tcreated.\tGo\tthrough\tthe\tUser Interface\tTour\tfrom\tJupyter’s\tHelp\tmenu\tto\tlearn\tthe\tbasics.\n\nFigure\t2-4.\tHello\tworld\tPython\tnotebook\n\nDownload\tthe\tData In\ttypical\tenvironments\tyour\tdata\twould\tbe\tavailable\tin\ta\trelational\tdatabase\t(or\tsome\tother\tcommon datastore)\tand\tspread\tacross\tmultiple\ttables/documents/files.\tTo\taccess\tit,\tyou\twould\tfirst\tneed\tto\tget your\tcredentials\tand\taccess\tauthorizations,9\tand\tfamiliarize\tyourself\twith\tthe\tdata\tschema.\tIn\tthis\tproject, however,\tthings\tare\tmuch\tsimpler:\tyou\twill\tjust\tdownload\ta\tsingle\tcompressed\tfile,\thousing.tgz,\twhich contains\ta\tcomma-separated\tvalue\t(CSV)\tfile\tcalled\thousing.csv\twith\tall\tthe\tdata.\n\nYou\tcould\tuse\tyour\tweb\tbrowser\tto\tdownload\tit,\tand\trun\ttar\txzf\thousing.tgz\tto\tdecompress\tthe\tfile and\textract\tthe\tCSV\tfile,\tbut\tit\tis\tpreferable\tto\tcreate\ta\tsmall\tfunction\tto\tdo\tthat.\tIt\tis\tuseful\tin\tparticular\tif data\tchanges\tregularly,\tas\tit\tallows\tyou\tto\twrite\ta\tsmall\tscript\tthat\tyou\tcan\trun\twhenever\tyou\tneed\tto fetch\tthe\tlatest\tdata\t(or\tyou\tcan\tset\tup\ta\tscheduled\tjob\tto\tdo\tthat\tautomatically\tat\tregular\tintervals). Automating\tthe\tprocess\tof\tfetching\tthe\tdata\tis\talso\tuseful\tif\tyou\tneed\tto\tinstall\tthe\tdataset\ton\tmultiple machines. Here\tis\tthe\tfunction\tto\tfetch\tthe\tdata:10\n\nimport\tos import\ttarfile from\tsix.moves\timport\turllib\n\nDOWNLOAD_ROOT\t=\t\"https://raw.githubusercontent.com/ageron/handson-ml/master/\" HOUSING_PATH\t=\tos.path.join(\"datasets\",\t\"housing\") HOUSING_URL\t=\tDOWNLOAD_ROOT\t+\t\"datasets/housing/housing.tgz\"\n\ndef\tfetch_housing_data(housing_url=HOUSING_URL,\thousing_path=HOUSING_PATH): \t\t\t\tif\tnot\tos.path.isdir(housing_path): \t\t\t\t\t\t\t\tos.makedirs(housing_path) \t\t\t\ttgz_path\t=\tos.path.join(housing_path,\t\"housing.tgz\") \t\t\t\turllib.request.urlretrieve(housing_url,\ttgz_path) \t\t\t\thousing_tgz\t=\ttarfile.open(tgz_path) \t\t\t\thousing_tgz.extractall(path=housing_path) \t\t\t\thousing_tgz.close()\n\nNow\twhen\tyou\tcall\tfetch_housing_data(),\tit\tcreates\ta\tdatasets/housing\tdirectory\tin\tyour\tworkspace, downloads\tthe\thousing.tgz\tfile,\tand\textracts\tthe\thousing.csv\tfrom\tit\tin\tthis\tdirectory.\n\nNow\tlet’s\tload\tthe\tdata\tusing\tPandas.\tOnce\tagain\tyou\tshould\twrite\ta\tsmall\tfunction\tto\tload\tthe\tdata:\n\nimport\tpandas\tas\tpd\n\ndef\tload_housing_data(housing_path=HOUSING_PATH): \t\t\t\tcsv_path\t=\tos.path.join(housing_path,\t\"housing.csv\") \t\t\t\treturn\tpd.read_csv(csv_path)\n\nThis\tfunction\treturns\ta\tPandas\tDataFrame\tobject\tcontaining\tall\tthe\tdata.\n\nTake\ta\tQuick\tLook\tat\tthe\tData\tStructure Let’s\ttake\ta\tlook\tat\tthe\ttop\tfive\trows\tusing\tthe\tDataFrame’s\thead()\tmethod\t(see\tFigure\t2-5).\n\nFigure\t2-5.\tTop\tfive\trows\tin\tthe\tdataset\n\nEach\trow\trepresents\tone\tdistrict.\tThere\tare\t10\tattributes\t(you\tcan\tsee\tthe\tfirst\t6\tin\tthe\tscreenshot): longitude,\tlatitude,\thousing_median_age,\ttotal_rooms,\ttotal_bedrooms,\tpopulation, households,\tmedian_income,\tmedian_house_value,\tand\tocean_proximity.\n\nThe\tinfo()\tmethod\tis\tuseful\tto\tget\ta\tquick\tdescription\tof\tthe\tdata,\tin\tparticular\tthe\ttotal\tnumber\tof\trows, and\teach\tattribute’s\ttype\tand\tnumber\tof\tnon-null\tvalues\t(see\tFigure\t2-6).\n\nFigure\t2-6.\tHousing\tinfo\n\nThere\tare\t20,640\tinstances\tin\tthe\tdataset,\twhich\tmeans\tthat\tit\tis\tfairly\tsmall\tby\tMachine\tLearning standards,\tbut\tit’s\tperfect\tto\tget\tstarted.\tNotice\tthat\tthe\ttotal_bedrooms\tattribute\thas\tonly\t20,433\tnon- null\tvalues,\tmeaning\tthat\t207\tdistricts\tare\tmissing\tthis\tfeature.\tWe\twill\tneed\tto\ttake\tcare\tof\tthis\tlater.\n\nAll\tattributes\tare\tnumerical,\texcept\tthe\tocean_proximity\tfield.\tIts\ttype\tis\tobject,\tso\tit\tcould\thold\tany kind\tof\tPython\tobject,\tbut\tsince\tyou\tloaded\tthis\tdata\tfrom\ta\tCSV\tfile\tyou\tknow\tthat\tit\tmust\tbe\ta\ttext attribute.\tWhen\tyou\tlooked\tat\tthe\ttop\tfive\trows,\tyou\tprobably\tnoticed\tthat\tthe\tvalues\tin\tthe ocean_proximity\tcolumn\twere\trepetitive,\twhich\tmeans\tthat\tit\tis\tprobably\ta\tcategorical\tattribute.\tYou can\tfind\tout\twhat\tcategories\texist\tand\thow\tmany\tdistricts\tbelong\tto\teach\tcategory\tby\tusing\tthe\n\nvalue_counts()\tmethod:\n\n>>>\thousing[\"ocean_proximity\"].value_counts() <1H\tOCEAN\t\t\t\t\t9136 INLAND\t\t\t\t\t\t\t\t6551 NEAR\tOCEAN\t\t\t\t2658 NEAR\tBAY\t\t\t\t\t\t2290 ISLAND\t\t\t\t\t\t\t\t\t\t\t5 Name:\tocean_proximity,\tdtype:\tint64\n\nLet’s\tlook\tat\tthe\tother\tfields.\tThe\tdescribe()\tmethod\tshows\ta\tsummary\tof\tthe\tnumerical\tattributes (Figure\t2-7).\n\nFigure\t2-7.\tSummary\tof\teach\tnumerical\tattribute\n\nThe\tcount,\tmean,\tmin,\tand\tmax\trows\tare\tself-explanatory.\tNote\tthat\tthe\tnull\tvalues\tare\tignored\t(so,\tfor example,\tcount\tof\ttotal_bedrooms\tis\t20,433,\tnot\t20,640).\tThe\tstd\trow\tshows\tthe\tstandard\tdeviation, which\tmeasures\thow\tdispersed\tthe\tvalues\tare.11\tThe\t25%,\t50%,\tand\t75%\trows\tshow\tthe\tcorresponding percentiles:\ta\tpercentile\tindicates\tthe\tvalue\tbelow\twhich\ta\tgiven\tpercentage\tof\tobservations\tin\ta\tgroup of\tobservations\tfalls.\tFor\texample,\t25%\tof\tthe\tdistricts\thave\ta\thousing_median_age\tlower\tthan\t18, while\t50%\tare\tlower\tthan\t29\tand\t75%\tare\tlower\tthan\t37.\tThese\tare\toften\tcalled\tthe\t25th\tpercentile\t(or 1st\tquartile),\tthe\tmedian,\tand\tthe\t75th\tpercentile\t(or\t3rd\tquartile).\n\nAnother\tquick\tway\tto\tget\ta\tfeel\tof\tthe\ttype\tof\tdata\tyou\tare\tdealing\twith\tis\tto\tplot\ta\thistogram\tfor\teach numerical\tattribute.\tA\thistogram\tshows\tthe\tnumber\tof\tinstances\t(on\tthe\tvertical\taxis)\tthat\thave\ta\tgiven value\trange\t(on\tthe\thorizontal\taxis).\tYou\tcan\teither\tplot\tthis\tone\tattribute\tat\ta\ttime,\tor\tyou\tcan\tcall\tthe hist()\tmethod\ton\tthe\twhole\tdataset,\tand\tit\twill\tplot\ta\thistogram\tfor\teach\tnumerical\tattribute\t(see Figure\t2-8).\tFor\texample,\tyou\tcan\tsee\tthat\tslightly\tover\t800\tdistricts\thave\ta\tmedian_house_value\tequal to\tabout\t$100,000.\n\n%matplotlib\tinline\t\t\t#\tonly\tin\ta\tJupyter\tnotebook import\tmatplotlib.pyplot\tas\tplt housing.hist(bins=50,\tfigsize=(20,15)) plt.show()\n\nNOTE\n\nThe\thist()\tmethod\trelies\ton\tMatplotlib,\twhich\tin\tturn\trelies\ton\ta\tuser-specified\tgraphical\tbackend\tto\tdraw\ton\tyour\tscreen.\tSo before\tyou\tcan\tplot\tanything,\tyou\tneed\tto\tspecify\twhich\tbackend\tMatplotlib\tshould\tuse.\tThe\tsimplest\toption\tis\tto\tuse\tJupyter’s magic\tcommand\t%matplotlib\tinline.\tThis\ttells\tJupyter\tto\tset\tup\tMatplotlib\tso\tit\tuses\tJupyter’s\town\tbackend.\tPlots\tare\tthen rendered\twithin\tthe\tnotebook\titself.\tNote\tthat\tcalling\tshow()\tis\toptional\tin\ta\tJupyter\tnotebook,\tas\tJupyter\twill\tautomatically display\tplots\twhen\ta\tcell\tis\texecuted.\n\nFigure\t2-8.\tA\thistogram\tfor\teach\tnumerical\tattribute\n\nNotice\ta\tfew\tthings\tin\tthese\thistograms:\n\n1.\t First,\tthe\tmedian\tincome\tattribute\tdoes\tnot\tlook\tlike\tit\tis\texpressed\tin\tUS\tdollars\t(USD).\tAfter checking\twith\tthe\tteam\tthat\tcollected\tthe\tdata,\tyou\tare\ttold\tthat\tthe\tdata\thas\tbeen\tscaled\tand\tcapped at\t15\t(actually\t15.0001)\tfor\thigher\tmedian\tincomes,\tand\tat\t0.5\t(actually\t0.4999)\tfor\tlower\tmedian incomes.\tWorking\twith\tpreprocessed\tattributes\tis\tcommon\tin\tMachine\tLearning,\tand\tit\tis\tnot necessarily\ta\tproblem,\tbut\tyou\tshould\ttry\tto\tunderstand\thow\tthe\tdata\twas\tcomputed.\n\n2.\t The\thousing\tmedian\tage\tand\tthe\tmedian\thouse\tvalue\twere\talso\tcapped.\tThe\tlatter\tmay\tbe\ta\tserious problem\tsince\tit\tis\tyour\ttarget\tattribute\t(your\tlabels).\tYour\tMachine\tLearning\talgorithms\tmay\tlearn that\tprices\tnever\tgo\tbeyond\tthat\tlimit.\tYou\tneed\tto\tcheck\twith\tyour\tclient\tteam\t(the\tteam\tthat\twill\tuse your\tsystem’s\toutput)\tto\tsee\tif\tthis\tis\ta\tproblem\tor\tnot.\tIf\tthey\ttell\tyou\tthat\tthey\tneed\tprecise predictions\teven\tbeyond\t$500,000,\tthen\tyou\thave\tmainly\ttwo\toptions: a.\t Collect\tproper\tlabels\tfor\tthe\tdistricts\twhose\tlabels\twere\tcapped.\n\nb.\t Remove\tthose\tdistricts\tfrom\tthe\ttraining\tset\t(and\talso\tfrom\tthe\ttest\tset,\tsince\tyour\tsystem\tshould\n\nnot\tbe\tevaluated\tpoorly\tif\tit\tpredicts\tvalues\tbeyond\t$500,000).\n\n3.\t These\tattributes\thave\tvery\tdifferent\tscales.\tWe\twill\tdiscuss\tthis\tlater\tin\tthis\tchapter\twhen\twe\n\nexplore\tfeature\tscaling.\n\n4.\t Finally,\tmany\thistograms\tare\ttail\theavy:\tthey\textend\tmuch\tfarther\tto\tthe\tright\tof\tthe\tmedian\tthan\tto the\tleft.\tThis\tmay\tmake\tit\ta\tbit\tharder\tfor\tsome\tMachine\tLearning\talgorithms\tto\tdetect\tpatterns.\tWe will\ttry\ttransforming\tthese\tattributes\tlater\ton\tto\thave\tmore\tbell-shaped\tdistributions.\n\nHopefully\tyou\tnow\thave\ta\tbetter\tunderstanding\tof\tthe\tkind\tof\tdata\tyou\tare\tdealing\twith.\n\nWARNING\n\nWait!\tBefore\tyou\tlook\tat\tthe\tdata\tany\tfurther,\tyou\tneed\tto\tcreate\ta\ttest\tset,\tput\tit\taside,\tand\tnever\tlook\tat\tit.\n\nCreate\ta\tTest\tSet It\tmay\tsound\tstrange\tto\tvoluntarily\tset\taside\tpart\tof\tthe\tdata\tat\tthis\tstage.\tAfter\tall,\tyou\thave\tonly\ttaken\ta quick\tglance\tat\tthe\tdata,\tand\tsurely\tyou\tshould\tlearn\ta\twhole\tlot\tmore\tabout\tit\tbefore\tyou\tdecide\twhat algorithms\tto\tuse,\tright?\tThis\tis\ttrue,\tbut\tyour\tbrain\tis\tan\tamazing\tpattern\tdetection\tsystem,\twhich\tmeans that\tit\tis\thighly\tprone\tto\toverfitting:\tif\tyou\tlook\tat\tthe\ttest\tset,\tyou\tmay\tstumble\tupon\tsome\tseemingly interesting\tpattern\tin\tthe\ttest\tdata\tthat\tleads\tyou\tto\tselect\ta\tparticular\tkind\tof\tMachine\tLearning\tmodel. When\tyou\testimate\tthe\tgeneralization\terror\tusing\tthe\ttest\tset,\tyour\testimate\twill\tbe\ttoo\toptimistic\tand\tyou will\tlaunch\ta\tsystem\tthat\twill\tnot\tperform\tas\twell\tas\texpected.\tThis\tis\tcalled\tdata\tsnooping\tbias.\n\nCreating\ta\ttest\tset\tis\ttheoretically\tquite\tsimple:\tjust\tpick\tsome\tinstances\trandomly,\ttypically\t20%\tof\tthe dataset,\tand\tset\tthem\taside:\n\nimport\tnumpy\tas\tnp\n\ndef\tsplit_train_test(data,\ttest_ratio): \t\t\t\tshuffled_indices\t=\tnp.random.permutation(len(data)) \t\t\t\ttest_set_size\t=\tint(len(data)\t*\ttest_ratio) \t\t\t\ttest_indices\t=\tshuffled_indices[:test_set_size] \t\t\t\ttrain_indices\t=\tshuffled_indices[test_set_size:] \t\t\t\treturn\tdata.iloc[train_indices],\tdata.iloc[test_indices]\n\nYou\tcan\tthen\tuse\tthis\tfunction\tlike\tthis:\n\n>>>\ttrain_set,\ttest_set\t=\tsplit_train_test(housing,\t0.2) >>>\tprint(len(train_set),\t\"train\t+\",\tlen(test_set),\t\"test\") 16512\ttrain\t+\t4128\ttest\n\nWell,\tthis\tworks,\tbut\tit\tis\tnot\tperfect:\tif\tyou\trun\tthe\tprogram\tagain,\tit\twill\tgenerate\ta\tdifferent\ttest\tset! Over\ttime,\tyou\t(or\tyour\tMachine\tLearning\talgorithms)\twill\tget\tto\tsee\tthe\twhole\tdataset,\twhich\tis\twhat you\twant\tto\tavoid.\n\nOne\tsolution\tis\tto\tsave\tthe\ttest\tset\ton\tthe\tfirst\trun\tand\tthen\tload\tit\tin\tsubsequent\truns.\tAnother\toption\tis\tto set\tthe\trandom\tnumber\tgenerator’s\tseed\t(e.g.,\tnp.random.seed(42))12\tbefore\tcalling np.random.permutation(),\tso\tthat\tit\talways\tgenerates\tthe\tsame\tshuffled\tindices.\n\nBut\tboth\tthese\tsolutions\twill\tbreak\tnext\ttime\tyou\tfetch\tan\tupdated\tdataset.\tA\tcommon\tsolution\tis\tto\tuse each\tinstance’s\tidentifier\tto\tdecide\twhether\tor\tnot\tit\tshould\tgo\tin\tthe\ttest\tset\t(assuming\tinstances\thave\ta unique\tand\timmutable\tidentifier).\tFor\texample,\tyou\tcould\tcompute\ta\thash\tof\teach\tinstance’s\tidentifier, keep\tonly\tthe\tlast\tbyte\tof\tthe\thash,\tand\tput\tthe\tinstance\tin\tthe\ttest\tset\tif\tthis\tvalue\tis\tlower\tor\tequal\tto\t51 (~20%\tof\t256).\tThis\tensures\tthat\tthe\ttest\tset\twill\tremain\tconsistent\tacross\tmultiple\truns,\teven\tif\tyou refresh\tthe\tdataset.\tThe\tnew\ttest\tset\twill\tcontain\t20%\tof\tthe\tnew\tinstances,\tbut\tit\twill\tnot\tcontain\tany instance\tthat\twas\tpreviously\tin\tthe\ttraining\tset.\tHere\tis\ta\tpossible\timplementation:\n\nimport\thashlib\n\ndef\ttest_set_check(identifier,\ttest_ratio,\thash): \t\t\t\treturn\thash(np.int64(identifier)).digest()[-1]\t<\t256\t*\ttest_ratio\n\ndef\tsplit_train_test_by_id(data,\ttest_ratio,\tid_column,\thash=hashlib.md5): \t\t\t\tids\t=\tdata[id_column] \t\t\t\tin_test_set\t=\tids.apply(lambda\tid_:\ttest_set_check(id_,\ttest_ratio,\thash)) \t\t\t\treturn\tdata.loc[~in_test_set],\tdata.loc[in_test_set]\n\nUnfortunately,\tthe\thousing\tdataset\tdoes\tnot\thave\tan\tidentifier\tcolumn.\tThe\tsimplest\tsolution\tis\tto\tuse\tthe row\tindex\tas\tthe\tID:\n\nhousing_with_id\t=\thousing.reset_index()\t\t\t#\tadds\tan\t`index`\tcolumn train_set,\ttest_set\t=\tsplit_train_test_by_id(housing_with_id,\t0.2,\t\"index\")\n\nIf\tyou\tuse\tthe\trow\tindex\tas\ta\tunique\tidentifier,\tyou\tneed\tto\tmake\tsure\tthat\tnew\tdata\tgets\tappended\tto\tthe end\tof\tthe\tdataset,\tand\tno\trow\tever\tgets\tdeleted.\tIf\tthis\tis\tnot\tpossible,\tthen\tyou\tcan\ttry\tto\tuse\tthe\tmost stable\tfeatures\tto\tbuild\ta\tunique\tidentifier.\tFor\texample,\ta\tdistrict’s\tlatitude\tand\tlongitude\tare\tguaranteed to\tbe\tstable\tfor\ta\tfew\tmillion\tyears,\tso\tyou\tcould\tcombine\tthem\tinto\tan\tID\tlike\tso:13\n\nhousing_with_id[\"id\"]\t=\thousing[\"longitude\"]\t*\t1000\t+\thousing[\"latitude\"] train_set,\ttest_set\t=\tsplit_train_test_by_id(housing_with_id,\t0.2,\t\"id\")\n\nScikit-Learn\tprovides\ta\tfew\tfunctions\tto\tsplit\tdatasets\tinto\tmultiple\tsubsets\tin\tvarious\tways.\tThe\tsimplest function\tis\ttrain_test_split,\twhich\tdoes\tpretty\tmuch\tthe\tsame\tthing\tas\tthe\tfunction split_train_test\tdefined\tearlier,\twith\ta\tcouple\tof\tadditional\tfeatures.\tFirst\tthere\tis\ta\trandom_state parameter\tthat\tallows\tyou\tto\tset\tthe\trandom\tgenerator\tseed\tas\texplained\tpreviously,\tand\tsecond\tyou\tcan pass\tit\tmultiple\tdatasets\twith\tan\tidentical\tnumber\tof\trows,\tand\tit\twill\tsplit\tthem\ton\tthe\tsame\tindices\t(this is\tvery\tuseful,\tfor\texample,\tif\tyou\thave\ta\tseparate\tDataFrame\tfor\tlabels):\n\nfrom\tsklearn.model_selection\timport\ttrain_test_split\n\ntrain_set,\ttest_set\t=\ttrain_test_split(housing,\ttest_size=0.2,\trandom_state=42)\n\nSo\tfar\twe\thave\tconsidered\tpurely\trandom\tsampling\tmethods.\tThis\tis\tgenerally\tfine\tif\tyour\tdataset\tis\tlarge enough\t(especially\trelative\tto\tthe\tnumber\tof\tattributes),\tbut\tif\tit\tis\tnot,\tyou\trun\tthe\trisk\tof\tintroducing\ta significant\tsampling\tbias.\tWhen\ta\tsurvey\tcompany\tdecides\tto\tcall\t1,000\tpeople\tto\task\tthem\ta\tfew questions,\tthey\tdon’t\tjust\tpick\t1,000\tpeople\trandomly\tin\ta\tphone\tbooth.\tThey\ttry\tto\tensure\tthat\tthese\t1,000 people\tare\trepresentative\tof\tthe\twhole\tpopulation.\tFor\texample,\tthe\tUS\tpopulation\tis\tcomposed\tof\t51.3% female\tand\t48.7%\tmale,\tso\ta\twell-conducted\tsurvey\tin\tthe\tUS\twould\ttry\tto\tmaintain\tthis\tratio\tin\tthe sample:\t513\tfemale\tand\t487\tmale.\tThis\tis\tcalled\tstratified\tsampling:\tthe\tpopulation\tis\tdivided\tinto homogeneous\tsubgroups\tcalled\tstrata,\tand\tthe\tright\tnumber\tof\tinstances\tis\tsampled\tfrom\teach\tstratum\tto guarantee\tthat\tthe\ttest\tset\tis\trepresentative\tof\tthe\toverall\tpopulation.\tIf\tthey\tused\tpurely\trandom\tsampling, there\twould\tbe\tabout\t12%\tchance\tof\tsampling\ta\tskewed\ttest\tset\twith\teither\tless\tthan\t49%\tfemale\tor\tmore than\t54%\tfemale.\tEither\tway,\tthe\tsurvey\tresults\twould\tbe\tsignificantly\tbiased.\n\nSuppose\tyou\tchatted\twith\texperts\twho\ttold\tyou\tthat\tthe\tmedian\tincome\tis\ta\tvery\timportant\tattribute\tto predict\tmedian\thousing\tprices.\tYou\tmay\twant\tto\tensure\tthat\tthe\ttest\tset\tis\trepresentative\tof\tthe\tvarious categories\tof\tincomes\tin\tthe\twhole\tdataset.\tSince\tthe\tmedian\tincome\tis\ta\tcontinuous\tnumerical\tattribute, you\tfirst\tneed\tto\tcreate\tan\tincome\tcategory\tattribute.\tLet’s\tlook\tat\tthe\tmedian\tincome\thistogram\tmore closely\t(see\tFigure\t2-8):\tmost\tmedian\tincome\tvalues\tare\tclustered\taround\t$20,000–$50,000,\tbut\tsome median\tincomes\tgo\tfar\tbeyond\t$60,000.\tIt\tis\timportant\tto\thave\ta\tsufficient\tnumber\tof\tinstances\tin\tyour dataset\tfor\teach\tstratum,\tor\telse\tthe\testimate\tof\tthe\tstratum’s\timportance\tmay\tbe\tbiased.\tThis\tmeans\tthat you\tshould\tnot\thave\ttoo\tmany\tstrata,\tand\teach\tstratum\tshould\tbe\tlarge\tenough.\tThe\tfollowing\tcode\tcreates an\tincome\tcategory\tattribute\tby\tdividing\tthe\tmedian\tincome\tby\t1.5\t(to\tlimit\tthe\tnumber\tof\tincome\n\ncategories),\tand\trounding\tup\tusing\tceil\t(to\thave\tdiscrete\tcategories),\tand\tthen\tmerging\tall\tthe\tcategories greater\tthan\t5\tinto\tcategory\t5:\n\nhousing[\"income_cat\"]\t=\tnp.ceil(housing[\"median_income\"]\t/\t1.5) housing[\"income_cat\"].where(housing[\"income_cat\"]\t<\t5,\t5.0,\tinplace=True)\n\nThese\tincome\tcategories\tare\trepresented\ton\tFigure\t2-9):\n\nFigure\t2-9.\tHistogram\tof\tincome\tcategories\n\nNow\tyou\tare\tready\tto\tdo\tstratified\tsampling\tbased\ton\tthe\tincome\tcategory.\tFor\tthis\tyou\tcan\tuse\tScikit- Learn’s\tStratifiedShuffleSplit\tclass:\n\nfrom\tsklearn.model_selection\timport\tStratifiedShuffleSplit\n\nsplit\t=\tStratifiedShuffleSplit(n_splits=1,\ttest_size=0.2,\trandom_state=42) for\ttrain_index,\ttest_index\tin\tsplit.split(housing,\thousing[\"income_cat\"]): \t\t\t\tstrat_train_set\t=\thousing.loc[train_index] \t\t\t\tstrat_test_set\t=\thousing.loc[test_index]\n\nLet’s\tsee\tif\tthis\tworked\tas\texpected.\tYou\tcan\tstart\tby\tlooking\tat\tthe\tincome\tcategory\tproportions\tin\tthe full\thousing\tdataset:\n\n>>>\thousing[\"income_cat\"].value_counts()\t/\tlen(housing) 3.0\t\t\t\t0.350581 2.0\t\t\t\t0.318847 4.0\t\t\t\t0.176308 5.0\t\t\t\t0.114438 1.0\t\t\t\t0.039826 Name:\tincome_cat,\tdtype:\tfloat64\n\nWith\tsimilar\tcode\tyou\tcan\tmeasure\tthe\tincome\tcategory\tproportions\tin\tthe\ttest\tset.\tFigure\t2-10\tcompares the\tincome\tcategory\tproportions\tin\tthe\toverall\tdataset,\tin\tthe\ttest\tset\tgenerated\twith\tstratified\tsampling, and\tin\ta\ttest\tset\tgenerated\tusing\tpurely\trandom\tsampling.\tAs\tyou\tcan\tsee,\tthe\ttest\tset\tgenerated\tusing\n\nstratified\tsampling\thas\tincome\tcategory\tproportions\talmost\tidentical\tto\tthose\tin\tthe\tfull\tdataset,\twhereas the\ttest\tset\tgenerated\tusing\tpurely\trandom\tsampling\tis\tquite\tskewed.\n\nFigure\t2-10.\tSampling\tbias\tcomparison\tof\tstratified\tversus\tpurely\trandom\tsampling\n\nNow\tyou\tshould\tremove\tthe\tincome_cat\tattribute\tso\tthe\tdata\tis\tback\tto\tits\toriginal\tstate:\n\nfor\tset_\tin\t(strat_train_set,\tstrat_test_set): \t\t\t\tset_.drop(\"income_cat\",\taxis=1,\tinplace=True)\n\nWe\tspent\tquite\ta\tbit\tof\ttime\ton\ttest\tset\tgeneration\tfor\ta\tgood\treason:\tthis\tis\tan\toften\tneglected\tbut\tcritical part\tof\ta\tMachine\tLearning\tproject.\tMoreover,\tmany\tof\tthese\tideas\twill\tbe\tuseful\tlater\twhen\twe\tdiscuss cross-validation.\tNow\tit’s\ttime\tto\tmove\ton\tto\tthe\tnext\tstage:\texploring\tthe\tdata.\n\nDiscover\tand\tVisualize\tthe\tData\tto\tGain\tInsights So\tfar\tyou\thave\tonly\ttaken\ta\tquick\tglance\tat\tthe\tdata\tto\tget\ta\tgeneral\tunderstanding\tof\tthe\tkind\tof\tdata\tyou are\tmanipulating.\tNow\tthe\tgoal\tis\tto\tgo\ta\tlittle\tbit\tmore\tin\tdepth.\n\nFirst,\tmake\tsure\tyou\thave\tput\tthe\ttest\tset\taside\tand\tyou\tare\tonly\texploring\tthe\ttraining\tset.\tAlso,\tif\tthe training\tset\tis\tvery\tlarge,\tyou\tmay\twant\tto\tsample\tan\texploration\tset,\tto\tmake\tmanipulations\teasy\tand\tfast. In\tour\tcase,\tthe\tset\tis\tquite\tsmall\tso\tyou\tcan\tjust\twork\tdirectly\ton\tthe\tfull\tset.\tLet’s\tcreate\ta\tcopy\tso\tyou can\tplay\twith\tit\twithout\tharming\tthe\ttraining\tset:\n\nhousing\t=\tstrat_train_set.copy()\n\nVisualizing\tGeographical\tData Since\tthere\tis\tgeographical\tinformation\t(latitude\tand\tlongitude),\tit\tis\ta\tgood\tidea\tto\tcreate\ta\tscatterplot\tof all\tdistricts\tto\tvisualize\tthe\tdata\t(Figure\t2-11):\n\nhousing.plot(kind=\"scatter\",\tx=\"longitude\",\ty=\"latitude\")\n\nFigure\t2-11.\tA\tgeographical\tscatterplot\tof\tthe\tdata\n\nThis\tlooks\tlike\tCalifornia\tall\tright,\tbut\tother\tthan\tthat\tit\tis\thard\tto\tsee\tany\tparticular\tpattern.\tSetting\tthe alpha\toption\tto\t0.1\tmakes\tit\tmuch\teasier\tto\tvisualize\tthe\tplaces\twhere\tthere\tis\ta\thigh\tdensity\tof\tdata points\t(Figure\t2-12):\n\nhousing.plot(kind=\"scatter\",\tx=\"longitude\",\ty=\"latitude\",\talpha=0.1)\n\nFigure\t2-12.\tA\tbetter\tvisualization\thighlighting\thigh-density\tareas\n\nNow\tthat’s\tmuch\tbetter:\tyou\tcan\tclearly\tsee\tthe\thigh-density\tareas,\tnamely\tthe\tBay\tArea\tand\taround\tLos Angeles\tand\tSan\tDiego,\tplus\ta\tlong\tline\tof\tfairly\thigh\tdensity\tin\tthe\tCentral\tValley,\tin\tparticular\taround Sacramento\tand\tFresno.\n\nMore\tgenerally,\tour\tbrains\tare\tvery\tgood\tat\tspotting\tpatterns\ton\tpictures,\tbut\tyou\tmay\tneed\tto\tplay\taround with\tvisualization\tparameters\tto\tmake\tthe\tpatterns\tstand\tout.\n\nNow\tlet’s\tlook\tat\tthe\thousing\tprices\t(Figure\t2-13).\tThe\tradius\tof\teach\tcircle\trepresents\tthe\tdistrict’s population\t(option\ts),\tand\tthe\tcolor\trepresents\tthe\tprice\t(option\tc).\tWe\twill\tuse\ta\tpredefined\tcolor\tmap (option\tcmap)\tcalled\tjet,\twhich\tranges\tfrom\tblue\t(low\tvalues)\tto\tred\t(high\tprices):14\n\nhousing.plot(kind=\"scatter\",\tx=\"longitude\",\ty=\"latitude\",\talpha=0.4, \t\t\t\ts=housing[\"population\"]/100,\tlabel=\"population\",\tfigsize=(10,7), \t\t\t\tc=\"median_house_value\",\tcmap=plt.get_cmap(\"jet\"),\tcolorbar=True, ) plt.legend()\n\nFigure\t2-13.\tCalifornia\thousing\tprices\n\nThis\timage\ttells\tyou\tthat\tthe\thousing\tprices\tare\tvery\tmuch\trelated\tto\tthe\tlocation\t(e.g.,\tclose\tto\tthe\tocean) and\tto\tthe\tpopulation\tdensity,\tas\tyou\tprobably\tknew\talready.\tIt\twill\tprobably\tbe\tuseful\tto\tuse\ta\tclustering algorithm\tto\tdetect\tthe\tmain\tclusters,\tand\tadd\tnew\tfeatures\tthat\tmeasure\tthe\tproximity\tto\tthe\tcluster centers.\tThe\tocean\tproximity\tattribute\tmay\tbe\tuseful\tas\twell,\talthough\tin\tNorthern\tCalifornia\tthe\thousing prices\tin\tcoastal\tdistricts\tare\tnot\ttoo\thigh,\tso\tit\tis\tnot\ta\tsimple\trule.\n\nLooking\tfor\tCorrelations Since\tthe\tdataset\tis\tnot\ttoo\tlarge,\tyou\tcan\teasily\tcompute\tthe\tstandard\tcorrelation\tcoefficient\t(also called\tPearson’s\tr)\tbetween\tevery\tpair\tof\tattributes\tusing\tthe\tcorr()\tmethod:\n\ncorr_matrix\t=\thousing.corr()\n\nNow\tlet’s\tlook\tat\thow\tmuch\teach\tattribute\tcorrelates\twith\tthe\tmedian\thouse\tvalue:\n\n>>>\tcorr_matrix[\"median_house_value\"].sort_values(ascending=False) median_house_value\t\t\t\t1.000000 median_income\t\t\t\t\t\t\t\t\t0.687170 total_rooms\t\t\t\t\t\t\t\t\t\t\t0.135231 housing_median_age\t\t\t\t0.114220 households\t\t\t\t\t\t\t\t\t\t\t\t0.064702 total_bedrooms\t\t\t\t\t\t\t\t0.047865 population\t\t\t\t\t\t\t\t\t\t\t-0.026699 longitude\t\t\t\t\t\t\t\t\t\t\t\t-0.047279 latitude\t\t\t\t\t\t\t\t\t\t\t\t\t-0.142826 Name:\tmedian_house_value,\tdtype:\tfloat64\n\nThe\tcorrelation\tcoefficient\tranges\tfrom\t–1\tto\t1.\tWhen\tit\tis\tclose\tto\t1,\tit\tmeans\tthat\tthere\tis\ta\tstrong positive\tcorrelation;\tfor\texample,\tthe\tmedian\thouse\tvalue\ttends\tto\tgo\tup\twhen\tthe\tmedian\tincome\tgoes\tup. When\tthe\tcoefficient\tis\tclose\tto\t–1,\tit\tmeans\tthat\tthere\tis\ta\tstrong\tnegative\tcorrelation;\tyou\tcan\tsee\ta small\tnegative\tcorrelation\tbetween\tthe\tlatitude\tand\tthe\tmedian\thouse\tvalue\t(i.e.,\tprices\thave\ta\tslight tendency\tto\tgo\tdown\twhen\tyou\tgo\tnorth).\tFinally,\tcoefficients\tclose\tto\tzero\tmean\tthat\tthere\tis\tno\tlinear correlation.\tFigure\t2-14\tshows\tvarious\tplots\talong\twith\tthe\tcorrelation\tcoefficient\tbetween\ttheir horizontal\tand\tvertical\taxes.\n\nFigure\t2-14.\tStandard\tcorrelation\tcoefficient\tof\tvarious\tdatasets\t(source:\tWikipedia;\tpublic\tdomain\timage)\n\nWARNING\n\nThe\tcorrelation\tcoefficient\tonly\tmeasures\tlinear\tcorrelations\t(“if\tx\tgoes\tup,\tthen\ty\tgenerally\tgoes\tup/down”).\tIt\tmay\tcompletely miss\tout\ton\tnonlinear\trelationships\t(e.g.,\t“if\tx\tis\tclose\tto\tzero\tthen\ty\tgenerally\tgoes\tup”).\tNote\thow\tall\tthe\tplots\tof\tthe\tbottom\trow have\ta\tcorrelation\tcoefficient\tequal\tto\tzero\tdespite\tthe\tfact\tthat\ttheir\taxes\tare\tclearly\tnot\tindependent:\tthese\tare\texamples\tof nonlinear\trelationships.\tAlso,\tthe\tsecond\trow\tshows\texamples\twhere\tthe\tcorrelation\tcoefficient\tis\tequal\tto\t1\tor\t–1;\tnotice\tthat\tthis has\tnothing\tto\tdo\twith\tthe\tslope.\tFor\texample,\tyour\theight\tin\tinches\thas\ta\tcorrelation\tcoefficient\tof\t1\twith\tyour\theight\tin\tfeet\tor\tin nanometers.\n\nAnother\tway\tto\tcheck\tfor\tcorrelation\tbetween\tattributes\tis\tto\tuse\tPandas’\tscatter_matrix\tfunction, which\tplots\tevery\tnumerical\tattribute\tagainst\tevery\tother\tnumerical\tattribute.\tSince\tthere\tare\tnow\t11 numerical\tattributes,\tyou\twould\tget\t112\t=\t121\tplots,\twhich\twould\tnot\tfit\ton\ta\tpage,\tso\tlet’s\tjust\tfocus\ton\ta few\tpromising\tattributes\tthat\tseem\tmost\tcorrelated\twith\tthe\tmedian\thousing\tvalue\t(Figure\t2-15):\n\nfrom\tpandas.tools.plotting\timport\tscatter_matrix\n\nattributes\t=\t[\"median_house_value\",\t\"median_income\",\t\"total_rooms\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\"housing_median_age\"] scatter_matrix(housing[attributes],\tfigsize=(12,\t8))\n\nFigure\t2-15.\tScatter\tmatrix\n\nThe\tmain\tdiagonal\t(top\tleft\tto\tbottom\tright)\twould\tbe\tfull\tof\tstraight\tlines\tif\tPandas\tplotted\teach\tvariable against\titself,\twhich\twould\tnot\tbe\tvery\tuseful.\tSo\tinstead\tPandas\tdisplays\ta\thistogram\tof\teach\tattribute (other\toptions\tare\tavailable;\tsee\tPandas’\tdocumentation\tfor\tmore\tdetails).\n\nThe\tmost\tpromising\tattribute\tto\tpredict\tthe\tmedian\thouse\tvalue\tis\tthe\tmedian\tincome,\tso\tlet’s\tzoom\tin\ton their\tcorrelation\tscatterplot\t(Figure\t2-16):\n\nhousing.plot(kind=\"scatter\",\tx=\"median_income\",\ty=\"median_house_value\",\n\nalpha=0.1)\n\nThis\tplot\treveals\ta\tfew\tthings.\tFirst,\tthe\tcorrelation\tis\tindeed\tvery\tstrong;\tyou\tcan\tclearly\tsee\tthe\tupward trend\tand\tthe\tpoints\tare\tnot\ttoo\tdispersed.\tSecond,\tthe\tprice\tcap\tthat\twe\tnoticed\tearlier\tis\tclearly\tvisible as\ta\thorizontal\tline\tat\t$500,000.\tBut\tthis\tplot\treveals\tother\tless\tobvious\tstraight\tlines:\ta\thorizontal\tline around\t$450,000,\tanother\taround\t$350,000,\tperhaps\tone\taround\t$280,000,\tand\ta\tfew\tmore\tbelow\tthat. You\tmay\twant\tto\ttry\tremoving\tthe\tcorresponding\tdistricts\tto\tprevent\tyour\talgorithms\tfrom\tlearning\tto reproduce\tthese\tdata\tquirks.\n\nFigure\t2-16.\tMedian\tincome\tversus\tmedian\thouse\tvalue\n\nExperimenting\twith\tAttribute\tCombinations Hopefully\tthe\tprevious\tsections\tgave\tyou\tan\tidea\tof\ta\tfew\tways\tyou\tcan\texplore\tthe\tdata\tand\tgain insights.\tYou\tidentified\ta\tfew\tdata\tquirks\tthat\tyou\tmay\twant\tto\tclean\tup\tbefore\tfeeding\tthe\tdata\tto\ta Machine\tLearning\talgorithm,\tand\tyou\tfound\tinteresting\tcorrelations\tbetween\tattributes,\tin\tparticular\twith the\ttarget\tattribute.\tYou\talso\tnoticed\tthat\tsome\tattributes\thave\ta\ttail-heavy\tdistribution,\tso\tyou\tmay\twant to\ttransform\tthem\t(e.g.,\tby\tcomputing\ttheir\tlogarithm).\tOf\tcourse,\tyour\tmileage\twill\tvary\tconsiderably with\teach\tproject,\tbut\tthe\tgeneral\tideas\tare\tsimilar.\n\nOne\tlast\tthing\tyou\tmay\twant\tto\tdo\tbefore\tactually\tpreparing\tthe\tdata\tfor\tMachine\tLearning\talgorithms\tis to\ttry\tout\tvarious\tattribute\tcombinations.\tFor\texample,\tthe\ttotal\tnumber\tof\trooms\tin\ta\tdistrict\tis\tnot\tvery useful\tif\tyou\tdon’t\tknow\thow\tmany\thouseholds\tthere\tare.\tWhat\tyou\treally\twant\tis\tthe\tnumber\tof\trooms per\thousehold.\tSimilarly,\tthe\ttotal\tnumber\tof\tbedrooms\tby\titself\tis\tnot\tvery\tuseful:\tyou\tprobably\twant\tto compare\tit\tto\tthe\tnumber\tof\trooms.\tAnd\tthe\tpopulation\tper\thousehold\talso\tseems\tlike\tan\tinteresting attribute\tcombination\tto\tlook\tat.\tLet’s\tcreate\tthese\tnew\tattributes:\n\nhousing[\"rooms_per_household\"]\t=\thousing[\"total_rooms\"]/housing[\"households\"] housing[\"bedrooms_per_room\"]\t=\thousing[\"total_bedrooms\"]/housing[\"total_rooms\"] housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n\nAnd\tnow\tlet’s\tlook\tat\tthe\tcorrelation\tmatrix\tagain:\n\n>>>\tcorr_matrix\t=\thousing.corr() >>>\tcorr_matrix[\"median_house_value\"].sort_values(ascending=False) median_house_value\t\t\t\t\t\t\t\t\t\t1.000000 median_income\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.687160 rooms_per_household\t\t\t\t\t\t\t\t\t0.146285 total_rooms\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.135097 housing_median_age\t\t\t\t\t\t\t\t\t\t0.114110 households\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.064506 total_bedrooms\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.047689 population_per_household\t\t\t-0.021985 population\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t-0.026920 longitude\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t-0.047432 latitude\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t-0.142724 bedrooms_per_room\t\t\t\t\t\t\t\t\t\t-0.259984 Name:\tmedian_house_value,\tdtype:\tfloat64\n\nHey,\tnot\tbad!\tThe\tnew\tbedrooms_per_room\tattribute\tis\tmuch\tmore\tcorrelated\twith\tthe\tmedian\thouse value\tthan\tthe\ttotal\tnumber\tof\trooms\tor\tbedrooms.\tApparently\thouses\twith\ta\tlower\tbedroom/room\tratio tend\tto\tbe\tmore\texpensive.\tThe\tnumber\tof\trooms\tper\thousehold\tis\talso\tmore\tinformative\tthan\tthe\ttotal number\tof\trooms\tin\ta\tdistrict\t—\tobviously\tthe\tlarger\tthe\thouses,\tthe\tmore\texpensive\tthey\tare.\n\nThis\tround\tof\texploration\tdoes\tnot\thave\tto\tbe\tabsolutely\tthorough;\tthe\tpoint\tis\tto\tstart\toff\ton\tthe\tright\tfoot and\tquickly\tgain\tinsights\tthat\twill\thelp\tyou\tget\ta\tfirst\treasonably\tgood\tprototype.\tBut\tthis\tis\tan\titerative process:\tonce\tyou\tget\ta\tprototype\tup\tand\trunning,\tyou\tcan\tanalyze\tits\toutput\tto\tgain\tmore\tinsights\tand come\tback\tto\tthis\texploration\tstep.\n\nPrepare\tthe\tData\tfor\tMachine\tLearning\tAlgorithms It’s\ttime\tto\tprepare\tthe\tdata\tfor\tyour\tMachine\tLearning\talgorithms.\tInstead\tof\tjust\tdoing\tthis\tmanually,\tyou should\twrite\tfunctions\tto\tdo\tthat,\tfor\tseveral\tgood\treasons:\n\nThis\twill\tallow\tyou\tto\treproduce\tthese\ttransformations\teasily\ton\tany\tdataset\t(e.g.,\tthe\tnext\ttime\tyou get\ta\tfresh\tdataset).\n\nYou\twill\tgradually\tbuild\ta\tlibrary\tof\ttransformation\tfunctions\tthat\tyou\tcan\treuse\tin\tfuture\tprojects.\n\nYou\tcan\tuse\tthese\tfunctions\tin\tyour\tlive\tsystem\tto\ttransform\tthe\tnew\tdata\tbefore\tfeeding\tit\tto\tyour algorithms.\n\nThis\twill\tmake\tit\tpossible\tfor\tyou\tto\teasily\ttry\tvarious\ttransformations\tand\tsee\twhich\tcombination of\ttransformations\tworks\tbest.\n\nBut\tfirst\tlet’s\trevert\tto\ta\tclean\ttraining\tset\t(by\tcopying\tstrat_train_set\tonce\tagain),\tand\tlet’s\tseparate the\tpredictors\tand\tthe\tlabels\tsince\twe\tdon’t\tnecessarily\twant\tto\tapply\tthe\tsame\ttransformations\tto\tthe predictors\tand\tthe\ttarget\tvalues\t(note\tthat\tdrop()\tcreates\ta\tcopy\tof\tthe\tdata\tand\tdoes\tnot\taffect strat_train_set):\n\nhousing\t=\tstrat_train_set.drop(\"median_house_value\",\taxis=1) housing_labels\t=\tstrat_train_set[\"median_house_value\"].copy()\n\nData\tCleaning Most\tMachine\tLearning\talgorithms\tcannot\twork\twith\tmissing\tfeatures,\tso\tlet’s\tcreate\ta\tfew\tfunctions\tto take\tcare\tof\tthem.\tYou\tnoticed\tearlier\tthat\tthe\ttotal_bedrooms\tattribute\thas\tsome\tmissing\tvalues,\tso let’s\tfix\tthis.\tYou\thave\tthree\toptions:\n\nGet\trid\tof\tthe\tcorresponding\tdistricts.\n\nGet\trid\tof\tthe\twhole\tattribute.\n\nSet\tthe\tvalues\tto\tsome\tvalue\t(zero,\tthe\tmean,\tthe\tmedian,\tetc.).\n\nYou\tcan\taccomplish\tthese\teasily\tusing\tDataFrame’s\tdropna(),\tdrop(),\tand\tfillna()\tmethods:\n\nhousing.dropna(subset=[\"total_bedrooms\"])\t\t\t\t#\toption\t1 housing.drop(\"total_bedrooms\",\taxis=1)\t\t\t\t\t\t\t#\toption\t2 median\t=\thousing[\"total_bedrooms\"].median()\t\t#\toption\t3 housing[\"total_bedrooms\"].fillna(median,\tinplace=True)\n\nIf\tyou\tchoose\toption\t3,\tyou\tshould\tcompute\tthe\tmedian\tvalue\ton\tthe\ttraining\tset,\tand\tuse\tit\tto\tfill\tthe missing\tvalues\tin\tthe\ttraining\tset,\tbut\talso\tdon’t\tforget\tto\tsave\tthe\tmedian\tvalue\tthat\tyou\thave\tcomputed. You\twill\tneed\tit\tlater\tto\treplace\tmissing\tvalues\tin\tthe\ttest\tset\twhen\tyou\twant\tto\tevaluate\tyour\tsystem,\tand also\tonce\tthe\tsystem\tgoes\tlive\tto\treplace\tmissing\tvalues\tin\tnew\tdata.\n\nScikit-Learn\tprovides\ta\thandy\tclass\tto\ttake\tcare\tof\tmissing\tvalues:\tImputer.\tHere\tis\thow\tto\tuse\tit.\tFirst, you\tneed\tto\tcreate\tan\tImputer\tinstance,\tspecifying\tthat\tyou\twant\tto\treplace\teach\tattribute’s\tmissing values\twith\tthe\tmedian\tof\tthat\tattribute:\n\nfrom\tsklearn.preprocessing\timport\tImputer\n\nimputer\t=\tImputer(strategy=\"median\")\n\nSince\tthe\tmedian\tcan\tonly\tbe\tcomputed\ton\tnumerical\tattributes,\twe\tneed\tto\tcreate\ta\tcopy\tof\tthe\tdata without\tthe\ttext\tattribute\tocean_proximity:\n\nhousing_num\t=\thousing.drop(\"ocean_proximity\",\taxis=1)\n\nNow\tyou\tcan\tfit\tthe\timputer\tinstance\tto\tthe\ttraining\tdata\tusing\tthe\tfit()\tmethod:\n\nimputer.fit(housing_num)\n\nThe\timputer\thas\tsimply\tcomputed\tthe\tmedian\tof\teach\tattribute\tand\tstored\tthe\tresult\tin\tits\tstatistics_ instance\tvariable.\tOnly\tthe\ttotal_bedrooms\tattribute\thad\tmissing\tvalues,\tbut\twe\tcannot\tbe\tsure\tthat there\twon’t\tbe\tany\tmissing\tvalues\tin\tnew\tdata\tafter\tthe\tsystem\tgoes\tlive,\tso\tit\tis\tsafer\tto\tapply\tthe imputer\tto\tall\tthe\tnumerical\tattributes:\n\n>>>\timputer.statistics_ array([\t-118.51\t,\t34.26\t,\t29.\t,\t2119.5\t,\t433.\t,\t1164.\t,\t408.\t,\t3.5409]) >>>\thousing_num.median().values\n\narray([\t-118.51\t,\t34.26\t,\t29.\t,\t2119.5\t,\t433.\t,\t1164.\t,\t408.\t,\t3.5409])\n\nNow\tyou\tcan\tuse\tthis\t“trained”\timputer\tto\ttransform\tthe\ttraining\tset\tby\treplacing\tmissing\tvalues\tby\tthe learned\tmedians:\n\nX\t=\timputer.transform(housing_num)\n\nThe\tresult\tis\ta\tplain\tNumpy\tarray\tcontaining\tthe\ttransformed\tfeatures.\tIf\tyou\twant\tto\tput\tit\tback\tinto\ta Pandas\tDataFrame,\tit’s\tsimple:\n\nhousing_tr\t=\tpd.DataFrame(X,\tcolumns=housing_num.columns)\n\nSCIKIT-LEARN\tDESIGN\n\nScikit-Learn’s\tAPI\tis\tremarkably\twell\tdesigned.\tThe\tmain\tdesign\tprinciples\tare:15 Consistency.\tAll\tobjects\tshare\ta\tconsistent\tand\tsimple\tinterface:\n\nEstimators.\tAny\tobject\tthat\tcan\testimate\tsome\tparameters\tbased\ton\ta\tdataset\tis\tcalled\tan\testimator\t(e.g.,\tan\timputer\tis an\testimator).\tThe\testimation\titself\tis\tperformed\tby\tthe\tfit()\tmethod,\tand\tit\ttakes\tonly\ta\tdataset\tas\ta\tparameter\t(or\ttwo for\tsupervised\tlearning\talgorithms;\tthe\tsecond\tdataset\tcontains\tthe\tlabels).\tAny\tother\tparameter\tneeded\tto\tguide\tthe estimation\tprocess\tis\tconsidered\ta\thyperparameter\t(such\tas\tan\timputer’s\tstrategy),\tand\tit\tmust\tbe\tset\tas\tan\tinstance variable\t(generally\tvia\ta\tconstructor\tparameter).\n\nTransformers.\tSome\testimators\t(such\tas\tan\timputer)\tcan\talso\ttransform\ta\tdataset;\tthese\tare\tcalled\ttransformers.\tOnce again,\tthe\tAPI\tis\tquite\tsimple:\tthe\ttransformation\tis\tperformed\tby\tthe\ttransform()\tmethod\twith\tthe\tdataset\tto\ttransform\tas a\tparameter.\tIt\treturns\tthe\ttransformed\tdataset.\tThis\ttransformation\tgenerally\trelies\ton\tthe\tlearned\tparameters,\tas\tis\tthe case\tfor\tan\timputer.\tAll\ttransformers\talso\thave\ta\tconvenience\tmethod\tcalled\tfit_transform()\tthat\tis\tequivalent\tto\tcalling fit()\tand\tthen\ttransform()\t(but\tsometimes\tfit_transform()\tis\toptimized\tand\truns\tmuch\tfaster).\n\nPredictors.\tFinally,\tsome\testimators\tare\tcapable\tof\tmaking\tpredictions\tgiven\ta\tdataset;\tthey\tare\tcalled\tpredictors.\tFor example,\tthe\tLinearRegression\tmodel\tin\tthe\tprevious\tchapter\twas\ta\tpredictor:\tit\tpredicted\tlife\tsatisfaction\tgiven\ta country’s\tGDP\tper\tcapita.\tA\tpredictor\thas\ta\tpredict()\tmethod\tthat\ttakes\ta\tdataset\tof\tnew\tinstances\tand\treturns\ta\tdataset of\tcorresponding\tpredictions.\tIt\talso\thas\ta\tscore()\tmethod\tthat\tmeasures\tthe\tquality\tof\tthe\tpredictions\tgiven\ta\ttest\tset\t(and the\tcorresponding\tlabels\tin\tthe\tcase\tof\tsupervised\tlearning\talgorithms).16\n\nInspection.\tAll\tthe\testimator’s\thyperparameters\tare\taccessible\tdirectly\tvia\tpublic\tinstance\tvariables\t(e.g.,\timputer.strategy), and\tall\tthe\testimator’s\tlearned\tparameters\tare\talso\taccessible\tvia\tpublic\tinstance\tvariables\twith\tan\tunderscore\tsuffix\t(e.g., imputer.statistics_).\n\nNonproliferation\tof\tclasses.\tDatasets\tare\trepresented\tas\tNumPy\tarrays\tor\tSciPy\tsparse\tmatrices,\tinstead\tof\thomemade classes.\tHyperparameters\tare\tjust\tregular\tPython\tstrings\tor\tnumbers.\n\nComposition.\tExisting\tbuilding\tblocks\tare\treused\tas\tmuch\tas\tpossible.\tFor\texample,\tit\tis\teasy\tto\tcreate\ta\tPipeline\testimator from\tan\tarbitrary\tsequence\tof\ttransformers\tfollowed\tby\ta\tfinal\testimator,\tas\twe\twill\tsee.\n\nSensible\tdefaults.\tScikit-Learn\tprovides\treasonable\tdefault\tvalues\tfor\tmost\tparameters,\tmaking\tit\teasy\tto\tcreate\ta\tbaseline working\tsystem\tquickly.\n\nHandling\tText\tand\tCategorical\tAttributes Earlier\twe\tleft\tout\tthe\tcategorical\tattribute\tocean_proximity\tbecause\tit\tis\ta\ttext\tattribute\tso\twe\tcannot compute\tits\tmedian.\tMost\tMachine\tLearning\talgorithms\tprefer\tto\twork\twith\tnumbers\tanyway,\tso\tlet’s convert\tthese\ttext\tlabels\tto\tnumbers.\n\nScikit-Learn\tprovides\ta\ttransformer\tfor\tthis\ttask\tcalled\tLabelEncoder:\n\n>>>\tfrom\tsklearn.preprocessing\timport\tLabelEncoder >>>\tencoder\t=\tLabelEncoder() >>>\thousing_cat\t=\thousing[\"ocean_proximity\"] >>>\thousing_cat_encoded\t=\tencoder.fit_transform(housing_cat) >>>\thousing_cat_encoded array([0,\t0,\t4,\t...,\t1,\t0,\t3])\n\nThis\tis\tbetter:\tnow\twe\tcan\tuse\tthis\tnumerical\tdata\tin\tany\tML\talgorithm.\tYou\tcan\tlook\tat\tthe\tmapping\tthat this\tencoder\thas\tlearned\tusing\tthe\tclasses_\tattribute\t(“<1H\tOCEAN”\tis\tmapped\tto\t0,\t“INLAND”\tis mapped\tto\t1,\tetc.):\n\n>>>\tprint(encoder.classes_) ['<1H\tOCEAN'\t'INLAND'\t'ISLAND'\t'NEAR\tBAY'\t'NEAR\tOCEAN']\n\nOne\tissue\twith\tthis\trepresentation\tis\tthat\tML\talgorithms\twill\tassume\tthat\ttwo\tnearby\tvalues\tare\tmore similar\tthan\ttwo\tdistant\tvalues.\tObviously\tthis\tis\tnot\tthe\tcase\t(for\texample,\tcategories\t0\tand\t4\tare\tmore similar\tthan\tcategories\t0\tand\t1).\tTo\tfix\tthis\tissue,\ta\tcommon\tsolution\tis\tto\tcreate\tone\tbinary\tattribute\tper category:\tone\tattribute\tequal\tto\t1\twhen\tthe\tcategory\tis\t“<1H\tOCEAN”\t(and\t0\totherwise),\tanother attribute\tequal\tto\t1\twhen\tthe\tcategory\tis\t“INLAND”\t(and\t0\totherwise),\tand\tso\ton.\tThis\tis\tcalled\tone-hot encoding,\tbecause\tonly\tone\tattribute\twill\tbe\tequal\tto\t1\t(hot),\twhile\tthe\tothers\twill\tbe\t0\t(cold).\n\nScikit-Learn\tprovides\ta\tOneHotEncoder\tencoder\tto\tconvert\tinteger\tcategorical\tvalues\tinto\tone-hot vectors.\tLet’s\tencode\tthe\tcategories\tas\tone-hot\tvectors.\tNote\tthat\tfit_transform()\texpects\ta\t2D\tarray, but\thousing_cat_encoded\tis\ta\t1D\tarray,\tso\twe\tneed\tto\treshape\tit:17\n\n>>>\tfrom\tsklearn.preprocessing\timport\tOneHotEncoder >>>\tencoder\t=\tOneHotEncoder() >>>\thousing_cat_1hot\t=\tencoder.fit_transform(housing_cat_encoded.reshape(-1,1)) >>>\thousing_cat_1hot <16512x5\tsparse\tmatrix\tof\ttype\t'<class\t'numpy.float64'>'\n\nwith\t16512\tstored\telements\tin\tCompressed\tSparse\tRow\tformat>\n\nNotice\tthat\tthe\toutput\tis\ta\tSciPy\tsparse\tmatrix,\tinstead\tof\ta\tNumPy\tarray.\tThis\tis\tvery\tuseful\twhen\tyou have\tcategorical\tattributes\twith\tthousands\tof\tcategories.\tAfter\tone-hot\tencoding\twe\tget\ta\tmatrix\twith thousands\tof\tcolumns,\tand\tthe\tmatrix\tis\tfull\tof\tzeros\texcept\tfor\tone\t1\tper\trow.\tUsing\tup\ttons\tof\tmemory mostly\tto\tstore\tzeros\twould\tbe\tvery\twasteful,\tso\tinstead\ta\tsparse\tmatrix\tonly\tstores\tthe\tlocation\tof\tthe nonzero\telements.\tYou\tcan\tuse\tit\tmostly\tlike\ta\tnormal\t2D\tarray,18\tbut\tif\tyou\treally\twant\tto\tconvert\tit\tto\ta (dense)\tNumPy\tarray,\tjust\tcall\tthe\ttoarray()\tmethod:\n\n>>>\thousing_cat_1hot.toarray() array([[\t1.,\t\t0.,\t\t0.,\t\t0.,\t\t0.], \t\t\t\t\t\t\t[\t1.,\t\t0.,\t\t0.,\t\t0.,\t\t0.], \t\t\t\t\t\t\t[\t0.,\t\t0.,\t\t0.,\t\t0.,\t\t1.],\n\n..., \t\t\t\t\t\t\t[\t0.,\t\t1.,\t\t0.,\t\t0.,\t\t0.], \t\t\t\t\t\t\t[\t1.,\t\t0.,\t\t0.,\t\t0.,\t\t0.], \t\t\t\t\t\t\t[\t0.,\t\t0.,\t\t0.,\t\t1.,\t\t0.]])\n\nWe\tcan\tapply\tboth\ttransformations\t(from\ttext\tcategories\tto\tinteger\tcategories,\tthen\tfrom\tinteger\tcategories to\tone-hot\tvectors)\tin\tone\tshot\tusing\tthe\tLabelBinarizer\tclass:\n\n>>>\tfrom\tsklearn.preprocessing\timport\tLabelBinarizer >>>\tencoder\t=\tLabelBinarizer() >>>\thousing_cat_1hot\t=\tencoder.fit_transform(housing_cat) >>>\thousing_cat_1hot array([[1,\t0,\t0,\t0,\t0], \t\t\t\t\t\t\t[1,\t0,\t0,\t0,\t0], \t\t\t\t\t\t\t[0,\t0,\t0,\t0,\t1], \t\t\t\t\t\t\t..., \t\t\t\t\t\t\t[0,\t1,\t0,\t0,\t0], \t\t\t\t\t\t\t[1,\t0,\t0,\t0,\t0], \t\t\t\t\t\t\t[0,\t0,\t0,\t1,\t0]])\n\nNote\tthat\tthis\treturns\ta\tdense\tNumPy\tarray\tby\tdefault.\tYou\tcan\tget\ta\tsparse\tmatrix\tinstead\tby\tpassing sparse_output=True\tto\tthe\tLabelBinarizer\tconstructor.\n\nCustom\tTransformers Although\tScikit-Learn\tprovides\tmany\tuseful\ttransformers,\tyou\twill\tneed\tto\twrite\tyour\town\tfor\ttasks\tsuch as\tcustom\tcleanup\toperations\tor\tcombining\tspecific\tattributes.\tYou\twill\twant\tyour\ttransformer\tto\twork seamlessly\twith\tScikit-Learn\tfunctionalities\t(such\tas\tpipelines),\tand\tsince\tScikit-Learn\trelies\ton\tduck typing\t(not\tinheritance),\tall\tyou\tneed\tis\tto\tcreate\ta\tclass\tand\timplement\tthree\tmethods:\tfit()\t(returning self),\ttransform(),\tand\tfit_transform().\tYou\tcan\tget\tthe\tlast\tone\tfor\tfree\tby\tsimply\tadding TransformerMixin\tas\ta\tbase\tclass.\tAlso,\tif\tyou\tadd\tBaseEstimator\tas\ta\tbase\tclass\t(and\tavoid\t*args and\t**kargs\tin\tyour\tconstructor)\tyou\twill\tget\ttwo\textra\tmethods\t(get_params()\tand\tset_params()) that\twill\tbe\tuseful\tfor\tautomatic\thyperparameter\ttuning.\tFor\texample,\there\tis\ta\tsmall\ttransformer\tclass that\tadds\tthe\tcombined\tattributes\twe\tdiscussed\tearlier:\n\nfrom\tsklearn.base\timport\tBaseEstimator,\tTransformerMixin\n\nrooms_ix,\tbedrooms_ix,\tpopulation_ix,\thousehold_ix\t=\t3,\t4,\t5,\t6\n\nclass\tCombinedAttributesAdder(BaseEstimator,\tTransformerMixin): \t\t\t\tdef\t__init__(self,\tadd_bedrooms_per_room\t=\tTrue):\t#\tno\t*args\tor\t**kargs \t\t\t\t\t\t\t\tself.add_bedrooms_per_room\t=\tadd_bedrooms_per_room \t\t\t\tdef\tfit(self,\tX,\ty=None): \t\t\t\t\t\t\t\treturn\tself\t\t#\tnothing\telse\tto\tdo \t\t\t\tdef\ttransform(self,\tX,\ty=None): \t\t\t\t\t\t\t\trooms_per_household\t=\tX[:,\trooms_ix]\t/\tX[:,\thousehold_ix] \t\t\t\t\t\t\t\tpopulation_per_household\t=\tX[:,\tpopulation_ix]\t/\tX[:,\thousehold_ix] \t\t\t\t\t\t\t\tif\tself.add_bedrooms_per_room: \t\t\t\t\t\t\t\t\t\t\t\tbedrooms_per_room\t=\tX[:,\tbedrooms_ix]\t/\tX[:,\trooms_ix] \t\t\t\t\t\t\t\t\t\t\t\treturn\tnp.c_[X,\trooms_per_household,\tpopulation_per_household, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbedrooms_per_room] \t\t\t\t\t\t\t\telse: \t\t\t\t\t\t\t\t\t\t\t\treturn\tnp.c_[X,\trooms_per_household,\tpopulation_per_household]\n\nattr_adder\t=\tCombinedAttributesAdder(add_bedrooms_per_room=False) housing_extra_attribs\t=\tattr_adder.transform(housing.values)\n\nIn\tthis\texample\tthe\ttransformer\thas\tone\thyperparameter,\tadd_bedrooms_per_room,\tset\tto\tTrue\tby\tdefault (it\tis\toften\thelpful\tto\tprovide\tsensible\tdefaults).\tThis\thyperparameter\twill\tallow\tyou\tto\teasily\tfind\tout whether\tadding\tthis\tattribute\thelps\tthe\tMachine\tLearning\talgorithms\tor\tnot.\tMore\tgenerally,\tyou\tcan\tadd\ta hyperparameter\tto\tgate\tany\tdata\tpreparation\tstep\tthat\tyou\tare\tnot\t100%\tsure\tabout.\tThe\tmore\tyou automate\tthese\tdata\tpreparation\tsteps,\tthe\tmore\tcombinations\tyou\tcan\tautomatically\ttry\tout,\tmaking\tit much\tmore\tlikely\tthat\tyou\twill\tfind\ta\tgreat\tcombination\t(and\tsaving\tyou\ta\tlot\tof\ttime).\n\nFeature\tScaling One\tof\tthe\tmost\timportant\ttransformations\tyou\tneed\tto\tapply\tto\tyour\tdata\tis\tfeature\tscaling.\tWith\tfew exceptions,\tMachine\tLearning\talgorithms\tdon’t\tperform\twell\twhen\tthe\tinput\tnumerical\tattributes\thave very\tdifferent\tscales.\tThis\tis\tthe\tcase\tfor\tthe\thousing\tdata:\tthe\ttotal\tnumber\tof\trooms\tranges\tfrom\tabout\t6 to\t39,320,\twhile\tthe\tmedian\tincomes\tonly\trange\tfrom\t0\tto\t15.\tNote\tthat\tscaling\tthe\ttarget\tvalues\tis generally\tnot\trequired.\n\nThere\tare\ttwo\tcommon\tways\tto\tget\tall\tattributes\tto\thave\tthe\tsame\tscale:\tmin-max\tscaling\tand standardization.\n\nMin-max\tscaling\t(many\tpeople\tcall\tthis\tnormalization)\tis\tquite\tsimple:\tvalues\tare\tshifted\tand\trescaled so\tthat\tthey\tend\tup\tranging\tfrom\t0\tto\t1.\tWe\tdo\tthis\tby\tsubtracting\tthe\tmin\tvalue\tand\tdividing\tby\tthe\tmax minus\tthe\tmin.\tScikit-Learn\tprovides\ta\ttransformer\tcalled\tMinMaxScaler\tfor\tthis.\tIt\thas\ta feature_range\thyperparameter\tthat\tlets\tyou\tchange\tthe\trange\tif\tyou\tdon’t\twant\t0–1\tfor\tsome\treason.\n\nStandardization\tis\tquite\tdifferent:\tfirst\tit\tsubtracts\tthe\tmean\tvalue\t(so\tstandardized\tvalues\talways\thave\ta zero\tmean),\tand\tthen\tit\tdivides\tby\tthe\tvariance\tso\tthat\tthe\tresulting\tdistribution\thas\tunit\tvariance.\tUnlike min-max\tscaling,\tstandardization\tdoes\tnot\tbound\tvalues\tto\ta\tspecific\trange,\twhich\tmay\tbe\ta\tproblem\tfor some\talgorithms\t(e.g.,\tneural\tnetworks\toften\texpect\tan\tinput\tvalue\tranging\tfrom\t0\tto\t1).\tHowever, standardization\tis\tmuch\tless\taffected\tby\toutliers.\tFor\texample,\tsuppose\ta\tdistrict\thad\ta\tmedian\tincome equal\tto\t100\t(by\tmistake).\tMin-max\tscaling\twould\tthen\tcrush\tall\tthe\tother\tvalues\tfrom\t0–15\tdown\tto\t0– 0.15,\twhereas\tstandardization\twould\tnot\tbe\tmuch\taffected.\tScikit-Learn\tprovides\ta\ttransformer\tcalled StandardScaler\tfor\tstandardization.\n\nWARNING\n\nAs\twith\tall\tthe\ttransformations,\tit\tis\timportant\tto\tfit\tthe\tscalers\tto\tthe\ttraining\tdata\tonly,\tnot\tto\tthe\tfull\tdataset\t(including\tthe\ttest set).\tOnly\tthen\tcan\tyou\tuse\tthem\tto\ttransform\tthe\ttraining\tset\tand\tthe\ttest\tset\t(and\tnew\tdata).\n\nTransformation\tPipelines As\tyou\tcan\tsee,\tthere\tare\tmany\tdata\ttransformation\tsteps\tthat\tneed\tto\tbe\texecuted\tin\tthe\tright\torder. Fortunately,\tScikit-Learn\tprovides\tthe\tPipeline\tclass\tto\thelp\twith\tsuch\tsequences\tof\ttransformations. Here\tis\ta\tsmall\tpipeline\tfor\tthe\tnumerical\tattributes:\n\nfrom\tsklearn.pipeline\timport\tPipeline from\tsklearn.preprocessing\timport\tStandardScaler\n\nnum_pipeline\t=\tPipeline([ \t\t\t\t\t\t\t\t('imputer',\tImputer(strategy=\"median\")), \t\t\t\t\t\t\t\t('attribs_adder',\tCombinedAttributesAdder()), \t\t\t\t\t\t\t\t('std_scaler',\tStandardScaler()), \t\t\t\t])\n\nhousing_num_tr\t=\tnum_pipeline.fit_transform(housing_num)\n\nThe\tPipeline\tconstructor\ttakes\ta\tlist\tof\tname/estimator\tpairs\tdefining\ta\tsequence\tof\tsteps.\tAll\tbut\tthe last\testimator\tmust\tbe\ttransformers\t(i.e.,\tthey\tmust\thave\ta\tfit_transform()\tmethod).\tThe\tnames\tcan\tbe anything\tyou\tlike\t(as\tlong\tas\tthey\tdon’t\tcontain\tdouble\tunderscores\t“__”).\n\nWhen\tyou\tcall\tthe\tpipeline’s\tfit()\tmethod,\tit\tcalls\tfit_transform()\tsequentially\ton\tall\ttransformers, passing\tthe\toutput\tof\teach\tcall\tas\tthe\tparameter\tto\tthe\tnext\tcall,\tuntil\tit\treaches\tthe\tfinal\testimator,\tfor which\tit\tjust\tcalls\tthe\tfit()\tmethod.\n\nThe\tpipeline\texposes\tthe\tsame\tmethods\tas\tthe\tfinal\testimator.\tIn\tthis\texample,\tthe\tlast\testimator\tis\ta StandardScaler,\twhich\tis\ta\ttransformer,\tso\tthe\tpipeline\thas\ta\ttransform()\tmethod\tthat\tapplies\tall\tthe transforms\tto\tthe\tdata\tin\tsequence\t(it\talso\thas\ta\tfit_transform\tmethod\tthat\twe\tcould\thave\tused\tinstead of\tcalling\tfit()\tand\tthen\ttransform()).\n\nNow\tit\twould\tbe\tnice\tif\twe\tcould\tfeed\ta\tPandas\tDataFrame\tdirectly\tinto\tour\tpipeline,\tinstead\tof\thaving to\tfirst\tmanually\textract\tthe\tnumerical\tcolumns\tinto\ta\tNumPy\tarray.\tThere\tis\tnothing\tin\tScikit-Learn\tto handle\tPandas\tDataFrames,19\tbut\twe\tcan\twrite\ta\tcustom\ttransformer\tfor\tthis\ttask:\n\nfrom\tsklearn.base\timport\tBaseEstimator,\tTransformerMixin\n\nclass\tDataFrameSelector(BaseEstimator,\tTransformerMixin): \t\t\t\tdef\t__init__(self,\tattribute_names): \t\t\t\t\t\t\t\tself.attribute_names\t=\tattribute_names \t\t\t\tdef\tfit(self,\tX,\ty=None): \t\t\t\t\t\t\t\treturn\tself \t\t\t\tdef\ttransform(self,\tX): \t\t\t\t\t\t\t\treturn\tX[self.attribute_names].values\n\nOur\tDataFrameSelector\twill\ttransform\tthe\tdata\tby\tselecting\tthe\tdesired\tattributes,\tdropping\tthe\trest, and\tconverting\tthe\tresulting\tDataFrame\tto\ta\tNumPy\tarray.\tWith\tthis,\tyou\tcan\teasily\twrite\ta\tpipeline\tthat will\ttake\ta\tPandas\tDataFrame\tand\thandle\tonly\tthe\tnumerical\tvalues:\tthe\tpipeline\twould\tjust\tstart\twith\ta DataFrameSelector\tto\tpick\tonly\tthe\tnumerical\tattributes,\tfollowed\tby\tthe\tother\tpreprocessing\tsteps\twe discussed\tearlier.\tAnd\tyou\tcan\tjust\tas\teasily\twrite\tanother\tpipeline\tfor\tthe\tcategorical\tattributes\tas\twell by\tsimply\tselecting\tthe\tcategorical\tattributes\tusing\ta\tDataFrameSelector\tand\tthen\tapplying\ta LabelBinarizer\t.\n\nnum_attribs\t=\tlist(housing_num) cat_attribs\t=\t[\"ocean_proximity\"]\n\nnum_pipeline\t=\tPipeline([ \t\t\t\t\t\t\t\t('selector',\tDataFrameSelector(num_attribs)), \t\t\t\t\t\t\t\t('imputer',\tImputer(strategy=\"median\")), \t\t\t\t\t\t\t\t('attribs_adder',\tCombinedAttributesAdder()), \t\t\t\t\t\t\t\t('std_scaler',\tStandardScaler()), \t\t\t\t])\n\ncat_pipeline\t=\tPipeline([ \t\t\t\t\t\t\t\t('selector',\tDataFrameSelector(cat_attribs)), \t\t\t\t\t\t\t\t('label_binarizer',\tLabelBinarizer()), \t\t\t\t])\n\nBut\thow\tcan\tyou\tjoin\tthese\ttwo\tpipelines\tinto\ta\tsingle\tpipeline?\tThe\tanswer\tis\tto\tuse\tScikit-Learn’s FeatureUnion\tclass.\tYou\tgive\tit\ta\tlist\tof\ttransformers\t(which\tcan\tbe\tentire\ttransformer\tpipelines);\twhen its\ttransform()\tmethod\tis\tcalled,\tit\truns\teach\ttransformer’s\ttransform()\tmethod\tin\tparallel,\twaits\tfor their\toutput,\tand\tthen\tconcatenates\tthem\tand\treturns\tthe\tresult\t(and\tof\tcourse\tcalling\tits\tfit()\tmethod calls\teach\ttransformer’s\tfit()\tmethod).\tA\tfull\tpipeline\thandling\tboth\tnumerical\tand\tcategorical attributes\tmay\tlook\tlike\tthis:\n\nfrom\tsklearn.pipeline\timport\tFeatureUnion\n\nfull_pipeline\t=\tFeatureUnion(transformer_list=[ \t\t\t\t\t\t\t\t(\"num_pipeline\",\tnum_pipeline), \t\t\t\t\t\t\t\t(\"cat_pipeline\",\tcat_pipeline), \t\t\t\t])\n\nAnd\tyou\tcan\trun\tthe\twhole\tpipeline\tsimply:\n\n>>>\thousing_prepared\t=\tfull_pipeline.fit_transform(housing) >>>\thousing_prepared array([[-1.15604281,\t\t0.77194962,\t\t0.74333089,\t...,\t\t0.\t\t\t\t\t\t\t\t, \t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t,\t\t0.\t\t\t\t\t\t\t\t], \t\t\t\t\t\t\t[-1.17602483,\t\t0.6596948\t,\t-1.1653172\t,\t...,\t\t0.\t\t\t\t\t\t\t\t, \t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t,\t\t0.\t\t\t\t\t\t\t\t], \t\t\t\t\t\t\t[...] >>>\thousing_prepared.shape (16512,\t16)\n\nSelect\tand\tTrain\ta\tModel At\tlast!\tYou\tframed\tthe\tproblem,\tyou\tgot\tthe\tdata\tand\texplored\tit,\tyou\tsampled\ta\ttraining\tset\tand\ta\ttest\tset, and\tyou\twrote\ttransformation\tpipelines\tto\tclean\tup\tand\tprepare\tyour\tdata\tfor\tMachine\tLearning algorithms\tautomatically.\tYou\tare\tnow\tready\tto\tselect\tand\ttrain\ta\tMachine\tLearning\tmodel.\n\nTraining\tand\tEvaluating\ton\tthe\tTraining\tSet The\tgood\tnews\tis\tthat\tthanks\tto\tall\tthese\tprevious\tsteps,\tthings\tare\tnow\tgoing\tto\tbe\tmuch\tsimpler\tthan\tyou might\tthink.\tLet’s\tfirst\ttrain\ta\tLinear\tRegression\tmodel,\tlike\twe\tdid\tin\tthe\tprevious\tchapter:\n\nfrom\tsklearn.linear_model\timport\tLinearRegression\n\nlin_reg\t=\tLinearRegression() lin_reg.fit(housing_prepared,\thousing_labels)\n\nDone!\tYou\tnow\thave\ta\tworking\tLinear\tRegression\tmodel.\tLet’s\ttry\tit\tout\ton\ta\tfew\tinstances\tfrom\tthe training\tset:\n\n>>>\tsome_data\t=\thousing.iloc[:5] >>>\tsome_labels\t=\thousing_labels.iloc[:5] >>>\tsome_data_prepared\t=\tfull_pipeline.transform(some_data) >>>\tprint(\"Predictions:\",\tlin_reg.predict(some_data_prepared)) Predictions:\t[\t210644.6045\t\t317768.8069\t\t210956.4333\t\t59218.9888\t\t189747.5584] >>>\tprint(\"Labels:\",\tlist(some_labels)) Labels:\t[286600.0,\t340600.0,\t196900.0,\t46300.0,\t254500.0]\n\nIt\tworks,\talthough\tthe\tpredictions\tare\tnot\texactly\taccurate\t(e.g.,\tthe\tfirst\tprediction\tis\toff\tby\tclose\tto 40%!).\tLet’s\tmeasure\tthis\tregression\tmodel’s\tRMSE\ton\tthe\twhole\ttraining\tset\tusing\tScikit-Learn’s mean_squared_error\tfunction:\n\n>>>\tfrom\tsklearn.metrics\timport\tmean_squared_error >>>\thousing_predictions\t=\tlin_reg.predict(housing_prepared) >>>\tlin_mse\t=\tmean_squared_error(housing_labels,\thousing_predictions) >>>\tlin_rmse\t=\tnp.sqrt(lin_mse) >>>\tlin_rmse 68628.198198489219\n\nOkay,\tthis\tis\tbetter\tthan\tnothing\tbut\tclearly\tnot\ta\tgreat\tscore:\tmost\tdistricts’\tmedian_housing_values range\tbetween\t$120,000\tand\t$265,000,\tso\ta\ttypical\tprediction\terror\tof\t$68,628\tis\tnot\tvery\tsatisfying. This\tis\tan\texample\tof\ta\tmodel\tunderfitting\tthe\ttraining\tdata.\tWhen\tthis\thappens\tit\tcan\tmean\tthat\tthe features\tdo\tnot\tprovide\tenough\tinformation\tto\tmake\tgood\tpredictions,\tor\tthat\tthe\tmodel\tis\tnot\tpowerful enough.\tAs\twe\tsaw\tin\tthe\tprevious\tchapter,\tthe\tmain\tways\tto\tfix\tunderfitting\tare\tto\tselect\ta\tmore powerful\tmodel,\tto\tfeed\tthe\ttraining\talgorithm\twith\tbetter\tfeatures,\tor\tto\treduce\tthe\tconstraints\ton\tthe model.\tThis\tmodel\tis\tnot\tregularized,\tso\tthis\trules\tout\tthe\tlast\toption.\tYou\tcould\ttry\tto\tadd\tmore\tfeatures (e.g.,\tthe\tlog\tof\tthe\tpopulation),\tbut\tfirst\tlet’s\ttry\ta\tmore\tcomplex\tmodel\tto\tsee\thow\tit\tdoes.\n\nLet’s\ttrain\ta\tDecisionTreeRegressor.\tThis\tis\ta\tpowerful\tmodel,\tcapable\tof\tfinding\tcomplex\tnonlinear relationships\tin\tthe\tdata\t(Decision\tTrees\tare\tpresented\tin\tmore\tdetail\tin\tChapter\t6).\tThe\tcode\tshould\tlook familiar\tby\tnow:\n\nfrom\tsklearn.tree\timport\tDecisionTreeRegressor\n\ntree_reg\t=\tDecisionTreeRegressor() tree_reg.fit(housing_prepared,\thousing_labels)\n\nNow\tthat\tthe\tmodel\tis\ttrained,\tlet’s\tevaluate\tit\ton\tthe\ttraining\tset:\n\n>>>\thousing_predictions\t=\ttree_reg.predict(housing_prepared) >>>\ttree_mse\t=\tmean_squared_error(housing_labels,\thousing_predictions) >>>\ttree_rmse\t=\tnp.sqrt(tree_mse) >>>\ttree_rmse 0.0\n\nWait,\twhat!?\tNo\terror\tat\tall?\tCould\tthis\tmodel\treally\tbe\tabsolutely\tperfect?\tOf\tcourse,\tit\tis\tmuch\tmore likely\tthat\tthe\tmodel\thas\tbadly\toverfit\tthe\tdata.\tHow\tcan\tyou\tbe\tsure?\tAs\twe\tsaw\tearlier,\tyou\tdon’t\twant to\ttouch\tthe\ttest\tset\tuntil\tyou\tare\tready\tto\tlaunch\ta\tmodel\tyou\tare\tconfident\tabout,\tso\tyou\tneed\tto\tuse\tpart of\tthe\ttraining\tset\tfor\ttraining,\tand\tpart\tfor\tmodel\tvalidation.\n\nBetter\tEvaluation\tUsing\tCross-Validation One\tway\tto\tevaluate\tthe\tDecision\tTree\tmodel\twould\tbe\tto\tuse\tthe\ttrain_test_split\tfunction\tto\tsplit the\ttraining\tset\tinto\ta\tsmaller\ttraining\tset\tand\ta\tvalidation\tset,\tthen\ttrain\tyour\tmodels\tagainst\tthe\tsmaller training\tset\tand\tevaluate\tthem\tagainst\tthe\tvalidation\tset.\tIt’s\ta\tbit\tof\twork,\tbut\tnothing\ttoo\tdifficult\tand\tit would\twork\tfairly\twell.\n\nA\tgreat\talternative\tis\tto\tuse\tScikit-Learn’s\tcross-validation\tfeature.\tThe\tfollowing\tcode\tperforms\tK-fold cross-validation:\tit\trandomly\tsplits\tthe\ttraining\tset\tinto\t10\tdistinct\tsubsets\tcalled\tfolds,\tthen\tit\ttrains\tand evaluates\tthe\tDecision\tTree\tmodel\t10\ttimes,\tpicking\ta\tdifferent\tfold\tfor\tevaluation\tevery\ttime\tand training\ton\tthe\tother\t9\tfolds.\tThe\tresult\tis\tan\tarray\tcontaining\tthe\t10\tevaluation\tscores:\n\nfrom\tsklearn.model_selection\timport\tcross_val_score scores\t=\tcross_val_score(tree_reg,\thousing_prepared,\thousing_labels, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscoring=\"neg_mean_squared_error\",\tcv=10) tree_rmse_scores\t=\tnp.sqrt(-scores)\n\nWARNING\n\nScikit-Learn\tcross-validation\tfeatures\texpect\ta\tutility\tfunction\t(greater\tis\tbetter)\trather\tthan\ta\tcost\tfunction\t(lower\tis\tbetter),\tso the\tscoring\tfunction\tis\tactually\tthe\topposite\tof\tthe\tMSE\t(i.e.,\ta\tnegative\tvalue),\twhich\tis\twhy\tthe\tpreceding\tcode\tcomputes\t- scores\tbefore\tcalculating\tthe\tsquare\troot.\n\nLet’s\tlook\tat\tthe\tresults:\n\n>>>\tdef\tdisplay_scores(scores): ...\t\t\t\t\tprint(\"Scores:\",\tscores) ...\t\t\t\t\tprint(\"Mean:\",\tscores.mean()) ...\t\t\t\t\tprint(\"Standard\tdeviation:\",\tscores.std()) ... >>>\tdisplay_scores(tree_rmse_scores) Scores:\t[\t70232.0136482\t\t\t66828.46839892\t\t72444.08721003\t\t70761.50186201 \t\t71125.52697653\t\t75581.29319857\t\t70169.59286164\t\t70055.37863456 \t\t75370.49116773\t\t71222.39081244] Mean:\t71379.0744771 Standard\tdeviation:\t2458.31882043\n\nNow\tthe\tDecision\tTree\tdoesn’t\tlook\tas\tgood\tas\tit\tdid\tearlier.\tIn\tfact,\tit\tseems\tto\tperform\tworse\tthan\tthe Linear\tRegression\tmodel!\tNotice\tthat\tcross-validation\tallows\tyou\tto\tget\tnot\tonly\tan\testimate\tof\tthe performance\tof\tyour\tmodel,\tbut\talso\ta\tmeasure\tof\thow\tprecise\tthis\testimate\tis\t(i.e.,\tits\tstandard deviation).\tThe\tDecision\tTree\thas\ta\tscore\tof\tapproximately\t71,379,\tgenerally\t±2,458.\tYou\twould\tnot have\tthis\tinformation\tif\tyou\tjust\tused\tone\tvalidation\tset.\tBut\tcross-validation\tcomes\tat\tthe\tcost\tof\ttraining the\tmodel\tseveral\ttimes,\tso\tit\tis\tnot\talways\tpossible.\n\nLet’s\tcompute\tthe\tsame\tscores\tfor\tthe\tLinear\tRegression\tmodel\tjust\tto\tbe\tsure:\n\n>>>\tlin_scores\t=\tcross_val_score(lin_reg,\thousing_prepared,\thousing_labels, ...\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscoring=\"neg_mean_squared_error\",\tcv=10) ... >>>\tlin_rmse_scores\t=\tnp.sqrt(-lin_scores) >>>\tdisplay_scores(lin_rmse_scores) Scores:\t[\t66760.97371572\t\t66962.61914244\t\t70349.94853401\t\t74757.02629506 \t\t68031.13388938\t\t71193.84183426\t\t64968.13706527\t\t68261.95557897\n\n71527.64217874\t\t67665.10082067] Mean:\t69047.8379055 Standard\tdeviation:\t2735.51074287\n\nThat’s\tright:\tthe\tDecision\tTree\tmodel\tis\toverfitting\tso\tbadly\tthat\tit\tperforms\tworse\tthan\tthe\tLinear Regression\tmodel.\n\nLet’s\ttry\tone\tlast\tmodel\tnow:\tthe\tRandomForestRegressor.\tAs\twe\twill\tsee\tin\tChapter\t7,\tRandom Forests\twork\tby\ttraining\tmany\tDecision\tTrees\ton\trandom\tsubsets\tof\tthe\tfeatures,\tthen\taveraging\tout\ttheir predictions.\tBuilding\ta\tmodel\ton\ttop\tof\tmany\tother\tmodels\tis\tcalled\tEnsemble\tLearning,\tand\tit\tis\toften\ta great\tway\tto\tpush\tML\talgorithms\teven\tfurther.\tWe\twill\tskip\tmost\tof\tthe\tcode\tsince\tit\tis\tessentially\tthe same\tas\tfor\tthe\tother\tmodels:\n\n>>>\tfrom\tsklearn.ensemble\timport\tRandomForestRegressor >>>\tforest_reg\t=\tRandomForestRegressor() >>>\tforest_reg.fit(housing_prepared,\thousing_labels) >>>\t[...] >>>\tforest_rmse 21941.911027380233 >>>\tdisplay_scores(forest_rmse_scores) Scores:\t[\t51650.94405471\t\t48920.80645498\t\t52979.16096752\t\t54412.74042021 \t\t50861.29381163\t\t56488.55699727\t\t51866.90120786\t\t49752.24599537 \t\t55399.50713191\t\t53309.74548294] Mean:\t52564.1902524 Standard\tdeviation:\t2301.87380392\n\nWow,\tthis\tis\tmuch\tbetter:\tRandom\tForests\tlook\tvery\tpromising.\tHowever,\tnote\tthat\tthe\tscore\ton\tthe training\tset\tis\tstill\tmuch\tlower\tthan\ton\tthe\tvalidation\tsets,\tmeaning\tthat\tthe\tmodel\tis\tstill\toverfitting\tthe training\tset.\tPossible\tsolutions\tfor\toverfitting\tare\tto\tsimplify\tthe\tmodel,\tconstrain\tit\t(i.e.,\tregularize\tit),\tor get\ta\tlot\tmore\ttraining\tdata.\tHowever,\tbefore\tyou\tdive\tmuch\tdeeper\tin\tRandom\tForests,\tyou\tshould\ttry\tout many\tother\tmodels\tfrom\tvarious\tcategories\tof\tMachine\tLearning\talgorithms\t(several\tSupport\tVector Machines\twith\tdifferent\tkernels,\tpossibly\ta\tneural\tnetwork,\tetc.),\twithout\tspending\ttoo\tmuch\ttime tweaking\tthe\thyperparameters.\tThe\tgoal\tis\tto\tshortlist\ta\tfew\t(two\tto\tfive)\tpromising\tmodels.\n\nTIP\n\nYou\tshould\tsave\tevery\tmodel\tyou\texperiment\twith,\tso\tyou\tcan\tcome\tback\teasily\tto\tany\tmodel\tyou\twant.\tMake\tsure\tyou\tsave both\tthe\thyperparameters\tand\tthe\ttrained\tparameters,\tas\twell\tas\tthe\tcross-validation\tscores\tand\tperhaps\tthe\tactual\tpredictions\tas well.\tThis\twill\tallow\tyou\tto\teasily\tcompare\tscores\tacross\tmodel\ttypes,\tand\tcompare\tthe\ttypes\tof\terrors\tthey\tmake.\tYou\tcan easily\tsave\tScikit-Learn\tmodels\tby\tusing\tPython’s\tpickle\tmodule,\tor\tusing\tsklearn.externals.joblib,\twhich\tis\tmore\tefficient at\tserializing\tlarge\tNumPy\tarrays:\n\nfrom\tsklearn.externals\timport\tjoblib\n\njoblib.dump(my_model,\t\"my_model.pkl\") #\tand\tlater... my_model_loaded\t=\tjoblib.load(\"my_model.pkl\")\n\nFine-Tune\tYour\tModel Let’s\tassume\tthat\tyou\tnow\thave\ta\tshortlist\tof\tpromising\tmodels.\tYou\tnow\tneed\tto\tfine-tune\tthem.\tLet’s look\tat\ta\tfew\tways\tyou\tcan\tdo\tthat.\n\nGrid\tSearch One\tway\tto\tdo\tthat\twould\tbe\tto\tfiddle\twith\tthe\thyperparameters\tmanually,\tuntil\tyou\tfind\ta\tgreat combination\tof\thyperparameter\tvalues.\tThis\twould\tbe\tvery\ttedious\twork,\tand\tyou\tmay\tnot\thave\ttime\tto explore\tmany\tcombinations.\n\nInstead\tyou\tshould\tget\tScikit-Learn’s\tGridSearchCV\tto\tsearch\tfor\tyou.\tAll\tyou\tneed\tto\tdo\tis\ttell\tit\twhich hyperparameters\tyou\twant\tit\tto\texperiment\twith,\tand\twhat\tvalues\tto\ttry\tout,\tand\tit\twill\tevaluate\tall\tthe possible\tcombinations\tof\thyperparameter\tvalues,\tusing\tcross-validation.\tFor\texample,\tthe\tfollowing\tcode searches\tfor\tthe\tbest\tcombination\tof\thyperparameter\tvalues\tfor\tthe\tRandomForestRegressor:\n\nfrom\tsklearn.model_selection\timport\tGridSearchCV\n\nparam_grid\t=\t[ \t\t\t\t{'n_estimators':\t[3,\t10,\t30],\t'max_features':\t[2,\t4,\t6,\t8]}, \t\t\t\t{'bootstrap':\t[False],\t'n_estimators':\t[3,\t10],\t'max_features':\t[2,\t3,\t4]}, \t\t]\n\nforest_reg\t=\tRandomForestRegressor()\n\ngrid_search\t=\tGridSearchCV(forest_reg,\tparam_grid,\tcv=5, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscoring='neg_mean_squared_error')\n\ngrid_search.fit(housing_prepared,\thousing_labels)\n\nTIP\n\nWhen\tyou\thave\tno\tidea\twhat\tvalue\ta\thyperparameter\tshould\thave,\ta\tsimple\tapproach\tis\tto\ttry\tout\tconsecutive\tpowers\tof\t10\t(or\ta smaller\tnumber\tif\tyou\twant\ta\tmore\tfine-grained\tsearch,\tas\tshown\tin\tthis\texample\twith\tthe\tn_estimators\thyperparameter).\n\nThis\tparam_grid\ttells\tScikit-Learn\tto\tfirst\tevaluate\tall\t3\t×\t4\t=\t12\tcombinations\tof\tn_estimators\tand max_features\thyperparameter\tvalues\tspecified\tin\tthe\tfirst\tdict\t(don’t\tworry\tabout\twhat\tthese hyperparameters\tmean\tfor\tnow;\tthey\twill\tbe\texplained\tin\tChapter\t7),\tthen\ttry\tall\t2\t×\t3\t=\t6\tcombinations of\thyperparameter\tvalues\tin\tthe\tsecond\tdict,\tbut\tthis\ttime\twith\tthe\tbootstrap\thyperparameter\tset\tto False\tinstead\tof\tTrue\t(which\tis\tthe\tdefault\tvalue\tfor\tthis\thyperparameter).\n\nAll\tin\tall,\tthe\tgrid\tsearch\twill\texplore\t12\t+\t6\t=\t18\tcombinations\tof\tRandomForestRegressor hyperparameter\tvalues,\tand\tit\twill\ttrain\teach\tmodel\tfive\ttimes\t(since\twe\tare\tusing\tfive-fold\tcross validation).\tIn\tother\twords,\tall\tin\tall,\tthere\twill\tbe\t18\t×\t5\t=\t90\trounds\tof\ttraining!\tIt\tmay\ttake\tquite\ta\tlong time,\tbut\twhen\tit\tis\tdone\tyou\tcan\tget\tthe\tbest\tcombination\tof\tparameters\tlike\tthis:\n\n>>>\tgrid_search.best_params_ {'max_features':\t8,\t'n_estimators':\t30}\n\nTIP\n\nSince\t8\tand\t30\tare\tthe\tmaximum\tvalues\tthat\twere\tevaluated,\tyou\tshould\tprobably\ttry\tsearching\tagain\twith\thigher\tvalues,\tsince the\tscore\tmay\tcontinue\tto\timprove.\n\nYou\tcan\talso\tget\tthe\tbest\testimator\tdirectly:\n\n>>>\tgrid_search.best_estimator_ RandomForestRegressor(bootstrap=True,\tcriterion='mse',\tmax_depth=None, \t\t\t\t\t\t\t\t\t\t\tmax_features=8,\tmax_leaf_nodes=None,\tmin_impurity_split=1e-07, \t\t\t\t\t\t\t\t\t\t\tmin_samples_leaf=1,\tmin_samples_split=2, \t\t\t\t\t\t\t\t\t\t\tmin_weight_fraction_leaf=0.0,\tn_estimators=30,\tn_jobs=1, \t\t\t\t\t\t\t\t\t\t\toob_score=False,\trandom_state=42,\tverbose=0,\twarm_start=False)\n\nNOTE\n\nIf\tGridSearchCV\tis\tinitialized\twith\trefit=True\t(which\tis\tthe\tdefault),\tthen\tonce\tit\tfinds\tthe\tbest\testimator\tusing\tcross-validation,\tit retrains\tit\ton\tthe\twhole\ttraining\tset.\tThis\tis\tusually\ta\tgood\tidea\tsince\tfeeding\tit\tmore\tdata\twill\tlikely\timprove\tits\tperformance.\n\nAnd\tof\tcourse\tthe\tevaluation\tscores\tare\talso\tavailable:\n\n>>>\tcvres\t=\tgrid_search.cv_results_ >>>\tfor\tmean_score,\tparams\tin\tzip(cvres[\"mean_test_score\"],\tcvres[\"params\"]): ...\t\t\t\t\tprint(np.sqrt(-mean_score),\tparams) ... 63825.0479302\t{'max_features':\t2,\t'n_estimators':\t3} 55643.8429091\t{'max_features':\t2,\t'n_estimators':\t10} 53380.6566859\t{'max_features':\t2,\t'n_estimators':\t30} 60959.1388585\t{'max_features':\t4,\t'n_estimators':\t3} 52740.5841667\t{'max_features':\t4,\t'n_estimators':\t10} 50374.1421461\t{'max_features':\t4,\t'n_estimators':\t30} 58661.2866462\t{'max_features':\t6,\t'n_estimators':\t3} 52009.9739798\t{'max_features':\t6,\t'n_estimators':\t10} 50154.1177737\t{'max_features':\t6,\t'n_estimators':\t30} 57865.3616801\t{'max_features':\t8,\t'n_estimators':\t3} 51730.0755087\t{'max_features':\t8,\t'n_estimators':\t10} 49694.8514333\t{'max_features':\t8,\t'n_estimators':\t30} 62874.4073931\t{'max_features':\t2,\t'n_estimators':\t3,\t'bootstrap':\tFalse} 54561.9398157\t{'max_features':\t2,\t'n_estimators':\t10,\t'bootstrap':\tFalse} 59416.6463145\t{'max_features':\t3,\t'n_estimators':\t3,\t'bootstrap':\tFalse} 52660.245911\t{'max_features':\t3,\t'n_estimators':\t10,\t'bootstrap':\tFalse} 57490.0168279\t{'max_features':\t4,\t'n_estimators':\t3,\t'bootstrap':\tFalse} 51093.9059428\t{'max_features':\t4,\t'n_estimators':\t10,\t'bootstrap':\tFalse}\n\nIn\tthis\texample,\twe\tobtain\tthe\tbest\tsolution\tby\tsetting\tthe\tmax_features\thyperparameter\tto\t8,\tand\tthe n_estimators\thyperparameter\tto\t30.\tThe\tRMSE\tscore\tfor\tthis\tcombination\tis\t49,694,\twhich\tis\tslightly better\tthan\tthe\tscore\tyou\tgot\tearlier\tusing\tthe\tdefault\thyperparameter\tvalues\t(which\twas\t52,564). Congratulations,\tyou\thave\tsuccessfully\tfine-tuned\tyour\tbest\tmodel!\n\nTIP\n\nDon’t\tforget\tthat\tyou\tcan\ttreat\tsome\tof\tthe\tdata\tpreparation\tsteps\tas\thyperparameters.\tFor\texample,\tthe\tgrid\tsearch\twill automatically\tfind\tout\twhether\tor\tnot\tto\tadd\ta\tfeature\tyou\twere\tnot\tsure\tabout\t(e.g.,\tusing\tthe\tadd_bedrooms_per_room hyperparameter\tof\tyour\tCombinedAttributesAdder\ttransformer).\tIt\tmay\tsimilarly\tbe\tused\tto\tautomatically\tfind\tthe\tbest\tway\tto handle\toutliers,\tmissing\tfeatures,\tfeature\tselection,\tand\tmore.\n\nRandomized\tSearch The\tgrid\tsearch\tapproach\tis\tfine\twhen\tyou\tare\texploring\trelatively\tfew\tcombinations,\tlike\tin\tthe\tprevious example,\tbut\twhen\tthe\thyperparameter\tsearch\tspace\tis\tlarge,\tit\tis\toften\tpreferable\tto\tuse RandomizedSearchCV\tinstead.\tThis\tclass\tcan\tbe\tused\tin\tmuch\tthe\tsame\tway\tas\tthe\tGridSearchCV\tclass, but\tinstead\tof\ttrying\tout\tall\tpossible\tcombinations,\tit\tevaluates\ta\tgiven\tnumber\tof\trandom\tcombinations by\tselecting\ta\trandom\tvalue\tfor\teach\thyperparameter\tat\tevery\titeration.\tThis\tapproach\thas\ttwo\tmain benefits:\n\nIf\tyou\tlet\tthe\trandomized\tsearch\trun\tfor,\tsay,\t1,000\titerations,\tthis\tapproach\twill\texplore\t1,000 different\tvalues\tfor\teach\thyperparameter\t(instead\tof\tjust\ta\tfew\tvalues\tper\thyperparameter\twith\tthe grid\tsearch\tapproach).\n\nYou\thave\tmore\tcontrol\tover\tthe\tcomputing\tbudget\tyou\twant\tto\tallocate\tto\thyperparameter\tsearch, simply\tby\tsetting\tthe\tnumber\tof\titerations.\n\nEnsemble\tMethods Another\tway\tto\tfine-tune\tyour\tsystem\tis\tto\ttry\tto\tcombine\tthe\tmodels\tthat\tperform\tbest.\tThe\tgroup\t(or “ensemble”)\twill\toften\tperform\tbetter\tthan\tthe\tbest\tindividual\tmodel\t(just\tlike\tRandom\tForests\tperform better\tthan\tthe\tindividual\tDecision\tTrees\tthey\trely\ton),\tespecially\tif\tthe\tindividual\tmodels\tmake\tvery different\ttypes\tof\terrors.\tWe\twill\tcover\tthis\ttopic\tin\tmore\tdetail\tin\tChapter\t7.\n\nAnalyze\tthe\tBest\tModels\tand\tTheir\tErrors You\twill\toften\tgain\tgood\tinsights\ton\tthe\tproblem\tby\tinspecting\tthe\tbest\tmodels.\tFor\texample,\tthe RandomForestRegressor\tcan\tindicate\tthe\trelative\timportance\tof\teach\tattribute\tfor\tmaking\taccurate predictions:\n\n>>>\tfeature_importances\t=\tgrid_search.best_estimator_.feature_importances_ >>>\tfeature_importances array([\t\t7.33442355e-02,\t\t\t6.29090705e-02,\t\t\t4.11437985e-02, \t\t\t\t\t\t\t\t\t1.46726854e-02,\t\t\t1.41064835e-02,\t\t\t1.48742809e-02, \t\t\t\t\t\t\t\t\t1.42575993e-02,\t\t\t3.66158981e-01,\t\t\t5.64191792e-02, \t\t\t\t\t\t\t\t\t1.08792957e-01,\t\t\t5.33510773e-02,\t\t\t1.03114883e-02, \t\t\t\t\t\t\t\t\t1.64780994e-01,\t\t\t6.02803867e-05,\t\t\t1.96041560e-03, \t\t\t\t\t\t\t\t\t2.85647464e-03])\n\nLet’s\tdisplay\tthese\timportance\tscores\tnext\tto\ttheir\tcorresponding\tattribute\tnames:\n\n>>>\textra_attribs\t=\t[\"rooms_per_hhold\",\t\"pop_per_hhold\",\t\"bedrooms_per_room\"] >>>\tcat_one_hot_attribs\t=\tlist(encoder.classes_) >>>\tattributes\t=\tnum_attribs\t+\textra_attribs\t+\tcat_one_hot_attribs >>>\tsorted(zip(feature_importances,\tattributes),\treverse=True) [(0.36615898061813418,\t'median_income'), \t(0.16478099356159051,\t'INLAND'), \t(0.10879295677551573,\t'pop_per_hhold'), \t(0.073344235516012421,\t'longitude'), \t(0.062909070482620302,\t'latitude'), \t(0.056419179181954007,\t'rooms_per_hhold'), \t(0.053351077347675809,\t'bedrooms_per_room'), \t(0.041143798478729635,\t'housing_median_age'), \t(0.014874280890402767,\t'population'), \t(0.014672685420543237,\t'total_rooms'), \t(0.014257599323407807,\t'households'), \t(0.014106483453584102,\t'total_bedrooms'), \t(0.010311488326303787,\t'<1H\tOCEAN'), \t(0.0028564746373201579,\t'NEAR\tOCEAN'), \t(0.0019604155994780701,\t'NEAR\tBAY'), \t(6.0280386727365991e-05,\t'ISLAND')]\n\nWith\tthis\tinformation,\tyou\tmay\twant\tto\ttry\tdropping\tsome\tof\tthe\tless\tuseful\tfeatures\t(e.g.,\tapparently\tonly one\tocean_proximity\tcategory\tis\treally\tuseful,\tso\tyou\tcould\ttry\tdropping\tthe\tothers).\n\nYou\tshould\talso\tlook\tat\tthe\tspecific\terrors\tthat\tyour\tsystem\tmakes,\tthen\ttry\tto\tunderstand\twhy\tit\tmakes them\tand\twhat\tcould\tfix\tthe\tproblem\t(adding\textra\tfeatures\tor,\ton\tthe\tcontrary,\tgetting\trid\tof\tuninformative ones,\tcleaning\tup\toutliers,\tetc.).\n\nEvaluate\tYour\tSystem\ton\tthe\tTest\tSet After\ttweaking\tyour\tmodels\tfor\ta\twhile,\tyou\teventually\thave\ta\tsystem\tthat\tperforms\tsufficiently\twell. Now\tis\tthe\ttime\tto\tevaluate\tthe\tfinal\tmodel\ton\tthe\ttest\tset.\tThere\tis\tnothing\tspecial\tabout\tthis\tprocess;\tjust get\tthe\tpredictors\tand\tthe\tlabels\tfrom\tyour\ttest\tset,\trun\tyour\tfull_pipeline\tto\ttransform\tthe\tdata\t(call transform(),\tnot\tfit_transform()!),\tand\tevaluate\tthe\tfinal\tmodel\ton\tthe\ttest\tset:\n\nfinal_model\t=\tgrid_search.best_estimator_\n\nX_test\t=\tstrat_test_set.drop(\"median_house_value\",\taxis=1) y_test\t=\tstrat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared\t=\tfull_pipeline.transform(X_test)\n\nfinal_predictions\t=\tfinal_model.predict(X_test_prepared)\n\nfinal_mse\t=\tmean_squared_error(y_test,\tfinal_predictions) final_rmse\t=\tnp.sqrt(final_mse)\t\t\t#\t=>\tevaluates\tto\t47,766.0\n\nThe\tperformance\twill\tusually\tbe\tslightly\tworse\tthan\twhat\tyou\tmeasured\tusing\tcross-validation\tif\tyou\tdid a\tlot\tof\thyperparameter\ttuning\t(because\tyour\tsystem\tends\tup\tfine-tuned\tto\tperform\twell\ton\tthe\tvalidation data,\tand\twill\tlikely\tnot\tperform\tas\twell\ton\tunknown\tdatasets).\tIt\tis\tnot\tthe\tcase\tin\tthis\texample,\tbut\twhen this\thappens\tyou\tmust\tresist\tthe\ttemptation\tto\ttweak\tthe\thyperparameters\tto\tmake\tthe\tnumbers\tlook\tgood on\tthe\ttest\tset;\tthe\timprovements\twould\tbe\tunlikely\tto\tgeneralize\tto\tnew\tdata.\n\nNow\tcomes\tthe\tproject\tprelaunch\tphase:\tyou\tneed\tto\tpresent\tyour\tsolution\t(highlighting\twhat\tyou\thave learned,\twhat\tworked\tand\twhat\tdid\tnot,\twhat\tassumptions\twere\tmade,\tand\twhat\tyour\tsystem’s\tlimitations are),\tdocument\teverything,\tand\tcreate\tnice\tpresentations\twith\tclear\tvisualizations\tand\teasy-to-remember statements\t(e.g.,\t“the\tmedian\tincome\tis\tthe\tnumber\tone\tpredictor\tof\thousing\tprices”).\n\nLaunch,\tMonitor,\tand\tMaintain\tYour\tSystem Perfect,\tyou\tgot\tapproval\tto\tlaunch!\tYou\tneed\tto\tget\tyour\tsolution\tready\tfor\tproduction,\tin\tparticular\tby plugging\tthe\tproduction\tinput\tdata\tsources\tinto\tyour\tsystem\tand\twriting\ttests.\n\nYou\talso\tneed\tto\twrite\tmonitoring\tcode\tto\tcheck\tyour\tsystem’s\tlive\tperformance\tat\tregular\tintervals\tand trigger\talerts\twhen\tit\tdrops.\tThis\tis\timportant\tto\tcatch\tnot\tonly\tsudden\tbreakage,\tbut\talso\tperformance degradation.\tThis\tis\tquite\tcommon\tbecause\tmodels\ttend\tto\t“rot”\tas\tdata\tevolves\tover\ttime,\tunless\tthe models\tare\tregularly\ttrained\ton\tfresh\tdata.\n\nEvaluating\tyour\tsystem’s\tperformance\twill\trequire\tsampling\tthe\tsystem’s\tpredictions\tand\tevaluating\tthem. This\twill\tgenerally\trequire\ta\thuman\tanalysis.\tThese\tanalysts\tmay\tbe\tfield\texperts,\tor\tworkers\ton\ta crowdsourcing\tplatform\t(such\tas\tAmazon\tMechanical\tTurk\tor\tCrowdFlower).\tEither\tway,\tyou\tneed\tto plug\tthe\thuman\tevaluation\tpipeline\tinto\tyour\tsystem.\n\nYou\tshould\talso\tmake\tsure\tyou\tevaluate\tthe\tsystem’s\tinput\tdata\tquality.\tSometimes\tperformance\twill degrade\tslightly\tbecause\tof\ta\tpoor\tquality\tsignal\t(e.g.,\ta\tmalfunctioning\tsensor\tsending\trandom\tvalues,\tor another\tteam’s\toutput\tbecoming\tstale),\tbut\tit\tmay\ttake\ta\twhile\tbefore\tyour\tsystem’s\tperformance\tdegrades enough\tto\ttrigger\tan\talert.\tIf\tyou\tmonitor\tyour\tsystem’s\tinputs,\tyou\tmay\tcatch\tthis\tearlier.\tMonitoring\tthe inputs\tis\tparticularly\timportant\tfor\tonline\tlearning\tsystems.\n\nFinally,\tyou\twill\tgenerally\twant\tto\ttrain\tyour\tmodels\ton\ta\tregular\tbasis\tusing\tfresh\tdata.\tYou\tshould automate\tthis\tprocess\tas\tmuch\tas\tpossible.\tIf\tyou\tdon’t,\tyou\tare\tvery\tlikely\tto\trefresh\tyour\tmodel\tonly every\tsix\tmonths\t(at\tbest),\tand\tyour\tsystem’s\tperformance\tmay\tfluctuate\tseverely\tover\ttime.\tIf\tyour system\tis\tan\tonline\tlearning\tsystem,\tyou\tshould\tmake\tsure\tyou\tsave\tsnapshots\tof\tits\tstate\tat\tregular intervals\tso\tyou\tcan\teasily\troll\tback\tto\ta\tpreviously\tworking\tstate.\n\nTry\tIt\tOut! Hopefully\tthis\tchapter\tgave\tyou\ta\tgood\tidea\tof\twhat\ta\tMachine\tLearning\tproject\tlooks\tlike,\tand\tshowed you\tsome\tof\tthe\ttools\tyou\tcan\tuse\tto\ttrain\ta\tgreat\tsystem.\tAs\tyou\tcan\tsee,\tmuch\tof\tthe\twork\tis\tin\tthe\tdata preparation\tstep,\tbuilding\tmonitoring\ttools,\tsetting\tup\thuman\tevaluation\tpipelines,\tand\tautomating\tregular model\ttraining.\tThe\tMachine\tLearning\talgorithms\tare\talso\timportant,\tof\tcourse,\tbut\tit\tis\tprobably preferable\tto\tbe\tcomfortable\twith\tthe\toverall\tprocess\tand\tknow\tthree\tor\tfour\talgorithms\twell\trather\tthan to\tspend\tall\tyour\ttime\texploring\tadvanced\talgorithms\tand\tnot\tenough\ttime\ton\tthe\toverall\tprocess.\n\nSo,\tif\tyou\thave\tnot\talready\tdone\tso,\tnow\tis\ta\tgood\ttime\tto\tpick\tup\ta\tlaptop,\tselect\ta\tdataset\tthat\tyou\tare interested\tin,\tand\ttry\tto\tgo\tthrough\tthe\twhole\tprocess\tfrom\tA\tto\tZ.\tA\tgood\tplace\tto\tstart\tis\ton\ta competition\twebsite\tsuch\tas\thttp://kaggle.com/:\tyou\twill\thave\ta\tdataset\tto\tplay\twith,\ta\tclear\tgoal,\tand people\tto\tshare\tthe\texperience\twith.\n\nExercises Using\tthis\tchapter’s\thousing\tdataset:\n\n1.\t Try\ta\tSupport\tVector\tMachine\tregressor\t(sklearn.svm.SVR),\twith\tvarious\thyperparameters\tsuch\tas kernel=\"linear\"\t(with\tvarious\tvalues\tfor\tthe\tC\thyperparameter)\tor\tkernel=\"rbf\"\t(with\tvarious values\tfor\tthe\tC\tand\tgamma\thyperparameters).\tDon’t\tworry\tabout\twhat\tthese\thyperparameters\tmean for\tnow.\tHow\tdoes\tthe\tbest\tSVR\tpredictor\tperform?\n\n2.\t Try\treplacing\tGridSearchCV\twith\tRandomizedSearchCV.\n\n3.\t Try\tadding\ta\ttransformer\tin\tthe\tpreparation\tpipeline\tto\tselect\tonly\tthe\tmost\timportant\tattributes.\n\n4.\t Try\tcreating\ta\tsingle\tpipeline\tthat\tdoes\tthe\tfull\tdata\tpreparation\tplus\tthe\tfinal\tprediction.\n\n5.\t Automatically\texplore\tsome\tpreparation\toptions\tusing\tGridSearchCV.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tthe\tonline\tJupyter\tnotebooks\tat https://github.com/ageron/handson-ml.\n\n1\n\nThe\texample\tproject\tis\tcompletely\tfictitious;\tthe\tgoal\tis\tjust\tto\tillustrate\tthe\tmain\tsteps\tof\ta\tMachine\tLearning\tproject,\tnot\tto\tlearn\tanything about\tthe\treal\testate\tbusiness.\n\n2\n\nThe\toriginal\tdataset\tappeared\tin\tR.\tKelley\tPace\tand\tRonald\tBarry,\t“Sparse\tSpatial\tAutoregressions,”\tStatistics\t&\tProbability\tLetters\t33, no.\t3\t(1997):\t291–297.\n\n3\n\nA\tpiece\tof\tinformation\tfed\tto\ta\tMachine\tLearning\tsystem\tis\toften\tcalled\ta\tsignal\tin\treference\tto\tShannon’s\tinformation\ttheory:\tyou\twant\ta high\tsignal/noise\tratio.\n\n4\n\nRecall\tthat\tthe\ttranspose\toperator\tflips\ta\tcolumn\tvector\tinto\ta\trow\tvector\t(and\tvice\tversa).\n\n5\n\nThe\tlatest\tversion\tof\tPython\t3\tis\trecommended.\tPython\t2.7+\tshould\twork\tfine\ttoo,\tbut\tit\tis\tdeprecated.\tIf\tyou\tuse\tPython\t2,\tyou\tmust\tadd from\t__future__\timport\tdivision,\tprint_function,\tunicode_literals\tat\tthe\tbeginning\tof\tyour\tcode.\n\n6\n\nWe\twill\tshow\tthe\tinstallation\tsteps\tusing\tpip\tin\ta\tbash\tshell\ton\ta\tLinux\tor\tmacOS\tsystem.\tYou\tmay\tneed\tto\tadapt\tthese\tcommands\tto\tyour own\tsystem.\tOn\tWindows,\twe\trecommend\tinstalling\tAnaconda\tinstead.\n\n7\n\nYou\tmay\tneed\tto\thave\tadministrator\trights\tto\trun\tthis\tcommand;\tif\tso,\ttry\tprefixing\tit\twith\tsudo.\n\n8\n\nNote\tthat\tJupyter\tcan\thandle\tmultiple\tversions\tof\tPython,\tand\teven\tmany\tother\tlanguages\tsuch\tas\tR\tor\tOctave.\n\n9\n\nYou\tmight\talso\tneed\tto\tcheck\tlegal\tconstraints,\tsuch\tas\tprivate\tfields\tthat\tshould\tnever\tbe\tcopied\tto\tunsafe\tdatastores.\n\n10\n\nIn\ta\treal\tproject\tyou\twould\tsave\tthis\tcode\tin\ta\tPython\tfile,\tbut\tfor\tnow\tyou\tcan\tjust\twrite\tit\tin\tyour\tJupyter\tnotebook.\n\n11\n\nThe\tstandard\tdeviation\tis\tgenerally\tdenoted\tσ\t(the\tGreek\tletter\tsigma),\tand\tit\tis\tthe\tsquare\troot\tof\tthe\tvariance,\twhich\tis\tthe\taverage\tof the\tsquared\tdeviation\tfrom\tthe\tmean.\tWhen\ta\tfeature\thas\ta\tbell-shaped\tnormal\tdistribution\t(also\tcalled\ta\tGaussian\tdistribution),\twhich is\tvery\tcommon,\tthe\t“68-95-99.7”\trule\tapplies:\tabout\t68%\tof\tthe\tvalues\tfall\twithin\t1σ\tof\tthe\tmean,\t95%\twithin\t2σ,\tand\t99.7%\twithin\t3σ.\n\n12\n\nYou\twill\toften\tsee\tpeople\tset\tthe\trandom\tseed\tto\t42.\tThis\tnumber\thas\tno\tspecial\tproperty,\tother\tthan\tto\tbe\tThe\tAnswer\tto\tthe\tUltimate Question\tof\tLife,\tthe\tUniverse,\tand\tEverything.\n\n13\n\nThe\tlocation\tinformation\tis\tactually\tquite\tcoarse,\tand\tas\ta\tresult\tmany\tdistricts\twill\thave\tthe\texact\tsame\tID,\tso\tthey\twill\tend\tup\tin\tthe\tsame set\t(test\tor\ttrain).\tThis\tintroduces\tsome\tunfortunate\tsampling\tbias.\n\n14\n\nIf\tyou\tare\treading\tthis\tin\tgrayscale,\tgrab\ta\tred\tpen\tand\tscribble\tover\tmost\tof\tthe\tcoastline\tfrom\tthe\tBay\tArea\tdown\tto\tSan\tDiego\t(as\tyou might\texpect).\tYou\tcan\tadd\ta\tpatch\tof\tyellow\taround\tSacramento\tas\twell.\n\n15\n\nFor\tmore\tdetails\ton\tthe\tdesign\tprinciples,\tsee\t“API\tdesign\tfor\tmachine\tlearning\tsoftware:\texperiences\tfrom\tthe\tscikit-learn\tproject,”\tL.\n\n16\n\n17\n\n18\n\n19\n\nBuitinck,\tG.\tLouppe,\tM.\tBlondel,\tF.\tPedregosa,\tA.\tMüller,\tet\tal.\t(2013).\n\nSome\tpredictors\talso\tprovide\tmethods\tto\tmeasure\tthe\tconfidence\tof\ttheir\tpredictions.\n\nNumPy’s\treshape()\tfunction\tallows\tone\tdimension\tto\tbe\t–1,\twhich\tmeans\t“unspecified”:\tthe\tvalue\tis\tinferred\tfrom\tthe\tlength\tof\tthe\tarray and\tthe\tremaining\tdimensions.\n\nSee\tSciPy’s\tdocumentation\tfor\tmore\tdetails.\n\nBut\tcheck\tout\tPull\tRequest\t#3886,\twhich\tmay\tintroduce\ta\tColumnTransformer\tclass\tmaking\tattribute-specific\ttransformations\teasy.\tYou could\talso\trun\tpip3\tinstall\tsklearn-pandas\tto\tget\ta\tDataFrameMapper\tclass\twith\ta\tsimilar\tobjective.\n\nChapter\t3.\tClassification\n\nIn\tChapter\t1\twe\tmentioned\tthat\tthe\tmost\tcommon\tsupervised\tlearning\ttasks\tare\tregression\t(predicting values)\tand\tclassification\t(predicting\tclasses).\tIn\tChapter\t2\twe\texplored\ta\tregression\ttask,\tpredicting housing\tvalues,\tusing\tvarious\talgorithms\tsuch\tas\tLinear\tRegression,\tDecision\tTrees,\tand\tRandom\tForests (which\twill\tbe\texplained\tin\tfurther\tdetail\tin\tlater\tchapters).\tNow\twe\twill\tturn\tour\tattention\tto classification\tsystems.\n\nMNIST In\tthis\tchapter,\twe\twill\tbe\tusing\tthe\tMNIST\tdataset,\twhich\tis\ta\tset\tof\t70,000\tsmall\timages\tof\tdigits handwritten\tby\thigh\tschool\tstudents\tand\temployees\tof\tthe\tUS\tCensus\tBureau.\tEach\timage\tis\tlabeled\twith the\tdigit\tit\trepresents.\tThis\tset\thas\tbeen\tstudied\tso\tmuch\tthat\tit\tis\toften\tcalled\tthe\t“Hello\tWorld”\tof Machine\tLearning:\twhenever\tpeople\tcome\tup\twith\ta\tnew\tclassification\talgorithm,\tthey\tare\tcurious\tto\tsee how\tit\twill\tperform\ton\tMNIST.\tWhenever\tsomeone\tlearns\tMachine\tLearning,\tsooner\tor\tlater\tthey\ttackle MNIST.\n\nScikit-Learn\tprovides\tmany\thelper\tfunctions\tto\tdownload\tpopular\tdatasets.\tMNIST\tis\tone\tof\tthem.\tThe following\tcode\tfetches\tthe\tMNIST\tdataset:1\n\n>>>\tfrom\tsklearn.datasets\timport\tfetch_mldata >>>\tmnist\t=\tfetch_mldata('MNIST\toriginal') >>>\tmnist {'COL_NAMES':\t['label',\t'data'], \t'DESCR':\t'mldata.org\tdataset:\tmnist-original', \t'data':\tarray([[0,\t0,\t0,\t...,\t0,\t0,\t0], \t\t\t\t\t\t\t\t[0,\t0,\t0,\t...,\t0,\t0,\t0], \t\t\t\t\t\t\t\t[0,\t0,\t0,\t...,\t0,\t0,\t0], \t\t\t\t\t\t\t\t..., \t\t\t\t\t\t\t\t[0,\t0,\t0,\t...,\t0,\t0,\t0], \t\t\t\t\t\t\t\t[0,\t0,\t0,\t...,\t0,\t0,\t0], \t\t\t\t\t\t\t\t[0,\t0,\t0,\t...,\t0,\t0,\t0]],\tdtype=uint8), \t'target':\tarray([\t0.,\t\t0.,\t\t0.,\t...,\t\t9.,\t\t9.,\t\t9.])}\n\nDatasets\tloaded\tby\tScikit-Learn\tgenerally\thave\ta\tsimilar\tdictionary\tstructure\tincluding:\n\nA\tDESCR\tkey\tdescribing\tthe\tdataset\n\nA\tdata\tkey\tcontaining\tan\tarray\twith\tone\trow\tper\tinstance\tand\tone\tcolumn\tper\tfeature\n\nA\ttarget\tkey\tcontaining\tan\tarray\twith\tthe\tlabels\n\nLet’s\tlook\tat\tthese\tarrays:\n\n>>>\tX,\ty\t=\tmnist[\"data\"],\tmnist[\"target\"] >>>\tX.shape (70000,\t784) >>>\ty.shape (70000,)\n\nThere\tare\t70,000\timages,\tand\teach\timage\thas\t784\tfeatures.\tThis\tis\tbecause\teach\timage\tis\t28×28\tpixels, and\teach\tfeature\tsimply\trepresents\tone\tpixel’s\tintensity,\tfrom\t0\t(white)\tto\t255\t(black).\tLet’s\ttake\ta\tpeek at\tone\tdigit\tfrom\tthe\tdataset.\tAll\tyou\tneed\tto\tdo\tis\tgrab\tan\tinstance’s\tfeature\tvector,\treshape\tit\tto\ta\t28×28 array,\tand\tdisplay\tit\tusing\tMatplotlib’s\timshow()\tfunction:\n\n%matplotlib\tinline import\tmatplotlib import\tmatplotlib.pyplot\tas\tplt\n\nsome_digit\t=\tX[36000] some_digit_image\t=\tsome_digit.reshape(28,\t28)\n\nplt.imshow(some_digit_image,\tcmap\t=\tmatplotlib.cm.binary, \t\t\t\t\t\t\t\t\t\t\tinterpolation=\"nearest\")",
      "page_number": 56
    },
    {
      "number": 3,
      "title": "Classification",
      "start_page": 112,
      "end_page": 142,
      "detection_method": "regex_chapter_title",
      "content": "plt.axis(\"off\") plt.show()\n\nThis\tlooks\tlike\ta\t5,\tand\tindeed\tthat’s\twhat\tthe\tlabel\ttells\tus:\n\n>>>\ty[36000] 5.0\n\nFigure\t3-1\tshows\ta\tfew\tmore\timages\tfrom\tthe\tMNIST\tdataset\tto\tgive\tyou\ta\tfeel\tfor\tthe\tcomplexity\tof\tthe classification\ttask.\n\nFigure\t3-1.\tA\tfew\tdigits\tfrom\tthe\tMNIST\tdataset\n\nBut\twait!\tYou\tshould\talways\tcreate\ta\ttest\tset\tand\tset\tit\taside\tbefore\tinspecting\tthe\tdata\tclosely.\tThe MNIST\tdataset\tis\tactually\talready\tsplit\tinto\ta\ttraining\tset\t(the\tfirst\t60,000\timages)\tand\ta\ttest\tset\t(the\tlast 10,000\timages):\n\nX_train,\tX_test,\ty_train,\ty_test\t=\tX[:60000],\tX[60000:],\ty[:60000],\ty[60000:]\n\nLet’s\talso\tshuffle\tthe\ttraining\tset;\tthis\twill\tguarantee\tthat\tall\tcross-validation\tfolds\twill\tbe\tsimilar\t(you don’t\twant\tone\tfold\tto\tbe\tmissing\tsome\tdigits).\tMoreover,\tsome\tlearning\talgorithms\tare\tsensitive\tto\tthe order\tof\tthe\ttraining\tinstances,\tand\tthey\tperform\tpoorly\tif\tthey\tget\tmany\tsimilar\tinstances\tin\ta\trow. Shuffling\tthe\tdataset\tensures\tthat\tthis\twon’t\thappen:2\n\nimport\tnumpy\tas\tnp\n\nshuffle_index\t=\tnp.random.permutation(60000) X_train,\ty_train\t=\tX_train[shuffle_index],\ty_train[shuffle_index]\n\nTraining\ta\tBinary\tClassifier Let’s\tsimplify\tthe\tproblem\tfor\tnow\tand\tonly\ttry\tto\tidentify\tone\tdigit\t—\tfor\texample,\tthe\tnumber\t5.\tThis “5-detector”\twill\tbe\tan\texample\tof\ta\tbinary\tclassifier,\tcapable\tof\tdistinguishing\tbetween\tjust\ttwo classes,\t5\tand\tnot-5.\tLet’s\tcreate\tthe\ttarget\tvectors\tfor\tthis\tclassification\ttask:\n\ny_train_5\t=\t(y_train\t==\t5)\t\t#\tTrue\tfor\tall\t5s,\tFalse\tfor\tall\tother\tdigits. y_test_5\t=\t(y_test\t==\t5)\n\nOkay,\tnow\tlet’s\tpick\ta\tclassifier\tand\ttrain\tit.\tA\tgood\tplace\tto\tstart\tis\twith\ta\tStochastic\tGradient\tDescent (SGD)\tclassifier,\tusing\tScikit-Learn’s\tSGDClassifier\tclass.\tThis\tclassifier\thas\tthe\tadvantage\tof\tbeing capable\tof\thandling\tvery\tlarge\tdatasets\tefficiently.\tThis\tis\tin\tpart\tbecause\tSGD\tdeals\twith\ttraining instances\tindependently,\tone\tat\ta\ttime\t(which\talso\tmakes\tSGD\twell\tsuited\tfor\tonline\tlearning),\tas\twe will\tsee\tlater.\tLet’s\tcreate\tan\tSGDClassifier\tand\ttrain\tit\ton\tthe\twhole\ttraining\tset:\n\nfrom\tsklearn.linear_model\timport\tSGDClassifier\n\nsgd_clf\t=\tSGDClassifier(random_state=42) sgd_clf.fit(X_train,\ty_train_5)\n\nTIP\n\nThe\tSGDClassifier\trelies\ton\trandomness\tduring\ttraining\t(hence\tthe\tname\t“stochastic”).\tIf\tyou\twant\treproducible\tresults,\tyou should\tset\tthe\trandom_state\tparameter.\n\nNow\tyou\tcan\tuse\tit\tto\tdetect\timages\tof\tthe\tnumber\t5:\n\n>>>\tsgd_clf.predict([some_digit]) array([\tTrue],\tdtype=bool)\n\nThe\tclassifier\tguesses\tthat\tthis\timage\trepresents\ta\t5\t(True).\tLooks\tlike\tit\tguessed\tright\tin\tthis\tparticular case!\tNow,\tlet’s\tevaluate\tthis\tmodel’s\tperformance.\n\nPerformance\tMeasures Evaluating\ta\tclassifier\tis\toften\tsignificantly\ttrickier\tthan\tevaluating\ta\tregressor,\tso\twe\twill\tspend\ta\tlarge part\tof\tthis\tchapter\ton\tthis\ttopic.\tThere\tare\tmany\tperformance\tmeasures\tavailable,\tso\tgrab\tanother\tcoffee and\tget\tready\tto\tlearn\tmany\tnew\tconcepts\tand\tacronyms!\n\nMeasuring\tAccuracy\tUsing\tCross-Validation A\tgood\tway\tto\tevaluate\ta\tmodel\tis\tto\tuse\tcross-validation,\tjust\tas\tyou\tdid\tin\tChapter\t2.\n\nIMPLEMENTING\tCROSS-VALIDATION\n\nOccasionally\tyou\twill\tneed\tmore\tcontrol\tover\tthe\tcross-validation\tprocess\tthan\twhat\tScikit-Learn\tprovides\toff-the-shelf.\tIn\tthese\tcases, you\tcan\timplement\tcross-validation\tyourself;\tit\tis\tactually\tfairly\tstraightforward.\tThe\tfollowing\tcode\tdoes\troughly\tthe\tsame\tthing\tas Scikit-Learn’s\tcross_val_score()\tfunction,\tand\tprints\tthe\tsame\tresult:\n\nfrom\tsklearn.model_selection\timport\tStratifiedKFold from\tsklearn.base\timport\tclone\n\nskfolds\t=\tStratifiedKFold(n_splits=3,\trandom_state=42)\n\nfor\ttrain_index,\ttest_index\tin\tskfolds.split(X_train,\ty_train_5): \t\t\t\tclone_clf\t=\tclone(sgd_clf) \t\t\t\tX_train_folds\t=\tX_train[train_index] \t\t\t\ty_train_folds\t=\t(y_train_5[train_index]) \t\t\t\tX_test_fold\t=\tX_train[test_index] \t\t\t\ty_test_fold\t=\t(y_train_5[test_index])\n\nclone_clf.fit(X_train_folds,\ty_train_folds) \t\t\t\ty_pred\t=\tclone_clf.predict(X_test_fold) \t\t\t\tn_correct\t=\tsum(y_pred\t==\ty_test_fold) \t\t\t\tprint(n_correct\t/\tlen(y_pred))\t\t#\tprints\t0.9502,\t0.96565\tand\t0.96495\n\nThe\tStratifiedKFold\tclass\tperforms\tstratified\tsampling\t(as\texplained\tin\tChapter\t2)\tto\tproduce\tfolds\tthat\tcontain\ta\trepresentative\tratio of\teach\tclass.\tAt\teach\titeration\tthe\tcode\tcreates\ta\tclone\tof\tthe\tclassifier,\ttrains\tthat\tclone\ton\tthe\ttraining\tfolds,\tand\tmakes\tpredictions\ton the\ttest\tfold.\tThen\tit\tcounts\tthe\tnumber\tof\tcorrect\tpredictions\tand\toutputs\tthe\tratio\tof\tcorrect\tpredictions.\n\nLet’s\tuse\tthe\tcross_val_score()\tfunction\tto\tevaluate\tyour\tSGDClassifier\tmodel\tusing\tK-fold\tcross- validation,\twith\tthree\tfolds.\tRemember\tthat\tK-fold\tcross-validation\tmeans\tsplitting\tthe\ttraining\tset\tinto K-folds\t(in\tthis\tcase,\tthree),\tthen\tmaking\tpredictions\tand\tevaluating\tthem\ton\teach\tfold\tusing\ta\tmodel trained\ton\tthe\tremaining\tfolds\t(see\tChapter\t2):\n\n>>>\tfrom\tsklearn.model_selection\timport\tcross_val_score >>>\tcross_val_score(sgd_clf,\tX_train,\ty_train_5,\tcv=3,\tscoring=\"accuracy\") array([\t0.9502\t,\t\t0.96565,\t\t0.96495])\n\nWow!\tAbove\t95%\taccuracy\t(ratio\tof\tcorrect\tpredictions)\ton\tall\tcross-validation\tfolds?\tThis\tlooks amazing,\tdoesn’t\tit?\tWell,\tbefore\tyou\tget\ttoo\texcited,\tlet’s\tlook\tat\ta\tvery\tdumb\tclassifier\tthat\tjust classifies\tevery\tsingle\timage\tin\tthe\t“not-5”\tclass:\n\nfrom\tsklearn.base\timport\tBaseEstimator\n\nclass\tNever5Classifier(BaseEstimator): \t\t\t\tdef\tfit(self,\tX,\ty=None): \t\t\t\t\t\t\t\tpass \t\t\t\tdef\tpredict(self,\tX): \t\t\t\t\t\t\t\treturn\tnp.zeros((len(X),\t1),\tdtype=bool)\n\nCan\tyou\tguess\tthis\tmodel’s\taccuracy?\tLet’s\tfind\tout:\n\n>>>\tnever_5_clf\t=\tNever5Classifier() >>>\tcross_val_score(never_5_clf,\tX_train,\ty_train_5,\tcv=3,\tscoring=\"accuracy\") array([\t0.909\t\t,\t\t0.90715,\t\t0.9128\t])\n\nThat’s\tright,\tit\thas\tover\t90%\taccuracy!\tThis\tis\tsimply\tbecause\tonly\tabout\t10%\tof\tthe\timages\tare\t5s,\tso\tif you\talways\tguess\tthat\tan\timage\tis\tnot\ta\t5,\tyou\twill\tbe\tright\tabout\t90%\tof\tthe\ttime.\tBeats\tNostradamus.\n\nThis\tdemonstrates\twhy\taccuracy\tis\tgenerally\tnot\tthe\tpreferred\tperformance\tmeasure\tfor\tclassifiers, especially\twhen\tyou\tare\tdealing\twith\tskewed\tdatasets\t(i.e.,\twhen\tsome\tclasses\tare\tmuch\tmore\tfrequent than\tothers).\n\nConfusion\tMatrix A\tmuch\tbetter\tway\tto\tevaluate\tthe\tperformance\tof\ta\tclassifier\tis\tto\tlook\tat\tthe\tconfusion\tmatrix.\tThe general\tidea\tis\tto\tcount\tthe\tnumber\tof\ttimes\tinstances\tof\tclass\tA\tare\tclassified\tas\tclass\tB.\tFor\texample,\tto know\tthe\tnumber\tof\ttimes\tthe\tclassifier\tconfused\timages\tof\t5s\twith\t3s,\tyou\twould\tlook\tin\tthe\t5th\trow\tand 3rd\tcolumn\tof\tthe\tconfusion\tmatrix.\n\nTo\tcompute\tthe\tconfusion\tmatrix,\tyou\tfirst\tneed\tto\thave\ta\tset\tof\tpredictions,\tso\tthey\tcan\tbe\tcompared\tto the\tactual\ttargets.\tYou\tcould\tmake\tpredictions\ton\tthe\ttest\tset,\tbut\tlet’s\tkeep\tit\tuntouched\tfor\tnow (remember\tthat\tyou\twant\tto\tuse\tthe\ttest\tset\tonly\tat\tthe\tvery\tend\tof\tyour\tproject,\tonce\tyou\thave\ta\tclassifier that\tyou\tare\tready\tto\tlaunch).\tInstead,\tyou\tcan\tuse\tthe\tcross_val_predict()\tfunction:\n\nfrom\tsklearn.model_selection\timport\tcross_val_predict\n\ny_train_pred\t=\tcross_val_predict(sgd_clf,\tX_train,\ty_train_5,\tcv=3)\n\nJust\tlike\tthe\tcross_val_score()\tfunction,\tcross_val_predict()\tperforms\tK-fold\tcross-validation, but\tinstead\tof\treturning\tthe\tevaluation\tscores,\tit\treturns\tthe\tpredictions\tmade\ton\teach\ttest\tfold.\tThis\tmeans that\tyou\tget\ta\tclean\tprediction\tfor\teach\tinstance\tin\tthe\ttraining\tset\t(“clean”\tmeaning\tthat\tthe\tprediction\tis made\tby\ta\tmodel\tthat\tnever\tsaw\tthe\tdata\tduring\ttraining).\n\nNow\tyou\tare\tready\tto\tget\tthe\tconfusion\tmatrix\tusing\tthe\tconfusion_matrix()\tfunction.\tJust\tpass\tit\tthe target\tclasses\t(y_train_5)\tand\tthe\tpredicted\tclasses\t(y_train_pred):\n\n>>>\tfrom\tsklearn.metrics\timport\tconfusion_matrix >>>\tconfusion_matrix(y_train_5,\ty_train_pred) array([[53272,\t\t1307], \t\t\t\t\t\t\t[\t1077,\t\t4344]])\n\nEach\trow\tin\ta\tconfusion\tmatrix\trepresents\tan\tactual\tclass,\twhile\teach\tcolumn\trepresents\ta\tpredicted class.\tThe\tfirst\trow\tof\tthis\tmatrix\tconsiders\tnon-5\timages\t(the\tnegative\tclass):\t53,272\tof\tthem\twere correctly\tclassified\tas\tnon-5s\t(they\tare\tcalled\ttrue\tnegatives),\twhile\tthe\tremaining\t1,307\twere\twrongly classified\tas\t5s\t(false\tpositives).\tThe\tsecond\trow\tconsiders\tthe\timages\tof\t5s\t(the\tpositive\tclass):\t1,077 were\twrongly\tclassified\tas\tnon-5s\t(false\tnegatives),\twhile\tthe\tremaining\t4,344\twere\tcorrectly\tclassified as\t5s\t(true\tpositives).\tA\tperfect\tclassifier\twould\thave\tonly\ttrue\tpositives\tand\ttrue\tnegatives,\tso\tits confusion\tmatrix\twould\thave\tnonzero\tvalues\tonly\ton\tits\tmain\tdiagonal\t(top\tleft\tto\tbottom\tright):\n\n>>>\tconfusion_matrix(y_train_5,\ty_train_perfect_predictions) array([[54579,\t\t\t\t0], \t\t\t\t\t\t\t[\t\t\t\t0,\t5421]])\n\nThe\tconfusion\tmatrix\tgives\tyou\ta\tlot\tof\tinformation,\tbut\tsometimes\tyou\tmay\tprefer\ta\tmore\tconcise\tmetric. An\tinteresting\tone\tto\tlook\tat\tis\tthe\taccuracy\tof\tthe\tpositive\tpredictions;\tthis\tis\tcalled\tthe\tprecision\tof\tthe classifier\t(Equation\t3-1).\n\nEquation\t3-1.\tPrecision\n\nTP\tis\tthe\tnumber\tof\ttrue\tpositives,\tand\tFP\tis\tthe\tnumber\tof\tfalse\tpositives.\n\nA\ttrivial\tway\tto\thave\tperfect\tprecision\tis\tto\tmake\tone\tsingle\tpositive\tprediction\tand\tensure\tit\tis\tcorrect (precision\t=\t1/1\t=\t100%).\tThis\twould\tnot\tbe\tvery\tuseful\tsince\tthe\tclassifier\twould\tignore\tall\tbut\tone positive\tinstance.\tSo\tprecision\tis\ttypically\tused\talong\twith\tanother\tmetric\tnamed\trecall,\talso\tcalled sensitivity\tor\ttrue\tpositive\trate\t(TPR):\tthis\tis\tthe\tratio\tof\tpositive\tinstances\tthat\tare\tcorrectly\tdetected\tby the\tclassifier\t(Equation\t3-2).\n\nEquation\t3-2.\tRecall\n\nFN\tis\tof\tcourse\tthe\tnumber\tof\tfalse\tnegatives.\n\nIf\tyou\tare\tconfused\tabout\tthe\tconfusion\tmatrix,\tFigure\t3-2\tmay\thelp.\n\nFigure\t3-2.\tAn\tillustrated\tconfusion\tmatrix\n\nPrecision\tand\tRecall Scikit-Learn\tprovides\tseveral\tfunctions\tto\tcompute\tclassifier\tmetrics,\tincluding\tprecision\tand\trecall:\n\n>>>\tfrom\tsklearn.metrics\timport\tprecision_score,\trecall_score >>>\tprecision_score(y_train_5,\ty_train_pred)\t#\t==\t4344\t/\t(4344\t+\t1307) 0.76871350203503808 >>>\trecall_score(y_train_5,\ty_train_pred)\t#\t==\t4344\t/\t(4344\t+\t1077) 0.80132816823464303\n\nNow\tyour\t5-detector\tdoes\tnot\tlook\tas\tshiny\tas\tit\tdid\twhen\tyou\tlooked\tat\tits\taccuracy.\tWhen\tit\tclaims\tan image\trepresents\ta\t5,\tit\tis\tcorrect\tonly\t77%\tof\tthe\ttime.\tMoreover,\tit\tonly\tdetects\t80%\tof\tthe\t5s.\n\nIt\tis\toften\tconvenient\tto\tcombine\tprecision\tand\trecall\tinto\ta\tsingle\tmetric\tcalled\tthe\tF1\tscore,\tin particular\tif\tyou\tneed\ta\tsimple\tway\tto\tcompare\ttwo\tclassifiers.\tThe\tF1\tscore\tis\tthe\tharmonic\tmean\tof precision\tand\trecall\t(Equation\t3-3).\tWhereas\tthe\tregular\tmean\ttreats\tall\tvalues\tequally,\tthe\tharmonic mean\tgives\tmuch\tmore\tweight\tto\tlow\tvalues.\tAs\ta\tresult,\tthe\tclassifier\twill\tonly\tget\ta\thigh\tF1\tscore\tif both\trecall\tand\tprecision\tare\thigh.\n\nEquation\t3-3.\tF1\tscore\n\nTo\tcompute\tthe\tF1\tscore,\tsimply\tcall\tthe\tf1_score()\tfunction:\n\n>>>\tfrom\tsklearn.metrics\timport\tf1_score >>>\tf1_score(y_train_5,\ty_train_pred) 0.78468208092485547\n\nThe\tF1\tscore\tfavors\tclassifiers\tthat\thave\tsimilar\tprecision\tand\trecall.\tThis\tis\tnot\talways\twhat\tyou\twant: in\tsome\tcontexts\tyou\tmostly\tcare\tabout\tprecision,\tand\tin\tother\tcontexts\tyou\treally\tcare\tabout\trecall.\tFor example,\tif\tyou\ttrained\ta\tclassifier\tto\tdetect\tvideos\tthat\tare\tsafe\tfor\tkids,\tyou\twould\tprobably\tprefer\ta classifier\tthat\trejects\tmany\tgood\tvideos\t(low\trecall)\tbut\tkeeps\tonly\tsafe\tones\t(high\tprecision),\trather\tthan a\tclassifier\tthat\thas\ta\tmuch\thigher\trecall\tbut\tlets\ta\tfew\treally\tbad\tvideos\tshow\tup\tin\tyour\tproduct\t(in\tsuch cases,\tyou\tmay\teven\twant\tto\tadd\ta\thuman\tpipeline\tto\tcheck\tthe\tclassifier’s\tvideo\tselection).\tOn\tthe\tother hand,\tsuppose\tyou\ttrain\ta\tclassifier\tto\tdetect\tshoplifters\ton\tsurveillance\timages:\tit\tis\tprobably\tfine\tif\tyour classifier\thas\tonly\t30%\tprecision\tas\tlong\tas\tit\thas\t99%\trecall\t(sure,\tthe\tsecurity\tguards\twill\tget\ta\tfew false\talerts,\tbut\talmost\tall\tshoplifters\twill\tget\tcaught).\n\nUnfortunately,\tyou\tcan’t\thave\tit\tboth\tways:\tincreasing\tprecision\treduces\trecall,\tand\tvice\tversa.\tThis\tis called\tthe\tprecision/recall\ttradeoff.\n\nPrecision/Recall\tTradeoff To\tunderstand\tthis\ttradeoff,\tlet’s\tlook\tat\thow\tthe\tSGDClassifier\tmakes\tits\tclassification\tdecisions.\tFor each\tinstance,\tit\tcomputes\ta\tscore\tbased\ton\ta\tdecision\tfunction,\tand\tif\tthat\tscore\tis\tgreater\tthan\ta threshold,\tit\tassigns\tthe\tinstance\tto\tthe\tpositive\tclass,\tor\telse\tit\tassigns\tit\tto\tthe\tnegative\tclass.\tFigure\t3-3 shows\ta\tfew\tdigits\tpositioned\tfrom\tthe\tlowest\tscore\ton\tthe\tleft\tto\tthe\thighest\tscore\ton\tthe\tright.\tSuppose the\tdecision\tthreshold\tis\tpositioned\tat\tthe\tcentral\tarrow\t(between\tthe\ttwo\t5s):\tyou\twill\tfind\t4\ttrue positives\t(actual\t5s)\ton\tthe\tright\tof\tthat\tthreshold,\tand\tone\tfalse\tpositive\t(actually\ta\t6).\tTherefore,\twith that\tthreshold,\tthe\tprecision\tis\t80%\t(4\tout\tof\t5).\tBut\tout\tof\t6\tactual\t5s,\tthe\tclassifier\tonly\tdetects\t4,\tso\tthe recall\tis\t67%\t(4\tout\tof\t6).\tNow\tif\tyou\traise\tthe\tthreshold\t(move\tit\tto\tthe\tarrow\ton\tthe\tright),\tthe\tfalse positive\t(the\t6)\tbecomes\ta\ttrue\tnegative,\tthereby\tincreasing\tprecision\t(up\tto\t100%\tin\tthis\tcase),\tbut\tone true\tpositive\tbecomes\ta\tfalse\tnegative,\tdecreasing\trecall\tdown\tto\t50%.\tConversely,\tlowering\tthe threshold\tincreases\trecall\tand\treduces\tprecision.\n\nFigure\t3-3.\tDecision\tthreshold\tand\tprecision/recall\ttradeoff\n\nScikit-Learn\tdoes\tnot\tlet\tyou\tset\tthe\tthreshold\tdirectly,\tbut\tit\tdoes\tgive\tyou\taccess\tto\tthe\tdecision\tscores that\tit\tuses\tto\tmake\tpredictions.\tInstead\tof\tcalling\tthe\tclassifier’s\tpredict()\tmethod,\tyou\tcan\tcall\tits decision_function()\tmethod,\twhich\treturns\ta\tscore\tfor\teach\tinstance,\tand\tthen\tmake\tpredictions\tbased on\tthose\tscores\tusing\tany\tthreshold\tyou\twant:\n\n>>>\ty_scores\t=\tsgd_clf.decision_function([some_digit]) >>>\ty_scores array([\t161855.74572176]) >>>\tthreshold\t=\t0 >>>\ty_some_digit_pred\t=\t(y_scores\t>\tthreshold) array([\tTrue],\tdtype=bool)\n\nThe\tSGDClassifier\tuses\ta\tthreshold\tequal\tto\t0,\tso\tthe\tprevious\tcode\treturns\tthe\tsame\tresult\tas\tthe predict()\tmethod\t(i.e.,\tTrue).\tLet’s\traise\tthe\tthreshold:\n\n>>>\tthreshold\t=\t200000 >>>\ty_some_digit_pred\t=\t(y_scores\t>\tthreshold) >>>\ty_some_digit_pred array([False],\tdtype=bool)\n\nThis\tconfirms\tthat\traising\tthe\tthreshold\tdecreases\trecall.\tThe\timage\tactually\trepresents\ta\t5,\tand\tthe classifier\tdetects\tit\twhen\tthe\tthreshold\tis\t0,\tbut\tit\tmisses\tit\twhen\tthe\tthreshold\tis\tincreased\tto\t200,000.\n\nSo\thow\tcan\tyou\tdecide\twhich\tthreshold\tto\tuse?\tFor\tthis\tyou\twill\tfirst\tneed\tto\tget\tthe\tscores\tof\tall instances\tin\tthe\ttraining\tset\tusing\tthe\tcross_val_predict()\tfunction\tagain,\tbut\tthis\ttime\tspecifying\tthat you\twant\tit\tto\treturn\tdecision\tscores\tinstead\tof\tpredictions:\n\ny_scores\t=\tcross_val_predict(sgd_clf,\tX_train,\ty_train_5,\tcv=3, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmethod=\"decision_function\")\n\nNow\twith\tthese\tscores\tyou\tcan\tcompute\tprecision\tand\trecall\tfor\tall\tpossible\tthresholds\tusing\tthe precision_recall_curve()\tfunction:\n\nfrom\tsklearn.metrics\timport\tprecision_recall_curve\n\nprecisions,\trecalls,\tthresholds\t=\tprecision_recall_curve(y_train_5,\ty_scores)\n\nFinally,\tyou\tcan\tplot\tprecision\tand\trecall\tas\tfunctions\tof\tthe\tthreshold\tvalue\tusing\tMatplotlib\t(Figure\t3- 4):\n\ndef\tplot_precision_recall_vs_threshold(precisions,\trecalls,\tthresholds): \t\t\t\tplt.plot(thresholds,\tprecisions[:-1],\t\"b--\",\tlabel=\"Precision\") \t\t\t\tplt.plot(thresholds,\trecalls[:-1],\t\"g-\",\tlabel=\"Recall\") \t\t\t\tplt.xlabel(\"Threshold\") \t\t\t\tplt.legend(loc=\"upper\tleft\") \t\t\t\tplt.ylim([0,\t1])\n\nplot_precision_recall_vs_threshold(precisions,\trecalls,\tthresholds) plt.show()\n\nFigure\t3-4.\tPrecision\tand\trecall\tversus\tthe\tdecision\tthreshold\n\nNOTE\n\nYou\tmay\twonder\twhy\tthe\tprecision\tcurve\tis\tbumpier\tthan\tthe\trecall\tcurve\tin\tFigure\t3-4.\tThe\treason\tis\tthat\tprecision\tmay sometimes\tgo\tdown\twhen\tyou\traise\tthe\tthreshold\t(although\tin\tgeneral\tit\twill\tgo\tup).\tTo\tunderstand\twhy,\tlook\tback\tat\tFigure\t3-3 and\tnotice\twhat\thappens\twhen\tyou\tstart\tfrom\tthe\tcentral\tthreshold\tand\tmove\tit\tjust\tone\tdigit\tto\tthe\tright:\tprecision\tgoes\tfrom\t4/5 (80%)\tdown\tto\t3/4\t(75%).\tOn\tthe\tother\thand,\trecall\tcan\tonly\tgo\tdown\twhen\tthe\tthreshold\tis\tincreased,\twhich\texplains\twhy\tits curve\tlooks\tsmooth.\n\nNow\tyou\tcan\tsimply\tselect\tthe\tthreshold\tvalue\tthat\tgives\tyou\tthe\tbest\tprecision/recall\ttradeoff\tfor\tyour task.\tAnother\tway\tto\tselect\ta\tgood\tprecision/recall\ttradeoff\tis\tto\tplot\tprecision\tdirectly\tagainst\trecall,\tas shown\tin\tFigure\t3-5.\n\nFigure\t3-5.\tPrecision\tversus\trecall\n\nYou\tcan\tsee\tthat\tprecision\treally\tstarts\tto\tfall\tsharply\taround\t80%\trecall.\tYou\twill\tprobably\twant\tto select\ta\tprecision/recall\ttradeoff\tjust\tbefore\tthat\tdrop\t—\tfor\texample,\tat\taround\t60%\trecall.\tBut\tof course\tthe\tchoice\tdepends\ton\tyour\tproject.\n\nSo\tlet’s\tsuppose\tyou\tdecide\tto\taim\tfor\t90%\tprecision.\tYou\tlook\tup\tthe\tfirst\tplot\t(zooming\tin\ta\tbit)\tand find\tthat\tyou\tneed\tto\tuse\ta\tthreshold\tof\tabout\t70,000.\tTo\tmake\tpredictions\t(on\tthe\ttraining\tset\tfor\tnow), instead\tof\tcalling\tthe\tclassifier’s\tpredict()\tmethod,\tyou\tcan\tjust\trun\tthis\tcode:\n\ny_train_pred_90\t=\t(y_scores\t>\t70000)\n\nLet’s\tcheck\tthese\tpredictions’\tprecision\tand\trecall:\n\n>>>\tprecision_score(y_train_5,\ty_train_pred_90) 0.86592051164915484 >>>\trecall_score(y_train_5,\ty_train_pred_90) 0.69931746910164172\n\nGreat,\tyou\thave\ta\t90%\tprecision\tclassifier\t(or\tclose\tenough)!\tAs\tyou\tcan\tsee,\tit\tis\tfairly\teasy\tto\tcreate\ta classifier\twith\tvirtually\tany\tprecision\tyou\twant:\tjust\tset\ta\thigh\tenough\tthreshold,\tand\tyou’re\tdone.\tHmm, not\tso\tfast.\tA\thigh-precision\tclassifier\tis\tnot\tvery\tuseful\tif\tits\trecall\tis\ttoo\tlow!\n\nTIP\n\nIf\tsomeone\tsays\t“let’s\treach\t99%\tprecision,”\tyou\tshould\task,\t“at\twhat\trecall?”\n\nThe\tROC\tCurve The\treceiver\toperating\tcharacteristic\t(ROC)\tcurve\tis\tanother\tcommon\ttool\tused\twith\tbinary\tclassifiers. It\tis\tvery\tsimilar\tto\tthe\tprecision/recall\tcurve,\tbut\tinstead\tof\tplotting\tprecision\tversus\trecall,\tthe\tROC curve\tplots\tthe\ttrue\tpositive\trate\t(another\tname\tfor\trecall)\tagainst\tthe\tfalse\tpositive\trate.\tThe\tFPR\tis\tthe ratio\tof\tnegative\tinstances\tthat\tare\tincorrectly\tclassified\tas\tpositive.\tIt\tis\tequal\tto\tone\tminus\tthe\ttrue negative\trate,\twhich\tis\tthe\tratio\tof\tnegative\tinstances\tthat\tare\tcorrectly\tclassified\tas\tnegative.\tThe\tTNR is\talso\tcalled\tspecificity.\tHence\tthe\tROC\tcurve\tplots\tsensitivity\t(recall)\tversus\t1\t–\tspecificity.\n\nTo\tplot\tthe\tROC\tcurve,\tyou\tfirst\tneed\tto\tcompute\tthe\tTPR\tand\tFPR\tfor\tvarious\tthreshold\tvalues,\tusing\tthe roc_curve()\tfunction:\n\nfrom\tsklearn.metrics\timport\troc_curve\n\nfpr,\ttpr,\tthresholds\t=\troc_curve(y_train_5,\ty_scores)\n\nThen\tyou\tcan\tplot\tthe\tFPR\tagainst\tthe\tTPR\tusing\tMatplotlib.\tThis\tcode\tproduces\tthe\tplot\tin\tFigure\t3-6:\n\ndef\tplot_roc_curve(fpr,\ttpr,\tlabel=None): \t\t\t\tplt.plot(fpr,\ttpr,\tlinewidth=2,\tlabel=label) \t\t\t\tplt.plot([0,\t1],\t[0,\t1],\t'k--') \t\t\t\tplt.axis([0,\t1,\t0,\t1]) \t\t\t\tplt.xlabel('False\tPositive\tRate') \t\t\t\tplt.ylabel('True\tPositive\tRate')\n\nplot_roc_curve(fpr,\ttpr) plt.show()\n\nFigure\t3-6.\tROC\tcurve\n\nOnce\tagain\tthere\tis\ta\ttradeoff:\tthe\thigher\tthe\trecall\t(TPR),\tthe\tmore\tfalse\tpositives\t(FPR)\tthe\tclassifier\n\nproduces.\tThe\tdotted\tline\trepresents\tthe\tROC\tcurve\tof\ta\tpurely\trandom\tclassifier;\ta\tgood\tclassifier\tstays as\tfar\taway\tfrom\tthat\tline\tas\tpossible\t(toward\tthe\ttop-left\tcorner).\n\nOne\tway\tto\tcompare\tclassifiers\tis\tto\tmeasure\tthe\tarea\tunder\tthe\tcurve\t(AUC).\tA\tperfect\tclassifier\twill have\ta\tROC\tAUC\tequal\tto\t1,\twhereas\ta\tpurely\trandom\tclassifier\twill\thave\ta\tROC\tAUC\tequal\tto\t0.5. Scikit-Learn\tprovides\ta\tfunction\tto\tcompute\tthe\tROC\tAUC:\n\n>>>\tfrom\tsklearn.metrics\timport\troc_auc_score >>>\troc_auc_score(y_train_5,\ty_scores) 0.96244965559671547\n\nTIP\n\nSince\tthe\tROC\tcurve\tis\tso\tsimilar\tto\tthe\tprecision/recall\t(or\tPR)\tcurve,\tyou\tmay\twonder\thow\tto\tdecide\twhich\tone\tto\tuse.\tAs\ta rule\tof\tthumb,\tyou\tshould\tprefer\tthe\tPR\tcurve\twhenever\tthe\tpositive\tclass\tis\trare\tor\twhen\tyou\tcare\tmore\tabout\tthe\tfalse positives\tthan\tthe\tfalse\tnegatives,\tand\tthe\tROC\tcurve\totherwise.\tFor\texample,\tlooking\tat\tthe\tprevious\tROC\tcurve\t(and\tthe\tROC AUC\tscore),\tyou\tmay\tthink\tthat\tthe\tclassifier\tis\treally\tgood.\tBut\tthis\tis\tmostly\tbecause\tthere\tare\tfew\tpositives\t(5s)\tcompared\tto the\tnegatives\t(non-5s).\tIn\tcontrast,\tthe\tPR\tcurve\tmakes\tit\tclear\tthat\tthe\tclassifier\thas\troom\tfor\timprovement\t(the\tcurve\tcould\tbe closer\tto\tthe\ttop-right\tcorner).\n\nLet’s\ttrain\ta\tRandomForestClassifier\tand\tcompare\tits\tROC\tcurve\tand\tROC\tAUC\tscore\tto\tthe SGDClassifier.\tFirst,\tyou\tneed\tto\tget\tscores\tfor\teach\tinstance\tin\tthe\ttraining\tset.\tBut\tdue\tto\tthe\tway\tit works\t(see\tChapter\t7),\tthe\tRandomForestClassifier\tclass\tdoes\tnot\thave\ta\tdecision_function() method.\tInstead\tit\thas\ta\tpredict_proba()\tmethod.\tScikit-Learn\tclassifiers\tgenerally\thave\tone\tor\tthe other.\tThe\tpredict_proba()\tmethod\treturns\tan\tarray\tcontaining\ta\trow\tper\tinstance\tand\ta\tcolumn\tper class,\teach\tcontaining\tthe\tprobability\tthat\tthe\tgiven\tinstance\tbelongs\tto\tthe\tgiven\tclass\t(e.g.,\t70%\tchance that\tthe\timage\trepresents\ta\t5):\n\nfrom\tsklearn.ensemble\timport\tRandomForestClassifier\n\nforest_clf\t=\tRandomForestClassifier(random_state=42) y_probas_forest\t=\tcross_val_predict(forest_clf,\tX_train,\ty_train_5,\tcv=3, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmethod=\"predict_proba\")\n\nBut\tto\tplot\ta\tROC\tcurve,\tyou\tneed\tscores,\tnot\tprobabilities.\tA\tsimple\tsolution\tis\tto\tuse\tthe\tpositive class’s\tprobability\tas\tthe\tscore:\n\ny_scores_forest\t=\ty_probas_forest[:,\t1]\t\t\t#\tscore\t=\tproba\tof\tpositive\tclass fpr_forest,\ttpr_forest,\tthresholds_forest\t=\troc_curve(y_train_5,y_scores_forest)\n\nNow\tyou\tare\tready\tto\tplot\tthe\tROC\tcurve.\tIt\tis\tuseful\tto\tplot\tthe\tfirst\tROC\tcurve\tas\twell\tto\tsee\thow\tthey compare\t(Figure\t3-7):\n\nplt.plot(fpr,\ttpr,\t\"b:\",\tlabel=\"SGD\") plot_roc_curve(fpr_forest,\ttpr_forest,\t\"Random\tForest\") plt.legend(loc=\"lower\tright\") plt.show()\n\nFigure\t3-7.\tComparing\tROC\tcurves\n\nAs\tyou\tcan\tsee\tin\tFigure\t3-7,\tthe\tRandomForestClassifier’s\tROC\tcurve\tlooks\tmuch\tbetter\tthan\tthe SGDClassifier’s:\tit\tcomes\tmuch\tcloser\tto\tthe\ttop-left\tcorner.\tAs\ta\tresult,\tits\tROC\tAUC\tscore\tis\talso significantly\tbetter:\n\n>>>\troc_auc_score(y_train_5,\ty_scores_forest) 0.99312433660038291\n\nTry\tmeasuring\tthe\tprecision\tand\trecall\tscores:\tyou\tshould\tfind\t98.5%\tprecision\tand\t82.8%\trecall.\tNot too\tbad!\n\nHopefully\tyou\tnow\tknow\thow\tto\ttrain\tbinary\tclassifiers,\tchoose\tthe\tappropriate\tmetric\tfor\tyour\ttask, evaluate\tyour\tclassifiers\tusing\tcross-validation,\tselect\tthe\tprecision/recall\ttradeoff\tthat\tfits\tyour\tneeds, and\tcompare\tvarious\tmodels\tusing\tROC\tcurves\tand\tROC\tAUC\tscores.\tNow\tlet’s\ttry\tto\tdetect\tmore\tthan just\tthe\t5s.\n\nMulticlass\tClassification Whereas\tbinary\tclassifiers\tdistinguish\tbetween\ttwo\tclasses,\tmulticlass\tclassifiers\t(also\tcalled multinomial\tclassifiers)\tcan\tdistinguish\tbetween\tmore\tthan\ttwo\tclasses.\n\nSome\talgorithms\t(such\tas\tRandom\tForest\tclassifiers\tor\tnaive\tBayes\tclassifiers)\tare\tcapable\tof\thandling multiple\tclasses\tdirectly.\tOthers\t(such\tas\tSupport\tVector\tMachine\tclassifiers\tor\tLinear\tclassifiers)\tare strictly\tbinary\tclassifiers.\tHowever,\tthere\tare\tvarious\tstrategies\tthat\tyou\tcan\tuse\tto\tperform\tmulticlass classification\tusing\tmultiple\tbinary\tclassifiers.\n\nFor\texample,\tone\tway\tto\tcreate\ta\tsystem\tthat\tcan\tclassify\tthe\tdigit\timages\tinto\t10\tclasses\t(from\t0\tto\t9)\tis to\ttrain\t10\tbinary\tclassifiers,\tone\tfor\teach\tdigit\t(a\t0-detector,\ta\t1-detector,\ta\t2-detector,\tand\tso\ton).\tThen when\tyou\twant\tto\tclassify\tan\timage,\tyou\tget\tthe\tdecision\tscore\tfrom\teach\tclassifier\tfor\tthat\timage\tand\tyou select\tthe\tclass\twhose\tclassifier\toutputs\tthe\thighest\tscore.\tThis\tis\tcalled\tthe\tone-versus-all\t(OvA) strategy\t(also\tcalled\tone-versus-the-rest).\n\nAnother\tstrategy\tis\tto\ttrain\ta\tbinary\tclassifier\tfor\tevery\tpair\tof\tdigits:\tone\tto\tdistinguish\t0s\tand\t1s, another\tto\tdistinguish\t0s\tand\t2s,\tanother\tfor\t1s\tand\t2s,\tand\tso\ton.\tThis\tis\tcalled\tthe\tone-versus-one (OvO)\tstrategy.\tIf\tthere\tare\tN\tclasses,\tyou\tneed\tto\ttrain\tN\t×\t(N\t–\t1)\t/\t2\tclassifiers.\tFor\tthe\tMNIST problem,\tthis\tmeans\ttraining\t45\tbinary\tclassifiers!\tWhen\tyou\twant\tto\tclassify\tan\timage,\tyou\thave\tto\trun the\timage\tthrough\tall\t45\tclassifiers\tand\tsee\twhich\tclass\twins\tthe\tmost\tduels.\tThe\tmain\tadvantage\tof\tOvO is\tthat\teach\tclassifier\tonly\tneeds\tto\tbe\ttrained\ton\tthe\tpart\tof\tthe\ttraining\tset\tfor\tthe\ttwo\tclasses\tthat\tit\tmust distinguish.\n\nSome\talgorithms\t(such\tas\tSupport\tVector\tMachine\tclassifiers)\tscale\tpoorly\twith\tthe\tsize\tof\tthe\ttraining set,\tso\tfor\tthese\talgorithms\tOvO\tis\tpreferred\tsince\tit\tis\tfaster\tto\ttrain\tmany\tclassifiers\ton\tsmall\ttraining sets\tthan\ttraining\tfew\tclassifiers\ton\tlarge\ttraining\tsets.\tFor\tmost\tbinary\tclassification\talgorithms, however,\tOvA\tis\tpreferred.\n\nScikit-Learn\tdetects\twhen\tyou\ttry\tto\tuse\ta\tbinary\tclassification\talgorithm\tfor\ta\tmulticlass\tclassification task,\tand\tit\tautomatically\truns\tOvA\t(except\tfor\tSVM\tclassifiers\tfor\twhich\tit\tuses\tOvO).\tLet’s\ttry\tthis\twith the\tSGDClassifier:\n\n>>>\tsgd_clf.fit(X_train,\ty_train)\t\t#\ty_train,\tnot\ty_train_5 >>>\tsgd_clf.predict([some_digit]) array([\t5.])\n\nThat\twas\teasy!\tThis\tcode\ttrains\tthe\tSGDClassifier\ton\tthe\ttraining\tset\tusing\tthe\toriginal\ttarget\tclasses from\t0\tto\t9\t(y_train),\tinstead\tof\tthe\t5-versus-all\ttarget\tclasses\t(y_train_5).\tThen\tit\tmakes\ta\tprediction (a\tcorrect\tone\tin\tthis\tcase).\tUnder\tthe\thood,\tScikit-Learn\tactually\ttrained\t10\tbinary\tclassifiers,\tgot\ttheir decision\tscores\tfor\tthe\timage,\tand\tselected\tthe\tclass\twith\tthe\thighest\tscore.\n\nTo\tsee\tthat\tthis\tis\tindeed\tthe\tcase,\tyou\tcan\tcall\tthe\tdecision_function()\tmethod.\tInstead\tof\treturning just\tone\tscore\tper\tinstance,\tit\tnow\treturns\t10\tscores,\tone\tper\tclass:\n\n>>>\tsome_digit_scores\t=\tsgd_clf.decision_function([some_digit]) >>>\tsome_digit_scores array([[-311402.62954431,\t-363517.28355739,\t-446449.5306454\t,\n\n-183226.61023518,\t-414337.15339485,\t\t161855.74572176, \t\t\t\t\t\t\t\t-452576.39616343,\t-471957.14962573,\t-518542.33997148, \t\t\t\t\t\t\t\t-536774.63961222]])\n\nThe\thighest\tscore\tis\tindeed\tthe\tone\tcorresponding\tto\tclass\t5:\n\n>>>\tnp.argmax(some_digit_scores) 5 >>>\tsgd_clf.classes_ array([\t0.,\t\t1.,\t\t2.,\t\t3.,\t\t4.,\t\t5.,\t\t6.,\t\t7.,\t\t8.,\t\t9.]) >>>\tsgd_clf.classes_[5] 5.0\n\nWARNING\n\nWhen\ta\tclassifier\tis\ttrained,\tit\tstores\tthe\tlist\tof\ttarget\tclasses\tin\tits\tclasses_\tattribute,\tordered\tby\tvalue.\tIn\tthis\tcase,\tthe\tindex\tof each\tclass\tin\tthe\tclasses_\tarray\tconveniently\tmatches\tthe\tclass\titself\t(e.g.,\tthe\tclass\tat\tindex\t5\thappens\tto\tbe\tclass\t5),\tbut\tin general\tyou\twon’t\tbe\tso\tlucky.\n\nIf\tyou\twant\tto\tforce\tScikitLearn\tto\tuse\tone-versus-one\tor\tone-versus-all,\tyou\tcan\tuse\tthe OneVsOneClassifier\tor\tOneVsRestClassifier\tclasses.\tSimply\tcreate\tan\tinstance\tand\tpass\ta\tbinary classifier\tto\tits\tconstructor.\tFor\texample,\tthis\tcode\tcreates\ta\tmulticlass\tclassifier\tusing\tthe\tOvO\tstrategy, based\ton\ta\tSGDClassifier:\n\n>>>\tfrom\tsklearn.multiclass\timport\tOneVsOneClassifier >>>\tovo_clf\t=\tOneVsOneClassifier(SGDClassifier(random_state=42)) >>>\tovo_clf.fit(X_train,\ty_train) >>>\tovo_clf.predict([some_digit]) array([\t5.]) >>>\tlen(ovo_clf.estimators_) 45\n\nTraining\ta\tRandomForestClassifier\tis\tjust\tas\teasy:\n\n>>>\tforest_clf.fit(X_train,\ty_train) >>>\tforest_clf.predict([some_digit]) array([\t5.])\n\nThis\ttime\tScikit-Learn\tdid\tnot\thave\tto\trun\tOvA\tor\tOvO\tbecause\tRandom\tForest\tclassifiers\tcan\tdirectly classify\tinstances\tinto\tmultiple\tclasses.\tYou\tcan\tcall\tpredict_proba()\tto\tget\tthe\tlist\tof\tprobabilities\tthat the\tclassifier\tassigned\tto\teach\tinstance\tfor\teach\tclass:\n\n>>>\tforest_clf.predict_proba([some_digit]) array([[\t0.1,\t\t0.\t,\t\t0.\t,\t\t0.1,\t\t0.\t,\t\t0.8,\t\t0.\t,\t\t0.\t,\t\t0.\t,\t\t0.\t]])\n\nYou\tcan\tsee\tthat\tthe\tclassifier\tis\tfairly\tconfident\tabout\tits\tprediction:\tthe\t0.8\tat\tthe\t5th\tindex\tin\tthe\tarray means\tthat\tthe\tmodel\testimates\tan\t80%\tprobability\tthat\tthe\timage\trepresents\ta\t5.\tIt\talso\tthinks\tthat\tthe image\tcould\tinstead\tbe\ta\t0\tor\ta\t3\t(10%\tchance\teach).\n\nNow\tof\tcourse\tyou\twant\tto\tevaluate\tthese\tclassifiers.\tAs\tusual,\tyou\twant\tto\tuse\tcross-validation.\tLet’s evaluate\tthe\tSGDClassifier’s\taccuracy\tusing\tthe\tcross_val_score()\tfunction:\n\n>>>\tcross_val_score(sgd_clf,\tX_train,\ty_train,\tcv=3,\tscoring=\"accuracy\") array([\t0.84063187,\t\t0.84899245,\t\t0.86652998])\n\nIt\tgets\tover\t84%\ton\tall\ttest\tfolds.\tIf\tyou\tused\ta\trandom\tclassifier,\tyou\twould\tget\t10%\taccuracy,\tso\tthis\tis not\tsuch\ta\tbad\tscore,\tbut\tyou\tcan\tstill\tdo\tmuch\tbetter.\tFor\texample,\tsimply\tscaling\tthe\tinputs\t(as discussed\tin\tChapter\t2)\tincreases\taccuracy\tabove\t90%:\n\n>>>\tfrom\tsklearn.preprocessing\timport\tStandardScaler >>>\tscaler\t=\tStandardScaler() >>>\tX_train_scaled\t=\tscaler.fit_transform(X_train.astype(np.float64)) >>>\tcross_val_score(sgd_clf,\tX_train_scaled,\ty_train,\tcv=3,\tscoring=\"accuracy\") array([\t0.91011798,\t\t0.90874544,\t\t0.906636\t\t])\n\nError\tAnalysis Of\tcourse,\tif\tthis\twere\ta\treal\tproject,\tyou\twould\tfollow\tthe\tsteps\tin\tyour\tMachine\tLearning\tproject checklist\t(see\tAppendix\tB):\texploring\tdata\tpreparation\toptions,\ttrying\tout\tmultiple\tmodels,\tshortlisting the\tbest\tones\tand\tfine-tuning\ttheir\thyperparameters\tusing\tGridSearchCV,\tand\tautomating\tas\tmuch\tas possible,\tas\tyou\tdid\tin\tthe\tprevious\tchapter.\tHere,\twe\twill\tassume\tthat\tyou\thave\tfound\ta\tpromising\tmodel and\tyou\twant\tto\tfind\tways\tto\timprove\tit.\tOne\tway\tto\tdo\tthis\tis\tto\tanalyze\tthe\ttypes\tof\terrors\tit\tmakes.\n\nFirst,\tyou\tcan\tlook\tat\tthe\tconfusion\tmatrix.\tYou\tneed\tto\tmake\tpredictions\tusing\tthe cross_val_predict()\tfunction,\tthen\tcall\tthe\tconfusion_matrix()\tfunction,\tjust\tlike\tyou\tdid\tearlier:\n\n>>>\ty_train_pred\t=\tcross_val_predict(sgd_clf,\tX_train_scaled,\ty_train,\tcv=3) >>>\tconf_mx\t=\tconfusion_matrix(y_train,\ty_train_pred) >>>\tconf_mx array([[5725,\t\t\t\t3,\t\t\t24,\t\t\t\t9,\t\t\t10,\t\t\t49,\t\t\t50,\t\t\t10,\t\t\t39,\t\t\t\t4], \t\t\t\t\t\t\t[\t\t\t2,\t6493,\t\t\t43,\t\t\t25,\t\t\t\t7,\t\t\t40,\t\t\t\t5,\t\t\t10,\t\t109,\t\t\t\t8], \t\t\t\t\t\t\t[\t\t51,\t\t\t41,\t5321,\t\t104,\t\t\t89,\t\t\t26,\t\t\t87,\t\t\t60,\t\t166,\t\t\t13], \t\t\t\t\t\t\t[\t\t47,\t\t\t46,\t\t141,\t5342,\t\t\t\t1,\t\t231,\t\t\t40,\t\t\t50,\t\t141,\t\t\t92], \t\t\t\t\t\t\t[\t\t19,\t\t\t29,\t\t\t41,\t\t\t10,\t5366,\t\t\t\t9,\t\t\t56,\t\t\t37,\t\t\t86,\t\t189], \t\t\t\t\t\t\t[\t\t73,\t\t\t45,\t\t\t36,\t\t193,\t\t\t64,\t4582,\t\t111,\t\t\t30,\t\t193,\t\t\t94], \t\t\t\t\t\t\t[\t\t29,\t\t\t34,\t\t\t44,\t\t\t\t2,\t\t\t42,\t\t\t85,\t5627,\t\t\t10,\t\t\t45,\t\t\t\t0], \t\t\t\t\t\t\t[\t\t25,\t\t\t24,\t\t\t74,\t\t\t32,\t\t\t54,\t\t\t12,\t\t\t\t6,\t5787,\t\t\t15,\t\t236], \t\t\t\t\t\t\t[\t\t52,\t\t161,\t\t\t73,\t\t156,\t\t\t10,\t\t163,\t\t\t61,\t\t\t25,\t5027,\t\t123], \t\t\t\t\t\t\t[\t\t43,\t\t\t35,\t\t\t26,\t\t\t92,\t\t178,\t\t\t28,\t\t\t\t2,\t\t223,\t\t\t82,\t5240]])\n\nThat’s\ta\tlot\tof\tnumbers.\tIt’s\toften\tmore\tconvenient\tto\tlook\tat\tan\timage\trepresentation\tof\tthe\tconfusion matrix,\tusing\tMatplotlib’s\tmatshow()\tfunction:\n\nplt.matshow(conf_mx,\tcmap=plt.cm.gray) plt.show()\n\nThis\tconfusion\tmatrix\tlooks\tfairly\tgood,\tsince\tmost\timages\tare\ton\tthe\tmain\tdiagonal,\twhich\tmeans\tthat they\twere\tclassified\tcorrectly.\tThe\t5s\tlook\tslightly\tdarker\tthan\tthe\tother\tdigits,\twhich\tcould\tmean\tthat there\tare\tfewer\timages\tof\t5s\tin\tthe\tdataset\tor\tthat\tthe\tclassifier\tdoes\tnot\tperform\tas\twell\ton\t5s\tas\ton\tother digits.\tIn\tfact,\tyou\tcan\tverify\tthat\tboth\tare\tthe\tcase.\n\nLet’s\tfocus\tthe\tplot\ton\tthe\terrors.\tFirst,\tyou\tneed\tto\tdivide\teach\tvalue\tin\tthe\tconfusion\tmatrix\tby\tthe number\tof\timages\tin\tthe\tcorresponding\tclass,\tso\tyou\tcan\tcompare\terror\trates\tinstead\tof\tabsolute\tnumber of\terrors\t(which\twould\tmake\tabundant\tclasses\tlook\tunfairly\tbad):\n\nrow_sums\t=\tconf_mx.sum(axis=1,\tkeepdims=True) norm_conf_mx\t=\tconf_mx\t/\trow_sums\n\nNow\tlet’s\tfill\tthe\tdiagonal\twith\tzeros\tto\tkeep\tonly\tthe\terrors,\tand\tlet’s\tplot\tthe\tresult:\n\nnp.fill_diagonal(norm_conf_mx,\t0) plt.matshow(norm_conf_mx,\tcmap=plt.cm.gray) plt.show()\n\nNow\tyou\tcan\tclearly\tsee\tthe\tkinds\tof\terrors\tthe\tclassifier\tmakes.\tRemember\tthat\trows\trepresent\tactual classes,\twhile\tcolumns\trepresent\tpredicted\tclasses.\tThe\tcolumns\tfor\tclasses\t8\tand\t9\tare\tquite\tbright, which\ttells\tyou\tthat\tmany\timages\tget\tmisclassified\tas\t8s\tor\t9s.\tSimilarly,\tthe\trows\tfor\tclasses\t8\tand\t9\tare also\tquite\tbright,\ttelling\tyou\tthat\t8s\tand\t9s\tare\toften\tconfused\twith\tother\tdigits.\tConversely,\tsome\trows are\tpretty\tdark,\tsuch\tas\trow\t1:\tthis\tmeans\tthat\tmost\t1s\tare\tclassified\tcorrectly\t(a\tfew\tare\tconfused\twith 8s,\tbut\tthat’s\tabout\tit).\tNotice\tthat\tthe\terrors\tare\tnot\tperfectly\tsymmetrical;\tfor\texample,\tthere\tare\tmore\t5s misclassified\tas\t8s\tthan\tthe\treverse.\n\nAnalyzing\tthe\tconfusion\tmatrix\tcan\toften\tgive\tyou\tinsights\ton\tways\tto\timprove\tyour\tclassifier.\tLooking\tat this\tplot,\tit\tseems\tthat\tyour\tefforts\tshould\tbe\tspent\ton\timproving\tclassification\tof\t8s\tand\t9s,\tas\twell\tas fixing\tthe\tspecific\t3/5\tconfusion.\tFor\texample,\tyou\tcould\ttry\tto\tgather\tmore\ttraining\tdata\tfor\tthese\tdigits. Or\tyou\tcould\tengineer\tnew\tfeatures\tthat\twould\thelp\tthe\tclassifier\t—\tfor\texample,\twriting\tan\talgorithm\tto count\tthe\tnumber\tof\tclosed\tloops\t(e.g.,\t8\thas\ttwo,\t6\thas\tone,\t5\thas\tnone).\tOr\tyou\tcould\tpreprocess\tthe images\t(e.g.,\tusing\tScikit-Image,\tPillow,\tor\tOpenCV)\tto\tmake\tsome\tpatterns\tstand\tout\tmore,\tsuch\tas closed\tloops.\n\nAnalyzing\tindividual\terrors\tcan\talso\tbe\ta\tgood\tway\tto\tgain\tinsights\ton\twhat\tyour\tclassifier\tis\tdoing\tand why\tit\tis\tfailing,\tbut\tit\tis\tmore\tdifficult\tand\ttime-consuming.\tFor\texample,\tlet’s\tplot\texamples\tof\t3s\tand\t5s (the\tplot_digits()\tfunction\tjust\tuses\tMatplotlib’s\timshow()\tfunction;\tsee\tthis\tchapter’s\tJupyter\n\nnotebook\tfor\tdetails):\n\ncl_a,\tcl_b\t=\t3,\t5 X_aa\t=\tX_train[(y_train\t==\tcl_a)\t&\t(y_train_pred\t==\tcl_a)] X_ab\t=\tX_train[(y_train\t==\tcl_a)\t&\t(y_train_pred\t==\tcl_b)] X_ba\t=\tX_train[(y_train\t==\tcl_b)\t&\t(y_train_pred\t==\tcl_a)] X_bb\t=\tX_train[(y_train\t==\tcl_b)\t&\t(y_train_pred\t==\tcl_b)]\n\nplt.figure(figsize=(8,8)) plt.subplot(221);\tplot_digits(X_aa[:25],\timages_per_row=5) plt.subplot(222);\tplot_digits(X_ab[:25],\timages_per_row=5) plt.subplot(223);\tplot_digits(X_ba[:25],\timages_per_row=5) plt.subplot(224);\tplot_digits(X_bb[:25],\timages_per_row=5) plt.show()\n\nThe\ttwo\t5×5\tblocks\ton\tthe\tleft\tshow\tdigits\tclassified\tas\t3s,\tand\tthe\ttwo\t5×5\tblocks\ton\tthe\tright\tshow images\tclassified\tas\t5s.\tSome\tof\tthe\tdigits\tthat\tthe\tclassifier\tgets\twrong\t(i.e.,\tin\tthe\tbottom-left\tand\ttop- right\tblocks)\tare\tso\tbadly\twritten\tthat\teven\ta\thuman\twould\thave\ttrouble\tclassifying\tthem\t(e.g.,\tthe\t5\ton the\t8th\trow\tand\t1st\tcolumn\ttruly\tlooks\tlike\ta\t3).\tHowever,\tmost\tmisclassified\timages\tseem\tlike\tobvious errors\tto\tus,\tand\tit’s\thard\tto\tunderstand\twhy\tthe\tclassifier\tmade\tthe\tmistakes\tit\tdid.3\tThe\treason\tis\tthat\twe used\ta\tsimple\tSGDClassifier,\twhich\tis\ta\tlinear\tmodel.\tAll\tit\tdoes\tis\tassign\ta\tweight\tper\tclass\tto\teach pixel,\tand\twhen\tit\tsees\ta\tnew\timage\tit\tjust\tsums\tup\tthe\tweighted\tpixel\tintensities\tto\tget\ta\tscore\tfor\teach class.\tSo\tsince\t3s\tand\t5s\tdiffer\tonly\tby\ta\tfew\tpixels,\tthis\tmodel\twill\teasily\tconfuse\tthem.\n\nThe\tmain\tdifference\tbetween\t3s\tand\t5s\tis\tthe\tposition\tof\tthe\tsmall\tline\tthat\tjoins\tthe\ttop\tline\tto\tthe\tbottom arc.\tIf\tyou\tdraw\ta\t3\twith\tthe\tjunction\tslightly\tshifted\tto\tthe\tleft,\tthe\tclassifier\tmight\tclassify\tit\tas\ta\t5,\tand vice\tversa.\tIn\tother\twords,\tthis\tclassifier\tis\tquite\tsensitive\tto\timage\tshifting\tand\trotation.\tSo\tone\tway\tto reduce\tthe\t3/5\tconfusion\twould\tbe\tto\tpreprocess\tthe\timages\tto\tensure\tthat\tthey\tare\twell\tcentered\tand\tnot too\trotated.\tThis\twill\tprobably\thelp\treduce\tother\terrors\tas\twell.\n\nMultilabel\tClassification Until\tnow\teach\tinstance\thas\talways\tbeen\tassigned\tto\tjust\tone\tclass.\tIn\tsome\tcases\tyou\tmay\twant\tyour classifier\tto\toutput\tmultiple\tclasses\tfor\teach\tinstance.\tFor\texample,\tconsider\ta\tface-recognition classifier:\twhat\tshould\tit\tdo\tif\tit\trecognizes\tseveral\tpeople\ton\tthe\tsame\tpicture?\tOf\tcourse\tit\tshould attach\tone\tlabel\tper\tperson\tit\trecognizes.\tSay\tthe\tclassifier\thas\tbeen\ttrained\tto\trecognize\tthree\tfaces, Alice,\tBob,\tand\tCharlie;\tthen\twhen\tit\tis\tshown\ta\tpicture\tof\tAlice\tand\tCharlie,\tit\tshould\toutput\t[1,\t0,\t1] (meaning\t“Alice\tyes,\tBob\tno,\tCharlie\tyes”).\tSuch\ta\tclassification\tsystem\tthat\toutputs\tmultiple\tbinary labels\tis\tcalled\ta\tmultilabel\tclassification\tsystem.\n\nWe\twon’t\tgo\tinto\tface\trecognition\tjust\tyet,\tbut\tlet’s\tlook\tat\ta\tsimpler\texample,\tjust\tfor\tillustration purposes:\n\nfrom\tsklearn.neighbors\timport\tKNeighborsClassifier\n\ny_train_large\t=\t(y_train\t>=\t7) y_train_odd\t=\t(y_train\t%\t2\t==\t1) y_multilabel\t=\tnp.c_[y_train_large,\ty_train_odd]\n\nknn_clf\t=\tKNeighborsClassifier() knn_clf.fit(X_train,\ty_multilabel)\n\nThis\tcode\tcreates\ta\ty_multilabel\tarray\tcontaining\ttwo\ttarget\tlabels\tfor\teach\tdigit\timage:\tthe\tfirst indicates\twhether\tor\tnot\tthe\tdigit\tis\tlarge\t(7,\t8,\tor\t9)\tand\tthe\tsecond\tindicates\twhether\tor\tnot\tit\tis\todd. The\tnext\tlines\tcreate\ta\tKNeighborsClassifier\tinstance\t(which\tsupports\tmultilabel\tclassification,\tbut not\tall\tclassifiers\tdo)\tand\twe\ttrain\tit\tusing\tthe\tmultiple\ttargets\tarray.\tNow\tyou\tcan\tmake\ta\tprediction,\tand notice\tthat\tit\toutputs\ttwo\tlabels:\n\n>>>\tknn_clf.predict([some_digit]) array([[False,\t\tTrue]],\tdtype=bool)\n\nAnd\tit\tgets\tit\tright!\tThe\tdigit\t5\tis\tindeed\tnot\tlarge\t(False)\tand\todd\t(True).\n\nThere\tare\tmany\tways\tto\tevaluate\ta\tmultilabel\tclassifier,\tand\tselecting\tthe\tright\tmetric\treally\tdepends\ton your\tproject.\tFor\texample,\tone\tapproach\tis\tto\tmeasure\tthe\tF1\tscore\tfor\teach\tindividual\tlabel\t(or\tany\tother binary\tclassifier\tmetric\tdiscussed\tearlier),\tthen\tsimply\tcompute\tthe\taverage\tscore.\tThis\tcode\tcomputes the\taverage\tF1\tscore\tacross\tall\tlabels:\n\n>>>\ty_train_knn_pred\t=\tcross_val_predict(knn_clf,\tX_train,\ty_train,\tcv=3) >>>\tf1_score(y_train,\ty_train_knn_pred,\taverage=\"macro\") 0.96845540180280221\n\nThis\tassumes\tthat\tall\tlabels\tare\tequally\timportant,\twhich\tmay\tnot\tbe\tthe\tcase.\tIn\tparticular,\tif\tyou\thave many\tmore\tpictures\tof\tAlice\tthan\tof\tBob\tor\tCharlie,\tyou\tmay\twant\tto\tgive\tmore\tweight\tto\tthe\tclassifier’s score\ton\tpictures\tof\tAlice.\tOne\tsimple\toption\tis\tto\tgive\teach\tlabel\ta\tweight\tequal\tto\tits\tsupport\t(i.e.,\tthe number\tof\tinstances\twith\tthat\ttarget\tlabel).\tTo\tdo\tthis,\tsimply\tset\taverage=\"weighted\"\tin\tthe\tpreceding code.4\n\nMultioutput\tClassification The\tlast\ttype\tof\tclassification\ttask\twe\tare\tgoing\tto\tdiscuss\there\tis\tcalled\tmultioutput-multiclass classification\t(or\tsimply\tmultioutput\tclassification).\tIt\tis\tsimply\ta\tgeneralization\tof\tmultilabel classification\twhere\teach\tlabel\tcan\tbe\tmulticlass\t(i.e.,\tit\tcan\thave\tmore\tthan\ttwo\tpossible\tvalues).\n\nTo\tillustrate\tthis,\tlet’s\tbuild\ta\tsystem\tthat\tremoves\tnoise\tfrom\timages.\tIt\twill\ttake\tas\tinput\ta\tnoisy\tdigit image,\tand\tit\twill\t(hopefully)\toutput\ta\tclean\tdigit\timage,\trepresented\tas\tan\tarray\tof\tpixel\tintensities,\tjust like\tthe\tMNIST\timages.\tNotice\tthat\tthe\tclassifier’s\toutput\tis\tmultilabel\t(one\tlabel\tper\tpixel)\tand\teach label\tcan\thave\tmultiple\tvalues\t(pixel\tintensity\tranges\tfrom\t0\tto\t255).\tIt\tis\tthus\tan\texample\tof\ta\tmultioutput classification\tsystem.\n\nNOTE\n\nThe\tline\tbetween\tclassification\tand\tregression\tis\tsometimes\tblurry,\tsuch\tas\tin\tthis\texample.\tArguably,\tpredicting\tpixel\tintensity\tis more\takin\tto\tregression\tthan\tto\tclassification.\tMoreover,\tmultioutput\tsystems\tare\tnot\tlimited\tto\tclassification\ttasks;\tyou\tcould\teven have\ta\tsystem\tthat\toutputs\tmultiple\tlabels\tper\tinstance,\tincluding\tboth\tclass\tlabels\tand\tvalue\tlabels.\n\nLet’s\tstart\tby\tcreating\tthe\ttraining\tand\ttest\tsets\tby\ttaking\tthe\tMNIST\timages\tand\tadding\tnoise\tto\ttheir\tpixel intensities\tusing\tNumPy’s\trandint()\tfunction.\tThe\ttarget\timages\twill\tbe\tthe\toriginal\timages:\n\nnoise\t=\tnp.random.randint(0,\t100,\t(len(X_train),\t784)) X_train_mod\t=\tX_train\t+\tnoise noise\t=\tnp.random.randint(0,\t100,\t(len(X_test),\t784)) X_test_mod\t=\tX_test\t+\tnoise y_train_mod\t=\tX_train y_test_mod\t=\tX_test\n\nLet’s\ttake\ta\tpeek\tat\tan\timage\tfrom\tthe\ttest\tset\t(yes,\twe’re\tsnooping\ton\tthe\ttest\tdata,\tso\tyou\tshould\tbe frowning\tright\tnow):\n\nOn\tthe\tleft\tis\tthe\tnoisy\tinput\timage,\tand\ton\tthe\tright\tis\tthe\tclean\ttarget\timage.\tNow\tlet’s\ttrain\tthe\tclassifier\n\nand\tmake\tit\tclean\tthis\timage:\n\nknn_clf.fit(X_train_mod,\ty_train_mod) clean_digit\t=\tknn_clf.predict([X_test_mod[some_index]]) plot_digit(clean_digit)\n\nLooks\tclose\tenough\tto\tthe\ttarget!\tThis\tconcludes\tour\ttour\tof\tclassification.\tHopefully\tyou\tshould\tnow know\thow\tto\tselect\tgood\tmetrics\tfor\tclassification\ttasks,\tpick\tthe\tappropriate\tprecision/recall\ttradeoff, compare\tclassifiers,\tand\tmore\tgenerally\tbuild\tgood\tclassification\tsystems\tfor\ta\tvariety\tof\ttasks.\n\nExercises\n\n1.\t Try\tto\tbuild\ta\tclassifier\tfor\tthe\tMNIST\tdataset\tthat\tachieves\tover\t97%\taccuracy\ton\tthe\ttest\tset.\tHint: the\tKNeighborsClassifier\tworks\tquite\twell\tfor\tthis\ttask;\tyou\tjust\tneed\tto\tfind\tgood hyperparameter\tvalues\t(try\ta\tgrid\tsearch\ton\tthe\tweights\tand\tn_neighbors\thyperparameters).\n\n2.\t Write\ta\tfunction\tthat\tcan\tshift\tan\tMNIST\timage\tin\tany\tdirection\t(left,\tright,\tup,\tor\tdown)\tby\tone pixel.5\tThen,\tfor\teach\timage\tin\tthe\ttraining\tset,\tcreate\tfour\tshifted\tcopies\t(one\tper\tdirection)\tand\tadd them\tto\tthe\ttraining\tset.\tFinally,\ttrain\tyour\tbest\tmodel\ton\tthis\texpanded\ttraining\tset\tand\tmeasure\tits accuracy\ton\tthe\ttest\tset.\tYou\tshould\tobserve\tthat\tyour\tmodel\tperforms\teven\tbetter\tnow!\tThis technique\tof\tartificially\tgrowing\tthe\ttraining\tset\tis\tcalled\tdata\taugmentation\tor\ttraining\tset expansion.\n\n3.\t Tackle\tthe\tTitanic\tdataset.\tA\tgreat\tplace\tto\tstart\tis\ton\tKaggle.\n\n4.\t Build\ta\tspam\tclassifier\t(a\tmore\tchallenging\texercise):\n\nDownload\texamples\tof\tspam\tand\tham\tfrom\tApache\tSpamAssassin’s\tpublic\tdatasets.\n\nUnzip\tthe\tdatasets\tand\tfamiliarize\tyourself\twith\tthe\tdata\tformat.\n\nSplit\tthe\tdatasets\tinto\ta\ttraining\tset\tand\ta\ttest\tset.\n\nWrite\ta\tdata\tpreparation\tpipeline\tto\tconvert\teach\temail\tinto\ta\tfeature\tvector.\tYour\tpreparation pipeline\tshould\ttransform\tan\temail\tinto\ta\t(sparse)\tvector\tindicating\tthe\tpresence\tor\tabsence\tof each\tpossible\tword.\tFor\texample,\tif\tall\temails\tonly\tever\tcontain\tfour\twords,\t“Hello,”\t“how,” “are,”\t“you,”\tthen\tthe\temail\t“Hello\tyou\tHello\tHello\tyou”\twould\tbe\tconverted\tinto\ta\tvector\t[1, 0,\t0,\t1]\t(meaning\t[“Hello”\tis\tpresent,\t“how”\tis\tabsent,\t“are”\tis\tabsent,\t“you”\tis\tpresent]),\tor [3,\t0,\t0,\t2]\tif\tyou\tprefer\tto\tcount\tthe\tnumber\tof\toccurrences\tof\teach\tword.\n\nYou\tmay\twant\tto\tadd\thyperparameters\tto\tyour\tpreparation\tpipeline\tto\tcontrol\twhether\tor\tnot\tto strip\toff\temail\theaders,\tconvert\teach\temail\tto\tlowercase,\tremove\tpunctuation,\treplace\tall\tURLs with\t“URL,”\treplace\tall\tnumbers\twith\t“NUMBER,”\tor\teven\tperform\tstemming\t(i.e.,\ttrim\toff word\tendings;\tthere\tare\tPython\tlibraries\tavailable\tto\tdo\tthis).\n\nThen\ttry\tout\tseveral\tclassifiers\tand\tsee\tif\tyou\tcan\tbuild\ta\tgreat\tspam\tclassifier,\twith\tboth\thigh recall\tand\thigh\tprecision.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tthe\tonline\tJupyter\tnotebooks\tat https://github.com/ageron/handson-ml.\n\n1\n\nBy\tdefault\tScikit-Learn\tcaches\tdownloaded\tdatasets\tin\ta\tdirectory\tcalled\t$HOME/scikit_learn_data.\n\n2\n\nShuffling\tmay\tbe\ta\tbad\tidea\tin\tsome\tcontexts\t—\tfor\texample,\tif\tyou\tare\tworking\ton\ttime\tseries\tdata\t(such\tas\tstock\tmarket\tprices\tor weather\tconditions).\tWe\twill\texplore\tthis\tin\tthe\tnext\tchapters.\n\n3\n\nBut\tremember\tthat\tour\tbrain\tis\ta\tfantastic\tpattern\trecognition\tsystem,\tand\tour\tvisual\tsystem\tdoes\ta\tlot\tof\tcomplex\tpreprocessing\tbefore any\tinformation\treaches\tour\tconsciousness,\tso\tthe\tfact\tthat\tit\tfeels\tsimple\tdoes\tnot\tmean\tthat\tit\tis.\n\n4\n\n4\n\nScikit-Learn\toffers\ta\tfew\tother\taveraging\toptions\tand\tmultilabel\tclassifier\tmetrics;\tsee\tthe\tdocumentation\tfor\tmore\tdetails.\n\nYou\tcan\tuse\tthe\tshift()\tfunction\tfrom\tthe\tscipy.ndimage.interpolation\tmodule.\tFor\texample,\tshift(image,\t[2,\t1],\tcval=0)\tshifts the\timage\t2\tpixels\tdown\tand\t1\tpixel\tto\tthe\tright.\n\n5\n\nChapter\t4.\tTraining\tModels\n\nSo\tfar\twe\thave\ttreated\tMachine\tLearning\tmodels\tand\ttheir\ttraining\talgorithms\tmostly\tlike\tblack\tboxes.\tIf you\twent\tthrough\tsome\tof\tthe\texercises\tin\tthe\tprevious\tchapters,\tyou\tmay\thave\tbeen\tsurprised\tby\thow much\tyou\tcan\tget\tdone\twithout\tknowing\tanything\tabout\twhat’s\tunder\tthe\thood:\tyou\toptimized\ta\tregression system,\tyou\timproved\ta\tdigit\timage\tclassifier,\tand\tyou\teven\tbuilt\ta\tspam\tclassifier\tfrom\tscratch\t—\tall this\twithout\tknowing\thow\tthey\tactually\twork.\tIndeed,\tin\tmany\tsituations\tyou\tdon’t\treally\tneed\tto\tknow\tthe implementation\tdetails.\n\nHowever,\thaving\ta\tgood\tunderstanding\tof\thow\tthings\twork\tcan\thelp\tyou\tquickly\thome\tin\ton\tthe appropriate\tmodel,\tthe\tright\ttraining\talgorithm\tto\tuse,\tand\ta\tgood\tset\tof\thyperparameters\tfor\tyour\ttask. Understanding\twhat’s\tunder\tthe\thood\twill\talso\thelp\tyou\tdebug\tissues\tand\tperform\terror\tanalysis\tmore efficiently.\tLastly,\tmost\tof\tthe\ttopics\tdiscussed\tin\tthis\tchapter\twill\tbe\tessential\tin\tunderstanding,\tbuilding, and\ttraining\tneural\tnetworks\t(discussed\tin\tPart\tII\tof\tthis\tbook).\n\nIn\tthis\tchapter,\twe\twill\tstart\tby\tlooking\tat\tthe\tLinear\tRegression\tmodel,\tone\tof\tthe\tsimplest\tmodels\tthere is.\tWe\twill\tdiscuss\ttwo\tvery\tdifferent\tways\tto\ttrain\tit:\n\nUsing\ta\tdirect\t“closed-form”\tequation\tthat\tdirectly\tcomputes\tthe\tmodel\tparameters\tthat\tbest\tfit\tthe model\tto\tthe\ttraining\tset\t(i.e.,\tthe\tmodel\tparameters\tthat\tminimize\tthe\tcost\tfunction\tover\tthe\ttraining set).\n\nUsing\tan\titerative\toptimization\tapproach,\tcalled\tGradient\tDescent\t(GD),\tthat\tgradually\ttweaks\tthe model\tparameters\tto\tminimize\tthe\tcost\tfunction\tover\tthe\ttraining\tset,\teventually\tconverging\tto\tthe same\tset\tof\tparameters\tas\tthe\tfirst\tmethod.\tWe\twill\tlook\tat\ta\tfew\tvariants\tof\tGradient\tDescent\tthat we\twill\tuse\tagain\tand\tagain\twhen\twe\tstudy\tneural\tnetworks\tin\tPart\tII:\tBatch\tGD,\tMini-batch\tGD, and\tStochastic\tGD.\n\nNext\twe\twill\tlook\tat\tPolynomial\tRegression,\ta\tmore\tcomplex\tmodel\tthat\tcan\tfit\tnonlinear\tdatasets.\tSince this\tmodel\thas\tmore\tparameters\tthan\tLinear\tRegression,\tit\tis\tmore\tprone\tto\toverfitting\tthe\ttraining\tdata, so\twe\twill\tlook\tat\thow\tto\tdetect\twhether\tor\tnot\tthis\tis\tthe\tcase,\tusing\tlearning\tcurves,\tand\tthen\twe\twill look\tat\tseveral\tregularization\ttechniques\tthat\tcan\treduce\tthe\trisk\tof\toverfitting\tthe\ttraining\tset.\n\nFinally,\twe\twill\tlook\tat\ttwo\tmore\tmodels\tthat\tare\tcommonly\tused\tfor\tclassification\ttasks:\tLogistic Regression\tand\tSoftmax\tRegression.\n\nWARNING\n\nThere\twill\tbe\tquite\ta\tfew\tmath\tequations\tin\tthis\tchapter,\tusing\tbasic\tnotions\tof\tlinear\talgebra\tand\tcalculus.\tTo\tunderstand\tthese equations,\tyou\twill\tneed\tto\tknow\twhat\tvectors\tand\tmatrices\tare,\thow\tto\ttranspose\tthem,\twhat\tthe\tdot\tproduct\tis,\twhat\tmatrix inverse\tis,\tand\twhat\tpartial\tderivatives\tare.\tIf\tyou\tare\tunfamiliar\twith\tthese\tconcepts,\tplease\tgo\tthrough\tthe\tlinear\talgebra\tand calculus\tintroductory\ttutorials\tavailable\tas\tJupyter\tnotebooks\tin\tthe\tonline\tsupplemental\tmaterial.\tFor\tthose\twho\tare\ttruly\tallergic to\tmathematics,\tyou\tshould\tstill\tgo\tthrough\tthis\tchapter\tand\tsimply\tskip\tthe\tequations;\thopefully,\tthe\ttext\twill\tbe\tsufficient\tto\thelp you\tunderstand\tmost\tof\tthe\tconcepts.\n\nLinear\tRegression In\tChapter\t1,\twe\tlooked\tat\ta\tsimple\tregression\tmodel\tof\tlife\tsatisfaction:\tlife_satisfaction\t=\tθ0\t+\tθ1\t× GDP_per_capita.\n\nThis\tmodel\tis\tjust\ta\tlinear\tfunction\tof\tthe\tinput\tfeature\tGDP_per_capita.\tθ0\tand\tθ1\tare\tthe\tmodel’s parameters.\n\nMore\tgenerally,\ta\tlinear\tmodel\tmakes\ta\tprediction\tby\tsimply\tcomputing\ta\tweighted\tsum\tof\tthe\tinput features,\tplus\ta\tconstant\tcalled\tthe\tbias\tterm\t(also\tcalled\tthe\tintercept\tterm),\tas\tshown\tin\tEquation\t4-1.\n\nEquation\t4-1.\tLinear\tRegression\tmodel\tprediction\n\nŷ\tis\tthe\tpredicted\tvalue.\n\nn\tis\tthe\tnumber\tof\tfeatures.\n\nxi\tis\tthe\tith\tfeature\tvalue.\n\nθj\tis\tthe\tjth\tmodel\tparameter\t(including\tthe\tbias\tterm\tθ0\tand\tthe\tfeature\tweights\tθ1,\tθ2,\t,\t θn).\n\nThis\tcan\tbe\twritten\tmuch\tmore\tconcisely\tusing\ta\tvectorized\tform,\tas\tshown\tin\tEquation\t4-2.\n\nEquation\t4-2.\tLinear\tRegression\tmodel\tprediction\t(vectorized\tform)\n\nθ\tis\tthe\tmodel’s\tparameter\tvector,\tcontaining\tthe\tbias\tterm\tθ0\tand\tthe\tfeature\tweights\tθ1\tto\tθn.\n\nθT\tis\tthe\ttranspose\tof\tθ\t(a\trow\tvector\tinstead\tof\ta\tcolumn\tvector).\n\nx\tis\tthe\tinstance’s\tfeature\tvector,\tcontaining\tx0\tto\txn,\twith\tx0\talways\tequal\tto\t1.\n\nθT\t·\tx\tis\tthe\tdot\tproduct\tof\tθT\tand\tx.\n\nhθ\tis\tthe\thypothesis\tfunction,\tusing\tthe\tmodel\tparameters\tθ.\n\nOkay,\tthat’s\tthe\tLinear\tRegression\tmodel,\tso\tnow\thow\tdo\twe\ttrain\tit?\tWell,\trecall\tthat\ttraining\ta\tmodel means\tsetting\tits\tparameters\tso\tthat\tthe\tmodel\tbest\tfits\tthe\ttraining\tset.\tFor\tthis\tpurpose,\twe\tfirst\tneed\ta measure\tof\thow\twell\t(or\tpoorly)\tthe\tmodel\tfits\tthe\ttraining\tdata.\tIn\tChapter\t2\twe\tsaw\tthat\tthe\tmost common\tperformance\tmeasure\tof\ta\tregression\tmodel\tis\tthe\tRoot\tMean\tSquare\tError\t(RMSE)\t(Equation 2-1).\tTherefore,\tto\ttrain\ta\tLinear\tRegression\tmodel,\tyou\tneed\tto\tfind\tthe\tvalue\tof\tθ\tthat\tminimizes\tthe",
      "page_number": 112
    },
    {
      "number": 4,
      "title": "Training\tModels",
      "start_page": 143,
      "end_page": 190,
      "detection_method": "regex_chapter_title",
      "content": "RMSE.\tIn\tpractice,\tit\tis\tsimpler\tto\tminimize\tthe\tMean\tSquare\tError\t(MSE)\tthan\tthe\tRMSE,\tand\tit\tleads to\tthe\tsame\tresult\t(because\tthe\tvalue\tthat\tminimizes\ta\tfunction\talso\tminimizes\tits\tsquare\troot).1\n\nThe\tMSE\tof\ta\tLinear\tRegression\thypothesis\thθ\ton\ta\ttraining\tset\tX\tis\tcalculated\tusing\tEquation\t4-3.\n\nEquation\t4-3.\tMSE\tcost\tfunction\tfor\ta\tLinear\tRegression\tmodel\n\nMost\tof\tthese\tnotations\twere\tpresented\tin\tChapter\t2\t(see\t“Notations”).\tThe\tonly\tdifference\tis\tthat\twe write\thθ\tinstead\tof\tjust\th\tin\torder\tto\tmake\tit\tclear\tthat\tthe\tmodel\tis\tparametrized\tby\tthe\tvector\tθ.\tTo simplify\tnotations,\twe\twill\tjust\twrite\tMSE(θ)\tinstead\tof\tMSE(X,\thθ).\n\nThe\tNormal\tEquation To\tfind\tthe\tvalue\tof\tθ\tthat\tminimizes\tthe\tcost\tfunction,\tthere\tis\ta\tclosed-form\tsolution\t—\tin\tother\twords, a\tmathematical\tequation\tthat\tgives\tthe\tresult\tdirectly.\tThis\tis\tcalled\tthe\tNormal\tEquation\t(Equation\t4-4).2\n\nEquation\t4-4.\tNormal\tEquation\n\nis\tthe\tvalue\tof\n\nthat\tminimizes\tthe\tcost\tfunction.\n\ny\tis\tthe\tvector\tof\ttarget\tvalues\tcontaining\ty(1)\tto\ty(m).\n\nLet’s\tgenerate\tsome\tlinear-looking\tdata\tto\ttest\tthis\tequation\ton\t(Figure\t4-1):\n\nimport\tnumpy\tas\tnp\n\nX\t=\t2\t*\tnp.random.rand(100,\t1) y\t=\t4\t+\t3\t*\tX\t+\tnp.random.randn(100,\t1)\n\nFigure\t4-1.\tRandomly\tgenerated\tlinear\tdataset\n\nNow\tlet’s\tcompute\t Algebra\tmodule\t(np.linalg)\tto\tcompute\tthe\tinverse\tof\ta\tmatrix,\tand\tthe\tdot()\tmethod\tfor\tmatrix multiplication:\n\nusing\tthe\tNormal\tEquation.\tWe\twill\tuse\tthe\tinv()\tfunction\tfrom\tNumPy’s\tLinear\n\nX_b\t=\tnp.c_[np.ones((100,\t1)),\tX]\t\t#\tadd\tx0\t=\t1\tto\teach\tinstance theta_best\t=\tnp.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n\nThe\tactual\tfunction\tthat\twe\tused\tto\tgenerate\tthe\tdata\tis\ty\t=\t4\t+\t3x0\t+\tGaussian\tnoise.\tLet’s\tsee\twhat\tthe equation\tfound:\n\n>>>\ttheta_best array([[\t4.21509616], \t\t\t\t\t\t\t[\t2.77011339]])\n\nWe\twould\thave\thoped\tfor\tθ0\t=\t4\tand\tθ1\t=\t3\tinstead\tof\tθ0\t=\t4.215\tand\tθ1\t=\t2.770.\tClose\tenough,\tbut\tthe noise\tmade\tit\timpossible\tto\trecover\tthe\texact\tparameters\tof\tthe\toriginal\tfunction.\n\nNow\tyou\tcan\tmake\tpredictions\tusing\t :\n\n>>>\tX_new\t=\tnp.array([[0],\t[2]]) >>>\tX_new_b\t=\tnp.c_[np.ones((2,\t1)),\tX_new]\t#\tadd\tx0\t=\t1\tto\teach\tinstance >>>\ty_predict\t=\tX_new_b.dot(theta_best) >>>\ty_predict array([[\t4.21509616], \t\t\t\t\t\t\t[\t9.75532293]])\n\nLet’s\tplot\tthis\tmodel’s\tpredictions\t(Figure\t4-2):\n\nplt.plot(X_new,\ty_predict,\t\"r-\") plt.plot(X,\ty,\t\"b.\") plt.axis([0,\t2,\t0,\t15]) plt.show()\n\nFigure\t4-2.\tLinear\tRegression\tmodel\tpredictions\n\nThe\tequivalent\tcode\tusing\tScikit-Learn\tlooks\tlike\tthis:3\n\n>>>\tfrom\tsklearn.linear_model\timport\tLinearRegression >>>\tlin_reg\t=\tLinearRegression() >>>\tlin_reg.fit(X,\ty) >>>\tlin_reg.intercept_,\tlin_reg.coef_ (array([\t4.21509616]),\tarray([[\t2.77011339]])) >>>\tlin_reg.predict(X_new)\n\narray([[\t4.21509616], \t\t\t\t\t\t\t[\t9.75532293]])\n\nComputational\tComplexity The\tNormal\tEquation\tcomputes\tthe\tinverse\tof\tXT\t·\tX,\twhich\tis\tan\tn\t×\tn\tmatrix\t(where\tn\tis\tthe\tnumber\tof features).\tThe\tcomputational\tcomplexity\tof\tinverting\tsuch\ta\tmatrix\tis\ttypically\tabout\tO(n2.4)\tto\tO(n3) (depending\ton\tthe\timplementation).\tIn\tother\twords,\tif\tyou\tdouble\tthe\tnumber\tof\tfeatures,\tyou\tmultiply\tthe computation\ttime\tby\troughly\t22.4\t=\t5.3\tto\t23\t=\t8.\n\nWARNING\n\nThe\tNormal\tEquation\tgets\tvery\tslow\twhen\tthe\tnumber\tof\tfeatures\tgrows\tlarge\t(e.g.,\t100,000).\n\nOn\tthe\tpositive\tside,\tthis\tequation\tis\tlinear\twith\tregards\tto\tthe\tnumber\tof\tinstances\tin\tthe\ttraining\tset\t(it\tis O(m)),\tso\tit\thandles\tlarge\ttraining\tsets\tefficiently,\tprovided\tthey\tcan\tfit\tin\tmemory.\n\nAlso,\tonce\tyou\thave\ttrained\tyour\tLinear\tRegression\tmodel\t(using\tthe\tNormal\tEquation\tor\tany\tother algorithm),\tpredictions\tare\tvery\tfast:\tthe\tcomputational\tcomplexity\tis\tlinear\twith\tregards\tto\tboth\tthe number\tof\tinstances\tyou\twant\tto\tmake\tpredictions\ton\tand\tthe\tnumber\tof\tfeatures.\tIn\tother\twords,\tmaking predictions\ton\ttwice\tas\tmany\tinstances\t(or\ttwice\tas\tmany\tfeatures)\twill\tjust\ttake\troughly\ttwice\tas\tmuch time.\n\nNow\twe\twill\tlook\tat\tvery\tdifferent\tways\tto\ttrain\ta\tLinear\tRegression\tmodel,\tbetter\tsuited\tfor\tcases where\tthere\tare\ta\tlarge\tnumber\tof\tfeatures,\tor\ttoo\tmany\ttraining\tinstances\tto\tfit\tin\tmemory.\n\nGradient\tDescent Gradient\tDescent\tis\ta\tvery\tgeneric\toptimization\talgorithm\tcapable\tof\tfinding\toptimal\tsolutions\tto\ta\twide range\tof\tproblems.\tThe\tgeneral\tidea\tof\tGradient\tDescent\tis\tto\ttweak\tparameters\titeratively\tin\torder\tto minimize\ta\tcost\tfunction.\n\nSuppose\tyou\tare\tlost\tin\tthe\tmountains\tin\ta\tdense\tfog;\tyou\tcan\tonly\tfeel\tthe\tslope\tof\tthe\tground\tbelow\tyour feet.\tA\tgood\tstrategy\tto\tget\tto\tthe\tbottom\tof\tthe\tvalley\tquickly\tis\tto\tgo\tdownhill\tin\tthe\tdirection\tof\tthe steepest\tslope.\tThis\tis\texactly\twhat\tGradient\tDescent\tdoes:\tit\tmeasures\tthe\tlocal\tgradient\tof\tthe\terror function\twith\tregards\tto\tthe\tparameter\tvector\tθ,\tand\tit\tgoes\tin\tthe\tdirection\tof\tdescending\tgradient.\tOnce the\tgradient\tis\tzero,\tyou\thave\treached\ta\tminimum!\n\nConcretely,\tyou\tstart\tby\tfilling\tθ\twith\trandom\tvalues\t(this\tis\tcalled\trandom\tinitialization),\tand\tthen\tyou improve\tit\tgradually,\ttaking\tone\tbaby\tstep\tat\ta\ttime,\teach\tstep\tattempting\tto\tdecrease\tthe\tcost\tfunction (e.g.,\tthe\tMSE),\tuntil\tthe\talgorithm\tconverges\tto\ta\tminimum\t(see\tFigure\t4-3).\n\nFigure\t4-3.\tGradient\tDescent\n\nAn\timportant\tparameter\tin\tGradient\tDescent\tis\tthe\tsize\tof\tthe\tsteps,\tdetermined\tby\tthe\tlearning\trate hyperparameter.\tIf\tthe\tlearning\trate\tis\ttoo\tsmall,\tthen\tthe\talgorithm\twill\thave\tto\tgo\tthrough\tmany\titerations to\tconverge,\twhich\twill\ttake\ta\tlong\ttime\t(see\tFigure\t4-4).\n\nFigure\t4-4.\tLearning\trate\ttoo\tsmall\n\nOn\tthe\tother\thand,\tif\tthe\tlearning\trate\tis\ttoo\thigh,\tyou\tmight\tjump\tacross\tthe\tvalley\tand\tend\tup\ton\tthe\tother side,\tpossibly\teven\thigher\tup\tthan\tyou\twere\tbefore.\tThis\tmight\tmake\tthe\talgorithm\tdiverge,\twith\tlarger and\tlarger\tvalues,\tfailing\tto\tfind\ta\tgood\tsolution\t(see\tFigure\t4-5).\n\nFigure\t4-5.\tLearning\trate\ttoo\tlarge\n\nFinally,\tnot\tall\tcost\tfunctions\tlook\tlike\tnice\tregular\tbowls.\tThere\tmay\tbe\tholes,\tridges,\tplateaus,\tand\tall sorts\tof\tirregular\tterrains,\tmaking\tconvergence\tto\tthe\tminimum\tvery\tdifficult.\tFigure\t4-6\tshows\tthe\ttwo main\tchallenges\twith\tGradient\tDescent:\tif\tthe\trandom\tinitialization\tstarts\tthe\talgorithm\ton\tthe\tleft,\tthen\tit will\tconverge\tto\ta\tlocal\tminimum,\twhich\tis\tnot\tas\tgood\tas\tthe\tglobal\tminimum.\tIf\tit\tstarts\ton\tthe\tright, then\tit\twill\ttake\ta\tvery\tlong\ttime\tto\tcross\tthe\tplateau,\tand\tif\tyou\tstop\ttoo\tearly\tyou\twill\tnever\treach\tthe global\tminimum.\n\nFigure\t4-6.\tGradient\tDescent\tpitfalls\n\nFortunately,\tthe\tMSE\tcost\tfunction\tfor\ta\tLinear\tRegression\tmodel\thappens\tto\tbe\ta\tconvex\tfunction,\twhich means\tthat\tif\tyou\tpick\tany\ttwo\tpoints\ton\tthe\tcurve,\tthe\tline\tsegment\tjoining\tthem\tnever\tcrosses\tthe\tcurve. This\timplies\tthat\tthere\tare\tno\tlocal\tminima,\tjust\tone\tglobal\tminimum.\tIt\tis\talso\ta\tcontinuous\tfunction\twith a\tslope\tthat\tnever\tchanges\tabruptly.4\tThese\ttwo\tfacts\thave\ta\tgreat\tconsequence:\tGradient\tDescent\tis guaranteed\tto\tapproach\tarbitrarily\tclose\tthe\tglobal\tminimum\t(if\tyou\twait\tlong\tenough\tand\tif\tthe\tlearning rate\tis\tnot\ttoo\thigh).\n\nIn\tfact,\tthe\tcost\tfunction\thas\tthe\tshape\tof\ta\tbowl,\tbut\tit\tcan\tbe\tan\telongated\tbowl\tif\tthe\tfeatures\thave\tvery different\tscales.\tFigure\t4-7\tshows\tGradient\tDescent\ton\ta\ttraining\tset\twhere\tfeatures\t1\tand\t2\thave\tthe same\tscale\t(on\tthe\tleft),\tand\ton\ta\ttraining\tset\twhere\tfeature\t1\thas\tmuch\tsmaller\tvalues\tthan\tfeature\t2\t(on the\tright).5\n\nFigure\t4-7.\tGradient\tDescent\twith\tand\twithout\tfeature\tscaling\n\nAs\tyou\tcan\tsee,\ton\tthe\tleft\tthe\tGradient\tDescent\talgorithm\tgoes\tstraight\ttoward\tthe\tminimum,\tthereby reaching\tit\tquickly,\twhereas\ton\tthe\tright\tit\tfirst\tgoes\tin\ta\tdirection\talmost\torthogonal\tto\tthe\tdirection\tof the\tglobal\tminimum,\tand\tit\tends\twith\ta\tlong\tmarch\tdown\tan\talmost\tflat\tvalley.\tIt\twill\teventually\treach\tthe minimum,\tbut\tit\twill\ttake\ta\tlong\ttime.\n\nWARNING\n\nWhen\tusing\tGradient\tDescent,\tyou\tshould\tensure\tthat\tall\tfeatures\thave\ta\tsimilar\tscale\t(e.g.,\tusing\tScikit-Learn’s\tStandardScaler class),\tor\telse\tit\twill\ttake\tmuch\tlonger\tto\tconverge.\n\nThis\tdiagram\talso\tillustrates\tthe\tfact\tthat\ttraining\ta\tmodel\tmeans\tsearching\tfor\ta\tcombination\tof\tmodel parameters\tthat\tminimizes\ta\tcost\tfunction\t(over\tthe\ttraining\tset).\tIt\tis\ta\tsearch\tin\tthe\tmodel’s\tparameter space:\tthe\tmore\tparameters\ta\tmodel\thas,\tthe\tmore\tdimensions\tthis\tspace\thas,\tand\tthe\tharder\tthe\tsearch\tis: searching\tfor\ta\tneedle\tin\ta\t300-dimensional\thaystack\tis\tmuch\ttrickier\tthan\tin\tthree\tdimensions. Fortunately,\tsince\tthe\tcost\tfunction\tis\tconvex\tin\tthe\tcase\tof\tLinear\tRegression,\tthe\tneedle\tis\tsimply\tat\tthe bottom\tof\tthe\tbowl.\n\nBatch\tGradient\tDescent To\timplement\tGradient\tDescent,\tyou\tneed\tto\tcompute\tthe\tgradient\tof\tthe\tcost\tfunction\twith\tregards\tto each\tmodel\tparameter\tθj.\tIn\tother\twords,\tyou\tneed\tto\tcalculate\thow\tmuch\tthe\tcost\tfunction\twill\tchange\tif you\tchange\tθj\tjust\ta\tlittle\tbit.\tThis\tis\tcalled\ta\tpartial\tderivative.\tIt\tis\tlike\tasking\t“what\tis\tthe\tslope\tof\tthe mountain\tunder\tmy\tfeet\tif\tI\tface\teast?”\tand\tthen\tasking\tthe\tsame\tquestion\tfacing\tnorth\t(and\tso\ton\tfor\tall other\tdimensions,\tif\tyou\tcan\timagine\ta\tuniverse\twith\tmore\tthan\tthree\tdimensions).\tEquation\t4-5\tcomputes\n\nthe\tpartial\tderivative\tof\tthe\tcost\tfunction\twith\tregards\tto\tparameter\tθj,\tnoted\n\n.\n\nEquation\t4-5.\tPartial\tderivatives\tof\tthe\tcost\tfunction\n\nInstead\tof\tcomputing\tthese\tpartial\tderivatives\tindividually,\tyou\tcan\tuse\tEquation\t4-6\tto\tcompute\tthem\tall in\tone\tgo.\tThe\tgradient\tvector,\tnoted\t θMSE(θ),\tcontains\tall\tthe\tpartial\tderivatives\tof\tthe\tcost\tfunction (one\tfor\teach\tmodel\tparameter).\n\nEquation\t4-6.\tGradient\tvector\tof\tthe\tcost\tfunction\n\nWARNING\n\nNotice\tthat\tthis\tformula\tinvolves\tcalculations\tover\tthe\tfull\ttraining\tset\tX,\tat\teach\tGradient\tDescent\tstep!\tThis\tis\twhy\tthe\talgorithm is\tcalled\tBatch\tGradient\tDescent:\tit\tuses\tthe\twhole\tbatch\tof\ttraining\tdata\tat\tevery\tstep.\tAs\ta\tresult\tit\tis\tterribly\tslow\ton\tvery large\ttraining\tsets\t(but\twe\twill\tsee\tmuch\tfaster\tGradient\tDescent\talgorithms\tshortly).\tHowever,\tGradient\tDescent\tscales\twell with\tthe\tnumber\tof\tfeatures;\ttraining\ta\tLinear\tRegression\tmodel\twhen\tthere\tare\thundreds\tof\tthousands\tof\tfeatures\tis\tmuch\tfaster using\tGradient\tDescent\tthan\tusing\tthe\tNormal\tEquation.\n\nOnce\tyou\thave\tthe\tgradient\tvector,\twhich\tpoints\tuphill,\tjust\tgo\tin\tthe\topposite\tdirection\tto\tgo\tdownhill. This\tmeans\tsubtracting\t θMSE(θ)\tfrom\tθ.\tThis\tis\twhere\tthe\tlearning\trate\tη\tcomes\tinto\tplay:6\tmultiply\tthe gradient\tvector\tby\tη\tto\tdetermine\tthe\tsize\tof\tthe\tdownhill\tstep\t(Equation\t4-7).\n\nEquation\t4-7.\tGradient\tDescent\tstep\n\nLet’s\tlook\tat\ta\tquick\timplementation\tof\tthis\talgorithm:\n\neta\t=\t0.1\t\t#\tlearning\trate n_iterations\t=\t1000 m\t=\t100\n\ntheta\t=\tnp.random.randn(2,1)\t\t#\trandom\tinitialization\n\nfor\titeration\tin\trange(n_iterations): \t\t\t\tgradients\t=\t2/m\t*\tX_b.T.dot(X_b.dot(theta)\t-\ty) \t\t\t\ttheta\t=\ttheta\t-\teta\t*\tgradients\n\nThat\twasn’t\ttoo\thard!\tLet’s\tlook\tat\tthe\tresulting\ttheta:\n\n>>>\ttheta array([[\t4.21509616], \t\t\t\t\t\t\t[\t2.77011339]])\n\nHey,\tthat’s\texactly\twhat\tthe\tNormal\tEquation\tfound!\tGradient\tDescent\tworked\tperfectly.\tBut\twhat\tif\tyou had\tused\ta\tdifferent\tlearning\trate\teta?\tFigure\t4-8\tshows\tthe\tfirst\t10\tsteps\tof\tGradient\tDescent\tusing\tthree different\tlearning\trates\t(the\tdashed\tline\trepresents\tthe\tstarting\tpoint).\n\nFigure\t4-8.\tGradient\tDescent\twith\tvarious\tlearning\trates\n\nOn\tthe\tleft,\tthe\tlearning\trate\tis\ttoo\tlow:\tthe\talgorithm\twill\teventually\treach\tthe\tsolution,\tbut\tit\twill\ttake\ta long\ttime.\tIn\tthe\tmiddle,\tthe\tlearning\trate\tlooks\tpretty\tgood:\tin\tjust\ta\tfew\titerations,\tit\thas\talready converged\tto\tthe\tsolution.\tOn\tthe\tright,\tthe\tlearning\trate\tis\ttoo\thigh:\tthe\talgorithm\tdiverges,\tjumping\tall over\tthe\tplace\tand\tactually\tgetting\tfurther\tand\tfurther\taway\tfrom\tthe\tsolution\tat\tevery\tstep.\n\nTo\tfind\ta\tgood\tlearning\trate,\tyou\tcan\tuse\tgrid\tsearch\t(see\tChapter\t2).\tHowever,\tyou\tmay\twant\tto\tlimit\tthe number\tof\titerations\tso\tthat\tgrid\tsearch\tcan\teliminate\tmodels\tthat\ttake\ttoo\tlong\tto\tconverge.\n\nYou\tmay\twonder\thow\tto\tset\tthe\tnumber\tof\titerations.\tIf\tit\tis\ttoo\tlow,\tyou\twill\tstill\tbe\tfar\taway\tfrom\tthe optimal\tsolution\twhen\tthe\talgorithm\tstops,\tbut\tif\tit\tis\ttoo\thigh,\tyou\twill\twaste\ttime\twhile\tthe\tmodel parameters\tdo\tnot\tchange\tanymore.\tA\tsimple\tsolution\tis\tto\tset\ta\tvery\tlarge\tnumber\tof\titerations\tbut\tto interrupt\tthe\talgorithm\twhen\tthe\tgradient\tvector\tbecomes\ttiny\t—\tthat\tis,\twhen\tits\tnorm\tbecomes\tsmaller than\ta\ttiny\tnumber\tϵ\t(called\tthe\ttolerance)\t—\tbecause\tthis\thappens\twhen\tGradient\tDescent\thas\t(almost)\n\nreached\tthe\tminimum.\n\nCONVERGENCE\tRATE\n\nWhen\tthe\tcost\tfunction\tis\tconvex\tand\tits\tslope\tdoes\tnot\tchange\tabruptly\t(as\tis\tthe\tcase\tfor\tthe\tMSE\tcost\tfunction),\tit\tcan\tbe\tshown\tthat\n\nBatch\tGradient\tDescent\twith\ta\tfixed\tlearning\trate\thas\ta\tconvergence\trate\tof\t tolerance\tϵ\tby\t10\t(to\thave\ta\tmore\tprecise\tsolution),\tthen\tthe\talgorithm\twill\thave\tto\trun\tabout\t10\ttimes\tmore\titerations.\n\n.\tIn\tother\twords,\tif\tyou\tdivide\tthe\n\nStochastic\tGradient\tDescent The\tmain\tproblem\twith\tBatch\tGradient\tDescent\tis\tthe\tfact\tthat\tit\tuses\tthe\twhole\ttraining\tset\tto\tcompute the\tgradients\tat\tevery\tstep,\twhich\tmakes\tit\tvery\tslow\twhen\tthe\ttraining\tset\tis\tlarge.\tAt\tthe\topposite extreme,\tStochastic\tGradient\tDescent\tjust\tpicks\ta\trandom\tinstance\tin\tthe\ttraining\tset\tat\tevery\tstep\tand computes\tthe\tgradients\tbased\tonly\ton\tthat\tsingle\tinstance.\tObviously\tthis\tmakes\tthe\talgorithm\tmuch\tfaster since\tit\thas\tvery\tlittle\tdata\tto\tmanipulate\tat\tevery\titeration.\tIt\talso\tmakes\tit\tpossible\tto\ttrain\ton\thuge training\tsets,\tsince\tonly\tone\tinstance\tneeds\tto\tbe\tin\tmemory\tat\teach\titeration\t(SGD\tcan\tbe\timplemented\tas an\tout-of-core\talgorithm.7)\n\nOn\tthe\tother\thand,\tdue\tto\tits\tstochastic\t(i.e.,\trandom)\tnature,\tthis\talgorithm\tis\tmuch\tless\tregular\tthan\tBatch Gradient\tDescent:\tinstead\tof\tgently\tdecreasing\tuntil\tit\treaches\tthe\tminimum,\tthe\tcost\tfunction\twill\tbounce up\tand\tdown,\tdecreasing\tonly\ton\taverage.\tOver\ttime\tit\twill\tend\tup\tvery\tclose\tto\tthe\tminimum,\tbut\tonce\tit gets\tthere\tit\twill\tcontinue\tto\tbounce\taround,\tnever\tsettling\tdown\t(see\tFigure\t4-9).\tSo\tonce\tthe\talgorithm stops,\tthe\tfinal\tparameter\tvalues\tare\tgood,\tbut\tnot\toptimal.\n\nFigure\t4-9.\tStochastic\tGradient\tDescent\n\nWhen\tthe\tcost\tfunction\tis\tvery\tirregular\t(as\tin\tFigure\t4-6),\tthis\tcan\tactually\thelp\tthe\talgorithm\tjump\tout\tof local\tminima,\tso\tStochastic\tGradient\tDescent\thas\ta\tbetter\tchance\tof\tfinding\tthe\tglobal\tminimum\tthan Batch\tGradient\tDescent\tdoes.\n\nTherefore\trandomness\tis\tgood\tto\tescape\tfrom\tlocal\toptima,\tbut\tbad\tbecause\tit\tmeans\tthat\tthe\talgorithm can\tnever\tsettle\tat\tthe\tminimum.\tOne\tsolution\tto\tthis\tdilemma\tis\tto\tgradually\treduce\tthe\tlearning\trate.\tThe steps\tstart\tout\tlarge\t(which\thelps\tmake\tquick\tprogress\tand\tescape\tlocal\tminima),\tthen\tget\tsmaller\tand smaller,\tallowing\tthe\talgorithm\tto\tsettle\tat\tthe\tglobal\tminimum.\tThis\tprocess\tis\tcalled\tsimulated\n\nannealing,\tbecause\tit\tresembles\tthe\tprocess\tof\tannealing\tin\tmetallurgy\twhere\tmolten\tmetal\tis\tslowly cooled\tdown.\tThe\tfunction\tthat\tdetermines\tthe\tlearning\trate\tat\teach\titeration\tis\tcalled\tthe\tlearning schedule.\tIf\tthe\tlearning\trate\tis\treduced\ttoo\tquickly,\tyou\tmay\tget\tstuck\tin\ta\tlocal\tminimum,\tor\teven\tend\tup frozen\thalfway\tto\tthe\tminimum.\tIf\tthe\tlearning\trate\tis\treduced\ttoo\tslowly,\tyou\tmay\tjump\taround\tthe minimum\tfor\ta\tlong\ttime\tand\tend\tup\twith\ta\tsuboptimal\tsolution\tif\tyou\thalt\ttraining\ttoo\tearly.\n\nThis\tcode\timplements\tStochastic\tGradient\tDescent\tusing\ta\tsimple\tlearning\tschedule:\n\nn_epochs\t=\t50 t0,\tt1\t=\t5,\t50\t\t#\tlearning\tschedule\thyperparameters\n\ndef\tlearning_schedule(t): \t\t\t\treturn\tt0\t/\t(t\t+\tt1)\n\ntheta\t=\tnp.random.randn(2,1)\t\t#\trandom\tinitialization\n\nfor\tepoch\tin\trange(n_epochs): \t\t\t\tfor\ti\tin\trange(m): \t\t\t\t\t\t\t\trandom_index\t=\tnp.random.randint(m) \t\t\t\t\t\t\t\txi\t=\tX_b[random_index:random_index+1] \t\t\t\t\t\t\t\tyi\t=\ty[random_index:random_index+1] \t\t\t\t\t\t\t\tgradients\t=\t2\t*\txi.T.dot(xi.dot(theta)\t-\tyi) \t\t\t\t\t\t\t\teta\t=\tlearning_schedule(epoch\t*\tm\t+\ti) \t\t\t\t\t\t\t\ttheta\t=\ttheta\t-\teta\t*\tgradients\n\nBy\tconvention\twe\titerate\tby\trounds\tof\tm\titerations;\teach\tround\tis\tcalled\tan\tepoch.\tWhile\tthe\tBatch Gradient\tDescent\tcode\titerated\t1,000\ttimes\tthrough\tthe\twhole\ttraining\tset,\tthis\tcode\tgoes\tthrough\tthe training\tset\tonly\t50\ttimes\tand\treaches\ta\tfairly\tgood\tsolution:\n\n>>>\ttheta array([[\t4.21076011], \t\t\t\t\t\t[\t2.74856079]])\n\nFigure\t4-10\tshows\tthe\tfirst\t10\tsteps\tof\ttraining\t(notice\thow\tirregular\tthe\tsteps\tare).\n\nFigure\t4-10.\tStochastic\tGradient\tDescent\tfirst\t10\tsteps\n\nNote\tthat\tsince\tinstances\tare\tpicked\trandomly,\tsome\tinstances\tmay\tbe\tpicked\tseveral\ttimes\tper\tepoch while\tothers\tmay\tnot\tbe\tpicked\tat\tall.\tIf\tyou\twant\tto\tbe\tsure\tthat\tthe\talgorithm\tgoes\tthrough\tevery\tinstance at\teach\tepoch,\tanother\tapproach\tis\tto\tshuffle\tthe\ttraining\tset,\tthen\tgo\tthrough\tit\tinstance\tby\tinstance,\tthen shuffle\tit\tagain,\tand\tso\ton.\tHowever,\tthis\tgenerally\tconverges\tmore\tslowly.\n\nTo\tperform\tLinear\tRegression\tusing\tSGD\twith\tScikit-Learn,\tyou\tcan\tuse\tthe\tSGDRegressor\tclass,\twhich defaults\tto\toptimizing\tthe\tsquared\terror\tcost\tfunction.\tThe\tfollowing\tcode\truns\t50\tepochs,\tstarting\twith\ta learning\trate\tof\t0.1\t(eta0=0.1),\tusing\tthe\tdefault\tlearning\tschedule\t(different\tfrom\tthe\tpreceding\tone), and\tit\tdoes\tnot\tuse\tany\tregularization\t(penalty=None;\tmore\tdetails\ton\tthis\tshortly):\n\nfrom\tsklearn.linear_model\timport\tSGDRegressor sgd_reg\t=\tSGDRegressor(n_iter=50,\tpenalty=None,\teta0=0.1) sgd_reg.fit(X,\ty.ravel())\n\nOnce\tagain,\tyou\tfind\ta\tsolution\tvery\tclose\tto\tthe\tone\treturned\tby\tthe\tNormal\tEquation:\n\n>>>\tsgd_reg.intercept_,\tsgd_reg.coef_ (array([\t4.16782089]),\tarray([\t2.72603052]))\n\nMini-batch\tGradient\tDescent The\tlast\tGradient\tDescent\talgorithm\twe\twill\tlook\tat\tis\tcalled\tMini-batch\tGradient\tDescent.\tIt\tis\tquite simple\tto\tunderstand\tonce\tyou\tknow\tBatch\tand\tStochastic\tGradient\tDescent:\tat\teach\tstep,\tinstead\tof computing\tthe\tgradients\tbased\ton\tthe\tfull\ttraining\tset\t(as\tin\tBatch\tGD)\tor\tbased\ton\tjust\tone\tinstance\t(as\tin Stochastic\tGD),\tMini-batch\tGD\tcomputes\tthe\tgradients\ton\tsmall\trandom\tsets\tof\tinstances\tcalled\tmini- batches.\tThe\tmain\tadvantage\tof\tMini-batch\tGD\tover\tStochastic\tGD\tis\tthat\tyou\tcan\tget\ta\tperformance boost\tfrom\thardware\toptimization\tof\tmatrix\toperations,\tespecially\twhen\tusing\tGPUs.\n\nThe\talgorithm’s\tprogress\tin\tparameter\tspace\tis\tless\terratic\tthan\twith\tSGD,\tespecially\twith\tfairly\tlarge mini-batches.\tAs\ta\tresult,\tMini-batch\tGD\twill\tend\tup\twalking\taround\ta\tbit\tcloser\tto\tthe\tminimum\tthan SGD.\tBut,\ton\tthe\tother\thand,\tit\tmay\tbe\tharder\tfor\tit\tto\tescape\tfrom\tlocal\tminima\t(in\tthe\tcase\tof\tproblems that\tsuffer\tfrom\tlocal\tminima,\tunlike\tLinear\tRegression\tas\twe\tsaw\tearlier).\tFigure\t4-11\tshows\tthe\tpaths taken\tby\tthe\tthree\tGradient\tDescent\talgorithms\tin\tparameter\tspace\tduring\ttraining.\tThey\tall\tend\tup\tnear the\tminimum,\tbut\tBatch\tGD’s\tpath\tactually\tstops\tat\tthe\tminimum,\twhile\tboth\tStochastic\tGD\tand\tMini- batch\tGD\tcontinue\tto\twalk\taround.\tHowever,\tdon’t\tforget\tthat\tBatch\tGD\ttakes\ta\tlot\tof\ttime\tto\ttake\teach step,\tand\tStochastic\tGD\tand\tMini-batch\tGD\twould\talso\treach\tthe\tminimum\tif\tyou\tused\ta\tgood\tlearning schedule.\n\nFigure\t4-11.\tGradient\tDescent\tpaths\tin\tparameter\tspace\n\nLet’s\tcompare\tthe\talgorithms\twe’ve\tdiscussed\tso\tfar\tfor\tLinear\tRegression8\t(recall\tthat\tm\tis\tthe\tnumber of\ttraining\tinstances\tand\tn\tis\tthe\tnumber\tof\tfeatures);\tsee\tTable\t4-1.\n\nTable\t4-1.\tComparison\tof\talgorithms\tfor\tLinear\tRegression\n\nAlgorithm\n\nLarge\tm Out-of-core\tsupport Large\tn Hyperparams Scaling\trequired Scikit-Learn\n\nNormal\tEquation Fast\n\nNo\n\nSlow\n\n0\n\nNo\n\nLinearRegression\n\nBatch\tGD\n\nSlow\n\nNo\n\nFast\n\n2\n\nYes\n\nn/a\n\nStochastic\tGD\n\nFast\n\nYes\n\nFast\n\n≥2\n\nYes\n\nSGDRegressor\n\nMini-batch\tGD Fast\n\nYes\n\nFast\n\n≥2\n\nYes\n\nn/a\n\nNOTE\n\nThere\tis\talmost\tno\tdifference\tafter\ttraining:\tall\tthese\talgorithms\tend\tup\twith\tvery\tsimilar\tmodels\tand\tmake\tpredictions\tin\texactly the\tsame\tway.\n\nPolynomial\tRegression What\tif\tyour\tdata\tis\tactually\tmore\tcomplex\tthan\ta\tsimple\tstraight\tline?\tSurprisingly,\tyou\tcan\tactually\tuse a\tlinear\tmodel\tto\tfit\tnonlinear\tdata.\tA\tsimple\tway\tto\tdo\tthis\tis\tto\tadd\tpowers\tof\teach\tfeature\tas\tnew features,\tthen\ttrain\ta\tlinear\tmodel\ton\tthis\textended\tset\tof\tfeatures.\tThis\ttechnique\tis\tcalled\tPolynomial Regression. Let’s\tlook\tat\tan\texample.\tFirst,\tlet’s\tgenerate\tsome\tnonlinear\tdata,\tbased\ton\ta\tsimple\tquadratic\tequation9 (plus\tsome\tnoise;\tsee\tFigure\t4-12):\n\nm\t=\t100 X\t=\t6\t*\tnp.random.rand(m,\t1)\t-\t3 y\t=\t0.5\t*\tX**2\t+\tX\t+\t2\t+\tnp.random.randn(m,\t1)\n\nFigure\t4-12.\tGenerated\tnonlinear\tand\tnoisy\tdataset\n\nClearly,\ta\tstraight\tline\twill\tnever\tfit\tthis\tdata\tproperly.\tSo\tlet’s\tuse\tScikit-Learn’s\tPolynomialFeatures class\tto\ttransform\tour\ttraining\tdata,\tadding\tthe\tsquare\t(2nd-degree\tpolynomial)\tof\teach\tfeature\tin\tthe training\tset\tas\tnew\tfeatures\t(in\tthis\tcase\tthere\tis\tjust\tone\tfeature):\n\n>>>\tfrom\tsklearn.preprocessing\timport\tPolynomialFeatures >>>\tpoly_features\t=\tPolynomialFeatures(degree=2,\tinclude_bias=False) >>>\tX_poly\t=\tpoly_features.fit_transform(X) >>>\tX[0] array([-0.75275929]) >>>\tX_poly[0] array([-0.75275929,\t\t0.56664654])\n\nX_poly\tnow\tcontains\tthe\toriginal\tfeature\tof\tX\tplus\tthe\tsquare\tof\tthis\tfeature.\tNow\tyou\tcan\tfit\ta LinearRegression\tmodel\tto\tthis\textended\ttraining\tdata\t(Figure\t4-13):\n\n>>>\tlin_reg\t=\tLinearRegression() >>>\tlin_reg.fit(X_poly,\ty) >>>\tlin_reg.intercept_,\tlin_reg.coef_ (array([\t1.78134581]),\tarray([[\t0.93366893,\t\t0.56456263]]))\n\nFigure\t4-13.\tPolynomial\tRegression\tmodel\tpredictions\n\nNot\tbad:\tthe\tmodel\testimates\n\nwhen\tin\tfact\tthe\toriginal\tfunction\twas\n\n.\n\nNote\tthat\twhen\tthere\tare\tmultiple\tfeatures,\tPolynomial\tRegression\tis\tcapable\tof\tfinding\trelationships between\tfeatures\t(which\tis\tsomething\ta\tplain\tLinear\tRegression\tmodel\tcannot\tdo).\tThis\tis\tmade\tpossible by\tthe\tfact\tthat\tPolynomialFeatures\talso\tadds\tall\tcombinations\tof\tfeatures\tup\tto\tthe\tgiven\tdegree.\tFor example,\tif\tthere\twere\ttwo\tfeatures\ta\tand\tb,\tPolynomialFeatures\twith\tdegree=3\twould\tnot\tonly\tadd the\tfeatures\ta2,\ta3,\tb2,\tand\tb3,\tbut\talso\tthe\tcombinations\tab,\ta2b,\tand\tab2.\n\nWARNING\n\nPolynomialFeatures(degree=d)\ttransforms\tan\tarray\tcontaining\tn\tfeatures\tinto\tan\tarray\tcontaining\t is\tthe\tfactorial\tof\tn,\tequal\tto\t1\t×\t2\t×\t3\t×\t\t×\t n.\tBeware\tof\tthe\tcombinatorial\texplosion\tof\tthe\tnumber\tof\tfeatures!\n\nfeatures,\twhere\tn!\n\nLearning\tCurves If\tyou\tperform\thigh-degree\tPolynomial\tRegression,\tyou\twill\tlikely\tfit\tthe\ttraining\tdata\tmuch\tbetter\tthan with\tplain\tLinear\tRegression.\tFor\texample,\tFigure\t4-14\tapplies\ta\t300-degree\tpolynomial\tmodel\tto\tthe preceding\ttraining\tdata,\tand\tcompares\tthe\tresult\twith\ta\tpure\tlinear\tmodel\tand\ta\tquadratic\tmodel\t(2nd- degree\tpolynomial).\tNotice\thow\tthe\t300-degree\tpolynomial\tmodel\twiggles\taround\tto\tget\tas\tclose\tas possible\tto\tthe\ttraining\tinstances.\n\nFigure\t4-14.\tHigh-degree\tPolynomial\tRegression\n\nOf\tcourse,\tthis\thigh-degree\tPolynomial\tRegression\tmodel\tis\tseverely\toverfitting\tthe\ttraining\tdata,\twhile the\tlinear\tmodel\tis\tunderfitting\tit.\tThe\tmodel\tthat\twill\tgeneralize\tbest\tin\tthis\tcase\tis\tthe\tquadratic\tmodel. It\tmakes\tsense\tsince\tthe\tdata\twas\tgenerated\tusing\ta\tquadratic\tmodel,\tbut\tin\tgeneral\tyou\twon’t\tknow\twhat function\tgenerated\tthe\tdata,\tso\thow\tcan\tyou\tdecide\thow\tcomplex\tyour\tmodel\tshould\tbe?\tHow\tcan\tyou\ttell that\tyour\tmodel\tis\toverfitting\tor\tunderfitting\tthe\tdata?\n\nIn\tChapter\t2\tyou\tused\tcross-validation\tto\tget\tan\testimate\tof\ta\tmodel’s\tgeneralization\tperformance.\tIf\ta model\tperforms\twell\ton\tthe\ttraining\tdata\tbut\tgeneralizes\tpoorly\taccording\tto\tthe\tcross-validation\tmetrics, then\tyour\tmodel\tis\toverfitting.\tIf\tit\tperforms\tpoorly\ton\tboth,\tthen\tit\tis\tunderfitting.\tThis\tis\tone\tway\tto\ttell when\ta\tmodel\tis\ttoo\tsimple\tor\ttoo\tcomplex.\n\nAnother\tway\tis\tto\tlook\tat\tthe\tlearning\tcurves:\tthese\tare\tplots\tof\tthe\tmodel’s\tperformance\ton\tthe\ttraining set\tand\tthe\tvalidation\tset\tas\ta\tfunction\tof\tthe\ttraining\tset\tsize.\tTo\tgenerate\tthe\tplots,\tsimply\ttrain\tthe\tmodel several\ttimes\ton\tdifferent\tsized\tsubsets\tof\tthe\ttraining\tset.\tThe\tfollowing\tcode\tdefines\ta\tfunction\tthat\tplots the\tlearning\tcurves\tof\ta\tmodel\tgiven\tsome\ttraining\tdata:\n\nfrom\tsklearn.metrics\timport\tmean_squared_error from\tsklearn.model_selection\timport\ttrain_test_split\n\ndef\tplot_learning_curves(model,\tX,\ty):\n\nX_train,\tX_val,\ty_train,\ty_val\t=\ttrain_test_split(X,\ty,\ttest_size=0.2) \t\t\t\ttrain_errors,\tval_errors\t=\t[],\t[] \t\t\t\tfor\tm\tin\trange(1,\tlen(X_train)): \t\t\t\t\t\t\t\tmodel.fit(X_train[:m],\ty_train[:m]) \t\t\t\t\t\t\t\ty_train_predict\t=\tmodel.predict(X_train[:m]) \t\t\t\t\t\t\t\ty_val_predict\t=\tmodel.predict(X_val) \t\t\t\t\t\t\t\ttrain_errors.append(mean_squared_error(y_train_predict,\ty_train[:m])) \t\t\t\t\t\t\t\tval_errors.append(mean_squared_error(y_val_predict,\ty_val)) \t\t\t\tplt.plot(np.sqrt(train_errors),\t\"r-+\",\tlinewidth=2,\tlabel=\"train\") \t\t\t\tplt.plot(np.sqrt(val_errors),\t\"b-\",\tlinewidth=3,\tlabel=\"val\")\n\nLet’s\tlook\tat\tthe\tlearning\tcurves\tof\tthe\tplain\tLinear\tRegression\tmodel\t(a\tstraight\tline;\tFigure\t4-15):\n\nlin_reg\t=\tLinearRegression() plot_learning_curves(lin_reg,\tX,\ty)\n\nFigure\t4-15.\tLearning\tcurves\n\nThis\tdeserves\ta\tbit\tof\texplanation.\tFirst,\tlet’s\tlook\tat\tthe\tperformance\ton\tthe\ttraining\tdata:\twhen\tthere\tare just\tone\tor\ttwo\tinstances\tin\tthe\ttraining\tset,\tthe\tmodel\tcan\tfit\tthem\tperfectly,\twhich\tis\twhy\tthe\tcurve\tstarts at\tzero.\tBut\tas\tnew\tinstances\tare\tadded\tto\tthe\ttraining\tset,\tit\tbecomes\timpossible\tfor\tthe\tmodel\tto\tfit\tthe training\tdata\tperfectly,\tboth\tbecause\tthe\tdata\tis\tnoisy\tand\tbecause\tit\tis\tnot\tlinear\tat\tall.\tSo\tthe\terror\ton\tthe training\tdata\tgoes\tup\tuntil\tit\treaches\ta\tplateau,\tat\twhich\tpoint\tadding\tnew\tinstances\tto\tthe\ttraining\tset doesn’t\tmake\tthe\taverage\terror\tmuch\tbetter\tor\tworse.\tNow\tlet’s\tlook\tat\tthe\tperformance\tof\tthe\tmodel\ton the\tvalidation\tdata.\tWhen\tthe\tmodel\tis\ttrained\ton\tvery\tfew\ttraining\tinstances,\tit\tis\tincapable\tof generalizing\tproperly,\twhich\tis\twhy\tthe\tvalidation\terror\tis\tinitially\tquite\tbig.\tThen\tas\tthe\tmodel\tis\tshown more\ttraining\texamples,\tit\tlearns\tand\tthus\tthe\tvalidation\terror\tslowly\tgoes\tdown.\tHowever,\tonce\tagain\ta straight\tline\tcannot\tdo\ta\tgood\tjob\tmodeling\tthe\tdata,\tso\tthe\terror\tends\tup\tat\ta\tplateau,\tvery\tclose\tto\tthe other\tcurve.\n\nThese\tlearning\tcurves\tare\ttypical\tof\tan\tunderfitting\tmodel.\tBoth\tcurves\thave\treached\ta\tplateau;\tthey\tare close\tand\tfairly\thigh.\n\nTIP\n\nIf\tyour\tmodel\tis\tunderfitting\tthe\ttraining\tdata,\tadding\tmore\ttraining\texamples\twill\tnot\thelp.\tYou\tneed\tto\tuse\ta\tmore\tcomplex model\tor\tcome\tup\twith\tbetter\tfeatures.\n\nNow\tlet’s\tlook\tat\tthe\tlearning\tcurves\tof\ta\t10th-degree\tpolynomial\tmodel\ton\tthe\tsame\tdata\t(Figure\t4-16):\n\nfrom\tsklearn.pipeline\timport\tPipeline\n\npolynomial_regression\t=\tPipeline(( \t\t\t\t\t\t\t\t(\"poly_features\",\tPolynomialFeatures(degree=10,\tinclude_bias=False)), \t\t\t\t\t\t\t\t(\"lin_reg\",\tLinearRegression()), \t\t\t\t))\n\nplot_learning_curves(polynomial_regression,\tX,\ty)\n\nThese\tlearning\tcurves\tlook\ta\tbit\tlike\tthe\tprevious\tones,\tbut\tthere\tare\ttwo\tvery\timportant\tdifferences:\n\nThe\terror\ton\tthe\ttraining\tdata\tis\tmuch\tlower\tthan\twith\tthe\tLinear\tRegression\tmodel.\n\nThere\tis\ta\tgap\tbetween\tthe\tcurves.\tThis\tmeans\tthat\tthe\tmodel\tperforms\tsignificantly\tbetter\ton\tthe training\tdata\tthan\ton\tthe\tvalidation\tdata,\twhich\tis\tthe\thallmark\tof\tan\toverfitting\tmodel.\tHowever,\tif you\tused\ta\tmuch\tlarger\ttraining\tset,\tthe\ttwo\tcurves\twould\tcontinue\tto\tget\tcloser.\n\nFigure\t4-16.\tLearning\tcurves\tfor\tthe\tpolynomial\tmodel\n\nTIP\n\nOne\tway\tto\timprove\tan\toverfitting\tmodel\tis\tto\tfeed\tit\tmore\ttraining\tdata\tuntil\tthe\tvalidation\terror\treaches\tthe\ttraining\terror.\n\nTHE\tBIAS/VARIANCE\tTRADEOFF\n\nAn\timportant\ttheoretical\tresult\tof\tstatistics\tand\tMachine\tLearning\tis\tthe\tfact\tthat\ta\tmodel’s\tgeneralization\terror\tcan\tbe\texpressed\tas\tthe sum\tof\tthree\tvery\tdifferent\terrors:\n\nBias\n\nThis\tpart\tof\tthe\tgeneralization\terror\tis\tdue\tto\twrong\tassumptions,\tsuch\tas\tassuming\tthat\tthe\tdata\tis\tlinear\twhen\tit\tis\tactually quadratic.\tA\thigh-bias\tmodel\tis\tmost\tlikely\tto\tunderfit\tthe\ttraining\tdata.10\n\nVariance\n\nThis\tpart\tis\tdue\tto\tthe\tmodel’s\texcessive\tsensitivity\tto\tsmall\tvariations\tin\tthe\ttraining\tdata.\tA\tmodel\twith\tmany\tdegrees\tof\tfreedom (such\tas\ta\thigh-degree\tpolynomial\tmodel)\tis\tlikely\tto\thave\thigh\tvariance,\tand\tthus\tto\toverfit\tthe\ttraining\tdata.\n\nIrreducible\terror\n\nThis\tpart\tis\tdue\tto\tthe\tnoisiness\tof\tthe\tdata\titself.\tThe\tonly\tway\tto\treduce\tthis\tpart\tof\tthe\terror\tis\tto\tclean\tup\tthe\tdata\t(e.g.,\tfix\tthe data\tsources,\tsuch\tas\tbroken\tsensors,\tor\tdetect\tand\tremove\toutliers).\n\nIncreasing\ta\tmodel’s\tcomplexity\twill\ttypically\tincrease\tits\tvariance\tand\treduce\tits\tbias.\tConversely,\treducing\ta\tmodel’s\tcomplexity increases\tits\tbias\tand\treduces\tits\tvariance.\tThis\tis\twhy\tit\tis\tcalled\ta\ttradeoff.\n\nRegularized\tLinear\tModels As\twe\tsaw\tin\tChapters\t1\tand\t2,\ta\tgood\tway\tto\treduce\toverfitting\tis\tto\tregularize\tthe\tmodel\t(i.e.,\tto constrain\tit):\tthe\tfewer\tdegrees\tof\tfreedom\tit\thas,\tthe\tharder\tit\twill\tbe\tfor\tit\tto\toverfit\tthe\tdata.\tFor example,\ta\tsimple\tway\tto\tregularize\ta\tpolynomial\tmodel\tis\tto\treduce\tthe\tnumber\tof\tpolynomial\tdegrees.\n\nFor\ta\tlinear\tmodel,\tregularization\tis\ttypically\tachieved\tby\tconstraining\tthe\tweights\tof\tthe\tmodel.\tWe\twill now\tlook\tat\tRidge\tRegression,\tLasso\tRegression,\tand\tElastic\tNet,\twhich\timplement\tthree\tdifferent\tways to\tconstrain\tthe\tweights.\n\nRidge\tRegression Ridge\tRegression\t(also\tcalled\tTikhonov\tregularization)\tis\ta\tregularized\tversion\tof\tLinear\tRegression:\ta\n\nregularization\tterm\tequal\tto\t not\tonly\tfit\tthe\tdata\tbut\talso\tkeep\tthe\tmodel\tweights\tas\tsmall\tas\tpossible.\tNote\tthat\tthe\tregularization\tterm should\tonly\tbe\tadded\tto\tthe\tcost\tfunction\tduring\ttraining.\tOnce\tthe\tmodel\tis\ttrained,\tyou\twant\tto\tevaluate the\tmodel’s\tperformance\tusing\tthe\tunregularized\tperformance\tmeasure.\n\nis\tadded\tto\tthe\tcost\tfunction.\tThis\tforces\tthe\tlearning\talgorithm\tto\n\nNOTE\n\nIt\tis\tquite\tcommon\tfor\tthe\tcost\tfunction\tused\tduring\ttraining\tto\tbe\tdifferent\tfrom\tthe\tperformance\tmeasure\tused\tfor\ttesting.\tApart from\tregularization,\tanother\treason\twhy\tthey\tmight\tbe\tdifferent\tis\tthat\ta\tgood\ttraining\tcost\tfunction\tshould\thave\toptimization- friendly\tderivatives,\twhile\tthe\tperformance\tmeasure\tused\tfor\ttesting\tshould\tbe\tas\tclose\tas\tpossible\tto\tthe\tfinal\tobjective.\tA\tgood example\tof\tthis\tis\ta\tclassifier\ttrained\tusing\ta\tcost\tfunction\tsuch\tas\tthe\tlog\tloss\t(discussed\tin\ta\tmoment)\tbut\tevaluated\tusing precision/recall.\n\nThe\thyperparameter\tα\tcontrols\thow\tmuch\tyou\twant\tto\tregularize\tthe\tmodel.\tIf\tα\t=\t0\tthen\tRidge Regression\tis\tjust\tLinear\tRegression.\tIf\tα\tis\tvery\tlarge,\tthen\tall\tweights\tend\tup\tvery\tclose\tto\tzero\tand\tthe result\tis\ta\tflat\tline\tgoing\tthrough\tthe\tdata’s\tmean.\tEquation\t4-8\tpresents\tthe\tRidge\tRegression\tcost function.11\n\nEquation\t4-8.\tRidge\tRegression\tcost\tfunction\n\nNote\tthat\tthe\tbias\tterm\tθ0\tis\tnot\tregularized\t(the\tsum\tstarts\tat\ti\t=\t1,\tnot\t0).\tIf\twe\tdefine\tw\tas\tthe\tvector\tof feature\tweights\t(θ1\tto\tθn),\tthen\tthe\tregularization\tterm\tis\tsimply\tequal\tto\t½(\t w\t2)2,\twhere\t\t·\t represents\tthe\tℓ2\tnorm\tof\tthe\tweight\tvector.12\tFor\tGradient\tDescent,\tjust\tadd\tαw\tto\tthe\tMSE\tgradient vector\t(Equation\t4-6).\n\nWARNING\n\nIt\tis\timportant\tto\tscale\tthe\tdata\t(e.g.,\tusing\ta\tStandardScaler)\tbefore\tperforming\tRidge\tRegression,\tas\tit\tis\tsensitive\tto\tthe\tscale of\tthe\tinput\tfeatures.\tThis\tis\ttrue\tof\tmost\tregularized\tmodels.\n\nFigure\t4-17\tshows\tseveral\tRidge\tmodels\ttrained\ton\tsome\tlinear\tdata\tusing\tdifferent\tα\tvalue.\tOn\tthe\tleft, plain\tRidge\tmodels\tare\tused,\tleading\tto\tlinear\tpredictions.\tOn\tthe\tright,\tthe\tdata\tis\tfirst\texpanded\tusing PolynomialFeatures(degree=10),\tthen\tit\tis\tscaled\tusing\ta\tStandardScaler,\tand\tfinally\tthe\tRidge\n\n2\n\nmodels\tare\tapplied\tto\tthe\tresulting\tfeatures:\tthis\tis\tPolynomial\tRegression\twith\tRidge\tregularization. Note\thow\tincreasing\tα\tleads\tto\tflatter\t(i.e.,\tless\textreme,\tmore\treasonable)\tpredictions;\tthis\treduces\tthe model’s\tvariance\tbut\tincreases\tits\tbias.\n\nAs\twith\tLinear\tRegression,\twe\tcan\tperform\tRidge\tRegression\teither\tby\tcomputing\ta\tclosed-form equation\tor\tby\tperforming\tGradient\tDescent.\tThe\tpros\tand\tcons\tare\tthe\tsame.\tEquation\t4-9\tshows\tthe closed-form\tsolution\t(where\tA\tis\tthe\tn\t×\tn\tidentity\tmatrix13\texcept\twith\ta\t0\tin\tthe\ttop-left\tcell, corresponding\tto\tthe\tbias\tterm).\n\nFigure\t4-17.\tRidge\tRegression\n\nEquation\t4-9.\tRidge\tRegression\tclosed-form\tsolution\n\nHere\tis\thow\tto\tperform\tRidge\tRegression\twith\tScikit-Learn\tusing\ta\tclosed-form\tsolution\t(a\tvariant\tof Equation\t4-9\tusing\ta\tmatrix\tfactorization\ttechnique\tby\tAndré-Louis\tCholesky):\n\n>>>\tfrom\tsklearn.linear_model\timport\tRidge >>>\tridge_reg\t=\tRidge(alpha=1,\tsolver=\"cholesky\") >>>\tridge_reg.fit(X,\ty) >>>\tridge_reg.predict([[1.5]]) array([[\t1.55071465]])\n\nAnd\tusing\tStochastic\tGradient\tDescent:14\n\n>>>\tsgd_reg\t=\tSGDRegressor(penalty=\"l2\") >>>\tsgd_reg.fit(X,\ty.ravel()) >>>\tsgd_reg.predict([[1.5]]) array([\t1.13500145])\n\nThe\tpenalty\thyperparameter\tsets\tthe\ttype\tof\tregularization\tterm\tto\tuse.\tSpecifying\t\"l2\"\tindicates\tthat you\twant\tSGD\tto\tadd\ta\tregularization\tterm\tto\tthe\tcost\tfunction\tequal\tto\thalf\tthe\tsquare\tof\tthe\tℓ2\tnorm\tof the\tweight\tvector:\tthis\tis\tsimply\tRidge\tRegression.\n\nLasso\tRegression Least\tAbsolute\tShrinkage\tand\tSelection\tOperator\tRegression\t(simply\tcalled\tLasso\tRegression)\tis another\tregularized\tversion\tof\tLinear\tRegression:\tjust\tlike\tRidge\tRegression,\tit\tadds\ta\tregularization\tterm to\tthe\tcost\tfunction,\tbut\tit\tuses\tthe\tℓ1\tnorm\tof\tthe\tweight\tvector\tinstead\tof\thalf\tthe\tsquare\tof\tthe\tℓ2\tnorm (see\tEquation\t4-10).\n\nEquation\t4-10.\tLasso\tRegression\tcost\tfunction\n\nFigure\t4-18\tshows\tthe\tsame\tthing\tas\tFigure\t4-17\tbut\treplaces\tRidge\tmodels\twith\tLasso\tmodels\tand\tuses smaller\tα\tvalues.\n\nFigure\t4-18.\tLasso\tRegression\n\nAn\timportant\tcharacteristic\tof\tLasso\tRegression\tis\tthat\tit\ttends\tto\tcompletely\teliminate\tthe\tweights\tof\tthe least\timportant\tfeatures\t(i.e.,\tset\tthem\tto\tzero).\tFor\texample,\tthe\tdashed\tline\tin\tthe\tright\tplot\ton\tFigure\t4- 18\t(with\tα\t=\t10-7)\tlooks\tquadratic,\talmost\tlinear:\tall\tthe\tweights\tfor\tthe\thigh-degree\tpolynomial\tfeatures are\tequal\tto\tzero.\tIn\tother\twords,\tLasso\tRegression\tautomatically\tperforms\tfeature\tselection\tand\toutputs\ta sparse\tmodel\t(i.e.,\twith\tfew\tnonzero\tfeature\tweights).\n\nYou\tcan\tget\ta\tsense\tof\twhy\tthis\tis\tthe\tcase\tby\tlooking\tat\tFigure\t4-19:\ton\tthe\ttop-left\tplot,\tthe\tbackground contours\t(ellipses)\trepresent\tan\tunregularized\tMSE\tcost\tfunction\t(α\t=\t0),\tand\tthe\twhite\tcircles\tshow\tthe Batch\tGradient\tDescent\tpath\twith\tthat\tcost\tfunction.\tThe\tforeground\tcontours\t(diamonds)\trepresent\tthe\tℓ1 penalty,\tand\tthe\ttriangles\tshow\tthe\tBGD\tpath\tfor\tthis\tpenalty\tonly\t(α\t→\t∞).\tNotice\thow\tthe\tpath\tfirst reaches\tθ1\t=\t0,\tthen\trolls\tdown\ta\tgutter\tuntil\tit\treaches\tθ2\t=\t0.\tOn\tthe\ttop-right\tplot,\tthe\tcontours\trepresent the\tsame\tcost\tfunction\tplus\tan\tℓ1\tpenalty\twith\tα\t=\t0.5.\tThe\tglobal\tminimum\tis\ton\tthe\tθ2\t=\t0\taxis.\tBGD\n\nfirst\treaches\tθ2\t=\t0,\tthen\trolls\tdown\tthe\tgutter\tuntil\tit\treaches\tthe\tglobal\tminimum.\tThe\ttwo\tbottom\tplots show\tthe\tsame\tthing\tbut\tuses\tan\tℓ2\tpenalty\tinstead.\tThe\tregularized\tminimum\tis\tcloser\tto\tθ\t=\t0\tthan\tthe unregularized\tminimum,\tbut\tthe\tweights\tdo\tnot\tget\tfully\teliminated.\n\nFigure\t4-19.\tLasso\tversus\tRidge\tregularization\n\nTIP\n\nOn\tthe\tLasso\tcost\tfunction,\tthe\tBGD\tpath\ttends\tto\tbounce\tacross\tthe\tgutter\ttoward\tthe\tend.\tThis\tis\tbecause\tthe\tslope\tchanges abruptly\tat\tθ2\t=\t0.\tYou\tneed\tto\tgradually\treduce\tthe\tlearning\trate\tin\torder\tto\tactually\tconverge\tto\tthe\tglobal\tminimum.\n\nThe\tLasso\tcost\tfunction\tis\tnot\tdifferentiable\tat\tθi\t=\t0\t(for\ti\t=\t1,\t2,\t,\t n),\tbut\tGradient\tDescent\tstill\tworks fine\tif\tyou\tuse\ta\tsubgradient\tvector\tg15\tinstead\twhen\tany\tθi\t=\t0.\tEquation\t4-11\tshows\ta\tsubgradient vector\tequation\tyou\tcan\tuse\tfor\tGradient\tDescent\twith\tthe\tLasso\tcost\tfunction.\n\nEquation\t4-11.\tLasso\tRegression\tsubgradient\tvector\n\nHere\tis\ta\tsmall\tScikit-Learn\texample\tusing\tthe\tLasso\tclass.\tNote\tthat\tyou\tcould\tinstead\tuse\tan SGDRegressor(penalty=\"l1\").\n\n>>>\tfrom\tsklearn.linear_model\timport\tLasso >>>\tlasso_reg\t=\tLasso(alpha=0.1) >>>\tlasso_reg.fit(X,\ty) >>>\tlasso_reg.predict([[1.5]]) array([\t1.53788174])\n\nElastic\tNet Elastic\tNet\tis\ta\tmiddle\tground\tbetween\tRidge\tRegression\tand\tLasso\tRegression.\tThe\tregularization\tterm is\ta\tsimple\tmix\tof\tboth\tRidge\tand\tLasso’s\tregularization\tterms,\tand\tyou\tcan\tcontrol\tthe\tmix\tratio\tr.\tWhen r\t=\t0,\tElastic\tNet\tis\tequivalent\tto\tRidge\tRegression,\tand\twhen\tr\t=\t1,\tit\tis\tequivalent\tto\tLasso\tRegression (see\tEquation\t4-12).\n\nEquation\t4-12.\tElastic\tNet\tcost\tfunction\n\nSo\twhen\tshould\tyou\tuse\tplain\tLinear\tRegression\t(i.e.,\twithout\tany\tregularization),\tRidge,\tLasso,\tor Elastic\tNet?\tIt\tis\talmost\talways\tpreferable\tto\thave\tat\tleast\ta\tlittle\tbit\tof\tregularization,\tso\tgenerally\tyou should\tavoid\tplain\tLinear\tRegression.\tRidge\tis\ta\tgood\tdefault,\tbut\tif\tyou\tsuspect\tthat\tonly\ta\tfew\tfeatures are\tactually\tuseful,\tyou\tshould\tprefer\tLasso\tor\tElastic\tNet\tsince\tthey\ttend\tto\treduce\tthe\tuseless\tfeatures’ weights\tdown\tto\tzero\tas\twe\thave\tdiscussed.\tIn\tgeneral,\tElastic\tNet\tis\tpreferred\tover\tLasso\tsince\tLasso may\tbehave\terratically\twhen\tthe\tnumber\tof\tfeatures\tis\tgreater\tthan\tthe\tnumber\tof\ttraining\tinstances\tor when\tseveral\tfeatures\tare\tstrongly\tcorrelated.\n\nHere\tis\ta\tshort\texample\tusing\tScikit-Learn’s\tElasticNet\t(l1_ratio\tcorresponds\tto\tthe\tmix\tratio\tr):\n\n>>>\tfrom\tsklearn.linear_model\timport\tElasticNet >>>\telastic_net\t=\tElasticNet(alpha=0.1,\tl1_ratio=0.5) >>>\telastic_net.fit(X,\ty) >>>\telastic_net.predict([[1.5]]) array([\t1.54333232])\n\nEarly\tStopping A\tvery\tdifferent\tway\tto\tregularize\titerative\tlearning\talgorithms\tsuch\tas\tGradient\tDescent\tis\tto\tstop training\tas\tsoon\tas\tthe\tvalidation\terror\treaches\ta\tminimum.\tThis\tis\tcalled\tearly\tstopping.\tFigure\t4-20 shows\ta\tcomplex\tmodel\t(in\tthis\tcase\ta\thigh-degree\tPolynomial\tRegression\tmodel)\tbeing\ttrained\tusing Batch\tGradient\tDescent.\tAs\tthe\tepochs\tgo\tby,\tthe\talgorithm\tlearns\tand\tits\tprediction\terror\t(RMSE)\ton\tthe training\tset\tnaturally\tgoes\tdown,\tand\tso\tdoes\tits\tprediction\terror\ton\tthe\tvalidation\tset.\tHowever,\tafter\ta while\tthe\tvalidation\terror\tstops\tdecreasing\tand\tactually\tstarts\tto\tgo\tback\tup.\tThis\tindicates\tthat\tthe\tmodel has\tstarted\tto\toverfit\tthe\ttraining\tdata.\tWith\tearly\tstopping\tyou\tjust\tstop\ttraining\tas\tsoon\tas\tthe\tvalidation error\treaches\tthe\tminimum.\tIt\tis\tsuch\ta\tsimple\tand\tefficient\tregularization\ttechnique\tthat\tGeoffrey\tHinton called\tit\ta\t“beautiful\tfree\tlunch.”\n\nFigure\t4-20.\tEarly\tstopping\tregularization\n\nTIP\n\nWith\tStochastic\tand\tMini-batch\tGradient\tDescent,\tthe\tcurves\tare\tnot\tso\tsmooth,\tand\tit\tmay\tbe\thard\tto\tknow\twhether\tyou\thave reached\tthe\tminimum\tor\tnot.\tOne\tsolution\tis\tto\tstop\tonly\tafter\tthe\tvalidation\terror\thas\tbeen\tabove\tthe\tminimum\tfor\tsome\ttime (when\tyou\tare\tconfident\tthat\tthe\tmodel\twill\tnot\tdo\tany\tbetter),\tthen\troll\tback\tthe\tmodel\tparameters\tto\tthe\tpoint\twhere\tthe validation\terror\twas\tat\ta\tminimum.\n\nHere\tis\ta\tbasic\timplementation\tof\tearly\tstopping:\n\nfrom\tsklearn.base\timport\tclone\n\nsgd_reg\t=\tSGDRegressor(n_iter=1,\twarm_start=True,\tpenalty=None, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlearning_rate=\"constant\",\teta0=0.0005)\n\nminimum_val_error\t=\tfloat(\"inf\") best_epoch\t=\tNone\n\nbest_model\t=\tNone for\tepoch\tin\trange(1000): \t\t\t\tsgd_reg.fit(X_train_poly_scaled,\ty_train)\t\t#\tcontinues\twhere\tit\tleft\toff \t\t\t\ty_val_predict\t=\tsgd_reg.predict(X_val_poly_scaled) \t\t\t\tval_error\t=\tmean_squared_error(y_val_predict,\ty_val) \t\t\t\tif\tval_error\t<\tminimum_val_error: \t\t\t\t\t\t\t\tminimum_val_error\t=\tval_error \t\t\t\t\t\t\t\tbest_epoch\t=\tepoch \t\t\t\t\t\t\t\tbest_model\t=\tclone(sgd_reg)\n\nNote\tthat\twith\twarm_start=True,\twhen\tthe\tfit()\tmethod\tis\tcalled,\tit\tjust\tcontinues\ttraining\twhere\tit\tleft off\tinstead\tof\trestarting\tfrom\tscratch.\n\nLogistic\tRegression As\twe\tdiscussed\tin\tChapter\t1,\tsome\tregression\talgorithms\tcan\tbe\tused\tfor\tclassification\tas\twell\t(and vice\tversa).\tLogistic\tRegression\t(also\tcalled\tLogit\tRegression)\tis\tcommonly\tused\tto\testimate\tthe probability\tthat\tan\tinstance\tbelongs\tto\ta\tparticular\tclass\t(e.g.,\twhat\tis\tthe\tprobability\tthat\tthis\temail\tis spam?).\tIf\tthe\testimated\tprobability\tis\tgreater\tthan\t50%,\tthen\tthe\tmodel\tpredicts\tthat\tthe\tinstance\tbelongs to\tthat\tclass\t(called\tthe\tpositive\tclass,\tlabeled\t“1”),\tor\telse\tit\tpredicts\tthat\tit\tdoes\tnot\t(i.e.,\tit\tbelongs\tto the\tnegative\tclass,\tlabeled\t“0”).\tThis\tmakes\tit\ta\tbinary\tclassifier.\n\nEstimating\tProbabilities So\thow\tdoes\tit\twork?\tJust\tlike\ta\tLinear\tRegression\tmodel,\ta\tLogistic\tRegression\tmodel\tcomputes\ta weighted\tsum\tof\tthe\tinput\tfeatures\t(plus\ta\tbias\tterm),\tbut\tinstead\tof\toutputting\tthe\tresult\tdirectly\tlike\tthe Linear\tRegression\tmodel\tdoes,\tit\toutputs\tthe\tlogistic\tof\tthis\tresult\t(see\tEquation\t4-13).\n\nEquation\t4-13.\tLogistic\tRegression\tmodel\testimated\tprobability\t(vectorized\tform)\n\nThe\tlogistic\t—\talso\tcalled\tthe\tlogit,\tnoted\tσ(·)\t—\tis\ta\tsigmoid\tfunction\t(i.e.,\tS-shaped)\tthat\toutputs\ta number\tbetween\t0\tand\t1.\tIt\tis\tdefined\tas\tshown\tin\tEquation\t4-14\tand\tFigure\t4-21.\n\nEquation\t4-14.\tLogistic\tfunction\n\nFigure\t4-21.\tLogistic\tfunction\n\nOnce\tthe\tLogistic\tRegression\tmodel\thas\testimated\tthe\tprobability\t the\tpositive\tclass,\tit\tcan\tmake\tits\tprediction\tŷ\teasily\t(see\tEquation\t4-15).\n\n=\thθ(x)\tthat\tan\tinstance\tx\tbelongs\tto\n\nEquation\t4-15.\tLogistic\tRegression\tmodel\tprediction\n\nNotice\tthat\tσ(t)\t<\t0.5\twhen\tt\t<\t0,\tand\tσ(t)\t≥\t0.5\twhen\tt\t≥\t0,\tso\ta\tLogistic\tRegression\tmodel\tpredicts\t1\tif\n\nθT\t·\tx\tis\tpositive,\tand\t0\tif\tit\tis\tnegative.\n\nTraining\tand\tCost\tFunction Good,\tnow\tyou\tknow\thow\ta\tLogistic\tRegression\tmodel\testimates\tprobabilities\tand\tmakes\tpredictions. But\thow\tis\tit\ttrained?\tThe\tobjective\tof\ttraining\tis\tto\tset\tthe\tparameter\tvector\tθ\tso\tthat\tthe\tmodel\testimates high\tprobabilities\tfor\tpositive\tinstances\t(y\t=\t1)\tand\tlow\tprobabilities\tfor\tnegative\tinstances\t(y\t=\t0).\tThis idea\tis\tcaptured\tby\tthe\tcost\tfunction\tshown\tin\tEquation\t4-16\tfor\ta\tsingle\ttraining\tinstance\tx.\n\nEquation\t4-16.\tCost\tfunction\tof\ta\tsingle\ttraining\tinstance\n\nThis\tcost\tfunction\tmakes\tsense\tbecause\t–\tlog(t)\tgrows\tvery\tlarge\twhen\tt\tapproaches\t0,\tso\tthe\tcost\twill be\tlarge\tif\tthe\tmodel\testimates\ta\tprobability\tclose\tto\t0\tfor\ta\tpositive\tinstance,\tand\tit\twill\talso\tbe\tvery large\tif\tthe\tmodel\testimates\ta\tprobability\tclose\tto\t1\tfor\ta\tnegative\tinstance.\tOn\tthe\tother\thand,\t–\tlog(t)\tis close\tto\t0\twhen\tt\tis\tclose\tto\t1,\tso\tthe\tcost\twill\tbe\tclose\tto\t0\tif\tthe\testimated\tprobability\tis\tclose\tto\t0\tfor\ta negative\tinstance\tor\tclose\tto\t1\tfor\ta\tpositive\tinstance,\twhich\tis\tprecisely\twhat\twe\twant.\n\nThe\tcost\tfunction\tover\tthe\twhole\ttraining\tset\tis\tsimply\tthe\taverage\tcost\tover\tall\ttraining\tinstances.\tIt\tcan be\twritten\tin\ta\tsingle\texpression\t(as\tyou\tcan\tverify\teasily),\tcalled\tthe\tlog\tloss,\tshown\tin\tEquation\t4-17.\n\nEquation\t4-17.\tLogistic\tRegression\tcost\tfunction\t(log\tloss)\n\nThe\tbad\tnews\tis\tthat\tthere\tis\tno\tknown\tclosed-form\tequation\tto\tcompute\tthe\tvalue\tof\tθ\tthat\tminimizes\tthis cost\tfunction\t(there\tis\tno\tequivalent\tof\tthe\tNormal\tEquation).\tBut\tthe\tgood\tnews\tis\tthat\tthis\tcost\tfunction is\tconvex,\tso\tGradient\tDescent\t(or\tany\tother\toptimization\talgorithm)\tis\tguaranteed\tto\tfind\tthe\tglobal minimum\t(if\tthe\tlearning\trate\tis\tnot\ttoo\tlarge\tand\tyou\twait\tlong\tenough).\tThe\tpartial\tderivatives\tof\tthe cost\tfunction\twith\tregards\tto\tthe\tjth\tmodel\tparameter\tθj\tis\tgiven\tby\tEquation\t4-18.\n\nEquation\t4-18.\tLogistic\tcost\tfunction\tpartial\tderivatives\n\nThis\tequation\tlooks\tvery\tmuch\tlike\tEquation\t4-5:\tfor\teach\tinstance\tit\tcomputes\tthe\tprediction\terror\tand multiplies\tit\tby\tthe\tjth\tfeature\tvalue,\tand\tthen\tit\tcomputes\tthe\taverage\tover\tall\ttraining\tinstances.\tOnce\tyou have\tthe\tgradient\tvector\tcontaining\tall\tthe\tpartial\tderivatives\tyou\tcan\tuse\tit\tin\tthe\tBatch\tGradient\tDescent algorithm.\tThat’s\tit:\tyou\tnow\tknow\thow\tto\ttrain\ta\tLogistic\tRegression\tmodel.\tFor\tStochastic\tGD\tyou\n\nwould\tof\tcourse\tjust\ttake\tone\tinstance\tat\ta\ttime,\tand\tfor\tMini-batch\tGD\tyou\twould\tuse\ta\tmini-batch\tat\ta time.\n\nDecision\tBoundaries Let’s\tuse\tthe\tiris\tdataset\tto\tillustrate\tLogistic\tRegression.\tThis\tis\ta\tfamous\tdataset\tthat\tcontains\tthe\tsepal and\tpetal\tlength\tand\twidth\tof\t150\tiris\tflowers\tof\tthree\tdifferent\tspecies:\tIris-Setosa,\tIris-Versicolor,\tand Iris-Virginica\t(see\tFigure\t4-22).\n\nFigure\t4-22.\tFlowers\tof\tthree\tiris\tplant\tspecies16\n\nLet’s\ttry\tto\tbuild\ta\tclassifier\tto\tdetect\tthe\tIris-Virginica\ttype\tbased\tonly\ton\tthe\tpetal\twidth\tfeature.\tFirst let’s\tload\tthe\tdata:\n\n>>>\tfrom\tsklearn\timport\tdatasets >>>\tiris\t=\tdatasets.load_iris() >>>\tlist(iris.keys()) ['data',\t'target_names',\t'feature_names',\t'target',\t'DESCR'] >>>\tX\t=\tiris[\"data\"][:,\t3:]\t\t#\tpetal\twidth >>>\ty\t=\t(iris[\"target\"]\t==\t2).astype(np.int)\t\t#\t1\tif\tIris-Virginica,\telse\t0\n\nNow\tlet’s\ttrain\ta\tLogistic\tRegression\tmodel:\n\nfrom\tsklearn.linear_model\timport\tLogisticRegression\n\nlog_reg\t=\tLogisticRegression() log_reg.fit(X,\ty)\n\nLet’s\tlook\tat\tthe\tmodel’s\testimated\tprobabilities\tfor\tflowers\twith\tpetal\twidths\tvarying\tfrom\t0\tto\t3\tcm (Figure\t4-23):\n\nX_new\t=\tnp.linspace(0,\t3,\t1000).reshape(-1,\t1) y_proba\t=\tlog_reg.predict_proba(X_new) plt.plot(X_new,\ty_proba[:,\t1],\t\"g-\",\tlabel=\"Iris-Virginica\") plt.plot(X_new,\ty_proba[:,\t0],\t\"b--\",\tlabel=\"Not\tIris-Virginica\") #\t+\tmore\tMatplotlib\tcode\tto\tmake\tthe\timage\tlook\tpretty\n\nFigure\t4-23.\tEstimated\tprobabilities\tand\tdecision\tboundary\n\nThe\tpetal\twidth\tof\tIris-Virginica\tflowers\t(represented\tby\ttriangles)\tranges\tfrom\t1.4\tcm\tto\t2.5\tcm,\twhile the\tother\tiris\tflowers\t(represented\tby\tsquares)\tgenerally\thave\ta\tsmaller\tpetal\twidth,\tranging\tfrom\t0.1\tcm to\t1.8\tcm.\tNotice\tthat\tthere\tis\ta\tbit\tof\toverlap.\tAbove\tabout\t2\tcm\tthe\tclassifier\tis\thighly\tconfident\tthat\tthe flower\tis\tan\tIris-Virginica\t(it\toutputs\ta\thigh\tprobability\tto\tthat\tclass),\twhile\tbelow\t1\tcm\tit\tis\thighly confident\tthat\tit\tis\tnot\tan\tIris-Virginica\t(high\tprobability\tfor\tthe\t“Not\tIris-Virginica”\tclass).\tIn\tbetween these\textremes,\tthe\tclassifier\tis\tunsure.\tHowever,\tif\tyou\task\tit\tto\tpredict\tthe\tclass\t(using\tthe\tpredict() method\trather\tthan\tthe\tpredict_proba()\tmethod),\tit\twill\treturn\twhichever\tclass\tis\tthe\tmost\tlikely. Therefore,\tthere\tis\ta\tdecision\tboundary\tat\taround\t1.6\tcm\twhere\tboth\tprobabilities\tare\tequal\tto\t50%:\tif the\tpetal\twidth\tis\thigher\tthan\t1.6\tcm,\tthe\tclassifier\twill\tpredict\tthat\tthe\tflower\tis\tan\tIris-Virginica,\tor\telse it\twill\tpredict\tthat\tit\tis\tnot\t(even\tif\tit\tis\tnot\tvery\tconfident):\n\n>>>\tlog_reg.predict([[1.7],\t[1.5]]) array([1,\t0])\n\nFigure\t4-24\tshows\tthe\tsame\tdataset\tbut\tthis\ttime\tdisplaying\ttwo\tfeatures:\tpetal\twidth\tand\tlength.\tOnce trained,\tthe\tLogistic\tRegression\tclassifier\tcan\testimate\tthe\tprobability\tthat\ta\tnew\tflower\tis\tan\tIris- Virginica\tbased\ton\tthese\ttwo\tfeatures.\tThe\tdashed\tline\trepresents\tthe\tpoints\twhere\tthe\tmodel\testimates\ta 50%\tprobability:\tthis\tis\tthe\tmodel’s\tdecision\tboundary.\tNote\tthat\tit\tis\ta\tlinear\tboundary.17\tEach\tparallel line\trepresents\tthe\tpoints\twhere\tthe\tmodel\toutputs\ta\tspecific\tprobability,\tfrom\t15%\t(bottom\tleft)\tto\t90% (top\tright).\tAll\tthe\tflowers\tbeyond\tthe\ttop-right\tline\thave\tan\tover\t90%\tchance\tof\tbeing\tIris-Virginica according\tto\tthe\tmodel.\n\nFigure\t4-24.\tLinear\tdecision\tboundary\n\nJust\tlike\tthe\tother\tlinear\tmodels,\tLogistic\tRegression\tmodels\tcan\tbe\tregularized\tusing\tℓ1\tor\tℓ2\tpenalties. Scitkit-Learn\tactually\tadds\tan\tℓ2\tpenalty\tby\tdefault.\n\nNOTE\n\nThe\thyperparameter\tcontrolling\tthe\tregularization\tstrength\tof\ta\tScikit-Learn\tLogisticRegression\tmodel\tis\tnot\talpha\t(as\tin\tother linear\tmodels),\tbut\tits\tinverse:\tC.\tThe\thigher\tthe\tvalue\tof\tC,\tthe\tless\tthe\tmodel\tis\tregularized.\n\nSoftmax\tRegression The\tLogistic\tRegression\tmodel\tcan\tbe\tgeneralized\tto\tsupport\tmultiple\tclasses\tdirectly,\twithout\thaving\tto train\tand\tcombine\tmultiple\tbinary\tclassifiers\t(as\tdiscussed\tin\tChapter\t3).\tThis\tis\tcalled\tSoftmax Regression,\tor\tMultinomial\tLogistic\tRegression.\n\nThe\tidea\tis\tquite\tsimple:\twhen\tgiven\tan\tinstance\tx,\tthe\tSoftmax\tRegression\tmodel\tfirst\tcomputes\ta\tscore sk(x)\tfor\teach\tclass\tk,\tthen\testimates\tthe\tprobability\tof\teach\tclass\tby\tapplying\tthe\tsoftmax\tfunction\t(also called\tthe\tnormalized\texponential)\tto\tthe\tscores.\tThe\tequation\tto\tcompute\tsk(x)\tshould\tlook\tfamiliar,\tas it\tis\tjust\tlike\tthe\tequation\tfor\tLinear\tRegression\tprediction\t(see\tEquation\t4-19).\n\nEquation\t4-19.\tSoftmax\tscore\tfor\tclass\tk\n\nNote\tthat\teach\tclass\thas\tits\town\tdedicated\tparameter\tvector\tθ(k).\tAll\tthese\tvectors\tare\ttypically\tstored\tas rows\tin\ta\tparameter\tmatrix\tΘ.\n\nOnce\tyou\thave\tcomputed\tthe\tscore\tof\tevery\tclass\tfor\tthe\tinstance\tx,\tyou\tcan\testimate\tthe\tprobability\t k that\tthe\tinstance\tbelongs\tto\tclass\tk\tby\trunning\tthe\tscores\tthrough\tthe\tsoftmax\tfunction\t(Equation\t4-20):\tit computes\tthe\texponential\tof\tevery\tscore,\tthen\tnormalizes\tthem\t(dividing\tby\tthe\tsum\tof\tall\tthe exponentials).\n\nEquation\t4-20.\tSoftmax\tfunction\n\nK\tis\tthe\tnumber\tof\tclasses.\n\ns(x)\tis\ta\tvector\tcontaining\tthe\tscores\tof\teach\tclass\tfor\tthe\tinstance\tx.\n\nσ(s(x))k\tis\tthe\testimated\tprobability\tthat\tthe\tinstance\tx\tbelongs\tto\tclass\tk\tgiven\tthe\tscores\tof\teach class\tfor\tthat\tinstance.\n\nJust\tlike\tthe\tLogistic\tRegression\tclassifier,\tthe\tSoftmax\tRegression\tclassifier\tpredicts\tthe\tclass\twith\tthe highest\testimated\tprobability\t(which\tis\tsimply\tthe\tclass\twith\tthe\thighest\tscore),\tas\tshown\tin\tEquation\t4- 21.\n\nEquation\t4-21.\tSoftmax\tRegression\tclassifier\tprediction\n\nThe\targmax\toperator\treturns\tthe\tvalue\tof\ta\tvariable\tthat\tmaximizes\ta\tfunction.\tIn\tthis\tequation,\tit returns\tthe\tvalue\tof\tk\tthat\tmaximizes\tthe\testimated\tprobability\tσ(s(x))k.\n\nTIP\n\nThe\tSoftmax\tRegression\tclassifier\tpredicts\tonly\tone\tclass\tat\ta\ttime\t(i.e.,\tit\tis\tmulticlass,\tnot\tmultioutput)\tso\tit\tshould\tbe\tused\tonly with\tmutually\texclusive\tclasses\tsuch\tas\tdifferent\ttypes\tof\tplants.\tYou\tcannot\tuse\tit\tto\trecognize\tmultiple\tpeople\tin\tone\tpicture.\n\nNow\tthat\tyou\tknow\thow\tthe\tmodel\testimates\tprobabilities\tand\tmakes\tpredictions,\tlet’s\ttake\ta\tlook\tat training.\tThe\tobjective\tis\tto\thave\ta\tmodel\tthat\testimates\ta\thigh\tprobability\tfor\tthe\ttarget\tclass\t(and consequently\ta\tlow\tprobability\tfor\tthe\tother\tclasses).\tMinimizing\tthe\tcost\tfunction\tshown\tin\tEquation\t4- 22,\tcalled\tthe\tcross\tentropy,\tshould\tlead\tto\tthis\tobjective\tbecause\tit\tpenalizes\tthe\tmodel\twhen\tit estimates\ta\tlow\tprobability\tfor\ta\ttarget\tclass.\tCross\tentropy\tis\tfrequently\tused\tto\tmeasure\thow\twell\ta\tset of\testimated\tclass\tprobabilities\tmatch\tthe\ttarget\tclasses\t(we\twill\tuse\tit\tagain\tseveral\ttimes\tin\tthe following\tchapters).\n\nEquation\t4-22.\tCross\tentropy\tcost\tfunction\n\nis\tequal\tto\t1\tif\tthe\ttarget\tclass\tfor\tthe\tith\tinstance\tis\tk;\totherwise,\tit\tis\tequal\tto\t0.\n\nNotice\tthat\twhen\tthere\tare\tjust\ttwo\tclasses\t(K\t=\t2),\tthis\tcost\tfunction\tis\tequivalent\tto\tthe\tLogistic Regression’s\tcost\tfunction\t(log\tloss;\tsee\tEquation\t4-17).\n\nCROSS\tENTROPY\n\nCross\tentropy\toriginated\tfrom\tinformation\ttheory.\tSuppose\tyou\twant\tto\tefficiently\ttransmit\tinformation\tabout\tthe\tweather\tevery\tday.\tIf there\tare\teight\toptions\t(sunny,\trainy,\tetc.),\tyou\tcould\tencode\teach\toption\tusing\t3\tbits\tsince\t23\t=\t8.\tHowever,\tif\tyou\tthink\tit\twill\tbe\tsunny almost\tevery\tday,\tit\twould\tbe\tmuch\tmore\tefficient\tto\tcode\t“sunny”\ton\tjust\tone\tbit\t(0)\tand\tthe\tother\tseven\toptions\ton\t4\tbits\t(starting\twith a\t1).\tCross\tentropy\tmeasures\tthe\taverage\tnumber\tof\tbits\tyou\tactually\tsend\tper\toption.\tIf\tyour\tassumption\tabout\tthe\tweather\tis\tperfect, cross\tentropy\twill\tjust\tbe\tequal\tto\tthe\tentropy\tof\tthe\tweather\titself\t(i.e.,\tits\tintrinsic\tunpredictability).\tBut\tif\tyour\tassumptions\tare\twrong (e.g.,\tif\tit\trains\toften),\tcross\tentropy\twill\tbe\tgreater\tby\tan\tamount\tcalled\tthe\tKullback–Leibler\tdivergence.\n\nThe\tcross\tentropy\tbetween\ttwo\tprobability\tdistributions\tp\tand\tq\tis\tdefined\tas\t when\tthe\tdistributions\tare\tdiscrete).\n\n(at\tleast\n\nThe\tgradient\tvector\tof\tthis\tcost\tfunction\twith\tregards\tto\tθ(k)\tis\tgiven\tby\tEquation\t4-23:\n\nEquation\t4-23.\tCross\tentropy\tgradient\tvector\tfor\tclass\tk\n\nNow\tyou\tcan\tcompute\tthe\tgradient\tvector\tfor\tevery\tclass,\tthen\tuse\tGradient\tDescent\t(or\tany\tother optimization\talgorithm)\tto\tfind\tthe\tparameter\tmatrix\tΘ\tthat\tminimizes\tthe\tcost\tfunction.\n\nLet’s\tuse\tSoftmax\tRegression\tto\tclassify\tthe\tiris\tflowers\tinto\tall\tthree\tclasses.\tScikit-Learn’s LogisticRegression\tuses\tone-versus-all\tby\tdefault\twhen\tyou\ttrain\tit\ton\tmore\tthan\ttwo\tclasses,\tbut\tyou can\tset\tthe\tmulti_class\thyperparameter\tto\t\"multinomial\"\tto\tswitch\tit\tto\tSoftmax\tRegression\tinstead. You\tmust\talso\tspecify\ta\tsolver\tthat\tsupports\tSoftmax\tRegression,\tsuch\tas\tthe\t\"lbfgs\"\tsolver\t(see\tScikit- Learn’s\tdocumentation\tfor\tmore\tdetails).\tIt\talso\tapplies\tℓ2\tregularization\tby\tdefault,\twhich\tyou\tcan control\tusing\tthe\thyperparameter\tC.\n\nX\t=\tiris[\"data\"][:,\t(2,\t3)]\t\t#\tpetal\tlength,\tpetal\twidth y\t=\tiris[\"target\"]\n\nsoftmax_reg\t=\tLogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",\tC=10) softmax_reg.fit(X,\ty)\n\nSo\tthe\tnext\ttime\tyou\tfind\tan\tiris\twith\t5\tcm\tlong\tand\t2\tcm\twide\tpetals,\tyou\tcan\task\tyour\tmodel\tto\ttell\tyou what\ttype\tof\tiris\tit\tis,\tand\tit\twill\tanswer\tIris-Virginica\t(class\t2)\twith\t94.2%\tprobability\t(or\tIris- Versicolor\twith\t5.8%\tprobability):\n\n>>>\tsoftmax_reg.predict([[5,\t2]]) array([2]) >>>\tsoftmax_reg.predict_proba([[5,\t2]]) array([[\t\t6.33134078e-07,\t\t\t5.75276067e-02,\t\t\t9.42471760e-01]])\n\nFigure\t4-25\tshows\tthe\tresulting\tdecision\tboundaries,\trepresented\tby\tthe\tbackground\tcolors.\tNotice\tthat the\tdecision\tboundaries\tbetween\tany\ttwo\tclasses\tare\tlinear.\tThe\tfigure\talso\tshows\tthe\tprobabilities\tfor the\tIris-Versicolor\tclass,\trepresented\tby\tthe\tcurved\tlines\t(e.g.,\tthe\tline\tlabeled\twith\t0.450\trepresents\tthe 45%\tprobability\tboundary).\tNotice\tthat\tthe\tmodel\tcan\tpredict\ta\tclass\tthat\thas\tan\testimated\tprobability below\t50%.\tFor\texample,\tat\tthe\tpoint\twhere\tall\tdecision\tboundaries\tmeet,\tall\tclasses\thave\tan\tequal estimated\tprobability\tof\t33%.\n\nFigure\t4-25.\tSoftmax\tRegression\tdecision\tboundaries\n\nExercises\n\n1.\t What\tLinear\tRegression\ttraining\talgorithm\tcan\tyou\tuse\tif\tyou\thave\ta\ttraining\tset\twith\tmillions\tof features?\n\n2.\t Suppose\tthe\tfeatures\tin\tyour\ttraining\tset\thave\tvery\tdifferent\tscales.\tWhat\talgorithms\tmight\tsuffer from\tthis,\tand\thow?\tWhat\tcan\tyou\tdo\tabout\tit?\n\n3.\t Can\tGradient\tDescent\tget\tstuck\tin\ta\tlocal\tminimum\twhen\ttraining\ta\tLogistic\tRegression\tmodel?\n\n4.\t Do\tall\tGradient\tDescent\talgorithms\tlead\tto\tthe\tsame\tmodel\tprovided\tyou\tlet\tthem\trun\tlong\tenough?\n\n5.\t Suppose\tyou\tuse\tBatch\tGradient\tDescent\tand\tyou\tplot\tthe\tvalidation\terror\tat\tevery\tepoch.\tIf\tyou notice\tthat\tthe\tvalidation\terror\tconsistently\tgoes\tup,\twhat\tis\tlikely\tgoing\ton?\tHow\tcan\tyou\tfix\tthis?\n\n6.\t Is\tit\ta\tgood\tidea\tto\tstop\tMini-batch\tGradient\tDescent\timmediately\twhen\tthe\tvalidation\terror\tgoes up?\n\n7.\t Which\tGradient\tDescent\talgorithm\t(among\tthose\twe\tdiscussed)\twill\treach\tthe\tvicinity\tof\tthe\toptimal solution\tthe\tfastest?\tWhich\twill\tactually\tconverge?\tHow\tcan\tyou\tmake\tthe\tothers\tconverge\tas\twell?\n\n8.\t Suppose\tyou\tare\tusing\tPolynomial\tRegression.\tYou\tplot\tthe\tlearning\tcurves\tand\tyou\tnotice\tthat\tthere is\ta\tlarge\tgap\tbetween\tthe\ttraining\terror\tand\tthe\tvalidation\terror.\tWhat\tis\thappening?\tWhat\tare\tthree ways\tto\tsolve\tthis?\n\n9.\t Suppose\tyou\tare\tusing\tRidge\tRegression\tand\tyou\tnotice\tthat\tthe\ttraining\terror\tand\tthe\tvalidation error\tare\talmost\tequal\tand\tfairly\thigh.\tWould\tyou\tsay\tthat\tthe\tmodel\tsuffers\tfrom\thigh\tbias\tor\thigh variance?\tShould\tyou\tincrease\tthe\tregularization\thyperparameter\tα\tor\treduce\tit?\n\n10.\t Why\twould\tyou\twant\tto\tuse:\n\nRidge\tRegression\tinstead\tof\tplain\tLinear\tRegression\t(i.e.,\twithout\tany\tregularization)?\n\nLasso\tinstead\tof\tRidge\tRegression?\n\nElastic\tNet\tinstead\tof\tLasso?\n\n11.\t Suppose\tyou\twant\tto\tclassify\tpictures\tas\toutdoor/indoor\tand\tdaytime/nighttime.\tShould\tyou implement\ttwo\tLogistic\tRegression\tclassifiers\tor\tone\tSoftmax\tRegression\tclassifier?\n\n12.\t Implement\tBatch\tGradient\tDescent\twith\tearly\tstopping\tfor\tSoftmax\tRegression\t(without\tusing\tScikit- Learn).\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nIt\tis\toften\tthe\tcase\tthat\ta\tlearning\talgorithm\twill\ttry\tto\toptimize\ta\tdifferent\tfunction\tthan\tthe\tperformance\tmeasure\tused\tto\tevaluate\tthe final\tmodel.\tThis\tis\tgenerally\tbecause\tthat\tfunction\tis\teasier\tto\tcompute,\tbecause\tit\thas\tuseful\tdifferentiation\tproperties\tthat\tthe performance\tmeasure\tlacks,\tor\tbecause\twe\twant\tto\tconstrain\tthe\tmodel\tduring\ttraining,\tas\twe\twill\tsee\twhen\twe\tdiscuss\tregularization.\n\n2\n\n3\n\n4\n\n5\n\n6\n\nth\n\n7\n\n8\n\n9\n\n2\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\nThe\tdemonstration\tthat\tthis\treturns\tthe\tvalue\tof\tθ\tthat\tminimizes\tthe\tcost\tfunction\tis\toutside\tthe\tscope\tof\tthis\tbook.\n\nNote\tthat\tScikit-Learn\tseparates\tthe\tbias\tterm\t(intercept_)\tfrom\tthe\tfeature\tweights\t(coef_).\n\nTechnically\tspeaking,\tits\tderivative\tis\tLipschitz\tcontinuous.\n\nSince\tfeature\t1\tis\tsmaller,\tit\ttakes\ta\tlarger\tchange\tin\tθ1\tto\taffect\tthe\tcost\tfunction,\twhich\tis\twhy\tthe\tbowl\tis\telongated\talong\tthe\tθ1\taxis.\n\nEta\t(η)\tis\tthe\t7\n\nletter\tof\tthe\tGreek\talphabet.\n\nOut-of-core\talgorithms\tare\tdiscussed\tin\tChapter\t1.\n\nWhile\tthe\tNormal\tEquation\tcan\tonly\tperform\tLinear\tRegression,\tthe\tGradient\tDescent\talgorithms\tcan\tbe\tused\tto\ttrain\tmany\tother\tmodels, as\twe\twill\tsee.\n\nA\tquadratic\tequation\tis\tof\tthe\tform\ty\t=\tax\n\n+\tbx\t+\tc.\n\nThis\tnotion\tof\tbias\tis\tnot\tto\tbe\tconfused\twith\tthe\tbias\tterm\tof\tlinear\tmodels.\n\nIt\tis\tcommon\tto\tuse\tthe\tnotation\tJ(θ)\tfor\tcost\tfunctions\tthat\tdon’t\thave\ta\tshort\tname;\twe\twill\toften\tuse\tthis\tnotation\tthroughout\tthe\trest\tof this\tbook.\tThe\tcontext\twill\tmake\tit\tclear\twhich\tcost\tfunction\tis\tbeing\tdiscussed.\n\nNorms\tare\tdiscussed\tin\tChapter\t2.\n\nA\tsquare\tmatrix\tfull\tof\t0s\texcept\tfor\t1s\ton\tthe\tmain\tdiagonal\t(top-left\tto\tbottom-right).\n\nAlternatively\tyou\tcan\tuse\tthe\tRidge\tclass\twith\tthe\t\"sag\"\tsolver.\tStochastic\tAverage\tGD\tis\ta\tvariant\tof\tSGD.\tFor\tmore\tdetails,\tsee\tthe presentation\t“Minimizing\tFinite\tSums\twith\tthe\tStochastic\tAverage\tGradient\tAlgorithm”\tby\tMark\tSchmidt\tet\tal.\tfrom\tthe\tUniversity\tof British\tColumbia.\n\nYou\tcan\tthink\tof\ta\tsubgradient\tvector\tat\ta\tnondifferentiable\tpoint\tas\tan\tintermediate\tvector\tbetween\tthe\tgradient\tvectors\taround\tthat\tpoint.\n\nPhotos\treproduced\tfrom\tthe\tcorresponding\tWikipedia\tpages.\tIris-Virginica\tphoto\tby\tFrank\tMayfield\t(Creative\tCommons\tBY-SA\t2.0),\tIris- Versicolor\tphoto\tby\tD.\tGordon\tE.\tRobertson\t(Creative\tCommons\tBY-SA\t3.0),\tand\tIris-Setosa\tphoto\tis\tpublic\tdomain.\n\nIt\tis\tthe\tthe\tset\tof\tpoints\tx\tsuch\tthat\tθ0\t+\tθ1x1\t+\tθ2x2\t=\t0,\twhich\tdefines\ta\tstraight\tline.\n\nChapter\t5.\tSupport\tVector\tMachines\n\nA\tSupport\tVector\tMachine\t(SVM)\tis\ta\tvery\tpowerful\tand\tversatile\tMachine\tLearning\tmodel,\tcapable\tof performing\tlinear\tor\tnonlinear\tclassification,\tregression,\tand\teven\toutlier\tdetection.\tIt\tis\tone\tof\tthe\tmost popular\tmodels\tin\tMachine\tLearning,\tand\tanyone\tinterested\tin\tMachine\tLearning\tshould\thave\tit\tin\ttheir toolbox.\tSVMs\tare\tparticularly\twell\tsuited\tfor\tclassification\tof\tcomplex\tbut\tsmall-\tor\tmedium-sized datasets.\n\nThis\tchapter\twill\texplain\tthe\tcore\tconcepts\tof\tSVMs,\thow\tto\tuse\tthem,\tand\thow\tthey\twork.\n\nLinear\tSVM\tClassification The\tfundamental\tidea\tbehind\tSVMs\tis\tbest\texplained\twith\tsome\tpictures.\tFigure\t5-1\tshows\tpart\tof\tthe iris\tdataset\tthat\twas\tintroduced\tat\tthe\tend\tof\tChapter\t4.\tThe\ttwo\tclasses\tcan\tclearly\tbe\tseparated\teasily with\ta\tstraight\tline\t(they\tare\tlinearly\tseparable).\tThe\tleft\tplot\tshows\tthe\tdecision\tboundaries\tof\tthree possible\tlinear\tclassifiers.\tThe\tmodel\twhose\tdecision\tboundary\tis\trepresented\tby\tthe\tdashed\tline\tis\tso bad\tthat\tit\tdoes\tnot\teven\tseparate\tthe\tclasses\tproperly.\tThe\tother\ttwo\tmodels\twork\tperfectly\ton\tthis training\tset,\tbut\ttheir\tdecision\tboundaries\tcome\tso\tclose\tto\tthe\tinstances\tthat\tthese\tmodels\twill\tprobably not\tperform\tas\twell\ton\tnew\tinstances.\tIn\tcontrast,\tthe\tsolid\tline\tin\tthe\tplot\ton\tthe\tright\trepresents\tthe decision\tboundary\tof\tan\tSVM\tclassifier;\tthis\tline\tnot\tonly\tseparates\tthe\ttwo\tclasses\tbut\talso\tstays\tas\tfar away\tfrom\tthe\tclosest\ttraining\tinstances\tas\tpossible.\tYou\tcan\tthink\tof\tan\tSVM\tclassifier\tas\tfitting\tthe widest\tpossible\tstreet\t(represented\tby\tthe\tparallel\tdashed\tlines)\tbetween\tthe\tclasses.\tThis\tis\tcalled\tlarge margin\tclassification.\n\nFigure\t5-1.\tLarge\tmargin\tclassification\n\nNotice\tthat\tadding\tmore\ttraining\tinstances\t“off\tthe\tstreet”\twill\tnot\taffect\tthe\tdecision\tboundary\tat\tall:\tit\tis fully\tdetermined\t(or\t“supported”)\tby\tthe\tinstances\tlocated\ton\tthe\tedge\tof\tthe\tstreet.\tThese\tinstances\tare called\tthe\tsupport\tvectors\t(they\tare\tcircled\tin\tFigure\t5-1).\n\nWARNING\n\nSVMs\tare\tsensitive\tto\tthe\tfeature\tscales,\tas\tyou\tcan\tsee\tin\tFigure\t5-2:\ton\tthe\tleft\tplot,\tthe\tvertical\tscale\tis\tmuch\tlarger\tthan\tthe horizontal\tscale,\tso\tthe\twidest\tpossible\tstreet\tis\tclose\tto\thorizontal.\tAfter\tfeature\tscaling\t(e.g.,\tusing\tScikit-Learn’s StandardScaler),\tthe\tdecision\tboundary\tlooks\tmuch\tbetter\t(on\tthe\tright\tplot).\n\nFigure\t5-2.\tSensitivity\tto\tfeature\tscales\n\nSoft\tMargin\tClassification If\twe\tstrictly\timpose\tthat\tall\tinstances\tbe\toff\tthe\tstreet\tand\ton\tthe\tright\tside,\tthis\tis\tcalled\thard\tmargin classification.\tThere\tare\ttwo\tmain\tissues\twith\thard\tmargin\tclassification.\tFirst,\tit\tonly\tworks\tif\tthe\tdata is\tlinearly\tseparable,\tand\tsecond\tit\tis\tquite\tsensitive\tto\toutliers.\tFigure\t5-3\tshows\tthe\tiris\tdataset\twith just\tone\tadditional\toutlier:\ton\tthe\tleft,\tit\tis\timpossible\tto\tfind\ta\thard\tmargin,\tand\ton\tthe\tright\tthe\tdecision boundary\tends\tup\tvery\tdifferent\tfrom\tthe\tone\twe\tsaw\tin\tFigure\t5-1\twithout\tthe\toutlier,\tand\tit\twill probably\tnot\tgeneralize\tas\twell.\n\nFigure\t5-3.\tHard\tmargin\tsensitivity\tto\toutliers\n\nTo\tavoid\tthese\tissues\tit\tis\tpreferable\tto\tuse\ta\tmore\tflexible\tmodel.\tThe\tobjective\tis\tto\tfind\ta\tgood balance\tbetween\tkeeping\tthe\tstreet\tas\tlarge\tas\tpossible\tand\tlimiting\tthe\tmargin\tviolations\t(i.e.,\tinstances that\tend\tup\tin\tthe\tmiddle\tof\tthe\tstreet\tor\teven\ton\tthe\twrong\tside).\tThis\tis\tcalled\tsoft\tmargin classification.\n\nIn\tScikit-Learn’s\tSVM\tclasses,\tyou\tcan\tcontrol\tthis\tbalance\tusing\tthe\tC\thyperparameter:\ta\tsmaller\tC\tvalue leads\tto\ta\twider\tstreet\tbut\tmore\tmargin\tviolations.\tFigure\t5-4\tshows\tthe\tdecision\tboundaries\tand\tmargins of\ttwo\tsoft\tmargin\tSVM\tclassifiers\ton\ta\tnonlinearly\tseparable\tdataset.\tOn\tthe\tleft,\tusing\ta\thigh\tC\tvalue the\tclassifier\tmakes\tfewer\tmargin\tviolations\tbut\tends\tup\twith\ta\tsmaller\tmargin.\tOn\tthe\tright,\tusing\ta\tlow C\tvalue\tthe\tmargin\tis\tmuch\tlarger,\tbut\tmany\tinstances\tend\tup\ton\tthe\tstreet.\tHowever,\tit\tseems\tlikely\tthat the\tsecond\tclassifier\twill\tgeneralize\tbetter:\tin\tfact\teven\ton\tthis\ttraining\tset\tit\tmakes\tfewer\tprediction errors,\tsince\tmost\tof\tthe\tmargin\tviolations\tare\tactually\ton\tthe\tcorrect\tside\tof\tthe\tdecision\tboundary.\n\nFigure\t5-4.\tFewer\tmargin\tviolations\tversus\tlarge\tmargin\n\nTIP\n\nIf\tyour\tSVM\tmodel\tis\toverfitting,\tyou\tcan\ttry\tregularizing\tit\tby\treducing\tC.",
      "page_number": 143
    },
    {
      "number": 5,
      "title": "Support\tVector\tMachines",
      "start_page": 191,
      "end_page": 214,
      "detection_method": "regex_chapter_title",
      "content": "The\tfollowing\tScikit-Learn\tcode\tloads\tthe\tiris\tdataset,\tscales\tthe\tfeatures,\tand\tthen\ttrains\ta\tlinear\tSVM model\t(using\tthe\tLinearSVC\tclass\twith\tC\t=\t0.1\tand\tthe\thinge\tloss\tfunction,\tdescribed\tshortly)\tto\tdetect Iris-Virginica\tflowers.\tThe\tresulting\tmodel\tis\trepresented\ton\tthe\tright\tof\tFigure\t5-4.\n\nimport\tnumpy\tas\tnp from\tsklearn\timport\tdatasets from\tsklearn.pipeline\timport\tPipeline from\tsklearn.preprocessing\timport\tStandardScaler from\tsklearn.svm\timport\tLinearSVC\n\niris\t=\tdatasets.load_iris() X\t=\tiris[\"data\"][:,\t(2,\t3)]\t\t#\tpetal\tlength,\tpetal\twidth y\t=\t(iris[\"target\"]\t==\t2).astype(np.float64)\t\t#\tIris-Virginica\n\nsvm_clf\t=\tPipeline(( \t\t\t\t\t\t\t\t(\"scaler\",\tStandardScaler()), \t\t\t\t\t\t\t\t(\"linear_svc\",\tLinearSVC(C=1,\tloss=\"hinge\")), \t\t\t\t))\n\nsvm_clf.fit(X,\ty)\n\nThen,\tas\tusual,\tyou\tcan\tuse\tthe\tmodel\tto\tmake\tpredictions:\n\n>>>\tsvm_clf.predict([[5.5,\t1.7]]) array([\t1.])\n\nNOTE\n\nUnlike\tLogistic\tRegression\tclassifiers,\tSVM\tclassifiers\tdo\tnot\toutput\tprobabilities\tfor\teach\tclass.\n\nAlternatively,\tyou\tcould\tuse\tthe\tSVC\tclass,\tusing\tSVC(kernel=\"linear\",\tC=1),\tbut\tit\tis\tmuch\tslower, especially\twith\tlarge\ttraining\tsets,\tso\tit\tis\tnot\trecommended.\tAnother\toption\tis\tto\tuse\tthe\tSGDClassifier class,\twith\tSGDClassifier(loss=\"hinge\",\talpha=1/(m*C)).\tThis\tapplies\tregular\tStochastic Gradient\tDescent\t(see\tChapter\t4)\tto\ttrain\ta\tlinear\tSVM\tclassifier.\tIt\tdoes\tnot\tconverge\tas\tfast\tas\tthe LinearSVC\tclass,\tbut\tit\tcan\tbe\tuseful\tto\thandle\thuge\tdatasets\tthat\tdo\tnot\tfit\tin\tmemory\t(out-of-core training),\tor\tto\thandle\tonline\tclassification\ttasks.\n\nTIP\n\nThe\tLinearSVC\tclass\tregularizes\tthe\tbias\tterm,\tso\tyou\tshould\tcenter\tthe\ttraining\tset\tfirst\tby\tsubtracting\tits\tmean.\tThis\tis automatic\tif\tyou\tscale\tthe\tdata\tusing\tthe\tStandardScaler.\tMoreover,\tmake\tsure\tyou\tset\tthe\tloss\thyperparameter\tto\t\"hinge\",\tas it\tis\tnot\tthe\tdefault\tvalue.\tFinally,\tfor\tbetter\tperformance\tyou\tshould\tset\tthe\tdual\thyperparameter\tto\tFalse,\tunless\tthere\tare\tmore features\tthan\ttraining\tinstances\t(we\twill\tdiscuss\tduality\tlater\tin\tthe\tchapter).\n\nNonlinear\tSVM\tClassification Although\tlinear\tSVM\tclassifiers\tare\tefficient\tand\twork\tsurprisingly\twell\tin\tmany\tcases,\tmany\tdatasets are\tnot\teven\tclose\tto\tbeing\tlinearly\tseparable.\tOne\tapproach\tto\thandling\tnonlinear\tdatasets\tis\tto\tadd\tmore features,\tsuch\tas\tpolynomial\tfeatures\t(as\tyou\tdid\tin\tChapter\t4);\tin\tsome\tcases\tthis\tcan\tresult\tin\ta\tlinearly separable\tdataset.\tConsider\tthe\tleft\tplot\tin\tFigure\t5-5:\tit\trepresents\ta\tsimple\tdataset\twith\tjust\tone\tfeature x1.\tThis\tdataset\tis\tnot\tlinearly\tseparable,\tas\tyou\tcan\tsee.\tBut\tif\tyou\tadd\ta\tsecond\tfeature\tx2\t=\t(x1)2,\tthe resulting\t2D\tdataset\tis\tperfectly\tlinearly\tseparable.\n\nFigure\t5-5.\tAdding\tfeatures\tto\tmake\ta\tdataset\tlinearly\tseparable\n\nTo\timplement\tthis\tidea\tusing\tScikit-Learn,\tyou\tcan\tcreate\ta\tPipeline\tcontaining\ta\tPolynomialFeatures transformer\t(discussed\tin\t“Polynomial\tRegression”),\tfollowed\tby\ta\tStandardScaler\tand\ta\tLinearSVC. Let’s\ttest\tthis\ton\tthe\tmoons\tdataset\t(see\tFigure\t5-6):\n\nfrom\tsklearn.datasets\timport\tmake_moons from\tsklearn.pipeline\timport\tPipeline from\tsklearn.preprocessing\timport\tPolynomialFeatures\n\npolynomial_svm_clf\t=\tPipeline(( \t\t\t\t\t\t\t\t(\"poly_features\",\tPolynomialFeatures(degree=3)), \t\t\t\t\t\t\t\t(\"scaler\",\tStandardScaler()), \t\t\t\t\t\t\t\t(\"svm_clf\",\tLinearSVC(C=10,\tloss=\"hinge\")) \t\t\t\t))\n\npolynomial_svm_clf.fit(X,\ty)\n\nFigure\t5-6.\tLinear\tSVM\tclassifier\tusing\tpolynomial\tfeatures\n\nPolynomial\tKernel Adding\tpolynomial\tfeatures\tis\tsimple\tto\timplement\tand\tcan\twork\tgreat\twith\tall\tsorts\tof\tMachine\tLearning algorithms\t(not\tjust\tSVMs),\tbut\tat\ta\tlow\tpolynomial\tdegree\tit\tcannot\tdeal\twith\tvery\tcomplex\tdatasets, and\twith\ta\thigh\tpolynomial\tdegree\tit\tcreates\ta\thuge\tnumber\tof\tfeatures,\tmaking\tthe\tmodel\ttoo\tslow.\n\nFortunately,\twhen\tusing\tSVMs\tyou\tcan\tapply\tan\talmost\tmiraculous\tmathematical\ttechnique\tcalled\tthe kernel\ttrick\t(it\tis\texplained\tin\ta\tmoment).\tIt\tmakes\tit\tpossible\tto\tget\tthe\tsame\tresult\tas\tif\tyou\tadded\tmany polynomial\tfeatures,\teven\twith\tvery\thigh-degree\tpolynomials,\twithout\tactually\thaving\tto\tadd\tthem.\tSo there\tis\tno\tcombinatorial\texplosion\tof\tthe\tnumber\tof\tfeatures\tsince\tyou\tdon’t\tactually\tadd\tany\tfeatures. This\ttrick\tis\timplemented\tby\tthe\tSVC\tclass.\tLet’s\ttest\tit\ton\tthe\tmoons\tdataset:\n\nfrom\tsklearn.svm\timport\tSVC poly_kernel_svm_clf\t=\tPipeline(( \t\t\t\t\t\t\t\t(\"scaler\",\tStandardScaler()), \t\t\t\t\t\t\t\t(\"svm_clf\",\tSVC(kernel=\"poly\",\tdegree=3,\tcoef0=1,\tC=5)) \t\t\t\t)) poly_kernel_svm_clf.fit(X,\ty)\n\nThis\tcode\ttrains\tan\tSVM\tclassifier\tusing\ta\t3rd-degree\tpolynomial\tkernel.\tIt\tis\trepresented\ton\tthe\tleft\tof Figure\t5-7.\tOn\tthe\tright\tis\tanother\tSVM\tclassifier\tusing\ta\t10th-degree\tpolynomial\tkernel.\tObviously,\tif your\tmodel\tis\toverfitting,\tyou\tmight\twant\tto\treduce\tthe\tpolynomial\tdegree.\tConversely,\tif\tit\tis underfitting,\tyou\tcan\ttry\tincreasing\tit.\tThe\thyperparameter\tcoef0\tcontrols\thow\tmuch\tthe\tmodel\tis influenced\tby\thigh-degree\tpolynomials\tversus\tlow-degree\tpolynomials.\n\nFigure\t5-7.\tSVM\tclassifiers\twith\ta\tpolynomial\tkernel\n\nTIP\n\nA\tcommon\tapproach\tto\tfind\tthe\tright\thyperparameter\tvalues\tis\tto\tuse\tgrid\tsearch\t(see\tChapter\t2).\tIt\tis\toften\tfaster\tto\tfirst\tdo\ta very\tcoarse\tgrid\tsearch,\tthen\ta\tfiner\tgrid\tsearch\taround\tthe\tbest\tvalues\tfound.\tHaving\ta\tgood\tsense\tof\twhat\teach hyperparameter\tactually\tdoes\tcan\talso\thelp\tyou\tsearch\tin\tthe\tright\tpart\tof\tthe\thyperparameter\tspace.\n\nAdding\tSimilarity\tFeatures Another\ttechnique\tto\ttackle\tnonlinear\tproblems\tis\tto\tadd\tfeatures\tcomputed\tusing\ta\tsimilarity\tfunction that\tmeasures\thow\tmuch\teach\tinstance\tresembles\ta\tparticular\tlandmark.\tFor\texample,\tlet’s\ttake\tthe\tone- dimensional\tdataset\tdiscussed\tearlier\tand\tadd\ttwo\tlandmarks\tto\tit\tat\tx1\t=\t–2\tand\tx1\t=\t1\t(see\tthe\tleft\tplot in\tFigure\t5-8).\tNext,\tlet’s\tdefine\tthe\tsimilarity\tfunction\tto\tbe\tthe\tGaussian\tRadial\tBasis\tFunction\t(RBF) with\tγ\t=\t0.3\t(see\tEquation\t5-1).\n\nEquation\t5-1.\tGaussian\tRBF\n\nIt\tis\ta\tbell-shaped\tfunction\tvarying\tfrom\t0\t(very\tfar\taway\tfrom\tthe\tlandmark)\tto\t1\t(at\tthe\tlandmark).\tNow we\tare\tready\tto\tcompute\tthe\tnew\tfeatures.\tFor\texample,\tlet’s\tlook\tat\tthe\tinstance\tx1\t=\t–1:\tit\tis\tlocated\tat\ta distance\tof\t1\tfrom\tthe\tfirst\tlandmark,\tand\t2\tfrom\tthe\tsecond\tlandmark.\tTherefore\tits\tnew\tfeatures\tare\tx2\t= exp\t(–0.3\t×\t12)\t≈\t0.74\tand\tx3\t=\texp\t(–0.3\t×\t22)\t≈\t0.30.\tThe\tplot\ton\tthe\tright\tof\tFigure\t5-8\tshows\tthe transformed\tdataset\t(dropping\tthe\toriginal\tfeatures).\tAs\tyou\tcan\tsee,\tit\tis\tnow\tlinearly\tseparable.\n\nFigure\t5-8.\tSimilarity\tfeatures\tusing\tthe\tGaussian\tRBF\n\nYou\tmay\twonder\thow\tto\tselect\tthe\tlandmarks.\tThe\tsimplest\tapproach\tis\tto\tcreate\ta\tlandmark\tat\tthe location\tof\teach\tand\tevery\tinstance\tin\tthe\tdataset.\tThis\tcreates\tmany\tdimensions\tand\tthus\tincreases\tthe chances\tthat\tthe\ttransformed\ttraining\tset\twill\tbe\tlinearly\tseparable.\tThe\tdownside\tis\tthat\ta\ttraining\tset with\tm\tinstances\tand\tn\tfeatures\tgets\ttransformed\tinto\ta\ttraining\tset\twith\tm\tinstances\tand\tm\tfeatures (assuming\tyou\tdrop\tthe\toriginal\tfeatures).\tIf\tyour\ttraining\tset\tis\tvery\tlarge,\tyou\tend\tup\twith\tan\tequally large\tnumber\tof\tfeatures.\n\nGaussian\tRBF\tKernel Just\tlike\tthe\tpolynomial\tfeatures\tmethod,\tthe\tsimilarity\tfeatures\tmethod\tcan\tbe\tuseful\twith\tany\tMachine Learning\talgorithm,\tbut\tit\tmay\tbe\tcomputationally\texpensive\tto\tcompute\tall\tthe\tadditional\tfeatures, especially\ton\tlarge\ttraining\tsets.\tHowever,\tonce\tagain\tthe\tkernel\ttrick\tdoes\tits\tSVM\tmagic:\tit\tmakes\tit possible\tto\tobtain\ta\tsimilar\tresult\tas\tif\tyou\thad\tadded\tmany\tsimilarity\tfeatures,\twithout\tactually\thaving\tto add\tthem.\tLet’s\ttry\tthe\tGaussian\tRBF\tkernel\tusing\tthe\tSVC\tclass:\n\nrbf_kernel_svm_clf\t=\tPipeline(( \t\t\t\t\t\t\t\t(\"scaler\",\tStandardScaler()), \t\t\t\t\t\t\t\t(\"svm_clf\",\tSVC(kernel=\"rbf\",\tgamma=5,\tC=0.001)) \t\t\t\t)) rbf_kernel_svm_clf.fit(X,\ty)\n\nThis\tmodel\tis\trepresented\ton\tthe\tbottom\tleft\tof\tFigure\t5-9.\tThe\tother\tplots\tshow\tmodels\ttrained\twith different\tvalues\tof\thyperparameters\tgamma\t(γ)\tand\tC.\tIncreasing\tgamma\tmakes\tthe\tbell-shape\tcurve narrower\t(see\tthe\tleft\tplot\tof\tFigure\t5-8),\tand\tas\ta\tresult\teach\tinstance’s\trange\tof\tinfluence\tis\tsmaller:\tthe decision\tboundary\tends\tup\tbeing\tmore\tirregular,\twiggling\taround\tindividual\tinstances.\tConversely,\ta small\tgamma\tvalue\tmakes\tthe\tbell-shaped\tcurve\twider,\tso\tinstances\thave\ta\tlarger\trange\tof\tinfluence,\tand the\tdecision\tboundary\tends\tup\tsmoother.\tSo\tγ\tacts\tlike\ta\tregularization\thyperparameter:\tif\tyour\tmodel\tis overfitting,\tyou\tshould\treduce\tit,\tand\tif\tit\tis\tunderfitting,\tyou\tshould\tincrease\tit\t(similar\tto\tthe\tC hyperparameter).\n\nFigure\t5-9.\tSVM\tclassifiers\tusing\tan\tRBF\tkernel\n\nOther\tkernels\texist\tbut\tare\tused\tmuch\tmore\trarely.\tFor\texample,\tsome\tkernels\tare\tspecialized\tfor\tspecific data\tstructures.\tString\tkernels\tare\tsometimes\tused\twhen\tclassifying\ttext\tdocuments\tor\tDNA\tsequences (e.g.,\tusing\tthe\tstring\tsubsequence\tkernel\tor\tkernels\tbased\ton\tthe\tLevenshtein\tdistance).\n\nTIP\n\nWith\tso\tmany\tkernels\tto\tchoose\tfrom,\thow\tcan\tyou\tdecide\twhich\tone\tto\tuse?\tAs\ta\trule\tof\tthumb,\tyou\tshould\talways\ttry\tthe\tlinear kernel\tfirst\t(remember\tthat\tLinearSVC\tis\tmuch\tfaster\tthan\tSVC(kernel=\"linear\")),\tespecially\tif\tthe\ttraining\tset\tis\tvery\tlarge\tor if\tit\thas\tplenty\tof\tfeatures.\tIf\tthe\ttraining\tset\tis\tnot\ttoo\tlarge,\tyou\tshould\ttry\tthe\tGaussian\tRBF\tkernel\tas\twell;\tit\tworks\twell\tin most\tcases.\tThen\tif\tyou\thave\tspare\ttime\tand\tcomputing\tpower,\tyou\tcan\talso\texperiment\twith\ta\tfew\tother\tkernels\tusing\tcross- validation\tand\tgrid\tsearch,\tespecially\tif\tthere\tare\tkernels\tspecialized\tfor\tyour\ttraining\tset’s\tdata\tstructure.\n\nComputational\tComplexity The\tLinearSVC\tclass\tis\tbased\ton\tthe\tliblinear\tlibrary,\twhich\timplements\tan\toptimized\talgorithm\tfor linear\tSVMs.1\tIt\tdoes\tnot\tsupport\tthe\tkernel\ttrick,\tbut\tit\tscales\talmost\tlinearly\twith\tthe\tnumber\tof\ttraining instances\tand\tthe\tnumber\tof\tfeatures:\tits\ttraining\ttime\tcomplexity\tis\troughly\tO(m\t×\tn).\n\nThe\talgorithm\ttakes\tlonger\tif\tyou\trequire\ta\tvery\thigh\tprecision.\tThis\tis\tcontrolled\tby\tthe\ttolerance hyperparameter\tϵ\t(called\ttol\tin\tScikit-Learn).\tIn\tmost\tclassification\ttasks,\tthe\tdefault\ttolerance\tis\tfine.\n\nThe\tSVC\tclass\tis\tbased\ton\tthe\tlibsvm\tlibrary,\twhich\timplements\tan\talgorithm\tthat\tsupports\tthe\tkernel trick.2\tThe\ttraining\ttime\tcomplexity\tis\tusually\tbetween\tO(m2\t×\tn)\tand\tO(m3\t×\tn).\tUnfortunately,\tthis means\tthat\tit\tgets\tdreadfully\tslow\twhen\tthe\tnumber\tof\ttraining\tinstances\tgets\tlarge\t(e.g.,\thundreds\tof thousands\tof\tinstances).\tThis\talgorithm\tis\tperfect\tfor\tcomplex\tbut\tsmall\tor\tmedium\ttraining\tsets. However,\tit\tscales\twell\twith\tthe\tnumber\tof\tfeatures,\tespecially\twith\tsparse\tfeatures\t(i.e.,\twhen\teach instance\thas\tfew\tnonzero\tfeatures).\tIn\tthis\tcase,\tthe\talgorithm\tscales\troughly\twith\tthe\taverage\tnumber\tof nonzero\tfeatures\tper\tinstance.\tTable\t5-1\tcompares\tScikit-Learn’s\tSVM\tclassification\tclasses.\n\nTable\t5-1.\tComparison\tof\tScikit-Learn\tclasses\tfor\tSVM\tclassification\n\nClass\n\nTime\tcomplexity\n\nOut-of-core\tsupport Scaling\trequired Kernel\ttrick\n\nLinearSVC\n\nO(m\t×\tn)\n\nNo\n\nYes\n\nNo\n\nSGDClassifier O(m\t×\tn)\n\nYes\n\nYes\n\nNo\n\nSVC\n\nO(m²\t×\tn)\tto\tO(m³\t×\tn) No\n\nYes\n\nYes\n\nSVM\tRegression As\twe\tmentioned\tearlier,\tthe\tSVM\talgorithm\tis\tquite\tversatile:\tnot\tonly\tdoes\tit\tsupport\tlinear\tand nonlinear\tclassification,\tbut\tit\talso\tsupports\tlinear\tand\tnonlinear\tregression.\tThe\ttrick\tis\tto\treverse\tthe objective:\tinstead\tof\ttrying\tto\tfit\tthe\tlargest\tpossible\tstreet\tbetween\ttwo\tclasses\twhile\tlimiting\tmargin violations,\tSVM\tRegression\ttries\tto\tfit\tas\tmany\tinstances\tas\tpossible\ton\tthe\tstreet\twhile\tlimiting\tmargin violations\t(i.e.,\tinstances\toff\tthe\tstreet).\tThe\twidth\tof\tthe\tstreet\tis\tcontrolled\tby\ta\thyperparameter\tϵ. Figure\t5-10\tshows\ttwo\tlinear\tSVM\tRegression\tmodels\ttrained\ton\tsome\trandom\tlinear\tdata,\tone\twith\ta large\tmargin\t(ϵ\t=\t1.5)\tand\tthe\tother\twith\ta\tsmall\tmargin\t(ϵ\t=\t0.5).\n\nFigure\t5-10.\tSVM\tRegression\n\nAdding\tmore\ttraining\tinstances\twithin\tthe\tmargin\tdoes\tnot\taffect\tthe\tmodel’s\tpredictions;\tthus,\tthe\tmodel is\tsaid\tto\tbe\tϵ-insensitive.\n\nYou\tcan\tuse\tScikit-Learn’s\tLinearSVR\tclass\tto\tperform\tlinear\tSVM\tRegression.\tThe\tfollowing\tcode produces\tthe\tmodel\trepresented\ton\tthe\tleft\tof\tFigure\t5-10\t(the\ttraining\tdata\tshould\tbe\tscaled\tand\tcentered first):\n\nfrom\tsklearn.svm\timport\tLinearSVR\n\nsvm_reg\t=\tLinearSVR(epsilon=1.5) svm_reg.fit(X,\ty)\n\nTo\ttackle\tnonlinear\tregression\ttasks,\tyou\tcan\tuse\ta\tkernelized\tSVM\tmodel.\tFor\texample,\tFigure\t5-11 shows\tSVM\tRegression\ton\ta\trandom\tquadratic\ttraining\tset,\tusing\ta\t2nd-degree\tpolynomial\tkernel.\tThere is\tlittle\tregularization\ton\tthe\tleft\tplot\t(i.e.,\ta\tlarge\tC\tvalue),\tand\tmuch\tmore\tregularization\ton\tthe\tright\tplot (i.e.,\ta\tsmall\tC\tvalue).\n\nFigure\t5-11.\tSVM\tregression\tusing\ta\t2nd-degree\tpolynomial\tkernel\n\nThe\tfollowing\tcode\tproduces\tthe\tmodel\trepresented\ton\tthe\tleft\tof\tFigure\t5-11\tusing\tScikit-Learn’s\tSVR class\t(which\tsupports\tthe\tkernel\ttrick).\tThe\tSVR\tclass\tis\tthe\tregression\tequivalent\tof\tthe\tSVC\tclass,\tand the\tLinearSVR\tclass\tis\tthe\tregression\tequivalent\tof\tthe\tLinearSVC\tclass.\tThe\tLinearSVR\tclass\tscales linearly\twith\tthe\tsize\tof\tthe\ttraining\tset\t(just\tlike\tthe\tLinearSVC\tclass),\twhile\tthe\tSVR\tclass\tgets\tmuch\ttoo slow\twhen\tthe\ttraining\tset\tgrows\tlarge\t(just\tlike\tthe\tSVC\tclass).\n\nfrom\tsklearn.svm\timport\tSVR\n\nsvm_poly_reg\t=\tSVR(kernel=\"poly\",\tdegree=2,\tC=100,\tepsilon=0.1) svm_poly_reg.fit(X,\ty)\n\nNOTE\n\nSVMs\tcan\talso\tbe\tused\tfor\toutlier\tdetection;\tsee\tScikit-Learn’s\tdocumentation\tfor\tmore\tdetails.\n\nUnder\tthe\tHood This\tsection\texplains\thow\tSVMs\tmake\tpredictions\tand\thow\ttheir\ttraining\talgorithms\twork,\tstarting\twith linear\tSVM\tclassifiers.\tYou\tcan\tsafely\tskip\tit\tand\tgo\tstraight\tto\tthe\texercises\tat\tthe\tend\tof\tthis\tchapter\tif you\tare\tjust\tgetting\tstarted\twith\tMachine\tLearning,\tand\tcome\tback\tlater\twhen\tyou\twant\tto\tget\ta\tdeeper understanding\tof\tSVMs.\n\nFirst,\ta\tword\tabout\tnotations:\tin\tChapter\t4\twe\tused\tthe\tconvention\tof\tputting\tall\tthe\tmodel\tparameters\tin one\tvector\tθ,\tincluding\tthe\tbias\tterm\tθ0\tand\tthe\tinput\tfeature\tweights\tθ1\tto\tθn,\tand\tadding\ta\tbias\tinput\tx0\t= 1\tto\tall\tinstances.\tIn\tthis\tchapter,\twe\twill\tuse\ta\tdifferent\tconvention,\twhich\tis\tmore\tconvenient\t(and\tmore common)\twhen\tyou\tare\tdealing\twith\tSVMs:\tthe\tbias\tterm\twill\tbe\tcalled\tb\tand\tthe\tfeature\tweights\tvector will\tbe\tcalled\tw.\tNo\tbias\tfeature\twill\tbe\tadded\tto\tthe\tinput\tfeature\tvectors.\n\nDecision\tFunction\tand\tPredictions The\tlinear\tSVM\tclassifier\tmodel\tpredicts\tthe\tclass\tof\ta\tnew\tinstance\tx\tby\tsimply\tcomputing\tthe\tdecision function\twT\t·\tx\t+\tb\t=\tw1\tx1\t+\t\t+\t wn\txn\t+\tb:\tif\tthe\tresult\tis\tpositive,\tthe\tpredicted\tclass\tŷ\tis\tthe\tpositive class\t(1),\tor\telse\tit\tis\tthe\tnegative\tclass\t(0);\tsee\tEquation\t5-2.\n\nEquation\t5-2.\tLinear\tSVM\tclassifier\tprediction\n\nFigure\t5-12\tshows\tthe\tdecision\tfunction\tthat\tcorresponds\tto\tthe\tmodel\ton\tthe\tright\tof\tFigure\t5-4:\tit\tis\ta two-dimensional\tplane\tsince\tthis\tdataset\thas\ttwo\tfeatures\t(petal\twidth\tand\tpetal\tlength).\tThe\tdecision boundary\tis\tthe\tset\tof\tpoints\twhere\tthe\tdecision\tfunction\tis\tequal\tto\t0:\tit\tis\tthe\tintersection\tof\ttwo\tplanes, which\tis\ta\tstraight\tline\t(represented\tby\tthe\tthick\tsolid\tline).3\n\nFigure\t5-12.\tDecision\tfunction\tfor\tthe\tiris\tdataset\n\nThe\tdashed\tlines\trepresent\tthe\tpoints\twhere\tthe\tdecision\tfunction\tis\tequal\tto\t1\tor\t–1:\tthey\tare\tparallel\tand at\tequal\tdistance\tto\tthe\tdecision\tboundary,\tforming\ta\tmargin\taround\tit.\tTraining\ta\tlinear\tSVM\tclassifier means\tfinding\tthe\tvalue\tof\tw\tand\tb\tthat\tmake\tthis\tmargin\tas\twide\tas\tpossible\twhile\tavoiding\tmargin violations\t(hard\tmargin)\tor\tlimiting\tthem\t(soft\tmargin).\n\nTraining\tObjective Consider\tthe\tslope\tof\tthe\tdecision\tfunction:\tit\tis\tequal\tto\tthe\tnorm\tof\tthe\tweight\tvector,\t\t w\t.\tIf\twe divide\tthis\tslope\tby\t2,\tthe\tpoints\twhere\tthe\tdecision\tfunction\tis\tequal\tto\t±1\tare\tgoing\tto\tbe\ttwice\tas\tfar away\tfrom\tthe\tdecision\tboundary.\tIn\tother\twords,\tdividing\tthe\tslope\tby\t2\twill\tmultiply\tthe\tmargin\tby\t2. Perhaps\tthis\tis\teasier\tto\tvisualize\tin\t2D\tin\tFigure\t5-13.\tThe\tsmaller\tthe\tweight\tvector\tw,\tthe\tlarger\tthe margin.\n\nFigure\t5-13.\tA\tsmaller\tweight\tvector\tresults\tin\ta\tlarger\tmargin\n\nSo\twe\twant\tto\tminimize\t\t w\t\tto\tget\ta\tlarge\tmargin.\tHowever,\tif\twe\talso\twant\tto\tavoid\tany\tmargin violation\t(hard\tmargin),\tthen\twe\tneed\tthe\tdecision\tfunction\tto\tbe\tgreater\tthan\t1\tfor\tall\tpositive\ttraining instances,\tand\tlower\tthan\t–1\tfor\tnegative\ttraining\tinstances.\tIf\twe\tdefine\tt(i)\t=\t–1\tfor\tnegative\tinstances\t(if y(i)\t=\t0)\tand\tt(i)\t=\t1\tfor\tpositive\tinstances\t(if\ty(i)\t=\t1),\tthen\twe\tcan\texpress\tthis\tconstraint\tas\tt(i)(wT\t·\tx(i)\t+ b)\t≥\t1\tfor\tall\tinstances.\n\nWe\tcan\ttherefore\texpress\tthe\thard\tmargin\tlinear\tSVM\tclassifier\tobjective\tas\tthe\tconstrained optimization\tproblem\tin\tEquation\t5-3.\n\nEquation\t5-3.\tHard\tmargin\tlinear\tSVM\tclassifier\tobjective\n\nNOTE\n\nWe\tare\tminimizing\t wT\t·\tw,\twhich\tis\tequal\tto\n\nw\t2,\trather\tthan\tminimizing\t\t w\t.\tThis\tis\tbecause\tit\twill\tgive\tthe\tsame\n\nw\t2\thas\ta\tnice\tand\tsimple result\t(since\tthe\tvalues\tof\tw\tand\tb\tthat\tminimize\ta\tvalue\talso\tminimize\thalf\tof\tits\tsquare),\tbut\t derivative\t(it\tis\tjust\tw)\twhile\t\t w\t\tis\tnot\tdifferentiable\tat\t w\t=\t0.\tOptimization\talgorithms\twork\tmuch\tbetter\ton\tdifferentiable functions.\n\nTo\tget\tthe\tsoft\tmargin\tobjective,\twe\tneed\tto\tintroduce\ta\tslack\tvariable\tζ(i)\t≥\t0\tfor\teach\tinstance:4\tζ(i) measures\thow\tmuch\tthe\tith\tinstance\tis\tallowed\tto\tviolate\tthe\tmargin.\tWe\tnow\thave\ttwo\tconflicting\n\nobjectives:\tmaking\tthe\tslack\tvariables\tas\tsmall\tas\tpossible\tto\treduce\tthe\tmargin\tviolations,\tand\tmaking\t wT\t·\tw\tas\tsmall\tas\tpossible\tto\tincrease\tthe\tmargin.\tThis\tis\twhere\tthe\tC\thyperparameter\tcomes\tin:\tit\n\nallows\tus\tto\tdefine\tthe\ttradeoff\tbetween\tthese\ttwo\tobjectives.\tThis\tgives\tus\tthe\tconstrained\toptimization problem\tin\tEquation\t5-4.\n\nEquation\t5-4.\tSoft\tmargin\tlinear\tSVM\tclassifier\tobjective\n\nQuadratic\tProgramming The\thard\tmargin\tand\tsoft\tmargin\tproblems\tare\tboth\tconvex\tquadratic\toptimization\tproblems\twith\tlinear constraints.\tSuch\tproblems\tare\tknown\tas\tQuadratic\tProgramming\t(QP)\tproblems.\tMany\toff-the-shelf solvers\tare\tavailable\tto\tsolve\tQP\tproblems\tusing\ta\tvariety\tof\ttechniques\tthat\tare\toutside\tthe\tscope\tof\tthis book.5\tThe\tgeneral\tproblem\tformulation\tis\tgiven\tby\tEquation\t5-5.\n\nEquation\t5-5.\tQuadratic\tProgramming\tproblem\n\nNote\tthat\tthe\texpression\tA\t·\tp\t≤\tb\tactually\tdefines\tnc\tconstraints:\tpT\t·\ta(i)\t≤\tb(i)\tfor\ti\t=\t1,\t2,\t,\t nc,\twhere a(i)\tis\tthe\tvector\tcontaining\tthe\telements\tof\tthe\tith\trow\tof\tA\tand\tb(i)\tis\tthe\tith\telement\tof\tb.\n\nYou\tcan\teasily\tverify\tthat\tif\tyou\tset\tthe\tQP\tparameters\tin\tthe\tfollowing\tway,\tyou\tget\tthe\thard\tmargin linear\tSVM\tclassifier\tobjective:\n\nnp\t=\tn\t+\t1,\twhere\tn\tis\tthe\tnumber\tof\tfeatures\t(the\t+1\tis\tfor\tthe\tbias\tterm).\n\nnc\t=\tm,\twhere\tm\tis\tthe\tnumber\tof\ttraining\tinstances.\n\nH\tis\tthe\tnp\t×\tnp\tidentity\tmatrix,\texcept\twith\ta\tzero\tin\tthe\ttop-left\tcell\t(to\tignore\tthe\tbias\tterm).\n\nf\t=\t0,\tan\tnp-dimensional\tvector\tfull\tof\t0s.\n\nb\t=\t1,\tan\tnc-dimensional\tvector\tfull\tof\t1s.\n\na(i)\t=\t–t(i)\n\n(i),\twhere\n\n(i)\tis\tequal\tto\tx(i)\twith\tan\textra\tbias\tfeature\n\n0\t=\t1.\n\nSo\tone\tway\tto\ttrain\ta\thard\tmargin\tlinear\tSVM\tclassifier\tis\tjust\tto\tuse\tan\toff-the-shelf\tQP\tsolver\tby passing\tit\tthe\tpreceding\tparameters.\tThe\tresulting\tvector\tp\twill\tcontain\tthe\tbias\tterm\tb\t=\tp0\tand\tthe feature\tweights\twi\t=\tpi\tfor\ti\t=\t1,\t2,\t,\t m.\tSimilarly,\tyou\tcan\tuse\ta\tQP\tsolver\tto\tsolve\tthe\tsoft\tmargin problem\t(see\tthe\texercises\tat\tthe\tend\tof\tthe\tchapter).\n\nHowever,\tto\tuse\tthe\tkernel\ttrick\twe\tare\tgoing\tto\tlook\tat\ta\tdifferent\tconstrained\toptimization\tproblem.\n\nThe\tDual\tProblem Given\ta\tconstrained\toptimization\tproblem,\tknown\tas\tthe\tprimal\tproblem,\tit\tis\tpossible\tto\texpress\ta different\tbut\tclosely\trelated\tproblem,\tcalled\tits\tdual\tproblem.\tThe\tsolution\tto\tthe\tdual\tproblem\ttypically gives\ta\tlower\tbound\tto\tthe\tsolution\tof\tthe\tprimal\tproblem,\tbut\tunder\tsome\tconditions\tit\tcan\teven\thave\tthe same\tsolutions\tas\tthe\tprimal\tproblem.\tLuckily,\tthe\tSVM\tproblem\thappens\tto\tmeet\tthese\tconditions,6\tso you\tcan\tchoose\tto\tsolve\tthe\tprimal\tproblem\tor\tthe\tdual\tproblem;\tboth\twill\thave\tthe\tsame\tsolution. Equation\t5-6\tshows\tthe\tdual\tform\tof\tthe\tlinear\tSVM\tobjective\t(if\tyou\tare\tinterested\tin\tknowing\thow\tto derive\tthe\tdual\tproblem\tfrom\tthe\tprimal\tproblem,\tsee\tAppendix\tC).\n\nEquation\t5-6.\tDual\tform\tof\tthe\tlinear\tSVM\tobjective\n\nOnce\tyou\tfind\tthe\tvector\t that\tminimize\tthe\tprimal\tproblem\tby\tusing\tEquation\t5-7.\n\nthat\tminimizes\tthis\tequation\t(using\ta\tQP\tsolver),\tyou\tcan\tcompute\n\nEquation\t5-7.\tFrom\tthe\tdual\tsolution\tto\tthe\tprimal\tsolution\n\nThe\tdual\tproblem\tis\tfaster\tto\tsolve\tthan\tthe\tprimal\twhen\tthe\tnumber\tof\ttraining\tinstances\tis\tsmaller\tthan the\tnumber\tof\tfeatures.\tMore\timportantly,\tit\tmakes\tthe\tkernel\ttrick\tpossible,\twhile\tthe\tprimal\tdoes\tnot.\tSo what\tis\tthis\tkernel\ttrick\tanyway?\n\nand\n\nKernelized\tSVM Suppose\tyou\twant\tto\tapply\ta\t2nd-degree\tpolynomial\ttransformation\tto\ta\ttwo-dimensional\ttraining\tset (such\tas\tthe\tmoons\ttraining\tset),\tthen\ttrain\ta\tlinear\tSVM\tclassifier\ton\tthe\ttransformed\ttraining\tset. Equation\t5-8\tshows\tthe\t2nd-degree\tpolynomial\tmapping\tfunction\tϕ\tthat\tyou\twant\tto\tapply.\n\nEquation\t5-8.\tSecond-degree\tpolynomial\tmapping\n\nNotice\tthat\tthe\ttransformed\tvector\tis\tthree-dimensional\tinstead\tof\ttwo-dimensional.\tNow\tlet’s\tlook\tat what\thappens\tto\ta\tcouple\tof\ttwo-dimensional\tvectors,\ta\tand\tb,\tif\twe\tapply\tthis\t2nd-degree\tpolynomial mapping\tand\tthen\tcompute\tthe\tdot\tproduct\tof\tthe\ttransformed\tvectors\t(See\tEquation\t5-9).\n\nEquation\t5-9.\tKernel\ttrick\tfor\ta\t2nd-degree\tpolynomial\tmapping\n\nHow\tabout\tthat?\tThe\tdot\tproduct\tof\tthe\ttransformed\tvectors\tis\tequal\tto\tthe\tsquare\tof\tthe\tdot\tproduct\tof\tthe original\tvectors:\tϕ(a)T\t·\tϕ(b)\t=\t(aT\t·\tb)2.\n\nNow\there\tis\tthe\tkey\tinsight:\tif\tyou\tapply\tthe\ttransformation\tϕ\tto\tall\ttraining\tinstances,\tthen\tthe\tdual problem\t(see\tEquation\t5-6)\twill\tcontain\tthe\tdot\tproduct\tϕ(x(i))T\t·\tϕ(x(j)).\tBut\tif\tϕ\tis\tthe\t2nd-degree polynomial\ttransformation\tdefined\tin\tEquation\t5-8,\tthen\tyou\tcan\treplace\tthis\tdot\tproduct\tof\ttransformed\n\nvectors\tsimply\tby\t .\tSo\tyou\tdon’t\tactually\tneed\tto\ttransform\tthe\ttraining\tinstances\tat\tall:\tjust replace\tthe\tdot\tproduct\tby\tits\tsquare\tin\tEquation\t5-6.\tThe\tresult\twill\tbe\tstrictly\tthe\tsame\tas\tif\tyou\twent through\tthe\ttrouble\tof\tactually\ttransforming\tthe\ttraining\tset\tthen\tfitting\ta\tlinear\tSVM\talgorithm,\tbut\tthis trick\tmakes\tthe\twhole\tprocess\tmuch\tmore\tcomputationally\tefficient.\tThis\tis\tthe\tessence\tof\tthe\tkernel trick. The\tfunction\tK(a,\tb)\t=\t(aT\t·\tb)2\tis\tcalled\ta\t2nd-degree\tpolynomial\tkernel.\tIn\tMachine\tLearning,\ta\tkernel is\ta\tfunction\tcapable\tof\tcomputing\tthe\tdot\tproduct\tϕ(a)T\t·\tϕ(b)\tbased\tonly\ton\tthe\toriginal\tvectors\ta\tand\tb,\n\nwithout\thaving\tto\tcompute\t(or\teven\tto\tknow\tabout)\tthe\ttransformation\tϕ.\tEquation\t5-10\tlists\tsome\tof\tthe most\tcommonly\tused\tkernels.\n\nEquation\t5-10.\tCommon\tkernels\n\nMERCER’S\tTHEOREM\n\nAccording\tto\tMercer’s\ttheorem,\tif\ta\tfunction\tK(a,\tb)\trespects\ta\tfew\tmathematical\tconditions\tcalled\tMercer’s\tconditions\t(K\tmust\tbe continuous,\tsymmetric\tin\tits\targuments\tso\tK(a,\tb)\t=\tK(b,\ta),\tetc.),\tthen\tthere\texists\ta\tfunction\tϕ\tthat\tmaps\ta\tand\tb\tinto\tanother\tspace (possibly\twith\tmuch\thigher\tdimensions)\tsuch\tthat\tK(a,\tb)\t=\tϕ(a)T\t·\tϕ(b).\tSo\tyou\tcan\tuse\tK\tas\ta\tkernel\tsince\tyou\tknow\tϕ\texists,\teven\tif you\tdon’t\tknow\twhat\tϕ\tis.\tIn\tthe\tcase\tof\tthe\tGaussian\tRBF\tkernel,\tit\tcan\tbe\tshown\tthat\tϕ\tactually\tmaps\teach\ttraining\tinstance\tto\tan infinite-dimensional\tspace,\tso\tit’s\ta\tgood\tthing\tyou\tdon’t\tneed\tto\tactually\tperform\tthe\tmapping!\n\nNote\tthat\tsome\tfrequently\tused\tkernels\t(such\tas\tthe\tSigmoid\tkernel)\tdon’t\trespect\tall\tof\tMercer’s\tconditions,\tyet\tthey\tgenerally\twork well\tin\tpractice.\n\nThere\tis\tstill\tone\tloose\tend\twe\tmust\ttie.\tEquation\t5-7\tshows\thow\tto\tgo\tfrom\tthe\tdual\tsolution\tto\tthe primal\tsolution\tin\tthe\tcase\tof\ta\tlinear\tSVM\tclassifier,\tbut\tif\tyou\tapply\tthe\tkernel\ttrick\tyou\tend\tup\twith equations\tthat\tinclude\tϕ(x(i)).\tIn\tfact,\t \tmust\thave\tthe\tsame\tnumber\tof\tdimensions\tas\tϕ(x(i)),\twhich\tmay ? be\thuge\tor\teven\tinfinite,\tso\tyou\tcan’t\tcompute\tit.\tBut\thow\tcan\tyou\tmake\tpredictions\twithout\tknowing\t Well,\tthe\tgood\tnews\tis\tthat\tyou\tcan\tplug\tin\tthe\tformula\tfor\t \tfrom\tEquation\t5-7\tinto\tthe\tdecision\tfunction for\ta\tnew\tinstance\tx(n),\tand\tyou\tget\tan\tequation\twith\tonly\tdot\tproducts\tbetween\tinput\tvectors.\tThis\tmakes it\tpossible\tto\tuse\tthe\tkernel\ttrick,\tonce\tagain\t(Equation\t5-11).\n\nEquation\t5-11.\tMaking\tpredictions\twith\ta\tkernelized\tSVM\n\nNote\tthat\tsince\tα(i)\t≠\t0\tonly\tfor\tsupport\tvectors,\tmaking\tpredictions\tinvolves\tcomputing\tthe\tdot\tproduct\tof the\tnew\tinput\tvector\tx(n)\twith\tonly\tthe\tsupport\tvectors,\tnot\tall\tthe\ttraining\tinstances.\tOf\tcourse,\tyou\talso ,\tusing\tthe\tsame\ttrick\t(Equation\t5-12). need\tto\tcompute\tthe\tbias\tterm\n\nEquation\t5-12.\tComputing\tthe\tbias\tterm\tusing\tthe\tkernel\ttrick\n\nIf\tyou\tare\tstarting\tto\tget\ta\theadache,\tit’s\tperfectly\tnormal:\tit’s\tan\tunfortunate\tside\teffects\tof\tthe\tkernel trick.\n\nOnline\tSVMs Before\tconcluding\tthis\tchapter,\tlet’s\ttake\ta\tquick\tlook\tat\tonline\tSVM\tclassifiers\t(recall\tthat\tonline learning\tmeans\tlearning\tincrementally,\ttypically\tas\tnew\tinstances\tarrive).\n\nFor\tlinear\tSVM\tclassifiers,\tone\tmethod\tis\tto\tuse\tGradient\tDescent\t(e.g.,\tusing\tSGDClassifier)\tto minimize\tthe\tcost\tfunction\tin\tEquation\t5-13,\twhich\tis\tderived\tfrom\tthe\tprimal\tproblem.\tUnfortunately\tit converges\tmuch\tmore\tslowly\tthan\tthe\tmethods\tbased\ton\tQP.\n\nEquation\t5-13.\tLinear\tSVM\tclassifier\tcost\tfunction\n\nThe\tfirst\tsum\tin\tthe\tcost\tfunction\twill\tpush\tthe\tmodel\tto\thave\ta\tsmall\tweight\tvector\tw,\tleading\tto\ta\tlarger margin.\tThe\tsecond\tsum\tcomputes\tthe\ttotal\tof\tall\tmargin\tviolations.\tAn\tinstance’s\tmargin\tviolation\tis equal\tto\t0\tif\tit\tis\tlocated\toff\tthe\tstreet\tand\ton\tthe\tcorrect\tside,\tor\telse\tit\tis\tproportional\tto\tthe\tdistance\tto the\tcorrect\tside\tof\tthe\tstreet.\tMinimizing\tthis\tterm\tensures\tthat\tthe\tmodel\tmakes\tthe\tmargin\tviolations\tas small\tand\tas\tfew\tas\tpossible\n\nHINGE\tLOSS\n\nThe\tfunction\tmax(0,\t1\t–\tt)\tis\tcalled\tthe\thinge\tloss\tfunction\t(represented\tbelow).\tIt\tis\tequal\tto\t0\twhen\tt\t≥\t1.\tIts\tderivative\t(slope)\tis\tequal to\t–1\tif\tt\t<\t1\tand\t0\tif\tt\t>\t1.\tIt\tis\tnot\tdifferentiable\tat\tt\t=\t1,\tbut\tjust\tlike\tfor\tLasso\tRegression\t(see\t“Lasso\tRegression”)\tyou\tcan\tstill\tuse Gradient\tDescent\tusing\tany\tsubderivative\tat\tt\t=\t1\t(i.e.,\tany\tvalue\tbetween\t–1\tand\t0).\n\nIt\tis\talso\tpossible\tto\timplement\tonline\tkernelized\tSVMs\t—\tfor\texample,\tusing\t“Incremental\tand Decremental\tSVM\tLearning”7\tor\t“Fast\tKernel\tClassifiers\twith\tOnline\tand\tActive\tLearning.”8\tHowever, these\tare\timplemented\tin\tMatlab\tand\tC++.\tFor\tlarge-scale\tnonlinear\tproblems,\tyou\tmay\twant\tto\tconsider using\tneural\tnetworks\tinstead\t(see\tPart\tII).\n\nExercises\n\n1.\t What\tis\tthe\tfundamental\tidea\tbehind\tSupport\tVector\tMachines?\n\n2.\t What\tis\ta\tsupport\tvector?\n\n3.\t Why\tis\tit\timportant\tto\tscale\tthe\tinputs\twhen\tusing\tSVMs?\n\n4.\t Can\tan\tSVM\tclassifier\toutput\ta\tconfidence\tscore\twhen\tit\tclassifies\tan\tinstance?\tWhat\tabout\ta probability?\n\n5.\t Should\tyou\tuse\tthe\tprimal\tor\tthe\tdual\tform\tof\tthe\tSVM\tproblem\tto\ttrain\ta\tmodel\ton\ta\ttraining\tset with\tmillions\tof\tinstances\tand\thundreds\tof\tfeatures?\n\n6.\t Say\tyou\ttrained\tan\tSVM\tclassifier\twith\tan\tRBF\tkernel.\tIt\tseems\tto\tunderfit\tthe\ttraining\tset:\tshould you\tincrease\tor\tdecrease\tγ\t(gamma)?\tWhat\tabout\tC?\n\n7.\t How\tshould\tyou\tset\tthe\tQP\tparameters\t(H,\tf,\tA,\tand\tb)\tto\tsolve\tthe\tsoft\tmargin\tlinear\tSVM classifier\tproblem\tusing\tan\toff-the-shelf\tQP\tsolver?\n\n8.\t Train\ta\tLinearSVC\ton\ta\tlinearly\tseparable\tdataset.\tThen\ttrain\tan\tSVC\tand\ta\tSGDClassifier\ton\tthe same\tdataset.\tSee\tif\tyou\tcan\tget\tthem\tto\tproduce\troughly\tthe\tsame\tmodel.\n\n9.\t Train\tan\tSVM\tclassifier\ton\tthe\tMNIST\tdataset.\tSince\tSVM\tclassifiers\tare\tbinary\tclassifiers,\tyou will\tneed\tto\tuse\tone-versus-all\tto\tclassify\tall\t10\tdigits.\tYou\tmay\twant\tto\ttune\tthe\thyperparameters using\tsmall\tvalidation\tsets\tto\tspeed\tup\tthe\tprocess.\tWhat\taccuracy\tcan\tyou\treach?\n\n10.\t Train\tan\tSVM\tregressor\ton\tthe\tCalifornia\thousing\tdataset.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“A\tDual\tCoordinate\tDescent\tMethod\tfor\tLarge-scale\tLinear\tSVM,”\tLin\tet\tal.\t(2008).\n\n2\n\n“Sequential\tMinimal\tOptimization\t(SMO),”\tJ.\tPlatt\t(1998).\n\n3\n\nMore\tgenerally,\twhen\tthere\tare\tn\tfeatures,\tthe\tdecision\tfunction\tis\tan\tn-dimensional\thyperplane,\tand\tthe\tdecision\tboundary\tis\tan\t(n\t–\t1)- dimensional\thyperplane.\n\n4\n\nZeta\t(ζ)\tis\tthe\t8\n\nth\n\nletter\tof\tthe\tGreek\talphabet.\n\n5\n\nTo\tlearn\tmore\tabout\tQuadratic\tProgramming,\tyou\tcan\tstart\tby\treading\tStephen\tBoyd\tand\tLieven\tVandenberghe,\tConvex\tOptimization (Cambridge,\tUK:\tCambridge\tUniversity\tPress,\t2004)\tor\twatch\tRichard\tBrown’s\tseries\tof\tvideo\tlectures.\n\n6\n\nThe\tobjective\tfunction\tis\tconvex,\tand\tthe\tinequality\tconstraints\tare\tcontinuously\tdifferentiable\tand\tconvex\tfunctions.\n\n7\n\n“Incremental\tand\tDecremental\tSupport\tVector\tMachine\tLearning,”\tG.\tCauwenberghs,\tT.\tPoggio\t(2001).\n\n8\n\n“Fast\tKernel\tClassifiers\twith\tOnline\tand\tActive\tLearning,“\tA.\tBordes,\tS.\tErtekin,\tJ.\tWeston,\tL.\tBottou\t(2005).\n\nChapter\t6.\tDecision\tTrees\n\nLike\tSVMs,\tDecision\tTrees\tare\tversatile\tMachine\tLearning\talgorithms\tthat\tcan\tperform\tboth classification\tand\tregression\ttasks,\tand\teven\tmultioutput\ttasks.\tThey\tare\tvery\tpowerful\talgorithms, capable\tof\tfitting\tcomplex\tdatasets.\tFor\texample,\tin\tChapter\t2\tyou\ttrained\ta\tDecisionTreeRegressor model\ton\tthe\tCalifornia\thousing\tdataset,\tfitting\tit\tperfectly\t(actually\toverfitting\tit).\n\nDecision\tTrees\tare\talso\tthe\tfundamental\tcomponents\tof\tRandom\tForests\t(see\tChapter\t7),\twhich\tare among\tthe\tmost\tpowerful\tMachine\tLearning\talgorithms\tavailable\ttoday.\n\nIn\tthis\tchapter\twe\twill\tstart\tby\tdiscussing\thow\tto\ttrain,\tvisualize,\tand\tmake\tpredictions\twith\tDecision Trees.\tThen\twe\twill\tgo\tthrough\tthe\tCART\ttraining\talgorithm\tused\tby\tScikit-Learn,\tand\twe\twill\tdiscuss how\tto\tregularize\ttrees\tand\tuse\tthem\tfor\tregression\ttasks.\tFinally,\twe\twill\tdiscuss\tsome\tof\tthe\tlimitations of\tDecision\tTrees.\n\nTraining\tand\tVisualizing\ta\tDecision\tTree To\tunderstand\tDecision\tTrees,\tlet’s\tjust\tbuild\tone\tand\ttake\ta\tlook\tat\thow\tit\tmakes\tpredictions.\tThe following\tcode\ttrains\ta\tDecisionTreeClassifier\ton\tthe\tiris\tdataset\t(see\tChapter\t4):\n\nfrom\tsklearn.datasets\timport\tload_iris from\tsklearn.tree\timport\tDecisionTreeClassifier\n\niris\t=\tload_iris() X\t=\tiris.data[:,\t2:]\t#\tpetal\tlength\tand\twidth y\t=\tiris.target\n\ntree_clf\t=\tDecisionTreeClassifier(max_depth=2) tree_clf.fit(X,\ty)\n\nYou\tcan\tvisualize\tthe\ttrained\tDecision\tTree\tby\tfirst\tusing\tthe\texport_graphviz()\tmethod\tto\toutput\ta graph\tdefinition\tfile\tcalled\tiris_tree.dot:\n\nfrom\tsklearn.tree\timport\texport_graphviz\n\nexport_graphviz( \t\t\t\t\t\t\t\ttree_clf, \t\t\t\t\t\t\t\tout_file=image_path(\"iris_tree.dot\"), \t\t\t\t\t\t\t\tfeature_names=iris.feature_names[2:], \t\t\t\t\t\t\t\tclass_names=iris.target_names, \t\t\t\t\t\t\t\trounded=True, \t\t\t\t\t\t\t\tfilled=True \t\t\t\t)\n\nThen\tyou\tcan\tconvert\tthis\t.dot\tfile\tto\ta\tvariety\tof\tformats\tsuch\tas\tPDF\tor\tPNG\tusing\tthe\tdot\tcommand- line\ttool\tfrom\tthe\tgraphviz\tpackage.1\tThis\tcommand\tline\tconverts\tthe\t.dot\tfile\tto\ta\t.png\timage\tfile:\n\n$\tdot\t-Tpng\tiris_tree.dot\t-o\tiris_tree.png\n\nYour\tfirst\tdecision\ttree\tlooks\tlike\tFigure\t6-1.\n\nFigure\t6-1.\tIris\tDecision\tTree",
      "page_number": 191
    },
    {
      "number": 6,
      "title": "Decision\tTrees",
      "start_page": 215,
      "end_page": 231,
      "detection_method": "regex_chapter_title",
      "content": "Making\tPredictions Let’s\tsee\thow\tthe\ttree\trepresented\tin\tFigure\t6-1\tmakes\tpredictions.\tSuppose\tyou\tfind\tan\tiris\tflower\tand you\twant\tto\tclassify\tit.\tYou\tstart\tat\tthe\troot\tnode\t(depth\t0,\tat\tthe\ttop):\tthis\tnode\tasks\twhether\tthe\tflower’s petal\tlength\tis\tsmaller\tthan\t2.45\tcm.\tIf\tit\tis,\tthen\tyou\tmove\tdown\tto\tthe\troot’s\tleft\tchild\tnode\t(depth\t1, left).\tIn\tthis\tcase,\tit\tis\ta\tleaf\tnode\t(i.e.,\tit\tdoes\tnot\thave\tany\tchildren\tnodes),\tso\tit\tdoes\tnot\task\tany questions:\tyou\tcan\tsimply\tlook\tat\tthe\tpredicted\tclass\tfor\tthat\tnode\tand\tthe\tDecision\tTree\tpredicts\tthat your\tflower\tis\tan\tIris-Setosa\t(class=setosa).\n\nNow\tsuppose\tyou\tfind\tanother\tflower,\tbut\tthis\ttime\tthe\tpetal\tlength\tis\tgreater\tthan\t2.45\tcm.\tYou\tmust move\tdown\tto\tthe\troot’s\tright\tchild\tnode\t(depth\t1,\tright),\twhich\tis\tnot\ta\tleaf\tnode,\tso\tit\tasks\tanother question:\tis\tthe\tpetal\twidth\tsmaller\tthan\t1.75\tcm?\tIf\tit\tis,\tthen\tyour\tflower\tis\tmost\tlikely\tan\tIris- Versicolor\t(depth\t2,\tleft).\tIf\tnot,\tit\tis\tlikely\tan\tIris-Virginica\t(depth\t2,\tright).\tIt’s\treally\tthat\tsimple.\n\nNOTE\n\nOne\tof\tthe\tmany\tqualities\tof\tDecision\tTrees\tis\tthat\tthey\trequire\tvery\tlittle\tdata\tpreparation.\tIn\tparticular,\tthey\tdon’t\trequire feature\tscaling\tor\tcentering\tat\tall.\n\nA\tnode’s\tsamples\tattribute\tcounts\thow\tmany\ttraining\tinstances\tit\tapplies\tto.\tFor\texample,\t100\ttraining instances\thave\ta\tpetal\tlength\tgreater\tthan\t2.45\tcm\t(depth\t1,\tright),\tamong\twhich\t54\thave\ta\tpetal\twidth smaller\tthan\t1.75\tcm\t(depth\t2,\tleft).\tA\tnode’s\tvalue\tattribute\ttells\tyou\thow\tmany\ttraining\tinstances\tof each\tclass\tthis\tnode\tapplies\tto:\tfor\texample,\tthe\tbottom-right\tnode\tapplies\tto\t0\tIris-Setosa,\t1\tIris- Versicolor,\tand\t45\tIris-Virginica.\tFinally,\ta\tnode’s\tgini\tattribute\tmeasures\tits\timpurity:\ta\tnode\tis\t“pure” (gini=0)\tif\tall\ttraining\tinstances\tit\tapplies\tto\tbelong\tto\tthe\tsame\tclass.\tFor\texample,\tsince\tthe\tdepth-1 left\tnode\tapplies\tonly\tto\tIris-Setosa\ttraining\tinstances,\tit\tis\tpure\tand\tits\tgini\tscore\tis\t0.\tEquation\t6-1 shows\thow\tthe\ttraining\talgorithm\tcomputes\tthe\tgini\tscore\tGi\tof\tthe\tith\tnode.\tFor\texample,\tthe\tdepth-2\tleft node\thas\ta\tgini\tscore\tequal\tto\t1\t–\t(0/54)2\t–\t(49/54)2\t–\t(5/54)2\t≈\t0.168.\tAnother\timpurity\tmeasure\tis discussed\tshortly.\n\nEquation\t6-1.\tGini\timpurity\n\npi,k\tis\tthe\tratio\tof\tclass\tk\tinstances\tamong\tthe\ttraining\tinstances\tin\tthe\tith\tnode.\n\nNOTE\n\nScikit-Learn\tuses\tthe\tCART\talgorithm,\twhich\tproduces\tonly\tbinary\ttrees:\tnonleaf\tnodes\talways\thave\ttwo\tchildren\t(i.e., questions\tonly\thave\tyes/no\tanswers).\tHowever,\tother\talgorithms\tsuch\tas\tID3\tcan\tproduce\tDecision\tTrees\twith\tnodes\tthat\thave more\tthan\ttwo\tchildren.\n\nFigure\t6-2\tshows\tthis\tDecision\tTree’s\tdecision\tboundaries.\tThe\tthick\tvertical\tline\trepresents\tthe\tdecision boundary\tof\tthe\troot\tnode\t(depth\t0):\tpetal\tlength\t=\t2.45\tcm.\tSince\tthe\tleft\tarea\tis\tpure\t(only\tIris-Setosa), it\tcannot\tbe\tsplit\tany\tfurther.\tHowever,\tthe\tright\tarea\tis\timpure,\tso\tthe\tdepth-1\tright\tnode\tsplits\tit\tat\tpetal width\t=\t1.75\tcm\t(represented\tby\tthe\tdashed\tline).\tSince\tmax_depth\twas\tset\tto\t2,\tthe\tDecision\tTree\tstops right\tthere.\tHowever,\tif\tyou\tset\tmax_depth\tto\t3,\tthen\tthe\ttwo\tdepth-2\tnodes\twould\teach\tadd\tanother decision\tboundary\t(represented\tby\tthe\tdotted\tlines).\n\nFigure\t6-2.\tDecision\tTree\tdecision\tboundaries\n\nMODEL\tINTERPRETATION:\tWHITE\tBOX\tVERSUS\tBLACK\tBOX\n\nAs\tyou\tcan\tsee\tDecision\tTrees\tare\tfairly\tintuitive\tand\ttheir\tdecisions\tare\teasy\tto\tinterpret.\tSuch\tmodels\tare\toften\tcalled\twhite\tbox models.\tIn\tcontrast,\tas\twe\twill\tsee,\tRandom\tForests\tor\tneural\tnetworks\tare\tgenerally\tconsidered\tblack\tbox\tmodels.\tThey\tmake\tgreat predictions,\tand\tyou\tcan\teasily\tcheck\tthe\tcalculations\tthat\tthey\tperformed\tto\tmake\tthese\tpredictions;\tnevertheless,\tit\tis\tusually\thard\tto explain\tin\tsimple\tterms\twhy\tthe\tpredictions\twere\tmade.\tFor\texample,\tif\ta\tneural\tnetwork\tsays\tthat\ta\tparticular\tperson\tappears\ton\ta picture,\tit\tis\thard\tto\tknow\twhat\tactually\tcontributed\tto\tthis\tprediction:\tdid\tthe\tmodel\trecognize\tthat\tperson’s\teyes?\tHer\tmouth?\tHer\tnose? Her\tshoes?\tOr\teven\tthe\tcouch\tthat\tshe\twas\tsitting\ton?\tConversely,\tDecision\tTrees\tprovide\tnice\tand\tsimple\tclassification\trules\tthat\tcan even\tbe\tapplied\tmanually\tif\tneed\tbe\t(e.g.,\tfor\tflower\tclassification).\n\nEstimating\tClass\tProbabilities A\tDecision\tTree\tcan\talso\testimate\tthe\tprobability\tthat\tan\tinstance\tbelongs\tto\ta\tparticular\tclass\tk:\tfirst\tit traverses\tthe\ttree\tto\tfind\tthe\tleaf\tnode\tfor\tthis\tinstance,\tand\tthen\tit\treturns\tthe\tratio\tof\ttraining\tinstances\tof class\tk\tin\tthis\tnode.\tFor\texample,\tsuppose\tyou\thave\tfound\ta\tflower\twhose\tpetals\tare\t5\tcm\tlong\tand\t1.5 cm\twide.\tThe\tcorresponding\tleaf\tnode\tis\tthe\tdepth-2\tleft\tnode,\tso\tthe\tDecision\tTree\tshould\toutput\tthe following\tprobabilities:\t0%\tfor\tIris-Setosa\t(0/54),\t90.7%\tfor\tIris-Versicolor\t(49/54),\tand\t9.3%\tfor\tIris- Virginica\t(5/54).\tAnd\tof\tcourse\tif\tyou\task\tit\tto\tpredict\tthe\tclass,\tit\tshould\toutput\tIris-Versicolor\t(class\t1) since\tit\thas\tthe\thighest\tprobability.\tLet’s\tcheck\tthis:\n\n>>>\ttree_clf.predict_proba([[5,\t1.5]]) array([[\t0.\t,\t\t0.90740741,\t\t0.09259259]]) >>>\ttree_clf.predict([[5,\t1.5]]) array([1])\n\nPerfect!\tNotice\tthat\tthe\testimated\tprobabilities\twould\tbe\tidentical\tanywhere\telse\tin\tthe\tbottom-right rectangle\tof\tFigure\t6-2\t—\tfor\texample,\tif\tthe\tpetals\twere\t6\tcm\tlong\tand\t1.5\tcm\twide\t(even\tthough\tit seems\tobvious\tthat\tit\twould\tmost\tlikely\tbe\tan\tIris-Virginica\tin\tthis\tcase).\n\nThe\tCART\tTraining\tAlgorithm Scikit-Learn\tuses\tthe\tClassification\tAnd\tRegression\tTree\t(CART)\talgorithm\tto\ttrain\tDecision\tTrees\t(also called\t“growing”\ttrees).\tThe\tidea\tis\treally\tquite\tsimple:\tthe\talgorithm\tfirst\tsplits\tthe\ttraining\tset\tin\ttwo subsets\tusing\ta\tsingle\tfeature\tk\tand\ta\tthreshold\ttk\t(e.g.,\t“petal\tlength\t≤\t2.45\tcm”).\tHow\tdoes\tit\tchoose\tk and\ttk?\tIt\tsearches\tfor\tthe\tpair\t(k,\ttk)\tthat\tproduces\tthe\tpurest\tsubsets\t(weighted\tby\ttheir\tsize).\tThe\tcost function\tthat\tthe\talgorithm\ttries\tto\tminimize\tis\tgiven\tby\tEquation\t6-2.\n\nEquation\t6-2.\tCART\tcost\tfunction\tfor\tclassification\n\nOnce\tit\thas\tsuccessfully\tsplit\tthe\ttraining\tset\tin\ttwo,\tit\tsplits\tthe\tsubsets\tusing\tthe\tsame\tlogic,\tthen\tthe\tsub- subsets\tand\tso\ton,\trecursively.\tIt\tstops\trecursing\tonce\tit\treaches\tthe\tmaximum\tdepth\t(defined\tby\tthe max_depth\thyperparameter),\tor\tif\tit\tcannot\tfind\ta\tsplit\tthat\twill\treduce\timpurity.\tA\tfew\tother hyperparameters\t(described\tin\ta\tmoment)\tcontrol\tadditional\tstopping\tconditions\t(min_samples_split, min_samples_leaf,\tmin_weight_fraction_leaf,\tand\tmax_leaf_nodes).\n\nWARNING\n\nAs\tyou\tcan\tsee,\tthe\tCART\talgorithm\tis\ta\tgreedy\talgorithm:\tit\tgreedily\tsearches\tfor\tan\toptimum\tsplit\tat\tthe\ttop\tlevel,\tthen repeats\tthe\tprocess\tat\teach\tlevel.\tIt\tdoes\tnot\tcheck\twhether\tor\tnot\tthe\tsplit\twill\tlead\tto\tthe\tlowest\tpossible\timpurity\tseveral\tlevels down.\tA\tgreedy\talgorithm\toften\tproduces\ta\treasonably\tgood\tsolution,\tbut\tit\tis\tnot\tguaranteed\tto\tbe\tthe\toptimal\tsolution.\n\nUnfortunately,\tfinding\tthe\toptimal\ttree\tis\tknown\tto\tbe\tan\tNP-Complete\tproblem:2\tit\trequires\tO(exp(m)) time,\tmaking\tthe\tproblem\tintractable\teven\tfor\tfairly\tsmall\ttraining\tsets.\tThis\tis\twhy\twe\tmust\tsettle\tfor\ta “reasonably\tgood”\tsolution.\n\nComputational\tComplexity Making\tpredictions\trequires\ttraversing\tthe\tDecision\tTree\tfrom\tthe\troot\tto\ta\tleaf.\tDecision\tTrees\tare generally\tapproximately\tbalanced,\tso\ttraversing\tthe\tDecision\tTree\trequires\tgoing\tthrough\troughly O(log2(m))\tnodes.3\tSince\teach\tnode\tonly\trequires\tchecking\tthe\tvalue\tof\tone\tfeature,\tthe\toverall prediction\tcomplexity\tis\tjust\tO(log2(m)),\tindependent\tof\tthe\tnumber\tof\tfeatures.\tSo\tpredictions\tare\tvery fast,\teven\twhen\tdealing\twith\tlarge\ttraining\tsets.\n\nHowever,\tthe\ttraining\talgorithm\tcompares\tall\tfeatures\t(or\tless\tif\tmax_features\tis\tset)\ton\tall\tsamples\tat each\tnode.\tThis\tresults\tin\ta\ttraining\tcomplexity\tof\tO(n\t×\tm\tlog(m)).\tFor\tsmall\ttraining\tsets\t(less\tthan\ta few\tthousand\tinstances),\tScikit-Learn\tcan\tspeed\tup\ttraining\tby\tpresorting\tthe\tdata\t(set\tpresort=True), but\tthis\tslows\tdown\ttraining\tconsiderably\tfor\tlarger\ttraining\tsets.\n\nGini\tImpurity\tor\tEntropy? By\tdefault,\tthe\tGini\timpurity\tmeasure\tis\tused,\tbut\tyou\tcan\tselect\tthe\tentropy\timpurity\tmeasure\tinstead\tby setting\tthe\tcriterion\thyperparameter\tto\t\"entropy\".\tThe\tconcept\tof\tentropy\toriginated\tin thermodynamics\tas\ta\tmeasure\tof\tmolecular\tdisorder:\tentropy\tapproaches\tzero\twhen\tmolecules\tare\tstill and\twell\tordered.\tIt\tlater\tspread\tto\ta\twide\tvariety\tof\tdomains,\tincluding\tShannon’s\tinformation\ttheory, where\tit\tmeasures\tthe\taverage\tinformation\tcontent\tof\ta\tmessage:4\tentropy\tis\tzero\twhen\tall\tmessages\tare identical.\tIn\tMachine\tLearning,\tit\tis\tfrequently\tused\tas\tan\timpurity\tmeasure:\ta\tset’s\tentropy\tis\tzero\twhen it\tcontains\tinstances\tof\tonly\tone\tclass.\tEquation\t6-3\tshows\tthe\tdefinition\tof\tthe\tentropy\tof\tthe\tith\tnode.\n\nFor\texample,\tthe\tdepth-2\tleft\tnode\tin\tFigure\t6-1\thas\tan\tentropy\tequal\tto\t ≈\t0.31.\n\nEquation\t6-3.\tEntropy\n\nSo\tshould\tyou\tuse\tGini\timpurity\tor\tentropy?\tThe\ttruth\tis,\tmost\tof\tthe\ttime\tit\tdoes\tnot\tmake\ta\tbig difference:\tthey\tlead\tto\tsimilar\ttrees.\tGini\timpurity\tis\tslightly\tfaster\tto\tcompute,\tso\tit\tis\ta\tgood\tdefault. However,\twhen\tthey\tdiffer,\tGini\timpurity\ttends\tto\tisolate\tthe\tmost\tfrequent\tclass\tin\tits\town\tbranch\tof\tthe tree,\twhile\tentropy\ttends\tto\tproduce\tslightly\tmore\tbalanced\ttrees.5\n\nRegularization\tHyperparameters Decision\tTrees\tmake\tvery\tfew\tassumptions\tabout\tthe\ttraining\tdata\t(as\topposed\tto\tlinear\tmodels,\twhich obviously\tassume\tthat\tthe\tdata\tis\tlinear,\tfor\texample).\tIf\tleft\tunconstrained,\tthe\ttree\tstructure\twill\tadapt itself\tto\tthe\ttraining\tdata,\tfitting\tit\tvery\tclosely,\tand\tmost\tlikely\toverfitting\tit.\tSuch\ta\tmodel\tis\toften\tcalled a\tnonparametric\tmodel,\tnot\tbecause\tit\tdoes\tnot\thave\tany\tparameters\t(it\toften\thas\ta\tlot)\tbut\tbecause\tthe number\tof\tparameters\tis\tnot\tdetermined\tprior\tto\ttraining,\tso\tthe\tmodel\tstructure\tis\tfree\tto\tstick\tclosely\tto the\tdata.\tIn\tcontrast,\ta\tparametric\tmodel\tsuch\tas\ta\tlinear\tmodel\thas\ta\tpredetermined\tnumber\tof parameters,\tso\tits\tdegree\tof\tfreedom\tis\tlimited,\treducing\tthe\trisk\tof\toverfitting\t(but\tincreasing\tthe\trisk\tof underfitting).\n\nTo\tavoid\toverfitting\tthe\ttraining\tdata,\tyou\tneed\tto\trestrict\tthe\tDecision\tTree’s\tfreedom\tduring\ttraining.\tAs you\tknow\tby\tnow,\tthis\tis\tcalled\tregularization.\tThe\tregularization\thyperparameters\tdepend\ton\tthe algorithm\tused,\tbut\tgenerally\tyou\tcan\tat\tleast\trestrict\tthe\tmaximum\tdepth\tof\tthe\tDecision\tTree.\tIn\tScikit- Learn,\tthis\tis\tcontrolled\tby\tthe\tmax_depth\thyperparameter\t(the\tdefault\tvalue\tis\tNone,\twhich\tmeans unlimited).\tReducing\tmax_depth\twill\tregularize\tthe\tmodel\tand\tthus\treduce\tthe\trisk\tof\toverfitting.\n\nThe\tDecisionTreeClassifier\tclass\thas\ta\tfew\tother\tparameters\tthat\tsimilarly\trestrict\tthe\tshape\tof\tthe Decision\tTree:\tmin_samples_split\t(the\tminimum\tnumber\tof\tsamples\ta\tnode\tmust\thave\tbefore\tit\tcan\tbe split),\tmin_samples_leaf\t(the\tminimum\tnumber\tof\tsamples\ta\tleaf\tnode\tmust\thave), min_weight_fraction_leaf\t(same\tas\tmin_samples_leaf\tbut\texpressed\tas\ta\tfraction\tof\tthe\ttotal number\tof\tweighted\tinstances),\tmax_leaf_nodes\t(maximum\tnumber\tof\tleaf\tnodes),\tand\tmax_features (maximum\tnumber\tof\tfeatures\tthat\tare\tevaluated\tfor\tsplitting\tat\teach\tnode).\tIncreasing\tmin_* hyperparameters\tor\treducing\tmax_*\thyperparameters\twill\tregularize\tthe\tmodel.\n\nNOTE\n\nOther\talgorithms\twork\tby\tfirst\ttraining\tthe\tDecision\tTree\twithout\trestrictions,\tthen\tpruning\t(deleting)\tunnecessary\tnodes.\tA\tnode whose\tchildren\tare\tall\tleaf\tnodes\tis\tconsidered\tunnecessary\tif\tthe\tpurity\timprovement\tit\tprovides\tis\tnot\tstatistically\tsignificant. Standard\tstatistical\ttests,\tsuch\tas\tthe\tχ2\ttest,\tare\tused\tto\testimate\tthe\tprobability\tthat\tthe\timprovement\tis\tpurely\tthe\tresult\tof chance\t(which\tis\tcalled\tthe\tnull\thypothesis).\tIf\tthis\tprobability,\tcalled\tthe\tp-value,\tis\thigher\tthan\ta\tgiven\tthreshold\t(typically\t5%, controlled\tby\ta\thyperparameter),\tthen\tthe\tnode\tis\tconsidered\tunnecessary\tand\tits\tchildren\tare\tdeleted.\tThe\tpruning\tcontinues\tuntil all\tunnecessary\tnodes\thave\tbeen\tpruned.\n\nFigure\t6-3\tshows\ttwo\tDecision\tTrees\ttrained\ton\tthe\tmoons\tdataset\t(introduced\tin\tChapter\t5).\tOn\tthe\tleft, the\tDecision\tTree\tis\ttrained\twith\tthe\tdefault\thyperparameters\t(i.e.,\tno\trestrictions),\tand\ton\tthe\tright\tthe Decision\tTree\tis\ttrained\twith\tmin_samples_leaf=4.\tIt\tis\tquite\tobvious\tthat\tthe\tmodel\ton\tthe\tleft\tis overfitting,\tand\tthe\tmodel\ton\tthe\tright\twill\tprobably\tgeneralize\tbetter.\n\nFigure\t6-3.\tRegularization\tusing\tmin_samples_leaf\n\nRegression Decision\tTrees\tare\talso\tcapable\tof\tperforming\tregression\ttasks.\tLet’s\tbuild\ta\tregression\ttree\tusing\tScikit- Learn’s\tDecisionTreeRegressor\tclass,\ttraining\tit\ton\ta\tnoisy\tquadratic\tdataset\twith\tmax_depth=2:\n\nfrom\tsklearn.tree\timport\tDecisionTreeRegressor\n\ntree_reg\t=\tDecisionTreeRegressor(max_depth=2) tree_reg.fit(X,\ty)\n\nThe\tresulting\ttree\tis\trepresented\ton\tFigure\t6-4.\n\nFigure\t6-4.\tA\tDecision\tTree\tfor\tregression\n\nThis\ttree\tlooks\tvery\tsimilar\tto\tthe\tclassification\ttree\tyou\tbuilt\tearlier.\tThe\tmain\tdifference\tis\tthat\tinstead of\tpredicting\ta\tclass\tin\teach\tnode,\tit\tpredicts\ta\tvalue.\tFor\texample,\tsuppose\tyou\twant\tto\tmake\ta prediction\tfor\ta\tnew\tinstance\twith\tx1\t=\t0.6.\tYou\ttraverse\tthe\ttree\tstarting\tat\tthe\troot,\tand\tyou\teventually reach\tthe\tleaf\tnode\tthat\tpredicts\tvalue=0.1106.\tThis\tprediction\tis\tsimply\tthe\taverage\ttarget\tvalue\tof\tthe 110\ttraining\tinstances\tassociated\tto\tthis\tleaf\tnode.\tThis\tprediction\tresults\tin\ta\tMean\tSquared\tError (MSE)\tequal\tto\t0.0151\tover\tthese\t110\tinstances.\n\nThis\tmodel’s\tpredictions\tare\trepresented\ton\tthe\tleft\tof\tFigure\t6-5.\tIf\tyou\tset\tmax_depth=3,\tyou\tget\tthe predictions\trepresented\ton\tthe\tright.\tNotice\thow\tthe\tpredicted\tvalue\tfor\teach\tregion\tis\talways\tthe\taverage target\tvalue\tof\tthe\tinstances\tin\tthat\tregion.\tThe\talgorithm\tsplits\teach\tregion\tin\ta\tway\tthat\tmakes\tmost training\tinstances\tas\tclose\tas\tpossible\tto\tthat\tpredicted\tvalue.\n\nFigure\t6-5.\tPredictions\tof\ttwo\tDecision\tTree\tregression\tmodels\n\nThe\tCART\talgorithm\tworks\tmostly\tthe\tsame\tway\tas\tearlier,\texcept\tthat\tinstead\tof\ttrying\tto\tsplit\tthe training\tset\tin\ta\tway\tthat\tminimizes\timpurity,\tit\tnow\ttries\tto\tsplit\tthe\ttraining\tset\tin\ta\tway\tthat\tminimizes the\tMSE.\tEquation\t6-4\tshows\tthe\tcost\tfunction\tthat\tthe\talgorithm\ttries\tto\tminimize.\n\nEquation\t6-4.\tCART\tcost\tfunction\tfor\tregression\n\nJust\tlike\tfor\tclassification\ttasks,\tDecision\tTrees\tare\tprone\tto\toverfitting\twhen\tdealing\twith\tregression tasks.\tWithout\tany\tregularization\t(i.e.,\tusing\tthe\tdefault\thyperparameters),\tyou\tget\tthe\tpredictions\ton\tthe left\tof\tFigure\t6-6.\tIt\tis\tobviously\toverfitting\tthe\ttraining\tset\tvery\tbadly.\tJust\tsetting min_samples_leaf=10\tresults\tin\ta\tmuch\tmore\treasonable\tmodel,\trepresented\ton\tthe\tright\tof\tFigure\t6-6.\n\nFigure\t6-6.\tRegularizing\ta\tDecision\tTree\tregressor\n\nInstability Hopefully\tby\tnow\tyou\tare\tconvinced\tthat\tDecision\tTrees\thave\ta\tlot\tgoing\tfor\tthem:\tthey\tare\tsimple\tto understand\tand\tinterpret,\teasy\tto\tuse,\tversatile,\tand\tpowerful.\tHowever\tthey\tdo\thave\ta\tfew\tlimitations. First,\tas\tyou\tmay\thave\tnoticed,\tDecision\tTrees\tlove\torthogonal\tdecision\tboundaries\t(all\tsplits\tare perpendicular\tto\tan\taxis),\twhich\tmakes\tthem\tsensitive\tto\ttraining\tset\trotation.\tFor\texample,\tFigure\t6-7 shows\ta\tsimple\tlinearly\tseparable\tdataset:\ton\tthe\tleft,\ta\tDecision\tTree\tcan\tsplit\tit\teasily,\twhile\ton\tthe right,\tafter\tthe\tdataset\tis\trotated\tby\t45°,\tthe\tdecision\tboundary\tlooks\tunnecessarily\tconvoluted.\tAlthough both\tDecision\tTrees\tfit\tthe\ttraining\tset\tperfectly,\tit\tis\tvery\tlikely\tthat\tthe\tmodel\ton\tthe\tright\twill\tnot generalize\twell.\tOne\tway\tto\tlimit\tthis\tproblem\tis\tto\tuse\tPCA\t(see\tChapter\t8),\twhich\toften\tresults\tin\ta better\torientation\tof\tthe\ttraining\tdata.\n\nFigure\t6-7.\tSensitivity\tto\ttraining\tset\trotation\n\nMore\tgenerally,\tthe\tmain\tissue\twith\tDecision\tTrees\tis\tthat\tthey\tare\tvery\tsensitive\tto\tsmall\tvariations\tin the\ttraining\tdata.\tFor\texample,\tif\tyou\tjust\tremove\tthe\twidest\tIris-Versicolor\tfrom\tthe\tiris\ttraining\tset\t(the one\twith\tpetals\t4.8\tcm\tlong\tand\t1.8\tcm\twide)\tand\ttrain\ta\tnew\tDecision\tTree,\tyou\tmay\tget\tthe\tmodel represented\tin\tFigure\t6-8.\tAs\tyou\tcan\tsee,\tit\tlooks\tvery\tdifferent\tfrom\tthe\tprevious\tDecision\tTree (Figure\t6-2).\tActually,\tsince\tthe\ttraining\talgorithm\tused\tby\tScikit-Learn\tis\tstochastic6\tyou\tmay\tget\tvery different\tmodels\teven\ton\tthe\tsame\ttraining\tdata\t(unless\tyou\tset\tthe\trandom_state\thyperparameter).\n\nFigure\t6-8.\tSensitivity\tto\ttraining\tset\tdetails\n\nRandom\tForests\tcan\tlimit\tthis\tinstability\tby\taveraging\tpredictions\tover\tmany\ttrees,\tas\twe\twill\tsee\tin\tthe next\tchapter.\n\nExercises\n\n1.\t What\tis\tthe\tapproximate\tdepth\tof\ta\tDecision\tTree\ttrained\t(without\trestrictions)\ton\ta\ttraining\tset\twith 1\tmillion\tinstances?\n\n2.\t Is\ta\tnode’s\tGini\timpurity\tgenerally\tlower\tor\tgreater\tthan\tits\tparent’s?\tIs\tit\tgenerally\tlower/greater, or\talways\tlower/greater?\n\n3.\t If\ta\tDecision\tTree\tis\toverfitting\tthe\ttraining\tset,\tis\tit\ta\tgood\tidea\tto\ttry\tdecreasing\tmax_depth?\n\n4.\t If\ta\tDecision\tTree\tis\tunderfitting\tthe\ttraining\tset,\tis\tit\ta\tgood\tidea\tto\ttry\tscaling\tthe\tinput\tfeatures?\n\n5.\t If\tit\ttakes\tone\thour\tto\ttrain\ta\tDecision\tTree\ton\ta\ttraining\tset\tcontaining\t1\tmillion\tinstances,\troughly how\tmuch\ttime\twill\tit\ttake\tto\ttrain\tanother\tDecision\tTree\ton\ta\ttraining\tset\tcontaining\t10\tmillion instances?\n\n6.\t If\tyour\ttraining\tset\tcontains\t100,000\tinstances,\twill\tsetting\tpresort=True\tspeed\tup\ttraining?\n\n7.\t Train\tand\tfine-tune\ta\tDecision\tTree\tfor\tthe\tmoons\tdataset.\n\na.\t Generate\ta\tmoons\tdataset\tusing\tmake_moons(n_samples=10000,\tnoise=0.4).\n\nb.\t Split\tit\tinto\ta\ttraining\tset\tand\ta\ttest\tset\tusing\ttrain_test_split().\n\nc.\t Use\tgrid\tsearch\twith\tcross-validation\t(with\tthe\thelp\tof\tthe\tGridSearchCV\tclass)\tto\tfind\tgood\n\nhyperparameter\tvalues\tfor\ta\tDecisionTreeClassifier.\tHint:\ttry\tvarious\tvalues\tfor max_leaf_nodes.\n\nd.\t Train\tit\ton\tthe\tfull\ttraining\tset\tusing\tthese\thyperparameters,\tand\tmeasure\tyour\tmodel’s\n\nperformance\ton\tthe\ttest\tset.\tYou\tshould\tget\troughly\t85%\tto\t87%\taccuracy.\n\n8.\t Grow\ta\tforest.\n\na.\t Continuing\tthe\tprevious\texercise,\tgenerate\t1,000\tsubsets\tof\tthe\ttraining\tset,\teach\tcontaining\t100 instances\tselected\trandomly.\tHint:\tyou\tcan\tuse\tScikit-Learn’s\tShuffleSplit\tclass\tfor\tthis.\n\nb.\t Train\tone\tDecision\tTree\ton\teach\tsubset,\tusing\tthe\tbest\thyperparameter\tvalues\tfound\tabove. Evaluate\tthese\t1,000\tDecision\tTrees\ton\tthe\ttest\tset.\tSince\tthey\twere\ttrained\ton\tsmaller\tsets, these\tDecision\tTrees\twill\tlikely\tperform\tworse\tthan\tthe\tfirst\tDecision\tTree,\tachieving\tonly about\t80%\taccuracy.\n\nc.\t Now\tcomes\tthe\tmagic.\tFor\teach\ttest\tset\tinstance,\tgenerate\tthe\tpredictions\tof\tthe\t1,000\tDecision Trees,\tand\tkeep\tonly\tthe\tmost\tfrequent\tprediction\t(you\tcan\tuse\tSciPy’s\tmode()\tfunction\tfor this).\tThis\tgives\tyou\tmajority-vote\tpredictions\tover\tthe\ttest\tset.\n\nd.\t Evaluate\tthese\tpredictions\ton\tthe\ttest\tset:\tyou\tshould\tobtain\ta\tslightly\thigher\taccuracy\tthan\tyour first\tmodel\t(about\t0.5\tto\t1.5%\thigher).\tCongratulations,\tyou\thave\ttrained\ta\tRandom\tForest classifier!\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nGraphviz\tis\tan\topen\tsource\tgraph\tvisualization\tsoftware\tpackage,\tavailable\tat\thttp://www.graphviz.org/.\n\n2\n\nP\tis\tthe\tset\tof\tproblems\tthat\tcan\tbe\tsolved\tin\tpolynomial\ttime.\tNP\tis\tthe\tset\tof\tproblems\twhose\tsolutions\tcan\tbe\tverified\tin\tpolynomial\ttime. An\tNP-Hard\tproblem\tis\ta\tproblem\tto\twhich\tany\tNP\tproblem\tcan\tbe\treduced\tin\tpolynomial\ttime.\tAn\tNP-Complete\tproblem\tis\tboth\tNP\tand NP-Hard.\tA\tmajor\topen\tmathematical\tquestion\tis\twhether\tor\tnot\tP\t=\tNP.\tIf\tP\t≠\tNP\t(which\tseems\tlikely),\tthen\tno\tpolynomial\talgorithm\twill ever\tbe\tfound\tfor\tany\tNP-Complete\tproblem\t(except\tperhaps\ton\ta\tquantum\tcomputer).\n\n3\n\nlog2\tis\tthe\tbinary\tlogarithm.\tIt\tis\tequal\tto\tlog2(m)\t=\tlog(m)\t/\tlog(2).\n\n4\n\nA\treduction\tof\tentropy\tis\toften\tcalled\tan\tinformation\tgain.\n\n5\n\nSee\tSebastian\tRaschka’s\tinteresting\tanalysis\tfor\tmore\tdetails.\n\n6\n\nIt\trandomly\tselects\tthe\tset\tof\tfeatures\tto\tevaluate\tat\teach\tnode.\n\nChapter\t7.\tEnsemble\tLearning\tand\tRandom Forests\n\nSuppose\tyou\task\ta\tcomplex\tquestion\tto\tthousands\tof\trandom\tpeople,\tthen\taggregate\ttheir\tanswers.\tIn many\tcases\tyou\twill\tfind\tthat\tthis\taggregated\tanswer\tis\tbetter\tthan\tan\texpert’s\tanswer.\tThis\tis\tcalled\tthe wisdom\tof\tthe\tcrowd.\tSimilarly,\tif\tyou\taggregate\tthe\tpredictions\tof\ta\tgroup\tof\tpredictors\t(such\tas classifiers\tor\tregressors),\tyou\twill\toften\tget\tbetter\tpredictions\tthan\twith\tthe\tbest\tindividual\tpredictor.\tA group\tof\tpredictors\tis\tcalled\tan\tensemble;\tthus,\tthis\ttechnique\tis\tcalled\tEnsemble\tLearning,\tand\tan Ensemble\tLearning\talgorithm\tis\tcalled\tan\tEnsemble\tmethod.\n\nFor\texample,\tyou\tcan\ttrain\ta\tgroup\tof\tDecision\tTree\tclassifiers,\teach\ton\ta\tdifferent\trandom\tsubset\tof\tthe training\tset.\tTo\tmake\tpredictions,\tyou\tjust\tobtain\tthe\tpredictions\tof\tall\tindividual\ttrees,\tthen\tpredict\tthe class\tthat\tgets\tthe\tmost\tvotes\t(see\tthe\tlast\texercise\tin\tChapter\t6).\tSuch\tan\tensemble\tof\tDecision\tTrees\tis called\ta\tRandom\tForest,\tand\tdespite\tits\tsimplicity,\tthis\tis\tone\tof\tthe\tmost\tpowerful\tMachine\tLearning algorithms\tavailable\ttoday.\n\nMoreover,\tas\twe\tdiscussed\tin\tChapter\t2,\tyou\twill\toften\tuse\tEnsemble\tmethods\tnear\tthe\tend\tof\ta\tproject, once\tyou\thave\talready\tbuilt\ta\tfew\tgood\tpredictors,\tto\tcombine\tthem\tinto\tan\teven\tbetter\tpredictor.\tIn\tfact, the\twinning\tsolutions\tin\tMachine\tLearning\tcompetitions\toften\tinvolve\tseveral\tEnsemble\tmethods\t(most famously\tin\tthe\tNetflix\tPrize\tcompetition).\n\nIn\tthis\tchapter\twe\twill\tdiscuss\tthe\tmost\tpopular\tEnsemble\tmethods,\tincluding\tbagging,\tboosting, stacking,\tand\ta\tfew\tothers.\tWe\twill\talso\texplore\tRandom\tForests.\n\nVoting\tClassifiers Suppose\tyou\thave\ttrained\ta\tfew\tclassifiers,\teach\tone\tachieving\tabout\t80%\taccuracy.\tYou\tmay\thave\ta Logistic\tRegression\tclassifier,\tan\tSVM\tclassifier,\ta\tRandom\tForest\tclassifier,\ta\tK-Nearest\tNeighbors classifier,\tand\tperhaps\ta\tfew\tmore\t(see\tFigure\t7-1).\n\nFigure\t7-1.\tTraining\tdiverse\tclassifiers\n\nA\tvery\tsimple\tway\tto\tcreate\tan\teven\tbetter\tclassifier\tis\tto\taggregate\tthe\tpredictions\tof\teach\tclassifier\tand predict\tthe\tclass\tthat\tgets\tthe\tmost\tvotes.\tThis\tmajority-vote\tclassifier\tis\tcalled\ta\thard\tvoting\tclassifier (see\tFigure\t7-2).\n\nFigure\t7-2.\tHard\tvoting\tclassifier\tpredictions\n\nSomewhat\tsurprisingly,\tthis\tvoting\tclassifier\toften\tachieves\ta\thigher\taccuracy\tthan\tthe\tbest\tclassifier\tin the\tensemble.\tIn\tfact,\teven\tif\teach\tclassifier\tis\ta\tweak\tlearner\t(meaning\tit\tdoes\tonly\tslightly\tbetter\tthan random\tguessing),\tthe\tensemble\tcan\tstill\tbe\ta\tstrong\tlearner\t(achieving\thigh\taccuracy),\tprovided\tthere are\ta\tsufficient\tnumber\tof\tweak\tlearners\tand\tthey\tare\tsufficiently\tdiverse.\n\nHow\tis\tthis\tpossible?\tThe\tfollowing\tanalogy\tcan\thelp\tshed\tsome\tlight\ton\tthis\tmystery.\tSuppose\tyou\thave\n\na\tslightly\tbiased\tcoin\tthat\thas\ta\t51%\tchance\tof\tcoming\tup\theads,\tand\t49%\tchance\tof\tcoming\tup\ttails.\tIf you\ttoss\tit\t1,000\ttimes,\tyou\twill\tgenerally\tget\tmore\tor\tless\t510\theads\tand\t490\ttails,\tand\thence\ta\tmajority of\theads.\tIf\tyou\tdo\tthe\tmath,\tyou\twill\tfind\tthat\tthe\tprobability\tof\tobtaining\ta\tmajority\tof\theads\tafter\t1,000 tosses\tis\tclose\tto\t75%.\tThe\tmore\tyou\ttoss\tthe\tcoin,\tthe\thigher\tthe\tprobability\t(e.g.,\twith\t10,000\ttosses,\tthe probability\tclimbs\tover\t97%).\tThis\tis\tdue\tto\tthe\tlaw\tof\tlarge\tnumbers:\tas\tyou\tkeep\ttossing\tthe\tcoin,\tthe ratio\tof\theads\tgets\tcloser\tand\tcloser\tto\tthe\tprobability\tof\theads\t(51%).\tFigure\t7-3\tshows\t10\tseries\tof biased\tcoin\ttosses.\tYou\tcan\tsee\tthat\tas\tthe\tnumber\tof\ttosses\tincreases,\tthe\tratio\tof\theads\tapproaches\t51%. Eventually\tall\t10\tseries\tend\tup\tso\tclose\tto\t51%\tthat\tthey\tare\tconsistently\tabove\t50%.\n\nFigure\t7-3.\tThe\tlaw\tof\tlarge\tnumbers\n\nSimilarly,\tsuppose\tyou\tbuild\tan\tensemble\tcontaining\t1,000\tclassifiers\tthat\tare\tindividually\tcorrect\tonly 51%\tof\tthe\ttime\t(barely\tbetter\tthan\trandom\tguessing).\tIf\tyou\tpredict\tthe\tmajority\tvoted\tclass,\tyou\tcan hope\tfor\tup\tto\t75%\taccuracy!\tHowever,\tthis\tis\tonly\ttrue\tif\tall\tclassifiers\tare\tperfectly\tindependent, making\tuncorrelated\terrors,\twhich\tis\tclearly\tnot\tthe\tcase\tsince\tthey\tare\ttrained\ton\tthe\tsame\tdata.\tThey\tare likely\tto\tmake\tthe\tsame\ttypes\tof\terrors,\tso\tthere\twill\tbe\tmany\tmajority\tvotes\tfor\tthe\twrong\tclass,\treducing the\tensemble’s\taccuracy.\n\nTIP\n\nEnsemble\tmethods\twork\tbest\twhen\tthe\tpredictors\tare\tas\tindependent\tfrom\tone\tanother\tas\tpossible.\tOne\tway\tto\tget\tdiverse classifiers\tis\tto\ttrain\tthem\tusing\tvery\tdifferent\talgorithms.\tThis\tincreases\tthe\tchance\tthat\tthey\twill\tmake\tvery\tdifferent\ttypes\tof errors,\timproving\tthe\tensemble’s\taccuracy.\n\nThe\tfollowing\tcode\tcreates\tand\ttrains\ta\tvoting\tclassifier\tin\tScikit-Learn,\tcomposed\tof\tthree\tdiverse classifiers\t(the\ttraining\tset\tis\tthe\tmoons\tdataset,\tintroduced\tin\tChapter\t5):\n\nfrom\tsklearn.ensemble\timport\tRandomForestClassifier from\tsklearn.ensemble\timport\tVotingClassifier from\tsklearn.linear_model\timport\tLogisticRegression from\tsklearn.svm\timport\tSVC\n\nlog_clf\t=\tLogisticRegression() rnd_clf\t=\tRandomForestClassifier() svm_clf\t=\tSVC()\n\nvoting_clf\t=\tVotingClassifier( \t\t\t\testimators=[('lr',\tlog_clf),\t('rf',\trnd_clf),\t('svc',\tsvm_clf)],",
      "page_number": 215
    },
    {
      "number": 7,
      "title": "Ensemble\tLearning\tand\tRandom Forests",
      "start_page": 232,
      "end_page": 258,
      "detection_method": "regex_chapter_title",
      "content": "voting='hard') voting_clf.fit(X_train,\ty_train)\n\nLet’s\tlook\tat\teach\tclassifier’s\taccuracy\ton\tthe\ttest\tset:\n\n>>>\tfrom\tsklearn.metrics\timport\taccuracy_score >>>\tfor\tclf\tin\t(log_clf,\trnd_clf,\tsvm_clf,\tvoting_clf): ...\t\t\t\t\tclf.fit(X_train,\ty_train) ...\t\t\t\t\ty_pred\t=\tclf.predict(X_test) ...\t\t\t\t\tprint(clf.__class__.__name__,\taccuracy_score(y_test,\ty_pred)) ... LogisticRegression\t0.864 RandomForestClassifier\t0.872 SVC\t0.888 VotingClassifier\t0.896\n\nThere\tyou\thave\tit!\tThe\tvoting\tclassifier\tslightly\toutperforms\tall\tthe\tindividual\tclassifiers.\n\nIf\tall\tclassifiers\tare\table\tto\testimate\tclass\tprobabilities\t(i.e.,\tthey\thave\ta\tpredict_proba()\tmethod), then\tyou\tcan\ttell\tScikit-Learn\tto\tpredict\tthe\tclass\twith\tthe\thighest\tclass\tprobability,\taveraged\tover\tall\tthe individual\tclassifiers.\tThis\tis\tcalled\tsoft\tvoting.\tIt\toften\tachieves\thigher\tperformance\tthan\thard\tvoting because\tit\tgives\tmore\tweight\tto\thighly\tconfident\tvotes.\tAll\tyou\tneed\tto\tdo\tis\treplace\tvoting=\"hard\" with\tvoting=\"soft\"\tand\tensure\tthat\tall\tclassifiers\tcan\testimate\tclass\tprobabilities.\tThis\tis\tnot\tthe\tcase of\tthe\tSVC\tclass\tby\tdefault,\tso\tyou\tneed\tto\tset\tits\tprobability\thyperparameter\tto\tTrue\t(this\twill\tmake the\tSVC\tclass\tuse\tcross-validation\tto\testimate\tclass\tprobabilities,\tslowing\tdown\ttraining,\tand\tit\twill\tadd a\tpredict_proba()\tmethod).\tIf\tyou\tmodify\tthe\tpreceding\tcode\tto\tuse\tsoft\tvoting,\tyou\twill\tfind\tthat\tthe voting\tclassifier\tachieves\tover\t91%\taccuracy!\n\nBagging\tand\tPasting One\tway\tto\tget\ta\tdiverse\tset\tof\tclassifiers\tis\tto\tuse\tvery\tdifferent\ttraining\talgorithms,\tas\tjust\tdiscussed. Another\tapproach\tis\tto\tuse\tthe\tsame\ttraining\talgorithm\tfor\tevery\tpredictor,\tbut\tto\ttrain\tthem\ton\tdifferent random\tsubsets\tof\tthe\ttraining\tset.\tWhen\tsampling\tis\tperformed\twith\treplacement,\tthis\tmethod\tis\tcalled bagging1\t(short\tfor\tbootstrap\taggregating2).\tWhen\tsampling\tis\tperformed\twithout\treplacement,\tit\tis called\tpasting.3\n\nIn\tother\twords,\tboth\tbagging\tand\tpasting\tallow\ttraining\tinstances\tto\tbe\tsampled\tseveral\ttimes\tacross multiple\tpredictors,\tbut\tonly\tbagging\tallows\ttraining\tinstances\tto\tbe\tsampled\tseveral\ttimes\tfor\tthe\tsame predictor.\tThis\tsampling\tand\ttraining\tprocess\tis\trepresented\tin\tFigure\t7-4.\n\nFigure\t7-4.\tPasting/bagging\ttraining\tset\tsampling\tand\ttraining\n\nOnce\tall\tpredictors\tare\ttrained,\tthe\tensemble\tcan\tmake\ta\tprediction\tfor\ta\tnew\tinstance\tby\tsimply aggregating\tthe\tpredictions\tof\tall\tpredictors.\tThe\taggregation\tfunction\tis\ttypically\tthe\tstatistical\tmode (i.e.,\tthe\tmost\tfrequent\tprediction,\tjust\tlike\ta\thard\tvoting\tclassifier)\tfor\tclassification,\tor\tthe\taverage\tfor regression.\tEach\tindividual\tpredictor\thas\ta\thigher\tbias\tthan\tif\tit\twere\ttrained\ton\tthe\toriginal\ttraining\tset, but\taggregation\treduces\tboth\tbias\tand\tvariance.4\tGenerally,\tthe\tnet\tresult\tis\tthat\tthe\tensemble\thas\ta similar\tbias\tbut\ta\tlower\tvariance\tthan\ta\tsingle\tpredictor\ttrained\ton\tthe\toriginal\ttraining\tset.\n\nAs\tyou\tcan\tsee\tin\tFigure\t7-4,\tpredictors\tcan\tall\tbe\ttrained\tin\tparallel,\tvia\tdifferent\tCPU\tcores\tor\teven different\tservers.\tSimilarly,\tpredictions\tcan\tbe\tmade\tin\tparallel.\tThis\tis\tone\tof\tthe\treasons\twhy\tbagging and\tpasting\tare\tsuch\tpopular\tmethods:\tthey\tscale\tvery\twell.\n\nBagging\tand\tPasting\tin\tScikit-Learn Scikit-Learn\toffers\ta\tsimple\tAPI\tfor\tboth\tbagging\tand\tpasting\twith\tthe\tBaggingClassifier\tclass\t(or BaggingRegressor\tfor\tregression).\tThe\tfollowing\tcode\ttrains\tan\tensemble\tof\t500\tDecision\tTree classifiers,5\teach\ttrained\ton\t100\ttraining\tinstances\trandomly\tsampled\tfrom\tthe\ttraining\tset\twith replacement\t(this\tis\tan\texample\tof\tbagging,\tbut\tif\tyou\twant\tto\tuse\tpasting\tinstead,\tjust\tset bootstrap=False).\tThe\tn_jobs\tparameter\ttells\tScikit-Learn\tthe\tnumber\tof\tCPU\tcores\tto\tuse\tfor\ttraining and\tpredictions\t(–1\ttells\tScikit-Learn\tto\tuse\tall\tavailable\tcores):\n\nfrom\tsklearn.ensemble\timport\tBaggingClassifier from\tsklearn.tree\timport\tDecisionTreeClassifier\n\nbag_clf\t=\tBaggingClassifier( \t\t\t\tDecisionTreeClassifier(),\tn_estimators=500, \t\t\t\tmax_samples=100,\tbootstrap=True,\tn_jobs=-1) bag_clf.fit(X_train,\ty_train) y_pred\t=\tbag_clf.predict(X_test)\n\nNOTE\n\nThe\tBaggingClassifier\tautomatically\tperforms\tsoft\tvoting\tinstead\tof\thard\tvoting\tif\tthe\tbase\tclassifier\tcan\testimate\tclass probabilities\t(i.e.,\tif\tit\thas\ta\tpredict_proba()\tmethod),\twhich\tis\tthe\tcase\twith\tDecision\tTrees\tclassifiers.\n\nFigure\t7-5\tcompares\tthe\tdecision\tboundary\tof\ta\tsingle\tDecision\tTree\twith\tthe\tdecision\tboundary\tof\ta bagging\tensemble\tof\t500\ttrees\t(from\tthe\tpreceding\tcode),\tboth\ttrained\ton\tthe\tmoons\tdataset.\tAs\tyou\tcan see,\tthe\tensemble’s\tpredictions\twill\tlikely\tgeneralize\tmuch\tbetter\tthan\tthe\tsingle\tDecision\tTree’s predictions:\tthe\tensemble\thas\ta\tcomparable\tbias\tbut\ta\tsmaller\tvariance\t(it\tmakes\troughly\tthe\tsame number\tof\terrors\ton\tthe\ttraining\tset,\tbut\tthe\tdecision\tboundary\tis\tless\tirregular).\n\nFigure\t7-5.\tA\tsingle\tDecision\tTree\tversus\ta\tbagging\tensemble\tof\t500\ttrees\n\nBootstrapping\tintroduces\ta\tbit\tmore\tdiversity\tin\tthe\tsubsets\tthat\teach\tpredictor\tis\ttrained\ton,\tso\tbagging ends\tup\twith\ta\tslightly\thigher\tbias\tthan\tpasting,\tbut\tthis\talso\tmeans\tthat\tpredictors\tend\tup\tbeing\tless correlated\tso\tthe\tensemble’s\tvariance\tis\treduced.\tOverall,\tbagging\toften\tresults\tin\tbetter\tmodels,\twhich explains\twhy\tit\tis\tgenerally\tpreferred.\tHowever,\tif\tyou\thave\tspare\ttime\tand\tCPU\tpower\tyou\tcan\tuse cross-validation\tto\tevaluate\tboth\tbagging\tand\tpasting\tand\tselect\tthe\tone\tthat\tworks\tbest.\n\nOut-of-Bag\tEvaluation With\tbagging,\tsome\tinstances\tmay\tbe\tsampled\tseveral\ttimes\tfor\tany\tgiven\tpredictor,\twhile\tothers\tmay\tnot be\tsampled\tat\tall.\tBy\tdefault\ta\tBaggingClassifier\tsamples\tm\ttraining\tinstances\twith\treplacement (bootstrap=True),\twhere\tm\tis\tthe\tsize\tof\tthe\ttraining\tset.\tThis\tmeans\tthat\tonly\tabout\t63%\tof\tthe\ttraining instances\tare\tsampled\ton\taverage\tfor\teach\tpredictor.6\tThe\tremaining\t37%\tof\tthe\ttraining\tinstances\tthat\tare not\tsampled\tare\tcalled\tout-of-bag\t(oob)\tinstances.\tNote\tthat\tthey\tare\tnot\tthe\tsame\t37%\tfor\tall\tpredictors.\n\nSince\ta\tpredictor\tnever\tsees\tthe\toob\tinstances\tduring\ttraining,\tit\tcan\tbe\tevaluated\ton\tthese\tinstances, without\tthe\tneed\tfor\ta\tseparate\tvalidation\tset\tor\tcross-validation.\tYou\tcan\tevaluate\tthe\tensemble\titself\tby averaging\tout\tthe\toob\tevaluations\tof\teach\tpredictor.\n\nIn\tScikit-Learn,\tyou\tcan\tset\toob_score=True\twhen\tcreating\ta\tBaggingClassifier\tto\trequest\tan automatic\toob\tevaluation\tafter\ttraining.\tThe\tfollowing\tcode\tdemonstrates\tthis.\tThe\tresulting\tevaluation score\tis\tavailable\tthrough\tthe\toob_score_\tvariable:\n\n>>>\tbag_clf\t=\tBaggingClassifier( ...\t\t\t\t\tDecisionTreeClassifier(),\tn_estimators=500, ...\t\t\t\t\tbootstrap=True,\tn_jobs=-1,\toob_score=True) ... >>>\tbag_clf.fit(X_train,\ty_train) >>>\tbag_clf.oob_score_ 0.90133333333333332\n\nAccording\tto\tthis\toob\tevaluation,\tthis\tBaggingClassifier\tis\tlikely\tto\tachieve\tabout\t90.1%\taccuracy\ton the\ttest\tset.\tLet’s\tverify\tthis:\n\n>>>\tfrom\tsklearn.metrics\timport\taccuracy_score >>>\ty_pred\t=\tbag_clf.predict(X_test) >>>\taccuracy_score(y_test,\ty_pred) 0.91200000000000003\n\nWe\tget\t91.2%\taccuracy\ton\tthe\ttest\tset\t—\tclose\tenough!\n\nThe\toob\tdecision\tfunction\tfor\teach\ttraining\tinstance\tis\talso\tavailable\tthrough\tthe oob_decision_function_\tvariable.\tIn\tthis\tcase\t(since\tthe\tbase\testimator\thas\ta\tpredict_proba() method)\tthe\tdecision\tfunction\treturns\tthe\tclass\tprobabilities\tfor\teach\ttraining\tinstance.\tFor\texample,\tthe oob\tevaluation\testimates\tthat\tthe\tsecond\ttraining\tinstance\thas\ta\t60.6%\tprobability\tof\tbelonging\tto\tthe positive\tclass\t(and\t39.4%\tof\tbelonging\tto\tthe\tpositive\tclass):\n\n>>>\tbag_clf.oob_decision_function_ array([[\t0.31746032,\t\t0.68253968], \t\t\t\t\t\t\t[\t0.34117647,\t\t0.65882353], \t\t\t\t\t\t\t[\t1.\t\t\t\t\t\t\t\t,\t\t0.\t\t\t\t\t\t\t\t], \t\t\t\t\t\t\t... \t\t\t\t\t\t\t[\t1.\t\t\t\t\t\t\t\t,\t\t0.\t\t\t\t\t\t\t\t], \t\t\t\t\t\t\t[\t0.03108808,\t\t0.96891192], \t\t\t\t\t\t\t[\t0.57291667,\t\t0.42708333]])\n\nRandom\tPatches\tand\tRandom\tSubspaces The\tBaggingClassifier\tclass\tsupports\tsampling\tthe\tfeatures\tas\twell.\tThis\tis\tcontrolled\tby\ttwo hyperparameters:\tmax_features\tand\tbootstrap_features.\tThey\twork\tthe\tsame\tway\tas\tmax_samples and\tbootstrap,\tbut\tfor\tfeature\tsampling\tinstead\tof\tinstance\tsampling.\tThus,\teach\tpredictor\twill\tbe trained\ton\ta\trandom\tsubset\tof\tthe\tinput\tfeatures.\n\nThis\tis\tparticularly\tuseful\twhen\tyou\tare\tdealing\twith\thigh-dimensional\tinputs\t(such\tas\timages).\tSampling both\ttraining\tinstances\tand\tfeatures\tis\tcalled\tthe\tRandom\tPatches\tmethod.7\tKeeping\tall\ttraining\tinstances (i.e.,\tbootstrap=False\tand\tmax_samples=1.0)\tbut\tsampling\tfeatures\t(i.e.,\tbootstrap_features=True and/or\tmax_features\tsmaller\tthan\t1.0)\tis\tcalled\tthe\tRandom\tSubspaces\tmethod.8\n\nSampling\tfeatures\tresults\tin\teven\tmore\tpredictor\tdiversity,\ttrading\ta\tbit\tmore\tbias\tfor\ta\tlower\tvariance.\n\nRandom\tForests As\twe\thave\tdiscussed,\ta\tRandom\tForest9\tis\tan\tensemble\tof\tDecision\tTrees,\tgenerally\ttrained\tvia\tthe bagging\tmethod\t(or\tsometimes\tpasting),\ttypically\twith\tmax_samples\tset\tto\tthe\tsize\tof\tthe\ttraining\tset. Instead\tof\tbuilding\ta\tBaggingClassifier\tand\tpassing\tit\ta\tDecisionTreeClassifier,\tyou\tcan\tinstead use\tthe\tRandomForestClassifier\tclass,\twhich\tis\tmore\tconvenient\tand\toptimized\tfor\tDecision\tTrees10 (similarly,\tthere\tis\ta\tRandomForestRegressor\tclass\tfor\tregression\ttasks).\tThe\tfollowing\tcode\ttrains\ta Random\tForest\tclassifier\twith\t500\ttrees\t(each\tlimited\tto\tmaximum\t16\tnodes),\tusing\tall\tavailable\tCPU cores:\n\nfrom\tsklearn.ensemble\timport\tRandomForestClassifier\n\nrnd_clf\t=\tRandomForestClassifier(n_estimators=500,\tmax_leaf_nodes=16,\tn_jobs=-1) rnd_clf.fit(X_train,\ty_train)\n\ny_pred_rf\t=\trnd_clf.predict(X_test)\n\nWith\ta\tfew\texceptions,\ta\tRandomForestClassifier\thas\tall\tthe\thyperparameters\tof\ta DecisionTreeClassifier\t(to\tcontrol\thow\ttrees\tare\tgrown),\tplus\tall\tthe\thyperparameters\tof\ta BaggingClassifier\tto\tcontrol\tthe\tensemble\titself.11\n\nThe\tRandom\tForest\talgorithm\tintroduces\textra\trandomness\twhen\tgrowing\ttrees;\tinstead\tof\tsearching\tfor the\tvery\tbest\tfeature\twhen\tsplitting\ta\tnode\t(see\tChapter\t6),\tit\tsearches\tfor\tthe\tbest\tfeature\tamong\ta random\tsubset\tof\tfeatures.\tThis\tresults\tin\ta\tgreater\ttree\tdiversity,\twhich\t(once\tagain)\ttrades\ta\thigher\tbias for\ta\tlower\tvariance,\tgenerally\tyielding\tan\toverall\tbetter\tmodel.\tThe\tfollowing\tBaggingClassifier\tis roughly\tequivalent\tto\tthe\tprevious\tRandomForestClassifier:\n\nbag_clf\t=\tBaggingClassifier( \t\t\t\tDecisionTreeClassifier(splitter=\"random\",\tmax_leaf_nodes=16), \t\t\t\tn_estimators=500,\tmax_samples=1.0,\tbootstrap=True,\tn_jobs=-1)\n\nExtra-Trees When\tyou\tare\tgrowing\ta\ttree\tin\ta\tRandom\tForest,\tat\teach\tnode\tonly\ta\trandom\tsubset\tof\tthe\tfeatures\tis considered\tfor\tsplitting\t(as\tdiscussed\tearlier).\tIt\tis\tpossible\tto\tmake\ttrees\teven\tmore\trandom\tby\talso using\trandom\tthresholds\tfor\teach\tfeature\trather\tthan\tsearching\tfor\tthe\tbest\tpossible\tthresholds\t(like regular\tDecision\tTrees\tdo). A\tforest\tof\tsuch\textremely\trandom\ttrees\tis\tsimply\tcalled\tan\tExtremely\tRandomized\tTrees\tensemble12\t(or Extra-Trees\tfor\tshort).\tOnce\tagain,\tthis\ttrades\tmore\tbias\tfor\ta\tlower\tvariance.\tIt\talso\tmakes\tExtra-Trees much\tfaster\tto\ttrain\tthan\tregular\tRandom\tForests\tsince\tfinding\tthe\tbest\tpossible\tthreshold\tfor\teach\tfeature at\tevery\tnode\tis\tone\tof\tthe\tmost\ttime-consuming\ttasks\tof\tgrowing\ta\ttree.\n\nYou\tcan\tcreate\tan\tExtra-Trees\tclassifier\tusing\tScikit-Learn’s\tExtraTreesClassifier\tclass.\tIts\tAPI\tis identical\tto\tthe\tRandomForestClassifier\tclass.\tSimilarly,\tthe\tExtraTreesRegressor\tclass\thas\tthe same\tAPI\tas\tthe\tRandomForestRegressor\tclass.\n\nTIP\n\nIt\tis\thard\tto\ttell\tin\tadvance\twhether\ta\tRandomForestClassifier\twill\tperform\tbetter\tor\tworse\tthan\tan\tExtraTreesClassifier. Generally,\tthe\tonly\tway\tto\tknow\tis\tto\ttry\tboth\tand\tcompare\tthem\tusing\tcross-validation\t(and\ttuning\tthe\thyperparameters\tusing grid\tsearch).\n\nFeature\tImportance Yet\tanother\tgreat\tquality\tof\tRandom\tForests\tis\tthat\tthey\tmake\tit\teasy\tto\tmeasure\tthe\trelative\timportance\tof each\tfeature.\tScikit-Learn\tmeasures\ta\tfeature’s\timportance\tby\tlooking\tat\thow\tmuch\tthe\ttree\tnodes\tthat\tuse that\tfeature\treduce\timpurity\ton\taverage\t(across\tall\ttrees\tin\tthe\tforest).\tMore\tprecisely,\tit\tis\ta\tweighted average,\twhere\teach\tnode’s\tweight\tis\tequal\tto\tthe\tnumber\tof\ttraining\tsamples\tthat\tare\tassociated\twith\tit (see\tChapter\t6).\n\nScikit-Learn\tcomputes\tthis\tscore\tautomatically\tfor\teach\tfeature\tafter\ttraining,\tthen\tit\tscales\tthe\tresults\tso that\tthe\tsum\tof\tall\timportances\tis\tequal\tto\t1.\tYou\tcan\taccess\tthe\tresult\tusing\tthe\tfeature_importances_ variable.\tFor\texample,\tthe\tfollowing\tcode\ttrains\ta\tRandomForestClassifier\ton\tthe\tiris\tdataset (introduced\tin\tChapter\t4)\tand\toutputs\teach\tfeature’s\timportance.\tIt\tseems\tthat\tthe\tmost\timportant\tfeatures are\tthe\tpetal\tlength\t(44%)\tand\twidth\t(42%),\twhile\tsepal\tlength\tand\twidth\tare\trather\tunimportant\tin comparison\t(11%\tand\t2%,\trespectively).\n\n>>>\tfrom\tsklearn.datasets\timport\tload_iris >>>\tiris\t=\tload_iris() >>>\trnd_clf\t=\tRandomForestClassifier(n_estimators=500,\tn_jobs=-1) >>>\trnd_clf.fit(iris[\"data\"],\tiris[\"target\"]) >>>\tfor\tname,\tscore\tin\tzip(iris[\"feature_names\"],\trnd_clf.feature_importances_): ...\t\t\t\t\tprint(name,\tscore) ... sepal\tlength\t(cm)\t0.112492250999 sepal\twidth\t(cm)\t0.0231192882825 petal\tlength\t(cm)\t0.441030464364 petal\twidth\t(cm)\t0.423357996355\n\nSimilarly,\tif\tyou\ttrain\ta\tRandom\tForest\tclassifier\ton\tthe\tMNIST\tdataset\t(introduced\tin\tChapter\t3)\tand plot\teach\tpixel’s\timportance,\tyou\tget\tthe\timage\trepresented\tin\tFigure\t7-6.\n\nFigure\t7-6.\tMNIST\tpixel\timportance\t(according\tto\ta\tRandom\tForest\tclassifier)\n\nRandom\tForests\tare\tvery\thandy\tto\tget\ta\tquick\tunderstanding\tof\twhat\tfeatures\tactually\tmatter,\tin\tparticular if\tyou\tneed\tto\tperform\tfeature\tselection.\n\nBoosting Boosting\t(originally\tcalled\thypothesis\tboosting)\trefers\tto\tany\tEnsemble\tmethod\tthat\tcan\tcombine\tseveral weak\tlearners\tinto\ta\tstrong\tlearner.\tThe\tgeneral\tidea\tof\tmost\tboosting\tmethods\tis\tto\ttrain\tpredictors sequentially,\teach\ttrying\tto\tcorrect\tits\tpredecessor.\tThere\tare\tmany\tboosting\tmethods\tavailable,\tbut\tby\tfar the\tmost\tpopular\tare\tAdaBoost13\t(short\tfor\tAdaptive\tBoosting)\tand\tGradient\tBoosting.\tLet’s\tstart\twith AdaBoost.\n\nAdaBoost One\tway\tfor\ta\tnew\tpredictor\tto\tcorrect\tits\tpredecessor\tis\tto\tpay\ta\tbit\tmore\tattention\tto\tthe\ttraining instances\tthat\tthe\tpredecessor\tunderfitted.\tThis\tresults\tin\tnew\tpredictors\tfocusing\tmore\tand\tmore\ton\tthe hard\tcases.\tThis\tis\tthe\ttechnique\tused\tby\tAdaBoost.\n\nFor\texample,\tto\tbuild\tan\tAdaBoost\tclassifier,\ta\tfirst\tbase\tclassifier\t(such\tas\ta\tDecision\tTree)\tis\ttrained and\tused\tto\tmake\tpredictions\ton\tthe\ttraining\tset.\tThe\trelative\tweight\tof\tmisclassified\ttraining\tinstances\tis then\tincreased.\tA\tsecond\tclassifier\tis\ttrained\tusing\tthe\tupdated\tweights\tand\tagain\tit\tmakes\tpredictions\ton the\ttraining\tset,\tweights\tare\tupdated,\tand\tso\ton\t(see\tFigure\t7-7).\n\nFigure\t7-7.\tAdaBoost\tsequential\ttraining\twith\tinstance\tweight\tupdates\n\nFigure\t7-8\tshows\tthe\tdecision\tboundaries\tof\tfive\tconsecutive\tpredictors\ton\tthe\tmoons\tdataset\t(in\tthis example,\teach\tpredictor\tis\ta\thighly\tregularized\tSVM\tclassifier\twith\tan\tRBF\tkernel14).\tThe\tfirst\tclassifier gets\tmany\tinstances\twrong,\tso\ttheir\tweights\tget\tboosted.\tThe\tsecond\tclassifier\ttherefore\tdoes\ta\tbetter\tjob on\tthese\tinstances,\tand\tso\ton.\tThe\tplot\ton\tthe\tright\trepresents\tthe\tsame\tsequence\tof\tpredictors\texcept\tthat the\tlearning\trate\tis\thalved\t(i.e.,\tthe\tmisclassified\tinstance\tweights\tare\tboosted\thalf\tas\tmuch\tat\tevery iteration).\tAs\tyou\tcan\tsee,\tthis\tsequential\tlearning\ttechnique\thas\tsome\tsimilarities\twith\tGradient\tDescent, except\tthat\tinstead\tof\ttweaking\ta\tsingle\tpredictor’s\tparameters\tto\tminimize\ta\tcost\tfunction,\tAdaBoost adds\tpredictors\tto\tthe\tensemble,\tgradually\tmaking\tit\tbetter.\n\nFigure\t7-8.\tDecision\tboundaries\tof\tconsecutive\tpredictors\n\nOnce\tall\tpredictors\tare\ttrained,\tthe\tensemble\tmakes\tpredictions\tvery\tmuch\tlike\tbagging\tor\tpasting,\texcept that\tpredictors\thave\tdifferent\tweights\tdepending\ton\ttheir\toverall\taccuracy\ton\tthe\tweighted\ttraining\tset.\n\nWARNING\n\nThere\tis\tone\timportant\tdrawback\tto\tthis\tsequential\tlearning\ttechnique:\tit\tcannot\tbe\tparallelized\t(or\tonly\tpartially),\tsince\teach predictor\tcan\tonly\tbe\ttrained\tafter\tthe\tprevious\tpredictor\thas\tbeen\ttrained\tand\tevaluated.\tAs\ta\tresult,\tit\tdoes\tnot\tscale\tas\twell\tas bagging\tor\tpasting.\n\nLet’s\ttake\ta\tcloser\tlook\tat\tthe\tAdaBoost\talgorithm.\tEach\tinstance\tweight\tw(i)\tis\tinitially\tset\tto\t predictor\tis\ttrained\tand\tits\tweighted\terror\trate\tr1\tis\tcomputed\ton\tthe\ttraining\tset;\tsee\tEquation\t7-1.\n\nEquation\t7-1.\tWeighted\terror\trate\tof\tthe\tjth\tpredictor\n\nThe\tpredictor’s\tweight\tαj\tis\tthen\tcomputed\tusing\tEquation\t7-2,\twhere\tη\tis\tthe\tlearning\trate hyperparameter\t(defaults\tto\t1).15\tThe\tmore\taccurate\tthe\tpredictor\tis,\tthe\thigher\tits\tweight\twill\tbe.\tIf\tit\tis just\tguessing\trandomly,\tthen\tits\tweight\twill\tbe\tclose\tto\tzero.\tHowever,\tif\tit\tis\tmost\toften\twrong\t(i.e.,\tless accurate\tthan\trandom\tguessing),\tthen\tits\tweight\twill\tbe\tnegative.\n\nEquation\t7-2.\tPredictor\tweight\n\nNext\tthe\tinstance\tweights\tare\tupdated\tusing\tEquation\t7-3:\tthe\tmisclassified\tinstances\tare\tboosted.\n\n.\tA\tfirst\n\nEquation\t7-3.\tWeight\tupdate\trule\n\nThen\tall\tthe\tinstance\tweights\tare\tnormalized\t(i.e.,\tdivided\tby\n\n).\n\nFinally,\ta\tnew\tpredictor\tis\ttrained\tusing\tthe\tupdated\tweights,\tand\tthe\twhole\tprocess\tis\trepeated\t(the\tnew predictor’s\tweight\tis\tcomputed,\tthe\tinstance\tweights\tare\tupdated,\tthen\tanother\tpredictor\tis\ttrained,\tand\tso on).\tThe\talgorithm\tstops\twhen\tthe\tdesired\tnumber\tof\tpredictors\tis\treached,\tor\twhen\ta\tperfect\tpredictor\tis found.\n\nTo\tmake\tpredictions,\tAdaBoost\tsimply\tcomputes\tthe\tpredictions\tof\tall\tthe\tpredictors\tand\tweighs\tthem using\tthe\tpredictor\tweights\tαj.\tThe\tpredicted\tclass\tis\tthe\tone\tthat\treceives\tthe\tmajority\tof\tweighted\tvotes (see\tEquation\t7-4).\n\nEquation\t7-4.\tAdaBoost\tpredictions\n\nScikit-Learn\tactually\tuses\ta\tmulticlass\tversion\tof\tAdaBoost\tcalled\tSAMME16\t(which\tstands\tfor Stagewise\tAdditive\tModeling\tusing\ta\tMulticlass\tExponential\tloss\tfunction).\tWhen\tthere\tare\tjust\ttwo classes,\tSAMME\tis\tequivalent\tto\tAdaBoost.\tMoreover,\tif\tthe\tpredictors\tcan\testimate\tclass\tprobabilities (i.e.,\tif\tthey\thave\ta\tpredict_proba()\tmethod),\tScikit-Learn\tcan\tuse\ta\tvariant\tof\tSAMME\tcalled SAMME.R\t(the\tR\tstands\tfor\t“Real”),\twhich\trelies\ton\tclass\tprobabilities\trather\tthan\tpredictions\tand generally\tperforms\tbetter.\n\nThe\tfollowing\tcode\ttrains\tan\tAdaBoost\tclassifier\tbased\ton\t200\tDecision\tStumps\tusing\tScikit-Learn’s AdaBoostClassifier\tclass\t(as\tyou\tmight\texpect,\tthere\tis\talso\tan\tAdaBoostRegressor\tclass).\tA Decision\tStump\tis\ta\tDecision\tTree\twith\tmax_depth=1\t—\tin\tother\twords,\ta\ttree\tcomposed\tof\ta\tsingle decision\tnode\tplus\ttwo\tleaf\tnodes.\tThis\tis\tthe\tdefault\tbase\testimator\tfor\tthe\tAdaBoostClassifier\tclass:\n\nfrom\tsklearn.ensemble\timport\tAdaBoostClassifier\n\nada_clf\t=\tAdaBoostClassifier( \t\t\t\tDecisionTreeClassifier(max_depth=1),\tn_estimators=200, \t\t\t\talgorithm=\"SAMME.R\",\tlearning_rate=0.5) ada_clf.fit(X_train,\ty_train)\n\nTIP\n\nIf\tyour\tAdaBoost\tensemble\tis\toverfitting\tthe\ttraining\tset,\tyou\tcan\ttry\treducing\tthe\tnumber\tof\testimators\tor\tmore\tstrongly regularizing\tthe\tbase\testimator.\n\nGradient\tBoosting Another\tvery\tpopular\tBoosting\talgorithm\tis\tGradient\tBoosting.17\tJust\tlike\tAdaBoost,\tGradient\tBoosting works\tby\tsequentially\tadding\tpredictors\tto\tan\tensemble,\teach\tone\tcorrecting\tits\tpredecessor.\tHowever, instead\tof\ttweaking\tthe\tinstance\tweights\tat\tevery\titeration\tlike\tAdaBoost\tdoes,\tthis\tmethod\ttries\tto\tfit\tthe new\tpredictor\tto\tthe\tresidual\terrors\tmade\tby\tthe\tprevious\tpredictor.\n\nLet’s\tgo\tthrough\ta\tsimple\tregression\texample\tusing\tDecision\tTrees\tas\tthe\tbase\tpredictors\t(of\tcourse Gradient\tBoosting\talso\tworks\tgreat\twith\tregression\ttasks).\tThis\tis\tcalled\tGradient\tTree\tBoosting,\tor Gradient\tBoosted\tRegression\tTrees\t(GBRT).\tFirst,\tlet’s\tfit\ta\tDecisionTreeRegressor\tto\tthe\ttraining\tset (for\texample,\ta\tnoisy\tquadratic\ttraining\tset):\n\nfrom\tsklearn.tree\timport\tDecisionTreeRegressor\n\ntree_reg1\t=\tDecisionTreeRegressor(max_depth=2) tree_reg1.fit(X,\ty)\n\nNow\ttrain\ta\tsecond\tDecisionTreeRegressor\ton\tthe\tresidual\terrors\tmade\tby\tthe\tfirst\tpredictor:\n\ny2\t=\ty\t-\ttree_reg1.predict(X) tree_reg2\t=\tDecisionTreeRegressor(max_depth=2) tree_reg2.fit(X,\ty2)\n\nThen\twe\ttrain\ta\tthird\tregressor\ton\tthe\tresidual\terrors\tmade\tby\tthe\tsecond\tpredictor:\n\ny3\t=\ty2\t-\ttree_reg2.predict(X) tree_reg3\t=\tDecisionTreeRegressor(max_depth=2) tree_reg3.fit(X,\ty3)\n\nNow\twe\thave\tan\tensemble\tcontaining\tthree\ttrees.\tIt\tcan\tmake\tpredictions\ton\ta\tnew\tinstance\tsimply\tby adding\tup\tthe\tpredictions\tof\tall\tthe\ttrees:\n\ny_pred\t=\tsum(tree.predict(X_new)\tfor\ttree\tin\t(tree_reg1,\ttree_reg2,\ttree_reg3))\n\nFigure\t7-9\trepresents\tthe\tpredictions\tof\tthese\tthree\ttrees\tin\tthe\tleft\tcolumn,\tand\tthe\tensemble’s predictions\tin\tthe\tright\tcolumn.\tIn\tthe\tfirst\trow,\tthe\tensemble\thas\tjust\tone\ttree,\tso\tits\tpredictions\tare exactly\tthe\tsame\tas\tthe\tfirst\ttree’s\tpredictions.\tIn\tthe\tsecond\trow,\ta\tnew\ttree\tis\ttrained\ton\tthe\tresidual errors\tof\tthe\tfirst\ttree.\tOn\tthe\tright\tyou\tcan\tsee\tthat\tthe\tensemble’s\tpredictions\tare\tequal\tto\tthe\tsum\tof\tthe predictions\tof\tthe\tfirst\ttwo\ttrees.\tSimilarly,\tin\tthe\tthird\trow\tanother\ttree\tis\ttrained\ton\tthe\tresidual\terrors of\tthe\tsecond\ttree.\tYou\tcan\tsee\tthat\tthe\tensemble’s\tpredictions\tgradually\tget\tbetter\tas\ttrees\tare\tadded\tto the\tensemble.\n\nA\tsimpler\tway\tto\ttrain\tGBRT\tensembles\tis\tto\tuse\tScikit-Learn’s\tGradientBoostingRegressor\tclass. Much\tlike\tthe\tRandomForestRegressor\tclass,\tit\thas\thyperparameters\tto\tcontrol\tthe\tgrowth\tof\tDecision Trees\t(e.g.,\tmax_depth,\tmin_samples_leaf,\tand\tso\ton),\tas\twell\tas\thyperparameters\tto\tcontrol\tthe ensemble\ttraining,\tsuch\tas\tthe\tnumber\tof\ttrees\t(n_estimators).\tThe\tfollowing\tcode\tcreates\tthe\tsame ensemble\tas\tthe\tprevious\tone:\n\nfrom\tsklearn.ensemble\timport\tGradientBoostingRegressor\n\ngbrt\t=\tGradientBoostingRegressor(max_depth=2,\tn_estimators=3,\tlearning_rate=1.0) gbrt.fit(X,\ty)\n\nFigure\t7-9.\tGradient\tBoosting\n\nThe\tlearning_rate\thyperparameter\tscales\tthe\tcontribution\tof\teach\ttree.\tIf\tyou\tset\tit\tto\ta\tlow\tvalue,\tsuch as\t0.1,\tyou\twill\tneed\tmore\ttrees\tin\tthe\tensemble\tto\tfit\tthe\ttraining\tset,\tbut\tthe\tpredictions\twill\tusually generalize\tbetter.\tThis\tis\ta\tregularization\ttechnique\tcalled\tshrinkage.\tFigure\t7-10\tshows\ttwo\tGBRT ensembles\ttrained\twith\ta\tlow\tlearning\trate:\tthe\tone\ton\tthe\tleft\tdoes\tnot\thave\tenough\ttrees\tto\tfit\tthe training\tset,\twhile\tthe\tone\ton\tthe\tright\thas\ttoo\tmany\ttrees\tand\toverfits\tthe\ttraining\tset.\n\nFigure\t7-10.\tGBRT\tensembles\twith\tnot\tenough\tpredictors\t(left)\tand\ttoo\tmany\t(right)\n\nIn\torder\tto\tfind\tthe\toptimal\tnumber\tof\ttrees,\tyou\tcan\tuse\tearly\tstopping\t(see\tChapter\t4).\tA\tsimple\tway\tto implement\tthis\tis\tto\tuse\tthe\tstaged_predict()\tmethod:\tit\treturns\tan\titerator\tover\tthe\tpredictions\tmade by\tthe\tensemble\tat\teach\tstage\tof\ttraining\t(with\tone\ttree,\ttwo\ttrees,\tetc.).\tThe\tfollowing\tcode\ttrains\ta GBRT\tensemble\twith\t120\ttrees,\tthen\tmeasures\tthe\tvalidation\terror\tat\teach\tstage\tof\ttraining\tto\tfind\tthe optimal\tnumber\tof\ttrees,\tand\tfinally\ttrains\tanother\tGBRT\tensemble\tusing\tthe\toptimal\tnumber\tof\ttrees:\n\nimport\tnumpy\tas\tnp from\tsklearn.model_selection\timport\ttrain_test_split from\tsklearn.metrics\timport\tmean_squared_error\n\nX_train,\tX_val,\ty_train,\ty_val\t=\ttrain_test_split(X,\ty)\n\ngbrt\t=\tGradientBoostingRegressor(max_depth=2,\tn_estimators=120) gbrt.fit(X_train,\ty_train)\n\nerrors\t=\t[mean_squared_error(y_val,\ty_pred) \t\t\t\t\t\t\t\t\t\tfor\ty_pred\tin\tgbrt.staged_predict(X_val)] bst_n_estimators\t=\tnp.argmin(errors)\n\ngbrt_best\t=\tGradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators) gbrt_best.fit(X_train,\ty_train)\n\nThe\tvalidation\terrors\tare\trepresented\ton\tthe\tleft\tof\tFigure\t7-11,\tand\tthe\tbest\tmodel’s\tpredictions\tare represented\ton\tthe\tright.\n\nFigure\t7-11.\tTuning\tthe\tnumber\tof\ttrees\tusing\tearly\tstopping\n\nIt\tis\talso\tpossible\tto\timplement\tearly\tstopping\tby\tactually\tstopping\ttraining\tearly\t(instead\tof\ttraining\ta large\tnumber\tof\ttrees\tfirst\tand\tthen\tlooking\tback\tto\tfind\tthe\toptimal\tnumber).\tYou\tcan\tdo\tso\tby\tsetting warm_start=True,\twhich\tmakes\tScikit-Learn\tkeep\texisting\ttrees\twhen\tthe\tfit()\tmethod\tis\tcalled, allowing\tincremental\ttraining.\tThe\tfollowing\tcode\tstops\ttraining\twhen\tthe\tvalidation\terror\tdoes\tnot\n\nimprove\tfor\tfive\titerations\tin\ta\trow:\n\ngbrt\t=\tGradientBoostingRegressor(max_depth=2,\twarm_start=True)\n\nmin_val_error\t=\tfloat(\"inf\") error_going_up\t=\t0 for\tn_estimators\tin\trange(1,\t120): \t\t\t\tgbrt.n_estimators\t=\tn_estimators \t\t\t\tgbrt.fit(X_train,\ty_train) \t\t\t\ty_pred\t=\tgbrt.predict(X_val) \t\t\t\tval_error\t=\tmean_squared_error(y_val,\ty_pred) \t\t\t\tif\tval_error\t<\tmin_val_error: \t\t\t\t\t\t\t\tmin_val_error\t=\tval_error \t\t\t\t\t\t\t\terror_going_up\t=\t0 \t\t\t\telse: \t\t\t\t\t\t\t\terror_going_up\t+=\t1 \t\t\t\t\t\t\t\tif\terror_going_up\t==\t5: \t\t\t\t\t\t\t\t\t\t\t\tbreak\t\t#\tearly\tstopping\n\nThe\tGradientBoostingRegressor\tclass\talso\tsupports\ta\tsubsample\thyperparameter,\twhich\tspecifies the\tfraction\tof\ttraining\tinstances\tto\tbe\tused\tfor\ttraining\teach\ttree.\tFor\texample,\tif\tsubsample=0.25,\tthen each\ttree\tis\ttrained\ton\t25%\tof\tthe\ttraining\tinstances,\tselected\trandomly.\tAs\tyou\tcan\tprobably\tguess\tby now,\tthis\ttrades\ta\thigher\tbias\tfor\ta\tlower\tvariance.\tIt\talso\tspeeds\tup\ttraining\tconsiderably.\tThis\ttechnique is\tcalled\tStochastic\tGradient\tBoosting.\n\nNOTE\n\nIt\tis\tpossible\tto\tuse\tGradient\tBoosting\twith\tother\tcost\tfunctions.\tThis\tis\tcontrolled\tby\tthe\tloss\thyperparameter\t(see\tScikit- Learn’s\tdocumentation\tfor\tmore\tdetails).\n\nStacking The\tlast\tEnsemble\tmethod\twe\twill\tdiscuss\tin\tthis\tchapter\tis\tcalled\tstacking\t(short\tfor\tstacked generalization).18\tIt\tis\tbased\ton\ta\tsimple\tidea:\tinstead\tof\tusing\ttrivial\tfunctions\t(such\tas\thard\tvoting)\tto aggregate\tthe\tpredictions\tof\tall\tpredictors\tin\tan\tensemble,\twhy\tdon’t\twe\ttrain\ta\tmodel\tto\tperform\tthis aggregation?\tFigure\t7-12\tshows\tsuch\tan\tensemble\tperforming\ta\tregression\ttask\ton\ta\tnew\tinstance.\tEach of\tthe\tbottom\tthree\tpredictors\tpredicts\ta\tdifferent\tvalue\t(3.1,\t2.7,\tand\t2.9),\tand\tthen\tthe\tfinal\tpredictor (called\ta\tblender,\tor\ta\tmeta\tlearner)\ttakes\tthese\tpredictions\tas\tinputs\tand\tmakes\tthe\tfinal\tprediction (3.0).\n\nFigure\t7-12.\tAggregating\tpredictions\tusing\ta\tblending\tpredictor\n\nTo\ttrain\tthe\tblender,\ta\tcommon\tapproach\tis\tto\tuse\ta\thold-out\tset.19\tLet’s\tsee\thow\tit\tworks.\tFirst,\tthe training\tset\tis\tsplit\tin\ttwo\tsubsets.\tThe\tfirst\tsubset\tis\tused\tto\ttrain\tthe\tpredictors\tin\tthe\tfirst\tlayer\t(see Figure\t7-13).\n\nFigure\t7-13.\tTraining\tthe\tfirst\tlayer\n\nNext,\tthe\tfirst\tlayer\tpredictors\tare\tused\tto\tmake\tpredictions\ton\tthe\tsecond\t(held-out)\tset\t(see\tFigure\t7- 14).\tThis\tensures\tthat\tthe\tpredictions\tare\t“clean,”\tsince\tthe\tpredictors\tnever\tsaw\tthese\tinstances\tduring training.\tNow\tfor\teach\tinstance\tin\tthe\thold-out\tset\tthere\tare\tthree\tpredicted\tvalues.\tWe\tcan\tcreate\ta\tnew training\tset\tusing\tthese\tpredicted\tvalues\tas\tinput\tfeatures\t(which\tmakes\tthis\tnew\ttraining\tset\tthree- dimensional),\tand\tkeeping\tthe\ttarget\tvalues.\tThe\tblender\tis\ttrained\ton\tthis\tnew\ttraining\tset,\tso\tit\tlearns\tto predict\tthe\ttarget\tvalue\tgiven\tthe\tfirst\tlayer’s\tpredictions.\n\nFigure\t7-14.\tTraining\tthe\tblender\n\nIt\tis\tactually\tpossible\tto\ttrain\tseveral\tdifferent\tblenders\tthis\tway\t(e.g.,\tone\tusing\tLinear\tRegression, another\tusing\tRandom\tForest\tRegression,\tand\tso\ton):\twe\tget\ta\twhole\tlayer\tof\tblenders.\tThe\ttrick\tis\tto split\tthe\ttraining\tset\tinto\tthree\tsubsets:\tthe\tfirst\tone\tis\tused\tto\ttrain\tthe\tfirst\tlayer,\tthe\tsecond\tone\tis\tused\tto create\tthe\ttraining\tset\tused\tto\ttrain\tthe\tsecond\tlayer\t(using\tpredictions\tmade\tby\tthe\tpredictors\tof\tthe\tfirst layer),\tand\tthe\tthird\tone\tis\tused\tto\tcreate\tthe\ttraining\tset\tto\ttrain\tthe\tthird\tlayer\t(using\tpredictions\tmade\tby the\tpredictors\tof\tthe\tsecond\tlayer).\tOnce\tthis\tis\tdone,\twe\tcan\tmake\ta\tprediction\tfor\ta\tnew\tinstance\tby going\tthrough\teach\tlayer\tsequentially,\tas\tshown\tin\tFigure\t7-15.\n\nFigure\t7-15.\tPredictions\tin\ta\tmultilayer\tstacking\tensemble\n\nUnfortunately,\tScikit-Learn\tdoes\tnot\tsupport\tstacking\tdirectly,\tbut\tit\tis\tnot\ttoo\thard\tto\troll\tout\tyour\town implementation\t(see\tthe\tfollowing\texercises).\tAlternatively,\tyou\tcan\tuse\tan\topen\tsource\timplementation such\tas\tbrew\t(available\tat\thttps://github.com/viisar/brew).\n\nExercises\n\n1.\t If\tyou\thave\ttrained\tfive\tdifferent\tmodels\ton\tthe\texact\tsame\ttraining\tdata,\tand\tthey\tall\tachieve\t95% precision,\tis\tthere\tany\tchance\tthat\tyou\tcan\tcombine\tthese\tmodels\tto\tget\tbetter\tresults?\tIf\tso,\thow?\tIf not,\twhy?\n\n2.\t What\tis\tthe\tdifference\tbetween\thard\tand\tsoft\tvoting\tclassifiers?\n\n3.\t Is\tit\tpossible\tto\tspeed\tup\ttraining\tof\ta\tbagging\tensemble\tby\tdistributing\tit\tacross\tmultiple\tservers? What\tabout\tpasting\tensembles,\tboosting\tensembles,\trandom\tforests,\tor\tstacking\tensembles?\n\n4.\t What\tis\tthe\tbenefit\tof\tout-of-bag\tevaluation?\n\n5.\t What\tmakes\tExtra-Trees\tmore\trandom\tthan\tregular\tRandom\tForests?\tHow\tcan\tthis\textra\trandomness help?\tAre\tExtra-Trees\tslower\tor\tfaster\tthan\tregular\tRandom\tForests?\n\n6.\t If\tyour\tAdaBoost\tensemble\tunderfits\tthe\ttraining\tdata,\twhat\thyperparameters\tshould\tyou\ttweak\tand how?\n\n7.\t If\tyour\tGradient\tBoosting\tensemble\toverfits\tthe\ttraining\tset,\tshould\tyou\tincrease\tor\tdecrease\tthe learning\trate?\n\n8.\t Load\tthe\tMNIST\tdata\t(introduced\tin\tChapter\t3),\tand\tsplit\tit\tinto\ta\ttraining\tset,\ta\tvalidation\tset,\tand\ta test\tset\t(e.g.,\tuse\t40,000\tinstances\tfor\ttraining,\t10,000\tfor\tvalidation,\tand\t10,000\tfor\ttesting).\tThen train\tvarious\tclassifiers,\tsuch\tas\ta\tRandom\tForest\tclassifier,\tan\tExtra-Trees\tclassifier,\tand\tan\tSVM. Next,\ttry\tto\tcombine\tthem\tinto\tan\tensemble\tthat\toutperforms\tthem\tall\ton\tthe\tvalidation\tset,\tusing\ta soft\tor\thard\tvoting\tclassifier.\tOnce\tyou\thave\tfound\tone,\ttry\tit\ton\tthe\ttest\tset.\tHow\tmuch\tbetter\tdoes\tit perform\tcompared\tto\tthe\tindividual\tclassifiers?\n\n9.\t Run\tthe\tindividual\tclassifiers\tfrom\tthe\tprevious\texercise\tto\tmake\tpredictions\ton\tthe\tvalidation\tset, and\tcreate\ta\tnew\ttraining\tset\twith\tthe\tresulting\tpredictions:\teach\ttraining\tinstance\tis\ta\tvector containing\tthe\tset\tof\tpredictions\tfrom\tall\tyour\tclassifiers\tfor\tan\timage,\tand\tthe\ttarget\tis\tthe\timage’s class.\tCongratulations,\tyou\thave\tjust\ttrained\ta\tblender,\tand\ttogether\twith\tthe\tclassifiers\tthey\tform\ta stacking\tensemble!\tNow\tlet’s\tevaluate\tthe\tensemble\ton\tthe\ttest\tset.\tFor\teach\timage\tin\tthe\ttest\tset, make\tpredictions\twith\tall\tyour\tclassifiers,\tthen\tfeed\tthe\tpredictions\tto\tthe\tblender\tto\tget\tthe ensemble’s\tpredictions.\tHow\tdoes\tit\tcompare\tto\tthe\tvoting\tclassifier\tyou\ttrained\tearlier?\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“Bagging\tPredictors,”\tL.\tBreiman\t(1996).\n\n2\n\nIn\tstatistics,\tresampling\twith\treplacement\tis\tcalled\tbootstrapping.\n\n3\n\n“Pasting\tsmall\tvotes\tfor\tclassification\tin\tlarge\tdatabases\tand\ton-line,”\tL.\tBreiman\t(1999).\n\n4\n\nBias\tand\tvariance\twere\tintroduced\tin\tChapter\t4.\n\n5\n\nmax_samples\tcan\talternatively\tbe\tset\tto\ta\tfloat\tbetween\t0.0\tand\t1.0,\tin\twhich\tcase\tthe\tmax\tnumber\tof\tinstances\tto\tsample\tis\tequal\tto\tthe size\tof\tthe\ttraining\tset\ttimes\tmax_samples.\n\n6\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\nAs\tm\tgrows,\tthis\tratio\tapproaches\t1\t–\texp(–1)\t≈\t63.212%.\n\n“Ensembles\ton\tRandom\tPatches,”\tG.\tLouppe\tand\tP.\tGeurts\t(2012).\n\n“The\trandom\tsubspace\tmethod\tfor\tconstructing\tdecision\tforests,”\tTin\tKam\tHo\t(1998).\n\n“Random\tDecision\tForests,”\tT.\tHo\t(1995).\n\nThe\tBaggingClassifier\tclass\tremains\tuseful\tif\tyou\twant\ta\tbag\tof\tsomething\tother\tthan\tDecision\tTrees.\n\nThere\tare\ta\tfew\tnotable\texceptions:\tsplitter\tis\tabsent\t(forced\tto\t\"random\"),\tpresort\tis\tabsent\t(forced\tto\tFalse),\tmax_samples\tis\tabsent (forced\tto\t1.0),\tand\tbase_estimator\tis\tabsent\t(forced\tto\tDecisionTreeClassifier\twith\tthe\tprovided\thyperparameters).\n\n“Extremely\trandomized\ttrees,”\tP.\tGeurts,\tD.\tErnst,\tL.\tWehenkel\t(2005).\n\n“A\tDecision-Theoretic\tGeneralization\tof\tOn-Line\tLearning\tand\tan\tApplication\tto\tBoosting,”\tYoav\tFreund,\tRobert\tE.\tSchapire\t(1997).\n\nThis\tis\tjust\tfor\tillustrative\tpurposes.\tSVMs\tare\tgenerally\tnot\tgood\tbase\tpredictors\tfor\tAdaBoost,\tbecause\tthey\tare\tslow\tand\ttend\tto\tbe unstable\twith\tAdaBoost.\n\nThe\toriginal\tAdaBoost\talgorithm\tdoes\tnot\tuse\ta\tlearning\trate\thyperparameter.\n\nFor\tmore\tdetails,\tsee\t“Multi-Class\tAdaBoost,”\tJ.\tZhu\tet\tal.\t(2006).\n\nFirst\tintroduced\tin\t“Arcing\tthe\tEdge,”\tL.\tBreiman\t(1997).\n\n“Stacked\tGeneralization,”\tD.\tWolpert\t(1992).\n\nAlternatively,\tit\tis\tpossible\tto\tuse\tout-of-fold\tpredictions.\tIn\tsome\tcontexts\tthis\tis\tcalled\tstacking,\twhile\tusing\ta\thold-out\tset\tis\tcalled blending.\tHowever,\tfor\tmany\tpeople\tthese\tterms\tare\tsynonymous.\n\nChapter\t8.\tDimensionality\tReduction\n\nMany\tMachine\tLearning\tproblems\tinvolve\tthousands\tor\teven\tmillions\tof\tfeatures\tfor\teach\ttraining instance.\tNot\tonly\tdoes\tthis\tmake\ttraining\textremely\tslow,\tit\tcan\talso\tmake\tit\tmuch\tharder\tto\tfind\ta\tgood solution,\tas\twe\twill\tsee.\tThis\tproblem\tis\toften\treferred\tto\tas\tthe\tcurse\tof\tdimensionality.\n\nFortunately,\tin\treal-world\tproblems,\tit\tis\toften\tpossible\tto\treduce\tthe\tnumber\tof\tfeatures\tconsiderably, turning\tan\tintractable\tproblem\tinto\ta\ttractable\tone.\tFor\texample,\tconsider\tthe\tMNIST\timages\t(introduced in\tChapter\t3):\tthe\tpixels\ton\tthe\timage\tborders\tare\talmost\talways\twhite,\tso\tyou\tcould\tcompletely\tdrop these\tpixels\tfrom\tthe\ttraining\tset\twithout\tlosing\tmuch\tinformation.\tFigure\t7-6\tconfirms\tthat\tthese\tpixels are\tutterly\tunimportant\tfor\tthe\tclassification\ttask.\tMoreover,\ttwo\tneighboring\tpixels\tare\toften\thighly correlated:\tif\tyou\tmerge\tthem\tinto\ta\tsingle\tpixel\t(e.g.,\tby\ttaking\tthe\tmean\tof\tthe\ttwo\tpixel\tintensities),\tyou will\tnot\tlose\tmuch\tinformation.\n\nWARNING\n\nReducing\tdimensionality\tdoes\tlose\tsome\tinformation\t(just\tlike\tcompressing\tan\timage\tto\tJPEG\tcan\tdegrade\tits\tquality),\tso\teven though\tit\twill\tspeed\tup\ttraining,\tit\tmay\talso\tmake\tyour\tsystem\tperform\tslightly\tworse.\tIt\talso\tmakes\tyour\tpipelines\ta\tbit\tmore complex\tand\tthus\tharder\tto\tmaintain.\tSo\tyou\tshould\tfirst\ttry\tto\ttrain\tyour\tsystem\twith\tthe\toriginal\tdata\tbefore\tconsidering\tusing dimensionality\treduction\tif\ttraining\tis\ttoo\tslow.\tIn\tsome\tcases,\thowever,\treducing\tthe\tdimensionality\tof\tthe\ttraining\tdata\tmay\tfilter out\tsome\tnoise\tand\tunnecessary\tdetails\tand\tthus\tresult\tin\thigher\tperformance\t(but\tin\tgeneral\tit\twon’t;\tit\twill\tjust\tspeed\tup training).\n\nApart\tfrom\tspeeding\tup\ttraining,\tdimensionality\treduction\tis\talso\textremely\tuseful\tfor\tdata\tvisualization (or\tDataViz).\tReducing\tthe\tnumber\tof\tdimensions\tdown\tto\ttwo\t(or\tthree)\tmakes\tit\tpossible\tto\tplot\ta\thigh- dimensional\ttraining\tset\ton\ta\tgraph\tand\toften\tgain\tsome\timportant\tinsights\tby\tvisually\tdetecting\tpatterns, such\tas\tclusters.\n\nIn\tthis\tchapter\twe\twill\tdiscuss\tthe\tcurse\tof\tdimensionality\tand\tget\ta\tsense\tof\twhat\tgoes\ton\tin\thigh- dimensional\tspace.\tThen,\twe\twill\tpresent\tthe\ttwo\tmain\tapproaches\tto\tdimensionality\treduction (projection\tand\tManifold\tLearning),\tand\twe\twill\tgo\tthrough\tthree\tof\tthe\tmost\tpopular\tdimensionality reduction\ttechniques:\tPCA,\tKernel\tPCA,\tand\tLLE.\n\nThe\tCurse\tof\tDimensionality We\tare\tso\tused\tto\tliving\tin\tthree\tdimensions1\tthat\tour\tintuition\tfails\tus\twhen\twe\ttry\tto\timagine\ta\thigh- dimensional\tspace.\tEven\ta\tbasic\t4D\thypercube\tis\tincredibly\thard\tto\tpicture\tin\tour\tmind\t(see\tFigure\t8-1), let\talone\ta\t200-dimensional\tellipsoid\tbent\tin\ta\t1,000-dimensional\tspace.\n\nFigure\t8-1.\tPoint,\tsegment,\tsquare,\tcube,\tand\ttesseract\t(0D\tto\t4D\thypercubes)2\n\nIt\tturns\tout\tthat\tmany\tthings\tbehave\tvery\tdifferently\tin\thigh-dimensional\tspace.\tFor\texample,\tif\tyou\tpick\ta random\tpoint\tin\ta\tunit\tsquare\t(a\t1\t×\t1\tsquare),\tit\twill\thave\tonly\tabout\ta\t0.4%\tchance\tof\tbeing\tlocated\tless than\t0.001\tfrom\ta\tborder\t(in\tother\twords,\tit\tis\tvery\tunlikely\tthat\ta\trandom\tpoint\twill\tbe\t“extreme”\talong any\tdimension).\tBut\tin\ta\t10,000-dimensional\tunit\thypercube\t(a\t1\t×\t1\t×\t\t×\t1\tcube,\twith\tten\tthousand\t1s), this\tprobability\tis\tgreater\tthan\t99.999999%.\tMost\tpoints\tin\ta\thigh-dimensional\thypercube\tare\tvery\tclose to\tthe\tborder.3\n\nHere\tis\ta\tmore\ttroublesome\tdifference:\tif\tyou\tpick\ttwo\tpoints\trandomly\tin\ta\tunit\tsquare,\tthe\tdistance between\tthese\ttwo\tpoints\twill\tbe,\ton\taverage,\troughly\t0.52.\tIf\tyou\tpick\ttwo\trandom\tpoints\tin\ta\tunit\t3D cube,\tthe\taverage\tdistance\twill\tbe\troughly\t0.66.\tBut\twhat\tabout\ttwo\tpoints\tpicked\trandomly\tin\ta 1,000,000-dimensional\thypercube?\tWell,\tthe\taverage\tdistance,\tbelieve\tit\tor\tnot,\twill\tbe\tabout\t408.25\n\n)!\tThis\tis\tquite\tcounterintuitive:\thow\tcan\ttwo\tpoints\tbe\tso\tfar\tapart\twhen (roughly\t they\tboth\tlie\twithin\tthe\tsame\tunit\thypercube?\tThis\tfact\timplies\tthat\thigh-dimensional\tdatasets\tare\tat\trisk of\tbeing\tvery\tsparse:\tmost\ttraining\tinstances\tare\tlikely\tto\tbe\tfar\taway\tfrom\teach\tother.\tOf\tcourse,\tthis also\tmeans\tthat\ta\tnew\tinstance\twill\tlikely\tbe\tfar\taway\tfrom\tany\ttraining\tinstance,\tmaking\tpredictions much\tless\treliable\tthan\tin\tlower\tdimensions,\tsince\tthey\twill\tbe\tbased\ton\tmuch\tlarger\textrapolations.\tIn short,\tthe\tmore\tdimensions\tthe\ttraining\tset\thas,\tthe\tgreater\tthe\trisk\tof\toverfitting\tit.\n\nIn\ttheory,\tone\tsolution\tto\tthe\tcurse\tof\tdimensionality\tcould\tbe\tto\tincrease\tthe\tsize\tof\tthe\ttraining\tset\tto reach\ta\tsufficient\tdensity\tof\ttraining\tinstances.\tUnfortunately,\tin\tpractice,\tthe\tnumber\tof\ttraining\tinstances required\tto\treach\ta\tgiven\tdensity\tgrows\texponentially\twith\tthe\tnumber\tof\tdimensions.\tWith\tjust\t100 features\t(much\tless\tthan\tin\tthe\tMNIST\tproblem),\tyou\twould\tneed\tmore\ttraining\tinstances\tthan\tatoms\tin the\tobservable\tuniverse\tin\torder\tfor\ttraining\tinstances\tto\tbe\twithin\t0.1\tof\teach\tother\ton\taverage,\tassuming they\twere\tspread\tout\tuniformly\tacross\tall\tdimensions.\n\nMain\tApproaches\tfor\tDimensionality\tReduction Before\twe\tdive\tinto\tspecific\tdimensionality\treduction\talgorithms,\tlet’s\ttake\ta\tlook\tat\tthe\ttwo\tmain approaches\tto\treducing\tdimensionality:\tprojection\tand\tManifold\tLearning.",
      "page_number": 232
    },
    {
      "number": 8,
      "title": "Dimensionality\tReduction",
      "start_page": 259,
      "end_page": 286,
      "detection_method": "regex_chapter_title",
      "content": "Projection In\tmost\treal-world\tproblems,\ttraining\tinstances\tare\tnot\tspread\tout\tuniformly\tacross\tall\tdimensions.\tMany features\tare\talmost\tconstant,\twhile\tothers\tare\thighly\tcorrelated\t(as\tdiscussed\tearlier\tfor\tMNIST).\tAs\ta result,\tall\ttraining\tinstances\tactually\tlie\twithin\t(or\tclose\tto)\ta\tmuch\tlower-dimensional\tsubspace\tof\tthe high-dimensional\tspace.\tThis\tsounds\tvery\tabstract,\tso\tlet’s\tlook\tat\tan\texample.\tIn\tFigure\t8-2\tyou\tcan\tsee a\t3D\tdataset\trepresented\tby\tthe\tcircles.\n\nFigure\t8-2.\tA\t3D\tdataset\tlying\tclose\tto\ta\t2D\tsubspace\n\nNotice\tthat\tall\ttraining\tinstances\tlie\tclose\tto\ta\tplane:\tthis\tis\ta\tlower-dimensional\t(2D)\tsubspace\tof\tthe high-dimensional\t(3D)\tspace.\tNow\tif\twe\tproject\tevery\ttraining\tinstance\tperpendicularly\tonto\tthis subspace\t(as\trepresented\tby\tthe\tshort\tlines\tconnecting\tthe\tinstances\tto\tthe\tplane),\twe\tget\tthe\tnew\t2D dataset\tshown\tin\tFigure\t8-3.\tTa-da!\tWe\thave\tjust\treduced\tthe\tdataset’s\tdimensionality\tfrom\t3D\tto\t2D. Note\tthat\tthe\taxes\tcorrespond\tto\tnew\tfeatures\tz1\tand\tz2\t(the\tcoordinates\tof\tthe\tprojections\ton\tthe\tplane).\n\nFigure\t8-3.\tThe\tnew\t2D\tdataset\tafter\tprojection\n\nHowever,\tprojection\tis\tnot\talways\tthe\tbest\tapproach\tto\tdimensionality\treduction.\tIn\tmany\tcases\tthe subspace\tmay\ttwist\tand\tturn,\tsuch\tas\tin\tthe\tfamous\tSwiss\troll\ttoy\tdataset\trepresented\tin\tFigure\t8-4.\n\nFigure\t8-4.\tSwiss\troll\tdataset\n\nSimply\tprojecting\tonto\ta\tplane\t(e.g.,\tby\tdropping\tx3)\twould\tsquash\tdifferent\tlayers\tof\tthe\tSwiss\troll together,\tas\tshown\ton\tthe\tleft\tof\tFigure\t8-5.\tHowever,\twhat\tyou\treally\twant\tis\tto\tunroll\tthe\tSwiss\troll\tto obtain\tthe\t2D\tdataset\ton\tthe\tright\tof\tFigure\t8-5.\n\nFigure\t8-5.\tSquashing\tby\tprojecting\tonto\ta\tplane\t(left)\tversus\tunrolling\tthe\tSwiss\troll\t(right)\n\nManifold\tLearning The\tSwiss\troll\tis\tan\texample\tof\ta\t2D\tmanifold.\tPut\tsimply,\ta\t2D\tmanifold\tis\ta\t2D\tshape\tthat\tcan\tbe\tbent and\ttwisted\tin\ta\thigher-dimensional\tspace.\tMore\tgenerally,\ta\td-dimensional\tmanifold\tis\ta\tpart\tof\tan\tn- dimensional\tspace\t(where\td\t<\tn)\tthat\tlocally\tresembles\ta\td-dimensional\thyperplane.\tIn\tthe\tcase\tof\tthe Swiss\troll,\td\t=\t2\tand\tn\t=\t3:\tit\tlocally\tresembles\ta\t2D\tplane,\tbut\tit\tis\trolled\tin\tthe\tthird\tdimension.\n\nMany\tdimensionality\treduction\talgorithms\twork\tby\tmodeling\tthe\tmanifold\ton\twhich\tthe\ttraining\tinstances lie;\tthis\tis\tcalled\tManifold\tLearning.\tIt\trelies\ton\tthe\tmanifold\tassumption,\talso\tcalled\tthe\tmanifold hypothesis,\twhich\tholds\tthat\tmost\treal-world\thigh-dimensional\tdatasets\tlie\tclose\tto\ta\tmuch\tlower- dimensional\tmanifold.\tThis\tassumption\tis\tvery\toften\tempirically\tobserved.\n\nOnce\tagain,\tthink\tabout\tthe\tMNIST\tdataset:\tall\thandwritten\tdigit\timages\thave\tsome\tsimilarities.\tThey\tare made\tof\tconnected\tlines,\tthe\tborders\tare\twhite,\tthey\tare\tmore\tor\tless\tcentered,\tand\tso\ton.\tIf\tyou\trandomly generated\timages,\tonly\ta\tridiculously\ttiny\tfraction\tof\tthem\twould\tlook\tlike\thandwritten\tdigits.\tIn\tother words,\tthe\tdegrees\tof\tfreedom\tavailable\tto\tyou\tif\tyou\ttry\tto\tcreate\ta\tdigit\timage\tare\tdramatically\tlower than\tthe\tdegrees\tof\tfreedom\tyou\twould\thave\tif\tyou\twere\tallowed\tto\tgenerate\tany\timage\tyou\twanted. These\tconstraints\ttend\tto\tsqueeze\tthe\tdataset\tinto\ta\tlower-dimensional\tmanifold.\n\nThe\tmanifold\tassumption\tis\toften\taccompanied\tby\tanother\timplicit\tassumption:\tthat\tthe\ttask\tat\thand\t(e.g., classification\tor\tregression)\twill\tbe\tsimpler\tif\texpressed\tin\tthe\tlower-dimensional\tspace\tof\tthe\tmanifold. For\texample,\tin\tthe\ttop\trow\tof\tFigure\t8-6\tthe\tSwiss\troll\tis\tsplit\tinto\ttwo\tclasses:\tin\tthe\t3D\tspace\t(on\tthe left),\tthe\tdecision\tboundary\twould\tbe\tfairly\tcomplex,\tbut\tin\tthe\t2D\tunrolled\tmanifold\tspace\t(on\tthe\tright), the\tdecision\tboundary\tis\ta\tsimple\tstraight\tline.\n\nHowever,\tthis\tassumption\tdoes\tnot\talways\thold.\tFor\texample,\tin\tthe\tbottom\trow\tof\tFigure\t8-6,\tthe decision\tboundary\tis\tlocated\tat\tx1\t=\t5.\tThis\tdecision\tboundary\tlooks\tvery\tsimple\tin\tthe\toriginal\t3D\tspace (a\tvertical\tplane),\tbut\tit\tlooks\tmore\tcomplex\tin\tthe\tunrolled\tmanifold\t(a\tcollection\tof\tfour\tindependent line\tsegments).\n\nIn\tshort,\tif\tyou\treduce\tthe\tdimensionality\tof\tyour\ttraining\tset\tbefore\ttraining\ta\tmodel,\tit\twill\tdefinitely speed\tup\ttraining,\tbut\tit\tmay\tnot\talways\tlead\tto\ta\tbetter\tor\tsimpler\tsolution;\tit\tall\tdepends\ton\tthe\tdataset.\n\nHopefully\tyou\tnow\thave\ta\tgood\tsense\tof\twhat\tthe\tcurse\tof\tdimensionality\tis\tand\thow\tdimensionality reduction\talgorithms\tcan\tfight\tit,\tespecially\twhen\tthe\tmanifold\tassumption\tholds.\tThe\trest\tof\tthis\tchapter will\tgo\tthrough\tsome\tof\tthe\tmost\tpopular\talgorithms.\n\nFigure\t8-6.\tThe\tdecision\tboundary\tmay\tnot\talways\tbe\tsimpler\twith\tlower\tdimensions\n\nPCA Principal\tComponent\tAnalysis\t(PCA)\tis\tby\tfar\tthe\tmost\tpopular\tdimensionality\treduction\talgorithm. First\tit\tidentifies\tthe\thyperplane\tthat\tlies\tclosest\tto\tthe\tdata,\tand\tthen\tit\tprojects\tthe\tdata\tonto\tit.\n\nPreserving\tthe\tVariance Before\tyou\tcan\tproject\tthe\ttraining\tset\tonto\ta\tlower-dimensional\thyperplane,\tyou\tfirst\tneed\tto\tchoose\tthe right\thyperplane.\tFor\texample,\ta\tsimple\t2D\tdataset\tis\trepresented\ton\tthe\tleft\tof\tFigure\t8-7,\talong\twith three\tdifferent\taxes\t(i.e.,\tone-dimensional\thyperplanes).\tOn\tthe\tright\tis\tthe\tresult\tof\tthe\tprojection\tof\tthe dataset\tonto\teach\tof\tthese\taxes.\tAs\tyou\tcan\tsee,\tthe\tprojection\tonto\tthe\tsolid\tline\tpreserves\tthe\tmaximum variance,\twhile\tthe\tprojection\tonto\tthe\tdotted\tline\tpreserves\tvery\tlittle\tvariance,\tand\tthe\tprojection\tonto the\tdashed\tline\tpreserves\tan\tintermediate\tamount\tof\tvariance.\n\nFigure\t8-7.\tSelecting\tthe\tsubspace\tonto\twhich\tto\tproject\n\nIt\tseems\treasonable\tto\tselect\tthe\taxis\tthat\tpreserves\tthe\tmaximum\tamount\tof\tvariance,\tas\tit\twill\tmost likely\tlose\tless\tinformation\tthan\tthe\tother\tprojections.\tAnother\tway\tto\tjustify\tthis\tchoice\tis\tthat\tit\tis\tthe axis\tthat\tminimizes\tthe\tmean\tsquared\tdistance\tbetween\tthe\toriginal\tdataset\tand\tits\tprojection\tonto\tthat axis.\tThis\tis\tthe\trather\tsimple\tidea\tbehind\tPCA.4\n\nPrincipal\tComponents PCA\tidentifies\tthe\taxis\tthat\taccounts\tfor\tthe\tlargest\tamount\tof\tvariance\tin\tthe\ttraining\tset.\tIn\tFigure\t8-7,\tit is\tthe\tsolid\tline.\tIt\talso\tfinds\ta\tsecond\taxis,\torthogonal\tto\tthe\tfirst\tone,\tthat\taccounts\tfor\tthe\tlargest\tamount of\tremaining\tvariance.\tIn\tthis\t2D\texample\tthere\tis\tno\tchoice:\tit\tis\tthe\tdotted\tline.\tIf\tit\twere\ta\thigher- dimensional\tdataset,\tPCA\twould\talso\tfind\ta\tthird\taxis,\torthogonal\tto\tboth\tprevious\taxes,\tand\ta\tfourth,\ta fifth,\tand\tso\ton\t—\tas\tmany\taxes\tas\tthe\tnumber\tof\tdimensions\tin\tthe\tdataset. The\tunit\tvector\tthat\tdefines\tthe\tith\taxis\tis\tcalled\tthe\tith\tprincipal\tcomponent\t(PC).\tIn\tFigure\t8-7,\tthe\t1st PC\tis\tc1\tand\tthe\t2nd\tPC\tis\tc2.\tIn\tFigure\t8-2\tthe\tfirst\ttwo\tPCs\tare\trepresented\tby\tthe\torthogonal\tarrows\tin the\tplane,\tand\tthe\tthird\tPC\twould\tbe\torthogonal\tto\tthe\tplane\t(pointing\tup\tor\tdown).\n\nNOTE\n\nThe\tdirection\tof\tthe\tprincipal\tcomponents\tis\tnot\tstable:\tif\tyou\tperturb\tthe\ttraining\tset\tslightly\tand\trun\tPCA\tagain,\tsome\tof\tthe\tnew PCs\tmay\tpoint\tin\tthe\topposite\tdirection\tof\tthe\toriginal\tPCs.\tHowever,\tthey\twill\tgenerally\tstill\tlie\ton\tthe\tsame\taxes.\tIn\tsome\tcases, a\tpair\tof\tPCs\tmay\teven\trotate\tor\tswap,\tbut\tthe\tplane\tthey\tdefine\twill\tgenerally\tremain\tthe\tsame.\n\nSo\thow\tcan\tyou\tfind\tthe\tprincipal\tcomponents\tof\ta\ttraining\tset?\tLuckily,\tthere\tis\ta\tstandard\tmatrix factorization\ttechnique\tcalled\tSingular\tValue\tDecomposition\t(SVD)\tthat\tcan\tdecompose\tthe\ttraining\tset matrix\tX\tinto\tthe\tdot\tproduct\tof\tthree\tmatrices\tU\t·\tΣ\t·\tVT,\twhere\tVT\tcontains\tall\tthe\tprincipal\tcomponents that\twe\tare\tlooking\tfor,\tas\tshown\tin\tEquation\t8-1.\n\nEquation\t8-1.\tPrincipal\tcomponents\tmatrix\n\nThe\tfollowing\tPython\tcode\tuses\tNumPy’s\tsvd()\tfunction\tto\tobtain\tall\tthe\tprincipal\tcomponents\tof\tthe training\tset,\tthen\textracts\tthe\tfirst\ttwo\tPCs:\n\nX_centered\t=\tX\t-\tX.mean(axis=0) U,\ts,\tV\t=\tnp.linalg.svd(X_centered) c1\t=\tV.T[:,\t0] c2\t=\tV.T[:,\t1]\n\nWARNING\n\nPCA\tassumes\tthat\tthe\tdataset\tis\tcentered\taround\tthe\torigin.\tAs\twe\twill\tsee,\tScikit-Learn’s\tPCA\tclasses\ttake\tcare\tof\tcentering the\tdata\tfor\tyou.\tHowever,\tif\tyou\timplement\tPCA\tyourself\t(as\tin\tthe\tpreceding\texample),\tor\tif\tyou\tuse\tother\tlibraries,\tdon’t forget\tto\tcenter\tthe\tdata\tfirst.\n\nProjecting\tDown\tto\td\tDimensions Once\tyou\thave\tidentified\tall\tthe\tprincipal\tcomponents,\tyou\tcan\treduce\tthe\tdimensionality\tof\tthe\tdataset down\tto\td\tdimensions\tby\tprojecting\tit\tonto\tthe\thyperplane\tdefined\tby\tthe\tfirst\td\tprincipal\tcomponents. Selecting\tthis\thyperplane\tensures\tthat\tthe\tprojection\twill\tpreserve\tas\tmuch\tvariance\tas\tpossible.\tFor example,\tin\tFigure\t8-2\tthe\t3D\tdataset\tis\tprojected\tdown\tto\tthe\t2D\tplane\tdefined\tby\tthe\tfirst\ttwo\tprincipal components,\tpreserving\ta\tlarge\tpart\tof\tthe\tdataset’s\tvariance.\tAs\ta\tresult,\tthe\t2D\tprojection\tlooks\tvery much\tlike\tthe\toriginal\t3D\tdataset.\n\nTo\tproject\tthe\ttraining\tset\tonto\tthe\thyperplane,\tyou\tcan\tsimply\tcompute\tthe\tdot\tproduct\tof\tthe\ttraining\tset matrix\tX\tby\tthe\tmatrix\tWd,\tdefined\tas\tthe\tmatrix\tcontaining\tthe\tfirst\td\tprincipal\tcomponents\t(i.e.,\tthe matrix\tcomposed\tof\tthe\tfirst\td\tcolumns\tof\tVT),\tas\tshown\tin\tEquation\t8-2.\n\nEquation\t8-2.\tProjecting\tthe\ttraining\tset\tdown\tto\td\tdimensions\n\nThe\tfollowing\tPython\tcode\tprojects\tthe\ttraining\tset\tonto\tthe\tplane\tdefined\tby\tthe\tfirst\ttwo\tprincipal components:\n\nW2\t=\tV.T[:,\t:2] X2D\t=\tX_centered.dot(W2)\n\nThere\tyou\thave\tit!\tYou\tnow\tknow\thow\tto\treduce\tthe\tdimensionality\tof\tany\tdataset\tdown\tto\tany\tnumber\tof dimensions,\twhile\tpreserving\tas\tmuch\tvariance\tas\tpossible.\n\nUsing\tScikit-Learn Scikit-Learn’s\tPCA\tclass\timplements\tPCA\tusing\tSVD\tdecomposition\tjust\tlike\twe\tdid\tbefore.\tThe following\tcode\tapplies\tPCA\tto\treduce\tthe\tdimensionality\tof\tthe\tdataset\tdown\tto\ttwo\tdimensions\t(note that\tit\tautomatically\ttakes\tcare\tof\tcentering\tthe\tdata):\n\nfrom\tsklearn.decomposition\timport\tPCA\n\npca\t=\tPCA(n_components\t=\t2) X2D\t=\tpca.fit_transform(X)\n\nAfter\tfitting\tthe\tPCA\ttransformer\tto\tthe\tdataset,\tyou\tcan\taccess\tthe\tprincipal\tcomponents\tusing\tthe components_\tvariable\t(note\tthat\tit\tcontains\tthe\tPCs\tas\thorizontal\tvectors,\tso,\tfor\texample,\tthe\tfirst principal\tcomponent\tis\tequal\tto\tpca.components_.T[:,\t0]).\n\nExplained\tVariance\tRatio Another\tvery\tuseful\tpiece\tof\tinformation\tis\tthe\texplained\tvariance\tratio\tof\teach\tprincipal\tcomponent, available\tvia\tthe\texplained_variance_ratio_\tvariable.\tIt\tindicates\tthe\tproportion\tof\tthe\tdataset’s variance\tthat\tlies\talong\tthe\taxis\tof\teach\tprincipal\tcomponent.\tFor\texample,\tlet’s\tlook\tat\tthe\texplained variance\tratios\tof\tthe\tfirst\ttwo\tcomponents\tof\tthe\t3D\tdataset\trepresented\tin\tFigure\t8-2:\n\n>>>\tpca.explained_variance_ratio_ array([\t0.84248607,\t\t0.14631839])\n\nThis\ttells\tyou\tthat\t84.2%\tof\tthe\tdataset’s\tvariance\tlies\talong\tthe\tfirst\taxis,\tand\t14.6%\tlies\talong\tthe second\taxis.\tThis\tleaves\tless\tthan\t1.2%\tfor\tthe\tthird\taxis,\tso\tit\tis\treasonable\tto\tassume\tthat\tit\tprobably carries\tlittle\tinformation.\n\nChoosing\tthe\tRight\tNumber\tof\tDimensions Instead\tof\tarbitrarily\tchoosing\tthe\tnumber\tof\tdimensions\tto\treduce\tdown\tto,\tit\tis\tgenerally\tpreferable\tto choose\tthe\tnumber\tof\tdimensions\tthat\tadd\tup\tto\ta\tsufficiently\tlarge\tportion\tof\tthe\tvariance\t(e.g.,\t95%). Unless,\tof\tcourse,\tyou\tare\treducing\tdimensionality\tfor\tdata\tvisualization\t—\tin\tthat\tcase\tyou\twill generally\twant\tto\treduce\tthe\tdimensionality\tdown\tto\t2\tor\t3.\n\nThe\tfollowing\tcode\tcomputes\tPCA\twithout\treducing\tdimensionality,\tthen\tcomputes\tthe\tminimum\tnumber of\tdimensions\trequired\tto\tpreserve\t95%\tof\tthe\ttraining\tset’s\tvariance:\n\npca\t=\tPCA() pca.fit(X_train) cumsum\t=\tnp.cumsum(pca.explained_variance_ratio_) d\t=\tnp.argmax(cumsum\t>=\t0.95)\t+\t1\n\nYou\tcould\tthen\tset\tn_components=d\tand\trun\tPCA\tagain.\tHowever,\tthere\tis\ta\tmuch\tbetter\toption:\tinstead of\tspecifying\tthe\tnumber\tof\tprincipal\tcomponents\tyou\twant\tto\tpreserve,\tyou\tcan\tset\tn_components\tto\tbe a\tfloat\tbetween\t0.0\tand\t1.0,\tindicating\tthe\tratio\tof\tvariance\tyou\twish\tto\tpreserve:\n\npca\t=\tPCA(n_components=0.95) X_reduced\t=\tpca.fit_transform(X_train)\n\nYet\tanother\toption\tis\tto\tplot\tthe\texplained\tvariance\tas\ta\tfunction\tof\tthe\tnumber\tof\tdimensions\t(simply\tplot cumsum;\tsee\tFigure\t8-8).\tThere\twill\tusually\tbe\tan\telbow\tin\tthe\tcurve,\twhere\tthe\texplained\tvariance\tstops growing\tfast.\tYou\tcan\tthink\tof\tthis\tas\tthe\tintrinsic\tdimensionality\tof\tthe\tdataset.\tIn\tthis\tcase,\tyou\tcan\tsee that\treducing\tthe\tdimensionality\tdown\tto\tabout\t100\tdimensions\twouldn’t\tlose\ttoo\tmuch\texplained variance.\n\nFigure\t8-8.\tExplained\tvariance\tas\ta\tfunction\tof\tthe\tnumber\tof\tdimensions\n\nPCA\tfor\tCompression Obviously\tafter\tdimensionality\treduction,\tthe\ttraining\tset\ttakes\tup\tmuch\tless\tspace.\tFor\texample,\ttry applying\tPCA\tto\tthe\tMNIST\tdataset\twhile\tpreserving\t95%\tof\tits\tvariance.\tYou\tshould\tfind\tthat\teach instance\twill\thave\tjust\tover\t150\tfeatures,\tinstead\tof\tthe\toriginal\t784\tfeatures.\tSo\twhile\tmost\tof\tthe variance\tis\tpreserved,\tthe\tdataset\tis\tnow\tless\tthan\t20%\tof\tits\toriginal\tsize!\tThis\tis\ta\treasonable compression\tratio,\tand\tyou\tcan\tsee\thow\tthis\tcan\tspeed\tup\ta\tclassification\talgorithm\t(such\tas\tan\tSVM classifier)\ttremendously.\n\nIt\tis\talso\tpossible\tto\tdecompress\tthe\treduced\tdataset\tback\tto\t784\tdimensions\tby\tapplying\tthe\tinverse transformation\tof\tthe\tPCA\tprojection.\tOf\tcourse\tthis\twon’t\tgive\tyou\tback\tthe\toriginal\tdata,\tsince\tthe projection\tlost\ta\tbit\tof\tinformation\t(within\tthe\t5%\tvariance\tthat\twas\tdropped),\tbut\tit\twill\tlikely\tbe\tquite close\tto\tthe\toriginal\tdata.\tThe\tmean\tsquared\tdistance\tbetween\tthe\toriginal\tdata\tand\tthe\treconstructed\tdata (compressed\tand\tthen\tdecompressed)\tis\tcalled\tthe\treconstruction\terror.\tFor\texample,\tthe\tfollowing\tcode compresses\tthe\tMNIST\tdataset\tdown\tto\t154\tdimensions,\tthen\tuses\tthe\tinverse_transform()\tmethod\tto decompress\tit\tback\tto\t784\tdimensions.\tFigure\t8-9\tshows\ta\tfew\tdigits\tfrom\tthe\toriginal\ttraining\tset\t(on\tthe left),\tand\tthe\tcorresponding\tdigits\tafter\tcompression\tand\tdecompression.\tYou\tcan\tsee\tthat\tthere\tis\ta\tslight image\tquality\tloss,\tbut\tthe\tdigits\tare\tstill\tmostly\tintact.\n\npca\t=\tPCA(n_components\t=\t154) X_reduced\t=\tpca.fit_transform(X_train) X_recovered\t=\tpca.inverse_transform(X_reduced)\n\nFigure\t8-9.\tMNIST\tcompression\tpreserving\t95%\tof\tthe\tvariance\n\nThe\tequation\tof\tthe\tinverse\ttransformation\tis\tshown\tin\tEquation\t8-3.\n\nEquation\t8-3.\tPCA\tinverse\ttransformation,\tback\tto\tthe\toriginal\tnumber\tof\tdimensions\n\nIncremental\tPCA One\tproblem\twith\tthe\tpreceding\timplementation\tof\tPCA\tis\tthat\tit\trequires\tthe\twhole\ttraining\tset\tto\tfit\tin memory\tin\torder\tfor\tthe\tSVD\talgorithm\tto\trun.\tFortunately,\tIncremental\tPCA\t(IPCA)\talgorithms\thave been\tdeveloped:\tyou\tcan\tsplit\tthe\ttraining\tset\tinto\tmini-batches\tand\tfeed\tan\tIPCA\talgorithm\tone\tmini- batch\tat\ta\ttime.\tThis\tis\tuseful\tfor\tlarge\ttraining\tsets,\tand\talso\tto\tapply\tPCA\tonline\t(i.e.,\ton\tthe\tfly,\tas\tnew instances\tarrive).\n\nThe\tfollowing\tcode\tsplits\tthe\tMNIST\tdataset\tinto\t100\tmini-batches\t(using\tNumPy’s\tarray_split() function)\tand\tfeeds\tthem\tto\tScikit-Learn’s\tIncrementalPCA\tclass5\tto\treduce\tthe\tdimensionality\tof\tthe MNIST\tdataset\tdown\tto\t154\tdimensions\t(just\tlike\tbefore).\tNote\tthat\tyou\tmust\tcall\tthe\tpartial_fit() method\twith\teach\tmini-batch\trather\tthan\tthe\tfit()\tmethod\twith\tthe\twhole\ttraining\tset:\n\nfrom\tsklearn.decomposition\timport\tIncrementalPCA\n\nn_batches\t=\t100 inc_pca\t=\tIncrementalPCA(n_components=154) for\tX_batch\tin\tnp.array_split(X_train,\tn_batches): \t\t\t\tinc_pca.partial_fit(X_batch)\n\nX_reduced\t=\tinc_pca.transform(X_train)\n\nAlternatively,\tyou\tcan\tuse\tNumPy’s\tmemmap\tclass,\twhich\tallows\tyou\tto\tmanipulate\ta\tlarge\tarray\tstored\tin a\tbinary\tfile\ton\tdisk\tas\tif\tit\twere\tentirely\tin\tmemory;\tthe\tclass\tloads\tonly\tthe\tdata\tit\tneeds\tin\tmemory, when\tit\tneeds\tit.\tSince\tthe\tIncrementalPCA\tclass\tuses\tonly\ta\tsmall\tpart\tof\tthe\tarray\tat\tany\tgiven\ttime, the\tmemory\tusage\tremains\tunder\tcontrol.\tThis\tmakes\tit\tpossible\tto\tcall\tthe\tusual\tfit()\tmethod,\tas\tyou can\tsee\tin\tthe\tfollowing\tcode:\n\nX_mm\t=\tnp.memmap(filename,\tdtype=\"float32\",\tmode=\"readonly\",\tshape=(m,\tn))\n\nbatch_size\t=\tm\t//\tn_batches inc_pca\t=\tIncrementalPCA(n_components=154,\tbatch_size=batch_size) inc_pca.fit(X_mm)\n\nRandomized\tPCA Scikit-Learn\toffers\tyet\tanother\toption\tto\tperform\tPCA,\tcalled\tRandomized\tPCA.\tThis\tis\ta\tstochastic algorithm\tthat\tquickly\tfinds\tan\tapproximation\tof\tthe\tfirst\td\tprincipal\tcomponents.\tIts\tcomputational complexity\tis\tO(m\t×\td2)\t+\tO(d3),\tinstead\tof\tO(m\t×\tn2)\t+\tO(n3),\tso\tit\tis\tdramatically\tfaster\tthan\tthe previous\talgorithms\twhen\td\tis\tmuch\tsmaller\tthan\tn.\n\nrnd_pca\t=\tPCA(n_components=154,\tsvd_solver=\"randomized\") X_reduced\t=\trnd_pca.fit_transform(X_train)\n\nKernel\tPCA In\tChapter\t5\twe\tdiscussed\tthe\tkernel\ttrick,\ta\tmathematical\ttechnique\tthat\timplicitly\tmaps\tinstances\tinto\ta very\thigh-dimensional\tspace\t(called\tthe\tfeature\tspace),\tenabling\tnonlinear\tclassification\tand\tregression with\tSupport\tVector\tMachines.\tRecall\tthat\ta\tlinear\tdecision\tboundary\tin\tthe\thigh-dimensional\tfeature space\tcorresponds\tto\ta\tcomplex\tnonlinear\tdecision\tboundary\tin\tthe\toriginal\tspace.\n\nIt\tturns\tout\tthat\tthe\tsame\ttrick\tcan\tbe\tapplied\tto\tPCA,\tmaking\tit\tpossible\tto\tperform\tcomplex\tnonlinear projections\tfor\tdimensionality\treduction.\tThis\tis\tcalled\tKernel\tPCA\t(kPCA).6\tIt\tis\toften\tgood\tat preserving\tclusters\tof\tinstances\tafter\tprojection,\tor\tsometimes\teven\tunrolling\tdatasets\tthat\tlie\tclose\tto\ta twisted\tmanifold.\n\nFor\texample,\tthe\tfollowing\tcode\tuses\tScikit-Learn’s\tKernelPCA\tclass\tto\tperform\tkPCA\twith\tan\tRBF kernel\t(see\tChapter\t5\tfor\tmore\tdetails\tabout\tthe\tRBF\tkernel\tand\tthe\tother\tkernels):\n\nfrom\tsklearn.decomposition\timport\tKernelPCA\n\nrbf_pca\t=\tKernelPCA(n_components\t=\t2,\tkernel=\"rbf\",\tgamma=0.04) X_reduced\t=\trbf_pca.fit_transform(X)\n\nFigure\t8-10\tshows\tthe\tSwiss\troll,\treduced\tto\ttwo\tdimensions\tusing\ta\tlinear\tkernel\t(equivalent\tto\tsimply using\tthe\tPCA\tclass),\tan\tRBF\tkernel,\tand\ta\tsigmoid\tkernel\t(Logistic).\n\nFigure\t8-10.\tSwiss\troll\treduced\tto\t2D\tusing\tkPCA\twith\tvarious\tkernels\n\nSelecting\ta\tKernel\tand\tTuning\tHyperparameters As\tkPCA\tis\tan\tunsupervised\tlearning\talgorithm,\tthere\tis\tno\tobvious\tperformance\tmeasure\tto\thelp\tyou select\tthe\tbest\tkernel\tand\thyperparameter\tvalues.\tHowever,\tdimensionality\treduction\tis\toften\ta preparation\tstep\tfor\ta\tsupervised\tlearning\ttask\t(e.g.,\tclassification),\tso\tyou\tcan\tsimply\tuse\tgrid\tsearch\tto select\tthe\tkernel\tand\thyperparameters\tthat\tlead\tto\tthe\tbest\tperformance\ton\tthat\ttask.\tFor\texample,\tthe following\tcode\tcreates\ta\ttwo-step\tpipeline,\tfirst\treducing\tdimensionality\tto\ttwo\tdimensions\tusing\tkPCA, then\tapplying\tLogistic\tRegression\tfor\tclassification.\tThen\tit\tuses\tGridSearchCV\tto\tfind\tthe\tbest\tkernel and\tgamma\tvalue\tfor\tkPCA\tin\torder\tto\tget\tthe\tbest\tclassification\taccuracy\tat\tthe\tend\tof\tthe\tpipeline:\n\nfrom\tsklearn.model_selection\timport\tGridSearchCV from\tsklearn.linear_model\timport\tLogisticRegression from\tsklearn.pipeline\timport\tPipeline\n\nclf\t=\tPipeline([ \t\t\t\t\t\t\t\t(\"kpca\",\tKernelPCA(n_components=2)), \t\t\t\t\t\t\t\t(\"log_reg\",\tLogisticRegression()) \t\t\t\t])\n\nparam_grid\t=\t[{ \t\t\t\t\t\t\t\t\"kpca__gamma\":\tnp.linspace(0.03,\t0.05,\t10), \t\t\t\t\t\t\t\t\"kpca__kernel\":\t[\"rbf\",\t\"sigmoid\"] \t\t\t\t}]\n\ngrid_search\t=\tGridSearchCV(clf,\tparam_grid,\tcv=3) grid_search.fit(X,\ty)\n\nThe\tbest\tkernel\tand\thyperparameters\tare\tthen\tavailable\tthrough\tthe\tbest_params_\tvariable:\n\n>>>\tprint(grid_search.best_params_) {'kpca__gamma':\t0.043333333333333335,\t'kpca__kernel':\t'rbf'}\n\nAnother\tapproach,\tthis\ttime\tentirely\tunsupervised,\tis\tto\tselect\tthe\tkernel\tand\thyperparameters\tthat\tyield the\tlowest\treconstruction\terror.\tHowever,\treconstruction\tis\tnot\tas\teasy\tas\twith\tlinear\tPCA.\tHere’s\twhy. Figure\t8-11\tshows\tthe\toriginal\tSwiss\troll\t3D\tdataset\t(top\tleft),\tand\tthe\tresulting\t2D\tdataset\tafter\tkPCA\tis applied\tusing\tan\tRBF\tkernel\t(top\tright).\tThanks\tto\tthe\tkernel\ttrick,\tthis\tis\tmathematically\tequivalent\tto mapping\tthe\ttraining\tset\tto\tan\tinfinite-dimensional\tfeature\tspace\t(bottom\tright)\tusing\tthe\tfeature\tmap\tφ, then\tprojecting\tthe\ttransformed\ttraining\tset\tdown\tto\t2D\tusing\tlinear\tPCA.\tNotice\tthat\tif\twe\tcould\tinvert the\tlinear\tPCA\tstep\tfor\ta\tgiven\tinstance\tin\tthe\treduced\tspace,\tthe\treconstructed\tpoint\twould\tlie\tin\tfeature space,\tnot\tin\tthe\toriginal\tspace\t(e.g.,\tlike\tthe\tone\trepresented\tby\tan\tx\tin\tthe\tdiagram).\tSince\tthe\tfeature space\tis\tinfinite-dimensional,\twe\tcannot\tcompute\tthe\treconstructed\tpoint,\tand\ttherefore\twe\tcannot compute\tthe\ttrue\treconstruction\terror.\tFortunately,\tit\tis\tpossible\tto\tfind\ta\tpoint\tin\tthe\toriginal\tspace\tthat would\tmap\tclose\tto\tthe\treconstructed\tpoint.\tThis\tis\tcalled\tthe\treconstruction\tpre-image.\tOnce\tyou\thave this\tpre-image,\tyou\tcan\tmeasure\tits\tsquared\tdistance\tto\tthe\toriginal\tinstance.\tYou\tcan\tthen\tselect\tthe kernel\tand\thyperparameters\tthat\tminimize\tthis\treconstruction\tpre-image\terror.\n\nFigure\t8-11.\tKernel\tPCA\tand\tthe\treconstruction\tpre-image\terror\n\nYou\tmay\tbe\twondering\thow\tto\tperform\tthis\treconstruction.\tOne\tsolution\tis\tto\ttrain\ta\tsupervised regression\tmodel,\twith\tthe\tprojected\tinstances\tas\tthe\ttraining\tset\tand\tthe\toriginal\tinstances\tas\tthe\ttargets. Scikit-Learn\twill\tdo\tthis\tautomatically\tif\tyou\tset\tfit_inverse_transform=True,\tas\tshown\tin\tthe following\tcode:7\n\nrbf_pca\t=\tKernelPCA(n_components\t=\t2,\tkernel=\"rbf\",\tgamma=0.0433, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfit_inverse_transform=True) X_reduced\t=\trbf_pca.fit_transform(X) X_preimage\t=\trbf_pca.inverse_transform(X_reduced)\n\nNOTE\n\nBy\tdefault,\tfit_inverse_transform=False\tand\tKernelPCA\thas\tno\tinverse_transform()\tmethod.\tThis\tmethod\tonly\tgets created\twhen\tyou\tset\tfit_inverse_transform=True.\n\nYou\tcan\tthen\tcompute\tthe\treconstruction\tpre-image\terror:\n\n>>>\tfrom\tsklearn.metrics\timport\tmean_squared_error >>>\tmean_squared_error(X,\tX_preimage) 32.786308795766132\n\nNow\tyou\tcan\tuse\tgrid\tsearch\twith\tcross-validation\tto\tfind\tthe\tkernel\tand\thyperparameters\tthat\tminimize this\tpre-image\treconstruction\terror.\n\nLLE Locally\tLinear\tEmbedding\t(LLE)8\tis\tanother\tvery\tpowerful\tnonlinear\tdimensionality\treduction (NLDR)\ttechnique.\tIt\tis\ta\tManifold\tLearning\ttechnique\tthat\tdoes\tnot\trely\ton\tprojections\tlike\tthe\tprevious algorithms.\tIn\ta\tnutshell,\tLLE\tworks\tby\tfirst\tmeasuring\thow\teach\ttraining\tinstance\tlinearly\trelates\tto\tits closest\tneighbors\t(c.n.),\tand\tthen\tlooking\tfor\ta\tlow-dimensional\trepresentation\tof\tthe\ttraining\tset\twhere these\tlocal\trelationships\tare\tbest\tpreserved\t(more\tdetails\tshortly).\tThis\tmakes\tit\tparticularly\tgood\tat unrolling\ttwisted\tmanifolds,\tespecially\twhen\tthere\tis\tnot\ttoo\tmuch\tnoise.\n\nFor\texample,\tthe\tfollowing\tcode\tuses\tScikit-Learn’s\tLocallyLinearEmbedding\tclass\tto\tunroll\tthe\tSwiss roll.\tThe\tresulting\t2D\tdataset\tis\tshown\tin\tFigure\t8-12.\tAs\tyou\tcan\tsee,\tthe\tSwiss\troll\tis\tcompletely unrolled\tand\tthe\tdistances\tbetween\tinstances\tare\tlocally\twell\tpreserved.\tHowever,\tdistances\tare\tnot preserved\ton\ta\tlarger\tscale:\tthe\tleft\tpart\tof\tthe\tunrolled\tSwiss\troll\tis\tsqueezed,\twhile\tthe\tright\tpart\tis stretched.\tNevertheless,\tLLE\tdid\ta\tpretty\tgood\tjob\tat\tmodeling\tthe\tmanifold.\n\nfrom\tsklearn.manifold\timport\tLocallyLinearEmbedding\n\nlle\t=\tLocallyLinearEmbedding(n_components=2,\tn_neighbors=10) X_reduced\t=\tlle.fit_transform(X)\n\nFigure\t8-12.\tUnrolled\tSwiss\troll\tusing\tLLE\n\nHere’s\thow\tLLE\tworks:\tfirst,\tfor\teach\ttraining\tinstance\tx(i),\tthe\talgorithm\tidentifies\tits\tk\tclosest neighbors\t(in\tthe\tpreceding\tcode\tk\t=\t10),\tthen\ttries\tto\treconstruct\tx(i)\tas\ta\tlinear\tfunction\tof\tthese neighbors.\tMore\tspecifically,\tit\tfinds\tthe\tweights\twi,j\tsuch\tthat\tthe\tsquared\tdistance\tbetween\tx(i)\tand\t \tis\tas\tsmall\tas\tpossible,\tassuming\twi,j\t=\t0\tif\tx(j)\tis\tnot\tone\tof\tthe\tk\tclosest\tneighbors\tof\tx(i).\n\nThus\tthe\tfirst\tstep\tof\tLLE\tis\tthe\tconstrained\toptimization\tproblem\tdescribed\tin\tEquation\t8-4,\twhere\tW\tis\n\nthe\tweight\tmatrix\tcontaining\tall\tthe\tweights\twi,j.\tThe\tsecond\tconstraint\tsimply\tnormalizes\tthe\tweights\tfor each\ttraining\tinstance\tx(i).\n\nEquation\t8-4.\tLLE\tstep\t1:\tlinearly\tmodeling\tlocal\trelationships\n\nAfter\tthis\tstep,\tthe\tweight\tmatrix\t between\tthe\ttraining\tinstances.\tNow\tthe\tsecond\tstep\tis\tto\tmap\tthe\ttraining\tinstances\tinto\ta\td-dimensional space\t(where\td\t<\tn)\twhile\tpreserving\tthese\tlocal\trelationships\tas\tmuch\tas\tpossible.\tIf\tz(i)\tis\tthe\timage\tof\n\n(containing\tthe\tweights\n\n)\tencodes\tthe\tlocal\tlinear\trelationships\n\nx(i)\tin\tthis\td-dimensional\tspace,\tthen\twe\twant\tthe\tsquared\tdistance\tbetween\tz(i)\tand\t small\tas\tpossible.\tThis\tidea\tleads\tto\tthe\tunconstrained\toptimization\tproblem\tdescribed\tin\tEquation\t8-5.\tIt looks\tvery\tsimilar\tto\tthe\tfirst\tstep,\tbut\tinstead\tof\tkeeping\tthe\tinstances\tfixed\tand\tfinding\tthe\toptimal weights,\twe\tare\tdoing\tthe\treverse:\tkeeping\tthe\tweights\tfixed\tand\tfinding\tthe\toptimal\tposition\tof\tthe instances’\timages\tin\tthe\tlow-dimensional\tspace.\tNote\tthat\tZ\tis\tthe\tmatrix\tcontaining\tall\tz(i).\n\nto\tbe\tas\n\nEquation\t8-5.\tLLE\tstep\t2:\treducing\tdimensionality\twhile\tpreserving\trelationships\n\nScikit-Learn’s\tLLE\timplementation\thas\tthe\tfollowing\tcomputational\tcomplexity:\tO(m\tlog(m)n\tlog(k))\tfor finding\tthe\tk\tnearest\tneighbors,\tO(mnk3)\tfor\toptimizing\tthe\tweights,\tand\tO(dm2)\tfor\tconstructing\tthe\tlow- dimensional\trepresentations.\tUnfortunately,\tthe\tm2\tin\tthe\tlast\tterm\tmakes\tthis\talgorithm\tscale\tpoorly\tto very\tlarge\tdatasets.\n\nOther\tDimensionality\tReduction\tTechniques There\tare\tmany\tother\tdimensionality\treduction\ttechniques,\tseveral\tof\twhich\tare\tavailable\tin\tScikit-Learn. Here\tare\tsome\tof\tthe\tmost\tpopular:\n\nMultidimensional\tScaling\t(MDS)\treduces\tdimensionality\twhile\ttrying\tto\tpreserve\tthe\tdistances between\tthe\tinstances\t(see\tFigure\t8-13).\n\nIsomap\tcreates\ta\tgraph\tby\tconnecting\teach\tinstance\tto\tits\tnearest\tneighbors,\tthen\treduces dimensionality\twhile\ttrying\tto\tpreserve\tthe\tgeodesic\tdistances9\tbetween\tthe\tinstances.\n\nt-Distributed\tStochastic\tNeighbor\tEmbedding\t(t-SNE)\treduces\tdimensionality\twhile\ttrying\tto\tkeep similar\tinstances\tclose\tand\tdissimilar\tinstances\tapart.\tIt\tis\tmostly\tused\tfor\tvisualization,\tin particular\tto\tvisualize\tclusters\tof\tinstances\tin\thigh-dimensional\tspace\t(e.g.,\tto\tvisualize\tthe\tMNIST images\tin\t2D).\n\nLinear\tDiscriminant\tAnalysis\t(LDA)\tis\tactually\ta\tclassification\talgorithm,\tbut\tduring\ttraining\tit learns\tthe\tmost\tdiscriminative\taxes\tbetween\tthe\tclasses,\tand\tthese\taxes\tcan\tthen\tbe\tused\tto\tdefine\ta hyperplane\tonto\twhich\tto\tproject\tthe\tdata.\tThe\tbenefit\tis\tthat\tthe\tprojection\twill\tkeep\tclasses\tas\tfar apart\tas\tpossible,\tso\tLDA\tis\ta\tgood\ttechnique\tto\treduce\tdimensionality\tbefore\trunning\tanother classification\talgorithm\tsuch\tas\tan\tSVM\tclassifier.\n\nFigure\t8-13.\tReducing\tthe\tSwiss\troll\tto\t2D\tusing\tvarious\ttechniques\n\nExercises\n\n1.\t What\tare\tthe\tmain\tmotivations\tfor\treducing\ta\tdataset’s\tdimensionality?\tWhat\tare\tthe\tmain drawbacks?\n\n2.\t What\tis\tthe\tcurse\tof\tdimensionality?\n\n3.\t Once\ta\tdataset’s\tdimensionality\thas\tbeen\treduced,\tis\tit\tpossible\tto\treverse\tthe\toperation?\tIf\tso, how?\tIf\tnot,\twhy?\n\n4.\t Can\tPCA\tbe\tused\tto\treduce\tthe\tdimensionality\tof\ta\thighly\tnonlinear\tdataset?\n\n5.\t Suppose\tyou\tperform\tPCA\ton\ta\t1,000-dimensional\tdataset,\tsetting\tthe\texplained\tvariance\tratio\tto 95%.\tHow\tmany\tdimensions\twill\tthe\tresulting\tdataset\thave?\n\n6.\t In\twhat\tcases\twould\tyou\tuse\tvanilla\tPCA,\tIncremental\tPCA,\tRandomized\tPCA,\tor\tKernel\tPCA?\n\n7.\t How\tcan\tyou\tevaluate\tthe\tperformance\tof\ta\tdimensionality\treduction\talgorithm\ton\tyour\tdataset?\n\n8.\t Does\tit\tmake\tany\tsense\tto\tchain\ttwo\tdifferent\tdimensionality\treduction\talgorithms?\n\n9.\t Load\tthe\tMNIST\tdataset\t(introduced\tin\tChapter\t3)\tand\tsplit\tit\tinto\ta\ttraining\tset\tand\ta\ttest\tset\t(take the\tfirst\t60,000\tinstances\tfor\ttraining,\tand\tthe\tremaining\t10,000\tfor\ttesting).\tTrain\ta\tRandom\tForest classifier\ton\tthe\tdataset\tand\ttime\thow\tlong\tit\ttakes,\tthen\tevaluate\tthe\tresulting\tmodel\ton\tthe\ttest\tset. Next,\tuse\tPCA\tto\treduce\tthe\tdataset’s\tdimensionality,\twith\tan\texplained\tvariance\tratio\tof\t95%.\tTrain a\tnew\tRandom\tForest\tclassifier\ton\tthe\treduced\tdataset\tand\tsee\thow\tlong\tit\ttakes.\tWas\ttraining\tmuch faster?\tNext\tevaluate\tthe\tclassifier\ton\tthe\ttest\tset:\thow\tdoes\tit\tcompare\tto\tthe\tprevious\tclassifier?\n\n10.\t Use\tt-SNE\tto\treduce\tthe\tMNIST\tdataset\tdown\tto\ttwo\tdimensions\tand\tplot\tthe\tresult\tusing Matplotlib.\tYou\tcan\tuse\ta\tscatterplot\tusing\t10\tdifferent\tcolors\tto\trepresent\teach\timage’s\ttarget\tclass. Alternatively,\tyou\tcan\twrite\tcolored\tdigits\tat\tthe\tlocation\tof\teach\tinstance,\tor\teven\tplot\tscaled-down versions\tof\tthe\tdigit\timages\tthemselves\t(if\tyou\tplot\tall\tdigits,\tthe\tvisualization\twill\tbe\ttoo\tcluttered, so\tyou\tshould\teither\tdraw\ta\trandom\tsample\tor\tplot\tan\tinstance\tonly\tif\tno\tother\tinstance\thas\talready been\tplotted\tat\ta\tclose\tdistance).\tYou\tshould\tget\ta\tnice\tvisualization\twith\twell-separated\tclusters\tof digits.\tTry\tusing\tother\tdimensionality\treduction\talgorithms\tsuch\tas\tPCA,\tLLE,\tor\tMDS\tand\tcompare the\tresulting\tvisualizations.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nWell,\tfour\tdimensions\tif\tyou\tcount\ttime,\tand\ta\tfew\tmore\tif\tyou\tare\ta\tstring\ttheorist.\n\n2\n\nWatch\ta\trotating\ttesseract\tprojected\tinto\t3D\tspace\tat\thttp://goo.gl/OM7ktJ.\tImage\tby\tWikipedia\tuser\tNerdBoy1392\t(Creative\tCommons BY-SA\t3.0).\tReproduced\tfrom\thttps://en.wikipedia.org/wiki/Tesseract.\n\n3\n\nFun\tfact:\tanyone\tyou\tknow\tis\tprobably\tan\textremist\tin\tat\tleast\tone\tdimension\t(e.g.,\thow\tmuch\tsugar\tthey\tput\tin\ttheir\tcoffee),\tif\tyou consider\tenough\tdimensions.\n\n4\n\n“On\tLines\tand\tPlanes\tof\tClosest\tFit\tto\tSystems\tof\tPoints\tin\tSpace,”\tK.\tPearson\t(1901).\n\n5\n\nScikit-Learn\tuses\tthe\talgorithm\tdescribed\tin\t“Incremental\tLearning\tfor\tRobust\tVisual\tTracking,”\tD.\tRoss\tet\tal.\t(2007).\n\n6\n\n7\n\n8\n\n9\n\n“Kernel\tPrincipal\tComponent\tAnalysis,”\tB.\tSchölkopf,\tA.\tSmola,\tK.\tMüller\t(1999).\n\nScikit-Learn\tuses\tthe\talgorithm\tbased\ton\tKernel\tRidge\tRegression\tdescribed\tin\tGokhan\tH.\tBakır,\tJason\tWeston,\tand\tBernhard\tScholkopf, “Learning\tto\tFind\tPre-images”\t(Tubingen,\tGermany:\tMax\tPlanck\tInstitute\tfor\tBiological\tCybernetics,\t2004).\n\n“Nonlinear\tDimensionality\tReduction\tby\tLocally\tLinear\tEmbedding,”\tS.\tRoweis,\tL.\tSaul\t(2000).\n\nThe\tgeodesic\tdistance\tbetween\ttwo\tnodes\tin\ta\tgraph\tis\tthe\tnumber\tof\tnodes\ton\tthe\tshortest\tpath\tbetween\tthese\tnodes.\n\nPart\tII.\tNeural\tNetworks\tand\tDeep\tLearning\n\nChapter\t9.\tUp\tand\tRunning\twith\tTensorFlow\n\nTensorFlow\tis\ta\tpowerful\topen\tsource\tsoftware\tlibrary\tfor\tnumerical\tcomputation,\tparticularly\twell suited\tand\tfine-tuned\tfor\tlarge-scale\tMachine\tLearning.\tIts\tbasic\tprinciple\tis\tsimple:\tyou\tfirst\tdefine\tin Python\ta\tgraph\tof\tcomputations\tto\tperform\t(for\texample,\tthe\tone\tin\tFigure\t9-1),\tand\tthen\tTensorFlow takes\tthat\tgraph\tand\truns\tit\tefficiently\tusing\toptimized\tC++\tcode.\n\nFigure\t9-1.\tA\tsimple\tcomputation\tgraph\n\nMost\timportantly,\tit\tis\tpossible\tto\tbreak\tup\tthe\tgraph\tinto\tseveral\tchunks\tand\trun\tthem\tin\tparallel\tacross multiple\tCPUs\tor\tGPUs\t(as\tshown\tin\tFigure\t9-2).\tTensorFlow\talso\tsupports\tdistributed\tcomputing,\tso you\tcan\ttrain\tcolossal\tneural\tnetworks\ton\thumongous\ttraining\tsets\tin\ta\treasonable\tamount\tof\ttime\tby splitting\tthe\tcomputations\tacross\thundreds\tof\tservers\t(see\tChapter\t12).\tTensorFlow\tcan\ttrain\ta\tnetwork with\tmillions\tof\tparameters\ton\ta\ttraining\tset\tcomposed\tof\tbillions\tof\tinstances\twith\tmillions\tof\tfeatures each.\tThis\tshould\tcome\tas\tno\tsurprise,\tsince\tTensorFlow\twas\tdeveloped\tby\tthe\tGoogle\tBrain\tteam\tand\tit powers\tmany\tof\tGoogle’s\tlarge-scale\tservices,\tsuch\tas\tGoogle\tCloud\tSpeech,\tGoogle\tPhotos,\tand Google\tSearch.\n\nFigure\t9-2.\tParallel\tcomputation\ton\tmultiple\tCPUs/GPUs/servers\n\nWhen\tTensorFlow\twas\topen-sourced\tin\tNovember\t2015,\tthere\twere\talready\tmany\tpopular\topen\tsource libraries\tfor\tDeep\tLearning\t(Table\t9-1\tlists\ta\tfew),\tand\tto\tbe\tfair\tmost\tof\tTensorFlow’s\tfeatures\talready existed\tin\tone\tlibrary\tor\tanother.\tNevertheless,\tTensorFlow’s\tclean\tdesign,\tscalability,\tflexibility,1\tand great\tdocumentation\t(not\tto\tmention\tGoogle’s\tname)\tquickly\tboosted\tit\tto\tthe\ttop\tof\tthe\tlist.\tIn\tshort, TensorFlow\twas\tdesigned\tto\tbe\tflexible,\tscalable,\tand\tproduction-ready,\tand\texisting\tframeworks arguably\thit\tonly\ttwo\tout\tof\tthe\tthree\tof\tthese.\tHere\tare\tsome\tof\tTensorFlow’s\thighlights:\n\nIt\truns\tnot\tonly\ton\tWindows,\tLinux,\tand\tmacOS,\tbut\talso\ton\tmobile\tdevices,\tincluding\tboth\tiOS\tand Android.\n\nIt\tprovides\ta\tvery\tsimple\tPython\tAPI\tcalled\tTF.Learn2\t(tensorflow.contrib.learn),\tcompatible with\tScikit-Learn.\tAs\tyou\twill\tsee,\tyou\tcan\tuse\tit\tto\ttrain\tvarious\ttypes\tof\tneural\tnetworks\tin\tjust\ta few\tlines\tof\tcode.\tIt\twas\tpreviously\tan\tindependent\tproject\tcalled\tScikit\tFlow\t(or\tskflow).\n\nIt\talso\tprovides\tanother\tsimple\tAPI\tcalled\tTF-slim\t(tensorflow.contrib.slim)\tto\tsimplify building,\ttraining,\tand\tevaluating\tneural\tnetworks.\n\nSeveral\tother\thigh-level\tAPIs\thave\tbeen\tbuilt\tindependently\ton\ttop\tof\tTensorFlow,\tsuch\tas\tKeras (now\tavailable\tin\ttensorflow.contrib.keras)\tor\tPretty\tTensor.\n\nIts\tmain\tPython\tAPI\toffers\tmuch\tmore\tflexibility\t(at\tthe\tcost\tof\thigher\tcomplexity)\tto\tcreate\tall\tsorts of\tcomputations,\tincluding\tany\tneural\tnetwork\tarchitecture\tyou\tcan\tthink\tof.\n\nIt\tincludes\thighly\tefficient\tC++\timplementations\tof\tmany\tML\toperations,\tparticularly\tthose\tneeded\tto build\tneural\tnetworks.\tThere\tis\talso\ta\tC++\tAPI\tto\tdefine\tyour\town\thigh-performance\toperations.\n\nIt\tprovides\tseveral\tadvanced\toptimization\tnodes\tto\tsearch\tfor\tthe\tparameters\tthat\tminimize\ta\tcost function.\tThese\tare\tvery\teasy\tto\tuse\tsince\tTensorFlow\tautomatically\ttakes\tcare\tof\tcomputing\tthe gradients\tof\tthe\tfunctions\tyou\tdefine.\tThis\tis\tcalled\tautomatic\tdifferentiating\t(or\tautodiff).\n\nIt\talso\tcomes\twith\ta\tgreat\tvisualization\ttool\tcalled\tTensorBoard\tthat\tallows\tyou\tto\tbrowse\tthrough the\tcomputation\tgraph,\tview\tlearning\tcurves,\tand\tmore.\n\nGoogle\talso\tlaunched\ta\tcloud\tservice\tto\trun\tTensorFlow\tgraphs.\n\nLast\tbut\tnot\tleast,\tit\thas\ta\tdedicated\tteam\tof\tpassionate\tand\thelpful\tdevelopers,\tand\ta\tgrowing community\tcontributing\tto\timproving\tit.\tIt\tis\tone\tof\tthe\tmost\tpopular\topen\tsource\tprojects\ton GitHub,\tand\tmore\tand\tmore\tgreat\tprojects\tare\tbeing\tbuilt\ton\ttop\tof\tit\t(for\texamples,\tcheck\tout\tthe resources\tpage\ton\thttps://www.tensorflow.org/,\tor\thttps://github.com/jtoy/awesome-tensorflow). To\task\ttechnical\tquestions,\tyou\tshould\tuse\thttp://stackoverflow.com/\tand\ttag\tyour\tquestion\twith \"tensorflow\".\tYou\tcan\tfile\tbugs\tand\tfeature\trequests\tthrough\tGitHub.\tFor\tgeneral\tdiscussions,\tjoin the\tGoogle\tgroup.\n\nIn\tthis\tchapter,\twe\twill\tgo\tthrough\tthe\tbasics\tof\tTensorFlow,\tfrom\tinstallation\tto\tcreating,\trunning,\tsaving, and\tvisualizing\tsimple\tcomputational\tgraphs.\tMastering\tthese\tbasics\tis\timportant\tbefore\tyou\tbuild\tyour first\tneural\tnetwork\t(which\twe\twill\tdo\tin\tthe\tnext\tchapter).\n\nTable\t9-1.\tOpen\tsource\tDeep\tLearning\tlibraries\t(not\tan\texhaustive\tlist)\n\nLibrary\n\nAPI\n\nPlatforms\n\nStarted\tby\n\nYear\n\nCaffe\n\nPython,\tC++,\tMatlab Linux,\tmacOS,\tWindows\n\nY.\tJia,\tUC\tBerkeley\t(BVLC)\n\n2013\n\nDeeplearning4j Java,\tScala,\tClojure Linux,\tmacOS,\tWindows,\tAndroid\n\nA.\tGibson,\tJ.Patterson\n\n2014\n\nH2O\n\nPython,\tR\n\nLinux,\tmacOS,\tWindows\n\nH2O.ai\n\n2014\n\nMXNet\n\nPython,\tC++,\tothers Linux,\tmacOS,\tWindows,\tiOS,\tAndroid DMLC\n\n2015\n\nTensorFlow\n\nPython,\tC++\n\nLinux,\tmacOS,\tWindows,\tiOS,\tAndroid Google\n\n2015\n\nTheano\n\nPython\n\nLinux,\tmacOS,\tiOS\n\nUniversity\tof\tMontreal\n\n2010\n\nTorch\n\nC++,\tLua\n\nLinux,\tmacOS,\tiOS,\tAndroid\n\nR.\tCollobert,\tK.\tKavukcuoglu,\tC.\tFarabet 2002",
      "page_number": 259
    },
    {
      "number": 9,
      "title": "Up\tand\tRunning\twith\tTensorFlow",
      "start_page": 287,
      "end_page": 316,
      "detection_method": "regex_chapter_title",
      "content": "Installation Let’s\tget\tstarted!\tAssuming\tyou\tinstalled\tJupyter\tand\tScikit-Learn\tby\tfollowing\tthe\tinstallation instructions\tin\tChapter\t2,\tyou\tcan\tsimply\tuse\tpip\tto\tinstall\tTensorFlow.\tIf\tyou\tcreated\tan\tisolated environment\tusing\tvirtualenv,\tyou\tfirst\tneed\tto\tactivate\tit:\n\n$\tcd\t$ML_PATH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tYour\tML\tworking\tdirectory\t(e.g.,\t$HOME/ml) $\tsource\tenv/bin/activate\n\nNext,\tinstall\tTensorFlow:\n\n$\tpip3\tinstall\t--upgrade\ttensorflow\n\nNOTE\n\nFor\tGPU\tsupport,\tyou\tneed\tto\tinstall\ttensorflow-gpu\tinstead\tof\ttensorflow.\tSee\tChapter\t12\tfor\tmore\tdetails.\n\nTo\ttest\tyour\tinstallation,\ttype\tthe\tfollowing\tcommand.\tIt\tshould\toutput\tthe\tversion\tof\tTensorFlow\tyou installed.\n\n$\tpython3\t-c\t'import\ttensorflow;\tprint(tensorflow.__version__)' 1.0.0\n\nCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession The\tfollowing\tcode\tcreates\tthe\tgraph\trepresented\tin\tFigure\t9-1:\n\nimport\ttensorflow\tas\ttf\n\nx\t=\ttf.Variable(3,\tname=\"x\") y\t=\ttf.Variable(4,\tname=\"y\") f\t=\tx*x*y\t+\ty\t+\t2\n\nThat’s\tall\tthere\tis\tto\tit!\tThe\tmost\timportant\tthing\tto\tunderstand\tis\tthat\tthis\tcode\tdoes\tnot\tactually\tperform any\tcomputation,\teven\tthough\tit\tlooks\tlike\tit\tdoes\t(especially\tthe\tlast\tline).\tIt\tjust\tcreates\ta\tcomputation graph.\tIn\tfact,\teven\tthe\tvariables\tare\tnot\tinitialized\tyet.\tTo\tevaluate\tthis\tgraph,\tyou\tneed\tto\topen\ta TensorFlow\tsession\tand\tuse\tit\tto\tinitialize\tthe\tvariables\tand\tevaluate\tf.\tA\tTensorFlow\tsession\ttakes\tcare of\tplacing\tthe\toperations\tonto\tdevices\tsuch\tas\tCPUs\tand\tGPUs\tand\trunning\tthem,\tand\tit\tholds\tall\tthe variable\tvalues.3\tThe\tfollowing\tcode\tcreates\ta\tsession,\tinitializes\tthe\tvariables,\tand\tevaluates,\tand\tf\tthen closes\tthe\tsession\t(which\tfrees\tup\tresources):\n\n>>>\tsess\t=\ttf.Session() >>>\tsess.run(x.initializer) >>>\tsess.run(y.initializer) >>>\tresult\t=\tsess.run(f) >>>\tprint(result) 42 >>>\tsess.close()\n\nHaving\tto\trepeat\tsess.run()\tall\tthe\ttime\tis\ta\tbit\tcumbersome,\tbut\tfortunately\tthere\tis\ta\tbetter\tway:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tx.initializer.run() \t\t\t\ty.initializer.run() \t\t\t\tresult\t=\tf.eval()\n\nInside\tthe\twith\tblock,\tthe\tsession\tis\tset\tas\tthe\tdefault\tsession.\tCalling\tx.initializer.run()\tis equivalent\tto\tcalling\ttf.get_default_session().run(x.initializer),\tand\tsimilarly\tf.eval()\tis equivalent\tto\tcalling\ttf.get_default_session().run(f).\tThis\tmakes\tthe\tcode\teasier\tto\tread. Moreover,\tthe\tsession\tis\tautomatically\tclosed\tat\tthe\tend\tof\tthe\tblock.\n\nInstead\tof\tmanually\trunning\tthe\tinitializer\tfor\tevery\tsingle\tvariable,\tyou\tcan\tuse\tthe global_variables_initializer()\tfunction.\tNote\tthat\tit\tdoes\tnot\tactually\tperform\tthe\tinitialization immediately,\tbut\trather\tcreates\ta\tnode\tin\tthe\tgraph\tthat\twill\tinitialize\tall\tvariables\twhen\tit\tis\trun:\n\ninit\t=\ttf.global_variables_initializer()\t\t#\tprepare\tan\tinit\tnode\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run()\t\t#\tactually\tinitialize\tall\tthe\tvariables \t\t\t\tresult\t=\tf.eval()\n\nInside\tJupyter\tor\twithin\ta\tPython\tshell\tyou\tmay\tprefer\tto\tcreate\tan\tInteractiveSession.\tThe\tonly difference\tfrom\ta\tregular\tSession\tis\tthat\twhen\tan\tInteractiveSession\tis\tcreated\tit\tautomatically\tsets\n\nitself\tas\tthe\tdefault\tsession,\tso\tyou\tdon’t\tneed\ta\twith\tblock\t(but\tyou\tdo\tneed\tto\tclose\tthe\tsession manually\twhen\tyou\tare\tdone\twith\tit):\n\n>>>\tsess\t=\ttf.InteractiveSession() >>>\tinit.run() >>>\tresult\t=\tf.eval() >>>\tprint(result) 42 >>>\tsess.close()\n\nA\tTensorFlow\tprogram\tis\ttypically\tsplit\tinto\ttwo\tparts:\tthe\tfirst\tpart\tbuilds\ta\tcomputation\tgraph\t(this\tis called\tthe\tconstruction\tphase),\tand\tthe\tsecond\tpart\truns\tit\t(this\tis\tthe\texecution\tphase).\tThe\tconstruction phase\ttypically\tbuilds\ta\tcomputation\tgraph\trepresenting\tthe\tML\tmodel\tand\tthe\tcomputations\trequired\tto train\tit.\tThe\texecution\tphase\tgenerally\truns\ta\tloop\tthat\tevaluates\ta\ttraining\tstep\trepeatedly\t(for\texample, one\tstep\tper\tmini-batch),\tgradually\timproving\tthe\tmodel\tparameters.\tWe\twill\tgo\tthrough\tan\texample shortly.\n\nManaging\tGraphs Any\tnode\tyou\tcreate\tis\tautomatically\tadded\tto\tthe\tdefault\tgraph:\n\n>>>\tx1\t=\ttf.Variable(1) >>>\tx1.graph\tis\ttf.get_default_graph() True\n\nIn\tmost\tcases\tthis\tis\tfine,\tbut\tsometimes\tyou\tmay\twant\tto\tmanage\tmultiple\tindependent\tgraphs.\tYou\tcan\tdo this\tby\tcreating\ta\tnew\tGraph\tand\ttemporarily\tmaking\tit\tthe\tdefault\tgraph\tinside\ta\twith\tblock,\tlike\tso:\n\n>>>\tgraph\t=\ttf.Graph() >>>\twith\tgraph.as_default(): ...\t\t\t\t\tx2\t=\ttf.Variable(2) ... >>>\tx2.graph\tis\tgraph True >>>\tx2.graph\tis\ttf.get_default_graph() False\n\nTIP\n\nIn\tJupyter\t(or\tin\ta\tPython\tshell),\tit\tis\tcommon\tto\trun\tthe\tsame\tcommands\tmore\tthan\tonce\twhile\tyou\tare\texperimenting.\tAs\ta result,\tyou\tmay\tend\tup\twith\ta\tdefault\tgraph\tcontaining\tmany\tduplicate\tnodes.\tOne\tsolution\tis\tto\trestart\tthe\tJupyter\tkernel\t(or\tthe Python\tshell),\tbut\ta\tmore\tconvenient\tsolution\tis\tto\tjust\treset\tthe\tdefault\tgraph\tby\trunning\ttf.reset_default_graph().\n\nLifecycle\tof\ta\tNode\tValue When\tyou\tevaluate\ta\tnode,\tTensorFlow\tautomatically\tdetermines\tthe\tset\tof\tnodes\tthat\tit\tdepends\ton\tand\tit evaluates\tthese\tnodes\tfirst.\tFor\texample,\tconsider\tthe\tfollowing\tcode:\n\nw\t=\ttf.constant(3) x\t=\tw\t+\t2 y\t=\tx\t+\t5 z\t=\tx\t*\t3\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tprint(y.eval())\t\t#\t10 \t\t\t\tprint(z.eval())\t\t#\t15\n\nFirst,\tthis\tcode\tdefines\ta\tvery\tsimple\tgraph.\tThen\tit\tstarts\ta\tsession\tand\truns\tthe\tgraph\tto\tevaluate\ty: TensorFlow\tautomatically\tdetects\tthat\ty\tdepends\ton\tx,\twhich\tdepends\ton\tw,\tso\tit\tfirst\tevaluates\tw,\tthen\tx, then\ty,\tand\treturns\tthe\tvalue\tof\ty.\tFinally,\tthe\tcode\truns\tthe\tgraph\tto\tevaluate\tz.\tOnce\tagain,\tTensorFlow detects\tthat\tit\tmust\tfirst\tevaluate\tw\tand\tx.\tIt\tis\timportant\tto\tnote\tthat\tit\twill\tnot\treuse\tthe\tresult\tof\tthe previous\tevaluation\tof\tw\tand\tx.\tIn\tshort,\tthe\tpreceding\tcode\tevaluates\tw\tand\tx\ttwice.\n\nAll\tnode\tvalues\tare\tdropped\tbetween\tgraph\truns,\texcept\tvariable\tvalues,\twhich\tare\tmaintained\tby\tthe session\tacross\tgraph\truns\t(queues\tand\treaders\talso\tmaintain\tsome\tstate,\tas\twe\twill\tsee\tin\tChapter\t12).\tA variable\tstarts\tits\tlife\twhen\tits\tinitializer\tis\trun,\tand\tit\tends\twhen\tthe\tsession\tis\tclosed.\n\nIf\tyou\twant\tto\tevaluate\ty\tand\tz\tefficiently,\twithout\tevaluating\tw\tand\tx\ttwice\tas\tin\tthe\tprevious\tcode,\tyou must\task\tTensorFlow\tto\tevaluate\tboth\ty\tand\tz\tin\tjust\tone\tgraph\trun,\tas\tshown\tin\tthe\tfollowing\tcode:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\ty_val,\tz_val\t=\tsess.run([y,\tz]) \t\t\t\tprint(y_val)\t\t#\t10 \t\t\t\tprint(z_val)\t\t#\t15\n\nWARNING\n\nIn\tsingle-process\tTensorFlow,\tmultiple\tsessions\tdo\tnot\tshare\tany\tstate,\teven\tif\tthey\treuse\tthe\tsame\tgraph\t(each\tsession\twould have\tits\town\tcopy\tof\tevery\tvariable).\tIn\tdistributed\tTensorFlow\t(see\tChapter\t12),\tvariable\tstate\tis\tstored\ton\tthe\tservers,\tnot\tin the\tsessions,\tso\tmultiple\tsessions\tcan\tshare\tthe\tsame\tvariables.\n\nLinear\tRegression\twith\tTensorFlow TensorFlow\toperations\t(also\tcalled\tops\tfor\tshort)\tcan\ttake\tany\tnumber\tof\tinputs\tand\tproduce\tany\tnumber of\toutputs.\tFor\texample,\tthe\taddition\tand\tmultiplication\tops\teach\ttake\ttwo\tinputs\tand\tproduce\tone\toutput. Constants\tand\tvariables\ttake\tno\tinput\t(they\tare\tcalled\tsource\tops).\tThe\tinputs\tand\toutputs\tare multidimensional\tarrays,\tcalled\ttensors\t(hence\tthe\tname\t“tensor\tflow”).\tJust\tlike\tNumPy\tarrays,\ttensors have\ta\ttype\tand\ta\tshape.\tIn\tfact,\tin\tthe\tPython\tAPI\ttensors\tare\tsimply\trepresented\tby\tNumPy\tndarrays. They\ttypically\tcontain\tfloats,\tbut\tyou\tcan\talso\tuse\tthem\tto\tcarry\tstrings\t(arbitrary\tbyte\tarrays).\n\nIn\tthe\texamples\tso\tfar,\tthe\ttensors\tjust\tcontained\ta\tsingle\tscalar\tvalue,\tbut\tyou\tcan\tof\tcourse\tperform computations\ton\tarrays\tof\tany\tshape.\tFor\texample,\tthe\tfollowing\tcode\tmanipulates\t2D\tarrays\tto\tperform Linear\tRegression\ton\tthe\tCalifornia\thousing\tdataset\t(introduced\tin\tChapter\t2).\tIt\tstarts\tby\tfetching\tthe dataset;\tthen\tit\tadds\tan\textra\tbias\tinput\tfeature\t(x0\t=\t1)\tto\tall\ttraining\tinstances\t(it\tdoes\tso\tusing\tNumPy\tso it\truns\timmediately);\tthen\tit\tcreates\ttwo\tTensorFlow\tconstant\tnodes,\tX\tand\ty,\tto\thold\tthis\tdata\tand\tthe targets,4\tand\tit\tuses\tsome\tof\tthe\tmatrix\toperations\tprovided\tby\tTensorFlow\tto\tdefine\ttheta.\tThese\tmatrix functions\t—\ttranspose(),\tmatmul(),\tand\tmatrix_inverse()\t—\tare\tself-explanatory,\tbut\tas\tusual\tthey do\tnot\tperform\tany\tcomputations\timmediately;\tinstead,\tthey\tcreate\tnodes\tin\tthe\tgraph\tthat\twill\tperform them\twhen\tthe\tgraph\tis\trun.\tYou\tmay\trecognize\tthat\tthe\tdefinition\tof\ttheta\tcorresponds\tto\tthe\tNormal Equation\t( \t=\t(XT\t·\tX)–1\t·\tXT\t·\ty;\tsee\tChapter\t4).\tFinally,\tthe\tcode\tcreates\ta\tsession\tand\tuses\tit\tto evaluate\ttheta.\n\nimport\tnumpy\tas\tnp from\tsklearn.datasets\timport\tfetch_california_housing\n\nhousing\t=\tfetch_california_housing() m,\tn\t=\thousing.data.shape housing_data_plus_bias\t=\tnp.c_[np.ones((m,\t1)),\thousing.data]\n\nX\t=\ttf.constant(housing_data_plus_bias,\tdtype=tf.float32,\tname=\"X\") y\t=\ttf.constant(housing.target.reshape(-1,\t1),\tdtype=tf.float32,\tname=\"y\") XT\t=\ttf.transpose(X) theta\t=\ttf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,\tX)),\tXT),\ty)\n\nwith\ttf.Session()\tas\tsess: \t\t\t\ttheta_value\t=\ttheta.eval()\n\nThe\tmain\tbenefit\tof\tthis\tcode\tversus\tcomputing\tthe\tNormal\tEquation\tdirectly\tusing\tNumPy\tis\tthat TensorFlow\twill\tautomatically\trun\tthis\ton\tyour\tGPU\tcard\tif\tyou\thave\tone\t(provided\tyou\tinstalled TensorFlow\twith\tGPU\tsupport,\tof\tcourse;\tsee\tChapter\t12\tfor\tmore\tdetails).\n\nImplementing\tGradient\tDescent Let’s\ttry\tusing\tBatch\tGradient\tDescent\t(introduced\tin\tChapter\t4)\tinstead\tof\tthe\tNormal\tEquation.\tFirst\twe will\tdo\tthis\tby\tmanually\tcomputing\tthe\tgradients,\tthen\twe\twill\tuse\tTensorFlow’s\tautodiff\tfeature\tto\tlet TensorFlow\tcompute\tthe\tgradients\tautomatically,\tand\tfinally\twe\twill\tuse\ta\tcouple\tof\tTensorFlow’s\tout- of-the-box\toptimizers.\n\nWARNING\n\nWhen\tusing\tGradient\tDescent,\tremember\tthat\tit\tis\timportant\tto\tfirst\tnormalize\tthe\tinput\tfeature\tvectors,\tor\telse\ttraining\tmay\tbe much\tslower.\tYou\tcan\tdo\tthis\tusing\tTensorFlow,\tNumPy,\tScikit-Learn’s\tStandardScaler,\tor\tany\tother\tsolution\tyou\tprefer.\tThe following\tcode\tassumes\tthat\tthis\tnormalization\thas\talready\tbeen\tdone.\n\nManually\tComputing\tthe\tGradients The\tfollowing\tcode\tshould\tbe\tfairly\tself-explanatory,\texcept\tfor\ta\tfew\tnew\telements:\n\nThe\trandom_uniform()\tfunction\tcreates\ta\tnode\tin\tthe\tgraph\tthat\twill\tgenerate\ta\ttensor\tcontaining random\tvalues,\tgiven\tits\tshape\tand\tvalue\trange,\tmuch\tlike\tNumPy’s\trand()\tfunction.\n\nThe\tassign()\tfunction\tcreates\ta\tnode\tthat\twill\tassign\ta\tnew\tvalue\tto\ta\tvariable.\tIn\tthis\tcase,\tit implements\tthe\tBatch\tGradient\tDescent\tstep\tθ(next\tstep)\t=\tθ\t–\tη θMSE(θ).\n\nThe\tmain\tloop\texecutes\tthe\ttraining\tstep\tover\tand\tover\tagain\t(n_epochs\ttimes),\tand\tevery\t100 iterations\tit\tprints\tout\tthe\tcurrent\tMean\tSquared\tError\t(mse).\tYou\tshould\tsee\tthe\tMSE\tgo\tdown\tat every\titeration.\n\nn_epochs\t=\t1000 learning_rate\t=\t0.01\n\nX\t=\ttf.constant(scaled_housing_data_plus_bias,\tdtype=tf.float32,\tname=\"X\") y\t=\ttf.constant(housing.target.reshape(-1,\t1),\tdtype=tf.float32,\tname=\"y\") theta\t=\ttf.Variable(tf.random_uniform([n\t+\t1,\t1],\t-1.0,\t1.0),\tname=\"theta\") y_pred\t=\ttf.matmul(X,\ttheta,\tname=\"predictions\") error\t=\ty_pred\t-\ty mse\t=\ttf.reduce_mean(tf.square(error),\tname=\"mse\") gradients\t=\t2/m\t*\ttf.matmul(tf.transpose(X),\terror) training_op\t=\ttf.assign(theta,\ttheta\t-\tlearning_rate\t*\tgradients)\n\ninit\t=\ttf.global_variables_initializer()\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsess.run(init)\n\nfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tif\tepoch\t%\t100\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tprint(\"Epoch\",\tepoch,\t\"MSE\t=\",\tmse.eval()) \t\t\t\t\t\t\t\tsess.run(training_op)\n\nbest_theta\t=\ttheta.eval()\n\nUsing\tautodiff The\tpreceding\tcode\tworks\tfine,\tbut\tit\trequires\tmathematically\tderiving\tthe\tgradients\tfrom\tthe\tcost function\t(MSE).\tIn\tthe\tcase\tof\tLinear\tRegression,\tit\tis\treasonably\teasy,\tbut\tif\tyou\thad\tto\tdo\tthis\twith\tdeep neural\tnetworks\tyou\twould\tget\tquite\ta\theadache:\tit\twould\tbe\ttedious\tand\terror-prone.\tYou\tcould\tuse symbolic\tdifferentiation\tto\tautomatically\tfind\tthe\tequations\tfor\tthe\tpartial\tderivatives\tfor\tyou,\tbut\tthe resulting\tcode\twould\tnot\tnecessarily\tbe\tvery\tefficient.\n\nTo\tunderstand\twhy,\tconsider\tthe\tfunction\tf(x)=\texp(exp(exp(x))).\tIf\tyou\tknow\tcalculus,\tyou\tcan\tfigure\tout its\tderivative\tf′(x)\t=\texp(x)\t×\texp(exp(x))\t×\texp(exp(exp(x))).\tIf\tyou\tcode\tf(x)\tand\tf′(x)\tseparately\tand exactly\tas\tthey\tappear,\tyour\tcode\twill\tnot\tbe\tas\tefficient\tas\tit\tcould\tbe.\tA\tmore\tefficient\tsolution\twould be\tto\twrite\ta\tfunction\tthat\tfirst\tcomputes\texp(x),\tthen\texp(exp(x)),\tthen\texp(exp(exp(x))),\tand\treturns\tall three.\tThis\tgives\tyou\tf(x)\tdirectly\t(the\tthird\tterm),\tand\tif\tyou\tneed\tthe\tderivative\tyou\tcan\tjust\tmultiply\tall three\tterms\tand\tyou\tare\tdone.\tWith\tthe\tnaïve\tapproach\tyou\twould\thave\thad\tto\tcall\tthe\texp\tfunction\tnine times\tto\tcompute\tboth\tf(x)\tand\tf′(x).\tWith\tthis\tapproach\tyou\tjust\tneed\tto\tcall\tit\tthree\ttimes.\n\nIt\tgets\tworse\twhen\tyour\tfunction\tis\tdefined\tby\tsome\tarbitrary\tcode.\tCan\tyou\tfind\tthe\tequation\t(or\tthe code)\tto\tcompute\tthe\tpartial\tderivatives\tof\tthe\tfollowing\tfunction?\tHint:\tdon’t\teven\ttry.\n\ndef\tmy_func(a,\tb): \t\t\t\tz\t=\t0 \t\t\t\tfor\ti\tin\trange(100): \t\t\t\t\t\t\t\tz\t=\ta\t*\tnp.cos(z\t+\ti)\t+\tz\t*\tnp.sin(b\t-\ti) \t\t\t\treturn\tz\n\nFortunately,\tTensorFlow’s\tautodiff\tfeature\tcomes\tto\tthe\trescue:\tit\tcan\tautomatically\tand\tefficiently compute\tthe\tgradients\tfor\tyou.\tSimply\treplace\tthe\tgradients\t=\t...\tline\tin\tthe\tGradient\tDescent\tcode\tin the\tprevious\tsection\twith\tthe\tfollowing\tline,\tand\tthe\tcode\twill\tcontinue\tto\twork\tjust\tfine:\n\ngradients\t=\ttf.gradients(mse,\t[theta])[0]\n\nThe\tgradients()\tfunction\ttakes\tan\top\t(in\tthis\tcase\tmse)\tand\ta\tlist\tof\tvariables\t(in\tthis\tcase\tjust\ttheta), and\tit\tcreates\ta\tlist\tof\tops\t(one\tper\tvariable)\tto\tcompute\tthe\tgradients\tof\tthe\top\twith\tregards\tto\teach variable.\tSo\tthe\tgradients\tnode\twill\tcompute\tthe\tgradient\tvector\tof\tthe\tMSE\twith\tregards\tto\ttheta.\n\nThere\tare\tfour\tmain\tapproaches\tto\tcomputing\tgradients\tautomatically.\tThey\tare\tsummarized\tin\tTable\t9-2. TensorFlow\tuses\treverse-mode\tautodiff,\twhich\tis\tperfect\t(efficient\tand\taccurate)\twhen\tthere\tare\tmany inputs\tand\tfew\toutputs,\tas\tis\toften\tthe\tcase\tin\tneural\tnetworks.\tIt\tcomputes\tall\tthe\tpartial\tderivatives\tof the\toutputs\twith\tregards\tto\tall\tthe\tinputs\tin\tjust\tnoutputs\t+\t1\tgraph\ttraversals.\n\nTable\t9-2.\tMain\tsolutions\tto\tcompute\tgradients\tautomatically\n\nTechnique\n\nNb\tof\tgraph\ttraversals\tto\tcompute\tall gradients\n\nAccuracy Supports\tarbitrary\n\ncode\n\nComment\n\nNumerical differentiation\n\nninputs\t+\t1\n\nLow\n\nYes\n\nTrivial\tto\timplement\n\nSymbolic\tdifferentiation N/A\n\nHigh\n\nNo\n\nBuilds\ta\tvery\tdifferent\n\nForward-mode\tautodiff ninputs\n\nHigh\n\nReverse-mode\tautodiff noutputs\t+\t1\n\nHigh\n\nIf\tyou\tare\tinterested\tin\thow\tthis\tmagic\tworks,\tcheck\tout\tAppendix\tD.\n\nYes\n\nYes\n\ngraph\n\nUses\tdual\tnumbers\n\nImplemented\tby TensorFlow\n\nUsing\tan\tOptimizer So\tTensorFlow\tcomputes\tthe\tgradients\tfor\tyou.\tBut\tit\tgets\teven\teasier:\tit\talso\tprovides\ta\tnumber\tof optimizers\tout\tof\tthe\tbox,\tincluding\ta\tGradient\tDescent\toptimizer.\tYou\tcan\tsimply\treplace\tthe\tpreceding gradients\t=\t...\tand\ttraining_op\t=\t...\tlines\twith\tthe\tfollowing\tcode,\tand\tonce\tagain\teverything will\tjust\twork\tfine:\n\noptimizer\t=\ttf.train.GradientDescentOptimizer(learning_rate=learning_rate) training_op\t=\toptimizer.minimize(mse)\n\nIf\tyou\twant\tto\tuse\ta\tdifferent\ttype\tof\toptimizer,\tyou\tjust\tneed\tto\tchange\tone\tline.\tFor\texample,\tyou\tcan\tuse a\tmomentum\toptimizer\t(which\toften\tconverges\tmuch\tfaster\tthan\tGradient\tDescent;\tsee\tChapter\t11)\tby defining\tthe\toptimizer\tlike\tthis:\n\noptimizer\t=\ttf.train.MomentumOptimizer(learning_rate=learning_rate, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=0.9)\n\nFeeding\tData\tto\tthe\tTraining\tAlgorithm Let’s\ttry\tto\tmodify\tthe\tprevious\tcode\tto\timplement\tMini-batch\tGradient\tDescent.\tFor\tthis,\twe\tneed\ta\tway to\treplace\tX\tand\ty\tat\tevery\titeration\twith\tthe\tnext\tmini-batch.\tThe\tsimplest\tway\tto\tdo\tthis\tis\tto\tuse placeholder\tnodes.\tThese\tnodes\tare\tspecial\tbecause\tthey\tdon’t\tactually\tperform\tany\tcomputation,\tthey just\toutput\tthe\tdata\tyou\ttell\tthem\tto\toutput\tat\truntime.\tThey\tare\ttypically\tused\tto\tpass\tthe\ttraining\tdata\tto TensorFlow\tduring\ttraining.\tIf\tyou\tdon’t\tspecify\ta\tvalue\tat\truntime\tfor\ta\tplaceholder,\tyou\tget\tan exception.\n\nTo\tcreate\ta\tplaceholder\tnode,\tyou\tmust\tcall\tthe\tplaceholder()\tfunction\tand\tspecify\tthe\toutput\ttensor’s data\ttype.\tOptionally,\tyou\tcan\talso\tspecify\tits\tshape,\tif\tyou\twant\tto\tenforce\tit.\tIf\tyou\tspecify\tNone\tfor\ta dimension,\tit\tmeans\t“any\tsize.”\tFor\texample,\tthe\tfollowing\tcode\tcreates\ta\tplaceholder\tnode\tA,\tand\talso\ta node\tB\t=\tA\t+\t5.\tWhen\twe\tevaluate\tB,\twe\tpass\ta\tfeed_dict\tto\tthe\teval()\tmethod\tthat\tspecifies\tthe value\tof\tA.\tNote\tthat\tA\tmust\thave\trank\t2\t(i.e.,\tit\tmust\tbe\ttwo-dimensional)\tand\tthere\tmust\tbe\tthree\tcolumns (or\telse\tan\texception\tis\traised),\tbut\tit\tcan\thave\tany\tnumber\tof\trows.\n\n>>>\tA\t=\ttf.placeholder(tf.float32,\tshape=(None,\t3)) >>>\tB\t=\tA\t+\t5 >>>\twith\ttf.Session()\tas\tsess: ...\t\t\t\t\tB_val_1\t=\tB.eval(feed_dict={A:\t[[1,\t2,\t3]]}) ...\t\t\t\t\tB_val_2\t=\tB.eval(feed_dict={A:\t[[4,\t5,\t6],\t[7,\t8,\t9]]}) ... >>>\tprint(B_val_1) [[\t6.\t\t7.\t\t8.]] >>>\tprint(B_val_2) [[\t\t9.\t\t10.\t\t11.] \t[\t12.\t\t13.\t\t14.]]\n\nNOTE\n\nYou\tcan\tactually\tfeed\tthe\toutput\tof\tany\toperations,\tnot\tjust\tplaceholders.\tIn\tthis\tcase\tTensorFlow\tdoes\tnot\ttry\tto\tevaluate\tthese operations;\tit\tuses\tthe\tvalues\tyou\tfeed\tit.\n\nTo\timplement\tMini-batch\tGradient\tDescent,\twe\tonly\tneed\tto\ttweak\tthe\texisting\tcode\tslightly.\tFirst\tchange the\tdefinition\tof\tX\tand\ty\tin\tthe\tconstruction\tphase\tto\tmake\tthem\tplaceholder\tnodes:\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn\t+\t1),\tname=\"X\") y\t=\ttf.placeholder(tf.float32,\tshape=(None,\t1),\tname=\"y\")\n\nThen\tdefine\tthe\tbatch\tsize\tand\tcompute\tthe\ttotal\tnumber\tof\tbatches:\n\nbatch_size\t=\t100 n_batches\t=\tint(np.ceil(m\t/\tbatch_size))\n\nFinally,\tin\tthe\texecution\tphase,\tfetch\tthe\tmini-batches\tone\tby\tone,\tthen\tprovide\tthe\tvalue\tof\tX\tand\ty\tvia the\tfeed_dict\tparameter\twhen\tevaluating\ta\tnode\tthat\tdepends\ton\teither\tof\tthem.\n\ndef\tfetch_batch(epoch,\tbatch_index,\tbatch_size):\n\n[...]\t#\tload\tthe\tdata\tfrom\tdisk \t\t\t\treturn\tX_batch,\ty_batch\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsess.run(init)\n\nfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tfor\tbatch_index\tin\trange(n_batches): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tfetch_batch(epoch,\tbatch_index,\tbatch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch})\n\nbest_theta\t=\ttheta.eval()\n\nNOTE\n\nWe\tdon’t\tneed\tto\tpass\tthe\tvalue\tof\tX\tand\ty\twhen\tevaluating\ttheta\tsince\tit\tdoes\tnot\tdepend\ton\teither\tof\tthem.\n\nSaving\tand\tRestoring\tModels Once\tyou\thave\ttrained\tyour\tmodel,\tyou\tshould\tsave\tits\tparameters\tto\tdisk\tso\tyou\tcan\tcome\tback\tto\tit whenever\tyou\twant,\tuse\tit\tin\tanother\tprogram,\tcompare\tit\tto\tother\tmodels,\tand\tso\ton.\tMoreover,\tyou probably\twant\tto\tsave\tcheckpoints\tat\tregular\tintervals\tduring\ttraining\tso\tthat\tif\tyour\tcomputer\tcrashes during\ttraining\tyou\tcan\tcontinue\tfrom\tthe\tlast\tcheckpoint\trather\tthan\tstart\tover\tfrom\tscratch.\n\nTensorFlow\tmakes\tsaving\tand\trestoring\ta\tmodel\tvery\teasy.\tJust\tcreate\ta\tSaver\tnode\tat\tthe\tend\tof\tthe construction\tphase\t(after\tall\tvariable\tnodes\tare\tcreated);\tthen,\tin\tthe\texecution\tphase,\tjust\tcall\tits\tsave() method\twhenever\tyou\twant\tto\tsave\tthe\tmodel,\tpassing\tit\tthe\tsession\tand\tpath\tof\tthe\tcheckpoint\tfile:\n\n[...] theta\t=\ttf.Variable(tf.random_uniform([n\t+\t1,\t1],\t-1.0,\t1.0),\tname=\"theta\") [...] init\t=\ttf.global_variables_initializer() saver\t=\ttf.train.Saver()\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsess.run(init)\n\nfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tif\tepoch\t%\t100\t==\t0:\t\t#\tcheckpoint\tevery\t100\tepochs \t\t\t\t\t\t\t\t\t\t\t\tsave_path\t=\tsaver.save(sess,\t\"/tmp/my_model.ckpt\")\n\nsess.run(training_op)\n\nbest_theta\t=\ttheta.eval() \t\t\t\tsave_path\t=\tsaver.save(sess,\t\"/tmp/my_model_final.ckpt\")\n\nRestoring\ta\tmodel\tis\tjust\tas\teasy:\tyou\tcreate\ta\tSaver\tat\tthe\tend\tof\tthe\tconstruction\tphase\tjust\tlike\tbefore, but\tthen\tat\tthe\tbeginning\tof\tthe\texecution\tphase,\tinstead\tof\tinitializing\tthe\tvariables\tusing\tthe\tinit\tnode, you\tcall\tthe\trestore()\tmethod\tof\tthe\tSaver\tobject:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsaver.restore(sess,\t\"/tmp/my_model_final.ckpt\") \t\t\t\t[...]\n\nBy\tdefault\ta\tSaver\tsaves\tand\trestores\tall\tvariables\tunder\ttheir\town\tname,\tbut\tif\tyou\tneed\tmore\tcontrol, you\tcan\tspecify\twhich\tvariables\tto\tsave\tor\trestore,\tand\twhat\tnames\tto\tuse.\tFor\texample,\tthe\tfollowing Saver\twill\tsave\tor\trestore\tonly\tthe\ttheta\tvariable\tunder\tthe\tname\tweights:\n\nsaver\t=\ttf.train.Saver({\"weights\":\ttheta})\n\nBy\tdefault,\tthe\tsave()\tmethod\talso\tsaves\tthe\tstructure\tof\tthe\tgraph\tin\ta\tsecond\tfile\twith\tthe\tsame\tname plus\ta\t.meta\textension.\tYou\tcan\tload\tthis\tgraph\tstructure\tusing\ttf.train.import_meta_graph().\tThis adds\tthe\tgraph\tto\tthe\tdefault\tgraph,\tand\treturns\ta\tSaver\tinstance\tthat\tyou\tcan\tthen\tuse\tto\trestore\tthe graph’s\tstate\t(i.e.,\tthe\tvariable\tvalues):\n\nsaver\t=\ttf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsaver.restore(sess,\t\"/tmp/my_model_final.ckpt\") \t\t\t\t[...]\n\nThis\tallows\tyou\tto\tfully\trestore\ta\tsaved\tmodel,\tincluding\tboth\tthe\tgraph\tstructure\tand\tthe\tvariable\tvalues, without\thaving\tto\tsearch\tfor\tthe\tcode\tthat\tbuilt\tit.\n\nVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard So\tnow\twe\thave\ta\tcomputation\tgraph\tthat\ttrains\ta\tLinear\tRegression\tmodel\tusing\tMini-batch\tGradient Descent,\tand\twe\tare\tsaving\tcheckpoints\tat\tregular\tintervals.\tSounds\tsophisticated,\tdoesn’t\tit?\tHowever, we\tare\tstill\trelying\ton\tthe\tprint()\tfunction\tto\tvisualize\tprogress\tduring\ttraining.\tThere\tis\ta\tbetter\tway: enter\tTensorBoard.\tIf\tyou\tfeed\tit\tsome\ttraining\tstats,\tit\twill\tdisplay\tnice\tinteractive\tvisualizations\tof these\tstats\tin\tyour\tweb\tbrowser\t(e.g.,\tlearning\tcurves).\tYou\tcan\talso\tprovide\tit\tthe\tgraph’s\tdefinition\tand it\twill\tgive\tyou\ta\tgreat\tinterface\tto\tbrowse\tthrough\tit.\tThis\tis\tvery\tuseful\tto\tidentify\terrors\tin\tthe\tgraph,\tto find\tbottlenecks,\tand\tso\ton.\n\nThe\tfirst\tstep\tis\tto\ttweak\tyour\tprogram\ta\tbit\tso\tit\twrites\tthe\tgraph\tdefinition\tand\tsome\ttraining\tstats\t—\tfor example,\tthe\ttraining\terror\t(MSE)\t—\tto\ta\tlog\tdirectory\tthat\tTensorBoard\twill\tread\tfrom.\tYou\tneed\tto\tuse a\tdifferent\tlog\tdirectory\tevery\ttime\tyou\trun\tyour\tprogram,\tor\telse\tTensorBoard\twill\tmerge\tstats\tfrom different\truns,\twhich\twill\tmess\tup\tthe\tvisualizations.\tThe\tsimplest\tsolution\tfor\tthis\tis\tto\tinclude\ta timestamp\tin\tthe\tlog\tdirectory\tname.\tAdd\tthe\tfollowing\tcode\tat\tthe\tbeginning\tof\tthe\tprogram:\n\nfrom\tdatetime\timport\tdatetime\n\nnow\t=\tdatetime.utcnow().strftime(\"%Y%m%d%H%M%S\") root_logdir\t=\t\"tf_logs\" logdir\t=\t\"{}/run-{}/\".format(root_logdir,\tnow)\n\nNext,\tadd\tthe\tfollowing\tcode\tat\tthe\tvery\tend\tof\tthe\tconstruction\tphase:\n\nmse_summary\t=\ttf.summary.scalar('MSE',\tmse) file_writer\t=\ttf.summary.FileWriter(logdir,\ttf.get_default_graph())\n\nThe\tfirst\tline\tcreates\ta\tnode\tin\tthe\tgraph\tthat\twill\tevaluate\tthe\tMSE\tvalue\tand\twrite\tit\tto\ta\tTensorBoard- compatible\tbinary\tlog\tstring\tcalled\ta\tsummary.\tThe\tsecond\tline\tcreates\ta\tFileWriter\tthat\tyou\twill\tuse to\twrite\tsummaries\tto\tlogfiles\tin\tthe\tlog\tdirectory.\tThe\tfirst\tparameter\tindicates\tthe\tpath\tof\tthe\tlog directory\t(in\tthis\tcase\tsomething\tlike\ttf_logs/run-20160906091959/,\trelative\tto\tthe\tcurrent\tdirectory). The\tsecond\t(optional)\tparameter\tis\tthe\tgraph\tyou\twant\tto\tvisualize.\tUpon\tcreation,\tthe\tFileWriter creates\tthe\tlog\tdirectory\tif\tit\tdoes\tnot\talready\texist\t(and\tits\tparent\tdirectories\tif\tneeded),\tand\twrites\tthe graph\tdefinition\tin\ta\tbinary\tlogfile\tcalled\tan\tevents\tfile.\n\nNext\tyou\tneed\tto\tupdate\tthe\texecution\tphase\tto\tevaluate\tthe\tmse_summary\tnode\tregularly\tduring\ttraining (e.g.,\tevery\t10\tmini-batches).\tThis\twill\toutput\ta\tsummary\tthat\tyou\tcan\tthen\twrite\tto\tthe\tevents\tfile\tusing the\tfile_writer.\tHere\tis\tthe\tupdated\tcode:\n\n[...] \t\t\t\tfor\tbatch_index\tin\trange(n_batches): \t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tfetch_batch(epoch,\tbatch_index,\tbatch_size) \t\t\t\t\t\t\t\tif\tbatch_index\t%\t10\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tsummary_str\t=\tmse_summary.eval(feed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\t\t\t\t\tstep\t=\tepoch\t*\tn_batches\t+\tbatch_index \t\t\t\t\t\t\t\t\t\t\t\tfile_writer.add_summary(summary_str,\tstep) \t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t[...]\n\nWARNING\n\nAvoid\tlogging\ttraining\tstats\tat\tevery\tsingle\ttraining\tstep,\tas\tthis\twould\tsignificantly\tslow\tdown\ttraining.\n\nFinally,\tyou\twant\tto\tclose\tthe\tFileWriter\tat\tthe\tend\tof\tthe\tprogram:\n\nfile_writer.close()\n\nNow\trun\tthis\tprogram:\tit\twill\tcreate\tthe\tlog\tdirectory\tand\twrite\tan\tevents\tfile\tin\tthis\tdirectory,\tcontaining both\tthe\tgraph\tdefinition\tand\tthe\tMSE\tvalues.\tOpen\tup\ta\tshell\tand\tgo\tto\tyour\tworking\tdirectory,\tthen\ttype ls\t-l\ttf_logs/run*\tto\tlist\tthe\tcontents\tof\tthe\tlog\tdirectory:\n\n$\tcd\t$ML_PATH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tYour\tML\tworking\tdirectory\t(e.g.,\t$HOME/ml) $\tls\t-l\ttf_logs/run* total\t40 -rw-r--r--\t1\tageron\tstaff\t18620\tSep\t6\t11:10\tevents.out.tfevents.1472553182.mymac\n\nIf\tyou\trun\tthe\tprogram\ta\tsecond\ttime,\tyou\tshould\tsee\ta\tsecond\tdirectory\tin\tthe\ttf_logs/\tdirectory:\n\n$\tls\t-l\ttf_logs/ total\t0 drwxr-xr-x\t\t3\tageron\t\tstaff\t\t102\tSep\t\t6\t10:07\trun-20160906091959 drwxr-xr-x\t\t3\tageron\t\tstaff\t\t102\tSep\t\t6\t10:22\trun-20160906092202\n\nGreat!\tNow\tit’s\ttime\tto\tfire\tup\tthe\tTensorBoard\tserver.\tYou\tneed\tto\tactivate\tyour\tvirtualenv\tenvironment if\tyou\tcreated\tone,\tthen\tstart\tthe\tserver\tby\trunning\tthe\ttensorboard\tcommand,\tpointing\tit\tto\tthe\troot\tlog directory.\tThis\tstarts\tthe\tTensorBoard\tweb\tserver,\tlistening\ton\tport\t6006\t(which\tis\t“goog”\twritten\tupside down):\n\n$\tsource\tenv/bin/activate $\ttensorboard\t--logdir\ttf_logs/ Starting\tTensorBoard\t\ton\tport\t6006 (You\tcan\tnavigate\tto\thttp://0.0.0.0:6006)\n\nNext\topen\ta\tbrowser\tand\tgo\tto\thttp://0.0.0.0:6006/\t(or\thttp://localhost:6006/).\tWelcome\tto TensorBoard!\tIn\tthe\tEvents\ttab\tyou\tshould\tsee\tMSE\ton\tthe\tright.\tIf\tyou\tclick\ton\tit,\tyou\twill\tsee\ta\tplot\tof the\tMSE\tduring\ttraining,\tfor\tboth\truns\t(Figure\t9-3).\tYou\tcan\tcheck\tor\tuncheck\tthe\truns\tyou\twant\tto\tsee, zoom\tin\tor\tout,\thover\tover\tthe\tcurve\tto\tget\tdetails,\tand\tso\ton.\n\nFigure\t9-3.\tVisualizing\ttraining\tstats\tusing\tTensorBoard\n\nNow\tclick\ton\tthe\tGraphs\ttab.\tYou\tshould\tsee\tthe\tgraph\tshown\tin\tFigure\t9-4.\n\nTo\treduce\tclutter,\tthe\tnodes\tthat\thave\tmany\tedges\t(i.e.,\tconnections\tto\tother\tnodes)\tare\tseparated\tout\tto\tan auxiliary\tarea\ton\tthe\tright\t(you\tcan\tmove\ta\tnode\tback\tand\tforth\tbetween\tthe\tmain\tgraph\tand\tthe\tauxiliary area\tby\tright-clicking\ton\tit).\tSome\tparts\tof\tthe\tgraph\tare\talso\tcollapsed\tby\tdefault.\tFor\texample,\ttry hovering\tover\tthe\tgradients\tnode,\tthen\tclick\ton\tthe\t subgraph,\ttry\texpanding\tthe\tmse_grad\tsubgraph.\n\nicon\tto\texpand\tthis\tsubgraph.\tNext,\tin\tthis\n\nFigure\t9-4.\tVisualizing\tthe\tgraph\tusing\tTensorBoard\n\nTIP\n\nIf\tyou\twant\tto\ttake\ta\tpeek\tat\tthe\tgraph\tdirectly\twithin\tJupyter,\tyou\tcan\tuse\tthe\tshow_graph()\tfunction\tavailable\tin\tthe\tnotebook for\tthis\tchapter.\tIt\twas\toriginally\twritten\tby\tA.\tMordvintsev\tin\this\tgreat\tdeepdream\ttutorial\tnotebook.\tAnother\toption\tis\tto\tinstall E.\tJang’s\tTensorFlow\tdebugger\ttool\twhich\tincludes\ta\tJupyter\textension\tfor\tgraph\tvisualization\t(and\tmore).\n\nName\tScopes When\tdealing\twith\tmore\tcomplex\tmodels\tsuch\tas\tneural\tnetworks,\tthe\tgraph\tcan\teasily\tbecome\tcluttered with\tthousands\tof\tnodes.\tTo\tavoid\tthis,\tyou\tcan\tcreate\tname\tscopes\tto\tgroup\trelated\tnodes.\tFor\texample, let’s\tmodify\tthe\tprevious\tcode\tto\tdefine\tthe\terror\tand\tmse\tops\twithin\ta\tname\tscope\tcalled\t\"loss\":\n\nwith\ttf.name_scope(\"loss\")\tas\tscope: \t\t\t\terror\t=\ty_pred\t-\ty \t\t\t\tmse\t=\ttf.reduce_mean(tf.square(error),\tname=\"mse\")\n\nThe\tname\tof\teach\top\tdefined\twithin\tthe\tscope\tis\tnow\tprefixed\twith\t\"loss/\":\n\n>>>\tprint(error.op.name) loss/sub >>>\tprint(mse.op.name) loss/mse\n\nIn\tTensorBoard,\tthe\tmse\tand\terror\tnodes\tnow\tappear\tinside\tthe\tloss\tnamespace,\twhich\tappears collapsed\tby\tdefault\t(Figure\t9-5).\n\nFigure\t9-5.\tA\tcollapsed\tnamescope\tin\tTensorBoard\n\nModularity Suppose\tyou\twant\tto\tcreate\ta\tgraph\tthat\tadds\tthe\toutput\tof\ttwo\trectified\tlinear\tunits\t(ReLU).\tA\tReLU computes\ta\tlinear\tfunction\tof\tthe\tinputs,\tand\toutputs\tthe\tresult\tif\tit\tis\tpositive,\tand\t0\totherwise,\tas\tshown in\tEquation\t9-1.\n\nEquation\t9-1.\tRectified\tlinear\tunit\n\nThe\tfollowing\tcode\tdoes\tthe\tjob,\tbut\tit’s\tquite\trepetitive:\n\nn_features\t=\t3 X\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features),\tname=\"X\")\n\nw1\t=\ttf.Variable(tf.random_normal((n_features,\t1)),\tname=\"weights1\") w2\t=\ttf.Variable(tf.random_normal((n_features,\t1)),\tname=\"weights2\") b1\t=\ttf.Variable(0.0,\tname=\"bias1\") b2\t=\ttf.Variable(0.0,\tname=\"bias2\")\n\nz1\t=\ttf.add(tf.matmul(X,\tw1),\tb1,\tname=\"z1\") z2\t=\ttf.add(tf.matmul(X,\tw2),\tb2,\tname=\"z2\")\n\nrelu1\t=\ttf.maximum(z1,\t0.,\tname=\"relu1\") relu2\t=\ttf.maximum(z1,\t0.,\tname=\"relu2\")\n\noutput\t=\ttf.add(relu1,\trelu2,\tname=\"output\")\n\nSuch\trepetitive\tcode\tis\thard\tto\tmaintain\tand\terror-prone\t(in\tfact,\tthis\tcode\tcontains\ta\tcut-and-paste\terror; did\tyou\tspot\tit?).\tIt\twould\tbecome\teven\tworse\tif\tyou\twanted\tto\tadd\ta\tfew\tmore\tReLUs.\tFortunately, TensorFlow\tlets\tyou\tstay\tDRY\t(Don’t\tRepeat\tYourself):\tsimply\tcreate\ta\tfunction\tto\tbuild\ta\tReLU.\tThe following\tcode\tcreates\tfive\tReLUs\tand\toutputs\ttheir\tsum\t(note\tthat\tadd_n()\tcreates\tan\toperation\tthat\twill compute\tthe\tsum\tof\ta\tlist\tof\ttensors):\n\ndef\trelu(X): \t\t\t\tw_shape\t=\t(int(X.get_shape()[1]),\t1) \t\t\t\tw\t=\ttf.Variable(tf.random_normal(w_shape),\tname=\"weights\") \t\t\t\tb\t=\ttf.Variable(0.0,\tname=\"bias\") \t\t\t\tz\t=\ttf.add(tf.matmul(X,\tw),\tb,\tname=\"z\") \t\t\t\treturn\ttf.maximum(z,\t0.,\tname=\"relu\")\n\nn_features\t=\t3 X\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features),\tname=\"X\") relus\t=\t[relu(X)\tfor\ti\tin\trange(5)] output\t=\ttf.add_n(relus,\tname=\"output\")\n\nNote\tthat\twhen\tyou\tcreate\ta\tnode,\tTensorFlow\tchecks\twhether\tits\tname\talready\texists,\tand\tif\tit\tdoes\tit appends\tan\tunderscore\tfollowed\tby\tan\tindex\tto\tmake\tthe\tname\tunique.\tSo\tthe\tfirst\tReLU\tcontains\tnodes named\t\"weights\",\t\"bias\",\t\"z\",\tand\t\"relu\"\t(plus\tmany\tmore\tnodes\twith\ttheir\tdefault\tname,\tsuch\tas \"MatMul\");\tthe\tsecond\tReLU\tcontains\tnodes\tnamed\t\"weights_1\",\t\"bias_1\",\tand\tso\ton;\tthe\tthird\tReLU contains\tnodes\tnamed\t\"weights_2\",\t\"bias_2\",\tand\tso\ton.\tTensorBoard\tidentifies\tsuch\tseries\tand collapses\tthem\ttogether\tto\treduce\tclutter\t(as\tyou\tcan\tsee\tin\tFigure\t9-6).\n\nFigure\t9-6.\tCollapsed\tnode\tseries\n\nUsing\tname\tscopes,\tyou\tcan\tmake\tthe\tgraph\tmuch\tclearer.\tSimply\tmove\tall\tthe\tcontent\tof\tthe\trelu() function\tinside\ta\tname\tscope.\tFigure\t9-7\tshows\tthe\tresulting\tgraph.\tNotice\tthat\tTensorFlow\talso\tgives\tthe name\tscopes\tunique\tnames\tby\tappending\t_1,\t_2,\tand\tso\ton.\n\ndef\trelu(X): \t\t\t\twith\ttf.name_scope(\"relu\"): \t\t\t\t\t\t\t\t[...]\n\nFigure\t9-7.\tA\tclearer\tgraph\tusing\tname-scoped\tunits\n\nSharing\tVariables If\tyou\twant\tto\tshare\ta\tvariable\tbetween\tvarious\tcomponents\tof\tyour\tgraph,\tone\tsimple\toption\tis\tto\tcreate it\tfirst,\tthen\tpass\tit\tas\ta\tparameter\tto\tthe\tfunctions\tthat\tneed\tit.\tFor\texample,\tsuppose\tyou\twant\tto\tcontrol the\tReLU\tthreshold\t(currently\thardcoded\tto\t0)\tusing\ta\tshared\tthreshold\tvariable\tfor\tall\tReLUs.\tYou could\tjust\tcreate\tthat\tvariable\tfirst,\tand\tthen\tpass\tit\tto\tthe\trelu()\tfunction:\n\ndef\trelu(X,\tthreshold): \t\t\t\twith\ttf.name_scope(\"relu\"): \t\t\t\t\t\t\t\t[...] \t\t\t\t\t\t\t\treturn\ttf.maximum(z,\tthreshold,\tname=\"max\")\n\nthreshold\t=\ttf.Variable(0.0,\tname=\"threshold\") X\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features),\tname=\"X\") relus\t=\t[relu(X,\tthreshold)\tfor\ti\tin\trange(5)] output\t=\ttf.add_n(relus,\tname=\"output\")\n\nThis\tworks\tfine:\tnow\tyou\tcan\tcontrol\tthe\tthreshold\tfor\tall\tReLUs\tusing\tthe\tthreshold\tvariable. However,\tif\tthere\tare\tmany\tshared\tparameters\tsuch\tas\tthis\tone,\tit\twill\tbe\tpainful\tto\thave\tto\tpass\tthem around\tas\tparameters\tall\tthe\ttime.\tMany\tpeople\tcreate\ta\tPython\tdictionary\tcontaining\tall\tthe\tvariables\tin their\tmodel,\tand\tpass\tit\taround\tto\tevery\tfunction.\tOthers\tcreate\ta\tclass\tfor\teach\tmodule\t(e.g.,\ta\tReLU\tclass using\tclass\tvariables\tto\thandle\tthe\tshared\tparameter).\tYet\tanother\toption\tis\tto\tset\tthe\tshared\tvariable\tas an\tattribute\tof\tthe\trelu()\tfunction\tupon\tthe\tfirst\tcall,\tlike\tso:\n\ndef\trelu(X): \t\t\t\twith\ttf.name_scope(\"relu\"): \t\t\t\t\t\t\t\tif\tnot\thasattr(relu,\t\"threshold\"): \t\t\t\t\t\t\t\t\t\t\t\trelu.threshold\t=\ttf.Variable(0.0,\tname=\"threshold\") \t\t\t\t\t\t\t\t[...] \t\t\t\t\t\t\t\treturn\ttf.maximum(z,\trelu.threshold,\tname=\"max\")\n\nTensorFlow\toffers\tanother\toption,\twhich\tmay\tlead\tto\tslightly\tcleaner\tand\tmore\tmodular\tcode\tthan\tthe previous\tsolutions.5\tThis\tsolution\tis\ta\tbit\ttricky\tto\tunderstand\tat\tfirst,\tbut\tsince\tit\tis\tused\ta\tlot\tin TensorFlow\tit\tis\tworth\tgoing\tinto\ta\tbit\tof\tdetail.\tThe\tidea\tis\tto\tuse\tthe\tget_variable()\tfunction\tto create\tthe\tshared\tvariable\tif\tit\tdoes\tnot\texist\tyet,\tor\treuse\tit\tif\tit\talready\texists.\tThe\tdesired\tbehavior (creating\tor\treusing)\tis\tcontrolled\tby\tan\tattribute\tof\tthe\tcurrent\tvariable_scope().\tFor\texample,\tthe following\tcode\twill\tcreate\ta\tvariable\tnamed\t\"relu/threshold\"\t(as\ta\tscalar,\tsince\tshape=(),\tand\tusing 0.0\tas\tthe\tinitial\tvalue):\n\nwith\ttf.variable_scope(\"relu\"): \t\t\t\tthreshold\t=\ttf.get_variable(\"threshold\",\tshape=(), \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tinitializer=tf.constant_initializer(0.0))\n\nNote\tthat\tif\tthe\tvariable\thas\talready\tbeen\tcreated\tby\tan\tearlier\tcall\tto\tget_variable(),\tthis\tcode\twill raise\tan\texception.\tThis\tbehavior\tprevents\treusing\tvariables\tby\tmistake.\tIf\tyou\twant\tto\treuse\ta\tvariable, you\tneed\tto\texplicitly\tsay\tso\tby\tsetting\tthe\tvariable\tscope’s\treuse\tattribute\tto\tTrue\t(in\twhich\tcase\tyou don’t\thave\tto\tspecify\tthe\tshape\tor\tthe\tinitializer):\n\nwith\ttf.variable_scope(\"relu\",\treuse=True):\n\nthreshold\t=\ttf.get_variable(\"threshold\")\n\nThis\tcode\twill\tfetch\tthe\texisting\t\"relu/threshold\"\tvariable,\tor\traise\tan\texception\tif\tit\tdoes\tnot\texist\tor if\tit\twas\tnot\tcreated\tusing\tget_variable().\tAlternatively,\tyou\tcan\tset\tthe\treuse\tattribute\tto\tTrue\tinside the\tblock\tby\tcalling\tthe\tscope’s\treuse_variables()\tmethod:\n\nwith\ttf.variable_scope(\"relu\")\tas\tscope: \t\t\t\tscope.reuse_variables() \t\t\t\tthreshold\t=\ttf.get_variable(\"threshold\")\n\nWARNING\n\nOnce\treuse\tis\tset\tto\tTrue,\tit\tcannot\tbe\tset\tback\tto\tFalse\twithin\tthe\tblock.\tMoreover,\tif\tyou\tdefine\tother\tvariable\tscopes\tinside this\tone,\tthey\twill\tautomatically\tinherit\treuse=True.\tLastly,\tonly\tvariables\tcreated\tby\tget_variable()\tcan\tbe\treused\tthis\tway.\n\nNow\tyou\thave\tall\tthe\tpieces\tyou\tneed\tto\tmake\tthe\trelu()\tfunction\taccess\tthe\tthreshold\tvariable without\thaving\tto\tpass\tit\tas\ta\tparameter:\n\ndef\trelu(X): \t\t\t\twith\ttf.variable_scope(\"relu\",\treuse=True): \t\t\t\t\t\t\t\tthreshold\t=\ttf.get_variable(\"threshold\")\t\t#\treuse\texisting\tvariable \t\t\t\t\t\t\t\t[...] \t\t\t\t\t\t\t\treturn\ttf.maximum(z,\tthreshold,\tname=\"max\")\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features),\tname=\"X\") with\ttf.variable_scope(\"relu\"):\t\t#\tcreate\tthe\tvariable \t\t\t\tthreshold\t=\ttf.get_variable(\"threshold\",\tshape=(), \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tinitializer=tf.constant_initializer(0.0)) relus\t=\t[relu(X)\tfor\trelu_index\tin\trange(5)] output\t=\ttf.add_n(relus,\tname=\"output\")\n\nThis\tcode\tfirst\tdefines\tthe\trelu()\tfunction,\tthen\tcreates\tthe\trelu/threshold\tvariable\t(as\ta\tscalar\tthat will\tlater\tbe\tinitialized\tto\t0.0)\tand\tbuilds\tfive\tReLUs\tby\tcalling\tthe\trelu()\tfunction.\tThe\trelu() function\treuses\tthe\trelu/threshold\tvariable,\tand\tcreates\tthe\tother\tReLU\tnodes.\n\nNOTE\n\nVariables\tcreated\tusing\tget_variable()\tare\talways\tnamed\tusing\tthe\tname\tof\ttheir\tvariable_scope\tas\ta\tprefix\t(e.g., \"relu/threshold\"),\tbut\tfor\tall\tother\tnodes\t(including\tvariables\tcreated\twith\ttf.Variable())\tthe\tvariable\tscope\tacts\tlike\ta\tnew name\tscope.\tIn\tparticular,\tif\ta\tname\tscope\twith\tan\tidentical\tname\twas\talready\tcreated,\tthen\ta\tsuffix\tis\tadded\tto\tmake\tthe\tname unique.\tFor\texample,\tall\tnodes\tcreated\tin\tthe\tpreceding\tcode\t(except\tthe\tthreshold\tvariable)\thave\ta\tname\tprefixed\twith \"relu_1/\"\tto\t\"relu_5/\",\tas\tshown\tin\tFigure\t9-8.\n\nFigure\t9-8.\tFive\tReLUs\tsharing\tthe\tthreshold\tvariable\n\nIt\tis\tsomewhat\tunfortunate\tthat\tthe\tthreshold\tvariable\tmust\tbe\tdefined\toutside\tthe\trelu()\tfunction, where\tall\tthe\trest\tof\tthe\tReLU\tcode\tresides.\tTo\tfix\tthis,\tthe\tfollowing\tcode\tcreates\tthe\tthreshold variable\twithin\tthe\trelu()\tfunction\tupon\tthe\tfirst\tcall,\tthen\treuses\tit\tin\tsubsequent\tcalls.\tNow\tthe\trelu() function\tdoes\tnot\thave\tto\tworry\tabout\tname\tscopes\tor\tvariable\tsharing:\tit\tjust\tcalls\tget_variable(), which\twill\tcreate\tor\treuse\tthe\tthreshold\tvariable\t(it\tdoes\tnot\tneed\tto\tknow\twhich\tis\tthe\tcase).\tThe\trest of\tthe\tcode\tcalls\trelu()\tfive\ttimes,\tmaking\tsure\tto\tset\treuse=False\ton\tthe\tfirst\tcall,\tand\treuse=True for\tthe\tother\tcalls.\n\ndef\trelu(X): \t\t\t\tthreshold\t=\ttf.get_variable(\"threshold\",\tshape=(), \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tinitializer=tf.constant_initializer(0.0)) \t\t\t\t[...] \t\t\t\treturn\ttf.maximum(z,\tthreshold,\tname=\"max\")\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features),\tname=\"X\") relus\t=\t[] for\trelu_index\tin\trange(5): \t\t\t\twith\ttf.variable_scope(\"relu\",\treuse=(relu_index\t>=\t1))\tas\tscope: \t\t\t\t\t\t\t\trelus.append(relu(X)) output\t=\ttf.add_n(relus,\tname=\"output\")\n\nThe\tresulting\tgraph\tis\tslightly\tdifferent\tthan\tbefore,\tsince\tthe\tshared\tvariable\tlives\twithin\tthe\tfirst\tReLU (see\tFigure\t9-9).\n\nFigure\t9-9.\tFive\tReLUs\tsharing\tthe\tthreshold\tvariable\n\nThis\tconcludes\tthis\tintroduction\tto\tTensorFlow.\tWe\twill\tdiscuss\tmore\tadvanced\ttopics\tas\twe\tgo\tthrough the\tfollowing\tchapters,\tin\tparticular\tmany\toperations\trelated\tto\tdeep\tneural\tnetworks,\tconvolutional neural\tnetworks,\tand\trecurrent\tneural\tnetworks\tas\twell\tas\thow\tto\tscale\tup\twith\tTensorFlow\tusing multithreading,\tqueues,\tmultiple\tGPUs,\tand\tmultiple\tservers.\n\nExercises\n\n1.\t What\tare\tthe\tmain\tbenefits\tof\tcreating\ta\tcomputation\tgraph\trather\tthan\tdirectly\texecuting\tthe computations?\tWhat\tare\tthe\tmain\tdrawbacks?\n\n2.\t Is\tthe\tstatement\ta_val\t=\ta.eval(session=sess)\tequivalent\tto\ta_val\t=\tsess.run(a)?\n\n3.\t Is\tthe\tstatement\ta_val,\tb_val\t=\ta.eval(session=sess),\tb.eval(session=sess)\tequivalent\tto a_val,\tb_val\t=\tsess.run([a,\tb])?\n\n4.\t Can\tyou\trun\ttwo\tgraphs\tin\tthe\tsame\tsession?\n\n5.\t If\tyou\tcreate\ta\tgraph\tg\tcontaining\ta\tvariable\tw,\tthen\tstart\ttwo\tthreads\tand\topen\ta\tsession\tin\teach thread,\tboth\tusing\tthe\tsame\tgraph\tg,\twill\teach\tsession\thave\tits\town\tcopy\tof\tthe\tvariable\tw\tor\twill\tit be\tshared?\n\n6.\t When\tis\ta\tvariable\tinitialized?\tWhen\tis\tit\tdestroyed?\n\n7.\t What\tis\tthe\tdifference\tbetween\ta\tplaceholder\tand\ta\tvariable?\n\n8.\t What\thappens\twhen\tyou\trun\tthe\tgraph\tto\tevaluate\tan\toperation\tthat\tdepends\ton\ta\tplaceholder\tbut\tyou don’t\tfeed\tits\tvalue?\tWhat\thappens\tif\tthe\toperation\tdoes\tnot\tdepend\ton\tthe\tplaceholder?\n\n9.\t When\tyou\trun\ta\tgraph,\tcan\tyou\tfeed\tthe\toutput\tvalue\tof\tany\toperation,\tor\tjust\tthe\tvalue\tof placeholders?\n\n10.\t How\tcan\tyou\tset\ta\tvariable\tto\tany\tvalue\tyou\twant\t(during\tthe\texecution\tphase)?\n\n11.\t How\tmany\ttimes\tdoes\treverse-mode\tautodiff\tneed\tto\ttraverse\tthe\tgraph\tin\torder\tto\tcompute\tthe gradients\tof\tthe\tcost\tfunction\twith\tregards\tto\t10\tvariables?\tWhat\tabout\tforward-mode\tautodiff?\tAnd symbolic\tdifferentiation?\n\n12.\t Implement\tLogistic\tRegression\twith\tMini-batch\tGradient\tDescent\tusing\tTensorFlow.\tTrain\tit\tand evaluate\tit\ton\tthe\tmoons\tdataset\t(introduced\tin\tChapter\t5).\tTry\tadding\tall\tthe\tbells\tand\twhistles: Define\tthe\tgraph\twithin\ta\tlogistic_regression()\tfunction\tthat\tcan\tbe\treused\teasily.\n\nSave\tcheckpoints\tusing\ta\tSaver\tat\tregular\tintervals\tduring\ttraining,\tand\tsave\tthe\tfinal\tmodel\tat the\tend\tof\ttraining.\n\nRestore\tthe\tlast\tcheckpoint\tupon\tstartup\tif\ttraining\twas\tinterrupted.\n\nDefine\tthe\tgraph\tusing\tnice\tscopes\tso\tthe\tgraph\tlooks\tgood\tin\tTensorBoard.\n\nAdd\tsummaries\tto\tvisualize\tthe\tlearning\tcurves\tin\tTensorBoard.\n\nTry\ttweaking\tsome\thyperparameters\tsuch\tas\tthe\tlearning\trate\tor\tthe\tmini-batch\tsize\tand\tlook\tat the\tshape\tof\tthe\tlearning\tcurve.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nTensorFlow\tis\tnot\tlimited\tto\tneural\tnetworks\tor\teven\tMachine\tLearning;\tyou\tcould\trun\tquantum\tphysics\tsimulations\tif\tyou\twanted.\n\n2\n\nNot\tto\tbe\tconfused\twith\tthe\tTFLearn\tlibrary,\twhich\tis\tan\tindependent\tproject.\n\n3\n\nIn\tdistributed\tTensorFlow,\tvariable\tvalues\tare\tstored\ton\tthe\tservers\tinstead\tof\tthe\tsession,\tas\twe\twill\tsee\tin\tChapter\t12.\n\n4\n\nNote\tthat\thousing.target\tis\ta\t1D\tarray,\tbut\twe\tneed\tto\treshape\tit\tto\ta\tcolumn\tvector\tto\tcompute\ttheta.\tRecall\tthat\tNumPy’s\treshape() function\taccepts\t–1\t(meaning\t“unspecified”)\tfor\tone\tof\tthe\tdimensions:\tthat\tdimension\twill\tbe\tcomputed\tbased\ton\tthe\tarray’s\tlength\tand the\tremaining\tdimensions.\n\n5\n\nCreating\ta\tReLU\tclass\tis\targuably\tthe\tcleanest\toption,\tbut\tit\tis\trather\theavyweight.\n\nChapter\t10.\tIntroduction\tto\tArtificial\tNeural Networks\n\nBirds\tinspired\tus\tto\tfly,\tburdock\tplants\tinspired\tvelcro,\tand\tnature\thas\tinspired\tmany\tother\tinventions.\tIt seems\tonly\tlogical,\tthen,\tto\tlook\tat\tthe\tbrain’s\tarchitecture\tfor\tinspiration\ton\thow\tto\tbuild\tan\tintelligent machine.\tThis\tis\tthe\tkey\tidea\tthat\tinspired\tartificial\tneural\tnetworks\t(ANNs).\tHowever,\talthough\tplanes were\tinspired\tby\tbirds,\tthey\tdon’t\thave\tto\tflap\ttheir\twings.\tSimilarly,\tANNs\thave\tgradually\tbecome\tquite different\tfrom\ttheir\tbiological\tcousins.\tSome\tresearchers\teven\targue\tthat\twe\tshould\tdrop\tthe\tbiological analogy\taltogether\t(e.g.,\tby\tsaying\t“units”\trather\tthan\t“neurons”),\tlest\twe\trestrict\tour\tcreativity\tto biologically\tplausible\tsystems.1\n\nANNs\tare\tat\tthe\tvery\tcore\tof\tDeep\tLearning.\tThey\tare\tversatile,\tpowerful,\tand\tscalable,\tmaking\tthem ideal\tto\ttackle\tlarge\tand\thighly\tcomplex\tMachine\tLearning\ttasks,\tsuch\tas\tclassifying\tbillions\tof\timages (e.g.,\tGoogle\tImages),\tpowering\tspeech\trecognition\tservices\t(e.g.,\tApple’s\tSiri),\trecommending\tthe\tbest videos\tto\twatch\tto\thundreds\tof\tmillions\tof\tusers\tevery\tday\t(e.g.,\tYouTube),\tor\tlearning\tto\tbeat\tthe\tworld champion\tat\tthe\tgame\tof\tGo\tby\texamining\tmillions\tof\tpast\tgames\tand\tthen\tplaying\tagainst\titself (DeepMind’s\tAlphaGo).\n\nIn\tthis\tchapter,\twe\twill\tintroduce\tartificial\tneural\tnetworks,\tstarting\twith\ta\tquick\ttour\tof\tthe\tvery\tfirst ANN\tarchitectures.\tThen\twe\twill\tpresent\tMulti-Layer\tPerceptrons\t(MLPs)\tand\timplement\tone\tusing TensorFlow\tto\ttackle\tthe\tMNIST\tdigit\tclassification\tproblem\t(introduced\tin\tChapter\t3).\n\nFrom\tBiological\tto\tArtificial\tNeurons Surprisingly,\tANNs\thave\tbeen\taround\tfor\tquite\ta\twhile:\tthey\twere\tfirst\tintroduced\tback\tin\t1943\tby\tthe neurophysiologist\tWarren\tMcCulloch\tand\tthe\tmathematician\tWalter\tPitts.\tIn\ttheir\tlandmark\tpaper,2\t“A Logical\tCalculus\tof\tIdeas\tImmanent\tin\tNervous\tActivity,”\tMcCulloch\tand\tPitts\tpresented\ta\tsimplified computational\tmodel\tof\thow\tbiological\tneurons\tmight\twork\ttogether\tin\tanimal\tbrains\tto\tperform\tcomplex computations\tusing\tpropositional\tlogic.\tThis\twas\tthe\tfirst\tartificial\tneural\tnetwork\tarchitecture.\tSince then\tmany\tother\tarchitectures\thave\tbeen\tinvented,\tas\twe\twill\tsee.\n\nThe\tearly\tsuccesses\tof\tANNs\tuntil\tthe\t1960s\tled\tto\tthe\twidespread\tbelief\tthat\twe\twould\tsoon\tbe conversing\twith\ttruly\tintelligent\tmachines.\tWhen\tit\tbecame\tclear\tthat\tthis\tpromise\twould\tgo\tunfulfilled (at\tleast\tfor\tquite\ta\twhile),\tfunding\tflew\telsewhere\tand\tANNs\tentered\ta\tlong\tdark\tera.\tIn\tthe\tearly\t1980s there\twas\ta\trevival\tof\tinterest\tin\tANNs\tas\tnew\tnetwork\tarchitectures\twere\tinvented\tand\tbetter\ttraining techniques\twere\tdeveloped.\tBut\tby\tthe\t1990s,\tpowerful\talternative\tMachine\tLearning\ttechniques\tsuch\tas Support\tVector\tMachines\t(see\tChapter\t5)\twere\tfavored\tby\tmost\tresearchers,\tas\tthey\tseemed\tto\toffer better\tresults\tand\tstronger\ttheoretical\tfoundations.\tFinally,\twe\tare\tnow\twitnessing\tyet\tanother\twave\tof interest\tin\tANNs.\tWill\tthis\twave\tdie\tout\tlike\tthe\tprevious\tones\tdid?\tThere\tare\ta\tfew\tgood\treasons\tto believe\tthat\tthis\tone\tis\tdifferent\tand\twill\thave\ta\tmuch\tmore\tprofound\timpact\ton\tour\tlives:\n\nThere\tis\tnow\ta\thuge\tquantity\tof\tdata\tavailable\tto\ttrain\tneural\tnetworks,\tand\tANNs\tfrequently outperform\tother\tML\ttechniques\ton\tvery\tlarge\tand\tcomplex\tproblems.\n\nThe\ttremendous\tincrease\tin\tcomputing\tpower\tsince\tthe\t1990s\tnow\tmakes\tit\tpossible\tto\ttrain\tlarge neural\tnetworks\tin\ta\treasonable\tamount\tof\ttime.\tThis\tis\tin\tpart\tdue\tto\tMoore’s\tLaw,\tbut\talso\tthanks to\tthe\tgaming\tindustry,\twhich\thas\tproduced\tpowerful\tGPU\tcards\tby\tthe\tmillions.\n\nThe\ttraining\talgorithms\thave\tbeen\timproved.\tTo\tbe\tfair\tthey\tare\tonly\tslightly\tdifferent\tfrom\tthe\tones used\tin\tthe\t1990s,\tbut\tthese\trelatively\tsmall\ttweaks\thave\ta\thuge\tpositive\timpact.\n\nSome\ttheoretical\tlimitations\tof\tANNs\thave\tturned\tout\tto\tbe\tbenign\tin\tpractice.\tFor\texample,\tmany people\tthought\tthat\tANN\ttraining\talgorithms\twere\tdoomed\tbecause\tthey\twere\tlikely\tto\tget\tstuck\tin local\toptima,\tbut\tit\tturns\tout\tthat\tthis\tis\trather\trare\tin\tpractice\t(or\twhen\tit\tis\tthe\tcase,\tthey\tare\tusually fairly\tclose\tto\tthe\tglobal\toptimum).\n\nANNs\tseem\tto\thave\tentered\ta\tvirtuous\tcircle\tof\tfunding\tand\tprogress.\tAmazing\tproducts\tbased\ton ANNs\tregularly\tmake\tthe\theadline\tnews,\twhich\tpulls\tmore\tand\tmore\tattention\tand\tfunding\ttoward them,\tresulting\tin\tmore\tand\tmore\tprogress,\tand\teven\tmore\tamazing\tproducts.\n\nBiological\tNeurons Before\twe\tdiscuss\tartificial\tneurons,\tlet’s\ttake\ta\tquick\tlook\tat\ta\tbiological\tneuron\t(represented\tin Figure\t10-1).\tIt\tis\tan\tunusual-looking\tcell\tmostly\tfound\tin\tanimal\tcerebral\tcortexes\t(e.g.,\tyour\tbrain), composed\tof\ta\tcell\tbody\tcontaining\tthe\tnucleus\tand\tmost\tof\tthe\tcell’s\tcomplex\tcomponents,\tand\tmany branching\textensions\tcalled\tdendrites,\tplus\tone\tvery\tlong\textension\tcalled\tthe\taxon.\tThe\taxon’s\tlength may\tbe\tjust\ta\tfew\ttimes\tlonger\tthan\tthe\tcell\tbody,\tor\tup\tto\ttens\tof\tthousands\tof\ttimes\tlonger.\tNear\tits extremity\tthe\taxon\tsplits\toff\tinto\tmany\tbranches\tcalled\ttelodendria,\tand\tat\tthe\ttip\tof\tthese\tbranches\tare minuscule\tstructures\tcalled\tsynaptic\tterminals\t(or\tsimply\tsynapses),\twhich\tare\tconnected\tto\tthe dendrites\t(or\tdirectly\tto\tthe\tcell\tbody)\tof\tother\tneurons.\tBiological\tneurons\treceive\tshort\telectrical impulses\tcalled\tsignals\tfrom\tother\tneurons\tvia\tthese\tsynapses.\tWhen\ta\tneuron\treceives\ta\tsufficient number\tof\tsignals\tfrom\tother\tneurons\twithin\ta\tfew\tmilliseconds,\tit\tfires\tits\town\tsignals.\n\nFigure\t10-1.\tBiological\tneuron3\n\nThus,\tindividual\tbiological\tneurons\tseem\tto\tbehave\tin\ta\trather\tsimple\tway,\tbut\tthey\tare\torganized\tin\ta vast\tnetwork\tof\tbillions\tof\tneurons,\teach\tneuron\ttypically\tconnected\tto\tthousands\tof\tother\tneurons.\tHighly complex\tcomputations\tcan\tbe\tperformed\tby\ta\tvast\tnetwork\tof\tfairly\tsimple\tneurons,\tmuch\tlike\ta\tcomplex anthill\tcan\temerge\tfrom\tthe\tcombined\tefforts\tof\tsimple\tants.\tThe\tarchitecture\tof\tbiological\tneural networks\t(BNN)4\tis\tstill\tthe\tsubject\tof\tactive\tresearch,\tbut\tsome\tparts\tof\tthe\tbrain\thave\tbeen\tmapped, and\tit\tseems\tthat\tneurons\tare\toften\torganized\tin\tconsecutive\tlayers,\tas\tshown\tin\tFigure\t10-2.",
      "page_number": 287
    },
    {
      "number": 10,
      "title": "Introduction\tto\tArtificial\tNeural Networks",
      "start_page": 317,
      "end_page": 342,
      "detection_method": "regex_chapter_title",
      "content": "Figure\t10-2.\tMultiple\tlayers\tin\ta\tbiological\tneural\tnetwork\t(human\tcortex)5\n\nLogical\tComputations\twith\tNeurons Warren\tMcCulloch\tand\tWalter\tPitts\tproposed\ta\tvery\tsimple\tmodel\tof\tthe\tbiological\tneuron,\twhich\tlater became\tknown\tas\tan\tartificial\tneuron:\tit\thas\tone\tor\tmore\tbinary\t(on/off)\tinputs\tand\tone\tbinary\toutput. The\tartificial\tneuron\tsimply\tactivates\tits\toutput\twhen\tmore\tthan\ta\tcertain\tnumber\tof\tits\tinputs\tare\tactive. McCulloch\tand\tPitts\tshowed\tthat\teven\twith\tsuch\ta\tsimplified\tmodel\tit\tis\tpossible\tto\tbuild\ta\tnetwork\tof artificial\tneurons\tthat\tcomputes\tany\tlogical\tproposition\tyou\twant.\tFor\texample,\tlet’s\tbuild\ta\tfew\tANNs that\tperform\tvarious\tlogical\tcomputations\t(see\tFigure\t10-3),\tassuming\tthat\ta\tneuron\tis\tactivated\twhen\tat least\ttwo\tof\tits\tinputs\tare\tactive.\n\nFigure\t10-3.\tANNs\tperforming\tsimple\tlogical\tcomputations\n\nThe\tfirst\tnetwork\ton\tthe\tleft\tis\tsimply\tthe\tidentity\tfunction:\tif\tneuron\tA\tis\tactivated,\tthen\tneuron\tC gets\tactivated\tas\twell\t(since\tit\treceives\ttwo\tinput\tsignals\tfrom\tneuron\tA),\tbut\tif\tneuron\tA\tis\toff,\tthen neuron\tC\tis\toff\tas\twell.\n\nThe\tsecond\tnetwork\tperforms\ta\tlogical\tAND:\tneuron\tC\tis\tactivated\tonly\twhen\tboth\tneurons\tA\tand\tB are\tactivated\t(a\tsingle\tinput\tsignal\tis\tnot\tenough\tto\tactivate\tneuron\tC).\n\nThe\tthird\tnetwork\tperforms\ta\tlogical\tOR:\tneuron\tC\tgets\tactivated\tif\teither\tneuron\tA\tor\tneuron\tB\tis activated\t(or\tboth).\n\nFinally,\tif\twe\tsuppose\tthat\tan\tinput\tconnection\tcan\tinhibit\tthe\tneuron’s\tactivity\t(which\tis\tthe\tcase with\tbiological\tneurons),\tthen\tthe\tfourth\tnetwork\tcomputes\ta\tslightly\tmore\tcomplex\tlogical proposition:\tneuron\tC\tis\tactivated\tonly\tif\tneuron\tA\tis\tactive\tand\tif\tneuron\tB\tis\toff.\tIf\tneuron\tA\tis active\tall\tthe\ttime,\tthen\tyou\tget\ta\tlogical\tNOT:\tneuron\tC\tis\tactive\twhen\tneuron\tB\tis\toff,\tand\tvice versa.\n\nYou\tcan\teasily\timagine\thow\tthese\tnetworks\tcan\tbe\tcombined\tto\tcompute\tcomplex\tlogical\texpressions (see\tthe\texercises\tat\tthe\tend\tof\tthe\tchapter).\n\nThe\tPerceptron The\tPerceptron\tis\tone\tof\tthe\tsimplest\tANN\tarchitectures,\tinvented\tin\t1957\tby\tFrank\tRosenblatt.\tIt\tis based\ton\ta\tslightly\tdifferent\tartificial\tneuron\t(see\tFigure\t10-4)\tcalled\ta\tlinear\tthreshold\tunit\t(LTU):\tthe inputs\tand\toutput\tare\tnow\tnumbers\t(instead\tof\tbinary\ton/off\tvalues)\tand\teach\tinput\tconnection\tis associated\twith\ta\tweight.\tThe\tLTU\tcomputes\ta\tweighted\tsum\tof\tits\tinputs\t(z\t=\tw1\tx1\t+\tw2\tx2\t+\t\t+\t wn\txn =\twT\t·\tx),\tthen\tapplies\ta\tstep\tfunction\tto\tthat\tsum\tand\toutputs\tthe\tresult:\thw(x)\t=\tstep\t(z)\t=\tstep\t(wT\t·\tx).\n\nFigure\t10-4.\tLinear\tthreshold\tunit\n\nThe\tmost\tcommon\tstep\tfunction\tused\tin\tPerceptrons\tis\tthe\tHeaviside\tstep\tfunction\t(see\tEquation\t10-1). Sometimes\tthe\tsign\tfunction\tis\tused\tinstead.\n\nEquation\t10-1.\tCommon\tstep\tfunctions\tused\tin\tPerceptrons\n\nA\tsingle\tLTU\tcan\tbe\tused\tfor\tsimple\tlinear\tbinary\tclassification.\tIt\tcomputes\ta\tlinear\tcombination\tof\tthe inputs\tand\tif\tthe\tresult\texceeds\ta\tthreshold,\tit\toutputs\tthe\tpositive\tclass\tor\telse\toutputs\tthe\tnegative\tclass (just\tlike\ta\tLogistic\tRegression\tclassifier\tor\ta\tlinear\tSVM).\tFor\texample,\tyou\tcould\tuse\ta\tsingle\tLTU\tto classify\tiris\tflowers\tbased\ton\tthe\tpetal\tlength\tand\twidth\t(also\tadding\tan\textra\tbias\tfeature\tx0\t=\t1,\tjust\tlike we\tdid\tin\tprevious\tchapters).\tTraining\tan\tLTU\tmeans\tfinding\tthe\tright\tvalues\tfor\tw0,\tw1,\tand\tw2\t(the training\talgorithm\tis\tdiscussed\tshortly). A\tPerceptron\tis\tsimply\tcomposed\tof\ta\tsingle\tlayer\tof\tLTUs,6\twith\teach\tneuron\tconnected\tto\tall\tthe\tinputs. These\tconnections\tare\toften\trepresented\tusing\tspecial\tpassthrough\tneurons\tcalled\tinput\tneurons:\tthey\tjust output\twhatever\tinput\tthey\tare\tfed.\tMoreover,\tan\textra\tbias\tfeature\tis\tgenerally\tadded\t(x0\t=\t1).\tThis\tbias feature\tis\ttypically\trepresented\tusing\ta\tspecial\ttype\tof\tneuron\tcalled\ta\tbias\tneuron,\twhich\tjust\toutputs\t1 all\tthe\ttime.\n\nA\tPerceptron\twith\ttwo\tinputs\tand\tthree\toutputs\tis\trepresented\tin\tFigure\t10-5.\tThis\tPerceptron\tcan classify\tinstances\tsimultaneously\tinto\tthree\tdifferent\tbinary\tclasses,\twhich\tmakes\tit\ta\tmultioutput classifier.\n\nFigure\t10-5.\tPerceptron\tdiagram\n\nSo\thow\tis\ta\tPerceptron\ttrained?\tThe\tPerceptron\ttraining\talgorithm\tproposed\tby\tFrank\tRosenblatt\twas largely\tinspired\tby\tHebb’s\trule.\tIn\this\tbook\tThe\tOrganization\tof\tBehavior,\tpublished\tin\t1949,\tDonald Hebb\tsuggested\tthat\twhen\ta\tbiological\tneuron\toften\ttriggers\tanother\tneuron,\tthe\tconnection\tbetween\tthese two\tneurons\tgrows\tstronger.\tThis\tidea\twas\tlater\tsummarized\tby\tSiegrid\tLöwel\tin\tthis\tcatchy\tphrase: “Cells\tthat\tfire\ttogether,\twire\ttogether.”\tThis\trule\tlater\tbecame\tknown\tas\tHebb’s\trule\t(or\tHebbian learning);\tthat\tis,\tthe\tconnection\tweight\tbetween\ttwo\tneurons\tis\tincreased\twhenever\tthey\thave\tthe\tsame output.\tPerceptrons\tare\ttrained\tusing\ta\tvariant\tof\tthis\trule\tthat\ttakes\tinto\taccount\tthe\terror\tmade\tby\tthe network;\tit\tdoes\tnot\treinforce\tconnections\tthat\tlead\tto\tthe\twrong\toutput.\tMore\tspecifically,\tthe\tPerceptron is\tfed\tone\ttraining\tinstance\tat\ta\ttime,\tand\tfor\teach\tinstance\tit\tmakes\tits\tpredictions.\tFor\tevery\toutput neuron\tthat\tproduced\ta\twrong\tprediction,\tit\treinforces\tthe\tconnection\tweights\tfrom\tthe\tinputs\tthat\twould have\tcontributed\tto\tthe\tcorrect\tprediction.\tThe\trule\tis\tshown\tin\tEquation\t10-2.\n\nEquation\t10-2.\tPerceptron\tlearning\trule\t(weight\tupdate)\n\nwi,\tj\tis\tthe\tconnection\tweight\tbetween\tthe\tith\tinput\tneuron\tand\tthe\tjth\toutput\tneuron.\n\nxi\tis\tthe\tith\tinput\tvalue\tof\tthe\tcurrent\ttraining\tinstance.\n\nj\tis\tthe\toutput\tof\tthe\tjth\toutput\tneuron\tfor\tthe\tcurrent\ttraining\tinstance.\n\nyj\tis\tthe\ttarget\toutput\tof\tthe\tjth\toutput\tneuron\tfor\tthe\tcurrent\ttraining\tinstance.\n\nη\tis\tthe\tlearning\trate.\n\nThe\tdecision\tboundary\tof\teach\toutput\tneuron\tis\tlinear,\tso\tPerceptrons\tare\tincapable\tof\tlearning\tcomplex patterns\t(just\tlike\tLogistic\tRegression\tclassifiers).\tHowever,\tif\tthe\ttraining\tinstances\tare\tlinearly separable,\tRosenblatt\tdemonstrated\tthat\tthis\talgorithm\twould\tconverge\tto\ta\tsolution.7\tThis\tis\tcalled\tthe Perceptron\tconvergence\ttheorem.\n\nScikit-Learn\tprovides\ta\tPerceptron\tclass\tthat\timplements\ta\tsingle\tLTU\tnetwork.\tIt\tcan\tbe\tused\tpretty much\tas\tyou\twould\texpect\t—\tfor\texample,\ton\tthe\tiris\tdataset\t(introduced\tin\tChapter\t4):\n\nimport\tnumpy\tas\tnp from\tsklearn.datasets\timport\tload_iris from\tsklearn.linear_model\timport\tPerceptron\n\niris\t=\tload_iris() X\t=\tiris.data[:,\t(2,\t3)]\t\t#\tpetal\tlength,\tpetal\twidth y\t=\t(iris.target\t==\t0).astype(np.int)\t\t#\tIris\tSetosa?\n\nper_clf\t=\tPerceptron(random_state=42) per_clf.fit(X,\ty)\n\ny_pred\t=\tper_clf.predict([[2,\t0.5]])\n\nYou\tmay\thave\trecognized\tthat\tthe\tPerceptron\tlearning\talgorithm\tstrongly\tresembles\tStochastic\tGradient Descent.\tIn\tfact,\tScikit-Learn’s\tPerceptron\tclass\tis\tequivalent\tto\tusing\tan\tSGDClassifier\twith\tthe following\thyperparameters:\tloss=\"perceptron\",\tlearning_rate=\"constant\",\teta0=1\t(the\tlearning rate),\tand\tpenalty=None\t(no\tregularization).\n\nNote\tthat\tcontrary\tto\tLogistic\tRegression\tclassifiers,\tPerceptrons\tdo\tnot\toutput\ta\tclass\tprobability;\trather, they\tjust\tmake\tpredictions\tbased\ton\ta\thard\tthreshold.\tThis\tis\tone\tof\tthe\tgood\treasons\tto\tprefer\tLogistic Regression\tover\tPerceptrons.\n\nIn\ttheir\t1969\tmonograph\ttitled\tPerceptrons,\tMarvin\tMinsky\tand\tSeymour\tPapert\thighlighted\ta\tnumber\tof serious\tweaknesses\tof\tPerceptrons,\tin\tparticular\tthe\tfact\tthat\tthey\tare\tincapable\tof\tsolving\tsome\ttrivial problems\t(e.g.,\tthe\tExclusive\tOR\t(XOR)\tclassification\tproblem;\tsee\tthe\tleft\tside\tof\tFigure\t10-6).\tOf course\tthis\tis\ttrue\tof\tany\tother\tlinear\tclassification\tmodel\tas\twell\t(such\tas\tLogistic\tRegression classifiers),\tbut\tresearchers\thad\texpected\tmuch\tmore\tfrom\tPerceptrons,\tand\ttheir\tdisappointment\twas great:\tas\ta\tresult,\tmany\tresearchers\tdropped\tconnectionism\taltogether\t(i.e.,\tthe\tstudy\tof\tneural\tnetworks) in\tfavor\tof\thigher-level\tproblems\tsuch\tas\tlogic,\tproblem\tsolving,\tand\tsearch.\n\nHowever,\tit\tturns\tout\tthat\tsome\tof\tthe\tlimitations\tof\tPerceptrons\tcan\tbe\teliminated\tby\tstacking\tmultiple Perceptrons.\tThe\tresulting\tANN\tis\tcalled\ta\tMulti-Layer\tPerceptron\t(MLP).\tIn\tparticular,\tan\tMLP\tcan solve\tthe\tXOR\tproblem,\tas\tyou\tcan\tverify\tby\tcomputing\tthe\toutput\tof\tthe\tMLP\trepresented\ton\tthe\tright\tof Figure\t10-6,\tfor\teach\tcombination\tof\tinputs:\twith\tinputs\t(0,\t0)\tor\t(1,\t1)\tthe\tnetwork\toutputs\t0,\tand\twith inputs\t(0,\t1)\tor\t(1,\t0)\tit\toutputs\t1.\n\nFigure\t10-6.\tXOR\tclassification\tproblem\tand\tan\tMLP\tthat\tsolves\tit\n\nMulti-Layer\tPerceptron\tand\tBackpropagation An\tMLP\tis\tcomposed\tof\tone\t(passthrough)\tinput\tlayer,\tone\tor\tmore\tlayers\tof\tLTUs,\tcalled\thidden\tlayers, and\tone\tfinal\tlayer\tof\tLTUs\tcalled\tthe\toutput\tlayer\t(see\tFigure\t10-7).\tEvery\tlayer\texcept\tthe\toutput\tlayer includes\ta\tbias\tneuron\tand\tis\tfully\tconnected\tto\tthe\tnext\tlayer.\tWhen\tan\tANN\thas\ttwo\tor\tmore\thidden layers,\tit\tis\tcalled\ta\tdeep\tneural\tnetwork\t(DNN).\n\nFigure\t10-7.\tMulti-Layer\tPerceptron\n\nFor\tmany\tyears\tresearchers\tstruggled\tto\tfind\ta\tway\tto\ttrain\tMLPs,\twithout\tsuccess.\tBut\tin\t1986,\tD.\tE. Rumelhart\tet\tal.\tpublished\ta\tgroundbreaking\tarticle8\tintroducing\tthe\tbackpropagation\ttraining\talgorithm.9 Today\twe\twould\tdescribe\tit\tas\tGradient\tDescent\tusing\treverse-mode\tautodiff\t(Gradient\tDescent\twas introduced\tin\tChapter\t4,\tand\tautodiff\twas\tdiscussed\tin\tChapter\t9).\n\nFor\teach\ttraining\tinstance,\tthe\talgorithm\tfeeds\tit\tto\tthe\tnetwork\tand\tcomputes\tthe\toutput\tof\tevery\tneuron in\teach\tconsecutive\tlayer\t(this\tis\tthe\tforward\tpass,\tjust\tlike\twhen\tmaking\tpredictions).\tThen\tit\tmeasures the\tnetwork’s\toutput\terror\t(i.e.,\tthe\tdifference\tbetween\tthe\tdesired\toutput\tand\tthe\tactual\toutput\tof\tthe network),\tand\tit\tcomputes\thow\tmuch\teach\tneuron\tin\tthe\tlast\thidden\tlayer\tcontributed\tto\teach\toutput neuron’s\terror.\tIt\tthen\tproceeds\tto\tmeasure\thow\tmuch\tof\tthese\terror\tcontributions\tcame\tfrom\teach\tneuron in\tthe\tprevious\thidden\tlayer\t—\tand\tso\ton\tuntil\tthe\talgorithm\treaches\tthe\tinput\tlayer.\tThis\treverse\tpass efficiently\tmeasures\tthe\terror\tgradient\tacross\tall\tthe\tconnection\tweights\tin\tthe\tnetwork\tby\tpropagating\tthe error\tgradient\tbackward\tin\tthe\tnetwork\t(hence\tthe\tname\tof\tthe\talgorithm).\tIf\tyou\tcheck\tout\tthe\treverse- mode\tautodiff\talgorithm\tin\tAppendix\tD,\tyou\twill\tfind\tthat\tthe\tforward\tand\treverse\tpasses\tof backpropagation\tsimply\tperform\treverse-mode\tautodiff.\tThe\tlast\tstep\tof\tthe\tbackpropagation\talgorithm\tis a\tGradient\tDescent\tstep\ton\tall\tthe\tconnection\tweights\tin\tthe\tnetwork,\tusing\tthe\terror\tgradients\tmeasured earlier.\n\nLet’s\tmake\tthis\teven\tshorter:\tfor\teach\ttraining\tinstance\tthe\tbackpropagation\talgorithm\tfirst\tmakes\ta prediction\t(forward\tpass),\tmeasures\tthe\terror,\tthen\tgoes\tthrough\teach\tlayer\tin\treverse\tto\tmeasure\tthe error\tcontribution\tfrom\teach\tconnection\t(reverse\tpass),\tand\tfinally\tslightly\ttweaks\tthe\tconnection\tweights to\treduce\tthe\terror\t(Gradient\tDescent\tstep).\n\nIn\torder\tfor\tthis\talgorithm\tto\twork\tproperly,\tthe\tauthors\tmade\ta\tkey\tchange\tto\tthe\tMLP’s\tarchitecture:\tthey replaced\tthe\tstep\tfunction\twith\tthe\tlogistic\tfunction,\tσ(z)\t=\t1\t/\t(1\t+\texp(–z)).\tThis\twas\tessential\tbecause the\tstep\tfunction\tcontains\tonly\tflat\tsegments,\tso\tthere\tis\tno\tgradient\tto\twork\twith\t(Gradient\tDescent cannot\tmove\ton\ta\tflat\tsurface),\twhile\tthe\tlogistic\tfunction\thas\ta\twell-defined\tnonzero\tderivative everywhere,\tallowing\tGradient\tDescent\tto\tmake\tsome\tprogress\tat\tevery\tstep.\tThe\tbackpropagation algorithm\tmay\tbe\tused\twith\tother\tactivation\tfunctions,\tinstead\tof\tthe\tlogistic\tfunction.\tTwo\tother\tpopular activation\tfunctions\tare:\n\nThe\thyperbolic\ttangent\tfunction\ttanh\t(z)\t=\t2σ(2z)\t–\t1\n\nJust\tlike\tthe\tlogistic\tfunction\tit\tis\tS-shaped,\tcontinuous,\tand\tdifferentiable,\tbut\tits\toutput\tvalue ranges\tfrom\t–1\tto\t1\t(instead\tof\t0\tto\t1\tin\tthe\tcase\tof\tthe\tlogistic\tfunction),\twhich\ttends\tto\tmake\teach layer’s\toutput\tmore\tor\tless\tnormalized\t(i.e.,\tcentered\taround\t0)\tat\tthe\tbeginning\tof\ttraining.\tThis often\thelps\tspeed\tup\tconvergence.\n\nThe\tReLU\tfunction\t(introduced\tin\tChapter\t9)\n\nReLU\t(z)\t=\tmax\t(0,\tz).\tIt\tis\tcontinuous\tbut\tunfortunately\tnot\tdifferentiable\tat\tz\t=\t0\t(the\tslope\tchanges abruptly,\twhich\tcan\tmake\tGradient\tDescent\tbounce\taround).\tHowever,\tin\tpractice\tit\tworks\tvery well\tand\thas\tthe\tadvantage\tof\tbeing\tfast\tto\tcompute.\tMost\timportantly,\tthe\tfact\tthat\tit\tdoes\tnot\thave\ta maximum\toutput\tvalue\talso\thelps\treduce\tsome\tissues\tduring\tGradient\tDescent\t(we\twill\tcome\tback to\tthis\tin\tChapter\t11).\n\nThese\tpopular\tactivation\tfunctions\tand\ttheir\tderivatives\tare\trepresented\tin\tFigure\t10-8.\n\nFigure\t10-8.\tActivation\tfunctions\tand\ttheir\tderivatives\n\nAn\tMLP\tis\toften\tused\tfor\tclassification,\twith\teach\toutput\tcorresponding\tto\ta\tdifferent\tbinary\tclass\t(e.g., spam/ham,\turgent/not-urgent,\tand\tso\ton).\tWhen\tthe\tclasses\tare\texclusive\t(e.g.,\tclasses\t0\tthrough\t9\tfor digit\timage\tclassification),\tthe\toutput\tlayer\tis\ttypically\tmodified\tby\treplacing\tthe\tindividual\tactivation functions\tby\ta\tshared\tsoftmax\tfunction\t(see\tFigure\t10-9).\tThe\tsoftmax\tfunction\twas\tintroduced\tin Chapter\t3.\tThe\toutput\tof\teach\tneuron\tcorresponds\tto\tthe\testimated\tprobability\tof\tthe\tcorresponding\tclass. Note\tthat\tthe\tsignal\tflows\tonly\tin\tone\tdirection\t(from\tthe\tinputs\tto\tthe\toutputs),\tso\tthis\tarchitecture\tis\tan example\tof\ta\tfeedforward\tneural\tnetwork\t(FNN).\n\nFigure\t10-9.\tA\tmodern\tMLP\t(including\tReLU\tand\tsoftmax)\tfor\tclassification\n\nNOTE\n\nBiological\tneurons\tseem\tto\timplement\ta\troughly\tsigmoid\t(S-shaped)\tactivation\tfunction,\tso\tresearchers\tstuck\tto\tsigmoid\tfunctions for\ta\tvery\tlong\ttime.\tBut\tit\tturns\tout\tthat\tthe\tReLU\tactivation\tfunction\tgenerally\tworks\tbetter\tin\tANNs.\tThis\tis\tone\tof\tthe\tcases where\tthe\tbiological\tanalogy\twas\tmisleading.\n\nTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI The\tsimplest\tway\tto\ttrain\tan\tMLP\twith\tTensorFlow\tis\tto\tuse\tthe\thigh-level\tAPI\tTF.Learn,\twhich\toffers\ta Scikit-Learn–compatible\tAPI.\tThe\tDNNClassifier\tclass\tmakes\tit\tfairly\teasy\tto\ttrain\ta\tdeep\tneural network\twith\tany\tnumber\tof\thidden\tlayers,\tand\ta\tsoftmax\toutput\tlayer\tto\toutput\testimated\tclass probabilities.\tFor\texample,\tthe\tfollowing\tcode\ttrains\ta\tDNN\tfor\tclassification\twith\ttwo\thidden\tlayers (one\twith\t300\tneurons,\tand\tthe\tother\twith\t100\tneurons)\tand\ta\tsoftmax\toutput\tlayer\twith\t10\tneurons:\n\nimport\ttensorflow\tas\ttf\n\nfeature_cols\t=\ttf.contrib.learn.infer_real_valued_columns_from_input(X_train) dnn_clf\t=\ttf.contrib.learn.DNNClassifier(hidden_units=[300,100],\tn_classes=10, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfeature_columns=feature_cols) dnn_clf\t=\ttf.contrib.learn.SKCompat(dnn_clf)\t\t#\tif\tTensorFlow\t>=\t1.1 dnn_clf.fit(X_train,\ty_train,\tbatch_size=50,\tsteps=40000)\n\nThe\tcode\tfirst\tcreates\ta\tset\tof\treal\tvalued\tcolumns\tfrom\tthe\ttraining\tset\t(other\ttypes\tof\tcolumns,\tsuch\tas categorical\tcolumns,\tare\tavailable).\tThen\twe\tcreate\tthe\tDNNClassifier,\tand\twe\twrap\tit\tin\ta\tScikit- Learn\tcompatibility\thelper.\tFinally,\twe\trun\t40,000\ttraining\titerations\tusing\tbatches\tof\t50\tinstances.\n\nIf\tyou\trun\tthis\tcode\ton\tthe\tMNIST\tdataset\t(after\tscaling\tit,\te.g.,\tby\tusing\tScikit-Learn’s StandardScaler),\tyou\twill\tactually\tget\ta\tmodel\tthat\tachieves\taround\t98.2%\taccuracy\ton\tthe\ttest\tset! That’s\tbetter\tthan\tthe\tbest\tmodel\twe\ttrained\tin\tChapter\t3:\n\n>>>\tfrom\tsklearn.metrics\timport\taccuracy_score >>>\ty_pred\t=\tdnn_clf.predict(X_test) >>>\taccuracy_score(y_test,\ty_pred['classes']) 0.98250000000000004\n\nWARNING\n\nThe\ttensorflow.contrib\tpackage\tcontains\tmany\tuseful\tfunctions,\tbut\tit\tis\ta\tplace\tfor\texperimental\tcode\tthat\thas\tnot\tyet graduated\tto\tbe\tpart\tof\tthe\tcore\tTensorFlow\tAPI.\tSo\tthe\tDNNClassifier\tclass\t(and\tany\tother\tcontrib\tcode)\tmay\tchange without\tnotice\tin\tthe\tfuture.\n\nUnder\tthe\thood,\tthe\tDNNClassifier\tclass\tcreates\tall\tthe\tneuron\tlayers,\tbased\ton\tthe\tReLU\tactivation function\t(we\tcan\tchange\tthis\tby\tsetting\tthe\tactivation_fn\thyperparameter).\tThe\toutput\tlayer\trelies\ton the\tsoftmax\tfunction,\tand\tthe\tcost\tfunction\tis\tcross\tentropy\t(introduced\tin\tChapter\t4).\n\nTraining\ta\tDNN\tUsing\tPlain\tTensorFlow If\tyou\twant\tmore\tcontrol\tover\tthe\tarchitecture\tof\tthe\tnetwork,\tyou\tmay\tprefer\tto\tuse\tTensorFlow’s\tlower- level\tPython\tAPI\t(introduced\tin\tChapter\t9).\tIn\tthis\tsection\twe\twill\tbuild\tthe\tsame\tmodel\tas\tbefore\tusing this\tAPI,\tand\twe\twill\timplement\tMini-batch\tGradient\tDescent\tto\ttrain\tit\ton\tthe\tMNIST\tdataset.\tThe\tfirst step\tis\tthe\tconstruction\tphase,\tbuilding\tthe\tTensorFlow\tgraph.\tThe\tsecond\tstep\tis\tthe\texecution\tphase, where\tyou\tactually\trun\tthe\tgraph\tto\ttrain\tthe\tmodel.\n\nConstruction\tPhase Let’s\tstart.\tFirst\twe\tneed\tto\timport\tthe\ttensorflow\tlibrary.\tThen\twe\tmust\tspecify\tthe\tnumber\tof\tinputs and\toutputs,\tand\tset\tthe\tnumber\tof\thidden\tneurons\tin\teach\tlayer:\n\nimport\ttensorflow\tas\ttf\n\nn_inputs\t=\t28*28\t\t#\tMNIST n_hidden1\t=\t300 n_hidden2\t=\t100 n_outputs\t=\t10\n\nNext,\tjust\tlike\tyou\tdid\tin\tChapter\t9,\tyou\tcan\tuse\tplaceholder\tnodes\tto\trepresent\tthe\ttraining\tdata\tand targets.\tThe\tshape\tof\tX\tis\tonly\tpartially\tdefined.\tWe\tknow\tthat\tit\twill\tbe\ta\t2D\ttensor\t(i.e.,\ta\tmatrix),\twith instances\talong\tthe\tfirst\tdimension\tand\tfeatures\talong\tthe\tsecond\tdimension,\tand\twe\tknow\tthat\tthe\tnumber of\tfeatures\tis\tgoing\tto\tbe\t28\tx\t28\t(one\tfeature\tper\tpixel),\tbut\twe\tdon’t\tknow\tyet\thow\tmany\tinstances\teach training\tbatch\twill\tcontain.\tSo\tthe\tshape\tof\tX\tis\t(None,\tn_inputs).\tSimilarly,\twe\tknow\tthat\ty\twill\tbe\ta 1D\ttensor\twith\tone\tentry\tper\tinstance,\tbut\tagain\twe\tdon’t\tknow\tthe\tsize\tof\tthe\ttraining\tbatch\tat\tthis\tpoint, so\tthe\tshape\tis\t(None).\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_inputs),\tname=\"X\") y\t=\ttf.placeholder(tf.int64,\tshape=(None),\tname=\"y\")\n\nNow\tlet’s\tcreate\tthe\tactual\tneural\tnetwork.\tThe\tplaceholder\tX\twill\tact\tas\tthe\tinput\tlayer;\tduring\tthe execution\tphase,\tit\twill\tbe\treplaced\twith\tone\ttraining\tbatch\tat\ta\ttime\t(note\tthat\tall\tthe\tinstances\tin\ta training\tbatch\twill\tbe\tprocessed\tsimultaneously\tby\tthe\tneural\tnetwork).\tNow\tyou\tneed\tto\tcreate\tthe\ttwo hidden\tlayers\tand\tthe\toutput\tlayer.\tThe\ttwo\thidden\tlayers\tare\talmost\tidentical:\tthey\tdiffer\tonly\tby\tthe inputs\tthey\tare\tconnected\tto\tand\tby\tthe\tnumber\tof\tneurons\tthey\tcontain.\tThe\toutput\tlayer\tis\talso\tvery similar,\tbut\tit\tuses\ta\tsoftmax\tactivation\tfunction\tinstead\tof\ta\tReLU\tactivation\tfunction.\tSo\tlet’s\tcreate\ta neuron_layer()\tfunction\tthat\twe\twill\tuse\tto\tcreate\tone\tlayer\tat\ta\ttime.\tIt\twill\tneed\tparameters\tto specify\tthe\tinputs,\tthe\tnumber\tof\tneurons,\tthe\tactivation\tfunction,\tand\tthe\tname\tof\tthe\tlayer:\n\ndef\tneuron_layer(X,\tn_neurons,\tname,\tactivation=None): \t\t\t\twith\ttf.name_scope(name): \t\t\t\t\t\t\t\tn_inputs\t=\tint(X.get_shape()[1]) \t\t\t\t\t\t\t\tstddev\t=\t2\t/\tnp.sqrt(n_inputs) \t\t\t\t\t\t\t\tinit\t=\ttf.truncated_normal((n_inputs,\tn_neurons),\tstddev=stddev) \t\t\t\t\t\t\t\tW\t=\ttf.Variable(init,\tname=\"kernel\") \t\t\t\t\t\t\t\tb\t=\ttf.Variable(tf.zeros([n_neurons]),\tname=\"bias\") \t\t\t\t\t\t\t\tZ\t=\ttf.matmul(X,\tW)\t+\tb \t\t\t\t\t\t\t\tif\tactivation\tis\tnot\tNone: \t\t\t\t\t\t\t\t\t\t\t\treturn\tactivation(Z) \t\t\t\t\t\t\t\telse: \t\t\t\t\t\t\t\t\t\t\t\treturn\tZ\n\nLet’s\tgo\tthrough\tthis\tcode\tline\tby\tline:\n\n1.\t First\twe\tcreate\ta\tname\tscope\tusing\tthe\tname\tof\tthe\tlayer:\tit\twill\tcontain\tall\tthe\tcomputation\tnodes for\tthis\tneuron\tlayer.\tThis\tis\toptional,\tbut\tthe\tgraph\twill\tlook\tmuch\tnicer\tin\tTensorBoard\tif\tits\tnodes are\twell\torganized.\n\n2.\t Next,\twe\tget\tthe\tnumber\tof\tinputs\tby\tlooking\tup\tthe\tinput\tmatrix’s\tshape\tand\tgetting\tthe\tsize\tof\tthe\n\nsecond\tdimension\t(the\tfirst\tdimension\tis\tfor\tinstances).\n\n3.\t The\tnext\tthree\tlines\tcreate\ta\tW\tvariable\tthat\twill\thold\tthe\tweights\tmatrix\t(often\tcalled\tthe\tlayer’s kernel).\tIt\twill\tbe\ta\t2D\ttensor\tcontaining\tall\tthe\tconnection\tweights\tbetween\teach\tinput\tand\teach neuron;\thence,\tits\tshape\twill\tbe\t(n_inputs,\tn_neurons).\tIt\twill\tbe\tinitialized\trandomly,\tusing\ta .\tUsing\tthis\n\n4.\t The\tnext\tline\tcreates\ta\tb\tvariable\tfor\tbiases,\tinitialized\tto\t0\t(no\tsymmetry\tissue\tin\tthis\tcase),\twith one\tbias\tparameter\tper\tneuron.\n\n5.\t Then\twe\tcreate\ta\tsubgraph\tto\tcompute\tZ\t=\tX\t·\tW\t+\tb.\tThis\tvectorized\timplementation\twill efficiently\tcompute\tthe\tweighted\tsums\tof\tthe\tinputs\tplus\tthe\tbias\tterm\tfor\teach\tand\tevery\tneuron\tin the\tlayer,\tfor\tall\tthe\tinstances\tin\tthe\tbatch\tin\tjust\tone\tshot.\n\n6.\t Finally,\tif\tan\tactivation\tparameter\tis\tprovided,\tsuch\tas\ttf.nn.relu\t(i.e.,\tmax\t(0,\tZ)),\tthen\tthe code\treturns\tactivation(Z),\tor\telse\tit\tjust\treturns\tZ.\n\nOkay,\tso\tnow\tyou\thave\ta\tnice\tfunction\tto\tcreate\ta\tneuron\tlayer.\tLet’s\tuse\tit\tto\tcreate\tthe\tdeep\tneural network!\tThe\tfirst\thidden\tlayer\ttakes\tX\tas\tits\tinput.\tThe\tsecond\ttakes\tthe\toutput\tof\tthe\tfirst\thidden\tlayer\tas its\tinput.\tAnd\tfinally,\tthe\toutput\tlayer\ttakes\tthe\toutput\tof\tthe\tsecond\thidden\tlayer\tas\tits\tinput.\n\nwith\ttf.name_scope(\"dnn\"): \t\t\t\thidden1\t=\tneuron_layer(X,\tn_hidden1,\tname=\"hidden1\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu) \t\t\t\thidden2\t=\tneuron_layer(hidden1,\tn_hidden2,\tname=\"hidden2\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu) \t\t\t\tlogits\t=\tneuron_layer(hidden2,\tn_outputs,\tname=\"outputs\")\n\nNotice\tthat\tonce\tagain\twe\tused\ta\tname\tscope\tfor\tclarity.\tAlso\tnote\tthat\tlogits\tis\tthe\toutput\tof\tthe\tneural network\tbefore\tgoing\tthrough\tthe\tsoftmax\tactivation\tfunction:\tfor\toptimization\treasons,\twe\twill\thandle\tthe softmax\tcomputation\tlater.\n\nAs\tyou\tmight\texpect,\tTensorFlow\tcomes\twith\tmany\thandy\tfunctions\tto\tcreate\tstandard\tneural\tnetwork layers,\tso\tthere’s\toften\tno\tneed\tto\tdefine\tyour\town\tneuron_layer()\tfunction\tlike\twe\tjust\tdid.\tFor example,\tTensorFlow’s\ttf.layers.dense()\tfunction\t(previously\tcalled tf.contrib.layers.fully_connected())\tcreates\ta\tfully\tconnected\tlayer,\twhere\tall\tthe\tinputs\tare connected\tto\tall\tthe\tneurons\tin\tthe\tlayer.\tIt\ttakes\tcare\tof\tcreating\tthe\tweights\tand\tbiases\tvariables,\tnamed kernel\tand\tbias\trespectively,\tusing\tthe\tappropriate\tinitialization\tstrategy,\tand\tyou\tcan\tset\tthe\tactivation function\tusing\tthe\tactivation\targument.\tAs\twe\twill\tsee\tin\tChapter\t11,\tit\talso\tsupports\tregularization parameters.\tLet’s\ttweak\tthe\tpreceding\tcode\tto\tuse\tthe\tdense()\tfunction\tinstead\tof\tour\tneuron_layer() function.\tSimply\treplace\tthe\tdnn\tconstruction\tsection\twith\tthe\tfollowing\tcode:\n\nwith\ttf.name_scope(\"dnn\"):\n\nhidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tname=\"hidden1\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu) \t\t\t\thidden2\t=\ttf.layers.dense(hidden1,\tn_hidden2,\tname=\"hidden2\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu) \t\t\t\tlogits\t=\ttf.layers.dense(hidden2,\tn_outputs,\tname=\"outputs\")\n\nNow\tthat\twe\thave\tthe\tneural\tnetwork\tmodel\tready\tto\tgo,\twe\tneed\tto\tdefine\tthe\tcost\tfunction\tthat\twe\twill use\tto\ttrain\tit.\tJust\tas\twe\tdid\tfor\tSoftmax\tRegression\tin\tChapter\t4,\twe\twill\tuse\tcross\tentropy.\tAs\twe discussed\tearlier,\tcross\tentropy\twill\tpenalize\tmodels\tthat\testimate\ta\tlow\tprobability\tfor\tthe\ttarget\tclass. TensorFlow\tprovides\tseveral\tfunctions\tto\tcompute\tcross\tentropy.\tWe\twill\tuse sparse_softmax_cross_entropy_with_logits():\tit\tcomputes\tthe\tcross\tentropy\tbased\ton\tthe\t“logits” (i.e.,\tthe\toutput\tof\tthe\tnetwork\tbefore\tgoing\tthrough\tthe\tsoftmax\tactivation\tfunction),\tand\tit\texpects\tlabels in\tthe\tform\tof\tintegers\tranging\tfrom\t0\tto\tthe\tnumber\tof\tclasses\tminus\t1\t(in\tour\tcase,\tfrom\t0\tto\t9).\tThis will\tgive\tus\ta\t1D\ttensor\tcontaining\tthe\tcross\tentropy\tfor\teach\tinstance.\tWe\tcan\tthen\tuse\tTensorFlow’s reduce_mean()\tfunction\tto\tcompute\tthe\tmean\tcross\tentropy\tover\tall\tinstances.\n\nwith\ttf.name_scope(\"loss\"): \t\t\t\txentropy\t=\ttf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogits=logits) \t\t\t\tloss\t=\ttf.reduce_mean(xentropy,\tname=\"loss\")\n\nNOTE\n\nThe\tsparse_softmax_cross_entropy_with_logits()\tfunction\tis\tequivalent\tto\tapplying\tthe\tsoftmax\tactivation\tfunction\tand\tthen computing\tthe\tcross\tentropy,\tbut\tit\tis\tmore\tefficient,\tand\tit\tproperly\ttakes\tcare\tof\tcorner\tcases\tlike\tlogits\tequal\tto\t0.\tThis\tis\twhy we\tdid\tnot\tapply\tthe\tsoftmax\tactivation\tfunction\tearlier.\tThere\tis\talso\tanother\tfunction\tcalled softmax_cross_entropy_with_logits(),\twhich\ttakes\tlabels\tin\tthe\tform\tof\tone-hot\tvectors\t(instead\tof\tints\tfrom\t0\tto\tthe\tnumber of\tclasses\tminus\t1).\n\nWe\thave\tthe\tneural\tnetwork\tmodel,\twe\thave\tthe\tcost\tfunction,\tand\tnow\twe\tneed\tto\tdefine\ta GradientDescentOptimizer\tthat\twill\ttweak\tthe\tmodel\tparameters\tto\tminimize\tthe\tcost\tfunction. Nothing\tnew;\tit’s\tjust\tlike\twe\tdid\tin\tChapter\t9:\n\nlearning_rate\t=\t0.01\n\nwith\ttf.name_scope(\"train\"): \t\t\t\toptimizer\t=\ttf.train.GradientDescentOptimizer(learning_rate) \t\t\t\ttraining_op\t=\toptimizer.minimize(loss)\n\nThe\tlast\timportant\tstep\tin\tthe\tconstruction\tphase\tis\tto\tspecify\thow\tto\tevaluate\tthe\tmodel.\tWe\twill\tsimply use\taccuracy\tas\tour\tperformance\tmeasure.\tFirst,\tfor\teach\tinstance,\tdetermine\tif\tthe\tneural\tnetwork’s prediction\tis\tcorrect\tby\tchecking\twhether\tor\tnot\tthe\thighest\tlogit\tcorresponds\tto\tthe\ttarget\tclass.\tFor\tthis you\tcan\tuse\tthe\tin_top_k()\tfunction.\tThis\treturns\ta\t1D\ttensor\tfull\tof\tboolean\tvalues,\tso\twe\tneed\tto\tcast these\tbooleans\tto\tfloats\tand\tthen\tcompute\tthe\taverage.\tThis\twill\tgive\tus\tthe\tnetwork’s\toverall\taccuracy.\n\nwith\ttf.name_scope(\"eval\"): \t\t\t\tcorrect\t=\ttf.nn.in_top_k(logits,\ty,\t1) \t\t\t\taccuracy\t=\ttf.reduce_mean(tf.cast(correct,\ttf.float32))\n\nAnd,\tas\tusual,\twe\tneed\tto\tcreate\ta\tnode\tto\tinitialize\tall\tvariables,\tand\twe\twill\talso\tcreate\ta\tSaver\tto\n\nsave\tour\ttrained\tmodel\tparameters\tto\tdisk:\n\ninit\t=\ttf.global_variables_initializer() saver\t=\ttf.train.Saver()\n\nPhew!\tThis\tconcludes\tthe\tconstruction\tphase.\tThis\twas\tfewer\tthan\t40\tlines\tof\tcode,\tbut\tit\twas\tpretty intense:\twe\tcreated\tplaceholders\tfor\tthe\tinputs\tand\tthe\ttargets,\twe\tcreated\ta\tfunction\tto\tbuild\ta\tneuron layer,\twe\tused\tit\tto\tcreate\tthe\tDNN,\twe\tdefined\tthe\tcost\tfunction,\twe\tcreated\tan\toptimizer,\tand\tfinally\twe defined\tthe\tperformance\tmeasure.\tNow\ton\tto\tthe\texecution\tphase.\n\nExecution\tPhase This\tpart\tis\tmuch\tshorter\tand\tsimpler.\tFirst,\tlet’s\tload\tMNIST.\tWe\tcould\tuse\tScikit-Learn\tfor\tthat\tas\twe did\tin\tprevious\tchapters,\tbut\tTensorFlow\toffers\tits\town\thelper\tthat\tfetches\tthe\tdata,\tscales\tit\t(between\t0 and\t1),\tshuffles\tit,\tand\tprovides\ta\tsimple\tfunction\tto\tload\tone\tmini-batch\ta\ttime.\tSo\tlet’s\tuse\tit\tinstead:\n\nfrom\ttensorflow.examples.tutorials.mnist\timport\tinput_data mnist\t=\tinput_data.read_data_sets(\"/tmp/data/\")\n\nNow\twe\tdefine\tthe\tnumber\tof\tepochs\tthat\twe\twant\tto\trun,\tas\twell\tas\tthe\tsize\tof\tthe\tmini-batches:\n\nn_epochs\t=\t40 batch_size\t=\t50\n\nAnd\tnow\twe\tcan\ttrain\tthe\tmodel:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tfor\titeration\tin\trange(mnist.train.num_examples\t//\tbatch_size): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\tacc_train\t=\taccuracy.eval(feed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\tacc_test\t=\taccuracy.eval(feed_dict={X:\tmnist.test.images, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ty:\tmnist.test.labels}) \t\t\t\t\t\t\t\tprint(epoch,\t\"Train\taccuracy:\",\tacc_train,\t\"Test\taccuracy:\",\tacc_test)\n\nsave_path\t=\tsaver.save(sess,\t\"./my_model_final.ckpt\")\n\nThis\tcode\topens\ta\tTensorFlow\tsession,\tand\tit\truns\tthe\tinit\tnode\tthat\tinitializes\tall\tthe\tvariables.\tThen\tit runs\tthe\tmain\ttraining\tloop:\tat\teach\tepoch,\tthe\tcode\titerates\tthrough\ta\tnumber\tof\tmini-batches\tthat corresponds\tto\tthe\ttraining\tset\tsize.\tEach\tmini-batch\tis\tfetched\tvia\tthe\tnext_batch()\tmethod,\tand\tthen the\tcode\tsimply\truns\tthe\ttraining\toperation,\tfeeding\tit\tthe\tcurrent\tmini-batch\tinput\tdata\tand\ttargets.\tNext, at\tthe\tend\tof\teach\tepoch,\tthe\tcode\tevaluates\tthe\tmodel\ton\tthe\tlast\tmini-batch\tand\ton\tthe\tfull\ttest\tset,\tand\tit prints\tout\tthe\tresult.\tFinally,\tthe\tmodel\tparameters\tare\tsaved\tto\tdisk.\n\nUsing\tthe\tNeural\tNetwork Now\tthat\tthe\tneural\tnetwork\tis\ttrained,\tyou\tcan\tuse\tit\tto\tmake\tpredictions.\tTo\tdo\tthat,\tyou\tcan\treuse\tthe same\tconstruction\tphase,\tbut\tchange\tthe\texecution\tphase\tlike\tthis:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsaver.restore(sess,\t\"./my_model_final.ckpt\") \t\t\t\tX_new_scaled\t=\t[...]\t\t#\tsome\tnew\timages\t(scaled\tfrom\t0\tto\t1) \t\t\t\tZ\t=\tlogits.eval(feed_dict={X:\tX_new_scaled}) \t\t\t\ty_pred\t=\tnp.argmax(Z,\taxis=1)\n\nFirst\tthe\tcode\tloads\tthe\tmodel\tparameters\tfrom\tdisk.\tThen\tit\tloads\tsome\tnew\timages\tthat\tyou\twant\tto classify.\tRemember\tto\tapply\tthe\tsame\tfeature\tscaling\tas\tfor\tthe\ttraining\tdata\t(in\tthis\tcase,\tscale\tit\tfrom\t0 to\t1).\tThen\tthe\tcode\tevaluates\tthe\tlogits\tnode.\tIf\tyou\twanted\tto\tknow\tall\tthe\testimated\tclass probabilities,\tyou\twould\tneed\tto\tapply\tthe\tsoftmax()\tfunction\tto\tthe\tlogits,\tbut\tif\tyou\tjust\twant\tto\tpredict a\tclass,\tyou\tcan\tsimply\tpick\tthe\tclass\tthat\thas\tthe\thighest\tlogit\tvalue\t(using\tthe\targmax()\tfunction\tdoes the\ttrick).\n\nFine-Tuning\tNeural\tNetwork\tHyperparameters The\tflexibility\tof\tneural\tnetworks\tis\talso\tone\tof\ttheir\tmain\tdrawbacks:\tthere\tare\tmany\thyperparameters\tto tweak.\tNot\tonly\tcan\tyou\tuse\tany\timaginable\tnetwork\ttopology\t(how\tneurons\tare\tinterconnected),\tbut\teven in\ta\tsimple\tMLP\tyou\tcan\tchange\tthe\tnumber\tof\tlayers,\tthe\tnumber\tof\tneurons\tper\tlayer,\tthe\ttype\tof activation\tfunction\tto\tuse\tin\teach\tlayer,\tthe\tweight\tinitialization\tlogic,\tand\tmuch\tmore.\tHow\tdo\tyou\tknow what\tcombination\tof\thyperparameters\tis\tthe\tbest\tfor\tyour\ttask?\n\nOf\tcourse,\tyou\tcan\tuse\tgrid\tsearch\twith\tcross-validation\tto\tfind\tthe\tright\thyperparameters,\tlike\tyou\tdid\tin previous\tchapters,\tbut\tsince\tthere\tare\tmany\thyperparameters\tto\ttune,\tand\tsince\ttraining\ta\tneural\tnetwork on\ta\tlarge\tdataset\ttakes\ta\tlot\tof\ttime,\tyou\twill\tonly\tbe\table\tto\texplore\ta\ttiny\tpart\tof\tthe\thyperparameter space\tin\ta\treasonable\tamount\tof\ttime.\tIt\tis\tmuch\tbetter\tto\tuse\trandomized\tsearch,\tas\twe\tdiscussed\tin Chapter\t2.\tAnother\toption\tis\tto\tuse\ta\ttool\tsuch\tas\tOscar,\twhich\timplements\tmore\tcomplex\talgorithms\tto help\tyou\tfind\ta\tgood\tset\tof\thyperparameters\tquickly.\n\nIt\thelps\tto\thave\tan\tidea\tof\twhat\tvalues\tare\treasonable\tfor\teach\thyperparameter,\tso\tyou\tcan\trestrict\tthe search\tspace.\tLet’s\tstart\twith\tthe\tnumber\tof\thidden\tlayers.\n\nNumber\tof\tHidden\tLayers For\tmany\tproblems,\tyou\tcan\tjust\tbegin\twith\ta\tsingle\thidden\tlayer\tand\tyou\twill\tget\treasonable\tresults.\tIt has\tactually\tbeen\tshown\tthat\tan\tMLP\twith\tjust\tone\thidden\tlayer\tcan\tmodel\teven\tthe\tmost\tcomplex functions\tprovided\tit\thas\tenough\tneurons.\tFor\ta\tlong\ttime,\tthese\tfacts\tconvinced\tresearchers\tthat\tthere was\tno\tneed\tto\tinvestigate\tany\tdeeper\tneural\tnetworks.\tBut\tthey\toverlooked\tthe\tfact\tthat\tdeep\tnetworks have\ta\tmuch\thigher\tparameter\tefficiency\tthan\tshallow\tones:\tthey\tcan\tmodel\tcomplex\tfunctions\tusing exponentially\tfewer\tneurons\tthan\tshallow\tnets,\tmaking\tthem\tmuch\tfaster\tto\ttrain.\n\nTo\tunderstand\twhy,\tsuppose\tyou\tare\tasked\tto\tdraw\ta\tforest\tusing\tsome\tdrawing\tsoftware,\tbut\tyou\tare forbidden\tto\tuse\tcopy/paste.\tYou\twould\thave\tto\tdraw\teach\ttree\tindividually,\tbranch\tper\tbranch,\tleaf\tper leaf.\tIf\tyou\tcould\tinstead\tdraw\tone\tleaf,\tcopy/paste\tit\tto\tdraw\ta\tbranch,\tthen\tcopy/paste\tthat\tbranch\tto create\ta\ttree,\tand\tfinally\tcopy/paste\tthis\ttree\tto\tmake\ta\tforest,\tyou\twould\tbe\tfinished\tin\tno\ttime.\tReal- world\tdata\tis\toften\tstructured\tin\tsuch\ta\thierarchical\tway\tand\tDNNs\tautomatically\ttake\tadvantage\tof\tthis fact:\tlower\thidden\tlayers\tmodel\tlow-level\tstructures\t(e.g.,\tline\tsegments\tof\tvarious\tshapes\tand orientations),\tintermediate\thidden\tlayers\tcombine\tthese\tlow-level\tstructures\tto\tmodel\tintermediate-level structures\t(e.g.,\tsquares,\tcircles),\tand\tthe\thighest\thidden\tlayers\tand\tthe\toutput\tlayer\tcombine\tthese intermediate\tstructures\tto\tmodel\thigh-level\tstructures\t(e.g.,\tfaces).\n\nNot\tonly\tdoes\tthis\thierarchical\tarchitecture\thelp\tDNNs\tconverge\tfaster\tto\ta\tgood\tsolution,\tit\talso improves\ttheir\tability\tto\tgeneralize\tto\tnew\tdatasets.\tFor\texample,\tif\tyou\thave\talready\ttrained\ta\tmodel\tto recognize\tfaces\tin\tpictures,\tand\tyou\tnow\twant\tto\ttrain\ta\tnew\tneural\tnetwork\tto\trecognize\thairstyles,\tthen you\tcan\tkickstart\ttraining\tby\treusing\tthe\tlower\tlayers\tof\tthe\tfirst\tnetwork.\tInstead\tof\trandomly\tinitializing the\tweights\tand\tbiases\tof\tthe\tfirst\tfew\tlayers\tof\tthe\tnew\tneural\tnetwork,\tyou\tcan\tinitialize\tthem\tto\tthe value\tof\tthe\tweights\tand\tbiases\tof\tthe\tlower\tlayers\tof\tthe\tfirst\tnetwork.\tThis\tway\tthe\tnetwork\twill\tnot have\tto\tlearn\tfrom\tscratch\tall\tthe\tlow-level\tstructures\tthat\toccur\tin\tmost\tpictures;\tit\twill\tonly\thave\tto learn\tthe\thigher-level\tstructures\t(e.g.,\thairstyles).\n\nIn\tsummary,\tfor\tmany\tproblems\tyou\tcan\tstart\twith\tjust\tone\tor\ttwo\thidden\tlayers\tand\tit\twill\twork\tjust\tfine (e.g.,\tyou\tcan\teasily\treach\tabove\t97%\taccuracy\ton\tthe\tMNIST\tdataset\tusing\tjust\tone\thidden\tlayer\twith\ta few\thundred\tneurons,\tand\tabove\t98%\taccuracy\tusing\ttwo\thidden\tlayers\twith\tthe\tsame\ttotal\tamount\tof neurons,\tin\troughly\tthe\tsame\tamount\tof\ttraining\ttime).\tFor\tmore\tcomplex\tproblems,\tyou\tcan\tgradually ramp\tup\tthe\tnumber\tof\thidden\tlayers,\tuntil\tyou\tstart\toverfitting\tthe\ttraining\tset.\tVery\tcomplex\ttasks,\tsuch as\tlarge\timage\tclassification\tor\tspeech\trecognition,\ttypically\trequire\tnetworks\twith\tdozens\tof\tlayers\t(or even\thundreds,\tbut\tnot\tfully\tconnected\tones,\tas\twe\twill\tsee\tin\tChapter\t13),\tand\tthey\tneed\ta\thuge\tamount of\ttraining\tdata.\tHowever,\tyou\twill\trarely\thave\tto\ttrain\tsuch\tnetworks\tfrom\tscratch:\tit\tis\tmuch\tmore common\tto\treuse\tparts\tof\ta\tpretrained\tstate-of-the-art\tnetwork\tthat\tperforms\ta\tsimilar\ttask.\tTraining\twill be\ta\tlot\tfaster\tand\trequire\tmuch\tless\tdata\t(we\twill\tdiscuss\tthis\tin\tChapter\t11).\n\nNumber\tof\tNeurons\tper\tHidden\tLayer Obviously\tthe\tnumber\tof\tneurons\tin\tthe\tinput\tand\toutput\tlayers\tis\tdetermined\tby\tthe\ttype\tof\tinput\tand output\tyour\ttask\trequires.\tFor\texample,\tthe\tMNIST\ttask\trequires\t28\tx\t28\t=\t784\tinput\tneurons\tand\t10 output\tneurons.\tAs\tfor\tthe\thidden\tlayers,\ta\tcommon\tpractice\tis\tto\tsize\tthem\tto\tform\ta\tfunnel,\twith\tfewer and\tfewer\tneurons\tat\teach\tlayer\t—\tthe\trationale\tbeing\tthat\tmany\tlow-level\tfeatures\tcan\tcoalesce\tinto\tfar fewer\thigh-level\tfeatures.\tFor\texample,\ta\ttypical\tneural\tnetwork\tfor\tMNIST\tmay\thave\ttwo\thidden\tlayers, the\tfirst\twith\t300\tneurons\tand\tthe\tsecond\twith\t100.\tHowever,\tthis\tpractice\tis\tnot\tas\tcommon\tnow,\tand you\tmay\tsimply\tuse\tthe\tsame\tsize\tfor\tall\thidden\tlayers\t—\tfor\texample,\tall\thidden\tlayers\twith\t150 neurons:\tthat’s\tjust\tone\thyperparameter\tto\ttune\tinstead\tof\tone\tper\tlayer.\tJust\tlike\tfor\tthe\tnumber\tof\tlayers, you\tcan\ttry\tincreasing\tthe\tnumber\tof\tneurons\tgradually\tuntil\tthe\tnetwork\tstarts\toverfitting.\tIn\tgeneral\tyou will\tget\tmore\tbang\tfor\tthe\tbuck\tby\tincreasing\tthe\tnumber\tof\tlayers\tthan\tthe\tnumber\tof\tneurons\tper\tlayer. Unfortunately,\tas\tyou\tcan\tsee,\tfinding\tthe\tperfect\tamount\tof\tneurons\tis\tstill\tsomewhat\tof\ta\tblack\tart.\n\nA\tsimpler\tapproach\tis\tto\tpick\ta\tmodel\twith\tmore\tlayers\tand\tneurons\tthan\tyou\tactually\tneed,\tthen\tuse\tearly stopping\tto\tprevent\tit\tfrom\toverfitting\t(and\tother\tregularization\ttechniques,\tespecially\tdropout,\tas\twe\twill see\tin\tChapter\t11).\tThis\thas\tbeen\tdubbed\tthe\t“stretch\tpants”\tapproach:12\tinstead\tof\twasting\ttime\tlooking for\tpants\tthat\tperfectly\tmatch\tyour\tsize,\tjust\tuse\tlarge\tstretch\tpants\tthat\twill\tshrink\tdown\tto\tthe\tright\tsize.\n\nActivation\tFunctions In\tmost\tcases\tyou\tcan\tuse\tthe\tReLU\tactivation\tfunction\tin\tthe\thidden\tlayers\t(or\tone\tof\tits\tvariants,\tas\twe will\tsee\tin\tChapter\t11).\tIt\tis\ta\tbit\tfaster\tto\tcompute\tthan\tother\tactivation\tfunctions,\tand\tGradient\tDescent does\tnot\tget\tstuck\tas\tmuch\ton\tplateaus,\tthanks\tto\tthe\tfact\tthat\tit\tdoes\tnot\tsaturate\tfor\tlarge\tinput\tvalues\t(as opposed\tto\tthe\tlogistic\tfunction\tor\tthe\thyperbolic\ttangent\tfunction,\twhich\tsaturate\tat\t1).\n\nFor\tthe\toutput\tlayer,\tthe\tsoftmax\tactivation\tfunction\tis\tgenerally\ta\tgood\tchoice\tfor\tclassification\ttasks (when\tthe\tclasses\tare\tmutually\texclusive).\tFor\tregression\ttasks,\tyou\tcan\tsimply\tuse\tno\tactivation\tfunction at\tall.\n\nThis\tconcludes\tthis\tintroduction\tto\tartificial\tneural\tnetworks.\tIn\tthe\tfollowing\tchapters,\twe\twill\tdiscuss techniques\tto\ttrain\tvery\tdeep\tnets,\tand\tdistribute\ttraining\tacross\tmultiple\tservers\tand\tGPUs.\tThen\twe\twill explore\ta\tfew\tother\tpopular\tneural\tnetwork\tarchitectures:\tconvolutional\tneural\tnetworks,\trecurrent\tneural networks,\tand\tautoencoders.13\n\nExercises\n\n1.\t Draw\tan\tANN\tusing\tthe\toriginal\tartificial\tneurons\t(like\tthe\tones\tin\tFigure\t10-3)\tthat\tcomputes\tA B).\n\n2.\t Why\tis\tit\tgenerally\tpreferable\tto\tuse\ta\tLogistic\tRegression\tclassifier\trather\tthan\ta\tclassical Perceptron\t(i.e.,\ta\tsingle\tlayer\tof\tlinear\tthreshold\tunits\ttrained\tusing\tthe\tPerceptron\ttraining algorithm)?\tHow\tcan\tyou\ttweak\ta\tPerceptron\tto\tmake\tit\tequivalent\tto\ta\tLogistic\tRegression classifier?\n\n3.\t Why\twas\tthe\tlogistic\tactivation\tfunction\ta\tkey\tingredient\tin\ttraining\tthe\tfirst\tMLPs?\n\n4.\t Name\tthree\tpopular\tactivation\tfunctions.\tCan\tyou\tdraw\tthem?\n\n5.\t Suppose\tyou\thave\tan\tMLP\tcomposed\tof\tone\tinput\tlayer\twith\t10\tpassthrough\tneurons,\tfollowed\tby one\thidden\tlayer\twith\t50\tartificial\tneurons,\tand\tfinally\tone\toutput\tlayer\twith\t3\tartificial\tneurons.\tAll artificial\tneurons\tuse\tthe\tReLU\tactivation\tfunction.\n\nWhat\tis\tthe\tshape\tof\tthe\tinput\tmatrix\tX?\n\nWhat\tabout\tthe\tshape\tof\tthe\thidden\tlayer’s\tweight\tvector\tWh,\tand\tthe\tshape\tof\tits\tbias\tvector bh?\n\nWhat\tis\tthe\tshape\tof\tthe\toutput\tlayer’s\tweight\tvector\tWo,\tand\tits\tbias\tvector\tbo?\n\nWhat\tis\tthe\tshape\tof\tthe\tnetwork’s\toutput\tmatrix\tY?\n\nWrite\tthe\tequation\tthat\tcomputes\tthe\tnetwork’s\toutput\tmatrix\tY\tas\ta\tfunction\tof\tX,\tWh,\tbh,\tWo and\tbo.\n\n6.\t How\tmany\tneurons\tdo\tyou\tneed\tin\tthe\toutput\tlayer\tif\tyou\twant\tto\tclassify\temail\tinto\tspam\tor\tham? What\tactivation\tfunction\tshould\tyou\tuse\tin\tthe\toutput\tlayer?\tIf\tinstead\tyou\twant\tto\ttackle\tMNIST, how\tmany\tneurons\tdo\tyou\tneed\tin\tthe\toutput\tlayer,\tusing\twhat\tactivation\tfunction?\tAnswer\tthe\tsame questions\tfor\tgetting\tyour\tnetwork\tto\tpredict\thousing\tprices\tas\tin\tChapter\t2.\n\n7.\t What\tis\tbackpropagation\tand\thow\tdoes\tit\twork?\tWhat\tis\tthe\tdifference\tbetween\tbackpropagation and\treverse-mode\tautodiff?\n\n8.\t Can\tyou\tlist\tall\tthe\thyperparameters\tyou\tcan\ttweak\tin\tan\tMLP?\tIf\tthe\tMLP\toverfits\tthe\ttraining\tdata, how\tcould\tyou\ttweak\tthese\thyperparameters\tto\ttry\tto\tsolve\tthe\tproblem?\n\n9.\t Train\ta\tdeep\tMLP\ton\tthe\tMNIST\tdataset\tand\tsee\tif\tyou\tcan\tget\tover\t98%\tprecision.\tJust\tlike\tin\tthe last\texercise\tof\tChapter\t9,\ttry\tadding\tall\tthe\tbells\tand\twhistles\t(i.e.,\tsave\tcheckpoints,\trestore\tthe last\tcheckpoint\tin\tcase\tof\tan\tinterruption,\tadd\tsummaries,\tplot\tlearning\tcurves\tusing\tTensorBoard, and\tso\ton).\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\nYou\tcan\tget\tthe\tbest\tof\tboth\tworlds\tby\tbeing\topen\tto\tbiological\tinspirations\twithout\tbeing\tafraid\tto\tcreate\tbiologically\tunrealistic\tmodels,\tas long\tas\tthey\twork\twell.\n\n“A\tLogical\tCalculus\tof\tIdeas\tImmanent\tin\tNervous\tActivity,”\tW.\tMcCulloch\tand\tW.\tPitts\t(1943).\n\nImage\tby\tBruce\tBlaus\t(Creative\tCommons\t3.0).\tReproduced\tfrom\thttps://en.wikipedia.org/wiki/Neuron.\n\nIn\tthe\tcontext\tof\tMachine\tLearning,\tthe\tphrase\t“neural\tnetworks”\tgenerally\trefers\tto\tANNs,\tnot\tBNNs.\n\nDrawing\tof\ta\tcortical\tlamination\tby\tS.\tRamon\ty\tCajal\t(public\tdomain).\tReproduced\tfrom\thttps://en.wikipedia.org/wiki/Cerebral_cortex.\n\nThe\tname\tPerceptron\tis\tsometimes\tused\tto\tmean\ta\ttiny\tnetwork\twith\ta\tsingle\tLTU.\n\nNote\tthat\tthis\tsolution\tis\tgenerally\tnot\tunique:\tin\tgeneral\twhen\tthe\tdata\tare\tlinearly\tseparable,\tthere\tis\tan\tinfinity\tof\thyperplanes\tthat\tcan separate\tthem.\n\n“Learning\tInternal\tRepresentations\tby\tError\tPropagation,”\tD.\tRumelhart,\tG.\tHinton,\tR.\tWilliams\t(1986).\n\nThis\talgorithm\twas\tactually\tinvented\tseveral\ttimes\tby\tvarious\tresearchers\tin\tdifferent\tfields,\tstarting\twith\tP.\tWerbos\tin\t1974.\n\nUsing\ta\ttruncated\tnormal\tdistribution\trather\tthan\ta\tregular\tnormal\tdistribution\tensures\tthat\tthere\twon’t\tbe\tany\tlarge\tweights,\twhich\tcould slow\tdown\ttraining.\n\nFor\texample,\tif\tyou\tset\tall\tthe\tweights\tto\t0,\tthen\tall\tneurons\twill\toutput\t0,\tand\tthe\terror\tgradient\twill\tbe\tthe\tsame\tfor\tall\tneurons\tin\ta\tgiven hidden\tlayer.\tThe\tGradient\tDescent\tstep\twill\tthen\tupdate\tall\tthe\tweights\tin\texactly\tthe\tsame\tway\tin\teach\tlayer,\tso\tthey\twill\tall\tremain equal.\tIn\tother\twords,\tdespite\thaving\thundreds\tof\tneurons\tper\tlayer,\tyour\tmodel\twill\tact\tas\tif\tthere\twere\tonly\tone\tneuron\tper\tlayer.\tIt\tis\tnot going\tto\tfly.\n\nBy\tVincent\tVanhoucke\tin\this\tDeep\tLearning\tclass\ton\tUdacity.com.\n\nA\tfew\textra\tANN\tarchitectures\tare\tpresented\tin\tAppendix\tE.\n\nChapter\t11.\tTraining\tDeep\tNeural\tNets\n\nIn\tChapter\t10\twe\tintroduced\tartificial\tneural\tnetworks\tand\ttrained\tour\tfirst\tdeep\tneural\tnetwork.\tBut\tit was\ta\tvery\tshallow\tDNN,\twith\tonly\ttwo\thidden\tlayers.\tWhat\tif\tyou\tneed\tto\ttackle\ta\tvery\tcomplex problem,\tsuch\tas\tdetecting\thundreds\tof\ttypes\tof\tobjects\tin\thigh-resolution\timages?\tYou\tmay\tneed\tto\ttrain a\tmuch\tdeeper\tDNN,\tperhaps\twith\t(say)\t10\tlayers,\teach\tcontaining\thundreds\tof\tneurons,\tconnected\tby hundreds\tof\tthousands\tof\tconnections.\tThis\twould\tnot\tbe\ta\twalk\tin\tthe\tpark:\n\nFirst,\tyou\twould\tbe\tfaced\twith\tthe\ttricky\tvanishing\tgradients\tproblem\t(or\tthe\trelated\texploding gradients\tproblem)\tthat\taffects\tdeep\tneural\tnetworks\tand\tmakes\tlower\tlayers\tvery\thard\tto\ttrain.\n\nSecond,\twith\tsuch\ta\tlarge\tnetwork,\ttraining\twould\tbe\textremely\tslow.\n\nThird,\ta\tmodel\twith\tmillions\tof\tparameters\twould\tseverely\trisk\toverfitting\tthe\ttraining\tset.\n\nIn\tthis\tchapter,\twe\twill\tgo\tthrough\teach\tof\tthese\tproblems\tin\tturn\tand\tpresent\ttechniques\tto\tsolve\tthem. We\twill\tstart\tby\texplaining\tthe\tvanishing\tgradients\tproblem\tand\texploring\tsome\tof\tthe\tmost\tpopular solutions\tto\tthis\tproblem.\tNext\twe\twill\tlook\tat\tvarious\toptimizers\tthat\tcan\tspeed\tup\ttraining\tlarge\tmodels tremendously\tcompared\tto\tplain\tGradient\tDescent.\tFinally,\twe\twill\tgo\tthrough\ta\tfew\tpopular regularization\ttechniques\tfor\tlarge\tneural\tnetworks.\n\nWith\tthese\ttools,\tyou\twill\tbe\table\tto\ttrain\tvery\tdeep\tnets:\twelcome\tto\tDeep\tLearning!\n\nVanishing/Exploding\tGradients\tProblems As\twe\tdiscussed\tin\tChapter\t10,\tthe\tbackpropagation\talgorithm\tworks\tby\tgoing\tfrom\tthe\toutput\tlayer\tto\tthe input\tlayer,\tpropagating\tthe\terror\tgradient\ton\tthe\tway.\tOnce\tthe\talgorithm\thas\tcomputed\tthe\tgradient\tof\tthe cost\tfunction\twith\tregards\tto\teach\tparameter\tin\tthe\tnetwork,\tit\tuses\tthese\tgradients\tto\tupdate\teach parameter\twith\ta\tGradient\tDescent\tstep.\n\nUnfortunately,\tgradients\toften\tget\tsmaller\tand\tsmaller\tas\tthe\talgorithm\tprogresses\tdown\tto\tthe\tlower layers.\tAs\ta\tresult,\tthe\tGradient\tDescent\tupdate\tleaves\tthe\tlower\tlayer\tconnection\tweights\tvirtually unchanged,\tand\ttraining\tnever\tconverges\tto\ta\tgood\tsolution.\tThis\tis\tcalled\tthe\tvanishing\tgradients problem.\tIn\tsome\tcases,\tthe\topposite\tcan\thappen:\tthe\tgradients\tcan\tgrow\tbigger\tand\tbigger,\tso\tmany layers\tget\tinsanely\tlarge\tweight\tupdates\tand\tthe\talgorithm\tdiverges.\tThis\tis\tthe\texploding\tgradients problem,\twhich\tis\tmostly\tencountered\tin\trecurrent\tneural\tnetworks\t(see\tChapter\t14).\tMore\tgenerally, deep\tneural\tnetworks\tsuffer\tfrom\tunstable\tgradients;\tdifferent\tlayers\tmay\tlearn\tat\twidely\tdifferent\tspeeds.\n\nAlthough\tthis\tunfortunate\tbehavior\thas\tbeen\tempirically\tobserved\tfor\tquite\ta\twhile\t(it\twas\tone\tof\tthe reasons\twhy\tdeep\tneural\tnetworks\twere\tmostly\tabandoned\tfor\ta\tlong\ttime),\tit\tis\tonly\taround\t2010\tthat significant\tprogress\twas\tmade\tin\tunderstanding\tit.\tA\tpaper\ttitled\t“Understanding\tthe\tDifficulty\tof\tTraining Deep\tFeedforward\tNeural\tNetworks”\tby\tXavier\tGlorot\tand\tYoshua\tBengio1\tfound\ta\tfew\tsuspects, including\tthe\tcombination\tof\tthe\tpopular\tlogistic\tsigmoid\tactivation\tfunction\tand\tthe\tweight\tinitialization technique\tthat\twas\tmost\tpopular\tat\tthe\ttime,\tnamely\trandom\tinitialization\tusing\ta\tnormal\tdistribution\twith a\tmean\tof\t0\tand\ta\tstandard\tdeviation\tof\t1.\tIn\tshort,\tthey\tshowed\tthat\twith\tthis\tactivation\tfunction\tand\tthis initialization\tscheme,\tthe\tvariance\tof\tthe\toutputs\tof\teach\tlayer\tis\tmuch\tgreater\tthan\tthe\tvariance\tof\tits inputs.\tGoing\tforward\tin\tthe\tnetwork,\tthe\tvariance\tkeeps\tincreasing\tafter\teach\tlayer\tuntil\tthe\tactivation function\tsaturates\tat\tthe\ttop\tlayers.\tThis\tis\tactually\tmade\tworse\tby\tthe\tfact\tthat\tthe\tlogistic\tfunction\thas\ta mean\tof\t0.5,\tnot\t0\t(the\thyperbolic\ttangent\tfunction\thas\ta\tmean\tof\t0\tand\tbehaves\tslightly\tbetter\tthan\tthe logistic\tfunction\tin\tdeep\tnetworks).\n\nLooking\tat\tthe\tlogistic\tactivation\tfunction\t(see\tFigure\t11-1),\tyou\tcan\tsee\tthat\twhen\tinputs\tbecome\tlarge (negative\tor\tpositive),\tthe\tfunction\tsaturates\tat\t0\tor\t1,\twith\ta\tderivative\textremely\tclose\tto\t0.\tThus\twhen backpropagation\tkicks\tin,\tit\thas\tvirtually\tno\tgradient\tto\tpropagate\tback\tthrough\tthe\tnetwork,\tand\twhat little\tgradient\texists\tkeeps\tgetting\tdiluted\tas\tbackpropagation\tprogresses\tdown\tthrough\tthe\ttop\tlayers,\tso there\tis\treally\tnothing\tleft\tfor\tthe\tlower\tlayers.\n\nFigure\t11-1.\tLogistic\tactivation\tfunction\tsaturation",
      "page_number": 317
    },
    {
      "number": 11,
      "title": "Training\tDeep\tNeural\tNets",
      "start_page": 343,
      "end_page": 392,
      "detection_method": "regex_chapter_title",
      "content": "Xavier\tand\tHe\tInitialization In\ttheir\tpaper,\tGlorot\tand\tBengio\tpropose\ta\tway\tto\tsignificantly\talleviate\tthis\tproblem.\tWe\tneed\tthe signal\tto\tflow\tproperly\tin\tboth\tdirections:\tin\tthe\tforward\tdirection\twhen\tmaking\tpredictions,\tand\tin\tthe reverse\tdirection\twhen\tbackpropagating\tgradients.\tWe\tdon’t\twant\tthe\tsignal\tto\tdie\tout,\tnor\tdo\twe\twant\tit to\texplode\tand\tsaturate.\tFor\tthe\tsignal\tto\tflow\tproperly,\tthe\tauthors\targue\tthat\twe\tneed\tthe\tvariance\tof\tthe outputs\tof\teach\tlayer\tto\tbe\tequal\tto\tthe\tvariance\tof\tits\tinputs,2\tand\twe\talso\tneed\tthe\tgradients\tto\thave equal\tvariance\tbefore\tand\tafter\tflowing\tthrough\ta\tlayer\tin\tthe\treverse\tdirection\t(please\tcheck\tout\tthe paper\tif\tyou\tare\tinterested\tin\tthe\tmathematical\tdetails).\tIt\tis\tactually\tnot\tpossible\tto\tguarantee\tboth\tunless the\tlayer\thas\tan\tequal\tnumber\tof\tinput\tand\toutput\tconnections,\tbut\tthey\tproposed\ta\tgood\tcompromise\tthat has\tproven\tto\twork\tvery\twell\tin\tpractice:\tthe\tconnection\tweights\tmust\tbe\tinitialized\trandomly\tas described\tin\tEquation\t11-1,\twhere\tninputs\tand\tnoutputs\tare\tthe\tnumber\tof\tinput\tand\toutput\tconnections\tfor the\tlayer\twhose\tweights\tare\tbeing\tinitialized\t(also\tcalled\tfan-in\tand\tfan-out).\tThis\tinitialization\tstrategy is\toften\tcalled\tXavier\tinitialization\t(after\tthe\tauthor’s\tfirst\tname),\tor\tsometimes\tGlorot\tinitialization.\n\nEquation\t11-1.\tXavier\tinitialization\t(when\tusing\tthe\tlogistic\tactivation\tfunction)\n\nWhen\tthe\tnumber\tof\tinput\tconnections\tis\troughly\tequal\tto\tthe\tnumber\tof\toutput\tconnections,\tyou\tget\n\nsimpler\tequations\t(e.g.,\t Chapter\t10.3\n\nor\n\n).\tWe\tused\tthis\tsimplified\tstrategy\tin\n\nUsing\tthe\tXavier\tinitialization\tstrategy\tcan\tspeed\tup\ttraining\tconsiderably,\tand\tit\tis\tone\tof\tthe\ttricks\tthat led\tto\tthe\tcurrent\tsuccess\tof\tDeep\tLearning.\tSome\trecent\tpapers4\thave\tprovided\tsimilar\tstrategies\tfor different\tactivation\tfunctions,\tas\tshown\tin\tTable\t11-1.\tThe\tinitialization\tstrategy\tfor\tthe\tReLU\tactivation function\t(and\tits\tvariants,\tincluding\tthe\tELU\tactivation\tdescribed\tshortly)\tis\tsometimes\tcalled\tHe initialization\t(after\tthe\tlast\tname\tof\tits\tauthor).\n\nTable\t11-1.\tInitialization\tparameters\tfor\teach\ttype\tof activation\tfunction\n\nActivation\tfunction\n\nUniform\tdistribution\t[–r,\tr] Normal\tdistribution\n\nLogistic\n\nHyperbolic\ttangent\n\nReLU\t(and\tits\tvariants)\n\nBy\tdefault,\tthe\ttf.layers.dense()\tfunction\t(introduced\tin\tChapter\t10)\tuses\tXavier\tinitialization\t(with a\tuniform\tdistribution).\tYou\tcan\tchange\tthis\tto\tHe\tinitialization\tby\tusing\tthe\n\nvariance_scaling_initializer()\tfunction\tlike\tthis:\n\nhe_init\t=\ttf.contrib.layers.variance_scaling_initializer() hidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=he_init,\tname=\"hidden1\")\n\nNOTE\n\nHe\tinitialization\tconsiders\tonly\tthe\tfan-in,\tnot\tthe\taverage\tbetween\tfan-in\tand\tfan-out\tlike\tin\tXavier\tinitialization.\tThis\tis\talso\tthe default\tfor\tthe\tvariance_scaling_initializer()\tfunction,\tbut\tyou\tcan\tchange\tthis\tby\tsetting\tthe\targument\tmode=\"FAN_AVG\".\n\nNonsaturating\tActivation\tFunctions One\tof\tthe\tinsights\tin\tthe\t2010\tpaper\tby\tGlorot\tand\tBengio\twas\tthat\tthe\tvanishing/exploding\tgradients problems\twere\tin\tpart\tdue\tto\ta\tpoor\tchoice\tof\tactivation\tfunction.\tUntil\tthen\tmost\tpeople\thad\tassumed that\tif\tMother\tNature\thad\tchosen\tto\tuse\troughly\tsigmoid\tactivation\tfunctions\tin\tbiological\tneurons,\tthey must\tbe\tan\texcellent\tchoice.\tBut\tit\tturns\tout\tthat\tother\tactivation\tfunctions\tbehave\tmuch\tbetter\tin\tdeep neural\tnetworks,\tin\tparticular\tthe\tReLU\tactivation\tfunction,\tmostly\tbecause\tit\tdoes\tnot\tsaturate\tfor positive\tvalues\t(and\talso\tbecause\tit\tis\tquite\tfast\tto\tcompute).\n\nUnfortunately,\tthe\tReLU\tactivation\tfunction\tis\tnot\tperfect.\tIt\tsuffers\tfrom\ta\tproblem\tknown\tas\tthe\tdying ReLUs:\tduring\ttraining,\tsome\tneurons\teffectively\tdie,\tmeaning\tthey\tstop\toutputting\tanything\tother\tthan\t0. In\tsome\tcases,\tyou\tmay\tfind\tthat\thalf\tof\tyour\tnetwork’s\tneurons\tare\tdead,\tespecially\tif\tyou\tused\ta\tlarge learning\trate.\tDuring\ttraining,\tif\ta\tneuron’s\tweights\tget\tupdated\tsuch\tthat\tthe\tweighted\tsum\tof\tthe\tneuron’s inputs\tis\tnegative,\tit\twill\tstart\toutputting\t0.\tWhen\tthis\thappen,\tthe\tneuron\tis\tunlikely\tto\tcome\tback\tto\tlife since\tthe\tgradient\tof\tthe\tReLU\tfunction\tis\t0\twhen\tits\tinput\tis\tnegative.\n\nTo\tsolve\tthis\tproblem,\tyou\tmay\twant\tto\tuse\ta\tvariant\tof\tthe\tReLU\tfunction,\tsuch\tas\tthe\tleaky\tReLU.\tThis function\tis\tdefined\tas\tLeakyReLUα(z)\t=\tmax(αz,\tz)\t(see\tFigure\t11-2).\tThe\thyperparameter\tα\tdefines\thow much\tthe\tfunction\t“leaks”:\tit\tis\tthe\tslope\tof\tthe\tfunction\tfor\tz\t<\t0,\tand\tis\ttypically\tset\tto\t0.01.\tThis\tsmall slope\tensures\tthat\tleaky\tReLUs\tnever\tdie;\tthey\tcan\tgo\tinto\ta\tlong\tcoma,\tbut\tthey\thave\ta\tchance\tto eventually\twake\tup.\tA\trecent\tpaper5\tcompared\tseveral\tvariants\tof\tthe\tReLU\tactivation\tfunction\tand\tone of\tits\tconclusions\twas\tthat\tthe\tleaky\tvariants\talways\toutperformed\tthe\tstrict\tReLU\tactivation\tfunction.\tIn fact,\tsetting\tα\t=\t0.2\t(huge\tleak)\tseemed\tto\tresult\tin\tbetter\tperformance\tthan\tα\t=\t0.01\t(small\tleak).\tThey also\tevaluated\tthe\trandomized\tleaky\tReLU\t(RReLU),\twhere\tα\tis\tpicked\trandomly\tin\ta\tgiven\trange\tduring training,\tand\tit\tis\tfixed\tto\tan\taverage\tvalue\tduring\ttesting.\tIt\talso\tperformed\tfairly\twell\tand\tseemed\tto\tact as\ta\tregularizer\t(reducing\tthe\trisk\tof\toverfitting\tthe\ttraining\tset).\tFinally,\tthey\talso\tevaluated\tthe parametric\tleaky\tReLU\t(PReLU),\twhere\tα\tis\tauthorized\tto\tbe\tlearned\tduring\ttraining\t(instead\tof\tbeing\ta hyperparameter,\tit\tbecomes\ta\tparameter\tthat\tcan\tbe\tmodified\tby\tbackpropagation\tlike\tany\tother parameter).\tThis\twas\treported\tto\tstrongly\toutperform\tReLU\ton\tlarge\timage\tdatasets,\tbut\ton\tsmaller datasets\tit\truns\tthe\trisk\tof\toverfitting\tthe\ttraining\tset.\n\nFigure\t11-2.\tLeaky\tReLU\n\nLast\tbut\tnot\tleast,\ta\t2015\tpaper\tby\tDjork-Arné\tClevert\tet\tal.6\tproposed\ta\tnew\tactivation\tfunction\tcalled the\texponential\tlinear\tunit\t(ELU)\tthat\toutperformed\tall\tthe\tReLU\tvariants\tin\ttheir\texperiments:\ttraining time\twas\treduced\tand\tthe\tneural\tnetwork\tperformed\tbetter\ton\tthe\ttest\tset.\tIt\tis\trepresented\tin\tFigure\t11-3, and\tEquation\t11-2\tshows\tits\tdefinition.\n\nEquation\t11-2.\tELU\tactivation\tfunction\n\nFigure\t11-3.\tELU\tactivation\tfunction\n\nIt\tlooks\ta\tlot\tlike\tthe\tReLU\tfunction,\twith\ta\tfew\tmajor\tdifferences:\n\nFirst\tit\ttakes\ton\tnegative\tvalues\twhen\tz\t<\t0,\twhich\tallows\tthe\tunit\tto\thave\tan\taverage\toutput\tcloser to\t0.\tThis\thelps\talleviate\tthe\tvanishing\tgradients\tproblem,\tas\tdiscussed\tearlier.\tThe\thyperparameter α\tdefines\tthe\tvalue\tthat\tthe\tELU\tfunction\tapproaches\twhen\tz\tis\ta\tlarge\tnegative\tnumber.\tIt\tis\tusually set\tto\t1,\tbut\tyou\tcan\ttweak\tit\tlike\tany\tother\thyperparameter\tif\tyou\twant.\n\nSecond,\tit\thas\ta\tnonzero\tgradient\tfor\tz\t<\t0,\twhich\tavoids\tthe\tdying\tunits\tissue.\n\nThird,\tthe\tfunction\tis\tsmooth\teverywhere,\tincluding\taround\tz\t=\t0,\twhich\thelps\tspeed\tup\tGradient Descent,\tsince\tit\tdoes\tnot\tbounce\tas\tmuch\tleft\tand\tright\tof\tz\t=\t0.\n\nThe\tmain\tdrawback\tof\tthe\tELU\tactivation\tfunction\tis\tthat\tit\tis\tslower\tto\tcompute\tthan\tthe\tReLU\tand\tits variants\t(due\tto\tthe\tuse\tof\tthe\texponential\tfunction),\tbut\tduring\ttraining\tthis\tis\tcompensated\tby\tthe\tfaster convergence\trate.\tHowever,\tat\ttest\ttime\tan\tELU\tnetwork\twill\tbe\tslower\tthan\ta\tReLU\tnetwork.\n\nTIP\n\nSo\twhich\tactivation\tfunction\tshould\tyou\tuse\tfor\tthe\thidden\tlayers\tof\tyour\tdeep\tneural\tnetworks?\tAlthough\tyour\tmileage\twill\tvary, in\tgeneral\tELU\t>\tleaky\tReLU\t(and\tits\tvariants)\t>\tReLU\t>\ttanh\t>\tlogistic.\tIf\tyou\tcare\ta\tlot\tabout\truntime\tperformance,\tthen\tyou may\tprefer\tleaky\tReLUs\tover\tELUs.\tIf\tyou\tdon’t\twant\tto\ttweak\tyet\tanother\thyperparameter,\tyou\tmay\tjust\tuse\tthe\tdefault\tα values\tsuggested\tearlier\t(0.01\tfor\tthe\tleaky\tReLU,\tand\t1\tfor\tELU).\tIf\tyou\thave\tspare\ttime\tand\tcomputing\tpower,\tyou\tcan\tuse cross-validation\tto\tevaluate\tother\tactivation\tfunctions,\tin\tparticular\tRReLU\tif\tyour\tnetwork\tis\toverfitting,\tor\tPReLU\tif\tyou\thave\ta huge\ttraining\tset.\n\nTensorFlow\toffers\tan\telu()\tfunction\tthat\tyou\tcan\tuse\tto\tbuild\tyour\tneural\tnetwork.\tSimply\tset\tthe activation\targument\twhen\tcalling\tthe\tdense()\tfunction,\tlike\tthis:\n\nhidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.elu,\tname=\"hidden1\")\n\nTensorFlow\tdoes\tnot\thave\ta\tpredefined\tfunction\tfor\tleaky\tReLUs,\tbut\tit\tis\teasy\tenough\tto\tdefine:\n\ndef\tleaky_relu(z,\tname=None): \t\t\t\treturn\ttf.maximum(0.01\t*\tz,\tz,\tname=name)\n\nhidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=leaky_relu,\tname=\"hidden1\")\n\nBatch\tNormalization Although\tusing\tHe\tinitialization\talong\twith\tELU\t(or\tany\tvariant\tof\tReLU)\tcan\tsignificantly\treduce\tthe vanishing/exploding\tgradients\tproblems\tat\tthe\tbeginning\tof\ttraining,\tit\tdoesn’t\tguarantee\tthat\tthey\twon’t come\tback\tduring\ttraining. In\ta\t2015\tpaper,7\tSergey\tIoffe\tand\tChristian\tSzegedy\tproposed\ta\ttechnique\tcalled\tBatch\tNormalization (BN)\tto\taddress\tthe\tvanishing/exploding\tgradients\tproblems,\tand\tmore\tgenerally\tthe\tproblem\tthat\tthe distribution\tof\teach\tlayer’s\tinputs\tchanges\tduring\ttraining,\tas\tthe\tparameters\tof\tthe\tprevious\tlayers\tchange (which\tthey\tcall\tthe\tInternal\tCovariate\tShift\tproblem).\n\nThe\ttechnique\tconsists\tof\tadding\tan\toperation\tin\tthe\tmodel\tjust\tbefore\tthe\tactivation\tfunction\tof\teach layer,\tsimply\tzero-centering\tand\tnormalizing\tthe\tinputs,\tthen\tscaling\tand\tshifting\tthe\tresult\tusing\ttwo\tnew parameters\tper\tlayer\t(one\tfor\tscaling,\tthe\tother\tfor\tshifting).\tIn\tother\twords,\tthis\toperation\tlets\tthe\tmodel learn\tthe\toptimal\tscale\tand\tmean\tof\tthe\tinputs\tfor\teach\tlayer.\n\nIn\torder\tto\tzero-center\tand\tnormalize\tthe\tinputs,\tthe\talgorithm\tneeds\tto\testimate\tthe\tinputs’\tmean\tand standard\tdeviation.\tIt\tdoes\tso\tby\tevaluating\tthe\tmean\tand\tstandard\tdeviation\tof\tthe\tinputs\tover\tthe\tcurrent mini-batch\t(hence\tthe\tname\t“Batch\tNormalization”).\tThe\twhole\toperation\tis\tsummarized\tin\tEquation\t11- 3.\n\nEquation\t11-3.\tBatch\tNormalization\talgorithm\n\nμB\tis\tthe\tempirical\tmean,\tevaluated\tover\tthe\twhole\tmini-batch\tB.\n\nσB\tis\tthe\tempirical\tstandard\tdeviation,\talso\tevaluated\tover\tthe\twhole\tmini-batch.\n\nmB\tis\tthe\tnumber\tof\tinstances\tin\tthe\tmini-batch.\n\n(i)\tis\tthe\tzero-centered\tand\tnormalized\tinput.\n\nγ\tis\tthe\tscaling\tparameter\tfor\tthe\tlayer.\n\nβ\tis\tthe\tshifting\tparameter\t(offset)\tfor\tthe\tlayer.\n\nϵ\tis\ta\ttiny\tnumber\tto\tavoid\tdivision\tby\tzero\t(typically\t10–5).\tThis\tis\tcalled\ta\tsmoothing\tterm.\n\nz(i)\tis\tthe\toutput\tof\tthe\tBN\toperation:\tit\tis\ta\tscaled\tand\tshifted\tversion\tof\tthe\tinputs.\n\nAt\ttest\ttime,\tthere\tis\tno\tmini-batch\tto\tcompute\tthe\tempirical\tmean\tand\tstandard\tdeviation,\tso\tinstead\tyou simply\tuse\tthe\twhole\ttraining\tset’s\tmean\tand\tstandard\tdeviation.\tThese\tare\ttypically\tefficiently\tcomputed during\ttraining\tusing\ta\tmoving\taverage.\tSo,\tin\ttotal,\tfour\tparameters\tare\tlearned\tfor\teach\tbatch- normalized\tlayer:\tγ\t(scale),\tβ\t(offset),\tμ\t(mean),\tand\tσ\t(standard\tdeviation).\n\nThe\tauthors\tdemonstrated\tthat\tthis\ttechnique\tconsiderably\timproved\tall\tthe\tdeep\tneural\tnetworks\tthey experimented\twith.\tThe\tvanishing\tgradients\tproblem\twas\tstrongly\treduced,\tto\tthe\tpoint\tthat\tthey\tcould\tuse saturating\tactivation\tfunctions\tsuch\tas\tthe\ttanh\tand\teven\tthe\tlogistic\tactivation\tfunction.\tThe\tnetworks were\talso\tmuch\tless\tsensitive\tto\tthe\tweight\tinitialization.\tThey\twere\table\tto\tuse\tmuch\tlarger\tlearning rates,\tsignificantly\tspeeding\tup\tthe\tlearning\tprocess.\tSpecifically,\tthey\tnote\tthat\t“Applied\tto\ta\tstate-of- the-art\timage\tclassification\tmodel,\tBatch\tNormalization\tachieves\tthe\tsame\taccuracy\twith\t14\ttimes\tfewer training\tsteps,\tand\tbeats\tthe\toriginal\tmodel\tby\ta\tsignificant\tmargin.\t[…]\tUsing\tan\tensemble\tof\tbatch- normalized\tnetworks,\twe\timprove\tupon\tthe\tbest\tpublished\tresult\ton\tImageNet\tclassification:\treaching 4.9%\ttop-5\tvalidation\terror\t(and\t4.8%\ttest\terror),\texceeding\tthe\taccuracy\tof\thuman\traters.”\tFinally,\tlike\ta gift\tthat\tkeeps\ton\tgiving,\tBatch\tNormalization\talso\tacts\tlike\ta\tregularizer,\treducing\tthe\tneed\tfor\tother regularization\ttechniques\t(such\tas\tdropout,\tdescribed\tlater\tin\tthe\tchapter).\n\nBatch\tNormalization\tdoes,\thowever,\tadd\tsome\tcomplexity\tto\tthe\tmodel\t(although\tit\tremoves\tthe\tneed\tfor normalizing\tthe\tinput\tdata\tsince\tthe\tfirst\thidden\tlayer\twill\ttake\tcare\tof\tthat,\tprovided\tit\tis\tbatch- normalized).\tMoreover,\tthere\tis\ta\truntime\tpenalty:\tthe\tneural\tnetwork\tmakes\tslower\tpredictions\tdue\tto the\textra\tcomputations\trequired\tat\teach\tlayer.\tSo\tif\tyou\tneed\tpredictions\tto\tbe\tlightning-fast,\tyou\tmay want\tto\tcheck\thow\twell\tplain\tELU\t+\tHe\tinitialization\tperform\tbefore\tplaying\twith\tBatch\tNormalization.\n\nNOTE\n\nYou\tmay\tfind\tthat\ttraining\tis\trather\tslow\tat\tfirst\twhile\tGradient\tDescent\tis\tsearching\tfor\tthe\toptimal\tscales\tand\toffsets\tfor\teach layer,\tbut\tit\taccelerates\tonce\tit\thas\tfound\treasonably\tgood\tvalues.\n\nImplementing\tBatch\tNormalization\twith\tTensorFlow\n\nTensorFlow\tprovides\ta\ttf.nn.batch_normalization()\tfunction\tthat\tsimply\tcenters\tand\tnormalizes\tthe inputs,\tbut\tyou\tmust\tcompute\tthe\tmean\tand\tstandard\tdeviation\tyourself\t(based\ton\tthe\tmini-batch\tdata\n\nduring\ttraining\tor\ton\tthe\tfull\tdataset\tduring\ttesting,\tas\tjust\tdiscussed)\tand\tpass\tthem\tas\tparameters\tto\tthis function,\tand\tyou\tmust\talso\thandle\tthe\tcreation\tof\tthe\tscaling\tand\toffset\tparameters\t(and\tpass\tthem\tto\tthis function).\tIt\tis\tdoable,\tbut\tnot\tthe\tmost\tconvenient\tapproach.\tInstead,\tyou\tshould\tuse\tthe tf.layers.batch_normalization()\tfunction,\twhich\thandles\tall\tthis\tfor\tyou,\tas\tin\tthe\tfollowing\tcode:\n\nimport\ttensorflow\tas\ttf\n\nn_inputs\t=\t28\t*\t28 n_hidden1\t=\t300 n_hidden2\t=\t100 n_outputs\t=\t10\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_inputs),\tname=\"X\")\n\ntraining\t=\ttf.placeholder_with_default(False,\tshape=(),\tname='training')\n\nhidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tname=\"hidden1\") bn1\t=\ttf.layers.batch_normalization(hidden1,\ttraining=training,\tmomentum=0.9) bn1_act\t=\ttf.nn.elu(bn1) hidden2\t=\ttf.layers.dense(bn1_act,\tn_hidden2,\tname=\"hidden2\") bn2\t=\ttf.layers.batch_normalization(hidden2,\ttraining=training,\tmomentum=0.9) bn2_act\t=\ttf.nn.elu(bn2) logits_before_bn\t=\ttf.layers.dense(bn2_act,\tn_outputs,\tname=\"outputs\") logits\t=\ttf.layers.batch_normalization(logits_before_bn,\ttraining=training, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=0.9)\n\nLet’s\twalk\tthrough\tthis\tcode.\tThe\tfirst\tlines\tare\tfairly\tself-explanatory,\tuntil\twe\tdefine\tthe\ttraining placeholder:\twe\twill\tset\tit\tto\tTrue\tduring\ttraining,\tbut\totherwise\tit\twill\tdefault\tto\tFalse.\tThis\twill\tbe used\tto\ttell\tthe\ttf.layers.batch_normalization()\tfunction\twhether\tit\tshould\tuse\tthe\tcurrent\tmini- batch’s\tmean\tand\tstandard\tdeviation\t(during\ttraining)\tor\tthe\twhole\ttraining\tset’s\tmean\tand\tstandard deviation\t(during\ttesting).\n\nThen,\twe\talternate\tfully\tconnected\tlayers\tand\tbatch\tnormalization\tlayers:\tthe\tfully\tconnected\tlayers\tare created\tusing\tthe\ttf.layers.dense()\tfunction,\tjust\tlike\twe\tdid\tin\tChapter\t10.\tNote\tthat\twe\tdon’t specify\tany\tactivation\tfunction\tfor\tthe\tfully\tconnected\tlayers\tbecause\twe\twant\tto\tapply\tthe\tactivation function\tafter\teach\tbatch\tnormalization\tlayer.8\tWe\tcreate\tthe\tbatch\tnormalization\tlayers\tusing\tthe tf.layers.batch_normalization()\tfunction,\tsetting\tits\ttraining\tand\tmomentum\tparameters.\tThe\tBN algorithm\tuses\texponential\tdecay\tto\tcompute\tthe\trunning\taverages,\twhich\tis\twhy\tit\trequires\tthe momentum\tparameter:\tgiven\ta\tnew\tvalue\tv,\tthe\trunning\taverage\n\nis\tupdated\tthrough\tthe\tequation:\n\nA\tgood\tmomentum\tvalue\tis\ttypically\tclose\tto\t1\t—\tfor\texample,\t0.9,\t0.99,\tor\t0.999\t(you\twant\tmore\t9s\tfor larger\tdatasets\tand\tsmaller\tmini-batches).\n\nYou\tmay\thave\tnoticed\tthat\tthe\tcode\tis\tquite\trepetitive,\twith\tthe\tsame\tbatch\tnormalization\tparameters appearing\tover\tand\tover\tagain.\tTo\tavoid\tthis\trepetition,\tyou\tcan\tuse\tthe\tpartial()\tfunction\tfrom\tthe functools\tmodule\t(part\tof\tPython’s\tstandard\tlibrary).\tIt\tcreates\ta\tthin\twrapper\taround\ta\tfunction\tand allows\tyou\tto\tdefine\tdefault\tvalues\tfor\tsome\tparameters.\tThe\tcreation\tof\tthe\tnetwork\tlayers\tin\tthe preceding\tcode\tcan\tbe\tmodified\tlike\tso:\n\nfrom\tfunctools\timport\tpartial\n\nmy_batch_norm_layer\t=\tpartial(tf.layers.batch_normalization,\n\ntraining=training,\tmomentum=0.9)\n\nhidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tname=\"hidden1\") bn1\t=\tmy_batch_norm_layer(hidden1) bn1_act\t=\ttf.nn.elu(bn1) hidden2\t=\ttf.layers.dense(bn1_act,\tn_hidden2,\tname=\"hidden2\") bn2\t=\tmy_batch_norm_layer(hidden2) bn2_act\t=\ttf.nn.elu(bn2) logits_before_bn\t=\ttf.layers.dense(bn2_act,\tn_outputs,\tname=\"outputs\") logits\t=\tmy_batch_norm_layer(logits_before_bn)\n\nIt\tmay\tnot\tlook\tmuch\tbetter\tthan\tbefore\tin\tthis\tsmall\texample,\tbut\tif\tyou\thave\t10\tlayers\tand\twant\tto\tuse the\tsame\tactivation\tfunction,\tinitializer,\tregularizer,\tand\tso\ton,\tin\tall\tlayers,\tthis\ttrick\twill\tmake\tyour\tcode much\tmore\treadable.\n\nThe\trest\tof\tthe\tconstruction\tphase\tis\tthe\tsame\tas\tin\tChapter\t10:\tdefine\tthe\tcost\tfunction,\tcreate\tan optimizer,\ttell\tit\tto\tminimize\tthe\tcost\tfunction,\tdefine\tthe\tevaluation\toperations,\tcreate\ta\tSaver,\tand\tso\ton.\n\nThe\texecution\tphase\tis\talso\tpretty\tmuch\tthe\tsame,\twith\ttwo\texceptions.\tFirst,\tduring\ttraining,\twhenever you\trun\tan\toperation\tthat\tdepends\ton\tthe\tbatch_normalization()\tlayer,\tyou\tneed\tto\tset\tthe\ttraining placeholder\tto\tTrue.\tSecond,\tthe\tbatch_normalization()\tfunction\tcreates\ta\tfew\toperations\tthat\tmust be\tevaluated\tat\teach\tstep\tduring\ttraining\tin\torder\tto\tupdate\tthe\tmoving\taverages\t(recall\tthat\tthese\tmoving averages\tare\tneeded\tto\tevaluate\tthe\ttraining\tset’s\tmean\tand\tstandard\tdeviation).\tThese\toperations\tare automatically\tadded\tto\tthe\tUPDATE_OPS\tcollection,\tso\tall\twe\tneed\tto\tdo\tis\tget\tthe\tlist\tof\toperations\tin\tthat collection\tand\trun\tthem\tat\teach\ttraining\titeration:\n\nextra_update_ops\t=\ttf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tfor\titeration\tin\trange(mnist.train.num_examples\t//\tbatch_size): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run([training_op,\textra_update_ops], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfeed_dict={training:\tTrue,\tX:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\taccuracy_val\t=\taccuracy.eval(feed_dict={X:\tmnist.test.images, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ty:\tmnist.test.labels}) \t\t\t\t\t\t\t\tprint(epoch,\t\"Test\taccuracy:\",\taccuracy_val)\n\nsave_path\t=\tsaver.save(sess,\t\"./my_model_final.ckpt\")\n\nThat’s\tall!\tIn\tthis\ttiny\texample\twith\tjust\ttwo\tlayers,\tit’s\tunlikely\tthat\tBatch\tNormalization\twill\thave\ta very\tpositive\timpact,\tbut\tfor\tdeeper\tnetworks\tit\tcan\tmake\ta\ttremendous\tdifference.\n\nGradient\tClipping A\tpopular\ttechnique\tto\tlessen\tthe\texploding\tgradients\tproblem\tis\tto\tsimply\tclip\tthe\tgradients\tduring backpropagation\tso\tthat\tthey\tnever\texceed\tsome\tthreshold\t(this\tis\tmostly\tuseful\tfor\trecurrent\tneural networks;\tsee\tChapter\t14).\tThis\tis\tcalled\tGradient\tClipping.9\tIn\tgeneral\tpeople\tnow\tprefer\tBatch Normalization,\tbut\tit’s\tstill\tuseful\tto\tknow\tabout\tGradient\tClipping\tand\thow\tto\timplement\tit.\n\nIn\tTensorFlow,\tthe\toptimizer’s\tminimize()\tfunction\ttakes\tcare\tof\tboth\tcomputing\tthe\tgradients\tand applying\tthem,\tso\tyou\tmust\tinstead\tcall\tthe\toptimizer’s\tcompute_gradients()\tmethod\tfirst,\tthen\tcreate an\toperation\tto\tclip\tthe\tgradients\tusing\tthe\tclip_by_value()\tfunction,\tand\tfinally\tcreate\tan\toperation\tto apply\tthe\tclipped\tgradients\tusing\tthe\toptimizer’s\tapply_gradients()\tmethod:\n\nthreshold\t=\t1.0 optimizer\t=\ttf.train.GradientDescentOptimizer(learning_rate) grads_and_vars\t=\toptimizer.compute_gradients(loss) capped_gvs\t=\t[(tf.clip_by_value(grad,\t-threshold,\tthreshold),\tvar) \t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tgrad,\tvar\tin\tgrads_and_vars] training_op\t=\toptimizer.apply_gradients(capped_gvs)\n\nYou\twould\tthen\trun\tthis\ttraining_op\tat\tevery\ttraining\tstep,\tas\tusual.\tIt\twill\tcompute\tthe\tgradients,\tclip them\tbetween\t–1.0\tand\t1.0,\tand\tapply\tthem.\tThe\tthreshold\tis\ta\thyperparameter\tyou\tcan\ttune.\n\nReusing\tPretrained\tLayers It\tis\tgenerally\tnot\ta\tgood\tidea\tto\ttrain\ta\tvery\tlarge\tDNN\tfrom\tscratch:\tinstead,\tyou\tshould\talways\ttry\tto find\tan\texisting\tneural\tnetwork\tthat\taccomplishes\ta\tsimilar\ttask\tto\tthe\tone\tyou\tare\ttrying\tto\ttackle,\tthen just\treuse\tthe\tlower\tlayers\tof\tthis\tnetwork:\tthis\tis\tcalled\ttransfer\tlearning.\tIt\twill\tnot\tonly\tspeed\tup training\tconsiderably,\tbut\twill\talso\trequire\tmuch\tless\ttraining\tdata.\n\nFor\texample,\tsuppose\tthat\tyou\thave\taccess\tto\ta\tDNN\tthat\twas\ttrained\tto\tclassify\tpictures\tinto\t100 different\tcategories,\tincluding\tanimals,\tplants,\tvehicles,\tand\teveryday\tobjects.\tYou\tnow\twant\tto\ttrain\ta DNN\tto\tclassify\tspecific\ttypes\tof\tvehicles.\tThese\ttasks\tare\tvery\tsimilar,\tso\tyou\tshould\ttry\tto\treuse\tparts of\tthe\tfirst\tnetwork\t(see\tFigure\t11-4).\n\nFigure\t11-4.\tReusing\tpretrained\tlayers\n\nNOTE\n\nIf\tthe\tinput\tpictures\tof\tyour\tnew\ttask\tdon’t\thave\tthe\tsame\tsize\tas\tthe\tones\tused\tin\tthe\toriginal\ttask,\tyou\twill\thave\tto\tadd\ta preprocessing\tstep\tto\tresize\tthem\tto\tthe\tsize\texpected\tby\tthe\toriginal\tmodel.\tMore\tgenerally,\ttransfer\tlearning\twill\tonly\twork\twell if\tthe\tinputs\thave\tsimilar\tlow-level\tfeatures.\n\nReusing\ta\tTensorFlow\tModel If\tthe\toriginal\tmodel\twas\ttrained\tusing\tTensorFlow,\tyou\tcan\tsimply\trestore\tit\tand\ttrain\tit\ton\tthe\tnew\ttask. As\twe\tdiscussed\tin\tChapter\t9,\tyou\tcan\tuse\tthe\timport_meta_graph()\tfunction\tto\timport\tthe\toperations into\tthe\tdefault\tgraph.\tThis\treturns\ta\tSaver\tthat\tyou\tcan\tlater\tuse\tto\tload\tthe\tmodel’s\tstate:\n\nsaver\t=\ttf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n\nYou\tmust\tthen\tget\ta\thandle\ton\tthe\toperations\tand\ttensors\tyou\twill\tneed\tfor\ttraining.\tFor\tthis,\tyou\tcan\tuse the\tgraph’s\tget_operation_by_name()\tand\tget_tensor_by_name()\tmethods.\tThe\tname\tof\ta\ttensor\tis the\tname\tof\tthe\toperation\tthat\toutputs\tit\tfollowed\tby\t:0\t(or\t:1\tif\tit\tis\tthe\tsecond\toutput,\t:2\tif\tit\tis\tthe third,\tand\tso\ton):\n\nX\t=\ttf.get_default_graph().get_tensor_by_name(\"X:0\") y\t=\ttf.get_default_graph().get_tensor_by_name(\"y:0\") accuracy\t=\ttf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\") training_op\t=\ttf.get_default_graph().get_operation_by_name(\"GradientDescent\")\n\nIf\tthe\tpretrained\tmodel\tis\tnot\twell\tdocumented,\tthen\tyou\twill\thave\tto\texplore\tthe\tgraph\tto\tfind\tthe\tnames of\tthe\toperations\tyou\twill\tneed.\tIn\tthis\tcase,\tyou\tcan\teither\texplore\tthe\tgraph\tusing\tTensorBoard\t(for\tthis you\tmust\tfirst\texport\tthe\tgraph\tusing\ta\tFileWriter,\tas\tdiscussed\tin\tChapter\t9),\tor\tyou\tcan\tuse\tthe\tgraph’s get_operations()\tmethod\tto\tlist\tall\tthe\toperations:\n\nfor\top\tin\ttf.get_default_graph().get_operations(): \t\t\t\tprint(op.name)\n\nIf\tyou\tare\tthe\tauthor\tof\tthe\toriginal\tmodel,\tyou\tcould\tmake\tthings\teasier\tfor\tpeople\twho\twill\treuse\tyour model\tby\tgiving\toperations\tvery\tclear\tnames\tand\tdocumenting\tthem.\tAnother\tapproach\tis\tto\tcreate\ta collection\tcontaining\tall\tthe\timportant\toperations\tthat\tpeople\twill\twant\tto\tget\ta\thandle\ton:\n\nfor\top\tin\t(X,\ty,\taccuracy,\ttraining_op): \t\t\t\ttf.add_to_collection(\"my_important_ops\",\top)\n\nThis\tway\tpeople\twho\treuse\tyour\tmodel\twill\tbe\table\tto\tsimply\twrite:\n\nX,\ty,\taccuracy,\ttraining_op\t=\ttf.get_collection(\"my_important_ops\")\n\nYou\tcan\tthen\trestore\tthe\tmodel’s\tstate\tusing\tthe\tSaver\tand\tcontinue\ttraining\tusing\tyour\town\tdata:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsaver.restore(sess,\t\"./my_model_final.ckpt\") \t\t\t\t[...]\t#\ttrain\tthe\tmodel\ton\tyour\town\tdata\n\nAlternatively,\tif\tyou\thave\taccess\tto\tthe\tPython\tcode\tthat\tbuilt\tthe\toriginal\tgraph,\tyou\tcan\tuse\tit\tinstead\tof import_meta_graph().\n\nIn\tgeneral,\tyou\twill\twant\tto\treuse\tonly\tpart\tof\tthe\toriginal\tmodel,\ttypically\tthe\tlower\tlayers.\tIf\tyou\tuse import_meta_graph()\tto\trestore\tthe\tgraph,\tit\twill\tload\tthe\tentire\toriginal\tgraph,\tbut\tnothing\tprevents\n\nyou\tfrom\tjust\tignoring\tthe\tlayers\tyou\tdo\tnot\tcare\tabout.\tFor\texample,\tas\tshown\tin\tFigure\t11-4,\tyou\tcould build\tnew\tlayers\t(e.g.,\tone\thidden\tlayer\tand\tone\toutput\tlayer)\ton\ttop\tof\ta\tpretrained\tlayer\t(e.g.,\tpretrained hidden\tlayer\t3).\tYou\twould\talso\tneed\tto\tcompute\tthe\tloss\tfor\tthis\tnew\toutput,\tand\tcreate\tan\toptimizer\tto minimize\tthat\tloss.\n\nIf\tyou\thave\taccess\tto\tthe\tpretrained\tgraph’s\tPython\tcode,\tyou\tcan\tjust\treuse\tthe\tparts\tyou\tneed\tand\tchop out\tthe\trest.\tHowever,\tin\tthis\tcase\tyou\tneed\ta\tSaver\tto\trestore\tthe\tpretrained\tmodel\t(specifying\twhich variables\tyou\twant\tto\trestore;\totherwise,\tTensorFlow\twill\tcomplain\tthat\tthe\tgraphs\tdo\tnot\tmatch),\tand another\tSaver\tto\tsave\tthe\tnew\tmodel.\tFor\texample,\tthe\tfollowing\tcode\trestores\tonly\thidden\tlayers\t1,\t2, and\t3:\n\n[...]\t#\tbuild\tthe\tnew\tmodel\twith\tthe\tsame\thidden\tlayers\t1-3\tas\tbefore\n\nreuse_vars\t=\ttf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscope=\"hidden[123]\")\t#\tregular\texpression reuse_vars_dict\t=\tdict([(var.op.name,\tvar)\tfor\tvar\tin\treuse_vars]) restore_saver\t=\ttf.train.Saver(reuse_vars_dict)\t#\tto\trestore\tlayers\t1-3\n\ninit\t=\ttf.global_variables_initializer()\t#\tto\tinit\tall\tvariables,\told\tand\tnew saver\t=\ttf.train.Saver()\t#\tto\tsave\tthe\tnew\tmodel\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\trestore_saver.restore(sess,\t\"./my_model_final.ckpt\") \t\t\t\t[...]\t#\ttrain\tthe\tmodel \t\t\t\tsave_path\t=\tsaver.save(sess,\t\"./my_new_model_final.ckpt\")\n\nFirst\twe\tbuild\tthe\tnew\tmodel,\tmaking\tsure\tto\tcopy\tthe\toriginal\tmodel’s\thidden\tlayers\t1\tto\t3.\tThen\twe\tget the\tlist\tof\tall\tvariables\tin\thidden\tlayers\t1\tto\t3,\tusing\tthe\tregular\texpression\t\"hidden[123]\".\tNext,\twe create\ta\tdictionary\tthat\tmaps\tthe\tname\tof\teach\tvariable\tin\tthe\toriginal\tmodel\tto\tits\tname\tin\tthe\tnew\tmodel (generally\tyou\twant\tto\tkeep\tthe\texact\tsame\tnames).\tThen\twe\tcreate\ta\tSaver\tthat\twill\trestore\tonly\tthese variables.\tWe\talso\tcreate\tan\toperation\tto\tinitialize\tall\tthe\tvariables\t(old\tand\tnew)\tand\ta\tsecond\tSaver\tto save\tthe\tentire\tnew\tmodel,\tnot\tjust\tlayers\t1\tto\t3.\tWe\tthen\tstart\ta\tsession\tand\tinitialize\tall\tvariables\tin\tthe model,\tthen\trestore\tthe\tvariable\tvalues\tfrom\tthe\toriginal\tmodel’s\tlayers\t1\tto\t3.\tFinally,\twe\ttrain\tthe\tmodel on\tthe\tnew\ttask\tand\tsave\tit.\n\nTIP\n\nThe\tmore\tsimilar\tthe\ttasks\tare,\tthe\tmore\tlayers\tyou\twant\tto\treuse\t(starting\twith\tthe\tlower\tlayers).\tFor\tvery\tsimilar\ttasks,\tyou\tcan try\tkeeping\tall\tthe\thidden\tlayers\tand\tjust\treplace\tthe\toutput\tlayer.\n\nReusing\tModels\tfrom\tOther\tFrameworks If\tthe\tmodel\twas\ttrained\tusing\tanother\tframework,\tyou\twill\tneed\tto\tload\tthe\tmodel\tparameters\tmanually (e.g.,\tusing\tTheano\tcode\tif\tit\twas\ttrained\twith\tTheano),\tthen\tassign\tthem\tto\tthe\tappropriate\tvariables. This\tcan\tbe\tquite\ttedious.\tFor\texample,\tthe\tfollowing\tcode\tshows\thow\tyou\twould\tcopy\tthe\tweight\tand biases\tfrom\tthe\tfirst\thidden\tlayer\tof\ta\tmodel\ttrained\tusing\tanother\tframework:\n\noriginal_w\t=\t[...]\t#\tLoad\tthe\tweights\tfrom\tthe\tother\tframework original_b\t=\t[...]\t#\tLoad\tthe\tbiases\tfrom\tthe\tother\tframework\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_inputs),\tname=\"X\") hidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.relu,\tname=\"hidden1\") [...]\t#\tBuild\tthe\trest\tof\tthe\tmodel\n\n#\tGet\ta\thandle\ton\tthe\tassignment\tnodes\tfor\tthe\thidden1\tvariables graph\t=\ttf.get_default_graph() assign_kernel\t=\tgraph.get_operation_by_name(\"hidden1/kernel/Assign\") assign_bias\t=\tgraph.get_operation_by_name(\"hidden1/bias/Assign\") init_kernel\t=\tassign_kernel.inputs[1] init_bias\t=\tassign_bias.inputs[1]\n\ninit\t=\ttf.global_variables_initializer()\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsess.run(init,\tfeed_dict={init_kernel:\toriginal_w,\tinit_bias:\toriginal_b}) \t\t\t\t#\t[...]\tTrain\tthe\tmodel\ton\tyour\tnew\ttask\n\nIn\tthis\timplementation,\twe\tfirst\tload\tthe\tpretrained\tmodel\tusing\tthe\tother\tframework\t(not\tshown\there), and\twe\textract\tfrom\tit\tthe\tmodel\tparameters\twe\twant\tto\treuse.\tNext,\twe\tbuild\tour\tTensorFlow\tmodel\tas usual.\tThen\tcomes\tthe\ttricky\tpart:\tevery\tTensorFlow\tvariable\thas\tan\tassociated\tassignment\toperation\tthat is\tused\tto\tinitialize\tit.\tWe\tstart\tby\tgetting\ta\thandle\ton\tthese\tassignment\toperations\t(they\thave\tthe\tsame name\tas\tthe\tvariable,\tplus\t\"/Assign\").\tWe\talso\tget\ta\thandle\ton\teach\tassignment\toperation’s\tsecond input:\tin\tthe\tcase\tof\tan\tassignment\toperation,\tthe\tsecond\tinput\tcorresponds\tto\tthe\tvalue\tthat\twill\tbe assigned\tto\tthe\tvariable,\tso\tin\tthis\tcase\tit\tis\tthe\tvariable’s\tinitialization\tvalue.\tOnce\twe\tstart\tthe\tsession, we\trun\tthe\tusual\tinitialization\toperation,\tbut\tthis\ttime\twe\tfeed\tit\tthe\tvalues\twe\twant\tfor\tthe\tvariables\twe want\tto\treuse.\tAlternatively,\twe\tcould\thave\tcreated\tnew\tassignment\toperations\tand\tplaceholders,\tand used\tthem\tto\tset\tthe\tvalues\tof\tthe\tvariables\tafter\tinitialization.\tBut\twhy\tcreate\tnew\tnodes\tin\tthe\tgraph when\teverything\twe\tneed\tis\talready\tthere?\n\nFreezing\tthe\tLower\tLayers It\tis\tlikely\tthat\tthe\tlower\tlayers\tof\tthe\tfirst\tDNN\thave\tlearned\tto\tdetect\tlow-level\tfeatures\tin\tpictures\tthat will\tbe\tuseful\tacross\tboth\timage\tclassification\ttasks,\tso\tyou\tcan\tjust\treuse\tthese\tlayers\tas\tthey\tare.\tIt\tis generally\ta\tgood\tidea\tto\t“freeze”\ttheir\tweights\twhen\ttraining\tthe\tnew\tDNN:\tif\tthe\tlower-layer\tweights are\tfixed,\tthen\tthe\thigher-layer\tweights\twill\tbe\teasier\tto\ttrain\t(because\tthey\twon’t\thave\tto\tlearn\ta\tmoving target).\tTo\tfreeze\tthe\tlower\tlayers\tduring\ttraining,\tone\tsolution\tis\tto\tgive\tthe\toptimizer\tthe\tlist\tof variables\tto\ttrain,\texcluding\tthe\tvariables\tfrom\tthe\tlower\tlayers:\n\ntrain_vars\t=\ttf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscope=\"hidden[34]|outputs\") training_op\t=\toptimizer.minimize(loss,\tvar_list=train_vars)\n\nThe\tfirst\tline\tgets\tthe\tlist\tof\tall\ttrainable\tvariables\tin\thidden\tlayers\t3\tand\t4\tand\tin\tthe\toutput\tlayer.\tThis leaves\tout\tthe\tvariables\tin\tthe\thidden\tlayers\t1\tand\t2.\tNext\twe\tprovide\tthis\trestricted\tlist\tof\ttrainable variables\tto\tthe\toptimizer’s\tminimize()\tfunction.\tTa-da!\tLayers\t1\tand\t2\tare\tnow\tfrozen:\tthey\twill\tnot budge\tduring\ttraining\t(these\tare\toften\tcalled\tfrozen\tlayers).\n\nAnother\toption\tis\tto\tadd\ta\tstop_gradient()\tlayer\tin\tthe\tgraph.\tAny\tlayer\tbelow\tit\twill\tbe\tfrozen:\n\nwith\ttf.name_scope(\"dnn\"): \t\t\t\thidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden1\")\t#\treused\tfrozen \t\t\t\thidden2\t=\ttf.layers.dense(hidden1,\tn_hidden2,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden2\")\t#\treused\tfrozen \t\t\t\thidden2_stop\t=\ttf.stop_gradient(hidden2) \t\t\t\thidden3\t=\ttf.layers.dense(hidden2_stop,\tn_hidden3,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden3\")\t#\treused,\tnot\tfrozen \t\t\t\thidden4\t=\ttf.layers.dense(hidden3,\tn_hidden4,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden4\")\t#\tnew! \t\t\t\tlogits\t=\ttf.layers.dense(hidden4,\tn_outputs,\tname=\"outputs\")\t#\tnew!\n\nCaching\tthe\tFrozen\tLayers Since\tthe\tfrozen\tlayers\twon’t\tchange,\tit\tis\tpossible\tto\tcache\tthe\toutput\tof\tthe\ttopmost\tfrozen\tlayer\tfor\teach training\tinstance.\tSince\ttraining\tgoes\tthrough\tthe\twhole\tdataset\tmany\ttimes,\tthis\twill\tgive\tyou\ta\thuge speed\tboost\tas\tyou\twill\tonly\tneed\tto\tgo\tthrough\tthe\tfrozen\tlayers\tonce\tper\ttraining\tinstance\t(instead\tof once\tper\tepoch).\tFor\texample,\tyou\tcould\tfirst\trun\tthe\twhole\ttraining\tset\tthrough\tthe\tlower\tlayers (assuming\tyou\thave\tenough\tRAM),\tthen\tduring\ttraining,\tinstead\tof\tbuilding\tbatches\tof\ttraining\tinstances, you\twould\tbuild\tbatches\tof\toutputs\tfrom\thidden\tlayer\t2\tand\tfeed\tthem\tto\tthe\ttraining\toperation:\n\nimport\tnumpy\tas\tnp\n\nn_batches\t=\tmnist.train.num_examples\t//\tbatch_size\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\trestore_saver.restore(sess,\t\"./my_model_final.ckpt\")\n\nh2_cache\t=\tsess.run(hidden2,\tfeed_dict={X:\tmnist.train.images})\n\nfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tshuffled_idx\t=\tnp.random.permutation(mnist.train.num_examples) \t\t\t\t\t\t\t\thidden2_batches\t=\tnp.array_split(h2_cache[shuffled_idx],\tn_batches) \t\t\t\t\t\t\t\ty_batches\t=\tnp.array_split(mnist.train.labels[shuffled_idx],\tn_batches) \t\t\t\t\t\t\t\tfor\thidden2_batch,\ty_batch\tin\tzip(hidden2_batches,\ty_batches): \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={hidden2:hidden2_batch,\ty:y_batch})\n\nsave_path\t=\tsaver.save(sess,\t\"./my_new_model_final.ckpt\")\n\nThe\tlast\tline\tof\tthe\ttraining\tloop\truns\tthe\ttraining\toperation\tdefined\tearlier\t(which\tdoes\tnot\ttouch\tlayers\t1 and\t2),\tand\tfeeds\tit\ta\tbatch\tof\toutputs\tfrom\tthe\tsecond\thidden\tlayer\t(as\twell\tas\tthe\ttargets\tfor\tthat\tbatch). Since\twe\tgive\tTensorFlow\tthe\toutput\tof\thidden\tlayer\t2,\tit\tdoes\tnot\ttry\tto\tevaluate\tit\t(or\tany\tnode\tit depends\ton).\n\nTweaking,\tDropping,\tor\tReplacing\tthe\tUpper\tLayers The\toutput\tlayer\tof\tthe\toriginal\tmodel\tshould\tusually\tbe\treplaced\tsince\tit\tis\tmost\tlikely\tnot\tuseful\tat\tall for\tthe\tnew\ttask,\tand\tit\tmay\tnot\teven\thave\tthe\tright\tnumber\tof\toutputs\tfor\tthe\tnew\ttask.\n\nSimilarly,\tthe\tupper\thidden\tlayers\tof\tthe\toriginal\tmodel\tare\tless\tlikely\tto\tbe\tas\tuseful\tas\tthe\tlower\tlayers, since\tthe\thigh-level\tfeatures\tthat\tare\tmost\tuseful\tfor\tthe\tnew\ttask\tmay\tdiffer\tsignificantly\tfrom\tthe\tones that\twere\tmost\tuseful\tfor\tthe\toriginal\ttask.\tYou\twant\tto\tfind\tthe\tright\tnumber\tof\tlayers\tto\treuse.\n\nTry\tfreezing\tall\tthe\tcopied\tlayers\tfirst,\tthen\ttrain\tyour\tmodel\tand\tsee\thow\tit\tperforms.\tThen\ttry\tunfreezing one\tor\ttwo\tof\tthe\ttop\thidden\tlayers\tto\tlet\tbackpropagation\ttweak\tthem\tand\tsee\tif\tperformance\timproves. The\tmore\ttraining\tdata\tyou\thave,\tthe\tmore\tlayers\tyou\tcan\tunfreeze.\n\nIf\tyou\tstill\tcannot\tget\tgood\tperformance,\tand\tyou\thave\tlittle\ttraining\tdata,\ttry\tdropping\tthe\ttop\thidden layer(s)\tand\tfreeze\tall\tremaining\thidden\tlayers\tagain.\tYou\tcan\titerate\tuntil\tyou\tfind\tthe\tright\tnumber\tof layers\tto\treuse.\tIf\tyou\thave\tplenty\tof\ttraining\tdata,\tyou\tmay\ttry\treplacing\tthe\ttop\thidden\tlayers\tinstead\tof dropping\tthem,\tand\teven\tadd\tmore\thidden\tlayers.\n\nModel\tZoos Where\tcan\tyou\tfind\ta\tneural\tnetwork\ttrained\tfor\ta\ttask\tsimilar\tto\tthe\tone\tyou\twant\tto\ttackle?\tThe\tfirst place\tto\tlook\tis\tobviously\tin\tyour\town\tcatalog\tof\tmodels.\tThis\tis\tone\tgood\treason\tto\tsave\tall\tyour\tmodels and\torganize\tthem\tso\tyou\tcan\tretrieve\tthem\tlater\teasily.\tAnother\toption\tis\tto\tsearch\tin\ta\tmodel\tzoo.\tMany people\ttrain\tMachine\tLearning\tmodels\tfor\tvarious\ttasks\tand\tkindly\trelease\ttheir\tpretrained\tmodels\tto\tthe public.\n\nTensorFlow\thas\tits\town\tmodel\tzoo\tavailable\tat\thttps://github.com/tensorflow/models.\tIn\tparticular,\tit contains\tmost\tof\tthe\tstate-of-the-art\timage\tclassification\tnets\tsuch\tas\tVGG,\tInception,\tand\tResNet\t(see Chapter\t13,\tand\tcheck\tout\tthe\tmodels/slim\tdirectory),\tincluding\tthe\tcode,\tthe\tpretrained\tmodels,\tand\ttools to\tdownload\tpopular\timage\tdatasets.\n\nAnother\tpopular\tmodel\tzoo\tis\tCaffe’s\tModel\tZoo.\tIt\talso\tcontains\tmany\tcomputer\tvision\tmodels\t(e.g., LeNet,\tAlexNet,\tZFNet,\tGoogLeNet,\tVGGNet,\tinception)\ttrained\ton\tvarious\tdatasets\t(e.g.,\tImageNet, Places\tDatabase,\tCIFAR10,\tetc.).\tSaumitro\tDasgupta\twrote\ta\tconverter,\twhich\tis\tavailable\tat https://github.com/ethereon/caffe-tensorflow.\n\nUnsupervised\tPretraining Suppose\tyou\twant\tto\ttackle\ta\tcomplex\ttask\tfor\twhich\tyou\tdon’t\thave\tmuch\tlabeled\ttraining\tdata,\tbut unfortunately\tyou\tcannot\tfind\ta\tmodel\ttrained\ton\ta\tsimilar\ttask.\tDon’t\tlose\tall\thope!\tFirst,\tyou\tshould\tof course\ttry\tto\tgather\tmore\tlabeled\ttraining\tdata,\tbut\tif\tthis\tis\ttoo\thard\tor\ttoo\texpensive,\tyou\tmay\tstill\tbe able\tto\tperform\tunsupervised\tpretraining\t(see\tFigure\t11-5).\tThat\tis,\tif\tyou\thave\tplenty\tof\tunlabeled training\tdata,\tyou\tcan\ttry\tto\ttrain\tthe\tlayers\tone\tby\tone,\tstarting\twith\tthe\tlowest\tlayer\tand\tthen\tgoing\tup, using\tan\tunsupervised\tfeature\tdetector\talgorithm\tsuch\tas\tRestricted\tBoltzmann\tMachines\t(RBMs;\tsee Appendix\tE)\tor\tautoencoders\t(see\tChapter\t15).\tEach\tlayer\tis\ttrained\ton\tthe\toutput\tof\tthe\tpreviously trained\tlayers\t(all\tlayers\texcept\tthe\tone\tbeing\ttrained\tare\tfrozen).\tOnce\tall\tlayers\thave\tbeen\ttrained\tthis way,\tyou\tcan\tfine-tune\tthe\tnetwork\tusing\tsupervised\tlearning\t(i.e.,\twith\tbackpropagation).\n\nThis\tis\ta\trather\tlong\tand\ttedious\tprocess,\tbut\tit\toften\tworks\twell;\tin\tfact,\tit\tis\tthis\ttechnique\tthat\tGeoffrey Hinton\tand\this\tteam\tused\tin\t2006\tand\twhich\tled\tto\tthe\trevival\tof\tneural\tnetworks\tand\tthe\tsuccess\tof\tDeep Learning.\tUntil\t2010,\tunsupervised\tpretraining\t(typically\tusing\tRBMs)\twas\tthe\tnorm\tfor\tdeep\tnets,\tand\tit was\tonly\tafter\tthe\tvanishing\tgradients\tproblem\twas\talleviated\tthat\tit\tbecame\tmuch\tmore\tcommon\tto\ttrain DNNs\tpurely\tusing\tbackpropagation.\tHowever,\tunsupervised\tpretraining\t(today\ttypically\tusing autoencoders\trather\tthan\tRBMs)\tis\tstill\ta\tgood\toption\twhen\tyou\thave\ta\tcomplex\ttask\tto\tsolve,\tno\tsimilar model\tyou\tcan\treuse,\tand\tlittle\tlabeled\ttraining\tdata\tbut\tplenty\tof\tunlabeled\ttraining\tdata.10\n\nFigure\t11-5.\tUnsupervised\tpretraining\n\nPretraining\ton\tan\tAuxiliary\tTask One\tlast\toption\tis\tto\ttrain\ta\tfirst\tneural\tnetwork\ton\tan\tauxiliary\ttask\tfor\twhich\tyou\tcan\teasily\tobtain\tor generate\tlabeled\ttraining\tdata,\tthen\treuse\tthe\tlower\tlayers\tof\tthat\tnetwork\tfor\tyour\tactual\ttask.\tThe\tfirst neural\tnetwork’s\tlower\tlayers\twill\tlearn\tfeature\tdetectors\tthat\twill\tlikely\tbe\treusable\tby\tthe\tsecond neural\tnetwork.\n\nFor\texample,\tif\tyou\twant\tto\tbuild\ta\tsystem\tto\trecognize\tfaces,\tyou\tmay\tonly\thave\ta\tfew\tpictures\tof\teach individual\t—\tclearly\tnot\tenough\tto\ttrain\ta\tgood\tclassifier.\tGathering\thundreds\tof\tpictures\tof\teach\tperson would\tnot\tbe\tpractical.\tHowever,\tyou\tcould\tgather\ta\tlot\tof\tpictures\tof\trandom\tpeople\ton\tthe\tinternet\tand train\ta\tfirst\tneural\tnetwork\tto\tdetect\twhether\tor\tnot\ttwo\tdifferent\tpictures\tfeature\tthe\tsame\tperson.\tSuch\ta network\twould\tlearn\tgood\tfeature\tdetectors\tfor\tfaces,\tso\treusing\tits\tlower\tlayers\twould\tallow\tyou\tto train\ta\tgood\tface\tclassifier\tusing\tlittle\ttraining\tdata.\n\nIt\tis\toften\trather\tcheap\tto\tgather\tunlabeled\ttraining\texamples,\tbut\tquite\texpensive\tto\tlabel\tthem.\tIn\tthis situation,\ta\tcommon\ttechnique\tis\tto\tlabel\tall\tyour\ttraining\texamples\tas\t“good,”\tthen\tgenerate\tmany\tnew training\tinstances\tby\tcorrupting\tthe\tgood\tones,\tand\tlabel\tthese\tcorrupted\tinstances\tas\t“bad.”\tThen\tyou\tcan train\ta\tfirst\tneural\tnetwork\tto\tclassify\tinstances\tas\tgood\tor\tbad.\tFor\texample,\tyou\tcould\tdownload millions\tof\tsentences,\tlabel\tthem\tas\t“good,”\tthen\trandomly\tchange\ta\tword\tin\teach\tsentence\tand\tlabel\tthe resulting\tsentences\tas\t“bad.”\tIf\ta\tneural\tnetwork\tcan\ttell\tthat\t“The\tdog\tsleeps”\tis\ta\tgood\tsentence\tbut “The\tdog\tthey”\tis\tbad,\tit\tprobably\tknows\tquite\ta\tlot\tabout\tlanguage.\tReusing\tits\tlower\tlayers\twill\tlikely help\tin\tmany\tlanguage\tprocessing\ttasks.\n\nAnother\tapproach\tis\tto\ttrain\ta\tfirst\tnetwork\tto\toutput\ta\tscore\tfor\teach\ttraining\tinstance,\tand\tuse\ta\tcost function\tthat\tensures\tthat\ta\tgood\tinstance’s\tscore\tis\tgreater\tthan\ta\tbad\tinstance’s\tscore\tby\tat\tleast\tsome margin.\tThis\tis\tcalled\tmax\tmargin\tlearning.\n\nFaster\tOptimizers Training\ta\tvery\tlarge\tdeep\tneural\tnetwork\tcan\tbe\tpainfully\tslow.\tSo\tfar\twe\thave\tseen\tfour\tways\tto\tspeed up\ttraining\t(and\treach\ta\tbetter\tsolution):\tapplying\ta\tgood\tinitialization\tstrategy\tfor\tthe\tconnection\tweights, using\ta\tgood\tactivation\tfunction,\tusing\tBatch\tNormalization,\tand\treusing\tparts\tof\ta\tpretrained\tnetwork. Another\thuge\tspeed\tboost\tcomes\tfrom\tusing\ta\tfaster\toptimizer\tthan\tthe\tregular\tGradient\tDescent optimizer.\tIn\tthis\tsection\twe\twill\tpresent\tthe\tmost\tpopular\tones:\tMomentum\toptimization,\tNesterov Accelerated\tGradient,\tAdaGrad,\tRMSProp,\tand\tfinally\tAdam\toptimization.\n\nMomentum\tOptimization Imagine\ta\tbowling\tball\trolling\tdown\ta\tgentle\tslope\ton\ta\tsmooth\tsurface:\tit\twill\tstart\tout\tslowly,\tbut\tit will\tquickly\tpick\tup\tmomentum\tuntil\tit\teventually\treaches\tterminal\tvelocity\t(if\tthere\tis\tsome\tfriction\tor air\tresistance).\tThis\tis\tthe\tvery\tsimple\tidea\tbehind\tMomentum\toptimization,\tproposed\tby\tBoris\tPolyak in\t1964.11\tIn\tcontrast,\tregular\tGradient\tDescent\twill\tsimply\ttake\tsmall\tregular\tsteps\tdown\tthe\tslope,\tso\tit will\ttake\tmuch\tmore\ttime\tto\treach\tthe\tbottom.\n\nRecall\tthat\tGradient\tDescent\tsimply\tupdates\tthe\tweights\tθ\tby\tdirectly\tsubtracting\tthe\tgradient\tof\tthe\tcost function\tJ(θ)\twith\tregards\tto\tthe\tweights\t( θJ(θ))\tmultiplied\tby\tthe\tlearning\trate\tη.\tThe\tequation\tis:\tθ\t← θ\t–\tη θJ(θ).\tIt\tdoes\tnot\tcare\tabout\twhat\tthe\tearlier\tgradients\twere.\tIf\tthe\tlocal\tgradient\tis\ttiny,\tit\tgoes very\tslowly.\n\nMomentum\toptimization\tcares\ta\tgreat\tdeal\tabout\twhat\tprevious\tgradients\twere:\tat\teach\titeration,\tit subtracts\tthe\tlocal\tgradient\tfrom\tthe\tmomentum\tvector\tm\t(multiplied\tby\tthe\tlearning\trate\tη),\tand\tit updates\tthe\tweights\tby\tsimply\tadding\tthis\tmomentum\tvector\t(see\tEquation\t11-4).\tIn\tother\twords,\tthe gradient\tis\tused\tas\tan\tacceleration,\tnot\tas\ta\tspeed.\tTo\tsimulate\tsome\tsort\tof\tfriction\tmechanism\tand prevent\tthe\tmomentum\tfrom\tgrowing\ttoo\tlarge,\tthe\talgorithm\tintroduces\ta\tnew\thyperparameter\tβ,\tsimply called\tthe\tmomentum,\twhich\tmust\tbe\tset\tbetween\t0\t(high\tfriction)\tand\t1\t(no\tfriction).\tA\ttypical momentum\tvalue\tis\t0.9.\n\nEquation\t11-4.\tMomentum\talgorithm\n\nYou\tcan\teasily\tverify\tthat\tif\tthe\tgradient\tremains\tconstant,\tthe\tterminal\tvelocity\t(i.e.,\tthe\tmaximum\tsize\tof\n\nthe\tweight\tupdates)\tis\tequal\tto\tthat\tgradient\tmultiplied\tby\tthe\tlearning\trate\tη\tmultiplied\tby\t the\tsign).\tFor\texample,\tif\tβ\t=\t0.9,\tthen\tthe\tterminal\tvelocity\tis\tequal\tto\t10\ttimes\tthe\tgradient\ttimes\tthe learning\trate,\tso\tMomentum\toptimization\tends\tup\tgoing\t10\ttimes\tfaster\tthan\tGradient\tDescent!\tThis allows\tMomentum\toptimization\tto\tescape\tfrom\tplateaus\tmuch\tfaster\tthan\tGradient\tDescent.\tIn\tparticular, we\tsaw\tin\tChapter\t4\tthat\twhen\tthe\tinputs\thave\tvery\tdifferent\tscales\tthe\tcost\tfunction\twill\tlook\tlike\tan elongated\tbowl\t(see\tFigure\t4-7).\tGradient\tDescent\tgoes\tdown\tthe\tsteep\tslope\tquite\tfast,\tbut\tthen\tit\ttakes a\tvery\tlong\ttime\tto\tgo\tdown\tthe\tvalley.\tIn\tcontrast,\tMomentum\toptimization\twill\troll\tdown\tthe\tbottom\tof the\tvalley\tfaster\tand\tfaster\tuntil\tit\treaches\tthe\tbottom\t(the\toptimum).\tIn\tdeep\tneural\tnetworks\tthat\tdon’t use\tBatch\tNormalization,\tthe\tupper\tlayers\twill\toften\tend\tup\thaving\tinputs\twith\tvery\tdifferent\tscales,\tso using\tMomentum\toptimization\thelps\ta\tlot.\tIt\tcan\talso\thelp\troll\tpast\tlocal\toptima.\n\n(ignoring\n\nNOTE\n\nDue\tto\tthe\tmomentum,\tthe\toptimizer\tmay\tovershoot\ta\tbit,\tthen\tcome\tback,\tovershoot\tagain,\tand\toscillate\tlike\tthis\tmany\ttimes before\tstabilizing\tat\tthe\tminimum.\tThis\tis\tone\tof\tthe\treasons\twhy\tit\tis\tgood\tto\thave\ta\tbit\tof\tfriction\tin\tthe\tsystem:\tit\tgets\trid\tof these\toscillations\tand\tthus\tspeeds\tup\tconvergence.\n\nImplementing\tMomentum\toptimization\tin\tTensorFlow\tis\ta\tno-brainer:\tjust\treplace\tthe GradientDescentOptimizer\twith\tthe\tMomentumOptimizer,\tthen\tlie\tback\tand\tprofit!\n\noptimizer\t=\ttf.train.MomentumOptimizer(learning_rate=learning_rate, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=0.9)\n\nThe\tone\tdrawback\tof\tMomentum\toptimization\tis\tthat\tit\tadds\tyet\tanother\thyperparameter\tto\ttune.\tHowever, the\tmomentum\tvalue\tof\t0.9\tusually\tworks\twell\tin\tpractice\tand\talmost\talways\tgoes\tfaster\tthan\tGradient Descent.\n\nNesterov\tAccelerated\tGradient One\tsmall\tvariant\tto\tMomentum\toptimization,\tproposed\tby\tYurii\tNesterov\tin\t1983,12\tis\talmost\talways faster\tthan\tvanilla\tMomentum\toptimization.\tThe\tidea\tof\tNesterov\tMomentum\toptimization,\tor\tNesterov Accelerated\tGradient\t(NAG),\tis\tto\tmeasure\tthe\tgradient\tof\tthe\tcost\tfunction\tnot\tat\tthe\tlocal\tposition\tbut slightly\tahead\tin\tthe\tdirection\tof\tthe\tmomentum\t(see\tEquation\t11-5).\tThe\tonly\tdifference\tfrom\tvanilla Momentum\toptimization\tis\tthat\tthe\tgradient\tis\tmeasured\tat\tθ\t+\tβm\trather\tthan\tat\tθ.\n\nEquation\t11-5.\tNesterov\tAccelerated\tGradient\talgorithm\n\nThis\tsmall\ttweak\tworks\tbecause\tin\tgeneral\tthe\tmomentum\tvector\twill\tbe\tpointing\tin\tthe\tright\tdirection (i.e.,\ttoward\tthe\toptimum),\tso\tit\twill\tbe\tslightly\tmore\taccurate\tto\tuse\tthe\tgradient\tmeasured\ta\tbit\tfarther\tin that\tdirection\trather\tthan\tusing\tthe\tgradient\tat\tthe\toriginal\tposition,\tas\tyou\tcan\tsee\tin\tFigure\t11-6\t(where 1\trepresents\tthe\tgradient\tof\tthe\tcost\tfunction\tmeasured\tat\tthe\tstarting\tpoint\tθ,\tand\t 2\trepresents\tthe\n\ngradient\tat\tthe\tpoint\tlocated\tat\tθ\t+\tβm).\tAs\tyou\tcan\tsee,\tthe\tNesterov\tupdate\tends\tup\tslightly\tcloser\tto\tthe optimum.\tAfter\ta\twhile,\tthese\tsmall\timprovements\tadd\tup\tand\tNAG\tends\tup\tbeing\tsignificantly\tfaster\tthan regular\tMomentum\toptimization.\tMoreover,\tnote\tthat\twhen\tthe\tmomentum\tpushes\tthe\tweights\tacross\ta valley,\t 1\tcontinues\tto\tpush\tfurther\tacross\tthe\tvalley,\twhile\t 2\tpushes\tback\ttoward\tthe\tbottom\tof\tthe valley.\tThis\thelps\treduce\toscillations\tand\tthus\tconverges\tfaster.\n\nNAG\twill\talmost\talways\tspeed\tup\ttraining\tcompared\tto\tregular\tMomentum\toptimization.\tTo\tuse\tit, simply\tset\tuse_nesterov=True\twhen\tcreating\tthe\tMomentumOptimizer:\n\noptimizer\t=\ttf.train.MomentumOptimizer(learning_rate=learning_rate, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=0.9,\tuse_nesterov=True)\n\nFigure\t11-6.\tRegular\tversus\tNesterov\tMomentum\toptimization\n\nAdaGrad Consider\tthe\telongated\tbowl\tproblem\tagain:\tGradient\tDescent\tstarts\tby\tquickly\tgoing\tdown\tthe\tsteepest slope,\tthen\tslowly\tgoes\tdown\tthe\tbottom\tof\tthe\tvalley.\tIt\twould\tbe\tnice\tif\tthe\talgorithm\tcould\tdetect\tthis early\ton\tand\tcorrect\tits\tdirection\tto\tpoint\ta\tbit\tmore\ttoward\tthe\tglobal\toptimum. The\tAdaGrad\talgorithm13\tachieves\tthis\tby\tscaling\tdown\tthe\tgradient\tvector\talong\tthe\tsteepest\tdimensions (see\tEquation\t11-6):\n\nEquation\t11-6.\tAdaGrad\talgorithm\n\nThe\tfirst\tstep\taccumulates\tthe\tsquare\tof\tthe\tgradients\tinto\tthe\tvector\ts\t(the\t element-wise\tmultiplication).\tThis\tvectorized\tform\tis\tequivalent\tto\tcomputing\tsi\t←\tsi\t+\t(∂\tJ(θ)\t/\t∂\tθi)2 for\teach\telement\tsi\tof\tthe\tvector\ts;\tin\tother\twords,\teach\tsi\taccumulates\tthe\tsquares\tof\tthe\tpartial derivative\tof\tthe\tcost\tfunction\twith\tregards\tto\tparameter\tθi.\tIf\tthe\tcost\tfunction\tis\tsteep\talong\tthe\tith dimension,\tthen\tsi\twill\tget\tlarger\tand\tlarger\tat\teach\titeration.\n\nsymbol\trepresents\tthe\n\nThe\tsecond\tstep\tis\talmost\tidentical\tto\tGradient\tDescent,\tbut\twith\tone\tbig\tdifference:\tthe\tgradient\tvector\n\nis\tscaled\tdown\tby\ta\tfactor\tof\t smoothing\tterm\tto\tavoid\tdivision\tby\tzero,\ttypically\tset\tto\t10–10).\tThis\tvectorized\tform\tis\tequivalent\tto\n\n(the\t\tsymbol\trepresents\tthe\telement-wise\tdivision,\tand\tϵ\tis\ta\n\ncomputing\n\nfor\tall\tparameters\tθi\t(simultaneously).\n\nIn\tshort,\tthis\talgorithm\tdecays\tthe\tlearning\trate,\tbut\tit\tdoes\tso\tfaster\tfor\tsteep\tdimensions\tthan\tfor dimensions\twith\tgentler\tslopes.\tThis\tis\tcalled\tan\tadaptive\tlearning\trate.\tIt\thelps\tpoint\tthe\tresulting updates\tmore\tdirectly\ttoward\tthe\tglobal\toptimum\t(see\tFigure\t11-7).\tOne\tadditional\tbenefit\tis\tthat\tit requires\tmuch\tless\ttuning\tof\tthe\tlearning\trate\thyperparameter\tη.\n\nFigure\t11-7.\tAdaGrad\tversus\tGradient\tDescent\n\nAdaGrad\toften\tperforms\twell\tfor\tsimple\tquadratic\tproblems,\tbut\tunfortunately\tit\toften\tstops\ttoo\tearly when\ttraining\tneural\tnetworks.\tThe\tlearning\trate\tgets\tscaled\tdown\tso\tmuch\tthat\tthe\talgorithm\tends\tup stopping\tentirely\tbefore\treaching\tthe\tglobal\toptimum.\tSo\teven\tthough\tTensorFlow\thas\tan AdagradOptimizer,\tyou\tshould\tnot\tuse\tit\tto\ttrain\tdeep\tneural\tnetworks\t(it\tmay\tbe\tefficient\tfor\tsimpler tasks\tsuch\tas\tLinear\tRegression,\tthough).\n\nRMSProp Although\tAdaGrad\tslows\tdown\ta\tbit\ttoo\tfast\tand\tends\tup\tnever\tconverging\tto\tthe\tglobal\toptimum,\tthe RMSProp\talgorithm14\tfixes\tthis\tby\taccumulating\tonly\tthe\tgradients\tfrom\tthe\tmost\trecent\titerations\t(as opposed\tto\tall\tthe\tgradients\tsince\tthe\tbeginning\tof\ttraining).\tIt\tdoes\tso\tby\tusing\texponential\tdecay\tin\tthe first\tstep\t(see\tEquation\t11-7).\n\nEquation\t11-7.\tRMSProp\talgorithm\n\nThe\tdecay\trate\tβ\tis\ttypically\tset\tto\t0.9.\tYes,\tit\tis\tonce\tagain\ta\tnew\thyperparameter,\tbut\tthis\tdefault\tvalue often\tworks\twell,\tso\tyou\tmay\tnot\tneed\tto\ttune\tit\tat\tall.\n\nAs\tyou\tmight\texpect,\tTensorFlow\thas\tan\tRMSPropOptimizer\tclass:\n\noptimizer\t=\ttf.train.RMSPropOptimizer(learning_rate=learning_rate, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=0.9,\tdecay=0.9,\tepsilon=1e-10)\n\nExcept\ton\tvery\tsimple\tproblems,\tthis\toptimizer\talmost\talways\tperforms\tmuch\tbetter\tthan\tAdaGrad.\tIt also\tgenerally\tconverges\tfaster\tthan\tMomentum\toptimization\tand\tNesterov\tAccelerated\tGradients.\tIn\tfact, it\twas\tthe\tpreferred\toptimization\talgorithm\tof\tmany\tresearchers\tuntil\tAdam\toptimization\tcame\taround.\n\nAdam\tOptimization Adam,15\twhich\tstands\tfor\tadaptive\tmoment\testimation,\tcombines\tthe\tideas\tof\tMomentum\toptimization and\tRMSProp:\tjust\tlike\tMomentum\toptimization\tit\tkeeps\ttrack\tof\tan\texponentially\tdecaying\taverage\tof past\tgradients,\tand\tjust\tlike\tRMSProp\tit\tkeeps\ttrack\tof\tan\texponentially\tdecaying\taverage\tof\tpast\tsquared gradients\t(see\tEquation\t11-8).16\n\nEquation\t11-8.\tAdam\talgorithm\n\nT\trepresents\tthe\titeration\tnumber\t(starting\tat\t1).\n\nIf\tyou\tjust\tlook\tat\tsteps\t1,\t2,\tand\t5,\tyou\twill\tnotice\tAdam’s\tclose\tsimilarity\tto\tboth\tMomentum optimization\tand\tRMSProp.\tThe\tonly\tdifference\tis\tthat\tstep\t1\tcomputes\tan\texponentially\tdecaying average\trather\tthan\tan\texponentially\tdecaying\tsum,\tbut\tthese\tare\tactually\tequivalent\texcept\tfor\ta\tconstant factor\t(the\tdecaying\taverage\tis\tjust\t1\t–\tβ1\ttimes\tthe\tdecaying\tsum).\tSteps\t3\tand\t4\tare\tsomewhat\tof\ta technical\tdetail:\tsince\tm\tand\ts\tare\tinitialized\tat\t0,\tthey\twill\tbe\tbiased\ttoward\t0\tat\tthe\tbeginning\tof training,\tso\tthese\ttwo\tsteps\twill\thelp\tboost\tm\tand\ts\tat\tthe\tbeginning\tof\ttraining.\n\nThe\tmomentum\tdecay\thyperparameter\tβ1\tis\ttypically\tinitialized\tto\t0.9,\twhile\tthe\tscaling\tdecay hyperparameter\tβ2\tis\toften\tinitialized\tto\t0.999.\tAs\tearlier,\tthe\tsmoothing\tterm\tϵ\tis\tusually\tinitialized\tto\ta tiny\tnumber\tsuch\tas\t10–8.\tThese\tare\tthe\tdefault\tvalues\tfor\tTensorFlow’s\tAdamOptimizer\tclass,\tso\tyou can\tsimply\tuse:\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate=learning_rate)\n\nIn\tfact,\tsince\tAdam\tis\tan\tadaptive\tlearning\trate\talgorithm\t(like\tAdaGrad\tand\tRMSProp),\tit\trequires\tless tuning\tof\tthe\tlearning\trate\thyperparameter\tη.\tYou\tcan\toften\tuse\tthe\tdefault\tvalue\tη\t=\t0.001,\tmaking\tAdam even\teasier\tto\tuse\tthan\tGradient\tDescent.\n\nWARNING\n\nThis\tbook\tinitially\trecommended\tusing\tAdam\toptimization,\tbecause\tit\twas\tgenerally\tconsidered\tfaster\tand\tbetter\tthan\tother methods.\tHowever,\ta\t2017\tpaper17\tby\tAshia\tC.\tWilson\tet\tal.\tshowed\tthat\tadaptive\toptimization\tmethods\t(i.e.,\tAdaGrad, RMSProp\tand\tAdam\toptimization)\tcan\tlead\tto\tsolutions\tthat\tgeneralize\tpoorly\ton\tsome\tdatasets.\tSo\tyou\tmay\twant\tto\tstick\tto Momentum\toptimization\tor\tNesterov\tAccelerated\tGradient\tfor\tnow,\tuntil\tresearchers\thave\ta\tbetter\tunderstanding\tof\tthis\tissue.\n\nAll\tthe\toptimization\ttechniques\tdiscussed\tso\tfar\tonly\trely\ton\tthe\tfirst-order\tpartial\tderivatives (Jacobians).\tThe\toptimization\tliterature\tcontains\tamazing\talgorithms\tbased\ton\tthe\tsecond-order\tpartial derivatives\t(the\tHessians).\tUnfortunately,\tthese\talgorithms\tare\tvery\thard\tto\tapply\tto\tdeep\tneural\tnetworks because\tthere\tare\tn2\tHessians\tper\toutput\t(where\tn\tis\tthe\tnumber\tof\tparameters),\tas\topposed\tto\tjust\tn Jacobians\tper\toutput.\tSince\tDNNs\ttypically\thave\ttens\tof\tthousands\tof\tparameters,\tthe\tsecond-order optimization\talgorithms\toften\tdon’t\teven\tfit\tin\tmemory,\tand\teven\twhen\tthey\tdo,\tcomputing\tthe\tHessians\tis just\ttoo\tslow.\n\nTRAINING\tSPARSE\tMODELS\n\nAll\tthe\toptimization\talgorithms\tjust\tpresented\tproduce\tdense\tmodels,\tmeaning\tthat\tmost\tparameters\twill\tbe\tnonzero.\tIf\tyou\tneed\ta blazingly\tfast\tmodel\tat\truntime,\tor\tif\tyou\tneed\tit\tto\ttake\tup\tless\tmemory,\tyou\tmay\tprefer\tto\tend\tup\twith\ta\tsparse\tmodel\tinstead.\n\nOne\ttrivial\tway\tto\tachieve\tthis\tis\tto\ttrain\tthe\tmodel\tas\tusual,\tthen\tget\trid\tof\tthe\ttiny\tweights\t(set\tthem\tto\t0).\n\nAnother\toption\tis\tto\tapply\tstrong\tℓ1\tregularization\tduring\ttraining,\tas\tit\tpushes\tthe\toptimizer\tto\tzero\tout\tas\tmany\tweights\tas\tit\tcan\t(as discussed\tin\tChapter\t4\tabout\tLasso\tRegression).\n\nHowever,\tin\tsome\tcases\tthese\ttechniques\tmay\tremain\tinsufficient.\tOne\tlast\toption\tis\tto\tapply\tDual\tAveraging,\toften\tcalled\tFollow\tThe Regularized\tLeader\t(FTRL),\ta\ttechnique\tproposed\tby\tYurii\tNesterov.18\tWhen\tused\twith\tℓ1\tregularization,\tthis\ttechnique\toften\tleads\tto very\tsparse\tmodels.\tTensorFlow\timplements\ta\tvariant\tof\tFTRL\tcalled\tFTRL-Proximal19\tin\tthe\tFTRLOptimizer\tclass.\n\nLearning\tRate\tScheduling Finding\ta\tgood\tlearning\trate\tcan\tbe\ttricky.\tIf\tyou\tset\tit\tway\ttoo\thigh,\ttraining\tmay\tactually\tdiverge\t(as\twe discussed\tin\tChapter\t4).\tIf\tyou\tset\tit\ttoo\tlow,\ttraining\twill\teventually\tconverge\tto\tthe\toptimum,\tbut\tit\twill take\ta\tvery\tlong\ttime.\tIf\tyou\tset\tit\tslightly\ttoo\thigh,\tit\twill\tmake\tprogress\tvery\tquickly\tat\tfirst,\tbut\tit\twill end\tup\tdancing\taround\tthe\toptimum,\tnever\tsettling\tdown\t(unless\tyou\tuse\tan\tadaptive\tlearning\trate optimization\talgorithm\tsuch\tas\tAdaGrad,\tRMSProp,\tor\tAdam,\tbut\teven\tthen\tit\tmay\ttake\ttime\tto\tsettle).\tIf you\thave\ta\tlimited\tcomputing\tbudget,\tyou\tmay\thave\tto\tinterrupt\ttraining\tbefore\tit\thas\tconverged\tproperly, yielding\ta\tsuboptimal\tsolution\t(see\tFigure\t11-8).\n\nFigure\t11-8.\tLearning\tcurves\tfor\tvarious\tlearning\trates\tη\n\nYou\tmay\tbe\table\tto\tfind\ta\tfairly\tgood\tlearning\trate\tby\ttraining\tyour\tnetwork\tseveral\ttimes\tduring\tjust\ta few\tepochs\tusing\tvarious\tlearning\trates\tand\tcomparing\tthe\tlearning\tcurves.\tThe\tideal\tlearning\trate\twill learn\tquickly\tand\tconverge\tto\tgood\tsolution.\n\nHowever,\tyou\tcan\tdo\tbetter\tthan\ta\tconstant\tlearning\trate:\tif\tyou\tstart\twith\ta\thigh\tlearning\trate\tand\tthen reduce\tit\tonce\tit\tstops\tmaking\tfast\tprogress,\tyou\tcan\treach\ta\tgood\tsolution\tfaster\tthan\twith\tthe\toptimal constant\tlearning\trate.\tThere\tare\tmany\tdifferent\tstrategies\tto\treduce\tthe\tlearning\trate\tduring\ttraining. These\tstrategies\tare\tcalled\tlearning\tschedules\t(we\tbriefly\tintroduced\tthis\tconcept\tin\tChapter\t4),\tthe\tmost common\tof\twhich\tare:\n\nPredetermined\tpiecewise\tconstant\tlearning\trate\n\nFor\texample,\tset\tthe\tlearning\trate\tto\tη0\t=\t0.1\tat\tfirst,\tthen\tto\tη1\t=\t0.001\tafter\t50\tepochs.\tAlthough this\tsolution\tcan\twork\tvery\twell,\tit\toften\trequires\tfiddling\taround\tto\tfigure\tout\tthe\tright\tlearning rates\tand\twhen\tto\tuse\tthem.\n\nPerformance\tscheduling\n\nMeasure\tthe\tvalidation\terror\tevery\tN\tsteps\t(just\tlike\tfor\tearly\tstopping)\tand\treduce\tthe\tlearning\trate by\ta\tfactor\tof\tλ\twhen\tthe\terror\tstops\tdropping.\n\nExponential\tscheduling\n\nSet\tthe\tlearning\trate\tto\ta\tfunction\tof\tthe\titeration\tnumber\tt:\tη(t)\t=\tη0\t10–t/r.\tThis\tworks\tgreat,\tbut\tit requires\ttuning\tη0\tand\tr.\tThe\tlearning\trate\twill\tdrop\tby\ta\tfactor\tof\t10\tevery\tr\tsteps.\n\nPower\tscheduling\n\nSet\tthe\tlearning\trate\tto\tη(t)\t=\tη0\t(1\t+\tt/r)–c.\tThe\thyperparameter\tc\tis\ttypically\tset\tto\t1.\tThis\tis similar\tto\texponential\tscheduling,\tbut\tthe\tlearning\trate\tdrops\tmuch\tmore\tslowly.\n\nA\t2013\tpaper20\tby\tAndrew\tSenior\tet\tal.\tcompared\tthe\tperformance\tof\tsome\tof\tthe\tmost\tpopular\tlearning schedules\twhen\ttraining\tdeep\tneural\tnetworks\tfor\tspeech\trecognition\tusing\tMomentum\toptimization.\tThe authors\tconcluded\tthat,\tin\tthis\tsetting,\tboth\tperformance\tscheduling\tand\texponential\tscheduling\tperformed well,\tbut\tthey\tfavored\texponential\tscheduling\tbecause\tit\tis\tsimpler\tto\timplement,\tis\teasy\tto\ttune,\tand converged\tslightly\tfaster\tto\tthe\toptimal\tsolution.\n\nImplementing\ta\tlearning\tschedule\twith\tTensorFlow\tis\tfairly\tstraightforward:\n\ninitial_learning_rate\t=\t0.1 decay_steps\t=\t10000 decay_rate\t=\t1/10 global_step\t=\ttf.Variable(0,\ttrainable=False,\tname=\"global_step\") learning_rate\t=\ttf.train.exponential_decay(initial_learning_rate,\tglobal_step, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdecay_steps,\tdecay_rate) optimizer\t=\ttf.train.MomentumOptimizer(learning_rate,\tmomentum=0.9) training_op\t=\toptimizer.minimize(loss,\tglobal_step=global_step)\n\nAfter\tsetting\tthe\thyperparameter\tvalues,\twe\tcreate\ta\tnontrainable\tvariable\tglobal_step\t(initialized\tto\t0) to\tkeep\ttrack\tof\tthe\tcurrent\ttraining\titeration\tnumber.\tThen\twe\tdefine\tan\texponentially\tdecaying\tlearning rate\t(with\tη0\t=\t0.1\tand\tr\t=\t10,000)\tusing\tTensorFlow’s\texponential_decay()\tfunction.\tNext,\twe\tcreate an\toptimizer\t(in\tthis\texample,\ta\tMomentumOptimizer)\tusing\tthis\tdecaying\tlearning\trate.\tFinally,\twe create\tthe\ttraining\toperation\tby\tcalling\tthe\toptimizer’s\tminimize()\tmethod;\tsince\twe\tpass\tit\tthe global_step\tvariable,\tit\twill\tkindly\ttake\tcare\tof\tincrementing\tit.\tThat’s\tit!\n\nSince\tAdaGrad,\tRMSProp,\tand\tAdam\toptimization\tautomatically\treduce\tthe\tlearning\trate\tduring\ttraining, it\tis\tnot\tnecessary\tto\tadd\tan\textra\tlearning\tschedule.\tFor\tother\toptimization\talgorithms,\tusing\texponential decay\tor\tperformance\tscheduling\tcan\tconsiderably\tspeed\tup\tconvergence.\n\nAvoiding\tOverfitting\tThrough\tRegularization With\tfour\tparameters\tI\tcan\tfit\tan\telephant\tand\twith\tfive\tI\tcan\tmake\thim\twiggle\this\ttrunk. John\tvon\tNeumann,\tcited\tby\tEnrico\tFermi\tin\tNature\t427\n\nDeep\tneural\tnetworks\ttypically\thave\ttens\tof\tthousands\tof\tparameters,\tsometimes\teven\tmillions.\tWith\tso many\tparameters,\tthe\tnetwork\thas\tan\tincredible\tamount\tof\tfreedom\tand\tcan\tfit\ta\thuge\tvariety\tof\tcomplex datasets.\tBut\tthis\tgreat\tflexibility\talso\tmeans\tthat\tit\tis\tprone\tto\toverfitting\tthe\ttraining\tset.\n\nWith\tmillions\tof\tparameters\tyou\tcan\tfit\tthe\twhole\tzoo.\tIn\tthis\tsection\twe\twill\tpresent\tsome\tof\tthe\tmost popular\tregularization\ttechniques\tfor\tneural\tnetworks,\tand\thow\tto\timplement\tthem\twith\tTensorFlow: early\tstopping,\tℓ1\tand\tℓ2\tregularization,\tdropout,\tmax-norm\tregularization,\tand\tdata\taugmentation.\n\nEarly\tStopping To\tavoid\toverfitting\tthe\ttraining\tset,\ta\tgreat\tsolution\tis\tearly\tstopping\t(introduced\tin\tChapter\t4):\tjust interrupt\ttraining\twhen\tits\tperformance\ton\tthe\tvalidation\tset\tstarts\tdropping.\n\nOne\tway\tto\timplement\tthis\twith\tTensorFlow\tis\tto\tevaluate\tthe\tmodel\ton\ta\tvalidation\tset\tat\tregular intervals\t(e.g.,\tevery\t50\tsteps),\tand\tsave\ta\t“winner”\tsnapshot\tif\tit\toutperforms\tprevious\t“winner” snapshots.\tCount\tthe\tnumber\tof\tsteps\tsince\tthe\tlast\t“winner”\tsnapshot\twas\tsaved,\tand\tinterrupt\ttraining when\tthis\tnumber\treaches\tsome\tlimit\t(e.g.,\t2,000\tsteps).\tThen\trestore\tthe\tlast\t“winner”\tsnapshot.\n\nAlthough\tearly\tstopping\tworks\tvery\twell\tin\tpractice,\tyou\tcan\tusually\tget\tmuch\thigher\tperformance\tout\tof your\tnetwork\tby\tcombining\tit\twith\tother\tregularization\ttechniques.\n\nℓ1\tand\tℓ2\tRegularization Just\tlike\tyou\tdid\tin\tChapter\t4\tfor\tsimple\tlinear\tmodels,\tyou\tcan\tuse\tℓ1\tand\tℓ2\tregularization\tto\tconstrain\ta neural\tnetwork’s\tconnection\tweights\t(but\ttypically\tnot\tits\tbiases).\n\nOne\tway\tto\tdo\tthis\tusing\tTensorFlow\tis\tto\tsimply\tadd\tthe\tappropriate\tregularization\tterms\tto\tyour\tcost function.\tFor\texample,\tassuming\tyou\thave\tjust\tone\thidden\tlayer\twith\tweights\tW1\tand\tone\toutput\tlayer\twith weights\tW2,\tthen\tyou\tcan\tapply\tℓ1\tregularization\tlike\tthis:\n\n[...]\t#\tconstruct\tthe\tneural\tnetwork W1\t=\ttf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\") W2\t=\ttf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n\nscale\t=\t0.001\t#\tl1\tregularization\thyperparameter\n\nwith\ttf.name_scope(\"loss\"): \t\t\t\txentropy\t=\ttf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogits=logits) \t\t\t\tbase_loss\t=\ttf.reduce_mean(xentropy,\tname=\"avg_xentropy\") \t\t\t\treg_losses\t=\ttf.reduce_sum(tf.abs(W1))\t+\ttf.reduce_sum(tf.abs(W2)) \t\t\t\tloss\t=\ttf.add(base_loss,\tscale\t*\treg_losses,\tname=\"loss\")\n\nHowever,\tif\tthere\tare\tmany\tlayers,\tthis\tapproach\tis\tnot\tvery\tconvenient.\tFortunately,\tTensorFlow provides\ta\tbetter\toption.\tMany\tfunctions\tthat\tcreate\tvariables\t(such\tas\tget_variable()\tor tf.layers.dense())\taccept\ta\t*_regularizer\targument\tfor\teach\tcreated\tvariable\t(e.g., kernel_regularizer).\tYou\tcan\tpass\tany\tfunction\tthat\ttakes\tweights\tas\tan\targument\tand\treturns\tthe corresponding\tregularization\tloss.\tThe\tl1_regularizer(),\tl2_regularizer(),\tand l1_l2_regularizer()\tfunctions\treturn\tsuch\tfunctions.\tThe\tfollowing\tcode\tputs\tall\tthis\ttogether:\n\nmy_dense_layer\t=\tpartial( \t\t\t\ttf.layers.dense,\tactivation=tf.nn.relu, \t\t\t\tkernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n\nwith\ttf.name_scope(\"dnn\"): \t\t\t\thidden1\t=\tmy_dense_layer(X,\tn_hidden1,\tname=\"hidden1\") \t\t\t\thidden2\t=\tmy_dense_layer(hidden1,\tn_hidden2,\tname=\"hidden2\") \t\t\t\tlogits\t=\tmy_dense_layer(hidden2,\tn_outputs,\tactivation=None, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"outputs\")\n\nThis\tcode\tcreates\ta\tneural\tnetwork\twith\ttwo\thidden\tlayers\tand\tone\toutput\tlayer,\tand\tit\talso\tcreates\tnodes in\tthe\tgraph\tto\tcompute\tthe\tℓ1\tregularization\tloss\tcorresponding\tto\teach\tlayer’s\tweights.\tTensorFlow automatically\tadds\tthese\tnodes\tto\ta\tspecial\tcollection\tcontaining\tall\tthe\tregularization\tlosses.\tYou\tjust need\tto\tadd\tthese\tregularization\tlosses\tto\tyour\toverall\tloss,\tlike\tthis:\n\nreg_losses\t=\ttf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) loss\t=\ttf.add_n([base_loss]\t+\treg_losses,\tname=\"loss\")\n\nWARNING\n\nDon’t\tforget\tto\tadd\tthe\tregularization\tlosses\tto\tyour\toverall\tloss,\tor\telse\tthey\twill\tsimply\tbe\tignored.\n\nDropout The\tmost\tpopular\tregularization\ttechnique\tfor\tdeep\tneural\tnetworks\tis\targuably\tdropout.\tIt\twas proposed21\tby\tG.\tE.\tHinton\tin\t2012\tand\tfurther\tdetailed\tin\ta\tpaper22\tby\tNitish\tSrivastava\tet\tal.,\tand\tit\thas proven\tto\tbe\thighly\tsuccessful:\teven\tthe\tstate-of-the-art\tneural\tnetworks\tgot\ta\t1–2%\taccuracy\tboost simply\tby\tadding\tdropout.\tThis\tmay\tnot\tsound\tlike\ta\tlot,\tbut\twhen\ta\tmodel\talready\thas\t95%\taccuracy, getting\ta\t2%\taccuracy\tboost\tmeans\tdropping\tthe\terror\trate\tby\talmost\t40%\t(going\tfrom\t5%\terror\tto roughly\t3%).\n\nIt\tis\ta\tfairly\tsimple\talgorithm:\tat\tevery\ttraining\tstep,\tevery\tneuron\t(including\tthe\tinput\tneurons\tbut excluding\tthe\toutput\tneurons)\thas\ta\tprobability\tp\tof\tbeing\ttemporarily\t“dropped\tout,”\tmeaning\tit\twill\tbe entirely\tignored\tduring\tthis\ttraining\tstep,\tbut\tit\tmay\tbe\tactive\tduring\tthe\tnext\tstep\t(see\tFigure\t11-9).\tThe hyperparameter\tp\tis\tcalled\tthe\tdropout\trate,\tand\tit\tis\ttypically\tset\tto\t50%.\tAfter\ttraining,\tneurons\tdon’t get\tdropped\tanymore.\tAnd\tthat’s\tall\t(except\tfor\ta\ttechnical\tdetail\twe\twill\tdiscuss\tmomentarily).\n\nFigure\t11-9.\tDropout\tregularization\n\nIt\tis\tquite\tsurprising\tat\tfirst\tthat\tthis\trather\tbrutal\ttechnique\tworks\tat\tall.\tWould\ta\tcompany\tperform\tbetter if\tits\temployees\twere\ttold\tto\ttoss\ta\tcoin\tevery\tmorning\tto\tdecide\twhether\tor\tnot\tto\tgo\tto\twork?\tWell,\twho knows;\tperhaps\tit\twould!\tThe\tcompany\twould\tobviously\tbe\tforced\tto\tadapt\tits\torganization;\tit\tcould\tnot rely\ton\tany\tsingle\tperson\tto\tfill\tin\tthe\tcoffee\tmachine\tor\tperform\tany\tother\tcritical\ttasks,\tso\tthis\texpertise would\thave\tto\tbe\tspread\tacross\tseveral\tpeople.\tEmployees\twould\thave\tto\tlearn\tto\tcooperate\twith\tmany of\ttheir\tcoworkers,\tnot\tjust\ta\thandful\tof\tthem.\tThe\tcompany\twould\tbecome\tmuch\tmore\tresilient.\tIf\tone person\tquit,\tit\twouldn’t\tmake\tmuch\tof\ta\tdifference.\tIt’s\tunclear\twhether\tthis\tidea\twould\tactually\twork\tfor companies,\tbut\tit\tcertainly\tdoes\tfor\tneural\tnetworks.\tNeurons\ttrained\twith\tdropout\tcannot\tco-adapt\twith their\tneighboring\tneurons;\tthey\thave\tto\tbe\tas\tuseful\tas\tpossible\ton\ttheir\town.\tThey\talso\tcannot\trely\n\nexcessively\ton\tjust\ta\tfew\tinput\tneurons;\tthey\tmust\tpay\tattention\tto\teach\tof\ttheir\tinput\tneurons.\tThey\tend\tup being\tless\tsensitive\tto\tslight\tchanges\tin\tthe\tinputs.\tIn\tthe\tend\tyou\tget\ta\tmore\trobust\tnetwork\tthat generalizes\tbetter.\n\nAnother\tway\tto\tunderstand\tthe\tpower\tof\tdropout\tis\tto\trealize\tthat\ta\tunique\tneural\tnetwork\tis\tgenerated\tat each\ttraining\tstep.\tSince\teach\tneuron\tcan\tbe\teither\tpresent\tor\tabsent,\tthere\tis\ta\ttotal\tof\t2N\tpossible networks\t(where\tN\tis\tthe\ttotal\tnumber\tof\tdroppable\tneurons).\tThis\tis\tsuch\ta\thuge\tnumber\tthat\tit\tis virtually\timpossible\tfor\tthe\tsame\tneural\tnetwork\tto\tbe\tsampled\ttwice.\tOnce\tyou\thave\trun\ta\t10,000 training\tsteps,\tyou\thave\tessentially\ttrained\t10,000\tdifferent\tneural\tnetworks\t(each\twith\tjust\tone\ttraining instance).\tThese\tneural\tnetworks\tare\tobviously\tnot\tindependent\tsince\tthey\tshare\tmany\tof\ttheir\tweights, but\tthey\tare\tnevertheless\tall\tdifferent.\tThe\tresulting\tneural\tnetwork\tcan\tbe\tseen\tas\tan\taveraging\tensemble of\tall\tthese\tsmaller\tneural\tnetworks.\n\nThere\tis\tone\tsmall\tbut\timportant\ttechnical\tdetail.\tSuppose\tp\t=\t50%,\tin\twhich\tcase\tduring\ttesting\ta\tneuron will\tbe\tconnected\tto\ttwice\tas\tmany\tinput\tneurons\tas\tit\twas\t(on\taverage)\tduring\ttraining.\tTo\tcompensate for\tthis\tfact,\twe\tneed\tto\tmultiply\teach\tneuron’s\tinput\tconnection\tweights\tby\t0.5\tafter\ttraining.\tIf\twe\tdon’t, each\tneuron\twill\tget\ta\ttotal\tinput\tsignal\troughly\ttwice\tas\tlarge\tas\twhat\tthe\tnetwork\twas\ttrained\ton,\tand\tit is\tunlikely\tto\tperform\twell.\tMore\tgenerally,\twe\tneed\tto\tmultiply\teach\tinput\tconnection\tweight\tby\tthe\tkeep probability\t(1\t–\tp)\tafter\ttraining.\tAlternatively,\twe\tcan\tdivide\teach\tneuron’s\toutput\tby\tthe\tkeep probability\tduring\ttraining\t(these\talternatives\tare\tnot\tperfectly\tequivalent,\tbut\tthey\twork\tequally\twell).\n\nTo\timplement\tdropout\tusing\tTensorFlow,\tyou\tcan\tsimply\tapply\tthe\ttf.layers.dropout()\tfunction\tto\tthe input\tlayer\tand/or\tto\tthe\toutput\tof\tany\thidden\tlayer\tyou\twant.\tDuring\ttraining,\tthis\tfunction\trandomly drops\tsome\titems\t(setting\tthem\tto\t0)\tand\tdivides\tthe\tremaining\titems\tby\tthe\tkeep\tprobability.\tAfter training,\tthis\tfunction\tdoes\tnothing\tat\tall.\tThe\tfollowing\tcode\tapplies\tdropout\tregularization\tto\tour\tthree- layer\tneural\tnetwork:\n\n[...] training\t=\ttf.placeholder_with_default(False,\tshape=(),\tname='training')\n\ndropout_rate\t=\t0.5\t\t#\t==\t1\t-\tkeep_prob X_drop\t=\ttf.layers.dropout(X,\tdropout_rate,\ttraining=training)\n\nwith\ttf.name_scope(\"dnn\"): \t\t\t\thidden1\t=\ttf.layers.dense(X_drop,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden1\") \t\t\t\thidden1_drop\t=\ttf.layers.dropout(hidden1,\tdropout_rate,\ttraining=training) \t\t\t\thidden2\t=\ttf.layers.dense(hidden1_drop,\tn_hidden2,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden2\") \t\t\t\thidden2_drop\t=\ttf.layers.dropout(hidden2,\tdropout_rate,\ttraining=training) \t\t\t\tlogits\t=\ttf.layers.dense(hidden2_drop,\tn_outputs,\tname=\"outputs\")\n\nWARNING\n\nYou\twant\tto\tuse\tthe\ttf.layers.dropout()\tfunction,\tnot\ttf.nn.dropout().\tThe\tfirst\tone\tturns\toff\t(no-op)\twhen\tnot\ttraining, which\tis\twhat\tyou\twant,\twhile\tthe\tsecond\tone\tdoes\tnot.\n\nOf\tcourse,\tjust\tlike\tyou\tdid\tearlier\tfor\tBatch\tNormalization,\tyou\tneed\tto\tset\ttraining\tto\tTrue\twhen training,\tand\tleave\tthe\tdefault\tFalse\tvalue\twhen\ttesting.\n\nIf\tyou\tobserve\tthat\tthe\tmodel\tis\toverfitting,\tyou\tcan\tincrease\tthe\tdropout\trate.\tConversely,\tyou\tshould\ttry\n\ndecreasing\tthe\tdropout\trate\tif\tthe\tmodel\tunderfits\tthe\ttraining\tset.\tIt\tcan\talso\thelp\tto\tincrease\tthe\tdropout rate\tfor\tlarge\tlayers,\tand\treduce\tit\tfor\tsmall\tones.\n\nDropout\tdoes\ttend\tto\tsignificantly\tslow\tdown\tconvergence,\tbut\tit\tusually\tresults\tin\ta\tmuch\tbetter\tmodel when\ttuned\tproperly.\tSo,\tit\tis\tgenerally\twell\tworth\tthe\textra\ttime\tand\teffort.\n\nNOTE\n\nDropconnect\tis\ta\tvariant\tof\tdropout\twhere\tindividual\tconnections\tare\tdropped\trandomly\trather\tthan\twhole\tneurons.\tIn\tgeneral dropout\tperforms\tbetter.\n\nMax-Norm\tRegularization Another\tregularization\ttechnique\tthat\tis\tquite\tpopular\tfor\tneural\tnetworks\tis\tcalled\tmax-norm regularization:\tfor\teach\tneuron,\tit\tconstrains\tthe\tweights\tw\tof\tthe\tincoming\tconnections\tsuch\tthat\t\t w\t2\t≤ r,\twhere\tr\tis\tthe\tmax-norm\thyperparameter\tand\t\t·\n\n2\tis\tthe\tℓ2\tnorm.\n\nWe\ttypically\timplement\tthis\tconstraint\tby\tcomputing\t w2\tafter\teach\ttraining\tstep\tand\tclipping\tw\tif\n\nneeded\t(\n\n).\n\nReducing\tr\tincreases\tthe\tamount\tof\tregularization\tand\thelps\treduce\toverfitting.\tMax-norm\tregularization can\talso\thelp\talleviate\tthe\tvanishing/exploding\tgradients\tproblems\t(if\tyou\tare\tnot\tusing\tBatch Normalization).\n\nTensorFlow\tdoes\tnot\tprovide\tan\toff-the-shelf\tmax-norm\tregularizer,\tbut\tit\tis\tnot\ttoo\thard\tto\timplement. The\tfollowing\tcode\tgets\ta\thandle\ton\tthe\tweights\tof\tthe\tfirst\thidden\tlayer,\tthen\tit\tuses\tthe clip_by_norm()\tfunction\tto\tcreate\tan\toperation\tthat\twill\tclip\tthe\tweights\talong\tthe\tsecond\taxis\tso\tthat each\trow\tvector\tends\tup\twith\ta\tmaximum\tnorm\tof\t1.0.\tThe\tlast\tline\tcreates\tan\tassignment\toperation\tthat will\tassign\tthe\tclipped\tweights\tto\tthe\tweights\tvariable:\n\nthreshold\t=\t1.0 weights\t=\ttf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\") clipped_weights\t=\ttf.clip_by_norm(weights,\tclip_norm=threshold,\taxes=1) clip_weights\t=\ttf.assign(weights,\tclipped_weights)\n\nThen\tyou\tjust\tapply\tthis\toperation\tafter\teach\ttraining\tstep,\tlike\tso:\n\nsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) clip_weights.eval()\n\nIn\tgeneral,\tyou\twould\tdo\tthis\tfor\tevery\thidden\tlayer.\tAlthough\tthis\tsolution\tshould\twork\tfine,\tit\tis\ta\tbit messy.\tA\tcleaner\tsolution\tis\tto\tcreate\ta\tmax_norm_regularizer()\tfunction\tand\tuse\tit\tjust\tlike\tthe\tearlier l1_regularizer()\tfunction:\n\ndef\tmax_norm_regularizer(threshold,\taxes=1,\tname=\"max_norm\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcollection=\"max_norm\"): \t\t\t\tdef\tmax_norm(weights): \t\t\t\t\t\t\t\tclipped\t=\ttf.clip_by_norm(weights,\tclip_norm=threshold,\taxes=axes) \t\t\t\t\t\t\t\tclip_weights\t=\ttf.assign(weights,\tclipped,\tname=name) \t\t\t\t\t\t\t\ttf.add_to_collection(collection,\tclip_weights) \t\t\t\t\t\t\t\treturn\tNone\t\t#\tthere\tis\tno\tregularization\tloss\tterm \t\t\t\treturn\tmax_norm\n\nThis\tfunction\treturns\ta\tparametrized\tmax_norm()\tfunction\tthat\tyou\tcan\tuse\tlike\tany\tother\tregularizer:\n\nmax_norm_reg\t=\tmax_norm_regularizer(threshold=1.0)\n\nwith\ttf.name_scope(\"dnn\"): \t\t\t\thidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_regularizer=max_norm_reg,\tname=\"hidden1\") \t\t\t\thidden2\t=\ttf.layers.dense(hidden1,\tn_hidden2,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_regularizer=max_norm_reg,\tname=\"hidden2\") \t\t\t\tlogits\t=\ttf.layers.dense(hidden2,\tn_outputs,\tname=\"outputs\")\n\nNote\tthat\tmax-norm\tregularization\tdoes\tnot\trequire\tadding\ta\tregularization\tloss\tterm\tto\tyour\toverall\tloss function,\twhich\tis\twhy\tthe\tmax_norm()\tfunction\treturns\tNone.\tBut\tyou\tstill\tneed\tto\tbe\table\tto\trun\tthe clip_weights\toperations\tafter\teach\ttraining\tstep,\tso\tyou\tneed\tto\tbe\table\tto\tget\ta\thandle\ton\tthem.\tThis\tis why\tthe\tmax_norm()\tfunction\tadds\tthe\tclip_weights\toperation\tto\ta\tcollection\tof\tmax-norm\tclipping operations.\tYou\tneed\tto\tfetch\tthese\tclipping\toperations\tand\trun\tthem\tafter\teach\ttraining\tstep:\n\nclip_all_weights\t=\ttf.get_collection(\"max_norm\")\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tfor\titeration\tin\trange(mnist.train.num_examples\t//\tbatch_size): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(clip_all_weights)\n\nMuch\tcleaner\tcode,\tisn’t\tit?\n\nData\tAugmentation One\tlast\tregularization\ttechnique,\tdata\taugmentation,\tconsists\tof\tgenerating\tnew\ttraining\tinstances\tfrom existing\tones,\tartificially\tboosting\tthe\tsize\tof\tthe\ttraining\tset.\tThis\twill\treduce\toverfitting,\tmaking\tthis\ta regularization\ttechnique.\tThe\ttrick\tis\tto\tgenerate\trealistic\ttraining\tinstances;\tideally,\ta\thuman\tshould\tnot be\table\tto\ttell\twhich\tinstances\twere\tgenerated\tand\twhich\tones\twere\tnot.\tMoreover,\tsimply\tadding\twhite noise\twill\tnot\thelp;\tthe\tmodifications\tyou\tapply\tshould\tbe\tlearnable\t(white\tnoise\tis\tnot).\n\nFor\texample,\tif\tyour\tmodel\tis\tmeant\tto\tclassify\tpictures\tof\tmushrooms,\tyou\tcan\tslightly\tshift,\trotate,\tand resize\tevery\tpicture\tin\tthe\ttraining\tset\tby\tvarious\tamounts\tand\tadd\tthe\tresulting\tpictures\tto\tthe\ttraining\tset (see\tFigure\t11-10).\tThis\tforces\tthe\tmodel\tto\tbe\tmore\ttolerant\tto\tthe\tposition,\torientation,\tand\tsize\tof\tthe mushrooms\tin\tthe\tpicture.\tIf\tyou\twant\tthe\tmodel\tto\tbe\tmore\ttolerant\tto\tlighting\tconditions,\tyou\tcan similarly\tgenerate\tmany\timages\twith\tvarious\tcontrasts.\tAssuming\tthe\tmushrooms\tare\tsymmetrical,\tyou can\talso\tflip\tthe\tpictures\thorizontally.\tBy\tcombining\tthese\ttransformations\tyou\tcan\tgreatly\tincrease\tthe size\tof\tyour\ttraining\tset.\n\nFigure\t11-10.\tGenerating\tnew\ttraining\tinstances\tfrom\texisting\tones\n\nIt\tis\toften\tpreferable\tto\tgenerate\ttraining\tinstances\ton\tthe\tfly\tduring\ttraining\trather\tthan\twasting\tstorage space\tand\tnetwork\tbandwidth.\tTensorFlow\toffers\tseveral\timage\tmanipulation\toperations\tsuch\tas transposing\t(shifting),\trotating,\tresizing,\tflipping,\tand\tcropping,\tas\twell\tas\tadjusting\tthe\tbrightness, contrast,\tsaturation,\tand\thue\t(see\tthe\tAPI\tdocumentation\tfor\tmore\tdetails).\tThis\tmakes\tit\teasy\tto implement\tdata\taugmentation\tfor\timage\tdatasets.\n\nNOTE\n\nAnother\tpowerful\ttechnique\tto\ttrain\tvery\tdeep\tneural\tnetworks\tis\tto\tadd\tskip\tconnections\t(a\tskip\tconnection\tis\twhen\tyou\tadd the\tinput\tof\ta\tlayer\tto\tthe\toutput\tof\ta\thigher\tlayer).\tWe\twill\texplore\tthis\tidea\tin\tChapter\t13\twhen\twe\ttalk\tabout\tdeep\tresidual networks.\n\nPractical\tGuidelines In\tthis\tchapter,\twe\thave\tcovered\ta\twide\trange\tof\ttechniques\tand\tyou\tmay\tbe\twondering\twhich\tones\tyou should\tuse.\tThe\tconfiguration\tin\tTable\t11-2\twill\twork\tfine\tin\tmost\tcases.\n\nTable\t11-2.\tDefault\tDNN\tconfiguration\n\nInitialization\n\nHe\tinitialization\n\nActivation\tfunction\n\nELU\n\nNormalization\n\nBatch\tNormalization\n\nRegularization\n\nDropout\n\nOptimizer\n\nNesterov\tAccelerated\tGradient\n\nLearning\trate\tschedule\n\nNone\n\nOf\tcourse,\tyou\tshould\ttry\tto\treuse\tparts\tof\ta\tpretrained\tneural\tnetwork\tif\tyou\tcan\tfind\tone\tthat\tsolves\ta similar\tproblem.\n\nThis\tdefault\tconfiguration\tmay\tneed\tto\tbe\ttweaked:\n\nIf\tyou\tcan’t\tfind\ta\tgood\tlearning\trate\t(convergence\twas\ttoo\tslow,\tso\tyou\tincreased\tthe\ttraining\trate, and\tnow\tconvergence\tis\tfast\tbut\tthe\tnetwork’s\taccuracy\tis\tsuboptimal),\tthen\tyou\tcan\ttry\tadding\ta learning\tschedule\tsuch\tas\texponential\tdecay.\n\nIf\tyour\ttraining\tset\tis\ta\tbit\ttoo\tsmall,\tyou\tcan\timplement\tdata\taugmentation.\n\nIf\tyou\tneed\ta\tsparse\tmodel,\tyou\tcan\tadd\tsome\tℓ1\tregularization\tto\tthe\tmix\t(and\toptionally\tzero\tout the\ttiny\tweights\tafter\ttraining).\tIf\tyou\tneed\tan\teven\tsparser\tmodel,\tyou\tcan\ttry\tusing\tFTRL\tinstead\tof Adam\toptimization,\talong\twith\tℓ1\tregularization.\n\nIf\tyou\tneed\ta\tlightning-fast\tmodel\tat\truntime,\tyou\tmay\twant\tto\tdrop\tBatch\tNormalization,\tand possibly\treplace\tthe\tELU\tactivation\tfunction\twith\tthe\tleaky\tReLU.\tHaving\ta\tsparse\tmodel\twill\talso help.\n\nWith\tthese\tguidelines,\tyou\tare\tnow\tready\tto\ttrain\tvery\tdeep\tnets\t—\twell,\tif\tyou\tare\tvery\tpatient,\tthat\tis!\tIf you\tuse\ta\tsingle\tmachine,\tyou\tmay\thave\tto\twait\tfor\tdays\tor\teven\tmonths\tfor\ttraining\tto\tcomplete.\tIn\tthe next\tchapter\twe\twill\tdiscuss\thow\tto\tuse\tdistributed\tTensorFlow\tto\ttrain\tand\trun\tmodels\tacross\tmany servers\tand\tGPUs.\n\nExercises\n\n1.\t Is\tit\tokay\tto\tinitialize\tall\tthe\tweights\tto\tthe\tsame\tvalue\tas\tlong\tas\tthat\tvalue\tis\tselected\trandomly using\tHe\tinitialization?\n\n2.\t Is\tit\tokay\tto\tinitialize\tthe\tbias\tterms\tto\t0?\n\n3.\t Name\tthree\tadvantages\tof\tthe\tELU\tactivation\tfunction\tover\tReLU.\n\n4.\t In\twhich\tcases\twould\tyou\twant\tto\tuse\teach\tof\tthe\tfollowing\tactivation\tfunctions:\tELU,\tleaky\tReLU (and\tits\tvariants),\tReLU,\ttanh,\tlogistic,\tand\tsoftmax?\n\n5.\t What\tmay\thappen\tif\tyou\tset\tthe\tmomentum\thyperparameter\ttoo\tclose\tto\t1\t(e.g.,\t0.99999)\twhen\tusing a\tMomentumOptimizer?\n\n6.\t Name\tthree\tways\tyou\tcan\tproduce\ta\tsparse\tmodel.\n\n7.\t Does\tdropout\tslow\tdown\ttraining?\tDoes\tit\tslow\tdown\tinference\t(i.e.,\tmaking\tpredictions\ton\tnew instances)?\n\n8.\t Deep\tLearning.\n\na.\t Build\ta\tDNN\twith\tfive\thidden\tlayers\tof\t100\tneurons\teach,\tHe\tinitialization,\tand\tthe\tELU\n\nactivation\tfunction.\n\nb.\t Using\tAdam\toptimization\tand\tearly\tstopping,\ttry\ttraining\tit\ton\tMNIST\tbut\tonly\ton\tdigits\t0\tto\t4, as\twe\twill\tuse\ttransfer\tlearning\tfor\tdigits\t5\tto\t9\tin\tthe\tnext\texercise.\tYou\twill\tneed\ta\tsoftmax output\tlayer\twith\tfive\tneurons,\tand\tas\talways\tmake\tsure\tto\tsave\tcheckpoints\tat\tregular\tintervals and\tsave\tthe\tfinal\tmodel\tso\tyou\tcan\treuse\tit\tlater.\n\nc.\t Tune\tthe\thyperparameters\tusing\tcross-validation\tand\tsee\twhat\tprecision\tyou\tcan\tachieve.\n\nd.\t Now\ttry\tadding\tBatch\tNormalization\tand\tcompare\tthe\tlearning\tcurves:\tis\tit\tconverging\tfaster\n\nthan\tbefore?\tDoes\tit\tproduce\ta\tbetter\tmodel?\n\ne.\t Is\tthe\tmodel\toverfitting\tthe\ttraining\tset?\tTry\tadding\tdropout\tto\tevery\tlayer\tand\ttry\tagain.\tDoes\tit\n\nhelp?\n\n9.\t Transfer\tlearning.\n\na.\t Create\ta\tnew\tDNN\tthat\treuses\tall\tthe\tpretrained\thidden\tlayers\tof\tthe\tprevious\tmodel,\tfreezes\n\nthem,\tand\treplaces\tthe\tsoftmax\toutput\tlayer\twith\ta\tnew\tone.\n\nb.\t Train\tthis\tnew\tDNN\ton\tdigits\t5\tto\t9,\tusing\tonly\t100\timages\tper\tdigit,\tand\ttime\thow\tlong\tit\n\ntakes.\tDespite\tthis\tsmall\tnumber\tof\texamples,\tcan\tyou\tachieve\thigh\tprecision?\n\nc.\t Try\tcaching\tthe\tfrozen\tlayers,\tand\ttrain\tthe\tmodel\tagain:\thow\tmuch\tfaster\tis\tit\tnow?\n\nd.\t Try\tagain\treusing\tjust\tfour\thidden\tlayers\tinstead\tof\tfive.\tCan\tyou\tachieve\ta\thigher\tprecision?\n\ne.\t Now\tunfreeze\tthe\ttop\ttwo\thidden\tlayers\tand\tcontinue\ttraining:\tcan\tyou\tget\tthe\tmodel\tto\tperform\n\neven\tbetter?\n\n10.\t Pretraining\ton\tan\tauxiliary\ttask.\n\na.\t In\tthis\texercise\tyou\twill\tbuild\ta\tDNN\tthat\tcompares\ttwo\tMNIST\tdigit\timages\tand\tpredicts whether\tthey\trepresent\tthe\tsame\tdigit\tor\tnot.\tThen\tyou\twill\treuse\tthe\tlower\tlayers\tof\tthis network\tto\ttrain\tan\tMNIST\tclassifier\tusing\tvery\tlittle\ttraining\tdata.\tStart\tby\tbuilding\ttwo\tDNNs (let’s\tcall\tthem\tDNN\tA\tand\tB),\tboth\tsimilar\tto\tthe\tone\tyou\tbuilt\tearlier\tbut\twithout\tthe\toutput layer:\teach\tDNN\tshould\thave\tfive\thidden\tlayers\tof\t100\tneurons\teach,\tHe\tinitialization,\tand ELU\tactivation.\tNext,\tadd\tone\tmore\thidden\tlayer\twith\t10\tunits\ton\ttop\tof\tboth\tDNNs.\tTo\tdo\tthis, you\tshould\tuse\tTensorFlow’s\tconcat()\tfunction\twith\taxis=1\tto\tconcatenate\tthe\toutputs\tof\tboth DNNs\tfor\teach\tinstance,\tthen\tfeed\tthe\tresult\tto\tthe\thidden\tlayer.\tFinally,\tadd\tan\toutput\tlayer with\ta\tsingle\tneuron\tusing\tthe\tlogistic\tactivation\tfunction.\n\nb.\t Split\tthe\tMNIST\ttraining\tset\tin\ttwo\tsets:\tsplit\t#1\tshould\tcontaining\t55,000\timages,\tand\tsplit\t#2 should\tcontain\tcontain\t5,000\timages.\tCreate\ta\tfunction\tthat\tgenerates\ta\ttraining\tbatch\twhere each\tinstance\tis\ta\tpair\tof\tMNIST\timages\tpicked\tfrom\tsplit\t#1.\tHalf\tof\tthe\ttraining\tinstances should\tbe\tpairs\tof\timages\tthat\tbelong\tto\tthe\tsame\tclass,\twhile\tthe\tother\thalf\tshould\tbe\timages from\tdifferent\tclasses.\tFor\teach\tpair,\tthe\ttraining\tlabel\tshould\tbe\t0\tif\tthe\timages\tare\tfrom\tthe same\tclass,\tor\t1\tif\tthey\tare\tfrom\tdifferent\tclasses.\n\nc.\t Train\tthe\tDNN\ton\tthis\ttraining\tset.\tFor\teach\timage\tpair,\tyou\tcan\tsimultaneously\tfeed\tthe\tfirst image\tto\tDNN\tA\tand\tthe\tsecond\timage\tto\tDNN\tB.\tThe\twhole\tnetwork\twill\tgradually\tlearn\tto tell\twhether\ttwo\timages\tbelong\tto\tthe\tsame\tclass\tor\tnot.\n\nd.\t Now\tcreate\ta\tnew\tDNN\tby\treusing\tand\tfreezing\tthe\thidden\tlayers\tof\tDNN\tA\tand\tadding\ta\n\nsoftmax\toutput\tlayer\ton\ttop\twith\t10\tneurons.\tTrain\tthis\tnetwork\ton\tsplit\t#2\tand\tsee\tif\tyou\tcan achieve\thigh\tperformance\tdespite\thaving\tonly\t500\timages\tper\tclass.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“Understanding\tthe\tDifficulty\tof\tTraining\tDeep\tFeedforward\tNeural\tNetworks,”\tX.\tGlorot,\tY\tBengio\t(2010).\n\n2\n\nHere’s\tan\tanalogy:\tif\tyou\tset\ta\tmicrophone\tamplifier’s\tknob\ttoo\tclose\tto\tzero,\tpeople\twon’t\thear\tyour\tvoice,\tbut\tif\tyou\tset\tit\ttoo\tclose\tto the\tmax,\tyour\tvoice\twill\tbe\tsaturated\tand\tpeople\twon’t\tunderstand\twhat\tyou\tare\tsaying.\tNow\timagine\ta\tchain\tof\tsuch\tamplifiers:\tthey\tall need\tto\tbe\tset\tproperly\tin\torder\tfor\tyour\tvoice\tto\tcome\tout\tloud\tand\tclear\tat\tthe\tend\tof\tthe\tchain.\tYour\tvoice\thas\tto\tcome\tout\tof\teach amplifier\tat\tthe\tsame\tamplitude\tas\tit\tcame\tin.\n\n3\n\nThis\tsimplified\tstrategy\twas\tactually\talready\tproposed\tmuch\tearlier\t—\tfor\texample,\tin\tthe\t1998\tbook\tNeural\tNetworks:\tTricks\tof\tthe Trade\tby\tGenevieve\tOrr\tand\tKlaus-Robert\tMüller\t(Springer).\n\n4\n\nSuch\tas\t“Delving\tDeep\tinto\tRectifiers:\tSurpassing\tHuman-Level\tPerformance\ton\tImageNet\tClassification,”\tK.\tHe\tet\tal.\t(2015).\n\n5\n\n“Empirical\tEvaluation\tof\tRectified\tActivations\tin\tConvolution\tNetwork,”\tB.\tXu\tet\tal.\t(2015).\n\n6\n\n“Fast\tand\tAccurate\tDeep\tNetwork\tLearning\tby\tExponential\tLinear\tUnits\t(ELUs),”\tD.\tClevert,\tT.\tUnterthiner,\tS.\tHochreiter\t(2015).\n\n7\n\n“Batch\tNormalization:\tAccelerating\tDeep\tNetwork\tTraining\tby\tReducing\tInternal\tCovariate\tShift,”\tS.\tIoffe\tand\tC.\tSzegedy\t(2015).\n\n8\n\nMany\tresearchers\targue\tthat\tit\tis\tjust\tas\tgood,\tor\teven\tbetter,\tto\tplace\tthe\tbatch\tnormalization\tlayers\tafter\t(rather\tthan\tbefore)\tthe activations.\n\n9\n\n“On\tthe\tdifficulty\tof\ttraining\trecurrent\tneural\tnetworks,”\tR.\tPascanu\tet\tal.\t(2013).\n\n10\n\nAnother\toption\tis\tto\tcome\tup\twith\ta\tsupervised\ttask\tfor\twhich\tyou\tcan\teasily\tgather\ta\tlot\tof\tlabeled\ttraining\tdata,\tthen\tuse\ttransfer\n\n11\n\n12\n\n2\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\nlearning,\tas\texplained\tearlier.\tFor\texample,\tif\tyou\twant\tto\ttrain\ta\tmodel\tto\tidentify\tyour\tfriends\tin\tpictures,\tyou\tcould\tdownload\tmillions\tof faces\ton\tthe\tinternet\tand\ttrain\ta\tclassifier\tto\tdetect\twhether\ttwo\tfaces\tare\tidentical\tor\tnot,\tthen\tuse\tthis\tclassifier\tto\tcompare\ta\tnew picture\twith\teach\tpicture\tof\tyour\tfriends.\n\n“Some\tmethods\tof\tspeeding\tup\tthe\tconvergence\tof\titeration\tmethods,”\tB.\tPolyak\t(1964).\n\n“A\tMethod\tfor\tUnconstrained\tConvex\tMinimization\tProblem\twith\tthe\tRate\tof\tConvergence\tO(1/k\n\n),”\tYurii\tNesterov\t(1983).\n\n“Adaptive\tSubgradient\tMethods\tfor\tOnline\tLearning\tand\tStochastic\tOptimization,”\tJ.\tDuchi\tet\tal.\t(2011).\n\nThis\talgorithm\twas\tcreated\tby\tTijmen\tTieleman\tand\tGeoffrey\tHinton\tin\t2012,\tand\tpresented\tby\tGeoffrey\tHinton\tin\this\tCoursera\tclass\ton neural\tnetworks\t(slides:\thttp://goo.gl/RsQeis;\tvideo:\thttps://goo.gl/XUbIyJ).\tAmusingly,\tsince\tthe\tauthors\thave\tnot\twritten\ta\tpaper\tto describe\tit,\tresearchers\toften\tcite\t“slide\t29\tin\tlecture\t6”\tin\ttheir\tpapers.\n\n“Adam:\tA\tMethod\tfor\tStochastic\tOptimization,”\tD.\tKingma,\tJ.\tBa\t(2015).\n\nThese\tare\testimations\tof\tthe\tmean\tand\t(uncentered)\tvariance\tof\tthe\tgradients.\tThe\tmean\tis\toften\tcalled\tthe\tfirst\tmoment,\twhile\tthe variance\tis\toften\tcalled\tthe\tsecond\tmoment,\thence\tthe\tname\tof\tthe\talgorithm.\n\n“The\tMarginal\tValue\tof\tAdaptive\tGradient\tMethods\tin\tMachine\tLearning,”\tA.\tC.\tWilson\tet\tal.\t(2017).\n\n“Primal-Dual\tSubgradient\tMethods\tfor\tConvex\tProblems,”\tYurii\tNesterov\t(2005).\n\n“Ad\tClick\tPrediction:\ta\tView\tfrom\tthe\tTrenches,”\tH.\tMcMahan\tet\tal.\t(2013).\n\n“An\tEmpirical\tStudy\tof\tLearning\tRates\tin\tDeep\tNeural\tNetworks\tfor\tSpeech\tRecognition,”\tA.\tSenior\tet\tal.\t(2013).\n\n“Improving\tneural\tnetworks\tby\tpreventing\tco-adaptation\tof\tfeature\tdetectors,”\tG.\tHinton\tet\tal.\t(2012).\n\n“Dropout:\tA\tSimple\tWay\tto\tPrevent\tNeural\tNetworks\tfrom\tOverfitting,”\tN.\tSrivastava\tet\tal.\t(2014).\n\nChapter\t12.\tDistributing\tTensorFlow\tAcross Devices\tand\tServers\n\nIn\tChapter\t11\twe\tdiscussed\tseveral\ttechniques\tthat\tcan\tconsiderably\tspeed\tup\ttraining:\tbetter\tweight initialization,\tBatch\tNormalization,\tsophisticated\toptimizers,\tand\tso\ton.\tHowever,\teven\twith\tall\tof\tthese techniques,\ttraining\ta\tlarge\tneural\tnetwork\ton\ta\tsingle\tmachine\twith\ta\tsingle\tCPU\tcan\ttake\tdays\tor\teven weeks.\n\nIn\tthis\tchapter\twe\twill\tsee\thow\tto\tuse\tTensorFlow\tto\tdistribute\tcomputations\tacross\tmultiple\tdevices (CPUs\tand\tGPUs)\tand\trun\tthem\tin\tparallel\t(see\tFigure\t12-1).\tFirst\twe\twill\tdistribute\tcomputations across\tmultiple\tdevices\ton\tjust\tone\tmachine,\tthen\ton\tmultiple\tdevices\tacross\tmultiple\tmachines.\n\nFigure\t12-1.\tExecuting\ta\tTensorFlow\tgraph\tacross\tmultiple\tdevices\tin\tparallel\n\nTensorFlow’s\tsupport\tof\tdistributed\tcomputing\tis\tone\tof\tits\tmain\thighlights\tcompared\tto\tother\tneural network\tframeworks.\tIt\tgives\tyou\tfull\tcontrol\tover\thow\tto\tsplit\t(or\treplicate)\tyour\tcomputation\tgraph across\tdevices\tand\tservers,\tand\tit\tlets\tyou\tparallelize\tand\tsynchronize\toperations\tin\tflexible\tways\tso\tyou can\tchoose\tbetween\tall\tsorts\tof\tparallelization\tapproaches.\n\nWe\twill\tlook\tat\tsome\tof\tthe\tmost\tpopular\tapproaches\tto\tparallelizing\tthe\texecution\tand\ttraining\tof\ta neural\tnetwork.\tInstead\tof\twaiting\tfor\tweeks\tfor\ta\ttraining\talgorithm\tto\tcomplete,\tyou\tmay\tend\tup\twaiting for\tjust\ta\tfew\thours.\tNot\tonly\tdoes\tthis\tsave\tan\tenormous\tamount\tof\ttime,\tit\talso\tmeans\tthat\tyou\tcan experiment\twith\tvarious\tmodels\tmuch\tmore\teasily,\tand\tfrequently\tretrain\tyour\tmodels\ton\tfresh\tdata.\n\nOther\tgreat\tuse\tcases\tof\tparallelization\tinclude\texploring\ta\tmuch\tlarger\thyperparameter\tspace\twhen\tfine- tuning\tyour\tmodel,\tand\trunning\tlarge\tensembles\tof\tneural\tnetworks\tefficiently.\n\nBut\twe\tmust\tlearn\tto\twalk\tbefore\twe\tcan\trun.\tLet’s\tstart\tby\tparallelizing\tsimple\tgraphs\tacross\tseveral GPUs\ton\ta\tsingle\tmachine.\n\nMultiple\tDevices\ton\ta\tSingle\tMachine You\tcan\toften\tget\ta\tmajor\tperformance\tboost\tsimply\tby\tadding\tGPU\tcards\tto\ta\tsingle\tmachine.\tIn\tfact,\tin many\tcases\tthis\twill\tsuffice;\tyou\twon’t\tneed\tto\tuse\tmultiple\tmachines\tat\tall.\tFor\texample,\tyou\tcan typically\ttrain\ta\tneural\tnetwork\tjust\tas\tfast\tusing\t8\tGPUs\ton\ta\tsingle\tmachine\trather\tthan\t16\tGPUs\tacross multiple\tmachines\t(due\tto\tthe\textra\tdelay\timposed\tby\tnetwork\tcommunications\tin\ta\tmultimachine\tsetup).\n\nIn\tthis\tsection\twe\twill\tlook\tat\thow\tto\tset\tup\tyour\tenvironment\tso\tthat\tTensorFlow\tcan\tuse\tmultiple\tGPU cards\ton\tone\tmachine.\tThen\twe\twill\tlook\tat\thow\tyou\tcan\tdistribute\toperations\tacross\tavailable\tdevices and\texecute\tthem\tin\tparallel.",
      "page_number": 343
    },
    {
      "number": 12,
      "title": "Distributing\tTensorFlow\tAcross Devices\tand\tServers",
      "start_page": 393,
      "end_page": 441,
      "detection_method": "regex_chapter_title",
      "content": "Installation In\torder\tto\trun\tTensorFlow\ton\tmultiple\tGPU\tcards,\tyou\tfirst\tneed\tto\tmake\tsure\tyour\tGPU\tcards\thave NVidia\tCompute\tCapability\t(greater\tor\tequal\tto\t3.0).\tThis\tincludes\tNvidia’s\tTitan,\tTitan\tX,\tK20,\tand K40\tcards\t(if\tyou\town\tanother\tcard,\tyou\tcan\tcheck\tits\tcompatibility\tat https://developer.nvidia.com/cuda-gpus).\n\nTIP\n\nIf\tyou\tdon’t\town\tany\tGPU\tcards,\tyou\tcan\tuse\ta\thosting\tservice\twith\tGPU\tcapability\tsuch\tas\tAmazon\tAWS.\tDetailed\tinstructions to\tset\tup\tTensorFlow\t0.9\twith\tPython\t3.5\ton\tan\tAmazon\tAWS\tGPU\tinstance\tare\tavailable\tin\tŽiga\tAvsec’s\thelpful\tblog\tpost.\tIt should\tnot\tbe\ttoo\thard\tto\tupdate\tit\tto\tthe\tlatest\tversion\tof\tTensorFlow.\tGoogle\talso\treleased\ta\tcloud\tservice\tcalled\tCloud Machine\tLearning\tto\trun\tTensorFlow\tgraphs.\tIn\tMay\t2016,\tthey\tannounced\tthat\ttheir\tplatform\tnow\tincludes\tservers\tequipped with\ttensor\tprocessing\tunits\t(TPUs),\tprocessors\tspecialized\tfor\tMachine\tLearning\tthat\tare\tmuch\tfaster\tthan\tGPUs\tfor\tmany ML\ttasks.\tOf\tcourse,\tanother\toption\tis\tsimply\tto\tbuy\tyour\town\tGPU\tcard.\tTim\tDettmers\twrote\ta\tgreat\tblog\tpost\tto\thelp\tyou choose,\tand\the\tupdates\tit\tfairly\tregularly.\n\nYou\tmust\tthen\tdownload\tand\tinstall\tthe\tappropriate\tversion\tof\tthe\tCUDA\tand\tcuDNN\tlibraries\t(CUDA 8.0\tand\tcuDNN\t5.1\tif\tyou\tare\tusing\tthe\tbinary\tinstallation\tof\tTensorFlow\t1.0.0),\tand\tset\ta\tfew environment\tvariables\tso\tTensorFlow\tknows\twhere\tto\tfind\tCUDA\tand\tcuDNN.\tThe\tdetailed\tinstallation instructions\tare\tlikely\tto\tchange\tfairly\tquickly,\tso\tit\tis\tbest\tthat\tyou\tfollow\tthe\tinstructions\ton TensorFlow’s\twebsite.\n\nNvidia’s\tCompute\tUnified\tDevice\tArchitecture\tlibrary\t(CUDA)\tallows\tdevelopers\tto\tuse\tCUDA- enabled\tGPUs\tfor\tall\tsorts\tof\tcomputations\t(not\tjust\tgraphics\tacceleration).\tNvidia’s\tCUDA\tDeep\tNeural Network\tlibrary\t(cuDNN)\tis\ta\tGPU-accelerated\tlibrary\tof\tprimitives\tfor\tDNNs.\tIt\tprovides\toptimized implementations\tof\tcommon\tDNN\tcomputations\tsuch\tas\tactivation\tlayers,\tnormalization,\tforward\tand backward\tconvolutions,\tand\tpooling\t(see\tChapter\t13).\tIt\tis\tpart\tof\tNvidia’s\tDeep\tLearning\tSDK\t(note that\tit\trequires\tcreating\tan\tNvidia\tdeveloper\taccount\tin\torder\tto\tdownload\tit).\tTensorFlow\tuses\tCUDA and\tcuDNN\tto\tcontrol\tthe\tGPU\tcards\tand\taccelerate\tcomputations\t(see\tFigure\t12-2).\n\nFigure\t12-2.\tTensorFlow\tuses\tCUDA\tand\tcuDNN\tto\tcontrol\tGPUs\tand\tboost\tDNNs\n\nYou\tcan\tuse\tthe\tnvidia-smi\tcommand\tto\tcheck\tthat\tCUDA\tis\tproperly\tinstalled.\tIt\tlists\tthe\tavailable GPU\tcards,\tas\twell\tas\tprocesses\trunning\ton\teach\tcard:\n\n$\tnvidia-smi Wed\tSep\t16\t09:50:03\t2016 +------------------------------------------------------+ |\tNVIDIA-SMI\t352.63\t\t\t\t\tDriver\tVersion:\t352.63\t\t\t\t\t\t\t\t\t| |-------------------------------+----------------------+----------------------+ |\tGPU\t\tName\t\t\t\t\t\t\t\tPersistence-M|\tBus-Id\t\t\t\t\t\t\t\tDisp.A\t|\tVolatile\tUncorr.\tECC\t| |\tFan\t\tTemp\t\tPerf\t\tPwr:Usage/Cap|\t\t\t\t\t\t\t\t\tMemory-Usage\t|\tGPU-Util\t\tCompute\tM.\t| |===============================+======================+======================| |\t\t\t0\t\tGRID\tK520\t\t\t\t\t\t\t\t\t\t\tOff\t\t|\t0000:00:03.0\t\t\t\t\tOff\t|\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tN/A\t| |\tN/A\t\t\t27C\t\t\t\tP8\t\t\t\t17W\t/\t125W\t|\t\t\t\t\t11MiB\t/\t\t4095MiB\t|\t\t\t\t\t\t0%\t\t\t\t\t\tDefault\t| +-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+ |\tProcesses:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tGPU\tMemory\t| |\t\tGPU\t\t\t\t\t\t\tPID\t\tType\t\tProcess\tname\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tUsage\t\t\t\t\t\t| |=============================================================================| |\t\tNo\trunning\tprocesses\tfound\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t| +-----------------------------------------------------------------------------+\n\nFinally,\tyou\tmust\tinstall\tTensorFlow\twith\tGPU\tsupport.\tIf\tyou\tcreated\tan\tisolated\tenvironment\tusing virtualenv,\tyou\tfirst\tneed\tto\tactivate\tit:\n\n$\tcd\t$ML_PATH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tYour\tML\tworking\tdirectory\t(e.g.,\t$HOME/ml) $\tsource\tenv/bin/activate\n\nThen\tinstall\tthe\tappropriate\tGPU-enabled\tversion\tof\tTensorFlow:\n\n$\tpip3\tinstall\t--upgrade\ttensorflow-gpu\n\nNow\tyou\tcan\topen\tup\ta\tPython\tshell\tand\tcheck\tthat\tTensorFlow\tdetects\tand\tuses\tCUDA\tand\tcuDNN properly\tby\timporting\tTensorFlow\tand\tcreating\ta\tsession:\n\n>>>\timport\ttensorflow\tas\ttf I\t[...]/dso_loader.cc:108]\tsuccessfully\topened\tCUDA\tlibrary\tlibcublas.so\tlocally I\t[...]/dso_loader.cc:108]\tsuccessfully\topened\tCUDA\tlibrary\tlibcudnn.so\tlocally I\t[...]/dso_loader.cc:108]\tsuccessfully\topened\tCUDA\tlibrary\tlibcufft.so\tlocally I\t[...]/dso_loader.cc:108]\tsuccessfully\topened\tCUDA\tlibrary\tlibcuda.so.1\tlocally I\t[...]/dso_loader.cc:108]\tsuccessfully\topened\tCUDA\tlibrary\tlibcurand.so\tlocally >>>\tsess\t=\ttf.Session() [...] I\t[...]/gpu_init.cc:102]\tFound\tdevice\t0\twith\tproperties: name:\tGRID\tK520 major:\t3\tminor:\t0\tmemoryClockRate\t(GHz)\t0.797 pciBusID\t0000:00:03.0 Total\tmemory:\t4.00GiB Free\tmemory:\t3.95GiB I\t[...]/gpu_init.cc:126]\tDMA:\t0 I\t[...]/gpu_init.cc:136]\t0:\t\t\tY I\t[...]/gpu_device.cc:839]\tCreating\tTensorFlow\tdevice (/gpu:0)\t->\t(device:\t0,\tname:\tGRID\tK520,\tpci\tbus\tid:\t0000:00:03.0)\n\nLooks\tgood!\tTensorFlow\tdetected\tthe\tCUDA\tand\tcuDNN\tlibraries,\tand\tit\tused\tthe\tCUDA\tlibrary\tto detect\tthe\tGPU\tcard\t(in\tthis\tcase\tan\tNvidia\tGrid\tK520\tcard).\n\nManaging\tthe\tGPU\tRAM By\tdefault\tTensorFlow\tautomatically\tgrabs\tall\tthe\tRAM\tin\tall\tavailable\tGPUs\tthe\tfirst\ttime\tyou\trun\ta graph,\tso\tyou\twill\tnot\tbe\table\tto\tstart\ta\tsecond\tTensorFlow\tprogram\twhile\tthe\tfirst\tone\tis\tstill\trunning.\tIf you\ttry,\tyou\twill\tget\tthe\tfollowing\terror:\n\nE\t[...]/cuda_driver.cc:965]\tfailed\tto\tallocate\t3.66G\t(3928915968\tbytes)\tfrom device:\tCUDA_ERROR_OUT_OF_MEMORY\n\nOne\tsolution\tis\tto\trun\teach\tprocess\ton\tdifferent\tGPU\tcards.\tTo\tdo\tthis,\tthe\tsimplest\toption\tis\tto\tset\tthe CUDA_VISIBLE_DEVICES\tenvironment\tvariable\tso\tthat\teach\tprocess\tonly\tsees\tthe\tappropriate\tGPU\tcards. For\texample,\tyou\tcould\tstart\ttwo\tprograms\tlike\tthis:\n\n$\tCUDA_VISIBLE_DEVICES=0,1\tpython3\tprogram_1.py #\tand\tin\tanother\tterminal: $\tCUDA_VISIBLE_DEVICES=3,2\tpython3\tprogram_2.py\n\nProgram\t#1\twill\tonly\tsee\tGPU\tcards\t0\tand\t1\t(numbered\t0\tand\t1,\trespectively),\tand\tprogram\t#2\twill\tonly see\tGPU\tcards\t2\tand\t3\t(numbered\t1\tand\t0,\trespectively).\tEverything\twill\twork\tfine\t(see\tFigure\t12-3).\n\nFigure\t12-3.\tEach\tprogram\tgets\ttwo\tGPUs\tfor\titself\n\nAnother\toption\tis\tto\ttell\tTensorFlow\tto\tgrab\tonly\ta\tfraction\tof\tthe\tmemory.\tFor\texample,\tto\tmake TensorFlow\tgrab\tonly\t40%\tof\teach\tGPU’s\tmemory,\tyou\tmust\tcreate\ta\tConfigProto\tobject,\tset\tits gpu_options.per_process_gpu_memory_fraction\toption\tto\t0.4,\tand\tcreate\tthe\tsession\tusing\tthis configuration:\n\nconfig\t=\ttf.ConfigProto() config.gpu_options.per_process_gpu_memory_fraction\t=\t0.4 session\t=\ttf.Session(config=config)\n\nNow\ttwo\tprograms\tlike\tthis\tone\tcan\trun\tin\tparallel\tusing\tthe\tsame\tGPU\tcards\t(but\tnot\tthree,\tsince\t3\t× 0.4\t>\t1).\tSee\tFigure\t12-4.\n\nFigure\t12-4.\tEach\tprogram\tgets\tall\tfour\tGPUs,\tbut\twith\tonly\t40%\tof\tthe\tRAM\teach\n\nIf\tyou\trun\tthe\tnvidia-smi\tcommand\twhile\tboth\tprograms\tare\trunning,\tyou\tshould\tsee\tthat\teach\tprocess holds\troughly\t40%\tof\tthe\ttotal\tRAM\tof\teach\tcard:\n\n$\tnvidia-smi [...] +-----------------------------------------------------------------------------+ |\tProcesses:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tGPU\tMemory\t| |\t\tGPU\t\t\t\t\t\t\tPID\t\tType\t\tProcess\tname\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tUsage\t\t\t\t\t\t| |=============================================================================| |\t\t\t\t0\t\t\t\t\t\t5231\t\t\t\tC\t\t\tpython\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1677MiB\t| |\t\t\t\t0\t\t\t\t\t\t5262\t\t\t\tC\t\t\tpython\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1677MiB\t| |\t\t\t\t1\t\t\t\t\t\t5231\t\t\t\tC\t\t\tpython\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1677MiB\t| |\t\t\t\t1\t\t\t\t\t\t5262\t\t\t\tC\t\t\tpython\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1677MiB\t| [...]\n\nYet\tanother\toption\tis\tto\ttell\tTensorFlow\tto\tgrab\tmemory\tonly\twhen\tit\tneeds\tit.\tTo\tdo\tthis\tyou\tmust\tset config.gpu_options.allow_growth\tto\tTrue.\tHowever,\tTensorFlow\tnever\treleases\tmemory\tonce\tit has\tgrabbed\tit\t(to\tavoid\tmemory\tfragmentation)\tso\tyou\tmay\tstill\trun\tout\tof\tmemory\tafter\ta\twhile.\tIt\tmay be\tharder\tto\tguarantee\ta\tdeterministic\tbehavior\tusing\tthis\toption,\tso\tin\tgeneral\tyou\tprobably\twant\tto\tstick with\tone\tof\tthe\tprevious\toptions.\n\nOkay,\tnow\tyou\thave\ta\tworking\tGPU-enabled\tTensorFlow\tinstallation.\tLet’s\tsee\thow\tto\tuse\tit!\n\nPlacing\tOperations\ton\tDevices The\tTensorFlow\twhitepaper1\tpresents\ta\tfriendly\tdynamic\tplacer\talgorithm\tthat\tautomagically\tdistributes operations\tacross\tall\tavailable\tdevices,\ttaking\tinto\taccount\tthings\tlike\tthe\tmeasured\tcomputation\ttime\tin previous\truns\tof\tthe\tgraph,\testimations\tof\tthe\tsize\tof\tthe\tinput\tand\toutput\ttensors\tto\teach\toperation,\tthe amount\tof\tRAM\tavailable\tin\teach\tdevice,\tcommunication\tdelay\twhen\ttransferring\tdata\tin\tand\tout\tof devices,\thints\tand\tconstraints\tfrom\tthe\tuser,\tand\tmore.\tUnfortunately,\tthis\tsophisticated\talgorithm\tis internal\tto\tGoogle;\tit\twas\tnot\treleased\tin\tthe\topen\tsource\tversion\tof\tTensorFlow.\tThe\treason\tit\twas\tleft out\tseems\tto\tbe\tthat\tin\tpractice\ta\tsmall\tset\tof\tplacement\trules\tspecified\tby\tthe\tuser\tactually\tresults\tin more\tefficient\tplacement\tthan\twhat\tthe\tdynamic\tplacer\tis\tcapable\tof.\tHowever,\tthe\tTensorFlow\tteam\tis working\ton\timproving\tthe\tdynamic\tplacer,\tand\tperhaps\tit\twill\teventually\tbe\tgood\tenough\tto\tbe\treleased.\n\nUntil\tthen\tTensorFlow\trelies\ton\tthe\tsimple\tplacer,\twhich\t(as\tits\tname\tsuggests)\tis\tvery\tbasic.\n\nSimple\tplacement\n\nWhenever\tyou\trun\ta\tgraph,\tif\tTensorFlow\tneeds\tto\tevaluate\ta\tnode\tthat\tis\tnot\tplaced\ton\ta\tdevice\tyet,\tit uses\tthe\tsimple\tplacer\tto\tplace\tit,\talong\twith\tall\tother\tnodes\tthat\tare\tnot\tplaced\tyet.\tThe\tsimple\tplacer respects\tthe\tfollowing\trules:\n\nIf\ta\tnode\twas\talready\tplaced\ton\ta\tdevice\tin\ta\tprevious\trun\tof\tthe\tgraph,\tit\tis\tleft\ton\tthat\tdevice.\n\nElse,\tif\tthe\tuser\tpinned\ta\tnode\tto\ta\tdevice\t(described\tnext),\tthe\tplacer\tplaces\tit\ton\tthat\tdevice.\n\nElse,\tit\tdefaults\tto\tGPU\t#0,\tor\tthe\tCPU\tif\tthere\tis\tno\tGPU.\n\nAs\tyou\tcan\tsee,\tplacing\toperations\ton\tthe\tappropriate\tdevice\tis\tmostly\tup\tto\tyou.\tIf\tyou\tdon’t\tdo\tanything, the\twhole\tgraph\twill\tbe\tplaced\ton\tthe\tdefault\tdevice.\tTo\tpin\tnodes\tonto\ta\tdevice,\tyou\tmust\tcreate\ta device\tblock\tusing\tthe\tdevice()\tfunction.\tFor\texample,\tthe\tfollowing\tcode\tpins\tthe\tvariable\ta\tand\tthe constant\tb\ton\tthe\tCPU,\tbut\tthe\tmultiplication\tnode\tc\tis\tnot\tpinned\ton\tany\tdevice,\tso\tit\twill\tbe\tplaced\ton the\tdefault\tdevice:\n\nwith\ttf.device(\"/cpu:0\"): \t\t\t\ta\t=\ttf.Variable(3.0) \t\t\t\tb\t=\ttf.constant(4.0)\n\nc\t=\ta\t*\tb\n\nNOTE\n\nThe\t\"/cpu:0\"\tdevice\taggregates\tall\tCPUs\ton\ta\tmulti-CPU\tsystem.\tThere\tis\tcurrently\tno\tway\tto\tpin\tnodes\ton\tspecific\tCPUs\tor to\tuse\tjust\ta\tsubset\tof\tall\tCPUs.\n\nLogging\tplacements\n\nLet’s\tcheck\tthat\tthe\tsimple\tplacer\trespects\tthe\tplacement\tconstraints\twe\thave\tjust\tdefined.\tFor\tthis\tyou can\tset\tthe\tlog_device_placement\toption\tto\tTrue;\tthis\ttells\tthe\tplacer\tto\tlog\ta\tmessage\twhenever\tit\n\nplaces\ta\tnode.\tFor\texample:\n\n>>>\tconfig\t=\ttf.ConfigProto() >>>\tconfig.log_device_placement\t=\tTrue >>>\tsess\t=\ttf.Session(config=config) I\t[...]\tCreating\tTensorFlow\tdevice\t(/gpu:0)\t->\t(device:\t0,\tname:\tGRID\tK520, pci\tbus\tid:\t0000:00:03.0) [...] >>>\tx.initializer.run(session=sess) I\t[...]\ta:\t/job:localhost/replica:0/task:0/cpu:0 I\t[...]\ta/read:\t/job:localhost/replica:0/task:0/cpu:0 I\t[...]\tmul:\t/job:localhost/replica:0/task:0/gpu:0 I\t[...]\ta/Assign:\t/job:localhost/replica:0/task:0/cpu:0 I\t[...]\tb:\t/job:localhost/replica:0/task:0/cpu:0 I\t[...]\ta/initial_value:\t/job:localhost/replica:0/task:0/cpu:0 >>>\tsess.run(c) 12\n\nThe\tlines\tstarting\twith\t\"I\"\tfor\tInfo\tare\tthe\tlog\tmessages.\tWhen\twe\tcreate\ta\tsession,\tTensorFlow\tlogs\ta message\tto\ttell\tus\tthat\tit\thas\tfound\ta\tGPU\tcard\t(in\tthis\tcase\tthe\tGrid\tK520\tcard).\tThen\tthe\tfirst\ttime\twe run\tthe\tgraph\t(in\tthis\tcase\twhen\tinitializing\tthe\tvariable\ta),\tthe\tsimple\tplacer\tis\trun\tand\tplaces\teach\tnode on\tthe\tdevice\tit\twas\tassigned\tto.\tAs\texpected,\tthe\tlog\tmessages\tshow\tthat\tall\tnodes\tare\tplaced\ton \"/cpu:0\"\texcept\tthe\tmultiplication\tnode,\twhich\tends\tup\ton\tthe\tdefault\tdevice\t\"/gpu:0\"\t(you\tcan\tsafely ignore\tthe\tprefix\t/job:localhost/replica:0/task:0\tfor\tnow;\twe\twill\ttalk\tabout\tit\tin\ta\tmoment). Notice\tthat\tthe\tsecond\ttime\twe\trun\tthe\tgraph\t(to\tcompute\tc),\tthe\tplacer\tis\tnot\tused\tsince\tall\tthe\tnodes TensorFlow\tneeds\tto\tcompute\tc\tare\talready\tplaced.\n\nDynamic\tplacement\tfunction\n\nWhen\tyou\tcreate\ta\tdevice\tblock,\tyou\tcan\tspecify\ta\tfunction\tinstead\tof\ta\tdevice\tname.\tTensorFlow\twill call\tthis\tfunction\tfor\teach\toperation\tit\tneeds\tto\tplace\tin\tthe\tdevice\tblock,\tand\tthe\tfunction\tmust\treturn\tthe name\tof\tthe\tdevice\tto\tpin\tthe\toperation\ton.\tFor\texample,\tthe\tfollowing\tcode\tpins\tall\tthe\tvariable\tnodes\tto \"/cpu:0\"\t(in\tthis\tcase\tjust\tthe\tvariable\ta)\tand\tall\tother\tnodes\tto\t\"/gpu:0\":\n\ndef\tvariables_on_cpu(op): \t\t\t\tif\top.type\t==\t\"Variable\": \t\t\t\t\t\t\t\treturn\t\"/cpu:0\" \t\t\t\telse: \t\t\t\t\t\t\t\treturn\t\"/gpu:0\"\n\nwith\ttf.device(variables_on_cpu): \t\t\t\ta\t=\ttf.Variable(3.0) \t\t\t\tb\t=\ttf.constant(4.0) \t\t\t\tc\t=\ta\t*\tb\n\nYou\tcan\teasily\timplement\tmore\tcomplex\talgorithms,\tsuch\tas\tpinning\tvariables\tacross\tGPUs\tin\ta\tround- robin\tfashion.\n\nOperations\tand\tkernels\n\nFor\ta\tTensorFlow\toperation\tto\trun\ton\ta\tdevice,\tit\tneeds\tto\thave\tan\timplementation\tfor\tthat\tdevice;\tthis\tis called\ta\tkernel.\tMany\toperations\thave\tkernels\tfor\tboth\tCPUs\tand\tGPUs,\tbut\tnot\tall\tof\tthem.\tFor\texample, TensorFlow\tdoes\tnot\thave\ta\tGPU\tkernel\tfor\tinteger\tvariables,\tso\tthe\tfollowing\tcode\twill\tfail\twhen TensorFlow\ttries\tto\tplace\tthe\tvariable\ti\ton\tGPU\t#0:\n\n>>>\twith\ttf.device(\"/gpu:0\"): ...\t\t\t\t\ti\t=\ttf.Variable(3) [...] >>>\tsess.run(i.initializer) Traceback\t(most\trecent\tcall\tlast): [...] tensorflow.python.framework.errors.InvalidArgumentError:\tCannot\tassign\ta\tdevice to\tnode\t'Variable':\tCould\tnot\tsatisfy\texplicit\tdevice\tspecification\n\nNote\tthat\tTensorFlow\tinfers\tthat\tthe\tvariable\tmust\tbe\tof\ttype\tint32\tsince\tthe\tinitialization\tvalue\tis\tan integer.\tIf\tyou\tchange\tthe\tinitialization\tvalue\tto\t3.0\tinstead\tof\t3,\tor\tif\tyou\texplicitly\tset dtype=tf.float32\twhen\tcreating\tthe\tvariable,\teverything\twill\twork\tfine.\n\nSoft\tplacement\n\nBy\tdefault,\tif\tyou\ttry\tto\tpin\tan\toperation\ton\ta\tdevice\tfor\twhich\tthe\toperation\thas\tno\tkernel,\tyou\tget\tthe exception\tshown\tearlier\twhen\tTensorFlow\ttries\tto\tplace\tthe\toperation\ton\tthe\tdevice.\tIf\tyou\tprefer TensorFlow\tto\tfall\tback\tto\tthe\tCPU\tinstead,\tyou\tcan\tset\tthe\tallow_soft_placement\tconfiguration\toption to\tTrue:\n\nwith\ttf.device(\"/gpu:0\"): \t\t\t\ti\t=\ttf.Variable(3)\n\nconfig\t=\ttf.ConfigProto() config.allow_soft_placement\t=\tTrue sess\t=\ttf.Session(config=config) sess.run(i.initializer)\t\t#\tthe\tplacer\truns\tand\tfalls\tback\tto\t/cpu:0\n\nSo\tfar\twe\thave\tdiscussed\thow\tto\tplace\tnodes\ton\tdifferent\tdevices.\tNow\tlet’s\tsee\thow\tTensorFlow\twill run\tthese\tnodes\tin\tparallel.\n\nParallel\tExecution When\tTensorFlow\truns\ta\tgraph,\tit\tstarts\tby\tfinding\tout\tthe\tlist\tof\tnodes\tthat\tneed\tto\tbe\tevaluated,\tand\tit counts\thow\tmany\tdependencies\teach\tof\tthem\thas.\tTensorFlow\tthen\tstarts\tevaluating\tthe\tnodes\twith\tzero dependencies\t(i.e.,\tsource\tnodes).\tIf\tthese\tnodes\tare\tplaced\ton\tseparate\tdevices,\tthey\tobviously\tget evaluated\tin\tparallel.\tIf\tthey\tare\tplaced\ton\tthe\tsame\tdevice,\tthey\tget\tevaluated\tin\tdifferent\tthreads,\tso\tthey may\trun\tin\tparallel\ttoo\t(in\tseparate\tGPU\tthreads\tor\tCPU\tcores).\n\nTensorFlow\tmanages\ta\tthread\tpool\ton\teach\tdevice\tto\tparallelize\toperations\t(see\tFigure\t12-5).\tThese\tare called\tthe\tinter-op\tthread\tpools.\tSome\toperations\thave\tmultithreaded\tkernels:\tthey\tcan\tuse\tother\tthread pools\t(one\tper\tdevice)\tcalled\tthe\tintra-op\tthread\tpools.\n\nFigure\t12-5.\tParallelized\texecution\tof\ta\tTensorFlow\tgraph\n\nFor\texample,\tin\tFigure\t12-5,\toperations\tA,\tB,\tand\tC\tare\tsource\tops,\tso\tthey\tcan\timmediately\tbe evaluated.\tOperations\tA\tand\tB\tare\tplaced\ton\tGPU\t#0,\tso\tthey\tare\tsent\tto\tthis\tdevice’s\tinter-op\tthread pool,\tand\timmediately\tevaluated\tin\tparallel.\tOperation\tA\thappens\tto\thave\ta\tmultithreaded\tkernel;\tits computations\tare\tsplit\tin\tthree\tparts,\twhich\tare\texecuted\tin\tparallel\tby\tthe\tintra-op\tthread\tpool.\tOperation C\tgoes\tto\tGPU\t#1’s\tinter-op\tthread\tpool.\n\nAs\tsoon\tas\toperation\tC\tfinishes,\tthe\tdependency\tcounters\tof\toperations\tD\tand\tE\twill\tbe\tdecremented\tand will\tboth\treach\t0,\tso\tboth\toperations\twill\tbe\tsent\tto\tthe\tinter-op\tthread\tpool\tto\tbe\texecuted.\n\nTIP\n\nYou\tcan\tcontrol\tthe\tnumber\tof\tthreads\tper\tinter-op\tpool\tby\tsetting\tthe\tinter_op_parallelism_threads\toption.\tNote\tthat\tthe first\tsession\tyou\tstart\tcreates\tthe\tinter-op\tthread\tpools.\tAll\tother\tsessions\twill\tjust\treuse\tthem\tunless\tyou\tset\tthe use_per_session_threads\toption\tto\tTrue.\tYou\tcan\tcontrol\tthe\tnumber\tof\tthreads\tper\tintra-op\tpool\tby\tsetting\tthe intra_op_parallelism_threads\toption.\n\nControl\tDependencies In\tsome\tcases,\tit\tmay\tbe\twise\tto\tpostpone\tthe\tevaluation\tof\tan\toperation\teven\tthough\tall\tthe\toperations\tit depends\ton\thave\tbeen\texecuted.\tFor\texample,\tif\tit\tuses\ta\tlot\tof\tmemory\tbut\tits\tvalue\tis\tneeded\tonly\tmuch further\tin\tthe\tgraph,\tit\twould\tbe\tbest\tto\tevaluate\tit\tat\tthe\tlast\tmoment\tto\tavoid\tneedlessly\toccupying\tRAM that\tother\toperations\tmay\tneed.\tAnother\texample\tis\ta\tset\tof\toperations\tthat\tdepend\ton\tdata\tlocated\toutside of\tthe\tdevice.\tIf\tthey\tall\trun\tat\tthe\tsame\ttime,\tthey\tmay\tsaturate\tthe\tdevice’s\tcommunication\tbandwidth, and\tthey\twill\tend\tup\tall\twaiting\ton\tI/O.\tOther\toperations\tthat\tneed\tto\tcommunicate\tdata\twill\talso\tbe blocked.\tIt\twould\tbe\tpreferable\tto\texecute\tthese\tcommunication-heavy\toperations\tsequentially,\tallowing the\tdevice\tto\tperform\tother\toperations\tin\tparallel.\n\nTo\tpostpone\tevaluation\tof\tsome\tnodes,\ta\tsimple\tsolution\tis\tto\tadd\tcontrol\tdependencies.\tFor\texample, the\tfollowing\tcode\ttells\tTensorFlow\tto\tevaluate\tx\tand\ty\tonly\tafter\ta\tand\tb\thave\tbeen\tevaluated:\n\na\t=\ttf.constant(1.0) b\t=\ta\t+\t2.0\n\nwith\ttf.control_dependencies([a,\tb]): \t\t\t\tx\t=\ttf.constant(3.0) \t\t\t\ty\t=\ttf.constant(4.0)\n\nz\t=\tx\t+\ty\n\nObviously,\tsince\tz\tdepends\ton\tx\tand\ty,\tevaluating\tz\talso\timplies\twaiting\tfor\ta\tand\tb\tto\tbe\tevaluated, even\tthough\tit\tis\tnot\texplicitly\tin\tthe\tcontrol_dependencies()\tblock.\tAlso,\tsince\tb\tdepends\ton\ta,\twe could\tsimplify\tthe\tpreceding\tcode\tby\tjust\tcreating\ta\tcontrol\tdependency\ton\t[b]\tinstead\tof\t[a,\tb],\tbut\tin some\tcases\t“explicit\tis\tbetter\tthan\timplicit.”\n\nGreat!\tNow\tyou\tknow:\n\nHow\tto\tplace\toperations\ton\tmultiple\tdevices\tin\tany\tway\tyou\tplease\n\nHow\tthese\toperations\tget\texecuted\tin\tparallel\n\nHow\tto\tcreate\tcontrol\tdependencies\tto\toptimize\tparallel\texecution\n\nIt’s\ttime\tto\tdistribute\tcomputations\tacross\tmultiple\tservers!\n\nMultiple\tDevices\tAcross\tMultiple\tServers To\trun\ta\tgraph\tacross\tmultiple\tservers,\tyou\tfirst\tneed\tto\tdefine\ta\tcluster.\tA\tcluster\tis\tcomposed\tof\tone\tor more\tTensorFlow\tservers,\tcalled\ttasks,\ttypically\tspread\tacross\tseveral\tmachines\t(see\tFigure\t12-6).\tEach task\tbelongs\tto\ta\tjob.\tA\tjob\tis\tjust\ta\tnamed\tgroup\tof\ttasks\tthat\ttypically\thave\ta\tcommon\trole,\tsuch\tas keeping\ttrack\tof\tthe\tmodel\tparameters\t(such\ta\tjob\tis\tusually\tnamed\t\"ps\"\tfor\tparameter\tserver),\tor performing\tcomputations\t(such\ta\tjob\tis\tusually\tnamed\t\"worker\").\n\nFigure\t12-6.\tTensorFlow\tcluster\n\nThe\tfollowing\tcluster\tspecification\tdefines\ttwo\tjobs,\t\"ps\"\tand\t\"worker\",\tcontaining\tone\ttask\tand\ttwo tasks,\trespectively.\tIn\tthis\texample,\tmachine\tA\thosts\ttwo\tTensorFlow\tservers\t(i.e.,\ttasks),\tlistening\ton different\tports:\tone\tis\tpart\tof\tthe\t\"ps\"\tjob,\tand\tthe\tother\tis\tpart\tof\tthe\t\"worker\"\tjob.\tMachine\tB\tjust\thosts one\tTensorFlow\tserver,\tpart\tof\tthe\t\"worker\"\tjob.\n\ncluster_spec\t=\ttf.train.ClusterSpec({ \t\t\t\t\"ps\":\t[ \t\t\t\t\t\t\t\t\"machine-a.example.com:2221\",\t\t#\t/job:ps/task:0 \t\t\t\t], \t\t\t\t\"worker\":\t[ \t\t\t\t\t\t\t\t\"machine-a.example.com:2222\",\t\t#\t/job:worker/task:0 \t\t\t\t\t\t\t\t\"machine-b.example.com:2222\",\t\t#\t/job:worker/task:1 \t\t\t\t]})\n\nTo\tstart\ta\tTensorFlow\tserver,\tyou\tmust\tcreate\ta\tServer\tobject,\tpassing\tit\tthe\tcluster\tspecification\t(so\tit can\tcommunicate\twith\tother\tservers)\tand\tits\town\tjob\tname\tand\ttask\tnumber.\tFor\texample,\tto\tstart\tthe\tfirst worker\ttask,\tyou\twould\trun\tthe\tfollowing\tcode\ton\tmachine\tA:\n\nserver\t=\ttf.train.Server(cluster_spec,\tjob_name=\"worker\",\ttask_index=0)\n\nIt\tis\tusually\tsimpler\tto\tjust\trun\tone\ttask\tper\tmachine,\tbut\tthe\tprevious\texample\tdemonstrates\tthat TensorFlow\tallows\tyou\tto\trun\tmultiple\ttasks\ton\tthe\tsame\tmachine\tif\tyou\twant.2\tIf\tyou\thave\tseveral servers\ton\tone\tmachine,\tyou\twill\tneed\tto\tensure\tthat\tthey\tdon’t\tall\ttry\tto\tgrab\tall\tthe\tRAM\tof\tevery\tGPU, as\texplained\tearlier.\tFor\texample,\tin\tFigure\t12-6\tthe\t\"ps\"\ttask\tdoes\tnot\tsee\tthe\tGPU\tdevices,\tsince presumably\tits\tprocess\twas\tlaunched\twith\tCUDA_VISIBLE_DEVICES=\"\".\tNote\tthat\tthe\tCPU\tis\tshared\tby all\ttasks\tlocated\ton\tthe\tsame\tmachine.\n\nIf\tyou\twant\tthe\tprocess\tto\tdo\tnothing\tother\tthan\trun\tthe\tTensorFlow\tserver,\tyou\tcan\tblock\tthe\tmain\tthread by\ttelling\tit\tto\twait\tfor\tthe\tserver\tto\tfinish\tusing\tthe\tjoin()\tmethod\t(otherwise\tthe\tserver\twill\tbe\tkilled as\tsoon\tas\tyour\tmain\tthread\texits).\tSince\tthere\tis\tcurrently\tno\tway\tto\tstop\tthe\tserver,\tthis\twill\tactually block\tforever:\n\nserver.join()\t\t#\tblocks\tuntil\tthe\tserver\tstops\t(i.e.,\tnever)\n\nOpening\ta\tSession Once\tall\tthe\ttasks\tare\tup\tand\trunning\t(doing\tnothing\tyet),\tyou\tcan\topen\ta\tsession\ton\tany\tof\tthe\tservers, from\ta\tclient\tlocated\tin\tany\tprocess\ton\tany\tmachine\t(even\tfrom\ta\tprocess\trunning\tone\tof\tthe\ttasks),\tand use\tthat\tsession\tlike\ta\tregular\tlocal\tsession.\tFor\texample:\n\na\t=\ttf.constant(1.0) b\t=\ta\t+\t2 c\t=\ta\t*\t3\n\nwith\ttf.Session(\"grpc://machine-b.example.com:2222\")\tas\tsess: \t\t\t\tprint(c.eval())\t\t#\t9.0\n\nThis\tclient\tcode\tfirst\tcreates\ta\tsimple\tgraph,\tthen\topens\ta\tsession\ton\tthe\tTensorFlow\tserver\tlocated\ton machine\tB\t(which\twe\twill\tcall\tthe\tmaster),\tand\tinstructs\tit\tto\tevaluate\tc.\tThe\tmaster\tstarts\tby\tplacing\tthe operations\ton\tthe\tappropriate\tdevices.\tIn\tthis\texample,\tsince\twe\tdid\tnot\tpin\tany\toperation\ton\tany\tdevice, the\tmaster\tsimply\tplaces\tthem\tall\ton\tits\town\tdefault\tdevice\t—\tin\tthis\tcase,\tmachine\tB’s\tGPU\tdevice. Then\tit\tjust\tevaluates\tc\tas\tinstructed\tby\tthe\tclient,\tand\tit\treturns\tthe\tresult.\n\nThe\tMaster\tand\tWorker\tServices The\tclient\tuses\tthe\tgRPC\tprotocol\t(Google\tRemote\tProcedure\tCall)\tto\tcommunicate\twith\tthe\tserver.\tThis is\tan\tefficient\topen\tsource\tframework\tto\tcall\tremote\tfunctions\tand\tget\ttheir\toutputs\tacross\ta\tvariety\tof platforms\tand\tlanguages.3\tIt\tis\tbased\ton\tHTTP2,\twhich\topens\ta\tconnection\tand\tleaves\tit\topen\tduring\tthe whole\tsession,\tallowing\tefficient\tbidirectional\tcommunication\tonce\tthe\tconnection\tis\testablished.\tData\tis transmitted\tin\tthe\tform\tof\tprotocol\tbuffers,\tanother\topen\tsource\tGoogle\ttechnology.\tThis\tis\ta\tlightweight binary\tdata\tinterchange\tformat.\n\nWARNING\n\nAll\tservers\tin\ta\tTensorFlow\tcluster\tmay\tcommunicate\twith\tany\tother\tserver\tin\tthe\tcluster,\tso\tmake\tsure\tto\topen\tthe\tappropriate ports\ton\tyour\tfirewall.\n\nEvery\tTensorFlow\tserver\tprovides\ttwo\tservices:\tthe\tmaster\tservice\tand\tthe\tworker\tservice.\tThe\tmaster service\tallows\tclients\tto\topen\tsessions\tand\tuse\tthem\tto\trun\tgraphs.\tIt\tcoordinates\tthe\tcomputations\tacross tasks,\trelying\ton\tthe\tworker\tservice\tto\tactually\texecute\tcomputations\ton\tother\ttasks\tand\tget\ttheir\tresults.\n\nThis\tarchitecture\tgives\tyou\ta\tlot\tof\tflexibility.\tOne\tclient\tcan\tconnect\tto\tmultiple\tservers\tby\topening multiple\tsessions\tin\tdifferent\tthreads.\tOne\tserver\tcan\thandle\tmultiple\tsessions\tsimultaneously\tfrom\tone\tor more\tclients.\tYou\tcan\trun\tone\tclient\tper\ttask\t(typically\twithin\tthe\tsame\tprocess),\tor\tjust\tone\tclient\tto control\tall\ttasks.\tAll\toptions\tare\topen.\n\nPinning\tOperations\tAcross\tTasks You\tcan\tuse\tdevice\tblocks\tto\tpin\toperations\ton\tany\tdevice\tmanaged\tby\tany\ttask,\tby\tspecifying\tthe\tjob name,\ttask\tindex,\tdevice\ttype,\tand\tdevice\tindex.\tFor\texample,\tthe\tfollowing\tcode\tpins\ta\tto\tthe\tCPU\tof\tthe first\ttask\tin\tthe\t\"ps\"\tjob\t(that’s\tthe\tCPU\ton\tmachine\tA),\tand\tit\tpins\tb\tto\tthe\tsecond\tGPU\tmanaged\tby\tthe first\ttask\tof\tthe\t\"worker\"\tjob\t(that’s\tGPU\t#1\ton\tmachine\tA).\tFinally,\tc\tis\tnot\tpinned\tto\tany\tdevice,\tso\tthe master\tplaces\tit\ton\tits\town\tdefault\tdevice\t(machine\tB’s\tGPU\t#0\tdevice).\n\nwith\ttf.device(\"/job:ps/task:0/cpu:0\") \t\t\t\ta\t=\ttf.constant(1.0)\n\nwith\ttf.device(\"/job:worker/task:0/gpu:1\") \t\t\t\tb\t=\ta\t+\t2\n\nc\t=\ta\t+\tb\n\nAs\tearlier,\tif\tyou\tomit\tthe\tdevice\ttype\tand\tindex,\tTensorFlow\twill\tdefault\tto\tthe\ttask’s\tdefault\tdevice;\tfor example,\tpinning\tan\toperation\tto\t\"/job:ps/task:0\"\twill\tplace\tit\ton\tthe\tdefault\tdevice\tof\tthe\tfirst\ttask\tof the\t\"ps\"\tjob\t(machine\tA’s\tCPU).\tIf\tyou\talso\tomit\tthe\ttask\tindex\t(e.g.,\t\"/job:ps\"),\tTensorFlow\tdefaults to\t\"/task:0\".\tIf\tyou\tomit\tthe\tjob\tname\tand\tthe\ttask\tindex,\tTensorFlow\tdefaults\tto\tthe\tsession’s\tmaster task.\n\nSharding\tVariables\tAcross\tMultiple\tParameter\tServers As\twe\twill\tsee\tshortly,\ta\tcommon\tpattern\twhen\ttraining\ta\tneural\tnetwork\ton\ta\tdistributed\tsetup\tis\tto\tstore the\tmodel\tparameters\ton\ta\tset\tof\tparameter\tservers\t(i.e.,\tthe\ttasks\tin\tthe\t\"ps\"\tjob)\twhile\tother\ttasks\tfocus on\tcomputations\t(i.e.,\tthe\ttasks\tin\tthe\t\"worker\"\tjob).\tFor\tlarge\tmodels\twith\tmillions\tof\tparameters,\tit\tis useful\tto\tshard\tthese\tparameters\tacross\tmultiple\tparameter\tservers,\tto\treduce\tthe\trisk\tof\tsaturating\ta single\tparameter\tserver’s\tnetwork\tcard.\tIf\tyou\twere\tto\tmanually\tpin\tevery\tvariable\tto\ta\tdifferent parameter\tserver,\tit\twould\tbe\tquite\ttedious.\tFortunately,\tTensorFlow\tprovides\tthe replica_device_setter()\tfunction,\twhich\tdistributes\tvariables\tacross\tall\tthe\t\"ps\"\ttasks\tin\ta\tround- robin\tfashion.\tFor\texample,\tthe\tfollowing\tcode\tpins\tfive\tvariables\tto\ttwo\tparameter\tservers:\n\nwith\ttf.device(tf.train.replica_device_setter(ps_tasks=2): \t\t\t\tv1\t=\ttf.Variable(1.0)\t\t#\tpinned\tto\t/job:ps/task:0 \t\t\t\tv2\t=\ttf.Variable(2.0)\t\t#\tpinned\tto\t/job:ps/task:1 \t\t\t\tv3\t=\ttf.Variable(3.0)\t\t#\tpinned\tto\t/job:ps/task:0 \t\t\t\tv4\t=\ttf.Variable(4.0)\t\t#\tpinned\tto\t/job:ps/task:1 \t\t\t\tv5\t=\ttf.Variable(5.0)\t\t#\tpinned\tto\t/job:ps/task:0\n\nInstead\tof\tpassing\tthe\tnumber\tof\tps_tasks,\tyou\tcan\tpass\tthe\tcluster\tspec\tcluster=cluster_spec\tand TensorFlow\twill\tsimply\tcount\tthe\tnumber\tof\ttasks\tin\tthe\t\"ps\"\tjob.\n\nIf\tyou\tcreate\tother\toperations\tin\tthe\tblock,\tbeyond\tjust\tvariables,\tTensorFlow\tautomatically\tpins\tthem\tto \"/job:worker\",\twhich\twill\tdefault\tto\tthe\tfirst\tdevice\tmanaged\tby\tthe\tfirst\ttask\tin\tthe\t\"worker\"\tjob.\tYou can\tpin\tthem\tto\tanother\tdevice\tby\tsetting\tthe\tworker_device\tparameter,\tbut\ta\tbetter\tapproach\tis\tto\tuse embedded\tdevice\tblocks.\tAn\tinner\tdevice\tblock\tcan\toverride\tthe\tjob,\ttask,\tor\tdevice\tdefined\tin\tan\touter block.\tFor\texample:\n\nwith\ttf.device(tf.train.replica_device_setter(ps_tasks=2)): \t\t\t\tv1\t=\ttf.Variable(1.0)\t\t#\tpinned\tto\t/job:ps/task:0\t(+\tdefaults\tto\t/cpu:0) \t\t\t\tv2\t=\ttf.Variable(2.0)\t\t#\tpinned\tto\t/job:ps/task:1\t(+\tdefaults\tto\t/cpu:0) \t\t\t\tv3\t=\ttf.Variable(3.0)\t\t#\tpinned\tto\t/job:ps/task:0\t(+\tdefaults\tto\t/cpu:0) \t\t\t\t[...] \t\t\t\ts\t=\tv1\t+\tv2\t\t\t\t\t\t\t\t\t\t\t\t#\tpinned\tto\t/job:worker\t(+\tdefaults\tto\ttask:0/gpu:0) \t\t\t\twith\ttf.device(\"/gpu:1\"): \t\t\t\t\t\t\t\tp1\t=\t2\t*\ts\t\t\t\t\t\t\t\t\t#\tpinned\tto\t/job:worker/gpu:1\t(+\tdefaults\tto\t/task:0) \t\t\t\t\t\t\t\twith\ttf.device(\"/task:1\"): \t\t\t\t\t\t\t\t\t\t\t\tp2\t=\t3\t*\ts\t\t\t\t\t#\tpinned\tto\t/job:worker/task:1/gpu:1\n\nNOTE\n\nThis\texample\tassumes\tthat\tthe\tparameter\tservers\tare\tCPU-only,\twhich\tis\ttypically\tthe\tcase\tsince\tthey\tonly\tneed\tto\tstore\tand communicate\tparameters,\tnot\tperform\tintensive\tcomputations.\n\nSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers When\tyou\tare\tusing\ta\tplain\tlocal\tsession\t(not\tthe\tdistributed\tkind),\teach\tvariable’s\tstate\tis\tmanaged\tby the\tsession\titself;\tas\tsoon\tas\tit\tends,\tall\tvariable\tvalues\tare\tlost.\tMoreover,\tmultiple\tlocal\tsessions\tcannot share\tany\tstate,\teven\tif\tthey\tboth\trun\tthe\tsame\tgraph;\teach\tsession\thas\tits\town\tcopy\tof\tevery\tvariable\t(as we\tdiscussed\tin\tChapter\t9).\tIn\tcontrast,\twhen\tyou\tare\tusing\tdistributed\tsessions,\tvariable\tstate\tis managed\tby\tresource\tcontainers\tlocated\ton\tthe\tcluster\titself,\tnot\tby\tthe\tsessions.\tSo\tif\tyou\tcreate\ta variable\tnamed\tx\tusing\tone\tclient\tsession,\tit\twill\tautomatically\tbe\tavailable\tto\tany\tother\tsession\ton\tthe same\tcluster\t(even\tif\tboth\tsessions\tare\tconnected\tto\ta\tdifferent\tserver).\tFor\texample,\tconsider\tthe following\tclient\tcode:\n\n#\tsimple_client.py import\ttensorflow\tas\ttf import\tsys\n\nx\t=\ttf.Variable(0.0,\tname=\"x\") increment_x\t=\ttf.assign(x,\tx\t+\t1)\n\nwith\ttf.Session(sys.argv[1])\tas\tsess: \t\t\t\tif\tsys.argv[2:]==[\"init\"]: \t\t\t\t\t\t\t\tsess.run(x.initializer) \t\t\t\tsess.run(increment_x) \t\t\t\tprint(x.eval())\n\nLet’s\tsuppose\tyou\thave\ta\tTensorFlow\tcluster\tup\tand\trunning\ton\tmachines\tA\tand\tB,\tport\t2222.\tYou\tcould launch\tthe\tclient,\thave\tit\topen\ta\tsession\twith\tthe\tserver\ton\tmachine\tA,\tand\ttell\tit\tto\tinitialize\tthe\tvariable, increment\tit,\tand\tprint\tits\tvalue\tby\tlaunching\tthe\tfollowing\tcommand:\n\n$\tpython3\tsimple_client.py\tgrpc://machine-a.example.com:2222\tinit 1.0\n\nNow\tif\tyou\tlaunch\tthe\tclient\twith\tthe\tfollowing\tcommand,\tit\twill\tconnect\tto\tthe\tserver\ton\tmachine\tB\tand magically\treuse\tthe\tsame\tvariable\tx\t(this\ttime\twe\tdon’t\task\tthe\tserver\tto\tinitialize\tthe\tvariable):\n\n$\tpython3\tsimple_client.py\tgrpc://machine-b.example.com:2222 2.0\n\nThis\tfeature\tcuts\tboth\tways:\tit’s\tgreat\tif\tyou\twant\tto\tshare\tvariables\tacross\tmultiple\tsessions,\tbut\tif\tyou want\tto\trun\tcompletely\tindependent\tcomputations\ton\tthe\tsame\tcluster\tyou\twill\thave\tto\tbe\tcareful\tnot\tto use\tthe\tsame\tvariable\tnames\tby\taccident.\tOne\tway\tto\tensure\tthat\tyou\twon’t\thave\tname\tclashes\tis\tto\twrap all\tof\tyour\tconstruction\tphase\tinside\ta\tvariable\tscope\twith\ta\tunique\tname\tfor\teach\tcomputation,\tfor example:\n\nwith\ttf.variable_scope(\"my_problem_1\"): \t\t\t\t[...]\t#\tConstruction\tphase\tof\tproblem\t1\n\nA\tbetter\toption\tis\tto\tuse\ta\tcontainer\tblock:\n\nwith\ttf.container(\"my_problem_1\"): \t\t\t\t[...]\t#\tConstruction\tphase\tof\tproblem\t1\n\nThis\twill\tuse\ta\tcontainer\tdedicated\tto\tproblem\t#1,\tinstead\tof\tthe\tdefault\tone\t(whose\tname\tis\tan\tempty string\t\"\").\tOne\tadvantage\tis\tthat\tvariable\tnames\tremain\tnice\tand\tshort.\tAnother\tadvantage\tis\tthat\tyou\tcan easily\treset\ta\tnamed\tcontainer.\tFor\texample,\tthe\tfollowing\tcommand\twill\tconnect\tto\tthe\tserver\ton machine\tA\tand\task\tit\tto\treset\tthe\tcontainer\tnamed\t\"my_problem_1\",\twhich\twill\tfree\tall\tthe\tresources\tthis container\tused\t(and\talso\tclose\tall\tsessions\topen\ton\tthe\tserver).\tAny\tvariable\tmanaged\tby\tthis\tcontainer must\tbe\tinitialized\tbefore\tyou\tcan\tuse\tit\tagain:\n\ntf.Session.reset(\"grpc://machine-a.example.com:2222\",\t[\"my_problem_1\"])\n\nResource\tcontainers\tmake\tit\teasy\tto\tshare\tvariables\tacross\tsessions\tin\tflexible\tways.\tFor\texample, Figure\t12-7\tshows\tfour\tclients\trunning\tdifferent\tgraphs\ton\tthe\tsame\tcluster,\tbut\tsharing\tsome\tvariables. Clients\tA\tand\tB\tshare\tthe\tsame\tvariable\tx\tmanaged\tby\tthe\tdefault\tcontainer,\twhile\tclients\tC\tand\tD\tshare another\tvariable\tnamed\tx\tmanaged\tby\tthe\tcontainer\tnamed\t\"my_problem_1\".\tNote\tthat\tclient\tC\teven\tuses variables\tfrom\tboth\tcontainers.\n\nFigure\t12-7.\tResource\tcontainers\n\nResource\tcontainers\talso\ttake\tcare\tof\tpreserving\tthe\tstate\tof\tother\tstateful\toperations,\tnamely\tqueues\tand readers.\tLet’s\ttake\ta\tlook\tat\tqueues\tfirst.\n\nAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues Queues\tare\tanother\tgreat\tway\tto\texchange\tdata\tbetween\tmultiple\tsessions;\tfor\texample,\tone\tcommon\tuse case\tis\tto\thave\ta\tclient\tcreate\ta\tgraph\tthat\tloads\tthe\ttraining\tdata\tand\tpushes\tit\tinto\ta\tqueue,\twhile\tanother client\tcreates\ta\tgraph\tthat\tpulls\tthe\tdata\tfrom\tthe\tqueue\tand\ttrains\ta\tmodel\t(see\tFigure\t12-8).\tThis\tcan speed\tup\ttraining\tconsiderably\tbecause\tthe\ttraining\toperations\tdon’t\thave\tto\twait\tfor\tthe\tnext\tmini-batch at\tevery\tstep.\n\nFigure\t12-8.\tUsing\tqueues\tto\tload\tthe\ttraining\tdata\tasynchronously\n\nTensorFlow\tprovides\tvarious\tkinds\tof\tqueues.\tThe\tsimplest\tkind\tis\tthe\tfirst-in\tfirst-out\t(FIFO)\tqueue. For\texample,\tthe\tfollowing\tcode\tcreates\ta\tFIFO\tqueue\tthat\tcan\tstore\tup\tto\t10\ttensors\tcontaining\ttwo\tfloat values\teach:\n\nq\t=\ttf.FIFOQueue(capacity=10,\tdtypes=[tf.float32],\tshapes=[[2]], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"q\",\tshared_name=\"shared_q\")\n\nWARNING\n\nTo\tshare\tvariables\tacross\tsessions,\tall\tyou\thad\tto\tdo\twas\tto\tspecify\tthe\tsame\tname\tand\tcontainer\ton\tboth\tends.\tWith\tqueues TensorFlow\tdoes\tnot\tuse\tthe\tname\tattribute\tbut\tinstead\tuses\tshared_name,\tso\tit\tis\timportant\tto\tspecify\tit\t(even\tif\tit\tis\tthe\tsame\tas the\tname).\tAnd,\tof\tcourse,\tuse\tthe\tsame\tcontainer.\n\nEnqueuing\tdata\n\nTo\tpush\tdata\tto\ta\tqueue,\tyou\tmust\tcreate\tan\tenqueue\toperation.\tFor\texample,\tthe\tfollowing\tcode\tpushes three\ttraining\tinstances\tto\tthe\tqueue:\n\n#\ttraining_data_loader.py import\ttensorflow\tas\ttf\n\nq\t=\t[...] training_instance\t=\ttf.placeholder(tf.float32,\tshape=(2)) enqueue\t=\tq.enqueue([training_instance])\n\nwith\ttf.Session(\"grpc://machine-a.example.com:2222\")\tas\tsess: \t\t\tsess.run(enqueue,\tfeed_dict={training_instance:\t[1.,\t2.]})\n\nsess.run(enqueue,\tfeed_dict={training_instance:\t[3.,\t4.]}) \t\t\tsess.run(enqueue,\tfeed_dict={training_instance:\t[5.,\t6.]})\n\nInstead\tof\tenqueuing\tinstances\tone\tby\tone,\tyou\tcan\tenqueue\tseveral\tat\ta\ttime\tusing\tan\tenqueue_many operation:\n\n[...] training_instances\t=\ttf.placeholder(tf.float32,\tshape=(None,\t2)) enqueue_many\t=\tq.enqueue([training_instances])\n\nwith\ttf.Session(\"grpc://machine-a.example.com:2222\")\tas\tsess: \t\t\tsess.run(enqueue_many, \t\t\t\t\t\t\t\t\t\t\t\tfeed_dict={training_instances:\t[[1.,\t2.],\t[3.,\t4.],\t[5.,\t6.]]})\n\nBoth\texamples\tenqueue\tthe\tsame\tthree\ttensors\tto\tthe\tqueue.\n\nDequeuing\tdata\n\nTo\tpull\tthe\tinstances\tout\tof\tthe\tqueue,\ton\tthe\tother\tend,\tyou\tneed\tto\tuse\ta\tdequeue\toperation:\n\n#\ttrainer.py import\ttensorflow\tas\ttf\n\nq\t=\t[...] dequeue\t=\tq.dequeue()\n\nwith\ttf.Session(\"grpc://machine-a.example.com:2222\")\tas\tsess: \t\t\tprint(sess.run(dequeue))\t\t#\t[1.,\t2.] \t\t\tprint(sess.run(dequeue))\t\t#\t[3.,\t4.] \t\t\tprint(sess.run(dequeue))\t\t#\t[5.,\t6.]\n\nIn\tgeneral\tyou\twill\twant\tto\tpull\ta\twhole\tmini-batch\tat\tonce,\tinstead\tof\tpulling\tjust\tone\tinstance\tat\ta\ttime. To\tdo\tso,\tyou\tmust\tuse\ta\tdequeue_many\toperation,\tspecifying\tthe\tmini-batch\tsize:\n\n[...] batch_size\t=\t2 dequeue_mini_batch=\tq.dequeue_many(batch_size)\n\nwith\ttf.Session(\"grpc://machine-a.example.com:2222\")\tas\tsess: \t\t\tprint(sess.run(dequeue_mini_batch))\t\t#\t[[1.,\t2.],\t[4.,\t5.]] \t\t\tprint(sess.run(dequeue_mini_batch))\t\t#\tblocked\twaiting\tfor\tanother\tinstance\n\nWhen\ta\tqueue\tis\tfull,\tthe\tenqueue\toperation\twill\tblock\tuntil\titems\tare\tpulled\tout\tby\ta\tdequeue\toperation. Similarly,\twhen\ta\tqueue\tis\tempty\t(or\tyou\tare\tusing\tdequeue_many()\tand\tthere\tare\tfewer\titems\tthan\tthe mini-batch\tsize),\tthe\tdequeue\toperation\twill\tblock\tuntil\tenough\titems\tare\tpushed\tinto\tthe\tqueue\tusing\tan enqueue\toperation.\n\nQueues\tof\ttuples\n\nEach\titem\tin\ta\tqueue\tcan\tbe\ta\ttuple\tof\ttensors\t(of\tvarious\ttypes\tand\tshapes)\tinstead\tof\tjust\ta\tsingle\ttensor. For\texample,\tthe\tfollowing\tqueue\tstores\tpairs\tof\ttensors,\tone\tof\ttype\tint32\tand\tshape\t(),\tand\tthe\tother\tof type\tfloat32\tand\tshape\t[3,2]:\n\nq\t=\ttf.FIFOQueue(capacity=10,\tdtypes=[tf.int32,\ttf.float32],\tshapes=[[],[3,2]], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"q\",\tshared_name=\"shared_q\")\n\nThe\tenqueue\toperation\tmust\tbe\tgiven\tpairs\tof\ttensors\t(note\tthat\teach\tpair\trepresents\tonly\tone\titem\tin\tthe\n\nqueue):\n\na\t=\ttf.placeholder(tf.int32,\tshape=()) b\t=\ttf.placeholder(tf.float32,\tshape=(3,\t2)) enqueue\t=\tq.enqueue((a,\tb))\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\tsess.run(enqueue,\tfeed_dict={a:\t10,\tb:[[1.,\t2.],\t[3.,\t4.],\t[5.,\t6.]]}) \t\t\t\tsess.run(enqueue,\tfeed_dict={a:\t11,\tb:[[2.,\t4.],\t[6.,\t8.],\t[0.,\t2.]]}) \t\t\t\tsess.run(enqueue,\tfeed_dict={a:\t12,\tb:[[3.,\t6.],\t[9.,\t2.],\t[5.,\t8.]]})\n\nOn\tthe\tother\tend,\tthe\tdequeue()\tfunction\tnow\tcreates\ta\tpair\tof\tdequeue\toperations:\n\ndequeue_a,\tdequeue_b\t=\tq.dequeue()\n\nIn\tgeneral,\tyou\tshould\trun\tthese\toperations\ttogether:\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\ta_val,\tb_val\t=\tsess.run([dequeue_a,\tdequeue_b]) \t\t\t\tprint(a_val)\t#\t10 \t\t\t\tprint(b_val)\t#\t[[1.,\t2.],\t[3.,\t4.],\t[5.,\t6.]]\n\nWARNING\n\nIf\tyou\trun\tdequeue_a\ton\tits\town,\tit\twill\tdequeue\ta\tpair\tand\treturn\tonly\tthe\tfirst\telement;\tthe\tsecond\telement\twill\tbe\tlost\t(and similarly,\tif\tyou\trun\tdequeue_b\ton\tits\town,\tthe\tfirst\telement\twill\tbe\tlost).\n\nThe\tdequeue_many()\tfunction\talso\treturns\ta\tpair\tof\toperations:\n\nbatch_size\t=\t2 dequeue_as,\tdequeue_bs\t=\tq.dequeue_many(batch_size)\n\nYou\tcan\tuse\tit\tas\tyou\twould\texpect:\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\ta,\tb\t=\tsess.run([dequeue_a,\tdequeue_b]) \t\t\t\tprint(a)\t#\t[10,\t11] \t\t\t\tprint(b)\t#\t[[[1.,\t2.],\t[3.,\t4.],\t[5.,\t6.]],\t[[2.,\t4.],\t[6.,\t8.],\t[0.,\t2.]]] \t\t\t\ta,\tb\t=\tsess.run([dequeue_a,\tdequeue_b])\t\t#\tblocked\twaiting\tfor\tanother\tpair\n\nClosing\ta\tqueue\n\nIt\tis\tpossible\tto\tclose\ta\tqueue\tto\tsignal\tto\tthe\tother\tsessions\tthat\tno\tmore\tdata\twill\tbe\tenqueued:\n\nclose_q\t=\tq.close()\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\t[...] \t\t\t\tsess.run(close_q)\n\nSubsequent\texecutions\tof\tenqueue\tor\tenqueue_many\toperations\twill\traise\tan\texception.\tBy\tdefault,\tany pending\tenqueue\trequest\twill\tbe\thonored,\tunless\tyou\tcall\tq.close(cancel_pending_enqueues=True).\n\nSubsequent\texecutions\tof\tdequeue\tor\tdequeue_many\toperations\twill\tcontinue\tto\tsucceed\tas\tlong\tas\tthere\n\nare\titems\tin\tthe\tqueue,\tbut\tthey\twill\tfail\twhen\tthere\tare\tnot\tenough\titems\tleft\tin\tthe\tqueue.\tIf\tyou\tare\tusing a\tdequeue_many\toperation\tand\tthere\tare\ta\tfew\tinstances\tleft\tin\tthe\tqueue,\tbut\tfewer\tthan\tthe\tmini-batch size,\tthey\twill\tbe\tlost.\tYou\tmay\tprefer\tto\tuse\ta\tdequeue_up_to\toperation\tinstead;\tit\tbehaves\texactly\tlike dequeue_many\texcept\twhen\ta\tqueue\tis\tclosed\tand\tthere\tare\tfewer\tthan\tbatch_size\tinstances\tleft\tin\tthe queue,\tin\twhich\tcase\tit\tjust\treturns\tthem.\n\nRandomShuffleQueue\n\nTensorFlow\talso\tsupports\ta\tcouple\tmore\ttypes\tof\tqueues,\tincluding\tRandomShuffleQueue,\twhich\tcan\tbe used\tjust\tlike\ta\tFIFOQueue\texcept\tthat\titems\tare\tdequeued\tin\ta\trandom\torder.\tThis\tcan\tbe\tuseful\tto\tshuffle training\tinstances\tat\teach\tepoch\tduring\ttraining.\tFirst,\tlet’s\tcreate\tthe\tqueue:\n\nq\t=\ttf.RandomShuffleQueue(capacity=50,\tmin_after_dequeue=10, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdtypes=[tf.float32],\tshapes=[()], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"q\",\tshared_name=\"shared_q\")\n\nThe\tmin_after_dequeue\tspecifies\tthe\tminimum\tnumber\tof\titems\tthat\tmust\tremain\tin\tthe\tqueue\tafter\ta dequeue\toperation.\tThis\tensures\tthat\tthere\twill\tbe\tenough\tinstances\tin\tthe\tqueue\tto\thave\tenough randomness\t(once\tthe\tqueue\tis\tclosed,\tthe\tmin_after_dequeue\tlimit\tis\tignored).\tNow\tsuppose\tthat\tyou enqueued\t22\titems\tin\tthis\tqueue\t(floats\t1.\tto\t22.).\tHere\tis\thow\tyou\tcould\tdequeue\tthem:\n\ndequeue\t=\tq.dequeue_many(5)\n\nwith\ttf.Session([...])\tas\tsess: \t\t\tprint(sess.run(dequeue))\t#\t[\t20.\t\t15.\t\t11.\t\t12.\t\t\t4.]\t\t\t(17\titems\tleft) \t\t\tprint(sess.run(dequeue))\t#\t[\t\t5.\t\t13.\t\t\t6.\t\t\t0.\t\t17.]\t\t\t(12\titems\tleft) \t\t\tprint(sess.run(dequeue))\t#\t12\t-\t5\t<\t10:\tblocked\twaiting\tfor\t3\tmore\tinstances\n\nPaddingFifoQueue\n\nA\tPaddingFIFOQueue\tcan\talso\tbe\tused\tjust\tlike\ta\tFIFOQueue\texcept\tthat\tit\taccepts\ttensors\tof\tvariable sizes\talong\tany\tdimension\t(but\twith\ta\tfixed\trank).\tWhen\tyou\tare\tdequeuing\tthem\twith\ta\tdequeue_many\tor dequeue_up_to\toperation,\teach\ttensor\tis\tpadded\twith\tzeros\talong\tevery\tvariable\tdimension\tto\tmake\tit the\tsame\tsize\tas\tthe\tlargest\ttensor\tin\tthe\tmini-batch.\tFor\texample,\tyou\tcould\tenqueue\t2D\ttensors (matrices)\tof\tarbitrary\tsizes:\n\nq\t=\ttf.PaddingFIFOQueue(capacity=50,\tdtypes=[tf.float32],\tshapes=[(None,\tNone)] \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"q\",\tshared_name=\"shared_q\") v\t=\ttf.placeholder(tf.float32,\tshape=(None,\tNone)) enqueue\t=\tq.enqueue([v])\n\nwith\ttf.Session([...])\tas\tsess: \t\t\tsess.run(enqueue,\tfeed_dict={v:\t[[1.,\t2.],\t[3.,\t4.],\t[5.,\t6.]]})\t\t\t\t\t\t\t#\t3x2 \t\t\tsess.run(enqueue,\tfeed_dict={v:\t[[1.]]})\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t1x1 \t\t\tsess.run(enqueue,\tfeed_dict={v:\t[[7.,\t8.,\t9.,\t5.],\t[6.,\t7.,\t8.,\t9.]]})\t#\t2x4\n\nIf\twe\tjust\tdequeue\tone\titem\tat\ta\ttime,\twe\tget\tthe\texact\tsame\ttensors\tthat\twere\tenqueued.\tBut\tif\twe dequeue\tseveral\titems\tat\ta\ttime\t(using\tdequeue_many()\tor\tdequeue_up_to()),\tthe\tqueue\tautomatically pads\tthe\ttensors\tappropriately.\tFor\texample,\tif\twe\tdequeue\tall\tthree\titems\tat\tonce,\tall\ttensors\twill\tbe padded\twith\tzeros\tto\tbecome\t3\t×\t4\ttensors,\tsince\tthe\tmaximum\tsize\tfor\tthe\tfirst\tdimension\tis\t3\t(first\titem) and\tthe\tmaximum\tsize\tfor\tthe\tsecond\tdimension\tis\t4\t(third\titem):\n\n>>>\tq\t=\t[...] >>>\tdequeue\t=\tq.dequeue_many(3) >>>\twith\ttf.Session([...])\tas\tsess: ...\t\t\t\t\tprint(sess.run(dequeue)) [[[\t1.\t\t2.\t\t0.\t\t0.] \t\t[\t3.\t\t4.\t\t0.\t\t0.] \t\t[\t5.\t\t6.\t\t0.\t\t0.]]\n\n[[\t1.\t\t0.\t\t0.\t\t0.] \t\t[\t0.\t\t0.\t\t0.\t\t0.] \t\t[\t0.\t\t0.\t\t0.\t\t0.]]\n\n[[\t7.\t\t8.\t\t9.\t\t5.] \t\t[\t6.\t\t7.\t\t8.\t\t9.] \t\t[\t0.\t\t0.\t\t0.\t\t0.]]]\n\nThis\ttype\tof\tqueue\tcan\tbe\tuseful\twhen\tyou\tare\tdealing\twith\tvariable\tlength\tinputs,\tsuch\tas\tsequences\tof words\t(see\tChapter\t14).\n\nOkay,\tnow\tlet’s\tpause\tfor\ta\tsecond:\tso\tfar\tyou\thave\tlearned\tto\tdistribute\tcomputations\tacross\tmultiple devices\tand\tservers,\tshare\tvariables\tacross\tsessions,\tand\tcommunicate\tasynchronously\tusing\tqueues. Before\tyou\tstart\ttraining\tneural\tnetworks,\tthough,\tthere’s\tone\tlast\ttopic\twe\tneed\tto\tdiscuss:\thow\tto efficiently\tload\ttraining\tdata.\n\nLoading\tData\tDirectly\tfrom\tthe\tGraph So\tfar\twe\thave\tassumed\tthat\tthe\tclients\twould\tload\tthe\ttraining\tdata\tand\tfeed\tit\tto\tthe\tcluster\tusing placeholders.\tThis\tis\tsimple\tand\tworks\tquite\twell\tfor\tsimple\tsetups,\tbut\tit\tis\trather\tinefficient\tsince\tit transfers\tthe\ttraining\tdata\tseveral\ttimes:\n\n1.\t From\tthe\tfilesystem\tto\tthe\tclient\n\n2.\t From\tthe\tclient\tto\tthe\tmaster\ttask\n\n3.\t Possibly\tfrom\tthe\tmaster\ttask\tto\tother\ttasks\twhere\tthe\tdata\tis\tneeded\n\nIt\tgets\tworse\tif\tyou\thave\tseveral\tclients\ttraining\tvarious\tneural\tnetworks\tusing\tthe\tsame\ttraining\tdata\t(for example,\tfor\thyperparameter\ttuning):\tif\tevery\tclient\tloads\tthe\tdata\tsimultaneously,\tyou\tmay\tend\tup\teven saturating\tyour\tfile\tserver\tor\tthe\tnetwork’s\tbandwidth.\n\nPreload\tthe\tdata\tinto\ta\tvariable\n\nFor\tdatasets\tthat\tcan\tfit\tin\tmemory,\ta\tbetter\toption\tis\tto\tload\tthe\ttraining\tdata\tonce\tand\tassign\tit\tto\ta variable,\tthen\tjust\tuse\tthat\tvariable\tin\tyour\tgraph.\tThis\tis\tcalled\tpreloading\tthe\ttraining\tset.\tThis\tway\tthe data\twill\tbe\ttransferred\tonly\tonce\tfrom\tthe\tclient\tto\tthe\tcluster\t(but\tit\tmay\tstill\tneed\tto\tbe\tmoved\taround from\ttask\tto\ttask\tdepending\ton\twhich\toperations\tneed\tit).\tThe\tfollowing\tcode\tshows\thow\tto\tload\tthe\tfull training\tset\tinto\ta\tvariable:\n\ntraining_set_init\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features)) training_set\t=\ttf.Variable(training_set_init,\ttrainable=False,\tcollections=[], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"training_set\")\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\tdata\t=\t[...]\t\t#\tload\tthe\ttraining\tdata\tfrom\tthe\tdatastore \t\t\t\tsess.run(training_set.initializer,\tfeed_dict={training_set_init:\tdata})\n\nYou\tmust\tset\ttrainable=False\tso\tthe\toptimizers\tdon’t\ttry\tto\ttweak\tthis\tvariable.\tYou\tshould\talso\tset collections=[]\tto\tensure\tthat\tthis\tvariable\twon’t\tget\tadded\tto\tthe\tGraphKeys.GLOBAL_VARIABLES collection,\twhich\tis\tused\tfor\tsaving\tand\trestoring\tcheckpoints.\n\nNOTE\n\nThis\texample\tassumes\tthat\tall\tof\tyour\ttraining\tset\t(including\tthe\tlabels)\tconsists\tonly\tof\tfloat32\tvalues.\tIf\tthat’s\tnot\tthe\tcase,\tyou will\tneed\tone\tvariable\tper\ttype.\n\nReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph\n\nIf\tthe\ttraining\tset\tdoes\tnot\tfit\tin\tmemory,\ta\tgood\tsolution\tis\tto\tuse\treader\toperations:\tthese\tare\toperations capable\tof\treading\tdata\tdirectly\tfrom\tthe\tfilesystem.\tThis\tway\tthe\ttraining\tdata\tnever\tneeds\tto\tflow through\tthe\tclients\tat\tall.\tTensorFlow\tprovides\treaders\tfor\tvarious\tfile\tformats:\n\nCSV\n\nFixed-length\tbinary\trecords\n\nTensorFlow’s\town\tTFRecords\tformat,\tbased\ton\tprotocol\tbuffers\n\nLet’s\tlook\tat\ta\tsimple\texample\treading\tfrom\ta\tCSV\tfile\t(for\tother\tformats,\tplease\tcheck\tout\tthe\tAPI documentation).\tSuppose\tyou\thave\tfile\tnamed\tmy_test.csv\tthat\tcontains\ttraining\tinstances,\tand\tyou\twant to\tcreate\toperations\tto\tread\tit.\tSuppose\tit\thas\tthe\tfollowing\tcontent,\twith\ttwo\tfloat\tfeatures\tx1\tand\tx2\tand one\tinteger\ttarget\trepresenting\ta\tbinary\tclass:\n\nx1,\t\tx2,\t\ttarget 1.\t,\t2.\t,\t0 4.\t,\t5\t\t,\t1 7.\t,\t\t\t\t,\t0\n\nFirst,\tlet’s\tcreate\ta\tTextLineReader\tto\tread\tthis\tfile.\tA\tTextLineReader\topens\ta\tfile\t(once\twe\ttell\tit which\tone\tto\topen)\tand\treads\tlines\tone\tby\tone.\tIt\tis\ta\tstateful\toperation,\tlike\tvariables\tand\tqueues:\tit preserves\tits\tstate\tacross\tmultiple\truns\tof\tthe\tgraph,\tkeeping\ttrack\tof\twhich\tfile\tit\tis\tcurrently\treading\tand what\tits\tcurrent\tposition\tis\tin\tthis\tfile.\n\nreader\t=\ttf.TextLineReader(skip_header_lines=1)\n\nNext,\twe\tcreate\ta\tqueue\tthat\tthe\treader\twill\tpull\tfrom\tto\tknow\twhich\tfile\tto\tread\tnext.\tWe\talso\tcreate\tan enqueue\toperation\tand\ta\tplaceholder\tto\tpush\tany\tfilename\twe\twant\tto\tthe\tqueue,\tand\twe\tcreate\tan operation\tto\tclose\tthe\tqueue\tonce\twe\thave\tno\tmore\tfiles\tto\tread:\n\nfilename_queue\t=\ttf.FIFOQueue(capacity=10,\tdtypes=[tf.string],\tshapes=[()]) filename\t=\ttf.placeholder(tf.string) enqueue_filename\t=\tfilename_queue.enqueue([filename]) close_filename_queue\t=\tfilename_queue.close()\n\nNow\twe\tare\tready\tto\tcreate\ta\tread\toperation\tthat\twill\tread\tone\trecord\t(i.e.,\ta\tline)\tat\ta\ttime\tand\treturn\ta key/value\tpair.\tThe\tkey\tis\tthe\trecord’s\tunique\tidentifier\t—\ta\tstring\tcomposed\tof\tthe\tfilename,\ta\tcolon\t(:), and\tthe\tline\tnumber\t—\tand\tthe\tvalue\tis\tsimply\ta\tstring\tcontaining\tthe\tcontent\tof\tthe\tline:\n\nkey,\tvalue\t=\treader.read(filename_queue)\n\nWe\thave\tall\twe\tneed\tto\tread\tthe\tfile\tline\tby\tline!\tBut\twe\tare\tnot\tquite\tdone\tyet\t—\twe\tneed\tto\tparse\tthis string\tto\tget\tthe\tfeatures\tand\ttarget:\n\nx1,\tx2,\ttarget\t=\ttf.decode_csv(value,\trecord_defaults=[[-1.],\t[-1.],\t[-1]]) features\t=\ttf.stack([x1,\tx2])\n\nThe\tfirst\tline\tuses\tTensorFlow’s\tCSV\tparser\tto\textract\tthe\tvalues\tfrom\tthe\tcurrent\tline.\tThe\tdefault\tvalues are\tused\twhen\ta\tfield\tis\tmissing\t(in\tthis\texample\tthe\tthird\ttraining\tinstance’s\tx2\tfeature),\tand\tthey\tare\talso used\tto\tdetermine\tthe\ttype\tof\teach\tfield\t(in\tthis\tcase\ttwo\tfloats\tand\tone\tinteger).\n\nFinally,\twe\tcan\tpush\tthis\ttraining\tinstance\tand\tits\ttarget\tto\ta\tRandomShuffleQueue\tthat\twe\twill\tshare with\tthe\ttraining\tgraph\t(so\tit\tcan\tpull\tmini-batches\tfrom\tit),\tand\twe\tcreate\tan\toperation\tto\tclose\tthat\tqueue when\twe\tare\tdone\tpushing\tinstances\tto\tit:\n\ninstance_queue\t=\ttf.RandomShuffleQueue( \t\t\t\tcapacity=10,\tmin_after_dequeue=2, \t\t\t\tdtypes=[tf.float32,\ttf.int32],\tshapes=[[2],[]], \t\t\t\tname=\"instance_q\",\tshared_name=\"shared_instance_q\") enqueue_instance\t=\tinstance_queue.enqueue([features,\ttarget]) close_instance_queue\t=\tinstance_queue.close()\n\nWow!\tThat\twas\ta\tlot\tof\twork\tjust\tto\tread\ta\tfile.\tPlus\twe\tonly\tcreated\tthe\tgraph,\tso\tnow\twe\tneed\tto\trun\tit:\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\tsess.run(enqueue_filename,\tfeed_dict={filename:\t\"my_test.csv\"}) \t\t\t\tsess.run(close_filename_queue) \t\t\t\ttry: \t\t\t\t\t\t\t\twhile\tTrue: \t\t\t\t\t\t\t\t\t\t\t\tsess.run(enqueue_instance) \t\t\t\texcept\ttf.errors.OutOfRangeError\tas\tex: \t\t\t\t\t\t\t\tpass\t#\tno\tmore\trecords\tin\tthe\tcurrent\tfile\tand\tno\tmore\tfiles\tto\tread \t\t\t\tsess.run(close_instance_queue)\n\nFirst\twe\topen\tthe\tsession,\tand\tthen\twe\tenqueue\tthe\tfilename\t\"my_test.csv\"\tand\timmediately\tclose\tthat queue\tsince\twe\twill\tnot\tenqueue\tany\tmore\tfilenames.\tThen\twe\trun\tan\tinfinite\tloop\tto\tenqueue\tinstances one\tby\tone.\tThe\tenqueue_instance\tdepends\ton\tthe\treader\treading\tthe\tnext\tline,\tso\tat\tevery\titeration\ta new\trecord\tis\tread\tuntil\tit\treaches\tthe\tend\tof\tthe\tfile.\tAt\tthat\tpoint\tit\ttries\tto\tread\tthe\tfilename\tqueue\tto know\twhich\tfile\tto\tread\tnext,\tand\tsince\tthe\tqueue\tis\tclosed\tit\tthrows\tan\tOutOfRangeError\texception\t(if we\tdid\tnot\tclose\tthe\tqueue,\tit\twould\tjust\tremain\tblocked\tuntil\twe\tpushed\tanother\tfilename\tor\tclosed\tthe queue).\tLastly,\twe\tclose\tthe\tinstance\tqueue\tso\tthat\tthe\ttraining\toperations\tpulling\tfrom\tit\twon’t\tget blocked\tforever.\tFigure\t12-9\tsummarizes\twhat\twe\thave\tlearned;\tit\trepresents\ta\ttypical\tgraph\tfor\treading training\tinstances\tfrom\ta\tset\tof\tCSV\tfiles.\n\nFigure\t12-9.\tA\tgraph\tdedicated\tto\treading\ttraining\tinstances\tfrom\tCSV\tfiles\n\nIn\tthe\ttraining\tgraph,\tyou\tneed\tto\tcreate\tthe\tshared\tinstance\tqueue\tand\tsimply\tdequeue\tmini-batches\tfrom it:\n\ninstance_queue\t=\ttf.RandomShuffleQueue([...],\tshared_name=\"shared_instance_q\") mini_batch_instances,\tmini_batch_targets\t=\tinstance_queue.dequeue_up_to(2) [...]\t#\tuse\tthe\tmini_batch\tinstances\tand\ttargets\tto\tbuild\tthe\ttraining\tgraph training_op\t=\t[...]\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\ttry: \t\t\t\t\t\t\t\tfor\tstep\tin\trange(max_steps): \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op) \t\t\t\texcept\ttf.errors.OutOfRangeError\tas\tex: \t\t\t\t\t\t\t\tpass\t#\tno\tmore\ttraining\tinstances\n\nIn\tthis\texample,\tthe\tfirst\tmini-batch\twill\tcontain\tthe\tfirst\ttwo\tinstances\tof\tthe\tCSV\tfile,\tand\tthe\tsecond mini-batch\twill\tcontain\tthe\tlast\tinstance.\n\nWARNING\n\nTensorFlow\tqueues\tdon’t\thandle\tsparse\ttensors\twell,\tso\tif\tyour\ttraining\tinstances\tare\tsparse\tyou\tshould\tparse\tthe\trecords\tafter the\tinstance\tqueue.\n\nThis\tarchitecture\twill\tonly\tuse\tone\tthread\tto\tread\trecords\tand\tpush\tthem\tto\tthe\tinstance\tqueue.\tYou\tcan get\ta\tmuch\thigher\tthroughput\tby\thaving\tmultiple\tthreads\tread\tsimultaneously\tfrom\tmultiple\tfiles\tusing multiple\treaders.\tLet’s\tsee\thow.\n\nMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\nTo\thave\tmultiple\tthreads\tread\tinstances\tsimultaneously,\tyou\tcould\tcreate\tPython\tthreads\t(using\tthe threading\tmodule)\tand\tmanage\tthem\tyourself.\tHowever,\tTensorFlow\tprovides\tsome\ttools\tto\tmake\tthis simpler:\tthe\tCoordinator\tclass\tand\tthe\tQueueRunner\tclass.\n\nA\tCoordinator\tis\ta\tvery\tsimple\tobject\twhose\tsole\tpurpose\tis\tto\tcoordinate\tstopping\tmultiple\tthreads. First\tyou\tcreate\ta\tCoordinator:\n\ncoord\t=\ttf.train.Coordinator()\n\nThen\tyou\tgive\tit\tto\tall\tthreads\tthat\tneed\tto\tstop\tjointly,\tand\ttheir\tmain\tloop\tlooks\tlike\tthis:\n\nwhile\tnot\tcoord.should_stop(): \t\t\t\t[...]\t#\tdo\tsomething\n\nAny\tthread\tcan\trequest\tthat\tevery\tthread\tstop\tby\tcalling\tthe\tCoordinator’s\trequest_stop()\tmethod:\n\ncoord.request_stop()\n\nEvery\tthread\twill\tstop\tas\tsoon\tas\tit\tfinishes\tits\tcurrent\titeration.\tYou\tcan\twait\tfor\tall\tof\tthe\tthreads\tto finish\tby\tcalling\tthe\tCoordinator’s\tjoin()\tmethod,\tpassing\tit\tthe\tlist\tof\tthreads:\n\ncoord.join(list_of_threads)\n\nA\tQueueRunner\truns\tmultiple\tthreads\tthat\teach\trun\tan\tenqueue\toperation\trepeatedly,\tfilling\tup\ta\tqueue\tas fast\tas\tpossible.\tAs\tsoon\tas\tthe\tqueue\tis\tclosed,\tthe\tnext\tthread\tthat\ttries\tto\tpush\tan\titem\tto\tthe\tqueue\twill get\tan\tOutOfRangeError;\tthis\tthread\tcatches\tthe\terror\tand\timmediately\ttells\tother\tthreads\tto\tstop\tusing\ta Coordinator.\tThe\tfollowing\tcode\tshows\thow\tyou\tcan\tuse\ta\tQueueRunner\tto\thave\tfive\tthreads\treading instances\tsimultaneously\tand\tpushing\tthem\tto\tan\tinstance\tqueue:\n\n[...]\t#\tsame\tconstruction\tphase\tas\tearlier queue_runner\t=\ttf.train.QueueRunner(instance_queue,\t[enqueue_instance]\t*\t5)\n\nwith\ttf.Session()\tas\tsess:\n\nsess.run(enqueue_filename,\tfeed_dict={filename:\t\"my_test.csv\"}) \t\t\t\tsess.run(close_filename_queue) \t\t\t\tcoord\t=\ttf.train.Coordinator() \t\t\t\tenqueue_threads\t=\tqueue_runner.create_threads(sess,\tcoord=coord,\tstart=True)\n\nThe\tfirst\tline\tcreates\tthe\tQueueRunner\tand\ttells\tit\tto\trun\tfive\tthreads,\tall\trunning\tthe\tsame enqueue_instance\toperation\trepeatedly.\tThen\twe\tstart\ta\tsession\tand\twe\tenqueue\tthe\tname\tof\tthe\tfiles\tto read\t(in\tthis\tcase\tjust\t\"my_test.csv\").\tNext\twe\tcreate\ta\tCoordinator\tthat\tthe\tQueueRunner\twill\tuse\tto stop\tgracefully,\tas\tjust\texplained.\tFinally,\twe\ttell\tthe\tQueueRunner\tto\tcreate\tthe\tthreads\tand\tstart\tthem. The\tthreads\twill\tread\tall\ttraining\tinstances\tand\tpush\tthem\tto\tthe\tinstance\tqueue,\tand\tthen\tthey\twill\tall\tstop gracefully.\n\nThis\twill\tbe\ta\tbit\tmore\tefficient\tthan\tearlier,\tbut\twe\tcan\tdo\tbetter.\tCurrently\tall\tthreads\tare\treading\tfrom the\tsame\tfile.\tWe\tcan\tmake\tthem\tread\tsimultaneously\tfrom\tseparate\tfiles\tinstead\t(assuming\tthe\ttraining data\tis\tsharded\tacross\tmultiple\tCSV\tfiles)\tby\tcreating\tmultiple\treaders\t(see\tFigure\t12-10).\n\nFigure\t12-10.\tReading\tsimultaneously\tfrom\tmultiple\tfiles\n\nFor\tthis\twe\tneed\tto\twrite\ta\tsmall\tfunction\tto\tcreate\ta\treader\tand\tthe\tnodes\tthat\twill\tread\tand\tpush\tone instance\tto\tthe\tinstance\tqueue:\n\ndef\tread_and_push_instance(filename_queue,\tinstance_queue): \t\t\t\treader\t=\ttf.TextLineReader(skip_header_lines=1) \t\t\t\tkey,\tvalue\t=\treader.read(filename_queue) \t\t\t\tx1,\tx2,\ttarget\t=\ttf.decode_csv(value,\trecord_defaults=[[-1.],\t[-1.],\t[-1]]) \t\t\t\tfeatures\t=\ttf.stack([x1,\tx2]) \t\t\t\tenqueue_instance\t=\tinstance_queue.enqueue([features,\ttarget]) \t\t\t\treturn\tenqueue_instance\n\nNext\twe\tdefine\tthe\tqueues:\n\nfilename_queue\t=\ttf.FIFOQueue(capacity=10,\tdtypes=[tf.string],\tshapes=[()]) filename\t=\ttf.placeholder(tf.string) enqueue_filename\t=\tfilename_queue.enqueue([filename]) close_filename_queue\t=\tfilename_queue.close()\n\ninstance_queue\t=\ttf.RandomShuffleQueue([...])\n\nAnd\tfinally\twe\tcreate\tthe\tQueueRunner,\tbut\tthis\ttime\twe\tgive\tit\ta\tlist\tof\tdifferent\tenqueue\toperations. Each\toperation\twill\tuse\ta\tdifferent\treader,\tso\tthe\tthreads\twill\tsimultaneously\tread\tfrom\tdifferent\tfiles:\n\nread_and_enqueue_ops\t=\t[ \t\t\t\tread_and_push_instance(filename_queue,\tinstance_queue)\n\nfor\ti\tin\trange(5)] queue_runner\t=\ttf.train.QueueRunner(instance_queue,\tread_and_enqueue_ops)\n\nThe\texecution\tphase\tis\tthen\tthe\tsame\tas\tbefore:\tfirst\tpush\tthe\tnames\tof\tthe\tfiles\tto\tread,\tthen\tcreate\ta Coordinator\tand\tcreate\tand\tstart\tthe\tQueueRunner\tthreads.\tThis\ttime\tall\tthreads\twill\tread\tfrom different\tfiles\tsimultaneously\tuntil\tall\tfiles\tare\tread\tentirely,\tand\tthen\tthe\tQueueRunner\twill\tclose\tthe instance\tqueue\tso\tthat\tother\tops\tpulling\tfrom\tit\tdon’t\tget\tblocked.\n\nOther\tconvenience\tfunctions\n\nTensorFlow\talso\toffers\ta\tfew\tconvenience\tfunctions\tto\tsimplify\tsome\tcommon\ttasks\twhen\treading training\tinstances.\tWe\twill\tgo\tover\tjust\ta\tfew\t(see\tthe\tAPI\tdocumentation\tfor\tthe\tfull\tlist).\n\nThe\tstring_input_producer()\ttakes\ta\t1D\ttensor\tcontaining\ta\tlist\tof\tfilenames,\tcreates\ta\tthread\tthat pushes\tone\tfilename\tat\ta\ttime\tto\tthe\tfilename\tqueue,\tand\tthen\tcloses\tthe\tqueue.\tIf\tyou\tspecify\ta\tnumber\tof epochs,\tit\twill\tcycle\tthrough\tthe\tfilenames\tonce\tper\tepoch\tbefore\tclosing\tthe\tqueue.\tBy\tdefault,\tit\tshuffles the\tfilenames\tat\teach\tepoch.\tIt\tcreates\ta\tQueueRunner\tto\tmanage\tits\tthread,\tand\tadds\tit\tto\tthe GraphKeys.QUEUE_RUNNERS\tcollection.\tTo\tstart\tevery\tQueueRunner\tin\tthat\tcollection,\tyou\tcan\tcall\tthe tf.train.start_queue_runners()\tfunction.\tNote\tthat\tif\tyou\tforget\tto\tstart\tthe\tQueueRunner,\tthe filename\tqueue\twill\tbe\topen\tand\tempty,\tand\tyour\treaders\twill\tbe\tblocked\tforever.\n\nThere\tare\ta\tfew\tother\tproducer\tfunctions\tthat\tsimilarly\tcreate\ta\tqueue\tand\ta\tcorresponding\tQueueRunner for\trunning\tan\tenqueue\toperation\t(e.g.,\tinput_producer(),\trange_input_producer(),\tand slice_input_producer()).\n\nThe\tshuffle_batch()\tfunction\ttakes\ta\tlist\tof\ttensors\t(e.g.,\t[features,\ttarget])\tand\tcreates:\n\nA\tRandomShuffleQueue\n\nA\tQueueRunner\tto\tenqueue\tthe\ttensors\tto\tthe\tqueue\t(added\tto\tthe\tGraphKeys.QUEUE_RUNNERS collection)\n\nA\tdequeue_many\toperation\tto\textract\ta\tmini-batch\tfrom\tthe\tqueue\n\nThis\tmakes\tit\teasy\tto\tmanage\tin\ta\tsingle\tprocess\ta\tmultithreaded\tinput\tpipeline\tfeeding\ta\tqueue\tand\ta training\tpipeline\treading\tmini-batches\tfrom\tthat\tqueue.\tAlso\tcheck\tout\tthe\tbatch(),\tbatch_join(),\tand shuffle_batch_join()\tfunctions\tthat\tprovide\tsimilar\tfunctionality.\n\nOkay!\tYou\tnow\thave\tall\tthe\ttools\tyou\tneed\tto\tstart\ttraining\tand\trunning\tneural\tnetworks\tefficiently\tacross multiple\tdevices\tand\tservers\ton\ta\tTensorFlow\tcluster.\tLet’s\treview\twhat\tyou\thave\tlearned:\n\nUsing\tmultiple\tGPU\tdevices\n\nSetting\tup\tand\tstarting\ta\tTensorFlow\tcluster\n\nDistributing\tcomputations\tacross\tmultiple\tdevices\tand\tservers\n\nSharing\tvariables\t(and\tother\tstateful\tops\tsuch\tas\tqueues\tand\treaders)\tacross\tsessions\tusing containers\n\nCoordinating\tmultiple\tgraphs\tworking\tasynchronously\tusing\tqueues\n\nReading\tinputs\tefficiently\tusing\treaders,\tqueue\trunners,\tand\tcoordinators\n\nNow\tlet’s\tuse\tall\tof\tthis\tto\tparallelize\tneural\tnetworks!\n\nParallelizing\tNeural\tNetworks\ton\ta\tTensorFlow\tCluster In\tthis\tsection,\tfirst\twe\twill\tlook\tat\thow\tto\tparallelize\tseveral\tneural\tnetworks\tby\tsimply\tplacing\teach one\ton\ta\tdifferent\tdevice.\tThen\twe\twill\tlook\tat\tthe\tmuch\ttrickier\tproblem\tof\ttraining\ta\tsingle\tneural network\tacross\tmultiple\tdevices\tand\tservers.\n\nOne\tNeural\tNetwork\tper\tDevice The\tmost\ttrivial\tway\tto\ttrain\tand\trun\tneural\tnetworks\ton\ta\tTensorFlow\tcluster\tis\tto\ttake\tthe\texact\tsame code\tyou\twould\tuse\tfor\ta\tsingle\tdevice\ton\ta\tsingle\tmachine,\tand\tspecify\tthe\tmaster\tserver’s\taddress when\tcreating\tthe\tsession.\tThat’s\tit\t—\tyou’re\tdone!\tYour\tcode\twill\tbe\trunning\ton\tthe\tserver’s\tdefault device.\tYou\tcan\tchange\tthe\tdevice\tthat\twill\trun\tyour\tgraph\tsimply\tby\tputting\tyour\tcode’s\tconstruction phase\twithin\ta\tdevice\tblock.\n\nBy\trunning\tseveral\tclient\tsessions\tin\tparallel\t(in\tdifferent\tthreads\tor\tdifferent\tprocesses),\tconnecting\tthem to\tdifferent\tservers,\tand\tconfiguring\tthem\tto\tuse\tdifferent\tdevices,\tyou\tcan\tquite\teasily\ttrain\tor\trun\tmany neural\tnetworks\tin\tparallel,\tacross\tall\tdevices\tand\tall\tmachines\tin\tyour\tcluster\t(see\tFigure\t12-11).\tThe speedup\tis\talmost\tlinear.4\tTraining\t100\tneural\tnetworks\tacross\t50\tservers\twith\t2\tGPUs\teach\twill\tnot\ttake much\tlonger\tthan\ttraining\tjust\t1\tneural\tnetwork\ton\t1\tGPU.\n\nFigure\t12-11.\tTraining\tone\tneural\tnetwork\tper\tdevice\n\nThis\tsolution\tis\tperfect\tfor\thyperparameter\ttuning:\teach\tdevice\tin\tthe\tcluster\twill\ttrain\ta\tdifferent\tmodel with\tits\town\tset\tof\thyperparameters.\tThe\tmore\tcomputing\tpower\tyou\thave,\tthe\tlarger\tthe\thyperparameter space\tyou\tcan\texplore.\n\nIt\talso\tworks\tperfectly\tif\tyou\thost\ta\tweb\tservice\tthat\treceives\ta\tlarge\tnumber\tof\tqueries\tper\tsecond (QPS)\tand\tyou\tneed\tyour\tneural\tnetwork\tto\tmake\ta\tprediction\tfor\teach\tquery.\tSimply\treplicate\tthe\tneural network\tacross\tall\tdevices\ton\tthe\tcluster\tand\tdispatch\tqueries\tacross\tall\tdevices.\tBy\tadding\tmore\tservers you\tcan\thandle\tan\tunlimited\tnumber\tof\tQPS\t(however,\tthis\twill\tnot\treduce\tthe\ttime\tit\ttakes\tto\tprocess\ta single\trequest\tsince\tit\twill\tstill\thave\tto\twait\tfor\ta\tneural\tnetwork\tto\tmake\ta\tprediction).\n\nNOTE\n\nAnother\toption\tis\tto\tserve\tyour\tneural\tnetworks\tusing\tTensorFlow\tServing.\tIt\tis\tan\topen\tsource\tsystem,\treleased\tby\tGoogle\tin February\t2016,\tdesigned\tto\tserve\ta\thigh\tvolume\tof\tqueries\tto\tMachine\tLearning\tmodels\t(typically\tbuilt\twith\tTensorFlow).\tIt handles\tmodel\tversioning,\tso\tyou\tcan\teasily\tdeploy\ta\tnew\tversion\tof\tyour\tnetwork\tto\tproduction,\tor\texperiment\twith\tvarious algorithms\twithout\tinterrupting\tyour\tservice,\tand\tit\tcan\tsustain\ta\theavy\tload\tby\tadding\tmore\tservers.\tFor\tmore\tdetails,\tcheck\tout https://tensorflow.github.io/serving/.\n\nIn-Graph\tVersus\tBetween-Graph\tReplication You\tcan\talso\tparallelize\tthe\ttraining\tof\ta\tlarge\tensemble\tof\tneural\tnetworks\tby\tsimply\tplacing\tevery neural\tnetwork\ton\ta\tdifferent\tdevice\t(ensembles\twere\tintroduced\tin\tChapter\t7).\tHowever,\tonce\tyou\twant to\trun\tthe\tensemble,\tyou\twill\tneed\tto\taggregate\tthe\tindividual\tpredictions\tmade\tby\teach\tneural\tnetwork\tto produce\tthe\tensemble’s\tprediction,\tand\tthis\trequires\ta\tbit\tof\tcoordination.\n\nThere\tare\ttwo\tmajor\tapproaches\tto\thandling\ta\tneural\tnetwork\tensemble\t(or\tany\tother\tgraph\tthat\tcontains large\tchunks\tof\tindependent\tcomputations):\n\nYou\tcan\tcreate\tone\tbig\tgraph,\tcontaining\tevery\tneural\tnetwork,\teach\tpinned\tto\ta\tdifferent\tdevice, plus\tthe\tcomputations\tneeded\tto\taggregate\tthe\tindividual\tpredictions\tfrom\tall\tthe\tneural\tnetworks (see\tFigure\t12-12).\tThen\tyou\tjust\tcreate\tone\tsession\tto\tany\tserver\tin\tthe\tcluster\tand\tlet\tit\ttake\tcare\tof everything\t(including\twaiting\tfor\tall\tindividual\tpredictions\tto\tbe\tavailable\tbefore\taggregating\tthem). This\tapproach\tis\tcalled\tin-graph\treplication.\n\nFigure\t12-12.\tIn-graph\treplication\n\nAlternatively,\tyou\tcan\tcreate\tone\tseparate\tgraph\tfor\teach\tneural\tnetwork\tand\thandle\tsynchronization between\tthese\tgraphs\tyourself.\tThis\tapproach\tis\tcalled\tbetween-graph\treplication.\tOne\ttypical implementation\tis\tto\tcoordinate\tthe\texecution\tof\tthese\tgraphs\tusing\tqueues\t(see\tFigure\t12-13).\tA\tset of\tclients\thandles\tone\tneural\tnetwork\teach,\treading\tfrom\tits\tdedicated\tinput\tqueue,\tand\twriting\tto\tits dedicated\tprediction\tqueue.\tAnother\tclient\tis\tin\tcharge\tof\treading\tthe\tinputs\tand\tpushing\tthem\tto\tall the\tinput\tqueues\t(copying\tall\tinputs\tto\tevery\tqueue).\tFinally,\tone\tlast\tclient\tis\tin\tcharge\tof\treading one\tprediction\tfrom\teach\tprediction\tqueue\tand\taggregating\tthem\tto\tproduce\tthe\tensemble’s prediction.\n\nFigure\t12-13.\tBetween-graph\treplication\n\nThese\tsolutions\thave\ttheir\tpros\tand\tcons.\tIn-graph\treplication\tis\tsomewhat\tsimpler\tto\timplement\tsince you\tdon’t\thave\tto\tmanage\tmultiple\tclients\tand\tmultiple\tqueues.\tHowever,\tbetween-graph\treplication\tis\ta bit\teasier\tto\torganize\tinto\twell-bounded\tand\teasy-to-test\tmodules.\tMoreover,\tit\tgives\tyou\tmore flexibility.\tFor\texample,\tyou\tcould\tadd\ta\tdequeue\ttimeout\tin\tthe\taggregator\tclient\tso\tthat\tthe\tensemble would\tnot\tfail\teven\tif\tone\tof\tthe\tneural\tnetwork\tclients\tcrashes\tor\tif\tone\tneural\tnetwork\ttakes\ttoo\tlong\tto produce\tits\tprediction.\tTensorFlow\tlets\tyou\tspecify\ta\ttimeout\twhen\tcalling\tthe\trun()\tfunction\tby\tpassing a\tRunOptions\twith\ttimeout_in_ms:\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\t[...] \t\t\t\trun_options\t=\ttf.RunOptions() \t\t\t\trun_options.timeout_in_ms\t=\t1000\t\t#\t1s\ttimeout \t\t\t\ttry: \t\t\t\t\t\t\t\tpred\t=\tsess.run(dequeue_prediction,\toptions=run_options) \t\t\t\texcept\ttf.errors.DeadlineExceededError\tas\tex: \t\t\t\t\t\t\t\t[...]\t#\tthe\tdequeue\toperation\ttimed\tout\tafter\t1s\n\nAnother\tway\tyou\tcan\tspecify\ta\ttimeout\tis\tto\tset\tthe\tsession’s\toperation_timeout_in_ms\tconfiguration option,\tbut\tin\tthis\tcase\tthe\trun()\tfunction\ttimes\tout\tif\tany\toperation\ttakes\tlonger\tthan\tthe\ttimeout\tdelay:\n\nconfig\t=\ttf.ConfigProto() config.operation_timeout_in_ms\t=\t1000\t\t#\t1s\ttimeout\tfor\tevery\toperation\n\nwith\ttf.Session([...],\tconfig=config)\tas\tsess: \t\t\t\t[...] \t\t\t\ttry: \t\t\t\t\t\t\t\tpred\t=\tsess.run(dequeue_prediction)\n\nexcept\ttf.errors.DeadlineExceededError\tas\tex: \t\t\t\t\t\t\t\t[...]\t\t#\tthe\tdequeue\toperation\ttimed\tout\tafter\t1s\n\nModel\tParallelism So\tfar\twe\thave\trun\teach\tneural\tnetwork\ton\ta\tsingle\tdevice.\tWhat\tif\twe\twant\tto\trun\ta\tsingle\tneural network\tacross\tmultiple\tdevices?\tThis\trequires\tchopping\tyour\tmodel\tinto\tseparate\tchunks\tand\trunning each\tchunk\ton\ta\tdifferent\tdevice.\tThis\tis\tcalled\tmodel\tparallelism.\tUnfortunately,\tmodel\tparallelism\tturns out\tto\tbe\tpretty\ttricky,\tand\tit\treally\tdepends\ton\tthe\tarchitecture\tof\tyour\tneural\tnetwork.\tFor\tfully connected\tnetworks,\tthere\tis\tgenerally\tnot\tmuch\tto\tbe\tgained\tfrom\tthis\tapproach\t(see\tFigure\t12-14). Intuitively,\tit\tmay\tseem\tthat\tan\teasy\tway\tto\tsplit\tthe\tmodel\tis\tto\tplace\teach\tlayer\ton\ta\tdifferent\tdevice,\tbut this\tdoes\tnot\twork\tsince\teach\tlayer\tneeds\tto\twait\tfor\tthe\toutput\tof\tthe\tprevious\tlayer\tbefore\tit\tcan\tdo anything.\tSo\tperhaps\tyou\tcan\tslice\tit\tvertically\t—\tfor\texample,\twith\tthe\tleft\thalf\tof\teach\tlayer\ton\tone device,\tand\tthe\tright\tpart\ton\tanother\tdevice?\tThis\tis\tslightly\tbetter,\tsince\tboth\thalves\tof\teach\tlayer\tcan indeed\twork\tin\tparallel,\tbut\tthe\tproblem\tis\tthat\teach\thalf\tof\tthe\tnext\tlayer\trequires\tthe\toutput\tof\tboth halves,\tso\tthere\twill\tbe\ta\tlot\tof\tcross-device\tcommunication\t(represented\tby\tthe\tdashed\tarrows).\tThis\tis likely\tto\tcompletely\tcancel\tout\tthe\tbenefit\tof\tthe\tparallel\tcomputation,\tsince\tcross-device\tcommunication is\tslow\t(especially\tif\tit\tis\tacross\tseparate\tmachines).\n\nFigure\t12-14.\tSplitting\ta\tfully\tconnected\tneural\tnetwork\n\nHowever,\tas\twe\twill\tsee\tin\tChapter\t13,\tsome\tneural\tnetwork\tarchitectures,\tsuch\tas\tconvolutional\tneural networks,\tcontain\tlayers\tthat\tare\tonly\tpartially\tconnected\tto\tthe\tlower\tlayers,\tso\tit\tis\tmuch\teasier\tto distribute\tchunks\tacross\tdevices\tin\tan\tefficient\tway.\n\nFigure\t12-15.\tSplitting\ta\tpartially\tconnected\tneural\tnetwork\n\nMoreover,\tas\twe\twill\tsee\tin\tChapter\t14,\tsome\tdeep\trecurrent\tneural\tnetworks\tare\tcomposed\tof\tseveral layers\tof\tmemory\tcells\t(see\tthe\tleft\tside\tof\tFigure\t12-16).\tA\tcell’s\toutput\tat\ttime\tt\tis\tfed\tback\tto\tits\tinput at\ttime\tt\t+\t1\t(as\tyou\tcan\tsee\tmore\tclearly\ton\tthe\tright\tside\tof\tFigure\t12-16).\tIf\tyou\tsplit\tsuch\ta\tnetwork horizontally,\tplacing\teach\tlayer\ton\ta\tdifferent\tdevice,\tthen\tat\tthe\tfirst\tstep\tonly\tone\tdevice\twill\tbe\tactive, at\tthe\tsecond\tstep\ttwo\twill\tbe\tactive,\tand\tby\tthe\ttime\tthe\tsignal\tpropagates\tto\tthe\toutput\tlayer\tall\tdevices will\tbe\tactive\tsimultaneously.\tThere\tis\tstill\ta\tlot\tof\tcross-device\tcommunication\tgoing\ton,\tbut\tsince\teach cell\tmay\tbe\tfairly\tcomplex,\tthe\tbenefit\tof\trunning\tmultiple\tcells\tin\tparallel\toften\toutweighs\tthe communication\tpenalty.\n\nFigure\t12-16.\tSplitting\ta\tdeep\trecurrent\tneural\tnetwork\n\nIn\tshort,\tmodel\tparallelism\tcan\tspeed\tup\trunning\tor\ttraining\tsome\ttypes\tof\tneural\tnetworks,\tbut\tnot\tall, and\tit\trequires\tspecial\tcare\tand\ttuning,\tsuch\tas\tmaking\tsure\tthat\tdevices\tthat\tneed\tto\tcommunicate\tthe most\trun\ton\tthe\tsame\tmachine.\n\nData\tParallelism Another\tway\tto\tparallelize\tthe\ttraining\tof\ta\tneural\tnetwork\tis\tto\treplicate\tit\ton\teach\tdevice,\trun\ta\ttraining step\tsimultaneously\ton\tall\treplicas\tusing\ta\tdifferent\tmini-batch\tfor\teach,\tand\tthen\taggregate\tthe\tgradients to\tupdate\tthe\tmodel\tparameters.\tThis\tis\tcalled\tdata\tparallelism\t(see\tFigure\t12-17).\n\nFigure\t12-17.\tData\tparallelism\n\nThere\tare\ttwo\tvariants\tof\tthis\tapproach:\tsynchronous\tupdates\tand\tasynchronous\tupdates.\n\nSynchronous\tupdates\n\nWith\tsynchronous\tupdates,\tthe\taggregator\twaits\tfor\tall\tgradients\tto\tbe\tavailable\tbefore\tcomputing\tthe average\tand\tapplying\tthe\tresult\t(i.e.,\tusing\tthe\taggregated\tgradients\tto\tupdate\tthe\tmodel\tparameters). Once\ta\treplica\thas\tfinished\tcomputing\tits\tgradients,\tit\tmust\twait\tfor\tthe\tparameters\tto\tbe\tupdated\tbefore\tit can\tproceed\tto\tthe\tnext\tmini-batch.\tThe\tdownside\tis\tthat\tsome\tdevices\tmay\tbe\tslower\tthan\tothers,\tso\tall other\tdevices\twill\thave\tto\twait\tfor\tthem\tat\tevery\tstep.\tMoreover,\tthe\tparameters\twill\tbe\tcopied\tto\tevery device\talmost\tat\tthe\tsame\ttime\t(immediately\tafter\tthe\tgradients\tare\tapplied),\twhich\tmay\tsaturate\tthe parameter\tservers’\tbandwidth.\n\nTIP\n\nTo\treduce\tthe\twaiting\ttime\tat\teach\tstep,\tyou\tcould\tignore\tthe\tgradients\tfrom\tthe\tslowest\tfew\treplicas\t(typically\t~10%).\tFor example,\tyou\tcould\trun\t20\treplicas,\tbut\tonly\taggregate\tthe\tgradients\tfrom\tthe\tfastest\t18\treplicas\tat\teach\tstep,\tand\tjust\tignore\tthe gradients\tfrom\tthe\tlast\t2.\tAs\tsoon\tas\tthe\tparameters\tare\tupdated,\tthe\tfirst\t18\treplicas\tcan\tstart\tworking\tagain\timmediately, without\thaving\tto\twait\tfor\tthe\t2\tslowest\treplicas.\tThis\tsetup\tis\tgenerally\tdescribed\tas\thaving\t18\treplicas\tplus\t2\tspare\treplicas.5\n\nAsynchronous\tupdates\n\nWith\tasynchronous\tupdates,\twhenever\ta\treplica\thas\tfinished\tcomputing\tthe\tgradients,\tit\timmediately\tuses them\tto\tupdate\tthe\tmodel\tparameters.\tThere\tis\tno\taggregation\t(remove\tthe\t“mean”\tstep\tin\tFigure\t12-17), and\tno\tsynchronization.\tReplicas\tjust\twork\tindependently\tof\tthe\tother\treplicas.\tSince\tthere\tis\tno\twaiting for\tthe\tother\treplicas,\tthis\tapproach\truns\tmore\ttraining\tsteps\tper\tminute.\tMoreover,\talthough\tthe parameters\tstill\tneed\tto\tbe\tcopied\tto\tevery\tdevice\tat\tevery\tstep,\tthis\thappens\tat\tdifferent\ttimes\tfor\teach replica\tso\tthe\trisk\tof\tbandwidth\tsaturation\tis\treduced.\n\nData\tparallelism\twith\tasynchronous\tupdates\tis\tan\tattractive\tchoice,\tbecause\tof\tits\tsimplicity,\tthe\tabsence of\tsynchronization\tdelay,\tand\ta\tbetter\tuse\tof\tthe\tbandwidth.\tHowever,\talthough\tit\tworks\treasonably\twell in\tpractice,\tit\tis\talmost\tsurprising\tthat\tit\tworks\tat\tall!\tIndeed,\tby\tthe\ttime\ta\treplica\thas\tfinished\tcomputing the\tgradients\tbased\ton\tsome\tparameter\tvalues,\tthese\tparameters\twill\thave\tbeen\tupdated\tseveral\ttimes\tby other\treplicas\t(on\taverage\tN\t–\t1\ttimes\tif\tthere\tare\tN\treplicas)\tand\tthere\tis\tno\tguarantee\tthat\tthe\tcomputed gradients\twill\tstill\tbe\tpointing\tin\tthe\tright\tdirection\t(see\tFigure\t12-18).\tWhen\tgradients\tare\tseverely\tout- of-date,\tthey\tare\tcalled\tstale\tgradients:\tthey\tcan\tslow\tdown\tconvergence,\tintroducing\tnoise\tand\twobble effects\t(the\tlearning\tcurve\tmay\tcontain\ttemporary\toscillations),\tor\tthey\tcan\teven\tmake\tthe\ttraining algorithm\tdiverge.\n\nFigure\t12-18.\tStale\tgradients\twhen\tusing\tasynchronous\tupdates\n\nThere\tare\ta\tfew\tways\tto\treduce\tthe\teffect\tof\tstale\tgradients:\n\nReduce\tthe\tlearning\trate.\n\nDrop\tstale\tgradients\tor\tscale\tthem\tdown.\n\nAdjust\tthe\tmini-batch\tsize.\n\nStart\tthe\tfirst\tfew\tepochs\tusing\tjust\tone\treplica\t(this\tis\tcalled\tthe\twarmup\tphase).\tStale\tgradients tend\tto\tbe\tmore\tdamaging\tat\tthe\tbeginning\tof\ttraining,\twhen\tgradients\tare\ttypically\tlarge\tand\tthe parameters\thave\tnot\tsettled\tinto\ta\tvalley\tof\tthe\tcost\tfunction\tyet,\tso\tdifferent\treplicas\tmay\tpush\tthe parameters\tin\tquite\tdifferent\tdirections.\n\nA\tpaper\tpublished\tby\tthe\tGoogle\tBrain\tteam\tin\tApril\t2016\tbenchmarked\tvarious\tapproaches\tand\tfound that\tdata\tparallelism\twith\tsynchronous\tupdates\tusing\ta\tfew\tspare\treplicas\twas\tthe\tmost\tefficient,\tnot\tonly converging\tfaster\tbut\talso\tproducing\ta\tbetter\tmodel.\tHowever,\tthis\tis\tstill\tan\tactive\tarea\tof\tresearch,\tso you\tshould\tnot\trule\tout\tasynchronous\tupdates\tquite\tyet.\n\nBandwidth\tsaturation\n\nWhether\tyou\tuse\tsynchronous\tor\tasynchronous\tupdates,\tdata\tparallelism\tstill\trequires\tcommunicating\tthe model\tparameters\tfrom\tthe\tparameter\tservers\tto\tevery\treplica\tat\tthe\tbeginning\tof\tevery\ttraining\tstep,\tand the\tgradients\tin\tthe\tother\tdirection\tat\tthe\tend\tof\teach\ttraining\tstep.\tUnfortunately,\tthis\tmeans\tthat\tthere always\tcomes\ta\tpoint\twhere\tadding\tan\textra\tGPU\twill\tnot\timprove\tperformance\tat\tall\tbecause\tthe\ttime spent\tmoving\tthe\tdata\tin\tand\tout\tof\tGPU\tRAM\t(and\tpossibly\tacross\tthe\tnetwork)\twill\toutweigh\tthe speedup\tobtained\tby\tsplitting\tthe\tcomputation\tload.\tAt\tthat\tpoint,\tadding\tmore\tGPUs\twill\tjust\tincrease saturation\tand\tslow\tdown\ttraining.\n\nTIP\n\nFor\tsome\tmodels,\ttypically\trelatively\tsmall\tand\ttrained\ton\ta\tvery\tlarge\ttraining\tset,\tyou\tare\toften\tbetter\toff\ttraining\tthe\tmodel\ton\ta single\tmachine\twith\ta\tsingle\tGPU.\n\nSaturation\tis\tmore\tsevere\tfor\tlarge\tdense\tmodels,\tsince\tthey\thave\ta\tlot\tof\tparameters\tand\tgradients\tto transfer.\tIt\tis\tless\tsevere\tfor\tsmall\tmodels\t(but\tthe\tparallelization\tgain\tis\tsmall)\tand\talso\tfor\tlarge\tsparse models\tsince\tthe\tgradients\tare\ttypically\tmostly\tzeros,\tso\tthey\tcan\tbe\tcommunicated\tefficiently.\tJeff\tDean, initiator\tand\tlead\tof\tthe\tGoogle\tBrain\tproject,\treported\ttypical\tspeedups\tof\t25–40x\twhen\tdistributing computations\tacross\t50\tGPUs\tfor\tdense\tmodels,\tand\t300x\tspeedup\tfor\tsparser\tmodels\ttrained\tacross\t500 GPUs.\tAs\tyou\tcan\tsee,\tsparse\tmodels\treally\tdo\tscale\tbetter.\tHere\tare\ta\tfew\tconcrete\texamples:\n\nNeural\tMachine\tTranslation:\t6x\tspeedup\ton\t8\tGPUs\n\nInception/ImageNet:\t32x\tspeedup\ton\t50\tGPUs\n\nRankBrain:\t300x\tspeedup\ton\t500\tGPUs\n\nThese\tnumbers\trepresent\tthe\tstate\tof\tthe\tart\tin\tQ1\t2016.\tBeyond\ta\tfew\tdozen\tGPUs\tfor\ta\tdense\tmodel\tor few\thundred\tGPUs\tfor\ta\tsparse\tmodel,\tsaturation\tkicks\tin\tand\tperformance\tdegrades.\tThere\tis\tplenty\tof research\tgoing\ton\tto\tsolve\tthis\tproblem\t(exploring\tpeer-to-peer\tarchitectures\trather\tthan\tcentralized parameter\tservers,\tusing\tlossy\tmodel\tcompression,\toptimizing\twhen\tand\twhat\tthe\treplicas\tneed\tto communicate,\tand\tso\ton),\tso\tthere\twill\tlikely\tbe\ta\tlot\tof\tprogress\tin\tparallelizing\tneural\tnetworks\tin\tthe next\tfew\tyears.\n\nIn\tthe\tmeantime,\there\tare\ta\tfew\tsimple\tsteps\tyou\tcan\ttake\tto\treduce\tthe\tsaturation\tproblem:\n\nGroup\tyour\tGPUs\ton\ta\tfew\tservers\trather\tthan\tscattering\tthem\tacross\tmany\tservers.\tThis\twill\tavoid unnecessary\tnetwork\thops.\n\nShard\tthe\tparameters\tacross\tmultiple\tparameter\tservers\t(as\tdiscussed\tearlier).\n\nDrop\tthe\tmodel\tparameters’\tfloat\tprecision\tfrom\t32\tbits\t(tf.float32)\tto\t16\tbits\t(tf.bfloat16). This\twill\tcut\tin\thalf\tthe\tamount\tof\tdata\tto\ttransfer,\twithout\tmuch\timpact\ton\tthe\tconvergence\trate\tor the\tmodel’s\tperformance.\n\nTIP\n\nAlthough\t16-bit\tprecision\tis\tthe\tminimum\tfor\ttraining\tneural\tnetwork,\tyou\tcan\tactually\tdrop\tdown\tto\t8-bit\tprecision\tafter\ttraining to\treduce\tthe\tsize\tof\tthe\tmodel\tand\tspeed\tup\tcomputations.\tThis\tis\tcalled\tquantizing\tthe\tneural\tnetwork.\tIt\tis\tparticularly\tuseful for\tdeploying\tand\trunning\tpretrained\tmodels\ton\tmobile\tphones.\tSee\tPete\tWarden’s\tgreat\tpost\ton\tthe\tsubject.\n\nTensorFlow\timplementation\n\nTo\timplement\tdata\tparallelism\tusing\tTensorFlow,\tyou\tfirst\tneed\tto\tchoose\twhether\tyou\twant\tin-graph replication\tor\tbetween-graph\treplication,\tand\twhether\tyou\twant\tsynchronous\tupdates\tor\tasynchronous updates.\tLet’s\tlook\tat\thow\tyou\twould\timplement\teach\tcombination\t(see\tthe\texercises\tand\tthe\tJupyter notebooks\tfor\tcomplete\tcode\texamples).\n\nWith\tin-graph\treplication\t+\tsynchronous\tupdates,\tyou\tbuild\tone\tbig\tgraph\tcontaining\tall\tthe\tmodel replicas\t(placed\ton\tdifferent\tdevices),\tand\ta\tfew\tnodes\tto\taggregate\tall\ttheir\tgradients\tand\tfeed\tthem\tto an\toptimizer.\tYour\tcode\topens\ta\tsession\tto\tthe\tcluster\tand\tsimply\truns\tthe\ttraining\toperation\trepeatedly.\n\nWith\tin-graph\treplication\t+\tasynchronous\tupdates,\tyou\talso\tcreate\tone\tbig\tgraph,\tbut\twith\tone\toptimizer per\treplica,\tand\tyou\trun\tone\tthread\tper\treplica,\trepeatedly\trunning\tthe\treplica’s\toptimizer.\n\nWith\tbetween-graph\treplication\t+\tasynchronous\tupdates,\tyou\trun\tmultiple\tindependent\tclients\t(typically in\tseparate\tprocesses),\teach\ttraining\tthe\tmodel\treplica\tas\tif\tit\twere\talone\tin\tthe\tworld,\tbut\tthe\tparameters are\tactually\tshared\twith\tother\treplicas\t(using\ta\tresource\tcontainer).\n\nWith\tbetween-graph\treplication\t+\tsynchronous\tupdates,\tonce\tagain\tyou\trun\tmultiple\tclients,\teach\ttraining a\tmodel\treplica\tbased\ton\tshared\tparameters,\tbut\tthis\ttime\tyou\twrap\tthe\toptimizer\t(e.g.,\ta MomentumOptimizer)\twithin\ta\tSyncReplicasOptimizer.\tEach\treplica\tuses\tthis\toptimizer\tas\tit\twould use\tany\tother\toptimizer,\tbut\tunder\tthe\thood\tthis\toptimizer\tsends\tthe\tgradients\tto\ta\tset\tof\tqueues\t(one\tper variable),\twhich\tis\tread\tby\tone\tof\tthe\treplica’s\tSyncReplicasOptimizer,\tcalled\tthe\tchief.\tThe\tchief aggregates\tthe\tgradients\tand\tapplies\tthem,\tthen\twrites\ta\ttoken\tto\ta\ttoken\tqueue\tfor\teach\treplica,\tsignaling it\tthat\tit\tcan\tgo\tahead\tand\tcompute\tthe\tnext\tgradients.\tThis\tapproach\tsupports\thaving\tspare\treplicas.\n\nIf\tyou\tgo\tthrough\tthe\texercises,\tyou\twill\timplement\teach\tof\tthese\tfour\tsolutions.\tYou\twill\teasily\tbe\table\tto apply\twhat\tyou\thave\tlearned\tto\ttrain\tlarge\tdeep\tneural\tnetworks\tacross\tdozens\tof\tservers\tand\tGPUs!\tIn the\tfollowing\tchapters\twe\twill\tgo\tthrough\ta\tfew\tmore\timportant\tneural\tnetwork\tarchitectures\tbefore\twe tackle\tReinforcement\tLearning.\n\nExercises\n\n1.\t If\tyou\tget\ta\tCUDA_ERROR_OUT_OF_MEMORY\twhen\tstarting\tyour\tTensorFlow\tprogram,\twhat\tis probably\tgoing\ton?\tWhat\tcan\tyou\tdo\tabout\tit?\n\n2.\t What\tis\tthe\tdifference\tbetween\tpinning\tan\toperation\ton\ta\tdevice\tand\tplacing\tan\toperation\ton\ta device?\n\n3.\t If\tyou\tare\trunning\ton\ta\tGPU-enabled\tTensorFlow\tinstallation,\tand\tyou\tjust\tuse\tthe\tdefault\tplacement, will\tall\toperations\tbe\tplaced\ton\tthe\tfirst\tGPU?\n\n4.\t If\tyou\tpin\ta\tvariable\tto\t\"/gpu:0\",\tcan\tit\tbe\tused\tby\toperations\tplaced\ton\t/gpu:1?\tOr\tby\toperations placed\ton\t\"/cpu:0\"?\tOr\tby\toperations\tpinned\tto\tdevices\tlocated\ton\tother\tservers?\n\n5.\t Can\ttwo\toperations\tplaced\ton\tthe\tsame\tdevice\trun\tin\tparallel?\n\n6.\t What\tis\ta\tcontrol\tdependency\tand\twhen\twould\tyou\twant\tto\tuse\tone?\n\n7.\t Suppose\tyou\ttrain\ta\tDNN\tfor\tdays\ton\ta\tTensorFlow\tcluster,\tand\timmediately\tafter\tyour\ttraining program\tends\tyou\trealize\tthat\tyou\tforgot\tto\tsave\tthe\tmodel\tusing\ta\tSaver.\tIs\tyour\ttrained\tmodel\tlost?\n\n8.\t Train\tseveral\tDNNs\tin\tparallel\ton\ta\tTensorFlow\tcluster,\tusing\tdifferent\thyperparameter\tvalues.\tThis could\tbe\tDNNs\tfor\tMNIST\tclassification\tor\tany\tother\ttask\tyou\tare\tinterested\tin.\tThe\tsimplest\toption is\tto\twrite\ta\tsingle\tclient\tprogram\tthat\ttrains\tonly\tone\tDNN,\tthen\trun\tthis\tprogram\tin\tmultiple processes\tin\tparallel,\twith\tdifferent\thyperparameter\tvalues\tfor\teach\tclient.\tThe\tprogram\tshould\thave command-line\toptions\tto\tcontrol\twhat\tserver\tand\tdevice\tthe\tDNN\tshould\tbe\tplaced\ton,\tand\twhat resource\tcontainer\tand\thyperparameter\tvalues\tto\tuse\t(make\tsure\tto\tuse\ta\tdifferent\tresource\tcontainer for\teach\tDNN).\tUse\ta\tvalidation\tset\tor\tcross-validation\tto\tselect\tthe\ttop\tthree\tmodels.\n\n9.\t Create\tan\tensemble\tusing\tthe\ttop\tthree\tmodels\tfrom\tthe\tprevious\texercise.\tDefine\tit\tin\ta\tsingle graph,\tensuring\tthat\teach\tDNN\truns\ton\ta\tdifferent\tdevice.\tEvaluate\tit\ton\tthe\tvalidation\tset:\tdoes\tthe ensemble\tperform\tbetter\tthan\tthe\tindividual\tDNNs?\n\n10.\t Train\ta\tDNN\tusing\tbetween-graph\treplication\tand\tdata\tparallelism\twith\tasynchronous\tupdates, timing\thow\tlong\tit\ttakes\tto\treach\ta\tsatisfying\tperformance.\tNext,\ttry\tagain\tusing\tsynchronous updates.\tDo\tsynchronous\tupdates\tproduce\ta\tbetter\tmodel?\tIs\ttraining\tfaster?\tSplit\tthe\tDNN vertically\tand\tplace\teach\tvertical\tslice\ton\ta\tdifferent\tdevice,\tand\ttrain\tthe\tmodel\tagain.\tIs\ttraining any\tfaster?\tIs\tthe\tperformance\tany\tdifferent?\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“TensorFlow:\tLarge-Scale\tMachine\tLearning\ton\tHeterogeneous\tDistributed\tSystems,”\tGoogle\tResearch\t(2015).\n\n2\n\nYou\tcan\teven\tstart\tmultiple\ttasks\tin\tthe\tsame\tprocess.\tIt\tmay\tbe\tuseful\tfor\ttests,\tbut\tit\tis\tnot\trecommended\tin\tproduction.\n\n3\n\nIt\tis\tthe\tnext\tversion\tof\tGoogle’s\tinternal\tStubby\tservice,\twhich\tGoogle\thas\tused\tsuccessfully\tfor\tover\ta\tdecade.\tSee\thttp://grpc.io/\tfor more\tdetails.\n\n4\n\nNot\t100%\tlinear\tif\tyou\twait\tfor\tall\tdevices\tto\tfinish,\tsince\tthe\ttotal\ttime\twill\tbe\tthe\ttime\ttaken\tby\tthe\tslowest\tdevice.\n\n5\n\nThis\tname\tis\tslightly\tconfusing\tsince\tit\tsounds\tlike\tsome\treplicas\tare\tspecial,\tdoing\tnothing.\tIn\treality,\tall\treplicas\tare\tequivalent:\tthey\tall work\thard\tto\tbe\tamong\tthe\tfastest\tat\teach\ttraining\tstep,\tand\tthe\tlosers\tvary\tat\tevery\tstep\t(unless\tsome\tdevices\tare\treally\tslower\tthan others).\n\nChapter\t13.\tConvolutional\tNeural\tNetworks\n\nAlthough\tIBM’s\tDeep\tBlue\tsupercomputer\tbeat\tthe\tchess\tworld\tchampion\tGarry\tKasparov\tback\tin\t1996, until\tquite\trecently\tcomputers\twere\tunable\tto\treliably\tperform\tseemingly\ttrivial\ttasks\tsuch\tas\tdetecting\ta puppy\tin\ta\tpicture\tor\trecognizing\tspoken\twords.\tWhy\tare\tthese\ttasks\tso\teffortless\tto\tus\thumans?\tThe answer\tlies\tin\tthe\tfact\tthat\tperception\tlargely\ttakes\tplace\toutside\tthe\trealm\tof\tour\tconsciousness,\twithin specialized\tvisual,\tauditory,\tand\tother\tsensory\tmodules\tin\tour\tbrains.\tBy\tthe\ttime\tsensory\tinformation reaches\tour\tconsciousness,\tit\tis\talready\tadorned\twith\thigh-level\tfeatures;\tfor\texample,\twhen\tyou\tlook\tat\ta picture\tof\ta\tcute\tpuppy,\tyou\tcannot\tchoose\tnot\tto\tsee\tthe\tpuppy,\tor\tnot\tto\tnotice\tits\tcuteness.\tNor\tcan\tyou explain\thow\tyou\trecognize\ta\tcute\tpuppy;\tit’s\tjust\tobvious\tto\tyou.\tThus,\twe\tcannot\ttrust\tour\tsubjective experience:\tperception\tis\tnot\ttrivial\tat\tall,\tand\tto\tunderstand\tit\twe\tmust\tlook\tat\thow\tthe\tsensory\tmodules work.\n\nConvolutional\tneural\tnetworks\t(CNNs)\temerged\tfrom\tthe\tstudy\tof\tthe\tbrain’s\tvisual\tcortex,\tand\tthey\thave been\tused\tin\timage\trecognition\tsince\tthe\t1980s.\tIn\tthe\tlast\tfew\tyears,\tthanks\tto\tthe\tincrease\tin computational\tpower,\tthe\tamount\tof\tavailable\ttraining\tdata,\tand\tthe\ttricks\tpresented\tin\tChapter\t11\tfor training\tdeep\tnets,\tCNNs\thave\tmanaged\tto\tachieve\tsuperhuman\tperformance\ton\tsome\tcomplex\tvisual tasks.\tThey\tpower\timage\tsearch\tservices,\tself-driving\tcars,\tautomatic\tvideo\tclassification\tsystems,\tand more.\tMoreover,\tCNNs\tare\tnot\trestricted\tto\tvisual\tperception:\tthey\tare\talso\tsuccessful\tat\tother\ttasks, such\tas\tvoice\trecognition\tor\tnatural\tlanguage\tprocessing\t(NLP);\thowever,\twe\twill\tfocus\ton\tvisual applications\tfor\tnow.\n\nIn\tthis\tchapter\twe\twill\tpresent\twhere\tCNNs\tcame\tfrom,\twhat\ttheir\tbuilding\tblocks\tlook\tlike,\tand\thow\tto implement\tthem\tusing\tTensorFlow.\tThen\twe\twill\tpresent\tsome\tof\tthe\tbest\tCNN\tarchitectures.\n\nThe\tArchitecture\tof\tthe\tVisual\tCortex David\tH.\tHubel\tand\tTorsten\tWiesel\tperformed\ta\tseries\tof\texperiments\ton\tcats\tin\t19581\tand\t19592\t(and\ta few\tyears\tlater\ton\tmonkeys3),\tgiving\tcrucial\tinsights\ton\tthe\tstructure\tof\tthe\tvisual\tcortex\t(the\tauthors received\tthe\tNobel\tPrize\tin\tPhysiology\tor\tMedicine\tin\t1981\tfor\ttheir\twork).\tIn\tparticular,\tthey\tshowed that\tmany\tneurons\tin\tthe\tvisual\tcortex\thave\ta\tsmall\tlocal\treceptive\tfield,\tmeaning\tthey\treact\tonly\tto\tvisual stimuli\tlocated\tin\ta\tlimited\tregion\tof\tthe\tvisual\tfield\t(see\tFigure\t13-1,\tin\twhich\tthe\tlocal\treceptive\tfields of\tfive\tneurons\tare\trepresented\tby\tdashed\tcircles).\tThe\treceptive\tfields\tof\tdifferent\tneurons\tmay\toverlap, and\ttogether\tthey\ttile\tthe\twhole\tvisual\tfield.\tMoreover,\tthe\tauthors\tshowed\tthat\tsome\tneurons\treact\tonly to\timages\tof\thorizontal\tlines,\twhile\tothers\treact\tonly\tto\tlines\twith\tdifferent\torientations\t(two\tneurons\tmay have\tthe\tsame\treceptive\tfield\tbut\treact\tto\tdifferent\tline\torientations).\tThey\talso\tnoticed\tthat\tsome\tneurons have\tlarger\treceptive\tfields,\tand\tthey\treact\tto\tmore\tcomplex\tpatterns\tthat\tare\tcombinations\tof\tthe\tlower- level\tpatterns.\tThese\tobservations\tled\tto\tthe\tidea\tthat\tthe\thigher-level\tneurons\tare\tbased\ton\tthe\toutputs\tof neighboring\tlower-level\tneurons\t(in\tFigure\t13-1,\tnotice\tthat\teach\tneuron\tis\tconnected\tonly\tto\ta\tfew neurons\tfrom\tthe\tprevious\tlayer).\tThis\tpowerful\tarchitecture\tis\table\tto\tdetect\tall\tsorts\tof\tcomplex\tpatterns in\tany\tarea\tof\tthe\tvisual\tfield.\n\nFigure\t13-1.\tLocal\treceptive\tfields\tin\tthe\tvisual\tcortex\n\nThese\tstudies\tof\tthe\tvisual\tcortex\tinspired\tthe\tneocognitron,\tintroduced\tin\t1980,4\twhich\tgradually evolved\tinto\twhat\twe\tnow\tcall\tconvolutional\tneural\tnetworks.\tAn\timportant\tmilestone\twas\ta\t1998 paper5\tby\tYann\tLeCun,\tLéon\tBottou,\tYoshua\tBengio,\tand\tPatrick\tHaffner,\twhich\tintroduced\tthe\tfamous LeNet-5\tarchitecture,\twidely\tused\tto\trecognize\thandwritten\tcheck\tnumbers.\tThis\tarchitecture\thas\tsome building\tblocks\tthat\tyou\talready\tknow,\tsuch\tas\tfully\tconnected\tlayers\tand\tsigmoid\tactivation\tfunctions, but\tit\talso\tintroduces\ttwo\tnew\tbuilding\tblocks:\tconvolutional\tlayers\tand\tpooling\tlayers.\tLet’s\tlook\tat them\tnow.\n\nNOTE\n\nWhy\tnot\tsimply\tuse\ta\tregular\tdeep\tneural\tnetwork\twith\tfully\tconnected\tlayers\tfor\timage\trecognition\ttasks?\tUnfortunately, although\tthis\tworks\tfine\tfor\tsmall\timages\t(e.g.,\tMNIST),\tit\tbreaks\tdown\tfor\tlarger\timages\tbecause\tof\tthe\thuge\tnumber\tof parameters\tit\trequires.\tFor\texample,\ta\t100\t×\t100\timage\thas\t10,000\tpixels,\tand\tif\tthe\tfirst\tlayer\thas\tjust\t1,000\tneurons\t(which already\tseverely\trestricts\tthe\tamount\tof\tinformation\ttransmitted\tto\tthe\tnext\tlayer),\tthis\tmeans\ta\ttotal\tof\t10\tmillion\tconnections. And\tthat’s\tjust\tthe\tfirst\tlayer.\tCNNs\tsolve\tthis\tproblem\tusing\tpartially\tconnected\tlayers.\n\nConvolutional\tLayer The\tmost\timportant\tbuilding\tblock\tof\ta\tCNN\tis\tthe\tconvolutional\tlayer:6\tneurons\tin\tthe\tfirst convolutional\tlayer\tare\tnot\tconnected\tto\tevery\tsingle\tpixel\tin\tthe\tinput\timage\t(like\tthey\twere\tin\tprevious chapters),\tbut\tonly\tto\tpixels\tin\ttheir\treceptive\tfields\t(see\tFigure\t13-2).\tIn\tturn,\teach\tneuron\tin\tthe\tsecond convolutional\tlayer\tis\tconnected\tonly\tto\tneurons\tlocated\twithin\ta\tsmall\trectangle\tin\tthe\tfirst\tlayer.\tThis architecture\tallows\tthe\tnetwork\tto\tconcentrate\ton\tlow-level\tfeatures\tin\tthe\tfirst\thidden\tlayer,\tthen assemble\tthem\tinto\thigher-level\tfeatures\tin\tthe\tnext\thidden\tlayer,\tand\tso\ton.\tThis\thierarchical\tstructure\tis common\tin\treal-world\timages,\twhich\tis\tone\tof\tthe\treasons\twhy\tCNNs\twork\tso\twell\tfor\timage recognition.\n\nFigure\t13-2.\tCNN\tlayers\twith\trectangular\tlocal\treceptive\tfields\n\nNOTE\n\nUntil\tnow,\tall\tmultilayer\tneural\tnetworks\twe\tlooked\tat\thad\tlayers\tcomposed\tof\ta\tlong\tline\tof\tneurons,\tand\twe\thad\tto\tflatten\tinput images\tto\t1D\tbefore\tfeeding\tthem\tto\tthe\tneural\tnetwork.\tNow\teach\tlayer\tis\trepresented\tin\t2D,\twhich\tmakes\tit\teasier\tto\tmatch neurons\twith\ttheir\tcorresponding\tinputs.\n\nA\tneuron\tlocated\tin\trow\ti,\tcolumn\tj\tof\ta\tgiven\tlayer\tis\tconnected\tto\tthe\toutputs\tof\tthe\tneurons\tin\tthe previous\tlayer\tlocated\tin\trows\ti\tto\ti\t+\tfh\t–\t1,\tcolumns\tj\tto\tj\t+\tfw\t–\t1,\twhere\tfh\tand\tfw\tare\tthe\theight\tand width\tof\tthe\treceptive\tfield\t(see\tFigure\t13-3).\tIn\torder\tfor\ta\tlayer\tto\thave\tthe\tsame\theight\tand\twidth\tas the\tprevious\tlayer,\tit\tis\tcommon\tto\tadd\tzeros\taround\tthe\tinputs,\tas\tshown\tin\tthe\tdiagram.\tThis\tis\tcalled zero\tpadding.",
      "page_number": 393
    },
    {
      "number": 13,
      "title": "Convolutional\tNeural\tNetworks",
      "start_page": 442,
      "end_page": 468,
      "detection_method": "regex_chapter_title",
      "content": "Figure\t13-3.\tConnections\tbetween\tlayers\tand\tzero\tpadding\n\nIt\tis\talso\tpossible\tto\tconnect\ta\tlarge\tinput\tlayer\tto\ta\tmuch\tsmaller\tlayer\tby\tspacing\tout\tthe\treceptive fields,\tas\tshown\tin\tFigure\t13-4.\tThe\tdistance\tbetween\ttwo\tconsecutive\treceptive\tfields\tis\tcalled\tthe stride.\tIn\tthe\tdiagram,\ta\t5\t×\t7\tinput\tlayer\t(plus\tzero\tpadding)\tis\tconnected\tto\ta\t3\t×\t4\tlayer,\tusing\t3\t×\t3 receptive\tfields\tand\ta\tstride\tof\t2\t(in\tthis\texample\tthe\tstride\tis\tthe\tsame\tin\tboth\tdirections,\tbut\tit\tdoes\tnot have\tto\tbe\tso).\tA\tneuron\tlocated\tin\trow\ti,\tcolumn\tj\tin\tthe\tupper\tlayer\tis\tconnected\tto\tthe\toutputs\tof\tthe neurons\tin\tthe\tprevious\tlayer\tlocated\tin\trows\ti\t×\tsh\tto\ti\t×\tsh\t+\tfh\t–\t1,\tcolumns\tj\t×\tsw\t+\tfw\t–\t1,\twhere\tsh and\tsw\tare\tthe\tvertical\tand\thorizontal\tstrides.\n\nFigure\t13-4.\tReducing\tdimensionality\tusing\ta\tstride\n\nFilters A\tneuron’s\tweights\tcan\tbe\trepresented\tas\ta\tsmall\timage\tthe\tsize\tof\tthe\treceptive\tfield.\tFor\texample, Figure\t13-5\tshows\ttwo\tpossible\tsets\tof\tweights,\tcalled\tfilters\t(or\tconvolution\tkernels).\tThe\tfirst\tone\tis represented\tas\ta\tblack\tsquare\twith\ta\tvertical\twhite\tline\tin\tthe\tmiddle\t(it\tis\ta\t7\t×\t7\tmatrix\tfull\tof\t0s\texcept for\tthe\tcentral\tcolumn,\twhich\tis\tfull\tof\t1s);\tneurons\tusing\tthese\tweights\twill\tignore\teverything\tin\ttheir receptive\tfield\texcept\tfor\tthe\tcentral\tvertical\tline\t(since\tall\tinputs\twill\tget\tmultiplied\tby\t0,\texcept\tfor\tthe ones\tlocated\tin\tthe\tcentral\tvertical\tline).\tThe\tsecond\tfilter\tis\ta\tblack\tsquare\twith\ta\thorizontal\twhite\tline in\tthe\tmiddle.\tOnce\tagain,\tneurons\tusing\tthese\tweights\twill\tignore\teverything\tin\ttheir\treceptive\tfield except\tfor\tthe\tcentral\thorizontal\tline.\n\nNow\tif\tall\tneurons\tin\ta\tlayer\tuse\tthe\tsame\tvertical\tline\tfilter\t(and\tthe\tsame\tbias\tterm),\tand\tyou\tfeed\tthe network\tthe\tinput\timage\tshown\tin\tFigure\t13-5\t(bottom\timage),\tthe\tlayer\twill\toutput\tthe\ttop-left\timage. Notice\tthat\tthe\tvertical\twhite\tlines\tget\tenhanced\twhile\tthe\trest\tgets\tblurred.\tSimilarly,\tthe\tupper-right image\tis\twhat\tyou\tget\tif\tall\tneurons\tuse\tthe\thorizontal\tline\tfilter;\tnotice\tthat\tthe\thorizontal\twhite\tlines\tget enhanced\twhile\tthe\trest\tis\tblurred\tout.\tThus,\ta\tlayer\tfull\tof\tneurons\tusing\tthe\tsame\tfilter\tgives\tyou\ta feature\tmap,\twhich\thighlights\tthe\tareas\tin\tan\timage\tthat\tare\tmost\tsimilar\tto\tthe\tfilter.\tDuring\ttraining,\ta CNN\tfinds\tthe\tmost\tuseful\tfilters\tfor\tits\ttask,\tand\tit\tlearns\tto\tcombine\tthem\tinto\tmore\tcomplex\tpatterns (e.g.,\ta\tcross\tis\tan\tarea\tin\tan\timage\twhere\tboth\tthe\tvertical\tfilter\tand\tthe\thorizontal\tfilter\tare\tactive).\n\nFigure\t13-5.\tApplying\ttwo\tdifferent\tfilters\tto\tget\ttwo\tfeature\tmaps\n\nStacking\tMultiple\tFeature\tMaps Up\tto\tnow,\tfor\tsimplicity,\twe\thave\trepresented\teach\tconvolutional\tlayer\tas\ta\tthin\t2D\tlayer,\tbut\tin\treality it\tis\tcomposed\tof\tseveral\tfeature\tmaps\tof\tequal\tsizes,\tso\tit\tis\tmore\taccurately\trepresented\tin\t3D\t(see Figure\t13-6).\tWithin\tone\tfeature\tmap,\tall\tneurons\tshare\tthe\tsame\tparameters\t(weights\tand\tbias\tterm),\tbut different\tfeature\tmaps\tmay\thave\tdifferent\tparameters.\tA\tneuron’s\treceptive\tfield\tis\tthe\tsame\tas\tdescribed earlier,\tbut\tit\textends\tacross\tall\tthe\tprevious\tlayers’\tfeature\tmaps.\tIn\tshort,\ta\tconvolutional\tlayer simultaneously\tapplies\tmultiple\tfilters\tto\tits\tinputs,\tmaking\tit\tcapable\tof\tdetecting\tmultiple\tfeatures anywhere\tin\tits\tinputs.\n\nNOTE\n\nThe\tfact\tthat\tall\tneurons\tin\ta\tfeature\tmap\tshare\tthe\tsame\tparameters\tdramatically\treduces\tthe\tnumber\tof\tparameters\tin\tthe model,\tbut\tmost\timportantly\tit\tmeans\tthat\tonce\tthe\tCNN\thas\tlearned\tto\trecognize\ta\tpattern\tin\tone\tlocation,\tit\tcan\trecognize\tit\tin any\tother\tlocation.\tIn\tcontrast,\tonce\ta\tregular\tDNN\thas\tlearned\tto\trecognize\ta\tpattern\tin\tone\tlocation,\tit\tcan\trecognize\tit\tonly\tin that\tparticular\tlocation.\n\nMoreover,\tinput\timages\tare\talso\tcomposed\tof\tmultiple\tsublayers:\tone\tper\tcolor\tchannel.\tThere\tare typically\tthree:\tred,\tgreen,\tand\tblue\t(RGB).\tGrayscale\timages\thave\tjust\tone\tchannel,\tbut\tsome\timages may\thave\tmuch\tmore\t—\tfor\texample,\tsatellite\timages\tthat\tcapture\textra\tlight\tfrequencies\t(such\tas infrared).\n\nFigure\t13-6.\tConvolution\tlayers\twith\tmultiple\tfeature\tmaps,\tand\timages\twith\tthree\tchannels\n\nSpecifically,\ta\tneuron\tlocated\tin\trow\ti,\tcolumn\tj\tof\tthe\tfeature\tmap\tk\tin\ta\tgiven\tconvolutional\tlayer\tl\tis connected\tto\tthe\toutputs\tof\tthe\tneurons\tin\tthe\tprevious\tlayer\tl\t–\t1,\tlocated\tin\trows\ti\t×\tsh\tto\ti\t×\tsh\t+\tfh\t–\t1 and\tcolumns\tj\t×\tsw\tto\tj\t×\tsw\t+\tfw\t–\t1,\tacross\tall\tfeature\tmaps\t(in\tlayer\tl\t–\t1).\tNote\tthat\tall\tneurons\tlocated in\tthe\tsame\trow\ti\tand\tcolumn\tj\tbut\tin\tdifferent\tfeature\tmaps\tare\tconnected\tto\tthe\toutputs\tof\tthe\texact\tsame neurons\tin\tthe\tprevious\tlayer.\n\nEquation\t13-1\tsummarizes\tthe\tpreceding\texplanations\tin\tone\tbig\tmathematical\tequation:\tit\tshows\thow\tto compute\tthe\toutput\tof\ta\tgiven\tneuron\tin\ta\tconvolutional\tlayer.\tIt\tis\ta\tbit\tugly\tdue\tto\tall\tthe\tdifferent indices,\tbut\tall\tit\tdoes\tis\tcalculate\tthe\tweighted\tsum\tof\tall\tthe\tinputs,\tplus\tthe\tbias\tterm.\n\nEquation\t13-1.\tComputing\tthe\toutput\tof\ta\tneuron\tin\ta\tconvolutional\tlayer\n\nzi,\tj,\tk\tis\tthe\toutput\tof\tthe\tneuron\tlocated\tin\trow\ti,\tcolumn\tj\tin\tfeature\tmap\tk\tof\tthe\tconvolutional\tlayer (layer\tl).\n\nAs\texplained\tearlier,\tsh\tand\tsw\tare\tthe\tvertical\tand\thorizontal\tstrides,\tfh\tand\tfw\tare\tthe\theight\tand width\tof\tthe\treceptive\tfield,\tand\tfn′\tis\tthe\tnumber\tof\tfeature\tmaps\tin\tthe\tprevious\tlayer\t(layer\tl\t–\t1).\n\nxi′,\tj′,\tk′\tis\tthe\toutput\tof\tthe\tneuron\tlocated\tin\tlayer\tl\t–\t1,\trow\ti′,\tcolumn\tj′,\tfeature\tmap\tk′\t(or\tchannel\tk′ if\tthe\tprevious\tlayer\tis\tthe\tinput\tlayer).\n\nbk\tis\tthe\tbias\tterm\tfor\tfeature\tmap\tk\t(in\tlayer\tl).\tYou\tcan\tthink\tof\tit\tas\ta\tknob\tthat\ttweaks\tthe\toverall brightness\tof\tthe\tfeature\tmap\tk.\n\nwu,\tv,\tk′\t,k\tis\tthe\tconnection\tweight\tbetween\tany\tneuron\tin\tfeature\tmap\tk\tof\tthe\tlayer\tl\tand\tits\tinput located\tat\trow\tu,\tcolumn\tv\t(relative\tto\tthe\tneuron’s\treceptive\tfield),\tand\tfeature\tmap\tk′.\n\nTensorFlow\tImplementation In\tTensorFlow,\teach\tinput\timage\tis\ttypically\trepresented\tas\ta\t3D\ttensor\tof\tshape\t[height,\twidth, channels].\tA\tmini-batch\tis\trepresented\tas\ta\t4D\ttensor\tof\tshape\t[mini-batch\tsize,\theight, width,\tchannels].\tThe\tweights\tof\ta\tconvolutional\tlayer\tare\trepresented\tas\ta\t4D\ttensor\tof\tshape\t[fh,\tfw, fn′,\tfn].\tThe\tbias\tterms\tof\ta\tconvolutional\tlayer\tare\tsimply\trepresented\tas\ta\t1D\ttensor\tof\tshape\t[fn].\n\nLet’s\tlook\tat\ta\tsimple\texample.\tThe\tfollowing\tcode\tloads\ttwo\tsample\timages,\tusing\tScikit-Learn’s load_sample_images()\t(which\tloads\ttwo\tcolor\timages,\tone\tof\ta\tChinese\ttemple,\tand\tthe\tother\tof\ta flower).\tThen\tit\tcreates\ttwo\t7\t×\t7\tfilters\t(one\twith\ta\tvertical\twhite\tline\tin\tthe\tmiddle,\tand\tthe\tother\twith\ta horizontal\twhite\tline\tin\tthe\tmiddle),\tand\tapplies\tthem\tto\tboth\timages\tusing\ta\tconvolutional\tlayer\tbuilt using\tTensorFlow’s\ttf.nn.conv2d()\tfunction\t(with\tzero\tpadding\tand\ta\tstride\tof\t2).\tFinally,\tit\tplots\tone of\tthe\tresulting\tfeature\tmaps\t(similar\tto\tthe\ttop-right\timage\tin\tFigure\t13-5).\n\nimport\tnumpy\tas\tnp from\tsklearn.datasets\timport\tload_sample_images\n\n#\tLoad\tsample\timages china\t=\tload_sample_image(\"china.jpg\") flower\t=\tload_sample_image(\"flower.jpg\") dataset\t=\tnp.array([china,\tflower],\tdtype=np.float32) batch_size,\theight,\twidth,\tchannels\t=\tdataset.shape\n\n#\tCreate\t2\tfilters filters\t=\tnp.zeros(shape=(7,\t7,\tchannels,\t2),\tdtype=np.float32) filters[:,\t3,\t:,\t0]\t=\t1\t\t#\tvertical\tline filters[3,\t:,\t:,\t1]\t=\t1\t\t#\thorizontal\tline\n\n#\tCreate\ta\tgraph\twith\tinput\tX\tplus\ta\tconvolutional\tlayer\tapplying\tthe\t2\tfilters X\t=\ttf.placeholder(tf.float32,\tshape=(None,\theight,\twidth,\tchannels)) convolution\t=\ttf.nn.conv2d(X,\tfilters,\tstrides=[1,2,2,1],\tpadding=\"SAME\")\n\nwith\ttf.Session()\tas\tsess: \t\t\t\toutput\t=\tsess.run(convolution,\tfeed_dict={X:\tdataset})\n\nplt.imshow(output[0,\t:,\t:,\t1],\tcmap=\"gray\")\t#\tplot\t1st\timage's\t2nd\tfeature\tmap plt.show()\n\nMost\tof\tthis\tcode\tis\tself-explanatory,\tbut\tthe\ttf.nn.conv2d()\tline\tdeserves\ta\tbit\tof\texplanation:\n\nX\tis\tthe\tinput\tmini-batch\t(a\t4D\ttensor,\tas\texplained\tearlier).\n\nfilters\tis\tthe\tset\tof\tfilters\tto\tapply\t(also\ta\t4D\ttensor,\tas\texplained\tearlier).\n\nstrides\tis\ta\tfour-element\t1D\tarray,\twhere\tthe\ttwo\tcentral\telements\tare\tthe\tvertical\tand\thorizontal strides\t(sh\tand\tsw).\tThe\tfirst\tand\tlast\telements\tmust\tcurrently\tbe\tequal\tto\t1.\tThey\tmay\tone\tday\tbe used\tto\tspecify\ta\tbatch\tstride\t(to\tskip\tsome\tinstances)\tand\ta\tchannel\tstride\t(to\tskip\tsome\tof\tthe previous\tlayer’s\tfeature\tmaps\tor\tchannels).\n\npadding\tmust\tbe\teither\t\"VALID\"\tor\t\"SAME\":\n\nIf\tset\tto\t\"VALID\",\tthe\tconvolutional\tlayer\tdoes\tnot\tuse\tzero\tpadding,\tand\tmay\tignore\tsome rows\tand\tcolumns\tat\tthe\tbottom\tand\tright\tof\tthe\tinput\timage,\tdepending\ton\tthe\tstride,\tas\tshown in\tFigure\t13-7\t(for\tsimplicity,\tonly\tthe\thorizontal\tdimension\tis\tshown\there,\tbut\tof\tcourse\tthe same\tlogic\tapplies\tto\tthe\tvertical\tdimension).\n\nIf\tset\tto\t\"SAME\",\tthe\tconvolutional\tlayer\tuses\tzero\tpadding\tif\tnecessary.\tIn\tthis\tcase,\tthe\tnumber of\toutput\tneurons\tis\tequal\tto\tthe\tnumber\tof\tinput\tneurons\tdivided\tby\tthe\tstride,\trounded\tup\t(in this\texample,\tceil\t(13\t/\t5)\t=\t3).\tThen\tzeros\tare\tadded\tas\tevenly\tas\tpossible\taround\tthe\tinputs.\n\nFigure\t13-7.\tPadding\toptions\t—\tinput\twidth:\t13,\tfilter\twidth:\t6,\tstride:\t5\n\nIn\tthis\tsimple\texample,\twe\tmanually\tcreated\tthe\tfilters,\tbut\tin\ta\treal\tCNN\tyou\twould\tlet\tthe\ttraining algorithm\tdiscover\tthe\tbest\tfilters\tautomatically.\tTensorFlow\thas\ta\ttf.layers.conv2d()\tfunction\twhich creates\tthe\tfilters\tvariable\tfor\tyou\t(called\tkernel),\tand\tinitializes\tit\trandomly.\tFor\texample,\tthe following\tcode\tcreates\tan\tinput\tplaceholder\tfollowed\tby\ta\tconvolutional\tlayer\twith\ttwo\t7\t×\t7\tfeature maps,\tusing\t2\t×\t2\tstrides\t(note\tthat\tthis\tfunction\tonly\texpects\tthe\tvertical\tand\thorizontal\tstrides),\tand \"SAME\"\tpadding:\n\nX\t=\ttf.placeholder(shape=(None,\theight,\twidth,\tchannels),\tdtype=tf.float32) conv\t=\ttf.layers.conv2d(X,\tfilters=2,\tkernel_size=7,\tstrides=[2,2], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpadding=\"SAME\")\n\nUnfortunately,\tconvolutional\tlayers\thave\tquite\ta\tfew\thyperparameters:\tyou\tmust\tchoose\tthe\tnumber\tof filters,\ttheir\theight\tand\twidth,\tthe\tstrides,\tand\tthe\tpadding\ttype.\tAs\talways,\tyou\tcan\tuse\tcross-validation to\tfind\tthe\tright\thyperparameter\tvalues,\tbut\tthis\tis\tvery\ttime-consuming.\tWe\twill\tdiscuss\tcommon\tCNN architectures\tlater,\tto\tgive\tyou\tsome\tidea\tof\twhat\thyperparameter\tvalues\twork\tbest\tin\tpractice.\n\nMemory\tRequirements Another\tproblem\twith\tCNNs\tis\tthat\tthe\tconvolutional\tlayers\trequire\ta\thuge\tamount\tof\tRAM,\tespecially during\ttraining,\tbecause\tthe\treverse\tpass\tof\tbackpropagation\trequires\tall\tthe\tintermediate\tvalues computed\tduring\tthe\tforward\tpass.\n\nFor\texample,\tconsider\ta\tconvolutional\tlayer\twith\t5\t×\t5\tfilters,\toutputting\t200\tfeature\tmaps\tof\tsize\t150\t× 100,\twith\tstride\t1\tand\tSAME\tpadding.\tIf\tthe\tinput\tis\ta\t150\t×\t100\tRGB\timage\t(three\tchannels),\tthen\tthe number\tof\tparameters\tis\t(5\t×\t5\t×\t3\t+\t1)\t×\t200\t=\t15,200\t(the\t+1\tcorresponds\tto\tthe\tbias\tterms),\twhich\tis fairly\tsmall\tcompared\tto\ta\tfully\tconnected\tlayer.7\tHowever,\teach\tof\tthe\t200\tfeature\tmaps\tcontains\t150\t× 100\tneurons,\tand\teach\tof\tthese\tneurons\tneeds\tto\tcompute\ta\tweighted\tsum\tof\tits\t5\t×\t5\t×\t3\t=\t75\tinputs: that’s\ta\ttotal\tof\t225\tmillion\tfloat\tmultiplications.\tNot\tas\tbad\tas\ta\tfully\tconnected\tlayer,\tbut\tstill\tquite computationally\tintensive.\tMoreover,\tif\tthe\tfeature\tmaps\tare\trepresented\tusing\t32-bit\tfloats,\tthen\tthe convolutional\tlayer’s\toutput\twill\toccupy\t200\t×\t150\t×\t100\t×\t32\t=\t96\tmillion\tbits\t(about\t11.4\tMB)\tof RAM.8\tAnd\tthat’s\tjust\tfor\tone\tinstance!\tIf\ta\ttraining\tbatch\tcontains\t100\tinstances,\tthen\tthis\tlayer\twill\tuse up\tover\t1\tGB\tof\tRAM!\n\nDuring\tinference\t(i.e.,\twhen\tmaking\ta\tprediction\tfor\ta\tnew\tinstance)\tthe\tRAM\toccupied\tby\tone\tlayer\tcan be\treleased\tas\tsoon\tas\tthe\tnext\tlayer\thas\tbeen\tcomputed,\tso\tyou\tonly\tneed\tas\tmuch\tRAM\tas\trequired\tby two\tconsecutive\tlayers.\tBut\tduring\ttraining\teverything\tcomputed\tduring\tthe\tforward\tpass\tneeds\tto\tbe preserved\tfor\tthe\treverse\tpass,\tso\tthe\tamount\tof\tRAM\tneeded\tis\t(at\tleast)\tthe\ttotal\tamount\tof\tRAM required\tby\tall\tlayers.\n\nTIP\n\nIf\ttraining\tcrashes\tbecause\tof\tan\tout-of-memory\terror,\tyou\tcan\ttry\treducing\tthe\tmini-batch\tsize.\tAlternatively,\tyou\tcan\ttry reducing\tdimensionality\tusing\ta\tstride,\tor\tremoving\ta\tfew\tlayers.\tOr\tyou\tcan\ttry\tusing\t16-bit\tfloats\tinstead\tof\t32-bit\tfloats.\tOr\tyou could\tdistribute\tthe\tCNN\tacross\tmultiple\tdevices.\n\nNow\tlet’s\tlook\tat\tthe\tsecond\tcommon\tbuilding\tblock\tof\tCNNs:\tthe\tpooling\tlayer.\n\nPooling\tLayer Once\tyou\tunderstand\thow\tconvolutional\tlayers\twork,\tthe\tpooling\tlayers\tare\tquite\teasy\tto\tgrasp.\tTheir goal\tis\tto\tsubsample\t(i.e.,\tshrink)\tthe\tinput\timage\tin\torder\tto\treduce\tthe\tcomputational\tload,\tthe\tmemory usage,\tand\tthe\tnumber\tof\tparameters\t(thereby\tlimiting\tthe\trisk\tof\toverfitting).\tReducing\tthe\tinput\timage size\talso\tmakes\tthe\tneural\tnetwork\ttolerate\ta\tlittle\tbit\tof\timage\tshift\t(location\tinvariance).\n\nJust\tlike\tin\tconvolutional\tlayers,\teach\tneuron\tin\ta\tpooling\tlayer\tis\tconnected\tto\tthe\toutputs\tof\ta\tlimited number\tof\tneurons\tin\tthe\tprevious\tlayer,\tlocated\twithin\ta\tsmall\trectangular\treceptive\tfield.\tYou\tmust define\tits\tsize,\tthe\tstride,\tand\tthe\tpadding\ttype,\tjust\tlike\tbefore.\tHowever,\ta\tpooling\tneuron\thas\tno weights;\tall\tit\tdoes\tis\taggregate\tthe\tinputs\tusing\tan\taggregation\tfunction\tsuch\tas\tthe\tmax\tor\tmean. Figure\t13-8\tshows\ta\tmax\tpooling\tlayer,\twhich\tis\tthe\tmost\tcommon\ttype\tof\tpooling\tlayer.\tIn\tthis\texample, we\tuse\ta\t2\t×\t2\tpooling\tkernel,\ta\tstride\tof\t2,\tand\tno\tpadding.\tNote\tthat\tonly\tthe\tmax\tinput\tvalue\tin\teach kernel\tmakes\tit\tto\tthe\tnext\tlayer.\tThe\tother\tinputs\tare\tdropped.\n\nFigure\t13-8.\tMax\tpooling\tlayer\t(2\t×\t2\tpooling\tkernel,\tstride\t2,\tno\tpadding)\n\nThis\tis\tobviously\ta\tvery\tdestructive\tkind\tof\tlayer:\teven\twith\ta\ttiny\t2\t×\t2\tkernel\tand\ta\tstride\tof\t2,\tthe output\twill\tbe\ttwo\ttimes\tsmaller\tin\tboth\tdirections\t(so\tits\tarea\twill\tbe\tfour\ttimes\tsmaller),\tsimply dropping\t75%\tof\tthe\tinput\tvalues.\n\nA\tpooling\tlayer\ttypically\tworks\ton\tevery\tinput\tchannel\tindependently,\tso\tthe\toutput\tdepth\tis\tthe\tsame\tas the\tinput\tdepth.\tYou\tmay\talternatively\tpool\tover\tthe\tdepth\tdimension,\tas\twe\twill\tsee\tnext,\tin\twhich\tcase the\timage’s\tspatial\tdimensions\t(height\tand\twidth)\tremain\tunchanged,\tbut\tthe\tnumber\tof\tchannels\tis reduced.\n\nImplementing\ta\tmax\tpooling\tlayer\tin\tTensorFlow\tis\tquite\teasy.\tThe\tfollowing\tcode\tcreates\ta\tmax\tpooling layer\tusing\ta\t2\t×\t2\tkernel,\tstride\t2,\tand\tno\tpadding,\tthen\tapplies\tit\tto\tall\tthe\timages\tin\tthe\tdataset:\n\n[...]\t#\tload\tthe\timage\tdataset,\tjust\tlike\tabove\n\n#\tCreate\ta\tgraph\twith\tinput\tX\tplus\ta\tmax\tpooling\tlayer X\t=\ttf.placeholder(tf.float32,\tshape=(None,\theight,\twidth,\tchannels)) max_pool\t=\ttf.nn.max_pool(X,\tksize=[1,2,2,1],\tstrides=[1,2,2,1],padding=\"VALID\")\n\nwith\ttf.Session()\tas\tsess: \t\t\t\toutput\t=\tsess.run(max_pool,\tfeed_dict={X:\tdataset})\n\nplt.imshow(output[0].astype(np.uint8))\t\t#\tplot\tthe\toutput\tfor\tthe\t1st\timage plt.show()\n\nThe\tksize\targument\tcontains\tthe\tkernel\tshape\talong\tall\tfour\tdimensions\tof\tthe\tinput\ttensor:\t[batch size,\theight,\twidth,\tchannels].\tTensorFlow\tcurrently\tdoes\tnot\tsupport\tpooling\tover\tmultiple instances,\tso\tthe\tfirst\telement\tof\tksize\tmust\tbe\tequal\tto\t1.\tMoreover,\tit\tdoes\tnot\tsupport\tpooling\tover both\tthe\tspatial\tdimensions\t(height\tand\twidth)\tand\tthe\tdepth\tdimension,\tso\teither\tksize[1]\tand ksize[2]\tmust\tboth\tbe\tequal\tto\t1,\tor\tksize[3]\tmust\tbe\tequal\tto\t1.\n\nTo\tcreate\tan\taverage\tpooling\tlayer,\tjust\tuse\tthe\tavg_pool()\tfunction\tinstead\tof\tmax_pool().\n\nNow\tyou\tknow\tall\tthe\tbuilding\tblocks\tto\tcreate\ta\tconvolutional\tneural\tnetwork.\tLet’s\tsee\thow\tto assemble\tthem.\n\nCNN\tArchitectures Typical\tCNN\tarchitectures\tstack\ta\tfew\tconvolutional\tlayers\t(each\tone\tgenerally\tfollowed\tby\ta\tReLU layer),\tthen\ta\tpooling\tlayer,\tthen\tanother\tfew\tconvolutional\tlayers\t(+ReLU),\tthen\tanother\tpooling\tlayer, and\tso\ton.\tThe\timage\tgets\tsmaller\tand\tsmaller\tas\tit\tprogresses\tthrough\tthe\tnetwork,\tbut\tit\talso\ttypically gets\tdeeper\tand\tdeeper\t(i.e.,\twith\tmore\tfeature\tmaps)\tthanks\tto\tthe\tconvolutional\tlayers\t(see\tFigure\t13- 9).\tAt\tthe\ttop\tof\tthe\tstack,\ta\tregular\tfeedforward\tneural\tnetwork\tis\tadded,\tcomposed\tof\ta\tfew\tfully connected\tlayers\t(+ReLUs),\tand\tthe\tfinal\tlayer\toutputs\tthe\tprediction\t(e.g.,\ta\tsoftmax\tlayer\tthat\toutputs estimated\tclass\tprobabilities).\n\nFigure\t13-9.\tTypical\tCNN\tarchitecture\n\nTIP\n\nA\tcommon\tmistake\tis\tto\tuse\tconvolution\tkernels\tthat\tare\ttoo\tlarge.\tYou\tcan\toften\tget\tthe\tsame\teffect\tas\ta\t9\t×\t9\tkernel\tby stacking\ttwo\t3\t×\t3\tkernels\ton\ttop\tof\teach\tother,\tfor\ta\tlot\tless\tcompute.\n\nOver\tthe\tyears,\tvariants\tof\tthis\tfundamental\tarchitecture\thave\tbeen\tdeveloped,\tleading\tto\tamazing advances\tin\tthe\tfield.\tA\tgood\tmeasure\tof\tthis\tprogress\tis\tthe\terror\trate\tin\tcompetitions\tsuch\tas\tthe ILSVRC\tImageNet\tchallenge.\tIn\tthis\tcompetition\tthe\ttop-5\terror\trate\tfor\timage\tclassification\tfell\tfrom over\t26%\tto\tbarely\tover\t3%\tin\tjust\tfive\tyears.\tThe\ttop-five\terror\trate\tis\tthe\tnumber\tof\ttest\timages\tfor which\tthe\tsystem’s\ttop\t5\tpredictions\tdid\tnot\tinclude\tthe\tcorrect\tanswer.\tThe\timages\tare\tlarge\t(256\tpixels high)\tand\tthere\tare\t1,000\tclasses,\tsome\tof\twhich\tare\treally\tsubtle\t(try\tdistinguishing\t120\tdog\tbreeds). Looking\tat\tthe\tevolution\tof\tthe\twinning\tentries\tis\ta\tgood\tway\tto\tunderstand\thow\tCNNs\twork.\n\nWe\twill\tfirst\tlook\tat\tthe\tclassical\tLeNet-5\tarchitecture\t(1998),\tthen\tthree\tof\tthe\twinners\tof\tthe\tILSVRC challenge:\tAlexNet\t(2012),\tGoogLeNet\t(2014),\tand\tResNet\t(2015).\n\nOTHER\tVISUAL\tTASKS\n\nThere\twas\tstunning\tprogress\tas\twell\tin\tother\tvisual\ttasks\tsuch\tas\tobject\tdetection\tand\tlocalization,\tand\timage\tsegmentation.\tIn\tobject detection\tand\tlocalization,\tthe\tneural\tnetwork\ttypically\toutputs\ta\tsequence\tof\tbounding\tboxes\taround\tvarious\tobjects\tin\tthe\timage.\tFor example,\tsee\tMaxine\tOquab\tet\tal.’s\t2015\tpaper\tthat\toutputs\ta\theat\tmap\tfor\teach\tobject\tclass,\tor\tRussell\tStewart\tet\tal.’s\t2015\tpaper\tthat uses\ta\tcombination\tof\ta\tCNN\tto\tdetect\tfaces\tand\ta\trecurrent\tneural\tnetwork\tto\toutput\ta\tsequence\tof\tbounding\tboxes\taround\tthem.\tIn image\tsegmentation,\tthe\tnet\toutputs\tan\timage\t(usually\tof\tthe\tsame\tsize\tas\tthe\tinput)\twhere\teach\tpixel\tindicates\tthe\tclass\tof\tthe\tobject\tto which\tthe\tcorresponding\tinput\tpixel\tbelongs.\tFor\texample,\tcheck\tout\tEvan\tShelhamer\tet\tal.’s\t2016\tpaper.\n\nLeNet-5 The\tLeNet-5\tarchitecture\tis\tperhaps\tthe\tmost\twidely\tknown\tCNN\tarchitecture.\tAs\tmentioned\tearlier,\tit was\tcreated\tby\tYann\tLeCun\tin\t1998\tand\twidely\tused\tfor\thandwritten\tdigit\trecognition\t(MNIST).\tIt\tis composed\tof\tthe\tlayers\tshown\tin\tTable\t13-1.\n\nTable\t13-1.\tLeNet-5\tarchitecture\n\nLayer Type\n\nMaps Size\n\nKernel\tsize Stride Activation\n\nOut\n\nFully\tConnected –\n\n10\n\n–\n\n–\n\nRBF\n\nF6\n\nFully\tConnected –\n\n84\n\n–\n\n–\n\ntanh\n\nC5\n\nConvolution\n\n120\n\n1\t×\t1\n\n5\t×\t5\n\n1\n\ntanh\n\nS4\n\nAvg\tPooling\n\n16\n\n5\t×\t5\n\n2\t×\t2\n\n2\n\ntanh\n\nC3\n\nConvolution\n\n16\n\n10\t×\t10 5\t×\t5\n\n1\n\ntanh\n\nS2\n\nAvg\tPooling\n\n6\n\n14\t×\t14 2\t×\t2\n\n2\n\ntanh\n\nC1\n\nConvolution\n\n6\n\n28\t×\t28 5\t×\t5\n\n1\n\ntanh\n\nIn\n\nInput\n\n1\n\n32\t×\t32 –\n\n–\n\n–\n\nThere\tare\ta\tfew\textra\tdetails\tto\tbe\tnoted:\n\nMNIST\timages\tare\t28\t×\t28\tpixels,\tbut\tthey\tare\tzero-padded\tto\t32\t×\t32\tpixels\tand\tnormalized\tbefore being\tfed\tto\tthe\tnetwork.\tThe\trest\tof\tthe\tnetwork\tdoes\tnot\tuse\tany\tpadding,\twhich\tis\twhy\tthe\tsize keeps\tshrinking\tas\tthe\timage\tprogresses\tthrough\tthe\tnetwork.\n\nThe\taverage\tpooling\tlayers\tare\tslightly\tmore\tcomplex\tthan\tusual:\teach\tneuron\tcomputes\tthe\tmean\tof its\tinputs,\tthen\tmultiplies\tthe\tresult\tby\ta\tlearnable\tcoefficient\t(one\tper\tmap)\tand\tadds\ta\tlearnable bias\tterm\t(again,\tone\tper\tmap),\tthen\tfinally\tapplies\tthe\tactivation\tfunction.\n\nMost\tneurons\tin\tC3\tmaps\tare\tconnected\tto\tneurons\tin\tonly\tthree\tor\tfour\tS2\tmaps\t(instead\tof\tall\tsix S2\tmaps).\tSee\ttable\t1\tin\tthe\toriginal\tpaper\tfor\tdetails.\n\nThe\toutput\tlayer\tis\ta\tbit\tspecial:\tinstead\tof\tcomputing\tthe\tdot\tproduct\tof\tthe\tinputs\tand\tthe\tweight vector,\teach\tneuron\toutputs\tthe\tsquare\tof\tthe\tEuclidian\tdistance\tbetween\tits\tinput\tvector\tand\tits weight\tvector.\tEach\toutput\tmeasures\thow\tmuch\tthe\timage\tbelongs\tto\ta\tparticular\tdigit\tclass.\tThe cross\tentropy\tcost\tfunction\tis\tnow\tpreferred,\tas\tit\tpenalizes\tbad\tpredictions\tmuch\tmore,\tproducing larger\tgradients\tand\tthus\tconverging\tfaster.\n\nYann\tLeCun’s\twebsite\t(“LENET”\tsection)\tfeatures\tgreat\tdemos\tof\tLeNet-5\tclassifying\tdigits.\n\nAlexNet The\tAlexNet\tCNN\tarchitecture9\twon\tthe\t2012\tImageNet\tILSVRC\tchallenge\tby\ta\tlarge\tmargin:\tit\tachieved 17%\ttop-5\terror\trate\twhile\tthe\tsecond\tbest\tachieved\tonly\t26%!\tIt\twas\tdeveloped\tby\tAlex\tKrizhevsky (hence\tthe\tname),\tIlya\tSutskever,\tand\tGeoffrey\tHinton.\tIt\tis\tquite\tsimilar\tto\tLeNet-5,\tonly\tmuch\tlarger\tand deeper,\tand\tit\twas\tthe\tfirst\tto\tstack\tconvolutional\tlayers\tdirectly\ton\ttop\tof\teach\tother,\tinstead\tof\tstacking\ta pooling\tlayer\ton\ttop\tof\teach\tconvolutional\tlayer.\tTable\t13-2\tpresents\tthis\tarchitecture.\n\nTable\t13-2.\tAlexNet\tarchitecture\n\nLayer Type\n\nMaps\n\nSize\n\nKernel\tsize Stride Padding Activation\n\nOut\n\nFully\tConnected –\n\n1,000\n\n–\n\n–\n\n–\n\nSoftmax\n\nF9\n\nFully\tConnected –\n\n4,096\n\n–\n\n–\n\n–\n\nReLU\n\nF8\n\nFully\tConnected –\n\n4,096\n\n–\n\n–\n\n–\n\nReLU\n\nC7\n\nConvolution\n\n256\n\n13\t×\t13\n\n3\t×\t3\n\n1\n\nSAME ReLU\n\nC6\n\nConvolution\n\n384\n\n13\t×\t13\n\n3\t×\t3\n\n1\n\nSAME ReLU\n\nC5\n\nConvolution\n\n384\n\n13\t×\t13\n\n3\t×\t3\n\n1\n\nSAME ReLU\n\nS4\n\nMax\tPooling\n\n256\n\n13\t×\t13\n\n3\t×\t3\n\n2\n\nVALID –\n\nC3\n\nConvolution\n\n256\n\n27\t×\t27\n\n5\t×\t5\n\n1\n\nSAME ReLU\n\nS2\n\nMax\tPooling\n\n96\n\n27\t×\t27\n\n3\t×\t3\n\n2\n\nVALID –\n\nC1\n\nConvolution\n\n96\n\n55\t×\t55\n\n11\t×\t11\n\n4\n\nSAME ReLU\n\nIn\n\nInput\n\n3\t(RGB) 224\t×\t224 –\n\n–\n\n–\n\n–\n\nTo\treduce\toverfitting,\tthe\tauthors\tused\ttwo\tregularization\ttechniques\twe\tdiscussed\tin\tprevious\tchapters: first\tthey\tapplied\tdropout\t(with\ta\t50%\tdropout\trate)\tduring\ttraining\tto\tthe\toutputs\tof\tlayers\tF8\tand\tF9. Second,\tthey\tperformed\tdata\taugmentation\tby\trandomly\tshifting\tthe\ttraining\timages\tby\tvarious\toffsets, flipping\tthem\thorizontally,\tand\tchanging\tthe\tlighting\tconditions.\n\nAlexNet\talso\tuses\ta\tcompetitive\tnormalization\tstep\timmediately\tafter\tthe\tReLU\tstep\tof\tlayers\tC1\tand\tC3, called\tlocal\tresponse\tnormalization.\tThis\tform\tof\tnormalization\tmakes\tthe\tneurons\tthat\tmost\tstrongly activate\tinhibit\tneurons\tat\tthe\tsame\tlocation\tbut\tin\tneighboring\tfeature\tmaps\t(such\tcompetitive\tactivation has\tbeen\tobserved\tin\tbiological\tneurons).\tThis\tencourages\tdifferent\tfeature\tmaps\tto\tspecialize,\tpushing them\tapart\tand\tforcing\tthem\tto\texplore\ta\twider\trange\tof\tfeatures,\tultimately\timproving\tgeneralization. Equation\t13-2\tshows\thow\tto\tapply\tLRN.\n\nEquation\t13-2.\tLocal\tresponse\tnormalization\n\nbi\tis\tthe\tnormalized\toutput\tof\tthe\tneuron\tlocated\tin\tfeature\tmap\ti,\tat\tsome\trow\tu\tand\tcolumn\tv\t(note that\tin\tthis\tequation\twe\tconsider\tonly\tneurons\tlocated\tat\tthis\trow\tand\tcolumn,\tso\tu\tand\tv\tare\tnot shown).\n\nai\tis\tthe\tactivation\tof\tthat\tneuron\tafter\tthe\tReLU\tstep,\tbut\tbefore\tnormalization.\n\nk,\tα,\tβ,\tand\tr\tare\thyperparameters.\tk\tis\tcalled\tthe\tbias,\tand\tr\tis\tcalled\tthe\tdepth\tradius.\n\nfn\tis\tthe\tnumber\tof\tfeature\tmaps.\n\nFor\texample,\tif\tr\t=\t2\tand\ta\tneuron\thas\ta\tstrong\tactivation,\tit\twill\tinhibit\tthe\tactivation\tof\tthe\tneurons located\tin\tthe\tfeature\tmaps\timmediately\tabove\tand\tbelow\tits\town.\n\nIn\tAlexNet,\tthe\thyperparameters\tare\tset\tas\tfollows:\tr\t=\t2,\tα\t=\t0.00002,\tβ\t=\t0.75,\tand\tk\t=\t1.\tThis\tstep\tcan be\timplemented\tusing\tTensorFlow’s\ttf.nn.local_response_normalization()\toperation.\n\nA\tvariant\tof\tAlexNet\tcalled\tZF\tNet\twas\tdeveloped\tby\tMatthew\tZeiler\tand\tRob\tFergus\tand\twon\tthe\t2013 ILSVRC\tchallenge.\tIt\tis\tessentially\tAlexNet\twith\ta\tfew\ttweaked\thyperparameters\t(number\tof\tfeature maps,\tkernel\tsize,\tstride,\tetc.).\n\nGoogLeNet The\tGoogLeNet\tarchitecture\twas\tdeveloped\tby\tChristian\tSzegedy\tet\tal.\tfrom\tGoogle\tResearch,10\tand\tit won\tthe\tILSVRC\t2014\tchallenge\tby\tpushing\tthe\ttop-5\terror\trate\tbelow\t7%.\tThis\tgreat\tperformance\tcame in\tlarge\tpart\tfrom\tthe\tfact\tthat\tthe\tnetwork\twas\tmuch\tdeeper\tthan\tprevious\tCNNs\t(see\tFigure\t13-11).\tThis was\tmade\tpossible\tby\tsub-networks\tcalled\tinception\tmodules,11\twhich\tallow\tGoogLeNet\tto\tuse parameters\tmuch\tmore\tefficiently\tthan\tprevious\tarchitectures:\tGoogLeNet\tactually\thas\t10\ttimes\tfewer parameters\tthan\tAlexNet\t(roughly\t6\tmillion\tinstead\tof\t60\tmillion).\n\nFigure\t13-10\tshows\tthe\tarchitecture\tof\tan\tinception\tmodule.\tThe\tnotation\t“3\t×\t3\t+\t2(S)”\tmeans\tthat\tthe layer\tuses\ta\t3\t×\t3\tkernel,\tstride\t2,\tand\tSAME\tpadding.\tThe\tinput\tsignal\tis\tfirst\tcopied\tand\tfed\tto\tfour different\tlayers.\tAll\tconvolutional\tlayers\tuse\tthe\tReLU\tactivation\tfunction.\tNote\tthat\tthe\tsecond\tset\tof convolutional\tlayers\tuses\tdifferent\tkernel\tsizes\t(1\t×\t1,\t3\t×\t3,\tand\t5\t×\t5),\tallowing\tthem\tto\tcapture patterns\tat\tdifferent\tscales.\tAlso\tnote\tthat\tevery\tsingle\tlayer\tuses\ta\tstride\tof\t1\tand\tSAME\tpadding\t(even the\tmax\tpooling\tlayer),\tso\ttheir\toutputs\tall\thave\tthe\tsame\theight\tand\twidth\tas\ttheir\tinputs.\tThis\tmakes\tit possible\tto\tconcatenate\tall\tthe\toutputs\talong\tthe\tdepth\tdimension\tin\tthe\tfinal\tdepth\tconcat\tlayer\t(i.e., stack\tthe\tfeature\tmaps\tfrom\tall\tfour\ttop\tconvolutional\tlayers).\tThis\tconcatenation\tlayer\tcan\tbe implemented\tin\tTensorFlow\tusing\tthe\ttf.concat()\toperation,\twith\taxis=3\t(axis\t3\tis\tthe\tdepth).\n\nFigure\t13-10.\tInception\tmodule\n\nYou\tmay\twonder\twhy\tinception\tmodules\thave\tconvolutional\tlayers\twith\t1\t×\t1\tkernels.\tSurely\tthese\tlayers cannot\tcapture\tany\tfeatures\tsince\tthey\tlook\tat\tonly\tone\tpixel\tat\ta\ttime?\tIn\tfact,\tthese\tlayers\tserve\ttwo purposes:\n\nFirst,\tthey\tare\tconfigured\tto\toutput\tmany\tfewer\tfeature\tmaps\tthan\ttheir\tinputs,\tso\tthey\tserve\tas bottleneck\tlayers,\tmeaning\tthey\treduce\tdimensionality.\tThis\tis\tparticularly\tuseful\tbefore\tthe\t3\t×\t3 and\t5\t×\t5\tconvolutions,\tsince\tthese\tare\tvery\tcomputationally\texpensive\tlayers.\n\nSecond,\teach\tpair\tof\tconvolutional\tlayers\t([1\t×\t1,\t3\t×\t3]\tand\t[1\t×\t1,\t5\t×\t5])\tacts\tlike\ta\tsingle,\n\npowerful\tconvolutional\tlayer,\tcapable\tof\tcapturing\tmore\tcomplex\tpatterns.\tIndeed,\tinstead\tof sweeping\ta\tsimple\tlinear\tclassifier\tacross\tthe\timage\t(as\ta\tsingle\tconvolutional\tlayer\tdoes),\tthis\tpair of\tconvolutional\tlayers\tsweeps\ta\ttwo-layer\tneural\tnetwork\tacross\tthe\timage.\n\nIn\tshort,\tyou\tcan\tthink\tof\tthe\twhole\tinception\tmodule\tas\ta\tconvolutional\tlayer\ton\tsteroids,\table\tto\toutput feature\tmaps\tthat\tcapture\tcomplex\tpatterns\tat\tvarious\tscales.\n\nWARNING\n\nThe\tnumber\tof\tconvolutional\tkernels\tfor\teach\tconvolutional\tlayer\tis\ta\thyperparameter.\tUnfortunately,\tthis\tmeans\tthat\tyou\thave six\tmore\thyperparameters\tto\ttweak\tfor\tevery\tinception\tlayer\tyou\tadd.\n\nNow\tlet’s\tlook\tat\tthe\tarchitecture\tof\tthe\tGoogLeNet\tCNN\t(see\tFigure\t13-11).\tIt\tis\tso\tdeep\tthat\twe\thad\tto represent\tit\tin\tthree\tcolumns,\tbut\tGoogLeNet\tis\tactually\tone\ttall\tstack,\tincluding\tnine\tinception\tmodules (the\tboxes\twith\tthe\tspinning\ttops)\tthat\tactually\tcontain\tthree\tlayers\teach.\tThe\tnumber\tof\tfeature\tmaps output\tby\teach\tconvolutional\tlayer\tand\teach\tpooling\tlayer\tis\tshown\tbefore\tthe\tkernel\tsize.\tThe\tsix numbers\tin\tthe\tinception\tmodules\trepresent\tthe\tnumber\tof\tfeature\tmaps\toutput\tby\teach\tconvolutional\tlayer in\tthe\tmodule\t(in\tthe\tsame\torder\tas\tin\tFigure\t13-10).\tNote\tthat\tall\tthe\tconvolutional\tlayers\tuse\tthe\tReLU activation\tfunction.\n\nFigure\t13-11.\tGoogLeNet\tarchitecture\n\nLet’s\tgo\tthrough\tthis\tnetwork:\n\nThe\tfirst\ttwo\tlayers\tdivide\tthe\timage’s\theight\tand\twidth\tby\t4\t(so\tits\tarea\tis\tdivided\tby\t16),\tto\treduce the\tcomputational\tload.\n\nThen\tthe\tlocal\tresponse\tnormalization\tlayer\tensures\tthat\tthe\tprevious\tlayers\tlearn\ta\twide\tvariety\tof features\t(as\tdiscussed\tearlier).\n\nTwo\tconvolutional\tlayers\tfollow,\twhere\tthe\tfirst\tacts\tlike\ta\tbottleneck\tlayer.\tAs\texplained\tearlier, you\tcan\tthink\tof\tthis\tpair\tas\ta\tsingle\tsmarter\tconvolutional\tlayer.\n\nAgain,\ta\tlocal\tresponse\tnormalization\tlayer\tensures\tthat\tthe\tprevious\tlayers\tcapture\ta\twide\tvariety of\tpatterns.\n\nNext\ta\tmax\tpooling\tlayer\treduces\tthe\timage\theight\tand\twidth\tby\t2,\tagain\tto\tspeed\tup\tcomputations.\n\nThen\tcomes\tthe\ttall\tstack\tof\tnine\tinception\tmodules,\tinterleaved\twith\ta\tcouple\tmax\tpooling\tlayers\tto reduce\tdimensionality\tand\tspeed\tup\tthe\tnet.\n\nNext,\tthe\taverage\tpooling\tlayer\tuses\ta\tkernel\tthe\tsize\tof\tthe\tfeature\tmaps\twith\tVALID\tpadding, outputting\t1\t×\t1\tfeature\tmaps:\tthis\tsurprising\tstrategy\tis\tcalled\tglobal\taverage\tpooling.\tIt\teffectively forces\tthe\tprevious\tlayers\tto\tproduce\tfeature\tmaps\tthat\tare\tactually\tconfidence\tmaps\tfor\teach\ttarget class\t(since\tother\tkinds\tof\tfeatures\twould\tbe\tdestroyed\tby\tthe\taveraging\tstep).\tThis\tmakes\tit unnecessary\tto\thave\tseveral\tfully\tconnected\tlayers\tat\tthe\ttop\tof\tthe\tCNN\t(like\tin\tAlexNet), considerably\treducing\tthe\tnumber\tof\tparameters\tin\tthe\tnetwork\tand\tlimiting\tthe\trisk\tof\toverfitting.\n\nThe\tlast\tlayers\tare\tself-explanatory:\tdropout\tfor\tregularization,\tthen\ta\tfully\tconnected\tlayer\twith\ta softmax\tactivation\tfunction\tto\toutput\testimated\tclass\tprobabilities.\n\nThis\tdiagram\tis\tslightly\tsimplified:\tthe\toriginal\tGoogLeNet\tarchitecture\talso\tincluded\ttwo\tauxiliary classifiers\tplugged\ton\ttop\tof\tthe\tthird\tand\tsixth\tinception\tmodules.\tThey\twere\tboth\tcomposed\tof\tone average\tpooling\tlayer,\tone\tconvolutional\tlayer,\ttwo\tfully\tconnected\tlayers,\tand\ta\tsoftmax\tactivation\tlayer. During\ttraining,\ttheir\tloss\t(scaled\tdown\tby\t70%)\twas\tadded\tto\tthe\toverall\tloss.\tThe\tgoal\twas\tto\tfight\tthe vanishing\tgradients\tproblem\tand\tregularize\tthe\tnetwork.\tHowever,\tit\twas\tshown\tthat\ttheir\teffect\twas relatively\tminor.\n\nResNet Last\tbut\tnot\tleast,\tthe\twinner\tof\tthe\tILSVRC\t2015\tchallenge\twas\tthe\tResidual\tNetwork\t(or\tResNet), developed\tby\tKaiming\tHe\tet\tal.,12\twhich\tdelivered\tan\tastounding\ttop-5\terror\trate\tunder\t3.6%,\tusing\tan extremely\tdeep\tCNN\tcomposed\tof\t152\tlayers.\tThe\tkey\tto\tbeing\table\tto\ttrain\tsuch\ta\tdeep\tnetwork\tis\tto\tuse skip\tconnections\t(also\tcalled\tshortcut\tconnections):\tthe\tsignal\tfeeding\tinto\ta\tlayer\tis\talso\tadded\tto\tthe output\tof\ta\tlayer\tlocated\ta\tbit\thigher\tup\tthe\tstack.\tLet’s\tsee\twhy\tthis\tis\tuseful.\n\nWhen\ttraining\ta\tneural\tnetwork,\tthe\tgoal\tis\tto\tmake\tit\tmodel\ta\ttarget\tfunction\th(x).\tIf\tyou\tadd\tthe\tinput\tx to\tthe\toutput\tof\tthe\tnetwork\t(i.e.,\tyou\tadd\ta\tskip\tconnection),\tthen\tthe\tnetwork\twill\tbe\tforced\tto\tmodel f(x)\t=\th(x)\t–\tx\trather\tthan\th(x).\tThis\tis\tcalled\tresidual\tlearning\t(see\tFigure\t13-12).\n\nFigure\t13-12.\tResidual\tlearning\n\nWhen\tyou\tinitialize\ta\tregular\tneural\tnetwork,\tits\tweights\tare\tclose\tto\tzero,\tso\tthe\tnetwork\tjust\toutputs values\tclose\tto\tzero.\tIf\tyou\tadd\ta\tskip\tconnection,\tthe\tresulting\tnetwork\tjust\toutputs\ta\tcopy\tof\tits\tinputs;\tin other\twords,\tit\tinitially\tmodels\tthe\tidentity\tfunction.\tIf\tthe\ttarget\tfunction\tis\tfairly\tclose\tto\tthe\tidentity function\t(which\tis\toften\tthe\tcase),\tthis\twill\tspeed\tup\ttraining\tconsiderably.\n\nMoreover,\tif\tyou\tadd\tmany\tskip\tconnections,\tthe\tnetwork\tcan\tstart\tmaking\tprogress\teven\tif\tseveral\tlayers have\tnot\tstarted\tlearning\tyet\t(see\tFigure\t13-13).\tThanks\tto\tskip\tconnections,\tthe\tsignal\tcan\teasily\tmake\tits way\tacross\tthe\twhole\tnetwork.\tThe\tdeep\tresidual\tnetwork\tcan\tbe\tseen\tas\ta\tstack\tof\tresidual\tunits,\twhere each\tresidual\tunit\tis\ta\tsmall\tneural\tnetwork\twith\ta\tskip\tconnection.\n\nFigure\t13-13.\tRegular\tdeep\tneural\tnetwork\t(left)\tand\tdeep\tresidual\tnetwork\t(right)\n\nNow\tlet’s\tlook\tat\tResNet’s\tarchitecture\t(see\tFigure\t13-14).\tIt\tis\tactually\tsurprisingly\tsimple.\tIt\tstarts\tand ends\texactly\tlike\tGoogLeNet\t(except\twithout\ta\tdropout\tlayer),\tand\tin\tbetween\tis\tjust\ta\tvery\tdeep\tstack\tof simple\tresidual\tunits.\tEach\tresidual\tunit\tis\tcomposed\tof\ttwo\tconvolutional\tlayers,\twith\tBatch Normalization\t(BN)\tand\tReLU\tactivation,\tusing\t3\t×\t3\tkernels\tand\tpreserving\tspatial\tdimensions\t(stride\t1, SAME\tpadding).\n\nFigure\t13-14.\tResNet\tarchitecture\n\nNote\tthat\tthe\tnumber\tof\tfeature\tmaps\tis\tdoubled\tevery\tfew\tresidual\tunits,\tat\tthe\tsame\ttime\tas\ttheir\theight and\twidth\tare\thalved\t(using\ta\tconvolutional\tlayer\twith\tstride\t2).\tWhen\tthis\thappens\tthe\tinputs\tcannot\tbe added\tdirectly\tto\tthe\toutputs\tof\tthe\tresidual\tunit\tsince\tthey\tdon’t\thave\tthe\tsame\tshape\t(for\texample,\tthis\n\nproblem\taffects\tthe\tskip\tconnection\trepresented\tby\tthe\tdashed\tarrow\tin\tFigure\t13-14).\tTo\tsolve\tthis problem,\tthe\tinputs\tare\tpassed\tthrough\ta\t1\t×\t1\tconvolutional\tlayer\twith\tstride\t2\tand\tthe\tright\tnumber\tof output\tfeature\tmaps\t(see\tFigure\t13-15).\n\nFigure\t13-15.\tSkip\tconnection\twhen\tchanging\tfeature\tmap\tsize\tand\tdepth\n\nResNet-34\tis\tthe\tResNet\twith\t34\tlayers\t(only\tcounting\tthe\tconvolutional\tlayers\tand\tthe\tfully\tconnected layer)\tcontaining\tthree\tresidual\tunits\tthat\toutput\t64\tfeature\tmaps,\t4\tRUs\twith\t128\tmaps,\t6\tRUs\twith\t256 maps,\tand\t3\tRUs\twith\t512\tmaps.\n\nResNets\tdeeper\tthan\tthat,\tsuch\tas\tResNet-152,\tuse\tslightly\tdifferent\tresidual\tunits.\tInstead\tof\ttwo\t3\t×\t3 convolutional\tlayers\twith\t(say)\t256\tfeature\tmaps,\tthey\tuse\tthree\tconvolutional\tlayers:\tfirst\ta\t1\t×\t1 convolutional\tlayer\twith\tjust\t64\tfeature\tmaps\t(4\ttimes\tless),\twhich\tacts\ta\ta\tbottleneck\tlayer\t(as\tdiscussed already),\tthen\ta\t3\t×\t3\tlayer\twith\t64\tfeature\tmaps,\tand\tfinally\tanother\t1\t×\t1\tconvolutional\tlayer\twith\t256 feature\tmaps\t(4\ttimes\t64)\tthat\trestores\tthe\toriginal\tdepth.\tResNet-152\tcontains\tthree\tsuch\tRUs\tthat\toutput 256\tmaps,\tthen\t8\tRUs\twith\t512\tmaps,\ta\twhopping\t36\tRUs\twith\t1,024\tmaps,\tand\tfinally\t3\tRUs\twith\t2,048 maps.\n\nAs\tyou\tcan\tsee,\tthe\tfield\tis\tmoving\trapidly,\twith\tall\tsorts\tof\tarchitectures\tpopping\tout\tevery\tyear.\tOne clear\ttrend\tis\tthat\tCNNs\tkeep\tgetting\tdeeper\tand\tdeeper.\tThey\tare\talso\tgetting\tlighter,\trequiring\tfewer\tand fewer\tparameters.\tAt\tpresent,\tthe\tResNet\tarchitecture\tis\tboth\tthe\tmost\tpowerful\tand\targuably\tthe simplest,\tso\tit\tis\treally\tthe\tone\tyou\tshould\tprobably\tuse\tfor\tnow,\tbut\tkeep\tlooking\tat\tthe\tILSVRC challenge\tevery\tyear.\tThe\t2016\twinners\twere\tthe\tTrimps-Soushen\tteam\tfrom\tChina\twith\tan\tastounding 2.99%\terror\trate.\tTo\tachieve\tthis\tthey\ttrained\tcombinations\tof\tthe\tprevious\tmodels\tand\tjoined\tthem\tinto an\tensemble.\tDepending\ton\tthe\ttask,\tthe\treduced\terror\trate\tmay\tor\tmay\tnot\tbe\tworth\tthe\textra\tcomplexity. There\tare\ta\tfew\tother\tarchitectures\tthat\tyou\tmay\twant\tto\tlook\tat,\tin\tparticular\tVGGNet13\t(runner-up\tof\tthe ILSVRC\t2014\tchallenge)\tand\tInception-v414\t(which\tmerges\tthe\tideas\tof\tGoogLeNet\tand\tResNet\tand achieves\tclose\tto\t3%\ttop-5\terror\trate\ton\tImageNet\tclassification).\n\nNOTE\n\nThere\tis\treally\tnothing\tspecial\tabout\timplementing\tthe\tvarious\tCNN\tarchitectures\twe\tjust\tdiscussed.\tWe\tsaw\tearlier\thow\tto\tbuild all\tthe\tindividual\tbuilding\tblocks,\tso\tnow\tall\tyou\tneed\tis\tto\tassemble\tthem\tto\tcreate\tthe\tdesired\tarchitecture.\tWe\twill\tbuild\ta complete\tCNN\tin\tthe\tupcoming\texercises\tand\tyou\twill\tfind\tfull\tworking\tcode\tin\tthe\tJupyter\tnotebooks.\n\nTENSORFLOW\tCONVOLUTION\tOPERATIONS\n\nTensorFlow\talso\toffers\ta\tfew\tother\tkinds\tof\tconvolutional\tlayers:\n\ntf.layers.conv1d()\tcreates\ta\tconvolutional\tlayer\tfor\t1D\tinputs.\tThis\tis\tuseful,\tfor\texample,\tin\tnatural\tlanguage\tprocessing, where\ta\tsentence\tmay\tbe\trepresented\tas\ta\t1D\tarray\tof\twords,\tand\tthe\treceptive\tfield\tcovers\ta\tfew\tneighboring\twords.\n\ntf.layers.conv3d()\tcreates\ta\tconvolutional\tlayer\tfor\t3D\tinputs,\tsuch\tas\t3D\tPET\tscan.\n\ntf.nn.atrous_conv2d()\tcreates\tan\tatrous\tconvolutional\tlayer\t(“à\ttrous”\tis\tFrench\tfor\t“with\tholes”).\tThis\tis\tequivalent\tto\tusing a\tregular\tconvolutional\tlayer\twith\ta\tfilter\tdilated\tby\tinserting\trows\tand\tcolumns\tof\tzeros\t(i.e.,\tholes).\tFor\texample,\ta\t1\t×\t3\tfilter equal\tto\t[[1,2,3]]\tmay\tbe\tdilated\twith\ta\tdilation\trate\tof\t4,\tresulting\tin\ta\tdilated\tfilter\t[[1,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t3]].\tThis allows\tthe\tconvolutional\tlayer\tto\thave\ta\tlarger\treceptive\tfield\tat\tno\tcomputational\tprice\tand\tusing\tno\textra\tparameters.\n\ntf.layers.conv2d_transpose()\tcreates\ta\ttranspose\tconvolutional\tlayer,\tsometimes\tcalled\ta\tdeconvolutional\tlayer,15\twhich upsamples\tan\timage.\tIt\tdoes\tso\tby\tinserting\tzeros\tbetween\tthe\tinputs,\tso\tyou\tcan\tthink\tof\tthis\tas\ta\tregular\tconvolutional\tlayer using\ta\tfractional\tstride.\tUpsampling\tis\tuseful,\tfor\texample,\tin\timage\tsegmentation:\tin\ta\ttypical\tCNN,\tfeature\tmaps\tget\tsmaller\tand smaller\tas\tyou\tprogress\tthrough\tthe\tnetwork,\tso\tif\tyou\twant\tto\toutput\tan\timage\tof\tthe\tsame\tsize\tas\tthe\tinput,\tyou\tneed\tan upsampling\tlayer.\n\ntf.nn.depthwise_conv2d()\tcreates\ta\tdepthwise\tconvolutional\tlayer\tthat\tapplies\tevery\tfilter\tto\tevery\tindividual\tinput\tchannel independently.\tThus,\tif\tthere\tare\tfn\tfilters\tand\tfn′\tinput\tchannels,\tthen\tthis\twill\toutput\tfn\t×\tfn′\tfeature\tmaps.\n\ntf.layers.separable_conv2d()\tcreates\ta\tseparable\tconvolutional\tlayer\tthat\tfirst\tacts\tlike\ta\tdepthwise\tconvolutional\tlayer, then\tapplies\ta\t1\t×\t1\tconvolutional\tlayer\tto\tthe\tresulting\tfeature\tmaps.\tThis\tmakes\tit\tpossible\tto\tapply\tfilters\tto\tarbitrary\tsets\tof inputs\tchannels.\n\nExercises\n\n1.\t What\tare\tthe\tadvantages\tof\ta\tCNN\tover\ta\tfully\tconnected\tDNN\tfor\timage\tclassification?\n\n2.\t Consider\ta\tCNN\tcomposed\tof\tthree\tconvolutional\tlayers,\teach\twith\t3\t×\t3\tkernels,\ta\tstride\tof\t2,\tand SAME\tpadding.\tThe\tlowest\tlayer\toutputs\t100\tfeature\tmaps,\tthe\tmiddle\tone\toutputs\t200,\tand\tthe\ttop one\toutputs\t400.\tThe\tinput\timages\tare\tRGB\timages\tof\t200\t×\t300\tpixels.\tWhat\tis\tthe\ttotal\tnumber\tof parameters\tin\tthe\tCNN?\tIf\twe\tare\tusing\t32-bit\tfloats,\tat\tleast\thow\tmuch\tRAM\twill\tthis\tnetwork require\twhen\tmaking\ta\tprediction\tfor\ta\tsingle\tinstance?\tWhat\tabout\twhen\ttraining\ton\ta\tmini-batch\tof 50\timages?\n\n3.\t If\tyour\tGPU\truns\tout\tof\tmemory\twhile\ttraining\ta\tCNN,\twhat\tare\tfive\tthings\tyou\tcould\ttry\tto\tsolve\tthe problem?\n\n4.\t Why\twould\tyou\twant\tto\tadd\ta\tmax\tpooling\tlayer\trather\tthan\ta\tconvolutional\tlayer\twith\tthe\tsame stride?\n\n5.\t When\twould\tyou\twant\tto\tadd\ta\tlocal\tresponse\tnormalization\tlayer?\n\n6.\t Can\tyou\tname\tthe\tmain\tinnovations\tin\tAlexNet,\tcompared\tto\tLeNet-5?\tWhat\tabout\tthe\tmain innovations\tin\tGoogLeNet\tand\tResNet?\n\n7.\t Build\tyour\town\tCNN\tand\ttry\tto\tachieve\tthe\thighest\tpossible\taccuracy\ton\tMNIST.\n\n8.\t Classifying\tlarge\timages\tusing\tInception\tv3.\n\na.\t Download\tsome\timages\tof\tvarious\tanimals.\tLoad\tthem\tin\tPython,\tfor\texample\tusing\tthe\n\nmatplotlib.image.mpimg.imread()\tfunction\tor\tthe\tscipy.misc.imread()\tfunction.\tResize and/or\tcrop\tthem\tto\t299\t×\t299\tpixels,\tand\tensure\tthat\tthey\thave\tjust\tthree\tchannels\t(RGB),\twith no\ttransparency\tchannel.\n\nb.\t Download\tthe\tlatest\tpretrained\tInception\tv3\tmodel:\tthe\tcheckpoint\tis\tavailable\tat\n\nhttps://goo.gl/nxSQvl.\n\nc.\t Create\tthe\tInception\tv3\tmodel\tby\tcalling\tthe\tinception_v3()\tfunction,\tas\tshown\tbelow.\tThis must\tbe\tdone\twithin\tan\targument\tscope\tcreated\tby\tthe\tinception_v3_arg_scope()\tfunction. Also,\tyou\tmust\tset\tis_training=False\tand\tnum_classes=1001\tlike\tso:\n\nfrom\ttensorflow.contrib.slim.nets\timport\tinception import\ttensorflow.contrib.slim\tas\tslim\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\t299,\t299,\t3],\tname=\"X\") with\tslim.arg_scope(inception.inception_v3_arg_scope()): \t\t\t\tlogits,\tend_points\t=\tinception.inception_v3( \t\t\t\t\t\t\t\tX,\tnum_classes=1001,\tis_training=False) predictions\t=\tend_points[\"Predictions\"] saver\t=\ttf.train.Saver()\n\nd.\t Open\ta\tsession\tand\tuse\tthe\tSaver\tto\trestore\tthe\tpretrained\tmodel\tcheckpoint\tyou\tdownloaded\n\nearlier.\n\ne.\t Run\tthe\tmodel\tto\tclassify\tthe\timages\tyou\tprepared.\tDisplay\tthe\ttop\tfive\tpredictions\tfor\teach\n\nimage,\talong\twith\tthe\testimated\tprobability\t(the\tlist\tof\tclass\tnames\tis\tavailable\tat https://goo.gl/brXRtZ).\tHow\taccurate\tis\tthe\tmodel?\n\n9.\t Transfer\tlearning\tfor\tlarge\timage\tclassification.\n\na.\t Create\ta\ttraining\tset\tcontaining\tat\tleast\t100\timages\tper\tclass.\tFor\texample,\tyou\tcould\tclassify your\town\tpictures\tbased\ton\tthe\tlocation\t(beach,\tmountain,\tcity,\tetc.),\tor\talternatively\tyou\tcan just\tuse\tan\texisting\tdataset,\tsuch\tas\tthe\tflowers\tdataset\tor\tMIT’s\tplaces\tdataset\t(requires registration,\tand\tit\tis\thuge).\n\nb.\t Write\ta\tpreprocessing\tstep\tthat\twill\tresize\tand\tcrop\tthe\timage\tto\t299\t×\t299,\twith\tsome\n\nrandomness\tfor\tdata\taugmentation.\n\nc.\t Using\tthe\tpretrained\tInception\tv3\tmodel\tfrom\tthe\tprevious\texercise,\tfreeze\tall\tlayers\tup\tto\tthe bottleneck\tlayer\t(i.e.,\tthe\tlast\tlayer\tbefore\tthe\toutput\tlayer),\tand\treplace\tthe\toutput\tlayer\twith the\tappropriate\tnumber\tof\toutputs\tfor\tyour\tnew\tclassification\ttask\t(e.g.,\tthe\tflowers\tdataset\thas five\tmutually\texclusive\tclasses\tso\tthe\toutput\tlayer\tmust\thave\tfive\tneurons\tand\tuse\tthe\tsoftmax activation\tfunction).\n\nd.\t Split\tyour\tdataset\tinto\ta\ttraining\tset\tand\ta\ttest\tset.\tTrain\tthe\tmodel\ton\tthe\ttraining\tset\tand\n\nevaluate\tit\ton\tthe\ttest\tset.\n\n10.\t Go\tthrough\tTensorFlow’s\tDeepDream\ttutorial.\tIt\tis\ta\tfun\tway\tto\tfamiliarize\tyourself\twith\tvarious ways\tof\tvisualizing\tthe\tpatterns\tlearned\tby\ta\tCNN,\tand\tto\tgenerate\tart\tusing\tDeep\tLearning.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“Single\tUnit\tActivity\tin\tStriate\tCortex\tof\tUnrestrained\tCats,”\tD.\tHubel\tand\tT.\tWiesel\t(1958).\n\n2\n\n“Receptive\tFields\tof\tSingle\tNeurones\tin\tthe\tCat’s\tStriate\tCortex,”\tD.\tHubel\tand\tT.\tWiesel\t(1959).\n\n3\n\n“Receptive\tFields\tand\tFunctional\tArchitecture\tof\tMonkey\tStriate\tCortex,”\tD.\tHubel\tand\tT.\tWiesel\t(1968).\n\n4\n\n“Neocognitron:\tA\tSelf-organizing\tNeural\tNetwork\tModel\tfor\ta\tMechanism\tof\tPattern\tRecognition\tUnaffected\tby\tShift\tin\tPosition,”\tK. Fukushima\t(1980).\n\n5\n\n“Gradient-Based\tLearning\tApplied\tto\tDocument\tRecognition,”\tY.\tLeCun\tet\tal.\t(1998).\n\n6\n\nA\tconvolution\tis\ta\tmathematical\toperation\tthat\tslides\tone\tfunction\tover\tanother\tand\tmeasures\tthe\tintegral\tof\ttheir\tpointwise\tmultiplication. It\thas\tdeep\tconnections\twith\tthe\tFourier\ttransform\tand\tthe\tLaplace\ttransform,\tand\tis\theavily\tused\tin\tsignal\tprocessing.\tConvolutional layers\tactually\tuse\tcross-correlations,\twhich\tare\tvery\tsimilar\tto\tconvolutions\t(see\thttp://goo.gl/HAfxXd\tfor\tmore\tdetails).\n\n7\n\nA\tfully\tconnected\tlayer\twith\t150\t×\t100\tneurons,\teach\tconnected\tto\tall\t150\t×\t100\t×\t3\tinputs,\twould\thave\t150\n\n2\n\n×\t100\n\n2\n\n×\t3\t=\t675\tmillion\tparameters!\n\n8\n\n1\tMB\t=\t1,024\tkB\t=\t1,024\t×\t1,024\tbytes\t=\t1,024\t×\t1,024\t×\t8\tbits.\n\n9\n\n“ImageNet\tClassification\twith\tDeep\tConvolutional\tNeural\tNetworks,”\tA.\tKrizhevsky\tet\tal.\t(2012).\n\n10\n\n“Going\tDeeper\twith\tConvolutions,”\tC.\tSzegedy\tet\tal.\t(2015).\n\n11\n\nIn\tthe\t2010\tmovie\tInception,\tthe\tcharacters\tkeep\tgoing\tdeeper\tand\tdeeper\tinto\tmultiple\tlayers\tof\tdreams,\thence\tthe\tname\tof\tthese modules.\n\n12\n\n12\n\n13\n\n14\n\n15\n\n“Deep\tResidual\tLearning\tfor\tImage\tRecognition,”\tK.\tHe\t(2015).\n\n“Very\tDeep\tConvolutional\tNetworks\tfor\tLarge-Scale\tImage\tRecognition,”\tK.\tSimonyan\tand\tA.\tZisserman\t(2015).\n\n“Inception-v4,\tInception-ResNet\tand\tthe\tImpact\tof\tResidual\tConnections\ton\tLearning,”\tC.\tSzegedy\tet\tal.\t(2016).\n\nThis\tname\tis\tquite\tmisleading\tsince\tthis\tlayer\tdoes\tnot\tperform\ta\tdeconvolution,\twhich\tis\ta\twell-defined\tmathematical\toperation\t(the inverse\tof\ta\tconvolution).\n\nChapter\t14.\tRecurrent\tNeural\tNetworks\n\nThe\tbatter\thits\tthe\tball.\tYou\timmediately\tstart\trunning,\tanticipating\tthe\tball’s\ttrajectory.\tYou\ttrack\tit\tand adapt\tyour\tmovements,\tand\tfinally\tcatch\tit\t(under\ta\tthunder\tof\tapplause).\tPredicting\tthe\tfuture\tis\twhat\tyou do\tall\tthe\ttime,\twhether\tyou\tare\tfinishing\ta\tfriend’s\tsentence\tor\tanticipating\tthe\tsmell\tof\tcoffee\tat breakfast.\tIn\tthis\tchapter,\twe\tare\tgoing\tto\tdiscuss\trecurrent\tneural\tnetworks\t(RNN),\ta\tclass\tof\tnets\tthat can\tpredict\tthe\tfuture\t(well,\tup\tto\ta\tpoint,\tof\tcourse).\tThey\tcan\tanalyze\ttime\tseries\tdata\tsuch\tas\tstock prices,\tand\ttell\tyou\twhen\tto\tbuy\tor\tsell.\tIn\tautonomous\tdriving\tsystems,\tthey\tcan\tanticipate\tcar trajectories\tand\thelp\tavoid\taccidents.\tMore\tgenerally,\tthey\tcan\twork\ton\tsequences\tof\tarbitrary\tlengths, rather\tthan\ton\tfixed-sized\tinputs\tlike\tall\tthe\tnets\twe\thave\tdiscussed\tso\tfar.\tFor\texample,\tthey\tcan\ttake sentences,\tdocuments,\tor\taudio\tsamples\tas\tinput,\tmaking\tthem\textremely\tuseful\tfor\tnatural\tlanguage processing\t(NLP)\tsystems\tsuch\tas\tautomatic\ttranslation,\tspeech-to-text,\tor\tsentiment\tanalysis\t(e.g., reading\tmovie\treviews\tand\textracting\tthe\trater’s\tfeeling\tabout\tthe\tmovie).\n\nMoreover,\tRNNs’\tability\tto\tanticipate\talso\tmakes\tthem\tcapable\tof\tsurprising\tcreativity.\tYou\tcan\task\tthem to\tpredict\twhich\tare\tthe\tmost\tlikely\tnext\tnotes\tin\ta\tmelody,\tthen\trandomly\tpick\tone\tof\tthese\tnotes\tand\tplay it.\tThen\task\tthe\tnet\tfor\tthe\tnext\tmost\tlikely\tnotes,\tplay\tit,\tand\trepeat\tthe\tprocess\tagain\tand\tagain.\tBefore you\tknow\tit,\tyour\tnet\twill\tcompose\ta\tmelody\tsuch\tas\tthe\tone\tproduced\tby\tGoogle’s\tMagenta\tproject. Similarly,\tRNNs\tcan\tgenerate\tsentences,\timage\tcaptions,\tand\tmuch\tmore.\tThe\tresult\tis\tnot\texactly Shakespeare\tor\tMozart\tyet,\tbut\twho\tknows\twhat\tthey\twill\tproduce\ta\tfew\tyears\tfrom\tnow?\n\nIn\tthis\tchapter,\twe\twill\tlook\tat\tthe\tfundamental\tconcepts\tunderlying\tRNNs,\tthe\tmain\tproblem\tthey\tface (namely,\tvanishing/exploding\tgradients,\tdiscussed\tin\tChapter\t11),\tand\tthe\tsolutions\twidely\tused\tto\tfight it:\tLSTM\tand\tGRU\tcells.\tAlong\tthe\tway,\tas\talways,\twe\twill\tshow\thow\tto\timplement\tRNNs\tusing TensorFlow.\tFinally,\twe\twill\ttake\ta\tlook\tat\tthe\tarchitecture\tof\ta\tmachine\ttranslation\tsystem.\n\nRecurrent\tNeurons Up\tto\tnow\twe\thave\tmostly\tlooked\tat\tfeedforward\tneural\tnetworks,\twhere\tthe\tactivations\tflow\tonly\tin\tone direction,\tfrom\tthe\tinput\tlayer\tto\tthe\toutput\tlayer\t(except\tfor\ta\tfew\tnetworks\tin\tAppendix\tE).\tA\trecurrent neural\tnetwork\tlooks\tvery\tmuch\tlike\ta\tfeedforward\tneural\tnetwork,\texcept\tit\talso\thas\tconnections pointing\tbackward.\tLet’s\tlook\tat\tthe\tsimplest\tpossible\tRNN,\tcomposed\tof\tjust\tone\tneuron\treceiving inputs,\tproducing\tan\toutput,\tand\tsending\tthat\toutput\tback\tto\titself,\tas\tshown\tin\tFigure\t14-1\t(left).\tAt\teach time\tstep\tt\t(also\tcalled\ta\tframe),\tthis\trecurrent\tneuron\treceives\tthe\tinputs\tx(t)\tas\twell\tas\tits\town\toutput from\tthe\tprevious\ttime\tstep,\ty(t–1).\tWe\tcan\trepresent\tthis\ttiny\tnetwork\tagainst\tthe\ttime\taxis,\tas\tshown\tin Figure\t14-1\t(right).\tThis\tis\tcalled\tunrolling\tthe\tnetwork\tthrough\ttime.\n\nFigure\t14-1.\tA\trecurrent\tneuron\t(left),\tunrolled\tthrough\ttime\t(right)\n\nYou\tcan\teasily\tcreate\ta\tlayer\tof\trecurrent\tneurons.\tAt\teach\ttime\tstep\tt,\tevery\tneuron\treceives\tboth\tthe input\tvector\tx(t)\tand\tthe\toutput\tvector\tfrom\tthe\tprevious\ttime\tstep\ty(t–1),\tas\tshown\tin\tFigure\t14-2.\tNote that\tboth\tthe\tinputs\tand\toutputs\tare\tvectors\tnow\t(when\tthere\twas\tjust\ta\tsingle\tneuron,\tthe\toutput\twas\ta scalar).\n\nFigure\t14-2.\tA\tlayer\tof\trecurrent\tneurons\t(left),\tunrolled\tthrough\ttime\t(right)\n\nEach\trecurrent\tneuron\thas\ttwo\tsets\tof\tweights:\tone\tfor\tthe\tinputs\tx(t)\tand\tthe\tother\tfor\tthe\toutputs\tof\tthe previous\ttime\tstep,\ty(t–1).\tLet’s\tcall\tthese\tweight\tvectors\twx\tand\twy.\tThe\toutput\tof\ta\trecurrent\tlayer\tcan\tbe\n\ncomputed\tpretty\tmuch\tas\tyou\tmight\texpect,\tas\tshown\tin\tEquation\t14-1\t(b\tis\tthe\tbias\tterm\tand\tϕ(·)\tis\tthe activation\tfunction,\te.g.,\tReLU1).\n\nEquation\t14-1.\tOutput\tof\ta\trecurrent\tlayer\tfor\ta\tsingle\tinstance\n\nJust\tlike\tfor\tfeedforward\tneural\tnetworks,\twe\tcan\tcompute\ta\trecurrent\tlayer’s\toutput\tin\tone\tshot\tfor\ta whole\tmini-batch\tusing\ta\tvectorized\tform\tof\tthe\tprevious\tequation\t(see\tEquation\t14-2).\n\nEquation\t14-2.\tOutputs\tof\ta\tlayer\tof\trecurrent\tneurons\tfor\tall\tinstances\tin\ta\tmini-batch\n\nY(t)\tis\tan\tm\t×\tnneurons\tmatrix\tcontaining\tthe\tlayer’s\toutputs\tat\ttime\tstep\tt\tfor\teach\tinstance\tin\tthe\tmini- batch\t(m\tis\tthe\tnumber\tof\tinstances\tin\tthe\tmini-batch\tand\tnneurons\tis\tthe\tnumber\tof\tneurons).\n\nX(t)\tis\tan\tm\t×\tninputs\tmatrix\tcontaining\tthe\tinputs\tfor\tall\tinstances\t(ninputs\tis\tthe\tnumber\tof\tinput features).\n\nWx\tis\tan\tninputs\t×\tnneurons\tmatrix\tcontaining\tthe\tconnection\tweights\tfor\tthe\tinputs\tof\tthe\tcurrent\ttime step.\n\nWy\tis\tan\tnneurons\t×\tnneurons\tmatrix\tcontaining\tthe\tconnection\tweights\tfor\tthe\toutputs\tof\tthe\tprevious time\tstep.\n\nThe\tweight\tmatrices\tWx\tand\tWy\tare\toften\tconcatenated\tinto\ta\tsingle\tweight\tmatrix\tW\tof\tshape (ninputs\t+\tnneurons)\t×\tnneurons\t(see\tthe\tsecond\tline\tof\tEquation\t14-2).\n\nb\tis\ta\tvector\tof\tsize\tnneurons\tcontaining\teach\tneuron’s\tbias\tterm.\n\nNotice\tthat\tY(t)\tis\ta\tfunction\tof\tX(t)\tand\tY(t–1),\twhich\tis\ta\tfunction\tof\tX(t–1)\tand\tY(t–2),\twhich\tis\ta\tfunction of\tX(t–2)\tand\tY(t–3),\tand\tso\ton.\tThis\tmakes\tY(t)\ta\tfunction\tof\tall\tthe\tinputs\tsince\ttime\tt\t=\t0\t(that\tis,\tX(0), X(1),\t…,\tX(t)).\tAt\tthe\tfirst\ttime\tstep,\tt\t=\t0,\tthere\tare\tno\tprevious\toutputs,\tso\tthey\tare\ttypically\tassumed\tto be\tall\tzeros.",
      "page_number": 442
    },
    {
      "number": 14,
      "title": "Recurrent\tNeural\tNetworks",
      "start_page": 469,
      "end_page": 505,
      "detection_method": "regex_chapter_title",
      "content": "Memory\tCells Since\tthe\toutput\tof\ta\trecurrent\tneuron\tat\ttime\tstep\tt\tis\ta\tfunction\tof\tall\tthe\tinputs\tfrom\tprevious\ttime\tsteps, you\tcould\tsay\tit\thas\ta\tform\tof\tmemory.\tA\tpart\tof\ta\tneural\tnetwork\tthat\tpreserves\tsome\tstate\tacross\ttime steps\tis\tcalled\ta\tmemory\tcell\t(or\tsimply\ta\tcell).\tA\tsingle\trecurrent\tneuron,\tor\ta\tlayer\tof\trecurrent neurons,\tis\ta\tvery\tbasic\tcell,\tbut\tlater\tin\tthis\tchapter\twe\twill\tlook\tat\tsome\tmore\tcomplex\tand\tpowerful types\tof\tcells.\n\nIn\tgeneral\ta\tcell’s\tstate\tat\ttime\tstep\tt,\tdenoted\th(t)\t(the\t“h”\tstands\tfor\t“hidden”),\tis\ta\tfunction\tof\tsome inputs\tat\tthat\ttime\tstep\tand\tits\tstate\tat\tthe\tprevious\ttime\tstep:\th(t)\t=\tf(h(t–1),\tx(t)).\tIts\toutput\tat\ttime\tstep\tt, denoted\ty(t),\tis\talso\ta\tfunction\tof\tthe\tprevious\tstate\tand\tthe\tcurrent\tinputs.\tIn\tthe\tcase\tof\tthe\tbasic\tcells\twe have\tdiscussed\tso\tfar,\tthe\toutput\tis\tsimply\tequal\tto\tthe\tstate,\tbut\tin\tmore\tcomplex\tcells\tthis\tis\tnot\talways the\tcase,\tas\tshown\tin\tFigure\t14-3.\n\nFigure\t14-3.\tA\tcell’s\thidden\tstate\tand\tits\toutput\tmay\tbe\tdifferent\n\nInput\tand\tOutput\tSequences An\tRNN\tcan\tsimultaneously\ttake\ta\tsequence\tof\tinputs\tand\tproduce\ta\tsequence\tof\toutputs\t(see\tFigure\t14- 4,\ttop-left\tnetwork).\tFor\texample,\tthis\ttype\tof\tnetwork\tis\tuseful\tfor\tpredicting\ttime\tseries\tsuch\tas\tstock prices:\tyou\tfeed\tit\tthe\tprices\tover\tthe\tlast\tN\tdays,\tand\tit\tmust\toutput\tthe\tprices\tshifted\tby\tone\tday\tinto\tthe future\t(i.e.,\tfrom\tN\t–\t1\tdays\tago\tto\ttomorrow).\n\nAlternatively,\tyou\tcould\tfeed\tthe\tnetwork\ta\tsequence\tof\tinputs,\tand\tignore\tall\toutputs\texcept\tfor\tthe\tlast one\t(see\tthe\ttop-right\tnetwork).\tIn\tother\twords,\tthis\tis\ta\tsequence-to-vector\tnetwork.\tFor\texample,\tyou could\tfeed\tthe\tnetwork\ta\tsequence\tof\twords\tcorresponding\tto\ta\tmovie\treview,\tand\tthe\tnetwork\twould output\ta\tsentiment\tscore\t(e.g.,\tfrom\t–1\t[hate]\tto\t+1\t[love]).\n\nConversely,\tyou\tcould\tfeed\tthe\tnetwork\ta\tsingle\tinput\tat\tthe\tfirst\ttime\tstep\t(and\tzeros\tfor\tall\tother\ttime steps),\tand\tlet\tit\toutput\ta\tsequence\t(see\tthe\tbottom-left\tnetwork).\tThis\tis\ta\tvector-to-sequence\tnetwork. For\texample,\tthe\tinput\tcould\tbe\tan\timage,\tand\tthe\toutput\tcould\tbe\ta\tcaption\tfor\tthat\timage.\n\nLastly,\tyou\tcould\thave\ta\tsequence-to-vector\tnetwork,\tcalled\tan\tencoder,\tfollowed\tby\ta\tvector-to- sequence\tnetwork,\tcalled\ta\tdecoder\t(see\tthe\tbottom-right\tnetwork).\tFor\texample,\tthis\tcan\tbe\tused\tfor translating\ta\tsentence\tfrom\tone\tlanguage\tto\tanother.\tYou\twould\tfeed\tthe\tnetwork\ta\tsentence\tin\tone language,\tthe\tencoder\twould\tconvert\tthis\tsentence\tinto\ta\tsingle\tvector\trepresentation,\tand\tthen\tthe decoder\twould\tdecode\tthis\tvector\tinto\ta\tsentence\tin\tanother\tlanguage.\tThis\ttwo-step\tmodel,\tcalled\tan Encoder–Decoder,\tworks\tmuch\tbetter\tthan\ttrying\tto\ttranslate\ton\tthe\tfly\twith\ta\tsingle\tsequence-to- sequence\tRNN\t(like\tthe\tone\trepresented\ton\tthe\ttop\tleft),\tsince\tthe\tlast\twords\tof\ta\tsentence\tcan\taffect\tthe first\twords\tof\tthe\ttranslation,\tso\tyou\tneed\tto\twait\tuntil\tyou\thave\theard\tthe\twhole\tsentence\tbefore translating\tit.\n\nFigure\t14-4.\tSeq\tto\tseq\t(top\tleft),\tseq\tto\tvector\t(top\tright),\tvector\tto\tseq\t(bottom\tleft),\tdelayed\tseq\tto\tseq\t(bottom\tright)\n\nSounds\tpromising,\tso\tlet’s\tstart\tcoding!\n\nBasic\tRNNs\tin\tTensorFlow First,\tlet’s\timplement\ta\tvery\tsimple\tRNN\tmodel,\twithout\tusing\tany\tof\tTensorFlow’s\tRNN\toperations,\tto better\tunderstand\twhat\tgoes\ton\tunder\tthe\thood.\tWe\twill\tcreate\tan\tRNN\tcomposed\tof\ta\tlayer\tof\tfive recurrent\tneurons\t(like\tthe\tRNN\trepresented\tin\tFigure\t14-2),\tusing\tthe\ttanh\tactivation\tfunction.\tWe\twill assume\tthat\tthe\tRNN\truns\tover\tonly\ttwo\ttime\tsteps,\ttaking\tinput\tvectors\tof\tsize\t3\tat\teach\ttime\tstep.\tThe following\tcode\tbuilds\tthis\tRNN,\tunrolled\tthrough\ttwo\ttime\tsteps:\n\nn_inputs\t=\t3 n_neurons\t=\t5\n\nX0\t=\ttf.placeholder(tf.float32,\t[None,\tn_inputs]) X1\t=\ttf.placeholder(tf.float32,\t[None,\tn_inputs])\n\nWx\t=\ttf.Variable(tf.random_normal(shape=[n_inputs,\tn_neurons],dtype=tf.float32)) Wy\t=\ttf.Variable(tf.random_normal(shape=[n_neurons,n_neurons],dtype=tf.float32)) b\t=\ttf.Variable(tf.zeros([1,\tn_neurons],\tdtype=tf.float32))\n\nY0\t=\ttf.tanh(tf.matmul(X0,\tWx)\t+\tb) Y1\t=\ttf.tanh(tf.matmul(Y0,\tWy)\t+\ttf.matmul(X1,\tWx)\t+\tb)\n\ninit\t=\ttf.global_variables_initializer()\n\nThis\tnetwork\tlooks\tmuch\tlike\ta\ttwo-layer\tfeedforward\tneural\tnetwork,\twith\ta\tfew\ttwists:\tfirst,\tthe\tsame weights\tand\tbias\tterms\tare\tshared\tby\tboth\tlayers,\tand\tsecond,\twe\tfeed\tinputs\tat\teach\tlayer,\tand\twe\tget outputs\tfrom\teach\tlayer.\tTo\trun\tthe\tmodel,\twe\tneed\tto\tfeed\tit\tthe\tinputs\tat\tboth\ttime\tsteps,\tlike\tso:\n\nimport\tnumpy\tas\tnp\n\n#\tMini-batch:\t\t\t\t\t\t\t\tinstance\t0,instance\t1,instance\t2,instance\t3 X0_batch\t=\tnp.array([[0,\t1,\t2],\t[3,\t4,\t5],\t[6,\t7,\t8],\t[9,\t0,\t1]])\t#\tt\t=\t0 X1_batch\t=\tnp.array([[9,\t8,\t7],\t[0,\t0,\t0],\t[6,\t5,\t4],\t[3,\t2,\t1]])\t#\tt\t=\t1\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tY0_val,\tY1_val\t=\tsess.run([Y0,\tY1],\tfeed_dict={X0:\tX0_batch,\tX1:\tX1_batch})\n\nThis\tmini-batch\tcontains\tfour\tinstances,\teach\twith\tan\tinput\tsequence\tcomposed\tof\texactly\ttwo\tinputs.\tAt the\tend,\tY0_val\tand\tY1_val\tcontain\tthe\toutputs\tof\tthe\tnetwork\tat\tboth\ttime\tsteps\tfor\tall\tneurons\tand\tall instances\tin\tthe\tmini-batch:\n\n>>>\tprint(Y0_val)\t\t#\toutput\tat\tt\t=\t0 [[-0.0664006\t\t\t0.96257669\t\t0.68105787\t\t0.70918542\t-0.89821595]\t\t#\tinstance\t0 \t[\t0.9977755\t\t-0.71978885\t-0.99657625\t\t0.9673925\t\t-0.99989718]\t\t#\tinstance\t1 \t[\t0.99999774\t-0.99898815\t-0.99999893\t\t0.99677622\t-0.99999988]\t\t#\tinstance\t2 \t[\t1.\t\t\t\t\t\t\t\t\t-1.\t\t\t\t\t\t\t\t\t-1.\t\t\t\t\t\t\t\t\t-0.99818915\t\t0.99950868]]\t#\tinstance\t3 >>>\tprint(Y1_val)\t\t#\toutput\tat\tt\t=\t1 [[\t1.\t\t\t\t\t\t\t\t\t-1.\t\t\t\t\t\t\t\t\t-1.\t\t\t\t\t\t\t\t\t\t0.40200216\t-1.\t\t\t\t\t\t\t\t]\t\t#\tinstance\t0 \t[-0.12210433\t\t0.62805319\t\t0.96718419\t-0.99371207\t-0.25839335]\t\t#\tinstance\t1 \t[\t0.99999827\t-0.9999994\t\t-0.9999975\t\t-0.85943311\t-0.9999879\t]\t\t#\tinstance\t2 \t[\t0.99928284\t-0.99999815\t-0.99990582\t\t0.98579615\t-0.92205751]]\t#\tinstance\t3\n\nThat\twasn’t\ttoo\thard,\tbut\tof\tcourse\tif\tyou\twant\tto\tbe\table\tto\trun\tan\tRNN\tover\t100\ttime\tsteps,\tthe\tgraph\tis going\tto\tbe\tpretty\tbig.\tNow\tlet’s\tlook\tat\thow\tto\tcreate\tthe\tsame\tmodel\tusing\tTensorFlow’s\tRNN operations.\n\nStatic\tUnrolling\tThrough\tTime The\tstatic_rnn()\tfunction\tcreates\tan\tunrolled\tRNN\tnetwork\tby\tchaining\tcells.\tThe\tfollowing\tcode creates\tthe\texact\tsame\tmodel\tas\tthe\tprevious\tone:\n\nX0\t=\ttf.placeholder(tf.float32,\t[None,\tn_inputs]) X1\t=\ttf.placeholder(tf.float32,\t[None,\tn_inputs])\n\nbasic_cell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons) output_seqs,\tstates\t=\ttf.contrib.rnn.static_rnn(basic_cell,\t[X0,\tX1], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdtype=tf.float32) Y0,\tY1\t=\toutput_seqs\n\nFirst\twe\tcreate\tthe\tinput\tplaceholders,\tas\tbefore.\tThen\twe\tcreate\ta\tBasicRNNCell,\twhich\tyou\tcan\tthink of\tas\ta\tfactory\tthat\tcreates\tcopies\tof\tthe\tcell\tto\tbuild\tthe\tunrolled\tRNN\t(one\tfor\teach\ttime\tstep).\tThen\twe call\tstatic_rnn(),\tgiving\tit\tthe\tcell\tfactory\tand\tthe\tinput\ttensors,\tand\ttelling\tit\tthe\tdata\ttype\tof\tthe\tinputs (this\tis\tused\tto\tcreate\tthe\tinitial\tstate\tmatrix,\twhich\tby\tdefault\tis\tfull\tof\tzeros).\tThe\tstatic_rnn() function\tcalls\tthe\tcell\tfactory’s\t__call__()\tfunction\tonce\tper\tinput,\tcreating\ttwo\tcopies\tof\tthe\tcell\t(each containing\ta\tlayer\tof\tfive\trecurrent\tneurons),\twith\tshared\tweights\tand\tbias\tterms,\tand\tit\tchains\tthem\tjust like\twe\tdid\tearlier.\tThe\tstatic_rnn()\tfunction\treturns\ttwo\tobjects.\tThe\tfirst\tis\ta\tPython\tlist\tcontaining the\toutput\ttensors\tfor\teach\ttime\tstep.\tThe\tsecond\tis\ta\ttensor\tcontaining\tthe\tfinal\tstates\tof\tthe\tnetwork. When\tyou\tare\tusing\tbasic\tcells,\tthe\tfinal\tstate\tis\tsimply\tequal\tto\tthe\tlast\toutput.\n\nIf\tthere\twere\t50\ttime\tsteps,\tit\twould\tnot\tbe\tvery\tconvenient\tto\thave\tto\tdefine\t50\tinput\tplaceholders\tand 50\toutput\ttensors.\tMoreover,\tat\texecution\ttime\tyou\twould\thave\tto\tfeed\teach\tof\tthe\t50\tplaceholders\tand manipulate\tthe\t50\toutputs.\tLet’s\tsimplify\tthis.\tThe\tfollowing\tcode\tbuilds\tthe\tsame\tRNN\tagain,\tbut\tthis time\tit\ttakes\ta\tsingle\tinput\tplaceholder\tof\tshape\t[None,\tn_steps,\tn_inputs]\twhere\tthe\tfirst\tdimension is\tthe\tmini-batch\tsize.\tThen\tit\textracts\tthe\tlist\tof\tinput\tsequences\tfor\teach\ttime\tstep.\tX_seqs\tis\ta\tPython list\tof\tn_steps\ttensors\tof\tshape\t[None,\tn_inputs],\twhere\tonce\tagain\tthe\tfirst\tdimension\tis\tthe\tmini- batch\tsize.\tTo\tdo\tthis,\twe\tfirst\tswap\tthe\tfirst\ttwo\tdimensions\tusing\tthe\ttranspose()\tfunction,\tso\tthat\tthe time\tsteps\tare\tnow\tthe\tfirst\tdimension.\tThen\twe\textract\ta\tPython\tlist\tof\ttensors\talong\tthe\tfirst\tdimension (i.e.,\tone\ttensor\tper\ttime\tstep)\tusing\tthe\tunstack()\tfunction.\tThe\tnext\ttwo\tlines\tare\tthe\tsame\tas\tbefore. Finally,\twe\tmerge\tall\tthe\toutput\ttensors\tinto\ta\tsingle\ttensor\tusing\tthe\tstack()\tfunction,\tand\twe\tswap\tthe first\ttwo\tdimensions\tto\tget\ta\tfinal\toutputs\ttensor\tof\tshape\t[None,\tn_steps,\tn_neurons]\t(again\tthe first\tdimension\tis\tthe\tmini-batch\tsize).\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_inputs]) X_seqs\t=\ttf.unstack(tf.transpose(X,\tperm=[1,\t0,\t2])) basic_cell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons) output_seqs,\tstates\t=\ttf.contrib.rnn.static_rnn(basic_cell,\tX_seqs, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdtype=tf.float32) outputs\t=\ttf.transpose(tf.stack(output_seqs),\tperm=[1,\t0,\t2])\n\nNow\twe\tcan\trun\tthe\tnetwork\tby\tfeeding\tit\ta\tsingle\ttensor\tthat\tcontains\tall\tthe\tmini-batch\tsequences:\n\nX_batch\t=\tnp.array([ \t\t\t\t\t\t\t\t\t#\tt\t=\t0\t\t\t\t\tt\t=\t1 \t\t\t\t\t\t\t\t[[0,\t1,\t2],\t[9,\t8,\t7]],\t#\tinstance\t0 \t\t\t\t\t\t\t\t[[3,\t4,\t5],\t[0,\t0,\t0]],\t#\tinstance\t1\n\n[[6,\t7,\t8],\t[6,\t5,\t4]],\t#\tinstance\t2 \t\t\t\t\t\t\t\t[[9,\t0,\t1],\t[3,\t2,\t1]],\t#\tinstance\t3 \t\t\t\t])\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\toutputs_val\t=\toutputs.eval(feed_dict={X:\tX_batch})\n\nAnd\twe\tget\ta\tsingle\toutputs_val\ttensor\tfor\tall\tinstances,\tall\ttime\tsteps,\tand\tall\tneurons:\n\n>>>\tprint(outputs_val) [[[-0.91279727\t\t0.83698678\t-0.89277941\t\t0.80308062\t-0.5283336\t] \t\t[-1.\t\t\t\t\t\t\t\t\t\t1.\t\t\t\t\t\t\t\t\t-0.99794829\t\t0.99985468\t-0.99273592]]\n\n[[-0.99994391\t\t0.99951613\t-0.9946925\t\t\t0.99030769\t-0.94413054] \t\t[\t0.48733309\t\t0.93389565\t-0.31362072\t\t0.88573611\t\t0.2424476\t]]\n\n[[-1.\t\t\t\t\t\t\t\t\t\t0.99999875\t-0.99975014\t\t0.99956584\t-0.99466234] \t\t[-0.99994856\t\t0.99999434\t-0.96058172\t\t0.99784708\t-0.9099462\t]]\n\n[[-0.95972425\t\t0.99951482\t\t0.96938795\t-0.969908\t\t\t-0.67668229] \t\t[-0.84596014\t\t0.96288228\t\t0.96856463\t-0.14777924\t-0.9119423\t]]]\n\nHowever,\tthis\tapproach\tstill\tbuilds\ta\tgraph\tcontaining\tone\tcell\tper\ttime\tstep.\tIf\tthere\twere\t50\ttime\tsteps, the\tgraph\twould\tlook\tpretty\tugly.\tIt\tis\ta\tbit\tlike\twriting\ta\tprogram\twithout\tever\tusing\tloops\t(e.g.,\tY0=f(0, X0);\tY1=f(Y0,\tX1);\tY2=f(Y1,\tX2);\t...;\tY50=f(Y49,\tX50)).\tWith\tsuch\tas\tlarge\tgraph,\tyou\tmay even\tget\tout-of-memory\t(OOM)\terrors\tduring\tbackpropagation\t(especially\twith\tthe\tlimited\tmemory\tof GPU\tcards),\tsince\tit\tmust\tstore\tall\ttensor\tvalues\tduring\tthe\tforward\tpass\tso\tit\tcan\tuse\tthem\tto\tcompute gradients\tduring\tthe\treverse\tpass.\n\nFortunately,\tthere\tis\ta\tbetter\tsolution:\tthe\tdynamic_rnn()\tfunction.\n\nDynamic\tUnrolling\tThrough\tTime The\tdynamic_rnn()\tfunction\tuses\ta\twhile_loop()\toperation\tto\trun\tover\tthe\tcell\tthe\tappropriate\tnumber of\ttimes,\tand\tyou\tcan\tset\tswap_memory=True\tif\tyou\twant\tit\tto\tswap\tthe\tGPU’s\tmemory\tto\tthe\tCPU’s memory\tduring\tbackpropagation\tto\tavoid\tOOM\terrors.\tConveniently,\tit\talso\taccepts\ta\tsingle\ttensor\tfor\tall inputs\tat\tevery\ttime\tstep\t(shape\t[None,\tn_steps,\tn_inputs])\tand\tit\toutputs\ta\tsingle\ttensor\tfor\tall outputs\tat\tevery\ttime\tstep\t(shape\t[None,\tn_steps,\tn_neurons]);\tthere\tis\tno\tneed\tto\tstack,\tunstack,\tor transpose.\tThe\tfollowing\tcode\tcreates\tthe\tsame\tRNN\tas\tearlier\tusing\tthe\tdynamic_rnn()\tfunction.\tIt’s\tso much\tnicer!\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_inputs])\n\nbasic_cell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons) outputs,\tstates\t=\ttf.nn.dynamic_rnn(basic_cell,\tX,\tdtype=tf.float32)\n\nNOTE\n\nDuring\tbackpropagation,\tthe\twhile_loop()\toperation\tdoes\tthe\tappropriate\tmagic:\tit\tstores\tthe\ttensor\tvalues\tfor\teach\titeration during\tthe\tforward\tpass\tso\tit\tcan\tuse\tthem\tto\tcompute\tgradients\tduring\tthe\treverse\tpass.\n\nHandling\tVariable\tLength\tInput\tSequences So\tfar\twe\thave\tused\tonly\tfixed-size\tinput\tsequences\t(all\texactly\ttwo\tsteps\tlong).\tWhat\tif\tthe\tinput sequences\thave\tvariable\tlengths\t(e.g.,\tlike\tsentences)?\tIn\tthis\tcase\tyou\tshould\tset\tthe\tsequence_length argument\twhen\tcalling\tthe\tdynamic_rnn()\t(or\tstatic_rnn())\tfunction;\tit\tmust\tbe\ta\t1D\ttensor\tindicating the\tlength\tof\tthe\tinput\tsequence\tfor\teach\tinstance.\tFor\texample:\n\nseq_length\t=\ttf.placeholder(tf.int32,\t[None])\n\n[...] outputs,\tstates\t=\ttf.nn.dynamic_rnn(basic_cell,\tX,\tdtype=tf.float32, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsequence_length=seq_length)\n\nFor\texample,\tsuppose\tthe\tsecond\tinput\tsequence\tcontains\tonly\tone\tinput\tinstead\tof\ttwo.\tIt\tmust\tbe\tpadded with\ta\tzero\tvector\tin\torder\tto\tfit\tin\tthe\tinput\ttensor\tX\t(because\tthe\tinput\ttensor’s\tsecond\tdimension\tis\tthe size\tof\tthe\tlongest\tsequence\t—\ti.e.,\t2).\n\nX_batch\t=\tnp.array([ \t\t\t\t\t\t\t\t#\tstep\t0\t\t\t\t\tstep\t1 \t\t\t\t\t\t\t\t[[0,\t1,\t2],\t[9,\t8,\t7]],\t#\tinstance\t0 \t\t\t\t\t\t\t\t[[3,\t4,\t5],\t[0,\t0,\t0]],\t#\tinstance\t1\t(padded\twith\ta\tzero\tvector) \t\t\t\t\t\t\t\t[[6,\t7,\t8],\t[6,\t5,\t4]],\t#\tinstance\t2 \t\t\t\t\t\t\t\t[[9,\t0,\t1],\t[3,\t2,\t1]],\t#\tinstance\t3 \t\t\t\t]) seq_length_batch\t=\tnp.array([2,\t1,\t2,\t2])\n\nOf\tcourse,\tyou\tnow\tneed\tto\tfeed\tvalues\tfor\tboth\tplaceholders\tX\tand\tseq_length:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\toutputs_val,\tstates_val\t=\tsess.run( \t\t\t\t\t\t\t\t[outputs,\tstates],\tfeed_dict={X:\tX_batch,\tseq_length:\tseq_length_batch})\n\nNow\tthe\tRNN\toutputs\tzero\tvectors\tfor\tevery\ttime\tstep\tpast\tthe\tinput\tsequence\tlength\t(look\tat\tthe\tsecond instance’s\toutput\tfor\tthe\tsecond\ttime\tstep):\n\n>>>\tprint(outputs_val) [[[-0.68579948\t-0.25901747\t-0.80249101\t-0.18141513\t-0.37491536] \t\t[-0.99996698\t-0.94501185\t\t0.98072106\t-0.9689762\t\t\t0.99966913]]\t\t#\tfinal\tstate\n\n[[-0.99099374\t-0.64768541\t-0.67801034\t-0.7415446\t\t\t0.7719509\t]\t\t\t#\tfinal\tstate \t\t[\t0.\t\t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t]]\t\t#\tzero\tvector\n\n[[-0.99978048\t-0.85583007\t-0.49696958\t-0.93838578\t\t0.98505187] \t\t[-0.99951065\t-0.89148796\t\t0.94170523\t-0.38407657\t\t0.97499216]]\t\t#\tfinal\tstate\n\n[[-0.02052618\t-0.94588047\t\t0.99935204\t\t0.37283331\t\t0.9998163\t] \t\t[-0.91052347\t\t0.05769409\t\t0.47446665\t-0.44611037\t\t0.89394671]]]\t#\tfinal\tstate\n\nMoreover,\tthe\tstates\ttensor\tcontains\tthe\tfinal\tstate\tof\teach\tcell\t(excluding\tthe\tzero\tvectors):\n\n>>>\tprint(states_val) [[-0.99996698\t-0.94501185\t\t0.98072106\t-0.9689762\t\t\t0.99966913]\t\t#\tt\t=\t1 \t[-0.99099374\t-0.64768541\t-0.67801034\t-0.7415446\t\t\t0.7719509\t]\t\t#\tt\t=\t0\t!!! \t[-0.99951065\t-0.89148796\t\t0.94170523\t-0.38407657\t\t0.97499216]\t\t#\tt\t=\t1 \t[-0.91052347\t\t0.05769409\t\t0.47446665\t-0.44611037\t\t0.89394671]]\t#\tt\t=\t1\n\nHandling\tVariable-Length\tOutput\tSequences What\tif\tthe\toutput\tsequences\thave\tvariable\tlengths\tas\twell?\tIf\tyou\tknow\tin\tadvance\twhat\tlength\teach sequence\twill\thave\t(for\texample\tif\tyou\tknow\tthat\tit\twill\tbe\tthe\tsame\tlength\tas\tthe\tinput\tsequence),\tthen you\tcan\tset\tthe\tsequence_length\tparameter\tas\tdescribed\tabove.\tUnfortunately,\tin\tgeneral\tthis\twill\tnot be\tpossible:\tfor\texample,\tthe\tlength\tof\ta\ttranslated\tsentence\tis\tgenerally\tdifferent\tfrom\tthe\tlength\tof\tthe input\tsentence.\tIn\tthis\tcase,\tthe\tmost\tcommon\tsolution\tis\tto\tdefine\ta\tspecial\toutput\tcalled\tan\tend-of- sequence\ttoken\t(EOS\ttoken).\tAny\toutput\tpast\tthe\tEOS\tshould\tbe\tignored\t(we\twill\tdiscuss\tthis\tlater\tin this\tchapter).\n\nOkay,\tnow\tyou\tknow\thow\tto\tbuild\tan\tRNN\tnetwork\t(or\tmore\tprecisely\tan\tRNN\tnetwork\tunrolled\tthrough time).\tBut\thow\tdo\tyou\ttrain\tit?\n\nTraining\tRNNs To\ttrain\tan\tRNN,\tthe\ttrick\tis\tto\tunroll\tit\tthrough\ttime\t(like\twe\tjust\tdid)\tand\tthen\tsimply\tuse\tregular backpropagation\t(see\tFigure\t14-5).\tThis\tstrategy\tis\tcalled\tbackpropagation\tthrough\ttime\t(BPTT).\n\nFigure\t14-5.\tBackpropagation\tthrough\ttime\n\nJust\tlike\tin\tregular\tbackpropagation,\tthere\tis\ta\tfirst\tforward\tpass\tthrough\tthe\tunrolled\tnetwork (represented\tby\tthe\tdashed\tarrows);\tthen\tthe\toutput\tsequence\tis\tevaluated\tusing\ta\tcost\tfunction\n\n(where\ttmin\tand\ttmax\tare\tthe\tfirst\tand\tlast\toutput\ttime\tsteps,\tnot\tcounting the\tignored\toutputs),\tand\tthe\tgradients\tof\tthat\tcost\tfunction\tare\tpropagated\tbackward\tthrough\tthe\tunrolled network\t(represented\tby\tthe\tsolid\tarrows);\tand\tfinally\tthe\tmodel\tparameters\tare\tupdated\tusing\tthe gradients\tcomputed\tduring\tBPTT.\tNote\tthat\tthe\tgradients\tflow\tbackward\tthrough\tall\tthe\toutputs\tused\tby the\tcost\tfunction,\tnot\tjust\tthrough\tthe\tfinal\toutput\t(for\texample,\tin\tFigure\t14-5\tthe\tcost\tfunction\tis computed\tusing\tthe\tlast\tthree\toutputs\tof\tthe\tnetwork,\tY(2),\tY(3),\tand\tY(4),\tso\tgradients\tflow\tthrough\tthese three\toutputs,\tbut\tnot\tthrough\tY(0)\tand\tY(1)).\tMoreover,\tsince\tthe\tsame\tparameters\tW\tand\tb\tare\tused\tat each\ttime\tstep,\tbackpropagation\twill\tdo\tthe\tright\tthing\tand\tsum\tover\tall\ttime\tsteps.\n\nTraining\ta\tSequence\tClassifier Let’s\ttrain\tan\tRNN\tto\tclassify\tMNIST\timages.\tA\tconvolutional\tneural\tnetwork\twould\tbe\tbetter\tsuited\tfor image\tclassification\t(see\tChapter\t13),\tbut\tthis\tmakes\tfor\ta\tsimple\texample\tthat\tyou\tare\talready\tfamiliar with.\tWe\twill\ttreat\teach\timage\tas\ta\tsequence\tof\t28\trows\tof\t28\tpixels\teach\t(since\teach\tMNIST\timage\tis 28\t×\t28\tpixels).\tWe\twill\tuse\tcells\tof\t150\trecurrent\tneurons,\tplus\ta\tfully\tconnected\tlayer\tcontaining\t10 neurons\t(one\tper\tclass)\tconnected\tto\tthe\toutput\tof\tthe\tlast\ttime\tstep,\tfollowed\tby\ta\tsoftmax\tlayer\t(see Figure\t14-6).\n\nFigure\t14-6.\tSequence\tclassifier\n\nThe\tconstruction\tphase\tis\tquite\tstraightforward;\tit’s\tpretty\tmuch\tthe\tsame\tas\tthe\tMNIST\tclassifier\twe built\tin\tChapter\t10\texcept\tthat\tan\tunrolled\tRNN\treplaces\tthe\thidden\tlayers.\tNote\tthat\tthe\tfully\tconnected layer\tis\tconnected\tto\tthe\tstates\ttensor,\twhich\tcontains\tonly\tthe\tfinal\tstate\tof\tthe\tRNN\t(i.e.,\tthe\t28th output).\tAlso\tnote\tthat\ty\tis\ta\tplaceholder\tfor\tthe\ttarget\tclasses.\n\nn_steps\t=\t28 n_inputs\t=\t28 n_neurons\t=\t150 n_outputs\t=\t10\n\nlearning_rate\t=\t0.001\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_inputs]) y\t=\ttf.placeholder(tf.int32,\t[None])\n\nbasic_cell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons) outputs,\tstates\t=\ttf.nn.dynamic_rnn(basic_cell,\tX,\tdtype=tf.float32)\n\nlogits\t=\ttf.layers.dense(states,\tn_outputs) xentropy\t=\ttf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogits=logits) loss\t=\ttf.reduce_mean(xentropy) optimizer\t=\ttf.train.AdamOptimizer(learning_rate=learning_rate) training_op\t=\toptimizer.minimize(loss) correct\t=\ttf.nn.in_top_k(logits,\ty,\t1) accuracy\t=\ttf.reduce_mean(tf.cast(correct,\ttf.float32))\n\ninit\t=\ttf.global_variables_initializer()\n\nNow\tlet’s\tload\tthe\tMNIST\tdata\tand\treshape\tthe\ttest\tdata\tto\t[batch_size,\tn_steps,\tn_inputs]\tas\tis expected\tby\tthe\tnetwork.\tWe\twill\ttake\tcare\tof\treshaping\tthe\ttraining\tdata\tin\ta\tmoment.\n\nfrom\ttensorflow.examples.tutorials.mnist\timport\tinput_data\n\nmnist\t=\tinput_data.read_data_sets(\"/tmp/data/\") X_test\t=\tmnist.test.images.reshape((-1,\tn_steps,\tn_inputs)) y_test\t=\tmnist.test.labels\n\nNow\twe\tare\tready\tto\ttrain\tthe\tRNN.\tThe\texecution\tphase\tis\texactly\tthe\tsame\tas\tfor\tthe\tMNIST\tclassifier in\tChapter\t10,\texcept\tthat\twe\treshape\teach\ttraining\tbatch\tbefore\tfeeding\tit\tto\tthe\tnetwork.\n\nn_epochs\t=\t100 batch_size\t=\t150\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tfor\titeration\tin\trange(mnist.train.num_examples\t//\tbatch_size): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tX_batch\t=\tX_batch.reshape((-1,\tn_steps,\tn_inputs)) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\tacc_train\t=\taccuracy.eval(feed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\tacc_test\t=\taccuracy.eval(feed_dict={X:\tX_test,\ty:\ty_test}) \t\t\t\t\t\t\t\tprint(epoch,\t\"Train\taccuracy:\",\tacc_train,\t\"Test\taccuracy:\",\tacc_test)\n\nThe\toutput\tshould\tlook\tlike\tthis:\n\n0\tTrain\taccuracy:\t0.94\tTest\taccuracy:\t0.9308 1\tTrain\taccuracy:\t0.933333\tTest\taccuracy:\t0.9431 [...] 98\tTrain\taccuracy:\t0.98\tTest\taccuracy:\t0.9794 99\tTrain\taccuracy:\t1.0\tTest\taccuracy:\t0.9804\n\nWe\tget\tover\t98%\taccuracy\t—\tnot\tbad!\tPlus\tyou\twould\tcertainly\tget\ta\tbetter\tresult\tby\ttuning\tthe hyperparameters,\tinitializing\tthe\tRNN\tweights\tusing\tHe\tinitialization,\ttraining\tlonger,\tor\tadding\ta\tbit\tof regularization\t(e.g.,\tdropout).\n\nTIP\n\nYou\tcan\tspecify\tan\tinitializer\tfor\tthe\tRNN\tby\twrapping\tits\tconstruction\tcode\tin\ta\tvariable\tscope\t(e.g.,\tuse variable_scope(\"rnn\",\tinitializer=variance_scaling_initializer())\tto\tuse\tHe\tinitialization).\n\nTraining\tto\tPredict\tTime\tSeries Now\tlet’s\ttake\ta\tlook\tat\thow\tto\thandle\ttime\tseries,\tsuch\tas\tstock\tprices,\tair\ttemperature,\tbrain\twave patterns,\tand\tso\ton.\tIn\tthis\tsection\twe\twill\ttrain\tan\tRNN\tto\tpredict\tthe\tnext\tvalue\tin\ta\tgenerated\ttime series.\tEach\ttraining\tinstance\tis\ta\trandomly\tselected\tsequence\tof\t20\tconsecutive\tvalues\tfrom\tthe\ttime series,\tand\tthe\ttarget\tsequence\tis\tthe\tsame\tas\tthe\tinput\tsequence,\texcept\tit\tis\tshifted\tby\tone\ttime\tstep\tinto the\tfuture\t(see\tFigure\t14-7).\n\nFigure\t14-7.\tTime\tseries\t(left),\tand\ta\ttraining\tinstance\tfrom\tthat\tseries\t(right)\n\nFirst,\tlet’s\tcreate\tthe\tRNN.\tIt\twill\tcontain\t100\trecurrent\tneurons\tand\twe\twill\tunroll\tit\tover\t20\ttime\tsteps since\teach\ttraining\tinstance\twill\tbe\t20\tinputs\tlong.\tEach\tinput\twill\tcontain\tonly\tone\tfeature\t(the\tvalue\tat that\ttime).\tThe\ttargets\tare\talso\tsequences\tof\t20\tinputs,\teach\tcontaining\ta\tsingle\tvalue.\tThe\tcode\tis\talmost the\tsame\tas\tearlier:\n\nn_steps\t=\t20 n_inputs\t=\t1 n_neurons\t=\t100 n_outputs\t=\t1\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_inputs]) y\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_outputs]) cell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\tactivation=tf.nn.relu) outputs,\tstates\t=\ttf.nn.dynamic_rnn(cell,\tX,\tdtype=tf.float32)\n\nNOTE\n\nIn\tgeneral\tyou\twould\thave\tmore\tthan\tjust\tone\tinput\tfeature.\tFor\texample,\tif\tyou\twere\ttrying\tto\tpredict\tstock\tprices,\tyou\twould likely\thave\tmany\tother\tinput\tfeatures\tat\teach\ttime\tstep,\tsuch\tas\tprices\tof\tcompeting\tstocks,\tratings\tfrom\tanalysts,\tor\tany\tother feature\tthat\tmight\thelp\tthe\tsystem\tmake\tits\tpredictions.\n\nAt\teach\ttime\tstep\twe\tnow\thave\tan\toutput\tvector\tof\tsize\t100.\tBut\twhat\twe\tactually\twant\tis\ta\tsingle\toutput value\tat\teach\ttime\tstep.\tThe\tsimplest\tsolution\tis\tto\twrap\tthe\tcell\tin\tan\tOutputProjectionWrapper.\tA cell\twrapper\tacts\tlike\ta\tnormal\tcell,\tproxying\tevery\tmethod\tcall\tto\tan\tunderlying\tcell,\tbut\tit\talso\tadds some\tfunctionality.\tThe\tOutputProjectionWrapper\tadds\ta\tfully\tconnected\tlayer\tof\tlinear\tneurons\t(i.e., without\tany\tactivation\tfunction)\ton\ttop\tof\teach\toutput\t(but\tit\tdoes\tnot\taffect\tthe\tcell\tstate).\tAll\tthese\tfully connected\tlayers\tshare\tthe\tsame\t(trainable)\tweights\tand\tbias\tterms.\tThe\tresulting\tRNN\tis\trepresented\tin\n\nFigure\t14-8.\n\nFigure\t14-8.\tRNN\tcells\tusing\toutput\tprojections\n\nWrapping\ta\tcell\tis\tquite\teasy.\tLet’s\ttweak\tthe\tpreceding\tcode\tby\twrapping\tthe\tBasicRNNCell\tinto\tan OutputProjectionWrapper:\n\ncell\t=\ttf.contrib.rnn.OutputProjectionWrapper( \t\t\t\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\tactivation=tf.nn.relu), \t\t\t\toutput_size=n_outputs)\n\nSo\tfar,\tso\tgood.\tNow\twe\tneed\tto\tdefine\tthe\tcost\tfunction.\tWe\twill\tuse\tthe\tMean\tSquared\tError\t(MSE),\tas we\tdid\tin\tprevious\tregression\ttasks.\tNext\twe\twill\tcreate\tan\tAdam\toptimizer,\tthe\ttraining\top,\tand\tthe variable\tinitialization\top,\tas\tusual:\n\nlearning_rate\t=\t0.001\n\nloss\t=\ttf.reduce_mean(tf.square(outputs\t-\ty)) optimizer\t=\ttf.train.AdamOptimizer(learning_rate=learning_rate) training_op\t=\toptimizer.minimize(loss)\n\ninit\t=\ttf.global_variables_initializer()\n\nNow\ton\tto\tthe\texecution\tphase:\n\nn_iterations\t=\t1500 batch_size\t=\t50\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\titeration\tin\trange(n_iterations): \t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\t[...]\t\t#\tfetch\tthe\tnext\ttraining\tbatch \t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\tif\titeration\t%\t100\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tmse\t=\tloss.eval(feed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\t\t\t\t\tprint(iteration,\t\"\\tMSE:\",\tmse)\n\nThe\tprogram’s\toutput\tshould\tlook\tlike\tthis:\n\n0\t\t\t\t\t\t\tMSE:\t13.6543 100\t\t\t\t\tMSE:\t0.538476 200\t\t\t\t\tMSE:\t0.168532 300\t\t\t\t\tMSE:\t0.0879579 400\t\t\t\t\tMSE:\t0.0633425 [...]\n\nOnce\tthe\tmodel\tis\ttrained,\tyou\tcan\tmake\tpredictions:\n\nX_new\t=\t[...]\t\t#\tNew\tsequences y_pred\t=\tsess.run(outputs,\tfeed_dict={X:\tX_new})\n\nFigure\t14-9\tshows\tthe\tpredicted\tsequence\tfor\tthe\tinstance\twe\tlooked\tat\tearlier\t(in\tFigure\t14-7),\tafter\tjust 1,000\ttraining\titerations.\n\nFigure\t14-9.\tTime\tseries\tprediction\n\nAlthough\tusing\tan\tOutputProjectionWrapper\tis\tthe\tsimplest\tsolution\tto\treduce\tthe\tdimensionality\tof the\tRNN’s\toutput\tsequences\tdown\tto\tjust\tone\tvalue\tper\ttime\tstep\t(per\tinstance),\tit\tis\tnot\tthe\tmost efficient.\tThere\tis\ta\ttrickier\tbut\tmore\tefficient\tsolution:\tyou\tcan\treshape\tthe\tRNN\toutputs\tfrom [batch_size,\tn_steps,\tn_neurons]\tto\t[batch_size\t*\tn_steps,\tn_neurons],\tthen\tapply\ta\tsingle fully\tconnected\tlayer\twith\tthe\tappropriate\toutput\tsize\t(in\tour\tcase\tjust\t1),\twhich\twill\tresult\tin\tan\toutput tensor\tof\tshape\t[batch_size\t*\tn_steps,\tn_outputs],\tand\tthen\treshape\tthis\ttensor\tto\t[batch_size, n_steps,\tn_outputs].\tThese\toperations\tare\trepresented\tin\tFigure\t14-10.\n\nFigure\t14-10.\tStack\tall\tthe\toutputs,\tapply\tthe\tprojection,\tthen\tunstack\tthe\tresult\n\nTo\timplement\tthis\tsolution,\twe\tfirst\trevert\tto\ta\tbasic\tcell,\twithout\tthe\tOutputProjectionWrapper:\n\ncell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\tactivation=tf.nn.relu) rnn_outputs,\tstates\t=\ttf.nn.dynamic_rnn(cell,\tX,\tdtype=tf.float32)\n\nThen\twe\tstack\tall\tthe\toutputs\tusing\tthe\treshape()\toperation,\tapply\tthe\tfully\tconnected\tlinear\tlayer (without\tusing\tany\tactivation\tfunction;\tthis\tis\tjust\ta\tprojection),\tand\tfinally\tunstack\tall\tthe\toutputs,\tagain using\treshape():\n\nstacked_rnn_outputs\t=\ttf.reshape(rnn_outputs,\t[-1,\tn_neurons]) stacked_outputs\t=\ttf.layers.dense(stacked_rnn_outputs,\tn_outputs) outputs\t=\ttf.reshape(stacked_outputs,\t[-1,\tn_steps,\tn_outputs])\n\nThe\trest\tof\tthe\tcode\tis\tthe\tsame\tas\tearlier.\tThis\tcan\tprovide\ta\tsignificant\tspeed\tboost\tsince\tthere\tis\tjust one\tfully\tconnected\tlayer\tinstead\tof\tone\tper\ttime\tstep.\n\nCreative\tRNN Now\tthat\twe\thave\ta\tmodel\tthat\tcan\tpredict\tthe\tfuture,\twe\tcan\tuse\tit\tto\tgenerate\tsome\tcreative\tsequences, as\texplained\tat\tthe\tbeginning\tof\tthe\tchapter.\tAll\twe\tneed\tis\tto\tprovide\tit\ta\tseed\tsequence\tcontaining n_steps\tvalues\t(e.g.,\tfull\tof\tzeros),\tuse\tthe\tmodel\tto\tpredict\tthe\tnext\tvalue,\tappend\tthis\tpredicted\tvalue to\tthe\tsequence,\tfeed\tthe\tlast\tn_steps\tvalues\tto\tthe\tmodel\tto\tpredict\tthe\tnext\tvalue,\tand\tso\ton.\tThis process\tgenerates\ta\tnew\tsequence\tthat\thas\tsome\tresemblance\tto\tthe\toriginal\ttime\tseries\t(see\tFigure\t14- 11).\n\nsequence\t=\t[0.]\t*\tn_steps for\titeration\tin\trange(300): \t\t\t\tX_batch\t=\tnp.array(sequence[-n_steps:]).reshape(1,\tn_steps,\t1) \t\t\t\ty_pred\t=\tsess.run(outputs,\tfeed_dict={X:\tX_batch}) \t\t\t\tsequence.append(y_pred[0,\t-1,\t0])\n\nFigure\t14-11.\tCreative\tsequences,\tseeded\twith\tzeros\t(left)\tor\twith\tan\tinstance\t(right)\n\nNow\tyou\tcan\ttry\tto\tfeed\tall\tyour\tJohn\tLennon\talbums\tto\tan\tRNN\tand\tsee\tif\tit\tcan\tgenerate\tthe\tnext “Imagine.”\tHowever,\tyou\twill\tprobably\tneed\ta\tmuch\tmore\tpowerful\tRNN,\twith\tmore\tneurons,\tand\talso much\tdeeper.\tLet’s\tlook\tat\tdeep\tRNNs\tnow.\n\nDeep\tRNNs It\tis\tquite\tcommon\tto\tstack\tmultiple\tlayers\tof\tcells,\tas\tshown\tin\tFigure\t14-12.\tThis\tgives\tyou\ta\tdeep RNN.\n\nTo\timplement\ta\tdeep\tRNN\tin\tTensorFlow,\tyou\tcan\tcreate\tseveral\tcells\tand\tstack\tthem\tinto\ta MultiRNNCell.\tIn\tthe\tfollowing\tcode\twe\tstack\tthree\tidentical\tcells\t(but\tyou\tcould\tvery\twell\tuse\tvarious kinds\tof\tcells\twith\ta\tdifferent\tnumber\tof\tneurons):\n\nn_neurons\t=\t100 n_layers\t=\t3\n\nlayers\t=\t[tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu) \t\t\t\t\t\t\t\t\t\tfor\tlayer\tin\trange(n_layers)] multi_layer_cell\t=\ttf.contrib.rnn.MultiRNNCell(layers) outputs,\tstates\t=\ttf.nn.dynamic_rnn(multi_layer_cell,\tX,\tdtype=tf.float32)\n\nFigure\t14-12.\tDeep\tRNN\t(left),\tunrolled\tthrough\ttime\t(right)\n\nThat’s\tall\tthere\tis\tto\tit!\tThe\tstates\tvariable\tis\ta\ttuple\tcontaining\tone\ttensor\tper\tlayer,\teach\trepresenting the\tfinal\tstate\tof\tthat\tlayer’s\tcell\t(with\tshape\t[batch_size,\tn_neurons]).\tIf\tyou\tset state_is_tuple=False\twhen\tcreating\tthe\tMultiRNNCell,\tthen\tstates\tbecomes\ta\tsingle\ttensor containing\tthe\tstates\tfrom\tevery\tlayer,\tconcatenated\talong\tthe\tcolumn\taxis\t(i.e.,\tits\tshape\tis [batch_size,\tn_layers\t*\tn_neurons]).\tNote\tthat\tbefore\tTensorFlow\t0.11.0,\tthis\tbehavior\twas\tthe default.\n\nDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs Chapter\t12\tpointed\tout\tthat\twe\tcan\tefficiently\tdistribute\tdeep\tRNNs\tacross\tmultiple\tGPUs\tby\tpinning each\tlayer\tto\ta\tdifferent\tGPU\t(see\tFigure\t12-16).\tHowever,\tif\tyou\ttry\tto\tcreate\teach\tcell\tin\ta\tdifferent device()\tblock,\tit\twill\tnot\twork:\n\nwith\ttf.device(\"/gpu:0\"):\t\t#\tBAD!\tThis\tis\tignored. \t\t\t\tlayer1\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n\nwith\ttf.device(\"/gpu:1\"):\t\t#\tBAD!\tIgnored\tagain. \t\t\t\tlayer2\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n\nThis\tfails\tbecause\ta\tBasicRNNCell\tis\ta\tcell\tfactory,\tnot\ta\tcell\tper\tse\t(as\tmentioned\tearlier);\tno\tcells\tget created\twhen\tyou\tcreate\tthe\tfactory,\tand\tthus\tno\tvariables\tdo\teither.\tThe\tdevice\tblock\tis\tsimply\tignored. The\tcells\tactually\tget\tcreated\tlater.\tWhen\tyou\tcall\tdynamic_rnn(),\tit\tcalls\tthe\tMultiRNNCell,\twhich calls\teach\tindividual\tBasicRNNCell,\twhich\tcreate\tthe\tactual\tcells\t(including\ttheir\tvariables). Unfortunately,\tnone\tof\tthese\tclasses\tprovide\tany\tway\tto\tcontrol\tthe\tdevices\ton\twhich\tthe\tvariables\tget created.\tIf\tyou\ttry\tto\tput\tthe\tdynamic_rnn()\tcall\twithin\ta\tdevice\tblock,\tthe\twhole\tRNN\tgets\tpinned\tto\ta single\tdevice.\tSo\tare\tyou\tstuck?\tFortunately\tnot!\tThe\ttrick\tis\tto\tcreate\tyour\town\tcell\twrapper:\n\nimport\ttensorflow\tas\ttf\n\nclass\tDeviceCellWrapper(tf.contrib.rnn.RNNCell): \t\tdef\t__init__(self,\tdevice,\tcell): \t\t\t\tself._cell\t=\tcell \t\t\t\tself._device\t=\tdevice\n\n@property \t\tdef\tstate_size(self): \t\t\t\treturn\tself._cell.state_size\n\n@property \t\tdef\toutput_size(self): \t\t\t\treturn\tself._cell.output_size\n\ndef\t__call__(self,\tinputs,\tstate,\tscope=None): \t\t\t\twith\ttf.device(self._device): \t\t\t\t\t\t\t\treturn\tself._cell(inputs,\tstate,\tscope)\n\nThis\twrapper\tsimply\tproxies\tevery\tmethod\tcall\tto\tanother\tcell,\texcept\tit\twraps\tthe\t__call__()\tfunction within\ta\tdevice\tblock.2\tNow\tyou\tcan\tdistribute\teach\tlayer\ton\ta\tdifferent\tGPU:\n\ndevices\t=\t[\"/gpu:0\",\t\"/gpu:1\",\t\"/gpu:2\"] cells\t=\t[DeviceCellWrapper(dev,tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)) \t\t\t\t\t\t\t\t\tfor\tdev\tin\tdevices] multi_layer_cell\t=\ttf.contrib.rnn.MultiRNNCell(cells) outputs,\tstates\t=\ttf.nn.dynamic_rnn(multi_layer_cell,\tX,\tdtype=tf.float32)\n\nWARNING\n\nDo\tnot\tset\tstate_is_tuple=False,\tor\tthe\tMultiRNNCell\twill\tconcatenate\tall\tthe\tcell\tstates\tinto\ta\tsingle\ttensor,\ton\ta\tsingle\tGPU.\n\nApplying\tDropout If\tyou\tbuild\ta\tvery\tdeep\tRNN,\tit\tmay\tend\tup\toverfitting\tthe\ttraining\tset.\tTo\tprevent\tthat,\ta\tcommon technique\tis\tto\tapply\tdropout\t(introduced\tin\tChapter\t11).\tYou\tcan\tsimply\tadd\ta\tdropout\tlayer\tbefore\tor after\tthe\tRNN\tas\tusual,\tbut\tif\tyou\talso\twant\tto\tapply\tdropout\tbetween\tthe\tRNN\tlayers,\tyou\tneed\tto\tuse\ta DropoutWrapper.\tThe\tfollowing\tcode\tapplies\tdropout\tto\tthe\tinputs\tof\teach\tlayer\tin\tthe\tRNN,\tdropping each\tinput\twith\ta\t50%\tprobability:\n\nkeep_prob\t=\t0.5\n\ncells\t=\t[tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) \t\t\t\t\t\t\t\t\tfor\tlayer\tin\trange(n_layers)] cells_drop\t=\t[tf.contrib.rnn.DropoutWrapper(cell,\tinput_keep_prob=keep_prob) \t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tcell\tin\tcells] multi_layer_cell\t=\ttf.contrib.rnn.MultiRNNCell(cells_drop) rnn_outputs,\tstates\t=\ttf.nn.dynamic_rnn(multi_layer_cell,\tX,\tdtype=tf.float32)\n\nNote\tthat\tit\tis\talso\tpossible\tto\tapply\tdropout\tto\tthe\toutputs\tby\tsetting\toutput_keep_prob.\n\nThe\tmain\tproblem\twith\tthis\tcode\tis\tthat\tit\twill\tapply\tdropout\tnot\tonly\tduring\ttraining\tbut\talso\tduring testing,\twhich\tis\tnot\twhat\tyou\twant\t(recall\tthat\tdropout\tshould\tbe\tapplied\tonly\tduring\ttraining). Unfortunately,\tthe\tDropoutWrapper\tdoes\tnot\tsupport\ta\ttraining\tplaceholder\t(yet?),\tso\tyou\tmust\teither write\tyour\town\tdropout\twrapper\tclass,\tor\thave\ttwo\tdifferent\tgraphs:\tone\tfor\ttraining,\tand\tthe\tother\tfor testing.\tThe\tsecond\toption\tlooks\tlike\tthis:\n\nimport\tsys training\t=\t(sys.argv[-1]\t==\t\"train\")\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_inputs]) y\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_outputs]) cells\t=\t[tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) \t\t\t\t\t\t\t\t\tfor\tlayer\tin\trange(n_layers)] if\ttraining: \t\t\t\tcells\t=\t[tf.contrib.rnn.DropoutWrapper(cell,\tinput_keep_prob=keep_prob) \t\t\t\t\t\t\t\t\t\t\t\t\tfor\tcell\tin\tcells] multi_layer_cell\t=\ttf.contrib.rnn.MultiRNNCell(cells) rnn_outputs,\tstates\t=\ttf.nn.dynamic_rnn(multi_layer_cell,\tX,\tdtype=tf.float32) [...]\t#\tbuild\tthe\trest\tof\tthe\tgraph\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tif\ttraining: \t\t\t\t\t\t\t\tinit.run() \t\t\t\t\t\t\t\tfor\titeration\tin\trange(n_iterations): \t\t\t\t\t\t\t\t\t\t\t\t[...]\t#\ttrain\tthe\tmodel \t\t\t\t\t\t\t\tsave_path\t=\tsaver.save(sess,\t\"/tmp/my_model.ckpt\") \t\t\t\telse: \t\t\t\t\t\t\t\tsaver.restore(sess,\t\"/tmp/my_model.ckpt\") \t\t\t\t\t\t\t\t[...]\t#\tuse\tthe\tmodel\n\nWith\tthat\tyou\tshould\tbe\table\tto\ttrain\tall\tsorts\tof\tRNNs!\tUnfortunately,\tif\tyou\twant\tto\ttrain\tan\tRNN\ton\tlong sequences,\tthings\twill\tget\ta\tbit\tharder.\tLet’s\tsee\twhy\tand\twhat\tyou\tcan\tdo\tabout\tit.\n\nThe\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps To\ttrain\tan\tRNN\ton\tlong\tsequences,\tyou\twill\tneed\tto\trun\tit\tover\tmany\ttime\tsteps,\tmaking\tthe\tunrolled RNN\ta\tvery\tdeep\tnetwork.\tJust\tlike\tany\tdeep\tneural\tnetwork\tit\tmay\tsuffer\tfrom\tthe\tvanishing/exploding gradients\tproblem\t(discussed\tin\tChapter\t11)\tand\ttake\tforever\tto\ttrain.\tMany\tof\tthe\ttricks\twe\tdiscussed\tto alleviate\tthis\tproblem\tcan\tbe\tused\tfor\tdeep\tunrolled\tRNNs\tas\twell:\tgood\tparameter\tinitialization, nonsaturating\tactivation\tfunctions\t(e.g.,\tReLU),\tBatch\tNormalization,\tGradient\tClipping,\tand\tfaster optimizers.\tHowever,\tif\tthe\tRNN\tneeds\tto\thandle\teven\tmoderately\tlong\tsequences\t(e.g.,\t100\tinputs),\tthen training\twill\tstill\tbe\tvery\tslow.\n\nThe\tsimplest\tand\tmost\tcommon\tsolution\tto\tthis\tproblem\tis\tto\tunroll\tthe\tRNN\tonly\tover\ta\tlimited\tnumber of\ttime\tsteps\tduring\ttraining.\tThis\tis\tcalled\ttruncated\tbackpropagation\tthrough\ttime.\tIn\tTensorFlow\tyou can\timplement\tit\tsimply\tby\ttruncating\tthe\tinput\tsequences.\tFor\texample,\tin\tthe\ttime\tseries\tprediction problem,\tyou\twould\tsimply\treduce\tn_steps\tduring\ttraining.\tThe\tproblem,\tof\tcourse,\tis\tthat\tthe\tmodel will\tnot\tbe\table\tto\tlearn\tlong-term\tpatterns.\tOne\tworkaround\tcould\tbe\tto\tmake\tsure\tthat\tthese\tshortened sequences\tcontain\tboth\told\tand\trecent\tdata,\tso\tthat\tthe\tmodel\tcan\tlearn\tto\tuse\tboth\t(e.g.,\tthe\tsequence could\tcontain\tmonthly\tdata\tfor\tthe\tlast\tfive\tmonths,\tthen\tweekly\tdata\tfor\tthe\tlast\tfive\tweeks,\tthen\tdaily data\tover\tthe\tlast\tfive\tdays).\tBut\tthis\tworkaround\thas\tits\tlimits:\twhat\tif\tfine-grained\tdata\tfrom\tlast\tyear\tis actually\tuseful?\tWhat\tif\tthere\twas\ta\tbrief\tbut\tsignificant\tevent\tthat\tabsolutely\tmust\tbe\ttaken\tinto\taccount, even\tyears\tlater\t(e.g.,\tthe\tresult\tof\tan\telection)?\n\nBesides\tthe\tlong\ttraining\ttime,\ta\tsecond\tproblem\tfaced\tby\tlong-running\tRNNs\tis\tthe\tfact\tthat\tthe\tmemory of\tthe\tfirst\tinputs\tgradually\tfades\taway.\tIndeed,\tdue\tto\tthe\ttransformations\tthat\tthe\tdata\tgoes\tthrough\twhen traversing\tan\tRNN,\tsome\tinformation\tis\tlost\tafter\teach\ttime\tstep.\tAfter\ta\twhile,\tthe\tRNN’s\tstate\tcontains virtually\tno\ttrace\tof\tthe\tfirst\tinputs.\tThis\tcan\tbe\ta\tshowstopper.\tFor\texample,\tsay\tyou\twant\tto\tperform sentiment\tanalysis\ton\ta\tlong\treview\tthat\tstarts\twith\tthe\tfour\twords\t“I\tloved\tthis\tmovie,”\tbut\tthe\trest\tof\tthe review\tlists\tthe\tmany\tthings\tthat\tcould\thave\tmade\tthe\tmovie\teven\tbetter.\tIf\tthe\tRNN\tgradually\tforgets\tthe first\tfour\twords,\tit\twill\tcompletely\tmisinterpret\tthe\treview.\tTo\tsolve\tthis\tproblem,\tvarious\ttypes\tof\tcells with\tlong-term\tmemory\thave\tbeen\tintroduced.\tThey\thave\tproved\tso\tsuccessful\tthat\tthe\tbasic\tcells\tare\tnot much\tused\tanymore.\tLet’s\tfirst\tlook\tat\tthe\tmost\tpopular\tof\tthese\tlong\tmemory\tcells:\tthe\tLSTM\tcell.\n\nLSTM\tCell The\tLong\tShort-Term\tMemory\t(LSTM)\tcell\twas\tproposed\tin\t19973\tby\tSepp\tHochreiter\tand\tJürgen Schmidhuber,\tand\tit\twas\tgradually\timproved\tover\tthe\tyears\tby\tseveral\tresearchers,\tsuch\tas\tAlex\tGraves, Haşim\tSak,4\tWojciech\tZaremba,5\tand\tmany\tmore.\tIf\tyou\tconsider\tthe\tLSTM\tcell\tas\ta\tblack\tbox,\tit\tcan\tbe used\tvery\tmuch\tlike\ta\tbasic\tcell,\texcept\tit\twill\tperform\tmuch\tbetter;\ttraining\twill\tconverge\tfaster\tand\tit will\tdetect\tlong-term\tdependencies\tin\tthe\tdata.\tIn\tTensorFlow,\tyou\tcan\tsimply\tuse\ta\tBasicLSTMCell instead\tof\ta\tBasicRNNCell:\n\nlstm_cell\t=\ttf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)\n\nLSTM\tcells\tmanage\ttwo\tstate\tvectors,\tand\tfor\tperformance\treasons\tthey\tare\tkept\tseparate\tby\tdefault.\tYou can\tchange\tthis\tdefault\tbehavior\tby\tsetting\tstate_is_tuple=False\twhen\tcreating\tthe\tBasicLSTMCell.\n\nSo\thow\tdoes\tan\tLSTM\tcell\twork?\tThe\tarchitecture\tof\ta\tbasic\tLSTM\tcell\tis\tshown\tin\tFigure\t14-13.\n\nFigure\t14-13.\tLSTM\tcell\n\nIf\tyou\tdon’t\tlook\tat\twhat’s\tinside\tthe\tbox,\tthe\tLSTM\tcell\tlooks\texactly\tlike\ta\tregular\tcell,\texcept\tthat\tits state\tis\tsplit\tin\ttwo\tvectors:\th(t)\tand\tc(t)\t(“c”\tstands\tfor\t“cell”).\tYou\tcan\tthink\tof\th(t)\tas\tthe\tshort-term\tstate and\tc(t)\tas\tthe\tlong-term\tstate.\n\nNow\tlet’s\topen\tthe\tbox!\tThe\tkey\tidea\tis\tthat\tthe\tnetwork\tcan\tlearn\twhat\tto\tstore\tin\tthe\tlong-term\tstate, what\tto\tthrow\taway,\tand\twhat\tto\tread\tfrom\tit.\tAs\tthe\tlong-term\tstate\tc(t–1)\ttraverses\tthe\tnetwork\tfrom\tleft to\tright,\tyou\tcan\tsee\tthat\tit\tfirst\tgoes\tthrough\ta\tforget\tgate,\tdropping\tsome\tmemories,\tand\tthen\tit\tadds some\tnew\tmemories\tvia\tthe\taddition\toperation\t(which\tadds\tthe\tmemories\tthat\twere\tselected\tby\tan\tinput gate).\tThe\tresult\tc(t)\tis\tsent\tstraight\tout,\twithout\tany\tfurther\ttransformation.\tSo,\tat\teach\ttime\tstep,\tsome memories\tare\tdropped\tand\tsome\tmemories\tare\tadded.\tMoreover,\tafter\tthe\taddition\toperation,\tthe\tlong- term\tstate\tis\tcopied\tand\tpassed\tthrough\tthe\ttanh\tfunction,\tand\tthen\tthe\tresult\tis\tfiltered\tby\tthe\toutput\tgate.\n\nThis\tproduces\tthe\tshort-term\tstate\th(t)\t(which\tis\tequal\tto\tthe\tcell’s\toutput\tfor\tthis\ttime\tstep\ty(t)).\tNow\tlet’s look\tat\twhere\tnew\tmemories\tcome\tfrom\tand\thow\tthe\tgates\twork.\n\nFirst,\tthe\tcurrent\tinput\tvector\tx(t)\tand\tthe\tprevious\tshort-term\tstate\th(t–1)\tare\tfed\tto\tfour\tdifferent\tfully connected\tlayers.\tThey\tall\tserve\ta\tdifferent\tpurpose:\n\nThe\tmain\tlayer\tis\tthe\tone\tthat\toutputs\tg(t).\tIt\thas\tthe\tusual\trole\tof\tanalyzing\tthe\tcurrent\tinputs\tx(t)\tand the\tprevious\t(short-term)\tstate\th(t–1).\tIn\ta\tbasic\tcell,\tthere\tis\tnothing\telse\tthan\tthis\tlayer,\tand\tits output\tgoes\tstraight\tout\tto\ty(t)\tand\th(t).\tIn\tcontrast,\tin\tan\tLSTM\tcell\tthis\tlayer’s\toutput\tdoes\tnot\tgo straight\tout,\tbut\tinstead\tit\tis\tpartially\tstored\tin\tthe\tlong-term\tstate.\n\nThe\tthree\tother\tlayers\tare\tgate\tcontrollers.\tSince\tthey\tuse\tthe\tlogistic\tactivation\tfunction,\ttheir outputs\trange\tfrom\t0\tto\t1.\tAs\tyou\tcan\tsee,\ttheir\toutputs\tare\tfed\tto\telement-wise\tmultiplication operations,\tso\tif\tthey\toutput\t0s,\tthey\tclose\tthe\tgate,\tand\tif\tthey\toutput\t1s,\tthey\topen\tit.\tSpecifically:\n\nThe\tforget\tgate\t(controlled\tby\tf(t))\tcontrols\twhich\tparts\tof\tthe\tlong-term\tstate\tshould\tbe\terased.\n\nThe\tinput\tgate\t(controlled\tby\ti(t))\tcontrols\twhich\tparts\tof\tg(t)\tshould\tbe\tadded\tto\tthe\tlong-term state\t(this\tis\twhy\twe\tsaid\tit\twas\tonly\t“partially\tstored”).\n\nFinally,\tthe\toutput\tgate\t(controlled\tby\to(t))\tcontrols\twhich\tparts\tof\tthe\tlong-term\tstate\tshould\tbe read\tand\toutput\tat\tthis\ttime\tstep\t(both\tto\th(t))\tand\ty(t).\n\nIn\tshort,\tan\tLSTM\tcell\tcan\tlearn\tto\trecognize\tan\timportant\tinput\t(that’s\tthe\trole\tof\tthe\tinput\tgate),\tstore\tit in\tthe\tlong-term\tstate,\tlearn\tto\tpreserve\tit\tfor\tas\tlong\tas\tit\tis\tneeded\t(that’s\tthe\trole\tof\tthe\tforget\tgate),\tand learn\tto\textract\tit\twhenever\tit\tis\tneeded.\tThis\texplains\twhy\tthey\thave\tbeen\tamazingly\tsuccessful\tat capturing\tlong-term\tpatterns\tin\ttime\tseries,\tlong\ttexts,\taudio\trecordings,\tand\tmore.\n\nEquation\t14-3\tsummarizes\thow\tto\tcompute\tthe\tcell’s\tlong-term\tstate,\tits\tshort-term\tstate,\tand\tits\toutput\tat each\ttime\tstep\tfor\ta\tsingle\tinstance\t(the\tequations\tfor\ta\twhole\tmini-batch\tare\tvery\tsimilar).\n\nEquation\t14-3.\tLSTM\tcomputations\n\nWxi,\tWxf,\tWxo,\tWxg\tare\tthe\tweight\tmatrices\tof\teach\tof\tthe\tfour\tlayers\tfor\ttheir\tconnection\tto\tthe\n\ninput\tvector\tx(t).\n\nWhi,\tWhf,\tWho,\tand\tWhg\tare\tthe\tweight\tmatrices\tof\teach\tof\tthe\tfour\tlayers\tfor\ttheir\tconnection\tto\tthe previous\tshort-term\tstate\th(t–1).\n\nbi,\tbf,\tbo,\tand\tbg\tare\tthe\tbias\tterms\tfor\teach\tof\tthe\tfour\tlayers.\tNote\tthat\tTensorFlow\tinitializes\tbf\tto a\tvector\tfull\tof\t1s\tinstead\tof\t0s.\tThis\tprevents\tforgetting\teverything\tat\tthe\tbeginning\tof\ttraining.\n\nPeephole\tConnections In\ta\tbasic\tLSTM\tcell,\tthe\tgate\tcontrollers\tcan\tlook\tonly\tat\tthe\tinput\tx(t)\tand\tthe\tprevious\tshort-term\tstate h(t–1).\tIt\tmay\tbe\ta\tgood\tidea\tto\tgive\tthem\ta\tbit\tmore\tcontext\tby\tletting\tthem\tpeek\tat\tthe\tlong-term\tstate\tas well.\tThis\tidea\twas\tproposed\tby\tFelix\tGers\tand\tJürgen\tSchmidhuber\tin\t2000.6\tThey\tproposed\tan\tLSTM variant\twith\textra\tconnections\tcalled\tpeephole\tconnections:\tthe\tprevious\tlong-term\tstate\tc(t–1)\tis\tadded as\tan\tinput\tto\tthe\tcontrollers\tof\tthe\tforget\tgate\tand\tthe\tinput\tgate,\tand\tthe\tcurrent\tlong-term\tstate\tc(t)\tis added\tas\tinput\tto\tthe\tcontroller\tof\tthe\toutput\tgate.\n\nTo\timplement\tpeephole\tconnections\tin\tTensorFlow,\tyou\tmust\tuse\tthe\tLSTMCell\tinstead\tof\tthe BasicLSTMCell\tand\tset\tuse_peepholes=True:\n\nlstm_cell\t=\ttf.contrib.rnn.LSTMCell(num_units=n_neurons,\tuse_peepholes=True)\n\nThere\tare\tmany\tother\tvariants\tof\tthe\tLSTM\tcell.\tOne\tparticularly\tpopular\tvariant\tis\tthe\tGRU\tcell,\twhich we\twill\tlook\tat\tnow.\n\nGRU\tCell The\tGated\tRecurrent\tUnit\t(GRU)\tcell\t(see\tFigure\t14-14)\twas\tproposed\tby\tKyunghyun\tCho\tet\tal.\tin\ta 2014\tpaper7\tthat\talso\tintroduced\tthe\tEncoder–Decoder\tnetwork\twe\tmentioned\tearlier.\n\nFigure\t14-14.\tGRU\tcell\n\nThe\tGRU\tcell\tis\ta\tsimplified\tversion\tof\tthe\tLSTM\tcell,\tand\tit\tseems\tto\tperform\tjust\tas\twell8\t(which explains\tits\tgrowing\tpopularity).\tThe\tmain\tsimplifications\tare:\n\nBoth\tstate\tvectors\tare\tmerged\tinto\ta\tsingle\tvector\th(t).\n\nA\tsingle\tgate\tcontroller\tcontrols\tboth\tthe\tforget\tgate\tand\tthe\tinput\tgate.\tIf\tthe\tgate\tcontroller\toutputs\ta 1,\tthe\tinput\tgate\tis\topen\tand\tthe\tforget\tgate\tis\tclosed.\tIf\tit\toutputs\ta\t0,\tthe\topposite\thappens.\tIn\tother words,\twhenever\ta\tmemory\tmust\tbe\tstored,\tthe\tlocation\twhere\tit\twill\tbe\tstored\tis\terased\tfirst.\tThis is\tactually\ta\tfrequent\tvariant\tto\tthe\tLSTM\tcell\tin\tand\tof\titself.\n\nThere\tis\tno\toutput\tgate;\tthe\tfull\tstate\tvector\tis\toutput\tat\tevery\ttime\tstep.\tHowever,\tthere\tis\ta\tnew\tgate controller\tthat\tcontrols\twhich\tpart\tof\tthe\tprevious\tstate\twill\tbe\tshown\tto\tthe\tmain\tlayer.\n\nEquation\t14-4\tsummarizes\thow\tto\tcompute\tthe\tcell’s\tstate\tat\teach\ttime\tstep\tfor\ta\tsingle\tinstance.\n\nEquation\t14-4.\tGRU\tcomputations\n\nCreating\ta\tGRU\tcell\tin\tTensorFlow\tis\ttrivial:\n\ngru_cell\t=\ttf.contrib.rnn.GRUCell(num_units=n_neurons)\n\nLSTM\tor\tGRU\tcells\tare\tone\tof\tthe\tmain\treasons\tbehind\tthe\tsuccess\tof\tRNNs\tin\trecent\tyears,\tin\tparticular for\tapplications\tin\tnatural\tlanguage\tprocessing\t(NLP).\n\nNatural\tLanguage\tProcessing Most\tof\tthe\tstate-of-the-art\tNLP\tapplications,\tsuch\tas\tmachine\ttranslation,\tautomatic\tsummarization, parsing,\tsentiment\tanalysis,\tand\tmore,\tare\tnow\tbased\t(at\tleast\tin\tpart)\ton\tRNNs.\tIn\tthis\tlast\tsection,\twe will\ttake\ta\tquick\tlook\tat\twhat\ta\tmachine\ttranslation\tmodel\tlooks\tlike.\tThis\ttopic\tis\tvery\twell\tcovered\tby TensorFlow’s\tawesome\tWord2Vec\tand\tSeq2Seq\ttutorials,\tso\tyou\tshould\tdefinitely\tcheck\tthem\tout.\n\nWord\tEmbeddings Before\twe\tstart,\twe\tneed\tto\tchoose\ta\tword\trepresentation.\tOne\toption\tcould\tbe\tto\trepresent\teach\tword using\ta\tone-hot\tvector.\tSuppose\tyour\tvocabulary\tcontains\t50,000\twords,\tthen\tthe\tnth\tword\twould\tbe represented\tas\ta\t50,000-dimensional\tvector,\tfull\tof\t0s\texcept\tfor\ta\t1\tat\tthe\tnth\tposition.\tHowever,\twith such\ta\tlarge\tvocabulary,\tthis\tsparse\trepresentation\twould\tnot\tbe\tefficient\tat\tall.\tIdeally,\tyou\twant\tsimilar words\tto\thave\tsimilar\trepresentations,\tmaking\tit\teasy\tfor\tthe\tmodel\tto\tgeneralize\twhat\tit\tlearns\tabout\ta word\tto\tall\tsimilar\twords.\tFor\texample,\tif\tthe\tmodel\tis\ttold\tthat\t“I\tdrink\tmilk”\tis\ta\tvalid\tsentence,\tand\tif it\tknows\tthat\t“milk”\tis\tclose\tto\t“water”\tbut\tfar\tfrom\t“shoes,”\tthen\tit\twill\tknow\tthat\t“I\tdrink\twater”\tis probably\ta\tvalid\tsentence\tas\twell,\twhile\t“I\tdrink\tshoes”\tis\tprobably\tnot.\tBut\thow\tcan\tyou\tcome\tup\twith such\ta\tmeaningful\trepresentation?\n\nThe\tmost\tcommon\tsolution\tis\tto\trepresent\teach\tword\tin\tthe\tvocabulary\tusing\ta\tfairly\tsmall\tand\tdense vector\t(e.g.,\t150\tdimensions),\tcalled\tan\tembedding,\tand\tjust\tlet\tthe\tneural\tnetwork\tlearn\ta\tgood embedding\tfor\teach\tword\tduring\ttraining.\tAt\tthe\tbeginning\tof\ttraining,\tembeddings\tare\tsimply\tchosen randomly,\tbut\tduring\ttraining,\tbackpropagation\tautomatically\tmoves\tthe\tembeddings\taround\tin\ta\tway\tthat helps\tthe\tneural\tnetwork\tperform\tits\ttask.\tTypically\tthis\tmeans\tthat\tsimilar\twords\twill\tgradually\tcluster close\tto\tone\tanother,\tand\teven\tend\tup\torganized\tin\ta\trather\tmeaningful\tway.\tFor\texample,\tembeddings may\tend\tup\tplaced\talong\tvarious\taxes\tthat\trepresent\tgender,\tsingular/plural,\tadjective/noun,\tand\tso\ton. The\tresult\tcan\tbe\ttruly\tamazing.9\n\nIn\tTensorFlow,\tyou\tfirst\tneed\tto\tcreate\tthe\tvariable\trepresenting\tthe\tembeddings\tfor\tevery\tword\tin\tyour vocabulary\t(initialized\trandomly):\n\nvocabulary_size\t=\t50000 embedding_size\t=\t150\n\ninit_embeds\t=\ttf.random_uniform([vocabulary_size,\tembedding_size],\t-1.0,\t1.0) embeddings\t=\ttf.Variable(init_embeds)\n\nNow\tsuppose\tyou\twant\tto\tfeed\tthe\tsentence\t“I\tdrink\tmilk”\tto\tyour\tneural\tnetwork.\tYou\tshould\tfirst preprocess\tthe\tsentence\tand\tbreak\tit\tinto\ta\tlist\tof\tknown\twords.\tFor\texample\tyou\tmay\tremove unnecessary\tcharacters,\treplace\tunknown\twords\tby\ta\tpredefined\ttoken\tword\tsuch\tas\t“[UNK]”,\treplace numerical\tvalues\tby\t“[NUM]”,\treplace\tURLs\tby\t“[URL]”,\tand\tso\ton.\tOnce\tyou\thave\ta\tlist\tof\tknown words,\tyou\tcan\tlook\tup\teach\tword’s\tinteger\tidentifier\t(from\t0\tto\t49999)\tin\ta\tdictionary,\tfor\texample\t[72, 3335,\t288].\tAt\tthat\tpoint,\tyou\tare\tready\tto\tfeed\tthese\tword\tidentifiers\tto\tTensorFlow\tusing\ta\tplaceholder, and\tapply\tthe\tembedding_lookup()\tfunction\tto\tget\tthe\tcorresponding\tembeddings:\n\ntrain_inputs\t=\ttf.placeholder(tf.int32,\tshape=[None])\t\t#\tfrom\tids... embed\t=\ttf.nn.embedding_lookup(embeddings,\ttrain_inputs)\t\t#\t...to\tembeddings\n\nOnce\tyour\tmodel\thas\tlearned\tgood\tword\tembeddings,\tthey\tcan\tactually\tbe\treused\tfairly\tefficiently\tin\tany NLP\tapplication:\tafter\tall,\t“milk”\tis\tstill\tclose\tto\t“water”\tand\tfar\tfrom\t“shoes”\tno\tmatter\twhat\tyour application\tis.\tIn\tfact,\tinstead\tof\ttraining\tyour\town\tword\tembeddings,\tyou\tmay\twant\tto\tdownload pretrained\tword\tembeddings.\tJust\tlike\twhen\treusing\tpretrained\tlayers\t(see\tChapter\t11),\tyou\tcan\tchoose\tto freeze\tthe\tpretrained\tembeddings\t(e.g.,\tcreating\tthe\tembeddings\tvariable\tusing\ttrainable=False)\tor\tlet\n\nbackpropagation\ttweak\tthem\tfor\tyour\tapplication.\tThe\tfirst\toption\twill\tspeed\tup\ttraining,\tbut\tthe\tsecond may\tlead\tto\tslightly\thigher\tperformance.\n\nTIP\n\nEmbeddings\tare\talso\tuseful\tfor\trepresenting\tcategorical\tattributes\tthat\tcan\ttake\ton\ta\tlarge\tnumber\tof\tdifferent\tvalues,\tespecially when\tthere\tare\tcomplex\tsimilarities\tbetween\tvalues.\tFor\texample,\tconsider\tprofessions,\thobbies,\tdishes,\tspecies,\tbrands,\tand\tso on.\n\nYou\tnow\thave\talmost\tall\tthe\ttools\tyou\tneed\tto\timplement\ta\tmachine\ttranslation\tsystem.\tLet’s\tlook\tat\tthis now.\n\nAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation Let’s\ttake\ta\tlook\tat\ta\tsimple\tmachine\ttranslation\tmodel10\tthat\twill\ttranslate\tEnglish\tsentences\tto\tFrench (see\tFigure\t14-15).\n\nFigure\t14-15.\tA\tsimple\tmachine\ttranslation\tmodel\n\nThe\tEnglish\tsentences\tare\tfed\tto\tthe\tencoder,\tand\tthe\tdecoder\toutputs\tthe\tFrench\ttranslations.\tNote\tthat the\tFrench\ttranslations\tare\talso\tused\tas\tinputs\tto\tthe\tdecoder,\tbut\tpushed\tback\tby\tone\tstep.\tIn\tother words,\tthe\tdecoder\tis\tgiven\tas\tinput\tthe\tword\tthat\tit\tshould\thave\toutput\tat\tthe\tprevious\tstep\t(regardless\tof what\tit\tactually\toutput).\tFor\tthe\tvery\tfirst\tword,\tit\tis\tgiven\ta\ttoken\tthat\trepresents\tthe\tbeginning\tof\tthe sentence\t(e.g.,\t“<go>”).\tThe\tdecoder\tis\texpected\tto\tend\tthe\tsentence\twith\tan\tend-of-sequence\t(EOS) token\t(e.g.,\t“<eos>”).\n\nNote\tthat\tthe\tEnglish\tsentences\tare\treversed\tbefore\tthey\tare\tfed\tto\tthe\tencoder.\tFor\texample\t“I\tdrink milk”\tis\treversed\tto\t“milk\tdrink\tI.”\tThis\tensures\tthat\tthe\tbeginning\tof\tthe\tEnglish\tsentence\twill\tbe\tfed\tlast to\tthe\tencoder,\twhich\tis\tuseful\tbecause\tthat’s\tgenerally\tthe\tfirst\tthing\tthat\tthe\tdecoder\tneeds\tto\ttranslate.\n\nEach\tword\tis\tinitially\trepresented\tby\ta\tsimple\tinteger\tidentifier\t(e.g.,\t288\tfor\tthe\tword\t“milk”).\tNext,\tan embedding\tlookup\treturns\tthe\tword\tembedding\t(as\texplained\tearlier,\tthis\tis\ta\tdense,\tfairly\tlow- dimensional\tvector).\tThese\tword\tembeddings\tare\twhat\tis\tactually\tfed\tto\tthe\tencoder\tand\tthe\tdecoder.\n\nAt\teach\tstep,\tthe\tdecoder\toutputs\ta\tscore\tfor\teach\tword\tin\tthe\toutput\tvocabulary\t(i.e.,\tFrench),\tand\tthen the\tSoftmax\tlayer\tturns\tthese\tscores\tinto\tprobabilities.\tFor\texample,\tat\tthe\tfirst\tstep\tthe\tword\t“Je”\tmay have\ta\tprobability\tof\t20%,\t“Tu”\tmay\thave\ta\tprobability\tof\t1%,\tand\tso\ton.\tThe\tword\twith\tthe\thighest probability\tis\toutput.\tThis\tis\tvery\tmuch\tlike\ta\tregular\tclassification\ttask,\tso\tyou\tcan\ttrain\tthe\tmodel\tusing the\tsoftmax_cross_entropy_with_logits()\tfunction.\n\nNote\tthat\tat\tinference\ttime\t(after\ttraining),\tyou\twill\tnot\thave\tthe\ttarget\tsentence\tto\tfeed\tto\tthe\tdecoder.\n\nInstead,\tsimply\tfeed\tthe\tdecoder\tthe\tword\tthat\tit\toutput\tat\tthe\tprevious\tstep,\tas\tshown\tin\tFigure\t14-16 (this\twill\trequire\tan\tembedding\tlookup\tthat\tis\tnot\tshown\ton\tthe\tdiagram).\n\nFigure\t14-16.\tFeeding\tthe\tprevious\toutput\tword\tas\tinput\tat\tinference\ttime\n\nOkay,\tnow\tyou\thave\tthe\tbig\tpicture.\tHowever,\tif\tyou\tgo\tthrough\tTensorFlow’s\tsequence-to-sequence tutorial\tand\tyou\tlook\tat\tthe\tcode\tin\trnn/translate/seq2seq_model.py\t(in\tthe\tTensorFlow\tmodels),\tyou will\tnotice\ta\tfew\timportant\tdifferences:\n\nFirst,\tso\tfar\twe\thave\tassumed\tthat\tall\tinput\tsequences\t(to\tthe\tencoder\tand\tto\tthe\tdecoder)\thave\ta constant\tlength.\tBut\tobviously\tsentence\tlengths\tmay\tvary.\tThere\tare\tseveral\tways\tthat\tthis\tcan\tbe handled\t—\tfor\texample,\tusing\tthe\tsequence_length\targument\tto\tthe\tstatic_rnn()\tor dynamic_rnn()\tfunctions\tto\tspecify\teach\tsentence’s\tlength\t(as\tdiscussed\tearlier).\tHowever,\tanother approach\tis\tused\tin\tthe\ttutorial\t(presumably\tfor\tperformance\treasons):\tsentences\tare\tgrouped\tinto buckets\tof\tsimilar\tlengths\t(e.g.,\ta\tbucket\tfor\tthe\t1-\tto\t6-word\tsentences,\tanother\tfor\tthe\t7-\tto\t12- word\tsentences,\tand\tso\ton11),\tand\tthe\tshorter\tsentences\tare\tpadded\tusing\ta\tspecial\tpadding\ttoken (e.g.,\t“<pad>”).\tFor\texample\t“I\tdrink\tmilk”\tbecomes\t“<pad>\t<pad>\t<pad>\tmilk\tdrink\tI”,\tand\tits translation\tbecomes\t“Je\tbois\tdu\tlait\t<eos>\t<pad>”.\tOf\tcourse,\twe\twant\tto\tignore\tany\toutput\tpast\tthe EOS\ttoken.\tFor\tthis,\tthe\ttutorial’s\timplementation\tuses\ta\ttarget_weights\tvector.\tFor\texample,\tfor the\ttarget\tsentence\t“Je\tbois\tdu\tlait\t<eos>\t<pad>”,\tthe\tweights\twould\tbe\tset\tto\t[1.0,\t1.0,\t1.0, 1.0,\t1.0,\t0.0]\t(notice\tthe\tweight\t0.0\tthat\tcorresponds\tto\tthe\tpadding\ttoken\tin\tthe\ttarget\tsentence). Simply\tmultiplying\tthe\tlosses\tby\tthe\ttarget\tweights\twill\tzero\tout\tthe\tlosses\tthat\tcorrespond\tto\twords past\tEOS\ttokens.\n\nSecond,\twhen\tthe\toutput\tvocabulary\tis\tlarge\t(which\tis\tthe\tcase\there),\toutputting\ta\tprobability\tfor each\tand\tevery\tpossible\tword\twould\tbe\tterribly\tslow.\tIf\tthe\ttarget\tvocabulary\tcontains,\tsay,\t50,000 French\twords,\tthen\tthe\tdecoder\twould\toutput\t50,000-dimensional\tvectors,\tand\tthen\tcomputing\tthe softmax\tfunction\tover\tsuch\ta\tlarge\tvector\twould\tbe\tvery\tcomputationally\tintensive.\tTo\tavoid\tthis, one\tsolution\tis\tto\tlet\tthe\tdecoder\toutput\tmuch\tsmaller\tvectors,\tsuch\tas\t1,000-dimensional\tvectors, then\tuse\ta\tsampling\ttechnique\tto\testimate\tthe\tloss\twithout\thaving\tto\tcompute\tit\tover\tevery\tsingle word\tin\tthe\ttarget\tvocabulary.\tThis\tSampled\tSoftmax\ttechnique\twas\tintroduced\tin\t2015\tby\tSébastien Jean\tet\tal.12\tIn\tTensorFlow\tyou\tcan\tuse\tthe\tsampled_softmax_loss()\tfunction.\n\nThird,\tthe\ttutorial’s\timplementation\tuses\tan\tattention\tmechanism\tthat\tlets\tthe\tdecoder\tpeek\tinto\tthe input\tsequence.\tAttention\taugmented\tRNNs\tare\tbeyond\tthe\tscope\tof\tthis\tbook,\tbut\tif\tyou\tare\n\ninterested\tthere\tare\thelpful\tpapers\tabout\tmachine\ttranslation,13\tmachine\treading,14\tand\timage captions15\tusing\tattention.\n\nFinally,\tthe\ttutorial’s\timplementation\tmakes\tuse\tof\tthe\ttf.nn.legacy_seq2seq\tmodule,\twhich provides\ttools\tto\tbuild\tvarious\tEncoder–Decoder\tmodels\teasily.\tFor\texample,\tthe embedding_rnn_seq2seq()\tfunction\tcreates\ta\tsimple\tEncoder–Decoder\tmodel\tthat\tautomatically takes\tcare\tof\tword\tembeddings\tfor\tyou,\tjust\tlike\tthe\tone\trepresented\tin\tFigure\t14-15.\tThis\tcode\twill likely\tbe\tupdated\tquickly\tto\tuse\tthe\tnew\ttf.nn.seq2seq\tmodule.\n\nYou\tnow\thave\tall\tthe\ttools\tyou\tneed\tto\tunderstand\tthe\tsequence-to-sequence\ttutorial’s\timplementation. Check\tit\tout\tand\ttrain\tyour\town\tEnglish-to-French\ttranslator!\n\nExercises\n\n1.\t Can\tyou\tthink\tof\ta\tfew\tapplications\tfor\ta\tsequence-to-sequence\tRNN?\tWhat\tabout\ta\tsequence-to- vector\tRNN?\tAnd\ta\tvector-to-sequence\tRNN?\n\n2.\t Why\tdo\tpeople\tuse\tencoder–decoder\tRNNs\trather\tthan\tplain\tsequence-to-sequence\tRNNs\tfor automatic\ttranslation?\n\n3.\t How\tcould\tyou\tcombine\ta\tconvolutional\tneural\tnetwork\twith\tan\tRNN\tto\tclassify\tvideos?\n\n4.\t What\tare\tthe\tadvantages\tof\tbuilding\tan\tRNN\tusing\tdynamic_rnn()\trather\tthan\tstatic_rnn()?\n\n5.\t How\tcan\tyou\tdeal\twith\tvariable-length\tinput\tsequences?\tWhat\tabout\tvariable-length\toutput sequences?\n\n6.\t What\tis\ta\tcommon\tway\tto\tdistribute\ttraining\tand\texecution\tof\ta\tdeep\tRNN\tacross\tmultiple\tGPUs?\n\n7.\t Embedded\tReber\tgrammars\twere\tused\tby\tHochreiter\tand\tSchmidhuber\tin\ttheir\tpaper\tabout\tLSTMs. They\tare\tartificial\tgrammars\tthat\tproduce\tstrings\tsuch\tas\t“BPBTSXXVPSEPE.”\tCheck\tout\tJenny Orr’s\tnice\tintroduction\tto\tthis\ttopic.\tChoose\ta\tparticular\tembedded\tReber\tgrammar\t(such\tas\tthe\tone represented\ton\tJenny\tOrr’s\tpage),\tthen\ttrain\tan\tRNN\tto\tidentify\twhether\ta\tstring\trespects\tthat grammar\tor\tnot.\tYou\twill\tfirst\tneed\tto\twrite\ta\tfunction\tcapable\tof\tgenerating\ta\ttraining\tbatch containing\tabout\t50%\tstrings\tthat\trespect\tthe\tgrammar,\tand\t50%\tthat\tdon’t.\n\n8.\t Tackle\tthe\t“How\tmuch\tdid\tit\train?\tII”\tKaggle\tcompetition.\tThis\tis\ta\ttime\tseries\tprediction\ttask:\tyou are\tgiven\tsnapshots\tof\tpolarimetric\tradar\tvalues\tand\tasked\tto\tpredict\tthe\thourly\train\tgauge\ttotal. Luis\tAndre\tDutra\te\tSilva’s\tinterview\tgives\tsome\tinteresting\tinsights\tinto\tthe\ttechniques\the\tused\tto reach\tsecond\tplace\tin\tthe\tcompetition.\tIn\tparticular,\the\tused\tan\tRNN\tcomposed\tof\ttwo\tLSTM\tlayers.\n\n9.\t Go\tthrough\tTensorFlow’s\tWord2Vec\ttutorial\tto\tcreate\tword\tembeddings,\tand\tthen\tgo\tthrough\tthe Seq2Seq\ttutorial\tto\ttrain\tan\tEnglish-to-French\ttranslation\tsystem.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nNote\tthat\tmany\tresearchers\tprefer\tto\tuse\tthe\thyperbolic\ttangent\t(tanh)\tactivation\tfunction\tin\tRNNs\trather\tthan\tthe\tReLU\tactivation function.\tFor\texample,\ttake\ta\tlook\tat\tby\tVu\tPham\tet\tal.’s\tpaper\t“Dropout\tImproves\tRecurrent\tNeural\tNetworks\tfor\tHandwriting Recognition”.\tHowever,\tReLU-based\tRNNs\tare\talso\tpossible,\tas\tshown\tin\tQuoc\tV.\tLe\tet\tal.’s\tpaper\t“A\tSimple\tWay\tto\tInitialize Recurrent\tNetworks\tof\tRectified\tLinear\tUnits”.\n\n2\n\nThis\tuses\tthe\tdecorator\tdesign\tpattern.\n\n3\n\n“Long\tShort-Term\tMemory,”\tS.\tHochreiter\tand\tJ.\tSchmidhuber\t(1997).\n\n4\n\n“Long\tShort-Term\tMemory\tRecurrent\tNeural\tNetwork\tArchitectures\tfor\tLarge\tScale\tAcoustic\tModeling,”\tH.\tSak\tet\tal.\t(2014).\n\n5\n\n“Recurrent\tNeural\tNetwork\tRegularization,”\tW.\tZaremba\tet\tal.\t(2015).\n\n6\n\n“Recurrent\tNets\tthat\tTime\tand\tCount,”\tF.\tGers\tand\tJ.\tSchmidhuber\t(2000).\n\n7\n\n“Learning\tPhrase\tRepresentations\tusing\tRNN\tEncoder–Decoder\tfor\tStatistical\tMachine\tTranslation,”\tK.\tCho\tet\tal.\t(2014).\n\n8\n\nA\t2015\tpaper\tby\tKlaus\tGreff\tet\tal.,\t“LSTM:\tA\tSearch\tSpace\tOdyssey,”\tseems\tto\tshow\tthat\tall\tLSTM\tvariants\tperform\troughly\tthe\tsame.\n\n9\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\nFor\tmore\tdetails,\tcheck\tout\tChristopher\tOlah’s\tgreat\tpost,\tor\tSebastian\tRuder’s\tseries\tof\tposts.\n\n“Sequence\tto\tSequence\tlearning\twith\tNeural\tNetworks,”\tI.\tSutskever\tet\tal.\t(2014).\n\nThe\tbucket\tsizes\tused\tin\tthe\ttutorial\tare\tdifferent.\n\n“On\tUsing\tVery\tLarge\tTarget\tVocabulary\tfor\tNeural\tMachine\tTranslation,”\tS.\tJean\tet\tal.\t(2015).\n\n“Neural\tMachine\tTranslation\tby\tJointly\tLearning\tto\tAlign\tand\tTranslate,”\tD.\tBahdanau\tet\tal.\t(2014).\n\n“Long\tShort-Term\tMemory-Networks\tfor\tMachine\tReading,”\tJ.\tCheng\t(2016).\n\n“Show,\tAttend\tand\tTell:\tNeural\tImage\tCaption\tGeneration\twith\tVisual\tAttention,”\tK.\tXu\tet\tal.\t(2015).\n\nChapter\t15.\tAutoencoders\n\nAutoencoders\tare\tartificial\tneural\tnetworks\tcapable\tof\tlearning\tefficient\trepresentations\tof\tthe\tinput\tdata, called\tcodings,\twithout\tany\tsupervision\t(i.e.,\tthe\ttraining\tset\tis\tunlabeled).\tThese\tcodings\ttypically\thave a\tmuch\tlower\tdimensionality\tthan\tthe\tinput\tdata,\tmaking\tautoencoders\tuseful\tfor\tdimensionality\treduction (see\tChapter\t8).\tMore\timportantly,\tautoencoders\tact\tas\tpowerful\tfeature\tdetectors,\tand\tthey\tcan\tbe\tused for\tunsupervised\tpretraining\tof\tdeep\tneural\tnetworks\t(as\twe\tdiscussed\tin\tChapter\t11).\tLastly,\tthey\tare capable\tof\trandomly\tgenerating\tnew\tdata\tthat\tlooks\tvery\tsimilar\tto\tthe\ttraining\tdata;\tthis\tis\tcalled\ta generative\tmodel.\tFor\texample,\tyou\tcould\ttrain\tan\tautoencoder\ton\tpictures\tof\tfaces,\tand\tit\twould\tthen\tbe able\tto\tgenerate\tnew\tfaces.\n\nSurprisingly,\tautoencoders\twork\tby\tsimply\tlearning\tto\tcopy\ttheir\tinputs\tto\ttheir\toutputs.\tThis\tmay\tsound like\ta\ttrivial\ttask,\tbut\twe\twill\tsee\tthat\tconstraining\tthe\tnetwork\tin\tvarious\tways\tcan\tmake\tit\trather difficult.\tFor\texample,\tyou\tcan\tlimit\tthe\tsize\tof\tthe\tinternal\trepresentation,\tor\tyou\tcan\tadd\tnoise\tto\tthe inputs\tand\ttrain\tthe\tnetwork\tto\trecover\tthe\toriginal\tinputs.\tThese\tconstraints\tprevent\tthe\tautoencoder\tfrom trivially\tcopying\tthe\tinputs\tdirectly\tto\tthe\toutputs,\twhich\tforces\tit\tto\tlearn\tefficient\tways\tof\trepresenting the\tdata.\tIn\tshort,\tthe\tcodings\tare\tbyproducts\tof\tthe\tautoencoder’s\tattempt\tto\tlearn\tthe\tidentity\tfunction under\tsome\tconstraints.\n\nIn\tthis\tchapter\twe\twill\texplain\tin\tmore\tdepth\thow\tautoencoders\twork,\twhat\ttypes\tof\tconstraints\tcan\tbe imposed,\tand\thow\tto\timplement\tthem\tusing\tTensorFlow,\twhether\tit\tis\tfor\tdimensionality\treduction, feature\textraction,\tunsupervised\tpretraining,\tor\tas\tgenerative\tmodels.\n\nEfficient\tData\tRepresentations Which\tof\tthe\tfollowing\tnumber\tsequences\tdo\tyou\tfind\tthe\teasiest\tto\tmemorize?\n\n40,\t27,\t25,\t36,\t81,\t57,\t10,\t73,\t19,\t68\n\n50,\t25,\t76,\t38,\t19,\t58,\t29,\t88,\t44,\t22,\t11,\t34,\t17,\t52,\t26,\t13,\t40,\t20\n\nAt\tfirst\tglance,\tit\twould\tseem\tthat\tthe\tfirst\tsequence\tshould\tbe\teasier,\tsince\tit\tis\tmuch\tshorter.\tHowever, if\tyou\tlook\tcarefully\tat\tthe\tsecond\tsequence,\tyou\tmay\tnotice\tthat\tit\tfollows\ttwo\tsimple\trules:\teven numbers\tare\tfollowed\tby\ttheir\thalf,\tand\todd\tnumbers\tare\tfollowed\tby\ttheir\ttriple\tplus\tone\t(this\tis\ta famous\tsequence\tknown\tas\tthe\thailstone\tsequence).\tOnce\tyou\tnotice\tthis\tpattern,\tthe\tsecond\tsequence becomes\tmuch\teasier\tto\tmemorize\tthan\tthe\tfirst\tbecause\tyou\tonly\tneed\tto\tmemorize\tthe\ttwo\trules,\tthe\tfirst number,\tand\tthe\tlength\tof\tthe\tsequence.\tNote\tthat\tif\tyou\tcould\tquickly\tand\teasily\tmemorize\tvery\tlong sequences,\tyou\twould\tnot\tcare\tmuch\tabout\tthe\texistence\tof\ta\tpattern\tin\tthe\tsecond\tsequence.\tYou\twould just\tlearn\tevery\tnumber\tby\theart,\tand\tthat\twould\tbe\tthat.\tIt\tis\tthe\tfact\tthat\tit\tis\thard\tto\tmemorize\tlong sequences\tthat\tmakes\tit\tuseful\tto\trecognize\tpatterns,\tand\thopefully\tthis\tclarifies\twhy\tconstraining\tan autoencoder\tduring\ttraining\tpushes\tit\tto\tdiscover\tand\texploit\tpatterns\tin\tthe\tdata.\n\nThe\trelationship\tbetween\tmemory,\tperception,\tand\tpattern\tmatching\twas\tfamously\tstudied\tby\tWilliam Chase\tand\tHerbert\tSimon\tin\tthe\tearly\t1970s.1\tThey\tobserved\tthat\texpert\tchess\tplayers\twere\table\tto memorize\tthe\tpositions\tof\tall\tthe\tpieces\tin\ta\tgame\tby\tlooking\tat\tthe\tboard\tfor\tjust\t5\tseconds,\ta\ttask\tthat most\tpeople\twould\tfind\timpossible.\tHowever,\tthis\twas\tonly\tthe\tcase\twhen\tthe\tpieces\twere\tplaced\tin realistic\tpositions\t(from\tactual\tgames),\tnot\twhen\tthe\tpieces\twere\tplaced\trandomly.\tChess\texperts\tdon’t have\ta\tmuch\tbetter\tmemory\tthan\tyou\tand\tI,\tthey\tjust\tsee\tchess\tpatterns\tmore\teasily\tthanks\tto\ttheir experience\twith\tthe\tgame.\tNoticing\tpatterns\thelps\tthem\tstore\tinformation\tefficiently.\n\nJust\tlike\tthe\tchess\tplayers\tin\tthis\tmemory\texperiment,\tan\tautoencoder\tlooks\tat\tthe\tinputs,\tconverts\tthem\tto an\tefficient\tinternal\trepresentation,\tand\tthen\tspits\tout\tsomething\tthat\t(hopefully)\tlooks\tvery\tclose\tto\tthe inputs.\tAn\tautoencoder\tis\talways\tcomposed\tof\ttwo\tparts:\tan\tencoder\t(or\trecognition\tnetwork)\tthat converts\tthe\tinputs\tto\tan\tinternal\trepresentation,\tfollowed\tby\ta\tdecoder\t(or\tgenerative\tnetwork)\tthat converts\tthe\tinternal\trepresentation\tto\tthe\toutputs\t(see\tFigure\t15-1).\n\nAs\tyou\tcan\tsee,\tan\tautoencoder\ttypically\thas\tthe\tsame\tarchitecture\tas\ta\tMulti-Layer\tPerceptron\t(MLP; see\tChapter\t10),\texcept\tthat\tthe\tnumber\tof\tneurons\tin\tthe\toutput\tlayer\tmust\tbe\tequal\tto\tthe\tnumber\tof inputs.\tIn\tthis\texample,\tthere\tis\tjust\tone\thidden\tlayer\tcomposed\tof\ttwo\tneurons\t(the\tencoder),\tand\tone output\tlayer\tcomposed\tof\tthree\tneurons\t(the\tdecoder).\tThe\toutputs\tare\toften\tcalled\tthe\treconstructions since\tthe\tautoencoder\ttries\tto\treconstruct\tthe\tinputs,\tand\tthe\tcost\tfunction\tcontains\ta\treconstruction\tloss that\tpenalizes\tthe\tmodel\twhen\tthe\treconstructions\tare\tdifferent\tfrom\tthe\tinputs.\n\nFigure\t15-1.\tThe\tchess\tmemory\texperiment\t(left)\tand\ta\tsimple\tautoencoder\t(right)\n\nBecause\tthe\tinternal\trepresentation\thas\ta\tlower\tdimensionality\tthan\tthe\tinput\tdata\t(it\tis\t2D\tinstead\tof\t3D), the\tautoencoder\tis\tsaid\tto\tbe\tundercomplete.\tAn\tundercomplete\tautoencoder\tcannot\ttrivially\tcopy\tits inputs\tto\tthe\tcodings,\tyet\tit\tmust\tfind\ta\tway\tto\toutput\ta\tcopy\tof\tits\tinputs.\tIt\tis\tforced\tto\tlearn\tthe\tmost important\tfeatures\tin\tthe\tinput\tdata\t(and\tdrop\tthe\tunimportant\tones).\n\nLet’s\tsee\thow\tto\timplement\ta\tvery\tsimple\tundercomplete\tautoencoder\tfor\tdimensionality\treduction.",
      "page_number": 469
    },
    {
      "number": 15,
      "title": "Autoencoders",
      "start_page": 506,
      "end_page": 533,
      "detection_method": "regex_chapter_title",
      "content": "Performing\tPCA\twith\tan\tUndercomplete\tLinear\tAutoencoder If\tthe\tautoencoder\tuses\tonly\tlinear\tactivations\tand\tthe\tcost\tfunction\tis\tthe\tMean\tSquared\tError\t(MSE), then\tit\tcan\tbe\tshown\tthat\tit\tends\tup\tperforming\tPrincipal\tComponent\tAnalysis\t(see\tChapter\t8).\n\nThe\tfollowing\tcode\tbuilds\ta\tsimple\tlinear\tautoencoder\tto\tperform\tPCA\ton\ta\t3D\tdataset,\tprojecting\tit\tto 2D:\n\nimport\ttensorflow\tas\ttf\n\nn_inputs\t=\t3\t\t#\t3D\tinputs n_hidden\t=\t2\t\t#\t2D\tcodings n_outputs\t=\tn_inputs\n\nlearning_rate\t=\t0.01\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs]) hidden\t=\ttf.layers.dense(X,\tn_hidden) outputs\t=\ttf.layers.dense(hidden,\tn_outputs)\n\nreconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX))\t\t#\tMSE\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate) training_op\t=\toptimizer.minimize(reconstruction_loss)\n\ninit\t=\ttf.global_variables_initializer()\n\nThis\tcode\tis\treally\tnot\tvery\tdifferent\tfrom\tall\tthe\tMLPs\twe\tbuilt\tin\tpast\tchapters.\tThe\ttwo\tthings\tto\tnote are:\n\nThe\tnumber\tof\toutputs\tis\tequal\tto\tthe\tnumber\tof\tinputs.\n\nTo\tperform\tsimple\tPCA,\twe\tdo\tnot\tuse\tany\tactivation\tfunction\t(i.e.,\tall\tneurons\tare\tlinear)\tand\tthe cost\tfunction\tis\tthe\tMSE.\tWe\twill\tsee\tmore\tcomplex\tautoencoders\tshortly.\n\nNow\tlet’s\tload\tthe\tdataset,\ttrain\tthe\tmodel\ton\tthe\ttraining\tset,\tand\tuse\tit\tto\tencode\tthe\ttest\tset\t(i.e.,\tproject it\tto\t2D):\n\nX_train,\tX_test\t=\t[...]\t#\tload\tthe\tdataset\n\nn_iterations\t=\t1000 codings\t=\thidden\t\t#\tthe\toutput\tof\tthe\thidden\tlayer\tprovides\tthe\tcodings\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\titeration\tin\trange(n_iterations): \t\t\t\t\t\t\t\ttraining_op.run(feed_dict={X:\tX_train})\t\t#\tno\tlabels\t(unsupervised) \t\t\t\tcodings_val\t=\tcodings.eval(feed_dict={X:\tX_test})\n\nFigure\t15-2\tshows\tthe\toriginal\t3D\tdataset\t(at\tthe\tleft)\tand\tthe\toutput\tof\tthe\tautoencoder’s\thidden\tlayer (i.e.,\tthe\tcoding\tlayer,\tat\tthe\tright).\tAs\tyou\tcan\tsee,\tthe\tautoencoder\tfound\tthe\tbest\t2D\tplane\tto\tproject\tthe data\tonto,\tpreserving\tas\tmuch\tvariance\tin\tthe\tdata\tas\tit\tcould\t(just\tlike\tPCA).\n\nFigure\t15-2.\tPCA\tperformed\tby\tan\tundercomplete\tlinear\tautoencoder\n\nStacked\tAutoencoders Just\tlike\tother\tneural\tnetworks\twe\thave\tdiscussed,\tautoencoders\tcan\thave\tmultiple\thidden\tlayers.\tIn\tthis case\tthey\tare\tcalled\tstacked\tautoencoders\t(or\tdeep\tautoencoders).\tAdding\tmore\tlayers\thelps\tthe autoencoder\tlearn\tmore\tcomplex\tcodings.\tHowever,\tone\tmust\tbe\tcareful\tnot\tto\tmake\tthe\tautoencoder\ttoo powerful.\tImagine\tan\tencoder\tso\tpowerful\tthat\tit\tjust\tlearns\tto\tmap\teach\tinput\tto\ta\tsingle\tarbitrary\tnumber (and\tthe\tdecoder\tlearns\tthe\treverse\tmapping).\tObviously\tsuch\tan\tautoencoder\twill\treconstruct\tthe\ttraining data\tperfectly,\tbut\tit\twill\tnot\thave\tlearned\tany\tuseful\tdata\trepresentation\tin\tthe\tprocess\t(and\tit\tis\tunlikely to\tgeneralize\twell\tto\tnew\tinstances).\n\nThe\tarchitecture\tof\ta\tstacked\tautoencoder\tis\ttypically\tsymmetrical\twith\tregards\tto\tthe\tcentral\thidden\tlayer (the\tcoding\tlayer).\tTo\tput\tit\tsimply,\tit\tlooks\tlike\ta\tsandwich.\tFor\texample,\tan\tautoencoder\tfor\tMNIST (introduced\tin\tChapter\t3)\tmay\thave\t784\tinputs,\tfollowed\tby\ta\thidden\tlayer\twith\t300\tneurons,\tthen\ta central\thidden\tlayer\tof\t150\tneurons,\tthen\tanother\thidden\tlayer\twith\t300\tneurons,\tand\tan\toutput\tlayer\twith 784\tneurons.\tThis\tstacked\tautoencoder\tis\trepresented\tin\tFigure\t15-3.\n\nFigure\t15-3.\tStacked\tautoencoder\n\nTensorFlow\tImplementation You\tcan\timplement\ta\tstacked\tautoencoder\tvery\tmuch\tlike\ta\tregular\tdeep\tMLP.\tIn\tparticular,\tthe\tsame techniques\twe\tused\tin\tChapter\t11\tfor\ttraining\tdeep\tnets\tcan\tbe\tapplied.\tFor\texample,\tthe\tfollowing\tcode builds\ta\tstacked\tautoencoder\tfor\tMNIST,\tusing\tHe\tinitialization,\tthe\tELU\tactivation\tfunction,\tand\tℓ2 regularization.\tThe\tcode\tshould\tlook\tvery\tfamiliar,\texcept\tthat\tthere\tare\tno\tlabels\t(no\ty):\n\nfrom\tfunctools\timport\tpartial\n\nn_inputs\t=\t28\t*\t28\t\t#\tfor\tMNIST n_hidden1\t=\t300 n_hidden2\t=\t150\t\t#\tcodings n_hidden3\t=\tn_hidden1 n_outputs\t=\tn_inputs\n\nlearning_rate\t=\t0.01 l2_reg\t=\t0.0001\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs])\n\nhe_init\t=\ttf.contrib.layers.variance_scaling_initializer() l2_regularizer\t=\ttf.contrib.layers.l2_regularizer(l2_reg) my_dense_layer\t=\tpartial(tf.layers.dense, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.elu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=he_init, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_regularizer=l2_regularizer)\n\nhidden1\t=\tmy_dense_layer(X,\tn_hidden1) hidden2\t=\tmy_dense_layer(hidden1,\tn_hidden2)\t\t#\tcodings hidden3\t=\tmy_dense_layer(hidden2,\tn_hidden3) outputs\t=\tmy_dense_layer(hidden3,\tn_outputs,\tactivation=None)\n\nreconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX))\t\t#\tMSE\n\nreg_losses\t=\ttf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) loss\t=\ttf.add_n([reconstruction_loss]\t+\treg_losses)\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate) training_op\t=\toptimizer.minimize(loss)\n\ninit\t=\ttf.global_variables_initializer()\n\nYou\tcan\tthen\ttrain\tthe\tmodel\tnormally.\tNote\tthat\tthe\tdigit\tlabels\t(y_batch)\tare\tunused:\n\nn_epochs\t=\t5 batch_size\t=\t150\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tn_batches\t=\tmnist.train.num_examples\t//\tbatch_size \t\t\t\t\t\t\t\tfor\titeration\tin\trange(n_batches): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch})\n\nTying\tWeights When\tan\tautoencoder\tis\tneatly\tsymmetrical,\tlike\tthe\tone\twe\tjust\tbuilt,\ta\tcommon\ttechnique\tis\tto\ttie\tthe weights\tof\tthe\tdecoder\tlayers\tto\tthe\tweights\tof\tthe\tencoder\tlayers.\tThis\thalves\tthe\tnumber\tof\tweights\tin the\tmodel,\tspeeding\tup\ttraining\tand\tlimiting\tthe\trisk\tof\toverfitting.\tSpecifically,\tif\tthe\tautoencoder\thas\ta total\tof\tN\tlayers\t(not\tcounting\tthe\tinput\tlayer),\tand\tWL\trepresents\tthe\tconnection\tweights\tof\tthe\tLth\tlayer\n\n(e.g.,\tlayer\t1\tis\tthe\tfirst\thidden\tlayer,\tlayer\n\nis\tthe\tcoding\tlayer,\tand\tlayer\tN\tis\tthe\toutput\tlayer),\tthen\tthe\n\ndecoder\tlayer\tweights\tcan\tbe\tdefined\tsimply\tas:\tWN–L+1\t=\tWL\n\nT\t(with\tL\t=\t1,\t2,\n\n).\n\nUnfortunately,\timplementing\ttied\tweights\tin\tTensorFlow\tusing\tthe\tdense()\tfunction\tis\ta\tbit\tcumbersome; it’s\tactually\teasier\tto\tjust\tdefine\tthe\tlayers\tmanually.\tThe\tcode\tends\tup\tsignificantly\tmore\tverbose:\n\nactivation\t=\ttf.nn.elu regularizer\t=\ttf.contrib.layers.l2_regularizer(l2_reg) initializer\t=\ttf.contrib.layers.variance_scaling_initializer()\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs])\n\nweights1_init\t=\tinitializer([n_inputs,\tn_hidden1]) weights2_init\t=\tinitializer([n_hidden1,\tn_hidden2])\n\nweights1\t=\ttf.Variable(weights1_init,\tdtype=tf.float32,\tname=\"weights1\") weights2\t=\ttf.Variable(weights2_init,\tdtype=tf.float32,\tname=\"weights2\") weights3\t=\ttf.transpose(weights2,\tname=\"weights3\")\t\t#\ttied\tweights weights4\t=\ttf.transpose(weights1,\tname=\"weights4\")\t\t#\ttied\tweights\n\nbiases1\t=\ttf.Variable(tf.zeros(n_hidden1),\tname=\"biases1\") biases2\t=\ttf.Variable(tf.zeros(n_hidden2),\tname=\"biases2\") biases3\t=\ttf.Variable(tf.zeros(n_hidden3),\tname=\"biases3\") biases4\t=\ttf.Variable(tf.zeros(n_outputs),\tname=\"biases4\")\n\nhidden1\t=\tactivation(tf.matmul(X,\tweights1)\t+\tbiases1) hidden2\t=\tactivation(tf.matmul(hidden1,\tweights2)\t+\tbiases2) hidden3\t=\tactivation(tf.matmul(hidden2,\tweights3)\t+\tbiases3) outputs\t=\ttf.matmul(hidden3,\tweights4)\t+\tbiases4\n\nreconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX)) reg_loss\t=\tregularizer(weights1)\t+\tregularizer(weights2) loss\t=\treconstruction_loss\t+\treg_loss\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate) training_op\t=\toptimizer.minimize(loss)\n\ninit\t=\ttf.global_variables_initializer()\n\nThis\tcode\tis\tfairly\tstraightforward,\tbut\tthere\tare\ta\tfew\timportant\tthings\tto\tnote:\n\nFirst,\tweight3\tand\tweights4\tare\tnot\tvariables,\tthey\tare\trespectively\tthe\ttranspose\tof\tweights2\tand weights1\t(they\tare\t“tied”\tto\tthem).\n\nSecond,\tsince\tthey\tare\tnot\tvariables,\tit’s\tno\tuse\tregularizing\tthem:\twe\tonly\tregularize\tweights1\tand weights2.\n\nThird,\tbiases\tare\tnever\ttied,\tand\tnever\tregularized.\n\nTraining\tOne\tAutoencoder\tat\ta\tTime Rather\tthan\ttraining\tthe\twhole\tstacked\tautoencoder\tin\tone\tgo\tlike\twe\tjust\tdid,\tit\tis\toften\tmuch\tfaster\tto train\tone\tshallow\tautoencoder\tat\ta\ttime,\tthen\tstack\tall\tof\tthem\tinto\ta\tsingle\tstacked\tautoencoder\t(hence the\tname),\tas\tshown\ton\tFigure\t15-4.\tThis\tis\tespecially\tuseful\tfor\tvery\tdeep\tautoencoders.\n\nFigure\t15-4.\tTraining\tone\tautoencoder\tat\ta\ttime\n\nDuring\tthe\tfirst\tphase\tof\ttraining,\tthe\tfirst\tautoencoder\tlearns\tto\treconstruct\tthe\tinputs.\tDuring\tthe\tsecond phase,\tthe\tsecond\tautoencoder\tlearns\tto\treconstruct\tthe\toutput\tof\tthe\tfirst\tautoencoder’s\thidden\tlayer. Finally,\tyou\tjust\tbuild\ta\tbig\tsandwich\tusing\tall\tthese\tautoencoders,\tas\tshown\tin\tFigure\t15-4\t(i.e.,\tyou\tfirst stack\tthe\thidden\tlayers\tof\teach\tautoencoder,\tthen\tthe\toutput\tlayers\tin\treverse\torder).\tThis\tgives\tyou\tthe final\tstacked\tautoencoder.\tYou\tcould\teasily\ttrain\tmore\tautoencoders\tthis\tway,\tbuilding\ta\tvery\tdeep stacked\tautoencoder.\n\nTo\timplement\tthis\tmultiphase\ttraining\talgorithm,\tthe\tsimplest\tapproach\tis\tto\tuse\ta\tdifferent\tTensorFlow graph\tfor\teach\tphase.\tAfter\ttraining\tan\tautoencoder,\tyou\tjust\trun\tthe\ttraining\tset\tthrough\tit\tand\tcapture\tthe output\tof\tthe\thidden\tlayer.\tThis\toutput\tthen\tserves\tas\tthe\ttraining\tset\tfor\tthe\tnext\tautoencoder.\tOnce\tall autoencoders\thave\tbeen\ttrained\tthis\tway,\tyou\tsimply\tcopy\tthe\tweights\tand\tbiases\tfrom\teach\tautoencoder and\tuse\tthem\tto\tbuild\tthe\tstacked\tautoencoder.\tImplementing\tthis\tapproach\tis\tquite\tstraightforward,\tso\twe won’t\tdetail\tit\there,\tbut\tplease\tcheck\tout\tthe\tcode\tin\tthe\tJupyter\tnotebooks\tfor\tan\texample.\n\nAnother\tapproach\tis\tto\tuse\ta\tsingle\tgraph\tcontaining\tthe\twhole\tstacked\tautoencoder,\tplus\tsome\textra operations\tto\tperform\teach\ttraining\tphase,\tas\tshown\tin\tFigure\t15-5.\n\nFigure\t15-5.\tA\tsingle\tgraph\tto\ttrain\ta\tstacked\tautoencoder\n\nThis\tdeserves\ta\tbit\tof\texplanation:\n\nThe\tcentral\tcolumn\tin\tthe\tgraph\tis\tthe\tfull\tstacked\tautoencoder.\tThis\tpart\tcan\tbe\tused\tafter\ttraining.\n\nThe\tleft\tcolumn\tis\tthe\tset\tof\toperations\tneeded\tto\trun\tthe\tfirst\tphase\tof\ttraining.\tIt\tcreates\tan\toutput layer\tthat\tbypasses\thidden\tlayers\t2\tand\t3.\tThis\toutput\tlayer\tshares\tthe\tsame\tweights\tand\tbiases\tas the\tstacked\tautoencoder’s\toutput\tlayer.\tOn\ttop\tof\tthat\tare\tthe\ttraining\toperations\tthat\twill\taim\tat making\tthe\toutput\tas\tclose\tas\tpossible\tto\tthe\tinputs.\tThus,\tthis\tphase\twill\ttrain\tthe\tweights\tand biases\tfor\tthe\thidden\tlayer\t1\tand\tthe\toutput\tlayer\t(i.e.,\tthe\tfirst\tautoencoder).\n\nThe\tright\tcolumn\tin\tthe\tgraph\tis\tthe\tset\tof\toperations\tneeded\tto\trun\tthe\tsecond\tphase\tof\ttraining.\tIt adds\tthe\ttraining\toperation\tthat\twill\taim\tat\tmaking\tthe\toutput\tof\thidden\tlayer\t3\tas\tclose\tas\tpossible to\tthe\toutput\tof\thidden\tlayer\t1.\tNote\tthat\twe\tmust\tfreeze\thidden\tlayer\t1\twhile\trunning\tphase\t2.\tThis phase\twill\ttrain\tthe\tweights\tand\tbiases\tfor\thidden\tlayers\t2\tand\t3\t(i.e.,\tthe\tsecond\tautoencoder).\n\nThe\tTensorFlow\tcode\tlooks\tlike\tthis:\n\n[...]\t#\tBuild\tthe\twhole\tstacked\tautoencoder\tnormally. \t\t\t\t\t\t#\tIn\tthis\texample,\tthe\tweights\tare\tnot\ttied.\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate)\n\nwith\ttf.name_scope(\"phase1\"): \t\t\t\tphase1_outputs\t=\ttf.matmul(hidden1,\tweights4)\t+\tbiases4 \t\t\t\tphase1_reconstruction_loss\t=\ttf.reduce_mean(tf.square(phase1_outputs\t-\tX)) \t\t\t\tphase1_reg_loss\t=\tregularizer(weights1)\t+\tregularizer(weights4) \t\t\t\tphase1_loss\t=\tphase1_reconstruction_loss\t+\tphase1_reg_loss \t\t\t\tphase1_training_op\t=\toptimizer.minimize(phase1_loss)\n\nwith\ttf.name_scope(\"phase2\"): \t\t\t\tphase2_reconstruction_loss\t=\ttf.reduce_mean(tf.square(hidden3\t-\thidden1)) \t\t\t\tphase2_reg_loss\t=\tregularizer(weights2)\t+\tregularizer(weights3) \t\t\t\tphase2_loss\t=\tphase2_reconstruction_loss\t+\tphase2_reg_loss \t\t\t\ttrain_vars\t=\t[weights2,\tbiases2,\tweights3,\tbiases3] \t\t\t\tphase2_training_op\t=\toptimizer.minimize(phase2_loss,\tvar_list=train_vars)\n\nThe\tfirst\tphase\tis\trather\tstraightforward:\twe\tjust\tcreate\tan\toutput\tlayer\tthat\tskips\thidden\tlayers\t2\tand\t3, then\tbuild\tthe\ttraining\toperations\tto\tminimize\tthe\tdistance\tbetween\tthe\toutputs\tand\tthe\tinputs\t(plus\tsome\n\nregularization).\n\nThe\tsecond\tphase\tjust\tadds\tthe\toperations\tneeded\tto\tminimize\tthe\tdistance\tbetween\tthe\toutput\tof\thidden layer\t3\tand\thidden\tlayer\t1\t(also\twith\tsome\tregularization).\tMost\timportantly,\twe\tprovide\tthe\tlist\tof trainable\tvariables\tto\tthe\tminimize()\tmethod,\tmaking\tsure\tto\tleave\tout\tweights1\tand\tbiases1;\tthis effectively\tfreezes\thidden\tlayer\t1\tduring\tphase\t2.\n\nDuring\tthe\texecution\tphase,\tall\tyou\tneed\tto\tdo\tis\trun\tthe\tphase\t1\ttraining\top\tfor\ta\tnumber\tof\tepochs,\tthen the\tphase\t2\ttraining\top\tfor\tsome\tmore\tepochs.\n\nTIP\n\nSince\thidden\tlayer\t1\tis\tfrozen\tduring\tphase\t2,\tits\toutput\twill\talways\tbe\tthe\tsame\tfor\tany\tgiven\ttraining\tinstance.\tTo\tavoid\thaving to\trecompute\tthe\toutput\tof\thidden\tlayer\t1\tat\tevery\tsingle\tepoch,\tyou\tcan\tcompute\tit\tfor\tthe\twhole\ttraining\tset\tat\tthe\tend\tof\tphase 1,\tthen\tdirectly\tfeed\tthe\tcached\toutput\tof\thidden\tlayer\t1\tduring\tphase\t2.\tThis\tcan\tgive\tyou\ta\tnice\tperformance\tboost.\n\nVisualizing\tthe\tReconstructions One\tway\tto\tensure\tthat\tan\tautoencoder\tis\tproperly\ttrained\tis\tto\tcompare\tthe\tinputs\tand\tthe\toutputs.\tThey must\tbe\tfairly\tsimilar,\tand\tthe\tdifferences\tshould\tbe\tunimportant\tdetails.\tLet’s\tplot\ttwo\trandom\tdigits\tand their\treconstructions:\n\nn_test_digits\t=\t2 X_test\t=\tmnist.test.images[:n_test_digits]\n\nwith\ttf.Session()\tas\tsess: \t\t\t\t[...]\t#\tTrain\tthe\tAutoencoder \t\t\t\toutputs_val\t=\toutputs.eval(feed_dict={X:\tX_test})\n\ndef\tplot_image(image,\tshape=[28,\t28]): \t\t\t\tplt.imshow(image.reshape(shape),\tcmap=\"Greys\",\tinterpolation=\"nearest\") \t\t\t\tplt.axis(\"off\")\n\nfor\tdigit_index\tin\trange(n_test_digits): \t\t\t\tplt.subplot(n_test_digits,\t2,\tdigit_index\t*\t2\t+\t1) \t\t\t\tplot_image(X_test[digit_index]) \t\t\t\tplt.subplot(n_test_digits,\t2,\tdigit_index\t*\t2\t+\t2) \t\t\t\tplot_image(outputs_val[digit_index])\n\nFigure\t15-6\tshows\tthe\tresulting\timages.\n\nFigure\t15-6.\tOriginal\tdigits\t(left)\tand\ttheir\treconstructions\t(right)\n\nLooks\tclose\tenough.\tSo\tthe\tautoencoder\thas\tproperly\tlearned\tto\treproduce\tits\tinputs,\tbut\thas\tit\tlearned useful\tfeatures?\tLet’s\ttake\ta\tlook.\n\nVisualizing\tFeatures Once\tyour\tautoencoder\thas\tlearned\tsome\tfeatures,\tyou\tmay\twant\tto\ttake\ta\tlook\tat\tthem.\tThere\tare\tvarious techniques\tfor\tthis.\tArguably\tthe\tsimplest\ttechnique\tis\tto\tconsider\teach\tneuron\tin\tevery\thidden\tlayer,\tand find\tthe\ttraining\tinstances\tthat\tactivate\tit\tthe\tmost.\tThis\tis\tespecially\tuseful\tfor\tthe\ttop\thidden\tlayers\tsince they\toften\tcapture\trelatively\tlarge\tfeatures\tthat\tyou\tcan\teasily\tspot\tin\ta\tgroup\tof\ttraining\tinstances\tthat contain\tthem.\tFor\texample,\tif\ta\tneuron\tstrongly\tactivates\twhen\tit\tsees\ta\tcat\tin\ta\tpicture,\tit\twill\tbe\tpretty obvious\tthat\tthe\tpictures\tthat\tactivate\tit\tthe\tmost\tall\tcontain\tcats.\tHowever,\tfor\tlower\tlayers,\tthis technique\tdoes\tnot\twork\tso\twell,\tas\tthe\tfeatures\tare\tsmaller\tand\tmore\tabstract,\tso\tit’s\toften\thard\tto understand\texactly\twhat\tthe\tneuron\tis\tgetting\tall\texcited\tabout.\n\nLet’s\tlook\tat\tanother\ttechnique.\tFor\teach\tneuron\tin\tthe\tfirst\thidden\tlayer,\tyou\tcan\tcreate\tan\timage\twhere\ta pixel’s\tintensity\tcorresponds\tto\tthe\tweight\tof\tthe\tconnection\tto\tthe\tgiven\tneuron.\tFor\texample,\tthe following\tcode\tplots\tthe\tfeatures\tlearned\tby\tfive\tneurons\tin\tthe\tfirst\thidden\tlayer:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\t[...]\t#\ttrain\tautoencoder \t\t\t\tweights1_val\t=\tweights1.eval()\n\nfor\ti\tin\trange(5): \t\t\t\tplt.subplot(1,\t5,\ti\t+\t1) \t\t\t\tplot_image(weights1_val.T[i])\n\nYou\tmay\tget\tlow-level\tfeatures\tsuch\tas\tthe\tones\tshown\tin\tFigure\t15-7.\n\nFigure\t15-7.\tFeatures\tlearned\tby\tfive\tneurons\tfrom\tthe\tfirst\thidden\tlayer\n\nThe\tfirst\tfour\tfeatures\tseem\tto\tcorrespond\tto\tsmall\tpatches,\twhile\tthe\tfifth\tfeature\tseems\tto\tlook\tfor vertical\tstrokes\t(note\tthat\tthese\tfeatures\tcome\tfrom\tthe\tstacked\tdenoising\tautoencoder\tthat\twe\twill discuss\tlater).\n\nAnother\ttechnique\tis\tto\tfeed\tthe\tautoencoder\ta\trandom\tinput\timage,\tmeasure\tthe\tactivation\tof\tthe\tneuron you\tare\tinterested\tin,\tand\tthen\tperform\tbackpropagation\tto\ttweak\tthe\timage\tin\tsuch\ta\tway\tthat\tthe\tneuron will\tactivate\teven\tmore.\tIf\tyou\titerate\tseveral\ttimes\t(performing\tgradient\tascent),\tthe\timage\twill gradually\tturn\tinto\tthe\tmost\texciting\timage\t(for\tthe\tneuron).\tThis\tis\ta\tuseful\ttechnique\tto\tvisualize\tthe kinds\tof\tinputs\tthat\ta\tneuron\tis\tlooking\tfor.\n\nFinally,\tif\tyou\tare\tusing\tan\tautoencoder\tto\tperform\tunsupervised\tpretraining\t—\tfor\texample,\tfor\ta classification\ttask\t—\ta\tsimple\tway\tto\tverify\tthat\tthe\tfeatures\tlearned\tby\tthe\tautoencoder\tare\tuseful\tis\tto measure\tthe\tperformance\tof\tthe\tclassifier.\n\nUnsupervised\tPretraining\tUsing\tStacked\tAutoencoders As\twe\tdiscussed\tin\tChapter\t11,\tif\tyou\tare\ttackling\ta\tcomplex\tsupervised\ttask\tbut\tyou\tdo\tnot\thave\ta\tlot\tof labeled\ttraining\tdata,\tone\tsolution\tis\tto\tfind\ta\tneural\tnetwork\tthat\tperforms\ta\tsimilar\ttask,\tand\tthen\treuse its\tlower\tlayers.\tThis\tmakes\tit\tpossible\tto\ttrain\ta\thigh-performance\tmodel\tusing\tonly\tlittle\ttraining\tdata because\tyour\tneural\tnetwork\twon’t\thave\tto\tlearn\tall\tthe\tlow-level\tfeatures;\tit\twill\tjust\treuse\tthe\tfeature detectors\tlearned\tby\tthe\texisting\tnet.\n\nSimilarly,\tif\tyou\thave\ta\tlarge\tdataset\tbut\tmost\tof\tit\tis\tunlabeled,\tyou\tcan\tfirst\ttrain\ta\tstacked\tautoencoder using\tall\tthe\tdata,\tthen\treuse\tthe\tlower\tlayers\tto\tcreate\ta\tneural\tnetwork\tfor\tyour\tactual\ttask,\tand\ttrain\tit using\tthe\tlabeled\tdata.\tFor\texample,\tFigure\t15-8\tshows\thow\tto\tuse\ta\tstacked\tautoencoder\tto\tperform unsupervised\tpretraining\tfor\ta\tclassification\tneural\tnetwork.\tThe\tstacked\tautoencoder\titself\tis\ttypically trained\tone\tautoencoder\tat\ta\ttime,\tas\tdiscussed\tearlier.\tWhen\ttraining\tthe\tclassifier,\tif\tyou\treally\tdon’t have\tmuch\tlabeled\ttraining\tdata,\tyou\tmay\twant\tto\tfreeze\tthe\tpretrained\tlayers\t(at\tleast\tthe\tlower\tones).\n\nFigure\t15-8.\tUnsupervised\tpretraining\tusing\tautoencoders\n\nNOTE\n\nThis\tsituation\tis\tactually\tquite\tcommon,\tbecause\tbuilding\ta\tlarge\tunlabeled\tdataset\tis\toften\tcheap\t(e.g.,\ta\tsimple\tscript\tcan download\tmillions\tof\timages\toff\tthe\tinternet),\tbut\tlabeling\tthem\tcan\tonly\tbe\tdone\treliably\tby\thumans\t(e.g.,\tclassifying\timages\tas cute\tor\tnot).\tLabeling\tinstances\tis\ttime-consuming\tand\tcostly,\tso\tit\tis\tquite\tcommon\tto\thave\tonly\ta\tfew\tthousand\tlabeled instances.\n\nAs\twe\tdiscussed\tearlier,\tone\tof\tthe\ttriggers\tof\tthe\tcurrent\tDeep\tLearning\ttsunami\tis\tthe\tdiscovery\tin\t2006 by\tGeoffrey\tHinton\tet\tal.\tthat\tdeep\tneural\tnetworks\tcan\tbe\tpretrained\tin\tan\tunsupervised\tfashion.\tThey used\trestricted\tBoltzmann\tmachines\tfor\tthat\t(see\tAppendix\tE),\tbut\tin\t2007\tYoshua\tBengio\tet\tal.\tshowed2 that\tautoencoders\tworked\tjust\tas\twell.\n\nThere\tis\tnothing\tspecial\tabout\tthe\tTensorFlow\timplementation:\tjust\ttrain\tan\tautoencoder\tusing\tall\tthe training\tdata,\tthen\treuse\tits\tencoder\tlayers\tto\tcreate\ta\tnew\tneural\tnetwork\t(see\tChapter\t11\tfor\tmore details\ton\thow\tto\treuse\tpretrained\tlayers,\tor\tcheck\tout\tthe\tcode\texamples\tin\tthe\tJupyter\tnotebooks).\n\nUp\tto\tnow,\tin\torder\tto\tforce\tthe\tautoencoder\tto\tlearn\tinteresting\tfeatures,\twe\thave\tlimited\tthe\tsize\tof\tthe coding\tlayer,\tmaking\tit\tundercomplete.\tThere\tare\tactually\tmany\tother\tkinds\tof\tconstraints\tthat\tcan\tbe used,\tincluding\tones\tthat\tallow\tthe\tcoding\tlayer\tto\tbe\tjust\tas\tlarge\tas\tthe\tinputs,\tor\teven\tlarger,\tresulting in\tan\tovercomplete\tautoencoder.\tLet’s\tlook\tat\tsome\tof\tthose\tapproaches\tnow.\n\nDenoising\tAutoencoders Another\tway\tto\tforce\tthe\tautoencoder\tto\tlearn\tuseful\tfeatures\tis\tto\tadd\tnoise\tto\tits\tinputs,\ttraining\tit\tto recover\tthe\toriginal,\tnoise-free\tinputs.\tThis\tprevents\tthe\tautoencoder\tfrom\ttrivially\tcopying\tits\tinputs\tto its\toutputs,\tso\tit\tends\tup\thaving\tto\tfind\tpatterns\tin\tthe\tdata.\n\nThe\tidea\tof\tusing\tautoencoders\tto\tremove\tnoise\thas\tbeen\taround\tsince\tthe\t1980s\t(e.g.,\tit\tis\tmentioned\tin Yann\tLeCun’s\t1987\tmaster’s\tthesis).\tIn\ta\t2008\tpaper,3\tPascal\tVincent\tet\tal.\tshowed\tthat\tautoencoders could\talso\tbe\tused\tfor\tfeature\textraction.\tIn\ta\t2010\tpaper,4\tVincent\tet\tal.\tintroduced\tstacked\tdenoising autoencoders.\n\nThe\tnoise\tcan\tbe\tpure\tGaussian\tnoise\tadded\tto\tthe\tinputs,\tor\tit\tcan\tbe\trandomly\tswitched\toff\tinputs,\tjust like\tin\tdropout\t(introduced\tin\tChapter\t11).\tFigure\t15-9\tshows\tboth\toptions.\n\nFigure\t15-9.\tDenoising\tautoencoders,\twith\tGaussian\tnoise\t(left)\tor\tdropout\t(right)\n\nTensorFlow\tImplementation Implementing\tdenoising\tautoencoders\tin\tTensorFlow\tis\tnot\ttoo\thard.\tLet’s\tstart\twith\tGaussian\tnoise.\tIt’s really\tjust\tlike\ttraining\ta\tregular\tautoencoder,\texcept\tyou\tadd\tnoise\tto\tthe\tinputs,\tand\tthe\treconstruction loss\tis\tcalculated\tbased\ton\tthe\toriginal\tinputs:\n\nnoise_level\t=\t1.0 X\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs]) X_noisy\t=\tX\t+\tnoise_level\t*\ttf.random_normal(tf.shape(X))\n\nhidden1\t=\ttf.layers.dense(X_noisy,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden1\") [...] reconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX))\t#\tMSE [...]\n\nWARNING\n\nSince\tthe\tshape\tof\tX\tis\tonly\tpartially\tdefined\tduring\tthe\tconstruction\tphase,\twe\tcannot\tknow\tin\tadvance\tthe\tshape\tof\tthe\tnoise that\twe\tmust\tadd\tto\tX.\tWe\tcannot\tcall\tX.get_shape()\tbecause\tthis\twould\tjust\treturn\tthe\tpartially\tdefined\tshape\tof\tX\t([None, n_inputs]),\tand\trandom_normal()\texpects\ta\tfully\tdefined\tshape\tso\tit\twould\traise\tan\texception.\tInstead,\twe\tcall\ttf.shape(X), which\tcreates\tan\toperation\tthat\twill\treturn\tthe\tshape\tof\tX\tat\truntime,\twhich\twill\tbe\tfully\tdefined\tat\tthat\tpoint.\n\nImplementing\tthe\tdropout\tversion,\twhich\tis\tmore\tcommon,\tis\tnot\tmuch\tharder:\n\ndropout_rate\t=\t0.3\n\ntraining\t=\ttf.placeholder_with_default(False,\tshape=(),\tname='training')\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs]) X_drop\t=\ttf.layers.dropout(X,\tdropout_rate,\ttraining=training)\n\nhidden1\t=\ttf.layers.dense(X_drop,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden1\") [...] reconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX))\t#\tMSE [...]\n\nDuring\ttraining\twe\tmust\tset\ttraining\tto\tTrue\t(as\texplained\tin\tChapter\t11)\tusing\tthe\tfeed_dict:\n\nsess.run(training_op,\tfeed_dict={X:\tX_batch,\ttraining:\tTrue})\n\nDuring\ttesting\tit\tis\tnot\tnecessary\tto\tset\ttraining\tto\tFalse,\tsince\twe\tset\tthat\tas\tthe\tdefault\tin\tthe\tcall\tto the\tplaceholder_with_default()\tfunction.\n\nSparse\tAutoencoders Another\tkind\tof\tconstraint\tthat\toften\tleads\tto\tgood\tfeature\textraction\tis\tsparsity:\tby\tadding\tan\tappropriate term\tto\tthe\tcost\tfunction,\tthe\tautoencoder\tis\tpushed\tto\treduce\tthe\tnumber\tof\tactive\tneurons\tin\tthe\tcoding layer.\tFor\texample,\tit\tmay\tbe\tpushed\tto\thave\ton\taverage\tonly\t5%\tsignificantly\tactive\tneurons\tin\tthe coding\tlayer.\tThis\tforces\tthe\tautoencoder\tto\trepresent\teach\tinput\tas\ta\tcombination\tof\ta\tsmall\tnumber\tof activations.\tAs\ta\tresult,\teach\tneuron\tin\tthe\tcoding\tlayer\ttypically\tends\tup\trepresenting\ta\tuseful\tfeature\t(if you\tcould\tspeak\tonly\ta\tfew\twords\tper\tmonth,\tyou\twould\tprobably\ttry\tto\tmake\tthem\tworth\tlistening\tto).\n\nIn\torder\tto\tfavor\tsparse\tmodels,\twe\tmust\tfirst\tmeasure\tthe\tactual\tsparsity\tof\tthe\tcoding\tlayer\tat\teach training\titeration.\tWe\tdo\tso\tby\tcomputing\tthe\taverage\tactivation\tof\teach\tneuron\tin\tthe\tcoding\tlayer,\tover the\twhole\ttraining\tbatch.\tThe\tbatch\tsize\tmust\tnot\tbe\ttoo\tsmall,\tor\telse\tthe\tmean\twill\tnot\tbe\taccurate.\n\nOnce\twe\thave\tthe\tmean\tactivation\tper\tneuron,\twe\twant\tto\tpenalize\tthe\tneurons\tthat\tare\ttoo\tactive\tby adding\ta\tsparsity\tloss\tto\tthe\tcost\tfunction.\tFor\texample,\tif\twe\tmeasure\tthat\ta\tneuron\thas\tan\taverage activation\tof\t0.3,\tbut\tthe\ttarget\tsparsity\tis\t0.1,\tit\tmust\tbe\tpenalized\tto\tactivate\tless.\tOne\tapproach\tcould be\tsimply\tadding\tthe\tsquared\terror\t(0.3\t–\t0.1)2\tto\tthe\tcost\tfunction,\tbut\tin\tpractice\ta\tbetter\tapproach\tis\tto use\tthe\tKullback–Leibler\tdivergence\t(briefly\tdiscussed\tin\tChapter\t4),\twhich\thas\tmuch\tstronger\tgradients than\tthe\tMean\tSquared\tError,\tas\tyou\tcan\tsee\tin\tFigure\t15-10.\n\nFigure\t15-10.\tSparsity\tloss\n\nGiven\ttwo\tdiscrete\tprobability\tdistributions\tP\tand\tQ,\tthe\tKL\tdivergence\tbetween\tthese\tdistributions, noted\tDKL(P\t\t Q),\tcan\tbe\tcomputed\tusing\tEquation\t15-1.\n\nEquation\t15-1.\tKullback–Leibler\tdivergence\n\nIn\tour\tcase,\twe\twant\tto\tmeasure\tthe\tdivergence\tbetween\tthe\ttarget\tprobability\tp\tthat\ta\tneuron\tin\tthe coding\tlayer\twill\tactivate,\tand\tthe\tactual\tprobability\tq\t(i.e.,\tthe\tmean\tactivation\tover\tthe\ttraining\tbatch). So\tthe\tKL\tdivergence\tsimplifies\tto\tEquation\t15-2.\n\nEquation\t15-2.\tKL\tdivergence\tbetween\tthe\ttarget\tsparsity\tp\tand\tthe\tactual\tsparsity\tq\n\nOnce\twe\thave\tcomputed\tthe\tsparsity\tloss\tfor\teach\tneuron\tin\tthe\tcoding\tlayer,\twe\tjust\tsum\tup\tthese\tlosses, and\tadd\tthe\tresult\tto\tthe\tcost\tfunction.\tIn\torder\tto\tcontrol\tthe\trelative\timportance\tof\tthe\tsparsity\tloss\tand the\treconstruction\tloss,\twe\tcan\tmultiply\tthe\tsparsity\tloss\tby\ta\tsparsity\tweight\thyperparameter.\tIf\tthis weight\tis\ttoo\thigh,\tthe\tmodel\twill\tstick\tclosely\tto\tthe\ttarget\tsparsity,\tbut\tit\tmay\tnot\treconstruct\tthe\tinputs properly,\tmaking\tthe\tmodel\tuseless.\tConversely,\tif\tit\tis\ttoo\tlow,\tthe\tmodel\twill\tmostly\tignore\tthe\tsparsity objective\tand\tit\twill\tnot\tlearn\tany\tinteresting\tfeatures.\n\nTensorFlow\tImplementation We\tnow\thave\tall\twe\tneed\tto\timplement\ta\tsparse\tautoencoder\tusing\tTensorFlow:\n\ndef\tkl_divergence(p,\tq): \t\t\t\treturn\tp\t*\ttf.log(p\t/\tq)\t+\t(1\t-\tp)\t*\ttf.log((1\t-\tp)\t/\t(1\t-\tq))\n\nlearning_rate\t=\t0.01 sparsity_target\t=\t0.1 sparsity_weight\t=\t0.2\n\n[...]\t#\tBuild\ta\tnormal\tautoencoder\t(in\tthis\texample\tthe\tcoding\tlayer\tis\thidden1)\n\nhidden1_mean\t=\ttf.reduce_mean(hidden1,\taxis=0)\t#\tbatch\tmean sparsity_loss\t=\ttf.reduce_sum(kl_divergence(sparsity_target,\thidden1_mean)) reconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX))\t#\tMSE loss\t=\treconstruction_loss\t+\tsparsity_weight\t*\tsparsity_loss optimizer\t=\ttf.train.AdamOptimizer(learning_rate) training_op\t=\toptimizer.minimize(loss)\n\nAn\timportant\tdetail\tis\tthe\tfact\tthat\tthe\tactivations\tof\tthe\tcoding\tlayer\tmust\tbe\tbetween\t0\tand\t1\t(but\tnot equal\tto\t0\tor\t1),\tor\telse\tthe\tKL\tdivergence\twill\treturn\tNaN\t(Not\ta\tNumber).\tA\tsimple\tsolution\tis\tto\tuse the\tlogistic\tactivation\tfunction\tfor\tthe\tcoding\tlayer:\n\nhidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.sigmoid)\n\nOne\tsimple\ttrick\tcan\tspeed\tup\tconvergence:\tinstead\tof\tusing\tthe\tMSE,\twe\tcan\tchoose\ta\treconstruction loss\tthat\twill\thave\tlarger\tgradients.\tCross\tentropy\tis\toften\ta\tgood\tchoice.\tTo\tuse\tit,\twe\tmust\tnormalize\tthe inputs\tto\tmake\tthem\ttake\ton\tvalues\tfrom\t0\tto\t1,\tand\tuse\tthe\tlogistic\tactivation\tfunction\tin\tthe\toutput\tlayer so\tthe\toutputs\talso\ttake\ton\tvalues\tfrom\t0\tto\t1.\tTensorFlow’s\tsigmoid_cross_entropy_with_logits() function\ttakes\tcare\tof\tefficiently\tapplying\tthe\tlogistic\t(sigmoid)\tactivation\tfunction\tto\tthe\toutputs\tand computing\tthe\tcross\tentropy:\n\n[...] logits\t=\ttf.layers.dense(hidden1,\tn_outputs) outputs\t=\ttf.nn.sigmoid(logits)\n\nxentropy\t=\ttf.nn.sigmoid_cross_entropy_with_logits(labels=X,\tlogits=logits) reconstruction_loss\t=\ttf.reduce_sum(xentropy)\n\nNote\tthat\tthe\toutputs\toperation\tis\tnot\tneeded\tduring\ttraining\t(we\tuse\tit\tonly\twhen\twe\twant\tto\tlook\tat\tthe reconstructions).\n\nVariational\tAutoencoders Another\timportant\tcategory\tof\tautoencoders\twas\tintroduced\tin\t2014\tby\tDiederik\tKingma\tand\tMax Welling,5\tand\thas\tquickly\tbecome\tone\tof\tthe\tmost\tpopular\ttypes\tof\tautoencoders:\tvariational autoencoders.\n\nThey\tare\tquite\tdifferent\tfrom\tall\tthe\tautoencoders\twe\thave\tdiscussed\tso\tfar,\tin\tparticular:\n\nThey\tare\tprobabilistic\tautoencoders,\tmeaning\tthat\ttheir\toutputs\tare\tpartly\tdetermined\tby\tchance, even\tafter\ttraining\t(as\topposed\tto\tdenoising\tautoencoders,\twhich\tuse\trandomness\tonly\tduring training).\n\nMost\timportantly,\tthey\tare\tgenerative\tautoencoders,\tmeaning\tthat\tthey\tcan\tgenerate\tnew\tinstances that\tlook\tlike\tthey\twere\tsampled\tfrom\tthe\ttraining\tset.\n\nBoth\tthese\tproperties\tmake\tthem\trather\tsimilar\tto\tRBMs\t(see\tAppendix\tE),\tbut\tthey\tare\teasier\tto\ttrain\tand the\tsampling\tprocess\tis\tmuch\tfaster\t(with\tRBMs\tyou\tneed\tto\twait\tfor\tthe\tnetwork\tto\tstabilize\tinto\ta “thermal\tequilibrium”\tbefore\tyou\tcan\tsample\ta\tnew\tinstance).\n\nLet’s\ttake\ta\tlook\tat\thow\tthey\twork.\tFigure\t15-11\t(left)\tshows\ta\tvariational\tautoencoder.\tYou\tcan recognize,\tof\tcourse,\tthe\tbasic\tstructure\tof\tall\tautoencoders,\twith\tan\tencoder\tfollowed\tby\ta\tdecoder\t(in this\texample,\tthey\tboth\thave\ttwo\thidden\tlayers),\tbut\tthere\tis\ta\ttwist:\tinstead\tof\tdirectly\tproducing\ta coding\tfor\ta\tgiven\tinput,\tthe\tencoder\tproduces\ta\tmean\tcoding\tμ\tand\ta\tstandard\tdeviation\tσ.\tThe\tactual coding\tis\tthen\tsampled\trandomly\tfrom\ta\tGaussian\tdistribution\twith\tmean\tμ\tand\tstandard\tdeviation\tσ. After\tthat\tthe\tdecoder\tjust\tdecodes\tthe\tsampled\tcoding\tnormally.\tThe\tright\tpart\tof\tthe\tdiagram\tshows\ta training\tinstance\tgoing\tthrough\tthis\tautoencoder.\tFirst,\tthe\tencoder\tproduces\tμ\tand\tσ,\tthen\ta\tcoding\tis sampled\trandomly\t(notice\tthat\tit\tis\tnot\texactly\tlocated\tat\tμ),\tand\tfinally\tthis\tcoding\tis\tdecoded,\tand\tthe final\toutput\tresembles\tthe\ttraining\tinstance.\n\nFigure\t15-11.\tVariational\tautoencoder\t(left),\tand\tan\tinstance\tgoing\tthrough\tit\t(right)\n\nAs\tyou\tcan\tsee\ton\tthe\tdiagram,\talthough\tthe\tinputs\tmay\thave\ta\tvery\tconvoluted\tdistribution,\ta\tvariational autoencoder\ttends\tto\tproduce\tcodings\tthat\tlook\tas\tthough\tthey\twere\tsampled\tfrom\ta\tsimple\tGaussian distribution:6\tduring\ttraining,\tthe\tcost\tfunction\t(discussed\tnext)\tpushes\tthe\tcodings\tto\tgradually\tmigrate within\tthe\tcoding\tspace\t(also\tcalled\tthe\tlatent\tspace)\tto\toccupy\ta\troughly\t(hyper)spherical\tregion\tthat looks\tlike\ta\tcloud\tof\tGaussian\tpoints.\tOne\tgreat\tconsequence\tis\tthat\tafter\ttraining\ta\tvariational autoencoder,\tyou\tcan\tvery\teasily\tgenerate\ta\tnew\tinstance:\tjust\tsample\ta\trandom\tcoding\tfrom\tthe\tGaussian distribution,\tdecode\tit,\tand\tvoilà!\n\nSo\tlet’s\tlook\tat\tthe\tcost\tfunction.\tIt\tis\tcomposed\tof\ttwo\tparts.\tThe\tfirst\tis\tthe\tusual\treconstruction\tloss\tthat pushes\tthe\tautoencoder\tto\treproduce\tits\tinputs\t(we\tcan\tuse\tcross\tentropy\tfor\tthis,\tas\tdiscussed\tearlier). The\tsecond\tis\tthe\tlatent\tloss\tthat\tpushes\tthe\tautoencoder\tto\thave\tcodings\tthat\tlook\tas\tthough\tthey\twere sampled\tfrom\ta\tsimple\tGaussian\tdistribution,\tfor\twhich\twe\tuse\tthe\tKL\tdivergence\tbetween\tthe\ttarget distribution\t(the\tGaussian\tdistribution)\tand\tthe\tactual\tdistribution\tof\tthe\tcodings.\tThe\tmath\tis\ta\tbit\tmore complex\tthan\tearlier,\tin\tparticular\tbecause\tof\tthe\tGaussian\tnoise,\twhich\tlimits\tthe\tamount\tof\tinformation that\tcan\tbe\ttransmitted\tto\tthe\tcoding\tlayer\t(thus\tpushing\tthe\tautoencoder\tto\tlearn\tuseful\tfeatures).\tLuckily, the\tequations\tsimplify\tto\tthe\tfollowing\tcode\tfor\tthe\tlatent\tloss:7\n\neps\t=\t1e-10\t\t#\tsmoothing\tterm\tto\tavoid\tcomputing\tlog(0)\twhich\tis\tNaN latent_loss\t=\t0.5\t*\ttf.reduce_sum( \t\t\t\ttf.square(hidden3_sigma)\t+\ttf.square(hidden3_mean) \t\t\t\t-\t1\t-\ttf.log(eps\t+\ttf.square(hidden3_sigma)))\n\nOne\tcommon\tvariant\tis\tto\ttrain\tthe\tencoder\tto\toutput\tγ\t=\tlog(σ2)\trather\tthan\tσ.\tWherever\twe\tneed\tσ\twe\n\ncan\tjust\tcompute\n\n.\tThis\tmakes\tit\ta\tbit\teasier\tfor\tthe\tencoder\tto\tcapture\tsigmas\tof\tdifferent\n\nscales,\tand\tthus\tit\thelps\tspeed\tup\tconvergence.\tThe\tlatent\tloss\tends\tup\ta\tbit\tsimpler:\n\nlatent_loss\t=\t0.5\t*\ttf.reduce_sum( \t\t\t\ttf.exp(hidden3_gamma)\t+\ttf.square(hidden3_mean)\t-\t1\t-\thidden3_gamma)\n\nThe\tfollowing\tcode\tbuilds\tthe\tvariational\tautoencoder\tshown\tin\tFigure\t15-11\t(left),\tusing\tthe\tlog(σ2) variant:\n\nfrom\tfunctools\timport\tpartial\n\nn_inputs\t=\t28\t*\t28 n_hidden1\t=\t500 n_hidden2\t=\t500 n_hidden3\t=\t20\t\t#\tcodings n_hidden4\t=\tn_hidden2 n_hidden5\t=\tn_hidden1 n_outputs\t=\tn_inputs learning_rate\t=\t0.001\n\ninitializer\t=\ttf.contrib.layers.variance_scaling_initializer() my_dense_layer\t=\tpartial( \t\t\t\ttf.layers.dense, \t\t\t\tactivation=tf.nn.elu, \t\t\t\tkernel_initializer=initializer)\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_inputs]) hidden1\t=\tmy_dense_layer(X,\tn_hidden1) hidden2\t=\tmy_dense_layer(hidden1,\tn_hidden2) hidden3_mean\t=\tmy_dense_layer(hidden2,\tn_hidden3,\tactivation=None) hidden3_gamma\t=\tmy_dense_layer(hidden2,\tn_hidden3,\tactivation=None) noise\t=\ttf.random_normal(tf.shape(hidden3_gamma),\tdtype=tf.float32) hidden3\t=\thidden3_mean\t+\ttf.exp(0.5\t*\thidden3_gamma)\t*\tnoise hidden4\t=\tmy_dense_layer(hidden3,\tn_hidden4) hidden5\t=\tmy_dense_layer(hidden4,\tn_hidden5) logits\t=\tmy_dense_layer(hidden5,\tn_outputs,\tactivation=None) outputs\t=\ttf.sigmoid(logits)\n\nxentropy\t=\ttf.nn.sigmoid_cross_entropy_with_logits(labels=X,\tlogits=logits) reconstruction_loss\t=\ttf.reduce_sum(xentropy) latent_loss\t=\t0.5\t*\ttf.reduce_sum( \t\t\t\ttf.exp(hidden3_gamma)\t+\ttf.square(hidden3_mean)\t-\t1\t-\thidden3_gamma) loss\t=\treconstruction_loss\t+\tlatent_loss\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate=learning_rate) training_op\t=\toptimizer.minimize(loss)\n\ninit\t=\ttf.global_variables_initializer() saver\t=\ttf.train.Saver()\n\nGenerating\tDigits Now\tlet’s\tuse\tthis\tvariational\tautoencoder\tto\tgenerate\timages\tthat\tlook\tlike\thandwritten\tdigits.\tAll\twe need\tto\tdo\tis\ttrain\tthe\tmodel,\tthen\tsample\trandom\tcodings\tfrom\ta\tGaussian\tdistribution\tand\tdecode\tthem.\n\nimport\tnumpy\tas\tnp\n\nn_digits\t=\t60 n_epochs\t=\t50 batch_size\t=\t150\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tn_batches\t=\tmnist.train.num_examples\t//\tbatch_size \t\t\t\t\t\t\t\tfor\titeration\tin\trange(n_batches): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch})\n\ncodings_rnd\t=\tnp.random.normal(size=[n_digits,\tn_hidden3]) \t\t\t\toutputs_val\t=\toutputs.eval(feed_dict={hidden3:\tcodings_rnd})\n\nThat’s\tit.\tNow\twe\tcan\tsee\twhat\tthe\t“handwritten”\tdigits\tproduced\tby\tthe\tautoencoder\tlook\tlike\t(see Figure\t15-12):\n\nfor\titeration\tin\trange(n_digits): \t\t\t\tplt.subplot(n_digits,\t10,\titeration\t+\t1) \t\t\t\tplot_image(outputs_val[iteration])\n\nFigure\t15-12.\tImages\tof\thandwritten\tdigits\tgenerated\tby\tthe\tvariational\tautoencoder\n\nA\tmajority\tof\tthese\tdigits\tlook\tpretty\tconvincing,\twhile\ta\tfew\tare\trather\t“creative.”\tBut\tdon’t\tbe\ttoo\tharsh on\tthe\tautoencoder\t—\tit\tonly\tstarted\tlearning\tless\tthan\tan\thour\tago.\tGive\tit\ta\tbit\tmore\ttraining\ttime,\tand those\tdigits\twill\tlook\tbetter\tand\tbetter.\n\nOther\tAutoencoders The\tamazing\tsuccesses\tof\tsupervised\tlearning\tin\timage\trecognition,\tspeech\trecognition,\ttext\ttranslation, and\tmore\thave\tsomewhat\tovershadowed\tunsupervised\tlearning,\tbut\tit\tis\tactually\tbooming.\tNew architectures\tfor\tautoencoders\tand\tother\tunsupervised\tlearning\talgorithms\tare\tinvented\tregularly,\tso\tmuch so\tthat\twe\tcannot\tcover\tthem\tall\tin\tthis\tbook.\tHere\tis\ta\tbrief\t(by\tno\tmeans\texhaustive)\toverview\tof\ta\tfew more\ttypes\tof\tautoencoders\tthat\tyou\tmay\twant\tto\tcheck\tout:\n\nContractive\tautoencoder\t(CAE)8\n\nThe\tautoencoder\tis\tconstrained\tduring\ttraining\tso\tthat\tthe\tderivatives\tof\tthe\tcodings\twith\tregards\tto the\tinputs\tare\tsmall.\tIn\tother\twords,\ttwo\tsimilar\tinputs\tmust\thave\tsimilar\tcodings.\n\nStacked\tconvolutional\tautoencoders9\n\nAutoencoders\tthat\tlearn\tto\textract\tvisual\tfeatures\tby\treconstructing\timages\tprocessed\tthrough convolutional\tlayers.\n\nGenerative\tstochastic\tnetwork\t(GSN)10\n\nA\tgeneralization\tof\tdenoising\tautoencoders,\twith\tthe\tadded\tcapability\tto\tgenerate\tdata.\n\nWinner-take-all\t(WTA)\tautoencoder11\n\nDuring\ttraining,\tafter\tcomputing\tthe\tactivations\tof\tall\tthe\tneurons\tin\tthe\tcoding\tlayer,\tonly\tthe\ttop\tk% activations\tfor\teach\tneuron\tover\tthe\ttraining\tbatch\tare\tpreserved,\tand\tthe\trest\tare\tset\tto\tzero. Naturally\tthis\tleads\tto\tsparse\tcodings.\tMoreover,\ta\tsimilar\tWTA\tapproach\tcan\tbe\tused\tto\tproduce sparse\tconvolutional\tautoencoders.\n\nAdversarial\tautoencoders12\n\nOne\tnetwork\tis\ttrained\tto\treproduce\tits\tinputs,\tand\tat\tthe\tsame\ttime\tanother\tis\ttrained\tto\tfind\tinputs that\tthe\tfirst\tnetwork\tis\tunable\tto\tproperly\treconstruct.\tThis\tpushes\tthe\tfirst\tautoencoder\tto\tlearn robust\tcodings.\n\nExercises\n\n1.\t What\tare\tthe\tmain\ttasks\tthat\tautoencoders\tare\tused\tfor?\n\n2.\t Suppose\tyou\twant\tto\ttrain\ta\tclassifier\tand\tyou\thave\tplenty\tof\tunlabeled\ttraining\tdata,\tbut\tonly\ta\tfew thousand\tlabeled\tinstances.\tHow\tcan\tautoencoders\thelp?\tHow\twould\tyou\tproceed?\n\n3.\t If\tan\tautoencoder\tperfectly\treconstructs\tthe\tinputs,\tis\tit\tnecessarily\ta\tgood\tautoencoder?\tHow\tcan you\tevaluate\tthe\tperformance\tof\tan\tautoencoder?\n\n4.\t What\tare\tundercomplete\tand\tovercomplete\tautoencoders?\tWhat\tis\tthe\tmain\trisk\tof\tan\texcessively undercomplete\tautoencoder?\tWhat\tabout\tthe\tmain\trisk\tof\tan\tovercomplete\tautoencoder?\n\n5.\t How\tdo\tyou\ttie\tweights\tin\ta\tstacked\tautoencoder?\tWhat\tis\tthe\tpoint\tof\tdoing\tso?\n\n6.\t What\tis\ta\tcommon\ttechnique\tto\tvisualize\tfeatures\tlearned\tby\tthe\tlower\tlayer\tof\ta\tstacked autoencoder?\tWhat\tabout\thigher\tlayers?\n\n7.\t What\tis\ta\tgenerative\tmodel?\tCan\tyou\tname\ta\ttype\tof\tgenerative\tautoencoder?\n\n8.\t Let’s\tuse\ta\tdenoising\tautoencoder\tto\tpretrain\tan\timage\tclassifier:\n\nYou\tcan\tuse\tMNIST\t(simplest),\tor\tanother\tlarge\tset\tof\timages\tsuch\tas\tCIFAR10\tif\tyou\twant\ta bigger\tchallenge.\tIf\tyou\tchoose\tCIFAR10,\tyou\tneed\tto\twrite\tcode\tto\tload\tbatches\tof\timages\tfor training.\tIf\tyou\twant\tto\tskip\tthis\tpart,\tTensorFlow’s\tmodel\tzoo\tcontains\ttools\tto\tdo\tjust\tthat.\n\nSplit\tthe\tdataset\tinto\ta\ttraining\tset\tand\ta\ttest\tset.\tTrain\ta\tdeep\tdenoising\tautoencoder\ton\tthe\tfull training\tset.\n\nCheck\tthat\tthe\timages\tare\tfairly\twell\treconstructed,\tand\tvisualize\tthe\tlow-level\tfeatures. Visualize\tthe\timages\tthat\tmost\tactivate\teach\tneuron\tin\tthe\tcoding\tlayer.\n\nBuild\ta\tclassification\tdeep\tneural\tnetwork,\treusing\tthe\tlower\tlayers\tof\tthe\tautoencoder.\tTrain\tit using\tonly\t10%\tof\tthe\ttraining\tset.\tCan\tyou\tget\tit\tto\tperform\tas\twell\tas\tthe\tsame\tclassifier trained\ton\tthe\tfull\ttraining\tset?\n\n9.\t Semantic\thashing,\tintroduced\tin\t2008\tby\tRuslan\tSalakhutdinov\tand\tGeoffrey\tHinton,13\tis\ta technique\tused\tfor\tefficient\tinformation\tretrieval:\ta\tdocument\t(e.g.,\tan\timage)\tis\tpassed\tthrough\ta system,\ttypically\ta\tneural\tnetwork,\twhich\toutputs\ta\tfairly\tlow-dimensional\tbinary\tvector\t(e.g.,\t30 bits).\tTwo\tsimilar\tdocuments\tare\tlikely\tto\thave\tidentical\tor\tvery\tsimilar\thashes.\tBy\tindexing\teach document\tusing\tits\thash,\tit\tis\tpossible\tto\tretrieve\tmany\tdocuments\tsimilar\tto\ta\tparticular\tdocument almost\tinstantly,\teven\tif\tthere\tare\tbillions\tof\tdocuments:\tjust\tcompute\tthe\thash\tof\tthe\tdocument\tand look\tup\tall\tdocuments\twith\tthat\tsame\thash\t(or\thashes\tdiffering\tby\tjust\tone\tor\ttwo\tbits).\tLet’s implement\tsemantic\thashing\tusing\ta\tslightly\ttweaked\tstacked\tautoencoder:\n\nCreate\ta\tstacked\tautoencoder\tcontaining\ttwo\thidden\tlayers\tbelow\tthe\tcoding\tlayer,\tand\ttrain\tit on\tthe\timage\tdataset\tyou\tused\tin\tthe\tprevious\texercise.\tThe\tcoding\tlayer\tshould\tcontain\t30 neurons\tand\tuse\tthe\tlogistic\tactivation\tfunction\tto\toutput\tvalues\tbetween\t0\tand\t1.\tAfter\ttraining,\n\nto\tproduce\tthe\thash\tof\tan\timage,\tyou\tcan\tsimply\trun\tit\tthrough\tthe\tautoencoder,\ttake\tthe\toutput of\tthe\tcoding\tlayer,\tand\tround\tevery\tvalue\tto\tthe\tclosest\tinteger\t(0\tor\t1).\n\nOne\tneat\ttrick\tproposed\tby\tSalakhutdinov\tand\tHinton\tis\tto\tadd\tGaussian\tnoise\t(with\tzero mean)\tto\tthe\tinputs\tof\tthe\tcoding\tlayer,\tduring\ttraining\tonly.\tIn\torder\tto\tpreserve\ta\thigh\tsignal- to-noise\tratio,\tthe\tautoencoder\twill\tlearn\tto\tfeed\tlarge\tvalues\tto\tthe\tcoding\tlayer\t(so\tthat\tthe noise\tbecomes\tnegligible).\tIn\tturn,\tthis\tmeans\tthat\tthe\tlogistic\tfunction\tof\tthe\tcoding\tlayer\twill likely\tsaturate\tat\t0\tor\t1.\tAs\ta\tresult,\trounding\tthe\tcodings\tto\t0\tor\t1\twon’t\tdistort\tthem\ttoo\tmuch, and\tthis\twill\timprove\tthe\treliability\tof\tthe\thashes.\n\nCompute\tthe\thash\tof\tevery\timage,\tand\tsee\tif\timages\twith\tidentical\thashes\tlook\talike.\tSince MNIST\tand\tCIFAR10\tare\tlabeled,\ta\tmore\tobjective\tway\tto\tmeasure\tthe\tperformance\tof\tthe autoencoder\tfor\tsemantic\thashing\tis\tto\tensure\tthat\timages\twith\tthe\tsame\thash\tgenerally\thave\tthe same\tclass.\tOne\tway\tto\tdo\tthis\tis\tto\tmeasure\tthe\taverage\tGini\tpurity\t(introduced\tin\tChapter\t6) of\tthe\tsets\tof\timages\twith\tidentical\t(or\tvery\tsimilar)\thashes.\n\nTry\tfine-tuning\tthe\thyperparameters\tusing\tcross-validation.\n\nNote\tthat\twith\ta\tlabeled\tdataset,\tanother\tapproach\tis\tto\ttrain\ta\tconvolutional\tneural\tnetwork (see\tChapter\t13)\tfor\tclassification,\tthen\tuse\tthe\tlayer\tbelow\tthe\toutput\tlayer\tto\tproduce\tthe hashes.\tSee\tJinma\tGua\tand\tJianmin\tLi’s\t2015\tpaper.14\tSee\tif\tthat\tperforms\tbetter.\n\n10.\t Train\ta\tvariational\tautoencoder\ton\tthe\timage\tdataset\tused\tin\tthe\tprevious\texercises\t(MNIST\tor CIFAR10),\tand\tmake\tit\tgenerate\timages.\tAlternatively,\tyou\tcan\ttry\tto\tfind\tan\tunlabeled\tdataset\tthat you\tare\tinterested\tin\tand\tsee\tif\tyou\tcan\tgenerate\tnew\tsamples.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“Perception\tin\tchess,”\tW.\tChase\tand\tH.\tSimon\t(1973).\n\n2\n\n“Greedy\tLayer-Wise\tTraining\tof\tDeep\tNetworks,”\tY.\tBengio\tet\tal.\t(2007).\n\n3\n\n“Extracting\tand\tComposing\tRobust\tFeatures\twith\tDenoising\tAutoencoders,”\tP.\tVincent\tet\tal.\t(2008).\n\n4\n\n“Stacked\tDenoising\tAutoencoders:\tLearning\tUseful\tRepresentations\tin\ta\tDeep\tNetwork\twith\ta\tLocal\tDenoising\tCriterion,”\tP.\tVincent\tet al.\t(2010).\n\n5\n\n“Auto-Encoding\tVariational\tBayes,”\tD.\tKingma\tand\tM.\tWelling\t(2014).\n\n6\n\nVariational\tautoencoders\tare\tactually\tmore\tgeneral;\tthe\tcodings\tare\tnot\tlimited\tto\tGaussian\tdistributions.\n\n7\n\nFor\tmore\tmathematical\tdetails,\tcheck\tout\tthe\toriginal\tpaper\ton\tvariational\tautoencoders,\tor\tCarl\tDoersch’s\tgreat\ttutorial\t(2016).\n\n8\n\n“Contractive\tAuto-Encoders:\tExplicit\tInvariance\tDuring\tFeature\tExtraction,”\tS.\tRifai\tet\tal.\t(2011).\n\n9\n\n“Stacked\tConvolutional\tAuto-Encoders\tfor\tHierarchical\tFeature\tExtraction,”\tJ.\tMasci\tet\tal.\t(2011).\n\n10\n\n“GSNs:\tGenerative\tStochastic\tNetworks,”\tG.\tAlain\tet\tal.\t(2015).\n\n11\n\n“Winner-Take-All\tAutoencoders,”\tA.\tMakhzani\tand\tB.\tFrey\t(2015).\n\n12\n\n“Adversarial\tAutoencoders,”\tA.\tMakhzani\tet\tal.\t(2016).\n\n13\n\n“Semantic\tHashing,”\tR.\tSalakhutdinov\tand\tG.\tHinton\t(2008).\n\n14\n\n“CNN\tBased\tHashing\tfor\tImage\tRetrieval,”\tJ.\tGua\tand\tJ.\tLi\t(2015).\n\nChapter\t16.\tReinforcement\tLearning\n\nReinforcement\tLearning\t(RL)\tis\tone\tof\tthe\tmost\texciting\tfields\tof\tMachine\tLearning\ttoday,\tand\talso\tone of\tthe\toldest.\tIt\thas\tbeen\taround\tsince\tthe\t1950s,\tproducing\tmany\tinteresting\tapplications\tover\tthe\tyears,1 in\tparticular\tin\tgames\t(e.g.,\tTD-Gammon,\ta\tBackgammon\tplaying\tprogram)\tand\tin\tmachine\tcontrol,\tbut seldom\tmaking\tthe\theadline\tnews.\tBut\ta\trevolution\ttook\tplace\tin\t2013\twhen\tresearchers\tfrom\tan\tEnglish startup\tcalled\tDeepMind\tdemonstrated\ta\tsystem\tthat\tcould\tlearn\tto\tplay\tjust\tabout\tany\tAtari\tgame\tfrom scratch,2\teventually\toutperforming\thumans3\tin\tmost\tof\tthem,\tusing\tonly\traw\tpixels\tas\tinputs\tand\twithout any\tprior\tknowledge\tof\tthe\trules\tof\tthe\tgames.4\tThis\twas\tthe\tfirst\tof\ta\tseries\tof\tamazing\tfeats,\tculminating in\tMarch\t2016\twith\tthe\tvictory\tof\ttheir\tsystem\tAlphaGo\tagainst\tLee\tSedol,\tthe\tworld\tchampion\tof\tthe game\tof\tGo.\tNo\tprogram\thad\tever\tcome\tclose\tto\tbeating\ta\tmaster\tof\tthis\tgame,\tlet\talone\tthe\tworld champion.\tToday\tthe\twhole\tfield\tof\tRL\tis\tboiling\twith\tnew\tideas,\twith\ta\twide\trange\tof\tapplications. DeepMind\twas\tbought\tby\tGoogle\tfor\tover\t500\tmillion\tdollars\tin\t2014.\n\nSo\thow\tdid\tthey\tdo\tit?\tWith\thindsight\tit\tseems\trather\tsimple:\tthey\tapplied\tthe\tpower\tof\tDeep\tLearning\tto the\tfield\tof\tReinforcement\tLearning,\tand\tit\tworked\tbeyond\ttheir\twildest\tdreams.\tIn\tthis\tchapter\twe\twill first\texplain\twhat\tReinforcement\tLearning\tis\tand\twhat\tit\tis\tgood\tat,\tand\tthen\twe\twill\tpresent\ttwo\tof\tthe most\timportant\ttechniques\tin\tdeep\tReinforcement\tLearning:\tpolicy\tgradients\tand\tdeep\tQ-networks (DQN),\tincluding\ta\tdiscussion\tof\tMarkov\tdecision\tprocesses\t(MDP).\tWe\twill\tuse\tthese\ttechniques\tto train\ta\tmodel\tto\tbalance\ta\tpole\ton\ta\tmoving\tcart,\tand\tanother\tto\tplay\tAtari\tgames.\tThe\tsame\ttechniques can\tbe\tused\tfor\ta\twide\tvariety\tof\ttasks,\tfrom\twalking\trobots\tto\tself-driving\tcars.\n\nLearning\tto\tOptimize\tRewards In\tReinforcement\tLearning,\ta\tsoftware\tagent\tmakes\tobservations\tand\ttakes\tactions\twithin\tan environment,\tand\tin\treturn\tit\treceives\trewards.\tIts\tobjective\tis\tto\tlearn\tto\tact\tin\ta\tway\tthat\twill\tmaximize its\texpected\tlong-term\trewards.\tIf\tyou\tdon’t\tmind\ta\tbit\tof\tanthropomorphism,\tyou\tcan\tthink\tof\tpositive rewards\tas\tpleasure,\tand\tnegative\trewards\tas\tpain\t(the\tterm\t“reward”\tis\ta\tbit\tmisleading\tin\tthis\tcase).\tIn short,\tthe\tagent\tacts\tin\tthe\tenvironment\tand\tlearns\tby\ttrial\tand\terror\tto\tmaximize\tits\tpleasure\tand\tminimize its\tpain.\n\nThis\tis\tquite\ta\tbroad\tsetting,\twhich\tcan\tapply\tto\ta\twide\tvariety\tof\ttasks.\tHere\tare\ta\tfew\texamples\t(see Figure\t16-1):\n\n1.\t The\tagent\tcan\tbe\tthe\tprogram\tcontrolling\ta\twalking\trobot.\tIn\tthis\tcase,\tthe\tenvironment\tis\tthe\treal world,\tthe\tagent\tobserves\tthe\tenvironment\tthrough\ta\tset\tof\tsensors\tsuch\tas\tcameras\tand\ttouch sensors,\tand\tits\tactions\tconsist\tof\tsending\tsignals\tto\tactivate\tmotors.\tIt\tmay\tbe\tprogrammed\tto\tget positive\trewards\twhenever\tit\tapproaches\tthe\ttarget\tdestination,\tand\tnegative\trewards\twhenever\tit wastes\ttime,\tgoes\tin\tthe\twrong\tdirection,\tor\tfalls\tdown.\n\n2.\t The\tagent\tcan\tbe\tthe\tprogram\tcontrolling\tMs.\tPac-Man.\tIn\tthis\tcase,\tthe\tenvironment\tis\ta\tsimulation of\tthe\tAtari\tgame,\tthe\tactions\tare\tthe\tnine\tpossible\tjoystick\tpositions\t(upper\tleft,\tdown,\tcenter,\tand so\ton),\tthe\tobservations\tare\tscreenshots,\tand\tthe\trewards\tare\tjust\tthe\tgame\tpoints.\n\n3.\t Similarly,\tthe\tagent\tcan\tbe\tthe\tprogram\tplaying\ta\tboard\tgame\tsuch\tas\tthe\tgame\tof\tGo.\n\n4.\t The\tagent\tdoes\tnot\thave\tto\tcontrol\ta\tphysically\t(or\tvirtually)\tmoving\tthing.\tFor\texample,\tit\tcan\tbe\ta smart\tthermostat,\tgetting\trewards\twhenever\tit\tis\tclose\tto\tthe\ttarget\ttemperature\tand\tsaves\tenergy,\tand negative\trewards\twhen\thumans\tneed\tto\ttweak\tthe\ttemperature,\tso\tthe\tagent\tmust\tlearn\tto\tanticipate human\tneeds.\n\n5.\t The\tagent\tcan\tobserve\tstock\tmarket\tprices\tand\tdecide\thow\tmuch\tto\tbuy\tor\tsell\tevery\tsecond. Rewards\tare\tobviously\tthe\tmonetary\tgains\tand\tlosses.\n\nFigure\t16-1.\tReinforcement\tLearning\texamples:\t(a)\twalking\trobot,\t(b)\tMs.\tPac-Man,\t(c)\tGo\tplayer,\t(d)\tthermostat,\t(e)\tautomatic trader5\n\nNote\tthat\tthere\tmay\tnot\tbe\tany\tpositive\trewards\tat\tall;\tfor\texample,\tthe\tagent\tmay\tmove\taround\tin\ta\tmaze, getting\ta\tnegative\treward\tat\tevery\ttime\tstep,\tso\tit\tbetter\tfind\tthe\texit\tas\tquickly\tas\tpossible!\tThere\tare many\tother\texamples\tof\ttasks\twhere\tReinforcement\tLearning\tis\twell\tsuited,\tsuch\tas\tself-driving\tcars, placing\tads\ton\ta\tweb\tpage,\tor\tcontrolling\twhere\tan\timage\tclassification\tsystem\tshould\tfocus\tits\tattention.",
      "page_number": 506
    },
    {
      "number": 16,
      "title": "Reinforcement\tLearning",
      "start_page": 534,
      "end_page": 718,
      "detection_method": "regex_chapter_title",
      "content": "Policy\tSearch The\talgorithm\tused\tby\tthe\tsoftware\tagent\tto\tdetermine\tits\tactions\tis\tcalled\tits\tpolicy.\tFor\texample,\tthe policy\tcould\tbe\ta\tneural\tnetwork\ttaking\tobservations\tas\tinputs\tand\toutputting\tthe\taction\tto\ttake\t(see Figure\t16-2).\n\nFigure\t16-2.\tReinforcement\tLearning\tusing\ta\tneural\tnetwork\tpolicy\n\nThe\tpolicy\tcan\tbe\tany\talgorithm\tyou\tcan\tthink\tof,\tand\tit\tdoes\tnot\teven\thave\tto\tbe\tdeterministic.\tFor example,\tconsider\ta\trobotic\tvacuum\tcleaner\twhose\treward\tis\tthe\tamount\tof\tdust\tit\tpicks\tup\tin\t30\tminutes. Its\tpolicy\tcould\tbe\tto\tmove\tforward\twith\tsome\tprobability\tp\tevery\tsecond,\tor\trandomly\trotate\tleft\tor right\twith\tprobability\t1\t–\tp.\tThe\trotation\tangle\twould\tbe\ta\trandom\tangle\tbetween\t–r\tand\t+r.\tSince\tthis policy\tinvolves\tsome\trandomness,\tit\tis\tcalled\ta\tstochastic\tpolicy.\tThe\trobot\twill\thave\tan\terratic trajectory,\twhich\tguarantees\tthat\tit\twill\teventually\tget\tto\tany\tplace\tit\tcan\treach\tand\tpick\tup\tall\tthe\tdust. The\tquestion\tis:\thow\tmuch\tdust\twill\tit\tpick\tup\tin\t30\tminutes?\n\nHow\twould\tyou\ttrain\tsuch\ta\trobot?\tThere\tare\tjust\ttwo\tpolicy\tparameters\tyou\tcan\ttweak:\tthe\tprobability p\tand\tthe\tangle\trange\tr.\tOne\tpossible\tlearning\talgorithm\tcould\tbe\tto\ttry\tout\tmany\tdifferent\tvalues\tfor these\tparameters,\tand\tpick\tthe\tcombination\tthat\tperforms\tbest\t(see\tFigure\t16-3).\tThis\tis\tan\texample\tof policy\tsearch,\tin\tthis\tcase\tusing\ta\tbrute\tforce\tapproach.\tHowever,\twhen\tthe\tpolicy\tspace\tis\ttoo\tlarge (which\tis\tgenerally\tthe\tcase),\tfinding\ta\tgood\tset\tof\tparameters\tthis\tway\tis\tlike\tsearching\tfor\ta\tneedle\tin\ta gigantic\thaystack.\n\nAnother\tway\tto\texplore\tthe\tpolicy\tspace\tis\tto\tuse\tgenetic\talgorithms.\tFor\texample,\tyou\tcould\trandomly create\ta\tfirst\tgeneration\tof\t100\tpolicies\tand\ttry\tthem\tout,\tthen\t“kill”\tthe\t80\tworst\tpolicies6\tand\tmake\tthe 20\tsurvivors\tproduce\t4\toffspring\teach.\tAn\toffspring\tis\tjust\ta\tcopy\tof\tits\tparent7\tplus\tsome\trandom variation.\tThe\tsurviving\tpolicies\tplus\ttheir\toffspring\ttogether\tconstitute\tthe\tsecond\tgeneration.\tYou\tcan continue\tto\titerate\tthrough\tgenerations\tthis\tway,\tuntil\tyou\tfind\ta\tgood\tpolicy.\n\nFigure\t16-3.\tFour\tpoints\tin\tpolicy\tspace\tand\tthe\tagent’s\tcorresponding\tbehavior\n\nYet\tanother\tapproach\tis\tto\tuse\toptimization\ttechniques,\tby\tevaluating\tthe\tgradients\tof\tthe\trewards\twith regards\tto\tthe\tpolicy\tparameters,\tthen\ttweaking\tthese\tparameters\tby\tfollowing\tthe\tgradient\ttoward\thigher rewards\t(gradient\tascent).\tThis\tapproach\tis\tcalled\tpolicy\tgradients\t(PG),\twhich\twe\twill\tdiscuss\tin more\tdetail\tlater\tin\tthis\tchapter.\tFor\texample,\tgoing\tback\tto\tthe\tvacuum\tcleaner\trobot,\tyou\tcould\tslightly increase\tp\tand\tevaluate\twhether\tthis\tincreases\tthe\tamount\tof\tdust\tpicked\tup\tby\tthe\trobot\tin\t30\tminutes;\tif it\tdoes,\tthen\tincrease\tp\tsome\tmore,\tor\telse\treduce\tp.\tWe\twill\timplement\ta\tpopular\tPG\talgorithm\tusing TensorFlow,\tbut\tbefore\twe\tdo\twe\tneed\tto\tcreate\tan\tenvironment\tfor\tthe\tagent\tto\tlive\tin,\tso\tit’s\ttime\tto introduce\tOpenAI\tgym.\n\nIntroduction\tto\tOpenAI\tGym One\tof\tthe\tchallenges\tof\tReinforcement\tLearning\tis\tthat\tin\torder\tto\ttrain\tan\tagent,\tyou\tfirst\tneed\tto\thave\ta working\tenvironment.\tIf\tyou\twant\tto\tprogram\tan\tagent\tthat\twill\tlearn\tto\tplay\tan\tAtari\tgame,\tyou\twill\tneed an\tAtari\tgame\tsimulator.\tIf\tyou\twant\tto\tprogram\ta\twalking\trobot,\tthen\tthe\tenvironment\tis\tthe\treal\tworld and\tyou\tcan\tdirectly\ttrain\tyour\trobot\tin\tthat\tenvironment,\tbut\tthis\thas\tits\tlimits:\tif\tthe\trobot\tfalls\toff\ta\tcliff, you\tcan’t\tjust\tclick\t“undo.”\tYou\tcan’t\tspeed\tup\ttime\teither;\tadding\tmore\tcomputing\tpower\twon’t\tmake\tthe robot\tmove\tany\tfaster.\tAnd\tit’s\tgenerally\ttoo\texpensive\tto\ttrain\t1,000\trobots\tin\tparallel.\tIn\tshort,\ttraining is\thard\tand\tslow\tin\tthe\treal\tworld,\tso\tyou\tgenerally\tneed\ta\tsimulated\tenvironment\tat\tleast\tto\tbootstrap training. OpenAI\tgym8\tis\ta\ttoolkit\tthat\tprovides\ta\twide\tvariety\tof\tsimulated\tenvironments\t(Atari\tgames,\tboard games,\t2D\tand\t3D\tphysical\tsimulations,\tand\tso\ton),\tso\tyou\tcan\ttrain\tagents,\tcompare\tthem,\tor\tdevelop new\tRL\talgorithms.\n\nLet’s\tinstall\tOpenAI\tgym.\tFor\ta\tminimal\tOpenAI\tgym\tinstallation,\tsimply\tuse\tpip:\n\n$\tpip3\tinstall\t--upgrade\tgym\n\nNext\topen\tup\ta\tPython\tshell\tor\ta\tJupyter\tnotebook\tand\tcreate\tyour\tfirst\tenvironment:\n\n>>>\timport\tgym >>>\tenv\t=\tgym.make(\"CartPole-v0\") [2016-10-14\t16:03:23,199]\tMaking\tnew\tenv:\tMsPacman-v0 >>>\tobs\t=\tenv.reset() >>>\tobs array([-0.03799846,\t-0.03288115,\t\t0.02337094,\t\t0.00720711]) >>>\tenv.render()\n\nThe\tmake()\tfunction\tcreates\tan\tenvironment,\tin\tthis\tcase\ta\tCartPole\tenvironment.\tThis\tis\ta\t2D\tsimulation in\twhich\ta\tcart\tcan\tbe\taccelerated\tleft\tor\tright\tin\torder\tto\tbalance\ta\tpole\tplaced\ton\ttop\tof\tit\t(see Figure\t16-4).\tAfter\tthe\tenvironment\tis\tcreated,\twe\tmust\tinitialize\tit\tusing\tthe\treset()\tmethod.\tThis returns\tthe\tfirst\tobservation.\tObservations\tdepend\ton\tthe\ttype\tof\tenvironment.\tFor\tthe\tCartPole environment,\teach\tobservation\tis\ta\t1D\tNumPy\tarray\tcontaining\tfour\tfloats:\tthese\tfloats\trepresent\tthe cart’s\thorizontal\tposition\t(0.0\t=\tcenter),\tits\tvelocity,\tthe\tangle\tof\tthe\tpole\t(0.0\t=\tvertical),\tand\tits angular\tvelocity.\tFinally,\tthe\trender()\tmethod\tdisplays\tthe\tenvironment\tas\tshown\tin\tFigure\t16-4.\n\nFigure\t16-4.\tThe\tCartPole\tenvironment\n\nIf\tyou\twant\trender()\tto\treturn\tthe\trendered\timage\tas\ta\tNumPy\tarray,\tyou\tcan\tset\tthe\tmode\tparameter\tto rgb_array\t(note\tthat\tother\tenvironments\tmay\tsupport\tdifferent\tmodes):\n\n>>>\timg\t=\tenv.render(mode=\"rgb_array\") >>>\timg.shape\t\t#\theight,\twidth,\tchannels\t(3=RGB) (400,\t600,\t3)\n\nTIP\n\nUnfortunately,\tthe\tCartPole\t(and\ta\tfew\tother\tenvironments)\trenders\tthe\timage\tto\tthe\tscreen\teven\tif\tyou\tset\tthe\tmode\tto \"rgb_array\".\tThe\tonly\tway\tto\tavoid\tthis\tis\tto\tuse\ta\tfake\tX\tserver\tsuch\tas\tXvfb\tor\tXdummy.\tFor\texample,\tyou\tcan\tinstall\tXvfb and\tstart\tPython\tusing\tthe\tfollowing\tcommand:\txvfb-run\t-s\t\"-screen\t0\t1400x900x24\"\tpython.\tOr\tuse\tthe\txvfbwrapper package.\n\nLet’s\task\tthe\tenvironment\twhat\tactions\tare\tpossible:\n\n>>>\tenv.action_space Discrete(2)\n\nDiscrete(2)\tmeans\tthat\tthe\tpossible\tactions\tare\tintegers\t0\tand\t1,\twhich\trepresent\taccelerating\tleft\t(0) or\tright\t(1).\tOther\tenvironments\tmay\thave\tmore\tdiscrete\tactions,\tor\tother\tkinds\tof\tactions\t(e.g., continuous).\tSince\tthe\tpole\tis\tleaning\ttoward\tthe\tright,\tlet’s\taccelerate\tthe\tcart\ttoward\tthe\tright:\n\n>>>\taction\t=\t1\t\t#\taccelerate\tright >>>\tobs,\treward,\tdone,\tinfo\t=\tenv.step(action) >>>\tobs array([-0.03865608,\t\t0.16189797,\t\t0.02351508,\t-0.27801135]) >>>\treward 1.0 >>>\tdone False >>>\tinfo\n\n{}\n\nThe\tstep()\tmethod\texecutes\tthe\tgiven\taction\tand\treturns\tfour\tvalues:\n\nobs\n\nThis\tis\tthe\tnew\tobservation.\tThe\tcart\tis\tnow\tmoving\ttoward\tthe\tright\t(obs[1]>0).\tThe\tpole\tis\tstill tilted\ttoward\tthe\tright\t(obs[2]>0),\tbut\tits\tangular\tvelocity\tis\tnow\tnegative\t(obs[3]<0),\tso\tit\twill likely\tbe\ttilted\ttoward\tthe\tleft\tafter\tthe\tnext\tstep.\n\nreward\n\nIn\tthis\tenvironment,\tyou\tget\ta\treward\tof\t1.0\tat\tevery\tstep,\tno\tmatter\twhat\tyou\tdo,\tso\tthe\tgoal\tis\tto keep\trunning\tas\tlong\tas\tpossible.\n\ndone\n\nThis\tvalue\twill\tbe\tTrue\twhen\tthe\tepisode\tis\tover.\tThis\twill\thappen\twhen\tthe\tpole\ttilts\ttoo\tmuch. After\tthat,\tthe\tenvironment\tmust\tbe\treset\tbefore\tit\tcan\tbe\tused\tagain.\n\ninfo\n\nThis\tdictionary\tmay\tprovide\textra\tdebug\tinformation\tin\tother\tenvironments.\tThis\tdata\tshould\tnot\tbe used\tfor\ttraining\t(it\twould\tbe\tcheating).\n\nLet’s\thardcode\ta\tsimple\tpolicy\tthat\taccelerates\tleft\twhen\tthe\tpole\tis\tleaning\ttoward\tthe\tleft\tand accelerates\tright\twhen\tthe\tpole\tis\tleaning\ttoward\tthe\tright.\tWe\twill\trun\tthis\tpolicy\tto\tsee\tthe\taverage rewards\tit\tgets\tover\t500\tepisodes:\n\ndef\tbasic_policy(obs): \t\t\t\tangle\t=\tobs[2] \t\t\t\treturn\t0\tif\tangle\t<\t0\telse\t1\n\ntotals\t=\t[] for\tepisode\tin\trange(500): \t\t\t\tepisode_rewards\t=\t0 \t\t\t\tobs\t=\tenv.reset() \t\t\t\tfor\tstep\tin\trange(1000):\t#\t1000\tsteps\tmax,\twe\tdon't\twant\tto\trun\tforever \t\t\t\t\t\t\t\taction\t=\tbasic_policy(obs) \t\t\t\t\t\t\t\tobs,\treward,\tdone,\tinfo\t=\tenv.step(action) \t\t\t\t\t\t\t\tepisode_rewards\t+=\treward \t\t\t\t\t\t\t\tif\tdone: \t\t\t\t\t\t\t\t\t\t\t\tbreak \t\t\t\ttotals.append(episode_rewards)\n\nThis\tcode\tis\thopefully\tself-explanatory.\tLet’s\tlook\tat\tthe\tresult:\n\n>>>\timport\tnumpy\tas\tnp >>>\tnp.mean(totals),\tnp.std(totals),\tnp.min(totals),\tnp.max(totals) (42.125999999999998,\t9.1237121830974033,\t24.0,\t68.0)\n\nEven\twith\t500\ttries,\tthis\tpolicy\tnever\tmanaged\tto\tkeep\tthe\tpole\tupright\tfor\tmore\tthan\t68\tconsecutive steps.\tNot\tgreat.\tIf\tyou\tlook\tat\tthe\tsimulation\tin\tthe\tJupyter\tnotebooks,\tyou\twill\tsee\tthat\tthe\tcart\toscillates left\tand\tright\tmore\tand\tmore\tstrongly\tuntil\tthe\tpole\ttilts\ttoo\tmuch.\tLet’s\tsee\tif\ta\tneural\tnetwork\tcan\tcome up\twith\ta\tbetter\tpolicy.\n\nNeural\tNetwork\tPolicies Let’s\tcreate\ta\tneural\tnetwork\tpolicy.\tJust\tlike\tthe\tpolicy\twe\thardcoded\tearlier,\tthis\tneural\tnetwork\twill take\tan\tobservation\tas\tinput,\tand\tit\twill\toutput\tthe\taction\tto\tbe\texecuted.\tMore\tprecisely,\tit\twill\testimate\ta probability\tfor\teach\taction,\tand\tthen\twe\twill\tselect\tan\taction\trandomly\taccording\tto\tthe\testimated probabilities\t(see\tFigure\t16-5).\tIn\tthe\tcase\tof\tthe\tCartPole\tenvironment,\tthere\tare\tjust\ttwo\tpossible actions\t(left\tor\tright),\tso\twe\tonly\tneed\tone\toutput\tneuron.\tIt\twill\toutput\tthe\tprobability\tp\tof\taction\t0\t(left), and\tof\tcourse\tthe\tprobability\tof\taction\t1\t(right)\twill\tbe\t1\t–\tp.\tFor\texample,\tif\tit\toutputs\t0.7,\tthen\twe\twill pick\taction\t0\twith\t70%\tprobability,\tand\taction\t1\twith\t30%\tprobability.\n\nFigure\t16-5.\tNeural\tnetwork\tpolicy\n\nYou\tmay\twonder\twhy\twe\tare\tpicking\ta\trandom\taction\tbased\ton\tthe\tprobability\tgiven\tby\tthe\tneural network,\trather\tthan\tjust\tpicking\tthe\taction\twith\tthe\thighest\tscore.\tThis\tapproach\tlets\tthe\tagent\tfind\tthe right\tbalance\tbetween\texploring\tnew\tactions\tand\texploiting\tthe\tactions\tthat\tare\tknown\tto\twork\twell. Here’s\tan\tanalogy:\tsuppose\tyou\tgo\tto\ta\trestaurant\tfor\tthe\tfirst\ttime,\tand\tall\tthe\tdishes\tlook\tequally appealing\tso\tyou\trandomly\tpick\tone.\tIf\tit\tturns\tout\tto\tbe\tgood,\tyou\tcan\tincrease\tthe\tprobability\tto\torder\tit next\ttime,\tbut\tyou\tshouldn’t\tincrease\tthat\tprobability\tup\tto\t100%,\tor\telse\tyou\twill\tnever\ttry\tout\tthe\tother dishes,\tsome\tof\twhich\tmay\tbe\teven\tbetter\tthan\tthe\tone\tyou\ttried.\n\nAlso\tnote\tthat\tin\tthis\tparticular\tenvironment,\tthe\tpast\tactions\tand\tobservations\tcan\tsafely\tbe\tignored,\n\nsince\teach\tobservation\tcontains\tthe\tenvironment’s\tfull\tstate.\tIf\tthere\twere\tsome\thidden\tstate,\tthen\tyou\tmay need\tto\tconsider\tpast\tactions\tand\tobservations\tas\twell.\tFor\texample,\tif\tthe\tenvironment\tonly\trevealed\tthe position\tof\tthe\tcart\tbut\tnot\tits\tvelocity,\tyou\twould\thave\tto\tconsider\tnot\tonly\tthe\tcurrent\tobservation\tbut also\tthe\tprevious\tobservation\tin\torder\tto\testimate\tthe\tcurrent\tvelocity.\tAnother\texample\tis\twhen\tthe observations\tare\tnoisy;\tin\tthat\tcase,\tyou\tgenerally\twant\tto\tuse\tthe\tpast\tfew\tobservations\tto\testimate\tthe most\tlikely\tcurrent\tstate.\tThe\tCartPole\tproblem\tis\tthus\tas\tsimple\tas\tcan\tbe;\tthe\tobservations\tare\tnoise- free\tand\tthey\tcontain\tthe\tenvironment’s\tfull\tstate.\n\nHere\tis\tthe\tcode\tto\tbuild\tthis\tneural\tnetwork\tpolicy\tusing\tTensorFlow:\n\nimport\ttensorflow\tas\ttf\n\n#\t1.\tSpecify\tthe\tneural\tnetwork\tarchitecture n_inputs\t=\t4\t\t#\t==\tenv.observation_space.shape[0] n_hidden\t=\t4\t\t#\tit's\ta\tsimple\ttask,\twe\tdon't\tneed\tmore\thidden\tneurons n_outputs\t=\t1\t#\tonly\toutputs\tthe\tprobability\tof\taccelerating\tleft initializer\t=\ttf.contrib.layers.variance_scaling_initializer()\n\n#\t2.\tBuild\tthe\tneural\tnetwork X\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs]) hidden\t=\ttf.layers.dense(X,\tn_hidden,\tactivation=tf.nn.elu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) logits\t=\ttf.layers.dense(hidden,\tn_outputs, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) outputs\t=\ttf.nn.sigmoid(logits)\n\n#\t3.\tSelect\ta\trandom\taction\tbased\ton\tthe\testimated\tprobabilities p_left_and_right\t=\ttf.concat(axis=1,\tvalues=[outputs,\t1\t-\toutputs]) action\t=\ttf.multinomial(tf.log(p_left_and_right),\tnum_samples=1)\n\ninit\t=\ttf.global_variables_initializer()\n\nLet’s\tgo\tthrough\tthis\tcode:\n\n1.\t After\tthe\timports,\twe\tdefine\tthe\tneural\tnetwork\tarchitecture.\tThe\tnumber\tof\tinputs\tis\tthe\tsize\tof\tthe observation\tspace\t(which\tin\tthe\tcase\tof\tthe\tCartPole\tis\tfour),\twe\tjust\thave\tfour\thidden\tunits\tand\tno need\tfor\tmore,\tand\twe\thave\tjust\tone\toutput\tprobability\t(the\tprobability\tof\tgoing\tleft).\n\n2.\t Next\twe\tbuild\tthe\tneural\tnetwork.\tIn\tthis\texample,\tit’s\ta\tvanilla\tMulti-Layer\tPerceptron,\twith\ta single\toutput.\tNote\tthat\tthe\toutput\tlayer\tuses\tthe\tlogistic\t(sigmoid)\tactivation\tfunction\tin\torder\tto output\ta\tprobability\tfrom\t0.0\tto\t1.0.\tIf\tthere\twere\tmore\tthan\ttwo\tpossible\tactions,\tthere\twould\tbe one\toutput\tneuron\tper\taction,\tand\tyou\twould\tuse\tthe\tsoftmax\tactivation\tfunction\tinstead.\n\n3.\t Lastly,\twe\tcall\tthe\tmultinomial()\tfunction\tto\tpick\ta\trandom\taction.\tThis\tfunction\tindependently samples\tone\t(or\tmore)\tintegers,\tgiven\tthe\tlog\tprobability\tof\teach\tinteger.\tFor\texample,\tif\tyou\tcall\tit with\tthe\tarray\t[np.log(0.5),\tnp.log(0.2),\tnp.log(0.3)]\tand\twith\tnum_samples=5,\tthen\tit will\toutput\tfive\tintegers,\teach\tof\twhich\twill\thave\ta\t50%\tprobability\tof\tbeing\t0,\t20%\tof\tbeing\t1,\tand 30%\tof\tbeing\t2.\tIn\tour\tcase\twe\tjust\tneed\tone\tinteger\trepresenting\tthe\taction\tto\ttake.\tSince\tthe outputs\ttensor\tonly\tcontains\tthe\tprobability\tof\tgoing\tleft,\twe\tmust\tfirst\tconcatenate\t1-outputs\tto\tit to\thave\ta\ttensor\tcontaining\tthe\tprobability\tof\tboth\tleft\tand\tright\tactions.\tNote\tthat\tif\tthere\twere\tmore than\ttwo\tpossible\tactions,\tthe\tneural\tnetwork\twould\thave\tto\toutput\tone\tprobability\tper\taction\tso\tyou would\tnot\tneed\tthe\tconcatenation\tstep.\n\nOkay,\twe\tnow\thave\ta\tneural\tnetwork\tpolicy\tthat\twill\ttake\tobservations\tand\toutput\tactions.\tBut\thow\tdo we\ttrain\tit?\n\nEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem If\twe\tknew\twhat\tthe\tbest\taction\twas\tat\teach\tstep,\twe\tcould\ttrain\tthe\tneural\tnetwork\tas\tusual,\tby minimizing\tthe\tcross\tentropy\tbetween\tthe\testimated\tprobability\tand\tthe\ttarget\tprobability.\tIt\twould\tjust\tbe regular\tsupervised\tlearning.\tHowever,\tin\tReinforcement\tLearning\tthe\tonly\tguidance\tthe\tagent\tgets\tis through\trewards,\tand\trewards\tare\ttypically\tsparse\tand\tdelayed.\tFor\texample,\tif\tthe\tagent\tmanages\tto balance\tthe\tpole\tfor\t100\tsteps,\thow\tcan\tit\tknow\twhich\tof\tthe\t100\tactions\tit\ttook\twere\tgood,\tand\twhich\tof them\twere\tbad?\tAll\tit\tknows\tis\tthat\tthe\tpole\tfell\tafter\tthe\tlast\taction,\tbut\tsurely\tthis\tlast\taction\tis\tnot entirely\tresponsible.\tThis\tis\tcalled\tthe\tcredit\tassignment\tproblem:\twhen\tthe\tagent\tgets\ta\treward,\tit\tis hard\tfor\tit\tto\tknow\twhich\tactions\tshould\tget\tcredited\t(or\tblamed)\tfor\tit.\tThink\tof\ta\tdog\tthat\tgets\trewarded hours\tafter\tit\tbehaved\twell;\twill\tit\tunderstand\twhat\tit\tis\trewarded\tfor?\n\nTo\ttackle\tthis\tproblem,\ta\tcommon\tstrategy\tis\tto\tevaluate\tan\taction\tbased\ton\tthe\tsum\tof\tall\tthe\trewards\tthat come\tafter\tit,\tusually\tapplying\ta\tdiscount\trate\tr\tat\teach\tstep.\tFor\texample\t(see\tFigure\t16-6),\tif\tan\tagent decides\tto\tgo\tright\tthree\ttimes\tin\ta\trow\tand\tgets\t+10\treward\tafter\tthe\tfirst\tstep,\t0\tafter\tthe\tsecond\tstep, and\tfinally\t–50\tafter\tthe\tthird\tstep,\tthen\tassuming\twe\tuse\ta\tdiscount\trate\tr\t=\t0.8,\tthe\tfirst\taction\twill\thave a\ttotal\tscore\tof\t10\t+\tr\t×\t0\t+\tr2\t×\t(–50)\t=\t–22.\tIf\tthe\tdiscount\trate\tis\tclose\tto\t0,\tthen\tfuture\trewards\twon’t count\tfor\tmuch\tcompared\tto\timmediate\trewards.\tConversely,\tif\tthe\tdiscount\trate\tis\tclose\tto\t1,\tthen rewards\tfar\tinto\tthe\tfuture\twill\tcount\talmost\tas\tmuch\tas\timmediate\trewards.\tTypical\tdiscount\trates\tare 0.95\tor\t0.99.\tWith\ta\tdiscount\trate\tof\t0.95,\trewards\t13\tsteps\tinto\tthe\tfuture\tcount\troughly\tfor\thalf\tas\tmuch as\timmediate\trewards\t(since\t0.9513\t≈\t0.5),\twhile\twith\ta\tdiscount\trate\tof\t0.99,\trewards\t69\tsteps\tinto\tthe future\tcount\tfor\thalf\tas\tmuch\tas\timmediate\trewards.\tIn\tthe\tCartPole\tenvironment,\tactions\thave\tfairly short-term\teffects,\tso\tchoosing\ta\tdiscount\trate\tof\t0.95\tseems\treasonable.\n\nFigure\t16-6.\tDiscounted\trewards\n\nOf\tcourse,\ta\tgood\taction\tmay\tbe\tfollowed\tby\tseveral\tbad\tactions\tthat\tcause\tthe\tpole\tto\tfall\tquickly, resulting\tin\tthe\tgood\taction\tgetting\ta\tlow\tscore\t(similarly,\ta\tgood\tactor\tmay\tsometimes\tstar\tin\ta\tterrible movie).\tHowever,\tif\twe\tplay\tthe\tgame\tenough\ttimes,\ton\taverage\tgood\tactions\twill\tget\ta\tbetter\tscore\tthan bad\tones.\tSo,\tto\tget\tfairly\treliable\taction\tscores,\twe\tmust\trun\tmany\tepisodes\tand\tnormalize\tall\tthe\taction\n\nscores\t(by\tsubtracting\tthe\tmean\tand\tdividing\tby\tthe\tstandard\tdeviation).\tAfter\tthat,\twe\tcan\treasonably assume\tthat\tactions\twith\ta\tnegative\tscore\twere\tbad\twhile\tactions\twith\ta\tpositive\tscore\twere\tgood. Perfect\t—\tnow\tthat\twe\thave\ta\tway\tto\tevaluate\teach\taction,\twe\tare\tready\tto\ttrain\tour\tfirst\tagent\tusing policy\tgradients.\tLet’s\tsee\thow.\n\nPolicy\tGradients As\tdiscussed\tearlier,\tPG\talgorithms\toptimize\tthe\tparameters\tof\ta\tpolicy\tby\tfollowing\tthe\tgradients toward\thigher\trewards.\tOne\tpopular\tclass\tof\tPG\talgorithms,\tcalled\tREINFORCE\talgorithms,\twas introduced\tback\tin\t19929\tby\tRonald\tWilliams.\tHere\tis\tone\tcommon\tvariant:\n\n1.\t First,\tlet\tthe\tneural\tnetwork\tpolicy\tplay\tthe\tgame\tseveral\ttimes\tand\tat\teach\tstep\tcompute\tthe gradients\tthat\twould\tmake\tthe\tchosen\taction\teven\tmore\tlikely,\tbut\tdon’t\tapply\tthese\tgradients\tyet.\n\n2.\t Once\tyou\thave\trun\tseveral\tepisodes,\tcompute\teach\taction’s\tscore\t(using\tthe\tmethod\tdescribed\tin\tthe previous\tparagraph).\n\n3.\t If\tan\taction’s\tscore\tis\tpositive,\tit\tmeans\tthat\tthe\taction\twas\tgood\tand\tyou\twant\tto\tapply\tthe\tgradients computed\tearlier\tto\tmake\tthe\taction\teven\tmore\tlikely\tto\tbe\tchosen\tin\tthe\tfuture.\tHowever,\tif\tthe score\tis\tnegative,\tit\tmeans\tthe\taction\twas\tbad\tand\tyou\twant\tto\tapply\tthe\topposite\tgradients\tto\tmake this\taction\tslightly\tless\tlikely\tin\tthe\tfuture.\tThe\tsolution\tis\tsimply\tto\tmultiply\teach\tgradient\tvector\tby the\tcorresponding\taction’s\tscore.\n\n4.\t Finally,\tcompute\tthe\tmean\tof\tall\tthe\tresulting\tgradient\tvectors,\tand\tuse\tit\tto\tperform\ta\tGradient Descent\tstep.\n\nLet’s\timplement\tthis\talgorithm\tusing\tTensorFlow.\tWe\twill\ttrain\tthe\tneural\tnetwork\tpolicy\twe\tbuilt\tearlier so\tthat\tit\tlearns\tto\tbalance\tthe\tpole\ton\tthe\tcart.\tLet’s\tstart\tby\tcompleting\tthe\tconstruction\tphase\twe\tcoded earlier\tto\tadd\tthe\ttarget\tprobability,\tthe\tcost\tfunction,\tand\tthe\ttraining\toperation.\tSince\twe\tare\tacting\tas though\tthe\tchosen\taction\tis\tthe\tbest\tpossible\taction,\tthe\ttarget\tprobability\tmust\tbe\t1.0\tif\tthe\tchosen\taction is\taction\t0\t(left)\tand\t0.0\tif\tit\tis\taction\t1\t(right):\n\ny\t=\t1.\t-\ttf.to_float(action)\n\nNow\tthat\twe\thave\ta\ttarget\tprobability,\twe\tcan\tdefine\tthe\tcost\tfunction\t(cross\tentropy)\tand\tcompute\tthe gradients:\n\nlearning_rate\t=\t0.01\n\ncross_entropy\t=\ttf.nn.sigmoid_cross_entropy_with_logits(labels=y, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogits=logits) optimizer\t=\ttf.train.AdamOptimizer(learning_rate) grads_and_vars\t=\toptimizer.compute_gradients(cross_entropy)\n\nNote\tthat\twe\tare\tcalling\tthe\toptimizer’s\tcompute_gradients()\tmethod\tinstead\tof\tthe\tminimize() method.\tThis\tis\tbecause\twe\twant\tto\ttweak\tthe\tgradients\tbefore\twe\tapply\tthem.10\tThe compute_gradients()\tmethod\treturns\ta\tlist\tof\tgradient\tvector/variable\tpairs\t(one\tpair\tper\ttrainable variable).\tLet’s\tput\tall\tthe\tgradients\tin\ta\tlist,\tto\tmake\tit\tmore\tconvenient\tto\tobtain\ttheir\tvalues:\n\ngradients\t=\t[grad\tfor\tgrad,\tvariable\tin\tgrads_and_vars]\n\nOkay,\tnow\tcomes\tthe\ttricky\tpart.\tDuring\tthe\texecution\tphase,\tthe\talgorithm\twill\trun\tthe\tpolicy\tand\tat\teach\n\nstep\tit\twill\tevaluate\tthese\tgradient\ttensors\tand\tstore\ttheir\tvalues.\tAfter\ta\tnumber\tof\tepisodes\tit\twill\ttweak these\tgradients\tas\texplained\tearlier\t(i.e.,\tmultiply\tthem\tby\tthe\taction\tscores\tand\tnormalize\tthem)\tand compute\tthe\tmean\tof\tthe\ttweaked\tgradients.\tNext,\tit\twill\tneed\tto\tfeed\tthe\tresulting\tgradients\tback\tto\tthe optimizer\tso\tthat\tit\tcan\tperform\tan\toptimization\tstep.\tThis\tmeans\twe\tneed\tone\tplaceholder\tper\tgradient vector.\tMoreover,\twe\tmust\tcreate\tthe\toperation\tthat\twill\tapply\tthe\tupdated\tgradients.\tFor\tthis\twe\twill call\tthe\toptimizer’s\tapply_gradients()\tfunction,\twhich\ttakes\ta\tlist\tof\tgradient\tvector/variable\tpairs. Instead\tof\tgiving\tit\tthe\toriginal\tgradient\tvectors,\twe\twill\tgive\tit\ta\tlist\tcontaining\tthe\tupdated\tgradients (i.e.,\tthe\tones\tfed\tthrough\tthe\tgradient\tplaceholders):\n\ngradient_placeholders\t=\t[] grads_and_vars_feed\t=\t[] for\tgrad,\tvariable\tin\tgrads_and_vars: \t\t\t\tgradient_placeholder\t=\ttf.placeholder(tf.float32,\tshape=grad.get_shape()) \t\t\t\tgradient_placeholders.append(gradient_placeholder) \t\t\t\tgrads_and_vars_feed.append((gradient_placeholder,\tvariable))\n\ntraining_op\t=\toptimizer.apply_gradients(grads_and_vars_feed)\n\nLet’s\tstep\tback\tand\ttake\ta\tlook\tat\tthe\tfull\tconstruction\tphase:\n\nn_inputs\t=\t4 n_hidden\t=\t4 n_outputs\t=\t1 initializer\t=\ttf.contrib.layers.variance_scaling_initializer()\n\nlearning_rate\t=\t0.01\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs]) hidden\t=\ttf.layers.dense(X,\tn_hidden,\tactivation=tf.nn.elu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) logits\t=\ttf.layers.dense(hidden,\tn_outputs, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) outputs\t=\ttf.nn.sigmoid(logits) p_left_and_right\t=\ttf.concat(axis=1,\tvalues=[outputs,\t1\t-\toutputs]) action\t=\ttf.multinomial(tf.log(p_left_and_right),\tnum_samples=1)\n\ny\t=\t1.\t-\ttf.to_float(action) cross_entropy\t=\ttf.nn.sigmoid_cross_entropy_with_logits( \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlabels=y,\tlogits=logits) optimizer\t=\ttf.train.AdamOptimizer(learning_rate) grads_and_vars\t=\toptimizer.compute_gradients(cross_entropy) gradients\t=\t[grad\tfor\tgrad,\tvariable\tin\tgrads_and_vars] gradient_placeholders\t=\t[] grads_and_vars_feed\t=\t[] for\tgrad,\tvariable\tin\tgrads_and_vars: \t\t\t\tgradient_placeholder\t=\ttf.placeholder(tf.float32,\tshape=grad.get_shape()) \t\t\t\tgradient_placeholders.append(gradient_placeholder) \t\t\t\tgrads_and_vars_feed.append((gradient_placeholder,\tvariable)) training_op\t=\toptimizer.apply_gradients(grads_and_vars_feed)\n\ninit\t=\ttf.global_variables_initializer() saver\t=\ttf.train.Saver()\n\nOn\tto\tthe\texecution\tphase!\tWe\twill\tneed\ta\tcouple\tof\tfunctions\tto\tcompute\tthe\ttotal\tdiscounted\trewards, given\tthe\traw\trewards,\tand\tto\tnormalize\tthe\tresults\tacross\tmultiple\tepisodes:\n\ndef\tdiscount_rewards(rewards,\tdiscount_rate): \t\t\t\tdiscounted_rewards\t=\tnp.empty(len(rewards)) \t\t\t\tcumulative_rewards\t=\t0 \t\t\t\tfor\tstep\tin\treversed(range(len(rewards))): \t\t\t\t\t\t\t\tcumulative_rewards\t=\trewards[step]\t+\tcumulative_rewards\t*\tdiscount_rate \t\t\t\t\t\t\t\tdiscounted_rewards[step]\t=\tcumulative_rewards \t\t\t\treturn\tdiscounted_rewards\n\ndef\tdiscount_and_normalize_rewards(all_rewards,\tdiscount_rate): \t\t\t\tall_discounted_rewards\t=\t[discount_rewards(rewards,\tdiscount_rate) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\trewards\tin\tall_rewards] \t\t\t\tflat_rewards\t=\tnp.concatenate(all_discounted_rewards) \t\t\t\treward_mean\t=\tflat_rewards.mean() \t\t\t\treward_std\t=\tflat_rewards.std() \t\t\t\treturn\t[(discounted_rewards\t-\treward_mean)/reward_std \t\t\t\t\t\t\t\t\t\t\t\tfor\tdiscounted_rewards\tin\tall_discounted_rewards]\n\nLet’s\tcheck\tthat\tthis\tworks:\n\n>>>\tdiscount_rewards([10,\t0,\t-50],\tdiscount_rate=0.8) array([-22.,\t-40.,\t-50.]) >>>\tdiscount_and_normalize_rewards([[10,\t0,\t-50],\t[10,\t20]],\tdiscount_rate=0.8) [array([-0.28435071,\t-0.86597718,\t-1.18910299]), \tarray([\t1.26665318,\t\t1.0727777\t])]\n\nThe\tcall\tto\tdiscount_rewards()\treturns\texactly\twhat\twe\texpect\t(see\tFigure\t16-6).\tYou\tcan\tverify\tthat the\tfunction\tdiscount_and_normalize_rewards()\tdoes\tindeed\treturn\tthe\tnormalized\tscores\tfor\teach action\tin\tboth\tepisodes.\tNotice\tthat\tthe\tfirst\tepisode\twas\tmuch\tworse\tthan\tthe\tsecond,\tso\tits\tnormalized scores\tare\tall\tnegative;\tall\tactions\tfrom\tthe\tfirst\tepisode\twould\tbe\tconsidered\tbad,\tand\tconversely\tall actions\tfrom\tthe\tsecond\tepisode\twould\tbe\tconsidered\tgood.\n\nWe\tnow\thave\tall\twe\tneed\tto\ttrain\tthe\tpolicy:\n\nn_iterations\t=\t250\t\t\t\t\t\t#\tnumber\tof\ttraining\titerations n_max_steps\t=\t1000\t\t\t\t\t\t#\tmax\tsteps\tper\tepisode n_games_per_update\t=\t10\t#\ttrain\tthe\tpolicy\tevery\t10\tepisodes save_iterations\t=\t10\t\t\t\t#\tsave\tthe\tmodel\tevery\t10\ttraining\titerations discount_rate\t=\t0.95\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\titeration\tin\trange(n_iterations): \t\t\t\t\t\t\t\tall_rewards\t=\t[]\t\t\t\t#\tall\tsequences\tof\traw\trewards\tfor\teach\tepisode \t\t\t\t\t\t\t\tall_gradients\t=\t[]\t\t#\tgradients\tsaved\tat\teach\tstep\tof\teach\tepisode \t\t\t\t\t\t\t\tfor\tgame\tin\trange(n_games_per_update): \t\t\t\t\t\t\t\t\t\t\t\tcurrent_rewards\t=\t[]\t\t\t#\tall\traw\trewards\tfrom\tthe\tcurrent\tepisode \t\t\t\t\t\t\t\t\t\t\t\tcurrent_gradients\t=\t[]\t#\tall\tgradients\tfrom\tthe\tcurrent\tepisode \t\t\t\t\t\t\t\t\t\t\t\tobs\t=\tenv.reset() \t\t\t\t\t\t\t\t\t\t\t\tfor\tstep\tin\trange(n_max_steps): \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taction_val,\tgradients_val\t=\tsess.run( \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[action,\tgradients], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfeed_dict={X:\tobs.reshape(1,\tn_inputs)})\t#\tone\tobs \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tobs,\treward,\tdone,\tinfo\t=\tenv.step(action_val[0][0]) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcurrent_rewards.append(reward) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcurrent_gradients.append(gradients_val) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tdone: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbreak \t\t\t\t\t\t\t\t\t\t\t\tall_rewards.append(current_rewards) \t\t\t\t\t\t\t\t\t\t\t\tall_gradients.append(current_gradients)\n\n#\tAt\tthis\tpoint\twe\thave\trun\tthe\tpolicy\tfor\t10\tepisodes,\tand\twe\tare \t\t\t\t\t\t\t\t#\tready\tfor\ta\tpolicy\tupdate\tusing\tthe\talgorithm\tdescribed\tearlier. \t\t\t\t\t\t\t\tall_rewards\t=\tdiscount_and_normalize_rewards(all_rewards,discount_rate) \t\t\t\t\t\t\t\tfeed_dict\t=\t{} \t\t\t\t\t\t\t\tfor\tvar_index,\tgrad_placeholder\tin\tenumerate(gradient_placeholders): \t\t\t\t\t\t\t\t\t\t\t\t#\tmultiply\tthe\tgradients\tby\tthe\taction\tscores,\tand\tcompute\tthe\tmean \t\t\t\t\t\t\t\t\t\t\t\tmean_gradients\t=\tnp.mean( \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[reward\t*\tall_gradients[game_index][step][var_index] \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tgame_index,\trewards\tin\tenumerate(all_rewards) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tstep,\treward\tin\tenumerate(rewards)], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taxis=0) \t\t\t\t\t\t\t\t\t\t\t\tfeed_dict[grad_placeholder]\t=\tmean_gradients \t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict=feed_dict) \t\t\t\t\t\t\t\tif\titeration\t%\tsave_iterations\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tsaver.save(sess,\t\"./my_policy_net_pg.ckpt\")\n\nEach\ttraining\titeration\tstarts\tby\trunning\tthe\tpolicy\tfor\t10\tepisodes\t(with\tmaximum\t1,000\tsteps\tper episode,\tto\tavoid\trunning\tforever).\tAt\teach\tstep,\twe\talso\tcompute\tthe\tgradients,\tpretending\tthat\tthe chosen\taction\twas\tthe\tbest.\tAfter\tthese\t10\tepisodes\thave\tbeen\trun,\twe\tcompute\tthe\taction\tscores\tusing\tthe discount_and_normalize_rewards()\tfunction;\twe\tgo\tthrough\teach\ttrainable\tvariable,\tacross\tall episodes\tand\tall\tsteps,\tto\tmultiply\teach\tgradient\tvector\tby\tits\tcorresponding\taction\tscore;\tand\twe compute\tthe\tmean\tof\tthe\tresulting\tgradients.\tFinally,\twe\trun\tthe\ttraining\toperation,\tfeeding\tit\tthese\tmean gradients\t(one\tper\ttrainable\tvariable).\tWe\talso\tsave\tthe\tmodel\tevery\t10\ttraining\toperations.\n\nAnd\twe’re\tdone!\tThis\tcode\twill\ttrain\tthe\tneural\tnetwork\tpolicy,\tand\tit\twill\tsuccessfully\tlearn\tto\tbalance the\tpole\ton\tthe\tcart\t(you\tcan\ttry\tit\tout\tin\tthe\tJupyter\tnotebooks).\tNote\tthat\tthere\tare\tactually\ttwo\tways\tthe agent\tcan\tlose\tthe\tgame:\teither\tthe\tpole\tcan\ttilt\ttoo\tmuch,\tor\tthe\tcart\tcan\tgo\tcompletely\toff\tthe\tscreen. With\t250\ttraining\titerations,\tthe\tpolicy\tlearns\tto\tbalance\tthe\tpole\tquite\twell,\tbut\tit\tis\tnot\tyet\tgood\tenough at\tavoiding\tgoing\toff\tthe\tscreen.\tA\tfew\thundred\tmore\ttraining\titerations\twill\tfix\tthat.\n\nTIP\n\nResearchers\ttry\tto\tfind\talgorithms\tthat\twork\twell\teven\twhen\tthe\tagent\tinitially\tknows\tnothing\tabout\tthe\tenvironment.\tHowever, unless\tyou\tare\twriting\ta\tpaper,\tyou\tshould\tinject\tas\tmuch\tprior\tknowledge\tas\tpossible\tinto\tthe\tagent,\tas\tit\twill\tspeed\tup\ttraining dramatically.\tFor\texample,\tyou\tcould\tadd\tnegative\trewards\tproportional\tto\tthe\tdistance\tfrom\tthe\tcenter\tof\tthe\tscreen,\tand\tto\tthe pole’s\tangle.\tAlso,\tif\tyou\talready\thave\ta\treasonably\tgood\tpolicy\t(e.g.,\thardcoded),\tyou\tmay\twant\tto\ttrain\tthe\tneural\tnetwork\tto imitate\tit\tbefore\tusing\tpolicy\tgradients\tto\timprove\tit.\n\nDespite\tits\trelative\tsimplicity,\tthis\talgorithm\tis\tquite\tpowerful.\tYou\tcan\tuse\tit\tto\ttackle\tmuch\tharder problems\tthan\tbalancing\ta\tpole\ton\ta\tcart.\tIn\tfact,\tAlphaGo\twas\tbased\ton\ta\tsimilar\tPG\talgorithm\t(plus Monte\tCarlo\tTree\tSearch,\twhich\tis\tbeyond\tthe\tscope\tof\tthis\tbook).\n\nWe\twill\tnow\tlook\tat\tanother\tpopular\tfamily\tof\talgorithms.\tWhereas\tPG\talgorithms\tdirectly\ttry\tto\toptimize the\tpolicy\tto\tincrease\trewards,\tthe\talgorithms\twe\twill\tlook\tat\tnow\tare\tless\tdirect:\tthe\tagent\tlearns\tto estimate\tthe\texpected\tsum\tof\tdiscounted\tfuture\trewards\tfor\teach\tstate,\tor\tthe\texpected\tsum\tof\tdiscounted future\trewards\tfor\teach\taction\tin\teach\tstate,\tthen\tuses\tthis\tknowledge\tto\tdecide\thow\tto\tact.\tTo\tunderstand these\talgorithms,\twe\tmust\tfirst\tintroduce\tMarkov\tdecision\tprocesses\t(MDP).\n\nMarkov\tDecision\tProcesses In\tthe\tearly\t20th\tcentury,\tthe\tmathematician\tAndrey\tMarkov\tstudied\tstochastic\tprocesses\twith\tno\tmemory, called\tMarkov\tchains.\tSuch\ta\tprocess\thas\ta\tfixed\tnumber\tof\tstates,\tand\tit\trandomly\tevolves\tfrom\tone state\tto\tanother\tat\teach\tstep.\tThe\tprobability\tfor\tit\tto\tevolve\tfrom\ta\tstate\ts\tto\ta\tstate\ts′\tis\tfixed,\tand\tit depends\tonly\ton\tthe\tpair\t(s,s′),\tnot\ton\tpast\tstates\t(the\tsystem\thas\tno\tmemory).\n\nFigure\t16-7\tshows\tan\texample\tof\ta\tMarkov\tchain\twith\tfour\tstates.\tSuppose\tthat\tthe\tprocess\tstarts\tin\tstate s0,\tand\tthere\tis\ta\t70%\tchance\tthat\tit\twill\tremain\tin\tthat\tstate\tat\tthe\tnext\tstep.\tEventually\tit\tis\tbound\tto leave\tthat\tstate\tand\tnever\tcome\tback\tsince\tno\tother\tstate\tpoints\tback\tto\ts0.\tIf\tit\tgoes\tto\tstate\ts1,\tit\twill\tthen most\tlikely\tgo\tto\tstate\ts2\t(90%\tprobability),\tthen\timmediately\tback\tto\tstate\ts1\t(with\t100%\tprobability).\tIt may\talternate\ta\tnumber\tof\ttimes\tbetween\tthese\ttwo\tstates,\tbut\teventually\tit\twill\tfall\tinto\tstate\ts3\tand remain\tthere\tforever\t(this\tis\ta\tterminal\tstate).\tMarkov\tchains\tcan\thave\tvery\tdifferent\tdynamics,\tand\tthey are\theavily\tused\tin\tthermodynamics,\tchemistry,\tstatistics,\tand\tmuch\tmore.\n\nFigure\t16-7.\tExample\tof\ta\tMarkov\tchain\n\nMarkov\tdecision\tprocesses\twere\tfirst\tdescribed\tin\tthe\t1950s\tby\tRichard\tBellman.11\tThey\tresemble Markov\tchains\tbut\twith\ta\ttwist:\tat\teach\tstep,\tan\tagent\tcan\tchoose\tone\tof\tseveral\tpossible\tactions,\tand\tthe transition\tprobabilities\tdepend\ton\tthe\tchosen\taction.\tMoreover,\tsome\tstate\ttransitions\treturn\tsome\treward (positive\tor\tnegative),\tand\tthe\tagent’s\tgoal\tis\tto\tfind\ta\tpolicy\tthat\twill\tmaximize\trewards\tover\ttime.\n\nFor\texample,\tthe\tMDP\trepresented\tin\tFigure\t16-8\thas\tthree\tstates\tand\tup\tto\tthree\tpossible\tdiscrete actions\tat\teach\tstep.\tIf\tit\tstarts\tin\tstate\ts0,\tthe\tagent\tcan\tchoose\tbetween\tactions\ta0,\ta1,\tor\ta2.\tIf\tit\tchooses action\ta1,\tit\tjust\tremains\tin\tstate\ts0\twith\tcertainty,\tand\twithout\tany\treward.\tIt\tcan\tthus\tdecide\tto\tstay\tthere forever\tif\tit\twants.\tBut\tif\tit\tchooses\taction\ta0,\tit\thas\ta\t70%\tprobability\tof\tgaining\ta\treward\tof\t+10,\tand remaining\tin\tstate\ts0.\tIt\tcan\tthen\ttry\tagain\tand\tagain\tto\tgain\tas\tmuch\treward\tas\tpossible.\tBut\tat\tone\tpoint\tit\n\nis\tgoing\tto\tend\tup\tinstead\tin\tstate\ts1.\tIn\tstate\ts1\tit\thas\tonly\ttwo\tpossible\tactions:\ta0\tor\ta2.\tIt\tcan\tchoose\tto stay\tput\tby\trepeatedly\tchoosing\taction\ta0,\tor\tit\tcan\tchoose\tto\tmove\ton\tto\tstate\ts2\tand\tget\ta\tnegative reward\tof\t–50\t(ouch).\tIn\tstate\ts2\tit\thas\tno\tother\tchoice\tthan\tto\ttake\taction\ta1,\twhich\twill\tmost\tlikely\tlead it\tback\tto\tstate\ts0,\tgaining\ta\treward\tof\t+40\ton\tthe\tway.\tYou\tget\tthe\tpicture.\tBy\tlooking\tat\tthis\tMDP,\tcan you\tguess\twhich\tstrategy\twill\tgain\tthe\tmost\treward\tover\ttime?\tIn\tstate\ts0\tit\tis\tclear\tthat\taction\ta0\tis\tthe best\toption,\tand\tin\tstate\ts2\tthe\tagent\thas\tno\tchoice\tbut\tto\ttake\taction\ta1,\tbut\tin\tstate\ts1\tit\tis\tnot\tobvious whether\tthe\tagent\tshould\tstay\tput\t(a0)\tor\tgo\tthrough\tthe\tfire\t(a2).\n\nFigure\t16-8.\tExample\tof\ta\tMarkov\tdecision\tprocess\n\nBellman\tfound\ta\tway\tto\testimate\tthe\toptimal\tstate\tvalue\tof\tany\tstate\ts,\tnoted\tV*(s),\twhich\tis\tthe\tsum\tof all\tdiscounted\tfuture\trewards\tthe\tagent\tcan\texpect\ton\taverage\tafter\tit\treaches\ta\tstate\ts,\tassuming\tit\tacts optimally.\tHe\tshowed\tthat\tif\tthe\tagent\tacts\toptimally,\tthen\tthe\tBellman\tOptimality\tEquation\tapplies\t(see Equation\t16-1).\tThis\trecursive\tequation\tsays\tthat\tif\tthe\tagent\tacts\toptimally,\tthen\tthe\toptimal\tvalue\tof\tthe current\tstate\tis\tequal\tto\tthe\treward\tit\twill\tget\ton\taverage\tafter\ttaking\tone\toptimal\taction,\tplus\tthe\texpected optimal\tvalue\tof\tall\tpossible\tnext\tstates\tthat\tthis\taction\tcan\tlead\tto.\n\nEquation\t16-1.\tBellman\tOptimality\tEquation\n\nT(s,\ta,\ts′)\tis\tthe\ttransition\tprobability\tfrom\tstate\ts\tto\tstate\ts′,\tgiven\tthat\tthe\tagent\tchose\taction\ta.\n\nR(s,\ta,\ts′)\tis\tthe\treward\tthat\tthe\tagent\tgets\twhen\tit\tgoes\tfrom\tstate\ts\tto\tstate\ts′,\tgiven\tthat\tthe\tagent chose\taction\ta.\n\nγ\tis\tthe\tdiscount\trate.\n\nThis\tequation\tleads\tdirectly\tto\tan\talgorithm\tthat\tcan\tprecisely\testimate\tthe\toptimal\tstate\tvalue\tof\tevery possible\tstate:\tyou\tfirst\tinitialize\tall\tthe\tstate\tvalue\testimates\tto\tzero,\tand\tthen\tyou\titeratively\tupdate\tthem using\tthe\tValue\tIteration\talgorithm\t(see\tEquation\t16-2).\tA\tremarkable\tresult\tis\tthat,\tgiven\tenough\ttime, these\testimates\tare\tguaranteed\tto\tconverge\tto\tthe\toptimal\tstate\tvalues,\tcorresponding\tto\tthe\toptimal\n\npolicy.\n\nEquation\t16-2.\tValue\tIteration\talgorithm\n\nVk(s)\tis\tthe\testimated\tvalue\tof\tstate\ts\tat\tthe\tkth\titeration\tof\tthe\talgorithm.\n\nNOTE\n\nThis\talgorithm\tis\tan\texample\tof\tDynamic\tProgramming,\twhich\tbreaks\tdown\ta\tcomplex\tproblem\t(in\tthis\tcase\testimating\ta potentially\tinfinite\tsum\tof\tdiscounted\tfuture\trewards)\tinto\ttractable\tsub-problems\tthat\tcan\tbe\ttackled\titeratively\t(in\tthis\tcase finding\tthe\taction\tthat\tmaximizes\tthe\taverage\treward\tplus\tthe\tdiscounted\tnext\tstate\tvalue).\n\nKnowing\tthe\toptimal\tstate\tvalues\tcan\tbe\tuseful,\tin\tparticular\tto\tevaluate\ta\tpolicy,\tbut\tit\tdoes\tnot\ttell\tthe agent\texplicitly\twhat\tto\tdo.\tLuckily,\tBellman\tfound\ta\tvery\tsimilar\talgorithm\tto\testimate\tthe\toptimal\tstate- action\tvalues,\tgenerally\tcalled\tQ-Values.\tThe\toptimal\tQ-Value\tof\tthe\tstate-action\tpair\t(s,a),\tnoted\tQ* (s,a),\tis\tthe\tsum\tof\tdiscounted\tfuture\trewards\tthe\tagent\tcan\texpect\ton\taverage\tafter\tit\treaches\tthe\tstate\ts and\tchooses\taction\ta,\tbut\tbefore\tit\tsees\tthe\toutcome\tof\tthis\taction,\tassuming\tit\tacts\toptimally\tafter\tthat action.\n\nHere\tis\thow\tit\tworks:\tonce\tagain,\tyou\tstart\tby\tinitializing\tall\tthe\tQ-Value\testimates\tto\tzero,\tthen\tyou update\tthem\tusing\tthe\tQ-Value\tIteration\talgorithm\t(see\tEquation\t16-3).\n\nEquation\t16-3.\tQ-Value\tIteration\talgorithm\n\nOnce\tyou\thave\tthe\toptimal\tQ-Values,\tdefining\tthe\toptimal\tpolicy,\tnoted\tπ*(s),\tis\ttrivial:\twhen\tthe\tagent\tis\n\nin\tstate\ts,\tit\tshould\tchoose\tthe\taction\twith\tthe\thighest\tQ-Value\tfor\tthat\tstate:\n\nLet’s\tapply\tthis\talgorithm\tto\tthe\tMDP\trepresented\tin\tFigure\t16-8.\tFirst,\twe\tneed\tto\tdefine\tthe\tMDP:\n\nnan=np.nan\t\t#\trepresents\timpossible\tactions T\t=\tnp.array([\t\t#\tshape=[s,\ta,\ts'] \t\t\t\t\t\t\t\t[[0.7,\t0.3,\t0.0],\t[1.0,\t0.0,\t0.0],\t[0.8,\t0.2,\t0.0]], \t\t\t\t\t\t\t\t[[0.0,\t1.0,\t0.0],\t[nan,\tnan,\tnan],\t[0.0,\t0.0,\t1.0]], \t\t\t\t\t\t\t\t[[nan,\tnan,\tnan],\t[0.8,\t0.1,\t0.1],\t[nan,\tnan,\tnan]], \t\t\t\t]) R\t=\tnp.array([\t\t#\tshape=[s,\ta,\ts'] \t\t\t\t\t\t\t\t[[10.,\t0.0,\t0.0],\t[0.0,\t0.0,\t0.0],\t[0.0,\t0.0,\t0.0]], \t\t\t\t\t\t\t\t[[10.,\t0.0,\t0.0],\t[nan,\tnan,\tnan],\t[0.0,\t0.0,\t-50.]], \t\t\t\t\t\t\t\t[[nan,\tnan,\tnan],\t[40.,\t0.0,\t0.0],\t[nan,\tnan,\tnan]], \t\t\t\t]) possible_actions\t=\t[[0,\t1,\t2],\t[0,\t2],\t[1]]\n\nNow\tlet’s\trun\tthe\tQ-Value\tIteration\talgorithm:\n\nQ\t=\tnp.full((3,\t3),\t-np.inf)\t\t#\t-inf\tfor\timpossible\tactions for\tstate,\tactions\tin\tenumerate(possible_actions): \t\t\t\tQ[state,\tactions]\t=\t0.0\t\t#\tInitial\tvalue\t=\t0.0,\tfor\tall\tpossible\tactions\n\n.\n\nlearning_rate\t=\t0.01 discount_rate\t=\t0.95 n_iterations\t=\t100\n\nfor\titeration\tin\trange(n_iterations): \t\t\t\tQ_prev\t=\tQ.copy() \t\t\t\tfor\ts\tin\trange(3): \t\t\t\t\t\t\t\tfor\ta\tin\tpossible_actions[s]: \t\t\t\t\t\t\t\t\t\t\t\tQ[s,\ta]\t=\tnp.sum([ \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tT[s,\ta,\tsp]\t*\t(R[s,\ta,\tsp]\t+\tdiscount_rate\t*\tnp.max(Q_prev[sp])) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tsp\tin\trange(3) \t\t\t\t\t\t\t\t\t\t\t\t])\n\nThe\tresulting\tQ-Values\tlook\tlike\tthis:\n\n>>>\tQ array([[\t21.89498982,\t\t20.80024033,\t\t16.86353093], \t\t\t\t\t\t\t[\t\t1.11669335,\t\t\t\t\t\t\t\t\t-inf,\t\t\t1.17573546], \t\t\t\t\t\t\t[\t\t\t\t\t\t\t\t-inf,\t\t53.86946068,\t\t\t\t\t\t\t\t\t-inf]]) >>>\tnp.argmax(Q,\taxis=1)\t\t#\toptimal\taction\tfor\teach\tstate array([0,\t2,\t1])\n\nThis\tgives\tus\tthe\toptimal\tpolicy\tfor\tthis\tMDP,\twhen\tusing\ta\tdiscount\trate\tof\t0.95:\tin\tstate\ts0\tchoose action\ta0,\tin\tstate\ts1\tchoose\taction\ta2\t(go\tthrough\tthe\tfire!),\tand\tin\tstate\ts2\tchoose\taction\ta1\t(the\tonly possible\taction).\tInterestingly,\tif\tyou\treduce\tthe\tdiscount\trate\tto\t0.9,\tthe\toptimal\tpolicy\tchanges:\tin\tstate s1\tthe\tbest\taction\tbecomes\ta0\t(stay\tput;\tdon’t\tgo\tthrough\tthe\tfire).\tIt\tmakes\tsense\tbecause\tif\tyou\tvalue\tthe present\tmuch\tmore\tthan\tthe\tfuture,\tthen\tthe\tprospect\tof\tfuture\trewards\tis\tnot\tworth\timmediate\tpain.\n\nTemporal\tDifference\tLearning\tand\tQ-Learning Reinforcement\tLearning\tproblems\twith\tdiscrete\tactions\tcan\toften\tbe\tmodeled\tas\tMarkov\tdecision processes,\tbut\tthe\tagent\tinitially\thas\tno\tidea\twhat\tthe\ttransition\tprobabilities\tare\t(it\tdoes\tnot\tknow\tT(s,\ta, s′)),\tand\tit\tdoes\tnot\tknow\twhat\tthe\trewards\tare\tgoing\tto\tbe\teither\t(it\tdoes\tnot\tknow\tR(s,\ta,\ts′)).\tIt\tmust experience\teach\tstate\tand\teach\ttransition\tat\tleast\tonce\tto\tknow\tthe\trewards,\tand\tit\tmust\texperience\tthem multiple\ttimes\tif\tit\tis\tto\thave\ta\treasonable\testimate\tof\tthe\ttransition\tprobabilities.\n\nThe\tTemporal\tDifference\tLearning\t(TD\tLearning)\talgorithm\tis\tvery\tsimilar\tto\tthe\tValue\tIteration algorithm,\tbut\ttweaked\tto\ttake\tinto\taccount\tthe\tfact\tthat\tthe\tagent\thas\tonly\tpartial\tknowledge\tof\tthe\tMDP. In\tgeneral\twe\tassume\tthat\tthe\tagent\tinitially\tknows\tonly\tthe\tpossible\tstates\tand\tactions,\tand\tnothing\tmore. The\tagent\tuses\tan\texploration\tpolicy\t—\tfor\texample,\ta\tpurely\trandom\tpolicy\t—\tto\texplore\tthe\tMDP,\tand as\tit\tprogresses\tthe\tTD\tLearning\talgorithm\tupdates\tthe\testimates\tof\tthe\tstate\tvalues\tbased\ton\tthe transitions\tand\trewards\tthat\tare\tactually\tobserved\t(see\tEquation\t16-4).\n\nEquation\t16-4.\tTD\tLearning\talgorithm\n\nα\tis\tthe\tlearning\trate\t(e.g.,\t0.01).\n\nTIP\n\nTD\tLearning\thas\tmany\tsimilarities\twith\tStochastic\tGradient\tDescent,\tin\tparticular\tthe\tfact\tthat\tit\thandles\tone\tsample\tat\ta\ttime. Just\tlike\tSGD,\tit\tcan\tonly\ttruly\tconverge\tif\tyou\tgradually\treduce\tthe\tlearning\trate\t(otherwise\tit\twill\tkeep\tbouncing\taround\tthe optimum).\n\nFor\teach\tstate\ts,\tthis\talgorithm\tsimply\tkeeps\ttrack\tof\ta\trunning\taverage\tof\tthe\timmediate\trewards\tthe agent\tgets\tupon\tleaving\tthat\tstate,\tplus\tthe\trewards\tit\texpects\tto\tget\tlater\t(assuming\tit\tacts\toptimally).\n\nSimilarly,\tthe\tQ-Learning\talgorithm\tis\tan\tadaptation\tof\tthe\tQ-Value\tIteration\talgorithm\tto\tthe\tsituation where\tthe\ttransition\tprobabilities\tand\tthe\trewards\tare\tinitially\tunknown\t(see\tEquation\t16-5).\n\nEquation\t16-5.\tQ-Learning\talgorithm\n\nFor\teach\tstate-action\tpair\t(s,\ta),\tthis\talgorithm\tkeeps\ttrack\tof\ta\trunning\taverage\tof\tthe\trewards\tr\tthe\tagent gets\tupon\tleaving\tthe\tstate\ts\twith\taction\ta,\tplus\tthe\trewards\tit\texpects\tto\tget\tlater.\tSince\tthe\ttarget\tpolicy would\tact\toptimally,\twe\ttake\tthe\tmaximum\tof\tthe\tQ-Value\testimates\tfor\tthe\tnext\tstate.\n\nHere\tis\thow\tQ-Learning\tcan\tbe\timplemented:\n\nimport\tnumpy.random\tas\trnd\n\nlearning_rate0\t=\t0.05 learning_rate_decay\t=\t0.1\n\nn_iterations\t=\t20000\n\ns\t=\t0\t#\tstart\tin\tstate\t0\n\nQ\t=\tnp.full((3,\t3),\t-np.inf)\t\t#\t-inf\tfor\timpossible\tactions for\tstate,\tactions\tin\tenumerate(possible_actions): \t\t\t\tQ[state,\tactions]\t=\t0.0\t\t#\tInitial\tvalue\t=\t0.0,\tfor\tall\tpossible\tactions\n\nfor\titeration\tin\trange(n_iterations): \t\t\t\ta\t=\trnd.choice(possible_actions[s])\t\t#\tchoose\tan\taction\t(randomly) \t\t\t\tsp\t=\trnd.choice(range(3),\tp=T[s,\ta])\t#\tpick\tnext\tstate\tusing\tT[s,\ta] \t\t\t\treward\t=\tR[s,\ta,\tsp] \t\t\t\tlearning_rate\t=\tlearning_rate0\t/\t(1\t+\titeration\t*\tlearning_rate_decay) \t\t\t\tQ[s,\ta]\t=\tlearning_rate\t*\tQ[s,\ta]\t+\t(1\t-\tlearning_rate)\t*\t( \t\t\t\t\t\t\t\t\t\t\t\treward\t+\tdiscount_rate\t*\tnp.max(Q[sp]) \t\t\t\t\t\t\t\t) \t\t\t\ts\t=\tsp\t#\tmove\tto\tnext\tstate\n\nGiven\tenough\titerations,\tthis\talgorithm\twill\tconverge\tto\tthe\toptimal\tQ-Values.\tThis\tis\tcalled\tan\toff-policy algorithm\tbecause\tthe\tpolicy\tbeing\ttrained\tis\tnot\tthe\tone\tbeing\texecuted.\tIt\tis\tsomewhat\tsurprising\tthat this\talgorithm\tis\tcapable\tof\tlearning\tthe\toptimal\tpolicy\tby\tjust\twatching\tan\tagent\tact\trandomly\t(imagine learning\tto\tplay\tgolf\twhen\tyour\tteacher\tis\ta\tdrunken\tmonkey).\tCan\twe\tdo\tbetter?\n\nExploration\tPolicies Of\tcourse\tQ-Learning\tcan\twork\tonly\tif\tthe\texploration\tpolicy\texplores\tthe\tMDP\tthoroughly\tenough. Although\ta\tpurely\trandom\tpolicy\tis\tguaranteed\tto\teventually\tvisit\tevery\tstate\tand\tevery\ttransition\tmany times,\tit\tmay\ttake\tan\textremely\tlong\ttime\tto\tdo\tso.\tTherefore,\ta\tbetter\toption\tis\tto\tuse\tthe\tε-greedy\tpolicy: at\teach\tstep\tit\tacts\trandomly\twith\tprobability\tε,\tor\tgreedily\t(choosing\tthe\taction\twith\tthe\thighest\tQ-Value) with\tprobability\t1-ε.\tThe\tadvantage\tof\tthe\tε-greedy\tpolicy\t(compared\tto\ta\tcompletely\trandom\tpolicy)\tis that\tit\twill\tspend\tmore\tand\tmore\ttime\texploring\tthe\tinteresting\tparts\tof\tthe\tenvironment,\tas\tthe\tQ-Value estimates\tget\tbetter\tand\tbetter,\twhile\tstill\tspending\tsome\ttime\tvisiting\tunknown\tregions\tof\tthe\tMDP.\tIt\tis quite\tcommon\tto\tstart\twith\ta\thigh\tvalue\tfor\tε\t(e.g.,\t1.0)\tand\tthen\tgradually\treduce\tit\t(e.g.,\tdown\tto\t0.05).\n\nAlternatively,\trather\tthan\trelying\ton\tchance\tfor\texploration,\tanother\tapproach\tis\tto\tencourage\tthe exploration\tpolicy\tto\ttry\tactions\tthat\tit\thas\tnot\ttried\tmuch\tbefore.\tThis\tcan\tbe\timplemented\tas\ta\tbonus added\tto\tthe\tQ-Value\testimates,\tas\tshown\tin\tEquation\t16-6.\n\nEquation\t16-6.\tQ-Learning\tusing\tan\texploration\tfunction\n\nN(s′,\ta′)\tcounts\tthe\tnumber\tof\ttimes\tthe\taction\ta′\twas\tchosen\tin\tstate\ts′.\n\nf(q,\tn)\tis\tan\texploration\tfunction,\tsuch\tas\tf(q,\tn)\t=\tq\t+\tK/(1\t+\tn),\twhere\tK\tis\ta\tcuriosity hyperparameter\tthat\tmeasures\thow\tmuch\tthe\tagent\tis\tattracted\tto\tto\tthe\tunknown.\n\nApproximate\tQ-Learning The\tmain\tproblem\twith\tQ-Learning\tis\tthat\tit\tdoes\tnot\tscale\twell\tto\tlarge\t(or\teven\tmedium)\tMDPs\twith many\tstates\tand\tactions.\tConsider\ttrying\tto\tuse\tQ-Learning\tto\ttrain\tan\tagent\tto\tplay\tMs.\tPac-Man.\tThere are\tover\t250\tpellets\tthat\tMs.\tPac-Man\tcan\teat,\teach\tof\twhich\tcan\tbe\tpresent\tor\tabsent\t(i.e.,\talready eaten).\tSo\tthe\tnumber\tof\tpossible\tstates\tis\tgreater\tthan\t2250\t≈\t1075\t(and\tthat’s\tconsidering\tthe\tpossible states\tonly\tof\tthe\tpellets).\tThis\tis\tway\tmore\tthan\tatoms\tin\tthe\tobservable\tuniverse,\tso\tthere’s\tabsolutely no\tway\tyou\tcan\tkeep\ttrack\tof\tan\testimate\tfor\tevery\tsingle\tQ-Value.\n\nThe\tsolution\tis\tto\tfind\ta\tfunction\tthat\tapproximates\tthe\tQ-Values\tusing\ta\tmanageable\tnumber\tof parameters.\tThis\tis\tcalled\tApproximate\tQ-Learning.\tFor\tyears\tit\twas\trecommended\tto\tuse\tlinear combinations\tof\thand-crafted\tfeatures\textracted\tfrom\tthe\tstate\t(e.g.,\tdistance\tof\tthe\tclosest\tghosts,\ttheir directions,\tand\tso\ton)\tto\testimate\tQ-Values,\tbut\tDeepMind\tshowed\tthat\tusing\tdeep\tneural\tnetworks\tcan work\tmuch\tbetter,\tespecially\tfor\tcomplex\tproblems,\tand\tit\tdoes\tnot\trequire\tany\tfeature\tengineering.\tA DNN\tused\tto\testimate\tQ-Values\tis\tcalled\ta\tdeep\tQ-network\t(DQN),\tand\tusing\ta\tDQN\tfor\tApproximate Q-Learning\tis\tcalled\tDeep\tQ-Learning.\n\nIn\tthe\trest\tof\tthis\tchapter,\twe\twill\tuse\tDeep\tQ-Learning\tto\ttrain\tan\tagent\tto\tplay\tMs.\tPac-Man,\tmuch\tlike DeepMind\tdid\tin\t2013.\tThe\tcode\tcan\teasily\tbe\ttweaked\tto\tlearn\tto\tplay\tthe\tmajority\tof\tAtari\tgames\tquite well.\tIt\tcan\tachieve\tsuperhuman\tskill\tat\tmost\taction\tgames,\tbut\tit\tis\tnot\tso\tgood\tat\tgames\twith\tlong- running\tstorylines.\n\nLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning Since\twe\twill\tbe\tusing\tan\tAtari\tenvironment,\twe\tmust\tfirst\tinstall\tOpenAI\tgym’s\tAtari\tdependencies. While\twe’re\tat\tit,\twe\twill\talso\tinstall\tdependencies\tfor\tother\tOpenAI\tgym\tenvironments\tthat\tyou\tmay want\tto\tplay\twith.\tOn\tmacOS,\tassuming\tyou\thave\tinstalled\tHomebrew,\tyou\tneed\tto\trun:\n\n$\tbrew\tinstall\tcmake\tboost\tboost-python\tsdl2\tswig\twget\n\nOn\tUbuntu,\ttype\tthe\tfollowing\tcommand\t(replacing\tpython3\twith\tpython\tif\tyou\tare\tusing\tPython\t2):\n\n$\tapt-get\tinstall\t-y\tpython3-numpy\tpython3-dev\tcmake\tzlib1g-dev\tlibjpeg-dev\\ \t\t\t\txvfb\tlibav-tools\txorg-dev\tpython3-opengl\tlibboost-all-dev\tlibsdl2-dev\tswig\n\nThen\tinstall\tthe\textra\tPython\tmodules:\n\n$\tpip3\tinstall\t--upgrade\t'gym[all]'\n\nIf\teverything\twent\twell,\tyou\tshould\tbe\table\tto\tcreate\ta\tMs.\tPac-Man\tenvironment:\n\n>>>\tenv\t=\tgym.make(\"MsPacman-v0\") >>>\tobs\t=\tenv.reset() >>>\tobs.shape\t\t#\t[height,\twidth,\tchannels] (210,\t160,\t3) >>>\tenv.action_space Discrete(9)\n\nAs\tyou\tcan\tsee,\tthere\tare\tnine\tdiscrete\tactions\tavailable,\twhich\tcorrespond\tto\tthe\tnine\tpossible\tpositions of\tthe\tjoystick\t(left,\tright,\tup,\tdown,\tcenter,\tupper\tleft,\tand\tso\ton),\tand\tthe\tobservations\tare\tsimply screenshots\tof\tthe\tAtari\tscreen\t(see\tFigure\t16-9,\tleft),\trepresented\tas\t3D\tNumPy\tarrays.\tThese\timages are\ta\tbit\tlarge,\tso\twe\twill\tcreate\ta\tsmall\tpreprocessing\tfunction\tthat\twill\tcrop\tthe\timage\tand\tshrink\tit down\tto\t88\t×\t80\tpixels,\tconvert\tit\tto\tgrayscale,\tand\timprove\tthe\tcontrast\tof\tMs.\tPac-Man.\tThis\twill reduce\tthe\tamount\tof\tcomputations\trequired\tby\tthe\tDQN,\tand\tspeed\tup\ttraining.\n\nmspacman_color\t=\tnp.array([210,\t164,\t74]).mean()\n\ndef\tpreprocess_observation(obs): \t\t\t\timg\t=\tobs[1:176:2,\t::2]\t#\tcrop\tand\tdownsize \t\t\t\timg\t=\timg.mean(axis=2)\t#\tto\tgreyscale \t\t\t\timg[img==mspacman_color]\t=\t0\t#\timprove\tcontrast \t\t\t\timg\t=\t(img\t-\t128)\t/\t128\t-\t1\t#\tnormalize\tfrom\t-1.\tto\t1. \t\t\t\treturn\timg.reshape(88,\t80,\t1)\n\nThe\tresult\tof\tpreprocessing\tis\tshown\tin\tFigure\t16-9\t(right).\n\nFigure\t16-9.\tMs.\tPac-Man\tobservation,\toriginal\t(left)\tand\tafter\tpreprocessing\t(right)\n\nNext,\tlet’s\tcreate\tthe\tDQN.\tIt\tcould\tjust\ttake\ta\tstate-action\tpair\t(s,a)\tas\tinput,\tand\toutput\tan\testimate\tof the\tcorresponding\tQ-Value\tQ(s,a),\tbut\tsince\tthe\tactions\tare\tdiscrete\tit\tis\tmore\tconvenient\tto\tuse\ta\tneural network\tthat\ttakes\tonly\ta\tstate\ts\tas\tinput\tand\toutputs\tone\tQ-Value\testimate\tper\taction.\tThe\tDQN\twill\tbe composed\tof\tthree\tconvolutional\tlayers,\tfollowed\tby\ttwo\tfully\tconnected\tlayers,\tincluding\tthe\toutput layer\t(see\tFigure\t16-10).\n\nFigure\t16-10.\tDeep\tQ-network\tto\tplay\tMs.\tPac-Man\n\nAs\twe\twill\tsee,\tthe\ttraining\talgorithm\twe\twill\tuse\trequires\ttwo\tDQNs\twith\tthe\tsame\tarchitecture\t(but different\tparameters):\tone\twill\tbe\tused\tto\tdrive\tMs.\tPac-Man\tduring\ttraining\t(the\tactor),\tand\tthe\tother will\twatch\tthe\tactor\tand\tlearn\tfrom\tits\ttrials\tand\terrors\t(the\tcritic).\tAt\tregular\tintervals\twe\twill\tcopy\tthe critic\tto\tthe\tactor.\tSince\twe\tneed\ttwo\tidentical\tDQNs,\twe\twill\tcreate\ta\tq_network()\tfunction\tto\tbuild them:\n\ninput_height\t=\t88 input_width\t=\t80 input_channels\t=\t1 conv_n_maps\t=\t[32,\t64,\t64] conv_kernel_sizes\t=\t[(8,8),\t(4,4),\t(3,3)] conv_strides\t=\t[4,\t2,\t1] conv_paddings\t=\t[\"SAME\"]\t*\t3 conv_activation\t=\t[tf.nn.relu]\t*\t3\n\nn_hidden_in\t=\t64\t*\t11\t*\t10\t\t#\tconv3\thas\t64\tmaps\tof\t11x10\teach n_hidden\t=\t512 hidden_activation\t=\ttf.nn.relu n_outputs\t=\tenv.action_space.n\t\t#\t9\tdiscrete\tactions\tare\tavailable initializer\t=\ttf.contrib.layers.variance_scaling_initializer()\n\ndef\tq_network(X_state,\tname): \t\t\t\tprev_layer\t=\tX_state \t\t\t\tconv_layers\t=\t[] \t\t\t\twith\ttf.variable_scope(name)\tas\tscope: \t\t\t\t\t\t\t\tfor\tn_maps,\tkernel_size,\tstride,\tpadding,\tactivation\tin\tzip( \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tconv_n_maps,\tconv_kernel_sizes,\tconv_strides, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tconv_paddings,\tconv_activation): \t\t\t\t\t\t\t\t\t\t\t\tprev_layer\t=\ttf.layers.conv2d( \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tprev_layer,\tfilters=n_maps,\tkernel_size=kernel_size, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstride=stride,\tpadding=padding,\tactivation=activation, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) \t\t\t\t\t\t\t\t\t\t\t\tconv_layers.append(prev_layer) \t\t\t\t\t\t\t\tlast_conv_layer_flat\t=\ttf.reshape(prev_layer,\tshape=[-1,\tn_hidden_in]) \t\t\t\t\t\t\t\thidden\t=\ttf.layers.dense(last_conv_layer_flat,\tn_hidden, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=hidden_activation, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) \t\t\t\t\t\t\t\toutputs\t=\ttf.layers.dense(hidden,\tn_outputs, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) \t\t\t\ttrainable_vars\t=\ttf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscope=scope.name) \t\t\t\ttrainable_vars_by_name\t=\t{var.name[len(scope.name):]:\tvar \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tvar\tin\ttrainable_vars} \t\t\t\treturn\toutputs,\ttrainable_vars_by_name\n\nThe\tfirst\tpart\tof\tthis\tcode\tdefines\tthe\thyperparameters\tof\tthe\tDQN\tarchitecture.\tThen\tthe\tq_network() function\tcreates\tthe\tDQN,\ttaking\tthe\tenvironment’s\tstate\tX_state\tas\tinput,\tand\tthe\tname\tof\tthe\tvariable scope.\tNote\tthat\twe\twill\tjust\tuse\tone\tobservation\tto\trepresent\tthe\tenvironment’s\tstate\tsince\tthere’s\talmost no\thidden\tstate\t(except\tfor\tblinking\tobjects\tand\tthe\tghosts’\tdirections).\n\nThe\ttrainable_vars_by_name\tdictionary\tgathers\tall\tthe\ttrainable\tvariables\tof\tthis\tDQN.\tIt\twill\tbe useful\tin\ta\tminute\twhen\twe\tcreate\toperations\tto\tcopy\tthe\tcritic\tDQN\tto\tthe\tactor\tDQN.\tThe\tkeys\tof\tthe dictionary\tare\tthe\tnames\tof\tthe\tvariables,\tstripping\tthe\tpart\tof\tthe\tprefix\tthat\tjust\tcorresponds\tto\tthe scope’s\tname.\tIt\tlooks\tlike\tthis:\n\n>>>\ttrainable_vars_by_name {'/conv2d/bias:0':\t<tensorflow.python.ops.variables.Variable\tat\t0x121cf7b50>, \t'/conv2d/kernel:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/conv2d_1/bias:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/conv2d_1/kernel:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/conv2d_2/bias:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/conv2d_2/kernel:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/dense/bias:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/dense/kernel:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/dense_1/bias:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/dense_1/kernel:0':\t<tensorflow.python.ops.variables.Variable...>}\n\nNow\tlet’s\tcreate\tthe\tinput\tplaceholder,\tthe\ttwo\tDQNs,\tand\tthe\toperation\tto\tcopy\tthe\tcritic\tDQN\tto\tthe actor\tDQN:\n\nX_state\t=\ttf.placeholder(tf.float32,\tshape=[None,\tinput_height,\tinput_width, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tinput_channels]) actor_q_values,\tactor_vars\t=\tq_network(X_state,\tname=\"q_networks/actor\") critic_q_values,\tcritic_vars\t=\tq_network(X_state,\tname=\"q_networks/critic\")\n\ncopy_ops\t=\t[actor_var.assign(critic_vars[var_name]) \t\t\t\t\t\t\t\t\t\t\t\tfor\tvar_name,\tactor_var\tin\tactor_vars.items()] copy_critic_to_actor\t=\ttf.group(*copy_ops)\n\nLet’s\tstep\tback\tfor\ta\tsecond:\twe\tnow\thave\ttwo\tDQNs\tthat\tare\tboth\tcapable\tof\ttaking\tan\tenvironment\tstate (i.e.,\ta\tpreprocessed\tobservation)\tas\tinput\tand\toutputting\tan\testimated\tQ-Value\tfor\teach\tpossible\taction\tin that\tstate.\tPlus\twe\thave\tan\toperation\tcalled\tcopy_critic_to_actor\tto\tcopy\tall\tthe\ttrainable\tvariables of\tthe\tcritic\tDQN\tto\tthe\tactor\tDQN.\tWe\tuse\tTensorFlow’s\ttf.group()\tfunction\tto\tgroup\tall\tthe assignment\toperations\tinto\ta\tsingle\tconvenient\toperation.\n\nThe\tactor\tDQN\tcan\tbe\tused\tto\tplay\tMs.\tPac-Man\t(initially\tvery\tbadly).\tAs\tdiscussed\tearlier,\tyou\twant\tit to\texplore\tthe\tgame\tthoroughly\tenough,\tso\tyou\tgenerally\twant\tto\tcombine\tit\twith\tan\tε-greedy\tpolicy\tor another\texploration\tstrategy.\n\nBut\twhat\tabout\tthe\tcritic\tDQN?\tHow\twill\tit\tlearn\tto\tplay\tthe\tgame?\tThe\tshort\tanswer\tis\tthat\tit\twill\ttry\tto make\tits\tQ-Value\tpredictions\tmatch\tthe\tQ-Values\testimated\tby\tthe\tactor\tthrough\tits\texperience\tof\tthe game.\tSpecifically,\twe\twill\tlet\tthe\tactor\tplay\tfor\ta\twhile,\tstoring\tall\tits\texperiences\tin\ta\treplay\tmemory. Each\tmemory\twill\tbe\ta\t5-tuple\t(state,\taction,\tnext\tstate,\treward,\tcontinue),\twhere\tthe\t“continue”\titem\twill be\tequal\tto\t0.0\twhen\tthe\tgame\tis\tover,\tor\t1.0\totherwise.\tNext,\tat\tregular\tintervals\twe\twill\tsample\ta\tbatch of\tmemories\tfrom\tthe\treplay\tmemory,\tand\twe\twill\testimate\tthe\tQ-Values\tfrom\tthese\tmemories.\tFinally, we\twill\ttrain\tthe\tcritic\tDQN\tto\tpredict\tthese\tQ-Values\tusing\tregular\tsupervised\tlearning\ttechniques.\tOnce every\tfew\ttraining\titerations,\twe\twill\tcopy\tthe\tcritic\tDQN\tto\tthe\tactor\tDQN.\tAnd\tthat’s\tit!\tEquation\t16-7 shows\tthe\tcost\tfunction\tused\tto\ttrain\tthe\tcritic\tDQN:\n\nEquation\t16-7.\tDeep\tQ-Learning\tcost\tfunction\n\ns(i),\ta(i),\tr(i)\tand\ts′(i)\tare\trespectively\tthe\tstate,\taction,\treward,\tand\tnext\tstate\tof\tthe\tith\tmemory sampled\tfrom\tthe\treplay\tmemory.\n\nm\tis\tthe\tsize\tof\tthe\tmemory\tbatch.\n\nθcritic\tand\tθactor\tare\tthe\tcritic\tand\tthe\tactor’s\tparameters.\n\nQ(s(i),a(i),θcritic)\tis\tthe\tcritic\tDQN’s\tprediction\tof\tthe\tith\tmemorized\tstate-action’s\tQ-Value.\n\nQ(s′(i),\ta′,\tθactor)\tis\tthe\tactor\tDQN’s\tprediction\tof\tthe\tQ-Value\tit\tcan\texpect\tfrom\tthe\tnext\tstate\ts′(i)\tif it\tchooses\taction\ta′.\n\ny(i)\tis\tthe\ttarget\tQ-Value\tfor\tthe\tith\tmemory.\tNote\tthat\tit\tis\tequal\tto\tthe\treward\tactually\tobserved\tby the\tactor,\tplus\tthe\tactor’s\tprediction\tof\twhat\tfuture\trewards\tit\tshould\texpect\tif\tit\twere\tto\tplay optimally\t(as\tfar\tas\tit\tknows).\n\nJ(θcritic)\tis\tthe\tcost\tfunction\tused\tto\ttrain\tthe\tcritic\tDQN.\tAs\tyou\tcan\tsee,\tit\tis\tjust\tthe\tMean\tSquared\n\nError\tbetween\tthe\ttarget\tQ-Values\ty(i)\tas\testimated\tby\tthe\tactor\tDQN,\tand\tthe\tcritic\tDQN’s predictions\tof\tthese\tQ-Values.\n\nNOTE\n\nThe\treplay\tmemory\tis\toptional,\tbut\thighly\trecommended.\tWithout\tit,\tyou\twould\ttrain\tthe\tcritic\tDQN\tusing\tconsecutive experiences\tthat\tmay\tbe\tvery\tcorrelated.\tThis\twould\tintroduce\ta\tlot\tof\tbias\tand\tslow\tdown\tthe\ttraining\talgorithm’s\tconvergence. By\tusing\ta\treplay\tmemory,\twe\tensure\tthat\tthe\tmemories\tfed\tto\tthe\ttraining\talgorithm\tcan\tbe\tfairly\tuncorrelated.\n\nLet’s\tadd\tthe\tcritic\tDQN’s\ttraining\toperations.\tFirst,\twe\tneed\tto\tbe\table\tto\tcompute\tits\tpredicted\tQ- Values\tfor\teach\tstate-action\tin\tthe\tmemory\tbatch.\tSince\tthe\tDQN\toutputs\tone\tQ-Value\tfor\tevery\tpossible action,\twe\tneed\tto\tkeep\tonly\tthe\tQ-Value\tthat\tcorresponds\tto\tthe\taction\tthat\twas\tactually\tchosen\tin\tthis memory.\tFor\tthis,\twe\twill\tconvert\tthe\taction\tto\ta\tone-hot\tvector\t(recall\tthat\tthis\tis\ta\tvector\tfull\tof\t0s except\tfor\ta\t1\tat\tthe\tith\tindex),\tand\tmultiply\tit\tby\tthe\tQ-Values:\tthis\twill\tzero\tout\tall\tQ-Values\texcept\tfor the\tone\tcorresponding\tto\tthe\tmemorized\taction.\tThen\tjust\tsum\tover\tthe\tfirst\taxis\tto\tobtain\tonly\tthe\tdesired Q-Value\tprediction\tfor\teach\tmemory.\n\nX_action\t=\ttf.placeholder(tf.int32,\tshape=[None]) q_value\t=\ttf.reduce_sum(critic_q_values\t*\ttf.one_hot(X_action,\tn_outputs), \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taxis=1,\tkeep_dims=True)\n\nNext\tlet’s\tadd\tthe\ttraining\toperations,\tassuming\tthe\ttarget\tQ-Values\twill\tbe\tfed\tthrough\ta\tplaceholder.\tWe also\tcreate\ta\tnontrainable\tvariable\tcalled\tglobal_step.\tThe\toptimizer’s\tminimize()\toperation\twill take\tcare\tof\tincrementing\tit.\tPlus\twe\tcreate\tthe\tusual\tinit\toperation\tand\ta\tSaver.\n\ny\t=\ttf.placeholder(tf.float32,\tshape=[None,\t1]) cost\t=\ttf.reduce_mean(tf.square(y\t-\tq_value)) global_step\t=\ttf.Variable(0,\ttrainable=False,\tname='global_step') optimizer\t=\ttf.train.AdamOptimizer(learning_rate) training_op\t=\toptimizer.minimize(cost,\tglobal_step=global_step)\n\ninit\t=\ttf.global_variables_initializer() saver\t=\ttf.train.Saver()\n\nThat’s\tit\tfor\tthe\tconstruction\tphase.\tBefore\twe\tlook\tat\tthe\texecution\tphase,\twe\twill\tneed\ta\tcouple\tof tools.\tFirst,\tlet’s\tstart\tby\timplementing\tthe\treplay\tmemory.\tWe\twill\tuse\ta\tdeque\tlist\tsince\tit\tis\tvery efficient\tat\tpushing\titems\tto\tthe\tqueue\tand\tpopping\tthem\tout\tfrom\tthe\tend\tof\tthe\tlist\twhen\tthe\tmaximum memory\tsize\tis\treached.\tWe\twill\talso\twrite\ta\tsmall\tfunction\tto\trandomly\tsample\ta\tbatch\tof\texperiences from\tthe\treplay\tmemory:\n\nfrom\tcollections\timport\tdeque\n\nreplay_memory_size\t=\t10000 replay_memory\t=\tdeque([],\tmaxlen=replay_memory_size)\n\ndef\tsample_memories(batch_size): \t\t\t\tindices\t=\trnd.permutation(len(replay_memory))[:batch_size] \t\t\t\tcols\t=\t[[],\t[],\t[],\t[],\t[]]\t#\tstate,\taction,\treward,\tnext_state,\tcontinue \t\t\t\tfor\tidx\tin\tindices: \t\t\t\t\t\t\t\tmemory\t=\treplay_memory[idx] \t\t\t\t\t\t\t\tfor\tcol,\tvalue\tin\tzip(cols,\tmemory): \t\t\t\t\t\t\t\t\t\t\t\tcol.append(value) \t\t\t\tcols\t=\t[np.array(col)\tfor\tcol\tin\tcols]\n\nreturn\t(cols[0],\tcols[1],\tcols[2].reshape(-1,\t1),\tcols[3], \t\t\t\t\t\t\t\t\t\t\t\tcols[4].reshape(-1,\t1))\n\nNext,\twe\twill\tneed\tthe\tactor\tto\texplore\tthe\tgame.\tWe\twill\tuse\tthe\tε-greedy\tpolicy,\tand\tgradually\tdecrease ε\tfrom\t1.0\tto\t0.05,\tin\t50,000\ttraining\tsteps:\n\neps_min\t=\t0.05 eps_max\t=\t1.0 eps_decay_steps\t=\t50000\n\ndef\tepsilon_greedy(q_values,\tstep): \t\t\t\tepsilon\t=\tmax(eps_min,\teps_max\t-\t(eps_max-eps_min)\t*\tstep/eps_decay_steps) \t\t\t\tif\trnd.rand()\t<\tepsilon: \t\t\t\t\t\t\t\treturn\trnd.randint(n_outputs)\t#\trandom\taction \t\t\t\telse: \t\t\t\t\t\t\t\treturn\tnp.argmax(q_values)\t#\toptimal\taction\n\nThat’s\tit!\tWe\thave\tall\twe\tneed\tto\tstart\ttraining.\tThe\texecution\tphase\tdoes\tnot\tcontain\tanything\ttoo complex,\tbut\tit\tis\ta\tbit\tlong,\tso\ttake\ta\tdeep\tbreath.\tReady?\tLet’s\tgo!\tFirst,\tlet’s\tinitialize\ta\tfew\tvariables:\n\nn_steps\t=\t100000\t\t#\ttotal\tnumber\tof\ttraining\tsteps training_start\t=\t1000\t\t#\tstart\ttraining\tafter\t1,000\tgame\titerations training_interval\t=\t3\t\t#\trun\ta\ttraining\tstep\tevery\t3\tgame\titerations save_steps\t=\t50\t\t#\tsave\tthe\tmodel\tevery\t50\ttraining\tsteps copy_steps\t=\t25\t\t#\tcopy\tthe\tcritic\tto\tthe\tactor\tevery\t25\ttraining\tsteps discount_rate\t=\t0.95 skip_start\t=\t90\t\t#\tskip\tthe\tstart\tof\tevery\tgame\t(it's\tjust\twaiting\ttime) batch_size\t=\t50 iteration\t=\t0\t\t#\tgame\titerations checkpoint_path\t=\t\"./my_dqn.ckpt\" done\t=\tTrue\t#\tenv\tneeds\tto\tbe\treset\n\nNext,\tlet’s\topen\tthe\tsession\tand\trun\tthe\tmain\ttraining\tloop:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tif\tos.path.isfile(checkpoint_path): \t\t\t\t\t\t\t\tsaver.restore(sess,\tcheckpoint_path) \t\t\t\telse: \t\t\t\t\t\t\t\tinit.run() \t\t\t\twhile\tTrue: \t\t\t\t\t\t\t\tstep\t=\tglobal_step.eval() \t\t\t\t\t\t\t\tif\tstep\t>=\tn_steps: \t\t\t\t\t\t\t\t\t\t\t\tbreak \t\t\t\t\t\t\t\titeration\t+=\t1 \t\t\t\t\t\t\t\tif\tdone:\t#\tgame\tover,\tstart\tagain \t\t\t\t\t\t\t\t\t\t\t\tobs\t=\tenv.reset() \t\t\t\t\t\t\t\t\t\t\t\tfor\tskip\tin\trange(skip_start):\t#\tskip\tthe\tstart\tof\teach\tgame \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tobs,\treward,\tdone,\tinfo\t=\tenv.step(0) \t\t\t\t\t\t\t\t\t\t\t\tstate\t=\tpreprocess_observation(obs)\n\n#\tActor\tevaluates\twhat\tto\tdo \t\t\t\t\t\t\t\tq_values\t=\tactor_q_values.eval(feed_dict={X_state:\t[state]}) \t\t\t\t\t\t\t\taction\t=\tepsilon_greedy(q_values,\tstep)\n\n#\tActor\tplays \t\t\t\t\t\t\t\tobs,\treward,\tdone,\tinfo\t=\tenv.step(action) \t\t\t\t\t\t\t\tnext_state\t=\tpreprocess_observation(obs)\n\n#\tLet's\tmemorize\twhat\tjust\thappened \t\t\t\t\t\t\t\treplay_memory.append((state,\taction,\treward,\tnext_state,\t1.0\t-\tdone)) \t\t\t\t\t\t\t\tstate\t=\tnext_state\n\nif\titeration\t<\ttraining_start\tor\titeration\t%\ttraining_interval\t!=\t0: \t\t\t\t\t\t\t\t\t\t\t\tcontinue\n\n#\tCritic\tlearns \t\t\t\t\t\t\t\tX_state_val,\tX_action_val,\trewards,\tX_next_state_val,\tcontinues\t=\t( \t\t\t\t\t\t\t\t\t\t\t\tsample_memories(batch_size)) \t\t\t\t\t\t\t\tnext_q_values\t=\tactor_q_values.eval(\n\nfeed_dict={X_state:\tX_next_state_val}) \t\t\t\t\t\t\t\tmax_next_q_values\t=\tnp.max(next_q_values,\taxis=1,\tkeepdims=True) \t\t\t\t\t\t\t\ty_val\t=\trewards\t+\tcontinues\t*\tdiscount_rate\t*\tmax_next_q_values \t\t\t\t\t\t\t\ttraining_op.run(feed_dict={X_state:\tX_state_val, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tX_action:\tX_action_val,\ty:\ty_val})\n\n#\tRegularly\tcopy\tcritic\tto\tactor \t\t\t\t\t\t\t\tif\tstep\t%\tcopy_steps\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tcopy_critic_to_actor.run()\n\n#\tAnd\tsave\tregularly \t\t\t\t\t\t\t\tif\tstep\t%\tsave_steps\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tsaver.save(sess,\tcheckpoint_path)\n\nWe\tstart\tby\trestoring\tthe\tmodels\tif\ta\tcheckpoint\tfile\texists,\tor\telse\twe\tjust\tinitialize\tthe\tvariables normally.\tThen\tthe\tmain\tloop\tstarts,\twhere\titeration\tcounts\tthe\ttotal\tnumber\tof\tgame\tsteps\twe\thave gone\tthrough\tsince\tthe\tprogram\tstarted,\tand\tstep\tcounts\tthe\ttotal\tnumber\tof\ttraining\tsteps\tsince\ttraining started\t(if\ta\tcheckpoint\tis\trestored,\tthe\tglobal\tstep\tis\trestored\tas\twell).\tThen\tthe\tcode\tresets\tthe\tgame (and\tskips\tthe\tfirst\tboring\tgame\tsteps,\twhere\tnothing\thappens).\tNext,\tthe\tactor\tevaluates\twhat\tto\tdo,\tand plays\tthe\tgame,\tand\tits\texperience\tis\tmemorized\tin\treplay\tmemory.\tThen,\tat\tregular\tintervals\t(after\ta warmup\tperiod),\tthe\tcritic\tgoes\tthrough\ta\ttraining\tstep.\tIt\tsamples\ta\tbatch\tof\tmemories\tand\tasks\tthe\tactor to\testimate\tthe\tQ-Values\tof\tall\tactions\tfor\tthe\tnext\tstate,\tand\tit\tapplies\tEquation\t16-7\tto\tcompute\tthe\ttarget Q-Value\ty_val.\tThe\tonly\ttricky\tpart\there\tis\tthat\twe\tmust\tmultiply\tthe\tnext\tstate’s\tQ-Values\tby\tthe continues\tvector\tto\tzero\tout\tthe\tQ-Values\tcorresponding\tto\tmemories\twhere\tthe\tgame\twas\tover.\tNext we\trun\ta\ttraining\toperation\tto\timprove\tthe\tcritic’s\tability\tto\tpredict\tQ-Values.\tFinally,\tat\tregular\tintervals we\tcopy\tthe\tcritic\tto\tthe\tactor,\tand\twe\tsave\tthe\tmodel.\n\nTIP\n\nUnfortunately,\ttraining\tis\tvery\tslow:\tif\tyou\tuse\tyour\tlaptop\tfor\ttraining,\tit\twill\ttake\tdays\tbefore\tMs.\tPac-Man\tgets\tany\tgood,\tand\tif you\tlook\tat\tthe\tlearning\tcurve,\tmeasuring\tthe\taverage\trewards\tper\tepisode,\tyou\twill\tnotice\tthat\tit\tis\textremely\tnoisy.\tAt\tsome points\tthere\tmay\tbe\tno\tapparent\tprogress\tfor\ta\tvery\tlong\ttime\tuntil\tsuddenly\tthe\tagent\tlearns\tto\tsurvive\ta\treasonable\tamount\tof time.\tAs\tmentioned\tearlier,\tone\tsolution\tis\tto\tinject\tas\tmuch\tprior\tknowledge\tas\tpossible\tinto\tthe\tmodel\t(e.g.,\tthrough preprocessing,\trewards,\tand\tso\ton),\tand\tyou\tcan\talso\ttry\tto\tbootstrap\tthe\tmodel\tby\tfirst\ttraining\tit\tto\timitate\ta\tbasic\tstrategy.\tIn any\tcase,\tRL\tstill\trequires\tquite\ta\tlot\tof\tpatience\tand\ttweaking,\tbut\tthe\tend\tresult\tis\tvery\texciting.\n\nExercises\n\n1.\t How\twould\tyou\tdefine\tReinforcement\tLearning?\tHow\tis\tit\tdifferent\tfrom\tregular\tsupervised\tor unsupervised\tlearning?\n\n2.\t Can\tyou\tthink\tof\tthree\tpossible\tapplications\tof\tRL\tthat\twere\tnot\tmentioned\tin\tthis\tchapter?\tFor\teach of\tthem,\twhat\tis\tthe\tenvironment?\tWhat\tis\tthe\tagent?\tWhat\tare\tpossible\tactions?\tWhat\tare\tthe rewards?\n\n3.\t What\tis\tthe\tdiscount\trate?\tCan\tthe\toptimal\tpolicy\tchange\tif\tyou\tmodify\tthe\tdiscount\trate?\n\n4.\t How\tdo\tyou\tmeasure\tthe\tperformance\tof\ta\tReinforcement\tLearning\tagent?\n\n5.\t What\tis\tthe\tcredit\tassignment\tproblem?\tWhen\tdoes\tit\toccur?\tHow\tcan\tyou\talleviate\tit?\n\n6.\t What\tis\tthe\tpoint\tof\tusing\ta\treplay\tmemory?\n\n7.\t What\tis\tan\toff-policy\tRL\talgorithm?\n\n8.\t Use\tDeep\tQ-Learning\tto\ttackle\tOpenAI\tgym’s\t“BypedalWalker-v2.”\tThe\tQ-networks\tdo\tnot\tneed\tto be\tvery\tdeep\tfor\tthis\ttask.\n\n9.\t Use\tpolicy\tgradients\tto\ttrain\tan\tagent\tto\tplay\tPong,\tthe\tfamous\tAtari\tgame\t(Pong-v0\tin\tthe\tOpenAI gym).\tBeware:\tan\tindividual\tobservation\tis\tinsufficient\tto\ttell\tthe\tdirection\tand\tspeed\tof\tthe\tball. One\tsolution\tis\tto\tpass\ttwo\tobservations\tat\ta\ttime\tto\tthe\tneural\tnetwork\tpolicy.\tTo\treduce dimensionality\tand\tspeed\tup\ttraining,\tyou\tshould\tdefinitely\tpreprocess\tthese\timages\t(crop,\tresize, and\tconvert\tthem\tto\tblack\tand\twhite),\tand\tpossibly\tmerge\tthem\tinto\ta\tsingle\timage\t(e.g.,\tby overlaying\tthem).\n\n10.\t If\tyou\thave\tabout\t$100\tto\tspare,\tyou\tcan\tpurchase\ta\tRaspberry\tPi\t3\tplus\tsome\tcheap\trobotics components,\tinstall\tTensorFlow\ton\tthe\tPi,\tand\tgo\twild!\tFor\tan\texample,\tcheck\tout\tthis\tfun\tpost\tby Lukas\tBiewald,\tor\ttake\ta\tlook\tat\tGoPiGo\tor\tBrickPi.\tWhy\tnot\ttry\tto\tbuild\ta\treal-life\tcartpole\tby training\tthe\trobot\tusing\tpolicy\tgradients?\tOr\tbuild\ta\trobotic\tspider\tthat\tlearns\tto\twalk;\tgive\tit rewards\tany\ttime\tit\tgets\tcloser\tto\tsome\tobjective\t(you\twill\tneed\tsensors\tto\tmeasure\tthe\tdistance\tto the\tobjective).\tThe\tonly\tlimit\tis\tyour\timagination.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\nThank\tYou! Before\twe\tclose\tthe\tlast\tchapter\tof\tthis\tbook,\tI\twould\tlike\tto\tthank\tyou\tfor\treading\tit\tup\tto\tthe\tlast paragraph.\tI\ttruly\thope\tthat\tyou\thad\tas\tmuch\tpleasure\treading\tthis\tbook\tas\tI\thad\twriting\tit,\tand\tthat\tit\twill be\tuseful\tfor\tyour\tprojects,\tbig\tor\tsmall.\n\nIf\tyou\tfind\terrors,\tplease\tsend\tfeedback.\tMore\tgenerally,\tI\twould\tlove\tto\tknow\twhat\tyou\tthink,\tso\tplease don’t\thesitate\tto\tcontact\tme\tvia\tO’Reilly,\tor\tthrough\tthe\tageron/handson-ml\tGitHub\tproject.\n\nGoing\tforward,\tmy\tbest\tadvice\tto\tyou\tis\tto\tpractice\tand\tpractice:\ttry\tgoing\tthrough\tall\tthe\texercises\tif\tyou have\tnot\tdone\tso\talready,\tplay\twith\tthe\tJupyter\tnotebooks,\tjoin\tKaggle.com\tor\tsome\tother\tML\tcommunity, watch\tML\tcourses,\tread\tpapers,\tattend\tconferences,\tmeet\texperts.\tYou\tmay\talso\twant\tto\tstudy\tsome\ttopics that\twe\tdid\tnot\tcover\tin\tthis\tbook,\tincluding\trecommender\tsystems,\tclustering\talgorithms,\tanomaly detection\talgorithms,\tand\tgenetic\talgorithms.\n\nMy\tgreatest\thope\tis\tthat\tthis\tbook\twill\tinspire\tyou\tto\tbuild\ta\twonderful\tML\tapplication\tthat\twill\tbenefit all\tof\tus!\tWhat\twill\tit\tbe?\n\nAurélien\tGéron,\tNovember\t26th,\t2016\n\n1\n\nFor\tmore\tdetails,\tbe\tsure\tto\tcheck\tout\tRichard\tSutton\tand\tAndrew\tBarto’s\tbook\ton\tRL,\tReinforcement\tLearning:\tAn\tIntroduction\t(MIT Press),\tor\tDavid\tSilver’s\tfree\tonline\tRL\tcourse\tat\tUniversity\tCollege\tLondon.\n\n2\n\n“Playing\tAtari\twith\tDeep\tReinforcement\tLearning,”\tV.\tMnih\tet\tal.\t(2013).\n\n3\n\n“Human-level\tcontrol\tthrough\tdeep\treinforcement\tlearning,”\tV.\tMnih\tet\tal.\t(2015).\n\n4\n\nCheck\tout\tthe\tvideos\tof\tDeepMind’s\tsystem\tlearning\tto\tplay\tSpace\tInvaders,\tBreakout,\tand\tmore\tat\thttps://goo.gl/yTsH6X.\n\n5\n\nImages\t(a),\t(c),\tand\t(d)\tare\treproduced\tfrom\tWikipedia.\t(a)\tand\t(d)\tare\tin\tthe\tpublic\tdomain.\t(c)\twas\tcreated\tby\tuser\tStevertigo\tand released\tunder\tCreative\tCommons\tBY-SA\t2.0.\t(b)\tis\ta\tscreenshot\tfrom\tthe\tMs.\tPac-Man\tgame,\tcopyright\tAtari\t(the\tauthor\tbelieves\tit\tto be\tfair\tuse\tin\tthis\tchapter).\t(e)\twas\treproduced\tfrom\tPixabay,\treleased\tunder\tCreative\tCommons\tCC0.\n\n6\n\nIt\tis\toften\tbetter\tto\tgive\tthe\tpoor\tperformers\ta\tslight\tchance\tof\tsurvival,\tto\tpreserve\tsome\tdiversity\tin\tthe\t“gene\tpool.”\n\n7\n\nIf\tthere\tis\ta\tsingle\tparent,\tthis\tis\tcalled\tasexual\treproduction.\tWith\ttwo\t(or\tmore)\tparents,\tit\tis\tcalled\tsexual\treproduction.\tAn\toffspring’s genome\t(in\tthis\tcase\ta\tset\tof\tpolicy\tparameters)\tis\trandomly\tcomposed\tof\tparts\tof\tits\tparents’\tgenomes.\n\n8\n\nOpenAI\tis\ta\tnonprofit\tartificial\tintelligence\tresearch\tcompany,\tfunded\tin\tpart\tby\tElon\tMusk.\tIts\tstated\tgoal\tis\tto\tpromote\tand\tdevelop friendly\tAIs\tthat\twill\tbenefit\thumanity\t(rather\tthan\texterminate\tit).\n\n9\n\n“Simple\tStatistical\tGradient-Following\tAlgorithms\tfor\tConnectionist\tReinforcement\tLearning,”\tR.\tWilliams\t(1992).\n\n10\n\nWe\talready\tdid\tsomething\tsimilar\tin\tChapter\t11\twhen\twe\tdiscussed\tGradient\tClipping:\twe\tfirst\tcomputed\tthe\tgradients,\tthen\twe\tclipped them,\tand\tfinally\twe\tapplied\tthe\tclipped\tgradients.\n\n11\n\n“A\tMarkovian\tDecision\tProcess,”\tR.\tBellman\t(1957).\n\nAppendix\tA.\tExercise\tSolutions\n\nNOTE\n\nSolutions\tto\tthe\tcoding\texercises\tare\tavailable\tin\tthe\tonline\tJupyter\tnotebooks\tat\thttps://github.com/ageron/handson-ml.\n\nChapter\t1:\tThe\tMachine\tLearning\tLandscape\n\n1.\t Machine\tLearning\tis\tabout\tbuilding\tsystems\tthat\tcan\tlearn\tfrom\tdata.\tLearning\tmeans\tgetting\tbetter\tat some\ttask,\tgiven\tsome\tperformance\tmeasure.\n\n2.\t Machine\tLearning\tis\tgreat\tfor\tcomplex\tproblems\tfor\twhich\twe\thave\tno\talgorithmic\tsolution,\tto replace\tlong\tlists\tof\thand-tuned\trules,\tto\tbuild\tsystems\tthat\tadapt\tto\tfluctuating\tenvironments,\tand finally\tto\thelp\thumans\tlearn\t(e.g.,\tdata\tmining).\n\n3.\t A\tlabeled\ttraining\tset\tis\ta\ttraining\tset\tthat\tcontains\tthe\tdesired\tsolution\t(a.k.a.\ta\tlabel)\tfor\teach instance.\n\n4.\t The\ttwo\tmost\tcommon\tsupervised\ttasks\tare\tregression\tand\tclassification.\n\n5.\t Common\tunsupervised\ttasks\tinclude\tclustering,\tvisualization,\tdimensionality\treduction,\tand association\trule\tlearning.\n\n6.\t Reinforcement\tLearning\tis\tlikely\tto\tperform\tbest\tif\twe\twant\ta\trobot\tto\tlearn\tto\twalk\tin\tvarious unknown\tterrains\tsince\tthis\tis\ttypically\tthe\ttype\tof\tproblem\tthat\tReinforcement\tLearning\ttackles.\tIt might\tbe\tpossible\tto\texpress\tthe\tproblem\tas\ta\tsupervised\tor\tsemisupervised\tlearning\tproblem,\tbut\tit would\tbe\tless\tnatural.\n\n7.\t If\tyou\tdon’t\tknow\thow\tto\tdefine\tthe\tgroups,\tthen\tyou\tcan\tuse\ta\tclustering\talgorithm\t(unsupervised learning)\tto\tsegment\tyour\tcustomers\tinto\tclusters\tof\tsimilar\tcustomers.\tHowever,\tif\tyou\tknow\twhat groups\tyou\twould\tlike\tto\thave,\tthen\tyou\tcan\tfeed\tmany\texamples\tof\teach\tgroup\tto\ta\tclassification algorithm\t(supervised\tlearning),\tand\tit\twill\tclassify\tall\tyour\tcustomers\tinto\tthese\tgroups.\n\n8.\t Spam\tdetection\tis\ta\ttypical\tsupervised\tlearning\tproblem:\tthe\talgorithm\tis\tfed\tmany\temails\talong with\ttheir\tlabel\t(spam\tor\tnot\tspam).\n\n9.\t An\tonline\tlearning\tsystem\tcan\tlearn\tincrementally,\tas\topposed\tto\ta\tbatch\tlearning\tsystem.\tThis makes\tit\tcapable\tof\tadapting\trapidly\tto\tboth\tchanging\tdata\tand\tautonomous\tsystems,\tand\tof\ttraining on\tvery\tlarge\tquantities\tof\tdata.\n\n10.\t Out-of-core\talgorithms\tcan\thandle\tvast\tquantities\tof\tdata\tthat\tcannot\tfit\tin\ta\tcomputer’s\tmain memory.\tAn\tout-of-core\tlearning\talgorithm\tchops\tthe\tdata\tinto\tmini-batches\tand\tuses\tonline\tlearning techniques\tto\tlearn\tfrom\tthese\tmini-batches.\n\n11.\t An\tinstance-based\tlearning\tsystem\tlearns\tthe\ttraining\tdata\tby\theart;\tthen,\twhen\tgiven\ta\tnew\tinstance, it\tuses\ta\tsimilarity\tmeasure\tto\tfind\tthe\tmost\tsimilar\tlearned\tinstances\tand\tuses\tthem\tto\tmake predictions.\n\n12.\t A\tmodel\thas\tone\tor\tmore\tmodel\tparameters\tthat\tdetermine\twhat\tit\twill\tpredict\tgiven\ta\tnew\tinstance (e.g.,\tthe\tslope\tof\ta\tlinear\tmodel).\tA\tlearning\talgorithm\ttries\tto\tfind\toptimal\tvalues\tfor\tthese parameters\tsuch\tthat\tthe\tmodel\tgeneralizes\twell\tto\tnew\tinstances.\tA\thyperparameter\tis\ta\tparameter of\tthe\tlearning\talgorithm\titself,\tnot\tof\tthe\tmodel\t(e.g.,\tthe\tamount\tof\tregularization\tto\tapply).\n\n13.\t Model-based\tlearning\talgorithms\tsearch\tfor\tan\toptimal\tvalue\tfor\tthe\tmodel\tparameters\tsuch\tthat\tthe model\twill\tgeneralize\twell\tto\tnew\tinstances.\tWe\tusually\ttrain\tsuch\tsystems\tby\tminimizing\ta\tcost function\tthat\tmeasures\thow\tbad\tthe\tsystem\tis\tat\tmaking\tpredictions\ton\tthe\ttraining\tdata,\tplus\ta penalty\tfor\tmodel\tcomplexity\tif\tthe\tmodel\tis\tregularized.\tTo\tmake\tpredictions,\twe\tfeed\tthe\tnew instance’s\tfeatures\tinto\tthe\tmodel’s\tprediction\tfunction,\tusing\tthe\tparameter\tvalues\tfound\tby\tthe learning\talgorithm.\n\n14.\t Some\tof\tthe\tmain\tchallenges\tin\tMachine\tLearning\tare\tthe\tlack\tof\tdata,\tpoor\tdata\tquality, nonrepresentative\tdata,\tuninformative\tfeatures,\texcessively\tsimple\tmodels\tthat\tunderfit\tthe\ttraining data,\tand\texcessively\tcomplex\tmodels\tthat\toverfit\tthe\tdata.\n\n15.\t If\ta\tmodel\tperforms\tgreat\ton\tthe\ttraining\tdata\tbut\tgeneralizes\tpoorly\tto\tnew\tinstances,\tthe\tmodel\tis likely\toverfitting\tthe\ttraining\tdata\t(or\twe\tgot\textremely\tlucky\ton\tthe\ttraining\tdata).\tPossible\tsolutions to\toverfitting\tare\tgetting\tmore\tdata,\tsimplifying\tthe\tmodel\t(selecting\ta\tsimpler\talgorithm,\treducing the\tnumber\tof\tparameters\tor\tfeatures\tused,\tor\tregularizing\tthe\tmodel),\tor\treducing\tthe\tnoise\tin\tthe training\tdata.\n\n16.\t A\ttest\tset\tis\tused\tto\testimate\tthe\tgeneralization\terror\tthat\ta\tmodel\twill\tmake\ton\tnew\tinstances,\tbefore the\tmodel\tis\tlaunched\tin\tproduction.\n\n17.\t A\tvalidation\tset\tis\tused\tto\tcompare\tmodels.\tIt\tmakes\tit\tpossible\tto\tselect\tthe\tbest\tmodel\tand\ttune\tthe hyperparameters.\n\n18.\t If\tyou\ttune\thyperparameters\tusing\tthe\ttest\tset,\tyou\trisk\toverfitting\tthe\ttest\tset,\tand\tthe\tgeneralization error\tyou\tmeasure\twill\tbe\toptimistic\t(you\tmay\tlaunch\ta\tmodel\tthat\tperforms\tworse\tthan\tyou\texpect).\n\n19.\t Cross-validation\tis\ta\ttechnique\tthat\tmakes\tit\tpossible\tto\tcompare\tmodels\t(for\tmodel\tselection\tand hyperparameter\ttuning)\twithout\tthe\tneed\tfor\ta\tseparate\tvalidation\tset.\tThis\tsaves\tprecious\ttraining data.\n\nChapter\t2:\tEnd-to-End\tMachine\tLearning\tProject See\tthe\tJupyter\tnotebooks\tavailable\tat\thttps://github.com/ageron/handson-ml.\n\nChapter\t3:\tClassification See\tthe\tJupyter\tnotebooks\tavailable\tat\thttps://github.com/ageron/handson-ml.\n\nChapter\t4:\tTraining\tModels\n\n1.\t If\tyou\thave\ta\ttraining\tset\twith\tmillions\tof\tfeatures\tyou\tcan\tuse\tStochastic\tGradient\tDescent\tor\tMini- batch\tGradient\tDescent,\tand\tperhaps\tBatch\tGradient\tDescent\tif\tthe\ttraining\tset\tfits\tin\tmemory.\tBut you\tcannot\tuse\tthe\tNormal\tEquation\tbecause\tthe\tcomputational\tcomplexity\tgrows\tquickly\t(more\tthan quadratically)\twith\tthe\tnumber\tof\tfeatures.\n\n2.\t If\tthe\tfeatures\tin\tyour\ttraining\tset\thave\tvery\tdifferent\tscales,\tthe\tcost\tfunction\twill\thave\tthe\tshape\tof an\telongated\tbowl,\tso\tthe\tGradient\tDescent\talgorithms\twill\ttake\ta\tlong\ttime\tto\tconverge.\tTo\tsolve this\tyou\tshould\tscale\tthe\tdata\tbefore\ttraining\tthe\tmodel.\tNote\tthat\tthe\tNormal\tEquation\twill\twork just\tfine\twithout\tscaling.\tMoreover,\tregularized\tmodels\tmay\tconverge\tto\ta\tsuboptimal\tsolution\tif\tthe features\tare\tnot\tscaled:\tindeed,\tsince\tregularization\tpenalizes\tlarge\tweights,\tfeatures\twith\tsmaller values\twill\ttend\tto\tbe\tignored\tcompared\tto\tfeatures\twith\tlarger\tvalues.\n\n3.\t Gradient\tDescent\tcannot\tget\tstuck\tin\ta\tlocal\tminimum\twhen\ttraining\ta\tLogistic\tRegression\tmodel because\tthe\tcost\tfunction\tis\tconvex.1\n\n4.\t If\tthe\toptimization\tproblem\tis\tconvex\t(such\tas\tLinear\tRegression\tor\tLogistic\tRegression),\tand assuming\tthe\tlearning\trate\tis\tnot\ttoo\thigh,\tthen\tall\tGradient\tDescent\talgorithms\twill\tapproach\tthe global\toptimum\tand\tend\tup\tproducing\tfairly\tsimilar\tmodels.\tHowever,\tunless\tyou\tgradually\treduce the\tlearning\trate,\tStochastic\tGD\tand\tMini-batch\tGD\twill\tnever\ttruly\tconverge;\tinstead,\tthey\twill keep\tjumping\tback\tand\tforth\taround\tthe\tglobal\toptimum.\tThis\tmeans\tthat\teven\tif\tyou\tlet\tthem\trun\tfor a\tvery\tlong\ttime,\tthese\tGradient\tDescent\talgorithms\twill\tproduce\tslightly\tdifferent\tmodels.\n\n5.\t If\tthe\tvalidation\terror\tconsistently\tgoes\tup\tafter\tevery\tepoch,\tthen\tone\tpossibility\tis\tthat\tthe\tlearning rate\tis\ttoo\thigh\tand\tthe\talgorithm\tis\tdiverging.\tIf\tthe\ttraining\terror\talso\tgoes\tup,\tthen\tthis\tis\tclearly the\tproblem\tand\tyou\tshould\treduce\tthe\tlearning\trate.\tHowever,\tif\tthe\ttraining\terror\tis\tnot\tgoing\tup, then\tyour\tmodel\tis\toverfitting\tthe\ttraining\tset\tand\tyou\tshould\tstop\ttraining.\n\n6.\t Due\tto\ttheir\trandom\tnature,\tneither\tStochastic\tGradient\tDescent\tnor\tMini-batch\tGradient\tDescent\tis guaranteed\tto\tmake\tprogress\tat\tevery\tsingle\ttraining\titeration.\tSo\tif\tyou\timmediately\tstop\ttraining when\tthe\tvalidation\terror\tgoes\tup,\tyou\tmay\tstop\tmuch\ttoo\tearly,\tbefore\tthe\toptimum\tis\treached.\tA better\toption\tis\tto\tsave\tthe\tmodel\tat\tregular\tintervals,\tand\twhen\tit\thas\tnot\timproved\tfor\ta\tlong\ttime (meaning\tit\twill\tprobably\tnever\tbeat\tthe\trecord),\tyou\tcan\trevert\tto\tthe\tbest\tsaved\tmodel.\n\n7.\t Stochastic\tGradient\tDescent\thas\tthe\tfastest\ttraining\titeration\tsince\tit\tconsiders\tonly\tone\ttraining instance\tat\ta\ttime,\tso\tit\tis\tgenerally\tthe\tfirst\tto\treach\tthe\tvicinity\tof\tthe\tglobal\toptimum\t(or\tMini- batch\tGD\twith\ta\tvery\tsmall\tmini-batch\tsize).\tHowever,\tonly\tBatch\tGradient\tDescent\twill\tactually converge,\tgiven\tenough\ttraining\ttime.\tAs\tmentioned,\tStochastic\tGD\tand\tMini-batch\tGD\twill\tbounce around\tthe\toptimum,\tunless\tyou\tgradually\treduce\tthe\tlearning\trate. 8.\t If\tthe\tvalidation\terror\tis\tmuch\thigher\tthan\tthe\ttraining\terror,\tthis\tis\tlikely\tbecause\tyour\tmodel\tis overfitting\tthe\ttraining\tset.\tOne\tway\tto\ttry\tto\tfix\tthis\tis\tto\treduce\tthe\tpolynomial\tdegree:\ta\tmodel with\tfewer\tdegrees\tof\tfreedom\tis\tless\tlikely\tto\toverfit.\tAnother\tthing\tyou\tcan\ttry\tis\tto\tregularize\tthe model\t—\tfor\texample,\tby\tadding\tan\tℓ2\tpenalty\t(Ridge)\tor\tan\tℓ1\tpenalty\t(Lasso)\tto\tthe\tcost\tfunction. This\twill\talso\treduce\tthe\tdegrees\tof\tfreedom\tof\tthe\tmodel.\tLastly,\tyou\tcan\ttry\tto\tincrease\tthe\tsize\tof\n\nthe\ttraining\tset.\n\n9.\t If\tboth\tthe\ttraining\terror\tand\tthe\tvalidation\terror\tare\talmost\tequal\tand\tfairly\thigh,\tthe\tmodel\tis\tlikely\n\nunderfitting\tthe\ttraining\tset,\twhich\tmeans\tit\thas\ta\thigh\tbias.\tYou\tshould\ttry\treducing\tthe regularization\thyperparameter\tα.\n\n10.\t Let’s\tsee:\n\nA\tmodel\twith\tsome\tregularization\ttypically\tperforms\tbetter\tthan\ta\tmodel\twithout\tany regularization,\tso\tyou\tshould\tgenerally\tprefer\tRidge\tRegression\tover\tplain\tLinear\tRegression.2\n\nLasso\tRegression\tuses\tan\tℓ1\tpenalty,\twhich\ttends\tto\tpush\tthe\tweights\tdown\tto\texactly\tzero. This\tleads\tto\tsparse\tmodels,\twhere\tall\tweights\tare\tzero\texcept\tfor\tthe\tmost\timportant\tweights. This\tis\ta\tway\tto\tperform\tfeature\tselection\tautomatically,\twhich\tis\tgood\tif\tyou\tsuspect\tthat\tonly a\tfew\tfeatures\tactually\tmatter.\tWhen\tyou\tare\tnot\tsure,\tyou\tshould\tprefer\tRidge\tRegression.\n\nElastic\tNet\tis\tgenerally\tpreferred\tover\tLasso\tsince\tLasso\tmay\tbehave\terratically\tin\tsome\tcases (when\tseveral\tfeatures\tare\tstrongly\tcorrelated\tor\twhen\tthere\tare\tmore\tfeatures\tthan\ttraining instances).\tHowever,\tit\tdoes\tadd\tan\textra\thyperparameter\tto\ttune.\tIf\tyou\tjust\twant\tLasso without\tthe\terratic\tbehavior,\tyou\tcan\tjust\tuse\tElastic\tNet\twith\tan\tl1_ratio\tclose\tto\t1.\n\n11.\t If\tyou\twant\tto\tclassify\tpictures\tas\toutdoor/indoor\tand\tdaytime/nighttime,\tsince\tthese\tare\tnot exclusive\tclasses\t(i.e.,\tall\tfour\tcombinations\tare\tpossible)\tyou\tshould\ttrain\ttwo\tLogistic\tRegression classifiers.\n\n12.\t See\tthe\tJupyter\tnotebooks\tavailable\tat\thttps://github.com/ageron/handson-ml.\n\nChapter\t5:\tSupport\tVector\tMachines\n\n1.\t The\tfundamental\tidea\tbehind\tSupport\tVector\tMachines\tis\tto\tfit\tthe\twidest\tpossible\t“street”\tbetween the\tclasses.\tIn\tother\twords,\tthe\tgoal\tis\tto\thave\tthe\tlargest\tpossible\tmargin\tbetween\tthe\tdecision boundary\tthat\tseparates\tthe\ttwo\tclasses\tand\tthe\ttraining\tinstances.\tWhen\tperforming\tsoft\tmargin classification,\tthe\tSVM\tsearches\tfor\ta\tcompromise\tbetween\tperfectly\tseparating\tthe\ttwo\tclasses\tand having\tthe\twidest\tpossible\tstreet\t(i.e.,\ta\tfew\tinstances\tmay\tend\tup\ton\tthe\tstreet).\tAnother\tkey\tidea\tis to\tuse\tkernels\twhen\ttraining\ton\tnonlinear\tdatasets.\n\n2.\t After\ttraining\tan\tSVM,\ta\tsupport\tvector\tis\tany\tinstance\tlocated\ton\tthe\t“street”\t(see\tthe\tprevious answer),\tincluding\tits\tborder.\tThe\tdecision\tboundary\tis\tentirely\tdetermined\tby\tthe\tsupport\tvectors. Any\tinstance\tthat\tis\tnot\ta\tsupport\tvector\t(i.e.,\toff\tthe\tstreet)\thas\tno\tinfluence\twhatsoever;\tyou\tcould remove\tthem,\tadd\tmore\tinstances,\tor\tmove\tthem\taround,\tand\tas\tlong\tas\tthey\tstay\toff\tthe\tstreet\tthey won’t\taffect\tthe\tdecision\tboundary.\tComputing\tthe\tpredictions\tonly\tinvolves\tthe\tsupport\tvectors,\tnot the\twhole\ttraining\tset.\n\n3.\t SVMs\ttry\tto\tfit\tthe\tlargest\tpossible\t“street”\tbetween\tthe\tclasses\t(see\tthe\tfirst\tanswer),\tso\tif\tthe training\tset\tis\tnot\tscaled,\tthe\tSVM\twill\ttend\tto\tneglect\tsmall\tfeatures\t(see\tFigure\t5-2).\n\n4.\t An\tSVM\tclassifier\tcan\toutput\tthe\tdistance\tbetween\tthe\ttest\tinstance\tand\tthe\tdecision\tboundary,\tand you\tcan\tuse\tthis\tas\ta\tconfidence\tscore.\tHowever,\tthis\tscore\tcannot\tbe\tdirectly\tconverted\tinto\tan estimation\tof\tthe\tclass\tprobability.\tIf\tyou\tset\tprobability=True\twhen\tcreating\tan\tSVM\tin\tScikit- Learn,\tthen\tafter\ttraining\tit\twill\tcalibrate\tthe\tprobabilities\tusing\tLogistic\tRegression\ton\tthe\tSVM’s scores\t(trained\tby\tan\tadditional\tfive-fold\tcross-validation\ton\tthe\ttraining\tdata).\tThis\twill\tadd\tthe predict_proba()\tand\tpredict_log_proba()\tmethods\tto\tthe\tSVM.\n\n5.\t This\tquestion\tapplies\tonly\tto\tlinear\tSVMs\tsince\tkernelized\tcan\tonly\tuse\tthe\tdual\tform.\tThe computational\tcomplexity\tof\tthe\tprimal\tform\tof\tthe\tSVM\tproblem\tis\tproportional\tto\tthe\tnumber\tof training\tinstances\tm,\twhile\tthe\tcomputational\tcomplexity\tof\tthe\tdual\tform\tis\tproportional\tto\ta number\tbetween\tm2\tand\tm3.\tSo\tif\tthere\tare\tmillions\tof\tinstances,\tyou\tshould\tdefinitely\tuse\tthe\tprimal form,\tbecause\tthe\tdual\tform\twill\tbe\tmuch\ttoo\tslow.\n\n6.\t If\tan\tSVM\tclassifier\ttrained\twith\tan\tRBF\tkernel\tunderfits\tthe\ttraining\tset,\tthere\tmight\tbe\ttoo\tmuch regularization.\tTo\tdecrease\tit,\tyou\tneed\tto\tincrease\tgamma\tor\tC\t(or\tboth).\n\n7.\t Let’s\tcall\tthe\tQP\tparameters\tfor\tthe\thard-margin\tproblem\tH′,\tf′,\tA′\tand\tb′\t(see\t“Quadratic Programming”).\tThe\tQP\tparameters\tfor\tthe\tsoft-margin\tproblem\thave\tm\tadditional\tparameters\t(np\t= n\t+\t1\t+\tm)\tand\tm\tadditional\tconstraints\t(nc\t=\t2m).\tThey\tcan\tbe\tdefined\tlike\tso:\n\nH\tis\tequal\tto\tH′,\tplus\tm\tcolumns\tof\t0s\ton\tthe\tright\tand\tm\trows\tof\t0s\tat\tthe\tbottom:\n\nf\tis\tequal\tto\tf′\twith\tm\tadditional\telements,\tall\tequal\tto\tthe\tvalue\tof\tthe\thyperparameter\tC.\n\nb\tis\tequal\tto\tb′\twith\tm\tadditional\telements,\tall\tequal\tto\t0.\n\nA\tis\tequal\tto\tA′,\twith\tan\textra\tm\t×\tm\tidentity\tmatrix\tIm\tappended\tto\tthe\tright,\t–\tIm\tjust\tbelow\tit,\n\nand\tthe\trest\tfilled\twith\tzeros:\n\nFor\tthe\tsolutions\tto\texercises\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.\n\nChapter\t6:\tDecision\tTrees\n\n1.\t The\tdepth\tof\ta\twell-balanced\tbinary\ttree\tcontaining\tm\tleaves\tis\tequal\tto\tlog2(m)3,\trounded\tup.\tA binary\tDecision\tTree\t(one\tthat\tmakes\tonly\tbinary\tdecisions,\tas\tis\tthe\tcase\tof\tall\ttrees\tin\tScikit- Learn)\twill\tend\tup\tmore\tor\tless\twell\tbalanced\tat\tthe\tend\tof\ttraining,\twith\tone\tleaf\tper\ttraining instance\tif\tit\tis\ttrained\twithout\trestrictions.\tThus,\tif\tthe\ttraining\tset\tcontains\tone\tmillion\tinstances, the\tDecision\tTree\twill\thave\ta\tdepth\tof\tlog2(106)\t≈\t20\t(actually\ta\tbit\tmore\tsince\tthe\ttree\twill generally\tnot\tbe\tperfectly\twell\tbalanced).\n\n2.\t A\tnode’s\tGini\timpurity\tis\tgenerally\tlower\tthan\tits\tparent’s.\tThis\tis\tdue\tto\tthe\tCART\ttraining algorithm’s\tcost\tfunction,\twhich\tsplits\teach\tnode\tin\ta\tway\tthat\tminimizes\tthe\tweighted\tsum\tof\tits children’s\tGini\timpurities.\tHowever,\tit\tis\tpossible\tfor\ta\tnode\tto\thave\ta\thigher\tGini\timpurity\tthan\tits parent,\tas\tlong\tas\tthis\tincrease\tis\tmore\tthan\tcompensated\tfor\tby\ta\tdecrease\tof\tthe\tother\tchild’s impurity.\tFor\texample,\tconsider\ta\tnode\tcontaining\tfour\tinstances\tof\tclass\tA\tand\t1\tof\tclass\tB.\tIts\tGini\n\nimpurity\tis\t \t=\t0.32.\tNow\tsuppose\tthe\tdataset\tis\tone-dimensional\tand\tthe\tinstances\tare lined\tup\tin\tthe\tfollowing\torder:\tA,\tB,\tA,\tA,\tA.\tYou\tcan\tverify\tthat\tthe\talgorithm\twill\tsplit\tthis\tnode after\tthe\tsecond\tinstance,\tproducing\tone\tchild\tnode\twith\tinstances\tA,\tB,\tand\tthe\tother\tchild\tnode\n\nwith\tinstances\tA,\tA,\tA.\tThe\tfirst\tchild\tnode’s\tGini\timpurity\tis\t than\tits\tparent.\tThis\tis\tcompensated\tfor\tby\tthe\tfact\tthat\tthe\tother\tnode\tis\tpure,\tso\tthe\toverall\n\n=\t0.5,\twhich\tis\thigher\n\nweighted\tGini\timpurity\tis\n\n0.5\t+\n\n=\t0.2\t,\twhich\tis\tlower\tthan\tthe\tparent’s\tGini\timpurity.\n\n3.\t If\ta\tDecision\tTree\tis\toverfitting\tthe\ttraining\tset,\tit\tmay\tbe\ta\tgood\tidea\tto\tdecrease\tmax_depth,\tsince this\twill\tconstrain\tthe\tmodel,\tregularizing\tit.\n\n4.\t Decision\tTrees\tdon’t\tcare\twhether\tor\tnot\tthe\ttraining\tdata\tis\tscaled\tor\tcentered;\tthat’s\tone\tof\tthe\tnice things\tabout\tthem.\tSo\tif\ta\tDecision\tTree\tunderfits\tthe\ttraining\tset,\tscaling\tthe\tinput\tfeatures\twill\tjust be\ta\twaste\tof\ttime.\n\n5.\t The\tcomputational\tcomplexity\tof\ttraining\ta\tDecision\tTree\tis\tO(n\t×\tm\tlog(m)).\tSo\tif\tyou\tmultiply\tthe training\tset\tsize\tby\t10,\tthe\ttraining\ttime\twill\tbe\tmultiplied\tby\tK\t=\t(n\t×\t10m\t×\tlog(10m))\t/\t(n\t×\tm\t× log(m))\t=\t10\t×\tlog(10m)\t/\tlog(m).\tIf\tm\t=\t106,\tthen\tK\t≈\t11.7,\tso\tyou\tcan\texpect\tthe\ttraining\ttime\tto\tbe roughly\t11.7\thours.\n\n6.\t Presorting\tthe\ttraining\tset\tspeeds\tup\ttraining\tonly\tif\tthe\tdataset\tis\tsmaller\tthan\ta\tfew\tthousand instances.\tIf\tit\tcontains\t100,000\tinstances,\tsetting\tpresort=True\twill\tconsiderably\tslow\tdown training.\n\nFor\tthe\tsolutions\tto\texercises\t7\tand\t8,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.\n\nChapter\t7:\tEnsemble\tLearning\tand\tRandom\tForests\n\n1.\t If\tyou\thave\ttrained\tfive\tdifferent\tmodels\tand\tthey\tall\tachieve\t95%\tprecision,\tyou\tcan\ttry\tcombining them\tinto\ta\tvoting\tensemble,\twhich\twill\toften\tgive\tyou\teven\tbetter\tresults.\tIt\tworks\tbetter\tif\tthe models\tare\tvery\tdifferent\t(e.g.,\tan\tSVM\tclassifier,\ta\tDecision\tTree\tclassifier,\ta\tLogistic\tRegression classifier,\tand\tso\ton).\tIt\tis\teven\tbetter\tif\tthey\tare\ttrained\ton\tdifferent\ttraining\tinstances\t(that’s\tthe whole\tpoint\tof\tbagging\tand\tpasting\tensembles),\tbut\tif\tnot\tit\twill\tstill\twork\tas\tlong\tas\tthe\tmodels\tare very\tdifferent.\n\n2.\t A\thard\tvoting\tclassifier\tjust\tcounts\tthe\tvotes\tof\teach\tclassifier\tin\tthe\tensemble\tand\tpicks\tthe\tclass that\tgets\tthe\tmost\tvotes.\tA\tsoft\tvoting\tclassifier\tcomputes\tthe\taverage\testimated\tclass\tprobability\tfor each\tclass\tand\tpicks\tthe\tclass\twith\tthe\thighest\tprobability.\tThis\tgives\thigh-confidence\tvotes\tmore weight\tand\toften\tperforms\tbetter,\tbut\tit\tworks\tonly\tif\tevery\tclassifier\tis\table\tto\testimate\tclass probabilities\t(e.g.,\tfor\tthe\tSVM\tclassifiers\tin\tScikit-Learn\tyou\tmust\tset\tprobability=True).\n\n3.\t It\tis\tquite\tpossible\tto\tspeed\tup\ttraining\tof\ta\tbagging\tensemble\tby\tdistributing\tit\tacross\tmultiple servers,\tsince\teach\tpredictor\tin\tthe\tensemble\tis\tindependent\tof\tthe\tothers.\tThe\tsame\tgoes\tfor\tpasting ensembles\tand\tRandom\tForests,\tfor\tthe\tsame\treason.\tHowever,\teach\tpredictor\tin\ta\tboosting ensemble\tis\tbuilt\tbased\ton\tthe\tprevious\tpredictor,\tso\ttraining\tis\tnecessarily\tsequential,\tand\tyou\twill not\tgain\tanything\tby\tdistributing\ttraining\tacross\tmultiple\tservers.\tRegarding\tstacking\tensembles,\tall the\tpredictors\tin\ta\tgiven\tlayer\tare\tindependent\tof\teach\tother,\tso\tthey\tcan\tbe\ttrained\tin\tparallel\ton multiple\tservers.\tHowever,\tthe\tpredictors\tin\tone\tlayer\tcan\tonly\tbe\ttrained\tafter\tthe\tpredictors\tin\tthe previous\tlayer\thave\tall\tbeen\ttrained.\n\n4.\t With\tout-of-bag\tevaluation,\teach\tpredictor\tin\ta\tbagging\tensemble\tis\tevaluated\tusing\tinstances\tthat\tit was\tnot\ttrained\ton\t(they\twere\theld\tout).\tThis\tmakes\tit\tpossible\tto\thave\ta\tfairly\tunbiased\tevaluation of\tthe\tensemble\twithout\tthe\tneed\tfor\tan\tadditional\tvalidation\tset.\tThus,\tyou\thave\tmore\tinstances available\tfor\ttraining,\tand\tyour\tensemble\tcan\tperform\tslightly\tbetter.\n\n5.\t When\tyou\tare\tgrowing\ta\ttree\tin\ta\tRandom\tForest,\tonly\ta\trandom\tsubset\tof\tthe\tfeatures\tis\tconsidered for\tsplitting\tat\teach\tnode.\tThis\tis\ttrue\tas\twell\tfor\tExtra-Trees,\tbut\tthey\tgo\tone\tstep\tfurther:\trather than\tsearching\tfor\tthe\tbest\tpossible\tthresholds,\tlike\tregular\tDecision\tTrees\tdo,\tthey\tuse\trandom thresholds\tfor\teach\tfeature.\tThis\textra\trandomness\tacts\tlike\ta\tform\tof\tregularization:\tif\ta\tRandom Forest\toverfits\tthe\ttraining\tdata,\tExtra-Trees\tmight\tperform\tbetter.\tMoreover,\tsince\tExtra-Trees don’t\tsearch\tfor\tthe\tbest\tpossible\tthresholds,\tthey\tare\tmuch\tfaster\tto\ttrain\tthan\tRandom\tForests. However,\tthey\tare\tneither\tfaster\tnor\tslower\tthan\tRandom\tForests\twhen\tmaking\tpredictions.\n\n6.\t If\tyour\tAdaBoost\tensemble\tunderfits\tthe\ttraining\tdata,\tyou\tcan\ttry\tincreasing\tthe\tnumber\tof estimators\tor\treducing\tthe\tregularization\thyperparameters\tof\tthe\tbase\testimator.\tYou\tmay\talso\ttry slightly\tincreasing\tthe\tlearning\trate.\n\n7.\t If\tyour\tGradient\tBoosting\tensemble\toverfits\tthe\ttraining\tset,\tyou\tshould\ttry\tdecreasing\tthe\tlearning rate.\tYou\tcould\talso\tuse\tearly\tstopping\tto\tfind\tthe\tright\tnumber\tof\tpredictors\t(you\tprobably\thave\ttoo many).\n\nFor\tthe\tsolutions\tto\texercises\t8\tand\t9,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat\n\nhttps://github.com/ageron/handson-ml.\n\nChapter\t8:\tDimensionality\tReduction\n\n1.\t Motivations\tand\tdrawbacks:\n\nThe\tmain\tmotivations\tfor\tdimensionality\treduction\tare:\n\nTo\tspeed\tup\ta\tsubsequent\ttraining\talgorithm\t(in\tsome\tcases\tit\tmay\teven\tremove\tnoise\tand redundant\tfeatures,\tmaking\tthe\ttraining\talgorithm\tperform\tbetter).\n\nTo\tvisualize\tthe\tdata\tand\tgain\tinsights\ton\tthe\tmost\timportant\tfeatures.\n\nSimply\tto\tsave\tspace\t(compression).\n\nThe\tmain\tdrawbacks\tare:\n\nSome\tinformation\tis\tlost,\tpossibly\tdegrading\tthe\tperformance\tof\tsubsequent\ttraining algorithms.\n\nIt\tcan\tbe\tcomputationally\tintensive.\n\nIt\tadds\tsome\tcomplexity\tto\tyour\tMachine\tLearning\tpipelines.\n\nTransformed\tfeatures\tare\toften\thard\tto\tinterpret.\n\n2.\t The\tcurse\tof\tdimensionality\trefers\tto\tthe\tfact\tthat\tmany\tproblems\tthat\tdo\tnot\texist\tin\tlow- dimensional\tspace\tarise\tin\thigh-dimensional\tspace.\tIn\tMachine\tLearning,\tone\tcommon\tmanifestation is\tthe\tfact\tthat\trandomly\tsampled\thigh-dimensional\tvectors\tare\tgenerally\tvery\tsparse,\tincreasing\tthe risk\tof\toverfitting\tand\tmaking\tit\tvery\tdifficult\tto\tidentify\tpatterns\tin\tthe\tdata\twithout\thaving\tplenty\tof training\tdata.\n\n3.\t Once\ta\tdataset’s\tdimensionality\thas\tbeen\treduced\tusing\tone\tof\tthe\talgorithms\twe\tdiscussed,\tit\tis almost\talways\timpossible\tto\tperfectly\treverse\tthe\toperation,\tbecause\tsome\tinformation\tgets\tlost during\tdimensionality\treduction.\tMoreover,\twhile\tsome\talgorithms\t(such\tas\tPCA)\thave\ta\tsimple reverse\ttransformation\tprocedure\tthat\tcan\treconstruct\ta\tdataset\trelatively\tsimilar\tto\tthe\toriginal, other\talgorithms\t(such\tas\tT-SNE)\tdo\tnot.\n\n4.\t PCA\tcan\tbe\tused\tto\tsignificantly\treduce\tthe\tdimensionality\tof\tmost\tdatasets,\teven\tif\tthey\tare\thighly nonlinear,\tbecause\tit\tcan\tat\tleast\tget\trid\tof\tuseless\tdimensions.\tHowever,\tif\tthere\tare\tno\tuseless dimensions\t—\tfor\texample,\tthe\tSwiss\troll\t—\tthen\treducing\tdimensionality\twith\tPCA\twill\tlose\ttoo much\tinformation.\tYou\twant\tto\tunroll\tthe\tSwiss\troll,\tnot\tsquash\tit.\n\n5.\t That’s\ta\ttrick\tquestion:\tit\tdepends\ton\tthe\tdataset.\tLet’s\tlook\tat\ttwo\textreme\texamples.\tFirst,\tsuppose the\tdataset\tis\tcomposed\tof\tpoints\tthat\tare\talmost\tperfectly\taligned.\tIn\tthis\tcase,\tPCA\tcan\treduce\tthe dataset\tdown\tto\tjust\tone\tdimension\twhile\tstill\tpreserving\t95%\tof\tthe\tvariance.\tNow\timagine\tthat\tthe dataset\tis\tcomposed\tof\tperfectly\trandom\tpoints,\tscattered\tall\taround\tthe\t1,000\tdimensions.\tIn\tthis case\troughly\t950\tdimensions\tare\trequired\tto\tpreserve\t95%\tof\tthe\tvariance.\tSo\tthe\tanswer\tis,\tit depends\ton\tthe\tdataset,\tand\tit\tcould\tbe\tany\tnumber\tbetween\t1\tand\t950.\tPlotting\tthe\texplained variance\tas\ta\tfunction\tof\tthe\tnumber\tof\tdimensions\tis\tone\tway\tto\tget\ta\trough\tidea\tof\tthe\tdataset’s intrinsic\tdimensionality.\n\n6.\t Regular\tPCA\tis\tthe\tdefault,\tbut\tit\tworks\tonly\tif\tthe\tdataset\tfits\tin\tmemory.\tIncremental\tPCA\tis\tuseful for\tlarge\tdatasets\tthat\tdon’t\tfit\tin\tmemory,\tbut\tit\tis\tslower\tthan\tregular\tPCA,\tso\tif\tthe\tdataset\tfits\tin memory\tyou\tshould\tprefer\tregular\tPCA.\tIncremental\tPCA\tis\talso\tuseful\tfor\tonline\ttasks,\twhen\tyou need\tto\tapply\tPCA\ton\tthe\tfly,\tevery\ttime\ta\tnew\tinstance\tarrives.\tRandomized\tPCA\tis\tuseful\twhen you\twant\tto\tconsiderably\treduce\tdimensionality\tand\tthe\tdataset\tfits\tin\tmemory;\tin\tthis\tcase,\tit\tis much\tfaster\tthan\tregular\tPCA.\tFinally,\tKernel\tPCA\tis\tuseful\tfor\tnonlinear\tdatasets.\n\n7.\t Intuitively,\ta\tdimensionality\treduction\talgorithm\tperforms\twell\tif\tit\teliminates\ta\tlot\tof\tdimensions from\tthe\tdataset\twithout\tlosing\ttoo\tmuch\tinformation.\tOne\tway\tto\tmeasure\tthis\tis\tto\tapply\tthe reverse\ttransformation\tand\tmeasure\tthe\treconstruction\terror.\tHowever,\tnot\tall\tdimensionality reduction\talgorithms\tprovide\ta\treverse\ttransformation.\tAlternatively,\tif\tyou\tare\tusing\tdimensionality reduction\tas\ta\tpreprocessing\tstep\tbefore\tanother\tMachine\tLearning\talgorithm\t(e.g.,\ta\tRandom\tForest classifier),\tthen\tyou\tcan\tsimply\tmeasure\tthe\tperformance\tof\tthat\tsecond\talgorithm;\tif\tdimensionality reduction\tdid\tnot\tlose\ttoo\tmuch\tinformation,\tthen\tthe\talgorithm\tshould\tperform\tjust\tas\twell\tas\twhen using\tthe\toriginal\tdataset.\n\n8.\t It\tcan\tabsolutely\tmake\tsense\tto\tchain\ttwo\tdifferent\tdimensionality\treduction\talgorithms.\tA\tcommon example\tis\tusing\tPCA\tto\tquickly\tget\trid\tof\ta\tlarge\tnumber\tof\tuseless\tdimensions,\tthen\tapplying another\tmuch\tslower\tdimensionality\treduction\talgorithm,\tsuch\tas\tLLE.\tThis\ttwo-step\tapproach\twill likely\tyield\tthe\tsame\tperformance\tas\tusing\tLLE\tonly,\tbut\tin\ta\tfraction\tof\tthe\ttime.\n\nFor\tthe\tsolutions\tto\texercises\t9\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.\n\nChapter\t9:\tUp\tand\tRunning\twith\tTensorFlow\n\n1.\t Main\tbenefits\tand\tdrawbacks\tof\tcreating\ta\tcomputation\tgraph\trather\tthan\tdirectly\texecuting\tthe computations:\n\nMain\tbenefits:\n\nTensorFlow\tcan\tautomatically\tcompute\tthe\tgradients\tfor\tyou\t(using\treverse-mode autodiff).\n\nTensorFlow\tcan\ttake\tcare\tof\trunning\tthe\toperations\tin\tparallel\tin\tdifferent\tthreads.\n\nIt\tmakes\tit\teasier\tto\trun\tthe\tsame\tmodel\tacross\tdifferent\tdevices.\n\nIt\tsimplifies\tintrospection\t—\tfor\texample,\tto\tview\tthe\tmodel\tin\tTensorBoard.\n\nMain\tdrawbacks:\n\nIt\tmakes\tthe\tlearning\tcurve\tsteeper.\n\nIt\tmakes\tstep-by-step\tdebugging\tharder.\n\n2.\t Yes,\tthe\tstatement\ta_val\t=\ta.eval(session=sess)\tis\tindeed\tequivalent\tto\ta_val\t=\tsess.run(a).\n\n3.\t No,\tthe\tstatement\ta_val,\tb_val\t=\ta.eval(session=sess),\tb.eval(session=sess)\tis\tnot equivalent\tto\ta_val,\tb_val\t=\tsess.run([a,\tb]).\tIndeed,\tthe\tfirst\tstatement\truns\tthe\tgraph\ttwice (once\tto\tcompute\ta,\tonce\tto\tcompute\tb),\twhile\tthe\tsecond\tstatement\truns\tthe\tgraph\tonly\tonce.\tIf\tany of\tthese\toperations\t(or\tthe\tops\tthey\tdepend\ton)\thave\tside\teffects\t(e.g.,\ta\tvariable\tis\tmodified,\tan item\tis\tinserted\tin\ta\tqueue,\tor\ta\treader\treads\ta\tfile),\tthen\tthe\teffects\twill\tbe\tdifferent.\tIf\tthey\tdon’t have\tside\teffects,\tboth\tstatements\twill\treturn\tthe\tsame\tresult,\tbut\tthe\tsecond\tstatement\twill\tbe\tfaster than\tthe\tfirst.\n\n4.\t No,\tyou\tcannot\trun\ttwo\tgraphs\tin\tthe\tsame\tsession.\tYou\twould\thave\tto\tmerge\tthe\tgraphs\tinto\ta\tsingle graph\tfirst.\n\n5.\t In\tlocal\tTensorFlow,\tsessions\tmanage\tvariable\tvalues,\tso\tif\tyou\tcreate\ta\tgraph\tg\tcontaining\ta variable\tw,\tthen\tstart\ttwo\tthreads\tand\topen\ta\tlocal\tsession\tin\teach\tthread,\tboth\tusing\tthe\tsame\tgraph g,\tthen\teach\tsession\twill\thave\tits\town\tcopy\tof\tthe\tvariable\tw.\tHowever,\tin\tdistributed\tTensorFlow, variable\tvalues\tare\tstored\tin\tcontainers\tmanaged\tby\tthe\tcluster,\tso\tif\tboth\tsessions\tconnect\tto\tthe same\tcluster\tand\tuse\tthe\tsame\tcontainer,\tthen\tthey\twill\tshare\tthe\tsame\tvariable\tvalue\tfor\tw.\n\n6.\t A\tvariable\tis\tinitialized\twhen\tyou\tcall\tits\tinitializer,\tand\tit\tis\tdestroyed\twhen\tthe\tsession\tends.\tIn distributed\tTensorFlow,\tvariables\tlive\tin\tcontainers\ton\tthe\tcluster,\tso\tclosing\ta\tsession\twill\tnot destroy\tthe\tvariable.\tTo\tdestroy\ta\tvariable,\tyou\tneed\tto\tclear\tits\tcontainer.\n\n7.\t Variables\tand\tplaceholders\tare\textremely\tdifferent,\tbut\tbeginners\toften\tconfuse\tthem:\n\nA\tvariable\tis\tan\toperation\tthat\tholds\ta\tvalue.\tIf\tyou\trun\tthe\tvariable,\tit\treturns\tthat\tvalue. Before\tyou\tcan\trun\tit,\tyou\tneed\tto\tinitialize\tit.\tYou\tcan\tchange\tthe\tvariable’s\tvalue\t(for\n\nexample,\tby\tusing\tan\tassignment\toperation).\tIt\tis\tstateful:\tthe\tvariable\tkeeps\tthe\tsame\tvalue upon\tsuccessive\truns\tof\tthe\tgraph.\tIt\tis\ttypically\tused\tto\thold\tmodel\tparameters\tbut\talso\tfor other\tpurposes\t(e.g.,\tto\tcount\tthe\tglobal\ttraining\tstep).\n\nPlaceholders\ttechnically\tdon’t\tdo\tmuch:\tthey\tjust\thold\tinformation\tabout\tthe\ttype\tand\tshape\tof the\ttensor\tthey\trepresent,\tbut\tthey\thave\tno\tvalue.\tIn\tfact,\tif\tyou\ttry\tto\tevaluate\tan\toperation\tthat depends\ton\ta\tplaceholder,\tyou\tmust\tfeed\tTensorFlow\tthe\tvalue\tof\tthe\tplaceholder\t(using\tthe feed_dict\targument)\tor\telse\tyou\twill\tget\tan\texception.\tPlaceholders\tare\ttypically\tused\tto\tfeed training\tor\ttest\tdata\tto\tTensorFlow\tduring\tthe\texecution\tphase.\tThey\tare\talso\tuseful\tto\tpass\ta value\tto\tan\tassignment\tnode,\tto\tchange\tthe\tvalue\tof\ta\tvariable\t(e.g.,\tmodel\tweights).\n\n8.\t If\tyou\trun\tthe\tgraph\tto\tevaluate\tan\toperation\tthat\tdepends\ton\ta\tplaceholder\tbut\tyou\tdon’t\tfeed\tits value,\tyou\tget\tan\texception.\tIf\tthe\toperation\tdoes\tnot\tdepend\ton\tthe\tplaceholder,\tthen\tno\texception\tis raised.\n\n9.\t When\tyou\trun\ta\tgraph,\tyou\tcan\tfeed\tthe\toutput\tvalue\tof\tany\toperation,\tnot\tjust\tthe\tvalue\tof placeholders.\tIn\tpractice,\thowever,\tthis\tis\trather\trare\t(it\tcan\tbe\tuseful,\tfor\texample,\twhen\tyou\tare caching\tthe\toutput\tof\tfrozen\tlayers;\tsee\tChapter\t11).\n\n10.\t You\tcan\tspecify\ta\tvariable’s\tinitial\tvalue\twhen\tconstructing\tthe\tgraph,\tand\tit\twill\tbe\tinitialized\tlater when\tyou\trun\tthe\tvariable’s\tinitializer\tduring\tthe\texecution\tphase.\tIf\tyou\twant\tto\tchange\tthat variable’s\tvalue\tto\tanything\tyou\twant\tduring\tthe\texecution\tphase,\tthen\tthe\tsimplest\toption\tis\tto\tcreate an\tassignment\tnode\t(during\tthe\tgraph\tconstruction\tphase)\tusing\tthe\ttf.assign()\tfunction,\tpassing the\tvariable\tand\ta\tplaceholder\tas\tparameters.\tDuring\tthe\texecution\tphase,\tyou\tcan\trun\tthe\tassignment operation\tand\tfeed\tthe\tvariable’s\tnew\tvalue\tusing\tthe\tplaceholder.\n\nimport\ttensorflow\tas\ttf\n\nx\t=\ttf.Variable(tf.random_uniform(shape=(),\tminval=0.0,\tmaxval=1.0)) x_new_val\t=\ttf.placeholder(shape=(),\tdtype=tf.float32) x_assign\t=\ttf.assign(x,\tx_new_val)\n\nwith\ttf.Session(): \t\t\t\tx.initializer.run()\t#\trandom\tnumber\tis\tsampled\t*now* \t\t\t\tprint(x.eval())\t#\t0.646157\t(some\trandom\tnumber) \t\t\t\tx_assign.eval(feed_dict={x_new_val:\t5.0}) \t\t\t\tprint(x.eval())\t#\t5.0\n\n11.\t Reverse-mode\tautodiff\t(implemented\tby\tTensorFlow)\tneeds\tto\ttraverse\tthe\tgraph\tonly\ttwice\tin order\tto\tcompute\tthe\tgradients\tof\tthe\tcost\tfunction\twith\tregards\tto\tany\tnumber\tof\tvariables.\tOn\tthe other\thand,\tforward-mode\tautodiff\twould\tneed\tto\trun\tonce\tfor\teach\tvariable\t(so\t10\ttimes\tif\twe\twant the\tgradients\twith\tregards\tto\t10\tdifferent\tvariables).\tAs\tfor\tsymbolic\tdifferentiation,\tit\twould\tbuild a\tdifferent\tgraph\tto\tcompute\tthe\tgradients,\tso\tit\twould\tnot\ttraverse\tthe\toriginal\tgraph\tat\tall\t(except when\tbuilding\tthe\tnew\tgradients\tgraph).\tA\thighly\toptimized\tsymbolic\tdifferentiation\tsystem\tcould potentially\trun\tthe\tnew\tgradients\tgraph\tonly\tonce\tto\tcompute\tthe\tgradients\twith\tregards\tto\tall variables,\tbut\tthat\tnew\tgraph\tmay\tbe\thorribly\tcomplex\tand\tinefficient\tcompared\tto\tthe\toriginal graph.\n\n12.\t See\tthe\tJupyter\tnotebooks\tavailable\tat\thttps://github.com/ageron/handson-ml.\n\nChapter\t10:\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\n1.\t Here\tis\ta\tneural\tnetwork\tbased\ton\tthe\toriginal\tartificial\tneurons\tthat\tcomputes\tA represents\tthe\texclusive\tOR),\tusing\tthe\tfact\tthat\tA\t solutions\t—\tfor\texample,\tusing\tthe\tfact\tthat\tA\t \tB),\tand\tso\ton.\n\n1.\t Here\tis\ta\tneural\tnetwork\tbased\ton\tthe\toriginal\tartificial\tneurons\tthat\tcomputes\tA (¬\tA\t \tB),\tor\tthe\tfact\tthat\tA\n\nB\t=\t(A\n\n(¬\tA\n\n2.\t A\tclassical\tPerceptron\twill\tconverge\tonly\tif\tthe\tdataset\tis\tlinearly\tseparable,\tand\tit\twon’t\tbe\table\tto estimate\tclass\tprobabilities.\tIn\tcontrast,\ta\tLogistic\tRegression\tclassifier\twill\tconverge\tto\ta\tgood solution\teven\tif\tthe\tdataset\tis\tnot\tlinearly\tseparable,\tand\tit\twill\toutput\tclass\tprobabilities.\tIf\tyou change\tthe\tPerceptron’s\tactivation\tfunction\tto\tthe\tlogistic\tactivation\tfunction\t(or\tthe\tsoftmax activation\tfunction\tif\tthere\tare\tmultiple\tneurons),\tand\tif\tyou\ttrain\tit\tusing\tGradient\tDescent\t(or\tsome other\toptimization\talgorithm\tminimizing\tthe\tcost\tfunction,\ttypically\tcross\tentropy),\tthen\tit\tbecomes equivalent\tto\ta\tLogistic\tRegression\tclassifier.\n\n3.\t The\tlogistic\tactivation\tfunction\twas\ta\tkey\tingredient\tin\ttraining\tthe\tfirst\tMLPs\tbecause\tits\tderivative is\talways\tnonzero,\tso\tGradient\tDescent\tcan\talways\troll\tdown\tthe\tslope.\tWhen\tthe\tactivation function\tis\ta\tstep\tfunction,\tGradient\tDescent\tcannot\tmove,\tas\tthere\tis\tno\tslope\tat\tall.\n\n4.\t The\tstep\tfunction,\tthe\tlogistic\tfunction,\tthe\thyperbolic\ttangent,\tthe\trectified\tlinear\tunit\t(see Figure\t10-8).\tSee\tChapter\t11\tfor\tother\texamples,\tsuch\tas\tELU\tand\tvariants\tof\tthe\tReLU.\n\n5.\t Considering\tthe\tMLP\tdescribed\tin\tthe\tquestion:\tsuppose\tyou\thave\tan\tMLP\tcomposed\tof\tone\tinput layer\twith\t10\tpassthrough\tneurons,\tfollowed\tby\tone\thidden\tlayer\twith\t50\tartificial\tneurons,\tand finally\tone\toutput\tlayer\twith\t3\tartificial\tneurons.\tAll\tartificial\tneurons\tuse\tthe\tReLU\tactivation function. The\tshape\tof\tthe\tinput\tmatrix\tX\tis\tm\t×\t10,\twhere\tm\trepresents\tthe\ttraining\tbatch\tsize.\n\nThe\tshape\tof\tthe\thidden\tlayer’s\tweight\tvector\tWh\tis\t10\t×\t50\tand\tthe\tlength\tof\tits\tbias\tvector\tbh is\t50.\n\nThe\tshape\tof\tthe\toutput\tlayer’s\tweight\tvector\tWo\tis\t50\t×\t3,\tand\tthe\tlength\tof\tits\tbias\tvector\tbo\n\nB\t=\t(A\n\nis\t3.\n\nThe\tshape\tof\tthe\tnetwork’s\toutput\tmatrix\tY\tis\tm\t×\t3.\n\nY\t=\tReLU(ReLU(X\t·\tWh\t+\tbh)\t·\tWo\t+\tbo).\tRecall\tthat\tthe\tReLU\tfunction\tjust\tsets\tevery negative\tnumber\tin\tthe\tmatrix\tto\tzero.\tAlso\tnote\tthat\twhen\tyou\tare\tadding\ta\tbias\tvector\tto\ta matrix,\tit\tis\tadded\tto\tevery\tsingle\trow\tin\tthe\tmatrix,\twhich\tis\tcalled\tbroadcasting.\n\n6.\t To\tclassify\temail\tinto\tspam\tor\tham,\tyou\tjust\tneed\tone\tneuron\tin\tthe\toutput\tlayer\tof\ta\tneural\tnetwork —\tfor\texample,\tindicating\tthe\tprobability\tthat\tthe\temail\tis\tspam.\tYou\twould\ttypically\tuse\tthe\tlogistic activation\tfunction\tin\tthe\toutput\tlayer\twhen\testimating\ta\tprobability.\tIf\tinstead\tyou\twant\tto\ttackle MNIST,\tyou\tneed\t10\tneurons\tin\tthe\toutput\tlayer,\tand\tyou\tmust\treplace\tthe\tlogistic\tfunction\twith\tthe softmax\tactivation\tfunction,\twhich\tcan\thandle\tmultiple\tclasses,\toutputting\tone\tprobability\tper\tclass. Now,\tif\tyou\twant\tyour\tneural\tnetwork\tto\tpredict\thousing\tprices\tlike\tin\tChapter\t2,\tthen\tyou\tneed\tone output\tneuron,\tusing\tno\tactivation\tfunction\tat\tall\tin\tthe\toutput\tlayer.4\n\n7.\t Backpropagation\tis\ta\ttechnique\tused\tto\ttrain\tartificial\tneural\tnetworks.\tIt\tfirst\tcomputes\tthe\tgradients of\tthe\tcost\tfunction\twith\tregards\tto\tevery\tmodel\tparameter\t(all\tthe\tweights\tand\tbiases),\tand\tthen\tit performs\ta\tGradient\tDescent\tstep\tusing\tthese\tgradients.\tThis\tbackpropagation\tstep\tis\ttypically performed\tthousands\tor\tmillions\tof\ttimes,\tusing\tmany\ttraining\tbatches,\tuntil\tthe\tmodel\tparameters converge\tto\tvalues\tthat\t(hopefully)\tminimize\tthe\tcost\tfunction.\tTo\tcompute\tthe\tgradients, backpropagation\tuses\treverse-mode\tautodiff\t(although\tit\twasn’t\tcalled\tthat\twhen\tbackpropagation was\tinvented,\tand\tit\thas\tbeen\treinvented\tseveral\ttimes).\tReverse-mode\tautodiff\tperforms\ta\tforward pass\tthrough\ta\tcomputation\tgraph,\tcomputing\tevery\tnode’s\tvalue\tfor\tthe\tcurrent\ttraining\tbatch,\tand then\tit\tperforms\ta\treverse\tpass,\tcomputing\tall\tthe\tgradients\tat\tonce\t(see\tAppendix\tD\tfor\tmore details).\tSo\twhat’s\tthe\tdifference?\tWell,\tbackpropagation\trefers\tto\tthe\twhole\tprocess\tof\ttraining\tan artificial\tneural\tnetwork\tusing\tmultiple\tbackpropagation\tsteps,\teach\tof\twhich\tcomputes\tgradients and\tuses\tthem\tto\tperform\ta\tGradient\tDescent\tstep.\tIn\tcontrast,\treverse-mode\tautodiff\tis\ta\tsimply\ta technique\tto\tcompute\tgradients\tefficiently,\tand\tit\thappens\tto\tbe\tused\tby\tbackpropagation.\n\n8.\t Here\tis\ta\tlist\tof\tall\tthe\thyperparameters\tyou\tcan\ttweak\tin\ta\tbasic\tMLP:\tthe\tnumber\tof\thidden\tlayers, the\tnumber\tof\tneurons\tin\teach\thidden\tlayer,\tand\tthe\tactivation\tfunction\tused\tin\teach\thidden\tlayer\tand in\tthe\toutput\tlayer.5\tIn\tgeneral,\tthe\tReLU\tactivation\tfunction\t(or\tone\tof\tits\tvariants;\tsee\tChapter\t11) is\ta\tgood\tdefault\tfor\tthe\thidden\tlayers.\tFor\tthe\toutput\tlayer,\tin\tgeneral\tyou\twill\twant\tthe\tlogistic activation\tfunction\tfor\tbinary\tclassification,\tthe\tsoftmax\tactivation\tfunction\tfor\tmulticlass classification,\tor\tno\tactivation\tfunction\tfor\tregression.\t If\tthe\tMLP\toverfits\tthe\ttraining\tdata,\tyou\tcan\ttry\treducing\tthe\tnumber\tof\thidden\tlayers\tand\treducing the\tnumber\tof\tneurons\tper\thidden\tlayer.\n\n9.\t See\tthe\tJupyter\tnotebooks\tavailable\tat\thttps://github.com/ageron/handson-ml.\n\nChapter\t11:\tTraining\tDeep\tNeural\tNets\n\n1.\t No,\tall\tweights\tshould\tbe\tsampled\tindependently;\tthey\tshould\tnot\tall\thave\tthe\tsame\tinitial\tvalue. One\timportant\tgoal\tof\tsampling\tweights\trandomly\tis\tto\tbreak\tsymmetries:\tif\tall\tthe\tweights\thave\tthe same\tinitial\tvalue,\teven\tif\tthat\tvalue\tis\tnot\tzero,\tthen\tsymmetry\tis\tnot\tbroken\t(i.e.,\tall\tneurons\tin\ta given\tlayer\tare\tequivalent),\tand\tbackpropagation\twill\tbe\tunable\tto\tbreak\tit.\tConcretely,\tthis\tmeans that\tall\tthe\tneurons\tin\tany\tgiven\tlayer\twill\talways\thave\tthe\tsame\tweights.\tIt’s\tlike\thaving\tjust\tone neuron\tper\tlayer,\tand\tmuch\tslower.\tIt\tis\tvirtually\timpossible\tfor\tsuch\ta\tconfiguration\tto\tconverge\tto a\tgood\tsolution.\n\n2.\t It\tis\tperfectly\tfine\tto\tinitialize\tthe\tbias\tterms\tto\tzero.\tSome\tpeople\tlike\tto\tinitialize\tthem\tjust\tlike weights,\tand\tthat’s\tokay\ttoo;\tit\tdoes\tnot\tmake\tmuch\tdifference.\n\n3.\t A\tfew\tadvantages\tof\tthe\tELU\tfunction\tover\tthe\tReLU\tfunction\tare:\n\nIt\tcan\ttake\ton\tnegative\tvalues,\tso\tthe\taverage\toutput\tof\tthe\tneurons\tin\tany\tgiven\tlayer\tis typically\tcloser\tto\t0\tthan\twhen\tusing\tthe\tReLU\tactivation\tfunction\t(which\tnever\toutputs negative\tvalues).\tThis\thelps\talleviate\tthe\tvanishing\tgradients\tproblem.\n\nIt\talways\thas\ta\tnonzero\tderivative,\twhich\tavoids\tthe\tdying\tunits\tissue\tthat\tcan\taffect\tReLU units.\n\nIt\tis\tsmooth\teverywhere,\twhereas\tthe\tReLU’s\tslope\tabruptly\tjumps\tfrom\t0\tto\t1\tat\tz\t=\t0.\tSuch\tan abrupt\tchange\tcan\tslow\tdown\tGradient\tDescent\tbecause\tit\twill\tbounce\taround\tz\t=\t0.\n\n4.\t The\tELU\tactivation\tfunction\tis\ta\tgood\tdefault.\tIf\tyou\tneed\tthe\tneural\tnetwork\tto\tbe\tas\tfast\tas possible,\tyou\tcan\tuse\tone\tof\tthe\tleaky\tReLU\tvariants\tinstead\t(e.g.,\ta\tsimple\tleaky\tReLU\tusing\tthe default\thyperparameter\tvalue).\tThe\tsimplicity\tof\tthe\tReLU\tactivation\tfunction\tmakes\tit\tmany people’s\tpreferred\toption,\tdespite\tthe\tfact\tthat\tthey\tare\tgenerally\toutperformed\tby\tthe\tELU\tand\tleaky ReLU.\tHowever,\tthe\tReLU\tactivation\tfunction’s\tcapability\tof\toutputting\tprecisely\tzero\tcan\tbe\tuseful in\tsome\tcases\t(e.g.,\tsee\tChapter\t15).\tThe\thyperbolic\ttangent\t(tanh)\tcan\tbe\tuseful\tin\tthe\toutput\tlayer\tif you\tneed\tto\toutput\ta\tnumber\tbetween\t–1\tand\t1,\tbut\tnowadays\tit\tis\tnot\tused\tmuch\tin\thidden\tlayers. The\tlogistic\tactivation\tfunction\tis\talso\tuseful\tin\tthe\toutput\tlayer\twhen\tyou\tneed\tto\testimate\ta probability\t(e.g.,\tfor\tbinary\tclassification),\tbut\tit\tis\talso\trarely\tused\tin\thidden\tlayers\t(there\tare exceptions\t—\tfor\texample,\tfor\tthe\tcoding\tlayer\tof\tvariational\tautoencoders;\tsee\tChapter\t15). Finally,\tthe\tsoftmax\tactivation\tfunction\tis\tuseful\tin\tthe\toutput\tlayer\tto\toutput\tprobabilities\tfor mutually\texclusive\tclasses,\tbut\tother\tthan\tthat\tit\tis\trarely\t(if\tever)\tused\tin\thidden\tlayers.\n\n5.\t If\tyou\tset\tthe\tmomentum\thyperparameter\ttoo\tclose\tto\t1\t(e.g.,\t0.99999)\twhen\tusing\ta\n\nMomentumOptimizer,\tthen\tthe\talgorithm\twill\tlikely\tpick\tup\ta\tlot\tof\tspeed,\thopefully\troughly\ttoward the\tglobal\tminimum,\tbut\tthen\tit\twill\tshoot\tright\tpast\tthe\tminimum,\tdue\tto\tits\tmomentum.\tThen\tit\twill slow\tdown\tand\tcome\tback,\taccelerate\tagain,\tovershoot\tagain,\tand\tso\ton.\tIt\tmay\toscillate\tthis\tway many\ttimes\tbefore\tconverging,\tso\toverall\tit\twill\ttake\tmuch\tlonger\tto\tconverge\tthan\twith\ta\tsmaller momentum\tvalue.\n\n6.\t One\tway\tto\tproduce\ta\tsparse\tmodel\t(i.e.,\twith\tmost\tweights\tequal\tto\tzero)\tis\tto\ttrain\tthe\tmodel normally,\tthen\tzero\tout\ttiny\tweights.\tFor\tmore\tsparsity,\tyou\tcan\tapply\tℓ1\tregularization\tduring\n\ntraining,\twhich\tpushes\tthe\toptimizer\ttoward\tsparsity.\tA\tthird\toption\tis\tto\tcombine\tℓ1\tregularization with\tdual\taveraging,\tusing\tTensorFlow’s\tFTRLOptimizer\tclass.\n\n7.\t Yes,\tdropout\tdoes\tslow\tdown\ttraining,\tin\tgeneral\troughly\tby\ta\tfactor\tof\ttwo.\tHowever,\tit\thas\tno impact\ton\tinference\tsince\tit\tis\tonly\tturned\ton\tduring\ttraining.\n\nFor\tthe\tsolutions\tto\texercises\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.\n\nChapter\t12:\tDistributing\tTensorFlow\tAcross\tDevices\tand\tServers\n\n1.\t When\ta\tTensorFlow\tprocess\tstarts,\tit\tgrabs\tall\tthe\tavailable\tmemory\ton\tall\tGPU\tdevices\tthat\tare visible\tto\tit,\tso\tif\tyou\tget\ta\tCUDA_ERROR_OUT_OF_MEMORY\twhen\tstarting\tyour\tTensorFlow\tprogram, it\tprobably\tmeans\tthat\tother\tprocesses\tare\trunning\tthat\thave\talready\tgrabbed\tall\tthe\tmemory\ton\tat least\tone\tvisible\tGPU\tdevice\t(most\tlikely\tit\tis\tanother\tTensorFlow\tprocess).\tTo\tfix\tthis\tproblem,\ta trivial\tsolution\tis\tto\tstop\tthe\tother\tprocesses\tand\ttry\tagain.\tHowever,\tif\tyou\tneed\tall\tprocesses\tto\trun simultaneously,\ta\tsimple\toption\tis\tto\tdedicate\tdifferent\tdevices\tto\teach\tprocess,\tby\tsetting\tthe CUDA_VISIBLE_DEVICES\tenvironment\tvariable\tappropriately\tfor\teach\tdevice.\tAnother\toption\tis\tto configure\tTensorFlow\tto\tgrab\tonly\tpart\tof\tthe\tGPU\tmemory,\tinstead\tof\tall\tof\tit,\tby\tcreating\ta ConfigProto,\tsetting\tits\tgpu_options.per_process_gpu_memory_fraction\tto\tthe\tproportion\tof the\ttotal\tmemory\tthat\tit\tshould\tgrab\t(e.g.,\t0.4),\tand\tusing\tthis\tConfigProto\twhen\topening\ta\tsession. The\tlast\toption\tis\tto\ttell\tTensorFlow\tto\tgrab\tmemory\tonly\twhen\tit\tneeds\tit\tby\tsetting\tthe gpu_options.allow_growth\tto\tTrue.\tHowever,\tthis\tlast\toption\tis\tusually\tnot\trecommended because\tany\tmemory\tthat\tTensorFlow\tgrabs\tis\tnever\treleased,\tand\tit\tis\tharder\tto\tguarantee\ta repeatable\tbehavior\t(there\tmay\tbe\trace\tconditions\tdepending\ton\twhich\tprocesses\tstart\tfirst,\thow much\tmemory\tthey\tneed\tduring\ttraining,\tand\tso\ton).\n\n2.\t By\tpinning\tan\toperation\ton\ta\tdevice,\tyou\tare\ttelling\tTensorFlow\tthat\tthis\tis\twhere\tyou\twould\tlike this\toperation\tto\tbe\tplaced.\tHowever,\tsome\tconstraints\tmay\tprevent\tTensorFlow\tfrom\thonoring\tyour request.\tFor\texample,\tthe\toperation\tmay\thave\tno\timplementation\t(called\ta\tkernel)\tfor\tthat\tparticular type\tof\tdevice.\tIn\tthis\tcase,\tTensorFlow\twill\traise\tan\texception\tby\tdefault,\tbut\tyou\tcan\tconfigure\tit to\tfall\tback\tto\tthe\tCPU\tinstead\t(this\tis\tcalled\tsoft\tplacement).\tAnother\texample\tis\tan\toperation\tthat can\tmodify\ta\tvariable;\tthis\toperation\tand\tthe\tvariable\tneed\tto\tbe\tcollocated.\tSo\tthe\tdifference between\tpinning\tan\toperation\tand\tplacing\tan\toperation\tis\tthat\tpinning\tis\twhat\tyou\task\tTensorFlow (“Please\tplace\tthis\toperation\ton\tGPU\t#1”)\twhile\tplacement\tis\twhat\tTensorFlow\tactually\tends\tup doing\t(“Sorry,\tfalling\tback\tto\tthe\tCPU”).\n\n3.\t If\tyou\tare\trunning\ton\ta\tGPU-enabled\tTensorFlow\tinstallation,\tand\tyou\tjust\tuse\tthe\tdefault\tplacement, then\tif\tall\toperations\thave\ta\tGPU\tkernel\t(i.e.,\ta\tGPU\timplementation),\tyes,\tthey\twill\tall\tbe\tplaced\ton the\tfirst\tGPU.\tHowever,\tif\tone\tor\tmore\toperations\tdo\tnot\thave\ta\tGPU\tkernel,\tthen\tby\tdefault TensorFlow\twill\traise\tan\texception.\tIf\tyou\tconfigure\tTensorFlow\tto\tfall\tback\tto\tthe\tCPU\tinstead (soft\tplacement),\tthen\tall\toperations\twill\tbe\tplaced\ton\tthe\tfirst\tGPU\texcept\tthe\tones\twithout\ta\tGPU kernel\tand\tall\tthe\toperations\tthat\tmust\tbe\tcollocated\twith\tthem\t(see\tthe\tanswer\tto\tthe\tprevious exercise).\n\n4.\t Yes,\tif\tyou\tpin\ta\tvariable\tto\t\"/gpu:0\",\tit\tcan\tbe\tused\tby\toperations\tplaced\ton\t/gpu:1.\tTensorFlow will\tautomatically\ttake\tcare\tof\tadding\tthe\tappropriate\toperations\tto\ttransfer\tthe\tvariable’s\tvalue across\tdevices.\tThe\tsame\tgoes\tfor\tdevices\tlocated\ton\tdifferent\tservers\t(as\tlong\tas\tthey\tare\tpart\tof the\tsame\tcluster). 5.\t Yes,\ttwo\toperations\tplaced\ton\tthe\tsame\tdevice\tcan\trun\tin\tparallel:\tTensorFlow\tautomatically\ttakes care\tof\trunning\toperations\tin\tparallel\t(on\tdifferent\tCPU\tcores\tor\tdifferent\tGPU\tthreads),\tas\tlong\tas no\toperation\tdepends\ton\tanother\toperation’s\toutput.\tMoreover,\tyou\tcan\tstart\tmultiple\tsessions\tin parallel\tthreads\t(or\tprocesses),\tand\tevaluate\toperations\tin\teach\tthread.\tSince\tsessions\tare independent,\tTensorFlow\twill\tbe\table\tto\tevaluate\tany\toperation\tfrom\tone\tsession\tin\tparallel\twith\n\nany\toperation\tfrom\tanother\tsession.\n\n6.\t Control\tdependencies\tare\tused\twhen\tyou\twant\tto\tpostpone\tthe\tevaluation\tof\tan\toperation\tX\tuntil after\tsome\tother\toperations\tare\trun,\teven\tthough\tthese\toperations\tare\tnot\trequired\tto\tcompute\tX. This\tis\tuseful\tin\tparticular\twhen\tX\twould\toccupy\ta\tlot\tof\tmemory\tand\tyou\tonly\tneed\tit\tlater\tin\tthe computation\tgraph,\tor\tif\tX\tuses\tup\ta\tlot\tof\tI/O\t(for\texample,\tit\trequires\ta\tlarge\tvariable\tvalue located\ton\ta\tdifferent\tdevice\tor\tserver)\tand\tyou\tdon’t\twant\tit\tto\trun\tat\tthe\tsame\ttime\tas\tother\tI/O- hungry\toperations,\tto\tavoid\tsaturating\tthe\tbandwidth.\n\n7.\t You’re\tin\tluck!\tIn\tdistributed\tTensorFlow,\tthe\tvariable\tvalues\tlive\tin\tcontainers\tmanaged\tby\tthe cluster,\tso\teven\tif\tyou\tclose\tthe\tsession\tand\texit\tthe\tclient\tprogram,\tthe\tmodel\tparameters\tare\tstill alive\tand\twell\ton\tthe\tcluster.\tYou\tsimply\tneed\tto\topen\ta\tnew\tsession\tto\tthe\tcluster\tand\tsave\tthe model\t(make\tsure\tyou\tdon’t\tcall\tthe\tvariable\tinitializers\tor\trestore\ta\tprevious\tmodel,\tas\tthis\twould destroy\tyour\tprecious\tnew\tmodel!).\n\nFor\tthe\tsolutions\tto\texercises\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.\n\nChapter\t13:\tConvolutional\tNeural\tNetworks\n\n1.\t These\tare\tthe\tmain\tadvantages\tof\ta\tCNN\tover\ta\tfully\tconnected\tDNN\tfor\timage\tclassification:\n\nBecause\tconsecutive\tlayers\tare\tonly\tpartially\tconnected\tand\tbecause\tit\theavily\treuses\tits weights,\ta\tCNN\thas\tmany\tfewer\tparameters\tthan\ta\tfully\tconnected\tDNN,\twhich\tmakes\tit\tmuch faster\tto\ttrain,\treduces\tthe\trisk\tof\toverfitting,\tand\trequires\tmuch\tless\ttraining\tdata.\n\nWhen\ta\tCNN\thas\tlearned\ta\tkernel\tthat\tcan\tdetect\ta\tparticular\tfeature,\tit\tcan\tdetect\tthat\tfeature anywhere\ton\tthe\timage.\tIn\tcontrast,\twhen\ta\tDNN\tlearns\ta\tfeature\tin\tone\tlocation,\tit\tcan\tdetect\tit only\tin\tthat\tparticular\tlocation.\tSince\timages\ttypically\thave\tvery\trepetitive\tfeatures,\tCNNs\tare able\tto\tgeneralize\tmuch\tbetter\tthan\tDNNs\tfor\timage\tprocessing\ttasks\tsuch\tas\tclassification, using\tfewer\ttraining\texamples.\n\nFinally,\ta\tDNN\thas\tno\tprior\tknowledge\tof\thow\tpixels\tare\torganized;\tit\tdoes\tnot\tknow\tthat nearby\tpixels\tare\tclose.\tA\tCNN’s\tarchitecture\tembeds\tthis\tprior\tknowledge.\tLower\tlayers typically\tidentify\tfeatures\tin\tsmall\tareas\tof\tthe\timages,\twhile\thigher\tlayers\tcombine\tthe\tlower- level\tfeatures\tinto\tlarger\tfeatures.\tThis\tworks\twell\twith\tmost\tnatural\timages,\tgiving\tCNNs\ta decisive\thead\tstart\tcompared\tto\tDNNs.\n\n2.\t Let’s\tcompute\thow\tmany\tparameters\tthe\tCNN\thas.\tSince\tits\tfirst\tconvolutional\tlayer\thas\t3\t×\t3 kernels,\tand\tthe\tinput\thas\tthree\tchannels\t(red,\tgreen,\tand\tblue),\tthen\teach\tfeature\tmap\thas\t3\t×\t3\t×\t3 weights,\tplus\ta\tbias\tterm.\tThat’s\t28\tparameters\tper\tfeature\tmap.\tSince\tthis\tfirst\tconvolutional\tlayer has\t100\tfeature\tmaps,\tit\thas\ta\ttotal\tof\t2,800\tparameters.\tThe\tsecond\tconvolutional\tlayer\thas\t3\t×\t3 kernels,\tand\tits\tinput\tis\tthe\tset\tof\t100\tfeature\tmaps\tof\tthe\tprevious\tlayer,\tso\teach\tfeature\tmap\thas\t3\t× 3\t×\t100\t=\t900\tweights,\tplus\ta\tbias\tterm.\tSince\tit\thas\t200\tfeature\tmaps,\tthis\tlayer\thas\t901\t×\t200\t= 180,200\tparameters.\tFinally,\tthe\tthird\tand\tlast\tconvolutional\tlayer\talso\thas\t3\t×\t3\tkernels,\tand\tits input\tis\tthe\tset\tof\t200\tfeature\tmaps\tof\tthe\tprevious\tlayers,\tso\teach\tfeature\tmap\thas\t3\t×\t3\t×\t200\t= 1,800\tweights,\tplus\ta\tbias\tterm.\tSince\tit\thas\t400\tfeature\tmaps,\tthis\tlayer\thas\ta\ttotal\tof\t1,801\t×\t400\t= 720,400\tparameters.\tAll\tin\tall,\tthe\tCNN\thas\t2,800\t+\t180,200\t+\t720,400\t=\t903,400\tparameters.\t Now\tlet’s\tcompute\thow\tmuch\tRAM\tthis\tneural\tnetwork\twill\trequire\t(at\tleast)\twhen\tmaking\ta prediction\tfor\ta\tsingle\tinstance.\tFirst\tlet’s\tcompute\tthe\tfeature\tmap\tsize\tfor\teach\tlayer.\tSince\twe\tare using\ta\tstride\tof\t2\tand\tSAME\tpadding,\tthe\thorizontal\tand\tvertical\tsize\tof\tthe\tfeature\tmaps\tare divided\tby\t2\tat\teach\tlayer\t(rounding\tup\tif\tnecessary),\tso\tas\tthe\tinput\tchannels\tare\t200\t×\t300\tpixels, the\tfirst\tlayer’s\tfeature\tmaps\tare\t100\t×\t150,\tthe\tsecond\tlayer’s\tfeature\tmaps\tare\t50\t×\t75,\tand\tthe third\tlayer’s\tfeature\tmaps\tare\t25\t×\t38.\tSince\t32\tbits\tis\t4\tbytes\tand\tthe\tfirst\tconvolutional\tlayer\thas 100\tfeature\tmaps,\tthis\tfirst\tlayer\ttakes\tup\t4\tx\t100\t×\t150\t×\t100\t=\t6\tmillion\tbytes\t(about\t5.7\tMB, considering\tthat\t1\tMB\t=\t1,024\tKB\tand\t1\tKB\t=\t1,024\tbytes).\tThe\tsecond\tlayer\ttakes\tup\t4\t×\t50\t×\t75 ×\t200\t=\t3\tmillion\tbytes\t(about\t2.9\tMB).\tFinally,\tthe\tthird\tlayer\ttakes\tup\t4\t×\t25\t×\t38\t×\t400\t= 1,520,000\tbytes\t(about\t1.4\tMB).\tHowever,\tonce\ta\tlayer\thas\tbeen\tcomputed,\tthe\tmemory\toccupied by\tthe\tprevious\tlayer\tcan\tbe\treleased,\tso\tif\teverything\tis\twell\toptimized,\tonly\t6\t+\t9\t=\t15\tmillion bytes\t(about\t14.3\tMB)\tof\tRAM\twill\tbe\trequired\t(when\tthe\tsecond\tlayer\thas\tjust\tbeen\tcomputed,\tbut the\tmemory\toccupied\tby\tthe\tfirst\tlayer\tis\tnot\treleased\tyet).\tBut\twait,\tyou\talso\tneed\tto\tadd\tthe memory\toccupied\tby\tthe\tCNN’s\tparameters.\tWe\tcomputed\tearlier\tthat\tit\thas\t903,400\tparameters, each\tusing\tup\t4\tbytes,\tso\tthis\tadds\t3,613,600\tbytes\t(about\t3.4\tMB).\tThe\ttotal\tRAM\trequired\tis\t(at least)\t18,613,600\tbytes\t(about\t17.8\tMB).\n\nLastly,\tlet’s\tcompute\tthe\tminimum\tamount\tof\tRAM\trequired\twhen\ttraining\tthe\tCNN\ton\ta\tmini-batch of\t50\timages.\tDuring\ttraining\tTensorFlow\tuses\tbackpropagation,\twhich\trequires\tkeeping\tall\tvalues computed\tduring\tthe\tforward\tpass\tuntil\tthe\treverse\tpass\tbegins.\tSo\twe\tmust\tcompute\tthe\ttotal\tRAM required\tby\tall\tlayers\tfor\ta\tsingle\tinstance\tand\tmultiply\tthat\tby\t50!\tAt\tthat\tpoint\tlet’s\tstart\tcounting\tin megabytes\trather\tthan\tbytes.\tWe\tcomputed\tbefore\tthat\tthe\tthree\tlayers\trequire\trespectively\t5.7,\t2.9, and\t1.4\tMB\tfor\teach\tinstance.\tThat’s\ta\ttotal\tof\t10.0\tMB\tper\tinstance.\tSo\tfor\t50\tinstances\tthe\ttotal RAM\tis\t500\tMB.\tAdd\tto\tthat\tthe\tRAM\trequired\tby\tthe\tinput\timages,\twhich\tis\t50\t×\t4\t×\t200\t×\t300\t× 3\t=\t36\tmillion\tbytes\t(about\t34.3\tMB),\tplus\tthe\tRAM\trequired\tfor\tthe\tmodel\tparameters,\twhich\tis about\t3.4\tMB\t(computed\tearlier),\tplus\tsome\tRAM\tfor\tthe\tgradients\t(we\twill\tneglect\tthem\tsince\tthey can\tbe\treleased\tgradually\tas\tbackpropagation\tgoes\tdown\tthe\tlayers\tduring\tthe\treverse\tpass).\tWe\tare up\tto\ta\ttotal\tof\troughly\t500.0\t+\t34.3\t+\t3.4\t=\t537.7\tMB.\tAnd\tthat’s\treally\tan\toptimistic\tbare minimum.\n\n3.\t If\tyour\tGPU\truns\tout\tof\tmemory\twhile\ttraining\ta\tCNN,\there\tare\tfive\tthings\tyou\tcould\ttry\tto\tsolve\tthe problem\t(other\tthan\tpurchasing\ta\tGPU\twith\tmore\tRAM):\n\nReduce\tthe\tmini-batch\tsize.\n\nReduce\tdimensionality\tusing\ta\tlarger\tstride\tin\tone\tor\tmore\tlayers.\n\nRemove\tone\tor\tmore\tlayers.\n\nUse\t16-bit\tfloats\tinstead\tof\t32-bit\tfloats.\n\nDistribute\tthe\tCNN\tacross\tmultiple\tdevices.\n\n4.\t A\tmax\tpooling\tlayer\thas\tno\tparameters\tat\tall,\twhereas\ta\tconvolutional\tlayer\thas\tquite\ta\tfew\t(see\tthe previous\tquestions).\n\n5.\t A\tlocal\tresponse\tnormalization\tlayer\tmakes\tthe\tneurons\tthat\tmost\tstrongly\tactivate\tinhibit\tneurons at\tthe\tsame\tlocation\tbut\tin\tneighboring\tfeature\tmaps,\twhich\tencourages\tdifferent\tfeature\tmaps\tto specialize\tand\tpushes\tthem\tapart,\tforcing\tthem\tto\texplore\ta\twider\trange\tof\tfeatures.\tIt\tis\ttypically used\tin\tthe\tlower\tlayers\tto\thave\ta\tlarger\tpool\tof\tlow-level\tfeatures\tthat\tthe\tupper\tlayers\tcan\tbuild upon.\n\n6.\t The\tmain\tinnovations\tin\tAlexNet\tcompared\tto\tLeNet-5\tare\t(1)\tit\tis\tmuch\tlarger\tand\tdeeper,\tand\t(2) it\tstacks\tconvolutional\tlayers\tdirectly\ton\ttop\tof\teach\tother,\tinstead\tof\tstacking\ta\tpooling\tlayer\ton\ttop of\teach\tconvolutional\tlayer.\tThe\tmain\tinnovation\tin\tGoogLeNet\tis\tthe\tintroduction\tof\tinception modules,\twhich\tmake\tit\tpossible\tto\thave\ta\tmuch\tdeeper\tnet\tthan\tprevious\tCNN\tarchitectures,\twith fewer\tparameters.\tFinally,\tResNet’s\tmain\tinnovation\tis\tthe\tintroduction\tof\tskip\tconnections,\twhich make\tit\tpossible\tto\tgo\twell\tbeyond\t100\tlayers.\tArguably,\tits\tsimplicity\tand\tconsistency\tare\talso rather\tinnovative.\n\nFor\tthe\tsolutions\tto\texercises\t7,\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.\n\nChapter\t14:\tRecurrent\tNeural\tNetworks\n\n1.\t Here\tare\ta\tfew\tRNN\tapplications:\n\nFor\ta\tsequence-to-sequence\tRNN:\tpredicting\tthe\tweather\t(or\tany\tother\ttime\tseries),\tmachine translation\t(using\tan\tencoder–decoder\tarchitecture),\tvideo\tcaptioning,\tspeech\tto\ttext,\tmusic generation\t(or\tother\tsequence\tgeneration),\tidentifying\tthe\tchords\tof\ta\tsong.\n\nFor\ta\tsequence-to-vector\tRNN:\tclassifying\tmusic\tsamples\tby\tmusic\tgenre,\tanalyzing\tthe sentiment\tof\ta\tbook\treview,\tpredicting\twhat\tword\tan\taphasic\tpatient\tis\tthinking\tof\tbased\ton readings\tfrom\tbrain\timplants,\tpredicting\tthe\tprobability\tthat\ta\tuser\twill\twant\tto\twatch\ta\tmovie based\ton\ther\twatch\thistory\t(this\tis\tone\tof\tmany\tpossible\timplementations\tof\tcollaborative filtering).\n\nFor\ta\tvector-to-sequence\tRNN:\timage\tcaptioning,\tcreating\ta\tmusic\tplaylist\tbased\ton\tan embedding\tof\tthe\tcurrent\tartist,\tgenerating\ta\tmelody\tbased\ton\ta\tset\tof\tparameters,\tlocating pedestrians\tin\ta\tpicture\t(e.g.,\ta\tvideo\tframe\tfrom\ta\tself-driving\tcar’s\tcamera).\n\n2.\t In\tgeneral,\tif\tyou\ttranslate\ta\tsentence\tone\tword\tat\ta\ttime,\tthe\tresult\twill\tbe\tterrible.\tFor\texample,\tthe French\tsentence\t“Je\tvous\ten\tprie”\tmeans\t“You\tare\twelcome,”\tbut\tif\tyou\ttranslate\tit\tone\tword\tat\ta time,\tyou\tget\t“I\tyou\tin\tpray.”\tHuh?\tIt\tis\tmuch\tbetter\tto\tread\tthe\twhole\tsentence\tfirst\tand\tthen\ttranslate it.\tA\tplain\tsequence-to-sequence\tRNN\twould\tstart\ttranslating\ta\tsentence\timmediately\tafter\treading the\tfirst\tword,\twhile\tan\tencoder–decoder\tRNN\twill\tfirst\tread\tthe\twhole\tsentence\tand\tthen\ttranslate it.\tThat\tsaid,\tone\tcould\timagine\ta\tplain\tsequence-to-sequence\tRNN\tthat\twould\toutput\tsilence whenever\tit\tis\tunsure\tabout\twhat\tto\tsay\tnext\t(just\tlike\thuman\ttranslators\tdo\twhen\tthey\tmust\ttranslate a\tlive\tbroadcast).\n\n3.\t To\tclassify\tvideos\tbased\ton\tthe\tvisual\tcontent,\tone\tpossible\tarchitecture\tcould\tbe\tto\ttake\t(say)\tone frame\tper\tsecond,\tthen\trun\teach\tframe\tthrough\ta\tconvolutional\tneural\tnetwork,\tfeed\tthe\toutput\tof\tthe CNN\tto\ta\tsequence-to-vector\tRNN,\tand\tfinally\trun\tits\toutput\tthrough\ta\tsoftmax\tlayer,\tgiving\tyou\tall the\tclass\tprobabilities.\tFor\ttraining\tyou\twould\tjust\tuse\tcross\tentropy\tas\tthe\tcost\tfunction.\tIf\tyou wanted\tto\tuse\tthe\taudio\tfor\tclassification\tas\twell,\tyou\tcould\tconvert\tevery\tsecond\tof\taudio\tto\ta spectrograph,\tfeed\tthis\tspectrograph\tto\ta\tCNN,\tand\tfeed\tthe\toutput\tof\tthis\tCNN\tto\tthe\tRNN\t(along with\tthe\tcorresponding\toutput\tof\tthe\tother\tCNN).\n\n4.\t Building\tan\tRNN\tusing\tdynamic_rnn()\trather\tthan\tstatic_rnn()\toffers\tseveral\tadvantages:\n\nIt\tis\tbased\ton\ta\twhile_loop()\toperation\tthat\tis\table\tto\tswap\tthe\tGPU’s\tmemory\tto\tthe\tCPU’s memory\tduring\tbackpropagation,\tavoiding\tout-of-memory\terrors.\n\nIt\tis\targuably\teasier\tto\tuse,\tas\tit\tcan\tdirectly\ttake\ta\tsingle\ttensor\tas\tinput\tand\toutput\t(covering all\ttime\tsteps),\trather\tthan\ta\tlist\tof\ttensors\t(one\tper\ttime\tstep).\tNo\tneed\tto\tstack,\tunstack,\tor transpose.\n\nIt\tgenerates\ta\tsmaller\tgraph,\teasier\tto\tvisualize\tin\tTensorBoard.\n\n5.\t To\thandle\tvariable\tlength\tinput\tsequences,\tthe\tsimplest\toption\tis\tto\tset\tthe\tsequence_length\n\nparameter\twhen\tcalling\tthe\tstatic_rnn()\tor\tdynamic_rnn()\tfunctions.\tAnother\toption\tis\tto\tpad\n\nthe\tsmaller\tinputs\t(e.g.,\twith\tzeros)\tto\tmake\tthem\tthe\tsame\tsize\tas\tthe\tlargest\tinput\t(this\tmay\tbe faster\tthan\tthe\tfirst\toption\tif\tthe\tinput\tsequences\tall\thave\tvery\tsimilar\tlengths).\tTo\thandle\tvariable- length\toutput\tsequences,\tif\tyou\tknow\tin\tadvance\tthe\tlength\tof\teach\toutput\tsequence,\tyou\tcan\tuse\tthe sequence_length\tparameter\t(for\texample,\tconsider\ta\tsequence-to-sequence\tRNN\tthat\tlabels\tevery frame\tin\ta\tvideo\twith\ta\tviolence\tscore:\tthe\toutput\tsequence\twill\tbe\texactly\tthe\tsame\tlength\tas\tthe input\tsequence).\tIf\tyou\tdon’t\tknow\tin\tadvance\tthe\tlength\tof\tthe\toutput\tsequence,\tyou\tcan\tuse\tthe padding\ttrick:\talways\toutput\tthe\tsame\tsize\tsequence,\tbut\tignore\tany\toutputs\tthat\tcome\tafter\tthe\tend- of-sequence\ttoken\t(by\tignoring\tthem\twhen\tcomputing\tthe\tcost\tfunction).\n\n6.\t To\tdistribute\ttraining\tand\texecution\tof\ta\tdeep\tRNN\tacross\tmultiple\tGPUs,\ta\tcommon\ttechnique\tis simply\tto\tplace\teach\tlayer\ton\ta\tdifferent\tGPU\t(see\tChapter\t12).\n\nFor\tthe\tsolutions\tto\texercises\t7,\t8,\tand\t9,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.\n\nChapter\t15:\tAutoencoders\n\n1.\t Here\tare\tsome\tof\tthe\tmain\ttasks\tthat\tautoencoders\tare\tused\tfor:\n\nFeature\textraction\n\nUnsupervised\tpretraining\n\nDimensionality\treduction\n\nGenerative\tmodels\n\nAnomaly\tdetection\t(an\tautoencoder\tis\tgenerally\tbad\tat\treconstructing\toutliers)\n\n2.\t If\tyou\twant\tto\ttrain\ta\tclassifier\tand\tyou\thave\tplenty\tof\tunlabeled\ttraining\tdata,\tbut\tonly\ta\tfew thousand\tlabeled\tinstances,\tthen\tyou\tcould\tfirst\ttrain\ta\tdeep\tautoencoder\ton\tthe\tfull\tdataset\t(labeled +\tunlabeled),\tthen\treuse\tits\tlower\thalf\tfor\tthe\tclassifier\t(i.e.,\treuse\tthe\tlayers\tup\tto\tthe\tcodings\tlayer, included)\tand\ttrain\tthe\tclassifier\tusing\tthe\tlabeled\tdata.\tIf\tyou\thave\tlittle\tlabeled\tdata,\tyou\tprobably want\tto\tfreeze\tthe\treused\tlayers\twhen\ttraining\tthe\tclassifier.\n\n3.\t The\tfact\tthat\tan\tautoencoder\tperfectly\treconstructs\tits\tinputs\tdoes\tnot\tnecessarily\tmean\tthat\tit\tis\ta good\tautoencoder;\tperhaps\tit\tis\tsimply\tan\tovercomplete\tautoencoder\tthat\tlearned\tto\tcopy\tits\tinputs to\tthe\tcodings\tlayer\tand\tthen\tto\tthe\toutputs.\tIn\tfact,\teven\tif\tthe\tcodings\tlayer\tcontained\ta\tsingle neuron,\tit\twould\tbe\tpossible\tfor\ta\tvery\tdeep\tautoencoder\tto\tlearn\tto\tmap\teach\ttraining\tinstance\tto\ta different\tcoding\t(e.g.,\tthe\tfirst\tinstance\tcould\tbe\tmapped\tto\t0.001,\tthe\tsecond\tto\t0.002,\tthe\tthird\tto 0.003,\tand\tso\ton),\tand\tit\tcould\tlearn\t“by\theart”\tto\treconstruct\tthe\tright\ttraining\tinstance\tfor\teach coding.\tIt\twould\tperfectly\treconstruct\tits\tinputs\twithout\treally\tlearning\tany\tuseful\tpattern\tin\tthe\tdata. In\tpractice\tsuch\ta\tmapping\tis\tunlikely\tto\thappen,\tbut\tit\tillustrates\tthe\tfact\tthat\tperfect\treconstructions are\tnot\ta\tguarantee\tthat\tthe\tautoencoder\tlearned\tanything\tuseful.\tHowever,\tif\tit\tproduces\tvery\tbad reconstructions,\tthen\tit\tis\talmost\tguaranteed\tto\tbe\ta\tbad\tautoencoder.\tTo\tevaluate\tthe\tperformance\tof an\tautoencoder,\tone\toption\tis\tto\tmeasure\tthe\treconstruction\tloss\t(e.g.,\tcompute\tthe\tMSE,\tthe\tmean square\tof\tthe\toutputs\tminus\tthe\tinputs).\tAgain,\ta\thigh\treconstruction\tloss\tis\ta\tgood\tsign\tthat\tthe autoencoder\tis\tbad,\tbut\ta\tlow\treconstruction\tloss\tis\tnot\ta\tguarantee\tthat\tit\tis\tgood.\tYou\tshould\talso evaluate\tthe\tautoencoder\taccording\tto\twhat\tit\twill\tbe\tused\tfor.\tFor\texample,\tif\tyou\tare\tusing\tit\tfor unsupervised\tpretraining\tof\ta\tclassifier,\tthen\tyou\tshould\talso\tevaluate\tthe\tclassifier’s\tperformance.\n\n4.\t An\tundercomplete\tautoencoder\tis\tone\twhose\tcodings\tlayer\tis\tsmaller\tthan\tthe\tinput\tand\toutput layers.\tIf\tit\tis\tlarger,\tthen\tit\tis\tan\tovercomplete\tautoencoder.\tThe\tmain\trisk\tof\tan\texcessively undercomplete\tautoencoder\tis\tthat\tit\tmay\tfail\tto\treconstruct\tthe\tinputs.\tThe\tmain\trisk\tof\tan overcomplete\tautoencoder\tis\tthat\tit\tmay\tjust\tcopy\tthe\tinputs\tto\tthe\toutputs,\twithout\tlearning\tany useful\tfeature.\n\n5.\t To\ttie\tthe\tweights\tof\tan\tencoder\tlayer\tand\tits\tcorresponding\tdecoder\tlayer,\tyou\tsimply\tmake\tthe decoder\tweights\tequal\tto\tthe\ttranspose\tof\tthe\tencoder\tweights.\tThis\treduces\tthe\tnumber\tof parameters\tin\tthe\tmodel\tby\thalf,\toften\tmaking\ttraining\tconverge\tfaster\twith\tless\ttraining\tdata,\tand reducing\tthe\trisk\tof\toverfitting\tthe\ttraining\tset.\n\n6.\t To\tvisualize\tthe\tfeatures\tlearned\tby\tthe\tlower\tlayer\tof\ta\tstacked\tautoencoder,\ta\tcommon\ttechnique\tis simply\tto\tplot\tthe\tweights\tof\teach\tneuron,\tby\treshaping\teach\tweight\tvector\tto\tthe\tsize\tof\tan\tinput image\t(e.g.,\tfor\tMNIST,\treshaping\ta\tweight\tvector\tof\tshape\t[784]\tto\t[28,\t28]).\tTo\tvisualize\tthe features\tlearned\tby\thigher\tlayers,\tone\ttechnique\tis\tto\tdisplay\tthe\ttraining\tinstances\tthat\tmost\tactivate each\tneuron.\n\n7.\t A\tgenerative\tmodel\tis\ta\tmodel\tcapable\tof\trandomly\tgenerating\toutputs\tthat\tresemble\tthe\ttraining instances.\tFor\texample,\tonce\ttrained\tsuccessfully\ton\tthe\tMNIST\tdataset,\ta\tgenerative\tmodel\tcan\tbe used\tto\trandomly\tgenerate\trealistic\timages\tof\tdigits.\tThe\toutput\tdistribution\tis\ttypically\tsimilar\tto the\ttraining\tdata.\tFor\texample,\tsince\tMNIST\tcontains\tmany\timages\tof\teach\tdigit,\tthe\tgenerative model\twould\toutput\troughly\tthe\tsame\tnumber\tof\timages\tof\teach\tdigit.\tSome\tgenerative\tmodels\tcan be\tparametrized\t—\tfor\texample,\tto\tgenerate\tonly\tsome\tkinds\tof\toutputs.\tAn\texample\tof\ta\tgenerative autoencoder\tis\tthe\tvariational\tautoencoder.\n\nFor\tthe\tsolutions\tto\texercises\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.\n\nChapter\t16:\tReinforcement\tLearning\n\n1.\t Reinforcement\tLearning\tis\tan\tarea\tof\tMachine\tLearning\taimed\tat\tcreating\tagents\tcapable\tof\ttaking actions\tin\tan\tenvironment\tin\ta\tway\tthat\tmaximizes\trewards\tover\ttime.\tThere\tare\tmany\tdifferences between\tRL\tand\tregular\tsupervised\tand\tunsupervised\tlearning.\tHere\tare\ta\tfew: In\tsupervised\tand\tunsupervised\tlearning,\tthe\tgoal\tis\tgenerally\tto\tfind\tpatterns\tin\tthe\tdata.\tIn Reinforcement\tLearning,\tthe\tgoal\tis\tto\tfind\ta\tgood\tpolicy.\n\nUnlike\tin\tsupervised\tlearning,\tthe\tagent\tis\tnot\texplicitly\tgiven\tthe\t“right”\tanswer.\tIt\tmust\tlearn by\ttrial\tand\terror.\n\nUnlike\tin\tunsupervised\tlearning,\tthere\tis\ta\tform\tof\tsupervision,\tthrough\trewards.\tWe\tdo\tnot\ttell the\tagent\thow\tto\tperform\tthe\ttask,\tbut\twe\tdo\ttell\tit\twhen\tit\tis\tmaking\tprogress\tor\twhen\tit\tis failing.\n\nA\tReinforcement\tLearning\tagent\tneeds\tto\tfind\tthe\tright\tbalance\tbetween\texploring\tthe environment,\tlooking\tfor\tnew\tways\tof\tgetting\trewards,\tand\texploiting\tsources\tof\trewards\tthat\tit already\tknows.\tIn\tcontrast,\tsupervised\tand\tunsupervised\tlearning\tsystems\tgenerally\tdon’t\tneed to\tworry\tabout\texploration;\tthey\tjust\tfeed\ton\tthe\ttraining\tdata\tthey\tare\tgiven.\n\nIn\tsupervised\tand\tunsupervised\tlearning,\ttraining\tinstances\tare\ttypically\tindependent\t(in\tfact, they\tare\tgenerally\tshuffled).\tIn\tReinforcement\tLearning,\tconsecutive\tobservations\tare\tgenerally not\tindependent.\tAn\tagent\tmay\tremain\tin\tthe\tsame\tregion\tof\tthe\tenvironment\tfor\ta\twhile\tbefore it\tmoves\ton,\tso\tconsecutive\tobservations\twill\tbe\tvery\tcorrelated.\tIn\tsome\tcases\ta\treplay memory\tis\tused\tto\tensure\tthat\tthe\ttraining\talgorithm\tgets\tfairly\tindependent\tobservations.\n\n2.\t Here\tare\ta\tfew\tpossible\tapplications\tof\tReinforcement\tLearning,\tother\tthan\tthose\tmentioned\tin Chapter\t16:\n\nMusic\tpersonalization\n\nThe\tenvironment\tis\ta\tuser’s\tpersonalized\tweb\tradio.\tThe\tagent\tis\tthe\tsoftware\tdeciding\twhat song\tto\tplay\tnext\tfor\tthat\tuser.\tIts\tpossible\tactions\tare\tto\tplay\tany\tsong\tin\tthe\tcatalog\t(it\tmust\ttry to\tchoose\ta\tsong\tthe\tuser\twill\tenjoy)\tor\tto\tplay\tan\tadvertisement\t(it\tmust\ttry\tto\tchoose\tan\tad that\tthe\tuser\twill\tbe\tinterested\tin).\tIt\tgets\ta\tsmall\treward\tevery\ttime\tthe\tuser\tlistens\tto\ta\tsong,\ta larger\treward\tevery\ttime\tthe\tuser\tlistens\tto\tan\tad,\ta\tnegative\treward\twhen\tthe\tuser\tskips\ta\tsong or\tan\tad,\tand\ta\tvery\tnegative\treward\tif\tthe\tuser\tleaves.\n\nMarketing\n\nThe\tenvironment\tis\tyour\tcompany’s\tmarketing\tdepartment.\tThe\tagent\tis\tthe\tsoftware\tthat\tdefines which\tcustomers\ta\tmailing\tcampaign\tshould\tbe\tsent\tto,\tgiven\ttheir\tprofile\tand\tpurchase\thistory (for\teach\tcustomer\tit\thas\ttwo\tpossible\tactions:\tsend\tor\tdon’t\tsend).\tIt\tgets\ta\tnegative\treward for\tthe\tcost\tof\tthe\tmailing\tcampaign,\tand\ta\tpositive\treward\tfor\testimated\trevenue\tgenerated from\tthis\tcampaign.\n\nProduct\tdelivery\n\nLet\tthe\tagent\tcontrol\ta\tfleet\tof\tdelivery\ttrucks,\tdeciding\twhat\tthey\tshould\tpick\tup\tat\tthe\tdepots, where\tthey\tshould\tgo,\twhat\tthey\tshould\tdrop\toff,\tand\tso\ton.\tThey\twould\tget\tpositive\trewards for\teach\tproduct\tdelivered\ton\ttime,\tand\tnegative\trewards\tfor\tlate\tdeliveries.\n\n3.\t When\testimating\tthe\tvalue\tof\tan\taction,\tReinforcement\tLearning\talgorithms\ttypically\tsum\tall\tthe rewards\tthat\tthis\taction\tled\tto,\tgiving\tmore\tweight\tto\timmediate\trewards,\tand\tless\tweight\tto\tlater rewards\t(considering\tthat\tan\taction\thas\tmore\tinfluence\ton\tthe\tnear\tfuture\tthan\ton\tthe\tdistant\tfuture). To\tmodel\tthis,\ta\tdiscount\trate\tis\ttypically\tapplied\tat\teach\ttime\tstep.\tFor\texample,\twith\ta\tdiscount rate\tof\t0.9,\ta\treward\tof\t100\tthat\tis\treceived\ttwo\ttime\tsteps\tlater\tis\tcounted\tas\tonly\t0.92\t×\t100\t=\t81 when\tyou\tare\testimating\tthe\tvalue\tof\tthe\taction.\tYou\tcan\tthink\tof\tthe\tdiscount\trate\tas\ta\tmeasure\tof how\tmuch\tthe\tfuture\tis\tvalued\trelative\tto\tthe\tpresent:\tif\tit\tis\tvery\tclose\tto\t1,\tthen\tthe\tfuture\tis\tvalued almost\tas\tmuch\tas\tthe\tpresent.\tIf\tit\tis\tclose\tto\t0,\tthen\tonly\timmediate\trewards\tmatter.\tOf\tcourse,\tthis impacts\tthe\toptimal\tpolicy\ttremendously:\tif\tyou\tvalue\tthe\tfuture,\tyou\tmay\tbe\twilling\tto\tput\tup\twith\ta lot\tof\timmediate\tpain\tfor\tthe\tprospect\tof\teventual\trewards,\twhile\tif\tyou\tdon’t\tvalue\tthe\tfuture,\tyou will\tjust\tgrab\tany\timmediate\treward\tyou\tcan\tfind,\tnever\tinvesting\tin\tthe\tfuture.\n\n4.\t To\tmeasure\tthe\tperformance\tof\ta\tReinforcement\tLearning\tagent,\tyou\tcan\tsimply\tsum\tup\tthe\trewards it\tgets.\tIn\ta\tsimulated\tenvironment,\tyou\tcan\trun\tmany\tepisodes\tand\tlook\tat\tthe\ttotal\trewards\tit\tgets\ton average\t(and\tpossibly\tlook\tat\tthe\tmin,\tmax,\tstandard\tdeviation,\tand\tso\ton).\n\n5.\t The\tcredit\tassignment\tproblem\tis\tthe\tfact\tthat\twhen\ta\tReinforcement\tLearning\tagent\treceives\ta reward,\tit\thas\tno\tdirect\tway\tof\tknowing\twhich\tof\tits\tprevious\tactions\tcontributed\tto\tthis\treward.\tIt typically\toccurs\twhen\tthere\tis\ta\tlarge\tdelay\tbetween\tan\taction\tand\tthe\tresulting\trewards\t(e.g.,\tduring a\tgame\tof\tAtari’s\tPong,\tthere\tmay\tbe\ta\tfew\tdozen\ttime\tsteps\tbetween\tthe\tmoment\tthe\tagent\thits\tthe ball\tand\tthe\tmoment\tit\twins\tthe\tpoint).\tOne\tway\tto\talleviate\tit\tis\tto\tprovide\tthe\tagent\twith\tshorter- term\trewards,\twhen\tpossible.\tThis\tusually\trequires\tprior\tknowledge\tabout\tthe\ttask.\tFor\texample,\tif we\twant\tto\tbuild\tan\tagent\tthat\twill\tlearn\tto\tplay\tchess,\tinstead\tof\tgiving\tit\ta\treward\tonly\twhen\tit wins\tthe\tgame,\twe\tcould\tgive\tit\ta\treward\tevery\ttime\tit\tcaptures\tone\tof\tthe\topponent’s\tpieces.\n\n6.\t An\tagent\tcan\toften\tremain\tin\tthe\tsame\tregion\tof\tits\tenvironment\tfor\ta\twhile,\tso\tall\tof\tits\texperiences will\tbe\tvery\tsimilar\tfor\tthat\tperiod\tof\ttime.\tThis\tcan\tintroduce\tsome\tbias\tin\tthe\tlearning\talgorithm.\tIt may\ttune\tits\tpolicy\tfor\tthis\tregion\tof\tthe\tenvironment,\tbut\tit\twill\tnot\tperform\twell\tas\tsoon\tas\tit moves\tout\tof\tthis\tregion.\tTo\tsolve\tthis\tproblem,\tyou\tcan\tuse\ta\treplay\tmemory;\tinstead\tof\tusing\tonly the\tmost\timmediate\texperiences\tfor\tlearning,\tthe\tagent\twill\tlearn\tbased\ton\ta\tbuffer\tof\tits\tpast experiences,\trecent\tand\tnot\tso\trecent\t(perhaps\tthis\tis\twhy\twe\tdream\tat\tnight:\tto\treplay\tour experiences\tof\tthe\tday\tand\tbetter\tlearn\tfrom\tthem?).\n\n7.\t An\toff-policy\tRL\talgorithm\tlearns\tthe\tvalue\tof\tthe\toptimal\tpolicy\t(i.e.,\tthe\tsum\tof\tdiscounted rewards\tthat\tcan\tbe\texpected\tfor\teach\tstate\tif\tthe\tagent\tacts\toptimally),\tindependently\tof\thow\tthe agent\tactually\tacts.\tQ-Learning\tis\ta\tgood\texample\tof\tsuch\tan\talgorithm.\tIn\tcontrast,\tan\ton-policy algorithm\tlearns\tthe\tvalue\tof\tthe\tpolicy\tthat\tthe\tagent\tactually\texecutes,\tincluding\tboth\texploration and\texploitation.\n\nFor\tthe\tsolutions\tto\texercises\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.\n\n1\n\nIf\tyou\tdraw\ta\tstraight\tline\tbetween\tany\ttwo\tpoints\ton\tthe\tcurve,\tthe\tline\tnever\tcrosses\tthe\tcurve.\n\n2\n\n3\n\n4\n\n5\n\nMoreover,\tthe\tNormal\tEquation\trequires\tcomputing\tthe\tinverse\tof\ta\tmatrix,\tbut\tthat\tmatrix\tis\tnot\talways\tinvertible.\tIn\tcontrast,\tthe\tmatrix for\tRidge\tRegression\tis\talways\tinvertible.\n\nlog2\tis\tthe\tbinary\tlog,\tlog2(m)\t=\tlog(m)\t/\tlog(2).\n\nWhen\tthe\tvalues\tto\tpredict\tcan\tvary\tby\tmany\torders\tof\tmagnitude,\tthen\tyou\tmay\twant\tto\tpredict\tthe\tlogarithm\tof\tthe\ttarget\tvalue\trather than\tthe\ttarget\tvalue\tdirectly.\tSimply\tcomputing\tthe\texponential\tof\tthe\tneural\tnetwork’s\toutput\twill\tgive\tyou\tthe\testimated\tvalue\t(since exp(log\tv)\t=\tv).\n\nIn\tChapter\t11\twe\tdiscuss\tmany\ttechniques\tthat\tintroduce\tadditional\thyperparameters:\ttype\tof\tweight\tinitialization,\tactivation\tfunction hyperparameters\t(e.g.,\tamount\tof\tleak\tin\tleaky\tReLU),\tGradient\tClipping\tthreshold,\ttype\tof\toptimizer\tand\tits\thyperparameters\t(e.g.,\tthe momentum\thyperparameter\twhen\tusing\ta\tMomentumOptimizer),\ttype\tof\tregularization\tfor\teach\tlayer,\tand\tthe\tregularization hyperparameters\t(e.g.,\tdropout\trate\twhen\tusing\tdropout)\tand\tso\ton.\n\nAppendix\tB.\tMachine\tLearning\tProject Checklist\n\nThis\tchecklist\tcan\tguide\tyou\tthrough\tyour\tMachine\tLearning\tprojects.\tThere\tare\teight\tmain\tsteps:\n\n1.\t Frame\tthe\tproblem\tand\tlook\tat\tthe\tbig\tpicture.\n\n2.\t Get\tthe\tdata.\n\n3.\t Explore\tthe\tdata\tto\tgain\tinsights.\n\n4.\t Prepare\tthe\tdata\tto\tbetter\texpose\tthe\tunderlying\tdata\tpatterns\tto\tMachine\tLearning\talgorithms.\n\n5.\t Explore\tmany\tdifferent\tmodels\tand\tshort-list\tthe\tbest\tones.\n\n6.\t Fine-tune\tyour\tmodels\tand\tcombine\tthem\tinto\ta\tgreat\tsolution.\n\n7.\t Present\tyour\tsolution.\n\n8.\t Launch,\tmonitor,\tand\tmaintain\tyour\tsystem.\n\nObviously,\tyou\tshould\tfeel\tfree\tto\tadapt\tthis\tchecklist\tto\tyour\tneeds.\n\nFrame\tthe\tProblem\tand\tLook\tat\tthe\tBig\tPicture\n\n1.\t Define\tthe\tobjective\tin\tbusiness\tterms.\n\n2.\t How\twill\tyour\tsolution\tbe\tused?\n\n3.\t What\tare\tthe\tcurrent\tsolutions/workarounds\t(if\tany)?\n\n4.\t How\tshould\tyou\tframe\tthis\tproblem\t(supervised/unsupervised,\tonline/offline,\tetc.)?\n\n5.\t How\tshould\tperformance\tbe\tmeasured?\n\n6.\t Is\tthe\tperformance\tmeasure\taligned\twith\tthe\tbusiness\tobjective?\n\n7.\t What\twould\tbe\tthe\tminimum\tperformance\tneeded\tto\treach\tthe\tbusiness\tobjective?\n\n8.\t What\tare\tcomparable\tproblems?\tCan\tyou\treuse\texperience\tor\ttools?\n\n9.\t Is\thuman\texpertise\tavailable?\n\n10.\t How\twould\tyou\tsolve\tthe\tproblem\tmanually?\n\n11.\t List\tthe\tassumptions\tyou\t(or\tothers)\thave\tmade\tso\tfar.\n\n12.\t Verify\tassumptions\tif\tpossible.\n\nGet\tthe\tData Note:\tautomate\tas\tmuch\tas\tpossible\tso\tyou\tcan\teasily\tget\tfresh\tdata.\n\n1.\t List\tthe\tdata\tyou\tneed\tand\thow\tmuch\tyou\tneed.\n\n2.\t Find\tand\tdocument\twhere\tyou\tcan\tget\tthat\tdata.\n\n3.\t Check\thow\tmuch\tspace\tit\twill\ttake.\n\n4.\t Check\tlegal\tobligations,\tand\tget\tauthorization\tif\tnecessary.\n\n5.\t Get\taccess\tauthorizations.\n\n6.\t Create\ta\tworkspace\t(with\tenough\tstorage\tspace).\n\n7.\t Get\tthe\tdata.\n\n8.\t Convert\tthe\tdata\tto\ta\tformat\tyou\tcan\teasily\tmanipulate\t(without\tchanging\tthe\tdata\titself).\n\n9.\t Ensure\tsensitive\tinformation\tis\tdeleted\tor\tprotected\t(e.g.,\tanonymized).\n\n10.\t Check\tthe\tsize\tand\ttype\tof\tdata\t(time\tseries,\tsample,\tgeographical,\tetc.).\n\n11.\t Sample\ta\ttest\tset,\tput\tit\taside,\tand\tnever\tlook\tat\tit\t(no\tdata\tsnooping!).\n\nExplore\tthe\tData Note:\ttry\tto\tget\tinsights\tfrom\ta\tfield\texpert\tfor\tthese\tsteps.\n\n1.\t Create\ta\tcopy\tof\tthe\tdata\tfor\texploration\t(sampling\tit\tdown\tto\ta\tmanageable\tsize\tif\tnecessary).\n\n2.\t Create\ta\tJupyter\tnotebook\tto\tkeep\ta\trecord\tof\tyour\tdata\texploration.\n\n3.\t Study\teach\tattribute\tand\tits\tcharacteristics:\n\nName\n\nType\t(categorical,\tint/float,\tbounded/unbounded,\ttext,\tstructured,\tetc.)\n\n%\tof\tmissing\tvalues\n\nNoisiness\tand\ttype\tof\tnoise\t(stochastic,\toutliers,\trounding\terrors,\tetc.)\n\nPossibly\tuseful\tfor\tthe\ttask?\n\nType\tof\tdistribution\t(Gaussian,\tuniform,\tlogarithmic,\tetc.)\n\n4.\t For\tsupervised\tlearning\ttasks,\tidentify\tthe\ttarget\tattribute(s).\n\n5.\t Visualize\tthe\tdata.\n\n6.\t Study\tthe\tcorrelations\tbetween\tattributes.\n\n7.\t Study\thow\tyou\twould\tsolve\tthe\tproblem\tmanually.\n\n8.\t Identify\tthe\tpromising\ttransformations\tyou\tmay\twant\tto\tapply.\n\n9.\t Identify\textra\tdata\tthat\twould\tbe\tuseful\t(go\tback\tto\t“Get\tthe\tData”).\n\n10.\t Document\twhat\tyou\thave\tlearned.\n\nPrepare\tthe\tData Notes:\n\nWork\ton\tcopies\tof\tthe\tdata\t(keep\tthe\toriginal\tdataset\tintact).\n\nWrite\tfunctions\tfor\tall\tdata\ttransformations\tyou\tapply,\tfor\tfive\treasons:\n\nSo\tyou\tcan\teasily\tprepare\tthe\tdata\tthe\tnext\ttime\tyou\tget\ta\tfresh\tdataset\n\nSo\tyou\tcan\tapply\tthese\ttransformations\tin\tfuture\tprojects\n\nTo\tclean\tand\tprepare\tthe\ttest\tset\n\nTo\tclean\tand\tprepare\tnew\tdata\tinstances\tonce\tyour\tsolution\tis\tlive\n\nTo\tmake\tit\teasy\tto\ttreat\tyour\tpreparation\tchoices\tas\thyperparameters\n\n1.\t Data\tcleaning:\n\nFix\tor\tremove\toutliers\t(optional).\n\nFill\tin\tmissing\tvalues\t(e.g.,\twith\tzero,\tmean,\tmedian…)\tor\tdrop\ttheir\trows\t(or\tcolumns).\n\n2.\t Feature\tselection\t(optional):\n\nDrop\tthe\tattributes\tthat\tprovide\tno\tuseful\tinformation\tfor\tthe\ttask.\n\n3.\t Feature\tengineering,\twhere\tappropriate:\n\nDiscretize\tcontinuous\tfeatures.\n\nDecompose\tfeatures\t(e.g.,\tcategorical,\tdate/time,\tetc.).\n\nAdd\tpromising\ttransformations\tof\tfeatures\t(e.g.,\tlog(x),\tsqrt(x),\tx^2,\tetc.).\n\nAggregate\tfeatures\tinto\tpromising\tnew\tfeatures.\n\n4.\t Feature\tscaling:\tstandardize\tor\tnormalize\tfeatures.\n\nShort-List\tPromising\tModels Notes:\n\nIf\tthe\tdata\tis\thuge,\tyou\tmay\twant\tto\tsample\tsmaller\ttraining\tsets\tso\tyou\tcan\ttrain\tmany\tdifferent models\tin\ta\treasonable\ttime\t(be\taware\tthat\tthis\tpenalizes\tcomplex\tmodels\tsuch\tas\tlarge\tneural\tnets or\tRandom\tForests).\n\nOnce\tagain,\ttry\tto\tautomate\tthese\tsteps\tas\tmuch\tas\tpossible.\n\n1.\t Train\tmany\tquick\tand\tdirty\tmodels\tfrom\tdifferent\tcategories\t(e.g.,\tlinear,\tnaive\tBayes,\tSVM, Random\tForests,\tneural\tnet,\tetc.)\tusing\tstandard\tparameters.\n\n2.\t Measure\tand\tcompare\ttheir\tperformance.\n\nFor\teach\tmodel,\tuse\tN-fold\tcross-validation\tand\tcompute\tthe\tmean\tand\tstandard\tdeviation\tof the\tperformance\tmeasure\ton\tthe\tN\tfolds.\n\n3.\t Analyze\tthe\tmost\tsignificant\tvariables\tfor\teach\talgorithm.\n\n4.\t Analyze\tthe\ttypes\tof\terrors\tthe\tmodels\tmake.\n\nWhat\tdata\twould\ta\thuman\thave\tused\tto\tavoid\tthese\terrors?\n\n5.\t Have\ta\tquick\tround\tof\tfeature\tselection\tand\tengineering.\n\n6.\t Have\tone\tor\ttwo\tmore\tquick\titerations\tof\tthe\tfive\tprevious\tsteps.\n\n7.\t Short-list\tthe\ttop\tthree\tto\tfive\tmost\tpromising\tmodels,\tpreferring\tmodels\tthat\tmake\tdifferent\ttypes\tof errors.\n\nFine-Tune\tthe\tSystem Notes:\n\nYou\twill\twant\tto\tuse\tas\tmuch\tdata\tas\tpossible\tfor\tthis\tstep,\tespecially\tas\tyou\tmove\ttoward\tthe\tend of\tfine-tuning.\n\nAs\talways\tautomate\twhat\tyou\tcan.\n\n1.\t Fine-tune\tthe\thyperparameters\tusing\tcross-validation.\n\nTreat\tyour\tdata\ttransformation\tchoices\tas\thyperparameters,\tespecially\twhen\tyou\tare\tnot\tsure about\tthem\t(e.g.,\tshould\tI\treplace\tmissing\tvalues\twith\tzero\tor\twith\tthe\tmedian\tvalue?\tOr\tjust drop\tthe\trows?).\n\nUnless\tthere\tare\tvery\tfew\thyperparameter\tvalues\tto\texplore,\tprefer\trandom\tsearch\tover\tgrid search.\tIf\ttraining\tis\tvery\tlong,\tyou\tmay\tprefer\ta\tBayesian\toptimization\tapproach\t(e.g.,\tusing Gaussian\tprocess\tpriors,\tas\tdescribed\tby\tJasper\tSnoek,\tHugo\tLarochelle,\tand\tRyan\tAdams).1\n\n2.\t Try\tEnsemble\tmethods.\tCombining\tyour\tbest\tmodels\twill\toften\tperform\tbetter\tthan\trunning\tthem individually.\n\n3.\t Once\tyou\tare\tconfident\tabout\tyour\tfinal\tmodel,\tmeasure\tits\tperformance\ton\tthe\ttest\tset\tto\testimate the\tgeneralization\terror.\n\nWARNING\n\nDon’t\ttweak\tyour\tmodel\tafter\tmeasuring\tthe\tgeneralization\terror:\tyou\twould\tjust\tstart\toverfitting\tthe\ttest\tset.\n\nPresent\tYour\tSolution\n\n1.\t Document\twhat\tyou\thave\tdone.\n\n2.\t Create\ta\tnice\tpresentation.\n\nMake\tsure\tyou\thighlight\tthe\tbig\tpicture\tfirst.\n\n3.\t Explain\twhy\tyour\tsolution\tachieves\tthe\tbusiness\tobjective.\n\n4.\t Don’t\tforget\tto\tpresent\tinteresting\tpoints\tyou\tnoticed\talong\tthe\tway.\n\nDescribe\twhat\tworked\tand\twhat\tdid\tnot.\n\nList\tyour\tassumptions\tand\tyour\tsystem’s\tlimitations.\n\n5.\t Ensure\tyour\tkey\tfindings\tare\tcommunicated\tthrough\tbeautiful\tvisualizations\tor\teasy-to-remember statements\t(e.g.,\t“the\tmedian\tincome\tis\tthe\tnumber-one\tpredictor\tof\thousing\tprices”).\n\nLaunch!\n\n1.\t Get\tyour\tsolution\tready\tfor\tproduction\t(plug\tinto\tproduction\tdata\tinputs,\twrite\tunit\ttests,\tetc.).\n\n2.\t Write\tmonitoring\tcode\tto\tcheck\tyour\tsystem’s\tlive\tperformance\tat\tregular\tintervals\tand\ttrigger\talerts when\tit\tdrops.\n\nBeware\tof\tslow\tdegradation\ttoo:\tmodels\ttend\tto\t“rot”\tas\tdata\tevolves.\n\nMeasuring\tperformance\tmay\trequire\ta\thuman\tpipeline\t(e.g.,\tvia\ta\tcrowdsourcing\tservice).\n\nAlso\tmonitor\tyour\tinputs’\tquality\t(e.g.,\ta\tmalfunctioning\tsensor\tsending\trandom\tvalues,\tor another\tteam’s\toutput\tbecoming\tstale).\tThis\tis\tparticularly\timportant\tfor\tonline\tlearning systems.\n\n3.\t Retrain\tyour\tmodels\ton\ta\tregular\tbasis\ton\tfresh\tdata\t(automate\tas\tmuch\tas\tpossible).\n\n1\n\n“Practical\tBayesian\tOptimization\tof\tMachine\tLearning\tAlgorithms,”\tJ.\tSnoek,\tH.\tLarochelle,\tR.\tAdams\t(2012).\n\nAppendix\tC.\tSVM\tDual\tProblem\n\nTo\tunderstand\tduality,\tyou\tfirst\tneed\tto\tunderstand\tthe\tLagrange\tmultipliers\tmethod.\tThe\tgeneral\tidea\tis to\ttransform\ta\tconstrained\toptimization\tobjective\tinto\tan\tunconstrained\tone,\tby\tmoving\tthe\tconstraints into\tthe\tobjective\tfunction.\tLet’s\tlook\tat\ta\tsimple\texample.\tSuppose\tyou\twant\tto\tfind\tthe\tvalues\tof\tx\tand\ty that\tminimize\tthe\tfunction\tf(x,y)\t=\tx2\t+\t2y,\tsubject\tto\tan\tequality\tconstraint:\t3x\t+\t2y\t+\t1\t=\t0.\tUsing\tthe Lagrange\tmultipliers\tmethod,\twe\tstart\tby\tdefining\ta\tnew\tfunction\tcalled\tthe\tLagrangian\t(or\tLagrange function):\tg(x,\ty,\tα)\t=\tf(x,\ty)\t–\tα(3x\t+\t2y\t+\t1).\tEach\tconstraint\t(in\tthis\tcase\tjust\tone)\tis\tsubtracted\tfrom the\toriginal\tobjective,\tmultiplied\tby\ta\tnew\tvariable\tcalled\ta\tLagrange\tmultiplier.\n\nJoseph-Louis\tLagrange\tshowed\tthat\tif\n\nis\ta\tsolution\tto\tthe\tconstrained\toptimization\tproblem,\tthen\n\nthere\tmust\texist\tan\t point\twhere\tall\tpartial\tderivatives\tare\tequal\tto\tzero).\tIn\tother\twords,\twe\tcan\tcompute\tthe\tpartial derivatives\tof\tg(x,\ty,\tα)\twith\tregards\tto\tx,\ty,\tand\tα;\twe\tcan\tfind\tthe\tpoints\twhere\tthese\tderivatives\tare\tall equal\tto\tzero;\tand\tthe\tsolutions\tto\tthe\tconstrained\toptimization\tproblem\t(if\tthey\texist)\tmust\tbe\tamong these\tstationary\tpoints.\n\nsuch\tthat\n\nis\ta\tstationary\tpoint\tof\tthe\tLagrangian\t(a\tstationary\tpoint\tis\ta\n\nIn\tthis\texample\tthe\tpartial\tderivatives\tare:\n\nWhen\tall\tthese\tpartial\tderivatives\tare\tequal\tto\t0,\twe\tfind\tthat\n\n,\tfrom\twhich\twe\tcan\teasily\tfind\tthat\n\n,\n\n,\tand\n\n.\tThis\tis\tthe\tonly\tstationary\tpoint,\tand\tas\tit\trespects\tthe\tconstraint,\tit\tmust\tbe\tthe\n\nsolution\tto\tthe\tconstrained\toptimization\tproblem.\n\nHowever,\tthis\tmethod\tapplies\tonly\tto\tequality\tconstraints.\tFortunately,\tunder\tsome\tregularity\tconditions (which\tare\trespected\tby\tthe\tSVM\tobjectives),\tthis\tmethod\tcan\tbe\tgeneralized\tto\tinequality\tconstraints\tas well\t(e.g.,\t3x\t+\t2y\t+\t1\t≥\t0).\tThe\tgeneralized\tLagrangian\tfor\tthe\thard\tmargin\tproblem\tis\tgiven\tby Equation\tC-1,\twhere\tthe\tα(i)\tvariables\tare\tcalled\tthe\tKarush–Kuhn–Tucker\t(KKT)\tmultipliers,\tand\tthey must\tbe\tgreater\tor\tequal\tto\tzero.\n\nEquation\tC-1.\tGeneralized\tLagrangian\tfor\tthe\thard\tmargin\tproblem\n\nJust\tlike\twith\tthe\tLagrange\tmultipliers\tmethod,\tyou\tcan\tcompute\tthe\tpartial\tderivatives\tand\tlocate\tthe\n\nstationary\tpoints.\tIf\tthere\tis\ta\tsolution,\tit\twill\tnecessarily\tbe\tamong\tthe\tstationary\tpoints\t respect\tthe\tKKT\tconditions:\n\nRespect\tthe\tproblem’s\tconstraints:\n\n,\n\nthat\n\nVerify\n\n,\n\nEither\n\nor\tthe\tith\tconstraint\tmust\tbe\tan\tactive\tconstraint,\tmeaning\tit\tmust\thold\tby\tequality:\n\n.\tThis\tcondition\tis\tcalled\tthe\tcomplementary\tslackness\tcondition.\tIt\timplies\n\nthat\teither\n\nor\tthe\tith\tinstance\tlies\ton\tthe\tboundary\t(it\tis\ta\tsupport\tvector).\n\nNote\tthat\tthe\tKKT\tconditions\tare\tnecessary\tconditions\tfor\ta\tstationary\tpoint\tto\tbe\ta\tsolution\tof\tthe constrained\toptimization\tproblem.\tUnder\tsome\tconditions,\tthey\tare\talso\tsufficient\tconditions.\tLuckily,\tthe SVM\toptimization\tproblem\thappens\tto\tmeet\tthese\tconditions,\tso\tany\tstationary\tpoint\tthat\tmeets\tthe\tKKT conditions\tis\tguaranteed\tto\tbe\ta\tsolution\tto\tthe\tconstrained\toptimization\tproblem.\n\nWe\tcan\tcompute\tthe\tpartial\tderivatives\tof\tthe\tgeneralized\tLagrangian\twith\tregards\tto\tw\tand\tb\twith Equation\tC-2.\n\nEquation\tC-2.\tPartial\tderivatives\tof\tthe\tgeneralized\tLagrangian\n\nWhen\tthese\tpartial\tderivatives\tare\tequal\tto\t0,\twe\thave\tEquation\tC-3.\n\nEquation\tC-3.\tProperties\tof\tthe\tstationary\tpoints\n\nIf\twe\tplug\tthese\tresults\tinto\tthe\tdefinition\tof\tthe\tgeneralized\tLagrangian,\tsome\tterms\tdisappear\tand\twe\n\nfind\tEquation\tC-4.\n\nEquation\tC-4.\tDual\tform\tof\tthe\tSVM\tproblem\n\nThe\tgoal\tis\tnow\tto\tfind\tthe\tvector\t constrained\toptimization\tproblem\tis\tthe\tdual\tproblem\twe\twere\tlooking\tfor.\n\nthat\tminimizes\tthis\tfunction,\twith\n\nfor\tall\tinstances.\tThis\n\nOnce\tyou\tfind\tthe\toptimal\t can\tuse\tthe\tfact\tthat\ta\tsupport\tvector\tverifies\tt(i)(wT\t·\tx(i)\t+\tb)\t=\t1,\tso\tif\tthe\tkth\tinstance\tis\ta\tsupport\tvector\n\n,\tyou\tcan\tcompute\n\nusing\tthe\tfirst\tline\tof\tEquation\tC-3.\tTo\tcompute\n\n,\tyou\n\n(i.e.,\tαk\t>\t0),\tyou\tcan\tuse\tit\tto\tcompute\t the\taverage\tover\tall\tsupport\tvectors\tto\tget\ta\tmore\tstable\tand\tprecise\tvalue,\tas\tin\tEquation\tC-5.\n\n.\tHowever,\tit\tis\toften\tprefered\tto\tcompute\n\nEquation\tC-5.\tBias\tterm\testimation\tusing\tthe\tdual\tform\n\nAppendix\tD.\tAutodiff\n\nThis\tappendix\texplains\thow\tTensorFlow’s\tautodiff\tfeature\tworks,\tand\thow\tit\tcompares\tto\tother\tsolutions.\n\nSuppose\tyou\tdefine\ta\tfunction\tf(x,y)\t=\tx2y\t+\ty\t+\t2,\tand\tyou\tneed\tits\tpartial\tderivatives\t typically\tto\tperform\tGradient\tDescent\t(or\tsome\tother\toptimization\talgorithm).\tYour\tmain\toptions\tare manual\tdifferentiation,\tsymbolic\tdifferentiation,\tnumerical\tdifferentiation,\tforward-mode\tautodiff,\tand finally\treverse-mode\tautodiff.\tTensorFlow\timplements\tthis\tlast\toption.\tLet’s\tgo\tthrough\teach\tof\tthese options.\n\nand\n\n,\n\nManual\tDifferentiation The\tfirst\tapproach\tis\tto\tpick\tup\ta\tpencil\tand\ta\tpiece\tof\tpaper\tand\tuse\tyour\tcalculus\tknowledge\tto\tderive the\tpartial\tderivatives\tmanually.\tFor\tthe\tfunction\tf(x,y)\tjust\tdefined,\tit\tis\tnot\ttoo\thard;\tyou\tjust\tneed\tto\tuse five\trules:\n\nThe\tderivative\tof\ta\tconstant\tis\t0.\n\nThe\tderivative\tof\tλx\tis\tλ\t(where\tλ\tis\ta\tconstant).\n\nThe\tderivative\tof\txλ\tis\tλxλ\t–\t1,\tso\tthe\tderivative\tof\tx2\tis\t2x.\n\nThe\tderivative\tof\ta\tsum\tof\tfunctions\tis\tthe\tsum\tof\tthese\tfunctions’\tderivatives.\n\nThe\tderivative\tof\tλ\ttimes\ta\tfunction\tis\tλ\ttimes\tits\tderivative.\n\nFrom\tthese\trules,\tyou\tcan\tderive\tEquation\tD-1:\n\nEquation\tD-1.\tPartial\tderivatives\tof\tf(x,y)\n\nThis\tapproach\tcan\tbecome\tvery\ttedious\tfor\tmore\tcomplex\tfunctions,\tand\tyou\trun\tthe\trisk\tof\tmaking mistakes.\tThe\tgood\tnews\tis\tthat\tderiving\tthe\tmathematical\tequations\tfor\tthe\tpartial\tderivatives\tlike\twe just\tdid\tcan\tbe\tautomated,\tthrough\ta\tprocess\tcalled\tsymbolic\tdifferentiation.\n\nSymbolic\tDifferentiation Figure\tD-1\tshows\thow\tsymbolic\tdifferentiation\tworks\ton\tan\teven\tsimpler\tfunction,\tg(x,y)\t=\t5\t+\txy.\tThe graph\tfor\tthat\tfunction\tis\trepresented\ton\tthe\tleft.\tAfter\tsymbolic\tdifferentiation,\twe\tget\tthe\tgraph\ton\tthe\n\nright,\twhich\trepresents\tthe\tpartial\tderivative\t the\tpartial\tderivative\twith\tregards\tto\ty).\n\n(we\tcould\tsimilarly\tobtain\n\nFigure\tD-1.\tSymbolic\tdifferentiation\n\nThe\talgorithm\tstarts\tby\tgetting\tthe\tpartial\tderivative\tof\tthe\tleaf\tnodes.\tThe\tconstant\tnode\t(5)\treturns\tthe constant\t0,\tsince\tthe\tderivative\tof\ta\tconstant\tis\talways\t0.\tThe\tvariable\tx\treturns\tthe\tconstant\t1\tsince\n\n,\tand\tthe\tvariable\ty\treturns\tthe\tconstant\t0\tsince\n\n(if\twe\twere\tlooking\tfor\tthe\tpartial\n\nderivative\twith\tregards\tto\ty,\tit\twould\tbe\tthe\treverse).\n\nNow\twe\thave\tall\twe\tneed\tto\tmove\tup\tthe\tgraph\tto\tthe\tmultiplication\tnode\tin\tfunction\tg.\tCalculus\ttells\tus\n\nthat\tthe\tderivative\tof\tthe\tproduct\tof\ttwo\tfunctions\tu\tand\tv\tis\t therefore\tconstruct\ta\tlarge\tpart\tof\tthe\tgraph\ton\tthe\tright,\trepresenting\t0\t×\tx\t+\ty\t×\t1.\n\n.\tWe\tcan\n\nFinally,\twe\tcan\tgo\tup\tto\tthe\taddition\tnode\tin\tfunction\tg.\tAs\tmentioned,\tthe\tderivative\tof\ta\tsum\tof functions\tis\tthe\tsum\tof\tthese\tfunctions’\tderivatives.\tSo\twe\tjust\tneed\tto\tcreate\tan\taddition\tnode\tand connect\tit\tto\tthe\tparts\tof\tthe\tgraph\twe\thave\talready\tcomputed.\tWe\tget\tthe\tcorrect\tpartial\tderivative:\n\n.\n\nHowever,\tit\tcan\tbe\tsimplified\t(a\tlot).\tA\tfew\ttrivial\tpruning\tsteps\tcan\tbe\tapplied\tto\tthis\tgraph\tto\tget\trid\tof\n\nall\tunnecessary\toperations,\tand\twe\tget\ta\tmuch\tsmaller\tgraph\twith\tjust\tone\tnode:\n\n.\n\nIn\tthis\tcase,\tsimplification\tis\tfairly\teasy,\tbut\tfor\ta\tmore\tcomplex\tfunction,\tsymbolic\tdifferentiation\tcan produce\ta\thuge\tgraph\tthat\tmay\tbe\ttough\tto\tsimplify\tand\tlead\tto\tsuboptimal\tperformance.\tMost\timportantly, symbolic\tdifferentiation\tcannot\tdeal\twith\tfunctions\tdefined\twith\tarbitrary\tcode\t—\tfor\texample,\tthe\n\nfollowing\tfunction\tdiscussed\tin\tChapter\t9:\n\ndef\tmy_func(a,\tb): \t\t\t\tz\t=\t0 \t\t\t\tfor\ti\tin\trange(100): \t\t\t\t\t\t\t\tz\t=\ta\t*\tnp.cos(z\t+\ti)\t+\tz\t*\tnp.sin(b\t-\ti) \t\t\t\treturn\tz\n\nNumerical\tDifferentiation The\tsimplest\tsolution\tis\tto\tcompute\tan\tapproximation\tof\tthe\tderivatives,\tnumerically.\tRecall\tthat\tthe derivative\th′(x0)\tof\ta\tfunction\th(x)\tat\ta\tpoint\tx0\tis\tthe\tslope\tof\tthe\tfunction\tat\tthat\tpoint,\tor\tmore\tprecisely Equation\tD-2.\n\nEquation\tD-2.\tDerivative\tof\ta\tfunction\th(x)\tat\tpoint\tx0\n\nSo\tif\twe\twant\tto\tcalculate\tthe\tpartial\tderivative\tof\tf(x,y)\twith\tregards\tto\tx,\tat\tx\t=\t3\tand\ty\t=\t4,\twe\tcan simply\tcompute\tf(3\t+\tϵ,\t4)\t–\tf(3,\t4)\tand\tdivide\tthe\tresult\tby\tϵ,\tusing\ta\tvery\tsmall\tvalue\tfor\tϵ.\tThat’s exactly\twhat\tthe\tfollowing\tcode\tdoes:\n\ndef\tf(x,\ty): \t\t\t\treturn\tx**2*y\t+\ty\t+\t2\n\ndef\tderivative(f,\tx,\ty,\tx_eps,\ty_eps): \t\t\t\treturn\t(f(x\t+\tx_eps,\ty\t+\ty_eps)\t-\tf(x,\ty))\t/\t(x_eps\t+\ty_eps)\n\ndf_dx\t=\tderivative(f,\t3,\t4,\t0.00001,\t0) df_dy\t=\tderivative(f,\t3,\t4,\t0,\t0.00001)\n\nUnfortunately,\tthe\tresult\tis\timprecise\t(and\tit\tgets\tworse\tfor\tmore\tcomplex\tfunctions).\tThe\tcorrect\tresults are\trespectively\t24\tand\t10,\tbut\tinstead\twe\tget:\n\n>>>\tprint(df_dx) 24.000039999805264 >>>\tprint(df_dy) 10.000000000331966\n\nNotice\tthat\tto\tcompute\tboth\tpartial\tderivatives,\twe\thave\tto\tcall\tf()\tat\tleast\tthree\ttimes\t(we\tcalled\tit\tfour times\tin\tthe\tpreceding\tcode,\tbut\tit\tcould\tbe\toptimized).\tIf\tthere\twere\t1,000\tparameters,\twe\twould\tneed\tto call\tf()\tat\tleast\t1,001\ttimes.\tWhen\tyou\tare\tdealing\twith\tlarge\tneural\tnetworks,\tthis\tmakes\tnumerical differentiation\tway\ttoo\tinefficient.\n\nHowever,\tnumerical\tdifferentiation\tis\tso\tsimple\tto\timplement\tthat\tit\tis\ta\tgreat\ttool\tto\tcheck\tthat\tthe\tother methods\tare\timplemented\tcorrectly.\tFor\texample,\tif\tit\tdisagrees\twith\tyour\tmanually\tderived\tfunction,\tthen your\tfunction\tprobably\tcontains\ta\tmistake.\n\nForward-Mode\tAutodiff Forward-mode\tautodiff\tis\tneither\tnumerical\tdifferentiation\tnor\tsymbolic\tdifferentiation,\tbut\tin\tsome ways\tit\tis\ttheir\tlove\tchild.\tIt\trelies\ton\tdual\tnumbers,\twhich\tare\t(weird\tbut\tfascinating)\tnumbers\tof\tthe form\ta\t+\tbϵ\twhere\ta\tand\tb\tare\treal\tnumbers\tand\tϵ\tis\tan\tinfinitesimal\tnumber\tsuch\tthat\tϵ2\t=\t0\t(but\tϵ\t≠\t0). You\tcan\tthink\tof\tthe\tdual\tnumber\t42\t+\t24ϵ\tas\tsomething\takin\tto\t42.0000000024\twith\tan\tinfinite\tnumber of\t0s\t(but\tof\tcourse\tthis\tis\tsimplified\tjust\tto\tgive\tyou\tsome\tidea\tof\twhat\tdual\tnumbers\tare).\tA\tdual number\tis\trepresented\tin\tmemory\tas\ta\tpair\tof\tfloats.\tFor\texample,\t42\t+\t24ϵ\tis\trepresented\tby\tthe\tpair (42.0,\t24.0).\n\nDual\tnumbers\tcan\tbe\tadded,\tmultiplied,\tand\tso\ton,\tas\tshown\tin\tEquation\tD-3.\n\nEquation\tD-3.\tA\tfew\toperations\twith\tdual\tnumbers\n\nMost\timportantly,\tit\tcan\tbe\tshown\tthat\th(a\t+\tbϵ)\t=\th(a)\t+\tb\t×\th′(a)ϵ,\tso\tcomputing\th(a\t+\tϵ)\tgives\tyou\tboth h(a)\tand\tthe\tderivative\th′(a)\tin\tjust\tone\tshot.\tFigure\tD-2\tshows\thow\tforward-mode\tautodiff\tcomputes\tthe partial\tderivative\tof\tf(x,y)\twith\tregards\tto\tx\tat\tx\t=\t3\tand\ty\t=\t4.\tAll\twe\tneed\tto\tdo\tis\tcompute\tf(3\t+\tϵ,\t4); this\twill\toutput\ta\tdual\tnumber\twhose\tfirst\tcomponent\tis\tequal\tto\tf(3,\t4)\tand\twhose\tsecond\tcomponent\tis\n\nequal\tto\n\n.\n\nFigure\tD-2.\tForward-mode\tautodiff\n\nTo\tcompute\n\nwe\twould\thave\tto\tgo\tthrough\tthe\tgraph\tagain,\tbut\tthis\ttime\twith\tx\t=\t3\tand\ty\t=\t4\t+\tϵ.\n\nSo\tforward-mode\tautodiff\tis\tmuch\tmore\taccurate\tthan\tnumerical\tdifferentiation,\tbut\tit\tsuffers\tfrom\tthe same\tmajor\tflaw:\tif\tthere\twere\t1,000\tparameters,\tit\twould\trequire\t1,000\tpasses\tthrough\tthe\tgraph\tto compute\tall\tthe\tpartial\tderivatives.\tThis\tis\twhere\treverse-mode\tautodiff\tshines:\tit\tcan\tcompute\tall\tof them\tin\tjust\ttwo\tpasses\tthrough\tthe\tgraph.\n\nReverse-Mode\tAutodiff Reverse-mode\tautodiff\tis\tthe\tsolution\timplemented\tby\tTensorFlow.\tIt\tfirst\tgoes\tthrough\tthe\tgraph\tin\tthe forward\tdirection\t(i.e.,\tfrom\tthe\tinputs\tto\tthe\toutput)\tto\tcompute\tthe\tvalue\tof\teach\tnode.\tThen\tit\tdoes\ta second\tpass,\tthis\ttime\tin\tthe\treverse\tdirection\t(i.e.,\tfrom\tthe\toutput\tto\tthe\tinputs)\tto\tcompute\tall\tthe\tpartial derivatives.\tFigure\tD-3\trepresents\tthe\tsecond\tpass.\tDuring\tthe\tfirst\tpass,\tall\tthe\tnode\tvalues\twere computed,\tstarting\tfrom\tx\t=\t3\tand\ty\t=\t4.\tYou\tcan\tsee\tthose\tvalues\tat\tthe\tbottom\tright\tof\teach\tnode\t(e.g.,\tx ×\tx\t=\t9).\tThe\tnodes\tare\tlabeled\tn1\tto\tn7\tfor\tclarity.\tThe\toutput\tnode\tis\tn7:\tf(3,4)\t=\tn7\t=\t42.\n\nFigure\tD-3.\tReverse-mode\tautodiff\n\nThe\tidea\tis\tto\tgradually\tgo\tdown\tthe\tgraph,\tcomputing\tthe\tpartial\tderivative\tof\tf(x,y)\twith\tregards\tto\teach consecutive\tnode,\tuntil\twe\treach\tthe\tvariable\tnodes.\tFor\tthis,\treverse-mode\tautodiff\trelies\theavily\ton\tthe chain\trule,\tshown\tin\tEquation\tD-4.\n\nEquation\tD-4.\tChain\trule\n\nSince\tn7\tis\tthe\toutput\tnode,\tf\t=\tn7\tso\ttrivially\n\n.\n\nLet’s\tcontinue\tdown\tthe\tgraph\tto\tn5:\thow\tmuch\tdoes\tf\tvary\twhen\tn5\tvaries?\tThe\tanswer\tis\n\n.\n\nWe\talready\tknow\tthat\n\n,\tso\tall\twe\tneed\tis\n\n.\tSince\tn7\tsimply\tperforms\tthe\tsum\tn5\t+\tn6,\twe\tfind\n\nthat\n\n,\tso\n\n.\n\nNow\twe\tcan\tproceed\tto\tnode\tn4:\thow\tmuch\tdoes\tf\tvary\twhen\tn4\tvaries?\tThe\tanswer\tis\n\nSince\tn5\t=\tn4\t×\tn2,\twe\tfind\tthat\n\n,\tso\n\n.\n\nThe\tprocess\tcontinues\tuntil\twe\treach\tthe\tbottom\tof\tthe\tgraph.\tAt\tthat\tpoint\twe\twill\thave\tcalculated\tall\tthe\n\npartial\tderivatives\tof\tf(x,y)\tat\tthe\tpoint\tx\t=\t3\tand\ty\t=\t4.\tIn\tthis\texample,\twe\tfind\t Sounds\tabout\tright!\n\nand\n\nReverse-mode\tautodiff\tis\ta\tvery\tpowerful\tand\taccurate\ttechnique,\tespecially\twhen\tthere\tare\tmany\tinputs and\tfew\toutputs,\tsince\tit\trequires\tonly\tone\tforward\tpass\tplus\tone\treverse\tpass\tper\toutput\tto\tcompute\tall the\tpartial\tderivatives\tfor\tall\toutputs\twith\tregards\tto\tall\tthe\tinputs.\tMost\timportantly,\tit\tcan\tdeal\twith functions\tdefined\tby\tarbitrary\tcode.\tIt\tcan\talso\thandle\tfunctions\tthat\tare\tnot\tentirely\tdifferentiable,\tas\tlong as\tyou\task\tit\tto\tcompute\tthe\tpartial\tderivatives\tat\tpoints\tthat\tare\tdifferentiable.\n\nTIP\n\nIf\tyou\timplement\ta\tnew\ttype\tof\toperation\tin\tTensorFlow\tand\tyou\twant\tto\tmake\tit\tcompatible\twith\tautodiff,\tthen\tyou\tneed\tto provide\ta\tfunction\tthat\tbuilds\ta\tsubgraph\tto\tcompute\tits\tpartial\tderivatives\twith\tregards\tto\tits\tinputs.\tFor\texample,\tsuppose\tyou implement\ta\tfunction\tthat\tcomputes\tthe\tsquare\tof\tits\tinput\tf(x)\t=\tx2.\tIn\tthat\tcase\tyou\twould\tneed\tto\tprovide\tthe\tcorresponding derivative\tfunction\tf′(x)\t=\t2x.\tNote\tthat\tthis\tfunction\tdoes\tnot\tcompute\ta\tnumerical\tresult,\tbut\tinstead\tbuilds\ta\tsubgraph\tthat\twill (later)\tcompute\tthe\tresult.\tThis\tis\tvery\tuseful\tbecause\tit\tmeans\tthat\tyou\tcan\tcompute\tgradients\tof\tgradients\t(to\tcompute\tsecond- order\tderivatives,\tor\teven\thigher-order\tderivatives).\n\n.\n\n.\n\nAppendix\tE.\tOther\tPopular\tANN\tArchitectures\n\nIn\tthis\tappendix\twe\twill\tgive\ta\tquick\toverview\tof\ta\tfew\thistorically\timportant\tneural\tnetwork architectures\tthat\tare\tmuch\tless\tused\ttoday\tthan\tdeep\tMulti-Layer\tPerceptrons\t(Chapter\t10), convolutional\tneural\tnetworks\t(Chapter\t13),\trecurrent\tneural\tnetworks\t(Chapter\t14),\tor\tautoencoders (Chapter\t15).\tThey\tare\toften\tmentioned\tin\tthe\tliterature,\tand\tsome\tare\tstill\tused\tin\tmany\tapplications,\tso it\tis\tworth\tknowing\tabout\tthem.\tMoreover,\twe\twill\tdiscuss\tdeep\tbelief\tnets\t(DBNs),\twhich\twere\tthe state\tof\tthe\tart\tin\tDeep\tLearning\tuntil\tthe\tearly\t2010s.\tThey\tare\tstill\tthe\tsubject\tof\tvery\tactive\tresearch,\tso they\tmay\twell\tcome\tback\twith\ta\tvengeance\tin\tthe\tnear\tfuture.\n\nHopfield\tNetworks Hopfield\tnetworks\twere\tfirst\tintroduced\tby\tW.\tA.\tLittle\tin\t1974,\tthen\tpopularized\tby\tJ.\tHopfield\tin\t1982. They\tare\tassociative\tmemory\tnetworks:\tyou\tfirst\tteach\tthem\tsome\tpatterns,\tand\tthen\twhen\tthey\tsee\ta\tnew pattern\tthey\t(hopefully)\toutput\tthe\tclosest\tlearned\tpattern.\tThis\thas\tmade\tthem\tuseful\tin\tparticular\tfor character\trecognition\tbefore\tthey\twere\toutperformed\tby\tother\tapproaches.\tYou\tfirst\ttrain\tthe\tnetwork\tby showing\tit\texamples\tof\tcharacter\timages\t(each\tbinary\tpixel\tmaps\tto\tone\tneuron),\tand\tthen\twhen\tyou\tshow it\ta\tnew\tcharacter\timage,\tafter\ta\tfew\titerations\tit\toutputs\tthe\tclosest\tlearned\tcharacter.\n\nThey\tare\tfully\tconnected\tgraphs\t(see\tFigure\tE-1);\tthat\tis,\tevery\tneuron\tis\tconnected\tto\tevery\tother\tneuron. Note\tthat\ton\tthe\tdiagram\tthe\timages\tare\t6\t×\t6\tpixels,\tso\tthe\tneural\tnetwork\ton\tthe\tleft\tshould\tcontain\t36 neurons\t(and\t648\tconnections),\tbut\tfor\tvisual\tclarity\ta\tmuch\tsmaller\tnetwork\tis\trepresented.\n\nFigure\tE-1.\tHopfield\tnetwork\n\nThe\ttraining\talgorithm\tworks\tby\tusing\tHebb’s\trule:\tfor\teach\ttraining\timage,\tthe\tweight\tbetween\ttwo neurons\tis\tincreased\tif\tthe\tcorresponding\tpixels\tare\tboth\ton\tor\tboth\toff,\tbut\tdecreased\tif\tone\tpixel\tis\ton and\tthe\tother\tis\toff.\n\nTo\tshow\ta\tnew\timage\tto\tthe\tnetwork,\tyou\tjust\tactivate\tthe\tneurons\tthat\tcorrespond\tto\tactive\tpixels.\tThe network\tthen\tcomputes\tthe\toutput\tof\tevery\tneuron,\tand\tthis\tgives\tyou\ta\tnew\timage.\tYou\tcan\tthen\ttake\tthis new\timage\tand\trepeat\tthe\twhole\tprocess.\tAfter\ta\twhile,\tthe\tnetwork\treaches\ta\tstable\tstate.\tGenerally,\tthis corresponds\tto\tthe\ttraining\timage\tthat\tmost\tresembles\tthe\tinput\timage.\n\nA\tso-called\tenergy\tfunction\tis\tassociated\twith\tHopfield\tnets.\tAt\teach\titeration,\tthe\tenergy\tdecreases,\tso the\tnetwork\tis\tguaranteed\tto\teventually\tstabilize\tto\ta\tlow-energy\tstate.\tThe\ttraining\talgorithm\ttweaks\tthe weights\tin\ta\tway\tthat\tdecreases\tthe\tenergy\tlevel\tof\tthe\ttraining\tpatterns,\tso\tthe\tnetwork\tis\tlikely\tto stabilize\tin\tone\tof\tthese\tlow-energy\tconfigurations.\tUnfortunately,\tsome\tpatterns\tthat\twere\tnot\tin\tthe training\tset\talso\tend\tup\twith\tlow\tenergy,\tso\tthe\tnetwork\tsometimes\tstabilizes\tin\ta\tconfiguration\tthat\twas\n\nnot\tlearned.\tThese\tare\tcalled\tspurious\tpatterns.\n\nAnother\tmajor\tflaw\twith\tHopfield\tnets\tis\tthat\tthey\tdon’t\tscale\tvery\twell\t—\ttheir\tmemory\tcapacity\tis roughly\tequal\tto\t14%\tof\tthe\tnumber\tof\tneurons.\tFor\texample,\tto\tclassify\t28\t×\t28\timages,\tyou\twould\tneed a\tHopfield\tnet\twith\t784\tfully\tconnected\tneurons\tand\t306,936\tweights.\tSuch\ta\tnetwork\twould\tonly\tbe\table to\tlearn\tabout\t110\tdifferent\tcharacters\t(14%\tof\t784).\tThat’s\ta\tlot\tof\tparameters\tfor\tsuch\ta\tsmall\tmemory.\n\nBoltzmann\tMachines Boltzmann\tmachines\twere\tinvented\tin\t1985\tby\tGeoffrey\tHinton\tand\tTerrence\tSejnowski.\tJust\tlike Hopfield\tnets,\tthey\tare\tfully\tconnected\tANNs,\tbut\tthey\tare\tbased\ton\tstochastic\tneurons:\tinstead\tof\tusing a\tdeterministic\tstep\tfunction\tto\tdecide\twhat\tvalue\tto\toutput,\tthese\tneurons\toutput\t1\twith\tsome\tprobability, and\t0\totherwise.\tThe\tprobability\tfunction\tthat\tthese\tANNs\tuse\tis\tbased\ton\tthe\tBoltzmann\tdistribution (used\tin\tstatistical\tmechanics)\thence\ttheir\tname.\tEquation\tE-1\tgives\tthe\tprobability\tthat\ta\tparticular neuron\twill\toutput\ta\t1.\n\nEquation\tE-1.\tProbability\tthat\tthe\tith\tneuron\twill\toutput\t1\n\nsj\tis\tthe\tjth\tneuron’s\tstate\t(0\tor\t1).\n\nwi,j\tis\tthe\tconnection\tweight\tbetween\tthe\tith\tand\tjth\tneurons.\tNote\tthat\twi,i\t=\t0.\n\nbi\tis\tthe\tith\tneuron’s\tbias\tterm.\tWe\tcan\timplement\tthis\tterm\tby\tadding\ta\tbias\tneuron\tto\tthe\tnetwork.\n\nN\tis\tthe\tnumber\tof\tneurons\tin\tthe\tnetwork.\n\nT\tis\ta\tnumber\tcalled\tthe\tnetwork’s\ttemperature;\tthe\thigher\tthe\ttemperature,\tthe\tmore\trandom\tthe output\tis\t(i.e.,\tthe\tmore\tthe\tprobability\tapproaches\t50%).\n\nσ\tis\tthe\tlogistic\tfunction.\n\nNeurons\tin\tBoltzmann\tmachines\tare\tseparated\tinto\ttwo\tgroups:\tvisible\tunits\tand\thidden\tunits\t(see Figure\tE-2).\tAll\tneurons\twork\tin\tthe\tsame\tstochastic\tway,\tbut\tthe\tvisible\tunits\tare\tthe\tones\tthat\treceive the\tinputs\tand\tfrom\twhich\toutputs\tare\tread.\n\nFigure\tE-2.\tBoltzmann\tmachine\n\nBecause\tof\tits\tstochastic\tnature,\ta\tBoltzmann\tmachine\twill\tnever\tstabilize\tinto\ta\tfixed\tconfiguration,\tbut instead\tit\twill\tkeep\tswitching\tbetween\tmany\tconfigurations.\tIf\tit\tis\tleft\trunning\tfor\ta\tsufficiently\tlong\ttime, the\tprobability\tof\tobserving\ta\tparticular\tconfiguration\twill\tonly\tbe\ta\tfunction\tof\tthe\tconnection\tweights and\tbias\tterms,\tnot\tof\tthe\toriginal\tconfiguration\t(similarly,\tafter\tyou\tshuffle\ta\tdeck\tof\tcards\tfor\tlong enough,\tthe\tconfiguration\tof\tthe\tdeck\tdoes\tnot\tdepend\ton\tthe\tinitial\tstate).\tWhen\tthe\tnetwork\treaches\tthis state\twhere\tthe\toriginal\tconfiguration\tis\t“forgotten,”\tit\tis\tsaid\tto\tbe\tin\tthermal\tequilibrium\t(although\tits configuration\tkeeps\tchanging\tall\tthe\ttime).\tBy\tsetting\tthe\tnetwork\tparameters\tappropriately,\tletting\tthe network\treach\tthermal\tequilibrium,\tand\tthen\tobserving\tits\tstate,\twe\tcan\tsimulate\ta\twide\trange\tof probability\tdistributions.\tThis\tis\tcalled\ta\tgenerative\tmodel.\n\nTraining\ta\tBoltzmann\tmachine\tmeans\tfinding\tthe\tparameters\tthat\twill\tmake\tthe\tnetwork\tapproximate\tthe training\tset’s\tprobability\tdistribution.\tFor\texample,\tif\tthere\tare\tthree\tvisible\tneurons\tand\tthe\ttraining\tset contains\t75%\t(0,\t1,\t1)\ttriplets,\t10%\t(0,\t0,\t1)\ttriplets,\tand\t15%\t(1,\t1,\t1)\ttriplets,\tthen\tafter\ttraining\ta Boltzmann\tmachine,\tyou\tcould\tuse\tit\tto\tgenerate\trandom\tbinary\ttriplets\twith\tabout\tthe\tsame\tprobability distribution.\tFor\texample,\tabout\t75%\tof\tthe\ttime\tit\twould\toutput\tthe\t(0,\t1,\t1)\ttriplet.\n\nSuch\ta\tgenerative\tmodel\tcan\tbe\tused\tin\ta\tvariety\tof\tways.\tFor\texample,\tif\tit\tis\ttrained\ton\timages,\tand\tyou provide\tan\tincomplete\tor\tnoisy\timage\tto\tthe\tnetwork,\tit\twill\tautomatically\t“repair”\tthe\timage\tin\ta reasonable\tway.\tYou\tcan\talso\tuse\ta\tgenerative\tmodel\tfor\tclassification.\tJust\tadd\ta\tfew\tvisible\tneurons\tto encode\tthe\ttraining\timage’s\tclass\t(e.g.,\tadd\t10\tvisible\tneurons\tand\tturn\ton\tonly\tthe\tfifth\tneuron\twhen\tthe training\timage\trepresents\ta\t5).\tThen,\twhen\tgiven\ta\tnew\timage,\tthe\tnetwork\twill\tautomatically\tturn\ton\tthe\n\nappropriate\tvisible\tneurons,\tindicating\tthe\timage’s\tclass\t(e.g.,\tit\twill\tturn\ton\tthe\tfifth\tvisible\tneuron\tif\tthe image\trepresents\ta\t5).\n\nUnfortunately,\tthere\tis\tno\tefficient\ttechnique\tto\ttrain\tBoltzmann\tmachines.\tHowever,\tfairly\tefficient algorithms\thave\tbeen\tdeveloped\tto\ttrain\trestricted\tBoltzmann\tmachines\t(RBM).\n\nRestricted\tBoltzmann\tMachines An\tRBM\tis\tsimply\ta\tBoltzmann\tmachine\tin\twhich\tthere\tare\tno\tconnections\tbetween\tvisible\tunits\tor between\thidden\tunits,\tonly\tbetween\tvisible\tand\thidden\tunits.\tFor\texample,\tFigure\tE-3\trepresents\tan\tRBM with\tthree\tvisible\tunits\tand\tfour\thidden\tunits.\n\nFigure\tE-3.\tRestricted\tBoltzmann\tmachine\n\nA\tvery\tefficient\ttraining\talgorithm,\tcalled\tContrastive\tDivergence,\twas\tintroduced\tin\t2005\tby\tMiguel\tÁ. Carreira-Perpiñán\tand\tGeoffrey\tHinton.1\tHere\tis\thow\tit\tworks:\tfor\teach\ttraining\tinstance\tx,\tthe\talgorithm starts\tby\tfeeding\tit\tto\tthe\tnetwork\tby\tsetting\tthe\tstate\tof\tthe\tvisible\tunits\tto\tx1,\tx2,\t,\t xn.\tThen\tyou\tcompute the\tstate\tof\tthe\thidden\tunits\tby\tapplying\tthe\tstochastic\tequation\tdescribed\tbefore\t(Equation\tE-1).\tThis gives\tyou\ta\thidden\tvector\th\t(where\thi\tis\tequal\tto\tthe\tstate\tof\tthe\tith\tunit).\tNext\tyou\tcompute\tthe\tstate\tof\tthe visible\tunits,\tby\tapplying\tthe\tsame\tstochastic\tequation.\tThis\tgives\tyou\ta\tvector\t compute\tthe\tstate\tof\tthe\thidden\tunits,\twhich\tgives\tyou\ta\tvector\t weight\tby\tapplying\tthe\trule\tin\tEquation\tE-2.\n\n.\tThen\tonce\tagain\tyou .\tNow\tyou\tcan\tupdate\teach\tconnection\n\nEquation\tE-2.\tContrastive\tdivergence\tweight\tupdate\n\nThe\tgreat\tbenefit\tof\tthis\talgorithm\tit\tthat\tit\tdoes\tnot\trequire\twaiting\tfor\tthe\tnetwork\tto\treach\tthermal equilibrium:\tit\tjust\tgoes\tforward,\tbackward,\tand\tforward\tagain,\tand\tthat’s\tit.\tThis\tmakes\tit\tincomparably more\tefficient\tthan\tprevious\talgorithms,\tand\tit\twas\ta\tkey\tingredient\tto\tthe\tfirst\tsuccess\tof\tDeep\tLearning based\ton\tmultiple\tstacked\tRBMs.\n\nDeep\tBelief\tNets Several\tlayers\tof\tRBMs\tcan\tbe\tstacked;\tthe\thidden\tunits\tof\tthe\tfirst-level\tRBM\tserves\tas\tthe\tvisible\tunits for\tthe\tsecond-layer\tRBM,\tand\tso\ton.\tSuch\tan\tRBM\tstack\tis\tcalled\ta\tdeep\tbelief\tnet\t(DBN).\n\nYee-Whye\tTeh,\tone\tof\tGeoffrey\tHinton’s\tstudents,\tobserved\tthat\tit\twas\tpossible\tto\ttrain\tDBNs\tone\tlayer at\ta\ttime\tusing\tContrastive\tDivergence,\tstarting\twith\tthe\tlower\tlayers\tand\tthen\tgradually\tmoving\tup\tto\tthe top\tlayers.\tThis\tled\tto\tthe\tgroundbreaking\tarticle\tthat\tkickstarted\tthe\tDeep\tLearning\ttsunami\tin\t2006.2\n\nJust\tlike\tRBMs,\tDBNs\tlearn\tto\treproduce\tthe\tprobability\tdistribution\tof\ttheir\tinputs,\twithout\tany supervision.\tHowever,\tthey\tare\tmuch\tbetter\tat\tit,\tfor\tthe\tsame\treason\tthat\tdeep\tneural\tnetworks\tare\tmore powerful\tthan\tshallow\tones:\treal-world\tdata\tis\toften\torganized\tin\thierarchical\tpatterns,\tand\tDBNs\ttake advantage\tof\tthat.\tTheir\tlower\tlayers\tlearn\tlow-level\tfeatures\tin\tthe\tinput\tdata,\twhile\thigher\tlayers\tlearn high-level\tfeatures.\n\nJust\tlike\tRBMs,\tDBNs\tare\tfundamentally\tunsupervised,\tbut\tyou\tcan\talso\ttrain\tthem\tin\ta\tsupervised manner\tby\tadding\tsome\tvisible\tunits\tto\trepresent\tthe\tlabels.\tMoreover,\tone\tgreat\tfeature\tof\tDBNs\tis\tthat they\tcan\tbe\ttrained\tin\ta\tsemisupervised\tfashion.\tFigure\tE-4\trepresents\tsuch\ta\tDBN\tconfigured\tfor semisupervised\tlearning.\n\nFigure\tE-4.\tA\tdeep\tbelief\tnetwork\tconfigured\tfor\tsemisupervised\tlearning\n\nFirst,\tthe\tRBM\t1\tis\ttrained\twithout\tsupervision.\tIt\tlearns\tlow-level\tfeatures\tin\tthe\ttraining\tdata.\tThen RBM\t2\tis\ttrained\twith\tRBM\t1’s\thidden\tunits\tas\tinputs,\tagain\twithout\tsupervision:\tit\tlearns\thigher-level features\t(note\tthat\tRBM\t2’s\thidden\tunits\tinclude\tonly\tthe\tthree\trightmost\tunits,\tnot\tthe\tlabel\tunits). Several\tmore\tRBMs\tcould\tbe\tstacked\tthis\tway,\tbut\tyou\tget\tthe\tidea.\tSo\tfar,\ttraining\twas\t100% unsupervised.\tLastly,\tRBM\t3\tis\ttrained\tusing\tboth\tRBM\t2’s\thidden\tunits\tas\tinputs,\tas\twell\tas\textra visible\tunits\tused\tto\trepresent\tthe\ttarget\tlabels\t(e.g.,\ta\tone-hot\tvector\trepresenting\tthe\tinstance\tclass).\tIt learns\tto\tassociate\thigh-level\tfeatures\twith\ttraining\tlabels.\tThis\tis\tthe\tsupervised\tstep.\n\nAt\tthe\tend\tof\ttraining,\tif\tyou\tfeed\tRBM\t1\ta\tnew\tinstance,\tthe\tsignal\twill\tpropagate\tup\tto\tRBM\t2,\tthen\tup to\tthe\ttop\tof\tRBM\t3,\tand\tthen\tback\tdown\tto\tthe\tlabel\tunits;\thopefully,\tthe\tappropriate\tlabel\twill\tlight\tup. This\tis\thow\ta\tDBN\tcan\tbe\tused\tfor\tclassification.\n\nOne\tgreat\tbenefit\tof\tthis\tsemisupervised\tapproach\tis\tthat\tyou\tdon’t\tneed\tmuch\tlabeled\ttraining\tdata.\tIf\tthe unsupervised\tRBMs\tdo\ta\tgood\tenough\tjob,\tthen\tonly\ta\tsmall\tamount\tof\tlabeled\ttraining\tinstances\tper class\twill\tbe\tnecessary.\tSimilarly,\ta\tbaby\tlearns\tto\trecognize\tobjects\twithout\tsupervision,\tso\twhen\tyou point\tto\ta\tchair\tand\tsay\t“chair,”\tthe\tbaby\tcan\tassociate\tthe\tword\t“chair”\twith\tthe\tclass\tof\tobjects\tit\thas already\tlearned\tto\trecognize\ton\tits\town.\tYou\tdon’t\tneed\tto\tpoint\tto\tevery\tsingle\tchair\tand\tsay\t“chair”; only\ta\tfew\texamples\twill\tsuffice\t(just\tenough\tso\tthe\tbaby\tcan\tbe\tsure\tthat\tyou\tare\tindeed\treferring\tto\tthe chair,\tnot\tto\tits\tcolor\tor\tone\tof\tthe\tchair’s\tparts).\n\nQuite\tamazingly,\tDBNs\tcan\talso\twork\tin\treverse.\tIf\tyou\tactivate\tone\tof\tthe\tlabel\tunits,\tthe\tsignal\twill propagate\tup\tto\tthe\thidden\tunits\tof\tRBM\t3,\tthen\tdown\tto\tRBM\t2,\tand\tthen\tRBM\t1,\tand\ta\tnew\tinstance will\tbe\toutput\tby\tthe\tvisible\tunits\tof\tRBM\t1.\tThis\tnew\tinstance\twill\tusually\tlook\tlike\ta\tregular\tinstance of\tthe\tclass\twhose\tlabel\tunit\tyou\tactivated.\tThis\tgenerative\tcapability\tof\tDBNs\tis\tquite\tpowerful.\tFor example,\tit\thas\tbeen\tused\tto\tautomatically\tgenerate\tcaptions\tfor\timages,\tand\tvice\tversa:\tfirst\ta\tDBN\tis trained\t(without\tsupervision)\tto\tlearn\tfeatures\tin\timages,\tand\tanother\tDBN\tis\ttrained\t(again\twithout supervision)\tto\tlearn\tfeatures\tin\tsets\tof\tcaptions\t(e.g.,\t“car”\toften\tcomes\twith\t“automobile”).\tThen\tan RBM\tis\tstacked\ton\ttop\tof\tboth\tDBNs\tand\ttrained\twith\ta\tset\tof\timages\talong\twith\ttheir\tcaptions;\tit\tlearns to\tassociate\thigh-level\tfeatures\tin\timages\twith\thigh-level\tfeatures\tin\tcaptions.\tNext,\tif\tyou\tfeed\tthe\timage DBN\tan\timage\tof\ta\tcar,\tthe\tsignal\twill\tpropagate\tthrough\tthe\tnetwork,\tup\tto\tthe\ttop-level\tRBM,\tand\tback down\tto\tthe\tbottom\tof\tthe\tcaption\tDBN,\tproducing\ta\tcaption.\tDue\tto\tthe\tstochastic\tnature\tof\tRBMs\tand DBNs,\tthe\tcaption\twill\tkeep\tchanging\trandomly,\tbut\tit\twill\tgenerally\tbe\tappropriate\tfor\tthe\timage.\tIf\tyou generate\ta\tfew\thundred\tcaptions,\tthe\tmost\tfrequently\tgenerated\tones\twill\tlikely\tbe\ta\tgood\tdescription\tof the\timage.3\n\nSelf-Organizing\tMaps Self-organizing\tmaps\t(SOM)\tare\tquite\tdifferent\tfrom\tall\tthe\tother\ttypes\tof\tneural\tnetworks\twe\thave discussed\tso\tfar.\tThey\tare\tused\tto\tproduce\ta\tlow-dimensional\trepresentation\tof\ta\thigh-dimensional dataset,\tgenerally\tfor\tvisualization,\tclustering,\tor\tclassification.\tThe\tneurons\tare\tspread\tacross\ta\tmap (typically\t2D\tfor\tvisualization,\tbut\tit\tcan\tbe\tany\tnumber\tof\tdimensions\tyou\twant),\tas\tshown\tin\tFigure\tE-5, and\teach\tneuron\thas\ta\tweighted\tconnection\tto\tevery\tinput\t(note\tthat\tthe\tdiagram\tshows\tjust\ttwo\tinputs,\tbut there\tare\ttypically\ta\tvery\tlarge\tnumber,\tsince\tthe\twhole\tpoint\tof\tSOMs\tis\tto\treduce\tdimensionality).\n\nFigure\tE-5.\tSelf-organizing\tmaps\n\nOnce\tthe\tnetwork\tis\ttrained,\tyou\tcan\tfeed\tit\ta\tnew\tinstance\tand\tthis\twill\tactivate\tonly\tone\tneuron\t(i.e., hence\tone\tpoint\ton\tthe\tmap):\tthe\tneuron\twhose\tweight\tvector\tis\tclosest\tto\tthe\tinput\tvector.\tIn\tgeneral, instances\tthat\tare\tnearby\tin\tthe\toriginal\tinput\tspace\twill\tactivate\tneurons\tthat\tare\tnearby\ton\tthe\tmap.\tThis makes\tSOMs\tuseful\tfor\tvisualization\t(in\tparticular,\tyou\tcan\teasily\tidentify\tclusters\ton\tthe\tmap),\tbut\talso for\tapplications\tlike\tspeech\trecognition.\tFor\texample,\tif\teach\tinstance\trepresents\tthe\taudio\trecording\tof\ta person\tpronouncing\ta\tvowel,\tthen\tdifferent\tpronunciations\tof\tthe\tvowel\t“a”\twill\tactivate\tneurons\tin\tthe same\tarea\tof\tthe\tmap,\twhile\tinstances\tof\tthe\tvowel\t“e”\twill\tactivate\tneurons\tin\tanother\tarea,\tand intermediate\tsounds\twill\tgenerally\tactivate\tintermediate\tneurons\ton\tthe\tmap.\n\nNOTE\n\nOne\timportant\tdifference\twith\tthe\tother\tdimensionality\treduction\ttechniques\tdiscussed\tin\tChapter\t8\tis\tthat\tall\tinstances\tget mapped\tto\ta\tdiscrete\tnumber\tof\tpoints\tin\tthe\tlow-dimensional\tspace\t(one\tpoint\tper\tneuron).\tWhen\tthere\tare\tvery\tfew\tneurons, this\ttechnique\tis\tbetter\tdescribed\tas\tclustering\trather\tthan\tdimensionality\treduction.\n\nThe\ttraining\talgorithm\tis\tunsupervised.\tIt\tworks\tby\thaving\tall\tthe\tneurons\tcompete\tagainst\teach\tother. First,\tall\tthe\tweights\tare\tinitialized\trandomly.\tThen\ta\ttraining\tinstance\tis\tpicked\trandomly\tand\tfed\tto\tthe network.\tAll\tneurons\tcompute\tthe\tdistance\tbetween\ttheir\tweight\tvector\tand\tthe\tinput\tvector\t(this\tis\tvery different\tfrom\tthe\tartificial\tneurons\twe\thave\tseen\tso\tfar).\tThe\tneuron\tthat\tmeasures\tthe\tsmallest\tdistance wins\tand\ttweaks\tits\tweight\tvector\tto\tbe\teven\tslightly\tcloser\tto\tthe\tinput\tvector,\tmaking\tit\tmore\tlikely\tto win\tfuture\tcompetitions\tfor\tother\tinputs\tsimilar\tto\tthis\tone.\tIt\talso\trecruits\tits\tneighboring\tneurons,\tand they\ttoo\tupdate\ttheir\tweight\tvector\tto\tbe\tslightly\tcloser\tto\tthe\tinput\tvector\t(but\tthey\tdon’t\tupdate\ttheir weights\tas\tmuch\tas\tthe\twinner\tneuron).\tThen\tthe\talgorithm\tpicks\tanother\ttraining\tinstance\tand\trepeats\tthe process,\tagain\tand\tagain.\tThis\talgorithm\ttends\tto\tmake\tnearby\tneurons\tgradually\tspecialize\tin\tsimilar inputs.4\n\n1\n\n“On\tContrastive\tDivergence\tLearning,”\tM.\tÁ.\tCarreira-Perpiñán\tand\tG.\tHinton\t(2005).\n\n2\n\n“A\tFast\tLearning\tAlgorithm\tfor\tDeep\tBelief\tNets,”\tG.\tHinton,\tS.\tOsindero,\tY.\tTeh\t(2006).\n\n3\n\nSee\tthis\tvideo\tby\tGeoffrey\tHinton\tfor\tmore\tdetails\tand\ta\tdemo:\thttp://goo.gl/7Z5QiS.\n\n4\n\nYou\tcan\timagine\ta\tclass\tof\tyoung\tchildren\twith\troughly\tsimilar\tskills.\tOne\tchild\thappens\tto\tbe\tslightly\tbetter\tat\tbasketball.\tThis\tmotivates her\tto\tpractice\tmore,\tespecially\twith\ther\tfriends.\tAfter\ta\twhile,\tthis\tgroup\tof\tfriends\tgets\tso\tgood\tat\tbasketball\tthat\tother\tkids\tcannot compete.\tBut\tthat’s\tokay,\tbecause\tthe\tother\tkids\tspecialize\tin\tother\ttopics.\tAfter\ta\twhile,\tthe\tclass\tis\tfull\tof\tlittle\tspecialized\tgroups.\n\nIndex\n\nSymbols\n\n__call__(),\tStatic\tUnrolling\tThrough\tTime\n\nε-greedy\tpolicy,\tExploration\tPolicies,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nε-insensitive,\tSVM\tRegression\n\nχ\t2\ttest\t(see\tchi\tsquare\ttest)\n\nℓ\t0\tnorm,\tSelect\ta\tPerformance\tMeasure\n\nℓ\t1\tand\tℓ\t2\tregularization,\tℓ1\tand\tℓ2\tRegularization-ℓ1\tand\tℓ2\tRegularization\n\nℓ\t1\tnorm,\tSelect\ta\tPerformance\tMeasure,\tLasso\tRegression,\tDecision\tBoundaries,\tAdam Optimization,\tAvoiding\tOverfitting\tThrough\tRegularization\n\nℓ\t2\tnorm,\tSelect\ta\tPerformance\tMeasure,\tRidge\tRegression-Lasso\tRegression,\tDecision Boundaries,\tSoftmax\tRegression,\tAvoiding\tOverfitting\tThrough\tRegularization,\tMax-Norm Regularization\n\nℓ\tk\tnorm,\tSelect\ta\tPerformance\tMeasure\n\nℓ\t∞\tnorm,\tSelect\ta\tPerformance\tMeasure\n\nA\n\naccuracy,\tWhat\tIs\tMachine\tLearning?,\tMeasuring\tAccuracy\tUsing\tCross-Validation-Measuring Accuracy\tUsing\tCross-Validation\n\nactions,\tevaluating,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem-Evaluating\tActions: The\tCredit\tAssignment\tProblem\n\nactivation\tfunctions,\tMulti-Layer\tPerceptron\tand\tBackpropagation-Multi-Layer\tPerceptron and\tBackpropagation\n\nactive\tconstraints,\tSVM\tDual\tProblem\n\nactors,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nactual\tclass,\tConfusion\tMatrix\n\nAdaBoost,\tAdaBoost-AdaBoost\n\nAdagrad,\tAdaGrad-AdaGrad\n\nAdam\toptimization,\tAdam\tOptimization-Adam\tOptimization,\tAdam\tOptimization\n\nadaptive\tlearning\trate,\tAdaGrad\n\nadaptive\tmoment\toptimization,\tAdam\tOptimization\n\nagents,\tLearning\tto\tOptimize\tRewards\n\nAlexNet\tarchitecture,\tAlexNet-AlexNet\n\nalgorithms\n\npreparing\tdata\tfor,\tPrepare\tthe\tData\tfor\tMachine\tLearning\tAlgorithms-Select\tand Train\ta\tModel\n\nAlphaGo,\tReinforcement\tLearning,\tIntroduction\tto\tArtificial\tNeural\tNetworks,\tReinforcement Learning,\tPolicy\tGradients\n\nAnaconda,\tCreate\tthe\tWorkspace\n\nanomaly\tdetection,\tUnsupervised\tlearning\n\nApple’s\tSiri,\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\napply_gradients(),\tGradient\tClipping,\tPolicy\tGradients\n\narea\tunder\tthe\tcurve\t(AUC),\tThe\tROC\tCurve\n\narray_split(),\tIncremental\tPCA\n\nartificial\tneural\tnetworks\t(ANNs),\tIntroduction\tto\tArtificial\tNeural\tNetworks-Exercises\n\nBoltzmann\tMachines,\tBoltzmann\tMachines-Boltzmann\tMachines\n\ndeep\tbelief\tnetworks\t(DBNs),\tDeep\tBelief\tNets-Deep\tBelief\tNets\n\nevolution\tof,\tFrom\tBiological\tto\tArtificial\tNeurons\n\nHopfield\tNetworks,\tHopfield\tNetworks-Hopfield\tNetworks\n\nhyperparameter\tfine-tuning,\tFine-Tuning\tNeural\tNetwork\tHyperparameters-Activation\n\nFunctions\n\noverview,\tIntroduction\tto\tArtificial\tNeural\tNetworks-From\tBiological\tto\tArtificial Neurons\n\nPerceptrons,\tThe\tPerceptron-Multi-Layer\tPerceptron\tand\tBackpropagation\n\nself-organizing\tmaps,\tSelf-Organizing\tMaps-Self-Organizing\tMaps\n\ntraining\ta\tDNN\twith\tTensorFlow,\tTraining\ta\tDNN\tUsing\tPlain\tTensorFlow-Using\tthe Neural\tNetwork\n\nartificial\tneuron,\tLogical\tComputations\twith\tNeurons\n\n(see\talso\tartificial\tneural\tnetwork\t(ANN))\n\nassign(),\tManually\tComputing\tthe\tGradients\n\nassociation\trule\tlearning,\tUnsupervised\tlearning\n\nassociative\tmemory\tnetworks,\tHopfield\tNetworks\n\nassumptions,\tchecking,\tCheck\tthe\tAssumptions\n\nasynchronous\tupdates,\tAsynchronous\tupdates-Asynchronous\tupdates\n\nasynchrous\tcommunication,\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues- PaddingFifoQueue\n\natrous_conv2d(),\tResNet\n\nattention\tmechanism,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nattributes,\tSupervised\tlearning,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure-Take\ta\tQuick\tLook at\tthe\tData\tStructure\n\n(see\talso\tdata\tstructure)\n\ncombinations\tof,\tExperimenting\twith\tAttribute\tCombinations-Experimenting\twith Attribute\tCombinations\n\npreprocessed,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\ntarget,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\nautodiff,\tUsing\tautodiff-Using\tautodiff,\tAutodiff-Reverse-Mode\tAutodiff\n\nforward-mode,\tForward-Mode\tAutodiff-Forward-Mode\tAutodiff\n\nmanual\tdifferentiation,\tManual\tDifferentiation\n\nnumerical\tdifferentiation,\tNumerical\tDifferentiation\n\nreverse-mode,\tReverse-Mode\tAutodiff-Reverse-Mode\tAutodiff\n\nsymbolic\tdifferentiation,\tSymbolic\tDifferentiation-Numerical\tDifferentiation\n\nautoencoders,\tAutoencoders-Exercises\n\nadversarial,\tOther\tAutoencoders\n\ncontractive,\tOther\tAutoencoders\n\ndenoising,\tDenoising\tAutoencoders-TensorFlow\tImplementation\n\nefficient\tdata\trepresentations,\tEfficient\tData\tRepresentations\n\ngenerative\tstochastic\tnetwork\t(GSN),\tOther\tAutoencoders\n\novercomplete,\tUnsupervised\tPretraining\tUsing\tStacked\tAutoencoders\n\nPCA\twith\tundercomplete\tlinear\tautoencoder,\tPerforming\tPCA\twith\tan\tUndercomplete Linear\tAutoencoder\n\nreconstructions,\tEfficient\tData\tRepresentations\n\nsparse,\tSparse\tAutoencoders-TensorFlow\tImplementation\n\nstacked,\tStacked\tAutoencoders-Unsupervised\tPretraining\tUsing\tStacked\tAutoencoders\n\nstacked\tconvolutional,\tOther\tAutoencoders\n\nundercomplete,\tEfficient\tData\tRepresentations\n\nvariational,\tVariational\tAutoencoders-Generating\tDigits\n\nvisualizing\tfeatures,\tVisualizing\tFeatures-Visualizing\tFeatures\n\nwinner-take-all\t(WTA),\tOther\tAutoencoders\n\nautomatic\tdifferentiating,\tUp\tand\tRunning\twith\tTensorFlow\n\nB\n\nautonomous\tdriving\tsystems,\tRecurrent\tNeural\tNetworks\n\nAverage\tAbsolute\tDeviation,\tSelect\ta\tPerformance\tMeasure\n\naverage\tpooling\tlayer,\tPooling\tLayer\n\navg_pool(),\tPooling\tLayer\n\nbackpropagation,\tMulti-Layer\tPerceptron\tand\tBackpropagation-Multi-Layer\tPerceptron\tand Backpropagation,\tVanishing/Exploding\tGradients\tProblems,\tUnsupervised\tPretraining, Visualizing\tFeatures\n\nbackpropagation\tthrough\ttime\t(BPTT),\tTraining\tRNNs\n\nbagging\tand\tpasting,\tBagging\tand\tPasting-Out-of-Bag\tEvaluation\n\nout-of-bag\tevaluation,\tOut-of-Bag\tEvaluation-Out-of-Bag\tEvaluation\n\nin\tScikit-Learn,\tBagging\tand\tPasting\tin\tScikit-Learn-Bagging\tand\tPasting\tin\tScikit- Learn\n\nbandwidth\tsaturation,\tBandwidth\tsaturation-Bandwidth\tsaturation\n\nBasicLSTMCell,\tLSTM\tCell\n\nBasicRNNCell,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs-Distributing\ta\tDeep\tRNN Across\tMultiple\tGPUs\n\nBatch\tGradient\tDescent,\tBatch\tGradient\tDescent-Batch\tGradient\tDescent,\tLasso\tRegression\n\nbatch\tlearning,\tBatch\tlearning-Batch\tlearning\n\nBatch\tNormalization,\tBatch\tNormalization-Implementing\tBatch\tNormalization\twith TensorFlow,\tResNet\n\noperation\tsummary,\tBatch\tNormalization\n\nwith\tTensorFlow,\tImplementing\tBatch\tNormalization\twith\tTensorFlow-Implementing Batch\tNormalization\twith\tTensorFlow\n\nbatch(),\tOther\tconvenience\tfunctions\n\nbatch_join(),\tOther\tconvenience\tfunctions\n\nC\n\nbatch_normalization(),\tImplementing\tBatch\tNormalization\twith\tTensorFlow-Implementing Batch\tNormalization\twith\tTensorFlow\n\nBellman\tOptimality\tEquation,\tMarkov\tDecision\tProcesses\n\nbetween-graph\treplication,\tIn-Graph\tVersus\tBetween-Graph\tReplication\n\nbias\tneurons,\tThe\tPerceptron\n\nbias\tterm,\tLinear\tRegression\n\nbias/variance\ttradeoff,\tLearning\tCurves\n\nbiases,\tConstruction\tPhase\n\nbinary\tclassifiers,\tTraining\ta\tBinary\tClassifier,\tLogistic\tRegression\n\nbiological\tneurons,\tFrom\tBiological\tto\tArtificial\tNeurons-Biological\tNeurons\n\nblack\tbox\tmodels,\tMaking\tPredictions\n\nblending,\tStacking-Exercises\n\nBoltzmann\tMachines,\tBoltzmann\tMachines-Boltzmann\tMachines\n\n(see\talso\trestricted\tBoltzman\tmachines\t(RBMs))\n\nboosting,\tBoosting-Gradient\tBoosting\n\nAdaBoost,\tAdaBoost-AdaBoost\n\nGradient\tBoosting,\tGradient\tBoosting-Gradient\tBoosting\n\nbootstrap\taggregation\t(see\tbagging)\n\nbootstrapping,\tGrid\tSearch,\tBagging\tand\tPasting,\tIntroduction\tto\tOpenAI\tGym,\tLearning\tto Play\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nbottleneck\tlayers,\tGoogLeNet\n\nbrew,\tStacking\n\nCaffe\tmodel\tzoo,\tModel\tZoos\n\nCART\t(Classification\tand\tRegression\tTree)\talgorithm,\tMaking\tPredictions-The\tCART\tTraining Algorithm,\tRegression\n\ncategorical\tattributes,\tHandling\tText\tand\tCategorical\tAttributes-Handling\tText\tand Categorical\tAttributes\n\ncell\twrapper,\tTraining\tto\tPredict\tTime\tSeries\n\nchi\tsquare\ttest,\tRegularization\tHyperparameters\n\nclassification\tversus\tregression,\tSupervised\tlearning,\tMultioutput\tClassification\n\nclassifiers\n\nbinary,\tTraining\ta\tBinary\tClassifier\n\nerror\tanalysis,\tError\tAnalysis-Error\tAnalysis\n\nevaluating,\tMulticlass\tClassification\n\nMNIST\tdataset,\tMNIST-MNIST\n\nmulticlass,\tMulticlass\tClassification-Multiclass\tClassification\n\nmultilabel,\tMultilabel\tClassification-Multilabel\tClassification\n\nmultioutput,\tMultioutput\tClassification-Multioutput\tClassification\n\nperformance\tmeasures,\tPerformance\tMeasures-The\tROC\tCurve\n\nprecision\tof,\tConfusion\tMatrix\n\nvoting,\tVoting\tClassifiers-Voting\tClassifiers\n\nclip_by_value(),\tGradient\tClipping\n\nclosed-form\tequation,\tTraining\tModels,\tRidge\tRegression,\tTraining\tand\tCost\tFunction\n\ncluster\tspecification,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\nclustering\talgorithms,\tUnsupervised\tlearning\n\nclusters,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\ncoding\tspace,\tVariational\tAutoencoders\n\ncodings,\tAutoencoders\n\ncomplementary\tslackness\tcondition,\tSVM\tDual\tProblem\n\ncomponents_,\tUsing\tScikit-Learn\n\ncomputational\tcomplexity,\tComputational\tComplexity,\tComputational\tComplexity, Computational\tComplexity\n\ncompute_gradients(),\tGradient\tClipping,\tPolicy\tGradients\n\nconcat(),\tGoogLeNet\n\nconfig.gpu_options,\tManaging\tthe\tGPU\tRAM\n\nConfigProto,\tManaging\tthe\tGPU\tRAM\n\nconfusion\tmatrix,\tConfusion\tMatrix-Confusion\tMatrix,\tError\tAnalysis-Error\tAnalysis\n\nconnectionism,\tThe\tPerceptron\n\nconstrained\toptimization,\tTraining\tObjective,\tSVM\tDual\tProblem\n\nContrastive\tDivergence,\tRestricted\tBoltzmann\tMachines\n\ncontrol\tdependencies,\tControl\tDependencies\n\nconv1d(),\tResNet\n\nconv2d_transpose(),\tResNet\n\nconv3d(),\tResNet\n\nconvergence\trate,\tBatch\tGradient\tDescent\n\nconvex\tfunction,\tGradient\tDescent\n\nconvolution\tkernels,\tFilters,\tCNN\tArchitectures,\tGoogLeNet\n\nconvolutional\tneural\tnetworks\t(CNNs),\tConvolutional\tNeural\tNetworks-Exercises\n\narchitectures,\tCNN\tArchitectures-ResNet\n\nAlexNet,\tAlexNet-AlexNet\n\nGoogleNet,\tGoogLeNet-GoogLeNet\n\nLeNet5,\tLeNet-5-LeNet-5\n\nResNet,\tResNet-ResNet\n\nconvolutional\tlayer,\tConvolutional\tLayer-Memory\tRequirements,\tGoogLeNet,\tResNet\n\nfeature\tmaps,\tStacking\tMultiple\tFeature\tMaps-TensorFlow\tImplementation\n\nfilters,\tFilters\n\nmemory\trequirement,\tMemory\tRequirements-Memory\tRequirements\n\nevolution\tof,\tThe\tArchitecture\tof\tthe\tVisual\tCortex\n\npooling\tlayer,\tPooling\tLayer-Pooling\tLayer\n\nTensorFlow\timplementation,\tTensorFlow\tImplementation-TensorFlow\tImplementation\n\nCoordinator\tclass,\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner- Multithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\ncorrelation\tcoefficient,\tLooking\tfor\tCorrelations-Looking\tfor\tCorrelations\n\ncorrelations,\tfinding,\tLooking\tfor\tCorrelations-Looking\tfor\tCorrelations\n\ncost\tfunction,\tModel-based\tlearning,\tSelect\ta\tPerformance\tMeasure\n\nin\tAdaBoost,\tAdaBoost\n\nin\tadagrad,\tAdaGrad\n\nin\tartificial\tneural\tnetworks,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI, Construction\tPhase-Construction\tPhase\n\nin\tautodiff,\tUsing\tautodiff\n\nin\tbatch\tnormalization,\tImplementing\tBatch\tNormalization\twith\tTensorFlow\n\ncross\tentropy,\tLeNet-5\n\ndeep\tQ-Learning,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nin\tElastic\tNet,\tElastic\tNet\n\nin\tGradient\tDescent,\tTraining\tModels,\tGradient\tDescent-Gradient\tDescent,\tBatch\n\nGradient\tDescent,\tBatch\tGradient\tDescent-Stochastic\tGradient\tDescent,\tGradient Boosting,\tVanishing/Exploding\tGradients\tProblems\n\nin\tLogistic\tRegression,\tTraining\tand\tCost\tFunction-Training\tand\tCost\tFunction\n\nin\tPG\talgorithms,\tPolicy\tGradients\n\nin\tvariational\tautoencoders,\tVariational\tAutoencoders\n\nin\tLasso\tRegression,\tLasso\tRegression-Lasso\tRegression\n\nin\tLinear\tRegression,\tThe\tNormal\tEquation,\tGradient\tDescent\n\nin\tMomentum\toptimization,\tMomentum\tOptimization-Nesterov\tAccelerated\tGradient\n\nin\tpretrained\tlayers\treuse,\tPretraining\ton\tan\tAuxiliary\tTask\n\nin\tridge\tregression,\tRidge\tRegression-Ridge\tRegression\n\nin\tRNNs,\tTraining\tRNNs,\tTraining\tto\tPredict\tTime\tSeries\n\nstale\tgradients\tand,\tAsynchronous\tupdates\n\ncreative\tsequences,\tCreative\tRNN\n\ncredit\tassignment\tproblem,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem-Evaluating Actions:\tThe\tCredit\tAssignment\tProblem\n\ncritics,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ncross\tentropy,\tSoftmax\tRegression-Softmax\tRegression,\tTraining\tan\tMLP\twith\tTensorFlow’s High-Level\tAPI,\tTensorFlow\tImplementation,\tPolicy\tGradients\n\ncross-validation,\tTesting\tand\tValidating,\tBetter\tEvaluation\tUsing\tCross-Validation-Better Evaluation\tUsing\tCross-Validation,\tMeasuring\tAccuracy\tUsing\tCross-Validation-Measuring Accuracy\tUsing\tCross-Validation\n\nCUDA\tlibrary,\tInstallation\n\ncuDNN\tlibrary,\tInstallation\n\ncurse\tof\tdimensionality,\tDimensionality\tReduction-The\tCurse\tof\tDimensionality\n\n(see\talso\tdimensionality\treduction)\n\nD\n\ncustom\ttransformers,\tCustom\tTransformers-Custom\tTransformers\n\ndata,\tTesting\tand\tValidating\n\n(see\talso\ttest\tdata;\ttraining\tdata)\n\ncreating\tworkspace\tfor,\tGet\tthe\tData-Download\tthe\tData\n\ndownloading,\tDownload\tthe\tData-Download\tthe\tData\n\nfinding\tcorrelations\tin,\tLooking\tfor\tCorrelations-Looking\tfor\tCorrelations\n\nmaking\tassumptions\tabout,\tTesting\tand\tValidating\n\npreparing\tfor\tMachine\tLearning\talgorithms,\tPrepare\tthe\tData\tfor\tMachine\tLearning Algorithms-Select\tand\tTrain\ta\tModel\n\ntest-set\tcreation,\tCreate\ta\tTest\tSet-Create\ta\tTest\tSet\n\nworking\twith\treal\tdata,\tWorking\twith\tReal\tData\n\ndata\taugmentation,\tData\tAugmentation-Data\tAugmentation\n\ndata\tcleaning,\tData\tCleaning-Handling\tText\tand\tCategorical\tAttributes\n\ndata\tmining,\tWhy\tUse\tMachine\tLearning?\n\ndata\tparallelism,\tData\tParallelism-TensorFlow\timplementation\n\nasynchronous\tupdates,\tAsynchronous\tupdates-Asynchronous\tupdates\n\nbandwidth\tsaturation,\tBandwidth\tsaturation-Bandwidth\tsaturation\n\nsynchronous\tupdates,\tSynchronous\tupdates\n\nTensorFlow\timplementation,\tTensorFlow\timplementation\n\ndata\tpipeline,\tFrame\tthe\tProblem\n\ndata\tsnooping\tbias,\tCreate\ta\tTest\tSet\n\ndata\tstructure,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure-Take\ta\tQuick\tLook\tat\tthe\tData Structure\n\ndata\tvisualization,\tVisualizing\tGeographical\tData-Visualizing\tGeographical\tData\n\nDataFrame,\tData\tCleaning\n\ndataquest,\tOther\tResources\n\ndecision\tboundaries,\tDecision\tBoundaries-Decision\tBoundaries,\tSoftmax\tRegression,\tMaking Predictions\n\ndecision\tfunction,\tPrecision/Recall\tTradeoff,\tDecision\tFunction\tand\tPredictions-Decision Function\tand\tPredictions\n\nDecision\tStumps,\tAdaBoost\n\ndecision\tthreshold,\tPrecision/Recall\tTradeoff\n\nDecision\tTrees,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet-Better\tEvaluation\tUsing\tCross- Validation,\tDecision\tTrees-Exercises,\tEnsemble\tLearning\tand\tRandom\tForests\n\nbinary\ttrees,\tMaking\tPredictions\n\nclass\tprobability\testimates,\tEstimating\tClass\tProbabilities\n\ncomputational\tcomplexity,\tComputational\tComplexity\n\ndecision\tboundaries,\tMaking\tPredictions\n\nGINI\timpurity,\tGini\tImpurity\tor\tEntropy?\n\ninstability\twith,\tInstability-Instability\n\nnumbers\tof\tchildren,\tMaking\tPredictions\n\npredictions,\tMaking\tPredictions-Estimating\tClass\tProbabilities\n\nRandom\tForests\t(see\tRandom\tForests)\n\nregression\ttasks,\tRegression-Regression\n\nregularization\thyperparameters,\tRegularization\tHyperparameters-Regularization Hyperparameters\n\ntraining\tand\tvisualizing,\tTraining\tand\tVisualizing\ta\tDecision\tTree-Making\tPredictions\n\ndecoder,\tEfficient\tData\tRepresentations\n\ndeconvolutional\tlayer,\tResNet\n\ndeep\tautoencoders\t(see\tstacked\tautoencoders)\n\ndeep\tbelief\tnetworks\t(DBNs),\tSemisupervised\tlearning,\tDeep\tBelief\tNets-Deep\tBelief\tNets\n\nDeep\tLearning,\tReinforcement\tLearning\n\n(see\talso\tReinforcement\tLearning;\tTensorFlow)\n\nabout,\tThe\tMachine\tLearning\tTsunami,\tRoadmap\n\nlibraries,\tUp\tand\tRunning\twith\tTensorFlow-Up\tand\tRunning\twith\tTensorFlow\n\ndeep\tneural\tnetworks\t(DNNs),\tMulti-Layer\tPerceptron\tand\tBackpropagation,\tTraining\tDeep Neural\tNets-Exercises\n\n(see\talso\tMulti-Layer\tPerceptrons\t(MLP))\n\nfaster\toptimizers\tfor,\tFaster\tOptimizers-Learning\tRate\tScheduling\n\nregularization,\tAvoiding\tOverfitting\tThrough\tRegularization-Data\tAugmentation\n\nreusing\tpretrained\tlayers,\tReusing\tPretrained\tLayers-Pretraining\ton\tan\tAuxiliary\tTask\n\ntraining\tguidelines\toverview,\tPractical\tGuidelines\n\ntraining\twith\tTensorFlow,\tTraining\ta\tDNN\tUsing\tPlain\tTensorFlow-Using\tthe\tNeural Network\n\ntraining\twith\tTF.Learn,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nunstable\tgradients,\tVanishing/Exploding\tGradients\tProblems\n\nvanishing\tand\texploding\tgradients,\tTraining\tDeep\tNeural\tNets-Gradient\tClipping\n\nDeep\tQ-Learning,\tApproximate\tQ-Learning-Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ- Learning\n\nMs.\tPac\tMan\texample,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning- Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ndeep\tQ-network,\tApproximate\tQ-Learning\n\ndeep\tRNNs,\tDeep\tRNNs-The\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps\n\napplying\tdropout,\tApplying\tDropout\n\ndistributing\tacross\tmultiple\tGPUs,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs\n\nlong\tsequence\tdifficulties,\tThe\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps\n\ntruncated\tbackpropagation\tthrough\ttime,\tThe\tDifficulty\tof\tTraining\tover\tMany\tTime Steps\n\nDeepMind,\tReinforcement\tLearning,\tIntroduction\tto\tArtificial\tNeural\tNetworks, Reinforcement\tLearning,\tApproximate\tQ-Learning\n\ndegrees\tof\tfreedom,\tOverfitting\tthe\tTraining\tData,\tLearning\tCurves\n\ndenoising\tautoencoders,\tDenoising\tAutoencoders-TensorFlow\tImplementation\n\ndense(),\tConstruction\tPhase,\tTying\tWeights\n\ndepth\tconcat\tlayer,\tGoogLeNet\n\ndepth\tradius,\tAlexNet\n\ndepthwise_conv2d(),\tResNet\n\ndequeue(),\tQueues\tof\ttuples\n\ndequeue_many(),\tQueues\tof\ttuples,\tPaddingFifoQueue\n\ndequeue_up_to(),\tClosing\ta\tqueue-PaddingFifoQueue\n\ndequeuing\tdata,\tDequeuing\tdata\n\ndescribe(),\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\ndevice\tblocks,\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\ndevice(),\tSimple\tplacement\n\ndimensionality\treduction,\tUnsupervised\tlearning,\tDimensionality\tReduction-Exercises, Autoencoders\n\napproaches\tto\n\nManifold\tLearning,\tManifold\tLearning\n\nprojection,\tProjection-Projection\n\nchoosing\tthe\tright\tnumber\tof\tdimensions,\tChoosing\tthe\tRight\tNumber\tof\tDimensions\n\ncurse\tof\tdimensionality,\tDimensionality\tReduction-The\tCurse\tof\tDimensionality\n\nand\tdata\tvisualization,\tDimensionality\tReduction\n\nIsomap,\tOther\tDimensionality\tReduction\tTechniques\n\nLLE\t(Locally\tLinear\tEmbedding),\tLLE-LLE\n\nMultidimensional\tScaling,\tOther\tDimensionality\tReduction\tTechniques-Other Dimensionality\tReduction\tTechniques\n\nPCA\t(Principal\tComponent\tAnalysis),\tPCA-Randomized\tPCA\n\nt-Distributed\tStochastic\tNeighbor\tEmbedding\t(t-SNE),\tOther\tDimensionality\tReduction Techniques\n\ndiscount\trate,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem\n\ndistributed\tcomputing,\tUp\tand\tRunning\twith\tTensorFlow\n\ndistributed\tsessions,\tSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers-Sharing\tState Across\tSessions\tUsing\tResource\tContainers\n\nDNNClassifier,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\ndrop(),\tPrepare\tthe\tData\tfor\tMachine\tLearning\tAlgorithms\n\ndropconnect,\tDropout\n\ndropna(),\tData\tCleaning\n\ndropout,\tNumber\tof\tNeurons\tper\tHidden\tLayer,\tApplying\tDropout\n\ndropout\trate,\tDropout\n\ndropout(),\tDropout\n\nDropoutWrapper,\tApplying\tDropout\n\nDRY\t(Don’t\tRepeat\tYourself),\tModularity\n\nE\n\nDual\tAveraging,\tAdam\tOptimization\n\ndual\tnumbers,\tForward-Mode\tAutodiff\n\ndual\tproblem,\tThe\tDual\tProblem\n\nduality,\tSVM\tDual\tProblem\n\ndying\tReLUs,\tNonsaturating\tActivation\tFunctions\n\ndynamic\tplacements,\tDynamic\tplacement\tfunction\n\ndynamic\tplacer,\tPlacing\tOperations\ton\tDevices\n\nDynamic\tProgramming,\tMarkov\tDecision\tProcesses\n\ndynamic\tunrolling\tthrough\ttime,\tDynamic\tUnrolling\tThrough\tTime\n\ndynamic_rnn(),\tDynamic\tUnrolling\tThrough\tTime,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple GPUs,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nearly\tstopping,\tEarly\tStopping-Early\tStopping,\tGradient\tBoosting,\tNumber\tof\tNeurons\tper Hidden\tLayer,\tEarly\tStopping\n\nElastic\tNet,\tElastic\tNet\n\nembedded\tdevice\tblocks,\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\nEmbedded\tReber\tgrammars,\tExercises\n\nembeddings,\tWord\tEmbeddings-Word\tEmbeddings\n\nembedding_lookup(),\tWord\tEmbeddings\n\nencoder,\tEfficient\tData\tRepresentations\n\nEncoder–Decoder,\tInput\tand\tOutput\tSequences\n\nend-of-sequence\t(EOS)\ttoken,\tHandling\tVariable-Length\tOutput\tSequences\n\nenergy\tfunctions,\tHopfield\tNetworks\n\nenqueuing\tdata,\tEnqueuing\tdata\n\nEnsemble\tLearning,\tBetter\tEvaluation\tUsing\tCross-Validation,\tEnsemble\tMethods,\tEnsemble Learning\tand\tRandom\tForests-Exercises\n\nbagging\tand\tpasting,\tBagging\tand\tPasting-Out-of-Bag\tEvaluation\n\nboosting,\tBoosting-Gradient\tBoosting\n\nin-graph\tversus\tbetween-graph\treplication,\tIn-Graph\tVersus\tBetween-Graph Replication-In-Graph\tVersus\tBetween-Graph\tReplication\n\nRandom\tForests,\tRandom\tForests-Feature\tImportance\n\n(see\talso\tRandom\tForests)\n\nrandom\tpatches\tand\trandom\tsubspaces,\tRandom\tPatches\tand\tRandom\tSubspaces\n\nstacking,\tStacking-Stacking\n\nentropy\timpurity\tmeasure,\tGini\tImpurity\tor\tEntropy?\n\nenvironments,\tin\treinforcement\tlearning,\tLearning\tto\tOptimize\tRewards-Evaluating\tActions: The\tCredit\tAssignment\tProblem,\tExploration\tPolicies,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing Deep\tQ-Learning\n\nepisodes\t(in\tRL),\tIntroduction\tto\tOpenAI\tGym,\tEvaluating\tActions:\tThe\tCredit\tAssignment Problem-Policy\tGradients,\tPolicy\tGradients-Policy\tGradients,\tLearning\tto\tPlay\tMs.\tPac-Man Using\tDeep\tQ-Learning\n\nepochs,\tStochastic\tGradient\tDescent\n\nε-insensitive,\tSVM\tRegression\n\nequality\tcontraints,\tSVM\tDual\tProblem\n\nerror\tanalysis,\tError\tAnalysis-Error\tAnalysis\n\nestimators,\tData\tCleaning\n\nEuclidian\tnorm,\tSelect\ta\tPerformance\tMeasure\n\neval(),\tFeeding\tData\tto\tthe\tTraining\tAlgorithm\n\nevaluating\tmodels,\tTesting\tand\tValidating-Testing\tand\tValidating\n\nexplained\tvariance,\tChoosing\tthe\tRight\tNumber\tof\tDimensions\n\nF\n\nexplained\tvariance\tratio,\tExplained\tVariance\tRatio\n\nexploding\tgradients,\tVanishing/Exploding\tGradients\tProblems\n\n(see\talso\tgradients,\tvanishing\tand\texploding)\n\nexploration\tpolicies,\tExploration\tPolicies\n\nexponential\tdecay,\tImplementing\tBatch\tNormalization\twith\tTensorFlow\n\nexponential\tlinear\tunit\t(ELU),\tNonsaturating\tActivation\tFunctions-Nonsaturating\tActivation Functions\n\nexponential\tscheduling,\tLearning\tRate\tScheduling\n\nExtra-Trees,\tExtra-Trees\n\nF-1\tscore,\tPrecision\tand\tRecall-Precision\tand\tRecall\n\nface-recognition,\tMultilabel\tClassification\n\nfake\tX\tserver,\tIntroduction\tto\tOpenAI\tGym\n\nfalse\tpositive\trate\t(FPR),\tThe\tROC\tCurve-The\tROC\tCurve\n\nfan-in,\tXavier\tand\tHe\tInitialization,\tXavier\tand\tHe\tInitialization\n\nfan-out,\tXavier\tand\tHe\tInitialization,\tXavier\tand\tHe\tInitialization\n\nfeature\tdetection,\tAutoencoders\n\nfeature\tengineering,\tIrrelevant\tFeatures\n\nfeature\textraction,\tUnsupervised\tlearning\n\nfeature\timportance,\tFeature\tImportance-Feature\tImportance\n\nfeature\tmaps,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters,\tFilters-TensorFlow Implementation,\tResNet\n\nfeature\tscaling,\tFeature\tScaling\n\nfeature\tselection,\tIrrelevant\tFeatures,\tGrid\tSearch,\tLasso\tRegression,\tFeature\tImportance,\n\nPrepare\tthe\tData\n\nfeature\tspace,\tKernel\tPCA,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nfeature\tvector,\tSelect\ta\tPerformance\tMeasure,\tLinear\tRegression,\tUnder\tthe\tHood, Implementing\tGradient\tDescent\n\nfeatures,\tSupervised\tlearning\n\nFeatureUnion,\tTransformation\tPipelines\n\nfeedforward\tneural\tnetwork\t(FNN),\tMulti-Layer\tPerceptron\tand\tBackpropagation\n\nfeed_dict,\tFeeding\tData\tto\tthe\tTraining\tAlgorithm\n\nFIFOQueue,\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues,\tRandomShuffleQueue\n\nfillna(),\tData\tCleaning\n\nfirst-in\tfirst-out\t(FIFO)\tqueues,\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues\n\nfirst-order\tpartial\tderivatives\t(Jacobians),\tAdam\tOptimization\n\nfit(),\tData\tCleaning,\tTransformation\tPipelines,\tIncremental\tPCA\n\nfitness\tfunction,\tModel-based\tlearning\n\nfit_inverse_transform=,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nfit_transform(),\tData\tCleaning,\tTransformation\tPipelines\n\nfolds,\tBetter\tEvaluation\tUsing\tCross-Validation,\tMNIST,\tMeasuring\tAccuracy\tUsing\tCross- Validation-Measuring\tAccuracy\tUsing\tCross-Validation\n\nFollow\tThe\tRegularized\tLeader\t(FTRL),\tAdam\tOptimization\n\nforget\tgate,\tLSTM\tCell\n\nforward-mode\tautodiff,\tForward-Mode\tAutodiff-Forward-Mode\tAutodiff\n\nframing\ta\tproblem,\tFrame\tthe\tProblem-Frame\tthe\tProblem\n\nfrozen\tlayers,\tFreezing\tthe\tLower\tLayers-Caching\tthe\tFrozen\tLayers\n\nG\n\nfunctools.partial(),\tImplementing\tBatch\tNormalization\twith\tTensorFlow,\tTensorFlow Implementation,\tVariational\tAutoencoders\n\ngame\tplay\t(see\treinforcement\tlearning)\n\ngamma\tvalue,\tGaussian\tRBF\tKernel\n\ngate\tcontrollers,\tLSTM\tCell\n\nGaussian\tdistribution,\tVariational\tAutoencoders,\tGenerating\tDigits\n\nGaussian\tRBF,\tAdding\tSimilarity\tFeatures\n\nGaussian\tRBF\tkernel,\tGaussian\tRBF\tKernel-Gaussian\tRBF\tKernel,\tKernelized\tSVM\n\ngeneralization\terror,\tTesting\tand\tValidating\n\ngeneralized\tLagrangian,\tSVM\tDual\tProblem-SVM\tDual\tProblem\n\ngenerative\tautoencoders,\tVariational\tAutoencoders\n\ngenerative\tmodels,\tAutoencoders,\tBoltzmann\tMachines\n\ngenetic\talgorithms,\tPolicy\tSearch\n\ngeodesic\tdistance,\tOther\tDimensionality\tReduction\tTechniques\n\nget_variable(),\tSharing\tVariables-Sharing\tVariables\n\nGINI\timpurity,\tMaking\tPredictions,\tGini\tImpurity\tor\tEntropy?\n\nglobal\taverage\tpooling,\tGoogLeNet\n\nglobal_step,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nglobal_variables_initializer(),\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\nGlorot\tinitialization,\tVanishing/Exploding\tGradients\tProblems-Xavier\tand\tHe\tInitialization\n\nGoogle,\tUp\tand\tRunning\twith\tTensorFlow\n\nGoogle\tImages,\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\nGoogle\tPhotos,\tSemisupervised\tlearning\n\nGoogleNet\tarchitecture,\tGoogLeNet-GoogLeNet\n\ngpu_options.per_process_gpu_memory_fraction,\tManaging\tthe\tGPU\tRAM\n\ngradient\tascent,\tPolicy\tSearch\n\nGradient\tBoosted\tRegression\tTrees\t(GBRT),\tGradient\tBoosting\n\nGradient\tBoosting,\tGradient\tBoosting-Gradient\tBoosting\n\nGradient\tDescent\t(GD),\tTraining\tModels,\tGradient\tDescent-Mini-batch\tGradient\tDescent, Online\tSVMs,\tTraining\tDeep\tNeural\tNets,\tMomentum\tOptimization,\tAdaGrad\n\nalgorithm\tcomparisons,\tMini-batch\tGradient\tDescent-Mini-batch\tGradient\tDescent\n\nautomatically\tcomputing\tgradients,\tUsing\tautodiff-Using\tautodiff\n\nBatch\tGD,\tBatch\tGradient\tDescent-Batch\tGradient\tDescent,\tLasso\tRegression\n\ndefining,\tGradient\tDescent\n\nlocal\tminimum\tversus\tglobal\tminimum,\tGradient\tDescent\n\nmanually\tcomputing\tgradients,\tManually\tComputing\tthe\tGradients\n\nMini-batch\tGD,\tMini-batch\tGradient\tDescent-Mini-batch\tGradient\tDescent,\tFeeding Data\tto\tthe\tTraining\tAlgorithm-Feeding\tData\tto\tthe\tTraining\tAlgorithm\n\noptimizer,\tUsing\tan\tOptimizer\n\nStochastic\tGD,\tStochastic\tGradient\tDescent-Stochastic\tGradient\tDescent,\tSoft\tMargin Classification\n\nwith\tTensorFlow,\tImplementing\tGradient\tDescent-Using\tan\tOptimizer\n\nGradient\tTree\tBoosting,\tGradient\tBoosting\n\nGradientDescentOptimizer,\tConstruction\tPhase\n\ngradients(),\tUsing\tautodiff\n\ngradients,\tvanishing\tand\texploding,\tTraining\tDeep\tNeural\tNets-Gradient\tClipping,\tThe Difficulty\tof\tTraining\tover\tMany\tTime\tSteps\n\nH\n\nBatch\tNormalization,\tBatch\tNormalization-Implementing\tBatch\tNormalization\twith TensorFlow\n\nGlorot\tand\tHe\tinitialization,\tVanishing/Exploding\tGradients\tProblems-Xavier\tand\tHe Initialization\n\ngradient\tclipping,\tGradient\tClipping\n\nnonsaturating\tactivation\tfunctions,\tNonsaturating\tActivation\tFunctions-Nonsaturating Activation\tFunctions\n\ngraphviz,\tTraining\tand\tVisualizing\ta\tDecision\tTree\n\ngreedy\talgorithm,\tThe\tCART\tTraining\tAlgorithm\n\ngrid\tsearch,\tFine-Tune\tYour\tModel-Grid\tSearch,\tPolynomial\tKernel\n\ngroup(),\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nGRU\t(Gated\tRecurrent\tUnit)\tcell,\tGRU\tCell-GRU\tCell\n\nhailstone\tsequence,\tEfficient\tData\tRepresentations\n\nhard\tmargin\tclassification,\tSoft\tMargin\tClassification-Soft\tMargin\tClassification\n\nhard\tvoting\tclassifiers,\tVoting\tClassifiers-Voting\tClassifiers\n\nharmonic\tmean,\tPrecision\tand\tRecall\n\nHe\tinitialization,\tVanishing/Exploding\tGradients\tProblems-Xavier\tand\tHe\tInitialization\n\nHeaviside\tstep\tfunction,\tThe\tPerceptron\n\nHebb's\trule,\tThe\tPerceptron,\tHopfield\tNetworks\n\nHebbian\tlearning,\tThe\tPerceptron\n\nhidden\tlayers,\tMulti-Layer\tPerceptron\tand\tBackpropagation\n\nhierarchical\tclustering,\tUnsupervised\tlearning\n\nhinge\tloss\tfunction,\tOnline\tSVMs\n\nI\n\nhistograms,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure-Take\ta\tQuick\tLook\tat\tthe\tData Structure\n\nhold-out\tsets,\tStacking\n\n(see\talso\tblenders)\n\nHopfield\tNetworks,\tHopfield\tNetworks-Hopfield\tNetworks\n\nhyperbolic\ttangent\t(htan\tactivation\tfunction),\tMulti-Layer\tPerceptron\tand\tBackpropagation, Activation\tFunctions,\tVanishing/Exploding\tGradients\tProblems,\tXavier\tand\tHe\tInitialization, Recurrent\tNeurons\n\nhyperparameters,\tOverfitting\tthe\tTraining\tData,\tCustom\tTransformers,\tGrid\tSearch-Grid Search,\tEvaluate\tYour\tSystem\ton\tthe\tTest\tSet,\tGradient\tDescent,\tPolynomial\tKernel, Computational\tComplexity,\tFine-Tuning\tNeural\tNetwork\tHyperparameters\n\n(see\talso\tneural\tnetwork\thyperparameters)\n\nhyperplane,\tDecision\tFunction\tand\tPredictions,\tManifold\tLearning-PCA,\tProjecting\tDown\tto\td Dimensions,\tOther\tDimensionality\tReduction\tTechniques\n\nhypothesis,\tSelect\ta\tPerformance\tMeasure\n\nmanifold,\tManifold\tLearning\n\nhypothesis\tboosting\t(see\tboosting)\n\nhypothesis\tfunction,\tLinear\tRegression\n\nhypothesis,\tnull,\tRegularization\tHyperparameters\n\nidentity\tmatrix,\tRidge\tRegression,\tQuadratic\tProgramming\n\nILSVRC\tImageNet\tchallenge,\tCNN\tArchitectures\n\nimage\tclassification,\tCNN\tArchitectures\n\nimpurity\tmeasures,\tMaking\tPredictions,\tGini\tImpurity\tor\tEntropy?\n\nin-graph\treplication,\tIn-Graph\tVersus\tBetween-Graph\tReplication\n\ninception\tmodules,\tGoogLeNet\n\nInception-v4,\tResNet\n\nincremental\tlearning,\tOnline\tlearning,\tIncremental\tPCA\n\ninequality\tconstraints,\tSVM\tDual\tProblem\n\ninference,\tModel-based\tlearning,\tExercises,\tMemory\tRequirements,\tAn\tEncoder–Decoder Network\tfor\tMachine\tTranslation\n\ninfo(),\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\ninformation\tgain,\tGini\tImpurity\tor\tEntropy?\n\ninformation\ttheory,\tGini\tImpurity\tor\tEntropy?\n\ninit\tnode,\tSaving\tand\tRestoring\tModels\n\ninput\tgate,\tLSTM\tCell\n\ninput\tneurons,\tThe\tPerceptron\n\ninput_keep_prob,\tApplying\tDropout\n\ninstance-based\tlearning,\tInstance-based\tlearning,\tModel-based\tlearning\n\nInteractiveSession,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\nintercept\tterm,\tLinear\tRegression\n\nInternal\tCovariate\tShift\tproblem,\tBatch\tNormalization\n\ninter_op_parallelism_threads,\tParallel\tExecution\n\nintra_op_parallelism_threads,\tParallel\tExecution\n\ninverse_transform(),\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nin_top_k(),\tConstruction\tPhase\n\nirreducible\terror,\tLearning\tCurves\n\nisolated\tenvironment,\tCreate\tthe\tWorkspace-Create\tthe\tWorkspace\n\nIsomap,\tOther\tDimensionality\tReduction\tTechniques\n\nJ\n\nK\n\nL\n\njobs,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\njoin(),\tMultiple\tDevices\tAcross\tMultiple\tServers,\tMultithreaded\treaders\tusing\ta\tCoordinator and\ta\tQueueRunner\n\nJupyter,\tCreate\tthe\tWorkspace,\tCreate\tthe\tWorkspace,\tTake\ta\tQuick\tLook\tat\tthe\tData Structure\n\nK-fold\tcross-validation,\tBetter\tEvaluation\tUsing\tCross-Validation-Better\tEvaluation\tUsing Cross-Validation,\tMeasuring\tAccuracy\tUsing\tCross-Validation\n\nk-Nearest\tNeighbors,\tModel-based\tlearning,\tMultilabel\tClassification\n\nKarush–Kuhn–Tucker\t(KKT)\tconditions,\tSVM\tDual\tProblem\n\nkeep\tprobability,\tDropout\n\nKeras,\tUp\tand\tRunning\twith\tTensorFlow\n\nKernel\tPCA\t(kPCA),\tKernel\tPCA-Selecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nkernel\ttrick,\tPolynomial\tKernel,\tGaussian\tRBF\tKernel,\tThe\tDual\tProblem-Kernelized\tSVM, Kernel\tPCA\n\nkernelized\tSVM,\tKernelized\tSVM-Kernelized\tSVM\n\nkernels,\tPolynomial\tKernel-Gaussian\tRBF\tKernel,\tOperations\tand\tkernels\n\nKullback–Leibler\tdivergence,\tSoftmax\tRegression,\tSparse\tAutoencoders\n\nl1_l2_regularizer(),\tℓ1\tand\tℓ2\tRegularization\n\nLabelBinarizer,\tTransformation\tPipelines\n\nlabels,\tSupervised\tlearning,\tFrame\tthe\tProblem\n\nLagrange\tfunction,\tSVM\tDual\tProblem-SVM\tDual\tProblem\n\nLagrange\tmultiplier,\tSVM\tDual\tProblem\n\nlandmarks,\tAdding\tSimilarity\tFeatures-Adding\tSimilarity\tFeatures\n\nlarge\tmargin\tclassification,\tLinear\tSVM\tClassification-Linear\tSVM\tClassification\n\nLasso\tRegression,\tLasso\tRegression-Lasso\tRegression\n\nlatent\tloss,\tVariational\tAutoencoders\n\nlatent\tspace,\tVariational\tAutoencoders\n\nlaw\tof\tlarge\tnumbers,\tVoting\tClassifiers\n\nleaky\tReLU,\tNonsaturating\tActivation\tFunctions\n\nlearning\trate,\tOnline\tlearning,\tGradient\tDescent,\tBatch\tGradient\tDescent-Stochastic\tGradient Descent\n\nlearning\trate\tscheduling,\tStochastic\tGradient\tDescent,\tLearning\tRate\tScheduling-Learning Rate\tScheduling\n\nLeNet-5\tarchitecture,\tThe\tArchitecture\tof\tthe\tVisual\tCortex,\tLeNet-5-LeNet-5\n\nLevenshtein\tdistance,\tGaussian\tRBF\tKernel\n\nliblinear\tlibrary,\tComputational\tComplexity\n\nlibsvm\tlibrary,\tComputational\tComplexity\n\nLinear\tDiscriminant\tAnalysis\t(LDA),\tOther\tDimensionality\tReduction\tTechniques\n\nlinear\tmodels\n\nearly\tstopping,\tEarly\tStopping-Early\tStopping\n\nElastic\tNet,\tElastic\tNet\n\nLasso\tRegression,\tLasso\tRegression-Lasso\tRegression\n\nLinear\tRegression\t(see\tLinear\tRegression)\n\nregression\t(see\tLinear\tRegression)\n\nRidge\tRegression,\tRidge\tRegression-Ridge\tRegression,\tElastic\tNet\n\nSVM,\tLinear\tSVM\tClassification-Soft\tMargin\tClassification\n\nLinear\tRegression,\tModel-based\tlearning,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet, Training\tModels-Mini-batch\tGradient\tDescent,\tElastic\tNet\n\ncomputational\tcomplexity,\tComputational\tComplexity\n\nGradient\tDescent\tin,\tGradient\tDescent-Mini-batch\tGradient\tDescent\n\nlearning\tcurves\tin,\tLearning\tCurves-Learning\tCurves\n\nNormal\tEquation,\tThe\tNormal\tEquation-Computational\tComplexity\n\nregularizing\tmodels\t(see\tregularization)\n\nusing\tStochastic\tGradient\tDescent\t(SGD),\tStochastic\tGradient\tDescent\n\nwith\tTensorFlow,\tLinear\tRegression\twith\tTensorFlow-Linear\tRegression\twith TensorFlow\n\nlinear\tSVM\tclassification,\tLinear\tSVM\tClassification-Soft\tMargin\tClassification\n\nlinear\tthreshold\tunits\t(LTUs),\tThe\tPerceptron\n\nLipschitz\tcontinuous,\tGradient\tDescent\n\nLLE\t(Locally\tLinear\tEmbedding),\tLLE-LLE\n\nload_sample_images(),\tTensorFlow\tImplementation\n\nlocal\treceptive\tfield,\tThe\tArchitecture\tof\tthe\tVisual\tCortex\n\nlocal\tresponse\tnormalization,\tAlexNet\n\nlocal\tsessions,\tSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers\n\nlocation\tinvariance,\tPooling\tLayer\n\nlog\tloss,\tTraining\tand\tCost\tFunction\n\nlogging\tplacements,\tLogging\tplacements-Logging\tplacements\n\nlogistic\tfunction,\tEstimating\tProbabilities\n\nLogistic\tRegression,\tSupervised\tlearning,\tLogistic\tRegression-Softmax\tRegression\n\ndecision\tboundaries,\tDecision\tBoundaries-Decision\tBoundaries\n\nestimating\tprobablities,\tEstimating\tProbabilities-Estimating\tProbabilities\n\nSoftmax\tRegression\tmodel,\tSoftmax\tRegression-Softmax\tRegression\n\ntraining\tand\tcost\tfunction,\tTraining\tand\tCost\tFunction-Training\tand\tCost\tFunction\n\nlog_device_placement,\tLogging\tplacements\n\nLSTM\t(Long\tShort-Term\tMemory)\tcell,\tLSTM\tCell-GRU\tCell\n\nM\n\nmachine\tcontrol\t(see\treinforcement\tlearning)\n\nMachine\tLearning\n\nlarge-scale\tprojects\t(see\tTensorFlow)\n\nnotations,\tSelect\ta\tPerformance\tMeasure-Select\ta\tPerformance\tMeasure\n\nprocess\texample,\tEnd-to-End\tMachine\tLearning\tProject-Exercises\n\nproject\tchecklist,\tLook\tat\tthe\tBig\tPicture,\tMachine\tLearning\tProject\tChecklist- Launch!\n\nresources\ton,\tOther\tResources-Other\tResources\n\nuses\tfor,\tMachine\tLearning\tin\tYour\tProjects-Machine\tLearning\tin\tYour\tProjects\n\nMachine\tLearning\tbasics\n\nattributes,\tSupervised\tlearning\n\nchallenges,\tMain\tChallenges\tof\tMachine\tLearning-Stepping\tBack\n\nalgorithm\tproblems,\tOverfitting\tthe\tTraining\tData-Underfitting\tthe\tTraining Data\n\ntraining\tdata\tproblems,\tPoor-Quality\tData\n\ndefinition,\tWhat\tIs\tMachine\tLearning?\n\nfeatures,\tSupervised\tlearning\n\noverview,\tThe\tMachine\tLearning\tLandscape\n\nreasons\tfor\tusing,\tWhy\tUse\tMachine\tLearning?-Why\tUse\tMachine\tLearning?\n\nspam\tfilter\texample,\tWhat\tIs\tMachine\tLearning?-Why\tUse\tMachine\tLearning?\n\nsummary,\tStepping\tBack\n\ntesting\tand\tvalidating,\tTesting\tand\tValidating-Testing\tand\tValidating\n\ntypes\tof\tsystems,\tTypes\tof\tMachine\tLearning\tSystems-Model-based\tlearning\n\nbatch\tand\tonline\tlearning,\tBatch\tand\tOnline\tLearning-Online\tlearning\n\ninstance-based\tversus\tmodel-based\tlearning,\tInstance-Based\tVersus\tModel- Based\tLearning-Model-based\tlearning\n\nsupervised/unsupervised\tlearning,\tSupervised/Unsupervised\tLearning- Reinforcement\tLearning\n\nworkflow\texample,\tModel-based\tlearning-Model-based\tlearning\n\nmachine\ttranslation\t(see\tnatural\tlanguage\tprocessing\t(NLP))\n\nmake(),\tIntroduction\tto\tOpenAI\tGym\n\nManhattan\tnorm,\tSelect\ta\tPerformance\tMeasure\n\nmanifold\tassumption/hypothesis,\tManifold\tLearning\n\nManifold\tLearning,\tManifold\tLearning,\tLLE\n\n(see\talso\tLLE\t(Locally\tLinear\tEmbedding)\n\nMapReduce,\tFrame\tthe\tProblem\n\nmargin\tviolations,\tSoft\tMargin\tClassification\n\nMarkov\tchains,\tMarkov\tDecision\tProcesses\n\nMarkov\tdecision\tprocesses,\tMarkov\tDecision\tProcesses-Markov\tDecision\tProcesses\n\nmaster\tservice,\tThe\tMaster\tand\tWorker\tServices\n\nMatplotlib,\tCreate\tthe\tWorkspace,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure,\tThe\tROC Curve,\tError\tAnalysis\n\nmax\tmargin\tlearning,\tPretraining\ton\tan\tAuxiliary\tTask\n\nmax\tpooling\tlayer,\tPooling\tLayer\n\nmax-norm\tregularization,\tMax-Norm\tRegularization-Max-Norm\tRegularization\n\nmax_norm(),\tMax-Norm\tRegularization\n\nmax_norm_regularizer(),\tMax-Norm\tRegularization\n\nmax_pool(),\tPooling\tLayer\n\nMean\tAbsolute\tError\t(MAE),\tSelect\ta\tPerformance\tMeasure-Select\ta\tPerformance\tMeasure\n\nmean\tcoding,\tVariational\tAutoencoders\n\nMean\tSquare\tError\t(MSE),\tLinear\tRegression,\tManually\tComputing\tthe\tGradients,\tSparse Autoencoders\n\nmeasure\tof\tsimilarity,\tInstance-based\tlearning\n\nmemmap,\tIncremental\tPCA\n\nmemory\tcells,\tModel\tParallelism,\tMemory\tCells\n\nMercer's\ttheorem,\tKernelized\tSVM\n\nmeta\tlearner\t(see\tblending)\n\nmin-max\tscaling,\tFeature\tScaling\n\nMini-batch\tGradient\tDescent,\tMini-batch\tGradient\tDescent-Mini-batch\tGradient\tDescent, Training\tand\tCost\tFunction,\tFeeding\tData\tto\tthe\tTraining\tAlgorithm-Feeding\tData\tto\tthe Training\tAlgorithm\n\nmini-batches,\tOnline\tlearning\n\nminimize(),\tGradient\tClipping,\tFreezing\tthe\tLower\tLayers,\tPolicy\tGradients,\tLearning\tto\tPlay Ms.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nmin_after_dequeue,\tRandomShuffleQueue\n\nMNIST\tdataset,\tMNIST-MNIST\n\nmodel\tparallelism,\tModel\tParallelism-Model\tParallelism\n\nmodel\tparameters,\tGradient\tDescent,\tBatch\tGradient\tDescent,\tEarly\tStopping,\tUnder\tthe Hood,\tQuadratic\tProgramming,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession, Construction\tPhase,\tTraining\tRNNs\n\ndefining,\tModel-based\tlearning\n\nmodel\tselection,\tModel-based\tlearning\n\nmodel\tzoos,\tModel\tZoos\n\nN\n\nmodel-based\tlearning,\tModel-based\tlearning-Model-based\tlearning\n\nmodels\n\nanalyzing,\tAnalyze\tthe\tBest\tModels\tand\tTheir\tErrors-Analyze\tthe\tBest\tModels\tand Their\tErrors\n\nevaluating\ton\ttest\tset,\tEvaluate\tYour\tSystem\ton\tthe\tTest\tSet-Evaluate\tYour\tSystem\ton the\tTest\tSet\n\nmoments,\tAdam\tOptimization\n\nMomentum\toptimization,\tMomentum\tOptimization-Momentum\tOptimization\n\nMonte\tCarlo\ttree\tsearch,\tPolicy\tGradients\n\nMulti-Layer\tPerceptrons\t(MLP),\tIntroduction\tto\tArtificial\tNeural\tNetworks,\tThe\tPerceptron- Multi-Layer\tPerceptron\tand\tBackpropagation,\tNeural\tNetwork\tPolicies\n\ntraining\twith\tTF.Learn,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nmulticlass\tclassifiers,\tMulticlass\tClassification-Multiclass\tClassification\n\nMultidimensional\tScaling\t(MDS),\tOther\tDimensionality\tReduction\tTechniques\n\nmultilabel\tclassifiers,\tMultilabel\tClassification-Multilabel\tClassification\n\nMultinomial\tLogistic\tRegression\t(see\tSoftmax\tRegression)\n\nmultinomial(),\tNeural\tNetwork\tPolicies\n\nmultioutput\tclassifiers,\tMultioutput\tClassification-Multioutput\tClassification\n\nMultiRNNCell,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs\n\nmultithreaded\treaders,\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner- Multithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\nmultivariate\tregression,\tFrame\tthe\tProblem\n\nnaive\tBayes\tclassifiers,\tMulticlass\tClassification\n\nname\tscopes,\tName\tScopes\n\nnatural\tlanguage\tprocessing\t(NLP),\tRecurrent\tNeural\tNetworks,\tNatural\tLanguage Processing-An\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nencoder-decoder\tnetwork\tfor\tmachine\ttranslation,\tAn\tEncoder–Decoder\tNetwork\tfor Machine\tTranslation-An\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nTensorFlow\ttutorials,\tNatural\tLanguage\tProcessing,\tAn\tEncoder–Decoder\tNetwork\tfor Machine\tTranslation\n\nword\tembeddings,\tWord\tEmbeddings-Word\tEmbeddings\n\nNesterov\tAccelerated\tGradient\t(NAG),\tNesterov\tAccelerated\tGradient-Nesterov\tAccelerated Gradient\n\nNesterov\tmomentum\toptimization,\tNesterov\tAccelerated\tGradient-Nesterov\tAccelerated Gradient\n\nnetwork\ttopology,\tFine-Tuning\tNeural\tNetwork\tHyperparameters\n\nneural\tnetwork\thyperparameters,\tFine-Tuning\tNeural\tNetwork\tHyperparameters-Activation Functions\n\nactivation\tfunctions,\tActivation\tFunctions\n\nneurons\tper\thidden\tlayer,\tNumber\tof\tNeurons\tper\tHidden\tLayer\n\nnumber\tof\thidden\tlayers,\tNumber\tof\tHidden\tLayers-Number\tof\tHidden\tLayers\n\nneural\tnetwork\tpolicies,\tNeural\tNetwork\tPolicies-Neural\tNetwork\tPolicies\n\nneurons\n\nbiological,\tFrom\tBiological\tto\tArtificial\tNeurons-Biological\tNeurons\n\nlogical\tcomputations\twith,\tLogical\tComputations\twith\tNeurons\n\nneuron_layer(),\tConstruction\tPhase\n\nnext_batch(),\tExecution\tPhase\n\nNo\tFree\tLunch\ttheorem,\tTesting\tand\tValidating\n\nnode\tedges,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\nnonlinear\tdimensionality\treduction\t(NLDR),\tLLE\n\n(see\talso\tKernel\tPCA;\tLLE\t(Locally\tLinear\tEmbedding))\n\nnonlinear\tSVM\tclassification,\tNonlinear\tSVM\tClassification-Computational\tComplexity\n\ncomputational\tcomplexity,\tComputational\tComplexity\n\nGaussian\tRBF\tkernel,\tGaussian\tRBF\tKernel-Gaussian\tRBF\tKernel\n\nwith\tpolynomial\tfeatures,\tNonlinear\tSVM\tClassification-Polynomial\tKernel\n\npolynomial\tkernel,\tPolynomial\tKernel-Polynomial\tKernel\n\nsimilarity\tfeatures,\tadding,\tAdding\tSimilarity\tFeatures-Adding\tSimilarity\tFeatures\n\nnonparametric\tmodels,\tRegularization\tHyperparameters\n\nnonresponse\tbias,\tNonrepresentative\tTraining\tData\n\nnonsaturating\tactivation\tfunctions,\tNonsaturating\tActivation\tFunctions-Nonsaturating Activation\tFunctions\n\nNormal\tEquation,\tThe\tNormal\tEquation-Computational\tComplexity\n\nnormalization,\tFeature\tScaling\n\nnormalized\texponential,\tSoftmax\tRegression\n\nnorms,\tSelect\ta\tPerformance\tMeasure\n\nnotations,\tSelect\ta\tPerformance\tMeasure-Select\ta\tPerformance\tMeasure\n\nNP-Complete\tproblems,\tThe\tCART\tTraining\tAlgorithm\n\nnull\thypothesis,\tRegularization\tHyperparameters\n\nnumerical\tdifferentiation,\tNumerical\tDifferentiation\n\nNumPy,\tCreate\tthe\tWorkspace\n\nNumPy\tarrays,\tHandling\tText\tand\tCategorical\tAttributes\n\nNVidia\tCompute\tCapability,\tInstallation\n\nnvidia-smi,\tManaging\tthe\tGPU\tRAM\n\nO\n\nn_components,\tChoosing\tthe\tRight\tNumber\tof\tDimensions\n\nobservation\tspace,\tNeural\tNetwork\tPolicies\n\noff-policy\talgorithm,\tTemporal\tDifference\tLearning\tand\tQ-Learning\n\noffline\tlearning,\tBatch\tlearning\n\none-hot\tencoding,\tHandling\tText\tand\tCategorical\tAttributes\n\none-versus-all\t(OvA)\tstrategy,\tMulticlass\tClassification,\tSoftmax\tRegression,\tExercises\n\none-versus-one\t(OvO)\tstrategy,\tMulticlass\tClassification\n\nonline\tlearning,\tOnline\tlearning-Online\tlearning\n\nonline\tSVMs,\tOnline\tSVMs-Online\tSVMs\n\nOpenAI\tGym,\tIntroduction\tto\tOpenAI\tGym-Introduction\tto\tOpenAI\tGym\n\noperation_timeout_in_ms,\tIn-Graph\tVersus\tBetween-Graph\tReplication\n\nOptical\tCharacter\tRecognition\t(OCR),\tThe\tMachine\tLearning\tLandscape\n\noptimal\tstate\tvalue,\tMarkov\tDecision\tProcesses\n\noptimizers,\tFaster\tOptimizers-Learning\tRate\tScheduling\n\nAdaGrad,\tAdaGrad-AdaGrad\n\nAdam\toptimization,\tAdam\tOptimization-Adam\tOptimization,\tAdam\tOptimization\n\nGradient\tDescent\t(see\tGradient\tDescent\toptimizer)\n\nlearning\trate\tscheduling,\tLearning\tRate\tScheduling-Learning\tRate\tScheduling\n\nMomentum\toptimization,\tMomentum\tOptimization-Momentum\tOptimization\n\nNesterov\tAccelerated\tGradient\t(NAG),\tNesterov\tAccelerated\tGradient-Nesterov Accelerated\tGradient\n\nRMSProp,\tRMSProp\n\nout-of-bag\tevaluation,\tOut-of-Bag\tEvaluation-Out-of-Bag\tEvaluation\n\nP\n\nout-of-core\tlearning,\tOnline\tlearning\n\nout-of-memory\t(OOM)\terrors,\tStatic\tUnrolling\tThrough\tTime\n\nout-of-sample\terror,\tTesting\tand\tValidating\n\nOutOfRangeError,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded\treaders using\ta\tCoordinator\tand\ta\tQueueRunner\n\noutput\tgate,\tLSTM\tCell\n\noutput\tlayer,\tMulti-Layer\tPerceptron\tand\tBackpropagation\n\nOutputProjectionWrapper,\tTraining\tto\tPredict\tTime\tSeries-Training\tto\tPredict\tTime\tSeries\n\noutput_keep_prob,\tApplying\tDropout\n\novercomplete\tautoencoder,\tUnsupervised\tPretraining\tUsing\tStacked\tAutoencoders\n\noverfitting,\tOverfitting\tthe\tTraining\tData-Overfitting\tthe\tTraining\tData,\tCreate\ta\tTest\tSet, Soft\tMargin\tClassification,\tGaussian\tRBF\tKernel,\tRegularization\tHyperparameters, Regression,\tNumber\tof\tNeurons\tper\tHidden\tLayer\n\navoiding\tthrough\tregularization,\tAvoiding\tOverfitting\tThrough\tRegularization-Data Augmentation\n\np-value,\tRegularization\tHyperparameters\n\nPaddingFIFOQueue,\tPaddingFifoQueue\n\nPandas,\tCreate\tthe\tWorkspace,\tDownload\tthe\tData\n\nscatter_matrix,\tLooking\tfor\tCorrelations-Looking\tfor\tCorrelations\n\nparallel\tdistributed\tcomputing,\tDistributing\tTensorFlow\tAcross\tDevices\tand\tServers-Exercises\n\ndata\tparallelism,\tData\tParallelism-TensorFlow\timplementation\n\nin-graph\tversus\tbetween-graph\treplication,\tIn-Graph\tVersus\tBetween-Graph Replication-Model\tParallelism\n\nmodel\tparallelism,\tModel\tParallelism-Model\tParallelism\n\nmultiple\tdevices\tacross\tmultiple\tservers,\tMultiple\tDevices\tAcross\tMultiple\tServers-\n\nOther\tconvenience\tfunctions\n\nasynchronous\tcommunication\tusing\tqueues,\tAsynchronous\tCommunication\tUsing TensorFlow\tQueues-PaddingFifoQueue\n\nloading\ttraining\tdata,\tLoading\tData\tDirectly\tfrom\tthe\tGraph-Other\tconvenience functions\n\nmaster\tand\tworker\tservices,\tThe\tMaster\tand\tWorker\tServices\n\nopening\ta\tsession,\tOpening\ta\tSession\n\npinning\toperations\tacross\ttasks,\tPinning\tOperations\tAcross\tTasks\n\nsharding\tvariables,\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\nsharing\tstate\tacross\tsessions,\tSharing\tState\tAcross\tSessions\tUsing\tResource Containers-Sharing\tState\tAcross\tSessions\tUsing\tResource\tContainers\n\nmultiple\tdevices\ton\ta\tsingle\tmachine,\tMultiple\tDevices\ton\ta\tSingle\tMachine-Control Dependencies\n\ncontrol\tdependencies,\tControl\tDependencies\n\ninstallation,\tInstallation-Installation\n\nmanaging\tthe\tGPU\tRAM,\tManaging\tthe\tGPU\tRAM-Managing\tthe\tGPU\tRAM\n\nparallel\texecution,\tParallel\tExecution-Parallel\tExecution\n\nplacing\toperations\ton\tdevices,\tPlacing\tOperations\ton\tDevices-Soft\tplacement\n\none\tneural\tnetwork\tper\tdevice,\tOne\tNeural\tNetwork\tper\tDevice-One\tNeural\tNetwork per\tDevice\n\nparameter\tefficiency,\tNumber\tof\tHidden\tLayers\n\nparameter\tmatrix,\tSoftmax\tRegression\n\nparameter\tserver\t(ps),\tMultiple\tDevices\tAcross\tMultiple\tServers\n\nparameter\tspace,\tGradient\tDescent\n\nparameter\tvector,\tLinear\tRegression,\tGradient\tDescent,\tTraining\tand\tCost\tFunction,\tSoftmax Regression\n\nparametric\tmodels,\tRegularization\tHyperparameters\n\npartial\tderivative,\tBatch\tGradient\tDescent\n\npartial_fit(),\tIncremental\tPCA\n\nPearson's\tr,\tLooking\tfor\tCorrelations\n\npeephole\tconnections,\tPeephole\tConnections\n\npenalties\t(see\trewards,\tin\tRL)\n\npercentiles,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\nPerceptron\tconvergence\ttheorem,\tThe\tPerceptron\n\nPerceptrons,\tThe\tPerceptron-Multi-Layer\tPerceptron\tand\tBackpropagation\n\nversus\tLogistic\tRegression,\tThe\tPerceptron\n\ntraining,\tThe\tPerceptron-The\tPerceptron\n\nperformance\tmeasures,\tSelect\ta\tPerformance\tMeasure-Select\ta\tPerformance\tMeasure\n\nconfusion\tmatrix,\tConfusion\tMatrix-Confusion\tMatrix\n\ncross-validation,\tMeasuring\tAccuracy\tUsing\tCross-Validation-Measuring\tAccuracy Using\tCross-Validation\n\nprecision\tand\trecall,\tPrecision\tand\tRecall-Precision/Recall\tTradeoff\n\nROC\t(receiver\toperating\tcharacteristic)\tcurve,\tThe\tROC\tCurve-The\tROC\tCurve\n\nperformance\tscheduling,\tLearning\tRate\tScheduling\n\npermutation(),\tCreate\ta\tTest\tSet\n\nPG\talgorithms,\tPolicy\tGradients\n\nphoto-hosting\tservices,\tSemisupervised\tlearning\n\npinning\toperations,\tPinning\tOperations\tAcross\tTasks\n\npip,\tCreate\tthe\tWorkspace\n\nPipeline\tconstructor,\tTransformation\tPipelines-Select\tand\tTrain\ta\tModel\n\npipelines,\tFrame\tthe\tProblem\n\nplaceholder\tnodes,\tFeeding\tData\tto\tthe\tTraining\tAlgorithm\n\nplacers\t(see\tsimple\tplacer;\tdynamic\tplacer)\n\npolicy,\tPolicy\tSearch\n\npolicy\tgradients,\tPolicy\tSearch\t(see\tPG\talgorithms)\n\npolicy\tspace,\tPolicy\tSearch\n\npolynomial\tfeatures,\tadding,\tNonlinear\tSVM\tClassification-Polynomial\tKernel\n\npolynomial\tkernel,\tPolynomial\tKernel-Polynomial\tKernel,\tKernelized\tSVM\n\nPolynomial\tRegression,\tTraining\tModels,\tPolynomial\tRegression-Polynomial\tRegression\n\nlearning\tcurves\tin,\tLearning\tCurves-Learning\tCurves\n\npooling\tkernel,\tPooling\tLayer\n\npooling\tlayer,\tPooling\tLayer-Pooling\tLayer\n\npower\tscheduling,\tLearning\tRate\tScheduling\n\nprecision,\tConfusion\tMatrix\n\nprecision\tand\trecall,\tPrecision\tand\tRecall-Precision/Recall\tTradeoff\n\nF-1\tscore,\tPrecision\tand\tRecall-Precision\tand\tRecall\n\nprecision/recall\t(PR)\tcurve,\tThe\tROC\tCurve\n\nprecision/recall\ttradeoff,\tPrecision/Recall\tTradeoff-Precision/Recall\tTradeoff\n\npredetermined\tpiecewise\tconstant\tlearning\trate,\tLearning\tRate\tScheduling\n\npredict(),\tData\tCleaning\n\npredicted\tclass,\tConfusion\tMatrix\n\npredictions,\tConfusion\tMatrix-Confusion\tMatrix,\tDecision\tFunction\tand\tPredictions-Decision Function\tand\tPredictions,\tMaking\tPredictions-Estimating\tClass\tProbabilities\n\npredictors,\tSupervised\tlearning,\tData\tCleaning\n\npreloading\ttraining\tdata,\tPreload\tthe\tdata\tinto\ta\tvariable\n\nPReLU\t(parametric\tleaky\tReLU),\tNonsaturating\tActivation\tFunctions\n\npreprocessed\tattributes,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\npretrained\tlayers\treuse,\tReusing\tPretrained\tLayers-Pretraining\ton\tan\tAuxiliary\tTask\n\nauxiliary\ttask,\tPretraining\ton\tan\tAuxiliary\tTask-Pretraining\ton\tan\tAuxiliary\tTask\n\ncaching\tfrozen\tlayers,\tCaching\tthe\tFrozen\tLayers\n\nfreezing\tlower\tlayers,\tFreezing\tthe\tLower\tLayers\n\nmodel\tzoos,\tModel\tZoos\n\nother\tframeworks,\tReusing\tModels\tfrom\tOther\tFrameworks\n\nTensorFlow\tmodel,\tReusing\ta\tTensorFlow\tModel-Reusing\ta\tTensorFlow\tModel\n\nunsupervised\tpretraining,\tUnsupervised\tPretraining-Unsupervised\tPretraining\n\nupper\tlayers,\tTweaking,\tDropping,\tor\tReplacing\tthe\tUpper\tLayers\n\nPretty\tTensor,\tUp\tand\tRunning\twith\tTensorFlow\n\nprimal\tproblem,\tThe\tDual\tProblem\n\nprincipal\tcomponent,\tPrincipal\tComponents\n\nPrincipal\tComponent\tAnalysis\t(PCA),\tPCA-Randomized\tPCA\n\nexplained\tvariance\tratios,\tExplained\tVariance\tRatio\n\nfinding\tprincipal\tcomponents,\tPrincipal\tComponents-Principal\tComponents\n\nfor\tcompression,\tPCA\tfor\tCompression-Incremental\tPCA\n\nIncremental\tPCA,\tIncremental\tPCA-Randomized\tPCA\n\nKernel\tPCA\t(kPCA),\tKernel\tPCA-Selecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nprojecting\tdown\tto\td\tdimensions,\tProjecting\tDown\tto\td\tDimensions\n\nQ\n\nRandomized\tPCA,\tRandomized\tPCA\n\nScikit\tLearn\tfor,\tUsing\tScikit-Learn\n\nvariance,\tpreserving,\tPreserving\tthe\tVariance-Preserving\tthe\tVariance\n\nprobabilistic\tautoencoders,\tVariational\tAutoencoders\n\nprobabilities,\testimating,\tEstimating\tProbabilities-Estimating\tProbabilities,\tEstimating\tClass Probabilities\n\nproducer\tfunctions,\tOther\tconvenience\tfunctions\n\nprojection,\tProjection-Projection\n\npropositional\tlogic,\tFrom\tBiological\tto\tArtificial\tNeurons\n\npruning,\tRegularization\tHyperparameters,\tSymbolic\tDifferentiation\n\nPython\n\nisolated\tenvironment\tin,\tCreate\tthe\tWorkspace-Create\tthe\tWorkspace\n\nnotebooks\tin,\tCreate\tthe\tWorkspace-Download\tthe\tData\n\npickle,\tBetter\tEvaluation\tUsing\tCross-Validation\n\npip,\tCreate\tthe\tWorkspace\n\nQ-Learning\talgorithm,\tTemporal\tDifference\tLearning\tand\tQ-Learning-Learning\tto\tPlay\tMs. Pac-Man\tUsing\tDeep\tQ-Learning\n\napproximate\tQ-Learning,\tApproximate\tQ-Learning\n\ndeep\tQ-Learning,\tApproximate\tQ-Learning-Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep Q-Learning\n\nQ-Value\tIteration\tAlgorithm,\tMarkov\tDecision\tProcesses\n\nQ-Values,\tMarkov\tDecision\tProcesses\n\nQuadratic\tProgramming\t(QP)\tProblems,\tQuadratic\tProgramming-Quadratic\tProgramming\n\nquantizing,\tBandwidth\tsaturation\n\nR\n\nqueries\tper\tsecond\t(QPS),\tOne\tNeural\tNetwork\tper\tDevice\n\nQueueRunner,\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner-Multithreaded readers\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\nqueues,\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues-PaddingFifoQueue\n\nclosing,\tClosing\ta\tqueue\n\ndequeuing\tdata,\tDequeuing\tdata\n\nenqueuing\tdata,\tEnqueuing\tdata\n\nfirst-in\tfirst-out\t(FIFO),\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues\n\nof\ttuples,\tQueues\tof\ttuples\n\nPaddingFIFOQueue,\tPaddingFifoQueue\n\nRandomShuffleQueue,\tRandomShuffleQueue\n\nq_network(),\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nRadial\tBasis\tFunction\t(RBF),\tAdding\tSimilarity\tFeatures\n\nRandom\tForests,\tBetter\tEvaluation\tUsing\tCross-Validation-Grid\tSearch,\tMulticlass Classification,\tDecision\tTrees,\tInstability,\tEnsemble\tLearning\tand\tRandom\tForests,\tRandom Forests-Feature\tImportance\n\nExtra-Trees,\tExtra-Trees\n\nfeature\timportance,\tFeature\tImportance-Feature\tImportance\n\nrandom\tinitialization,\tGradient\tDescent,\tBatch\tGradient\tDescent,\tStochastic\tGradient Descent,\tVanishing/Exploding\tGradients\tProblems\n\nRandom\tPatches\tand\tRandom\tSubspaces,\tRandom\tPatches\tand\tRandom\tSubspaces\n\nrandomized\tleaky\tReLU\t(RReLU),\tNonsaturating\tActivation\tFunctions\n\nRandomized\tPCA,\tRandomized\tPCA\n\nrandomized\tsearch,\tRandomized\tSearch,\tFine-Tuning\tNeural\tNetwork\tHyperparameters\n\nRandomShuffleQueue,\tRandomShuffleQueue,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe graph\n\nrandom_uniform(),\tManually\tComputing\tthe\tGradients\n\nreader\toperations,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph\n\nrecall,\tConfusion\tMatrix\n\nrecognition\tnetwork,\tEfficient\tData\tRepresentations\n\nreconstruction\terror,\tPCA\tfor\tCompression\n\nreconstruction\tloss,\tEfficient\tData\tRepresentations,\tTensorFlow\tImplementation,\tVariational Autoencoders\n\nreconstruction\tpre-image,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nreconstructions,\tEfficient\tData\tRepresentations\n\nrecurrent\tneural\tnetworks\t(RNNs),\tRecurrent\tNeural\tNetworks-Exercises\n\ndeep\tRNNs,\tDeep\tRNNs-The\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps\n\nexploration\tpolicies,\tExploration\tPolicies\n\nGRU\tcell,\tGRU\tCell-GRU\tCell\n\ninput\tand\toutput\tsequences,\tInput\tand\tOutput\tSequences-Input\tand\tOutput\tSequences\n\nLSTM\tcell,\tLSTM\tCell-GRU\tCell\n\nnatural\tlanguage\tprocessing\t(NLP),\tNatural\tLanguage\tProcessing-An\tEncoder–Decoder Network\tfor\tMachine\tTranslation\n\nin\tTensorFlow,\tBasic\tRNNs\tin\tTensorFlow-Handling\tVariable-Length\tOutput\tSequences\n\ndynamic\tunrolling\tthrough\ttime,\tDynamic\tUnrolling\tThrough\tTime\n\nstatic\tunrolling\tthrough\ttime,\tStatic\tUnrolling\tThrough\tTime-Static\tUnrolling Through\tTime\n\nvariable\tlength\tinput\tsequences,\tHandling\tVariable\tLength\tInput\tSequences\n\nvariable\tlength\toutput\tsequences,\tHandling\tVariable-Length\tOutput\tSequences\n\ntraining,\tTraining\tRNNs-Creative\tRNN\n\nbackpropagation\tthrough\ttime\t(BPTT),\tTraining\tRNNs\n\ncreative\tsequences,\tCreative\tRNN\n\nsequence\tclassifiers,\tTraining\ta\tSequence\tClassifier-Training\ta\tSequence Classifier\n\ntime\tseries\tpredictions,\tTraining\tto\tPredict\tTime\tSeries-Training\tto\tPredict\tTime Series\n\nrecurrent\tneurons,\tRecurrent\tNeurons-Input\tand\tOutput\tSequences\n\nmemory\tcells,\tMemory\tCells\n\nreduce_mean(),\tConstruction\tPhase\n\nreduce_sum(),\tTensorFlow\tImplementation-TensorFlow\tImplementation,\tVariational Autoencoders,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nregression,\tSupervised\tlearning\n\nDecision\tTrees,\tRegression-Regression\n\nregression\tmodels\n\nlinear,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet\n\nregression\tversus\tclassification,\tMultioutput\tClassification\n\nregularization,\tOverfitting\tthe\tTraining\tData-Overfitting\tthe\tTraining\tData,\tTesting\tand Validating,\tRegularized\tLinear\tModels-Early\tStopping\n\ndata\taugmentation,\tData\tAugmentation-Data\tAugmentation\n\nDecision\tTrees,\tRegularization\tHyperparameters-Regularization\tHyperparameters\n\ndropout,\tDropout-Dropout\n\nearly\tstopping,\tEarly\tStopping-Early\tStopping,\tEarly\tStopping\n\nElastic\tNet,\tElastic\tNet\n\nLasso\tRegression,\tLasso\tRegression-Lasso\tRegression\n\nmax-norm,\tMax-Norm\tRegularization-Max-Norm\tRegularization\n\nRidge\tRegression,\tRidge\tRegression-Ridge\tRegression\n\nshrinkage,\tGradient\tBoosting\n\nℓ\t1\tand\tℓ\t2\tregularization,\tℓ1\tand\tℓ2\tRegularization-ℓ1\tand\tℓ2\tRegularization\n\nREINFORCE\talgorithms,\tPolicy\tGradients\n\nReinforcement\tLearning\t(RL),\tReinforcement\tLearning-Reinforcement\tLearning, Reinforcement\tLearning-Thank\tYou!\n\nactions,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem-Evaluating\tActions:\tThe Credit\tAssignment\tProblem\n\ncredit\tassignment\tproblem,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem- Evaluating\tActions:\tThe\tCredit\tAssignment\tProblem\n\ndiscount\trate,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem\n\nexamples\tof,\tLearning\tto\tOptimize\tRewards\n\nMarkov\tdecision\tprocesses,\tMarkov\tDecision\tProcesses-Markov\tDecision\tProcesses\n\nneural\tnetwork\tpolicies,\tNeural\tNetwork\tPolicies-Neural\tNetwork\tPolicies\n\nOpenAI\tgym,\tIntroduction\tto\tOpenAI\tGym-Introduction\tto\tOpenAI\tGym\n\nPG\talgorithms,\tPolicy\tGradients-Policy\tGradients\n\npolicy\tsearch,\tPolicy\tSearch-Policy\tSearch\n\nQ-Learning\talgorithm,\tTemporal\tDifference\tLearning\tand\tQ-Learning-Learning\tto\tPlay Ms.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nrewards,\tlearning\tto\toptimize,\tLearning\tto\tOptimize\tRewards-Learning\tto\tOptimize Rewards\n\nTemporal\tDifference\t(TD)\tLearning,\tTemporal\tDifference\tLearning\tand\tQ-Learning- Temporal\tDifference\tLearning\tand\tQ-Learning\n\nReLU\t(rectified\tlinear\tunits),\tModularity-Modularity\n\nReLU\tactivation,\tResNet\n\nReLU\tfunction,\tMulti-Layer\tPerceptron\tand\tBackpropagation,\tActivation\tFunctions,\tXavier and\tHe\tInitialization-Nonsaturating\tActivation\tFunctions\n\nrelu(z),\tConstruction\tPhase\n\nrender(),\tIntroduction\tto\tOpenAI\tGym\n\nreplay\tmemory,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nreplica_device_setter(),\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\nrequest_stop(),\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\nreset(),\tIntroduction\tto\tOpenAI\tGym\n\nreset_default_graph(),\tManaging\tGraphs\n\nreshape(),\tTraining\tto\tPredict\tTime\tSeries\n\nresidual\terrors,\tGradient\tBoosting-Gradient\tBoosting\n\nresidual\tlearning,\tResNet\n\nresidual\tnetwork\t(ResNet),\tModel\tZoos,\tResNet-ResNet\n\nresidual\tunits,\tResNet\n\nResNet,\tResNet-ResNet\n\nresource\tcontainers,\tSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers-Sharing\tState Across\tSessions\tUsing\tResource\tContainers\n\nrestore(),\tSaving\tand\tRestoring\tModels\n\nrestricted\tBoltzmann\tmachines\t(RBMs),\tSemisupervised\tlearning,\tUnsupervised\tPretraining, Boltzmann\tMachines\n\nreuse_variables(),\tSharing\tVariables\n\nreverse-mode\tautodiff,\tReverse-Mode\tAutodiff-Reverse-Mode\tAutodiff\n\nrewards,\tin\tRL,\tLearning\tto\tOptimize\tRewards-Learning\tto\tOptimize\tRewards\n\nrgb_array,\tIntroduction\tto\tOpenAI\tGym\n\nS\n\nRidge\tRegression,\tRidge\tRegression-Ridge\tRegression,\tElastic\tNet\n\nRMSProp,\tRMSProp\n\nROC\t(receiver\toperating\tcharacteristic)\tcurve,\tThe\tROC\tCurve-The\tROC\tCurve\n\nRoot\tMean\tSquare\tError\t(RMSE),\tSelect\ta\tPerformance\tMeasure-Select\ta\tPerformance Measure,\tLinear\tRegression\n\nRReLU\t(randomized\tleaky\tReLU),\tNonsaturating\tActivation\tFunctions\n\nrun(),\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession,\tIn-Graph\tVersus\tBetween-Graph Replication\n\nSampled\tSoftmax,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nsampling\tbias,\tNonrepresentative\tTraining\tData-Poor-Quality\tData,\tCreate\ta\tTest\tSet\n\nsampling\tnoise,\tNonrepresentative\tTraining\tData\n\nsave(),\tSaving\tand\tRestoring\tModels\n\nSaver\tnode,\tSaving\tand\tRestoring\tModels\n\nScikit\tFlow,\tUp\tand\tRunning\twith\tTensorFlow\n\nScikit-Learn,\tCreate\tthe\tWorkspace\n\nabout,\tObjective\tand\tApproach\n\nbagging\tand\tpasting\tin,\tBagging\tand\tPasting\tin\tScikit-Learn-Bagging\tand\tPasting\tin Scikit-Learn\n\nCART\talgorithm,\tMaking\tPredictions-The\tCART\tTraining\tAlgorithm,\tRegression\n\ncross-validation,\tBetter\tEvaluation\tUsing\tCross-Validation-Better\tEvaluation\tUsing Cross-Validation\n\ndesign\tprinciples,\tData\tCleaning-Data\tCleaning\n\nimputer,\tData\tCleaning-Handling\tText\tand\tCategorical\tAttributes\n\nLinearSVR\tclass,\tSVM\tRegression\n\nMinMaxScaler,\tFeature\tScaling\n\nmin_\tand\tmax_\thyperparameters,\tRegularization\tHyperparameters\n\nPCA\timplementation,\tUsing\tScikit-Learn\n\nPerceptron\tclass,\tThe\tPerceptron\n\nPipeline\tconstructor,\tTransformation\tPipelines-Select\tand\tTrain\ta\tModel,\tNonlinear SVM\tClassification\n\nRandomized\tPCA,\tRandomized\tPCA\n\nRidge\tRegression\twith,\tRidge\tRegression\n\nSAMME,\tAdaBoost\n\nSGDClassifier,\tTraining\ta\tBinary\tClassifier,\tPrecision/Recall\tTradeoff-Precision/Recall Tradeoff,\tMulticlass\tClassification\n\nSGDRegressor,\tStochastic\tGradient\tDescent\n\nsklearn.base.BaseEstimator,\tCustom\tTransformers,\tTransformation\tPipelines, Measuring\tAccuracy\tUsing\tCross-Validation\n\nsklearn.base.clone(),\tMeasuring\tAccuracy\tUsing\tCross-Validation,\tEarly\tStopping\n\nsklearn.base.TransformerMixin,\tCustom\tTransformers,\tTransformation\tPipelines\n\nsklearn.datasets.fetch_california_housing(),\tLinear\tRegression\twith\tTensorFlow\n\nsklearn.datasets.fetch_mldata(),\tMNIST\n\nsklearn.datasets.load_iris(),\tDecision\tBoundaries,\tSoft\tMargin\tClassification,\tTraining and\tVisualizing\ta\tDecision\tTree,\tFeature\tImportance,\tThe\tPerceptron\n\nsklearn.datasets.load_sample_images(),\tTensorFlow\tImplementation-TensorFlow Implementation\n\nsklearn.datasets.make_moons(),\tNonlinear\tSVM\tClassification,\tExercises\n\nsklearn.decomposition.IncrementalPCA,\tIncremental\tPCA\n\nsklearn.decomposition.KernelPCA,\tKernel\tPCA-Selecting\ta\tKernel\tand\tTuning\n\nHyperparameters,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nsklearn.decomposition.PCA,\tUsing\tScikit-Learn\n\nsklearn.ensemble.AdaBoostClassifier,\tAdaBoost\n\nsklearn.ensemble.BaggingClassifier,\tBagging\tand\tPasting\tin\tScikit-Learn-Random Forests\n\nsklearn.ensemble.GradientBoostingRegressor,\tGradient\tBoosting,\tGradient\tBoosting- Gradient\tBoosting\n\nsklearn.ensemble.RandomForestClassifier,\tThe\tROC\tCurve,\tMulticlass\tClassification, Voting\tClassifiers\n\nsklearn.ensemble.RandomForestRegressor,\tBetter\tEvaluation\tUsing\tCross-Validation, Grid\tSearch-Analyze\tthe\tBest\tModels\tand\tTheir\tErrors,\tRandom\tForests-Extra-Trees, Gradient\tBoosting\n\nsklearn.ensemble.VotingClassifier,\tVoting\tClassifiers\n\nsklearn.externals.joblib,\tBetter\tEvaluation\tUsing\tCross-Validation\n\nsklearn.linear_model.ElasticNet,\tElastic\tNet\n\nsklearn.linear_model.Lasso,\tLasso\tRegression\n\nsklearn.linear_model.LinearRegression,\tModel-based\tlearning-Model-based\tlearning, Data\tCleaning,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet,\tThe\tNormal\tEquation, Mini-batch\tGradient\tDescent,\tPolynomial\tRegression,\tLearning\tCurves-Learning Curves\n\nsklearn.linear_model.LogisticRegression,\tDecision\tBoundaries,\tDecision\tBoundaries, Softmax\tRegression,\tVoting\tClassifiers,\tSelecting\ta\tKernel\tand\tTuning Hyperparameters\n\nsklearn.linear_model.Perceptron,\tThe\tPerceptron\n\nsklearn.linear_model.Ridge,\tRidge\tRegression\n\nsklearn.linear_model.SGDClassifier,\tTraining\ta\tBinary\tClassifier\n\nsklearn.linear_model.SGDRegressor,\tStochastic\tGradient\tDescent-Mini-batch\tGradient\n\nDescent,\tRidge\tRegression,\tLasso\tRegression-Early\tStopping\n\nsklearn.manifold.LocallyLinearEmbedding,\tLLE-LLE\n\nsklearn.metrics.accuracy_score(),\tVoting\tClassifiers,\tOut-of-Bag\tEvaluation,\tTraining an\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nsklearn.metrics.confusion_matrix(),\tConfusion\tMatrix,\tError\tAnalysis\n\nsklearn.metrics.f1_score(),\tPrecision\tand\tRecall,\tMultilabel\tClassification\n\nsklearn.metrics.mean_squared_error(),\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet- Training\tand\tEvaluating\ton\tthe\tTraining\tSet,\tEvaluate\tYour\tSystem\ton\tthe\tTest\tSet, Learning\tCurves,\tEarly\tStopping,\tGradient\tBoosting-Gradient\tBoosting,\tSelecting\ta Kernel\tand\tTuning\tHyperparameters\n\nsklearn.metrics.precision_recall_curve(),\tPrecision/Recall\tTradeoff\n\nsklearn.metrics.precision_score(),\tPrecision\tand\tRecall,\tPrecision/Recall\tTradeoff\n\nsklearn.metrics.recall_score(),\tPrecision\tand\tRecall,\tPrecision/Recall\tTradeoff\n\nsklearn.metrics.roc_auc_score(),\tThe\tROC\tCurve-The\tROC\tCurve\n\nsklearn.metrics.roc_curve(),\tThe\tROC\tCurve-The\tROC\tCurve\n\nsklearn.model_selection.cross_val_predict(),\tConfusion\tMatrix,\tPrecision/Recall Tradeoff,\tThe\tROC\tCurve,\tError\tAnalysis,\tMultilabel\tClassification\n\nsklearn.model_selection.cross_val_score(),\tBetter\tEvaluation\tUsing\tCross-Validation- Better\tEvaluation\tUsing\tCross-Validation,\tMeasuring\tAccuracy\tUsing\tCross-Validation- Confusion\tMatrix\n\nsklearn.model_selection.GridSearchCV,\tGrid\tSearch-Randomized\tSearch,\tExercises, Error\tAnalysis,\tExercises,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nsklearn.model_selection.StratifiedKFold,\tMeasuring\tAccuracy\tUsing\tCross-Validation\n\nsklearn.model_selection.StratifiedShuffleSplit,\tCreate\ta\tTest\tSet\n\nsklearn.model_selection.train_test_split(),\tCreate\ta\tTest\tSet,\tTraining\tand\tEvaluating on\tthe\tTraining\tSet,\tLearning\tCurves,\tExercises,\tGradient\tBoosting\n\nsklearn.multiclass.OneVsOneClassifier,\tMulticlass\tClassification\n\nsklearn.neighbors.KNeighborsClassifier,\tMultilabel\tClassification,\tExercises\n\nsklearn.neighbors.KNeighborsRegressor,\tModel-based\tlearning\n\nsklearn.pipeline.FeatureUnion,\tTransformation\tPipelines\n\nsklearn.pipeline.Pipeline,\tTransformation\tPipelines,\tLearning\tCurves,\tSoft\tMargin Classification-Nonlinear\tSVM\tClassification,\tSelecting\ta\tKernel\tand\tTuning Hyperparameters\n\nsklearn.preprocessing.Imputer,\tData\tCleaning,\tTransformation\tPipelines\n\nsklearn.preprocessing.LabelBinarizer,\tHandling\tText\tand\tCategorical\tAttributes, Transformation\tPipelines\n\nsklearn.preprocessing.LabelEncoder,\tHandling\tText\tand\tCategorical\tAttributes\n\nsklearn.preprocessing.OneHotEncoder,\tHandling\tText\tand\tCategorical\tAttributes\n\nsklearn.preprocessing.PolynomialFeatures,\tPolynomial\tRegression-Polynomial Regression,\tLearning\tCurves,\tRidge\tRegression,\tNonlinear\tSVM\tClassification\n\nsklearn.preprocessing.StandardScaler,\tFeature\tScaling-Transformation\tPipelines, Multiclass\tClassification,\tGradient\tDescent,\tRidge\tRegression,\tLinear\tSVM Classification,\tSoft\tMargin\tClassification-Polynomial\tKernel,\tGaussian\tRBF\tKernel, Implementing\tGradient\tDescent,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nsklearn.svm.LinearSVC,\tSoft\tMargin\tClassification-Nonlinear\tSVM\tClassification, Gaussian\tRBF\tKernel-Computational\tComplexity,\tSVM\tRegression,\tExercises\n\nsklearn.svm.LinearSVR,\tSVM\tRegression-SVM\tRegression\n\nsklearn.svm.SVC,\tSoft\tMargin\tClassification,\tPolynomial\tKernel,\tGaussian\tRBF\tKernel- Computational\tComplexity,\tSVM\tRegression,\tExercises,\tVoting\tClassifiers\n\nsklearn.svm.SVR,\tExercises,\tSVM\tRegression\n\nsklearn.tree.DecisionTreeClassifier,\tRegularization\tHyperparameters,\tExercises, Bagging\tand\tPasting\tin\tScikit-Learn-Out-of-Bag\tEvaluation,\tRandom\tForests, AdaBoost\n\nsklearn.tree.DecisionTreeRegressor,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet, Decision\tTrees,\tRegression,\tGradient\tBoosting-Gradient\tBoosting\n\nsklearn.tree.export_graphviz(),\tTraining\tand\tVisualizing\ta\tDecision\tTree\n\nStandardScaler,\tGradient\tDescent,\tImplementing\tGradient\tDescent,\tTraining\tan\tMLP with\tTensorFlow’s\tHigh-Level\tAPI\n\nSVM\tclassification\tclasses,\tComputational\tComplexity\n\nTF.Learn,\tUp\tand\tRunning\twith\tTensorFlow\n\nuser\tguide,\tOther\tResources\n\nscore(),\tData\tCleaning\n\nsearch\tspace,\tRandomized\tSearch,\tFine-Tuning\tNeural\tNetwork\tHyperparameters\n\nsecond-order\tpartial\tderivatives\t(Hessians),\tAdam\tOptimization\n\nself-organizing\tmaps\t(SOMs),\tSelf-Organizing\tMaps-Self-Organizing\tMaps\n\nsemantic\thashing,\tExercises\n\nsemisupervised\tlearning,\tSemisupervised\tlearning\n\nsensitivity,\tConfusion\tMatrix,\tThe\tROC\tCurve\n\nsentiment\tanalysis,\tRecurrent\tNeural\tNetworks\n\nseparable_conv2d(),\tResNet\n\nsequences,\tRecurrent\tNeural\tNetworks\n\nsequence_length,\tHandling\tVariable\tLength\tInput\tSequences-Handling\tVariable-Length\tOutput Sequences,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nShannon's\tinformation\ttheory,\tGini\tImpurity\tor\tEntropy?\n\nshortcut\tconnections,\tResNet\n\nshow(),\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\nshow_graph(),\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\nshrinkage,\tGradient\tBoosting\n\nshuffle_batch(),\tOther\tconvenience\tfunctions\n\nshuffle_batch_join(),\tOther\tconvenience\tfunctions\n\nsigmoid\tfunction,\tEstimating\tProbabilities\n\nsigmoid_cross_entropy_with_logits(),\tTensorFlow\tImplementation\n\nsimilarity\tfunction,\tAdding\tSimilarity\tFeatures-Adding\tSimilarity\tFeatures\n\nsimulated\tannealing,\tStochastic\tGradient\tDescent\n\nsimulated\tenvironments,\tIntroduction\tto\tOpenAI\tGym\n\n(see\talso\tOpenAI\tGym)\n\nSingular\tValue\tDecomposition\t(SVD),\tPrincipal\tComponents\n\nskewed\tdatasets,\tMeasuring\tAccuracy\tUsing\tCross-Validation\n\nskip\tconnections,\tData\tAugmentation,\tResNet\n\nslack\tvariable,\tTraining\tObjective\n\nsmoothing\tterms,\tBatch\tNormalization,\tAdaGrad,\tAdam\tOptimization,\tVariational Autoencoders\n\nsoft\tmargin\tclassification,\tSoft\tMargin\tClassification-Soft\tMargin\tClassification\n\nsoft\tplacements,\tSoft\tplacement\n\nsoft\tvoting,\tVoting\tClassifiers\n\nsoftmax\tfunction,\tSoftmax\tRegression,\tMulti-Layer\tPerceptron\tand\tBackpropagation,\tTraining an\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nSoftmax\tRegression,\tSoftmax\tRegression-Softmax\tRegression\n\nsource\tops,\tLinear\tRegression\twith\tTensorFlow,\tParallel\tExecution\n\nspam\tfilters,\tThe\tMachine\tLearning\tLandscape-Why\tUse\tMachine\tLearning?,\tSupervised learning\n\nsparse\tautoencoders,\tSparse\tAutoencoders-TensorFlow\tImplementation\n\nsparse\tmatrix,\tHandling\tText\tand\tCategorical\tAttributes\n\nsparse\tmodels,\tLasso\tRegression,\tAdam\tOptimization\n\nsparse_softmax_cross_entropy_with_logits(),\tConstruction\tPhase\n\nsparsity\tloss,\tSparse\tAutoencoders\n\nspecificity,\tThe\tROC\tCurve\n\nspeech\trecognition,\tWhy\tUse\tMachine\tLearning?\n\nspurious\tpatterns,\tHopfield\tNetworks\n\nstack(),\tStatic\tUnrolling\tThrough\tTime\n\nstacked\tautoencoders,\tStacked\tAutoencoders-Unsupervised\tPretraining\tUsing\tStacked Autoencoders\n\nTensorFlow\timplementation,\tTensorFlow\tImplementation\n\ntraining\tone-at-a-time,\tTraining\tOne\tAutoencoder\tat\ta\tTime-Training\tOne\tAutoencoder at\ta\tTime\n\ntying\tweights,\tTying\tWeights-Tying\tWeights\n\nunsupervised\tpretraining\twith,\tUnsupervised\tPretraining\tUsing\tStacked\tAutoencoders- Unsupervised\tPretraining\tUsing\tStacked\tAutoencoders\n\nvisualizing\tthe\treconstructions,\tVisualizing\tthe\tReconstructions-Visualizing\tthe Reconstructions\n\nstacked\tdenoising\tautoencoders,\tVisualizing\tFeatures,\tDenoising\tAutoencoders\n\nstacked\tdenoising\tencoders,\tDenoising\tAutoencoders\n\nstacked\tgeneralization\t(see\tstacking)\n\nstacking,\tStacking-Stacking\n\nstale\tgradients,\tAsynchronous\tupdates\n\nstandard\tcorrelation\tcoefficient,\tLooking\tfor\tCorrelations\n\nstandardization,\tFeature\tScaling\n\nStandardScaler,\tTransformation\tPipelines,\tImplementing\tGradient\tDescent,\tTraining\tan\tMLP with\tTensorFlow’s\tHigh-Level\tAPI\n\nstate-action\tvalues,\tMarkov\tDecision\tProcesses\n\nstates\ttensor,\tHandling\tVariable\tLength\tInput\tSequences\n\nstate_is_tuple,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs,\tLSTM\tCell\n\nstatic\tunrolling\tthrough\ttime,\tStatic\tUnrolling\tThrough\tTime-Static\tUnrolling\tThrough\tTime\n\nstatic_rnn(),\tStatic\tUnrolling\tThrough\tTime-Static\tUnrolling\tThrough\tTime,\tAn\tEncoder– Decoder\tNetwork\tfor\tMachine\tTranslation\n\nstationary\tpoint,\tSVM\tDual\tProblem-SVM\tDual\tProblem\n\nstatistical\tmode,\tBagging\tand\tPasting\n\nstatistical\tsignificance,\tRegularization\tHyperparameters\n\nstemming,\tExercises\n\nstep\tfunctions,\tThe\tPerceptron\n\nstep(),\tIntroduction\tto\tOpenAI\tGym\n\nStochastic\tGradient\tBoosting,\tGradient\tBoosting\n\nStochastic\tGradient\tDescent\t(SGD),\tStochastic\tGradient\tDescent-Stochastic\tGradient Descent,\tSoft\tMargin\tClassification,\tThe\tPerceptron\n\ntraining,\tTraining\tand\tCost\tFunction\n\nStochastic\tGradient\tDescent\t(SGD)\tclassifier,\tTraining\ta\tBinary\tClassifier,\tRidge\tRegression\n\nstochastic\tneurons,\tBoltzmann\tMachines\n\nstochastic\tpolicy,\tPolicy\tSearch\n\nstratified\tsampling,\tCreate\ta\tTest\tSet-Create\ta\tTest\tSet,\tMeasuring\tAccuracy\tUsing\tCross- Validation\n\nstride,\tConvolutional\tLayer\n\nstring\tkernels,\tGaussian\tRBF\tKernel\n\nstring_input_producer(),\tOther\tconvenience\tfunctions\n\nstrong\tlearners,\tVoting\tClassifiers\n\nsubderivatives,\tOnline\tSVMs\n\nsubgradient\tvector,\tLasso\tRegression\n\nsubsample,\tGradient\tBoosting,\tPooling\tLayer\n\nsupervised\tlearning,\tSupervised/Unsupervised\tLearning-Supervised\tlearning\n\nSupport\tVector\tMachines\t(SVMs),\tMulticlass\tClassification,\tSupport\tVector\tMachines- Exercises\n\ndecision\tfunction\tand\tpredictions,\tDecision\tFunction\tand\tPredictions-Decision\tFunction and\tPredictions\n\ndual\tproblem,\tSVM\tDual\tProblem-SVM\tDual\tProblem\n\nkernelized\tSVM,\tKernelized\tSVM-Kernelized\tSVM\n\nlinear\tclassification,\tLinear\tSVM\tClassification-Soft\tMargin\tClassification\n\nmechanics\tof,\tUnder\tthe\tHood-Online\tSVMs\n\nnonlinear\tclassification,\tNonlinear\tSVM\tClassification-Computational\tComplexity\n\nonline\tSVMs,\tOnline\tSVMs-Online\tSVMs\n\nQuadratic\tProgramming\t(QP)\tproblems,\tQuadratic\tProgramming-Quadratic Programming\n\nSVM\tregression,\tSVM\tRegression-Online\tSVMs\n\nthe\tdual\tproblem,\tThe\tDual\tProblem\n\ntraining\tobjective,\tTraining\tObjective-Training\tObjective\n\nsupport\tvectors,\tLinear\tSVM\tClassification\n\nsvd(),\tPrincipal\tComponents\n\nsymbolic\tdifferentiation,\tUsing\tautodiff,\tSymbolic\tDifferentiation-Numerical\tDifferentiation\n\nsynchronous\tupdates,\tSynchronous\tupdates\n\nT\n\nt-Distributed\tStochastic\tNeighbor\tEmbedding\t(t-SNE),\tOther\tDimensionality\tReduction Techniques\n\ntail\theavy,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\ntarget\tattributes,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\ntarget_weights,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\ntasks,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\nTemporal\tDifference\t(TD)\tLearning,\tTemporal\tDifference\tLearning\tand\tQ-Learning-Temporal Difference\tLearning\tand\tQ-Learning\n\ntensor\tprocessing\tunits\t(TPUs),\tInstallation\n\nTensorBoard,\tUp\tand\tRunning\twith\tTensorFlow\n\nTensorFlow,\tUp\tand\tRunning\twith\tTensorFlow-Exercises\n\nabout,\tObjective\tand\tApproach\n\nautodiff,\tUsing\tautodiff-Using\tautodiff,\tAutodiff-Reverse-Mode\tAutodiff\n\nBatch\tNormalization\twith,\tImplementing\tBatch\tNormalization\twith\tTensorFlow- Implementing\tBatch\tNormalization\twith\tTensorFlow\n\nconstruction\tphase,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\ncontrol\tdependencies,\tControl\tDependencies\n\nconvenience\tfunctions,\tOther\tconvenience\tfunctions\n\nconvolutional\tlayers,\tResNet\n\nconvolutional\tneural\tnetworks\tand,\tTensorFlow\tImplementation-TensorFlow Implementation\n\ndata\tparallelism\tand,\tTensorFlow\timplementation\n\ndenoising\tautoencoders,\tTensorFlow\tImplementation-TensorFlow\tImplementation\n\ndropout\twith,\tDropout\n\ndynamic\tplacer,\tPlacing\tOperations\ton\tDevices\n\nexecution\tphase,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\nfeeding\tdata\tto\tthe\ttraining\talgorithm,\tFeeding\tData\tto\tthe\tTraining\tAlgorithm-Feeding Data\tto\tthe\tTraining\tAlgorithm\n\nGradient\tDescent\twith,\tImplementing\tGradient\tDescent-Using\tan\tOptimizer\n\ngraphs,\tmanaging,\tManaging\tGraphs\n\ninitial\tgraph\tcreation\tand\tsession\trun,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta Session-Creating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\ninstallation,\tInstallation\n\nl1\tand\tl2\tregularization\twith,\tℓ1\tand\tℓ2\tRegularization\n\nlearning\tschedules\tin,\tLearning\tRate\tScheduling\n\nLinear\tRegression\twith,\tLinear\tRegression\twith\tTensorFlow-Linear\tRegression\twith TensorFlow\n\nmax\tpooling\tlayer\tin,\tPooling\tLayer\n\nmax-norm\tregularization\twith,\tMax-Norm\tRegularization\n\nmodel\tzoo,\tModel\tZoos\n\nmodularity,\tModularity-Modularity\n\nMomentum\toptimization\tin,\tMomentum\tOptimization\n\nname\tscopes,\tName\tScopes\n\nneural\tnetwork\tpolicies,\tNeural\tNetwork\tPolicies\n\nNLP\ttutorials,\tNatural\tLanguage\tProcessing,\tAn\tEncoder–Decoder\tNetwork\tfor Machine\tTranslation\n\nnode\tvalue\tlifecycle,\tLifecycle\tof\ta\tNode\tValue\n\noperations\t(ops),\tLinear\tRegression\twith\tTensorFlow\n\noptimizer,\tUsing\tan\tOptimizer\n\noverview,\tUp\tand\tRunning\twith\tTensorFlow-Up\tand\tRunning\twith\tTensorFlow\n\nparallel\tdistributed\tcomputing\t(see\tparallel\tdistributed\tcomputing\twith\tTensorFlow)\n\nPython\tAPI\n\nconstruction,\tConstruction\tPhase-Construction\tPhase\n\nexecution,\tExecution\tPhase\n\nusing\tthe\tneural\tnetwork,\tUsing\tthe\tNeural\tNetwork\n\nqueues\t(see\tqueues)\n\nreusing\tpretrained\tlayers,\tReusing\ta\tTensorFlow\tModel-Reusing\ta\tTensorFlow\tModel\n\nRNNs\tin,\tBasic\tRNNs\tin\tTensorFlow-Handling\tVariable-Length\tOutput\tSequences\n\n(see\talso\trecurrent\tneural\tnetworks\t(RNNs))\n\nsaving\tand\trestoring\tmodels,\tSaving\tand\tRestoring\tModels-Saving\tand\tRestoring Models\n\nsharing\tvariables,\tSharing\tVariables-Sharing\tVariables\n\nsimple\tplacer,\tPlacing\tOperations\ton\tDevices\n\nsparse\tautoencoders\twith,\tTensorFlow\tImplementation\n\nand\tstacked\tautoencoders,\tTensorFlow\tImplementation\n\nTensorBoard,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard- Visualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\ntf.abs(),\tℓ1\tand\tℓ2\tRegularization\n\ntf.add(),\tModularity,\tℓ1\tand\tℓ2\tRegularization\n\ntf.add_n(),\tModularity-Sharing\tVariables,\tSharing\tVariables-Sharing\tVariables\n\ntf.add_to_collection(),\tMax-Norm\tRegularization\n\ntf.assign(),\tManually\tComputing\tthe\tGradients,\tReusing\tModels\tfrom\tOther\n\nFrameworks,\tMax-Norm\tRegularization-Max-Norm\tRegularization,\tChapter\t9:\tUp\tand Running\twith\tTensorFlow\n\ntf.bfloat16,\tBandwidth\tsaturation\n\ntf.bool,\tDropout\n\ntf.cast(),\tConstruction\tPhase,\tTraining\ta\tSequence\tClassifier\n\ntf.clip_by_norm(),\tMax-Norm\tRegularization-Max-Norm\tRegularization\n\ntf.clip_by_value(),\tGradient\tClipping\n\ntf.concat(),\tExercises,\tGoogLeNet,\tNeural\tNetwork\tPolicies,\tPolicy\tGradients\n\ntf.ConfigProto,\tManaging\tthe\tGPU\tRAM,\tLogging\tplacements-Soft\tplacement,\tIn- Graph\tVersus\tBetween-Graph\tReplication,\tChapter\t12:\tDistributing\tTensorFlow\tAcross Devices\tand\tServers\n\ntf.constant(),\tLifecycle\tof\ta\tNode\tValue-Manually\tComputing\tthe\tGradients,\tSimple placement-Dynamic\tplacement\tfunction,\tControl\tDependencies,\tOpening\ta\tSession- Pinning\tOperations\tAcross\tTasks\n\ntf.constant_initializer(),\tSharing\tVariables-Sharing\tVariables\n\ntf.container(),\tSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers-Asynchronous Communication\tUsing\tTensorFlow\tQueues,\tTensorFlow\timplementation-Exercises, Chapter\t9:\tUp\tand\tRunning\twith\tTensorFlow\n\ntf.contrib.layers.l1_regularizer(),\tℓ1\tand\tℓ2\tRegularization,\tMax-Norm\tRegularization\n\ntf.contrib.layers.l2_regularizer(),\tℓ1\tand\tℓ2\tRegularization,\tTensorFlow Implementation-Tying\tWeights\n\ntf.contrib.layers.variance_scaling_initializer(),\tXavier\tand\tHe\tInitialization-Xavier\tand He\tInitialization,\tTraining\ta\tSequence\tClassifier,\tTensorFlow\tImplementation-Tying Weights,\tVariational\tAutoencoders,\tNeural\tNetwork\tPolicies,\tPolicy\tGradients, Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.contrib.learn.DNNClassifier,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\ntf.contrib.learn.infer_real_valued_columns_from_input(),\tTraining\tan\tMLP\twith TensorFlow’s\tHigh-Level\tAPI\n\ntf.contrib.rnn.BasicLSTMCell,\tLSTM\tCell,\tPeephole\tConnections\n\ntf.contrib.rnn.BasicRNNCell,\tStatic\tUnrolling\tThrough\tTime-Dynamic\tUnrolling Through\tTime,\tTraining\ta\tSequence\tClassifier,\tTraining\tto\tPredict\tTime\tSeries-Training to\tPredict\tTime\tSeries,\tTraining\tto\tPredict\tTime\tSeries,\tDeep\tRNNs-Applying\tDropout, LSTM\tCell\n\ntf.contrib.rnn.DropoutWrapper,\tApplying\tDropout\n\ntf.contrib.rnn.GRUCell,\tGRU\tCell\n\ntf.contrib.rnn.LSTMCell,\tPeephole\tConnections\n\ntf.contrib.rnn.MultiRNNCell,\tDeep\tRNNs-Applying\tDropout\n\ntf.contrib.rnn.OutputProjectionWrapper,\tTraining\tto\tPredict\tTime\tSeries-Training\tto Predict\tTime\tSeries\n\ntf.contrib.rnn.RNNCell,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs\n\ntf.contrib.rnn.static_rnn(),\tBasic\tRNNs\tin\tTensorFlow-Handling\tVariable\tLength\tInput Sequences,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation-Exercises, Chapter\t14:\tRecurrent\tNeural\tNetworks-Chapter\t14:\tRecurrent\tNeural\tNetworks\n\ntf.contrib.slim\tmodule,\tUp\tand\tRunning\twith\tTensorFlow,\tExercises\n\ntf.contrib.slim.nets\tmodule\t(nets),\tExercises\n\ntf.control_dependencies(),\tControl\tDependencies\n\ntf.decode_csv(),\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded readers\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\ntf.device(),\tSimple\tplacement-Soft\tplacement,\tPinning\tOperations\tAcross\tTasks- Sharding\tVariables\tAcross\tMultiple\tParameter\tServers,\tDistributing\ta\tDeep\tRNN Across\tMultiple\tGPUs-Distributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs\n\ntf.exp(),\tVariational\tAutoencoders-Generating\tDigits\n\ntf.FIFOQueue,\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues,\tQueues\tof tuples-RandomShuffleQueue,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph, Multithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\ntf.float32,\tLinear\tRegression\twith\tTensorFlow,\tChapter\t9:\tUp\tand\tRunning\twith TensorFlow\n\ntf.get_collection(),\tReusing\ta\tTensorFlow\tModel-Freezing\tthe\tLower\tLayers,\tℓ1\tand\tℓ2 Regularization,\tMax-Norm\tRegularization,\tTensorFlow\tImplementation,\tLearning\tto Play\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.get_default_graph(),\tManaging\tGraphs,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves Using\tTensorBoard\n\ntf.get_default_session(),\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\ntf.get_variable(),\tSharing\tVariables-Sharing\tVariables,\tReusing\tModels\tfrom\tOther Frameworks,\tℓ1\tand\tℓ2\tRegularization\n\ntf.global_variables(),\tReusing\ta\tTensorFlow\tModel\n\ntf.global_variables_initializer(),\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession, Manually\tComputing\tthe\tGradients\n\ntf.gradients(),\tUsing\tautodiff\n\ntf.Graph,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession,\tManaging\tGraphs, Visualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard,\tLoading\tData\tDirectly from\tthe\tGraph,\tIn-Graph\tVersus\tBetween-Graph\tReplication\n\ntf.GraphKeys.GLOBAL_VARIABLES,\tReusing\ta\tTensorFlow\tModel-Freezing\tthe Lower\tLayers\n\ntf.GraphKeys.REGULARIZATION_LOSSES,\tℓ1\tand\tℓ2\tRegularization,\tTensorFlow Implementation\n\ntf.group(),\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.int32,\tOperations\tand\tkernels-Queues\tof\ttuples,\tReading\tthe\ttraining\tdata\tdirectly from\tthe\tgraph,\tHandling\tVariable\tLength\tInput\tSequences,\tTraining\ta\tSequence Classifier,\tWord\tEmbeddings,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.int64,\tConstruction\tPhase\n\ntf.InteractiveSession,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\ntf.layers.batch_normalization(),\tImplementing\tBatch\tNormalization\twith\tTensorFlow-\n\nImplementing\tBatch\tNormalization\twith\tTensorFlow\n\ntf.layers.dense(),\tConstruction\tPhase\n\nTF.Learn,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\ntf.log(),\tTensorFlow\tImplementation,\tVariational\tAutoencoders,\tNeural\tNetwork Policies,\tPolicy\tGradients\n\ntf.matmul(),\tLinear\tRegression\twith\tTensorFlow-Manually\tComputing\tthe\tGradients, Modularity,\tConstruction\tPhase,\tBasic\tRNNs\tin\tTensorFlow,\tTying\tWeights,\tTraining One\tAutoencoder\tat\ta\tTime,\tTensorFlow\tImplementation,\tTensorFlow\tImplementation- TensorFlow\tImplementation\n\ntf.matrix_inverse(),\tLinear\tRegression\twith\tTensorFlow\n\ntf.maximum(),\tModularity,\tSharing\tVariables-Sharing\tVariables,\tNonsaturating Activation\tFunctions\n\ntf.multinomial(),\tNeural\tNetwork\tPolicies,\tPolicy\tGradients\n\ntf.name_scope(),\tName\tScopes,\tModularity-Sharing\tVariables,\tConstruction\tPhase, Construction\tPhase-Construction\tPhase,\tTraining\tOne\tAutoencoder\tat\ta\tTime-Training One\tAutoencoder\tat\ta\tTime\n\ntf.nn.conv2d(),\tTensorFlow\tImplementation-TensorFlow\tImplementation\n\ntf.nn.dynamic_rnn(),\tStatic\tUnrolling\tThrough\tTime-Dynamic\tUnrolling\tThrough\tTime, Training\ta\tSequence\tClassifier,\tTraining\tto\tPredict\tTime\tSeries,\tTraining\tto\tPredict Time\tSeries,\tDeep\tRNNs-Applying\tDropout,\tAn\tEncoder–Decoder\tNetwork\tfor Machine\tTranslation-Exercises,\tChapter\t14:\tRecurrent\tNeural\tNetworks-Chapter\t14: Recurrent\tNeural\tNetworks\n\ntf.nn.elu(),\tNonsaturating\tActivation\tFunctions,\tTensorFlow\tImplementation-Tying Weights,\tVariational\tAutoencoders,\tNeural\tNetwork\tPolicies,\tPolicy\tGradients\n\ntf.nn.embedding_lookup(),\tWord\tEmbeddings\n\ntf.nn.in_top_k(),\tConstruction\tPhase,\tTraining\ta\tSequence\tClassifier\n\ntf.nn.max_pool(),\tPooling\tLayer-Pooling\tLayer\n\ntf.nn.relu(),\tConstruction\tPhase,\tTraining\tto\tPredict\tTime\tSeries-Training\tto\tPredict\n\nTime\tSeries,\tTraining\tto\tPredict\tTime\tSeries,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing Deep\tQ-Learning\n\ntf.nn.sigmoid_cross_entropy_with_logits(),\tTensorFlow\tImplementation,\tGenerating Digits,\tPolicy\tGradients-Policy\tGradients\n\ntf.nn.sparse_softmax_cross_entropy_with_logits(),\tConstruction\tPhase-Construction Phase,\tTraining\ta\tSequence\tClassifier\n\ntf.one_hot(),\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.PaddingFIFOQueue,\tPaddingFifoQueue\n\ntf.placeholder(),\tFeeding\tData\tto\tthe\tTraining\tAlgorithm-Feeding\tData\tto\tthe\tTraining Algorithm,\tChapter\t9:\tUp\tand\tRunning\twith\tTensorFlow\n\ntf.placeholder_with_default(),\tTensorFlow\tImplementation\n\ntf.RandomShuffleQueue,\tRandomShuffleQueue,\tReading\tthe\ttraining\tdata\tdirectly\tfrom the\tgraph-Reading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded\treaders using\ta\tCoordinator\tand\ta\tQueueRunner-Other\tconvenience\tfunctions\n\ntf.random_normal(),\tModularity,\tBasic\tRNNs\tin\tTensorFlow,\tTensorFlow Implementation,\tVariational\tAutoencoders\n\ntf.random_uniform(),\tManually\tComputing\tthe\tGradients,\tSaving\tand\tRestoring\tModels, Word\tEmbeddings,\tChapter\t9:\tUp\tand\tRunning\twith\tTensorFlow\n\ntf.reduce_mean(),\tManually\tComputing\tthe\tGradients,\tName\tScopes,\tConstruction Phase-Construction\tPhase,\tℓ1\tand\tℓ2\tRegularization,\tTraining\ta\tSequence\tClassifier- Training\ta\tSequence\tClassifier,\tPerforming\tPCA\twith\tan\tUndercomplete\tLinear Autoencoder,\tTensorFlow\tImplementation,\tTraining\tOne\tAutoencoder\tat\ta\tTime, Training\tOne\tAutoencoder\tat\ta\tTime,\tTensorFlow\tImplementation,\tTensorFlow Implementation,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.reduce_sum(),\tℓ1\tand\tℓ2\tRegularization,\tTensorFlow\tImplementation-TensorFlow Implementation,\tVariational\tAutoencoders-Generating\tDigits,\tLearning\tto\tPlay\tMs. Pac-Man\tUsing\tDeep\tQ-Learning-Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ- Learning\n\ntf.reset_default_graph(),\tManaging\tGraphs\n\ntf.reshape(),\tTraining\tto\tPredict\tTime\tSeries,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing Deep\tQ-Learning\n\ntf.RunOptions,\tIn-Graph\tVersus\tBetween-Graph\tReplication\n\ntf.Session,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession,\tChapter\t9:\tUp\tand Running\twith\tTensorFlow\n\ntf.shape(),\tTensorFlow\tImplementation,\tVariational\tAutoencoders\n\ntf.square(),\tManually\tComputing\tthe\tGradients,\tName\tScopes,\tTraining\tto\tPredict\tTime Series,\tPerforming\tPCA\twith\tan\tUndercomplete\tLinear\tAutoencoder,\tTensorFlow Implementation,\tTraining\tOne\tAutoencoder\tat\ta\tTime,\tTraining\tOne\tAutoencoder\tat\ta Time,\tTensorFlow\tImplementation,\tTensorFlow\tImplementation,\tVariational Autoencoders-Generating\tDigits,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ- Learning\n\ntf.stack(),\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded\treaders using\ta\tCoordinator\tand\ta\tQueueRunner,\tStatic\tUnrolling\tThrough\tTime\n\ntf.string,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded\treaders\tusing a\tCoordinator\tand\ta\tQueueRunner\n\ntf.summary.FileWriter,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard- Visualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\ntf.summary.scalar(),\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\ntf.tanh(),\tBasic\tRNNs\tin\tTensorFlow\n\ntf.TextLineReader,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded readers\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\ntf.to_float(),\tPolicy\tGradients-Policy\tGradients\n\ntf.train.AdamOptimizer,\tAdam\tOptimization,\tAdam\tOptimization,\tTraining\ta\tSequence Classifier,\tTraining\tto\tPredict\tTime\tSeries,\tPerforming\tPCA\twith\tan\tUndercomplete Linear\tAutoencoder,\tTensorFlow\tImplementation-Tying\tWeights,\tTraining\tOne Autoencoder\tat\ta\tTime,\tTensorFlow\tImplementation,\tGenerating\tDigits,\tPolicy Gradients-Policy\tGradients,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.train.ClusterSpec,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\ntf.train.Coordinator,\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner- Multithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\ntf.train.exponential_decay(),\tLearning\tRate\tScheduling\n\ntf.train.GradientDescentOptimizer,\tUsing\tan\tOptimizer,\tConstruction\tPhase,\tGradient Clipping,\tMomentum\tOptimization,\tAdam\tOptimization\n\ntf.train.MomentumOptimizer,\tUsing\tan\tOptimizer,\tMomentum\tOptimization-Nesterov Accelerated\tGradient,\tLearning\tRate\tScheduling,\tExercises,\tTensorFlow implementation,\tChapter\t10:\tIntroduction\tto\tArtificial\tNeural\tNetworks-Chapter\t11: Training\tDeep\tNeural\tNets\n\ntf.train.QueueRunner,\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner- Other\tconvenience\tfunctions\n\ntf.train.replica_device_setter(),\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers- Sharing\tState\tAcross\tSessions\tUsing\tResource\tContainers\n\ntf.train.RMSPropOptimizer,\tRMSProp\n\ntf.train.Saver,\tSaving\tand\tRestoring\tModels-Saving\tand\tRestoring\tModels,\tConstruction Phase,\tExercises,\tApplying\tDropout,\tPolicy\tGradients,\tLearning\tto\tPlay\tMs.\tPac-Man Using\tDeep\tQ-Learning\n\ntf.train.Server,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\ntf.train.start_queue_runners(),\tOther\tconvenience\tfunctions\n\ntf.transpose(),\tLinear\tRegression\twith\tTensorFlow-Manually\tComputing\tthe\tGradients, Static\tUnrolling\tThrough\tTime,\tTying\tWeights\n\ntf.truncated_normal(),\tConstruction\tPhase\n\ntf.unstack(),\tStatic\tUnrolling\tThrough\tTime-Dynamic\tUnrolling\tThrough\tTime,\tTraining to\tPredict\tTime\tSeries,\tChapter\t14:\tRecurrent\tNeural\tNetworks\n\ntf.Variable,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession,\tChapter\t9:\tUp\tand Running\twith\tTensorFlow\n\ntf.variable_scope(),\tSharing\tVariables-Sharing\tVariables,\tReusing\tModels\tfrom\tOther Frameworks,\tSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers,\tTraining\ta\n\nSequence\tClassifier,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.zeros(),\tConstruction\tPhase,\tBasic\tRNNs\tin\tTensorFlow,\tTying\tWeights\n\ntruncated\tbackpropagation\tthrough\ttime,\tThe\tDifficulty\tof\tTraining\tover\tMany\tTime Steps\n\nvisualizing\tgraph\tand\ttraining\tcurves,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing TensorBoard-Visualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\nTensorFlow\tServing,\tOne\tNeural\tNetwork\tper\tDevice\n\ntensorflow.contrib,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\ntest\tset,\tTesting\tand\tValidating,\tCreate\ta\tTest\tSet-Create\ta\tTest\tSet,\tMNIST\n\ntesting\tand\tvalidating,\tTesting\tand\tValidating-Testing\tand\tValidating\n\ntext\tattributes,\tHandling\tText\tand\tCategorical\tAttributes-Handling\tText\tand\tCategorical Attributes\n\nTextLineReader,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph\n\nTF-slim,\tUp\tand\tRunning\twith\tTensorFlow\n\ntf.layers.conv1d(),\tResNet\n\ntf.layers.conv2d(),\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.layers.conv2d_transpose(),\tResNet\n\ntf.layers.conv3d(),\tResNet\n\ntf.layers.dense(),\tXavier\tand\tHe\tInitialization,\tImplementing\tBatch\tNormalization\twith TensorFlow\n\ntf.layers.separable_conv2d(),\tResNet\n\nTF.Learn,\tUp\tand\tRunning\twith\tTensorFlow,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level API\n\ntf.nn.atrous_conv2d(),\tResNet\n\ntf.nn.depthwise_conv2d(),\tResNet\n\nthermal\tequilibrium,\tBoltzmann\tMachines\n\nthread\tpools\t(inter-op/intra-op,\tin\tTensorFlow,\tParallel\tExecution\n\nthreshold\tvariable,\tSharing\tVariables-Sharing\tVariables\n\nTikhonov\tregularization,\tRidge\tRegression\n\ntime\tseries\tdata,\tRecurrent\tNeural\tNetworks\n\ntoarray(),\tHandling\tText\tand\tCategorical\tAttributes\n\ntolerance\thyperparameter,\tComputational\tComplexity\n\ntraining,\tImplementing\tBatch\tNormalization\twith\tTensorFlow-Implementing\tBatch Normalization\twith\tTensorFlow,\tApplying\tDropout\n\ntraining\tdata,\tWhat\tIs\tMachine\tLearning?\n\ninsufficient\tquantities,\tInsufficient\tQuantity\tof\tTraining\tData\n\nirrelevant\tfeatures,\tIrrelevant\tFeatures\n\nloading,\tLoading\tData\tDirectly\tfrom\tthe\tGraph-Other\tconvenience\tfunctions\n\nnonrepresentative,\tNonrepresentative\tTraining\tData\n\noverfitting,\tOverfitting\tthe\tTraining\tData-Overfitting\tthe\tTraining\tData\n\npoor\tquality,\tPoor-Quality\tData\n\nunderfitting,\tUnderfitting\tthe\tTraining\tData\n\ntraining\tinstance,\tWhat\tIs\tMachine\tLearning?\n\ntraining\tmodels,\tModel-based\tlearning,\tTraining\tModels-Exercises\n\nlearning\tcurves\tin,\tLearning\tCurves-Learning\tCurves\n\nLinear\tRegression,\tTraining\tModels,\tLinear\tRegression-Mini-batch\tGradient\tDescent\n\nLogistic\tRegression,\tLogistic\tRegression-Softmax\tRegression\n\noverview,\tTraining\tModels-Training\tModels\n\nPolynomial\tRegression,\tTraining\tModels,\tPolynomial\tRegression-Polynomial\tRegression\n\nU\n\ntraining\tobjectives,\tTraining\tObjective-Training\tObjective\n\ntraining\tset,\tWhat\tIs\tMachine\tLearning?,\tTesting\tand\tValidating,\tDiscover\tand\tVisualize\tthe Data\tto\tGain\tInsights,\tPrepare\tthe\tData\tfor\tMachine\tLearning\tAlgorithms,\tTraining\tand Evaluating\ton\tthe\tTraining\tSet-Training\tand\tEvaluating\ton\tthe\tTraining\tSet\n\ncost\tfunction\tof,\tTraining\tand\tCost\tFunction-Training\tand\tCost\tFunction\n\nshuffling,\tMNIST\n\ntransfer\tlearning,\tReusing\tPretrained\tLayers-Pretraining\ton\tan\tAuxiliary\tTask\n\n(see\talso\tpretrained\tlayers\treuse)\n\ntransform(),\tData\tCleaning,\tTransformation\tPipelines\n\ntransformation\tpipelines,\tTransformation\tPipelines-Select\tand\tTrain\ta\tModel\n\ntransformers,\tData\tCleaning\n\ntransformers,\tcustom,\tCustom\tTransformers-Custom\tTransformers\n\ntranspose(),\tStatic\tUnrolling\tThrough\tTime\n\ntrue\tnegative\trate\t(TNR),\tThe\tROC\tCurve\n\ntrue\tpositive\trate\t(TPR),\tConfusion\tMatrix,\tThe\tROC\tCurve\n\ntruncated\tbackpropagation\tthrough\ttime,\tThe\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps\n\ntuples,\tQueues\tof\ttuples\n\ntying\tweights,\tTying\tWeights\n\nunderfitting,\tUnderfitting\tthe\tTraining\tData,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet, Gaussian\tRBF\tKernel\n\nunivariate\tregression,\tFrame\tthe\tProblem\n\nunstack(),\tStatic\tUnrolling\tThrough\tTime\n\nunsupervised\tlearning,\tUnsupervised\tlearning-Unsupervised\tlearning\n\nanomaly\tdetection,\tUnsupervised\tlearning\n\nV\n\nassociation\trule\tlearning,\tUnsupervised\tlearning,\tUnsupervised\tlearning\n\nclustering,\tUnsupervised\tlearning\n\ndimensionality\treduction\talgorithm,\tUnsupervised\tlearning\n\nvisualization\talgorithms,\tUnsupervised\tlearning\n\nunsupervised\tpretraining,\tUnsupervised\tPretraining-Unsupervised\tPretraining,\tUnsupervised Pretraining\tUsing\tStacked\tAutoencoders-Unsupervised\tPretraining\tUsing\tStacked Autoencoders\n\nupsampling,\tResNet\n\nutility\tfunction,\tModel-based\tlearning\n\nvalidation\tset,\tTesting\tand\tValidating\n\nValue\tIteration,\tMarkov\tDecision\tProcesses\n\nvalue_counts(),\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\nvanishing\tgradients,\tVanishing/Exploding\tGradients\tProblems\n\n(see\talso\tgradients,\tvanishing\tand\texploding)\n\nvariables,\tsharing,\tSharing\tVariables-Sharing\tVariables\n\nvariable_scope(),\tSharing\tVariables-Sharing\tVariables\n\nvariance\n\nbias/variance\ttradeoff,\tLearning\tCurves\n\nvariance\tpreservation,\tPreserving\tthe\tVariance-Preserving\tthe\tVariance\n\nvariance_scaling_initializer(),\tXavier\tand\tHe\tInitialization\n\nvariational\tautoencoders,\tVariational\tAutoencoders-Generating\tDigits\n\nVGGNet,\tResNet\n\nvisual\tcortex,\tThe\tArchitecture\tof\tthe\tVisual\tCortex\n\nW\n\nX\n\nY\n\nZ\n\nvisualization,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard-Visualizing\tthe Graph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\nvisualization\talgorithms,\tUnsupervised\tlearning-Unsupervised\tlearning\n\nvoice\trecognition,\tConvolutional\tNeural\tNetworks\n\nvoting\tclassifiers,\tVoting\tClassifiers-Voting\tClassifiers\n\nwarmup\tphase,\tAsynchronous\tupdates\n\nweak\tlearners,\tVoting\tClassifiers\n\nweight-tying,\tTying\tWeights\n\nweights,\tConstruction\tPhase\n\nfreezing,\tFreezing\tthe\tLower\tLayers\n\nwhile_loop(),\tDynamic\tUnrolling\tThrough\tTime\n\nwhite\tbox\tmodels,\tMaking\tPredictions\n\nworker,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\nworker\tservice,\tThe\tMaster\tand\tWorker\tServices\n\nworker_device,\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\nworkspace\tdirectory,\tGet\tthe\tData-Download\tthe\tData\n\nXavier\tinitialization,\tVanishing/Exploding\tGradients\tProblems-Xavier\tand\tHe\tInitialization\n\nYouTube,\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\nzero\tpadding,\tConvolutional\tLayer,\tTensorFlow\tImplementation\n\nAbout\tthe\tAuthor\n\nAurélien\tGéron\tis\ta\tMachine\tLearning\tconsultant.\tA\tformer\tGoogler,\the\tled\tthe\tYouTube\tvideo classification\tteam\tfrom\t2013\tto\t2016.\tHe\twas\talso\ta\tfounder\tand\tCTO\tof\tWifirst\tfrom\t2002\tto\t2012,\ta leading\tWireless\tISP\tin\tFrance;\tand\ta\tfounder\tand\tCTO\tof\tPolyconseil\tin\t2001,\tthe\tfirm\tthat\tnow manages\tthe\telectric\tcar\tsharing\tservice\tAutolib’.\n\nBefore\tthis\the\tworked\tas\tan\tengineer\tin\ta\tvariety\tof\tdomains:\tfinance\t(JP\tMorgan\tand\tSociété\tGénérale), defense\t(Canada’s\tDOD),\tand\thealthcare\t(blood\ttransfusion).\tHe\tpublished\ta\tfew\ttechnical\tbooks\t(on C++,\tWiFi,\tand\tinternet\tarchitectures),\tand\twas\ta\tComputer\tScience\tlecturer\tin\ta\tFrench\tengineering school.\n\nA\tfew\tfun\tfacts:\the\ttaught\this\tthree\tchildren\tto\tcount\tin\tbinary\twith\ttheir\tfingers\t(up\tto\t1023),\the\tstudied microbiology\tand\tevolutionary\tgenetics\tbefore\tgoing\tinto\tsoftware\tengineering,\tand\this\tparachute\tdidn’t open\ton\tthe\tsecond\tjump.\n\nColophon\n\nThe\tanimal\ton\tthe\tcover\tof\tHands-On\tMachine\tLearning\twith\tScikit-Learn\tand\tTensorFlow\tis\tthe\tfar eastern\tfire\tsalamander\t(Salamandra\tinfraimmaculata),\tan\tamphibian\tfound\tin\tthe\tMiddle\tEast.\tThey have\tblack\tskin\tfeaturing\tlarge\tyellow\tspots\ton\ttheir\tback\tand\thead.\tThese\tspots\tare\ta\twarning\tcoloration meant\tto\tkeep\tpredators\tat\tbay.\tFull-grown\tsalamanders\tcan\tbe\tover\ta\tfoot\tin\tlength.\n\nFar\teastern\tfire\tsalamanders\tlive\tin\tsubtropical\tshrubland\tand\tforests\tnear\trivers\tor\tother\tfreshwater bodies.\tThey\tspend\tmost\tof\ttheir\tlife\ton\tland,\tbut\tlay\ttheir\teggs\tin\tthe\twater.\tThey\tsubsist\tmostly\ton\ta\tdiet of\tinsects,\tworms,\tand\tsmall\tcrustaceans,\tbut\toccasionally\teat\tother\tsalamanders.\tMales\tof\tthe\tspecies have\tbeen\tknown\tto\tlive\tup\tto\t23\tyears,\twhile\tfemales\tcan\tlive\tup\tto\t21\tyears.\n\nAlthough\tnot\tyet\tendangered,\tthe\tfar\teastern\tfire\tsalamander\tpopulation\tis\tin\tdecline.\tPrimary\tthreats include\tdamming\tof\trivers\t(which\tdisrupts\tthe\tsalamander’s\tbreeding)\tand\tpollution.\tThey\tare\talso threatened\tby\tthe\trecent\tintroduction\tof\tpredatory\tfish,\tsuch\tas\tthe\tmosquitofish.\tThese\tfish\twere\tintended to\tcontrol\tthe\tmosquito\tpopulation,\tbut\tthey\talso\tfeed\ton\tyoung\tsalamanders.\n\nMany\tof\tthe\tanimals\ton\tO’Reilly\tcovers\tare\tendangered;\tall\tof\tthem\tare\timportant\tto\tthe\tworld.\tTo\tlearn more\tabout\thow\tyou\tcan\thelp,\tgo\tto\tanimals.oreilly.com.\n\nThe\tcover\timage\tis\tfrom\tWood’s\tIllustrated\tNatural\tHistory.\tThe\tcover\tfonts\tare\tURW\tTypewriter\tand Guardian\tSans.\tThe\ttext\tfont\tis\tAdobe\tMinion\tPro;\tthe\theading\tfont\tis\tAdobe\tMyriad\tCondensed;\tand\tthe code\tfont\tis\tDalton\tMaag’s\tUbuntu\tMono.\n\nPreface\n\nThe\tMachine\tLearning\tTsunami\n\nMachine\tLearning\tin\tYour\tProjects\n\nObjective\tand\tApproach\n\nPrerequisites\n\nRoadmap\n\nOther\tResources\n\nConventions\tUsed\tin\tThis\tBook\n\nUsing\tCode\tExamples\n\nO’Reilly\tSafari\n\nHow\tto\tContact\tUs\n\nAcknowledgments\n\nI.\tThe\tFundamentals\tof\tMachine\tLearning\n\n1.\tThe\tMachine\tLearning\tLandscape\n\nWhat\tIs\tMachine\tLearning?\n\nWhy\tUse\tMachine\tLearning?\n\nTypes\tof\tMachine\tLearning\tSystems\n\nSupervised/Unsupervised\tLearning\n\nBatch\tand\tOnline\tLearning\n\nInstance-Based\tVersus\tModel-Based\tLearning\n\nMain\tChallenges\tof\tMachine\tLearning\n\nInsufficient\tQuantity\tof\tTraining\tData\n\nNonrepresentative\tTraining\tData\n\nPoor-Quality\tData\n\nIrrelevant\tFeatures\n\nOverfitting\tthe\tTraining\tData\n\nUnderfitting\tthe\tTraining\tData\n\nStepping\tBack\n\nTesting\tand\tValidating\n\nExercises\n\n2.\tEnd-to-End\tMachine\tLearning\tProject\n\nWorking\twith\tReal\tData\n\nLook\tat\tthe\tBig\tPicture\n\nFrame\tthe\tProblem\n\nSelect\ta\tPerformance\tMeasure\n\nCheck\tthe\tAssumptions\n\nGet\tthe\tData\n\nCreate\tthe\tWorkspace\n\nDownload\tthe\tData\n\nTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\nCreate\ta\tTest\tSet\n\nDiscover\tand\tVisualize\tthe\tData\tto\tGain\tInsights\n\nVisualizing\tGeographical\tData\n\nLooking\tfor\tCorrelations\n\nExperimenting\twith\tAttribute\tCombinations\n\nPrepare\tthe\tData\tfor\tMachine\tLearning\tAlgorithms\n\nData\tCleaning\n\nHandling\tText\tand\tCategorical\tAttributes\n\nCustom\tTransformers\n\nFeature\tScaling\n\nTransformation\tPipelines\n\nSelect\tand\tTrain\ta\tModel\n\nTraining\tand\tEvaluating\ton\tthe\tTraining\tSet\n\nBetter\tEvaluation\tUsing\tCross-Validation\n\nFine-Tune\tYour\tModel\n\nGrid\tSearch\n\nRandomized\tSearch\n\nEnsemble\tMethods\n\nAnalyze\tthe\tBest\tModels\tand\tTheir\tErrors\n\nEvaluate\tYour\tSystem\ton\tthe\tTest\tSet\n\nLaunch,\tMonitor,\tand\tMaintain\tYour\tSystem\n\nTry\tIt\tOut!\n\nExercises\n\n3.\tClassification\n\nMNIST\n\nTraining\ta\tBinary\tClassifier\n\nPerformance\tMeasures\n\nMeasuring\tAccuracy\tUsing\tCross-Validation\n\nConfusion\tMatrix\n\nPrecision\tand\tRecall\n\nPrecision/Recall\tTradeoff\n\nThe\tROC\tCurve\n\nMulticlass\tClassification\n\nError\tAnalysis\n\nMultilabel\tClassification\n\nMultioutput\tClassification\n\nExercises\n\n4.\tTraining\tModels\n\nLinear\tRegression\n\nThe\tNormal\tEquation\n\nComputational\tComplexity\n\nGradient\tDescent\n\nBatch\tGradient\tDescent\n\nStochastic\tGradient\tDescent\n\nMini-batch\tGradient\tDescent\n\nPolynomial\tRegression\n\nLearning\tCurves\n\nRegularized\tLinear\tModels\n\nRidge\tRegression\n\nLasso\tRegression\n\nElastic\tNet\n\nEarly\tStopping\n\nLogistic\tRegression\n\nEstimating\tProbabilities\n\nTraining\tand\tCost\tFunction\n\nDecision\tBoundaries\n\nSoftmax\tRegression\n\nExercises\n\n5.\tSupport\tVector\tMachines\n\nLinear\tSVM\tClassification\n\nSoft\tMargin\tClassification\n\nNonlinear\tSVM\tClassification\n\nPolynomial\tKernel\n\nAdding\tSimilarity\tFeatures\n\nGaussian\tRBF\tKernel\n\nComputational\tComplexity\n\nSVM\tRegression\n\nUnder\tthe\tHood\n\nDecision\tFunction\tand\tPredictions\n\nTraining\tObjective\n\nQuadratic\tProgramming\n\nThe\tDual\tProblem\n\nKernelized\tSVM\n\nOnline\tSVMs\n\nExercises\n\n6.\tDecision\tTrees\n\nTraining\tand\tVisualizing\ta\tDecision\tTree\n\nMaking\tPredictions\n\nEstimating\tClass\tProbabilities\n\nThe\tCART\tTraining\tAlgorithm\n\nComputational\tComplexity\n\nGini\tImpurity\tor\tEntropy?\n\nRegularization\tHyperparameters\n\nRegression\n\nInstability\n\nExercises\n\n7.\tEnsemble\tLearning\tand\tRandom\tForests\n\nVoting\tClassifiers\n\nBagging\tand\tPasting\n\nBagging\tand\tPasting\tin\tScikit-Learn\n\nOut-of-Bag\tEvaluation\n\nRandom\tPatches\tand\tRandom\tSubspaces\n\nRandom\tForests\n\nExtra-Trees\n\nFeature\tImportance\n\nBoosting\n\nAdaBoost\n\nGradient\tBoosting\n\nStacking\n\nExercises\n\n8.\tDimensionality\tReduction\n\nThe\tCurse\tof\tDimensionality\n\nMain\tApproaches\tfor\tDimensionality\tReduction\n\nProjection\n\nManifold\tLearning\n\nPCA\n\nPreserving\tthe\tVariance\n\nPrincipal\tComponents\n\nProjecting\tDown\tto\td\tDimensions\n\nUsing\tScikit-Learn\n\nExplained\tVariance\tRatio\n\nChoosing\tthe\tRight\tNumber\tof\tDimensions\n\nPCA\tfor\tCompression\n\nIncremental\tPCA\n\nRandomized\tPCA\n\nKernel\tPCA\n\nSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nLLE\n\nOther\tDimensionality\tReduction\tTechniques\n\nExercises\n\nII.\tNeural\tNetworks\tand\tDeep\tLearning\n\n9.\tUp\tand\tRunning\twith\tTensorFlow\n\nInstallation\n\nCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\nManaging\tGraphs\n\nLifecycle\tof\ta\tNode\tValue\n\nLinear\tRegression\twith\tTensorFlow\n\nImplementing\tGradient\tDescent\n\nManually\tComputing\tthe\tGradients\n\nUsing\tautodiff\n\nUsing\tan\tOptimizer\n\nFeeding\tData\tto\tthe\tTraining\tAlgorithm\n\nSaving\tand\tRestoring\tModels\n\nVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\nName\tScopes\n\nModularity\n\nSharing\tVariables\n\nExercises\n\n10.\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\nFrom\tBiological\tto\tArtificial\tNeurons\n\nBiological\tNeurons\n\nLogical\tComputations\twith\tNeurons\n\nThe\tPerceptron\n\nMulti-Layer\tPerceptron\tand\tBackpropagation\n\nTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nTraining\ta\tDNN\tUsing\tPlain\tTensorFlow\n\nConstruction\tPhase\n\nExecution\tPhase\n\nUsing\tthe\tNeural\tNetwork\n\nFine-Tuning\tNeural\tNetwork\tHyperparameters\n\nNumber\tof\tHidden\tLayers\n\nNumber\tof\tNeurons\tper\tHidden\tLayer\n\nActivation\tFunctions\n\nExercises\n\n11.\tTraining\tDeep\tNeural\tNets\n\nVanishing/Exploding\tGradients\tProblems\n\nXavier\tand\tHe\tInitialization\n\nNonsaturating\tActivation\tFunctions\n\nBatch\tNormalization\n\nGradient\tClipping\n\nReusing\tPretrained\tLayers\n\nReusing\ta\tTensorFlow\tModel\n\nReusing\tModels\tfrom\tOther\tFrameworks\n\nFreezing\tthe\tLower\tLayers\n\nCaching\tthe\tFrozen\tLayers\n\nTweaking,\tDropping,\tor\tReplacing\tthe\tUpper\tLayers\n\nModel\tZoos\n\nUnsupervised\tPretraining\n\nPretraining\ton\tan\tAuxiliary\tTask\n\nFaster\tOptimizers\n\nMomentum\tOptimization\n\nNesterov\tAccelerated\tGradient\n\nAdaGrad\n\nRMSProp\n\nAdam\tOptimization\n\nLearning\tRate\tScheduling\n\nAvoiding\tOverfitting\tThrough\tRegularization\n\nEarly\tStopping\n\nℓ1\tand\tℓ2\tRegularization\n\nDropout\n\nMax-Norm\tRegularization\n\nData\tAugmentation\n\nPractical\tGuidelines\n\nExercises\n\n12.\tDistributing\tTensorFlow\tAcross\tDevices\tand\tServers\n\nMultiple\tDevices\ton\ta\tSingle\tMachine\n\nInstallation\n\nManaging\tthe\tGPU\tRAM\n\nPlacing\tOperations\ton\tDevices\n\nParallel\tExecution\n\nControl\tDependencies\n\nMultiple\tDevices\tAcross\tMultiple\tServers\n\nOpening\ta\tSession\n\nThe\tMaster\tand\tWorker\tServices\n\nPinning\tOperations\tAcross\tTasks\n\nSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\nSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers\n\nAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues\n\nLoading\tData\tDirectly\tfrom\tthe\tGraph\n\nParallelizing\tNeural\tNetworks\ton\ta\tTensorFlow\tCluster\n\nOne\tNeural\tNetwork\tper\tDevice\n\nIn-Graph\tVersus\tBetween-Graph\tReplication\n\nModel\tParallelism\n\nData\tParallelism\n\nExercises\n\n13.\tConvolutional\tNeural\tNetworks\n\nThe\tArchitecture\tof\tthe\tVisual\tCortex\n\nConvolutional\tLayer\n\nFilters\n\nStacking\tMultiple\tFeature\tMaps\n\nTensorFlow\tImplementation\n\nMemory\tRequirements\n\nPooling\tLayer\n\nCNN\tArchitectures\n\nLeNet-5\n\nAlexNet\n\nGoogLeNet\n\nResNet\n\nExercises\n\n14.\tRecurrent\tNeural\tNetworks\n\nRecurrent\tNeurons\n\nMemory\tCells\n\nInput\tand\tOutput\tSequences\n\nBasic\tRNNs\tin\tTensorFlow\n\nStatic\tUnrolling\tThrough\tTime\n\nDynamic\tUnrolling\tThrough\tTime\n\nHandling\tVariable\tLength\tInput\tSequences\n\nHandling\tVariable-Length\tOutput\tSequences\n\nTraining\tRNNs\n\nTraining\ta\tSequence\tClassifier\n\nTraining\tto\tPredict\tTime\tSeries\n\nCreative\tRNN\n\nDeep\tRNNs\n\nDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs\n\nApplying\tDropout\n\nThe\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps\n\nLSTM\tCell\n\nPeephole\tConnections\n\nGRU\tCell\n\nNatural\tLanguage\tProcessing\n\nWord\tEmbeddings\n\nAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nExercises\n\n15.\tAutoencoders\n\nEfficient\tData\tRepresentations\n\nPerforming\tPCA\twith\tan\tUndercomplete\tLinear\tAutoencoder\n\nStacked\tAutoencoders\n\nTensorFlow\tImplementation\n\nTying\tWeights\n\nTraining\tOne\tAutoencoder\tat\ta\tTime\n\nVisualizing\tthe\tReconstructions\n\nVisualizing\tFeatures\n\nUnsupervised\tPretraining\tUsing\tStacked\tAutoencoders\n\nDenoising\tAutoencoders\n\nTensorFlow\tImplementation\n\nSparse\tAutoencoders\n\nTensorFlow\tImplementation\n\nVariational\tAutoencoders\n\nGenerating\tDigits\n\nOther\tAutoencoders\n\nExercises\n\n16.\tReinforcement\tLearning\n\nLearning\tto\tOptimize\tRewards\n\nPolicy\tSearch\n\nIntroduction\tto\tOpenAI\tGym\n\nNeural\tNetwork\tPolicies\n\nEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem\n\nPolicy\tGradients\n\nMarkov\tDecision\tProcesses\n\nTemporal\tDifference\tLearning\tand\tQ-Learning\n\nExploration\tPolicies\n\nApproximate\tQ-Learning\n\nLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nExercises\n\nThank\tYou!\n\nA.\tExercise\tSolutions\n\nChapter\t1:\tThe\tMachine\tLearning\tLandscape\n\nChapter\t2:\tEnd-to-End\tMachine\tLearning\tProject\n\nChapter\t3:\tClassification\n\nChapter\t4:\tTraining\tModels\n\nChapter\t5:\tSupport\tVector\tMachines\n\nChapter\t6:\tDecision\tTrees\n\nChapter\t7:\tEnsemble\tLearning\tand\tRandom\tForests\n\nChapter\t8:\tDimensionality\tReduction\n\nChapter\t9:\tUp\tand\tRunning\twith\tTensorFlow\n\nChapter\t10:\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\nChapter\t11:\tTraining\tDeep\tNeural\tNets\n\nChapter\t12:\tDistributing\tTensorFlow\tAcross\tDevices\tand\tServers\n\nChapter\t13:\tConvolutional\tNeural\tNetworks\n\nChapter\t14:\tRecurrent\tNeural\tNetworks\n\nChapter\t15:\tAutoencoders\n\nChapter\t16:\tReinforcement\tLearning\n\nB.\tMachine\tLearning\tProject\tChecklist\n\nFrame\tthe\tProblem\tand\tLook\tat\tthe\tBig\tPicture\n\nGet\tthe\tData\n\nExplore\tthe\tData\n\nPrepare\tthe\tData\n\nShort-List\tPromising\tModels\n\nFine-Tune\tthe\tSystem\n\nPresent\tYour\tSolution\n\nLaunch!\n\nC.\tSVM\tDual\tProblem\n\nD.\tAutodiff\n\nManual\tDifferentiation\n\nSymbolic\tDifferentiation\n\nNumerical\tDifferentiation\n\nForward-Mode\tAutodiff\n\nReverse-Mode\tAutodiff\n\nE.\tOther\tPopular\tANN\tArchitectures\n\nHopfield\tNetworks\n\nBoltzmann\tMachines\n\nRestricted\tBoltzmann\tMachines\n\nDeep\tBelief\tNets\n\nSelf-Organizing\tMaps\n\nIndex",
      "page_number": 534
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Hands-On\tMachine\tLearning\twith\tScikit-Learn and\tTensorFlow\n\nConcepts,\tTools,\tand\tTechniques\tto\tBuild\tIntelligent\tSystems\n\nAurélien\tGéron",
      "content_length": 136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Hands-On\tMachine\tLearning\twith\tScikit-Learn\tand\tTensorFlow\n\nby\tAurélien\tGéron\n\nCopyright\t©\t2017\tAurélien\tGéron.\tAll\trights\treserved.\n\nPrinted\tin\tthe\tUnited\tStates\tof\tAmerica.\n\nPublished\tby\tO’Reilly\tMedia,\tInc.,\t1005\tGravenstein\tHighway\tNorth,\tSebastopol,\tCA\t95472.\n\nO’Reilly\tbooks\tmay\tbe\tpurchased\tfor\teducational,\tbusiness,\tor\tsales\tpromotional\tuse.\tOnline\teditions\tare also\tavailable\tfor\tmost\ttitles\t(http://oreilly.com/safari).\tFor\tmore\tinformation,\tcontact\tour corporate/institutional\tsales\tdepartment:\t800-998-9938\tor\tcorporate@oreilly.com.\n\nEditor:\tNicole\tTache\n\nProduction\tEditor:\tNicholas\tAdams\n\nCopyeditor:\tRachel\tMonaghan\n\nProofreader:\tCharles\tRoumeliotis\n\nIndexer:\tWendy\tCatalano\n\nInterior\tDesigner:\tDavid\tFutato\n\nCover\tDesigner:\tRandy\tComer\n\nIllustrator:\tRebecca\tDemarest\n\nMarch\t2017:\tFirst\tEdition",
      "content_length": 810,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Revision\tHistory\tfor\tthe\tFirst\tEdition\n\n2017-03-10:\tFirst\tRelease\n\n2017-06-09:\tSecond\tRelease\n\nSee\thttp://oreilly.com/catalog/errata.csp?isbn=9781491962299\tfor\trelease\tdetails.\n\nThe\tO’Reilly\tlogo\tis\ta\tregistered\ttrademark\tof\tO’Reilly\tMedia,\tInc.\tHands-On\tMachine\tLearning\twith Scikit-Learn\tand\tTensorFlow,\tthe\tcover\timage,\tand\trelated\ttrade\tdress\tare\ttrademarks\tof\tO’Reilly Media,\tInc.\n\nWhile\tthe\tpublisher\tand\tthe\tauthor\thave\tused\tgood\tfaith\tefforts\tto\tensure\tthat\tthe\tinformation\tand instructions\tcontained\tin\tthis\twork\tare\taccurate,\tthe\tpublisher\tand\tthe\tauthor\tdisclaim\tall\tresponsibility for\terrors\tor\tomissions,\tincluding\twithout\tlimitation\tresponsibility\tfor\tdamages\tresulting\tfrom\tthe\tuse\tof or\treliance\ton\tthis\twork.\tUse\tof\tthe\tinformation\tand\tinstructions\tcontained\tin\tthis\twork\tis\tat\tyour\town risk.\tIf\tany\tcode\tsamples\tor\tother\ttechnology\tthis\twork\tcontains\tor\tdescribes\tis\tsubject\tto\topen\tsource licenses\tor\tthe\tintellectual\tproperty\trights\tof\tothers,\tit\tis\tyour\tresponsibility\tto\tensure\tthat\tyour\tuse\tthereof complies\twith\tsuch\tlicenses\tand/or\trights.\n\n978-1-491-96229-9\n\n[LSI]",
      "content_length": 1090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Preface",
      "content_length": 7,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "The\tMachine\tLearning\tTsunami In\t2006,\tGeoffrey\tHinton\tet\tal.\tpublished\ta\tpaper1\tshowing\thow\tto\ttrain\ta\tdeep\tneural\tnetwork\tcapable\tof recognizing\thandwritten\tdigits\twith\tstate-of-the-art\tprecision\t(>98%).\tThey\tbranded\tthis\ttechnique\t“Deep Learning.”\tTraining\ta\tdeep\tneural\tnet\twas\twidely\tconsidered\timpossible\tat\tthe\ttime,2\tand\tmost researchers\thad\tabandoned\tthe\tidea\tsince\tthe\t1990s.\tThis\tpaper\trevived\tthe\tinterest\tof\tthe\tscientific community\tand\tbefore\tlong\tmany\tnew\tpapers\tdemonstrated\tthat\tDeep\tLearning\twas\tnot\tonly\tpossible,\tbut capable\tof\tmind-blowing\tachievements\tthat\tno\tother\tMachine\tLearning\t(ML)\ttechnique\tcould\thope\tto match\t(with\tthe\thelp\tof\ttremendous\tcomputing\tpower\tand\tgreat\tamounts\tof\tdata).\tThis\tenthusiasm\tsoon extended\tto\tmany\tother\tareas\tof\tMachine\tLearning.\n\nFast-forward\t10\tyears\tand\tMachine\tLearning\thas\tconquered\tthe\tindustry:\tit\tis\tnow\tat\tthe\theart\tof\tmuch\tof the\tmagic\tin\ttoday’s\thigh-tech\tproducts,\tranking\tyour\tweb\tsearch\tresults,\tpowering\tyour\tsmartphone’s speech\trecognition,\tand\trecommending\tvideos,\tbeating\tthe\tworld\tchampion\tat\tthe\tgame\tof\tGo.\tBefore\tyou know\tit,\tit\twill\tbe\tdriving\tyour\tcar.",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Machine\tLearning\tin\tYour\tProjects So\tnaturally\tyou\tare\texcited\tabout\tMachine\tLearning\tand\tyou\twould\tlove\tto\tjoin\tthe\tparty!\n\nPerhaps\tyou\twould\tlike\tto\tgive\tyour\thomemade\trobot\ta\tbrain\tof\tits\town?\tMake\tit\trecognize\tfaces?\tOr learn\tto\twalk\taround?\n\nOr\tmaybe\tyour\tcompany\thas\ttons\tof\tdata\t(user\tlogs,\tfinancial\tdata,\tproduction\tdata,\tmachine\tsensor\tdata, hotline\tstats,\tHR\treports,\tetc.),\tand\tmore\tthan\tlikely\tyou\tcould\tunearth\tsome\thidden\tgems\tif\tyou\tjust\tknew where\tto\tlook;\tfor\texample:\n\nSegment\tcustomers\tand\tfind\tthe\tbest\tmarketing\tstrategy\tfor\teach\tgroup\n\nRecommend\tproducts\tfor\teach\tclient\tbased\ton\twhat\tsimilar\tclients\tbought\n\nDetect\twhich\ttransactions\tare\tlikely\tto\tbe\tfraudulent\n\nPredict\tnext\tyear’s\trevenue\n\nAnd\tmore\n\nWhatever\tthe\treason,\tyou\thave\tdecided\tto\tlearn\tMachine\tLearning\tand\timplement\tit\tin\tyour\tprojects. Great\tidea!",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Objective\tand\tApproach This\tbook\tassumes\tthat\tyou\tknow\tclose\tto\tnothing\tabout\tMachine\tLearning.\tIts\tgoal\tis\tto\tgive\tyou\tthe concepts,\tthe\tintuitions,\tand\tthe\ttools\tyou\tneed\tto\tactually\timplement\tprograms\tcapable\tof\tlearning\tfrom data.\n\nWe\twill\tcover\ta\tlarge\tnumber\tof\ttechniques,\tfrom\tthe\tsimplest\tand\tmost\tcommonly\tused\t(such\tas\tlinear regression)\tto\tsome\tof\tthe\tDeep\tLearning\ttechniques\tthat\tregularly\twin\tcompetitions.\n\nRather\tthan\timplementing\tour\town\ttoy\tversions\tof\teach\talgorithm,\twe\twill\tbe\tusing\tactual\tproduction- ready\tPython\tframeworks:\n\nScikit-Learn\tis\tvery\teasy\tto\tuse,\tyet\tit\timplements\tmany\tMachine\tLearning\talgorithms\tefficiently,\tso it\tmakes\tfor\ta\tgreat\tentry\tpoint\tto\tlearn\tMachine\tLearning.\n\nTensorFlow\tis\ta\tmore\tcomplex\tlibrary\tfor\tdistributed\tnumerical\tcomputation\tusing\tdata\tflow\tgraphs. It\tmakes\tit\tpossible\tto\ttrain\tand\trun\tvery\tlarge\tneural\tnetworks\tefficiently\tby\tdistributing\tthe computations\tacross\tpotentially\tthousands\tof\tmulti-GPU\tservers.\tTensorFlow\twas\tcreated\tat\tGoogle and\tsupports\tmany\tof\ttheir\tlarge-scale\tMachine\tLearning\tapplications.\tIt\twas\topen-sourced\tin November\t2015.\n\nThe\tbook\tfavors\ta\thands-on\tapproach,\tgrowing\tan\tintuitive\tunderstanding\tof\tMachine\tLearning\tthrough concrete\tworking\texamples\tand\tjust\ta\tlittle\tbit\tof\ttheory.\tWhile\tyou\tcan\tread\tthis\tbook\twithout\tpicking\tup your\tlaptop,\twe\thighly\trecommend\tyou\texperiment\twith\tthe\tcode\texamples\tavailable\tonline\tas\tJupyter notebooks\tat\thttps://github.com/ageron/handson-ml.",
      "content_length": 1470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Prerequisites This\tbook\tassumes\tthat\tyou\thave\tsome\tPython\tprogramming\texperience\tand\tthat\tyou\tare\tfamiliar\twith Python’s\tmain\tscientific\tlibraries,\tin\tparticular\tNumPy,\tPandas,\tand\tMatplotlib.\n\nAlso,\tif\tyou\tcare\tabout\twhat’s\tunder\tthe\thood\tyou\tshould\thave\ta\treasonable\tunderstanding\tof\tcollege- level\tmath\tas\twell\t(calculus,\tlinear\talgebra,\tprobabilities,\tand\tstatistics).\n\nIf\tyou\tdon’t\tknow\tPython\tyet,\thttp://learnpython.org/\tis\ta\tgreat\tplace\tto\tstart.\tThe\tofficial\ttutorial\ton python.org\tis\talso\tquite\tgood.\n\nIf\tyou\thave\tnever\tused\tJupyter,\tChapter\t2\twill\tguide\tyou\tthrough\tinstallation\tand\tthe\tbasics:\tit\tis\ta\tgreat tool\tto\thave\tin\tyour\ttoolbox.\n\nIf\tyou\tare\tnot\tfamiliar\twith\tPython’s\tscientific\tlibraries,\tthe\tprovided\tJupyter\tnotebooks\tinclude\ta\tfew tutorials.\tThere\tis\talso\ta\tquick\tmath\ttutorial\tfor\tlinear\talgebra.",
      "content_length": 822,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Roadmap This\tbook\tis\torganized\tin\ttwo\tparts.\tPart\tI,\tThe\tFundamentals\tof\tMachine\tLearning,\tcovers\tthe following\ttopics:\n\nWhat\tis\tMachine\tLearning?\tWhat\tproblems\tdoes\tit\ttry\tto\tsolve?\tWhat\tare\tthe\tmain\tcategories\tand fundamental\tconcepts\tof\tMachine\tLearning\tsystems?\n\nThe\tmain\tsteps\tin\ta\ttypical\tMachine\tLearning\tproject.\n\nLearning\tby\tfitting\ta\tmodel\tto\tdata.\n\nOptimizing\ta\tcost\tfunction.\n\nHandling,\tcleaning,\tand\tpreparing\tdata.\n\nSelecting\tand\tengineering\tfeatures.\n\nSelecting\ta\tmodel\tand\ttuning\thyperparameters\tusing\tcross-validation.\n\nThe\tmain\tchallenges\tof\tMachine\tLearning,\tin\tparticular\tunderfitting\tand\toverfitting\t(the bias/variance\ttradeoff).\n\nReducing\tthe\tdimensionality\tof\tthe\ttraining\tdata\tto\tfight\tthe\tcurse\tof\tdimensionality.\n\nThe\tmost\tcommon\tlearning\talgorithms:\tLinear\tand\tPolynomial\tRegression,\tLogistic\tRegression,\tk- Nearest\tNeighbors,\tSupport\tVector\tMachines,\tDecision\tTrees,\tRandom\tForests,\tand\tEnsemble methods.\n\nPart\tII,\tNeural\tNetworks\tand\tDeep\tLearning,\tcovers\tthe\tfollowing\ttopics:\n\nWhat\tare\tneural\tnets?\tWhat\tare\tthey\tgood\tfor?\n\nBuilding\tand\ttraining\tneural\tnets\tusing\tTensorFlow.\n\nThe\tmost\timportant\tneural\tnet\tarchitectures:\tfeedforward\tneural\tnets,\tconvolutional\tnets,\trecurrent nets,\tlong\tshort-term\tmemory\t(LSTM)\tnets,\tand\tautoencoders.\n\nTechniques\tfor\ttraining\tdeep\tneural\tnets.\n\nScaling\tneural\tnetworks\tfor\thuge\tdatasets.\n\nReinforcement\tlearning.\n\nThe\tfirst\tpart\tis\tbased\tmostly\ton\tScikit-Learn\twhile\tthe\tsecond\tpart\tuses\tTensorFlow.",
      "content_length": 1466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "CAUTION\n\nDon’t\tjump\tinto\tdeep\twaters\ttoo\thastily:\twhile\tDeep\tLearning\tis\tno\tdoubt\tone\tof\tthe\tmost\texciting\tareas\tin\tMachine\tLearning, you\tshould\tmaster\tthe\tfundamentals\tfirst.\tMoreover,\tmost\tproblems\tcan\tbe\tsolved\tquite\twell\tusing\tsimpler\ttechniques\tsuch\tas Random\tForests\tand\tEnsemble\tmethods\t(discussed\tin\tPart\tI).\tDeep\tLearning\tis\tbest\tsuited\tfor\tcomplex\tproblems\tsuch\tas\timage recognition,\tspeech\trecognition,\tor\tnatural\tlanguage\tprocessing,\tprovided\tyou\thave\tenough\tdata,\tcomputing\tpower,\tand\tpatience.",
      "content_length": 507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Other\tResources Many\tresources\tare\tavailable\tto\tlearn\tabout\tMachine\tLearning.\tAndrew\tNg’s\tML\tcourse\ton\tCoursera\tand Geoffrey\tHinton’s\tcourse\ton\tneural\tnetworks\tand\tDeep\tLearning\tare\tamazing,\talthough\tthey\tboth\trequire\ta significant\ttime\tinvestment\t(think\tmonths).\n\nThere\tare\talso\tmany\tinteresting\twebsites\tabout\tMachine\tLearning,\tincluding\tof\tcourse\tScikit-Learn’s exceptional\tUser\tGuide.\tYou\tmay\talso\tenjoy\tDataquest,\twhich\tprovides\tvery\tnice\tinteractive\ttutorials, and\tML\tblogs\tsuch\tas\tthose\tlisted\ton\tQuora.\tFinally,\tthe\tDeep\tLearning\twebsite\thas\ta\tgood\tlist\tof resources\tto\tlearn\tmore.\n\nOf\tcourse\tthere\tare\talso\tmany\tother\tintroductory\tbooks\tabout\tMachine\tLearning,\tin\tparticular:\n\nJoel\tGrus,\tData\tScience\tfrom\tScratch\t(O’Reilly).\tThis\tbook\tpresents\tthe\tfundamentals\tof\tMachine Learning,\tand\timplements\tsome\tof\tthe\tmain\talgorithms\tin\tpure\tPython\t(from\tscratch,\tas\tthe\tname suggests).\n\nStephen\tMarsland,\tMachine\tLearning:\tAn\tAlgorithmic\tPerspective\t(Chapman\tand\tHall).\tThis\tbook is\ta\tgreat\tintroduction\tto\tMachine\tLearning,\tcovering\ta\twide\trange\tof\ttopics\tin\tdepth,\twith\tcode examples\tin\tPython\t(also\tfrom\tscratch,\tbut\tusing\tNumPy).\n\nSebastian\tRaschka,\tPython\tMachine\tLearning\t(Packt\tPublishing).\tAlso\ta\tgreat\tintroduction\tto Machine\tLearning,\tthis\tbook\tleverages\tPython\topen\tsource\tlibraries\t(Pylearn\t2\tand\tTheano).\n\nYaser\tS.\tAbu-Mostafa,\tMalik\tMagdon-Ismail,\tand\tHsuan-Tien\tLin,\tLearning\tfrom\tData (AMLBook).\tA\trather\ttheoretical\tapproach\tto\tML,\tthis\tbook\tprovides\tdeep\tinsights,\tin\tparticular\ton the\tbias/variance\ttradeoff\t(see\tChapter\t4).\n\nStuart\tRussell\tand\tPeter\tNorvig,\tArtificial\tIntelligence:\tA\tModern\tApproach,\t3rd\tEdition (Pearson).\tThis\tis\ta\tgreat\t(and\thuge)\tbook\tcovering\tan\tincredible\tamount\tof\ttopics,\tincluding Machine\tLearning.\tIt\thelps\tput\tML\tinto\tperspective.\n\nFinally,\ta\tgreat\tway\tto\tlearn\tis\tto\tjoin\tML\tcompetition\twebsites\tsuch\tas\tKaggle.com\tthis\twill\tallow\tyou to\tpractice\tyour\tskills\ton\treal-world\tproblems,\twith\thelp\tand\tinsights\tfrom\tsome\tof\tthe\tbest\tML professionals\tout\tthere.",
      "content_length": 2007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Conventions\tUsed\tin\tThis\tBook The\tfollowing\ttypographical\tconventions\tare\tused\tin\tthis\tbook:\n\nItalic\n\nIndicates\tnew\tterms,\tURLs,\temail\taddresses,\tfilenames,\tand\tfile\textensions.\n\nConstant\twidth\n\nUsed\tfor\tprogram\tlistings,\tas\twell\tas\twithin\tparagraphs\tto\trefer\tto\tprogram\telements\tsuch\tas variable\tor\tfunction\tnames,\tdatabases,\tdata\ttypes,\tenvironment\tvariables,\tstatements\tand\tkeywords.\n\nConstant\twidth\tbold\n\nShows\tcommands\tor\tother\ttext\tthat\tshould\tbe\ttyped\tliterally\tby\tthe\tuser.\n\nConstant\twidth\titalic\n\nShows\ttext\tthat\tshould\tbe\treplaced\twith\tuser-supplied\tvalues\tor\tby\tvalues\tdetermined\tby\tcontext.\n\nTIP\n\nThis\telement\tsignifies\ta\ttip\tor\tsuggestion.\n\nNOTE\n\nThis\telement\tsignifies\ta\tgeneral\tnote.\n\nWARNING\n\nThis\telement\tindicates\ta\twarning\tor\tcaution.",
      "content_length": 753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Using\tCode\tExamples Supplemental\tmaterial\t(code\texamples,\texercises,\tetc.)\tis\tavailable\tfor\tdownload\tat https://github.com/ageron/handson-ml.\n\nThis\tbook\tis\there\tto\thelp\tyou\tget\tyour\tjob\tdone.\tIn\tgeneral,\tif\texample\tcode\tis\toffered\twith\tthis\tbook,\tyou may\tuse\tit\tin\tyour\tprograms\tand\tdocumentation.\tYou\tdo\tnot\tneed\tto\tcontact\tus\tfor\tpermission\tunless you’re\treproducing\ta\tsignificant\tportion\tof\tthe\tcode.\tFor\texample,\twriting\ta\tprogram\tthat\tuses\tseveral chunks\tof\tcode\tfrom\tthis\tbook\tdoes\tnot\trequire\tpermission.\tSelling\tor\tdistributing\ta\tCD-ROM\tof examples\tfrom\tO’Reilly\tbooks\tdoes\trequire\tpermission.\tAnswering\ta\tquestion\tby\tciting\tthis\tbook\tand quoting\texample\tcode\tdoes\tnot\trequire\tpermission.\tIncorporating\ta\tsignificant\tamount\tof\texample\tcode from\tthis\tbook\tinto\tyour\tproduct’s\tdocumentation\tdoes\trequire\tpermission.\n\nWe\tappreciate,\tbut\tdo\tnot\trequire,\tattribution.\tAn\tattribution\tusually\tincludes\tthe\ttitle,\tauthor,\tpublisher, and\tISBN.\tFor\texample:\t“Hands-On\tMachine\tLearning\twith\tScikit-Learn\tand\tTensorFlow\tby\tAurélien Géron\t(O’Reilly).\tCopyright\t2017\tAurélien\tGéron,\t978-1-491-96229-9.”\n\nIf\tyou\tfeel\tyour\tuse\tof\tcode\texamples\tfalls\toutside\tfair\tuse\tor\tthe\tpermission\tgiven\tabove,\tfeel\tfree\tto contact\tus\tat\tpermissions@oreilly.com.",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "O’Reilly\tSafari\n\nNOTE\n\nSafari\t(formerly\tSafari\tBooks\tOnline)\tis\ta\tmembership-based\ttraining\tand\treference\tplatform\tfor enterprise,\tgovernment,\teducators,\tand\tindividuals.\n\nMembers\thave\taccess\tto\tthousands\tof\tbooks,\ttraining\tvideos,\tLearning\tPaths,\tinteractive\ttutorials,\tand curated\tplaylists\tfrom\tover\t250\tpublishers,\tincluding\tO’Reilly\tMedia,\tHarvard\tBusiness\tReview, Prentice\tHall\tProfessional,\tAddison-Wesley\tProfessional,\tMicrosoft\tPress,\tSams,\tQue,\tPeachpit\tPress, Adobe,\tFocal\tPress,\tCisco\tPress,\tJohn\tWiley\t&\tSons,\tSyngress,\tMorgan\tKaufmann,\tIBM\tRedbooks, Packt,\tAdobe\tPress,\tFT\tPress,\tApress,\tManning,\tNew\tRiders,\tMcGraw-Hill,\tJones\t&\tBartlett,\tand Course\tTechnology,\tamong\tothers.\n\nFor\tmore\tinformation,\tplease\tvisit\thttp://oreilly.com/safari.",
      "content_length": 753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "How\tto\tContact\tUs Please\taddress\tcomments\tand\tquestions\tconcerning\tthis\tbook\tto\tthe\tpublisher:\n\nO’Reilly\tMedia,\tInc.\n\n1005\tGravenstein\tHighway\tNorth\n\nSebastopol,\tCA\t95472\n\n800-998-9938\t(in\tthe\tUnited\tStates\tor\tCanada)\n\n707-829-0515\t(international\tor\tlocal)\n\n707-829-0104\t(fax)\n\nWe\thave\ta\tweb\tpage\tfor\tthis\tbook,\twhere\twe\tlist\terrata,\texamples,\tand\tany\tadditional\tinformation.\tYou can\taccess\tthis\tpage\tat\thttp://bit.ly/hands-on-machine-learning-with-scikit-learn-and-tensorflow.\n\nTo\tcomment\tor\task\ttechnical\tquestions\tabout\tthis\tbook,\tsend\temail\tto\tbookquestions@oreilly.com.\n\nFor\tmore\tinformation\tabout\tour\tbooks,\tcourses,\tconferences,\tand\tnews,\tsee\tour\twebsite\tat http://www.oreilly.com.\n\nFind\tus\ton\tFacebook:\thttp://facebook.com/oreilly\n\nFollow\tus\ton\tTwitter:\thttp://twitter.com/oreillymedia\n\nWatch\tus\ton\tYouTube:\thttp://www.youtube.com/oreillymedia",
      "content_length": 851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "Acknowledgments I\twould\tlike\tto\tthank\tmy\tGoogle\tcolleagues,\tin\tparticular\tthe\tYouTube\tvideo\tclassification\tteam,\tfor teaching\tme\tso\tmuch\tabout\tMachine\tLearning.\tI\tcould\tnever\thave\tstarted\tthis\tproject\twithout\tthem. Special\tthanks\tto\tmy\tpersonal\tML\tgurus:\tClément\tCourbet,\tJulien\tDubois,\tMathias\tKende,\tDaniel Kitachewsky,\tJames\tPack,\tAlexander\tPak,\tAnosh\tRaj,\tVitor\tSessak,\tWiktor\tTomczak,\tIngrid\tvon\tGlehn, Rich\tWashington,\tand\teveryone\tat\tYouTube\tParis.\n\nI\tam\tincredibly\tgrateful\tto\tall\tthe\tamazing\tpeople\twho\ttook\ttime\tout\tof\ttheir\tbusy\tlives\tto\treview\tmy\tbook in\tso\tmuch\tdetail.\tThanks\tto\tPete\tWarden\tfor\tanswering\tall\tmy\tTensorFlow\tquestions,\treviewing\tPart\tII, providing\tmany\tinteresting\tinsights,\tand\tof\tcourse\tfor\tbeing\tpart\tof\tthe\tcore\tTensorFlow\tteam.\tYou\tshould definitely\tcheck\tout\this\tblog!\tMany\tthanks\tto\tLukas\tBiewald\tfor\this\tvery\tthorough\treview\tof\tPart\tII:\the left\tno\tstone\tunturned,\ttested\tall\tthe\tcode\t(and\tcaught\ta\tfew\terrors),\tmade\tmany\tgreat\tsuggestions,\tand\this enthusiasm\twas\tcontagious.\tYou\tshould\tcheck\tout\this\tblog\tand\this\tcool\trobots!\tThanks\tto\tJustin\tFrancis, who\talso\treviewed\tPart\tII\tvery\tthoroughly,\tcatching\terrors\tand\tproviding\tgreat\tinsights,\tin\tparticular\tin Chapter\t16.\tCheck\tout\this\tposts\ton\tTensorFlow!\n\nHuge\tthanks\tas\twell\tto\tDavid\tAndrzejewski,\twho\treviewed\tPart\tI\tand\tprovided\tincredibly\tuseful feedback,\tidentifying\tunclear\tsections\tand\tsuggesting\thow\tto\timprove\tthem.\tCheck\tout\this\twebsite! Thanks\tto\tGrégoire\tMesnil,\twho\treviewed\tPart\tII\tand\tcontributed\tvery\tinteresting\tpractical\tadvice\ton training\tneural\tnetworks.\tThanks\tas\twell\tto\tEddy\tHung,\tSalim\tSémaoune,\tKarim\tMatrah,\tIngrid\tvon Glehn,\tIain\tSmears,\tand\tVincent\tGuilbeau\tfor\treviewing\tPart\tI\tand\tmaking\tmany\tuseful\tsuggestions.\tAnd\tI also\twish\tto\tthank\tmy\tfather-in-law,\tMichel\tTessier,\tformer\tmathematics\tteacher\tand\tnow\ta\tgreat translator\tof\tAnton\tChekhov,\tfor\thelping\tme\tiron\tout\tsome\tof\tthe\tmathematics\tand\tnotations\tin\tthis\tbook and\treviewing\tthe\tlinear\talgebra\tJupyter\tnotebook.\n\nAnd\tof\tcourse,\ta\tgigantic\t“thank\tyou”\tto\tmy\tdear\tbrother\tSylvain,\twho\treviewed\tevery\tsingle\tchapter, tested\tevery\tline\tof\tcode,\tprovided\tfeedback\ton\tvirtually\tevery\tsection,\tand\tencouraged\tme\tfrom\tthe\tfirst line\tto\tthe\tlast.\tLove\tyou,\tbro!\n\nMany\tthanks\tas\twell\tto\tO’Reilly’s\tfantastic\tstaff,\tin\tparticular\tNicole\tTache,\twho\tgave\tme\tinsightful feedback,\talways\tcheerful,\tencouraging,\tand\thelpful.\tThanks\tas\twell\tto\tMarie\tBeaugureau,\tBen\tLorica, Mike\tLoukides,\tand\tLaurel\tRuma\tfor\tbelieving\tin\tthis\tproject\tand\thelping\tme\tdefine\tits\tscope.\tThanks\tto Matt\tHacker\tand\tall\tof\tthe\tAtlas\tteam\tfor\tanswering\tall\tmy\ttechnical\tquestions\tregarding\tformatting, asciidoc,\tand\tLaTeX,\tand\tthanks\tto\tRachel\tMonaghan,\tNick\tAdams,\tand\tall\tof\tthe\tproduction\tteam\tfor their\tfinal\treview\tand\ttheir\thundreds\tof\tcorrections.\n\nLast\tbut\tnot\tleast,\tI\tam\tinfinitely\tgrateful\tto\tmy\tbeloved\twife,\tEmmanuelle,\tand\tto\tour\tthree\twonderful kids,\tAlexandre,\tRémi,\tand\tGabrielle,\tfor\tencouraging\tme\tto\twork\thard\ton\tthis\tbook,\tasking\tmany questions\t(who\tsaid\tyou\tcan’t\tteach\tneural\tnetworks\tto\ta\tseven-year-old?),\tand\teven\tbringing\tme\tcookies and\tcoffee.\tWhat\tmore\tcan\tone\tdream\tof?\n\n1\n\nAvailable\ton\tHinton’s\thome\tpage\tat\thttp://www.cs.toronto.edu/~hinton/.\n\n2\n\nDespite\tthe\tfact\tthat\tYann\tLecun’s\tdeep\tconvolutional\tneural\tnetworks\thad\tworked\twell\tfor\timage\trecognition\tsince\tthe\t1990s,\talthough they\twere\tnot\tas\tgeneral\tpurpose.",
      "content_length": 3381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Part\tI.\tThe\tFundamentals\tof\tMachine\tLearning",
      "content_length": 44,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "Chapter\t1.\tThe\tMachine\tLearning\tLandscape\n\nWhen\tmost\tpeople\thear\t“Machine\tLearning,”\tthey\tpicture\ta\trobot:\ta\tdependable\tbutler\tor\ta\tdeadly Terminator\tdepending\ton\twho\tyou\task.\tBut\tMachine\tLearning\tis\tnot\tjust\ta\tfuturistic\tfantasy,\tit’s\talready here.\tIn\tfact,\tit\thas\tbeen\taround\tfor\tdecades\tin\tsome\tspecialized\tapplications,\tsuch\tas\tOptical\tCharacter Recognition\t(OCR).\tBut\tthe\tfirst\tML\tapplication\tthat\treally\tbecame\tmainstream,\timproving\tthe\tlives\tof hundreds\tof\tmillions\tof\tpeople,\ttook\tover\tthe\tworld\tback\tin\tthe\t1990s:\tit\twas\tthe\tspam\tfilter.\tNot\texactly a\tself-aware\tSkynet,\tbut\tit\tdoes\ttechnically\tqualify\tas\tMachine\tLearning\t(it\thas\tactually\tlearned\tso\twell that\tyou\tseldom\tneed\tto\tflag\tan\temail\tas\tspam\tanymore).\tIt\twas\tfollowed\tby\thundreds\tof\tML\tapplications that\tnow\tquietly\tpower\thundreds\tof\tproducts\tand\tfeatures\tthat\tyou\tuse\tregularly,\tfrom\tbetter recommendations\tto\tvoice\tsearch.\n\nWhere\tdoes\tMachine\tLearning\tstart\tand\twhere\tdoes\tit\tend?\tWhat\texactly\tdoes\tit\tmean\tfor\ta\tmachine\tto learn\tsomething?\tIf\tI\tdownload\ta\tcopy\tof\tWikipedia,\thas\tmy\tcomputer\treally\t“learned”\tsomething?\tIs\tit suddenly\tsmarter?\tIn\tthis\tchapter\twe\twill\tstart\tby\tclarifying\twhat\tMachine\tLearning\tis\tand\twhy\tyou\tmay want\tto\tuse\tit.\n\nThen,\tbefore\twe\tset\tout\tto\texplore\tthe\tMachine\tLearning\tcontinent,\twe\twill\ttake\ta\tlook\tat\tthe\tmap\tand learn\tabout\tthe\tmain\tregions\tand\tthe\tmost\tnotable\tlandmarks:\tsupervised\tversus\tunsupervised\tlearning, online\tversus\tbatch\tlearning,\tinstance-based\tversus\tmodel-based\tlearning.\tThen\twe\twill\tlook\tat\tthe workflow\tof\ta\ttypical\tML\tproject,\tdiscuss\tthe\tmain\tchallenges\tyou\tmay\tface,\tand\tcover\thow\tto\tevaluate and\tfine-tune\ta\tMachine\tLearning\tsystem.\n\nThis\tchapter\tintroduces\ta\tlot\tof\tfundamental\tconcepts\t(and\tjargon)\tthat\tevery\tdata\tscientist\tshould\tknow by\theart.\tIt\twill\tbe\ta\thigh-level\toverview\t(the\tonly\tchapter\twithout\tmuch\tcode),\tall\trather\tsimple,\tbut\tyou should\tmake\tsure\teverything\tis\tcrystal-clear\tto\tyou\tbefore\tcontinuing\tto\tthe\trest\tof\tthe\tbook.\tSo\tgrab\ta coffee\tand\tlet’s\tget\tstarted!\n\nTIP\n\nIf\tyou\talready\tknow\tall\tthe\tMachine\tLearning\tbasics,\tyou\tmay\twant\tto\tskip\tdirectly\tto\tChapter\t2.\tIf\tyou\tare\tnot\tsure,\ttry\tto answer\tall\tthe\tquestions\tlisted\tat\tthe\tend\tof\tthe\tchapter\tbefore\tmoving\ton.",
      "content_length": 2218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "What\tIs\tMachine\tLearning? Machine\tLearning\tis\tthe\tscience\t(and\tart)\tof\tprogramming\tcomputers\tso\tthey\tcan\tlearn\tfrom\tdata.\n\nHere\tis\ta\tslightly\tmore\tgeneral\tdefinition:\n\n[Machine\tLearning\tis\tthe]\tfield\tof\tstudy\tthat\tgives\tcomputers\tthe\tability\tto\tlearn\twithout\tbeing explicitly\tprogrammed. Arthur\tSamuel,\t1959\n\nAnd\ta\tmore\tengineering-oriented\tone:\n\nA\tcomputer\tprogram\tis\tsaid\tto\tlearn\tfrom\texperience\tE\twith\trespect\tto\tsome\ttask\tT\tand\tsome performance\tmeasure\tP,\tif\tits\tperformance\ton\tT,\tas\tmeasured\tby\tP,\timproves\twith\texperience\tE. Tom\tMitchell,\t1997\n\nFor\texample,\tyour\tspam\tfilter\tis\ta\tMachine\tLearning\tprogram\tthat\tcan\tlearn\tto\tflag\tspam\tgiven\texamples of\tspam\temails\t(e.g.,\tflagged\tby\tusers)\tand\texamples\tof\tregular\t(nonspam,\talso\tcalled\t“ham”)\temails.\tThe examples\tthat\tthe\tsystem\tuses\tto\tlearn\tare\tcalled\tthe\ttraining\tset.\tEach\ttraining\texample\tis\tcalled\ta training\tinstance\t(or\tsample).\tIn\tthis\tcase,\tthe\ttask\tT\tis\tto\tflag\tspam\tfor\tnew\temails,\tthe\texperience\tE\tis the\ttraining\tdata,\tand\tthe\tperformance\tmeasure\tP\tneeds\tto\tbe\tdefined;\tfor\texample,\tyou\tcan\tuse\tthe\tratio of\tcorrectly\tclassified\temails.\tThis\tparticular\tperformance\tmeasure\tis\tcalled\taccuracy\tand\tit\tis\toften used\tin\tclassification\ttasks.\n\nIf\tyou\tjust\tdownload\ta\tcopy\tof\tWikipedia,\tyour\tcomputer\thas\ta\tlot\tmore\tdata,\tbut\tit\tis\tnot\tsuddenly\tbetter at\tany\ttask.\tThus,\tit\tis\tnot\tMachine\tLearning.",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Why\tUse\tMachine\tLearning? Consider\thow\tyou\twould\twrite\ta\tspam\tfilter\tusing\ttraditional\tprogramming\ttechniques\t(Figure\t1-1):\n\n1.\t First\tyou\twould\tlook\tat\twhat\tspam\ttypically\tlooks\tlike.\tYou\tmight\tnotice\tthat\tsome\twords\tor\tphrases (such\tas\t“4U,”\t“credit\tcard,”\t“free,”\tand\t“amazing”)\ttend\tto\tcome\tup\ta\tlot\tin\tthe\tsubject.\tPerhaps you\twould\talso\tnotice\ta\tfew\tother\tpatterns\tin\tthe\tsender’s\tname,\tthe\temail’s\tbody,\tand\tso\ton.\n\n2.\t You\twould\twrite\ta\tdetection\talgorithm\tfor\teach\tof\tthe\tpatterns\tthat\tyou\tnoticed,\tand\tyour\tprogram would\tflag\temails\tas\tspam\tif\ta\tnumber\tof\tthese\tpatterns\tare\tdetected.\n\n3.\t You\twould\ttest\tyour\tprogram,\tand\trepeat\tsteps\t1\tand\t2\tuntil\tit\tis\tgood\tenough.\n\nFigure\t1-1.\tThe\ttraditional\tapproach\n\nSince\tthe\tproblem\tis\tnot\ttrivial,\tyour\tprogram\twill\tlikely\tbecome\ta\tlong\tlist\tof\tcomplex\trules\t—\tpretty hard\tto\tmaintain.\n\nIn\tcontrast,\ta\tspam\tfilter\tbased\ton\tMachine\tLearning\ttechniques\tautomatically\tlearns\twhich\twords\tand phrases\tare\tgood\tpredictors\tof\tspam\tby\tdetecting\tunusually\tfrequent\tpatterns\tof\twords\tin\tthe\tspam examples\tcompared\tto\tthe\tham\texamples\t(Figure\t1-2).\tThe\tprogram\tis\tmuch\tshorter,\teasier\tto\tmaintain, and\tmost\tlikely\tmore\taccurate.",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Figure\t1-2.\tMachine\tLearning\tapproach\n\nMoreover,\tif\tspammers\tnotice\tthat\tall\ttheir\temails\tcontaining\t“4U”\tare\tblocked,\tthey\tmight\tstart\twriting “For\tU”\tinstead.\tA\tspam\tfilter\tusing\ttraditional\tprogramming\ttechniques\twould\tneed\tto\tbe\tupdated\tto\tflag “For\tU”\temails.\tIf\tspammers\tkeep\tworking\taround\tyour\tspam\tfilter,\tyou\twill\tneed\tto\tkeep\twriting\tnew rules\tforever.\n\nIn\tcontrast,\ta\tspam\tfilter\tbased\ton\tMachine\tLearning\ttechniques\tautomatically\tnotices\tthat\t“For\tU”\thas become\tunusually\tfrequent\tin\tspam\tflagged\tby\tusers,\tand\tit\tstarts\tflagging\tthem\twithout\tyour\tintervention (Figure\t1-3).\n\nFigure\t1-3.\tAutomatically\tadapting\tto\tchange\n\nAnother\tarea\twhere\tMachine\tLearning\tshines\tis\tfor\tproblems\tthat\teither\tare\ttoo\tcomplex\tfor\ttraditional approaches\tor\thave\tno\tknown\talgorithm.\tFor\texample,\tconsider\tspeech\trecognition:\tsay\tyou\twant\tto\tstart simple\tand\twrite\ta\tprogram\tcapable\tof\tdistinguishing\tthe\twords\t“one”\tand\t“two.”\tYou\tmight\tnotice\tthat the\tword\t“two”\tstarts\twith\ta\thigh-pitch\tsound\t(“T”),\tso\tyou\tcould\thardcode\tan\talgorithm\tthat\tmeasures high-pitch\tsound\tintensity\tand\tuse\tthat\tto\tdistinguish\tones\tand\ttwos.\tObviously\tthis\ttechnique\twill\tnot scale\tto\tthousands\tof\twords\tspoken\tby\tmillions\tof\tvery\tdifferent\tpeople\tin\tnoisy\tenvironments\tand\tin",
      "content_length": 1249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "dozens\tof\tlanguages.\tThe\tbest\tsolution\t(at\tleast\ttoday)\tis\tto\twrite\tan\talgorithm\tthat\tlearns\tby\titself,\tgiven many\texample\trecordings\tfor\teach\tword.\n\nFinally,\tMachine\tLearning\tcan\thelp\thumans\tlearn\t(Figure\t1-4):\tML\talgorithms\tcan\tbe\tinspected\tto\tsee what\tthey\thave\tlearned\t(although\tfor\tsome\talgorithms\tthis\tcan\tbe\ttricky).\tFor\tinstance,\tonce\tthe\tspam filter\thas\tbeen\ttrained\ton\tenough\tspam,\tit\tcan\teasily\tbe\tinspected\tto\treveal\tthe\tlist\tof\twords\tand combinations\tof\twords\tthat\tit\tbelieves\tare\tthe\tbest\tpredictors\tof\tspam.\tSometimes\tthis\twill\treveal unsuspected\tcorrelations\tor\tnew\ttrends,\tand\tthereby\tlead\tto\ta\tbetter\tunderstanding\tof\tthe\tproblem.\n\nApplying\tML\ttechniques\tto\tdig\tinto\tlarge\tamounts\tof\tdata\tcan\thelp\tdiscover\tpatterns\tthat\twere\tnot immediately\tapparent.\tThis\tis\tcalled\tdata\tmining.\n\nFigure\t1-4.\tMachine\tLearning\tcan\thelp\thumans\tlearn\n\nTo\tsummarize,\tMachine\tLearning\tis\tgreat\tfor:\n\nProblems\tfor\twhich\texisting\tsolutions\trequire\ta\tlot\tof\thand-tuning\tor\tlong\tlists\tof\trules:\tone\tMachine Learning\talgorithm\tcan\toften\tsimplify\tcode\tand\tperform\tbetter.\n\nComplex\tproblems\tfor\twhich\tthere\tis\tno\tgood\tsolution\tat\tall\tusing\ta\ttraditional\tapproach:\tthe\tbest Machine\tLearning\ttechniques\tcan\tfind\ta\tsolution.\n\nFluctuating\tenvironments:\ta\tMachine\tLearning\tsystem\tcan\tadapt\tto\tnew\tdata.\n\nGetting\tinsights\tabout\tcomplex\tproblems\tand\tlarge\tamounts\tof\tdata.",
      "content_length": 1355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "Types\tof\tMachine\tLearning\tSystems There\tare\tso\tmany\tdifferent\ttypes\tof\tMachine\tLearning\tsystems\tthat\tit\tis\tuseful\tto\tclassify\tthem\tin\tbroad categories\tbased\ton:\n\nWhether\tor\tnot\tthey\tare\ttrained\twith\thuman\tsupervision\t(supervised,\tunsupervised,\tsemisupervised, and\tReinforcement\tLearning)\n\nWhether\tor\tnot\tthey\tcan\tlearn\tincrementally\ton\tthe\tfly\t(online\tversus\tbatch\tlearning)\n\nWhether\tthey\twork\tby\tsimply\tcomparing\tnew\tdata\tpoints\tto\tknown\tdata\tpoints,\tor\tinstead\tdetect patterns\tin\tthe\ttraining\tdata\tand\tbuild\ta\tpredictive\tmodel,\tmuch\tlike\tscientists\tdo\t(instance-based versus\tmodel-based\tlearning)\n\nThese\tcriteria\tare\tnot\texclusive;\tyou\tcan\tcombine\tthem\tin\tany\tway\tyou\tlike.\tFor\texample,\ta\tstate-of-the- art\tspam\tfilter\tmay\tlearn\ton\tthe\tfly\tusing\ta\tdeep\tneural\tnetwork\tmodel\ttrained\tusing\texamples\tof\tspam\tand ham;\tthis\tmakes\tit\tan\tonline,\tmodel-based,\tsupervised\tlearning\tsystem.\n\nLet’s\tlook\tat\teach\tof\tthese\tcriteria\ta\tbit\tmore\tclosely.",
      "content_length": 939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Supervised/Unsupervised\tLearning Machine\tLearning\tsystems\tcan\tbe\tclassified\taccording\tto\tthe\tamount\tand\ttype\tof\tsupervision\tthey\tget during\ttraining.\tThere\tare\tfour\tmajor\tcategories:\tsupervised\tlearning,\tunsupervised\tlearning, semisupervised\tlearning,\tand\tReinforcement\tLearning.\n\nSupervised\tlearning\n\nIn\tsupervised\tlearning,\tthe\ttraining\tdata\tyou\tfeed\tto\tthe\talgorithm\tincludes\tthe\tdesired\tsolutions,\tcalled labels\t(Figure\t1-5).\n\nFigure\t1-5.\tA\tlabeled\ttraining\tset\tfor\tsupervised\tlearning\t(e.g.,\tspam\tclassification)\n\nA\ttypical\tsupervised\tlearning\ttask\tis\tclassification.\tThe\tspam\tfilter\tis\ta\tgood\texample\tof\tthis:\tit\tis\ttrained with\tmany\texample\temails\talong\twith\ttheir\tclass\t(spam\tor\tham),\tand\tit\tmust\tlearn\thow\tto\tclassify\tnew emails.\n\nAnother\ttypical\ttask\tis\tto\tpredict\ta\ttarget\tnumeric\tvalue,\tsuch\tas\tthe\tprice\tof\ta\tcar,\tgiven\ta\tset\tof\tfeatures (mileage,\tage,\tbrand,\tetc.)\tcalled\tpredictors.\tThis\tsort\tof\ttask\tis\tcalled\tregression\t(Figure\t1-6).1\tTo\ttrain the\tsystem,\tyou\tneed\tto\tgive\tit\tmany\texamples\tof\tcars,\tincluding\tboth\ttheir\tpredictors\tand\ttheir\tlabels (i.e.,\ttheir\tprices).\n\nNOTE\n\nIn\tMachine\tLearning\tan\tattribute\tis\ta\tdata\ttype\t(e.g.,\t“Mileage”),\twhile\ta\tfeature\thas\tseveral\tmeanings\tdepending\ton\tthe context,\tbut\tgenerally\tmeans\tan\tattribute\tplus\tits\tvalue\t(e.g.,\t“Mileage\t=\t15,000”).\tMany\tpeople\tuse\tthe\twords\tattribute\tand feature\tinterchangeably,\tthough.",
      "content_length": 1372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Figure\t1-6.\tRegression\n\nNote\tthat\tsome\tregression\talgorithms\tcan\tbe\tused\tfor\tclassification\tas\twell,\tand\tvice\tversa.\tFor\texample, Logistic\tRegression\tis\tcommonly\tused\tfor\tclassification,\tas\tit\tcan\toutput\ta\tvalue\tthat\tcorresponds\tto\tthe probability\tof\tbelonging\tto\ta\tgiven\tclass\t(e.g.,\t20%\tchance\tof\tbeing\tspam).\n\nHere\tare\tsome\tof\tthe\tmost\timportant\tsupervised\tlearning\talgorithms\t(covered\tin\tthis\tbook):\n\nk-Nearest\tNeighbors\n\nLinear\tRegression\n\nLogistic\tRegression\n\nSupport\tVector\tMachines\t(SVMs)\n\nDecision\tTrees\tand\tRandom\tForests\n\nNeural\tnetworks2\n\nUnsupervised\tlearning\n\nIn\tunsupervised\tlearning,\tas\tyou\tmight\tguess,\tthe\ttraining\tdata\tis\tunlabeled\t(Figure\t1-7).\tThe\tsystem\ttries to\tlearn\twithout\ta\tteacher.",
      "content_length": 709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Figure\t1-7.\tAn\tunlabeled\ttraining\tset\tfor\tunsupervised\tlearning\n\nHere\tare\tsome\tof\tthe\tmost\timportant\tunsupervised\tlearning\talgorithms\t(we\twill\tcover\tdimensionality reduction\tin\tChapter\t8):\n\nClustering\n\nk-Means\n\nHierarchical\tCluster\tAnalysis\t(HCA)\n\nExpectation\tMaximization\n\nVisualization\tand\tdimensionality\treduction\n\nPrincipal\tComponent\tAnalysis\t(PCA)\n\nKernel\tPCA\n\nLocally-Linear\tEmbedding\t(LLE)\n\nt-distributed\tStochastic\tNeighbor\tEmbedding\t(t-SNE)\n\nAssociation\trule\tlearning\n\nApriori\n\nEclat\n\nFor\texample,\tsay\tyou\thave\ta\tlot\tof\tdata\tabout\tyour\tblog’s\tvisitors.\tYou\tmay\twant\tto\trun\ta\tclustering algorithm\tto\ttry\tto\tdetect\tgroups\tof\tsimilar\tvisitors\t(Figure\t1-8).\tAt\tno\tpoint\tdo\tyou\ttell\tthe\talgorithm which\tgroup\ta\tvisitor\tbelongs\tto:\tit\tfinds\tthose\tconnections\twithout\tyour\thelp.\tFor\texample,\tit\tmight notice\tthat\t40%\tof\tyour\tvisitors\tare\tmales\twho\tlove\tcomic\tbooks\tand\tgenerally\tread\tyour\tblog\tin\tthe evening,\twhile\t20%\tare\tyoung\tsci-fi\tlovers\twho\tvisit\tduring\tthe\tweekends,\tand\tso\ton.\tIf\tyou\tuse\ta hierarchical\tclustering\talgorithm,\tit\tmay\talso\tsubdivide\teach\tgroup\tinto\tsmaller\tgroups.\tThis\tmay\thelp you\ttarget\tyour\tposts\tfor\teach\tgroup.",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Figure\t1-8.\tClustering\n\nVisualization\talgorithms\tare\talso\tgood\texamples\tof\tunsupervised\tlearning\talgorithms:\tyou\tfeed\tthem\ta\tlot of\tcomplex\tand\tunlabeled\tdata,\tand\tthey\toutput\ta\t2D\tor\t3D\trepresentation\tof\tyour\tdata\tthat\tcan\teasily\tbe plotted\t(Figure\t1-9).\tThese\talgorithms\ttry\tto\tpreserve\tas\tmuch\tstructure\tas\tthey\tcan\t(e.g.,\ttrying\tto\tkeep separate\tclusters\tin\tthe\tinput\tspace\tfrom\toverlapping\tin\tthe\tvisualization),\tso\tyou\tcan\tunderstand\thow\tthe data\tis\torganized\tand\tperhaps\tidentify\tunsuspected\tpatterns.\n\nFigure\t1-9.\tExample\tof\ta\tt-SNE\tvisualization\thighlighting\tsemantic\tclusters3\n\nA\trelated\ttask\tis\tdimensionality\treduction,\tin\twhich\tthe\tgoal\tis\tto\tsimplify\tthe\tdata\twithout\tlosing\ttoo much\tinformation.\tOne\tway\tto\tdo\tthis\tis\tto\tmerge\tseveral\tcorrelated\tfeatures\tinto\tone.\tFor\texample,\ta car’s\tmileage\tmay\tbe\tvery\tcorrelated\twith\tits\tage,\tso\tthe\tdimensionality\treduction\talgorithm\twill\tmerge them\tinto\tone\tfeature\tthat\trepresents\tthe\tcar’s\twear\tand\ttear.\tThis\tis\tcalled\tfeature\textraction.",
      "content_length": 996,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "TIP\n\nIt\tis\toften\ta\tgood\tidea\tto\ttry\tto\treduce\tthe\tdimension\tof\tyour\ttraining\tdata\tusing\ta\tdimensionality\treduction\talgorithm\tbefore\tyou feed\tit\tto\tanother\tMachine\tLearning\talgorithm\t(such\tas\ta\tsupervised\tlearning\talgorithm).\tIt\twill\trun\tmuch\tfaster,\tthe\tdata\twill\ttake up\tless\tdisk\tand\tmemory\tspace,\tand\tin\tsome\tcases\tit\tmay\talso\tperform\tbetter.\n\nYet\tanother\timportant\tunsupervised\ttask\tis\tanomaly\tdetection\t—\tfor\texample,\tdetecting\tunusual\tcredit card\ttransactions\tto\tprevent\tfraud,\tcatching\tmanufacturing\tdefects,\tor\tautomatically\tremoving\toutliers from\ta\tdataset\tbefore\tfeeding\tit\tto\tanother\tlearning\talgorithm.\tThe\tsystem\tis\ttrained\twith\tnormal instances,\tand\twhen\tit\tsees\ta\tnew\tinstance\tit\tcan\ttell\twhether\tit\tlooks\tlike\ta\tnormal\tone\tor\twhether\tit\tis likely\tan\tanomaly\t(see\tFigure\t1-10).\n\nFigure\t1-10.\tAnomaly\tdetection\n\nFinally,\tanother\tcommon\tunsupervised\ttask\tis\tassociation\trule\tlearning,\tin\twhich\tthe\tgoal\tis\tto\tdig\tinto large\tamounts\tof\tdata\tand\tdiscover\tinteresting\trelations\tbetween\tattributes.\tFor\texample,\tsuppose\tyou own\ta\tsupermarket.\tRunning\tan\tassociation\trule\ton\tyour\tsales\tlogs\tmay\treveal\tthat\tpeople\twho\tpurchase barbecue\tsauce\tand\tpotato\tchips\talso\ttend\tto\tbuy\tsteak.\tThus,\tyou\tmay\twant\tto\tplace\tthese\titems\tclose\tto each\tother.\n\nSemisupervised\tlearning\n\nSome\talgorithms\tcan\tdeal\twith\tpartially\tlabeled\ttraining\tdata,\tusually\ta\tlot\tof\tunlabeled\tdata\tand\ta\tlittle bit\tof\tlabeled\tdata.\tThis\tis\tcalled\tsemisupervised\tlearning\t(Figure\t1-11).\n\nSome\tphoto-hosting\tservices,\tsuch\tas\tGoogle\tPhotos,\tare\tgood\texamples\tof\tthis.\tOnce\tyou\tupload\tall\tyour family\tphotos\tto\tthe\tservice,\tit\tautomatically\trecognizes\tthat\tthe\tsame\tperson\tA\tshows\tup\tin\tphotos\t1,\t5, and\t11,\twhile\tanother\tperson\tB\tshows\tup\tin\tphotos\t2,\t5,\tand\t7.\tThis\tis\tthe\tunsupervised\tpart\tof\tthe algorithm\t(clustering).\tNow\tall\tthe\tsystem\tneeds\tis\tfor\tyou\tto\ttell\tit\twho\tthese\tpeople\tare.\tJust\tone\tlabel per\tperson,4\tand\tit\tis\table\tto\tname\teveryone\tin\tevery\tphoto,\twhich\tis\tuseful\tfor\tsearching\tphotos.",
      "content_length": 1978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Figure\t1-11.\tSemisupervised\tlearning\n\nMost\tsemisupervised\tlearning\talgorithms\tare\tcombinations\tof\tunsupervised\tand\tsupervised\talgorithms. For\texample,\tdeep\tbelief\tnetworks\t(DBNs)\tare\tbased\ton\tunsupervised\tcomponents\tcalled\trestricted Boltzmann\tmachines\t(RBMs)\tstacked\ton\ttop\tof\tone\tanother.\tRBMs\tare\ttrained\tsequentially\tin\tan unsupervised\tmanner,\tand\tthen\tthe\twhole\tsystem\tis\tfine-tuned\tusing\tsupervised\tlearning\ttechniques.\n\nReinforcement\tLearning\n\nReinforcement\tLearning\tis\ta\tvery\tdifferent\tbeast.\tThe\tlearning\tsystem,\tcalled\tan\tagent\tin\tthis\tcontext, can\tobserve\tthe\tenvironment,\tselect\tand\tperform\tactions,\tand\tget\trewards\tin\treturn\t(or\tpenalties\tin\tthe form\tof\tnegative\trewards,\tas\tin\tFigure\t1-12).\tIt\tmust\tthen\tlearn\tby\titself\twhat\tis\tthe\tbest\tstrategy,\tcalled\ta policy,\tto\tget\tthe\tmost\treward\tover\ttime.\tA\tpolicy\tdefines\twhat\taction\tthe\tagent\tshould\tchoose\twhen\tit\tis in\ta\tgiven\tsituation.",
      "content_length": 897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Figure\t1-12.\tReinforcement\tLearning\n\nFor\texample,\tmany\trobots\timplement\tReinforcement\tLearning\talgorithms\tto\tlearn\thow\tto\twalk. DeepMind’s\tAlphaGo\tprogram\tis\talso\ta\tgood\texample\tof\tReinforcement\tLearning:\tit\tmade\tthe\theadlines in\tMarch\t2016\twhen\tit\tbeat\tthe\tworld\tchampion\tLee\tSedol\tat\tthe\tgame\tof\tGo.\tIt\tlearned\tits\twinning policy\tby\tanalyzing\tmillions\tof\tgames,\tand\tthen\tplaying\tmany\tgames\tagainst\titself.\tNote\tthat\tlearning\twas turned\toff\tduring\tthe\tgames\tagainst\tthe\tchampion;\tAlphaGo\twas\tjust\tapplying\tthe\tpolicy\tit\thad\tlearned.",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Batch\tand\tOnline\tLearning Another\tcriterion\tused\tto\tclassify\tMachine\tLearning\tsystems\tis\twhether\tor\tnot\tthe\tsystem\tcan\tlearn incrementally\tfrom\ta\tstream\tof\tincoming\tdata.\n\nBatch\tlearning\n\nIn\tbatch\tlearning,\tthe\tsystem\tis\tincapable\tof\tlearning\tincrementally:\tit\tmust\tbe\ttrained\tusing\tall\tthe available\tdata.\tThis\twill\tgenerally\ttake\ta\tlot\tof\ttime\tand\tcomputing\tresources,\tso\tit\tis\ttypically\tdone offline.\tFirst\tthe\tsystem\tis\ttrained,\tand\tthen\tit\tis\tlaunched\tinto\tproduction\tand\truns\twithout\tlearning anymore;\tit\tjust\tapplies\twhat\tit\thas\tlearned.\tThis\tis\tcalled\toffline\tlearning.\n\nIf\tyou\twant\ta\tbatch\tlearning\tsystem\tto\tknow\tabout\tnew\tdata\t(such\tas\ta\tnew\ttype\tof\tspam),\tyou\tneed\tto train\ta\tnew\tversion\tof\tthe\tsystem\tfrom\tscratch\ton\tthe\tfull\tdataset\t(not\tjust\tthe\tnew\tdata,\tbut\talso\tthe\told data),\tthen\tstop\tthe\told\tsystem\tand\treplace\tit\twith\tthe\tnew\tone.\n\nFortunately,\tthe\twhole\tprocess\tof\ttraining,\tevaluating,\tand\tlaunching\ta\tMachine\tLearning\tsystem\tcan\tbe automated\tfairly\teasily\t(as\tshown\tin\tFigure\t1-3),\tso\teven\ta\tbatch\tlearning\tsystem\tcan\tadapt\tto\tchange. Simply\tupdate\tthe\tdata\tand\ttrain\ta\tnew\tversion\tof\tthe\tsystem\tfrom\tscratch\tas\toften\tas\tneeded.\n\nThis\tsolution\tis\tsimple\tand\toften\tworks\tfine,\tbut\ttraining\tusing\tthe\tfull\tset\tof\tdata\tcan\ttake\tmany\thours,\tso you\twould\ttypically\ttrain\ta\tnew\tsystem\tonly\tevery\t24\thours\tor\teven\tjust\tweekly.\tIf\tyour\tsystem\tneeds\tto adapt\tto\trapidly\tchanging\tdata\t(e.g.,\tto\tpredict\tstock\tprices),\tthen\tyou\tneed\ta\tmore\treactive\tsolution.\n\nAlso,\ttraining\ton\tthe\tfull\tset\tof\tdata\trequires\ta\tlot\tof\tcomputing\tresources\t(CPU,\tmemory\tspace,\tdisk space,\tdisk\tI/O,\tnetwork\tI/O,\tetc.).\tIf\tyou\thave\ta\tlot\tof\tdata\tand\tyou\tautomate\tyour\tsystem\tto\ttrain\tfrom scratch\tevery\tday,\tit\twill\tend\tup\tcosting\tyou\ta\tlot\tof\tmoney.\tIf\tthe\tamount\tof\tdata\tis\thuge,\tit\tmay\teven\tbe impossible\tto\tuse\ta\tbatch\tlearning\talgorithm.\n\nFinally,\tif\tyour\tsystem\tneeds\tto\tbe\table\tto\tlearn\tautonomously\tand\tit\thas\tlimited\tresources\t(e.g.,\ta smartphone\tapplication\tor\ta\trover\ton\tMars),\tthen\tcarrying\taround\tlarge\tamounts\tof\ttraining\tdata\tand taking\tup\ta\tlot\tof\tresources\tto\ttrain\tfor\thours\tevery\tday\tis\ta\tshowstopper.\n\nFortunately,\ta\tbetter\toption\tin\tall\tthese\tcases\tis\tto\tuse\talgorithms\tthat\tare\tcapable\tof\tlearning incrementally.\n\nOnline\tlearning\n\nIn\tonline\tlearning,\tyou\ttrain\tthe\tsystem\tincrementally\tby\tfeeding\tit\tdata\tinstances\tsequentially,\teither individually\tor\tby\tsmall\tgroups\tcalled\tmini-batches.\tEach\tlearning\tstep\tis\tfast\tand\tcheap,\tso\tthe\tsystem can\tlearn\tabout\tnew\tdata\ton\tthe\tfly,\tas\tit\tarrives\t(see\tFigure\t1-13).",
      "content_length": 2525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Figure\t1-13.\tOnline\tlearning\n\nOnline\tlearning\tis\tgreat\tfor\tsystems\tthat\treceive\tdata\tas\ta\tcontinuous\tflow\t(e.g.,\tstock\tprices)\tand\tneed\tto adapt\tto\tchange\trapidly\tor\tautonomously.\tIt\tis\talso\ta\tgood\toption\tif\tyou\thave\tlimited\tcomputing\tresources: once\tan\tonline\tlearning\tsystem\thas\tlearned\tabout\tnew\tdata\tinstances,\tit\tdoes\tnot\tneed\tthem\tanymore,\tso you\tcan\tdiscard\tthem\t(unless\tyou\twant\tto\tbe\table\tto\troll\tback\tto\ta\tprevious\tstate\tand\t“replay”\tthe\tdata). This\tcan\tsave\ta\thuge\tamount\tof\tspace.\n\nOnline\tlearning\talgorithms\tcan\talso\tbe\tused\tto\ttrain\tsystems\ton\thuge\tdatasets\tthat\tcannot\tfit\tin\tone machine’s\tmain\tmemory\t(this\tis\tcalled\tout-of-core\tlearning).\tThe\talgorithm\tloads\tpart\tof\tthe\tdata,\truns\ta training\tstep\ton\tthat\tdata,\tand\trepeats\tthe\tprocess\tuntil\tit\thas\trun\ton\tall\tof\tthe\tdata\t(see\tFigure\t1-14).\n\nWARNING\n\nThis\twhole\tprocess\tis\tusually\tdone\toffline\t(i.e.,\tnot\ton\tthe\tlive\tsystem),\tso\tonline\tlearning\tcan\tbe\ta\tconfusing\tname.\tThink\tof\tit as\tincremental\tlearning.",
      "content_length": 973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Figure\t1-14.\tUsing\tonline\tlearning\tto\thandle\thuge\tdatasets\n\nOne\timportant\tparameter\tof\tonline\tlearning\tsystems\tis\thow\tfast\tthey\tshould\tadapt\tto\tchanging\tdata:\tthis\tis called\tthe\tlearning\trate.\tIf\tyou\tset\ta\thigh\tlearning\trate,\tthen\tyour\tsystem\twill\trapidly\tadapt\tto\tnew\tdata, but\tit\twill\talso\ttend\tto\tquickly\tforget\tthe\told\tdata\t(you\tdon’t\twant\ta\tspam\tfilter\tto\tflag\tonly\tthe\tlatest\tkinds of\tspam\tit\twas\tshown).\tConversely,\tif\tyou\tset\ta\tlow\tlearning\trate,\tthe\tsystem\twill\thave\tmore\tinertia;\tthat is,\tit\twill\tlearn\tmore\tslowly,\tbut\tit\twill\talso\tbe\tless\tsensitive\tto\tnoise\tin\tthe\tnew\tdata\tor\tto\tsequences\tof nonrepresentative\tdata\tpoints.\n\nA\tbig\tchallenge\twith\tonline\tlearning\tis\tthat\tif\tbad\tdata\tis\tfed\tto\tthe\tsystem,\tthe\tsystem’s\tperformance\twill gradually\tdecline.\tIf\twe\tare\ttalking\tabout\ta\tlive\tsystem,\tyour\tclients\twill\tnotice.\tFor\texample,\tbad\tdata could\tcome\tfrom\ta\tmalfunctioning\tsensor\ton\ta\trobot,\tor\tfrom\tsomeone\tspamming\ta\tsearch\tengine\tto\ttry\tto rank\thigh\tin\tsearch\tresults.\tTo\treduce\tthis\trisk,\tyou\tneed\tto\tmonitor\tyour\tsystem\tclosely\tand\tpromptly switch\tlearning\toff\t(and\tpossibly\trevert\tto\ta\tpreviously\tworking\tstate)\tif\tyou\tdetect\ta\tdrop\tin performance.\tYou\tmay\talso\twant\tto\tmonitor\tthe\tinput\tdata\tand\treact\tto\tabnormal\tdata\t(e.g.,\tusing\tan anomaly\tdetection\talgorithm).",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Instance-Based\tVersus\tModel-Based\tLearning One\tmore\tway\tto\tcategorize\tMachine\tLearning\tsystems\tis\tby\thow\tthey\tgeneralize.\tMost\tMachine Learning\ttasks\tare\tabout\tmaking\tpredictions.\tThis\tmeans\tthat\tgiven\ta\tnumber\tof\ttraining\texamples,\tthe system\tneeds\tto\tbe\table\tto\tgeneralize\tto\texamples\tit\thas\tnever\tseen\tbefore.\tHaving\ta\tgood\tperformance measure\ton\tthe\ttraining\tdata\tis\tgood,\tbut\tinsufficient;\tthe\ttrue\tgoal\tis\tto\tperform\twell\ton\tnew\tinstances.\n\nThere\tare\ttwo\tmain\tapproaches\tto\tgeneralization:\tinstance-based\tlearning\tand\tmodel-based\tlearning.\n\nInstance-based\tlearning\n\nPossibly\tthe\tmost\ttrivial\tform\tof\tlearning\tis\tsimply\tto\tlearn\tby\theart.\tIf\tyou\twere\tto\tcreate\ta\tspam\tfilter this\tway,\tit\twould\tjust\tflag\tall\temails\tthat\tare\tidentical\tto\temails\tthat\thave\talready\tbeen\tflagged\tby\tusers —\tnot\tthe\tworst\tsolution,\tbut\tcertainly\tnot\tthe\tbest.\n\nInstead\tof\tjust\tflagging\temails\tthat\tare\tidentical\tto\tknown\tspam\temails,\tyour\tspam\tfilter\tcould\tbe programmed\tto\talso\tflag\temails\tthat\tare\tvery\tsimilar\tto\tknown\tspam\temails.\tThis\trequires\ta\tmeasure\tof similarity\tbetween\ttwo\temails.\tA\t(very\tbasic)\tsimilarity\tmeasure\tbetween\ttwo\temails\tcould\tbe\tto\tcount the\tnumber\tof\twords\tthey\thave\tin\tcommon.\tThe\tsystem\twould\tflag\tan\temail\tas\tspam\tif\tit\thas\tmany\twords in\tcommon\twith\ta\tknown\tspam\temail.\n\nThis\tis\tcalled\tinstance-based\tlearning:\tthe\tsystem\tlearns\tthe\texamples\tby\theart,\tthen\tgeneralizes\tto\tnew cases\tusing\ta\tsimilarity\tmeasure\t(Figure\t1-15).\n\nFigure\t1-15.\tInstance-based\tlearning\n\nModel-based\tlearning\n\nAnother\tway\tto\tgeneralize\tfrom\ta\tset\tof\texamples\tis\tto\tbuild\ta\tmodel\tof\tthese\texamples,\tthen\tuse\tthat model\tto\tmake\tpredictions.\tThis\tis\tcalled\tmodel-based\tlearning\t(Figure\t1-16).",
      "content_length": 1677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Figure\t1-16.\tModel-based\tlearning\n\nFor\texample,\tsuppose\tyou\twant\tto\tknow\tif\tmoney\tmakes\tpeople\thappy,\tso\tyou\tdownload\tthe\tBetter\tLife Index\tdata\tfrom\tthe\tOECD’s\twebsite\tas\twell\tas\tstats\tabout\tGDP\tper\tcapita\tfrom\tthe\tIMF’s\twebsite.\tThen you\tjoin\tthe\ttables\tand\tsort\tby\tGDP\tper\tcapita.\tTable\t1-1\tshows\tan\texcerpt\tof\twhat\tyou\tget.\n\nTable\t1-1.\tDoes\tmoney\tmake\tpeople happier?\n\nCountry\n\nGDP\tper\tcapita\t(USD) Life\tsatisfaction\n\nHungary\n\n12,240\n\n4.9\n\nKorea\n\n27,195\n\n5.8\n\nFrance\n\n37,675\n\n6.5\n\nAustralia\n\n50,962\n\n7.3\n\nUnited\tStates 55,805\n\n7.2\n\nLet’s\tplot\tthe\tdata\tfor\ta\tfew\trandom\tcountries\t(Figure\t1-17).",
      "content_length": 597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Figure\t1-17.\tDo\tyou\tsee\ta\ttrend\there?\n\nThere\tdoes\tseem\tto\tbe\ta\ttrend\there!\tAlthough\tthe\tdata\tis\tnoisy\t(i.e.,\tpartly\trandom),\tit\tlooks\tlike\tlife satisfaction\tgoes\tup\tmore\tor\tless\tlinearly\tas\tthe\tcountry’s\tGDP\tper\tcapita\tincreases.\tSo\tyou\tdecide\tto model\tlife\tsatisfaction\tas\ta\tlinear\tfunction\tof\tGDP\tper\tcapita.\tThis\tstep\tis\tcalled\tmodel\tselection:\tyou selected\ta\tlinear\tmodel\tof\tlife\tsatisfaction\twith\tjust\tone\tattribute,\tGDP\tper\tcapita\t(Equation\t1-1).\n\nEquation\t1-1.\tA\tsimple\tlinear\tmodel\n\nThis\tmodel\thas\ttwo\tmodel\tparameters,\tθ0\tand\tθ1.5\tBy\ttweaking\tthese\tparameters,\tyou\tcan\tmake\tyour model\trepresent\tany\tlinear\tfunction,\tas\tshown\tin\tFigure\t1-18.\n\nFigure\t1-18.\tA\tfew\tpossible\tlinear\tmodels\n\nBefore\tyou\tcan\tuse\tyour\tmodel,\tyou\tneed\tto\tdefine\tthe\tparameter\tvalues\tθ0\tand\tθ1.\tHow\tcan\tyou\tknow",
      "content_length": 792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "which\tvalues\twill\tmake\tyour\tmodel\tperform\tbest?\tTo\tanswer\tthis\tquestion,\tyou\tneed\tto\tspecify\ta performance\tmeasure.\tYou\tcan\teither\tdefine\ta\tutility\tfunction\t(or\tfitness\tfunction)\tthat\tmeasures\thow good\tyour\tmodel\tis,\tor\tyou\tcan\tdefine\ta\tcost\tfunction\tthat\tmeasures\thow\tbad\tit\tis.\tFor\tlinear\tregression problems,\tpeople\ttypically\tuse\ta\tcost\tfunction\tthat\tmeasures\tthe\tdistance\tbetween\tthe\tlinear\tmodel’s predictions\tand\tthe\ttraining\texamples;\tthe\tobjective\tis\tto\tminimize\tthis\tdistance.\n\nThis\tis\twhere\tthe\tLinear\tRegression\talgorithm\tcomes\tin:\tyou\tfeed\tit\tyour\ttraining\texamples\tand\tit\tfinds the\tparameters\tthat\tmake\tthe\tlinear\tmodel\tfit\tbest\tto\tyour\tdata.\tThis\tis\tcalled\ttraining\tthe\tmodel.\tIn\tour case\tthe\talgorithm\tfinds\tthat\tthe\toptimal\tparameter\tvalues\tare\tθ0\t=\t4.85\tand\tθ1\t=\t4.91\t×\t10–5. Now\tthe\tmodel\tfits\tthe\ttraining\tdata\tas\tclosely\tas\tpossible\t(for\ta\tlinear\tmodel),\tas\tyou\tcan\tsee\tin Figure\t1-19.\n\nFigure\t1-19.\tThe\tlinear\tmodel\tthat\tfits\tthe\ttraining\tdata\tbest\n\nYou\tare\tfinally\tready\tto\trun\tthe\tmodel\tto\tmake\tpredictions.\tFor\texample,\tsay\tyou\twant\tto\tknow\thow happy\tCypriots\tare,\tand\tthe\tOECD\tdata\tdoes\tnot\thave\tthe\tanswer.\tFortunately,\tyou\tcan\tuse\tyour\tmodel\tto make\ta\tgood\tprediction:\tyou\tlook\tup\tCyprus’s\tGDP\tper\tcapita,\tfind\t$22,587,\tand\tthen\tapply\tyour\tmodel and\tfind\tthat\tlife\tsatisfaction\tis\tlikely\tto\tbe\tsomewhere\taround\t4.85\t+\t22,587\t×\t4.91\t×\t10-5\t=\t5.96. To\twhet\tyour\tappetite,\tExample\t1-1\tshows\tthe\tPython\tcode\tthat\tloads\tthe\tdata,\tprepares\tit,6\tcreates\ta scatterplot\tfor\tvisualization,\tand\tthen\ttrains\ta\tlinear\tmodel\tand\tmakes\ta\tprediction.7\n\nExample\t1-1.\tTraining\tand\trunning\ta\tlinear\tmodel\tusing\tScikit-Learn import\tmatplotlib import\tmatplotlib.pyplot\tas\tplt import\tnumpy\tas\tnp import\tpandas\tas\tpd import\tsklearn\n\n#\tLoad\tthe\tdata oecd_bli\t=\tpd.read_csv(\"oecd_bli_2015.csv\",\tthousands=',') gdp_per_capita\t=\tpd.read_csv(\"gdp_per_capita.csv\",thousands=',',delimiter='\\t', \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tencoding='latin1',\tna_values=\"n/a\")\n\n#\tPrepare\tthe\tdata country_stats\t=\tprepare_country_stats(oecd_bli,\tgdp_per_capita) X\t=\tnp.c_[country_stats[\"GDP\tper\tcapita\"]]",
      "content_length": 2083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "y\t=\tnp.c_[country_stats[\"Life\tsatisfaction\"]]\n\n#\tVisualize\tthe\tdata country_stats.plot(kind='scatter',\tx=\"GDP\tper\tcapita\",\ty='Life\tsatisfaction') plt.show()\n\n#\tSelect\ta\tlinear\tmodel model\t=\tsklearn.linear_model.LinearRegression()\n\n#\tTrain\tthe\tmodel model.fit(X,\ty)\n\n#\tMake\ta\tprediction\tfor\tCyprus X_new\t=\t[[22587]]\t\t#\tCyprus'\tGDP\tper\tcapita print(model.predict(X_new))\t#\toutputs\t[[\t5.96242338]]\n\nNOTE\n\nIf\tyou\thad\tused\tan\tinstance-based\tlearning\talgorithm\tinstead,\tyou\twould\thave\tfound\tthat\tSlovenia\thas\tthe\tclosest\tGDP\tper\tcapita to\tthat\tof\tCyprus\t($20,732),\tand\tsince\tthe\tOECD\tdata\ttells\tus\tthat\tSlovenians’\tlife\tsatisfaction\tis\t5.7,\tyou\twould\thave\tpredicted\ta life\tsatisfaction\tof\t5.7\tfor\tCyprus.\tIf\tyou\tzoom\tout\ta\tbit\tand\tlook\tat\tthe\ttwo\tnext\tclosest\tcountries,\tyou\twill\tfind\tPortugal\tand Spain\twith\tlife\tsatisfactions\tof\t5.1\tand\t6.5,\trespectively.\tAveraging\tthese\tthree\tvalues,\tyou\tget\t5.77,\twhich\tis\tpretty\tclose\tto\tyour model-based\tprediction.\tThis\tsimple\talgorithm\tis\tcalled\tk-Nearest\tNeighbors\tregression\t(in\tthis\texample,\tk\t=\t3).\n\nReplacing\tthe\tLinear\tRegression\tmodel\twith\tk-Nearest\tNeighbors\tregression\tin\tthe\tprevious\tcode\tis\tas\tsimple\tas\treplacing\tthis line:\n\nmodel\t=\tsklearn.linear_model.LinearRegression()\n\nwith\tthis\tone:\n\nmodel\t=\tsklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\n\nIf\tall\twent\twell,\tyour\tmodel\twill\tmake\tgood\tpredictions.\tIf\tnot,\tyou\tmay\tneed\tto\tuse\tmore\tattributes (employment\trate,\thealth,\tair\tpollution,\tetc.),\tget\tmore\tor\tbetter\tquality\ttraining\tdata,\tor\tperhaps\tselect\ta more\tpowerful\tmodel\t(e.g.,\ta\tPolynomial\tRegression\tmodel).\n\nIn\tsummary:\n\nYou\tstudied\tthe\tdata.\n\nYou\tselected\ta\tmodel.\n\nYou\ttrained\tit\ton\tthe\ttraining\tdata\t(i.e.,\tthe\tlearning\talgorithm\tsearched\tfor\tthe\tmodel\tparameter values\tthat\tminimize\ta\tcost\tfunction).\n\nFinally,\tyou\tapplied\tthe\tmodel\tto\tmake\tpredictions\ton\tnew\tcases\t(this\tis\tcalled\tinference),\thoping that\tthis\tmodel\twill\tgeneralize\twell.\n\nThis\tis\twhat\ta\ttypical\tMachine\tLearning\tproject\tlooks\tlike.\tIn\tChapter\t2\tyou\twill\texperience\tthis\tfirst- hand\tby\tgoing\tthrough\tan\tend-to-end\tproject.\n\nWe\thave\tcovered\ta\tlot\tof\tground\tso\tfar:\tyou\tnow\tknow\twhat\tMachine\tLearning\tis\treally\tabout,\twhy\tit\tis useful,\twhat\tsome\tof\tthe\tmost\tcommon\tcategories\tof\tML\tsystems\tare,\tand\twhat\ta\ttypical\tproject\tworkflow looks\tlike.\tNow\tlet’s\tlook\tat\twhat\tcan\tgo\twrong\tin\tlearning\tand\tprevent\tyou\tfrom\tmaking\taccurate",
      "content_length": 2359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "predictions.",
      "content_length": 12,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Main\tChallenges\tof\tMachine\tLearning In\tshort,\tsince\tyour\tmain\ttask\tis\tto\tselect\ta\tlearning\talgorithm\tand\ttrain\tit\ton\tsome\tdata,\tthe\ttwo\tthings\tthat can\tgo\twrong\tare\t“bad\talgorithm”\tand\t“bad\tdata.”\tLet’s\tstart\twith\texamples\tof\tbad\tdata.",
      "content_length": 235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Insufficient\tQuantity\tof\tTraining\tData For\ta\ttoddler\tto\tlearn\twhat\tan\tapple\tis,\tall\tit\ttakes\tis\tfor\tyou\tto\tpoint\tto\tan\tapple\tand\tsay\t“apple” (possibly\trepeating\tthis\tprocedure\ta\tfew\ttimes).\tNow\tthe\tchild\tis\table\tto\trecognize\tapples\tin\tall\tsorts\tof colors\tand\tshapes.\tGenius.\n\nMachine\tLearning\tis\tnot\tquite\tthere\tyet;\tit\ttakes\ta\tlot\tof\tdata\tfor\tmost\tMachine\tLearning\talgorithms\tto work\tproperly.\tEven\tfor\tvery\tsimple\tproblems\tyou\ttypically\tneed\tthousands\tof\texamples,\tand\tfor complex\tproblems\tsuch\tas\timage\tor\tspeech\trecognition\tyou\tmay\tneed\tmillions\tof\texamples\t(unless\tyou can\treuse\tparts\tof\tan\texisting\tmodel).",
      "content_length": 612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "THE\tUNREASONABLE\tEFFECTIVENESS\tOF\tDATA\n\nIn\ta\tfamous\tpaper\tpublished\tin\t2001,\tMicrosoft\tresearchers\tMichele\tBanko\tand\tEric\tBrill\tshowed\tthat\tvery\tdifferent\tMachine\tLearning algorithms,\tincluding\tfairly\tsimple\tones,\tperformed\talmost\tidentically\twell\ton\ta\tcomplex\tproblem\tof\tnatural\tlanguage\tdisambiguation8\tonce they\twere\tgiven\tenough\tdata\t(as\tyou\tcan\tsee\tin\tFigure\t1-20).\n\nFigure\t1-20.\tThe\timportance\tof\tdata\tversus\talgorithms9\n\nAs\tthe\tauthors\tput\tit:\t“these\tresults\tsuggest\tthat\twe\tmay\twant\tto\treconsider\tthe\ttrade-off\tbetween\tspending\ttime\tand\tmoney\ton\talgorithm development\tversus\tspending\tit\ton\tcorpus\tdevelopment.”\n\nThe\tidea\tthat\tdata\tmatters\tmore\tthan\talgorithms\tfor\tcomplex\tproblems\twas\tfurther\tpopularized\tby\tPeter\tNorvig\tet\tal.\tin\ta\tpaper\ttitled “The\tUnreasonable\tEffectiveness\tof\tData”\tpublished\tin\t2009.10\tIt\tshould\tbe\tnoted,\thowever,\tthat\tsmall-\tand\tmedium-sized\tdatasets\tare still\tvery\tcommon,\tand\tit\tis\tnot\talways\teasy\tor\tcheap\tto\tget\textra\ttraining\tdata,\tso\tdon’t\tabandon\talgorithms\tjust\tyet.",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Nonrepresentative\tTraining\tData In\torder\tto\tgeneralize\twell,\tit\tis\tcrucial\tthat\tyour\ttraining\tdata\tbe\trepresentative\tof\tthe\tnew\tcases\tyou want\tto\tgeneralize\tto.\tThis\tis\ttrue\twhether\tyou\tuse\tinstance-based\tlearning\tor\tmodel-based\tlearning.\n\nFor\texample,\tthe\tset\tof\tcountries\twe\tused\tearlier\tfor\ttraining\tthe\tlinear\tmodel\twas\tnot\tperfectly representative;\ta\tfew\tcountries\twere\tmissing.\tFigure\t1-21\tshows\twhat\tthe\tdata\tlooks\tlike\twhen\tyou\tadd the\tmissing\tcountries.\n\nFigure\t1-21.\tA\tmore\trepresentative\ttraining\tsample\n\nIf\tyou\ttrain\ta\tlinear\tmodel\ton\tthis\tdata,\tyou\tget\tthe\tsolid\tline,\twhile\tthe\told\tmodel\tis\trepresented\tby\tthe dotted\tline.\tAs\tyou\tcan\tsee,\tnot\tonly\tdoes\tadding\ta\tfew\tmissing\tcountries\tsignificantly\talter\tthe\tmodel,\tbut it\tmakes\tit\tclear\tthat\tsuch\ta\tsimple\tlinear\tmodel\tis\tprobably\tnever\tgoing\tto\twork\twell.\tIt\tseems\tthat\tvery rich\tcountries\tare\tnot\thappier\tthan\tmoderately\trich\tcountries\t(in\tfact\tthey\tseem\tunhappier),\tand conversely\tsome\tpoor\tcountries\tseem\thappier\tthan\tmany\trich\tcountries.\n\nBy\tusing\ta\tnonrepresentative\ttraining\tset,\twe\ttrained\ta\tmodel\tthat\tis\tunlikely\tto\tmake\taccurate predictions,\tespecially\tfor\tvery\tpoor\tand\tvery\trich\tcountries.\n\nIt\tis\tcrucial\tto\tuse\ta\ttraining\tset\tthat\tis\trepresentative\tof\tthe\tcases\tyou\twant\tto\tgeneralize\tto.\tThis\tis\toften harder\tthan\tit\tsounds:\tif\tthe\tsample\tis\ttoo\tsmall,\tyou\twill\thave\tsampling\tnoise\t(i.e.,\tnonrepresentative data\tas\ta\tresult\tof\tchance),\tbut\teven\tvery\tlarge\tsamples\tcan\tbe\tnonrepresentative\tif\tthe\tsampling\tmethod is\tflawed.\tThis\tis\tcalled\tsampling\tbias.",
      "content_length": 1531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "A\tFAMOUS\tEXAMPLE\tOF\tSAMPLING\tBIAS\n\nPerhaps\tthe\tmost\tfamous\texample\tof\tsampling\tbias\thappened\tduring\tthe\tUS\tpresidential\telection\tin\t1936,\twhich\tpitted\tLandon\tagainst Roosevelt:\tthe\tLiterary\tDigest\tconducted\ta\tvery\tlarge\tpoll,\tsending\tmail\tto\tabout\t10\tmillion\tpeople.\tIt\tgot\t2.4\tmillion\tanswers,\tand predicted\twith\thigh\tconfidence\tthat\tLandon\twould\tget\t57%\tof\tthe\tvotes.\tInstead,\tRoosevelt\twon\twith\t62%\tof\tthe\tvotes.\tThe\tflaw\twas\tin the\tLiterary\tDigest’s\tsampling\tmethod:\n\nFirst,\tto\tobtain\tthe\taddresses\tto\tsend\tthe\tpolls\tto,\tthe\tLiterary\tDigest\tused\ttelephone\tdirectories,\tlists\tof\tmagazine\tsubscribers,\tclub membership\tlists,\tand\tthe\tlike.\tAll\tof\tthese\tlists\ttend\tto\tfavor\twealthier\tpeople,\twho\tare\tmore\tlikely\tto\tvote\tRepublican\t(hence Landon).\n\nSecond,\tless\tthan\t25%\tof\tthe\tpeople\twho\treceived\tthe\tpoll\tanswered.\tAgain,\tthis\tintroduces\ta\tsampling\tbias,\tby\truling\tout\tpeople who\tdon’t\tcare\tmuch\tabout\tpolitics,\tpeople\twho\tdon’t\tlike\tthe\tLiterary\tDigest,\tand\tother\tkey\tgroups.\tThis\tis\ta\tspecial\ttype\tof sampling\tbias\tcalled\tnonresponse\tbias.\n\nHere\tis\tanother\texample:\tsay\tyou\twant\tto\tbuild\ta\tsystem\tto\trecognize\tfunk\tmusic\tvideos.\tOne\tway\tto\tbuild\tyour\ttraining\tset\tis\tto\tsearch “funk\tmusic”\ton\tYouTube\tand\tuse\tthe\tresulting\tvideos.\tBut\tthis\tassumes\tthat\tYouTube’s\tsearch\tengine\treturns\ta\tset\tof\tvideos\tthat\tare representative\tof\tall\tthe\tfunk\tmusic\tvideos\ton\tYouTube.\tIn\treality,\tthe\tsearch\tresults\tare\tlikely\tto\tbe\tbiased\ttoward\tpopular\tartists\t(and\tif you\tlive\tin\tBrazil\tyou\twill\tget\ta\tlot\tof\t“funk\tcarioca”\tvideos,\twhich\tsound\tnothing\tlike\tJames\tBrown).\tOn\tthe\tother\thand,\thow\telse\tcan you\tget\ta\tlarge\ttraining\tset?",
      "content_length": 1619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Poor-Quality\tData Obviously,\tif\tyour\ttraining\tdata\tis\tfull\tof\terrors,\toutliers,\tand\tnoise\t(e.g.,\tdue\tto\tpoor-quality measurements),\tit\twill\tmake\tit\tharder\tfor\tthe\tsystem\tto\tdetect\tthe\tunderlying\tpatterns,\tso\tyour\tsystem\tis less\tlikely\tto\tperform\twell.\tIt\tis\toften\twell\tworth\tthe\teffort\tto\tspend\ttime\tcleaning\tup\tyour\ttraining\tdata. The\ttruth\tis,\tmost\tdata\tscientists\tspend\ta\tsignificant\tpart\tof\ttheir\ttime\tdoing\tjust\tthat.\tFor\texample:\n\nIf\tsome\tinstances\tare\tclearly\toutliers,\tit\tmay\thelp\tto\tsimply\tdiscard\tthem\tor\ttry\tto\tfix\tthe\terrors manually.\n\nIf\tsome\tinstances\tare\tmissing\ta\tfew\tfeatures\t(e.g.,\t5%\tof\tyour\tcustomers\tdid\tnot\tspecify\ttheir\tage), you\tmust\tdecide\twhether\tyou\twant\tto\tignore\tthis\tattribute\taltogether,\tignore\tthese\tinstances,\tfill\tin\tthe missing\tvalues\t(e.g.,\twith\tthe\tmedian\tage),\tor\ttrain\tone\tmodel\twith\tthe\tfeature\tand\tone\tmodel\twithout it,\tand\tso\ton.",
      "content_length": 871,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Irrelevant\tFeatures As\tthe\tsaying\tgoes:\tgarbage\tin,\tgarbage\tout.\tYour\tsystem\twill\tonly\tbe\tcapable\tof\tlearning\tif\tthe\ttraining data\tcontains\tenough\trelevant\tfeatures\tand\tnot\ttoo\tmany\tirrelevant\tones.\tA\tcritical\tpart\tof\tthe\tsuccess\tof\ta Machine\tLearning\tproject\tis\tcoming\tup\twith\ta\tgood\tset\tof\tfeatures\tto\ttrain\ton.\tThis\tprocess,\tcalled feature\tengineering,\tinvolves:\n\nFeature\tselection:\tselecting\tthe\tmost\tuseful\tfeatures\tto\ttrain\ton\tamong\texisting\tfeatures.\n\nFeature\textraction:\tcombining\texisting\tfeatures\tto\tproduce\ta\tmore\tuseful\tone\t(as\twe\tsaw\tearlier, dimensionality\treduction\talgorithms\tcan\thelp).\n\nCreating\tnew\tfeatures\tby\tgathering\tnew\tdata.\n\nNow\tthat\twe\thave\tlooked\tat\tmany\texamples\tof\tbad\tdata,\tlet’s\tlook\tat\ta\tcouple\tof\texamples\tof\tbad algorithms.",
      "content_length": 757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Overfitting\tthe\tTraining\tData Say\tyou\tare\tvisiting\ta\tforeign\tcountry\tand\tthe\ttaxi\tdriver\trips\tyou\toff.\tYou\tmight\tbe\ttempted\tto\tsay\tthat\tall taxi\tdrivers\tin\tthat\tcountry\tare\tthieves.\tOvergeneralizing\tis\tsomething\tthat\twe\thumans\tdo\tall\ttoo\toften,\tand unfortunately\tmachines\tcan\tfall\tinto\tthe\tsame\ttrap\tif\twe\tare\tnot\tcareful.\tIn\tMachine\tLearning\tthis\tis\tcalled overfitting:\tit\tmeans\tthat\tthe\tmodel\tperforms\twell\ton\tthe\ttraining\tdata,\tbut\tit\tdoes\tnot\tgeneralize\twell.\n\nFigure\t1-22\tshows\tan\texample\tof\ta\thigh-degree\tpolynomial\tlife\tsatisfaction\tmodel\tthat\tstrongly\toverfits the\ttraining\tdata.\tEven\tthough\tit\tperforms\tmuch\tbetter\ton\tthe\ttraining\tdata\tthan\tthe\tsimple\tlinear\tmodel, would\tyou\treally\ttrust\tits\tpredictions?\n\nFigure\t1-22.\tOverfitting\tthe\ttraining\tdata\n\nComplex\tmodels\tsuch\tas\tdeep\tneural\tnetworks\tcan\tdetect\tsubtle\tpatterns\tin\tthe\tdata,\tbut\tif\tthe\ttraining\tset is\tnoisy,\tor\tif\tit\tis\ttoo\tsmall\t(which\tintroduces\tsampling\tnoise),\tthen\tthe\tmodel\tis\tlikely\tto\tdetect\tpatterns in\tthe\tnoise\titself.\tObviously\tthese\tpatterns\twill\tnot\tgeneralize\tto\tnew\tinstances.\tFor\texample,\tsay\tyou feed\tyour\tlife\tsatisfaction\tmodel\tmany\tmore\tattributes,\tincluding\tuninformative\tones\tsuch\tas\tthe\tcountry’s name.\tIn\tthat\tcase,\ta\tcomplex\tmodel\tmay\tdetect\tpatterns\tlike\tthe\tfact\tthat\tall\tcountries\tin\tthe\ttraining\tdata with\ta\tw\tin\ttheir\tname\thave\ta\tlife\tsatisfaction\tgreater\tthan\t7:\tNew\tZealand\t(7.3),\tNorway\t(7.4),\tSweden (7.2),\tand\tSwitzerland\t(7.5).\tHow\tconfident\tare\tyou\tthat\tthe\tW-satisfaction\trule\tgeneralizes\tto\tRwanda\tor Zimbabwe?\tObviously\tthis\tpattern\toccurred\tin\tthe\ttraining\tdata\tby\tpure\tchance,\tbut\tthe\tmodel\thas\tno\tway to\ttell\twhether\ta\tpattern\tis\treal\tor\tsimply\tthe\tresult\tof\tnoise\tin\tthe\tdata.\n\nWARNING\n\nOverfitting\thappens\twhen\tthe\tmodel\tis\ttoo\tcomplex\trelative\tto\tthe\tamount\tand\tnoisiness\tof\tthe\ttraining\tdata.\tThe\tpossible solutions\tare:\n\nTo\tsimplify\tthe\tmodel\tby\tselecting\tone\twith\tfewer\tparameters\t(e.g.,\ta\tlinear\tmodel\trather\tthan\ta\thigh-degree\tpolynomial model),\tby\treducing\tthe\tnumber\tof\tattributes\tin\tthe\ttraining\tdata\tor\tby\tconstraining\tthe\tmodel\n\nTo\tgather\tmore\ttraining\tdata\n\nTo\treduce\tthe\tnoise\tin\tthe\ttraining\tdata\t(e.g.,\tfix\tdata\terrors\tand\tremove\toutliers)\n\nConstraining\ta\tmodel\tto\tmake\tit\tsimpler\tand\treduce\tthe\trisk\tof\toverfitting\tis\tcalled\tregularization.\tFor",
      "content_length": 2276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "example,\tthe\tlinear\tmodel\twe\tdefined\tearlier\thas\ttwo\tparameters,\tθ0\tand\tθ1.\tThis\tgives\tthe\tlearning algorithm\ttwo\tdegrees\tof\tfreedom\tto\tadapt\tthe\tmodel\tto\tthe\ttraining\tdata:\tit\tcan\ttweak\tboth\tthe\theight\t(θ0) and\tthe\tslope\t(θ1)\tof\tthe\tline.\tIf\twe\tforced\tθ1\t=\t0,\tthe\talgorithm\twould\thave\tonly\tone\tdegree\tof\tfreedom and\twould\thave\ta\tmuch\tharder\ttime\tfitting\tthe\tdata\tproperly:\tall\tit\tcould\tdo\tis\tmove\tthe\tline\tup\tor\tdown to\tget\tas\tclose\tas\tpossible\tto\tthe\ttraining\tinstances,\tso\tit\twould\tend\tup\taround\tthe\tmean.\tA\tvery\tsimple model\tindeed!\tIf\twe\tallow\tthe\talgorithm\tto\tmodify\tθ1\tbut\twe\tforce\tit\tto\tkeep\tit\tsmall,\tthen\tthe\tlearning algorithm\twill\teffectively\thave\tsomewhere\tin\tbetween\tone\tand\ttwo\tdegrees\tof\tfreedom.\tIt\twill\tproduce\ta simpler\tmodel\tthan\twith\ttwo\tdegrees\tof\tfreedom,\tbut\tmore\tcomplex\tthan\twith\tjust\tone.\tYou\twant\tto\tfind the\tright\tbalance\tbetween\tfitting\tthe\tdata\tperfectly\tand\tkeeping\tthe\tmodel\tsimple\tenough\tto\tensure\tthat\tit will\tgeneralize\twell.\n\nFigure\t1-23\tshows\tthree\tmodels:\tthe\tdotted\tline\trepresents\tthe\toriginal\tmodel\tthat\twas\ttrained\twith\ta\tfew countries\tmissing,\tthe\tdashed\tline\tis\tour\tsecond\tmodel\ttrained\twith\tall\tcountries,\tand\tthe\tsolid\tline\tis\ta linear\tmodel\ttrained\twith\tthe\tsame\tdata\tas\tthe\tfirst\tmodel\tbut\twith\ta\tregularization\tconstraint.\tYou\tcan\tsee that\tregularization\tforced\tthe\tmodel\tto\thave\ta\tsmaller\tslope,\twhich\tfits\ta\tbit\tless\tthe\ttraining\tdata\tthat\tthe model\twas\ttrained\ton,\tbut\tactually\tallows\tit\tto\tgeneralize\tbetter\tto\tnew\texamples.\n\nFigure\t1-23.\tRegularization\treduces\tthe\trisk\tof\toverfitting\n\nThe\tamount\tof\tregularization\tto\tapply\tduring\tlearning\tcan\tbe\tcontrolled\tby\ta\thyperparameter.\tA hyperparameter\tis\ta\tparameter\tof\ta\tlearning\talgorithm\t(not\tof\tthe\tmodel).\tAs\tsuch,\tit\tis\tnot\taffected\tby\tthe learning\talgorithm\titself;\tit\tmust\tbe\tset\tprior\tto\ttraining\tand\tremains\tconstant\tduring\ttraining.\tIf\tyou\tset\tthe regularization\thyperparameter\tto\ta\tvery\tlarge\tvalue,\tyou\twill\tget\tan\talmost\tflat\tmodel\t(a\tslope\tclose\tto zero);\tthe\tlearning\talgorithm\twill\talmost\tcertainly\tnot\toverfit\tthe\ttraining\tdata,\tbut\tit\twill\tbe\tless\tlikely\tto find\ta\tgood\tsolution.\tTuning\thyperparameters\tis\tan\timportant\tpart\tof\tbuilding\ta\tMachine\tLearning\tsystem (you\twill\tsee\ta\tdetailed\texample\tin\tthe\tnext\tchapter).",
      "content_length": 2234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "Underfitting\tthe\tTraining\tData As\tyou\tmight\tguess,\tunderfitting\tis\tthe\topposite\tof\toverfitting:\tit\toccurs\twhen\tyour\tmodel\tis\ttoo\tsimple\tto learn\tthe\tunderlying\tstructure\tof\tthe\tdata.\tFor\texample,\ta\tlinear\tmodel\tof\tlife\tsatisfaction\tis\tprone\tto underfit;\treality\tis\tjust\tmore\tcomplex\tthan\tthe\tmodel,\tso\tits\tpredictions\tare\tbound\tto\tbe\tinaccurate,\teven on\tthe\ttraining\texamples.\n\nThe\tmain\toptions\tto\tfix\tthis\tproblem\tare:\n\nSelecting\ta\tmore\tpowerful\tmodel,\twith\tmore\tparameters\n\nFeeding\tbetter\tfeatures\tto\tthe\tlearning\talgorithm\t(feature\tengineering)\n\nReducing\tthe\tconstraints\ton\tthe\tmodel\t(e.g.,\treducing\tthe\tregularization\thyperparameter)",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Stepping\tBack By\tnow\tyou\talready\tknow\ta\tlot\tabout\tMachine\tLearning.\tHowever,\twe\twent\tthrough\tso\tmany\tconcepts that\tyou\tmay\tbe\tfeeling\ta\tlittle\tlost,\tso\tlet’s\tstep\tback\tand\tlook\tat\tthe\tbig\tpicture:\n\nMachine\tLearning\tis\tabout\tmaking\tmachines\tget\tbetter\tat\tsome\ttask\tby\tlearning\tfrom\tdata,\tinstead\tof having\tto\texplicitly\tcode\trules.\n\nThere\tare\tmany\tdifferent\ttypes\tof\tML\tsystems:\tsupervised\tor\tnot,\tbatch\tor\tonline,\tinstance-based\tor model-based,\tand\tso\ton.\n\nIn\ta\tML\tproject\tyou\tgather\tdata\tin\ta\ttraining\tset,\tand\tyou\tfeed\tthe\ttraining\tset\tto\ta\tlearning\talgorithm. If\tthe\talgorithm\tis\tmodel-based\tit\ttunes\tsome\tparameters\tto\tfit\tthe\tmodel\tto\tthe\ttraining\tset\t(i.e.,\tto make\tgood\tpredictions\ton\tthe\ttraining\tset\titself),\tand\tthen\thopefully\tit\twill\tbe\table\tto\tmake\tgood predictions\ton\tnew\tcases\tas\twell.\tIf\tthe\talgorithm\tis\tinstance-based,\tit\tjust\tlearns\tthe\texamples\tby heart\tand\tuses\ta\tsimilarity\tmeasure\tto\tgeneralize\tto\tnew\tinstances.\n\nThe\tsystem\twill\tnot\tperform\twell\tif\tyour\ttraining\tset\tis\ttoo\tsmall,\tor\tif\tthe\tdata\tis\tnot\trepresentative, noisy,\tor\tpolluted\twith\tirrelevant\tfeatures\t(garbage\tin,\tgarbage\tout).\tLastly,\tyour\tmodel\tneeds\tto\tbe neither\ttoo\tsimple\t(in\twhich\tcase\tit\twill\tunderfit)\tnor\ttoo\tcomplex\t(in\twhich\tcase\tit\twill\toverfit).\n\nThere’s\tjust\tone\tlast\timportant\ttopic\tto\tcover:\tonce\tyou\thave\ttrained\ta\tmodel,\tyou\tdon’t\twant\tto\tjust “hope”\tit\tgeneralizes\tto\tnew\tcases.\tYou\twant\tto\tevaluate\tit,\tand\tfine-tune\tit\tif\tnecessary.\tLet’s\tsee\thow.",
      "content_length": 1454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Testing\tand\tValidating The\tonly\tway\tto\tknow\thow\twell\ta\tmodel\twill\tgeneralize\tto\tnew\tcases\tis\tto\tactually\ttry\tit\tout\ton\tnew cases.\tOne\tway\tto\tdo\tthat\tis\tto\tput\tyour\tmodel\tin\tproduction\tand\tmonitor\thow\twell\tit\tperforms.\tThis works\twell,\tbut\tif\tyour\tmodel\tis\thorribly\tbad,\tyour\tusers\twill\tcomplain\t—\tnot\tthe\tbest\tidea.\n\nA\tbetter\toption\tis\tto\tsplit\tyour\tdata\tinto\ttwo\tsets:\tthe\ttraining\tset\tand\tthe\ttest\tset.\tAs\tthese\tnames\timply, you\ttrain\tyour\tmodel\tusing\tthe\ttraining\tset,\tand\tyou\ttest\tit\tusing\tthe\ttest\tset.\tThe\terror\trate\ton\tnew\tcases\tis called\tthe\tgeneralization\terror\t(or\tout-of-sample\terror),\tand\tby\tevaluating\tyour\tmodel\ton\tthe\ttest\tset, you\tget\tan\testimation\tof\tthis\terror.\tThis\tvalue\ttells\tyou\thow\twell\tyour\tmodel\twill\tperform\ton\tinstances\tit has\tnever\tseen\tbefore.\n\nIf\tthe\ttraining\terror\tis\tlow\t(i.e.,\tyour\tmodel\tmakes\tfew\tmistakes\ton\tthe\ttraining\tset)\tbut\tthe\tgeneralization error\tis\thigh,\tit\tmeans\tthat\tyour\tmodel\tis\toverfitting\tthe\ttraining\tdata.\n\nTIP\n\nIt\tis\tcommon\tto\tuse\t80%\tof\tthe\tdata\tfor\ttraining\tand\thold\tout\t20%\tfor\ttesting.\n\nSo\tevaluating\ta\tmodel\tis\tsimple\tenough:\tjust\tuse\ta\ttest\tset.\tNow\tsuppose\tyou\tare\thesitating\tbetween\ttwo models\t(say\ta\tlinear\tmodel\tand\ta\tpolynomial\tmodel):\thow\tcan\tyou\tdecide?\tOne\toption\tis\tto\ttrain\tboth and\tcompare\thow\twell\tthey\tgeneralize\tusing\tthe\ttest\tset.\n\nNow\tsuppose\tthat\tthe\tlinear\tmodel\tgeneralizes\tbetter,\tbut\tyou\twant\tto\tapply\tsome\tregularization\tto\tavoid overfitting.\tThe\tquestion\tis:\thow\tdo\tyou\tchoose\tthe\tvalue\tof\tthe\tregularization\thyperparameter?\tOne option\tis\tto\ttrain\t100\tdifferent\tmodels\tusing\t100\tdifferent\tvalues\tfor\tthis\thyperparameter.\tSuppose\tyou find\tthe\tbest\thyperparameter\tvalue\tthat\tproduces\ta\tmodel\twith\tthe\tlowest\tgeneralization\terror,\tsay\tjust 5%\terror.\n\nSo\tyou\tlaunch\tthis\tmodel\tinto\tproduction,\tbut\tunfortunately\tit\tdoes\tnot\tperform\tas\twell\tas\texpected\tand produces\t15%\terrors.\tWhat\tjust\thappened?\n\nThe\tproblem\tis\tthat\tyou\tmeasured\tthe\tgeneralization\terror\tmultiple\ttimes\ton\tthe\ttest\tset,\tand\tyou\tadapted the\tmodel\tand\thyperparameters\tto\tproduce\tthe\tbest\tmodel\tfor\tthat\tset.\tThis\tmeans\tthat\tthe\tmodel\tis unlikely\tto\tperform\tas\twell\ton\tnew\tdata.\n\nA\tcommon\tsolution\tto\tthis\tproblem\tis\tto\thave\ta\tsecond\tholdout\tset\tcalled\tthe\tvalidation\tset.\tYou\ttrain multiple\tmodels\twith\tvarious\thyperparameters\tusing\tthe\ttraining\tset,\tyou\tselect\tthe\tmodel\tand hyperparameters\tthat\tperform\tbest\ton\tthe\tvalidation\tset,\tand\twhen\tyou’re\thappy\twith\tyour\tmodel\tyou\trun a\tsingle\tfinal\ttest\tagainst\tthe\ttest\tset\tto\tget\tan\testimate\tof\tthe\tgeneralization\terror.\n\nTo\tavoid\t“wasting”\ttoo\tmuch\ttraining\tdata\tin\tvalidation\tsets,\ta\tcommon\ttechnique\tis\tto\tuse\tcross- validation:\tthe\ttraining\tset\tis\tsplit\tinto\tcomplementary\tsubsets,\tand\teach\tmodel\tis\ttrained\tagainst\ta different\tcombination\tof\tthese\tsubsets\tand\tvalidated\tagainst\tthe\tremaining\tparts.\tOnce\tthe\tmodel\ttype\tand hyperparameters\thave\tbeen\tselected,\ta\tfinal\tmodel\tis\ttrained\tusing\tthese\thyperparameters\ton\tthe\tfull training\tset,\tand\tthe\tgeneralized\terror\tis\tmeasured\ton\tthe\ttest\tset.",
      "content_length": 2988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "NO\tFREE\tLUNCH\tTHEOREM\n\nA\tmodel\tis\ta\tsimplified\tversion\tof\tthe\tobservations.\tThe\tsimplifications\tare\tmeant\tto\tdiscard\tthe\tsuperfluous\tdetails\tthat\tare\tunlikely\tto generalize\tto\tnew\tinstances.\tHowever,\tto\tdecide\twhat\tdata\tto\tdiscard\tand\twhat\tdata\tto\tkeep,\tyou\tmust\tmake\tassumptions.\tFor\texample, a\tlinear\tmodel\tmakes\tthe\tassumption\tthat\tthe\tdata\tis\tfundamentally\tlinear\tand\tthat\tthe\tdistance\tbetween\tthe\tinstances\tand\tthe\tstraight\tline is\tjust\tnoise,\twhich\tcan\tsafely\tbe\tignored.\n\nIn\ta\tfamous\t1996\tpaper,11\tDavid\tWolpert\tdemonstrated\tthat\tif\tyou\tmake\tabsolutely\tno\tassumption\tabout\tthe\tdata,\tthen\tthere\tis\tno\treason to\tprefer\tone\tmodel\tover\tany\tother.\tThis\tis\tcalled\tthe\tNo\tFree\tLunch\t(NFL)\ttheorem.\tFor\tsome\tdatasets\tthe\tbest\tmodel\tis\ta\tlinear model,\twhile\tfor\tother\tdatasets\tit\tis\ta\tneural\tnetwork.\tThere\tis\tno\tmodel\tthat\tis\ta\tpriori\tguaranteed\tto\twork\tbetter\t(hence\tthe\tname\tof the\ttheorem).\tThe\tonly\tway\tto\tknow\tfor\tsure\twhich\tmodel\tis\tbest\tis\tto\tevaluate\tthem\tall.\tSince\tthis\tis\tnot\tpossible,\tin\tpractice\tyou\tmake some\treasonable\tassumptions\tabout\tthe\tdata\tand\tyou\tevaluate\tonly\ta\tfew\treasonable\tmodels.\tFor\texample,\tfor\tsimple\ttasks\tyou\tmay evaluate\tlinear\tmodels\twith\tvarious\tlevels\tof\tregularization,\tand\tfor\ta\tcomplex\tproblem\tyou\tmay\tevaluate\tvarious\tneural\tnetworks.",
      "content_length": 1274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Exercises In\tthis\tchapter\twe\thave\tcovered\tsome\tof\tthe\tmost\timportant\tconcepts\tin\tMachine\tLearning.\tIn\tthe\tnext chapters\twe\twill\tdive\tdeeper\tand\twrite\tmore\tcode,\tbut\tbefore\twe\tdo,\tmake\tsure\tyou\tknow\thow\tto\tanswer the\tfollowing\tquestions:\n\n1.\t How\twould\tyou\tdefine\tMachine\tLearning?\n\n2.\t Can\tyou\tname\tfour\ttypes\tof\tproblems\twhere\tit\tshines?\n\n3.\t What\tis\ta\tlabeled\ttraining\tset?\n\n4.\t What\tare\tthe\ttwo\tmost\tcommon\tsupervised\ttasks?\n\n5.\t Can\tyou\tname\tfour\tcommon\tunsupervised\ttasks?\n\n6.\t What\ttype\tof\tMachine\tLearning\talgorithm\twould\tyou\tuse\tto\tallow\ta\trobot\tto\twalk\tin\tvarious unknown\tterrains?\n\n7.\t What\ttype\tof\talgorithm\twould\tyou\tuse\tto\tsegment\tyour\tcustomers\tinto\tmultiple\tgroups?\n\n8.\t Would\tyou\tframe\tthe\tproblem\tof\tspam\tdetection\tas\ta\tsupervised\tlearning\tproblem\tor\tan unsupervised\tlearning\tproblem?\n\n9.\t What\tis\tan\tonline\tlearning\tsystem?\n\n10.\t What\tis\tout-of-core\tlearning?\n\n11.\t What\ttype\tof\tlearning\talgorithm\trelies\ton\ta\tsimilarity\tmeasure\tto\tmake\tpredictions?\n\n12.\t What\tis\tthe\tdifference\tbetween\ta\tmodel\tparameter\tand\ta\tlearning\talgorithm’s\thyperparameter?\n\n13.\t What\tdo\tmodel-based\tlearning\talgorithms\tsearch\tfor?\tWhat\tis\tthe\tmost\tcommon\tstrategy\tthey\tuse\tto succeed?\tHow\tdo\tthey\tmake\tpredictions?\n\n14.\t Can\tyou\tname\tfour\tof\tthe\tmain\tchallenges\tin\tMachine\tLearning?\n\n15.\t If\tyour\tmodel\tperforms\tgreat\ton\tthe\ttraining\tdata\tbut\tgeneralizes\tpoorly\tto\tnew\tinstances,\twhat\tis happening?\tCan\tyou\tname\tthree\tpossible\tsolutions?\n\n16.\t What\tis\ta\ttest\tset\tand\twhy\twould\tyou\twant\tto\tuse\tit?\n\n17.\t What\tis\tthe\tpurpose\tof\ta\tvalidation\tset?\n\n18.\t What\tcan\tgo\twrong\tif\tyou\ttune\thyperparameters\tusing\tthe\ttest\tset?\n\n19.\t What\tis\tcross-validation\tand\twhy\twould\tyou\tprefer\tit\tto\ta\tvalidation\tset?",
      "content_length": 1688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "Solutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nFun\tfact:\tthis\todd-sounding\tname\tis\ta\tstatistics\tterm\tintroduced\tby\tFrancis\tGalton\twhile\the\twas\tstudying\tthe\tfact\tthat\tthe\tchildren\tof\ttall people\ttend\tto\tbe\tshorter\tthan\ttheir\tparents.\tSince\tchildren\twere\tshorter,\the\tcalled\tthis\tregression\tto\tthe\tmean.\tThis\tname\twas\tthen applied\tto\tthe\tmethods\the\tused\tto\tanalyze\tcorrelations\tbetween\tvariables.\n\n2\n\nSome\tneural\tnetwork\tarchitectures\tcan\tbe\tunsupervised,\tsuch\tas\tautoencoders\tand\trestricted\tBoltzmann\tmachines.\tThey\tcan\talso\tbe semisupervised,\tsuch\tas\tin\tdeep\tbelief\tnetworks\tand\tunsupervised\tpretraining.\n\n3\n\nNotice\thow\tanimals\tare\trather\twell\tseparated\tfrom\tvehicles,\thow\thorses\tare\tclose\tto\tdeer\tbut\tfar\tfrom\tbirds,\tand\tso\ton.\tFigure\treproduced with\tpermission\tfrom\tSocher,\tGanjoo,\tManning,\tand\tNg\t(2013),\t“T-SNE\tvisualization\tof\tthe\tsemantic\tword\tspace.”\n\n4\n\nThat’s\twhen\tthe\tsystem\tworks\tperfectly.\tIn\tpractice\tit\toften\tcreates\ta\tfew\tclusters\tper\tperson,\tand\tsometimes\tmixes\tup\ttwo\tpeople\twho look\talike,\tso\tyou\tneed\tto\tprovide\ta\tfew\tlabels\tper\tperson\tand\tmanually\tclean\tup\tsome\tclusters.\n\n5\n\nBy\tconvention,\tthe\tGreek\tletter\tθ\t(theta)\tis\tfrequently\tused\tto\trepresent\tmodel\tparameters.\n\n6\n\nThe\tcode\tassumes\tthat\tprepare_country_stats()\tis\talready\tdefined:\tit\tmerges\tthe\tGDP\tand\tlife\tsatisfaction\tdata\tinto\ta\tsingle\tPandas dataframe.\n\n7\n\nIt’s\tokay\tif\tyou\tdon’t\tunderstand\tall\tthe\tcode\tyet;\twe\twill\tpresent\tScikit-Learn\tin\tthe\tfollowing\tchapters.\n\n8\n\nFor\texample,\tknowing\twhether\tto\twrite\t“to,”\t“two,”\tor\t“too”\tdepending\ton\tthe\tcontext.\n\n9\n\nFigure\treproduced\twith\tpermission\tfrom\tBanko\tand\tBrill\t(2001),\t“Learning\tCurves\tfor\tConfusion\tSet\tDisambiguation.”\n\n10\n\n“The\tUnreasonable\tEffectiveness\tof\tData,”\tPeter\tNorvig\tet\tal.\t(2009).\n\n11\n\n“The\tLack\tof\tA\tPriori\tDistinctions\tBetween\tLearning\tAlgorithms,”\tD.\tWolperts\t(1996).",
      "content_length": 1834,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Chapter\t2.\tEnd-to-End\tMachine\tLearning Project\n\nIn\tthis\tchapter,\tyou\twill\tgo\tthrough\tan\texample\tproject\tend\tto\tend,\tpretending\tto\tbe\ta\trecently\thired\tdata scientist\tin\ta\treal\testate\tcompany.1\tHere\tare\tthe\tmain\tsteps\tyou\twill\tgo\tthrough:\n\n1.\t Look\tat\tthe\tbig\tpicture.\n\n2.\t Get\tthe\tdata.\n\n3.\t Discover\tand\tvisualize\tthe\tdata\tto\tgain\tinsights.\n\n4.\t Prepare\tthe\tdata\tfor\tMachine\tLearning\talgorithms.\n\n5.\t Select\ta\tmodel\tand\ttrain\tit.\n\n6.\t Fine-tune\tyour\tmodel.\n\n7.\t Present\tyour\tsolution.\n\n8.\t Launch,\tmonitor,\tand\tmaintain\tyour\tsystem.",
      "content_length": 532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Working\twith\tReal\tData When\tyou\tare\tlearning\tabout\tMachine\tLearning\tit\tis\tbest\tto\tactually\texperiment\twith\treal-world\tdata,\tnot just\tartificial\tdatasets.\tFortunately,\tthere\tare\tthousands\tof\topen\tdatasets\tto\tchoose\tfrom,\tranging\tacross\tall sorts\tof\tdomains.\tHere\tare\ta\tfew\tplaces\tyou\tcan\tlook\tto\tget\tdata:\n\nPopular\topen\tdata\trepositories:\n\nUC\tIrvine\tMachine\tLearning\tRepository\n\nKaggle\tdatasets\n\nAmazon’s\tAWS\tdatasets\n\nMeta\tportals\t(they\tlist\topen\tdata\trepositories):\n\nhttp://dataportals.org/\n\nhttp://opendatamonitor.eu/\n\nhttp://quandl.com/\n\nOther\tpages\tlisting\tmany\tpopular\topen\tdata\trepositories:\n\nWikipedia’s\tlist\tof\tMachine\tLearning\tdatasets\n\nQuora.com\tquestion\n\nDatasets\tsubreddit\n\nIn\tthis\tchapter\twe\tchose\tthe\tCalifornia\tHousing\tPrices\tdataset\tfrom\tthe\tStatLib\trepository2\t(see\tFigure\t2- 1).\tThis\tdataset\twas\tbased\ton\tdata\tfrom\tthe\t1990\tCalifornia\tcensus.\tIt\tis\tnot\texactly\trecent\t(you\tcould still\tafford\ta\tnice\thouse\tin\tthe\tBay\tArea\tat\tthe\ttime),\tbut\tit\thas\tmany\tqualities\tfor\tlearning,\tso\twe\twill pretend\tit\tis\trecent\tdata.\tWe\talso\tadded\ta\tcategorical\tattribute\tand\tremoved\ta\tfew\tfeatures\tfor\tteaching purposes.",
      "content_length": 1118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Figure\t2-1.\tCalifornia\thousing\tprices",
      "content_length": 37,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Look\tat\tthe\tBig\tPicture Welcome\tto\tMachine\tLearning\tHousing\tCorporation!\tThe\tfirst\ttask\tyou\tare\tasked\tto\tperform\tis\tto\tbuild\ta model\tof\thousing\tprices\tin\tCalifornia\tusing\tthe\tCalifornia\tcensus\tdata.\tThis\tdata\thas\tmetrics\tsuch\tas\tthe population,\tmedian\tincome,\tmedian\thousing\tprice,\tand\tso\ton\tfor\teach\tblock\tgroup\tin\tCalifornia.\tBlock groups\tare\tthe\tsmallest\tgeographical\tunit\tfor\twhich\tthe\tUS\tCensus\tBureau\tpublishes\tsample\tdata\t(a\tblock group\ttypically\thas\ta\tpopulation\tof\t600\tto\t3,000\tpeople).\tWe\twill\tjust\tcall\tthem\t“districts”\tfor\tshort.\n\nYour\tmodel\tshould\tlearn\tfrom\tthis\tdata\tand\tbe\table\tto\tpredict\tthe\tmedian\thousing\tprice\tin\tany\tdistrict, given\tall\tthe\tother\tmetrics.\n\nTIP\n\nSince\tyou\tare\ta\twell-organized\tdata\tscientist,\tthe\tfirst\tthing\tyou\tdo\tis\tto\tpull\tout\tyour\tMachine\tLearning\tproject\tchecklist.\tYou\tcan start\twith\tthe\tone\tin\tAppendix\tB;\tit\tshould\twork\treasonably\twell\tfor\tmost\tMachine\tLearning\tprojects\tbut\tmake\tsure\tto\tadapt\tit\tto your\tneeds.\tIn\tthis\tchapter\twe\twill\tgo\tthrough\tmany\tchecklist\titems,\tbut\twe\twill\talso\tskip\ta\tfew,\teither\tbecause\tthey\tare\tself- explanatory\tor\tbecause\tthey\twill\tbe\tdiscussed\tin\tlater\tchapters.",
      "content_length": 1137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Frame\tthe\tProblem The\tfirst\tquestion\tto\task\tyour\tboss\tis\twhat\texactly\tis\tthe\tbusiness\tobjective;\tbuilding\ta\tmodel\tis\tprobably not\tthe\tend\tgoal.\tHow\tdoes\tthe\tcompany\texpect\tto\tuse\tand\tbenefit\tfrom\tthis\tmodel?\tThis\tis\timportant because\tit\twill\tdetermine\thow\tyou\tframe\tthe\tproblem,\twhat\talgorithms\tyou\twill\tselect,\twhat\tperformance measure\tyou\twill\tuse\tto\tevaluate\tyour\tmodel,\tand\thow\tmuch\teffort\tyou\tshould\tspend\ttweaking\tit.\n\nYour\tboss\tanswers\tthat\tyour\tmodel’s\toutput\t(a\tprediction\tof\ta\tdistrict’s\tmedian\thousing\tprice)\twill\tbe\tfed to\tanother\tMachine\tLearning\tsystem\t(see\tFigure\t2-2),\talong\twith\tmany\tother\tsignals.3\tThis\tdownstream system\twill\tdetermine\twhether\tit\tis\tworth\tinvesting\tin\ta\tgiven\tarea\tor\tnot.\tGetting\tthis\tright\tis\tcritical,\tas it\tdirectly\taffects\trevenue.\n\nFigure\t2-2.\tA\tMachine\tLearning\tpipeline\tfor\treal\testate\tinvestments\n\nPIPELINES\n\nA\tsequence\tof\tdata\tprocessing\tcomponents\tis\tcalled\ta\tdata\tpipeline.\tPipelines\tare\tvery\tcommon\tin\tMachine\tLearning\tsystems,\tsince there\tis\ta\tlot\tof\tdata\tto\tmanipulate\tand\tmany\tdata\ttransformations\tto\tapply.\n\nComponents\ttypically\trun\tasynchronously.\tEach\tcomponent\tpulls\tin\ta\tlarge\tamount\tof\tdata,\tprocesses\tit,\tand\tspits\tout\tthe\tresult\tin\tanother data\tstore,\tand\tthen\tsome\ttime\tlater\tthe\tnext\tcomponent\tin\tthe\tpipeline\tpulls\tthis\tdata\tand\tspits\tout\tits\town\toutput,\tand\tso\ton.\tEach component\tis\tfairly\tself-contained:\tthe\tinterface\tbetween\tcomponents\tis\tsimply\tthe\tdata\tstore.\tThis\tmakes\tthe\tsystem\tquite\tsimple\tto grasp\t(with\tthe\thelp\tof\ta\tdata\tflow\tgraph),\tand\tdifferent\tteams\tcan\tfocus\ton\tdifferent\tcomponents.\tMoreover,\tif\ta\tcomponent\tbreaks down,\tthe\tdownstream\tcomponents\tcan\toften\tcontinue\tto\trun\tnormally\t(at\tleast\tfor\ta\twhile)\tby\tjust\tusing\tthe\tlast\toutput\tfrom\tthe\tbroken component.\tThis\tmakes\tthe\tarchitecture\tquite\trobust.\n\nOn\tthe\tother\thand,\ta\tbroken\tcomponent\tcan\tgo\tunnoticed\tfor\tsome\ttime\tif\tproper\tmonitoring\tis\tnot\timplemented.\tThe\tdata\tgets\tstale\tand the\toverall\tsystem’s\tperformance\tdrops.\n\nThe\tnext\tquestion\tto\task\tis\twhat\tthe\tcurrent\tsolution\tlooks\tlike\t(if\tany).\tIt\twill\toften\tgive\tyou\ta\treference performance,\tas\twell\tas\tinsights\ton\thow\tto\tsolve\tthe\tproblem.\tYour\tboss\tanswers\tthat\tthe\tdistrict\thousing prices\tare\tcurrently\testimated\tmanually\tby\texperts:\ta\tteam\tgathers\tup-to-date\tinformation\tabout\ta\tdistrict, and\twhen\tthey\tcannot\tget\tthe\tmedian\thousing\tprice,\tthey\testimate\tit\tusing\tcomplex\trules.\n\nThis\tis\tcostly\tand\ttime-consuming,\tand\ttheir\testimates\tare\tnot\tgreat;\tin\tcases\twhere\tthey\tmanage\tto\tfind out\tthe\tactual\tmedian\thousing\tprice,\tthey\toften\trealize\tthat\ttheir\testimates\twere\toff\tby\tmore\tthan\t10%. This\tis\twhy\tthe\tcompany\tthinks\tthat\tit\twould\tbe\tuseful\tto\ttrain\ta\tmodel\tto\tpredict\ta\tdistrict’s\tmedian housing\tprice\tgiven\tother\tdata\tabout\tthat\tdistrict.\tThe\tcensus\tdata\tlooks\tlike\ta\tgreat\tdataset\tto\texploit\tfor this\tpurpose,\tsince\tit\tincludes\tthe\tmedian\thousing\tprices\tof\tthousands\tof\tdistricts,\tas\twell\tas\tother\tdata.\n\nOkay,\twith\tall\tthis\tinformation\tyou\tare\tnow\tready\tto\tstart\tdesigning\tyour\tsystem.\tFirst,\tyou\tneed\tto\tframe",
      "content_length": 3014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "the\tproblem:\tis\tit\tsupervised,\tunsupervised,\tor\tReinforcement\tLearning?\tIs\tit\ta\tclassification\ttask,\ta regression\ttask,\tor\tsomething\telse?\tShould\tyou\tuse\tbatch\tlearning\tor\tonline\tlearning\ttechniques?\tBefore you\tread\ton,\tpause\tand\ttry\tto\tanswer\tthese\tquestions\tfor\tyourself.\n\nHave\tyou\tfound\tthe\tanswers?\tLet’s\tsee:\tit\tis\tclearly\ta\ttypical\tsupervised\tlearning\ttask\tsince\tyou\tare\tgiven labeled\ttraining\texamples\t(each\tinstance\tcomes\twith\tthe\texpected\toutput,\ti.e.,\tthe\tdistrict’s\tmedian housing\tprice).\tMoreover,\tit\tis\talso\ta\ttypical\tregression\ttask,\tsince\tyou\tare\tasked\tto\tpredict\ta\tvalue.\tMore specifically,\tthis\tis\ta\tmultivariate\tregression\tproblem\tsince\tthe\tsystem\twill\tuse\tmultiple\tfeatures\tto\tmake a\tprediction\t(it\twill\tuse\tthe\tdistrict’s\tpopulation,\tthe\tmedian\tincome,\tetc.).\tIn\tthe\tfirst\tchapter,\tyou predicted\tlife\tsatisfaction\tbased\ton\tjust\tone\tfeature,\tthe\tGDP\tper\tcapita,\tso\tit\twas\ta\tunivariate\tregression problem.\tFinally,\tthere\tis\tno\tcontinuous\tflow\tof\tdata\tcoming\tin\tthe\tsystem,\tthere\tis\tno\tparticular\tneed\tto adjust\tto\tchanging\tdata\trapidly,\tand\tthe\tdata\tis\tsmall\tenough\tto\tfit\tin\tmemory,\tso\tplain\tbatch\tlearning should\tdo\tjust\tfine.\n\nTIP\n\nIf\tthe\tdata\twas\thuge,\tyou\tcould\teither\tsplit\tyour\tbatch\tlearning\twork\tacross\tmultiple\tservers\t(using\tthe\tMapReduce\ttechnique,\tas we\twill\tsee\tlater),\tor\tyou\tcould\tuse\tan\tonline\tlearning\ttechnique\tinstead.",
      "content_length": 1356,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Select\ta\tPerformance\tMeasure Your\tnext\tstep\tis\tto\tselect\ta\tperformance\tmeasure.\tA\ttypical\tperformance\tmeasure\tfor\tregression problems\tis\tthe\tRoot\tMean\tSquare\tError\t(RMSE).\tIt\tgives\tan\tidea\tof\thow\tmuch\terror\tthe\tsystem\ttypically makes\tin\tits\tpredictions,\twith\ta\thigher\tweight\tfor\tlarge\terrors.\tEquation\t2-1\tshows\tthe\tmathematical formula\tto\tcompute\tthe\tRMSE.\n\nEquation\t2-1.\tRoot\tMean\tSquare\tError\t(RMSE)\n\nNOTATIONS\n\nThis\tequation\tintroduces\tseveral\tvery\tcommon\tMachine\tLearning\tnotations\tthat\twe\twill\tuse\tthroughout\tthis\tbook:\n\nm\tis\tthe\tnumber\tof\tinstances\tin\tthe\tdataset\tyou\tare\tmeasuring\tthe\tRMSE\ton.\n\nFor\texample,\tif\tyou\tare\tevaluating\tthe\tRMSE\ton\ta\tvalidation\tset\tof\t2,000\tdistricts,\tthen\tm\t=\t2,000.\n\nx(i)\tis\ta\tvector\tof\tall\tthe\tfeature\tvalues\t(excluding\tthe\tlabel)\tof\tthe\tith\tinstance\tin\tthe\tdataset,\tand\ty(i)\tis\tits\tlabel\t(the\tdesired output\tvalue\tfor\tthat\tinstance).\n\nFor\texample,\tif\tthe\tfirst\tdistrict\tin\tthe\tdataset\tis\tlocated\tat\tlongitude\t–118.29°,\tlatitude\t33.91°,\tand\tit\thas\t1,416\tinhabitants with\ta\tmedian\tincome\tof\t$38,372,\tand\tthe\tmedian\thouse\tvalue\tis\t$156,400\t(ignoring\tthe\tother\tfeatures\tfor\tnow),\tthen:\n\nand:\n\nX\tis\ta\tmatrix\tcontaining\tall\tthe\tfeature\tvalues\t(excluding\tlabels)\tof\tall\tinstances\tin\tthe\tdataset.\tThere\tis\tone\trow\tper\tinstance\tand the\tith\trow\tis\tequal\tto\tthe\ttranspose\tof\tx(i),\tnoted\t(x(i))T.4\n\nFor\texample,\tif\tthe\tfirst\tdistrict\tis\tas\tjust\tdescribed,\tthen\tthe\tmatrix\tX\tlooks\tlike\tthis:",
      "content_length": 1417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "h\tis\tyour\tsystem’s\tprediction\tfunction,\talso\tcalled\ta\thypothesis.\tWhen\tyour\tsystem\tis\tgiven\tan\tinstance’s\tfeature\tvector\tx(i),\tit outputs\ta\tpredicted\tvalue\tŷ(i)\t=\th(x(i))\tfor\tthat\tinstance\t(ŷ\tis\tpronounced\t“y-hat”).\n\nFor\texample,\tif\tyour\tsystem\tpredicts\tthat\tthe\tmedian\thousing\tprice\tin\tthe\tfirst\tdistrict\tis\t$158,400,\tthen\tŷ(1)\t=\th(x(1))\t= 158,400.\tThe\tprediction\terror\tfor\tthis\tdistrict\tis\tŷ(1)\t–\ty(1)\t=\t2,000.\n\nRMSE(X,h)\tis\tthe\tcost\tfunction\tmeasured\ton\tthe\tset\tof\texamples\tusing\tyour\thypothesis\th.\n\nWe\tuse\tlowercase\titalic\tfont\tfor\tscalar\tvalues\t(such\tas\tm\tor\ty(i))\tand\tfunction\tnames\t(such\tas\th),\tlowercase\tbold\tfont\tfor\tvectors\t(such as\tx(i)),\tand\tuppercase\tbold\tfont\tfor\tmatrices\t(such\tas\tX).\n\nEven\tthough\tthe\tRMSE\tis\tgenerally\tthe\tpreferred\tperformance\tmeasure\tfor\tregression\ttasks,\tin\tsome contexts\tyou\tmay\tprefer\tto\tuse\tanother\tfunction.\tFor\texample,\tsuppose\tthat\tthere\tare\tmany\toutlier\tdistricts. In\tthat\tcase,\tyou\tmay\tconsider\tusing\tthe\tMean\tAbsolute\tError\t(also\tcalled\tthe\tAverage\tAbsolute Deviation;\tsee\tEquation\t2-2):\n\nEquation\t2-2.\tMean\tAbsolute\tError\n\nBoth\tthe\tRMSE\tand\tthe\tMAE\tare\tways\tto\tmeasure\tthe\tdistance\tbetween\ttwo\tvectors:\tthe\tvector\tof predictions\tand\tthe\tvector\tof\ttarget\tvalues.\tVarious\tdistance\tmeasures,\tor\tnorms,\tare\tpossible:\n\nComputing\tthe\troot\tof\ta\tsum\tof\tsquares\t(RMSE)\tcorresponds\tto\tthe\tEuclidian\tnorm:\tit\tis\tthe\tnotion of\tdistance\tyou\tare\tfamiliar\twith.\tIt\tis\talso\tcalled\tthe\tℓ2\tnorm,\tnoted\t\t·\n\n2\t(or\tjust\t\t·\t).\n\nComputing\tthe\tsum\tof\tabsolutes\t(MAE)\tcorresponds\tto\tthe\tℓ1\tnorm,\tnoted\t\t·\t called\tthe\tManhattan\tnorm\tbecause\tit\tmeasures\tthe\tdistance\tbetween\ttwo\tpoints\tin\ta\tcity\tif\tyou\tcan only\ttravel\talong\torthogonal\tcity\tblocks.\n\n1.\tIt\tis\tsometimes\n\nMore\tgenerally,\tthe\tℓk\tnorm\tof\ta\tvector\tv\tcontaining\tn\telements\tis\tdefined\tas\n\n.\tℓ0\tjust\tgives\tthe\tnumber\tof\tnon-zero\telements\tin\tthe\tvector,\n\nand\tℓ∞\tgives\tthe\tmaximum\tabsolute\tvalue\tin\tthe\tvector.\n\nThe\thigher\tthe\tnorm\tindex,\tthe\tmore\tit\tfocuses\ton\tlarge\tvalues\tand\tneglects\tsmall\tones.\tThis\tis\twhy the\tRMSE\tis\tmore\tsensitive\tto\toutliers\tthan\tthe\tMAE.\tBut\twhen\toutliers\tare\texponentially\trare\t(like in\ta\tbell-shaped\tcurve),\tthe\tRMSE\tperforms\tvery\twell\tand\tis\tgenerally\tpreferred.",
      "content_length": 2168,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Check\tthe\tAssumptions Lastly,\tit\tis\tgood\tpractice\tto\tlist\tand\tverify\tthe\tassumptions\tthat\twere\tmade\tso\tfar\t(by\tyou\tor\tothers);\tthis can\tcatch\tserious\tissues\tearly\ton.\tFor\texample,\tthe\tdistrict\tprices\tthat\tyour\tsystem\toutputs\tare\tgoing\tto\tbe fed\tinto\ta\tdownstream\tMachine\tLearning\tsystem,\tand\twe\tassume\tthat\tthese\tprices\tare\tgoing\tto\tbe\tused\tas such.\tBut\twhat\tif\tthe\tdownstream\tsystem\tactually\tconverts\tthe\tprices\tinto\tcategories\t(e.g.,\t“cheap,” “medium,”\tor\t“expensive”)\tand\tthen\tuses\tthose\tcategories\tinstead\tof\tthe\tprices\tthemselves?\tIn\tthis\tcase, getting\tthe\tprice\tperfectly\tright\tis\tnot\timportant\tat\tall;\tyour\tsystem\tjust\tneeds\tto\tget\tthe\tcategory\tright.\tIf that’s\tso,\tthen\tthe\tproblem\tshould\thave\tbeen\tframed\tas\ta\tclassification\ttask,\tnot\ta\tregression\ttask.\tYou don’t\twant\tto\tfind\tthis\tout\tafter\tworking\ton\ta\tregression\tsystem\tfor\tmonths.\n\nFortunately,\tafter\ttalking\twith\tthe\tteam\tin\tcharge\tof\tthe\tdownstream\tsystem,\tyou\tare\tconfident\tthat\tthey\tdo indeed\tneed\tthe\tactual\tprices,\tnot\tjust\tcategories.\tGreat!\tYou’re\tall\tset,\tthe\tlights\tare\tgreen,\tand\tyou\tcan start\tcoding\tnow!",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Get\tthe\tData It’s\ttime\tto\tget\tyour\thands\tdirty.\tDon’t\thesitate\tto\tpick\tup\tyour\tlaptop\tand\twalk\tthrough\tthe\tfollowing\tcode examples\tin\ta\tJupyter\tnotebook.\tThe\tfull\tJupyter\tnotebook\tis\tavailable\tat https://github.com/ageron/handson-ml.",
      "content_length": 233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Create\tthe\tWorkspace First\tyou\twill\tneed\tto\thave\tPython\tinstalled.\tIt\tis\tprobably\talready\tinstalled\ton\tyour\tsystem.\tIf\tnot,\tyou can\tget\tit\tat\thttps://www.python.org/.5\n\nNext\tyou\tneed\tto\tcreate\ta\tworkspace\tdirectory\tfor\tyour\tMachine\tLearning\tcode\tand\tdatasets.\tOpen\ta terminal\tand\ttype\tthe\tfollowing\tcommands\t(after\tthe\t$\tprompts):\n\n$\texport\tML_PATH=\"$HOME/ml\"\t\t\t\t\t\t#\tYou\tcan\tchange\tthe\tpath\tif\tyou\tprefer $\tmkdir\t-p\t$ML_PATH\n\nYou\twill\tneed\ta\tnumber\tof\tPython\tmodules:\tJupyter,\tNumPy,\tPandas,\tMatplotlib,\tand\tScikit-Learn.\tIf\tyou already\thave\tJupyter\trunning\twith\tall\tthese\tmodules\tinstalled,\tyou\tcan\tsafely\tskip\tto\t“Download\tthe Data”.\tIf\tyou\tdon’t\thave\tthem\tyet,\tthere\tare\tmany\tways\tto\tinstall\tthem\t(and\ttheir\tdependencies).\tYou\tcan use\tyour\tsystem’s\tpackaging\tsystem\t(e.g.,\tapt-get\ton\tUbuntu,\tor\tMacPorts\tor\tHomeBrew\ton\tmacOS), install\ta\tScientific\tPython\tdistribution\tsuch\tas\tAnaconda\tand\tuse\tits\tpackaging\tsystem,\tor\tjust\tuse Python’s\town\tpackaging\tsystem,\tpip,\twhich\tis\tincluded\tby\tdefault\twith\tthe\tPython\tbinary\tinstallers\t(since Python\t2.7.9).6\tYou\tcan\tcheck\tto\tsee\tif\tpip\tis\tinstalled\tby\ttyping\tthe\tfollowing\tcommand:\n\n$\tpip3\t--version pip\t9.0.1\tfrom\t[...]/lib/python3.5/site-packages\t(python\t3.5)\n\nYou\tshould\tmake\tsure\tyou\thave\ta\trecent\tversion\tof\tpip\tinstalled,\tat\tthe\tvery\tleast\t>1.4\tto\tsupport\tbinary module\tinstallation\t(a.k.a.\twheels).\tTo\tupgrade\tthe\tpip\tmodule,\ttype:7\n\n$\tpip3\tinstall\t--upgrade\tpip Collecting\tpip [...] Successfully\tinstalled\tpip-9.0.1",
      "content_length": 1467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "CREATING\tAN\tISOLATED\tENVIRONMENT\n\nIf\tyou\twould\tlike\tto\twork\tin\tan\tisolated\tenvironment\t(which\tis\tstrongly\trecommended\tso\tyou\tcan\twork\ton\tdifferent\tprojects\twithout having\tconflicting\tlibrary\tversions),\tinstall\tvirtualenv\tby\trunning\tthe\tfollowing\tpip\tcommand:\n\n$\tpip3\tinstall\t--user\t--upgrade\tvirtualenv Collecting\tvirtualenv [...] Successfully\tinstalled\tvirtualenv\n\nNow\tyou\tcan\tcreate\tan\tisolated\tPython\tenvironment\tby\ttyping:\n\n$\tcd\t$ML_PATH $\tvirtualenv\tenv Using\tbase\tprefix\t'[...]' New\tpython\texecutable\tin\t[...]/ml/env/bin/python3.5 Also\tcreating\texecutable\tin\t[...]/ml/env/bin/python Installing\tsetuptools,\tpip,\twheel...done.\n\nNow\tevery\ttime\tyou\twant\tto\tactivate\tthis\tenvironment,\tjust\topen\ta\tterminal\tand\ttype:\n\n$\tcd\t$ML_PATH $\tsource\tenv/bin/activate\n\nWhile\tthe\tenvironment\tis\tactive,\tany\tpackage\tyou\tinstall\tusing\tpip\twill\tbe\tinstalled\tin\tthis\tisolated\tenvironment,\tand\tPython\twill\tonly\thave access\tto\tthese\tpackages\t(if\tyou\talso\twant\taccess\tto\tthe\tsystem’s\tsite\tpackages,\tyou\tshould\tcreate\tthe\tenvironment\tusing\tvirtualenv’s\t-- system-site-packages\toption).\tCheck\tout\tvirtualenv’s\tdocumentation\tfor\tmore\tinformation.\n\nNow\tyou\tcan\tinstall\tall\tthe\trequired\tmodules\tand\ttheir\tdependencies\tusing\tthis\tsimple\tpip\tcommand:\n\n$\tpip3\tinstall\t--upgrade\tjupyter\tmatplotlib\tnumpy\tpandas\tscipy\tscikit-learn Collecting\tjupyter \t\tDownloading\tjupyter-1.0.0-py2.py3-none-any.whl Collecting\tmatplotlib \t\t[...]\n\nTo\tcheck\tyour\tinstallation,\ttry\tto\timport\tevery\tmodule\tlike\tthis:\n\n$\tpython3\t-c\t\"import\tjupyter,\tmatplotlib,\tnumpy,\tpandas,\tscipy,\tsklearn\"\n\nThere\tshould\tbe\tno\toutput\tand\tno\terror.\tNow\tyou\tcan\tfire\tup\tJupyter\tby\ttyping:\n\n$\tjupyter\tnotebook [I\t15:24\tNotebookApp]\tServing\tnotebooks\tfrom\tlocal\tdirectory:\t[...]/ml [I\t15:24\tNotebookApp]\t0\tactive\tkernels [I\t15:24\tNotebookApp]\tThe\tJupyter\tNotebook\tis\trunning\tat:\thttp://localhost:8888/ [I\t15:24\tNotebookApp]\tUse\tControl-C\tto\tstop\tthis\tserver\tand\tshut\tdown\tall kernels\t(twice\tto\tskip\tconfirmation).\n\nA\tJupyter\tserver\tis\tnow\trunning\tin\tyour\tterminal,\tlistening\tto\tport\t8888.\tYou\tcan\tvisit\tthis\tserver\tby opening\tyour\tweb\tbrowser\tto\thttp://localhost:8888/\t(this\tusually\thappens\tautomatically\twhen\tthe\tserver starts).\tYou\tshould\tsee\tyour\tempty\tworkspace\tdirectory\t(containing\tonly\tthe\tenv\tdirectory\tif\tyou\tfollowed the\tpreceding\tvirtualenv\tinstructions).\n\nNow\tcreate\ta\tnew\tPython\tnotebook\tby\tclicking\ton\tthe\tNew\tbutton\tand\tselecting\tthe\tappropriate\tPython version8\t(see\tFigure\t2-3).",
      "content_length": 2425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "This\tdoes\tthree\tthings:\tfirst,\tit\tcreates\ta\tnew\tnotebook\tfile\tcalled\tUntitled.ipynb\tin\tyour\tworkspace; second,\tit\tstarts\ta\tJupyter\tPython\tkernel\tto\trun\tthis\tnotebook;\tand\tthird,\tit\topens\tthis\tnotebook\tin\ta\tnew tab.\tYou\tshould\tstart\tby\trenaming\tthis\tnotebook\tto\t“Housing”\t(this\twill\tautomatically\trename\tthe\tfile\tto Housing.ipynb)\tby\tclicking\tUntitled\tand\ttyping\tthe\tnew\tname.\n\nFigure\t2-3.\tYour\tworkspace\tin\tJupyter\n\nA\tnotebook\tcontains\ta\tlist\tof\tcells.\tEach\tcell\tcan\tcontain\texecutable\tcode\tor\tformatted\ttext.\tRight\tnow\tthe notebook\tcontains\tonly\tone\tempty\tcode\tcell,\tlabeled\t“In\t[1]:”.\tTry\ttyping\tprint(\"Hello\tworld!\")\tin the\tcell,\tand\tclick\ton\tthe\tplay\tbutton\t(see\tFigure\t2-4)\tor\tpress\tShift-Enter.\tThis\tsends\tthe\tcurrent\tcell\tto this\tnotebook’s\tPython\tkernel,\twhich\truns\tit\tand\treturns\tthe\toutput.\tThe\tresult\tis\tdisplayed\tbelow\tthe\tcell, and\tsince\twe\treached\tthe\tend\tof\tthe\tnotebook,\ta\tnew\tcell\tis\tautomatically\tcreated.\tGo\tthrough\tthe\tUser Interface\tTour\tfrom\tJupyter’s\tHelp\tmenu\tto\tlearn\tthe\tbasics.\n\nFigure\t2-4.\tHello\tworld\tPython\tnotebook",
      "content_length": 1045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Download\tthe\tData In\ttypical\tenvironments\tyour\tdata\twould\tbe\tavailable\tin\ta\trelational\tdatabase\t(or\tsome\tother\tcommon datastore)\tand\tspread\tacross\tmultiple\ttables/documents/files.\tTo\taccess\tit,\tyou\twould\tfirst\tneed\tto\tget your\tcredentials\tand\taccess\tauthorizations,9\tand\tfamiliarize\tyourself\twith\tthe\tdata\tschema.\tIn\tthis\tproject, however,\tthings\tare\tmuch\tsimpler:\tyou\twill\tjust\tdownload\ta\tsingle\tcompressed\tfile,\thousing.tgz,\twhich contains\ta\tcomma-separated\tvalue\t(CSV)\tfile\tcalled\thousing.csv\twith\tall\tthe\tdata.\n\nYou\tcould\tuse\tyour\tweb\tbrowser\tto\tdownload\tit,\tand\trun\ttar\txzf\thousing.tgz\tto\tdecompress\tthe\tfile and\textract\tthe\tCSV\tfile,\tbut\tit\tis\tpreferable\tto\tcreate\ta\tsmall\tfunction\tto\tdo\tthat.\tIt\tis\tuseful\tin\tparticular\tif data\tchanges\tregularly,\tas\tit\tallows\tyou\tto\twrite\ta\tsmall\tscript\tthat\tyou\tcan\trun\twhenever\tyou\tneed\tto fetch\tthe\tlatest\tdata\t(or\tyou\tcan\tset\tup\ta\tscheduled\tjob\tto\tdo\tthat\tautomatically\tat\tregular\tintervals). Automating\tthe\tprocess\tof\tfetching\tthe\tdata\tis\talso\tuseful\tif\tyou\tneed\tto\tinstall\tthe\tdataset\ton\tmultiple machines. Here\tis\tthe\tfunction\tto\tfetch\tthe\tdata:10\n\nimport\tos import\ttarfile from\tsix.moves\timport\turllib\n\nDOWNLOAD_ROOT\t=\t\"https://raw.githubusercontent.com/ageron/handson-ml/master/\" HOUSING_PATH\t=\tos.path.join(\"datasets\",\t\"housing\") HOUSING_URL\t=\tDOWNLOAD_ROOT\t+\t\"datasets/housing/housing.tgz\"\n\ndef\tfetch_housing_data(housing_url=HOUSING_URL,\thousing_path=HOUSING_PATH): \t\t\t\tif\tnot\tos.path.isdir(housing_path): \t\t\t\t\t\t\t\tos.makedirs(housing_path) \t\t\t\ttgz_path\t=\tos.path.join(housing_path,\t\"housing.tgz\") \t\t\t\turllib.request.urlretrieve(housing_url,\ttgz_path) \t\t\t\thousing_tgz\t=\ttarfile.open(tgz_path) \t\t\t\thousing_tgz.extractall(path=housing_path) \t\t\t\thousing_tgz.close()\n\nNow\twhen\tyou\tcall\tfetch_housing_data(),\tit\tcreates\ta\tdatasets/housing\tdirectory\tin\tyour\tworkspace, downloads\tthe\thousing.tgz\tfile,\tand\textracts\tthe\thousing.csv\tfrom\tit\tin\tthis\tdirectory.\n\nNow\tlet’s\tload\tthe\tdata\tusing\tPandas.\tOnce\tagain\tyou\tshould\twrite\ta\tsmall\tfunction\tto\tload\tthe\tdata:\n\nimport\tpandas\tas\tpd\n\ndef\tload_housing_data(housing_path=HOUSING_PATH): \t\t\t\tcsv_path\t=\tos.path.join(housing_path,\t\"housing.csv\") \t\t\t\treturn\tpd.read_csv(csv_path)\n\nThis\tfunction\treturns\ta\tPandas\tDataFrame\tobject\tcontaining\tall\tthe\tdata.",
      "content_length": 2240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "Take\ta\tQuick\tLook\tat\tthe\tData\tStructure Let’s\ttake\ta\tlook\tat\tthe\ttop\tfive\trows\tusing\tthe\tDataFrame’s\thead()\tmethod\t(see\tFigure\t2-5).\n\nFigure\t2-5.\tTop\tfive\trows\tin\tthe\tdataset\n\nEach\trow\trepresents\tone\tdistrict.\tThere\tare\t10\tattributes\t(you\tcan\tsee\tthe\tfirst\t6\tin\tthe\tscreenshot): longitude,\tlatitude,\thousing_median_age,\ttotal_rooms,\ttotal_bedrooms,\tpopulation, households,\tmedian_income,\tmedian_house_value,\tand\tocean_proximity.\n\nThe\tinfo()\tmethod\tis\tuseful\tto\tget\ta\tquick\tdescription\tof\tthe\tdata,\tin\tparticular\tthe\ttotal\tnumber\tof\trows, and\teach\tattribute’s\ttype\tand\tnumber\tof\tnon-null\tvalues\t(see\tFigure\t2-6).\n\nFigure\t2-6.\tHousing\tinfo\n\nThere\tare\t20,640\tinstances\tin\tthe\tdataset,\twhich\tmeans\tthat\tit\tis\tfairly\tsmall\tby\tMachine\tLearning standards,\tbut\tit’s\tperfect\tto\tget\tstarted.\tNotice\tthat\tthe\ttotal_bedrooms\tattribute\thas\tonly\t20,433\tnon- null\tvalues,\tmeaning\tthat\t207\tdistricts\tare\tmissing\tthis\tfeature.\tWe\twill\tneed\tto\ttake\tcare\tof\tthis\tlater.\n\nAll\tattributes\tare\tnumerical,\texcept\tthe\tocean_proximity\tfield.\tIts\ttype\tis\tobject,\tso\tit\tcould\thold\tany kind\tof\tPython\tobject,\tbut\tsince\tyou\tloaded\tthis\tdata\tfrom\ta\tCSV\tfile\tyou\tknow\tthat\tit\tmust\tbe\ta\ttext attribute.\tWhen\tyou\tlooked\tat\tthe\ttop\tfive\trows,\tyou\tprobably\tnoticed\tthat\tthe\tvalues\tin\tthe ocean_proximity\tcolumn\twere\trepetitive,\twhich\tmeans\tthat\tit\tis\tprobably\ta\tcategorical\tattribute.\tYou can\tfind\tout\twhat\tcategories\texist\tand\thow\tmany\tdistricts\tbelong\tto\teach\tcategory\tby\tusing\tthe",
      "content_length": 1447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "value_counts()\tmethod:\n\n>>>\thousing[\"ocean_proximity\"].value_counts() <1H\tOCEAN\t\t\t\t\t9136 INLAND\t\t\t\t\t\t\t\t6551 NEAR\tOCEAN\t\t\t\t2658 NEAR\tBAY\t\t\t\t\t\t2290 ISLAND\t\t\t\t\t\t\t\t\t\t\t5 Name:\tocean_proximity,\tdtype:\tint64\n\nLet’s\tlook\tat\tthe\tother\tfields.\tThe\tdescribe()\tmethod\tshows\ta\tsummary\tof\tthe\tnumerical\tattributes (Figure\t2-7).\n\nFigure\t2-7.\tSummary\tof\teach\tnumerical\tattribute\n\nThe\tcount,\tmean,\tmin,\tand\tmax\trows\tare\tself-explanatory.\tNote\tthat\tthe\tnull\tvalues\tare\tignored\t(so,\tfor example,\tcount\tof\ttotal_bedrooms\tis\t20,433,\tnot\t20,640).\tThe\tstd\trow\tshows\tthe\tstandard\tdeviation, which\tmeasures\thow\tdispersed\tthe\tvalues\tare.11\tThe\t25%,\t50%,\tand\t75%\trows\tshow\tthe\tcorresponding percentiles:\ta\tpercentile\tindicates\tthe\tvalue\tbelow\twhich\ta\tgiven\tpercentage\tof\tobservations\tin\ta\tgroup of\tobservations\tfalls.\tFor\texample,\t25%\tof\tthe\tdistricts\thave\ta\thousing_median_age\tlower\tthan\t18, while\t50%\tare\tlower\tthan\t29\tand\t75%\tare\tlower\tthan\t37.\tThese\tare\toften\tcalled\tthe\t25th\tpercentile\t(or 1st\tquartile),\tthe\tmedian,\tand\tthe\t75th\tpercentile\t(or\t3rd\tquartile).\n\nAnother\tquick\tway\tto\tget\ta\tfeel\tof\tthe\ttype\tof\tdata\tyou\tare\tdealing\twith\tis\tto\tplot\ta\thistogram\tfor\teach numerical\tattribute.\tA\thistogram\tshows\tthe\tnumber\tof\tinstances\t(on\tthe\tvertical\taxis)\tthat\thave\ta\tgiven value\trange\t(on\tthe\thorizontal\taxis).\tYou\tcan\teither\tplot\tthis\tone\tattribute\tat\ta\ttime,\tor\tyou\tcan\tcall\tthe hist()\tmethod\ton\tthe\twhole\tdataset,\tand\tit\twill\tplot\ta\thistogram\tfor\teach\tnumerical\tattribute\t(see Figure\t2-8).\tFor\texample,\tyou\tcan\tsee\tthat\tslightly\tover\t800\tdistricts\thave\ta\tmedian_house_value\tequal to\tabout\t$100,000.\n\n%matplotlib\tinline\t\t\t#\tonly\tin\ta\tJupyter\tnotebook import\tmatplotlib.pyplot\tas\tplt housing.hist(bins=50,\tfigsize=(20,15)) plt.show()",
      "content_length": 1709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "NOTE\n\nThe\thist()\tmethod\trelies\ton\tMatplotlib,\twhich\tin\tturn\trelies\ton\ta\tuser-specified\tgraphical\tbackend\tto\tdraw\ton\tyour\tscreen.\tSo before\tyou\tcan\tplot\tanything,\tyou\tneed\tto\tspecify\twhich\tbackend\tMatplotlib\tshould\tuse.\tThe\tsimplest\toption\tis\tto\tuse\tJupyter’s magic\tcommand\t%matplotlib\tinline.\tThis\ttells\tJupyter\tto\tset\tup\tMatplotlib\tso\tit\tuses\tJupyter’s\town\tbackend.\tPlots\tare\tthen rendered\twithin\tthe\tnotebook\titself.\tNote\tthat\tcalling\tshow()\tis\toptional\tin\ta\tJupyter\tnotebook,\tas\tJupyter\twill\tautomatically display\tplots\twhen\ta\tcell\tis\texecuted.\n\nFigure\t2-8.\tA\thistogram\tfor\teach\tnumerical\tattribute\n\nNotice\ta\tfew\tthings\tin\tthese\thistograms:\n\n1.\t First,\tthe\tmedian\tincome\tattribute\tdoes\tnot\tlook\tlike\tit\tis\texpressed\tin\tUS\tdollars\t(USD).\tAfter checking\twith\tthe\tteam\tthat\tcollected\tthe\tdata,\tyou\tare\ttold\tthat\tthe\tdata\thas\tbeen\tscaled\tand\tcapped at\t15\t(actually\t15.0001)\tfor\thigher\tmedian\tincomes,\tand\tat\t0.5\t(actually\t0.4999)\tfor\tlower\tmedian incomes.\tWorking\twith\tpreprocessed\tattributes\tis\tcommon\tin\tMachine\tLearning,\tand\tit\tis\tnot necessarily\ta\tproblem,\tbut\tyou\tshould\ttry\tto\tunderstand\thow\tthe\tdata\twas\tcomputed.\n\n2.\t The\thousing\tmedian\tage\tand\tthe\tmedian\thouse\tvalue\twere\talso\tcapped.\tThe\tlatter\tmay\tbe\ta\tserious problem\tsince\tit\tis\tyour\ttarget\tattribute\t(your\tlabels).\tYour\tMachine\tLearning\talgorithms\tmay\tlearn that\tprices\tnever\tgo\tbeyond\tthat\tlimit.\tYou\tneed\tto\tcheck\twith\tyour\tclient\tteam\t(the\tteam\tthat\twill\tuse your\tsystem’s\toutput)\tto\tsee\tif\tthis\tis\ta\tproblem\tor\tnot.\tIf\tthey\ttell\tyou\tthat\tthey\tneed\tprecise predictions\teven\tbeyond\t$500,000,\tthen\tyou\thave\tmainly\ttwo\toptions: a.\t Collect\tproper\tlabels\tfor\tthe\tdistricts\twhose\tlabels\twere\tcapped.\n\nb.\t Remove\tthose\tdistricts\tfrom\tthe\ttraining\tset\t(and\talso\tfrom\tthe\ttest\tset,\tsince\tyour\tsystem\tshould\n\nnot\tbe\tevaluated\tpoorly\tif\tit\tpredicts\tvalues\tbeyond\t$500,000).",
      "content_length": 1829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "3.\t These\tattributes\thave\tvery\tdifferent\tscales.\tWe\twill\tdiscuss\tthis\tlater\tin\tthis\tchapter\twhen\twe\n\nexplore\tfeature\tscaling.\n\n4.\t Finally,\tmany\thistograms\tare\ttail\theavy:\tthey\textend\tmuch\tfarther\tto\tthe\tright\tof\tthe\tmedian\tthan\tto the\tleft.\tThis\tmay\tmake\tit\ta\tbit\tharder\tfor\tsome\tMachine\tLearning\talgorithms\tto\tdetect\tpatterns.\tWe will\ttry\ttransforming\tthese\tattributes\tlater\ton\tto\thave\tmore\tbell-shaped\tdistributions.\n\nHopefully\tyou\tnow\thave\ta\tbetter\tunderstanding\tof\tthe\tkind\tof\tdata\tyou\tare\tdealing\twith.\n\nWARNING\n\nWait!\tBefore\tyou\tlook\tat\tthe\tdata\tany\tfurther,\tyou\tneed\tto\tcreate\ta\ttest\tset,\tput\tit\taside,\tand\tnever\tlook\tat\tit.",
      "content_length": 632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "Create\ta\tTest\tSet It\tmay\tsound\tstrange\tto\tvoluntarily\tset\taside\tpart\tof\tthe\tdata\tat\tthis\tstage.\tAfter\tall,\tyou\thave\tonly\ttaken\ta quick\tglance\tat\tthe\tdata,\tand\tsurely\tyou\tshould\tlearn\ta\twhole\tlot\tmore\tabout\tit\tbefore\tyou\tdecide\twhat algorithms\tto\tuse,\tright?\tThis\tis\ttrue,\tbut\tyour\tbrain\tis\tan\tamazing\tpattern\tdetection\tsystem,\twhich\tmeans that\tit\tis\thighly\tprone\tto\toverfitting:\tif\tyou\tlook\tat\tthe\ttest\tset,\tyou\tmay\tstumble\tupon\tsome\tseemingly interesting\tpattern\tin\tthe\ttest\tdata\tthat\tleads\tyou\tto\tselect\ta\tparticular\tkind\tof\tMachine\tLearning\tmodel. When\tyou\testimate\tthe\tgeneralization\terror\tusing\tthe\ttest\tset,\tyour\testimate\twill\tbe\ttoo\toptimistic\tand\tyou will\tlaunch\ta\tsystem\tthat\twill\tnot\tperform\tas\twell\tas\texpected.\tThis\tis\tcalled\tdata\tsnooping\tbias.\n\nCreating\ta\ttest\tset\tis\ttheoretically\tquite\tsimple:\tjust\tpick\tsome\tinstances\trandomly,\ttypically\t20%\tof\tthe dataset,\tand\tset\tthem\taside:\n\nimport\tnumpy\tas\tnp\n\ndef\tsplit_train_test(data,\ttest_ratio): \t\t\t\tshuffled_indices\t=\tnp.random.permutation(len(data)) \t\t\t\ttest_set_size\t=\tint(len(data)\t*\ttest_ratio) \t\t\t\ttest_indices\t=\tshuffled_indices[:test_set_size] \t\t\t\ttrain_indices\t=\tshuffled_indices[test_set_size:] \t\t\t\treturn\tdata.iloc[train_indices],\tdata.iloc[test_indices]\n\nYou\tcan\tthen\tuse\tthis\tfunction\tlike\tthis:\n\n>>>\ttrain_set,\ttest_set\t=\tsplit_train_test(housing,\t0.2) >>>\tprint(len(train_set),\t\"train\t+\",\tlen(test_set),\t\"test\") 16512\ttrain\t+\t4128\ttest\n\nWell,\tthis\tworks,\tbut\tit\tis\tnot\tperfect:\tif\tyou\trun\tthe\tprogram\tagain,\tit\twill\tgenerate\ta\tdifferent\ttest\tset! Over\ttime,\tyou\t(or\tyour\tMachine\tLearning\talgorithms)\twill\tget\tto\tsee\tthe\twhole\tdataset,\twhich\tis\twhat you\twant\tto\tavoid.\n\nOne\tsolution\tis\tto\tsave\tthe\ttest\tset\ton\tthe\tfirst\trun\tand\tthen\tload\tit\tin\tsubsequent\truns.\tAnother\toption\tis\tto set\tthe\trandom\tnumber\tgenerator’s\tseed\t(e.g.,\tnp.random.seed(42))12\tbefore\tcalling np.random.permutation(),\tso\tthat\tit\talways\tgenerates\tthe\tsame\tshuffled\tindices.\n\nBut\tboth\tthese\tsolutions\twill\tbreak\tnext\ttime\tyou\tfetch\tan\tupdated\tdataset.\tA\tcommon\tsolution\tis\tto\tuse each\tinstance’s\tidentifier\tto\tdecide\twhether\tor\tnot\tit\tshould\tgo\tin\tthe\ttest\tset\t(assuming\tinstances\thave\ta unique\tand\timmutable\tidentifier).\tFor\texample,\tyou\tcould\tcompute\ta\thash\tof\teach\tinstance’s\tidentifier, keep\tonly\tthe\tlast\tbyte\tof\tthe\thash,\tand\tput\tthe\tinstance\tin\tthe\ttest\tset\tif\tthis\tvalue\tis\tlower\tor\tequal\tto\t51 (~20%\tof\t256).\tThis\tensures\tthat\tthe\ttest\tset\twill\tremain\tconsistent\tacross\tmultiple\truns,\teven\tif\tyou refresh\tthe\tdataset.\tThe\tnew\ttest\tset\twill\tcontain\t20%\tof\tthe\tnew\tinstances,\tbut\tit\twill\tnot\tcontain\tany instance\tthat\twas\tpreviously\tin\tthe\ttraining\tset.\tHere\tis\ta\tpossible\timplementation:\n\nimport\thashlib\n\ndef\ttest_set_check(identifier,\ttest_ratio,\thash): \t\t\t\treturn\thash(np.int64(identifier)).digest()[-1]\t<\t256\t*\ttest_ratio\n\ndef\tsplit_train_test_by_id(data,\ttest_ratio,\tid_column,\thash=hashlib.md5): \t\t\t\tids\t=\tdata[id_column] \t\t\t\tin_test_set\t=\tids.apply(lambda\tid_:\ttest_set_check(id_,\ttest_ratio,\thash)) \t\t\t\treturn\tdata.loc[~in_test_set],\tdata.loc[in_test_set]",
      "content_length": 3014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "Unfortunately,\tthe\thousing\tdataset\tdoes\tnot\thave\tan\tidentifier\tcolumn.\tThe\tsimplest\tsolution\tis\tto\tuse\tthe row\tindex\tas\tthe\tID:\n\nhousing_with_id\t=\thousing.reset_index()\t\t\t#\tadds\tan\t`index`\tcolumn train_set,\ttest_set\t=\tsplit_train_test_by_id(housing_with_id,\t0.2,\t\"index\")\n\nIf\tyou\tuse\tthe\trow\tindex\tas\ta\tunique\tidentifier,\tyou\tneed\tto\tmake\tsure\tthat\tnew\tdata\tgets\tappended\tto\tthe end\tof\tthe\tdataset,\tand\tno\trow\tever\tgets\tdeleted.\tIf\tthis\tis\tnot\tpossible,\tthen\tyou\tcan\ttry\tto\tuse\tthe\tmost stable\tfeatures\tto\tbuild\ta\tunique\tidentifier.\tFor\texample,\ta\tdistrict’s\tlatitude\tand\tlongitude\tare\tguaranteed to\tbe\tstable\tfor\ta\tfew\tmillion\tyears,\tso\tyou\tcould\tcombine\tthem\tinto\tan\tID\tlike\tso:13\n\nhousing_with_id[\"id\"]\t=\thousing[\"longitude\"]\t*\t1000\t+\thousing[\"latitude\"] train_set,\ttest_set\t=\tsplit_train_test_by_id(housing_with_id,\t0.2,\t\"id\")\n\nScikit-Learn\tprovides\ta\tfew\tfunctions\tto\tsplit\tdatasets\tinto\tmultiple\tsubsets\tin\tvarious\tways.\tThe\tsimplest function\tis\ttrain_test_split,\twhich\tdoes\tpretty\tmuch\tthe\tsame\tthing\tas\tthe\tfunction split_train_test\tdefined\tearlier,\twith\ta\tcouple\tof\tadditional\tfeatures.\tFirst\tthere\tis\ta\trandom_state parameter\tthat\tallows\tyou\tto\tset\tthe\trandom\tgenerator\tseed\tas\texplained\tpreviously,\tand\tsecond\tyou\tcan pass\tit\tmultiple\tdatasets\twith\tan\tidentical\tnumber\tof\trows,\tand\tit\twill\tsplit\tthem\ton\tthe\tsame\tindices\t(this is\tvery\tuseful,\tfor\texample,\tif\tyou\thave\ta\tseparate\tDataFrame\tfor\tlabels):\n\nfrom\tsklearn.model_selection\timport\ttrain_test_split\n\ntrain_set,\ttest_set\t=\ttrain_test_split(housing,\ttest_size=0.2,\trandom_state=42)\n\nSo\tfar\twe\thave\tconsidered\tpurely\trandom\tsampling\tmethods.\tThis\tis\tgenerally\tfine\tif\tyour\tdataset\tis\tlarge enough\t(especially\trelative\tto\tthe\tnumber\tof\tattributes),\tbut\tif\tit\tis\tnot,\tyou\trun\tthe\trisk\tof\tintroducing\ta significant\tsampling\tbias.\tWhen\ta\tsurvey\tcompany\tdecides\tto\tcall\t1,000\tpeople\tto\task\tthem\ta\tfew questions,\tthey\tdon’t\tjust\tpick\t1,000\tpeople\trandomly\tin\ta\tphone\tbooth.\tThey\ttry\tto\tensure\tthat\tthese\t1,000 people\tare\trepresentative\tof\tthe\twhole\tpopulation.\tFor\texample,\tthe\tUS\tpopulation\tis\tcomposed\tof\t51.3% female\tand\t48.7%\tmale,\tso\ta\twell-conducted\tsurvey\tin\tthe\tUS\twould\ttry\tto\tmaintain\tthis\tratio\tin\tthe sample:\t513\tfemale\tand\t487\tmale.\tThis\tis\tcalled\tstratified\tsampling:\tthe\tpopulation\tis\tdivided\tinto homogeneous\tsubgroups\tcalled\tstrata,\tand\tthe\tright\tnumber\tof\tinstances\tis\tsampled\tfrom\teach\tstratum\tto guarantee\tthat\tthe\ttest\tset\tis\trepresentative\tof\tthe\toverall\tpopulation.\tIf\tthey\tused\tpurely\trandom\tsampling, there\twould\tbe\tabout\t12%\tchance\tof\tsampling\ta\tskewed\ttest\tset\twith\teither\tless\tthan\t49%\tfemale\tor\tmore than\t54%\tfemale.\tEither\tway,\tthe\tsurvey\tresults\twould\tbe\tsignificantly\tbiased.\n\nSuppose\tyou\tchatted\twith\texperts\twho\ttold\tyou\tthat\tthe\tmedian\tincome\tis\ta\tvery\timportant\tattribute\tto predict\tmedian\thousing\tprices.\tYou\tmay\twant\tto\tensure\tthat\tthe\ttest\tset\tis\trepresentative\tof\tthe\tvarious categories\tof\tincomes\tin\tthe\twhole\tdataset.\tSince\tthe\tmedian\tincome\tis\ta\tcontinuous\tnumerical\tattribute, you\tfirst\tneed\tto\tcreate\tan\tincome\tcategory\tattribute.\tLet’s\tlook\tat\tthe\tmedian\tincome\thistogram\tmore closely\t(see\tFigure\t2-8):\tmost\tmedian\tincome\tvalues\tare\tclustered\taround\t$20,000–$50,000,\tbut\tsome median\tincomes\tgo\tfar\tbeyond\t$60,000.\tIt\tis\timportant\tto\thave\ta\tsufficient\tnumber\tof\tinstances\tin\tyour dataset\tfor\teach\tstratum,\tor\telse\tthe\testimate\tof\tthe\tstratum’s\timportance\tmay\tbe\tbiased.\tThis\tmeans\tthat you\tshould\tnot\thave\ttoo\tmany\tstrata,\tand\teach\tstratum\tshould\tbe\tlarge\tenough.\tThe\tfollowing\tcode\tcreates an\tincome\tcategory\tattribute\tby\tdividing\tthe\tmedian\tincome\tby\t1.5\t(to\tlimit\tthe\tnumber\tof\tincome",
      "content_length": 3591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "categories),\tand\trounding\tup\tusing\tceil\t(to\thave\tdiscrete\tcategories),\tand\tthen\tmerging\tall\tthe\tcategories greater\tthan\t5\tinto\tcategory\t5:\n\nhousing[\"income_cat\"]\t=\tnp.ceil(housing[\"median_income\"]\t/\t1.5) housing[\"income_cat\"].where(housing[\"income_cat\"]\t<\t5,\t5.0,\tinplace=True)\n\nThese\tincome\tcategories\tare\trepresented\ton\tFigure\t2-9):\n\nFigure\t2-9.\tHistogram\tof\tincome\tcategories\n\nNow\tyou\tare\tready\tto\tdo\tstratified\tsampling\tbased\ton\tthe\tincome\tcategory.\tFor\tthis\tyou\tcan\tuse\tScikit- Learn’s\tStratifiedShuffleSplit\tclass:\n\nfrom\tsklearn.model_selection\timport\tStratifiedShuffleSplit\n\nsplit\t=\tStratifiedShuffleSplit(n_splits=1,\ttest_size=0.2,\trandom_state=42) for\ttrain_index,\ttest_index\tin\tsplit.split(housing,\thousing[\"income_cat\"]): \t\t\t\tstrat_train_set\t=\thousing.loc[train_index] \t\t\t\tstrat_test_set\t=\thousing.loc[test_index]\n\nLet’s\tsee\tif\tthis\tworked\tas\texpected.\tYou\tcan\tstart\tby\tlooking\tat\tthe\tincome\tcategory\tproportions\tin\tthe full\thousing\tdataset:\n\n>>>\thousing[\"income_cat\"].value_counts()\t/\tlen(housing) 3.0\t\t\t\t0.350581 2.0\t\t\t\t0.318847 4.0\t\t\t\t0.176308 5.0\t\t\t\t0.114438 1.0\t\t\t\t0.039826 Name:\tincome_cat,\tdtype:\tfloat64\n\nWith\tsimilar\tcode\tyou\tcan\tmeasure\tthe\tincome\tcategory\tproportions\tin\tthe\ttest\tset.\tFigure\t2-10\tcompares the\tincome\tcategory\tproportions\tin\tthe\toverall\tdataset,\tin\tthe\ttest\tset\tgenerated\twith\tstratified\tsampling, and\tin\ta\ttest\tset\tgenerated\tusing\tpurely\trandom\tsampling.\tAs\tyou\tcan\tsee,\tthe\ttest\tset\tgenerated\tusing",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "stratified\tsampling\thas\tincome\tcategory\tproportions\talmost\tidentical\tto\tthose\tin\tthe\tfull\tdataset,\twhereas the\ttest\tset\tgenerated\tusing\tpurely\trandom\tsampling\tis\tquite\tskewed.\n\nFigure\t2-10.\tSampling\tbias\tcomparison\tof\tstratified\tversus\tpurely\trandom\tsampling\n\nNow\tyou\tshould\tremove\tthe\tincome_cat\tattribute\tso\tthe\tdata\tis\tback\tto\tits\toriginal\tstate:\n\nfor\tset_\tin\t(strat_train_set,\tstrat_test_set): \t\t\t\tset_.drop(\"income_cat\",\taxis=1,\tinplace=True)\n\nWe\tspent\tquite\ta\tbit\tof\ttime\ton\ttest\tset\tgeneration\tfor\ta\tgood\treason:\tthis\tis\tan\toften\tneglected\tbut\tcritical part\tof\ta\tMachine\tLearning\tproject.\tMoreover,\tmany\tof\tthese\tideas\twill\tbe\tuseful\tlater\twhen\twe\tdiscuss cross-validation.\tNow\tit’s\ttime\tto\tmove\ton\tto\tthe\tnext\tstage:\texploring\tthe\tdata.",
      "content_length": 744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Discover\tand\tVisualize\tthe\tData\tto\tGain\tInsights So\tfar\tyou\thave\tonly\ttaken\ta\tquick\tglance\tat\tthe\tdata\tto\tget\ta\tgeneral\tunderstanding\tof\tthe\tkind\tof\tdata\tyou are\tmanipulating.\tNow\tthe\tgoal\tis\tto\tgo\ta\tlittle\tbit\tmore\tin\tdepth.\n\nFirst,\tmake\tsure\tyou\thave\tput\tthe\ttest\tset\taside\tand\tyou\tare\tonly\texploring\tthe\ttraining\tset.\tAlso,\tif\tthe training\tset\tis\tvery\tlarge,\tyou\tmay\twant\tto\tsample\tan\texploration\tset,\tto\tmake\tmanipulations\teasy\tand\tfast. In\tour\tcase,\tthe\tset\tis\tquite\tsmall\tso\tyou\tcan\tjust\twork\tdirectly\ton\tthe\tfull\tset.\tLet’s\tcreate\ta\tcopy\tso\tyou can\tplay\twith\tit\twithout\tharming\tthe\ttraining\tset:\n\nhousing\t=\tstrat_train_set.copy()",
      "content_length": 636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Visualizing\tGeographical\tData Since\tthere\tis\tgeographical\tinformation\t(latitude\tand\tlongitude),\tit\tis\ta\tgood\tidea\tto\tcreate\ta\tscatterplot\tof all\tdistricts\tto\tvisualize\tthe\tdata\t(Figure\t2-11):\n\nhousing.plot(kind=\"scatter\",\tx=\"longitude\",\ty=\"latitude\")\n\nFigure\t2-11.\tA\tgeographical\tscatterplot\tof\tthe\tdata\n\nThis\tlooks\tlike\tCalifornia\tall\tright,\tbut\tother\tthan\tthat\tit\tis\thard\tto\tsee\tany\tparticular\tpattern.\tSetting\tthe alpha\toption\tto\t0.1\tmakes\tit\tmuch\teasier\tto\tvisualize\tthe\tplaces\twhere\tthere\tis\ta\thigh\tdensity\tof\tdata points\t(Figure\t2-12):\n\nhousing.plot(kind=\"scatter\",\tx=\"longitude\",\ty=\"latitude\",\talpha=0.1)",
      "content_length": 611,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Figure\t2-12.\tA\tbetter\tvisualization\thighlighting\thigh-density\tareas\n\nNow\tthat’s\tmuch\tbetter:\tyou\tcan\tclearly\tsee\tthe\thigh-density\tareas,\tnamely\tthe\tBay\tArea\tand\taround\tLos Angeles\tand\tSan\tDiego,\tplus\ta\tlong\tline\tof\tfairly\thigh\tdensity\tin\tthe\tCentral\tValley,\tin\tparticular\taround Sacramento\tand\tFresno.\n\nMore\tgenerally,\tour\tbrains\tare\tvery\tgood\tat\tspotting\tpatterns\ton\tpictures,\tbut\tyou\tmay\tneed\tto\tplay\taround with\tvisualization\tparameters\tto\tmake\tthe\tpatterns\tstand\tout.\n\nNow\tlet’s\tlook\tat\tthe\thousing\tprices\t(Figure\t2-13).\tThe\tradius\tof\teach\tcircle\trepresents\tthe\tdistrict’s population\t(option\ts),\tand\tthe\tcolor\trepresents\tthe\tprice\t(option\tc).\tWe\twill\tuse\ta\tpredefined\tcolor\tmap (option\tcmap)\tcalled\tjet,\twhich\tranges\tfrom\tblue\t(low\tvalues)\tto\tred\t(high\tprices):14\n\nhousing.plot(kind=\"scatter\",\tx=\"longitude\",\ty=\"latitude\",\talpha=0.4, \t\t\t\ts=housing[\"population\"]/100,\tlabel=\"population\",\tfigsize=(10,7), \t\t\t\tc=\"median_house_value\",\tcmap=plt.get_cmap(\"jet\"),\tcolorbar=True, ) plt.legend()",
      "content_length": 990,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Figure\t2-13.\tCalifornia\thousing\tprices\n\nThis\timage\ttells\tyou\tthat\tthe\thousing\tprices\tare\tvery\tmuch\trelated\tto\tthe\tlocation\t(e.g.,\tclose\tto\tthe\tocean) and\tto\tthe\tpopulation\tdensity,\tas\tyou\tprobably\tknew\talready.\tIt\twill\tprobably\tbe\tuseful\tto\tuse\ta\tclustering algorithm\tto\tdetect\tthe\tmain\tclusters,\tand\tadd\tnew\tfeatures\tthat\tmeasure\tthe\tproximity\tto\tthe\tcluster centers.\tThe\tocean\tproximity\tattribute\tmay\tbe\tuseful\tas\twell,\talthough\tin\tNorthern\tCalifornia\tthe\thousing prices\tin\tcoastal\tdistricts\tare\tnot\ttoo\thigh,\tso\tit\tis\tnot\ta\tsimple\trule.",
      "content_length": 539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "Looking\tfor\tCorrelations Since\tthe\tdataset\tis\tnot\ttoo\tlarge,\tyou\tcan\teasily\tcompute\tthe\tstandard\tcorrelation\tcoefficient\t(also called\tPearson’s\tr)\tbetween\tevery\tpair\tof\tattributes\tusing\tthe\tcorr()\tmethod:\n\ncorr_matrix\t=\thousing.corr()\n\nNow\tlet’s\tlook\tat\thow\tmuch\teach\tattribute\tcorrelates\twith\tthe\tmedian\thouse\tvalue:\n\n>>>\tcorr_matrix[\"median_house_value\"].sort_values(ascending=False) median_house_value\t\t\t\t1.000000 median_income\t\t\t\t\t\t\t\t\t0.687170 total_rooms\t\t\t\t\t\t\t\t\t\t\t0.135231 housing_median_age\t\t\t\t0.114220 households\t\t\t\t\t\t\t\t\t\t\t\t0.064702 total_bedrooms\t\t\t\t\t\t\t\t0.047865 population\t\t\t\t\t\t\t\t\t\t\t-0.026699 longitude\t\t\t\t\t\t\t\t\t\t\t\t-0.047279 latitude\t\t\t\t\t\t\t\t\t\t\t\t\t-0.142826 Name:\tmedian_house_value,\tdtype:\tfloat64\n\nThe\tcorrelation\tcoefficient\tranges\tfrom\t–1\tto\t1.\tWhen\tit\tis\tclose\tto\t1,\tit\tmeans\tthat\tthere\tis\ta\tstrong positive\tcorrelation;\tfor\texample,\tthe\tmedian\thouse\tvalue\ttends\tto\tgo\tup\twhen\tthe\tmedian\tincome\tgoes\tup. When\tthe\tcoefficient\tis\tclose\tto\t–1,\tit\tmeans\tthat\tthere\tis\ta\tstrong\tnegative\tcorrelation;\tyou\tcan\tsee\ta small\tnegative\tcorrelation\tbetween\tthe\tlatitude\tand\tthe\tmedian\thouse\tvalue\t(i.e.,\tprices\thave\ta\tslight tendency\tto\tgo\tdown\twhen\tyou\tgo\tnorth).\tFinally,\tcoefficients\tclose\tto\tzero\tmean\tthat\tthere\tis\tno\tlinear correlation.\tFigure\t2-14\tshows\tvarious\tplots\talong\twith\tthe\tcorrelation\tcoefficient\tbetween\ttheir horizontal\tand\tvertical\taxes.\n\nFigure\t2-14.\tStandard\tcorrelation\tcoefficient\tof\tvarious\tdatasets\t(source:\tWikipedia;\tpublic\tdomain\timage)",
      "content_length": 1464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "WARNING\n\nThe\tcorrelation\tcoefficient\tonly\tmeasures\tlinear\tcorrelations\t(“if\tx\tgoes\tup,\tthen\ty\tgenerally\tgoes\tup/down”).\tIt\tmay\tcompletely miss\tout\ton\tnonlinear\trelationships\t(e.g.,\t“if\tx\tis\tclose\tto\tzero\tthen\ty\tgenerally\tgoes\tup”).\tNote\thow\tall\tthe\tplots\tof\tthe\tbottom\trow have\ta\tcorrelation\tcoefficient\tequal\tto\tzero\tdespite\tthe\tfact\tthat\ttheir\taxes\tare\tclearly\tnot\tindependent:\tthese\tare\texamples\tof nonlinear\trelationships.\tAlso,\tthe\tsecond\trow\tshows\texamples\twhere\tthe\tcorrelation\tcoefficient\tis\tequal\tto\t1\tor\t–1;\tnotice\tthat\tthis has\tnothing\tto\tdo\twith\tthe\tslope.\tFor\texample,\tyour\theight\tin\tinches\thas\ta\tcorrelation\tcoefficient\tof\t1\twith\tyour\theight\tin\tfeet\tor\tin nanometers.\n\nAnother\tway\tto\tcheck\tfor\tcorrelation\tbetween\tattributes\tis\tto\tuse\tPandas’\tscatter_matrix\tfunction, which\tplots\tevery\tnumerical\tattribute\tagainst\tevery\tother\tnumerical\tattribute.\tSince\tthere\tare\tnow\t11 numerical\tattributes,\tyou\twould\tget\t112\t=\t121\tplots,\twhich\twould\tnot\tfit\ton\ta\tpage,\tso\tlet’s\tjust\tfocus\ton\ta few\tpromising\tattributes\tthat\tseem\tmost\tcorrelated\twith\tthe\tmedian\thousing\tvalue\t(Figure\t2-15):\n\nfrom\tpandas.tools.plotting\timport\tscatter_matrix\n\nattributes\t=\t[\"median_house_value\",\t\"median_income\",\t\"total_rooms\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\"housing_median_age\"] scatter_matrix(housing[attributes],\tfigsize=(12,\t8))\n\nFigure\t2-15.\tScatter\tmatrix\n\nThe\tmain\tdiagonal\t(top\tleft\tto\tbottom\tright)\twould\tbe\tfull\tof\tstraight\tlines\tif\tPandas\tplotted\teach\tvariable against\titself,\twhich\twould\tnot\tbe\tvery\tuseful.\tSo\tinstead\tPandas\tdisplays\ta\thistogram\tof\teach\tattribute (other\toptions\tare\tavailable;\tsee\tPandas’\tdocumentation\tfor\tmore\tdetails).\n\nThe\tmost\tpromising\tattribute\tto\tpredict\tthe\tmedian\thouse\tvalue\tis\tthe\tmedian\tincome,\tso\tlet’s\tzoom\tin\ton their\tcorrelation\tscatterplot\t(Figure\t2-16):\n\nhousing.plot(kind=\"scatter\",\tx=\"median_income\",\ty=\"median_house_value\",",
      "content_length": 1839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "alpha=0.1)\n\nThis\tplot\treveals\ta\tfew\tthings.\tFirst,\tthe\tcorrelation\tis\tindeed\tvery\tstrong;\tyou\tcan\tclearly\tsee\tthe\tupward trend\tand\tthe\tpoints\tare\tnot\ttoo\tdispersed.\tSecond,\tthe\tprice\tcap\tthat\twe\tnoticed\tearlier\tis\tclearly\tvisible as\ta\thorizontal\tline\tat\t$500,000.\tBut\tthis\tplot\treveals\tother\tless\tobvious\tstraight\tlines:\ta\thorizontal\tline around\t$450,000,\tanother\taround\t$350,000,\tperhaps\tone\taround\t$280,000,\tand\ta\tfew\tmore\tbelow\tthat. You\tmay\twant\tto\ttry\tremoving\tthe\tcorresponding\tdistricts\tto\tprevent\tyour\talgorithms\tfrom\tlearning\tto reproduce\tthese\tdata\tquirks.\n\nFigure\t2-16.\tMedian\tincome\tversus\tmedian\thouse\tvalue",
      "content_length": 620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Experimenting\twith\tAttribute\tCombinations Hopefully\tthe\tprevious\tsections\tgave\tyou\tan\tidea\tof\ta\tfew\tways\tyou\tcan\texplore\tthe\tdata\tand\tgain insights.\tYou\tidentified\ta\tfew\tdata\tquirks\tthat\tyou\tmay\twant\tto\tclean\tup\tbefore\tfeeding\tthe\tdata\tto\ta Machine\tLearning\talgorithm,\tand\tyou\tfound\tinteresting\tcorrelations\tbetween\tattributes,\tin\tparticular\twith the\ttarget\tattribute.\tYou\talso\tnoticed\tthat\tsome\tattributes\thave\ta\ttail-heavy\tdistribution,\tso\tyou\tmay\twant to\ttransform\tthem\t(e.g.,\tby\tcomputing\ttheir\tlogarithm).\tOf\tcourse,\tyour\tmileage\twill\tvary\tconsiderably with\teach\tproject,\tbut\tthe\tgeneral\tideas\tare\tsimilar.\n\nOne\tlast\tthing\tyou\tmay\twant\tto\tdo\tbefore\tactually\tpreparing\tthe\tdata\tfor\tMachine\tLearning\talgorithms\tis to\ttry\tout\tvarious\tattribute\tcombinations.\tFor\texample,\tthe\ttotal\tnumber\tof\trooms\tin\ta\tdistrict\tis\tnot\tvery useful\tif\tyou\tdon’t\tknow\thow\tmany\thouseholds\tthere\tare.\tWhat\tyou\treally\twant\tis\tthe\tnumber\tof\trooms per\thousehold.\tSimilarly,\tthe\ttotal\tnumber\tof\tbedrooms\tby\titself\tis\tnot\tvery\tuseful:\tyou\tprobably\twant\tto compare\tit\tto\tthe\tnumber\tof\trooms.\tAnd\tthe\tpopulation\tper\thousehold\talso\tseems\tlike\tan\tinteresting attribute\tcombination\tto\tlook\tat.\tLet’s\tcreate\tthese\tnew\tattributes:\n\nhousing[\"rooms_per_household\"]\t=\thousing[\"total_rooms\"]/housing[\"households\"] housing[\"bedrooms_per_room\"]\t=\thousing[\"total_bedrooms\"]/housing[\"total_rooms\"] housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n\nAnd\tnow\tlet’s\tlook\tat\tthe\tcorrelation\tmatrix\tagain:\n\n>>>\tcorr_matrix\t=\thousing.corr() >>>\tcorr_matrix[\"median_house_value\"].sort_values(ascending=False) median_house_value\t\t\t\t\t\t\t\t\t\t1.000000 median_income\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.687160 rooms_per_household\t\t\t\t\t\t\t\t\t0.146285 total_rooms\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.135097 housing_median_age\t\t\t\t\t\t\t\t\t\t0.114110 households\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.064506 total_bedrooms\t\t\t\t\t\t\t\t\t\t\t\t\t\t0.047689 population_per_household\t\t\t-0.021985 population\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t-0.026920 longitude\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t-0.047432 latitude\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t-0.142724 bedrooms_per_room\t\t\t\t\t\t\t\t\t\t-0.259984 Name:\tmedian_house_value,\tdtype:\tfloat64\n\nHey,\tnot\tbad!\tThe\tnew\tbedrooms_per_room\tattribute\tis\tmuch\tmore\tcorrelated\twith\tthe\tmedian\thouse value\tthan\tthe\ttotal\tnumber\tof\trooms\tor\tbedrooms.\tApparently\thouses\twith\ta\tlower\tbedroom/room\tratio tend\tto\tbe\tmore\texpensive.\tThe\tnumber\tof\trooms\tper\thousehold\tis\talso\tmore\tinformative\tthan\tthe\ttotal number\tof\trooms\tin\ta\tdistrict\t—\tobviously\tthe\tlarger\tthe\thouses,\tthe\tmore\texpensive\tthey\tare.\n\nThis\tround\tof\texploration\tdoes\tnot\thave\tto\tbe\tabsolutely\tthorough;\tthe\tpoint\tis\tto\tstart\toff\ton\tthe\tright\tfoot and\tquickly\tgain\tinsights\tthat\twill\thelp\tyou\tget\ta\tfirst\treasonably\tgood\tprototype.\tBut\tthis\tis\tan\titerative process:\tonce\tyou\tget\ta\tprototype\tup\tand\trunning,\tyou\tcan\tanalyze\tits\toutput\tto\tgain\tmore\tinsights\tand come\tback\tto\tthis\texploration\tstep.",
      "content_length": 2828,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Prepare\tthe\tData\tfor\tMachine\tLearning\tAlgorithms It’s\ttime\tto\tprepare\tthe\tdata\tfor\tyour\tMachine\tLearning\talgorithms.\tInstead\tof\tjust\tdoing\tthis\tmanually,\tyou should\twrite\tfunctions\tto\tdo\tthat,\tfor\tseveral\tgood\treasons:\n\nThis\twill\tallow\tyou\tto\treproduce\tthese\ttransformations\teasily\ton\tany\tdataset\t(e.g.,\tthe\tnext\ttime\tyou get\ta\tfresh\tdataset).\n\nYou\twill\tgradually\tbuild\ta\tlibrary\tof\ttransformation\tfunctions\tthat\tyou\tcan\treuse\tin\tfuture\tprojects.\n\nYou\tcan\tuse\tthese\tfunctions\tin\tyour\tlive\tsystem\tto\ttransform\tthe\tnew\tdata\tbefore\tfeeding\tit\tto\tyour algorithms.\n\nThis\twill\tmake\tit\tpossible\tfor\tyou\tto\teasily\ttry\tvarious\ttransformations\tand\tsee\twhich\tcombination of\ttransformations\tworks\tbest.\n\nBut\tfirst\tlet’s\trevert\tto\ta\tclean\ttraining\tset\t(by\tcopying\tstrat_train_set\tonce\tagain),\tand\tlet’s\tseparate the\tpredictors\tand\tthe\tlabels\tsince\twe\tdon’t\tnecessarily\twant\tto\tapply\tthe\tsame\ttransformations\tto\tthe predictors\tand\tthe\ttarget\tvalues\t(note\tthat\tdrop()\tcreates\ta\tcopy\tof\tthe\tdata\tand\tdoes\tnot\taffect strat_train_set):\n\nhousing\t=\tstrat_train_set.drop(\"median_house_value\",\taxis=1) housing_labels\t=\tstrat_train_set[\"median_house_value\"].copy()",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Data\tCleaning Most\tMachine\tLearning\talgorithms\tcannot\twork\twith\tmissing\tfeatures,\tso\tlet’s\tcreate\ta\tfew\tfunctions\tto take\tcare\tof\tthem.\tYou\tnoticed\tearlier\tthat\tthe\ttotal_bedrooms\tattribute\thas\tsome\tmissing\tvalues,\tso let’s\tfix\tthis.\tYou\thave\tthree\toptions:\n\nGet\trid\tof\tthe\tcorresponding\tdistricts.\n\nGet\trid\tof\tthe\twhole\tattribute.\n\nSet\tthe\tvalues\tto\tsome\tvalue\t(zero,\tthe\tmean,\tthe\tmedian,\tetc.).\n\nYou\tcan\taccomplish\tthese\teasily\tusing\tDataFrame’s\tdropna(),\tdrop(),\tand\tfillna()\tmethods:\n\nhousing.dropna(subset=[\"total_bedrooms\"])\t\t\t\t#\toption\t1 housing.drop(\"total_bedrooms\",\taxis=1)\t\t\t\t\t\t\t#\toption\t2 median\t=\thousing[\"total_bedrooms\"].median()\t\t#\toption\t3 housing[\"total_bedrooms\"].fillna(median,\tinplace=True)\n\nIf\tyou\tchoose\toption\t3,\tyou\tshould\tcompute\tthe\tmedian\tvalue\ton\tthe\ttraining\tset,\tand\tuse\tit\tto\tfill\tthe missing\tvalues\tin\tthe\ttraining\tset,\tbut\talso\tdon’t\tforget\tto\tsave\tthe\tmedian\tvalue\tthat\tyou\thave\tcomputed. You\twill\tneed\tit\tlater\tto\treplace\tmissing\tvalues\tin\tthe\ttest\tset\twhen\tyou\twant\tto\tevaluate\tyour\tsystem,\tand also\tonce\tthe\tsystem\tgoes\tlive\tto\treplace\tmissing\tvalues\tin\tnew\tdata.\n\nScikit-Learn\tprovides\ta\thandy\tclass\tto\ttake\tcare\tof\tmissing\tvalues:\tImputer.\tHere\tis\thow\tto\tuse\tit.\tFirst, you\tneed\tto\tcreate\tan\tImputer\tinstance,\tspecifying\tthat\tyou\twant\tto\treplace\teach\tattribute’s\tmissing values\twith\tthe\tmedian\tof\tthat\tattribute:\n\nfrom\tsklearn.preprocessing\timport\tImputer\n\nimputer\t=\tImputer(strategy=\"median\")\n\nSince\tthe\tmedian\tcan\tonly\tbe\tcomputed\ton\tnumerical\tattributes,\twe\tneed\tto\tcreate\ta\tcopy\tof\tthe\tdata without\tthe\ttext\tattribute\tocean_proximity:\n\nhousing_num\t=\thousing.drop(\"ocean_proximity\",\taxis=1)\n\nNow\tyou\tcan\tfit\tthe\timputer\tinstance\tto\tthe\ttraining\tdata\tusing\tthe\tfit()\tmethod:\n\nimputer.fit(housing_num)\n\nThe\timputer\thas\tsimply\tcomputed\tthe\tmedian\tof\teach\tattribute\tand\tstored\tthe\tresult\tin\tits\tstatistics_ instance\tvariable.\tOnly\tthe\ttotal_bedrooms\tattribute\thad\tmissing\tvalues,\tbut\twe\tcannot\tbe\tsure\tthat there\twon’t\tbe\tany\tmissing\tvalues\tin\tnew\tdata\tafter\tthe\tsystem\tgoes\tlive,\tso\tit\tis\tsafer\tto\tapply\tthe imputer\tto\tall\tthe\tnumerical\tattributes:\n\n>>>\timputer.statistics_ array([\t-118.51\t,\t34.26\t,\t29.\t,\t2119.5\t,\t433.\t,\t1164.\t,\t408.\t,\t3.5409]) >>>\thousing_num.median().values",
      "content_length": 2218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "array([\t-118.51\t,\t34.26\t,\t29.\t,\t2119.5\t,\t433.\t,\t1164.\t,\t408.\t,\t3.5409])\n\nNow\tyou\tcan\tuse\tthis\t“trained”\timputer\tto\ttransform\tthe\ttraining\tset\tby\treplacing\tmissing\tvalues\tby\tthe learned\tmedians:\n\nX\t=\timputer.transform(housing_num)\n\nThe\tresult\tis\ta\tplain\tNumpy\tarray\tcontaining\tthe\ttransformed\tfeatures.\tIf\tyou\twant\tto\tput\tit\tback\tinto\ta Pandas\tDataFrame,\tit’s\tsimple:\n\nhousing_tr\t=\tpd.DataFrame(X,\tcolumns=housing_num.columns)\n\nSCIKIT-LEARN\tDESIGN\n\nScikit-Learn’s\tAPI\tis\tremarkably\twell\tdesigned.\tThe\tmain\tdesign\tprinciples\tare:15 Consistency.\tAll\tobjects\tshare\ta\tconsistent\tand\tsimple\tinterface:\n\nEstimators.\tAny\tobject\tthat\tcan\testimate\tsome\tparameters\tbased\ton\ta\tdataset\tis\tcalled\tan\testimator\t(e.g.,\tan\timputer\tis an\testimator).\tThe\testimation\titself\tis\tperformed\tby\tthe\tfit()\tmethod,\tand\tit\ttakes\tonly\ta\tdataset\tas\ta\tparameter\t(or\ttwo for\tsupervised\tlearning\talgorithms;\tthe\tsecond\tdataset\tcontains\tthe\tlabels).\tAny\tother\tparameter\tneeded\tto\tguide\tthe estimation\tprocess\tis\tconsidered\ta\thyperparameter\t(such\tas\tan\timputer’s\tstrategy),\tand\tit\tmust\tbe\tset\tas\tan\tinstance variable\t(generally\tvia\ta\tconstructor\tparameter).\n\nTransformers.\tSome\testimators\t(such\tas\tan\timputer)\tcan\talso\ttransform\ta\tdataset;\tthese\tare\tcalled\ttransformers.\tOnce again,\tthe\tAPI\tis\tquite\tsimple:\tthe\ttransformation\tis\tperformed\tby\tthe\ttransform()\tmethod\twith\tthe\tdataset\tto\ttransform\tas a\tparameter.\tIt\treturns\tthe\ttransformed\tdataset.\tThis\ttransformation\tgenerally\trelies\ton\tthe\tlearned\tparameters,\tas\tis\tthe case\tfor\tan\timputer.\tAll\ttransformers\talso\thave\ta\tconvenience\tmethod\tcalled\tfit_transform()\tthat\tis\tequivalent\tto\tcalling fit()\tand\tthen\ttransform()\t(but\tsometimes\tfit_transform()\tis\toptimized\tand\truns\tmuch\tfaster).\n\nPredictors.\tFinally,\tsome\testimators\tare\tcapable\tof\tmaking\tpredictions\tgiven\ta\tdataset;\tthey\tare\tcalled\tpredictors.\tFor example,\tthe\tLinearRegression\tmodel\tin\tthe\tprevious\tchapter\twas\ta\tpredictor:\tit\tpredicted\tlife\tsatisfaction\tgiven\ta country’s\tGDP\tper\tcapita.\tA\tpredictor\thas\ta\tpredict()\tmethod\tthat\ttakes\ta\tdataset\tof\tnew\tinstances\tand\treturns\ta\tdataset of\tcorresponding\tpredictions.\tIt\talso\thas\ta\tscore()\tmethod\tthat\tmeasures\tthe\tquality\tof\tthe\tpredictions\tgiven\ta\ttest\tset\t(and the\tcorresponding\tlabels\tin\tthe\tcase\tof\tsupervised\tlearning\talgorithms).16\n\nInspection.\tAll\tthe\testimator’s\thyperparameters\tare\taccessible\tdirectly\tvia\tpublic\tinstance\tvariables\t(e.g.,\timputer.strategy), and\tall\tthe\testimator’s\tlearned\tparameters\tare\talso\taccessible\tvia\tpublic\tinstance\tvariables\twith\tan\tunderscore\tsuffix\t(e.g., imputer.statistics_).\n\nNonproliferation\tof\tclasses.\tDatasets\tare\trepresented\tas\tNumPy\tarrays\tor\tSciPy\tsparse\tmatrices,\tinstead\tof\thomemade classes.\tHyperparameters\tare\tjust\tregular\tPython\tstrings\tor\tnumbers.\n\nComposition.\tExisting\tbuilding\tblocks\tare\treused\tas\tmuch\tas\tpossible.\tFor\texample,\tit\tis\teasy\tto\tcreate\ta\tPipeline\testimator from\tan\tarbitrary\tsequence\tof\ttransformers\tfollowed\tby\ta\tfinal\testimator,\tas\twe\twill\tsee.\n\nSensible\tdefaults.\tScikit-Learn\tprovides\treasonable\tdefault\tvalues\tfor\tmost\tparameters,\tmaking\tit\teasy\tto\tcreate\ta\tbaseline working\tsystem\tquickly.",
      "content_length": 3089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Handling\tText\tand\tCategorical\tAttributes Earlier\twe\tleft\tout\tthe\tcategorical\tattribute\tocean_proximity\tbecause\tit\tis\ta\ttext\tattribute\tso\twe\tcannot compute\tits\tmedian.\tMost\tMachine\tLearning\talgorithms\tprefer\tto\twork\twith\tnumbers\tanyway,\tso\tlet’s convert\tthese\ttext\tlabels\tto\tnumbers.\n\nScikit-Learn\tprovides\ta\ttransformer\tfor\tthis\ttask\tcalled\tLabelEncoder:\n\n>>>\tfrom\tsklearn.preprocessing\timport\tLabelEncoder >>>\tencoder\t=\tLabelEncoder() >>>\thousing_cat\t=\thousing[\"ocean_proximity\"] >>>\thousing_cat_encoded\t=\tencoder.fit_transform(housing_cat) >>>\thousing_cat_encoded array([0,\t0,\t4,\t...,\t1,\t0,\t3])\n\nThis\tis\tbetter:\tnow\twe\tcan\tuse\tthis\tnumerical\tdata\tin\tany\tML\talgorithm.\tYou\tcan\tlook\tat\tthe\tmapping\tthat this\tencoder\thas\tlearned\tusing\tthe\tclasses_\tattribute\t(“<1H\tOCEAN”\tis\tmapped\tto\t0,\t“INLAND”\tis mapped\tto\t1,\tetc.):\n\n>>>\tprint(encoder.classes_) ['<1H\tOCEAN'\t'INLAND'\t'ISLAND'\t'NEAR\tBAY'\t'NEAR\tOCEAN']\n\nOne\tissue\twith\tthis\trepresentation\tis\tthat\tML\talgorithms\twill\tassume\tthat\ttwo\tnearby\tvalues\tare\tmore similar\tthan\ttwo\tdistant\tvalues.\tObviously\tthis\tis\tnot\tthe\tcase\t(for\texample,\tcategories\t0\tand\t4\tare\tmore similar\tthan\tcategories\t0\tand\t1).\tTo\tfix\tthis\tissue,\ta\tcommon\tsolution\tis\tto\tcreate\tone\tbinary\tattribute\tper category:\tone\tattribute\tequal\tto\t1\twhen\tthe\tcategory\tis\t“<1H\tOCEAN”\t(and\t0\totherwise),\tanother attribute\tequal\tto\t1\twhen\tthe\tcategory\tis\t“INLAND”\t(and\t0\totherwise),\tand\tso\ton.\tThis\tis\tcalled\tone-hot encoding,\tbecause\tonly\tone\tattribute\twill\tbe\tequal\tto\t1\t(hot),\twhile\tthe\tothers\twill\tbe\t0\t(cold).\n\nScikit-Learn\tprovides\ta\tOneHotEncoder\tencoder\tto\tconvert\tinteger\tcategorical\tvalues\tinto\tone-hot vectors.\tLet’s\tencode\tthe\tcategories\tas\tone-hot\tvectors.\tNote\tthat\tfit_transform()\texpects\ta\t2D\tarray, but\thousing_cat_encoded\tis\ta\t1D\tarray,\tso\twe\tneed\tto\treshape\tit:17\n\n>>>\tfrom\tsklearn.preprocessing\timport\tOneHotEncoder >>>\tencoder\t=\tOneHotEncoder() >>>\thousing_cat_1hot\t=\tencoder.fit_transform(housing_cat_encoded.reshape(-1,1)) >>>\thousing_cat_1hot <16512x5\tsparse\tmatrix\tof\ttype\t'<class\t'numpy.float64'>'\n\nwith\t16512\tstored\telements\tin\tCompressed\tSparse\tRow\tformat>\n\nNotice\tthat\tthe\toutput\tis\ta\tSciPy\tsparse\tmatrix,\tinstead\tof\ta\tNumPy\tarray.\tThis\tis\tvery\tuseful\twhen\tyou have\tcategorical\tattributes\twith\tthousands\tof\tcategories.\tAfter\tone-hot\tencoding\twe\tget\ta\tmatrix\twith thousands\tof\tcolumns,\tand\tthe\tmatrix\tis\tfull\tof\tzeros\texcept\tfor\tone\t1\tper\trow.\tUsing\tup\ttons\tof\tmemory mostly\tto\tstore\tzeros\twould\tbe\tvery\twasteful,\tso\tinstead\ta\tsparse\tmatrix\tonly\tstores\tthe\tlocation\tof\tthe nonzero\telements.\tYou\tcan\tuse\tit\tmostly\tlike\ta\tnormal\t2D\tarray,18\tbut\tif\tyou\treally\twant\tto\tconvert\tit\tto\ta (dense)\tNumPy\tarray,\tjust\tcall\tthe\ttoarray()\tmethod:\n\n>>>\thousing_cat_1hot.toarray() array([[\t1.,\t\t0.,\t\t0.,\t\t0.,\t\t0.], \t\t\t\t\t\t\t[\t1.,\t\t0.,\t\t0.,\t\t0.,\t\t0.], \t\t\t\t\t\t\t[\t0.,\t\t0.,\t\t0.,\t\t0.,\t\t1.],",
      "content_length": 2797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "..., \t\t\t\t\t\t\t[\t0.,\t\t1.,\t\t0.,\t\t0.,\t\t0.], \t\t\t\t\t\t\t[\t1.,\t\t0.,\t\t0.,\t\t0.,\t\t0.], \t\t\t\t\t\t\t[\t0.,\t\t0.,\t\t0.,\t\t1.,\t\t0.]])\n\nWe\tcan\tapply\tboth\ttransformations\t(from\ttext\tcategories\tto\tinteger\tcategories,\tthen\tfrom\tinteger\tcategories to\tone-hot\tvectors)\tin\tone\tshot\tusing\tthe\tLabelBinarizer\tclass:\n\n>>>\tfrom\tsklearn.preprocessing\timport\tLabelBinarizer >>>\tencoder\t=\tLabelBinarizer() >>>\thousing_cat_1hot\t=\tencoder.fit_transform(housing_cat) >>>\thousing_cat_1hot array([[1,\t0,\t0,\t0,\t0], \t\t\t\t\t\t\t[1,\t0,\t0,\t0,\t0], \t\t\t\t\t\t\t[0,\t0,\t0,\t0,\t1], \t\t\t\t\t\t\t..., \t\t\t\t\t\t\t[0,\t1,\t0,\t0,\t0], \t\t\t\t\t\t\t[1,\t0,\t0,\t0,\t0], \t\t\t\t\t\t\t[0,\t0,\t0,\t1,\t0]])\n\nNote\tthat\tthis\treturns\ta\tdense\tNumPy\tarray\tby\tdefault.\tYou\tcan\tget\ta\tsparse\tmatrix\tinstead\tby\tpassing sparse_output=True\tto\tthe\tLabelBinarizer\tconstructor.",
      "content_length": 758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Custom\tTransformers Although\tScikit-Learn\tprovides\tmany\tuseful\ttransformers,\tyou\twill\tneed\tto\twrite\tyour\town\tfor\ttasks\tsuch as\tcustom\tcleanup\toperations\tor\tcombining\tspecific\tattributes.\tYou\twill\twant\tyour\ttransformer\tto\twork seamlessly\twith\tScikit-Learn\tfunctionalities\t(such\tas\tpipelines),\tand\tsince\tScikit-Learn\trelies\ton\tduck typing\t(not\tinheritance),\tall\tyou\tneed\tis\tto\tcreate\ta\tclass\tand\timplement\tthree\tmethods:\tfit()\t(returning self),\ttransform(),\tand\tfit_transform().\tYou\tcan\tget\tthe\tlast\tone\tfor\tfree\tby\tsimply\tadding TransformerMixin\tas\ta\tbase\tclass.\tAlso,\tif\tyou\tadd\tBaseEstimator\tas\ta\tbase\tclass\t(and\tavoid\t*args and\t**kargs\tin\tyour\tconstructor)\tyou\twill\tget\ttwo\textra\tmethods\t(get_params()\tand\tset_params()) that\twill\tbe\tuseful\tfor\tautomatic\thyperparameter\ttuning.\tFor\texample,\there\tis\ta\tsmall\ttransformer\tclass that\tadds\tthe\tcombined\tattributes\twe\tdiscussed\tearlier:\n\nfrom\tsklearn.base\timport\tBaseEstimator,\tTransformerMixin\n\nrooms_ix,\tbedrooms_ix,\tpopulation_ix,\thousehold_ix\t=\t3,\t4,\t5,\t6\n\nclass\tCombinedAttributesAdder(BaseEstimator,\tTransformerMixin): \t\t\t\tdef\t__init__(self,\tadd_bedrooms_per_room\t=\tTrue):\t#\tno\t*args\tor\t**kargs \t\t\t\t\t\t\t\tself.add_bedrooms_per_room\t=\tadd_bedrooms_per_room \t\t\t\tdef\tfit(self,\tX,\ty=None): \t\t\t\t\t\t\t\treturn\tself\t\t#\tnothing\telse\tto\tdo \t\t\t\tdef\ttransform(self,\tX,\ty=None): \t\t\t\t\t\t\t\trooms_per_household\t=\tX[:,\trooms_ix]\t/\tX[:,\thousehold_ix] \t\t\t\t\t\t\t\tpopulation_per_household\t=\tX[:,\tpopulation_ix]\t/\tX[:,\thousehold_ix] \t\t\t\t\t\t\t\tif\tself.add_bedrooms_per_room: \t\t\t\t\t\t\t\t\t\t\t\tbedrooms_per_room\t=\tX[:,\tbedrooms_ix]\t/\tX[:,\trooms_ix] \t\t\t\t\t\t\t\t\t\t\t\treturn\tnp.c_[X,\trooms_per_household,\tpopulation_per_household, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbedrooms_per_room] \t\t\t\t\t\t\t\telse: \t\t\t\t\t\t\t\t\t\t\t\treturn\tnp.c_[X,\trooms_per_household,\tpopulation_per_household]\n\nattr_adder\t=\tCombinedAttributesAdder(add_bedrooms_per_room=False) housing_extra_attribs\t=\tattr_adder.transform(housing.values)\n\nIn\tthis\texample\tthe\ttransformer\thas\tone\thyperparameter,\tadd_bedrooms_per_room,\tset\tto\tTrue\tby\tdefault (it\tis\toften\thelpful\tto\tprovide\tsensible\tdefaults).\tThis\thyperparameter\twill\tallow\tyou\tto\teasily\tfind\tout whether\tadding\tthis\tattribute\thelps\tthe\tMachine\tLearning\talgorithms\tor\tnot.\tMore\tgenerally,\tyou\tcan\tadd\ta hyperparameter\tto\tgate\tany\tdata\tpreparation\tstep\tthat\tyou\tare\tnot\t100%\tsure\tabout.\tThe\tmore\tyou automate\tthese\tdata\tpreparation\tsteps,\tthe\tmore\tcombinations\tyou\tcan\tautomatically\ttry\tout,\tmaking\tit much\tmore\tlikely\tthat\tyou\twill\tfind\ta\tgreat\tcombination\t(and\tsaving\tyou\ta\tlot\tof\ttime).",
      "content_length": 2497,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "Feature\tScaling One\tof\tthe\tmost\timportant\ttransformations\tyou\tneed\tto\tapply\tto\tyour\tdata\tis\tfeature\tscaling.\tWith\tfew exceptions,\tMachine\tLearning\talgorithms\tdon’t\tperform\twell\twhen\tthe\tinput\tnumerical\tattributes\thave very\tdifferent\tscales.\tThis\tis\tthe\tcase\tfor\tthe\thousing\tdata:\tthe\ttotal\tnumber\tof\trooms\tranges\tfrom\tabout\t6 to\t39,320,\twhile\tthe\tmedian\tincomes\tonly\trange\tfrom\t0\tto\t15.\tNote\tthat\tscaling\tthe\ttarget\tvalues\tis generally\tnot\trequired.\n\nThere\tare\ttwo\tcommon\tways\tto\tget\tall\tattributes\tto\thave\tthe\tsame\tscale:\tmin-max\tscaling\tand standardization.\n\nMin-max\tscaling\t(many\tpeople\tcall\tthis\tnormalization)\tis\tquite\tsimple:\tvalues\tare\tshifted\tand\trescaled so\tthat\tthey\tend\tup\tranging\tfrom\t0\tto\t1.\tWe\tdo\tthis\tby\tsubtracting\tthe\tmin\tvalue\tand\tdividing\tby\tthe\tmax minus\tthe\tmin.\tScikit-Learn\tprovides\ta\ttransformer\tcalled\tMinMaxScaler\tfor\tthis.\tIt\thas\ta feature_range\thyperparameter\tthat\tlets\tyou\tchange\tthe\trange\tif\tyou\tdon’t\twant\t0–1\tfor\tsome\treason.\n\nStandardization\tis\tquite\tdifferent:\tfirst\tit\tsubtracts\tthe\tmean\tvalue\t(so\tstandardized\tvalues\talways\thave\ta zero\tmean),\tand\tthen\tit\tdivides\tby\tthe\tvariance\tso\tthat\tthe\tresulting\tdistribution\thas\tunit\tvariance.\tUnlike min-max\tscaling,\tstandardization\tdoes\tnot\tbound\tvalues\tto\ta\tspecific\trange,\twhich\tmay\tbe\ta\tproblem\tfor some\talgorithms\t(e.g.,\tneural\tnetworks\toften\texpect\tan\tinput\tvalue\tranging\tfrom\t0\tto\t1).\tHowever, standardization\tis\tmuch\tless\taffected\tby\toutliers.\tFor\texample,\tsuppose\ta\tdistrict\thad\ta\tmedian\tincome equal\tto\t100\t(by\tmistake).\tMin-max\tscaling\twould\tthen\tcrush\tall\tthe\tother\tvalues\tfrom\t0–15\tdown\tto\t0– 0.15,\twhereas\tstandardization\twould\tnot\tbe\tmuch\taffected.\tScikit-Learn\tprovides\ta\ttransformer\tcalled StandardScaler\tfor\tstandardization.\n\nWARNING\n\nAs\twith\tall\tthe\ttransformations,\tit\tis\timportant\tto\tfit\tthe\tscalers\tto\tthe\ttraining\tdata\tonly,\tnot\tto\tthe\tfull\tdataset\t(including\tthe\ttest set).\tOnly\tthen\tcan\tyou\tuse\tthem\tto\ttransform\tthe\ttraining\tset\tand\tthe\ttest\tset\t(and\tnew\tdata).",
      "content_length": 1963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Transformation\tPipelines As\tyou\tcan\tsee,\tthere\tare\tmany\tdata\ttransformation\tsteps\tthat\tneed\tto\tbe\texecuted\tin\tthe\tright\torder. Fortunately,\tScikit-Learn\tprovides\tthe\tPipeline\tclass\tto\thelp\twith\tsuch\tsequences\tof\ttransformations. Here\tis\ta\tsmall\tpipeline\tfor\tthe\tnumerical\tattributes:\n\nfrom\tsklearn.pipeline\timport\tPipeline from\tsklearn.preprocessing\timport\tStandardScaler\n\nnum_pipeline\t=\tPipeline([ \t\t\t\t\t\t\t\t('imputer',\tImputer(strategy=\"median\")), \t\t\t\t\t\t\t\t('attribs_adder',\tCombinedAttributesAdder()), \t\t\t\t\t\t\t\t('std_scaler',\tStandardScaler()), \t\t\t\t])\n\nhousing_num_tr\t=\tnum_pipeline.fit_transform(housing_num)\n\nThe\tPipeline\tconstructor\ttakes\ta\tlist\tof\tname/estimator\tpairs\tdefining\ta\tsequence\tof\tsteps.\tAll\tbut\tthe last\testimator\tmust\tbe\ttransformers\t(i.e.,\tthey\tmust\thave\ta\tfit_transform()\tmethod).\tThe\tnames\tcan\tbe anything\tyou\tlike\t(as\tlong\tas\tthey\tdon’t\tcontain\tdouble\tunderscores\t“__”).\n\nWhen\tyou\tcall\tthe\tpipeline’s\tfit()\tmethod,\tit\tcalls\tfit_transform()\tsequentially\ton\tall\ttransformers, passing\tthe\toutput\tof\teach\tcall\tas\tthe\tparameter\tto\tthe\tnext\tcall,\tuntil\tit\treaches\tthe\tfinal\testimator,\tfor which\tit\tjust\tcalls\tthe\tfit()\tmethod.\n\nThe\tpipeline\texposes\tthe\tsame\tmethods\tas\tthe\tfinal\testimator.\tIn\tthis\texample,\tthe\tlast\testimator\tis\ta StandardScaler,\twhich\tis\ta\ttransformer,\tso\tthe\tpipeline\thas\ta\ttransform()\tmethod\tthat\tapplies\tall\tthe transforms\tto\tthe\tdata\tin\tsequence\t(it\talso\thas\ta\tfit_transform\tmethod\tthat\twe\tcould\thave\tused\tinstead of\tcalling\tfit()\tand\tthen\ttransform()).\n\nNow\tit\twould\tbe\tnice\tif\twe\tcould\tfeed\ta\tPandas\tDataFrame\tdirectly\tinto\tour\tpipeline,\tinstead\tof\thaving to\tfirst\tmanually\textract\tthe\tnumerical\tcolumns\tinto\ta\tNumPy\tarray.\tThere\tis\tnothing\tin\tScikit-Learn\tto handle\tPandas\tDataFrames,19\tbut\twe\tcan\twrite\ta\tcustom\ttransformer\tfor\tthis\ttask:\n\nfrom\tsklearn.base\timport\tBaseEstimator,\tTransformerMixin\n\nclass\tDataFrameSelector(BaseEstimator,\tTransformerMixin): \t\t\t\tdef\t__init__(self,\tattribute_names): \t\t\t\t\t\t\t\tself.attribute_names\t=\tattribute_names \t\t\t\tdef\tfit(self,\tX,\ty=None): \t\t\t\t\t\t\t\treturn\tself \t\t\t\tdef\ttransform(self,\tX): \t\t\t\t\t\t\t\treturn\tX[self.attribute_names].values\n\nOur\tDataFrameSelector\twill\ttransform\tthe\tdata\tby\tselecting\tthe\tdesired\tattributes,\tdropping\tthe\trest, and\tconverting\tthe\tresulting\tDataFrame\tto\ta\tNumPy\tarray.\tWith\tthis,\tyou\tcan\teasily\twrite\ta\tpipeline\tthat will\ttake\ta\tPandas\tDataFrame\tand\thandle\tonly\tthe\tnumerical\tvalues:\tthe\tpipeline\twould\tjust\tstart\twith\ta DataFrameSelector\tto\tpick\tonly\tthe\tnumerical\tattributes,\tfollowed\tby\tthe\tother\tpreprocessing\tsteps\twe discussed\tearlier.\tAnd\tyou\tcan\tjust\tas\teasily\twrite\tanother\tpipeline\tfor\tthe\tcategorical\tattributes\tas\twell by\tsimply\tselecting\tthe\tcategorical\tattributes\tusing\ta\tDataFrameSelector\tand\tthen\tapplying\ta LabelBinarizer\t.",
      "content_length": 2739,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "num_attribs\t=\tlist(housing_num) cat_attribs\t=\t[\"ocean_proximity\"]\n\nnum_pipeline\t=\tPipeline([ \t\t\t\t\t\t\t\t('selector',\tDataFrameSelector(num_attribs)), \t\t\t\t\t\t\t\t('imputer',\tImputer(strategy=\"median\")), \t\t\t\t\t\t\t\t('attribs_adder',\tCombinedAttributesAdder()), \t\t\t\t\t\t\t\t('std_scaler',\tStandardScaler()), \t\t\t\t])\n\ncat_pipeline\t=\tPipeline([ \t\t\t\t\t\t\t\t('selector',\tDataFrameSelector(cat_attribs)), \t\t\t\t\t\t\t\t('label_binarizer',\tLabelBinarizer()), \t\t\t\t])\n\nBut\thow\tcan\tyou\tjoin\tthese\ttwo\tpipelines\tinto\ta\tsingle\tpipeline?\tThe\tanswer\tis\tto\tuse\tScikit-Learn’s FeatureUnion\tclass.\tYou\tgive\tit\ta\tlist\tof\ttransformers\t(which\tcan\tbe\tentire\ttransformer\tpipelines);\twhen its\ttransform()\tmethod\tis\tcalled,\tit\truns\teach\ttransformer’s\ttransform()\tmethod\tin\tparallel,\twaits\tfor their\toutput,\tand\tthen\tconcatenates\tthem\tand\treturns\tthe\tresult\t(and\tof\tcourse\tcalling\tits\tfit()\tmethod calls\teach\ttransformer’s\tfit()\tmethod).\tA\tfull\tpipeline\thandling\tboth\tnumerical\tand\tcategorical attributes\tmay\tlook\tlike\tthis:\n\nfrom\tsklearn.pipeline\timport\tFeatureUnion\n\nfull_pipeline\t=\tFeatureUnion(transformer_list=[ \t\t\t\t\t\t\t\t(\"num_pipeline\",\tnum_pipeline), \t\t\t\t\t\t\t\t(\"cat_pipeline\",\tcat_pipeline), \t\t\t\t])\n\nAnd\tyou\tcan\trun\tthe\twhole\tpipeline\tsimply:\n\n>>>\thousing_prepared\t=\tfull_pipeline.fit_transform(housing) >>>\thousing_prepared array([[-1.15604281,\t\t0.77194962,\t\t0.74333089,\t...,\t\t0.\t\t\t\t\t\t\t\t, \t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t,\t\t0.\t\t\t\t\t\t\t\t], \t\t\t\t\t\t\t[-1.17602483,\t\t0.6596948\t,\t-1.1653172\t,\t...,\t\t0.\t\t\t\t\t\t\t\t, \t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t,\t\t0.\t\t\t\t\t\t\t\t], \t\t\t\t\t\t\t[...] >>>\thousing_prepared.shape (16512,\t16)",
      "content_length": 1531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Select\tand\tTrain\ta\tModel At\tlast!\tYou\tframed\tthe\tproblem,\tyou\tgot\tthe\tdata\tand\texplored\tit,\tyou\tsampled\ta\ttraining\tset\tand\ta\ttest\tset, and\tyou\twrote\ttransformation\tpipelines\tto\tclean\tup\tand\tprepare\tyour\tdata\tfor\tMachine\tLearning algorithms\tautomatically.\tYou\tare\tnow\tready\tto\tselect\tand\ttrain\ta\tMachine\tLearning\tmodel.",
      "content_length": 318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Training\tand\tEvaluating\ton\tthe\tTraining\tSet The\tgood\tnews\tis\tthat\tthanks\tto\tall\tthese\tprevious\tsteps,\tthings\tare\tnow\tgoing\tto\tbe\tmuch\tsimpler\tthan\tyou might\tthink.\tLet’s\tfirst\ttrain\ta\tLinear\tRegression\tmodel,\tlike\twe\tdid\tin\tthe\tprevious\tchapter:\n\nfrom\tsklearn.linear_model\timport\tLinearRegression\n\nlin_reg\t=\tLinearRegression() lin_reg.fit(housing_prepared,\thousing_labels)\n\nDone!\tYou\tnow\thave\ta\tworking\tLinear\tRegression\tmodel.\tLet’s\ttry\tit\tout\ton\ta\tfew\tinstances\tfrom\tthe training\tset:\n\n>>>\tsome_data\t=\thousing.iloc[:5] >>>\tsome_labels\t=\thousing_labels.iloc[:5] >>>\tsome_data_prepared\t=\tfull_pipeline.transform(some_data) >>>\tprint(\"Predictions:\",\tlin_reg.predict(some_data_prepared)) Predictions:\t[\t210644.6045\t\t317768.8069\t\t210956.4333\t\t59218.9888\t\t189747.5584] >>>\tprint(\"Labels:\",\tlist(some_labels)) Labels:\t[286600.0,\t340600.0,\t196900.0,\t46300.0,\t254500.0]\n\nIt\tworks,\talthough\tthe\tpredictions\tare\tnot\texactly\taccurate\t(e.g.,\tthe\tfirst\tprediction\tis\toff\tby\tclose\tto 40%!).\tLet’s\tmeasure\tthis\tregression\tmodel’s\tRMSE\ton\tthe\twhole\ttraining\tset\tusing\tScikit-Learn’s mean_squared_error\tfunction:\n\n>>>\tfrom\tsklearn.metrics\timport\tmean_squared_error >>>\thousing_predictions\t=\tlin_reg.predict(housing_prepared) >>>\tlin_mse\t=\tmean_squared_error(housing_labels,\thousing_predictions) >>>\tlin_rmse\t=\tnp.sqrt(lin_mse) >>>\tlin_rmse 68628.198198489219\n\nOkay,\tthis\tis\tbetter\tthan\tnothing\tbut\tclearly\tnot\ta\tgreat\tscore:\tmost\tdistricts’\tmedian_housing_values range\tbetween\t$120,000\tand\t$265,000,\tso\ta\ttypical\tprediction\terror\tof\t$68,628\tis\tnot\tvery\tsatisfying. This\tis\tan\texample\tof\ta\tmodel\tunderfitting\tthe\ttraining\tdata.\tWhen\tthis\thappens\tit\tcan\tmean\tthat\tthe features\tdo\tnot\tprovide\tenough\tinformation\tto\tmake\tgood\tpredictions,\tor\tthat\tthe\tmodel\tis\tnot\tpowerful enough.\tAs\twe\tsaw\tin\tthe\tprevious\tchapter,\tthe\tmain\tways\tto\tfix\tunderfitting\tare\tto\tselect\ta\tmore powerful\tmodel,\tto\tfeed\tthe\ttraining\talgorithm\twith\tbetter\tfeatures,\tor\tto\treduce\tthe\tconstraints\ton\tthe model.\tThis\tmodel\tis\tnot\tregularized,\tso\tthis\trules\tout\tthe\tlast\toption.\tYou\tcould\ttry\tto\tadd\tmore\tfeatures (e.g.,\tthe\tlog\tof\tthe\tpopulation),\tbut\tfirst\tlet’s\ttry\ta\tmore\tcomplex\tmodel\tto\tsee\thow\tit\tdoes.\n\nLet’s\ttrain\ta\tDecisionTreeRegressor.\tThis\tis\ta\tpowerful\tmodel,\tcapable\tof\tfinding\tcomplex\tnonlinear relationships\tin\tthe\tdata\t(Decision\tTrees\tare\tpresented\tin\tmore\tdetail\tin\tChapter\t6).\tThe\tcode\tshould\tlook familiar\tby\tnow:\n\nfrom\tsklearn.tree\timport\tDecisionTreeRegressor\n\ntree_reg\t=\tDecisionTreeRegressor() tree_reg.fit(housing_prepared,\thousing_labels)\n\nNow\tthat\tthe\tmodel\tis\ttrained,\tlet’s\tevaluate\tit\ton\tthe\ttraining\tset:",
      "content_length": 2586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": ">>>\thousing_predictions\t=\ttree_reg.predict(housing_prepared) >>>\ttree_mse\t=\tmean_squared_error(housing_labels,\thousing_predictions) >>>\ttree_rmse\t=\tnp.sqrt(tree_mse) >>>\ttree_rmse 0.0\n\nWait,\twhat!?\tNo\terror\tat\tall?\tCould\tthis\tmodel\treally\tbe\tabsolutely\tperfect?\tOf\tcourse,\tit\tis\tmuch\tmore likely\tthat\tthe\tmodel\thas\tbadly\toverfit\tthe\tdata.\tHow\tcan\tyou\tbe\tsure?\tAs\twe\tsaw\tearlier,\tyou\tdon’t\twant to\ttouch\tthe\ttest\tset\tuntil\tyou\tare\tready\tto\tlaunch\ta\tmodel\tyou\tare\tconfident\tabout,\tso\tyou\tneed\tto\tuse\tpart of\tthe\ttraining\tset\tfor\ttraining,\tand\tpart\tfor\tmodel\tvalidation.",
      "content_length": 567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Better\tEvaluation\tUsing\tCross-Validation One\tway\tto\tevaluate\tthe\tDecision\tTree\tmodel\twould\tbe\tto\tuse\tthe\ttrain_test_split\tfunction\tto\tsplit the\ttraining\tset\tinto\ta\tsmaller\ttraining\tset\tand\ta\tvalidation\tset,\tthen\ttrain\tyour\tmodels\tagainst\tthe\tsmaller training\tset\tand\tevaluate\tthem\tagainst\tthe\tvalidation\tset.\tIt’s\ta\tbit\tof\twork,\tbut\tnothing\ttoo\tdifficult\tand\tit would\twork\tfairly\twell.\n\nA\tgreat\talternative\tis\tto\tuse\tScikit-Learn’s\tcross-validation\tfeature.\tThe\tfollowing\tcode\tperforms\tK-fold cross-validation:\tit\trandomly\tsplits\tthe\ttraining\tset\tinto\t10\tdistinct\tsubsets\tcalled\tfolds,\tthen\tit\ttrains\tand evaluates\tthe\tDecision\tTree\tmodel\t10\ttimes,\tpicking\ta\tdifferent\tfold\tfor\tevaluation\tevery\ttime\tand training\ton\tthe\tother\t9\tfolds.\tThe\tresult\tis\tan\tarray\tcontaining\tthe\t10\tevaluation\tscores:\n\nfrom\tsklearn.model_selection\timport\tcross_val_score scores\t=\tcross_val_score(tree_reg,\thousing_prepared,\thousing_labels, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscoring=\"neg_mean_squared_error\",\tcv=10) tree_rmse_scores\t=\tnp.sqrt(-scores)\n\nWARNING\n\nScikit-Learn\tcross-validation\tfeatures\texpect\ta\tutility\tfunction\t(greater\tis\tbetter)\trather\tthan\ta\tcost\tfunction\t(lower\tis\tbetter),\tso the\tscoring\tfunction\tis\tactually\tthe\topposite\tof\tthe\tMSE\t(i.e.,\ta\tnegative\tvalue),\twhich\tis\twhy\tthe\tpreceding\tcode\tcomputes\t- scores\tbefore\tcalculating\tthe\tsquare\troot.\n\nLet’s\tlook\tat\tthe\tresults:\n\n>>>\tdef\tdisplay_scores(scores): ...\t\t\t\t\tprint(\"Scores:\",\tscores) ...\t\t\t\t\tprint(\"Mean:\",\tscores.mean()) ...\t\t\t\t\tprint(\"Standard\tdeviation:\",\tscores.std()) ... >>>\tdisplay_scores(tree_rmse_scores) Scores:\t[\t70232.0136482\t\t\t66828.46839892\t\t72444.08721003\t\t70761.50186201 \t\t71125.52697653\t\t75581.29319857\t\t70169.59286164\t\t70055.37863456 \t\t75370.49116773\t\t71222.39081244] Mean:\t71379.0744771 Standard\tdeviation:\t2458.31882043\n\nNow\tthe\tDecision\tTree\tdoesn’t\tlook\tas\tgood\tas\tit\tdid\tearlier.\tIn\tfact,\tit\tseems\tto\tperform\tworse\tthan\tthe Linear\tRegression\tmodel!\tNotice\tthat\tcross-validation\tallows\tyou\tto\tget\tnot\tonly\tan\testimate\tof\tthe performance\tof\tyour\tmodel,\tbut\talso\ta\tmeasure\tof\thow\tprecise\tthis\testimate\tis\t(i.e.,\tits\tstandard deviation).\tThe\tDecision\tTree\thas\ta\tscore\tof\tapproximately\t71,379,\tgenerally\t±2,458.\tYou\twould\tnot have\tthis\tinformation\tif\tyou\tjust\tused\tone\tvalidation\tset.\tBut\tcross-validation\tcomes\tat\tthe\tcost\tof\ttraining the\tmodel\tseveral\ttimes,\tso\tit\tis\tnot\talways\tpossible.\n\nLet’s\tcompute\tthe\tsame\tscores\tfor\tthe\tLinear\tRegression\tmodel\tjust\tto\tbe\tsure:\n\n>>>\tlin_scores\t=\tcross_val_score(lin_reg,\thousing_prepared,\thousing_labels, ...\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscoring=\"neg_mean_squared_error\",\tcv=10) ... >>>\tlin_rmse_scores\t=\tnp.sqrt(-lin_scores) >>>\tdisplay_scores(lin_rmse_scores) Scores:\t[\t66760.97371572\t\t66962.61914244\t\t70349.94853401\t\t74757.02629506 \t\t68031.13388938\t\t71193.84183426\t\t64968.13706527\t\t68261.95557897",
      "content_length": 2803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "71527.64217874\t\t67665.10082067] Mean:\t69047.8379055 Standard\tdeviation:\t2735.51074287\n\nThat’s\tright:\tthe\tDecision\tTree\tmodel\tis\toverfitting\tso\tbadly\tthat\tit\tperforms\tworse\tthan\tthe\tLinear Regression\tmodel.\n\nLet’s\ttry\tone\tlast\tmodel\tnow:\tthe\tRandomForestRegressor.\tAs\twe\twill\tsee\tin\tChapter\t7,\tRandom Forests\twork\tby\ttraining\tmany\tDecision\tTrees\ton\trandom\tsubsets\tof\tthe\tfeatures,\tthen\taveraging\tout\ttheir predictions.\tBuilding\ta\tmodel\ton\ttop\tof\tmany\tother\tmodels\tis\tcalled\tEnsemble\tLearning,\tand\tit\tis\toften\ta great\tway\tto\tpush\tML\talgorithms\teven\tfurther.\tWe\twill\tskip\tmost\tof\tthe\tcode\tsince\tit\tis\tessentially\tthe same\tas\tfor\tthe\tother\tmodels:\n\n>>>\tfrom\tsklearn.ensemble\timport\tRandomForestRegressor >>>\tforest_reg\t=\tRandomForestRegressor() >>>\tforest_reg.fit(housing_prepared,\thousing_labels) >>>\t[...] >>>\tforest_rmse 21941.911027380233 >>>\tdisplay_scores(forest_rmse_scores) Scores:\t[\t51650.94405471\t\t48920.80645498\t\t52979.16096752\t\t54412.74042021 \t\t50861.29381163\t\t56488.55699727\t\t51866.90120786\t\t49752.24599537 \t\t55399.50713191\t\t53309.74548294] Mean:\t52564.1902524 Standard\tdeviation:\t2301.87380392\n\nWow,\tthis\tis\tmuch\tbetter:\tRandom\tForests\tlook\tvery\tpromising.\tHowever,\tnote\tthat\tthe\tscore\ton\tthe training\tset\tis\tstill\tmuch\tlower\tthan\ton\tthe\tvalidation\tsets,\tmeaning\tthat\tthe\tmodel\tis\tstill\toverfitting\tthe training\tset.\tPossible\tsolutions\tfor\toverfitting\tare\tto\tsimplify\tthe\tmodel,\tconstrain\tit\t(i.e.,\tregularize\tit),\tor get\ta\tlot\tmore\ttraining\tdata.\tHowever,\tbefore\tyou\tdive\tmuch\tdeeper\tin\tRandom\tForests,\tyou\tshould\ttry\tout many\tother\tmodels\tfrom\tvarious\tcategories\tof\tMachine\tLearning\talgorithms\t(several\tSupport\tVector Machines\twith\tdifferent\tkernels,\tpossibly\ta\tneural\tnetwork,\tetc.),\twithout\tspending\ttoo\tmuch\ttime tweaking\tthe\thyperparameters.\tThe\tgoal\tis\tto\tshortlist\ta\tfew\t(two\tto\tfive)\tpromising\tmodels.\n\nTIP\n\nYou\tshould\tsave\tevery\tmodel\tyou\texperiment\twith,\tso\tyou\tcan\tcome\tback\teasily\tto\tany\tmodel\tyou\twant.\tMake\tsure\tyou\tsave both\tthe\thyperparameters\tand\tthe\ttrained\tparameters,\tas\twell\tas\tthe\tcross-validation\tscores\tand\tperhaps\tthe\tactual\tpredictions\tas well.\tThis\twill\tallow\tyou\tto\teasily\tcompare\tscores\tacross\tmodel\ttypes,\tand\tcompare\tthe\ttypes\tof\terrors\tthey\tmake.\tYou\tcan easily\tsave\tScikit-Learn\tmodels\tby\tusing\tPython’s\tpickle\tmodule,\tor\tusing\tsklearn.externals.joblib,\twhich\tis\tmore\tefficient at\tserializing\tlarge\tNumPy\tarrays:\n\nfrom\tsklearn.externals\timport\tjoblib\n\njoblib.dump(my_model,\t\"my_model.pkl\") #\tand\tlater... my_model_loaded\t=\tjoblib.load(\"my_model.pkl\")",
      "content_length": 2494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Fine-Tune\tYour\tModel Let’s\tassume\tthat\tyou\tnow\thave\ta\tshortlist\tof\tpromising\tmodels.\tYou\tnow\tneed\tto\tfine-tune\tthem.\tLet’s look\tat\ta\tfew\tways\tyou\tcan\tdo\tthat.",
      "content_length": 158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Grid\tSearch One\tway\tto\tdo\tthat\twould\tbe\tto\tfiddle\twith\tthe\thyperparameters\tmanually,\tuntil\tyou\tfind\ta\tgreat combination\tof\thyperparameter\tvalues.\tThis\twould\tbe\tvery\ttedious\twork,\tand\tyou\tmay\tnot\thave\ttime\tto explore\tmany\tcombinations.\n\nInstead\tyou\tshould\tget\tScikit-Learn’s\tGridSearchCV\tto\tsearch\tfor\tyou.\tAll\tyou\tneed\tto\tdo\tis\ttell\tit\twhich hyperparameters\tyou\twant\tit\tto\texperiment\twith,\tand\twhat\tvalues\tto\ttry\tout,\tand\tit\twill\tevaluate\tall\tthe possible\tcombinations\tof\thyperparameter\tvalues,\tusing\tcross-validation.\tFor\texample,\tthe\tfollowing\tcode searches\tfor\tthe\tbest\tcombination\tof\thyperparameter\tvalues\tfor\tthe\tRandomForestRegressor:\n\nfrom\tsklearn.model_selection\timport\tGridSearchCV\n\nparam_grid\t=\t[ \t\t\t\t{'n_estimators':\t[3,\t10,\t30],\t'max_features':\t[2,\t4,\t6,\t8]}, \t\t\t\t{'bootstrap':\t[False],\t'n_estimators':\t[3,\t10],\t'max_features':\t[2,\t3,\t4]}, \t\t]\n\nforest_reg\t=\tRandomForestRegressor()\n\ngrid_search\t=\tGridSearchCV(forest_reg,\tparam_grid,\tcv=5, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscoring='neg_mean_squared_error')\n\ngrid_search.fit(housing_prepared,\thousing_labels)\n\nTIP\n\nWhen\tyou\thave\tno\tidea\twhat\tvalue\ta\thyperparameter\tshould\thave,\ta\tsimple\tapproach\tis\tto\ttry\tout\tconsecutive\tpowers\tof\t10\t(or\ta smaller\tnumber\tif\tyou\twant\ta\tmore\tfine-grained\tsearch,\tas\tshown\tin\tthis\texample\twith\tthe\tn_estimators\thyperparameter).\n\nThis\tparam_grid\ttells\tScikit-Learn\tto\tfirst\tevaluate\tall\t3\t×\t4\t=\t12\tcombinations\tof\tn_estimators\tand max_features\thyperparameter\tvalues\tspecified\tin\tthe\tfirst\tdict\t(don’t\tworry\tabout\twhat\tthese hyperparameters\tmean\tfor\tnow;\tthey\twill\tbe\texplained\tin\tChapter\t7),\tthen\ttry\tall\t2\t×\t3\t=\t6\tcombinations of\thyperparameter\tvalues\tin\tthe\tsecond\tdict,\tbut\tthis\ttime\twith\tthe\tbootstrap\thyperparameter\tset\tto False\tinstead\tof\tTrue\t(which\tis\tthe\tdefault\tvalue\tfor\tthis\thyperparameter).\n\nAll\tin\tall,\tthe\tgrid\tsearch\twill\texplore\t12\t+\t6\t=\t18\tcombinations\tof\tRandomForestRegressor hyperparameter\tvalues,\tand\tit\twill\ttrain\teach\tmodel\tfive\ttimes\t(since\twe\tare\tusing\tfive-fold\tcross validation).\tIn\tother\twords,\tall\tin\tall,\tthere\twill\tbe\t18\t×\t5\t=\t90\trounds\tof\ttraining!\tIt\tmay\ttake\tquite\ta\tlong time,\tbut\twhen\tit\tis\tdone\tyou\tcan\tget\tthe\tbest\tcombination\tof\tparameters\tlike\tthis:\n\n>>>\tgrid_search.best_params_ {'max_features':\t8,\t'n_estimators':\t30}\n\nTIP\n\nSince\t8\tand\t30\tare\tthe\tmaximum\tvalues\tthat\twere\tevaluated,\tyou\tshould\tprobably\ttry\tsearching\tagain\twith\thigher\tvalues,\tsince the\tscore\tmay\tcontinue\tto\timprove.",
      "content_length": 2412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "You\tcan\talso\tget\tthe\tbest\testimator\tdirectly:\n\n>>>\tgrid_search.best_estimator_ RandomForestRegressor(bootstrap=True,\tcriterion='mse',\tmax_depth=None, \t\t\t\t\t\t\t\t\t\t\tmax_features=8,\tmax_leaf_nodes=None,\tmin_impurity_split=1e-07, \t\t\t\t\t\t\t\t\t\t\tmin_samples_leaf=1,\tmin_samples_split=2, \t\t\t\t\t\t\t\t\t\t\tmin_weight_fraction_leaf=0.0,\tn_estimators=30,\tn_jobs=1, \t\t\t\t\t\t\t\t\t\t\toob_score=False,\trandom_state=42,\tverbose=0,\twarm_start=False)\n\nNOTE\n\nIf\tGridSearchCV\tis\tinitialized\twith\trefit=True\t(which\tis\tthe\tdefault),\tthen\tonce\tit\tfinds\tthe\tbest\testimator\tusing\tcross-validation,\tit retrains\tit\ton\tthe\twhole\ttraining\tset.\tThis\tis\tusually\ta\tgood\tidea\tsince\tfeeding\tit\tmore\tdata\twill\tlikely\timprove\tits\tperformance.\n\nAnd\tof\tcourse\tthe\tevaluation\tscores\tare\talso\tavailable:\n\n>>>\tcvres\t=\tgrid_search.cv_results_ >>>\tfor\tmean_score,\tparams\tin\tzip(cvres[\"mean_test_score\"],\tcvres[\"params\"]): ...\t\t\t\t\tprint(np.sqrt(-mean_score),\tparams) ... 63825.0479302\t{'max_features':\t2,\t'n_estimators':\t3} 55643.8429091\t{'max_features':\t2,\t'n_estimators':\t10} 53380.6566859\t{'max_features':\t2,\t'n_estimators':\t30} 60959.1388585\t{'max_features':\t4,\t'n_estimators':\t3} 52740.5841667\t{'max_features':\t4,\t'n_estimators':\t10} 50374.1421461\t{'max_features':\t4,\t'n_estimators':\t30} 58661.2866462\t{'max_features':\t6,\t'n_estimators':\t3} 52009.9739798\t{'max_features':\t6,\t'n_estimators':\t10} 50154.1177737\t{'max_features':\t6,\t'n_estimators':\t30} 57865.3616801\t{'max_features':\t8,\t'n_estimators':\t3} 51730.0755087\t{'max_features':\t8,\t'n_estimators':\t10} 49694.8514333\t{'max_features':\t8,\t'n_estimators':\t30} 62874.4073931\t{'max_features':\t2,\t'n_estimators':\t3,\t'bootstrap':\tFalse} 54561.9398157\t{'max_features':\t2,\t'n_estimators':\t10,\t'bootstrap':\tFalse} 59416.6463145\t{'max_features':\t3,\t'n_estimators':\t3,\t'bootstrap':\tFalse} 52660.245911\t{'max_features':\t3,\t'n_estimators':\t10,\t'bootstrap':\tFalse} 57490.0168279\t{'max_features':\t4,\t'n_estimators':\t3,\t'bootstrap':\tFalse} 51093.9059428\t{'max_features':\t4,\t'n_estimators':\t10,\t'bootstrap':\tFalse}\n\nIn\tthis\texample,\twe\tobtain\tthe\tbest\tsolution\tby\tsetting\tthe\tmax_features\thyperparameter\tto\t8,\tand\tthe n_estimators\thyperparameter\tto\t30.\tThe\tRMSE\tscore\tfor\tthis\tcombination\tis\t49,694,\twhich\tis\tslightly better\tthan\tthe\tscore\tyou\tgot\tearlier\tusing\tthe\tdefault\thyperparameter\tvalues\t(which\twas\t52,564). Congratulations,\tyou\thave\tsuccessfully\tfine-tuned\tyour\tbest\tmodel!\n\nTIP\n\nDon’t\tforget\tthat\tyou\tcan\ttreat\tsome\tof\tthe\tdata\tpreparation\tsteps\tas\thyperparameters.\tFor\texample,\tthe\tgrid\tsearch\twill automatically\tfind\tout\twhether\tor\tnot\tto\tadd\ta\tfeature\tyou\twere\tnot\tsure\tabout\t(e.g.,\tusing\tthe\tadd_bedrooms_per_room hyperparameter\tof\tyour\tCombinedAttributesAdder\ttransformer).\tIt\tmay\tsimilarly\tbe\tused\tto\tautomatically\tfind\tthe\tbest\tway\tto handle\toutliers,\tmissing\tfeatures,\tfeature\tselection,\tand\tmore.",
      "content_length": 2796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Randomized\tSearch The\tgrid\tsearch\tapproach\tis\tfine\twhen\tyou\tare\texploring\trelatively\tfew\tcombinations,\tlike\tin\tthe\tprevious example,\tbut\twhen\tthe\thyperparameter\tsearch\tspace\tis\tlarge,\tit\tis\toften\tpreferable\tto\tuse RandomizedSearchCV\tinstead.\tThis\tclass\tcan\tbe\tused\tin\tmuch\tthe\tsame\tway\tas\tthe\tGridSearchCV\tclass, but\tinstead\tof\ttrying\tout\tall\tpossible\tcombinations,\tit\tevaluates\ta\tgiven\tnumber\tof\trandom\tcombinations by\tselecting\ta\trandom\tvalue\tfor\teach\thyperparameter\tat\tevery\titeration.\tThis\tapproach\thas\ttwo\tmain benefits:\n\nIf\tyou\tlet\tthe\trandomized\tsearch\trun\tfor,\tsay,\t1,000\titerations,\tthis\tapproach\twill\texplore\t1,000 different\tvalues\tfor\teach\thyperparameter\t(instead\tof\tjust\ta\tfew\tvalues\tper\thyperparameter\twith\tthe grid\tsearch\tapproach).\n\nYou\thave\tmore\tcontrol\tover\tthe\tcomputing\tbudget\tyou\twant\tto\tallocate\tto\thyperparameter\tsearch, simply\tby\tsetting\tthe\tnumber\tof\titerations.",
      "content_length": 886,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Ensemble\tMethods Another\tway\tto\tfine-tune\tyour\tsystem\tis\tto\ttry\tto\tcombine\tthe\tmodels\tthat\tperform\tbest.\tThe\tgroup\t(or “ensemble”)\twill\toften\tperform\tbetter\tthan\tthe\tbest\tindividual\tmodel\t(just\tlike\tRandom\tForests\tperform better\tthan\tthe\tindividual\tDecision\tTrees\tthey\trely\ton),\tespecially\tif\tthe\tindividual\tmodels\tmake\tvery different\ttypes\tof\terrors.\tWe\twill\tcover\tthis\ttopic\tin\tmore\tdetail\tin\tChapter\t7.",
      "content_length": 405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Analyze\tthe\tBest\tModels\tand\tTheir\tErrors You\twill\toften\tgain\tgood\tinsights\ton\tthe\tproblem\tby\tinspecting\tthe\tbest\tmodels.\tFor\texample,\tthe RandomForestRegressor\tcan\tindicate\tthe\trelative\timportance\tof\teach\tattribute\tfor\tmaking\taccurate predictions:\n\n>>>\tfeature_importances\t=\tgrid_search.best_estimator_.feature_importances_ >>>\tfeature_importances array([\t\t7.33442355e-02,\t\t\t6.29090705e-02,\t\t\t4.11437985e-02, \t\t\t\t\t\t\t\t\t1.46726854e-02,\t\t\t1.41064835e-02,\t\t\t1.48742809e-02, \t\t\t\t\t\t\t\t\t1.42575993e-02,\t\t\t3.66158981e-01,\t\t\t5.64191792e-02, \t\t\t\t\t\t\t\t\t1.08792957e-01,\t\t\t5.33510773e-02,\t\t\t1.03114883e-02, \t\t\t\t\t\t\t\t\t1.64780994e-01,\t\t\t6.02803867e-05,\t\t\t1.96041560e-03, \t\t\t\t\t\t\t\t\t2.85647464e-03])\n\nLet’s\tdisplay\tthese\timportance\tscores\tnext\tto\ttheir\tcorresponding\tattribute\tnames:\n\n>>>\textra_attribs\t=\t[\"rooms_per_hhold\",\t\"pop_per_hhold\",\t\"bedrooms_per_room\"] >>>\tcat_one_hot_attribs\t=\tlist(encoder.classes_) >>>\tattributes\t=\tnum_attribs\t+\textra_attribs\t+\tcat_one_hot_attribs >>>\tsorted(zip(feature_importances,\tattributes),\treverse=True) [(0.36615898061813418,\t'median_income'), \t(0.16478099356159051,\t'INLAND'), \t(0.10879295677551573,\t'pop_per_hhold'), \t(0.073344235516012421,\t'longitude'), \t(0.062909070482620302,\t'latitude'), \t(0.056419179181954007,\t'rooms_per_hhold'), \t(0.053351077347675809,\t'bedrooms_per_room'), \t(0.041143798478729635,\t'housing_median_age'), \t(0.014874280890402767,\t'population'), \t(0.014672685420543237,\t'total_rooms'), \t(0.014257599323407807,\t'households'), \t(0.014106483453584102,\t'total_bedrooms'), \t(0.010311488326303787,\t'<1H\tOCEAN'), \t(0.0028564746373201579,\t'NEAR\tOCEAN'), \t(0.0019604155994780701,\t'NEAR\tBAY'), \t(6.0280386727365991e-05,\t'ISLAND')]\n\nWith\tthis\tinformation,\tyou\tmay\twant\tto\ttry\tdropping\tsome\tof\tthe\tless\tuseful\tfeatures\t(e.g.,\tapparently\tonly one\tocean_proximity\tcategory\tis\treally\tuseful,\tso\tyou\tcould\ttry\tdropping\tthe\tothers).\n\nYou\tshould\talso\tlook\tat\tthe\tspecific\terrors\tthat\tyour\tsystem\tmakes,\tthen\ttry\tto\tunderstand\twhy\tit\tmakes them\tand\twhat\tcould\tfix\tthe\tproblem\t(adding\textra\tfeatures\tor,\ton\tthe\tcontrary,\tgetting\trid\tof\tuninformative ones,\tcleaning\tup\toutliers,\tetc.).",
      "content_length": 2106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Evaluate\tYour\tSystem\ton\tthe\tTest\tSet After\ttweaking\tyour\tmodels\tfor\ta\twhile,\tyou\teventually\thave\ta\tsystem\tthat\tperforms\tsufficiently\twell. Now\tis\tthe\ttime\tto\tevaluate\tthe\tfinal\tmodel\ton\tthe\ttest\tset.\tThere\tis\tnothing\tspecial\tabout\tthis\tprocess;\tjust get\tthe\tpredictors\tand\tthe\tlabels\tfrom\tyour\ttest\tset,\trun\tyour\tfull_pipeline\tto\ttransform\tthe\tdata\t(call transform(),\tnot\tfit_transform()!),\tand\tevaluate\tthe\tfinal\tmodel\ton\tthe\ttest\tset:\n\nfinal_model\t=\tgrid_search.best_estimator_\n\nX_test\t=\tstrat_test_set.drop(\"median_house_value\",\taxis=1) y_test\t=\tstrat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared\t=\tfull_pipeline.transform(X_test)\n\nfinal_predictions\t=\tfinal_model.predict(X_test_prepared)\n\nfinal_mse\t=\tmean_squared_error(y_test,\tfinal_predictions) final_rmse\t=\tnp.sqrt(final_mse)\t\t\t#\t=>\tevaluates\tto\t47,766.0\n\nThe\tperformance\twill\tusually\tbe\tslightly\tworse\tthan\twhat\tyou\tmeasured\tusing\tcross-validation\tif\tyou\tdid a\tlot\tof\thyperparameter\ttuning\t(because\tyour\tsystem\tends\tup\tfine-tuned\tto\tperform\twell\ton\tthe\tvalidation data,\tand\twill\tlikely\tnot\tperform\tas\twell\ton\tunknown\tdatasets).\tIt\tis\tnot\tthe\tcase\tin\tthis\texample,\tbut\twhen this\thappens\tyou\tmust\tresist\tthe\ttemptation\tto\ttweak\tthe\thyperparameters\tto\tmake\tthe\tnumbers\tlook\tgood on\tthe\ttest\tset;\tthe\timprovements\twould\tbe\tunlikely\tto\tgeneralize\tto\tnew\tdata.\n\nNow\tcomes\tthe\tproject\tprelaunch\tphase:\tyou\tneed\tto\tpresent\tyour\tsolution\t(highlighting\twhat\tyou\thave learned,\twhat\tworked\tand\twhat\tdid\tnot,\twhat\tassumptions\twere\tmade,\tand\twhat\tyour\tsystem’s\tlimitations are),\tdocument\teverything,\tand\tcreate\tnice\tpresentations\twith\tclear\tvisualizations\tand\teasy-to-remember statements\t(e.g.,\t“the\tmedian\tincome\tis\tthe\tnumber\tone\tpredictor\tof\thousing\tprices”).",
      "content_length": 1716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Launch,\tMonitor,\tand\tMaintain\tYour\tSystem Perfect,\tyou\tgot\tapproval\tto\tlaunch!\tYou\tneed\tto\tget\tyour\tsolution\tready\tfor\tproduction,\tin\tparticular\tby plugging\tthe\tproduction\tinput\tdata\tsources\tinto\tyour\tsystem\tand\twriting\ttests.\n\nYou\talso\tneed\tto\twrite\tmonitoring\tcode\tto\tcheck\tyour\tsystem’s\tlive\tperformance\tat\tregular\tintervals\tand trigger\talerts\twhen\tit\tdrops.\tThis\tis\timportant\tto\tcatch\tnot\tonly\tsudden\tbreakage,\tbut\talso\tperformance degradation.\tThis\tis\tquite\tcommon\tbecause\tmodels\ttend\tto\t“rot”\tas\tdata\tevolves\tover\ttime,\tunless\tthe models\tare\tregularly\ttrained\ton\tfresh\tdata.\n\nEvaluating\tyour\tsystem’s\tperformance\twill\trequire\tsampling\tthe\tsystem’s\tpredictions\tand\tevaluating\tthem. This\twill\tgenerally\trequire\ta\thuman\tanalysis.\tThese\tanalysts\tmay\tbe\tfield\texperts,\tor\tworkers\ton\ta crowdsourcing\tplatform\t(such\tas\tAmazon\tMechanical\tTurk\tor\tCrowdFlower).\tEither\tway,\tyou\tneed\tto plug\tthe\thuman\tevaluation\tpipeline\tinto\tyour\tsystem.\n\nYou\tshould\talso\tmake\tsure\tyou\tevaluate\tthe\tsystem’s\tinput\tdata\tquality.\tSometimes\tperformance\twill degrade\tslightly\tbecause\tof\ta\tpoor\tquality\tsignal\t(e.g.,\ta\tmalfunctioning\tsensor\tsending\trandom\tvalues,\tor another\tteam’s\toutput\tbecoming\tstale),\tbut\tit\tmay\ttake\ta\twhile\tbefore\tyour\tsystem’s\tperformance\tdegrades enough\tto\ttrigger\tan\talert.\tIf\tyou\tmonitor\tyour\tsystem’s\tinputs,\tyou\tmay\tcatch\tthis\tearlier.\tMonitoring\tthe inputs\tis\tparticularly\timportant\tfor\tonline\tlearning\tsystems.\n\nFinally,\tyou\twill\tgenerally\twant\tto\ttrain\tyour\tmodels\ton\ta\tregular\tbasis\tusing\tfresh\tdata.\tYou\tshould automate\tthis\tprocess\tas\tmuch\tas\tpossible.\tIf\tyou\tdon’t,\tyou\tare\tvery\tlikely\tto\trefresh\tyour\tmodel\tonly every\tsix\tmonths\t(at\tbest),\tand\tyour\tsystem’s\tperformance\tmay\tfluctuate\tseverely\tover\ttime.\tIf\tyour system\tis\tan\tonline\tlearning\tsystem,\tyou\tshould\tmake\tsure\tyou\tsave\tsnapshots\tof\tits\tstate\tat\tregular intervals\tso\tyou\tcan\teasily\troll\tback\tto\ta\tpreviously\tworking\tstate.",
      "content_length": 1893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "Try\tIt\tOut! Hopefully\tthis\tchapter\tgave\tyou\ta\tgood\tidea\tof\twhat\ta\tMachine\tLearning\tproject\tlooks\tlike,\tand\tshowed you\tsome\tof\tthe\ttools\tyou\tcan\tuse\tto\ttrain\ta\tgreat\tsystem.\tAs\tyou\tcan\tsee,\tmuch\tof\tthe\twork\tis\tin\tthe\tdata preparation\tstep,\tbuilding\tmonitoring\ttools,\tsetting\tup\thuman\tevaluation\tpipelines,\tand\tautomating\tregular model\ttraining.\tThe\tMachine\tLearning\talgorithms\tare\talso\timportant,\tof\tcourse,\tbut\tit\tis\tprobably preferable\tto\tbe\tcomfortable\twith\tthe\toverall\tprocess\tand\tknow\tthree\tor\tfour\talgorithms\twell\trather\tthan to\tspend\tall\tyour\ttime\texploring\tadvanced\talgorithms\tand\tnot\tenough\ttime\ton\tthe\toverall\tprocess.\n\nSo,\tif\tyou\thave\tnot\talready\tdone\tso,\tnow\tis\ta\tgood\ttime\tto\tpick\tup\ta\tlaptop,\tselect\ta\tdataset\tthat\tyou\tare interested\tin,\tand\ttry\tto\tgo\tthrough\tthe\twhole\tprocess\tfrom\tA\tto\tZ.\tA\tgood\tplace\tto\tstart\tis\ton\ta competition\twebsite\tsuch\tas\thttp://kaggle.com/:\tyou\twill\thave\ta\tdataset\tto\tplay\twith,\ta\tclear\tgoal,\tand people\tto\tshare\tthe\texperience\twith.",
      "content_length": 974,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Exercises Using\tthis\tchapter’s\thousing\tdataset:\n\n1.\t Try\ta\tSupport\tVector\tMachine\tregressor\t(sklearn.svm.SVR),\twith\tvarious\thyperparameters\tsuch\tas kernel=\"linear\"\t(with\tvarious\tvalues\tfor\tthe\tC\thyperparameter)\tor\tkernel=\"rbf\"\t(with\tvarious values\tfor\tthe\tC\tand\tgamma\thyperparameters).\tDon’t\tworry\tabout\twhat\tthese\thyperparameters\tmean for\tnow.\tHow\tdoes\tthe\tbest\tSVR\tpredictor\tperform?\n\n2.\t Try\treplacing\tGridSearchCV\twith\tRandomizedSearchCV.\n\n3.\t Try\tadding\ta\ttransformer\tin\tthe\tpreparation\tpipeline\tto\tselect\tonly\tthe\tmost\timportant\tattributes.\n\n4.\t Try\tcreating\ta\tsingle\tpipeline\tthat\tdoes\tthe\tfull\tdata\tpreparation\tplus\tthe\tfinal\tprediction.\n\n5.\t Automatically\texplore\tsome\tpreparation\toptions\tusing\tGridSearchCV.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tthe\tonline\tJupyter\tnotebooks\tat https://github.com/ageron/handson-ml.\n\n1\n\nThe\texample\tproject\tis\tcompletely\tfictitious;\tthe\tgoal\tis\tjust\tto\tillustrate\tthe\tmain\tsteps\tof\ta\tMachine\tLearning\tproject,\tnot\tto\tlearn\tanything about\tthe\treal\testate\tbusiness.\n\n2\n\nThe\toriginal\tdataset\tappeared\tin\tR.\tKelley\tPace\tand\tRonald\tBarry,\t“Sparse\tSpatial\tAutoregressions,”\tStatistics\t&\tProbability\tLetters\t33, no.\t3\t(1997):\t291–297.\n\n3\n\nA\tpiece\tof\tinformation\tfed\tto\ta\tMachine\tLearning\tsystem\tis\toften\tcalled\ta\tsignal\tin\treference\tto\tShannon’s\tinformation\ttheory:\tyou\twant\ta high\tsignal/noise\tratio.\n\n4\n\nRecall\tthat\tthe\ttranspose\toperator\tflips\ta\tcolumn\tvector\tinto\ta\trow\tvector\t(and\tvice\tversa).\n\n5\n\nThe\tlatest\tversion\tof\tPython\t3\tis\trecommended.\tPython\t2.7+\tshould\twork\tfine\ttoo,\tbut\tit\tis\tdeprecated.\tIf\tyou\tuse\tPython\t2,\tyou\tmust\tadd from\t__future__\timport\tdivision,\tprint_function,\tunicode_literals\tat\tthe\tbeginning\tof\tyour\tcode.\n\n6\n\nWe\twill\tshow\tthe\tinstallation\tsteps\tusing\tpip\tin\ta\tbash\tshell\ton\ta\tLinux\tor\tmacOS\tsystem.\tYou\tmay\tneed\tto\tadapt\tthese\tcommands\tto\tyour own\tsystem.\tOn\tWindows,\twe\trecommend\tinstalling\tAnaconda\tinstead.\n\n7\n\nYou\tmay\tneed\tto\thave\tadministrator\trights\tto\trun\tthis\tcommand;\tif\tso,\ttry\tprefixing\tit\twith\tsudo.\n\n8\n\nNote\tthat\tJupyter\tcan\thandle\tmultiple\tversions\tof\tPython,\tand\teven\tmany\tother\tlanguages\tsuch\tas\tR\tor\tOctave.\n\n9\n\nYou\tmight\talso\tneed\tto\tcheck\tlegal\tconstraints,\tsuch\tas\tprivate\tfields\tthat\tshould\tnever\tbe\tcopied\tto\tunsafe\tdatastores.\n\n10\n\nIn\ta\treal\tproject\tyou\twould\tsave\tthis\tcode\tin\ta\tPython\tfile,\tbut\tfor\tnow\tyou\tcan\tjust\twrite\tit\tin\tyour\tJupyter\tnotebook.\n\n11\n\nThe\tstandard\tdeviation\tis\tgenerally\tdenoted\tσ\t(the\tGreek\tletter\tsigma),\tand\tit\tis\tthe\tsquare\troot\tof\tthe\tvariance,\twhich\tis\tthe\taverage\tof the\tsquared\tdeviation\tfrom\tthe\tmean.\tWhen\ta\tfeature\thas\ta\tbell-shaped\tnormal\tdistribution\t(also\tcalled\ta\tGaussian\tdistribution),\twhich is\tvery\tcommon,\tthe\t“68-95-99.7”\trule\tapplies:\tabout\t68%\tof\tthe\tvalues\tfall\twithin\t1σ\tof\tthe\tmean,\t95%\twithin\t2σ,\tand\t99.7%\twithin\t3σ.\n\n12\n\nYou\twill\toften\tsee\tpeople\tset\tthe\trandom\tseed\tto\t42.\tThis\tnumber\thas\tno\tspecial\tproperty,\tother\tthan\tto\tbe\tThe\tAnswer\tto\tthe\tUltimate Question\tof\tLife,\tthe\tUniverse,\tand\tEverything.\n\n13\n\nThe\tlocation\tinformation\tis\tactually\tquite\tcoarse,\tand\tas\ta\tresult\tmany\tdistricts\twill\thave\tthe\texact\tsame\tID,\tso\tthey\twill\tend\tup\tin\tthe\tsame set\t(test\tor\ttrain).\tThis\tintroduces\tsome\tunfortunate\tsampling\tbias.\n\n14\n\nIf\tyou\tare\treading\tthis\tin\tgrayscale,\tgrab\ta\tred\tpen\tand\tscribble\tover\tmost\tof\tthe\tcoastline\tfrom\tthe\tBay\tArea\tdown\tto\tSan\tDiego\t(as\tyou might\texpect).\tYou\tcan\tadd\ta\tpatch\tof\tyellow\taround\tSacramento\tas\twell.\n\n15\n\nFor\tmore\tdetails\ton\tthe\tdesign\tprinciples,\tsee\t“API\tdesign\tfor\tmachine\tlearning\tsoftware:\texperiences\tfrom\tthe\tscikit-learn\tproject,”\tL.",
      "content_length": 3526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "16\n\n17\n\n18\n\n19\n\nBuitinck,\tG.\tLouppe,\tM.\tBlondel,\tF.\tPedregosa,\tA.\tMüller,\tet\tal.\t(2013).\n\nSome\tpredictors\talso\tprovide\tmethods\tto\tmeasure\tthe\tconfidence\tof\ttheir\tpredictions.\n\nNumPy’s\treshape()\tfunction\tallows\tone\tdimension\tto\tbe\t–1,\twhich\tmeans\t“unspecified”:\tthe\tvalue\tis\tinferred\tfrom\tthe\tlength\tof\tthe\tarray and\tthe\tremaining\tdimensions.\n\nSee\tSciPy’s\tdocumentation\tfor\tmore\tdetails.\n\nBut\tcheck\tout\tPull\tRequest\t#3886,\twhich\tmay\tintroduce\ta\tColumnTransformer\tclass\tmaking\tattribute-specific\ttransformations\teasy.\tYou could\talso\trun\tpip3\tinstall\tsklearn-pandas\tto\tget\ta\tDataFrameMapper\tclass\twith\ta\tsimilar\tobjective.",
      "content_length": 619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Chapter\t3.\tClassification\n\nIn\tChapter\t1\twe\tmentioned\tthat\tthe\tmost\tcommon\tsupervised\tlearning\ttasks\tare\tregression\t(predicting values)\tand\tclassification\t(predicting\tclasses).\tIn\tChapter\t2\twe\texplored\ta\tregression\ttask,\tpredicting housing\tvalues,\tusing\tvarious\talgorithms\tsuch\tas\tLinear\tRegression,\tDecision\tTrees,\tand\tRandom\tForests (which\twill\tbe\texplained\tin\tfurther\tdetail\tin\tlater\tchapters).\tNow\twe\twill\tturn\tour\tattention\tto classification\tsystems.",
      "content_length": 454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "MNIST In\tthis\tchapter,\twe\twill\tbe\tusing\tthe\tMNIST\tdataset,\twhich\tis\ta\tset\tof\t70,000\tsmall\timages\tof\tdigits handwritten\tby\thigh\tschool\tstudents\tand\temployees\tof\tthe\tUS\tCensus\tBureau.\tEach\timage\tis\tlabeled\twith the\tdigit\tit\trepresents.\tThis\tset\thas\tbeen\tstudied\tso\tmuch\tthat\tit\tis\toften\tcalled\tthe\t“Hello\tWorld”\tof Machine\tLearning:\twhenever\tpeople\tcome\tup\twith\ta\tnew\tclassification\talgorithm,\tthey\tare\tcurious\tto\tsee how\tit\twill\tperform\ton\tMNIST.\tWhenever\tsomeone\tlearns\tMachine\tLearning,\tsooner\tor\tlater\tthey\ttackle MNIST.\n\nScikit-Learn\tprovides\tmany\thelper\tfunctions\tto\tdownload\tpopular\tdatasets.\tMNIST\tis\tone\tof\tthem.\tThe following\tcode\tfetches\tthe\tMNIST\tdataset:1\n\n>>>\tfrom\tsklearn.datasets\timport\tfetch_mldata >>>\tmnist\t=\tfetch_mldata('MNIST\toriginal') >>>\tmnist {'COL_NAMES':\t['label',\t'data'], \t'DESCR':\t'mldata.org\tdataset:\tmnist-original', \t'data':\tarray([[0,\t0,\t0,\t...,\t0,\t0,\t0], \t\t\t\t\t\t\t\t[0,\t0,\t0,\t...,\t0,\t0,\t0], \t\t\t\t\t\t\t\t[0,\t0,\t0,\t...,\t0,\t0,\t0], \t\t\t\t\t\t\t\t..., \t\t\t\t\t\t\t\t[0,\t0,\t0,\t...,\t0,\t0,\t0], \t\t\t\t\t\t\t\t[0,\t0,\t0,\t...,\t0,\t0,\t0], \t\t\t\t\t\t\t\t[0,\t0,\t0,\t...,\t0,\t0,\t0]],\tdtype=uint8), \t'target':\tarray([\t0.,\t\t0.,\t\t0.,\t...,\t\t9.,\t\t9.,\t\t9.])}\n\nDatasets\tloaded\tby\tScikit-Learn\tgenerally\thave\ta\tsimilar\tdictionary\tstructure\tincluding:\n\nA\tDESCR\tkey\tdescribing\tthe\tdataset\n\nA\tdata\tkey\tcontaining\tan\tarray\twith\tone\trow\tper\tinstance\tand\tone\tcolumn\tper\tfeature\n\nA\ttarget\tkey\tcontaining\tan\tarray\twith\tthe\tlabels\n\nLet’s\tlook\tat\tthese\tarrays:\n\n>>>\tX,\ty\t=\tmnist[\"data\"],\tmnist[\"target\"] >>>\tX.shape (70000,\t784) >>>\ty.shape (70000,)\n\nThere\tare\t70,000\timages,\tand\teach\timage\thas\t784\tfeatures.\tThis\tis\tbecause\teach\timage\tis\t28×28\tpixels, and\teach\tfeature\tsimply\trepresents\tone\tpixel’s\tintensity,\tfrom\t0\t(white)\tto\t255\t(black).\tLet’s\ttake\ta\tpeek at\tone\tdigit\tfrom\tthe\tdataset.\tAll\tyou\tneed\tto\tdo\tis\tgrab\tan\tinstance’s\tfeature\tvector,\treshape\tit\tto\ta\t28×28 array,\tand\tdisplay\tit\tusing\tMatplotlib’s\timshow()\tfunction:\n\n%matplotlib\tinline import\tmatplotlib import\tmatplotlib.pyplot\tas\tplt\n\nsome_digit\t=\tX[36000] some_digit_image\t=\tsome_digit.reshape(28,\t28)\n\nplt.imshow(some_digit_image,\tcmap\t=\tmatplotlib.cm.binary, \t\t\t\t\t\t\t\t\t\t\tinterpolation=\"nearest\")",
      "content_length": 2129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "plt.axis(\"off\") plt.show()\n\nThis\tlooks\tlike\ta\t5,\tand\tindeed\tthat’s\twhat\tthe\tlabel\ttells\tus:\n\n>>>\ty[36000] 5.0\n\nFigure\t3-1\tshows\ta\tfew\tmore\timages\tfrom\tthe\tMNIST\tdataset\tto\tgive\tyou\ta\tfeel\tfor\tthe\tcomplexity\tof\tthe classification\ttask.",
      "content_length": 234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "Figure\t3-1.\tA\tfew\tdigits\tfrom\tthe\tMNIST\tdataset\n\nBut\twait!\tYou\tshould\talways\tcreate\ta\ttest\tset\tand\tset\tit\taside\tbefore\tinspecting\tthe\tdata\tclosely.\tThe MNIST\tdataset\tis\tactually\talready\tsplit\tinto\ta\ttraining\tset\t(the\tfirst\t60,000\timages)\tand\ta\ttest\tset\t(the\tlast 10,000\timages):\n\nX_train,\tX_test,\ty_train,\ty_test\t=\tX[:60000],\tX[60000:],\ty[:60000],\ty[60000:]\n\nLet’s\talso\tshuffle\tthe\ttraining\tset;\tthis\twill\tguarantee\tthat\tall\tcross-validation\tfolds\twill\tbe\tsimilar\t(you don’t\twant\tone\tfold\tto\tbe\tmissing\tsome\tdigits).\tMoreover,\tsome\tlearning\talgorithms\tare\tsensitive\tto\tthe order\tof\tthe\ttraining\tinstances,\tand\tthey\tperform\tpoorly\tif\tthey\tget\tmany\tsimilar\tinstances\tin\ta\trow. Shuffling\tthe\tdataset\tensures\tthat\tthis\twon’t\thappen:2\n\nimport\tnumpy\tas\tnp\n\nshuffle_index\t=\tnp.random.permutation(60000) X_train,\ty_train\t=\tX_train[shuffle_index],\ty_train[shuffle_index]",
      "content_length": 861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Training\ta\tBinary\tClassifier Let’s\tsimplify\tthe\tproblem\tfor\tnow\tand\tonly\ttry\tto\tidentify\tone\tdigit\t—\tfor\texample,\tthe\tnumber\t5.\tThis “5-detector”\twill\tbe\tan\texample\tof\ta\tbinary\tclassifier,\tcapable\tof\tdistinguishing\tbetween\tjust\ttwo classes,\t5\tand\tnot-5.\tLet’s\tcreate\tthe\ttarget\tvectors\tfor\tthis\tclassification\ttask:\n\ny_train_5\t=\t(y_train\t==\t5)\t\t#\tTrue\tfor\tall\t5s,\tFalse\tfor\tall\tother\tdigits. y_test_5\t=\t(y_test\t==\t5)\n\nOkay,\tnow\tlet’s\tpick\ta\tclassifier\tand\ttrain\tit.\tA\tgood\tplace\tto\tstart\tis\twith\ta\tStochastic\tGradient\tDescent (SGD)\tclassifier,\tusing\tScikit-Learn’s\tSGDClassifier\tclass.\tThis\tclassifier\thas\tthe\tadvantage\tof\tbeing capable\tof\thandling\tvery\tlarge\tdatasets\tefficiently.\tThis\tis\tin\tpart\tbecause\tSGD\tdeals\twith\ttraining instances\tindependently,\tone\tat\ta\ttime\t(which\talso\tmakes\tSGD\twell\tsuited\tfor\tonline\tlearning),\tas\twe will\tsee\tlater.\tLet’s\tcreate\tan\tSGDClassifier\tand\ttrain\tit\ton\tthe\twhole\ttraining\tset:\n\nfrom\tsklearn.linear_model\timport\tSGDClassifier\n\nsgd_clf\t=\tSGDClassifier(random_state=42) sgd_clf.fit(X_train,\ty_train_5)\n\nTIP\n\nThe\tSGDClassifier\trelies\ton\trandomness\tduring\ttraining\t(hence\tthe\tname\t“stochastic”).\tIf\tyou\twant\treproducible\tresults,\tyou should\tset\tthe\trandom_state\tparameter.\n\nNow\tyou\tcan\tuse\tit\tto\tdetect\timages\tof\tthe\tnumber\t5:\n\n>>>\tsgd_clf.predict([some_digit]) array([\tTrue],\tdtype=bool)\n\nThe\tclassifier\tguesses\tthat\tthis\timage\trepresents\ta\t5\t(True).\tLooks\tlike\tit\tguessed\tright\tin\tthis\tparticular case!\tNow,\tlet’s\tevaluate\tthis\tmodel’s\tperformance.",
      "content_length": 1485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "Performance\tMeasures Evaluating\ta\tclassifier\tis\toften\tsignificantly\ttrickier\tthan\tevaluating\ta\tregressor,\tso\twe\twill\tspend\ta\tlarge part\tof\tthis\tchapter\ton\tthis\ttopic.\tThere\tare\tmany\tperformance\tmeasures\tavailable,\tso\tgrab\tanother\tcoffee and\tget\tready\tto\tlearn\tmany\tnew\tconcepts\tand\tacronyms!",
      "content_length": 291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "Measuring\tAccuracy\tUsing\tCross-Validation A\tgood\tway\tto\tevaluate\ta\tmodel\tis\tto\tuse\tcross-validation,\tjust\tas\tyou\tdid\tin\tChapter\t2.\n\nIMPLEMENTING\tCROSS-VALIDATION\n\nOccasionally\tyou\twill\tneed\tmore\tcontrol\tover\tthe\tcross-validation\tprocess\tthan\twhat\tScikit-Learn\tprovides\toff-the-shelf.\tIn\tthese\tcases, you\tcan\timplement\tcross-validation\tyourself;\tit\tis\tactually\tfairly\tstraightforward.\tThe\tfollowing\tcode\tdoes\troughly\tthe\tsame\tthing\tas Scikit-Learn’s\tcross_val_score()\tfunction,\tand\tprints\tthe\tsame\tresult:\n\nfrom\tsklearn.model_selection\timport\tStratifiedKFold from\tsklearn.base\timport\tclone\n\nskfolds\t=\tStratifiedKFold(n_splits=3,\trandom_state=42)\n\nfor\ttrain_index,\ttest_index\tin\tskfolds.split(X_train,\ty_train_5): \t\t\t\tclone_clf\t=\tclone(sgd_clf) \t\t\t\tX_train_folds\t=\tX_train[train_index] \t\t\t\ty_train_folds\t=\t(y_train_5[train_index]) \t\t\t\tX_test_fold\t=\tX_train[test_index] \t\t\t\ty_test_fold\t=\t(y_train_5[test_index])\n\nclone_clf.fit(X_train_folds,\ty_train_folds) \t\t\t\ty_pred\t=\tclone_clf.predict(X_test_fold) \t\t\t\tn_correct\t=\tsum(y_pred\t==\ty_test_fold) \t\t\t\tprint(n_correct\t/\tlen(y_pred))\t\t#\tprints\t0.9502,\t0.96565\tand\t0.96495\n\nThe\tStratifiedKFold\tclass\tperforms\tstratified\tsampling\t(as\texplained\tin\tChapter\t2)\tto\tproduce\tfolds\tthat\tcontain\ta\trepresentative\tratio of\teach\tclass.\tAt\teach\titeration\tthe\tcode\tcreates\ta\tclone\tof\tthe\tclassifier,\ttrains\tthat\tclone\ton\tthe\ttraining\tfolds,\tand\tmakes\tpredictions\ton the\ttest\tfold.\tThen\tit\tcounts\tthe\tnumber\tof\tcorrect\tpredictions\tand\toutputs\tthe\tratio\tof\tcorrect\tpredictions.\n\nLet’s\tuse\tthe\tcross_val_score()\tfunction\tto\tevaluate\tyour\tSGDClassifier\tmodel\tusing\tK-fold\tcross- validation,\twith\tthree\tfolds.\tRemember\tthat\tK-fold\tcross-validation\tmeans\tsplitting\tthe\ttraining\tset\tinto K-folds\t(in\tthis\tcase,\tthree),\tthen\tmaking\tpredictions\tand\tevaluating\tthem\ton\teach\tfold\tusing\ta\tmodel trained\ton\tthe\tremaining\tfolds\t(see\tChapter\t2):\n\n>>>\tfrom\tsklearn.model_selection\timport\tcross_val_score >>>\tcross_val_score(sgd_clf,\tX_train,\ty_train_5,\tcv=3,\tscoring=\"accuracy\") array([\t0.9502\t,\t\t0.96565,\t\t0.96495])\n\nWow!\tAbove\t95%\taccuracy\t(ratio\tof\tcorrect\tpredictions)\ton\tall\tcross-validation\tfolds?\tThis\tlooks amazing,\tdoesn’t\tit?\tWell,\tbefore\tyou\tget\ttoo\texcited,\tlet’s\tlook\tat\ta\tvery\tdumb\tclassifier\tthat\tjust classifies\tevery\tsingle\timage\tin\tthe\t“not-5”\tclass:\n\nfrom\tsklearn.base\timport\tBaseEstimator\n\nclass\tNever5Classifier(BaseEstimator): \t\t\t\tdef\tfit(self,\tX,\ty=None): \t\t\t\t\t\t\t\tpass \t\t\t\tdef\tpredict(self,\tX): \t\t\t\t\t\t\t\treturn\tnp.zeros((len(X),\t1),\tdtype=bool)\n\nCan\tyou\tguess\tthis\tmodel’s\taccuracy?\tLet’s\tfind\tout:\n\n>>>\tnever_5_clf\t=\tNever5Classifier() >>>\tcross_val_score(never_5_clf,\tX_train,\ty_train_5,\tcv=3,\tscoring=\"accuracy\") array([\t0.909\t\t,\t\t0.90715,\t\t0.9128\t])",
      "content_length": 2687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "That’s\tright,\tit\thas\tover\t90%\taccuracy!\tThis\tis\tsimply\tbecause\tonly\tabout\t10%\tof\tthe\timages\tare\t5s,\tso\tif you\talways\tguess\tthat\tan\timage\tis\tnot\ta\t5,\tyou\twill\tbe\tright\tabout\t90%\tof\tthe\ttime.\tBeats\tNostradamus.\n\nThis\tdemonstrates\twhy\taccuracy\tis\tgenerally\tnot\tthe\tpreferred\tperformance\tmeasure\tfor\tclassifiers, especially\twhen\tyou\tare\tdealing\twith\tskewed\tdatasets\t(i.e.,\twhen\tsome\tclasses\tare\tmuch\tmore\tfrequent than\tothers).",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Confusion\tMatrix A\tmuch\tbetter\tway\tto\tevaluate\tthe\tperformance\tof\ta\tclassifier\tis\tto\tlook\tat\tthe\tconfusion\tmatrix.\tThe general\tidea\tis\tto\tcount\tthe\tnumber\tof\ttimes\tinstances\tof\tclass\tA\tare\tclassified\tas\tclass\tB.\tFor\texample,\tto know\tthe\tnumber\tof\ttimes\tthe\tclassifier\tconfused\timages\tof\t5s\twith\t3s,\tyou\twould\tlook\tin\tthe\t5th\trow\tand 3rd\tcolumn\tof\tthe\tconfusion\tmatrix.\n\nTo\tcompute\tthe\tconfusion\tmatrix,\tyou\tfirst\tneed\tto\thave\ta\tset\tof\tpredictions,\tso\tthey\tcan\tbe\tcompared\tto the\tactual\ttargets.\tYou\tcould\tmake\tpredictions\ton\tthe\ttest\tset,\tbut\tlet’s\tkeep\tit\tuntouched\tfor\tnow (remember\tthat\tyou\twant\tto\tuse\tthe\ttest\tset\tonly\tat\tthe\tvery\tend\tof\tyour\tproject,\tonce\tyou\thave\ta\tclassifier that\tyou\tare\tready\tto\tlaunch).\tInstead,\tyou\tcan\tuse\tthe\tcross_val_predict()\tfunction:\n\nfrom\tsklearn.model_selection\timport\tcross_val_predict\n\ny_train_pred\t=\tcross_val_predict(sgd_clf,\tX_train,\ty_train_5,\tcv=3)\n\nJust\tlike\tthe\tcross_val_score()\tfunction,\tcross_val_predict()\tperforms\tK-fold\tcross-validation, but\tinstead\tof\treturning\tthe\tevaluation\tscores,\tit\treturns\tthe\tpredictions\tmade\ton\teach\ttest\tfold.\tThis\tmeans that\tyou\tget\ta\tclean\tprediction\tfor\teach\tinstance\tin\tthe\ttraining\tset\t(“clean”\tmeaning\tthat\tthe\tprediction\tis made\tby\ta\tmodel\tthat\tnever\tsaw\tthe\tdata\tduring\ttraining).\n\nNow\tyou\tare\tready\tto\tget\tthe\tconfusion\tmatrix\tusing\tthe\tconfusion_matrix()\tfunction.\tJust\tpass\tit\tthe target\tclasses\t(y_train_5)\tand\tthe\tpredicted\tclasses\t(y_train_pred):\n\n>>>\tfrom\tsklearn.metrics\timport\tconfusion_matrix >>>\tconfusion_matrix(y_train_5,\ty_train_pred) array([[53272,\t\t1307], \t\t\t\t\t\t\t[\t1077,\t\t4344]])\n\nEach\trow\tin\ta\tconfusion\tmatrix\trepresents\tan\tactual\tclass,\twhile\teach\tcolumn\trepresents\ta\tpredicted class.\tThe\tfirst\trow\tof\tthis\tmatrix\tconsiders\tnon-5\timages\t(the\tnegative\tclass):\t53,272\tof\tthem\twere correctly\tclassified\tas\tnon-5s\t(they\tare\tcalled\ttrue\tnegatives),\twhile\tthe\tremaining\t1,307\twere\twrongly classified\tas\t5s\t(false\tpositives).\tThe\tsecond\trow\tconsiders\tthe\timages\tof\t5s\t(the\tpositive\tclass):\t1,077 were\twrongly\tclassified\tas\tnon-5s\t(false\tnegatives),\twhile\tthe\tremaining\t4,344\twere\tcorrectly\tclassified as\t5s\t(true\tpositives).\tA\tperfect\tclassifier\twould\thave\tonly\ttrue\tpositives\tand\ttrue\tnegatives,\tso\tits confusion\tmatrix\twould\thave\tnonzero\tvalues\tonly\ton\tits\tmain\tdiagonal\t(top\tleft\tto\tbottom\tright):\n\n>>>\tconfusion_matrix(y_train_5,\ty_train_perfect_predictions) array([[54579,\t\t\t\t0], \t\t\t\t\t\t\t[\t\t\t\t0,\t5421]])\n\nThe\tconfusion\tmatrix\tgives\tyou\ta\tlot\tof\tinformation,\tbut\tsometimes\tyou\tmay\tprefer\ta\tmore\tconcise\tmetric. An\tinteresting\tone\tto\tlook\tat\tis\tthe\taccuracy\tof\tthe\tpositive\tpredictions;\tthis\tis\tcalled\tthe\tprecision\tof\tthe classifier\t(Equation\t3-1).\n\nEquation\t3-1.\tPrecision",
      "content_length": 2676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "TP\tis\tthe\tnumber\tof\ttrue\tpositives,\tand\tFP\tis\tthe\tnumber\tof\tfalse\tpositives.\n\nA\ttrivial\tway\tto\thave\tperfect\tprecision\tis\tto\tmake\tone\tsingle\tpositive\tprediction\tand\tensure\tit\tis\tcorrect (precision\t=\t1/1\t=\t100%).\tThis\twould\tnot\tbe\tvery\tuseful\tsince\tthe\tclassifier\twould\tignore\tall\tbut\tone positive\tinstance.\tSo\tprecision\tis\ttypically\tused\talong\twith\tanother\tmetric\tnamed\trecall,\talso\tcalled sensitivity\tor\ttrue\tpositive\trate\t(TPR):\tthis\tis\tthe\tratio\tof\tpositive\tinstances\tthat\tare\tcorrectly\tdetected\tby the\tclassifier\t(Equation\t3-2).\n\nEquation\t3-2.\tRecall\n\nFN\tis\tof\tcourse\tthe\tnumber\tof\tfalse\tnegatives.\n\nIf\tyou\tare\tconfused\tabout\tthe\tconfusion\tmatrix,\tFigure\t3-2\tmay\thelp.\n\nFigure\t3-2.\tAn\tillustrated\tconfusion\tmatrix",
      "content_length": 716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Precision\tand\tRecall Scikit-Learn\tprovides\tseveral\tfunctions\tto\tcompute\tclassifier\tmetrics,\tincluding\tprecision\tand\trecall:\n\n>>>\tfrom\tsklearn.metrics\timport\tprecision_score,\trecall_score >>>\tprecision_score(y_train_5,\ty_train_pred)\t#\t==\t4344\t/\t(4344\t+\t1307) 0.76871350203503808 >>>\trecall_score(y_train_5,\ty_train_pred)\t#\t==\t4344\t/\t(4344\t+\t1077) 0.80132816823464303\n\nNow\tyour\t5-detector\tdoes\tnot\tlook\tas\tshiny\tas\tit\tdid\twhen\tyou\tlooked\tat\tits\taccuracy.\tWhen\tit\tclaims\tan image\trepresents\ta\t5,\tit\tis\tcorrect\tonly\t77%\tof\tthe\ttime.\tMoreover,\tit\tonly\tdetects\t80%\tof\tthe\t5s.\n\nIt\tis\toften\tconvenient\tto\tcombine\tprecision\tand\trecall\tinto\ta\tsingle\tmetric\tcalled\tthe\tF1\tscore,\tin particular\tif\tyou\tneed\ta\tsimple\tway\tto\tcompare\ttwo\tclassifiers.\tThe\tF1\tscore\tis\tthe\tharmonic\tmean\tof precision\tand\trecall\t(Equation\t3-3).\tWhereas\tthe\tregular\tmean\ttreats\tall\tvalues\tequally,\tthe\tharmonic mean\tgives\tmuch\tmore\tweight\tto\tlow\tvalues.\tAs\ta\tresult,\tthe\tclassifier\twill\tonly\tget\ta\thigh\tF1\tscore\tif both\trecall\tand\tprecision\tare\thigh.\n\nEquation\t3-3.\tF1\tscore\n\nTo\tcompute\tthe\tF1\tscore,\tsimply\tcall\tthe\tf1_score()\tfunction:\n\n>>>\tfrom\tsklearn.metrics\timport\tf1_score >>>\tf1_score(y_train_5,\ty_train_pred) 0.78468208092485547\n\nThe\tF1\tscore\tfavors\tclassifiers\tthat\thave\tsimilar\tprecision\tand\trecall.\tThis\tis\tnot\talways\twhat\tyou\twant: in\tsome\tcontexts\tyou\tmostly\tcare\tabout\tprecision,\tand\tin\tother\tcontexts\tyou\treally\tcare\tabout\trecall.\tFor example,\tif\tyou\ttrained\ta\tclassifier\tto\tdetect\tvideos\tthat\tare\tsafe\tfor\tkids,\tyou\twould\tprobably\tprefer\ta classifier\tthat\trejects\tmany\tgood\tvideos\t(low\trecall)\tbut\tkeeps\tonly\tsafe\tones\t(high\tprecision),\trather\tthan a\tclassifier\tthat\thas\ta\tmuch\thigher\trecall\tbut\tlets\ta\tfew\treally\tbad\tvideos\tshow\tup\tin\tyour\tproduct\t(in\tsuch cases,\tyou\tmay\teven\twant\tto\tadd\ta\thuman\tpipeline\tto\tcheck\tthe\tclassifier’s\tvideo\tselection).\tOn\tthe\tother hand,\tsuppose\tyou\ttrain\ta\tclassifier\tto\tdetect\tshoplifters\ton\tsurveillance\timages:\tit\tis\tprobably\tfine\tif\tyour classifier\thas\tonly\t30%\tprecision\tas\tlong\tas\tit\thas\t99%\trecall\t(sure,\tthe\tsecurity\tguards\twill\tget\ta\tfew false\talerts,\tbut\talmost\tall\tshoplifters\twill\tget\tcaught).\n\nUnfortunately,\tyou\tcan’t\thave\tit\tboth\tways:\tincreasing\tprecision\treduces\trecall,\tand\tvice\tversa.\tThis\tis called\tthe\tprecision/recall\ttradeoff.",
      "content_length": 2262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Precision/Recall\tTradeoff To\tunderstand\tthis\ttradeoff,\tlet’s\tlook\tat\thow\tthe\tSGDClassifier\tmakes\tits\tclassification\tdecisions.\tFor each\tinstance,\tit\tcomputes\ta\tscore\tbased\ton\ta\tdecision\tfunction,\tand\tif\tthat\tscore\tis\tgreater\tthan\ta threshold,\tit\tassigns\tthe\tinstance\tto\tthe\tpositive\tclass,\tor\telse\tit\tassigns\tit\tto\tthe\tnegative\tclass.\tFigure\t3-3 shows\ta\tfew\tdigits\tpositioned\tfrom\tthe\tlowest\tscore\ton\tthe\tleft\tto\tthe\thighest\tscore\ton\tthe\tright.\tSuppose the\tdecision\tthreshold\tis\tpositioned\tat\tthe\tcentral\tarrow\t(between\tthe\ttwo\t5s):\tyou\twill\tfind\t4\ttrue positives\t(actual\t5s)\ton\tthe\tright\tof\tthat\tthreshold,\tand\tone\tfalse\tpositive\t(actually\ta\t6).\tTherefore,\twith that\tthreshold,\tthe\tprecision\tis\t80%\t(4\tout\tof\t5).\tBut\tout\tof\t6\tactual\t5s,\tthe\tclassifier\tonly\tdetects\t4,\tso\tthe recall\tis\t67%\t(4\tout\tof\t6).\tNow\tif\tyou\traise\tthe\tthreshold\t(move\tit\tto\tthe\tarrow\ton\tthe\tright),\tthe\tfalse positive\t(the\t6)\tbecomes\ta\ttrue\tnegative,\tthereby\tincreasing\tprecision\t(up\tto\t100%\tin\tthis\tcase),\tbut\tone true\tpositive\tbecomes\ta\tfalse\tnegative,\tdecreasing\trecall\tdown\tto\t50%.\tConversely,\tlowering\tthe threshold\tincreases\trecall\tand\treduces\tprecision.\n\nFigure\t3-3.\tDecision\tthreshold\tand\tprecision/recall\ttradeoff\n\nScikit-Learn\tdoes\tnot\tlet\tyou\tset\tthe\tthreshold\tdirectly,\tbut\tit\tdoes\tgive\tyou\taccess\tto\tthe\tdecision\tscores that\tit\tuses\tto\tmake\tpredictions.\tInstead\tof\tcalling\tthe\tclassifier’s\tpredict()\tmethod,\tyou\tcan\tcall\tits decision_function()\tmethod,\twhich\treturns\ta\tscore\tfor\teach\tinstance,\tand\tthen\tmake\tpredictions\tbased on\tthose\tscores\tusing\tany\tthreshold\tyou\twant:\n\n>>>\ty_scores\t=\tsgd_clf.decision_function([some_digit]) >>>\ty_scores array([\t161855.74572176]) >>>\tthreshold\t=\t0 >>>\ty_some_digit_pred\t=\t(y_scores\t>\tthreshold) array([\tTrue],\tdtype=bool)\n\nThe\tSGDClassifier\tuses\ta\tthreshold\tequal\tto\t0,\tso\tthe\tprevious\tcode\treturns\tthe\tsame\tresult\tas\tthe predict()\tmethod\t(i.e.,\tTrue).\tLet’s\traise\tthe\tthreshold:\n\n>>>\tthreshold\t=\t200000 >>>\ty_some_digit_pred\t=\t(y_scores\t>\tthreshold) >>>\ty_some_digit_pred array([False],\tdtype=bool)\n\nThis\tconfirms\tthat\traising\tthe\tthreshold\tdecreases\trecall.\tThe\timage\tactually\trepresents\ta\t5,\tand\tthe classifier\tdetects\tit\twhen\tthe\tthreshold\tis\t0,\tbut\tit\tmisses\tit\twhen\tthe\tthreshold\tis\tincreased\tto\t200,000.",
      "content_length": 2233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "So\thow\tcan\tyou\tdecide\twhich\tthreshold\tto\tuse?\tFor\tthis\tyou\twill\tfirst\tneed\tto\tget\tthe\tscores\tof\tall instances\tin\tthe\ttraining\tset\tusing\tthe\tcross_val_predict()\tfunction\tagain,\tbut\tthis\ttime\tspecifying\tthat you\twant\tit\tto\treturn\tdecision\tscores\tinstead\tof\tpredictions:\n\ny_scores\t=\tcross_val_predict(sgd_clf,\tX_train,\ty_train_5,\tcv=3, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmethod=\"decision_function\")\n\nNow\twith\tthese\tscores\tyou\tcan\tcompute\tprecision\tand\trecall\tfor\tall\tpossible\tthresholds\tusing\tthe precision_recall_curve()\tfunction:\n\nfrom\tsklearn.metrics\timport\tprecision_recall_curve\n\nprecisions,\trecalls,\tthresholds\t=\tprecision_recall_curve(y_train_5,\ty_scores)\n\nFinally,\tyou\tcan\tplot\tprecision\tand\trecall\tas\tfunctions\tof\tthe\tthreshold\tvalue\tusing\tMatplotlib\t(Figure\t3- 4):\n\ndef\tplot_precision_recall_vs_threshold(precisions,\trecalls,\tthresholds): \t\t\t\tplt.plot(thresholds,\tprecisions[:-1],\t\"b--\",\tlabel=\"Precision\") \t\t\t\tplt.plot(thresholds,\trecalls[:-1],\t\"g-\",\tlabel=\"Recall\") \t\t\t\tplt.xlabel(\"Threshold\") \t\t\t\tplt.legend(loc=\"upper\tleft\") \t\t\t\tplt.ylim([0,\t1])\n\nplot_precision_recall_vs_threshold(precisions,\trecalls,\tthresholds) plt.show()\n\nFigure\t3-4.\tPrecision\tand\trecall\tversus\tthe\tdecision\tthreshold\n\nNOTE\n\nYou\tmay\twonder\twhy\tthe\tprecision\tcurve\tis\tbumpier\tthan\tthe\trecall\tcurve\tin\tFigure\t3-4.\tThe\treason\tis\tthat\tprecision\tmay sometimes\tgo\tdown\twhen\tyou\traise\tthe\tthreshold\t(although\tin\tgeneral\tit\twill\tgo\tup).\tTo\tunderstand\twhy,\tlook\tback\tat\tFigure\t3-3 and\tnotice\twhat\thappens\twhen\tyou\tstart\tfrom\tthe\tcentral\tthreshold\tand\tmove\tit\tjust\tone\tdigit\tto\tthe\tright:\tprecision\tgoes\tfrom\t4/5 (80%)\tdown\tto\t3/4\t(75%).\tOn\tthe\tother\thand,\trecall\tcan\tonly\tgo\tdown\twhen\tthe\tthreshold\tis\tincreased,\twhich\texplains\twhy\tits curve\tlooks\tsmooth.",
      "content_length": 1723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Now\tyou\tcan\tsimply\tselect\tthe\tthreshold\tvalue\tthat\tgives\tyou\tthe\tbest\tprecision/recall\ttradeoff\tfor\tyour task.\tAnother\tway\tto\tselect\ta\tgood\tprecision/recall\ttradeoff\tis\tto\tplot\tprecision\tdirectly\tagainst\trecall,\tas shown\tin\tFigure\t3-5.\n\nFigure\t3-5.\tPrecision\tversus\trecall\n\nYou\tcan\tsee\tthat\tprecision\treally\tstarts\tto\tfall\tsharply\taround\t80%\trecall.\tYou\twill\tprobably\twant\tto select\ta\tprecision/recall\ttradeoff\tjust\tbefore\tthat\tdrop\t—\tfor\texample,\tat\taround\t60%\trecall.\tBut\tof course\tthe\tchoice\tdepends\ton\tyour\tproject.\n\nSo\tlet’s\tsuppose\tyou\tdecide\tto\taim\tfor\t90%\tprecision.\tYou\tlook\tup\tthe\tfirst\tplot\t(zooming\tin\ta\tbit)\tand find\tthat\tyou\tneed\tto\tuse\ta\tthreshold\tof\tabout\t70,000.\tTo\tmake\tpredictions\t(on\tthe\ttraining\tset\tfor\tnow), instead\tof\tcalling\tthe\tclassifier’s\tpredict()\tmethod,\tyou\tcan\tjust\trun\tthis\tcode:\n\ny_train_pred_90\t=\t(y_scores\t>\t70000)\n\nLet’s\tcheck\tthese\tpredictions’\tprecision\tand\trecall:\n\n>>>\tprecision_score(y_train_5,\ty_train_pred_90) 0.86592051164915484 >>>\trecall_score(y_train_5,\ty_train_pred_90) 0.69931746910164172\n\nGreat,\tyou\thave\ta\t90%\tprecision\tclassifier\t(or\tclose\tenough)!\tAs\tyou\tcan\tsee,\tit\tis\tfairly\teasy\tto\tcreate\ta classifier\twith\tvirtually\tany\tprecision\tyou\twant:\tjust\tset\ta\thigh\tenough\tthreshold,\tand\tyou’re\tdone.\tHmm, not\tso\tfast.\tA\thigh-precision\tclassifier\tis\tnot\tvery\tuseful\tif\tits\trecall\tis\ttoo\tlow!",
      "content_length": 1339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "TIP\n\nIf\tsomeone\tsays\t“let’s\treach\t99%\tprecision,”\tyou\tshould\task,\t“at\twhat\trecall?”",
      "content_length": 83,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "The\tROC\tCurve The\treceiver\toperating\tcharacteristic\t(ROC)\tcurve\tis\tanother\tcommon\ttool\tused\twith\tbinary\tclassifiers. It\tis\tvery\tsimilar\tto\tthe\tprecision/recall\tcurve,\tbut\tinstead\tof\tplotting\tprecision\tversus\trecall,\tthe\tROC curve\tplots\tthe\ttrue\tpositive\trate\t(another\tname\tfor\trecall)\tagainst\tthe\tfalse\tpositive\trate.\tThe\tFPR\tis\tthe ratio\tof\tnegative\tinstances\tthat\tare\tincorrectly\tclassified\tas\tpositive.\tIt\tis\tequal\tto\tone\tminus\tthe\ttrue negative\trate,\twhich\tis\tthe\tratio\tof\tnegative\tinstances\tthat\tare\tcorrectly\tclassified\tas\tnegative.\tThe\tTNR is\talso\tcalled\tspecificity.\tHence\tthe\tROC\tcurve\tplots\tsensitivity\t(recall)\tversus\t1\t–\tspecificity.\n\nTo\tplot\tthe\tROC\tcurve,\tyou\tfirst\tneed\tto\tcompute\tthe\tTPR\tand\tFPR\tfor\tvarious\tthreshold\tvalues,\tusing\tthe roc_curve()\tfunction:\n\nfrom\tsklearn.metrics\timport\troc_curve\n\nfpr,\ttpr,\tthresholds\t=\troc_curve(y_train_5,\ty_scores)\n\nThen\tyou\tcan\tplot\tthe\tFPR\tagainst\tthe\tTPR\tusing\tMatplotlib.\tThis\tcode\tproduces\tthe\tplot\tin\tFigure\t3-6:\n\ndef\tplot_roc_curve(fpr,\ttpr,\tlabel=None): \t\t\t\tplt.plot(fpr,\ttpr,\tlinewidth=2,\tlabel=label) \t\t\t\tplt.plot([0,\t1],\t[0,\t1],\t'k--') \t\t\t\tplt.axis([0,\t1,\t0,\t1]) \t\t\t\tplt.xlabel('False\tPositive\tRate') \t\t\t\tplt.ylabel('True\tPositive\tRate')\n\nplot_roc_curve(fpr,\ttpr) plt.show()\n\nFigure\t3-6.\tROC\tcurve\n\nOnce\tagain\tthere\tis\ta\ttradeoff:\tthe\thigher\tthe\trecall\t(TPR),\tthe\tmore\tfalse\tpositives\t(FPR)\tthe\tclassifier",
      "content_length": 1369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "produces.\tThe\tdotted\tline\trepresents\tthe\tROC\tcurve\tof\ta\tpurely\trandom\tclassifier;\ta\tgood\tclassifier\tstays as\tfar\taway\tfrom\tthat\tline\tas\tpossible\t(toward\tthe\ttop-left\tcorner).\n\nOne\tway\tto\tcompare\tclassifiers\tis\tto\tmeasure\tthe\tarea\tunder\tthe\tcurve\t(AUC).\tA\tperfect\tclassifier\twill have\ta\tROC\tAUC\tequal\tto\t1,\twhereas\ta\tpurely\trandom\tclassifier\twill\thave\ta\tROC\tAUC\tequal\tto\t0.5. Scikit-Learn\tprovides\ta\tfunction\tto\tcompute\tthe\tROC\tAUC:\n\n>>>\tfrom\tsklearn.metrics\timport\troc_auc_score >>>\troc_auc_score(y_train_5,\ty_scores) 0.96244965559671547\n\nTIP\n\nSince\tthe\tROC\tcurve\tis\tso\tsimilar\tto\tthe\tprecision/recall\t(or\tPR)\tcurve,\tyou\tmay\twonder\thow\tto\tdecide\twhich\tone\tto\tuse.\tAs\ta rule\tof\tthumb,\tyou\tshould\tprefer\tthe\tPR\tcurve\twhenever\tthe\tpositive\tclass\tis\trare\tor\twhen\tyou\tcare\tmore\tabout\tthe\tfalse positives\tthan\tthe\tfalse\tnegatives,\tand\tthe\tROC\tcurve\totherwise.\tFor\texample,\tlooking\tat\tthe\tprevious\tROC\tcurve\t(and\tthe\tROC AUC\tscore),\tyou\tmay\tthink\tthat\tthe\tclassifier\tis\treally\tgood.\tBut\tthis\tis\tmostly\tbecause\tthere\tare\tfew\tpositives\t(5s)\tcompared\tto the\tnegatives\t(non-5s).\tIn\tcontrast,\tthe\tPR\tcurve\tmakes\tit\tclear\tthat\tthe\tclassifier\thas\troom\tfor\timprovement\t(the\tcurve\tcould\tbe closer\tto\tthe\ttop-right\tcorner).\n\nLet’s\ttrain\ta\tRandomForestClassifier\tand\tcompare\tits\tROC\tcurve\tand\tROC\tAUC\tscore\tto\tthe SGDClassifier.\tFirst,\tyou\tneed\tto\tget\tscores\tfor\teach\tinstance\tin\tthe\ttraining\tset.\tBut\tdue\tto\tthe\tway\tit works\t(see\tChapter\t7),\tthe\tRandomForestClassifier\tclass\tdoes\tnot\thave\ta\tdecision_function() method.\tInstead\tit\thas\ta\tpredict_proba()\tmethod.\tScikit-Learn\tclassifiers\tgenerally\thave\tone\tor\tthe other.\tThe\tpredict_proba()\tmethod\treturns\tan\tarray\tcontaining\ta\trow\tper\tinstance\tand\ta\tcolumn\tper class,\teach\tcontaining\tthe\tprobability\tthat\tthe\tgiven\tinstance\tbelongs\tto\tthe\tgiven\tclass\t(e.g.,\t70%\tchance that\tthe\timage\trepresents\ta\t5):\n\nfrom\tsklearn.ensemble\timport\tRandomForestClassifier\n\nforest_clf\t=\tRandomForestClassifier(random_state=42) y_probas_forest\t=\tcross_val_predict(forest_clf,\tX_train,\ty_train_5,\tcv=3, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmethod=\"predict_proba\")\n\nBut\tto\tplot\ta\tROC\tcurve,\tyou\tneed\tscores,\tnot\tprobabilities.\tA\tsimple\tsolution\tis\tto\tuse\tthe\tpositive class’s\tprobability\tas\tthe\tscore:\n\ny_scores_forest\t=\ty_probas_forest[:,\t1]\t\t\t#\tscore\t=\tproba\tof\tpositive\tclass fpr_forest,\ttpr_forest,\tthresholds_forest\t=\troc_curve(y_train_5,y_scores_forest)\n\nNow\tyou\tare\tready\tto\tplot\tthe\tROC\tcurve.\tIt\tis\tuseful\tto\tplot\tthe\tfirst\tROC\tcurve\tas\twell\tto\tsee\thow\tthey compare\t(Figure\t3-7):\n\nplt.plot(fpr,\ttpr,\t\"b:\",\tlabel=\"SGD\") plot_roc_curve(fpr_forest,\ttpr_forest,\t\"Random\tForest\") plt.legend(loc=\"lower\tright\") plt.show()",
      "content_length": 2632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Figure\t3-7.\tComparing\tROC\tcurves\n\nAs\tyou\tcan\tsee\tin\tFigure\t3-7,\tthe\tRandomForestClassifier’s\tROC\tcurve\tlooks\tmuch\tbetter\tthan\tthe SGDClassifier’s:\tit\tcomes\tmuch\tcloser\tto\tthe\ttop-left\tcorner.\tAs\ta\tresult,\tits\tROC\tAUC\tscore\tis\talso significantly\tbetter:\n\n>>>\troc_auc_score(y_train_5,\ty_scores_forest) 0.99312433660038291\n\nTry\tmeasuring\tthe\tprecision\tand\trecall\tscores:\tyou\tshould\tfind\t98.5%\tprecision\tand\t82.8%\trecall.\tNot too\tbad!\n\nHopefully\tyou\tnow\tknow\thow\tto\ttrain\tbinary\tclassifiers,\tchoose\tthe\tappropriate\tmetric\tfor\tyour\ttask, evaluate\tyour\tclassifiers\tusing\tcross-validation,\tselect\tthe\tprecision/recall\ttradeoff\tthat\tfits\tyour\tneeds, and\tcompare\tvarious\tmodels\tusing\tROC\tcurves\tand\tROC\tAUC\tscores.\tNow\tlet’s\ttry\tto\tdetect\tmore\tthan just\tthe\t5s.",
      "content_length": 752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "Multiclass\tClassification Whereas\tbinary\tclassifiers\tdistinguish\tbetween\ttwo\tclasses,\tmulticlass\tclassifiers\t(also\tcalled multinomial\tclassifiers)\tcan\tdistinguish\tbetween\tmore\tthan\ttwo\tclasses.\n\nSome\talgorithms\t(such\tas\tRandom\tForest\tclassifiers\tor\tnaive\tBayes\tclassifiers)\tare\tcapable\tof\thandling multiple\tclasses\tdirectly.\tOthers\t(such\tas\tSupport\tVector\tMachine\tclassifiers\tor\tLinear\tclassifiers)\tare strictly\tbinary\tclassifiers.\tHowever,\tthere\tare\tvarious\tstrategies\tthat\tyou\tcan\tuse\tto\tperform\tmulticlass classification\tusing\tmultiple\tbinary\tclassifiers.\n\nFor\texample,\tone\tway\tto\tcreate\ta\tsystem\tthat\tcan\tclassify\tthe\tdigit\timages\tinto\t10\tclasses\t(from\t0\tto\t9)\tis to\ttrain\t10\tbinary\tclassifiers,\tone\tfor\teach\tdigit\t(a\t0-detector,\ta\t1-detector,\ta\t2-detector,\tand\tso\ton).\tThen when\tyou\twant\tto\tclassify\tan\timage,\tyou\tget\tthe\tdecision\tscore\tfrom\teach\tclassifier\tfor\tthat\timage\tand\tyou select\tthe\tclass\twhose\tclassifier\toutputs\tthe\thighest\tscore.\tThis\tis\tcalled\tthe\tone-versus-all\t(OvA) strategy\t(also\tcalled\tone-versus-the-rest).\n\nAnother\tstrategy\tis\tto\ttrain\ta\tbinary\tclassifier\tfor\tevery\tpair\tof\tdigits:\tone\tto\tdistinguish\t0s\tand\t1s, another\tto\tdistinguish\t0s\tand\t2s,\tanother\tfor\t1s\tand\t2s,\tand\tso\ton.\tThis\tis\tcalled\tthe\tone-versus-one (OvO)\tstrategy.\tIf\tthere\tare\tN\tclasses,\tyou\tneed\tto\ttrain\tN\t×\t(N\t–\t1)\t/\t2\tclassifiers.\tFor\tthe\tMNIST problem,\tthis\tmeans\ttraining\t45\tbinary\tclassifiers!\tWhen\tyou\twant\tto\tclassify\tan\timage,\tyou\thave\tto\trun the\timage\tthrough\tall\t45\tclassifiers\tand\tsee\twhich\tclass\twins\tthe\tmost\tduels.\tThe\tmain\tadvantage\tof\tOvO is\tthat\teach\tclassifier\tonly\tneeds\tto\tbe\ttrained\ton\tthe\tpart\tof\tthe\ttraining\tset\tfor\tthe\ttwo\tclasses\tthat\tit\tmust distinguish.\n\nSome\talgorithms\t(such\tas\tSupport\tVector\tMachine\tclassifiers)\tscale\tpoorly\twith\tthe\tsize\tof\tthe\ttraining set,\tso\tfor\tthese\talgorithms\tOvO\tis\tpreferred\tsince\tit\tis\tfaster\tto\ttrain\tmany\tclassifiers\ton\tsmall\ttraining sets\tthan\ttraining\tfew\tclassifiers\ton\tlarge\ttraining\tsets.\tFor\tmost\tbinary\tclassification\talgorithms, however,\tOvA\tis\tpreferred.\n\nScikit-Learn\tdetects\twhen\tyou\ttry\tto\tuse\ta\tbinary\tclassification\talgorithm\tfor\ta\tmulticlass\tclassification task,\tand\tit\tautomatically\truns\tOvA\t(except\tfor\tSVM\tclassifiers\tfor\twhich\tit\tuses\tOvO).\tLet’s\ttry\tthis\twith the\tSGDClassifier:\n\n>>>\tsgd_clf.fit(X_train,\ty_train)\t\t#\ty_train,\tnot\ty_train_5 >>>\tsgd_clf.predict([some_digit]) array([\t5.])\n\nThat\twas\teasy!\tThis\tcode\ttrains\tthe\tSGDClassifier\ton\tthe\ttraining\tset\tusing\tthe\toriginal\ttarget\tclasses from\t0\tto\t9\t(y_train),\tinstead\tof\tthe\t5-versus-all\ttarget\tclasses\t(y_train_5).\tThen\tit\tmakes\ta\tprediction (a\tcorrect\tone\tin\tthis\tcase).\tUnder\tthe\thood,\tScikit-Learn\tactually\ttrained\t10\tbinary\tclassifiers,\tgot\ttheir decision\tscores\tfor\tthe\timage,\tand\tselected\tthe\tclass\twith\tthe\thighest\tscore.\n\nTo\tsee\tthat\tthis\tis\tindeed\tthe\tcase,\tyou\tcan\tcall\tthe\tdecision_function()\tmethod.\tInstead\tof\treturning just\tone\tscore\tper\tinstance,\tit\tnow\treturns\t10\tscores,\tone\tper\tclass:\n\n>>>\tsome_digit_scores\t=\tsgd_clf.decision_function([some_digit]) >>>\tsome_digit_scores array([[-311402.62954431,\t-363517.28355739,\t-446449.5306454\t,",
      "content_length": 3081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "-183226.61023518,\t-414337.15339485,\t\t161855.74572176, \t\t\t\t\t\t\t\t-452576.39616343,\t-471957.14962573,\t-518542.33997148, \t\t\t\t\t\t\t\t-536774.63961222]])\n\nThe\thighest\tscore\tis\tindeed\tthe\tone\tcorresponding\tto\tclass\t5:\n\n>>>\tnp.argmax(some_digit_scores) 5 >>>\tsgd_clf.classes_ array([\t0.,\t\t1.,\t\t2.,\t\t3.,\t\t4.,\t\t5.,\t\t6.,\t\t7.,\t\t8.,\t\t9.]) >>>\tsgd_clf.classes_[5] 5.0\n\nWARNING\n\nWhen\ta\tclassifier\tis\ttrained,\tit\tstores\tthe\tlist\tof\ttarget\tclasses\tin\tits\tclasses_\tattribute,\tordered\tby\tvalue.\tIn\tthis\tcase,\tthe\tindex\tof each\tclass\tin\tthe\tclasses_\tarray\tconveniently\tmatches\tthe\tclass\titself\t(e.g.,\tthe\tclass\tat\tindex\t5\thappens\tto\tbe\tclass\t5),\tbut\tin general\tyou\twon’t\tbe\tso\tlucky.\n\nIf\tyou\twant\tto\tforce\tScikitLearn\tto\tuse\tone-versus-one\tor\tone-versus-all,\tyou\tcan\tuse\tthe OneVsOneClassifier\tor\tOneVsRestClassifier\tclasses.\tSimply\tcreate\tan\tinstance\tand\tpass\ta\tbinary classifier\tto\tits\tconstructor.\tFor\texample,\tthis\tcode\tcreates\ta\tmulticlass\tclassifier\tusing\tthe\tOvO\tstrategy, based\ton\ta\tSGDClassifier:\n\n>>>\tfrom\tsklearn.multiclass\timport\tOneVsOneClassifier >>>\tovo_clf\t=\tOneVsOneClassifier(SGDClassifier(random_state=42)) >>>\tovo_clf.fit(X_train,\ty_train) >>>\tovo_clf.predict([some_digit]) array([\t5.]) >>>\tlen(ovo_clf.estimators_) 45\n\nTraining\ta\tRandomForestClassifier\tis\tjust\tas\teasy:\n\n>>>\tforest_clf.fit(X_train,\ty_train) >>>\tforest_clf.predict([some_digit]) array([\t5.])\n\nThis\ttime\tScikit-Learn\tdid\tnot\thave\tto\trun\tOvA\tor\tOvO\tbecause\tRandom\tForest\tclassifiers\tcan\tdirectly classify\tinstances\tinto\tmultiple\tclasses.\tYou\tcan\tcall\tpredict_proba()\tto\tget\tthe\tlist\tof\tprobabilities\tthat the\tclassifier\tassigned\tto\teach\tinstance\tfor\teach\tclass:\n\n>>>\tforest_clf.predict_proba([some_digit]) array([[\t0.1,\t\t0.\t,\t\t0.\t,\t\t0.1,\t\t0.\t,\t\t0.8,\t\t0.\t,\t\t0.\t,\t\t0.\t,\t\t0.\t]])\n\nYou\tcan\tsee\tthat\tthe\tclassifier\tis\tfairly\tconfident\tabout\tits\tprediction:\tthe\t0.8\tat\tthe\t5th\tindex\tin\tthe\tarray means\tthat\tthe\tmodel\testimates\tan\t80%\tprobability\tthat\tthe\timage\trepresents\ta\t5.\tIt\talso\tthinks\tthat\tthe image\tcould\tinstead\tbe\ta\t0\tor\ta\t3\t(10%\tchance\teach).\n\nNow\tof\tcourse\tyou\twant\tto\tevaluate\tthese\tclassifiers.\tAs\tusual,\tyou\twant\tto\tuse\tcross-validation.\tLet’s evaluate\tthe\tSGDClassifier’s\taccuracy\tusing\tthe\tcross_val_score()\tfunction:",
      "content_length": 2188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": ">>>\tcross_val_score(sgd_clf,\tX_train,\ty_train,\tcv=3,\tscoring=\"accuracy\") array([\t0.84063187,\t\t0.84899245,\t\t0.86652998])\n\nIt\tgets\tover\t84%\ton\tall\ttest\tfolds.\tIf\tyou\tused\ta\trandom\tclassifier,\tyou\twould\tget\t10%\taccuracy,\tso\tthis\tis not\tsuch\ta\tbad\tscore,\tbut\tyou\tcan\tstill\tdo\tmuch\tbetter.\tFor\texample,\tsimply\tscaling\tthe\tinputs\t(as discussed\tin\tChapter\t2)\tincreases\taccuracy\tabove\t90%:\n\n>>>\tfrom\tsklearn.preprocessing\timport\tStandardScaler >>>\tscaler\t=\tStandardScaler() >>>\tX_train_scaled\t=\tscaler.fit_transform(X_train.astype(np.float64)) >>>\tcross_val_score(sgd_clf,\tX_train_scaled,\ty_train,\tcv=3,\tscoring=\"accuracy\") array([\t0.91011798,\t\t0.90874544,\t\t0.906636\t\t])",
      "content_length": 662,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "Error\tAnalysis Of\tcourse,\tif\tthis\twere\ta\treal\tproject,\tyou\twould\tfollow\tthe\tsteps\tin\tyour\tMachine\tLearning\tproject checklist\t(see\tAppendix\tB):\texploring\tdata\tpreparation\toptions,\ttrying\tout\tmultiple\tmodels,\tshortlisting the\tbest\tones\tand\tfine-tuning\ttheir\thyperparameters\tusing\tGridSearchCV,\tand\tautomating\tas\tmuch\tas possible,\tas\tyou\tdid\tin\tthe\tprevious\tchapter.\tHere,\twe\twill\tassume\tthat\tyou\thave\tfound\ta\tpromising\tmodel and\tyou\twant\tto\tfind\tways\tto\timprove\tit.\tOne\tway\tto\tdo\tthis\tis\tto\tanalyze\tthe\ttypes\tof\terrors\tit\tmakes.\n\nFirst,\tyou\tcan\tlook\tat\tthe\tconfusion\tmatrix.\tYou\tneed\tto\tmake\tpredictions\tusing\tthe cross_val_predict()\tfunction,\tthen\tcall\tthe\tconfusion_matrix()\tfunction,\tjust\tlike\tyou\tdid\tearlier:\n\n>>>\ty_train_pred\t=\tcross_val_predict(sgd_clf,\tX_train_scaled,\ty_train,\tcv=3) >>>\tconf_mx\t=\tconfusion_matrix(y_train,\ty_train_pred) >>>\tconf_mx array([[5725,\t\t\t\t3,\t\t\t24,\t\t\t\t9,\t\t\t10,\t\t\t49,\t\t\t50,\t\t\t10,\t\t\t39,\t\t\t\t4], \t\t\t\t\t\t\t[\t\t\t2,\t6493,\t\t\t43,\t\t\t25,\t\t\t\t7,\t\t\t40,\t\t\t\t5,\t\t\t10,\t\t109,\t\t\t\t8], \t\t\t\t\t\t\t[\t\t51,\t\t\t41,\t5321,\t\t104,\t\t\t89,\t\t\t26,\t\t\t87,\t\t\t60,\t\t166,\t\t\t13], \t\t\t\t\t\t\t[\t\t47,\t\t\t46,\t\t141,\t5342,\t\t\t\t1,\t\t231,\t\t\t40,\t\t\t50,\t\t141,\t\t\t92], \t\t\t\t\t\t\t[\t\t19,\t\t\t29,\t\t\t41,\t\t\t10,\t5366,\t\t\t\t9,\t\t\t56,\t\t\t37,\t\t\t86,\t\t189], \t\t\t\t\t\t\t[\t\t73,\t\t\t45,\t\t\t36,\t\t193,\t\t\t64,\t4582,\t\t111,\t\t\t30,\t\t193,\t\t\t94], \t\t\t\t\t\t\t[\t\t29,\t\t\t34,\t\t\t44,\t\t\t\t2,\t\t\t42,\t\t\t85,\t5627,\t\t\t10,\t\t\t45,\t\t\t\t0], \t\t\t\t\t\t\t[\t\t25,\t\t\t24,\t\t\t74,\t\t\t32,\t\t\t54,\t\t\t12,\t\t\t\t6,\t5787,\t\t\t15,\t\t236], \t\t\t\t\t\t\t[\t\t52,\t\t161,\t\t\t73,\t\t156,\t\t\t10,\t\t163,\t\t\t61,\t\t\t25,\t5027,\t\t123], \t\t\t\t\t\t\t[\t\t43,\t\t\t35,\t\t\t26,\t\t\t92,\t\t178,\t\t\t28,\t\t\t\t2,\t\t223,\t\t\t82,\t5240]])\n\nThat’s\ta\tlot\tof\tnumbers.\tIt’s\toften\tmore\tconvenient\tto\tlook\tat\tan\timage\trepresentation\tof\tthe\tconfusion matrix,\tusing\tMatplotlib’s\tmatshow()\tfunction:\n\nplt.matshow(conf_mx,\tcmap=plt.cm.gray) plt.show()",
      "content_length": 1749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "This\tconfusion\tmatrix\tlooks\tfairly\tgood,\tsince\tmost\timages\tare\ton\tthe\tmain\tdiagonal,\twhich\tmeans\tthat they\twere\tclassified\tcorrectly.\tThe\t5s\tlook\tslightly\tdarker\tthan\tthe\tother\tdigits,\twhich\tcould\tmean\tthat there\tare\tfewer\timages\tof\t5s\tin\tthe\tdataset\tor\tthat\tthe\tclassifier\tdoes\tnot\tperform\tas\twell\ton\t5s\tas\ton\tother digits.\tIn\tfact,\tyou\tcan\tverify\tthat\tboth\tare\tthe\tcase.\n\nLet’s\tfocus\tthe\tplot\ton\tthe\terrors.\tFirst,\tyou\tneed\tto\tdivide\teach\tvalue\tin\tthe\tconfusion\tmatrix\tby\tthe number\tof\timages\tin\tthe\tcorresponding\tclass,\tso\tyou\tcan\tcompare\terror\trates\tinstead\tof\tabsolute\tnumber of\terrors\t(which\twould\tmake\tabundant\tclasses\tlook\tunfairly\tbad):\n\nrow_sums\t=\tconf_mx.sum(axis=1,\tkeepdims=True) norm_conf_mx\t=\tconf_mx\t/\trow_sums\n\nNow\tlet’s\tfill\tthe\tdiagonal\twith\tzeros\tto\tkeep\tonly\tthe\terrors,\tand\tlet’s\tplot\tthe\tresult:\n\nnp.fill_diagonal(norm_conf_mx,\t0) plt.matshow(norm_conf_mx,\tcmap=plt.cm.gray) plt.show()",
      "content_length": 908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Now\tyou\tcan\tclearly\tsee\tthe\tkinds\tof\terrors\tthe\tclassifier\tmakes.\tRemember\tthat\trows\trepresent\tactual classes,\twhile\tcolumns\trepresent\tpredicted\tclasses.\tThe\tcolumns\tfor\tclasses\t8\tand\t9\tare\tquite\tbright, which\ttells\tyou\tthat\tmany\timages\tget\tmisclassified\tas\t8s\tor\t9s.\tSimilarly,\tthe\trows\tfor\tclasses\t8\tand\t9\tare also\tquite\tbright,\ttelling\tyou\tthat\t8s\tand\t9s\tare\toften\tconfused\twith\tother\tdigits.\tConversely,\tsome\trows are\tpretty\tdark,\tsuch\tas\trow\t1:\tthis\tmeans\tthat\tmost\t1s\tare\tclassified\tcorrectly\t(a\tfew\tare\tconfused\twith 8s,\tbut\tthat’s\tabout\tit).\tNotice\tthat\tthe\terrors\tare\tnot\tperfectly\tsymmetrical;\tfor\texample,\tthere\tare\tmore\t5s misclassified\tas\t8s\tthan\tthe\treverse.\n\nAnalyzing\tthe\tconfusion\tmatrix\tcan\toften\tgive\tyou\tinsights\ton\tways\tto\timprove\tyour\tclassifier.\tLooking\tat this\tplot,\tit\tseems\tthat\tyour\tefforts\tshould\tbe\tspent\ton\timproving\tclassification\tof\t8s\tand\t9s,\tas\twell\tas fixing\tthe\tspecific\t3/5\tconfusion.\tFor\texample,\tyou\tcould\ttry\tto\tgather\tmore\ttraining\tdata\tfor\tthese\tdigits. Or\tyou\tcould\tengineer\tnew\tfeatures\tthat\twould\thelp\tthe\tclassifier\t—\tfor\texample,\twriting\tan\talgorithm\tto count\tthe\tnumber\tof\tclosed\tloops\t(e.g.,\t8\thas\ttwo,\t6\thas\tone,\t5\thas\tnone).\tOr\tyou\tcould\tpreprocess\tthe images\t(e.g.,\tusing\tScikit-Image,\tPillow,\tor\tOpenCV)\tto\tmake\tsome\tpatterns\tstand\tout\tmore,\tsuch\tas closed\tloops.\n\nAnalyzing\tindividual\terrors\tcan\talso\tbe\ta\tgood\tway\tto\tgain\tinsights\ton\twhat\tyour\tclassifier\tis\tdoing\tand why\tit\tis\tfailing,\tbut\tit\tis\tmore\tdifficult\tand\ttime-consuming.\tFor\texample,\tlet’s\tplot\texamples\tof\t3s\tand\t5s (the\tplot_digits()\tfunction\tjust\tuses\tMatplotlib’s\timshow()\tfunction;\tsee\tthis\tchapter’s\tJupyter",
      "content_length": 1629,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "notebook\tfor\tdetails):\n\ncl_a,\tcl_b\t=\t3,\t5 X_aa\t=\tX_train[(y_train\t==\tcl_a)\t&\t(y_train_pred\t==\tcl_a)] X_ab\t=\tX_train[(y_train\t==\tcl_a)\t&\t(y_train_pred\t==\tcl_b)] X_ba\t=\tX_train[(y_train\t==\tcl_b)\t&\t(y_train_pred\t==\tcl_a)] X_bb\t=\tX_train[(y_train\t==\tcl_b)\t&\t(y_train_pred\t==\tcl_b)]\n\nplt.figure(figsize=(8,8)) plt.subplot(221);\tplot_digits(X_aa[:25],\timages_per_row=5) plt.subplot(222);\tplot_digits(X_ab[:25],\timages_per_row=5) plt.subplot(223);\tplot_digits(X_ba[:25],\timages_per_row=5) plt.subplot(224);\tplot_digits(X_bb[:25],\timages_per_row=5) plt.show()\n\nThe\ttwo\t5×5\tblocks\ton\tthe\tleft\tshow\tdigits\tclassified\tas\t3s,\tand\tthe\ttwo\t5×5\tblocks\ton\tthe\tright\tshow images\tclassified\tas\t5s.\tSome\tof\tthe\tdigits\tthat\tthe\tclassifier\tgets\twrong\t(i.e.,\tin\tthe\tbottom-left\tand\ttop- right\tblocks)\tare\tso\tbadly\twritten\tthat\teven\ta\thuman\twould\thave\ttrouble\tclassifying\tthem\t(e.g.,\tthe\t5\ton the\t8th\trow\tand\t1st\tcolumn\ttruly\tlooks\tlike\ta\t3).\tHowever,\tmost\tmisclassified\timages\tseem\tlike\tobvious errors\tto\tus,\tand\tit’s\thard\tto\tunderstand\twhy\tthe\tclassifier\tmade\tthe\tmistakes\tit\tdid.3\tThe\treason\tis\tthat\twe used\ta\tsimple\tSGDClassifier,\twhich\tis\ta\tlinear\tmodel.\tAll\tit\tdoes\tis\tassign\ta\tweight\tper\tclass\tto\teach pixel,\tand\twhen\tit\tsees\ta\tnew\timage\tit\tjust\tsums\tup\tthe\tweighted\tpixel\tintensities\tto\tget\ta\tscore\tfor\teach class.\tSo\tsince\t3s\tand\t5s\tdiffer\tonly\tby\ta\tfew\tpixels,\tthis\tmodel\twill\teasily\tconfuse\tthem.",
      "content_length": 1384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "The\tmain\tdifference\tbetween\t3s\tand\t5s\tis\tthe\tposition\tof\tthe\tsmall\tline\tthat\tjoins\tthe\ttop\tline\tto\tthe\tbottom arc.\tIf\tyou\tdraw\ta\t3\twith\tthe\tjunction\tslightly\tshifted\tto\tthe\tleft,\tthe\tclassifier\tmight\tclassify\tit\tas\ta\t5,\tand vice\tversa.\tIn\tother\twords,\tthis\tclassifier\tis\tquite\tsensitive\tto\timage\tshifting\tand\trotation.\tSo\tone\tway\tto reduce\tthe\t3/5\tconfusion\twould\tbe\tto\tpreprocess\tthe\timages\tto\tensure\tthat\tthey\tare\twell\tcentered\tand\tnot too\trotated.\tThis\twill\tprobably\thelp\treduce\tother\terrors\tas\twell.",
      "content_length": 503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Multilabel\tClassification Until\tnow\teach\tinstance\thas\talways\tbeen\tassigned\tto\tjust\tone\tclass.\tIn\tsome\tcases\tyou\tmay\twant\tyour classifier\tto\toutput\tmultiple\tclasses\tfor\teach\tinstance.\tFor\texample,\tconsider\ta\tface-recognition classifier:\twhat\tshould\tit\tdo\tif\tit\trecognizes\tseveral\tpeople\ton\tthe\tsame\tpicture?\tOf\tcourse\tit\tshould attach\tone\tlabel\tper\tperson\tit\trecognizes.\tSay\tthe\tclassifier\thas\tbeen\ttrained\tto\trecognize\tthree\tfaces, Alice,\tBob,\tand\tCharlie;\tthen\twhen\tit\tis\tshown\ta\tpicture\tof\tAlice\tand\tCharlie,\tit\tshould\toutput\t[1,\t0,\t1] (meaning\t“Alice\tyes,\tBob\tno,\tCharlie\tyes”).\tSuch\ta\tclassification\tsystem\tthat\toutputs\tmultiple\tbinary labels\tis\tcalled\ta\tmultilabel\tclassification\tsystem.\n\nWe\twon’t\tgo\tinto\tface\trecognition\tjust\tyet,\tbut\tlet’s\tlook\tat\ta\tsimpler\texample,\tjust\tfor\tillustration purposes:\n\nfrom\tsklearn.neighbors\timport\tKNeighborsClassifier\n\ny_train_large\t=\t(y_train\t>=\t7) y_train_odd\t=\t(y_train\t%\t2\t==\t1) y_multilabel\t=\tnp.c_[y_train_large,\ty_train_odd]\n\nknn_clf\t=\tKNeighborsClassifier() knn_clf.fit(X_train,\ty_multilabel)\n\nThis\tcode\tcreates\ta\ty_multilabel\tarray\tcontaining\ttwo\ttarget\tlabels\tfor\teach\tdigit\timage:\tthe\tfirst indicates\twhether\tor\tnot\tthe\tdigit\tis\tlarge\t(7,\t8,\tor\t9)\tand\tthe\tsecond\tindicates\twhether\tor\tnot\tit\tis\todd. The\tnext\tlines\tcreate\ta\tKNeighborsClassifier\tinstance\t(which\tsupports\tmultilabel\tclassification,\tbut not\tall\tclassifiers\tdo)\tand\twe\ttrain\tit\tusing\tthe\tmultiple\ttargets\tarray.\tNow\tyou\tcan\tmake\ta\tprediction,\tand notice\tthat\tit\toutputs\ttwo\tlabels:\n\n>>>\tknn_clf.predict([some_digit]) array([[False,\t\tTrue]],\tdtype=bool)\n\nAnd\tit\tgets\tit\tright!\tThe\tdigit\t5\tis\tindeed\tnot\tlarge\t(False)\tand\todd\t(True).\n\nThere\tare\tmany\tways\tto\tevaluate\ta\tmultilabel\tclassifier,\tand\tselecting\tthe\tright\tmetric\treally\tdepends\ton your\tproject.\tFor\texample,\tone\tapproach\tis\tto\tmeasure\tthe\tF1\tscore\tfor\teach\tindividual\tlabel\t(or\tany\tother binary\tclassifier\tmetric\tdiscussed\tearlier),\tthen\tsimply\tcompute\tthe\taverage\tscore.\tThis\tcode\tcomputes the\taverage\tF1\tscore\tacross\tall\tlabels:\n\n>>>\ty_train_knn_pred\t=\tcross_val_predict(knn_clf,\tX_train,\ty_train,\tcv=3) >>>\tf1_score(y_train,\ty_train_knn_pred,\taverage=\"macro\") 0.96845540180280221\n\nThis\tassumes\tthat\tall\tlabels\tare\tequally\timportant,\twhich\tmay\tnot\tbe\tthe\tcase.\tIn\tparticular,\tif\tyou\thave many\tmore\tpictures\tof\tAlice\tthan\tof\tBob\tor\tCharlie,\tyou\tmay\twant\tto\tgive\tmore\tweight\tto\tthe\tclassifier’s score\ton\tpictures\tof\tAlice.\tOne\tsimple\toption\tis\tto\tgive\teach\tlabel\ta\tweight\tequal\tto\tits\tsupport\t(i.e.,\tthe number\tof\tinstances\twith\tthat\ttarget\tlabel).\tTo\tdo\tthis,\tsimply\tset\taverage=\"weighted\"\tin\tthe\tpreceding code.4",
      "content_length": 2586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "Multioutput\tClassification The\tlast\ttype\tof\tclassification\ttask\twe\tare\tgoing\tto\tdiscuss\there\tis\tcalled\tmultioutput-multiclass classification\t(or\tsimply\tmultioutput\tclassification).\tIt\tis\tsimply\ta\tgeneralization\tof\tmultilabel classification\twhere\teach\tlabel\tcan\tbe\tmulticlass\t(i.e.,\tit\tcan\thave\tmore\tthan\ttwo\tpossible\tvalues).\n\nTo\tillustrate\tthis,\tlet’s\tbuild\ta\tsystem\tthat\tremoves\tnoise\tfrom\timages.\tIt\twill\ttake\tas\tinput\ta\tnoisy\tdigit image,\tand\tit\twill\t(hopefully)\toutput\ta\tclean\tdigit\timage,\trepresented\tas\tan\tarray\tof\tpixel\tintensities,\tjust like\tthe\tMNIST\timages.\tNotice\tthat\tthe\tclassifier’s\toutput\tis\tmultilabel\t(one\tlabel\tper\tpixel)\tand\teach label\tcan\thave\tmultiple\tvalues\t(pixel\tintensity\tranges\tfrom\t0\tto\t255).\tIt\tis\tthus\tan\texample\tof\ta\tmultioutput classification\tsystem.\n\nNOTE\n\nThe\tline\tbetween\tclassification\tand\tregression\tis\tsometimes\tblurry,\tsuch\tas\tin\tthis\texample.\tArguably,\tpredicting\tpixel\tintensity\tis more\takin\tto\tregression\tthan\tto\tclassification.\tMoreover,\tmultioutput\tsystems\tare\tnot\tlimited\tto\tclassification\ttasks;\tyou\tcould\teven have\ta\tsystem\tthat\toutputs\tmultiple\tlabels\tper\tinstance,\tincluding\tboth\tclass\tlabels\tand\tvalue\tlabels.\n\nLet’s\tstart\tby\tcreating\tthe\ttraining\tand\ttest\tsets\tby\ttaking\tthe\tMNIST\timages\tand\tadding\tnoise\tto\ttheir\tpixel intensities\tusing\tNumPy’s\trandint()\tfunction.\tThe\ttarget\timages\twill\tbe\tthe\toriginal\timages:\n\nnoise\t=\tnp.random.randint(0,\t100,\t(len(X_train),\t784)) X_train_mod\t=\tX_train\t+\tnoise noise\t=\tnp.random.randint(0,\t100,\t(len(X_test),\t784)) X_test_mod\t=\tX_test\t+\tnoise y_train_mod\t=\tX_train y_test_mod\t=\tX_test\n\nLet’s\ttake\ta\tpeek\tat\tan\timage\tfrom\tthe\ttest\tset\t(yes,\twe’re\tsnooping\ton\tthe\ttest\tdata,\tso\tyou\tshould\tbe frowning\tright\tnow):\n\nOn\tthe\tleft\tis\tthe\tnoisy\tinput\timage,\tand\ton\tthe\tright\tis\tthe\tclean\ttarget\timage.\tNow\tlet’s\ttrain\tthe\tclassifier",
      "content_length": 1813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "and\tmake\tit\tclean\tthis\timage:\n\nknn_clf.fit(X_train_mod,\ty_train_mod) clean_digit\t=\tknn_clf.predict([X_test_mod[some_index]]) plot_digit(clean_digit)\n\nLooks\tclose\tenough\tto\tthe\ttarget!\tThis\tconcludes\tour\ttour\tof\tclassification.\tHopefully\tyou\tshould\tnow know\thow\tto\tselect\tgood\tmetrics\tfor\tclassification\ttasks,\tpick\tthe\tappropriate\tprecision/recall\ttradeoff, compare\tclassifiers,\tand\tmore\tgenerally\tbuild\tgood\tclassification\tsystems\tfor\ta\tvariety\tof\ttasks.",
      "content_length": 455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "Exercises\n\n1.\t Try\tto\tbuild\ta\tclassifier\tfor\tthe\tMNIST\tdataset\tthat\tachieves\tover\t97%\taccuracy\ton\tthe\ttest\tset.\tHint: the\tKNeighborsClassifier\tworks\tquite\twell\tfor\tthis\ttask;\tyou\tjust\tneed\tto\tfind\tgood hyperparameter\tvalues\t(try\ta\tgrid\tsearch\ton\tthe\tweights\tand\tn_neighbors\thyperparameters).\n\n2.\t Write\ta\tfunction\tthat\tcan\tshift\tan\tMNIST\timage\tin\tany\tdirection\t(left,\tright,\tup,\tor\tdown)\tby\tone pixel.5\tThen,\tfor\teach\timage\tin\tthe\ttraining\tset,\tcreate\tfour\tshifted\tcopies\t(one\tper\tdirection)\tand\tadd them\tto\tthe\ttraining\tset.\tFinally,\ttrain\tyour\tbest\tmodel\ton\tthis\texpanded\ttraining\tset\tand\tmeasure\tits accuracy\ton\tthe\ttest\tset.\tYou\tshould\tobserve\tthat\tyour\tmodel\tperforms\teven\tbetter\tnow!\tThis technique\tof\tartificially\tgrowing\tthe\ttraining\tset\tis\tcalled\tdata\taugmentation\tor\ttraining\tset expansion.\n\n3.\t Tackle\tthe\tTitanic\tdataset.\tA\tgreat\tplace\tto\tstart\tis\ton\tKaggle.\n\n4.\t Build\ta\tspam\tclassifier\t(a\tmore\tchallenging\texercise):\n\nDownload\texamples\tof\tspam\tand\tham\tfrom\tApache\tSpamAssassin’s\tpublic\tdatasets.\n\nUnzip\tthe\tdatasets\tand\tfamiliarize\tyourself\twith\tthe\tdata\tformat.\n\nSplit\tthe\tdatasets\tinto\ta\ttraining\tset\tand\ta\ttest\tset.\n\nWrite\ta\tdata\tpreparation\tpipeline\tto\tconvert\teach\temail\tinto\ta\tfeature\tvector.\tYour\tpreparation pipeline\tshould\ttransform\tan\temail\tinto\ta\t(sparse)\tvector\tindicating\tthe\tpresence\tor\tabsence\tof each\tpossible\tword.\tFor\texample,\tif\tall\temails\tonly\tever\tcontain\tfour\twords,\t“Hello,”\t“how,” “are,”\t“you,”\tthen\tthe\temail\t“Hello\tyou\tHello\tHello\tyou”\twould\tbe\tconverted\tinto\ta\tvector\t[1, 0,\t0,\t1]\t(meaning\t[“Hello”\tis\tpresent,\t“how”\tis\tabsent,\t“are”\tis\tabsent,\t“you”\tis\tpresent]),\tor [3,\t0,\t0,\t2]\tif\tyou\tprefer\tto\tcount\tthe\tnumber\tof\toccurrences\tof\teach\tword.\n\nYou\tmay\twant\tto\tadd\thyperparameters\tto\tyour\tpreparation\tpipeline\tto\tcontrol\twhether\tor\tnot\tto strip\toff\temail\theaders,\tconvert\teach\temail\tto\tlowercase,\tremove\tpunctuation,\treplace\tall\tURLs with\t“URL,”\treplace\tall\tnumbers\twith\t“NUMBER,”\tor\teven\tperform\tstemming\t(i.e.,\ttrim\toff word\tendings;\tthere\tare\tPython\tlibraries\tavailable\tto\tdo\tthis).\n\nThen\ttry\tout\tseveral\tclassifiers\tand\tsee\tif\tyou\tcan\tbuild\ta\tgreat\tspam\tclassifier,\twith\tboth\thigh recall\tand\thigh\tprecision.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tthe\tonline\tJupyter\tnotebooks\tat https://github.com/ageron/handson-ml.\n\n1\n\nBy\tdefault\tScikit-Learn\tcaches\tdownloaded\tdatasets\tin\ta\tdirectory\tcalled\t$HOME/scikit_learn_data.\n\n2\n\nShuffling\tmay\tbe\ta\tbad\tidea\tin\tsome\tcontexts\t—\tfor\texample,\tif\tyou\tare\tworking\ton\ttime\tseries\tdata\t(such\tas\tstock\tmarket\tprices\tor weather\tconditions).\tWe\twill\texplore\tthis\tin\tthe\tnext\tchapters.\n\n3\n\nBut\tremember\tthat\tour\tbrain\tis\ta\tfantastic\tpattern\trecognition\tsystem,\tand\tour\tvisual\tsystem\tdoes\ta\tlot\tof\tcomplex\tpreprocessing\tbefore any\tinformation\treaches\tour\tconsciousness,\tso\tthe\tfact\tthat\tit\tfeels\tsimple\tdoes\tnot\tmean\tthat\tit\tis.\n\n4",
      "content_length": 2815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "4\n\nScikit-Learn\toffers\ta\tfew\tother\taveraging\toptions\tand\tmultilabel\tclassifier\tmetrics;\tsee\tthe\tdocumentation\tfor\tmore\tdetails.\n\nYou\tcan\tuse\tthe\tshift()\tfunction\tfrom\tthe\tscipy.ndimage.interpolation\tmodule.\tFor\texample,\tshift(image,\t[2,\t1],\tcval=0)\tshifts the\timage\t2\tpixels\tdown\tand\t1\tpixel\tto\tthe\tright.\n\n5",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "Chapter\t4.\tTraining\tModels\n\nSo\tfar\twe\thave\ttreated\tMachine\tLearning\tmodels\tand\ttheir\ttraining\talgorithms\tmostly\tlike\tblack\tboxes.\tIf you\twent\tthrough\tsome\tof\tthe\texercises\tin\tthe\tprevious\tchapters,\tyou\tmay\thave\tbeen\tsurprised\tby\thow much\tyou\tcan\tget\tdone\twithout\tknowing\tanything\tabout\twhat’s\tunder\tthe\thood:\tyou\toptimized\ta\tregression system,\tyou\timproved\ta\tdigit\timage\tclassifier,\tand\tyou\teven\tbuilt\ta\tspam\tclassifier\tfrom\tscratch\t—\tall this\twithout\tknowing\thow\tthey\tactually\twork.\tIndeed,\tin\tmany\tsituations\tyou\tdon’t\treally\tneed\tto\tknow\tthe implementation\tdetails.\n\nHowever,\thaving\ta\tgood\tunderstanding\tof\thow\tthings\twork\tcan\thelp\tyou\tquickly\thome\tin\ton\tthe appropriate\tmodel,\tthe\tright\ttraining\talgorithm\tto\tuse,\tand\ta\tgood\tset\tof\thyperparameters\tfor\tyour\ttask. Understanding\twhat’s\tunder\tthe\thood\twill\talso\thelp\tyou\tdebug\tissues\tand\tperform\terror\tanalysis\tmore efficiently.\tLastly,\tmost\tof\tthe\ttopics\tdiscussed\tin\tthis\tchapter\twill\tbe\tessential\tin\tunderstanding,\tbuilding, and\ttraining\tneural\tnetworks\t(discussed\tin\tPart\tII\tof\tthis\tbook).\n\nIn\tthis\tchapter,\twe\twill\tstart\tby\tlooking\tat\tthe\tLinear\tRegression\tmodel,\tone\tof\tthe\tsimplest\tmodels\tthere is.\tWe\twill\tdiscuss\ttwo\tvery\tdifferent\tways\tto\ttrain\tit:\n\nUsing\ta\tdirect\t“closed-form”\tequation\tthat\tdirectly\tcomputes\tthe\tmodel\tparameters\tthat\tbest\tfit\tthe model\tto\tthe\ttraining\tset\t(i.e.,\tthe\tmodel\tparameters\tthat\tminimize\tthe\tcost\tfunction\tover\tthe\ttraining set).\n\nUsing\tan\titerative\toptimization\tapproach,\tcalled\tGradient\tDescent\t(GD),\tthat\tgradually\ttweaks\tthe model\tparameters\tto\tminimize\tthe\tcost\tfunction\tover\tthe\ttraining\tset,\teventually\tconverging\tto\tthe same\tset\tof\tparameters\tas\tthe\tfirst\tmethod.\tWe\twill\tlook\tat\ta\tfew\tvariants\tof\tGradient\tDescent\tthat we\twill\tuse\tagain\tand\tagain\twhen\twe\tstudy\tneural\tnetworks\tin\tPart\tII:\tBatch\tGD,\tMini-batch\tGD, and\tStochastic\tGD.\n\nNext\twe\twill\tlook\tat\tPolynomial\tRegression,\ta\tmore\tcomplex\tmodel\tthat\tcan\tfit\tnonlinear\tdatasets.\tSince this\tmodel\thas\tmore\tparameters\tthan\tLinear\tRegression,\tit\tis\tmore\tprone\tto\toverfitting\tthe\ttraining\tdata, so\twe\twill\tlook\tat\thow\tto\tdetect\twhether\tor\tnot\tthis\tis\tthe\tcase,\tusing\tlearning\tcurves,\tand\tthen\twe\twill look\tat\tseveral\tregularization\ttechniques\tthat\tcan\treduce\tthe\trisk\tof\toverfitting\tthe\ttraining\tset.\n\nFinally,\twe\twill\tlook\tat\ttwo\tmore\tmodels\tthat\tare\tcommonly\tused\tfor\tclassification\ttasks:\tLogistic Regression\tand\tSoftmax\tRegression.\n\nWARNING\n\nThere\twill\tbe\tquite\ta\tfew\tmath\tequations\tin\tthis\tchapter,\tusing\tbasic\tnotions\tof\tlinear\talgebra\tand\tcalculus.\tTo\tunderstand\tthese equations,\tyou\twill\tneed\tto\tknow\twhat\tvectors\tand\tmatrices\tare,\thow\tto\ttranspose\tthem,\twhat\tthe\tdot\tproduct\tis,\twhat\tmatrix inverse\tis,\tand\twhat\tpartial\tderivatives\tare.\tIf\tyou\tare\tunfamiliar\twith\tthese\tconcepts,\tplease\tgo\tthrough\tthe\tlinear\talgebra\tand calculus\tintroductory\ttutorials\tavailable\tas\tJupyter\tnotebooks\tin\tthe\tonline\tsupplemental\tmaterial.\tFor\tthose\twho\tare\ttruly\tallergic to\tmathematics,\tyou\tshould\tstill\tgo\tthrough\tthis\tchapter\tand\tsimply\tskip\tthe\tequations;\thopefully,\tthe\ttext\twill\tbe\tsufficient\tto\thelp you\tunderstand\tmost\tof\tthe\tconcepts.",
      "content_length": 3083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Linear\tRegression In\tChapter\t1,\twe\tlooked\tat\ta\tsimple\tregression\tmodel\tof\tlife\tsatisfaction:\tlife_satisfaction\t=\tθ0\t+\tθ1\t× GDP_per_capita.\n\nThis\tmodel\tis\tjust\ta\tlinear\tfunction\tof\tthe\tinput\tfeature\tGDP_per_capita.\tθ0\tand\tθ1\tare\tthe\tmodel’s parameters.\n\nMore\tgenerally,\ta\tlinear\tmodel\tmakes\ta\tprediction\tby\tsimply\tcomputing\ta\tweighted\tsum\tof\tthe\tinput features,\tplus\ta\tconstant\tcalled\tthe\tbias\tterm\t(also\tcalled\tthe\tintercept\tterm),\tas\tshown\tin\tEquation\t4-1.\n\nEquation\t4-1.\tLinear\tRegression\tmodel\tprediction\n\nŷ\tis\tthe\tpredicted\tvalue.\n\nn\tis\tthe\tnumber\tof\tfeatures.\n\nxi\tis\tthe\tith\tfeature\tvalue.\n\nθj\tis\tthe\tjth\tmodel\tparameter\t(including\tthe\tbias\tterm\tθ0\tand\tthe\tfeature\tweights\tθ1,\tθ2,\t,\t θn).\n\nThis\tcan\tbe\twritten\tmuch\tmore\tconcisely\tusing\ta\tvectorized\tform,\tas\tshown\tin\tEquation\t4-2.\n\nEquation\t4-2.\tLinear\tRegression\tmodel\tprediction\t(vectorized\tform)\n\nθ\tis\tthe\tmodel’s\tparameter\tvector,\tcontaining\tthe\tbias\tterm\tθ0\tand\tthe\tfeature\tweights\tθ1\tto\tθn.\n\nθT\tis\tthe\ttranspose\tof\tθ\t(a\trow\tvector\tinstead\tof\ta\tcolumn\tvector).\n\nx\tis\tthe\tinstance’s\tfeature\tvector,\tcontaining\tx0\tto\txn,\twith\tx0\talways\tequal\tto\t1.\n\nθT\t·\tx\tis\tthe\tdot\tproduct\tof\tθT\tand\tx.\n\nhθ\tis\tthe\thypothesis\tfunction,\tusing\tthe\tmodel\tparameters\tθ.\n\nOkay,\tthat’s\tthe\tLinear\tRegression\tmodel,\tso\tnow\thow\tdo\twe\ttrain\tit?\tWell,\trecall\tthat\ttraining\ta\tmodel means\tsetting\tits\tparameters\tso\tthat\tthe\tmodel\tbest\tfits\tthe\ttraining\tset.\tFor\tthis\tpurpose,\twe\tfirst\tneed\ta measure\tof\thow\twell\t(or\tpoorly)\tthe\tmodel\tfits\tthe\ttraining\tdata.\tIn\tChapter\t2\twe\tsaw\tthat\tthe\tmost common\tperformance\tmeasure\tof\ta\tregression\tmodel\tis\tthe\tRoot\tMean\tSquare\tError\t(RMSE)\t(Equation 2-1).\tTherefore,\tto\ttrain\ta\tLinear\tRegression\tmodel,\tyou\tneed\tto\tfind\tthe\tvalue\tof\tθ\tthat\tminimizes\tthe",
      "content_length": 1721,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "RMSE.\tIn\tpractice,\tit\tis\tsimpler\tto\tminimize\tthe\tMean\tSquare\tError\t(MSE)\tthan\tthe\tRMSE,\tand\tit\tleads to\tthe\tsame\tresult\t(because\tthe\tvalue\tthat\tminimizes\ta\tfunction\talso\tminimizes\tits\tsquare\troot).1\n\nThe\tMSE\tof\ta\tLinear\tRegression\thypothesis\thθ\ton\ta\ttraining\tset\tX\tis\tcalculated\tusing\tEquation\t4-3.\n\nEquation\t4-3.\tMSE\tcost\tfunction\tfor\ta\tLinear\tRegression\tmodel\n\nMost\tof\tthese\tnotations\twere\tpresented\tin\tChapter\t2\t(see\t“Notations”).\tThe\tonly\tdifference\tis\tthat\twe write\thθ\tinstead\tof\tjust\th\tin\torder\tto\tmake\tit\tclear\tthat\tthe\tmodel\tis\tparametrized\tby\tthe\tvector\tθ.\tTo simplify\tnotations,\twe\twill\tjust\twrite\tMSE(θ)\tinstead\tof\tMSE(X,\thθ).",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "The\tNormal\tEquation To\tfind\tthe\tvalue\tof\tθ\tthat\tminimizes\tthe\tcost\tfunction,\tthere\tis\ta\tclosed-form\tsolution\t—\tin\tother\twords, a\tmathematical\tequation\tthat\tgives\tthe\tresult\tdirectly.\tThis\tis\tcalled\tthe\tNormal\tEquation\t(Equation\t4-4).2\n\nEquation\t4-4.\tNormal\tEquation\n\nis\tthe\tvalue\tof\n\nthat\tminimizes\tthe\tcost\tfunction.\n\ny\tis\tthe\tvector\tof\ttarget\tvalues\tcontaining\ty(1)\tto\ty(m).\n\nLet’s\tgenerate\tsome\tlinear-looking\tdata\tto\ttest\tthis\tequation\ton\t(Figure\t4-1):\n\nimport\tnumpy\tas\tnp\n\nX\t=\t2\t*\tnp.random.rand(100,\t1) y\t=\t4\t+\t3\t*\tX\t+\tnp.random.randn(100,\t1)\n\nFigure\t4-1.\tRandomly\tgenerated\tlinear\tdataset\n\nNow\tlet’s\tcompute\t Algebra\tmodule\t(np.linalg)\tto\tcompute\tthe\tinverse\tof\ta\tmatrix,\tand\tthe\tdot()\tmethod\tfor\tmatrix multiplication:\n\nusing\tthe\tNormal\tEquation.\tWe\twill\tuse\tthe\tinv()\tfunction\tfrom\tNumPy’s\tLinear\n\nX_b\t=\tnp.c_[np.ones((100,\t1)),\tX]\t\t#\tadd\tx0\t=\t1\tto\teach\tinstance theta_best\t=\tnp.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)",
      "content_length": 932,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "The\tactual\tfunction\tthat\twe\tused\tto\tgenerate\tthe\tdata\tis\ty\t=\t4\t+\t3x0\t+\tGaussian\tnoise.\tLet’s\tsee\twhat\tthe equation\tfound:\n\n>>>\ttheta_best array([[\t4.21509616], \t\t\t\t\t\t\t[\t2.77011339]])\n\nWe\twould\thave\thoped\tfor\tθ0\t=\t4\tand\tθ1\t=\t3\tinstead\tof\tθ0\t=\t4.215\tand\tθ1\t=\t2.770.\tClose\tenough,\tbut\tthe noise\tmade\tit\timpossible\tto\trecover\tthe\texact\tparameters\tof\tthe\toriginal\tfunction.\n\nNow\tyou\tcan\tmake\tpredictions\tusing\t :\n\n>>>\tX_new\t=\tnp.array([[0],\t[2]]) >>>\tX_new_b\t=\tnp.c_[np.ones((2,\t1)),\tX_new]\t#\tadd\tx0\t=\t1\tto\teach\tinstance >>>\ty_predict\t=\tX_new_b.dot(theta_best) >>>\ty_predict array([[\t4.21509616], \t\t\t\t\t\t\t[\t9.75532293]])\n\nLet’s\tplot\tthis\tmodel’s\tpredictions\t(Figure\t4-2):\n\nplt.plot(X_new,\ty_predict,\t\"r-\") plt.plot(X,\ty,\t\"b.\") plt.axis([0,\t2,\t0,\t15]) plt.show()\n\nFigure\t4-2.\tLinear\tRegression\tmodel\tpredictions\n\nThe\tequivalent\tcode\tusing\tScikit-Learn\tlooks\tlike\tthis:3\n\n>>>\tfrom\tsklearn.linear_model\timport\tLinearRegression >>>\tlin_reg\t=\tLinearRegression() >>>\tlin_reg.fit(X,\ty) >>>\tlin_reg.intercept_,\tlin_reg.coef_ (array([\t4.21509616]),\tarray([[\t2.77011339]])) >>>\tlin_reg.predict(X_new)",
      "content_length": 1084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "array([[\t4.21509616], \t\t\t\t\t\t\t[\t9.75532293]])",
      "content_length": 44,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Computational\tComplexity The\tNormal\tEquation\tcomputes\tthe\tinverse\tof\tXT\t·\tX,\twhich\tis\tan\tn\t×\tn\tmatrix\t(where\tn\tis\tthe\tnumber\tof features).\tThe\tcomputational\tcomplexity\tof\tinverting\tsuch\ta\tmatrix\tis\ttypically\tabout\tO(n2.4)\tto\tO(n3) (depending\ton\tthe\timplementation).\tIn\tother\twords,\tif\tyou\tdouble\tthe\tnumber\tof\tfeatures,\tyou\tmultiply\tthe computation\ttime\tby\troughly\t22.4\t=\t5.3\tto\t23\t=\t8.\n\nWARNING\n\nThe\tNormal\tEquation\tgets\tvery\tslow\twhen\tthe\tnumber\tof\tfeatures\tgrows\tlarge\t(e.g.,\t100,000).\n\nOn\tthe\tpositive\tside,\tthis\tequation\tis\tlinear\twith\tregards\tto\tthe\tnumber\tof\tinstances\tin\tthe\ttraining\tset\t(it\tis O(m)),\tso\tit\thandles\tlarge\ttraining\tsets\tefficiently,\tprovided\tthey\tcan\tfit\tin\tmemory.\n\nAlso,\tonce\tyou\thave\ttrained\tyour\tLinear\tRegression\tmodel\t(using\tthe\tNormal\tEquation\tor\tany\tother algorithm),\tpredictions\tare\tvery\tfast:\tthe\tcomputational\tcomplexity\tis\tlinear\twith\tregards\tto\tboth\tthe number\tof\tinstances\tyou\twant\tto\tmake\tpredictions\ton\tand\tthe\tnumber\tof\tfeatures.\tIn\tother\twords,\tmaking predictions\ton\ttwice\tas\tmany\tinstances\t(or\ttwice\tas\tmany\tfeatures)\twill\tjust\ttake\troughly\ttwice\tas\tmuch time.\n\nNow\twe\twill\tlook\tat\tvery\tdifferent\tways\tto\ttrain\ta\tLinear\tRegression\tmodel,\tbetter\tsuited\tfor\tcases where\tthere\tare\ta\tlarge\tnumber\tof\tfeatures,\tor\ttoo\tmany\ttraining\tinstances\tto\tfit\tin\tmemory.",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Gradient\tDescent Gradient\tDescent\tis\ta\tvery\tgeneric\toptimization\talgorithm\tcapable\tof\tfinding\toptimal\tsolutions\tto\ta\twide range\tof\tproblems.\tThe\tgeneral\tidea\tof\tGradient\tDescent\tis\tto\ttweak\tparameters\titeratively\tin\torder\tto minimize\ta\tcost\tfunction.\n\nSuppose\tyou\tare\tlost\tin\tthe\tmountains\tin\ta\tdense\tfog;\tyou\tcan\tonly\tfeel\tthe\tslope\tof\tthe\tground\tbelow\tyour feet.\tA\tgood\tstrategy\tto\tget\tto\tthe\tbottom\tof\tthe\tvalley\tquickly\tis\tto\tgo\tdownhill\tin\tthe\tdirection\tof\tthe steepest\tslope.\tThis\tis\texactly\twhat\tGradient\tDescent\tdoes:\tit\tmeasures\tthe\tlocal\tgradient\tof\tthe\terror function\twith\tregards\tto\tthe\tparameter\tvector\tθ,\tand\tit\tgoes\tin\tthe\tdirection\tof\tdescending\tgradient.\tOnce the\tgradient\tis\tzero,\tyou\thave\treached\ta\tminimum!\n\nConcretely,\tyou\tstart\tby\tfilling\tθ\twith\trandom\tvalues\t(this\tis\tcalled\trandom\tinitialization),\tand\tthen\tyou improve\tit\tgradually,\ttaking\tone\tbaby\tstep\tat\ta\ttime,\teach\tstep\tattempting\tto\tdecrease\tthe\tcost\tfunction (e.g.,\tthe\tMSE),\tuntil\tthe\talgorithm\tconverges\tto\ta\tminimum\t(see\tFigure\t4-3).\n\nFigure\t4-3.\tGradient\tDescent\n\nAn\timportant\tparameter\tin\tGradient\tDescent\tis\tthe\tsize\tof\tthe\tsteps,\tdetermined\tby\tthe\tlearning\trate hyperparameter.\tIf\tthe\tlearning\trate\tis\ttoo\tsmall,\tthen\tthe\talgorithm\twill\thave\tto\tgo\tthrough\tmany\titerations to\tconverge,\twhich\twill\ttake\ta\tlong\ttime\t(see\tFigure\t4-4).",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "Figure\t4-4.\tLearning\trate\ttoo\tsmall\n\nOn\tthe\tother\thand,\tif\tthe\tlearning\trate\tis\ttoo\thigh,\tyou\tmight\tjump\tacross\tthe\tvalley\tand\tend\tup\ton\tthe\tother side,\tpossibly\teven\thigher\tup\tthan\tyou\twere\tbefore.\tThis\tmight\tmake\tthe\talgorithm\tdiverge,\twith\tlarger and\tlarger\tvalues,\tfailing\tto\tfind\ta\tgood\tsolution\t(see\tFigure\t4-5).\n\nFigure\t4-5.\tLearning\trate\ttoo\tlarge\n\nFinally,\tnot\tall\tcost\tfunctions\tlook\tlike\tnice\tregular\tbowls.\tThere\tmay\tbe\tholes,\tridges,\tplateaus,\tand\tall sorts\tof\tirregular\tterrains,\tmaking\tconvergence\tto\tthe\tminimum\tvery\tdifficult.\tFigure\t4-6\tshows\tthe\ttwo main\tchallenges\twith\tGradient\tDescent:\tif\tthe\trandom\tinitialization\tstarts\tthe\talgorithm\ton\tthe\tleft,\tthen\tit will\tconverge\tto\ta\tlocal\tminimum,\twhich\tis\tnot\tas\tgood\tas\tthe\tglobal\tminimum.\tIf\tit\tstarts\ton\tthe\tright, then\tit\twill\ttake\ta\tvery\tlong\ttime\tto\tcross\tthe\tplateau,\tand\tif\tyou\tstop\ttoo\tearly\tyou\twill\tnever\treach\tthe global\tminimum.",
      "content_length": 907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Figure\t4-6.\tGradient\tDescent\tpitfalls\n\nFortunately,\tthe\tMSE\tcost\tfunction\tfor\ta\tLinear\tRegression\tmodel\thappens\tto\tbe\ta\tconvex\tfunction,\twhich means\tthat\tif\tyou\tpick\tany\ttwo\tpoints\ton\tthe\tcurve,\tthe\tline\tsegment\tjoining\tthem\tnever\tcrosses\tthe\tcurve. This\timplies\tthat\tthere\tare\tno\tlocal\tminima,\tjust\tone\tglobal\tminimum.\tIt\tis\talso\ta\tcontinuous\tfunction\twith a\tslope\tthat\tnever\tchanges\tabruptly.4\tThese\ttwo\tfacts\thave\ta\tgreat\tconsequence:\tGradient\tDescent\tis guaranteed\tto\tapproach\tarbitrarily\tclose\tthe\tglobal\tminimum\t(if\tyou\twait\tlong\tenough\tand\tif\tthe\tlearning rate\tis\tnot\ttoo\thigh).\n\nIn\tfact,\tthe\tcost\tfunction\thas\tthe\tshape\tof\ta\tbowl,\tbut\tit\tcan\tbe\tan\telongated\tbowl\tif\tthe\tfeatures\thave\tvery different\tscales.\tFigure\t4-7\tshows\tGradient\tDescent\ton\ta\ttraining\tset\twhere\tfeatures\t1\tand\t2\thave\tthe same\tscale\t(on\tthe\tleft),\tand\ton\ta\ttraining\tset\twhere\tfeature\t1\thas\tmuch\tsmaller\tvalues\tthan\tfeature\t2\t(on the\tright).5\n\nFigure\t4-7.\tGradient\tDescent\twith\tand\twithout\tfeature\tscaling\n\nAs\tyou\tcan\tsee,\ton\tthe\tleft\tthe\tGradient\tDescent\talgorithm\tgoes\tstraight\ttoward\tthe\tminimum,\tthereby reaching\tit\tquickly,\twhereas\ton\tthe\tright\tit\tfirst\tgoes\tin\ta\tdirection\talmost\torthogonal\tto\tthe\tdirection\tof the\tglobal\tminimum,\tand\tit\tends\twith\ta\tlong\tmarch\tdown\tan\talmost\tflat\tvalley.\tIt\twill\teventually\treach\tthe minimum,\tbut\tit\twill\ttake\ta\tlong\ttime.",
      "content_length": 1338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "WARNING\n\nWhen\tusing\tGradient\tDescent,\tyou\tshould\tensure\tthat\tall\tfeatures\thave\ta\tsimilar\tscale\t(e.g.,\tusing\tScikit-Learn’s\tStandardScaler class),\tor\telse\tit\twill\ttake\tmuch\tlonger\tto\tconverge.\n\nThis\tdiagram\talso\tillustrates\tthe\tfact\tthat\ttraining\ta\tmodel\tmeans\tsearching\tfor\ta\tcombination\tof\tmodel parameters\tthat\tminimizes\ta\tcost\tfunction\t(over\tthe\ttraining\tset).\tIt\tis\ta\tsearch\tin\tthe\tmodel’s\tparameter space:\tthe\tmore\tparameters\ta\tmodel\thas,\tthe\tmore\tdimensions\tthis\tspace\thas,\tand\tthe\tharder\tthe\tsearch\tis: searching\tfor\ta\tneedle\tin\ta\t300-dimensional\thaystack\tis\tmuch\ttrickier\tthan\tin\tthree\tdimensions. Fortunately,\tsince\tthe\tcost\tfunction\tis\tconvex\tin\tthe\tcase\tof\tLinear\tRegression,\tthe\tneedle\tis\tsimply\tat\tthe bottom\tof\tthe\tbowl.",
      "content_length": 734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Batch\tGradient\tDescent To\timplement\tGradient\tDescent,\tyou\tneed\tto\tcompute\tthe\tgradient\tof\tthe\tcost\tfunction\twith\tregards\tto each\tmodel\tparameter\tθj.\tIn\tother\twords,\tyou\tneed\tto\tcalculate\thow\tmuch\tthe\tcost\tfunction\twill\tchange\tif you\tchange\tθj\tjust\ta\tlittle\tbit.\tThis\tis\tcalled\ta\tpartial\tderivative.\tIt\tis\tlike\tasking\t“what\tis\tthe\tslope\tof\tthe mountain\tunder\tmy\tfeet\tif\tI\tface\teast?”\tand\tthen\tasking\tthe\tsame\tquestion\tfacing\tnorth\t(and\tso\ton\tfor\tall other\tdimensions,\tif\tyou\tcan\timagine\ta\tuniverse\twith\tmore\tthan\tthree\tdimensions).\tEquation\t4-5\tcomputes\n\nthe\tpartial\tderivative\tof\tthe\tcost\tfunction\twith\tregards\tto\tparameter\tθj,\tnoted\n\n.\n\nEquation\t4-5.\tPartial\tderivatives\tof\tthe\tcost\tfunction\n\nInstead\tof\tcomputing\tthese\tpartial\tderivatives\tindividually,\tyou\tcan\tuse\tEquation\t4-6\tto\tcompute\tthem\tall in\tone\tgo.\tThe\tgradient\tvector,\tnoted\t θMSE(θ),\tcontains\tall\tthe\tpartial\tderivatives\tof\tthe\tcost\tfunction (one\tfor\teach\tmodel\tparameter).\n\nEquation\t4-6.\tGradient\tvector\tof\tthe\tcost\tfunction\n\nWARNING\n\nNotice\tthat\tthis\tformula\tinvolves\tcalculations\tover\tthe\tfull\ttraining\tset\tX,\tat\teach\tGradient\tDescent\tstep!\tThis\tis\twhy\tthe\talgorithm is\tcalled\tBatch\tGradient\tDescent:\tit\tuses\tthe\twhole\tbatch\tof\ttraining\tdata\tat\tevery\tstep.\tAs\ta\tresult\tit\tis\tterribly\tslow\ton\tvery large\ttraining\tsets\t(but\twe\twill\tsee\tmuch\tfaster\tGradient\tDescent\talgorithms\tshortly).\tHowever,\tGradient\tDescent\tscales\twell with\tthe\tnumber\tof\tfeatures;\ttraining\ta\tLinear\tRegression\tmodel\twhen\tthere\tare\thundreds\tof\tthousands\tof\tfeatures\tis\tmuch\tfaster using\tGradient\tDescent\tthan\tusing\tthe\tNormal\tEquation.\n\nOnce\tyou\thave\tthe\tgradient\tvector,\twhich\tpoints\tuphill,\tjust\tgo\tin\tthe\topposite\tdirection\tto\tgo\tdownhill. This\tmeans\tsubtracting\t θMSE(θ)\tfrom\tθ.\tThis\tis\twhere\tthe\tlearning\trate\tη\tcomes\tinto\tplay:6\tmultiply\tthe gradient\tvector\tby\tη\tto\tdetermine\tthe\tsize\tof\tthe\tdownhill\tstep\t(Equation\t4-7).\n\nEquation\t4-7.\tGradient\tDescent\tstep",
      "content_length": 1900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Let’s\tlook\tat\ta\tquick\timplementation\tof\tthis\talgorithm:\n\neta\t=\t0.1\t\t#\tlearning\trate n_iterations\t=\t1000 m\t=\t100\n\ntheta\t=\tnp.random.randn(2,1)\t\t#\trandom\tinitialization\n\nfor\titeration\tin\trange(n_iterations): \t\t\t\tgradients\t=\t2/m\t*\tX_b.T.dot(X_b.dot(theta)\t-\ty) \t\t\t\ttheta\t=\ttheta\t-\teta\t*\tgradients\n\nThat\twasn’t\ttoo\thard!\tLet’s\tlook\tat\tthe\tresulting\ttheta:\n\n>>>\ttheta array([[\t4.21509616], \t\t\t\t\t\t\t[\t2.77011339]])\n\nHey,\tthat’s\texactly\twhat\tthe\tNormal\tEquation\tfound!\tGradient\tDescent\tworked\tperfectly.\tBut\twhat\tif\tyou had\tused\ta\tdifferent\tlearning\trate\teta?\tFigure\t4-8\tshows\tthe\tfirst\t10\tsteps\tof\tGradient\tDescent\tusing\tthree different\tlearning\trates\t(the\tdashed\tline\trepresents\tthe\tstarting\tpoint).\n\nFigure\t4-8.\tGradient\tDescent\twith\tvarious\tlearning\trates\n\nOn\tthe\tleft,\tthe\tlearning\trate\tis\ttoo\tlow:\tthe\talgorithm\twill\teventually\treach\tthe\tsolution,\tbut\tit\twill\ttake\ta long\ttime.\tIn\tthe\tmiddle,\tthe\tlearning\trate\tlooks\tpretty\tgood:\tin\tjust\ta\tfew\titerations,\tit\thas\talready converged\tto\tthe\tsolution.\tOn\tthe\tright,\tthe\tlearning\trate\tis\ttoo\thigh:\tthe\talgorithm\tdiverges,\tjumping\tall over\tthe\tplace\tand\tactually\tgetting\tfurther\tand\tfurther\taway\tfrom\tthe\tsolution\tat\tevery\tstep.\n\nTo\tfind\ta\tgood\tlearning\trate,\tyou\tcan\tuse\tgrid\tsearch\t(see\tChapter\t2).\tHowever,\tyou\tmay\twant\tto\tlimit\tthe number\tof\titerations\tso\tthat\tgrid\tsearch\tcan\teliminate\tmodels\tthat\ttake\ttoo\tlong\tto\tconverge.\n\nYou\tmay\twonder\thow\tto\tset\tthe\tnumber\tof\titerations.\tIf\tit\tis\ttoo\tlow,\tyou\twill\tstill\tbe\tfar\taway\tfrom\tthe optimal\tsolution\twhen\tthe\talgorithm\tstops,\tbut\tif\tit\tis\ttoo\thigh,\tyou\twill\twaste\ttime\twhile\tthe\tmodel parameters\tdo\tnot\tchange\tanymore.\tA\tsimple\tsolution\tis\tto\tset\ta\tvery\tlarge\tnumber\tof\titerations\tbut\tto interrupt\tthe\talgorithm\twhen\tthe\tgradient\tvector\tbecomes\ttiny\t—\tthat\tis,\twhen\tits\tnorm\tbecomes\tsmaller than\ta\ttiny\tnumber\tϵ\t(called\tthe\ttolerance)\t—\tbecause\tthis\thappens\twhen\tGradient\tDescent\thas\t(almost)",
      "content_length": 1888,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "reached\tthe\tminimum.\n\nCONVERGENCE\tRATE\n\nWhen\tthe\tcost\tfunction\tis\tconvex\tand\tits\tslope\tdoes\tnot\tchange\tabruptly\t(as\tis\tthe\tcase\tfor\tthe\tMSE\tcost\tfunction),\tit\tcan\tbe\tshown\tthat\n\nBatch\tGradient\tDescent\twith\ta\tfixed\tlearning\trate\thas\ta\tconvergence\trate\tof\t tolerance\tϵ\tby\t10\t(to\thave\ta\tmore\tprecise\tsolution),\tthen\tthe\talgorithm\twill\thave\tto\trun\tabout\t10\ttimes\tmore\titerations.\n\n.\tIn\tother\twords,\tif\tyou\tdivide\tthe",
      "content_length": 412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Stochastic\tGradient\tDescent The\tmain\tproblem\twith\tBatch\tGradient\tDescent\tis\tthe\tfact\tthat\tit\tuses\tthe\twhole\ttraining\tset\tto\tcompute the\tgradients\tat\tevery\tstep,\twhich\tmakes\tit\tvery\tslow\twhen\tthe\ttraining\tset\tis\tlarge.\tAt\tthe\topposite extreme,\tStochastic\tGradient\tDescent\tjust\tpicks\ta\trandom\tinstance\tin\tthe\ttraining\tset\tat\tevery\tstep\tand computes\tthe\tgradients\tbased\tonly\ton\tthat\tsingle\tinstance.\tObviously\tthis\tmakes\tthe\talgorithm\tmuch\tfaster since\tit\thas\tvery\tlittle\tdata\tto\tmanipulate\tat\tevery\titeration.\tIt\talso\tmakes\tit\tpossible\tto\ttrain\ton\thuge training\tsets,\tsince\tonly\tone\tinstance\tneeds\tto\tbe\tin\tmemory\tat\teach\titeration\t(SGD\tcan\tbe\timplemented\tas an\tout-of-core\talgorithm.7)\n\nOn\tthe\tother\thand,\tdue\tto\tits\tstochastic\t(i.e.,\trandom)\tnature,\tthis\talgorithm\tis\tmuch\tless\tregular\tthan\tBatch Gradient\tDescent:\tinstead\tof\tgently\tdecreasing\tuntil\tit\treaches\tthe\tminimum,\tthe\tcost\tfunction\twill\tbounce up\tand\tdown,\tdecreasing\tonly\ton\taverage.\tOver\ttime\tit\twill\tend\tup\tvery\tclose\tto\tthe\tminimum,\tbut\tonce\tit gets\tthere\tit\twill\tcontinue\tto\tbounce\taround,\tnever\tsettling\tdown\t(see\tFigure\t4-9).\tSo\tonce\tthe\talgorithm stops,\tthe\tfinal\tparameter\tvalues\tare\tgood,\tbut\tnot\toptimal.\n\nFigure\t4-9.\tStochastic\tGradient\tDescent\n\nWhen\tthe\tcost\tfunction\tis\tvery\tirregular\t(as\tin\tFigure\t4-6),\tthis\tcan\tactually\thelp\tthe\talgorithm\tjump\tout\tof local\tminima,\tso\tStochastic\tGradient\tDescent\thas\ta\tbetter\tchance\tof\tfinding\tthe\tglobal\tminimum\tthan Batch\tGradient\tDescent\tdoes.\n\nTherefore\trandomness\tis\tgood\tto\tescape\tfrom\tlocal\toptima,\tbut\tbad\tbecause\tit\tmeans\tthat\tthe\talgorithm can\tnever\tsettle\tat\tthe\tminimum.\tOne\tsolution\tto\tthis\tdilemma\tis\tto\tgradually\treduce\tthe\tlearning\trate.\tThe steps\tstart\tout\tlarge\t(which\thelps\tmake\tquick\tprogress\tand\tescape\tlocal\tminima),\tthen\tget\tsmaller\tand smaller,\tallowing\tthe\talgorithm\tto\tsettle\tat\tthe\tglobal\tminimum.\tThis\tprocess\tis\tcalled\tsimulated",
      "content_length": 1867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "annealing,\tbecause\tit\tresembles\tthe\tprocess\tof\tannealing\tin\tmetallurgy\twhere\tmolten\tmetal\tis\tslowly cooled\tdown.\tThe\tfunction\tthat\tdetermines\tthe\tlearning\trate\tat\teach\titeration\tis\tcalled\tthe\tlearning schedule.\tIf\tthe\tlearning\trate\tis\treduced\ttoo\tquickly,\tyou\tmay\tget\tstuck\tin\ta\tlocal\tminimum,\tor\teven\tend\tup frozen\thalfway\tto\tthe\tminimum.\tIf\tthe\tlearning\trate\tis\treduced\ttoo\tslowly,\tyou\tmay\tjump\taround\tthe minimum\tfor\ta\tlong\ttime\tand\tend\tup\twith\ta\tsuboptimal\tsolution\tif\tyou\thalt\ttraining\ttoo\tearly.\n\nThis\tcode\timplements\tStochastic\tGradient\tDescent\tusing\ta\tsimple\tlearning\tschedule:\n\nn_epochs\t=\t50 t0,\tt1\t=\t5,\t50\t\t#\tlearning\tschedule\thyperparameters\n\ndef\tlearning_schedule(t): \t\t\t\treturn\tt0\t/\t(t\t+\tt1)\n\ntheta\t=\tnp.random.randn(2,1)\t\t#\trandom\tinitialization\n\nfor\tepoch\tin\trange(n_epochs): \t\t\t\tfor\ti\tin\trange(m): \t\t\t\t\t\t\t\trandom_index\t=\tnp.random.randint(m) \t\t\t\t\t\t\t\txi\t=\tX_b[random_index:random_index+1] \t\t\t\t\t\t\t\tyi\t=\ty[random_index:random_index+1] \t\t\t\t\t\t\t\tgradients\t=\t2\t*\txi.T.dot(xi.dot(theta)\t-\tyi) \t\t\t\t\t\t\t\teta\t=\tlearning_schedule(epoch\t*\tm\t+\ti) \t\t\t\t\t\t\t\ttheta\t=\ttheta\t-\teta\t*\tgradients\n\nBy\tconvention\twe\titerate\tby\trounds\tof\tm\titerations;\teach\tround\tis\tcalled\tan\tepoch.\tWhile\tthe\tBatch Gradient\tDescent\tcode\titerated\t1,000\ttimes\tthrough\tthe\twhole\ttraining\tset,\tthis\tcode\tgoes\tthrough\tthe training\tset\tonly\t50\ttimes\tand\treaches\ta\tfairly\tgood\tsolution:\n\n>>>\ttheta array([[\t4.21076011], \t\t\t\t\t\t[\t2.74856079]])\n\nFigure\t4-10\tshows\tthe\tfirst\t10\tsteps\tof\ttraining\t(notice\thow\tirregular\tthe\tsteps\tare).\n\nFigure\t4-10.\tStochastic\tGradient\tDescent\tfirst\t10\tsteps",
      "content_length": 1552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Note\tthat\tsince\tinstances\tare\tpicked\trandomly,\tsome\tinstances\tmay\tbe\tpicked\tseveral\ttimes\tper\tepoch while\tothers\tmay\tnot\tbe\tpicked\tat\tall.\tIf\tyou\twant\tto\tbe\tsure\tthat\tthe\talgorithm\tgoes\tthrough\tevery\tinstance at\teach\tepoch,\tanother\tapproach\tis\tto\tshuffle\tthe\ttraining\tset,\tthen\tgo\tthrough\tit\tinstance\tby\tinstance,\tthen shuffle\tit\tagain,\tand\tso\ton.\tHowever,\tthis\tgenerally\tconverges\tmore\tslowly.\n\nTo\tperform\tLinear\tRegression\tusing\tSGD\twith\tScikit-Learn,\tyou\tcan\tuse\tthe\tSGDRegressor\tclass,\twhich defaults\tto\toptimizing\tthe\tsquared\terror\tcost\tfunction.\tThe\tfollowing\tcode\truns\t50\tepochs,\tstarting\twith\ta learning\trate\tof\t0.1\t(eta0=0.1),\tusing\tthe\tdefault\tlearning\tschedule\t(different\tfrom\tthe\tpreceding\tone), and\tit\tdoes\tnot\tuse\tany\tregularization\t(penalty=None;\tmore\tdetails\ton\tthis\tshortly):\n\nfrom\tsklearn.linear_model\timport\tSGDRegressor sgd_reg\t=\tSGDRegressor(n_iter=50,\tpenalty=None,\teta0=0.1) sgd_reg.fit(X,\ty.ravel())\n\nOnce\tagain,\tyou\tfind\ta\tsolution\tvery\tclose\tto\tthe\tone\treturned\tby\tthe\tNormal\tEquation:\n\n>>>\tsgd_reg.intercept_,\tsgd_reg.coef_ (array([\t4.16782089]),\tarray([\t2.72603052]))",
      "content_length": 1095,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Mini-batch\tGradient\tDescent The\tlast\tGradient\tDescent\talgorithm\twe\twill\tlook\tat\tis\tcalled\tMini-batch\tGradient\tDescent.\tIt\tis\tquite simple\tto\tunderstand\tonce\tyou\tknow\tBatch\tand\tStochastic\tGradient\tDescent:\tat\teach\tstep,\tinstead\tof computing\tthe\tgradients\tbased\ton\tthe\tfull\ttraining\tset\t(as\tin\tBatch\tGD)\tor\tbased\ton\tjust\tone\tinstance\t(as\tin Stochastic\tGD),\tMini-batch\tGD\tcomputes\tthe\tgradients\ton\tsmall\trandom\tsets\tof\tinstances\tcalled\tmini- batches.\tThe\tmain\tadvantage\tof\tMini-batch\tGD\tover\tStochastic\tGD\tis\tthat\tyou\tcan\tget\ta\tperformance boost\tfrom\thardware\toptimization\tof\tmatrix\toperations,\tespecially\twhen\tusing\tGPUs.\n\nThe\talgorithm’s\tprogress\tin\tparameter\tspace\tis\tless\terratic\tthan\twith\tSGD,\tespecially\twith\tfairly\tlarge mini-batches.\tAs\ta\tresult,\tMini-batch\tGD\twill\tend\tup\twalking\taround\ta\tbit\tcloser\tto\tthe\tminimum\tthan SGD.\tBut,\ton\tthe\tother\thand,\tit\tmay\tbe\tharder\tfor\tit\tto\tescape\tfrom\tlocal\tminima\t(in\tthe\tcase\tof\tproblems that\tsuffer\tfrom\tlocal\tminima,\tunlike\tLinear\tRegression\tas\twe\tsaw\tearlier).\tFigure\t4-11\tshows\tthe\tpaths taken\tby\tthe\tthree\tGradient\tDescent\talgorithms\tin\tparameter\tspace\tduring\ttraining.\tThey\tall\tend\tup\tnear the\tminimum,\tbut\tBatch\tGD’s\tpath\tactually\tstops\tat\tthe\tminimum,\twhile\tboth\tStochastic\tGD\tand\tMini- batch\tGD\tcontinue\tto\twalk\taround.\tHowever,\tdon’t\tforget\tthat\tBatch\tGD\ttakes\ta\tlot\tof\ttime\tto\ttake\teach step,\tand\tStochastic\tGD\tand\tMini-batch\tGD\twould\talso\treach\tthe\tminimum\tif\tyou\tused\ta\tgood\tlearning schedule.\n\nFigure\t4-11.\tGradient\tDescent\tpaths\tin\tparameter\tspace\n\nLet’s\tcompare\tthe\talgorithms\twe’ve\tdiscussed\tso\tfar\tfor\tLinear\tRegression8\t(recall\tthat\tm\tis\tthe\tnumber of\ttraining\tinstances\tand\tn\tis\tthe\tnumber\tof\tfeatures);\tsee\tTable\t4-1.\n\nTable\t4-1.\tComparison\tof\talgorithms\tfor\tLinear\tRegression\n\nAlgorithm\n\nLarge\tm Out-of-core\tsupport Large\tn Hyperparams Scaling\trequired Scikit-Learn\n\nNormal\tEquation Fast\n\nNo\n\nSlow\n\n0\n\nNo\n\nLinearRegression\n\nBatch\tGD\n\nSlow\n\nNo\n\nFast\n\n2\n\nYes\n\nn/a",
      "content_length": 1927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "Stochastic\tGD\n\nFast\n\nYes\n\nFast\n\n≥2\n\nYes\n\nSGDRegressor\n\nMini-batch\tGD Fast\n\nYes\n\nFast\n\n≥2\n\nYes\n\nn/a\n\nNOTE\n\nThere\tis\talmost\tno\tdifference\tafter\ttraining:\tall\tthese\talgorithms\tend\tup\twith\tvery\tsimilar\tmodels\tand\tmake\tpredictions\tin\texactly the\tsame\tway.",
      "content_length": 250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Polynomial\tRegression What\tif\tyour\tdata\tis\tactually\tmore\tcomplex\tthan\ta\tsimple\tstraight\tline?\tSurprisingly,\tyou\tcan\tactually\tuse a\tlinear\tmodel\tto\tfit\tnonlinear\tdata.\tA\tsimple\tway\tto\tdo\tthis\tis\tto\tadd\tpowers\tof\teach\tfeature\tas\tnew features,\tthen\ttrain\ta\tlinear\tmodel\ton\tthis\textended\tset\tof\tfeatures.\tThis\ttechnique\tis\tcalled\tPolynomial Regression. Let’s\tlook\tat\tan\texample.\tFirst,\tlet’s\tgenerate\tsome\tnonlinear\tdata,\tbased\ton\ta\tsimple\tquadratic\tequation9 (plus\tsome\tnoise;\tsee\tFigure\t4-12):\n\nm\t=\t100 X\t=\t6\t*\tnp.random.rand(m,\t1)\t-\t3 y\t=\t0.5\t*\tX**2\t+\tX\t+\t2\t+\tnp.random.randn(m,\t1)\n\nFigure\t4-12.\tGenerated\tnonlinear\tand\tnoisy\tdataset\n\nClearly,\ta\tstraight\tline\twill\tnever\tfit\tthis\tdata\tproperly.\tSo\tlet’s\tuse\tScikit-Learn’s\tPolynomialFeatures class\tto\ttransform\tour\ttraining\tdata,\tadding\tthe\tsquare\t(2nd-degree\tpolynomial)\tof\teach\tfeature\tin\tthe training\tset\tas\tnew\tfeatures\t(in\tthis\tcase\tthere\tis\tjust\tone\tfeature):\n\n>>>\tfrom\tsklearn.preprocessing\timport\tPolynomialFeatures >>>\tpoly_features\t=\tPolynomialFeatures(degree=2,\tinclude_bias=False) >>>\tX_poly\t=\tpoly_features.fit_transform(X) >>>\tX[0] array([-0.75275929]) >>>\tX_poly[0] array([-0.75275929,\t\t0.56664654])\n\nX_poly\tnow\tcontains\tthe\toriginal\tfeature\tof\tX\tplus\tthe\tsquare\tof\tthis\tfeature.\tNow\tyou\tcan\tfit\ta LinearRegression\tmodel\tto\tthis\textended\ttraining\tdata\t(Figure\t4-13):",
      "content_length": 1330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": ">>>\tlin_reg\t=\tLinearRegression() >>>\tlin_reg.fit(X_poly,\ty) >>>\tlin_reg.intercept_,\tlin_reg.coef_ (array([\t1.78134581]),\tarray([[\t0.93366893,\t\t0.56456263]]))\n\nFigure\t4-13.\tPolynomial\tRegression\tmodel\tpredictions\n\nNot\tbad:\tthe\tmodel\testimates\n\nwhen\tin\tfact\tthe\toriginal\tfunction\twas\n\n.\n\nNote\tthat\twhen\tthere\tare\tmultiple\tfeatures,\tPolynomial\tRegression\tis\tcapable\tof\tfinding\trelationships between\tfeatures\t(which\tis\tsomething\ta\tplain\tLinear\tRegression\tmodel\tcannot\tdo).\tThis\tis\tmade\tpossible by\tthe\tfact\tthat\tPolynomialFeatures\talso\tadds\tall\tcombinations\tof\tfeatures\tup\tto\tthe\tgiven\tdegree.\tFor example,\tif\tthere\twere\ttwo\tfeatures\ta\tand\tb,\tPolynomialFeatures\twith\tdegree=3\twould\tnot\tonly\tadd the\tfeatures\ta2,\ta3,\tb2,\tand\tb3,\tbut\talso\tthe\tcombinations\tab,\ta2b,\tand\tab2.\n\nWARNING\n\nPolynomialFeatures(degree=d)\ttransforms\tan\tarray\tcontaining\tn\tfeatures\tinto\tan\tarray\tcontaining\t is\tthe\tfactorial\tof\tn,\tequal\tto\t1\t×\t2\t×\t3\t×\t\t×\t n.\tBeware\tof\tthe\tcombinatorial\texplosion\tof\tthe\tnumber\tof\tfeatures!\n\nfeatures,\twhere\tn!",
      "content_length": 1010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Learning\tCurves If\tyou\tperform\thigh-degree\tPolynomial\tRegression,\tyou\twill\tlikely\tfit\tthe\ttraining\tdata\tmuch\tbetter\tthan with\tplain\tLinear\tRegression.\tFor\texample,\tFigure\t4-14\tapplies\ta\t300-degree\tpolynomial\tmodel\tto\tthe preceding\ttraining\tdata,\tand\tcompares\tthe\tresult\twith\ta\tpure\tlinear\tmodel\tand\ta\tquadratic\tmodel\t(2nd- degree\tpolynomial).\tNotice\thow\tthe\t300-degree\tpolynomial\tmodel\twiggles\taround\tto\tget\tas\tclose\tas possible\tto\tthe\ttraining\tinstances.\n\nFigure\t4-14.\tHigh-degree\tPolynomial\tRegression\n\nOf\tcourse,\tthis\thigh-degree\tPolynomial\tRegression\tmodel\tis\tseverely\toverfitting\tthe\ttraining\tdata,\twhile the\tlinear\tmodel\tis\tunderfitting\tit.\tThe\tmodel\tthat\twill\tgeneralize\tbest\tin\tthis\tcase\tis\tthe\tquadratic\tmodel. It\tmakes\tsense\tsince\tthe\tdata\twas\tgenerated\tusing\ta\tquadratic\tmodel,\tbut\tin\tgeneral\tyou\twon’t\tknow\twhat function\tgenerated\tthe\tdata,\tso\thow\tcan\tyou\tdecide\thow\tcomplex\tyour\tmodel\tshould\tbe?\tHow\tcan\tyou\ttell that\tyour\tmodel\tis\toverfitting\tor\tunderfitting\tthe\tdata?\n\nIn\tChapter\t2\tyou\tused\tcross-validation\tto\tget\tan\testimate\tof\ta\tmodel’s\tgeneralization\tperformance.\tIf\ta model\tperforms\twell\ton\tthe\ttraining\tdata\tbut\tgeneralizes\tpoorly\taccording\tto\tthe\tcross-validation\tmetrics, then\tyour\tmodel\tis\toverfitting.\tIf\tit\tperforms\tpoorly\ton\tboth,\tthen\tit\tis\tunderfitting.\tThis\tis\tone\tway\tto\ttell when\ta\tmodel\tis\ttoo\tsimple\tor\ttoo\tcomplex.\n\nAnother\tway\tis\tto\tlook\tat\tthe\tlearning\tcurves:\tthese\tare\tplots\tof\tthe\tmodel’s\tperformance\ton\tthe\ttraining set\tand\tthe\tvalidation\tset\tas\ta\tfunction\tof\tthe\ttraining\tset\tsize.\tTo\tgenerate\tthe\tplots,\tsimply\ttrain\tthe\tmodel several\ttimes\ton\tdifferent\tsized\tsubsets\tof\tthe\ttraining\tset.\tThe\tfollowing\tcode\tdefines\ta\tfunction\tthat\tplots the\tlearning\tcurves\tof\ta\tmodel\tgiven\tsome\ttraining\tdata:\n\nfrom\tsklearn.metrics\timport\tmean_squared_error from\tsklearn.model_selection\timport\ttrain_test_split\n\ndef\tplot_learning_curves(model,\tX,\ty):",
      "content_length": 1878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "X_train,\tX_val,\ty_train,\ty_val\t=\ttrain_test_split(X,\ty,\ttest_size=0.2) \t\t\t\ttrain_errors,\tval_errors\t=\t[],\t[] \t\t\t\tfor\tm\tin\trange(1,\tlen(X_train)): \t\t\t\t\t\t\t\tmodel.fit(X_train[:m],\ty_train[:m]) \t\t\t\t\t\t\t\ty_train_predict\t=\tmodel.predict(X_train[:m]) \t\t\t\t\t\t\t\ty_val_predict\t=\tmodel.predict(X_val) \t\t\t\t\t\t\t\ttrain_errors.append(mean_squared_error(y_train_predict,\ty_train[:m])) \t\t\t\t\t\t\t\tval_errors.append(mean_squared_error(y_val_predict,\ty_val)) \t\t\t\tplt.plot(np.sqrt(train_errors),\t\"r-+\",\tlinewidth=2,\tlabel=\"train\") \t\t\t\tplt.plot(np.sqrt(val_errors),\t\"b-\",\tlinewidth=3,\tlabel=\"val\")\n\nLet’s\tlook\tat\tthe\tlearning\tcurves\tof\tthe\tplain\tLinear\tRegression\tmodel\t(a\tstraight\tline;\tFigure\t4-15):\n\nlin_reg\t=\tLinearRegression() plot_learning_curves(lin_reg,\tX,\ty)\n\nFigure\t4-15.\tLearning\tcurves\n\nThis\tdeserves\ta\tbit\tof\texplanation.\tFirst,\tlet’s\tlook\tat\tthe\tperformance\ton\tthe\ttraining\tdata:\twhen\tthere\tare just\tone\tor\ttwo\tinstances\tin\tthe\ttraining\tset,\tthe\tmodel\tcan\tfit\tthem\tperfectly,\twhich\tis\twhy\tthe\tcurve\tstarts at\tzero.\tBut\tas\tnew\tinstances\tare\tadded\tto\tthe\ttraining\tset,\tit\tbecomes\timpossible\tfor\tthe\tmodel\tto\tfit\tthe training\tdata\tperfectly,\tboth\tbecause\tthe\tdata\tis\tnoisy\tand\tbecause\tit\tis\tnot\tlinear\tat\tall.\tSo\tthe\terror\ton\tthe training\tdata\tgoes\tup\tuntil\tit\treaches\ta\tplateau,\tat\twhich\tpoint\tadding\tnew\tinstances\tto\tthe\ttraining\tset doesn’t\tmake\tthe\taverage\terror\tmuch\tbetter\tor\tworse.\tNow\tlet’s\tlook\tat\tthe\tperformance\tof\tthe\tmodel\ton the\tvalidation\tdata.\tWhen\tthe\tmodel\tis\ttrained\ton\tvery\tfew\ttraining\tinstances,\tit\tis\tincapable\tof generalizing\tproperly,\twhich\tis\twhy\tthe\tvalidation\terror\tis\tinitially\tquite\tbig.\tThen\tas\tthe\tmodel\tis\tshown more\ttraining\texamples,\tit\tlearns\tand\tthus\tthe\tvalidation\terror\tslowly\tgoes\tdown.\tHowever,\tonce\tagain\ta straight\tline\tcannot\tdo\ta\tgood\tjob\tmodeling\tthe\tdata,\tso\tthe\terror\tends\tup\tat\ta\tplateau,\tvery\tclose\tto\tthe other\tcurve.\n\nThese\tlearning\tcurves\tare\ttypical\tof\tan\tunderfitting\tmodel.\tBoth\tcurves\thave\treached\ta\tplateau;\tthey\tare close\tand\tfairly\thigh.",
      "content_length": 1981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "TIP\n\nIf\tyour\tmodel\tis\tunderfitting\tthe\ttraining\tdata,\tadding\tmore\ttraining\texamples\twill\tnot\thelp.\tYou\tneed\tto\tuse\ta\tmore\tcomplex model\tor\tcome\tup\twith\tbetter\tfeatures.\n\nNow\tlet’s\tlook\tat\tthe\tlearning\tcurves\tof\ta\t10th-degree\tpolynomial\tmodel\ton\tthe\tsame\tdata\t(Figure\t4-16):\n\nfrom\tsklearn.pipeline\timport\tPipeline\n\npolynomial_regression\t=\tPipeline(( \t\t\t\t\t\t\t\t(\"poly_features\",\tPolynomialFeatures(degree=10,\tinclude_bias=False)), \t\t\t\t\t\t\t\t(\"lin_reg\",\tLinearRegression()), \t\t\t\t))\n\nplot_learning_curves(polynomial_regression,\tX,\ty)\n\nThese\tlearning\tcurves\tlook\ta\tbit\tlike\tthe\tprevious\tones,\tbut\tthere\tare\ttwo\tvery\timportant\tdifferences:\n\nThe\terror\ton\tthe\ttraining\tdata\tis\tmuch\tlower\tthan\twith\tthe\tLinear\tRegression\tmodel.\n\nThere\tis\ta\tgap\tbetween\tthe\tcurves.\tThis\tmeans\tthat\tthe\tmodel\tperforms\tsignificantly\tbetter\ton\tthe training\tdata\tthan\ton\tthe\tvalidation\tdata,\twhich\tis\tthe\thallmark\tof\tan\toverfitting\tmodel.\tHowever,\tif you\tused\ta\tmuch\tlarger\ttraining\tset,\tthe\ttwo\tcurves\twould\tcontinue\tto\tget\tcloser.\n\nFigure\t4-16.\tLearning\tcurves\tfor\tthe\tpolynomial\tmodel\n\nTIP\n\nOne\tway\tto\timprove\tan\toverfitting\tmodel\tis\tto\tfeed\tit\tmore\ttraining\tdata\tuntil\tthe\tvalidation\terror\treaches\tthe\ttraining\terror.",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "THE\tBIAS/VARIANCE\tTRADEOFF\n\nAn\timportant\ttheoretical\tresult\tof\tstatistics\tand\tMachine\tLearning\tis\tthe\tfact\tthat\ta\tmodel’s\tgeneralization\terror\tcan\tbe\texpressed\tas\tthe sum\tof\tthree\tvery\tdifferent\terrors:\n\nBias\n\nThis\tpart\tof\tthe\tgeneralization\terror\tis\tdue\tto\twrong\tassumptions,\tsuch\tas\tassuming\tthat\tthe\tdata\tis\tlinear\twhen\tit\tis\tactually quadratic.\tA\thigh-bias\tmodel\tis\tmost\tlikely\tto\tunderfit\tthe\ttraining\tdata.10\n\nVariance\n\nThis\tpart\tis\tdue\tto\tthe\tmodel’s\texcessive\tsensitivity\tto\tsmall\tvariations\tin\tthe\ttraining\tdata.\tA\tmodel\twith\tmany\tdegrees\tof\tfreedom (such\tas\ta\thigh-degree\tpolynomial\tmodel)\tis\tlikely\tto\thave\thigh\tvariance,\tand\tthus\tto\toverfit\tthe\ttraining\tdata.\n\nIrreducible\terror\n\nThis\tpart\tis\tdue\tto\tthe\tnoisiness\tof\tthe\tdata\titself.\tThe\tonly\tway\tto\treduce\tthis\tpart\tof\tthe\terror\tis\tto\tclean\tup\tthe\tdata\t(e.g.,\tfix\tthe data\tsources,\tsuch\tas\tbroken\tsensors,\tor\tdetect\tand\tremove\toutliers).\n\nIncreasing\ta\tmodel’s\tcomplexity\twill\ttypically\tincrease\tits\tvariance\tand\treduce\tits\tbias.\tConversely,\treducing\ta\tmodel’s\tcomplexity increases\tits\tbias\tand\treduces\tits\tvariance.\tThis\tis\twhy\tit\tis\tcalled\ta\ttradeoff.",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Regularized\tLinear\tModels As\twe\tsaw\tin\tChapters\t1\tand\t2,\ta\tgood\tway\tto\treduce\toverfitting\tis\tto\tregularize\tthe\tmodel\t(i.e.,\tto constrain\tit):\tthe\tfewer\tdegrees\tof\tfreedom\tit\thas,\tthe\tharder\tit\twill\tbe\tfor\tit\tto\toverfit\tthe\tdata.\tFor example,\ta\tsimple\tway\tto\tregularize\ta\tpolynomial\tmodel\tis\tto\treduce\tthe\tnumber\tof\tpolynomial\tdegrees.\n\nFor\ta\tlinear\tmodel,\tregularization\tis\ttypically\tachieved\tby\tconstraining\tthe\tweights\tof\tthe\tmodel.\tWe\twill now\tlook\tat\tRidge\tRegression,\tLasso\tRegression,\tand\tElastic\tNet,\twhich\timplement\tthree\tdifferent\tways to\tconstrain\tthe\tweights.",
      "content_length": 570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "Ridge\tRegression Ridge\tRegression\t(also\tcalled\tTikhonov\tregularization)\tis\ta\tregularized\tversion\tof\tLinear\tRegression:\ta\n\nregularization\tterm\tequal\tto\t not\tonly\tfit\tthe\tdata\tbut\talso\tkeep\tthe\tmodel\tweights\tas\tsmall\tas\tpossible.\tNote\tthat\tthe\tregularization\tterm should\tonly\tbe\tadded\tto\tthe\tcost\tfunction\tduring\ttraining.\tOnce\tthe\tmodel\tis\ttrained,\tyou\twant\tto\tevaluate the\tmodel’s\tperformance\tusing\tthe\tunregularized\tperformance\tmeasure.\n\nis\tadded\tto\tthe\tcost\tfunction.\tThis\tforces\tthe\tlearning\talgorithm\tto\n\nNOTE\n\nIt\tis\tquite\tcommon\tfor\tthe\tcost\tfunction\tused\tduring\ttraining\tto\tbe\tdifferent\tfrom\tthe\tperformance\tmeasure\tused\tfor\ttesting.\tApart from\tregularization,\tanother\treason\twhy\tthey\tmight\tbe\tdifferent\tis\tthat\ta\tgood\ttraining\tcost\tfunction\tshould\thave\toptimization- friendly\tderivatives,\twhile\tthe\tperformance\tmeasure\tused\tfor\ttesting\tshould\tbe\tas\tclose\tas\tpossible\tto\tthe\tfinal\tobjective.\tA\tgood example\tof\tthis\tis\ta\tclassifier\ttrained\tusing\ta\tcost\tfunction\tsuch\tas\tthe\tlog\tloss\t(discussed\tin\ta\tmoment)\tbut\tevaluated\tusing precision/recall.\n\nThe\thyperparameter\tα\tcontrols\thow\tmuch\tyou\twant\tto\tregularize\tthe\tmodel.\tIf\tα\t=\t0\tthen\tRidge Regression\tis\tjust\tLinear\tRegression.\tIf\tα\tis\tvery\tlarge,\tthen\tall\tweights\tend\tup\tvery\tclose\tto\tzero\tand\tthe result\tis\ta\tflat\tline\tgoing\tthrough\tthe\tdata’s\tmean.\tEquation\t4-8\tpresents\tthe\tRidge\tRegression\tcost function.11\n\nEquation\t4-8.\tRidge\tRegression\tcost\tfunction\n\nNote\tthat\tthe\tbias\tterm\tθ0\tis\tnot\tregularized\t(the\tsum\tstarts\tat\ti\t=\t1,\tnot\t0).\tIf\twe\tdefine\tw\tas\tthe\tvector\tof feature\tweights\t(θ1\tto\tθn),\tthen\tthe\tregularization\tterm\tis\tsimply\tequal\tto\t½(\t w\t2)2,\twhere\t\t·\t represents\tthe\tℓ2\tnorm\tof\tthe\tweight\tvector.12\tFor\tGradient\tDescent,\tjust\tadd\tαw\tto\tthe\tMSE\tgradient vector\t(Equation\t4-6).\n\nWARNING\n\nIt\tis\timportant\tto\tscale\tthe\tdata\t(e.g.,\tusing\ta\tStandardScaler)\tbefore\tperforming\tRidge\tRegression,\tas\tit\tis\tsensitive\tto\tthe\tscale of\tthe\tinput\tfeatures.\tThis\tis\ttrue\tof\tmost\tregularized\tmodels.\n\nFigure\t4-17\tshows\tseveral\tRidge\tmodels\ttrained\ton\tsome\tlinear\tdata\tusing\tdifferent\tα\tvalue.\tOn\tthe\tleft, plain\tRidge\tmodels\tare\tused,\tleading\tto\tlinear\tpredictions.\tOn\tthe\tright,\tthe\tdata\tis\tfirst\texpanded\tusing PolynomialFeatures(degree=10),\tthen\tit\tis\tscaled\tusing\ta\tStandardScaler,\tand\tfinally\tthe\tRidge\n\n2",
      "content_length": 2263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "models\tare\tapplied\tto\tthe\tresulting\tfeatures:\tthis\tis\tPolynomial\tRegression\twith\tRidge\tregularization. Note\thow\tincreasing\tα\tleads\tto\tflatter\t(i.e.,\tless\textreme,\tmore\treasonable)\tpredictions;\tthis\treduces\tthe model’s\tvariance\tbut\tincreases\tits\tbias.\n\nAs\twith\tLinear\tRegression,\twe\tcan\tperform\tRidge\tRegression\teither\tby\tcomputing\ta\tclosed-form equation\tor\tby\tperforming\tGradient\tDescent.\tThe\tpros\tand\tcons\tare\tthe\tsame.\tEquation\t4-9\tshows\tthe closed-form\tsolution\t(where\tA\tis\tthe\tn\t×\tn\tidentity\tmatrix13\texcept\twith\ta\t0\tin\tthe\ttop-left\tcell, corresponding\tto\tthe\tbias\tterm).\n\nFigure\t4-17.\tRidge\tRegression\n\nEquation\t4-9.\tRidge\tRegression\tclosed-form\tsolution\n\nHere\tis\thow\tto\tperform\tRidge\tRegression\twith\tScikit-Learn\tusing\ta\tclosed-form\tsolution\t(a\tvariant\tof Equation\t4-9\tusing\ta\tmatrix\tfactorization\ttechnique\tby\tAndré-Louis\tCholesky):\n\n>>>\tfrom\tsklearn.linear_model\timport\tRidge >>>\tridge_reg\t=\tRidge(alpha=1,\tsolver=\"cholesky\") >>>\tridge_reg.fit(X,\ty) >>>\tridge_reg.predict([[1.5]]) array([[\t1.55071465]])\n\nAnd\tusing\tStochastic\tGradient\tDescent:14\n\n>>>\tsgd_reg\t=\tSGDRegressor(penalty=\"l2\") >>>\tsgd_reg.fit(X,\ty.ravel()) >>>\tsgd_reg.predict([[1.5]]) array([\t1.13500145])\n\nThe\tpenalty\thyperparameter\tsets\tthe\ttype\tof\tregularization\tterm\tto\tuse.\tSpecifying\t\"l2\"\tindicates\tthat you\twant\tSGD\tto\tadd\ta\tregularization\tterm\tto\tthe\tcost\tfunction\tequal\tto\thalf\tthe\tsquare\tof\tthe\tℓ2\tnorm\tof the\tweight\tvector:\tthis\tis\tsimply\tRidge\tRegression.",
      "content_length": 1437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Lasso\tRegression Least\tAbsolute\tShrinkage\tand\tSelection\tOperator\tRegression\t(simply\tcalled\tLasso\tRegression)\tis another\tregularized\tversion\tof\tLinear\tRegression:\tjust\tlike\tRidge\tRegression,\tit\tadds\ta\tregularization\tterm to\tthe\tcost\tfunction,\tbut\tit\tuses\tthe\tℓ1\tnorm\tof\tthe\tweight\tvector\tinstead\tof\thalf\tthe\tsquare\tof\tthe\tℓ2\tnorm (see\tEquation\t4-10).\n\nEquation\t4-10.\tLasso\tRegression\tcost\tfunction\n\nFigure\t4-18\tshows\tthe\tsame\tthing\tas\tFigure\t4-17\tbut\treplaces\tRidge\tmodels\twith\tLasso\tmodels\tand\tuses smaller\tα\tvalues.\n\nFigure\t4-18.\tLasso\tRegression\n\nAn\timportant\tcharacteristic\tof\tLasso\tRegression\tis\tthat\tit\ttends\tto\tcompletely\teliminate\tthe\tweights\tof\tthe least\timportant\tfeatures\t(i.e.,\tset\tthem\tto\tzero).\tFor\texample,\tthe\tdashed\tline\tin\tthe\tright\tplot\ton\tFigure\t4- 18\t(with\tα\t=\t10-7)\tlooks\tquadratic,\talmost\tlinear:\tall\tthe\tweights\tfor\tthe\thigh-degree\tpolynomial\tfeatures are\tequal\tto\tzero.\tIn\tother\twords,\tLasso\tRegression\tautomatically\tperforms\tfeature\tselection\tand\toutputs\ta sparse\tmodel\t(i.e.,\twith\tfew\tnonzero\tfeature\tweights).\n\nYou\tcan\tget\ta\tsense\tof\twhy\tthis\tis\tthe\tcase\tby\tlooking\tat\tFigure\t4-19:\ton\tthe\ttop-left\tplot,\tthe\tbackground contours\t(ellipses)\trepresent\tan\tunregularized\tMSE\tcost\tfunction\t(α\t=\t0),\tand\tthe\twhite\tcircles\tshow\tthe Batch\tGradient\tDescent\tpath\twith\tthat\tcost\tfunction.\tThe\tforeground\tcontours\t(diamonds)\trepresent\tthe\tℓ1 penalty,\tand\tthe\ttriangles\tshow\tthe\tBGD\tpath\tfor\tthis\tpenalty\tonly\t(α\t→\t∞).\tNotice\thow\tthe\tpath\tfirst reaches\tθ1\t=\t0,\tthen\trolls\tdown\ta\tgutter\tuntil\tit\treaches\tθ2\t=\t0.\tOn\tthe\ttop-right\tplot,\tthe\tcontours\trepresent the\tsame\tcost\tfunction\tplus\tan\tℓ1\tpenalty\twith\tα\t=\t0.5.\tThe\tglobal\tminimum\tis\ton\tthe\tθ2\t=\t0\taxis.\tBGD",
      "content_length": 1671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "first\treaches\tθ2\t=\t0,\tthen\trolls\tdown\tthe\tgutter\tuntil\tit\treaches\tthe\tglobal\tminimum.\tThe\ttwo\tbottom\tplots show\tthe\tsame\tthing\tbut\tuses\tan\tℓ2\tpenalty\tinstead.\tThe\tregularized\tminimum\tis\tcloser\tto\tθ\t=\t0\tthan\tthe unregularized\tminimum,\tbut\tthe\tweights\tdo\tnot\tget\tfully\teliminated.\n\nFigure\t4-19.\tLasso\tversus\tRidge\tregularization\n\nTIP\n\nOn\tthe\tLasso\tcost\tfunction,\tthe\tBGD\tpath\ttends\tto\tbounce\tacross\tthe\tgutter\ttoward\tthe\tend.\tThis\tis\tbecause\tthe\tslope\tchanges abruptly\tat\tθ2\t=\t0.\tYou\tneed\tto\tgradually\treduce\tthe\tlearning\trate\tin\torder\tto\tactually\tconverge\tto\tthe\tglobal\tminimum.\n\nThe\tLasso\tcost\tfunction\tis\tnot\tdifferentiable\tat\tθi\t=\t0\t(for\ti\t=\t1,\t2,\t,\t n),\tbut\tGradient\tDescent\tstill\tworks fine\tif\tyou\tuse\ta\tsubgradient\tvector\tg15\tinstead\twhen\tany\tθi\t=\t0.\tEquation\t4-11\tshows\ta\tsubgradient vector\tequation\tyou\tcan\tuse\tfor\tGradient\tDescent\twith\tthe\tLasso\tcost\tfunction.\n\nEquation\t4-11.\tLasso\tRegression\tsubgradient\tvector\n\nHere\tis\ta\tsmall\tScikit-Learn\texample\tusing\tthe\tLasso\tclass.\tNote\tthat\tyou\tcould\tinstead\tuse\tan SGDRegressor(penalty=\"l1\").\n\n>>>\tfrom\tsklearn.linear_model\timport\tLasso >>>\tlasso_reg\t=\tLasso(alpha=0.1) >>>\tlasso_reg.fit(X,\ty) >>>\tlasso_reg.predict([[1.5]]) array([\t1.53788174])",
      "content_length": 1197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Elastic\tNet Elastic\tNet\tis\ta\tmiddle\tground\tbetween\tRidge\tRegression\tand\tLasso\tRegression.\tThe\tregularization\tterm is\ta\tsimple\tmix\tof\tboth\tRidge\tand\tLasso’s\tregularization\tterms,\tand\tyou\tcan\tcontrol\tthe\tmix\tratio\tr.\tWhen r\t=\t0,\tElastic\tNet\tis\tequivalent\tto\tRidge\tRegression,\tand\twhen\tr\t=\t1,\tit\tis\tequivalent\tto\tLasso\tRegression (see\tEquation\t4-12).\n\nEquation\t4-12.\tElastic\tNet\tcost\tfunction\n\nSo\twhen\tshould\tyou\tuse\tplain\tLinear\tRegression\t(i.e.,\twithout\tany\tregularization),\tRidge,\tLasso,\tor Elastic\tNet?\tIt\tis\talmost\talways\tpreferable\tto\thave\tat\tleast\ta\tlittle\tbit\tof\tregularization,\tso\tgenerally\tyou should\tavoid\tplain\tLinear\tRegression.\tRidge\tis\ta\tgood\tdefault,\tbut\tif\tyou\tsuspect\tthat\tonly\ta\tfew\tfeatures are\tactually\tuseful,\tyou\tshould\tprefer\tLasso\tor\tElastic\tNet\tsince\tthey\ttend\tto\treduce\tthe\tuseless\tfeatures’ weights\tdown\tto\tzero\tas\twe\thave\tdiscussed.\tIn\tgeneral,\tElastic\tNet\tis\tpreferred\tover\tLasso\tsince\tLasso may\tbehave\terratically\twhen\tthe\tnumber\tof\tfeatures\tis\tgreater\tthan\tthe\tnumber\tof\ttraining\tinstances\tor when\tseveral\tfeatures\tare\tstrongly\tcorrelated.\n\nHere\tis\ta\tshort\texample\tusing\tScikit-Learn’s\tElasticNet\t(l1_ratio\tcorresponds\tto\tthe\tmix\tratio\tr):\n\n>>>\tfrom\tsklearn.linear_model\timport\tElasticNet >>>\telastic_net\t=\tElasticNet(alpha=0.1,\tl1_ratio=0.5) >>>\telastic_net.fit(X,\ty) >>>\telastic_net.predict([[1.5]]) array([\t1.54333232])",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Early\tStopping A\tvery\tdifferent\tway\tto\tregularize\titerative\tlearning\talgorithms\tsuch\tas\tGradient\tDescent\tis\tto\tstop training\tas\tsoon\tas\tthe\tvalidation\terror\treaches\ta\tminimum.\tThis\tis\tcalled\tearly\tstopping.\tFigure\t4-20 shows\ta\tcomplex\tmodel\t(in\tthis\tcase\ta\thigh-degree\tPolynomial\tRegression\tmodel)\tbeing\ttrained\tusing Batch\tGradient\tDescent.\tAs\tthe\tepochs\tgo\tby,\tthe\talgorithm\tlearns\tand\tits\tprediction\terror\t(RMSE)\ton\tthe training\tset\tnaturally\tgoes\tdown,\tand\tso\tdoes\tits\tprediction\terror\ton\tthe\tvalidation\tset.\tHowever,\tafter\ta while\tthe\tvalidation\terror\tstops\tdecreasing\tand\tactually\tstarts\tto\tgo\tback\tup.\tThis\tindicates\tthat\tthe\tmodel has\tstarted\tto\toverfit\tthe\ttraining\tdata.\tWith\tearly\tstopping\tyou\tjust\tstop\ttraining\tas\tsoon\tas\tthe\tvalidation error\treaches\tthe\tminimum.\tIt\tis\tsuch\ta\tsimple\tand\tefficient\tregularization\ttechnique\tthat\tGeoffrey\tHinton called\tit\ta\t“beautiful\tfree\tlunch.”\n\nFigure\t4-20.\tEarly\tstopping\tregularization\n\nTIP\n\nWith\tStochastic\tand\tMini-batch\tGradient\tDescent,\tthe\tcurves\tare\tnot\tso\tsmooth,\tand\tit\tmay\tbe\thard\tto\tknow\twhether\tyou\thave reached\tthe\tminimum\tor\tnot.\tOne\tsolution\tis\tto\tstop\tonly\tafter\tthe\tvalidation\terror\thas\tbeen\tabove\tthe\tminimum\tfor\tsome\ttime (when\tyou\tare\tconfident\tthat\tthe\tmodel\twill\tnot\tdo\tany\tbetter),\tthen\troll\tback\tthe\tmodel\tparameters\tto\tthe\tpoint\twhere\tthe validation\terror\twas\tat\ta\tminimum.\n\nHere\tis\ta\tbasic\timplementation\tof\tearly\tstopping:\n\nfrom\tsklearn.base\timport\tclone\n\nsgd_reg\t=\tSGDRegressor(n_iter=1,\twarm_start=True,\tpenalty=None, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlearning_rate=\"constant\",\teta0=0.0005)\n\nminimum_val_error\t=\tfloat(\"inf\") best_epoch\t=\tNone",
      "content_length": 1610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "best_model\t=\tNone for\tepoch\tin\trange(1000): \t\t\t\tsgd_reg.fit(X_train_poly_scaled,\ty_train)\t\t#\tcontinues\twhere\tit\tleft\toff \t\t\t\ty_val_predict\t=\tsgd_reg.predict(X_val_poly_scaled) \t\t\t\tval_error\t=\tmean_squared_error(y_val_predict,\ty_val) \t\t\t\tif\tval_error\t<\tminimum_val_error: \t\t\t\t\t\t\t\tminimum_val_error\t=\tval_error \t\t\t\t\t\t\t\tbest_epoch\t=\tepoch \t\t\t\t\t\t\t\tbest_model\t=\tclone(sgd_reg)\n\nNote\tthat\twith\twarm_start=True,\twhen\tthe\tfit()\tmethod\tis\tcalled,\tit\tjust\tcontinues\ttraining\twhere\tit\tleft off\tinstead\tof\trestarting\tfrom\tscratch.",
      "content_length": 518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Logistic\tRegression As\twe\tdiscussed\tin\tChapter\t1,\tsome\tregression\talgorithms\tcan\tbe\tused\tfor\tclassification\tas\twell\t(and vice\tversa).\tLogistic\tRegression\t(also\tcalled\tLogit\tRegression)\tis\tcommonly\tused\tto\testimate\tthe probability\tthat\tan\tinstance\tbelongs\tto\ta\tparticular\tclass\t(e.g.,\twhat\tis\tthe\tprobability\tthat\tthis\temail\tis spam?).\tIf\tthe\testimated\tprobability\tis\tgreater\tthan\t50%,\tthen\tthe\tmodel\tpredicts\tthat\tthe\tinstance\tbelongs to\tthat\tclass\t(called\tthe\tpositive\tclass,\tlabeled\t“1”),\tor\telse\tit\tpredicts\tthat\tit\tdoes\tnot\t(i.e.,\tit\tbelongs\tto the\tnegative\tclass,\tlabeled\t“0”).\tThis\tmakes\tit\ta\tbinary\tclassifier.",
      "content_length": 617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Estimating\tProbabilities So\thow\tdoes\tit\twork?\tJust\tlike\ta\tLinear\tRegression\tmodel,\ta\tLogistic\tRegression\tmodel\tcomputes\ta weighted\tsum\tof\tthe\tinput\tfeatures\t(plus\ta\tbias\tterm),\tbut\tinstead\tof\toutputting\tthe\tresult\tdirectly\tlike\tthe Linear\tRegression\tmodel\tdoes,\tit\toutputs\tthe\tlogistic\tof\tthis\tresult\t(see\tEquation\t4-13).\n\nEquation\t4-13.\tLogistic\tRegression\tmodel\testimated\tprobability\t(vectorized\tform)\n\nThe\tlogistic\t—\talso\tcalled\tthe\tlogit,\tnoted\tσ(·)\t—\tis\ta\tsigmoid\tfunction\t(i.e.,\tS-shaped)\tthat\toutputs\ta number\tbetween\t0\tand\t1.\tIt\tis\tdefined\tas\tshown\tin\tEquation\t4-14\tand\tFigure\t4-21.\n\nEquation\t4-14.\tLogistic\tfunction\n\nFigure\t4-21.\tLogistic\tfunction\n\nOnce\tthe\tLogistic\tRegression\tmodel\thas\testimated\tthe\tprobability\t the\tpositive\tclass,\tit\tcan\tmake\tits\tprediction\tŷ\teasily\t(see\tEquation\t4-15).\n\n=\thθ(x)\tthat\tan\tinstance\tx\tbelongs\tto\n\nEquation\t4-15.\tLogistic\tRegression\tmodel\tprediction\n\nNotice\tthat\tσ(t)\t<\t0.5\twhen\tt\t<\t0,\tand\tσ(t)\t≥\t0.5\twhen\tt\t≥\t0,\tso\ta\tLogistic\tRegression\tmodel\tpredicts\t1\tif",
      "content_length": 1000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "θT\t·\tx\tis\tpositive,\tand\t0\tif\tit\tis\tnegative.",
      "content_length": 44,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Training\tand\tCost\tFunction Good,\tnow\tyou\tknow\thow\ta\tLogistic\tRegression\tmodel\testimates\tprobabilities\tand\tmakes\tpredictions. But\thow\tis\tit\ttrained?\tThe\tobjective\tof\ttraining\tis\tto\tset\tthe\tparameter\tvector\tθ\tso\tthat\tthe\tmodel\testimates high\tprobabilities\tfor\tpositive\tinstances\t(y\t=\t1)\tand\tlow\tprobabilities\tfor\tnegative\tinstances\t(y\t=\t0).\tThis idea\tis\tcaptured\tby\tthe\tcost\tfunction\tshown\tin\tEquation\t4-16\tfor\ta\tsingle\ttraining\tinstance\tx.\n\nEquation\t4-16.\tCost\tfunction\tof\ta\tsingle\ttraining\tinstance\n\nThis\tcost\tfunction\tmakes\tsense\tbecause\t–\tlog(t)\tgrows\tvery\tlarge\twhen\tt\tapproaches\t0,\tso\tthe\tcost\twill be\tlarge\tif\tthe\tmodel\testimates\ta\tprobability\tclose\tto\t0\tfor\ta\tpositive\tinstance,\tand\tit\twill\talso\tbe\tvery large\tif\tthe\tmodel\testimates\ta\tprobability\tclose\tto\t1\tfor\ta\tnegative\tinstance.\tOn\tthe\tother\thand,\t–\tlog(t)\tis close\tto\t0\twhen\tt\tis\tclose\tto\t1,\tso\tthe\tcost\twill\tbe\tclose\tto\t0\tif\tthe\testimated\tprobability\tis\tclose\tto\t0\tfor\ta negative\tinstance\tor\tclose\tto\t1\tfor\ta\tpositive\tinstance,\twhich\tis\tprecisely\twhat\twe\twant.\n\nThe\tcost\tfunction\tover\tthe\twhole\ttraining\tset\tis\tsimply\tthe\taverage\tcost\tover\tall\ttraining\tinstances.\tIt\tcan be\twritten\tin\ta\tsingle\texpression\t(as\tyou\tcan\tverify\teasily),\tcalled\tthe\tlog\tloss,\tshown\tin\tEquation\t4-17.\n\nEquation\t4-17.\tLogistic\tRegression\tcost\tfunction\t(log\tloss)\n\nThe\tbad\tnews\tis\tthat\tthere\tis\tno\tknown\tclosed-form\tequation\tto\tcompute\tthe\tvalue\tof\tθ\tthat\tminimizes\tthis cost\tfunction\t(there\tis\tno\tequivalent\tof\tthe\tNormal\tEquation).\tBut\tthe\tgood\tnews\tis\tthat\tthis\tcost\tfunction is\tconvex,\tso\tGradient\tDescent\t(or\tany\tother\toptimization\talgorithm)\tis\tguaranteed\tto\tfind\tthe\tglobal minimum\t(if\tthe\tlearning\trate\tis\tnot\ttoo\tlarge\tand\tyou\twait\tlong\tenough).\tThe\tpartial\tderivatives\tof\tthe cost\tfunction\twith\tregards\tto\tthe\tjth\tmodel\tparameter\tθj\tis\tgiven\tby\tEquation\t4-18.\n\nEquation\t4-18.\tLogistic\tcost\tfunction\tpartial\tderivatives\n\nThis\tequation\tlooks\tvery\tmuch\tlike\tEquation\t4-5:\tfor\teach\tinstance\tit\tcomputes\tthe\tprediction\terror\tand multiplies\tit\tby\tthe\tjth\tfeature\tvalue,\tand\tthen\tit\tcomputes\tthe\taverage\tover\tall\ttraining\tinstances.\tOnce\tyou have\tthe\tgradient\tvector\tcontaining\tall\tthe\tpartial\tderivatives\tyou\tcan\tuse\tit\tin\tthe\tBatch\tGradient\tDescent algorithm.\tThat’s\tit:\tyou\tnow\tknow\thow\tto\ttrain\ta\tLogistic\tRegression\tmodel.\tFor\tStochastic\tGD\tyou",
      "content_length": 2289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "would\tof\tcourse\tjust\ttake\tone\tinstance\tat\ta\ttime,\tand\tfor\tMini-batch\tGD\tyou\twould\tuse\ta\tmini-batch\tat\ta time.",
      "content_length": 109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Decision\tBoundaries Let’s\tuse\tthe\tiris\tdataset\tto\tillustrate\tLogistic\tRegression.\tThis\tis\ta\tfamous\tdataset\tthat\tcontains\tthe\tsepal and\tpetal\tlength\tand\twidth\tof\t150\tiris\tflowers\tof\tthree\tdifferent\tspecies:\tIris-Setosa,\tIris-Versicolor,\tand Iris-Virginica\t(see\tFigure\t4-22).\n\nFigure\t4-22.\tFlowers\tof\tthree\tiris\tplant\tspecies16\n\nLet’s\ttry\tto\tbuild\ta\tclassifier\tto\tdetect\tthe\tIris-Virginica\ttype\tbased\tonly\ton\tthe\tpetal\twidth\tfeature.\tFirst let’s\tload\tthe\tdata:\n\n>>>\tfrom\tsklearn\timport\tdatasets >>>\tiris\t=\tdatasets.load_iris() >>>\tlist(iris.keys()) ['data',\t'target_names',\t'feature_names',\t'target',\t'DESCR'] >>>\tX\t=\tiris[\"data\"][:,\t3:]\t\t#\tpetal\twidth >>>\ty\t=\t(iris[\"target\"]\t==\t2).astype(np.int)\t\t#\t1\tif\tIris-Virginica,\telse\t0\n\nNow\tlet’s\ttrain\ta\tLogistic\tRegression\tmodel:\n\nfrom\tsklearn.linear_model\timport\tLogisticRegression\n\nlog_reg\t=\tLogisticRegression() log_reg.fit(X,\ty)\n\nLet’s\tlook\tat\tthe\tmodel’s\testimated\tprobabilities\tfor\tflowers\twith\tpetal\twidths\tvarying\tfrom\t0\tto\t3\tcm (Figure\t4-23):\n\nX_new\t=\tnp.linspace(0,\t3,\t1000).reshape(-1,\t1) y_proba\t=\tlog_reg.predict_proba(X_new) plt.plot(X_new,\ty_proba[:,\t1],\t\"g-\",\tlabel=\"Iris-Virginica\") plt.plot(X_new,\ty_proba[:,\t0],\t\"b--\",\tlabel=\"Not\tIris-Virginica\") #\t+\tmore\tMatplotlib\tcode\tto\tmake\tthe\timage\tlook\tpretty",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "Figure\t4-23.\tEstimated\tprobabilities\tand\tdecision\tboundary\n\nThe\tpetal\twidth\tof\tIris-Virginica\tflowers\t(represented\tby\ttriangles)\tranges\tfrom\t1.4\tcm\tto\t2.5\tcm,\twhile the\tother\tiris\tflowers\t(represented\tby\tsquares)\tgenerally\thave\ta\tsmaller\tpetal\twidth,\tranging\tfrom\t0.1\tcm to\t1.8\tcm.\tNotice\tthat\tthere\tis\ta\tbit\tof\toverlap.\tAbove\tabout\t2\tcm\tthe\tclassifier\tis\thighly\tconfident\tthat\tthe flower\tis\tan\tIris-Virginica\t(it\toutputs\ta\thigh\tprobability\tto\tthat\tclass),\twhile\tbelow\t1\tcm\tit\tis\thighly confident\tthat\tit\tis\tnot\tan\tIris-Virginica\t(high\tprobability\tfor\tthe\t“Not\tIris-Virginica”\tclass).\tIn\tbetween these\textremes,\tthe\tclassifier\tis\tunsure.\tHowever,\tif\tyou\task\tit\tto\tpredict\tthe\tclass\t(using\tthe\tpredict() method\trather\tthan\tthe\tpredict_proba()\tmethod),\tit\twill\treturn\twhichever\tclass\tis\tthe\tmost\tlikely. Therefore,\tthere\tis\ta\tdecision\tboundary\tat\taround\t1.6\tcm\twhere\tboth\tprobabilities\tare\tequal\tto\t50%:\tif the\tpetal\twidth\tis\thigher\tthan\t1.6\tcm,\tthe\tclassifier\twill\tpredict\tthat\tthe\tflower\tis\tan\tIris-Virginica,\tor\telse it\twill\tpredict\tthat\tit\tis\tnot\t(even\tif\tit\tis\tnot\tvery\tconfident):\n\n>>>\tlog_reg.predict([[1.7],\t[1.5]]) array([1,\t0])\n\nFigure\t4-24\tshows\tthe\tsame\tdataset\tbut\tthis\ttime\tdisplaying\ttwo\tfeatures:\tpetal\twidth\tand\tlength.\tOnce trained,\tthe\tLogistic\tRegression\tclassifier\tcan\testimate\tthe\tprobability\tthat\ta\tnew\tflower\tis\tan\tIris- Virginica\tbased\ton\tthese\ttwo\tfeatures.\tThe\tdashed\tline\trepresents\tthe\tpoints\twhere\tthe\tmodel\testimates\ta 50%\tprobability:\tthis\tis\tthe\tmodel’s\tdecision\tboundary.\tNote\tthat\tit\tis\ta\tlinear\tboundary.17\tEach\tparallel line\trepresents\tthe\tpoints\twhere\tthe\tmodel\toutputs\ta\tspecific\tprobability,\tfrom\t15%\t(bottom\tleft)\tto\t90% (top\tright).\tAll\tthe\tflowers\tbeyond\tthe\ttop-right\tline\thave\tan\tover\t90%\tchance\tof\tbeing\tIris-Virginica according\tto\tthe\tmodel.\n\nFigure\t4-24.\tLinear\tdecision\tboundary",
      "content_length": 1825,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Just\tlike\tthe\tother\tlinear\tmodels,\tLogistic\tRegression\tmodels\tcan\tbe\tregularized\tusing\tℓ1\tor\tℓ2\tpenalties. Scitkit-Learn\tactually\tadds\tan\tℓ2\tpenalty\tby\tdefault.\n\nNOTE\n\nThe\thyperparameter\tcontrolling\tthe\tregularization\tstrength\tof\ta\tScikit-Learn\tLogisticRegression\tmodel\tis\tnot\talpha\t(as\tin\tother linear\tmodels),\tbut\tits\tinverse:\tC.\tThe\thigher\tthe\tvalue\tof\tC,\tthe\tless\tthe\tmodel\tis\tregularized.",
      "content_length": 393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Softmax\tRegression The\tLogistic\tRegression\tmodel\tcan\tbe\tgeneralized\tto\tsupport\tmultiple\tclasses\tdirectly,\twithout\thaving\tto train\tand\tcombine\tmultiple\tbinary\tclassifiers\t(as\tdiscussed\tin\tChapter\t3).\tThis\tis\tcalled\tSoftmax Regression,\tor\tMultinomial\tLogistic\tRegression.\n\nThe\tidea\tis\tquite\tsimple:\twhen\tgiven\tan\tinstance\tx,\tthe\tSoftmax\tRegression\tmodel\tfirst\tcomputes\ta\tscore sk(x)\tfor\teach\tclass\tk,\tthen\testimates\tthe\tprobability\tof\teach\tclass\tby\tapplying\tthe\tsoftmax\tfunction\t(also called\tthe\tnormalized\texponential)\tto\tthe\tscores.\tThe\tequation\tto\tcompute\tsk(x)\tshould\tlook\tfamiliar,\tas it\tis\tjust\tlike\tthe\tequation\tfor\tLinear\tRegression\tprediction\t(see\tEquation\t4-19).\n\nEquation\t4-19.\tSoftmax\tscore\tfor\tclass\tk\n\nNote\tthat\teach\tclass\thas\tits\town\tdedicated\tparameter\tvector\tθ(k).\tAll\tthese\tvectors\tare\ttypically\tstored\tas rows\tin\ta\tparameter\tmatrix\tΘ.\n\nOnce\tyou\thave\tcomputed\tthe\tscore\tof\tevery\tclass\tfor\tthe\tinstance\tx,\tyou\tcan\testimate\tthe\tprobability\t k that\tthe\tinstance\tbelongs\tto\tclass\tk\tby\trunning\tthe\tscores\tthrough\tthe\tsoftmax\tfunction\t(Equation\t4-20):\tit computes\tthe\texponential\tof\tevery\tscore,\tthen\tnormalizes\tthem\t(dividing\tby\tthe\tsum\tof\tall\tthe exponentials).\n\nEquation\t4-20.\tSoftmax\tfunction\n\nK\tis\tthe\tnumber\tof\tclasses.\n\ns(x)\tis\ta\tvector\tcontaining\tthe\tscores\tof\teach\tclass\tfor\tthe\tinstance\tx.\n\nσ(s(x))k\tis\tthe\testimated\tprobability\tthat\tthe\tinstance\tx\tbelongs\tto\tclass\tk\tgiven\tthe\tscores\tof\teach class\tfor\tthat\tinstance.\n\nJust\tlike\tthe\tLogistic\tRegression\tclassifier,\tthe\tSoftmax\tRegression\tclassifier\tpredicts\tthe\tclass\twith\tthe highest\testimated\tprobability\t(which\tis\tsimply\tthe\tclass\twith\tthe\thighest\tscore),\tas\tshown\tin\tEquation\t4- 21.\n\nEquation\t4-21.\tSoftmax\tRegression\tclassifier\tprediction",
      "content_length": 1713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "The\targmax\toperator\treturns\tthe\tvalue\tof\ta\tvariable\tthat\tmaximizes\ta\tfunction.\tIn\tthis\tequation,\tit returns\tthe\tvalue\tof\tk\tthat\tmaximizes\tthe\testimated\tprobability\tσ(s(x))k.\n\nTIP\n\nThe\tSoftmax\tRegression\tclassifier\tpredicts\tonly\tone\tclass\tat\ta\ttime\t(i.e.,\tit\tis\tmulticlass,\tnot\tmultioutput)\tso\tit\tshould\tbe\tused\tonly with\tmutually\texclusive\tclasses\tsuch\tas\tdifferent\ttypes\tof\tplants.\tYou\tcannot\tuse\tit\tto\trecognize\tmultiple\tpeople\tin\tone\tpicture.\n\nNow\tthat\tyou\tknow\thow\tthe\tmodel\testimates\tprobabilities\tand\tmakes\tpredictions,\tlet’s\ttake\ta\tlook\tat training.\tThe\tobjective\tis\tto\thave\ta\tmodel\tthat\testimates\ta\thigh\tprobability\tfor\tthe\ttarget\tclass\t(and consequently\ta\tlow\tprobability\tfor\tthe\tother\tclasses).\tMinimizing\tthe\tcost\tfunction\tshown\tin\tEquation\t4- 22,\tcalled\tthe\tcross\tentropy,\tshould\tlead\tto\tthis\tobjective\tbecause\tit\tpenalizes\tthe\tmodel\twhen\tit estimates\ta\tlow\tprobability\tfor\ta\ttarget\tclass.\tCross\tentropy\tis\tfrequently\tused\tto\tmeasure\thow\twell\ta\tset of\testimated\tclass\tprobabilities\tmatch\tthe\ttarget\tclasses\t(we\twill\tuse\tit\tagain\tseveral\ttimes\tin\tthe following\tchapters).\n\nEquation\t4-22.\tCross\tentropy\tcost\tfunction\n\nis\tequal\tto\t1\tif\tthe\ttarget\tclass\tfor\tthe\tith\tinstance\tis\tk;\totherwise,\tit\tis\tequal\tto\t0.\n\nNotice\tthat\twhen\tthere\tare\tjust\ttwo\tclasses\t(K\t=\t2),\tthis\tcost\tfunction\tis\tequivalent\tto\tthe\tLogistic Regression’s\tcost\tfunction\t(log\tloss;\tsee\tEquation\t4-17).\n\nCROSS\tENTROPY\n\nCross\tentropy\toriginated\tfrom\tinformation\ttheory.\tSuppose\tyou\twant\tto\tefficiently\ttransmit\tinformation\tabout\tthe\tweather\tevery\tday.\tIf there\tare\teight\toptions\t(sunny,\trainy,\tetc.),\tyou\tcould\tencode\teach\toption\tusing\t3\tbits\tsince\t23\t=\t8.\tHowever,\tif\tyou\tthink\tit\twill\tbe\tsunny almost\tevery\tday,\tit\twould\tbe\tmuch\tmore\tefficient\tto\tcode\t“sunny”\ton\tjust\tone\tbit\t(0)\tand\tthe\tother\tseven\toptions\ton\t4\tbits\t(starting\twith a\t1).\tCross\tentropy\tmeasures\tthe\taverage\tnumber\tof\tbits\tyou\tactually\tsend\tper\toption.\tIf\tyour\tassumption\tabout\tthe\tweather\tis\tperfect, cross\tentropy\twill\tjust\tbe\tequal\tto\tthe\tentropy\tof\tthe\tweather\titself\t(i.e.,\tits\tintrinsic\tunpredictability).\tBut\tif\tyour\tassumptions\tare\twrong (e.g.,\tif\tit\trains\toften),\tcross\tentropy\twill\tbe\tgreater\tby\tan\tamount\tcalled\tthe\tKullback–Leibler\tdivergence.\n\nThe\tcross\tentropy\tbetween\ttwo\tprobability\tdistributions\tp\tand\tq\tis\tdefined\tas\t when\tthe\tdistributions\tare\tdiscrete).\n\n(at\tleast\n\nThe\tgradient\tvector\tof\tthis\tcost\tfunction\twith\tregards\tto\tθ(k)\tis\tgiven\tby\tEquation\t4-23:\n\nEquation\t4-23.\tCross\tentropy\tgradient\tvector\tfor\tclass\tk",
      "content_length": 2476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Now\tyou\tcan\tcompute\tthe\tgradient\tvector\tfor\tevery\tclass,\tthen\tuse\tGradient\tDescent\t(or\tany\tother optimization\talgorithm)\tto\tfind\tthe\tparameter\tmatrix\tΘ\tthat\tminimizes\tthe\tcost\tfunction.\n\nLet’s\tuse\tSoftmax\tRegression\tto\tclassify\tthe\tiris\tflowers\tinto\tall\tthree\tclasses.\tScikit-Learn’s LogisticRegression\tuses\tone-versus-all\tby\tdefault\twhen\tyou\ttrain\tit\ton\tmore\tthan\ttwo\tclasses,\tbut\tyou can\tset\tthe\tmulti_class\thyperparameter\tto\t\"multinomial\"\tto\tswitch\tit\tto\tSoftmax\tRegression\tinstead. You\tmust\talso\tspecify\ta\tsolver\tthat\tsupports\tSoftmax\tRegression,\tsuch\tas\tthe\t\"lbfgs\"\tsolver\t(see\tScikit- Learn’s\tdocumentation\tfor\tmore\tdetails).\tIt\talso\tapplies\tℓ2\tregularization\tby\tdefault,\twhich\tyou\tcan control\tusing\tthe\thyperparameter\tC.\n\nX\t=\tiris[\"data\"][:,\t(2,\t3)]\t\t#\tpetal\tlength,\tpetal\twidth y\t=\tiris[\"target\"]\n\nsoftmax_reg\t=\tLogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\",\tC=10) softmax_reg.fit(X,\ty)\n\nSo\tthe\tnext\ttime\tyou\tfind\tan\tiris\twith\t5\tcm\tlong\tand\t2\tcm\twide\tpetals,\tyou\tcan\task\tyour\tmodel\tto\ttell\tyou what\ttype\tof\tiris\tit\tis,\tand\tit\twill\tanswer\tIris-Virginica\t(class\t2)\twith\t94.2%\tprobability\t(or\tIris- Versicolor\twith\t5.8%\tprobability):\n\n>>>\tsoftmax_reg.predict([[5,\t2]]) array([2]) >>>\tsoftmax_reg.predict_proba([[5,\t2]]) array([[\t\t6.33134078e-07,\t\t\t5.75276067e-02,\t\t\t9.42471760e-01]])\n\nFigure\t4-25\tshows\tthe\tresulting\tdecision\tboundaries,\trepresented\tby\tthe\tbackground\tcolors.\tNotice\tthat the\tdecision\tboundaries\tbetween\tany\ttwo\tclasses\tare\tlinear.\tThe\tfigure\talso\tshows\tthe\tprobabilities\tfor the\tIris-Versicolor\tclass,\trepresented\tby\tthe\tcurved\tlines\t(e.g.,\tthe\tline\tlabeled\twith\t0.450\trepresents\tthe 45%\tprobability\tboundary).\tNotice\tthat\tthe\tmodel\tcan\tpredict\ta\tclass\tthat\thas\tan\testimated\tprobability below\t50%.\tFor\texample,\tat\tthe\tpoint\twhere\tall\tdecision\tboundaries\tmeet,\tall\tclasses\thave\tan\tequal estimated\tprobability\tof\t33%.",
      "content_length": 1851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Figure\t4-25.\tSoftmax\tRegression\tdecision\tboundaries",
      "content_length": 51,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Exercises\n\n1.\t What\tLinear\tRegression\ttraining\talgorithm\tcan\tyou\tuse\tif\tyou\thave\ta\ttraining\tset\twith\tmillions\tof features?\n\n2.\t Suppose\tthe\tfeatures\tin\tyour\ttraining\tset\thave\tvery\tdifferent\tscales.\tWhat\talgorithms\tmight\tsuffer from\tthis,\tand\thow?\tWhat\tcan\tyou\tdo\tabout\tit?\n\n3.\t Can\tGradient\tDescent\tget\tstuck\tin\ta\tlocal\tminimum\twhen\ttraining\ta\tLogistic\tRegression\tmodel?\n\n4.\t Do\tall\tGradient\tDescent\talgorithms\tlead\tto\tthe\tsame\tmodel\tprovided\tyou\tlet\tthem\trun\tlong\tenough?\n\n5.\t Suppose\tyou\tuse\tBatch\tGradient\tDescent\tand\tyou\tplot\tthe\tvalidation\terror\tat\tevery\tepoch.\tIf\tyou notice\tthat\tthe\tvalidation\terror\tconsistently\tgoes\tup,\twhat\tis\tlikely\tgoing\ton?\tHow\tcan\tyou\tfix\tthis?\n\n6.\t Is\tit\ta\tgood\tidea\tto\tstop\tMini-batch\tGradient\tDescent\timmediately\twhen\tthe\tvalidation\terror\tgoes up?\n\n7.\t Which\tGradient\tDescent\talgorithm\t(among\tthose\twe\tdiscussed)\twill\treach\tthe\tvicinity\tof\tthe\toptimal solution\tthe\tfastest?\tWhich\twill\tactually\tconverge?\tHow\tcan\tyou\tmake\tthe\tothers\tconverge\tas\twell?\n\n8.\t Suppose\tyou\tare\tusing\tPolynomial\tRegression.\tYou\tplot\tthe\tlearning\tcurves\tand\tyou\tnotice\tthat\tthere is\ta\tlarge\tgap\tbetween\tthe\ttraining\terror\tand\tthe\tvalidation\terror.\tWhat\tis\thappening?\tWhat\tare\tthree ways\tto\tsolve\tthis?\n\n9.\t Suppose\tyou\tare\tusing\tRidge\tRegression\tand\tyou\tnotice\tthat\tthe\ttraining\terror\tand\tthe\tvalidation error\tare\talmost\tequal\tand\tfairly\thigh.\tWould\tyou\tsay\tthat\tthe\tmodel\tsuffers\tfrom\thigh\tbias\tor\thigh variance?\tShould\tyou\tincrease\tthe\tregularization\thyperparameter\tα\tor\treduce\tit?\n\n10.\t Why\twould\tyou\twant\tto\tuse:\n\nRidge\tRegression\tinstead\tof\tplain\tLinear\tRegression\t(i.e.,\twithout\tany\tregularization)?\n\nLasso\tinstead\tof\tRidge\tRegression?\n\nElastic\tNet\tinstead\tof\tLasso?\n\n11.\t Suppose\tyou\twant\tto\tclassify\tpictures\tas\toutdoor/indoor\tand\tdaytime/nighttime.\tShould\tyou implement\ttwo\tLogistic\tRegression\tclassifiers\tor\tone\tSoftmax\tRegression\tclassifier?\n\n12.\t Implement\tBatch\tGradient\tDescent\twith\tearly\tstopping\tfor\tSoftmax\tRegression\t(without\tusing\tScikit- Learn).\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nIt\tis\toften\tthe\tcase\tthat\ta\tlearning\talgorithm\twill\ttry\tto\toptimize\ta\tdifferent\tfunction\tthan\tthe\tperformance\tmeasure\tused\tto\tevaluate\tthe final\tmodel.\tThis\tis\tgenerally\tbecause\tthat\tfunction\tis\teasier\tto\tcompute,\tbecause\tit\thas\tuseful\tdifferentiation\tproperties\tthat\tthe performance\tmeasure\tlacks,\tor\tbecause\twe\twant\tto\tconstrain\tthe\tmodel\tduring\ttraining,\tas\twe\twill\tsee\twhen\twe\tdiscuss\tregularization.",
      "content_length": 2442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "2\n\n3\n\n4\n\n5\n\n6\n\nth\n\n7\n\n8\n\n9\n\n2\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\nThe\tdemonstration\tthat\tthis\treturns\tthe\tvalue\tof\tθ\tthat\tminimizes\tthe\tcost\tfunction\tis\toutside\tthe\tscope\tof\tthis\tbook.\n\nNote\tthat\tScikit-Learn\tseparates\tthe\tbias\tterm\t(intercept_)\tfrom\tthe\tfeature\tweights\t(coef_).\n\nTechnically\tspeaking,\tits\tderivative\tis\tLipschitz\tcontinuous.\n\nSince\tfeature\t1\tis\tsmaller,\tit\ttakes\ta\tlarger\tchange\tin\tθ1\tto\taffect\tthe\tcost\tfunction,\twhich\tis\twhy\tthe\tbowl\tis\telongated\talong\tthe\tθ1\taxis.\n\nEta\t(η)\tis\tthe\t7\n\nletter\tof\tthe\tGreek\talphabet.\n\nOut-of-core\talgorithms\tare\tdiscussed\tin\tChapter\t1.\n\nWhile\tthe\tNormal\tEquation\tcan\tonly\tperform\tLinear\tRegression,\tthe\tGradient\tDescent\talgorithms\tcan\tbe\tused\tto\ttrain\tmany\tother\tmodels, as\twe\twill\tsee.\n\nA\tquadratic\tequation\tis\tof\tthe\tform\ty\t=\tax\n\n+\tbx\t+\tc.\n\nThis\tnotion\tof\tbias\tis\tnot\tto\tbe\tconfused\twith\tthe\tbias\tterm\tof\tlinear\tmodels.\n\nIt\tis\tcommon\tto\tuse\tthe\tnotation\tJ(θ)\tfor\tcost\tfunctions\tthat\tdon’t\thave\ta\tshort\tname;\twe\twill\toften\tuse\tthis\tnotation\tthroughout\tthe\trest\tof this\tbook.\tThe\tcontext\twill\tmake\tit\tclear\twhich\tcost\tfunction\tis\tbeing\tdiscussed.\n\nNorms\tare\tdiscussed\tin\tChapter\t2.\n\nA\tsquare\tmatrix\tfull\tof\t0s\texcept\tfor\t1s\ton\tthe\tmain\tdiagonal\t(top-left\tto\tbottom-right).\n\nAlternatively\tyou\tcan\tuse\tthe\tRidge\tclass\twith\tthe\t\"sag\"\tsolver.\tStochastic\tAverage\tGD\tis\ta\tvariant\tof\tSGD.\tFor\tmore\tdetails,\tsee\tthe presentation\t“Minimizing\tFinite\tSums\twith\tthe\tStochastic\tAverage\tGradient\tAlgorithm”\tby\tMark\tSchmidt\tet\tal.\tfrom\tthe\tUniversity\tof British\tColumbia.\n\nYou\tcan\tthink\tof\ta\tsubgradient\tvector\tat\ta\tnondifferentiable\tpoint\tas\tan\tintermediate\tvector\tbetween\tthe\tgradient\tvectors\taround\tthat\tpoint.\n\nPhotos\treproduced\tfrom\tthe\tcorresponding\tWikipedia\tpages.\tIris-Virginica\tphoto\tby\tFrank\tMayfield\t(Creative\tCommons\tBY-SA\t2.0),\tIris- Versicolor\tphoto\tby\tD.\tGordon\tE.\tRobertson\t(Creative\tCommons\tBY-SA\t3.0),\tand\tIris-Setosa\tphoto\tis\tpublic\tdomain.\n\nIt\tis\tthe\tthe\tset\tof\tpoints\tx\tsuch\tthat\tθ0\t+\tθ1x1\t+\tθ2x2\t=\t0,\twhich\tdefines\ta\tstraight\tline.",
      "content_length": 1987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "Chapter\t5.\tSupport\tVector\tMachines\n\nA\tSupport\tVector\tMachine\t(SVM)\tis\ta\tvery\tpowerful\tand\tversatile\tMachine\tLearning\tmodel,\tcapable\tof performing\tlinear\tor\tnonlinear\tclassification,\tregression,\tand\teven\toutlier\tdetection.\tIt\tis\tone\tof\tthe\tmost popular\tmodels\tin\tMachine\tLearning,\tand\tanyone\tinterested\tin\tMachine\tLearning\tshould\thave\tit\tin\ttheir toolbox.\tSVMs\tare\tparticularly\twell\tsuited\tfor\tclassification\tof\tcomplex\tbut\tsmall-\tor\tmedium-sized datasets.\n\nThis\tchapter\twill\texplain\tthe\tcore\tconcepts\tof\tSVMs,\thow\tto\tuse\tthem,\tand\thow\tthey\twork.",
      "content_length": 545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Linear\tSVM\tClassification The\tfundamental\tidea\tbehind\tSVMs\tis\tbest\texplained\twith\tsome\tpictures.\tFigure\t5-1\tshows\tpart\tof\tthe iris\tdataset\tthat\twas\tintroduced\tat\tthe\tend\tof\tChapter\t4.\tThe\ttwo\tclasses\tcan\tclearly\tbe\tseparated\teasily with\ta\tstraight\tline\t(they\tare\tlinearly\tseparable).\tThe\tleft\tplot\tshows\tthe\tdecision\tboundaries\tof\tthree possible\tlinear\tclassifiers.\tThe\tmodel\twhose\tdecision\tboundary\tis\trepresented\tby\tthe\tdashed\tline\tis\tso bad\tthat\tit\tdoes\tnot\teven\tseparate\tthe\tclasses\tproperly.\tThe\tother\ttwo\tmodels\twork\tperfectly\ton\tthis training\tset,\tbut\ttheir\tdecision\tboundaries\tcome\tso\tclose\tto\tthe\tinstances\tthat\tthese\tmodels\twill\tprobably not\tperform\tas\twell\ton\tnew\tinstances.\tIn\tcontrast,\tthe\tsolid\tline\tin\tthe\tplot\ton\tthe\tright\trepresents\tthe decision\tboundary\tof\tan\tSVM\tclassifier;\tthis\tline\tnot\tonly\tseparates\tthe\ttwo\tclasses\tbut\talso\tstays\tas\tfar away\tfrom\tthe\tclosest\ttraining\tinstances\tas\tpossible.\tYou\tcan\tthink\tof\tan\tSVM\tclassifier\tas\tfitting\tthe widest\tpossible\tstreet\t(represented\tby\tthe\tparallel\tdashed\tlines)\tbetween\tthe\tclasses.\tThis\tis\tcalled\tlarge margin\tclassification.\n\nFigure\t5-1.\tLarge\tmargin\tclassification\n\nNotice\tthat\tadding\tmore\ttraining\tinstances\t“off\tthe\tstreet”\twill\tnot\taffect\tthe\tdecision\tboundary\tat\tall:\tit\tis fully\tdetermined\t(or\t“supported”)\tby\tthe\tinstances\tlocated\ton\tthe\tedge\tof\tthe\tstreet.\tThese\tinstances\tare called\tthe\tsupport\tvectors\t(they\tare\tcircled\tin\tFigure\t5-1).\n\nWARNING\n\nSVMs\tare\tsensitive\tto\tthe\tfeature\tscales,\tas\tyou\tcan\tsee\tin\tFigure\t5-2:\ton\tthe\tleft\tplot,\tthe\tvertical\tscale\tis\tmuch\tlarger\tthan\tthe horizontal\tscale,\tso\tthe\twidest\tpossible\tstreet\tis\tclose\tto\thorizontal.\tAfter\tfeature\tscaling\t(e.g.,\tusing\tScikit-Learn’s StandardScaler),\tthe\tdecision\tboundary\tlooks\tmuch\tbetter\t(on\tthe\tright\tplot).\n\nFigure\t5-2.\tSensitivity\tto\tfeature\tscales",
      "content_length": 1802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Soft\tMargin\tClassification If\twe\tstrictly\timpose\tthat\tall\tinstances\tbe\toff\tthe\tstreet\tand\ton\tthe\tright\tside,\tthis\tis\tcalled\thard\tmargin classification.\tThere\tare\ttwo\tmain\tissues\twith\thard\tmargin\tclassification.\tFirst,\tit\tonly\tworks\tif\tthe\tdata is\tlinearly\tseparable,\tand\tsecond\tit\tis\tquite\tsensitive\tto\toutliers.\tFigure\t5-3\tshows\tthe\tiris\tdataset\twith just\tone\tadditional\toutlier:\ton\tthe\tleft,\tit\tis\timpossible\tto\tfind\ta\thard\tmargin,\tand\ton\tthe\tright\tthe\tdecision boundary\tends\tup\tvery\tdifferent\tfrom\tthe\tone\twe\tsaw\tin\tFigure\t5-1\twithout\tthe\toutlier,\tand\tit\twill probably\tnot\tgeneralize\tas\twell.\n\nFigure\t5-3.\tHard\tmargin\tsensitivity\tto\toutliers\n\nTo\tavoid\tthese\tissues\tit\tis\tpreferable\tto\tuse\ta\tmore\tflexible\tmodel.\tThe\tobjective\tis\tto\tfind\ta\tgood balance\tbetween\tkeeping\tthe\tstreet\tas\tlarge\tas\tpossible\tand\tlimiting\tthe\tmargin\tviolations\t(i.e.,\tinstances that\tend\tup\tin\tthe\tmiddle\tof\tthe\tstreet\tor\teven\ton\tthe\twrong\tside).\tThis\tis\tcalled\tsoft\tmargin classification.\n\nIn\tScikit-Learn’s\tSVM\tclasses,\tyou\tcan\tcontrol\tthis\tbalance\tusing\tthe\tC\thyperparameter:\ta\tsmaller\tC\tvalue leads\tto\ta\twider\tstreet\tbut\tmore\tmargin\tviolations.\tFigure\t5-4\tshows\tthe\tdecision\tboundaries\tand\tmargins of\ttwo\tsoft\tmargin\tSVM\tclassifiers\ton\ta\tnonlinearly\tseparable\tdataset.\tOn\tthe\tleft,\tusing\ta\thigh\tC\tvalue the\tclassifier\tmakes\tfewer\tmargin\tviolations\tbut\tends\tup\twith\ta\tsmaller\tmargin.\tOn\tthe\tright,\tusing\ta\tlow C\tvalue\tthe\tmargin\tis\tmuch\tlarger,\tbut\tmany\tinstances\tend\tup\ton\tthe\tstreet.\tHowever,\tit\tseems\tlikely\tthat the\tsecond\tclassifier\twill\tgeneralize\tbetter:\tin\tfact\teven\ton\tthis\ttraining\tset\tit\tmakes\tfewer\tprediction errors,\tsince\tmost\tof\tthe\tmargin\tviolations\tare\tactually\ton\tthe\tcorrect\tside\tof\tthe\tdecision\tboundary.\n\nFigure\t5-4.\tFewer\tmargin\tviolations\tversus\tlarge\tmargin\n\nTIP\n\nIf\tyour\tSVM\tmodel\tis\toverfitting,\tyou\tcan\ttry\tregularizing\tit\tby\treducing\tC.",
      "content_length": 1843,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "The\tfollowing\tScikit-Learn\tcode\tloads\tthe\tiris\tdataset,\tscales\tthe\tfeatures,\tand\tthen\ttrains\ta\tlinear\tSVM model\t(using\tthe\tLinearSVC\tclass\twith\tC\t=\t0.1\tand\tthe\thinge\tloss\tfunction,\tdescribed\tshortly)\tto\tdetect Iris-Virginica\tflowers.\tThe\tresulting\tmodel\tis\trepresented\ton\tthe\tright\tof\tFigure\t5-4.\n\nimport\tnumpy\tas\tnp from\tsklearn\timport\tdatasets from\tsklearn.pipeline\timport\tPipeline from\tsklearn.preprocessing\timport\tStandardScaler from\tsklearn.svm\timport\tLinearSVC\n\niris\t=\tdatasets.load_iris() X\t=\tiris[\"data\"][:,\t(2,\t3)]\t\t#\tpetal\tlength,\tpetal\twidth y\t=\t(iris[\"target\"]\t==\t2).astype(np.float64)\t\t#\tIris-Virginica\n\nsvm_clf\t=\tPipeline(( \t\t\t\t\t\t\t\t(\"scaler\",\tStandardScaler()), \t\t\t\t\t\t\t\t(\"linear_svc\",\tLinearSVC(C=1,\tloss=\"hinge\")), \t\t\t\t))\n\nsvm_clf.fit(X,\ty)\n\nThen,\tas\tusual,\tyou\tcan\tuse\tthe\tmodel\tto\tmake\tpredictions:\n\n>>>\tsvm_clf.predict([[5.5,\t1.7]]) array([\t1.])\n\nNOTE\n\nUnlike\tLogistic\tRegression\tclassifiers,\tSVM\tclassifiers\tdo\tnot\toutput\tprobabilities\tfor\teach\tclass.\n\nAlternatively,\tyou\tcould\tuse\tthe\tSVC\tclass,\tusing\tSVC(kernel=\"linear\",\tC=1),\tbut\tit\tis\tmuch\tslower, especially\twith\tlarge\ttraining\tsets,\tso\tit\tis\tnot\trecommended.\tAnother\toption\tis\tto\tuse\tthe\tSGDClassifier class,\twith\tSGDClassifier(loss=\"hinge\",\talpha=1/(m*C)).\tThis\tapplies\tregular\tStochastic Gradient\tDescent\t(see\tChapter\t4)\tto\ttrain\ta\tlinear\tSVM\tclassifier.\tIt\tdoes\tnot\tconverge\tas\tfast\tas\tthe LinearSVC\tclass,\tbut\tit\tcan\tbe\tuseful\tto\thandle\thuge\tdatasets\tthat\tdo\tnot\tfit\tin\tmemory\t(out-of-core training),\tor\tto\thandle\tonline\tclassification\ttasks.\n\nTIP\n\nThe\tLinearSVC\tclass\tregularizes\tthe\tbias\tterm,\tso\tyou\tshould\tcenter\tthe\ttraining\tset\tfirst\tby\tsubtracting\tits\tmean.\tThis\tis automatic\tif\tyou\tscale\tthe\tdata\tusing\tthe\tStandardScaler.\tMoreover,\tmake\tsure\tyou\tset\tthe\tloss\thyperparameter\tto\t\"hinge\",\tas it\tis\tnot\tthe\tdefault\tvalue.\tFinally,\tfor\tbetter\tperformance\tyou\tshould\tset\tthe\tdual\thyperparameter\tto\tFalse,\tunless\tthere\tare\tmore features\tthan\ttraining\tinstances\t(we\twill\tdiscuss\tduality\tlater\tin\tthe\tchapter).",
      "content_length": 1990,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "Nonlinear\tSVM\tClassification Although\tlinear\tSVM\tclassifiers\tare\tefficient\tand\twork\tsurprisingly\twell\tin\tmany\tcases,\tmany\tdatasets are\tnot\teven\tclose\tto\tbeing\tlinearly\tseparable.\tOne\tapproach\tto\thandling\tnonlinear\tdatasets\tis\tto\tadd\tmore features,\tsuch\tas\tpolynomial\tfeatures\t(as\tyou\tdid\tin\tChapter\t4);\tin\tsome\tcases\tthis\tcan\tresult\tin\ta\tlinearly separable\tdataset.\tConsider\tthe\tleft\tplot\tin\tFigure\t5-5:\tit\trepresents\ta\tsimple\tdataset\twith\tjust\tone\tfeature x1.\tThis\tdataset\tis\tnot\tlinearly\tseparable,\tas\tyou\tcan\tsee.\tBut\tif\tyou\tadd\ta\tsecond\tfeature\tx2\t=\t(x1)2,\tthe resulting\t2D\tdataset\tis\tperfectly\tlinearly\tseparable.\n\nFigure\t5-5.\tAdding\tfeatures\tto\tmake\ta\tdataset\tlinearly\tseparable\n\nTo\timplement\tthis\tidea\tusing\tScikit-Learn,\tyou\tcan\tcreate\ta\tPipeline\tcontaining\ta\tPolynomialFeatures transformer\t(discussed\tin\t“Polynomial\tRegression”),\tfollowed\tby\ta\tStandardScaler\tand\ta\tLinearSVC. Let’s\ttest\tthis\ton\tthe\tmoons\tdataset\t(see\tFigure\t5-6):\n\nfrom\tsklearn.datasets\timport\tmake_moons from\tsklearn.pipeline\timport\tPipeline from\tsklearn.preprocessing\timport\tPolynomialFeatures\n\npolynomial_svm_clf\t=\tPipeline(( \t\t\t\t\t\t\t\t(\"poly_features\",\tPolynomialFeatures(degree=3)), \t\t\t\t\t\t\t\t(\"scaler\",\tStandardScaler()), \t\t\t\t\t\t\t\t(\"svm_clf\",\tLinearSVC(C=10,\tloss=\"hinge\")) \t\t\t\t))\n\npolynomial_svm_clf.fit(X,\ty)",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Figure\t5-6.\tLinear\tSVM\tclassifier\tusing\tpolynomial\tfeatures",
      "content_length": 59,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "Polynomial\tKernel Adding\tpolynomial\tfeatures\tis\tsimple\tto\timplement\tand\tcan\twork\tgreat\twith\tall\tsorts\tof\tMachine\tLearning algorithms\t(not\tjust\tSVMs),\tbut\tat\ta\tlow\tpolynomial\tdegree\tit\tcannot\tdeal\twith\tvery\tcomplex\tdatasets, and\twith\ta\thigh\tpolynomial\tdegree\tit\tcreates\ta\thuge\tnumber\tof\tfeatures,\tmaking\tthe\tmodel\ttoo\tslow.\n\nFortunately,\twhen\tusing\tSVMs\tyou\tcan\tapply\tan\talmost\tmiraculous\tmathematical\ttechnique\tcalled\tthe kernel\ttrick\t(it\tis\texplained\tin\ta\tmoment).\tIt\tmakes\tit\tpossible\tto\tget\tthe\tsame\tresult\tas\tif\tyou\tadded\tmany polynomial\tfeatures,\teven\twith\tvery\thigh-degree\tpolynomials,\twithout\tactually\thaving\tto\tadd\tthem.\tSo there\tis\tno\tcombinatorial\texplosion\tof\tthe\tnumber\tof\tfeatures\tsince\tyou\tdon’t\tactually\tadd\tany\tfeatures. This\ttrick\tis\timplemented\tby\tthe\tSVC\tclass.\tLet’s\ttest\tit\ton\tthe\tmoons\tdataset:\n\nfrom\tsklearn.svm\timport\tSVC poly_kernel_svm_clf\t=\tPipeline(( \t\t\t\t\t\t\t\t(\"scaler\",\tStandardScaler()), \t\t\t\t\t\t\t\t(\"svm_clf\",\tSVC(kernel=\"poly\",\tdegree=3,\tcoef0=1,\tC=5)) \t\t\t\t)) poly_kernel_svm_clf.fit(X,\ty)\n\nThis\tcode\ttrains\tan\tSVM\tclassifier\tusing\ta\t3rd-degree\tpolynomial\tkernel.\tIt\tis\trepresented\ton\tthe\tleft\tof Figure\t5-7.\tOn\tthe\tright\tis\tanother\tSVM\tclassifier\tusing\ta\t10th-degree\tpolynomial\tkernel.\tObviously,\tif your\tmodel\tis\toverfitting,\tyou\tmight\twant\tto\treduce\tthe\tpolynomial\tdegree.\tConversely,\tif\tit\tis underfitting,\tyou\tcan\ttry\tincreasing\tit.\tThe\thyperparameter\tcoef0\tcontrols\thow\tmuch\tthe\tmodel\tis influenced\tby\thigh-degree\tpolynomials\tversus\tlow-degree\tpolynomials.\n\nFigure\t5-7.\tSVM\tclassifiers\twith\ta\tpolynomial\tkernel\n\nTIP\n\nA\tcommon\tapproach\tto\tfind\tthe\tright\thyperparameter\tvalues\tis\tto\tuse\tgrid\tsearch\t(see\tChapter\t2).\tIt\tis\toften\tfaster\tto\tfirst\tdo\ta very\tcoarse\tgrid\tsearch,\tthen\ta\tfiner\tgrid\tsearch\taround\tthe\tbest\tvalues\tfound.\tHaving\ta\tgood\tsense\tof\twhat\teach hyperparameter\tactually\tdoes\tcan\talso\thelp\tyou\tsearch\tin\tthe\tright\tpart\tof\tthe\thyperparameter\tspace.",
      "content_length": 1894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "Adding\tSimilarity\tFeatures Another\ttechnique\tto\ttackle\tnonlinear\tproblems\tis\tto\tadd\tfeatures\tcomputed\tusing\ta\tsimilarity\tfunction that\tmeasures\thow\tmuch\teach\tinstance\tresembles\ta\tparticular\tlandmark.\tFor\texample,\tlet’s\ttake\tthe\tone- dimensional\tdataset\tdiscussed\tearlier\tand\tadd\ttwo\tlandmarks\tto\tit\tat\tx1\t=\t–2\tand\tx1\t=\t1\t(see\tthe\tleft\tplot in\tFigure\t5-8).\tNext,\tlet’s\tdefine\tthe\tsimilarity\tfunction\tto\tbe\tthe\tGaussian\tRadial\tBasis\tFunction\t(RBF) with\tγ\t=\t0.3\t(see\tEquation\t5-1).\n\nEquation\t5-1.\tGaussian\tRBF\n\nIt\tis\ta\tbell-shaped\tfunction\tvarying\tfrom\t0\t(very\tfar\taway\tfrom\tthe\tlandmark)\tto\t1\t(at\tthe\tlandmark).\tNow we\tare\tready\tto\tcompute\tthe\tnew\tfeatures.\tFor\texample,\tlet’s\tlook\tat\tthe\tinstance\tx1\t=\t–1:\tit\tis\tlocated\tat\ta distance\tof\t1\tfrom\tthe\tfirst\tlandmark,\tand\t2\tfrom\tthe\tsecond\tlandmark.\tTherefore\tits\tnew\tfeatures\tare\tx2\t= exp\t(–0.3\t×\t12)\t≈\t0.74\tand\tx3\t=\texp\t(–0.3\t×\t22)\t≈\t0.30.\tThe\tplot\ton\tthe\tright\tof\tFigure\t5-8\tshows\tthe transformed\tdataset\t(dropping\tthe\toriginal\tfeatures).\tAs\tyou\tcan\tsee,\tit\tis\tnow\tlinearly\tseparable.\n\nFigure\t5-8.\tSimilarity\tfeatures\tusing\tthe\tGaussian\tRBF\n\nYou\tmay\twonder\thow\tto\tselect\tthe\tlandmarks.\tThe\tsimplest\tapproach\tis\tto\tcreate\ta\tlandmark\tat\tthe location\tof\teach\tand\tevery\tinstance\tin\tthe\tdataset.\tThis\tcreates\tmany\tdimensions\tand\tthus\tincreases\tthe chances\tthat\tthe\ttransformed\ttraining\tset\twill\tbe\tlinearly\tseparable.\tThe\tdownside\tis\tthat\ta\ttraining\tset with\tm\tinstances\tand\tn\tfeatures\tgets\ttransformed\tinto\ta\ttraining\tset\twith\tm\tinstances\tand\tm\tfeatures (assuming\tyou\tdrop\tthe\toriginal\tfeatures).\tIf\tyour\ttraining\tset\tis\tvery\tlarge,\tyou\tend\tup\twith\tan\tequally large\tnumber\tof\tfeatures.",
      "content_length": 1629,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Gaussian\tRBF\tKernel Just\tlike\tthe\tpolynomial\tfeatures\tmethod,\tthe\tsimilarity\tfeatures\tmethod\tcan\tbe\tuseful\twith\tany\tMachine Learning\talgorithm,\tbut\tit\tmay\tbe\tcomputationally\texpensive\tto\tcompute\tall\tthe\tadditional\tfeatures, especially\ton\tlarge\ttraining\tsets.\tHowever,\tonce\tagain\tthe\tkernel\ttrick\tdoes\tits\tSVM\tmagic:\tit\tmakes\tit possible\tto\tobtain\ta\tsimilar\tresult\tas\tif\tyou\thad\tadded\tmany\tsimilarity\tfeatures,\twithout\tactually\thaving\tto add\tthem.\tLet’s\ttry\tthe\tGaussian\tRBF\tkernel\tusing\tthe\tSVC\tclass:\n\nrbf_kernel_svm_clf\t=\tPipeline(( \t\t\t\t\t\t\t\t(\"scaler\",\tStandardScaler()), \t\t\t\t\t\t\t\t(\"svm_clf\",\tSVC(kernel=\"rbf\",\tgamma=5,\tC=0.001)) \t\t\t\t)) rbf_kernel_svm_clf.fit(X,\ty)\n\nThis\tmodel\tis\trepresented\ton\tthe\tbottom\tleft\tof\tFigure\t5-9.\tThe\tother\tplots\tshow\tmodels\ttrained\twith different\tvalues\tof\thyperparameters\tgamma\t(γ)\tand\tC.\tIncreasing\tgamma\tmakes\tthe\tbell-shape\tcurve narrower\t(see\tthe\tleft\tplot\tof\tFigure\t5-8),\tand\tas\ta\tresult\teach\tinstance’s\trange\tof\tinfluence\tis\tsmaller:\tthe decision\tboundary\tends\tup\tbeing\tmore\tirregular,\twiggling\taround\tindividual\tinstances.\tConversely,\ta small\tgamma\tvalue\tmakes\tthe\tbell-shaped\tcurve\twider,\tso\tinstances\thave\ta\tlarger\trange\tof\tinfluence,\tand the\tdecision\tboundary\tends\tup\tsmoother.\tSo\tγ\tacts\tlike\ta\tregularization\thyperparameter:\tif\tyour\tmodel\tis overfitting,\tyou\tshould\treduce\tit,\tand\tif\tit\tis\tunderfitting,\tyou\tshould\tincrease\tit\t(similar\tto\tthe\tC hyperparameter).\n\nFigure\t5-9.\tSVM\tclassifiers\tusing\tan\tRBF\tkernel\n\nOther\tkernels\texist\tbut\tare\tused\tmuch\tmore\trarely.\tFor\texample,\tsome\tkernels\tare\tspecialized\tfor\tspecific data\tstructures.\tString\tkernels\tare\tsometimes\tused\twhen\tclassifying\ttext\tdocuments\tor\tDNA\tsequences (e.g.,\tusing\tthe\tstring\tsubsequence\tkernel\tor\tkernels\tbased\ton\tthe\tLevenshtein\tdistance).",
      "content_length": 1750,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "TIP\n\nWith\tso\tmany\tkernels\tto\tchoose\tfrom,\thow\tcan\tyou\tdecide\twhich\tone\tto\tuse?\tAs\ta\trule\tof\tthumb,\tyou\tshould\talways\ttry\tthe\tlinear kernel\tfirst\t(remember\tthat\tLinearSVC\tis\tmuch\tfaster\tthan\tSVC(kernel=\"linear\")),\tespecially\tif\tthe\ttraining\tset\tis\tvery\tlarge\tor if\tit\thas\tplenty\tof\tfeatures.\tIf\tthe\ttraining\tset\tis\tnot\ttoo\tlarge,\tyou\tshould\ttry\tthe\tGaussian\tRBF\tkernel\tas\twell;\tit\tworks\twell\tin most\tcases.\tThen\tif\tyou\thave\tspare\ttime\tand\tcomputing\tpower,\tyou\tcan\talso\texperiment\twith\ta\tfew\tother\tkernels\tusing\tcross- validation\tand\tgrid\tsearch,\tespecially\tif\tthere\tare\tkernels\tspecialized\tfor\tyour\ttraining\tset’s\tdata\tstructure.",
      "content_length": 628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Computational\tComplexity The\tLinearSVC\tclass\tis\tbased\ton\tthe\tliblinear\tlibrary,\twhich\timplements\tan\toptimized\talgorithm\tfor linear\tSVMs.1\tIt\tdoes\tnot\tsupport\tthe\tkernel\ttrick,\tbut\tit\tscales\talmost\tlinearly\twith\tthe\tnumber\tof\ttraining instances\tand\tthe\tnumber\tof\tfeatures:\tits\ttraining\ttime\tcomplexity\tis\troughly\tO(m\t×\tn).\n\nThe\talgorithm\ttakes\tlonger\tif\tyou\trequire\ta\tvery\thigh\tprecision.\tThis\tis\tcontrolled\tby\tthe\ttolerance hyperparameter\tϵ\t(called\ttol\tin\tScikit-Learn).\tIn\tmost\tclassification\ttasks,\tthe\tdefault\ttolerance\tis\tfine.\n\nThe\tSVC\tclass\tis\tbased\ton\tthe\tlibsvm\tlibrary,\twhich\timplements\tan\talgorithm\tthat\tsupports\tthe\tkernel trick.2\tThe\ttraining\ttime\tcomplexity\tis\tusually\tbetween\tO(m2\t×\tn)\tand\tO(m3\t×\tn).\tUnfortunately,\tthis means\tthat\tit\tgets\tdreadfully\tslow\twhen\tthe\tnumber\tof\ttraining\tinstances\tgets\tlarge\t(e.g.,\thundreds\tof thousands\tof\tinstances).\tThis\talgorithm\tis\tperfect\tfor\tcomplex\tbut\tsmall\tor\tmedium\ttraining\tsets. However,\tit\tscales\twell\twith\tthe\tnumber\tof\tfeatures,\tespecially\twith\tsparse\tfeatures\t(i.e.,\twhen\teach instance\thas\tfew\tnonzero\tfeatures).\tIn\tthis\tcase,\tthe\talgorithm\tscales\troughly\twith\tthe\taverage\tnumber\tof nonzero\tfeatures\tper\tinstance.\tTable\t5-1\tcompares\tScikit-Learn’s\tSVM\tclassification\tclasses.\n\nTable\t5-1.\tComparison\tof\tScikit-Learn\tclasses\tfor\tSVM\tclassification\n\nClass\n\nTime\tcomplexity\n\nOut-of-core\tsupport Scaling\trequired Kernel\ttrick\n\nLinearSVC\n\nO(m\t×\tn)\n\nNo\n\nYes\n\nNo\n\nSGDClassifier O(m\t×\tn)\n\nYes\n\nYes\n\nNo\n\nSVC\n\nO(m²\t×\tn)\tto\tO(m³\t×\tn) No\n\nYes\n\nYes",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "SVM\tRegression As\twe\tmentioned\tearlier,\tthe\tSVM\talgorithm\tis\tquite\tversatile:\tnot\tonly\tdoes\tit\tsupport\tlinear\tand nonlinear\tclassification,\tbut\tit\talso\tsupports\tlinear\tand\tnonlinear\tregression.\tThe\ttrick\tis\tto\treverse\tthe objective:\tinstead\tof\ttrying\tto\tfit\tthe\tlargest\tpossible\tstreet\tbetween\ttwo\tclasses\twhile\tlimiting\tmargin violations,\tSVM\tRegression\ttries\tto\tfit\tas\tmany\tinstances\tas\tpossible\ton\tthe\tstreet\twhile\tlimiting\tmargin violations\t(i.e.,\tinstances\toff\tthe\tstreet).\tThe\twidth\tof\tthe\tstreet\tis\tcontrolled\tby\ta\thyperparameter\tϵ. Figure\t5-10\tshows\ttwo\tlinear\tSVM\tRegression\tmodels\ttrained\ton\tsome\trandom\tlinear\tdata,\tone\twith\ta large\tmargin\t(ϵ\t=\t1.5)\tand\tthe\tother\twith\ta\tsmall\tmargin\t(ϵ\t=\t0.5).\n\nFigure\t5-10.\tSVM\tRegression\n\nAdding\tmore\ttraining\tinstances\twithin\tthe\tmargin\tdoes\tnot\taffect\tthe\tmodel’s\tpredictions;\tthus,\tthe\tmodel is\tsaid\tto\tbe\tϵ-insensitive.\n\nYou\tcan\tuse\tScikit-Learn’s\tLinearSVR\tclass\tto\tperform\tlinear\tSVM\tRegression.\tThe\tfollowing\tcode produces\tthe\tmodel\trepresented\ton\tthe\tleft\tof\tFigure\t5-10\t(the\ttraining\tdata\tshould\tbe\tscaled\tand\tcentered first):\n\nfrom\tsklearn.svm\timport\tLinearSVR\n\nsvm_reg\t=\tLinearSVR(epsilon=1.5) svm_reg.fit(X,\ty)\n\nTo\ttackle\tnonlinear\tregression\ttasks,\tyou\tcan\tuse\ta\tkernelized\tSVM\tmodel.\tFor\texample,\tFigure\t5-11 shows\tSVM\tRegression\ton\ta\trandom\tquadratic\ttraining\tset,\tusing\ta\t2nd-degree\tpolynomial\tkernel.\tThere is\tlittle\tregularization\ton\tthe\tleft\tplot\t(i.e.,\ta\tlarge\tC\tvalue),\tand\tmuch\tmore\tregularization\ton\tthe\tright\tplot (i.e.,\ta\tsmall\tC\tvalue).",
      "content_length": 1509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "Figure\t5-11.\tSVM\tregression\tusing\ta\t2nd-degree\tpolynomial\tkernel\n\nThe\tfollowing\tcode\tproduces\tthe\tmodel\trepresented\ton\tthe\tleft\tof\tFigure\t5-11\tusing\tScikit-Learn’s\tSVR class\t(which\tsupports\tthe\tkernel\ttrick).\tThe\tSVR\tclass\tis\tthe\tregression\tequivalent\tof\tthe\tSVC\tclass,\tand the\tLinearSVR\tclass\tis\tthe\tregression\tequivalent\tof\tthe\tLinearSVC\tclass.\tThe\tLinearSVR\tclass\tscales linearly\twith\tthe\tsize\tof\tthe\ttraining\tset\t(just\tlike\tthe\tLinearSVC\tclass),\twhile\tthe\tSVR\tclass\tgets\tmuch\ttoo slow\twhen\tthe\ttraining\tset\tgrows\tlarge\t(just\tlike\tthe\tSVC\tclass).\n\nfrom\tsklearn.svm\timport\tSVR\n\nsvm_poly_reg\t=\tSVR(kernel=\"poly\",\tdegree=2,\tC=100,\tepsilon=0.1) svm_poly_reg.fit(X,\ty)\n\nNOTE\n\nSVMs\tcan\talso\tbe\tused\tfor\toutlier\tdetection;\tsee\tScikit-Learn’s\tdocumentation\tfor\tmore\tdetails.",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Under\tthe\tHood This\tsection\texplains\thow\tSVMs\tmake\tpredictions\tand\thow\ttheir\ttraining\talgorithms\twork,\tstarting\twith linear\tSVM\tclassifiers.\tYou\tcan\tsafely\tskip\tit\tand\tgo\tstraight\tto\tthe\texercises\tat\tthe\tend\tof\tthis\tchapter\tif you\tare\tjust\tgetting\tstarted\twith\tMachine\tLearning,\tand\tcome\tback\tlater\twhen\tyou\twant\tto\tget\ta\tdeeper understanding\tof\tSVMs.\n\nFirst,\ta\tword\tabout\tnotations:\tin\tChapter\t4\twe\tused\tthe\tconvention\tof\tputting\tall\tthe\tmodel\tparameters\tin one\tvector\tθ,\tincluding\tthe\tbias\tterm\tθ0\tand\tthe\tinput\tfeature\tweights\tθ1\tto\tθn,\tand\tadding\ta\tbias\tinput\tx0\t= 1\tto\tall\tinstances.\tIn\tthis\tchapter,\twe\twill\tuse\ta\tdifferent\tconvention,\twhich\tis\tmore\tconvenient\t(and\tmore common)\twhen\tyou\tare\tdealing\twith\tSVMs:\tthe\tbias\tterm\twill\tbe\tcalled\tb\tand\tthe\tfeature\tweights\tvector will\tbe\tcalled\tw.\tNo\tbias\tfeature\twill\tbe\tadded\tto\tthe\tinput\tfeature\tvectors.",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Decision\tFunction\tand\tPredictions The\tlinear\tSVM\tclassifier\tmodel\tpredicts\tthe\tclass\tof\ta\tnew\tinstance\tx\tby\tsimply\tcomputing\tthe\tdecision function\twT\t·\tx\t+\tb\t=\tw1\tx1\t+\t\t+\t wn\txn\t+\tb:\tif\tthe\tresult\tis\tpositive,\tthe\tpredicted\tclass\tŷ\tis\tthe\tpositive class\t(1),\tor\telse\tit\tis\tthe\tnegative\tclass\t(0);\tsee\tEquation\t5-2.\n\nEquation\t5-2.\tLinear\tSVM\tclassifier\tprediction\n\nFigure\t5-12\tshows\tthe\tdecision\tfunction\tthat\tcorresponds\tto\tthe\tmodel\ton\tthe\tright\tof\tFigure\t5-4:\tit\tis\ta two-dimensional\tplane\tsince\tthis\tdataset\thas\ttwo\tfeatures\t(petal\twidth\tand\tpetal\tlength).\tThe\tdecision boundary\tis\tthe\tset\tof\tpoints\twhere\tthe\tdecision\tfunction\tis\tequal\tto\t0:\tit\tis\tthe\tintersection\tof\ttwo\tplanes, which\tis\ta\tstraight\tline\t(represented\tby\tthe\tthick\tsolid\tline).3\n\nFigure\t5-12.\tDecision\tfunction\tfor\tthe\tiris\tdataset\n\nThe\tdashed\tlines\trepresent\tthe\tpoints\twhere\tthe\tdecision\tfunction\tis\tequal\tto\t1\tor\t–1:\tthey\tare\tparallel\tand at\tequal\tdistance\tto\tthe\tdecision\tboundary,\tforming\ta\tmargin\taround\tit.\tTraining\ta\tlinear\tSVM\tclassifier means\tfinding\tthe\tvalue\tof\tw\tand\tb\tthat\tmake\tthis\tmargin\tas\twide\tas\tpossible\twhile\tavoiding\tmargin violations\t(hard\tmargin)\tor\tlimiting\tthem\t(soft\tmargin).",
      "content_length": 1172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Training\tObjective Consider\tthe\tslope\tof\tthe\tdecision\tfunction:\tit\tis\tequal\tto\tthe\tnorm\tof\tthe\tweight\tvector,\t\t w\t.\tIf\twe divide\tthis\tslope\tby\t2,\tthe\tpoints\twhere\tthe\tdecision\tfunction\tis\tequal\tto\t±1\tare\tgoing\tto\tbe\ttwice\tas\tfar away\tfrom\tthe\tdecision\tboundary.\tIn\tother\twords,\tdividing\tthe\tslope\tby\t2\twill\tmultiply\tthe\tmargin\tby\t2. Perhaps\tthis\tis\teasier\tto\tvisualize\tin\t2D\tin\tFigure\t5-13.\tThe\tsmaller\tthe\tweight\tvector\tw,\tthe\tlarger\tthe margin.\n\nFigure\t5-13.\tA\tsmaller\tweight\tvector\tresults\tin\ta\tlarger\tmargin\n\nSo\twe\twant\tto\tminimize\t\t w\t\tto\tget\ta\tlarge\tmargin.\tHowever,\tif\twe\talso\twant\tto\tavoid\tany\tmargin violation\t(hard\tmargin),\tthen\twe\tneed\tthe\tdecision\tfunction\tto\tbe\tgreater\tthan\t1\tfor\tall\tpositive\ttraining instances,\tand\tlower\tthan\t–1\tfor\tnegative\ttraining\tinstances.\tIf\twe\tdefine\tt(i)\t=\t–1\tfor\tnegative\tinstances\t(if y(i)\t=\t0)\tand\tt(i)\t=\t1\tfor\tpositive\tinstances\t(if\ty(i)\t=\t1),\tthen\twe\tcan\texpress\tthis\tconstraint\tas\tt(i)(wT\t·\tx(i)\t+ b)\t≥\t1\tfor\tall\tinstances.\n\nWe\tcan\ttherefore\texpress\tthe\thard\tmargin\tlinear\tSVM\tclassifier\tobjective\tas\tthe\tconstrained optimization\tproblem\tin\tEquation\t5-3.\n\nEquation\t5-3.\tHard\tmargin\tlinear\tSVM\tclassifier\tobjective\n\nNOTE\n\nWe\tare\tminimizing\t wT\t·\tw,\twhich\tis\tequal\tto\n\nw\t2,\trather\tthan\tminimizing\t\t w\t.\tThis\tis\tbecause\tit\twill\tgive\tthe\tsame\n\nw\t2\thas\ta\tnice\tand\tsimple result\t(since\tthe\tvalues\tof\tw\tand\tb\tthat\tminimize\ta\tvalue\talso\tminimize\thalf\tof\tits\tsquare),\tbut\t derivative\t(it\tis\tjust\tw)\twhile\t\t w\t\tis\tnot\tdifferentiable\tat\t w\t=\t0.\tOptimization\talgorithms\twork\tmuch\tbetter\ton\tdifferentiable functions.\n\nTo\tget\tthe\tsoft\tmargin\tobjective,\twe\tneed\tto\tintroduce\ta\tslack\tvariable\tζ(i)\t≥\t0\tfor\teach\tinstance:4\tζ(i) measures\thow\tmuch\tthe\tith\tinstance\tis\tallowed\tto\tviolate\tthe\tmargin.\tWe\tnow\thave\ttwo\tconflicting\n\nobjectives:\tmaking\tthe\tslack\tvariables\tas\tsmall\tas\tpossible\tto\treduce\tthe\tmargin\tviolations,\tand\tmaking\t wT\t·\tw\tas\tsmall\tas\tpossible\tto\tincrease\tthe\tmargin.\tThis\tis\twhere\tthe\tC\thyperparameter\tcomes\tin:\tit",
      "content_length": 1960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "allows\tus\tto\tdefine\tthe\ttradeoff\tbetween\tthese\ttwo\tobjectives.\tThis\tgives\tus\tthe\tconstrained\toptimization problem\tin\tEquation\t5-4.\n\nEquation\t5-4.\tSoft\tmargin\tlinear\tSVM\tclassifier\tobjective",
      "content_length": 189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Quadratic\tProgramming The\thard\tmargin\tand\tsoft\tmargin\tproblems\tare\tboth\tconvex\tquadratic\toptimization\tproblems\twith\tlinear constraints.\tSuch\tproblems\tare\tknown\tas\tQuadratic\tProgramming\t(QP)\tproblems.\tMany\toff-the-shelf solvers\tare\tavailable\tto\tsolve\tQP\tproblems\tusing\ta\tvariety\tof\ttechniques\tthat\tare\toutside\tthe\tscope\tof\tthis book.5\tThe\tgeneral\tproblem\tformulation\tis\tgiven\tby\tEquation\t5-5.\n\nEquation\t5-5.\tQuadratic\tProgramming\tproblem\n\nNote\tthat\tthe\texpression\tA\t·\tp\t≤\tb\tactually\tdefines\tnc\tconstraints:\tpT\t·\ta(i)\t≤\tb(i)\tfor\ti\t=\t1,\t2,\t,\t nc,\twhere a(i)\tis\tthe\tvector\tcontaining\tthe\telements\tof\tthe\tith\trow\tof\tA\tand\tb(i)\tis\tthe\tith\telement\tof\tb.\n\nYou\tcan\teasily\tverify\tthat\tif\tyou\tset\tthe\tQP\tparameters\tin\tthe\tfollowing\tway,\tyou\tget\tthe\thard\tmargin linear\tSVM\tclassifier\tobjective:\n\nnp\t=\tn\t+\t1,\twhere\tn\tis\tthe\tnumber\tof\tfeatures\t(the\t+1\tis\tfor\tthe\tbias\tterm).\n\nnc\t=\tm,\twhere\tm\tis\tthe\tnumber\tof\ttraining\tinstances.\n\nH\tis\tthe\tnp\t×\tnp\tidentity\tmatrix,\texcept\twith\ta\tzero\tin\tthe\ttop-left\tcell\t(to\tignore\tthe\tbias\tterm).\n\nf\t=\t0,\tan\tnp-dimensional\tvector\tfull\tof\t0s.\n\nb\t=\t1,\tan\tnc-dimensional\tvector\tfull\tof\t1s.\n\na(i)\t=\t–t(i)\n\n(i),\twhere\n\n(i)\tis\tequal\tto\tx(i)\twith\tan\textra\tbias\tfeature\n\n0\t=\t1.\n\nSo\tone\tway\tto\ttrain\ta\thard\tmargin\tlinear\tSVM\tclassifier\tis\tjust\tto\tuse\tan\toff-the-shelf\tQP\tsolver\tby passing\tit\tthe\tpreceding\tparameters.\tThe\tresulting\tvector\tp\twill\tcontain\tthe\tbias\tterm\tb\t=\tp0\tand\tthe feature\tweights\twi\t=\tpi\tfor\ti\t=\t1,\t2,\t,\t m.\tSimilarly,\tyou\tcan\tuse\ta\tQP\tsolver\tto\tsolve\tthe\tsoft\tmargin problem\t(see\tthe\texercises\tat\tthe\tend\tof\tthe\tchapter).\n\nHowever,\tto\tuse\tthe\tkernel\ttrick\twe\tare\tgoing\tto\tlook\tat\ta\tdifferent\tconstrained\toptimization\tproblem.",
      "content_length": 1656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "The\tDual\tProblem Given\ta\tconstrained\toptimization\tproblem,\tknown\tas\tthe\tprimal\tproblem,\tit\tis\tpossible\tto\texpress\ta different\tbut\tclosely\trelated\tproblem,\tcalled\tits\tdual\tproblem.\tThe\tsolution\tto\tthe\tdual\tproblem\ttypically gives\ta\tlower\tbound\tto\tthe\tsolution\tof\tthe\tprimal\tproblem,\tbut\tunder\tsome\tconditions\tit\tcan\teven\thave\tthe same\tsolutions\tas\tthe\tprimal\tproblem.\tLuckily,\tthe\tSVM\tproblem\thappens\tto\tmeet\tthese\tconditions,6\tso you\tcan\tchoose\tto\tsolve\tthe\tprimal\tproblem\tor\tthe\tdual\tproblem;\tboth\twill\thave\tthe\tsame\tsolution. Equation\t5-6\tshows\tthe\tdual\tform\tof\tthe\tlinear\tSVM\tobjective\t(if\tyou\tare\tinterested\tin\tknowing\thow\tto derive\tthe\tdual\tproblem\tfrom\tthe\tprimal\tproblem,\tsee\tAppendix\tC).\n\nEquation\t5-6.\tDual\tform\tof\tthe\tlinear\tSVM\tobjective\n\nOnce\tyou\tfind\tthe\tvector\t that\tminimize\tthe\tprimal\tproblem\tby\tusing\tEquation\t5-7.\n\nthat\tminimizes\tthis\tequation\t(using\ta\tQP\tsolver),\tyou\tcan\tcompute\n\nEquation\t5-7.\tFrom\tthe\tdual\tsolution\tto\tthe\tprimal\tsolution\n\nThe\tdual\tproblem\tis\tfaster\tto\tsolve\tthan\tthe\tprimal\twhen\tthe\tnumber\tof\ttraining\tinstances\tis\tsmaller\tthan the\tnumber\tof\tfeatures.\tMore\timportantly,\tit\tmakes\tthe\tkernel\ttrick\tpossible,\twhile\tthe\tprimal\tdoes\tnot.\tSo what\tis\tthis\tkernel\ttrick\tanyway?\n\nand",
      "content_length": 1213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "Kernelized\tSVM Suppose\tyou\twant\tto\tapply\ta\t2nd-degree\tpolynomial\ttransformation\tto\ta\ttwo-dimensional\ttraining\tset (such\tas\tthe\tmoons\ttraining\tset),\tthen\ttrain\ta\tlinear\tSVM\tclassifier\ton\tthe\ttransformed\ttraining\tset. Equation\t5-8\tshows\tthe\t2nd-degree\tpolynomial\tmapping\tfunction\tϕ\tthat\tyou\twant\tto\tapply.\n\nEquation\t5-8.\tSecond-degree\tpolynomial\tmapping\n\nNotice\tthat\tthe\ttransformed\tvector\tis\tthree-dimensional\tinstead\tof\ttwo-dimensional.\tNow\tlet’s\tlook\tat what\thappens\tto\ta\tcouple\tof\ttwo-dimensional\tvectors,\ta\tand\tb,\tif\twe\tapply\tthis\t2nd-degree\tpolynomial mapping\tand\tthen\tcompute\tthe\tdot\tproduct\tof\tthe\ttransformed\tvectors\t(See\tEquation\t5-9).\n\nEquation\t5-9.\tKernel\ttrick\tfor\ta\t2nd-degree\tpolynomial\tmapping\n\nHow\tabout\tthat?\tThe\tdot\tproduct\tof\tthe\ttransformed\tvectors\tis\tequal\tto\tthe\tsquare\tof\tthe\tdot\tproduct\tof\tthe original\tvectors:\tϕ(a)T\t·\tϕ(b)\t=\t(aT\t·\tb)2.\n\nNow\there\tis\tthe\tkey\tinsight:\tif\tyou\tapply\tthe\ttransformation\tϕ\tto\tall\ttraining\tinstances,\tthen\tthe\tdual problem\t(see\tEquation\t5-6)\twill\tcontain\tthe\tdot\tproduct\tϕ(x(i))T\t·\tϕ(x(j)).\tBut\tif\tϕ\tis\tthe\t2nd-degree polynomial\ttransformation\tdefined\tin\tEquation\t5-8,\tthen\tyou\tcan\treplace\tthis\tdot\tproduct\tof\ttransformed\n\nvectors\tsimply\tby\t .\tSo\tyou\tdon’t\tactually\tneed\tto\ttransform\tthe\ttraining\tinstances\tat\tall:\tjust replace\tthe\tdot\tproduct\tby\tits\tsquare\tin\tEquation\t5-6.\tThe\tresult\twill\tbe\tstrictly\tthe\tsame\tas\tif\tyou\twent through\tthe\ttrouble\tof\tactually\ttransforming\tthe\ttraining\tset\tthen\tfitting\ta\tlinear\tSVM\talgorithm,\tbut\tthis trick\tmakes\tthe\twhole\tprocess\tmuch\tmore\tcomputationally\tefficient.\tThis\tis\tthe\tessence\tof\tthe\tkernel trick. The\tfunction\tK(a,\tb)\t=\t(aT\t·\tb)2\tis\tcalled\ta\t2nd-degree\tpolynomial\tkernel.\tIn\tMachine\tLearning,\ta\tkernel is\ta\tfunction\tcapable\tof\tcomputing\tthe\tdot\tproduct\tϕ(a)T\t·\tϕ(b)\tbased\tonly\ton\tthe\toriginal\tvectors\ta\tand\tb,",
      "content_length": 1806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "without\thaving\tto\tcompute\t(or\teven\tto\tknow\tabout)\tthe\ttransformation\tϕ.\tEquation\t5-10\tlists\tsome\tof\tthe most\tcommonly\tused\tkernels.\n\nEquation\t5-10.\tCommon\tkernels\n\nMERCER’S\tTHEOREM\n\nAccording\tto\tMercer’s\ttheorem,\tif\ta\tfunction\tK(a,\tb)\trespects\ta\tfew\tmathematical\tconditions\tcalled\tMercer’s\tconditions\t(K\tmust\tbe continuous,\tsymmetric\tin\tits\targuments\tso\tK(a,\tb)\t=\tK(b,\ta),\tetc.),\tthen\tthere\texists\ta\tfunction\tϕ\tthat\tmaps\ta\tand\tb\tinto\tanother\tspace (possibly\twith\tmuch\thigher\tdimensions)\tsuch\tthat\tK(a,\tb)\t=\tϕ(a)T\t·\tϕ(b).\tSo\tyou\tcan\tuse\tK\tas\ta\tkernel\tsince\tyou\tknow\tϕ\texists,\teven\tif you\tdon’t\tknow\twhat\tϕ\tis.\tIn\tthe\tcase\tof\tthe\tGaussian\tRBF\tkernel,\tit\tcan\tbe\tshown\tthat\tϕ\tactually\tmaps\teach\ttraining\tinstance\tto\tan infinite-dimensional\tspace,\tso\tit’s\ta\tgood\tthing\tyou\tdon’t\tneed\tto\tactually\tperform\tthe\tmapping!\n\nNote\tthat\tsome\tfrequently\tused\tkernels\t(such\tas\tthe\tSigmoid\tkernel)\tdon’t\trespect\tall\tof\tMercer’s\tconditions,\tyet\tthey\tgenerally\twork well\tin\tpractice.\n\nThere\tis\tstill\tone\tloose\tend\twe\tmust\ttie.\tEquation\t5-7\tshows\thow\tto\tgo\tfrom\tthe\tdual\tsolution\tto\tthe primal\tsolution\tin\tthe\tcase\tof\ta\tlinear\tSVM\tclassifier,\tbut\tif\tyou\tapply\tthe\tkernel\ttrick\tyou\tend\tup\twith equations\tthat\tinclude\tϕ(x(i)).\tIn\tfact,\t \tmust\thave\tthe\tsame\tnumber\tof\tdimensions\tas\tϕ(x(i)),\twhich\tmay ? be\thuge\tor\teven\tinfinite,\tso\tyou\tcan’t\tcompute\tit.\tBut\thow\tcan\tyou\tmake\tpredictions\twithout\tknowing\t Well,\tthe\tgood\tnews\tis\tthat\tyou\tcan\tplug\tin\tthe\tformula\tfor\t \tfrom\tEquation\t5-7\tinto\tthe\tdecision\tfunction for\ta\tnew\tinstance\tx(n),\tand\tyou\tget\tan\tequation\twith\tonly\tdot\tproducts\tbetween\tinput\tvectors.\tThis\tmakes it\tpossible\tto\tuse\tthe\tkernel\ttrick,\tonce\tagain\t(Equation\t5-11).\n\nEquation\t5-11.\tMaking\tpredictions\twith\ta\tkernelized\tSVM\n\nNote\tthat\tsince\tα(i)\t≠\t0\tonly\tfor\tsupport\tvectors,\tmaking\tpredictions\tinvolves\tcomputing\tthe\tdot\tproduct\tof the\tnew\tinput\tvector\tx(n)\twith\tonly\tthe\tsupport\tvectors,\tnot\tall\tthe\ttraining\tinstances.\tOf\tcourse,\tyou\talso ,\tusing\tthe\tsame\ttrick\t(Equation\t5-12). need\tto\tcompute\tthe\tbias\tterm\n\nEquation\t5-12.\tComputing\tthe\tbias\tterm\tusing\tthe\tkernel\ttrick",
      "content_length": 2066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "If\tyou\tare\tstarting\tto\tget\ta\theadache,\tit’s\tperfectly\tnormal:\tit’s\tan\tunfortunate\tside\teffects\tof\tthe\tkernel trick.",
      "content_length": 115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "Online\tSVMs Before\tconcluding\tthis\tchapter,\tlet’s\ttake\ta\tquick\tlook\tat\tonline\tSVM\tclassifiers\t(recall\tthat\tonline learning\tmeans\tlearning\tincrementally,\ttypically\tas\tnew\tinstances\tarrive).\n\nFor\tlinear\tSVM\tclassifiers,\tone\tmethod\tis\tto\tuse\tGradient\tDescent\t(e.g.,\tusing\tSGDClassifier)\tto minimize\tthe\tcost\tfunction\tin\tEquation\t5-13,\twhich\tis\tderived\tfrom\tthe\tprimal\tproblem.\tUnfortunately\tit converges\tmuch\tmore\tslowly\tthan\tthe\tmethods\tbased\ton\tQP.\n\nEquation\t5-13.\tLinear\tSVM\tclassifier\tcost\tfunction\n\nThe\tfirst\tsum\tin\tthe\tcost\tfunction\twill\tpush\tthe\tmodel\tto\thave\ta\tsmall\tweight\tvector\tw,\tleading\tto\ta\tlarger margin.\tThe\tsecond\tsum\tcomputes\tthe\ttotal\tof\tall\tmargin\tviolations.\tAn\tinstance’s\tmargin\tviolation\tis equal\tto\t0\tif\tit\tis\tlocated\toff\tthe\tstreet\tand\ton\tthe\tcorrect\tside,\tor\telse\tit\tis\tproportional\tto\tthe\tdistance\tto the\tcorrect\tside\tof\tthe\tstreet.\tMinimizing\tthis\tterm\tensures\tthat\tthe\tmodel\tmakes\tthe\tmargin\tviolations\tas small\tand\tas\tfew\tas\tpossible\n\nHINGE\tLOSS\n\nThe\tfunction\tmax(0,\t1\t–\tt)\tis\tcalled\tthe\thinge\tloss\tfunction\t(represented\tbelow).\tIt\tis\tequal\tto\t0\twhen\tt\t≥\t1.\tIts\tderivative\t(slope)\tis\tequal to\t–1\tif\tt\t<\t1\tand\t0\tif\tt\t>\t1.\tIt\tis\tnot\tdifferentiable\tat\tt\t=\t1,\tbut\tjust\tlike\tfor\tLasso\tRegression\t(see\t“Lasso\tRegression”)\tyou\tcan\tstill\tuse Gradient\tDescent\tusing\tany\tsubderivative\tat\tt\t=\t1\t(i.e.,\tany\tvalue\tbetween\t–1\tand\t0).\n\nIt\tis\talso\tpossible\tto\timplement\tonline\tkernelized\tSVMs\t—\tfor\texample,\tusing\t“Incremental\tand Decremental\tSVM\tLearning”7\tor\t“Fast\tKernel\tClassifiers\twith\tOnline\tand\tActive\tLearning.”8\tHowever, these\tare\timplemented\tin\tMatlab\tand\tC++.\tFor\tlarge-scale\tnonlinear\tproblems,\tyou\tmay\twant\tto\tconsider using\tneural\tnetworks\tinstead\t(see\tPart\tII).",
      "content_length": 1687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "Exercises\n\n1.\t What\tis\tthe\tfundamental\tidea\tbehind\tSupport\tVector\tMachines?\n\n2.\t What\tis\ta\tsupport\tvector?\n\n3.\t Why\tis\tit\timportant\tto\tscale\tthe\tinputs\twhen\tusing\tSVMs?\n\n4.\t Can\tan\tSVM\tclassifier\toutput\ta\tconfidence\tscore\twhen\tit\tclassifies\tan\tinstance?\tWhat\tabout\ta probability?\n\n5.\t Should\tyou\tuse\tthe\tprimal\tor\tthe\tdual\tform\tof\tthe\tSVM\tproblem\tto\ttrain\ta\tmodel\ton\ta\ttraining\tset with\tmillions\tof\tinstances\tand\thundreds\tof\tfeatures?\n\n6.\t Say\tyou\ttrained\tan\tSVM\tclassifier\twith\tan\tRBF\tkernel.\tIt\tseems\tto\tunderfit\tthe\ttraining\tset:\tshould you\tincrease\tor\tdecrease\tγ\t(gamma)?\tWhat\tabout\tC?\n\n7.\t How\tshould\tyou\tset\tthe\tQP\tparameters\t(H,\tf,\tA,\tand\tb)\tto\tsolve\tthe\tsoft\tmargin\tlinear\tSVM classifier\tproblem\tusing\tan\toff-the-shelf\tQP\tsolver?\n\n8.\t Train\ta\tLinearSVC\ton\ta\tlinearly\tseparable\tdataset.\tThen\ttrain\tan\tSVC\tand\ta\tSGDClassifier\ton\tthe same\tdataset.\tSee\tif\tyou\tcan\tget\tthem\tto\tproduce\troughly\tthe\tsame\tmodel.\n\n9.\t Train\tan\tSVM\tclassifier\ton\tthe\tMNIST\tdataset.\tSince\tSVM\tclassifiers\tare\tbinary\tclassifiers,\tyou will\tneed\tto\tuse\tone-versus-all\tto\tclassify\tall\t10\tdigits.\tYou\tmay\twant\tto\ttune\tthe\thyperparameters using\tsmall\tvalidation\tsets\tto\tspeed\tup\tthe\tprocess.\tWhat\taccuracy\tcan\tyou\treach?\n\n10.\t Train\tan\tSVM\tregressor\ton\tthe\tCalifornia\thousing\tdataset.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“A\tDual\tCoordinate\tDescent\tMethod\tfor\tLarge-scale\tLinear\tSVM,”\tLin\tet\tal.\t(2008).\n\n2\n\n“Sequential\tMinimal\tOptimization\t(SMO),”\tJ.\tPlatt\t(1998).\n\n3\n\nMore\tgenerally,\twhen\tthere\tare\tn\tfeatures,\tthe\tdecision\tfunction\tis\tan\tn-dimensional\thyperplane,\tand\tthe\tdecision\tboundary\tis\tan\t(n\t–\t1)- dimensional\thyperplane.\n\n4\n\nZeta\t(ζ)\tis\tthe\t8\n\nth\n\nletter\tof\tthe\tGreek\talphabet.\n\n5\n\nTo\tlearn\tmore\tabout\tQuadratic\tProgramming,\tyou\tcan\tstart\tby\treading\tStephen\tBoyd\tand\tLieven\tVandenberghe,\tConvex\tOptimization (Cambridge,\tUK:\tCambridge\tUniversity\tPress,\t2004)\tor\twatch\tRichard\tBrown’s\tseries\tof\tvideo\tlectures.\n\n6\n\nThe\tobjective\tfunction\tis\tconvex,\tand\tthe\tinequality\tconstraints\tare\tcontinuously\tdifferentiable\tand\tconvex\tfunctions.\n\n7\n\n“Incremental\tand\tDecremental\tSupport\tVector\tMachine\tLearning,”\tG.\tCauwenberghs,\tT.\tPoggio\t(2001).\n\n8\n\n“Fast\tKernel\tClassifiers\twith\tOnline\tand\tActive\tLearning,“\tA.\tBordes,\tS.\tErtekin,\tJ.\tWeston,\tL.\tBottou\t(2005).",
      "content_length": 2259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Chapter\t6.\tDecision\tTrees\n\nLike\tSVMs,\tDecision\tTrees\tare\tversatile\tMachine\tLearning\talgorithms\tthat\tcan\tperform\tboth classification\tand\tregression\ttasks,\tand\teven\tmultioutput\ttasks.\tThey\tare\tvery\tpowerful\talgorithms, capable\tof\tfitting\tcomplex\tdatasets.\tFor\texample,\tin\tChapter\t2\tyou\ttrained\ta\tDecisionTreeRegressor model\ton\tthe\tCalifornia\thousing\tdataset,\tfitting\tit\tperfectly\t(actually\toverfitting\tit).\n\nDecision\tTrees\tare\talso\tthe\tfundamental\tcomponents\tof\tRandom\tForests\t(see\tChapter\t7),\twhich\tare among\tthe\tmost\tpowerful\tMachine\tLearning\talgorithms\tavailable\ttoday.\n\nIn\tthis\tchapter\twe\twill\tstart\tby\tdiscussing\thow\tto\ttrain,\tvisualize,\tand\tmake\tpredictions\twith\tDecision Trees.\tThen\twe\twill\tgo\tthrough\tthe\tCART\ttraining\talgorithm\tused\tby\tScikit-Learn,\tand\twe\twill\tdiscuss how\tto\tregularize\ttrees\tand\tuse\tthem\tfor\tregression\ttasks.\tFinally,\twe\twill\tdiscuss\tsome\tof\tthe\tlimitations of\tDecision\tTrees.",
      "content_length": 903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "Training\tand\tVisualizing\ta\tDecision\tTree To\tunderstand\tDecision\tTrees,\tlet’s\tjust\tbuild\tone\tand\ttake\ta\tlook\tat\thow\tit\tmakes\tpredictions.\tThe following\tcode\ttrains\ta\tDecisionTreeClassifier\ton\tthe\tiris\tdataset\t(see\tChapter\t4):\n\nfrom\tsklearn.datasets\timport\tload_iris from\tsklearn.tree\timport\tDecisionTreeClassifier\n\niris\t=\tload_iris() X\t=\tiris.data[:,\t2:]\t#\tpetal\tlength\tand\twidth y\t=\tiris.target\n\ntree_clf\t=\tDecisionTreeClassifier(max_depth=2) tree_clf.fit(X,\ty)\n\nYou\tcan\tvisualize\tthe\ttrained\tDecision\tTree\tby\tfirst\tusing\tthe\texport_graphviz()\tmethod\tto\toutput\ta graph\tdefinition\tfile\tcalled\tiris_tree.dot:\n\nfrom\tsklearn.tree\timport\texport_graphviz\n\nexport_graphviz( \t\t\t\t\t\t\t\ttree_clf, \t\t\t\t\t\t\t\tout_file=image_path(\"iris_tree.dot\"), \t\t\t\t\t\t\t\tfeature_names=iris.feature_names[2:], \t\t\t\t\t\t\t\tclass_names=iris.target_names, \t\t\t\t\t\t\t\trounded=True, \t\t\t\t\t\t\t\tfilled=True \t\t\t\t)\n\nThen\tyou\tcan\tconvert\tthis\t.dot\tfile\tto\ta\tvariety\tof\tformats\tsuch\tas\tPDF\tor\tPNG\tusing\tthe\tdot\tcommand- line\ttool\tfrom\tthe\tgraphviz\tpackage.1\tThis\tcommand\tline\tconverts\tthe\t.dot\tfile\tto\ta\t.png\timage\tfile:\n\n$\tdot\t-Tpng\tiris_tree.dot\t-o\tiris_tree.png\n\nYour\tfirst\tdecision\ttree\tlooks\tlike\tFigure\t6-1.",
      "content_length": 1160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "Figure\t6-1.\tIris\tDecision\tTree",
      "content_length": 30,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Making\tPredictions Let’s\tsee\thow\tthe\ttree\trepresented\tin\tFigure\t6-1\tmakes\tpredictions.\tSuppose\tyou\tfind\tan\tiris\tflower\tand you\twant\tto\tclassify\tit.\tYou\tstart\tat\tthe\troot\tnode\t(depth\t0,\tat\tthe\ttop):\tthis\tnode\tasks\twhether\tthe\tflower’s petal\tlength\tis\tsmaller\tthan\t2.45\tcm.\tIf\tit\tis,\tthen\tyou\tmove\tdown\tto\tthe\troot’s\tleft\tchild\tnode\t(depth\t1, left).\tIn\tthis\tcase,\tit\tis\ta\tleaf\tnode\t(i.e.,\tit\tdoes\tnot\thave\tany\tchildren\tnodes),\tso\tit\tdoes\tnot\task\tany questions:\tyou\tcan\tsimply\tlook\tat\tthe\tpredicted\tclass\tfor\tthat\tnode\tand\tthe\tDecision\tTree\tpredicts\tthat your\tflower\tis\tan\tIris-Setosa\t(class=setosa).\n\nNow\tsuppose\tyou\tfind\tanother\tflower,\tbut\tthis\ttime\tthe\tpetal\tlength\tis\tgreater\tthan\t2.45\tcm.\tYou\tmust move\tdown\tto\tthe\troot’s\tright\tchild\tnode\t(depth\t1,\tright),\twhich\tis\tnot\ta\tleaf\tnode,\tso\tit\tasks\tanother question:\tis\tthe\tpetal\twidth\tsmaller\tthan\t1.75\tcm?\tIf\tit\tis,\tthen\tyour\tflower\tis\tmost\tlikely\tan\tIris- Versicolor\t(depth\t2,\tleft).\tIf\tnot,\tit\tis\tlikely\tan\tIris-Virginica\t(depth\t2,\tright).\tIt’s\treally\tthat\tsimple.\n\nNOTE\n\nOne\tof\tthe\tmany\tqualities\tof\tDecision\tTrees\tis\tthat\tthey\trequire\tvery\tlittle\tdata\tpreparation.\tIn\tparticular,\tthey\tdon’t\trequire feature\tscaling\tor\tcentering\tat\tall.\n\nA\tnode’s\tsamples\tattribute\tcounts\thow\tmany\ttraining\tinstances\tit\tapplies\tto.\tFor\texample,\t100\ttraining instances\thave\ta\tpetal\tlength\tgreater\tthan\t2.45\tcm\t(depth\t1,\tright),\tamong\twhich\t54\thave\ta\tpetal\twidth smaller\tthan\t1.75\tcm\t(depth\t2,\tleft).\tA\tnode’s\tvalue\tattribute\ttells\tyou\thow\tmany\ttraining\tinstances\tof each\tclass\tthis\tnode\tapplies\tto:\tfor\texample,\tthe\tbottom-right\tnode\tapplies\tto\t0\tIris-Setosa,\t1\tIris- Versicolor,\tand\t45\tIris-Virginica.\tFinally,\ta\tnode’s\tgini\tattribute\tmeasures\tits\timpurity:\ta\tnode\tis\t“pure” (gini=0)\tif\tall\ttraining\tinstances\tit\tapplies\tto\tbelong\tto\tthe\tsame\tclass.\tFor\texample,\tsince\tthe\tdepth-1 left\tnode\tapplies\tonly\tto\tIris-Setosa\ttraining\tinstances,\tit\tis\tpure\tand\tits\tgini\tscore\tis\t0.\tEquation\t6-1 shows\thow\tthe\ttraining\talgorithm\tcomputes\tthe\tgini\tscore\tGi\tof\tthe\tith\tnode.\tFor\texample,\tthe\tdepth-2\tleft node\thas\ta\tgini\tscore\tequal\tto\t1\t–\t(0/54)2\t–\t(49/54)2\t–\t(5/54)2\t≈\t0.168.\tAnother\timpurity\tmeasure\tis discussed\tshortly.\n\nEquation\t6-1.\tGini\timpurity\n\npi,k\tis\tthe\tratio\tof\tclass\tk\tinstances\tamong\tthe\ttraining\tinstances\tin\tthe\tith\tnode.",
      "content_length": 2265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "NOTE\n\nScikit-Learn\tuses\tthe\tCART\talgorithm,\twhich\tproduces\tonly\tbinary\ttrees:\tnonleaf\tnodes\talways\thave\ttwo\tchildren\t(i.e., questions\tonly\thave\tyes/no\tanswers).\tHowever,\tother\talgorithms\tsuch\tas\tID3\tcan\tproduce\tDecision\tTrees\twith\tnodes\tthat\thave more\tthan\ttwo\tchildren.\n\nFigure\t6-2\tshows\tthis\tDecision\tTree’s\tdecision\tboundaries.\tThe\tthick\tvertical\tline\trepresents\tthe\tdecision boundary\tof\tthe\troot\tnode\t(depth\t0):\tpetal\tlength\t=\t2.45\tcm.\tSince\tthe\tleft\tarea\tis\tpure\t(only\tIris-Setosa), it\tcannot\tbe\tsplit\tany\tfurther.\tHowever,\tthe\tright\tarea\tis\timpure,\tso\tthe\tdepth-1\tright\tnode\tsplits\tit\tat\tpetal width\t=\t1.75\tcm\t(represented\tby\tthe\tdashed\tline).\tSince\tmax_depth\twas\tset\tto\t2,\tthe\tDecision\tTree\tstops right\tthere.\tHowever,\tif\tyou\tset\tmax_depth\tto\t3,\tthen\tthe\ttwo\tdepth-2\tnodes\twould\teach\tadd\tanother decision\tboundary\t(represented\tby\tthe\tdotted\tlines).\n\nFigure\t6-2.\tDecision\tTree\tdecision\tboundaries\n\nMODEL\tINTERPRETATION:\tWHITE\tBOX\tVERSUS\tBLACK\tBOX\n\nAs\tyou\tcan\tsee\tDecision\tTrees\tare\tfairly\tintuitive\tand\ttheir\tdecisions\tare\teasy\tto\tinterpret.\tSuch\tmodels\tare\toften\tcalled\twhite\tbox models.\tIn\tcontrast,\tas\twe\twill\tsee,\tRandom\tForests\tor\tneural\tnetworks\tare\tgenerally\tconsidered\tblack\tbox\tmodels.\tThey\tmake\tgreat predictions,\tand\tyou\tcan\teasily\tcheck\tthe\tcalculations\tthat\tthey\tperformed\tto\tmake\tthese\tpredictions;\tnevertheless,\tit\tis\tusually\thard\tto explain\tin\tsimple\tterms\twhy\tthe\tpredictions\twere\tmade.\tFor\texample,\tif\ta\tneural\tnetwork\tsays\tthat\ta\tparticular\tperson\tappears\ton\ta picture,\tit\tis\thard\tto\tknow\twhat\tactually\tcontributed\tto\tthis\tprediction:\tdid\tthe\tmodel\trecognize\tthat\tperson’s\teyes?\tHer\tmouth?\tHer\tnose? Her\tshoes?\tOr\teven\tthe\tcouch\tthat\tshe\twas\tsitting\ton?\tConversely,\tDecision\tTrees\tprovide\tnice\tand\tsimple\tclassification\trules\tthat\tcan even\tbe\tapplied\tmanually\tif\tneed\tbe\t(e.g.,\tfor\tflower\tclassification).",
      "content_length": 1830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Estimating\tClass\tProbabilities A\tDecision\tTree\tcan\talso\testimate\tthe\tprobability\tthat\tan\tinstance\tbelongs\tto\ta\tparticular\tclass\tk:\tfirst\tit traverses\tthe\ttree\tto\tfind\tthe\tleaf\tnode\tfor\tthis\tinstance,\tand\tthen\tit\treturns\tthe\tratio\tof\ttraining\tinstances\tof class\tk\tin\tthis\tnode.\tFor\texample,\tsuppose\tyou\thave\tfound\ta\tflower\twhose\tpetals\tare\t5\tcm\tlong\tand\t1.5 cm\twide.\tThe\tcorresponding\tleaf\tnode\tis\tthe\tdepth-2\tleft\tnode,\tso\tthe\tDecision\tTree\tshould\toutput\tthe following\tprobabilities:\t0%\tfor\tIris-Setosa\t(0/54),\t90.7%\tfor\tIris-Versicolor\t(49/54),\tand\t9.3%\tfor\tIris- Virginica\t(5/54).\tAnd\tof\tcourse\tif\tyou\task\tit\tto\tpredict\tthe\tclass,\tit\tshould\toutput\tIris-Versicolor\t(class\t1) since\tit\thas\tthe\thighest\tprobability.\tLet’s\tcheck\tthis:\n\n>>>\ttree_clf.predict_proba([[5,\t1.5]]) array([[\t0.\t,\t\t0.90740741,\t\t0.09259259]]) >>>\ttree_clf.predict([[5,\t1.5]]) array([1])\n\nPerfect!\tNotice\tthat\tthe\testimated\tprobabilities\twould\tbe\tidentical\tanywhere\telse\tin\tthe\tbottom-right rectangle\tof\tFigure\t6-2\t—\tfor\texample,\tif\tthe\tpetals\twere\t6\tcm\tlong\tand\t1.5\tcm\twide\t(even\tthough\tit seems\tobvious\tthat\tit\twould\tmost\tlikely\tbe\tan\tIris-Virginica\tin\tthis\tcase).",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "The\tCART\tTraining\tAlgorithm Scikit-Learn\tuses\tthe\tClassification\tAnd\tRegression\tTree\t(CART)\talgorithm\tto\ttrain\tDecision\tTrees\t(also called\t“growing”\ttrees).\tThe\tidea\tis\treally\tquite\tsimple:\tthe\talgorithm\tfirst\tsplits\tthe\ttraining\tset\tin\ttwo subsets\tusing\ta\tsingle\tfeature\tk\tand\ta\tthreshold\ttk\t(e.g.,\t“petal\tlength\t≤\t2.45\tcm”).\tHow\tdoes\tit\tchoose\tk and\ttk?\tIt\tsearches\tfor\tthe\tpair\t(k,\ttk)\tthat\tproduces\tthe\tpurest\tsubsets\t(weighted\tby\ttheir\tsize).\tThe\tcost function\tthat\tthe\talgorithm\ttries\tto\tminimize\tis\tgiven\tby\tEquation\t6-2.\n\nEquation\t6-2.\tCART\tcost\tfunction\tfor\tclassification\n\nOnce\tit\thas\tsuccessfully\tsplit\tthe\ttraining\tset\tin\ttwo,\tit\tsplits\tthe\tsubsets\tusing\tthe\tsame\tlogic,\tthen\tthe\tsub- subsets\tand\tso\ton,\trecursively.\tIt\tstops\trecursing\tonce\tit\treaches\tthe\tmaximum\tdepth\t(defined\tby\tthe max_depth\thyperparameter),\tor\tif\tit\tcannot\tfind\ta\tsplit\tthat\twill\treduce\timpurity.\tA\tfew\tother hyperparameters\t(described\tin\ta\tmoment)\tcontrol\tadditional\tstopping\tconditions\t(min_samples_split, min_samples_leaf,\tmin_weight_fraction_leaf,\tand\tmax_leaf_nodes).\n\nWARNING\n\nAs\tyou\tcan\tsee,\tthe\tCART\talgorithm\tis\ta\tgreedy\talgorithm:\tit\tgreedily\tsearches\tfor\tan\toptimum\tsplit\tat\tthe\ttop\tlevel,\tthen repeats\tthe\tprocess\tat\teach\tlevel.\tIt\tdoes\tnot\tcheck\twhether\tor\tnot\tthe\tsplit\twill\tlead\tto\tthe\tlowest\tpossible\timpurity\tseveral\tlevels down.\tA\tgreedy\talgorithm\toften\tproduces\ta\treasonably\tgood\tsolution,\tbut\tit\tis\tnot\tguaranteed\tto\tbe\tthe\toptimal\tsolution.\n\nUnfortunately,\tfinding\tthe\toptimal\ttree\tis\tknown\tto\tbe\tan\tNP-Complete\tproblem:2\tit\trequires\tO(exp(m)) time,\tmaking\tthe\tproblem\tintractable\teven\tfor\tfairly\tsmall\ttraining\tsets.\tThis\tis\twhy\twe\tmust\tsettle\tfor\ta “reasonably\tgood”\tsolution.",
      "content_length": 1683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Computational\tComplexity Making\tpredictions\trequires\ttraversing\tthe\tDecision\tTree\tfrom\tthe\troot\tto\ta\tleaf.\tDecision\tTrees\tare generally\tapproximately\tbalanced,\tso\ttraversing\tthe\tDecision\tTree\trequires\tgoing\tthrough\troughly O(log2(m))\tnodes.3\tSince\teach\tnode\tonly\trequires\tchecking\tthe\tvalue\tof\tone\tfeature,\tthe\toverall prediction\tcomplexity\tis\tjust\tO(log2(m)),\tindependent\tof\tthe\tnumber\tof\tfeatures.\tSo\tpredictions\tare\tvery fast,\teven\twhen\tdealing\twith\tlarge\ttraining\tsets.\n\nHowever,\tthe\ttraining\talgorithm\tcompares\tall\tfeatures\t(or\tless\tif\tmax_features\tis\tset)\ton\tall\tsamples\tat each\tnode.\tThis\tresults\tin\ta\ttraining\tcomplexity\tof\tO(n\t×\tm\tlog(m)).\tFor\tsmall\ttraining\tsets\t(less\tthan\ta few\tthousand\tinstances),\tScikit-Learn\tcan\tspeed\tup\ttraining\tby\tpresorting\tthe\tdata\t(set\tpresort=True), but\tthis\tslows\tdown\ttraining\tconsiderably\tfor\tlarger\ttraining\tsets.",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "Gini\tImpurity\tor\tEntropy? By\tdefault,\tthe\tGini\timpurity\tmeasure\tis\tused,\tbut\tyou\tcan\tselect\tthe\tentropy\timpurity\tmeasure\tinstead\tby setting\tthe\tcriterion\thyperparameter\tto\t\"entropy\".\tThe\tconcept\tof\tentropy\toriginated\tin thermodynamics\tas\ta\tmeasure\tof\tmolecular\tdisorder:\tentropy\tapproaches\tzero\twhen\tmolecules\tare\tstill and\twell\tordered.\tIt\tlater\tspread\tto\ta\twide\tvariety\tof\tdomains,\tincluding\tShannon’s\tinformation\ttheory, where\tit\tmeasures\tthe\taverage\tinformation\tcontent\tof\ta\tmessage:4\tentropy\tis\tzero\twhen\tall\tmessages\tare identical.\tIn\tMachine\tLearning,\tit\tis\tfrequently\tused\tas\tan\timpurity\tmeasure:\ta\tset’s\tentropy\tis\tzero\twhen it\tcontains\tinstances\tof\tonly\tone\tclass.\tEquation\t6-3\tshows\tthe\tdefinition\tof\tthe\tentropy\tof\tthe\tith\tnode.\n\nFor\texample,\tthe\tdepth-2\tleft\tnode\tin\tFigure\t6-1\thas\tan\tentropy\tequal\tto\t ≈\t0.31.\n\nEquation\t6-3.\tEntropy\n\nSo\tshould\tyou\tuse\tGini\timpurity\tor\tentropy?\tThe\ttruth\tis,\tmost\tof\tthe\ttime\tit\tdoes\tnot\tmake\ta\tbig difference:\tthey\tlead\tto\tsimilar\ttrees.\tGini\timpurity\tis\tslightly\tfaster\tto\tcompute,\tso\tit\tis\ta\tgood\tdefault. However,\twhen\tthey\tdiffer,\tGini\timpurity\ttends\tto\tisolate\tthe\tmost\tfrequent\tclass\tin\tits\town\tbranch\tof\tthe tree,\twhile\tentropy\ttends\tto\tproduce\tslightly\tmore\tbalanced\ttrees.5",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "Regularization\tHyperparameters Decision\tTrees\tmake\tvery\tfew\tassumptions\tabout\tthe\ttraining\tdata\t(as\topposed\tto\tlinear\tmodels,\twhich obviously\tassume\tthat\tthe\tdata\tis\tlinear,\tfor\texample).\tIf\tleft\tunconstrained,\tthe\ttree\tstructure\twill\tadapt itself\tto\tthe\ttraining\tdata,\tfitting\tit\tvery\tclosely,\tand\tmost\tlikely\toverfitting\tit.\tSuch\ta\tmodel\tis\toften\tcalled a\tnonparametric\tmodel,\tnot\tbecause\tit\tdoes\tnot\thave\tany\tparameters\t(it\toften\thas\ta\tlot)\tbut\tbecause\tthe number\tof\tparameters\tis\tnot\tdetermined\tprior\tto\ttraining,\tso\tthe\tmodel\tstructure\tis\tfree\tto\tstick\tclosely\tto the\tdata.\tIn\tcontrast,\ta\tparametric\tmodel\tsuch\tas\ta\tlinear\tmodel\thas\ta\tpredetermined\tnumber\tof parameters,\tso\tits\tdegree\tof\tfreedom\tis\tlimited,\treducing\tthe\trisk\tof\toverfitting\t(but\tincreasing\tthe\trisk\tof underfitting).\n\nTo\tavoid\toverfitting\tthe\ttraining\tdata,\tyou\tneed\tto\trestrict\tthe\tDecision\tTree’s\tfreedom\tduring\ttraining.\tAs you\tknow\tby\tnow,\tthis\tis\tcalled\tregularization.\tThe\tregularization\thyperparameters\tdepend\ton\tthe algorithm\tused,\tbut\tgenerally\tyou\tcan\tat\tleast\trestrict\tthe\tmaximum\tdepth\tof\tthe\tDecision\tTree.\tIn\tScikit- Learn,\tthis\tis\tcontrolled\tby\tthe\tmax_depth\thyperparameter\t(the\tdefault\tvalue\tis\tNone,\twhich\tmeans unlimited).\tReducing\tmax_depth\twill\tregularize\tthe\tmodel\tand\tthus\treduce\tthe\trisk\tof\toverfitting.\n\nThe\tDecisionTreeClassifier\tclass\thas\ta\tfew\tother\tparameters\tthat\tsimilarly\trestrict\tthe\tshape\tof\tthe Decision\tTree:\tmin_samples_split\t(the\tminimum\tnumber\tof\tsamples\ta\tnode\tmust\thave\tbefore\tit\tcan\tbe split),\tmin_samples_leaf\t(the\tminimum\tnumber\tof\tsamples\ta\tleaf\tnode\tmust\thave), min_weight_fraction_leaf\t(same\tas\tmin_samples_leaf\tbut\texpressed\tas\ta\tfraction\tof\tthe\ttotal number\tof\tweighted\tinstances),\tmax_leaf_nodes\t(maximum\tnumber\tof\tleaf\tnodes),\tand\tmax_features (maximum\tnumber\tof\tfeatures\tthat\tare\tevaluated\tfor\tsplitting\tat\teach\tnode).\tIncreasing\tmin_* hyperparameters\tor\treducing\tmax_*\thyperparameters\twill\tregularize\tthe\tmodel.\n\nNOTE\n\nOther\talgorithms\twork\tby\tfirst\ttraining\tthe\tDecision\tTree\twithout\trestrictions,\tthen\tpruning\t(deleting)\tunnecessary\tnodes.\tA\tnode whose\tchildren\tare\tall\tleaf\tnodes\tis\tconsidered\tunnecessary\tif\tthe\tpurity\timprovement\tit\tprovides\tis\tnot\tstatistically\tsignificant. Standard\tstatistical\ttests,\tsuch\tas\tthe\tχ2\ttest,\tare\tused\tto\testimate\tthe\tprobability\tthat\tthe\timprovement\tis\tpurely\tthe\tresult\tof chance\t(which\tis\tcalled\tthe\tnull\thypothesis).\tIf\tthis\tprobability,\tcalled\tthe\tp-value,\tis\thigher\tthan\ta\tgiven\tthreshold\t(typically\t5%, controlled\tby\ta\thyperparameter),\tthen\tthe\tnode\tis\tconsidered\tunnecessary\tand\tits\tchildren\tare\tdeleted.\tThe\tpruning\tcontinues\tuntil all\tunnecessary\tnodes\thave\tbeen\tpruned.\n\nFigure\t6-3\tshows\ttwo\tDecision\tTrees\ttrained\ton\tthe\tmoons\tdataset\t(introduced\tin\tChapter\t5).\tOn\tthe\tleft, the\tDecision\tTree\tis\ttrained\twith\tthe\tdefault\thyperparameters\t(i.e.,\tno\trestrictions),\tand\ton\tthe\tright\tthe Decision\tTree\tis\ttrained\twith\tmin_samples_leaf=4.\tIt\tis\tquite\tobvious\tthat\tthe\tmodel\ton\tthe\tleft\tis overfitting,\tand\tthe\tmodel\ton\tthe\tright\twill\tprobably\tgeneralize\tbetter.",
      "content_length": 3028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Figure\t6-3.\tRegularization\tusing\tmin_samples_leaf",
      "content_length": 49,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Regression Decision\tTrees\tare\talso\tcapable\tof\tperforming\tregression\ttasks.\tLet’s\tbuild\ta\tregression\ttree\tusing\tScikit- Learn’s\tDecisionTreeRegressor\tclass,\ttraining\tit\ton\ta\tnoisy\tquadratic\tdataset\twith\tmax_depth=2:\n\nfrom\tsklearn.tree\timport\tDecisionTreeRegressor\n\ntree_reg\t=\tDecisionTreeRegressor(max_depth=2) tree_reg.fit(X,\ty)\n\nThe\tresulting\ttree\tis\trepresented\ton\tFigure\t6-4.\n\nFigure\t6-4.\tA\tDecision\tTree\tfor\tregression\n\nThis\ttree\tlooks\tvery\tsimilar\tto\tthe\tclassification\ttree\tyou\tbuilt\tearlier.\tThe\tmain\tdifference\tis\tthat\tinstead of\tpredicting\ta\tclass\tin\teach\tnode,\tit\tpredicts\ta\tvalue.\tFor\texample,\tsuppose\tyou\twant\tto\tmake\ta prediction\tfor\ta\tnew\tinstance\twith\tx1\t=\t0.6.\tYou\ttraverse\tthe\ttree\tstarting\tat\tthe\troot,\tand\tyou\teventually reach\tthe\tleaf\tnode\tthat\tpredicts\tvalue=0.1106.\tThis\tprediction\tis\tsimply\tthe\taverage\ttarget\tvalue\tof\tthe 110\ttraining\tinstances\tassociated\tto\tthis\tleaf\tnode.\tThis\tprediction\tresults\tin\ta\tMean\tSquared\tError (MSE)\tequal\tto\t0.0151\tover\tthese\t110\tinstances.\n\nThis\tmodel’s\tpredictions\tare\trepresented\ton\tthe\tleft\tof\tFigure\t6-5.\tIf\tyou\tset\tmax_depth=3,\tyou\tget\tthe predictions\trepresented\ton\tthe\tright.\tNotice\thow\tthe\tpredicted\tvalue\tfor\teach\tregion\tis\talways\tthe\taverage target\tvalue\tof\tthe\tinstances\tin\tthat\tregion.\tThe\talgorithm\tsplits\teach\tregion\tin\ta\tway\tthat\tmakes\tmost training\tinstances\tas\tclose\tas\tpossible\tto\tthat\tpredicted\tvalue.",
      "content_length": 1375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "Figure\t6-5.\tPredictions\tof\ttwo\tDecision\tTree\tregression\tmodels\n\nThe\tCART\talgorithm\tworks\tmostly\tthe\tsame\tway\tas\tearlier,\texcept\tthat\tinstead\tof\ttrying\tto\tsplit\tthe training\tset\tin\ta\tway\tthat\tminimizes\timpurity,\tit\tnow\ttries\tto\tsplit\tthe\ttraining\tset\tin\ta\tway\tthat\tminimizes the\tMSE.\tEquation\t6-4\tshows\tthe\tcost\tfunction\tthat\tthe\talgorithm\ttries\tto\tminimize.\n\nEquation\t6-4.\tCART\tcost\tfunction\tfor\tregression\n\nJust\tlike\tfor\tclassification\ttasks,\tDecision\tTrees\tare\tprone\tto\toverfitting\twhen\tdealing\twith\tregression tasks.\tWithout\tany\tregularization\t(i.e.,\tusing\tthe\tdefault\thyperparameters),\tyou\tget\tthe\tpredictions\ton\tthe left\tof\tFigure\t6-6.\tIt\tis\tobviously\toverfitting\tthe\ttraining\tset\tvery\tbadly.\tJust\tsetting min_samples_leaf=10\tresults\tin\ta\tmuch\tmore\treasonable\tmodel,\trepresented\ton\tthe\tright\tof\tFigure\t6-6.\n\nFigure\t6-6.\tRegularizing\ta\tDecision\tTree\tregressor",
      "content_length": 863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Instability Hopefully\tby\tnow\tyou\tare\tconvinced\tthat\tDecision\tTrees\thave\ta\tlot\tgoing\tfor\tthem:\tthey\tare\tsimple\tto understand\tand\tinterpret,\teasy\tto\tuse,\tversatile,\tand\tpowerful.\tHowever\tthey\tdo\thave\ta\tfew\tlimitations. First,\tas\tyou\tmay\thave\tnoticed,\tDecision\tTrees\tlove\torthogonal\tdecision\tboundaries\t(all\tsplits\tare perpendicular\tto\tan\taxis),\twhich\tmakes\tthem\tsensitive\tto\ttraining\tset\trotation.\tFor\texample,\tFigure\t6-7 shows\ta\tsimple\tlinearly\tseparable\tdataset:\ton\tthe\tleft,\ta\tDecision\tTree\tcan\tsplit\tit\teasily,\twhile\ton\tthe right,\tafter\tthe\tdataset\tis\trotated\tby\t45°,\tthe\tdecision\tboundary\tlooks\tunnecessarily\tconvoluted.\tAlthough both\tDecision\tTrees\tfit\tthe\ttraining\tset\tperfectly,\tit\tis\tvery\tlikely\tthat\tthe\tmodel\ton\tthe\tright\twill\tnot generalize\twell.\tOne\tway\tto\tlimit\tthis\tproblem\tis\tto\tuse\tPCA\t(see\tChapter\t8),\twhich\toften\tresults\tin\ta better\torientation\tof\tthe\ttraining\tdata.\n\nFigure\t6-7.\tSensitivity\tto\ttraining\tset\trotation\n\nMore\tgenerally,\tthe\tmain\tissue\twith\tDecision\tTrees\tis\tthat\tthey\tare\tvery\tsensitive\tto\tsmall\tvariations\tin the\ttraining\tdata.\tFor\texample,\tif\tyou\tjust\tremove\tthe\twidest\tIris-Versicolor\tfrom\tthe\tiris\ttraining\tset\t(the one\twith\tpetals\t4.8\tcm\tlong\tand\t1.8\tcm\twide)\tand\ttrain\ta\tnew\tDecision\tTree,\tyou\tmay\tget\tthe\tmodel represented\tin\tFigure\t6-8.\tAs\tyou\tcan\tsee,\tit\tlooks\tvery\tdifferent\tfrom\tthe\tprevious\tDecision\tTree (Figure\t6-2).\tActually,\tsince\tthe\ttraining\talgorithm\tused\tby\tScikit-Learn\tis\tstochastic6\tyou\tmay\tget\tvery different\tmodels\teven\ton\tthe\tsame\ttraining\tdata\t(unless\tyou\tset\tthe\trandom_state\thyperparameter).\n\nFigure\t6-8.\tSensitivity\tto\ttraining\tset\tdetails",
      "content_length": 1600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "Random\tForests\tcan\tlimit\tthis\tinstability\tby\taveraging\tpredictions\tover\tmany\ttrees,\tas\twe\twill\tsee\tin\tthe next\tchapter.",
      "content_length": 119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Exercises\n\n1.\t What\tis\tthe\tapproximate\tdepth\tof\ta\tDecision\tTree\ttrained\t(without\trestrictions)\ton\ta\ttraining\tset\twith 1\tmillion\tinstances?\n\n2.\t Is\ta\tnode’s\tGini\timpurity\tgenerally\tlower\tor\tgreater\tthan\tits\tparent’s?\tIs\tit\tgenerally\tlower/greater, or\talways\tlower/greater?\n\n3.\t If\ta\tDecision\tTree\tis\toverfitting\tthe\ttraining\tset,\tis\tit\ta\tgood\tidea\tto\ttry\tdecreasing\tmax_depth?\n\n4.\t If\ta\tDecision\tTree\tis\tunderfitting\tthe\ttraining\tset,\tis\tit\ta\tgood\tidea\tto\ttry\tscaling\tthe\tinput\tfeatures?\n\n5.\t If\tit\ttakes\tone\thour\tto\ttrain\ta\tDecision\tTree\ton\ta\ttraining\tset\tcontaining\t1\tmillion\tinstances,\troughly how\tmuch\ttime\twill\tit\ttake\tto\ttrain\tanother\tDecision\tTree\ton\ta\ttraining\tset\tcontaining\t10\tmillion instances?\n\n6.\t If\tyour\ttraining\tset\tcontains\t100,000\tinstances,\twill\tsetting\tpresort=True\tspeed\tup\ttraining?\n\n7.\t Train\tand\tfine-tune\ta\tDecision\tTree\tfor\tthe\tmoons\tdataset.\n\na.\t Generate\ta\tmoons\tdataset\tusing\tmake_moons(n_samples=10000,\tnoise=0.4).\n\nb.\t Split\tit\tinto\ta\ttraining\tset\tand\ta\ttest\tset\tusing\ttrain_test_split().\n\nc.\t Use\tgrid\tsearch\twith\tcross-validation\t(with\tthe\thelp\tof\tthe\tGridSearchCV\tclass)\tto\tfind\tgood\n\nhyperparameter\tvalues\tfor\ta\tDecisionTreeClassifier.\tHint:\ttry\tvarious\tvalues\tfor max_leaf_nodes.\n\nd.\t Train\tit\ton\tthe\tfull\ttraining\tset\tusing\tthese\thyperparameters,\tand\tmeasure\tyour\tmodel’s\n\nperformance\ton\tthe\ttest\tset.\tYou\tshould\tget\troughly\t85%\tto\t87%\taccuracy.\n\n8.\t Grow\ta\tforest.\n\na.\t Continuing\tthe\tprevious\texercise,\tgenerate\t1,000\tsubsets\tof\tthe\ttraining\tset,\teach\tcontaining\t100 instances\tselected\trandomly.\tHint:\tyou\tcan\tuse\tScikit-Learn’s\tShuffleSplit\tclass\tfor\tthis.\n\nb.\t Train\tone\tDecision\tTree\ton\teach\tsubset,\tusing\tthe\tbest\thyperparameter\tvalues\tfound\tabove. Evaluate\tthese\t1,000\tDecision\tTrees\ton\tthe\ttest\tset.\tSince\tthey\twere\ttrained\ton\tsmaller\tsets, these\tDecision\tTrees\twill\tlikely\tperform\tworse\tthan\tthe\tfirst\tDecision\tTree,\tachieving\tonly about\t80%\taccuracy.\n\nc.\t Now\tcomes\tthe\tmagic.\tFor\teach\ttest\tset\tinstance,\tgenerate\tthe\tpredictions\tof\tthe\t1,000\tDecision Trees,\tand\tkeep\tonly\tthe\tmost\tfrequent\tprediction\t(you\tcan\tuse\tSciPy’s\tmode()\tfunction\tfor this).\tThis\tgives\tyou\tmajority-vote\tpredictions\tover\tthe\ttest\tset.\n\nd.\t Evaluate\tthese\tpredictions\ton\tthe\ttest\tset:\tyou\tshould\tobtain\ta\tslightly\thigher\taccuracy\tthan\tyour first\tmodel\t(about\t0.5\tto\t1.5%\thigher).\tCongratulations,\tyou\thave\ttrained\ta\tRandom\tForest classifier!",
      "content_length": 2361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "Solutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nGraphviz\tis\tan\topen\tsource\tgraph\tvisualization\tsoftware\tpackage,\tavailable\tat\thttp://www.graphviz.org/.\n\n2\n\nP\tis\tthe\tset\tof\tproblems\tthat\tcan\tbe\tsolved\tin\tpolynomial\ttime.\tNP\tis\tthe\tset\tof\tproblems\twhose\tsolutions\tcan\tbe\tverified\tin\tpolynomial\ttime. An\tNP-Hard\tproblem\tis\ta\tproblem\tto\twhich\tany\tNP\tproblem\tcan\tbe\treduced\tin\tpolynomial\ttime.\tAn\tNP-Complete\tproblem\tis\tboth\tNP\tand NP-Hard.\tA\tmajor\topen\tmathematical\tquestion\tis\twhether\tor\tnot\tP\t=\tNP.\tIf\tP\t≠\tNP\t(which\tseems\tlikely),\tthen\tno\tpolynomial\talgorithm\twill ever\tbe\tfound\tfor\tany\tNP-Complete\tproblem\t(except\tperhaps\ton\ta\tquantum\tcomputer).\n\n3\n\nlog2\tis\tthe\tbinary\tlogarithm.\tIt\tis\tequal\tto\tlog2(m)\t=\tlog(m)\t/\tlog(2).\n\n4\n\nA\treduction\tof\tentropy\tis\toften\tcalled\tan\tinformation\tgain.\n\n5\n\nSee\tSebastian\tRaschka’s\tinteresting\tanalysis\tfor\tmore\tdetails.\n\n6\n\nIt\trandomly\tselects\tthe\tset\tof\tfeatures\tto\tevaluate\tat\teach\tnode.",
      "content_length": 935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Chapter\t7.\tEnsemble\tLearning\tand\tRandom Forests\n\nSuppose\tyou\task\ta\tcomplex\tquestion\tto\tthousands\tof\trandom\tpeople,\tthen\taggregate\ttheir\tanswers.\tIn many\tcases\tyou\twill\tfind\tthat\tthis\taggregated\tanswer\tis\tbetter\tthan\tan\texpert’s\tanswer.\tThis\tis\tcalled\tthe wisdom\tof\tthe\tcrowd.\tSimilarly,\tif\tyou\taggregate\tthe\tpredictions\tof\ta\tgroup\tof\tpredictors\t(such\tas classifiers\tor\tregressors),\tyou\twill\toften\tget\tbetter\tpredictions\tthan\twith\tthe\tbest\tindividual\tpredictor.\tA group\tof\tpredictors\tis\tcalled\tan\tensemble;\tthus,\tthis\ttechnique\tis\tcalled\tEnsemble\tLearning,\tand\tan Ensemble\tLearning\talgorithm\tis\tcalled\tan\tEnsemble\tmethod.\n\nFor\texample,\tyou\tcan\ttrain\ta\tgroup\tof\tDecision\tTree\tclassifiers,\teach\ton\ta\tdifferent\trandom\tsubset\tof\tthe training\tset.\tTo\tmake\tpredictions,\tyou\tjust\tobtain\tthe\tpredictions\tof\tall\tindividual\ttrees,\tthen\tpredict\tthe class\tthat\tgets\tthe\tmost\tvotes\t(see\tthe\tlast\texercise\tin\tChapter\t6).\tSuch\tan\tensemble\tof\tDecision\tTrees\tis called\ta\tRandom\tForest,\tand\tdespite\tits\tsimplicity,\tthis\tis\tone\tof\tthe\tmost\tpowerful\tMachine\tLearning algorithms\tavailable\ttoday.\n\nMoreover,\tas\twe\tdiscussed\tin\tChapter\t2,\tyou\twill\toften\tuse\tEnsemble\tmethods\tnear\tthe\tend\tof\ta\tproject, once\tyou\thave\talready\tbuilt\ta\tfew\tgood\tpredictors,\tto\tcombine\tthem\tinto\tan\teven\tbetter\tpredictor.\tIn\tfact, the\twinning\tsolutions\tin\tMachine\tLearning\tcompetitions\toften\tinvolve\tseveral\tEnsemble\tmethods\t(most famously\tin\tthe\tNetflix\tPrize\tcompetition).\n\nIn\tthis\tchapter\twe\twill\tdiscuss\tthe\tmost\tpopular\tEnsemble\tmethods,\tincluding\tbagging,\tboosting, stacking,\tand\ta\tfew\tothers.\tWe\twill\talso\texplore\tRandom\tForests.",
      "content_length": 1590,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "Voting\tClassifiers Suppose\tyou\thave\ttrained\ta\tfew\tclassifiers,\teach\tone\tachieving\tabout\t80%\taccuracy.\tYou\tmay\thave\ta Logistic\tRegression\tclassifier,\tan\tSVM\tclassifier,\ta\tRandom\tForest\tclassifier,\ta\tK-Nearest\tNeighbors classifier,\tand\tperhaps\ta\tfew\tmore\t(see\tFigure\t7-1).\n\nFigure\t7-1.\tTraining\tdiverse\tclassifiers\n\nA\tvery\tsimple\tway\tto\tcreate\tan\teven\tbetter\tclassifier\tis\tto\taggregate\tthe\tpredictions\tof\teach\tclassifier\tand predict\tthe\tclass\tthat\tgets\tthe\tmost\tvotes.\tThis\tmajority-vote\tclassifier\tis\tcalled\ta\thard\tvoting\tclassifier (see\tFigure\t7-2).\n\nFigure\t7-2.\tHard\tvoting\tclassifier\tpredictions\n\nSomewhat\tsurprisingly,\tthis\tvoting\tclassifier\toften\tachieves\ta\thigher\taccuracy\tthan\tthe\tbest\tclassifier\tin the\tensemble.\tIn\tfact,\teven\tif\teach\tclassifier\tis\ta\tweak\tlearner\t(meaning\tit\tdoes\tonly\tslightly\tbetter\tthan random\tguessing),\tthe\tensemble\tcan\tstill\tbe\ta\tstrong\tlearner\t(achieving\thigh\taccuracy),\tprovided\tthere are\ta\tsufficient\tnumber\tof\tweak\tlearners\tand\tthey\tare\tsufficiently\tdiverse.\n\nHow\tis\tthis\tpossible?\tThe\tfollowing\tanalogy\tcan\thelp\tshed\tsome\tlight\ton\tthis\tmystery.\tSuppose\tyou\thave",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "a\tslightly\tbiased\tcoin\tthat\thas\ta\t51%\tchance\tof\tcoming\tup\theads,\tand\t49%\tchance\tof\tcoming\tup\ttails.\tIf you\ttoss\tit\t1,000\ttimes,\tyou\twill\tgenerally\tget\tmore\tor\tless\t510\theads\tand\t490\ttails,\tand\thence\ta\tmajority of\theads.\tIf\tyou\tdo\tthe\tmath,\tyou\twill\tfind\tthat\tthe\tprobability\tof\tobtaining\ta\tmajority\tof\theads\tafter\t1,000 tosses\tis\tclose\tto\t75%.\tThe\tmore\tyou\ttoss\tthe\tcoin,\tthe\thigher\tthe\tprobability\t(e.g.,\twith\t10,000\ttosses,\tthe probability\tclimbs\tover\t97%).\tThis\tis\tdue\tto\tthe\tlaw\tof\tlarge\tnumbers:\tas\tyou\tkeep\ttossing\tthe\tcoin,\tthe ratio\tof\theads\tgets\tcloser\tand\tcloser\tto\tthe\tprobability\tof\theads\t(51%).\tFigure\t7-3\tshows\t10\tseries\tof biased\tcoin\ttosses.\tYou\tcan\tsee\tthat\tas\tthe\tnumber\tof\ttosses\tincreases,\tthe\tratio\tof\theads\tapproaches\t51%. Eventually\tall\t10\tseries\tend\tup\tso\tclose\tto\t51%\tthat\tthey\tare\tconsistently\tabove\t50%.\n\nFigure\t7-3.\tThe\tlaw\tof\tlarge\tnumbers\n\nSimilarly,\tsuppose\tyou\tbuild\tan\tensemble\tcontaining\t1,000\tclassifiers\tthat\tare\tindividually\tcorrect\tonly 51%\tof\tthe\ttime\t(barely\tbetter\tthan\trandom\tguessing).\tIf\tyou\tpredict\tthe\tmajority\tvoted\tclass,\tyou\tcan hope\tfor\tup\tto\t75%\taccuracy!\tHowever,\tthis\tis\tonly\ttrue\tif\tall\tclassifiers\tare\tperfectly\tindependent, making\tuncorrelated\terrors,\twhich\tis\tclearly\tnot\tthe\tcase\tsince\tthey\tare\ttrained\ton\tthe\tsame\tdata.\tThey\tare likely\tto\tmake\tthe\tsame\ttypes\tof\terrors,\tso\tthere\twill\tbe\tmany\tmajority\tvotes\tfor\tthe\twrong\tclass,\treducing the\tensemble’s\taccuracy.\n\nTIP\n\nEnsemble\tmethods\twork\tbest\twhen\tthe\tpredictors\tare\tas\tindependent\tfrom\tone\tanother\tas\tpossible.\tOne\tway\tto\tget\tdiverse classifiers\tis\tto\ttrain\tthem\tusing\tvery\tdifferent\talgorithms.\tThis\tincreases\tthe\tchance\tthat\tthey\twill\tmake\tvery\tdifferent\ttypes\tof errors,\timproving\tthe\tensemble’s\taccuracy.\n\nThe\tfollowing\tcode\tcreates\tand\ttrains\ta\tvoting\tclassifier\tin\tScikit-Learn,\tcomposed\tof\tthree\tdiverse classifiers\t(the\ttraining\tset\tis\tthe\tmoons\tdataset,\tintroduced\tin\tChapter\t5):\n\nfrom\tsklearn.ensemble\timport\tRandomForestClassifier from\tsklearn.ensemble\timport\tVotingClassifier from\tsklearn.linear_model\timport\tLogisticRegression from\tsklearn.svm\timport\tSVC\n\nlog_clf\t=\tLogisticRegression() rnd_clf\t=\tRandomForestClassifier() svm_clf\t=\tSVC()\n\nvoting_clf\t=\tVotingClassifier( \t\t\t\testimators=[('lr',\tlog_clf),\t('rf',\trnd_clf),\t('svc',\tsvm_clf)],",
      "content_length": 2263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "voting='hard') voting_clf.fit(X_train,\ty_train)\n\nLet’s\tlook\tat\teach\tclassifier’s\taccuracy\ton\tthe\ttest\tset:\n\n>>>\tfrom\tsklearn.metrics\timport\taccuracy_score >>>\tfor\tclf\tin\t(log_clf,\trnd_clf,\tsvm_clf,\tvoting_clf): ...\t\t\t\t\tclf.fit(X_train,\ty_train) ...\t\t\t\t\ty_pred\t=\tclf.predict(X_test) ...\t\t\t\t\tprint(clf.__class__.__name__,\taccuracy_score(y_test,\ty_pred)) ... LogisticRegression\t0.864 RandomForestClassifier\t0.872 SVC\t0.888 VotingClassifier\t0.896\n\nThere\tyou\thave\tit!\tThe\tvoting\tclassifier\tslightly\toutperforms\tall\tthe\tindividual\tclassifiers.\n\nIf\tall\tclassifiers\tare\table\tto\testimate\tclass\tprobabilities\t(i.e.,\tthey\thave\ta\tpredict_proba()\tmethod), then\tyou\tcan\ttell\tScikit-Learn\tto\tpredict\tthe\tclass\twith\tthe\thighest\tclass\tprobability,\taveraged\tover\tall\tthe individual\tclassifiers.\tThis\tis\tcalled\tsoft\tvoting.\tIt\toften\tachieves\thigher\tperformance\tthan\thard\tvoting because\tit\tgives\tmore\tweight\tto\thighly\tconfident\tvotes.\tAll\tyou\tneed\tto\tdo\tis\treplace\tvoting=\"hard\" with\tvoting=\"soft\"\tand\tensure\tthat\tall\tclassifiers\tcan\testimate\tclass\tprobabilities.\tThis\tis\tnot\tthe\tcase of\tthe\tSVC\tclass\tby\tdefault,\tso\tyou\tneed\tto\tset\tits\tprobability\thyperparameter\tto\tTrue\t(this\twill\tmake the\tSVC\tclass\tuse\tcross-validation\tto\testimate\tclass\tprobabilities,\tslowing\tdown\ttraining,\tand\tit\twill\tadd a\tpredict_proba()\tmethod).\tIf\tyou\tmodify\tthe\tpreceding\tcode\tto\tuse\tsoft\tvoting,\tyou\twill\tfind\tthat\tthe voting\tclassifier\tachieves\tover\t91%\taccuracy!",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Bagging\tand\tPasting One\tway\tto\tget\ta\tdiverse\tset\tof\tclassifiers\tis\tto\tuse\tvery\tdifferent\ttraining\talgorithms,\tas\tjust\tdiscussed. Another\tapproach\tis\tto\tuse\tthe\tsame\ttraining\talgorithm\tfor\tevery\tpredictor,\tbut\tto\ttrain\tthem\ton\tdifferent random\tsubsets\tof\tthe\ttraining\tset.\tWhen\tsampling\tis\tperformed\twith\treplacement,\tthis\tmethod\tis\tcalled bagging1\t(short\tfor\tbootstrap\taggregating2).\tWhen\tsampling\tis\tperformed\twithout\treplacement,\tit\tis called\tpasting.3\n\nIn\tother\twords,\tboth\tbagging\tand\tpasting\tallow\ttraining\tinstances\tto\tbe\tsampled\tseveral\ttimes\tacross multiple\tpredictors,\tbut\tonly\tbagging\tallows\ttraining\tinstances\tto\tbe\tsampled\tseveral\ttimes\tfor\tthe\tsame predictor.\tThis\tsampling\tand\ttraining\tprocess\tis\trepresented\tin\tFigure\t7-4.\n\nFigure\t7-4.\tPasting/bagging\ttraining\tset\tsampling\tand\ttraining\n\nOnce\tall\tpredictors\tare\ttrained,\tthe\tensemble\tcan\tmake\ta\tprediction\tfor\ta\tnew\tinstance\tby\tsimply aggregating\tthe\tpredictions\tof\tall\tpredictors.\tThe\taggregation\tfunction\tis\ttypically\tthe\tstatistical\tmode (i.e.,\tthe\tmost\tfrequent\tprediction,\tjust\tlike\ta\thard\tvoting\tclassifier)\tfor\tclassification,\tor\tthe\taverage\tfor regression.\tEach\tindividual\tpredictor\thas\ta\thigher\tbias\tthan\tif\tit\twere\ttrained\ton\tthe\toriginal\ttraining\tset, but\taggregation\treduces\tboth\tbias\tand\tvariance.4\tGenerally,\tthe\tnet\tresult\tis\tthat\tthe\tensemble\thas\ta similar\tbias\tbut\ta\tlower\tvariance\tthan\ta\tsingle\tpredictor\ttrained\ton\tthe\toriginal\ttraining\tset.\n\nAs\tyou\tcan\tsee\tin\tFigure\t7-4,\tpredictors\tcan\tall\tbe\ttrained\tin\tparallel,\tvia\tdifferent\tCPU\tcores\tor\teven different\tservers.\tSimilarly,\tpredictions\tcan\tbe\tmade\tin\tparallel.\tThis\tis\tone\tof\tthe\treasons\twhy\tbagging and\tpasting\tare\tsuch\tpopular\tmethods:\tthey\tscale\tvery\twell.",
      "content_length": 1697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Bagging\tand\tPasting\tin\tScikit-Learn Scikit-Learn\toffers\ta\tsimple\tAPI\tfor\tboth\tbagging\tand\tpasting\twith\tthe\tBaggingClassifier\tclass\t(or BaggingRegressor\tfor\tregression).\tThe\tfollowing\tcode\ttrains\tan\tensemble\tof\t500\tDecision\tTree classifiers,5\teach\ttrained\ton\t100\ttraining\tinstances\trandomly\tsampled\tfrom\tthe\ttraining\tset\twith replacement\t(this\tis\tan\texample\tof\tbagging,\tbut\tif\tyou\twant\tto\tuse\tpasting\tinstead,\tjust\tset bootstrap=False).\tThe\tn_jobs\tparameter\ttells\tScikit-Learn\tthe\tnumber\tof\tCPU\tcores\tto\tuse\tfor\ttraining and\tpredictions\t(–1\ttells\tScikit-Learn\tto\tuse\tall\tavailable\tcores):\n\nfrom\tsklearn.ensemble\timport\tBaggingClassifier from\tsklearn.tree\timport\tDecisionTreeClassifier\n\nbag_clf\t=\tBaggingClassifier( \t\t\t\tDecisionTreeClassifier(),\tn_estimators=500, \t\t\t\tmax_samples=100,\tbootstrap=True,\tn_jobs=-1) bag_clf.fit(X_train,\ty_train) y_pred\t=\tbag_clf.predict(X_test)\n\nNOTE\n\nThe\tBaggingClassifier\tautomatically\tperforms\tsoft\tvoting\tinstead\tof\thard\tvoting\tif\tthe\tbase\tclassifier\tcan\testimate\tclass probabilities\t(i.e.,\tif\tit\thas\ta\tpredict_proba()\tmethod),\twhich\tis\tthe\tcase\twith\tDecision\tTrees\tclassifiers.\n\nFigure\t7-5\tcompares\tthe\tdecision\tboundary\tof\ta\tsingle\tDecision\tTree\twith\tthe\tdecision\tboundary\tof\ta bagging\tensemble\tof\t500\ttrees\t(from\tthe\tpreceding\tcode),\tboth\ttrained\ton\tthe\tmoons\tdataset.\tAs\tyou\tcan see,\tthe\tensemble’s\tpredictions\twill\tlikely\tgeneralize\tmuch\tbetter\tthan\tthe\tsingle\tDecision\tTree’s predictions:\tthe\tensemble\thas\ta\tcomparable\tbias\tbut\ta\tsmaller\tvariance\t(it\tmakes\troughly\tthe\tsame number\tof\terrors\ton\tthe\ttraining\tset,\tbut\tthe\tdecision\tboundary\tis\tless\tirregular).\n\nFigure\t7-5.\tA\tsingle\tDecision\tTree\tversus\ta\tbagging\tensemble\tof\t500\ttrees\n\nBootstrapping\tintroduces\ta\tbit\tmore\tdiversity\tin\tthe\tsubsets\tthat\teach\tpredictor\tis\ttrained\ton,\tso\tbagging ends\tup\twith\ta\tslightly\thigher\tbias\tthan\tpasting,\tbut\tthis\talso\tmeans\tthat\tpredictors\tend\tup\tbeing\tless correlated\tso\tthe\tensemble’s\tvariance\tis\treduced.\tOverall,\tbagging\toften\tresults\tin\tbetter\tmodels,\twhich explains\twhy\tit\tis\tgenerally\tpreferred.\tHowever,\tif\tyou\thave\tspare\ttime\tand\tCPU\tpower\tyou\tcan\tuse cross-validation\tto\tevaluate\tboth\tbagging\tand\tpasting\tand\tselect\tthe\tone\tthat\tworks\tbest.",
      "content_length": 2175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Out-of-Bag\tEvaluation With\tbagging,\tsome\tinstances\tmay\tbe\tsampled\tseveral\ttimes\tfor\tany\tgiven\tpredictor,\twhile\tothers\tmay\tnot be\tsampled\tat\tall.\tBy\tdefault\ta\tBaggingClassifier\tsamples\tm\ttraining\tinstances\twith\treplacement (bootstrap=True),\twhere\tm\tis\tthe\tsize\tof\tthe\ttraining\tset.\tThis\tmeans\tthat\tonly\tabout\t63%\tof\tthe\ttraining instances\tare\tsampled\ton\taverage\tfor\teach\tpredictor.6\tThe\tremaining\t37%\tof\tthe\ttraining\tinstances\tthat\tare not\tsampled\tare\tcalled\tout-of-bag\t(oob)\tinstances.\tNote\tthat\tthey\tare\tnot\tthe\tsame\t37%\tfor\tall\tpredictors.\n\nSince\ta\tpredictor\tnever\tsees\tthe\toob\tinstances\tduring\ttraining,\tit\tcan\tbe\tevaluated\ton\tthese\tinstances, without\tthe\tneed\tfor\ta\tseparate\tvalidation\tset\tor\tcross-validation.\tYou\tcan\tevaluate\tthe\tensemble\titself\tby averaging\tout\tthe\toob\tevaluations\tof\teach\tpredictor.\n\nIn\tScikit-Learn,\tyou\tcan\tset\toob_score=True\twhen\tcreating\ta\tBaggingClassifier\tto\trequest\tan automatic\toob\tevaluation\tafter\ttraining.\tThe\tfollowing\tcode\tdemonstrates\tthis.\tThe\tresulting\tevaluation score\tis\tavailable\tthrough\tthe\toob_score_\tvariable:\n\n>>>\tbag_clf\t=\tBaggingClassifier( ...\t\t\t\t\tDecisionTreeClassifier(),\tn_estimators=500, ...\t\t\t\t\tbootstrap=True,\tn_jobs=-1,\toob_score=True) ... >>>\tbag_clf.fit(X_train,\ty_train) >>>\tbag_clf.oob_score_ 0.90133333333333332\n\nAccording\tto\tthis\toob\tevaluation,\tthis\tBaggingClassifier\tis\tlikely\tto\tachieve\tabout\t90.1%\taccuracy\ton the\ttest\tset.\tLet’s\tverify\tthis:\n\n>>>\tfrom\tsklearn.metrics\timport\taccuracy_score >>>\ty_pred\t=\tbag_clf.predict(X_test) >>>\taccuracy_score(y_test,\ty_pred) 0.91200000000000003\n\nWe\tget\t91.2%\taccuracy\ton\tthe\ttest\tset\t—\tclose\tenough!\n\nThe\toob\tdecision\tfunction\tfor\teach\ttraining\tinstance\tis\talso\tavailable\tthrough\tthe oob_decision_function_\tvariable.\tIn\tthis\tcase\t(since\tthe\tbase\testimator\thas\ta\tpredict_proba() method)\tthe\tdecision\tfunction\treturns\tthe\tclass\tprobabilities\tfor\teach\ttraining\tinstance.\tFor\texample,\tthe oob\tevaluation\testimates\tthat\tthe\tsecond\ttraining\tinstance\thas\ta\t60.6%\tprobability\tof\tbelonging\tto\tthe positive\tclass\t(and\t39.4%\tof\tbelonging\tto\tthe\tpositive\tclass):\n\n>>>\tbag_clf.oob_decision_function_ array([[\t0.31746032,\t\t0.68253968], \t\t\t\t\t\t\t[\t0.34117647,\t\t0.65882353], \t\t\t\t\t\t\t[\t1.\t\t\t\t\t\t\t\t,\t\t0.\t\t\t\t\t\t\t\t], \t\t\t\t\t\t\t... \t\t\t\t\t\t\t[\t1.\t\t\t\t\t\t\t\t,\t\t0.\t\t\t\t\t\t\t\t], \t\t\t\t\t\t\t[\t0.03108808,\t\t0.96891192], \t\t\t\t\t\t\t[\t0.57291667,\t\t0.42708333]])",
      "content_length": 2314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "Random\tPatches\tand\tRandom\tSubspaces The\tBaggingClassifier\tclass\tsupports\tsampling\tthe\tfeatures\tas\twell.\tThis\tis\tcontrolled\tby\ttwo hyperparameters:\tmax_features\tand\tbootstrap_features.\tThey\twork\tthe\tsame\tway\tas\tmax_samples and\tbootstrap,\tbut\tfor\tfeature\tsampling\tinstead\tof\tinstance\tsampling.\tThus,\teach\tpredictor\twill\tbe trained\ton\ta\trandom\tsubset\tof\tthe\tinput\tfeatures.\n\nThis\tis\tparticularly\tuseful\twhen\tyou\tare\tdealing\twith\thigh-dimensional\tinputs\t(such\tas\timages).\tSampling both\ttraining\tinstances\tand\tfeatures\tis\tcalled\tthe\tRandom\tPatches\tmethod.7\tKeeping\tall\ttraining\tinstances (i.e.,\tbootstrap=False\tand\tmax_samples=1.0)\tbut\tsampling\tfeatures\t(i.e.,\tbootstrap_features=True and/or\tmax_features\tsmaller\tthan\t1.0)\tis\tcalled\tthe\tRandom\tSubspaces\tmethod.8\n\nSampling\tfeatures\tresults\tin\teven\tmore\tpredictor\tdiversity,\ttrading\ta\tbit\tmore\tbias\tfor\ta\tlower\tvariance.",
      "content_length": 864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Random\tForests As\twe\thave\tdiscussed,\ta\tRandom\tForest9\tis\tan\tensemble\tof\tDecision\tTrees,\tgenerally\ttrained\tvia\tthe bagging\tmethod\t(or\tsometimes\tpasting),\ttypically\twith\tmax_samples\tset\tto\tthe\tsize\tof\tthe\ttraining\tset. Instead\tof\tbuilding\ta\tBaggingClassifier\tand\tpassing\tit\ta\tDecisionTreeClassifier,\tyou\tcan\tinstead use\tthe\tRandomForestClassifier\tclass,\twhich\tis\tmore\tconvenient\tand\toptimized\tfor\tDecision\tTrees10 (similarly,\tthere\tis\ta\tRandomForestRegressor\tclass\tfor\tregression\ttasks).\tThe\tfollowing\tcode\ttrains\ta Random\tForest\tclassifier\twith\t500\ttrees\t(each\tlimited\tto\tmaximum\t16\tnodes),\tusing\tall\tavailable\tCPU cores:\n\nfrom\tsklearn.ensemble\timport\tRandomForestClassifier\n\nrnd_clf\t=\tRandomForestClassifier(n_estimators=500,\tmax_leaf_nodes=16,\tn_jobs=-1) rnd_clf.fit(X_train,\ty_train)\n\ny_pred_rf\t=\trnd_clf.predict(X_test)\n\nWith\ta\tfew\texceptions,\ta\tRandomForestClassifier\thas\tall\tthe\thyperparameters\tof\ta DecisionTreeClassifier\t(to\tcontrol\thow\ttrees\tare\tgrown),\tplus\tall\tthe\thyperparameters\tof\ta BaggingClassifier\tto\tcontrol\tthe\tensemble\titself.11\n\nThe\tRandom\tForest\talgorithm\tintroduces\textra\trandomness\twhen\tgrowing\ttrees;\tinstead\tof\tsearching\tfor the\tvery\tbest\tfeature\twhen\tsplitting\ta\tnode\t(see\tChapter\t6),\tit\tsearches\tfor\tthe\tbest\tfeature\tamong\ta random\tsubset\tof\tfeatures.\tThis\tresults\tin\ta\tgreater\ttree\tdiversity,\twhich\t(once\tagain)\ttrades\ta\thigher\tbias for\ta\tlower\tvariance,\tgenerally\tyielding\tan\toverall\tbetter\tmodel.\tThe\tfollowing\tBaggingClassifier\tis roughly\tequivalent\tto\tthe\tprevious\tRandomForestClassifier:\n\nbag_clf\t=\tBaggingClassifier( \t\t\t\tDecisionTreeClassifier(splitter=\"random\",\tmax_leaf_nodes=16), \t\t\t\tn_estimators=500,\tmax_samples=1.0,\tbootstrap=True,\tn_jobs=-1)",
      "content_length": 1682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "Extra-Trees When\tyou\tare\tgrowing\ta\ttree\tin\ta\tRandom\tForest,\tat\teach\tnode\tonly\ta\trandom\tsubset\tof\tthe\tfeatures\tis considered\tfor\tsplitting\t(as\tdiscussed\tearlier).\tIt\tis\tpossible\tto\tmake\ttrees\teven\tmore\trandom\tby\talso using\trandom\tthresholds\tfor\teach\tfeature\trather\tthan\tsearching\tfor\tthe\tbest\tpossible\tthresholds\t(like regular\tDecision\tTrees\tdo). A\tforest\tof\tsuch\textremely\trandom\ttrees\tis\tsimply\tcalled\tan\tExtremely\tRandomized\tTrees\tensemble12\t(or Extra-Trees\tfor\tshort).\tOnce\tagain,\tthis\ttrades\tmore\tbias\tfor\ta\tlower\tvariance.\tIt\talso\tmakes\tExtra-Trees much\tfaster\tto\ttrain\tthan\tregular\tRandom\tForests\tsince\tfinding\tthe\tbest\tpossible\tthreshold\tfor\teach\tfeature at\tevery\tnode\tis\tone\tof\tthe\tmost\ttime-consuming\ttasks\tof\tgrowing\ta\ttree.\n\nYou\tcan\tcreate\tan\tExtra-Trees\tclassifier\tusing\tScikit-Learn’s\tExtraTreesClassifier\tclass.\tIts\tAPI\tis identical\tto\tthe\tRandomForestClassifier\tclass.\tSimilarly,\tthe\tExtraTreesRegressor\tclass\thas\tthe same\tAPI\tas\tthe\tRandomForestRegressor\tclass.\n\nTIP\n\nIt\tis\thard\tto\ttell\tin\tadvance\twhether\ta\tRandomForestClassifier\twill\tperform\tbetter\tor\tworse\tthan\tan\tExtraTreesClassifier. Generally,\tthe\tonly\tway\tto\tknow\tis\tto\ttry\tboth\tand\tcompare\tthem\tusing\tcross-validation\t(and\ttuning\tthe\thyperparameters\tusing grid\tsearch).",
      "content_length": 1244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "Feature\tImportance Yet\tanother\tgreat\tquality\tof\tRandom\tForests\tis\tthat\tthey\tmake\tit\teasy\tto\tmeasure\tthe\trelative\timportance\tof each\tfeature.\tScikit-Learn\tmeasures\ta\tfeature’s\timportance\tby\tlooking\tat\thow\tmuch\tthe\ttree\tnodes\tthat\tuse that\tfeature\treduce\timpurity\ton\taverage\t(across\tall\ttrees\tin\tthe\tforest).\tMore\tprecisely,\tit\tis\ta\tweighted average,\twhere\teach\tnode’s\tweight\tis\tequal\tto\tthe\tnumber\tof\ttraining\tsamples\tthat\tare\tassociated\twith\tit (see\tChapter\t6).\n\nScikit-Learn\tcomputes\tthis\tscore\tautomatically\tfor\teach\tfeature\tafter\ttraining,\tthen\tit\tscales\tthe\tresults\tso that\tthe\tsum\tof\tall\timportances\tis\tequal\tto\t1.\tYou\tcan\taccess\tthe\tresult\tusing\tthe\tfeature_importances_ variable.\tFor\texample,\tthe\tfollowing\tcode\ttrains\ta\tRandomForestClassifier\ton\tthe\tiris\tdataset (introduced\tin\tChapter\t4)\tand\toutputs\teach\tfeature’s\timportance.\tIt\tseems\tthat\tthe\tmost\timportant\tfeatures are\tthe\tpetal\tlength\t(44%)\tand\twidth\t(42%),\twhile\tsepal\tlength\tand\twidth\tare\trather\tunimportant\tin comparison\t(11%\tand\t2%,\trespectively).\n\n>>>\tfrom\tsklearn.datasets\timport\tload_iris >>>\tiris\t=\tload_iris() >>>\trnd_clf\t=\tRandomForestClassifier(n_estimators=500,\tn_jobs=-1) >>>\trnd_clf.fit(iris[\"data\"],\tiris[\"target\"]) >>>\tfor\tname,\tscore\tin\tzip(iris[\"feature_names\"],\trnd_clf.feature_importances_): ...\t\t\t\t\tprint(name,\tscore) ... sepal\tlength\t(cm)\t0.112492250999 sepal\twidth\t(cm)\t0.0231192882825 petal\tlength\t(cm)\t0.441030464364 petal\twidth\t(cm)\t0.423357996355\n\nSimilarly,\tif\tyou\ttrain\ta\tRandom\tForest\tclassifier\ton\tthe\tMNIST\tdataset\t(introduced\tin\tChapter\t3)\tand plot\teach\tpixel’s\timportance,\tyou\tget\tthe\timage\trepresented\tin\tFigure\t7-6.\n\nFigure\t7-6.\tMNIST\tpixel\timportance\t(according\tto\ta\tRandom\tForest\tclassifier)",
      "content_length": 1693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "Random\tForests\tare\tvery\thandy\tto\tget\ta\tquick\tunderstanding\tof\twhat\tfeatures\tactually\tmatter,\tin\tparticular if\tyou\tneed\tto\tperform\tfeature\tselection.",
      "content_length": 148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "Boosting Boosting\t(originally\tcalled\thypothesis\tboosting)\trefers\tto\tany\tEnsemble\tmethod\tthat\tcan\tcombine\tseveral weak\tlearners\tinto\ta\tstrong\tlearner.\tThe\tgeneral\tidea\tof\tmost\tboosting\tmethods\tis\tto\ttrain\tpredictors sequentially,\teach\ttrying\tto\tcorrect\tits\tpredecessor.\tThere\tare\tmany\tboosting\tmethods\tavailable,\tbut\tby\tfar the\tmost\tpopular\tare\tAdaBoost13\t(short\tfor\tAdaptive\tBoosting)\tand\tGradient\tBoosting.\tLet’s\tstart\twith AdaBoost.",
      "content_length": 434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "AdaBoost One\tway\tfor\ta\tnew\tpredictor\tto\tcorrect\tits\tpredecessor\tis\tto\tpay\ta\tbit\tmore\tattention\tto\tthe\ttraining instances\tthat\tthe\tpredecessor\tunderfitted.\tThis\tresults\tin\tnew\tpredictors\tfocusing\tmore\tand\tmore\ton\tthe hard\tcases.\tThis\tis\tthe\ttechnique\tused\tby\tAdaBoost.\n\nFor\texample,\tto\tbuild\tan\tAdaBoost\tclassifier,\ta\tfirst\tbase\tclassifier\t(such\tas\ta\tDecision\tTree)\tis\ttrained and\tused\tto\tmake\tpredictions\ton\tthe\ttraining\tset.\tThe\trelative\tweight\tof\tmisclassified\ttraining\tinstances\tis then\tincreased.\tA\tsecond\tclassifier\tis\ttrained\tusing\tthe\tupdated\tweights\tand\tagain\tit\tmakes\tpredictions\ton the\ttraining\tset,\tweights\tare\tupdated,\tand\tso\ton\t(see\tFigure\t7-7).\n\nFigure\t7-7.\tAdaBoost\tsequential\ttraining\twith\tinstance\tweight\tupdates\n\nFigure\t7-8\tshows\tthe\tdecision\tboundaries\tof\tfive\tconsecutive\tpredictors\ton\tthe\tmoons\tdataset\t(in\tthis example,\teach\tpredictor\tis\ta\thighly\tregularized\tSVM\tclassifier\twith\tan\tRBF\tkernel14).\tThe\tfirst\tclassifier gets\tmany\tinstances\twrong,\tso\ttheir\tweights\tget\tboosted.\tThe\tsecond\tclassifier\ttherefore\tdoes\ta\tbetter\tjob on\tthese\tinstances,\tand\tso\ton.\tThe\tplot\ton\tthe\tright\trepresents\tthe\tsame\tsequence\tof\tpredictors\texcept\tthat the\tlearning\trate\tis\thalved\t(i.e.,\tthe\tmisclassified\tinstance\tweights\tare\tboosted\thalf\tas\tmuch\tat\tevery iteration).\tAs\tyou\tcan\tsee,\tthis\tsequential\tlearning\ttechnique\thas\tsome\tsimilarities\twith\tGradient\tDescent, except\tthat\tinstead\tof\ttweaking\ta\tsingle\tpredictor’s\tparameters\tto\tminimize\ta\tcost\tfunction,\tAdaBoost adds\tpredictors\tto\tthe\tensemble,\tgradually\tmaking\tit\tbetter.",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "Figure\t7-8.\tDecision\tboundaries\tof\tconsecutive\tpredictors\n\nOnce\tall\tpredictors\tare\ttrained,\tthe\tensemble\tmakes\tpredictions\tvery\tmuch\tlike\tbagging\tor\tpasting,\texcept that\tpredictors\thave\tdifferent\tweights\tdepending\ton\ttheir\toverall\taccuracy\ton\tthe\tweighted\ttraining\tset.\n\nWARNING\n\nThere\tis\tone\timportant\tdrawback\tto\tthis\tsequential\tlearning\ttechnique:\tit\tcannot\tbe\tparallelized\t(or\tonly\tpartially),\tsince\teach predictor\tcan\tonly\tbe\ttrained\tafter\tthe\tprevious\tpredictor\thas\tbeen\ttrained\tand\tevaluated.\tAs\ta\tresult,\tit\tdoes\tnot\tscale\tas\twell\tas bagging\tor\tpasting.\n\nLet’s\ttake\ta\tcloser\tlook\tat\tthe\tAdaBoost\talgorithm.\tEach\tinstance\tweight\tw(i)\tis\tinitially\tset\tto\t predictor\tis\ttrained\tand\tits\tweighted\terror\trate\tr1\tis\tcomputed\ton\tthe\ttraining\tset;\tsee\tEquation\t7-1.\n\nEquation\t7-1.\tWeighted\terror\trate\tof\tthe\tjth\tpredictor\n\nThe\tpredictor’s\tweight\tαj\tis\tthen\tcomputed\tusing\tEquation\t7-2,\twhere\tη\tis\tthe\tlearning\trate hyperparameter\t(defaults\tto\t1).15\tThe\tmore\taccurate\tthe\tpredictor\tis,\tthe\thigher\tits\tweight\twill\tbe.\tIf\tit\tis just\tguessing\trandomly,\tthen\tits\tweight\twill\tbe\tclose\tto\tzero.\tHowever,\tif\tit\tis\tmost\toften\twrong\t(i.e.,\tless accurate\tthan\trandom\tguessing),\tthen\tits\tweight\twill\tbe\tnegative.\n\nEquation\t7-2.\tPredictor\tweight\n\nNext\tthe\tinstance\tweights\tare\tupdated\tusing\tEquation\t7-3:\tthe\tmisclassified\tinstances\tare\tboosted.\n\n.\tA\tfirst",
      "content_length": 1342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "Equation\t7-3.\tWeight\tupdate\trule\n\nThen\tall\tthe\tinstance\tweights\tare\tnormalized\t(i.e.,\tdivided\tby\n\n).\n\nFinally,\ta\tnew\tpredictor\tis\ttrained\tusing\tthe\tupdated\tweights,\tand\tthe\twhole\tprocess\tis\trepeated\t(the\tnew predictor’s\tweight\tis\tcomputed,\tthe\tinstance\tweights\tare\tupdated,\tthen\tanother\tpredictor\tis\ttrained,\tand\tso on).\tThe\talgorithm\tstops\twhen\tthe\tdesired\tnumber\tof\tpredictors\tis\treached,\tor\twhen\ta\tperfect\tpredictor\tis found.\n\nTo\tmake\tpredictions,\tAdaBoost\tsimply\tcomputes\tthe\tpredictions\tof\tall\tthe\tpredictors\tand\tweighs\tthem using\tthe\tpredictor\tweights\tαj.\tThe\tpredicted\tclass\tis\tthe\tone\tthat\treceives\tthe\tmajority\tof\tweighted\tvotes (see\tEquation\t7-4).\n\nEquation\t7-4.\tAdaBoost\tpredictions\n\nScikit-Learn\tactually\tuses\ta\tmulticlass\tversion\tof\tAdaBoost\tcalled\tSAMME16\t(which\tstands\tfor Stagewise\tAdditive\tModeling\tusing\ta\tMulticlass\tExponential\tloss\tfunction).\tWhen\tthere\tare\tjust\ttwo classes,\tSAMME\tis\tequivalent\tto\tAdaBoost.\tMoreover,\tif\tthe\tpredictors\tcan\testimate\tclass\tprobabilities (i.e.,\tif\tthey\thave\ta\tpredict_proba()\tmethod),\tScikit-Learn\tcan\tuse\ta\tvariant\tof\tSAMME\tcalled SAMME.R\t(the\tR\tstands\tfor\t“Real”),\twhich\trelies\ton\tclass\tprobabilities\trather\tthan\tpredictions\tand generally\tperforms\tbetter.\n\nThe\tfollowing\tcode\ttrains\tan\tAdaBoost\tclassifier\tbased\ton\t200\tDecision\tStumps\tusing\tScikit-Learn’s AdaBoostClassifier\tclass\t(as\tyou\tmight\texpect,\tthere\tis\talso\tan\tAdaBoostRegressor\tclass).\tA Decision\tStump\tis\ta\tDecision\tTree\twith\tmax_depth=1\t—\tin\tother\twords,\ta\ttree\tcomposed\tof\ta\tsingle decision\tnode\tplus\ttwo\tleaf\tnodes.\tThis\tis\tthe\tdefault\tbase\testimator\tfor\tthe\tAdaBoostClassifier\tclass:\n\nfrom\tsklearn.ensemble\timport\tAdaBoostClassifier\n\nada_clf\t=\tAdaBoostClassifier( \t\t\t\tDecisionTreeClassifier(max_depth=1),\tn_estimators=200, \t\t\t\talgorithm=\"SAMME.R\",\tlearning_rate=0.5) ada_clf.fit(X_train,\ty_train)",
      "content_length": 1815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "TIP\n\nIf\tyour\tAdaBoost\tensemble\tis\toverfitting\tthe\ttraining\tset,\tyou\tcan\ttry\treducing\tthe\tnumber\tof\testimators\tor\tmore\tstrongly regularizing\tthe\tbase\testimator.",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "Gradient\tBoosting Another\tvery\tpopular\tBoosting\talgorithm\tis\tGradient\tBoosting.17\tJust\tlike\tAdaBoost,\tGradient\tBoosting works\tby\tsequentially\tadding\tpredictors\tto\tan\tensemble,\teach\tone\tcorrecting\tits\tpredecessor.\tHowever, instead\tof\ttweaking\tthe\tinstance\tweights\tat\tevery\titeration\tlike\tAdaBoost\tdoes,\tthis\tmethod\ttries\tto\tfit\tthe new\tpredictor\tto\tthe\tresidual\terrors\tmade\tby\tthe\tprevious\tpredictor.\n\nLet’s\tgo\tthrough\ta\tsimple\tregression\texample\tusing\tDecision\tTrees\tas\tthe\tbase\tpredictors\t(of\tcourse Gradient\tBoosting\talso\tworks\tgreat\twith\tregression\ttasks).\tThis\tis\tcalled\tGradient\tTree\tBoosting,\tor Gradient\tBoosted\tRegression\tTrees\t(GBRT).\tFirst,\tlet’s\tfit\ta\tDecisionTreeRegressor\tto\tthe\ttraining\tset (for\texample,\ta\tnoisy\tquadratic\ttraining\tset):\n\nfrom\tsklearn.tree\timport\tDecisionTreeRegressor\n\ntree_reg1\t=\tDecisionTreeRegressor(max_depth=2) tree_reg1.fit(X,\ty)\n\nNow\ttrain\ta\tsecond\tDecisionTreeRegressor\ton\tthe\tresidual\terrors\tmade\tby\tthe\tfirst\tpredictor:\n\ny2\t=\ty\t-\ttree_reg1.predict(X) tree_reg2\t=\tDecisionTreeRegressor(max_depth=2) tree_reg2.fit(X,\ty2)\n\nThen\twe\ttrain\ta\tthird\tregressor\ton\tthe\tresidual\terrors\tmade\tby\tthe\tsecond\tpredictor:\n\ny3\t=\ty2\t-\ttree_reg2.predict(X) tree_reg3\t=\tDecisionTreeRegressor(max_depth=2) tree_reg3.fit(X,\ty3)\n\nNow\twe\thave\tan\tensemble\tcontaining\tthree\ttrees.\tIt\tcan\tmake\tpredictions\ton\ta\tnew\tinstance\tsimply\tby adding\tup\tthe\tpredictions\tof\tall\tthe\ttrees:\n\ny_pred\t=\tsum(tree.predict(X_new)\tfor\ttree\tin\t(tree_reg1,\ttree_reg2,\ttree_reg3))\n\nFigure\t7-9\trepresents\tthe\tpredictions\tof\tthese\tthree\ttrees\tin\tthe\tleft\tcolumn,\tand\tthe\tensemble’s predictions\tin\tthe\tright\tcolumn.\tIn\tthe\tfirst\trow,\tthe\tensemble\thas\tjust\tone\ttree,\tso\tits\tpredictions\tare exactly\tthe\tsame\tas\tthe\tfirst\ttree’s\tpredictions.\tIn\tthe\tsecond\trow,\ta\tnew\ttree\tis\ttrained\ton\tthe\tresidual errors\tof\tthe\tfirst\ttree.\tOn\tthe\tright\tyou\tcan\tsee\tthat\tthe\tensemble’s\tpredictions\tare\tequal\tto\tthe\tsum\tof\tthe predictions\tof\tthe\tfirst\ttwo\ttrees.\tSimilarly,\tin\tthe\tthird\trow\tanother\ttree\tis\ttrained\ton\tthe\tresidual\terrors of\tthe\tsecond\ttree.\tYou\tcan\tsee\tthat\tthe\tensemble’s\tpredictions\tgradually\tget\tbetter\tas\ttrees\tare\tadded\tto the\tensemble.\n\nA\tsimpler\tway\tto\ttrain\tGBRT\tensembles\tis\tto\tuse\tScikit-Learn’s\tGradientBoostingRegressor\tclass. Much\tlike\tthe\tRandomForestRegressor\tclass,\tit\thas\thyperparameters\tto\tcontrol\tthe\tgrowth\tof\tDecision Trees\t(e.g.,\tmax_depth,\tmin_samples_leaf,\tand\tso\ton),\tas\twell\tas\thyperparameters\tto\tcontrol\tthe ensemble\ttraining,\tsuch\tas\tthe\tnumber\tof\ttrees\t(n_estimators).\tThe\tfollowing\tcode\tcreates\tthe\tsame ensemble\tas\tthe\tprevious\tone:",
      "content_length": 2549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "from\tsklearn.ensemble\timport\tGradientBoostingRegressor\n\ngbrt\t=\tGradientBoostingRegressor(max_depth=2,\tn_estimators=3,\tlearning_rate=1.0) gbrt.fit(X,\ty)\n\nFigure\t7-9.\tGradient\tBoosting\n\nThe\tlearning_rate\thyperparameter\tscales\tthe\tcontribution\tof\teach\ttree.\tIf\tyou\tset\tit\tto\ta\tlow\tvalue,\tsuch as\t0.1,\tyou\twill\tneed\tmore\ttrees\tin\tthe\tensemble\tto\tfit\tthe\ttraining\tset,\tbut\tthe\tpredictions\twill\tusually generalize\tbetter.\tThis\tis\ta\tregularization\ttechnique\tcalled\tshrinkage.\tFigure\t7-10\tshows\ttwo\tGBRT ensembles\ttrained\twith\ta\tlow\tlearning\trate:\tthe\tone\ton\tthe\tleft\tdoes\tnot\thave\tenough\ttrees\tto\tfit\tthe training\tset,\twhile\tthe\tone\ton\tthe\tright\thas\ttoo\tmany\ttrees\tand\toverfits\tthe\ttraining\tset.",
      "content_length": 688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Figure\t7-10.\tGBRT\tensembles\twith\tnot\tenough\tpredictors\t(left)\tand\ttoo\tmany\t(right)\n\nIn\torder\tto\tfind\tthe\toptimal\tnumber\tof\ttrees,\tyou\tcan\tuse\tearly\tstopping\t(see\tChapter\t4).\tA\tsimple\tway\tto implement\tthis\tis\tto\tuse\tthe\tstaged_predict()\tmethod:\tit\treturns\tan\titerator\tover\tthe\tpredictions\tmade by\tthe\tensemble\tat\teach\tstage\tof\ttraining\t(with\tone\ttree,\ttwo\ttrees,\tetc.).\tThe\tfollowing\tcode\ttrains\ta GBRT\tensemble\twith\t120\ttrees,\tthen\tmeasures\tthe\tvalidation\terror\tat\teach\tstage\tof\ttraining\tto\tfind\tthe optimal\tnumber\tof\ttrees,\tand\tfinally\ttrains\tanother\tGBRT\tensemble\tusing\tthe\toptimal\tnumber\tof\ttrees:\n\nimport\tnumpy\tas\tnp from\tsklearn.model_selection\timport\ttrain_test_split from\tsklearn.metrics\timport\tmean_squared_error\n\nX_train,\tX_val,\ty_train,\ty_val\t=\ttrain_test_split(X,\ty)\n\ngbrt\t=\tGradientBoostingRegressor(max_depth=2,\tn_estimators=120) gbrt.fit(X_train,\ty_train)\n\nerrors\t=\t[mean_squared_error(y_val,\ty_pred) \t\t\t\t\t\t\t\t\t\tfor\ty_pred\tin\tgbrt.staged_predict(X_val)] bst_n_estimators\t=\tnp.argmin(errors)\n\ngbrt_best\t=\tGradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators) gbrt_best.fit(X_train,\ty_train)\n\nThe\tvalidation\terrors\tare\trepresented\ton\tthe\tleft\tof\tFigure\t7-11,\tand\tthe\tbest\tmodel’s\tpredictions\tare represented\ton\tthe\tright.\n\nFigure\t7-11.\tTuning\tthe\tnumber\tof\ttrees\tusing\tearly\tstopping\n\nIt\tis\talso\tpossible\tto\timplement\tearly\tstopping\tby\tactually\tstopping\ttraining\tearly\t(instead\tof\ttraining\ta large\tnumber\tof\ttrees\tfirst\tand\tthen\tlooking\tback\tto\tfind\tthe\toptimal\tnumber).\tYou\tcan\tdo\tso\tby\tsetting warm_start=True,\twhich\tmakes\tScikit-Learn\tkeep\texisting\ttrees\twhen\tthe\tfit()\tmethod\tis\tcalled, allowing\tincremental\ttraining.\tThe\tfollowing\tcode\tstops\ttraining\twhen\tthe\tvalidation\terror\tdoes\tnot",
      "content_length": 1716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "improve\tfor\tfive\titerations\tin\ta\trow:\n\ngbrt\t=\tGradientBoostingRegressor(max_depth=2,\twarm_start=True)\n\nmin_val_error\t=\tfloat(\"inf\") error_going_up\t=\t0 for\tn_estimators\tin\trange(1,\t120): \t\t\t\tgbrt.n_estimators\t=\tn_estimators \t\t\t\tgbrt.fit(X_train,\ty_train) \t\t\t\ty_pred\t=\tgbrt.predict(X_val) \t\t\t\tval_error\t=\tmean_squared_error(y_val,\ty_pred) \t\t\t\tif\tval_error\t<\tmin_val_error: \t\t\t\t\t\t\t\tmin_val_error\t=\tval_error \t\t\t\t\t\t\t\terror_going_up\t=\t0 \t\t\t\telse: \t\t\t\t\t\t\t\terror_going_up\t+=\t1 \t\t\t\t\t\t\t\tif\terror_going_up\t==\t5: \t\t\t\t\t\t\t\t\t\t\t\tbreak\t\t#\tearly\tstopping\n\nThe\tGradientBoostingRegressor\tclass\talso\tsupports\ta\tsubsample\thyperparameter,\twhich\tspecifies the\tfraction\tof\ttraining\tinstances\tto\tbe\tused\tfor\ttraining\teach\ttree.\tFor\texample,\tif\tsubsample=0.25,\tthen each\ttree\tis\ttrained\ton\t25%\tof\tthe\ttraining\tinstances,\tselected\trandomly.\tAs\tyou\tcan\tprobably\tguess\tby now,\tthis\ttrades\ta\thigher\tbias\tfor\ta\tlower\tvariance.\tIt\talso\tspeeds\tup\ttraining\tconsiderably.\tThis\ttechnique is\tcalled\tStochastic\tGradient\tBoosting.\n\nNOTE\n\nIt\tis\tpossible\tto\tuse\tGradient\tBoosting\twith\tother\tcost\tfunctions.\tThis\tis\tcontrolled\tby\tthe\tloss\thyperparameter\t(see\tScikit- Learn’s\tdocumentation\tfor\tmore\tdetails).",
      "content_length": 1165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "Stacking The\tlast\tEnsemble\tmethod\twe\twill\tdiscuss\tin\tthis\tchapter\tis\tcalled\tstacking\t(short\tfor\tstacked generalization).18\tIt\tis\tbased\ton\ta\tsimple\tidea:\tinstead\tof\tusing\ttrivial\tfunctions\t(such\tas\thard\tvoting)\tto aggregate\tthe\tpredictions\tof\tall\tpredictors\tin\tan\tensemble,\twhy\tdon’t\twe\ttrain\ta\tmodel\tto\tperform\tthis aggregation?\tFigure\t7-12\tshows\tsuch\tan\tensemble\tperforming\ta\tregression\ttask\ton\ta\tnew\tinstance.\tEach of\tthe\tbottom\tthree\tpredictors\tpredicts\ta\tdifferent\tvalue\t(3.1,\t2.7,\tand\t2.9),\tand\tthen\tthe\tfinal\tpredictor (called\ta\tblender,\tor\ta\tmeta\tlearner)\ttakes\tthese\tpredictions\tas\tinputs\tand\tmakes\tthe\tfinal\tprediction (3.0).\n\nFigure\t7-12.\tAggregating\tpredictions\tusing\ta\tblending\tpredictor\n\nTo\ttrain\tthe\tblender,\ta\tcommon\tapproach\tis\tto\tuse\ta\thold-out\tset.19\tLet’s\tsee\thow\tit\tworks.\tFirst,\tthe training\tset\tis\tsplit\tin\ttwo\tsubsets.\tThe\tfirst\tsubset\tis\tused\tto\ttrain\tthe\tpredictors\tin\tthe\tfirst\tlayer\t(see Figure\t7-13).",
      "content_length": 928,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "Figure\t7-13.\tTraining\tthe\tfirst\tlayer\n\nNext,\tthe\tfirst\tlayer\tpredictors\tare\tused\tto\tmake\tpredictions\ton\tthe\tsecond\t(held-out)\tset\t(see\tFigure\t7- 14).\tThis\tensures\tthat\tthe\tpredictions\tare\t“clean,”\tsince\tthe\tpredictors\tnever\tsaw\tthese\tinstances\tduring training.\tNow\tfor\teach\tinstance\tin\tthe\thold-out\tset\tthere\tare\tthree\tpredicted\tvalues.\tWe\tcan\tcreate\ta\tnew training\tset\tusing\tthese\tpredicted\tvalues\tas\tinput\tfeatures\t(which\tmakes\tthis\tnew\ttraining\tset\tthree- dimensional),\tand\tkeeping\tthe\ttarget\tvalues.\tThe\tblender\tis\ttrained\ton\tthis\tnew\ttraining\tset,\tso\tit\tlearns\tto predict\tthe\ttarget\tvalue\tgiven\tthe\tfirst\tlayer’s\tpredictions.",
      "content_length": 630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Figure\t7-14.\tTraining\tthe\tblender\n\nIt\tis\tactually\tpossible\tto\ttrain\tseveral\tdifferent\tblenders\tthis\tway\t(e.g.,\tone\tusing\tLinear\tRegression, another\tusing\tRandom\tForest\tRegression,\tand\tso\ton):\twe\tget\ta\twhole\tlayer\tof\tblenders.\tThe\ttrick\tis\tto split\tthe\ttraining\tset\tinto\tthree\tsubsets:\tthe\tfirst\tone\tis\tused\tto\ttrain\tthe\tfirst\tlayer,\tthe\tsecond\tone\tis\tused\tto create\tthe\ttraining\tset\tused\tto\ttrain\tthe\tsecond\tlayer\t(using\tpredictions\tmade\tby\tthe\tpredictors\tof\tthe\tfirst layer),\tand\tthe\tthird\tone\tis\tused\tto\tcreate\tthe\ttraining\tset\tto\ttrain\tthe\tthird\tlayer\t(using\tpredictions\tmade\tby the\tpredictors\tof\tthe\tsecond\tlayer).\tOnce\tthis\tis\tdone,\twe\tcan\tmake\ta\tprediction\tfor\ta\tnew\tinstance\tby going\tthrough\teach\tlayer\tsequentially,\tas\tshown\tin\tFigure\t7-15.",
      "content_length": 748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "Figure\t7-15.\tPredictions\tin\ta\tmultilayer\tstacking\tensemble\n\nUnfortunately,\tScikit-Learn\tdoes\tnot\tsupport\tstacking\tdirectly,\tbut\tit\tis\tnot\ttoo\thard\tto\troll\tout\tyour\town implementation\t(see\tthe\tfollowing\texercises).\tAlternatively,\tyou\tcan\tuse\tan\topen\tsource\timplementation such\tas\tbrew\t(available\tat\thttps://github.com/viisar/brew).",
      "content_length": 330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Exercises\n\n1.\t If\tyou\thave\ttrained\tfive\tdifferent\tmodels\ton\tthe\texact\tsame\ttraining\tdata,\tand\tthey\tall\tachieve\t95% precision,\tis\tthere\tany\tchance\tthat\tyou\tcan\tcombine\tthese\tmodels\tto\tget\tbetter\tresults?\tIf\tso,\thow?\tIf not,\twhy?\n\n2.\t What\tis\tthe\tdifference\tbetween\thard\tand\tsoft\tvoting\tclassifiers?\n\n3.\t Is\tit\tpossible\tto\tspeed\tup\ttraining\tof\ta\tbagging\tensemble\tby\tdistributing\tit\tacross\tmultiple\tservers? What\tabout\tpasting\tensembles,\tboosting\tensembles,\trandom\tforests,\tor\tstacking\tensembles?\n\n4.\t What\tis\tthe\tbenefit\tof\tout-of-bag\tevaluation?\n\n5.\t What\tmakes\tExtra-Trees\tmore\trandom\tthan\tregular\tRandom\tForests?\tHow\tcan\tthis\textra\trandomness help?\tAre\tExtra-Trees\tslower\tor\tfaster\tthan\tregular\tRandom\tForests?\n\n6.\t If\tyour\tAdaBoost\tensemble\tunderfits\tthe\ttraining\tdata,\twhat\thyperparameters\tshould\tyou\ttweak\tand how?\n\n7.\t If\tyour\tGradient\tBoosting\tensemble\toverfits\tthe\ttraining\tset,\tshould\tyou\tincrease\tor\tdecrease\tthe learning\trate?\n\n8.\t Load\tthe\tMNIST\tdata\t(introduced\tin\tChapter\t3),\tand\tsplit\tit\tinto\ta\ttraining\tset,\ta\tvalidation\tset,\tand\ta test\tset\t(e.g.,\tuse\t40,000\tinstances\tfor\ttraining,\t10,000\tfor\tvalidation,\tand\t10,000\tfor\ttesting).\tThen train\tvarious\tclassifiers,\tsuch\tas\ta\tRandom\tForest\tclassifier,\tan\tExtra-Trees\tclassifier,\tand\tan\tSVM. Next,\ttry\tto\tcombine\tthem\tinto\tan\tensemble\tthat\toutperforms\tthem\tall\ton\tthe\tvalidation\tset,\tusing\ta soft\tor\thard\tvoting\tclassifier.\tOnce\tyou\thave\tfound\tone,\ttry\tit\ton\tthe\ttest\tset.\tHow\tmuch\tbetter\tdoes\tit perform\tcompared\tto\tthe\tindividual\tclassifiers?\n\n9.\t Run\tthe\tindividual\tclassifiers\tfrom\tthe\tprevious\texercise\tto\tmake\tpredictions\ton\tthe\tvalidation\tset, and\tcreate\ta\tnew\ttraining\tset\twith\tthe\tresulting\tpredictions:\teach\ttraining\tinstance\tis\ta\tvector containing\tthe\tset\tof\tpredictions\tfrom\tall\tyour\tclassifiers\tfor\tan\timage,\tand\tthe\ttarget\tis\tthe\timage’s class.\tCongratulations,\tyou\thave\tjust\ttrained\ta\tblender,\tand\ttogether\twith\tthe\tclassifiers\tthey\tform\ta stacking\tensemble!\tNow\tlet’s\tevaluate\tthe\tensemble\ton\tthe\ttest\tset.\tFor\teach\timage\tin\tthe\ttest\tset, make\tpredictions\twith\tall\tyour\tclassifiers,\tthen\tfeed\tthe\tpredictions\tto\tthe\tblender\tto\tget\tthe ensemble’s\tpredictions.\tHow\tdoes\tit\tcompare\tto\tthe\tvoting\tclassifier\tyou\ttrained\tearlier?\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“Bagging\tPredictors,”\tL.\tBreiman\t(1996).\n\n2\n\nIn\tstatistics,\tresampling\twith\treplacement\tis\tcalled\tbootstrapping.\n\n3\n\n“Pasting\tsmall\tvotes\tfor\tclassification\tin\tlarge\tdatabases\tand\ton-line,”\tL.\tBreiman\t(1999).\n\n4\n\nBias\tand\tvariance\twere\tintroduced\tin\tChapter\t4.\n\n5\n\nmax_samples\tcan\talternatively\tbe\tset\tto\ta\tfloat\tbetween\t0.0\tand\t1.0,\tin\twhich\tcase\tthe\tmax\tnumber\tof\tinstances\tto\tsample\tis\tequal\tto\tthe size\tof\tthe\ttraining\tset\ttimes\tmax_samples.\n\n6",
      "content_length": 2713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\nAs\tm\tgrows,\tthis\tratio\tapproaches\t1\t–\texp(–1)\t≈\t63.212%.\n\n“Ensembles\ton\tRandom\tPatches,”\tG.\tLouppe\tand\tP.\tGeurts\t(2012).\n\n“The\trandom\tsubspace\tmethod\tfor\tconstructing\tdecision\tforests,”\tTin\tKam\tHo\t(1998).\n\n“Random\tDecision\tForests,”\tT.\tHo\t(1995).\n\nThe\tBaggingClassifier\tclass\tremains\tuseful\tif\tyou\twant\ta\tbag\tof\tsomething\tother\tthan\tDecision\tTrees.\n\nThere\tare\ta\tfew\tnotable\texceptions:\tsplitter\tis\tabsent\t(forced\tto\t\"random\"),\tpresort\tis\tabsent\t(forced\tto\tFalse),\tmax_samples\tis\tabsent (forced\tto\t1.0),\tand\tbase_estimator\tis\tabsent\t(forced\tto\tDecisionTreeClassifier\twith\tthe\tprovided\thyperparameters).\n\n“Extremely\trandomized\ttrees,”\tP.\tGeurts,\tD.\tErnst,\tL.\tWehenkel\t(2005).\n\n“A\tDecision-Theoretic\tGeneralization\tof\tOn-Line\tLearning\tand\tan\tApplication\tto\tBoosting,”\tYoav\tFreund,\tRobert\tE.\tSchapire\t(1997).\n\nThis\tis\tjust\tfor\tillustrative\tpurposes.\tSVMs\tare\tgenerally\tnot\tgood\tbase\tpredictors\tfor\tAdaBoost,\tbecause\tthey\tare\tslow\tand\ttend\tto\tbe unstable\twith\tAdaBoost.\n\nThe\toriginal\tAdaBoost\talgorithm\tdoes\tnot\tuse\ta\tlearning\trate\thyperparameter.\n\nFor\tmore\tdetails,\tsee\t“Multi-Class\tAdaBoost,”\tJ.\tZhu\tet\tal.\t(2006).\n\nFirst\tintroduced\tin\t“Arcing\tthe\tEdge,”\tL.\tBreiman\t(1997).\n\n“Stacked\tGeneralization,”\tD.\tWolpert\t(1992).\n\nAlternatively,\tit\tis\tpossible\tto\tuse\tout-of-fold\tpredictions.\tIn\tsome\tcontexts\tthis\tis\tcalled\tstacking,\twhile\tusing\ta\thold-out\tset\tis\tcalled blending.\tHowever,\tfor\tmany\tpeople\tthese\tterms\tare\tsynonymous.",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Chapter\t8.\tDimensionality\tReduction\n\nMany\tMachine\tLearning\tproblems\tinvolve\tthousands\tor\teven\tmillions\tof\tfeatures\tfor\teach\ttraining instance.\tNot\tonly\tdoes\tthis\tmake\ttraining\textremely\tslow,\tit\tcan\talso\tmake\tit\tmuch\tharder\tto\tfind\ta\tgood solution,\tas\twe\twill\tsee.\tThis\tproblem\tis\toften\treferred\tto\tas\tthe\tcurse\tof\tdimensionality.\n\nFortunately,\tin\treal-world\tproblems,\tit\tis\toften\tpossible\tto\treduce\tthe\tnumber\tof\tfeatures\tconsiderably, turning\tan\tintractable\tproblem\tinto\ta\ttractable\tone.\tFor\texample,\tconsider\tthe\tMNIST\timages\t(introduced in\tChapter\t3):\tthe\tpixels\ton\tthe\timage\tborders\tare\talmost\talways\twhite,\tso\tyou\tcould\tcompletely\tdrop these\tpixels\tfrom\tthe\ttraining\tset\twithout\tlosing\tmuch\tinformation.\tFigure\t7-6\tconfirms\tthat\tthese\tpixels are\tutterly\tunimportant\tfor\tthe\tclassification\ttask.\tMoreover,\ttwo\tneighboring\tpixels\tare\toften\thighly correlated:\tif\tyou\tmerge\tthem\tinto\ta\tsingle\tpixel\t(e.g.,\tby\ttaking\tthe\tmean\tof\tthe\ttwo\tpixel\tintensities),\tyou will\tnot\tlose\tmuch\tinformation.\n\nWARNING\n\nReducing\tdimensionality\tdoes\tlose\tsome\tinformation\t(just\tlike\tcompressing\tan\timage\tto\tJPEG\tcan\tdegrade\tits\tquality),\tso\teven though\tit\twill\tspeed\tup\ttraining,\tit\tmay\talso\tmake\tyour\tsystem\tperform\tslightly\tworse.\tIt\talso\tmakes\tyour\tpipelines\ta\tbit\tmore complex\tand\tthus\tharder\tto\tmaintain.\tSo\tyou\tshould\tfirst\ttry\tto\ttrain\tyour\tsystem\twith\tthe\toriginal\tdata\tbefore\tconsidering\tusing dimensionality\treduction\tif\ttraining\tis\ttoo\tslow.\tIn\tsome\tcases,\thowever,\treducing\tthe\tdimensionality\tof\tthe\ttraining\tdata\tmay\tfilter out\tsome\tnoise\tand\tunnecessary\tdetails\tand\tthus\tresult\tin\thigher\tperformance\t(but\tin\tgeneral\tit\twon’t;\tit\twill\tjust\tspeed\tup training).\n\nApart\tfrom\tspeeding\tup\ttraining,\tdimensionality\treduction\tis\talso\textremely\tuseful\tfor\tdata\tvisualization (or\tDataViz).\tReducing\tthe\tnumber\tof\tdimensions\tdown\tto\ttwo\t(or\tthree)\tmakes\tit\tpossible\tto\tplot\ta\thigh- dimensional\ttraining\tset\ton\ta\tgraph\tand\toften\tgain\tsome\timportant\tinsights\tby\tvisually\tdetecting\tpatterns, such\tas\tclusters.\n\nIn\tthis\tchapter\twe\twill\tdiscuss\tthe\tcurse\tof\tdimensionality\tand\tget\ta\tsense\tof\twhat\tgoes\ton\tin\thigh- dimensional\tspace.\tThen,\twe\twill\tpresent\tthe\ttwo\tmain\tapproaches\tto\tdimensionality\treduction (projection\tand\tManifold\tLearning),\tand\twe\twill\tgo\tthrough\tthree\tof\tthe\tmost\tpopular\tdimensionality reduction\ttechniques:\tPCA,\tKernel\tPCA,\tand\tLLE.",
      "content_length": 2335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "The\tCurse\tof\tDimensionality We\tare\tso\tused\tto\tliving\tin\tthree\tdimensions1\tthat\tour\tintuition\tfails\tus\twhen\twe\ttry\tto\timagine\ta\thigh- dimensional\tspace.\tEven\ta\tbasic\t4D\thypercube\tis\tincredibly\thard\tto\tpicture\tin\tour\tmind\t(see\tFigure\t8-1), let\talone\ta\t200-dimensional\tellipsoid\tbent\tin\ta\t1,000-dimensional\tspace.\n\nFigure\t8-1.\tPoint,\tsegment,\tsquare,\tcube,\tand\ttesseract\t(0D\tto\t4D\thypercubes)2\n\nIt\tturns\tout\tthat\tmany\tthings\tbehave\tvery\tdifferently\tin\thigh-dimensional\tspace.\tFor\texample,\tif\tyou\tpick\ta random\tpoint\tin\ta\tunit\tsquare\t(a\t1\t×\t1\tsquare),\tit\twill\thave\tonly\tabout\ta\t0.4%\tchance\tof\tbeing\tlocated\tless than\t0.001\tfrom\ta\tborder\t(in\tother\twords,\tit\tis\tvery\tunlikely\tthat\ta\trandom\tpoint\twill\tbe\t“extreme”\talong any\tdimension).\tBut\tin\ta\t10,000-dimensional\tunit\thypercube\t(a\t1\t×\t1\t×\t\t×\t1\tcube,\twith\tten\tthousand\t1s), this\tprobability\tis\tgreater\tthan\t99.999999%.\tMost\tpoints\tin\ta\thigh-dimensional\thypercube\tare\tvery\tclose to\tthe\tborder.3\n\nHere\tis\ta\tmore\ttroublesome\tdifference:\tif\tyou\tpick\ttwo\tpoints\trandomly\tin\ta\tunit\tsquare,\tthe\tdistance between\tthese\ttwo\tpoints\twill\tbe,\ton\taverage,\troughly\t0.52.\tIf\tyou\tpick\ttwo\trandom\tpoints\tin\ta\tunit\t3D cube,\tthe\taverage\tdistance\twill\tbe\troughly\t0.66.\tBut\twhat\tabout\ttwo\tpoints\tpicked\trandomly\tin\ta 1,000,000-dimensional\thypercube?\tWell,\tthe\taverage\tdistance,\tbelieve\tit\tor\tnot,\twill\tbe\tabout\t408.25\n\n)!\tThis\tis\tquite\tcounterintuitive:\thow\tcan\ttwo\tpoints\tbe\tso\tfar\tapart\twhen (roughly\t they\tboth\tlie\twithin\tthe\tsame\tunit\thypercube?\tThis\tfact\timplies\tthat\thigh-dimensional\tdatasets\tare\tat\trisk of\tbeing\tvery\tsparse:\tmost\ttraining\tinstances\tare\tlikely\tto\tbe\tfar\taway\tfrom\teach\tother.\tOf\tcourse,\tthis also\tmeans\tthat\ta\tnew\tinstance\twill\tlikely\tbe\tfar\taway\tfrom\tany\ttraining\tinstance,\tmaking\tpredictions much\tless\treliable\tthan\tin\tlower\tdimensions,\tsince\tthey\twill\tbe\tbased\ton\tmuch\tlarger\textrapolations.\tIn short,\tthe\tmore\tdimensions\tthe\ttraining\tset\thas,\tthe\tgreater\tthe\trisk\tof\toverfitting\tit.\n\nIn\ttheory,\tone\tsolution\tto\tthe\tcurse\tof\tdimensionality\tcould\tbe\tto\tincrease\tthe\tsize\tof\tthe\ttraining\tset\tto reach\ta\tsufficient\tdensity\tof\ttraining\tinstances.\tUnfortunately,\tin\tpractice,\tthe\tnumber\tof\ttraining\tinstances required\tto\treach\ta\tgiven\tdensity\tgrows\texponentially\twith\tthe\tnumber\tof\tdimensions.\tWith\tjust\t100 features\t(much\tless\tthan\tin\tthe\tMNIST\tproblem),\tyou\twould\tneed\tmore\ttraining\tinstances\tthan\tatoms\tin the\tobservable\tuniverse\tin\torder\tfor\ttraining\tinstances\tto\tbe\twithin\t0.1\tof\teach\tother\ton\taverage,\tassuming they\twere\tspread\tout\tuniformly\tacross\tall\tdimensions.",
      "content_length": 2515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "Main\tApproaches\tfor\tDimensionality\tReduction Before\twe\tdive\tinto\tspecific\tdimensionality\treduction\talgorithms,\tlet’s\ttake\ta\tlook\tat\tthe\ttwo\tmain approaches\tto\treducing\tdimensionality:\tprojection\tand\tManifold\tLearning.",
      "content_length": 217,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Projection In\tmost\treal-world\tproblems,\ttraining\tinstances\tare\tnot\tspread\tout\tuniformly\tacross\tall\tdimensions.\tMany features\tare\talmost\tconstant,\twhile\tothers\tare\thighly\tcorrelated\t(as\tdiscussed\tearlier\tfor\tMNIST).\tAs\ta result,\tall\ttraining\tinstances\tactually\tlie\twithin\t(or\tclose\tto)\ta\tmuch\tlower-dimensional\tsubspace\tof\tthe high-dimensional\tspace.\tThis\tsounds\tvery\tabstract,\tso\tlet’s\tlook\tat\tan\texample.\tIn\tFigure\t8-2\tyou\tcan\tsee a\t3D\tdataset\trepresented\tby\tthe\tcircles.\n\nFigure\t8-2.\tA\t3D\tdataset\tlying\tclose\tto\ta\t2D\tsubspace\n\nNotice\tthat\tall\ttraining\tinstances\tlie\tclose\tto\ta\tplane:\tthis\tis\ta\tlower-dimensional\t(2D)\tsubspace\tof\tthe high-dimensional\t(3D)\tspace.\tNow\tif\twe\tproject\tevery\ttraining\tinstance\tperpendicularly\tonto\tthis subspace\t(as\trepresented\tby\tthe\tshort\tlines\tconnecting\tthe\tinstances\tto\tthe\tplane),\twe\tget\tthe\tnew\t2D dataset\tshown\tin\tFigure\t8-3.\tTa-da!\tWe\thave\tjust\treduced\tthe\tdataset’s\tdimensionality\tfrom\t3D\tto\t2D. Note\tthat\tthe\taxes\tcorrespond\tto\tnew\tfeatures\tz1\tand\tz2\t(the\tcoordinates\tof\tthe\tprojections\ton\tthe\tplane).",
      "content_length": 1041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "Figure\t8-3.\tThe\tnew\t2D\tdataset\tafter\tprojection\n\nHowever,\tprojection\tis\tnot\talways\tthe\tbest\tapproach\tto\tdimensionality\treduction.\tIn\tmany\tcases\tthe subspace\tmay\ttwist\tand\tturn,\tsuch\tas\tin\tthe\tfamous\tSwiss\troll\ttoy\tdataset\trepresented\tin\tFigure\t8-4.",
      "content_length": 248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "Figure\t8-4.\tSwiss\troll\tdataset\n\nSimply\tprojecting\tonto\ta\tplane\t(e.g.,\tby\tdropping\tx3)\twould\tsquash\tdifferent\tlayers\tof\tthe\tSwiss\troll together,\tas\tshown\ton\tthe\tleft\tof\tFigure\t8-5.\tHowever,\twhat\tyou\treally\twant\tis\tto\tunroll\tthe\tSwiss\troll\tto obtain\tthe\t2D\tdataset\ton\tthe\tright\tof\tFigure\t8-5.\n\nFigure\t8-5.\tSquashing\tby\tprojecting\tonto\ta\tplane\t(left)\tversus\tunrolling\tthe\tSwiss\troll\t(right)",
      "content_length": 387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "Manifold\tLearning The\tSwiss\troll\tis\tan\texample\tof\ta\t2D\tmanifold.\tPut\tsimply,\ta\t2D\tmanifold\tis\ta\t2D\tshape\tthat\tcan\tbe\tbent and\ttwisted\tin\ta\thigher-dimensional\tspace.\tMore\tgenerally,\ta\td-dimensional\tmanifold\tis\ta\tpart\tof\tan\tn- dimensional\tspace\t(where\td\t<\tn)\tthat\tlocally\tresembles\ta\td-dimensional\thyperplane.\tIn\tthe\tcase\tof\tthe Swiss\troll,\td\t=\t2\tand\tn\t=\t3:\tit\tlocally\tresembles\ta\t2D\tplane,\tbut\tit\tis\trolled\tin\tthe\tthird\tdimension.\n\nMany\tdimensionality\treduction\talgorithms\twork\tby\tmodeling\tthe\tmanifold\ton\twhich\tthe\ttraining\tinstances lie;\tthis\tis\tcalled\tManifold\tLearning.\tIt\trelies\ton\tthe\tmanifold\tassumption,\talso\tcalled\tthe\tmanifold hypothesis,\twhich\tholds\tthat\tmost\treal-world\thigh-dimensional\tdatasets\tlie\tclose\tto\ta\tmuch\tlower- dimensional\tmanifold.\tThis\tassumption\tis\tvery\toften\tempirically\tobserved.\n\nOnce\tagain,\tthink\tabout\tthe\tMNIST\tdataset:\tall\thandwritten\tdigit\timages\thave\tsome\tsimilarities.\tThey\tare made\tof\tconnected\tlines,\tthe\tborders\tare\twhite,\tthey\tare\tmore\tor\tless\tcentered,\tand\tso\ton.\tIf\tyou\trandomly generated\timages,\tonly\ta\tridiculously\ttiny\tfraction\tof\tthem\twould\tlook\tlike\thandwritten\tdigits.\tIn\tother words,\tthe\tdegrees\tof\tfreedom\tavailable\tto\tyou\tif\tyou\ttry\tto\tcreate\ta\tdigit\timage\tare\tdramatically\tlower than\tthe\tdegrees\tof\tfreedom\tyou\twould\thave\tif\tyou\twere\tallowed\tto\tgenerate\tany\timage\tyou\twanted. These\tconstraints\ttend\tto\tsqueeze\tthe\tdataset\tinto\ta\tlower-dimensional\tmanifold.\n\nThe\tmanifold\tassumption\tis\toften\taccompanied\tby\tanother\timplicit\tassumption:\tthat\tthe\ttask\tat\thand\t(e.g., classification\tor\tregression)\twill\tbe\tsimpler\tif\texpressed\tin\tthe\tlower-dimensional\tspace\tof\tthe\tmanifold. For\texample,\tin\tthe\ttop\trow\tof\tFigure\t8-6\tthe\tSwiss\troll\tis\tsplit\tinto\ttwo\tclasses:\tin\tthe\t3D\tspace\t(on\tthe left),\tthe\tdecision\tboundary\twould\tbe\tfairly\tcomplex,\tbut\tin\tthe\t2D\tunrolled\tmanifold\tspace\t(on\tthe\tright), the\tdecision\tboundary\tis\ta\tsimple\tstraight\tline.\n\nHowever,\tthis\tassumption\tdoes\tnot\talways\thold.\tFor\texample,\tin\tthe\tbottom\trow\tof\tFigure\t8-6,\tthe decision\tboundary\tis\tlocated\tat\tx1\t=\t5.\tThis\tdecision\tboundary\tlooks\tvery\tsimple\tin\tthe\toriginal\t3D\tspace (a\tvertical\tplane),\tbut\tit\tlooks\tmore\tcomplex\tin\tthe\tunrolled\tmanifold\t(a\tcollection\tof\tfour\tindependent line\tsegments).\n\nIn\tshort,\tif\tyou\treduce\tthe\tdimensionality\tof\tyour\ttraining\tset\tbefore\ttraining\ta\tmodel,\tit\twill\tdefinitely speed\tup\ttraining,\tbut\tit\tmay\tnot\talways\tlead\tto\ta\tbetter\tor\tsimpler\tsolution;\tit\tall\tdepends\ton\tthe\tdataset.\n\nHopefully\tyou\tnow\thave\ta\tgood\tsense\tof\twhat\tthe\tcurse\tof\tdimensionality\tis\tand\thow\tdimensionality reduction\talgorithms\tcan\tfight\tit,\tespecially\twhen\tthe\tmanifold\tassumption\tholds.\tThe\trest\tof\tthis\tchapter will\tgo\tthrough\tsome\tof\tthe\tmost\tpopular\talgorithms.",
      "content_length": 2690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Figure\t8-6.\tThe\tdecision\tboundary\tmay\tnot\talways\tbe\tsimpler\twith\tlower\tdimensions",
      "content_length": 81,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "PCA Principal\tComponent\tAnalysis\t(PCA)\tis\tby\tfar\tthe\tmost\tpopular\tdimensionality\treduction\talgorithm. First\tit\tidentifies\tthe\thyperplane\tthat\tlies\tclosest\tto\tthe\tdata,\tand\tthen\tit\tprojects\tthe\tdata\tonto\tit.",
      "content_length": 206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "Preserving\tthe\tVariance Before\tyou\tcan\tproject\tthe\ttraining\tset\tonto\ta\tlower-dimensional\thyperplane,\tyou\tfirst\tneed\tto\tchoose\tthe right\thyperplane.\tFor\texample,\ta\tsimple\t2D\tdataset\tis\trepresented\ton\tthe\tleft\tof\tFigure\t8-7,\talong\twith three\tdifferent\taxes\t(i.e.,\tone-dimensional\thyperplanes).\tOn\tthe\tright\tis\tthe\tresult\tof\tthe\tprojection\tof\tthe dataset\tonto\teach\tof\tthese\taxes.\tAs\tyou\tcan\tsee,\tthe\tprojection\tonto\tthe\tsolid\tline\tpreserves\tthe\tmaximum variance,\twhile\tthe\tprojection\tonto\tthe\tdotted\tline\tpreserves\tvery\tlittle\tvariance,\tand\tthe\tprojection\tonto the\tdashed\tline\tpreserves\tan\tintermediate\tamount\tof\tvariance.\n\nFigure\t8-7.\tSelecting\tthe\tsubspace\tonto\twhich\tto\tproject\n\nIt\tseems\treasonable\tto\tselect\tthe\taxis\tthat\tpreserves\tthe\tmaximum\tamount\tof\tvariance,\tas\tit\twill\tmost likely\tlose\tless\tinformation\tthan\tthe\tother\tprojections.\tAnother\tway\tto\tjustify\tthis\tchoice\tis\tthat\tit\tis\tthe axis\tthat\tminimizes\tthe\tmean\tsquared\tdistance\tbetween\tthe\toriginal\tdataset\tand\tits\tprojection\tonto\tthat axis.\tThis\tis\tthe\trather\tsimple\tidea\tbehind\tPCA.4",
      "content_length": 1044,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "Principal\tComponents PCA\tidentifies\tthe\taxis\tthat\taccounts\tfor\tthe\tlargest\tamount\tof\tvariance\tin\tthe\ttraining\tset.\tIn\tFigure\t8-7,\tit is\tthe\tsolid\tline.\tIt\talso\tfinds\ta\tsecond\taxis,\torthogonal\tto\tthe\tfirst\tone,\tthat\taccounts\tfor\tthe\tlargest\tamount of\tremaining\tvariance.\tIn\tthis\t2D\texample\tthere\tis\tno\tchoice:\tit\tis\tthe\tdotted\tline.\tIf\tit\twere\ta\thigher- dimensional\tdataset,\tPCA\twould\talso\tfind\ta\tthird\taxis,\torthogonal\tto\tboth\tprevious\taxes,\tand\ta\tfourth,\ta fifth,\tand\tso\ton\t—\tas\tmany\taxes\tas\tthe\tnumber\tof\tdimensions\tin\tthe\tdataset. The\tunit\tvector\tthat\tdefines\tthe\tith\taxis\tis\tcalled\tthe\tith\tprincipal\tcomponent\t(PC).\tIn\tFigure\t8-7,\tthe\t1st PC\tis\tc1\tand\tthe\t2nd\tPC\tis\tc2.\tIn\tFigure\t8-2\tthe\tfirst\ttwo\tPCs\tare\trepresented\tby\tthe\torthogonal\tarrows\tin the\tplane,\tand\tthe\tthird\tPC\twould\tbe\torthogonal\tto\tthe\tplane\t(pointing\tup\tor\tdown).\n\nNOTE\n\nThe\tdirection\tof\tthe\tprincipal\tcomponents\tis\tnot\tstable:\tif\tyou\tperturb\tthe\ttraining\tset\tslightly\tand\trun\tPCA\tagain,\tsome\tof\tthe\tnew PCs\tmay\tpoint\tin\tthe\topposite\tdirection\tof\tthe\toriginal\tPCs.\tHowever,\tthey\twill\tgenerally\tstill\tlie\ton\tthe\tsame\taxes.\tIn\tsome\tcases, a\tpair\tof\tPCs\tmay\teven\trotate\tor\tswap,\tbut\tthe\tplane\tthey\tdefine\twill\tgenerally\tremain\tthe\tsame.\n\nSo\thow\tcan\tyou\tfind\tthe\tprincipal\tcomponents\tof\ta\ttraining\tset?\tLuckily,\tthere\tis\ta\tstandard\tmatrix factorization\ttechnique\tcalled\tSingular\tValue\tDecomposition\t(SVD)\tthat\tcan\tdecompose\tthe\ttraining\tset matrix\tX\tinto\tthe\tdot\tproduct\tof\tthree\tmatrices\tU\t·\tΣ\t·\tVT,\twhere\tVT\tcontains\tall\tthe\tprincipal\tcomponents that\twe\tare\tlooking\tfor,\tas\tshown\tin\tEquation\t8-1.\n\nEquation\t8-1.\tPrincipal\tcomponents\tmatrix\n\nThe\tfollowing\tPython\tcode\tuses\tNumPy’s\tsvd()\tfunction\tto\tobtain\tall\tthe\tprincipal\tcomponents\tof\tthe training\tset,\tthen\textracts\tthe\tfirst\ttwo\tPCs:\n\nX_centered\t=\tX\t-\tX.mean(axis=0) U,\ts,\tV\t=\tnp.linalg.svd(X_centered) c1\t=\tV.T[:,\t0] c2\t=\tV.T[:,\t1]",
      "content_length": 1854,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "WARNING\n\nPCA\tassumes\tthat\tthe\tdataset\tis\tcentered\taround\tthe\torigin.\tAs\twe\twill\tsee,\tScikit-Learn’s\tPCA\tclasses\ttake\tcare\tof\tcentering the\tdata\tfor\tyou.\tHowever,\tif\tyou\timplement\tPCA\tyourself\t(as\tin\tthe\tpreceding\texample),\tor\tif\tyou\tuse\tother\tlibraries,\tdon’t forget\tto\tcenter\tthe\tdata\tfirst.",
      "content_length": 292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "Projecting\tDown\tto\td\tDimensions Once\tyou\thave\tidentified\tall\tthe\tprincipal\tcomponents,\tyou\tcan\treduce\tthe\tdimensionality\tof\tthe\tdataset down\tto\td\tdimensions\tby\tprojecting\tit\tonto\tthe\thyperplane\tdefined\tby\tthe\tfirst\td\tprincipal\tcomponents. Selecting\tthis\thyperplane\tensures\tthat\tthe\tprojection\twill\tpreserve\tas\tmuch\tvariance\tas\tpossible.\tFor example,\tin\tFigure\t8-2\tthe\t3D\tdataset\tis\tprojected\tdown\tto\tthe\t2D\tplane\tdefined\tby\tthe\tfirst\ttwo\tprincipal components,\tpreserving\ta\tlarge\tpart\tof\tthe\tdataset’s\tvariance.\tAs\ta\tresult,\tthe\t2D\tprojection\tlooks\tvery much\tlike\tthe\toriginal\t3D\tdataset.\n\nTo\tproject\tthe\ttraining\tset\tonto\tthe\thyperplane,\tyou\tcan\tsimply\tcompute\tthe\tdot\tproduct\tof\tthe\ttraining\tset matrix\tX\tby\tthe\tmatrix\tWd,\tdefined\tas\tthe\tmatrix\tcontaining\tthe\tfirst\td\tprincipal\tcomponents\t(i.e.,\tthe matrix\tcomposed\tof\tthe\tfirst\td\tcolumns\tof\tVT),\tas\tshown\tin\tEquation\t8-2.\n\nEquation\t8-2.\tProjecting\tthe\ttraining\tset\tdown\tto\td\tdimensions\n\nThe\tfollowing\tPython\tcode\tprojects\tthe\ttraining\tset\tonto\tthe\tplane\tdefined\tby\tthe\tfirst\ttwo\tprincipal components:\n\nW2\t=\tV.T[:,\t:2] X2D\t=\tX_centered.dot(W2)\n\nThere\tyou\thave\tit!\tYou\tnow\tknow\thow\tto\treduce\tthe\tdimensionality\tof\tany\tdataset\tdown\tto\tany\tnumber\tof dimensions,\twhile\tpreserving\tas\tmuch\tvariance\tas\tpossible.",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "Using\tScikit-Learn Scikit-Learn’s\tPCA\tclass\timplements\tPCA\tusing\tSVD\tdecomposition\tjust\tlike\twe\tdid\tbefore.\tThe following\tcode\tapplies\tPCA\tto\treduce\tthe\tdimensionality\tof\tthe\tdataset\tdown\tto\ttwo\tdimensions\t(note that\tit\tautomatically\ttakes\tcare\tof\tcentering\tthe\tdata):\n\nfrom\tsklearn.decomposition\timport\tPCA\n\npca\t=\tPCA(n_components\t=\t2) X2D\t=\tpca.fit_transform(X)\n\nAfter\tfitting\tthe\tPCA\ttransformer\tto\tthe\tdataset,\tyou\tcan\taccess\tthe\tprincipal\tcomponents\tusing\tthe components_\tvariable\t(note\tthat\tit\tcontains\tthe\tPCs\tas\thorizontal\tvectors,\tso,\tfor\texample,\tthe\tfirst principal\tcomponent\tis\tequal\tto\tpca.components_.T[:,\t0]).",
      "content_length": 624,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Explained\tVariance\tRatio Another\tvery\tuseful\tpiece\tof\tinformation\tis\tthe\texplained\tvariance\tratio\tof\teach\tprincipal\tcomponent, available\tvia\tthe\texplained_variance_ratio_\tvariable.\tIt\tindicates\tthe\tproportion\tof\tthe\tdataset’s variance\tthat\tlies\talong\tthe\taxis\tof\teach\tprincipal\tcomponent.\tFor\texample,\tlet’s\tlook\tat\tthe\texplained variance\tratios\tof\tthe\tfirst\ttwo\tcomponents\tof\tthe\t3D\tdataset\trepresented\tin\tFigure\t8-2:\n\n>>>\tpca.explained_variance_ratio_ array([\t0.84248607,\t\t0.14631839])\n\nThis\ttells\tyou\tthat\t84.2%\tof\tthe\tdataset’s\tvariance\tlies\talong\tthe\tfirst\taxis,\tand\t14.6%\tlies\talong\tthe second\taxis.\tThis\tleaves\tless\tthan\t1.2%\tfor\tthe\tthird\taxis,\tso\tit\tis\treasonable\tto\tassume\tthat\tit\tprobably carries\tlittle\tinformation.",
      "content_length": 727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "Choosing\tthe\tRight\tNumber\tof\tDimensions Instead\tof\tarbitrarily\tchoosing\tthe\tnumber\tof\tdimensions\tto\treduce\tdown\tto,\tit\tis\tgenerally\tpreferable\tto choose\tthe\tnumber\tof\tdimensions\tthat\tadd\tup\tto\ta\tsufficiently\tlarge\tportion\tof\tthe\tvariance\t(e.g.,\t95%). Unless,\tof\tcourse,\tyou\tare\treducing\tdimensionality\tfor\tdata\tvisualization\t—\tin\tthat\tcase\tyou\twill generally\twant\tto\treduce\tthe\tdimensionality\tdown\tto\t2\tor\t3.\n\nThe\tfollowing\tcode\tcomputes\tPCA\twithout\treducing\tdimensionality,\tthen\tcomputes\tthe\tminimum\tnumber of\tdimensions\trequired\tto\tpreserve\t95%\tof\tthe\ttraining\tset’s\tvariance:\n\npca\t=\tPCA() pca.fit(X_train) cumsum\t=\tnp.cumsum(pca.explained_variance_ratio_) d\t=\tnp.argmax(cumsum\t>=\t0.95)\t+\t1\n\nYou\tcould\tthen\tset\tn_components=d\tand\trun\tPCA\tagain.\tHowever,\tthere\tis\ta\tmuch\tbetter\toption:\tinstead of\tspecifying\tthe\tnumber\tof\tprincipal\tcomponents\tyou\twant\tto\tpreserve,\tyou\tcan\tset\tn_components\tto\tbe a\tfloat\tbetween\t0.0\tand\t1.0,\tindicating\tthe\tratio\tof\tvariance\tyou\twish\tto\tpreserve:\n\npca\t=\tPCA(n_components=0.95) X_reduced\t=\tpca.fit_transform(X_train)\n\nYet\tanother\toption\tis\tto\tplot\tthe\texplained\tvariance\tas\ta\tfunction\tof\tthe\tnumber\tof\tdimensions\t(simply\tplot cumsum;\tsee\tFigure\t8-8).\tThere\twill\tusually\tbe\tan\telbow\tin\tthe\tcurve,\twhere\tthe\texplained\tvariance\tstops growing\tfast.\tYou\tcan\tthink\tof\tthis\tas\tthe\tintrinsic\tdimensionality\tof\tthe\tdataset.\tIn\tthis\tcase,\tyou\tcan\tsee that\treducing\tthe\tdimensionality\tdown\tto\tabout\t100\tdimensions\twouldn’t\tlose\ttoo\tmuch\texplained variance.\n\nFigure\t8-8.\tExplained\tvariance\tas\ta\tfunction\tof\tthe\tnumber\tof\tdimensions",
      "content_length": 1552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "PCA\tfor\tCompression Obviously\tafter\tdimensionality\treduction,\tthe\ttraining\tset\ttakes\tup\tmuch\tless\tspace.\tFor\texample,\ttry applying\tPCA\tto\tthe\tMNIST\tdataset\twhile\tpreserving\t95%\tof\tits\tvariance.\tYou\tshould\tfind\tthat\teach instance\twill\thave\tjust\tover\t150\tfeatures,\tinstead\tof\tthe\toriginal\t784\tfeatures.\tSo\twhile\tmost\tof\tthe variance\tis\tpreserved,\tthe\tdataset\tis\tnow\tless\tthan\t20%\tof\tits\toriginal\tsize!\tThis\tis\ta\treasonable compression\tratio,\tand\tyou\tcan\tsee\thow\tthis\tcan\tspeed\tup\ta\tclassification\talgorithm\t(such\tas\tan\tSVM classifier)\ttremendously.\n\nIt\tis\talso\tpossible\tto\tdecompress\tthe\treduced\tdataset\tback\tto\t784\tdimensions\tby\tapplying\tthe\tinverse transformation\tof\tthe\tPCA\tprojection.\tOf\tcourse\tthis\twon’t\tgive\tyou\tback\tthe\toriginal\tdata,\tsince\tthe projection\tlost\ta\tbit\tof\tinformation\t(within\tthe\t5%\tvariance\tthat\twas\tdropped),\tbut\tit\twill\tlikely\tbe\tquite close\tto\tthe\toriginal\tdata.\tThe\tmean\tsquared\tdistance\tbetween\tthe\toriginal\tdata\tand\tthe\treconstructed\tdata (compressed\tand\tthen\tdecompressed)\tis\tcalled\tthe\treconstruction\terror.\tFor\texample,\tthe\tfollowing\tcode compresses\tthe\tMNIST\tdataset\tdown\tto\t154\tdimensions,\tthen\tuses\tthe\tinverse_transform()\tmethod\tto decompress\tit\tback\tto\t784\tdimensions.\tFigure\t8-9\tshows\ta\tfew\tdigits\tfrom\tthe\toriginal\ttraining\tset\t(on\tthe left),\tand\tthe\tcorresponding\tdigits\tafter\tcompression\tand\tdecompression.\tYou\tcan\tsee\tthat\tthere\tis\ta\tslight image\tquality\tloss,\tbut\tthe\tdigits\tare\tstill\tmostly\tintact.\n\npca\t=\tPCA(n_components\t=\t154) X_reduced\t=\tpca.fit_transform(X_train) X_recovered\t=\tpca.inverse_transform(X_reduced)\n\nFigure\t8-9.\tMNIST\tcompression\tpreserving\t95%\tof\tthe\tvariance\n\nThe\tequation\tof\tthe\tinverse\ttransformation\tis\tshown\tin\tEquation\t8-3.\n\nEquation\t8-3.\tPCA\tinverse\ttransformation,\tback\tto\tthe\toriginal\tnumber\tof\tdimensions",
      "content_length": 1774,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "Incremental\tPCA One\tproblem\twith\tthe\tpreceding\timplementation\tof\tPCA\tis\tthat\tit\trequires\tthe\twhole\ttraining\tset\tto\tfit\tin memory\tin\torder\tfor\tthe\tSVD\talgorithm\tto\trun.\tFortunately,\tIncremental\tPCA\t(IPCA)\talgorithms\thave been\tdeveloped:\tyou\tcan\tsplit\tthe\ttraining\tset\tinto\tmini-batches\tand\tfeed\tan\tIPCA\talgorithm\tone\tmini- batch\tat\ta\ttime.\tThis\tis\tuseful\tfor\tlarge\ttraining\tsets,\tand\talso\tto\tapply\tPCA\tonline\t(i.e.,\ton\tthe\tfly,\tas\tnew instances\tarrive).\n\nThe\tfollowing\tcode\tsplits\tthe\tMNIST\tdataset\tinto\t100\tmini-batches\t(using\tNumPy’s\tarray_split() function)\tand\tfeeds\tthem\tto\tScikit-Learn’s\tIncrementalPCA\tclass5\tto\treduce\tthe\tdimensionality\tof\tthe MNIST\tdataset\tdown\tto\t154\tdimensions\t(just\tlike\tbefore).\tNote\tthat\tyou\tmust\tcall\tthe\tpartial_fit() method\twith\teach\tmini-batch\trather\tthan\tthe\tfit()\tmethod\twith\tthe\twhole\ttraining\tset:\n\nfrom\tsklearn.decomposition\timport\tIncrementalPCA\n\nn_batches\t=\t100 inc_pca\t=\tIncrementalPCA(n_components=154) for\tX_batch\tin\tnp.array_split(X_train,\tn_batches): \t\t\t\tinc_pca.partial_fit(X_batch)\n\nX_reduced\t=\tinc_pca.transform(X_train)\n\nAlternatively,\tyou\tcan\tuse\tNumPy’s\tmemmap\tclass,\twhich\tallows\tyou\tto\tmanipulate\ta\tlarge\tarray\tstored\tin a\tbinary\tfile\ton\tdisk\tas\tif\tit\twere\tentirely\tin\tmemory;\tthe\tclass\tloads\tonly\tthe\tdata\tit\tneeds\tin\tmemory, when\tit\tneeds\tit.\tSince\tthe\tIncrementalPCA\tclass\tuses\tonly\ta\tsmall\tpart\tof\tthe\tarray\tat\tany\tgiven\ttime, the\tmemory\tusage\tremains\tunder\tcontrol.\tThis\tmakes\tit\tpossible\tto\tcall\tthe\tusual\tfit()\tmethod,\tas\tyou can\tsee\tin\tthe\tfollowing\tcode:\n\nX_mm\t=\tnp.memmap(filename,\tdtype=\"float32\",\tmode=\"readonly\",\tshape=(m,\tn))\n\nbatch_size\t=\tm\t//\tn_batches inc_pca\t=\tIncrementalPCA(n_components=154,\tbatch_size=batch_size) inc_pca.fit(X_mm)",
      "content_length": 1705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "Randomized\tPCA Scikit-Learn\toffers\tyet\tanother\toption\tto\tperform\tPCA,\tcalled\tRandomized\tPCA.\tThis\tis\ta\tstochastic algorithm\tthat\tquickly\tfinds\tan\tapproximation\tof\tthe\tfirst\td\tprincipal\tcomponents.\tIts\tcomputational complexity\tis\tO(m\t×\td2)\t+\tO(d3),\tinstead\tof\tO(m\t×\tn2)\t+\tO(n3),\tso\tit\tis\tdramatically\tfaster\tthan\tthe previous\talgorithms\twhen\td\tis\tmuch\tsmaller\tthan\tn.\n\nrnd_pca\t=\tPCA(n_components=154,\tsvd_solver=\"randomized\") X_reduced\t=\trnd_pca.fit_transform(X_train)",
      "content_length": 467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "Kernel\tPCA In\tChapter\t5\twe\tdiscussed\tthe\tkernel\ttrick,\ta\tmathematical\ttechnique\tthat\timplicitly\tmaps\tinstances\tinto\ta very\thigh-dimensional\tspace\t(called\tthe\tfeature\tspace),\tenabling\tnonlinear\tclassification\tand\tregression with\tSupport\tVector\tMachines.\tRecall\tthat\ta\tlinear\tdecision\tboundary\tin\tthe\thigh-dimensional\tfeature space\tcorresponds\tto\ta\tcomplex\tnonlinear\tdecision\tboundary\tin\tthe\toriginal\tspace.\n\nIt\tturns\tout\tthat\tthe\tsame\ttrick\tcan\tbe\tapplied\tto\tPCA,\tmaking\tit\tpossible\tto\tperform\tcomplex\tnonlinear projections\tfor\tdimensionality\treduction.\tThis\tis\tcalled\tKernel\tPCA\t(kPCA).6\tIt\tis\toften\tgood\tat preserving\tclusters\tof\tinstances\tafter\tprojection,\tor\tsometimes\teven\tunrolling\tdatasets\tthat\tlie\tclose\tto\ta twisted\tmanifold.\n\nFor\texample,\tthe\tfollowing\tcode\tuses\tScikit-Learn’s\tKernelPCA\tclass\tto\tperform\tkPCA\twith\tan\tRBF kernel\t(see\tChapter\t5\tfor\tmore\tdetails\tabout\tthe\tRBF\tkernel\tand\tthe\tother\tkernels):\n\nfrom\tsklearn.decomposition\timport\tKernelPCA\n\nrbf_pca\t=\tKernelPCA(n_components\t=\t2,\tkernel=\"rbf\",\tgamma=0.04) X_reduced\t=\trbf_pca.fit_transform(X)\n\nFigure\t8-10\tshows\tthe\tSwiss\troll,\treduced\tto\ttwo\tdimensions\tusing\ta\tlinear\tkernel\t(equivalent\tto\tsimply using\tthe\tPCA\tclass),\tan\tRBF\tkernel,\tand\ta\tsigmoid\tkernel\t(Logistic).\n\nFigure\t8-10.\tSwiss\troll\treduced\tto\t2D\tusing\tkPCA\twith\tvarious\tkernels",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "Selecting\ta\tKernel\tand\tTuning\tHyperparameters As\tkPCA\tis\tan\tunsupervised\tlearning\talgorithm,\tthere\tis\tno\tobvious\tperformance\tmeasure\tto\thelp\tyou select\tthe\tbest\tkernel\tand\thyperparameter\tvalues.\tHowever,\tdimensionality\treduction\tis\toften\ta preparation\tstep\tfor\ta\tsupervised\tlearning\ttask\t(e.g.,\tclassification),\tso\tyou\tcan\tsimply\tuse\tgrid\tsearch\tto select\tthe\tkernel\tand\thyperparameters\tthat\tlead\tto\tthe\tbest\tperformance\ton\tthat\ttask.\tFor\texample,\tthe following\tcode\tcreates\ta\ttwo-step\tpipeline,\tfirst\treducing\tdimensionality\tto\ttwo\tdimensions\tusing\tkPCA, then\tapplying\tLogistic\tRegression\tfor\tclassification.\tThen\tit\tuses\tGridSearchCV\tto\tfind\tthe\tbest\tkernel and\tgamma\tvalue\tfor\tkPCA\tin\torder\tto\tget\tthe\tbest\tclassification\taccuracy\tat\tthe\tend\tof\tthe\tpipeline:\n\nfrom\tsklearn.model_selection\timport\tGridSearchCV from\tsklearn.linear_model\timport\tLogisticRegression from\tsklearn.pipeline\timport\tPipeline\n\nclf\t=\tPipeline([ \t\t\t\t\t\t\t\t(\"kpca\",\tKernelPCA(n_components=2)), \t\t\t\t\t\t\t\t(\"log_reg\",\tLogisticRegression()) \t\t\t\t])\n\nparam_grid\t=\t[{ \t\t\t\t\t\t\t\t\"kpca__gamma\":\tnp.linspace(0.03,\t0.05,\t10), \t\t\t\t\t\t\t\t\"kpca__kernel\":\t[\"rbf\",\t\"sigmoid\"] \t\t\t\t}]\n\ngrid_search\t=\tGridSearchCV(clf,\tparam_grid,\tcv=3) grid_search.fit(X,\ty)\n\nThe\tbest\tkernel\tand\thyperparameters\tare\tthen\tavailable\tthrough\tthe\tbest_params_\tvariable:\n\n>>>\tprint(grid_search.best_params_) {'kpca__gamma':\t0.043333333333333335,\t'kpca__kernel':\t'rbf'}\n\nAnother\tapproach,\tthis\ttime\tentirely\tunsupervised,\tis\tto\tselect\tthe\tkernel\tand\thyperparameters\tthat\tyield the\tlowest\treconstruction\terror.\tHowever,\treconstruction\tis\tnot\tas\teasy\tas\twith\tlinear\tPCA.\tHere’s\twhy. Figure\t8-11\tshows\tthe\toriginal\tSwiss\troll\t3D\tdataset\t(top\tleft),\tand\tthe\tresulting\t2D\tdataset\tafter\tkPCA\tis applied\tusing\tan\tRBF\tkernel\t(top\tright).\tThanks\tto\tthe\tkernel\ttrick,\tthis\tis\tmathematically\tequivalent\tto mapping\tthe\ttraining\tset\tto\tan\tinfinite-dimensional\tfeature\tspace\t(bottom\tright)\tusing\tthe\tfeature\tmap\tφ, then\tprojecting\tthe\ttransformed\ttraining\tset\tdown\tto\t2D\tusing\tlinear\tPCA.\tNotice\tthat\tif\twe\tcould\tinvert the\tlinear\tPCA\tstep\tfor\ta\tgiven\tinstance\tin\tthe\treduced\tspace,\tthe\treconstructed\tpoint\twould\tlie\tin\tfeature space,\tnot\tin\tthe\toriginal\tspace\t(e.g.,\tlike\tthe\tone\trepresented\tby\tan\tx\tin\tthe\tdiagram).\tSince\tthe\tfeature space\tis\tinfinite-dimensional,\twe\tcannot\tcompute\tthe\treconstructed\tpoint,\tand\ttherefore\twe\tcannot compute\tthe\ttrue\treconstruction\terror.\tFortunately,\tit\tis\tpossible\tto\tfind\ta\tpoint\tin\tthe\toriginal\tspace\tthat would\tmap\tclose\tto\tthe\treconstructed\tpoint.\tThis\tis\tcalled\tthe\treconstruction\tpre-image.\tOnce\tyou\thave this\tpre-image,\tyou\tcan\tmeasure\tits\tsquared\tdistance\tto\tthe\toriginal\tinstance.\tYou\tcan\tthen\tselect\tthe kernel\tand\thyperparameters\tthat\tminimize\tthis\treconstruction\tpre-image\terror.",
      "content_length": 2738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "Figure\t8-11.\tKernel\tPCA\tand\tthe\treconstruction\tpre-image\terror\n\nYou\tmay\tbe\twondering\thow\tto\tperform\tthis\treconstruction.\tOne\tsolution\tis\tto\ttrain\ta\tsupervised regression\tmodel,\twith\tthe\tprojected\tinstances\tas\tthe\ttraining\tset\tand\tthe\toriginal\tinstances\tas\tthe\ttargets. Scikit-Learn\twill\tdo\tthis\tautomatically\tif\tyou\tset\tfit_inverse_transform=True,\tas\tshown\tin\tthe following\tcode:7\n\nrbf_pca\t=\tKernelPCA(n_components\t=\t2,\tkernel=\"rbf\",\tgamma=0.0433, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfit_inverse_transform=True) X_reduced\t=\trbf_pca.fit_transform(X) X_preimage\t=\trbf_pca.inverse_transform(X_reduced)\n\nNOTE\n\nBy\tdefault,\tfit_inverse_transform=False\tand\tKernelPCA\thas\tno\tinverse_transform()\tmethod.\tThis\tmethod\tonly\tgets created\twhen\tyou\tset\tfit_inverse_transform=True.\n\nYou\tcan\tthen\tcompute\tthe\treconstruction\tpre-image\terror:\n\n>>>\tfrom\tsklearn.metrics\timport\tmean_squared_error >>>\tmean_squared_error(X,\tX_preimage) 32.786308795766132\n\nNow\tyou\tcan\tuse\tgrid\tsearch\twith\tcross-validation\tto\tfind\tthe\tkernel\tand\thyperparameters\tthat\tminimize this\tpre-image\treconstruction\terror.",
      "content_length": 1057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "LLE Locally\tLinear\tEmbedding\t(LLE)8\tis\tanother\tvery\tpowerful\tnonlinear\tdimensionality\treduction (NLDR)\ttechnique.\tIt\tis\ta\tManifold\tLearning\ttechnique\tthat\tdoes\tnot\trely\ton\tprojections\tlike\tthe\tprevious algorithms.\tIn\ta\tnutshell,\tLLE\tworks\tby\tfirst\tmeasuring\thow\teach\ttraining\tinstance\tlinearly\trelates\tto\tits closest\tneighbors\t(c.n.),\tand\tthen\tlooking\tfor\ta\tlow-dimensional\trepresentation\tof\tthe\ttraining\tset\twhere these\tlocal\trelationships\tare\tbest\tpreserved\t(more\tdetails\tshortly).\tThis\tmakes\tit\tparticularly\tgood\tat unrolling\ttwisted\tmanifolds,\tespecially\twhen\tthere\tis\tnot\ttoo\tmuch\tnoise.\n\nFor\texample,\tthe\tfollowing\tcode\tuses\tScikit-Learn’s\tLocallyLinearEmbedding\tclass\tto\tunroll\tthe\tSwiss roll.\tThe\tresulting\t2D\tdataset\tis\tshown\tin\tFigure\t8-12.\tAs\tyou\tcan\tsee,\tthe\tSwiss\troll\tis\tcompletely unrolled\tand\tthe\tdistances\tbetween\tinstances\tare\tlocally\twell\tpreserved.\tHowever,\tdistances\tare\tnot preserved\ton\ta\tlarger\tscale:\tthe\tleft\tpart\tof\tthe\tunrolled\tSwiss\troll\tis\tsqueezed,\twhile\tthe\tright\tpart\tis stretched.\tNevertheless,\tLLE\tdid\ta\tpretty\tgood\tjob\tat\tmodeling\tthe\tmanifold.\n\nfrom\tsklearn.manifold\timport\tLocallyLinearEmbedding\n\nlle\t=\tLocallyLinearEmbedding(n_components=2,\tn_neighbors=10) X_reduced\t=\tlle.fit_transform(X)\n\nFigure\t8-12.\tUnrolled\tSwiss\troll\tusing\tLLE\n\nHere’s\thow\tLLE\tworks:\tfirst,\tfor\teach\ttraining\tinstance\tx(i),\tthe\talgorithm\tidentifies\tits\tk\tclosest neighbors\t(in\tthe\tpreceding\tcode\tk\t=\t10),\tthen\ttries\tto\treconstruct\tx(i)\tas\ta\tlinear\tfunction\tof\tthese neighbors.\tMore\tspecifically,\tit\tfinds\tthe\tweights\twi,j\tsuch\tthat\tthe\tsquared\tdistance\tbetween\tx(i)\tand\t \tis\tas\tsmall\tas\tpossible,\tassuming\twi,j\t=\t0\tif\tx(j)\tis\tnot\tone\tof\tthe\tk\tclosest\tneighbors\tof\tx(i).\n\nThus\tthe\tfirst\tstep\tof\tLLE\tis\tthe\tconstrained\toptimization\tproblem\tdescribed\tin\tEquation\t8-4,\twhere\tW\tis",
      "content_length": 1786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "the\tweight\tmatrix\tcontaining\tall\tthe\tweights\twi,j.\tThe\tsecond\tconstraint\tsimply\tnormalizes\tthe\tweights\tfor each\ttraining\tinstance\tx(i).\n\nEquation\t8-4.\tLLE\tstep\t1:\tlinearly\tmodeling\tlocal\trelationships\n\nAfter\tthis\tstep,\tthe\tweight\tmatrix\t between\tthe\ttraining\tinstances.\tNow\tthe\tsecond\tstep\tis\tto\tmap\tthe\ttraining\tinstances\tinto\ta\td-dimensional space\t(where\td\t<\tn)\twhile\tpreserving\tthese\tlocal\trelationships\tas\tmuch\tas\tpossible.\tIf\tz(i)\tis\tthe\timage\tof\n\n(containing\tthe\tweights\n\n)\tencodes\tthe\tlocal\tlinear\trelationships\n\nx(i)\tin\tthis\td-dimensional\tspace,\tthen\twe\twant\tthe\tsquared\tdistance\tbetween\tz(i)\tand\t small\tas\tpossible.\tThis\tidea\tleads\tto\tthe\tunconstrained\toptimization\tproblem\tdescribed\tin\tEquation\t8-5.\tIt looks\tvery\tsimilar\tto\tthe\tfirst\tstep,\tbut\tinstead\tof\tkeeping\tthe\tinstances\tfixed\tand\tfinding\tthe\toptimal weights,\twe\tare\tdoing\tthe\treverse:\tkeeping\tthe\tweights\tfixed\tand\tfinding\tthe\toptimal\tposition\tof\tthe instances’\timages\tin\tthe\tlow-dimensional\tspace.\tNote\tthat\tZ\tis\tthe\tmatrix\tcontaining\tall\tz(i).\n\nto\tbe\tas\n\nEquation\t8-5.\tLLE\tstep\t2:\treducing\tdimensionality\twhile\tpreserving\trelationships\n\nScikit-Learn’s\tLLE\timplementation\thas\tthe\tfollowing\tcomputational\tcomplexity:\tO(m\tlog(m)n\tlog(k))\tfor finding\tthe\tk\tnearest\tneighbors,\tO(mnk3)\tfor\toptimizing\tthe\tweights,\tand\tO(dm2)\tfor\tconstructing\tthe\tlow- dimensional\trepresentations.\tUnfortunately,\tthe\tm2\tin\tthe\tlast\tterm\tmakes\tthis\talgorithm\tscale\tpoorly\tto very\tlarge\tdatasets.",
      "content_length": 1440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "Other\tDimensionality\tReduction\tTechniques There\tare\tmany\tother\tdimensionality\treduction\ttechniques,\tseveral\tof\twhich\tare\tavailable\tin\tScikit-Learn. Here\tare\tsome\tof\tthe\tmost\tpopular:\n\nMultidimensional\tScaling\t(MDS)\treduces\tdimensionality\twhile\ttrying\tto\tpreserve\tthe\tdistances between\tthe\tinstances\t(see\tFigure\t8-13).\n\nIsomap\tcreates\ta\tgraph\tby\tconnecting\teach\tinstance\tto\tits\tnearest\tneighbors,\tthen\treduces dimensionality\twhile\ttrying\tto\tpreserve\tthe\tgeodesic\tdistances9\tbetween\tthe\tinstances.\n\nt-Distributed\tStochastic\tNeighbor\tEmbedding\t(t-SNE)\treduces\tdimensionality\twhile\ttrying\tto\tkeep similar\tinstances\tclose\tand\tdissimilar\tinstances\tapart.\tIt\tis\tmostly\tused\tfor\tvisualization,\tin particular\tto\tvisualize\tclusters\tof\tinstances\tin\thigh-dimensional\tspace\t(e.g.,\tto\tvisualize\tthe\tMNIST images\tin\t2D).\n\nLinear\tDiscriminant\tAnalysis\t(LDA)\tis\tactually\ta\tclassification\talgorithm,\tbut\tduring\ttraining\tit learns\tthe\tmost\tdiscriminative\taxes\tbetween\tthe\tclasses,\tand\tthese\taxes\tcan\tthen\tbe\tused\tto\tdefine\ta hyperplane\tonto\twhich\tto\tproject\tthe\tdata.\tThe\tbenefit\tis\tthat\tthe\tprojection\twill\tkeep\tclasses\tas\tfar apart\tas\tpossible,\tso\tLDA\tis\ta\tgood\ttechnique\tto\treduce\tdimensionality\tbefore\trunning\tanother classification\talgorithm\tsuch\tas\tan\tSVM\tclassifier.\n\nFigure\t8-13.\tReducing\tthe\tSwiss\troll\tto\t2D\tusing\tvarious\ttechniques",
      "content_length": 1323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "Exercises\n\n1.\t What\tare\tthe\tmain\tmotivations\tfor\treducing\ta\tdataset’s\tdimensionality?\tWhat\tare\tthe\tmain drawbacks?\n\n2.\t What\tis\tthe\tcurse\tof\tdimensionality?\n\n3.\t Once\ta\tdataset’s\tdimensionality\thas\tbeen\treduced,\tis\tit\tpossible\tto\treverse\tthe\toperation?\tIf\tso, how?\tIf\tnot,\twhy?\n\n4.\t Can\tPCA\tbe\tused\tto\treduce\tthe\tdimensionality\tof\ta\thighly\tnonlinear\tdataset?\n\n5.\t Suppose\tyou\tperform\tPCA\ton\ta\t1,000-dimensional\tdataset,\tsetting\tthe\texplained\tvariance\tratio\tto 95%.\tHow\tmany\tdimensions\twill\tthe\tresulting\tdataset\thave?\n\n6.\t In\twhat\tcases\twould\tyou\tuse\tvanilla\tPCA,\tIncremental\tPCA,\tRandomized\tPCA,\tor\tKernel\tPCA?\n\n7.\t How\tcan\tyou\tevaluate\tthe\tperformance\tof\ta\tdimensionality\treduction\talgorithm\ton\tyour\tdataset?\n\n8.\t Does\tit\tmake\tany\tsense\tto\tchain\ttwo\tdifferent\tdimensionality\treduction\talgorithms?\n\n9.\t Load\tthe\tMNIST\tdataset\t(introduced\tin\tChapter\t3)\tand\tsplit\tit\tinto\ta\ttraining\tset\tand\ta\ttest\tset\t(take the\tfirst\t60,000\tinstances\tfor\ttraining,\tand\tthe\tremaining\t10,000\tfor\ttesting).\tTrain\ta\tRandom\tForest classifier\ton\tthe\tdataset\tand\ttime\thow\tlong\tit\ttakes,\tthen\tevaluate\tthe\tresulting\tmodel\ton\tthe\ttest\tset. Next,\tuse\tPCA\tto\treduce\tthe\tdataset’s\tdimensionality,\twith\tan\texplained\tvariance\tratio\tof\t95%.\tTrain a\tnew\tRandom\tForest\tclassifier\ton\tthe\treduced\tdataset\tand\tsee\thow\tlong\tit\ttakes.\tWas\ttraining\tmuch faster?\tNext\tevaluate\tthe\tclassifier\ton\tthe\ttest\tset:\thow\tdoes\tit\tcompare\tto\tthe\tprevious\tclassifier?\n\n10.\t Use\tt-SNE\tto\treduce\tthe\tMNIST\tdataset\tdown\tto\ttwo\tdimensions\tand\tplot\tthe\tresult\tusing Matplotlib.\tYou\tcan\tuse\ta\tscatterplot\tusing\t10\tdifferent\tcolors\tto\trepresent\teach\timage’s\ttarget\tclass. Alternatively,\tyou\tcan\twrite\tcolored\tdigits\tat\tthe\tlocation\tof\teach\tinstance,\tor\teven\tplot\tscaled-down versions\tof\tthe\tdigit\timages\tthemselves\t(if\tyou\tplot\tall\tdigits,\tthe\tvisualization\twill\tbe\ttoo\tcluttered, so\tyou\tshould\teither\tdraw\ta\trandom\tsample\tor\tplot\tan\tinstance\tonly\tif\tno\tother\tinstance\thas\talready been\tplotted\tat\ta\tclose\tdistance).\tYou\tshould\tget\ta\tnice\tvisualization\twith\twell-separated\tclusters\tof digits.\tTry\tusing\tother\tdimensionality\treduction\talgorithms\tsuch\tas\tPCA,\tLLE,\tor\tMDS\tand\tcompare the\tresulting\tvisualizations.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nWell,\tfour\tdimensions\tif\tyou\tcount\ttime,\tand\ta\tfew\tmore\tif\tyou\tare\ta\tstring\ttheorist.\n\n2\n\nWatch\ta\trotating\ttesseract\tprojected\tinto\t3D\tspace\tat\thttp://goo.gl/OM7ktJ.\tImage\tby\tWikipedia\tuser\tNerdBoy1392\t(Creative\tCommons BY-SA\t3.0).\tReproduced\tfrom\thttps://en.wikipedia.org/wiki/Tesseract.\n\n3\n\nFun\tfact:\tanyone\tyou\tknow\tis\tprobably\tan\textremist\tin\tat\tleast\tone\tdimension\t(e.g.,\thow\tmuch\tsugar\tthey\tput\tin\ttheir\tcoffee),\tif\tyou consider\tenough\tdimensions.\n\n4\n\n“On\tLines\tand\tPlanes\tof\tClosest\tFit\tto\tSystems\tof\tPoints\tin\tSpace,”\tK.\tPearson\t(1901).\n\n5\n\nScikit-Learn\tuses\tthe\talgorithm\tdescribed\tin\t“Incremental\tLearning\tfor\tRobust\tVisual\tTracking,”\tD.\tRoss\tet\tal.\t(2007).",
      "content_length": 2882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "6\n\n7\n\n8\n\n9\n\n“Kernel\tPrincipal\tComponent\tAnalysis,”\tB.\tSchölkopf,\tA.\tSmola,\tK.\tMüller\t(1999).\n\nScikit-Learn\tuses\tthe\talgorithm\tbased\ton\tKernel\tRidge\tRegression\tdescribed\tin\tGokhan\tH.\tBakır,\tJason\tWeston,\tand\tBernhard\tScholkopf, “Learning\tto\tFind\tPre-images”\t(Tubingen,\tGermany:\tMax\tPlanck\tInstitute\tfor\tBiological\tCybernetics,\t2004).\n\n“Nonlinear\tDimensionality\tReduction\tby\tLocally\tLinear\tEmbedding,”\tS.\tRoweis,\tL.\tSaul\t(2000).\n\nThe\tgeodesic\tdistance\tbetween\ttwo\tnodes\tin\ta\tgraph\tis\tthe\tnumber\tof\tnodes\ton\tthe\tshortest\tpath\tbetween\tthese\tnodes.",
      "content_length": 543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "Part\tII.\tNeural\tNetworks\tand\tDeep\tLearning",
      "content_length": 42,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Chapter\t9.\tUp\tand\tRunning\twith\tTensorFlow\n\nTensorFlow\tis\ta\tpowerful\topen\tsource\tsoftware\tlibrary\tfor\tnumerical\tcomputation,\tparticularly\twell suited\tand\tfine-tuned\tfor\tlarge-scale\tMachine\tLearning.\tIts\tbasic\tprinciple\tis\tsimple:\tyou\tfirst\tdefine\tin Python\ta\tgraph\tof\tcomputations\tto\tperform\t(for\texample,\tthe\tone\tin\tFigure\t9-1),\tand\tthen\tTensorFlow takes\tthat\tgraph\tand\truns\tit\tefficiently\tusing\toptimized\tC++\tcode.\n\nFigure\t9-1.\tA\tsimple\tcomputation\tgraph\n\nMost\timportantly,\tit\tis\tpossible\tto\tbreak\tup\tthe\tgraph\tinto\tseveral\tchunks\tand\trun\tthem\tin\tparallel\tacross multiple\tCPUs\tor\tGPUs\t(as\tshown\tin\tFigure\t9-2).\tTensorFlow\talso\tsupports\tdistributed\tcomputing,\tso you\tcan\ttrain\tcolossal\tneural\tnetworks\ton\thumongous\ttraining\tsets\tin\ta\treasonable\tamount\tof\ttime\tby splitting\tthe\tcomputations\tacross\thundreds\tof\tservers\t(see\tChapter\t12).\tTensorFlow\tcan\ttrain\ta\tnetwork with\tmillions\tof\tparameters\ton\ta\ttraining\tset\tcomposed\tof\tbillions\tof\tinstances\twith\tmillions\tof\tfeatures each.\tThis\tshould\tcome\tas\tno\tsurprise,\tsince\tTensorFlow\twas\tdeveloped\tby\tthe\tGoogle\tBrain\tteam\tand\tit powers\tmany\tof\tGoogle’s\tlarge-scale\tservices,\tsuch\tas\tGoogle\tCloud\tSpeech,\tGoogle\tPhotos,\tand Google\tSearch.",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Figure\t9-2.\tParallel\tcomputation\ton\tmultiple\tCPUs/GPUs/servers\n\nWhen\tTensorFlow\twas\topen-sourced\tin\tNovember\t2015,\tthere\twere\talready\tmany\tpopular\topen\tsource libraries\tfor\tDeep\tLearning\t(Table\t9-1\tlists\ta\tfew),\tand\tto\tbe\tfair\tmost\tof\tTensorFlow’s\tfeatures\talready existed\tin\tone\tlibrary\tor\tanother.\tNevertheless,\tTensorFlow’s\tclean\tdesign,\tscalability,\tflexibility,1\tand great\tdocumentation\t(not\tto\tmention\tGoogle’s\tname)\tquickly\tboosted\tit\tto\tthe\ttop\tof\tthe\tlist.\tIn\tshort, TensorFlow\twas\tdesigned\tto\tbe\tflexible,\tscalable,\tand\tproduction-ready,\tand\texisting\tframeworks arguably\thit\tonly\ttwo\tout\tof\tthe\tthree\tof\tthese.\tHere\tare\tsome\tof\tTensorFlow’s\thighlights:\n\nIt\truns\tnot\tonly\ton\tWindows,\tLinux,\tand\tmacOS,\tbut\talso\ton\tmobile\tdevices,\tincluding\tboth\tiOS\tand Android.\n\nIt\tprovides\ta\tvery\tsimple\tPython\tAPI\tcalled\tTF.Learn2\t(tensorflow.contrib.learn),\tcompatible with\tScikit-Learn.\tAs\tyou\twill\tsee,\tyou\tcan\tuse\tit\tto\ttrain\tvarious\ttypes\tof\tneural\tnetworks\tin\tjust\ta few\tlines\tof\tcode.\tIt\twas\tpreviously\tan\tindependent\tproject\tcalled\tScikit\tFlow\t(or\tskflow).\n\nIt\talso\tprovides\tanother\tsimple\tAPI\tcalled\tTF-slim\t(tensorflow.contrib.slim)\tto\tsimplify building,\ttraining,\tand\tevaluating\tneural\tnetworks.\n\nSeveral\tother\thigh-level\tAPIs\thave\tbeen\tbuilt\tindependently\ton\ttop\tof\tTensorFlow,\tsuch\tas\tKeras (now\tavailable\tin\ttensorflow.contrib.keras)\tor\tPretty\tTensor.",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "Its\tmain\tPython\tAPI\toffers\tmuch\tmore\tflexibility\t(at\tthe\tcost\tof\thigher\tcomplexity)\tto\tcreate\tall\tsorts of\tcomputations,\tincluding\tany\tneural\tnetwork\tarchitecture\tyou\tcan\tthink\tof.\n\nIt\tincludes\thighly\tefficient\tC++\timplementations\tof\tmany\tML\toperations,\tparticularly\tthose\tneeded\tto build\tneural\tnetworks.\tThere\tis\talso\ta\tC++\tAPI\tto\tdefine\tyour\town\thigh-performance\toperations.\n\nIt\tprovides\tseveral\tadvanced\toptimization\tnodes\tto\tsearch\tfor\tthe\tparameters\tthat\tminimize\ta\tcost function.\tThese\tare\tvery\teasy\tto\tuse\tsince\tTensorFlow\tautomatically\ttakes\tcare\tof\tcomputing\tthe gradients\tof\tthe\tfunctions\tyou\tdefine.\tThis\tis\tcalled\tautomatic\tdifferentiating\t(or\tautodiff).\n\nIt\talso\tcomes\twith\ta\tgreat\tvisualization\ttool\tcalled\tTensorBoard\tthat\tallows\tyou\tto\tbrowse\tthrough the\tcomputation\tgraph,\tview\tlearning\tcurves,\tand\tmore.\n\nGoogle\talso\tlaunched\ta\tcloud\tservice\tto\trun\tTensorFlow\tgraphs.\n\nLast\tbut\tnot\tleast,\tit\thas\ta\tdedicated\tteam\tof\tpassionate\tand\thelpful\tdevelopers,\tand\ta\tgrowing community\tcontributing\tto\timproving\tit.\tIt\tis\tone\tof\tthe\tmost\tpopular\topen\tsource\tprojects\ton GitHub,\tand\tmore\tand\tmore\tgreat\tprojects\tare\tbeing\tbuilt\ton\ttop\tof\tit\t(for\texamples,\tcheck\tout\tthe resources\tpage\ton\thttps://www.tensorflow.org/,\tor\thttps://github.com/jtoy/awesome-tensorflow). To\task\ttechnical\tquestions,\tyou\tshould\tuse\thttp://stackoverflow.com/\tand\ttag\tyour\tquestion\twith \"tensorflow\".\tYou\tcan\tfile\tbugs\tand\tfeature\trequests\tthrough\tGitHub.\tFor\tgeneral\tdiscussions,\tjoin the\tGoogle\tgroup.\n\nIn\tthis\tchapter,\twe\twill\tgo\tthrough\tthe\tbasics\tof\tTensorFlow,\tfrom\tinstallation\tto\tcreating,\trunning,\tsaving, and\tvisualizing\tsimple\tcomputational\tgraphs.\tMastering\tthese\tbasics\tis\timportant\tbefore\tyou\tbuild\tyour first\tneural\tnetwork\t(which\twe\twill\tdo\tin\tthe\tnext\tchapter).\n\nTable\t9-1.\tOpen\tsource\tDeep\tLearning\tlibraries\t(not\tan\texhaustive\tlist)\n\nLibrary\n\nAPI\n\nPlatforms\n\nStarted\tby\n\nYear\n\nCaffe\n\nPython,\tC++,\tMatlab Linux,\tmacOS,\tWindows\n\nY.\tJia,\tUC\tBerkeley\t(BVLC)\n\n2013\n\nDeeplearning4j Java,\tScala,\tClojure Linux,\tmacOS,\tWindows,\tAndroid\n\nA.\tGibson,\tJ.Patterson\n\n2014\n\nH2O\n\nPython,\tR\n\nLinux,\tmacOS,\tWindows\n\nH2O.ai\n\n2014\n\nMXNet\n\nPython,\tC++,\tothers Linux,\tmacOS,\tWindows,\tiOS,\tAndroid DMLC\n\n2015\n\nTensorFlow\n\nPython,\tC++\n\nLinux,\tmacOS,\tWindows,\tiOS,\tAndroid Google\n\n2015\n\nTheano\n\nPython\n\nLinux,\tmacOS,\tiOS\n\nUniversity\tof\tMontreal\n\n2010\n\nTorch\n\nC++,\tLua\n\nLinux,\tmacOS,\tiOS,\tAndroid\n\nR.\tCollobert,\tK.\tKavukcuoglu,\tC.\tFarabet 2002",
      "content_length": 2417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "Installation Let’s\tget\tstarted!\tAssuming\tyou\tinstalled\tJupyter\tand\tScikit-Learn\tby\tfollowing\tthe\tinstallation instructions\tin\tChapter\t2,\tyou\tcan\tsimply\tuse\tpip\tto\tinstall\tTensorFlow.\tIf\tyou\tcreated\tan\tisolated environment\tusing\tvirtualenv,\tyou\tfirst\tneed\tto\tactivate\tit:\n\n$\tcd\t$ML_PATH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tYour\tML\tworking\tdirectory\t(e.g.,\t$HOME/ml) $\tsource\tenv/bin/activate\n\nNext,\tinstall\tTensorFlow:\n\n$\tpip3\tinstall\t--upgrade\ttensorflow\n\nNOTE\n\nFor\tGPU\tsupport,\tyou\tneed\tto\tinstall\ttensorflow-gpu\tinstead\tof\ttensorflow.\tSee\tChapter\t12\tfor\tmore\tdetails.\n\nTo\ttest\tyour\tinstallation,\ttype\tthe\tfollowing\tcommand.\tIt\tshould\toutput\tthe\tversion\tof\tTensorFlow\tyou installed.\n\n$\tpython3\t-c\t'import\ttensorflow;\tprint(tensorflow.__version__)' 1.0.0",
      "content_length": 734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "Creating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession The\tfollowing\tcode\tcreates\tthe\tgraph\trepresented\tin\tFigure\t9-1:\n\nimport\ttensorflow\tas\ttf\n\nx\t=\ttf.Variable(3,\tname=\"x\") y\t=\ttf.Variable(4,\tname=\"y\") f\t=\tx*x*y\t+\ty\t+\t2\n\nThat’s\tall\tthere\tis\tto\tit!\tThe\tmost\timportant\tthing\tto\tunderstand\tis\tthat\tthis\tcode\tdoes\tnot\tactually\tperform any\tcomputation,\teven\tthough\tit\tlooks\tlike\tit\tdoes\t(especially\tthe\tlast\tline).\tIt\tjust\tcreates\ta\tcomputation graph.\tIn\tfact,\teven\tthe\tvariables\tare\tnot\tinitialized\tyet.\tTo\tevaluate\tthis\tgraph,\tyou\tneed\tto\topen\ta TensorFlow\tsession\tand\tuse\tit\tto\tinitialize\tthe\tvariables\tand\tevaluate\tf.\tA\tTensorFlow\tsession\ttakes\tcare of\tplacing\tthe\toperations\tonto\tdevices\tsuch\tas\tCPUs\tand\tGPUs\tand\trunning\tthem,\tand\tit\tholds\tall\tthe variable\tvalues.3\tThe\tfollowing\tcode\tcreates\ta\tsession,\tinitializes\tthe\tvariables,\tand\tevaluates,\tand\tf\tthen closes\tthe\tsession\t(which\tfrees\tup\tresources):\n\n>>>\tsess\t=\ttf.Session() >>>\tsess.run(x.initializer) >>>\tsess.run(y.initializer) >>>\tresult\t=\tsess.run(f) >>>\tprint(result) 42 >>>\tsess.close()\n\nHaving\tto\trepeat\tsess.run()\tall\tthe\ttime\tis\ta\tbit\tcumbersome,\tbut\tfortunately\tthere\tis\ta\tbetter\tway:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tx.initializer.run() \t\t\t\ty.initializer.run() \t\t\t\tresult\t=\tf.eval()\n\nInside\tthe\twith\tblock,\tthe\tsession\tis\tset\tas\tthe\tdefault\tsession.\tCalling\tx.initializer.run()\tis equivalent\tto\tcalling\ttf.get_default_session().run(x.initializer),\tand\tsimilarly\tf.eval()\tis equivalent\tto\tcalling\ttf.get_default_session().run(f).\tThis\tmakes\tthe\tcode\teasier\tto\tread. Moreover,\tthe\tsession\tis\tautomatically\tclosed\tat\tthe\tend\tof\tthe\tblock.\n\nInstead\tof\tmanually\trunning\tthe\tinitializer\tfor\tevery\tsingle\tvariable,\tyou\tcan\tuse\tthe global_variables_initializer()\tfunction.\tNote\tthat\tit\tdoes\tnot\tactually\tperform\tthe\tinitialization immediately,\tbut\trather\tcreates\ta\tnode\tin\tthe\tgraph\tthat\twill\tinitialize\tall\tvariables\twhen\tit\tis\trun:\n\ninit\t=\ttf.global_variables_initializer()\t\t#\tprepare\tan\tinit\tnode\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run()\t\t#\tactually\tinitialize\tall\tthe\tvariables \t\t\t\tresult\t=\tf.eval()\n\nInside\tJupyter\tor\twithin\ta\tPython\tshell\tyou\tmay\tprefer\tto\tcreate\tan\tInteractiveSession.\tThe\tonly difference\tfrom\ta\tregular\tSession\tis\tthat\twhen\tan\tInteractiveSession\tis\tcreated\tit\tautomatically\tsets",
      "content_length": 2263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "itself\tas\tthe\tdefault\tsession,\tso\tyou\tdon’t\tneed\ta\twith\tblock\t(but\tyou\tdo\tneed\tto\tclose\tthe\tsession manually\twhen\tyou\tare\tdone\twith\tit):\n\n>>>\tsess\t=\ttf.InteractiveSession() >>>\tinit.run() >>>\tresult\t=\tf.eval() >>>\tprint(result) 42 >>>\tsess.close()\n\nA\tTensorFlow\tprogram\tis\ttypically\tsplit\tinto\ttwo\tparts:\tthe\tfirst\tpart\tbuilds\ta\tcomputation\tgraph\t(this\tis called\tthe\tconstruction\tphase),\tand\tthe\tsecond\tpart\truns\tit\t(this\tis\tthe\texecution\tphase).\tThe\tconstruction phase\ttypically\tbuilds\ta\tcomputation\tgraph\trepresenting\tthe\tML\tmodel\tand\tthe\tcomputations\trequired\tto train\tit.\tThe\texecution\tphase\tgenerally\truns\ta\tloop\tthat\tevaluates\ta\ttraining\tstep\trepeatedly\t(for\texample, one\tstep\tper\tmini-batch),\tgradually\timproving\tthe\tmodel\tparameters.\tWe\twill\tgo\tthrough\tan\texample shortly.",
      "content_length": 780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "Managing\tGraphs Any\tnode\tyou\tcreate\tis\tautomatically\tadded\tto\tthe\tdefault\tgraph:\n\n>>>\tx1\t=\ttf.Variable(1) >>>\tx1.graph\tis\ttf.get_default_graph() True\n\nIn\tmost\tcases\tthis\tis\tfine,\tbut\tsometimes\tyou\tmay\twant\tto\tmanage\tmultiple\tindependent\tgraphs.\tYou\tcan\tdo this\tby\tcreating\ta\tnew\tGraph\tand\ttemporarily\tmaking\tit\tthe\tdefault\tgraph\tinside\ta\twith\tblock,\tlike\tso:\n\n>>>\tgraph\t=\ttf.Graph() >>>\twith\tgraph.as_default(): ...\t\t\t\t\tx2\t=\ttf.Variable(2) ... >>>\tx2.graph\tis\tgraph True >>>\tx2.graph\tis\ttf.get_default_graph() False\n\nTIP\n\nIn\tJupyter\t(or\tin\ta\tPython\tshell),\tit\tis\tcommon\tto\trun\tthe\tsame\tcommands\tmore\tthan\tonce\twhile\tyou\tare\texperimenting.\tAs\ta result,\tyou\tmay\tend\tup\twith\ta\tdefault\tgraph\tcontaining\tmany\tduplicate\tnodes.\tOne\tsolution\tis\tto\trestart\tthe\tJupyter\tkernel\t(or\tthe Python\tshell),\tbut\ta\tmore\tconvenient\tsolution\tis\tto\tjust\treset\tthe\tdefault\tgraph\tby\trunning\ttf.reset_default_graph().",
      "content_length": 892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "Lifecycle\tof\ta\tNode\tValue When\tyou\tevaluate\ta\tnode,\tTensorFlow\tautomatically\tdetermines\tthe\tset\tof\tnodes\tthat\tit\tdepends\ton\tand\tit evaluates\tthese\tnodes\tfirst.\tFor\texample,\tconsider\tthe\tfollowing\tcode:\n\nw\t=\ttf.constant(3) x\t=\tw\t+\t2 y\t=\tx\t+\t5 z\t=\tx\t*\t3\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tprint(y.eval())\t\t#\t10 \t\t\t\tprint(z.eval())\t\t#\t15\n\nFirst,\tthis\tcode\tdefines\ta\tvery\tsimple\tgraph.\tThen\tit\tstarts\ta\tsession\tand\truns\tthe\tgraph\tto\tevaluate\ty: TensorFlow\tautomatically\tdetects\tthat\ty\tdepends\ton\tx,\twhich\tdepends\ton\tw,\tso\tit\tfirst\tevaluates\tw,\tthen\tx, then\ty,\tand\treturns\tthe\tvalue\tof\ty.\tFinally,\tthe\tcode\truns\tthe\tgraph\tto\tevaluate\tz.\tOnce\tagain,\tTensorFlow detects\tthat\tit\tmust\tfirst\tevaluate\tw\tand\tx.\tIt\tis\timportant\tto\tnote\tthat\tit\twill\tnot\treuse\tthe\tresult\tof\tthe previous\tevaluation\tof\tw\tand\tx.\tIn\tshort,\tthe\tpreceding\tcode\tevaluates\tw\tand\tx\ttwice.\n\nAll\tnode\tvalues\tare\tdropped\tbetween\tgraph\truns,\texcept\tvariable\tvalues,\twhich\tare\tmaintained\tby\tthe session\tacross\tgraph\truns\t(queues\tand\treaders\talso\tmaintain\tsome\tstate,\tas\twe\twill\tsee\tin\tChapter\t12).\tA variable\tstarts\tits\tlife\twhen\tits\tinitializer\tis\trun,\tand\tit\tends\twhen\tthe\tsession\tis\tclosed.\n\nIf\tyou\twant\tto\tevaluate\ty\tand\tz\tefficiently,\twithout\tevaluating\tw\tand\tx\ttwice\tas\tin\tthe\tprevious\tcode,\tyou must\task\tTensorFlow\tto\tevaluate\tboth\ty\tand\tz\tin\tjust\tone\tgraph\trun,\tas\tshown\tin\tthe\tfollowing\tcode:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\ty_val,\tz_val\t=\tsess.run([y,\tz]) \t\t\t\tprint(y_val)\t\t#\t10 \t\t\t\tprint(z_val)\t\t#\t15\n\nWARNING\n\nIn\tsingle-process\tTensorFlow,\tmultiple\tsessions\tdo\tnot\tshare\tany\tstate,\teven\tif\tthey\treuse\tthe\tsame\tgraph\t(each\tsession\twould have\tits\town\tcopy\tof\tevery\tvariable).\tIn\tdistributed\tTensorFlow\t(see\tChapter\t12),\tvariable\tstate\tis\tstored\ton\tthe\tservers,\tnot\tin the\tsessions,\tso\tmultiple\tsessions\tcan\tshare\tthe\tsame\tvariables.",
      "content_length": 1796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "Linear\tRegression\twith\tTensorFlow TensorFlow\toperations\t(also\tcalled\tops\tfor\tshort)\tcan\ttake\tany\tnumber\tof\tinputs\tand\tproduce\tany\tnumber of\toutputs.\tFor\texample,\tthe\taddition\tand\tmultiplication\tops\teach\ttake\ttwo\tinputs\tand\tproduce\tone\toutput. Constants\tand\tvariables\ttake\tno\tinput\t(they\tare\tcalled\tsource\tops).\tThe\tinputs\tand\toutputs\tare multidimensional\tarrays,\tcalled\ttensors\t(hence\tthe\tname\t“tensor\tflow”).\tJust\tlike\tNumPy\tarrays,\ttensors have\ta\ttype\tand\ta\tshape.\tIn\tfact,\tin\tthe\tPython\tAPI\ttensors\tare\tsimply\trepresented\tby\tNumPy\tndarrays. They\ttypically\tcontain\tfloats,\tbut\tyou\tcan\talso\tuse\tthem\tto\tcarry\tstrings\t(arbitrary\tbyte\tarrays).\n\nIn\tthe\texamples\tso\tfar,\tthe\ttensors\tjust\tcontained\ta\tsingle\tscalar\tvalue,\tbut\tyou\tcan\tof\tcourse\tperform computations\ton\tarrays\tof\tany\tshape.\tFor\texample,\tthe\tfollowing\tcode\tmanipulates\t2D\tarrays\tto\tperform Linear\tRegression\ton\tthe\tCalifornia\thousing\tdataset\t(introduced\tin\tChapter\t2).\tIt\tstarts\tby\tfetching\tthe dataset;\tthen\tit\tadds\tan\textra\tbias\tinput\tfeature\t(x0\t=\t1)\tto\tall\ttraining\tinstances\t(it\tdoes\tso\tusing\tNumPy\tso it\truns\timmediately);\tthen\tit\tcreates\ttwo\tTensorFlow\tconstant\tnodes,\tX\tand\ty,\tto\thold\tthis\tdata\tand\tthe targets,4\tand\tit\tuses\tsome\tof\tthe\tmatrix\toperations\tprovided\tby\tTensorFlow\tto\tdefine\ttheta.\tThese\tmatrix functions\t—\ttranspose(),\tmatmul(),\tand\tmatrix_inverse()\t—\tare\tself-explanatory,\tbut\tas\tusual\tthey do\tnot\tperform\tany\tcomputations\timmediately;\tinstead,\tthey\tcreate\tnodes\tin\tthe\tgraph\tthat\twill\tperform them\twhen\tthe\tgraph\tis\trun.\tYou\tmay\trecognize\tthat\tthe\tdefinition\tof\ttheta\tcorresponds\tto\tthe\tNormal Equation\t( \t=\t(XT\t·\tX)–1\t·\tXT\t·\ty;\tsee\tChapter\t4).\tFinally,\tthe\tcode\tcreates\ta\tsession\tand\tuses\tit\tto evaluate\ttheta.\n\nimport\tnumpy\tas\tnp from\tsklearn.datasets\timport\tfetch_california_housing\n\nhousing\t=\tfetch_california_housing() m,\tn\t=\thousing.data.shape housing_data_plus_bias\t=\tnp.c_[np.ones((m,\t1)),\thousing.data]\n\nX\t=\ttf.constant(housing_data_plus_bias,\tdtype=tf.float32,\tname=\"X\") y\t=\ttf.constant(housing.target.reshape(-1,\t1),\tdtype=tf.float32,\tname=\"y\") XT\t=\ttf.transpose(X) theta\t=\ttf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,\tX)),\tXT),\ty)\n\nwith\ttf.Session()\tas\tsess: \t\t\t\ttheta_value\t=\ttheta.eval()\n\nThe\tmain\tbenefit\tof\tthis\tcode\tversus\tcomputing\tthe\tNormal\tEquation\tdirectly\tusing\tNumPy\tis\tthat TensorFlow\twill\tautomatically\trun\tthis\ton\tyour\tGPU\tcard\tif\tyou\thave\tone\t(provided\tyou\tinstalled TensorFlow\twith\tGPU\tsupport,\tof\tcourse;\tsee\tChapter\t12\tfor\tmore\tdetails).",
      "content_length": 2459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "Implementing\tGradient\tDescent Let’s\ttry\tusing\tBatch\tGradient\tDescent\t(introduced\tin\tChapter\t4)\tinstead\tof\tthe\tNormal\tEquation.\tFirst\twe will\tdo\tthis\tby\tmanually\tcomputing\tthe\tgradients,\tthen\twe\twill\tuse\tTensorFlow’s\tautodiff\tfeature\tto\tlet TensorFlow\tcompute\tthe\tgradients\tautomatically,\tand\tfinally\twe\twill\tuse\ta\tcouple\tof\tTensorFlow’s\tout- of-the-box\toptimizers.\n\nWARNING\n\nWhen\tusing\tGradient\tDescent,\tremember\tthat\tit\tis\timportant\tto\tfirst\tnormalize\tthe\tinput\tfeature\tvectors,\tor\telse\ttraining\tmay\tbe much\tslower.\tYou\tcan\tdo\tthis\tusing\tTensorFlow,\tNumPy,\tScikit-Learn’s\tStandardScaler,\tor\tany\tother\tsolution\tyou\tprefer.\tThe following\tcode\tassumes\tthat\tthis\tnormalization\thas\talready\tbeen\tdone.",
      "content_length": 696,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "Manually\tComputing\tthe\tGradients The\tfollowing\tcode\tshould\tbe\tfairly\tself-explanatory,\texcept\tfor\ta\tfew\tnew\telements:\n\nThe\trandom_uniform()\tfunction\tcreates\ta\tnode\tin\tthe\tgraph\tthat\twill\tgenerate\ta\ttensor\tcontaining random\tvalues,\tgiven\tits\tshape\tand\tvalue\trange,\tmuch\tlike\tNumPy’s\trand()\tfunction.\n\nThe\tassign()\tfunction\tcreates\ta\tnode\tthat\twill\tassign\ta\tnew\tvalue\tto\ta\tvariable.\tIn\tthis\tcase,\tit implements\tthe\tBatch\tGradient\tDescent\tstep\tθ(next\tstep)\t=\tθ\t–\tη θMSE(θ).\n\nThe\tmain\tloop\texecutes\tthe\ttraining\tstep\tover\tand\tover\tagain\t(n_epochs\ttimes),\tand\tevery\t100 iterations\tit\tprints\tout\tthe\tcurrent\tMean\tSquared\tError\t(mse).\tYou\tshould\tsee\tthe\tMSE\tgo\tdown\tat every\titeration.\n\nn_epochs\t=\t1000 learning_rate\t=\t0.01\n\nX\t=\ttf.constant(scaled_housing_data_plus_bias,\tdtype=tf.float32,\tname=\"X\") y\t=\ttf.constant(housing.target.reshape(-1,\t1),\tdtype=tf.float32,\tname=\"y\") theta\t=\ttf.Variable(tf.random_uniform([n\t+\t1,\t1],\t-1.0,\t1.0),\tname=\"theta\") y_pred\t=\ttf.matmul(X,\ttheta,\tname=\"predictions\") error\t=\ty_pred\t-\ty mse\t=\ttf.reduce_mean(tf.square(error),\tname=\"mse\") gradients\t=\t2/m\t*\ttf.matmul(tf.transpose(X),\terror) training_op\t=\ttf.assign(theta,\ttheta\t-\tlearning_rate\t*\tgradients)\n\ninit\t=\ttf.global_variables_initializer()\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsess.run(init)\n\nfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tif\tepoch\t%\t100\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tprint(\"Epoch\",\tepoch,\t\"MSE\t=\",\tmse.eval()) \t\t\t\t\t\t\t\tsess.run(training_op)\n\nbest_theta\t=\ttheta.eval()",
      "content_length": 1441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Using\tautodiff The\tpreceding\tcode\tworks\tfine,\tbut\tit\trequires\tmathematically\tderiving\tthe\tgradients\tfrom\tthe\tcost function\t(MSE).\tIn\tthe\tcase\tof\tLinear\tRegression,\tit\tis\treasonably\teasy,\tbut\tif\tyou\thad\tto\tdo\tthis\twith\tdeep neural\tnetworks\tyou\twould\tget\tquite\ta\theadache:\tit\twould\tbe\ttedious\tand\terror-prone.\tYou\tcould\tuse symbolic\tdifferentiation\tto\tautomatically\tfind\tthe\tequations\tfor\tthe\tpartial\tderivatives\tfor\tyou,\tbut\tthe resulting\tcode\twould\tnot\tnecessarily\tbe\tvery\tefficient.\n\nTo\tunderstand\twhy,\tconsider\tthe\tfunction\tf(x)=\texp(exp(exp(x))).\tIf\tyou\tknow\tcalculus,\tyou\tcan\tfigure\tout its\tderivative\tf′(x)\t=\texp(x)\t×\texp(exp(x))\t×\texp(exp(exp(x))).\tIf\tyou\tcode\tf(x)\tand\tf′(x)\tseparately\tand exactly\tas\tthey\tappear,\tyour\tcode\twill\tnot\tbe\tas\tefficient\tas\tit\tcould\tbe.\tA\tmore\tefficient\tsolution\twould be\tto\twrite\ta\tfunction\tthat\tfirst\tcomputes\texp(x),\tthen\texp(exp(x)),\tthen\texp(exp(exp(x))),\tand\treturns\tall three.\tThis\tgives\tyou\tf(x)\tdirectly\t(the\tthird\tterm),\tand\tif\tyou\tneed\tthe\tderivative\tyou\tcan\tjust\tmultiply\tall three\tterms\tand\tyou\tare\tdone.\tWith\tthe\tnaïve\tapproach\tyou\twould\thave\thad\tto\tcall\tthe\texp\tfunction\tnine times\tto\tcompute\tboth\tf(x)\tand\tf′(x).\tWith\tthis\tapproach\tyou\tjust\tneed\tto\tcall\tit\tthree\ttimes.\n\nIt\tgets\tworse\twhen\tyour\tfunction\tis\tdefined\tby\tsome\tarbitrary\tcode.\tCan\tyou\tfind\tthe\tequation\t(or\tthe code)\tto\tcompute\tthe\tpartial\tderivatives\tof\tthe\tfollowing\tfunction?\tHint:\tdon’t\teven\ttry.\n\ndef\tmy_func(a,\tb): \t\t\t\tz\t=\t0 \t\t\t\tfor\ti\tin\trange(100): \t\t\t\t\t\t\t\tz\t=\ta\t*\tnp.cos(z\t+\ti)\t+\tz\t*\tnp.sin(b\t-\ti) \t\t\t\treturn\tz\n\nFortunately,\tTensorFlow’s\tautodiff\tfeature\tcomes\tto\tthe\trescue:\tit\tcan\tautomatically\tand\tefficiently compute\tthe\tgradients\tfor\tyou.\tSimply\treplace\tthe\tgradients\t=\t...\tline\tin\tthe\tGradient\tDescent\tcode\tin the\tprevious\tsection\twith\tthe\tfollowing\tline,\tand\tthe\tcode\twill\tcontinue\tto\twork\tjust\tfine:\n\ngradients\t=\ttf.gradients(mse,\t[theta])[0]\n\nThe\tgradients()\tfunction\ttakes\tan\top\t(in\tthis\tcase\tmse)\tand\ta\tlist\tof\tvariables\t(in\tthis\tcase\tjust\ttheta), and\tit\tcreates\ta\tlist\tof\tops\t(one\tper\tvariable)\tto\tcompute\tthe\tgradients\tof\tthe\top\twith\tregards\tto\teach variable.\tSo\tthe\tgradients\tnode\twill\tcompute\tthe\tgradient\tvector\tof\tthe\tMSE\twith\tregards\tto\ttheta.\n\nThere\tare\tfour\tmain\tapproaches\tto\tcomputing\tgradients\tautomatically.\tThey\tare\tsummarized\tin\tTable\t9-2. TensorFlow\tuses\treverse-mode\tautodiff,\twhich\tis\tperfect\t(efficient\tand\taccurate)\twhen\tthere\tare\tmany inputs\tand\tfew\toutputs,\tas\tis\toften\tthe\tcase\tin\tneural\tnetworks.\tIt\tcomputes\tall\tthe\tpartial\tderivatives\tof the\toutputs\twith\tregards\tto\tall\tthe\tinputs\tin\tjust\tnoutputs\t+\t1\tgraph\ttraversals.\n\nTable\t9-2.\tMain\tsolutions\tto\tcompute\tgradients\tautomatically\n\nTechnique\n\nNb\tof\tgraph\ttraversals\tto\tcompute\tall gradients\n\nAccuracy Supports\tarbitrary\n\ncode\n\nComment\n\nNumerical differentiation\n\nninputs\t+\t1\n\nLow\n\nYes\n\nTrivial\tto\timplement\n\nSymbolic\tdifferentiation N/A\n\nHigh\n\nNo\n\nBuilds\ta\tvery\tdifferent",
      "content_length": 2880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "Forward-mode\tautodiff ninputs\n\nHigh\n\nReverse-mode\tautodiff noutputs\t+\t1\n\nHigh\n\nIf\tyou\tare\tinterested\tin\thow\tthis\tmagic\tworks,\tcheck\tout\tAppendix\tD.\n\nYes\n\nYes\n\ngraph\n\nUses\tdual\tnumbers\n\nImplemented\tby TensorFlow",
      "content_length": 210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "Using\tan\tOptimizer So\tTensorFlow\tcomputes\tthe\tgradients\tfor\tyou.\tBut\tit\tgets\teven\teasier:\tit\talso\tprovides\ta\tnumber\tof optimizers\tout\tof\tthe\tbox,\tincluding\ta\tGradient\tDescent\toptimizer.\tYou\tcan\tsimply\treplace\tthe\tpreceding gradients\t=\t...\tand\ttraining_op\t=\t...\tlines\twith\tthe\tfollowing\tcode,\tand\tonce\tagain\teverything will\tjust\twork\tfine:\n\noptimizer\t=\ttf.train.GradientDescentOptimizer(learning_rate=learning_rate) training_op\t=\toptimizer.minimize(mse)\n\nIf\tyou\twant\tto\tuse\ta\tdifferent\ttype\tof\toptimizer,\tyou\tjust\tneed\tto\tchange\tone\tline.\tFor\texample,\tyou\tcan\tuse a\tmomentum\toptimizer\t(which\toften\tconverges\tmuch\tfaster\tthan\tGradient\tDescent;\tsee\tChapter\t11)\tby defining\tthe\toptimizer\tlike\tthis:\n\noptimizer\t=\ttf.train.MomentumOptimizer(learning_rate=learning_rate, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=0.9)",
      "content_length": 816,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "Feeding\tData\tto\tthe\tTraining\tAlgorithm Let’s\ttry\tto\tmodify\tthe\tprevious\tcode\tto\timplement\tMini-batch\tGradient\tDescent.\tFor\tthis,\twe\tneed\ta\tway to\treplace\tX\tand\ty\tat\tevery\titeration\twith\tthe\tnext\tmini-batch.\tThe\tsimplest\tway\tto\tdo\tthis\tis\tto\tuse placeholder\tnodes.\tThese\tnodes\tare\tspecial\tbecause\tthey\tdon’t\tactually\tperform\tany\tcomputation,\tthey just\toutput\tthe\tdata\tyou\ttell\tthem\tto\toutput\tat\truntime.\tThey\tare\ttypically\tused\tto\tpass\tthe\ttraining\tdata\tto TensorFlow\tduring\ttraining.\tIf\tyou\tdon’t\tspecify\ta\tvalue\tat\truntime\tfor\ta\tplaceholder,\tyou\tget\tan exception.\n\nTo\tcreate\ta\tplaceholder\tnode,\tyou\tmust\tcall\tthe\tplaceholder()\tfunction\tand\tspecify\tthe\toutput\ttensor’s data\ttype.\tOptionally,\tyou\tcan\talso\tspecify\tits\tshape,\tif\tyou\twant\tto\tenforce\tit.\tIf\tyou\tspecify\tNone\tfor\ta dimension,\tit\tmeans\t“any\tsize.”\tFor\texample,\tthe\tfollowing\tcode\tcreates\ta\tplaceholder\tnode\tA,\tand\talso\ta node\tB\t=\tA\t+\t5.\tWhen\twe\tevaluate\tB,\twe\tpass\ta\tfeed_dict\tto\tthe\teval()\tmethod\tthat\tspecifies\tthe value\tof\tA.\tNote\tthat\tA\tmust\thave\trank\t2\t(i.e.,\tit\tmust\tbe\ttwo-dimensional)\tand\tthere\tmust\tbe\tthree\tcolumns (or\telse\tan\texception\tis\traised),\tbut\tit\tcan\thave\tany\tnumber\tof\trows.\n\n>>>\tA\t=\ttf.placeholder(tf.float32,\tshape=(None,\t3)) >>>\tB\t=\tA\t+\t5 >>>\twith\ttf.Session()\tas\tsess: ...\t\t\t\t\tB_val_1\t=\tB.eval(feed_dict={A:\t[[1,\t2,\t3]]}) ...\t\t\t\t\tB_val_2\t=\tB.eval(feed_dict={A:\t[[4,\t5,\t6],\t[7,\t8,\t9]]}) ... >>>\tprint(B_val_1) [[\t6.\t\t7.\t\t8.]] >>>\tprint(B_val_2) [[\t\t9.\t\t10.\t\t11.] \t[\t12.\t\t13.\t\t14.]]\n\nNOTE\n\nYou\tcan\tactually\tfeed\tthe\toutput\tof\tany\toperations,\tnot\tjust\tplaceholders.\tIn\tthis\tcase\tTensorFlow\tdoes\tnot\ttry\tto\tevaluate\tthese operations;\tit\tuses\tthe\tvalues\tyou\tfeed\tit.\n\nTo\timplement\tMini-batch\tGradient\tDescent,\twe\tonly\tneed\tto\ttweak\tthe\texisting\tcode\tslightly.\tFirst\tchange the\tdefinition\tof\tX\tand\ty\tin\tthe\tconstruction\tphase\tto\tmake\tthem\tplaceholder\tnodes:\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn\t+\t1),\tname=\"X\") y\t=\ttf.placeholder(tf.float32,\tshape=(None,\t1),\tname=\"y\")\n\nThen\tdefine\tthe\tbatch\tsize\tand\tcompute\tthe\ttotal\tnumber\tof\tbatches:\n\nbatch_size\t=\t100 n_batches\t=\tint(np.ceil(m\t/\tbatch_size))\n\nFinally,\tin\tthe\texecution\tphase,\tfetch\tthe\tmini-batches\tone\tby\tone,\tthen\tprovide\tthe\tvalue\tof\tX\tand\ty\tvia the\tfeed_dict\tparameter\twhen\tevaluating\ta\tnode\tthat\tdepends\ton\teither\tof\tthem.\n\ndef\tfetch_batch(epoch,\tbatch_index,\tbatch_size):",
      "content_length": 2321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "[...]\t#\tload\tthe\tdata\tfrom\tdisk \t\t\t\treturn\tX_batch,\ty_batch\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsess.run(init)\n\nfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tfor\tbatch_index\tin\trange(n_batches): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tfetch_batch(epoch,\tbatch_index,\tbatch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch})\n\nbest_theta\t=\ttheta.eval()\n\nNOTE\n\nWe\tdon’t\tneed\tto\tpass\tthe\tvalue\tof\tX\tand\ty\twhen\tevaluating\ttheta\tsince\tit\tdoes\tnot\tdepend\ton\teither\tof\tthem.",
      "content_length": 470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "Saving\tand\tRestoring\tModels Once\tyou\thave\ttrained\tyour\tmodel,\tyou\tshould\tsave\tits\tparameters\tto\tdisk\tso\tyou\tcan\tcome\tback\tto\tit whenever\tyou\twant,\tuse\tit\tin\tanother\tprogram,\tcompare\tit\tto\tother\tmodels,\tand\tso\ton.\tMoreover,\tyou probably\twant\tto\tsave\tcheckpoints\tat\tregular\tintervals\tduring\ttraining\tso\tthat\tif\tyour\tcomputer\tcrashes during\ttraining\tyou\tcan\tcontinue\tfrom\tthe\tlast\tcheckpoint\trather\tthan\tstart\tover\tfrom\tscratch.\n\nTensorFlow\tmakes\tsaving\tand\trestoring\ta\tmodel\tvery\teasy.\tJust\tcreate\ta\tSaver\tnode\tat\tthe\tend\tof\tthe construction\tphase\t(after\tall\tvariable\tnodes\tare\tcreated);\tthen,\tin\tthe\texecution\tphase,\tjust\tcall\tits\tsave() method\twhenever\tyou\twant\tto\tsave\tthe\tmodel,\tpassing\tit\tthe\tsession\tand\tpath\tof\tthe\tcheckpoint\tfile:\n\n[...] theta\t=\ttf.Variable(tf.random_uniform([n\t+\t1,\t1],\t-1.0,\t1.0),\tname=\"theta\") [...] init\t=\ttf.global_variables_initializer() saver\t=\ttf.train.Saver()\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsess.run(init)\n\nfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tif\tepoch\t%\t100\t==\t0:\t\t#\tcheckpoint\tevery\t100\tepochs \t\t\t\t\t\t\t\t\t\t\t\tsave_path\t=\tsaver.save(sess,\t\"/tmp/my_model.ckpt\")\n\nsess.run(training_op)\n\nbest_theta\t=\ttheta.eval() \t\t\t\tsave_path\t=\tsaver.save(sess,\t\"/tmp/my_model_final.ckpt\")\n\nRestoring\ta\tmodel\tis\tjust\tas\teasy:\tyou\tcreate\ta\tSaver\tat\tthe\tend\tof\tthe\tconstruction\tphase\tjust\tlike\tbefore, but\tthen\tat\tthe\tbeginning\tof\tthe\texecution\tphase,\tinstead\tof\tinitializing\tthe\tvariables\tusing\tthe\tinit\tnode, you\tcall\tthe\trestore()\tmethod\tof\tthe\tSaver\tobject:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsaver.restore(sess,\t\"/tmp/my_model_final.ckpt\") \t\t\t\t[...]\n\nBy\tdefault\ta\tSaver\tsaves\tand\trestores\tall\tvariables\tunder\ttheir\town\tname,\tbut\tif\tyou\tneed\tmore\tcontrol, you\tcan\tspecify\twhich\tvariables\tto\tsave\tor\trestore,\tand\twhat\tnames\tto\tuse.\tFor\texample,\tthe\tfollowing Saver\twill\tsave\tor\trestore\tonly\tthe\ttheta\tvariable\tunder\tthe\tname\tweights:\n\nsaver\t=\ttf.train.Saver({\"weights\":\ttheta})\n\nBy\tdefault,\tthe\tsave()\tmethod\talso\tsaves\tthe\tstructure\tof\tthe\tgraph\tin\ta\tsecond\tfile\twith\tthe\tsame\tname plus\ta\t.meta\textension.\tYou\tcan\tload\tthis\tgraph\tstructure\tusing\ttf.train.import_meta_graph().\tThis adds\tthe\tgraph\tto\tthe\tdefault\tgraph,\tand\treturns\ta\tSaver\tinstance\tthat\tyou\tcan\tthen\tuse\tto\trestore\tthe graph’s\tstate\t(i.e.,\tthe\tvariable\tvalues):\n\nsaver\t=\ttf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsaver.restore(sess,\t\"/tmp/my_model_final.ckpt\") \t\t\t\t[...]",
      "content_length": 2398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "This\tallows\tyou\tto\tfully\trestore\ta\tsaved\tmodel,\tincluding\tboth\tthe\tgraph\tstructure\tand\tthe\tvariable\tvalues, without\thaving\tto\tsearch\tfor\tthe\tcode\tthat\tbuilt\tit.",
      "content_length": 160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "Visualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard So\tnow\twe\thave\ta\tcomputation\tgraph\tthat\ttrains\ta\tLinear\tRegression\tmodel\tusing\tMini-batch\tGradient Descent,\tand\twe\tare\tsaving\tcheckpoints\tat\tregular\tintervals.\tSounds\tsophisticated,\tdoesn’t\tit?\tHowever, we\tare\tstill\trelying\ton\tthe\tprint()\tfunction\tto\tvisualize\tprogress\tduring\ttraining.\tThere\tis\ta\tbetter\tway: enter\tTensorBoard.\tIf\tyou\tfeed\tit\tsome\ttraining\tstats,\tit\twill\tdisplay\tnice\tinteractive\tvisualizations\tof these\tstats\tin\tyour\tweb\tbrowser\t(e.g.,\tlearning\tcurves).\tYou\tcan\talso\tprovide\tit\tthe\tgraph’s\tdefinition\tand it\twill\tgive\tyou\ta\tgreat\tinterface\tto\tbrowse\tthrough\tit.\tThis\tis\tvery\tuseful\tto\tidentify\terrors\tin\tthe\tgraph,\tto find\tbottlenecks,\tand\tso\ton.\n\nThe\tfirst\tstep\tis\tto\ttweak\tyour\tprogram\ta\tbit\tso\tit\twrites\tthe\tgraph\tdefinition\tand\tsome\ttraining\tstats\t—\tfor example,\tthe\ttraining\terror\t(MSE)\t—\tto\ta\tlog\tdirectory\tthat\tTensorBoard\twill\tread\tfrom.\tYou\tneed\tto\tuse a\tdifferent\tlog\tdirectory\tevery\ttime\tyou\trun\tyour\tprogram,\tor\telse\tTensorBoard\twill\tmerge\tstats\tfrom different\truns,\twhich\twill\tmess\tup\tthe\tvisualizations.\tThe\tsimplest\tsolution\tfor\tthis\tis\tto\tinclude\ta timestamp\tin\tthe\tlog\tdirectory\tname.\tAdd\tthe\tfollowing\tcode\tat\tthe\tbeginning\tof\tthe\tprogram:\n\nfrom\tdatetime\timport\tdatetime\n\nnow\t=\tdatetime.utcnow().strftime(\"%Y%m%d%H%M%S\") root_logdir\t=\t\"tf_logs\" logdir\t=\t\"{}/run-{}/\".format(root_logdir,\tnow)\n\nNext,\tadd\tthe\tfollowing\tcode\tat\tthe\tvery\tend\tof\tthe\tconstruction\tphase:\n\nmse_summary\t=\ttf.summary.scalar('MSE',\tmse) file_writer\t=\ttf.summary.FileWriter(logdir,\ttf.get_default_graph())\n\nThe\tfirst\tline\tcreates\ta\tnode\tin\tthe\tgraph\tthat\twill\tevaluate\tthe\tMSE\tvalue\tand\twrite\tit\tto\ta\tTensorBoard- compatible\tbinary\tlog\tstring\tcalled\ta\tsummary.\tThe\tsecond\tline\tcreates\ta\tFileWriter\tthat\tyou\twill\tuse to\twrite\tsummaries\tto\tlogfiles\tin\tthe\tlog\tdirectory.\tThe\tfirst\tparameter\tindicates\tthe\tpath\tof\tthe\tlog directory\t(in\tthis\tcase\tsomething\tlike\ttf_logs/run-20160906091959/,\trelative\tto\tthe\tcurrent\tdirectory). The\tsecond\t(optional)\tparameter\tis\tthe\tgraph\tyou\twant\tto\tvisualize.\tUpon\tcreation,\tthe\tFileWriter creates\tthe\tlog\tdirectory\tif\tit\tdoes\tnot\talready\texist\t(and\tits\tparent\tdirectories\tif\tneeded),\tand\twrites\tthe graph\tdefinition\tin\ta\tbinary\tlogfile\tcalled\tan\tevents\tfile.\n\nNext\tyou\tneed\tto\tupdate\tthe\texecution\tphase\tto\tevaluate\tthe\tmse_summary\tnode\tregularly\tduring\ttraining (e.g.,\tevery\t10\tmini-batches).\tThis\twill\toutput\ta\tsummary\tthat\tyou\tcan\tthen\twrite\tto\tthe\tevents\tfile\tusing the\tfile_writer.\tHere\tis\tthe\tupdated\tcode:\n\n[...] \t\t\t\tfor\tbatch_index\tin\trange(n_batches): \t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tfetch_batch(epoch,\tbatch_index,\tbatch_size) \t\t\t\t\t\t\t\tif\tbatch_index\t%\t10\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tsummary_str\t=\tmse_summary.eval(feed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\t\t\t\t\tstep\t=\tepoch\t*\tn_batches\t+\tbatch_index \t\t\t\t\t\t\t\t\t\t\t\tfile_writer.add_summary(summary_str,\tstep) \t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t[...]",
      "content_length": 2927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "WARNING\n\nAvoid\tlogging\ttraining\tstats\tat\tevery\tsingle\ttraining\tstep,\tas\tthis\twould\tsignificantly\tslow\tdown\ttraining.\n\nFinally,\tyou\twant\tto\tclose\tthe\tFileWriter\tat\tthe\tend\tof\tthe\tprogram:\n\nfile_writer.close()\n\nNow\trun\tthis\tprogram:\tit\twill\tcreate\tthe\tlog\tdirectory\tand\twrite\tan\tevents\tfile\tin\tthis\tdirectory,\tcontaining both\tthe\tgraph\tdefinition\tand\tthe\tMSE\tvalues.\tOpen\tup\ta\tshell\tand\tgo\tto\tyour\tworking\tdirectory,\tthen\ttype ls\t-l\ttf_logs/run*\tto\tlist\tthe\tcontents\tof\tthe\tlog\tdirectory:\n\n$\tcd\t$ML_PATH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tYour\tML\tworking\tdirectory\t(e.g.,\t$HOME/ml) $\tls\t-l\ttf_logs/run* total\t40 -rw-r--r--\t1\tageron\tstaff\t18620\tSep\t6\t11:10\tevents.out.tfevents.1472553182.mymac\n\nIf\tyou\trun\tthe\tprogram\ta\tsecond\ttime,\tyou\tshould\tsee\ta\tsecond\tdirectory\tin\tthe\ttf_logs/\tdirectory:\n\n$\tls\t-l\ttf_logs/ total\t0 drwxr-xr-x\t\t3\tageron\t\tstaff\t\t102\tSep\t\t6\t10:07\trun-20160906091959 drwxr-xr-x\t\t3\tageron\t\tstaff\t\t102\tSep\t\t6\t10:22\trun-20160906092202\n\nGreat!\tNow\tit’s\ttime\tto\tfire\tup\tthe\tTensorBoard\tserver.\tYou\tneed\tto\tactivate\tyour\tvirtualenv\tenvironment if\tyou\tcreated\tone,\tthen\tstart\tthe\tserver\tby\trunning\tthe\ttensorboard\tcommand,\tpointing\tit\tto\tthe\troot\tlog directory.\tThis\tstarts\tthe\tTensorBoard\tweb\tserver,\tlistening\ton\tport\t6006\t(which\tis\t“goog”\twritten\tupside down):\n\n$\tsource\tenv/bin/activate $\ttensorboard\t--logdir\ttf_logs/ Starting\tTensorBoard\t\ton\tport\t6006 (You\tcan\tnavigate\tto\thttp://0.0.0.0:6006)\n\nNext\topen\ta\tbrowser\tand\tgo\tto\thttp://0.0.0.0:6006/\t(or\thttp://localhost:6006/).\tWelcome\tto TensorBoard!\tIn\tthe\tEvents\ttab\tyou\tshould\tsee\tMSE\ton\tthe\tright.\tIf\tyou\tclick\ton\tit,\tyou\twill\tsee\ta\tplot\tof the\tMSE\tduring\ttraining,\tfor\tboth\truns\t(Figure\t9-3).\tYou\tcan\tcheck\tor\tuncheck\tthe\truns\tyou\twant\tto\tsee, zoom\tin\tor\tout,\thover\tover\tthe\tcurve\tto\tget\tdetails,\tand\tso\ton.",
      "content_length": 1755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "Figure\t9-3.\tVisualizing\ttraining\tstats\tusing\tTensorBoard\n\nNow\tclick\ton\tthe\tGraphs\ttab.\tYou\tshould\tsee\tthe\tgraph\tshown\tin\tFigure\t9-4.\n\nTo\treduce\tclutter,\tthe\tnodes\tthat\thave\tmany\tedges\t(i.e.,\tconnections\tto\tother\tnodes)\tare\tseparated\tout\tto\tan auxiliary\tarea\ton\tthe\tright\t(you\tcan\tmove\ta\tnode\tback\tand\tforth\tbetween\tthe\tmain\tgraph\tand\tthe\tauxiliary area\tby\tright-clicking\ton\tit).\tSome\tparts\tof\tthe\tgraph\tare\talso\tcollapsed\tby\tdefault.\tFor\texample,\ttry hovering\tover\tthe\tgradients\tnode,\tthen\tclick\ton\tthe\t subgraph,\ttry\texpanding\tthe\tmse_grad\tsubgraph.\n\nicon\tto\texpand\tthis\tsubgraph.\tNext,\tin\tthis\n\nFigure\t9-4.\tVisualizing\tthe\tgraph\tusing\tTensorBoard\n\nTIP\n\nIf\tyou\twant\tto\ttake\ta\tpeek\tat\tthe\tgraph\tdirectly\twithin\tJupyter,\tyou\tcan\tuse\tthe\tshow_graph()\tfunction\tavailable\tin\tthe\tnotebook for\tthis\tchapter.\tIt\twas\toriginally\twritten\tby\tA.\tMordvintsev\tin\this\tgreat\tdeepdream\ttutorial\tnotebook.\tAnother\toption\tis\tto\tinstall E.\tJang’s\tTensorFlow\tdebugger\ttool\twhich\tincludes\ta\tJupyter\textension\tfor\tgraph\tvisualization\t(and\tmore).",
      "content_length": 1022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "Name\tScopes When\tdealing\twith\tmore\tcomplex\tmodels\tsuch\tas\tneural\tnetworks,\tthe\tgraph\tcan\teasily\tbecome\tcluttered with\tthousands\tof\tnodes.\tTo\tavoid\tthis,\tyou\tcan\tcreate\tname\tscopes\tto\tgroup\trelated\tnodes.\tFor\texample, let’s\tmodify\tthe\tprevious\tcode\tto\tdefine\tthe\terror\tand\tmse\tops\twithin\ta\tname\tscope\tcalled\t\"loss\":\n\nwith\ttf.name_scope(\"loss\")\tas\tscope: \t\t\t\terror\t=\ty_pred\t-\ty \t\t\t\tmse\t=\ttf.reduce_mean(tf.square(error),\tname=\"mse\")\n\nThe\tname\tof\teach\top\tdefined\twithin\tthe\tscope\tis\tnow\tprefixed\twith\t\"loss/\":\n\n>>>\tprint(error.op.name) loss/sub >>>\tprint(mse.op.name) loss/mse\n\nIn\tTensorBoard,\tthe\tmse\tand\terror\tnodes\tnow\tappear\tinside\tthe\tloss\tnamespace,\twhich\tappears collapsed\tby\tdefault\t(Figure\t9-5).\n\nFigure\t9-5.\tA\tcollapsed\tnamescope\tin\tTensorBoard",
      "content_length": 751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "Modularity Suppose\tyou\twant\tto\tcreate\ta\tgraph\tthat\tadds\tthe\toutput\tof\ttwo\trectified\tlinear\tunits\t(ReLU).\tA\tReLU computes\ta\tlinear\tfunction\tof\tthe\tinputs,\tand\toutputs\tthe\tresult\tif\tit\tis\tpositive,\tand\t0\totherwise,\tas\tshown in\tEquation\t9-1.\n\nEquation\t9-1.\tRectified\tlinear\tunit\n\nThe\tfollowing\tcode\tdoes\tthe\tjob,\tbut\tit’s\tquite\trepetitive:\n\nn_features\t=\t3 X\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features),\tname=\"X\")\n\nw1\t=\ttf.Variable(tf.random_normal((n_features,\t1)),\tname=\"weights1\") w2\t=\ttf.Variable(tf.random_normal((n_features,\t1)),\tname=\"weights2\") b1\t=\ttf.Variable(0.0,\tname=\"bias1\") b2\t=\ttf.Variable(0.0,\tname=\"bias2\")\n\nz1\t=\ttf.add(tf.matmul(X,\tw1),\tb1,\tname=\"z1\") z2\t=\ttf.add(tf.matmul(X,\tw2),\tb2,\tname=\"z2\")\n\nrelu1\t=\ttf.maximum(z1,\t0.,\tname=\"relu1\") relu2\t=\ttf.maximum(z1,\t0.,\tname=\"relu2\")\n\noutput\t=\ttf.add(relu1,\trelu2,\tname=\"output\")\n\nSuch\trepetitive\tcode\tis\thard\tto\tmaintain\tand\terror-prone\t(in\tfact,\tthis\tcode\tcontains\ta\tcut-and-paste\terror; did\tyou\tspot\tit?).\tIt\twould\tbecome\teven\tworse\tif\tyou\twanted\tto\tadd\ta\tfew\tmore\tReLUs.\tFortunately, TensorFlow\tlets\tyou\tstay\tDRY\t(Don’t\tRepeat\tYourself):\tsimply\tcreate\ta\tfunction\tto\tbuild\ta\tReLU.\tThe following\tcode\tcreates\tfive\tReLUs\tand\toutputs\ttheir\tsum\t(note\tthat\tadd_n()\tcreates\tan\toperation\tthat\twill compute\tthe\tsum\tof\ta\tlist\tof\ttensors):\n\ndef\trelu(X): \t\t\t\tw_shape\t=\t(int(X.get_shape()[1]),\t1) \t\t\t\tw\t=\ttf.Variable(tf.random_normal(w_shape),\tname=\"weights\") \t\t\t\tb\t=\ttf.Variable(0.0,\tname=\"bias\") \t\t\t\tz\t=\ttf.add(tf.matmul(X,\tw),\tb,\tname=\"z\") \t\t\t\treturn\ttf.maximum(z,\t0.,\tname=\"relu\")\n\nn_features\t=\t3 X\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features),\tname=\"X\") relus\t=\t[relu(X)\tfor\ti\tin\trange(5)] output\t=\ttf.add_n(relus,\tname=\"output\")\n\nNote\tthat\twhen\tyou\tcreate\ta\tnode,\tTensorFlow\tchecks\twhether\tits\tname\talready\texists,\tand\tif\tit\tdoes\tit appends\tan\tunderscore\tfollowed\tby\tan\tindex\tto\tmake\tthe\tname\tunique.\tSo\tthe\tfirst\tReLU\tcontains\tnodes named\t\"weights\",\t\"bias\",\t\"z\",\tand\t\"relu\"\t(plus\tmany\tmore\tnodes\twith\ttheir\tdefault\tname,\tsuch\tas \"MatMul\");\tthe\tsecond\tReLU\tcontains\tnodes\tnamed\t\"weights_1\",\t\"bias_1\",\tand\tso\ton;\tthe\tthird\tReLU contains\tnodes\tnamed\t\"weights_2\",\t\"bias_2\",\tand\tso\ton.\tTensorBoard\tidentifies\tsuch\tseries\tand collapses\tthem\ttogether\tto\treduce\tclutter\t(as\tyou\tcan\tsee\tin\tFigure\t9-6).",
      "content_length": 2271,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "Figure\t9-6.\tCollapsed\tnode\tseries\n\nUsing\tname\tscopes,\tyou\tcan\tmake\tthe\tgraph\tmuch\tclearer.\tSimply\tmove\tall\tthe\tcontent\tof\tthe\trelu() function\tinside\ta\tname\tscope.\tFigure\t9-7\tshows\tthe\tresulting\tgraph.\tNotice\tthat\tTensorFlow\talso\tgives\tthe name\tscopes\tunique\tnames\tby\tappending\t_1,\t_2,\tand\tso\ton.\n\ndef\trelu(X): \t\t\t\twith\ttf.name_scope(\"relu\"): \t\t\t\t\t\t\t\t[...]",
      "content_length": 355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "Figure\t9-7.\tA\tclearer\tgraph\tusing\tname-scoped\tunits",
      "content_length": 51,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "Sharing\tVariables If\tyou\twant\tto\tshare\ta\tvariable\tbetween\tvarious\tcomponents\tof\tyour\tgraph,\tone\tsimple\toption\tis\tto\tcreate it\tfirst,\tthen\tpass\tit\tas\ta\tparameter\tto\tthe\tfunctions\tthat\tneed\tit.\tFor\texample,\tsuppose\tyou\twant\tto\tcontrol the\tReLU\tthreshold\t(currently\thardcoded\tto\t0)\tusing\ta\tshared\tthreshold\tvariable\tfor\tall\tReLUs.\tYou could\tjust\tcreate\tthat\tvariable\tfirst,\tand\tthen\tpass\tit\tto\tthe\trelu()\tfunction:\n\ndef\trelu(X,\tthreshold): \t\t\t\twith\ttf.name_scope(\"relu\"): \t\t\t\t\t\t\t\t[...] \t\t\t\t\t\t\t\treturn\ttf.maximum(z,\tthreshold,\tname=\"max\")\n\nthreshold\t=\ttf.Variable(0.0,\tname=\"threshold\") X\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features),\tname=\"X\") relus\t=\t[relu(X,\tthreshold)\tfor\ti\tin\trange(5)] output\t=\ttf.add_n(relus,\tname=\"output\")\n\nThis\tworks\tfine:\tnow\tyou\tcan\tcontrol\tthe\tthreshold\tfor\tall\tReLUs\tusing\tthe\tthreshold\tvariable. However,\tif\tthere\tare\tmany\tshared\tparameters\tsuch\tas\tthis\tone,\tit\twill\tbe\tpainful\tto\thave\tto\tpass\tthem around\tas\tparameters\tall\tthe\ttime.\tMany\tpeople\tcreate\ta\tPython\tdictionary\tcontaining\tall\tthe\tvariables\tin their\tmodel,\tand\tpass\tit\taround\tto\tevery\tfunction.\tOthers\tcreate\ta\tclass\tfor\teach\tmodule\t(e.g.,\ta\tReLU\tclass using\tclass\tvariables\tto\thandle\tthe\tshared\tparameter).\tYet\tanother\toption\tis\tto\tset\tthe\tshared\tvariable\tas an\tattribute\tof\tthe\trelu()\tfunction\tupon\tthe\tfirst\tcall,\tlike\tso:\n\ndef\trelu(X): \t\t\t\twith\ttf.name_scope(\"relu\"): \t\t\t\t\t\t\t\tif\tnot\thasattr(relu,\t\"threshold\"): \t\t\t\t\t\t\t\t\t\t\t\trelu.threshold\t=\ttf.Variable(0.0,\tname=\"threshold\") \t\t\t\t\t\t\t\t[...] \t\t\t\t\t\t\t\treturn\ttf.maximum(z,\trelu.threshold,\tname=\"max\")\n\nTensorFlow\toffers\tanother\toption,\twhich\tmay\tlead\tto\tslightly\tcleaner\tand\tmore\tmodular\tcode\tthan\tthe previous\tsolutions.5\tThis\tsolution\tis\ta\tbit\ttricky\tto\tunderstand\tat\tfirst,\tbut\tsince\tit\tis\tused\ta\tlot\tin TensorFlow\tit\tis\tworth\tgoing\tinto\ta\tbit\tof\tdetail.\tThe\tidea\tis\tto\tuse\tthe\tget_variable()\tfunction\tto create\tthe\tshared\tvariable\tif\tit\tdoes\tnot\texist\tyet,\tor\treuse\tit\tif\tit\talready\texists.\tThe\tdesired\tbehavior (creating\tor\treusing)\tis\tcontrolled\tby\tan\tattribute\tof\tthe\tcurrent\tvariable_scope().\tFor\texample,\tthe following\tcode\twill\tcreate\ta\tvariable\tnamed\t\"relu/threshold\"\t(as\ta\tscalar,\tsince\tshape=(),\tand\tusing 0.0\tas\tthe\tinitial\tvalue):\n\nwith\ttf.variable_scope(\"relu\"): \t\t\t\tthreshold\t=\ttf.get_variable(\"threshold\",\tshape=(), \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tinitializer=tf.constant_initializer(0.0))\n\nNote\tthat\tif\tthe\tvariable\thas\talready\tbeen\tcreated\tby\tan\tearlier\tcall\tto\tget_variable(),\tthis\tcode\twill raise\tan\texception.\tThis\tbehavior\tprevents\treusing\tvariables\tby\tmistake.\tIf\tyou\twant\tto\treuse\ta\tvariable, you\tneed\tto\texplicitly\tsay\tso\tby\tsetting\tthe\tvariable\tscope’s\treuse\tattribute\tto\tTrue\t(in\twhich\tcase\tyou don’t\thave\tto\tspecify\tthe\tshape\tor\tthe\tinitializer):\n\nwith\ttf.variable_scope(\"relu\",\treuse=True):",
      "content_length": 2768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "threshold\t=\ttf.get_variable(\"threshold\")\n\nThis\tcode\twill\tfetch\tthe\texisting\t\"relu/threshold\"\tvariable,\tor\traise\tan\texception\tif\tit\tdoes\tnot\texist\tor if\tit\twas\tnot\tcreated\tusing\tget_variable().\tAlternatively,\tyou\tcan\tset\tthe\treuse\tattribute\tto\tTrue\tinside the\tblock\tby\tcalling\tthe\tscope’s\treuse_variables()\tmethod:\n\nwith\ttf.variable_scope(\"relu\")\tas\tscope: \t\t\t\tscope.reuse_variables() \t\t\t\tthreshold\t=\ttf.get_variable(\"threshold\")\n\nWARNING\n\nOnce\treuse\tis\tset\tto\tTrue,\tit\tcannot\tbe\tset\tback\tto\tFalse\twithin\tthe\tblock.\tMoreover,\tif\tyou\tdefine\tother\tvariable\tscopes\tinside this\tone,\tthey\twill\tautomatically\tinherit\treuse=True.\tLastly,\tonly\tvariables\tcreated\tby\tget_variable()\tcan\tbe\treused\tthis\tway.\n\nNow\tyou\thave\tall\tthe\tpieces\tyou\tneed\tto\tmake\tthe\trelu()\tfunction\taccess\tthe\tthreshold\tvariable without\thaving\tto\tpass\tit\tas\ta\tparameter:\n\ndef\trelu(X): \t\t\t\twith\ttf.variable_scope(\"relu\",\treuse=True): \t\t\t\t\t\t\t\tthreshold\t=\ttf.get_variable(\"threshold\")\t\t#\treuse\texisting\tvariable \t\t\t\t\t\t\t\t[...] \t\t\t\t\t\t\t\treturn\ttf.maximum(z,\tthreshold,\tname=\"max\")\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features),\tname=\"X\") with\ttf.variable_scope(\"relu\"):\t\t#\tcreate\tthe\tvariable \t\t\t\tthreshold\t=\ttf.get_variable(\"threshold\",\tshape=(), \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tinitializer=tf.constant_initializer(0.0)) relus\t=\t[relu(X)\tfor\trelu_index\tin\trange(5)] output\t=\ttf.add_n(relus,\tname=\"output\")\n\nThis\tcode\tfirst\tdefines\tthe\trelu()\tfunction,\tthen\tcreates\tthe\trelu/threshold\tvariable\t(as\ta\tscalar\tthat will\tlater\tbe\tinitialized\tto\t0.0)\tand\tbuilds\tfive\tReLUs\tby\tcalling\tthe\trelu()\tfunction.\tThe\trelu() function\treuses\tthe\trelu/threshold\tvariable,\tand\tcreates\tthe\tother\tReLU\tnodes.\n\nNOTE\n\nVariables\tcreated\tusing\tget_variable()\tare\talways\tnamed\tusing\tthe\tname\tof\ttheir\tvariable_scope\tas\ta\tprefix\t(e.g., \"relu/threshold\"),\tbut\tfor\tall\tother\tnodes\t(including\tvariables\tcreated\twith\ttf.Variable())\tthe\tvariable\tscope\tacts\tlike\ta\tnew name\tscope.\tIn\tparticular,\tif\ta\tname\tscope\twith\tan\tidentical\tname\twas\talready\tcreated,\tthen\ta\tsuffix\tis\tadded\tto\tmake\tthe\tname unique.\tFor\texample,\tall\tnodes\tcreated\tin\tthe\tpreceding\tcode\t(except\tthe\tthreshold\tvariable)\thave\ta\tname\tprefixed\twith \"relu_1/\"\tto\t\"relu_5/\",\tas\tshown\tin\tFigure\t9-8.",
      "content_length": 2198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "Figure\t9-8.\tFive\tReLUs\tsharing\tthe\tthreshold\tvariable\n\nIt\tis\tsomewhat\tunfortunate\tthat\tthe\tthreshold\tvariable\tmust\tbe\tdefined\toutside\tthe\trelu()\tfunction, where\tall\tthe\trest\tof\tthe\tReLU\tcode\tresides.\tTo\tfix\tthis,\tthe\tfollowing\tcode\tcreates\tthe\tthreshold variable\twithin\tthe\trelu()\tfunction\tupon\tthe\tfirst\tcall,\tthen\treuses\tit\tin\tsubsequent\tcalls.\tNow\tthe\trelu() function\tdoes\tnot\thave\tto\tworry\tabout\tname\tscopes\tor\tvariable\tsharing:\tit\tjust\tcalls\tget_variable(), which\twill\tcreate\tor\treuse\tthe\tthreshold\tvariable\t(it\tdoes\tnot\tneed\tto\tknow\twhich\tis\tthe\tcase).\tThe\trest of\tthe\tcode\tcalls\trelu()\tfive\ttimes,\tmaking\tsure\tto\tset\treuse=False\ton\tthe\tfirst\tcall,\tand\treuse=True for\tthe\tother\tcalls.\n\ndef\trelu(X): \t\t\t\tthreshold\t=\ttf.get_variable(\"threshold\",\tshape=(), \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tinitializer=tf.constant_initializer(0.0)) \t\t\t\t[...] \t\t\t\treturn\ttf.maximum(z,\tthreshold,\tname=\"max\")\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features),\tname=\"X\") relus\t=\t[] for\trelu_index\tin\trange(5): \t\t\t\twith\ttf.variable_scope(\"relu\",\treuse=(relu_index\t>=\t1))\tas\tscope: \t\t\t\t\t\t\t\trelus.append(relu(X)) output\t=\ttf.add_n(relus,\tname=\"output\")\n\nThe\tresulting\tgraph\tis\tslightly\tdifferent\tthan\tbefore,\tsince\tthe\tshared\tvariable\tlives\twithin\tthe\tfirst\tReLU (see\tFigure\t9-9).\n\nFigure\t9-9.\tFive\tReLUs\tsharing\tthe\tthreshold\tvariable\n\nThis\tconcludes\tthis\tintroduction\tto\tTensorFlow.\tWe\twill\tdiscuss\tmore\tadvanced\ttopics\tas\twe\tgo\tthrough the\tfollowing\tchapters,\tin\tparticular\tmany\toperations\trelated\tto\tdeep\tneural\tnetworks,\tconvolutional neural\tnetworks,\tand\trecurrent\tneural\tnetworks\tas\twell\tas\thow\tto\tscale\tup\twith\tTensorFlow\tusing multithreading,\tqueues,\tmultiple\tGPUs,\tand\tmultiple\tservers.",
      "content_length": 1682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "Exercises\n\n1.\t What\tare\tthe\tmain\tbenefits\tof\tcreating\ta\tcomputation\tgraph\trather\tthan\tdirectly\texecuting\tthe computations?\tWhat\tare\tthe\tmain\tdrawbacks?\n\n2.\t Is\tthe\tstatement\ta_val\t=\ta.eval(session=sess)\tequivalent\tto\ta_val\t=\tsess.run(a)?\n\n3.\t Is\tthe\tstatement\ta_val,\tb_val\t=\ta.eval(session=sess),\tb.eval(session=sess)\tequivalent\tto a_val,\tb_val\t=\tsess.run([a,\tb])?\n\n4.\t Can\tyou\trun\ttwo\tgraphs\tin\tthe\tsame\tsession?\n\n5.\t If\tyou\tcreate\ta\tgraph\tg\tcontaining\ta\tvariable\tw,\tthen\tstart\ttwo\tthreads\tand\topen\ta\tsession\tin\teach thread,\tboth\tusing\tthe\tsame\tgraph\tg,\twill\teach\tsession\thave\tits\town\tcopy\tof\tthe\tvariable\tw\tor\twill\tit be\tshared?\n\n6.\t When\tis\ta\tvariable\tinitialized?\tWhen\tis\tit\tdestroyed?\n\n7.\t What\tis\tthe\tdifference\tbetween\ta\tplaceholder\tand\ta\tvariable?\n\n8.\t What\thappens\twhen\tyou\trun\tthe\tgraph\tto\tevaluate\tan\toperation\tthat\tdepends\ton\ta\tplaceholder\tbut\tyou don’t\tfeed\tits\tvalue?\tWhat\thappens\tif\tthe\toperation\tdoes\tnot\tdepend\ton\tthe\tplaceholder?\n\n9.\t When\tyou\trun\ta\tgraph,\tcan\tyou\tfeed\tthe\toutput\tvalue\tof\tany\toperation,\tor\tjust\tthe\tvalue\tof placeholders?\n\n10.\t How\tcan\tyou\tset\ta\tvariable\tto\tany\tvalue\tyou\twant\t(during\tthe\texecution\tphase)?\n\n11.\t How\tmany\ttimes\tdoes\treverse-mode\tautodiff\tneed\tto\ttraverse\tthe\tgraph\tin\torder\tto\tcompute\tthe gradients\tof\tthe\tcost\tfunction\twith\tregards\tto\t10\tvariables?\tWhat\tabout\tforward-mode\tautodiff?\tAnd symbolic\tdifferentiation?\n\n12.\t Implement\tLogistic\tRegression\twith\tMini-batch\tGradient\tDescent\tusing\tTensorFlow.\tTrain\tit\tand evaluate\tit\ton\tthe\tmoons\tdataset\t(introduced\tin\tChapter\t5).\tTry\tadding\tall\tthe\tbells\tand\twhistles: Define\tthe\tgraph\twithin\ta\tlogistic_regression()\tfunction\tthat\tcan\tbe\treused\teasily.\n\nSave\tcheckpoints\tusing\ta\tSaver\tat\tregular\tintervals\tduring\ttraining,\tand\tsave\tthe\tfinal\tmodel\tat the\tend\tof\ttraining.\n\nRestore\tthe\tlast\tcheckpoint\tupon\tstartup\tif\ttraining\twas\tinterrupted.\n\nDefine\tthe\tgraph\tusing\tnice\tscopes\tso\tthe\tgraph\tlooks\tgood\tin\tTensorBoard.\n\nAdd\tsummaries\tto\tvisualize\tthe\tlearning\tcurves\tin\tTensorBoard.\n\nTry\ttweaking\tsome\thyperparameters\tsuch\tas\tthe\tlearning\trate\tor\tthe\tmini-batch\tsize\tand\tlook\tat the\tshape\tof\tthe\tlearning\tcurve.",
      "content_length": 2108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "Solutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nTensorFlow\tis\tnot\tlimited\tto\tneural\tnetworks\tor\teven\tMachine\tLearning;\tyou\tcould\trun\tquantum\tphysics\tsimulations\tif\tyou\twanted.\n\n2\n\nNot\tto\tbe\tconfused\twith\tthe\tTFLearn\tlibrary,\twhich\tis\tan\tindependent\tproject.\n\n3\n\nIn\tdistributed\tTensorFlow,\tvariable\tvalues\tare\tstored\ton\tthe\tservers\tinstead\tof\tthe\tsession,\tas\twe\twill\tsee\tin\tChapter\t12.\n\n4\n\nNote\tthat\thousing.target\tis\ta\t1D\tarray,\tbut\twe\tneed\tto\treshape\tit\tto\ta\tcolumn\tvector\tto\tcompute\ttheta.\tRecall\tthat\tNumPy’s\treshape() function\taccepts\t–1\t(meaning\t“unspecified”)\tfor\tone\tof\tthe\tdimensions:\tthat\tdimension\twill\tbe\tcomputed\tbased\ton\tthe\tarray’s\tlength\tand the\tremaining\tdimensions.\n\n5\n\nCreating\ta\tReLU\tclass\tis\targuably\tthe\tcleanest\toption,\tbut\tit\tis\trather\theavyweight.",
      "content_length": 785,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "Chapter\t10.\tIntroduction\tto\tArtificial\tNeural Networks\n\nBirds\tinspired\tus\tto\tfly,\tburdock\tplants\tinspired\tvelcro,\tand\tnature\thas\tinspired\tmany\tother\tinventions.\tIt seems\tonly\tlogical,\tthen,\tto\tlook\tat\tthe\tbrain’s\tarchitecture\tfor\tinspiration\ton\thow\tto\tbuild\tan\tintelligent machine.\tThis\tis\tthe\tkey\tidea\tthat\tinspired\tartificial\tneural\tnetworks\t(ANNs).\tHowever,\talthough\tplanes were\tinspired\tby\tbirds,\tthey\tdon’t\thave\tto\tflap\ttheir\twings.\tSimilarly,\tANNs\thave\tgradually\tbecome\tquite different\tfrom\ttheir\tbiological\tcousins.\tSome\tresearchers\teven\targue\tthat\twe\tshould\tdrop\tthe\tbiological analogy\taltogether\t(e.g.,\tby\tsaying\t“units”\trather\tthan\t“neurons”),\tlest\twe\trestrict\tour\tcreativity\tto biologically\tplausible\tsystems.1\n\nANNs\tare\tat\tthe\tvery\tcore\tof\tDeep\tLearning.\tThey\tare\tversatile,\tpowerful,\tand\tscalable,\tmaking\tthem ideal\tto\ttackle\tlarge\tand\thighly\tcomplex\tMachine\tLearning\ttasks,\tsuch\tas\tclassifying\tbillions\tof\timages (e.g.,\tGoogle\tImages),\tpowering\tspeech\trecognition\tservices\t(e.g.,\tApple’s\tSiri),\trecommending\tthe\tbest videos\tto\twatch\tto\thundreds\tof\tmillions\tof\tusers\tevery\tday\t(e.g.,\tYouTube),\tor\tlearning\tto\tbeat\tthe\tworld champion\tat\tthe\tgame\tof\tGo\tby\texamining\tmillions\tof\tpast\tgames\tand\tthen\tplaying\tagainst\titself (DeepMind’s\tAlphaGo).\n\nIn\tthis\tchapter,\twe\twill\tintroduce\tartificial\tneural\tnetworks,\tstarting\twith\ta\tquick\ttour\tof\tthe\tvery\tfirst ANN\tarchitectures.\tThen\twe\twill\tpresent\tMulti-Layer\tPerceptrons\t(MLPs)\tand\timplement\tone\tusing TensorFlow\tto\ttackle\tthe\tMNIST\tdigit\tclassification\tproblem\t(introduced\tin\tChapter\t3).",
      "content_length": 1544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "From\tBiological\tto\tArtificial\tNeurons Surprisingly,\tANNs\thave\tbeen\taround\tfor\tquite\ta\twhile:\tthey\twere\tfirst\tintroduced\tback\tin\t1943\tby\tthe neurophysiologist\tWarren\tMcCulloch\tand\tthe\tmathematician\tWalter\tPitts.\tIn\ttheir\tlandmark\tpaper,2\t“A Logical\tCalculus\tof\tIdeas\tImmanent\tin\tNervous\tActivity,”\tMcCulloch\tand\tPitts\tpresented\ta\tsimplified computational\tmodel\tof\thow\tbiological\tneurons\tmight\twork\ttogether\tin\tanimal\tbrains\tto\tperform\tcomplex computations\tusing\tpropositional\tlogic.\tThis\twas\tthe\tfirst\tartificial\tneural\tnetwork\tarchitecture.\tSince then\tmany\tother\tarchitectures\thave\tbeen\tinvented,\tas\twe\twill\tsee.\n\nThe\tearly\tsuccesses\tof\tANNs\tuntil\tthe\t1960s\tled\tto\tthe\twidespread\tbelief\tthat\twe\twould\tsoon\tbe conversing\twith\ttruly\tintelligent\tmachines.\tWhen\tit\tbecame\tclear\tthat\tthis\tpromise\twould\tgo\tunfulfilled (at\tleast\tfor\tquite\ta\twhile),\tfunding\tflew\telsewhere\tand\tANNs\tentered\ta\tlong\tdark\tera.\tIn\tthe\tearly\t1980s there\twas\ta\trevival\tof\tinterest\tin\tANNs\tas\tnew\tnetwork\tarchitectures\twere\tinvented\tand\tbetter\ttraining techniques\twere\tdeveloped.\tBut\tby\tthe\t1990s,\tpowerful\talternative\tMachine\tLearning\ttechniques\tsuch\tas Support\tVector\tMachines\t(see\tChapter\t5)\twere\tfavored\tby\tmost\tresearchers,\tas\tthey\tseemed\tto\toffer better\tresults\tand\tstronger\ttheoretical\tfoundations.\tFinally,\twe\tare\tnow\twitnessing\tyet\tanother\twave\tof interest\tin\tANNs.\tWill\tthis\twave\tdie\tout\tlike\tthe\tprevious\tones\tdid?\tThere\tare\ta\tfew\tgood\treasons\tto believe\tthat\tthis\tone\tis\tdifferent\tand\twill\thave\ta\tmuch\tmore\tprofound\timpact\ton\tour\tlives:\n\nThere\tis\tnow\ta\thuge\tquantity\tof\tdata\tavailable\tto\ttrain\tneural\tnetworks,\tand\tANNs\tfrequently outperform\tother\tML\ttechniques\ton\tvery\tlarge\tand\tcomplex\tproblems.\n\nThe\ttremendous\tincrease\tin\tcomputing\tpower\tsince\tthe\t1990s\tnow\tmakes\tit\tpossible\tto\ttrain\tlarge neural\tnetworks\tin\ta\treasonable\tamount\tof\ttime.\tThis\tis\tin\tpart\tdue\tto\tMoore’s\tLaw,\tbut\talso\tthanks to\tthe\tgaming\tindustry,\twhich\thas\tproduced\tpowerful\tGPU\tcards\tby\tthe\tmillions.\n\nThe\ttraining\talgorithms\thave\tbeen\timproved.\tTo\tbe\tfair\tthey\tare\tonly\tslightly\tdifferent\tfrom\tthe\tones used\tin\tthe\t1990s,\tbut\tthese\trelatively\tsmall\ttweaks\thave\ta\thuge\tpositive\timpact.\n\nSome\ttheoretical\tlimitations\tof\tANNs\thave\tturned\tout\tto\tbe\tbenign\tin\tpractice.\tFor\texample,\tmany people\tthought\tthat\tANN\ttraining\talgorithms\twere\tdoomed\tbecause\tthey\twere\tlikely\tto\tget\tstuck\tin local\toptima,\tbut\tit\tturns\tout\tthat\tthis\tis\trather\trare\tin\tpractice\t(or\twhen\tit\tis\tthe\tcase,\tthey\tare\tusually fairly\tclose\tto\tthe\tglobal\toptimum).\n\nANNs\tseem\tto\thave\tentered\ta\tvirtuous\tcircle\tof\tfunding\tand\tprogress.\tAmazing\tproducts\tbased\ton ANNs\tregularly\tmake\tthe\theadline\tnews,\twhich\tpulls\tmore\tand\tmore\tattention\tand\tfunding\ttoward them,\tresulting\tin\tmore\tand\tmore\tprogress,\tand\teven\tmore\tamazing\tproducts.",
      "content_length": 2746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "Biological\tNeurons Before\twe\tdiscuss\tartificial\tneurons,\tlet’s\ttake\ta\tquick\tlook\tat\ta\tbiological\tneuron\t(represented\tin Figure\t10-1).\tIt\tis\tan\tunusual-looking\tcell\tmostly\tfound\tin\tanimal\tcerebral\tcortexes\t(e.g.,\tyour\tbrain), composed\tof\ta\tcell\tbody\tcontaining\tthe\tnucleus\tand\tmost\tof\tthe\tcell’s\tcomplex\tcomponents,\tand\tmany branching\textensions\tcalled\tdendrites,\tplus\tone\tvery\tlong\textension\tcalled\tthe\taxon.\tThe\taxon’s\tlength may\tbe\tjust\ta\tfew\ttimes\tlonger\tthan\tthe\tcell\tbody,\tor\tup\tto\ttens\tof\tthousands\tof\ttimes\tlonger.\tNear\tits extremity\tthe\taxon\tsplits\toff\tinto\tmany\tbranches\tcalled\ttelodendria,\tand\tat\tthe\ttip\tof\tthese\tbranches\tare minuscule\tstructures\tcalled\tsynaptic\tterminals\t(or\tsimply\tsynapses),\twhich\tare\tconnected\tto\tthe dendrites\t(or\tdirectly\tto\tthe\tcell\tbody)\tof\tother\tneurons.\tBiological\tneurons\treceive\tshort\telectrical impulses\tcalled\tsignals\tfrom\tother\tneurons\tvia\tthese\tsynapses.\tWhen\ta\tneuron\treceives\ta\tsufficient number\tof\tsignals\tfrom\tother\tneurons\twithin\ta\tfew\tmilliseconds,\tit\tfires\tits\town\tsignals.\n\nFigure\t10-1.\tBiological\tneuron3\n\nThus,\tindividual\tbiological\tneurons\tseem\tto\tbehave\tin\ta\trather\tsimple\tway,\tbut\tthey\tare\torganized\tin\ta vast\tnetwork\tof\tbillions\tof\tneurons,\teach\tneuron\ttypically\tconnected\tto\tthousands\tof\tother\tneurons.\tHighly complex\tcomputations\tcan\tbe\tperformed\tby\ta\tvast\tnetwork\tof\tfairly\tsimple\tneurons,\tmuch\tlike\ta\tcomplex anthill\tcan\temerge\tfrom\tthe\tcombined\tefforts\tof\tsimple\tants.\tThe\tarchitecture\tof\tbiological\tneural networks\t(BNN)4\tis\tstill\tthe\tsubject\tof\tactive\tresearch,\tbut\tsome\tparts\tof\tthe\tbrain\thave\tbeen\tmapped, and\tit\tseems\tthat\tneurons\tare\toften\torganized\tin\tconsecutive\tlayers,\tas\tshown\tin\tFigure\t10-2.",
      "content_length": 1666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "Figure\t10-2.\tMultiple\tlayers\tin\ta\tbiological\tneural\tnetwork\t(human\tcortex)5",
      "content_length": 75,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "Logical\tComputations\twith\tNeurons Warren\tMcCulloch\tand\tWalter\tPitts\tproposed\ta\tvery\tsimple\tmodel\tof\tthe\tbiological\tneuron,\twhich\tlater became\tknown\tas\tan\tartificial\tneuron:\tit\thas\tone\tor\tmore\tbinary\t(on/off)\tinputs\tand\tone\tbinary\toutput. The\tartificial\tneuron\tsimply\tactivates\tits\toutput\twhen\tmore\tthan\ta\tcertain\tnumber\tof\tits\tinputs\tare\tactive. McCulloch\tand\tPitts\tshowed\tthat\teven\twith\tsuch\ta\tsimplified\tmodel\tit\tis\tpossible\tto\tbuild\ta\tnetwork\tof artificial\tneurons\tthat\tcomputes\tany\tlogical\tproposition\tyou\twant.\tFor\texample,\tlet’s\tbuild\ta\tfew\tANNs that\tperform\tvarious\tlogical\tcomputations\t(see\tFigure\t10-3),\tassuming\tthat\ta\tneuron\tis\tactivated\twhen\tat least\ttwo\tof\tits\tinputs\tare\tactive.\n\nFigure\t10-3.\tANNs\tperforming\tsimple\tlogical\tcomputations\n\nThe\tfirst\tnetwork\ton\tthe\tleft\tis\tsimply\tthe\tidentity\tfunction:\tif\tneuron\tA\tis\tactivated,\tthen\tneuron\tC gets\tactivated\tas\twell\t(since\tit\treceives\ttwo\tinput\tsignals\tfrom\tneuron\tA),\tbut\tif\tneuron\tA\tis\toff,\tthen neuron\tC\tis\toff\tas\twell.\n\nThe\tsecond\tnetwork\tperforms\ta\tlogical\tAND:\tneuron\tC\tis\tactivated\tonly\twhen\tboth\tneurons\tA\tand\tB are\tactivated\t(a\tsingle\tinput\tsignal\tis\tnot\tenough\tto\tactivate\tneuron\tC).\n\nThe\tthird\tnetwork\tperforms\ta\tlogical\tOR:\tneuron\tC\tgets\tactivated\tif\teither\tneuron\tA\tor\tneuron\tB\tis activated\t(or\tboth).\n\nFinally,\tif\twe\tsuppose\tthat\tan\tinput\tconnection\tcan\tinhibit\tthe\tneuron’s\tactivity\t(which\tis\tthe\tcase with\tbiological\tneurons),\tthen\tthe\tfourth\tnetwork\tcomputes\ta\tslightly\tmore\tcomplex\tlogical proposition:\tneuron\tC\tis\tactivated\tonly\tif\tneuron\tA\tis\tactive\tand\tif\tneuron\tB\tis\toff.\tIf\tneuron\tA\tis active\tall\tthe\ttime,\tthen\tyou\tget\ta\tlogical\tNOT:\tneuron\tC\tis\tactive\twhen\tneuron\tB\tis\toff,\tand\tvice versa.\n\nYou\tcan\teasily\timagine\thow\tthese\tnetworks\tcan\tbe\tcombined\tto\tcompute\tcomplex\tlogical\texpressions (see\tthe\texercises\tat\tthe\tend\tof\tthe\tchapter).",
      "content_length": 1821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "The\tPerceptron The\tPerceptron\tis\tone\tof\tthe\tsimplest\tANN\tarchitectures,\tinvented\tin\t1957\tby\tFrank\tRosenblatt.\tIt\tis based\ton\ta\tslightly\tdifferent\tartificial\tneuron\t(see\tFigure\t10-4)\tcalled\ta\tlinear\tthreshold\tunit\t(LTU):\tthe inputs\tand\toutput\tare\tnow\tnumbers\t(instead\tof\tbinary\ton/off\tvalues)\tand\teach\tinput\tconnection\tis associated\twith\ta\tweight.\tThe\tLTU\tcomputes\ta\tweighted\tsum\tof\tits\tinputs\t(z\t=\tw1\tx1\t+\tw2\tx2\t+\t\t+\t wn\txn =\twT\t·\tx),\tthen\tapplies\ta\tstep\tfunction\tto\tthat\tsum\tand\toutputs\tthe\tresult:\thw(x)\t=\tstep\t(z)\t=\tstep\t(wT\t·\tx).\n\nFigure\t10-4.\tLinear\tthreshold\tunit\n\nThe\tmost\tcommon\tstep\tfunction\tused\tin\tPerceptrons\tis\tthe\tHeaviside\tstep\tfunction\t(see\tEquation\t10-1). Sometimes\tthe\tsign\tfunction\tis\tused\tinstead.\n\nEquation\t10-1.\tCommon\tstep\tfunctions\tused\tin\tPerceptrons\n\nA\tsingle\tLTU\tcan\tbe\tused\tfor\tsimple\tlinear\tbinary\tclassification.\tIt\tcomputes\ta\tlinear\tcombination\tof\tthe inputs\tand\tif\tthe\tresult\texceeds\ta\tthreshold,\tit\toutputs\tthe\tpositive\tclass\tor\telse\toutputs\tthe\tnegative\tclass (just\tlike\ta\tLogistic\tRegression\tclassifier\tor\ta\tlinear\tSVM).\tFor\texample,\tyou\tcould\tuse\ta\tsingle\tLTU\tto classify\tiris\tflowers\tbased\ton\tthe\tpetal\tlength\tand\twidth\t(also\tadding\tan\textra\tbias\tfeature\tx0\t=\t1,\tjust\tlike we\tdid\tin\tprevious\tchapters).\tTraining\tan\tLTU\tmeans\tfinding\tthe\tright\tvalues\tfor\tw0,\tw1,\tand\tw2\t(the training\talgorithm\tis\tdiscussed\tshortly). A\tPerceptron\tis\tsimply\tcomposed\tof\ta\tsingle\tlayer\tof\tLTUs,6\twith\teach\tneuron\tconnected\tto\tall\tthe\tinputs. These\tconnections\tare\toften\trepresented\tusing\tspecial\tpassthrough\tneurons\tcalled\tinput\tneurons:\tthey\tjust output\twhatever\tinput\tthey\tare\tfed.\tMoreover,\tan\textra\tbias\tfeature\tis\tgenerally\tadded\t(x0\t=\t1).\tThis\tbias feature\tis\ttypically\trepresented\tusing\ta\tspecial\ttype\tof\tneuron\tcalled\ta\tbias\tneuron,\twhich\tjust\toutputs\t1 all\tthe\ttime.",
      "content_length": 1792,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "A\tPerceptron\twith\ttwo\tinputs\tand\tthree\toutputs\tis\trepresented\tin\tFigure\t10-5.\tThis\tPerceptron\tcan classify\tinstances\tsimultaneously\tinto\tthree\tdifferent\tbinary\tclasses,\twhich\tmakes\tit\ta\tmultioutput classifier.\n\nFigure\t10-5.\tPerceptron\tdiagram\n\nSo\thow\tis\ta\tPerceptron\ttrained?\tThe\tPerceptron\ttraining\talgorithm\tproposed\tby\tFrank\tRosenblatt\twas largely\tinspired\tby\tHebb’s\trule.\tIn\this\tbook\tThe\tOrganization\tof\tBehavior,\tpublished\tin\t1949,\tDonald Hebb\tsuggested\tthat\twhen\ta\tbiological\tneuron\toften\ttriggers\tanother\tneuron,\tthe\tconnection\tbetween\tthese two\tneurons\tgrows\tstronger.\tThis\tidea\twas\tlater\tsummarized\tby\tSiegrid\tLöwel\tin\tthis\tcatchy\tphrase: “Cells\tthat\tfire\ttogether,\twire\ttogether.”\tThis\trule\tlater\tbecame\tknown\tas\tHebb’s\trule\t(or\tHebbian learning);\tthat\tis,\tthe\tconnection\tweight\tbetween\ttwo\tneurons\tis\tincreased\twhenever\tthey\thave\tthe\tsame output.\tPerceptrons\tare\ttrained\tusing\ta\tvariant\tof\tthis\trule\tthat\ttakes\tinto\taccount\tthe\terror\tmade\tby\tthe network;\tit\tdoes\tnot\treinforce\tconnections\tthat\tlead\tto\tthe\twrong\toutput.\tMore\tspecifically,\tthe\tPerceptron is\tfed\tone\ttraining\tinstance\tat\ta\ttime,\tand\tfor\teach\tinstance\tit\tmakes\tits\tpredictions.\tFor\tevery\toutput neuron\tthat\tproduced\ta\twrong\tprediction,\tit\treinforces\tthe\tconnection\tweights\tfrom\tthe\tinputs\tthat\twould have\tcontributed\tto\tthe\tcorrect\tprediction.\tThe\trule\tis\tshown\tin\tEquation\t10-2.\n\nEquation\t10-2.\tPerceptron\tlearning\trule\t(weight\tupdate)\n\nwi,\tj\tis\tthe\tconnection\tweight\tbetween\tthe\tith\tinput\tneuron\tand\tthe\tjth\toutput\tneuron.\n\nxi\tis\tthe\tith\tinput\tvalue\tof\tthe\tcurrent\ttraining\tinstance.\n\nj\tis\tthe\toutput\tof\tthe\tjth\toutput\tneuron\tfor\tthe\tcurrent\ttraining\tinstance.\n\nyj\tis\tthe\ttarget\toutput\tof\tthe\tjth\toutput\tneuron\tfor\tthe\tcurrent\ttraining\tinstance.",
      "content_length": 1722,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "η\tis\tthe\tlearning\trate.\n\nThe\tdecision\tboundary\tof\teach\toutput\tneuron\tis\tlinear,\tso\tPerceptrons\tare\tincapable\tof\tlearning\tcomplex patterns\t(just\tlike\tLogistic\tRegression\tclassifiers).\tHowever,\tif\tthe\ttraining\tinstances\tare\tlinearly separable,\tRosenblatt\tdemonstrated\tthat\tthis\talgorithm\twould\tconverge\tto\ta\tsolution.7\tThis\tis\tcalled\tthe Perceptron\tconvergence\ttheorem.\n\nScikit-Learn\tprovides\ta\tPerceptron\tclass\tthat\timplements\ta\tsingle\tLTU\tnetwork.\tIt\tcan\tbe\tused\tpretty much\tas\tyou\twould\texpect\t—\tfor\texample,\ton\tthe\tiris\tdataset\t(introduced\tin\tChapter\t4):\n\nimport\tnumpy\tas\tnp from\tsklearn.datasets\timport\tload_iris from\tsklearn.linear_model\timport\tPerceptron\n\niris\t=\tload_iris() X\t=\tiris.data[:,\t(2,\t3)]\t\t#\tpetal\tlength,\tpetal\twidth y\t=\t(iris.target\t==\t0).astype(np.int)\t\t#\tIris\tSetosa?\n\nper_clf\t=\tPerceptron(random_state=42) per_clf.fit(X,\ty)\n\ny_pred\t=\tper_clf.predict([[2,\t0.5]])\n\nYou\tmay\thave\trecognized\tthat\tthe\tPerceptron\tlearning\talgorithm\tstrongly\tresembles\tStochastic\tGradient Descent.\tIn\tfact,\tScikit-Learn’s\tPerceptron\tclass\tis\tequivalent\tto\tusing\tan\tSGDClassifier\twith\tthe following\thyperparameters:\tloss=\"perceptron\",\tlearning_rate=\"constant\",\teta0=1\t(the\tlearning rate),\tand\tpenalty=None\t(no\tregularization).\n\nNote\tthat\tcontrary\tto\tLogistic\tRegression\tclassifiers,\tPerceptrons\tdo\tnot\toutput\ta\tclass\tprobability;\trather, they\tjust\tmake\tpredictions\tbased\ton\ta\thard\tthreshold.\tThis\tis\tone\tof\tthe\tgood\treasons\tto\tprefer\tLogistic Regression\tover\tPerceptrons.\n\nIn\ttheir\t1969\tmonograph\ttitled\tPerceptrons,\tMarvin\tMinsky\tand\tSeymour\tPapert\thighlighted\ta\tnumber\tof serious\tweaknesses\tof\tPerceptrons,\tin\tparticular\tthe\tfact\tthat\tthey\tare\tincapable\tof\tsolving\tsome\ttrivial problems\t(e.g.,\tthe\tExclusive\tOR\t(XOR)\tclassification\tproblem;\tsee\tthe\tleft\tside\tof\tFigure\t10-6).\tOf course\tthis\tis\ttrue\tof\tany\tother\tlinear\tclassification\tmodel\tas\twell\t(such\tas\tLogistic\tRegression classifiers),\tbut\tresearchers\thad\texpected\tmuch\tmore\tfrom\tPerceptrons,\tand\ttheir\tdisappointment\twas great:\tas\ta\tresult,\tmany\tresearchers\tdropped\tconnectionism\taltogether\t(i.e.,\tthe\tstudy\tof\tneural\tnetworks) in\tfavor\tof\thigher-level\tproblems\tsuch\tas\tlogic,\tproblem\tsolving,\tand\tsearch.\n\nHowever,\tit\tturns\tout\tthat\tsome\tof\tthe\tlimitations\tof\tPerceptrons\tcan\tbe\teliminated\tby\tstacking\tmultiple Perceptrons.\tThe\tresulting\tANN\tis\tcalled\ta\tMulti-Layer\tPerceptron\t(MLP).\tIn\tparticular,\tan\tMLP\tcan solve\tthe\tXOR\tproblem,\tas\tyou\tcan\tverify\tby\tcomputing\tthe\toutput\tof\tthe\tMLP\trepresented\ton\tthe\tright\tof Figure\t10-6,\tfor\teach\tcombination\tof\tinputs:\twith\tinputs\t(0,\t0)\tor\t(1,\t1)\tthe\tnetwork\toutputs\t0,\tand\twith inputs\t(0,\t1)\tor\t(1,\t0)\tit\toutputs\t1.",
      "content_length": 2612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "Figure\t10-6.\tXOR\tclassification\tproblem\tand\tan\tMLP\tthat\tsolves\tit",
      "content_length": 65,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "Multi-Layer\tPerceptron\tand\tBackpropagation An\tMLP\tis\tcomposed\tof\tone\t(passthrough)\tinput\tlayer,\tone\tor\tmore\tlayers\tof\tLTUs,\tcalled\thidden\tlayers, and\tone\tfinal\tlayer\tof\tLTUs\tcalled\tthe\toutput\tlayer\t(see\tFigure\t10-7).\tEvery\tlayer\texcept\tthe\toutput\tlayer includes\ta\tbias\tneuron\tand\tis\tfully\tconnected\tto\tthe\tnext\tlayer.\tWhen\tan\tANN\thas\ttwo\tor\tmore\thidden layers,\tit\tis\tcalled\ta\tdeep\tneural\tnetwork\t(DNN).\n\nFigure\t10-7.\tMulti-Layer\tPerceptron\n\nFor\tmany\tyears\tresearchers\tstruggled\tto\tfind\ta\tway\tto\ttrain\tMLPs,\twithout\tsuccess.\tBut\tin\t1986,\tD.\tE. Rumelhart\tet\tal.\tpublished\ta\tgroundbreaking\tarticle8\tintroducing\tthe\tbackpropagation\ttraining\talgorithm.9 Today\twe\twould\tdescribe\tit\tas\tGradient\tDescent\tusing\treverse-mode\tautodiff\t(Gradient\tDescent\twas introduced\tin\tChapter\t4,\tand\tautodiff\twas\tdiscussed\tin\tChapter\t9).\n\nFor\teach\ttraining\tinstance,\tthe\talgorithm\tfeeds\tit\tto\tthe\tnetwork\tand\tcomputes\tthe\toutput\tof\tevery\tneuron in\teach\tconsecutive\tlayer\t(this\tis\tthe\tforward\tpass,\tjust\tlike\twhen\tmaking\tpredictions).\tThen\tit\tmeasures the\tnetwork’s\toutput\terror\t(i.e.,\tthe\tdifference\tbetween\tthe\tdesired\toutput\tand\tthe\tactual\toutput\tof\tthe network),\tand\tit\tcomputes\thow\tmuch\teach\tneuron\tin\tthe\tlast\thidden\tlayer\tcontributed\tto\teach\toutput neuron’s\terror.\tIt\tthen\tproceeds\tto\tmeasure\thow\tmuch\tof\tthese\terror\tcontributions\tcame\tfrom\teach\tneuron in\tthe\tprevious\thidden\tlayer\t—\tand\tso\ton\tuntil\tthe\talgorithm\treaches\tthe\tinput\tlayer.\tThis\treverse\tpass efficiently\tmeasures\tthe\terror\tgradient\tacross\tall\tthe\tconnection\tweights\tin\tthe\tnetwork\tby\tpropagating\tthe error\tgradient\tbackward\tin\tthe\tnetwork\t(hence\tthe\tname\tof\tthe\talgorithm).\tIf\tyou\tcheck\tout\tthe\treverse- mode\tautodiff\talgorithm\tin\tAppendix\tD,\tyou\twill\tfind\tthat\tthe\tforward\tand\treverse\tpasses\tof backpropagation\tsimply\tperform\treverse-mode\tautodiff.\tThe\tlast\tstep\tof\tthe\tbackpropagation\talgorithm\tis a\tGradient\tDescent\tstep\ton\tall\tthe\tconnection\tweights\tin\tthe\tnetwork,\tusing\tthe\terror\tgradients\tmeasured earlier.",
      "content_length": 1959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "Let’s\tmake\tthis\teven\tshorter:\tfor\teach\ttraining\tinstance\tthe\tbackpropagation\talgorithm\tfirst\tmakes\ta prediction\t(forward\tpass),\tmeasures\tthe\terror,\tthen\tgoes\tthrough\teach\tlayer\tin\treverse\tto\tmeasure\tthe error\tcontribution\tfrom\teach\tconnection\t(reverse\tpass),\tand\tfinally\tslightly\ttweaks\tthe\tconnection\tweights to\treduce\tthe\terror\t(Gradient\tDescent\tstep).\n\nIn\torder\tfor\tthis\talgorithm\tto\twork\tproperly,\tthe\tauthors\tmade\ta\tkey\tchange\tto\tthe\tMLP’s\tarchitecture:\tthey replaced\tthe\tstep\tfunction\twith\tthe\tlogistic\tfunction,\tσ(z)\t=\t1\t/\t(1\t+\texp(–z)).\tThis\twas\tessential\tbecause the\tstep\tfunction\tcontains\tonly\tflat\tsegments,\tso\tthere\tis\tno\tgradient\tto\twork\twith\t(Gradient\tDescent cannot\tmove\ton\ta\tflat\tsurface),\twhile\tthe\tlogistic\tfunction\thas\ta\twell-defined\tnonzero\tderivative everywhere,\tallowing\tGradient\tDescent\tto\tmake\tsome\tprogress\tat\tevery\tstep.\tThe\tbackpropagation algorithm\tmay\tbe\tused\twith\tother\tactivation\tfunctions,\tinstead\tof\tthe\tlogistic\tfunction.\tTwo\tother\tpopular activation\tfunctions\tare:\n\nThe\thyperbolic\ttangent\tfunction\ttanh\t(z)\t=\t2σ(2z)\t–\t1\n\nJust\tlike\tthe\tlogistic\tfunction\tit\tis\tS-shaped,\tcontinuous,\tand\tdifferentiable,\tbut\tits\toutput\tvalue ranges\tfrom\t–1\tto\t1\t(instead\tof\t0\tto\t1\tin\tthe\tcase\tof\tthe\tlogistic\tfunction),\twhich\ttends\tto\tmake\teach layer’s\toutput\tmore\tor\tless\tnormalized\t(i.e.,\tcentered\taround\t0)\tat\tthe\tbeginning\tof\ttraining.\tThis often\thelps\tspeed\tup\tconvergence.\n\nThe\tReLU\tfunction\t(introduced\tin\tChapter\t9)\n\nReLU\t(z)\t=\tmax\t(0,\tz).\tIt\tis\tcontinuous\tbut\tunfortunately\tnot\tdifferentiable\tat\tz\t=\t0\t(the\tslope\tchanges abruptly,\twhich\tcan\tmake\tGradient\tDescent\tbounce\taround).\tHowever,\tin\tpractice\tit\tworks\tvery well\tand\thas\tthe\tadvantage\tof\tbeing\tfast\tto\tcompute.\tMost\timportantly,\tthe\tfact\tthat\tit\tdoes\tnot\thave\ta maximum\toutput\tvalue\talso\thelps\treduce\tsome\tissues\tduring\tGradient\tDescent\t(we\twill\tcome\tback to\tthis\tin\tChapter\t11).\n\nThese\tpopular\tactivation\tfunctions\tand\ttheir\tderivatives\tare\trepresented\tin\tFigure\t10-8.\n\nFigure\t10-8.\tActivation\tfunctions\tand\ttheir\tderivatives\n\nAn\tMLP\tis\toften\tused\tfor\tclassification,\twith\teach\toutput\tcorresponding\tto\ta\tdifferent\tbinary\tclass\t(e.g., spam/ham,\turgent/not-urgent,\tand\tso\ton).\tWhen\tthe\tclasses\tare\texclusive\t(e.g.,\tclasses\t0\tthrough\t9\tfor digit\timage\tclassification),\tthe\toutput\tlayer\tis\ttypically\tmodified\tby\treplacing\tthe\tindividual\tactivation functions\tby\ta\tshared\tsoftmax\tfunction\t(see\tFigure\t10-9).\tThe\tsoftmax\tfunction\twas\tintroduced\tin Chapter\t3.\tThe\toutput\tof\teach\tneuron\tcorresponds\tto\tthe\testimated\tprobability\tof\tthe\tcorresponding\tclass. Note\tthat\tthe\tsignal\tflows\tonly\tin\tone\tdirection\t(from\tthe\tinputs\tto\tthe\toutputs),\tso\tthis\tarchitecture\tis\tan example\tof\ta\tfeedforward\tneural\tnetwork\t(FNN).",
      "content_length": 2684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "Figure\t10-9.\tA\tmodern\tMLP\t(including\tReLU\tand\tsoftmax)\tfor\tclassification\n\nNOTE\n\nBiological\tneurons\tseem\tto\timplement\ta\troughly\tsigmoid\t(S-shaped)\tactivation\tfunction,\tso\tresearchers\tstuck\tto\tsigmoid\tfunctions for\ta\tvery\tlong\ttime.\tBut\tit\tturns\tout\tthat\tthe\tReLU\tactivation\tfunction\tgenerally\tworks\tbetter\tin\tANNs.\tThis\tis\tone\tof\tthe\tcases where\tthe\tbiological\tanalogy\twas\tmisleading.",
      "content_length": 384,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "Training\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI The\tsimplest\tway\tto\ttrain\tan\tMLP\twith\tTensorFlow\tis\tto\tuse\tthe\thigh-level\tAPI\tTF.Learn,\twhich\toffers\ta Scikit-Learn–compatible\tAPI.\tThe\tDNNClassifier\tclass\tmakes\tit\tfairly\teasy\tto\ttrain\ta\tdeep\tneural network\twith\tany\tnumber\tof\thidden\tlayers,\tand\ta\tsoftmax\toutput\tlayer\tto\toutput\testimated\tclass probabilities.\tFor\texample,\tthe\tfollowing\tcode\ttrains\ta\tDNN\tfor\tclassification\twith\ttwo\thidden\tlayers (one\twith\t300\tneurons,\tand\tthe\tother\twith\t100\tneurons)\tand\ta\tsoftmax\toutput\tlayer\twith\t10\tneurons:\n\nimport\ttensorflow\tas\ttf\n\nfeature_cols\t=\ttf.contrib.learn.infer_real_valued_columns_from_input(X_train) dnn_clf\t=\ttf.contrib.learn.DNNClassifier(hidden_units=[300,100],\tn_classes=10, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfeature_columns=feature_cols) dnn_clf\t=\ttf.contrib.learn.SKCompat(dnn_clf)\t\t#\tif\tTensorFlow\t>=\t1.1 dnn_clf.fit(X_train,\ty_train,\tbatch_size=50,\tsteps=40000)\n\nThe\tcode\tfirst\tcreates\ta\tset\tof\treal\tvalued\tcolumns\tfrom\tthe\ttraining\tset\t(other\ttypes\tof\tcolumns,\tsuch\tas categorical\tcolumns,\tare\tavailable).\tThen\twe\tcreate\tthe\tDNNClassifier,\tand\twe\twrap\tit\tin\ta\tScikit- Learn\tcompatibility\thelper.\tFinally,\twe\trun\t40,000\ttraining\titerations\tusing\tbatches\tof\t50\tinstances.\n\nIf\tyou\trun\tthis\tcode\ton\tthe\tMNIST\tdataset\t(after\tscaling\tit,\te.g.,\tby\tusing\tScikit-Learn’s StandardScaler),\tyou\twill\tactually\tget\ta\tmodel\tthat\tachieves\taround\t98.2%\taccuracy\ton\tthe\ttest\tset! That’s\tbetter\tthan\tthe\tbest\tmodel\twe\ttrained\tin\tChapter\t3:\n\n>>>\tfrom\tsklearn.metrics\timport\taccuracy_score >>>\ty_pred\t=\tdnn_clf.predict(X_test) >>>\taccuracy_score(y_test,\ty_pred['classes']) 0.98250000000000004\n\nWARNING\n\nThe\ttensorflow.contrib\tpackage\tcontains\tmany\tuseful\tfunctions,\tbut\tit\tis\ta\tplace\tfor\texperimental\tcode\tthat\thas\tnot\tyet graduated\tto\tbe\tpart\tof\tthe\tcore\tTensorFlow\tAPI.\tSo\tthe\tDNNClassifier\tclass\t(and\tany\tother\tcontrib\tcode)\tmay\tchange without\tnotice\tin\tthe\tfuture.\n\nUnder\tthe\thood,\tthe\tDNNClassifier\tclass\tcreates\tall\tthe\tneuron\tlayers,\tbased\ton\tthe\tReLU\tactivation function\t(we\tcan\tchange\tthis\tby\tsetting\tthe\tactivation_fn\thyperparameter).\tThe\toutput\tlayer\trelies\ton the\tsoftmax\tfunction,\tand\tthe\tcost\tfunction\tis\tcross\tentropy\t(introduced\tin\tChapter\t4).",
      "content_length": 2203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "Training\ta\tDNN\tUsing\tPlain\tTensorFlow If\tyou\twant\tmore\tcontrol\tover\tthe\tarchitecture\tof\tthe\tnetwork,\tyou\tmay\tprefer\tto\tuse\tTensorFlow’s\tlower- level\tPython\tAPI\t(introduced\tin\tChapter\t9).\tIn\tthis\tsection\twe\twill\tbuild\tthe\tsame\tmodel\tas\tbefore\tusing this\tAPI,\tand\twe\twill\timplement\tMini-batch\tGradient\tDescent\tto\ttrain\tit\ton\tthe\tMNIST\tdataset.\tThe\tfirst step\tis\tthe\tconstruction\tphase,\tbuilding\tthe\tTensorFlow\tgraph.\tThe\tsecond\tstep\tis\tthe\texecution\tphase, where\tyou\tactually\trun\tthe\tgraph\tto\ttrain\tthe\tmodel.",
      "content_length": 507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "Construction\tPhase Let’s\tstart.\tFirst\twe\tneed\tto\timport\tthe\ttensorflow\tlibrary.\tThen\twe\tmust\tspecify\tthe\tnumber\tof\tinputs and\toutputs,\tand\tset\tthe\tnumber\tof\thidden\tneurons\tin\teach\tlayer:\n\nimport\ttensorflow\tas\ttf\n\nn_inputs\t=\t28*28\t\t#\tMNIST n_hidden1\t=\t300 n_hidden2\t=\t100 n_outputs\t=\t10\n\nNext,\tjust\tlike\tyou\tdid\tin\tChapter\t9,\tyou\tcan\tuse\tplaceholder\tnodes\tto\trepresent\tthe\ttraining\tdata\tand targets.\tThe\tshape\tof\tX\tis\tonly\tpartially\tdefined.\tWe\tknow\tthat\tit\twill\tbe\ta\t2D\ttensor\t(i.e.,\ta\tmatrix),\twith instances\talong\tthe\tfirst\tdimension\tand\tfeatures\talong\tthe\tsecond\tdimension,\tand\twe\tknow\tthat\tthe\tnumber of\tfeatures\tis\tgoing\tto\tbe\t28\tx\t28\t(one\tfeature\tper\tpixel),\tbut\twe\tdon’t\tknow\tyet\thow\tmany\tinstances\teach training\tbatch\twill\tcontain.\tSo\tthe\tshape\tof\tX\tis\t(None,\tn_inputs).\tSimilarly,\twe\tknow\tthat\ty\twill\tbe\ta 1D\ttensor\twith\tone\tentry\tper\tinstance,\tbut\tagain\twe\tdon’t\tknow\tthe\tsize\tof\tthe\ttraining\tbatch\tat\tthis\tpoint, so\tthe\tshape\tis\t(None).\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_inputs),\tname=\"X\") y\t=\ttf.placeholder(tf.int64,\tshape=(None),\tname=\"y\")\n\nNow\tlet’s\tcreate\tthe\tactual\tneural\tnetwork.\tThe\tplaceholder\tX\twill\tact\tas\tthe\tinput\tlayer;\tduring\tthe execution\tphase,\tit\twill\tbe\treplaced\twith\tone\ttraining\tbatch\tat\ta\ttime\t(note\tthat\tall\tthe\tinstances\tin\ta training\tbatch\twill\tbe\tprocessed\tsimultaneously\tby\tthe\tneural\tnetwork).\tNow\tyou\tneed\tto\tcreate\tthe\ttwo hidden\tlayers\tand\tthe\toutput\tlayer.\tThe\ttwo\thidden\tlayers\tare\talmost\tidentical:\tthey\tdiffer\tonly\tby\tthe inputs\tthey\tare\tconnected\tto\tand\tby\tthe\tnumber\tof\tneurons\tthey\tcontain.\tThe\toutput\tlayer\tis\talso\tvery similar,\tbut\tit\tuses\ta\tsoftmax\tactivation\tfunction\tinstead\tof\ta\tReLU\tactivation\tfunction.\tSo\tlet’s\tcreate\ta neuron_layer()\tfunction\tthat\twe\twill\tuse\tto\tcreate\tone\tlayer\tat\ta\ttime.\tIt\twill\tneed\tparameters\tto specify\tthe\tinputs,\tthe\tnumber\tof\tneurons,\tthe\tactivation\tfunction,\tand\tthe\tname\tof\tthe\tlayer:\n\ndef\tneuron_layer(X,\tn_neurons,\tname,\tactivation=None): \t\t\t\twith\ttf.name_scope(name): \t\t\t\t\t\t\t\tn_inputs\t=\tint(X.get_shape()[1]) \t\t\t\t\t\t\t\tstddev\t=\t2\t/\tnp.sqrt(n_inputs) \t\t\t\t\t\t\t\tinit\t=\ttf.truncated_normal((n_inputs,\tn_neurons),\tstddev=stddev) \t\t\t\t\t\t\t\tW\t=\ttf.Variable(init,\tname=\"kernel\") \t\t\t\t\t\t\t\tb\t=\ttf.Variable(tf.zeros([n_neurons]),\tname=\"bias\") \t\t\t\t\t\t\t\tZ\t=\ttf.matmul(X,\tW)\t+\tb \t\t\t\t\t\t\t\tif\tactivation\tis\tnot\tNone: \t\t\t\t\t\t\t\t\t\t\t\treturn\tactivation(Z) \t\t\t\t\t\t\t\telse: \t\t\t\t\t\t\t\t\t\t\t\treturn\tZ\n\nLet’s\tgo\tthrough\tthis\tcode\tline\tby\tline:\n\n1.\t First\twe\tcreate\ta\tname\tscope\tusing\tthe\tname\tof\tthe\tlayer:\tit\twill\tcontain\tall\tthe\tcomputation\tnodes for\tthis\tneuron\tlayer.\tThis\tis\toptional,\tbut\tthe\tgraph\twill\tlook\tmuch\tnicer\tin\tTensorBoard\tif\tits\tnodes are\twell\torganized.",
      "content_length": 2635,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "2.\t Next,\twe\tget\tthe\tnumber\tof\tinputs\tby\tlooking\tup\tthe\tinput\tmatrix’s\tshape\tand\tgetting\tthe\tsize\tof\tthe\n\nsecond\tdimension\t(the\tfirst\tdimension\tis\tfor\tinstances).\n\n3.\t The\tnext\tthree\tlines\tcreate\ta\tW\tvariable\tthat\twill\thold\tthe\tweights\tmatrix\t(often\tcalled\tthe\tlayer’s kernel).\tIt\twill\tbe\ta\t2D\ttensor\tcontaining\tall\tthe\tconnection\tweights\tbetween\teach\tinput\tand\teach neuron;\thence,\tits\tshape\twill\tbe\t(n_inputs,\tn_neurons).\tIt\twill\tbe\tinitialized\trandomly,\tusing\ta .\tUsing\tthis\n\n4.\t The\tnext\tline\tcreates\ta\tb\tvariable\tfor\tbiases,\tinitialized\tto\t0\t(no\tsymmetry\tissue\tin\tthis\tcase),\twith one\tbias\tparameter\tper\tneuron.\n\n5.\t Then\twe\tcreate\ta\tsubgraph\tto\tcompute\tZ\t=\tX\t·\tW\t+\tb.\tThis\tvectorized\timplementation\twill efficiently\tcompute\tthe\tweighted\tsums\tof\tthe\tinputs\tplus\tthe\tbias\tterm\tfor\teach\tand\tevery\tneuron\tin the\tlayer,\tfor\tall\tthe\tinstances\tin\tthe\tbatch\tin\tjust\tone\tshot.\n\n6.\t Finally,\tif\tan\tactivation\tparameter\tis\tprovided,\tsuch\tas\ttf.nn.relu\t(i.e.,\tmax\t(0,\tZ)),\tthen\tthe code\treturns\tactivation(Z),\tor\telse\tit\tjust\treturns\tZ.\n\nOkay,\tso\tnow\tyou\thave\ta\tnice\tfunction\tto\tcreate\ta\tneuron\tlayer.\tLet’s\tuse\tit\tto\tcreate\tthe\tdeep\tneural network!\tThe\tfirst\thidden\tlayer\ttakes\tX\tas\tits\tinput.\tThe\tsecond\ttakes\tthe\toutput\tof\tthe\tfirst\thidden\tlayer\tas its\tinput.\tAnd\tfinally,\tthe\toutput\tlayer\ttakes\tthe\toutput\tof\tthe\tsecond\thidden\tlayer\tas\tits\tinput.\n\nwith\ttf.name_scope(\"dnn\"): \t\t\t\thidden1\t=\tneuron_layer(X,\tn_hidden1,\tname=\"hidden1\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu) \t\t\t\thidden2\t=\tneuron_layer(hidden1,\tn_hidden2,\tname=\"hidden2\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu) \t\t\t\tlogits\t=\tneuron_layer(hidden2,\tn_outputs,\tname=\"outputs\")\n\nNotice\tthat\tonce\tagain\twe\tused\ta\tname\tscope\tfor\tclarity.\tAlso\tnote\tthat\tlogits\tis\tthe\toutput\tof\tthe\tneural network\tbefore\tgoing\tthrough\tthe\tsoftmax\tactivation\tfunction:\tfor\toptimization\treasons,\twe\twill\thandle\tthe softmax\tcomputation\tlater.\n\nAs\tyou\tmight\texpect,\tTensorFlow\tcomes\twith\tmany\thandy\tfunctions\tto\tcreate\tstandard\tneural\tnetwork layers,\tso\tthere’s\toften\tno\tneed\tto\tdefine\tyour\town\tneuron_layer()\tfunction\tlike\twe\tjust\tdid.\tFor example,\tTensorFlow’s\ttf.layers.dense()\tfunction\t(previously\tcalled tf.contrib.layers.fully_connected())\tcreates\ta\tfully\tconnected\tlayer,\twhere\tall\tthe\tinputs\tare connected\tto\tall\tthe\tneurons\tin\tthe\tlayer.\tIt\ttakes\tcare\tof\tcreating\tthe\tweights\tand\tbiases\tvariables,\tnamed kernel\tand\tbias\trespectively,\tusing\tthe\tappropriate\tinitialization\tstrategy,\tand\tyou\tcan\tset\tthe\tactivation function\tusing\tthe\tactivation\targument.\tAs\twe\twill\tsee\tin\tChapter\t11,\tit\talso\tsupports\tregularization parameters.\tLet’s\ttweak\tthe\tpreceding\tcode\tto\tuse\tthe\tdense()\tfunction\tinstead\tof\tour\tneuron_layer() function.\tSimply\treplace\tthe\tdnn\tconstruction\tsection\twith\tthe\tfollowing\tcode:\n\nwith\ttf.name_scope(\"dnn\"):",
      "content_length": 2782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "hidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tname=\"hidden1\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu) \t\t\t\thidden2\t=\ttf.layers.dense(hidden1,\tn_hidden2,\tname=\"hidden2\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu) \t\t\t\tlogits\t=\ttf.layers.dense(hidden2,\tn_outputs,\tname=\"outputs\")\n\nNow\tthat\twe\thave\tthe\tneural\tnetwork\tmodel\tready\tto\tgo,\twe\tneed\tto\tdefine\tthe\tcost\tfunction\tthat\twe\twill use\tto\ttrain\tit.\tJust\tas\twe\tdid\tfor\tSoftmax\tRegression\tin\tChapter\t4,\twe\twill\tuse\tcross\tentropy.\tAs\twe discussed\tearlier,\tcross\tentropy\twill\tpenalize\tmodels\tthat\testimate\ta\tlow\tprobability\tfor\tthe\ttarget\tclass. TensorFlow\tprovides\tseveral\tfunctions\tto\tcompute\tcross\tentropy.\tWe\twill\tuse sparse_softmax_cross_entropy_with_logits():\tit\tcomputes\tthe\tcross\tentropy\tbased\ton\tthe\t“logits” (i.e.,\tthe\toutput\tof\tthe\tnetwork\tbefore\tgoing\tthrough\tthe\tsoftmax\tactivation\tfunction),\tand\tit\texpects\tlabels in\tthe\tform\tof\tintegers\tranging\tfrom\t0\tto\tthe\tnumber\tof\tclasses\tminus\t1\t(in\tour\tcase,\tfrom\t0\tto\t9).\tThis will\tgive\tus\ta\t1D\ttensor\tcontaining\tthe\tcross\tentropy\tfor\teach\tinstance.\tWe\tcan\tthen\tuse\tTensorFlow’s reduce_mean()\tfunction\tto\tcompute\tthe\tmean\tcross\tentropy\tover\tall\tinstances.\n\nwith\ttf.name_scope(\"loss\"): \t\t\t\txentropy\t=\ttf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogits=logits) \t\t\t\tloss\t=\ttf.reduce_mean(xentropy,\tname=\"loss\")\n\nNOTE\n\nThe\tsparse_softmax_cross_entropy_with_logits()\tfunction\tis\tequivalent\tto\tapplying\tthe\tsoftmax\tactivation\tfunction\tand\tthen computing\tthe\tcross\tentropy,\tbut\tit\tis\tmore\tefficient,\tand\tit\tproperly\ttakes\tcare\tof\tcorner\tcases\tlike\tlogits\tequal\tto\t0.\tThis\tis\twhy we\tdid\tnot\tapply\tthe\tsoftmax\tactivation\tfunction\tearlier.\tThere\tis\talso\tanother\tfunction\tcalled softmax_cross_entropy_with_logits(),\twhich\ttakes\tlabels\tin\tthe\tform\tof\tone-hot\tvectors\t(instead\tof\tints\tfrom\t0\tto\tthe\tnumber of\tclasses\tminus\t1).\n\nWe\thave\tthe\tneural\tnetwork\tmodel,\twe\thave\tthe\tcost\tfunction,\tand\tnow\twe\tneed\tto\tdefine\ta GradientDescentOptimizer\tthat\twill\ttweak\tthe\tmodel\tparameters\tto\tminimize\tthe\tcost\tfunction. Nothing\tnew;\tit’s\tjust\tlike\twe\tdid\tin\tChapter\t9:\n\nlearning_rate\t=\t0.01\n\nwith\ttf.name_scope(\"train\"): \t\t\t\toptimizer\t=\ttf.train.GradientDescentOptimizer(learning_rate) \t\t\t\ttraining_op\t=\toptimizer.minimize(loss)\n\nThe\tlast\timportant\tstep\tin\tthe\tconstruction\tphase\tis\tto\tspecify\thow\tto\tevaluate\tthe\tmodel.\tWe\twill\tsimply use\taccuracy\tas\tour\tperformance\tmeasure.\tFirst,\tfor\teach\tinstance,\tdetermine\tif\tthe\tneural\tnetwork’s prediction\tis\tcorrect\tby\tchecking\twhether\tor\tnot\tthe\thighest\tlogit\tcorresponds\tto\tthe\ttarget\tclass.\tFor\tthis you\tcan\tuse\tthe\tin_top_k()\tfunction.\tThis\treturns\ta\t1D\ttensor\tfull\tof\tboolean\tvalues,\tso\twe\tneed\tto\tcast these\tbooleans\tto\tfloats\tand\tthen\tcompute\tthe\taverage.\tThis\twill\tgive\tus\tthe\tnetwork’s\toverall\taccuracy.\n\nwith\ttf.name_scope(\"eval\"): \t\t\t\tcorrect\t=\ttf.nn.in_top_k(logits,\ty,\t1) \t\t\t\taccuracy\t=\ttf.reduce_mean(tf.cast(correct,\ttf.float32))\n\nAnd,\tas\tusual,\twe\tneed\tto\tcreate\ta\tnode\tto\tinitialize\tall\tvariables,\tand\twe\twill\talso\tcreate\ta\tSaver\tto",
      "content_length": 3062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "save\tour\ttrained\tmodel\tparameters\tto\tdisk:\n\ninit\t=\ttf.global_variables_initializer() saver\t=\ttf.train.Saver()\n\nPhew!\tThis\tconcludes\tthe\tconstruction\tphase.\tThis\twas\tfewer\tthan\t40\tlines\tof\tcode,\tbut\tit\twas\tpretty intense:\twe\tcreated\tplaceholders\tfor\tthe\tinputs\tand\tthe\ttargets,\twe\tcreated\ta\tfunction\tto\tbuild\ta\tneuron layer,\twe\tused\tit\tto\tcreate\tthe\tDNN,\twe\tdefined\tthe\tcost\tfunction,\twe\tcreated\tan\toptimizer,\tand\tfinally\twe defined\tthe\tperformance\tmeasure.\tNow\ton\tto\tthe\texecution\tphase.",
      "content_length": 487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "Execution\tPhase This\tpart\tis\tmuch\tshorter\tand\tsimpler.\tFirst,\tlet’s\tload\tMNIST.\tWe\tcould\tuse\tScikit-Learn\tfor\tthat\tas\twe did\tin\tprevious\tchapters,\tbut\tTensorFlow\toffers\tits\town\thelper\tthat\tfetches\tthe\tdata,\tscales\tit\t(between\t0 and\t1),\tshuffles\tit,\tand\tprovides\ta\tsimple\tfunction\tto\tload\tone\tmini-batch\ta\ttime.\tSo\tlet’s\tuse\tit\tinstead:\n\nfrom\ttensorflow.examples.tutorials.mnist\timport\tinput_data mnist\t=\tinput_data.read_data_sets(\"/tmp/data/\")\n\nNow\twe\tdefine\tthe\tnumber\tof\tepochs\tthat\twe\twant\tto\trun,\tas\twell\tas\tthe\tsize\tof\tthe\tmini-batches:\n\nn_epochs\t=\t40 batch_size\t=\t50\n\nAnd\tnow\twe\tcan\ttrain\tthe\tmodel:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tfor\titeration\tin\trange(mnist.train.num_examples\t//\tbatch_size): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\tacc_train\t=\taccuracy.eval(feed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\tacc_test\t=\taccuracy.eval(feed_dict={X:\tmnist.test.images, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ty:\tmnist.test.labels}) \t\t\t\t\t\t\t\tprint(epoch,\t\"Train\taccuracy:\",\tacc_train,\t\"Test\taccuracy:\",\tacc_test)\n\nsave_path\t=\tsaver.save(sess,\t\"./my_model_final.ckpt\")\n\nThis\tcode\topens\ta\tTensorFlow\tsession,\tand\tit\truns\tthe\tinit\tnode\tthat\tinitializes\tall\tthe\tvariables.\tThen\tit runs\tthe\tmain\ttraining\tloop:\tat\teach\tepoch,\tthe\tcode\titerates\tthrough\ta\tnumber\tof\tmini-batches\tthat corresponds\tto\tthe\ttraining\tset\tsize.\tEach\tmini-batch\tis\tfetched\tvia\tthe\tnext_batch()\tmethod,\tand\tthen the\tcode\tsimply\truns\tthe\ttraining\toperation,\tfeeding\tit\tthe\tcurrent\tmini-batch\tinput\tdata\tand\ttargets.\tNext, at\tthe\tend\tof\teach\tepoch,\tthe\tcode\tevaluates\tthe\tmodel\ton\tthe\tlast\tmini-batch\tand\ton\tthe\tfull\ttest\tset,\tand\tit prints\tout\tthe\tresult.\tFinally,\tthe\tmodel\tparameters\tare\tsaved\tto\tdisk.",
      "content_length": 1831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Using\tthe\tNeural\tNetwork Now\tthat\tthe\tneural\tnetwork\tis\ttrained,\tyou\tcan\tuse\tit\tto\tmake\tpredictions.\tTo\tdo\tthat,\tyou\tcan\treuse\tthe same\tconstruction\tphase,\tbut\tchange\tthe\texecution\tphase\tlike\tthis:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsaver.restore(sess,\t\"./my_model_final.ckpt\") \t\t\t\tX_new_scaled\t=\t[...]\t\t#\tsome\tnew\timages\t(scaled\tfrom\t0\tto\t1) \t\t\t\tZ\t=\tlogits.eval(feed_dict={X:\tX_new_scaled}) \t\t\t\ty_pred\t=\tnp.argmax(Z,\taxis=1)\n\nFirst\tthe\tcode\tloads\tthe\tmodel\tparameters\tfrom\tdisk.\tThen\tit\tloads\tsome\tnew\timages\tthat\tyou\twant\tto classify.\tRemember\tto\tapply\tthe\tsame\tfeature\tscaling\tas\tfor\tthe\ttraining\tdata\t(in\tthis\tcase,\tscale\tit\tfrom\t0 to\t1).\tThen\tthe\tcode\tevaluates\tthe\tlogits\tnode.\tIf\tyou\twanted\tto\tknow\tall\tthe\testimated\tclass probabilities,\tyou\twould\tneed\tto\tapply\tthe\tsoftmax()\tfunction\tto\tthe\tlogits,\tbut\tif\tyou\tjust\twant\tto\tpredict a\tclass,\tyou\tcan\tsimply\tpick\tthe\tclass\tthat\thas\tthe\thighest\tlogit\tvalue\t(using\tthe\targmax()\tfunction\tdoes the\ttrick).",
      "content_length": 953,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "Fine-Tuning\tNeural\tNetwork\tHyperparameters The\tflexibility\tof\tneural\tnetworks\tis\talso\tone\tof\ttheir\tmain\tdrawbacks:\tthere\tare\tmany\thyperparameters\tto tweak.\tNot\tonly\tcan\tyou\tuse\tany\timaginable\tnetwork\ttopology\t(how\tneurons\tare\tinterconnected),\tbut\teven in\ta\tsimple\tMLP\tyou\tcan\tchange\tthe\tnumber\tof\tlayers,\tthe\tnumber\tof\tneurons\tper\tlayer,\tthe\ttype\tof activation\tfunction\tto\tuse\tin\teach\tlayer,\tthe\tweight\tinitialization\tlogic,\tand\tmuch\tmore.\tHow\tdo\tyou\tknow what\tcombination\tof\thyperparameters\tis\tthe\tbest\tfor\tyour\ttask?\n\nOf\tcourse,\tyou\tcan\tuse\tgrid\tsearch\twith\tcross-validation\tto\tfind\tthe\tright\thyperparameters,\tlike\tyou\tdid\tin previous\tchapters,\tbut\tsince\tthere\tare\tmany\thyperparameters\tto\ttune,\tand\tsince\ttraining\ta\tneural\tnetwork on\ta\tlarge\tdataset\ttakes\ta\tlot\tof\ttime,\tyou\twill\tonly\tbe\table\tto\texplore\ta\ttiny\tpart\tof\tthe\thyperparameter space\tin\ta\treasonable\tamount\tof\ttime.\tIt\tis\tmuch\tbetter\tto\tuse\trandomized\tsearch,\tas\twe\tdiscussed\tin Chapter\t2.\tAnother\toption\tis\tto\tuse\ta\ttool\tsuch\tas\tOscar,\twhich\timplements\tmore\tcomplex\talgorithms\tto help\tyou\tfind\ta\tgood\tset\tof\thyperparameters\tquickly.\n\nIt\thelps\tto\thave\tan\tidea\tof\twhat\tvalues\tare\treasonable\tfor\teach\thyperparameter,\tso\tyou\tcan\trestrict\tthe search\tspace.\tLet’s\tstart\twith\tthe\tnumber\tof\thidden\tlayers.",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "Number\tof\tHidden\tLayers For\tmany\tproblems,\tyou\tcan\tjust\tbegin\twith\ta\tsingle\thidden\tlayer\tand\tyou\twill\tget\treasonable\tresults.\tIt has\tactually\tbeen\tshown\tthat\tan\tMLP\twith\tjust\tone\thidden\tlayer\tcan\tmodel\teven\tthe\tmost\tcomplex functions\tprovided\tit\thas\tenough\tneurons.\tFor\ta\tlong\ttime,\tthese\tfacts\tconvinced\tresearchers\tthat\tthere was\tno\tneed\tto\tinvestigate\tany\tdeeper\tneural\tnetworks.\tBut\tthey\toverlooked\tthe\tfact\tthat\tdeep\tnetworks have\ta\tmuch\thigher\tparameter\tefficiency\tthan\tshallow\tones:\tthey\tcan\tmodel\tcomplex\tfunctions\tusing exponentially\tfewer\tneurons\tthan\tshallow\tnets,\tmaking\tthem\tmuch\tfaster\tto\ttrain.\n\nTo\tunderstand\twhy,\tsuppose\tyou\tare\tasked\tto\tdraw\ta\tforest\tusing\tsome\tdrawing\tsoftware,\tbut\tyou\tare forbidden\tto\tuse\tcopy/paste.\tYou\twould\thave\tto\tdraw\teach\ttree\tindividually,\tbranch\tper\tbranch,\tleaf\tper leaf.\tIf\tyou\tcould\tinstead\tdraw\tone\tleaf,\tcopy/paste\tit\tto\tdraw\ta\tbranch,\tthen\tcopy/paste\tthat\tbranch\tto create\ta\ttree,\tand\tfinally\tcopy/paste\tthis\ttree\tto\tmake\ta\tforest,\tyou\twould\tbe\tfinished\tin\tno\ttime.\tReal- world\tdata\tis\toften\tstructured\tin\tsuch\ta\thierarchical\tway\tand\tDNNs\tautomatically\ttake\tadvantage\tof\tthis fact:\tlower\thidden\tlayers\tmodel\tlow-level\tstructures\t(e.g.,\tline\tsegments\tof\tvarious\tshapes\tand orientations),\tintermediate\thidden\tlayers\tcombine\tthese\tlow-level\tstructures\tto\tmodel\tintermediate-level structures\t(e.g.,\tsquares,\tcircles),\tand\tthe\thighest\thidden\tlayers\tand\tthe\toutput\tlayer\tcombine\tthese intermediate\tstructures\tto\tmodel\thigh-level\tstructures\t(e.g.,\tfaces).\n\nNot\tonly\tdoes\tthis\thierarchical\tarchitecture\thelp\tDNNs\tconverge\tfaster\tto\ta\tgood\tsolution,\tit\talso improves\ttheir\tability\tto\tgeneralize\tto\tnew\tdatasets.\tFor\texample,\tif\tyou\thave\talready\ttrained\ta\tmodel\tto recognize\tfaces\tin\tpictures,\tand\tyou\tnow\twant\tto\ttrain\ta\tnew\tneural\tnetwork\tto\trecognize\thairstyles,\tthen you\tcan\tkickstart\ttraining\tby\treusing\tthe\tlower\tlayers\tof\tthe\tfirst\tnetwork.\tInstead\tof\trandomly\tinitializing the\tweights\tand\tbiases\tof\tthe\tfirst\tfew\tlayers\tof\tthe\tnew\tneural\tnetwork,\tyou\tcan\tinitialize\tthem\tto\tthe value\tof\tthe\tweights\tand\tbiases\tof\tthe\tlower\tlayers\tof\tthe\tfirst\tnetwork.\tThis\tway\tthe\tnetwork\twill\tnot have\tto\tlearn\tfrom\tscratch\tall\tthe\tlow-level\tstructures\tthat\toccur\tin\tmost\tpictures;\tit\twill\tonly\thave\tto learn\tthe\thigher-level\tstructures\t(e.g.,\thairstyles).\n\nIn\tsummary,\tfor\tmany\tproblems\tyou\tcan\tstart\twith\tjust\tone\tor\ttwo\thidden\tlayers\tand\tit\twill\twork\tjust\tfine (e.g.,\tyou\tcan\teasily\treach\tabove\t97%\taccuracy\ton\tthe\tMNIST\tdataset\tusing\tjust\tone\thidden\tlayer\twith\ta few\thundred\tneurons,\tand\tabove\t98%\taccuracy\tusing\ttwo\thidden\tlayers\twith\tthe\tsame\ttotal\tamount\tof neurons,\tin\troughly\tthe\tsame\tamount\tof\ttraining\ttime).\tFor\tmore\tcomplex\tproblems,\tyou\tcan\tgradually ramp\tup\tthe\tnumber\tof\thidden\tlayers,\tuntil\tyou\tstart\toverfitting\tthe\ttraining\tset.\tVery\tcomplex\ttasks,\tsuch as\tlarge\timage\tclassification\tor\tspeech\trecognition,\ttypically\trequire\tnetworks\twith\tdozens\tof\tlayers\t(or even\thundreds,\tbut\tnot\tfully\tconnected\tones,\tas\twe\twill\tsee\tin\tChapter\t13),\tand\tthey\tneed\ta\thuge\tamount of\ttraining\tdata.\tHowever,\tyou\twill\trarely\thave\tto\ttrain\tsuch\tnetworks\tfrom\tscratch:\tit\tis\tmuch\tmore common\tto\treuse\tparts\tof\ta\tpretrained\tstate-of-the-art\tnetwork\tthat\tperforms\ta\tsimilar\ttask.\tTraining\twill be\ta\tlot\tfaster\tand\trequire\tmuch\tless\tdata\t(we\twill\tdiscuss\tthis\tin\tChapter\t11).",
      "content_length": 3307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "Number\tof\tNeurons\tper\tHidden\tLayer Obviously\tthe\tnumber\tof\tneurons\tin\tthe\tinput\tand\toutput\tlayers\tis\tdetermined\tby\tthe\ttype\tof\tinput\tand output\tyour\ttask\trequires.\tFor\texample,\tthe\tMNIST\ttask\trequires\t28\tx\t28\t=\t784\tinput\tneurons\tand\t10 output\tneurons.\tAs\tfor\tthe\thidden\tlayers,\ta\tcommon\tpractice\tis\tto\tsize\tthem\tto\tform\ta\tfunnel,\twith\tfewer and\tfewer\tneurons\tat\teach\tlayer\t—\tthe\trationale\tbeing\tthat\tmany\tlow-level\tfeatures\tcan\tcoalesce\tinto\tfar fewer\thigh-level\tfeatures.\tFor\texample,\ta\ttypical\tneural\tnetwork\tfor\tMNIST\tmay\thave\ttwo\thidden\tlayers, the\tfirst\twith\t300\tneurons\tand\tthe\tsecond\twith\t100.\tHowever,\tthis\tpractice\tis\tnot\tas\tcommon\tnow,\tand you\tmay\tsimply\tuse\tthe\tsame\tsize\tfor\tall\thidden\tlayers\t—\tfor\texample,\tall\thidden\tlayers\twith\t150 neurons:\tthat’s\tjust\tone\thyperparameter\tto\ttune\tinstead\tof\tone\tper\tlayer.\tJust\tlike\tfor\tthe\tnumber\tof\tlayers, you\tcan\ttry\tincreasing\tthe\tnumber\tof\tneurons\tgradually\tuntil\tthe\tnetwork\tstarts\toverfitting.\tIn\tgeneral\tyou will\tget\tmore\tbang\tfor\tthe\tbuck\tby\tincreasing\tthe\tnumber\tof\tlayers\tthan\tthe\tnumber\tof\tneurons\tper\tlayer. Unfortunately,\tas\tyou\tcan\tsee,\tfinding\tthe\tperfect\tamount\tof\tneurons\tis\tstill\tsomewhat\tof\ta\tblack\tart.\n\nA\tsimpler\tapproach\tis\tto\tpick\ta\tmodel\twith\tmore\tlayers\tand\tneurons\tthan\tyou\tactually\tneed,\tthen\tuse\tearly stopping\tto\tprevent\tit\tfrom\toverfitting\t(and\tother\tregularization\ttechniques,\tespecially\tdropout,\tas\twe\twill see\tin\tChapter\t11).\tThis\thas\tbeen\tdubbed\tthe\t“stretch\tpants”\tapproach:12\tinstead\tof\twasting\ttime\tlooking for\tpants\tthat\tperfectly\tmatch\tyour\tsize,\tjust\tuse\tlarge\tstretch\tpants\tthat\twill\tshrink\tdown\tto\tthe\tright\tsize.",
      "content_length": 1605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "Activation\tFunctions In\tmost\tcases\tyou\tcan\tuse\tthe\tReLU\tactivation\tfunction\tin\tthe\thidden\tlayers\t(or\tone\tof\tits\tvariants,\tas\twe will\tsee\tin\tChapter\t11).\tIt\tis\ta\tbit\tfaster\tto\tcompute\tthan\tother\tactivation\tfunctions,\tand\tGradient\tDescent does\tnot\tget\tstuck\tas\tmuch\ton\tplateaus,\tthanks\tto\tthe\tfact\tthat\tit\tdoes\tnot\tsaturate\tfor\tlarge\tinput\tvalues\t(as opposed\tto\tthe\tlogistic\tfunction\tor\tthe\thyperbolic\ttangent\tfunction,\twhich\tsaturate\tat\t1).\n\nFor\tthe\toutput\tlayer,\tthe\tsoftmax\tactivation\tfunction\tis\tgenerally\ta\tgood\tchoice\tfor\tclassification\ttasks (when\tthe\tclasses\tare\tmutually\texclusive).\tFor\tregression\ttasks,\tyou\tcan\tsimply\tuse\tno\tactivation\tfunction at\tall.\n\nThis\tconcludes\tthis\tintroduction\tto\tartificial\tneural\tnetworks.\tIn\tthe\tfollowing\tchapters,\twe\twill\tdiscuss techniques\tto\ttrain\tvery\tdeep\tnets,\tand\tdistribute\ttraining\tacross\tmultiple\tservers\tand\tGPUs.\tThen\twe\twill explore\ta\tfew\tother\tpopular\tneural\tnetwork\tarchitectures:\tconvolutional\tneural\tnetworks,\trecurrent\tneural networks,\tand\tautoencoders.13",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "Exercises\n\n1.\t Draw\tan\tANN\tusing\tthe\toriginal\tartificial\tneurons\t(like\tthe\tones\tin\tFigure\t10-3)\tthat\tcomputes\tA B).\n\n2.\t Why\tis\tit\tgenerally\tpreferable\tto\tuse\ta\tLogistic\tRegression\tclassifier\trather\tthan\ta\tclassical Perceptron\t(i.e.,\ta\tsingle\tlayer\tof\tlinear\tthreshold\tunits\ttrained\tusing\tthe\tPerceptron\ttraining algorithm)?\tHow\tcan\tyou\ttweak\ta\tPerceptron\tto\tmake\tit\tequivalent\tto\ta\tLogistic\tRegression classifier?\n\n3.\t Why\twas\tthe\tlogistic\tactivation\tfunction\ta\tkey\tingredient\tin\ttraining\tthe\tfirst\tMLPs?\n\n4.\t Name\tthree\tpopular\tactivation\tfunctions.\tCan\tyou\tdraw\tthem?\n\n5.\t Suppose\tyou\thave\tan\tMLP\tcomposed\tof\tone\tinput\tlayer\twith\t10\tpassthrough\tneurons,\tfollowed\tby one\thidden\tlayer\twith\t50\tartificial\tneurons,\tand\tfinally\tone\toutput\tlayer\twith\t3\tartificial\tneurons.\tAll artificial\tneurons\tuse\tthe\tReLU\tactivation\tfunction.\n\nWhat\tis\tthe\tshape\tof\tthe\tinput\tmatrix\tX?\n\nWhat\tabout\tthe\tshape\tof\tthe\thidden\tlayer’s\tweight\tvector\tWh,\tand\tthe\tshape\tof\tits\tbias\tvector bh?\n\nWhat\tis\tthe\tshape\tof\tthe\toutput\tlayer’s\tweight\tvector\tWo,\tand\tits\tbias\tvector\tbo?\n\nWhat\tis\tthe\tshape\tof\tthe\tnetwork’s\toutput\tmatrix\tY?\n\nWrite\tthe\tequation\tthat\tcomputes\tthe\tnetwork’s\toutput\tmatrix\tY\tas\ta\tfunction\tof\tX,\tWh,\tbh,\tWo and\tbo.\n\n6.\t How\tmany\tneurons\tdo\tyou\tneed\tin\tthe\toutput\tlayer\tif\tyou\twant\tto\tclassify\temail\tinto\tspam\tor\tham? What\tactivation\tfunction\tshould\tyou\tuse\tin\tthe\toutput\tlayer?\tIf\tinstead\tyou\twant\tto\ttackle\tMNIST, how\tmany\tneurons\tdo\tyou\tneed\tin\tthe\toutput\tlayer,\tusing\twhat\tactivation\tfunction?\tAnswer\tthe\tsame questions\tfor\tgetting\tyour\tnetwork\tto\tpredict\thousing\tprices\tas\tin\tChapter\t2.\n\n7.\t What\tis\tbackpropagation\tand\thow\tdoes\tit\twork?\tWhat\tis\tthe\tdifference\tbetween\tbackpropagation and\treverse-mode\tautodiff?\n\n8.\t Can\tyou\tlist\tall\tthe\thyperparameters\tyou\tcan\ttweak\tin\tan\tMLP?\tIf\tthe\tMLP\toverfits\tthe\ttraining\tdata, how\tcould\tyou\ttweak\tthese\thyperparameters\tto\ttry\tto\tsolve\tthe\tproblem?\n\n9.\t Train\ta\tdeep\tMLP\ton\tthe\tMNIST\tdataset\tand\tsee\tif\tyou\tcan\tget\tover\t98%\tprecision.\tJust\tlike\tin\tthe last\texercise\tof\tChapter\t9,\ttry\tadding\tall\tthe\tbells\tand\twhistles\t(i.e.,\tsave\tcheckpoints,\trestore\tthe last\tcheckpoint\tin\tcase\tof\tan\tinterruption,\tadd\tsummaries,\tplot\tlearning\tcurves\tusing\tTensorBoard, and\tso\ton).\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.",
      "content_length": 2260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\nYou\tcan\tget\tthe\tbest\tof\tboth\tworlds\tby\tbeing\topen\tto\tbiological\tinspirations\twithout\tbeing\tafraid\tto\tcreate\tbiologically\tunrealistic\tmodels,\tas long\tas\tthey\twork\twell.\n\n“A\tLogical\tCalculus\tof\tIdeas\tImmanent\tin\tNervous\tActivity,”\tW.\tMcCulloch\tand\tW.\tPitts\t(1943).\n\nImage\tby\tBruce\tBlaus\t(Creative\tCommons\t3.0).\tReproduced\tfrom\thttps://en.wikipedia.org/wiki/Neuron.\n\nIn\tthe\tcontext\tof\tMachine\tLearning,\tthe\tphrase\t“neural\tnetworks”\tgenerally\trefers\tto\tANNs,\tnot\tBNNs.\n\nDrawing\tof\ta\tcortical\tlamination\tby\tS.\tRamon\ty\tCajal\t(public\tdomain).\tReproduced\tfrom\thttps://en.wikipedia.org/wiki/Cerebral_cortex.\n\nThe\tname\tPerceptron\tis\tsometimes\tused\tto\tmean\ta\ttiny\tnetwork\twith\ta\tsingle\tLTU.\n\nNote\tthat\tthis\tsolution\tis\tgenerally\tnot\tunique:\tin\tgeneral\twhen\tthe\tdata\tare\tlinearly\tseparable,\tthere\tis\tan\tinfinity\tof\thyperplanes\tthat\tcan separate\tthem.\n\n“Learning\tInternal\tRepresentations\tby\tError\tPropagation,”\tD.\tRumelhart,\tG.\tHinton,\tR.\tWilliams\t(1986).\n\nThis\talgorithm\twas\tactually\tinvented\tseveral\ttimes\tby\tvarious\tresearchers\tin\tdifferent\tfields,\tstarting\twith\tP.\tWerbos\tin\t1974.\n\nUsing\ta\ttruncated\tnormal\tdistribution\trather\tthan\ta\tregular\tnormal\tdistribution\tensures\tthat\tthere\twon’t\tbe\tany\tlarge\tweights,\twhich\tcould slow\tdown\ttraining.\n\nFor\texample,\tif\tyou\tset\tall\tthe\tweights\tto\t0,\tthen\tall\tneurons\twill\toutput\t0,\tand\tthe\terror\tgradient\twill\tbe\tthe\tsame\tfor\tall\tneurons\tin\ta\tgiven hidden\tlayer.\tThe\tGradient\tDescent\tstep\twill\tthen\tupdate\tall\tthe\tweights\tin\texactly\tthe\tsame\tway\tin\teach\tlayer,\tso\tthey\twill\tall\tremain equal.\tIn\tother\twords,\tdespite\thaving\thundreds\tof\tneurons\tper\tlayer,\tyour\tmodel\twill\tact\tas\tif\tthere\twere\tonly\tone\tneuron\tper\tlayer.\tIt\tis\tnot going\tto\tfly.\n\nBy\tVincent\tVanhoucke\tin\this\tDeep\tLearning\tclass\ton\tUdacity.com.\n\nA\tfew\textra\tANN\tarchitectures\tare\tpresented\tin\tAppendix\tE.",
      "content_length": 1838,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "Chapter\t11.\tTraining\tDeep\tNeural\tNets\n\nIn\tChapter\t10\twe\tintroduced\tartificial\tneural\tnetworks\tand\ttrained\tour\tfirst\tdeep\tneural\tnetwork.\tBut\tit was\ta\tvery\tshallow\tDNN,\twith\tonly\ttwo\thidden\tlayers.\tWhat\tif\tyou\tneed\tto\ttackle\ta\tvery\tcomplex problem,\tsuch\tas\tdetecting\thundreds\tof\ttypes\tof\tobjects\tin\thigh-resolution\timages?\tYou\tmay\tneed\tto\ttrain a\tmuch\tdeeper\tDNN,\tperhaps\twith\t(say)\t10\tlayers,\teach\tcontaining\thundreds\tof\tneurons,\tconnected\tby hundreds\tof\tthousands\tof\tconnections.\tThis\twould\tnot\tbe\ta\twalk\tin\tthe\tpark:\n\nFirst,\tyou\twould\tbe\tfaced\twith\tthe\ttricky\tvanishing\tgradients\tproblem\t(or\tthe\trelated\texploding gradients\tproblem)\tthat\taffects\tdeep\tneural\tnetworks\tand\tmakes\tlower\tlayers\tvery\thard\tto\ttrain.\n\nSecond,\twith\tsuch\ta\tlarge\tnetwork,\ttraining\twould\tbe\textremely\tslow.\n\nThird,\ta\tmodel\twith\tmillions\tof\tparameters\twould\tseverely\trisk\toverfitting\tthe\ttraining\tset.\n\nIn\tthis\tchapter,\twe\twill\tgo\tthrough\teach\tof\tthese\tproblems\tin\tturn\tand\tpresent\ttechniques\tto\tsolve\tthem. We\twill\tstart\tby\texplaining\tthe\tvanishing\tgradients\tproblem\tand\texploring\tsome\tof\tthe\tmost\tpopular solutions\tto\tthis\tproblem.\tNext\twe\twill\tlook\tat\tvarious\toptimizers\tthat\tcan\tspeed\tup\ttraining\tlarge\tmodels tremendously\tcompared\tto\tplain\tGradient\tDescent.\tFinally,\twe\twill\tgo\tthrough\ta\tfew\tpopular regularization\ttechniques\tfor\tlarge\tneural\tnetworks.\n\nWith\tthese\ttools,\tyou\twill\tbe\table\tto\ttrain\tvery\tdeep\tnets:\twelcome\tto\tDeep\tLearning!",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "Vanishing/Exploding\tGradients\tProblems As\twe\tdiscussed\tin\tChapter\t10,\tthe\tbackpropagation\talgorithm\tworks\tby\tgoing\tfrom\tthe\toutput\tlayer\tto\tthe input\tlayer,\tpropagating\tthe\terror\tgradient\ton\tthe\tway.\tOnce\tthe\talgorithm\thas\tcomputed\tthe\tgradient\tof\tthe cost\tfunction\twith\tregards\tto\teach\tparameter\tin\tthe\tnetwork,\tit\tuses\tthese\tgradients\tto\tupdate\teach parameter\twith\ta\tGradient\tDescent\tstep.\n\nUnfortunately,\tgradients\toften\tget\tsmaller\tand\tsmaller\tas\tthe\talgorithm\tprogresses\tdown\tto\tthe\tlower layers.\tAs\ta\tresult,\tthe\tGradient\tDescent\tupdate\tleaves\tthe\tlower\tlayer\tconnection\tweights\tvirtually unchanged,\tand\ttraining\tnever\tconverges\tto\ta\tgood\tsolution.\tThis\tis\tcalled\tthe\tvanishing\tgradients problem.\tIn\tsome\tcases,\tthe\topposite\tcan\thappen:\tthe\tgradients\tcan\tgrow\tbigger\tand\tbigger,\tso\tmany layers\tget\tinsanely\tlarge\tweight\tupdates\tand\tthe\talgorithm\tdiverges.\tThis\tis\tthe\texploding\tgradients problem,\twhich\tis\tmostly\tencountered\tin\trecurrent\tneural\tnetworks\t(see\tChapter\t14).\tMore\tgenerally, deep\tneural\tnetworks\tsuffer\tfrom\tunstable\tgradients;\tdifferent\tlayers\tmay\tlearn\tat\twidely\tdifferent\tspeeds.\n\nAlthough\tthis\tunfortunate\tbehavior\thas\tbeen\tempirically\tobserved\tfor\tquite\ta\twhile\t(it\twas\tone\tof\tthe reasons\twhy\tdeep\tneural\tnetworks\twere\tmostly\tabandoned\tfor\ta\tlong\ttime),\tit\tis\tonly\taround\t2010\tthat significant\tprogress\twas\tmade\tin\tunderstanding\tit.\tA\tpaper\ttitled\t“Understanding\tthe\tDifficulty\tof\tTraining Deep\tFeedforward\tNeural\tNetworks”\tby\tXavier\tGlorot\tand\tYoshua\tBengio1\tfound\ta\tfew\tsuspects, including\tthe\tcombination\tof\tthe\tpopular\tlogistic\tsigmoid\tactivation\tfunction\tand\tthe\tweight\tinitialization technique\tthat\twas\tmost\tpopular\tat\tthe\ttime,\tnamely\trandom\tinitialization\tusing\ta\tnormal\tdistribution\twith a\tmean\tof\t0\tand\ta\tstandard\tdeviation\tof\t1.\tIn\tshort,\tthey\tshowed\tthat\twith\tthis\tactivation\tfunction\tand\tthis initialization\tscheme,\tthe\tvariance\tof\tthe\toutputs\tof\teach\tlayer\tis\tmuch\tgreater\tthan\tthe\tvariance\tof\tits inputs.\tGoing\tforward\tin\tthe\tnetwork,\tthe\tvariance\tkeeps\tincreasing\tafter\teach\tlayer\tuntil\tthe\tactivation function\tsaturates\tat\tthe\ttop\tlayers.\tThis\tis\tactually\tmade\tworse\tby\tthe\tfact\tthat\tthe\tlogistic\tfunction\thas\ta mean\tof\t0.5,\tnot\t0\t(the\thyperbolic\ttangent\tfunction\thas\ta\tmean\tof\t0\tand\tbehaves\tslightly\tbetter\tthan\tthe logistic\tfunction\tin\tdeep\tnetworks).\n\nLooking\tat\tthe\tlogistic\tactivation\tfunction\t(see\tFigure\t11-1),\tyou\tcan\tsee\tthat\twhen\tinputs\tbecome\tlarge (negative\tor\tpositive),\tthe\tfunction\tsaturates\tat\t0\tor\t1,\twith\ta\tderivative\textremely\tclose\tto\t0.\tThus\twhen backpropagation\tkicks\tin,\tit\thas\tvirtually\tno\tgradient\tto\tpropagate\tback\tthrough\tthe\tnetwork,\tand\twhat little\tgradient\texists\tkeeps\tgetting\tdiluted\tas\tbackpropagation\tprogresses\tdown\tthrough\tthe\ttop\tlayers,\tso there\tis\treally\tnothing\tleft\tfor\tthe\tlower\tlayers.",
      "content_length": 2769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "Figure\t11-1.\tLogistic\tactivation\tfunction\tsaturation",
      "content_length": 52,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "Xavier\tand\tHe\tInitialization In\ttheir\tpaper,\tGlorot\tand\tBengio\tpropose\ta\tway\tto\tsignificantly\talleviate\tthis\tproblem.\tWe\tneed\tthe signal\tto\tflow\tproperly\tin\tboth\tdirections:\tin\tthe\tforward\tdirection\twhen\tmaking\tpredictions,\tand\tin\tthe reverse\tdirection\twhen\tbackpropagating\tgradients.\tWe\tdon’t\twant\tthe\tsignal\tto\tdie\tout,\tnor\tdo\twe\twant\tit to\texplode\tand\tsaturate.\tFor\tthe\tsignal\tto\tflow\tproperly,\tthe\tauthors\targue\tthat\twe\tneed\tthe\tvariance\tof\tthe outputs\tof\teach\tlayer\tto\tbe\tequal\tto\tthe\tvariance\tof\tits\tinputs,2\tand\twe\talso\tneed\tthe\tgradients\tto\thave equal\tvariance\tbefore\tand\tafter\tflowing\tthrough\ta\tlayer\tin\tthe\treverse\tdirection\t(please\tcheck\tout\tthe paper\tif\tyou\tare\tinterested\tin\tthe\tmathematical\tdetails).\tIt\tis\tactually\tnot\tpossible\tto\tguarantee\tboth\tunless the\tlayer\thas\tan\tequal\tnumber\tof\tinput\tand\toutput\tconnections,\tbut\tthey\tproposed\ta\tgood\tcompromise\tthat has\tproven\tto\twork\tvery\twell\tin\tpractice:\tthe\tconnection\tweights\tmust\tbe\tinitialized\trandomly\tas described\tin\tEquation\t11-1,\twhere\tninputs\tand\tnoutputs\tare\tthe\tnumber\tof\tinput\tand\toutput\tconnections\tfor the\tlayer\twhose\tweights\tare\tbeing\tinitialized\t(also\tcalled\tfan-in\tand\tfan-out).\tThis\tinitialization\tstrategy is\toften\tcalled\tXavier\tinitialization\t(after\tthe\tauthor’s\tfirst\tname),\tor\tsometimes\tGlorot\tinitialization.\n\nEquation\t11-1.\tXavier\tinitialization\t(when\tusing\tthe\tlogistic\tactivation\tfunction)\n\nWhen\tthe\tnumber\tof\tinput\tconnections\tis\troughly\tequal\tto\tthe\tnumber\tof\toutput\tconnections,\tyou\tget\n\nsimpler\tequations\t(e.g.,\t Chapter\t10.3\n\nor\n\n).\tWe\tused\tthis\tsimplified\tstrategy\tin\n\nUsing\tthe\tXavier\tinitialization\tstrategy\tcan\tspeed\tup\ttraining\tconsiderably,\tand\tit\tis\tone\tof\tthe\ttricks\tthat led\tto\tthe\tcurrent\tsuccess\tof\tDeep\tLearning.\tSome\trecent\tpapers4\thave\tprovided\tsimilar\tstrategies\tfor different\tactivation\tfunctions,\tas\tshown\tin\tTable\t11-1.\tThe\tinitialization\tstrategy\tfor\tthe\tReLU\tactivation function\t(and\tits\tvariants,\tincluding\tthe\tELU\tactivation\tdescribed\tshortly)\tis\tsometimes\tcalled\tHe initialization\t(after\tthe\tlast\tname\tof\tits\tauthor).\n\nTable\t11-1.\tInitialization\tparameters\tfor\teach\ttype\tof activation\tfunction\n\nActivation\tfunction\n\nUniform\tdistribution\t[–r,\tr] Normal\tdistribution\n\nLogistic\n\nHyperbolic\ttangent\n\nReLU\t(and\tits\tvariants)\n\nBy\tdefault,\tthe\ttf.layers.dense()\tfunction\t(introduced\tin\tChapter\t10)\tuses\tXavier\tinitialization\t(with a\tuniform\tdistribution).\tYou\tcan\tchange\tthis\tto\tHe\tinitialization\tby\tusing\tthe",
      "content_length": 2415,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "variance_scaling_initializer()\tfunction\tlike\tthis:\n\nhe_init\t=\ttf.contrib.layers.variance_scaling_initializer() hidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=he_init,\tname=\"hidden1\")\n\nNOTE\n\nHe\tinitialization\tconsiders\tonly\tthe\tfan-in,\tnot\tthe\taverage\tbetween\tfan-in\tand\tfan-out\tlike\tin\tXavier\tinitialization.\tThis\tis\talso\tthe default\tfor\tthe\tvariance_scaling_initializer()\tfunction,\tbut\tyou\tcan\tchange\tthis\tby\tsetting\tthe\targument\tmode=\"FAN_AVG\".",
      "content_length": 507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "Nonsaturating\tActivation\tFunctions One\tof\tthe\tinsights\tin\tthe\t2010\tpaper\tby\tGlorot\tand\tBengio\twas\tthat\tthe\tvanishing/exploding\tgradients problems\twere\tin\tpart\tdue\tto\ta\tpoor\tchoice\tof\tactivation\tfunction.\tUntil\tthen\tmost\tpeople\thad\tassumed that\tif\tMother\tNature\thad\tchosen\tto\tuse\troughly\tsigmoid\tactivation\tfunctions\tin\tbiological\tneurons,\tthey must\tbe\tan\texcellent\tchoice.\tBut\tit\tturns\tout\tthat\tother\tactivation\tfunctions\tbehave\tmuch\tbetter\tin\tdeep neural\tnetworks,\tin\tparticular\tthe\tReLU\tactivation\tfunction,\tmostly\tbecause\tit\tdoes\tnot\tsaturate\tfor positive\tvalues\t(and\talso\tbecause\tit\tis\tquite\tfast\tto\tcompute).\n\nUnfortunately,\tthe\tReLU\tactivation\tfunction\tis\tnot\tperfect.\tIt\tsuffers\tfrom\ta\tproblem\tknown\tas\tthe\tdying ReLUs:\tduring\ttraining,\tsome\tneurons\teffectively\tdie,\tmeaning\tthey\tstop\toutputting\tanything\tother\tthan\t0. In\tsome\tcases,\tyou\tmay\tfind\tthat\thalf\tof\tyour\tnetwork’s\tneurons\tare\tdead,\tespecially\tif\tyou\tused\ta\tlarge learning\trate.\tDuring\ttraining,\tif\ta\tneuron’s\tweights\tget\tupdated\tsuch\tthat\tthe\tweighted\tsum\tof\tthe\tneuron’s inputs\tis\tnegative,\tit\twill\tstart\toutputting\t0.\tWhen\tthis\thappen,\tthe\tneuron\tis\tunlikely\tto\tcome\tback\tto\tlife since\tthe\tgradient\tof\tthe\tReLU\tfunction\tis\t0\twhen\tits\tinput\tis\tnegative.\n\nTo\tsolve\tthis\tproblem,\tyou\tmay\twant\tto\tuse\ta\tvariant\tof\tthe\tReLU\tfunction,\tsuch\tas\tthe\tleaky\tReLU.\tThis function\tis\tdefined\tas\tLeakyReLUα(z)\t=\tmax(αz,\tz)\t(see\tFigure\t11-2).\tThe\thyperparameter\tα\tdefines\thow much\tthe\tfunction\t“leaks”:\tit\tis\tthe\tslope\tof\tthe\tfunction\tfor\tz\t<\t0,\tand\tis\ttypically\tset\tto\t0.01.\tThis\tsmall slope\tensures\tthat\tleaky\tReLUs\tnever\tdie;\tthey\tcan\tgo\tinto\ta\tlong\tcoma,\tbut\tthey\thave\ta\tchance\tto eventually\twake\tup.\tA\trecent\tpaper5\tcompared\tseveral\tvariants\tof\tthe\tReLU\tactivation\tfunction\tand\tone of\tits\tconclusions\twas\tthat\tthe\tleaky\tvariants\talways\toutperformed\tthe\tstrict\tReLU\tactivation\tfunction.\tIn fact,\tsetting\tα\t=\t0.2\t(huge\tleak)\tseemed\tto\tresult\tin\tbetter\tperformance\tthan\tα\t=\t0.01\t(small\tleak).\tThey also\tevaluated\tthe\trandomized\tleaky\tReLU\t(RReLU),\twhere\tα\tis\tpicked\trandomly\tin\ta\tgiven\trange\tduring training,\tand\tit\tis\tfixed\tto\tan\taverage\tvalue\tduring\ttesting.\tIt\talso\tperformed\tfairly\twell\tand\tseemed\tto\tact as\ta\tregularizer\t(reducing\tthe\trisk\tof\toverfitting\tthe\ttraining\tset).\tFinally,\tthey\talso\tevaluated\tthe parametric\tleaky\tReLU\t(PReLU),\twhere\tα\tis\tauthorized\tto\tbe\tlearned\tduring\ttraining\t(instead\tof\tbeing\ta hyperparameter,\tit\tbecomes\ta\tparameter\tthat\tcan\tbe\tmodified\tby\tbackpropagation\tlike\tany\tother parameter).\tThis\twas\treported\tto\tstrongly\toutperform\tReLU\ton\tlarge\timage\tdatasets,\tbut\ton\tsmaller datasets\tit\truns\tthe\trisk\tof\toverfitting\tthe\ttraining\tset.",
      "content_length": 2621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "Figure\t11-2.\tLeaky\tReLU\n\nLast\tbut\tnot\tleast,\ta\t2015\tpaper\tby\tDjork-Arné\tClevert\tet\tal.6\tproposed\ta\tnew\tactivation\tfunction\tcalled the\texponential\tlinear\tunit\t(ELU)\tthat\toutperformed\tall\tthe\tReLU\tvariants\tin\ttheir\texperiments:\ttraining time\twas\treduced\tand\tthe\tneural\tnetwork\tperformed\tbetter\ton\tthe\ttest\tset.\tIt\tis\trepresented\tin\tFigure\t11-3, and\tEquation\t11-2\tshows\tits\tdefinition.\n\nEquation\t11-2.\tELU\tactivation\tfunction",
      "content_length": 422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "Figure\t11-3.\tELU\tactivation\tfunction\n\nIt\tlooks\ta\tlot\tlike\tthe\tReLU\tfunction,\twith\ta\tfew\tmajor\tdifferences:\n\nFirst\tit\ttakes\ton\tnegative\tvalues\twhen\tz\t<\t0,\twhich\tallows\tthe\tunit\tto\thave\tan\taverage\toutput\tcloser to\t0.\tThis\thelps\talleviate\tthe\tvanishing\tgradients\tproblem,\tas\tdiscussed\tearlier.\tThe\thyperparameter α\tdefines\tthe\tvalue\tthat\tthe\tELU\tfunction\tapproaches\twhen\tz\tis\ta\tlarge\tnegative\tnumber.\tIt\tis\tusually set\tto\t1,\tbut\tyou\tcan\ttweak\tit\tlike\tany\tother\thyperparameter\tif\tyou\twant.\n\nSecond,\tit\thas\ta\tnonzero\tgradient\tfor\tz\t<\t0,\twhich\tavoids\tthe\tdying\tunits\tissue.\n\nThird,\tthe\tfunction\tis\tsmooth\teverywhere,\tincluding\taround\tz\t=\t0,\twhich\thelps\tspeed\tup\tGradient Descent,\tsince\tit\tdoes\tnot\tbounce\tas\tmuch\tleft\tand\tright\tof\tz\t=\t0.\n\nThe\tmain\tdrawback\tof\tthe\tELU\tactivation\tfunction\tis\tthat\tit\tis\tslower\tto\tcompute\tthan\tthe\tReLU\tand\tits variants\t(due\tto\tthe\tuse\tof\tthe\texponential\tfunction),\tbut\tduring\ttraining\tthis\tis\tcompensated\tby\tthe\tfaster convergence\trate.\tHowever,\tat\ttest\ttime\tan\tELU\tnetwork\twill\tbe\tslower\tthan\ta\tReLU\tnetwork.\n\nTIP\n\nSo\twhich\tactivation\tfunction\tshould\tyou\tuse\tfor\tthe\thidden\tlayers\tof\tyour\tdeep\tneural\tnetworks?\tAlthough\tyour\tmileage\twill\tvary, in\tgeneral\tELU\t>\tleaky\tReLU\t(and\tits\tvariants)\t>\tReLU\t>\ttanh\t>\tlogistic.\tIf\tyou\tcare\ta\tlot\tabout\truntime\tperformance,\tthen\tyou may\tprefer\tleaky\tReLUs\tover\tELUs.\tIf\tyou\tdon’t\twant\tto\ttweak\tyet\tanother\thyperparameter,\tyou\tmay\tjust\tuse\tthe\tdefault\tα values\tsuggested\tearlier\t(0.01\tfor\tthe\tleaky\tReLU,\tand\t1\tfor\tELU).\tIf\tyou\thave\tspare\ttime\tand\tcomputing\tpower,\tyou\tcan\tuse cross-validation\tto\tevaluate\tother\tactivation\tfunctions,\tin\tparticular\tRReLU\tif\tyour\tnetwork\tis\toverfitting,\tor\tPReLU\tif\tyou\thave\ta huge\ttraining\tset.\n\nTensorFlow\toffers\tan\telu()\tfunction\tthat\tyou\tcan\tuse\tto\tbuild\tyour\tneural\tnetwork.\tSimply\tset\tthe activation\targument\twhen\tcalling\tthe\tdense()\tfunction,\tlike\tthis:",
      "content_length": 1856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "hidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.elu,\tname=\"hidden1\")\n\nTensorFlow\tdoes\tnot\thave\ta\tpredefined\tfunction\tfor\tleaky\tReLUs,\tbut\tit\tis\teasy\tenough\tto\tdefine:\n\ndef\tleaky_relu(z,\tname=None): \t\t\t\treturn\ttf.maximum(0.01\t*\tz,\tz,\tname=name)\n\nhidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=leaky_relu,\tname=\"hidden1\")",
      "content_length": 332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "Batch\tNormalization Although\tusing\tHe\tinitialization\talong\twith\tELU\t(or\tany\tvariant\tof\tReLU)\tcan\tsignificantly\treduce\tthe vanishing/exploding\tgradients\tproblems\tat\tthe\tbeginning\tof\ttraining,\tit\tdoesn’t\tguarantee\tthat\tthey\twon’t come\tback\tduring\ttraining. In\ta\t2015\tpaper,7\tSergey\tIoffe\tand\tChristian\tSzegedy\tproposed\ta\ttechnique\tcalled\tBatch\tNormalization (BN)\tto\taddress\tthe\tvanishing/exploding\tgradients\tproblems,\tand\tmore\tgenerally\tthe\tproblem\tthat\tthe distribution\tof\teach\tlayer’s\tinputs\tchanges\tduring\ttraining,\tas\tthe\tparameters\tof\tthe\tprevious\tlayers\tchange (which\tthey\tcall\tthe\tInternal\tCovariate\tShift\tproblem).\n\nThe\ttechnique\tconsists\tof\tadding\tan\toperation\tin\tthe\tmodel\tjust\tbefore\tthe\tactivation\tfunction\tof\teach layer,\tsimply\tzero-centering\tand\tnormalizing\tthe\tinputs,\tthen\tscaling\tand\tshifting\tthe\tresult\tusing\ttwo\tnew parameters\tper\tlayer\t(one\tfor\tscaling,\tthe\tother\tfor\tshifting).\tIn\tother\twords,\tthis\toperation\tlets\tthe\tmodel learn\tthe\toptimal\tscale\tand\tmean\tof\tthe\tinputs\tfor\teach\tlayer.\n\nIn\torder\tto\tzero-center\tand\tnormalize\tthe\tinputs,\tthe\talgorithm\tneeds\tto\testimate\tthe\tinputs’\tmean\tand standard\tdeviation.\tIt\tdoes\tso\tby\tevaluating\tthe\tmean\tand\tstandard\tdeviation\tof\tthe\tinputs\tover\tthe\tcurrent mini-batch\t(hence\tthe\tname\t“Batch\tNormalization”).\tThe\twhole\toperation\tis\tsummarized\tin\tEquation\t11- 3.\n\nEquation\t11-3.\tBatch\tNormalization\talgorithm\n\nμB\tis\tthe\tempirical\tmean,\tevaluated\tover\tthe\twhole\tmini-batch\tB.",
      "content_length": 1433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "σB\tis\tthe\tempirical\tstandard\tdeviation,\talso\tevaluated\tover\tthe\twhole\tmini-batch.\n\nmB\tis\tthe\tnumber\tof\tinstances\tin\tthe\tmini-batch.\n\n(i)\tis\tthe\tzero-centered\tand\tnormalized\tinput.\n\nγ\tis\tthe\tscaling\tparameter\tfor\tthe\tlayer.\n\nβ\tis\tthe\tshifting\tparameter\t(offset)\tfor\tthe\tlayer.\n\nϵ\tis\ta\ttiny\tnumber\tto\tavoid\tdivision\tby\tzero\t(typically\t10–5).\tThis\tis\tcalled\ta\tsmoothing\tterm.\n\nz(i)\tis\tthe\toutput\tof\tthe\tBN\toperation:\tit\tis\ta\tscaled\tand\tshifted\tversion\tof\tthe\tinputs.\n\nAt\ttest\ttime,\tthere\tis\tno\tmini-batch\tto\tcompute\tthe\tempirical\tmean\tand\tstandard\tdeviation,\tso\tinstead\tyou simply\tuse\tthe\twhole\ttraining\tset’s\tmean\tand\tstandard\tdeviation.\tThese\tare\ttypically\tefficiently\tcomputed during\ttraining\tusing\ta\tmoving\taverage.\tSo,\tin\ttotal,\tfour\tparameters\tare\tlearned\tfor\teach\tbatch- normalized\tlayer:\tγ\t(scale),\tβ\t(offset),\tμ\t(mean),\tand\tσ\t(standard\tdeviation).\n\nThe\tauthors\tdemonstrated\tthat\tthis\ttechnique\tconsiderably\timproved\tall\tthe\tdeep\tneural\tnetworks\tthey experimented\twith.\tThe\tvanishing\tgradients\tproblem\twas\tstrongly\treduced,\tto\tthe\tpoint\tthat\tthey\tcould\tuse saturating\tactivation\tfunctions\tsuch\tas\tthe\ttanh\tand\teven\tthe\tlogistic\tactivation\tfunction.\tThe\tnetworks were\talso\tmuch\tless\tsensitive\tto\tthe\tweight\tinitialization.\tThey\twere\table\tto\tuse\tmuch\tlarger\tlearning rates,\tsignificantly\tspeeding\tup\tthe\tlearning\tprocess.\tSpecifically,\tthey\tnote\tthat\t“Applied\tto\ta\tstate-of- the-art\timage\tclassification\tmodel,\tBatch\tNormalization\tachieves\tthe\tsame\taccuracy\twith\t14\ttimes\tfewer training\tsteps,\tand\tbeats\tthe\toriginal\tmodel\tby\ta\tsignificant\tmargin.\t[…]\tUsing\tan\tensemble\tof\tbatch- normalized\tnetworks,\twe\timprove\tupon\tthe\tbest\tpublished\tresult\ton\tImageNet\tclassification:\treaching 4.9%\ttop-5\tvalidation\terror\t(and\t4.8%\ttest\terror),\texceeding\tthe\taccuracy\tof\thuman\traters.”\tFinally,\tlike\ta gift\tthat\tkeeps\ton\tgiving,\tBatch\tNormalization\talso\tacts\tlike\ta\tregularizer,\treducing\tthe\tneed\tfor\tother regularization\ttechniques\t(such\tas\tdropout,\tdescribed\tlater\tin\tthe\tchapter).\n\nBatch\tNormalization\tdoes,\thowever,\tadd\tsome\tcomplexity\tto\tthe\tmodel\t(although\tit\tremoves\tthe\tneed\tfor normalizing\tthe\tinput\tdata\tsince\tthe\tfirst\thidden\tlayer\twill\ttake\tcare\tof\tthat,\tprovided\tit\tis\tbatch- normalized).\tMoreover,\tthere\tis\ta\truntime\tpenalty:\tthe\tneural\tnetwork\tmakes\tslower\tpredictions\tdue\tto the\textra\tcomputations\trequired\tat\teach\tlayer.\tSo\tif\tyou\tneed\tpredictions\tto\tbe\tlightning-fast,\tyou\tmay want\tto\tcheck\thow\twell\tplain\tELU\t+\tHe\tinitialization\tperform\tbefore\tplaying\twith\tBatch\tNormalization.\n\nNOTE\n\nYou\tmay\tfind\tthat\ttraining\tis\trather\tslow\tat\tfirst\twhile\tGradient\tDescent\tis\tsearching\tfor\tthe\toptimal\tscales\tand\toffsets\tfor\teach layer,\tbut\tit\taccelerates\tonce\tit\thas\tfound\treasonably\tgood\tvalues.\n\nImplementing\tBatch\tNormalization\twith\tTensorFlow\n\nTensorFlow\tprovides\ta\ttf.nn.batch_normalization()\tfunction\tthat\tsimply\tcenters\tand\tnormalizes\tthe inputs,\tbut\tyou\tmust\tcompute\tthe\tmean\tand\tstandard\tdeviation\tyourself\t(based\ton\tthe\tmini-batch\tdata",
      "content_length": 2940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "during\ttraining\tor\ton\tthe\tfull\tdataset\tduring\ttesting,\tas\tjust\tdiscussed)\tand\tpass\tthem\tas\tparameters\tto\tthis function,\tand\tyou\tmust\talso\thandle\tthe\tcreation\tof\tthe\tscaling\tand\toffset\tparameters\t(and\tpass\tthem\tto\tthis function).\tIt\tis\tdoable,\tbut\tnot\tthe\tmost\tconvenient\tapproach.\tInstead,\tyou\tshould\tuse\tthe tf.layers.batch_normalization()\tfunction,\twhich\thandles\tall\tthis\tfor\tyou,\tas\tin\tthe\tfollowing\tcode:\n\nimport\ttensorflow\tas\ttf\n\nn_inputs\t=\t28\t*\t28 n_hidden1\t=\t300 n_hidden2\t=\t100 n_outputs\t=\t10\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_inputs),\tname=\"X\")\n\ntraining\t=\ttf.placeholder_with_default(False,\tshape=(),\tname='training')\n\nhidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tname=\"hidden1\") bn1\t=\ttf.layers.batch_normalization(hidden1,\ttraining=training,\tmomentum=0.9) bn1_act\t=\ttf.nn.elu(bn1) hidden2\t=\ttf.layers.dense(bn1_act,\tn_hidden2,\tname=\"hidden2\") bn2\t=\ttf.layers.batch_normalization(hidden2,\ttraining=training,\tmomentum=0.9) bn2_act\t=\ttf.nn.elu(bn2) logits_before_bn\t=\ttf.layers.dense(bn2_act,\tn_outputs,\tname=\"outputs\") logits\t=\ttf.layers.batch_normalization(logits_before_bn,\ttraining=training, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=0.9)\n\nLet’s\twalk\tthrough\tthis\tcode.\tThe\tfirst\tlines\tare\tfairly\tself-explanatory,\tuntil\twe\tdefine\tthe\ttraining placeholder:\twe\twill\tset\tit\tto\tTrue\tduring\ttraining,\tbut\totherwise\tit\twill\tdefault\tto\tFalse.\tThis\twill\tbe used\tto\ttell\tthe\ttf.layers.batch_normalization()\tfunction\twhether\tit\tshould\tuse\tthe\tcurrent\tmini- batch’s\tmean\tand\tstandard\tdeviation\t(during\ttraining)\tor\tthe\twhole\ttraining\tset’s\tmean\tand\tstandard deviation\t(during\ttesting).\n\nThen,\twe\talternate\tfully\tconnected\tlayers\tand\tbatch\tnormalization\tlayers:\tthe\tfully\tconnected\tlayers\tare created\tusing\tthe\ttf.layers.dense()\tfunction,\tjust\tlike\twe\tdid\tin\tChapter\t10.\tNote\tthat\twe\tdon’t specify\tany\tactivation\tfunction\tfor\tthe\tfully\tconnected\tlayers\tbecause\twe\twant\tto\tapply\tthe\tactivation function\tafter\teach\tbatch\tnormalization\tlayer.8\tWe\tcreate\tthe\tbatch\tnormalization\tlayers\tusing\tthe tf.layers.batch_normalization()\tfunction,\tsetting\tits\ttraining\tand\tmomentum\tparameters.\tThe\tBN algorithm\tuses\texponential\tdecay\tto\tcompute\tthe\trunning\taverages,\twhich\tis\twhy\tit\trequires\tthe momentum\tparameter:\tgiven\ta\tnew\tvalue\tv,\tthe\trunning\taverage\n\nis\tupdated\tthrough\tthe\tequation:\n\nA\tgood\tmomentum\tvalue\tis\ttypically\tclose\tto\t1\t—\tfor\texample,\t0.9,\t0.99,\tor\t0.999\t(you\twant\tmore\t9s\tfor larger\tdatasets\tand\tsmaller\tmini-batches).\n\nYou\tmay\thave\tnoticed\tthat\tthe\tcode\tis\tquite\trepetitive,\twith\tthe\tsame\tbatch\tnormalization\tparameters appearing\tover\tand\tover\tagain.\tTo\tavoid\tthis\trepetition,\tyou\tcan\tuse\tthe\tpartial()\tfunction\tfrom\tthe functools\tmodule\t(part\tof\tPython’s\tstandard\tlibrary).\tIt\tcreates\ta\tthin\twrapper\taround\ta\tfunction\tand allows\tyou\tto\tdefine\tdefault\tvalues\tfor\tsome\tparameters.\tThe\tcreation\tof\tthe\tnetwork\tlayers\tin\tthe preceding\tcode\tcan\tbe\tmodified\tlike\tso:\n\nfrom\tfunctools\timport\tpartial\n\nmy_batch_norm_layer\t=\tpartial(tf.layers.batch_normalization,",
      "content_length": 2976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "training=training,\tmomentum=0.9)\n\nhidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tname=\"hidden1\") bn1\t=\tmy_batch_norm_layer(hidden1) bn1_act\t=\ttf.nn.elu(bn1) hidden2\t=\ttf.layers.dense(bn1_act,\tn_hidden2,\tname=\"hidden2\") bn2\t=\tmy_batch_norm_layer(hidden2) bn2_act\t=\ttf.nn.elu(bn2) logits_before_bn\t=\ttf.layers.dense(bn2_act,\tn_outputs,\tname=\"outputs\") logits\t=\tmy_batch_norm_layer(logits_before_bn)\n\nIt\tmay\tnot\tlook\tmuch\tbetter\tthan\tbefore\tin\tthis\tsmall\texample,\tbut\tif\tyou\thave\t10\tlayers\tand\twant\tto\tuse the\tsame\tactivation\tfunction,\tinitializer,\tregularizer,\tand\tso\ton,\tin\tall\tlayers,\tthis\ttrick\twill\tmake\tyour\tcode much\tmore\treadable.\n\nThe\trest\tof\tthe\tconstruction\tphase\tis\tthe\tsame\tas\tin\tChapter\t10:\tdefine\tthe\tcost\tfunction,\tcreate\tan optimizer,\ttell\tit\tto\tminimize\tthe\tcost\tfunction,\tdefine\tthe\tevaluation\toperations,\tcreate\ta\tSaver,\tand\tso\ton.\n\nThe\texecution\tphase\tis\talso\tpretty\tmuch\tthe\tsame,\twith\ttwo\texceptions.\tFirst,\tduring\ttraining,\twhenever you\trun\tan\toperation\tthat\tdepends\ton\tthe\tbatch_normalization()\tlayer,\tyou\tneed\tto\tset\tthe\ttraining placeholder\tto\tTrue.\tSecond,\tthe\tbatch_normalization()\tfunction\tcreates\ta\tfew\toperations\tthat\tmust be\tevaluated\tat\teach\tstep\tduring\ttraining\tin\torder\tto\tupdate\tthe\tmoving\taverages\t(recall\tthat\tthese\tmoving averages\tare\tneeded\tto\tevaluate\tthe\ttraining\tset’s\tmean\tand\tstandard\tdeviation).\tThese\toperations\tare automatically\tadded\tto\tthe\tUPDATE_OPS\tcollection,\tso\tall\twe\tneed\tto\tdo\tis\tget\tthe\tlist\tof\toperations\tin\tthat collection\tand\trun\tthem\tat\teach\ttraining\titeration:\n\nextra_update_ops\t=\ttf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tfor\titeration\tin\trange(mnist.train.num_examples\t//\tbatch_size): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run([training_op,\textra_update_ops], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfeed_dict={training:\tTrue,\tX:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\taccuracy_val\t=\taccuracy.eval(feed_dict={X:\tmnist.test.images, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ty:\tmnist.test.labels}) \t\t\t\t\t\t\t\tprint(epoch,\t\"Test\taccuracy:\",\taccuracy_val)\n\nsave_path\t=\tsaver.save(sess,\t\"./my_model_final.ckpt\")\n\nThat’s\tall!\tIn\tthis\ttiny\texample\twith\tjust\ttwo\tlayers,\tit’s\tunlikely\tthat\tBatch\tNormalization\twill\thave\ta very\tpositive\timpact,\tbut\tfor\tdeeper\tnetworks\tit\tcan\tmake\ta\ttremendous\tdifference.",
      "content_length": 2358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "Gradient\tClipping A\tpopular\ttechnique\tto\tlessen\tthe\texploding\tgradients\tproblem\tis\tto\tsimply\tclip\tthe\tgradients\tduring backpropagation\tso\tthat\tthey\tnever\texceed\tsome\tthreshold\t(this\tis\tmostly\tuseful\tfor\trecurrent\tneural networks;\tsee\tChapter\t14).\tThis\tis\tcalled\tGradient\tClipping.9\tIn\tgeneral\tpeople\tnow\tprefer\tBatch Normalization,\tbut\tit’s\tstill\tuseful\tto\tknow\tabout\tGradient\tClipping\tand\thow\tto\timplement\tit.\n\nIn\tTensorFlow,\tthe\toptimizer’s\tminimize()\tfunction\ttakes\tcare\tof\tboth\tcomputing\tthe\tgradients\tand applying\tthem,\tso\tyou\tmust\tinstead\tcall\tthe\toptimizer’s\tcompute_gradients()\tmethod\tfirst,\tthen\tcreate an\toperation\tto\tclip\tthe\tgradients\tusing\tthe\tclip_by_value()\tfunction,\tand\tfinally\tcreate\tan\toperation\tto apply\tthe\tclipped\tgradients\tusing\tthe\toptimizer’s\tapply_gradients()\tmethod:\n\nthreshold\t=\t1.0 optimizer\t=\ttf.train.GradientDescentOptimizer(learning_rate) grads_and_vars\t=\toptimizer.compute_gradients(loss) capped_gvs\t=\t[(tf.clip_by_value(grad,\t-threshold,\tthreshold),\tvar) \t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tgrad,\tvar\tin\tgrads_and_vars] training_op\t=\toptimizer.apply_gradients(capped_gvs)\n\nYou\twould\tthen\trun\tthis\ttraining_op\tat\tevery\ttraining\tstep,\tas\tusual.\tIt\twill\tcompute\tthe\tgradients,\tclip them\tbetween\t–1.0\tand\t1.0,\tand\tapply\tthem.\tThe\tthreshold\tis\ta\thyperparameter\tyou\tcan\ttune.",
      "content_length": 1286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "Reusing\tPretrained\tLayers It\tis\tgenerally\tnot\ta\tgood\tidea\tto\ttrain\ta\tvery\tlarge\tDNN\tfrom\tscratch:\tinstead,\tyou\tshould\talways\ttry\tto find\tan\texisting\tneural\tnetwork\tthat\taccomplishes\ta\tsimilar\ttask\tto\tthe\tone\tyou\tare\ttrying\tto\ttackle,\tthen just\treuse\tthe\tlower\tlayers\tof\tthis\tnetwork:\tthis\tis\tcalled\ttransfer\tlearning.\tIt\twill\tnot\tonly\tspeed\tup training\tconsiderably,\tbut\twill\talso\trequire\tmuch\tless\ttraining\tdata.\n\nFor\texample,\tsuppose\tthat\tyou\thave\taccess\tto\ta\tDNN\tthat\twas\ttrained\tto\tclassify\tpictures\tinto\t100 different\tcategories,\tincluding\tanimals,\tplants,\tvehicles,\tand\teveryday\tobjects.\tYou\tnow\twant\tto\ttrain\ta DNN\tto\tclassify\tspecific\ttypes\tof\tvehicles.\tThese\ttasks\tare\tvery\tsimilar,\tso\tyou\tshould\ttry\tto\treuse\tparts of\tthe\tfirst\tnetwork\t(see\tFigure\t11-4).\n\nFigure\t11-4.\tReusing\tpretrained\tlayers\n\nNOTE\n\nIf\tthe\tinput\tpictures\tof\tyour\tnew\ttask\tdon’t\thave\tthe\tsame\tsize\tas\tthe\tones\tused\tin\tthe\toriginal\ttask,\tyou\twill\thave\tto\tadd\ta preprocessing\tstep\tto\tresize\tthem\tto\tthe\tsize\texpected\tby\tthe\toriginal\tmodel.\tMore\tgenerally,\ttransfer\tlearning\twill\tonly\twork\twell if\tthe\tinputs\thave\tsimilar\tlow-level\tfeatures.",
      "content_length": 1116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "Reusing\ta\tTensorFlow\tModel If\tthe\toriginal\tmodel\twas\ttrained\tusing\tTensorFlow,\tyou\tcan\tsimply\trestore\tit\tand\ttrain\tit\ton\tthe\tnew\ttask. As\twe\tdiscussed\tin\tChapter\t9,\tyou\tcan\tuse\tthe\timport_meta_graph()\tfunction\tto\timport\tthe\toperations into\tthe\tdefault\tgraph.\tThis\treturns\ta\tSaver\tthat\tyou\tcan\tlater\tuse\tto\tload\tthe\tmodel’s\tstate:\n\nsaver\t=\ttf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n\nYou\tmust\tthen\tget\ta\thandle\ton\tthe\toperations\tand\ttensors\tyou\twill\tneed\tfor\ttraining.\tFor\tthis,\tyou\tcan\tuse the\tgraph’s\tget_operation_by_name()\tand\tget_tensor_by_name()\tmethods.\tThe\tname\tof\ta\ttensor\tis the\tname\tof\tthe\toperation\tthat\toutputs\tit\tfollowed\tby\t:0\t(or\t:1\tif\tit\tis\tthe\tsecond\toutput,\t:2\tif\tit\tis\tthe third,\tand\tso\ton):\n\nX\t=\ttf.get_default_graph().get_tensor_by_name(\"X:0\") y\t=\ttf.get_default_graph().get_tensor_by_name(\"y:0\") accuracy\t=\ttf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\") training_op\t=\ttf.get_default_graph().get_operation_by_name(\"GradientDescent\")\n\nIf\tthe\tpretrained\tmodel\tis\tnot\twell\tdocumented,\tthen\tyou\twill\thave\tto\texplore\tthe\tgraph\tto\tfind\tthe\tnames of\tthe\toperations\tyou\twill\tneed.\tIn\tthis\tcase,\tyou\tcan\teither\texplore\tthe\tgraph\tusing\tTensorBoard\t(for\tthis you\tmust\tfirst\texport\tthe\tgraph\tusing\ta\tFileWriter,\tas\tdiscussed\tin\tChapter\t9),\tor\tyou\tcan\tuse\tthe\tgraph’s get_operations()\tmethod\tto\tlist\tall\tthe\toperations:\n\nfor\top\tin\ttf.get_default_graph().get_operations(): \t\t\t\tprint(op.name)\n\nIf\tyou\tare\tthe\tauthor\tof\tthe\toriginal\tmodel,\tyou\tcould\tmake\tthings\teasier\tfor\tpeople\twho\twill\treuse\tyour model\tby\tgiving\toperations\tvery\tclear\tnames\tand\tdocumenting\tthem.\tAnother\tapproach\tis\tto\tcreate\ta collection\tcontaining\tall\tthe\timportant\toperations\tthat\tpeople\twill\twant\tto\tget\ta\thandle\ton:\n\nfor\top\tin\t(X,\ty,\taccuracy,\ttraining_op): \t\t\t\ttf.add_to_collection(\"my_important_ops\",\top)\n\nThis\tway\tpeople\twho\treuse\tyour\tmodel\twill\tbe\table\tto\tsimply\twrite:\n\nX,\ty,\taccuracy,\ttraining_op\t=\ttf.get_collection(\"my_important_ops\")\n\nYou\tcan\tthen\trestore\tthe\tmodel’s\tstate\tusing\tthe\tSaver\tand\tcontinue\ttraining\tusing\tyour\town\tdata:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsaver.restore(sess,\t\"./my_model_final.ckpt\") \t\t\t\t[...]\t#\ttrain\tthe\tmodel\ton\tyour\town\tdata\n\nAlternatively,\tif\tyou\thave\taccess\tto\tthe\tPython\tcode\tthat\tbuilt\tthe\toriginal\tgraph,\tyou\tcan\tuse\tit\tinstead\tof import_meta_graph().\n\nIn\tgeneral,\tyou\twill\twant\tto\treuse\tonly\tpart\tof\tthe\toriginal\tmodel,\ttypically\tthe\tlower\tlayers.\tIf\tyou\tuse import_meta_graph()\tto\trestore\tthe\tgraph,\tit\twill\tload\tthe\tentire\toriginal\tgraph,\tbut\tnothing\tprevents",
      "content_length": 2515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "you\tfrom\tjust\tignoring\tthe\tlayers\tyou\tdo\tnot\tcare\tabout.\tFor\texample,\tas\tshown\tin\tFigure\t11-4,\tyou\tcould build\tnew\tlayers\t(e.g.,\tone\thidden\tlayer\tand\tone\toutput\tlayer)\ton\ttop\tof\ta\tpretrained\tlayer\t(e.g.,\tpretrained hidden\tlayer\t3).\tYou\twould\talso\tneed\tto\tcompute\tthe\tloss\tfor\tthis\tnew\toutput,\tand\tcreate\tan\toptimizer\tto minimize\tthat\tloss.\n\nIf\tyou\thave\taccess\tto\tthe\tpretrained\tgraph’s\tPython\tcode,\tyou\tcan\tjust\treuse\tthe\tparts\tyou\tneed\tand\tchop out\tthe\trest.\tHowever,\tin\tthis\tcase\tyou\tneed\ta\tSaver\tto\trestore\tthe\tpretrained\tmodel\t(specifying\twhich variables\tyou\twant\tto\trestore;\totherwise,\tTensorFlow\twill\tcomplain\tthat\tthe\tgraphs\tdo\tnot\tmatch),\tand another\tSaver\tto\tsave\tthe\tnew\tmodel.\tFor\texample,\tthe\tfollowing\tcode\trestores\tonly\thidden\tlayers\t1,\t2, and\t3:\n\n[...]\t#\tbuild\tthe\tnew\tmodel\twith\tthe\tsame\thidden\tlayers\t1-3\tas\tbefore\n\nreuse_vars\t=\ttf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscope=\"hidden[123]\")\t#\tregular\texpression reuse_vars_dict\t=\tdict([(var.op.name,\tvar)\tfor\tvar\tin\treuse_vars]) restore_saver\t=\ttf.train.Saver(reuse_vars_dict)\t#\tto\trestore\tlayers\t1-3\n\ninit\t=\ttf.global_variables_initializer()\t#\tto\tinit\tall\tvariables,\told\tand\tnew saver\t=\ttf.train.Saver()\t#\tto\tsave\tthe\tnew\tmodel\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\trestore_saver.restore(sess,\t\"./my_model_final.ckpt\") \t\t\t\t[...]\t#\ttrain\tthe\tmodel \t\t\t\tsave_path\t=\tsaver.save(sess,\t\"./my_new_model_final.ckpt\")\n\nFirst\twe\tbuild\tthe\tnew\tmodel,\tmaking\tsure\tto\tcopy\tthe\toriginal\tmodel’s\thidden\tlayers\t1\tto\t3.\tThen\twe\tget the\tlist\tof\tall\tvariables\tin\thidden\tlayers\t1\tto\t3,\tusing\tthe\tregular\texpression\t\"hidden[123]\".\tNext,\twe create\ta\tdictionary\tthat\tmaps\tthe\tname\tof\teach\tvariable\tin\tthe\toriginal\tmodel\tto\tits\tname\tin\tthe\tnew\tmodel (generally\tyou\twant\tto\tkeep\tthe\texact\tsame\tnames).\tThen\twe\tcreate\ta\tSaver\tthat\twill\trestore\tonly\tthese variables.\tWe\talso\tcreate\tan\toperation\tto\tinitialize\tall\tthe\tvariables\t(old\tand\tnew)\tand\ta\tsecond\tSaver\tto save\tthe\tentire\tnew\tmodel,\tnot\tjust\tlayers\t1\tto\t3.\tWe\tthen\tstart\ta\tsession\tand\tinitialize\tall\tvariables\tin\tthe model,\tthen\trestore\tthe\tvariable\tvalues\tfrom\tthe\toriginal\tmodel’s\tlayers\t1\tto\t3.\tFinally,\twe\ttrain\tthe\tmodel on\tthe\tnew\ttask\tand\tsave\tit.\n\nTIP\n\nThe\tmore\tsimilar\tthe\ttasks\tare,\tthe\tmore\tlayers\tyou\twant\tto\treuse\t(starting\twith\tthe\tlower\tlayers).\tFor\tvery\tsimilar\ttasks,\tyou\tcan try\tkeeping\tall\tthe\thidden\tlayers\tand\tjust\treplace\tthe\toutput\tlayer.",
      "content_length": 2407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "Reusing\tModels\tfrom\tOther\tFrameworks If\tthe\tmodel\twas\ttrained\tusing\tanother\tframework,\tyou\twill\tneed\tto\tload\tthe\tmodel\tparameters\tmanually (e.g.,\tusing\tTheano\tcode\tif\tit\twas\ttrained\twith\tTheano),\tthen\tassign\tthem\tto\tthe\tappropriate\tvariables. This\tcan\tbe\tquite\ttedious.\tFor\texample,\tthe\tfollowing\tcode\tshows\thow\tyou\twould\tcopy\tthe\tweight\tand biases\tfrom\tthe\tfirst\thidden\tlayer\tof\ta\tmodel\ttrained\tusing\tanother\tframework:\n\noriginal_w\t=\t[...]\t#\tLoad\tthe\tweights\tfrom\tthe\tother\tframework original_b\t=\t[...]\t#\tLoad\tthe\tbiases\tfrom\tthe\tother\tframework\n\nX\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_inputs),\tname=\"X\") hidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.relu,\tname=\"hidden1\") [...]\t#\tBuild\tthe\trest\tof\tthe\tmodel\n\n#\tGet\ta\thandle\ton\tthe\tassignment\tnodes\tfor\tthe\thidden1\tvariables graph\t=\ttf.get_default_graph() assign_kernel\t=\tgraph.get_operation_by_name(\"hidden1/kernel/Assign\") assign_bias\t=\tgraph.get_operation_by_name(\"hidden1/bias/Assign\") init_kernel\t=\tassign_kernel.inputs[1] init_bias\t=\tassign_bias.inputs[1]\n\ninit\t=\ttf.global_variables_initializer()\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tsess.run(init,\tfeed_dict={init_kernel:\toriginal_w,\tinit_bias:\toriginal_b}) \t\t\t\t#\t[...]\tTrain\tthe\tmodel\ton\tyour\tnew\ttask\n\nIn\tthis\timplementation,\twe\tfirst\tload\tthe\tpretrained\tmodel\tusing\tthe\tother\tframework\t(not\tshown\there), and\twe\textract\tfrom\tit\tthe\tmodel\tparameters\twe\twant\tto\treuse.\tNext,\twe\tbuild\tour\tTensorFlow\tmodel\tas usual.\tThen\tcomes\tthe\ttricky\tpart:\tevery\tTensorFlow\tvariable\thas\tan\tassociated\tassignment\toperation\tthat is\tused\tto\tinitialize\tit.\tWe\tstart\tby\tgetting\ta\thandle\ton\tthese\tassignment\toperations\t(they\thave\tthe\tsame name\tas\tthe\tvariable,\tplus\t\"/Assign\").\tWe\talso\tget\ta\thandle\ton\teach\tassignment\toperation’s\tsecond input:\tin\tthe\tcase\tof\tan\tassignment\toperation,\tthe\tsecond\tinput\tcorresponds\tto\tthe\tvalue\tthat\twill\tbe assigned\tto\tthe\tvariable,\tso\tin\tthis\tcase\tit\tis\tthe\tvariable’s\tinitialization\tvalue.\tOnce\twe\tstart\tthe\tsession, we\trun\tthe\tusual\tinitialization\toperation,\tbut\tthis\ttime\twe\tfeed\tit\tthe\tvalues\twe\twant\tfor\tthe\tvariables\twe want\tto\treuse.\tAlternatively,\twe\tcould\thave\tcreated\tnew\tassignment\toperations\tand\tplaceholders,\tand used\tthem\tto\tset\tthe\tvalues\tof\tthe\tvariables\tafter\tinitialization.\tBut\twhy\tcreate\tnew\tnodes\tin\tthe\tgraph when\teverything\twe\tneed\tis\talready\tthere?",
      "content_length": 2307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "Freezing\tthe\tLower\tLayers It\tis\tlikely\tthat\tthe\tlower\tlayers\tof\tthe\tfirst\tDNN\thave\tlearned\tto\tdetect\tlow-level\tfeatures\tin\tpictures\tthat will\tbe\tuseful\tacross\tboth\timage\tclassification\ttasks,\tso\tyou\tcan\tjust\treuse\tthese\tlayers\tas\tthey\tare.\tIt\tis generally\ta\tgood\tidea\tto\t“freeze”\ttheir\tweights\twhen\ttraining\tthe\tnew\tDNN:\tif\tthe\tlower-layer\tweights are\tfixed,\tthen\tthe\thigher-layer\tweights\twill\tbe\teasier\tto\ttrain\t(because\tthey\twon’t\thave\tto\tlearn\ta\tmoving target).\tTo\tfreeze\tthe\tlower\tlayers\tduring\ttraining,\tone\tsolution\tis\tto\tgive\tthe\toptimizer\tthe\tlist\tof variables\tto\ttrain,\texcluding\tthe\tvariables\tfrom\tthe\tlower\tlayers:\n\ntrain_vars\t=\ttf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscope=\"hidden[34]|outputs\") training_op\t=\toptimizer.minimize(loss,\tvar_list=train_vars)\n\nThe\tfirst\tline\tgets\tthe\tlist\tof\tall\ttrainable\tvariables\tin\thidden\tlayers\t3\tand\t4\tand\tin\tthe\toutput\tlayer.\tThis leaves\tout\tthe\tvariables\tin\tthe\thidden\tlayers\t1\tand\t2.\tNext\twe\tprovide\tthis\trestricted\tlist\tof\ttrainable variables\tto\tthe\toptimizer’s\tminimize()\tfunction.\tTa-da!\tLayers\t1\tand\t2\tare\tnow\tfrozen:\tthey\twill\tnot budge\tduring\ttraining\t(these\tare\toften\tcalled\tfrozen\tlayers).\n\nAnother\toption\tis\tto\tadd\ta\tstop_gradient()\tlayer\tin\tthe\tgraph.\tAny\tlayer\tbelow\tit\twill\tbe\tfrozen:\n\nwith\ttf.name_scope(\"dnn\"): \t\t\t\thidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden1\")\t#\treused\tfrozen \t\t\t\thidden2\t=\ttf.layers.dense(hidden1,\tn_hidden2,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden2\")\t#\treused\tfrozen \t\t\t\thidden2_stop\t=\ttf.stop_gradient(hidden2) \t\t\t\thidden3\t=\ttf.layers.dense(hidden2_stop,\tn_hidden3,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden3\")\t#\treused,\tnot\tfrozen \t\t\t\thidden4\t=\ttf.layers.dense(hidden3,\tn_hidden4,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden4\")\t#\tnew! \t\t\t\tlogits\t=\ttf.layers.dense(hidden4,\tn_outputs,\tname=\"outputs\")\t#\tnew!",
      "content_length": 1970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "Caching\tthe\tFrozen\tLayers Since\tthe\tfrozen\tlayers\twon’t\tchange,\tit\tis\tpossible\tto\tcache\tthe\toutput\tof\tthe\ttopmost\tfrozen\tlayer\tfor\teach training\tinstance.\tSince\ttraining\tgoes\tthrough\tthe\twhole\tdataset\tmany\ttimes,\tthis\twill\tgive\tyou\ta\thuge speed\tboost\tas\tyou\twill\tonly\tneed\tto\tgo\tthrough\tthe\tfrozen\tlayers\tonce\tper\ttraining\tinstance\t(instead\tof once\tper\tepoch).\tFor\texample,\tyou\tcould\tfirst\trun\tthe\twhole\ttraining\tset\tthrough\tthe\tlower\tlayers (assuming\tyou\thave\tenough\tRAM),\tthen\tduring\ttraining,\tinstead\tof\tbuilding\tbatches\tof\ttraining\tinstances, you\twould\tbuild\tbatches\tof\toutputs\tfrom\thidden\tlayer\t2\tand\tfeed\tthem\tto\tthe\ttraining\toperation:\n\nimport\tnumpy\tas\tnp\n\nn_batches\t=\tmnist.train.num_examples\t//\tbatch_size\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\trestore_saver.restore(sess,\t\"./my_model_final.ckpt\")\n\nh2_cache\t=\tsess.run(hidden2,\tfeed_dict={X:\tmnist.train.images})\n\nfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tshuffled_idx\t=\tnp.random.permutation(mnist.train.num_examples) \t\t\t\t\t\t\t\thidden2_batches\t=\tnp.array_split(h2_cache[shuffled_idx],\tn_batches) \t\t\t\t\t\t\t\ty_batches\t=\tnp.array_split(mnist.train.labels[shuffled_idx],\tn_batches) \t\t\t\t\t\t\t\tfor\thidden2_batch,\ty_batch\tin\tzip(hidden2_batches,\ty_batches): \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={hidden2:hidden2_batch,\ty:y_batch})\n\nsave_path\t=\tsaver.save(sess,\t\"./my_new_model_final.ckpt\")\n\nThe\tlast\tline\tof\tthe\ttraining\tloop\truns\tthe\ttraining\toperation\tdefined\tearlier\t(which\tdoes\tnot\ttouch\tlayers\t1 and\t2),\tand\tfeeds\tit\ta\tbatch\tof\toutputs\tfrom\tthe\tsecond\thidden\tlayer\t(as\twell\tas\tthe\ttargets\tfor\tthat\tbatch). Since\twe\tgive\tTensorFlow\tthe\toutput\tof\thidden\tlayer\t2,\tit\tdoes\tnot\ttry\tto\tevaluate\tit\t(or\tany\tnode\tit depends\ton).",
      "content_length": 1683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "Tweaking,\tDropping,\tor\tReplacing\tthe\tUpper\tLayers The\toutput\tlayer\tof\tthe\toriginal\tmodel\tshould\tusually\tbe\treplaced\tsince\tit\tis\tmost\tlikely\tnot\tuseful\tat\tall for\tthe\tnew\ttask,\tand\tit\tmay\tnot\teven\thave\tthe\tright\tnumber\tof\toutputs\tfor\tthe\tnew\ttask.\n\nSimilarly,\tthe\tupper\thidden\tlayers\tof\tthe\toriginal\tmodel\tare\tless\tlikely\tto\tbe\tas\tuseful\tas\tthe\tlower\tlayers, since\tthe\thigh-level\tfeatures\tthat\tare\tmost\tuseful\tfor\tthe\tnew\ttask\tmay\tdiffer\tsignificantly\tfrom\tthe\tones that\twere\tmost\tuseful\tfor\tthe\toriginal\ttask.\tYou\twant\tto\tfind\tthe\tright\tnumber\tof\tlayers\tto\treuse.\n\nTry\tfreezing\tall\tthe\tcopied\tlayers\tfirst,\tthen\ttrain\tyour\tmodel\tand\tsee\thow\tit\tperforms.\tThen\ttry\tunfreezing one\tor\ttwo\tof\tthe\ttop\thidden\tlayers\tto\tlet\tbackpropagation\ttweak\tthem\tand\tsee\tif\tperformance\timproves. The\tmore\ttraining\tdata\tyou\thave,\tthe\tmore\tlayers\tyou\tcan\tunfreeze.\n\nIf\tyou\tstill\tcannot\tget\tgood\tperformance,\tand\tyou\thave\tlittle\ttraining\tdata,\ttry\tdropping\tthe\ttop\thidden layer(s)\tand\tfreeze\tall\tremaining\thidden\tlayers\tagain.\tYou\tcan\titerate\tuntil\tyou\tfind\tthe\tright\tnumber\tof layers\tto\treuse.\tIf\tyou\thave\tplenty\tof\ttraining\tdata,\tyou\tmay\ttry\treplacing\tthe\ttop\thidden\tlayers\tinstead\tof dropping\tthem,\tand\teven\tadd\tmore\thidden\tlayers.",
      "content_length": 1212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "Model\tZoos Where\tcan\tyou\tfind\ta\tneural\tnetwork\ttrained\tfor\ta\ttask\tsimilar\tto\tthe\tone\tyou\twant\tto\ttackle?\tThe\tfirst place\tto\tlook\tis\tobviously\tin\tyour\town\tcatalog\tof\tmodels.\tThis\tis\tone\tgood\treason\tto\tsave\tall\tyour\tmodels and\torganize\tthem\tso\tyou\tcan\tretrieve\tthem\tlater\teasily.\tAnother\toption\tis\tto\tsearch\tin\ta\tmodel\tzoo.\tMany people\ttrain\tMachine\tLearning\tmodels\tfor\tvarious\ttasks\tand\tkindly\trelease\ttheir\tpretrained\tmodels\tto\tthe public.\n\nTensorFlow\thas\tits\town\tmodel\tzoo\tavailable\tat\thttps://github.com/tensorflow/models.\tIn\tparticular,\tit contains\tmost\tof\tthe\tstate-of-the-art\timage\tclassification\tnets\tsuch\tas\tVGG,\tInception,\tand\tResNet\t(see Chapter\t13,\tand\tcheck\tout\tthe\tmodels/slim\tdirectory),\tincluding\tthe\tcode,\tthe\tpretrained\tmodels,\tand\ttools to\tdownload\tpopular\timage\tdatasets.\n\nAnother\tpopular\tmodel\tzoo\tis\tCaffe’s\tModel\tZoo.\tIt\talso\tcontains\tmany\tcomputer\tvision\tmodels\t(e.g., LeNet,\tAlexNet,\tZFNet,\tGoogLeNet,\tVGGNet,\tinception)\ttrained\ton\tvarious\tdatasets\t(e.g.,\tImageNet, Places\tDatabase,\tCIFAR10,\tetc.).\tSaumitro\tDasgupta\twrote\ta\tconverter,\twhich\tis\tavailable\tat https://github.com/ethereon/caffe-tensorflow.",
      "content_length": 1126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "Unsupervised\tPretraining Suppose\tyou\twant\tto\ttackle\ta\tcomplex\ttask\tfor\twhich\tyou\tdon’t\thave\tmuch\tlabeled\ttraining\tdata,\tbut unfortunately\tyou\tcannot\tfind\ta\tmodel\ttrained\ton\ta\tsimilar\ttask.\tDon’t\tlose\tall\thope!\tFirst,\tyou\tshould\tof course\ttry\tto\tgather\tmore\tlabeled\ttraining\tdata,\tbut\tif\tthis\tis\ttoo\thard\tor\ttoo\texpensive,\tyou\tmay\tstill\tbe able\tto\tperform\tunsupervised\tpretraining\t(see\tFigure\t11-5).\tThat\tis,\tif\tyou\thave\tplenty\tof\tunlabeled training\tdata,\tyou\tcan\ttry\tto\ttrain\tthe\tlayers\tone\tby\tone,\tstarting\twith\tthe\tlowest\tlayer\tand\tthen\tgoing\tup, using\tan\tunsupervised\tfeature\tdetector\talgorithm\tsuch\tas\tRestricted\tBoltzmann\tMachines\t(RBMs;\tsee Appendix\tE)\tor\tautoencoders\t(see\tChapter\t15).\tEach\tlayer\tis\ttrained\ton\tthe\toutput\tof\tthe\tpreviously trained\tlayers\t(all\tlayers\texcept\tthe\tone\tbeing\ttrained\tare\tfrozen).\tOnce\tall\tlayers\thave\tbeen\ttrained\tthis way,\tyou\tcan\tfine-tune\tthe\tnetwork\tusing\tsupervised\tlearning\t(i.e.,\twith\tbackpropagation).\n\nThis\tis\ta\trather\tlong\tand\ttedious\tprocess,\tbut\tit\toften\tworks\twell;\tin\tfact,\tit\tis\tthis\ttechnique\tthat\tGeoffrey Hinton\tand\this\tteam\tused\tin\t2006\tand\twhich\tled\tto\tthe\trevival\tof\tneural\tnetworks\tand\tthe\tsuccess\tof\tDeep Learning.\tUntil\t2010,\tunsupervised\tpretraining\t(typically\tusing\tRBMs)\twas\tthe\tnorm\tfor\tdeep\tnets,\tand\tit was\tonly\tafter\tthe\tvanishing\tgradients\tproblem\twas\talleviated\tthat\tit\tbecame\tmuch\tmore\tcommon\tto\ttrain DNNs\tpurely\tusing\tbackpropagation.\tHowever,\tunsupervised\tpretraining\t(today\ttypically\tusing autoencoders\trather\tthan\tRBMs)\tis\tstill\ta\tgood\toption\twhen\tyou\thave\ta\tcomplex\ttask\tto\tsolve,\tno\tsimilar model\tyou\tcan\treuse,\tand\tlittle\tlabeled\ttraining\tdata\tbut\tplenty\tof\tunlabeled\ttraining\tdata.10\n\nFigure\t11-5.\tUnsupervised\tpretraining",
      "content_length": 1701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "Pretraining\ton\tan\tAuxiliary\tTask One\tlast\toption\tis\tto\ttrain\ta\tfirst\tneural\tnetwork\ton\tan\tauxiliary\ttask\tfor\twhich\tyou\tcan\teasily\tobtain\tor generate\tlabeled\ttraining\tdata,\tthen\treuse\tthe\tlower\tlayers\tof\tthat\tnetwork\tfor\tyour\tactual\ttask.\tThe\tfirst neural\tnetwork’s\tlower\tlayers\twill\tlearn\tfeature\tdetectors\tthat\twill\tlikely\tbe\treusable\tby\tthe\tsecond neural\tnetwork.\n\nFor\texample,\tif\tyou\twant\tto\tbuild\ta\tsystem\tto\trecognize\tfaces,\tyou\tmay\tonly\thave\ta\tfew\tpictures\tof\teach individual\t—\tclearly\tnot\tenough\tto\ttrain\ta\tgood\tclassifier.\tGathering\thundreds\tof\tpictures\tof\teach\tperson would\tnot\tbe\tpractical.\tHowever,\tyou\tcould\tgather\ta\tlot\tof\tpictures\tof\trandom\tpeople\ton\tthe\tinternet\tand train\ta\tfirst\tneural\tnetwork\tto\tdetect\twhether\tor\tnot\ttwo\tdifferent\tpictures\tfeature\tthe\tsame\tperson.\tSuch\ta network\twould\tlearn\tgood\tfeature\tdetectors\tfor\tfaces,\tso\treusing\tits\tlower\tlayers\twould\tallow\tyou\tto train\ta\tgood\tface\tclassifier\tusing\tlittle\ttraining\tdata.\n\nIt\tis\toften\trather\tcheap\tto\tgather\tunlabeled\ttraining\texamples,\tbut\tquite\texpensive\tto\tlabel\tthem.\tIn\tthis situation,\ta\tcommon\ttechnique\tis\tto\tlabel\tall\tyour\ttraining\texamples\tas\t“good,”\tthen\tgenerate\tmany\tnew training\tinstances\tby\tcorrupting\tthe\tgood\tones,\tand\tlabel\tthese\tcorrupted\tinstances\tas\t“bad.”\tThen\tyou\tcan train\ta\tfirst\tneural\tnetwork\tto\tclassify\tinstances\tas\tgood\tor\tbad.\tFor\texample,\tyou\tcould\tdownload millions\tof\tsentences,\tlabel\tthem\tas\t“good,”\tthen\trandomly\tchange\ta\tword\tin\teach\tsentence\tand\tlabel\tthe resulting\tsentences\tas\t“bad.”\tIf\ta\tneural\tnetwork\tcan\ttell\tthat\t“The\tdog\tsleeps”\tis\ta\tgood\tsentence\tbut “The\tdog\tthey”\tis\tbad,\tit\tprobably\tknows\tquite\ta\tlot\tabout\tlanguage.\tReusing\tits\tlower\tlayers\twill\tlikely help\tin\tmany\tlanguage\tprocessing\ttasks.\n\nAnother\tapproach\tis\tto\ttrain\ta\tfirst\tnetwork\tto\toutput\ta\tscore\tfor\teach\ttraining\tinstance,\tand\tuse\ta\tcost function\tthat\tensures\tthat\ta\tgood\tinstance’s\tscore\tis\tgreater\tthan\ta\tbad\tinstance’s\tscore\tby\tat\tleast\tsome margin.\tThis\tis\tcalled\tmax\tmargin\tlearning.",
      "content_length": 1977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "Faster\tOptimizers Training\ta\tvery\tlarge\tdeep\tneural\tnetwork\tcan\tbe\tpainfully\tslow.\tSo\tfar\twe\thave\tseen\tfour\tways\tto\tspeed up\ttraining\t(and\treach\ta\tbetter\tsolution):\tapplying\ta\tgood\tinitialization\tstrategy\tfor\tthe\tconnection\tweights, using\ta\tgood\tactivation\tfunction,\tusing\tBatch\tNormalization,\tand\treusing\tparts\tof\ta\tpretrained\tnetwork. Another\thuge\tspeed\tboost\tcomes\tfrom\tusing\ta\tfaster\toptimizer\tthan\tthe\tregular\tGradient\tDescent optimizer.\tIn\tthis\tsection\twe\twill\tpresent\tthe\tmost\tpopular\tones:\tMomentum\toptimization,\tNesterov Accelerated\tGradient,\tAdaGrad,\tRMSProp,\tand\tfinally\tAdam\toptimization.",
      "content_length": 600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "Momentum\tOptimization Imagine\ta\tbowling\tball\trolling\tdown\ta\tgentle\tslope\ton\ta\tsmooth\tsurface:\tit\twill\tstart\tout\tslowly,\tbut\tit will\tquickly\tpick\tup\tmomentum\tuntil\tit\teventually\treaches\tterminal\tvelocity\t(if\tthere\tis\tsome\tfriction\tor air\tresistance).\tThis\tis\tthe\tvery\tsimple\tidea\tbehind\tMomentum\toptimization,\tproposed\tby\tBoris\tPolyak in\t1964.11\tIn\tcontrast,\tregular\tGradient\tDescent\twill\tsimply\ttake\tsmall\tregular\tsteps\tdown\tthe\tslope,\tso\tit will\ttake\tmuch\tmore\ttime\tto\treach\tthe\tbottom.\n\nRecall\tthat\tGradient\tDescent\tsimply\tupdates\tthe\tweights\tθ\tby\tdirectly\tsubtracting\tthe\tgradient\tof\tthe\tcost function\tJ(θ)\twith\tregards\tto\tthe\tweights\t( θJ(θ))\tmultiplied\tby\tthe\tlearning\trate\tη.\tThe\tequation\tis:\tθ\t← θ\t–\tη θJ(θ).\tIt\tdoes\tnot\tcare\tabout\twhat\tthe\tearlier\tgradients\twere.\tIf\tthe\tlocal\tgradient\tis\ttiny,\tit\tgoes very\tslowly.\n\nMomentum\toptimization\tcares\ta\tgreat\tdeal\tabout\twhat\tprevious\tgradients\twere:\tat\teach\titeration,\tit subtracts\tthe\tlocal\tgradient\tfrom\tthe\tmomentum\tvector\tm\t(multiplied\tby\tthe\tlearning\trate\tη),\tand\tit updates\tthe\tweights\tby\tsimply\tadding\tthis\tmomentum\tvector\t(see\tEquation\t11-4).\tIn\tother\twords,\tthe gradient\tis\tused\tas\tan\tacceleration,\tnot\tas\ta\tspeed.\tTo\tsimulate\tsome\tsort\tof\tfriction\tmechanism\tand prevent\tthe\tmomentum\tfrom\tgrowing\ttoo\tlarge,\tthe\talgorithm\tintroduces\ta\tnew\thyperparameter\tβ,\tsimply called\tthe\tmomentum,\twhich\tmust\tbe\tset\tbetween\t0\t(high\tfriction)\tand\t1\t(no\tfriction).\tA\ttypical momentum\tvalue\tis\t0.9.\n\nEquation\t11-4.\tMomentum\talgorithm\n\nYou\tcan\teasily\tverify\tthat\tif\tthe\tgradient\tremains\tconstant,\tthe\tterminal\tvelocity\t(i.e.,\tthe\tmaximum\tsize\tof\n\nthe\tweight\tupdates)\tis\tequal\tto\tthat\tgradient\tmultiplied\tby\tthe\tlearning\trate\tη\tmultiplied\tby\t the\tsign).\tFor\texample,\tif\tβ\t=\t0.9,\tthen\tthe\tterminal\tvelocity\tis\tequal\tto\t10\ttimes\tthe\tgradient\ttimes\tthe learning\trate,\tso\tMomentum\toptimization\tends\tup\tgoing\t10\ttimes\tfaster\tthan\tGradient\tDescent!\tThis allows\tMomentum\toptimization\tto\tescape\tfrom\tplateaus\tmuch\tfaster\tthan\tGradient\tDescent.\tIn\tparticular, we\tsaw\tin\tChapter\t4\tthat\twhen\tthe\tinputs\thave\tvery\tdifferent\tscales\tthe\tcost\tfunction\twill\tlook\tlike\tan elongated\tbowl\t(see\tFigure\t4-7).\tGradient\tDescent\tgoes\tdown\tthe\tsteep\tslope\tquite\tfast,\tbut\tthen\tit\ttakes a\tvery\tlong\ttime\tto\tgo\tdown\tthe\tvalley.\tIn\tcontrast,\tMomentum\toptimization\twill\troll\tdown\tthe\tbottom\tof the\tvalley\tfaster\tand\tfaster\tuntil\tit\treaches\tthe\tbottom\t(the\toptimum).\tIn\tdeep\tneural\tnetworks\tthat\tdon’t use\tBatch\tNormalization,\tthe\tupper\tlayers\twill\toften\tend\tup\thaving\tinputs\twith\tvery\tdifferent\tscales,\tso using\tMomentum\toptimization\thelps\ta\tlot.\tIt\tcan\talso\thelp\troll\tpast\tlocal\toptima.\n\n(ignoring",
      "content_length": 2612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "NOTE\n\nDue\tto\tthe\tmomentum,\tthe\toptimizer\tmay\tovershoot\ta\tbit,\tthen\tcome\tback,\tovershoot\tagain,\tand\toscillate\tlike\tthis\tmany\ttimes before\tstabilizing\tat\tthe\tminimum.\tThis\tis\tone\tof\tthe\treasons\twhy\tit\tis\tgood\tto\thave\ta\tbit\tof\tfriction\tin\tthe\tsystem:\tit\tgets\trid\tof these\toscillations\tand\tthus\tspeeds\tup\tconvergence.\n\nImplementing\tMomentum\toptimization\tin\tTensorFlow\tis\ta\tno-brainer:\tjust\treplace\tthe GradientDescentOptimizer\twith\tthe\tMomentumOptimizer,\tthen\tlie\tback\tand\tprofit!\n\noptimizer\t=\ttf.train.MomentumOptimizer(learning_rate=learning_rate, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=0.9)\n\nThe\tone\tdrawback\tof\tMomentum\toptimization\tis\tthat\tit\tadds\tyet\tanother\thyperparameter\tto\ttune.\tHowever, the\tmomentum\tvalue\tof\t0.9\tusually\tworks\twell\tin\tpractice\tand\talmost\talways\tgoes\tfaster\tthan\tGradient Descent.",
      "content_length": 812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "Nesterov\tAccelerated\tGradient One\tsmall\tvariant\tto\tMomentum\toptimization,\tproposed\tby\tYurii\tNesterov\tin\t1983,12\tis\talmost\talways faster\tthan\tvanilla\tMomentum\toptimization.\tThe\tidea\tof\tNesterov\tMomentum\toptimization,\tor\tNesterov Accelerated\tGradient\t(NAG),\tis\tto\tmeasure\tthe\tgradient\tof\tthe\tcost\tfunction\tnot\tat\tthe\tlocal\tposition\tbut slightly\tahead\tin\tthe\tdirection\tof\tthe\tmomentum\t(see\tEquation\t11-5).\tThe\tonly\tdifference\tfrom\tvanilla Momentum\toptimization\tis\tthat\tthe\tgradient\tis\tmeasured\tat\tθ\t+\tβm\trather\tthan\tat\tθ.\n\nEquation\t11-5.\tNesterov\tAccelerated\tGradient\talgorithm\n\nThis\tsmall\ttweak\tworks\tbecause\tin\tgeneral\tthe\tmomentum\tvector\twill\tbe\tpointing\tin\tthe\tright\tdirection (i.e.,\ttoward\tthe\toptimum),\tso\tit\twill\tbe\tslightly\tmore\taccurate\tto\tuse\tthe\tgradient\tmeasured\ta\tbit\tfarther\tin that\tdirection\trather\tthan\tusing\tthe\tgradient\tat\tthe\toriginal\tposition,\tas\tyou\tcan\tsee\tin\tFigure\t11-6\t(where 1\trepresents\tthe\tgradient\tof\tthe\tcost\tfunction\tmeasured\tat\tthe\tstarting\tpoint\tθ,\tand\t 2\trepresents\tthe\n\ngradient\tat\tthe\tpoint\tlocated\tat\tθ\t+\tβm).\tAs\tyou\tcan\tsee,\tthe\tNesterov\tupdate\tends\tup\tslightly\tcloser\tto\tthe optimum.\tAfter\ta\twhile,\tthese\tsmall\timprovements\tadd\tup\tand\tNAG\tends\tup\tbeing\tsignificantly\tfaster\tthan regular\tMomentum\toptimization.\tMoreover,\tnote\tthat\twhen\tthe\tmomentum\tpushes\tthe\tweights\tacross\ta valley,\t 1\tcontinues\tto\tpush\tfurther\tacross\tthe\tvalley,\twhile\t 2\tpushes\tback\ttoward\tthe\tbottom\tof\tthe valley.\tThis\thelps\treduce\toscillations\tand\tthus\tconverges\tfaster.\n\nNAG\twill\talmost\talways\tspeed\tup\ttraining\tcompared\tto\tregular\tMomentum\toptimization.\tTo\tuse\tit, simply\tset\tuse_nesterov=True\twhen\tcreating\tthe\tMomentumOptimizer:\n\noptimizer\t=\ttf.train.MomentumOptimizer(learning_rate=learning_rate, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=0.9,\tuse_nesterov=True)",
      "content_length": 1782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "Figure\t11-6.\tRegular\tversus\tNesterov\tMomentum\toptimization",
      "content_length": 58,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "AdaGrad Consider\tthe\telongated\tbowl\tproblem\tagain:\tGradient\tDescent\tstarts\tby\tquickly\tgoing\tdown\tthe\tsteepest slope,\tthen\tslowly\tgoes\tdown\tthe\tbottom\tof\tthe\tvalley.\tIt\twould\tbe\tnice\tif\tthe\talgorithm\tcould\tdetect\tthis early\ton\tand\tcorrect\tits\tdirection\tto\tpoint\ta\tbit\tmore\ttoward\tthe\tglobal\toptimum. The\tAdaGrad\talgorithm13\tachieves\tthis\tby\tscaling\tdown\tthe\tgradient\tvector\talong\tthe\tsteepest\tdimensions (see\tEquation\t11-6):\n\nEquation\t11-6.\tAdaGrad\talgorithm\n\nThe\tfirst\tstep\taccumulates\tthe\tsquare\tof\tthe\tgradients\tinto\tthe\tvector\ts\t(the\t element-wise\tmultiplication).\tThis\tvectorized\tform\tis\tequivalent\tto\tcomputing\tsi\t←\tsi\t+\t(∂\tJ(θ)\t/\t∂\tθi)2 for\teach\telement\tsi\tof\tthe\tvector\ts;\tin\tother\twords,\teach\tsi\taccumulates\tthe\tsquares\tof\tthe\tpartial derivative\tof\tthe\tcost\tfunction\twith\tregards\tto\tparameter\tθi.\tIf\tthe\tcost\tfunction\tis\tsteep\talong\tthe\tith dimension,\tthen\tsi\twill\tget\tlarger\tand\tlarger\tat\teach\titeration.\n\nsymbol\trepresents\tthe\n\nThe\tsecond\tstep\tis\talmost\tidentical\tto\tGradient\tDescent,\tbut\twith\tone\tbig\tdifference:\tthe\tgradient\tvector\n\nis\tscaled\tdown\tby\ta\tfactor\tof\t smoothing\tterm\tto\tavoid\tdivision\tby\tzero,\ttypically\tset\tto\t10–10).\tThis\tvectorized\tform\tis\tequivalent\tto\n\n(the\t\tsymbol\trepresents\tthe\telement-wise\tdivision,\tand\tϵ\tis\ta\n\ncomputing\n\nfor\tall\tparameters\tθi\t(simultaneously).\n\nIn\tshort,\tthis\talgorithm\tdecays\tthe\tlearning\trate,\tbut\tit\tdoes\tso\tfaster\tfor\tsteep\tdimensions\tthan\tfor dimensions\twith\tgentler\tslopes.\tThis\tis\tcalled\tan\tadaptive\tlearning\trate.\tIt\thelps\tpoint\tthe\tresulting updates\tmore\tdirectly\ttoward\tthe\tglobal\toptimum\t(see\tFigure\t11-7).\tOne\tadditional\tbenefit\tis\tthat\tit requires\tmuch\tless\ttuning\tof\tthe\tlearning\trate\thyperparameter\tη.",
      "content_length": 1668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "Figure\t11-7.\tAdaGrad\tversus\tGradient\tDescent\n\nAdaGrad\toften\tperforms\twell\tfor\tsimple\tquadratic\tproblems,\tbut\tunfortunately\tit\toften\tstops\ttoo\tearly when\ttraining\tneural\tnetworks.\tThe\tlearning\trate\tgets\tscaled\tdown\tso\tmuch\tthat\tthe\talgorithm\tends\tup stopping\tentirely\tbefore\treaching\tthe\tglobal\toptimum.\tSo\teven\tthough\tTensorFlow\thas\tan AdagradOptimizer,\tyou\tshould\tnot\tuse\tit\tto\ttrain\tdeep\tneural\tnetworks\t(it\tmay\tbe\tefficient\tfor\tsimpler tasks\tsuch\tas\tLinear\tRegression,\tthough).",
      "content_length": 480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "RMSProp Although\tAdaGrad\tslows\tdown\ta\tbit\ttoo\tfast\tand\tends\tup\tnever\tconverging\tto\tthe\tglobal\toptimum,\tthe RMSProp\talgorithm14\tfixes\tthis\tby\taccumulating\tonly\tthe\tgradients\tfrom\tthe\tmost\trecent\titerations\t(as opposed\tto\tall\tthe\tgradients\tsince\tthe\tbeginning\tof\ttraining).\tIt\tdoes\tso\tby\tusing\texponential\tdecay\tin\tthe first\tstep\t(see\tEquation\t11-7).\n\nEquation\t11-7.\tRMSProp\talgorithm\n\nThe\tdecay\trate\tβ\tis\ttypically\tset\tto\t0.9.\tYes,\tit\tis\tonce\tagain\ta\tnew\thyperparameter,\tbut\tthis\tdefault\tvalue often\tworks\twell,\tso\tyou\tmay\tnot\tneed\tto\ttune\tit\tat\tall.\n\nAs\tyou\tmight\texpect,\tTensorFlow\thas\tan\tRMSPropOptimizer\tclass:\n\noptimizer\t=\ttf.train.RMSPropOptimizer(learning_rate=learning_rate, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmomentum=0.9,\tdecay=0.9,\tepsilon=1e-10)\n\nExcept\ton\tvery\tsimple\tproblems,\tthis\toptimizer\talmost\talways\tperforms\tmuch\tbetter\tthan\tAdaGrad.\tIt also\tgenerally\tconverges\tfaster\tthan\tMomentum\toptimization\tand\tNesterov\tAccelerated\tGradients.\tIn\tfact, it\twas\tthe\tpreferred\toptimization\talgorithm\tof\tmany\tresearchers\tuntil\tAdam\toptimization\tcame\taround.",
      "content_length": 1064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "Adam\tOptimization Adam,15\twhich\tstands\tfor\tadaptive\tmoment\testimation,\tcombines\tthe\tideas\tof\tMomentum\toptimization and\tRMSProp:\tjust\tlike\tMomentum\toptimization\tit\tkeeps\ttrack\tof\tan\texponentially\tdecaying\taverage\tof past\tgradients,\tand\tjust\tlike\tRMSProp\tit\tkeeps\ttrack\tof\tan\texponentially\tdecaying\taverage\tof\tpast\tsquared gradients\t(see\tEquation\t11-8).16\n\nEquation\t11-8.\tAdam\talgorithm\n\nT\trepresents\tthe\titeration\tnumber\t(starting\tat\t1).\n\nIf\tyou\tjust\tlook\tat\tsteps\t1,\t2,\tand\t5,\tyou\twill\tnotice\tAdam’s\tclose\tsimilarity\tto\tboth\tMomentum optimization\tand\tRMSProp.\tThe\tonly\tdifference\tis\tthat\tstep\t1\tcomputes\tan\texponentially\tdecaying average\trather\tthan\tan\texponentially\tdecaying\tsum,\tbut\tthese\tare\tactually\tequivalent\texcept\tfor\ta\tconstant factor\t(the\tdecaying\taverage\tis\tjust\t1\t–\tβ1\ttimes\tthe\tdecaying\tsum).\tSteps\t3\tand\t4\tare\tsomewhat\tof\ta technical\tdetail:\tsince\tm\tand\ts\tare\tinitialized\tat\t0,\tthey\twill\tbe\tbiased\ttoward\t0\tat\tthe\tbeginning\tof training,\tso\tthese\ttwo\tsteps\twill\thelp\tboost\tm\tand\ts\tat\tthe\tbeginning\tof\ttraining.\n\nThe\tmomentum\tdecay\thyperparameter\tβ1\tis\ttypically\tinitialized\tto\t0.9,\twhile\tthe\tscaling\tdecay hyperparameter\tβ2\tis\toften\tinitialized\tto\t0.999.\tAs\tearlier,\tthe\tsmoothing\tterm\tϵ\tis\tusually\tinitialized\tto\ta tiny\tnumber\tsuch\tas\t10–8.\tThese\tare\tthe\tdefault\tvalues\tfor\tTensorFlow’s\tAdamOptimizer\tclass,\tso\tyou can\tsimply\tuse:\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate=learning_rate)\n\nIn\tfact,\tsince\tAdam\tis\tan\tadaptive\tlearning\trate\talgorithm\t(like\tAdaGrad\tand\tRMSProp),\tit\trequires\tless tuning\tof\tthe\tlearning\trate\thyperparameter\tη.\tYou\tcan\toften\tuse\tthe\tdefault\tvalue\tη\t=\t0.001,\tmaking\tAdam even\teasier\tto\tuse\tthan\tGradient\tDescent.",
      "content_length": 1661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "WARNING\n\nThis\tbook\tinitially\trecommended\tusing\tAdam\toptimization,\tbecause\tit\twas\tgenerally\tconsidered\tfaster\tand\tbetter\tthan\tother methods.\tHowever,\ta\t2017\tpaper17\tby\tAshia\tC.\tWilson\tet\tal.\tshowed\tthat\tadaptive\toptimization\tmethods\t(i.e.,\tAdaGrad, RMSProp\tand\tAdam\toptimization)\tcan\tlead\tto\tsolutions\tthat\tgeneralize\tpoorly\ton\tsome\tdatasets.\tSo\tyou\tmay\twant\tto\tstick\tto Momentum\toptimization\tor\tNesterov\tAccelerated\tGradient\tfor\tnow,\tuntil\tresearchers\thave\ta\tbetter\tunderstanding\tof\tthis\tissue.\n\nAll\tthe\toptimization\ttechniques\tdiscussed\tso\tfar\tonly\trely\ton\tthe\tfirst-order\tpartial\tderivatives (Jacobians).\tThe\toptimization\tliterature\tcontains\tamazing\talgorithms\tbased\ton\tthe\tsecond-order\tpartial derivatives\t(the\tHessians).\tUnfortunately,\tthese\talgorithms\tare\tvery\thard\tto\tapply\tto\tdeep\tneural\tnetworks because\tthere\tare\tn2\tHessians\tper\toutput\t(where\tn\tis\tthe\tnumber\tof\tparameters),\tas\topposed\tto\tjust\tn Jacobians\tper\toutput.\tSince\tDNNs\ttypically\thave\ttens\tof\tthousands\tof\tparameters,\tthe\tsecond-order optimization\talgorithms\toften\tdon’t\teven\tfit\tin\tmemory,\tand\teven\twhen\tthey\tdo,\tcomputing\tthe\tHessians\tis just\ttoo\tslow.\n\nTRAINING\tSPARSE\tMODELS\n\nAll\tthe\toptimization\talgorithms\tjust\tpresented\tproduce\tdense\tmodels,\tmeaning\tthat\tmost\tparameters\twill\tbe\tnonzero.\tIf\tyou\tneed\ta blazingly\tfast\tmodel\tat\truntime,\tor\tif\tyou\tneed\tit\tto\ttake\tup\tless\tmemory,\tyou\tmay\tprefer\tto\tend\tup\twith\ta\tsparse\tmodel\tinstead.\n\nOne\ttrivial\tway\tto\tachieve\tthis\tis\tto\ttrain\tthe\tmodel\tas\tusual,\tthen\tget\trid\tof\tthe\ttiny\tweights\t(set\tthem\tto\t0).\n\nAnother\toption\tis\tto\tapply\tstrong\tℓ1\tregularization\tduring\ttraining,\tas\tit\tpushes\tthe\toptimizer\tto\tzero\tout\tas\tmany\tweights\tas\tit\tcan\t(as discussed\tin\tChapter\t4\tabout\tLasso\tRegression).\n\nHowever,\tin\tsome\tcases\tthese\ttechniques\tmay\tremain\tinsufficient.\tOne\tlast\toption\tis\tto\tapply\tDual\tAveraging,\toften\tcalled\tFollow\tThe Regularized\tLeader\t(FTRL),\ta\ttechnique\tproposed\tby\tYurii\tNesterov.18\tWhen\tused\twith\tℓ1\tregularization,\tthis\ttechnique\toften\tleads\tto very\tsparse\tmodels.\tTensorFlow\timplements\ta\tvariant\tof\tFTRL\tcalled\tFTRL-Proximal19\tin\tthe\tFTRLOptimizer\tclass.",
      "content_length": 2085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "Learning\tRate\tScheduling Finding\ta\tgood\tlearning\trate\tcan\tbe\ttricky.\tIf\tyou\tset\tit\tway\ttoo\thigh,\ttraining\tmay\tactually\tdiverge\t(as\twe discussed\tin\tChapter\t4).\tIf\tyou\tset\tit\ttoo\tlow,\ttraining\twill\teventually\tconverge\tto\tthe\toptimum,\tbut\tit\twill take\ta\tvery\tlong\ttime.\tIf\tyou\tset\tit\tslightly\ttoo\thigh,\tit\twill\tmake\tprogress\tvery\tquickly\tat\tfirst,\tbut\tit\twill end\tup\tdancing\taround\tthe\toptimum,\tnever\tsettling\tdown\t(unless\tyou\tuse\tan\tadaptive\tlearning\trate optimization\talgorithm\tsuch\tas\tAdaGrad,\tRMSProp,\tor\tAdam,\tbut\teven\tthen\tit\tmay\ttake\ttime\tto\tsettle).\tIf you\thave\ta\tlimited\tcomputing\tbudget,\tyou\tmay\thave\tto\tinterrupt\ttraining\tbefore\tit\thas\tconverged\tproperly, yielding\ta\tsuboptimal\tsolution\t(see\tFigure\t11-8).\n\nFigure\t11-8.\tLearning\tcurves\tfor\tvarious\tlearning\trates\tη\n\nYou\tmay\tbe\table\tto\tfind\ta\tfairly\tgood\tlearning\trate\tby\ttraining\tyour\tnetwork\tseveral\ttimes\tduring\tjust\ta few\tepochs\tusing\tvarious\tlearning\trates\tand\tcomparing\tthe\tlearning\tcurves.\tThe\tideal\tlearning\trate\twill learn\tquickly\tand\tconverge\tto\tgood\tsolution.\n\nHowever,\tyou\tcan\tdo\tbetter\tthan\ta\tconstant\tlearning\trate:\tif\tyou\tstart\twith\ta\thigh\tlearning\trate\tand\tthen reduce\tit\tonce\tit\tstops\tmaking\tfast\tprogress,\tyou\tcan\treach\ta\tgood\tsolution\tfaster\tthan\twith\tthe\toptimal constant\tlearning\trate.\tThere\tare\tmany\tdifferent\tstrategies\tto\treduce\tthe\tlearning\trate\tduring\ttraining. These\tstrategies\tare\tcalled\tlearning\tschedules\t(we\tbriefly\tintroduced\tthis\tconcept\tin\tChapter\t4),\tthe\tmost common\tof\twhich\tare:\n\nPredetermined\tpiecewise\tconstant\tlearning\trate\n\nFor\texample,\tset\tthe\tlearning\trate\tto\tη0\t=\t0.1\tat\tfirst,\tthen\tto\tη1\t=\t0.001\tafter\t50\tepochs.\tAlthough this\tsolution\tcan\twork\tvery\twell,\tit\toften\trequires\tfiddling\taround\tto\tfigure\tout\tthe\tright\tlearning rates\tand\twhen\tto\tuse\tthem.\n\nPerformance\tscheduling\n\nMeasure\tthe\tvalidation\terror\tevery\tN\tsteps\t(just\tlike\tfor\tearly\tstopping)\tand\treduce\tthe\tlearning\trate by\ta\tfactor\tof\tλ\twhen\tthe\terror\tstops\tdropping.\n\nExponential\tscheduling",
      "content_length": 1952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "Set\tthe\tlearning\trate\tto\ta\tfunction\tof\tthe\titeration\tnumber\tt:\tη(t)\t=\tη0\t10–t/r.\tThis\tworks\tgreat,\tbut\tit requires\ttuning\tη0\tand\tr.\tThe\tlearning\trate\twill\tdrop\tby\ta\tfactor\tof\t10\tevery\tr\tsteps.\n\nPower\tscheduling\n\nSet\tthe\tlearning\trate\tto\tη(t)\t=\tη0\t(1\t+\tt/r)–c.\tThe\thyperparameter\tc\tis\ttypically\tset\tto\t1.\tThis\tis similar\tto\texponential\tscheduling,\tbut\tthe\tlearning\trate\tdrops\tmuch\tmore\tslowly.\n\nA\t2013\tpaper20\tby\tAndrew\tSenior\tet\tal.\tcompared\tthe\tperformance\tof\tsome\tof\tthe\tmost\tpopular\tlearning schedules\twhen\ttraining\tdeep\tneural\tnetworks\tfor\tspeech\trecognition\tusing\tMomentum\toptimization.\tThe authors\tconcluded\tthat,\tin\tthis\tsetting,\tboth\tperformance\tscheduling\tand\texponential\tscheduling\tperformed well,\tbut\tthey\tfavored\texponential\tscheduling\tbecause\tit\tis\tsimpler\tto\timplement,\tis\teasy\tto\ttune,\tand converged\tslightly\tfaster\tto\tthe\toptimal\tsolution.\n\nImplementing\ta\tlearning\tschedule\twith\tTensorFlow\tis\tfairly\tstraightforward:\n\ninitial_learning_rate\t=\t0.1 decay_steps\t=\t10000 decay_rate\t=\t1/10 global_step\t=\ttf.Variable(0,\ttrainable=False,\tname=\"global_step\") learning_rate\t=\ttf.train.exponential_decay(initial_learning_rate,\tglobal_step, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdecay_steps,\tdecay_rate) optimizer\t=\ttf.train.MomentumOptimizer(learning_rate,\tmomentum=0.9) training_op\t=\toptimizer.minimize(loss,\tglobal_step=global_step)\n\nAfter\tsetting\tthe\thyperparameter\tvalues,\twe\tcreate\ta\tnontrainable\tvariable\tglobal_step\t(initialized\tto\t0) to\tkeep\ttrack\tof\tthe\tcurrent\ttraining\titeration\tnumber.\tThen\twe\tdefine\tan\texponentially\tdecaying\tlearning rate\t(with\tη0\t=\t0.1\tand\tr\t=\t10,000)\tusing\tTensorFlow’s\texponential_decay()\tfunction.\tNext,\twe\tcreate an\toptimizer\t(in\tthis\texample,\ta\tMomentumOptimizer)\tusing\tthis\tdecaying\tlearning\trate.\tFinally,\twe create\tthe\ttraining\toperation\tby\tcalling\tthe\toptimizer’s\tminimize()\tmethod;\tsince\twe\tpass\tit\tthe global_step\tvariable,\tit\twill\tkindly\ttake\tcare\tof\tincrementing\tit.\tThat’s\tit!\n\nSince\tAdaGrad,\tRMSProp,\tand\tAdam\toptimization\tautomatically\treduce\tthe\tlearning\trate\tduring\ttraining, it\tis\tnot\tnecessary\tto\tadd\tan\textra\tlearning\tschedule.\tFor\tother\toptimization\talgorithms,\tusing\texponential decay\tor\tperformance\tscheduling\tcan\tconsiderably\tspeed\tup\tconvergence.",
      "content_length": 2214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "Avoiding\tOverfitting\tThrough\tRegularization With\tfour\tparameters\tI\tcan\tfit\tan\telephant\tand\twith\tfive\tI\tcan\tmake\thim\twiggle\this\ttrunk. John\tvon\tNeumann,\tcited\tby\tEnrico\tFermi\tin\tNature\t427\n\nDeep\tneural\tnetworks\ttypically\thave\ttens\tof\tthousands\tof\tparameters,\tsometimes\teven\tmillions.\tWith\tso many\tparameters,\tthe\tnetwork\thas\tan\tincredible\tamount\tof\tfreedom\tand\tcan\tfit\ta\thuge\tvariety\tof\tcomplex datasets.\tBut\tthis\tgreat\tflexibility\talso\tmeans\tthat\tit\tis\tprone\tto\toverfitting\tthe\ttraining\tset.\n\nWith\tmillions\tof\tparameters\tyou\tcan\tfit\tthe\twhole\tzoo.\tIn\tthis\tsection\twe\twill\tpresent\tsome\tof\tthe\tmost popular\tregularization\ttechniques\tfor\tneural\tnetworks,\tand\thow\tto\timplement\tthem\twith\tTensorFlow: early\tstopping,\tℓ1\tand\tℓ2\tregularization,\tdropout,\tmax-norm\tregularization,\tand\tdata\taugmentation.",
      "content_length": 793,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "Early\tStopping To\tavoid\toverfitting\tthe\ttraining\tset,\ta\tgreat\tsolution\tis\tearly\tstopping\t(introduced\tin\tChapter\t4):\tjust interrupt\ttraining\twhen\tits\tperformance\ton\tthe\tvalidation\tset\tstarts\tdropping.\n\nOne\tway\tto\timplement\tthis\twith\tTensorFlow\tis\tto\tevaluate\tthe\tmodel\ton\ta\tvalidation\tset\tat\tregular intervals\t(e.g.,\tevery\t50\tsteps),\tand\tsave\ta\t“winner”\tsnapshot\tif\tit\toutperforms\tprevious\t“winner” snapshots.\tCount\tthe\tnumber\tof\tsteps\tsince\tthe\tlast\t“winner”\tsnapshot\twas\tsaved,\tand\tinterrupt\ttraining when\tthis\tnumber\treaches\tsome\tlimit\t(e.g.,\t2,000\tsteps).\tThen\trestore\tthe\tlast\t“winner”\tsnapshot.\n\nAlthough\tearly\tstopping\tworks\tvery\twell\tin\tpractice,\tyou\tcan\tusually\tget\tmuch\thigher\tperformance\tout\tof your\tnetwork\tby\tcombining\tit\twith\tother\tregularization\ttechniques.",
      "content_length": 771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "ℓ1\tand\tℓ2\tRegularization Just\tlike\tyou\tdid\tin\tChapter\t4\tfor\tsimple\tlinear\tmodels,\tyou\tcan\tuse\tℓ1\tand\tℓ2\tregularization\tto\tconstrain\ta neural\tnetwork’s\tconnection\tweights\t(but\ttypically\tnot\tits\tbiases).\n\nOne\tway\tto\tdo\tthis\tusing\tTensorFlow\tis\tto\tsimply\tadd\tthe\tappropriate\tregularization\tterms\tto\tyour\tcost function.\tFor\texample,\tassuming\tyou\thave\tjust\tone\thidden\tlayer\twith\tweights\tW1\tand\tone\toutput\tlayer\twith weights\tW2,\tthen\tyou\tcan\tapply\tℓ1\tregularization\tlike\tthis:\n\n[...]\t#\tconstruct\tthe\tneural\tnetwork W1\t=\ttf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\") W2\t=\ttf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n\nscale\t=\t0.001\t#\tl1\tregularization\thyperparameter\n\nwith\ttf.name_scope(\"loss\"): \t\t\t\txentropy\t=\ttf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogits=logits) \t\t\t\tbase_loss\t=\ttf.reduce_mean(xentropy,\tname=\"avg_xentropy\") \t\t\t\treg_losses\t=\ttf.reduce_sum(tf.abs(W1))\t+\ttf.reduce_sum(tf.abs(W2)) \t\t\t\tloss\t=\ttf.add(base_loss,\tscale\t*\treg_losses,\tname=\"loss\")\n\nHowever,\tif\tthere\tare\tmany\tlayers,\tthis\tapproach\tis\tnot\tvery\tconvenient.\tFortunately,\tTensorFlow provides\ta\tbetter\toption.\tMany\tfunctions\tthat\tcreate\tvariables\t(such\tas\tget_variable()\tor tf.layers.dense())\taccept\ta\t*_regularizer\targument\tfor\teach\tcreated\tvariable\t(e.g., kernel_regularizer).\tYou\tcan\tpass\tany\tfunction\tthat\ttakes\tweights\tas\tan\targument\tand\treturns\tthe corresponding\tregularization\tloss.\tThe\tl1_regularizer(),\tl2_regularizer(),\tand l1_l2_regularizer()\tfunctions\treturn\tsuch\tfunctions.\tThe\tfollowing\tcode\tputs\tall\tthis\ttogether:\n\nmy_dense_layer\t=\tpartial( \t\t\t\ttf.layers.dense,\tactivation=tf.nn.relu, \t\t\t\tkernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n\nwith\ttf.name_scope(\"dnn\"): \t\t\t\thidden1\t=\tmy_dense_layer(X,\tn_hidden1,\tname=\"hidden1\") \t\t\t\thidden2\t=\tmy_dense_layer(hidden1,\tn_hidden2,\tname=\"hidden2\") \t\t\t\tlogits\t=\tmy_dense_layer(hidden2,\tn_outputs,\tactivation=None, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"outputs\")\n\nThis\tcode\tcreates\ta\tneural\tnetwork\twith\ttwo\thidden\tlayers\tand\tone\toutput\tlayer,\tand\tit\talso\tcreates\tnodes in\tthe\tgraph\tto\tcompute\tthe\tℓ1\tregularization\tloss\tcorresponding\tto\teach\tlayer’s\tweights.\tTensorFlow automatically\tadds\tthese\tnodes\tto\ta\tspecial\tcollection\tcontaining\tall\tthe\tregularization\tlosses.\tYou\tjust need\tto\tadd\tthese\tregularization\tlosses\tto\tyour\toverall\tloss,\tlike\tthis:\n\nreg_losses\t=\ttf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) loss\t=\ttf.add_n([base_loss]\t+\treg_losses,\tname=\"loss\")\n\nWARNING\n\nDon’t\tforget\tto\tadd\tthe\tregularization\tlosses\tto\tyour\toverall\tloss,\tor\telse\tthey\twill\tsimply\tbe\tignored.",
      "content_length": 2630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "Dropout The\tmost\tpopular\tregularization\ttechnique\tfor\tdeep\tneural\tnetworks\tis\targuably\tdropout.\tIt\twas proposed21\tby\tG.\tE.\tHinton\tin\t2012\tand\tfurther\tdetailed\tin\ta\tpaper22\tby\tNitish\tSrivastava\tet\tal.,\tand\tit\thas proven\tto\tbe\thighly\tsuccessful:\teven\tthe\tstate-of-the-art\tneural\tnetworks\tgot\ta\t1–2%\taccuracy\tboost simply\tby\tadding\tdropout.\tThis\tmay\tnot\tsound\tlike\ta\tlot,\tbut\twhen\ta\tmodel\talready\thas\t95%\taccuracy, getting\ta\t2%\taccuracy\tboost\tmeans\tdropping\tthe\terror\trate\tby\talmost\t40%\t(going\tfrom\t5%\terror\tto roughly\t3%).\n\nIt\tis\ta\tfairly\tsimple\talgorithm:\tat\tevery\ttraining\tstep,\tevery\tneuron\t(including\tthe\tinput\tneurons\tbut excluding\tthe\toutput\tneurons)\thas\ta\tprobability\tp\tof\tbeing\ttemporarily\t“dropped\tout,”\tmeaning\tit\twill\tbe entirely\tignored\tduring\tthis\ttraining\tstep,\tbut\tit\tmay\tbe\tactive\tduring\tthe\tnext\tstep\t(see\tFigure\t11-9).\tThe hyperparameter\tp\tis\tcalled\tthe\tdropout\trate,\tand\tit\tis\ttypically\tset\tto\t50%.\tAfter\ttraining,\tneurons\tdon’t get\tdropped\tanymore.\tAnd\tthat’s\tall\t(except\tfor\ta\ttechnical\tdetail\twe\twill\tdiscuss\tmomentarily).\n\nFigure\t11-9.\tDropout\tregularization\n\nIt\tis\tquite\tsurprising\tat\tfirst\tthat\tthis\trather\tbrutal\ttechnique\tworks\tat\tall.\tWould\ta\tcompany\tperform\tbetter if\tits\temployees\twere\ttold\tto\ttoss\ta\tcoin\tevery\tmorning\tto\tdecide\twhether\tor\tnot\tto\tgo\tto\twork?\tWell,\twho knows;\tperhaps\tit\twould!\tThe\tcompany\twould\tobviously\tbe\tforced\tto\tadapt\tits\torganization;\tit\tcould\tnot rely\ton\tany\tsingle\tperson\tto\tfill\tin\tthe\tcoffee\tmachine\tor\tperform\tany\tother\tcritical\ttasks,\tso\tthis\texpertise would\thave\tto\tbe\tspread\tacross\tseveral\tpeople.\tEmployees\twould\thave\tto\tlearn\tto\tcooperate\twith\tmany of\ttheir\tcoworkers,\tnot\tjust\ta\thandful\tof\tthem.\tThe\tcompany\twould\tbecome\tmuch\tmore\tresilient.\tIf\tone person\tquit,\tit\twouldn’t\tmake\tmuch\tof\ta\tdifference.\tIt’s\tunclear\twhether\tthis\tidea\twould\tactually\twork\tfor companies,\tbut\tit\tcertainly\tdoes\tfor\tneural\tnetworks.\tNeurons\ttrained\twith\tdropout\tcannot\tco-adapt\twith their\tneighboring\tneurons;\tthey\thave\tto\tbe\tas\tuseful\tas\tpossible\ton\ttheir\town.\tThey\talso\tcannot\trely",
      "content_length": 2024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "excessively\ton\tjust\ta\tfew\tinput\tneurons;\tthey\tmust\tpay\tattention\tto\teach\tof\ttheir\tinput\tneurons.\tThey\tend\tup being\tless\tsensitive\tto\tslight\tchanges\tin\tthe\tinputs.\tIn\tthe\tend\tyou\tget\ta\tmore\trobust\tnetwork\tthat generalizes\tbetter.\n\nAnother\tway\tto\tunderstand\tthe\tpower\tof\tdropout\tis\tto\trealize\tthat\ta\tunique\tneural\tnetwork\tis\tgenerated\tat each\ttraining\tstep.\tSince\teach\tneuron\tcan\tbe\teither\tpresent\tor\tabsent,\tthere\tis\ta\ttotal\tof\t2N\tpossible networks\t(where\tN\tis\tthe\ttotal\tnumber\tof\tdroppable\tneurons).\tThis\tis\tsuch\ta\thuge\tnumber\tthat\tit\tis virtually\timpossible\tfor\tthe\tsame\tneural\tnetwork\tto\tbe\tsampled\ttwice.\tOnce\tyou\thave\trun\ta\t10,000 training\tsteps,\tyou\thave\tessentially\ttrained\t10,000\tdifferent\tneural\tnetworks\t(each\twith\tjust\tone\ttraining instance).\tThese\tneural\tnetworks\tare\tobviously\tnot\tindependent\tsince\tthey\tshare\tmany\tof\ttheir\tweights, but\tthey\tare\tnevertheless\tall\tdifferent.\tThe\tresulting\tneural\tnetwork\tcan\tbe\tseen\tas\tan\taveraging\tensemble of\tall\tthese\tsmaller\tneural\tnetworks.\n\nThere\tis\tone\tsmall\tbut\timportant\ttechnical\tdetail.\tSuppose\tp\t=\t50%,\tin\twhich\tcase\tduring\ttesting\ta\tneuron will\tbe\tconnected\tto\ttwice\tas\tmany\tinput\tneurons\tas\tit\twas\t(on\taverage)\tduring\ttraining.\tTo\tcompensate for\tthis\tfact,\twe\tneed\tto\tmultiply\teach\tneuron’s\tinput\tconnection\tweights\tby\t0.5\tafter\ttraining.\tIf\twe\tdon’t, each\tneuron\twill\tget\ta\ttotal\tinput\tsignal\troughly\ttwice\tas\tlarge\tas\twhat\tthe\tnetwork\twas\ttrained\ton,\tand\tit is\tunlikely\tto\tperform\twell.\tMore\tgenerally,\twe\tneed\tto\tmultiply\teach\tinput\tconnection\tweight\tby\tthe\tkeep probability\t(1\t–\tp)\tafter\ttraining.\tAlternatively,\twe\tcan\tdivide\teach\tneuron’s\toutput\tby\tthe\tkeep probability\tduring\ttraining\t(these\talternatives\tare\tnot\tperfectly\tequivalent,\tbut\tthey\twork\tequally\twell).\n\nTo\timplement\tdropout\tusing\tTensorFlow,\tyou\tcan\tsimply\tapply\tthe\ttf.layers.dropout()\tfunction\tto\tthe input\tlayer\tand/or\tto\tthe\toutput\tof\tany\thidden\tlayer\tyou\twant.\tDuring\ttraining,\tthis\tfunction\trandomly drops\tsome\titems\t(setting\tthem\tto\t0)\tand\tdivides\tthe\tremaining\titems\tby\tthe\tkeep\tprobability.\tAfter training,\tthis\tfunction\tdoes\tnothing\tat\tall.\tThe\tfollowing\tcode\tapplies\tdropout\tregularization\tto\tour\tthree- layer\tneural\tnetwork:\n\n[...] training\t=\ttf.placeholder_with_default(False,\tshape=(),\tname='training')\n\ndropout_rate\t=\t0.5\t\t#\t==\t1\t-\tkeep_prob X_drop\t=\ttf.layers.dropout(X,\tdropout_rate,\ttraining=training)\n\nwith\ttf.name_scope(\"dnn\"): \t\t\t\thidden1\t=\ttf.layers.dense(X_drop,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden1\") \t\t\t\thidden1_drop\t=\ttf.layers.dropout(hidden1,\tdropout_rate,\ttraining=training) \t\t\t\thidden2\t=\ttf.layers.dense(hidden1_drop,\tn_hidden2,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden2\") \t\t\t\thidden2_drop\t=\ttf.layers.dropout(hidden2,\tdropout_rate,\ttraining=training) \t\t\t\tlogits\t=\ttf.layers.dense(hidden2_drop,\tn_outputs,\tname=\"outputs\")\n\nWARNING\n\nYou\twant\tto\tuse\tthe\ttf.layers.dropout()\tfunction,\tnot\ttf.nn.dropout().\tThe\tfirst\tone\tturns\toff\t(no-op)\twhen\tnot\ttraining, which\tis\twhat\tyou\twant,\twhile\tthe\tsecond\tone\tdoes\tnot.\n\nOf\tcourse,\tjust\tlike\tyou\tdid\tearlier\tfor\tBatch\tNormalization,\tyou\tneed\tto\tset\ttraining\tto\tTrue\twhen training,\tand\tleave\tthe\tdefault\tFalse\tvalue\twhen\ttesting.\n\nIf\tyou\tobserve\tthat\tthe\tmodel\tis\toverfitting,\tyou\tcan\tincrease\tthe\tdropout\trate.\tConversely,\tyou\tshould\ttry",
      "content_length": 3299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "decreasing\tthe\tdropout\trate\tif\tthe\tmodel\tunderfits\tthe\ttraining\tset.\tIt\tcan\talso\thelp\tto\tincrease\tthe\tdropout rate\tfor\tlarge\tlayers,\tand\treduce\tit\tfor\tsmall\tones.\n\nDropout\tdoes\ttend\tto\tsignificantly\tslow\tdown\tconvergence,\tbut\tit\tusually\tresults\tin\ta\tmuch\tbetter\tmodel when\ttuned\tproperly.\tSo,\tit\tis\tgenerally\twell\tworth\tthe\textra\ttime\tand\teffort.\n\nNOTE\n\nDropconnect\tis\ta\tvariant\tof\tdropout\twhere\tindividual\tconnections\tare\tdropped\trandomly\trather\tthan\twhole\tneurons.\tIn\tgeneral dropout\tperforms\tbetter.",
      "content_length": 502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "Max-Norm\tRegularization Another\tregularization\ttechnique\tthat\tis\tquite\tpopular\tfor\tneural\tnetworks\tis\tcalled\tmax-norm regularization:\tfor\teach\tneuron,\tit\tconstrains\tthe\tweights\tw\tof\tthe\tincoming\tconnections\tsuch\tthat\t\t w\t2\t≤ r,\twhere\tr\tis\tthe\tmax-norm\thyperparameter\tand\t\t·\n\n2\tis\tthe\tℓ2\tnorm.\n\nWe\ttypically\timplement\tthis\tconstraint\tby\tcomputing\t w2\tafter\teach\ttraining\tstep\tand\tclipping\tw\tif\n\nneeded\t(\n\n).\n\nReducing\tr\tincreases\tthe\tamount\tof\tregularization\tand\thelps\treduce\toverfitting.\tMax-norm\tregularization can\talso\thelp\talleviate\tthe\tvanishing/exploding\tgradients\tproblems\t(if\tyou\tare\tnot\tusing\tBatch Normalization).\n\nTensorFlow\tdoes\tnot\tprovide\tan\toff-the-shelf\tmax-norm\tregularizer,\tbut\tit\tis\tnot\ttoo\thard\tto\timplement. The\tfollowing\tcode\tgets\ta\thandle\ton\tthe\tweights\tof\tthe\tfirst\thidden\tlayer,\tthen\tit\tuses\tthe clip_by_norm()\tfunction\tto\tcreate\tan\toperation\tthat\twill\tclip\tthe\tweights\talong\tthe\tsecond\taxis\tso\tthat each\trow\tvector\tends\tup\twith\ta\tmaximum\tnorm\tof\t1.0.\tThe\tlast\tline\tcreates\tan\tassignment\toperation\tthat will\tassign\tthe\tclipped\tweights\tto\tthe\tweights\tvariable:\n\nthreshold\t=\t1.0 weights\t=\ttf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\") clipped_weights\t=\ttf.clip_by_norm(weights,\tclip_norm=threshold,\taxes=1) clip_weights\t=\ttf.assign(weights,\tclipped_weights)\n\nThen\tyou\tjust\tapply\tthis\toperation\tafter\teach\ttraining\tstep,\tlike\tso:\n\nsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) clip_weights.eval()\n\nIn\tgeneral,\tyou\twould\tdo\tthis\tfor\tevery\thidden\tlayer.\tAlthough\tthis\tsolution\tshould\twork\tfine,\tit\tis\ta\tbit messy.\tA\tcleaner\tsolution\tis\tto\tcreate\ta\tmax_norm_regularizer()\tfunction\tand\tuse\tit\tjust\tlike\tthe\tearlier l1_regularizer()\tfunction:\n\ndef\tmax_norm_regularizer(threshold,\taxes=1,\tname=\"max_norm\", \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcollection=\"max_norm\"): \t\t\t\tdef\tmax_norm(weights): \t\t\t\t\t\t\t\tclipped\t=\ttf.clip_by_norm(weights,\tclip_norm=threshold,\taxes=axes) \t\t\t\t\t\t\t\tclip_weights\t=\ttf.assign(weights,\tclipped,\tname=name) \t\t\t\t\t\t\t\ttf.add_to_collection(collection,\tclip_weights) \t\t\t\t\t\t\t\treturn\tNone\t\t#\tthere\tis\tno\tregularization\tloss\tterm \t\t\t\treturn\tmax_norm\n\nThis\tfunction\treturns\ta\tparametrized\tmax_norm()\tfunction\tthat\tyou\tcan\tuse\tlike\tany\tother\tregularizer:\n\nmax_norm_reg\t=\tmax_norm_regularizer(threshold=1.0)\n\nwith\ttf.name_scope(\"dnn\"): \t\t\t\thidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_regularizer=max_norm_reg,\tname=\"hidden1\") \t\t\t\thidden2\t=\ttf.layers.dense(hidden1,\tn_hidden2,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_regularizer=max_norm_reg,\tname=\"hidden2\") \t\t\t\tlogits\t=\ttf.layers.dense(hidden2,\tn_outputs,\tname=\"outputs\")",
      "content_length": 2642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "Note\tthat\tmax-norm\tregularization\tdoes\tnot\trequire\tadding\ta\tregularization\tloss\tterm\tto\tyour\toverall\tloss function,\twhich\tis\twhy\tthe\tmax_norm()\tfunction\treturns\tNone.\tBut\tyou\tstill\tneed\tto\tbe\table\tto\trun\tthe clip_weights\toperations\tafter\teach\ttraining\tstep,\tso\tyou\tneed\tto\tbe\table\tto\tget\ta\thandle\ton\tthem.\tThis\tis why\tthe\tmax_norm()\tfunction\tadds\tthe\tclip_weights\toperation\tto\ta\tcollection\tof\tmax-norm\tclipping operations.\tYou\tneed\tto\tfetch\tthese\tclipping\toperations\tand\trun\tthem\tafter\teach\ttraining\tstep:\n\nclip_all_weights\t=\ttf.get_collection(\"max_norm\")\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tfor\titeration\tin\trange(mnist.train.num_examples\t//\tbatch_size): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(clip_all_weights)\n\nMuch\tcleaner\tcode,\tisn’t\tit?",
      "content_length": 909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "Data\tAugmentation One\tlast\tregularization\ttechnique,\tdata\taugmentation,\tconsists\tof\tgenerating\tnew\ttraining\tinstances\tfrom existing\tones,\tartificially\tboosting\tthe\tsize\tof\tthe\ttraining\tset.\tThis\twill\treduce\toverfitting,\tmaking\tthis\ta regularization\ttechnique.\tThe\ttrick\tis\tto\tgenerate\trealistic\ttraining\tinstances;\tideally,\ta\thuman\tshould\tnot be\table\tto\ttell\twhich\tinstances\twere\tgenerated\tand\twhich\tones\twere\tnot.\tMoreover,\tsimply\tadding\twhite noise\twill\tnot\thelp;\tthe\tmodifications\tyou\tapply\tshould\tbe\tlearnable\t(white\tnoise\tis\tnot).\n\nFor\texample,\tif\tyour\tmodel\tis\tmeant\tto\tclassify\tpictures\tof\tmushrooms,\tyou\tcan\tslightly\tshift,\trotate,\tand resize\tevery\tpicture\tin\tthe\ttraining\tset\tby\tvarious\tamounts\tand\tadd\tthe\tresulting\tpictures\tto\tthe\ttraining\tset (see\tFigure\t11-10).\tThis\tforces\tthe\tmodel\tto\tbe\tmore\ttolerant\tto\tthe\tposition,\torientation,\tand\tsize\tof\tthe mushrooms\tin\tthe\tpicture.\tIf\tyou\twant\tthe\tmodel\tto\tbe\tmore\ttolerant\tto\tlighting\tconditions,\tyou\tcan similarly\tgenerate\tmany\timages\twith\tvarious\tcontrasts.\tAssuming\tthe\tmushrooms\tare\tsymmetrical,\tyou can\talso\tflip\tthe\tpictures\thorizontally.\tBy\tcombining\tthese\ttransformations\tyou\tcan\tgreatly\tincrease\tthe size\tof\tyour\ttraining\tset.\n\nFigure\t11-10.\tGenerating\tnew\ttraining\tinstances\tfrom\texisting\tones\n\nIt\tis\toften\tpreferable\tto\tgenerate\ttraining\tinstances\ton\tthe\tfly\tduring\ttraining\trather\tthan\twasting\tstorage space\tand\tnetwork\tbandwidth.\tTensorFlow\toffers\tseveral\timage\tmanipulation\toperations\tsuch\tas transposing\t(shifting),\trotating,\tresizing,\tflipping,\tand\tcropping,\tas\twell\tas\tadjusting\tthe\tbrightness, contrast,\tsaturation,\tand\thue\t(see\tthe\tAPI\tdocumentation\tfor\tmore\tdetails).\tThis\tmakes\tit\teasy\tto implement\tdata\taugmentation\tfor\timage\tdatasets.",
      "content_length": 1715,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "NOTE\n\nAnother\tpowerful\ttechnique\tto\ttrain\tvery\tdeep\tneural\tnetworks\tis\tto\tadd\tskip\tconnections\t(a\tskip\tconnection\tis\twhen\tyou\tadd the\tinput\tof\ta\tlayer\tto\tthe\toutput\tof\ta\thigher\tlayer).\tWe\twill\texplore\tthis\tidea\tin\tChapter\t13\twhen\twe\ttalk\tabout\tdeep\tresidual networks.",
      "content_length": 267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "Practical\tGuidelines In\tthis\tchapter,\twe\thave\tcovered\ta\twide\trange\tof\ttechniques\tand\tyou\tmay\tbe\twondering\twhich\tones\tyou should\tuse.\tThe\tconfiguration\tin\tTable\t11-2\twill\twork\tfine\tin\tmost\tcases.\n\nTable\t11-2.\tDefault\tDNN\tconfiguration\n\nInitialization\n\nHe\tinitialization\n\nActivation\tfunction\n\nELU\n\nNormalization\n\nBatch\tNormalization\n\nRegularization\n\nDropout\n\nOptimizer\n\nNesterov\tAccelerated\tGradient\n\nLearning\trate\tschedule\n\nNone\n\nOf\tcourse,\tyou\tshould\ttry\tto\treuse\tparts\tof\ta\tpretrained\tneural\tnetwork\tif\tyou\tcan\tfind\tone\tthat\tsolves\ta similar\tproblem.\n\nThis\tdefault\tconfiguration\tmay\tneed\tto\tbe\ttweaked:\n\nIf\tyou\tcan’t\tfind\ta\tgood\tlearning\trate\t(convergence\twas\ttoo\tslow,\tso\tyou\tincreased\tthe\ttraining\trate, and\tnow\tconvergence\tis\tfast\tbut\tthe\tnetwork’s\taccuracy\tis\tsuboptimal),\tthen\tyou\tcan\ttry\tadding\ta learning\tschedule\tsuch\tas\texponential\tdecay.\n\nIf\tyour\ttraining\tset\tis\ta\tbit\ttoo\tsmall,\tyou\tcan\timplement\tdata\taugmentation.\n\nIf\tyou\tneed\ta\tsparse\tmodel,\tyou\tcan\tadd\tsome\tℓ1\tregularization\tto\tthe\tmix\t(and\toptionally\tzero\tout the\ttiny\tweights\tafter\ttraining).\tIf\tyou\tneed\tan\teven\tsparser\tmodel,\tyou\tcan\ttry\tusing\tFTRL\tinstead\tof Adam\toptimization,\talong\twith\tℓ1\tregularization.\n\nIf\tyou\tneed\ta\tlightning-fast\tmodel\tat\truntime,\tyou\tmay\twant\tto\tdrop\tBatch\tNormalization,\tand possibly\treplace\tthe\tELU\tactivation\tfunction\twith\tthe\tleaky\tReLU.\tHaving\ta\tsparse\tmodel\twill\talso help.\n\nWith\tthese\tguidelines,\tyou\tare\tnow\tready\tto\ttrain\tvery\tdeep\tnets\t—\twell,\tif\tyou\tare\tvery\tpatient,\tthat\tis!\tIf you\tuse\ta\tsingle\tmachine,\tyou\tmay\thave\tto\twait\tfor\tdays\tor\teven\tmonths\tfor\ttraining\tto\tcomplete.\tIn\tthe next\tchapter\twe\twill\tdiscuss\thow\tto\tuse\tdistributed\tTensorFlow\tto\ttrain\tand\trun\tmodels\tacross\tmany servers\tand\tGPUs.",
      "content_length": 1709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "Exercises\n\n1.\t Is\tit\tokay\tto\tinitialize\tall\tthe\tweights\tto\tthe\tsame\tvalue\tas\tlong\tas\tthat\tvalue\tis\tselected\trandomly using\tHe\tinitialization?\n\n2.\t Is\tit\tokay\tto\tinitialize\tthe\tbias\tterms\tto\t0?\n\n3.\t Name\tthree\tadvantages\tof\tthe\tELU\tactivation\tfunction\tover\tReLU.\n\n4.\t In\twhich\tcases\twould\tyou\twant\tto\tuse\teach\tof\tthe\tfollowing\tactivation\tfunctions:\tELU,\tleaky\tReLU (and\tits\tvariants),\tReLU,\ttanh,\tlogistic,\tand\tsoftmax?\n\n5.\t What\tmay\thappen\tif\tyou\tset\tthe\tmomentum\thyperparameter\ttoo\tclose\tto\t1\t(e.g.,\t0.99999)\twhen\tusing a\tMomentumOptimizer?\n\n6.\t Name\tthree\tways\tyou\tcan\tproduce\ta\tsparse\tmodel.\n\n7.\t Does\tdropout\tslow\tdown\ttraining?\tDoes\tit\tslow\tdown\tinference\t(i.e.,\tmaking\tpredictions\ton\tnew instances)?\n\n8.\t Deep\tLearning.\n\na.\t Build\ta\tDNN\twith\tfive\thidden\tlayers\tof\t100\tneurons\teach,\tHe\tinitialization,\tand\tthe\tELU\n\nactivation\tfunction.\n\nb.\t Using\tAdam\toptimization\tand\tearly\tstopping,\ttry\ttraining\tit\ton\tMNIST\tbut\tonly\ton\tdigits\t0\tto\t4, as\twe\twill\tuse\ttransfer\tlearning\tfor\tdigits\t5\tto\t9\tin\tthe\tnext\texercise.\tYou\twill\tneed\ta\tsoftmax output\tlayer\twith\tfive\tneurons,\tand\tas\talways\tmake\tsure\tto\tsave\tcheckpoints\tat\tregular\tintervals and\tsave\tthe\tfinal\tmodel\tso\tyou\tcan\treuse\tit\tlater.\n\nc.\t Tune\tthe\thyperparameters\tusing\tcross-validation\tand\tsee\twhat\tprecision\tyou\tcan\tachieve.\n\nd.\t Now\ttry\tadding\tBatch\tNormalization\tand\tcompare\tthe\tlearning\tcurves:\tis\tit\tconverging\tfaster\n\nthan\tbefore?\tDoes\tit\tproduce\ta\tbetter\tmodel?\n\ne.\t Is\tthe\tmodel\toverfitting\tthe\ttraining\tset?\tTry\tadding\tdropout\tto\tevery\tlayer\tand\ttry\tagain.\tDoes\tit\n\nhelp?\n\n9.\t Transfer\tlearning.\n\na.\t Create\ta\tnew\tDNN\tthat\treuses\tall\tthe\tpretrained\thidden\tlayers\tof\tthe\tprevious\tmodel,\tfreezes\n\nthem,\tand\treplaces\tthe\tsoftmax\toutput\tlayer\twith\ta\tnew\tone.\n\nb.\t Train\tthis\tnew\tDNN\ton\tdigits\t5\tto\t9,\tusing\tonly\t100\timages\tper\tdigit,\tand\ttime\thow\tlong\tit\n\ntakes.\tDespite\tthis\tsmall\tnumber\tof\texamples,\tcan\tyou\tachieve\thigh\tprecision?\n\nc.\t Try\tcaching\tthe\tfrozen\tlayers,\tand\ttrain\tthe\tmodel\tagain:\thow\tmuch\tfaster\tis\tit\tnow?\n\nd.\t Try\tagain\treusing\tjust\tfour\thidden\tlayers\tinstead\tof\tfive.\tCan\tyou\tachieve\ta\thigher\tprecision?",
      "content_length": 2083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "e.\t Now\tunfreeze\tthe\ttop\ttwo\thidden\tlayers\tand\tcontinue\ttraining:\tcan\tyou\tget\tthe\tmodel\tto\tperform\n\neven\tbetter?\n\n10.\t Pretraining\ton\tan\tauxiliary\ttask.\n\na.\t In\tthis\texercise\tyou\twill\tbuild\ta\tDNN\tthat\tcompares\ttwo\tMNIST\tdigit\timages\tand\tpredicts whether\tthey\trepresent\tthe\tsame\tdigit\tor\tnot.\tThen\tyou\twill\treuse\tthe\tlower\tlayers\tof\tthis network\tto\ttrain\tan\tMNIST\tclassifier\tusing\tvery\tlittle\ttraining\tdata.\tStart\tby\tbuilding\ttwo\tDNNs (let’s\tcall\tthem\tDNN\tA\tand\tB),\tboth\tsimilar\tto\tthe\tone\tyou\tbuilt\tearlier\tbut\twithout\tthe\toutput layer:\teach\tDNN\tshould\thave\tfive\thidden\tlayers\tof\t100\tneurons\teach,\tHe\tinitialization,\tand ELU\tactivation.\tNext,\tadd\tone\tmore\thidden\tlayer\twith\t10\tunits\ton\ttop\tof\tboth\tDNNs.\tTo\tdo\tthis, you\tshould\tuse\tTensorFlow’s\tconcat()\tfunction\twith\taxis=1\tto\tconcatenate\tthe\toutputs\tof\tboth DNNs\tfor\teach\tinstance,\tthen\tfeed\tthe\tresult\tto\tthe\thidden\tlayer.\tFinally,\tadd\tan\toutput\tlayer with\ta\tsingle\tneuron\tusing\tthe\tlogistic\tactivation\tfunction.\n\nb.\t Split\tthe\tMNIST\ttraining\tset\tin\ttwo\tsets:\tsplit\t#1\tshould\tcontaining\t55,000\timages,\tand\tsplit\t#2 should\tcontain\tcontain\t5,000\timages.\tCreate\ta\tfunction\tthat\tgenerates\ta\ttraining\tbatch\twhere each\tinstance\tis\ta\tpair\tof\tMNIST\timages\tpicked\tfrom\tsplit\t#1.\tHalf\tof\tthe\ttraining\tinstances should\tbe\tpairs\tof\timages\tthat\tbelong\tto\tthe\tsame\tclass,\twhile\tthe\tother\thalf\tshould\tbe\timages from\tdifferent\tclasses.\tFor\teach\tpair,\tthe\ttraining\tlabel\tshould\tbe\t0\tif\tthe\timages\tare\tfrom\tthe same\tclass,\tor\t1\tif\tthey\tare\tfrom\tdifferent\tclasses.\n\nc.\t Train\tthe\tDNN\ton\tthis\ttraining\tset.\tFor\teach\timage\tpair,\tyou\tcan\tsimultaneously\tfeed\tthe\tfirst image\tto\tDNN\tA\tand\tthe\tsecond\timage\tto\tDNN\tB.\tThe\twhole\tnetwork\twill\tgradually\tlearn\tto tell\twhether\ttwo\timages\tbelong\tto\tthe\tsame\tclass\tor\tnot.\n\nd.\t Now\tcreate\ta\tnew\tDNN\tby\treusing\tand\tfreezing\tthe\thidden\tlayers\tof\tDNN\tA\tand\tadding\ta\n\nsoftmax\toutput\tlayer\ton\ttop\twith\t10\tneurons.\tTrain\tthis\tnetwork\ton\tsplit\t#2\tand\tsee\tif\tyou\tcan achieve\thigh\tperformance\tdespite\thaving\tonly\t500\timages\tper\tclass.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“Understanding\tthe\tDifficulty\tof\tTraining\tDeep\tFeedforward\tNeural\tNetworks,”\tX.\tGlorot,\tY\tBengio\t(2010).\n\n2\n\nHere’s\tan\tanalogy:\tif\tyou\tset\ta\tmicrophone\tamplifier’s\tknob\ttoo\tclose\tto\tzero,\tpeople\twon’t\thear\tyour\tvoice,\tbut\tif\tyou\tset\tit\ttoo\tclose\tto the\tmax,\tyour\tvoice\twill\tbe\tsaturated\tand\tpeople\twon’t\tunderstand\twhat\tyou\tare\tsaying.\tNow\timagine\ta\tchain\tof\tsuch\tamplifiers:\tthey\tall need\tto\tbe\tset\tproperly\tin\torder\tfor\tyour\tvoice\tto\tcome\tout\tloud\tand\tclear\tat\tthe\tend\tof\tthe\tchain.\tYour\tvoice\thas\tto\tcome\tout\tof\teach amplifier\tat\tthe\tsame\tamplitude\tas\tit\tcame\tin.\n\n3\n\nThis\tsimplified\tstrategy\twas\tactually\talready\tproposed\tmuch\tearlier\t—\tfor\texample,\tin\tthe\t1998\tbook\tNeural\tNetworks:\tTricks\tof\tthe Trade\tby\tGenevieve\tOrr\tand\tKlaus-Robert\tMüller\t(Springer).\n\n4\n\nSuch\tas\t“Delving\tDeep\tinto\tRectifiers:\tSurpassing\tHuman-Level\tPerformance\ton\tImageNet\tClassification,”\tK.\tHe\tet\tal.\t(2015).\n\n5\n\n“Empirical\tEvaluation\tof\tRectified\tActivations\tin\tConvolution\tNetwork,”\tB.\tXu\tet\tal.\t(2015).\n\n6\n\n“Fast\tand\tAccurate\tDeep\tNetwork\tLearning\tby\tExponential\tLinear\tUnits\t(ELUs),”\tD.\tClevert,\tT.\tUnterthiner,\tS.\tHochreiter\t(2015).\n\n7\n\n“Batch\tNormalization:\tAccelerating\tDeep\tNetwork\tTraining\tby\tReducing\tInternal\tCovariate\tShift,”\tS.\tIoffe\tand\tC.\tSzegedy\t(2015).\n\n8\n\nMany\tresearchers\targue\tthat\tit\tis\tjust\tas\tgood,\tor\teven\tbetter,\tto\tplace\tthe\tbatch\tnormalization\tlayers\tafter\t(rather\tthan\tbefore)\tthe activations.\n\n9\n\n“On\tthe\tdifficulty\tof\ttraining\trecurrent\tneural\tnetworks,”\tR.\tPascanu\tet\tal.\t(2013).\n\n10\n\nAnother\toption\tis\tto\tcome\tup\twith\ta\tsupervised\ttask\tfor\twhich\tyou\tcan\teasily\tgather\ta\tlot\tof\tlabeled\ttraining\tdata,\tthen\tuse\ttransfer",
      "content_length": 3688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "11\n\n12\n\n2\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\nlearning,\tas\texplained\tearlier.\tFor\texample,\tif\tyou\twant\tto\ttrain\ta\tmodel\tto\tidentify\tyour\tfriends\tin\tpictures,\tyou\tcould\tdownload\tmillions\tof faces\ton\tthe\tinternet\tand\ttrain\ta\tclassifier\tto\tdetect\twhether\ttwo\tfaces\tare\tidentical\tor\tnot,\tthen\tuse\tthis\tclassifier\tto\tcompare\ta\tnew picture\twith\teach\tpicture\tof\tyour\tfriends.\n\n“Some\tmethods\tof\tspeeding\tup\tthe\tconvergence\tof\titeration\tmethods,”\tB.\tPolyak\t(1964).\n\n“A\tMethod\tfor\tUnconstrained\tConvex\tMinimization\tProblem\twith\tthe\tRate\tof\tConvergence\tO(1/k\n\n),”\tYurii\tNesterov\t(1983).\n\n“Adaptive\tSubgradient\tMethods\tfor\tOnline\tLearning\tand\tStochastic\tOptimization,”\tJ.\tDuchi\tet\tal.\t(2011).\n\nThis\talgorithm\twas\tcreated\tby\tTijmen\tTieleman\tand\tGeoffrey\tHinton\tin\t2012,\tand\tpresented\tby\tGeoffrey\tHinton\tin\this\tCoursera\tclass\ton neural\tnetworks\t(slides:\thttp://goo.gl/RsQeis;\tvideo:\thttps://goo.gl/XUbIyJ).\tAmusingly,\tsince\tthe\tauthors\thave\tnot\twritten\ta\tpaper\tto describe\tit,\tresearchers\toften\tcite\t“slide\t29\tin\tlecture\t6”\tin\ttheir\tpapers.\n\n“Adam:\tA\tMethod\tfor\tStochastic\tOptimization,”\tD.\tKingma,\tJ.\tBa\t(2015).\n\nThese\tare\testimations\tof\tthe\tmean\tand\t(uncentered)\tvariance\tof\tthe\tgradients.\tThe\tmean\tis\toften\tcalled\tthe\tfirst\tmoment,\twhile\tthe variance\tis\toften\tcalled\tthe\tsecond\tmoment,\thence\tthe\tname\tof\tthe\talgorithm.\n\n“The\tMarginal\tValue\tof\tAdaptive\tGradient\tMethods\tin\tMachine\tLearning,”\tA.\tC.\tWilson\tet\tal.\t(2017).\n\n“Primal-Dual\tSubgradient\tMethods\tfor\tConvex\tProblems,”\tYurii\tNesterov\t(2005).\n\n“Ad\tClick\tPrediction:\ta\tView\tfrom\tthe\tTrenches,”\tH.\tMcMahan\tet\tal.\t(2013).\n\n“An\tEmpirical\tStudy\tof\tLearning\tRates\tin\tDeep\tNeural\tNetworks\tfor\tSpeech\tRecognition,”\tA.\tSenior\tet\tal.\t(2013).\n\n“Improving\tneural\tnetworks\tby\tpreventing\tco-adaptation\tof\tfeature\tdetectors,”\tG.\tHinton\tet\tal.\t(2012).\n\n“Dropout:\tA\tSimple\tWay\tto\tPrevent\tNeural\tNetworks\tfrom\tOverfitting,”\tN.\tSrivastava\tet\tal.\t(2014).",
      "content_length": 1883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "Chapter\t12.\tDistributing\tTensorFlow\tAcross Devices\tand\tServers\n\nIn\tChapter\t11\twe\tdiscussed\tseveral\ttechniques\tthat\tcan\tconsiderably\tspeed\tup\ttraining:\tbetter\tweight initialization,\tBatch\tNormalization,\tsophisticated\toptimizers,\tand\tso\ton.\tHowever,\teven\twith\tall\tof\tthese techniques,\ttraining\ta\tlarge\tneural\tnetwork\ton\ta\tsingle\tmachine\twith\ta\tsingle\tCPU\tcan\ttake\tdays\tor\teven weeks.\n\nIn\tthis\tchapter\twe\twill\tsee\thow\tto\tuse\tTensorFlow\tto\tdistribute\tcomputations\tacross\tmultiple\tdevices (CPUs\tand\tGPUs)\tand\trun\tthem\tin\tparallel\t(see\tFigure\t12-1).\tFirst\twe\twill\tdistribute\tcomputations across\tmultiple\tdevices\ton\tjust\tone\tmachine,\tthen\ton\tmultiple\tdevices\tacross\tmultiple\tmachines.\n\nFigure\t12-1.\tExecuting\ta\tTensorFlow\tgraph\tacross\tmultiple\tdevices\tin\tparallel\n\nTensorFlow’s\tsupport\tof\tdistributed\tcomputing\tis\tone\tof\tits\tmain\thighlights\tcompared\tto\tother\tneural network\tframeworks.\tIt\tgives\tyou\tfull\tcontrol\tover\thow\tto\tsplit\t(or\treplicate)\tyour\tcomputation\tgraph across\tdevices\tand\tservers,\tand\tit\tlets\tyou\tparallelize\tand\tsynchronize\toperations\tin\tflexible\tways\tso\tyou can\tchoose\tbetween\tall\tsorts\tof\tparallelization\tapproaches.\n\nWe\twill\tlook\tat\tsome\tof\tthe\tmost\tpopular\tapproaches\tto\tparallelizing\tthe\texecution\tand\ttraining\tof\ta neural\tnetwork.\tInstead\tof\twaiting\tfor\tweeks\tfor\ta\ttraining\talgorithm\tto\tcomplete,\tyou\tmay\tend\tup\twaiting for\tjust\ta\tfew\thours.\tNot\tonly\tdoes\tthis\tsave\tan\tenormous\tamount\tof\ttime,\tit\talso\tmeans\tthat\tyou\tcan experiment\twith\tvarious\tmodels\tmuch\tmore\teasily,\tand\tfrequently\tretrain\tyour\tmodels\ton\tfresh\tdata.\n\nOther\tgreat\tuse\tcases\tof\tparallelization\tinclude\texploring\ta\tmuch\tlarger\thyperparameter\tspace\twhen\tfine- tuning\tyour\tmodel,\tand\trunning\tlarge\tensembles\tof\tneural\tnetworks\tefficiently.",
      "content_length": 1720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "But\twe\tmust\tlearn\tto\twalk\tbefore\twe\tcan\trun.\tLet’s\tstart\tby\tparallelizing\tsimple\tgraphs\tacross\tseveral GPUs\ton\ta\tsingle\tmachine.",
      "content_length": 128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "Multiple\tDevices\ton\ta\tSingle\tMachine You\tcan\toften\tget\ta\tmajor\tperformance\tboost\tsimply\tby\tadding\tGPU\tcards\tto\ta\tsingle\tmachine.\tIn\tfact,\tin many\tcases\tthis\twill\tsuffice;\tyou\twon’t\tneed\tto\tuse\tmultiple\tmachines\tat\tall.\tFor\texample,\tyou\tcan typically\ttrain\ta\tneural\tnetwork\tjust\tas\tfast\tusing\t8\tGPUs\ton\ta\tsingle\tmachine\trather\tthan\t16\tGPUs\tacross multiple\tmachines\t(due\tto\tthe\textra\tdelay\timposed\tby\tnetwork\tcommunications\tin\ta\tmultimachine\tsetup).\n\nIn\tthis\tsection\twe\twill\tlook\tat\thow\tto\tset\tup\tyour\tenvironment\tso\tthat\tTensorFlow\tcan\tuse\tmultiple\tGPU cards\ton\tone\tmachine.\tThen\twe\twill\tlook\tat\thow\tyou\tcan\tdistribute\toperations\tacross\tavailable\tdevices and\texecute\tthem\tin\tparallel.",
      "content_length": 683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "Installation In\torder\tto\trun\tTensorFlow\ton\tmultiple\tGPU\tcards,\tyou\tfirst\tneed\tto\tmake\tsure\tyour\tGPU\tcards\thave NVidia\tCompute\tCapability\t(greater\tor\tequal\tto\t3.0).\tThis\tincludes\tNvidia’s\tTitan,\tTitan\tX,\tK20,\tand K40\tcards\t(if\tyou\town\tanother\tcard,\tyou\tcan\tcheck\tits\tcompatibility\tat https://developer.nvidia.com/cuda-gpus).\n\nTIP\n\nIf\tyou\tdon’t\town\tany\tGPU\tcards,\tyou\tcan\tuse\ta\thosting\tservice\twith\tGPU\tcapability\tsuch\tas\tAmazon\tAWS.\tDetailed\tinstructions to\tset\tup\tTensorFlow\t0.9\twith\tPython\t3.5\ton\tan\tAmazon\tAWS\tGPU\tinstance\tare\tavailable\tin\tŽiga\tAvsec’s\thelpful\tblog\tpost.\tIt should\tnot\tbe\ttoo\thard\tto\tupdate\tit\tto\tthe\tlatest\tversion\tof\tTensorFlow.\tGoogle\talso\treleased\ta\tcloud\tservice\tcalled\tCloud Machine\tLearning\tto\trun\tTensorFlow\tgraphs.\tIn\tMay\t2016,\tthey\tannounced\tthat\ttheir\tplatform\tnow\tincludes\tservers\tequipped with\ttensor\tprocessing\tunits\t(TPUs),\tprocessors\tspecialized\tfor\tMachine\tLearning\tthat\tare\tmuch\tfaster\tthan\tGPUs\tfor\tmany ML\ttasks.\tOf\tcourse,\tanother\toption\tis\tsimply\tto\tbuy\tyour\town\tGPU\tcard.\tTim\tDettmers\twrote\ta\tgreat\tblog\tpost\tto\thelp\tyou choose,\tand\the\tupdates\tit\tfairly\tregularly.\n\nYou\tmust\tthen\tdownload\tand\tinstall\tthe\tappropriate\tversion\tof\tthe\tCUDA\tand\tcuDNN\tlibraries\t(CUDA 8.0\tand\tcuDNN\t5.1\tif\tyou\tare\tusing\tthe\tbinary\tinstallation\tof\tTensorFlow\t1.0.0),\tand\tset\ta\tfew environment\tvariables\tso\tTensorFlow\tknows\twhere\tto\tfind\tCUDA\tand\tcuDNN.\tThe\tdetailed\tinstallation instructions\tare\tlikely\tto\tchange\tfairly\tquickly,\tso\tit\tis\tbest\tthat\tyou\tfollow\tthe\tinstructions\ton TensorFlow’s\twebsite.\n\nNvidia’s\tCompute\tUnified\tDevice\tArchitecture\tlibrary\t(CUDA)\tallows\tdevelopers\tto\tuse\tCUDA- enabled\tGPUs\tfor\tall\tsorts\tof\tcomputations\t(not\tjust\tgraphics\tacceleration).\tNvidia’s\tCUDA\tDeep\tNeural Network\tlibrary\t(cuDNN)\tis\ta\tGPU-accelerated\tlibrary\tof\tprimitives\tfor\tDNNs.\tIt\tprovides\toptimized implementations\tof\tcommon\tDNN\tcomputations\tsuch\tas\tactivation\tlayers,\tnormalization,\tforward\tand backward\tconvolutions,\tand\tpooling\t(see\tChapter\t13).\tIt\tis\tpart\tof\tNvidia’s\tDeep\tLearning\tSDK\t(note that\tit\trequires\tcreating\tan\tNvidia\tdeveloper\taccount\tin\torder\tto\tdownload\tit).\tTensorFlow\tuses\tCUDA and\tcuDNN\tto\tcontrol\tthe\tGPU\tcards\tand\taccelerate\tcomputations\t(see\tFigure\t12-2).",
      "content_length": 2193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "Figure\t12-2.\tTensorFlow\tuses\tCUDA\tand\tcuDNN\tto\tcontrol\tGPUs\tand\tboost\tDNNs\n\nYou\tcan\tuse\tthe\tnvidia-smi\tcommand\tto\tcheck\tthat\tCUDA\tis\tproperly\tinstalled.\tIt\tlists\tthe\tavailable GPU\tcards,\tas\twell\tas\tprocesses\trunning\ton\teach\tcard:\n\n$\tnvidia-smi Wed\tSep\t16\t09:50:03\t2016 +------------------------------------------------------+ |\tNVIDIA-SMI\t352.63\t\t\t\t\tDriver\tVersion:\t352.63\t\t\t\t\t\t\t\t\t| |-------------------------------+----------------------+----------------------+ |\tGPU\t\tName\t\t\t\t\t\t\t\tPersistence-M|\tBus-Id\t\t\t\t\t\t\t\tDisp.A\t|\tVolatile\tUncorr.\tECC\t| |\tFan\t\tTemp\t\tPerf\t\tPwr:Usage/Cap|\t\t\t\t\t\t\t\t\tMemory-Usage\t|\tGPU-Util\t\tCompute\tM.\t| |===============================+======================+======================| |\t\t\t0\t\tGRID\tK520\t\t\t\t\t\t\t\t\t\t\tOff\t\t|\t0000:00:03.0\t\t\t\t\tOff\t|\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tN/A\t| |\tN/A\t\t\t27C\t\t\t\tP8\t\t\t\t17W\t/\t125W\t|\t\t\t\t\t11MiB\t/\t\t4095MiB\t|\t\t\t\t\t\t0%\t\t\t\t\t\tDefault\t| +-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+ |\tProcesses:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tGPU\tMemory\t| |\t\tGPU\t\t\t\t\t\t\tPID\t\tType\t\tProcess\tname\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tUsage\t\t\t\t\t\t| |=============================================================================| |\t\tNo\trunning\tprocesses\tfound\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t| +-----------------------------------------------------------------------------+\n\nFinally,\tyou\tmust\tinstall\tTensorFlow\twith\tGPU\tsupport.\tIf\tyou\tcreated\tan\tisolated\tenvironment\tusing virtualenv,\tyou\tfirst\tneed\tto\tactivate\tit:\n\n$\tcd\t$ML_PATH\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\tYour\tML\tworking\tdirectory\t(e.g.,\t$HOME/ml) $\tsource\tenv/bin/activate\n\nThen\tinstall\tthe\tappropriate\tGPU-enabled\tversion\tof\tTensorFlow:\n\n$\tpip3\tinstall\t--upgrade\ttensorflow-gpu",
      "content_length": 1773,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "Now\tyou\tcan\topen\tup\ta\tPython\tshell\tand\tcheck\tthat\tTensorFlow\tdetects\tand\tuses\tCUDA\tand\tcuDNN properly\tby\timporting\tTensorFlow\tand\tcreating\ta\tsession:\n\n>>>\timport\ttensorflow\tas\ttf I\t[...]/dso_loader.cc:108]\tsuccessfully\topened\tCUDA\tlibrary\tlibcublas.so\tlocally I\t[...]/dso_loader.cc:108]\tsuccessfully\topened\tCUDA\tlibrary\tlibcudnn.so\tlocally I\t[...]/dso_loader.cc:108]\tsuccessfully\topened\tCUDA\tlibrary\tlibcufft.so\tlocally I\t[...]/dso_loader.cc:108]\tsuccessfully\topened\tCUDA\tlibrary\tlibcuda.so.1\tlocally I\t[...]/dso_loader.cc:108]\tsuccessfully\topened\tCUDA\tlibrary\tlibcurand.so\tlocally >>>\tsess\t=\ttf.Session() [...] I\t[...]/gpu_init.cc:102]\tFound\tdevice\t0\twith\tproperties: name:\tGRID\tK520 major:\t3\tminor:\t0\tmemoryClockRate\t(GHz)\t0.797 pciBusID\t0000:00:03.0 Total\tmemory:\t4.00GiB Free\tmemory:\t3.95GiB I\t[...]/gpu_init.cc:126]\tDMA:\t0 I\t[...]/gpu_init.cc:136]\t0:\t\t\tY I\t[...]/gpu_device.cc:839]\tCreating\tTensorFlow\tdevice (/gpu:0)\t->\t(device:\t0,\tname:\tGRID\tK520,\tpci\tbus\tid:\t0000:00:03.0)\n\nLooks\tgood!\tTensorFlow\tdetected\tthe\tCUDA\tand\tcuDNN\tlibraries,\tand\tit\tused\tthe\tCUDA\tlibrary\tto detect\tthe\tGPU\tcard\t(in\tthis\tcase\tan\tNvidia\tGrid\tK520\tcard).",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "Managing\tthe\tGPU\tRAM By\tdefault\tTensorFlow\tautomatically\tgrabs\tall\tthe\tRAM\tin\tall\tavailable\tGPUs\tthe\tfirst\ttime\tyou\trun\ta graph,\tso\tyou\twill\tnot\tbe\table\tto\tstart\ta\tsecond\tTensorFlow\tprogram\twhile\tthe\tfirst\tone\tis\tstill\trunning.\tIf you\ttry,\tyou\twill\tget\tthe\tfollowing\terror:\n\nE\t[...]/cuda_driver.cc:965]\tfailed\tto\tallocate\t3.66G\t(3928915968\tbytes)\tfrom device:\tCUDA_ERROR_OUT_OF_MEMORY\n\nOne\tsolution\tis\tto\trun\teach\tprocess\ton\tdifferent\tGPU\tcards.\tTo\tdo\tthis,\tthe\tsimplest\toption\tis\tto\tset\tthe CUDA_VISIBLE_DEVICES\tenvironment\tvariable\tso\tthat\teach\tprocess\tonly\tsees\tthe\tappropriate\tGPU\tcards. For\texample,\tyou\tcould\tstart\ttwo\tprograms\tlike\tthis:\n\n$\tCUDA_VISIBLE_DEVICES=0,1\tpython3\tprogram_1.py #\tand\tin\tanother\tterminal: $\tCUDA_VISIBLE_DEVICES=3,2\tpython3\tprogram_2.py\n\nProgram\t#1\twill\tonly\tsee\tGPU\tcards\t0\tand\t1\t(numbered\t0\tand\t1,\trespectively),\tand\tprogram\t#2\twill\tonly see\tGPU\tcards\t2\tand\t3\t(numbered\t1\tand\t0,\trespectively).\tEverything\twill\twork\tfine\t(see\tFigure\t12-3).\n\nFigure\t12-3.\tEach\tprogram\tgets\ttwo\tGPUs\tfor\titself\n\nAnother\toption\tis\tto\ttell\tTensorFlow\tto\tgrab\tonly\ta\tfraction\tof\tthe\tmemory.\tFor\texample,\tto\tmake TensorFlow\tgrab\tonly\t40%\tof\teach\tGPU’s\tmemory,\tyou\tmust\tcreate\ta\tConfigProto\tobject,\tset\tits gpu_options.per_process_gpu_memory_fraction\toption\tto\t0.4,\tand\tcreate\tthe\tsession\tusing\tthis configuration:\n\nconfig\t=\ttf.ConfigProto() config.gpu_options.per_process_gpu_memory_fraction\t=\t0.4 session\t=\ttf.Session(config=config)\n\nNow\ttwo\tprograms\tlike\tthis\tone\tcan\trun\tin\tparallel\tusing\tthe\tsame\tGPU\tcards\t(but\tnot\tthree,\tsince\t3\t× 0.4\t>\t1).\tSee\tFigure\t12-4.",
      "content_length": 1573,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "Figure\t12-4.\tEach\tprogram\tgets\tall\tfour\tGPUs,\tbut\twith\tonly\t40%\tof\tthe\tRAM\teach\n\nIf\tyou\trun\tthe\tnvidia-smi\tcommand\twhile\tboth\tprograms\tare\trunning,\tyou\tshould\tsee\tthat\teach\tprocess holds\troughly\t40%\tof\tthe\ttotal\tRAM\tof\teach\tcard:\n\n$\tnvidia-smi [...] +-----------------------------------------------------------------------------+ |\tProcesses:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tGPU\tMemory\t| |\t\tGPU\t\t\t\t\t\t\tPID\t\tType\t\tProcess\tname\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tUsage\t\t\t\t\t\t| |=============================================================================| |\t\t\t\t0\t\t\t\t\t\t5231\t\t\t\tC\t\t\tpython\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1677MiB\t| |\t\t\t\t0\t\t\t\t\t\t5262\t\t\t\tC\t\t\tpython\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1677MiB\t| |\t\t\t\t1\t\t\t\t\t\t5231\t\t\t\tC\t\t\tpython\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1677MiB\t| |\t\t\t\t1\t\t\t\t\t\t5262\t\t\t\tC\t\t\tpython\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1677MiB\t| [...]\n\nYet\tanother\toption\tis\tto\ttell\tTensorFlow\tto\tgrab\tmemory\tonly\twhen\tit\tneeds\tit.\tTo\tdo\tthis\tyou\tmust\tset config.gpu_options.allow_growth\tto\tTrue.\tHowever,\tTensorFlow\tnever\treleases\tmemory\tonce\tit has\tgrabbed\tit\t(to\tavoid\tmemory\tfragmentation)\tso\tyou\tmay\tstill\trun\tout\tof\tmemory\tafter\ta\twhile.\tIt\tmay be\tharder\tto\tguarantee\ta\tdeterministic\tbehavior\tusing\tthis\toption,\tso\tin\tgeneral\tyou\tprobably\twant\tto\tstick with\tone\tof\tthe\tprevious\toptions.\n\nOkay,\tnow\tyou\thave\ta\tworking\tGPU-enabled\tTensorFlow\tinstallation.\tLet’s\tsee\thow\tto\tuse\tit!",
      "content_length": 1428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "Placing\tOperations\ton\tDevices The\tTensorFlow\twhitepaper1\tpresents\ta\tfriendly\tdynamic\tplacer\talgorithm\tthat\tautomagically\tdistributes operations\tacross\tall\tavailable\tdevices,\ttaking\tinto\taccount\tthings\tlike\tthe\tmeasured\tcomputation\ttime\tin previous\truns\tof\tthe\tgraph,\testimations\tof\tthe\tsize\tof\tthe\tinput\tand\toutput\ttensors\tto\teach\toperation,\tthe amount\tof\tRAM\tavailable\tin\teach\tdevice,\tcommunication\tdelay\twhen\ttransferring\tdata\tin\tand\tout\tof devices,\thints\tand\tconstraints\tfrom\tthe\tuser,\tand\tmore.\tUnfortunately,\tthis\tsophisticated\talgorithm\tis internal\tto\tGoogle;\tit\twas\tnot\treleased\tin\tthe\topen\tsource\tversion\tof\tTensorFlow.\tThe\treason\tit\twas\tleft out\tseems\tto\tbe\tthat\tin\tpractice\ta\tsmall\tset\tof\tplacement\trules\tspecified\tby\tthe\tuser\tactually\tresults\tin more\tefficient\tplacement\tthan\twhat\tthe\tdynamic\tplacer\tis\tcapable\tof.\tHowever,\tthe\tTensorFlow\tteam\tis working\ton\timproving\tthe\tdynamic\tplacer,\tand\tperhaps\tit\twill\teventually\tbe\tgood\tenough\tto\tbe\treleased.\n\nUntil\tthen\tTensorFlow\trelies\ton\tthe\tsimple\tplacer,\twhich\t(as\tits\tname\tsuggests)\tis\tvery\tbasic.\n\nSimple\tplacement\n\nWhenever\tyou\trun\ta\tgraph,\tif\tTensorFlow\tneeds\tto\tevaluate\ta\tnode\tthat\tis\tnot\tplaced\ton\ta\tdevice\tyet,\tit uses\tthe\tsimple\tplacer\tto\tplace\tit,\talong\twith\tall\tother\tnodes\tthat\tare\tnot\tplaced\tyet.\tThe\tsimple\tplacer respects\tthe\tfollowing\trules:\n\nIf\ta\tnode\twas\talready\tplaced\ton\ta\tdevice\tin\ta\tprevious\trun\tof\tthe\tgraph,\tit\tis\tleft\ton\tthat\tdevice.\n\nElse,\tif\tthe\tuser\tpinned\ta\tnode\tto\ta\tdevice\t(described\tnext),\tthe\tplacer\tplaces\tit\ton\tthat\tdevice.\n\nElse,\tit\tdefaults\tto\tGPU\t#0,\tor\tthe\tCPU\tif\tthere\tis\tno\tGPU.\n\nAs\tyou\tcan\tsee,\tplacing\toperations\ton\tthe\tappropriate\tdevice\tis\tmostly\tup\tto\tyou.\tIf\tyou\tdon’t\tdo\tanything, the\twhole\tgraph\twill\tbe\tplaced\ton\tthe\tdefault\tdevice.\tTo\tpin\tnodes\tonto\ta\tdevice,\tyou\tmust\tcreate\ta device\tblock\tusing\tthe\tdevice()\tfunction.\tFor\texample,\tthe\tfollowing\tcode\tpins\tthe\tvariable\ta\tand\tthe constant\tb\ton\tthe\tCPU,\tbut\tthe\tmultiplication\tnode\tc\tis\tnot\tpinned\ton\tany\tdevice,\tso\tit\twill\tbe\tplaced\ton the\tdefault\tdevice:\n\nwith\ttf.device(\"/cpu:0\"): \t\t\t\ta\t=\ttf.Variable(3.0) \t\t\t\tb\t=\ttf.constant(4.0)\n\nc\t=\ta\t*\tb\n\nNOTE\n\nThe\t\"/cpu:0\"\tdevice\taggregates\tall\tCPUs\ton\ta\tmulti-CPU\tsystem.\tThere\tis\tcurrently\tno\tway\tto\tpin\tnodes\ton\tspecific\tCPUs\tor to\tuse\tjust\ta\tsubset\tof\tall\tCPUs.\n\nLogging\tplacements\n\nLet’s\tcheck\tthat\tthe\tsimple\tplacer\trespects\tthe\tplacement\tconstraints\twe\thave\tjust\tdefined.\tFor\tthis\tyou can\tset\tthe\tlog_device_placement\toption\tto\tTrue;\tthis\ttells\tthe\tplacer\tto\tlog\ta\tmessage\twhenever\tit",
      "content_length": 2491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "places\ta\tnode.\tFor\texample:\n\n>>>\tconfig\t=\ttf.ConfigProto() >>>\tconfig.log_device_placement\t=\tTrue >>>\tsess\t=\ttf.Session(config=config) I\t[...]\tCreating\tTensorFlow\tdevice\t(/gpu:0)\t->\t(device:\t0,\tname:\tGRID\tK520, pci\tbus\tid:\t0000:00:03.0) [...] >>>\tx.initializer.run(session=sess) I\t[...]\ta:\t/job:localhost/replica:0/task:0/cpu:0 I\t[...]\ta/read:\t/job:localhost/replica:0/task:0/cpu:0 I\t[...]\tmul:\t/job:localhost/replica:0/task:0/gpu:0 I\t[...]\ta/Assign:\t/job:localhost/replica:0/task:0/cpu:0 I\t[...]\tb:\t/job:localhost/replica:0/task:0/cpu:0 I\t[...]\ta/initial_value:\t/job:localhost/replica:0/task:0/cpu:0 >>>\tsess.run(c) 12\n\nThe\tlines\tstarting\twith\t\"I\"\tfor\tInfo\tare\tthe\tlog\tmessages.\tWhen\twe\tcreate\ta\tsession,\tTensorFlow\tlogs\ta message\tto\ttell\tus\tthat\tit\thas\tfound\ta\tGPU\tcard\t(in\tthis\tcase\tthe\tGrid\tK520\tcard).\tThen\tthe\tfirst\ttime\twe run\tthe\tgraph\t(in\tthis\tcase\twhen\tinitializing\tthe\tvariable\ta),\tthe\tsimple\tplacer\tis\trun\tand\tplaces\teach\tnode on\tthe\tdevice\tit\twas\tassigned\tto.\tAs\texpected,\tthe\tlog\tmessages\tshow\tthat\tall\tnodes\tare\tplaced\ton \"/cpu:0\"\texcept\tthe\tmultiplication\tnode,\twhich\tends\tup\ton\tthe\tdefault\tdevice\t\"/gpu:0\"\t(you\tcan\tsafely ignore\tthe\tprefix\t/job:localhost/replica:0/task:0\tfor\tnow;\twe\twill\ttalk\tabout\tit\tin\ta\tmoment). Notice\tthat\tthe\tsecond\ttime\twe\trun\tthe\tgraph\t(to\tcompute\tc),\tthe\tplacer\tis\tnot\tused\tsince\tall\tthe\tnodes TensorFlow\tneeds\tto\tcompute\tc\tare\talready\tplaced.\n\nDynamic\tplacement\tfunction\n\nWhen\tyou\tcreate\ta\tdevice\tblock,\tyou\tcan\tspecify\ta\tfunction\tinstead\tof\ta\tdevice\tname.\tTensorFlow\twill call\tthis\tfunction\tfor\teach\toperation\tit\tneeds\tto\tplace\tin\tthe\tdevice\tblock,\tand\tthe\tfunction\tmust\treturn\tthe name\tof\tthe\tdevice\tto\tpin\tthe\toperation\ton.\tFor\texample,\tthe\tfollowing\tcode\tpins\tall\tthe\tvariable\tnodes\tto \"/cpu:0\"\t(in\tthis\tcase\tjust\tthe\tvariable\ta)\tand\tall\tother\tnodes\tto\t\"/gpu:0\":\n\ndef\tvariables_on_cpu(op): \t\t\t\tif\top.type\t==\t\"Variable\": \t\t\t\t\t\t\t\treturn\t\"/cpu:0\" \t\t\t\telse: \t\t\t\t\t\t\t\treturn\t\"/gpu:0\"\n\nwith\ttf.device(variables_on_cpu): \t\t\t\ta\t=\ttf.Variable(3.0) \t\t\t\tb\t=\ttf.constant(4.0) \t\t\t\tc\t=\ta\t*\tb\n\nYou\tcan\teasily\timplement\tmore\tcomplex\talgorithms,\tsuch\tas\tpinning\tvariables\tacross\tGPUs\tin\ta\tround- robin\tfashion.\n\nOperations\tand\tkernels\n\nFor\ta\tTensorFlow\toperation\tto\trun\ton\ta\tdevice,\tit\tneeds\tto\thave\tan\timplementation\tfor\tthat\tdevice;\tthis\tis called\ta\tkernel.\tMany\toperations\thave\tkernels\tfor\tboth\tCPUs\tand\tGPUs,\tbut\tnot\tall\tof\tthem.\tFor\texample, TensorFlow\tdoes\tnot\thave\ta\tGPU\tkernel\tfor\tinteger\tvariables,\tso\tthe\tfollowing\tcode\twill\tfail\twhen TensorFlow\ttries\tto\tplace\tthe\tvariable\ti\ton\tGPU\t#0:",
      "content_length": 2527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": ">>>\twith\ttf.device(\"/gpu:0\"): ...\t\t\t\t\ti\t=\ttf.Variable(3) [...] >>>\tsess.run(i.initializer) Traceback\t(most\trecent\tcall\tlast): [...] tensorflow.python.framework.errors.InvalidArgumentError:\tCannot\tassign\ta\tdevice to\tnode\t'Variable':\tCould\tnot\tsatisfy\texplicit\tdevice\tspecification\n\nNote\tthat\tTensorFlow\tinfers\tthat\tthe\tvariable\tmust\tbe\tof\ttype\tint32\tsince\tthe\tinitialization\tvalue\tis\tan integer.\tIf\tyou\tchange\tthe\tinitialization\tvalue\tto\t3.0\tinstead\tof\t3,\tor\tif\tyou\texplicitly\tset dtype=tf.float32\twhen\tcreating\tthe\tvariable,\teverything\twill\twork\tfine.\n\nSoft\tplacement\n\nBy\tdefault,\tif\tyou\ttry\tto\tpin\tan\toperation\ton\ta\tdevice\tfor\twhich\tthe\toperation\thas\tno\tkernel,\tyou\tget\tthe exception\tshown\tearlier\twhen\tTensorFlow\ttries\tto\tplace\tthe\toperation\ton\tthe\tdevice.\tIf\tyou\tprefer TensorFlow\tto\tfall\tback\tto\tthe\tCPU\tinstead,\tyou\tcan\tset\tthe\tallow_soft_placement\tconfiguration\toption to\tTrue:\n\nwith\ttf.device(\"/gpu:0\"): \t\t\t\ti\t=\ttf.Variable(3)\n\nconfig\t=\ttf.ConfigProto() config.allow_soft_placement\t=\tTrue sess\t=\ttf.Session(config=config) sess.run(i.initializer)\t\t#\tthe\tplacer\truns\tand\tfalls\tback\tto\t/cpu:0\n\nSo\tfar\twe\thave\tdiscussed\thow\tto\tplace\tnodes\ton\tdifferent\tdevices.\tNow\tlet’s\tsee\thow\tTensorFlow\twill run\tthese\tnodes\tin\tparallel.",
      "content_length": 1226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "Parallel\tExecution When\tTensorFlow\truns\ta\tgraph,\tit\tstarts\tby\tfinding\tout\tthe\tlist\tof\tnodes\tthat\tneed\tto\tbe\tevaluated,\tand\tit counts\thow\tmany\tdependencies\teach\tof\tthem\thas.\tTensorFlow\tthen\tstarts\tevaluating\tthe\tnodes\twith\tzero dependencies\t(i.e.,\tsource\tnodes).\tIf\tthese\tnodes\tare\tplaced\ton\tseparate\tdevices,\tthey\tobviously\tget evaluated\tin\tparallel.\tIf\tthey\tare\tplaced\ton\tthe\tsame\tdevice,\tthey\tget\tevaluated\tin\tdifferent\tthreads,\tso\tthey may\trun\tin\tparallel\ttoo\t(in\tseparate\tGPU\tthreads\tor\tCPU\tcores).\n\nTensorFlow\tmanages\ta\tthread\tpool\ton\teach\tdevice\tto\tparallelize\toperations\t(see\tFigure\t12-5).\tThese\tare called\tthe\tinter-op\tthread\tpools.\tSome\toperations\thave\tmultithreaded\tkernels:\tthey\tcan\tuse\tother\tthread pools\t(one\tper\tdevice)\tcalled\tthe\tintra-op\tthread\tpools.\n\nFigure\t12-5.\tParallelized\texecution\tof\ta\tTensorFlow\tgraph\n\nFor\texample,\tin\tFigure\t12-5,\toperations\tA,\tB,\tand\tC\tare\tsource\tops,\tso\tthey\tcan\timmediately\tbe evaluated.\tOperations\tA\tand\tB\tare\tplaced\ton\tGPU\t#0,\tso\tthey\tare\tsent\tto\tthis\tdevice’s\tinter-op\tthread pool,\tand\timmediately\tevaluated\tin\tparallel.\tOperation\tA\thappens\tto\thave\ta\tmultithreaded\tkernel;\tits computations\tare\tsplit\tin\tthree\tparts,\twhich\tare\texecuted\tin\tparallel\tby\tthe\tintra-op\tthread\tpool.\tOperation C\tgoes\tto\tGPU\t#1’s\tinter-op\tthread\tpool.\n\nAs\tsoon\tas\toperation\tC\tfinishes,\tthe\tdependency\tcounters\tof\toperations\tD\tand\tE\twill\tbe\tdecremented\tand will\tboth\treach\t0,\tso\tboth\toperations\twill\tbe\tsent\tto\tthe\tinter-op\tthread\tpool\tto\tbe\texecuted.",
      "content_length": 1474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "TIP\n\nYou\tcan\tcontrol\tthe\tnumber\tof\tthreads\tper\tinter-op\tpool\tby\tsetting\tthe\tinter_op_parallelism_threads\toption.\tNote\tthat\tthe first\tsession\tyou\tstart\tcreates\tthe\tinter-op\tthread\tpools.\tAll\tother\tsessions\twill\tjust\treuse\tthem\tunless\tyou\tset\tthe use_per_session_threads\toption\tto\tTrue.\tYou\tcan\tcontrol\tthe\tnumber\tof\tthreads\tper\tintra-op\tpool\tby\tsetting\tthe intra_op_parallelism_threads\toption.",
      "content_length": 392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "Control\tDependencies In\tsome\tcases,\tit\tmay\tbe\twise\tto\tpostpone\tthe\tevaluation\tof\tan\toperation\teven\tthough\tall\tthe\toperations\tit depends\ton\thave\tbeen\texecuted.\tFor\texample,\tif\tit\tuses\ta\tlot\tof\tmemory\tbut\tits\tvalue\tis\tneeded\tonly\tmuch further\tin\tthe\tgraph,\tit\twould\tbe\tbest\tto\tevaluate\tit\tat\tthe\tlast\tmoment\tto\tavoid\tneedlessly\toccupying\tRAM that\tother\toperations\tmay\tneed.\tAnother\texample\tis\ta\tset\tof\toperations\tthat\tdepend\ton\tdata\tlocated\toutside of\tthe\tdevice.\tIf\tthey\tall\trun\tat\tthe\tsame\ttime,\tthey\tmay\tsaturate\tthe\tdevice’s\tcommunication\tbandwidth, and\tthey\twill\tend\tup\tall\twaiting\ton\tI/O.\tOther\toperations\tthat\tneed\tto\tcommunicate\tdata\twill\talso\tbe blocked.\tIt\twould\tbe\tpreferable\tto\texecute\tthese\tcommunication-heavy\toperations\tsequentially,\tallowing the\tdevice\tto\tperform\tother\toperations\tin\tparallel.\n\nTo\tpostpone\tevaluation\tof\tsome\tnodes,\ta\tsimple\tsolution\tis\tto\tadd\tcontrol\tdependencies.\tFor\texample, the\tfollowing\tcode\ttells\tTensorFlow\tto\tevaluate\tx\tand\ty\tonly\tafter\ta\tand\tb\thave\tbeen\tevaluated:\n\na\t=\ttf.constant(1.0) b\t=\ta\t+\t2.0\n\nwith\ttf.control_dependencies([a,\tb]): \t\t\t\tx\t=\ttf.constant(3.0) \t\t\t\ty\t=\ttf.constant(4.0)\n\nz\t=\tx\t+\ty\n\nObviously,\tsince\tz\tdepends\ton\tx\tand\ty,\tevaluating\tz\talso\timplies\twaiting\tfor\ta\tand\tb\tto\tbe\tevaluated, even\tthough\tit\tis\tnot\texplicitly\tin\tthe\tcontrol_dependencies()\tblock.\tAlso,\tsince\tb\tdepends\ton\ta,\twe could\tsimplify\tthe\tpreceding\tcode\tby\tjust\tcreating\ta\tcontrol\tdependency\ton\t[b]\tinstead\tof\t[a,\tb],\tbut\tin some\tcases\t“explicit\tis\tbetter\tthan\timplicit.”\n\nGreat!\tNow\tyou\tknow:\n\nHow\tto\tplace\toperations\ton\tmultiple\tdevices\tin\tany\tway\tyou\tplease\n\nHow\tthese\toperations\tget\texecuted\tin\tparallel\n\nHow\tto\tcreate\tcontrol\tdependencies\tto\toptimize\tparallel\texecution\n\nIt’s\ttime\tto\tdistribute\tcomputations\tacross\tmultiple\tservers!",
      "content_length": 1761,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "Multiple\tDevices\tAcross\tMultiple\tServers To\trun\ta\tgraph\tacross\tmultiple\tservers,\tyou\tfirst\tneed\tto\tdefine\ta\tcluster.\tA\tcluster\tis\tcomposed\tof\tone\tor more\tTensorFlow\tservers,\tcalled\ttasks,\ttypically\tspread\tacross\tseveral\tmachines\t(see\tFigure\t12-6).\tEach task\tbelongs\tto\ta\tjob.\tA\tjob\tis\tjust\ta\tnamed\tgroup\tof\ttasks\tthat\ttypically\thave\ta\tcommon\trole,\tsuch\tas keeping\ttrack\tof\tthe\tmodel\tparameters\t(such\ta\tjob\tis\tusually\tnamed\t\"ps\"\tfor\tparameter\tserver),\tor performing\tcomputations\t(such\ta\tjob\tis\tusually\tnamed\t\"worker\").\n\nFigure\t12-6.\tTensorFlow\tcluster\n\nThe\tfollowing\tcluster\tspecification\tdefines\ttwo\tjobs,\t\"ps\"\tand\t\"worker\",\tcontaining\tone\ttask\tand\ttwo tasks,\trespectively.\tIn\tthis\texample,\tmachine\tA\thosts\ttwo\tTensorFlow\tservers\t(i.e.,\ttasks),\tlistening\ton different\tports:\tone\tis\tpart\tof\tthe\t\"ps\"\tjob,\tand\tthe\tother\tis\tpart\tof\tthe\t\"worker\"\tjob.\tMachine\tB\tjust\thosts one\tTensorFlow\tserver,\tpart\tof\tthe\t\"worker\"\tjob.\n\ncluster_spec\t=\ttf.train.ClusterSpec({ \t\t\t\t\"ps\":\t[ \t\t\t\t\t\t\t\t\"machine-a.example.com:2221\",\t\t#\t/job:ps/task:0 \t\t\t\t], \t\t\t\t\"worker\":\t[ \t\t\t\t\t\t\t\t\"machine-a.example.com:2222\",\t\t#\t/job:worker/task:0 \t\t\t\t\t\t\t\t\"machine-b.example.com:2222\",\t\t#\t/job:worker/task:1 \t\t\t\t]})\n\nTo\tstart\ta\tTensorFlow\tserver,\tyou\tmust\tcreate\ta\tServer\tobject,\tpassing\tit\tthe\tcluster\tspecification\t(so\tit can\tcommunicate\twith\tother\tservers)\tand\tits\town\tjob\tname\tand\ttask\tnumber.\tFor\texample,\tto\tstart\tthe\tfirst worker\ttask,\tyou\twould\trun\tthe\tfollowing\tcode\ton\tmachine\tA:\n\nserver\t=\ttf.train.Server(cluster_spec,\tjob_name=\"worker\",\ttask_index=0)",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "It\tis\tusually\tsimpler\tto\tjust\trun\tone\ttask\tper\tmachine,\tbut\tthe\tprevious\texample\tdemonstrates\tthat TensorFlow\tallows\tyou\tto\trun\tmultiple\ttasks\ton\tthe\tsame\tmachine\tif\tyou\twant.2\tIf\tyou\thave\tseveral servers\ton\tone\tmachine,\tyou\twill\tneed\tto\tensure\tthat\tthey\tdon’t\tall\ttry\tto\tgrab\tall\tthe\tRAM\tof\tevery\tGPU, as\texplained\tearlier.\tFor\texample,\tin\tFigure\t12-6\tthe\t\"ps\"\ttask\tdoes\tnot\tsee\tthe\tGPU\tdevices,\tsince presumably\tits\tprocess\twas\tlaunched\twith\tCUDA_VISIBLE_DEVICES=\"\".\tNote\tthat\tthe\tCPU\tis\tshared\tby all\ttasks\tlocated\ton\tthe\tsame\tmachine.\n\nIf\tyou\twant\tthe\tprocess\tto\tdo\tnothing\tother\tthan\trun\tthe\tTensorFlow\tserver,\tyou\tcan\tblock\tthe\tmain\tthread by\ttelling\tit\tto\twait\tfor\tthe\tserver\tto\tfinish\tusing\tthe\tjoin()\tmethod\t(otherwise\tthe\tserver\twill\tbe\tkilled as\tsoon\tas\tyour\tmain\tthread\texits).\tSince\tthere\tis\tcurrently\tno\tway\tto\tstop\tthe\tserver,\tthis\twill\tactually block\tforever:\n\nserver.join()\t\t#\tblocks\tuntil\tthe\tserver\tstops\t(i.e.,\tnever)",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "Opening\ta\tSession Once\tall\tthe\ttasks\tare\tup\tand\trunning\t(doing\tnothing\tyet),\tyou\tcan\topen\ta\tsession\ton\tany\tof\tthe\tservers, from\ta\tclient\tlocated\tin\tany\tprocess\ton\tany\tmachine\t(even\tfrom\ta\tprocess\trunning\tone\tof\tthe\ttasks),\tand use\tthat\tsession\tlike\ta\tregular\tlocal\tsession.\tFor\texample:\n\na\t=\ttf.constant(1.0) b\t=\ta\t+\t2 c\t=\ta\t*\t3\n\nwith\ttf.Session(\"grpc://machine-b.example.com:2222\")\tas\tsess: \t\t\t\tprint(c.eval())\t\t#\t9.0\n\nThis\tclient\tcode\tfirst\tcreates\ta\tsimple\tgraph,\tthen\topens\ta\tsession\ton\tthe\tTensorFlow\tserver\tlocated\ton machine\tB\t(which\twe\twill\tcall\tthe\tmaster),\tand\tinstructs\tit\tto\tevaluate\tc.\tThe\tmaster\tstarts\tby\tplacing\tthe operations\ton\tthe\tappropriate\tdevices.\tIn\tthis\texample,\tsince\twe\tdid\tnot\tpin\tany\toperation\ton\tany\tdevice, the\tmaster\tsimply\tplaces\tthem\tall\ton\tits\town\tdefault\tdevice\t—\tin\tthis\tcase,\tmachine\tB’s\tGPU\tdevice. Then\tit\tjust\tevaluates\tc\tas\tinstructed\tby\tthe\tclient,\tand\tit\treturns\tthe\tresult.",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "The\tMaster\tand\tWorker\tServices The\tclient\tuses\tthe\tgRPC\tprotocol\t(Google\tRemote\tProcedure\tCall)\tto\tcommunicate\twith\tthe\tserver.\tThis is\tan\tefficient\topen\tsource\tframework\tto\tcall\tremote\tfunctions\tand\tget\ttheir\toutputs\tacross\ta\tvariety\tof platforms\tand\tlanguages.3\tIt\tis\tbased\ton\tHTTP2,\twhich\topens\ta\tconnection\tand\tleaves\tit\topen\tduring\tthe whole\tsession,\tallowing\tefficient\tbidirectional\tcommunication\tonce\tthe\tconnection\tis\testablished.\tData\tis transmitted\tin\tthe\tform\tof\tprotocol\tbuffers,\tanother\topen\tsource\tGoogle\ttechnology.\tThis\tis\ta\tlightweight binary\tdata\tinterchange\tformat.\n\nWARNING\n\nAll\tservers\tin\ta\tTensorFlow\tcluster\tmay\tcommunicate\twith\tany\tother\tserver\tin\tthe\tcluster,\tso\tmake\tsure\tto\topen\tthe\tappropriate ports\ton\tyour\tfirewall.\n\nEvery\tTensorFlow\tserver\tprovides\ttwo\tservices:\tthe\tmaster\tservice\tand\tthe\tworker\tservice.\tThe\tmaster service\tallows\tclients\tto\topen\tsessions\tand\tuse\tthem\tto\trun\tgraphs.\tIt\tcoordinates\tthe\tcomputations\tacross tasks,\trelying\ton\tthe\tworker\tservice\tto\tactually\texecute\tcomputations\ton\tother\ttasks\tand\tget\ttheir\tresults.\n\nThis\tarchitecture\tgives\tyou\ta\tlot\tof\tflexibility.\tOne\tclient\tcan\tconnect\tto\tmultiple\tservers\tby\topening multiple\tsessions\tin\tdifferent\tthreads.\tOne\tserver\tcan\thandle\tmultiple\tsessions\tsimultaneously\tfrom\tone\tor more\tclients.\tYou\tcan\trun\tone\tclient\tper\ttask\t(typically\twithin\tthe\tsame\tprocess),\tor\tjust\tone\tclient\tto control\tall\ttasks.\tAll\toptions\tare\topen.",
      "content_length": 1420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "Pinning\tOperations\tAcross\tTasks You\tcan\tuse\tdevice\tblocks\tto\tpin\toperations\ton\tany\tdevice\tmanaged\tby\tany\ttask,\tby\tspecifying\tthe\tjob name,\ttask\tindex,\tdevice\ttype,\tand\tdevice\tindex.\tFor\texample,\tthe\tfollowing\tcode\tpins\ta\tto\tthe\tCPU\tof\tthe first\ttask\tin\tthe\t\"ps\"\tjob\t(that’s\tthe\tCPU\ton\tmachine\tA),\tand\tit\tpins\tb\tto\tthe\tsecond\tGPU\tmanaged\tby\tthe first\ttask\tof\tthe\t\"worker\"\tjob\t(that’s\tGPU\t#1\ton\tmachine\tA).\tFinally,\tc\tis\tnot\tpinned\tto\tany\tdevice,\tso\tthe master\tplaces\tit\ton\tits\town\tdefault\tdevice\t(machine\tB’s\tGPU\t#0\tdevice).\n\nwith\ttf.device(\"/job:ps/task:0/cpu:0\") \t\t\t\ta\t=\ttf.constant(1.0)\n\nwith\ttf.device(\"/job:worker/task:0/gpu:1\") \t\t\t\tb\t=\ta\t+\t2\n\nc\t=\ta\t+\tb\n\nAs\tearlier,\tif\tyou\tomit\tthe\tdevice\ttype\tand\tindex,\tTensorFlow\twill\tdefault\tto\tthe\ttask’s\tdefault\tdevice;\tfor example,\tpinning\tan\toperation\tto\t\"/job:ps/task:0\"\twill\tplace\tit\ton\tthe\tdefault\tdevice\tof\tthe\tfirst\ttask\tof the\t\"ps\"\tjob\t(machine\tA’s\tCPU).\tIf\tyou\talso\tomit\tthe\ttask\tindex\t(e.g.,\t\"/job:ps\"),\tTensorFlow\tdefaults to\t\"/task:0\".\tIf\tyou\tomit\tthe\tjob\tname\tand\tthe\ttask\tindex,\tTensorFlow\tdefaults\tto\tthe\tsession’s\tmaster task.",
      "content_length": 1086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "Sharding\tVariables\tAcross\tMultiple\tParameter\tServers As\twe\twill\tsee\tshortly,\ta\tcommon\tpattern\twhen\ttraining\ta\tneural\tnetwork\ton\ta\tdistributed\tsetup\tis\tto\tstore the\tmodel\tparameters\ton\ta\tset\tof\tparameter\tservers\t(i.e.,\tthe\ttasks\tin\tthe\t\"ps\"\tjob)\twhile\tother\ttasks\tfocus on\tcomputations\t(i.e.,\tthe\ttasks\tin\tthe\t\"worker\"\tjob).\tFor\tlarge\tmodels\twith\tmillions\tof\tparameters,\tit\tis useful\tto\tshard\tthese\tparameters\tacross\tmultiple\tparameter\tservers,\tto\treduce\tthe\trisk\tof\tsaturating\ta single\tparameter\tserver’s\tnetwork\tcard.\tIf\tyou\twere\tto\tmanually\tpin\tevery\tvariable\tto\ta\tdifferent parameter\tserver,\tit\twould\tbe\tquite\ttedious.\tFortunately,\tTensorFlow\tprovides\tthe replica_device_setter()\tfunction,\twhich\tdistributes\tvariables\tacross\tall\tthe\t\"ps\"\ttasks\tin\ta\tround- robin\tfashion.\tFor\texample,\tthe\tfollowing\tcode\tpins\tfive\tvariables\tto\ttwo\tparameter\tservers:\n\nwith\ttf.device(tf.train.replica_device_setter(ps_tasks=2): \t\t\t\tv1\t=\ttf.Variable(1.0)\t\t#\tpinned\tto\t/job:ps/task:0 \t\t\t\tv2\t=\ttf.Variable(2.0)\t\t#\tpinned\tto\t/job:ps/task:1 \t\t\t\tv3\t=\ttf.Variable(3.0)\t\t#\tpinned\tto\t/job:ps/task:0 \t\t\t\tv4\t=\ttf.Variable(4.0)\t\t#\tpinned\tto\t/job:ps/task:1 \t\t\t\tv5\t=\ttf.Variable(5.0)\t\t#\tpinned\tto\t/job:ps/task:0\n\nInstead\tof\tpassing\tthe\tnumber\tof\tps_tasks,\tyou\tcan\tpass\tthe\tcluster\tspec\tcluster=cluster_spec\tand TensorFlow\twill\tsimply\tcount\tthe\tnumber\tof\ttasks\tin\tthe\t\"ps\"\tjob.\n\nIf\tyou\tcreate\tother\toperations\tin\tthe\tblock,\tbeyond\tjust\tvariables,\tTensorFlow\tautomatically\tpins\tthem\tto \"/job:worker\",\twhich\twill\tdefault\tto\tthe\tfirst\tdevice\tmanaged\tby\tthe\tfirst\ttask\tin\tthe\t\"worker\"\tjob.\tYou can\tpin\tthem\tto\tanother\tdevice\tby\tsetting\tthe\tworker_device\tparameter,\tbut\ta\tbetter\tapproach\tis\tto\tuse embedded\tdevice\tblocks.\tAn\tinner\tdevice\tblock\tcan\toverride\tthe\tjob,\ttask,\tor\tdevice\tdefined\tin\tan\touter block.\tFor\texample:\n\nwith\ttf.device(tf.train.replica_device_setter(ps_tasks=2)): \t\t\t\tv1\t=\ttf.Variable(1.0)\t\t#\tpinned\tto\t/job:ps/task:0\t(+\tdefaults\tto\t/cpu:0) \t\t\t\tv2\t=\ttf.Variable(2.0)\t\t#\tpinned\tto\t/job:ps/task:1\t(+\tdefaults\tto\t/cpu:0) \t\t\t\tv3\t=\ttf.Variable(3.0)\t\t#\tpinned\tto\t/job:ps/task:0\t(+\tdefaults\tto\t/cpu:0) \t\t\t\t[...] \t\t\t\ts\t=\tv1\t+\tv2\t\t\t\t\t\t\t\t\t\t\t\t#\tpinned\tto\t/job:worker\t(+\tdefaults\tto\ttask:0/gpu:0) \t\t\t\twith\ttf.device(\"/gpu:1\"): \t\t\t\t\t\t\t\tp1\t=\t2\t*\ts\t\t\t\t\t\t\t\t\t#\tpinned\tto\t/job:worker/gpu:1\t(+\tdefaults\tto\t/task:0) \t\t\t\t\t\t\t\twith\ttf.device(\"/task:1\"): \t\t\t\t\t\t\t\t\t\t\t\tp2\t=\t3\t*\ts\t\t\t\t\t#\tpinned\tto\t/job:worker/task:1/gpu:1\n\nNOTE\n\nThis\texample\tassumes\tthat\tthe\tparameter\tservers\tare\tCPU-only,\twhich\tis\ttypically\tthe\tcase\tsince\tthey\tonly\tneed\tto\tstore\tand communicate\tparameters,\tnot\tperform\tintensive\tcomputations.",
      "content_length": 2568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "Sharing\tState\tAcross\tSessions\tUsing\tResource\tContainers When\tyou\tare\tusing\ta\tplain\tlocal\tsession\t(not\tthe\tdistributed\tkind),\teach\tvariable’s\tstate\tis\tmanaged\tby the\tsession\titself;\tas\tsoon\tas\tit\tends,\tall\tvariable\tvalues\tare\tlost.\tMoreover,\tmultiple\tlocal\tsessions\tcannot share\tany\tstate,\teven\tif\tthey\tboth\trun\tthe\tsame\tgraph;\teach\tsession\thas\tits\town\tcopy\tof\tevery\tvariable\t(as we\tdiscussed\tin\tChapter\t9).\tIn\tcontrast,\twhen\tyou\tare\tusing\tdistributed\tsessions,\tvariable\tstate\tis managed\tby\tresource\tcontainers\tlocated\ton\tthe\tcluster\titself,\tnot\tby\tthe\tsessions.\tSo\tif\tyou\tcreate\ta variable\tnamed\tx\tusing\tone\tclient\tsession,\tit\twill\tautomatically\tbe\tavailable\tto\tany\tother\tsession\ton\tthe same\tcluster\t(even\tif\tboth\tsessions\tare\tconnected\tto\ta\tdifferent\tserver).\tFor\texample,\tconsider\tthe following\tclient\tcode:\n\n#\tsimple_client.py import\ttensorflow\tas\ttf import\tsys\n\nx\t=\ttf.Variable(0.0,\tname=\"x\") increment_x\t=\ttf.assign(x,\tx\t+\t1)\n\nwith\ttf.Session(sys.argv[1])\tas\tsess: \t\t\t\tif\tsys.argv[2:]==[\"init\"]: \t\t\t\t\t\t\t\tsess.run(x.initializer) \t\t\t\tsess.run(increment_x) \t\t\t\tprint(x.eval())\n\nLet’s\tsuppose\tyou\thave\ta\tTensorFlow\tcluster\tup\tand\trunning\ton\tmachines\tA\tand\tB,\tport\t2222.\tYou\tcould launch\tthe\tclient,\thave\tit\topen\ta\tsession\twith\tthe\tserver\ton\tmachine\tA,\tand\ttell\tit\tto\tinitialize\tthe\tvariable, increment\tit,\tand\tprint\tits\tvalue\tby\tlaunching\tthe\tfollowing\tcommand:\n\n$\tpython3\tsimple_client.py\tgrpc://machine-a.example.com:2222\tinit 1.0\n\nNow\tif\tyou\tlaunch\tthe\tclient\twith\tthe\tfollowing\tcommand,\tit\twill\tconnect\tto\tthe\tserver\ton\tmachine\tB\tand magically\treuse\tthe\tsame\tvariable\tx\t(this\ttime\twe\tdon’t\task\tthe\tserver\tto\tinitialize\tthe\tvariable):\n\n$\tpython3\tsimple_client.py\tgrpc://machine-b.example.com:2222 2.0\n\nThis\tfeature\tcuts\tboth\tways:\tit’s\tgreat\tif\tyou\twant\tto\tshare\tvariables\tacross\tmultiple\tsessions,\tbut\tif\tyou want\tto\trun\tcompletely\tindependent\tcomputations\ton\tthe\tsame\tcluster\tyou\twill\thave\tto\tbe\tcareful\tnot\tto use\tthe\tsame\tvariable\tnames\tby\taccident.\tOne\tway\tto\tensure\tthat\tyou\twon’t\thave\tname\tclashes\tis\tto\twrap all\tof\tyour\tconstruction\tphase\tinside\ta\tvariable\tscope\twith\ta\tunique\tname\tfor\teach\tcomputation,\tfor example:\n\nwith\ttf.variable_scope(\"my_problem_1\"): \t\t\t\t[...]\t#\tConstruction\tphase\tof\tproblem\t1\n\nA\tbetter\toption\tis\tto\tuse\ta\tcontainer\tblock:\n\nwith\ttf.container(\"my_problem_1\"): \t\t\t\t[...]\t#\tConstruction\tphase\tof\tproblem\t1",
      "content_length": 2339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "This\twill\tuse\ta\tcontainer\tdedicated\tto\tproblem\t#1,\tinstead\tof\tthe\tdefault\tone\t(whose\tname\tis\tan\tempty string\t\"\").\tOne\tadvantage\tis\tthat\tvariable\tnames\tremain\tnice\tand\tshort.\tAnother\tadvantage\tis\tthat\tyou\tcan easily\treset\ta\tnamed\tcontainer.\tFor\texample,\tthe\tfollowing\tcommand\twill\tconnect\tto\tthe\tserver\ton machine\tA\tand\task\tit\tto\treset\tthe\tcontainer\tnamed\t\"my_problem_1\",\twhich\twill\tfree\tall\tthe\tresources\tthis container\tused\t(and\talso\tclose\tall\tsessions\topen\ton\tthe\tserver).\tAny\tvariable\tmanaged\tby\tthis\tcontainer must\tbe\tinitialized\tbefore\tyou\tcan\tuse\tit\tagain:\n\ntf.Session.reset(\"grpc://machine-a.example.com:2222\",\t[\"my_problem_1\"])\n\nResource\tcontainers\tmake\tit\teasy\tto\tshare\tvariables\tacross\tsessions\tin\tflexible\tways.\tFor\texample, Figure\t12-7\tshows\tfour\tclients\trunning\tdifferent\tgraphs\ton\tthe\tsame\tcluster,\tbut\tsharing\tsome\tvariables. Clients\tA\tand\tB\tshare\tthe\tsame\tvariable\tx\tmanaged\tby\tthe\tdefault\tcontainer,\twhile\tclients\tC\tand\tD\tshare another\tvariable\tnamed\tx\tmanaged\tby\tthe\tcontainer\tnamed\t\"my_problem_1\".\tNote\tthat\tclient\tC\teven\tuses variables\tfrom\tboth\tcontainers.\n\nFigure\t12-7.\tResource\tcontainers\n\nResource\tcontainers\talso\ttake\tcare\tof\tpreserving\tthe\tstate\tof\tother\tstateful\toperations,\tnamely\tqueues\tand readers.\tLet’s\ttake\ta\tlook\tat\tqueues\tfirst.",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "Asynchronous\tCommunication\tUsing\tTensorFlow\tQueues Queues\tare\tanother\tgreat\tway\tto\texchange\tdata\tbetween\tmultiple\tsessions;\tfor\texample,\tone\tcommon\tuse case\tis\tto\thave\ta\tclient\tcreate\ta\tgraph\tthat\tloads\tthe\ttraining\tdata\tand\tpushes\tit\tinto\ta\tqueue,\twhile\tanother client\tcreates\ta\tgraph\tthat\tpulls\tthe\tdata\tfrom\tthe\tqueue\tand\ttrains\ta\tmodel\t(see\tFigure\t12-8).\tThis\tcan speed\tup\ttraining\tconsiderably\tbecause\tthe\ttraining\toperations\tdon’t\thave\tto\twait\tfor\tthe\tnext\tmini-batch at\tevery\tstep.\n\nFigure\t12-8.\tUsing\tqueues\tto\tload\tthe\ttraining\tdata\tasynchronously\n\nTensorFlow\tprovides\tvarious\tkinds\tof\tqueues.\tThe\tsimplest\tkind\tis\tthe\tfirst-in\tfirst-out\t(FIFO)\tqueue. For\texample,\tthe\tfollowing\tcode\tcreates\ta\tFIFO\tqueue\tthat\tcan\tstore\tup\tto\t10\ttensors\tcontaining\ttwo\tfloat values\teach:\n\nq\t=\ttf.FIFOQueue(capacity=10,\tdtypes=[tf.float32],\tshapes=[[2]], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"q\",\tshared_name=\"shared_q\")\n\nWARNING\n\nTo\tshare\tvariables\tacross\tsessions,\tall\tyou\thad\tto\tdo\twas\tto\tspecify\tthe\tsame\tname\tand\tcontainer\ton\tboth\tends.\tWith\tqueues TensorFlow\tdoes\tnot\tuse\tthe\tname\tattribute\tbut\tinstead\tuses\tshared_name,\tso\tit\tis\timportant\tto\tspecify\tit\t(even\tif\tit\tis\tthe\tsame\tas the\tname).\tAnd,\tof\tcourse,\tuse\tthe\tsame\tcontainer.\n\nEnqueuing\tdata\n\nTo\tpush\tdata\tto\ta\tqueue,\tyou\tmust\tcreate\tan\tenqueue\toperation.\tFor\texample,\tthe\tfollowing\tcode\tpushes three\ttraining\tinstances\tto\tthe\tqueue:\n\n#\ttraining_data_loader.py import\ttensorflow\tas\ttf\n\nq\t=\t[...] training_instance\t=\ttf.placeholder(tf.float32,\tshape=(2)) enqueue\t=\tq.enqueue([training_instance])\n\nwith\ttf.Session(\"grpc://machine-a.example.com:2222\")\tas\tsess: \t\t\tsess.run(enqueue,\tfeed_dict={training_instance:\t[1.,\t2.]})",
      "content_length": 1657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "sess.run(enqueue,\tfeed_dict={training_instance:\t[3.,\t4.]}) \t\t\tsess.run(enqueue,\tfeed_dict={training_instance:\t[5.,\t6.]})\n\nInstead\tof\tenqueuing\tinstances\tone\tby\tone,\tyou\tcan\tenqueue\tseveral\tat\ta\ttime\tusing\tan\tenqueue_many operation:\n\n[...] training_instances\t=\ttf.placeholder(tf.float32,\tshape=(None,\t2)) enqueue_many\t=\tq.enqueue([training_instances])\n\nwith\ttf.Session(\"grpc://machine-a.example.com:2222\")\tas\tsess: \t\t\tsess.run(enqueue_many, \t\t\t\t\t\t\t\t\t\t\t\tfeed_dict={training_instances:\t[[1.,\t2.],\t[3.,\t4.],\t[5.,\t6.]]})\n\nBoth\texamples\tenqueue\tthe\tsame\tthree\ttensors\tto\tthe\tqueue.\n\nDequeuing\tdata\n\nTo\tpull\tthe\tinstances\tout\tof\tthe\tqueue,\ton\tthe\tother\tend,\tyou\tneed\tto\tuse\ta\tdequeue\toperation:\n\n#\ttrainer.py import\ttensorflow\tas\ttf\n\nq\t=\t[...] dequeue\t=\tq.dequeue()\n\nwith\ttf.Session(\"grpc://machine-a.example.com:2222\")\tas\tsess: \t\t\tprint(sess.run(dequeue))\t\t#\t[1.,\t2.] \t\t\tprint(sess.run(dequeue))\t\t#\t[3.,\t4.] \t\t\tprint(sess.run(dequeue))\t\t#\t[5.,\t6.]\n\nIn\tgeneral\tyou\twill\twant\tto\tpull\ta\twhole\tmini-batch\tat\tonce,\tinstead\tof\tpulling\tjust\tone\tinstance\tat\ta\ttime. To\tdo\tso,\tyou\tmust\tuse\ta\tdequeue_many\toperation,\tspecifying\tthe\tmini-batch\tsize:\n\n[...] batch_size\t=\t2 dequeue_mini_batch=\tq.dequeue_many(batch_size)\n\nwith\ttf.Session(\"grpc://machine-a.example.com:2222\")\tas\tsess: \t\t\tprint(sess.run(dequeue_mini_batch))\t\t#\t[[1.,\t2.],\t[4.,\t5.]] \t\t\tprint(sess.run(dequeue_mini_batch))\t\t#\tblocked\twaiting\tfor\tanother\tinstance\n\nWhen\ta\tqueue\tis\tfull,\tthe\tenqueue\toperation\twill\tblock\tuntil\titems\tare\tpulled\tout\tby\ta\tdequeue\toperation. Similarly,\twhen\ta\tqueue\tis\tempty\t(or\tyou\tare\tusing\tdequeue_many()\tand\tthere\tare\tfewer\titems\tthan\tthe mini-batch\tsize),\tthe\tdequeue\toperation\twill\tblock\tuntil\tenough\titems\tare\tpushed\tinto\tthe\tqueue\tusing\tan enqueue\toperation.\n\nQueues\tof\ttuples\n\nEach\titem\tin\ta\tqueue\tcan\tbe\ta\ttuple\tof\ttensors\t(of\tvarious\ttypes\tand\tshapes)\tinstead\tof\tjust\ta\tsingle\ttensor. For\texample,\tthe\tfollowing\tqueue\tstores\tpairs\tof\ttensors,\tone\tof\ttype\tint32\tand\tshape\t(),\tand\tthe\tother\tof type\tfloat32\tand\tshape\t[3,2]:\n\nq\t=\ttf.FIFOQueue(capacity=10,\tdtypes=[tf.int32,\ttf.float32],\tshapes=[[],[3,2]], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"q\",\tshared_name=\"shared_q\")\n\nThe\tenqueue\toperation\tmust\tbe\tgiven\tpairs\tof\ttensors\t(note\tthat\teach\tpair\trepresents\tonly\tone\titem\tin\tthe",
      "content_length": 2243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "queue):\n\na\t=\ttf.placeholder(tf.int32,\tshape=()) b\t=\ttf.placeholder(tf.float32,\tshape=(3,\t2)) enqueue\t=\tq.enqueue((a,\tb))\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\tsess.run(enqueue,\tfeed_dict={a:\t10,\tb:[[1.,\t2.],\t[3.,\t4.],\t[5.,\t6.]]}) \t\t\t\tsess.run(enqueue,\tfeed_dict={a:\t11,\tb:[[2.,\t4.],\t[6.,\t8.],\t[0.,\t2.]]}) \t\t\t\tsess.run(enqueue,\tfeed_dict={a:\t12,\tb:[[3.,\t6.],\t[9.,\t2.],\t[5.,\t8.]]})\n\nOn\tthe\tother\tend,\tthe\tdequeue()\tfunction\tnow\tcreates\ta\tpair\tof\tdequeue\toperations:\n\ndequeue_a,\tdequeue_b\t=\tq.dequeue()\n\nIn\tgeneral,\tyou\tshould\trun\tthese\toperations\ttogether:\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\ta_val,\tb_val\t=\tsess.run([dequeue_a,\tdequeue_b]) \t\t\t\tprint(a_val)\t#\t10 \t\t\t\tprint(b_val)\t#\t[[1.,\t2.],\t[3.,\t4.],\t[5.,\t6.]]\n\nWARNING\n\nIf\tyou\trun\tdequeue_a\ton\tits\town,\tit\twill\tdequeue\ta\tpair\tand\treturn\tonly\tthe\tfirst\telement;\tthe\tsecond\telement\twill\tbe\tlost\t(and similarly,\tif\tyou\trun\tdequeue_b\ton\tits\town,\tthe\tfirst\telement\twill\tbe\tlost).\n\nThe\tdequeue_many()\tfunction\talso\treturns\ta\tpair\tof\toperations:\n\nbatch_size\t=\t2 dequeue_as,\tdequeue_bs\t=\tq.dequeue_many(batch_size)\n\nYou\tcan\tuse\tit\tas\tyou\twould\texpect:\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\ta,\tb\t=\tsess.run([dequeue_a,\tdequeue_b]) \t\t\t\tprint(a)\t#\t[10,\t11] \t\t\t\tprint(b)\t#\t[[[1.,\t2.],\t[3.,\t4.],\t[5.,\t6.]],\t[[2.,\t4.],\t[6.,\t8.],\t[0.,\t2.]]] \t\t\t\ta,\tb\t=\tsess.run([dequeue_a,\tdequeue_b])\t\t#\tblocked\twaiting\tfor\tanother\tpair\n\nClosing\ta\tqueue\n\nIt\tis\tpossible\tto\tclose\ta\tqueue\tto\tsignal\tto\tthe\tother\tsessions\tthat\tno\tmore\tdata\twill\tbe\tenqueued:\n\nclose_q\t=\tq.close()\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\t[...] \t\t\t\tsess.run(close_q)\n\nSubsequent\texecutions\tof\tenqueue\tor\tenqueue_many\toperations\twill\traise\tan\texception.\tBy\tdefault,\tany pending\tenqueue\trequest\twill\tbe\thonored,\tunless\tyou\tcall\tq.close(cancel_pending_enqueues=True).\n\nSubsequent\texecutions\tof\tdequeue\tor\tdequeue_many\toperations\twill\tcontinue\tto\tsucceed\tas\tlong\tas\tthere",
      "content_length": 1860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "are\titems\tin\tthe\tqueue,\tbut\tthey\twill\tfail\twhen\tthere\tare\tnot\tenough\titems\tleft\tin\tthe\tqueue.\tIf\tyou\tare\tusing a\tdequeue_many\toperation\tand\tthere\tare\ta\tfew\tinstances\tleft\tin\tthe\tqueue,\tbut\tfewer\tthan\tthe\tmini-batch size,\tthey\twill\tbe\tlost.\tYou\tmay\tprefer\tto\tuse\ta\tdequeue_up_to\toperation\tinstead;\tit\tbehaves\texactly\tlike dequeue_many\texcept\twhen\ta\tqueue\tis\tclosed\tand\tthere\tare\tfewer\tthan\tbatch_size\tinstances\tleft\tin\tthe queue,\tin\twhich\tcase\tit\tjust\treturns\tthem.\n\nRandomShuffleQueue\n\nTensorFlow\talso\tsupports\ta\tcouple\tmore\ttypes\tof\tqueues,\tincluding\tRandomShuffleQueue,\twhich\tcan\tbe used\tjust\tlike\ta\tFIFOQueue\texcept\tthat\titems\tare\tdequeued\tin\ta\trandom\torder.\tThis\tcan\tbe\tuseful\tto\tshuffle training\tinstances\tat\teach\tepoch\tduring\ttraining.\tFirst,\tlet’s\tcreate\tthe\tqueue:\n\nq\t=\ttf.RandomShuffleQueue(capacity=50,\tmin_after_dequeue=10, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdtypes=[tf.float32],\tshapes=[()], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"q\",\tshared_name=\"shared_q\")\n\nThe\tmin_after_dequeue\tspecifies\tthe\tminimum\tnumber\tof\titems\tthat\tmust\tremain\tin\tthe\tqueue\tafter\ta dequeue\toperation.\tThis\tensures\tthat\tthere\twill\tbe\tenough\tinstances\tin\tthe\tqueue\tto\thave\tenough randomness\t(once\tthe\tqueue\tis\tclosed,\tthe\tmin_after_dequeue\tlimit\tis\tignored).\tNow\tsuppose\tthat\tyou enqueued\t22\titems\tin\tthis\tqueue\t(floats\t1.\tto\t22.).\tHere\tis\thow\tyou\tcould\tdequeue\tthem:\n\ndequeue\t=\tq.dequeue_many(5)\n\nwith\ttf.Session([...])\tas\tsess: \t\t\tprint(sess.run(dequeue))\t#\t[\t20.\t\t15.\t\t11.\t\t12.\t\t\t4.]\t\t\t(17\titems\tleft) \t\t\tprint(sess.run(dequeue))\t#\t[\t\t5.\t\t13.\t\t\t6.\t\t\t0.\t\t17.]\t\t\t(12\titems\tleft) \t\t\tprint(sess.run(dequeue))\t#\t12\t-\t5\t<\t10:\tblocked\twaiting\tfor\t3\tmore\tinstances\n\nPaddingFifoQueue\n\nA\tPaddingFIFOQueue\tcan\talso\tbe\tused\tjust\tlike\ta\tFIFOQueue\texcept\tthat\tit\taccepts\ttensors\tof\tvariable sizes\talong\tany\tdimension\t(but\twith\ta\tfixed\trank).\tWhen\tyou\tare\tdequeuing\tthem\twith\ta\tdequeue_many\tor dequeue_up_to\toperation,\teach\ttensor\tis\tpadded\twith\tzeros\talong\tevery\tvariable\tdimension\tto\tmake\tit the\tsame\tsize\tas\tthe\tlargest\ttensor\tin\tthe\tmini-batch.\tFor\texample,\tyou\tcould\tenqueue\t2D\ttensors (matrices)\tof\tarbitrary\tsizes:\n\nq\t=\ttf.PaddingFIFOQueue(capacity=50,\tdtypes=[tf.float32],\tshapes=[(None,\tNone)] \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"q\",\tshared_name=\"shared_q\") v\t=\ttf.placeholder(tf.float32,\tshape=(None,\tNone)) enqueue\t=\tq.enqueue([v])\n\nwith\ttf.Session([...])\tas\tsess: \t\t\tsess.run(enqueue,\tfeed_dict={v:\t[[1.,\t2.],\t[3.,\t4.],\t[5.,\t6.]]})\t\t\t\t\t\t\t#\t3x2 \t\t\tsess.run(enqueue,\tfeed_dict={v:\t[[1.]]})\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\t1x1 \t\t\tsess.run(enqueue,\tfeed_dict={v:\t[[7.,\t8.,\t9.,\t5.],\t[6.,\t7.,\t8.,\t9.]]})\t#\t2x4\n\nIf\twe\tjust\tdequeue\tone\titem\tat\ta\ttime,\twe\tget\tthe\texact\tsame\ttensors\tthat\twere\tenqueued.\tBut\tif\twe dequeue\tseveral\titems\tat\ta\ttime\t(using\tdequeue_many()\tor\tdequeue_up_to()),\tthe\tqueue\tautomatically pads\tthe\ttensors\tappropriately.\tFor\texample,\tif\twe\tdequeue\tall\tthree\titems\tat\tonce,\tall\ttensors\twill\tbe padded\twith\tzeros\tto\tbecome\t3\t×\t4\ttensors,\tsince\tthe\tmaximum\tsize\tfor\tthe\tfirst\tdimension\tis\t3\t(first\titem) and\tthe\tmaximum\tsize\tfor\tthe\tsecond\tdimension\tis\t4\t(third\titem):",
      "content_length": 3043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": ">>>\tq\t=\t[...] >>>\tdequeue\t=\tq.dequeue_many(3) >>>\twith\ttf.Session([...])\tas\tsess: ...\t\t\t\t\tprint(sess.run(dequeue)) [[[\t1.\t\t2.\t\t0.\t\t0.] \t\t[\t3.\t\t4.\t\t0.\t\t0.] \t\t[\t5.\t\t6.\t\t0.\t\t0.]]\n\n[[\t1.\t\t0.\t\t0.\t\t0.] \t\t[\t0.\t\t0.\t\t0.\t\t0.] \t\t[\t0.\t\t0.\t\t0.\t\t0.]]\n\n[[\t7.\t\t8.\t\t9.\t\t5.] \t\t[\t6.\t\t7.\t\t8.\t\t9.] \t\t[\t0.\t\t0.\t\t0.\t\t0.]]]\n\nThis\ttype\tof\tqueue\tcan\tbe\tuseful\twhen\tyou\tare\tdealing\twith\tvariable\tlength\tinputs,\tsuch\tas\tsequences\tof words\t(see\tChapter\t14).\n\nOkay,\tnow\tlet’s\tpause\tfor\ta\tsecond:\tso\tfar\tyou\thave\tlearned\tto\tdistribute\tcomputations\tacross\tmultiple devices\tand\tservers,\tshare\tvariables\tacross\tsessions,\tand\tcommunicate\tasynchronously\tusing\tqueues. Before\tyou\tstart\ttraining\tneural\tnetworks,\tthough,\tthere’s\tone\tlast\ttopic\twe\tneed\tto\tdiscuss:\thow\tto efficiently\tload\ttraining\tdata.",
      "content_length": 763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "Loading\tData\tDirectly\tfrom\tthe\tGraph So\tfar\twe\thave\tassumed\tthat\tthe\tclients\twould\tload\tthe\ttraining\tdata\tand\tfeed\tit\tto\tthe\tcluster\tusing placeholders.\tThis\tis\tsimple\tand\tworks\tquite\twell\tfor\tsimple\tsetups,\tbut\tit\tis\trather\tinefficient\tsince\tit transfers\tthe\ttraining\tdata\tseveral\ttimes:\n\n1.\t From\tthe\tfilesystem\tto\tthe\tclient\n\n2.\t From\tthe\tclient\tto\tthe\tmaster\ttask\n\n3.\t Possibly\tfrom\tthe\tmaster\ttask\tto\tother\ttasks\twhere\tthe\tdata\tis\tneeded\n\nIt\tgets\tworse\tif\tyou\thave\tseveral\tclients\ttraining\tvarious\tneural\tnetworks\tusing\tthe\tsame\ttraining\tdata\t(for example,\tfor\thyperparameter\ttuning):\tif\tevery\tclient\tloads\tthe\tdata\tsimultaneously,\tyou\tmay\tend\tup\teven saturating\tyour\tfile\tserver\tor\tthe\tnetwork’s\tbandwidth.\n\nPreload\tthe\tdata\tinto\ta\tvariable\n\nFor\tdatasets\tthat\tcan\tfit\tin\tmemory,\ta\tbetter\toption\tis\tto\tload\tthe\ttraining\tdata\tonce\tand\tassign\tit\tto\ta variable,\tthen\tjust\tuse\tthat\tvariable\tin\tyour\tgraph.\tThis\tis\tcalled\tpreloading\tthe\ttraining\tset.\tThis\tway\tthe data\twill\tbe\ttransferred\tonly\tonce\tfrom\tthe\tclient\tto\tthe\tcluster\t(but\tit\tmay\tstill\tneed\tto\tbe\tmoved\taround from\ttask\tto\ttask\tdepending\ton\twhich\toperations\tneed\tit).\tThe\tfollowing\tcode\tshows\thow\tto\tload\tthe\tfull training\tset\tinto\ta\tvariable:\n\ntraining_set_init\t=\ttf.placeholder(tf.float32,\tshape=(None,\tn_features)) training_set\t=\ttf.Variable(training_set_init,\ttrainable=False,\tcollections=[], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"training_set\")\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\tdata\t=\t[...]\t\t#\tload\tthe\ttraining\tdata\tfrom\tthe\tdatastore \t\t\t\tsess.run(training_set.initializer,\tfeed_dict={training_set_init:\tdata})\n\nYou\tmust\tset\ttrainable=False\tso\tthe\toptimizers\tdon’t\ttry\tto\ttweak\tthis\tvariable.\tYou\tshould\talso\tset collections=[]\tto\tensure\tthat\tthis\tvariable\twon’t\tget\tadded\tto\tthe\tGraphKeys.GLOBAL_VARIABLES collection,\twhich\tis\tused\tfor\tsaving\tand\trestoring\tcheckpoints.\n\nNOTE\n\nThis\texample\tassumes\tthat\tall\tof\tyour\ttraining\tset\t(including\tthe\tlabels)\tconsists\tonly\tof\tfloat32\tvalues.\tIf\tthat’s\tnot\tthe\tcase,\tyou will\tneed\tone\tvariable\tper\ttype.\n\nReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph\n\nIf\tthe\ttraining\tset\tdoes\tnot\tfit\tin\tmemory,\ta\tgood\tsolution\tis\tto\tuse\treader\toperations:\tthese\tare\toperations capable\tof\treading\tdata\tdirectly\tfrom\tthe\tfilesystem.\tThis\tway\tthe\ttraining\tdata\tnever\tneeds\tto\tflow through\tthe\tclients\tat\tall.\tTensorFlow\tprovides\treaders\tfor\tvarious\tfile\tformats:\n\nCSV",
      "content_length": 2362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "Fixed-length\tbinary\trecords\n\nTensorFlow’s\town\tTFRecords\tformat,\tbased\ton\tprotocol\tbuffers\n\nLet’s\tlook\tat\ta\tsimple\texample\treading\tfrom\ta\tCSV\tfile\t(for\tother\tformats,\tplease\tcheck\tout\tthe\tAPI documentation).\tSuppose\tyou\thave\tfile\tnamed\tmy_test.csv\tthat\tcontains\ttraining\tinstances,\tand\tyou\twant to\tcreate\toperations\tto\tread\tit.\tSuppose\tit\thas\tthe\tfollowing\tcontent,\twith\ttwo\tfloat\tfeatures\tx1\tand\tx2\tand one\tinteger\ttarget\trepresenting\ta\tbinary\tclass:\n\nx1,\t\tx2,\t\ttarget 1.\t,\t2.\t,\t0 4.\t,\t5\t\t,\t1 7.\t,\t\t\t\t,\t0\n\nFirst,\tlet’s\tcreate\ta\tTextLineReader\tto\tread\tthis\tfile.\tA\tTextLineReader\topens\ta\tfile\t(once\twe\ttell\tit which\tone\tto\topen)\tand\treads\tlines\tone\tby\tone.\tIt\tis\ta\tstateful\toperation,\tlike\tvariables\tand\tqueues:\tit preserves\tits\tstate\tacross\tmultiple\truns\tof\tthe\tgraph,\tkeeping\ttrack\tof\twhich\tfile\tit\tis\tcurrently\treading\tand what\tits\tcurrent\tposition\tis\tin\tthis\tfile.\n\nreader\t=\ttf.TextLineReader(skip_header_lines=1)\n\nNext,\twe\tcreate\ta\tqueue\tthat\tthe\treader\twill\tpull\tfrom\tto\tknow\twhich\tfile\tto\tread\tnext.\tWe\talso\tcreate\tan enqueue\toperation\tand\ta\tplaceholder\tto\tpush\tany\tfilename\twe\twant\tto\tthe\tqueue,\tand\twe\tcreate\tan operation\tto\tclose\tthe\tqueue\tonce\twe\thave\tno\tmore\tfiles\tto\tread:\n\nfilename_queue\t=\ttf.FIFOQueue(capacity=10,\tdtypes=[tf.string],\tshapes=[()]) filename\t=\ttf.placeholder(tf.string) enqueue_filename\t=\tfilename_queue.enqueue([filename]) close_filename_queue\t=\tfilename_queue.close()\n\nNow\twe\tare\tready\tto\tcreate\ta\tread\toperation\tthat\twill\tread\tone\trecord\t(i.e.,\ta\tline)\tat\ta\ttime\tand\treturn\ta key/value\tpair.\tThe\tkey\tis\tthe\trecord’s\tunique\tidentifier\t—\ta\tstring\tcomposed\tof\tthe\tfilename,\ta\tcolon\t(:), and\tthe\tline\tnumber\t—\tand\tthe\tvalue\tis\tsimply\ta\tstring\tcontaining\tthe\tcontent\tof\tthe\tline:\n\nkey,\tvalue\t=\treader.read(filename_queue)\n\nWe\thave\tall\twe\tneed\tto\tread\tthe\tfile\tline\tby\tline!\tBut\twe\tare\tnot\tquite\tdone\tyet\t—\twe\tneed\tto\tparse\tthis string\tto\tget\tthe\tfeatures\tand\ttarget:\n\nx1,\tx2,\ttarget\t=\ttf.decode_csv(value,\trecord_defaults=[[-1.],\t[-1.],\t[-1]]) features\t=\ttf.stack([x1,\tx2])\n\nThe\tfirst\tline\tuses\tTensorFlow’s\tCSV\tparser\tto\textract\tthe\tvalues\tfrom\tthe\tcurrent\tline.\tThe\tdefault\tvalues are\tused\twhen\ta\tfield\tis\tmissing\t(in\tthis\texample\tthe\tthird\ttraining\tinstance’s\tx2\tfeature),\tand\tthey\tare\talso used\tto\tdetermine\tthe\ttype\tof\teach\tfield\t(in\tthis\tcase\ttwo\tfloats\tand\tone\tinteger).\n\nFinally,\twe\tcan\tpush\tthis\ttraining\tinstance\tand\tits\ttarget\tto\ta\tRandomShuffleQueue\tthat\twe\twill\tshare with\tthe\ttraining\tgraph\t(so\tit\tcan\tpull\tmini-batches\tfrom\tit),\tand\twe\tcreate\tan\toperation\tto\tclose\tthat\tqueue when\twe\tare\tdone\tpushing\tinstances\tto\tit:",
      "content_length": 2559,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "instance_queue\t=\ttf.RandomShuffleQueue( \t\t\t\tcapacity=10,\tmin_after_dequeue=2, \t\t\t\tdtypes=[tf.float32,\ttf.int32],\tshapes=[[2],[]], \t\t\t\tname=\"instance_q\",\tshared_name=\"shared_instance_q\") enqueue_instance\t=\tinstance_queue.enqueue([features,\ttarget]) close_instance_queue\t=\tinstance_queue.close()\n\nWow!\tThat\twas\ta\tlot\tof\twork\tjust\tto\tread\ta\tfile.\tPlus\twe\tonly\tcreated\tthe\tgraph,\tso\tnow\twe\tneed\tto\trun\tit:\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\tsess.run(enqueue_filename,\tfeed_dict={filename:\t\"my_test.csv\"}) \t\t\t\tsess.run(close_filename_queue) \t\t\t\ttry: \t\t\t\t\t\t\t\twhile\tTrue: \t\t\t\t\t\t\t\t\t\t\t\tsess.run(enqueue_instance) \t\t\t\texcept\ttf.errors.OutOfRangeError\tas\tex: \t\t\t\t\t\t\t\tpass\t#\tno\tmore\trecords\tin\tthe\tcurrent\tfile\tand\tno\tmore\tfiles\tto\tread \t\t\t\tsess.run(close_instance_queue)\n\nFirst\twe\topen\tthe\tsession,\tand\tthen\twe\tenqueue\tthe\tfilename\t\"my_test.csv\"\tand\timmediately\tclose\tthat queue\tsince\twe\twill\tnot\tenqueue\tany\tmore\tfilenames.\tThen\twe\trun\tan\tinfinite\tloop\tto\tenqueue\tinstances one\tby\tone.\tThe\tenqueue_instance\tdepends\ton\tthe\treader\treading\tthe\tnext\tline,\tso\tat\tevery\titeration\ta new\trecord\tis\tread\tuntil\tit\treaches\tthe\tend\tof\tthe\tfile.\tAt\tthat\tpoint\tit\ttries\tto\tread\tthe\tfilename\tqueue\tto know\twhich\tfile\tto\tread\tnext,\tand\tsince\tthe\tqueue\tis\tclosed\tit\tthrows\tan\tOutOfRangeError\texception\t(if we\tdid\tnot\tclose\tthe\tqueue,\tit\twould\tjust\tremain\tblocked\tuntil\twe\tpushed\tanother\tfilename\tor\tclosed\tthe queue).\tLastly,\twe\tclose\tthe\tinstance\tqueue\tso\tthat\tthe\ttraining\toperations\tpulling\tfrom\tit\twon’t\tget blocked\tforever.\tFigure\t12-9\tsummarizes\twhat\twe\thave\tlearned;\tit\trepresents\ta\ttypical\tgraph\tfor\treading training\tinstances\tfrom\ta\tset\tof\tCSV\tfiles.\n\nFigure\t12-9.\tA\tgraph\tdedicated\tto\treading\ttraining\tinstances\tfrom\tCSV\tfiles\n\nIn\tthe\ttraining\tgraph,\tyou\tneed\tto\tcreate\tthe\tshared\tinstance\tqueue\tand\tsimply\tdequeue\tmini-batches\tfrom it:\n\ninstance_queue\t=\ttf.RandomShuffleQueue([...],\tshared_name=\"shared_instance_q\") mini_batch_instances,\tmini_batch_targets\t=\tinstance_queue.dequeue_up_to(2) [...]\t#\tuse\tthe\tmini_batch\tinstances\tand\ttargets\tto\tbuild\tthe\ttraining\tgraph training_op\t=\t[...]\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\ttry: \t\t\t\t\t\t\t\tfor\tstep\tin\trange(max_steps): \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op) \t\t\t\texcept\ttf.errors.OutOfRangeError\tas\tex: \t\t\t\t\t\t\t\tpass\t#\tno\tmore\ttraining\tinstances",
      "content_length": 2273,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "In\tthis\texample,\tthe\tfirst\tmini-batch\twill\tcontain\tthe\tfirst\ttwo\tinstances\tof\tthe\tCSV\tfile,\tand\tthe\tsecond mini-batch\twill\tcontain\tthe\tlast\tinstance.\n\nWARNING\n\nTensorFlow\tqueues\tdon’t\thandle\tsparse\ttensors\twell,\tso\tif\tyour\ttraining\tinstances\tare\tsparse\tyou\tshould\tparse\tthe\trecords\tafter the\tinstance\tqueue.\n\nThis\tarchitecture\twill\tonly\tuse\tone\tthread\tto\tread\trecords\tand\tpush\tthem\tto\tthe\tinstance\tqueue.\tYou\tcan get\ta\tmuch\thigher\tthroughput\tby\thaving\tmultiple\tthreads\tread\tsimultaneously\tfrom\tmultiple\tfiles\tusing multiple\treaders.\tLet’s\tsee\thow.\n\nMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\nTo\thave\tmultiple\tthreads\tread\tinstances\tsimultaneously,\tyou\tcould\tcreate\tPython\tthreads\t(using\tthe threading\tmodule)\tand\tmanage\tthem\tyourself.\tHowever,\tTensorFlow\tprovides\tsome\ttools\tto\tmake\tthis simpler:\tthe\tCoordinator\tclass\tand\tthe\tQueueRunner\tclass.\n\nA\tCoordinator\tis\ta\tvery\tsimple\tobject\twhose\tsole\tpurpose\tis\tto\tcoordinate\tstopping\tmultiple\tthreads. First\tyou\tcreate\ta\tCoordinator:\n\ncoord\t=\ttf.train.Coordinator()\n\nThen\tyou\tgive\tit\tto\tall\tthreads\tthat\tneed\tto\tstop\tjointly,\tand\ttheir\tmain\tloop\tlooks\tlike\tthis:\n\nwhile\tnot\tcoord.should_stop(): \t\t\t\t[...]\t#\tdo\tsomething\n\nAny\tthread\tcan\trequest\tthat\tevery\tthread\tstop\tby\tcalling\tthe\tCoordinator’s\trequest_stop()\tmethod:\n\ncoord.request_stop()\n\nEvery\tthread\twill\tstop\tas\tsoon\tas\tit\tfinishes\tits\tcurrent\titeration.\tYou\tcan\twait\tfor\tall\tof\tthe\tthreads\tto finish\tby\tcalling\tthe\tCoordinator’s\tjoin()\tmethod,\tpassing\tit\tthe\tlist\tof\tthreads:\n\ncoord.join(list_of_threads)\n\nA\tQueueRunner\truns\tmultiple\tthreads\tthat\teach\trun\tan\tenqueue\toperation\trepeatedly,\tfilling\tup\ta\tqueue\tas fast\tas\tpossible.\tAs\tsoon\tas\tthe\tqueue\tis\tclosed,\tthe\tnext\tthread\tthat\ttries\tto\tpush\tan\titem\tto\tthe\tqueue\twill get\tan\tOutOfRangeError;\tthis\tthread\tcatches\tthe\terror\tand\timmediately\ttells\tother\tthreads\tto\tstop\tusing\ta Coordinator.\tThe\tfollowing\tcode\tshows\thow\tyou\tcan\tuse\ta\tQueueRunner\tto\thave\tfive\tthreads\treading instances\tsimultaneously\tand\tpushing\tthem\tto\tan\tinstance\tqueue:\n\n[...]\t#\tsame\tconstruction\tphase\tas\tearlier queue_runner\t=\ttf.train.QueueRunner(instance_queue,\t[enqueue_instance]\t*\t5)\n\nwith\ttf.Session()\tas\tsess:",
      "content_length": 2157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "sess.run(enqueue_filename,\tfeed_dict={filename:\t\"my_test.csv\"}) \t\t\t\tsess.run(close_filename_queue) \t\t\t\tcoord\t=\ttf.train.Coordinator() \t\t\t\tenqueue_threads\t=\tqueue_runner.create_threads(sess,\tcoord=coord,\tstart=True)\n\nThe\tfirst\tline\tcreates\tthe\tQueueRunner\tand\ttells\tit\tto\trun\tfive\tthreads,\tall\trunning\tthe\tsame enqueue_instance\toperation\trepeatedly.\tThen\twe\tstart\ta\tsession\tand\twe\tenqueue\tthe\tname\tof\tthe\tfiles\tto read\t(in\tthis\tcase\tjust\t\"my_test.csv\").\tNext\twe\tcreate\ta\tCoordinator\tthat\tthe\tQueueRunner\twill\tuse\tto stop\tgracefully,\tas\tjust\texplained.\tFinally,\twe\ttell\tthe\tQueueRunner\tto\tcreate\tthe\tthreads\tand\tstart\tthem. The\tthreads\twill\tread\tall\ttraining\tinstances\tand\tpush\tthem\tto\tthe\tinstance\tqueue,\tand\tthen\tthey\twill\tall\tstop gracefully.\n\nThis\twill\tbe\ta\tbit\tmore\tefficient\tthan\tearlier,\tbut\twe\tcan\tdo\tbetter.\tCurrently\tall\tthreads\tare\treading\tfrom the\tsame\tfile.\tWe\tcan\tmake\tthem\tread\tsimultaneously\tfrom\tseparate\tfiles\tinstead\t(assuming\tthe\ttraining data\tis\tsharded\tacross\tmultiple\tCSV\tfiles)\tby\tcreating\tmultiple\treaders\t(see\tFigure\t12-10).\n\nFigure\t12-10.\tReading\tsimultaneously\tfrom\tmultiple\tfiles\n\nFor\tthis\twe\tneed\tto\twrite\ta\tsmall\tfunction\tto\tcreate\ta\treader\tand\tthe\tnodes\tthat\twill\tread\tand\tpush\tone instance\tto\tthe\tinstance\tqueue:\n\ndef\tread_and_push_instance(filename_queue,\tinstance_queue): \t\t\t\treader\t=\ttf.TextLineReader(skip_header_lines=1) \t\t\t\tkey,\tvalue\t=\treader.read(filename_queue) \t\t\t\tx1,\tx2,\ttarget\t=\ttf.decode_csv(value,\trecord_defaults=[[-1.],\t[-1.],\t[-1]]) \t\t\t\tfeatures\t=\ttf.stack([x1,\tx2]) \t\t\t\tenqueue_instance\t=\tinstance_queue.enqueue([features,\ttarget]) \t\t\t\treturn\tenqueue_instance\n\nNext\twe\tdefine\tthe\tqueues:\n\nfilename_queue\t=\ttf.FIFOQueue(capacity=10,\tdtypes=[tf.string],\tshapes=[()]) filename\t=\ttf.placeholder(tf.string) enqueue_filename\t=\tfilename_queue.enqueue([filename]) close_filename_queue\t=\tfilename_queue.close()\n\ninstance_queue\t=\ttf.RandomShuffleQueue([...])\n\nAnd\tfinally\twe\tcreate\tthe\tQueueRunner,\tbut\tthis\ttime\twe\tgive\tit\ta\tlist\tof\tdifferent\tenqueue\toperations. Each\toperation\twill\tuse\ta\tdifferent\treader,\tso\tthe\tthreads\twill\tsimultaneously\tread\tfrom\tdifferent\tfiles:\n\nread_and_enqueue_ops\t=\t[ \t\t\t\tread_and_push_instance(filename_queue,\tinstance_queue)",
      "content_length": 2194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "for\ti\tin\trange(5)] queue_runner\t=\ttf.train.QueueRunner(instance_queue,\tread_and_enqueue_ops)\n\nThe\texecution\tphase\tis\tthen\tthe\tsame\tas\tbefore:\tfirst\tpush\tthe\tnames\tof\tthe\tfiles\tto\tread,\tthen\tcreate\ta Coordinator\tand\tcreate\tand\tstart\tthe\tQueueRunner\tthreads.\tThis\ttime\tall\tthreads\twill\tread\tfrom different\tfiles\tsimultaneously\tuntil\tall\tfiles\tare\tread\tentirely,\tand\tthen\tthe\tQueueRunner\twill\tclose\tthe instance\tqueue\tso\tthat\tother\tops\tpulling\tfrom\tit\tdon’t\tget\tblocked.\n\nOther\tconvenience\tfunctions\n\nTensorFlow\talso\toffers\ta\tfew\tconvenience\tfunctions\tto\tsimplify\tsome\tcommon\ttasks\twhen\treading training\tinstances.\tWe\twill\tgo\tover\tjust\ta\tfew\t(see\tthe\tAPI\tdocumentation\tfor\tthe\tfull\tlist).\n\nThe\tstring_input_producer()\ttakes\ta\t1D\ttensor\tcontaining\ta\tlist\tof\tfilenames,\tcreates\ta\tthread\tthat pushes\tone\tfilename\tat\ta\ttime\tto\tthe\tfilename\tqueue,\tand\tthen\tcloses\tthe\tqueue.\tIf\tyou\tspecify\ta\tnumber\tof epochs,\tit\twill\tcycle\tthrough\tthe\tfilenames\tonce\tper\tepoch\tbefore\tclosing\tthe\tqueue.\tBy\tdefault,\tit\tshuffles the\tfilenames\tat\teach\tepoch.\tIt\tcreates\ta\tQueueRunner\tto\tmanage\tits\tthread,\tand\tadds\tit\tto\tthe GraphKeys.QUEUE_RUNNERS\tcollection.\tTo\tstart\tevery\tQueueRunner\tin\tthat\tcollection,\tyou\tcan\tcall\tthe tf.train.start_queue_runners()\tfunction.\tNote\tthat\tif\tyou\tforget\tto\tstart\tthe\tQueueRunner,\tthe filename\tqueue\twill\tbe\topen\tand\tempty,\tand\tyour\treaders\twill\tbe\tblocked\tforever.\n\nThere\tare\ta\tfew\tother\tproducer\tfunctions\tthat\tsimilarly\tcreate\ta\tqueue\tand\ta\tcorresponding\tQueueRunner for\trunning\tan\tenqueue\toperation\t(e.g.,\tinput_producer(),\trange_input_producer(),\tand slice_input_producer()).\n\nThe\tshuffle_batch()\tfunction\ttakes\ta\tlist\tof\ttensors\t(e.g.,\t[features,\ttarget])\tand\tcreates:\n\nA\tRandomShuffleQueue\n\nA\tQueueRunner\tto\tenqueue\tthe\ttensors\tto\tthe\tqueue\t(added\tto\tthe\tGraphKeys.QUEUE_RUNNERS collection)\n\nA\tdequeue_many\toperation\tto\textract\ta\tmini-batch\tfrom\tthe\tqueue\n\nThis\tmakes\tit\teasy\tto\tmanage\tin\ta\tsingle\tprocess\ta\tmultithreaded\tinput\tpipeline\tfeeding\ta\tqueue\tand\ta training\tpipeline\treading\tmini-batches\tfrom\tthat\tqueue.\tAlso\tcheck\tout\tthe\tbatch(),\tbatch_join(),\tand shuffle_batch_join()\tfunctions\tthat\tprovide\tsimilar\tfunctionality.\n\nOkay!\tYou\tnow\thave\tall\tthe\ttools\tyou\tneed\tto\tstart\ttraining\tand\trunning\tneural\tnetworks\tefficiently\tacross multiple\tdevices\tand\tservers\ton\ta\tTensorFlow\tcluster.\tLet’s\treview\twhat\tyou\thave\tlearned:\n\nUsing\tmultiple\tGPU\tdevices\n\nSetting\tup\tand\tstarting\ta\tTensorFlow\tcluster\n\nDistributing\tcomputations\tacross\tmultiple\tdevices\tand\tservers\n\nSharing\tvariables\t(and\tother\tstateful\tops\tsuch\tas\tqueues\tand\treaders)\tacross\tsessions\tusing containers",
      "content_length": 2581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "Coordinating\tmultiple\tgraphs\tworking\tasynchronously\tusing\tqueues\n\nReading\tinputs\tefficiently\tusing\treaders,\tqueue\trunners,\tand\tcoordinators\n\nNow\tlet’s\tuse\tall\tof\tthis\tto\tparallelize\tneural\tnetworks!",
      "content_length": 198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "Parallelizing\tNeural\tNetworks\ton\ta\tTensorFlow\tCluster In\tthis\tsection,\tfirst\twe\twill\tlook\tat\thow\tto\tparallelize\tseveral\tneural\tnetworks\tby\tsimply\tplacing\teach one\ton\ta\tdifferent\tdevice.\tThen\twe\twill\tlook\tat\tthe\tmuch\ttrickier\tproblem\tof\ttraining\ta\tsingle\tneural network\tacross\tmultiple\tdevices\tand\tservers.",
      "content_length": 305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "One\tNeural\tNetwork\tper\tDevice The\tmost\ttrivial\tway\tto\ttrain\tand\trun\tneural\tnetworks\ton\ta\tTensorFlow\tcluster\tis\tto\ttake\tthe\texact\tsame code\tyou\twould\tuse\tfor\ta\tsingle\tdevice\ton\ta\tsingle\tmachine,\tand\tspecify\tthe\tmaster\tserver’s\taddress when\tcreating\tthe\tsession.\tThat’s\tit\t—\tyou’re\tdone!\tYour\tcode\twill\tbe\trunning\ton\tthe\tserver’s\tdefault device.\tYou\tcan\tchange\tthe\tdevice\tthat\twill\trun\tyour\tgraph\tsimply\tby\tputting\tyour\tcode’s\tconstruction phase\twithin\ta\tdevice\tblock.\n\nBy\trunning\tseveral\tclient\tsessions\tin\tparallel\t(in\tdifferent\tthreads\tor\tdifferent\tprocesses),\tconnecting\tthem to\tdifferent\tservers,\tand\tconfiguring\tthem\tto\tuse\tdifferent\tdevices,\tyou\tcan\tquite\teasily\ttrain\tor\trun\tmany neural\tnetworks\tin\tparallel,\tacross\tall\tdevices\tand\tall\tmachines\tin\tyour\tcluster\t(see\tFigure\t12-11).\tThe speedup\tis\talmost\tlinear.4\tTraining\t100\tneural\tnetworks\tacross\t50\tservers\twith\t2\tGPUs\teach\twill\tnot\ttake much\tlonger\tthan\ttraining\tjust\t1\tneural\tnetwork\ton\t1\tGPU.\n\nFigure\t12-11.\tTraining\tone\tneural\tnetwork\tper\tdevice\n\nThis\tsolution\tis\tperfect\tfor\thyperparameter\ttuning:\teach\tdevice\tin\tthe\tcluster\twill\ttrain\ta\tdifferent\tmodel with\tits\town\tset\tof\thyperparameters.\tThe\tmore\tcomputing\tpower\tyou\thave,\tthe\tlarger\tthe\thyperparameter space\tyou\tcan\texplore.\n\nIt\talso\tworks\tperfectly\tif\tyou\thost\ta\tweb\tservice\tthat\treceives\ta\tlarge\tnumber\tof\tqueries\tper\tsecond (QPS)\tand\tyou\tneed\tyour\tneural\tnetwork\tto\tmake\ta\tprediction\tfor\teach\tquery.\tSimply\treplicate\tthe\tneural network\tacross\tall\tdevices\ton\tthe\tcluster\tand\tdispatch\tqueries\tacross\tall\tdevices.\tBy\tadding\tmore\tservers you\tcan\thandle\tan\tunlimited\tnumber\tof\tQPS\t(however,\tthis\twill\tnot\treduce\tthe\ttime\tit\ttakes\tto\tprocess\ta single\trequest\tsince\tit\twill\tstill\thave\tto\twait\tfor\ta\tneural\tnetwork\tto\tmake\ta\tprediction).",
      "content_length": 1749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "NOTE\n\nAnother\toption\tis\tto\tserve\tyour\tneural\tnetworks\tusing\tTensorFlow\tServing.\tIt\tis\tan\topen\tsource\tsystem,\treleased\tby\tGoogle\tin February\t2016,\tdesigned\tto\tserve\ta\thigh\tvolume\tof\tqueries\tto\tMachine\tLearning\tmodels\t(typically\tbuilt\twith\tTensorFlow).\tIt handles\tmodel\tversioning,\tso\tyou\tcan\teasily\tdeploy\ta\tnew\tversion\tof\tyour\tnetwork\tto\tproduction,\tor\texperiment\twith\tvarious algorithms\twithout\tinterrupting\tyour\tservice,\tand\tit\tcan\tsustain\ta\theavy\tload\tby\tadding\tmore\tservers.\tFor\tmore\tdetails,\tcheck\tout https://tensorflow.github.io/serving/.",
      "content_length": 545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "In-Graph\tVersus\tBetween-Graph\tReplication You\tcan\talso\tparallelize\tthe\ttraining\tof\ta\tlarge\tensemble\tof\tneural\tnetworks\tby\tsimply\tplacing\tevery neural\tnetwork\ton\ta\tdifferent\tdevice\t(ensembles\twere\tintroduced\tin\tChapter\t7).\tHowever,\tonce\tyou\twant to\trun\tthe\tensemble,\tyou\twill\tneed\tto\taggregate\tthe\tindividual\tpredictions\tmade\tby\teach\tneural\tnetwork\tto produce\tthe\tensemble’s\tprediction,\tand\tthis\trequires\ta\tbit\tof\tcoordination.\n\nThere\tare\ttwo\tmajor\tapproaches\tto\thandling\ta\tneural\tnetwork\tensemble\t(or\tany\tother\tgraph\tthat\tcontains large\tchunks\tof\tindependent\tcomputations):\n\nYou\tcan\tcreate\tone\tbig\tgraph,\tcontaining\tevery\tneural\tnetwork,\teach\tpinned\tto\ta\tdifferent\tdevice, plus\tthe\tcomputations\tneeded\tto\taggregate\tthe\tindividual\tpredictions\tfrom\tall\tthe\tneural\tnetworks (see\tFigure\t12-12).\tThen\tyou\tjust\tcreate\tone\tsession\tto\tany\tserver\tin\tthe\tcluster\tand\tlet\tit\ttake\tcare\tof everything\t(including\twaiting\tfor\tall\tindividual\tpredictions\tto\tbe\tavailable\tbefore\taggregating\tthem). This\tapproach\tis\tcalled\tin-graph\treplication.\n\nFigure\t12-12.\tIn-graph\treplication\n\nAlternatively,\tyou\tcan\tcreate\tone\tseparate\tgraph\tfor\teach\tneural\tnetwork\tand\thandle\tsynchronization between\tthese\tgraphs\tyourself.\tThis\tapproach\tis\tcalled\tbetween-graph\treplication.\tOne\ttypical implementation\tis\tto\tcoordinate\tthe\texecution\tof\tthese\tgraphs\tusing\tqueues\t(see\tFigure\t12-13).\tA\tset of\tclients\thandles\tone\tneural\tnetwork\teach,\treading\tfrom\tits\tdedicated\tinput\tqueue,\tand\twriting\tto\tits dedicated\tprediction\tqueue.\tAnother\tclient\tis\tin\tcharge\tof\treading\tthe\tinputs\tand\tpushing\tthem\tto\tall the\tinput\tqueues\t(copying\tall\tinputs\tto\tevery\tqueue).\tFinally,\tone\tlast\tclient\tis\tin\tcharge\tof\treading one\tprediction\tfrom\teach\tprediction\tqueue\tand\taggregating\tthem\tto\tproduce\tthe\tensemble’s prediction.",
      "content_length": 1766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "Figure\t12-13.\tBetween-graph\treplication\n\nThese\tsolutions\thave\ttheir\tpros\tand\tcons.\tIn-graph\treplication\tis\tsomewhat\tsimpler\tto\timplement\tsince you\tdon’t\thave\tto\tmanage\tmultiple\tclients\tand\tmultiple\tqueues.\tHowever,\tbetween-graph\treplication\tis\ta bit\teasier\tto\torganize\tinto\twell-bounded\tand\teasy-to-test\tmodules.\tMoreover,\tit\tgives\tyou\tmore flexibility.\tFor\texample,\tyou\tcould\tadd\ta\tdequeue\ttimeout\tin\tthe\taggregator\tclient\tso\tthat\tthe\tensemble would\tnot\tfail\teven\tif\tone\tof\tthe\tneural\tnetwork\tclients\tcrashes\tor\tif\tone\tneural\tnetwork\ttakes\ttoo\tlong\tto produce\tits\tprediction.\tTensorFlow\tlets\tyou\tspecify\ta\ttimeout\twhen\tcalling\tthe\trun()\tfunction\tby\tpassing a\tRunOptions\twith\ttimeout_in_ms:\n\nwith\ttf.Session([...])\tas\tsess: \t\t\t\t[...] \t\t\t\trun_options\t=\ttf.RunOptions() \t\t\t\trun_options.timeout_in_ms\t=\t1000\t\t#\t1s\ttimeout \t\t\t\ttry: \t\t\t\t\t\t\t\tpred\t=\tsess.run(dequeue_prediction,\toptions=run_options) \t\t\t\texcept\ttf.errors.DeadlineExceededError\tas\tex: \t\t\t\t\t\t\t\t[...]\t#\tthe\tdequeue\toperation\ttimed\tout\tafter\t1s\n\nAnother\tway\tyou\tcan\tspecify\ta\ttimeout\tis\tto\tset\tthe\tsession’s\toperation_timeout_in_ms\tconfiguration option,\tbut\tin\tthis\tcase\tthe\trun()\tfunction\ttimes\tout\tif\tany\toperation\ttakes\tlonger\tthan\tthe\ttimeout\tdelay:\n\nconfig\t=\ttf.ConfigProto() config.operation_timeout_in_ms\t=\t1000\t\t#\t1s\ttimeout\tfor\tevery\toperation\n\nwith\ttf.Session([...],\tconfig=config)\tas\tsess: \t\t\t\t[...] \t\t\t\ttry: \t\t\t\t\t\t\t\tpred\t=\tsess.run(dequeue_prediction)",
      "content_length": 1418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "except\ttf.errors.DeadlineExceededError\tas\tex: \t\t\t\t\t\t\t\t[...]\t\t#\tthe\tdequeue\toperation\ttimed\tout\tafter\t1s",
      "content_length": 103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "Model\tParallelism So\tfar\twe\thave\trun\teach\tneural\tnetwork\ton\ta\tsingle\tdevice.\tWhat\tif\twe\twant\tto\trun\ta\tsingle\tneural network\tacross\tmultiple\tdevices?\tThis\trequires\tchopping\tyour\tmodel\tinto\tseparate\tchunks\tand\trunning each\tchunk\ton\ta\tdifferent\tdevice.\tThis\tis\tcalled\tmodel\tparallelism.\tUnfortunately,\tmodel\tparallelism\tturns out\tto\tbe\tpretty\ttricky,\tand\tit\treally\tdepends\ton\tthe\tarchitecture\tof\tyour\tneural\tnetwork.\tFor\tfully connected\tnetworks,\tthere\tis\tgenerally\tnot\tmuch\tto\tbe\tgained\tfrom\tthis\tapproach\t(see\tFigure\t12-14). Intuitively,\tit\tmay\tseem\tthat\tan\teasy\tway\tto\tsplit\tthe\tmodel\tis\tto\tplace\teach\tlayer\ton\ta\tdifferent\tdevice,\tbut this\tdoes\tnot\twork\tsince\teach\tlayer\tneeds\tto\twait\tfor\tthe\toutput\tof\tthe\tprevious\tlayer\tbefore\tit\tcan\tdo anything.\tSo\tperhaps\tyou\tcan\tslice\tit\tvertically\t—\tfor\texample,\twith\tthe\tleft\thalf\tof\teach\tlayer\ton\tone device,\tand\tthe\tright\tpart\ton\tanother\tdevice?\tThis\tis\tslightly\tbetter,\tsince\tboth\thalves\tof\teach\tlayer\tcan indeed\twork\tin\tparallel,\tbut\tthe\tproblem\tis\tthat\teach\thalf\tof\tthe\tnext\tlayer\trequires\tthe\toutput\tof\tboth halves,\tso\tthere\twill\tbe\ta\tlot\tof\tcross-device\tcommunication\t(represented\tby\tthe\tdashed\tarrows).\tThis\tis likely\tto\tcompletely\tcancel\tout\tthe\tbenefit\tof\tthe\tparallel\tcomputation,\tsince\tcross-device\tcommunication is\tslow\t(especially\tif\tit\tis\tacross\tseparate\tmachines).\n\nFigure\t12-14.\tSplitting\ta\tfully\tconnected\tneural\tnetwork\n\nHowever,\tas\twe\twill\tsee\tin\tChapter\t13,\tsome\tneural\tnetwork\tarchitectures,\tsuch\tas\tconvolutional\tneural networks,\tcontain\tlayers\tthat\tare\tonly\tpartially\tconnected\tto\tthe\tlower\tlayers,\tso\tit\tis\tmuch\teasier\tto distribute\tchunks\tacross\tdevices\tin\tan\tefficient\tway.",
      "content_length": 1641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "Figure\t12-15.\tSplitting\ta\tpartially\tconnected\tneural\tnetwork\n\nMoreover,\tas\twe\twill\tsee\tin\tChapter\t14,\tsome\tdeep\trecurrent\tneural\tnetworks\tare\tcomposed\tof\tseveral layers\tof\tmemory\tcells\t(see\tthe\tleft\tside\tof\tFigure\t12-16).\tA\tcell’s\toutput\tat\ttime\tt\tis\tfed\tback\tto\tits\tinput at\ttime\tt\t+\t1\t(as\tyou\tcan\tsee\tmore\tclearly\ton\tthe\tright\tside\tof\tFigure\t12-16).\tIf\tyou\tsplit\tsuch\ta\tnetwork horizontally,\tplacing\teach\tlayer\ton\ta\tdifferent\tdevice,\tthen\tat\tthe\tfirst\tstep\tonly\tone\tdevice\twill\tbe\tactive, at\tthe\tsecond\tstep\ttwo\twill\tbe\tactive,\tand\tby\tthe\ttime\tthe\tsignal\tpropagates\tto\tthe\toutput\tlayer\tall\tdevices will\tbe\tactive\tsimultaneously.\tThere\tis\tstill\ta\tlot\tof\tcross-device\tcommunication\tgoing\ton,\tbut\tsince\teach cell\tmay\tbe\tfairly\tcomplex,\tthe\tbenefit\tof\trunning\tmultiple\tcells\tin\tparallel\toften\toutweighs\tthe communication\tpenalty.\n\nFigure\t12-16.\tSplitting\ta\tdeep\trecurrent\tneural\tnetwork",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "In\tshort,\tmodel\tparallelism\tcan\tspeed\tup\trunning\tor\ttraining\tsome\ttypes\tof\tneural\tnetworks,\tbut\tnot\tall, and\tit\trequires\tspecial\tcare\tand\ttuning,\tsuch\tas\tmaking\tsure\tthat\tdevices\tthat\tneed\tto\tcommunicate\tthe most\trun\ton\tthe\tsame\tmachine.",
      "content_length": 237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "Data\tParallelism Another\tway\tto\tparallelize\tthe\ttraining\tof\ta\tneural\tnetwork\tis\tto\treplicate\tit\ton\teach\tdevice,\trun\ta\ttraining step\tsimultaneously\ton\tall\treplicas\tusing\ta\tdifferent\tmini-batch\tfor\teach,\tand\tthen\taggregate\tthe\tgradients to\tupdate\tthe\tmodel\tparameters.\tThis\tis\tcalled\tdata\tparallelism\t(see\tFigure\t12-17).\n\nFigure\t12-17.\tData\tparallelism\n\nThere\tare\ttwo\tvariants\tof\tthis\tapproach:\tsynchronous\tupdates\tand\tasynchronous\tupdates.\n\nSynchronous\tupdates\n\nWith\tsynchronous\tupdates,\tthe\taggregator\twaits\tfor\tall\tgradients\tto\tbe\tavailable\tbefore\tcomputing\tthe average\tand\tapplying\tthe\tresult\t(i.e.,\tusing\tthe\taggregated\tgradients\tto\tupdate\tthe\tmodel\tparameters). Once\ta\treplica\thas\tfinished\tcomputing\tits\tgradients,\tit\tmust\twait\tfor\tthe\tparameters\tto\tbe\tupdated\tbefore\tit can\tproceed\tto\tthe\tnext\tmini-batch.\tThe\tdownside\tis\tthat\tsome\tdevices\tmay\tbe\tslower\tthan\tothers,\tso\tall other\tdevices\twill\thave\tto\twait\tfor\tthem\tat\tevery\tstep.\tMoreover,\tthe\tparameters\twill\tbe\tcopied\tto\tevery device\talmost\tat\tthe\tsame\ttime\t(immediately\tafter\tthe\tgradients\tare\tapplied),\twhich\tmay\tsaturate\tthe parameter\tservers’\tbandwidth.\n\nTIP\n\nTo\treduce\tthe\twaiting\ttime\tat\teach\tstep,\tyou\tcould\tignore\tthe\tgradients\tfrom\tthe\tslowest\tfew\treplicas\t(typically\t~10%).\tFor example,\tyou\tcould\trun\t20\treplicas,\tbut\tonly\taggregate\tthe\tgradients\tfrom\tthe\tfastest\t18\treplicas\tat\teach\tstep,\tand\tjust\tignore\tthe gradients\tfrom\tthe\tlast\t2.\tAs\tsoon\tas\tthe\tparameters\tare\tupdated,\tthe\tfirst\t18\treplicas\tcan\tstart\tworking\tagain\timmediately, without\thaving\tto\twait\tfor\tthe\t2\tslowest\treplicas.\tThis\tsetup\tis\tgenerally\tdescribed\tas\thaving\t18\treplicas\tplus\t2\tspare\treplicas.5",
      "content_length": 1632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "Asynchronous\tupdates\n\nWith\tasynchronous\tupdates,\twhenever\ta\treplica\thas\tfinished\tcomputing\tthe\tgradients,\tit\timmediately\tuses them\tto\tupdate\tthe\tmodel\tparameters.\tThere\tis\tno\taggregation\t(remove\tthe\t“mean”\tstep\tin\tFigure\t12-17), and\tno\tsynchronization.\tReplicas\tjust\twork\tindependently\tof\tthe\tother\treplicas.\tSince\tthere\tis\tno\twaiting for\tthe\tother\treplicas,\tthis\tapproach\truns\tmore\ttraining\tsteps\tper\tminute.\tMoreover,\talthough\tthe parameters\tstill\tneed\tto\tbe\tcopied\tto\tevery\tdevice\tat\tevery\tstep,\tthis\thappens\tat\tdifferent\ttimes\tfor\teach replica\tso\tthe\trisk\tof\tbandwidth\tsaturation\tis\treduced.\n\nData\tparallelism\twith\tasynchronous\tupdates\tis\tan\tattractive\tchoice,\tbecause\tof\tits\tsimplicity,\tthe\tabsence of\tsynchronization\tdelay,\tand\ta\tbetter\tuse\tof\tthe\tbandwidth.\tHowever,\talthough\tit\tworks\treasonably\twell in\tpractice,\tit\tis\talmost\tsurprising\tthat\tit\tworks\tat\tall!\tIndeed,\tby\tthe\ttime\ta\treplica\thas\tfinished\tcomputing the\tgradients\tbased\ton\tsome\tparameter\tvalues,\tthese\tparameters\twill\thave\tbeen\tupdated\tseveral\ttimes\tby other\treplicas\t(on\taverage\tN\t–\t1\ttimes\tif\tthere\tare\tN\treplicas)\tand\tthere\tis\tno\tguarantee\tthat\tthe\tcomputed gradients\twill\tstill\tbe\tpointing\tin\tthe\tright\tdirection\t(see\tFigure\t12-18).\tWhen\tgradients\tare\tseverely\tout- of-date,\tthey\tare\tcalled\tstale\tgradients:\tthey\tcan\tslow\tdown\tconvergence,\tintroducing\tnoise\tand\twobble effects\t(the\tlearning\tcurve\tmay\tcontain\ttemporary\toscillations),\tor\tthey\tcan\teven\tmake\tthe\ttraining algorithm\tdiverge.\n\nFigure\t12-18.\tStale\tgradients\twhen\tusing\tasynchronous\tupdates\n\nThere\tare\ta\tfew\tways\tto\treduce\tthe\teffect\tof\tstale\tgradients:\n\nReduce\tthe\tlearning\trate.\n\nDrop\tstale\tgradients\tor\tscale\tthem\tdown.\n\nAdjust\tthe\tmini-batch\tsize.\n\nStart\tthe\tfirst\tfew\tepochs\tusing\tjust\tone\treplica\t(this\tis\tcalled\tthe\twarmup\tphase).\tStale\tgradients tend\tto\tbe\tmore\tdamaging\tat\tthe\tbeginning\tof\ttraining,\twhen\tgradients\tare\ttypically\tlarge\tand\tthe parameters\thave\tnot\tsettled\tinto\ta\tvalley\tof\tthe\tcost\tfunction\tyet,\tso\tdifferent\treplicas\tmay\tpush\tthe parameters\tin\tquite\tdifferent\tdirections.",
      "content_length": 2030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "A\tpaper\tpublished\tby\tthe\tGoogle\tBrain\tteam\tin\tApril\t2016\tbenchmarked\tvarious\tapproaches\tand\tfound that\tdata\tparallelism\twith\tsynchronous\tupdates\tusing\ta\tfew\tspare\treplicas\twas\tthe\tmost\tefficient,\tnot\tonly converging\tfaster\tbut\talso\tproducing\ta\tbetter\tmodel.\tHowever,\tthis\tis\tstill\tan\tactive\tarea\tof\tresearch,\tso you\tshould\tnot\trule\tout\tasynchronous\tupdates\tquite\tyet.\n\nBandwidth\tsaturation\n\nWhether\tyou\tuse\tsynchronous\tor\tasynchronous\tupdates,\tdata\tparallelism\tstill\trequires\tcommunicating\tthe model\tparameters\tfrom\tthe\tparameter\tservers\tto\tevery\treplica\tat\tthe\tbeginning\tof\tevery\ttraining\tstep,\tand the\tgradients\tin\tthe\tother\tdirection\tat\tthe\tend\tof\teach\ttraining\tstep.\tUnfortunately,\tthis\tmeans\tthat\tthere always\tcomes\ta\tpoint\twhere\tadding\tan\textra\tGPU\twill\tnot\timprove\tperformance\tat\tall\tbecause\tthe\ttime spent\tmoving\tthe\tdata\tin\tand\tout\tof\tGPU\tRAM\t(and\tpossibly\tacross\tthe\tnetwork)\twill\toutweigh\tthe speedup\tobtained\tby\tsplitting\tthe\tcomputation\tload.\tAt\tthat\tpoint,\tadding\tmore\tGPUs\twill\tjust\tincrease saturation\tand\tslow\tdown\ttraining.\n\nTIP\n\nFor\tsome\tmodels,\ttypically\trelatively\tsmall\tand\ttrained\ton\ta\tvery\tlarge\ttraining\tset,\tyou\tare\toften\tbetter\toff\ttraining\tthe\tmodel\ton\ta single\tmachine\twith\ta\tsingle\tGPU.\n\nSaturation\tis\tmore\tsevere\tfor\tlarge\tdense\tmodels,\tsince\tthey\thave\ta\tlot\tof\tparameters\tand\tgradients\tto transfer.\tIt\tis\tless\tsevere\tfor\tsmall\tmodels\t(but\tthe\tparallelization\tgain\tis\tsmall)\tand\talso\tfor\tlarge\tsparse models\tsince\tthe\tgradients\tare\ttypically\tmostly\tzeros,\tso\tthey\tcan\tbe\tcommunicated\tefficiently.\tJeff\tDean, initiator\tand\tlead\tof\tthe\tGoogle\tBrain\tproject,\treported\ttypical\tspeedups\tof\t25–40x\twhen\tdistributing computations\tacross\t50\tGPUs\tfor\tdense\tmodels,\tand\t300x\tspeedup\tfor\tsparser\tmodels\ttrained\tacross\t500 GPUs.\tAs\tyou\tcan\tsee,\tsparse\tmodels\treally\tdo\tscale\tbetter.\tHere\tare\ta\tfew\tconcrete\texamples:\n\nNeural\tMachine\tTranslation:\t6x\tspeedup\ton\t8\tGPUs\n\nInception/ImageNet:\t32x\tspeedup\ton\t50\tGPUs\n\nRankBrain:\t300x\tspeedup\ton\t500\tGPUs\n\nThese\tnumbers\trepresent\tthe\tstate\tof\tthe\tart\tin\tQ1\t2016.\tBeyond\ta\tfew\tdozen\tGPUs\tfor\ta\tdense\tmodel\tor few\thundred\tGPUs\tfor\ta\tsparse\tmodel,\tsaturation\tkicks\tin\tand\tperformance\tdegrades.\tThere\tis\tplenty\tof research\tgoing\ton\tto\tsolve\tthis\tproblem\t(exploring\tpeer-to-peer\tarchitectures\trather\tthan\tcentralized parameter\tservers,\tusing\tlossy\tmodel\tcompression,\toptimizing\twhen\tand\twhat\tthe\treplicas\tneed\tto communicate,\tand\tso\ton),\tso\tthere\twill\tlikely\tbe\ta\tlot\tof\tprogress\tin\tparallelizing\tneural\tnetworks\tin\tthe next\tfew\tyears.\n\nIn\tthe\tmeantime,\there\tare\ta\tfew\tsimple\tsteps\tyou\tcan\ttake\tto\treduce\tthe\tsaturation\tproblem:\n\nGroup\tyour\tGPUs\ton\ta\tfew\tservers\trather\tthan\tscattering\tthem\tacross\tmany\tservers.\tThis\twill\tavoid unnecessary\tnetwork\thops.\n\nShard\tthe\tparameters\tacross\tmultiple\tparameter\tservers\t(as\tdiscussed\tearlier).",
      "content_length": 2790,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "Drop\tthe\tmodel\tparameters’\tfloat\tprecision\tfrom\t32\tbits\t(tf.float32)\tto\t16\tbits\t(tf.bfloat16). This\twill\tcut\tin\thalf\tthe\tamount\tof\tdata\tto\ttransfer,\twithout\tmuch\timpact\ton\tthe\tconvergence\trate\tor the\tmodel’s\tperformance.\n\nTIP\n\nAlthough\t16-bit\tprecision\tis\tthe\tminimum\tfor\ttraining\tneural\tnetwork,\tyou\tcan\tactually\tdrop\tdown\tto\t8-bit\tprecision\tafter\ttraining to\treduce\tthe\tsize\tof\tthe\tmodel\tand\tspeed\tup\tcomputations.\tThis\tis\tcalled\tquantizing\tthe\tneural\tnetwork.\tIt\tis\tparticularly\tuseful for\tdeploying\tand\trunning\tpretrained\tmodels\ton\tmobile\tphones.\tSee\tPete\tWarden’s\tgreat\tpost\ton\tthe\tsubject.\n\nTensorFlow\timplementation\n\nTo\timplement\tdata\tparallelism\tusing\tTensorFlow,\tyou\tfirst\tneed\tto\tchoose\twhether\tyou\twant\tin-graph replication\tor\tbetween-graph\treplication,\tand\twhether\tyou\twant\tsynchronous\tupdates\tor\tasynchronous updates.\tLet’s\tlook\tat\thow\tyou\twould\timplement\teach\tcombination\t(see\tthe\texercises\tand\tthe\tJupyter notebooks\tfor\tcomplete\tcode\texamples).\n\nWith\tin-graph\treplication\t+\tsynchronous\tupdates,\tyou\tbuild\tone\tbig\tgraph\tcontaining\tall\tthe\tmodel replicas\t(placed\ton\tdifferent\tdevices),\tand\ta\tfew\tnodes\tto\taggregate\tall\ttheir\tgradients\tand\tfeed\tthem\tto an\toptimizer.\tYour\tcode\topens\ta\tsession\tto\tthe\tcluster\tand\tsimply\truns\tthe\ttraining\toperation\trepeatedly.\n\nWith\tin-graph\treplication\t+\tasynchronous\tupdates,\tyou\talso\tcreate\tone\tbig\tgraph,\tbut\twith\tone\toptimizer per\treplica,\tand\tyou\trun\tone\tthread\tper\treplica,\trepeatedly\trunning\tthe\treplica’s\toptimizer.\n\nWith\tbetween-graph\treplication\t+\tasynchronous\tupdates,\tyou\trun\tmultiple\tindependent\tclients\t(typically in\tseparate\tprocesses),\teach\ttraining\tthe\tmodel\treplica\tas\tif\tit\twere\talone\tin\tthe\tworld,\tbut\tthe\tparameters are\tactually\tshared\twith\tother\treplicas\t(using\ta\tresource\tcontainer).\n\nWith\tbetween-graph\treplication\t+\tsynchronous\tupdates,\tonce\tagain\tyou\trun\tmultiple\tclients,\teach\ttraining a\tmodel\treplica\tbased\ton\tshared\tparameters,\tbut\tthis\ttime\tyou\twrap\tthe\toptimizer\t(e.g.,\ta MomentumOptimizer)\twithin\ta\tSyncReplicasOptimizer.\tEach\treplica\tuses\tthis\toptimizer\tas\tit\twould use\tany\tother\toptimizer,\tbut\tunder\tthe\thood\tthis\toptimizer\tsends\tthe\tgradients\tto\ta\tset\tof\tqueues\t(one\tper variable),\twhich\tis\tread\tby\tone\tof\tthe\treplica’s\tSyncReplicasOptimizer,\tcalled\tthe\tchief.\tThe\tchief aggregates\tthe\tgradients\tand\tapplies\tthem,\tthen\twrites\ta\ttoken\tto\ta\ttoken\tqueue\tfor\teach\treplica,\tsignaling it\tthat\tit\tcan\tgo\tahead\tand\tcompute\tthe\tnext\tgradients.\tThis\tapproach\tsupports\thaving\tspare\treplicas.\n\nIf\tyou\tgo\tthrough\tthe\texercises,\tyou\twill\timplement\teach\tof\tthese\tfour\tsolutions.\tYou\twill\teasily\tbe\table\tto apply\twhat\tyou\thave\tlearned\tto\ttrain\tlarge\tdeep\tneural\tnetworks\tacross\tdozens\tof\tservers\tand\tGPUs!\tIn the\tfollowing\tchapters\twe\twill\tgo\tthrough\ta\tfew\tmore\timportant\tneural\tnetwork\tarchitectures\tbefore\twe tackle\tReinforcement\tLearning.",
      "content_length": 2805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "Exercises\n\n1.\t If\tyou\tget\ta\tCUDA_ERROR_OUT_OF_MEMORY\twhen\tstarting\tyour\tTensorFlow\tprogram,\twhat\tis probably\tgoing\ton?\tWhat\tcan\tyou\tdo\tabout\tit?\n\n2.\t What\tis\tthe\tdifference\tbetween\tpinning\tan\toperation\ton\ta\tdevice\tand\tplacing\tan\toperation\ton\ta device?\n\n3.\t If\tyou\tare\trunning\ton\ta\tGPU-enabled\tTensorFlow\tinstallation,\tand\tyou\tjust\tuse\tthe\tdefault\tplacement, will\tall\toperations\tbe\tplaced\ton\tthe\tfirst\tGPU?\n\n4.\t If\tyou\tpin\ta\tvariable\tto\t\"/gpu:0\",\tcan\tit\tbe\tused\tby\toperations\tplaced\ton\t/gpu:1?\tOr\tby\toperations placed\ton\t\"/cpu:0\"?\tOr\tby\toperations\tpinned\tto\tdevices\tlocated\ton\tother\tservers?\n\n5.\t Can\ttwo\toperations\tplaced\ton\tthe\tsame\tdevice\trun\tin\tparallel?\n\n6.\t What\tis\ta\tcontrol\tdependency\tand\twhen\twould\tyou\twant\tto\tuse\tone?\n\n7.\t Suppose\tyou\ttrain\ta\tDNN\tfor\tdays\ton\ta\tTensorFlow\tcluster,\tand\timmediately\tafter\tyour\ttraining program\tends\tyou\trealize\tthat\tyou\tforgot\tto\tsave\tthe\tmodel\tusing\ta\tSaver.\tIs\tyour\ttrained\tmodel\tlost?\n\n8.\t Train\tseveral\tDNNs\tin\tparallel\ton\ta\tTensorFlow\tcluster,\tusing\tdifferent\thyperparameter\tvalues.\tThis could\tbe\tDNNs\tfor\tMNIST\tclassification\tor\tany\tother\ttask\tyou\tare\tinterested\tin.\tThe\tsimplest\toption is\tto\twrite\ta\tsingle\tclient\tprogram\tthat\ttrains\tonly\tone\tDNN,\tthen\trun\tthis\tprogram\tin\tmultiple processes\tin\tparallel,\twith\tdifferent\thyperparameter\tvalues\tfor\teach\tclient.\tThe\tprogram\tshould\thave command-line\toptions\tto\tcontrol\twhat\tserver\tand\tdevice\tthe\tDNN\tshould\tbe\tplaced\ton,\tand\twhat resource\tcontainer\tand\thyperparameter\tvalues\tto\tuse\t(make\tsure\tto\tuse\ta\tdifferent\tresource\tcontainer for\teach\tDNN).\tUse\ta\tvalidation\tset\tor\tcross-validation\tto\tselect\tthe\ttop\tthree\tmodels.\n\n9.\t Create\tan\tensemble\tusing\tthe\ttop\tthree\tmodels\tfrom\tthe\tprevious\texercise.\tDefine\tit\tin\ta\tsingle graph,\tensuring\tthat\teach\tDNN\truns\ton\ta\tdifferent\tdevice.\tEvaluate\tit\ton\tthe\tvalidation\tset:\tdoes\tthe ensemble\tperform\tbetter\tthan\tthe\tindividual\tDNNs?\n\n10.\t Train\ta\tDNN\tusing\tbetween-graph\treplication\tand\tdata\tparallelism\twith\tasynchronous\tupdates, timing\thow\tlong\tit\ttakes\tto\treach\ta\tsatisfying\tperformance.\tNext,\ttry\tagain\tusing\tsynchronous updates.\tDo\tsynchronous\tupdates\tproduce\ta\tbetter\tmodel?\tIs\ttraining\tfaster?\tSplit\tthe\tDNN vertically\tand\tplace\teach\tvertical\tslice\ton\ta\tdifferent\tdevice,\tand\ttrain\tthe\tmodel\tagain.\tIs\ttraining any\tfaster?\tIs\tthe\tperformance\tany\tdifferent?\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“TensorFlow:\tLarge-Scale\tMachine\tLearning\ton\tHeterogeneous\tDistributed\tSystems,”\tGoogle\tResearch\t(2015).\n\n2\n\nYou\tcan\teven\tstart\tmultiple\ttasks\tin\tthe\tsame\tprocess.\tIt\tmay\tbe\tuseful\tfor\ttests,\tbut\tit\tis\tnot\trecommended\tin\tproduction.\n\n3\n\nIt\tis\tthe\tnext\tversion\tof\tGoogle’s\tinternal\tStubby\tservice,\twhich\tGoogle\thas\tused\tsuccessfully\tfor\tover\ta\tdecade.\tSee\thttp://grpc.io/\tfor more\tdetails.\n\n4\n\nNot\t100%\tlinear\tif\tyou\twait\tfor\tall\tdevices\tto\tfinish,\tsince\tthe\ttotal\ttime\twill\tbe\tthe\ttime\ttaken\tby\tthe\tslowest\tdevice.",
      "content_length": 2874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "5\n\nThis\tname\tis\tslightly\tconfusing\tsince\tit\tsounds\tlike\tsome\treplicas\tare\tspecial,\tdoing\tnothing.\tIn\treality,\tall\treplicas\tare\tequivalent:\tthey\tall work\thard\tto\tbe\tamong\tthe\tfastest\tat\teach\ttraining\tstep,\tand\tthe\tlosers\tvary\tat\tevery\tstep\t(unless\tsome\tdevices\tare\treally\tslower\tthan others).",
      "content_length": 291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "Chapter\t13.\tConvolutional\tNeural\tNetworks\n\nAlthough\tIBM’s\tDeep\tBlue\tsupercomputer\tbeat\tthe\tchess\tworld\tchampion\tGarry\tKasparov\tback\tin\t1996, until\tquite\trecently\tcomputers\twere\tunable\tto\treliably\tperform\tseemingly\ttrivial\ttasks\tsuch\tas\tdetecting\ta puppy\tin\ta\tpicture\tor\trecognizing\tspoken\twords.\tWhy\tare\tthese\ttasks\tso\teffortless\tto\tus\thumans?\tThe answer\tlies\tin\tthe\tfact\tthat\tperception\tlargely\ttakes\tplace\toutside\tthe\trealm\tof\tour\tconsciousness,\twithin specialized\tvisual,\tauditory,\tand\tother\tsensory\tmodules\tin\tour\tbrains.\tBy\tthe\ttime\tsensory\tinformation reaches\tour\tconsciousness,\tit\tis\talready\tadorned\twith\thigh-level\tfeatures;\tfor\texample,\twhen\tyou\tlook\tat\ta picture\tof\ta\tcute\tpuppy,\tyou\tcannot\tchoose\tnot\tto\tsee\tthe\tpuppy,\tor\tnot\tto\tnotice\tits\tcuteness.\tNor\tcan\tyou explain\thow\tyou\trecognize\ta\tcute\tpuppy;\tit’s\tjust\tobvious\tto\tyou.\tThus,\twe\tcannot\ttrust\tour\tsubjective experience:\tperception\tis\tnot\ttrivial\tat\tall,\tand\tto\tunderstand\tit\twe\tmust\tlook\tat\thow\tthe\tsensory\tmodules work.\n\nConvolutional\tneural\tnetworks\t(CNNs)\temerged\tfrom\tthe\tstudy\tof\tthe\tbrain’s\tvisual\tcortex,\tand\tthey\thave been\tused\tin\timage\trecognition\tsince\tthe\t1980s.\tIn\tthe\tlast\tfew\tyears,\tthanks\tto\tthe\tincrease\tin computational\tpower,\tthe\tamount\tof\tavailable\ttraining\tdata,\tand\tthe\ttricks\tpresented\tin\tChapter\t11\tfor training\tdeep\tnets,\tCNNs\thave\tmanaged\tto\tachieve\tsuperhuman\tperformance\ton\tsome\tcomplex\tvisual tasks.\tThey\tpower\timage\tsearch\tservices,\tself-driving\tcars,\tautomatic\tvideo\tclassification\tsystems,\tand more.\tMoreover,\tCNNs\tare\tnot\trestricted\tto\tvisual\tperception:\tthey\tare\talso\tsuccessful\tat\tother\ttasks, such\tas\tvoice\trecognition\tor\tnatural\tlanguage\tprocessing\t(NLP);\thowever,\twe\twill\tfocus\ton\tvisual applications\tfor\tnow.\n\nIn\tthis\tchapter\twe\twill\tpresent\twhere\tCNNs\tcame\tfrom,\twhat\ttheir\tbuilding\tblocks\tlook\tlike,\tand\thow\tto implement\tthem\tusing\tTensorFlow.\tThen\twe\twill\tpresent\tsome\tof\tthe\tbest\tCNN\tarchitectures.",
      "content_length": 1908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "The\tArchitecture\tof\tthe\tVisual\tCortex David\tH.\tHubel\tand\tTorsten\tWiesel\tperformed\ta\tseries\tof\texperiments\ton\tcats\tin\t19581\tand\t19592\t(and\ta few\tyears\tlater\ton\tmonkeys3),\tgiving\tcrucial\tinsights\ton\tthe\tstructure\tof\tthe\tvisual\tcortex\t(the\tauthors received\tthe\tNobel\tPrize\tin\tPhysiology\tor\tMedicine\tin\t1981\tfor\ttheir\twork).\tIn\tparticular,\tthey\tshowed that\tmany\tneurons\tin\tthe\tvisual\tcortex\thave\ta\tsmall\tlocal\treceptive\tfield,\tmeaning\tthey\treact\tonly\tto\tvisual stimuli\tlocated\tin\ta\tlimited\tregion\tof\tthe\tvisual\tfield\t(see\tFigure\t13-1,\tin\twhich\tthe\tlocal\treceptive\tfields of\tfive\tneurons\tare\trepresented\tby\tdashed\tcircles).\tThe\treceptive\tfields\tof\tdifferent\tneurons\tmay\toverlap, and\ttogether\tthey\ttile\tthe\twhole\tvisual\tfield.\tMoreover,\tthe\tauthors\tshowed\tthat\tsome\tneurons\treact\tonly to\timages\tof\thorizontal\tlines,\twhile\tothers\treact\tonly\tto\tlines\twith\tdifferent\torientations\t(two\tneurons\tmay have\tthe\tsame\treceptive\tfield\tbut\treact\tto\tdifferent\tline\torientations).\tThey\talso\tnoticed\tthat\tsome\tneurons have\tlarger\treceptive\tfields,\tand\tthey\treact\tto\tmore\tcomplex\tpatterns\tthat\tare\tcombinations\tof\tthe\tlower- level\tpatterns.\tThese\tobservations\tled\tto\tthe\tidea\tthat\tthe\thigher-level\tneurons\tare\tbased\ton\tthe\toutputs\tof neighboring\tlower-level\tneurons\t(in\tFigure\t13-1,\tnotice\tthat\teach\tneuron\tis\tconnected\tonly\tto\ta\tfew neurons\tfrom\tthe\tprevious\tlayer).\tThis\tpowerful\tarchitecture\tis\table\tto\tdetect\tall\tsorts\tof\tcomplex\tpatterns in\tany\tarea\tof\tthe\tvisual\tfield.\n\nFigure\t13-1.\tLocal\treceptive\tfields\tin\tthe\tvisual\tcortex\n\nThese\tstudies\tof\tthe\tvisual\tcortex\tinspired\tthe\tneocognitron,\tintroduced\tin\t1980,4\twhich\tgradually evolved\tinto\twhat\twe\tnow\tcall\tconvolutional\tneural\tnetworks.\tAn\timportant\tmilestone\twas\ta\t1998 paper5\tby\tYann\tLeCun,\tLéon\tBottou,\tYoshua\tBengio,\tand\tPatrick\tHaffner,\twhich\tintroduced\tthe\tfamous LeNet-5\tarchitecture,\twidely\tused\tto\trecognize\thandwritten\tcheck\tnumbers.\tThis\tarchitecture\thas\tsome building\tblocks\tthat\tyou\talready\tknow,\tsuch\tas\tfully\tconnected\tlayers\tand\tsigmoid\tactivation\tfunctions, but\tit\talso\tintroduces\ttwo\tnew\tbuilding\tblocks:\tconvolutional\tlayers\tand\tpooling\tlayers.\tLet’s\tlook\tat them\tnow.\n\nNOTE\n\nWhy\tnot\tsimply\tuse\ta\tregular\tdeep\tneural\tnetwork\twith\tfully\tconnected\tlayers\tfor\timage\trecognition\ttasks?\tUnfortunately, although\tthis\tworks\tfine\tfor\tsmall\timages\t(e.g.,\tMNIST),\tit\tbreaks\tdown\tfor\tlarger\timages\tbecause\tof\tthe\thuge\tnumber\tof parameters\tit\trequires.\tFor\texample,\ta\t100\t×\t100\timage\thas\t10,000\tpixels,\tand\tif\tthe\tfirst\tlayer\thas\tjust\t1,000\tneurons\t(which already\tseverely\trestricts\tthe\tamount\tof\tinformation\ttransmitted\tto\tthe\tnext\tlayer),\tthis\tmeans\ta\ttotal\tof\t10\tmillion\tconnections. And\tthat’s\tjust\tthe\tfirst\tlayer.\tCNNs\tsolve\tthis\tproblem\tusing\tpartially\tconnected\tlayers.",
      "content_length": 2720,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "Convolutional\tLayer The\tmost\timportant\tbuilding\tblock\tof\ta\tCNN\tis\tthe\tconvolutional\tlayer:6\tneurons\tin\tthe\tfirst convolutional\tlayer\tare\tnot\tconnected\tto\tevery\tsingle\tpixel\tin\tthe\tinput\timage\t(like\tthey\twere\tin\tprevious chapters),\tbut\tonly\tto\tpixels\tin\ttheir\treceptive\tfields\t(see\tFigure\t13-2).\tIn\tturn,\teach\tneuron\tin\tthe\tsecond convolutional\tlayer\tis\tconnected\tonly\tto\tneurons\tlocated\twithin\ta\tsmall\trectangle\tin\tthe\tfirst\tlayer.\tThis architecture\tallows\tthe\tnetwork\tto\tconcentrate\ton\tlow-level\tfeatures\tin\tthe\tfirst\thidden\tlayer,\tthen assemble\tthem\tinto\thigher-level\tfeatures\tin\tthe\tnext\thidden\tlayer,\tand\tso\ton.\tThis\thierarchical\tstructure\tis common\tin\treal-world\timages,\twhich\tis\tone\tof\tthe\treasons\twhy\tCNNs\twork\tso\twell\tfor\timage recognition.\n\nFigure\t13-2.\tCNN\tlayers\twith\trectangular\tlocal\treceptive\tfields\n\nNOTE\n\nUntil\tnow,\tall\tmultilayer\tneural\tnetworks\twe\tlooked\tat\thad\tlayers\tcomposed\tof\ta\tlong\tline\tof\tneurons,\tand\twe\thad\tto\tflatten\tinput images\tto\t1D\tbefore\tfeeding\tthem\tto\tthe\tneural\tnetwork.\tNow\teach\tlayer\tis\trepresented\tin\t2D,\twhich\tmakes\tit\teasier\tto\tmatch neurons\twith\ttheir\tcorresponding\tinputs.\n\nA\tneuron\tlocated\tin\trow\ti,\tcolumn\tj\tof\ta\tgiven\tlayer\tis\tconnected\tto\tthe\toutputs\tof\tthe\tneurons\tin\tthe previous\tlayer\tlocated\tin\trows\ti\tto\ti\t+\tfh\t–\t1,\tcolumns\tj\tto\tj\t+\tfw\t–\t1,\twhere\tfh\tand\tfw\tare\tthe\theight\tand width\tof\tthe\treceptive\tfield\t(see\tFigure\t13-3).\tIn\torder\tfor\ta\tlayer\tto\thave\tthe\tsame\theight\tand\twidth\tas the\tprevious\tlayer,\tit\tis\tcommon\tto\tadd\tzeros\taround\tthe\tinputs,\tas\tshown\tin\tthe\tdiagram.\tThis\tis\tcalled zero\tpadding.",
      "content_length": 1552,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "Figure\t13-3.\tConnections\tbetween\tlayers\tand\tzero\tpadding\n\nIt\tis\talso\tpossible\tto\tconnect\ta\tlarge\tinput\tlayer\tto\ta\tmuch\tsmaller\tlayer\tby\tspacing\tout\tthe\treceptive fields,\tas\tshown\tin\tFigure\t13-4.\tThe\tdistance\tbetween\ttwo\tconsecutive\treceptive\tfields\tis\tcalled\tthe stride.\tIn\tthe\tdiagram,\ta\t5\t×\t7\tinput\tlayer\t(plus\tzero\tpadding)\tis\tconnected\tto\ta\t3\t×\t4\tlayer,\tusing\t3\t×\t3 receptive\tfields\tand\ta\tstride\tof\t2\t(in\tthis\texample\tthe\tstride\tis\tthe\tsame\tin\tboth\tdirections,\tbut\tit\tdoes\tnot have\tto\tbe\tso).\tA\tneuron\tlocated\tin\trow\ti,\tcolumn\tj\tin\tthe\tupper\tlayer\tis\tconnected\tto\tthe\toutputs\tof\tthe neurons\tin\tthe\tprevious\tlayer\tlocated\tin\trows\ti\t×\tsh\tto\ti\t×\tsh\t+\tfh\t–\t1,\tcolumns\tj\t×\tsw\t+\tfw\t–\t1,\twhere\tsh and\tsw\tare\tthe\tvertical\tand\thorizontal\tstrides.\n\nFigure\t13-4.\tReducing\tdimensionality\tusing\ta\tstride",
      "content_length": 794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "Filters A\tneuron’s\tweights\tcan\tbe\trepresented\tas\ta\tsmall\timage\tthe\tsize\tof\tthe\treceptive\tfield.\tFor\texample, Figure\t13-5\tshows\ttwo\tpossible\tsets\tof\tweights,\tcalled\tfilters\t(or\tconvolution\tkernels).\tThe\tfirst\tone\tis represented\tas\ta\tblack\tsquare\twith\ta\tvertical\twhite\tline\tin\tthe\tmiddle\t(it\tis\ta\t7\t×\t7\tmatrix\tfull\tof\t0s\texcept for\tthe\tcentral\tcolumn,\twhich\tis\tfull\tof\t1s);\tneurons\tusing\tthese\tweights\twill\tignore\teverything\tin\ttheir receptive\tfield\texcept\tfor\tthe\tcentral\tvertical\tline\t(since\tall\tinputs\twill\tget\tmultiplied\tby\t0,\texcept\tfor\tthe ones\tlocated\tin\tthe\tcentral\tvertical\tline).\tThe\tsecond\tfilter\tis\ta\tblack\tsquare\twith\ta\thorizontal\twhite\tline in\tthe\tmiddle.\tOnce\tagain,\tneurons\tusing\tthese\tweights\twill\tignore\teverything\tin\ttheir\treceptive\tfield except\tfor\tthe\tcentral\thorizontal\tline.\n\nNow\tif\tall\tneurons\tin\ta\tlayer\tuse\tthe\tsame\tvertical\tline\tfilter\t(and\tthe\tsame\tbias\tterm),\tand\tyou\tfeed\tthe network\tthe\tinput\timage\tshown\tin\tFigure\t13-5\t(bottom\timage),\tthe\tlayer\twill\toutput\tthe\ttop-left\timage. Notice\tthat\tthe\tvertical\twhite\tlines\tget\tenhanced\twhile\tthe\trest\tgets\tblurred.\tSimilarly,\tthe\tupper-right image\tis\twhat\tyou\tget\tif\tall\tneurons\tuse\tthe\thorizontal\tline\tfilter;\tnotice\tthat\tthe\thorizontal\twhite\tlines\tget enhanced\twhile\tthe\trest\tis\tblurred\tout.\tThus,\ta\tlayer\tfull\tof\tneurons\tusing\tthe\tsame\tfilter\tgives\tyou\ta feature\tmap,\twhich\thighlights\tthe\tareas\tin\tan\timage\tthat\tare\tmost\tsimilar\tto\tthe\tfilter.\tDuring\ttraining,\ta CNN\tfinds\tthe\tmost\tuseful\tfilters\tfor\tits\ttask,\tand\tit\tlearns\tto\tcombine\tthem\tinto\tmore\tcomplex\tpatterns (e.g.,\ta\tcross\tis\tan\tarea\tin\tan\timage\twhere\tboth\tthe\tvertical\tfilter\tand\tthe\thorizontal\tfilter\tare\tactive).\n\nFigure\t13-5.\tApplying\ttwo\tdifferent\tfilters\tto\tget\ttwo\tfeature\tmaps",
      "content_length": 1718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "Stacking\tMultiple\tFeature\tMaps Up\tto\tnow,\tfor\tsimplicity,\twe\thave\trepresented\teach\tconvolutional\tlayer\tas\ta\tthin\t2D\tlayer,\tbut\tin\treality it\tis\tcomposed\tof\tseveral\tfeature\tmaps\tof\tequal\tsizes,\tso\tit\tis\tmore\taccurately\trepresented\tin\t3D\t(see Figure\t13-6).\tWithin\tone\tfeature\tmap,\tall\tneurons\tshare\tthe\tsame\tparameters\t(weights\tand\tbias\tterm),\tbut different\tfeature\tmaps\tmay\thave\tdifferent\tparameters.\tA\tneuron’s\treceptive\tfield\tis\tthe\tsame\tas\tdescribed earlier,\tbut\tit\textends\tacross\tall\tthe\tprevious\tlayers’\tfeature\tmaps.\tIn\tshort,\ta\tconvolutional\tlayer simultaneously\tapplies\tmultiple\tfilters\tto\tits\tinputs,\tmaking\tit\tcapable\tof\tdetecting\tmultiple\tfeatures anywhere\tin\tits\tinputs.\n\nNOTE\n\nThe\tfact\tthat\tall\tneurons\tin\ta\tfeature\tmap\tshare\tthe\tsame\tparameters\tdramatically\treduces\tthe\tnumber\tof\tparameters\tin\tthe model,\tbut\tmost\timportantly\tit\tmeans\tthat\tonce\tthe\tCNN\thas\tlearned\tto\trecognize\ta\tpattern\tin\tone\tlocation,\tit\tcan\trecognize\tit\tin any\tother\tlocation.\tIn\tcontrast,\tonce\ta\tregular\tDNN\thas\tlearned\tto\trecognize\ta\tpattern\tin\tone\tlocation,\tit\tcan\trecognize\tit\tonly\tin that\tparticular\tlocation.\n\nMoreover,\tinput\timages\tare\talso\tcomposed\tof\tmultiple\tsublayers:\tone\tper\tcolor\tchannel.\tThere\tare typically\tthree:\tred,\tgreen,\tand\tblue\t(RGB).\tGrayscale\timages\thave\tjust\tone\tchannel,\tbut\tsome\timages may\thave\tmuch\tmore\t—\tfor\texample,\tsatellite\timages\tthat\tcapture\textra\tlight\tfrequencies\t(such\tas infrared).",
      "content_length": 1405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "Figure\t13-6.\tConvolution\tlayers\twith\tmultiple\tfeature\tmaps,\tand\timages\twith\tthree\tchannels\n\nSpecifically,\ta\tneuron\tlocated\tin\trow\ti,\tcolumn\tj\tof\tthe\tfeature\tmap\tk\tin\ta\tgiven\tconvolutional\tlayer\tl\tis connected\tto\tthe\toutputs\tof\tthe\tneurons\tin\tthe\tprevious\tlayer\tl\t–\t1,\tlocated\tin\trows\ti\t×\tsh\tto\ti\t×\tsh\t+\tfh\t–\t1 and\tcolumns\tj\t×\tsw\tto\tj\t×\tsw\t+\tfw\t–\t1,\tacross\tall\tfeature\tmaps\t(in\tlayer\tl\t–\t1).\tNote\tthat\tall\tneurons\tlocated in\tthe\tsame\trow\ti\tand\tcolumn\tj\tbut\tin\tdifferent\tfeature\tmaps\tare\tconnected\tto\tthe\toutputs\tof\tthe\texact\tsame neurons\tin\tthe\tprevious\tlayer.\n\nEquation\t13-1\tsummarizes\tthe\tpreceding\texplanations\tin\tone\tbig\tmathematical\tequation:\tit\tshows\thow\tto compute\tthe\toutput\tof\ta\tgiven\tneuron\tin\ta\tconvolutional\tlayer.\tIt\tis\ta\tbit\tugly\tdue\tto\tall\tthe\tdifferent indices,\tbut\tall\tit\tdoes\tis\tcalculate\tthe\tweighted\tsum\tof\tall\tthe\tinputs,\tplus\tthe\tbias\tterm.\n\nEquation\t13-1.\tComputing\tthe\toutput\tof\ta\tneuron\tin\ta\tconvolutional\tlayer\n\nzi,\tj,\tk\tis\tthe\toutput\tof\tthe\tneuron\tlocated\tin\trow\ti,\tcolumn\tj\tin\tfeature\tmap\tk\tof\tthe\tconvolutional\tlayer (layer\tl).\n\nAs\texplained\tearlier,\tsh\tand\tsw\tare\tthe\tvertical\tand\thorizontal\tstrides,\tfh\tand\tfw\tare\tthe\theight\tand width\tof\tthe\treceptive\tfield,\tand\tfn′\tis\tthe\tnumber\tof\tfeature\tmaps\tin\tthe\tprevious\tlayer\t(layer\tl\t–\t1).",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "xi′,\tj′,\tk′\tis\tthe\toutput\tof\tthe\tneuron\tlocated\tin\tlayer\tl\t–\t1,\trow\ti′,\tcolumn\tj′,\tfeature\tmap\tk′\t(or\tchannel\tk′ if\tthe\tprevious\tlayer\tis\tthe\tinput\tlayer).\n\nbk\tis\tthe\tbias\tterm\tfor\tfeature\tmap\tk\t(in\tlayer\tl).\tYou\tcan\tthink\tof\tit\tas\ta\tknob\tthat\ttweaks\tthe\toverall brightness\tof\tthe\tfeature\tmap\tk.\n\nwu,\tv,\tk′\t,k\tis\tthe\tconnection\tweight\tbetween\tany\tneuron\tin\tfeature\tmap\tk\tof\tthe\tlayer\tl\tand\tits\tinput located\tat\trow\tu,\tcolumn\tv\t(relative\tto\tthe\tneuron’s\treceptive\tfield),\tand\tfeature\tmap\tk′.",
      "content_length": 490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "TensorFlow\tImplementation In\tTensorFlow,\teach\tinput\timage\tis\ttypically\trepresented\tas\ta\t3D\ttensor\tof\tshape\t[height,\twidth, channels].\tA\tmini-batch\tis\trepresented\tas\ta\t4D\ttensor\tof\tshape\t[mini-batch\tsize,\theight, width,\tchannels].\tThe\tweights\tof\ta\tconvolutional\tlayer\tare\trepresented\tas\ta\t4D\ttensor\tof\tshape\t[fh,\tfw, fn′,\tfn].\tThe\tbias\tterms\tof\ta\tconvolutional\tlayer\tare\tsimply\trepresented\tas\ta\t1D\ttensor\tof\tshape\t[fn].\n\nLet’s\tlook\tat\ta\tsimple\texample.\tThe\tfollowing\tcode\tloads\ttwo\tsample\timages,\tusing\tScikit-Learn’s load_sample_images()\t(which\tloads\ttwo\tcolor\timages,\tone\tof\ta\tChinese\ttemple,\tand\tthe\tother\tof\ta flower).\tThen\tit\tcreates\ttwo\t7\t×\t7\tfilters\t(one\twith\ta\tvertical\twhite\tline\tin\tthe\tmiddle,\tand\tthe\tother\twith\ta horizontal\twhite\tline\tin\tthe\tmiddle),\tand\tapplies\tthem\tto\tboth\timages\tusing\ta\tconvolutional\tlayer\tbuilt using\tTensorFlow’s\ttf.nn.conv2d()\tfunction\t(with\tzero\tpadding\tand\ta\tstride\tof\t2).\tFinally,\tit\tplots\tone of\tthe\tresulting\tfeature\tmaps\t(similar\tto\tthe\ttop-right\timage\tin\tFigure\t13-5).\n\nimport\tnumpy\tas\tnp from\tsklearn.datasets\timport\tload_sample_images\n\n#\tLoad\tsample\timages china\t=\tload_sample_image(\"china.jpg\") flower\t=\tload_sample_image(\"flower.jpg\") dataset\t=\tnp.array([china,\tflower],\tdtype=np.float32) batch_size,\theight,\twidth,\tchannels\t=\tdataset.shape\n\n#\tCreate\t2\tfilters filters\t=\tnp.zeros(shape=(7,\t7,\tchannels,\t2),\tdtype=np.float32) filters[:,\t3,\t:,\t0]\t=\t1\t\t#\tvertical\tline filters[3,\t:,\t:,\t1]\t=\t1\t\t#\thorizontal\tline\n\n#\tCreate\ta\tgraph\twith\tinput\tX\tplus\ta\tconvolutional\tlayer\tapplying\tthe\t2\tfilters X\t=\ttf.placeholder(tf.float32,\tshape=(None,\theight,\twidth,\tchannels)) convolution\t=\ttf.nn.conv2d(X,\tfilters,\tstrides=[1,2,2,1],\tpadding=\"SAME\")\n\nwith\ttf.Session()\tas\tsess: \t\t\t\toutput\t=\tsess.run(convolution,\tfeed_dict={X:\tdataset})\n\nplt.imshow(output[0,\t:,\t:,\t1],\tcmap=\"gray\")\t#\tplot\t1st\timage's\t2nd\tfeature\tmap plt.show()\n\nMost\tof\tthis\tcode\tis\tself-explanatory,\tbut\tthe\ttf.nn.conv2d()\tline\tdeserves\ta\tbit\tof\texplanation:\n\nX\tis\tthe\tinput\tmini-batch\t(a\t4D\ttensor,\tas\texplained\tearlier).\n\nfilters\tis\tthe\tset\tof\tfilters\tto\tapply\t(also\ta\t4D\ttensor,\tas\texplained\tearlier).\n\nstrides\tis\ta\tfour-element\t1D\tarray,\twhere\tthe\ttwo\tcentral\telements\tare\tthe\tvertical\tand\thorizontal strides\t(sh\tand\tsw).\tThe\tfirst\tand\tlast\telements\tmust\tcurrently\tbe\tequal\tto\t1.\tThey\tmay\tone\tday\tbe used\tto\tspecify\ta\tbatch\tstride\t(to\tskip\tsome\tinstances)\tand\ta\tchannel\tstride\t(to\tskip\tsome\tof\tthe previous\tlayer’s\tfeature\tmaps\tor\tchannels).\n\npadding\tmust\tbe\teither\t\"VALID\"\tor\t\"SAME\":\n\nIf\tset\tto\t\"VALID\",\tthe\tconvolutional\tlayer\tdoes\tnot\tuse\tzero\tpadding,\tand\tmay\tignore\tsome rows\tand\tcolumns\tat\tthe\tbottom\tand\tright\tof\tthe\tinput\timage,\tdepending\ton\tthe\tstride,\tas\tshown in\tFigure\t13-7\t(for\tsimplicity,\tonly\tthe\thorizontal\tdimension\tis\tshown\there,\tbut\tof\tcourse\tthe same\tlogic\tapplies\tto\tthe\tvertical\tdimension).",
      "content_length": 2814,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "If\tset\tto\t\"SAME\",\tthe\tconvolutional\tlayer\tuses\tzero\tpadding\tif\tnecessary.\tIn\tthis\tcase,\tthe\tnumber of\toutput\tneurons\tis\tequal\tto\tthe\tnumber\tof\tinput\tneurons\tdivided\tby\tthe\tstride,\trounded\tup\t(in this\texample,\tceil\t(13\t/\t5)\t=\t3).\tThen\tzeros\tare\tadded\tas\tevenly\tas\tpossible\taround\tthe\tinputs.\n\nFigure\t13-7.\tPadding\toptions\t—\tinput\twidth:\t13,\tfilter\twidth:\t6,\tstride:\t5\n\nIn\tthis\tsimple\texample,\twe\tmanually\tcreated\tthe\tfilters,\tbut\tin\ta\treal\tCNN\tyou\twould\tlet\tthe\ttraining algorithm\tdiscover\tthe\tbest\tfilters\tautomatically.\tTensorFlow\thas\ta\ttf.layers.conv2d()\tfunction\twhich creates\tthe\tfilters\tvariable\tfor\tyou\t(called\tkernel),\tand\tinitializes\tit\trandomly.\tFor\texample,\tthe following\tcode\tcreates\tan\tinput\tplaceholder\tfollowed\tby\ta\tconvolutional\tlayer\twith\ttwo\t7\t×\t7\tfeature maps,\tusing\t2\t×\t2\tstrides\t(note\tthat\tthis\tfunction\tonly\texpects\tthe\tvertical\tand\thorizontal\tstrides),\tand \"SAME\"\tpadding:\n\nX\t=\ttf.placeholder(shape=(None,\theight,\twidth,\tchannels),\tdtype=tf.float32) conv\t=\ttf.layers.conv2d(X,\tfilters=2,\tkernel_size=7,\tstrides=[2,2], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpadding=\"SAME\")\n\nUnfortunately,\tconvolutional\tlayers\thave\tquite\ta\tfew\thyperparameters:\tyou\tmust\tchoose\tthe\tnumber\tof filters,\ttheir\theight\tand\twidth,\tthe\tstrides,\tand\tthe\tpadding\ttype.\tAs\talways,\tyou\tcan\tuse\tcross-validation to\tfind\tthe\tright\thyperparameter\tvalues,\tbut\tthis\tis\tvery\ttime-consuming.\tWe\twill\tdiscuss\tcommon\tCNN architectures\tlater,\tto\tgive\tyou\tsome\tidea\tof\twhat\thyperparameter\tvalues\twork\tbest\tin\tpractice.",
      "content_length": 1485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "Memory\tRequirements Another\tproblem\twith\tCNNs\tis\tthat\tthe\tconvolutional\tlayers\trequire\ta\thuge\tamount\tof\tRAM,\tespecially during\ttraining,\tbecause\tthe\treverse\tpass\tof\tbackpropagation\trequires\tall\tthe\tintermediate\tvalues computed\tduring\tthe\tforward\tpass.\n\nFor\texample,\tconsider\ta\tconvolutional\tlayer\twith\t5\t×\t5\tfilters,\toutputting\t200\tfeature\tmaps\tof\tsize\t150\t× 100,\twith\tstride\t1\tand\tSAME\tpadding.\tIf\tthe\tinput\tis\ta\t150\t×\t100\tRGB\timage\t(three\tchannels),\tthen\tthe number\tof\tparameters\tis\t(5\t×\t5\t×\t3\t+\t1)\t×\t200\t=\t15,200\t(the\t+1\tcorresponds\tto\tthe\tbias\tterms),\twhich\tis fairly\tsmall\tcompared\tto\ta\tfully\tconnected\tlayer.7\tHowever,\teach\tof\tthe\t200\tfeature\tmaps\tcontains\t150\t× 100\tneurons,\tand\teach\tof\tthese\tneurons\tneeds\tto\tcompute\ta\tweighted\tsum\tof\tits\t5\t×\t5\t×\t3\t=\t75\tinputs: that’s\ta\ttotal\tof\t225\tmillion\tfloat\tmultiplications.\tNot\tas\tbad\tas\ta\tfully\tconnected\tlayer,\tbut\tstill\tquite computationally\tintensive.\tMoreover,\tif\tthe\tfeature\tmaps\tare\trepresented\tusing\t32-bit\tfloats,\tthen\tthe convolutional\tlayer’s\toutput\twill\toccupy\t200\t×\t150\t×\t100\t×\t32\t=\t96\tmillion\tbits\t(about\t11.4\tMB)\tof RAM.8\tAnd\tthat’s\tjust\tfor\tone\tinstance!\tIf\ta\ttraining\tbatch\tcontains\t100\tinstances,\tthen\tthis\tlayer\twill\tuse up\tover\t1\tGB\tof\tRAM!\n\nDuring\tinference\t(i.e.,\twhen\tmaking\ta\tprediction\tfor\ta\tnew\tinstance)\tthe\tRAM\toccupied\tby\tone\tlayer\tcan be\treleased\tas\tsoon\tas\tthe\tnext\tlayer\thas\tbeen\tcomputed,\tso\tyou\tonly\tneed\tas\tmuch\tRAM\tas\trequired\tby two\tconsecutive\tlayers.\tBut\tduring\ttraining\teverything\tcomputed\tduring\tthe\tforward\tpass\tneeds\tto\tbe preserved\tfor\tthe\treverse\tpass,\tso\tthe\tamount\tof\tRAM\tneeded\tis\t(at\tleast)\tthe\ttotal\tamount\tof\tRAM required\tby\tall\tlayers.\n\nTIP\n\nIf\ttraining\tcrashes\tbecause\tof\tan\tout-of-memory\terror,\tyou\tcan\ttry\treducing\tthe\tmini-batch\tsize.\tAlternatively,\tyou\tcan\ttry reducing\tdimensionality\tusing\ta\tstride,\tor\tremoving\ta\tfew\tlayers.\tOr\tyou\tcan\ttry\tusing\t16-bit\tfloats\tinstead\tof\t32-bit\tfloats.\tOr\tyou could\tdistribute\tthe\tCNN\tacross\tmultiple\tdevices.\n\nNow\tlet’s\tlook\tat\tthe\tsecond\tcommon\tbuilding\tblock\tof\tCNNs:\tthe\tpooling\tlayer.",
      "content_length": 2030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "Pooling\tLayer Once\tyou\tunderstand\thow\tconvolutional\tlayers\twork,\tthe\tpooling\tlayers\tare\tquite\teasy\tto\tgrasp.\tTheir goal\tis\tto\tsubsample\t(i.e.,\tshrink)\tthe\tinput\timage\tin\torder\tto\treduce\tthe\tcomputational\tload,\tthe\tmemory usage,\tand\tthe\tnumber\tof\tparameters\t(thereby\tlimiting\tthe\trisk\tof\toverfitting).\tReducing\tthe\tinput\timage size\talso\tmakes\tthe\tneural\tnetwork\ttolerate\ta\tlittle\tbit\tof\timage\tshift\t(location\tinvariance).\n\nJust\tlike\tin\tconvolutional\tlayers,\teach\tneuron\tin\ta\tpooling\tlayer\tis\tconnected\tto\tthe\toutputs\tof\ta\tlimited number\tof\tneurons\tin\tthe\tprevious\tlayer,\tlocated\twithin\ta\tsmall\trectangular\treceptive\tfield.\tYou\tmust define\tits\tsize,\tthe\tstride,\tand\tthe\tpadding\ttype,\tjust\tlike\tbefore.\tHowever,\ta\tpooling\tneuron\thas\tno weights;\tall\tit\tdoes\tis\taggregate\tthe\tinputs\tusing\tan\taggregation\tfunction\tsuch\tas\tthe\tmax\tor\tmean. Figure\t13-8\tshows\ta\tmax\tpooling\tlayer,\twhich\tis\tthe\tmost\tcommon\ttype\tof\tpooling\tlayer.\tIn\tthis\texample, we\tuse\ta\t2\t×\t2\tpooling\tkernel,\ta\tstride\tof\t2,\tand\tno\tpadding.\tNote\tthat\tonly\tthe\tmax\tinput\tvalue\tin\teach kernel\tmakes\tit\tto\tthe\tnext\tlayer.\tThe\tother\tinputs\tare\tdropped.\n\nFigure\t13-8.\tMax\tpooling\tlayer\t(2\t×\t2\tpooling\tkernel,\tstride\t2,\tno\tpadding)\n\nThis\tis\tobviously\ta\tvery\tdestructive\tkind\tof\tlayer:\teven\twith\ta\ttiny\t2\t×\t2\tkernel\tand\ta\tstride\tof\t2,\tthe output\twill\tbe\ttwo\ttimes\tsmaller\tin\tboth\tdirections\t(so\tits\tarea\twill\tbe\tfour\ttimes\tsmaller),\tsimply dropping\t75%\tof\tthe\tinput\tvalues.\n\nA\tpooling\tlayer\ttypically\tworks\ton\tevery\tinput\tchannel\tindependently,\tso\tthe\toutput\tdepth\tis\tthe\tsame\tas the\tinput\tdepth.\tYou\tmay\talternatively\tpool\tover\tthe\tdepth\tdimension,\tas\twe\twill\tsee\tnext,\tin\twhich\tcase the\timage’s\tspatial\tdimensions\t(height\tand\twidth)\tremain\tunchanged,\tbut\tthe\tnumber\tof\tchannels\tis reduced.\n\nImplementing\ta\tmax\tpooling\tlayer\tin\tTensorFlow\tis\tquite\teasy.\tThe\tfollowing\tcode\tcreates\ta\tmax\tpooling layer\tusing\ta\t2\t×\t2\tkernel,\tstride\t2,\tand\tno\tpadding,\tthen\tapplies\tit\tto\tall\tthe\timages\tin\tthe\tdataset:\n\n[...]\t#\tload\tthe\timage\tdataset,\tjust\tlike\tabove\n\n#\tCreate\ta\tgraph\twith\tinput\tX\tplus\ta\tmax\tpooling\tlayer X\t=\ttf.placeholder(tf.float32,\tshape=(None,\theight,\twidth,\tchannels)) max_pool\t=\ttf.nn.max_pool(X,\tksize=[1,2,2,1],\tstrides=[1,2,2,1],padding=\"VALID\")\n\nwith\ttf.Session()\tas\tsess: \t\t\t\toutput\t=\tsess.run(max_pool,\tfeed_dict={X:\tdataset})",
      "content_length": 2290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "plt.imshow(output[0].astype(np.uint8))\t\t#\tplot\tthe\toutput\tfor\tthe\t1st\timage plt.show()\n\nThe\tksize\targument\tcontains\tthe\tkernel\tshape\talong\tall\tfour\tdimensions\tof\tthe\tinput\ttensor:\t[batch size,\theight,\twidth,\tchannels].\tTensorFlow\tcurrently\tdoes\tnot\tsupport\tpooling\tover\tmultiple instances,\tso\tthe\tfirst\telement\tof\tksize\tmust\tbe\tequal\tto\t1.\tMoreover,\tit\tdoes\tnot\tsupport\tpooling\tover both\tthe\tspatial\tdimensions\t(height\tand\twidth)\tand\tthe\tdepth\tdimension,\tso\teither\tksize[1]\tand ksize[2]\tmust\tboth\tbe\tequal\tto\t1,\tor\tksize[3]\tmust\tbe\tequal\tto\t1.\n\nTo\tcreate\tan\taverage\tpooling\tlayer,\tjust\tuse\tthe\tavg_pool()\tfunction\tinstead\tof\tmax_pool().\n\nNow\tyou\tknow\tall\tthe\tbuilding\tblocks\tto\tcreate\ta\tconvolutional\tneural\tnetwork.\tLet’s\tsee\thow\tto assemble\tthem.",
      "content_length": 748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "CNN\tArchitectures Typical\tCNN\tarchitectures\tstack\ta\tfew\tconvolutional\tlayers\t(each\tone\tgenerally\tfollowed\tby\ta\tReLU layer),\tthen\ta\tpooling\tlayer,\tthen\tanother\tfew\tconvolutional\tlayers\t(+ReLU),\tthen\tanother\tpooling\tlayer, and\tso\ton.\tThe\timage\tgets\tsmaller\tand\tsmaller\tas\tit\tprogresses\tthrough\tthe\tnetwork,\tbut\tit\talso\ttypically gets\tdeeper\tand\tdeeper\t(i.e.,\twith\tmore\tfeature\tmaps)\tthanks\tto\tthe\tconvolutional\tlayers\t(see\tFigure\t13- 9).\tAt\tthe\ttop\tof\tthe\tstack,\ta\tregular\tfeedforward\tneural\tnetwork\tis\tadded,\tcomposed\tof\ta\tfew\tfully connected\tlayers\t(+ReLUs),\tand\tthe\tfinal\tlayer\toutputs\tthe\tprediction\t(e.g.,\ta\tsoftmax\tlayer\tthat\toutputs estimated\tclass\tprobabilities).\n\nFigure\t13-9.\tTypical\tCNN\tarchitecture\n\nTIP\n\nA\tcommon\tmistake\tis\tto\tuse\tconvolution\tkernels\tthat\tare\ttoo\tlarge.\tYou\tcan\toften\tget\tthe\tsame\teffect\tas\ta\t9\t×\t9\tkernel\tby stacking\ttwo\t3\t×\t3\tkernels\ton\ttop\tof\teach\tother,\tfor\ta\tlot\tless\tcompute.\n\nOver\tthe\tyears,\tvariants\tof\tthis\tfundamental\tarchitecture\thave\tbeen\tdeveloped,\tleading\tto\tamazing advances\tin\tthe\tfield.\tA\tgood\tmeasure\tof\tthis\tprogress\tis\tthe\terror\trate\tin\tcompetitions\tsuch\tas\tthe ILSVRC\tImageNet\tchallenge.\tIn\tthis\tcompetition\tthe\ttop-5\terror\trate\tfor\timage\tclassification\tfell\tfrom over\t26%\tto\tbarely\tover\t3%\tin\tjust\tfive\tyears.\tThe\ttop-five\terror\trate\tis\tthe\tnumber\tof\ttest\timages\tfor which\tthe\tsystem’s\ttop\t5\tpredictions\tdid\tnot\tinclude\tthe\tcorrect\tanswer.\tThe\timages\tare\tlarge\t(256\tpixels high)\tand\tthere\tare\t1,000\tclasses,\tsome\tof\twhich\tare\treally\tsubtle\t(try\tdistinguishing\t120\tdog\tbreeds). Looking\tat\tthe\tevolution\tof\tthe\twinning\tentries\tis\ta\tgood\tway\tto\tunderstand\thow\tCNNs\twork.\n\nWe\twill\tfirst\tlook\tat\tthe\tclassical\tLeNet-5\tarchitecture\t(1998),\tthen\tthree\tof\tthe\twinners\tof\tthe\tILSVRC challenge:\tAlexNet\t(2012),\tGoogLeNet\t(2014),\tand\tResNet\t(2015).\n\nOTHER\tVISUAL\tTASKS\n\nThere\twas\tstunning\tprogress\tas\twell\tin\tother\tvisual\ttasks\tsuch\tas\tobject\tdetection\tand\tlocalization,\tand\timage\tsegmentation.\tIn\tobject detection\tand\tlocalization,\tthe\tneural\tnetwork\ttypically\toutputs\ta\tsequence\tof\tbounding\tboxes\taround\tvarious\tobjects\tin\tthe\timage.\tFor example,\tsee\tMaxine\tOquab\tet\tal.’s\t2015\tpaper\tthat\toutputs\ta\theat\tmap\tfor\teach\tobject\tclass,\tor\tRussell\tStewart\tet\tal.’s\t2015\tpaper\tthat uses\ta\tcombination\tof\ta\tCNN\tto\tdetect\tfaces\tand\ta\trecurrent\tneural\tnetwork\tto\toutput\ta\tsequence\tof\tbounding\tboxes\taround\tthem.\tIn image\tsegmentation,\tthe\tnet\toutputs\tan\timage\t(usually\tof\tthe\tsame\tsize\tas\tthe\tinput)\twhere\teach\tpixel\tindicates\tthe\tclass\tof\tthe\tobject\tto which\tthe\tcorresponding\tinput\tpixel\tbelongs.\tFor\texample,\tcheck\tout\tEvan\tShelhamer\tet\tal.’s\t2016\tpaper.",
      "content_length": 2588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "LeNet-5 The\tLeNet-5\tarchitecture\tis\tperhaps\tthe\tmost\twidely\tknown\tCNN\tarchitecture.\tAs\tmentioned\tearlier,\tit was\tcreated\tby\tYann\tLeCun\tin\t1998\tand\twidely\tused\tfor\thandwritten\tdigit\trecognition\t(MNIST).\tIt\tis composed\tof\tthe\tlayers\tshown\tin\tTable\t13-1.\n\nTable\t13-1.\tLeNet-5\tarchitecture\n\nLayer Type\n\nMaps Size\n\nKernel\tsize Stride Activation\n\nOut\n\nFully\tConnected –\n\n10\n\n–\n\n–\n\nRBF\n\nF6\n\nFully\tConnected –\n\n84\n\n–\n\n–\n\ntanh\n\nC5\n\nConvolution\n\n120\n\n1\t×\t1\n\n5\t×\t5\n\n1\n\ntanh\n\nS4\n\nAvg\tPooling\n\n16\n\n5\t×\t5\n\n2\t×\t2\n\n2\n\ntanh\n\nC3\n\nConvolution\n\n16\n\n10\t×\t10 5\t×\t5\n\n1\n\ntanh\n\nS2\n\nAvg\tPooling\n\n6\n\n14\t×\t14 2\t×\t2\n\n2\n\ntanh\n\nC1\n\nConvolution\n\n6\n\n28\t×\t28 5\t×\t5\n\n1\n\ntanh\n\nIn\n\nInput\n\n1\n\n32\t×\t32 –\n\n–\n\n–\n\nThere\tare\ta\tfew\textra\tdetails\tto\tbe\tnoted:\n\nMNIST\timages\tare\t28\t×\t28\tpixels,\tbut\tthey\tare\tzero-padded\tto\t32\t×\t32\tpixels\tand\tnormalized\tbefore being\tfed\tto\tthe\tnetwork.\tThe\trest\tof\tthe\tnetwork\tdoes\tnot\tuse\tany\tpadding,\twhich\tis\twhy\tthe\tsize keeps\tshrinking\tas\tthe\timage\tprogresses\tthrough\tthe\tnetwork.\n\nThe\taverage\tpooling\tlayers\tare\tslightly\tmore\tcomplex\tthan\tusual:\teach\tneuron\tcomputes\tthe\tmean\tof its\tinputs,\tthen\tmultiplies\tthe\tresult\tby\ta\tlearnable\tcoefficient\t(one\tper\tmap)\tand\tadds\ta\tlearnable bias\tterm\t(again,\tone\tper\tmap),\tthen\tfinally\tapplies\tthe\tactivation\tfunction.\n\nMost\tneurons\tin\tC3\tmaps\tare\tconnected\tto\tneurons\tin\tonly\tthree\tor\tfour\tS2\tmaps\t(instead\tof\tall\tsix S2\tmaps).\tSee\ttable\t1\tin\tthe\toriginal\tpaper\tfor\tdetails.\n\nThe\toutput\tlayer\tis\ta\tbit\tspecial:\tinstead\tof\tcomputing\tthe\tdot\tproduct\tof\tthe\tinputs\tand\tthe\tweight vector,\teach\tneuron\toutputs\tthe\tsquare\tof\tthe\tEuclidian\tdistance\tbetween\tits\tinput\tvector\tand\tits weight\tvector.\tEach\toutput\tmeasures\thow\tmuch\tthe\timage\tbelongs\tto\ta\tparticular\tdigit\tclass.\tThe cross\tentropy\tcost\tfunction\tis\tnow\tpreferred,\tas\tit\tpenalizes\tbad\tpredictions\tmuch\tmore,\tproducing larger\tgradients\tand\tthus\tconverging\tfaster.\n\nYann\tLeCun’s\twebsite\t(“LENET”\tsection)\tfeatures\tgreat\tdemos\tof\tLeNet-5\tclassifying\tdigits.",
      "content_length": 1939,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "AlexNet The\tAlexNet\tCNN\tarchitecture9\twon\tthe\t2012\tImageNet\tILSVRC\tchallenge\tby\ta\tlarge\tmargin:\tit\tachieved 17%\ttop-5\terror\trate\twhile\tthe\tsecond\tbest\tachieved\tonly\t26%!\tIt\twas\tdeveloped\tby\tAlex\tKrizhevsky (hence\tthe\tname),\tIlya\tSutskever,\tand\tGeoffrey\tHinton.\tIt\tis\tquite\tsimilar\tto\tLeNet-5,\tonly\tmuch\tlarger\tand deeper,\tand\tit\twas\tthe\tfirst\tto\tstack\tconvolutional\tlayers\tdirectly\ton\ttop\tof\teach\tother,\tinstead\tof\tstacking\ta pooling\tlayer\ton\ttop\tof\teach\tconvolutional\tlayer.\tTable\t13-2\tpresents\tthis\tarchitecture.\n\nTable\t13-2.\tAlexNet\tarchitecture\n\nLayer Type\n\nMaps\n\nSize\n\nKernel\tsize Stride Padding Activation\n\nOut\n\nFully\tConnected –\n\n1,000\n\n–\n\n–\n\n–\n\nSoftmax\n\nF9\n\nFully\tConnected –\n\n4,096\n\n–\n\n–\n\n–\n\nReLU\n\nF8\n\nFully\tConnected –\n\n4,096\n\n–\n\n–\n\n–\n\nReLU\n\nC7\n\nConvolution\n\n256\n\n13\t×\t13\n\n3\t×\t3\n\n1\n\nSAME ReLU\n\nC6\n\nConvolution\n\n384\n\n13\t×\t13\n\n3\t×\t3\n\n1\n\nSAME ReLU\n\nC5\n\nConvolution\n\n384\n\n13\t×\t13\n\n3\t×\t3\n\n1\n\nSAME ReLU\n\nS4\n\nMax\tPooling\n\n256\n\n13\t×\t13\n\n3\t×\t3\n\n2\n\nVALID –\n\nC3\n\nConvolution\n\n256\n\n27\t×\t27\n\n5\t×\t5\n\n1\n\nSAME ReLU\n\nS2\n\nMax\tPooling\n\n96\n\n27\t×\t27\n\n3\t×\t3\n\n2\n\nVALID –\n\nC1\n\nConvolution\n\n96\n\n55\t×\t55\n\n11\t×\t11\n\n4\n\nSAME ReLU\n\nIn\n\nInput\n\n3\t(RGB) 224\t×\t224 –\n\n–\n\n–\n\n–\n\nTo\treduce\toverfitting,\tthe\tauthors\tused\ttwo\tregularization\ttechniques\twe\tdiscussed\tin\tprevious\tchapters: first\tthey\tapplied\tdropout\t(with\ta\t50%\tdropout\trate)\tduring\ttraining\tto\tthe\toutputs\tof\tlayers\tF8\tand\tF9. Second,\tthey\tperformed\tdata\taugmentation\tby\trandomly\tshifting\tthe\ttraining\timages\tby\tvarious\toffsets, flipping\tthem\thorizontally,\tand\tchanging\tthe\tlighting\tconditions.\n\nAlexNet\talso\tuses\ta\tcompetitive\tnormalization\tstep\timmediately\tafter\tthe\tReLU\tstep\tof\tlayers\tC1\tand\tC3, called\tlocal\tresponse\tnormalization.\tThis\tform\tof\tnormalization\tmakes\tthe\tneurons\tthat\tmost\tstrongly activate\tinhibit\tneurons\tat\tthe\tsame\tlocation\tbut\tin\tneighboring\tfeature\tmaps\t(such\tcompetitive\tactivation has\tbeen\tobserved\tin\tbiological\tneurons).\tThis\tencourages\tdifferent\tfeature\tmaps\tto\tspecialize,\tpushing them\tapart\tand\tforcing\tthem\tto\texplore\ta\twider\trange\tof\tfeatures,\tultimately\timproving\tgeneralization. Equation\t13-2\tshows\thow\tto\tapply\tLRN.\n\nEquation\t13-2.\tLocal\tresponse\tnormalization",
      "content_length": 2133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "bi\tis\tthe\tnormalized\toutput\tof\tthe\tneuron\tlocated\tin\tfeature\tmap\ti,\tat\tsome\trow\tu\tand\tcolumn\tv\t(note that\tin\tthis\tequation\twe\tconsider\tonly\tneurons\tlocated\tat\tthis\trow\tand\tcolumn,\tso\tu\tand\tv\tare\tnot shown).\n\nai\tis\tthe\tactivation\tof\tthat\tneuron\tafter\tthe\tReLU\tstep,\tbut\tbefore\tnormalization.\n\nk,\tα,\tβ,\tand\tr\tare\thyperparameters.\tk\tis\tcalled\tthe\tbias,\tand\tr\tis\tcalled\tthe\tdepth\tradius.\n\nfn\tis\tthe\tnumber\tof\tfeature\tmaps.\n\nFor\texample,\tif\tr\t=\t2\tand\ta\tneuron\thas\ta\tstrong\tactivation,\tit\twill\tinhibit\tthe\tactivation\tof\tthe\tneurons located\tin\tthe\tfeature\tmaps\timmediately\tabove\tand\tbelow\tits\town.\n\nIn\tAlexNet,\tthe\thyperparameters\tare\tset\tas\tfollows:\tr\t=\t2,\tα\t=\t0.00002,\tβ\t=\t0.75,\tand\tk\t=\t1.\tThis\tstep\tcan be\timplemented\tusing\tTensorFlow’s\ttf.nn.local_response_normalization()\toperation.\n\nA\tvariant\tof\tAlexNet\tcalled\tZF\tNet\twas\tdeveloped\tby\tMatthew\tZeiler\tand\tRob\tFergus\tand\twon\tthe\t2013 ILSVRC\tchallenge.\tIt\tis\tessentially\tAlexNet\twith\ta\tfew\ttweaked\thyperparameters\t(number\tof\tfeature maps,\tkernel\tsize,\tstride,\tetc.).",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "GoogLeNet The\tGoogLeNet\tarchitecture\twas\tdeveloped\tby\tChristian\tSzegedy\tet\tal.\tfrom\tGoogle\tResearch,10\tand\tit won\tthe\tILSVRC\t2014\tchallenge\tby\tpushing\tthe\ttop-5\terror\trate\tbelow\t7%.\tThis\tgreat\tperformance\tcame in\tlarge\tpart\tfrom\tthe\tfact\tthat\tthe\tnetwork\twas\tmuch\tdeeper\tthan\tprevious\tCNNs\t(see\tFigure\t13-11).\tThis was\tmade\tpossible\tby\tsub-networks\tcalled\tinception\tmodules,11\twhich\tallow\tGoogLeNet\tto\tuse parameters\tmuch\tmore\tefficiently\tthan\tprevious\tarchitectures:\tGoogLeNet\tactually\thas\t10\ttimes\tfewer parameters\tthan\tAlexNet\t(roughly\t6\tmillion\tinstead\tof\t60\tmillion).\n\nFigure\t13-10\tshows\tthe\tarchitecture\tof\tan\tinception\tmodule.\tThe\tnotation\t“3\t×\t3\t+\t2(S)”\tmeans\tthat\tthe layer\tuses\ta\t3\t×\t3\tkernel,\tstride\t2,\tand\tSAME\tpadding.\tThe\tinput\tsignal\tis\tfirst\tcopied\tand\tfed\tto\tfour different\tlayers.\tAll\tconvolutional\tlayers\tuse\tthe\tReLU\tactivation\tfunction.\tNote\tthat\tthe\tsecond\tset\tof convolutional\tlayers\tuses\tdifferent\tkernel\tsizes\t(1\t×\t1,\t3\t×\t3,\tand\t5\t×\t5),\tallowing\tthem\tto\tcapture patterns\tat\tdifferent\tscales.\tAlso\tnote\tthat\tevery\tsingle\tlayer\tuses\ta\tstride\tof\t1\tand\tSAME\tpadding\t(even the\tmax\tpooling\tlayer),\tso\ttheir\toutputs\tall\thave\tthe\tsame\theight\tand\twidth\tas\ttheir\tinputs.\tThis\tmakes\tit possible\tto\tconcatenate\tall\tthe\toutputs\talong\tthe\tdepth\tdimension\tin\tthe\tfinal\tdepth\tconcat\tlayer\t(i.e., stack\tthe\tfeature\tmaps\tfrom\tall\tfour\ttop\tconvolutional\tlayers).\tThis\tconcatenation\tlayer\tcan\tbe implemented\tin\tTensorFlow\tusing\tthe\ttf.concat()\toperation,\twith\taxis=3\t(axis\t3\tis\tthe\tdepth).\n\nFigure\t13-10.\tInception\tmodule\n\nYou\tmay\twonder\twhy\tinception\tmodules\thave\tconvolutional\tlayers\twith\t1\t×\t1\tkernels.\tSurely\tthese\tlayers cannot\tcapture\tany\tfeatures\tsince\tthey\tlook\tat\tonly\tone\tpixel\tat\ta\ttime?\tIn\tfact,\tthese\tlayers\tserve\ttwo purposes:\n\nFirst,\tthey\tare\tconfigured\tto\toutput\tmany\tfewer\tfeature\tmaps\tthan\ttheir\tinputs,\tso\tthey\tserve\tas bottleneck\tlayers,\tmeaning\tthey\treduce\tdimensionality.\tThis\tis\tparticularly\tuseful\tbefore\tthe\t3\t×\t3 and\t5\t×\t5\tconvolutions,\tsince\tthese\tare\tvery\tcomputationally\texpensive\tlayers.\n\nSecond,\teach\tpair\tof\tconvolutional\tlayers\t([1\t×\t1,\t3\t×\t3]\tand\t[1\t×\t1,\t5\t×\t5])\tacts\tlike\ta\tsingle,",
      "content_length": 2121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "powerful\tconvolutional\tlayer,\tcapable\tof\tcapturing\tmore\tcomplex\tpatterns.\tIndeed,\tinstead\tof sweeping\ta\tsimple\tlinear\tclassifier\tacross\tthe\timage\t(as\ta\tsingle\tconvolutional\tlayer\tdoes),\tthis\tpair of\tconvolutional\tlayers\tsweeps\ta\ttwo-layer\tneural\tnetwork\tacross\tthe\timage.\n\nIn\tshort,\tyou\tcan\tthink\tof\tthe\twhole\tinception\tmodule\tas\ta\tconvolutional\tlayer\ton\tsteroids,\table\tto\toutput feature\tmaps\tthat\tcapture\tcomplex\tpatterns\tat\tvarious\tscales.\n\nWARNING\n\nThe\tnumber\tof\tconvolutional\tkernels\tfor\teach\tconvolutional\tlayer\tis\ta\thyperparameter.\tUnfortunately,\tthis\tmeans\tthat\tyou\thave six\tmore\thyperparameters\tto\ttweak\tfor\tevery\tinception\tlayer\tyou\tadd.\n\nNow\tlet’s\tlook\tat\tthe\tarchitecture\tof\tthe\tGoogLeNet\tCNN\t(see\tFigure\t13-11).\tIt\tis\tso\tdeep\tthat\twe\thad\tto represent\tit\tin\tthree\tcolumns,\tbut\tGoogLeNet\tis\tactually\tone\ttall\tstack,\tincluding\tnine\tinception\tmodules (the\tboxes\twith\tthe\tspinning\ttops)\tthat\tactually\tcontain\tthree\tlayers\teach.\tThe\tnumber\tof\tfeature\tmaps output\tby\teach\tconvolutional\tlayer\tand\teach\tpooling\tlayer\tis\tshown\tbefore\tthe\tkernel\tsize.\tThe\tsix numbers\tin\tthe\tinception\tmodules\trepresent\tthe\tnumber\tof\tfeature\tmaps\toutput\tby\teach\tconvolutional\tlayer in\tthe\tmodule\t(in\tthe\tsame\torder\tas\tin\tFigure\t13-10).\tNote\tthat\tall\tthe\tconvolutional\tlayers\tuse\tthe\tReLU activation\tfunction.\n\nFigure\t13-11.\tGoogLeNet\tarchitecture",
      "content_length": 1330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "Let’s\tgo\tthrough\tthis\tnetwork:\n\nThe\tfirst\ttwo\tlayers\tdivide\tthe\timage’s\theight\tand\twidth\tby\t4\t(so\tits\tarea\tis\tdivided\tby\t16),\tto\treduce the\tcomputational\tload.\n\nThen\tthe\tlocal\tresponse\tnormalization\tlayer\tensures\tthat\tthe\tprevious\tlayers\tlearn\ta\twide\tvariety\tof features\t(as\tdiscussed\tearlier).\n\nTwo\tconvolutional\tlayers\tfollow,\twhere\tthe\tfirst\tacts\tlike\ta\tbottleneck\tlayer.\tAs\texplained\tearlier, you\tcan\tthink\tof\tthis\tpair\tas\ta\tsingle\tsmarter\tconvolutional\tlayer.\n\nAgain,\ta\tlocal\tresponse\tnormalization\tlayer\tensures\tthat\tthe\tprevious\tlayers\tcapture\ta\twide\tvariety of\tpatterns.\n\nNext\ta\tmax\tpooling\tlayer\treduces\tthe\timage\theight\tand\twidth\tby\t2,\tagain\tto\tspeed\tup\tcomputations.\n\nThen\tcomes\tthe\ttall\tstack\tof\tnine\tinception\tmodules,\tinterleaved\twith\ta\tcouple\tmax\tpooling\tlayers\tto reduce\tdimensionality\tand\tspeed\tup\tthe\tnet.\n\nNext,\tthe\taverage\tpooling\tlayer\tuses\ta\tkernel\tthe\tsize\tof\tthe\tfeature\tmaps\twith\tVALID\tpadding, outputting\t1\t×\t1\tfeature\tmaps:\tthis\tsurprising\tstrategy\tis\tcalled\tglobal\taverage\tpooling.\tIt\teffectively forces\tthe\tprevious\tlayers\tto\tproduce\tfeature\tmaps\tthat\tare\tactually\tconfidence\tmaps\tfor\teach\ttarget class\t(since\tother\tkinds\tof\tfeatures\twould\tbe\tdestroyed\tby\tthe\taveraging\tstep).\tThis\tmakes\tit unnecessary\tto\thave\tseveral\tfully\tconnected\tlayers\tat\tthe\ttop\tof\tthe\tCNN\t(like\tin\tAlexNet), considerably\treducing\tthe\tnumber\tof\tparameters\tin\tthe\tnetwork\tand\tlimiting\tthe\trisk\tof\toverfitting.\n\nThe\tlast\tlayers\tare\tself-explanatory:\tdropout\tfor\tregularization,\tthen\ta\tfully\tconnected\tlayer\twith\ta softmax\tactivation\tfunction\tto\toutput\testimated\tclass\tprobabilities.\n\nThis\tdiagram\tis\tslightly\tsimplified:\tthe\toriginal\tGoogLeNet\tarchitecture\talso\tincluded\ttwo\tauxiliary classifiers\tplugged\ton\ttop\tof\tthe\tthird\tand\tsixth\tinception\tmodules.\tThey\twere\tboth\tcomposed\tof\tone average\tpooling\tlayer,\tone\tconvolutional\tlayer,\ttwo\tfully\tconnected\tlayers,\tand\ta\tsoftmax\tactivation\tlayer. During\ttraining,\ttheir\tloss\t(scaled\tdown\tby\t70%)\twas\tadded\tto\tthe\toverall\tloss.\tThe\tgoal\twas\tto\tfight\tthe vanishing\tgradients\tproblem\tand\tregularize\tthe\tnetwork.\tHowever,\tit\twas\tshown\tthat\ttheir\teffect\twas relatively\tminor.",
      "content_length": 2117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "ResNet Last\tbut\tnot\tleast,\tthe\twinner\tof\tthe\tILSVRC\t2015\tchallenge\twas\tthe\tResidual\tNetwork\t(or\tResNet), developed\tby\tKaiming\tHe\tet\tal.,12\twhich\tdelivered\tan\tastounding\ttop-5\terror\trate\tunder\t3.6%,\tusing\tan extremely\tdeep\tCNN\tcomposed\tof\t152\tlayers.\tThe\tkey\tto\tbeing\table\tto\ttrain\tsuch\ta\tdeep\tnetwork\tis\tto\tuse skip\tconnections\t(also\tcalled\tshortcut\tconnections):\tthe\tsignal\tfeeding\tinto\ta\tlayer\tis\talso\tadded\tto\tthe output\tof\ta\tlayer\tlocated\ta\tbit\thigher\tup\tthe\tstack.\tLet’s\tsee\twhy\tthis\tis\tuseful.\n\nWhen\ttraining\ta\tneural\tnetwork,\tthe\tgoal\tis\tto\tmake\tit\tmodel\ta\ttarget\tfunction\th(x).\tIf\tyou\tadd\tthe\tinput\tx to\tthe\toutput\tof\tthe\tnetwork\t(i.e.,\tyou\tadd\ta\tskip\tconnection),\tthen\tthe\tnetwork\twill\tbe\tforced\tto\tmodel f(x)\t=\th(x)\t–\tx\trather\tthan\th(x).\tThis\tis\tcalled\tresidual\tlearning\t(see\tFigure\t13-12).\n\nFigure\t13-12.\tResidual\tlearning\n\nWhen\tyou\tinitialize\ta\tregular\tneural\tnetwork,\tits\tweights\tare\tclose\tto\tzero,\tso\tthe\tnetwork\tjust\toutputs values\tclose\tto\tzero.\tIf\tyou\tadd\ta\tskip\tconnection,\tthe\tresulting\tnetwork\tjust\toutputs\ta\tcopy\tof\tits\tinputs;\tin other\twords,\tit\tinitially\tmodels\tthe\tidentity\tfunction.\tIf\tthe\ttarget\tfunction\tis\tfairly\tclose\tto\tthe\tidentity function\t(which\tis\toften\tthe\tcase),\tthis\twill\tspeed\tup\ttraining\tconsiderably.\n\nMoreover,\tif\tyou\tadd\tmany\tskip\tconnections,\tthe\tnetwork\tcan\tstart\tmaking\tprogress\teven\tif\tseveral\tlayers have\tnot\tstarted\tlearning\tyet\t(see\tFigure\t13-13).\tThanks\tto\tskip\tconnections,\tthe\tsignal\tcan\teasily\tmake\tits way\tacross\tthe\twhole\tnetwork.\tThe\tdeep\tresidual\tnetwork\tcan\tbe\tseen\tas\ta\tstack\tof\tresidual\tunits,\twhere each\tresidual\tunit\tis\ta\tsmall\tneural\tnetwork\twith\ta\tskip\tconnection.",
      "content_length": 1628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "Figure\t13-13.\tRegular\tdeep\tneural\tnetwork\t(left)\tand\tdeep\tresidual\tnetwork\t(right)\n\nNow\tlet’s\tlook\tat\tResNet’s\tarchitecture\t(see\tFigure\t13-14).\tIt\tis\tactually\tsurprisingly\tsimple.\tIt\tstarts\tand ends\texactly\tlike\tGoogLeNet\t(except\twithout\ta\tdropout\tlayer),\tand\tin\tbetween\tis\tjust\ta\tvery\tdeep\tstack\tof simple\tresidual\tunits.\tEach\tresidual\tunit\tis\tcomposed\tof\ttwo\tconvolutional\tlayers,\twith\tBatch Normalization\t(BN)\tand\tReLU\tactivation,\tusing\t3\t×\t3\tkernels\tand\tpreserving\tspatial\tdimensions\t(stride\t1, SAME\tpadding).\n\nFigure\t13-14.\tResNet\tarchitecture\n\nNote\tthat\tthe\tnumber\tof\tfeature\tmaps\tis\tdoubled\tevery\tfew\tresidual\tunits,\tat\tthe\tsame\ttime\tas\ttheir\theight and\twidth\tare\thalved\t(using\ta\tconvolutional\tlayer\twith\tstride\t2).\tWhen\tthis\thappens\tthe\tinputs\tcannot\tbe added\tdirectly\tto\tthe\toutputs\tof\tthe\tresidual\tunit\tsince\tthey\tdon’t\thave\tthe\tsame\tshape\t(for\texample,\tthis",
      "content_length": 868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "problem\taffects\tthe\tskip\tconnection\trepresented\tby\tthe\tdashed\tarrow\tin\tFigure\t13-14).\tTo\tsolve\tthis problem,\tthe\tinputs\tare\tpassed\tthrough\ta\t1\t×\t1\tconvolutional\tlayer\twith\tstride\t2\tand\tthe\tright\tnumber\tof output\tfeature\tmaps\t(see\tFigure\t13-15).\n\nFigure\t13-15.\tSkip\tconnection\twhen\tchanging\tfeature\tmap\tsize\tand\tdepth\n\nResNet-34\tis\tthe\tResNet\twith\t34\tlayers\t(only\tcounting\tthe\tconvolutional\tlayers\tand\tthe\tfully\tconnected layer)\tcontaining\tthree\tresidual\tunits\tthat\toutput\t64\tfeature\tmaps,\t4\tRUs\twith\t128\tmaps,\t6\tRUs\twith\t256 maps,\tand\t3\tRUs\twith\t512\tmaps.\n\nResNets\tdeeper\tthan\tthat,\tsuch\tas\tResNet-152,\tuse\tslightly\tdifferent\tresidual\tunits.\tInstead\tof\ttwo\t3\t×\t3 convolutional\tlayers\twith\t(say)\t256\tfeature\tmaps,\tthey\tuse\tthree\tconvolutional\tlayers:\tfirst\ta\t1\t×\t1 convolutional\tlayer\twith\tjust\t64\tfeature\tmaps\t(4\ttimes\tless),\twhich\tacts\ta\ta\tbottleneck\tlayer\t(as\tdiscussed already),\tthen\ta\t3\t×\t3\tlayer\twith\t64\tfeature\tmaps,\tand\tfinally\tanother\t1\t×\t1\tconvolutional\tlayer\twith\t256 feature\tmaps\t(4\ttimes\t64)\tthat\trestores\tthe\toriginal\tdepth.\tResNet-152\tcontains\tthree\tsuch\tRUs\tthat\toutput 256\tmaps,\tthen\t8\tRUs\twith\t512\tmaps,\ta\twhopping\t36\tRUs\twith\t1,024\tmaps,\tand\tfinally\t3\tRUs\twith\t2,048 maps.\n\nAs\tyou\tcan\tsee,\tthe\tfield\tis\tmoving\trapidly,\twith\tall\tsorts\tof\tarchitectures\tpopping\tout\tevery\tyear.\tOne clear\ttrend\tis\tthat\tCNNs\tkeep\tgetting\tdeeper\tand\tdeeper.\tThey\tare\talso\tgetting\tlighter,\trequiring\tfewer\tand fewer\tparameters.\tAt\tpresent,\tthe\tResNet\tarchitecture\tis\tboth\tthe\tmost\tpowerful\tand\targuably\tthe simplest,\tso\tit\tis\treally\tthe\tone\tyou\tshould\tprobably\tuse\tfor\tnow,\tbut\tkeep\tlooking\tat\tthe\tILSVRC challenge\tevery\tyear.\tThe\t2016\twinners\twere\tthe\tTrimps-Soushen\tteam\tfrom\tChina\twith\tan\tastounding 2.99%\terror\trate.\tTo\tachieve\tthis\tthey\ttrained\tcombinations\tof\tthe\tprevious\tmodels\tand\tjoined\tthem\tinto an\tensemble.\tDepending\ton\tthe\ttask,\tthe\treduced\terror\trate\tmay\tor\tmay\tnot\tbe\tworth\tthe\textra\tcomplexity. There\tare\ta\tfew\tother\tarchitectures\tthat\tyou\tmay\twant\tto\tlook\tat,\tin\tparticular\tVGGNet13\t(runner-up\tof\tthe ILSVRC\t2014\tchallenge)\tand\tInception-v414\t(which\tmerges\tthe\tideas\tof\tGoogLeNet\tand\tResNet\tand achieves\tclose\tto\t3%\ttop-5\terror\trate\ton\tImageNet\tclassification).",
      "content_length": 2174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "NOTE\n\nThere\tis\treally\tnothing\tspecial\tabout\timplementing\tthe\tvarious\tCNN\tarchitectures\twe\tjust\tdiscussed.\tWe\tsaw\tearlier\thow\tto\tbuild all\tthe\tindividual\tbuilding\tblocks,\tso\tnow\tall\tyou\tneed\tis\tto\tassemble\tthem\tto\tcreate\tthe\tdesired\tarchitecture.\tWe\twill\tbuild\ta complete\tCNN\tin\tthe\tupcoming\texercises\tand\tyou\twill\tfind\tfull\tworking\tcode\tin\tthe\tJupyter\tnotebooks.\n\nTENSORFLOW\tCONVOLUTION\tOPERATIONS\n\nTensorFlow\talso\toffers\ta\tfew\tother\tkinds\tof\tconvolutional\tlayers:\n\ntf.layers.conv1d()\tcreates\ta\tconvolutional\tlayer\tfor\t1D\tinputs.\tThis\tis\tuseful,\tfor\texample,\tin\tnatural\tlanguage\tprocessing, where\ta\tsentence\tmay\tbe\trepresented\tas\ta\t1D\tarray\tof\twords,\tand\tthe\treceptive\tfield\tcovers\ta\tfew\tneighboring\twords.\n\ntf.layers.conv3d()\tcreates\ta\tconvolutional\tlayer\tfor\t3D\tinputs,\tsuch\tas\t3D\tPET\tscan.\n\ntf.nn.atrous_conv2d()\tcreates\tan\tatrous\tconvolutional\tlayer\t(“à\ttrous”\tis\tFrench\tfor\t“with\tholes”).\tThis\tis\tequivalent\tto\tusing a\tregular\tconvolutional\tlayer\twith\ta\tfilter\tdilated\tby\tinserting\trows\tand\tcolumns\tof\tzeros\t(i.e.,\tholes).\tFor\texample,\ta\t1\t×\t3\tfilter equal\tto\t[[1,2,3]]\tmay\tbe\tdilated\twith\ta\tdilation\trate\tof\t4,\tresulting\tin\ta\tdilated\tfilter\t[[1,\t0,\t0,\t0,\t2,\t0,\t0,\t0,\t3]].\tThis allows\tthe\tconvolutional\tlayer\tto\thave\ta\tlarger\treceptive\tfield\tat\tno\tcomputational\tprice\tand\tusing\tno\textra\tparameters.\n\ntf.layers.conv2d_transpose()\tcreates\ta\ttranspose\tconvolutional\tlayer,\tsometimes\tcalled\ta\tdeconvolutional\tlayer,15\twhich upsamples\tan\timage.\tIt\tdoes\tso\tby\tinserting\tzeros\tbetween\tthe\tinputs,\tso\tyou\tcan\tthink\tof\tthis\tas\ta\tregular\tconvolutional\tlayer using\ta\tfractional\tstride.\tUpsampling\tis\tuseful,\tfor\texample,\tin\timage\tsegmentation:\tin\ta\ttypical\tCNN,\tfeature\tmaps\tget\tsmaller\tand smaller\tas\tyou\tprogress\tthrough\tthe\tnetwork,\tso\tif\tyou\twant\tto\toutput\tan\timage\tof\tthe\tsame\tsize\tas\tthe\tinput,\tyou\tneed\tan upsampling\tlayer.\n\ntf.nn.depthwise_conv2d()\tcreates\ta\tdepthwise\tconvolutional\tlayer\tthat\tapplies\tevery\tfilter\tto\tevery\tindividual\tinput\tchannel independently.\tThus,\tif\tthere\tare\tfn\tfilters\tand\tfn′\tinput\tchannels,\tthen\tthis\twill\toutput\tfn\t×\tfn′\tfeature\tmaps.\n\ntf.layers.separable_conv2d()\tcreates\ta\tseparable\tconvolutional\tlayer\tthat\tfirst\tacts\tlike\ta\tdepthwise\tconvolutional\tlayer, then\tapplies\ta\t1\t×\t1\tconvolutional\tlayer\tto\tthe\tresulting\tfeature\tmaps.\tThis\tmakes\tit\tpossible\tto\tapply\tfilters\tto\tarbitrary\tsets\tof inputs\tchannels.",
      "content_length": 2338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "Exercises\n\n1.\t What\tare\tthe\tadvantages\tof\ta\tCNN\tover\ta\tfully\tconnected\tDNN\tfor\timage\tclassification?\n\n2.\t Consider\ta\tCNN\tcomposed\tof\tthree\tconvolutional\tlayers,\teach\twith\t3\t×\t3\tkernels,\ta\tstride\tof\t2,\tand SAME\tpadding.\tThe\tlowest\tlayer\toutputs\t100\tfeature\tmaps,\tthe\tmiddle\tone\toutputs\t200,\tand\tthe\ttop one\toutputs\t400.\tThe\tinput\timages\tare\tRGB\timages\tof\t200\t×\t300\tpixels.\tWhat\tis\tthe\ttotal\tnumber\tof parameters\tin\tthe\tCNN?\tIf\twe\tare\tusing\t32-bit\tfloats,\tat\tleast\thow\tmuch\tRAM\twill\tthis\tnetwork require\twhen\tmaking\ta\tprediction\tfor\ta\tsingle\tinstance?\tWhat\tabout\twhen\ttraining\ton\ta\tmini-batch\tof 50\timages?\n\n3.\t If\tyour\tGPU\truns\tout\tof\tmemory\twhile\ttraining\ta\tCNN,\twhat\tare\tfive\tthings\tyou\tcould\ttry\tto\tsolve\tthe problem?\n\n4.\t Why\twould\tyou\twant\tto\tadd\ta\tmax\tpooling\tlayer\trather\tthan\ta\tconvolutional\tlayer\twith\tthe\tsame stride?\n\n5.\t When\twould\tyou\twant\tto\tadd\ta\tlocal\tresponse\tnormalization\tlayer?\n\n6.\t Can\tyou\tname\tthe\tmain\tinnovations\tin\tAlexNet,\tcompared\tto\tLeNet-5?\tWhat\tabout\tthe\tmain innovations\tin\tGoogLeNet\tand\tResNet?\n\n7.\t Build\tyour\town\tCNN\tand\ttry\tto\tachieve\tthe\thighest\tpossible\taccuracy\ton\tMNIST.\n\n8.\t Classifying\tlarge\timages\tusing\tInception\tv3.\n\na.\t Download\tsome\timages\tof\tvarious\tanimals.\tLoad\tthem\tin\tPython,\tfor\texample\tusing\tthe\n\nmatplotlib.image.mpimg.imread()\tfunction\tor\tthe\tscipy.misc.imread()\tfunction.\tResize and/or\tcrop\tthem\tto\t299\t×\t299\tpixels,\tand\tensure\tthat\tthey\thave\tjust\tthree\tchannels\t(RGB),\twith no\ttransparency\tchannel.\n\nb.\t Download\tthe\tlatest\tpretrained\tInception\tv3\tmodel:\tthe\tcheckpoint\tis\tavailable\tat\n\nhttps://goo.gl/nxSQvl.\n\nc.\t Create\tthe\tInception\tv3\tmodel\tby\tcalling\tthe\tinception_v3()\tfunction,\tas\tshown\tbelow.\tThis must\tbe\tdone\twithin\tan\targument\tscope\tcreated\tby\tthe\tinception_v3_arg_scope()\tfunction. Also,\tyou\tmust\tset\tis_training=False\tand\tnum_classes=1001\tlike\tso:\n\nfrom\ttensorflow.contrib.slim.nets\timport\tinception import\ttensorflow.contrib.slim\tas\tslim\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\t299,\t299,\t3],\tname=\"X\") with\tslim.arg_scope(inception.inception_v3_arg_scope()): \t\t\t\tlogits,\tend_points\t=\tinception.inception_v3( \t\t\t\t\t\t\t\tX,\tnum_classes=1001,\tis_training=False) predictions\t=\tend_points[\"Predictions\"] saver\t=\ttf.train.Saver()\n\nd.\t Open\ta\tsession\tand\tuse\tthe\tSaver\tto\trestore\tthe\tpretrained\tmodel\tcheckpoint\tyou\tdownloaded\n\nearlier.",
      "content_length": 2301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "e.\t Run\tthe\tmodel\tto\tclassify\tthe\timages\tyou\tprepared.\tDisplay\tthe\ttop\tfive\tpredictions\tfor\teach\n\nimage,\talong\twith\tthe\testimated\tprobability\t(the\tlist\tof\tclass\tnames\tis\tavailable\tat https://goo.gl/brXRtZ).\tHow\taccurate\tis\tthe\tmodel?\n\n9.\t Transfer\tlearning\tfor\tlarge\timage\tclassification.\n\na.\t Create\ta\ttraining\tset\tcontaining\tat\tleast\t100\timages\tper\tclass.\tFor\texample,\tyou\tcould\tclassify your\town\tpictures\tbased\ton\tthe\tlocation\t(beach,\tmountain,\tcity,\tetc.),\tor\talternatively\tyou\tcan just\tuse\tan\texisting\tdataset,\tsuch\tas\tthe\tflowers\tdataset\tor\tMIT’s\tplaces\tdataset\t(requires registration,\tand\tit\tis\thuge).\n\nb.\t Write\ta\tpreprocessing\tstep\tthat\twill\tresize\tand\tcrop\tthe\timage\tto\t299\t×\t299,\twith\tsome\n\nrandomness\tfor\tdata\taugmentation.\n\nc.\t Using\tthe\tpretrained\tInception\tv3\tmodel\tfrom\tthe\tprevious\texercise,\tfreeze\tall\tlayers\tup\tto\tthe bottleneck\tlayer\t(i.e.,\tthe\tlast\tlayer\tbefore\tthe\toutput\tlayer),\tand\treplace\tthe\toutput\tlayer\twith the\tappropriate\tnumber\tof\toutputs\tfor\tyour\tnew\tclassification\ttask\t(e.g.,\tthe\tflowers\tdataset\thas five\tmutually\texclusive\tclasses\tso\tthe\toutput\tlayer\tmust\thave\tfive\tneurons\tand\tuse\tthe\tsoftmax activation\tfunction).\n\nd.\t Split\tyour\tdataset\tinto\ta\ttraining\tset\tand\ta\ttest\tset.\tTrain\tthe\tmodel\ton\tthe\ttraining\tset\tand\n\nevaluate\tit\ton\tthe\ttest\tset.\n\n10.\t Go\tthrough\tTensorFlow’s\tDeepDream\ttutorial.\tIt\tis\ta\tfun\tway\tto\tfamiliarize\tyourself\twith\tvarious ways\tof\tvisualizing\tthe\tpatterns\tlearned\tby\ta\tCNN,\tand\tto\tgenerate\tart\tusing\tDeep\tLearning.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“Single\tUnit\tActivity\tin\tStriate\tCortex\tof\tUnrestrained\tCats,”\tD.\tHubel\tand\tT.\tWiesel\t(1958).\n\n2\n\n“Receptive\tFields\tof\tSingle\tNeurones\tin\tthe\tCat’s\tStriate\tCortex,”\tD.\tHubel\tand\tT.\tWiesel\t(1959).\n\n3\n\n“Receptive\tFields\tand\tFunctional\tArchitecture\tof\tMonkey\tStriate\tCortex,”\tD.\tHubel\tand\tT.\tWiesel\t(1968).\n\n4\n\n“Neocognitron:\tA\tSelf-organizing\tNeural\tNetwork\tModel\tfor\ta\tMechanism\tof\tPattern\tRecognition\tUnaffected\tby\tShift\tin\tPosition,”\tK. Fukushima\t(1980).\n\n5\n\n“Gradient-Based\tLearning\tApplied\tto\tDocument\tRecognition,”\tY.\tLeCun\tet\tal.\t(1998).\n\n6\n\nA\tconvolution\tis\ta\tmathematical\toperation\tthat\tslides\tone\tfunction\tover\tanother\tand\tmeasures\tthe\tintegral\tof\ttheir\tpointwise\tmultiplication. It\thas\tdeep\tconnections\twith\tthe\tFourier\ttransform\tand\tthe\tLaplace\ttransform,\tand\tis\theavily\tused\tin\tsignal\tprocessing.\tConvolutional layers\tactually\tuse\tcross-correlations,\twhich\tare\tvery\tsimilar\tto\tconvolutions\t(see\thttp://goo.gl/HAfxXd\tfor\tmore\tdetails).\n\n7\n\nA\tfully\tconnected\tlayer\twith\t150\t×\t100\tneurons,\teach\tconnected\tto\tall\t150\t×\t100\t×\t3\tinputs,\twould\thave\t150\n\n2\n\n×\t100\n\n2\n\n×\t3\t=\t675\tmillion\tparameters!\n\n8\n\n1\tMB\t=\t1,024\tkB\t=\t1,024\t×\t1,024\tbytes\t=\t1,024\t×\t1,024\t×\t8\tbits.\n\n9\n\n“ImageNet\tClassification\twith\tDeep\tConvolutional\tNeural\tNetworks,”\tA.\tKrizhevsky\tet\tal.\t(2012).\n\n10\n\n“Going\tDeeper\twith\tConvolutions,”\tC.\tSzegedy\tet\tal.\t(2015).\n\n11\n\nIn\tthe\t2010\tmovie\tInception,\tthe\tcharacters\tkeep\tgoing\tdeeper\tand\tdeeper\tinto\tmultiple\tlayers\tof\tdreams,\thence\tthe\tname\tof\tthese modules.\n\n12",
      "content_length": 3019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "12\n\n13\n\n14\n\n15\n\n“Deep\tResidual\tLearning\tfor\tImage\tRecognition,”\tK.\tHe\t(2015).\n\n“Very\tDeep\tConvolutional\tNetworks\tfor\tLarge-Scale\tImage\tRecognition,”\tK.\tSimonyan\tand\tA.\tZisserman\t(2015).\n\n“Inception-v4,\tInception-ResNet\tand\tthe\tImpact\tof\tResidual\tConnections\ton\tLearning,”\tC.\tSzegedy\tet\tal.\t(2016).\n\nThis\tname\tis\tquite\tmisleading\tsince\tthis\tlayer\tdoes\tnot\tperform\ta\tdeconvolution,\twhich\tis\ta\twell-defined\tmathematical\toperation\t(the inverse\tof\ta\tconvolution).",
      "content_length": 458,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 469,
      "content": "Chapter\t14.\tRecurrent\tNeural\tNetworks\n\nThe\tbatter\thits\tthe\tball.\tYou\timmediately\tstart\trunning,\tanticipating\tthe\tball’s\ttrajectory.\tYou\ttrack\tit\tand adapt\tyour\tmovements,\tand\tfinally\tcatch\tit\t(under\ta\tthunder\tof\tapplause).\tPredicting\tthe\tfuture\tis\twhat\tyou do\tall\tthe\ttime,\twhether\tyou\tare\tfinishing\ta\tfriend’s\tsentence\tor\tanticipating\tthe\tsmell\tof\tcoffee\tat breakfast.\tIn\tthis\tchapter,\twe\tare\tgoing\tto\tdiscuss\trecurrent\tneural\tnetworks\t(RNN),\ta\tclass\tof\tnets\tthat can\tpredict\tthe\tfuture\t(well,\tup\tto\ta\tpoint,\tof\tcourse).\tThey\tcan\tanalyze\ttime\tseries\tdata\tsuch\tas\tstock prices,\tand\ttell\tyou\twhen\tto\tbuy\tor\tsell.\tIn\tautonomous\tdriving\tsystems,\tthey\tcan\tanticipate\tcar trajectories\tand\thelp\tavoid\taccidents.\tMore\tgenerally,\tthey\tcan\twork\ton\tsequences\tof\tarbitrary\tlengths, rather\tthan\ton\tfixed-sized\tinputs\tlike\tall\tthe\tnets\twe\thave\tdiscussed\tso\tfar.\tFor\texample,\tthey\tcan\ttake sentences,\tdocuments,\tor\taudio\tsamples\tas\tinput,\tmaking\tthem\textremely\tuseful\tfor\tnatural\tlanguage processing\t(NLP)\tsystems\tsuch\tas\tautomatic\ttranslation,\tspeech-to-text,\tor\tsentiment\tanalysis\t(e.g., reading\tmovie\treviews\tand\textracting\tthe\trater’s\tfeeling\tabout\tthe\tmovie).\n\nMoreover,\tRNNs’\tability\tto\tanticipate\talso\tmakes\tthem\tcapable\tof\tsurprising\tcreativity.\tYou\tcan\task\tthem to\tpredict\twhich\tare\tthe\tmost\tlikely\tnext\tnotes\tin\ta\tmelody,\tthen\trandomly\tpick\tone\tof\tthese\tnotes\tand\tplay it.\tThen\task\tthe\tnet\tfor\tthe\tnext\tmost\tlikely\tnotes,\tplay\tit,\tand\trepeat\tthe\tprocess\tagain\tand\tagain.\tBefore you\tknow\tit,\tyour\tnet\twill\tcompose\ta\tmelody\tsuch\tas\tthe\tone\tproduced\tby\tGoogle’s\tMagenta\tproject. Similarly,\tRNNs\tcan\tgenerate\tsentences,\timage\tcaptions,\tand\tmuch\tmore.\tThe\tresult\tis\tnot\texactly Shakespeare\tor\tMozart\tyet,\tbut\twho\tknows\twhat\tthey\twill\tproduce\ta\tfew\tyears\tfrom\tnow?\n\nIn\tthis\tchapter,\twe\twill\tlook\tat\tthe\tfundamental\tconcepts\tunderlying\tRNNs,\tthe\tmain\tproblem\tthey\tface (namely,\tvanishing/exploding\tgradients,\tdiscussed\tin\tChapter\t11),\tand\tthe\tsolutions\twidely\tused\tto\tfight it:\tLSTM\tand\tGRU\tcells.\tAlong\tthe\tway,\tas\talways,\twe\twill\tshow\thow\tto\timplement\tRNNs\tusing TensorFlow.\tFinally,\twe\twill\ttake\ta\tlook\tat\tthe\tarchitecture\tof\ta\tmachine\ttranslation\tsystem.",
      "content_length": 2147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "Recurrent\tNeurons Up\tto\tnow\twe\thave\tmostly\tlooked\tat\tfeedforward\tneural\tnetworks,\twhere\tthe\tactivations\tflow\tonly\tin\tone direction,\tfrom\tthe\tinput\tlayer\tto\tthe\toutput\tlayer\t(except\tfor\ta\tfew\tnetworks\tin\tAppendix\tE).\tA\trecurrent neural\tnetwork\tlooks\tvery\tmuch\tlike\ta\tfeedforward\tneural\tnetwork,\texcept\tit\talso\thas\tconnections pointing\tbackward.\tLet’s\tlook\tat\tthe\tsimplest\tpossible\tRNN,\tcomposed\tof\tjust\tone\tneuron\treceiving inputs,\tproducing\tan\toutput,\tand\tsending\tthat\toutput\tback\tto\titself,\tas\tshown\tin\tFigure\t14-1\t(left).\tAt\teach time\tstep\tt\t(also\tcalled\ta\tframe),\tthis\trecurrent\tneuron\treceives\tthe\tinputs\tx(t)\tas\twell\tas\tits\town\toutput from\tthe\tprevious\ttime\tstep,\ty(t–1).\tWe\tcan\trepresent\tthis\ttiny\tnetwork\tagainst\tthe\ttime\taxis,\tas\tshown\tin Figure\t14-1\t(right).\tThis\tis\tcalled\tunrolling\tthe\tnetwork\tthrough\ttime.\n\nFigure\t14-1.\tA\trecurrent\tneuron\t(left),\tunrolled\tthrough\ttime\t(right)\n\nYou\tcan\teasily\tcreate\ta\tlayer\tof\trecurrent\tneurons.\tAt\teach\ttime\tstep\tt,\tevery\tneuron\treceives\tboth\tthe input\tvector\tx(t)\tand\tthe\toutput\tvector\tfrom\tthe\tprevious\ttime\tstep\ty(t–1),\tas\tshown\tin\tFigure\t14-2.\tNote that\tboth\tthe\tinputs\tand\toutputs\tare\tvectors\tnow\t(when\tthere\twas\tjust\ta\tsingle\tneuron,\tthe\toutput\twas\ta scalar).\n\nFigure\t14-2.\tA\tlayer\tof\trecurrent\tneurons\t(left),\tunrolled\tthrough\ttime\t(right)\n\nEach\trecurrent\tneuron\thas\ttwo\tsets\tof\tweights:\tone\tfor\tthe\tinputs\tx(t)\tand\tthe\tother\tfor\tthe\toutputs\tof\tthe previous\ttime\tstep,\ty(t–1).\tLet’s\tcall\tthese\tweight\tvectors\twx\tand\twy.\tThe\toutput\tof\ta\trecurrent\tlayer\tcan\tbe",
      "content_length": 1513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "computed\tpretty\tmuch\tas\tyou\tmight\texpect,\tas\tshown\tin\tEquation\t14-1\t(b\tis\tthe\tbias\tterm\tand\tϕ(·)\tis\tthe activation\tfunction,\te.g.,\tReLU1).\n\nEquation\t14-1.\tOutput\tof\ta\trecurrent\tlayer\tfor\ta\tsingle\tinstance\n\nJust\tlike\tfor\tfeedforward\tneural\tnetworks,\twe\tcan\tcompute\ta\trecurrent\tlayer’s\toutput\tin\tone\tshot\tfor\ta whole\tmini-batch\tusing\ta\tvectorized\tform\tof\tthe\tprevious\tequation\t(see\tEquation\t14-2).\n\nEquation\t14-2.\tOutputs\tof\ta\tlayer\tof\trecurrent\tneurons\tfor\tall\tinstances\tin\ta\tmini-batch\n\nY(t)\tis\tan\tm\t×\tnneurons\tmatrix\tcontaining\tthe\tlayer’s\toutputs\tat\ttime\tstep\tt\tfor\teach\tinstance\tin\tthe\tmini- batch\t(m\tis\tthe\tnumber\tof\tinstances\tin\tthe\tmini-batch\tand\tnneurons\tis\tthe\tnumber\tof\tneurons).\n\nX(t)\tis\tan\tm\t×\tninputs\tmatrix\tcontaining\tthe\tinputs\tfor\tall\tinstances\t(ninputs\tis\tthe\tnumber\tof\tinput features).\n\nWx\tis\tan\tninputs\t×\tnneurons\tmatrix\tcontaining\tthe\tconnection\tweights\tfor\tthe\tinputs\tof\tthe\tcurrent\ttime step.\n\nWy\tis\tan\tnneurons\t×\tnneurons\tmatrix\tcontaining\tthe\tconnection\tweights\tfor\tthe\toutputs\tof\tthe\tprevious time\tstep.\n\nThe\tweight\tmatrices\tWx\tand\tWy\tare\toften\tconcatenated\tinto\ta\tsingle\tweight\tmatrix\tW\tof\tshape (ninputs\t+\tnneurons)\t×\tnneurons\t(see\tthe\tsecond\tline\tof\tEquation\t14-2).\n\nb\tis\ta\tvector\tof\tsize\tnneurons\tcontaining\teach\tneuron’s\tbias\tterm.\n\nNotice\tthat\tY(t)\tis\ta\tfunction\tof\tX(t)\tand\tY(t–1),\twhich\tis\ta\tfunction\tof\tX(t–1)\tand\tY(t–2),\twhich\tis\ta\tfunction of\tX(t–2)\tand\tY(t–3),\tand\tso\ton.\tThis\tmakes\tY(t)\ta\tfunction\tof\tall\tthe\tinputs\tsince\ttime\tt\t=\t0\t(that\tis,\tX(0), X(1),\t…,\tX(t)).\tAt\tthe\tfirst\ttime\tstep,\tt\t=\t0,\tthere\tare\tno\tprevious\toutputs,\tso\tthey\tare\ttypically\tassumed\tto be\tall\tzeros.",
      "content_length": 1610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "Memory\tCells Since\tthe\toutput\tof\ta\trecurrent\tneuron\tat\ttime\tstep\tt\tis\ta\tfunction\tof\tall\tthe\tinputs\tfrom\tprevious\ttime\tsteps, you\tcould\tsay\tit\thas\ta\tform\tof\tmemory.\tA\tpart\tof\ta\tneural\tnetwork\tthat\tpreserves\tsome\tstate\tacross\ttime steps\tis\tcalled\ta\tmemory\tcell\t(or\tsimply\ta\tcell).\tA\tsingle\trecurrent\tneuron,\tor\ta\tlayer\tof\trecurrent neurons,\tis\ta\tvery\tbasic\tcell,\tbut\tlater\tin\tthis\tchapter\twe\twill\tlook\tat\tsome\tmore\tcomplex\tand\tpowerful types\tof\tcells.\n\nIn\tgeneral\ta\tcell’s\tstate\tat\ttime\tstep\tt,\tdenoted\th(t)\t(the\t“h”\tstands\tfor\t“hidden”),\tis\ta\tfunction\tof\tsome inputs\tat\tthat\ttime\tstep\tand\tits\tstate\tat\tthe\tprevious\ttime\tstep:\th(t)\t=\tf(h(t–1),\tx(t)).\tIts\toutput\tat\ttime\tstep\tt, denoted\ty(t),\tis\talso\ta\tfunction\tof\tthe\tprevious\tstate\tand\tthe\tcurrent\tinputs.\tIn\tthe\tcase\tof\tthe\tbasic\tcells\twe have\tdiscussed\tso\tfar,\tthe\toutput\tis\tsimply\tequal\tto\tthe\tstate,\tbut\tin\tmore\tcomplex\tcells\tthis\tis\tnot\talways the\tcase,\tas\tshown\tin\tFigure\t14-3.\n\nFigure\t14-3.\tA\tcell’s\thidden\tstate\tand\tits\toutput\tmay\tbe\tdifferent",
      "content_length": 1000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "Input\tand\tOutput\tSequences An\tRNN\tcan\tsimultaneously\ttake\ta\tsequence\tof\tinputs\tand\tproduce\ta\tsequence\tof\toutputs\t(see\tFigure\t14- 4,\ttop-left\tnetwork).\tFor\texample,\tthis\ttype\tof\tnetwork\tis\tuseful\tfor\tpredicting\ttime\tseries\tsuch\tas\tstock prices:\tyou\tfeed\tit\tthe\tprices\tover\tthe\tlast\tN\tdays,\tand\tit\tmust\toutput\tthe\tprices\tshifted\tby\tone\tday\tinto\tthe future\t(i.e.,\tfrom\tN\t–\t1\tdays\tago\tto\ttomorrow).\n\nAlternatively,\tyou\tcould\tfeed\tthe\tnetwork\ta\tsequence\tof\tinputs,\tand\tignore\tall\toutputs\texcept\tfor\tthe\tlast one\t(see\tthe\ttop-right\tnetwork).\tIn\tother\twords,\tthis\tis\ta\tsequence-to-vector\tnetwork.\tFor\texample,\tyou could\tfeed\tthe\tnetwork\ta\tsequence\tof\twords\tcorresponding\tto\ta\tmovie\treview,\tand\tthe\tnetwork\twould output\ta\tsentiment\tscore\t(e.g.,\tfrom\t–1\t[hate]\tto\t+1\t[love]).\n\nConversely,\tyou\tcould\tfeed\tthe\tnetwork\ta\tsingle\tinput\tat\tthe\tfirst\ttime\tstep\t(and\tzeros\tfor\tall\tother\ttime steps),\tand\tlet\tit\toutput\ta\tsequence\t(see\tthe\tbottom-left\tnetwork).\tThis\tis\ta\tvector-to-sequence\tnetwork. For\texample,\tthe\tinput\tcould\tbe\tan\timage,\tand\tthe\toutput\tcould\tbe\ta\tcaption\tfor\tthat\timage.\n\nLastly,\tyou\tcould\thave\ta\tsequence-to-vector\tnetwork,\tcalled\tan\tencoder,\tfollowed\tby\ta\tvector-to- sequence\tnetwork,\tcalled\ta\tdecoder\t(see\tthe\tbottom-right\tnetwork).\tFor\texample,\tthis\tcan\tbe\tused\tfor translating\ta\tsentence\tfrom\tone\tlanguage\tto\tanother.\tYou\twould\tfeed\tthe\tnetwork\ta\tsentence\tin\tone language,\tthe\tencoder\twould\tconvert\tthis\tsentence\tinto\ta\tsingle\tvector\trepresentation,\tand\tthen\tthe decoder\twould\tdecode\tthis\tvector\tinto\ta\tsentence\tin\tanother\tlanguage.\tThis\ttwo-step\tmodel,\tcalled\tan Encoder–Decoder,\tworks\tmuch\tbetter\tthan\ttrying\tto\ttranslate\ton\tthe\tfly\twith\ta\tsingle\tsequence-to- sequence\tRNN\t(like\tthe\tone\trepresented\ton\tthe\ttop\tleft),\tsince\tthe\tlast\twords\tof\ta\tsentence\tcan\taffect\tthe first\twords\tof\tthe\ttranslation,\tso\tyou\tneed\tto\twait\tuntil\tyou\thave\theard\tthe\twhole\tsentence\tbefore translating\tit.\n\nFigure\t14-4.\tSeq\tto\tseq\t(top\tleft),\tseq\tto\tvector\t(top\tright),\tvector\tto\tseq\t(bottom\tleft),\tdelayed\tseq\tto\tseq\t(bottom\tright)\n\nSounds\tpromising,\tso\tlet’s\tstart\tcoding!",
      "content_length": 2059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "Basic\tRNNs\tin\tTensorFlow First,\tlet’s\timplement\ta\tvery\tsimple\tRNN\tmodel,\twithout\tusing\tany\tof\tTensorFlow’s\tRNN\toperations,\tto better\tunderstand\twhat\tgoes\ton\tunder\tthe\thood.\tWe\twill\tcreate\tan\tRNN\tcomposed\tof\ta\tlayer\tof\tfive recurrent\tneurons\t(like\tthe\tRNN\trepresented\tin\tFigure\t14-2),\tusing\tthe\ttanh\tactivation\tfunction.\tWe\twill assume\tthat\tthe\tRNN\truns\tover\tonly\ttwo\ttime\tsteps,\ttaking\tinput\tvectors\tof\tsize\t3\tat\teach\ttime\tstep.\tThe following\tcode\tbuilds\tthis\tRNN,\tunrolled\tthrough\ttwo\ttime\tsteps:\n\nn_inputs\t=\t3 n_neurons\t=\t5\n\nX0\t=\ttf.placeholder(tf.float32,\t[None,\tn_inputs]) X1\t=\ttf.placeholder(tf.float32,\t[None,\tn_inputs])\n\nWx\t=\ttf.Variable(tf.random_normal(shape=[n_inputs,\tn_neurons],dtype=tf.float32)) Wy\t=\ttf.Variable(tf.random_normal(shape=[n_neurons,n_neurons],dtype=tf.float32)) b\t=\ttf.Variable(tf.zeros([1,\tn_neurons],\tdtype=tf.float32))\n\nY0\t=\ttf.tanh(tf.matmul(X0,\tWx)\t+\tb) Y1\t=\ttf.tanh(tf.matmul(Y0,\tWy)\t+\ttf.matmul(X1,\tWx)\t+\tb)\n\ninit\t=\ttf.global_variables_initializer()\n\nThis\tnetwork\tlooks\tmuch\tlike\ta\ttwo-layer\tfeedforward\tneural\tnetwork,\twith\ta\tfew\ttwists:\tfirst,\tthe\tsame weights\tand\tbias\tterms\tare\tshared\tby\tboth\tlayers,\tand\tsecond,\twe\tfeed\tinputs\tat\teach\tlayer,\tand\twe\tget outputs\tfrom\teach\tlayer.\tTo\trun\tthe\tmodel,\twe\tneed\tto\tfeed\tit\tthe\tinputs\tat\tboth\ttime\tsteps,\tlike\tso:\n\nimport\tnumpy\tas\tnp\n\n#\tMini-batch:\t\t\t\t\t\t\t\tinstance\t0,instance\t1,instance\t2,instance\t3 X0_batch\t=\tnp.array([[0,\t1,\t2],\t[3,\t4,\t5],\t[6,\t7,\t8],\t[9,\t0,\t1]])\t#\tt\t=\t0 X1_batch\t=\tnp.array([[9,\t8,\t7],\t[0,\t0,\t0],\t[6,\t5,\t4],\t[3,\t2,\t1]])\t#\tt\t=\t1\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tY0_val,\tY1_val\t=\tsess.run([Y0,\tY1],\tfeed_dict={X0:\tX0_batch,\tX1:\tX1_batch})\n\nThis\tmini-batch\tcontains\tfour\tinstances,\teach\twith\tan\tinput\tsequence\tcomposed\tof\texactly\ttwo\tinputs.\tAt the\tend,\tY0_val\tand\tY1_val\tcontain\tthe\toutputs\tof\tthe\tnetwork\tat\tboth\ttime\tsteps\tfor\tall\tneurons\tand\tall instances\tin\tthe\tmini-batch:\n\n>>>\tprint(Y0_val)\t\t#\toutput\tat\tt\t=\t0 [[-0.0664006\t\t\t0.96257669\t\t0.68105787\t\t0.70918542\t-0.89821595]\t\t#\tinstance\t0 \t[\t0.9977755\t\t-0.71978885\t-0.99657625\t\t0.9673925\t\t-0.99989718]\t\t#\tinstance\t1 \t[\t0.99999774\t-0.99898815\t-0.99999893\t\t0.99677622\t-0.99999988]\t\t#\tinstance\t2 \t[\t1.\t\t\t\t\t\t\t\t\t-1.\t\t\t\t\t\t\t\t\t-1.\t\t\t\t\t\t\t\t\t-0.99818915\t\t0.99950868]]\t#\tinstance\t3 >>>\tprint(Y1_val)\t\t#\toutput\tat\tt\t=\t1 [[\t1.\t\t\t\t\t\t\t\t\t-1.\t\t\t\t\t\t\t\t\t-1.\t\t\t\t\t\t\t\t\t\t0.40200216\t-1.\t\t\t\t\t\t\t\t]\t\t#\tinstance\t0 \t[-0.12210433\t\t0.62805319\t\t0.96718419\t-0.99371207\t-0.25839335]\t\t#\tinstance\t1 \t[\t0.99999827\t-0.9999994\t\t-0.9999975\t\t-0.85943311\t-0.9999879\t]\t\t#\tinstance\t2 \t[\t0.99928284\t-0.99999815\t-0.99990582\t\t0.98579615\t-0.92205751]]\t#\tinstance\t3\n\nThat\twasn’t\ttoo\thard,\tbut\tof\tcourse\tif\tyou\twant\tto\tbe\table\tto\trun\tan\tRNN\tover\t100\ttime\tsteps,\tthe\tgraph\tis going\tto\tbe\tpretty\tbig.\tNow\tlet’s\tlook\tat\thow\tto\tcreate\tthe\tsame\tmodel\tusing\tTensorFlow’s\tRNN operations.",
      "content_length": 2795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "Static\tUnrolling\tThrough\tTime The\tstatic_rnn()\tfunction\tcreates\tan\tunrolled\tRNN\tnetwork\tby\tchaining\tcells.\tThe\tfollowing\tcode creates\tthe\texact\tsame\tmodel\tas\tthe\tprevious\tone:\n\nX0\t=\ttf.placeholder(tf.float32,\t[None,\tn_inputs]) X1\t=\ttf.placeholder(tf.float32,\t[None,\tn_inputs])\n\nbasic_cell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons) output_seqs,\tstates\t=\ttf.contrib.rnn.static_rnn(basic_cell,\t[X0,\tX1], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdtype=tf.float32) Y0,\tY1\t=\toutput_seqs\n\nFirst\twe\tcreate\tthe\tinput\tplaceholders,\tas\tbefore.\tThen\twe\tcreate\ta\tBasicRNNCell,\twhich\tyou\tcan\tthink of\tas\ta\tfactory\tthat\tcreates\tcopies\tof\tthe\tcell\tto\tbuild\tthe\tunrolled\tRNN\t(one\tfor\teach\ttime\tstep).\tThen\twe call\tstatic_rnn(),\tgiving\tit\tthe\tcell\tfactory\tand\tthe\tinput\ttensors,\tand\ttelling\tit\tthe\tdata\ttype\tof\tthe\tinputs (this\tis\tused\tto\tcreate\tthe\tinitial\tstate\tmatrix,\twhich\tby\tdefault\tis\tfull\tof\tzeros).\tThe\tstatic_rnn() function\tcalls\tthe\tcell\tfactory’s\t__call__()\tfunction\tonce\tper\tinput,\tcreating\ttwo\tcopies\tof\tthe\tcell\t(each containing\ta\tlayer\tof\tfive\trecurrent\tneurons),\twith\tshared\tweights\tand\tbias\tterms,\tand\tit\tchains\tthem\tjust like\twe\tdid\tearlier.\tThe\tstatic_rnn()\tfunction\treturns\ttwo\tobjects.\tThe\tfirst\tis\ta\tPython\tlist\tcontaining the\toutput\ttensors\tfor\teach\ttime\tstep.\tThe\tsecond\tis\ta\ttensor\tcontaining\tthe\tfinal\tstates\tof\tthe\tnetwork. When\tyou\tare\tusing\tbasic\tcells,\tthe\tfinal\tstate\tis\tsimply\tequal\tto\tthe\tlast\toutput.\n\nIf\tthere\twere\t50\ttime\tsteps,\tit\twould\tnot\tbe\tvery\tconvenient\tto\thave\tto\tdefine\t50\tinput\tplaceholders\tand 50\toutput\ttensors.\tMoreover,\tat\texecution\ttime\tyou\twould\thave\tto\tfeed\teach\tof\tthe\t50\tplaceholders\tand manipulate\tthe\t50\toutputs.\tLet’s\tsimplify\tthis.\tThe\tfollowing\tcode\tbuilds\tthe\tsame\tRNN\tagain,\tbut\tthis time\tit\ttakes\ta\tsingle\tinput\tplaceholder\tof\tshape\t[None,\tn_steps,\tn_inputs]\twhere\tthe\tfirst\tdimension is\tthe\tmini-batch\tsize.\tThen\tit\textracts\tthe\tlist\tof\tinput\tsequences\tfor\teach\ttime\tstep.\tX_seqs\tis\ta\tPython list\tof\tn_steps\ttensors\tof\tshape\t[None,\tn_inputs],\twhere\tonce\tagain\tthe\tfirst\tdimension\tis\tthe\tmini- batch\tsize.\tTo\tdo\tthis,\twe\tfirst\tswap\tthe\tfirst\ttwo\tdimensions\tusing\tthe\ttranspose()\tfunction,\tso\tthat\tthe time\tsteps\tare\tnow\tthe\tfirst\tdimension.\tThen\twe\textract\ta\tPython\tlist\tof\ttensors\talong\tthe\tfirst\tdimension (i.e.,\tone\ttensor\tper\ttime\tstep)\tusing\tthe\tunstack()\tfunction.\tThe\tnext\ttwo\tlines\tare\tthe\tsame\tas\tbefore. Finally,\twe\tmerge\tall\tthe\toutput\ttensors\tinto\ta\tsingle\ttensor\tusing\tthe\tstack()\tfunction,\tand\twe\tswap\tthe first\ttwo\tdimensions\tto\tget\ta\tfinal\toutputs\ttensor\tof\tshape\t[None,\tn_steps,\tn_neurons]\t(again\tthe first\tdimension\tis\tthe\tmini-batch\tsize).\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_inputs]) X_seqs\t=\ttf.unstack(tf.transpose(X,\tperm=[1,\t0,\t2])) basic_cell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons) output_seqs,\tstates\t=\ttf.contrib.rnn.static_rnn(basic_cell,\tX_seqs, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdtype=tf.float32) outputs\t=\ttf.transpose(tf.stack(output_seqs),\tperm=[1,\t0,\t2])\n\nNow\twe\tcan\trun\tthe\tnetwork\tby\tfeeding\tit\ta\tsingle\ttensor\tthat\tcontains\tall\tthe\tmini-batch\tsequences:\n\nX_batch\t=\tnp.array([ \t\t\t\t\t\t\t\t\t#\tt\t=\t0\t\t\t\t\tt\t=\t1 \t\t\t\t\t\t\t\t[[0,\t1,\t2],\t[9,\t8,\t7]],\t#\tinstance\t0 \t\t\t\t\t\t\t\t[[3,\t4,\t5],\t[0,\t0,\t0]],\t#\tinstance\t1",
      "content_length": 3231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": "[[6,\t7,\t8],\t[6,\t5,\t4]],\t#\tinstance\t2 \t\t\t\t\t\t\t\t[[9,\t0,\t1],\t[3,\t2,\t1]],\t#\tinstance\t3 \t\t\t\t])\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\toutputs_val\t=\toutputs.eval(feed_dict={X:\tX_batch})\n\nAnd\twe\tget\ta\tsingle\toutputs_val\ttensor\tfor\tall\tinstances,\tall\ttime\tsteps,\tand\tall\tneurons:\n\n>>>\tprint(outputs_val) [[[-0.91279727\t\t0.83698678\t-0.89277941\t\t0.80308062\t-0.5283336\t] \t\t[-1.\t\t\t\t\t\t\t\t\t\t1.\t\t\t\t\t\t\t\t\t-0.99794829\t\t0.99985468\t-0.99273592]]\n\n[[-0.99994391\t\t0.99951613\t-0.9946925\t\t\t0.99030769\t-0.94413054] \t\t[\t0.48733309\t\t0.93389565\t-0.31362072\t\t0.88573611\t\t0.2424476\t]]\n\n[[-1.\t\t\t\t\t\t\t\t\t\t0.99999875\t-0.99975014\t\t0.99956584\t-0.99466234] \t\t[-0.99994856\t\t0.99999434\t-0.96058172\t\t0.99784708\t-0.9099462\t]]\n\n[[-0.95972425\t\t0.99951482\t\t0.96938795\t-0.969908\t\t\t-0.67668229] \t\t[-0.84596014\t\t0.96288228\t\t0.96856463\t-0.14777924\t-0.9119423\t]]]\n\nHowever,\tthis\tapproach\tstill\tbuilds\ta\tgraph\tcontaining\tone\tcell\tper\ttime\tstep.\tIf\tthere\twere\t50\ttime\tsteps, the\tgraph\twould\tlook\tpretty\tugly.\tIt\tis\ta\tbit\tlike\twriting\ta\tprogram\twithout\tever\tusing\tloops\t(e.g.,\tY0=f(0, X0);\tY1=f(Y0,\tX1);\tY2=f(Y1,\tX2);\t...;\tY50=f(Y49,\tX50)).\tWith\tsuch\tas\tlarge\tgraph,\tyou\tmay even\tget\tout-of-memory\t(OOM)\terrors\tduring\tbackpropagation\t(especially\twith\tthe\tlimited\tmemory\tof GPU\tcards),\tsince\tit\tmust\tstore\tall\ttensor\tvalues\tduring\tthe\tforward\tpass\tso\tit\tcan\tuse\tthem\tto\tcompute gradients\tduring\tthe\treverse\tpass.\n\nFortunately,\tthere\tis\ta\tbetter\tsolution:\tthe\tdynamic_rnn()\tfunction.",
      "content_length": 1434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "Dynamic\tUnrolling\tThrough\tTime The\tdynamic_rnn()\tfunction\tuses\ta\twhile_loop()\toperation\tto\trun\tover\tthe\tcell\tthe\tappropriate\tnumber of\ttimes,\tand\tyou\tcan\tset\tswap_memory=True\tif\tyou\twant\tit\tto\tswap\tthe\tGPU’s\tmemory\tto\tthe\tCPU’s memory\tduring\tbackpropagation\tto\tavoid\tOOM\terrors.\tConveniently,\tit\talso\taccepts\ta\tsingle\ttensor\tfor\tall inputs\tat\tevery\ttime\tstep\t(shape\t[None,\tn_steps,\tn_inputs])\tand\tit\toutputs\ta\tsingle\ttensor\tfor\tall outputs\tat\tevery\ttime\tstep\t(shape\t[None,\tn_steps,\tn_neurons]);\tthere\tis\tno\tneed\tto\tstack,\tunstack,\tor transpose.\tThe\tfollowing\tcode\tcreates\tthe\tsame\tRNN\tas\tearlier\tusing\tthe\tdynamic_rnn()\tfunction.\tIt’s\tso much\tnicer!\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_inputs])\n\nbasic_cell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons) outputs,\tstates\t=\ttf.nn.dynamic_rnn(basic_cell,\tX,\tdtype=tf.float32)\n\nNOTE\n\nDuring\tbackpropagation,\tthe\twhile_loop()\toperation\tdoes\tthe\tappropriate\tmagic:\tit\tstores\tthe\ttensor\tvalues\tfor\teach\titeration during\tthe\tforward\tpass\tso\tit\tcan\tuse\tthem\tto\tcompute\tgradients\tduring\tthe\treverse\tpass.",
      "content_length": 1062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "Handling\tVariable\tLength\tInput\tSequences So\tfar\twe\thave\tused\tonly\tfixed-size\tinput\tsequences\t(all\texactly\ttwo\tsteps\tlong).\tWhat\tif\tthe\tinput sequences\thave\tvariable\tlengths\t(e.g.,\tlike\tsentences)?\tIn\tthis\tcase\tyou\tshould\tset\tthe\tsequence_length argument\twhen\tcalling\tthe\tdynamic_rnn()\t(or\tstatic_rnn())\tfunction;\tit\tmust\tbe\ta\t1D\ttensor\tindicating the\tlength\tof\tthe\tinput\tsequence\tfor\teach\tinstance.\tFor\texample:\n\nseq_length\t=\ttf.placeholder(tf.int32,\t[None])\n\n[...] outputs,\tstates\t=\ttf.nn.dynamic_rnn(basic_cell,\tX,\tdtype=tf.float32, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsequence_length=seq_length)\n\nFor\texample,\tsuppose\tthe\tsecond\tinput\tsequence\tcontains\tonly\tone\tinput\tinstead\tof\ttwo.\tIt\tmust\tbe\tpadded with\ta\tzero\tvector\tin\torder\tto\tfit\tin\tthe\tinput\ttensor\tX\t(because\tthe\tinput\ttensor’s\tsecond\tdimension\tis\tthe size\tof\tthe\tlongest\tsequence\t—\ti.e.,\t2).\n\nX_batch\t=\tnp.array([ \t\t\t\t\t\t\t\t#\tstep\t0\t\t\t\t\tstep\t1 \t\t\t\t\t\t\t\t[[0,\t1,\t2],\t[9,\t8,\t7]],\t#\tinstance\t0 \t\t\t\t\t\t\t\t[[3,\t4,\t5],\t[0,\t0,\t0]],\t#\tinstance\t1\t(padded\twith\ta\tzero\tvector) \t\t\t\t\t\t\t\t[[6,\t7,\t8],\t[6,\t5,\t4]],\t#\tinstance\t2 \t\t\t\t\t\t\t\t[[9,\t0,\t1],\t[3,\t2,\t1]],\t#\tinstance\t3 \t\t\t\t]) seq_length_batch\t=\tnp.array([2,\t1,\t2,\t2])\n\nOf\tcourse,\tyou\tnow\tneed\tto\tfeed\tvalues\tfor\tboth\tplaceholders\tX\tand\tseq_length:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\toutputs_val,\tstates_val\t=\tsess.run( \t\t\t\t\t\t\t\t[outputs,\tstates],\tfeed_dict={X:\tX_batch,\tseq_length:\tseq_length_batch})\n\nNow\tthe\tRNN\toutputs\tzero\tvectors\tfor\tevery\ttime\tstep\tpast\tthe\tinput\tsequence\tlength\t(look\tat\tthe\tsecond instance’s\toutput\tfor\tthe\tsecond\ttime\tstep):\n\n>>>\tprint(outputs_val) [[[-0.68579948\t-0.25901747\t-0.80249101\t-0.18141513\t-0.37491536] \t\t[-0.99996698\t-0.94501185\t\t0.98072106\t-0.9689762\t\t\t0.99966913]]\t\t#\tfinal\tstate\n\n[[-0.99099374\t-0.64768541\t-0.67801034\t-0.7415446\t\t\t0.7719509\t]\t\t\t#\tfinal\tstate \t\t[\t0.\t\t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t\t\t0.\t\t\t\t\t\t\t\t]]\t\t#\tzero\tvector\n\n[[-0.99978048\t-0.85583007\t-0.49696958\t-0.93838578\t\t0.98505187] \t\t[-0.99951065\t-0.89148796\t\t0.94170523\t-0.38407657\t\t0.97499216]]\t\t#\tfinal\tstate\n\n[[-0.02052618\t-0.94588047\t\t0.99935204\t\t0.37283331\t\t0.9998163\t] \t\t[-0.91052347\t\t0.05769409\t\t0.47446665\t-0.44611037\t\t0.89394671]]]\t#\tfinal\tstate\n\nMoreover,\tthe\tstates\ttensor\tcontains\tthe\tfinal\tstate\tof\teach\tcell\t(excluding\tthe\tzero\tvectors):\n\n>>>\tprint(states_val) [[-0.99996698\t-0.94501185\t\t0.98072106\t-0.9689762\t\t\t0.99966913]\t\t#\tt\t=\t1 \t[-0.99099374\t-0.64768541\t-0.67801034\t-0.7415446\t\t\t0.7719509\t]\t\t#\tt\t=\t0\t!!! \t[-0.99951065\t-0.89148796\t\t0.94170523\t-0.38407657\t\t0.97499216]\t\t#\tt\t=\t1 \t[-0.91052347\t\t0.05769409\t\t0.47446665\t-0.44611037\t\t0.89394671]]\t#\tt\t=\t1",
      "content_length": 2583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "Handling\tVariable-Length\tOutput\tSequences What\tif\tthe\toutput\tsequences\thave\tvariable\tlengths\tas\twell?\tIf\tyou\tknow\tin\tadvance\twhat\tlength\teach sequence\twill\thave\t(for\texample\tif\tyou\tknow\tthat\tit\twill\tbe\tthe\tsame\tlength\tas\tthe\tinput\tsequence),\tthen you\tcan\tset\tthe\tsequence_length\tparameter\tas\tdescribed\tabove.\tUnfortunately,\tin\tgeneral\tthis\twill\tnot be\tpossible:\tfor\texample,\tthe\tlength\tof\ta\ttranslated\tsentence\tis\tgenerally\tdifferent\tfrom\tthe\tlength\tof\tthe input\tsentence.\tIn\tthis\tcase,\tthe\tmost\tcommon\tsolution\tis\tto\tdefine\ta\tspecial\toutput\tcalled\tan\tend-of- sequence\ttoken\t(EOS\ttoken).\tAny\toutput\tpast\tthe\tEOS\tshould\tbe\tignored\t(we\twill\tdiscuss\tthis\tlater\tin this\tchapter).\n\nOkay,\tnow\tyou\tknow\thow\tto\tbuild\tan\tRNN\tnetwork\t(or\tmore\tprecisely\tan\tRNN\tnetwork\tunrolled\tthrough time).\tBut\thow\tdo\tyou\ttrain\tit?",
      "content_length": 806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "Training\tRNNs To\ttrain\tan\tRNN,\tthe\ttrick\tis\tto\tunroll\tit\tthrough\ttime\t(like\twe\tjust\tdid)\tand\tthen\tsimply\tuse\tregular backpropagation\t(see\tFigure\t14-5).\tThis\tstrategy\tis\tcalled\tbackpropagation\tthrough\ttime\t(BPTT).\n\nFigure\t14-5.\tBackpropagation\tthrough\ttime\n\nJust\tlike\tin\tregular\tbackpropagation,\tthere\tis\ta\tfirst\tforward\tpass\tthrough\tthe\tunrolled\tnetwork (represented\tby\tthe\tdashed\tarrows);\tthen\tthe\toutput\tsequence\tis\tevaluated\tusing\ta\tcost\tfunction\n\n(where\ttmin\tand\ttmax\tare\tthe\tfirst\tand\tlast\toutput\ttime\tsteps,\tnot\tcounting the\tignored\toutputs),\tand\tthe\tgradients\tof\tthat\tcost\tfunction\tare\tpropagated\tbackward\tthrough\tthe\tunrolled network\t(represented\tby\tthe\tsolid\tarrows);\tand\tfinally\tthe\tmodel\tparameters\tare\tupdated\tusing\tthe gradients\tcomputed\tduring\tBPTT.\tNote\tthat\tthe\tgradients\tflow\tbackward\tthrough\tall\tthe\toutputs\tused\tby the\tcost\tfunction,\tnot\tjust\tthrough\tthe\tfinal\toutput\t(for\texample,\tin\tFigure\t14-5\tthe\tcost\tfunction\tis computed\tusing\tthe\tlast\tthree\toutputs\tof\tthe\tnetwork,\tY(2),\tY(3),\tand\tY(4),\tso\tgradients\tflow\tthrough\tthese three\toutputs,\tbut\tnot\tthrough\tY(0)\tand\tY(1)).\tMoreover,\tsince\tthe\tsame\tparameters\tW\tand\tb\tare\tused\tat each\ttime\tstep,\tbackpropagation\twill\tdo\tthe\tright\tthing\tand\tsum\tover\tall\ttime\tsteps.",
      "content_length": 1232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "Training\ta\tSequence\tClassifier Let’s\ttrain\tan\tRNN\tto\tclassify\tMNIST\timages.\tA\tconvolutional\tneural\tnetwork\twould\tbe\tbetter\tsuited\tfor image\tclassification\t(see\tChapter\t13),\tbut\tthis\tmakes\tfor\ta\tsimple\texample\tthat\tyou\tare\talready\tfamiliar with.\tWe\twill\ttreat\teach\timage\tas\ta\tsequence\tof\t28\trows\tof\t28\tpixels\teach\t(since\teach\tMNIST\timage\tis 28\t×\t28\tpixels).\tWe\twill\tuse\tcells\tof\t150\trecurrent\tneurons,\tplus\ta\tfully\tconnected\tlayer\tcontaining\t10 neurons\t(one\tper\tclass)\tconnected\tto\tthe\toutput\tof\tthe\tlast\ttime\tstep,\tfollowed\tby\ta\tsoftmax\tlayer\t(see Figure\t14-6).\n\nFigure\t14-6.\tSequence\tclassifier\n\nThe\tconstruction\tphase\tis\tquite\tstraightforward;\tit’s\tpretty\tmuch\tthe\tsame\tas\tthe\tMNIST\tclassifier\twe built\tin\tChapter\t10\texcept\tthat\tan\tunrolled\tRNN\treplaces\tthe\thidden\tlayers.\tNote\tthat\tthe\tfully\tconnected layer\tis\tconnected\tto\tthe\tstates\ttensor,\twhich\tcontains\tonly\tthe\tfinal\tstate\tof\tthe\tRNN\t(i.e.,\tthe\t28th output).\tAlso\tnote\tthat\ty\tis\ta\tplaceholder\tfor\tthe\ttarget\tclasses.\n\nn_steps\t=\t28 n_inputs\t=\t28 n_neurons\t=\t150 n_outputs\t=\t10\n\nlearning_rate\t=\t0.001\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_inputs]) y\t=\ttf.placeholder(tf.int32,\t[None])\n\nbasic_cell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons) outputs,\tstates\t=\ttf.nn.dynamic_rnn(basic_cell,\tX,\tdtype=tf.float32)\n\nlogits\t=\ttf.layers.dense(states,\tn_outputs) xentropy\t=\ttf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogits=logits) loss\t=\ttf.reduce_mean(xentropy) optimizer\t=\ttf.train.AdamOptimizer(learning_rate=learning_rate) training_op\t=\toptimizer.minimize(loss) correct\t=\ttf.nn.in_top_k(logits,\ty,\t1) accuracy\t=\ttf.reduce_mean(tf.cast(correct,\ttf.float32))",
      "content_length": 1701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "init\t=\ttf.global_variables_initializer()\n\nNow\tlet’s\tload\tthe\tMNIST\tdata\tand\treshape\tthe\ttest\tdata\tto\t[batch_size,\tn_steps,\tn_inputs]\tas\tis expected\tby\tthe\tnetwork.\tWe\twill\ttake\tcare\tof\treshaping\tthe\ttraining\tdata\tin\ta\tmoment.\n\nfrom\ttensorflow.examples.tutorials.mnist\timport\tinput_data\n\nmnist\t=\tinput_data.read_data_sets(\"/tmp/data/\") X_test\t=\tmnist.test.images.reshape((-1,\tn_steps,\tn_inputs)) y_test\t=\tmnist.test.labels\n\nNow\twe\tare\tready\tto\ttrain\tthe\tRNN.\tThe\texecution\tphase\tis\texactly\tthe\tsame\tas\tfor\tthe\tMNIST\tclassifier in\tChapter\t10,\texcept\tthat\twe\treshape\teach\ttraining\tbatch\tbefore\tfeeding\tit\tto\tthe\tnetwork.\n\nn_epochs\t=\t100 batch_size\t=\t150\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tfor\titeration\tin\trange(mnist.train.num_examples\t//\tbatch_size): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tX_batch\t=\tX_batch.reshape((-1,\tn_steps,\tn_inputs)) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\tacc_train\t=\taccuracy.eval(feed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\tacc_test\t=\taccuracy.eval(feed_dict={X:\tX_test,\ty:\ty_test}) \t\t\t\t\t\t\t\tprint(epoch,\t\"Train\taccuracy:\",\tacc_train,\t\"Test\taccuracy:\",\tacc_test)\n\nThe\toutput\tshould\tlook\tlike\tthis:\n\n0\tTrain\taccuracy:\t0.94\tTest\taccuracy:\t0.9308 1\tTrain\taccuracy:\t0.933333\tTest\taccuracy:\t0.9431 [...] 98\tTrain\taccuracy:\t0.98\tTest\taccuracy:\t0.9794 99\tTrain\taccuracy:\t1.0\tTest\taccuracy:\t0.9804\n\nWe\tget\tover\t98%\taccuracy\t—\tnot\tbad!\tPlus\tyou\twould\tcertainly\tget\ta\tbetter\tresult\tby\ttuning\tthe hyperparameters,\tinitializing\tthe\tRNN\tweights\tusing\tHe\tinitialization,\ttraining\tlonger,\tor\tadding\ta\tbit\tof regularization\t(e.g.,\tdropout).\n\nTIP\n\nYou\tcan\tspecify\tan\tinitializer\tfor\tthe\tRNN\tby\twrapping\tits\tconstruction\tcode\tin\ta\tvariable\tscope\t(e.g.,\tuse variable_scope(\"rnn\",\tinitializer=variance_scaling_initializer())\tto\tuse\tHe\tinitialization).",
      "content_length": 1883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "Training\tto\tPredict\tTime\tSeries Now\tlet’s\ttake\ta\tlook\tat\thow\tto\thandle\ttime\tseries,\tsuch\tas\tstock\tprices,\tair\ttemperature,\tbrain\twave patterns,\tand\tso\ton.\tIn\tthis\tsection\twe\twill\ttrain\tan\tRNN\tto\tpredict\tthe\tnext\tvalue\tin\ta\tgenerated\ttime series.\tEach\ttraining\tinstance\tis\ta\trandomly\tselected\tsequence\tof\t20\tconsecutive\tvalues\tfrom\tthe\ttime series,\tand\tthe\ttarget\tsequence\tis\tthe\tsame\tas\tthe\tinput\tsequence,\texcept\tit\tis\tshifted\tby\tone\ttime\tstep\tinto the\tfuture\t(see\tFigure\t14-7).\n\nFigure\t14-7.\tTime\tseries\t(left),\tand\ta\ttraining\tinstance\tfrom\tthat\tseries\t(right)\n\nFirst,\tlet’s\tcreate\tthe\tRNN.\tIt\twill\tcontain\t100\trecurrent\tneurons\tand\twe\twill\tunroll\tit\tover\t20\ttime\tsteps since\teach\ttraining\tinstance\twill\tbe\t20\tinputs\tlong.\tEach\tinput\twill\tcontain\tonly\tone\tfeature\t(the\tvalue\tat that\ttime).\tThe\ttargets\tare\talso\tsequences\tof\t20\tinputs,\teach\tcontaining\ta\tsingle\tvalue.\tThe\tcode\tis\talmost the\tsame\tas\tearlier:\n\nn_steps\t=\t20 n_inputs\t=\t1 n_neurons\t=\t100 n_outputs\t=\t1\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_inputs]) y\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_outputs]) cell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\tactivation=tf.nn.relu) outputs,\tstates\t=\ttf.nn.dynamic_rnn(cell,\tX,\tdtype=tf.float32)\n\nNOTE\n\nIn\tgeneral\tyou\twould\thave\tmore\tthan\tjust\tone\tinput\tfeature.\tFor\texample,\tif\tyou\twere\ttrying\tto\tpredict\tstock\tprices,\tyou\twould likely\thave\tmany\tother\tinput\tfeatures\tat\teach\ttime\tstep,\tsuch\tas\tprices\tof\tcompeting\tstocks,\tratings\tfrom\tanalysts,\tor\tany\tother feature\tthat\tmight\thelp\tthe\tsystem\tmake\tits\tpredictions.\n\nAt\teach\ttime\tstep\twe\tnow\thave\tan\toutput\tvector\tof\tsize\t100.\tBut\twhat\twe\tactually\twant\tis\ta\tsingle\toutput value\tat\teach\ttime\tstep.\tThe\tsimplest\tsolution\tis\tto\twrap\tthe\tcell\tin\tan\tOutputProjectionWrapper.\tA cell\twrapper\tacts\tlike\ta\tnormal\tcell,\tproxying\tevery\tmethod\tcall\tto\tan\tunderlying\tcell,\tbut\tit\talso\tadds some\tfunctionality.\tThe\tOutputProjectionWrapper\tadds\ta\tfully\tconnected\tlayer\tof\tlinear\tneurons\t(i.e., without\tany\tactivation\tfunction)\ton\ttop\tof\teach\toutput\t(but\tit\tdoes\tnot\taffect\tthe\tcell\tstate).\tAll\tthese\tfully connected\tlayers\tshare\tthe\tsame\t(trainable)\tweights\tand\tbias\tterms.\tThe\tresulting\tRNN\tis\trepresented\tin",
      "content_length": 2174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "Figure\t14-8.\n\nFigure\t14-8.\tRNN\tcells\tusing\toutput\tprojections\n\nWrapping\ta\tcell\tis\tquite\teasy.\tLet’s\ttweak\tthe\tpreceding\tcode\tby\twrapping\tthe\tBasicRNNCell\tinto\tan OutputProjectionWrapper:\n\ncell\t=\ttf.contrib.rnn.OutputProjectionWrapper( \t\t\t\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\tactivation=tf.nn.relu), \t\t\t\toutput_size=n_outputs)\n\nSo\tfar,\tso\tgood.\tNow\twe\tneed\tto\tdefine\tthe\tcost\tfunction.\tWe\twill\tuse\tthe\tMean\tSquared\tError\t(MSE),\tas we\tdid\tin\tprevious\tregression\ttasks.\tNext\twe\twill\tcreate\tan\tAdam\toptimizer,\tthe\ttraining\top,\tand\tthe variable\tinitialization\top,\tas\tusual:\n\nlearning_rate\t=\t0.001\n\nloss\t=\ttf.reduce_mean(tf.square(outputs\t-\ty)) optimizer\t=\ttf.train.AdamOptimizer(learning_rate=learning_rate) training_op\t=\toptimizer.minimize(loss)\n\ninit\t=\ttf.global_variables_initializer()\n\nNow\ton\tto\tthe\texecution\tphase:\n\nn_iterations\t=\t1500 batch_size\t=\t50\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\titeration\tin\trange(n_iterations): \t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\t[...]\t\t#\tfetch\tthe\tnext\ttraining\tbatch \t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\tif\titeration\t%\t100\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tmse\t=\tloss.eval(feed_dict={X:\tX_batch,\ty:\ty_batch}) \t\t\t\t\t\t\t\t\t\t\t\tprint(iteration,\t\"\\tMSE:\",\tmse)",
      "content_length": 1223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "The\tprogram’s\toutput\tshould\tlook\tlike\tthis:\n\n0\t\t\t\t\t\t\tMSE:\t13.6543 100\t\t\t\t\tMSE:\t0.538476 200\t\t\t\t\tMSE:\t0.168532 300\t\t\t\t\tMSE:\t0.0879579 400\t\t\t\t\tMSE:\t0.0633425 [...]\n\nOnce\tthe\tmodel\tis\ttrained,\tyou\tcan\tmake\tpredictions:\n\nX_new\t=\t[...]\t\t#\tNew\tsequences y_pred\t=\tsess.run(outputs,\tfeed_dict={X:\tX_new})\n\nFigure\t14-9\tshows\tthe\tpredicted\tsequence\tfor\tthe\tinstance\twe\tlooked\tat\tearlier\t(in\tFigure\t14-7),\tafter\tjust 1,000\ttraining\titerations.\n\nFigure\t14-9.\tTime\tseries\tprediction\n\nAlthough\tusing\tan\tOutputProjectionWrapper\tis\tthe\tsimplest\tsolution\tto\treduce\tthe\tdimensionality\tof the\tRNN’s\toutput\tsequences\tdown\tto\tjust\tone\tvalue\tper\ttime\tstep\t(per\tinstance),\tit\tis\tnot\tthe\tmost efficient.\tThere\tis\ta\ttrickier\tbut\tmore\tefficient\tsolution:\tyou\tcan\treshape\tthe\tRNN\toutputs\tfrom [batch_size,\tn_steps,\tn_neurons]\tto\t[batch_size\t*\tn_steps,\tn_neurons],\tthen\tapply\ta\tsingle fully\tconnected\tlayer\twith\tthe\tappropriate\toutput\tsize\t(in\tour\tcase\tjust\t1),\twhich\twill\tresult\tin\tan\toutput tensor\tof\tshape\t[batch_size\t*\tn_steps,\tn_outputs],\tand\tthen\treshape\tthis\ttensor\tto\t[batch_size, n_steps,\tn_outputs].\tThese\toperations\tare\trepresented\tin\tFigure\t14-10.",
      "content_length": 1131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "Figure\t14-10.\tStack\tall\tthe\toutputs,\tapply\tthe\tprojection,\tthen\tunstack\tthe\tresult\n\nTo\timplement\tthis\tsolution,\twe\tfirst\trevert\tto\ta\tbasic\tcell,\twithout\tthe\tOutputProjectionWrapper:\n\ncell\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\tactivation=tf.nn.relu) rnn_outputs,\tstates\t=\ttf.nn.dynamic_rnn(cell,\tX,\tdtype=tf.float32)\n\nThen\twe\tstack\tall\tthe\toutputs\tusing\tthe\treshape()\toperation,\tapply\tthe\tfully\tconnected\tlinear\tlayer (without\tusing\tany\tactivation\tfunction;\tthis\tis\tjust\ta\tprojection),\tand\tfinally\tunstack\tall\tthe\toutputs,\tagain using\treshape():\n\nstacked_rnn_outputs\t=\ttf.reshape(rnn_outputs,\t[-1,\tn_neurons]) stacked_outputs\t=\ttf.layers.dense(stacked_rnn_outputs,\tn_outputs) outputs\t=\ttf.reshape(stacked_outputs,\t[-1,\tn_steps,\tn_outputs])\n\nThe\trest\tof\tthe\tcode\tis\tthe\tsame\tas\tearlier.\tThis\tcan\tprovide\ta\tsignificant\tspeed\tboost\tsince\tthere\tis\tjust one\tfully\tconnected\tlayer\tinstead\tof\tone\tper\ttime\tstep.",
      "content_length": 916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "Creative\tRNN Now\tthat\twe\thave\ta\tmodel\tthat\tcan\tpredict\tthe\tfuture,\twe\tcan\tuse\tit\tto\tgenerate\tsome\tcreative\tsequences, as\texplained\tat\tthe\tbeginning\tof\tthe\tchapter.\tAll\twe\tneed\tis\tto\tprovide\tit\ta\tseed\tsequence\tcontaining n_steps\tvalues\t(e.g.,\tfull\tof\tzeros),\tuse\tthe\tmodel\tto\tpredict\tthe\tnext\tvalue,\tappend\tthis\tpredicted\tvalue to\tthe\tsequence,\tfeed\tthe\tlast\tn_steps\tvalues\tto\tthe\tmodel\tto\tpredict\tthe\tnext\tvalue,\tand\tso\ton.\tThis process\tgenerates\ta\tnew\tsequence\tthat\thas\tsome\tresemblance\tto\tthe\toriginal\ttime\tseries\t(see\tFigure\t14- 11).\n\nsequence\t=\t[0.]\t*\tn_steps for\titeration\tin\trange(300): \t\t\t\tX_batch\t=\tnp.array(sequence[-n_steps:]).reshape(1,\tn_steps,\t1) \t\t\t\ty_pred\t=\tsess.run(outputs,\tfeed_dict={X:\tX_batch}) \t\t\t\tsequence.append(y_pred[0,\t-1,\t0])\n\nFigure\t14-11.\tCreative\tsequences,\tseeded\twith\tzeros\t(left)\tor\twith\tan\tinstance\t(right)\n\nNow\tyou\tcan\ttry\tto\tfeed\tall\tyour\tJohn\tLennon\talbums\tto\tan\tRNN\tand\tsee\tif\tit\tcan\tgenerate\tthe\tnext “Imagine.”\tHowever,\tyou\twill\tprobably\tneed\ta\tmuch\tmore\tpowerful\tRNN,\twith\tmore\tneurons,\tand\talso much\tdeeper.\tLet’s\tlook\tat\tdeep\tRNNs\tnow.",
      "content_length": 1078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "Deep\tRNNs It\tis\tquite\tcommon\tto\tstack\tmultiple\tlayers\tof\tcells,\tas\tshown\tin\tFigure\t14-12.\tThis\tgives\tyou\ta\tdeep RNN.\n\nTo\timplement\ta\tdeep\tRNN\tin\tTensorFlow,\tyou\tcan\tcreate\tseveral\tcells\tand\tstack\tthem\tinto\ta MultiRNNCell.\tIn\tthe\tfollowing\tcode\twe\tstack\tthree\tidentical\tcells\t(but\tyou\tcould\tvery\twell\tuse\tvarious kinds\tof\tcells\twith\ta\tdifferent\tnumber\tof\tneurons):\n\nn_neurons\t=\t100 n_layers\t=\t3\n\nlayers\t=\t[tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.relu) \t\t\t\t\t\t\t\t\t\tfor\tlayer\tin\trange(n_layers)] multi_layer_cell\t=\ttf.contrib.rnn.MultiRNNCell(layers) outputs,\tstates\t=\ttf.nn.dynamic_rnn(multi_layer_cell,\tX,\tdtype=tf.float32)\n\nFigure\t14-12.\tDeep\tRNN\t(left),\tunrolled\tthrough\ttime\t(right)\n\nThat’s\tall\tthere\tis\tto\tit!\tThe\tstates\tvariable\tis\ta\ttuple\tcontaining\tone\ttensor\tper\tlayer,\teach\trepresenting the\tfinal\tstate\tof\tthat\tlayer’s\tcell\t(with\tshape\t[batch_size,\tn_neurons]).\tIf\tyou\tset state_is_tuple=False\twhen\tcreating\tthe\tMultiRNNCell,\tthen\tstates\tbecomes\ta\tsingle\ttensor containing\tthe\tstates\tfrom\tevery\tlayer,\tconcatenated\talong\tthe\tcolumn\taxis\t(i.e.,\tits\tshape\tis [batch_size,\tn_layers\t*\tn_neurons]).\tNote\tthat\tbefore\tTensorFlow\t0.11.0,\tthis\tbehavior\twas\tthe default.",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 489,
      "content": "Distributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs Chapter\t12\tpointed\tout\tthat\twe\tcan\tefficiently\tdistribute\tdeep\tRNNs\tacross\tmultiple\tGPUs\tby\tpinning each\tlayer\tto\ta\tdifferent\tGPU\t(see\tFigure\t12-16).\tHowever,\tif\tyou\ttry\tto\tcreate\teach\tcell\tin\ta\tdifferent device()\tblock,\tit\twill\tnot\twork:\n\nwith\ttf.device(\"/gpu:0\"):\t\t#\tBAD!\tThis\tis\tignored. \t\t\t\tlayer1\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n\nwith\ttf.device(\"/gpu:1\"):\t\t#\tBAD!\tIgnored\tagain. \t\t\t\tlayer2\t=\ttf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n\nThis\tfails\tbecause\ta\tBasicRNNCell\tis\ta\tcell\tfactory,\tnot\ta\tcell\tper\tse\t(as\tmentioned\tearlier);\tno\tcells\tget created\twhen\tyou\tcreate\tthe\tfactory,\tand\tthus\tno\tvariables\tdo\teither.\tThe\tdevice\tblock\tis\tsimply\tignored. The\tcells\tactually\tget\tcreated\tlater.\tWhen\tyou\tcall\tdynamic_rnn(),\tit\tcalls\tthe\tMultiRNNCell,\twhich calls\teach\tindividual\tBasicRNNCell,\twhich\tcreate\tthe\tactual\tcells\t(including\ttheir\tvariables). Unfortunately,\tnone\tof\tthese\tclasses\tprovide\tany\tway\tto\tcontrol\tthe\tdevices\ton\twhich\tthe\tvariables\tget created.\tIf\tyou\ttry\tto\tput\tthe\tdynamic_rnn()\tcall\twithin\ta\tdevice\tblock,\tthe\twhole\tRNN\tgets\tpinned\tto\ta single\tdevice.\tSo\tare\tyou\tstuck?\tFortunately\tnot!\tThe\ttrick\tis\tto\tcreate\tyour\town\tcell\twrapper:\n\nimport\ttensorflow\tas\ttf\n\nclass\tDeviceCellWrapper(tf.contrib.rnn.RNNCell): \t\tdef\t__init__(self,\tdevice,\tcell): \t\t\t\tself._cell\t=\tcell \t\t\t\tself._device\t=\tdevice\n\n@property \t\tdef\tstate_size(self): \t\t\t\treturn\tself._cell.state_size\n\n@property \t\tdef\toutput_size(self): \t\t\t\treturn\tself._cell.output_size\n\ndef\t__call__(self,\tinputs,\tstate,\tscope=None): \t\t\t\twith\ttf.device(self._device): \t\t\t\t\t\t\t\treturn\tself._cell(inputs,\tstate,\tscope)\n\nThis\twrapper\tsimply\tproxies\tevery\tmethod\tcall\tto\tanother\tcell,\texcept\tit\twraps\tthe\t__call__()\tfunction within\ta\tdevice\tblock.2\tNow\tyou\tcan\tdistribute\teach\tlayer\ton\ta\tdifferent\tGPU:\n\ndevices\t=\t[\"/gpu:0\",\t\"/gpu:1\",\t\"/gpu:2\"] cells\t=\t[DeviceCellWrapper(dev,tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)) \t\t\t\t\t\t\t\t\tfor\tdev\tin\tdevices] multi_layer_cell\t=\ttf.contrib.rnn.MultiRNNCell(cells) outputs,\tstates\t=\ttf.nn.dynamic_rnn(multi_layer_cell,\tX,\tdtype=tf.float32)\n\nWARNING\n\nDo\tnot\tset\tstate_is_tuple=False,\tor\tthe\tMultiRNNCell\twill\tconcatenate\tall\tthe\tcell\tstates\tinto\ta\tsingle\ttensor,\ton\ta\tsingle\tGPU.",
      "content_length": 2251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "Applying\tDropout If\tyou\tbuild\ta\tvery\tdeep\tRNN,\tit\tmay\tend\tup\toverfitting\tthe\ttraining\tset.\tTo\tprevent\tthat,\ta\tcommon technique\tis\tto\tapply\tdropout\t(introduced\tin\tChapter\t11).\tYou\tcan\tsimply\tadd\ta\tdropout\tlayer\tbefore\tor after\tthe\tRNN\tas\tusual,\tbut\tif\tyou\talso\twant\tto\tapply\tdropout\tbetween\tthe\tRNN\tlayers,\tyou\tneed\tto\tuse\ta DropoutWrapper.\tThe\tfollowing\tcode\tapplies\tdropout\tto\tthe\tinputs\tof\teach\tlayer\tin\tthe\tRNN,\tdropping each\tinput\twith\ta\t50%\tprobability:\n\nkeep_prob\t=\t0.5\n\ncells\t=\t[tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) \t\t\t\t\t\t\t\t\tfor\tlayer\tin\trange(n_layers)] cells_drop\t=\t[tf.contrib.rnn.DropoutWrapper(cell,\tinput_keep_prob=keep_prob) \t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tcell\tin\tcells] multi_layer_cell\t=\ttf.contrib.rnn.MultiRNNCell(cells_drop) rnn_outputs,\tstates\t=\ttf.nn.dynamic_rnn(multi_layer_cell,\tX,\tdtype=tf.float32)\n\nNote\tthat\tit\tis\talso\tpossible\tto\tapply\tdropout\tto\tthe\toutputs\tby\tsetting\toutput_keep_prob.\n\nThe\tmain\tproblem\twith\tthis\tcode\tis\tthat\tit\twill\tapply\tdropout\tnot\tonly\tduring\ttraining\tbut\talso\tduring testing,\twhich\tis\tnot\twhat\tyou\twant\t(recall\tthat\tdropout\tshould\tbe\tapplied\tonly\tduring\ttraining). Unfortunately,\tthe\tDropoutWrapper\tdoes\tnot\tsupport\ta\ttraining\tplaceholder\t(yet?),\tso\tyou\tmust\teither write\tyour\town\tdropout\twrapper\tclass,\tor\thave\ttwo\tdifferent\tgraphs:\tone\tfor\ttraining,\tand\tthe\tother\tfor testing.\tThe\tsecond\toption\tlooks\tlike\tthis:\n\nimport\tsys training\t=\t(sys.argv[-1]\t==\t\"train\")\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_inputs]) y\t=\ttf.placeholder(tf.float32,\t[None,\tn_steps,\tn_outputs]) cells\t=\t[tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) \t\t\t\t\t\t\t\t\tfor\tlayer\tin\trange(n_layers)] if\ttraining: \t\t\t\tcells\t=\t[tf.contrib.rnn.DropoutWrapper(cell,\tinput_keep_prob=keep_prob) \t\t\t\t\t\t\t\t\t\t\t\t\tfor\tcell\tin\tcells] multi_layer_cell\t=\ttf.contrib.rnn.MultiRNNCell(cells) rnn_outputs,\tstates\t=\ttf.nn.dynamic_rnn(multi_layer_cell,\tX,\tdtype=tf.float32) [...]\t#\tbuild\tthe\trest\tof\tthe\tgraph\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tif\ttraining: \t\t\t\t\t\t\t\tinit.run() \t\t\t\t\t\t\t\tfor\titeration\tin\trange(n_iterations): \t\t\t\t\t\t\t\t\t\t\t\t[...]\t#\ttrain\tthe\tmodel \t\t\t\t\t\t\t\tsave_path\t=\tsaver.save(sess,\t\"/tmp/my_model.ckpt\") \t\t\t\telse: \t\t\t\t\t\t\t\tsaver.restore(sess,\t\"/tmp/my_model.ckpt\") \t\t\t\t\t\t\t\t[...]\t#\tuse\tthe\tmodel\n\nWith\tthat\tyou\tshould\tbe\table\tto\ttrain\tall\tsorts\tof\tRNNs!\tUnfortunately,\tif\tyou\twant\tto\ttrain\tan\tRNN\ton\tlong sequences,\tthings\twill\tget\ta\tbit\tharder.\tLet’s\tsee\twhy\tand\twhat\tyou\tcan\tdo\tabout\tit.",
      "content_length": 2407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "The\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps To\ttrain\tan\tRNN\ton\tlong\tsequences,\tyou\twill\tneed\tto\trun\tit\tover\tmany\ttime\tsteps,\tmaking\tthe\tunrolled RNN\ta\tvery\tdeep\tnetwork.\tJust\tlike\tany\tdeep\tneural\tnetwork\tit\tmay\tsuffer\tfrom\tthe\tvanishing/exploding gradients\tproblem\t(discussed\tin\tChapter\t11)\tand\ttake\tforever\tto\ttrain.\tMany\tof\tthe\ttricks\twe\tdiscussed\tto alleviate\tthis\tproblem\tcan\tbe\tused\tfor\tdeep\tunrolled\tRNNs\tas\twell:\tgood\tparameter\tinitialization, nonsaturating\tactivation\tfunctions\t(e.g.,\tReLU),\tBatch\tNormalization,\tGradient\tClipping,\tand\tfaster optimizers.\tHowever,\tif\tthe\tRNN\tneeds\tto\thandle\teven\tmoderately\tlong\tsequences\t(e.g.,\t100\tinputs),\tthen training\twill\tstill\tbe\tvery\tslow.\n\nThe\tsimplest\tand\tmost\tcommon\tsolution\tto\tthis\tproblem\tis\tto\tunroll\tthe\tRNN\tonly\tover\ta\tlimited\tnumber of\ttime\tsteps\tduring\ttraining.\tThis\tis\tcalled\ttruncated\tbackpropagation\tthrough\ttime.\tIn\tTensorFlow\tyou can\timplement\tit\tsimply\tby\ttruncating\tthe\tinput\tsequences.\tFor\texample,\tin\tthe\ttime\tseries\tprediction problem,\tyou\twould\tsimply\treduce\tn_steps\tduring\ttraining.\tThe\tproblem,\tof\tcourse,\tis\tthat\tthe\tmodel will\tnot\tbe\table\tto\tlearn\tlong-term\tpatterns.\tOne\tworkaround\tcould\tbe\tto\tmake\tsure\tthat\tthese\tshortened sequences\tcontain\tboth\told\tand\trecent\tdata,\tso\tthat\tthe\tmodel\tcan\tlearn\tto\tuse\tboth\t(e.g.,\tthe\tsequence could\tcontain\tmonthly\tdata\tfor\tthe\tlast\tfive\tmonths,\tthen\tweekly\tdata\tfor\tthe\tlast\tfive\tweeks,\tthen\tdaily data\tover\tthe\tlast\tfive\tdays).\tBut\tthis\tworkaround\thas\tits\tlimits:\twhat\tif\tfine-grained\tdata\tfrom\tlast\tyear\tis actually\tuseful?\tWhat\tif\tthere\twas\ta\tbrief\tbut\tsignificant\tevent\tthat\tabsolutely\tmust\tbe\ttaken\tinto\taccount, even\tyears\tlater\t(e.g.,\tthe\tresult\tof\tan\telection)?\n\nBesides\tthe\tlong\ttraining\ttime,\ta\tsecond\tproblem\tfaced\tby\tlong-running\tRNNs\tis\tthe\tfact\tthat\tthe\tmemory of\tthe\tfirst\tinputs\tgradually\tfades\taway.\tIndeed,\tdue\tto\tthe\ttransformations\tthat\tthe\tdata\tgoes\tthrough\twhen traversing\tan\tRNN,\tsome\tinformation\tis\tlost\tafter\teach\ttime\tstep.\tAfter\ta\twhile,\tthe\tRNN’s\tstate\tcontains virtually\tno\ttrace\tof\tthe\tfirst\tinputs.\tThis\tcan\tbe\ta\tshowstopper.\tFor\texample,\tsay\tyou\twant\tto\tperform sentiment\tanalysis\ton\ta\tlong\treview\tthat\tstarts\twith\tthe\tfour\twords\t“I\tloved\tthis\tmovie,”\tbut\tthe\trest\tof\tthe review\tlists\tthe\tmany\tthings\tthat\tcould\thave\tmade\tthe\tmovie\teven\tbetter.\tIf\tthe\tRNN\tgradually\tforgets\tthe first\tfour\twords,\tit\twill\tcompletely\tmisinterpret\tthe\treview.\tTo\tsolve\tthis\tproblem,\tvarious\ttypes\tof\tcells with\tlong-term\tmemory\thave\tbeen\tintroduced.\tThey\thave\tproved\tso\tsuccessful\tthat\tthe\tbasic\tcells\tare\tnot much\tused\tanymore.\tLet’s\tfirst\tlook\tat\tthe\tmost\tpopular\tof\tthese\tlong\tmemory\tcells:\tthe\tLSTM\tcell.",
      "content_length": 2638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "LSTM\tCell The\tLong\tShort-Term\tMemory\t(LSTM)\tcell\twas\tproposed\tin\t19973\tby\tSepp\tHochreiter\tand\tJürgen Schmidhuber,\tand\tit\twas\tgradually\timproved\tover\tthe\tyears\tby\tseveral\tresearchers,\tsuch\tas\tAlex\tGraves, Haşim\tSak,4\tWojciech\tZaremba,5\tand\tmany\tmore.\tIf\tyou\tconsider\tthe\tLSTM\tcell\tas\ta\tblack\tbox,\tit\tcan\tbe used\tvery\tmuch\tlike\ta\tbasic\tcell,\texcept\tit\twill\tperform\tmuch\tbetter;\ttraining\twill\tconverge\tfaster\tand\tit will\tdetect\tlong-term\tdependencies\tin\tthe\tdata.\tIn\tTensorFlow,\tyou\tcan\tsimply\tuse\ta\tBasicLSTMCell instead\tof\ta\tBasicRNNCell:\n\nlstm_cell\t=\ttf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)\n\nLSTM\tcells\tmanage\ttwo\tstate\tvectors,\tand\tfor\tperformance\treasons\tthey\tare\tkept\tseparate\tby\tdefault.\tYou can\tchange\tthis\tdefault\tbehavior\tby\tsetting\tstate_is_tuple=False\twhen\tcreating\tthe\tBasicLSTMCell.\n\nSo\thow\tdoes\tan\tLSTM\tcell\twork?\tThe\tarchitecture\tof\ta\tbasic\tLSTM\tcell\tis\tshown\tin\tFigure\t14-13.\n\nFigure\t14-13.\tLSTM\tcell\n\nIf\tyou\tdon’t\tlook\tat\twhat’s\tinside\tthe\tbox,\tthe\tLSTM\tcell\tlooks\texactly\tlike\ta\tregular\tcell,\texcept\tthat\tits state\tis\tsplit\tin\ttwo\tvectors:\th(t)\tand\tc(t)\t(“c”\tstands\tfor\t“cell”).\tYou\tcan\tthink\tof\th(t)\tas\tthe\tshort-term\tstate and\tc(t)\tas\tthe\tlong-term\tstate.\n\nNow\tlet’s\topen\tthe\tbox!\tThe\tkey\tidea\tis\tthat\tthe\tnetwork\tcan\tlearn\twhat\tto\tstore\tin\tthe\tlong-term\tstate, what\tto\tthrow\taway,\tand\twhat\tto\tread\tfrom\tit.\tAs\tthe\tlong-term\tstate\tc(t–1)\ttraverses\tthe\tnetwork\tfrom\tleft to\tright,\tyou\tcan\tsee\tthat\tit\tfirst\tgoes\tthrough\ta\tforget\tgate,\tdropping\tsome\tmemories,\tand\tthen\tit\tadds some\tnew\tmemories\tvia\tthe\taddition\toperation\t(which\tadds\tthe\tmemories\tthat\twere\tselected\tby\tan\tinput gate).\tThe\tresult\tc(t)\tis\tsent\tstraight\tout,\twithout\tany\tfurther\ttransformation.\tSo,\tat\teach\ttime\tstep,\tsome memories\tare\tdropped\tand\tsome\tmemories\tare\tadded.\tMoreover,\tafter\tthe\taddition\toperation,\tthe\tlong- term\tstate\tis\tcopied\tand\tpassed\tthrough\tthe\ttanh\tfunction,\tand\tthen\tthe\tresult\tis\tfiltered\tby\tthe\toutput\tgate.",
      "content_length": 1923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "This\tproduces\tthe\tshort-term\tstate\th(t)\t(which\tis\tequal\tto\tthe\tcell’s\toutput\tfor\tthis\ttime\tstep\ty(t)).\tNow\tlet’s look\tat\twhere\tnew\tmemories\tcome\tfrom\tand\thow\tthe\tgates\twork.\n\nFirst,\tthe\tcurrent\tinput\tvector\tx(t)\tand\tthe\tprevious\tshort-term\tstate\th(t–1)\tare\tfed\tto\tfour\tdifferent\tfully connected\tlayers.\tThey\tall\tserve\ta\tdifferent\tpurpose:\n\nThe\tmain\tlayer\tis\tthe\tone\tthat\toutputs\tg(t).\tIt\thas\tthe\tusual\trole\tof\tanalyzing\tthe\tcurrent\tinputs\tx(t)\tand the\tprevious\t(short-term)\tstate\th(t–1).\tIn\ta\tbasic\tcell,\tthere\tis\tnothing\telse\tthan\tthis\tlayer,\tand\tits output\tgoes\tstraight\tout\tto\ty(t)\tand\th(t).\tIn\tcontrast,\tin\tan\tLSTM\tcell\tthis\tlayer’s\toutput\tdoes\tnot\tgo straight\tout,\tbut\tinstead\tit\tis\tpartially\tstored\tin\tthe\tlong-term\tstate.\n\nThe\tthree\tother\tlayers\tare\tgate\tcontrollers.\tSince\tthey\tuse\tthe\tlogistic\tactivation\tfunction,\ttheir outputs\trange\tfrom\t0\tto\t1.\tAs\tyou\tcan\tsee,\ttheir\toutputs\tare\tfed\tto\telement-wise\tmultiplication operations,\tso\tif\tthey\toutput\t0s,\tthey\tclose\tthe\tgate,\tand\tif\tthey\toutput\t1s,\tthey\topen\tit.\tSpecifically:\n\nThe\tforget\tgate\t(controlled\tby\tf(t))\tcontrols\twhich\tparts\tof\tthe\tlong-term\tstate\tshould\tbe\terased.\n\nThe\tinput\tgate\t(controlled\tby\ti(t))\tcontrols\twhich\tparts\tof\tg(t)\tshould\tbe\tadded\tto\tthe\tlong-term state\t(this\tis\twhy\twe\tsaid\tit\twas\tonly\t“partially\tstored”).\n\nFinally,\tthe\toutput\tgate\t(controlled\tby\to(t))\tcontrols\twhich\tparts\tof\tthe\tlong-term\tstate\tshould\tbe read\tand\toutput\tat\tthis\ttime\tstep\t(both\tto\th(t))\tand\ty(t).\n\nIn\tshort,\tan\tLSTM\tcell\tcan\tlearn\tto\trecognize\tan\timportant\tinput\t(that’s\tthe\trole\tof\tthe\tinput\tgate),\tstore\tit in\tthe\tlong-term\tstate,\tlearn\tto\tpreserve\tit\tfor\tas\tlong\tas\tit\tis\tneeded\t(that’s\tthe\trole\tof\tthe\tforget\tgate),\tand learn\tto\textract\tit\twhenever\tit\tis\tneeded.\tThis\texplains\twhy\tthey\thave\tbeen\tamazingly\tsuccessful\tat capturing\tlong-term\tpatterns\tin\ttime\tseries,\tlong\ttexts,\taudio\trecordings,\tand\tmore.\n\nEquation\t14-3\tsummarizes\thow\tto\tcompute\tthe\tcell’s\tlong-term\tstate,\tits\tshort-term\tstate,\tand\tits\toutput\tat each\ttime\tstep\tfor\ta\tsingle\tinstance\t(the\tequations\tfor\ta\twhole\tmini-batch\tare\tvery\tsimilar).\n\nEquation\t14-3.\tLSTM\tcomputations\n\nWxi,\tWxf,\tWxo,\tWxg\tare\tthe\tweight\tmatrices\tof\teach\tof\tthe\tfour\tlayers\tfor\ttheir\tconnection\tto\tthe",
      "content_length": 2198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "input\tvector\tx(t).\n\nWhi,\tWhf,\tWho,\tand\tWhg\tare\tthe\tweight\tmatrices\tof\teach\tof\tthe\tfour\tlayers\tfor\ttheir\tconnection\tto\tthe previous\tshort-term\tstate\th(t–1).\n\nbi,\tbf,\tbo,\tand\tbg\tare\tthe\tbias\tterms\tfor\teach\tof\tthe\tfour\tlayers.\tNote\tthat\tTensorFlow\tinitializes\tbf\tto a\tvector\tfull\tof\t1s\tinstead\tof\t0s.\tThis\tprevents\tforgetting\teverything\tat\tthe\tbeginning\tof\ttraining.",
      "content_length": 363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "Peephole\tConnections In\ta\tbasic\tLSTM\tcell,\tthe\tgate\tcontrollers\tcan\tlook\tonly\tat\tthe\tinput\tx(t)\tand\tthe\tprevious\tshort-term\tstate h(t–1).\tIt\tmay\tbe\ta\tgood\tidea\tto\tgive\tthem\ta\tbit\tmore\tcontext\tby\tletting\tthem\tpeek\tat\tthe\tlong-term\tstate\tas well.\tThis\tidea\twas\tproposed\tby\tFelix\tGers\tand\tJürgen\tSchmidhuber\tin\t2000.6\tThey\tproposed\tan\tLSTM variant\twith\textra\tconnections\tcalled\tpeephole\tconnections:\tthe\tprevious\tlong-term\tstate\tc(t–1)\tis\tadded as\tan\tinput\tto\tthe\tcontrollers\tof\tthe\tforget\tgate\tand\tthe\tinput\tgate,\tand\tthe\tcurrent\tlong-term\tstate\tc(t)\tis added\tas\tinput\tto\tthe\tcontroller\tof\tthe\toutput\tgate.\n\nTo\timplement\tpeephole\tconnections\tin\tTensorFlow,\tyou\tmust\tuse\tthe\tLSTMCell\tinstead\tof\tthe BasicLSTMCell\tand\tset\tuse_peepholes=True:\n\nlstm_cell\t=\ttf.contrib.rnn.LSTMCell(num_units=n_neurons,\tuse_peepholes=True)\n\nThere\tare\tmany\tother\tvariants\tof\tthe\tLSTM\tcell.\tOne\tparticularly\tpopular\tvariant\tis\tthe\tGRU\tcell,\twhich we\twill\tlook\tat\tnow.",
      "content_length": 941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 496,
      "content": "GRU\tCell The\tGated\tRecurrent\tUnit\t(GRU)\tcell\t(see\tFigure\t14-14)\twas\tproposed\tby\tKyunghyun\tCho\tet\tal.\tin\ta 2014\tpaper7\tthat\talso\tintroduced\tthe\tEncoder–Decoder\tnetwork\twe\tmentioned\tearlier.\n\nFigure\t14-14.\tGRU\tcell\n\nThe\tGRU\tcell\tis\ta\tsimplified\tversion\tof\tthe\tLSTM\tcell,\tand\tit\tseems\tto\tperform\tjust\tas\twell8\t(which explains\tits\tgrowing\tpopularity).\tThe\tmain\tsimplifications\tare:\n\nBoth\tstate\tvectors\tare\tmerged\tinto\ta\tsingle\tvector\th(t).\n\nA\tsingle\tgate\tcontroller\tcontrols\tboth\tthe\tforget\tgate\tand\tthe\tinput\tgate.\tIf\tthe\tgate\tcontroller\toutputs\ta 1,\tthe\tinput\tgate\tis\topen\tand\tthe\tforget\tgate\tis\tclosed.\tIf\tit\toutputs\ta\t0,\tthe\topposite\thappens.\tIn\tother words,\twhenever\ta\tmemory\tmust\tbe\tstored,\tthe\tlocation\twhere\tit\twill\tbe\tstored\tis\terased\tfirst.\tThis is\tactually\ta\tfrequent\tvariant\tto\tthe\tLSTM\tcell\tin\tand\tof\titself.\n\nThere\tis\tno\toutput\tgate;\tthe\tfull\tstate\tvector\tis\toutput\tat\tevery\ttime\tstep.\tHowever,\tthere\tis\ta\tnew\tgate controller\tthat\tcontrols\twhich\tpart\tof\tthe\tprevious\tstate\twill\tbe\tshown\tto\tthe\tmain\tlayer.\n\nEquation\t14-4\tsummarizes\thow\tto\tcompute\tthe\tcell’s\tstate\tat\teach\ttime\tstep\tfor\ta\tsingle\tinstance.\n\nEquation\t14-4.\tGRU\tcomputations",
      "content_length": 1147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "Creating\ta\tGRU\tcell\tin\tTensorFlow\tis\ttrivial:\n\ngru_cell\t=\ttf.contrib.rnn.GRUCell(num_units=n_neurons)\n\nLSTM\tor\tGRU\tcells\tare\tone\tof\tthe\tmain\treasons\tbehind\tthe\tsuccess\tof\tRNNs\tin\trecent\tyears,\tin\tparticular for\tapplications\tin\tnatural\tlanguage\tprocessing\t(NLP).",
      "content_length": 261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "Natural\tLanguage\tProcessing Most\tof\tthe\tstate-of-the-art\tNLP\tapplications,\tsuch\tas\tmachine\ttranslation,\tautomatic\tsummarization, parsing,\tsentiment\tanalysis,\tand\tmore,\tare\tnow\tbased\t(at\tleast\tin\tpart)\ton\tRNNs.\tIn\tthis\tlast\tsection,\twe will\ttake\ta\tquick\tlook\tat\twhat\ta\tmachine\ttranslation\tmodel\tlooks\tlike.\tThis\ttopic\tis\tvery\twell\tcovered\tby TensorFlow’s\tawesome\tWord2Vec\tand\tSeq2Seq\ttutorials,\tso\tyou\tshould\tdefinitely\tcheck\tthem\tout.",
      "content_length": 434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "Word\tEmbeddings Before\twe\tstart,\twe\tneed\tto\tchoose\ta\tword\trepresentation.\tOne\toption\tcould\tbe\tto\trepresent\teach\tword using\ta\tone-hot\tvector.\tSuppose\tyour\tvocabulary\tcontains\t50,000\twords,\tthen\tthe\tnth\tword\twould\tbe represented\tas\ta\t50,000-dimensional\tvector,\tfull\tof\t0s\texcept\tfor\ta\t1\tat\tthe\tnth\tposition.\tHowever,\twith such\ta\tlarge\tvocabulary,\tthis\tsparse\trepresentation\twould\tnot\tbe\tefficient\tat\tall.\tIdeally,\tyou\twant\tsimilar words\tto\thave\tsimilar\trepresentations,\tmaking\tit\teasy\tfor\tthe\tmodel\tto\tgeneralize\twhat\tit\tlearns\tabout\ta word\tto\tall\tsimilar\twords.\tFor\texample,\tif\tthe\tmodel\tis\ttold\tthat\t“I\tdrink\tmilk”\tis\ta\tvalid\tsentence,\tand\tif it\tknows\tthat\t“milk”\tis\tclose\tto\t“water”\tbut\tfar\tfrom\t“shoes,”\tthen\tit\twill\tknow\tthat\t“I\tdrink\twater”\tis probably\ta\tvalid\tsentence\tas\twell,\twhile\t“I\tdrink\tshoes”\tis\tprobably\tnot.\tBut\thow\tcan\tyou\tcome\tup\twith such\ta\tmeaningful\trepresentation?\n\nThe\tmost\tcommon\tsolution\tis\tto\trepresent\teach\tword\tin\tthe\tvocabulary\tusing\ta\tfairly\tsmall\tand\tdense vector\t(e.g.,\t150\tdimensions),\tcalled\tan\tembedding,\tand\tjust\tlet\tthe\tneural\tnetwork\tlearn\ta\tgood embedding\tfor\teach\tword\tduring\ttraining.\tAt\tthe\tbeginning\tof\ttraining,\tembeddings\tare\tsimply\tchosen randomly,\tbut\tduring\ttraining,\tbackpropagation\tautomatically\tmoves\tthe\tembeddings\taround\tin\ta\tway\tthat helps\tthe\tneural\tnetwork\tperform\tits\ttask.\tTypically\tthis\tmeans\tthat\tsimilar\twords\twill\tgradually\tcluster close\tto\tone\tanother,\tand\teven\tend\tup\torganized\tin\ta\trather\tmeaningful\tway.\tFor\texample,\tembeddings may\tend\tup\tplaced\talong\tvarious\taxes\tthat\trepresent\tgender,\tsingular/plural,\tadjective/noun,\tand\tso\ton. The\tresult\tcan\tbe\ttruly\tamazing.9\n\nIn\tTensorFlow,\tyou\tfirst\tneed\tto\tcreate\tthe\tvariable\trepresenting\tthe\tembeddings\tfor\tevery\tword\tin\tyour vocabulary\t(initialized\trandomly):\n\nvocabulary_size\t=\t50000 embedding_size\t=\t150\n\ninit_embeds\t=\ttf.random_uniform([vocabulary_size,\tembedding_size],\t-1.0,\t1.0) embeddings\t=\ttf.Variable(init_embeds)\n\nNow\tsuppose\tyou\twant\tto\tfeed\tthe\tsentence\t“I\tdrink\tmilk”\tto\tyour\tneural\tnetwork.\tYou\tshould\tfirst preprocess\tthe\tsentence\tand\tbreak\tit\tinto\ta\tlist\tof\tknown\twords.\tFor\texample\tyou\tmay\tremove unnecessary\tcharacters,\treplace\tunknown\twords\tby\ta\tpredefined\ttoken\tword\tsuch\tas\t“[UNK]”,\treplace numerical\tvalues\tby\t“[NUM]”,\treplace\tURLs\tby\t“[URL]”,\tand\tso\ton.\tOnce\tyou\thave\ta\tlist\tof\tknown words,\tyou\tcan\tlook\tup\teach\tword’s\tinteger\tidentifier\t(from\t0\tto\t49999)\tin\ta\tdictionary,\tfor\texample\t[72, 3335,\t288].\tAt\tthat\tpoint,\tyou\tare\tready\tto\tfeed\tthese\tword\tidentifiers\tto\tTensorFlow\tusing\ta\tplaceholder, and\tapply\tthe\tembedding_lookup()\tfunction\tto\tget\tthe\tcorresponding\tembeddings:\n\ntrain_inputs\t=\ttf.placeholder(tf.int32,\tshape=[None])\t\t#\tfrom\tids... embed\t=\ttf.nn.embedding_lookup(embeddings,\ttrain_inputs)\t\t#\t...to\tembeddings\n\nOnce\tyour\tmodel\thas\tlearned\tgood\tword\tembeddings,\tthey\tcan\tactually\tbe\treused\tfairly\tefficiently\tin\tany NLP\tapplication:\tafter\tall,\t“milk”\tis\tstill\tclose\tto\t“water”\tand\tfar\tfrom\t“shoes”\tno\tmatter\twhat\tyour application\tis.\tIn\tfact,\tinstead\tof\ttraining\tyour\town\tword\tembeddings,\tyou\tmay\twant\tto\tdownload pretrained\tword\tembeddings.\tJust\tlike\twhen\treusing\tpretrained\tlayers\t(see\tChapter\t11),\tyou\tcan\tchoose\tto freeze\tthe\tpretrained\tembeddings\t(e.g.,\tcreating\tthe\tembeddings\tvariable\tusing\ttrainable=False)\tor\tlet",
      "content_length": 3266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "backpropagation\ttweak\tthem\tfor\tyour\tapplication.\tThe\tfirst\toption\twill\tspeed\tup\ttraining,\tbut\tthe\tsecond may\tlead\tto\tslightly\thigher\tperformance.\n\nTIP\n\nEmbeddings\tare\talso\tuseful\tfor\trepresenting\tcategorical\tattributes\tthat\tcan\ttake\ton\ta\tlarge\tnumber\tof\tdifferent\tvalues,\tespecially when\tthere\tare\tcomplex\tsimilarities\tbetween\tvalues.\tFor\texample,\tconsider\tprofessions,\thobbies,\tdishes,\tspecies,\tbrands,\tand\tso on.\n\nYou\tnow\thave\talmost\tall\tthe\ttools\tyou\tneed\tto\timplement\ta\tmachine\ttranslation\tsystem.\tLet’s\tlook\tat\tthis now.",
      "content_length": 525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "An\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation Let’s\ttake\ta\tlook\tat\ta\tsimple\tmachine\ttranslation\tmodel10\tthat\twill\ttranslate\tEnglish\tsentences\tto\tFrench (see\tFigure\t14-15).\n\nFigure\t14-15.\tA\tsimple\tmachine\ttranslation\tmodel\n\nThe\tEnglish\tsentences\tare\tfed\tto\tthe\tencoder,\tand\tthe\tdecoder\toutputs\tthe\tFrench\ttranslations.\tNote\tthat the\tFrench\ttranslations\tare\talso\tused\tas\tinputs\tto\tthe\tdecoder,\tbut\tpushed\tback\tby\tone\tstep.\tIn\tother words,\tthe\tdecoder\tis\tgiven\tas\tinput\tthe\tword\tthat\tit\tshould\thave\toutput\tat\tthe\tprevious\tstep\t(regardless\tof what\tit\tactually\toutput).\tFor\tthe\tvery\tfirst\tword,\tit\tis\tgiven\ta\ttoken\tthat\trepresents\tthe\tbeginning\tof\tthe sentence\t(e.g.,\t“<go>”).\tThe\tdecoder\tis\texpected\tto\tend\tthe\tsentence\twith\tan\tend-of-sequence\t(EOS) token\t(e.g.,\t“<eos>”).\n\nNote\tthat\tthe\tEnglish\tsentences\tare\treversed\tbefore\tthey\tare\tfed\tto\tthe\tencoder.\tFor\texample\t“I\tdrink milk”\tis\treversed\tto\t“milk\tdrink\tI.”\tThis\tensures\tthat\tthe\tbeginning\tof\tthe\tEnglish\tsentence\twill\tbe\tfed\tlast to\tthe\tencoder,\twhich\tis\tuseful\tbecause\tthat’s\tgenerally\tthe\tfirst\tthing\tthat\tthe\tdecoder\tneeds\tto\ttranslate.\n\nEach\tword\tis\tinitially\trepresented\tby\ta\tsimple\tinteger\tidentifier\t(e.g.,\t288\tfor\tthe\tword\t“milk”).\tNext,\tan embedding\tlookup\treturns\tthe\tword\tembedding\t(as\texplained\tearlier,\tthis\tis\ta\tdense,\tfairly\tlow- dimensional\tvector).\tThese\tword\tembeddings\tare\twhat\tis\tactually\tfed\tto\tthe\tencoder\tand\tthe\tdecoder.\n\nAt\teach\tstep,\tthe\tdecoder\toutputs\ta\tscore\tfor\teach\tword\tin\tthe\toutput\tvocabulary\t(i.e.,\tFrench),\tand\tthen the\tSoftmax\tlayer\tturns\tthese\tscores\tinto\tprobabilities.\tFor\texample,\tat\tthe\tfirst\tstep\tthe\tword\t“Je”\tmay have\ta\tprobability\tof\t20%,\t“Tu”\tmay\thave\ta\tprobability\tof\t1%,\tand\tso\ton.\tThe\tword\twith\tthe\thighest probability\tis\toutput.\tThis\tis\tvery\tmuch\tlike\ta\tregular\tclassification\ttask,\tso\tyou\tcan\ttrain\tthe\tmodel\tusing the\tsoftmax_cross_entropy_with_logits()\tfunction.\n\nNote\tthat\tat\tinference\ttime\t(after\ttraining),\tyou\twill\tnot\thave\tthe\ttarget\tsentence\tto\tfeed\tto\tthe\tdecoder.",
      "content_length": 1982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "Instead,\tsimply\tfeed\tthe\tdecoder\tthe\tword\tthat\tit\toutput\tat\tthe\tprevious\tstep,\tas\tshown\tin\tFigure\t14-16 (this\twill\trequire\tan\tembedding\tlookup\tthat\tis\tnot\tshown\ton\tthe\tdiagram).\n\nFigure\t14-16.\tFeeding\tthe\tprevious\toutput\tword\tas\tinput\tat\tinference\ttime\n\nOkay,\tnow\tyou\thave\tthe\tbig\tpicture.\tHowever,\tif\tyou\tgo\tthrough\tTensorFlow’s\tsequence-to-sequence tutorial\tand\tyou\tlook\tat\tthe\tcode\tin\trnn/translate/seq2seq_model.py\t(in\tthe\tTensorFlow\tmodels),\tyou will\tnotice\ta\tfew\timportant\tdifferences:\n\nFirst,\tso\tfar\twe\thave\tassumed\tthat\tall\tinput\tsequences\t(to\tthe\tencoder\tand\tto\tthe\tdecoder)\thave\ta constant\tlength.\tBut\tobviously\tsentence\tlengths\tmay\tvary.\tThere\tare\tseveral\tways\tthat\tthis\tcan\tbe handled\t—\tfor\texample,\tusing\tthe\tsequence_length\targument\tto\tthe\tstatic_rnn()\tor dynamic_rnn()\tfunctions\tto\tspecify\teach\tsentence’s\tlength\t(as\tdiscussed\tearlier).\tHowever,\tanother approach\tis\tused\tin\tthe\ttutorial\t(presumably\tfor\tperformance\treasons):\tsentences\tare\tgrouped\tinto buckets\tof\tsimilar\tlengths\t(e.g.,\ta\tbucket\tfor\tthe\t1-\tto\t6-word\tsentences,\tanother\tfor\tthe\t7-\tto\t12- word\tsentences,\tand\tso\ton11),\tand\tthe\tshorter\tsentences\tare\tpadded\tusing\ta\tspecial\tpadding\ttoken (e.g.,\t“<pad>”).\tFor\texample\t“I\tdrink\tmilk”\tbecomes\t“<pad>\t<pad>\t<pad>\tmilk\tdrink\tI”,\tand\tits translation\tbecomes\t“Je\tbois\tdu\tlait\t<eos>\t<pad>”.\tOf\tcourse,\twe\twant\tto\tignore\tany\toutput\tpast\tthe EOS\ttoken.\tFor\tthis,\tthe\ttutorial’s\timplementation\tuses\ta\ttarget_weights\tvector.\tFor\texample,\tfor the\ttarget\tsentence\t“Je\tbois\tdu\tlait\t<eos>\t<pad>”,\tthe\tweights\twould\tbe\tset\tto\t[1.0,\t1.0,\t1.0, 1.0,\t1.0,\t0.0]\t(notice\tthe\tweight\t0.0\tthat\tcorresponds\tto\tthe\tpadding\ttoken\tin\tthe\ttarget\tsentence). Simply\tmultiplying\tthe\tlosses\tby\tthe\ttarget\tweights\twill\tzero\tout\tthe\tlosses\tthat\tcorrespond\tto\twords past\tEOS\ttokens.\n\nSecond,\twhen\tthe\toutput\tvocabulary\tis\tlarge\t(which\tis\tthe\tcase\there),\toutputting\ta\tprobability\tfor each\tand\tevery\tpossible\tword\twould\tbe\tterribly\tslow.\tIf\tthe\ttarget\tvocabulary\tcontains,\tsay,\t50,000 French\twords,\tthen\tthe\tdecoder\twould\toutput\t50,000-dimensional\tvectors,\tand\tthen\tcomputing\tthe softmax\tfunction\tover\tsuch\ta\tlarge\tvector\twould\tbe\tvery\tcomputationally\tintensive.\tTo\tavoid\tthis, one\tsolution\tis\tto\tlet\tthe\tdecoder\toutput\tmuch\tsmaller\tvectors,\tsuch\tas\t1,000-dimensional\tvectors, then\tuse\ta\tsampling\ttechnique\tto\testimate\tthe\tloss\twithout\thaving\tto\tcompute\tit\tover\tevery\tsingle word\tin\tthe\ttarget\tvocabulary.\tThis\tSampled\tSoftmax\ttechnique\twas\tintroduced\tin\t2015\tby\tSébastien Jean\tet\tal.12\tIn\tTensorFlow\tyou\tcan\tuse\tthe\tsampled_softmax_loss()\tfunction.\n\nThird,\tthe\ttutorial’s\timplementation\tuses\tan\tattention\tmechanism\tthat\tlets\tthe\tdecoder\tpeek\tinto\tthe input\tsequence.\tAttention\taugmented\tRNNs\tare\tbeyond\tthe\tscope\tof\tthis\tbook,\tbut\tif\tyou\tare",
      "content_length": 2729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "interested\tthere\tare\thelpful\tpapers\tabout\tmachine\ttranslation,13\tmachine\treading,14\tand\timage captions15\tusing\tattention.\n\nFinally,\tthe\ttutorial’s\timplementation\tmakes\tuse\tof\tthe\ttf.nn.legacy_seq2seq\tmodule,\twhich provides\ttools\tto\tbuild\tvarious\tEncoder–Decoder\tmodels\teasily.\tFor\texample,\tthe embedding_rnn_seq2seq()\tfunction\tcreates\ta\tsimple\tEncoder–Decoder\tmodel\tthat\tautomatically takes\tcare\tof\tword\tembeddings\tfor\tyou,\tjust\tlike\tthe\tone\trepresented\tin\tFigure\t14-15.\tThis\tcode\twill likely\tbe\tupdated\tquickly\tto\tuse\tthe\tnew\ttf.nn.seq2seq\tmodule.\n\nYou\tnow\thave\tall\tthe\ttools\tyou\tneed\tto\tunderstand\tthe\tsequence-to-sequence\ttutorial’s\timplementation. Check\tit\tout\tand\ttrain\tyour\town\tEnglish-to-French\ttranslator!",
      "content_length": 713,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "Exercises\n\n1.\t Can\tyou\tthink\tof\ta\tfew\tapplications\tfor\ta\tsequence-to-sequence\tRNN?\tWhat\tabout\ta\tsequence-to- vector\tRNN?\tAnd\ta\tvector-to-sequence\tRNN?\n\n2.\t Why\tdo\tpeople\tuse\tencoder–decoder\tRNNs\trather\tthan\tplain\tsequence-to-sequence\tRNNs\tfor automatic\ttranslation?\n\n3.\t How\tcould\tyou\tcombine\ta\tconvolutional\tneural\tnetwork\twith\tan\tRNN\tto\tclassify\tvideos?\n\n4.\t What\tare\tthe\tadvantages\tof\tbuilding\tan\tRNN\tusing\tdynamic_rnn()\trather\tthan\tstatic_rnn()?\n\n5.\t How\tcan\tyou\tdeal\twith\tvariable-length\tinput\tsequences?\tWhat\tabout\tvariable-length\toutput sequences?\n\n6.\t What\tis\ta\tcommon\tway\tto\tdistribute\ttraining\tand\texecution\tof\ta\tdeep\tRNN\tacross\tmultiple\tGPUs?\n\n7.\t Embedded\tReber\tgrammars\twere\tused\tby\tHochreiter\tand\tSchmidhuber\tin\ttheir\tpaper\tabout\tLSTMs. They\tare\tartificial\tgrammars\tthat\tproduce\tstrings\tsuch\tas\t“BPBTSXXVPSEPE.”\tCheck\tout\tJenny Orr’s\tnice\tintroduction\tto\tthis\ttopic.\tChoose\ta\tparticular\tembedded\tReber\tgrammar\t(such\tas\tthe\tone represented\ton\tJenny\tOrr’s\tpage),\tthen\ttrain\tan\tRNN\tto\tidentify\twhether\ta\tstring\trespects\tthat grammar\tor\tnot.\tYou\twill\tfirst\tneed\tto\twrite\ta\tfunction\tcapable\tof\tgenerating\ta\ttraining\tbatch containing\tabout\t50%\tstrings\tthat\trespect\tthe\tgrammar,\tand\t50%\tthat\tdon’t.\n\n8.\t Tackle\tthe\t“How\tmuch\tdid\tit\train?\tII”\tKaggle\tcompetition.\tThis\tis\ta\ttime\tseries\tprediction\ttask:\tyou are\tgiven\tsnapshots\tof\tpolarimetric\tradar\tvalues\tand\tasked\tto\tpredict\tthe\thourly\train\tgauge\ttotal. Luis\tAndre\tDutra\te\tSilva’s\tinterview\tgives\tsome\tinteresting\tinsights\tinto\tthe\ttechniques\the\tused\tto reach\tsecond\tplace\tin\tthe\tcompetition.\tIn\tparticular,\the\tused\tan\tRNN\tcomposed\tof\ttwo\tLSTM\tlayers.\n\n9.\t Go\tthrough\tTensorFlow’s\tWord2Vec\ttutorial\tto\tcreate\tword\tembeddings,\tand\tthen\tgo\tthrough\tthe Seq2Seq\ttutorial\tto\ttrain\tan\tEnglish-to-French\ttranslation\tsystem.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\nNote\tthat\tmany\tresearchers\tprefer\tto\tuse\tthe\thyperbolic\ttangent\t(tanh)\tactivation\tfunction\tin\tRNNs\trather\tthan\tthe\tReLU\tactivation function.\tFor\texample,\ttake\ta\tlook\tat\tby\tVu\tPham\tet\tal.’s\tpaper\t“Dropout\tImproves\tRecurrent\tNeural\tNetworks\tfor\tHandwriting Recognition”.\tHowever,\tReLU-based\tRNNs\tare\talso\tpossible,\tas\tshown\tin\tQuoc\tV.\tLe\tet\tal.’s\tpaper\t“A\tSimple\tWay\tto\tInitialize Recurrent\tNetworks\tof\tRectified\tLinear\tUnits”.\n\n2\n\nThis\tuses\tthe\tdecorator\tdesign\tpattern.\n\n3\n\n“Long\tShort-Term\tMemory,”\tS.\tHochreiter\tand\tJ.\tSchmidhuber\t(1997).\n\n4\n\n“Long\tShort-Term\tMemory\tRecurrent\tNeural\tNetwork\tArchitectures\tfor\tLarge\tScale\tAcoustic\tModeling,”\tH.\tSak\tet\tal.\t(2014).\n\n5\n\n“Recurrent\tNeural\tNetwork\tRegularization,”\tW.\tZaremba\tet\tal.\t(2015).\n\n6\n\n“Recurrent\tNets\tthat\tTime\tand\tCount,”\tF.\tGers\tand\tJ.\tSchmidhuber\t(2000).\n\n7\n\n“Learning\tPhrase\tRepresentations\tusing\tRNN\tEncoder–Decoder\tfor\tStatistical\tMachine\tTranslation,”\tK.\tCho\tet\tal.\t(2014).\n\n8\n\nA\t2015\tpaper\tby\tKlaus\tGreff\tet\tal.,\t“LSTM:\tA\tSearch\tSpace\tOdyssey,”\tseems\tto\tshow\tthat\tall\tLSTM\tvariants\tperform\troughly\tthe\tsame.\n\n9",
      "content_length": 2913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\nFor\tmore\tdetails,\tcheck\tout\tChristopher\tOlah’s\tgreat\tpost,\tor\tSebastian\tRuder’s\tseries\tof\tposts.\n\n“Sequence\tto\tSequence\tlearning\twith\tNeural\tNetworks,”\tI.\tSutskever\tet\tal.\t(2014).\n\nThe\tbucket\tsizes\tused\tin\tthe\ttutorial\tare\tdifferent.\n\n“On\tUsing\tVery\tLarge\tTarget\tVocabulary\tfor\tNeural\tMachine\tTranslation,”\tS.\tJean\tet\tal.\t(2015).\n\n“Neural\tMachine\tTranslation\tby\tJointly\tLearning\tto\tAlign\tand\tTranslate,”\tD.\tBahdanau\tet\tal.\t(2014).\n\n“Long\tShort-Term\tMemory-Networks\tfor\tMachine\tReading,”\tJ.\tCheng\t(2016).\n\n“Show,\tAttend\tand\tTell:\tNeural\tImage\tCaption\tGeneration\twith\tVisual\tAttention,”\tK.\tXu\tet\tal.\t(2015).",
      "content_length": 632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "Chapter\t15.\tAutoencoders\n\nAutoencoders\tare\tartificial\tneural\tnetworks\tcapable\tof\tlearning\tefficient\trepresentations\tof\tthe\tinput\tdata, called\tcodings,\twithout\tany\tsupervision\t(i.e.,\tthe\ttraining\tset\tis\tunlabeled).\tThese\tcodings\ttypically\thave a\tmuch\tlower\tdimensionality\tthan\tthe\tinput\tdata,\tmaking\tautoencoders\tuseful\tfor\tdimensionality\treduction (see\tChapter\t8).\tMore\timportantly,\tautoencoders\tact\tas\tpowerful\tfeature\tdetectors,\tand\tthey\tcan\tbe\tused for\tunsupervised\tpretraining\tof\tdeep\tneural\tnetworks\t(as\twe\tdiscussed\tin\tChapter\t11).\tLastly,\tthey\tare capable\tof\trandomly\tgenerating\tnew\tdata\tthat\tlooks\tvery\tsimilar\tto\tthe\ttraining\tdata;\tthis\tis\tcalled\ta generative\tmodel.\tFor\texample,\tyou\tcould\ttrain\tan\tautoencoder\ton\tpictures\tof\tfaces,\tand\tit\twould\tthen\tbe able\tto\tgenerate\tnew\tfaces.\n\nSurprisingly,\tautoencoders\twork\tby\tsimply\tlearning\tto\tcopy\ttheir\tinputs\tto\ttheir\toutputs.\tThis\tmay\tsound like\ta\ttrivial\ttask,\tbut\twe\twill\tsee\tthat\tconstraining\tthe\tnetwork\tin\tvarious\tways\tcan\tmake\tit\trather difficult.\tFor\texample,\tyou\tcan\tlimit\tthe\tsize\tof\tthe\tinternal\trepresentation,\tor\tyou\tcan\tadd\tnoise\tto\tthe inputs\tand\ttrain\tthe\tnetwork\tto\trecover\tthe\toriginal\tinputs.\tThese\tconstraints\tprevent\tthe\tautoencoder\tfrom trivially\tcopying\tthe\tinputs\tdirectly\tto\tthe\toutputs,\twhich\tforces\tit\tto\tlearn\tefficient\tways\tof\trepresenting the\tdata.\tIn\tshort,\tthe\tcodings\tare\tbyproducts\tof\tthe\tautoencoder’s\tattempt\tto\tlearn\tthe\tidentity\tfunction under\tsome\tconstraints.\n\nIn\tthis\tchapter\twe\twill\texplain\tin\tmore\tdepth\thow\tautoencoders\twork,\twhat\ttypes\tof\tconstraints\tcan\tbe imposed,\tand\thow\tto\timplement\tthem\tusing\tTensorFlow,\twhether\tit\tis\tfor\tdimensionality\treduction, feature\textraction,\tunsupervised\tpretraining,\tor\tas\tgenerative\tmodels.",
      "content_length": 1725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "Efficient\tData\tRepresentations Which\tof\tthe\tfollowing\tnumber\tsequences\tdo\tyou\tfind\tthe\teasiest\tto\tmemorize?\n\n40,\t27,\t25,\t36,\t81,\t57,\t10,\t73,\t19,\t68\n\n50,\t25,\t76,\t38,\t19,\t58,\t29,\t88,\t44,\t22,\t11,\t34,\t17,\t52,\t26,\t13,\t40,\t20\n\nAt\tfirst\tglance,\tit\twould\tseem\tthat\tthe\tfirst\tsequence\tshould\tbe\teasier,\tsince\tit\tis\tmuch\tshorter.\tHowever, if\tyou\tlook\tcarefully\tat\tthe\tsecond\tsequence,\tyou\tmay\tnotice\tthat\tit\tfollows\ttwo\tsimple\trules:\teven numbers\tare\tfollowed\tby\ttheir\thalf,\tand\todd\tnumbers\tare\tfollowed\tby\ttheir\ttriple\tplus\tone\t(this\tis\ta famous\tsequence\tknown\tas\tthe\thailstone\tsequence).\tOnce\tyou\tnotice\tthis\tpattern,\tthe\tsecond\tsequence becomes\tmuch\teasier\tto\tmemorize\tthan\tthe\tfirst\tbecause\tyou\tonly\tneed\tto\tmemorize\tthe\ttwo\trules,\tthe\tfirst number,\tand\tthe\tlength\tof\tthe\tsequence.\tNote\tthat\tif\tyou\tcould\tquickly\tand\teasily\tmemorize\tvery\tlong sequences,\tyou\twould\tnot\tcare\tmuch\tabout\tthe\texistence\tof\ta\tpattern\tin\tthe\tsecond\tsequence.\tYou\twould just\tlearn\tevery\tnumber\tby\theart,\tand\tthat\twould\tbe\tthat.\tIt\tis\tthe\tfact\tthat\tit\tis\thard\tto\tmemorize\tlong sequences\tthat\tmakes\tit\tuseful\tto\trecognize\tpatterns,\tand\thopefully\tthis\tclarifies\twhy\tconstraining\tan autoencoder\tduring\ttraining\tpushes\tit\tto\tdiscover\tand\texploit\tpatterns\tin\tthe\tdata.\n\nThe\trelationship\tbetween\tmemory,\tperception,\tand\tpattern\tmatching\twas\tfamously\tstudied\tby\tWilliam Chase\tand\tHerbert\tSimon\tin\tthe\tearly\t1970s.1\tThey\tobserved\tthat\texpert\tchess\tplayers\twere\table\tto memorize\tthe\tpositions\tof\tall\tthe\tpieces\tin\ta\tgame\tby\tlooking\tat\tthe\tboard\tfor\tjust\t5\tseconds,\ta\ttask\tthat most\tpeople\twould\tfind\timpossible.\tHowever,\tthis\twas\tonly\tthe\tcase\twhen\tthe\tpieces\twere\tplaced\tin realistic\tpositions\t(from\tactual\tgames),\tnot\twhen\tthe\tpieces\twere\tplaced\trandomly.\tChess\texperts\tdon’t have\ta\tmuch\tbetter\tmemory\tthan\tyou\tand\tI,\tthey\tjust\tsee\tchess\tpatterns\tmore\teasily\tthanks\tto\ttheir experience\twith\tthe\tgame.\tNoticing\tpatterns\thelps\tthem\tstore\tinformation\tefficiently.\n\nJust\tlike\tthe\tchess\tplayers\tin\tthis\tmemory\texperiment,\tan\tautoencoder\tlooks\tat\tthe\tinputs,\tconverts\tthem\tto an\tefficient\tinternal\trepresentation,\tand\tthen\tspits\tout\tsomething\tthat\t(hopefully)\tlooks\tvery\tclose\tto\tthe inputs.\tAn\tautoencoder\tis\talways\tcomposed\tof\ttwo\tparts:\tan\tencoder\t(or\trecognition\tnetwork)\tthat converts\tthe\tinputs\tto\tan\tinternal\trepresentation,\tfollowed\tby\ta\tdecoder\t(or\tgenerative\tnetwork)\tthat converts\tthe\tinternal\trepresentation\tto\tthe\toutputs\t(see\tFigure\t15-1).\n\nAs\tyou\tcan\tsee,\tan\tautoencoder\ttypically\thas\tthe\tsame\tarchitecture\tas\ta\tMulti-Layer\tPerceptron\t(MLP; see\tChapter\t10),\texcept\tthat\tthe\tnumber\tof\tneurons\tin\tthe\toutput\tlayer\tmust\tbe\tequal\tto\tthe\tnumber\tof inputs.\tIn\tthis\texample,\tthere\tis\tjust\tone\thidden\tlayer\tcomposed\tof\ttwo\tneurons\t(the\tencoder),\tand\tone output\tlayer\tcomposed\tof\tthree\tneurons\t(the\tdecoder).\tThe\toutputs\tare\toften\tcalled\tthe\treconstructions since\tthe\tautoencoder\ttries\tto\treconstruct\tthe\tinputs,\tand\tthe\tcost\tfunction\tcontains\ta\treconstruction\tloss that\tpenalizes\tthe\tmodel\twhen\tthe\treconstructions\tare\tdifferent\tfrom\tthe\tinputs.",
      "content_length": 3007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "Figure\t15-1.\tThe\tchess\tmemory\texperiment\t(left)\tand\ta\tsimple\tautoencoder\t(right)\n\nBecause\tthe\tinternal\trepresentation\thas\ta\tlower\tdimensionality\tthan\tthe\tinput\tdata\t(it\tis\t2D\tinstead\tof\t3D), the\tautoencoder\tis\tsaid\tto\tbe\tundercomplete.\tAn\tundercomplete\tautoencoder\tcannot\ttrivially\tcopy\tits inputs\tto\tthe\tcodings,\tyet\tit\tmust\tfind\ta\tway\tto\toutput\ta\tcopy\tof\tits\tinputs.\tIt\tis\tforced\tto\tlearn\tthe\tmost important\tfeatures\tin\tthe\tinput\tdata\t(and\tdrop\tthe\tunimportant\tones).\n\nLet’s\tsee\thow\tto\timplement\ta\tvery\tsimple\tundercomplete\tautoencoder\tfor\tdimensionality\treduction.",
      "content_length": 567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "Performing\tPCA\twith\tan\tUndercomplete\tLinear\tAutoencoder If\tthe\tautoencoder\tuses\tonly\tlinear\tactivations\tand\tthe\tcost\tfunction\tis\tthe\tMean\tSquared\tError\t(MSE), then\tit\tcan\tbe\tshown\tthat\tit\tends\tup\tperforming\tPrincipal\tComponent\tAnalysis\t(see\tChapter\t8).\n\nThe\tfollowing\tcode\tbuilds\ta\tsimple\tlinear\tautoencoder\tto\tperform\tPCA\ton\ta\t3D\tdataset,\tprojecting\tit\tto 2D:\n\nimport\ttensorflow\tas\ttf\n\nn_inputs\t=\t3\t\t#\t3D\tinputs n_hidden\t=\t2\t\t#\t2D\tcodings n_outputs\t=\tn_inputs\n\nlearning_rate\t=\t0.01\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs]) hidden\t=\ttf.layers.dense(X,\tn_hidden) outputs\t=\ttf.layers.dense(hidden,\tn_outputs)\n\nreconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX))\t\t#\tMSE\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate) training_op\t=\toptimizer.minimize(reconstruction_loss)\n\ninit\t=\ttf.global_variables_initializer()\n\nThis\tcode\tis\treally\tnot\tvery\tdifferent\tfrom\tall\tthe\tMLPs\twe\tbuilt\tin\tpast\tchapters.\tThe\ttwo\tthings\tto\tnote are:\n\nThe\tnumber\tof\toutputs\tis\tequal\tto\tthe\tnumber\tof\tinputs.\n\nTo\tperform\tsimple\tPCA,\twe\tdo\tnot\tuse\tany\tactivation\tfunction\t(i.e.,\tall\tneurons\tare\tlinear)\tand\tthe cost\tfunction\tis\tthe\tMSE.\tWe\twill\tsee\tmore\tcomplex\tautoencoders\tshortly.\n\nNow\tlet’s\tload\tthe\tdataset,\ttrain\tthe\tmodel\ton\tthe\ttraining\tset,\tand\tuse\tit\tto\tencode\tthe\ttest\tset\t(i.e.,\tproject it\tto\t2D):\n\nX_train,\tX_test\t=\t[...]\t#\tload\tthe\tdataset\n\nn_iterations\t=\t1000 codings\t=\thidden\t\t#\tthe\toutput\tof\tthe\thidden\tlayer\tprovides\tthe\tcodings\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\titeration\tin\trange(n_iterations): \t\t\t\t\t\t\t\ttraining_op.run(feed_dict={X:\tX_train})\t\t#\tno\tlabels\t(unsupervised) \t\t\t\tcodings_val\t=\tcodings.eval(feed_dict={X:\tX_test})\n\nFigure\t15-2\tshows\tthe\toriginal\t3D\tdataset\t(at\tthe\tleft)\tand\tthe\toutput\tof\tthe\tautoencoder’s\thidden\tlayer (i.e.,\tthe\tcoding\tlayer,\tat\tthe\tright).\tAs\tyou\tcan\tsee,\tthe\tautoencoder\tfound\tthe\tbest\t2D\tplane\tto\tproject\tthe data\tonto,\tpreserving\tas\tmuch\tvariance\tin\tthe\tdata\tas\tit\tcould\t(just\tlike\tPCA).",
      "content_length": 1956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 510,
      "content": "Figure\t15-2.\tPCA\tperformed\tby\tan\tundercomplete\tlinear\tautoencoder",
      "content_length": 65,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "Stacked\tAutoencoders Just\tlike\tother\tneural\tnetworks\twe\thave\tdiscussed,\tautoencoders\tcan\thave\tmultiple\thidden\tlayers.\tIn\tthis case\tthey\tare\tcalled\tstacked\tautoencoders\t(or\tdeep\tautoencoders).\tAdding\tmore\tlayers\thelps\tthe autoencoder\tlearn\tmore\tcomplex\tcodings.\tHowever,\tone\tmust\tbe\tcareful\tnot\tto\tmake\tthe\tautoencoder\ttoo powerful.\tImagine\tan\tencoder\tso\tpowerful\tthat\tit\tjust\tlearns\tto\tmap\teach\tinput\tto\ta\tsingle\tarbitrary\tnumber (and\tthe\tdecoder\tlearns\tthe\treverse\tmapping).\tObviously\tsuch\tan\tautoencoder\twill\treconstruct\tthe\ttraining data\tperfectly,\tbut\tit\twill\tnot\thave\tlearned\tany\tuseful\tdata\trepresentation\tin\tthe\tprocess\t(and\tit\tis\tunlikely to\tgeneralize\twell\tto\tnew\tinstances).\n\nThe\tarchitecture\tof\ta\tstacked\tautoencoder\tis\ttypically\tsymmetrical\twith\tregards\tto\tthe\tcentral\thidden\tlayer (the\tcoding\tlayer).\tTo\tput\tit\tsimply,\tit\tlooks\tlike\ta\tsandwich.\tFor\texample,\tan\tautoencoder\tfor\tMNIST (introduced\tin\tChapter\t3)\tmay\thave\t784\tinputs,\tfollowed\tby\ta\thidden\tlayer\twith\t300\tneurons,\tthen\ta central\thidden\tlayer\tof\t150\tneurons,\tthen\tanother\thidden\tlayer\twith\t300\tneurons,\tand\tan\toutput\tlayer\twith 784\tneurons.\tThis\tstacked\tautoencoder\tis\trepresented\tin\tFigure\t15-3.\n\nFigure\t15-3.\tStacked\tautoencoder",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": "TensorFlow\tImplementation You\tcan\timplement\ta\tstacked\tautoencoder\tvery\tmuch\tlike\ta\tregular\tdeep\tMLP.\tIn\tparticular,\tthe\tsame techniques\twe\tused\tin\tChapter\t11\tfor\ttraining\tdeep\tnets\tcan\tbe\tapplied.\tFor\texample,\tthe\tfollowing\tcode builds\ta\tstacked\tautoencoder\tfor\tMNIST,\tusing\tHe\tinitialization,\tthe\tELU\tactivation\tfunction,\tand\tℓ2 regularization.\tThe\tcode\tshould\tlook\tvery\tfamiliar,\texcept\tthat\tthere\tare\tno\tlabels\t(no\ty):\n\nfrom\tfunctools\timport\tpartial\n\nn_inputs\t=\t28\t*\t28\t\t#\tfor\tMNIST n_hidden1\t=\t300 n_hidden2\t=\t150\t\t#\tcodings n_hidden3\t=\tn_hidden1 n_outputs\t=\tn_inputs\n\nlearning_rate\t=\t0.01 l2_reg\t=\t0.0001\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs])\n\nhe_init\t=\ttf.contrib.layers.variance_scaling_initializer() l2_regularizer\t=\ttf.contrib.layers.l2_regularizer(l2_reg) my_dense_layer\t=\tpartial(tf.layers.dense, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=tf.nn.elu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=he_init, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_regularizer=l2_regularizer)\n\nhidden1\t=\tmy_dense_layer(X,\tn_hidden1) hidden2\t=\tmy_dense_layer(hidden1,\tn_hidden2)\t\t#\tcodings hidden3\t=\tmy_dense_layer(hidden2,\tn_hidden3) outputs\t=\tmy_dense_layer(hidden3,\tn_outputs,\tactivation=None)\n\nreconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX))\t\t#\tMSE\n\nreg_losses\t=\ttf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) loss\t=\ttf.add_n([reconstruction_loss]\t+\treg_losses)\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate) training_op\t=\toptimizer.minimize(loss)\n\ninit\t=\ttf.global_variables_initializer()\n\nYou\tcan\tthen\ttrain\tthe\tmodel\tnormally.\tNote\tthat\tthe\tdigit\tlabels\t(y_batch)\tare\tunused:\n\nn_epochs\t=\t5 batch_size\t=\t150\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tn_batches\t=\tmnist.train.num_examples\t//\tbatch_size \t\t\t\t\t\t\t\tfor\titeration\tin\trange(n_batches): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch})",
      "content_length": 1932,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "Tying\tWeights When\tan\tautoencoder\tis\tneatly\tsymmetrical,\tlike\tthe\tone\twe\tjust\tbuilt,\ta\tcommon\ttechnique\tis\tto\ttie\tthe weights\tof\tthe\tdecoder\tlayers\tto\tthe\tweights\tof\tthe\tencoder\tlayers.\tThis\thalves\tthe\tnumber\tof\tweights\tin the\tmodel,\tspeeding\tup\ttraining\tand\tlimiting\tthe\trisk\tof\toverfitting.\tSpecifically,\tif\tthe\tautoencoder\thas\ta total\tof\tN\tlayers\t(not\tcounting\tthe\tinput\tlayer),\tand\tWL\trepresents\tthe\tconnection\tweights\tof\tthe\tLth\tlayer\n\n(e.g.,\tlayer\t1\tis\tthe\tfirst\thidden\tlayer,\tlayer\n\nis\tthe\tcoding\tlayer,\tand\tlayer\tN\tis\tthe\toutput\tlayer),\tthen\tthe\n\ndecoder\tlayer\tweights\tcan\tbe\tdefined\tsimply\tas:\tWN–L+1\t=\tWL\n\nT\t(with\tL\t=\t1,\t2,\n\n).\n\nUnfortunately,\timplementing\ttied\tweights\tin\tTensorFlow\tusing\tthe\tdense()\tfunction\tis\ta\tbit\tcumbersome; it’s\tactually\teasier\tto\tjust\tdefine\tthe\tlayers\tmanually.\tThe\tcode\tends\tup\tsignificantly\tmore\tverbose:\n\nactivation\t=\ttf.nn.elu regularizer\t=\ttf.contrib.layers.l2_regularizer(l2_reg) initializer\t=\ttf.contrib.layers.variance_scaling_initializer()\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs])\n\nweights1_init\t=\tinitializer([n_inputs,\tn_hidden1]) weights2_init\t=\tinitializer([n_hidden1,\tn_hidden2])\n\nweights1\t=\ttf.Variable(weights1_init,\tdtype=tf.float32,\tname=\"weights1\") weights2\t=\ttf.Variable(weights2_init,\tdtype=tf.float32,\tname=\"weights2\") weights3\t=\ttf.transpose(weights2,\tname=\"weights3\")\t\t#\ttied\tweights weights4\t=\ttf.transpose(weights1,\tname=\"weights4\")\t\t#\ttied\tweights\n\nbiases1\t=\ttf.Variable(tf.zeros(n_hidden1),\tname=\"biases1\") biases2\t=\ttf.Variable(tf.zeros(n_hidden2),\tname=\"biases2\") biases3\t=\ttf.Variable(tf.zeros(n_hidden3),\tname=\"biases3\") biases4\t=\ttf.Variable(tf.zeros(n_outputs),\tname=\"biases4\")\n\nhidden1\t=\tactivation(tf.matmul(X,\tweights1)\t+\tbiases1) hidden2\t=\tactivation(tf.matmul(hidden1,\tweights2)\t+\tbiases2) hidden3\t=\tactivation(tf.matmul(hidden2,\tweights3)\t+\tbiases3) outputs\t=\ttf.matmul(hidden3,\tweights4)\t+\tbiases4\n\nreconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX)) reg_loss\t=\tregularizer(weights1)\t+\tregularizer(weights2) loss\t=\treconstruction_loss\t+\treg_loss\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate) training_op\t=\toptimizer.minimize(loss)\n\ninit\t=\ttf.global_variables_initializer()\n\nThis\tcode\tis\tfairly\tstraightforward,\tbut\tthere\tare\ta\tfew\timportant\tthings\tto\tnote:\n\nFirst,\tweight3\tand\tweights4\tare\tnot\tvariables,\tthey\tare\trespectively\tthe\ttranspose\tof\tweights2\tand weights1\t(they\tare\t“tied”\tto\tthem).\n\nSecond,\tsince\tthey\tare\tnot\tvariables,\tit’s\tno\tuse\tregularizing\tthem:\twe\tonly\tregularize\tweights1\tand weights2.\n\nThird,\tbiases\tare\tnever\ttied,\tand\tnever\tregularized.",
      "content_length": 2565,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 514,
      "content": "Training\tOne\tAutoencoder\tat\ta\tTime Rather\tthan\ttraining\tthe\twhole\tstacked\tautoencoder\tin\tone\tgo\tlike\twe\tjust\tdid,\tit\tis\toften\tmuch\tfaster\tto train\tone\tshallow\tautoencoder\tat\ta\ttime,\tthen\tstack\tall\tof\tthem\tinto\ta\tsingle\tstacked\tautoencoder\t(hence the\tname),\tas\tshown\ton\tFigure\t15-4.\tThis\tis\tespecially\tuseful\tfor\tvery\tdeep\tautoencoders.\n\nFigure\t15-4.\tTraining\tone\tautoencoder\tat\ta\ttime\n\nDuring\tthe\tfirst\tphase\tof\ttraining,\tthe\tfirst\tautoencoder\tlearns\tto\treconstruct\tthe\tinputs.\tDuring\tthe\tsecond phase,\tthe\tsecond\tautoencoder\tlearns\tto\treconstruct\tthe\toutput\tof\tthe\tfirst\tautoencoder’s\thidden\tlayer. Finally,\tyou\tjust\tbuild\ta\tbig\tsandwich\tusing\tall\tthese\tautoencoders,\tas\tshown\tin\tFigure\t15-4\t(i.e.,\tyou\tfirst stack\tthe\thidden\tlayers\tof\teach\tautoencoder,\tthen\tthe\toutput\tlayers\tin\treverse\torder).\tThis\tgives\tyou\tthe final\tstacked\tautoencoder.\tYou\tcould\teasily\ttrain\tmore\tautoencoders\tthis\tway,\tbuilding\ta\tvery\tdeep stacked\tautoencoder.\n\nTo\timplement\tthis\tmultiphase\ttraining\talgorithm,\tthe\tsimplest\tapproach\tis\tto\tuse\ta\tdifferent\tTensorFlow graph\tfor\teach\tphase.\tAfter\ttraining\tan\tautoencoder,\tyou\tjust\trun\tthe\ttraining\tset\tthrough\tit\tand\tcapture\tthe output\tof\tthe\thidden\tlayer.\tThis\toutput\tthen\tserves\tas\tthe\ttraining\tset\tfor\tthe\tnext\tautoencoder.\tOnce\tall autoencoders\thave\tbeen\ttrained\tthis\tway,\tyou\tsimply\tcopy\tthe\tweights\tand\tbiases\tfrom\teach\tautoencoder and\tuse\tthem\tto\tbuild\tthe\tstacked\tautoencoder.\tImplementing\tthis\tapproach\tis\tquite\tstraightforward,\tso\twe won’t\tdetail\tit\there,\tbut\tplease\tcheck\tout\tthe\tcode\tin\tthe\tJupyter\tnotebooks\tfor\tan\texample.\n\nAnother\tapproach\tis\tto\tuse\ta\tsingle\tgraph\tcontaining\tthe\twhole\tstacked\tautoencoder,\tplus\tsome\textra operations\tto\tperform\teach\ttraining\tphase,\tas\tshown\tin\tFigure\t15-5.",
      "content_length": 1727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 515,
      "content": "Figure\t15-5.\tA\tsingle\tgraph\tto\ttrain\ta\tstacked\tautoencoder\n\nThis\tdeserves\ta\tbit\tof\texplanation:\n\nThe\tcentral\tcolumn\tin\tthe\tgraph\tis\tthe\tfull\tstacked\tautoencoder.\tThis\tpart\tcan\tbe\tused\tafter\ttraining.\n\nThe\tleft\tcolumn\tis\tthe\tset\tof\toperations\tneeded\tto\trun\tthe\tfirst\tphase\tof\ttraining.\tIt\tcreates\tan\toutput layer\tthat\tbypasses\thidden\tlayers\t2\tand\t3.\tThis\toutput\tlayer\tshares\tthe\tsame\tweights\tand\tbiases\tas the\tstacked\tautoencoder’s\toutput\tlayer.\tOn\ttop\tof\tthat\tare\tthe\ttraining\toperations\tthat\twill\taim\tat making\tthe\toutput\tas\tclose\tas\tpossible\tto\tthe\tinputs.\tThus,\tthis\tphase\twill\ttrain\tthe\tweights\tand biases\tfor\tthe\thidden\tlayer\t1\tand\tthe\toutput\tlayer\t(i.e.,\tthe\tfirst\tautoencoder).\n\nThe\tright\tcolumn\tin\tthe\tgraph\tis\tthe\tset\tof\toperations\tneeded\tto\trun\tthe\tsecond\tphase\tof\ttraining.\tIt adds\tthe\ttraining\toperation\tthat\twill\taim\tat\tmaking\tthe\toutput\tof\thidden\tlayer\t3\tas\tclose\tas\tpossible to\tthe\toutput\tof\thidden\tlayer\t1.\tNote\tthat\twe\tmust\tfreeze\thidden\tlayer\t1\twhile\trunning\tphase\t2.\tThis phase\twill\ttrain\tthe\tweights\tand\tbiases\tfor\thidden\tlayers\t2\tand\t3\t(i.e.,\tthe\tsecond\tautoencoder).\n\nThe\tTensorFlow\tcode\tlooks\tlike\tthis:\n\n[...]\t#\tBuild\tthe\twhole\tstacked\tautoencoder\tnormally. \t\t\t\t\t\t#\tIn\tthis\texample,\tthe\tweights\tare\tnot\ttied.\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate)\n\nwith\ttf.name_scope(\"phase1\"): \t\t\t\tphase1_outputs\t=\ttf.matmul(hidden1,\tweights4)\t+\tbiases4 \t\t\t\tphase1_reconstruction_loss\t=\ttf.reduce_mean(tf.square(phase1_outputs\t-\tX)) \t\t\t\tphase1_reg_loss\t=\tregularizer(weights1)\t+\tregularizer(weights4) \t\t\t\tphase1_loss\t=\tphase1_reconstruction_loss\t+\tphase1_reg_loss \t\t\t\tphase1_training_op\t=\toptimizer.minimize(phase1_loss)\n\nwith\ttf.name_scope(\"phase2\"): \t\t\t\tphase2_reconstruction_loss\t=\ttf.reduce_mean(tf.square(hidden3\t-\thidden1)) \t\t\t\tphase2_reg_loss\t=\tregularizer(weights2)\t+\tregularizer(weights3) \t\t\t\tphase2_loss\t=\tphase2_reconstruction_loss\t+\tphase2_reg_loss \t\t\t\ttrain_vars\t=\t[weights2,\tbiases2,\tweights3,\tbiases3] \t\t\t\tphase2_training_op\t=\toptimizer.minimize(phase2_loss,\tvar_list=train_vars)\n\nThe\tfirst\tphase\tis\trather\tstraightforward:\twe\tjust\tcreate\tan\toutput\tlayer\tthat\tskips\thidden\tlayers\t2\tand\t3, then\tbuild\tthe\ttraining\toperations\tto\tminimize\tthe\tdistance\tbetween\tthe\toutputs\tand\tthe\tinputs\t(plus\tsome",
      "content_length": 2230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 516,
      "content": "regularization).\n\nThe\tsecond\tphase\tjust\tadds\tthe\toperations\tneeded\tto\tminimize\tthe\tdistance\tbetween\tthe\toutput\tof\thidden layer\t3\tand\thidden\tlayer\t1\t(also\twith\tsome\tregularization).\tMost\timportantly,\twe\tprovide\tthe\tlist\tof trainable\tvariables\tto\tthe\tminimize()\tmethod,\tmaking\tsure\tto\tleave\tout\tweights1\tand\tbiases1;\tthis effectively\tfreezes\thidden\tlayer\t1\tduring\tphase\t2.\n\nDuring\tthe\texecution\tphase,\tall\tyou\tneed\tto\tdo\tis\trun\tthe\tphase\t1\ttraining\top\tfor\ta\tnumber\tof\tepochs,\tthen the\tphase\t2\ttraining\top\tfor\tsome\tmore\tepochs.\n\nTIP\n\nSince\thidden\tlayer\t1\tis\tfrozen\tduring\tphase\t2,\tits\toutput\twill\talways\tbe\tthe\tsame\tfor\tany\tgiven\ttraining\tinstance.\tTo\tavoid\thaving to\trecompute\tthe\toutput\tof\thidden\tlayer\t1\tat\tevery\tsingle\tepoch,\tyou\tcan\tcompute\tit\tfor\tthe\twhole\ttraining\tset\tat\tthe\tend\tof\tphase 1,\tthen\tdirectly\tfeed\tthe\tcached\toutput\tof\thidden\tlayer\t1\tduring\tphase\t2.\tThis\tcan\tgive\tyou\ta\tnice\tperformance\tboost.",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 517,
      "content": "Visualizing\tthe\tReconstructions One\tway\tto\tensure\tthat\tan\tautoencoder\tis\tproperly\ttrained\tis\tto\tcompare\tthe\tinputs\tand\tthe\toutputs.\tThey must\tbe\tfairly\tsimilar,\tand\tthe\tdifferences\tshould\tbe\tunimportant\tdetails.\tLet’s\tplot\ttwo\trandom\tdigits\tand their\treconstructions:\n\nn_test_digits\t=\t2 X_test\t=\tmnist.test.images[:n_test_digits]\n\nwith\ttf.Session()\tas\tsess: \t\t\t\t[...]\t#\tTrain\tthe\tAutoencoder \t\t\t\toutputs_val\t=\toutputs.eval(feed_dict={X:\tX_test})\n\ndef\tplot_image(image,\tshape=[28,\t28]): \t\t\t\tplt.imshow(image.reshape(shape),\tcmap=\"Greys\",\tinterpolation=\"nearest\") \t\t\t\tplt.axis(\"off\")\n\nfor\tdigit_index\tin\trange(n_test_digits): \t\t\t\tplt.subplot(n_test_digits,\t2,\tdigit_index\t*\t2\t+\t1) \t\t\t\tplot_image(X_test[digit_index]) \t\t\t\tplt.subplot(n_test_digits,\t2,\tdigit_index\t*\t2\t+\t2) \t\t\t\tplot_image(outputs_val[digit_index])\n\nFigure\t15-6\tshows\tthe\tresulting\timages.\n\nFigure\t15-6.\tOriginal\tdigits\t(left)\tand\ttheir\treconstructions\t(right)",
      "content_length": 922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 518,
      "content": "Looks\tclose\tenough.\tSo\tthe\tautoencoder\thas\tproperly\tlearned\tto\treproduce\tits\tinputs,\tbut\thas\tit\tlearned useful\tfeatures?\tLet’s\ttake\ta\tlook.",
      "content_length": 139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 519,
      "content": "Visualizing\tFeatures Once\tyour\tautoencoder\thas\tlearned\tsome\tfeatures,\tyou\tmay\twant\tto\ttake\ta\tlook\tat\tthem.\tThere\tare\tvarious techniques\tfor\tthis.\tArguably\tthe\tsimplest\ttechnique\tis\tto\tconsider\teach\tneuron\tin\tevery\thidden\tlayer,\tand find\tthe\ttraining\tinstances\tthat\tactivate\tit\tthe\tmost.\tThis\tis\tespecially\tuseful\tfor\tthe\ttop\thidden\tlayers\tsince they\toften\tcapture\trelatively\tlarge\tfeatures\tthat\tyou\tcan\teasily\tspot\tin\ta\tgroup\tof\ttraining\tinstances\tthat contain\tthem.\tFor\texample,\tif\ta\tneuron\tstrongly\tactivates\twhen\tit\tsees\ta\tcat\tin\ta\tpicture,\tit\twill\tbe\tpretty obvious\tthat\tthe\tpictures\tthat\tactivate\tit\tthe\tmost\tall\tcontain\tcats.\tHowever,\tfor\tlower\tlayers,\tthis technique\tdoes\tnot\twork\tso\twell,\tas\tthe\tfeatures\tare\tsmaller\tand\tmore\tabstract,\tso\tit’s\toften\thard\tto understand\texactly\twhat\tthe\tneuron\tis\tgetting\tall\texcited\tabout.\n\nLet’s\tlook\tat\tanother\ttechnique.\tFor\teach\tneuron\tin\tthe\tfirst\thidden\tlayer,\tyou\tcan\tcreate\tan\timage\twhere\ta pixel’s\tintensity\tcorresponds\tto\tthe\tweight\tof\tthe\tconnection\tto\tthe\tgiven\tneuron.\tFor\texample,\tthe following\tcode\tplots\tthe\tfeatures\tlearned\tby\tfive\tneurons\tin\tthe\tfirst\thidden\tlayer:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\t[...]\t#\ttrain\tautoencoder \t\t\t\tweights1_val\t=\tweights1.eval()\n\nfor\ti\tin\trange(5): \t\t\t\tplt.subplot(1,\t5,\ti\t+\t1) \t\t\t\tplot_image(weights1_val.T[i])\n\nYou\tmay\tget\tlow-level\tfeatures\tsuch\tas\tthe\tones\tshown\tin\tFigure\t15-7.\n\nFigure\t15-7.\tFeatures\tlearned\tby\tfive\tneurons\tfrom\tthe\tfirst\thidden\tlayer\n\nThe\tfirst\tfour\tfeatures\tseem\tto\tcorrespond\tto\tsmall\tpatches,\twhile\tthe\tfifth\tfeature\tseems\tto\tlook\tfor vertical\tstrokes\t(note\tthat\tthese\tfeatures\tcome\tfrom\tthe\tstacked\tdenoising\tautoencoder\tthat\twe\twill discuss\tlater).\n\nAnother\ttechnique\tis\tto\tfeed\tthe\tautoencoder\ta\trandom\tinput\timage,\tmeasure\tthe\tactivation\tof\tthe\tneuron you\tare\tinterested\tin,\tand\tthen\tperform\tbackpropagation\tto\ttweak\tthe\timage\tin\tsuch\ta\tway\tthat\tthe\tneuron will\tactivate\teven\tmore.\tIf\tyou\titerate\tseveral\ttimes\t(performing\tgradient\tascent),\tthe\timage\twill gradually\tturn\tinto\tthe\tmost\texciting\timage\t(for\tthe\tneuron).\tThis\tis\ta\tuseful\ttechnique\tto\tvisualize\tthe kinds\tof\tinputs\tthat\ta\tneuron\tis\tlooking\tfor.\n\nFinally,\tif\tyou\tare\tusing\tan\tautoencoder\tto\tperform\tunsupervised\tpretraining\t—\tfor\texample,\tfor\ta classification\ttask\t—\ta\tsimple\tway\tto\tverify\tthat\tthe\tfeatures\tlearned\tby\tthe\tautoencoder\tare\tuseful\tis\tto measure\tthe\tperformance\tof\tthe\tclassifier.",
      "content_length": 2376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 520,
      "content": "Unsupervised\tPretraining\tUsing\tStacked\tAutoencoders As\twe\tdiscussed\tin\tChapter\t11,\tif\tyou\tare\ttackling\ta\tcomplex\tsupervised\ttask\tbut\tyou\tdo\tnot\thave\ta\tlot\tof labeled\ttraining\tdata,\tone\tsolution\tis\tto\tfind\ta\tneural\tnetwork\tthat\tperforms\ta\tsimilar\ttask,\tand\tthen\treuse its\tlower\tlayers.\tThis\tmakes\tit\tpossible\tto\ttrain\ta\thigh-performance\tmodel\tusing\tonly\tlittle\ttraining\tdata because\tyour\tneural\tnetwork\twon’t\thave\tto\tlearn\tall\tthe\tlow-level\tfeatures;\tit\twill\tjust\treuse\tthe\tfeature detectors\tlearned\tby\tthe\texisting\tnet.\n\nSimilarly,\tif\tyou\thave\ta\tlarge\tdataset\tbut\tmost\tof\tit\tis\tunlabeled,\tyou\tcan\tfirst\ttrain\ta\tstacked\tautoencoder using\tall\tthe\tdata,\tthen\treuse\tthe\tlower\tlayers\tto\tcreate\ta\tneural\tnetwork\tfor\tyour\tactual\ttask,\tand\ttrain\tit using\tthe\tlabeled\tdata.\tFor\texample,\tFigure\t15-8\tshows\thow\tto\tuse\ta\tstacked\tautoencoder\tto\tperform unsupervised\tpretraining\tfor\ta\tclassification\tneural\tnetwork.\tThe\tstacked\tautoencoder\titself\tis\ttypically trained\tone\tautoencoder\tat\ta\ttime,\tas\tdiscussed\tearlier.\tWhen\ttraining\tthe\tclassifier,\tif\tyou\treally\tdon’t have\tmuch\tlabeled\ttraining\tdata,\tyou\tmay\twant\tto\tfreeze\tthe\tpretrained\tlayers\t(at\tleast\tthe\tlower\tones).\n\nFigure\t15-8.\tUnsupervised\tpretraining\tusing\tautoencoders\n\nNOTE\n\nThis\tsituation\tis\tactually\tquite\tcommon,\tbecause\tbuilding\ta\tlarge\tunlabeled\tdataset\tis\toften\tcheap\t(e.g.,\ta\tsimple\tscript\tcan download\tmillions\tof\timages\toff\tthe\tinternet),\tbut\tlabeling\tthem\tcan\tonly\tbe\tdone\treliably\tby\thumans\t(e.g.,\tclassifying\timages\tas cute\tor\tnot).\tLabeling\tinstances\tis\ttime-consuming\tand\tcostly,\tso\tit\tis\tquite\tcommon\tto\thave\tonly\ta\tfew\tthousand\tlabeled instances.\n\nAs\twe\tdiscussed\tearlier,\tone\tof\tthe\ttriggers\tof\tthe\tcurrent\tDeep\tLearning\ttsunami\tis\tthe\tdiscovery\tin\t2006 by\tGeoffrey\tHinton\tet\tal.\tthat\tdeep\tneural\tnetworks\tcan\tbe\tpretrained\tin\tan\tunsupervised\tfashion.\tThey used\trestricted\tBoltzmann\tmachines\tfor\tthat\t(see\tAppendix\tE),\tbut\tin\t2007\tYoshua\tBengio\tet\tal.\tshowed2 that\tautoencoders\tworked\tjust\tas\twell.",
      "content_length": 1963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 521,
      "content": "There\tis\tnothing\tspecial\tabout\tthe\tTensorFlow\timplementation:\tjust\ttrain\tan\tautoencoder\tusing\tall\tthe training\tdata,\tthen\treuse\tits\tencoder\tlayers\tto\tcreate\ta\tnew\tneural\tnetwork\t(see\tChapter\t11\tfor\tmore details\ton\thow\tto\treuse\tpretrained\tlayers,\tor\tcheck\tout\tthe\tcode\texamples\tin\tthe\tJupyter\tnotebooks).\n\nUp\tto\tnow,\tin\torder\tto\tforce\tthe\tautoencoder\tto\tlearn\tinteresting\tfeatures,\twe\thave\tlimited\tthe\tsize\tof\tthe coding\tlayer,\tmaking\tit\tundercomplete.\tThere\tare\tactually\tmany\tother\tkinds\tof\tconstraints\tthat\tcan\tbe used,\tincluding\tones\tthat\tallow\tthe\tcoding\tlayer\tto\tbe\tjust\tas\tlarge\tas\tthe\tinputs,\tor\teven\tlarger,\tresulting in\tan\tovercomplete\tautoencoder.\tLet’s\tlook\tat\tsome\tof\tthose\tapproaches\tnow.",
      "content_length": 700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 522,
      "content": "Denoising\tAutoencoders Another\tway\tto\tforce\tthe\tautoencoder\tto\tlearn\tuseful\tfeatures\tis\tto\tadd\tnoise\tto\tits\tinputs,\ttraining\tit\tto recover\tthe\toriginal,\tnoise-free\tinputs.\tThis\tprevents\tthe\tautoencoder\tfrom\ttrivially\tcopying\tits\tinputs\tto its\toutputs,\tso\tit\tends\tup\thaving\tto\tfind\tpatterns\tin\tthe\tdata.\n\nThe\tidea\tof\tusing\tautoencoders\tto\tremove\tnoise\thas\tbeen\taround\tsince\tthe\t1980s\t(e.g.,\tit\tis\tmentioned\tin Yann\tLeCun’s\t1987\tmaster’s\tthesis).\tIn\ta\t2008\tpaper,3\tPascal\tVincent\tet\tal.\tshowed\tthat\tautoencoders could\talso\tbe\tused\tfor\tfeature\textraction.\tIn\ta\t2010\tpaper,4\tVincent\tet\tal.\tintroduced\tstacked\tdenoising autoencoders.\n\nThe\tnoise\tcan\tbe\tpure\tGaussian\tnoise\tadded\tto\tthe\tinputs,\tor\tit\tcan\tbe\trandomly\tswitched\toff\tinputs,\tjust like\tin\tdropout\t(introduced\tin\tChapter\t11).\tFigure\t15-9\tshows\tboth\toptions.\n\nFigure\t15-9.\tDenoising\tautoencoders,\twith\tGaussian\tnoise\t(left)\tor\tdropout\t(right)",
      "content_length": 895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 523,
      "content": "TensorFlow\tImplementation Implementing\tdenoising\tautoencoders\tin\tTensorFlow\tis\tnot\ttoo\thard.\tLet’s\tstart\twith\tGaussian\tnoise.\tIt’s really\tjust\tlike\ttraining\ta\tregular\tautoencoder,\texcept\tyou\tadd\tnoise\tto\tthe\tinputs,\tand\tthe\treconstruction loss\tis\tcalculated\tbased\ton\tthe\toriginal\tinputs:\n\nnoise_level\t=\t1.0 X\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs]) X_noisy\t=\tX\t+\tnoise_level\t*\ttf.random_normal(tf.shape(X))\n\nhidden1\t=\ttf.layers.dense(X_noisy,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden1\") [...] reconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX))\t#\tMSE [...]\n\nWARNING\n\nSince\tthe\tshape\tof\tX\tis\tonly\tpartially\tdefined\tduring\tthe\tconstruction\tphase,\twe\tcannot\tknow\tin\tadvance\tthe\tshape\tof\tthe\tnoise that\twe\tmust\tadd\tto\tX.\tWe\tcannot\tcall\tX.get_shape()\tbecause\tthis\twould\tjust\treturn\tthe\tpartially\tdefined\tshape\tof\tX\t([None, n_inputs]),\tand\trandom_normal()\texpects\ta\tfully\tdefined\tshape\tso\tit\twould\traise\tan\texception.\tInstead,\twe\tcall\ttf.shape(X), which\tcreates\tan\toperation\tthat\twill\treturn\tthe\tshape\tof\tX\tat\truntime,\twhich\twill\tbe\tfully\tdefined\tat\tthat\tpoint.\n\nImplementing\tthe\tdropout\tversion,\twhich\tis\tmore\tcommon,\tis\tnot\tmuch\tharder:\n\ndropout_rate\t=\t0.3\n\ntraining\t=\ttf.placeholder_with_default(False,\tshape=(),\tname='training')\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs]) X_drop\t=\ttf.layers.dropout(X,\tdropout_rate,\ttraining=training)\n\nhidden1\t=\ttf.layers.dense(X_drop,\tn_hidden1,\tactivation=tf.nn.relu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname=\"hidden1\") [...] reconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX))\t#\tMSE [...]\n\nDuring\ttraining\twe\tmust\tset\ttraining\tto\tTrue\t(as\texplained\tin\tChapter\t11)\tusing\tthe\tfeed_dict:\n\nsess.run(training_op,\tfeed_dict={X:\tX_batch,\ttraining:\tTrue})\n\nDuring\ttesting\tit\tis\tnot\tnecessary\tto\tset\ttraining\tto\tFalse,\tsince\twe\tset\tthat\tas\tthe\tdefault\tin\tthe\tcall\tto the\tplaceholder_with_default()\tfunction.",
      "content_length": 1899,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 524,
      "content": "Sparse\tAutoencoders Another\tkind\tof\tconstraint\tthat\toften\tleads\tto\tgood\tfeature\textraction\tis\tsparsity:\tby\tadding\tan\tappropriate term\tto\tthe\tcost\tfunction,\tthe\tautoencoder\tis\tpushed\tto\treduce\tthe\tnumber\tof\tactive\tneurons\tin\tthe\tcoding layer.\tFor\texample,\tit\tmay\tbe\tpushed\tto\thave\ton\taverage\tonly\t5%\tsignificantly\tactive\tneurons\tin\tthe coding\tlayer.\tThis\tforces\tthe\tautoencoder\tto\trepresent\teach\tinput\tas\ta\tcombination\tof\ta\tsmall\tnumber\tof activations.\tAs\ta\tresult,\teach\tneuron\tin\tthe\tcoding\tlayer\ttypically\tends\tup\trepresenting\ta\tuseful\tfeature\t(if you\tcould\tspeak\tonly\ta\tfew\twords\tper\tmonth,\tyou\twould\tprobably\ttry\tto\tmake\tthem\tworth\tlistening\tto).\n\nIn\torder\tto\tfavor\tsparse\tmodels,\twe\tmust\tfirst\tmeasure\tthe\tactual\tsparsity\tof\tthe\tcoding\tlayer\tat\teach training\titeration.\tWe\tdo\tso\tby\tcomputing\tthe\taverage\tactivation\tof\teach\tneuron\tin\tthe\tcoding\tlayer,\tover the\twhole\ttraining\tbatch.\tThe\tbatch\tsize\tmust\tnot\tbe\ttoo\tsmall,\tor\telse\tthe\tmean\twill\tnot\tbe\taccurate.\n\nOnce\twe\thave\tthe\tmean\tactivation\tper\tneuron,\twe\twant\tto\tpenalize\tthe\tneurons\tthat\tare\ttoo\tactive\tby adding\ta\tsparsity\tloss\tto\tthe\tcost\tfunction.\tFor\texample,\tif\twe\tmeasure\tthat\ta\tneuron\thas\tan\taverage activation\tof\t0.3,\tbut\tthe\ttarget\tsparsity\tis\t0.1,\tit\tmust\tbe\tpenalized\tto\tactivate\tless.\tOne\tapproach\tcould be\tsimply\tadding\tthe\tsquared\terror\t(0.3\t–\t0.1)2\tto\tthe\tcost\tfunction,\tbut\tin\tpractice\ta\tbetter\tapproach\tis\tto use\tthe\tKullback–Leibler\tdivergence\t(briefly\tdiscussed\tin\tChapter\t4),\twhich\thas\tmuch\tstronger\tgradients than\tthe\tMean\tSquared\tError,\tas\tyou\tcan\tsee\tin\tFigure\t15-10.\n\nFigure\t15-10.\tSparsity\tloss\n\nGiven\ttwo\tdiscrete\tprobability\tdistributions\tP\tand\tQ,\tthe\tKL\tdivergence\tbetween\tthese\tdistributions, noted\tDKL(P\t\t Q),\tcan\tbe\tcomputed\tusing\tEquation\t15-1.\n\nEquation\t15-1.\tKullback–Leibler\tdivergence",
      "content_length": 1778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 525,
      "content": "In\tour\tcase,\twe\twant\tto\tmeasure\tthe\tdivergence\tbetween\tthe\ttarget\tprobability\tp\tthat\ta\tneuron\tin\tthe coding\tlayer\twill\tactivate,\tand\tthe\tactual\tprobability\tq\t(i.e.,\tthe\tmean\tactivation\tover\tthe\ttraining\tbatch). So\tthe\tKL\tdivergence\tsimplifies\tto\tEquation\t15-2.\n\nEquation\t15-2.\tKL\tdivergence\tbetween\tthe\ttarget\tsparsity\tp\tand\tthe\tactual\tsparsity\tq\n\nOnce\twe\thave\tcomputed\tthe\tsparsity\tloss\tfor\teach\tneuron\tin\tthe\tcoding\tlayer,\twe\tjust\tsum\tup\tthese\tlosses, and\tadd\tthe\tresult\tto\tthe\tcost\tfunction.\tIn\torder\tto\tcontrol\tthe\trelative\timportance\tof\tthe\tsparsity\tloss\tand the\treconstruction\tloss,\twe\tcan\tmultiply\tthe\tsparsity\tloss\tby\ta\tsparsity\tweight\thyperparameter.\tIf\tthis weight\tis\ttoo\thigh,\tthe\tmodel\twill\tstick\tclosely\tto\tthe\ttarget\tsparsity,\tbut\tit\tmay\tnot\treconstruct\tthe\tinputs properly,\tmaking\tthe\tmodel\tuseless.\tConversely,\tif\tit\tis\ttoo\tlow,\tthe\tmodel\twill\tmostly\tignore\tthe\tsparsity objective\tand\tit\twill\tnot\tlearn\tany\tinteresting\tfeatures.",
      "content_length": 944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 526,
      "content": "TensorFlow\tImplementation We\tnow\thave\tall\twe\tneed\tto\timplement\ta\tsparse\tautoencoder\tusing\tTensorFlow:\n\ndef\tkl_divergence(p,\tq): \t\t\t\treturn\tp\t*\ttf.log(p\t/\tq)\t+\t(1\t-\tp)\t*\ttf.log((1\t-\tp)\t/\t(1\t-\tq))\n\nlearning_rate\t=\t0.01 sparsity_target\t=\t0.1 sparsity_weight\t=\t0.2\n\n[...]\t#\tBuild\ta\tnormal\tautoencoder\t(in\tthis\texample\tthe\tcoding\tlayer\tis\thidden1)\n\nhidden1_mean\t=\ttf.reduce_mean(hidden1,\taxis=0)\t#\tbatch\tmean sparsity_loss\t=\ttf.reduce_sum(kl_divergence(sparsity_target,\thidden1_mean)) reconstruction_loss\t=\ttf.reduce_mean(tf.square(outputs\t-\tX))\t#\tMSE loss\t=\treconstruction_loss\t+\tsparsity_weight\t*\tsparsity_loss optimizer\t=\ttf.train.AdamOptimizer(learning_rate) training_op\t=\toptimizer.minimize(loss)\n\nAn\timportant\tdetail\tis\tthe\tfact\tthat\tthe\tactivations\tof\tthe\tcoding\tlayer\tmust\tbe\tbetween\t0\tand\t1\t(but\tnot equal\tto\t0\tor\t1),\tor\telse\tthe\tKL\tdivergence\twill\treturn\tNaN\t(Not\ta\tNumber).\tA\tsimple\tsolution\tis\tto\tuse the\tlogistic\tactivation\tfunction\tfor\tthe\tcoding\tlayer:\n\nhidden1\t=\ttf.layers.dense(X,\tn_hidden1,\tactivation=tf.nn.sigmoid)\n\nOne\tsimple\ttrick\tcan\tspeed\tup\tconvergence:\tinstead\tof\tusing\tthe\tMSE,\twe\tcan\tchoose\ta\treconstruction loss\tthat\twill\thave\tlarger\tgradients.\tCross\tentropy\tis\toften\ta\tgood\tchoice.\tTo\tuse\tit,\twe\tmust\tnormalize\tthe inputs\tto\tmake\tthem\ttake\ton\tvalues\tfrom\t0\tto\t1,\tand\tuse\tthe\tlogistic\tactivation\tfunction\tin\tthe\toutput\tlayer so\tthe\toutputs\talso\ttake\ton\tvalues\tfrom\t0\tto\t1.\tTensorFlow’s\tsigmoid_cross_entropy_with_logits() function\ttakes\tcare\tof\tefficiently\tapplying\tthe\tlogistic\t(sigmoid)\tactivation\tfunction\tto\tthe\toutputs\tand computing\tthe\tcross\tentropy:\n\n[...] logits\t=\ttf.layers.dense(hidden1,\tn_outputs) outputs\t=\ttf.nn.sigmoid(logits)\n\nxentropy\t=\ttf.nn.sigmoid_cross_entropy_with_logits(labels=X,\tlogits=logits) reconstruction_loss\t=\ttf.reduce_sum(xentropy)\n\nNote\tthat\tthe\toutputs\toperation\tis\tnot\tneeded\tduring\ttraining\t(we\tuse\tit\tonly\twhen\twe\twant\tto\tlook\tat\tthe reconstructions).",
      "content_length": 1912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 527,
      "content": "Variational\tAutoencoders Another\timportant\tcategory\tof\tautoencoders\twas\tintroduced\tin\t2014\tby\tDiederik\tKingma\tand\tMax Welling,5\tand\thas\tquickly\tbecome\tone\tof\tthe\tmost\tpopular\ttypes\tof\tautoencoders:\tvariational autoencoders.\n\nThey\tare\tquite\tdifferent\tfrom\tall\tthe\tautoencoders\twe\thave\tdiscussed\tso\tfar,\tin\tparticular:\n\nThey\tare\tprobabilistic\tautoencoders,\tmeaning\tthat\ttheir\toutputs\tare\tpartly\tdetermined\tby\tchance, even\tafter\ttraining\t(as\topposed\tto\tdenoising\tautoencoders,\twhich\tuse\trandomness\tonly\tduring training).\n\nMost\timportantly,\tthey\tare\tgenerative\tautoencoders,\tmeaning\tthat\tthey\tcan\tgenerate\tnew\tinstances that\tlook\tlike\tthey\twere\tsampled\tfrom\tthe\ttraining\tset.\n\nBoth\tthese\tproperties\tmake\tthem\trather\tsimilar\tto\tRBMs\t(see\tAppendix\tE),\tbut\tthey\tare\teasier\tto\ttrain\tand the\tsampling\tprocess\tis\tmuch\tfaster\t(with\tRBMs\tyou\tneed\tto\twait\tfor\tthe\tnetwork\tto\tstabilize\tinto\ta “thermal\tequilibrium”\tbefore\tyou\tcan\tsample\ta\tnew\tinstance).\n\nLet’s\ttake\ta\tlook\tat\thow\tthey\twork.\tFigure\t15-11\t(left)\tshows\ta\tvariational\tautoencoder.\tYou\tcan recognize,\tof\tcourse,\tthe\tbasic\tstructure\tof\tall\tautoencoders,\twith\tan\tencoder\tfollowed\tby\ta\tdecoder\t(in this\texample,\tthey\tboth\thave\ttwo\thidden\tlayers),\tbut\tthere\tis\ta\ttwist:\tinstead\tof\tdirectly\tproducing\ta coding\tfor\ta\tgiven\tinput,\tthe\tencoder\tproduces\ta\tmean\tcoding\tμ\tand\ta\tstandard\tdeviation\tσ.\tThe\tactual coding\tis\tthen\tsampled\trandomly\tfrom\ta\tGaussian\tdistribution\twith\tmean\tμ\tand\tstandard\tdeviation\tσ. After\tthat\tthe\tdecoder\tjust\tdecodes\tthe\tsampled\tcoding\tnormally.\tThe\tright\tpart\tof\tthe\tdiagram\tshows\ta training\tinstance\tgoing\tthrough\tthis\tautoencoder.\tFirst,\tthe\tencoder\tproduces\tμ\tand\tσ,\tthen\ta\tcoding\tis sampled\trandomly\t(notice\tthat\tit\tis\tnot\texactly\tlocated\tat\tμ),\tand\tfinally\tthis\tcoding\tis\tdecoded,\tand\tthe final\toutput\tresembles\tthe\ttraining\tinstance.",
      "content_length": 1806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 528,
      "content": "Figure\t15-11.\tVariational\tautoencoder\t(left),\tand\tan\tinstance\tgoing\tthrough\tit\t(right)\n\nAs\tyou\tcan\tsee\ton\tthe\tdiagram,\talthough\tthe\tinputs\tmay\thave\ta\tvery\tconvoluted\tdistribution,\ta\tvariational autoencoder\ttends\tto\tproduce\tcodings\tthat\tlook\tas\tthough\tthey\twere\tsampled\tfrom\ta\tsimple\tGaussian distribution:6\tduring\ttraining,\tthe\tcost\tfunction\t(discussed\tnext)\tpushes\tthe\tcodings\tto\tgradually\tmigrate within\tthe\tcoding\tspace\t(also\tcalled\tthe\tlatent\tspace)\tto\toccupy\ta\troughly\t(hyper)spherical\tregion\tthat looks\tlike\ta\tcloud\tof\tGaussian\tpoints.\tOne\tgreat\tconsequence\tis\tthat\tafter\ttraining\ta\tvariational autoencoder,\tyou\tcan\tvery\teasily\tgenerate\ta\tnew\tinstance:\tjust\tsample\ta\trandom\tcoding\tfrom\tthe\tGaussian distribution,\tdecode\tit,\tand\tvoilà!\n\nSo\tlet’s\tlook\tat\tthe\tcost\tfunction.\tIt\tis\tcomposed\tof\ttwo\tparts.\tThe\tfirst\tis\tthe\tusual\treconstruction\tloss\tthat pushes\tthe\tautoencoder\tto\treproduce\tits\tinputs\t(we\tcan\tuse\tcross\tentropy\tfor\tthis,\tas\tdiscussed\tearlier). The\tsecond\tis\tthe\tlatent\tloss\tthat\tpushes\tthe\tautoencoder\tto\thave\tcodings\tthat\tlook\tas\tthough\tthey\twere sampled\tfrom\ta\tsimple\tGaussian\tdistribution,\tfor\twhich\twe\tuse\tthe\tKL\tdivergence\tbetween\tthe\ttarget distribution\t(the\tGaussian\tdistribution)\tand\tthe\tactual\tdistribution\tof\tthe\tcodings.\tThe\tmath\tis\ta\tbit\tmore complex\tthan\tearlier,\tin\tparticular\tbecause\tof\tthe\tGaussian\tnoise,\twhich\tlimits\tthe\tamount\tof\tinformation that\tcan\tbe\ttransmitted\tto\tthe\tcoding\tlayer\t(thus\tpushing\tthe\tautoencoder\tto\tlearn\tuseful\tfeatures).\tLuckily, the\tequations\tsimplify\tto\tthe\tfollowing\tcode\tfor\tthe\tlatent\tloss:7\n\neps\t=\t1e-10\t\t#\tsmoothing\tterm\tto\tavoid\tcomputing\tlog(0)\twhich\tis\tNaN latent_loss\t=\t0.5\t*\ttf.reduce_sum( \t\t\t\ttf.square(hidden3_sigma)\t+\ttf.square(hidden3_mean) \t\t\t\t-\t1\t-\ttf.log(eps\t+\ttf.square(hidden3_sigma)))\n\nOne\tcommon\tvariant\tis\tto\ttrain\tthe\tencoder\tto\toutput\tγ\t=\tlog(σ2)\trather\tthan\tσ.\tWherever\twe\tneed\tσ\twe\n\ncan\tjust\tcompute\n\n.\tThis\tmakes\tit\ta\tbit\teasier\tfor\tthe\tencoder\tto\tcapture\tsigmas\tof\tdifferent",
      "content_length": 1962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 529,
      "content": "scales,\tand\tthus\tit\thelps\tspeed\tup\tconvergence.\tThe\tlatent\tloss\tends\tup\ta\tbit\tsimpler:\n\nlatent_loss\t=\t0.5\t*\ttf.reduce_sum( \t\t\t\ttf.exp(hidden3_gamma)\t+\ttf.square(hidden3_mean)\t-\t1\t-\thidden3_gamma)\n\nThe\tfollowing\tcode\tbuilds\tthe\tvariational\tautoencoder\tshown\tin\tFigure\t15-11\t(left),\tusing\tthe\tlog(σ2) variant:\n\nfrom\tfunctools\timport\tpartial\n\nn_inputs\t=\t28\t*\t28 n_hidden1\t=\t500 n_hidden2\t=\t500 n_hidden3\t=\t20\t\t#\tcodings n_hidden4\t=\tn_hidden2 n_hidden5\t=\tn_hidden1 n_outputs\t=\tn_inputs learning_rate\t=\t0.001\n\ninitializer\t=\ttf.contrib.layers.variance_scaling_initializer() my_dense_layer\t=\tpartial( \t\t\t\ttf.layers.dense, \t\t\t\tactivation=tf.nn.elu, \t\t\t\tkernel_initializer=initializer)\n\nX\t=\ttf.placeholder(tf.float32,\t[None,\tn_inputs]) hidden1\t=\tmy_dense_layer(X,\tn_hidden1) hidden2\t=\tmy_dense_layer(hidden1,\tn_hidden2) hidden3_mean\t=\tmy_dense_layer(hidden2,\tn_hidden3,\tactivation=None) hidden3_gamma\t=\tmy_dense_layer(hidden2,\tn_hidden3,\tactivation=None) noise\t=\ttf.random_normal(tf.shape(hidden3_gamma),\tdtype=tf.float32) hidden3\t=\thidden3_mean\t+\ttf.exp(0.5\t*\thidden3_gamma)\t*\tnoise hidden4\t=\tmy_dense_layer(hidden3,\tn_hidden4) hidden5\t=\tmy_dense_layer(hidden4,\tn_hidden5) logits\t=\tmy_dense_layer(hidden5,\tn_outputs,\tactivation=None) outputs\t=\ttf.sigmoid(logits)\n\nxentropy\t=\ttf.nn.sigmoid_cross_entropy_with_logits(labels=X,\tlogits=logits) reconstruction_loss\t=\ttf.reduce_sum(xentropy) latent_loss\t=\t0.5\t*\ttf.reduce_sum( \t\t\t\ttf.exp(hidden3_gamma)\t+\ttf.square(hidden3_mean)\t-\t1\t-\thidden3_gamma) loss\t=\treconstruction_loss\t+\tlatent_loss\n\noptimizer\t=\ttf.train.AdamOptimizer(learning_rate=learning_rate) training_op\t=\toptimizer.minimize(loss)\n\ninit\t=\ttf.global_variables_initializer() saver\t=\ttf.train.Saver()",
      "content_length": 1697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 530,
      "content": "Generating\tDigits Now\tlet’s\tuse\tthis\tvariational\tautoencoder\tto\tgenerate\timages\tthat\tlook\tlike\thandwritten\tdigits.\tAll\twe need\tto\tdo\tis\ttrain\tthe\tmodel,\tthen\tsample\trandom\tcodings\tfrom\ta\tGaussian\tdistribution\tand\tdecode\tthem.\n\nimport\tnumpy\tas\tnp\n\nn_digits\t=\t60 n_epochs\t=\t50 batch_size\t=\t150\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\tepoch\tin\trange(n_epochs): \t\t\t\t\t\t\t\tn_batches\t=\tmnist.train.num_examples\t//\tbatch_size \t\t\t\t\t\t\t\tfor\titeration\tin\trange(n_batches): \t\t\t\t\t\t\t\t\t\t\t\tX_batch,\ty_batch\t=\tmnist.train.next_batch(batch_size) \t\t\t\t\t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict={X:\tX_batch})\n\ncodings_rnd\t=\tnp.random.normal(size=[n_digits,\tn_hidden3]) \t\t\t\toutputs_val\t=\toutputs.eval(feed_dict={hidden3:\tcodings_rnd})\n\nThat’s\tit.\tNow\twe\tcan\tsee\twhat\tthe\t“handwritten”\tdigits\tproduced\tby\tthe\tautoencoder\tlook\tlike\t(see Figure\t15-12):\n\nfor\titeration\tin\trange(n_digits): \t\t\t\tplt.subplot(n_digits,\t10,\titeration\t+\t1) \t\t\t\tplot_image(outputs_val[iteration])\n\nFigure\t15-12.\tImages\tof\thandwritten\tdigits\tgenerated\tby\tthe\tvariational\tautoencoder\n\nA\tmajority\tof\tthese\tdigits\tlook\tpretty\tconvincing,\twhile\ta\tfew\tare\trather\t“creative.”\tBut\tdon’t\tbe\ttoo\tharsh on\tthe\tautoencoder\t—\tit\tonly\tstarted\tlearning\tless\tthan\tan\thour\tago.\tGive\tit\ta\tbit\tmore\ttraining\ttime,\tand those\tdigits\twill\tlook\tbetter\tand\tbetter.",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 531,
      "content": "Other\tAutoencoders The\tamazing\tsuccesses\tof\tsupervised\tlearning\tin\timage\trecognition,\tspeech\trecognition,\ttext\ttranslation, and\tmore\thave\tsomewhat\tovershadowed\tunsupervised\tlearning,\tbut\tit\tis\tactually\tbooming.\tNew architectures\tfor\tautoencoders\tand\tother\tunsupervised\tlearning\talgorithms\tare\tinvented\tregularly,\tso\tmuch so\tthat\twe\tcannot\tcover\tthem\tall\tin\tthis\tbook.\tHere\tis\ta\tbrief\t(by\tno\tmeans\texhaustive)\toverview\tof\ta\tfew more\ttypes\tof\tautoencoders\tthat\tyou\tmay\twant\tto\tcheck\tout:\n\nContractive\tautoencoder\t(CAE)8\n\nThe\tautoencoder\tis\tconstrained\tduring\ttraining\tso\tthat\tthe\tderivatives\tof\tthe\tcodings\twith\tregards\tto the\tinputs\tare\tsmall.\tIn\tother\twords,\ttwo\tsimilar\tinputs\tmust\thave\tsimilar\tcodings.\n\nStacked\tconvolutional\tautoencoders9\n\nAutoencoders\tthat\tlearn\tto\textract\tvisual\tfeatures\tby\treconstructing\timages\tprocessed\tthrough convolutional\tlayers.\n\nGenerative\tstochastic\tnetwork\t(GSN)10\n\nA\tgeneralization\tof\tdenoising\tautoencoders,\twith\tthe\tadded\tcapability\tto\tgenerate\tdata.\n\nWinner-take-all\t(WTA)\tautoencoder11\n\nDuring\ttraining,\tafter\tcomputing\tthe\tactivations\tof\tall\tthe\tneurons\tin\tthe\tcoding\tlayer,\tonly\tthe\ttop\tk% activations\tfor\teach\tneuron\tover\tthe\ttraining\tbatch\tare\tpreserved,\tand\tthe\trest\tare\tset\tto\tzero. Naturally\tthis\tleads\tto\tsparse\tcodings.\tMoreover,\ta\tsimilar\tWTA\tapproach\tcan\tbe\tused\tto\tproduce sparse\tconvolutional\tautoencoders.\n\nAdversarial\tautoencoders12\n\nOne\tnetwork\tis\ttrained\tto\treproduce\tits\tinputs,\tand\tat\tthe\tsame\ttime\tanother\tis\ttrained\tto\tfind\tinputs that\tthe\tfirst\tnetwork\tis\tunable\tto\tproperly\treconstruct.\tThis\tpushes\tthe\tfirst\tautoencoder\tto\tlearn robust\tcodings.",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 532,
      "content": "Exercises\n\n1.\t What\tare\tthe\tmain\ttasks\tthat\tautoencoders\tare\tused\tfor?\n\n2.\t Suppose\tyou\twant\tto\ttrain\ta\tclassifier\tand\tyou\thave\tplenty\tof\tunlabeled\ttraining\tdata,\tbut\tonly\ta\tfew thousand\tlabeled\tinstances.\tHow\tcan\tautoencoders\thelp?\tHow\twould\tyou\tproceed?\n\n3.\t If\tan\tautoencoder\tperfectly\treconstructs\tthe\tinputs,\tis\tit\tnecessarily\ta\tgood\tautoencoder?\tHow\tcan you\tevaluate\tthe\tperformance\tof\tan\tautoencoder?\n\n4.\t What\tare\tundercomplete\tand\tovercomplete\tautoencoders?\tWhat\tis\tthe\tmain\trisk\tof\tan\texcessively undercomplete\tautoencoder?\tWhat\tabout\tthe\tmain\trisk\tof\tan\tovercomplete\tautoencoder?\n\n5.\t How\tdo\tyou\ttie\tweights\tin\ta\tstacked\tautoencoder?\tWhat\tis\tthe\tpoint\tof\tdoing\tso?\n\n6.\t What\tis\ta\tcommon\ttechnique\tto\tvisualize\tfeatures\tlearned\tby\tthe\tlower\tlayer\tof\ta\tstacked autoencoder?\tWhat\tabout\thigher\tlayers?\n\n7.\t What\tis\ta\tgenerative\tmodel?\tCan\tyou\tname\ta\ttype\tof\tgenerative\tautoencoder?\n\n8.\t Let’s\tuse\ta\tdenoising\tautoencoder\tto\tpretrain\tan\timage\tclassifier:\n\nYou\tcan\tuse\tMNIST\t(simplest),\tor\tanother\tlarge\tset\tof\timages\tsuch\tas\tCIFAR10\tif\tyou\twant\ta bigger\tchallenge.\tIf\tyou\tchoose\tCIFAR10,\tyou\tneed\tto\twrite\tcode\tto\tload\tbatches\tof\timages\tfor training.\tIf\tyou\twant\tto\tskip\tthis\tpart,\tTensorFlow’s\tmodel\tzoo\tcontains\ttools\tto\tdo\tjust\tthat.\n\nSplit\tthe\tdataset\tinto\ta\ttraining\tset\tand\ta\ttest\tset.\tTrain\ta\tdeep\tdenoising\tautoencoder\ton\tthe\tfull training\tset.\n\nCheck\tthat\tthe\timages\tare\tfairly\twell\treconstructed,\tand\tvisualize\tthe\tlow-level\tfeatures. Visualize\tthe\timages\tthat\tmost\tactivate\teach\tneuron\tin\tthe\tcoding\tlayer.\n\nBuild\ta\tclassification\tdeep\tneural\tnetwork,\treusing\tthe\tlower\tlayers\tof\tthe\tautoencoder.\tTrain\tit using\tonly\t10%\tof\tthe\ttraining\tset.\tCan\tyou\tget\tit\tto\tperform\tas\twell\tas\tthe\tsame\tclassifier trained\ton\tthe\tfull\ttraining\tset?\n\n9.\t Semantic\thashing,\tintroduced\tin\t2008\tby\tRuslan\tSalakhutdinov\tand\tGeoffrey\tHinton,13\tis\ta technique\tused\tfor\tefficient\tinformation\tretrieval:\ta\tdocument\t(e.g.,\tan\timage)\tis\tpassed\tthrough\ta system,\ttypically\ta\tneural\tnetwork,\twhich\toutputs\ta\tfairly\tlow-dimensional\tbinary\tvector\t(e.g.,\t30 bits).\tTwo\tsimilar\tdocuments\tare\tlikely\tto\thave\tidentical\tor\tvery\tsimilar\thashes.\tBy\tindexing\teach document\tusing\tits\thash,\tit\tis\tpossible\tto\tretrieve\tmany\tdocuments\tsimilar\tto\ta\tparticular\tdocument almost\tinstantly,\teven\tif\tthere\tare\tbillions\tof\tdocuments:\tjust\tcompute\tthe\thash\tof\tthe\tdocument\tand look\tup\tall\tdocuments\twith\tthat\tsame\thash\t(or\thashes\tdiffering\tby\tjust\tone\tor\ttwo\tbits).\tLet’s implement\tsemantic\thashing\tusing\ta\tslightly\ttweaked\tstacked\tautoencoder:\n\nCreate\ta\tstacked\tautoencoder\tcontaining\ttwo\thidden\tlayers\tbelow\tthe\tcoding\tlayer,\tand\ttrain\tit on\tthe\timage\tdataset\tyou\tused\tin\tthe\tprevious\texercise.\tThe\tcoding\tlayer\tshould\tcontain\t30 neurons\tand\tuse\tthe\tlogistic\tactivation\tfunction\tto\toutput\tvalues\tbetween\t0\tand\t1.\tAfter\ttraining,",
      "content_length": 2796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 533,
      "content": "to\tproduce\tthe\thash\tof\tan\timage,\tyou\tcan\tsimply\trun\tit\tthrough\tthe\tautoencoder,\ttake\tthe\toutput of\tthe\tcoding\tlayer,\tand\tround\tevery\tvalue\tto\tthe\tclosest\tinteger\t(0\tor\t1).\n\nOne\tneat\ttrick\tproposed\tby\tSalakhutdinov\tand\tHinton\tis\tto\tadd\tGaussian\tnoise\t(with\tzero mean)\tto\tthe\tinputs\tof\tthe\tcoding\tlayer,\tduring\ttraining\tonly.\tIn\torder\tto\tpreserve\ta\thigh\tsignal- to-noise\tratio,\tthe\tautoencoder\twill\tlearn\tto\tfeed\tlarge\tvalues\tto\tthe\tcoding\tlayer\t(so\tthat\tthe noise\tbecomes\tnegligible).\tIn\tturn,\tthis\tmeans\tthat\tthe\tlogistic\tfunction\tof\tthe\tcoding\tlayer\twill likely\tsaturate\tat\t0\tor\t1.\tAs\ta\tresult,\trounding\tthe\tcodings\tto\t0\tor\t1\twon’t\tdistort\tthem\ttoo\tmuch, and\tthis\twill\timprove\tthe\treliability\tof\tthe\thashes.\n\nCompute\tthe\thash\tof\tevery\timage,\tand\tsee\tif\timages\twith\tidentical\thashes\tlook\talike.\tSince MNIST\tand\tCIFAR10\tare\tlabeled,\ta\tmore\tobjective\tway\tto\tmeasure\tthe\tperformance\tof\tthe autoencoder\tfor\tsemantic\thashing\tis\tto\tensure\tthat\timages\twith\tthe\tsame\thash\tgenerally\thave\tthe same\tclass.\tOne\tway\tto\tdo\tthis\tis\tto\tmeasure\tthe\taverage\tGini\tpurity\t(introduced\tin\tChapter\t6) of\tthe\tsets\tof\timages\twith\tidentical\t(or\tvery\tsimilar)\thashes.\n\nTry\tfine-tuning\tthe\thyperparameters\tusing\tcross-validation.\n\nNote\tthat\twith\ta\tlabeled\tdataset,\tanother\tapproach\tis\tto\ttrain\ta\tconvolutional\tneural\tnetwork (see\tChapter\t13)\tfor\tclassification,\tthen\tuse\tthe\tlayer\tbelow\tthe\toutput\tlayer\tto\tproduce\tthe hashes.\tSee\tJinma\tGua\tand\tJianmin\tLi’s\t2015\tpaper.14\tSee\tif\tthat\tperforms\tbetter.\n\n10.\t Train\ta\tvariational\tautoencoder\ton\tthe\timage\tdataset\tused\tin\tthe\tprevious\texercises\t(MNIST\tor CIFAR10),\tand\tmake\tit\tgenerate\timages.\tAlternatively,\tyou\tcan\ttry\tto\tfind\tan\tunlabeled\tdataset\tthat you\tare\tinterested\tin\tand\tsee\tif\tyou\tcan\tgenerate\tnew\tsamples.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.\n\n1\n\n“Perception\tin\tchess,”\tW.\tChase\tand\tH.\tSimon\t(1973).\n\n2\n\n“Greedy\tLayer-Wise\tTraining\tof\tDeep\tNetworks,”\tY.\tBengio\tet\tal.\t(2007).\n\n3\n\n“Extracting\tand\tComposing\tRobust\tFeatures\twith\tDenoising\tAutoencoders,”\tP.\tVincent\tet\tal.\t(2008).\n\n4\n\n“Stacked\tDenoising\tAutoencoders:\tLearning\tUseful\tRepresentations\tin\ta\tDeep\tNetwork\twith\ta\tLocal\tDenoising\tCriterion,”\tP.\tVincent\tet al.\t(2010).\n\n5\n\n“Auto-Encoding\tVariational\tBayes,”\tD.\tKingma\tand\tM.\tWelling\t(2014).\n\n6\n\nVariational\tautoencoders\tare\tactually\tmore\tgeneral;\tthe\tcodings\tare\tnot\tlimited\tto\tGaussian\tdistributions.\n\n7\n\nFor\tmore\tmathematical\tdetails,\tcheck\tout\tthe\toriginal\tpaper\ton\tvariational\tautoencoders,\tor\tCarl\tDoersch’s\tgreat\ttutorial\t(2016).\n\n8\n\n“Contractive\tAuto-Encoders:\tExplicit\tInvariance\tDuring\tFeature\tExtraction,”\tS.\tRifai\tet\tal.\t(2011).\n\n9\n\n“Stacked\tConvolutional\tAuto-Encoders\tfor\tHierarchical\tFeature\tExtraction,”\tJ.\tMasci\tet\tal.\t(2011).\n\n10\n\n“GSNs:\tGenerative\tStochastic\tNetworks,”\tG.\tAlain\tet\tal.\t(2015).\n\n11\n\n“Winner-Take-All\tAutoencoders,”\tA.\tMakhzani\tand\tB.\tFrey\t(2015).\n\n12\n\n“Adversarial\tAutoencoders,”\tA.\tMakhzani\tet\tal.\t(2016).\n\n13\n\n“Semantic\tHashing,”\tR.\tSalakhutdinov\tand\tG.\tHinton\t(2008).\n\n14\n\n“CNN\tBased\tHashing\tfor\tImage\tRetrieval,”\tJ.\tGua\tand\tJ.\tLi\t(2015).",
      "content_length": 3034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 534,
      "content": "Chapter\t16.\tReinforcement\tLearning\n\nReinforcement\tLearning\t(RL)\tis\tone\tof\tthe\tmost\texciting\tfields\tof\tMachine\tLearning\ttoday,\tand\talso\tone of\tthe\toldest.\tIt\thas\tbeen\taround\tsince\tthe\t1950s,\tproducing\tmany\tinteresting\tapplications\tover\tthe\tyears,1 in\tparticular\tin\tgames\t(e.g.,\tTD-Gammon,\ta\tBackgammon\tplaying\tprogram)\tand\tin\tmachine\tcontrol,\tbut seldom\tmaking\tthe\theadline\tnews.\tBut\ta\trevolution\ttook\tplace\tin\t2013\twhen\tresearchers\tfrom\tan\tEnglish startup\tcalled\tDeepMind\tdemonstrated\ta\tsystem\tthat\tcould\tlearn\tto\tplay\tjust\tabout\tany\tAtari\tgame\tfrom scratch,2\teventually\toutperforming\thumans3\tin\tmost\tof\tthem,\tusing\tonly\traw\tpixels\tas\tinputs\tand\twithout any\tprior\tknowledge\tof\tthe\trules\tof\tthe\tgames.4\tThis\twas\tthe\tfirst\tof\ta\tseries\tof\tamazing\tfeats,\tculminating in\tMarch\t2016\twith\tthe\tvictory\tof\ttheir\tsystem\tAlphaGo\tagainst\tLee\tSedol,\tthe\tworld\tchampion\tof\tthe game\tof\tGo.\tNo\tprogram\thad\tever\tcome\tclose\tto\tbeating\ta\tmaster\tof\tthis\tgame,\tlet\talone\tthe\tworld champion.\tToday\tthe\twhole\tfield\tof\tRL\tis\tboiling\twith\tnew\tideas,\twith\ta\twide\trange\tof\tapplications. DeepMind\twas\tbought\tby\tGoogle\tfor\tover\t500\tmillion\tdollars\tin\t2014.\n\nSo\thow\tdid\tthey\tdo\tit?\tWith\thindsight\tit\tseems\trather\tsimple:\tthey\tapplied\tthe\tpower\tof\tDeep\tLearning\tto the\tfield\tof\tReinforcement\tLearning,\tand\tit\tworked\tbeyond\ttheir\twildest\tdreams.\tIn\tthis\tchapter\twe\twill first\texplain\twhat\tReinforcement\tLearning\tis\tand\twhat\tit\tis\tgood\tat,\tand\tthen\twe\twill\tpresent\ttwo\tof\tthe most\timportant\ttechniques\tin\tdeep\tReinforcement\tLearning:\tpolicy\tgradients\tand\tdeep\tQ-networks (DQN),\tincluding\ta\tdiscussion\tof\tMarkov\tdecision\tprocesses\t(MDP).\tWe\twill\tuse\tthese\ttechniques\tto train\ta\tmodel\tto\tbalance\ta\tpole\ton\ta\tmoving\tcart,\tand\tanother\tto\tplay\tAtari\tgames.\tThe\tsame\ttechniques can\tbe\tused\tfor\ta\twide\tvariety\tof\ttasks,\tfrom\twalking\trobots\tto\tself-driving\tcars.",
      "content_length": 1821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 535,
      "content": "Learning\tto\tOptimize\tRewards In\tReinforcement\tLearning,\ta\tsoftware\tagent\tmakes\tobservations\tand\ttakes\tactions\twithin\tan environment,\tand\tin\treturn\tit\treceives\trewards.\tIts\tobjective\tis\tto\tlearn\tto\tact\tin\ta\tway\tthat\twill\tmaximize its\texpected\tlong-term\trewards.\tIf\tyou\tdon’t\tmind\ta\tbit\tof\tanthropomorphism,\tyou\tcan\tthink\tof\tpositive rewards\tas\tpleasure,\tand\tnegative\trewards\tas\tpain\t(the\tterm\t“reward”\tis\ta\tbit\tmisleading\tin\tthis\tcase).\tIn short,\tthe\tagent\tacts\tin\tthe\tenvironment\tand\tlearns\tby\ttrial\tand\terror\tto\tmaximize\tits\tpleasure\tand\tminimize its\tpain.\n\nThis\tis\tquite\ta\tbroad\tsetting,\twhich\tcan\tapply\tto\ta\twide\tvariety\tof\ttasks.\tHere\tare\ta\tfew\texamples\t(see Figure\t16-1):\n\n1.\t The\tagent\tcan\tbe\tthe\tprogram\tcontrolling\ta\twalking\trobot.\tIn\tthis\tcase,\tthe\tenvironment\tis\tthe\treal world,\tthe\tagent\tobserves\tthe\tenvironment\tthrough\ta\tset\tof\tsensors\tsuch\tas\tcameras\tand\ttouch sensors,\tand\tits\tactions\tconsist\tof\tsending\tsignals\tto\tactivate\tmotors.\tIt\tmay\tbe\tprogrammed\tto\tget positive\trewards\twhenever\tit\tapproaches\tthe\ttarget\tdestination,\tand\tnegative\trewards\twhenever\tit wastes\ttime,\tgoes\tin\tthe\twrong\tdirection,\tor\tfalls\tdown.\n\n2.\t The\tagent\tcan\tbe\tthe\tprogram\tcontrolling\tMs.\tPac-Man.\tIn\tthis\tcase,\tthe\tenvironment\tis\ta\tsimulation of\tthe\tAtari\tgame,\tthe\tactions\tare\tthe\tnine\tpossible\tjoystick\tpositions\t(upper\tleft,\tdown,\tcenter,\tand so\ton),\tthe\tobservations\tare\tscreenshots,\tand\tthe\trewards\tare\tjust\tthe\tgame\tpoints.\n\n3.\t Similarly,\tthe\tagent\tcan\tbe\tthe\tprogram\tplaying\ta\tboard\tgame\tsuch\tas\tthe\tgame\tof\tGo.\n\n4.\t The\tagent\tdoes\tnot\thave\tto\tcontrol\ta\tphysically\t(or\tvirtually)\tmoving\tthing.\tFor\texample,\tit\tcan\tbe\ta smart\tthermostat,\tgetting\trewards\twhenever\tit\tis\tclose\tto\tthe\ttarget\ttemperature\tand\tsaves\tenergy,\tand negative\trewards\twhen\thumans\tneed\tto\ttweak\tthe\ttemperature,\tso\tthe\tagent\tmust\tlearn\tto\tanticipate human\tneeds.\n\n5.\t The\tagent\tcan\tobserve\tstock\tmarket\tprices\tand\tdecide\thow\tmuch\tto\tbuy\tor\tsell\tevery\tsecond. Rewards\tare\tobviously\tthe\tmonetary\tgains\tand\tlosses.",
      "content_length": 1980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 536,
      "content": "Figure\t16-1.\tReinforcement\tLearning\texamples:\t(a)\twalking\trobot,\t(b)\tMs.\tPac-Man,\t(c)\tGo\tplayer,\t(d)\tthermostat,\t(e)\tautomatic trader5\n\nNote\tthat\tthere\tmay\tnot\tbe\tany\tpositive\trewards\tat\tall;\tfor\texample,\tthe\tagent\tmay\tmove\taround\tin\ta\tmaze, getting\ta\tnegative\treward\tat\tevery\ttime\tstep,\tso\tit\tbetter\tfind\tthe\texit\tas\tquickly\tas\tpossible!\tThere\tare many\tother\texamples\tof\ttasks\twhere\tReinforcement\tLearning\tis\twell\tsuited,\tsuch\tas\tself-driving\tcars, placing\tads\ton\ta\tweb\tpage,\tor\tcontrolling\twhere\tan\timage\tclassification\tsystem\tshould\tfocus\tits\tattention.",
      "content_length": 556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 537,
      "content": "Policy\tSearch The\talgorithm\tused\tby\tthe\tsoftware\tagent\tto\tdetermine\tits\tactions\tis\tcalled\tits\tpolicy.\tFor\texample,\tthe policy\tcould\tbe\ta\tneural\tnetwork\ttaking\tobservations\tas\tinputs\tand\toutputting\tthe\taction\tto\ttake\t(see Figure\t16-2).\n\nFigure\t16-2.\tReinforcement\tLearning\tusing\ta\tneural\tnetwork\tpolicy\n\nThe\tpolicy\tcan\tbe\tany\talgorithm\tyou\tcan\tthink\tof,\tand\tit\tdoes\tnot\teven\thave\tto\tbe\tdeterministic.\tFor example,\tconsider\ta\trobotic\tvacuum\tcleaner\twhose\treward\tis\tthe\tamount\tof\tdust\tit\tpicks\tup\tin\t30\tminutes. Its\tpolicy\tcould\tbe\tto\tmove\tforward\twith\tsome\tprobability\tp\tevery\tsecond,\tor\trandomly\trotate\tleft\tor right\twith\tprobability\t1\t–\tp.\tThe\trotation\tangle\twould\tbe\ta\trandom\tangle\tbetween\t–r\tand\t+r.\tSince\tthis policy\tinvolves\tsome\trandomness,\tit\tis\tcalled\ta\tstochastic\tpolicy.\tThe\trobot\twill\thave\tan\terratic trajectory,\twhich\tguarantees\tthat\tit\twill\teventually\tget\tto\tany\tplace\tit\tcan\treach\tand\tpick\tup\tall\tthe\tdust. The\tquestion\tis:\thow\tmuch\tdust\twill\tit\tpick\tup\tin\t30\tminutes?\n\nHow\twould\tyou\ttrain\tsuch\ta\trobot?\tThere\tare\tjust\ttwo\tpolicy\tparameters\tyou\tcan\ttweak:\tthe\tprobability p\tand\tthe\tangle\trange\tr.\tOne\tpossible\tlearning\talgorithm\tcould\tbe\tto\ttry\tout\tmany\tdifferent\tvalues\tfor these\tparameters,\tand\tpick\tthe\tcombination\tthat\tperforms\tbest\t(see\tFigure\t16-3).\tThis\tis\tan\texample\tof policy\tsearch,\tin\tthis\tcase\tusing\ta\tbrute\tforce\tapproach.\tHowever,\twhen\tthe\tpolicy\tspace\tis\ttoo\tlarge (which\tis\tgenerally\tthe\tcase),\tfinding\ta\tgood\tset\tof\tparameters\tthis\tway\tis\tlike\tsearching\tfor\ta\tneedle\tin\ta gigantic\thaystack.\n\nAnother\tway\tto\texplore\tthe\tpolicy\tspace\tis\tto\tuse\tgenetic\talgorithms.\tFor\texample,\tyou\tcould\trandomly create\ta\tfirst\tgeneration\tof\t100\tpolicies\tand\ttry\tthem\tout,\tthen\t“kill”\tthe\t80\tworst\tpolicies6\tand\tmake\tthe 20\tsurvivors\tproduce\t4\toffspring\teach.\tAn\toffspring\tis\tjust\ta\tcopy\tof\tits\tparent7\tplus\tsome\trandom variation.\tThe\tsurviving\tpolicies\tplus\ttheir\toffspring\ttogether\tconstitute\tthe\tsecond\tgeneration.\tYou\tcan continue\tto\titerate\tthrough\tgenerations\tthis\tway,\tuntil\tyou\tfind\ta\tgood\tpolicy.",
      "content_length": 2016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 538,
      "content": "Figure\t16-3.\tFour\tpoints\tin\tpolicy\tspace\tand\tthe\tagent’s\tcorresponding\tbehavior\n\nYet\tanother\tapproach\tis\tto\tuse\toptimization\ttechniques,\tby\tevaluating\tthe\tgradients\tof\tthe\trewards\twith regards\tto\tthe\tpolicy\tparameters,\tthen\ttweaking\tthese\tparameters\tby\tfollowing\tthe\tgradient\ttoward\thigher rewards\t(gradient\tascent).\tThis\tapproach\tis\tcalled\tpolicy\tgradients\t(PG),\twhich\twe\twill\tdiscuss\tin more\tdetail\tlater\tin\tthis\tchapter.\tFor\texample,\tgoing\tback\tto\tthe\tvacuum\tcleaner\trobot,\tyou\tcould\tslightly increase\tp\tand\tevaluate\twhether\tthis\tincreases\tthe\tamount\tof\tdust\tpicked\tup\tby\tthe\trobot\tin\t30\tminutes;\tif it\tdoes,\tthen\tincrease\tp\tsome\tmore,\tor\telse\treduce\tp.\tWe\twill\timplement\ta\tpopular\tPG\talgorithm\tusing TensorFlow,\tbut\tbefore\twe\tdo\twe\tneed\tto\tcreate\tan\tenvironment\tfor\tthe\tagent\tto\tlive\tin,\tso\tit’s\ttime\tto introduce\tOpenAI\tgym.",
      "content_length": 829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 539,
      "content": "Introduction\tto\tOpenAI\tGym One\tof\tthe\tchallenges\tof\tReinforcement\tLearning\tis\tthat\tin\torder\tto\ttrain\tan\tagent,\tyou\tfirst\tneed\tto\thave\ta working\tenvironment.\tIf\tyou\twant\tto\tprogram\tan\tagent\tthat\twill\tlearn\tto\tplay\tan\tAtari\tgame,\tyou\twill\tneed an\tAtari\tgame\tsimulator.\tIf\tyou\twant\tto\tprogram\ta\twalking\trobot,\tthen\tthe\tenvironment\tis\tthe\treal\tworld and\tyou\tcan\tdirectly\ttrain\tyour\trobot\tin\tthat\tenvironment,\tbut\tthis\thas\tits\tlimits:\tif\tthe\trobot\tfalls\toff\ta\tcliff, you\tcan’t\tjust\tclick\t“undo.”\tYou\tcan’t\tspeed\tup\ttime\teither;\tadding\tmore\tcomputing\tpower\twon’t\tmake\tthe robot\tmove\tany\tfaster.\tAnd\tit’s\tgenerally\ttoo\texpensive\tto\ttrain\t1,000\trobots\tin\tparallel.\tIn\tshort,\ttraining is\thard\tand\tslow\tin\tthe\treal\tworld,\tso\tyou\tgenerally\tneed\ta\tsimulated\tenvironment\tat\tleast\tto\tbootstrap training. OpenAI\tgym8\tis\ta\ttoolkit\tthat\tprovides\ta\twide\tvariety\tof\tsimulated\tenvironments\t(Atari\tgames,\tboard games,\t2D\tand\t3D\tphysical\tsimulations,\tand\tso\ton),\tso\tyou\tcan\ttrain\tagents,\tcompare\tthem,\tor\tdevelop new\tRL\talgorithms.\n\nLet’s\tinstall\tOpenAI\tgym.\tFor\ta\tminimal\tOpenAI\tgym\tinstallation,\tsimply\tuse\tpip:\n\n$\tpip3\tinstall\t--upgrade\tgym\n\nNext\topen\tup\ta\tPython\tshell\tor\ta\tJupyter\tnotebook\tand\tcreate\tyour\tfirst\tenvironment:\n\n>>>\timport\tgym >>>\tenv\t=\tgym.make(\"CartPole-v0\") [2016-10-14\t16:03:23,199]\tMaking\tnew\tenv:\tMsPacman-v0 >>>\tobs\t=\tenv.reset() >>>\tobs array([-0.03799846,\t-0.03288115,\t\t0.02337094,\t\t0.00720711]) >>>\tenv.render()\n\nThe\tmake()\tfunction\tcreates\tan\tenvironment,\tin\tthis\tcase\ta\tCartPole\tenvironment.\tThis\tis\ta\t2D\tsimulation in\twhich\ta\tcart\tcan\tbe\taccelerated\tleft\tor\tright\tin\torder\tto\tbalance\ta\tpole\tplaced\ton\ttop\tof\tit\t(see Figure\t16-4).\tAfter\tthe\tenvironment\tis\tcreated,\twe\tmust\tinitialize\tit\tusing\tthe\treset()\tmethod.\tThis returns\tthe\tfirst\tobservation.\tObservations\tdepend\ton\tthe\ttype\tof\tenvironment.\tFor\tthe\tCartPole environment,\teach\tobservation\tis\ta\t1D\tNumPy\tarray\tcontaining\tfour\tfloats:\tthese\tfloats\trepresent\tthe cart’s\thorizontal\tposition\t(0.0\t=\tcenter),\tits\tvelocity,\tthe\tangle\tof\tthe\tpole\t(0.0\t=\tvertical),\tand\tits angular\tvelocity.\tFinally,\tthe\trender()\tmethod\tdisplays\tthe\tenvironment\tas\tshown\tin\tFigure\t16-4.",
      "content_length": 2125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 540,
      "content": "Figure\t16-4.\tThe\tCartPole\tenvironment\n\nIf\tyou\twant\trender()\tto\treturn\tthe\trendered\timage\tas\ta\tNumPy\tarray,\tyou\tcan\tset\tthe\tmode\tparameter\tto rgb_array\t(note\tthat\tother\tenvironments\tmay\tsupport\tdifferent\tmodes):\n\n>>>\timg\t=\tenv.render(mode=\"rgb_array\") >>>\timg.shape\t\t#\theight,\twidth,\tchannels\t(3=RGB) (400,\t600,\t3)\n\nTIP\n\nUnfortunately,\tthe\tCartPole\t(and\ta\tfew\tother\tenvironments)\trenders\tthe\timage\tto\tthe\tscreen\teven\tif\tyou\tset\tthe\tmode\tto \"rgb_array\".\tThe\tonly\tway\tto\tavoid\tthis\tis\tto\tuse\ta\tfake\tX\tserver\tsuch\tas\tXvfb\tor\tXdummy.\tFor\texample,\tyou\tcan\tinstall\tXvfb and\tstart\tPython\tusing\tthe\tfollowing\tcommand:\txvfb-run\t-s\t\"-screen\t0\t1400x900x24\"\tpython.\tOr\tuse\tthe\txvfbwrapper package.\n\nLet’s\task\tthe\tenvironment\twhat\tactions\tare\tpossible:\n\n>>>\tenv.action_space Discrete(2)\n\nDiscrete(2)\tmeans\tthat\tthe\tpossible\tactions\tare\tintegers\t0\tand\t1,\twhich\trepresent\taccelerating\tleft\t(0) or\tright\t(1).\tOther\tenvironments\tmay\thave\tmore\tdiscrete\tactions,\tor\tother\tkinds\tof\tactions\t(e.g., continuous).\tSince\tthe\tpole\tis\tleaning\ttoward\tthe\tright,\tlet’s\taccelerate\tthe\tcart\ttoward\tthe\tright:\n\n>>>\taction\t=\t1\t\t#\taccelerate\tright >>>\tobs,\treward,\tdone,\tinfo\t=\tenv.step(action) >>>\tobs array([-0.03865608,\t\t0.16189797,\t\t0.02351508,\t-0.27801135]) >>>\treward 1.0 >>>\tdone False >>>\tinfo",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 541,
      "content": "{}\n\nThe\tstep()\tmethod\texecutes\tthe\tgiven\taction\tand\treturns\tfour\tvalues:\n\nobs\n\nThis\tis\tthe\tnew\tobservation.\tThe\tcart\tis\tnow\tmoving\ttoward\tthe\tright\t(obs[1]>0).\tThe\tpole\tis\tstill tilted\ttoward\tthe\tright\t(obs[2]>0),\tbut\tits\tangular\tvelocity\tis\tnow\tnegative\t(obs[3]<0),\tso\tit\twill likely\tbe\ttilted\ttoward\tthe\tleft\tafter\tthe\tnext\tstep.\n\nreward\n\nIn\tthis\tenvironment,\tyou\tget\ta\treward\tof\t1.0\tat\tevery\tstep,\tno\tmatter\twhat\tyou\tdo,\tso\tthe\tgoal\tis\tto keep\trunning\tas\tlong\tas\tpossible.\n\ndone\n\nThis\tvalue\twill\tbe\tTrue\twhen\tthe\tepisode\tis\tover.\tThis\twill\thappen\twhen\tthe\tpole\ttilts\ttoo\tmuch. After\tthat,\tthe\tenvironment\tmust\tbe\treset\tbefore\tit\tcan\tbe\tused\tagain.\n\ninfo\n\nThis\tdictionary\tmay\tprovide\textra\tdebug\tinformation\tin\tother\tenvironments.\tThis\tdata\tshould\tnot\tbe used\tfor\ttraining\t(it\twould\tbe\tcheating).\n\nLet’s\thardcode\ta\tsimple\tpolicy\tthat\taccelerates\tleft\twhen\tthe\tpole\tis\tleaning\ttoward\tthe\tleft\tand accelerates\tright\twhen\tthe\tpole\tis\tleaning\ttoward\tthe\tright.\tWe\twill\trun\tthis\tpolicy\tto\tsee\tthe\taverage rewards\tit\tgets\tover\t500\tepisodes:\n\ndef\tbasic_policy(obs): \t\t\t\tangle\t=\tobs[2] \t\t\t\treturn\t0\tif\tangle\t<\t0\telse\t1\n\ntotals\t=\t[] for\tepisode\tin\trange(500): \t\t\t\tepisode_rewards\t=\t0 \t\t\t\tobs\t=\tenv.reset() \t\t\t\tfor\tstep\tin\trange(1000):\t#\t1000\tsteps\tmax,\twe\tdon't\twant\tto\trun\tforever \t\t\t\t\t\t\t\taction\t=\tbasic_policy(obs) \t\t\t\t\t\t\t\tobs,\treward,\tdone,\tinfo\t=\tenv.step(action) \t\t\t\t\t\t\t\tepisode_rewards\t+=\treward \t\t\t\t\t\t\t\tif\tdone: \t\t\t\t\t\t\t\t\t\t\t\tbreak \t\t\t\ttotals.append(episode_rewards)\n\nThis\tcode\tis\thopefully\tself-explanatory.\tLet’s\tlook\tat\tthe\tresult:\n\n>>>\timport\tnumpy\tas\tnp >>>\tnp.mean(totals),\tnp.std(totals),\tnp.min(totals),\tnp.max(totals) (42.125999999999998,\t9.1237121830974033,\t24.0,\t68.0)\n\nEven\twith\t500\ttries,\tthis\tpolicy\tnever\tmanaged\tto\tkeep\tthe\tpole\tupright\tfor\tmore\tthan\t68\tconsecutive steps.\tNot\tgreat.\tIf\tyou\tlook\tat\tthe\tsimulation\tin\tthe\tJupyter\tnotebooks,\tyou\twill\tsee\tthat\tthe\tcart\toscillates left\tand\tright\tmore\tand\tmore\tstrongly\tuntil\tthe\tpole\ttilts\ttoo\tmuch.\tLet’s\tsee\tif\ta\tneural\tnetwork\tcan\tcome up\twith\ta\tbetter\tpolicy.",
      "content_length": 2024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 542,
      "content": "Neural\tNetwork\tPolicies Let’s\tcreate\ta\tneural\tnetwork\tpolicy.\tJust\tlike\tthe\tpolicy\twe\thardcoded\tearlier,\tthis\tneural\tnetwork\twill take\tan\tobservation\tas\tinput,\tand\tit\twill\toutput\tthe\taction\tto\tbe\texecuted.\tMore\tprecisely,\tit\twill\testimate\ta probability\tfor\teach\taction,\tand\tthen\twe\twill\tselect\tan\taction\trandomly\taccording\tto\tthe\testimated probabilities\t(see\tFigure\t16-5).\tIn\tthe\tcase\tof\tthe\tCartPole\tenvironment,\tthere\tare\tjust\ttwo\tpossible actions\t(left\tor\tright),\tso\twe\tonly\tneed\tone\toutput\tneuron.\tIt\twill\toutput\tthe\tprobability\tp\tof\taction\t0\t(left), and\tof\tcourse\tthe\tprobability\tof\taction\t1\t(right)\twill\tbe\t1\t–\tp.\tFor\texample,\tif\tit\toutputs\t0.7,\tthen\twe\twill pick\taction\t0\twith\t70%\tprobability,\tand\taction\t1\twith\t30%\tprobability.\n\nFigure\t16-5.\tNeural\tnetwork\tpolicy\n\nYou\tmay\twonder\twhy\twe\tare\tpicking\ta\trandom\taction\tbased\ton\tthe\tprobability\tgiven\tby\tthe\tneural network,\trather\tthan\tjust\tpicking\tthe\taction\twith\tthe\thighest\tscore.\tThis\tapproach\tlets\tthe\tagent\tfind\tthe right\tbalance\tbetween\texploring\tnew\tactions\tand\texploiting\tthe\tactions\tthat\tare\tknown\tto\twork\twell. Here’s\tan\tanalogy:\tsuppose\tyou\tgo\tto\ta\trestaurant\tfor\tthe\tfirst\ttime,\tand\tall\tthe\tdishes\tlook\tequally appealing\tso\tyou\trandomly\tpick\tone.\tIf\tit\tturns\tout\tto\tbe\tgood,\tyou\tcan\tincrease\tthe\tprobability\tto\torder\tit next\ttime,\tbut\tyou\tshouldn’t\tincrease\tthat\tprobability\tup\tto\t100%,\tor\telse\tyou\twill\tnever\ttry\tout\tthe\tother dishes,\tsome\tof\twhich\tmay\tbe\teven\tbetter\tthan\tthe\tone\tyou\ttried.\n\nAlso\tnote\tthat\tin\tthis\tparticular\tenvironment,\tthe\tpast\tactions\tand\tobservations\tcan\tsafely\tbe\tignored,",
      "content_length": 1563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 543,
      "content": "since\teach\tobservation\tcontains\tthe\tenvironment’s\tfull\tstate.\tIf\tthere\twere\tsome\thidden\tstate,\tthen\tyou\tmay need\tto\tconsider\tpast\tactions\tand\tobservations\tas\twell.\tFor\texample,\tif\tthe\tenvironment\tonly\trevealed\tthe position\tof\tthe\tcart\tbut\tnot\tits\tvelocity,\tyou\twould\thave\tto\tconsider\tnot\tonly\tthe\tcurrent\tobservation\tbut also\tthe\tprevious\tobservation\tin\torder\tto\testimate\tthe\tcurrent\tvelocity.\tAnother\texample\tis\twhen\tthe observations\tare\tnoisy;\tin\tthat\tcase,\tyou\tgenerally\twant\tto\tuse\tthe\tpast\tfew\tobservations\tto\testimate\tthe most\tlikely\tcurrent\tstate.\tThe\tCartPole\tproblem\tis\tthus\tas\tsimple\tas\tcan\tbe;\tthe\tobservations\tare\tnoise- free\tand\tthey\tcontain\tthe\tenvironment’s\tfull\tstate.\n\nHere\tis\tthe\tcode\tto\tbuild\tthis\tneural\tnetwork\tpolicy\tusing\tTensorFlow:\n\nimport\ttensorflow\tas\ttf\n\n#\t1.\tSpecify\tthe\tneural\tnetwork\tarchitecture n_inputs\t=\t4\t\t#\t==\tenv.observation_space.shape[0] n_hidden\t=\t4\t\t#\tit's\ta\tsimple\ttask,\twe\tdon't\tneed\tmore\thidden\tneurons n_outputs\t=\t1\t#\tonly\toutputs\tthe\tprobability\tof\taccelerating\tleft initializer\t=\ttf.contrib.layers.variance_scaling_initializer()\n\n#\t2.\tBuild\tthe\tneural\tnetwork X\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs]) hidden\t=\ttf.layers.dense(X,\tn_hidden,\tactivation=tf.nn.elu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) logits\t=\ttf.layers.dense(hidden,\tn_outputs, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) outputs\t=\ttf.nn.sigmoid(logits)\n\n#\t3.\tSelect\ta\trandom\taction\tbased\ton\tthe\testimated\tprobabilities p_left_and_right\t=\ttf.concat(axis=1,\tvalues=[outputs,\t1\t-\toutputs]) action\t=\ttf.multinomial(tf.log(p_left_and_right),\tnum_samples=1)\n\ninit\t=\ttf.global_variables_initializer()\n\nLet’s\tgo\tthrough\tthis\tcode:\n\n1.\t After\tthe\timports,\twe\tdefine\tthe\tneural\tnetwork\tarchitecture.\tThe\tnumber\tof\tinputs\tis\tthe\tsize\tof\tthe observation\tspace\t(which\tin\tthe\tcase\tof\tthe\tCartPole\tis\tfour),\twe\tjust\thave\tfour\thidden\tunits\tand\tno need\tfor\tmore,\tand\twe\thave\tjust\tone\toutput\tprobability\t(the\tprobability\tof\tgoing\tleft).\n\n2.\t Next\twe\tbuild\tthe\tneural\tnetwork.\tIn\tthis\texample,\tit’s\ta\tvanilla\tMulti-Layer\tPerceptron,\twith\ta single\toutput.\tNote\tthat\tthe\toutput\tlayer\tuses\tthe\tlogistic\t(sigmoid)\tactivation\tfunction\tin\torder\tto output\ta\tprobability\tfrom\t0.0\tto\t1.0.\tIf\tthere\twere\tmore\tthan\ttwo\tpossible\tactions,\tthere\twould\tbe one\toutput\tneuron\tper\taction,\tand\tyou\twould\tuse\tthe\tsoftmax\tactivation\tfunction\tinstead.\n\n3.\t Lastly,\twe\tcall\tthe\tmultinomial()\tfunction\tto\tpick\ta\trandom\taction.\tThis\tfunction\tindependently samples\tone\t(or\tmore)\tintegers,\tgiven\tthe\tlog\tprobability\tof\teach\tinteger.\tFor\texample,\tif\tyou\tcall\tit with\tthe\tarray\t[np.log(0.5),\tnp.log(0.2),\tnp.log(0.3)]\tand\twith\tnum_samples=5,\tthen\tit will\toutput\tfive\tintegers,\teach\tof\twhich\twill\thave\ta\t50%\tprobability\tof\tbeing\t0,\t20%\tof\tbeing\t1,\tand 30%\tof\tbeing\t2.\tIn\tour\tcase\twe\tjust\tneed\tone\tinteger\trepresenting\tthe\taction\tto\ttake.\tSince\tthe outputs\ttensor\tonly\tcontains\tthe\tprobability\tof\tgoing\tleft,\twe\tmust\tfirst\tconcatenate\t1-outputs\tto\tit to\thave\ta\ttensor\tcontaining\tthe\tprobability\tof\tboth\tleft\tand\tright\tactions.\tNote\tthat\tif\tthere\twere\tmore than\ttwo\tpossible\tactions,\tthe\tneural\tnetwork\twould\thave\tto\toutput\tone\tprobability\tper\taction\tso\tyou would\tnot\tneed\tthe\tconcatenation\tstep.\n\nOkay,\twe\tnow\thave\ta\tneural\tnetwork\tpolicy\tthat\twill\ttake\tobservations\tand\toutput\tactions.\tBut\thow\tdo we\ttrain\tit?",
      "content_length": 3320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 544,
      "content": "Evaluating\tActions:\tThe\tCredit\tAssignment\tProblem If\twe\tknew\twhat\tthe\tbest\taction\twas\tat\teach\tstep,\twe\tcould\ttrain\tthe\tneural\tnetwork\tas\tusual,\tby minimizing\tthe\tcross\tentropy\tbetween\tthe\testimated\tprobability\tand\tthe\ttarget\tprobability.\tIt\twould\tjust\tbe regular\tsupervised\tlearning.\tHowever,\tin\tReinforcement\tLearning\tthe\tonly\tguidance\tthe\tagent\tgets\tis through\trewards,\tand\trewards\tare\ttypically\tsparse\tand\tdelayed.\tFor\texample,\tif\tthe\tagent\tmanages\tto balance\tthe\tpole\tfor\t100\tsteps,\thow\tcan\tit\tknow\twhich\tof\tthe\t100\tactions\tit\ttook\twere\tgood,\tand\twhich\tof them\twere\tbad?\tAll\tit\tknows\tis\tthat\tthe\tpole\tfell\tafter\tthe\tlast\taction,\tbut\tsurely\tthis\tlast\taction\tis\tnot entirely\tresponsible.\tThis\tis\tcalled\tthe\tcredit\tassignment\tproblem:\twhen\tthe\tagent\tgets\ta\treward,\tit\tis hard\tfor\tit\tto\tknow\twhich\tactions\tshould\tget\tcredited\t(or\tblamed)\tfor\tit.\tThink\tof\ta\tdog\tthat\tgets\trewarded hours\tafter\tit\tbehaved\twell;\twill\tit\tunderstand\twhat\tit\tis\trewarded\tfor?\n\nTo\ttackle\tthis\tproblem,\ta\tcommon\tstrategy\tis\tto\tevaluate\tan\taction\tbased\ton\tthe\tsum\tof\tall\tthe\trewards\tthat come\tafter\tit,\tusually\tapplying\ta\tdiscount\trate\tr\tat\teach\tstep.\tFor\texample\t(see\tFigure\t16-6),\tif\tan\tagent decides\tto\tgo\tright\tthree\ttimes\tin\ta\trow\tand\tgets\t+10\treward\tafter\tthe\tfirst\tstep,\t0\tafter\tthe\tsecond\tstep, and\tfinally\t–50\tafter\tthe\tthird\tstep,\tthen\tassuming\twe\tuse\ta\tdiscount\trate\tr\t=\t0.8,\tthe\tfirst\taction\twill\thave a\ttotal\tscore\tof\t10\t+\tr\t×\t0\t+\tr2\t×\t(–50)\t=\t–22.\tIf\tthe\tdiscount\trate\tis\tclose\tto\t0,\tthen\tfuture\trewards\twon’t count\tfor\tmuch\tcompared\tto\timmediate\trewards.\tConversely,\tif\tthe\tdiscount\trate\tis\tclose\tto\t1,\tthen rewards\tfar\tinto\tthe\tfuture\twill\tcount\talmost\tas\tmuch\tas\timmediate\trewards.\tTypical\tdiscount\trates\tare 0.95\tor\t0.99.\tWith\ta\tdiscount\trate\tof\t0.95,\trewards\t13\tsteps\tinto\tthe\tfuture\tcount\troughly\tfor\thalf\tas\tmuch as\timmediate\trewards\t(since\t0.9513\t≈\t0.5),\twhile\twith\ta\tdiscount\trate\tof\t0.99,\trewards\t69\tsteps\tinto\tthe future\tcount\tfor\thalf\tas\tmuch\tas\timmediate\trewards.\tIn\tthe\tCartPole\tenvironment,\tactions\thave\tfairly short-term\teffects,\tso\tchoosing\ta\tdiscount\trate\tof\t0.95\tseems\treasonable.\n\nFigure\t16-6.\tDiscounted\trewards\n\nOf\tcourse,\ta\tgood\taction\tmay\tbe\tfollowed\tby\tseveral\tbad\tactions\tthat\tcause\tthe\tpole\tto\tfall\tquickly, resulting\tin\tthe\tgood\taction\tgetting\ta\tlow\tscore\t(similarly,\ta\tgood\tactor\tmay\tsometimes\tstar\tin\ta\tterrible movie).\tHowever,\tif\twe\tplay\tthe\tgame\tenough\ttimes,\ton\taverage\tgood\tactions\twill\tget\ta\tbetter\tscore\tthan bad\tones.\tSo,\tto\tget\tfairly\treliable\taction\tscores,\twe\tmust\trun\tmany\tepisodes\tand\tnormalize\tall\tthe\taction",
      "content_length": 2540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 545,
      "content": "scores\t(by\tsubtracting\tthe\tmean\tand\tdividing\tby\tthe\tstandard\tdeviation).\tAfter\tthat,\twe\tcan\treasonably assume\tthat\tactions\twith\ta\tnegative\tscore\twere\tbad\twhile\tactions\twith\ta\tpositive\tscore\twere\tgood. Perfect\t—\tnow\tthat\twe\thave\ta\tway\tto\tevaluate\teach\taction,\twe\tare\tready\tto\ttrain\tour\tfirst\tagent\tusing policy\tgradients.\tLet’s\tsee\thow.",
      "content_length": 335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 546,
      "content": "Policy\tGradients As\tdiscussed\tearlier,\tPG\talgorithms\toptimize\tthe\tparameters\tof\ta\tpolicy\tby\tfollowing\tthe\tgradients toward\thigher\trewards.\tOne\tpopular\tclass\tof\tPG\talgorithms,\tcalled\tREINFORCE\talgorithms,\twas introduced\tback\tin\t19929\tby\tRonald\tWilliams.\tHere\tis\tone\tcommon\tvariant:\n\n1.\t First,\tlet\tthe\tneural\tnetwork\tpolicy\tplay\tthe\tgame\tseveral\ttimes\tand\tat\teach\tstep\tcompute\tthe gradients\tthat\twould\tmake\tthe\tchosen\taction\teven\tmore\tlikely,\tbut\tdon’t\tapply\tthese\tgradients\tyet.\n\n2.\t Once\tyou\thave\trun\tseveral\tepisodes,\tcompute\teach\taction’s\tscore\t(using\tthe\tmethod\tdescribed\tin\tthe previous\tparagraph).\n\n3.\t If\tan\taction’s\tscore\tis\tpositive,\tit\tmeans\tthat\tthe\taction\twas\tgood\tand\tyou\twant\tto\tapply\tthe\tgradients computed\tearlier\tto\tmake\tthe\taction\teven\tmore\tlikely\tto\tbe\tchosen\tin\tthe\tfuture.\tHowever,\tif\tthe score\tis\tnegative,\tit\tmeans\tthe\taction\twas\tbad\tand\tyou\twant\tto\tapply\tthe\topposite\tgradients\tto\tmake this\taction\tslightly\tless\tlikely\tin\tthe\tfuture.\tThe\tsolution\tis\tsimply\tto\tmultiply\teach\tgradient\tvector\tby the\tcorresponding\taction’s\tscore.\n\n4.\t Finally,\tcompute\tthe\tmean\tof\tall\tthe\tresulting\tgradient\tvectors,\tand\tuse\tit\tto\tperform\ta\tGradient Descent\tstep.\n\nLet’s\timplement\tthis\talgorithm\tusing\tTensorFlow.\tWe\twill\ttrain\tthe\tneural\tnetwork\tpolicy\twe\tbuilt\tearlier so\tthat\tit\tlearns\tto\tbalance\tthe\tpole\ton\tthe\tcart.\tLet’s\tstart\tby\tcompleting\tthe\tconstruction\tphase\twe\tcoded earlier\tto\tadd\tthe\ttarget\tprobability,\tthe\tcost\tfunction,\tand\tthe\ttraining\toperation.\tSince\twe\tare\tacting\tas though\tthe\tchosen\taction\tis\tthe\tbest\tpossible\taction,\tthe\ttarget\tprobability\tmust\tbe\t1.0\tif\tthe\tchosen\taction is\taction\t0\t(left)\tand\t0.0\tif\tit\tis\taction\t1\t(right):\n\ny\t=\t1.\t-\ttf.to_float(action)\n\nNow\tthat\twe\thave\ta\ttarget\tprobability,\twe\tcan\tdefine\tthe\tcost\tfunction\t(cross\tentropy)\tand\tcompute\tthe gradients:\n\nlearning_rate\t=\t0.01\n\ncross_entropy\t=\ttf.nn.sigmoid_cross_entropy_with_logits(labels=y, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlogits=logits) optimizer\t=\ttf.train.AdamOptimizer(learning_rate) grads_and_vars\t=\toptimizer.compute_gradients(cross_entropy)\n\nNote\tthat\twe\tare\tcalling\tthe\toptimizer’s\tcompute_gradients()\tmethod\tinstead\tof\tthe\tminimize() method.\tThis\tis\tbecause\twe\twant\tto\ttweak\tthe\tgradients\tbefore\twe\tapply\tthem.10\tThe compute_gradients()\tmethod\treturns\ta\tlist\tof\tgradient\tvector/variable\tpairs\t(one\tpair\tper\ttrainable variable).\tLet’s\tput\tall\tthe\tgradients\tin\ta\tlist,\tto\tmake\tit\tmore\tconvenient\tto\tobtain\ttheir\tvalues:\n\ngradients\t=\t[grad\tfor\tgrad,\tvariable\tin\tgrads_and_vars]\n\nOkay,\tnow\tcomes\tthe\ttricky\tpart.\tDuring\tthe\texecution\tphase,\tthe\talgorithm\twill\trun\tthe\tpolicy\tand\tat\teach",
      "content_length": 2615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 547,
      "content": "step\tit\twill\tevaluate\tthese\tgradient\ttensors\tand\tstore\ttheir\tvalues.\tAfter\ta\tnumber\tof\tepisodes\tit\twill\ttweak these\tgradients\tas\texplained\tearlier\t(i.e.,\tmultiply\tthem\tby\tthe\taction\tscores\tand\tnormalize\tthem)\tand compute\tthe\tmean\tof\tthe\ttweaked\tgradients.\tNext,\tit\twill\tneed\tto\tfeed\tthe\tresulting\tgradients\tback\tto\tthe optimizer\tso\tthat\tit\tcan\tperform\tan\toptimization\tstep.\tThis\tmeans\twe\tneed\tone\tplaceholder\tper\tgradient vector.\tMoreover,\twe\tmust\tcreate\tthe\toperation\tthat\twill\tapply\tthe\tupdated\tgradients.\tFor\tthis\twe\twill call\tthe\toptimizer’s\tapply_gradients()\tfunction,\twhich\ttakes\ta\tlist\tof\tgradient\tvector/variable\tpairs. Instead\tof\tgiving\tit\tthe\toriginal\tgradient\tvectors,\twe\twill\tgive\tit\ta\tlist\tcontaining\tthe\tupdated\tgradients (i.e.,\tthe\tones\tfed\tthrough\tthe\tgradient\tplaceholders):\n\ngradient_placeholders\t=\t[] grads_and_vars_feed\t=\t[] for\tgrad,\tvariable\tin\tgrads_and_vars: \t\t\t\tgradient_placeholder\t=\ttf.placeholder(tf.float32,\tshape=grad.get_shape()) \t\t\t\tgradient_placeholders.append(gradient_placeholder) \t\t\t\tgrads_and_vars_feed.append((gradient_placeholder,\tvariable))\n\ntraining_op\t=\toptimizer.apply_gradients(grads_and_vars_feed)\n\nLet’s\tstep\tback\tand\ttake\ta\tlook\tat\tthe\tfull\tconstruction\tphase:\n\nn_inputs\t=\t4 n_hidden\t=\t4 n_outputs\t=\t1 initializer\t=\ttf.contrib.layers.variance_scaling_initializer()\n\nlearning_rate\t=\t0.01\n\nX\t=\ttf.placeholder(tf.float32,\tshape=[None,\tn_inputs]) hidden\t=\ttf.layers.dense(X,\tn_hidden,\tactivation=tf.nn.elu, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) logits\t=\ttf.layers.dense(hidden,\tn_outputs, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) outputs\t=\ttf.nn.sigmoid(logits) p_left_and_right\t=\ttf.concat(axis=1,\tvalues=[outputs,\t1\t-\toutputs]) action\t=\ttf.multinomial(tf.log(p_left_and_right),\tnum_samples=1)\n\ny\t=\t1.\t-\ttf.to_float(action) cross_entropy\t=\ttf.nn.sigmoid_cross_entropy_with_logits( \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tlabels=y,\tlogits=logits) optimizer\t=\ttf.train.AdamOptimizer(learning_rate) grads_and_vars\t=\toptimizer.compute_gradients(cross_entropy) gradients\t=\t[grad\tfor\tgrad,\tvariable\tin\tgrads_and_vars] gradient_placeholders\t=\t[] grads_and_vars_feed\t=\t[] for\tgrad,\tvariable\tin\tgrads_and_vars: \t\t\t\tgradient_placeholder\t=\ttf.placeholder(tf.float32,\tshape=grad.get_shape()) \t\t\t\tgradient_placeholders.append(gradient_placeholder) \t\t\t\tgrads_and_vars_feed.append((gradient_placeholder,\tvariable)) training_op\t=\toptimizer.apply_gradients(grads_and_vars_feed)\n\ninit\t=\ttf.global_variables_initializer() saver\t=\ttf.train.Saver()\n\nOn\tto\tthe\texecution\tphase!\tWe\twill\tneed\ta\tcouple\tof\tfunctions\tto\tcompute\tthe\ttotal\tdiscounted\trewards, given\tthe\traw\trewards,\tand\tto\tnormalize\tthe\tresults\tacross\tmultiple\tepisodes:\n\ndef\tdiscount_rewards(rewards,\tdiscount_rate): \t\t\t\tdiscounted_rewards\t=\tnp.empty(len(rewards)) \t\t\t\tcumulative_rewards\t=\t0 \t\t\t\tfor\tstep\tin\treversed(range(len(rewards))): \t\t\t\t\t\t\t\tcumulative_rewards\t=\trewards[step]\t+\tcumulative_rewards\t*\tdiscount_rate \t\t\t\t\t\t\t\tdiscounted_rewards[step]\t=\tcumulative_rewards \t\t\t\treturn\tdiscounted_rewards",
      "content_length": 3001,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 548,
      "content": "def\tdiscount_and_normalize_rewards(all_rewards,\tdiscount_rate): \t\t\t\tall_discounted_rewards\t=\t[discount_rewards(rewards,\tdiscount_rate) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\trewards\tin\tall_rewards] \t\t\t\tflat_rewards\t=\tnp.concatenate(all_discounted_rewards) \t\t\t\treward_mean\t=\tflat_rewards.mean() \t\t\t\treward_std\t=\tflat_rewards.std() \t\t\t\treturn\t[(discounted_rewards\t-\treward_mean)/reward_std \t\t\t\t\t\t\t\t\t\t\t\tfor\tdiscounted_rewards\tin\tall_discounted_rewards]\n\nLet’s\tcheck\tthat\tthis\tworks:\n\n>>>\tdiscount_rewards([10,\t0,\t-50],\tdiscount_rate=0.8) array([-22.,\t-40.,\t-50.]) >>>\tdiscount_and_normalize_rewards([[10,\t0,\t-50],\t[10,\t20]],\tdiscount_rate=0.8) [array([-0.28435071,\t-0.86597718,\t-1.18910299]), \tarray([\t1.26665318,\t\t1.0727777\t])]\n\nThe\tcall\tto\tdiscount_rewards()\treturns\texactly\twhat\twe\texpect\t(see\tFigure\t16-6).\tYou\tcan\tverify\tthat the\tfunction\tdiscount_and_normalize_rewards()\tdoes\tindeed\treturn\tthe\tnormalized\tscores\tfor\teach action\tin\tboth\tepisodes.\tNotice\tthat\tthe\tfirst\tepisode\twas\tmuch\tworse\tthan\tthe\tsecond,\tso\tits\tnormalized scores\tare\tall\tnegative;\tall\tactions\tfrom\tthe\tfirst\tepisode\twould\tbe\tconsidered\tbad,\tand\tconversely\tall actions\tfrom\tthe\tsecond\tepisode\twould\tbe\tconsidered\tgood.\n\nWe\tnow\thave\tall\twe\tneed\tto\ttrain\tthe\tpolicy:\n\nn_iterations\t=\t250\t\t\t\t\t\t#\tnumber\tof\ttraining\titerations n_max_steps\t=\t1000\t\t\t\t\t\t#\tmax\tsteps\tper\tepisode n_games_per_update\t=\t10\t#\ttrain\tthe\tpolicy\tevery\t10\tepisodes save_iterations\t=\t10\t\t\t\t#\tsave\tthe\tmodel\tevery\t10\ttraining\titerations discount_rate\t=\t0.95\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tinit.run() \t\t\t\tfor\titeration\tin\trange(n_iterations): \t\t\t\t\t\t\t\tall_rewards\t=\t[]\t\t\t\t#\tall\tsequences\tof\traw\trewards\tfor\teach\tepisode \t\t\t\t\t\t\t\tall_gradients\t=\t[]\t\t#\tgradients\tsaved\tat\teach\tstep\tof\teach\tepisode \t\t\t\t\t\t\t\tfor\tgame\tin\trange(n_games_per_update): \t\t\t\t\t\t\t\t\t\t\t\tcurrent_rewards\t=\t[]\t\t\t#\tall\traw\trewards\tfrom\tthe\tcurrent\tepisode \t\t\t\t\t\t\t\t\t\t\t\tcurrent_gradients\t=\t[]\t#\tall\tgradients\tfrom\tthe\tcurrent\tepisode \t\t\t\t\t\t\t\t\t\t\t\tobs\t=\tenv.reset() \t\t\t\t\t\t\t\t\t\t\t\tfor\tstep\tin\trange(n_max_steps): \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taction_val,\tgradients_val\t=\tsess.run( \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[action,\tgradients], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfeed_dict={X:\tobs.reshape(1,\tn_inputs)})\t#\tone\tobs \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tobs,\treward,\tdone,\tinfo\t=\tenv.step(action_val[0][0]) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcurrent_rewards.append(reward) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcurrent_gradients.append(gradients_val) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif\tdone: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tbreak \t\t\t\t\t\t\t\t\t\t\t\tall_rewards.append(current_rewards) \t\t\t\t\t\t\t\t\t\t\t\tall_gradients.append(current_gradients)\n\n#\tAt\tthis\tpoint\twe\thave\trun\tthe\tpolicy\tfor\t10\tepisodes,\tand\twe\tare \t\t\t\t\t\t\t\t#\tready\tfor\ta\tpolicy\tupdate\tusing\tthe\talgorithm\tdescribed\tearlier. \t\t\t\t\t\t\t\tall_rewards\t=\tdiscount_and_normalize_rewards(all_rewards,discount_rate) \t\t\t\t\t\t\t\tfeed_dict\t=\t{} \t\t\t\t\t\t\t\tfor\tvar_index,\tgrad_placeholder\tin\tenumerate(gradient_placeholders): \t\t\t\t\t\t\t\t\t\t\t\t#\tmultiply\tthe\tgradients\tby\tthe\taction\tscores,\tand\tcompute\tthe\tmean \t\t\t\t\t\t\t\t\t\t\t\tmean_gradients\t=\tnp.mean( \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[reward\t*\tall_gradients[game_index][step][var_index] \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tgame_index,\trewards\tin\tenumerate(all_rewards) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tstep,\treward\tin\tenumerate(rewards)], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taxis=0) \t\t\t\t\t\t\t\t\t\t\t\tfeed_dict[grad_placeholder]\t=\tmean_gradients \t\t\t\t\t\t\t\tsess.run(training_op,\tfeed_dict=feed_dict) \t\t\t\t\t\t\t\tif\titeration\t%\tsave_iterations\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tsaver.save(sess,\t\"./my_policy_net_pg.ckpt\")",
      "content_length": 3370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 549,
      "content": "Each\ttraining\titeration\tstarts\tby\trunning\tthe\tpolicy\tfor\t10\tepisodes\t(with\tmaximum\t1,000\tsteps\tper episode,\tto\tavoid\trunning\tforever).\tAt\teach\tstep,\twe\talso\tcompute\tthe\tgradients,\tpretending\tthat\tthe chosen\taction\twas\tthe\tbest.\tAfter\tthese\t10\tepisodes\thave\tbeen\trun,\twe\tcompute\tthe\taction\tscores\tusing\tthe discount_and_normalize_rewards()\tfunction;\twe\tgo\tthrough\teach\ttrainable\tvariable,\tacross\tall episodes\tand\tall\tsteps,\tto\tmultiply\teach\tgradient\tvector\tby\tits\tcorresponding\taction\tscore;\tand\twe compute\tthe\tmean\tof\tthe\tresulting\tgradients.\tFinally,\twe\trun\tthe\ttraining\toperation,\tfeeding\tit\tthese\tmean gradients\t(one\tper\ttrainable\tvariable).\tWe\talso\tsave\tthe\tmodel\tevery\t10\ttraining\toperations.\n\nAnd\twe’re\tdone!\tThis\tcode\twill\ttrain\tthe\tneural\tnetwork\tpolicy,\tand\tit\twill\tsuccessfully\tlearn\tto\tbalance the\tpole\ton\tthe\tcart\t(you\tcan\ttry\tit\tout\tin\tthe\tJupyter\tnotebooks).\tNote\tthat\tthere\tare\tactually\ttwo\tways\tthe agent\tcan\tlose\tthe\tgame:\teither\tthe\tpole\tcan\ttilt\ttoo\tmuch,\tor\tthe\tcart\tcan\tgo\tcompletely\toff\tthe\tscreen. With\t250\ttraining\titerations,\tthe\tpolicy\tlearns\tto\tbalance\tthe\tpole\tquite\twell,\tbut\tit\tis\tnot\tyet\tgood\tenough at\tavoiding\tgoing\toff\tthe\tscreen.\tA\tfew\thundred\tmore\ttraining\titerations\twill\tfix\tthat.\n\nTIP\n\nResearchers\ttry\tto\tfind\talgorithms\tthat\twork\twell\teven\twhen\tthe\tagent\tinitially\tknows\tnothing\tabout\tthe\tenvironment.\tHowever, unless\tyou\tare\twriting\ta\tpaper,\tyou\tshould\tinject\tas\tmuch\tprior\tknowledge\tas\tpossible\tinto\tthe\tagent,\tas\tit\twill\tspeed\tup\ttraining dramatically.\tFor\texample,\tyou\tcould\tadd\tnegative\trewards\tproportional\tto\tthe\tdistance\tfrom\tthe\tcenter\tof\tthe\tscreen,\tand\tto\tthe pole’s\tangle.\tAlso,\tif\tyou\talready\thave\ta\treasonably\tgood\tpolicy\t(e.g.,\thardcoded),\tyou\tmay\twant\tto\ttrain\tthe\tneural\tnetwork\tto imitate\tit\tbefore\tusing\tpolicy\tgradients\tto\timprove\tit.\n\nDespite\tits\trelative\tsimplicity,\tthis\talgorithm\tis\tquite\tpowerful.\tYou\tcan\tuse\tit\tto\ttackle\tmuch\tharder problems\tthan\tbalancing\ta\tpole\ton\ta\tcart.\tIn\tfact,\tAlphaGo\twas\tbased\ton\ta\tsimilar\tPG\talgorithm\t(plus Monte\tCarlo\tTree\tSearch,\twhich\tis\tbeyond\tthe\tscope\tof\tthis\tbook).\n\nWe\twill\tnow\tlook\tat\tanother\tpopular\tfamily\tof\talgorithms.\tWhereas\tPG\talgorithms\tdirectly\ttry\tto\toptimize the\tpolicy\tto\tincrease\trewards,\tthe\talgorithms\twe\twill\tlook\tat\tnow\tare\tless\tdirect:\tthe\tagent\tlearns\tto estimate\tthe\texpected\tsum\tof\tdiscounted\tfuture\trewards\tfor\teach\tstate,\tor\tthe\texpected\tsum\tof\tdiscounted future\trewards\tfor\teach\taction\tin\teach\tstate,\tthen\tuses\tthis\tknowledge\tto\tdecide\thow\tto\tact.\tTo\tunderstand these\talgorithms,\twe\tmust\tfirst\tintroduce\tMarkov\tdecision\tprocesses\t(MDP).",
      "content_length": 2563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 550,
      "content": "Markov\tDecision\tProcesses In\tthe\tearly\t20th\tcentury,\tthe\tmathematician\tAndrey\tMarkov\tstudied\tstochastic\tprocesses\twith\tno\tmemory, called\tMarkov\tchains.\tSuch\ta\tprocess\thas\ta\tfixed\tnumber\tof\tstates,\tand\tit\trandomly\tevolves\tfrom\tone state\tto\tanother\tat\teach\tstep.\tThe\tprobability\tfor\tit\tto\tevolve\tfrom\ta\tstate\ts\tto\ta\tstate\ts′\tis\tfixed,\tand\tit depends\tonly\ton\tthe\tpair\t(s,s′),\tnot\ton\tpast\tstates\t(the\tsystem\thas\tno\tmemory).\n\nFigure\t16-7\tshows\tan\texample\tof\ta\tMarkov\tchain\twith\tfour\tstates.\tSuppose\tthat\tthe\tprocess\tstarts\tin\tstate s0,\tand\tthere\tis\ta\t70%\tchance\tthat\tit\twill\tremain\tin\tthat\tstate\tat\tthe\tnext\tstep.\tEventually\tit\tis\tbound\tto leave\tthat\tstate\tand\tnever\tcome\tback\tsince\tno\tother\tstate\tpoints\tback\tto\ts0.\tIf\tit\tgoes\tto\tstate\ts1,\tit\twill\tthen most\tlikely\tgo\tto\tstate\ts2\t(90%\tprobability),\tthen\timmediately\tback\tto\tstate\ts1\t(with\t100%\tprobability).\tIt may\talternate\ta\tnumber\tof\ttimes\tbetween\tthese\ttwo\tstates,\tbut\teventually\tit\twill\tfall\tinto\tstate\ts3\tand remain\tthere\tforever\t(this\tis\ta\tterminal\tstate).\tMarkov\tchains\tcan\thave\tvery\tdifferent\tdynamics,\tand\tthey are\theavily\tused\tin\tthermodynamics,\tchemistry,\tstatistics,\tand\tmuch\tmore.\n\nFigure\t16-7.\tExample\tof\ta\tMarkov\tchain\n\nMarkov\tdecision\tprocesses\twere\tfirst\tdescribed\tin\tthe\t1950s\tby\tRichard\tBellman.11\tThey\tresemble Markov\tchains\tbut\twith\ta\ttwist:\tat\teach\tstep,\tan\tagent\tcan\tchoose\tone\tof\tseveral\tpossible\tactions,\tand\tthe transition\tprobabilities\tdepend\ton\tthe\tchosen\taction.\tMoreover,\tsome\tstate\ttransitions\treturn\tsome\treward (positive\tor\tnegative),\tand\tthe\tagent’s\tgoal\tis\tto\tfind\ta\tpolicy\tthat\twill\tmaximize\trewards\tover\ttime.\n\nFor\texample,\tthe\tMDP\trepresented\tin\tFigure\t16-8\thas\tthree\tstates\tand\tup\tto\tthree\tpossible\tdiscrete actions\tat\teach\tstep.\tIf\tit\tstarts\tin\tstate\ts0,\tthe\tagent\tcan\tchoose\tbetween\tactions\ta0,\ta1,\tor\ta2.\tIf\tit\tchooses action\ta1,\tit\tjust\tremains\tin\tstate\ts0\twith\tcertainty,\tand\twithout\tany\treward.\tIt\tcan\tthus\tdecide\tto\tstay\tthere forever\tif\tit\twants.\tBut\tif\tit\tchooses\taction\ta0,\tit\thas\ta\t70%\tprobability\tof\tgaining\ta\treward\tof\t+10,\tand remaining\tin\tstate\ts0.\tIt\tcan\tthen\ttry\tagain\tand\tagain\tto\tgain\tas\tmuch\treward\tas\tpossible.\tBut\tat\tone\tpoint\tit",
      "content_length": 2137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 551,
      "content": "is\tgoing\tto\tend\tup\tinstead\tin\tstate\ts1.\tIn\tstate\ts1\tit\thas\tonly\ttwo\tpossible\tactions:\ta0\tor\ta2.\tIt\tcan\tchoose\tto stay\tput\tby\trepeatedly\tchoosing\taction\ta0,\tor\tit\tcan\tchoose\tto\tmove\ton\tto\tstate\ts2\tand\tget\ta\tnegative reward\tof\t–50\t(ouch).\tIn\tstate\ts2\tit\thas\tno\tother\tchoice\tthan\tto\ttake\taction\ta1,\twhich\twill\tmost\tlikely\tlead it\tback\tto\tstate\ts0,\tgaining\ta\treward\tof\t+40\ton\tthe\tway.\tYou\tget\tthe\tpicture.\tBy\tlooking\tat\tthis\tMDP,\tcan you\tguess\twhich\tstrategy\twill\tgain\tthe\tmost\treward\tover\ttime?\tIn\tstate\ts0\tit\tis\tclear\tthat\taction\ta0\tis\tthe best\toption,\tand\tin\tstate\ts2\tthe\tagent\thas\tno\tchoice\tbut\tto\ttake\taction\ta1,\tbut\tin\tstate\ts1\tit\tis\tnot\tobvious whether\tthe\tagent\tshould\tstay\tput\t(a0)\tor\tgo\tthrough\tthe\tfire\t(a2).\n\nFigure\t16-8.\tExample\tof\ta\tMarkov\tdecision\tprocess\n\nBellman\tfound\ta\tway\tto\testimate\tthe\toptimal\tstate\tvalue\tof\tany\tstate\ts,\tnoted\tV*(s),\twhich\tis\tthe\tsum\tof all\tdiscounted\tfuture\trewards\tthe\tagent\tcan\texpect\ton\taverage\tafter\tit\treaches\ta\tstate\ts,\tassuming\tit\tacts optimally.\tHe\tshowed\tthat\tif\tthe\tagent\tacts\toptimally,\tthen\tthe\tBellman\tOptimality\tEquation\tapplies\t(see Equation\t16-1).\tThis\trecursive\tequation\tsays\tthat\tif\tthe\tagent\tacts\toptimally,\tthen\tthe\toptimal\tvalue\tof\tthe current\tstate\tis\tequal\tto\tthe\treward\tit\twill\tget\ton\taverage\tafter\ttaking\tone\toptimal\taction,\tplus\tthe\texpected optimal\tvalue\tof\tall\tpossible\tnext\tstates\tthat\tthis\taction\tcan\tlead\tto.\n\nEquation\t16-1.\tBellman\tOptimality\tEquation\n\nT(s,\ta,\ts′)\tis\tthe\ttransition\tprobability\tfrom\tstate\ts\tto\tstate\ts′,\tgiven\tthat\tthe\tagent\tchose\taction\ta.\n\nR(s,\ta,\ts′)\tis\tthe\treward\tthat\tthe\tagent\tgets\twhen\tit\tgoes\tfrom\tstate\ts\tto\tstate\ts′,\tgiven\tthat\tthe\tagent chose\taction\ta.\n\nγ\tis\tthe\tdiscount\trate.\n\nThis\tequation\tleads\tdirectly\tto\tan\talgorithm\tthat\tcan\tprecisely\testimate\tthe\toptimal\tstate\tvalue\tof\tevery possible\tstate:\tyou\tfirst\tinitialize\tall\tthe\tstate\tvalue\testimates\tto\tzero,\tand\tthen\tyou\titeratively\tupdate\tthem using\tthe\tValue\tIteration\talgorithm\t(see\tEquation\t16-2).\tA\tremarkable\tresult\tis\tthat,\tgiven\tenough\ttime, these\testimates\tare\tguaranteed\tto\tconverge\tto\tthe\toptimal\tstate\tvalues,\tcorresponding\tto\tthe\toptimal",
      "content_length": 2100,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 552,
      "content": "policy.\n\nEquation\t16-2.\tValue\tIteration\talgorithm\n\nVk(s)\tis\tthe\testimated\tvalue\tof\tstate\ts\tat\tthe\tkth\titeration\tof\tthe\talgorithm.\n\nNOTE\n\nThis\talgorithm\tis\tan\texample\tof\tDynamic\tProgramming,\twhich\tbreaks\tdown\ta\tcomplex\tproblem\t(in\tthis\tcase\testimating\ta potentially\tinfinite\tsum\tof\tdiscounted\tfuture\trewards)\tinto\ttractable\tsub-problems\tthat\tcan\tbe\ttackled\titeratively\t(in\tthis\tcase finding\tthe\taction\tthat\tmaximizes\tthe\taverage\treward\tplus\tthe\tdiscounted\tnext\tstate\tvalue).\n\nKnowing\tthe\toptimal\tstate\tvalues\tcan\tbe\tuseful,\tin\tparticular\tto\tevaluate\ta\tpolicy,\tbut\tit\tdoes\tnot\ttell\tthe agent\texplicitly\twhat\tto\tdo.\tLuckily,\tBellman\tfound\ta\tvery\tsimilar\talgorithm\tto\testimate\tthe\toptimal\tstate- action\tvalues,\tgenerally\tcalled\tQ-Values.\tThe\toptimal\tQ-Value\tof\tthe\tstate-action\tpair\t(s,a),\tnoted\tQ* (s,a),\tis\tthe\tsum\tof\tdiscounted\tfuture\trewards\tthe\tagent\tcan\texpect\ton\taverage\tafter\tit\treaches\tthe\tstate\ts and\tchooses\taction\ta,\tbut\tbefore\tit\tsees\tthe\toutcome\tof\tthis\taction,\tassuming\tit\tacts\toptimally\tafter\tthat action.\n\nHere\tis\thow\tit\tworks:\tonce\tagain,\tyou\tstart\tby\tinitializing\tall\tthe\tQ-Value\testimates\tto\tzero,\tthen\tyou update\tthem\tusing\tthe\tQ-Value\tIteration\talgorithm\t(see\tEquation\t16-3).\n\nEquation\t16-3.\tQ-Value\tIteration\talgorithm\n\nOnce\tyou\thave\tthe\toptimal\tQ-Values,\tdefining\tthe\toptimal\tpolicy,\tnoted\tπ*(s),\tis\ttrivial:\twhen\tthe\tagent\tis\n\nin\tstate\ts,\tit\tshould\tchoose\tthe\taction\twith\tthe\thighest\tQ-Value\tfor\tthat\tstate:\n\nLet’s\tapply\tthis\talgorithm\tto\tthe\tMDP\trepresented\tin\tFigure\t16-8.\tFirst,\twe\tneed\tto\tdefine\tthe\tMDP:\n\nnan=np.nan\t\t#\trepresents\timpossible\tactions T\t=\tnp.array([\t\t#\tshape=[s,\ta,\ts'] \t\t\t\t\t\t\t\t[[0.7,\t0.3,\t0.0],\t[1.0,\t0.0,\t0.0],\t[0.8,\t0.2,\t0.0]], \t\t\t\t\t\t\t\t[[0.0,\t1.0,\t0.0],\t[nan,\tnan,\tnan],\t[0.0,\t0.0,\t1.0]], \t\t\t\t\t\t\t\t[[nan,\tnan,\tnan],\t[0.8,\t0.1,\t0.1],\t[nan,\tnan,\tnan]], \t\t\t\t]) R\t=\tnp.array([\t\t#\tshape=[s,\ta,\ts'] \t\t\t\t\t\t\t\t[[10.,\t0.0,\t0.0],\t[0.0,\t0.0,\t0.0],\t[0.0,\t0.0,\t0.0]], \t\t\t\t\t\t\t\t[[10.,\t0.0,\t0.0],\t[nan,\tnan,\tnan],\t[0.0,\t0.0,\t-50.]], \t\t\t\t\t\t\t\t[[nan,\tnan,\tnan],\t[40.,\t0.0,\t0.0],\t[nan,\tnan,\tnan]], \t\t\t\t]) possible_actions\t=\t[[0,\t1,\t2],\t[0,\t2],\t[1]]\n\nNow\tlet’s\trun\tthe\tQ-Value\tIteration\talgorithm:\n\nQ\t=\tnp.full((3,\t3),\t-np.inf)\t\t#\t-inf\tfor\timpossible\tactions for\tstate,\tactions\tin\tenumerate(possible_actions): \t\t\t\tQ[state,\tactions]\t=\t0.0\t\t#\tInitial\tvalue\t=\t0.0,\tfor\tall\tpossible\tactions\n\n.",
      "content_length": 2309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 553,
      "content": "learning_rate\t=\t0.01 discount_rate\t=\t0.95 n_iterations\t=\t100\n\nfor\titeration\tin\trange(n_iterations): \t\t\t\tQ_prev\t=\tQ.copy() \t\t\t\tfor\ts\tin\trange(3): \t\t\t\t\t\t\t\tfor\ta\tin\tpossible_actions[s]: \t\t\t\t\t\t\t\t\t\t\t\tQ[s,\ta]\t=\tnp.sum([ \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tT[s,\ta,\tsp]\t*\t(R[s,\ta,\tsp]\t+\tdiscount_rate\t*\tnp.max(Q_prev[sp])) \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tsp\tin\trange(3) \t\t\t\t\t\t\t\t\t\t\t\t])\n\nThe\tresulting\tQ-Values\tlook\tlike\tthis:\n\n>>>\tQ array([[\t21.89498982,\t\t20.80024033,\t\t16.86353093], \t\t\t\t\t\t\t[\t\t1.11669335,\t\t\t\t\t\t\t\t\t-inf,\t\t\t1.17573546], \t\t\t\t\t\t\t[\t\t\t\t\t\t\t\t-inf,\t\t53.86946068,\t\t\t\t\t\t\t\t\t-inf]]) >>>\tnp.argmax(Q,\taxis=1)\t\t#\toptimal\taction\tfor\teach\tstate array([0,\t2,\t1])\n\nThis\tgives\tus\tthe\toptimal\tpolicy\tfor\tthis\tMDP,\twhen\tusing\ta\tdiscount\trate\tof\t0.95:\tin\tstate\ts0\tchoose action\ta0,\tin\tstate\ts1\tchoose\taction\ta2\t(go\tthrough\tthe\tfire!),\tand\tin\tstate\ts2\tchoose\taction\ta1\t(the\tonly possible\taction).\tInterestingly,\tif\tyou\treduce\tthe\tdiscount\trate\tto\t0.9,\tthe\toptimal\tpolicy\tchanges:\tin\tstate s1\tthe\tbest\taction\tbecomes\ta0\t(stay\tput;\tdon’t\tgo\tthrough\tthe\tfire).\tIt\tmakes\tsense\tbecause\tif\tyou\tvalue\tthe present\tmuch\tmore\tthan\tthe\tfuture,\tthen\tthe\tprospect\tof\tfuture\trewards\tis\tnot\tworth\timmediate\tpain.",
      "content_length": 1149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 554,
      "content": "Temporal\tDifference\tLearning\tand\tQ-Learning Reinforcement\tLearning\tproblems\twith\tdiscrete\tactions\tcan\toften\tbe\tmodeled\tas\tMarkov\tdecision processes,\tbut\tthe\tagent\tinitially\thas\tno\tidea\twhat\tthe\ttransition\tprobabilities\tare\t(it\tdoes\tnot\tknow\tT(s,\ta, s′)),\tand\tit\tdoes\tnot\tknow\twhat\tthe\trewards\tare\tgoing\tto\tbe\teither\t(it\tdoes\tnot\tknow\tR(s,\ta,\ts′)).\tIt\tmust experience\teach\tstate\tand\teach\ttransition\tat\tleast\tonce\tto\tknow\tthe\trewards,\tand\tit\tmust\texperience\tthem multiple\ttimes\tif\tit\tis\tto\thave\ta\treasonable\testimate\tof\tthe\ttransition\tprobabilities.\n\nThe\tTemporal\tDifference\tLearning\t(TD\tLearning)\talgorithm\tis\tvery\tsimilar\tto\tthe\tValue\tIteration algorithm,\tbut\ttweaked\tto\ttake\tinto\taccount\tthe\tfact\tthat\tthe\tagent\thas\tonly\tpartial\tknowledge\tof\tthe\tMDP. In\tgeneral\twe\tassume\tthat\tthe\tagent\tinitially\tknows\tonly\tthe\tpossible\tstates\tand\tactions,\tand\tnothing\tmore. The\tagent\tuses\tan\texploration\tpolicy\t—\tfor\texample,\ta\tpurely\trandom\tpolicy\t—\tto\texplore\tthe\tMDP,\tand as\tit\tprogresses\tthe\tTD\tLearning\talgorithm\tupdates\tthe\testimates\tof\tthe\tstate\tvalues\tbased\ton\tthe transitions\tand\trewards\tthat\tare\tactually\tobserved\t(see\tEquation\t16-4).\n\nEquation\t16-4.\tTD\tLearning\talgorithm\n\nα\tis\tthe\tlearning\trate\t(e.g.,\t0.01).\n\nTIP\n\nTD\tLearning\thas\tmany\tsimilarities\twith\tStochastic\tGradient\tDescent,\tin\tparticular\tthe\tfact\tthat\tit\thandles\tone\tsample\tat\ta\ttime. Just\tlike\tSGD,\tit\tcan\tonly\ttruly\tconverge\tif\tyou\tgradually\treduce\tthe\tlearning\trate\t(otherwise\tit\twill\tkeep\tbouncing\taround\tthe optimum).\n\nFor\teach\tstate\ts,\tthis\talgorithm\tsimply\tkeeps\ttrack\tof\ta\trunning\taverage\tof\tthe\timmediate\trewards\tthe agent\tgets\tupon\tleaving\tthat\tstate,\tplus\tthe\trewards\tit\texpects\tto\tget\tlater\t(assuming\tit\tacts\toptimally).\n\nSimilarly,\tthe\tQ-Learning\talgorithm\tis\tan\tadaptation\tof\tthe\tQ-Value\tIteration\talgorithm\tto\tthe\tsituation where\tthe\ttransition\tprobabilities\tand\tthe\trewards\tare\tinitially\tunknown\t(see\tEquation\t16-5).\n\nEquation\t16-5.\tQ-Learning\talgorithm\n\nFor\teach\tstate-action\tpair\t(s,\ta),\tthis\talgorithm\tkeeps\ttrack\tof\ta\trunning\taverage\tof\tthe\trewards\tr\tthe\tagent gets\tupon\tleaving\tthe\tstate\ts\twith\taction\ta,\tplus\tthe\trewards\tit\texpects\tto\tget\tlater.\tSince\tthe\ttarget\tpolicy would\tact\toptimally,\twe\ttake\tthe\tmaximum\tof\tthe\tQ-Value\testimates\tfor\tthe\tnext\tstate.\n\nHere\tis\thow\tQ-Learning\tcan\tbe\timplemented:\n\nimport\tnumpy.random\tas\trnd\n\nlearning_rate0\t=\t0.05 learning_rate_decay\t=\t0.1",
      "content_length": 2355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 555,
      "content": "n_iterations\t=\t20000\n\ns\t=\t0\t#\tstart\tin\tstate\t0\n\nQ\t=\tnp.full((3,\t3),\t-np.inf)\t\t#\t-inf\tfor\timpossible\tactions for\tstate,\tactions\tin\tenumerate(possible_actions): \t\t\t\tQ[state,\tactions]\t=\t0.0\t\t#\tInitial\tvalue\t=\t0.0,\tfor\tall\tpossible\tactions\n\nfor\titeration\tin\trange(n_iterations): \t\t\t\ta\t=\trnd.choice(possible_actions[s])\t\t#\tchoose\tan\taction\t(randomly) \t\t\t\tsp\t=\trnd.choice(range(3),\tp=T[s,\ta])\t#\tpick\tnext\tstate\tusing\tT[s,\ta] \t\t\t\treward\t=\tR[s,\ta,\tsp] \t\t\t\tlearning_rate\t=\tlearning_rate0\t/\t(1\t+\titeration\t*\tlearning_rate_decay) \t\t\t\tQ[s,\ta]\t=\tlearning_rate\t*\tQ[s,\ta]\t+\t(1\t-\tlearning_rate)\t*\t( \t\t\t\t\t\t\t\t\t\t\t\treward\t+\tdiscount_rate\t*\tnp.max(Q[sp]) \t\t\t\t\t\t\t\t) \t\t\t\ts\t=\tsp\t#\tmove\tto\tnext\tstate\n\nGiven\tenough\titerations,\tthis\talgorithm\twill\tconverge\tto\tthe\toptimal\tQ-Values.\tThis\tis\tcalled\tan\toff-policy algorithm\tbecause\tthe\tpolicy\tbeing\ttrained\tis\tnot\tthe\tone\tbeing\texecuted.\tIt\tis\tsomewhat\tsurprising\tthat this\talgorithm\tis\tcapable\tof\tlearning\tthe\toptimal\tpolicy\tby\tjust\twatching\tan\tagent\tact\trandomly\t(imagine learning\tto\tplay\tgolf\twhen\tyour\tteacher\tis\ta\tdrunken\tmonkey).\tCan\twe\tdo\tbetter?",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 556,
      "content": "Exploration\tPolicies Of\tcourse\tQ-Learning\tcan\twork\tonly\tif\tthe\texploration\tpolicy\texplores\tthe\tMDP\tthoroughly\tenough. Although\ta\tpurely\trandom\tpolicy\tis\tguaranteed\tto\teventually\tvisit\tevery\tstate\tand\tevery\ttransition\tmany times,\tit\tmay\ttake\tan\textremely\tlong\ttime\tto\tdo\tso.\tTherefore,\ta\tbetter\toption\tis\tto\tuse\tthe\tε-greedy\tpolicy: at\teach\tstep\tit\tacts\trandomly\twith\tprobability\tε,\tor\tgreedily\t(choosing\tthe\taction\twith\tthe\thighest\tQ-Value) with\tprobability\t1-ε.\tThe\tadvantage\tof\tthe\tε-greedy\tpolicy\t(compared\tto\ta\tcompletely\trandom\tpolicy)\tis that\tit\twill\tspend\tmore\tand\tmore\ttime\texploring\tthe\tinteresting\tparts\tof\tthe\tenvironment,\tas\tthe\tQ-Value estimates\tget\tbetter\tand\tbetter,\twhile\tstill\tspending\tsome\ttime\tvisiting\tunknown\tregions\tof\tthe\tMDP.\tIt\tis quite\tcommon\tto\tstart\twith\ta\thigh\tvalue\tfor\tε\t(e.g.,\t1.0)\tand\tthen\tgradually\treduce\tit\t(e.g.,\tdown\tto\t0.05).\n\nAlternatively,\trather\tthan\trelying\ton\tchance\tfor\texploration,\tanother\tapproach\tis\tto\tencourage\tthe exploration\tpolicy\tto\ttry\tactions\tthat\tit\thas\tnot\ttried\tmuch\tbefore.\tThis\tcan\tbe\timplemented\tas\ta\tbonus added\tto\tthe\tQ-Value\testimates,\tas\tshown\tin\tEquation\t16-6.\n\nEquation\t16-6.\tQ-Learning\tusing\tan\texploration\tfunction\n\nN(s′,\ta′)\tcounts\tthe\tnumber\tof\ttimes\tthe\taction\ta′\twas\tchosen\tin\tstate\ts′.\n\nf(q,\tn)\tis\tan\texploration\tfunction,\tsuch\tas\tf(q,\tn)\t=\tq\t+\tK/(1\t+\tn),\twhere\tK\tis\ta\tcuriosity hyperparameter\tthat\tmeasures\thow\tmuch\tthe\tagent\tis\tattracted\tto\tto\tthe\tunknown.",
      "content_length": 1433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 557,
      "content": "Approximate\tQ-Learning The\tmain\tproblem\twith\tQ-Learning\tis\tthat\tit\tdoes\tnot\tscale\twell\tto\tlarge\t(or\teven\tmedium)\tMDPs\twith many\tstates\tand\tactions.\tConsider\ttrying\tto\tuse\tQ-Learning\tto\ttrain\tan\tagent\tto\tplay\tMs.\tPac-Man.\tThere are\tover\t250\tpellets\tthat\tMs.\tPac-Man\tcan\teat,\teach\tof\twhich\tcan\tbe\tpresent\tor\tabsent\t(i.e.,\talready eaten).\tSo\tthe\tnumber\tof\tpossible\tstates\tis\tgreater\tthan\t2250\t≈\t1075\t(and\tthat’s\tconsidering\tthe\tpossible states\tonly\tof\tthe\tpellets).\tThis\tis\tway\tmore\tthan\tatoms\tin\tthe\tobservable\tuniverse,\tso\tthere’s\tabsolutely no\tway\tyou\tcan\tkeep\ttrack\tof\tan\testimate\tfor\tevery\tsingle\tQ-Value.\n\nThe\tsolution\tis\tto\tfind\ta\tfunction\tthat\tapproximates\tthe\tQ-Values\tusing\ta\tmanageable\tnumber\tof parameters.\tThis\tis\tcalled\tApproximate\tQ-Learning.\tFor\tyears\tit\twas\trecommended\tto\tuse\tlinear combinations\tof\thand-crafted\tfeatures\textracted\tfrom\tthe\tstate\t(e.g.,\tdistance\tof\tthe\tclosest\tghosts,\ttheir directions,\tand\tso\ton)\tto\testimate\tQ-Values,\tbut\tDeepMind\tshowed\tthat\tusing\tdeep\tneural\tnetworks\tcan work\tmuch\tbetter,\tespecially\tfor\tcomplex\tproblems,\tand\tit\tdoes\tnot\trequire\tany\tfeature\tengineering.\tA DNN\tused\tto\testimate\tQ-Values\tis\tcalled\ta\tdeep\tQ-network\t(DQN),\tand\tusing\ta\tDQN\tfor\tApproximate Q-Learning\tis\tcalled\tDeep\tQ-Learning.\n\nIn\tthe\trest\tof\tthis\tchapter,\twe\twill\tuse\tDeep\tQ-Learning\tto\ttrain\tan\tagent\tto\tplay\tMs.\tPac-Man,\tmuch\tlike DeepMind\tdid\tin\t2013.\tThe\tcode\tcan\teasily\tbe\ttweaked\tto\tlearn\tto\tplay\tthe\tmajority\tof\tAtari\tgames\tquite well.\tIt\tcan\tachieve\tsuperhuman\tskill\tat\tmost\taction\tgames,\tbut\tit\tis\tnot\tso\tgood\tat\tgames\twith\tlong- running\tstorylines.",
      "content_length": 1575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 558,
      "content": "Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning Since\twe\twill\tbe\tusing\tan\tAtari\tenvironment,\twe\tmust\tfirst\tinstall\tOpenAI\tgym’s\tAtari\tdependencies. While\twe’re\tat\tit,\twe\twill\talso\tinstall\tdependencies\tfor\tother\tOpenAI\tgym\tenvironments\tthat\tyou\tmay want\tto\tplay\twith.\tOn\tmacOS,\tassuming\tyou\thave\tinstalled\tHomebrew,\tyou\tneed\tto\trun:\n\n$\tbrew\tinstall\tcmake\tboost\tboost-python\tsdl2\tswig\twget\n\nOn\tUbuntu,\ttype\tthe\tfollowing\tcommand\t(replacing\tpython3\twith\tpython\tif\tyou\tare\tusing\tPython\t2):\n\n$\tapt-get\tinstall\t-y\tpython3-numpy\tpython3-dev\tcmake\tzlib1g-dev\tlibjpeg-dev\\ \t\t\t\txvfb\tlibav-tools\txorg-dev\tpython3-opengl\tlibboost-all-dev\tlibsdl2-dev\tswig\n\nThen\tinstall\tthe\textra\tPython\tmodules:\n\n$\tpip3\tinstall\t--upgrade\t'gym[all]'\n\nIf\teverything\twent\twell,\tyou\tshould\tbe\table\tto\tcreate\ta\tMs.\tPac-Man\tenvironment:\n\n>>>\tenv\t=\tgym.make(\"MsPacman-v0\") >>>\tobs\t=\tenv.reset() >>>\tobs.shape\t\t#\t[height,\twidth,\tchannels] (210,\t160,\t3) >>>\tenv.action_space Discrete(9)\n\nAs\tyou\tcan\tsee,\tthere\tare\tnine\tdiscrete\tactions\tavailable,\twhich\tcorrespond\tto\tthe\tnine\tpossible\tpositions of\tthe\tjoystick\t(left,\tright,\tup,\tdown,\tcenter,\tupper\tleft,\tand\tso\ton),\tand\tthe\tobservations\tare\tsimply screenshots\tof\tthe\tAtari\tscreen\t(see\tFigure\t16-9,\tleft),\trepresented\tas\t3D\tNumPy\tarrays.\tThese\timages are\ta\tbit\tlarge,\tso\twe\twill\tcreate\ta\tsmall\tpreprocessing\tfunction\tthat\twill\tcrop\tthe\timage\tand\tshrink\tit down\tto\t88\t×\t80\tpixels,\tconvert\tit\tto\tgrayscale,\tand\timprove\tthe\tcontrast\tof\tMs.\tPac-Man.\tThis\twill reduce\tthe\tamount\tof\tcomputations\trequired\tby\tthe\tDQN,\tand\tspeed\tup\ttraining.\n\nmspacman_color\t=\tnp.array([210,\t164,\t74]).mean()\n\ndef\tpreprocess_observation(obs): \t\t\t\timg\t=\tobs[1:176:2,\t::2]\t#\tcrop\tand\tdownsize \t\t\t\timg\t=\timg.mean(axis=2)\t#\tto\tgreyscale \t\t\t\timg[img==mspacman_color]\t=\t0\t#\timprove\tcontrast \t\t\t\timg\t=\t(img\t-\t128)\t/\t128\t-\t1\t#\tnormalize\tfrom\t-1.\tto\t1. \t\t\t\treturn\timg.reshape(88,\t80,\t1)\n\nThe\tresult\tof\tpreprocessing\tis\tshown\tin\tFigure\t16-9\t(right).",
      "content_length": 1929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 559,
      "content": "Figure\t16-9.\tMs.\tPac-Man\tobservation,\toriginal\t(left)\tand\tafter\tpreprocessing\t(right)\n\nNext,\tlet’s\tcreate\tthe\tDQN.\tIt\tcould\tjust\ttake\ta\tstate-action\tpair\t(s,a)\tas\tinput,\tand\toutput\tan\testimate\tof the\tcorresponding\tQ-Value\tQ(s,a),\tbut\tsince\tthe\tactions\tare\tdiscrete\tit\tis\tmore\tconvenient\tto\tuse\ta\tneural network\tthat\ttakes\tonly\ta\tstate\ts\tas\tinput\tand\toutputs\tone\tQ-Value\testimate\tper\taction.\tThe\tDQN\twill\tbe composed\tof\tthree\tconvolutional\tlayers,\tfollowed\tby\ttwo\tfully\tconnected\tlayers,\tincluding\tthe\toutput layer\t(see\tFigure\t16-10).",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 560,
      "content": "Figure\t16-10.\tDeep\tQ-network\tto\tplay\tMs.\tPac-Man\n\nAs\twe\twill\tsee,\tthe\ttraining\talgorithm\twe\twill\tuse\trequires\ttwo\tDQNs\twith\tthe\tsame\tarchitecture\t(but different\tparameters):\tone\twill\tbe\tused\tto\tdrive\tMs.\tPac-Man\tduring\ttraining\t(the\tactor),\tand\tthe\tother will\twatch\tthe\tactor\tand\tlearn\tfrom\tits\ttrials\tand\terrors\t(the\tcritic).\tAt\tregular\tintervals\twe\twill\tcopy\tthe critic\tto\tthe\tactor.\tSince\twe\tneed\ttwo\tidentical\tDQNs,\twe\twill\tcreate\ta\tq_network()\tfunction\tto\tbuild them:\n\ninput_height\t=\t88 input_width\t=\t80 input_channels\t=\t1 conv_n_maps\t=\t[32,\t64,\t64] conv_kernel_sizes\t=\t[(8,8),\t(4,4),\t(3,3)] conv_strides\t=\t[4,\t2,\t1] conv_paddings\t=\t[\"SAME\"]\t*\t3 conv_activation\t=\t[tf.nn.relu]\t*\t3",
      "content_length": 685,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 561,
      "content": "n_hidden_in\t=\t64\t*\t11\t*\t10\t\t#\tconv3\thas\t64\tmaps\tof\t11x10\teach n_hidden\t=\t512 hidden_activation\t=\ttf.nn.relu n_outputs\t=\tenv.action_space.n\t\t#\t9\tdiscrete\tactions\tare\tavailable initializer\t=\ttf.contrib.layers.variance_scaling_initializer()\n\ndef\tq_network(X_state,\tname): \t\t\t\tprev_layer\t=\tX_state \t\t\t\tconv_layers\t=\t[] \t\t\t\twith\ttf.variable_scope(name)\tas\tscope: \t\t\t\t\t\t\t\tfor\tn_maps,\tkernel_size,\tstride,\tpadding,\tactivation\tin\tzip( \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tconv_n_maps,\tconv_kernel_sizes,\tconv_strides, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tconv_paddings,\tconv_activation): \t\t\t\t\t\t\t\t\t\t\t\tprev_layer\t=\ttf.layers.conv2d( \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tprev_layer,\tfilters=n_maps,\tkernel_size=kernel_size, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstride=stride,\tpadding=padding,\tactivation=activation, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) \t\t\t\t\t\t\t\t\t\t\t\tconv_layers.append(prev_layer) \t\t\t\t\t\t\t\tlast_conv_layer_flat\t=\ttf.reshape(prev_layer,\tshape=[-1,\tn_hidden_in]) \t\t\t\t\t\t\t\thidden\t=\ttf.layers.dense(last_conv_layer_flat,\tn_hidden, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tactivation=hidden_activation, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) \t\t\t\t\t\t\t\toutputs\t=\ttf.layers.dense(hidden,\tn_outputs, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tkernel_initializer=initializer) \t\t\t\ttrainable_vars\t=\ttf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tscope=scope.name) \t\t\t\ttrainable_vars_by_name\t=\t{var.name[len(scope.name):]:\tvar \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor\tvar\tin\ttrainable_vars} \t\t\t\treturn\toutputs,\ttrainable_vars_by_name\n\nThe\tfirst\tpart\tof\tthis\tcode\tdefines\tthe\thyperparameters\tof\tthe\tDQN\tarchitecture.\tThen\tthe\tq_network() function\tcreates\tthe\tDQN,\ttaking\tthe\tenvironment’s\tstate\tX_state\tas\tinput,\tand\tthe\tname\tof\tthe\tvariable scope.\tNote\tthat\twe\twill\tjust\tuse\tone\tobservation\tto\trepresent\tthe\tenvironment’s\tstate\tsince\tthere’s\talmost no\thidden\tstate\t(except\tfor\tblinking\tobjects\tand\tthe\tghosts’\tdirections).\n\nThe\ttrainable_vars_by_name\tdictionary\tgathers\tall\tthe\ttrainable\tvariables\tof\tthis\tDQN.\tIt\twill\tbe useful\tin\ta\tminute\twhen\twe\tcreate\toperations\tto\tcopy\tthe\tcritic\tDQN\tto\tthe\tactor\tDQN.\tThe\tkeys\tof\tthe dictionary\tare\tthe\tnames\tof\tthe\tvariables,\tstripping\tthe\tpart\tof\tthe\tprefix\tthat\tjust\tcorresponds\tto\tthe scope’s\tname.\tIt\tlooks\tlike\tthis:\n\n>>>\ttrainable_vars_by_name {'/conv2d/bias:0':\t<tensorflow.python.ops.variables.Variable\tat\t0x121cf7b50>, \t'/conv2d/kernel:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/conv2d_1/bias:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/conv2d_1/kernel:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/conv2d_2/bias:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/conv2d_2/kernel:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/dense/bias:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/dense/kernel:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/dense_1/bias:0':\t<tensorflow.python.ops.variables.Variable...>, \t'/dense_1/kernel:0':\t<tensorflow.python.ops.variables.Variable...>}\n\nNow\tlet’s\tcreate\tthe\tinput\tplaceholder,\tthe\ttwo\tDQNs,\tand\tthe\toperation\tto\tcopy\tthe\tcritic\tDQN\tto\tthe actor\tDQN:\n\nX_state\t=\ttf.placeholder(tf.float32,\tshape=[None,\tinput_height,\tinput_width, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tinput_channels]) actor_q_values,\tactor_vars\t=\tq_network(X_state,\tname=\"q_networks/actor\") critic_q_values,\tcritic_vars\t=\tq_network(X_state,\tname=\"q_networks/critic\")\n\ncopy_ops\t=\t[actor_var.assign(critic_vars[var_name]) \t\t\t\t\t\t\t\t\t\t\t\tfor\tvar_name,\tactor_var\tin\tactor_vars.items()] copy_critic_to_actor\t=\ttf.group(*copy_ops)",
      "content_length": 3498,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 562,
      "content": "Let’s\tstep\tback\tfor\ta\tsecond:\twe\tnow\thave\ttwo\tDQNs\tthat\tare\tboth\tcapable\tof\ttaking\tan\tenvironment\tstate (i.e.,\ta\tpreprocessed\tobservation)\tas\tinput\tand\toutputting\tan\testimated\tQ-Value\tfor\teach\tpossible\taction\tin that\tstate.\tPlus\twe\thave\tan\toperation\tcalled\tcopy_critic_to_actor\tto\tcopy\tall\tthe\ttrainable\tvariables of\tthe\tcritic\tDQN\tto\tthe\tactor\tDQN.\tWe\tuse\tTensorFlow’s\ttf.group()\tfunction\tto\tgroup\tall\tthe assignment\toperations\tinto\ta\tsingle\tconvenient\toperation.\n\nThe\tactor\tDQN\tcan\tbe\tused\tto\tplay\tMs.\tPac-Man\t(initially\tvery\tbadly).\tAs\tdiscussed\tearlier,\tyou\twant\tit to\texplore\tthe\tgame\tthoroughly\tenough,\tso\tyou\tgenerally\twant\tto\tcombine\tit\twith\tan\tε-greedy\tpolicy\tor another\texploration\tstrategy.\n\nBut\twhat\tabout\tthe\tcritic\tDQN?\tHow\twill\tit\tlearn\tto\tplay\tthe\tgame?\tThe\tshort\tanswer\tis\tthat\tit\twill\ttry\tto make\tits\tQ-Value\tpredictions\tmatch\tthe\tQ-Values\testimated\tby\tthe\tactor\tthrough\tits\texperience\tof\tthe game.\tSpecifically,\twe\twill\tlet\tthe\tactor\tplay\tfor\ta\twhile,\tstoring\tall\tits\texperiences\tin\ta\treplay\tmemory. Each\tmemory\twill\tbe\ta\t5-tuple\t(state,\taction,\tnext\tstate,\treward,\tcontinue),\twhere\tthe\t“continue”\titem\twill be\tequal\tto\t0.0\twhen\tthe\tgame\tis\tover,\tor\t1.0\totherwise.\tNext,\tat\tregular\tintervals\twe\twill\tsample\ta\tbatch of\tmemories\tfrom\tthe\treplay\tmemory,\tand\twe\twill\testimate\tthe\tQ-Values\tfrom\tthese\tmemories.\tFinally, we\twill\ttrain\tthe\tcritic\tDQN\tto\tpredict\tthese\tQ-Values\tusing\tregular\tsupervised\tlearning\ttechniques.\tOnce every\tfew\ttraining\titerations,\twe\twill\tcopy\tthe\tcritic\tDQN\tto\tthe\tactor\tDQN.\tAnd\tthat’s\tit!\tEquation\t16-7 shows\tthe\tcost\tfunction\tused\tto\ttrain\tthe\tcritic\tDQN:\n\nEquation\t16-7.\tDeep\tQ-Learning\tcost\tfunction\n\ns(i),\ta(i),\tr(i)\tand\ts′(i)\tare\trespectively\tthe\tstate,\taction,\treward,\tand\tnext\tstate\tof\tthe\tith\tmemory sampled\tfrom\tthe\treplay\tmemory.\n\nm\tis\tthe\tsize\tof\tthe\tmemory\tbatch.\n\nθcritic\tand\tθactor\tare\tthe\tcritic\tand\tthe\tactor’s\tparameters.\n\nQ(s(i),a(i),θcritic)\tis\tthe\tcritic\tDQN’s\tprediction\tof\tthe\tith\tmemorized\tstate-action’s\tQ-Value.\n\nQ(s′(i),\ta′,\tθactor)\tis\tthe\tactor\tDQN’s\tprediction\tof\tthe\tQ-Value\tit\tcan\texpect\tfrom\tthe\tnext\tstate\ts′(i)\tif it\tchooses\taction\ta′.\n\ny(i)\tis\tthe\ttarget\tQ-Value\tfor\tthe\tith\tmemory.\tNote\tthat\tit\tis\tequal\tto\tthe\treward\tactually\tobserved\tby the\tactor,\tplus\tthe\tactor’s\tprediction\tof\twhat\tfuture\trewards\tit\tshould\texpect\tif\tit\twere\tto\tplay optimally\t(as\tfar\tas\tit\tknows).\n\nJ(θcritic)\tis\tthe\tcost\tfunction\tused\tto\ttrain\tthe\tcritic\tDQN.\tAs\tyou\tcan\tsee,\tit\tis\tjust\tthe\tMean\tSquared",
      "content_length": 2453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 563,
      "content": "Error\tbetween\tthe\ttarget\tQ-Values\ty(i)\tas\testimated\tby\tthe\tactor\tDQN,\tand\tthe\tcritic\tDQN’s predictions\tof\tthese\tQ-Values.\n\nNOTE\n\nThe\treplay\tmemory\tis\toptional,\tbut\thighly\trecommended.\tWithout\tit,\tyou\twould\ttrain\tthe\tcritic\tDQN\tusing\tconsecutive experiences\tthat\tmay\tbe\tvery\tcorrelated.\tThis\twould\tintroduce\ta\tlot\tof\tbias\tand\tslow\tdown\tthe\ttraining\talgorithm’s\tconvergence. By\tusing\ta\treplay\tmemory,\twe\tensure\tthat\tthe\tmemories\tfed\tto\tthe\ttraining\talgorithm\tcan\tbe\tfairly\tuncorrelated.\n\nLet’s\tadd\tthe\tcritic\tDQN’s\ttraining\toperations.\tFirst,\twe\tneed\tto\tbe\table\tto\tcompute\tits\tpredicted\tQ- Values\tfor\teach\tstate-action\tin\tthe\tmemory\tbatch.\tSince\tthe\tDQN\toutputs\tone\tQ-Value\tfor\tevery\tpossible action,\twe\tneed\tto\tkeep\tonly\tthe\tQ-Value\tthat\tcorresponds\tto\tthe\taction\tthat\twas\tactually\tchosen\tin\tthis memory.\tFor\tthis,\twe\twill\tconvert\tthe\taction\tto\ta\tone-hot\tvector\t(recall\tthat\tthis\tis\ta\tvector\tfull\tof\t0s except\tfor\ta\t1\tat\tthe\tith\tindex),\tand\tmultiply\tit\tby\tthe\tQ-Values:\tthis\twill\tzero\tout\tall\tQ-Values\texcept\tfor the\tone\tcorresponding\tto\tthe\tmemorized\taction.\tThen\tjust\tsum\tover\tthe\tfirst\taxis\tto\tobtain\tonly\tthe\tdesired Q-Value\tprediction\tfor\teach\tmemory.\n\nX_action\t=\ttf.placeholder(tf.int32,\tshape=[None]) q_value\t=\ttf.reduce_sum(critic_q_values\t*\ttf.one_hot(X_action,\tn_outputs), \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taxis=1,\tkeep_dims=True)\n\nNext\tlet’s\tadd\tthe\ttraining\toperations,\tassuming\tthe\ttarget\tQ-Values\twill\tbe\tfed\tthrough\ta\tplaceholder.\tWe also\tcreate\ta\tnontrainable\tvariable\tcalled\tglobal_step.\tThe\toptimizer’s\tminimize()\toperation\twill take\tcare\tof\tincrementing\tit.\tPlus\twe\tcreate\tthe\tusual\tinit\toperation\tand\ta\tSaver.\n\ny\t=\ttf.placeholder(tf.float32,\tshape=[None,\t1]) cost\t=\ttf.reduce_mean(tf.square(y\t-\tq_value)) global_step\t=\ttf.Variable(0,\ttrainable=False,\tname='global_step') optimizer\t=\ttf.train.AdamOptimizer(learning_rate) training_op\t=\toptimizer.minimize(cost,\tglobal_step=global_step)\n\ninit\t=\ttf.global_variables_initializer() saver\t=\ttf.train.Saver()\n\nThat’s\tit\tfor\tthe\tconstruction\tphase.\tBefore\twe\tlook\tat\tthe\texecution\tphase,\twe\twill\tneed\ta\tcouple\tof tools.\tFirst,\tlet’s\tstart\tby\timplementing\tthe\treplay\tmemory.\tWe\twill\tuse\ta\tdeque\tlist\tsince\tit\tis\tvery efficient\tat\tpushing\titems\tto\tthe\tqueue\tand\tpopping\tthem\tout\tfrom\tthe\tend\tof\tthe\tlist\twhen\tthe\tmaximum memory\tsize\tis\treached.\tWe\twill\talso\twrite\ta\tsmall\tfunction\tto\trandomly\tsample\ta\tbatch\tof\texperiences from\tthe\treplay\tmemory:\n\nfrom\tcollections\timport\tdeque\n\nreplay_memory_size\t=\t10000 replay_memory\t=\tdeque([],\tmaxlen=replay_memory_size)\n\ndef\tsample_memories(batch_size): \t\t\t\tindices\t=\trnd.permutation(len(replay_memory))[:batch_size] \t\t\t\tcols\t=\t[[],\t[],\t[],\t[],\t[]]\t#\tstate,\taction,\treward,\tnext_state,\tcontinue \t\t\t\tfor\tidx\tin\tindices: \t\t\t\t\t\t\t\tmemory\t=\treplay_memory[idx] \t\t\t\t\t\t\t\tfor\tcol,\tvalue\tin\tzip(cols,\tmemory): \t\t\t\t\t\t\t\t\t\t\t\tcol.append(value) \t\t\t\tcols\t=\t[np.array(col)\tfor\tcol\tin\tcols]",
      "content_length": 2860,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 564,
      "content": "return\t(cols[0],\tcols[1],\tcols[2].reshape(-1,\t1),\tcols[3], \t\t\t\t\t\t\t\t\t\t\t\tcols[4].reshape(-1,\t1))\n\nNext,\twe\twill\tneed\tthe\tactor\tto\texplore\tthe\tgame.\tWe\twill\tuse\tthe\tε-greedy\tpolicy,\tand\tgradually\tdecrease ε\tfrom\t1.0\tto\t0.05,\tin\t50,000\ttraining\tsteps:\n\neps_min\t=\t0.05 eps_max\t=\t1.0 eps_decay_steps\t=\t50000\n\ndef\tepsilon_greedy(q_values,\tstep): \t\t\t\tepsilon\t=\tmax(eps_min,\teps_max\t-\t(eps_max-eps_min)\t*\tstep/eps_decay_steps) \t\t\t\tif\trnd.rand()\t<\tepsilon: \t\t\t\t\t\t\t\treturn\trnd.randint(n_outputs)\t#\trandom\taction \t\t\t\telse: \t\t\t\t\t\t\t\treturn\tnp.argmax(q_values)\t#\toptimal\taction\n\nThat’s\tit!\tWe\thave\tall\twe\tneed\tto\tstart\ttraining.\tThe\texecution\tphase\tdoes\tnot\tcontain\tanything\ttoo complex,\tbut\tit\tis\ta\tbit\tlong,\tso\ttake\ta\tdeep\tbreath.\tReady?\tLet’s\tgo!\tFirst,\tlet’s\tinitialize\ta\tfew\tvariables:\n\nn_steps\t=\t100000\t\t#\ttotal\tnumber\tof\ttraining\tsteps training_start\t=\t1000\t\t#\tstart\ttraining\tafter\t1,000\tgame\titerations training_interval\t=\t3\t\t#\trun\ta\ttraining\tstep\tevery\t3\tgame\titerations save_steps\t=\t50\t\t#\tsave\tthe\tmodel\tevery\t50\ttraining\tsteps copy_steps\t=\t25\t\t#\tcopy\tthe\tcritic\tto\tthe\tactor\tevery\t25\ttraining\tsteps discount_rate\t=\t0.95 skip_start\t=\t90\t\t#\tskip\tthe\tstart\tof\tevery\tgame\t(it's\tjust\twaiting\ttime) batch_size\t=\t50 iteration\t=\t0\t\t#\tgame\titerations checkpoint_path\t=\t\"./my_dqn.ckpt\" done\t=\tTrue\t#\tenv\tneeds\tto\tbe\treset\n\nNext,\tlet’s\topen\tthe\tsession\tand\trun\tthe\tmain\ttraining\tloop:\n\nwith\ttf.Session()\tas\tsess: \t\t\t\tif\tos.path.isfile(checkpoint_path): \t\t\t\t\t\t\t\tsaver.restore(sess,\tcheckpoint_path) \t\t\t\telse: \t\t\t\t\t\t\t\tinit.run() \t\t\t\twhile\tTrue: \t\t\t\t\t\t\t\tstep\t=\tglobal_step.eval() \t\t\t\t\t\t\t\tif\tstep\t>=\tn_steps: \t\t\t\t\t\t\t\t\t\t\t\tbreak \t\t\t\t\t\t\t\titeration\t+=\t1 \t\t\t\t\t\t\t\tif\tdone:\t#\tgame\tover,\tstart\tagain \t\t\t\t\t\t\t\t\t\t\t\tobs\t=\tenv.reset() \t\t\t\t\t\t\t\t\t\t\t\tfor\tskip\tin\trange(skip_start):\t#\tskip\tthe\tstart\tof\teach\tgame \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tobs,\treward,\tdone,\tinfo\t=\tenv.step(0) \t\t\t\t\t\t\t\t\t\t\t\tstate\t=\tpreprocess_observation(obs)\n\n#\tActor\tevaluates\twhat\tto\tdo \t\t\t\t\t\t\t\tq_values\t=\tactor_q_values.eval(feed_dict={X_state:\t[state]}) \t\t\t\t\t\t\t\taction\t=\tepsilon_greedy(q_values,\tstep)\n\n#\tActor\tplays \t\t\t\t\t\t\t\tobs,\treward,\tdone,\tinfo\t=\tenv.step(action) \t\t\t\t\t\t\t\tnext_state\t=\tpreprocess_observation(obs)\n\n#\tLet's\tmemorize\twhat\tjust\thappened \t\t\t\t\t\t\t\treplay_memory.append((state,\taction,\treward,\tnext_state,\t1.0\t-\tdone)) \t\t\t\t\t\t\t\tstate\t=\tnext_state\n\nif\titeration\t<\ttraining_start\tor\titeration\t%\ttraining_interval\t!=\t0: \t\t\t\t\t\t\t\t\t\t\t\tcontinue\n\n#\tCritic\tlearns \t\t\t\t\t\t\t\tX_state_val,\tX_action_val,\trewards,\tX_next_state_val,\tcontinues\t=\t( \t\t\t\t\t\t\t\t\t\t\t\tsample_memories(batch_size)) \t\t\t\t\t\t\t\tnext_q_values\t=\tactor_q_values.eval(",
      "content_length": 2551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 565,
      "content": "feed_dict={X_state:\tX_next_state_val}) \t\t\t\t\t\t\t\tmax_next_q_values\t=\tnp.max(next_q_values,\taxis=1,\tkeepdims=True) \t\t\t\t\t\t\t\ty_val\t=\trewards\t+\tcontinues\t*\tdiscount_rate\t*\tmax_next_q_values \t\t\t\t\t\t\t\ttraining_op.run(feed_dict={X_state:\tX_state_val, \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tX_action:\tX_action_val,\ty:\ty_val})\n\n#\tRegularly\tcopy\tcritic\tto\tactor \t\t\t\t\t\t\t\tif\tstep\t%\tcopy_steps\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tcopy_critic_to_actor.run()\n\n#\tAnd\tsave\tregularly \t\t\t\t\t\t\t\tif\tstep\t%\tsave_steps\t==\t0: \t\t\t\t\t\t\t\t\t\t\t\tsaver.save(sess,\tcheckpoint_path)\n\nWe\tstart\tby\trestoring\tthe\tmodels\tif\ta\tcheckpoint\tfile\texists,\tor\telse\twe\tjust\tinitialize\tthe\tvariables normally.\tThen\tthe\tmain\tloop\tstarts,\twhere\titeration\tcounts\tthe\ttotal\tnumber\tof\tgame\tsteps\twe\thave gone\tthrough\tsince\tthe\tprogram\tstarted,\tand\tstep\tcounts\tthe\ttotal\tnumber\tof\ttraining\tsteps\tsince\ttraining started\t(if\ta\tcheckpoint\tis\trestored,\tthe\tglobal\tstep\tis\trestored\tas\twell).\tThen\tthe\tcode\tresets\tthe\tgame (and\tskips\tthe\tfirst\tboring\tgame\tsteps,\twhere\tnothing\thappens).\tNext,\tthe\tactor\tevaluates\twhat\tto\tdo,\tand plays\tthe\tgame,\tand\tits\texperience\tis\tmemorized\tin\treplay\tmemory.\tThen,\tat\tregular\tintervals\t(after\ta warmup\tperiod),\tthe\tcritic\tgoes\tthrough\ta\ttraining\tstep.\tIt\tsamples\ta\tbatch\tof\tmemories\tand\tasks\tthe\tactor to\testimate\tthe\tQ-Values\tof\tall\tactions\tfor\tthe\tnext\tstate,\tand\tit\tapplies\tEquation\t16-7\tto\tcompute\tthe\ttarget Q-Value\ty_val.\tThe\tonly\ttricky\tpart\there\tis\tthat\twe\tmust\tmultiply\tthe\tnext\tstate’s\tQ-Values\tby\tthe continues\tvector\tto\tzero\tout\tthe\tQ-Values\tcorresponding\tto\tmemories\twhere\tthe\tgame\twas\tover.\tNext we\trun\ta\ttraining\toperation\tto\timprove\tthe\tcritic’s\tability\tto\tpredict\tQ-Values.\tFinally,\tat\tregular\tintervals we\tcopy\tthe\tcritic\tto\tthe\tactor,\tand\twe\tsave\tthe\tmodel.\n\nTIP\n\nUnfortunately,\ttraining\tis\tvery\tslow:\tif\tyou\tuse\tyour\tlaptop\tfor\ttraining,\tit\twill\ttake\tdays\tbefore\tMs.\tPac-Man\tgets\tany\tgood,\tand\tif you\tlook\tat\tthe\tlearning\tcurve,\tmeasuring\tthe\taverage\trewards\tper\tepisode,\tyou\twill\tnotice\tthat\tit\tis\textremely\tnoisy.\tAt\tsome points\tthere\tmay\tbe\tno\tapparent\tprogress\tfor\ta\tvery\tlong\ttime\tuntil\tsuddenly\tthe\tagent\tlearns\tto\tsurvive\ta\treasonable\tamount\tof time.\tAs\tmentioned\tearlier,\tone\tsolution\tis\tto\tinject\tas\tmuch\tprior\tknowledge\tas\tpossible\tinto\tthe\tmodel\t(e.g.,\tthrough preprocessing,\trewards,\tand\tso\ton),\tand\tyou\tcan\talso\ttry\tto\tbootstrap\tthe\tmodel\tby\tfirst\ttraining\tit\tto\timitate\ta\tbasic\tstrategy.\tIn any\tcase,\tRL\tstill\trequires\tquite\ta\tlot\tof\tpatience\tand\ttweaking,\tbut\tthe\tend\tresult\tis\tvery\texciting.",
      "content_length": 2477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 566,
      "content": "Exercises\n\n1.\t How\twould\tyou\tdefine\tReinforcement\tLearning?\tHow\tis\tit\tdifferent\tfrom\tregular\tsupervised\tor unsupervised\tlearning?\n\n2.\t Can\tyou\tthink\tof\tthree\tpossible\tapplications\tof\tRL\tthat\twere\tnot\tmentioned\tin\tthis\tchapter?\tFor\teach of\tthem,\twhat\tis\tthe\tenvironment?\tWhat\tis\tthe\tagent?\tWhat\tare\tpossible\tactions?\tWhat\tare\tthe rewards?\n\n3.\t What\tis\tthe\tdiscount\trate?\tCan\tthe\toptimal\tpolicy\tchange\tif\tyou\tmodify\tthe\tdiscount\trate?\n\n4.\t How\tdo\tyou\tmeasure\tthe\tperformance\tof\ta\tReinforcement\tLearning\tagent?\n\n5.\t What\tis\tthe\tcredit\tassignment\tproblem?\tWhen\tdoes\tit\toccur?\tHow\tcan\tyou\talleviate\tit?\n\n6.\t What\tis\tthe\tpoint\tof\tusing\ta\treplay\tmemory?\n\n7.\t What\tis\tan\toff-policy\tRL\talgorithm?\n\n8.\t Use\tDeep\tQ-Learning\tto\ttackle\tOpenAI\tgym’s\t“BypedalWalker-v2.”\tThe\tQ-networks\tdo\tnot\tneed\tto be\tvery\tdeep\tfor\tthis\ttask.\n\n9.\t Use\tpolicy\tgradients\tto\ttrain\tan\tagent\tto\tplay\tPong,\tthe\tfamous\tAtari\tgame\t(Pong-v0\tin\tthe\tOpenAI gym).\tBeware:\tan\tindividual\tobservation\tis\tinsufficient\tto\ttell\tthe\tdirection\tand\tspeed\tof\tthe\tball. One\tsolution\tis\tto\tpass\ttwo\tobservations\tat\ta\ttime\tto\tthe\tneural\tnetwork\tpolicy.\tTo\treduce dimensionality\tand\tspeed\tup\ttraining,\tyou\tshould\tdefinitely\tpreprocess\tthese\timages\t(crop,\tresize, and\tconvert\tthem\tto\tblack\tand\twhite),\tand\tpossibly\tmerge\tthem\tinto\ta\tsingle\timage\t(e.g.,\tby overlaying\tthem).\n\n10.\t If\tyou\thave\tabout\t$100\tto\tspare,\tyou\tcan\tpurchase\ta\tRaspberry\tPi\t3\tplus\tsome\tcheap\trobotics components,\tinstall\tTensorFlow\ton\tthe\tPi,\tand\tgo\twild!\tFor\tan\texample,\tcheck\tout\tthis\tfun\tpost\tby Lukas\tBiewald,\tor\ttake\ta\tlook\tat\tGoPiGo\tor\tBrickPi.\tWhy\tnot\ttry\tto\tbuild\ta\treal-life\tcartpole\tby training\tthe\trobot\tusing\tpolicy\tgradients?\tOr\tbuild\ta\trobotic\tspider\tthat\tlearns\tto\twalk;\tgive\tit rewards\tany\ttime\tit\tgets\tcloser\tto\tsome\tobjective\t(you\twill\tneed\tsensors\tto\tmeasure\tthe\tdistance\tto the\tobjective).\tThe\tonly\tlimit\tis\tyour\timagination.\n\nSolutions\tto\tthese\texercises\tare\tavailable\tin\tAppendix\tA.",
      "content_length": 1919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 567,
      "content": "Thank\tYou! Before\twe\tclose\tthe\tlast\tchapter\tof\tthis\tbook,\tI\twould\tlike\tto\tthank\tyou\tfor\treading\tit\tup\tto\tthe\tlast paragraph.\tI\ttruly\thope\tthat\tyou\thad\tas\tmuch\tpleasure\treading\tthis\tbook\tas\tI\thad\twriting\tit,\tand\tthat\tit\twill be\tuseful\tfor\tyour\tprojects,\tbig\tor\tsmall.\n\nIf\tyou\tfind\terrors,\tplease\tsend\tfeedback.\tMore\tgenerally,\tI\twould\tlove\tto\tknow\twhat\tyou\tthink,\tso\tplease don’t\thesitate\tto\tcontact\tme\tvia\tO’Reilly,\tor\tthrough\tthe\tageron/handson-ml\tGitHub\tproject.\n\nGoing\tforward,\tmy\tbest\tadvice\tto\tyou\tis\tto\tpractice\tand\tpractice:\ttry\tgoing\tthrough\tall\tthe\texercises\tif\tyou have\tnot\tdone\tso\talready,\tplay\twith\tthe\tJupyter\tnotebooks,\tjoin\tKaggle.com\tor\tsome\tother\tML\tcommunity, watch\tML\tcourses,\tread\tpapers,\tattend\tconferences,\tmeet\texperts.\tYou\tmay\talso\twant\tto\tstudy\tsome\ttopics that\twe\tdid\tnot\tcover\tin\tthis\tbook,\tincluding\trecommender\tsystems,\tclustering\talgorithms,\tanomaly detection\talgorithms,\tand\tgenetic\talgorithms.\n\nMy\tgreatest\thope\tis\tthat\tthis\tbook\twill\tinspire\tyou\tto\tbuild\ta\twonderful\tML\tapplication\tthat\twill\tbenefit all\tof\tus!\tWhat\twill\tit\tbe?\n\nAurélien\tGéron,\tNovember\t26th,\t2016\n\n1\n\nFor\tmore\tdetails,\tbe\tsure\tto\tcheck\tout\tRichard\tSutton\tand\tAndrew\tBarto’s\tbook\ton\tRL,\tReinforcement\tLearning:\tAn\tIntroduction\t(MIT Press),\tor\tDavid\tSilver’s\tfree\tonline\tRL\tcourse\tat\tUniversity\tCollege\tLondon.\n\n2\n\n“Playing\tAtari\twith\tDeep\tReinforcement\tLearning,”\tV.\tMnih\tet\tal.\t(2013).\n\n3\n\n“Human-level\tcontrol\tthrough\tdeep\treinforcement\tlearning,”\tV.\tMnih\tet\tal.\t(2015).\n\n4\n\nCheck\tout\tthe\tvideos\tof\tDeepMind’s\tsystem\tlearning\tto\tplay\tSpace\tInvaders,\tBreakout,\tand\tmore\tat\thttps://goo.gl/yTsH6X.\n\n5\n\nImages\t(a),\t(c),\tand\t(d)\tare\treproduced\tfrom\tWikipedia.\t(a)\tand\t(d)\tare\tin\tthe\tpublic\tdomain.\t(c)\twas\tcreated\tby\tuser\tStevertigo\tand released\tunder\tCreative\tCommons\tBY-SA\t2.0.\t(b)\tis\ta\tscreenshot\tfrom\tthe\tMs.\tPac-Man\tgame,\tcopyright\tAtari\t(the\tauthor\tbelieves\tit\tto be\tfair\tuse\tin\tthis\tchapter).\t(e)\twas\treproduced\tfrom\tPixabay,\treleased\tunder\tCreative\tCommons\tCC0.\n\n6\n\nIt\tis\toften\tbetter\tto\tgive\tthe\tpoor\tperformers\ta\tslight\tchance\tof\tsurvival,\tto\tpreserve\tsome\tdiversity\tin\tthe\t“gene\tpool.”\n\n7\n\nIf\tthere\tis\ta\tsingle\tparent,\tthis\tis\tcalled\tasexual\treproduction.\tWith\ttwo\t(or\tmore)\tparents,\tit\tis\tcalled\tsexual\treproduction.\tAn\toffspring’s genome\t(in\tthis\tcase\ta\tset\tof\tpolicy\tparameters)\tis\trandomly\tcomposed\tof\tparts\tof\tits\tparents’\tgenomes.\n\n8\n\nOpenAI\tis\ta\tnonprofit\tartificial\tintelligence\tresearch\tcompany,\tfunded\tin\tpart\tby\tElon\tMusk.\tIts\tstated\tgoal\tis\tto\tpromote\tand\tdevelop friendly\tAIs\tthat\twill\tbenefit\thumanity\t(rather\tthan\texterminate\tit).\n\n9\n\n“Simple\tStatistical\tGradient-Following\tAlgorithms\tfor\tConnectionist\tReinforcement\tLearning,”\tR.\tWilliams\t(1992).\n\n10\n\nWe\talready\tdid\tsomething\tsimilar\tin\tChapter\t11\twhen\twe\tdiscussed\tGradient\tClipping:\twe\tfirst\tcomputed\tthe\tgradients,\tthen\twe\tclipped them,\tand\tfinally\twe\tapplied\tthe\tclipped\tgradients.\n\n11\n\n“A\tMarkovian\tDecision\tProcess,”\tR.\tBellman\t(1957).",
      "content_length": 2916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 568,
      "content": "Appendix\tA.\tExercise\tSolutions\n\nNOTE\n\nSolutions\tto\tthe\tcoding\texercises\tare\tavailable\tin\tthe\tonline\tJupyter\tnotebooks\tat\thttps://github.com/ageron/handson-ml.",
      "content_length": 158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 569,
      "content": "Chapter\t1:\tThe\tMachine\tLearning\tLandscape\n\n1.\t Machine\tLearning\tis\tabout\tbuilding\tsystems\tthat\tcan\tlearn\tfrom\tdata.\tLearning\tmeans\tgetting\tbetter\tat some\ttask,\tgiven\tsome\tperformance\tmeasure.\n\n2.\t Machine\tLearning\tis\tgreat\tfor\tcomplex\tproblems\tfor\twhich\twe\thave\tno\talgorithmic\tsolution,\tto replace\tlong\tlists\tof\thand-tuned\trules,\tto\tbuild\tsystems\tthat\tadapt\tto\tfluctuating\tenvironments,\tand finally\tto\thelp\thumans\tlearn\t(e.g.,\tdata\tmining).\n\n3.\t A\tlabeled\ttraining\tset\tis\ta\ttraining\tset\tthat\tcontains\tthe\tdesired\tsolution\t(a.k.a.\ta\tlabel)\tfor\teach instance.\n\n4.\t The\ttwo\tmost\tcommon\tsupervised\ttasks\tare\tregression\tand\tclassification.\n\n5.\t Common\tunsupervised\ttasks\tinclude\tclustering,\tvisualization,\tdimensionality\treduction,\tand association\trule\tlearning.\n\n6.\t Reinforcement\tLearning\tis\tlikely\tto\tperform\tbest\tif\twe\twant\ta\trobot\tto\tlearn\tto\twalk\tin\tvarious unknown\tterrains\tsince\tthis\tis\ttypically\tthe\ttype\tof\tproblem\tthat\tReinforcement\tLearning\ttackles.\tIt might\tbe\tpossible\tto\texpress\tthe\tproblem\tas\ta\tsupervised\tor\tsemisupervised\tlearning\tproblem,\tbut\tit would\tbe\tless\tnatural.\n\n7.\t If\tyou\tdon’t\tknow\thow\tto\tdefine\tthe\tgroups,\tthen\tyou\tcan\tuse\ta\tclustering\talgorithm\t(unsupervised learning)\tto\tsegment\tyour\tcustomers\tinto\tclusters\tof\tsimilar\tcustomers.\tHowever,\tif\tyou\tknow\twhat groups\tyou\twould\tlike\tto\thave,\tthen\tyou\tcan\tfeed\tmany\texamples\tof\teach\tgroup\tto\ta\tclassification algorithm\t(supervised\tlearning),\tand\tit\twill\tclassify\tall\tyour\tcustomers\tinto\tthese\tgroups.\n\n8.\t Spam\tdetection\tis\ta\ttypical\tsupervised\tlearning\tproblem:\tthe\talgorithm\tis\tfed\tmany\temails\talong with\ttheir\tlabel\t(spam\tor\tnot\tspam).\n\n9.\t An\tonline\tlearning\tsystem\tcan\tlearn\tincrementally,\tas\topposed\tto\ta\tbatch\tlearning\tsystem.\tThis makes\tit\tcapable\tof\tadapting\trapidly\tto\tboth\tchanging\tdata\tand\tautonomous\tsystems,\tand\tof\ttraining on\tvery\tlarge\tquantities\tof\tdata.\n\n10.\t Out-of-core\talgorithms\tcan\thandle\tvast\tquantities\tof\tdata\tthat\tcannot\tfit\tin\ta\tcomputer’s\tmain memory.\tAn\tout-of-core\tlearning\talgorithm\tchops\tthe\tdata\tinto\tmini-batches\tand\tuses\tonline\tlearning techniques\tto\tlearn\tfrom\tthese\tmini-batches.\n\n11.\t An\tinstance-based\tlearning\tsystem\tlearns\tthe\ttraining\tdata\tby\theart;\tthen,\twhen\tgiven\ta\tnew\tinstance, it\tuses\ta\tsimilarity\tmeasure\tto\tfind\tthe\tmost\tsimilar\tlearned\tinstances\tand\tuses\tthem\tto\tmake predictions.\n\n12.\t A\tmodel\thas\tone\tor\tmore\tmodel\tparameters\tthat\tdetermine\twhat\tit\twill\tpredict\tgiven\ta\tnew\tinstance (e.g.,\tthe\tslope\tof\ta\tlinear\tmodel).\tA\tlearning\talgorithm\ttries\tto\tfind\toptimal\tvalues\tfor\tthese parameters\tsuch\tthat\tthe\tmodel\tgeneralizes\twell\tto\tnew\tinstances.\tA\thyperparameter\tis\ta\tparameter of\tthe\tlearning\talgorithm\titself,\tnot\tof\tthe\tmodel\t(e.g.,\tthe\tamount\tof\tregularization\tto\tapply).",
      "content_length": 2701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 570,
      "content": "13.\t Model-based\tlearning\talgorithms\tsearch\tfor\tan\toptimal\tvalue\tfor\tthe\tmodel\tparameters\tsuch\tthat\tthe model\twill\tgeneralize\twell\tto\tnew\tinstances.\tWe\tusually\ttrain\tsuch\tsystems\tby\tminimizing\ta\tcost function\tthat\tmeasures\thow\tbad\tthe\tsystem\tis\tat\tmaking\tpredictions\ton\tthe\ttraining\tdata,\tplus\ta penalty\tfor\tmodel\tcomplexity\tif\tthe\tmodel\tis\tregularized.\tTo\tmake\tpredictions,\twe\tfeed\tthe\tnew instance’s\tfeatures\tinto\tthe\tmodel’s\tprediction\tfunction,\tusing\tthe\tparameter\tvalues\tfound\tby\tthe learning\talgorithm.\n\n14.\t Some\tof\tthe\tmain\tchallenges\tin\tMachine\tLearning\tare\tthe\tlack\tof\tdata,\tpoor\tdata\tquality, nonrepresentative\tdata,\tuninformative\tfeatures,\texcessively\tsimple\tmodels\tthat\tunderfit\tthe\ttraining data,\tand\texcessively\tcomplex\tmodels\tthat\toverfit\tthe\tdata.\n\n15.\t If\ta\tmodel\tperforms\tgreat\ton\tthe\ttraining\tdata\tbut\tgeneralizes\tpoorly\tto\tnew\tinstances,\tthe\tmodel\tis likely\toverfitting\tthe\ttraining\tdata\t(or\twe\tgot\textremely\tlucky\ton\tthe\ttraining\tdata).\tPossible\tsolutions to\toverfitting\tare\tgetting\tmore\tdata,\tsimplifying\tthe\tmodel\t(selecting\ta\tsimpler\talgorithm,\treducing the\tnumber\tof\tparameters\tor\tfeatures\tused,\tor\tregularizing\tthe\tmodel),\tor\treducing\tthe\tnoise\tin\tthe training\tdata.\n\n16.\t A\ttest\tset\tis\tused\tto\testimate\tthe\tgeneralization\terror\tthat\ta\tmodel\twill\tmake\ton\tnew\tinstances,\tbefore the\tmodel\tis\tlaunched\tin\tproduction.\n\n17.\t A\tvalidation\tset\tis\tused\tto\tcompare\tmodels.\tIt\tmakes\tit\tpossible\tto\tselect\tthe\tbest\tmodel\tand\ttune\tthe hyperparameters.\n\n18.\t If\tyou\ttune\thyperparameters\tusing\tthe\ttest\tset,\tyou\trisk\toverfitting\tthe\ttest\tset,\tand\tthe\tgeneralization error\tyou\tmeasure\twill\tbe\toptimistic\t(you\tmay\tlaunch\ta\tmodel\tthat\tperforms\tworse\tthan\tyou\texpect).\n\n19.\t Cross-validation\tis\ta\ttechnique\tthat\tmakes\tit\tpossible\tto\tcompare\tmodels\t(for\tmodel\tselection\tand hyperparameter\ttuning)\twithout\tthe\tneed\tfor\ta\tseparate\tvalidation\tset.\tThis\tsaves\tprecious\ttraining data.",
      "content_length": 1887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 571,
      "content": "Chapter\t2:\tEnd-to-End\tMachine\tLearning\tProject See\tthe\tJupyter\tnotebooks\tavailable\tat\thttps://github.com/ageron/handson-ml.",
      "content_length": 123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 572,
      "content": "Chapter\t3:\tClassification See\tthe\tJupyter\tnotebooks\tavailable\tat\thttps://github.com/ageron/handson-ml.",
      "content_length": 102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 573,
      "content": "Chapter\t4:\tTraining\tModels\n\n1.\t If\tyou\thave\ta\ttraining\tset\twith\tmillions\tof\tfeatures\tyou\tcan\tuse\tStochastic\tGradient\tDescent\tor\tMini- batch\tGradient\tDescent,\tand\tperhaps\tBatch\tGradient\tDescent\tif\tthe\ttraining\tset\tfits\tin\tmemory.\tBut you\tcannot\tuse\tthe\tNormal\tEquation\tbecause\tthe\tcomputational\tcomplexity\tgrows\tquickly\t(more\tthan quadratically)\twith\tthe\tnumber\tof\tfeatures.\n\n2.\t If\tthe\tfeatures\tin\tyour\ttraining\tset\thave\tvery\tdifferent\tscales,\tthe\tcost\tfunction\twill\thave\tthe\tshape\tof an\telongated\tbowl,\tso\tthe\tGradient\tDescent\talgorithms\twill\ttake\ta\tlong\ttime\tto\tconverge.\tTo\tsolve this\tyou\tshould\tscale\tthe\tdata\tbefore\ttraining\tthe\tmodel.\tNote\tthat\tthe\tNormal\tEquation\twill\twork just\tfine\twithout\tscaling.\tMoreover,\tregularized\tmodels\tmay\tconverge\tto\ta\tsuboptimal\tsolution\tif\tthe features\tare\tnot\tscaled:\tindeed,\tsince\tregularization\tpenalizes\tlarge\tweights,\tfeatures\twith\tsmaller values\twill\ttend\tto\tbe\tignored\tcompared\tto\tfeatures\twith\tlarger\tvalues.\n\n3.\t Gradient\tDescent\tcannot\tget\tstuck\tin\ta\tlocal\tminimum\twhen\ttraining\ta\tLogistic\tRegression\tmodel because\tthe\tcost\tfunction\tis\tconvex.1\n\n4.\t If\tthe\toptimization\tproblem\tis\tconvex\t(such\tas\tLinear\tRegression\tor\tLogistic\tRegression),\tand assuming\tthe\tlearning\trate\tis\tnot\ttoo\thigh,\tthen\tall\tGradient\tDescent\talgorithms\twill\tapproach\tthe global\toptimum\tand\tend\tup\tproducing\tfairly\tsimilar\tmodels.\tHowever,\tunless\tyou\tgradually\treduce the\tlearning\trate,\tStochastic\tGD\tand\tMini-batch\tGD\twill\tnever\ttruly\tconverge;\tinstead,\tthey\twill keep\tjumping\tback\tand\tforth\taround\tthe\tglobal\toptimum.\tThis\tmeans\tthat\teven\tif\tyou\tlet\tthem\trun\tfor a\tvery\tlong\ttime,\tthese\tGradient\tDescent\talgorithms\twill\tproduce\tslightly\tdifferent\tmodels.\n\n5.\t If\tthe\tvalidation\terror\tconsistently\tgoes\tup\tafter\tevery\tepoch,\tthen\tone\tpossibility\tis\tthat\tthe\tlearning rate\tis\ttoo\thigh\tand\tthe\talgorithm\tis\tdiverging.\tIf\tthe\ttraining\terror\talso\tgoes\tup,\tthen\tthis\tis\tclearly the\tproblem\tand\tyou\tshould\treduce\tthe\tlearning\trate.\tHowever,\tif\tthe\ttraining\terror\tis\tnot\tgoing\tup, then\tyour\tmodel\tis\toverfitting\tthe\ttraining\tset\tand\tyou\tshould\tstop\ttraining.\n\n6.\t Due\tto\ttheir\trandom\tnature,\tneither\tStochastic\tGradient\tDescent\tnor\tMini-batch\tGradient\tDescent\tis guaranteed\tto\tmake\tprogress\tat\tevery\tsingle\ttraining\titeration.\tSo\tif\tyou\timmediately\tstop\ttraining when\tthe\tvalidation\terror\tgoes\tup,\tyou\tmay\tstop\tmuch\ttoo\tearly,\tbefore\tthe\toptimum\tis\treached.\tA better\toption\tis\tto\tsave\tthe\tmodel\tat\tregular\tintervals,\tand\twhen\tit\thas\tnot\timproved\tfor\ta\tlong\ttime (meaning\tit\twill\tprobably\tnever\tbeat\tthe\trecord),\tyou\tcan\trevert\tto\tthe\tbest\tsaved\tmodel.\n\n7.\t Stochastic\tGradient\tDescent\thas\tthe\tfastest\ttraining\titeration\tsince\tit\tconsiders\tonly\tone\ttraining instance\tat\ta\ttime,\tso\tit\tis\tgenerally\tthe\tfirst\tto\treach\tthe\tvicinity\tof\tthe\tglobal\toptimum\t(or\tMini- batch\tGD\twith\ta\tvery\tsmall\tmini-batch\tsize).\tHowever,\tonly\tBatch\tGradient\tDescent\twill\tactually converge,\tgiven\tenough\ttraining\ttime.\tAs\tmentioned,\tStochastic\tGD\tand\tMini-batch\tGD\twill\tbounce around\tthe\toptimum,\tunless\tyou\tgradually\treduce\tthe\tlearning\trate. 8.\t If\tthe\tvalidation\terror\tis\tmuch\thigher\tthan\tthe\ttraining\terror,\tthis\tis\tlikely\tbecause\tyour\tmodel\tis overfitting\tthe\ttraining\tset.\tOne\tway\tto\ttry\tto\tfix\tthis\tis\tto\treduce\tthe\tpolynomial\tdegree:\ta\tmodel with\tfewer\tdegrees\tof\tfreedom\tis\tless\tlikely\tto\toverfit.\tAnother\tthing\tyou\tcan\ttry\tis\tto\tregularize\tthe model\t—\tfor\texample,\tby\tadding\tan\tℓ2\tpenalty\t(Ridge)\tor\tan\tℓ1\tpenalty\t(Lasso)\tto\tthe\tcost\tfunction. This\twill\talso\treduce\tthe\tdegrees\tof\tfreedom\tof\tthe\tmodel.\tLastly,\tyou\tcan\ttry\tto\tincrease\tthe\tsize\tof",
      "content_length": 3544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 574,
      "content": "the\ttraining\tset.\n\n9.\t If\tboth\tthe\ttraining\terror\tand\tthe\tvalidation\terror\tare\talmost\tequal\tand\tfairly\thigh,\tthe\tmodel\tis\tlikely\n\nunderfitting\tthe\ttraining\tset,\twhich\tmeans\tit\thas\ta\thigh\tbias.\tYou\tshould\ttry\treducing\tthe regularization\thyperparameter\tα.\n\n10.\t Let’s\tsee:\n\nA\tmodel\twith\tsome\tregularization\ttypically\tperforms\tbetter\tthan\ta\tmodel\twithout\tany regularization,\tso\tyou\tshould\tgenerally\tprefer\tRidge\tRegression\tover\tplain\tLinear\tRegression.2\n\nLasso\tRegression\tuses\tan\tℓ1\tpenalty,\twhich\ttends\tto\tpush\tthe\tweights\tdown\tto\texactly\tzero. This\tleads\tto\tsparse\tmodels,\twhere\tall\tweights\tare\tzero\texcept\tfor\tthe\tmost\timportant\tweights. This\tis\ta\tway\tto\tperform\tfeature\tselection\tautomatically,\twhich\tis\tgood\tif\tyou\tsuspect\tthat\tonly a\tfew\tfeatures\tactually\tmatter.\tWhen\tyou\tare\tnot\tsure,\tyou\tshould\tprefer\tRidge\tRegression.\n\nElastic\tNet\tis\tgenerally\tpreferred\tover\tLasso\tsince\tLasso\tmay\tbehave\terratically\tin\tsome\tcases (when\tseveral\tfeatures\tare\tstrongly\tcorrelated\tor\twhen\tthere\tare\tmore\tfeatures\tthan\ttraining instances).\tHowever,\tit\tdoes\tadd\tan\textra\thyperparameter\tto\ttune.\tIf\tyou\tjust\twant\tLasso without\tthe\terratic\tbehavior,\tyou\tcan\tjust\tuse\tElastic\tNet\twith\tan\tl1_ratio\tclose\tto\t1.\n\n11.\t If\tyou\twant\tto\tclassify\tpictures\tas\toutdoor/indoor\tand\tdaytime/nighttime,\tsince\tthese\tare\tnot exclusive\tclasses\t(i.e.,\tall\tfour\tcombinations\tare\tpossible)\tyou\tshould\ttrain\ttwo\tLogistic\tRegression classifiers.\n\n12.\t See\tthe\tJupyter\tnotebooks\tavailable\tat\thttps://github.com/ageron/handson-ml.",
      "content_length": 1489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 575,
      "content": "Chapter\t5:\tSupport\tVector\tMachines\n\n1.\t The\tfundamental\tidea\tbehind\tSupport\tVector\tMachines\tis\tto\tfit\tthe\twidest\tpossible\t“street”\tbetween the\tclasses.\tIn\tother\twords,\tthe\tgoal\tis\tto\thave\tthe\tlargest\tpossible\tmargin\tbetween\tthe\tdecision boundary\tthat\tseparates\tthe\ttwo\tclasses\tand\tthe\ttraining\tinstances.\tWhen\tperforming\tsoft\tmargin classification,\tthe\tSVM\tsearches\tfor\ta\tcompromise\tbetween\tperfectly\tseparating\tthe\ttwo\tclasses\tand having\tthe\twidest\tpossible\tstreet\t(i.e.,\ta\tfew\tinstances\tmay\tend\tup\ton\tthe\tstreet).\tAnother\tkey\tidea\tis to\tuse\tkernels\twhen\ttraining\ton\tnonlinear\tdatasets.\n\n2.\t After\ttraining\tan\tSVM,\ta\tsupport\tvector\tis\tany\tinstance\tlocated\ton\tthe\t“street”\t(see\tthe\tprevious answer),\tincluding\tits\tborder.\tThe\tdecision\tboundary\tis\tentirely\tdetermined\tby\tthe\tsupport\tvectors. Any\tinstance\tthat\tis\tnot\ta\tsupport\tvector\t(i.e.,\toff\tthe\tstreet)\thas\tno\tinfluence\twhatsoever;\tyou\tcould remove\tthem,\tadd\tmore\tinstances,\tor\tmove\tthem\taround,\tand\tas\tlong\tas\tthey\tstay\toff\tthe\tstreet\tthey won’t\taffect\tthe\tdecision\tboundary.\tComputing\tthe\tpredictions\tonly\tinvolves\tthe\tsupport\tvectors,\tnot the\twhole\ttraining\tset.\n\n3.\t SVMs\ttry\tto\tfit\tthe\tlargest\tpossible\t“street”\tbetween\tthe\tclasses\t(see\tthe\tfirst\tanswer),\tso\tif\tthe training\tset\tis\tnot\tscaled,\tthe\tSVM\twill\ttend\tto\tneglect\tsmall\tfeatures\t(see\tFigure\t5-2).\n\n4.\t An\tSVM\tclassifier\tcan\toutput\tthe\tdistance\tbetween\tthe\ttest\tinstance\tand\tthe\tdecision\tboundary,\tand you\tcan\tuse\tthis\tas\ta\tconfidence\tscore.\tHowever,\tthis\tscore\tcannot\tbe\tdirectly\tconverted\tinto\tan estimation\tof\tthe\tclass\tprobability.\tIf\tyou\tset\tprobability=True\twhen\tcreating\tan\tSVM\tin\tScikit- Learn,\tthen\tafter\ttraining\tit\twill\tcalibrate\tthe\tprobabilities\tusing\tLogistic\tRegression\ton\tthe\tSVM’s scores\t(trained\tby\tan\tadditional\tfive-fold\tcross-validation\ton\tthe\ttraining\tdata).\tThis\twill\tadd\tthe predict_proba()\tand\tpredict_log_proba()\tmethods\tto\tthe\tSVM.\n\n5.\t This\tquestion\tapplies\tonly\tto\tlinear\tSVMs\tsince\tkernelized\tcan\tonly\tuse\tthe\tdual\tform.\tThe computational\tcomplexity\tof\tthe\tprimal\tform\tof\tthe\tSVM\tproblem\tis\tproportional\tto\tthe\tnumber\tof training\tinstances\tm,\twhile\tthe\tcomputational\tcomplexity\tof\tthe\tdual\tform\tis\tproportional\tto\ta number\tbetween\tm2\tand\tm3.\tSo\tif\tthere\tare\tmillions\tof\tinstances,\tyou\tshould\tdefinitely\tuse\tthe\tprimal form,\tbecause\tthe\tdual\tform\twill\tbe\tmuch\ttoo\tslow.\n\n6.\t If\tan\tSVM\tclassifier\ttrained\twith\tan\tRBF\tkernel\tunderfits\tthe\ttraining\tset,\tthere\tmight\tbe\ttoo\tmuch regularization.\tTo\tdecrease\tit,\tyou\tneed\tto\tincrease\tgamma\tor\tC\t(or\tboth).\n\n7.\t Let’s\tcall\tthe\tQP\tparameters\tfor\tthe\thard-margin\tproblem\tH′,\tf′,\tA′\tand\tb′\t(see\t“Quadratic Programming”).\tThe\tQP\tparameters\tfor\tthe\tsoft-margin\tproblem\thave\tm\tadditional\tparameters\t(np\t= n\t+\t1\t+\tm)\tand\tm\tadditional\tconstraints\t(nc\t=\t2m).\tThey\tcan\tbe\tdefined\tlike\tso:\n\nH\tis\tequal\tto\tH′,\tplus\tm\tcolumns\tof\t0s\ton\tthe\tright\tand\tm\trows\tof\t0s\tat\tthe\tbottom:\n\nf\tis\tequal\tto\tf′\twith\tm\tadditional\telements,\tall\tequal\tto\tthe\tvalue\tof\tthe\thyperparameter\tC.\n\nb\tis\tequal\tto\tb′\twith\tm\tadditional\telements,\tall\tequal\tto\t0.\n\nA\tis\tequal\tto\tA′,\twith\tan\textra\tm\t×\tm\tidentity\tmatrix\tIm\tappended\tto\tthe\tright,\t–\tIm\tjust\tbelow\tit,",
      "content_length": 3108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 576,
      "content": "and\tthe\trest\tfilled\twith\tzeros:\n\nFor\tthe\tsolutions\tto\texercises\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.",
      "content_length": 161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 577,
      "content": "Chapter\t6:\tDecision\tTrees\n\n1.\t The\tdepth\tof\ta\twell-balanced\tbinary\ttree\tcontaining\tm\tleaves\tis\tequal\tto\tlog2(m)3,\trounded\tup.\tA binary\tDecision\tTree\t(one\tthat\tmakes\tonly\tbinary\tdecisions,\tas\tis\tthe\tcase\tof\tall\ttrees\tin\tScikit- Learn)\twill\tend\tup\tmore\tor\tless\twell\tbalanced\tat\tthe\tend\tof\ttraining,\twith\tone\tleaf\tper\ttraining instance\tif\tit\tis\ttrained\twithout\trestrictions.\tThus,\tif\tthe\ttraining\tset\tcontains\tone\tmillion\tinstances, the\tDecision\tTree\twill\thave\ta\tdepth\tof\tlog2(106)\t≈\t20\t(actually\ta\tbit\tmore\tsince\tthe\ttree\twill generally\tnot\tbe\tperfectly\twell\tbalanced).\n\n2.\t A\tnode’s\tGini\timpurity\tis\tgenerally\tlower\tthan\tits\tparent’s.\tThis\tis\tdue\tto\tthe\tCART\ttraining algorithm’s\tcost\tfunction,\twhich\tsplits\teach\tnode\tin\ta\tway\tthat\tminimizes\tthe\tweighted\tsum\tof\tits children’s\tGini\timpurities.\tHowever,\tit\tis\tpossible\tfor\ta\tnode\tto\thave\ta\thigher\tGini\timpurity\tthan\tits parent,\tas\tlong\tas\tthis\tincrease\tis\tmore\tthan\tcompensated\tfor\tby\ta\tdecrease\tof\tthe\tother\tchild’s impurity.\tFor\texample,\tconsider\ta\tnode\tcontaining\tfour\tinstances\tof\tclass\tA\tand\t1\tof\tclass\tB.\tIts\tGini\n\nimpurity\tis\t \t=\t0.32.\tNow\tsuppose\tthe\tdataset\tis\tone-dimensional\tand\tthe\tinstances\tare lined\tup\tin\tthe\tfollowing\torder:\tA,\tB,\tA,\tA,\tA.\tYou\tcan\tverify\tthat\tthe\talgorithm\twill\tsplit\tthis\tnode after\tthe\tsecond\tinstance,\tproducing\tone\tchild\tnode\twith\tinstances\tA,\tB,\tand\tthe\tother\tchild\tnode\n\nwith\tinstances\tA,\tA,\tA.\tThe\tfirst\tchild\tnode’s\tGini\timpurity\tis\t than\tits\tparent.\tThis\tis\tcompensated\tfor\tby\tthe\tfact\tthat\tthe\tother\tnode\tis\tpure,\tso\tthe\toverall\n\n=\t0.5,\twhich\tis\thigher\n\nweighted\tGini\timpurity\tis\n\n0.5\t+\n\n=\t0.2\t,\twhich\tis\tlower\tthan\tthe\tparent’s\tGini\timpurity.\n\n3.\t If\ta\tDecision\tTree\tis\toverfitting\tthe\ttraining\tset,\tit\tmay\tbe\ta\tgood\tidea\tto\tdecrease\tmax_depth,\tsince this\twill\tconstrain\tthe\tmodel,\tregularizing\tit.\n\n4.\t Decision\tTrees\tdon’t\tcare\twhether\tor\tnot\tthe\ttraining\tdata\tis\tscaled\tor\tcentered;\tthat’s\tone\tof\tthe\tnice things\tabout\tthem.\tSo\tif\ta\tDecision\tTree\tunderfits\tthe\ttraining\tset,\tscaling\tthe\tinput\tfeatures\twill\tjust be\ta\twaste\tof\ttime.\n\n5.\t The\tcomputational\tcomplexity\tof\ttraining\ta\tDecision\tTree\tis\tO(n\t×\tm\tlog(m)).\tSo\tif\tyou\tmultiply\tthe training\tset\tsize\tby\t10,\tthe\ttraining\ttime\twill\tbe\tmultiplied\tby\tK\t=\t(n\t×\t10m\t×\tlog(10m))\t/\t(n\t×\tm\t× log(m))\t=\t10\t×\tlog(10m)\t/\tlog(m).\tIf\tm\t=\t106,\tthen\tK\t≈\t11.7,\tso\tyou\tcan\texpect\tthe\ttraining\ttime\tto\tbe roughly\t11.7\thours.\n\n6.\t Presorting\tthe\ttraining\tset\tspeeds\tup\ttraining\tonly\tif\tthe\tdataset\tis\tsmaller\tthan\ta\tfew\tthousand instances.\tIf\tit\tcontains\t100,000\tinstances,\tsetting\tpresort=True\twill\tconsiderably\tslow\tdown training.\n\nFor\tthe\tsolutions\tto\texercises\t7\tand\t8,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.",
      "content_length": 2687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 578,
      "content": "Chapter\t7:\tEnsemble\tLearning\tand\tRandom\tForests\n\n1.\t If\tyou\thave\ttrained\tfive\tdifferent\tmodels\tand\tthey\tall\tachieve\t95%\tprecision,\tyou\tcan\ttry\tcombining them\tinto\ta\tvoting\tensemble,\twhich\twill\toften\tgive\tyou\teven\tbetter\tresults.\tIt\tworks\tbetter\tif\tthe models\tare\tvery\tdifferent\t(e.g.,\tan\tSVM\tclassifier,\ta\tDecision\tTree\tclassifier,\ta\tLogistic\tRegression classifier,\tand\tso\ton).\tIt\tis\teven\tbetter\tif\tthey\tare\ttrained\ton\tdifferent\ttraining\tinstances\t(that’s\tthe whole\tpoint\tof\tbagging\tand\tpasting\tensembles),\tbut\tif\tnot\tit\twill\tstill\twork\tas\tlong\tas\tthe\tmodels\tare very\tdifferent.\n\n2.\t A\thard\tvoting\tclassifier\tjust\tcounts\tthe\tvotes\tof\teach\tclassifier\tin\tthe\tensemble\tand\tpicks\tthe\tclass that\tgets\tthe\tmost\tvotes.\tA\tsoft\tvoting\tclassifier\tcomputes\tthe\taverage\testimated\tclass\tprobability\tfor each\tclass\tand\tpicks\tthe\tclass\twith\tthe\thighest\tprobability.\tThis\tgives\thigh-confidence\tvotes\tmore weight\tand\toften\tperforms\tbetter,\tbut\tit\tworks\tonly\tif\tevery\tclassifier\tis\table\tto\testimate\tclass probabilities\t(e.g.,\tfor\tthe\tSVM\tclassifiers\tin\tScikit-Learn\tyou\tmust\tset\tprobability=True).\n\n3.\t It\tis\tquite\tpossible\tto\tspeed\tup\ttraining\tof\ta\tbagging\tensemble\tby\tdistributing\tit\tacross\tmultiple servers,\tsince\teach\tpredictor\tin\tthe\tensemble\tis\tindependent\tof\tthe\tothers.\tThe\tsame\tgoes\tfor\tpasting ensembles\tand\tRandom\tForests,\tfor\tthe\tsame\treason.\tHowever,\teach\tpredictor\tin\ta\tboosting ensemble\tis\tbuilt\tbased\ton\tthe\tprevious\tpredictor,\tso\ttraining\tis\tnecessarily\tsequential,\tand\tyou\twill not\tgain\tanything\tby\tdistributing\ttraining\tacross\tmultiple\tservers.\tRegarding\tstacking\tensembles,\tall the\tpredictors\tin\ta\tgiven\tlayer\tare\tindependent\tof\teach\tother,\tso\tthey\tcan\tbe\ttrained\tin\tparallel\ton multiple\tservers.\tHowever,\tthe\tpredictors\tin\tone\tlayer\tcan\tonly\tbe\ttrained\tafter\tthe\tpredictors\tin\tthe previous\tlayer\thave\tall\tbeen\ttrained.\n\n4.\t With\tout-of-bag\tevaluation,\teach\tpredictor\tin\ta\tbagging\tensemble\tis\tevaluated\tusing\tinstances\tthat\tit was\tnot\ttrained\ton\t(they\twere\theld\tout).\tThis\tmakes\tit\tpossible\tto\thave\ta\tfairly\tunbiased\tevaluation of\tthe\tensemble\twithout\tthe\tneed\tfor\tan\tadditional\tvalidation\tset.\tThus,\tyou\thave\tmore\tinstances available\tfor\ttraining,\tand\tyour\tensemble\tcan\tperform\tslightly\tbetter.\n\n5.\t When\tyou\tare\tgrowing\ta\ttree\tin\ta\tRandom\tForest,\tonly\ta\trandom\tsubset\tof\tthe\tfeatures\tis\tconsidered for\tsplitting\tat\teach\tnode.\tThis\tis\ttrue\tas\twell\tfor\tExtra-Trees,\tbut\tthey\tgo\tone\tstep\tfurther:\trather than\tsearching\tfor\tthe\tbest\tpossible\tthresholds,\tlike\tregular\tDecision\tTrees\tdo,\tthey\tuse\trandom thresholds\tfor\teach\tfeature.\tThis\textra\trandomness\tacts\tlike\ta\tform\tof\tregularization:\tif\ta\tRandom Forest\toverfits\tthe\ttraining\tdata,\tExtra-Trees\tmight\tperform\tbetter.\tMoreover,\tsince\tExtra-Trees don’t\tsearch\tfor\tthe\tbest\tpossible\tthresholds,\tthey\tare\tmuch\tfaster\tto\ttrain\tthan\tRandom\tForests. However,\tthey\tare\tneither\tfaster\tnor\tslower\tthan\tRandom\tForests\twhen\tmaking\tpredictions.\n\n6.\t If\tyour\tAdaBoost\tensemble\tunderfits\tthe\ttraining\tdata,\tyou\tcan\ttry\tincreasing\tthe\tnumber\tof estimators\tor\treducing\tthe\tregularization\thyperparameters\tof\tthe\tbase\testimator.\tYou\tmay\talso\ttry slightly\tincreasing\tthe\tlearning\trate.\n\n7.\t If\tyour\tGradient\tBoosting\tensemble\toverfits\tthe\ttraining\tset,\tyou\tshould\ttry\tdecreasing\tthe\tlearning rate.\tYou\tcould\talso\tuse\tearly\tstopping\tto\tfind\tthe\tright\tnumber\tof\tpredictors\t(you\tprobably\thave\ttoo many).\n\nFor\tthe\tsolutions\tto\texercises\t8\tand\t9,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat",
      "content_length": 3420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 579,
      "content": "https://github.com/ageron/handson-ml.",
      "content_length": 37,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 580,
      "content": "Chapter\t8:\tDimensionality\tReduction\n\n1.\t Motivations\tand\tdrawbacks:\n\nThe\tmain\tmotivations\tfor\tdimensionality\treduction\tare:\n\nTo\tspeed\tup\ta\tsubsequent\ttraining\talgorithm\t(in\tsome\tcases\tit\tmay\teven\tremove\tnoise\tand redundant\tfeatures,\tmaking\tthe\ttraining\talgorithm\tperform\tbetter).\n\nTo\tvisualize\tthe\tdata\tand\tgain\tinsights\ton\tthe\tmost\timportant\tfeatures.\n\nSimply\tto\tsave\tspace\t(compression).\n\nThe\tmain\tdrawbacks\tare:\n\nSome\tinformation\tis\tlost,\tpossibly\tdegrading\tthe\tperformance\tof\tsubsequent\ttraining algorithms.\n\nIt\tcan\tbe\tcomputationally\tintensive.\n\nIt\tadds\tsome\tcomplexity\tto\tyour\tMachine\tLearning\tpipelines.\n\nTransformed\tfeatures\tare\toften\thard\tto\tinterpret.\n\n2.\t The\tcurse\tof\tdimensionality\trefers\tto\tthe\tfact\tthat\tmany\tproblems\tthat\tdo\tnot\texist\tin\tlow- dimensional\tspace\tarise\tin\thigh-dimensional\tspace.\tIn\tMachine\tLearning,\tone\tcommon\tmanifestation is\tthe\tfact\tthat\trandomly\tsampled\thigh-dimensional\tvectors\tare\tgenerally\tvery\tsparse,\tincreasing\tthe risk\tof\toverfitting\tand\tmaking\tit\tvery\tdifficult\tto\tidentify\tpatterns\tin\tthe\tdata\twithout\thaving\tplenty\tof training\tdata.\n\n3.\t Once\ta\tdataset’s\tdimensionality\thas\tbeen\treduced\tusing\tone\tof\tthe\talgorithms\twe\tdiscussed,\tit\tis almost\talways\timpossible\tto\tperfectly\treverse\tthe\toperation,\tbecause\tsome\tinformation\tgets\tlost during\tdimensionality\treduction.\tMoreover,\twhile\tsome\talgorithms\t(such\tas\tPCA)\thave\ta\tsimple reverse\ttransformation\tprocedure\tthat\tcan\treconstruct\ta\tdataset\trelatively\tsimilar\tto\tthe\toriginal, other\talgorithms\t(such\tas\tT-SNE)\tdo\tnot.\n\n4.\t PCA\tcan\tbe\tused\tto\tsignificantly\treduce\tthe\tdimensionality\tof\tmost\tdatasets,\teven\tif\tthey\tare\thighly nonlinear,\tbecause\tit\tcan\tat\tleast\tget\trid\tof\tuseless\tdimensions.\tHowever,\tif\tthere\tare\tno\tuseless dimensions\t—\tfor\texample,\tthe\tSwiss\troll\t—\tthen\treducing\tdimensionality\twith\tPCA\twill\tlose\ttoo much\tinformation.\tYou\twant\tto\tunroll\tthe\tSwiss\troll,\tnot\tsquash\tit.\n\n5.\t That’s\ta\ttrick\tquestion:\tit\tdepends\ton\tthe\tdataset.\tLet’s\tlook\tat\ttwo\textreme\texamples.\tFirst,\tsuppose the\tdataset\tis\tcomposed\tof\tpoints\tthat\tare\talmost\tperfectly\taligned.\tIn\tthis\tcase,\tPCA\tcan\treduce\tthe dataset\tdown\tto\tjust\tone\tdimension\twhile\tstill\tpreserving\t95%\tof\tthe\tvariance.\tNow\timagine\tthat\tthe dataset\tis\tcomposed\tof\tperfectly\trandom\tpoints,\tscattered\tall\taround\tthe\t1,000\tdimensions.\tIn\tthis case\troughly\t950\tdimensions\tare\trequired\tto\tpreserve\t95%\tof\tthe\tvariance.\tSo\tthe\tanswer\tis,\tit depends\ton\tthe\tdataset,\tand\tit\tcould\tbe\tany\tnumber\tbetween\t1\tand\t950.\tPlotting\tthe\texplained variance\tas\ta\tfunction\tof\tthe\tnumber\tof\tdimensions\tis\tone\tway\tto\tget\ta\trough\tidea\tof\tthe\tdataset’s intrinsic\tdimensionality.",
      "content_length": 2600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 581,
      "content": "6.\t Regular\tPCA\tis\tthe\tdefault,\tbut\tit\tworks\tonly\tif\tthe\tdataset\tfits\tin\tmemory.\tIncremental\tPCA\tis\tuseful for\tlarge\tdatasets\tthat\tdon’t\tfit\tin\tmemory,\tbut\tit\tis\tslower\tthan\tregular\tPCA,\tso\tif\tthe\tdataset\tfits\tin memory\tyou\tshould\tprefer\tregular\tPCA.\tIncremental\tPCA\tis\talso\tuseful\tfor\tonline\ttasks,\twhen\tyou need\tto\tapply\tPCA\ton\tthe\tfly,\tevery\ttime\ta\tnew\tinstance\tarrives.\tRandomized\tPCA\tis\tuseful\twhen you\twant\tto\tconsiderably\treduce\tdimensionality\tand\tthe\tdataset\tfits\tin\tmemory;\tin\tthis\tcase,\tit\tis much\tfaster\tthan\tregular\tPCA.\tFinally,\tKernel\tPCA\tis\tuseful\tfor\tnonlinear\tdatasets.\n\n7.\t Intuitively,\ta\tdimensionality\treduction\talgorithm\tperforms\twell\tif\tit\teliminates\ta\tlot\tof\tdimensions from\tthe\tdataset\twithout\tlosing\ttoo\tmuch\tinformation.\tOne\tway\tto\tmeasure\tthis\tis\tto\tapply\tthe reverse\ttransformation\tand\tmeasure\tthe\treconstruction\terror.\tHowever,\tnot\tall\tdimensionality reduction\talgorithms\tprovide\ta\treverse\ttransformation.\tAlternatively,\tif\tyou\tare\tusing\tdimensionality reduction\tas\ta\tpreprocessing\tstep\tbefore\tanother\tMachine\tLearning\talgorithm\t(e.g.,\ta\tRandom\tForest classifier),\tthen\tyou\tcan\tsimply\tmeasure\tthe\tperformance\tof\tthat\tsecond\talgorithm;\tif\tdimensionality reduction\tdid\tnot\tlose\ttoo\tmuch\tinformation,\tthen\tthe\talgorithm\tshould\tperform\tjust\tas\twell\tas\twhen using\tthe\toriginal\tdataset.\n\n8.\t It\tcan\tabsolutely\tmake\tsense\tto\tchain\ttwo\tdifferent\tdimensionality\treduction\talgorithms.\tA\tcommon example\tis\tusing\tPCA\tto\tquickly\tget\trid\tof\ta\tlarge\tnumber\tof\tuseless\tdimensions,\tthen\tapplying another\tmuch\tslower\tdimensionality\treduction\talgorithm,\tsuch\tas\tLLE.\tThis\ttwo-step\tapproach\twill likely\tyield\tthe\tsame\tperformance\tas\tusing\tLLE\tonly,\tbut\tin\ta\tfraction\tof\tthe\ttime.\n\nFor\tthe\tsolutions\tto\texercises\t9\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.",
      "content_length": 1814,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 582,
      "content": "Chapter\t9:\tUp\tand\tRunning\twith\tTensorFlow\n\n1.\t Main\tbenefits\tand\tdrawbacks\tof\tcreating\ta\tcomputation\tgraph\trather\tthan\tdirectly\texecuting\tthe computations:\n\nMain\tbenefits:\n\nTensorFlow\tcan\tautomatically\tcompute\tthe\tgradients\tfor\tyou\t(using\treverse-mode autodiff).\n\nTensorFlow\tcan\ttake\tcare\tof\trunning\tthe\toperations\tin\tparallel\tin\tdifferent\tthreads.\n\nIt\tmakes\tit\teasier\tto\trun\tthe\tsame\tmodel\tacross\tdifferent\tdevices.\n\nIt\tsimplifies\tintrospection\t—\tfor\texample,\tto\tview\tthe\tmodel\tin\tTensorBoard.\n\nMain\tdrawbacks:\n\nIt\tmakes\tthe\tlearning\tcurve\tsteeper.\n\nIt\tmakes\tstep-by-step\tdebugging\tharder.\n\n2.\t Yes,\tthe\tstatement\ta_val\t=\ta.eval(session=sess)\tis\tindeed\tequivalent\tto\ta_val\t=\tsess.run(a).\n\n3.\t No,\tthe\tstatement\ta_val,\tb_val\t=\ta.eval(session=sess),\tb.eval(session=sess)\tis\tnot equivalent\tto\ta_val,\tb_val\t=\tsess.run([a,\tb]).\tIndeed,\tthe\tfirst\tstatement\truns\tthe\tgraph\ttwice (once\tto\tcompute\ta,\tonce\tto\tcompute\tb),\twhile\tthe\tsecond\tstatement\truns\tthe\tgraph\tonly\tonce.\tIf\tany of\tthese\toperations\t(or\tthe\tops\tthey\tdepend\ton)\thave\tside\teffects\t(e.g.,\ta\tvariable\tis\tmodified,\tan item\tis\tinserted\tin\ta\tqueue,\tor\ta\treader\treads\ta\tfile),\tthen\tthe\teffects\twill\tbe\tdifferent.\tIf\tthey\tdon’t have\tside\teffects,\tboth\tstatements\twill\treturn\tthe\tsame\tresult,\tbut\tthe\tsecond\tstatement\twill\tbe\tfaster than\tthe\tfirst.\n\n4.\t No,\tyou\tcannot\trun\ttwo\tgraphs\tin\tthe\tsame\tsession.\tYou\twould\thave\tto\tmerge\tthe\tgraphs\tinto\ta\tsingle graph\tfirst.\n\n5.\t In\tlocal\tTensorFlow,\tsessions\tmanage\tvariable\tvalues,\tso\tif\tyou\tcreate\ta\tgraph\tg\tcontaining\ta variable\tw,\tthen\tstart\ttwo\tthreads\tand\topen\ta\tlocal\tsession\tin\teach\tthread,\tboth\tusing\tthe\tsame\tgraph g,\tthen\teach\tsession\twill\thave\tits\town\tcopy\tof\tthe\tvariable\tw.\tHowever,\tin\tdistributed\tTensorFlow, variable\tvalues\tare\tstored\tin\tcontainers\tmanaged\tby\tthe\tcluster,\tso\tif\tboth\tsessions\tconnect\tto\tthe same\tcluster\tand\tuse\tthe\tsame\tcontainer,\tthen\tthey\twill\tshare\tthe\tsame\tvariable\tvalue\tfor\tw.\n\n6.\t A\tvariable\tis\tinitialized\twhen\tyou\tcall\tits\tinitializer,\tand\tit\tis\tdestroyed\twhen\tthe\tsession\tends.\tIn distributed\tTensorFlow,\tvariables\tlive\tin\tcontainers\ton\tthe\tcluster,\tso\tclosing\ta\tsession\twill\tnot destroy\tthe\tvariable.\tTo\tdestroy\ta\tvariable,\tyou\tneed\tto\tclear\tits\tcontainer.\n\n7.\t Variables\tand\tplaceholders\tare\textremely\tdifferent,\tbut\tbeginners\toften\tconfuse\tthem:\n\nA\tvariable\tis\tan\toperation\tthat\tholds\ta\tvalue.\tIf\tyou\trun\tthe\tvariable,\tit\treturns\tthat\tvalue. Before\tyou\tcan\trun\tit,\tyou\tneed\tto\tinitialize\tit.\tYou\tcan\tchange\tthe\tvariable’s\tvalue\t(for",
      "content_length": 2472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 583,
      "content": "example,\tby\tusing\tan\tassignment\toperation).\tIt\tis\tstateful:\tthe\tvariable\tkeeps\tthe\tsame\tvalue upon\tsuccessive\truns\tof\tthe\tgraph.\tIt\tis\ttypically\tused\tto\thold\tmodel\tparameters\tbut\talso\tfor other\tpurposes\t(e.g.,\tto\tcount\tthe\tglobal\ttraining\tstep).\n\nPlaceholders\ttechnically\tdon’t\tdo\tmuch:\tthey\tjust\thold\tinformation\tabout\tthe\ttype\tand\tshape\tof the\ttensor\tthey\trepresent,\tbut\tthey\thave\tno\tvalue.\tIn\tfact,\tif\tyou\ttry\tto\tevaluate\tan\toperation\tthat depends\ton\ta\tplaceholder,\tyou\tmust\tfeed\tTensorFlow\tthe\tvalue\tof\tthe\tplaceholder\t(using\tthe feed_dict\targument)\tor\telse\tyou\twill\tget\tan\texception.\tPlaceholders\tare\ttypically\tused\tto\tfeed training\tor\ttest\tdata\tto\tTensorFlow\tduring\tthe\texecution\tphase.\tThey\tare\talso\tuseful\tto\tpass\ta value\tto\tan\tassignment\tnode,\tto\tchange\tthe\tvalue\tof\ta\tvariable\t(e.g.,\tmodel\tweights).\n\n8.\t If\tyou\trun\tthe\tgraph\tto\tevaluate\tan\toperation\tthat\tdepends\ton\ta\tplaceholder\tbut\tyou\tdon’t\tfeed\tits value,\tyou\tget\tan\texception.\tIf\tthe\toperation\tdoes\tnot\tdepend\ton\tthe\tplaceholder,\tthen\tno\texception\tis raised.\n\n9.\t When\tyou\trun\ta\tgraph,\tyou\tcan\tfeed\tthe\toutput\tvalue\tof\tany\toperation,\tnot\tjust\tthe\tvalue\tof placeholders.\tIn\tpractice,\thowever,\tthis\tis\trather\trare\t(it\tcan\tbe\tuseful,\tfor\texample,\twhen\tyou\tare caching\tthe\toutput\tof\tfrozen\tlayers;\tsee\tChapter\t11).\n\n10.\t You\tcan\tspecify\ta\tvariable’s\tinitial\tvalue\twhen\tconstructing\tthe\tgraph,\tand\tit\twill\tbe\tinitialized\tlater when\tyou\trun\tthe\tvariable’s\tinitializer\tduring\tthe\texecution\tphase.\tIf\tyou\twant\tto\tchange\tthat variable’s\tvalue\tto\tanything\tyou\twant\tduring\tthe\texecution\tphase,\tthen\tthe\tsimplest\toption\tis\tto\tcreate an\tassignment\tnode\t(during\tthe\tgraph\tconstruction\tphase)\tusing\tthe\ttf.assign()\tfunction,\tpassing the\tvariable\tand\ta\tplaceholder\tas\tparameters.\tDuring\tthe\texecution\tphase,\tyou\tcan\trun\tthe\tassignment operation\tand\tfeed\tthe\tvariable’s\tnew\tvalue\tusing\tthe\tplaceholder.\n\nimport\ttensorflow\tas\ttf\n\nx\t=\ttf.Variable(tf.random_uniform(shape=(),\tminval=0.0,\tmaxval=1.0)) x_new_val\t=\ttf.placeholder(shape=(),\tdtype=tf.float32) x_assign\t=\ttf.assign(x,\tx_new_val)\n\nwith\ttf.Session(): \t\t\t\tx.initializer.run()\t#\trandom\tnumber\tis\tsampled\t*now* \t\t\t\tprint(x.eval())\t#\t0.646157\t(some\trandom\tnumber) \t\t\t\tx_assign.eval(feed_dict={x_new_val:\t5.0}) \t\t\t\tprint(x.eval())\t#\t5.0\n\n11.\t Reverse-mode\tautodiff\t(implemented\tby\tTensorFlow)\tneeds\tto\ttraverse\tthe\tgraph\tonly\ttwice\tin order\tto\tcompute\tthe\tgradients\tof\tthe\tcost\tfunction\twith\tregards\tto\tany\tnumber\tof\tvariables.\tOn\tthe other\thand,\tforward-mode\tautodiff\twould\tneed\tto\trun\tonce\tfor\teach\tvariable\t(so\t10\ttimes\tif\twe\twant the\tgradients\twith\tregards\tto\t10\tdifferent\tvariables).\tAs\tfor\tsymbolic\tdifferentiation,\tit\twould\tbuild a\tdifferent\tgraph\tto\tcompute\tthe\tgradients,\tso\tit\twould\tnot\ttraverse\tthe\toriginal\tgraph\tat\tall\t(except when\tbuilding\tthe\tnew\tgradients\tgraph).\tA\thighly\toptimized\tsymbolic\tdifferentiation\tsystem\tcould potentially\trun\tthe\tnew\tgradients\tgraph\tonly\tonce\tto\tcompute\tthe\tgradients\twith\tregards\tto\tall variables,\tbut\tthat\tnew\tgraph\tmay\tbe\thorribly\tcomplex\tand\tinefficient\tcompared\tto\tthe\toriginal graph.\n\n12.\t See\tthe\tJupyter\tnotebooks\tavailable\tat\thttps://github.com/ageron/handson-ml.",
      "content_length": 3117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 584,
      "content": "Chapter\t10:\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\n1.\t Here\tis\ta\tneural\tnetwork\tbased\ton\tthe\toriginal\tartificial\tneurons\tthat\tcomputes\tA represents\tthe\texclusive\tOR),\tusing\tthe\tfact\tthat\tA\t solutions\t—\tfor\texample,\tusing\tthe\tfact\tthat\tA\t \tB),\tand\tso\ton.\n\n1.\t Here\tis\ta\tneural\tnetwork\tbased\ton\tthe\toriginal\tartificial\tneurons\tthat\tcomputes\tA (¬\tA\t \tB),\tor\tthe\tfact\tthat\tA\n\nB\t=\t(A\n\n(¬\tA\n\n2.\t A\tclassical\tPerceptron\twill\tconverge\tonly\tif\tthe\tdataset\tis\tlinearly\tseparable,\tand\tit\twon’t\tbe\table\tto estimate\tclass\tprobabilities.\tIn\tcontrast,\ta\tLogistic\tRegression\tclassifier\twill\tconverge\tto\ta\tgood solution\teven\tif\tthe\tdataset\tis\tnot\tlinearly\tseparable,\tand\tit\twill\toutput\tclass\tprobabilities.\tIf\tyou change\tthe\tPerceptron’s\tactivation\tfunction\tto\tthe\tlogistic\tactivation\tfunction\t(or\tthe\tsoftmax activation\tfunction\tif\tthere\tare\tmultiple\tneurons),\tand\tif\tyou\ttrain\tit\tusing\tGradient\tDescent\t(or\tsome other\toptimization\talgorithm\tminimizing\tthe\tcost\tfunction,\ttypically\tcross\tentropy),\tthen\tit\tbecomes equivalent\tto\ta\tLogistic\tRegression\tclassifier.\n\n3.\t The\tlogistic\tactivation\tfunction\twas\ta\tkey\tingredient\tin\ttraining\tthe\tfirst\tMLPs\tbecause\tits\tderivative is\talways\tnonzero,\tso\tGradient\tDescent\tcan\talways\troll\tdown\tthe\tslope.\tWhen\tthe\tactivation function\tis\ta\tstep\tfunction,\tGradient\tDescent\tcannot\tmove,\tas\tthere\tis\tno\tslope\tat\tall.\n\n4.\t The\tstep\tfunction,\tthe\tlogistic\tfunction,\tthe\thyperbolic\ttangent,\tthe\trectified\tlinear\tunit\t(see Figure\t10-8).\tSee\tChapter\t11\tfor\tother\texamples,\tsuch\tas\tELU\tand\tvariants\tof\tthe\tReLU.\n\n5.\t Considering\tthe\tMLP\tdescribed\tin\tthe\tquestion:\tsuppose\tyou\thave\tan\tMLP\tcomposed\tof\tone\tinput layer\twith\t10\tpassthrough\tneurons,\tfollowed\tby\tone\thidden\tlayer\twith\t50\tartificial\tneurons,\tand finally\tone\toutput\tlayer\twith\t3\tartificial\tneurons.\tAll\tartificial\tneurons\tuse\tthe\tReLU\tactivation function. The\tshape\tof\tthe\tinput\tmatrix\tX\tis\tm\t×\t10,\twhere\tm\trepresents\tthe\ttraining\tbatch\tsize.\n\nThe\tshape\tof\tthe\thidden\tlayer’s\tweight\tvector\tWh\tis\t10\t×\t50\tand\tthe\tlength\tof\tits\tbias\tvector\tbh is\t50.\n\nThe\tshape\tof\tthe\toutput\tlayer’s\tweight\tvector\tWo\tis\t50\t×\t3,\tand\tthe\tlength\tof\tits\tbias\tvector\tbo\n\nB\t=\t(A",
      "content_length": 2128,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 585,
      "content": "is\t3.\n\nThe\tshape\tof\tthe\tnetwork’s\toutput\tmatrix\tY\tis\tm\t×\t3.\n\nY\t=\tReLU(ReLU(X\t·\tWh\t+\tbh)\t·\tWo\t+\tbo).\tRecall\tthat\tthe\tReLU\tfunction\tjust\tsets\tevery negative\tnumber\tin\tthe\tmatrix\tto\tzero.\tAlso\tnote\tthat\twhen\tyou\tare\tadding\ta\tbias\tvector\tto\ta matrix,\tit\tis\tadded\tto\tevery\tsingle\trow\tin\tthe\tmatrix,\twhich\tis\tcalled\tbroadcasting.\n\n6.\t To\tclassify\temail\tinto\tspam\tor\tham,\tyou\tjust\tneed\tone\tneuron\tin\tthe\toutput\tlayer\tof\ta\tneural\tnetwork —\tfor\texample,\tindicating\tthe\tprobability\tthat\tthe\temail\tis\tspam.\tYou\twould\ttypically\tuse\tthe\tlogistic activation\tfunction\tin\tthe\toutput\tlayer\twhen\testimating\ta\tprobability.\tIf\tinstead\tyou\twant\tto\ttackle MNIST,\tyou\tneed\t10\tneurons\tin\tthe\toutput\tlayer,\tand\tyou\tmust\treplace\tthe\tlogistic\tfunction\twith\tthe softmax\tactivation\tfunction,\twhich\tcan\thandle\tmultiple\tclasses,\toutputting\tone\tprobability\tper\tclass. Now,\tif\tyou\twant\tyour\tneural\tnetwork\tto\tpredict\thousing\tprices\tlike\tin\tChapter\t2,\tthen\tyou\tneed\tone output\tneuron,\tusing\tno\tactivation\tfunction\tat\tall\tin\tthe\toutput\tlayer.4\n\n7.\t Backpropagation\tis\ta\ttechnique\tused\tto\ttrain\tartificial\tneural\tnetworks.\tIt\tfirst\tcomputes\tthe\tgradients of\tthe\tcost\tfunction\twith\tregards\tto\tevery\tmodel\tparameter\t(all\tthe\tweights\tand\tbiases),\tand\tthen\tit performs\ta\tGradient\tDescent\tstep\tusing\tthese\tgradients.\tThis\tbackpropagation\tstep\tis\ttypically performed\tthousands\tor\tmillions\tof\ttimes,\tusing\tmany\ttraining\tbatches,\tuntil\tthe\tmodel\tparameters converge\tto\tvalues\tthat\t(hopefully)\tminimize\tthe\tcost\tfunction.\tTo\tcompute\tthe\tgradients, backpropagation\tuses\treverse-mode\tautodiff\t(although\tit\twasn’t\tcalled\tthat\twhen\tbackpropagation was\tinvented,\tand\tit\thas\tbeen\treinvented\tseveral\ttimes).\tReverse-mode\tautodiff\tperforms\ta\tforward pass\tthrough\ta\tcomputation\tgraph,\tcomputing\tevery\tnode’s\tvalue\tfor\tthe\tcurrent\ttraining\tbatch,\tand then\tit\tperforms\ta\treverse\tpass,\tcomputing\tall\tthe\tgradients\tat\tonce\t(see\tAppendix\tD\tfor\tmore details).\tSo\twhat’s\tthe\tdifference?\tWell,\tbackpropagation\trefers\tto\tthe\twhole\tprocess\tof\ttraining\tan artificial\tneural\tnetwork\tusing\tmultiple\tbackpropagation\tsteps,\teach\tof\twhich\tcomputes\tgradients and\tuses\tthem\tto\tperform\ta\tGradient\tDescent\tstep.\tIn\tcontrast,\treverse-mode\tautodiff\tis\ta\tsimply\ta technique\tto\tcompute\tgradients\tefficiently,\tand\tit\thappens\tto\tbe\tused\tby\tbackpropagation.\n\n8.\t Here\tis\ta\tlist\tof\tall\tthe\thyperparameters\tyou\tcan\ttweak\tin\ta\tbasic\tMLP:\tthe\tnumber\tof\thidden\tlayers, the\tnumber\tof\tneurons\tin\teach\thidden\tlayer,\tand\tthe\tactivation\tfunction\tused\tin\teach\thidden\tlayer\tand in\tthe\toutput\tlayer.5\tIn\tgeneral,\tthe\tReLU\tactivation\tfunction\t(or\tone\tof\tits\tvariants;\tsee\tChapter\t11) is\ta\tgood\tdefault\tfor\tthe\thidden\tlayers.\tFor\tthe\toutput\tlayer,\tin\tgeneral\tyou\twill\twant\tthe\tlogistic activation\tfunction\tfor\tbinary\tclassification,\tthe\tsoftmax\tactivation\tfunction\tfor\tmulticlass classification,\tor\tno\tactivation\tfunction\tfor\tregression.\t If\tthe\tMLP\toverfits\tthe\ttraining\tdata,\tyou\tcan\ttry\treducing\tthe\tnumber\tof\thidden\tlayers\tand\treducing the\tnumber\tof\tneurons\tper\thidden\tlayer.\n\n9.\t See\tthe\tJupyter\tnotebooks\tavailable\tat\thttps://github.com/ageron/handson-ml.",
      "content_length": 3065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 586,
      "content": "Chapter\t11:\tTraining\tDeep\tNeural\tNets\n\n1.\t No,\tall\tweights\tshould\tbe\tsampled\tindependently;\tthey\tshould\tnot\tall\thave\tthe\tsame\tinitial\tvalue. One\timportant\tgoal\tof\tsampling\tweights\trandomly\tis\tto\tbreak\tsymmetries:\tif\tall\tthe\tweights\thave\tthe same\tinitial\tvalue,\teven\tif\tthat\tvalue\tis\tnot\tzero,\tthen\tsymmetry\tis\tnot\tbroken\t(i.e.,\tall\tneurons\tin\ta given\tlayer\tare\tequivalent),\tand\tbackpropagation\twill\tbe\tunable\tto\tbreak\tit.\tConcretely,\tthis\tmeans that\tall\tthe\tneurons\tin\tany\tgiven\tlayer\twill\talways\thave\tthe\tsame\tweights.\tIt’s\tlike\thaving\tjust\tone neuron\tper\tlayer,\tand\tmuch\tslower.\tIt\tis\tvirtually\timpossible\tfor\tsuch\ta\tconfiguration\tto\tconverge\tto a\tgood\tsolution.\n\n2.\t It\tis\tperfectly\tfine\tto\tinitialize\tthe\tbias\tterms\tto\tzero.\tSome\tpeople\tlike\tto\tinitialize\tthem\tjust\tlike weights,\tand\tthat’s\tokay\ttoo;\tit\tdoes\tnot\tmake\tmuch\tdifference.\n\n3.\t A\tfew\tadvantages\tof\tthe\tELU\tfunction\tover\tthe\tReLU\tfunction\tare:\n\nIt\tcan\ttake\ton\tnegative\tvalues,\tso\tthe\taverage\toutput\tof\tthe\tneurons\tin\tany\tgiven\tlayer\tis typically\tcloser\tto\t0\tthan\twhen\tusing\tthe\tReLU\tactivation\tfunction\t(which\tnever\toutputs negative\tvalues).\tThis\thelps\talleviate\tthe\tvanishing\tgradients\tproblem.\n\nIt\talways\thas\ta\tnonzero\tderivative,\twhich\tavoids\tthe\tdying\tunits\tissue\tthat\tcan\taffect\tReLU units.\n\nIt\tis\tsmooth\teverywhere,\twhereas\tthe\tReLU’s\tslope\tabruptly\tjumps\tfrom\t0\tto\t1\tat\tz\t=\t0.\tSuch\tan abrupt\tchange\tcan\tslow\tdown\tGradient\tDescent\tbecause\tit\twill\tbounce\taround\tz\t=\t0.\n\n4.\t The\tELU\tactivation\tfunction\tis\ta\tgood\tdefault.\tIf\tyou\tneed\tthe\tneural\tnetwork\tto\tbe\tas\tfast\tas possible,\tyou\tcan\tuse\tone\tof\tthe\tleaky\tReLU\tvariants\tinstead\t(e.g.,\ta\tsimple\tleaky\tReLU\tusing\tthe default\thyperparameter\tvalue).\tThe\tsimplicity\tof\tthe\tReLU\tactivation\tfunction\tmakes\tit\tmany people’s\tpreferred\toption,\tdespite\tthe\tfact\tthat\tthey\tare\tgenerally\toutperformed\tby\tthe\tELU\tand\tleaky ReLU.\tHowever,\tthe\tReLU\tactivation\tfunction’s\tcapability\tof\toutputting\tprecisely\tzero\tcan\tbe\tuseful in\tsome\tcases\t(e.g.,\tsee\tChapter\t15).\tThe\thyperbolic\ttangent\t(tanh)\tcan\tbe\tuseful\tin\tthe\toutput\tlayer\tif you\tneed\tto\toutput\ta\tnumber\tbetween\t–1\tand\t1,\tbut\tnowadays\tit\tis\tnot\tused\tmuch\tin\thidden\tlayers. The\tlogistic\tactivation\tfunction\tis\talso\tuseful\tin\tthe\toutput\tlayer\twhen\tyou\tneed\tto\testimate\ta probability\t(e.g.,\tfor\tbinary\tclassification),\tbut\tit\tis\talso\trarely\tused\tin\thidden\tlayers\t(there\tare exceptions\t—\tfor\texample,\tfor\tthe\tcoding\tlayer\tof\tvariational\tautoencoders;\tsee\tChapter\t15). Finally,\tthe\tsoftmax\tactivation\tfunction\tis\tuseful\tin\tthe\toutput\tlayer\tto\toutput\tprobabilities\tfor mutually\texclusive\tclasses,\tbut\tother\tthan\tthat\tit\tis\trarely\t(if\tever)\tused\tin\thidden\tlayers.\n\n5.\t If\tyou\tset\tthe\tmomentum\thyperparameter\ttoo\tclose\tto\t1\t(e.g.,\t0.99999)\twhen\tusing\ta\n\nMomentumOptimizer,\tthen\tthe\talgorithm\twill\tlikely\tpick\tup\ta\tlot\tof\tspeed,\thopefully\troughly\ttoward the\tglobal\tminimum,\tbut\tthen\tit\twill\tshoot\tright\tpast\tthe\tminimum,\tdue\tto\tits\tmomentum.\tThen\tit\twill slow\tdown\tand\tcome\tback,\taccelerate\tagain,\tovershoot\tagain,\tand\tso\ton.\tIt\tmay\toscillate\tthis\tway many\ttimes\tbefore\tconverging,\tso\toverall\tit\twill\ttake\tmuch\tlonger\tto\tconverge\tthan\twith\ta\tsmaller momentum\tvalue.\n\n6.\t One\tway\tto\tproduce\ta\tsparse\tmodel\t(i.e.,\twith\tmost\tweights\tequal\tto\tzero)\tis\tto\ttrain\tthe\tmodel normally,\tthen\tzero\tout\ttiny\tweights.\tFor\tmore\tsparsity,\tyou\tcan\tapply\tℓ1\tregularization\tduring",
      "content_length": 3313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 587,
      "content": "training,\twhich\tpushes\tthe\toptimizer\ttoward\tsparsity.\tA\tthird\toption\tis\tto\tcombine\tℓ1\tregularization with\tdual\taveraging,\tusing\tTensorFlow’s\tFTRLOptimizer\tclass.\n\n7.\t Yes,\tdropout\tdoes\tslow\tdown\ttraining,\tin\tgeneral\troughly\tby\ta\tfactor\tof\ttwo.\tHowever,\tit\thas\tno impact\ton\tinference\tsince\tit\tis\tonly\tturned\ton\tduring\ttraining.\n\nFor\tthe\tsolutions\tto\texercises\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.",
      "content_length": 456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 588,
      "content": "Chapter\t12:\tDistributing\tTensorFlow\tAcross\tDevices\tand\tServers\n\n1.\t When\ta\tTensorFlow\tprocess\tstarts,\tit\tgrabs\tall\tthe\tavailable\tmemory\ton\tall\tGPU\tdevices\tthat\tare visible\tto\tit,\tso\tif\tyou\tget\ta\tCUDA_ERROR_OUT_OF_MEMORY\twhen\tstarting\tyour\tTensorFlow\tprogram, it\tprobably\tmeans\tthat\tother\tprocesses\tare\trunning\tthat\thave\talready\tgrabbed\tall\tthe\tmemory\ton\tat least\tone\tvisible\tGPU\tdevice\t(most\tlikely\tit\tis\tanother\tTensorFlow\tprocess).\tTo\tfix\tthis\tproblem,\ta trivial\tsolution\tis\tto\tstop\tthe\tother\tprocesses\tand\ttry\tagain.\tHowever,\tif\tyou\tneed\tall\tprocesses\tto\trun simultaneously,\ta\tsimple\toption\tis\tto\tdedicate\tdifferent\tdevices\tto\teach\tprocess,\tby\tsetting\tthe CUDA_VISIBLE_DEVICES\tenvironment\tvariable\tappropriately\tfor\teach\tdevice.\tAnother\toption\tis\tto configure\tTensorFlow\tto\tgrab\tonly\tpart\tof\tthe\tGPU\tmemory,\tinstead\tof\tall\tof\tit,\tby\tcreating\ta ConfigProto,\tsetting\tits\tgpu_options.per_process_gpu_memory_fraction\tto\tthe\tproportion\tof the\ttotal\tmemory\tthat\tit\tshould\tgrab\t(e.g.,\t0.4),\tand\tusing\tthis\tConfigProto\twhen\topening\ta\tsession. The\tlast\toption\tis\tto\ttell\tTensorFlow\tto\tgrab\tmemory\tonly\twhen\tit\tneeds\tit\tby\tsetting\tthe gpu_options.allow_growth\tto\tTrue.\tHowever,\tthis\tlast\toption\tis\tusually\tnot\trecommended because\tany\tmemory\tthat\tTensorFlow\tgrabs\tis\tnever\treleased,\tand\tit\tis\tharder\tto\tguarantee\ta repeatable\tbehavior\t(there\tmay\tbe\trace\tconditions\tdepending\ton\twhich\tprocesses\tstart\tfirst,\thow much\tmemory\tthey\tneed\tduring\ttraining,\tand\tso\ton).\n\n2.\t By\tpinning\tan\toperation\ton\ta\tdevice,\tyou\tare\ttelling\tTensorFlow\tthat\tthis\tis\twhere\tyou\twould\tlike this\toperation\tto\tbe\tplaced.\tHowever,\tsome\tconstraints\tmay\tprevent\tTensorFlow\tfrom\thonoring\tyour request.\tFor\texample,\tthe\toperation\tmay\thave\tno\timplementation\t(called\ta\tkernel)\tfor\tthat\tparticular type\tof\tdevice.\tIn\tthis\tcase,\tTensorFlow\twill\traise\tan\texception\tby\tdefault,\tbut\tyou\tcan\tconfigure\tit to\tfall\tback\tto\tthe\tCPU\tinstead\t(this\tis\tcalled\tsoft\tplacement).\tAnother\texample\tis\tan\toperation\tthat can\tmodify\ta\tvariable;\tthis\toperation\tand\tthe\tvariable\tneed\tto\tbe\tcollocated.\tSo\tthe\tdifference between\tpinning\tan\toperation\tand\tplacing\tan\toperation\tis\tthat\tpinning\tis\twhat\tyou\task\tTensorFlow (“Please\tplace\tthis\toperation\ton\tGPU\t#1”)\twhile\tplacement\tis\twhat\tTensorFlow\tactually\tends\tup doing\t(“Sorry,\tfalling\tback\tto\tthe\tCPU”).\n\n3.\t If\tyou\tare\trunning\ton\ta\tGPU-enabled\tTensorFlow\tinstallation,\tand\tyou\tjust\tuse\tthe\tdefault\tplacement, then\tif\tall\toperations\thave\ta\tGPU\tkernel\t(i.e.,\ta\tGPU\timplementation),\tyes,\tthey\twill\tall\tbe\tplaced\ton the\tfirst\tGPU.\tHowever,\tif\tone\tor\tmore\toperations\tdo\tnot\thave\ta\tGPU\tkernel,\tthen\tby\tdefault TensorFlow\twill\traise\tan\texception.\tIf\tyou\tconfigure\tTensorFlow\tto\tfall\tback\tto\tthe\tCPU\tinstead (soft\tplacement),\tthen\tall\toperations\twill\tbe\tplaced\ton\tthe\tfirst\tGPU\texcept\tthe\tones\twithout\ta\tGPU kernel\tand\tall\tthe\toperations\tthat\tmust\tbe\tcollocated\twith\tthem\t(see\tthe\tanswer\tto\tthe\tprevious exercise).\n\n4.\t Yes,\tif\tyou\tpin\ta\tvariable\tto\t\"/gpu:0\",\tit\tcan\tbe\tused\tby\toperations\tplaced\ton\t/gpu:1.\tTensorFlow will\tautomatically\ttake\tcare\tof\tadding\tthe\tappropriate\toperations\tto\ttransfer\tthe\tvariable’s\tvalue across\tdevices.\tThe\tsame\tgoes\tfor\tdevices\tlocated\ton\tdifferent\tservers\t(as\tlong\tas\tthey\tare\tpart\tof the\tsame\tcluster). 5.\t Yes,\ttwo\toperations\tplaced\ton\tthe\tsame\tdevice\tcan\trun\tin\tparallel:\tTensorFlow\tautomatically\ttakes care\tof\trunning\toperations\tin\tparallel\t(on\tdifferent\tCPU\tcores\tor\tdifferent\tGPU\tthreads),\tas\tlong\tas no\toperation\tdepends\ton\tanother\toperation’s\toutput.\tMoreover,\tyou\tcan\tstart\tmultiple\tsessions\tin parallel\tthreads\t(or\tprocesses),\tand\tevaluate\toperations\tin\teach\tthread.\tSince\tsessions\tare independent,\tTensorFlow\twill\tbe\table\tto\tevaluate\tany\toperation\tfrom\tone\tsession\tin\tparallel\twith",
      "content_length": 3701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 589,
      "content": "any\toperation\tfrom\tanother\tsession.\n\n6.\t Control\tdependencies\tare\tused\twhen\tyou\twant\tto\tpostpone\tthe\tevaluation\tof\tan\toperation\tX\tuntil after\tsome\tother\toperations\tare\trun,\teven\tthough\tthese\toperations\tare\tnot\trequired\tto\tcompute\tX. This\tis\tuseful\tin\tparticular\twhen\tX\twould\toccupy\ta\tlot\tof\tmemory\tand\tyou\tonly\tneed\tit\tlater\tin\tthe computation\tgraph,\tor\tif\tX\tuses\tup\ta\tlot\tof\tI/O\t(for\texample,\tit\trequires\ta\tlarge\tvariable\tvalue located\ton\ta\tdifferent\tdevice\tor\tserver)\tand\tyou\tdon’t\twant\tit\tto\trun\tat\tthe\tsame\ttime\tas\tother\tI/O- hungry\toperations,\tto\tavoid\tsaturating\tthe\tbandwidth.\n\n7.\t You’re\tin\tluck!\tIn\tdistributed\tTensorFlow,\tthe\tvariable\tvalues\tlive\tin\tcontainers\tmanaged\tby\tthe cluster,\tso\teven\tif\tyou\tclose\tthe\tsession\tand\texit\tthe\tclient\tprogram,\tthe\tmodel\tparameters\tare\tstill alive\tand\twell\ton\tthe\tcluster.\tYou\tsimply\tneed\tto\topen\ta\tnew\tsession\tto\tthe\tcluster\tand\tsave\tthe model\t(make\tsure\tyou\tdon’t\tcall\tthe\tvariable\tinitializers\tor\trestore\ta\tprevious\tmodel,\tas\tthis\twould destroy\tyour\tprecious\tnew\tmodel!).\n\nFor\tthe\tsolutions\tto\texercises\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.",
      "content_length": 1150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 590,
      "content": "Chapter\t13:\tConvolutional\tNeural\tNetworks\n\n1.\t These\tare\tthe\tmain\tadvantages\tof\ta\tCNN\tover\ta\tfully\tconnected\tDNN\tfor\timage\tclassification:\n\nBecause\tconsecutive\tlayers\tare\tonly\tpartially\tconnected\tand\tbecause\tit\theavily\treuses\tits weights,\ta\tCNN\thas\tmany\tfewer\tparameters\tthan\ta\tfully\tconnected\tDNN,\twhich\tmakes\tit\tmuch faster\tto\ttrain,\treduces\tthe\trisk\tof\toverfitting,\tand\trequires\tmuch\tless\ttraining\tdata.\n\nWhen\ta\tCNN\thas\tlearned\ta\tkernel\tthat\tcan\tdetect\ta\tparticular\tfeature,\tit\tcan\tdetect\tthat\tfeature anywhere\ton\tthe\timage.\tIn\tcontrast,\twhen\ta\tDNN\tlearns\ta\tfeature\tin\tone\tlocation,\tit\tcan\tdetect\tit only\tin\tthat\tparticular\tlocation.\tSince\timages\ttypically\thave\tvery\trepetitive\tfeatures,\tCNNs\tare able\tto\tgeneralize\tmuch\tbetter\tthan\tDNNs\tfor\timage\tprocessing\ttasks\tsuch\tas\tclassification, using\tfewer\ttraining\texamples.\n\nFinally,\ta\tDNN\thas\tno\tprior\tknowledge\tof\thow\tpixels\tare\torganized;\tit\tdoes\tnot\tknow\tthat nearby\tpixels\tare\tclose.\tA\tCNN’s\tarchitecture\tembeds\tthis\tprior\tknowledge.\tLower\tlayers typically\tidentify\tfeatures\tin\tsmall\tareas\tof\tthe\timages,\twhile\thigher\tlayers\tcombine\tthe\tlower- level\tfeatures\tinto\tlarger\tfeatures.\tThis\tworks\twell\twith\tmost\tnatural\timages,\tgiving\tCNNs\ta decisive\thead\tstart\tcompared\tto\tDNNs.\n\n2.\t Let’s\tcompute\thow\tmany\tparameters\tthe\tCNN\thas.\tSince\tits\tfirst\tconvolutional\tlayer\thas\t3\t×\t3 kernels,\tand\tthe\tinput\thas\tthree\tchannels\t(red,\tgreen,\tand\tblue),\tthen\teach\tfeature\tmap\thas\t3\t×\t3\t×\t3 weights,\tplus\ta\tbias\tterm.\tThat’s\t28\tparameters\tper\tfeature\tmap.\tSince\tthis\tfirst\tconvolutional\tlayer has\t100\tfeature\tmaps,\tit\thas\ta\ttotal\tof\t2,800\tparameters.\tThe\tsecond\tconvolutional\tlayer\thas\t3\t×\t3 kernels,\tand\tits\tinput\tis\tthe\tset\tof\t100\tfeature\tmaps\tof\tthe\tprevious\tlayer,\tso\teach\tfeature\tmap\thas\t3\t× 3\t×\t100\t=\t900\tweights,\tplus\ta\tbias\tterm.\tSince\tit\thas\t200\tfeature\tmaps,\tthis\tlayer\thas\t901\t×\t200\t= 180,200\tparameters.\tFinally,\tthe\tthird\tand\tlast\tconvolutional\tlayer\talso\thas\t3\t×\t3\tkernels,\tand\tits input\tis\tthe\tset\tof\t200\tfeature\tmaps\tof\tthe\tprevious\tlayers,\tso\teach\tfeature\tmap\thas\t3\t×\t3\t×\t200\t= 1,800\tweights,\tplus\ta\tbias\tterm.\tSince\tit\thas\t400\tfeature\tmaps,\tthis\tlayer\thas\ta\ttotal\tof\t1,801\t×\t400\t= 720,400\tparameters.\tAll\tin\tall,\tthe\tCNN\thas\t2,800\t+\t180,200\t+\t720,400\t=\t903,400\tparameters.\t Now\tlet’s\tcompute\thow\tmuch\tRAM\tthis\tneural\tnetwork\twill\trequire\t(at\tleast)\twhen\tmaking\ta prediction\tfor\ta\tsingle\tinstance.\tFirst\tlet’s\tcompute\tthe\tfeature\tmap\tsize\tfor\teach\tlayer.\tSince\twe\tare using\ta\tstride\tof\t2\tand\tSAME\tpadding,\tthe\thorizontal\tand\tvertical\tsize\tof\tthe\tfeature\tmaps\tare divided\tby\t2\tat\teach\tlayer\t(rounding\tup\tif\tnecessary),\tso\tas\tthe\tinput\tchannels\tare\t200\t×\t300\tpixels, the\tfirst\tlayer’s\tfeature\tmaps\tare\t100\t×\t150,\tthe\tsecond\tlayer’s\tfeature\tmaps\tare\t50\t×\t75,\tand\tthe third\tlayer’s\tfeature\tmaps\tare\t25\t×\t38.\tSince\t32\tbits\tis\t4\tbytes\tand\tthe\tfirst\tconvolutional\tlayer\thas 100\tfeature\tmaps,\tthis\tfirst\tlayer\ttakes\tup\t4\tx\t100\t×\t150\t×\t100\t=\t6\tmillion\tbytes\t(about\t5.7\tMB, considering\tthat\t1\tMB\t=\t1,024\tKB\tand\t1\tKB\t=\t1,024\tbytes).\tThe\tsecond\tlayer\ttakes\tup\t4\t×\t50\t×\t75 ×\t200\t=\t3\tmillion\tbytes\t(about\t2.9\tMB).\tFinally,\tthe\tthird\tlayer\ttakes\tup\t4\t×\t25\t×\t38\t×\t400\t= 1,520,000\tbytes\t(about\t1.4\tMB).\tHowever,\tonce\ta\tlayer\thas\tbeen\tcomputed,\tthe\tmemory\toccupied by\tthe\tprevious\tlayer\tcan\tbe\treleased,\tso\tif\teverything\tis\twell\toptimized,\tonly\t6\t+\t9\t=\t15\tmillion bytes\t(about\t14.3\tMB)\tof\tRAM\twill\tbe\trequired\t(when\tthe\tsecond\tlayer\thas\tjust\tbeen\tcomputed,\tbut the\tmemory\toccupied\tby\tthe\tfirst\tlayer\tis\tnot\treleased\tyet).\tBut\twait,\tyou\talso\tneed\tto\tadd\tthe memory\toccupied\tby\tthe\tCNN’s\tparameters.\tWe\tcomputed\tearlier\tthat\tit\thas\t903,400\tparameters, each\tusing\tup\t4\tbytes,\tso\tthis\tadds\t3,613,600\tbytes\t(about\t3.4\tMB).\tThe\ttotal\tRAM\trequired\tis\t(at least)\t18,613,600\tbytes\t(about\t17.8\tMB).",
      "content_length": 3725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 591,
      "content": "Lastly,\tlet’s\tcompute\tthe\tminimum\tamount\tof\tRAM\trequired\twhen\ttraining\tthe\tCNN\ton\ta\tmini-batch of\t50\timages.\tDuring\ttraining\tTensorFlow\tuses\tbackpropagation,\twhich\trequires\tkeeping\tall\tvalues computed\tduring\tthe\tforward\tpass\tuntil\tthe\treverse\tpass\tbegins.\tSo\twe\tmust\tcompute\tthe\ttotal\tRAM required\tby\tall\tlayers\tfor\ta\tsingle\tinstance\tand\tmultiply\tthat\tby\t50!\tAt\tthat\tpoint\tlet’s\tstart\tcounting\tin megabytes\trather\tthan\tbytes.\tWe\tcomputed\tbefore\tthat\tthe\tthree\tlayers\trequire\trespectively\t5.7,\t2.9, and\t1.4\tMB\tfor\teach\tinstance.\tThat’s\ta\ttotal\tof\t10.0\tMB\tper\tinstance.\tSo\tfor\t50\tinstances\tthe\ttotal RAM\tis\t500\tMB.\tAdd\tto\tthat\tthe\tRAM\trequired\tby\tthe\tinput\timages,\twhich\tis\t50\t×\t4\t×\t200\t×\t300\t× 3\t=\t36\tmillion\tbytes\t(about\t34.3\tMB),\tplus\tthe\tRAM\trequired\tfor\tthe\tmodel\tparameters,\twhich\tis about\t3.4\tMB\t(computed\tearlier),\tplus\tsome\tRAM\tfor\tthe\tgradients\t(we\twill\tneglect\tthem\tsince\tthey can\tbe\treleased\tgradually\tas\tbackpropagation\tgoes\tdown\tthe\tlayers\tduring\tthe\treverse\tpass).\tWe\tare up\tto\ta\ttotal\tof\troughly\t500.0\t+\t34.3\t+\t3.4\t=\t537.7\tMB.\tAnd\tthat’s\treally\tan\toptimistic\tbare minimum.\n\n3.\t If\tyour\tGPU\truns\tout\tof\tmemory\twhile\ttraining\ta\tCNN,\there\tare\tfive\tthings\tyou\tcould\ttry\tto\tsolve\tthe problem\t(other\tthan\tpurchasing\ta\tGPU\twith\tmore\tRAM):\n\nReduce\tthe\tmini-batch\tsize.\n\nReduce\tdimensionality\tusing\ta\tlarger\tstride\tin\tone\tor\tmore\tlayers.\n\nRemove\tone\tor\tmore\tlayers.\n\nUse\t16-bit\tfloats\tinstead\tof\t32-bit\tfloats.\n\nDistribute\tthe\tCNN\tacross\tmultiple\tdevices.\n\n4.\t A\tmax\tpooling\tlayer\thas\tno\tparameters\tat\tall,\twhereas\ta\tconvolutional\tlayer\thas\tquite\ta\tfew\t(see\tthe previous\tquestions).\n\n5.\t A\tlocal\tresponse\tnormalization\tlayer\tmakes\tthe\tneurons\tthat\tmost\tstrongly\tactivate\tinhibit\tneurons at\tthe\tsame\tlocation\tbut\tin\tneighboring\tfeature\tmaps,\twhich\tencourages\tdifferent\tfeature\tmaps\tto specialize\tand\tpushes\tthem\tapart,\tforcing\tthem\tto\texplore\ta\twider\trange\tof\tfeatures.\tIt\tis\ttypically used\tin\tthe\tlower\tlayers\tto\thave\ta\tlarger\tpool\tof\tlow-level\tfeatures\tthat\tthe\tupper\tlayers\tcan\tbuild upon.\n\n6.\t The\tmain\tinnovations\tin\tAlexNet\tcompared\tto\tLeNet-5\tare\t(1)\tit\tis\tmuch\tlarger\tand\tdeeper,\tand\t(2) it\tstacks\tconvolutional\tlayers\tdirectly\ton\ttop\tof\teach\tother,\tinstead\tof\tstacking\ta\tpooling\tlayer\ton\ttop of\teach\tconvolutional\tlayer.\tThe\tmain\tinnovation\tin\tGoogLeNet\tis\tthe\tintroduction\tof\tinception modules,\twhich\tmake\tit\tpossible\tto\thave\ta\tmuch\tdeeper\tnet\tthan\tprevious\tCNN\tarchitectures,\twith fewer\tparameters.\tFinally,\tResNet’s\tmain\tinnovation\tis\tthe\tintroduction\tof\tskip\tconnections,\twhich make\tit\tpossible\tto\tgo\twell\tbeyond\t100\tlayers.\tArguably,\tits\tsimplicity\tand\tconsistency\tare\talso rather\tinnovative.\n\nFor\tthe\tsolutions\tto\texercises\t7,\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.",
      "content_length": 2743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 592,
      "content": "Chapter\t14:\tRecurrent\tNeural\tNetworks\n\n1.\t Here\tare\ta\tfew\tRNN\tapplications:\n\nFor\ta\tsequence-to-sequence\tRNN:\tpredicting\tthe\tweather\t(or\tany\tother\ttime\tseries),\tmachine translation\t(using\tan\tencoder–decoder\tarchitecture),\tvideo\tcaptioning,\tspeech\tto\ttext,\tmusic generation\t(or\tother\tsequence\tgeneration),\tidentifying\tthe\tchords\tof\ta\tsong.\n\nFor\ta\tsequence-to-vector\tRNN:\tclassifying\tmusic\tsamples\tby\tmusic\tgenre,\tanalyzing\tthe sentiment\tof\ta\tbook\treview,\tpredicting\twhat\tword\tan\taphasic\tpatient\tis\tthinking\tof\tbased\ton readings\tfrom\tbrain\timplants,\tpredicting\tthe\tprobability\tthat\ta\tuser\twill\twant\tto\twatch\ta\tmovie based\ton\ther\twatch\thistory\t(this\tis\tone\tof\tmany\tpossible\timplementations\tof\tcollaborative filtering).\n\nFor\ta\tvector-to-sequence\tRNN:\timage\tcaptioning,\tcreating\ta\tmusic\tplaylist\tbased\ton\tan embedding\tof\tthe\tcurrent\tartist,\tgenerating\ta\tmelody\tbased\ton\ta\tset\tof\tparameters,\tlocating pedestrians\tin\ta\tpicture\t(e.g.,\ta\tvideo\tframe\tfrom\ta\tself-driving\tcar’s\tcamera).\n\n2.\t In\tgeneral,\tif\tyou\ttranslate\ta\tsentence\tone\tword\tat\ta\ttime,\tthe\tresult\twill\tbe\tterrible.\tFor\texample,\tthe French\tsentence\t“Je\tvous\ten\tprie”\tmeans\t“You\tare\twelcome,”\tbut\tif\tyou\ttranslate\tit\tone\tword\tat\ta time,\tyou\tget\t“I\tyou\tin\tpray.”\tHuh?\tIt\tis\tmuch\tbetter\tto\tread\tthe\twhole\tsentence\tfirst\tand\tthen\ttranslate it.\tA\tplain\tsequence-to-sequence\tRNN\twould\tstart\ttranslating\ta\tsentence\timmediately\tafter\treading the\tfirst\tword,\twhile\tan\tencoder–decoder\tRNN\twill\tfirst\tread\tthe\twhole\tsentence\tand\tthen\ttranslate it.\tThat\tsaid,\tone\tcould\timagine\ta\tplain\tsequence-to-sequence\tRNN\tthat\twould\toutput\tsilence whenever\tit\tis\tunsure\tabout\twhat\tto\tsay\tnext\t(just\tlike\thuman\ttranslators\tdo\twhen\tthey\tmust\ttranslate a\tlive\tbroadcast).\n\n3.\t To\tclassify\tvideos\tbased\ton\tthe\tvisual\tcontent,\tone\tpossible\tarchitecture\tcould\tbe\tto\ttake\t(say)\tone frame\tper\tsecond,\tthen\trun\teach\tframe\tthrough\ta\tconvolutional\tneural\tnetwork,\tfeed\tthe\toutput\tof\tthe CNN\tto\ta\tsequence-to-vector\tRNN,\tand\tfinally\trun\tits\toutput\tthrough\ta\tsoftmax\tlayer,\tgiving\tyou\tall the\tclass\tprobabilities.\tFor\ttraining\tyou\twould\tjust\tuse\tcross\tentropy\tas\tthe\tcost\tfunction.\tIf\tyou wanted\tto\tuse\tthe\taudio\tfor\tclassification\tas\twell,\tyou\tcould\tconvert\tevery\tsecond\tof\taudio\tto\ta spectrograph,\tfeed\tthis\tspectrograph\tto\ta\tCNN,\tand\tfeed\tthe\toutput\tof\tthis\tCNN\tto\tthe\tRNN\t(along with\tthe\tcorresponding\toutput\tof\tthe\tother\tCNN).\n\n4.\t Building\tan\tRNN\tusing\tdynamic_rnn()\trather\tthan\tstatic_rnn()\toffers\tseveral\tadvantages:\n\nIt\tis\tbased\ton\ta\twhile_loop()\toperation\tthat\tis\table\tto\tswap\tthe\tGPU’s\tmemory\tto\tthe\tCPU’s memory\tduring\tbackpropagation,\tavoiding\tout-of-memory\terrors.\n\nIt\tis\targuably\teasier\tto\tuse,\tas\tit\tcan\tdirectly\ttake\ta\tsingle\ttensor\tas\tinput\tand\toutput\t(covering all\ttime\tsteps),\trather\tthan\ta\tlist\tof\ttensors\t(one\tper\ttime\tstep).\tNo\tneed\tto\tstack,\tunstack,\tor transpose.\n\nIt\tgenerates\ta\tsmaller\tgraph,\teasier\tto\tvisualize\tin\tTensorBoard.\n\n5.\t To\thandle\tvariable\tlength\tinput\tsequences,\tthe\tsimplest\toption\tis\tto\tset\tthe\tsequence_length\n\nparameter\twhen\tcalling\tthe\tstatic_rnn()\tor\tdynamic_rnn()\tfunctions.\tAnother\toption\tis\tto\tpad",
      "content_length": 3064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 593,
      "content": "the\tsmaller\tinputs\t(e.g.,\twith\tzeros)\tto\tmake\tthem\tthe\tsame\tsize\tas\tthe\tlargest\tinput\t(this\tmay\tbe faster\tthan\tthe\tfirst\toption\tif\tthe\tinput\tsequences\tall\thave\tvery\tsimilar\tlengths).\tTo\thandle\tvariable- length\toutput\tsequences,\tif\tyou\tknow\tin\tadvance\tthe\tlength\tof\teach\toutput\tsequence,\tyou\tcan\tuse\tthe sequence_length\tparameter\t(for\texample,\tconsider\ta\tsequence-to-sequence\tRNN\tthat\tlabels\tevery frame\tin\ta\tvideo\twith\ta\tviolence\tscore:\tthe\toutput\tsequence\twill\tbe\texactly\tthe\tsame\tlength\tas\tthe input\tsequence).\tIf\tyou\tdon’t\tknow\tin\tadvance\tthe\tlength\tof\tthe\toutput\tsequence,\tyou\tcan\tuse\tthe padding\ttrick:\talways\toutput\tthe\tsame\tsize\tsequence,\tbut\tignore\tany\toutputs\tthat\tcome\tafter\tthe\tend- of-sequence\ttoken\t(by\tignoring\tthem\twhen\tcomputing\tthe\tcost\tfunction).\n\n6.\t To\tdistribute\ttraining\tand\texecution\tof\ta\tdeep\tRNN\tacross\tmultiple\tGPUs,\ta\tcommon\ttechnique\tis simply\tto\tplace\teach\tlayer\ton\ta\tdifferent\tGPU\t(see\tChapter\t12).\n\nFor\tthe\tsolutions\tto\texercises\t7,\t8,\tand\t9,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.",
      "content_length": 1057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 594,
      "content": "Chapter\t15:\tAutoencoders\n\n1.\t Here\tare\tsome\tof\tthe\tmain\ttasks\tthat\tautoencoders\tare\tused\tfor:\n\nFeature\textraction\n\nUnsupervised\tpretraining\n\nDimensionality\treduction\n\nGenerative\tmodels\n\nAnomaly\tdetection\t(an\tautoencoder\tis\tgenerally\tbad\tat\treconstructing\toutliers)\n\n2.\t If\tyou\twant\tto\ttrain\ta\tclassifier\tand\tyou\thave\tplenty\tof\tunlabeled\ttraining\tdata,\tbut\tonly\ta\tfew thousand\tlabeled\tinstances,\tthen\tyou\tcould\tfirst\ttrain\ta\tdeep\tautoencoder\ton\tthe\tfull\tdataset\t(labeled +\tunlabeled),\tthen\treuse\tits\tlower\thalf\tfor\tthe\tclassifier\t(i.e.,\treuse\tthe\tlayers\tup\tto\tthe\tcodings\tlayer, included)\tand\ttrain\tthe\tclassifier\tusing\tthe\tlabeled\tdata.\tIf\tyou\thave\tlittle\tlabeled\tdata,\tyou\tprobably want\tto\tfreeze\tthe\treused\tlayers\twhen\ttraining\tthe\tclassifier.\n\n3.\t The\tfact\tthat\tan\tautoencoder\tperfectly\treconstructs\tits\tinputs\tdoes\tnot\tnecessarily\tmean\tthat\tit\tis\ta good\tautoencoder;\tperhaps\tit\tis\tsimply\tan\tovercomplete\tautoencoder\tthat\tlearned\tto\tcopy\tits\tinputs to\tthe\tcodings\tlayer\tand\tthen\tto\tthe\toutputs.\tIn\tfact,\teven\tif\tthe\tcodings\tlayer\tcontained\ta\tsingle neuron,\tit\twould\tbe\tpossible\tfor\ta\tvery\tdeep\tautoencoder\tto\tlearn\tto\tmap\teach\ttraining\tinstance\tto\ta different\tcoding\t(e.g.,\tthe\tfirst\tinstance\tcould\tbe\tmapped\tto\t0.001,\tthe\tsecond\tto\t0.002,\tthe\tthird\tto 0.003,\tand\tso\ton),\tand\tit\tcould\tlearn\t“by\theart”\tto\treconstruct\tthe\tright\ttraining\tinstance\tfor\teach coding.\tIt\twould\tperfectly\treconstruct\tits\tinputs\twithout\treally\tlearning\tany\tuseful\tpattern\tin\tthe\tdata. In\tpractice\tsuch\ta\tmapping\tis\tunlikely\tto\thappen,\tbut\tit\tillustrates\tthe\tfact\tthat\tperfect\treconstructions are\tnot\ta\tguarantee\tthat\tthe\tautoencoder\tlearned\tanything\tuseful.\tHowever,\tif\tit\tproduces\tvery\tbad reconstructions,\tthen\tit\tis\talmost\tguaranteed\tto\tbe\ta\tbad\tautoencoder.\tTo\tevaluate\tthe\tperformance\tof an\tautoencoder,\tone\toption\tis\tto\tmeasure\tthe\treconstruction\tloss\t(e.g.,\tcompute\tthe\tMSE,\tthe\tmean square\tof\tthe\toutputs\tminus\tthe\tinputs).\tAgain,\ta\thigh\treconstruction\tloss\tis\ta\tgood\tsign\tthat\tthe autoencoder\tis\tbad,\tbut\ta\tlow\treconstruction\tloss\tis\tnot\ta\tguarantee\tthat\tit\tis\tgood.\tYou\tshould\talso evaluate\tthe\tautoencoder\taccording\tto\twhat\tit\twill\tbe\tused\tfor.\tFor\texample,\tif\tyou\tare\tusing\tit\tfor unsupervised\tpretraining\tof\ta\tclassifier,\tthen\tyou\tshould\talso\tevaluate\tthe\tclassifier’s\tperformance.\n\n4.\t An\tundercomplete\tautoencoder\tis\tone\twhose\tcodings\tlayer\tis\tsmaller\tthan\tthe\tinput\tand\toutput layers.\tIf\tit\tis\tlarger,\tthen\tit\tis\tan\tovercomplete\tautoencoder.\tThe\tmain\trisk\tof\tan\texcessively undercomplete\tautoencoder\tis\tthat\tit\tmay\tfail\tto\treconstruct\tthe\tinputs.\tThe\tmain\trisk\tof\tan overcomplete\tautoencoder\tis\tthat\tit\tmay\tjust\tcopy\tthe\tinputs\tto\tthe\toutputs,\twithout\tlearning\tany useful\tfeature.\n\n5.\t To\ttie\tthe\tweights\tof\tan\tencoder\tlayer\tand\tits\tcorresponding\tdecoder\tlayer,\tyou\tsimply\tmake\tthe decoder\tweights\tequal\tto\tthe\ttranspose\tof\tthe\tencoder\tweights.\tThis\treduces\tthe\tnumber\tof parameters\tin\tthe\tmodel\tby\thalf,\toften\tmaking\ttraining\tconverge\tfaster\twith\tless\ttraining\tdata,\tand reducing\tthe\trisk\tof\toverfitting\tthe\ttraining\tset.",
      "content_length": 3016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 595,
      "content": "6.\t To\tvisualize\tthe\tfeatures\tlearned\tby\tthe\tlower\tlayer\tof\ta\tstacked\tautoencoder,\ta\tcommon\ttechnique\tis simply\tto\tplot\tthe\tweights\tof\teach\tneuron,\tby\treshaping\teach\tweight\tvector\tto\tthe\tsize\tof\tan\tinput image\t(e.g.,\tfor\tMNIST,\treshaping\ta\tweight\tvector\tof\tshape\t[784]\tto\t[28,\t28]).\tTo\tvisualize\tthe features\tlearned\tby\thigher\tlayers,\tone\ttechnique\tis\tto\tdisplay\tthe\ttraining\tinstances\tthat\tmost\tactivate each\tneuron.\n\n7.\t A\tgenerative\tmodel\tis\ta\tmodel\tcapable\tof\trandomly\tgenerating\toutputs\tthat\tresemble\tthe\ttraining instances.\tFor\texample,\tonce\ttrained\tsuccessfully\ton\tthe\tMNIST\tdataset,\ta\tgenerative\tmodel\tcan\tbe used\tto\trandomly\tgenerate\trealistic\timages\tof\tdigits.\tThe\toutput\tdistribution\tis\ttypically\tsimilar\tto the\ttraining\tdata.\tFor\texample,\tsince\tMNIST\tcontains\tmany\timages\tof\teach\tdigit,\tthe\tgenerative model\twould\toutput\troughly\tthe\tsame\tnumber\tof\timages\tof\teach\tdigit.\tSome\tgenerative\tmodels\tcan be\tparametrized\t—\tfor\texample,\tto\tgenerate\tonly\tsome\tkinds\tof\toutputs.\tAn\texample\tof\ta\tgenerative autoencoder\tis\tthe\tvariational\tautoencoder.\n\nFor\tthe\tsolutions\tto\texercises\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.",
      "content_length": 1180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 596,
      "content": "Chapter\t16:\tReinforcement\tLearning\n\n1.\t Reinforcement\tLearning\tis\tan\tarea\tof\tMachine\tLearning\taimed\tat\tcreating\tagents\tcapable\tof\ttaking actions\tin\tan\tenvironment\tin\ta\tway\tthat\tmaximizes\trewards\tover\ttime.\tThere\tare\tmany\tdifferences between\tRL\tand\tregular\tsupervised\tand\tunsupervised\tlearning.\tHere\tare\ta\tfew: In\tsupervised\tand\tunsupervised\tlearning,\tthe\tgoal\tis\tgenerally\tto\tfind\tpatterns\tin\tthe\tdata.\tIn Reinforcement\tLearning,\tthe\tgoal\tis\tto\tfind\ta\tgood\tpolicy.\n\nUnlike\tin\tsupervised\tlearning,\tthe\tagent\tis\tnot\texplicitly\tgiven\tthe\t“right”\tanswer.\tIt\tmust\tlearn by\ttrial\tand\terror.\n\nUnlike\tin\tunsupervised\tlearning,\tthere\tis\ta\tform\tof\tsupervision,\tthrough\trewards.\tWe\tdo\tnot\ttell the\tagent\thow\tto\tperform\tthe\ttask,\tbut\twe\tdo\ttell\tit\twhen\tit\tis\tmaking\tprogress\tor\twhen\tit\tis failing.\n\nA\tReinforcement\tLearning\tagent\tneeds\tto\tfind\tthe\tright\tbalance\tbetween\texploring\tthe environment,\tlooking\tfor\tnew\tways\tof\tgetting\trewards,\tand\texploiting\tsources\tof\trewards\tthat\tit already\tknows.\tIn\tcontrast,\tsupervised\tand\tunsupervised\tlearning\tsystems\tgenerally\tdon’t\tneed to\tworry\tabout\texploration;\tthey\tjust\tfeed\ton\tthe\ttraining\tdata\tthey\tare\tgiven.\n\nIn\tsupervised\tand\tunsupervised\tlearning,\ttraining\tinstances\tare\ttypically\tindependent\t(in\tfact, they\tare\tgenerally\tshuffled).\tIn\tReinforcement\tLearning,\tconsecutive\tobservations\tare\tgenerally not\tindependent.\tAn\tagent\tmay\tremain\tin\tthe\tsame\tregion\tof\tthe\tenvironment\tfor\ta\twhile\tbefore it\tmoves\ton,\tso\tconsecutive\tobservations\twill\tbe\tvery\tcorrelated.\tIn\tsome\tcases\ta\treplay memory\tis\tused\tto\tensure\tthat\tthe\ttraining\talgorithm\tgets\tfairly\tindependent\tobservations.\n\n2.\t Here\tare\ta\tfew\tpossible\tapplications\tof\tReinforcement\tLearning,\tother\tthan\tthose\tmentioned\tin Chapter\t16:\n\nMusic\tpersonalization\n\nThe\tenvironment\tis\ta\tuser’s\tpersonalized\tweb\tradio.\tThe\tagent\tis\tthe\tsoftware\tdeciding\twhat song\tto\tplay\tnext\tfor\tthat\tuser.\tIts\tpossible\tactions\tare\tto\tplay\tany\tsong\tin\tthe\tcatalog\t(it\tmust\ttry to\tchoose\ta\tsong\tthe\tuser\twill\tenjoy)\tor\tto\tplay\tan\tadvertisement\t(it\tmust\ttry\tto\tchoose\tan\tad that\tthe\tuser\twill\tbe\tinterested\tin).\tIt\tgets\ta\tsmall\treward\tevery\ttime\tthe\tuser\tlistens\tto\ta\tsong,\ta larger\treward\tevery\ttime\tthe\tuser\tlistens\tto\tan\tad,\ta\tnegative\treward\twhen\tthe\tuser\tskips\ta\tsong or\tan\tad,\tand\ta\tvery\tnegative\treward\tif\tthe\tuser\tleaves.\n\nMarketing\n\nThe\tenvironment\tis\tyour\tcompany’s\tmarketing\tdepartment.\tThe\tagent\tis\tthe\tsoftware\tthat\tdefines which\tcustomers\ta\tmailing\tcampaign\tshould\tbe\tsent\tto,\tgiven\ttheir\tprofile\tand\tpurchase\thistory (for\teach\tcustomer\tit\thas\ttwo\tpossible\tactions:\tsend\tor\tdon’t\tsend).\tIt\tgets\ta\tnegative\treward for\tthe\tcost\tof\tthe\tmailing\tcampaign,\tand\ta\tpositive\treward\tfor\testimated\trevenue\tgenerated from\tthis\tcampaign.\n\nProduct\tdelivery",
      "content_length": 2716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 597,
      "content": "Let\tthe\tagent\tcontrol\ta\tfleet\tof\tdelivery\ttrucks,\tdeciding\twhat\tthey\tshould\tpick\tup\tat\tthe\tdepots, where\tthey\tshould\tgo,\twhat\tthey\tshould\tdrop\toff,\tand\tso\ton.\tThey\twould\tget\tpositive\trewards for\teach\tproduct\tdelivered\ton\ttime,\tand\tnegative\trewards\tfor\tlate\tdeliveries.\n\n3.\t When\testimating\tthe\tvalue\tof\tan\taction,\tReinforcement\tLearning\talgorithms\ttypically\tsum\tall\tthe rewards\tthat\tthis\taction\tled\tto,\tgiving\tmore\tweight\tto\timmediate\trewards,\tand\tless\tweight\tto\tlater rewards\t(considering\tthat\tan\taction\thas\tmore\tinfluence\ton\tthe\tnear\tfuture\tthan\ton\tthe\tdistant\tfuture). To\tmodel\tthis,\ta\tdiscount\trate\tis\ttypically\tapplied\tat\teach\ttime\tstep.\tFor\texample,\twith\ta\tdiscount rate\tof\t0.9,\ta\treward\tof\t100\tthat\tis\treceived\ttwo\ttime\tsteps\tlater\tis\tcounted\tas\tonly\t0.92\t×\t100\t=\t81 when\tyou\tare\testimating\tthe\tvalue\tof\tthe\taction.\tYou\tcan\tthink\tof\tthe\tdiscount\trate\tas\ta\tmeasure\tof how\tmuch\tthe\tfuture\tis\tvalued\trelative\tto\tthe\tpresent:\tif\tit\tis\tvery\tclose\tto\t1,\tthen\tthe\tfuture\tis\tvalued almost\tas\tmuch\tas\tthe\tpresent.\tIf\tit\tis\tclose\tto\t0,\tthen\tonly\timmediate\trewards\tmatter.\tOf\tcourse,\tthis impacts\tthe\toptimal\tpolicy\ttremendously:\tif\tyou\tvalue\tthe\tfuture,\tyou\tmay\tbe\twilling\tto\tput\tup\twith\ta lot\tof\timmediate\tpain\tfor\tthe\tprospect\tof\teventual\trewards,\twhile\tif\tyou\tdon’t\tvalue\tthe\tfuture,\tyou will\tjust\tgrab\tany\timmediate\treward\tyou\tcan\tfind,\tnever\tinvesting\tin\tthe\tfuture.\n\n4.\t To\tmeasure\tthe\tperformance\tof\ta\tReinforcement\tLearning\tagent,\tyou\tcan\tsimply\tsum\tup\tthe\trewards it\tgets.\tIn\ta\tsimulated\tenvironment,\tyou\tcan\trun\tmany\tepisodes\tand\tlook\tat\tthe\ttotal\trewards\tit\tgets\ton average\t(and\tpossibly\tlook\tat\tthe\tmin,\tmax,\tstandard\tdeviation,\tand\tso\ton).\n\n5.\t The\tcredit\tassignment\tproblem\tis\tthe\tfact\tthat\twhen\ta\tReinforcement\tLearning\tagent\treceives\ta reward,\tit\thas\tno\tdirect\tway\tof\tknowing\twhich\tof\tits\tprevious\tactions\tcontributed\tto\tthis\treward.\tIt typically\toccurs\twhen\tthere\tis\ta\tlarge\tdelay\tbetween\tan\taction\tand\tthe\tresulting\trewards\t(e.g.,\tduring a\tgame\tof\tAtari’s\tPong,\tthere\tmay\tbe\ta\tfew\tdozen\ttime\tsteps\tbetween\tthe\tmoment\tthe\tagent\thits\tthe ball\tand\tthe\tmoment\tit\twins\tthe\tpoint).\tOne\tway\tto\talleviate\tit\tis\tto\tprovide\tthe\tagent\twith\tshorter- term\trewards,\twhen\tpossible.\tThis\tusually\trequires\tprior\tknowledge\tabout\tthe\ttask.\tFor\texample,\tif we\twant\tto\tbuild\tan\tagent\tthat\twill\tlearn\tto\tplay\tchess,\tinstead\tof\tgiving\tit\ta\treward\tonly\twhen\tit wins\tthe\tgame,\twe\tcould\tgive\tit\ta\treward\tevery\ttime\tit\tcaptures\tone\tof\tthe\topponent’s\tpieces.\n\n6.\t An\tagent\tcan\toften\tremain\tin\tthe\tsame\tregion\tof\tits\tenvironment\tfor\ta\twhile,\tso\tall\tof\tits\texperiences will\tbe\tvery\tsimilar\tfor\tthat\tperiod\tof\ttime.\tThis\tcan\tintroduce\tsome\tbias\tin\tthe\tlearning\talgorithm.\tIt may\ttune\tits\tpolicy\tfor\tthis\tregion\tof\tthe\tenvironment,\tbut\tit\twill\tnot\tperform\twell\tas\tsoon\tas\tit moves\tout\tof\tthis\tregion.\tTo\tsolve\tthis\tproblem,\tyou\tcan\tuse\ta\treplay\tmemory;\tinstead\tof\tusing\tonly the\tmost\timmediate\texperiences\tfor\tlearning,\tthe\tagent\twill\tlearn\tbased\ton\ta\tbuffer\tof\tits\tpast experiences,\trecent\tand\tnot\tso\trecent\t(perhaps\tthis\tis\twhy\twe\tdream\tat\tnight:\tto\treplay\tour experiences\tof\tthe\tday\tand\tbetter\tlearn\tfrom\tthem?).\n\n7.\t An\toff-policy\tRL\talgorithm\tlearns\tthe\tvalue\tof\tthe\toptimal\tpolicy\t(i.e.,\tthe\tsum\tof\tdiscounted rewards\tthat\tcan\tbe\texpected\tfor\teach\tstate\tif\tthe\tagent\tacts\toptimally),\tindependently\tof\thow\tthe agent\tactually\tacts.\tQ-Learning\tis\ta\tgood\texample\tof\tsuch\tan\talgorithm.\tIn\tcontrast,\tan\ton-policy algorithm\tlearns\tthe\tvalue\tof\tthe\tpolicy\tthat\tthe\tagent\tactually\texecutes,\tincluding\tboth\texploration and\texploitation.\n\nFor\tthe\tsolutions\tto\texercises\t8,\t9,\tand\t10,\tplease\tsee\tthe\tJupyter\tnotebooks\tavailable\tat https://github.com/ageron/handson-ml.\n\n1\n\nIf\tyou\tdraw\ta\tstraight\tline\tbetween\tany\ttwo\tpoints\ton\tthe\tcurve,\tthe\tline\tnever\tcrosses\tthe\tcurve.",
      "content_length": 3748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 598,
      "content": "2\n\n3\n\n4\n\n5\n\nMoreover,\tthe\tNormal\tEquation\trequires\tcomputing\tthe\tinverse\tof\ta\tmatrix,\tbut\tthat\tmatrix\tis\tnot\talways\tinvertible.\tIn\tcontrast,\tthe\tmatrix for\tRidge\tRegression\tis\talways\tinvertible.\n\nlog2\tis\tthe\tbinary\tlog,\tlog2(m)\t=\tlog(m)\t/\tlog(2).\n\nWhen\tthe\tvalues\tto\tpredict\tcan\tvary\tby\tmany\torders\tof\tmagnitude,\tthen\tyou\tmay\twant\tto\tpredict\tthe\tlogarithm\tof\tthe\ttarget\tvalue\trather than\tthe\ttarget\tvalue\tdirectly.\tSimply\tcomputing\tthe\texponential\tof\tthe\tneural\tnetwork’s\toutput\twill\tgive\tyou\tthe\testimated\tvalue\t(since exp(log\tv)\t=\tv).\n\nIn\tChapter\t11\twe\tdiscuss\tmany\ttechniques\tthat\tintroduce\tadditional\thyperparameters:\ttype\tof\tweight\tinitialization,\tactivation\tfunction hyperparameters\t(e.g.,\tamount\tof\tleak\tin\tleaky\tReLU),\tGradient\tClipping\tthreshold,\ttype\tof\toptimizer\tand\tits\thyperparameters\t(e.g.,\tthe momentum\thyperparameter\twhen\tusing\ta\tMomentumOptimizer),\ttype\tof\tregularization\tfor\teach\tlayer,\tand\tthe\tregularization hyperparameters\t(e.g.,\tdropout\trate\twhen\tusing\tdropout)\tand\tso\ton.",
      "content_length": 994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 599,
      "content": "Appendix\tB.\tMachine\tLearning\tProject Checklist\n\nThis\tchecklist\tcan\tguide\tyou\tthrough\tyour\tMachine\tLearning\tprojects.\tThere\tare\teight\tmain\tsteps:\n\n1.\t Frame\tthe\tproblem\tand\tlook\tat\tthe\tbig\tpicture.\n\n2.\t Get\tthe\tdata.\n\n3.\t Explore\tthe\tdata\tto\tgain\tinsights.\n\n4.\t Prepare\tthe\tdata\tto\tbetter\texpose\tthe\tunderlying\tdata\tpatterns\tto\tMachine\tLearning\talgorithms.\n\n5.\t Explore\tmany\tdifferent\tmodels\tand\tshort-list\tthe\tbest\tones.\n\n6.\t Fine-tune\tyour\tmodels\tand\tcombine\tthem\tinto\ta\tgreat\tsolution.\n\n7.\t Present\tyour\tsolution.\n\n8.\t Launch,\tmonitor,\tand\tmaintain\tyour\tsystem.\n\nObviously,\tyou\tshould\tfeel\tfree\tto\tadapt\tthis\tchecklist\tto\tyour\tneeds.",
      "content_length": 635,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 600,
      "content": "Frame\tthe\tProblem\tand\tLook\tat\tthe\tBig\tPicture\n\n1.\t Define\tthe\tobjective\tin\tbusiness\tterms.\n\n2.\t How\twill\tyour\tsolution\tbe\tused?\n\n3.\t What\tare\tthe\tcurrent\tsolutions/workarounds\t(if\tany)?\n\n4.\t How\tshould\tyou\tframe\tthis\tproblem\t(supervised/unsupervised,\tonline/offline,\tetc.)?\n\n5.\t How\tshould\tperformance\tbe\tmeasured?\n\n6.\t Is\tthe\tperformance\tmeasure\taligned\twith\tthe\tbusiness\tobjective?\n\n7.\t What\twould\tbe\tthe\tminimum\tperformance\tneeded\tto\treach\tthe\tbusiness\tobjective?\n\n8.\t What\tare\tcomparable\tproblems?\tCan\tyou\treuse\texperience\tor\ttools?\n\n9.\t Is\thuman\texpertise\tavailable?\n\n10.\t How\twould\tyou\tsolve\tthe\tproblem\tmanually?\n\n11.\t List\tthe\tassumptions\tyou\t(or\tothers)\thave\tmade\tso\tfar.\n\n12.\t Verify\tassumptions\tif\tpossible.",
      "content_length": 718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 601,
      "content": "Get\tthe\tData Note:\tautomate\tas\tmuch\tas\tpossible\tso\tyou\tcan\teasily\tget\tfresh\tdata.\n\n1.\t List\tthe\tdata\tyou\tneed\tand\thow\tmuch\tyou\tneed.\n\n2.\t Find\tand\tdocument\twhere\tyou\tcan\tget\tthat\tdata.\n\n3.\t Check\thow\tmuch\tspace\tit\twill\ttake.\n\n4.\t Check\tlegal\tobligations,\tand\tget\tauthorization\tif\tnecessary.\n\n5.\t Get\taccess\tauthorizations.\n\n6.\t Create\ta\tworkspace\t(with\tenough\tstorage\tspace).\n\n7.\t Get\tthe\tdata.\n\n8.\t Convert\tthe\tdata\tto\ta\tformat\tyou\tcan\teasily\tmanipulate\t(without\tchanging\tthe\tdata\titself).\n\n9.\t Ensure\tsensitive\tinformation\tis\tdeleted\tor\tprotected\t(e.g.,\tanonymized).\n\n10.\t Check\tthe\tsize\tand\ttype\tof\tdata\t(time\tseries,\tsample,\tgeographical,\tetc.).\n\n11.\t Sample\ta\ttest\tset,\tput\tit\taside,\tand\tnever\tlook\tat\tit\t(no\tdata\tsnooping!).",
      "content_length": 730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 602,
      "content": "Explore\tthe\tData Note:\ttry\tto\tget\tinsights\tfrom\ta\tfield\texpert\tfor\tthese\tsteps.\n\n1.\t Create\ta\tcopy\tof\tthe\tdata\tfor\texploration\t(sampling\tit\tdown\tto\ta\tmanageable\tsize\tif\tnecessary).\n\n2.\t Create\ta\tJupyter\tnotebook\tto\tkeep\ta\trecord\tof\tyour\tdata\texploration.\n\n3.\t Study\teach\tattribute\tand\tits\tcharacteristics:\n\nName\n\nType\t(categorical,\tint/float,\tbounded/unbounded,\ttext,\tstructured,\tetc.)\n\n%\tof\tmissing\tvalues\n\nNoisiness\tand\ttype\tof\tnoise\t(stochastic,\toutliers,\trounding\terrors,\tetc.)\n\nPossibly\tuseful\tfor\tthe\ttask?\n\nType\tof\tdistribution\t(Gaussian,\tuniform,\tlogarithmic,\tetc.)\n\n4.\t For\tsupervised\tlearning\ttasks,\tidentify\tthe\ttarget\tattribute(s).\n\n5.\t Visualize\tthe\tdata.\n\n6.\t Study\tthe\tcorrelations\tbetween\tattributes.\n\n7.\t Study\thow\tyou\twould\tsolve\tthe\tproblem\tmanually.\n\n8.\t Identify\tthe\tpromising\ttransformations\tyou\tmay\twant\tto\tapply.\n\n9.\t Identify\textra\tdata\tthat\twould\tbe\tuseful\t(go\tback\tto\t“Get\tthe\tData”).\n\n10.\t Document\twhat\tyou\thave\tlearned.",
      "content_length": 949,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 603,
      "content": "Prepare\tthe\tData Notes:\n\nWork\ton\tcopies\tof\tthe\tdata\t(keep\tthe\toriginal\tdataset\tintact).\n\nWrite\tfunctions\tfor\tall\tdata\ttransformations\tyou\tapply,\tfor\tfive\treasons:\n\nSo\tyou\tcan\teasily\tprepare\tthe\tdata\tthe\tnext\ttime\tyou\tget\ta\tfresh\tdataset\n\nSo\tyou\tcan\tapply\tthese\ttransformations\tin\tfuture\tprojects\n\nTo\tclean\tand\tprepare\tthe\ttest\tset\n\nTo\tclean\tand\tprepare\tnew\tdata\tinstances\tonce\tyour\tsolution\tis\tlive\n\nTo\tmake\tit\teasy\tto\ttreat\tyour\tpreparation\tchoices\tas\thyperparameters\n\n1.\t Data\tcleaning:\n\nFix\tor\tremove\toutliers\t(optional).\n\nFill\tin\tmissing\tvalues\t(e.g.,\twith\tzero,\tmean,\tmedian…)\tor\tdrop\ttheir\trows\t(or\tcolumns).\n\n2.\t Feature\tselection\t(optional):\n\nDrop\tthe\tattributes\tthat\tprovide\tno\tuseful\tinformation\tfor\tthe\ttask.\n\n3.\t Feature\tengineering,\twhere\tappropriate:\n\nDiscretize\tcontinuous\tfeatures.\n\nDecompose\tfeatures\t(e.g.,\tcategorical,\tdate/time,\tetc.).\n\nAdd\tpromising\ttransformations\tof\tfeatures\t(e.g.,\tlog(x),\tsqrt(x),\tx^2,\tetc.).\n\nAggregate\tfeatures\tinto\tpromising\tnew\tfeatures.\n\n4.\t Feature\tscaling:\tstandardize\tor\tnormalize\tfeatures.",
      "content_length": 1040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 604,
      "content": "Short-List\tPromising\tModels Notes:\n\nIf\tthe\tdata\tis\thuge,\tyou\tmay\twant\tto\tsample\tsmaller\ttraining\tsets\tso\tyou\tcan\ttrain\tmany\tdifferent models\tin\ta\treasonable\ttime\t(be\taware\tthat\tthis\tpenalizes\tcomplex\tmodels\tsuch\tas\tlarge\tneural\tnets or\tRandom\tForests).\n\nOnce\tagain,\ttry\tto\tautomate\tthese\tsteps\tas\tmuch\tas\tpossible.\n\n1.\t Train\tmany\tquick\tand\tdirty\tmodels\tfrom\tdifferent\tcategories\t(e.g.,\tlinear,\tnaive\tBayes,\tSVM, Random\tForests,\tneural\tnet,\tetc.)\tusing\tstandard\tparameters.\n\n2.\t Measure\tand\tcompare\ttheir\tperformance.\n\nFor\teach\tmodel,\tuse\tN-fold\tcross-validation\tand\tcompute\tthe\tmean\tand\tstandard\tdeviation\tof the\tperformance\tmeasure\ton\tthe\tN\tfolds.\n\n3.\t Analyze\tthe\tmost\tsignificant\tvariables\tfor\teach\talgorithm.\n\n4.\t Analyze\tthe\ttypes\tof\terrors\tthe\tmodels\tmake.\n\nWhat\tdata\twould\ta\thuman\thave\tused\tto\tavoid\tthese\terrors?\n\n5.\t Have\ta\tquick\tround\tof\tfeature\tselection\tand\tengineering.\n\n6.\t Have\tone\tor\ttwo\tmore\tquick\titerations\tof\tthe\tfive\tprevious\tsteps.\n\n7.\t Short-list\tthe\ttop\tthree\tto\tfive\tmost\tpromising\tmodels,\tpreferring\tmodels\tthat\tmake\tdifferent\ttypes\tof errors.",
      "content_length": 1070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 605,
      "content": "Fine-Tune\tthe\tSystem Notes:\n\nYou\twill\twant\tto\tuse\tas\tmuch\tdata\tas\tpossible\tfor\tthis\tstep,\tespecially\tas\tyou\tmove\ttoward\tthe\tend of\tfine-tuning.\n\nAs\talways\tautomate\twhat\tyou\tcan.\n\n1.\t Fine-tune\tthe\thyperparameters\tusing\tcross-validation.\n\nTreat\tyour\tdata\ttransformation\tchoices\tas\thyperparameters,\tespecially\twhen\tyou\tare\tnot\tsure about\tthem\t(e.g.,\tshould\tI\treplace\tmissing\tvalues\twith\tzero\tor\twith\tthe\tmedian\tvalue?\tOr\tjust drop\tthe\trows?).\n\nUnless\tthere\tare\tvery\tfew\thyperparameter\tvalues\tto\texplore,\tprefer\trandom\tsearch\tover\tgrid search.\tIf\ttraining\tis\tvery\tlong,\tyou\tmay\tprefer\ta\tBayesian\toptimization\tapproach\t(e.g.,\tusing Gaussian\tprocess\tpriors,\tas\tdescribed\tby\tJasper\tSnoek,\tHugo\tLarochelle,\tand\tRyan\tAdams).1\n\n2.\t Try\tEnsemble\tmethods.\tCombining\tyour\tbest\tmodels\twill\toften\tperform\tbetter\tthan\trunning\tthem individually.\n\n3.\t Once\tyou\tare\tconfident\tabout\tyour\tfinal\tmodel,\tmeasure\tits\tperformance\ton\tthe\ttest\tset\tto\testimate the\tgeneralization\terror.\n\nWARNING\n\nDon’t\ttweak\tyour\tmodel\tafter\tmeasuring\tthe\tgeneralization\terror:\tyou\twould\tjust\tstart\toverfitting\tthe\ttest\tset.",
      "content_length": 1081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 606,
      "content": "Present\tYour\tSolution\n\n1.\t Document\twhat\tyou\thave\tdone.\n\n2.\t Create\ta\tnice\tpresentation.\n\nMake\tsure\tyou\thighlight\tthe\tbig\tpicture\tfirst.\n\n3.\t Explain\twhy\tyour\tsolution\tachieves\tthe\tbusiness\tobjective.\n\n4.\t Don’t\tforget\tto\tpresent\tinteresting\tpoints\tyou\tnoticed\talong\tthe\tway.\n\nDescribe\twhat\tworked\tand\twhat\tdid\tnot.\n\nList\tyour\tassumptions\tand\tyour\tsystem’s\tlimitations.\n\n5.\t Ensure\tyour\tkey\tfindings\tare\tcommunicated\tthrough\tbeautiful\tvisualizations\tor\teasy-to-remember statements\t(e.g.,\t“the\tmedian\tincome\tis\tthe\tnumber-one\tpredictor\tof\thousing\tprices”).",
      "content_length": 555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 607,
      "content": "Launch!\n\n1.\t Get\tyour\tsolution\tready\tfor\tproduction\t(plug\tinto\tproduction\tdata\tinputs,\twrite\tunit\ttests,\tetc.).\n\n2.\t Write\tmonitoring\tcode\tto\tcheck\tyour\tsystem’s\tlive\tperformance\tat\tregular\tintervals\tand\ttrigger\talerts when\tit\tdrops.\n\nBeware\tof\tslow\tdegradation\ttoo:\tmodels\ttend\tto\t“rot”\tas\tdata\tevolves.\n\nMeasuring\tperformance\tmay\trequire\ta\thuman\tpipeline\t(e.g.,\tvia\ta\tcrowdsourcing\tservice).\n\nAlso\tmonitor\tyour\tinputs’\tquality\t(e.g.,\ta\tmalfunctioning\tsensor\tsending\trandom\tvalues,\tor another\tteam’s\toutput\tbecoming\tstale).\tThis\tis\tparticularly\timportant\tfor\tonline\tlearning systems.\n\n3.\t Retrain\tyour\tmodels\ton\ta\tregular\tbasis\ton\tfresh\tdata\t(automate\tas\tmuch\tas\tpossible).\n\n1\n\n“Practical\tBayesian\tOptimization\tof\tMachine\tLearning\tAlgorithms,”\tJ.\tSnoek,\tH.\tLarochelle,\tR.\tAdams\t(2012).",
      "content_length": 786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 608,
      "content": "Appendix\tC.\tSVM\tDual\tProblem\n\nTo\tunderstand\tduality,\tyou\tfirst\tneed\tto\tunderstand\tthe\tLagrange\tmultipliers\tmethod.\tThe\tgeneral\tidea\tis to\ttransform\ta\tconstrained\toptimization\tobjective\tinto\tan\tunconstrained\tone,\tby\tmoving\tthe\tconstraints into\tthe\tobjective\tfunction.\tLet’s\tlook\tat\ta\tsimple\texample.\tSuppose\tyou\twant\tto\tfind\tthe\tvalues\tof\tx\tand\ty that\tminimize\tthe\tfunction\tf(x,y)\t=\tx2\t+\t2y,\tsubject\tto\tan\tequality\tconstraint:\t3x\t+\t2y\t+\t1\t=\t0.\tUsing\tthe Lagrange\tmultipliers\tmethod,\twe\tstart\tby\tdefining\ta\tnew\tfunction\tcalled\tthe\tLagrangian\t(or\tLagrange function):\tg(x,\ty,\tα)\t=\tf(x,\ty)\t–\tα(3x\t+\t2y\t+\t1).\tEach\tconstraint\t(in\tthis\tcase\tjust\tone)\tis\tsubtracted\tfrom the\toriginal\tobjective,\tmultiplied\tby\ta\tnew\tvariable\tcalled\ta\tLagrange\tmultiplier.\n\nJoseph-Louis\tLagrange\tshowed\tthat\tif\n\nis\ta\tsolution\tto\tthe\tconstrained\toptimization\tproblem,\tthen\n\nthere\tmust\texist\tan\t point\twhere\tall\tpartial\tderivatives\tare\tequal\tto\tzero).\tIn\tother\twords,\twe\tcan\tcompute\tthe\tpartial derivatives\tof\tg(x,\ty,\tα)\twith\tregards\tto\tx,\ty,\tand\tα;\twe\tcan\tfind\tthe\tpoints\twhere\tthese\tderivatives\tare\tall equal\tto\tzero;\tand\tthe\tsolutions\tto\tthe\tconstrained\toptimization\tproblem\t(if\tthey\texist)\tmust\tbe\tamong these\tstationary\tpoints.\n\nsuch\tthat\n\nis\ta\tstationary\tpoint\tof\tthe\tLagrangian\t(a\tstationary\tpoint\tis\ta\n\nIn\tthis\texample\tthe\tpartial\tderivatives\tare:\n\nWhen\tall\tthese\tpartial\tderivatives\tare\tequal\tto\t0,\twe\tfind\tthat\n\n,\tfrom\twhich\twe\tcan\teasily\tfind\tthat\n\n,\n\n,\tand\n\n.\tThis\tis\tthe\tonly\tstationary\tpoint,\tand\tas\tit\trespects\tthe\tconstraint,\tit\tmust\tbe\tthe\n\nsolution\tto\tthe\tconstrained\toptimization\tproblem.\n\nHowever,\tthis\tmethod\tapplies\tonly\tto\tequality\tconstraints.\tFortunately,\tunder\tsome\tregularity\tconditions (which\tare\trespected\tby\tthe\tSVM\tobjectives),\tthis\tmethod\tcan\tbe\tgeneralized\tto\tinequality\tconstraints\tas well\t(e.g.,\t3x\t+\t2y\t+\t1\t≥\t0).\tThe\tgeneralized\tLagrangian\tfor\tthe\thard\tmargin\tproblem\tis\tgiven\tby Equation\tC-1,\twhere\tthe\tα(i)\tvariables\tare\tcalled\tthe\tKarush–Kuhn–Tucker\t(KKT)\tmultipliers,\tand\tthey must\tbe\tgreater\tor\tequal\tto\tzero.\n\nEquation\tC-1.\tGeneralized\tLagrangian\tfor\tthe\thard\tmargin\tproblem\n\nJust\tlike\twith\tthe\tLagrange\tmultipliers\tmethod,\tyou\tcan\tcompute\tthe\tpartial\tderivatives\tand\tlocate\tthe\n\nstationary\tpoints.\tIf\tthere\tis\ta\tsolution,\tit\twill\tnecessarily\tbe\tamong\tthe\tstationary\tpoints\t respect\tthe\tKKT\tconditions:\n\nRespect\tthe\tproblem’s\tconstraints:\n\n,\n\nthat",
      "content_length": 2359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 609,
      "content": "Verify\n\n,\n\nEither\n\nor\tthe\tith\tconstraint\tmust\tbe\tan\tactive\tconstraint,\tmeaning\tit\tmust\thold\tby\tequality:\n\n.\tThis\tcondition\tis\tcalled\tthe\tcomplementary\tslackness\tcondition.\tIt\timplies\n\nthat\teither\n\nor\tthe\tith\tinstance\tlies\ton\tthe\tboundary\t(it\tis\ta\tsupport\tvector).\n\nNote\tthat\tthe\tKKT\tconditions\tare\tnecessary\tconditions\tfor\ta\tstationary\tpoint\tto\tbe\ta\tsolution\tof\tthe constrained\toptimization\tproblem.\tUnder\tsome\tconditions,\tthey\tare\talso\tsufficient\tconditions.\tLuckily,\tthe SVM\toptimization\tproblem\thappens\tto\tmeet\tthese\tconditions,\tso\tany\tstationary\tpoint\tthat\tmeets\tthe\tKKT conditions\tis\tguaranteed\tto\tbe\ta\tsolution\tto\tthe\tconstrained\toptimization\tproblem.\n\nWe\tcan\tcompute\tthe\tpartial\tderivatives\tof\tthe\tgeneralized\tLagrangian\twith\tregards\tto\tw\tand\tb\twith Equation\tC-2.\n\nEquation\tC-2.\tPartial\tderivatives\tof\tthe\tgeneralized\tLagrangian\n\nWhen\tthese\tpartial\tderivatives\tare\tequal\tto\t0,\twe\thave\tEquation\tC-3.\n\nEquation\tC-3.\tProperties\tof\tthe\tstationary\tpoints\n\nIf\twe\tplug\tthese\tresults\tinto\tthe\tdefinition\tof\tthe\tgeneralized\tLagrangian,\tsome\tterms\tdisappear\tand\twe",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 610,
      "content": "find\tEquation\tC-4.\n\nEquation\tC-4.\tDual\tform\tof\tthe\tSVM\tproblem\n\nThe\tgoal\tis\tnow\tto\tfind\tthe\tvector\t constrained\toptimization\tproblem\tis\tthe\tdual\tproblem\twe\twere\tlooking\tfor.\n\nthat\tminimizes\tthis\tfunction,\twith\n\nfor\tall\tinstances.\tThis\n\nOnce\tyou\tfind\tthe\toptimal\t can\tuse\tthe\tfact\tthat\ta\tsupport\tvector\tverifies\tt(i)(wT\t·\tx(i)\t+\tb)\t=\t1,\tso\tif\tthe\tkth\tinstance\tis\ta\tsupport\tvector\n\n,\tyou\tcan\tcompute\n\nusing\tthe\tfirst\tline\tof\tEquation\tC-3.\tTo\tcompute\n\n,\tyou\n\n(i.e.,\tαk\t>\t0),\tyou\tcan\tuse\tit\tto\tcompute\t the\taverage\tover\tall\tsupport\tvectors\tto\tget\ta\tmore\tstable\tand\tprecise\tvalue,\tas\tin\tEquation\tC-5.\n\n.\tHowever,\tit\tis\toften\tprefered\tto\tcompute\n\nEquation\tC-5.\tBias\tterm\testimation\tusing\tthe\tdual\tform",
      "content_length": 695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 611,
      "content": "Appendix\tD.\tAutodiff\n\nThis\tappendix\texplains\thow\tTensorFlow’s\tautodiff\tfeature\tworks,\tand\thow\tit\tcompares\tto\tother\tsolutions.\n\nSuppose\tyou\tdefine\ta\tfunction\tf(x,y)\t=\tx2y\t+\ty\t+\t2,\tand\tyou\tneed\tits\tpartial\tderivatives\t typically\tto\tperform\tGradient\tDescent\t(or\tsome\tother\toptimization\talgorithm).\tYour\tmain\toptions\tare manual\tdifferentiation,\tsymbolic\tdifferentiation,\tnumerical\tdifferentiation,\tforward-mode\tautodiff,\tand finally\treverse-mode\tautodiff.\tTensorFlow\timplements\tthis\tlast\toption.\tLet’s\tgo\tthrough\teach\tof\tthese options.\n\nand\n\n,",
      "content_length": 539,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 612,
      "content": "Manual\tDifferentiation The\tfirst\tapproach\tis\tto\tpick\tup\ta\tpencil\tand\ta\tpiece\tof\tpaper\tand\tuse\tyour\tcalculus\tknowledge\tto\tderive the\tpartial\tderivatives\tmanually.\tFor\tthe\tfunction\tf(x,y)\tjust\tdefined,\tit\tis\tnot\ttoo\thard;\tyou\tjust\tneed\tto\tuse five\trules:\n\nThe\tderivative\tof\ta\tconstant\tis\t0.\n\nThe\tderivative\tof\tλx\tis\tλ\t(where\tλ\tis\ta\tconstant).\n\nThe\tderivative\tof\txλ\tis\tλxλ\t–\t1,\tso\tthe\tderivative\tof\tx2\tis\t2x.\n\nThe\tderivative\tof\ta\tsum\tof\tfunctions\tis\tthe\tsum\tof\tthese\tfunctions’\tderivatives.\n\nThe\tderivative\tof\tλ\ttimes\ta\tfunction\tis\tλ\ttimes\tits\tderivative.\n\nFrom\tthese\trules,\tyou\tcan\tderive\tEquation\tD-1:\n\nEquation\tD-1.\tPartial\tderivatives\tof\tf(x,y)\n\nThis\tapproach\tcan\tbecome\tvery\ttedious\tfor\tmore\tcomplex\tfunctions,\tand\tyou\trun\tthe\trisk\tof\tmaking mistakes.\tThe\tgood\tnews\tis\tthat\tderiving\tthe\tmathematical\tequations\tfor\tthe\tpartial\tderivatives\tlike\twe just\tdid\tcan\tbe\tautomated,\tthrough\ta\tprocess\tcalled\tsymbolic\tdifferentiation.",
      "content_length": 925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 613,
      "content": "Symbolic\tDifferentiation Figure\tD-1\tshows\thow\tsymbolic\tdifferentiation\tworks\ton\tan\teven\tsimpler\tfunction,\tg(x,y)\t=\t5\t+\txy.\tThe graph\tfor\tthat\tfunction\tis\trepresented\ton\tthe\tleft.\tAfter\tsymbolic\tdifferentiation,\twe\tget\tthe\tgraph\ton\tthe\n\nright,\twhich\trepresents\tthe\tpartial\tderivative\t the\tpartial\tderivative\twith\tregards\tto\ty).\n\n(we\tcould\tsimilarly\tobtain\n\nFigure\tD-1.\tSymbolic\tdifferentiation\n\nThe\talgorithm\tstarts\tby\tgetting\tthe\tpartial\tderivative\tof\tthe\tleaf\tnodes.\tThe\tconstant\tnode\t(5)\treturns\tthe constant\t0,\tsince\tthe\tderivative\tof\ta\tconstant\tis\talways\t0.\tThe\tvariable\tx\treturns\tthe\tconstant\t1\tsince\n\n,\tand\tthe\tvariable\ty\treturns\tthe\tconstant\t0\tsince\n\n(if\twe\twere\tlooking\tfor\tthe\tpartial\n\nderivative\twith\tregards\tto\ty,\tit\twould\tbe\tthe\treverse).\n\nNow\twe\thave\tall\twe\tneed\tto\tmove\tup\tthe\tgraph\tto\tthe\tmultiplication\tnode\tin\tfunction\tg.\tCalculus\ttells\tus\n\nthat\tthe\tderivative\tof\tthe\tproduct\tof\ttwo\tfunctions\tu\tand\tv\tis\t therefore\tconstruct\ta\tlarge\tpart\tof\tthe\tgraph\ton\tthe\tright,\trepresenting\t0\t×\tx\t+\ty\t×\t1.\n\n.\tWe\tcan\n\nFinally,\twe\tcan\tgo\tup\tto\tthe\taddition\tnode\tin\tfunction\tg.\tAs\tmentioned,\tthe\tderivative\tof\ta\tsum\tof functions\tis\tthe\tsum\tof\tthese\tfunctions’\tderivatives.\tSo\twe\tjust\tneed\tto\tcreate\tan\taddition\tnode\tand connect\tit\tto\tthe\tparts\tof\tthe\tgraph\twe\thave\talready\tcomputed.\tWe\tget\tthe\tcorrect\tpartial\tderivative:\n\n.\n\nHowever,\tit\tcan\tbe\tsimplified\t(a\tlot).\tA\tfew\ttrivial\tpruning\tsteps\tcan\tbe\tapplied\tto\tthis\tgraph\tto\tget\trid\tof\n\nall\tunnecessary\toperations,\tand\twe\tget\ta\tmuch\tsmaller\tgraph\twith\tjust\tone\tnode:\n\n.\n\nIn\tthis\tcase,\tsimplification\tis\tfairly\teasy,\tbut\tfor\ta\tmore\tcomplex\tfunction,\tsymbolic\tdifferentiation\tcan produce\ta\thuge\tgraph\tthat\tmay\tbe\ttough\tto\tsimplify\tand\tlead\tto\tsuboptimal\tperformance.\tMost\timportantly, symbolic\tdifferentiation\tcannot\tdeal\twith\tfunctions\tdefined\twith\tarbitrary\tcode\t—\tfor\texample,\tthe",
      "content_length": 1832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 614,
      "content": "following\tfunction\tdiscussed\tin\tChapter\t9:\n\ndef\tmy_func(a,\tb): \t\t\t\tz\t=\t0 \t\t\t\tfor\ti\tin\trange(100): \t\t\t\t\t\t\t\tz\t=\ta\t*\tnp.cos(z\t+\ti)\t+\tz\t*\tnp.sin(b\t-\ti) \t\t\t\treturn\tz",
      "content_length": 160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 615,
      "content": "Numerical\tDifferentiation The\tsimplest\tsolution\tis\tto\tcompute\tan\tapproximation\tof\tthe\tderivatives,\tnumerically.\tRecall\tthat\tthe derivative\th′(x0)\tof\ta\tfunction\th(x)\tat\ta\tpoint\tx0\tis\tthe\tslope\tof\tthe\tfunction\tat\tthat\tpoint,\tor\tmore\tprecisely Equation\tD-2.\n\nEquation\tD-2.\tDerivative\tof\ta\tfunction\th(x)\tat\tpoint\tx0\n\nSo\tif\twe\twant\tto\tcalculate\tthe\tpartial\tderivative\tof\tf(x,y)\twith\tregards\tto\tx,\tat\tx\t=\t3\tand\ty\t=\t4,\twe\tcan simply\tcompute\tf(3\t+\tϵ,\t4)\t–\tf(3,\t4)\tand\tdivide\tthe\tresult\tby\tϵ,\tusing\ta\tvery\tsmall\tvalue\tfor\tϵ.\tThat’s exactly\twhat\tthe\tfollowing\tcode\tdoes:\n\ndef\tf(x,\ty): \t\t\t\treturn\tx**2*y\t+\ty\t+\t2\n\ndef\tderivative(f,\tx,\ty,\tx_eps,\ty_eps): \t\t\t\treturn\t(f(x\t+\tx_eps,\ty\t+\ty_eps)\t-\tf(x,\ty))\t/\t(x_eps\t+\ty_eps)\n\ndf_dx\t=\tderivative(f,\t3,\t4,\t0.00001,\t0) df_dy\t=\tderivative(f,\t3,\t4,\t0,\t0.00001)\n\nUnfortunately,\tthe\tresult\tis\timprecise\t(and\tit\tgets\tworse\tfor\tmore\tcomplex\tfunctions).\tThe\tcorrect\tresults are\trespectively\t24\tand\t10,\tbut\tinstead\twe\tget:\n\n>>>\tprint(df_dx) 24.000039999805264 >>>\tprint(df_dy) 10.000000000331966\n\nNotice\tthat\tto\tcompute\tboth\tpartial\tderivatives,\twe\thave\tto\tcall\tf()\tat\tleast\tthree\ttimes\t(we\tcalled\tit\tfour times\tin\tthe\tpreceding\tcode,\tbut\tit\tcould\tbe\toptimized).\tIf\tthere\twere\t1,000\tparameters,\twe\twould\tneed\tto call\tf()\tat\tleast\t1,001\ttimes.\tWhen\tyou\tare\tdealing\twith\tlarge\tneural\tnetworks,\tthis\tmakes\tnumerical differentiation\tway\ttoo\tinefficient.\n\nHowever,\tnumerical\tdifferentiation\tis\tso\tsimple\tto\timplement\tthat\tit\tis\ta\tgreat\ttool\tto\tcheck\tthat\tthe\tother methods\tare\timplemented\tcorrectly.\tFor\texample,\tif\tit\tdisagrees\twith\tyour\tmanually\tderived\tfunction,\tthen your\tfunction\tprobably\tcontains\ta\tmistake.",
      "content_length": 1628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 616,
      "content": "Forward-Mode\tAutodiff Forward-mode\tautodiff\tis\tneither\tnumerical\tdifferentiation\tnor\tsymbolic\tdifferentiation,\tbut\tin\tsome ways\tit\tis\ttheir\tlove\tchild.\tIt\trelies\ton\tdual\tnumbers,\twhich\tare\t(weird\tbut\tfascinating)\tnumbers\tof\tthe form\ta\t+\tbϵ\twhere\ta\tand\tb\tare\treal\tnumbers\tand\tϵ\tis\tan\tinfinitesimal\tnumber\tsuch\tthat\tϵ2\t=\t0\t(but\tϵ\t≠\t0). You\tcan\tthink\tof\tthe\tdual\tnumber\t42\t+\t24ϵ\tas\tsomething\takin\tto\t42.0000000024\twith\tan\tinfinite\tnumber of\t0s\t(but\tof\tcourse\tthis\tis\tsimplified\tjust\tto\tgive\tyou\tsome\tidea\tof\twhat\tdual\tnumbers\tare).\tA\tdual number\tis\trepresented\tin\tmemory\tas\ta\tpair\tof\tfloats.\tFor\texample,\t42\t+\t24ϵ\tis\trepresented\tby\tthe\tpair (42.0,\t24.0).\n\nDual\tnumbers\tcan\tbe\tadded,\tmultiplied,\tand\tso\ton,\tas\tshown\tin\tEquation\tD-3.\n\nEquation\tD-3.\tA\tfew\toperations\twith\tdual\tnumbers\n\nMost\timportantly,\tit\tcan\tbe\tshown\tthat\th(a\t+\tbϵ)\t=\th(a)\t+\tb\t×\th′(a)ϵ,\tso\tcomputing\th(a\t+\tϵ)\tgives\tyou\tboth h(a)\tand\tthe\tderivative\th′(a)\tin\tjust\tone\tshot.\tFigure\tD-2\tshows\thow\tforward-mode\tautodiff\tcomputes\tthe partial\tderivative\tof\tf(x,y)\twith\tregards\tto\tx\tat\tx\t=\t3\tand\ty\t=\t4.\tAll\twe\tneed\tto\tdo\tis\tcompute\tf(3\t+\tϵ,\t4); this\twill\toutput\ta\tdual\tnumber\twhose\tfirst\tcomponent\tis\tequal\tto\tf(3,\t4)\tand\twhose\tsecond\tcomponent\tis\n\nequal\tto\n\n.",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 617,
      "content": "Figure\tD-2.\tForward-mode\tautodiff\n\nTo\tcompute\n\nwe\twould\thave\tto\tgo\tthrough\tthe\tgraph\tagain,\tbut\tthis\ttime\twith\tx\t=\t3\tand\ty\t=\t4\t+\tϵ.\n\nSo\tforward-mode\tautodiff\tis\tmuch\tmore\taccurate\tthan\tnumerical\tdifferentiation,\tbut\tit\tsuffers\tfrom\tthe same\tmajor\tflaw:\tif\tthere\twere\t1,000\tparameters,\tit\twould\trequire\t1,000\tpasses\tthrough\tthe\tgraph\tto compute\tall\tthe\tpartial\tderivatives.\tThis\tis\twhere\treverse-mode\tautodiff\tshines:\tit\tcan\tcompute\tall\tof them\tin\tjust\ttwo\tpasses\tthrough\tthe\tgraph.",
      "content_length": 481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 618,
      "content": "Reverse-Mode\tAutodiff Reverse-mode\tautodiff\tis\tthe\tsolution\timplemented\tby\tTensorFlow.\tIt\tfirst\tgoes\tthrough\tthe\tgraph\tin\tthe forward\tdirection\t(i.e.,\tfrom\tthe\tinputs\tto\tthe\toutput)\tto\tcompute\tthe\tvalue\tof\teach\tnode.\tThen\tit\tdoes\ta second\tpass,\tthis\ttime\tin\tthe\treverse\tdirection\t(i.e.,\tfrom\tthe\toutput\tto\tthe\tinputs)\tto\tcompute\tall\tthe\tpartial derivatives.\tFigure\tD-3\trepresents\tthe\tsecond\tpass.\tDuring\tthe\tfirst\tpass,\tall\tthe\tnode\tvalues\twere computed,\tstarting\tfrom\tx\t=\t3\tand\ty\t=\t4.\tYou\tcan\tsee\tthose\tvalues\tat\tthe\tbottom\tright\tof\teach\tnode\t(e.g.,\tx ×\tx\t=\t9).\tThe\tnodes\tare\tlabeled\tn1\tto\tn7\tfor\tclarity.\tThe\toutput\tnode\tis\tn7:\tf(3,4)\t=\tn7\t=\t42.\n\nFigure\tD-3.\tReverse-mode\tautodiff\n\nThe\tidea\tis\tto\tgradually\tgo\tdown\tthe\tgraph,\tcomputing\tthe\tpartial\tderivative\tof\tf(x,y)\twith\tregards\tto\teach consecutive\tnode,\tuntil\twe\treach\tthe\tvariable\tnodes.\tFor\tthis,\treverse-mode\tautodiff\trelies\theavily\ton\tthe chain\trule,\tshown\tin\tEquation\tD-4.\n\nEquation\tD-4.\tChain\trule\n\nSince\tn7\tis\tthe\toutput\tnode,\tf\t=\tn7\tso\ttrivially\n\n.\n\nLet’s\tcontinue\tdown\tthe\tgraph\tto\tn5:\thow\tmuch\tdoes\tf\tvary\twhen\tn5\tvaries?\tThe\tanswer\tis\n\n.",
      "content_length": 1104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 619,
      "content": "We\talready\tknow\tthat\n\n,\tso\tall\twe\tneed\tis\n\n.\tSince\tn7\tsimply\tperforms\tthe\tsum\tn5\t+\tn6,\twe\tfind\n\nthat\n\n,\tso\n\n.\n\nNow\twe\tcan\tproceed\tto\tnode\tn4:\thow\tmuch\tdoes\tf\tvary\twhen\tn4\tvaries?\tThe\tanswer\tis\n\nSince\tn5\t=\tn4\t×\tn2,\twe\tfind\tthat\n\n,\tso\n\n.\n\nThe\tprocess\tcontinues\tuntil\twe\treach\tthe\tbottom\tof\tthe\tgraph.\tAt\tthat\tpoint\twe\twill\thave\tcalculated\tall\tthe\n\npartial\tderivatives\tof\tf(x,y)\tat\tthe\tpoint\tx\t=\t3\tand\ty\t=\t4.\tIn\tthis\texample,\twe\tfind\t Sounds\tabout\tright!\n\nand\n\nReverse-mode\tautodiff\tis\ta\tvery\tpowerful\tand\taccurate\ttechnique,\tespecially\twhen\tthere\tare\tmany\tinputs and\tfew\toutputs,\tsince\tit\trequires\tonly\tone\tforward\tpass\tplus\tone\treverse\tpass\tper\toutput\tto\tcompute\tall the\tpartial\tderivatives\tfor\tall\toutputs\twith\tregards\tto\tall\tthe\tinputs.\tMost\timportantly,\tit\tcan\tdeal\twith functions\tdefined\tby\tarbitrary\tcode.\tIt\tcan\talso\thandle\tfunctions\tthat\tare\tnot\tentirely\tdifferentiable,\tas\tlong as\tyou\task\tit\tto\tcompute\tthe\tpartial\tderivatives\tat\tpoints\tthat\tare\tdifferentiable.\n\nTIP\n\nIf\tyou\timplement\ta\tnew\ttype\tof\toperation\tin\tTensorFlow\tand\tyou\twant\tto\tmake\tit\tcompatible\twith\tautodiff,\tthen\tyou\tneed\tto provide\ta\tfunction\tthat\tbuilds\ta\tsubgraph\tto\tcompute\tits\tpartial\tderivatives\twith\tregards\tto\tits\tinputs.\tFor\texample,\tsuppose\tyou implement\ta\tfunction\tthat\tcomputes\tthe\tsquare\tof\tits\tinput\tf(x)\t=\tx2.\tIn\tthat\tcase\tyou\twould\tneed\tto\tprovide\tthe\tcorresponding derivative\tfunction\tf′(x)\t=\t2x.\tNote\tthat\tthis\tfunction\tdoes\tnot\tcompute\ta\tnumerical\tresult,\tbut\tinstead\tbuilds\ta\tsubgraph\tthat\twill (later)\tcompute\tthe\tresult.\tThis\tis\tvery\tuseful\tbecause\tit\tmeans\tthat\tyou\tcan\tcompute\tgradients\tof\tgradients\t(to\tcompute\tsecond- order\tderivatives,\tor\teven\thigher-order\tderivatives).\n\n.\n\n.",
      "content_length": 1675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 620,
      "content": "Appendix\tE.\tOther\tPopular\tANN\tArchitectures\n\nIn\tthis\tappendix\twe\twill\tgive\ta\tquick\toverview\tof\ta\tfew\thistorically\timportant\tneural\tnetwork architectures\tthat\tare\tmuch\tless\tused\ttoday\tthan\tdeep\tMulti-Layer\tPerceptrons\t(Chapter\t10), convolutional\tneural\tnetworks\t(Chapter\t13),\trecurrent\tneural\tnetworks\t(Chapter\t14),\tor\tautoencoders (Chapter\t15).\tThey\tare\toften\tmentioned\tin\tthe\tliterature,\tand\tsome\tare\tstill\tused\tin\tmany\tapplications,\tso it\tis\tworth\tknowing\tabout\tthem.\tMoreover,\twe\twill\tdiscuss\tdeep\tbelief\tnets\t(DBNs),\twhich\twere\tthe state\tof\tthe\tart\tin\tDeep\tLearning\tuntil\tthe\tearly\t2010s.\tThey\tare\tstill\tthe\tsubject\tof\tvery\tactive\tresearch,\tso they\tmay\twell\tcome\tback\twith\ta\tvengeance\tin\tthe\tnear\tfuture.",
      "content_length": 708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 621,
      "content": "Hopfield\tNetworks Hopfield\tnetworks\twere\tfirst\tintroduced\tby\tW.\tA.\tLittle\tin\t1974,\tthen\tpopularized\tby\tJ.\tHopfield\tin\t1982. They\tare\tassociative\tmemory\tnetworks:\tyou\tfirst\tteach\tthem\tsome\tpatterns,\tand\tthen\twhen\tthey\tsee\ta\tnew pattern\tthey\t(hopefully)\toutput\tthe\tclosest\tlearned\tpattern.\tThis\thas\tmade\tthem\tuseful\tin\tparticular\tfor character\trecognition\tbefore\tthey\twere\toutperformed\tby\tother\tapproaches.\tYou\tfirst\ttrain\tthe\tnetwork\tby showing\tit\texamples\tof\tcharacter\timages\t(each\tbinary\tpixel\tmaps\tto\tone\tneuron),\tand\tthen\twhen\tyou\tshow it\ta\tnew\tcharacter\timage,\tafter\ta\tfew\titerations\tit\toutputs\tthe\tclosest\tlearned\tcharacter.\n\nThey\tare\tfully\tconnected\tgraphs\t(see\tFigure\tE-1);\tthat\tis,\tevery\tneuron\tis\tconnected\tto\tevery\tother\tneuron. Note\tthat\ton\tthe\tdiagram\tthe\timages\tare\t6\t×\t6\tpixels,\tso\tthe\tneural\tnetwork\ton\tthe\tleft\tshould\tcontain\t36 neurons\t(and\t648\tconnections),\tbut\tfor\tvisual\tclarity\ta\tmuch\tsmaller\tnetwork\tis\trepresented.\n\nFigure\tE-1.\tHopfield\tnetwork\n\nThe\ttraining\talgorithm\tworks\tby\tusing\tHebb’s\trule:\tfor\teach\ttraining\timage,\tthe\tweight\tbetween\ttwo neurons\tis\tincreased\tif\tthe\tcorresponding\tpixels\tare\tboth\ton\tor\tboth\toff,\tbut\tdecreased\tif\tone\tpixel\tis\ton and\tthe\tother\tis\toff.\n\nTo\tshow\ta\tnew\timage\tto\tthe\tnetwork,\tyou\tjust\tactivate\tthe\tneurons\tthat\tcorrespond\tto\tactive\tpixels.\tThe network\tthen\tcomputes\tthe\toutput\tof\tevery\tneuron,\tand\tthis\tgives\tyou\ta\tnew\timage.\tYou\tcan\tthen\ttake\tthis new\timage\tand\trepeat\tthe\twhole\tprocess.\tAfter\ta\twhile,\tthe\tnetwork\treaches\ta\tstable\tstate.\tGenerally,\tthis corresponds\tto\tthe\ttraining\timage\tthat\tmost\tresembles\tthe\tinput\timage.\n\nA\tso-called\tenergy\tfunction\tis\tassociated\twith\tHopfield\tnets.\tAt\teach\titeration,\tthe\tenergy\tdecreases,\tso the\tnetwork\tis\tguaranteed\tto\teventually\tstabilize\tto\ta\tlow-energy\tstate.\tThe\ttraining\talgorithm\ttweaks\tthe weights\tin\ta\tway\tthat\tdecreases\tthe\tenergy\tlevel\tof\tthe\ttraining\tpatterns,\tso\tthe\tnetwork\tis\tlikely\tto stabilize\tin\tone\tof\tthese\tlow-energy\tconfigurations.\tUnfortunately,\tsome\tpatterns\tthat\twere\tnot\tin\tthe training\tset\talso\tend\tup\twith\tlow\tenergy,\tso\tthe\tnetwork\tsometimes\tstabilizes\tin\ta\tconfiguration\tthat\twas",
      "content_length": 2110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 622,
      "content": "not\tlearned.\tThese\tare\tcalled\tspurious\tpatterns.\n\nAnother\tmajor\tflaw\twith\tHopfield\tnets\tis\tthat\tthey\tdon’t\tscale\tvery\twell\t—\ttheir\tmemory\tcapacity\tis roughly\tequal\tto\t14%\tof\tthe\tnumber\tof\tneurons.\tFor\texample,\tto\tclassify\t28\t×\t28\timages,\tyou\twould\tneed a\tHopfield\tnet\twith\t784\tfully\tconnected\tneurons\tand\t306,936\tweights.\tSuch\ta\tnetwork\twould\tonly\tbe\table to\tlearn\tabout\t110\tdifferent\tcharacters\t(14%\tof\t784).\tThat’s\ta\tlot\tof\tparameters\tfor\tsuch\ta\tsmall\tmemory.",
      "content_length": 461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 623,
      "content": "Boltzmann\tMachines Boltzmann\tmachines\twere\tinvented\tin\t1985\tby\tGeoffrey\tHinton\tand\tTerrence\tSejnowski.\tJust\tlike Hopfield\tnets,\tthey\tare\tfully\tconnected\tANNs,\tbut\tthey\tare\tbased\ton\tstochastic\tneurons:\tinstead\tof\tusing a\tdeterministic\tstep\tfunction\tto\tdecide\twhat\tvalue\tto\toutput,\tthese\tneurons\toutput\t1\twith\tsome\tprobability, and\t0\totherwise.\tThe\tprobability\tfunction\tthat\tthese\tANNs\tuse\tis\tbased\ton\tthe\tBoltzmann\tdistribution (used\tin\tstatistical\tmechanics)\thence\ttheir\tname.\tEquation\tE-1\tgives\tthe\tprobability\tthat\ta\tparticular neuron\twill\toutput\ta\t1.\n\nEquation\tE-1.\tProbability\tthat\tthe\tith\tneuron\twill\toutput\t1\n\nsj\tis\tthe\tjth\tneuron’s\tstate\t(0\tor\t1).\n\nwi,j\tis\tthe\tconnection\tweight\tbetween\tthe\tith\tand\tjth\tneurons.\tNote\tthat\twi,i\t=\t0.\n\nbi\tis\tthe\tith\tneuron’s\tbias\tterm.\tWe\tcan\timplement\tthis\tterm\tby\tadding\ta\tbias\tneuron\tto\tthe\tnetwork.\n\nN\tis\tthe\tnumber\tof\tneurons\tin\tthe\tnetwork.\n\nT\tis\ta\tnumber\tcalled\tthe\tnetwork’s\ttemperature;\tthe\thigher\tthe\ttemperature,\tthe\tmore\trandom\tthe output\tis\t(i.e.,\tthe\tmore\tthe\tprobability\tapproaches\t50%).\n\nσ\tis\tthe\tlogistic\tfunction.\n\nNeurons\tin\tBoltzmann\tmachines\tare\tseparated\tinto\ttwo\tgroups:\tvisible\tunits\tand\thidden\tunits\t(see Figure\tE-2).\tAll\tneurons\twork\tin\tthe\tsame\tstochastic\tway,\tbut\tthe\tvisible\tunits\tare\tthe\tones\tthat\treceive the\tinputs\tand\tfrom\twhich\toutputs\tare\tread.",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 624,
      "content": "Figure\tE-2.\tBoltzmann\tmachine\n\nBecause\tof\tits\tstochastic\tnature,\ta\tBoltzmann\tmachine\twill\tnever\tstabilize\tinto\ta\tfixed\tconfiguration,\tbut instead\tit\twill\tkeep\tswitching\tbetween\tmany\tconfigurations.\tIf\tit\tis\tleft\trunning\tfor\ta\tsufficiently\tlong\ttime, the\tprobability\tof\tobserving\ta\tparticular\tconfiguration\twill\tonly\tbe\ta\tfunction\tof\tthe\tconnection\tweights and\tbias\tterms,\tnot\tof\tthe\toriginal\tconfiguration\t(similarly,\tafter\tyou\tshuffle\ta\tdeck\tof\tcards\tfor\tlong enough,\tthe\tconfiguration\tof\tthe\tdeck\tdoes\tnot\tdepend\ton\tthe\tinitial\tstate).\tWhen\tthe\tnetwork\treaches\tthis state\twhere\tthe\toriginal\tconfiguration\tis\t“forgotten,”\tit\tis\tsaid\tto\tbe\tin\tthermal\tequilibrium\t(although\tits configuration\tkeeps\tchanging\tall\tthe\ttime).\tBy\tsetting\tthe\tnetwork\tparameters\tappropriately,\tletting\tthe network\treach\tthermal\tequilibrium,\tand\tthen\tobserving\tits\tstate,\twe\tcan\tsimulate\ta\twide\trange\tof probability\tdistributions.\tThis\tis\tcalled\ta\tgenerative\tmodel.\n\nTraining\ta\tBoltzmann\tmachine\tmeans\tfinding\tthe\tparameters\tthat\twill\tmake\tthe\tnetwork\tapproximate\tthe training\tset’s\tprobability\tdistribution.\tFor\texample,\tif\tthere\tare\tthree\tvisible\tneurons\tand\tthe\ttraining\tset contains\t75%\t(0,\t1,\t1)\ttriplets,\t10%\t(0,\t0,\t1)\ttriplets,\tand\t15%\t(1,\t1,\t1)\ttriplets,\tthen\tafter\ttraining\ta Boltzmann\tmachine,\tyou\tcould\tuse\tit\tto\tgenerate\trandom\tbinary\ttriplets\twith\tabout\tthe\tsame\tprobability distribution.\tFor\texample,\tabout\t75%\tof\tthe\ttime\tit\twould\toutput\tthe\t(0,\t1,\t1)\ttriplet.\n\nSuch\ta\tgenerative\tmodel\tcan\tbe\tused\tin\ta\tvariety\tof\tways.\tFor\texample,\tif\tit\tis\ttrained\ton\timages,\tand\tyou provide\tan\tincomplete\tor\tnoisy\timage\tto\tthe\tnetwork,\tit\twill\tautomatically\t“repair”\tthe\timage\tin\ta reasonable\tway.\tYou\tcan\talso\tuse\ta\tgenerative\tmodel\tfor\tclassification.\tJust\tadd\ta\tfew\tvisible\tneurons\tto encode\tthe\ttraining\timage’s\tclass\t(e.g.,\tadd\t10\tvisible\tneurons\tand\tturn\ton\tonly\tthe\tfifth\tneuron\twhen\tthe training\timage\trepresents\ta\t5).\tThen,\twhen\tgiven\ta\tnew\timage,\tthe\tnetwork\twill\tautomatically\tturn\ton\tthe",
      "content_length": 1975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 625,
      "content": "appropriate\tvisible\tneurons,\tindicating\tthe\timage’s\tclass\t(e.g.,\tit\twill\tturn\ton\tthe\tfifth\tvisible\tneuron\tif\tthe image\trepresents\ta\t5).\n\nUnfortunately,\tthere\tis\tno\tefficient\ttechnique\tto\ttrain\tBoltzmann\tmachines.\tHowever,\tfairly\tefficient algorithms\thave\tbeen\tdeveloped\tto\ttrain\trestricted\tBoltzmann\tmachines\t(RBM).",
      "content_length": 315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 626,
      "content": "Restricted\tBoltzmann\tMachines An\tRBM\tis\tsimply\ta\tBoltzmann\tmachine\tin\twhich\tthere\tare\tno\tconnections\tbetween\tvisible\tunits\tor between\thidden\tunits,\tonly\tbetween\tvisible\tand\thidden\tunits.\tFor\texample,\tFigure\tE-3\trepresents\tan\tRBM with\tthree\tvisible\tunits\tand\tfour\thidden\tunits.\n\nFigure\tE-3.\tRestricted\tBoltzmann\tmachine\n\nA\tvery\tefficient\ttraining\talgorithm,\tcalled\tContrastive\tDivergence,\twas\tintroduced\tin\t2005\tby\tMiguel\tÁ. Carreira-Perpiñán\tand\tGeoffrey\tHinton.1\tHere\tis\thow\tit\tworks:\tfor\teach\ttraining\tinstance\tx,\tthe\talgorithm starts\tby\tfeeding\tit\tto\tthe\tnetwork\tby\tsetting\tthe\tstate\tof\tthe\tvisible\tunits\tto\tx1,\tx2,\t,\t xn.\tThen\tyou\tcompute the\tstate\tof\tthe\thidden\tunits\tby\tapplying\tthe\tstochastic\tequation\tdescribed\tbefore\t(Equation\tE-1).\tThis gives\tyou\ta\thidden\tvector\th\t(where\thi\tis\tequal\tto\tthe\tstate\tof\tthe\tith\tunit).\tNext\tyou\tcompute\tthe\tstate\tof\tthe visible\tunits,\tby\tapplying\tthe\tsame\tstochastic\tequation.\tThis\tgives\tyou\ta\tvector\t compute\tthe\tstate\tof\tthe\thidden\tunits,\twhich\tgives\tyou\ta\tvector\t weight\tby\tapplying\tthe\trule\tin\tEquation\tE-2.\n\n.\tThen\tonce\tagain\tyou .\tNow\tyou\tcan\tupdate\teach\tconnection\n\nEquation\tE-2.\tContrastive\tdivergence\tweight\tupdate\n\nThe\tgreat\tbenefit\tof\tthis\talgorithm\tit\tthat\tit\tdoes\tnot\trequire\twaiting\tfor\tthe\tnetwork\tto\treach\tthermal equilibrium:\tit\tjust\tgoes\tforward,\tbackward,\tand\tforward\tagain,\tand\tthat’s\tit.\tThis\tmakes\tit\tincomparably more\tefficient\tthan\tprevious\talgorithms,\tand\tit\twas\ta\tkey\tingredient\tto\tthe\tfirst\tsuccess\tof\tDeep\tLearning based\ton\tmultiple\tstacked\tRBMs.",
      "content_length": 1513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 627,
      "content": "Deep\tBelief\tNets Several\tlayers\tof\tRBMs\tcan\tbe\tstacked;\tthe\thidden\tunits\tof\tthe\tfirst-level\tRBM\tserves\tas\tthe\tvisible\tunits for\tthe\tsecond-layer\tRBM,\tand\tso\ton.\tSuch\tan\tRBM\tstack\tis\tcalled\ta\tdeep\tbelief\tnet\t(DBN).\n\nYee-Whye\tTeh,\tone\tof\tGeoffrey\tHinton’s\tstudents,\tobserved\tthat\tit\twas\tpossible\tto\ttrain\tDBNs\tone\tlayer at\ta\ttime\tusing\tContrastive\tDivergence,\tstarting\twith\tthe\tlower\tlayers\tand\tthen\tgradually\tmoving\tup\tto\tthe top\tlayers.\tThis\tled\tto\tthe\tgroundbreaking\tarticle\tthat\tkickstarted\tthe\tDeep\tLearning\ttsunami\tin\t2006.2\n\nJust\tlike\tRBMs,\tDBNs\tlearn\tto\treproduce\tthe\tprobability\tdistribution\tof\ttheir\tinputs,\twithout\tany supervision.\tHowever,\tthey\tare\tmuch\tbetter\tat\tit,\tfor\tthe\tsame\treason\tthat\tdeep\tneural\tnetworks\tare\tmore powerful\tthan\tshallow\tones:\treal-world\tdata\tis\toften\torganized\tin\thierarchical\tpatterns,\tand\tDBNs\ttake advantage\tof\tthat.\tTheir\tlower\tlayers\tlearn\tlow-level\tfeatures\tin\tthe\tinput\tdata,\twhile\thigher\tlayers\tlearn high-level\tfeatures.\n\nJust\tlike\tRBMs,\tDBNs\tare\tfundamentally\tunsupervised,\tbut\tyou\tcan\talso\ttrain\tthem\tin\ta\tsupervised manner\tby\tadding\tsome\tvisible\tunits\tto\trepresent\tthe\tlabels.\tMoreover,\tone\tgreat\tfeature\tof\tDBNs\tis\tthat they\tcan\tbe\ttrained\tin\ta\tsemisupervised\tfashion.\tFigure\tE-4\trepresents\tsuch\ta\tDBN\tconfigured\tfor semisupervised\tlearning.\n\nFigure\tE-4.\tA\tdeep\tbelief\tnetwork\tconfigured\tfor\tsemisupervised\tlearning\n\nFirst,\tthe\tRBM\t1\tis\ttrained\twithout\tsupervision.\tIt\tlearns\tlow-level\tfeatures\tin\tthe\ttraining\tdata.\tThen RBM\t2\tis\ttrained\twith\tRBM\t1’s\thidden\tunits\tas\tinputs,\tagain\twithout\tsupervision:\tit\tlearns\thigher-level features\t(note\tthat\tRBM\t2’s\thidden\tunits\tinclude\tonly\tthe\tthree\trightmost\tunits,\tnot\tthe\tlabel\tunits). Several\tmore\tRBMs\tcould\tbe\tstacked\tthis\tway,\tbut\tyou\tget\tthe\tidea.\tSo\tfar,\ttraining\twas\t100% unsupervised.\tLastly,\tRBM\t3\tis\ttrained\tusing\tboth\tRBM\t2’s\thidden\tunits\tas\tinputs,\tas\twell\tas\textra visible\tunits\tused\tto\trepresent\tthe\ttarget\tlabels\t(e.g.,\ta\tone-hot\tvector\trepresenting\tthe\tinstance\tclass).\tIt learns\tto\tassociate\thigh-level\tfeatures\twith\ttraining\tlabels.\tThis\tis\tthe\tsupervised\tstep.",
      "content_length": 2070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 628,
      "content": "At\tthe\tend\tof\ttraining,\tif\tyou\tfeed\tRBM\t1\ta\tnew\tinstance,\tthe\tsignal\twill\tpropagate\tup\tto\tRBM\t2,\tthen\tup to\tthe\ttop\tof\tRBM\t3,\tand\tthen\tback\tdown\tto\tthe\tlabel\tunits;\thopefully,\tthe\tappropriate\tlabel\twill\tlight\tup. This\tis\thow\ta\tDBN\tcan\tbe\tused\tfor\tclassification.\n\nOne\tgreat\tbenefit\tof\tthis\tsemisupervised\tapproach\tis\tthat\tyou\tdon’t\tneed\tmuch\tlabeled\ttraining\tdata.\tIf\tthe unsupervised\tRBMs\tdo\ta\tgood\tenough\tjob,\tthen\tonly\ta\tsmall\tamount\tof\tlabeled\ttraining\tinstances\tper class\twill\tbe\tnecessary.\tSimilarly,\ta\tbaby\tlearns\tto\trecognize\tobjects\twithout\tsupervision,\tso\twhen\tyou point\tto\ta\tchair\tand\tsay\t“chair,”\tthe\tbaby\tcan\tassociate\tthe\tword\t“chair”\twith\tthe\tclass\tof\tobjects\tit\thas already\tlearned\tto\trecognize\ton\tits\town.\tYou\tdon’t\tneed\tto\tpoint\tto\tevery\tsingle\tchair\tand\tsay\t“chair”; only\ta\tfew\texamples\twill\tsuffice\t(just\tenough\tso\tthe\tbaby\tcan\tbe\tsure\tthat\tyou\tare\tindeed\treferring\tto\tthe chair,\tnot\tto\tits\tcolor\tor\tone\tof\tthe\tchair’s\tparts).\n\nQuite\tamazingly,\tDBNs\tcan\talso\twork\tin\treverse.\tIf\tyou\tactivate\tone\tof\tthe\tlabel\tunits,\tthe\tsignal\twill propagate\tup\tto\tthe\thidden\tunits\tof\tRBM\t3,\tthen\tdown\tto\tRBM\t2,\tand\tthen\tRBM\t1,\tand\ta\tnew\tinstance will\tbe\toutput\tby\tthe\tvisible\tunits\tof\tRBM\t1.\tThis\tnew\tinstance\twill\tusually\tlook\tlike\ta\tregular\tinstance of\tthe\tclass\twhose\tlabel\tunit\tyou\tactivated.\tThis\tgenerative\tcapability\tof\tDBNs\tis\tquite\tpowerful.\tFor example,\tit\thas\tbeen\tused\tto\tautomatically\tgenerate\tcaptions\tfor\timages,\tand\tvice\tversa:\tfirst\ta\tDBN\tis trained\t(without\tsupervision)\tto\tlearn\tfeatures\tin\timages,\tand\tanother\tDBN\tis\ttrained\t(again\twithout supervision)\tto\tlearn\tfeatures\tin\tsets\tof\tcaptions\t(e.g.,\t“car”\toften\tcomes\twith\t“automobile”).\tThen\tan RBM\tis\tstacked\ton\ttop\tof\tboth\tDBNs\tand\ttrained\twith\ta\tset\tof\timages\talong\twith\ttheir\tcaptions;\tit\tlearns to\tassociate\thigh-level\tfeatures\tin\timages\twith\thigh-level\tfeatures\tin\tcaptions.\tNext,\tif\tyou\tfeed\tthe\timage DBN\tan\timage\tof\ta\tcar,\tthe\tsignal\twill\tpropagate\tthrough\tthe\tnetwork,\tup\tto\tthe\ttop-level\tRBM,\tand\tback down\tto\tthe\tbottom\tof\tthe\tcaption\tDBN,\tproducing\ta\tcaption.\tDue\tto\tthe\tstochastic\tnature\tof\tRBMs\tand DBNs,\tthe\tcaption\twill\tkeep\tchanging\trandomly,\tbut\tit\twill\tgenerally\tbe\tappropriate\tfor\tthe\timage.\tIf\tyou generate\ta\tfew\thundred\tcaptions,\tthe\tmost\tfrequently\tgenerated\tones\twill\tlikely\tbe\ta\tgood\tdescription\tof the\timage.3",
      "content_length": 2309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 629,
      "content": "Self-Organizing\tMaps Self-organizing\tmaps\t(SOM)\tare\tquite\tdifferent\tfrom\tall\tthe\tother\ttypes\tof\tneural\tnetworks\twe\thave discussed\tso\tfar.\tThey\tare\tused\tto\tproduce\ta\tlow-dimensional\trepresentation\tof\ta\thigh-dimensional dataset,\tgenerally\tfor\tvisualization,\tclustering,\tor\tclassification.\tThe\tneurons\tare\tspread\tacross\ta\tmap (typically\t2D\tfor\tvisualization,\tbut\tit\tcan\tbe\tany\tnumber\tof\tdimensions\tyou\twant),\tas\tshown\tin\tFigure\tE-5, and\teach\tneuron\thas\ta\tweighted\tconnection\tto\tevery\tinput\t(note\tthat\tthe\tdiagram\tshows\tjust\ttwo\tinputs,\tbut there\tare\ttypically\ta\tvery\tlarge\tnumber,\tsince\tthe\twhole\tpoint\tof\tSOMs\tis\tto\treduce\tdimensionality).\n\nFigure\tE-5.\tSelf-organizing\tmaps\n\nOnce\tthe\tnetwork\tis\ttrained,\tyou\tcan\tfeed\tit\ta\tnew\tinstance\tand\tthis\twill\tactivate\tonly\tone\tneuron\t(i.e., hence\tone\tpoint\ton\tthe\tmap):\tthe\tneuron\twhose\tweight\tvector\tis\tclosest\tto\tthe\tinput\tvector.\tIn\tgeneral, instances\tthat\tare\tnearby\tin\tthe\toriginal\tinput\tspace\twill\tactivate\tneurons\tthat\tare\tnearby\ton\tthe\tmap.\tThis makes\tSOMs\tuseful\tfor\tvisualization\t(in\tparticular,\tyou\tcan\teasily\tidentify\tclusters\ton\tthe\tmap),\tbut\talso for\tapplications\tlike\tspeech\trecognition.\tFor\texample,\tif\teach\tinstance\trepresents\tthe\taudio\trecording\tof\ta person\tpronouncing\ta\tvowel,\tthen\tdifferent\tpronunciations\tof\tthe\tvowel\t“a”\twill\tactivate\tneurons\tin\tthe same\tarea\tof\tthe\tmap,\twhile\tinstances\tof\tthe\tvowel\t“e”\twill\tactivate\tneurons\tin\tanother\tarea,\tand intermediate\tsounds\twill\tgenerally\tactivate\tintermediate\tneurons\ton\tthe\tmap.",
      "content_length": 1485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 630,
      "content": "NOTE\n\nOne\timportant\tdifference\twith\tthe\tother\tdimensionality\treduction\ttechniques\tdiscussed\tin\tChapter\t8\tis\tthat\tall\tinstances\tget mapped\tto\ta\tdiscrete\tnumber\tof\tpoints\tin\tthe\tlow-dimensional\tspace\t(one\tpoint\tper\tneuron).\tWhen\tthere\tare\tvery\tfew\tneurons, this\ttechnique\tis\tbetter\tdescribed\tas\tclustering\trather\tthan\tdimensionality\treduction.\n\nThe\ttraining\talgorithm\tis\tunsupervised.\tIt\tworks\tby\thaving\tall\tthe\tneurons\tcompete\tagainst\teach\tother. First,\tall\tthe\tweights\tare\tinitialized\trandomly.\tThen\ta\ttraining\tinstance\tis\tpicked\trandomly\tand\tfed\tto\tthe network.\tAll\tneurons\tcompute\tthe\tdistance\tbetween\ttheir\tweight\tvector\tand\tthe\tinput\tvector\t(this\tis\tvery different\tfrom\tthe\tartificial\tneurons\twe\thave\tseen\tso\tfar).\tThe\tneuron\tthat\tmeasures\tthe\tsmallest\tdistance wins\tand\ttweaks\tits\tweight\tvector\tto\tbe\teven\tslightly\tcloser\tto\tthe\tinput\tvector,\tmaking\tit\tmore\tlikely\tto win\tfuture\tcompetitions\tfor\tother\tinputs\tsimilar\tto\tthis\tone.\tIt\talso\trecruits\tits\tneighboring\tneurons,\tand they\ttoo\tupdate\ttheir\tweight\tvector\tto\tbe\tslightly\tcloser\tto\tthe\tinput\tvector\t(but\tthey\tdon’t\tupdate\ttheir weights\tas\tmuch\tas\tthe\twinner\tneuron).\tThen\tthe\talgorithm\tpicks\tanother\ttraining\tinstance\tand\trepeats\tthe process,\tagain\tand\tagain.\tThis\talgorithm\ttends\tto\tmake\tnearby\tneurons\tgradually\tspecialize\tin\tsimilar inputs.4\n\n1\n\n“On\tContrastive\tDivergence\tLearning,”\tM.\tÁ.\tCarreira-Perpiñán\tand\tG.\tHinton\t(2005).\n\n2\n\n“A\tFast\tLearning\tAlgorithm\tfor\tDeep\tBelief\tNets,”\tG.\tHinton,\tS.\tOsindero,\tY.\tTeh\t(2006).\n\n3\n\nSee\tthis\tvideo\tby\tGeoffrey\tHinton\tfor\tmore\tdetails\tand\ta\tdemo:\thttp://goo.gl/7Z5QiS.\n\n4\n\nYou\tcan\timagine\ta\tclass\tof\tyoung\tchildren\twith\troughly\tsimilar\tskills.\tOne\tchild\thappens\tto\tbe\tslightly\tbetter\tat\tbasketball.\tThis\tmotivates her\tto\tpractice\tmore,\tespecially\twith\ther\tfriends.\tAfter\ta\twhile,\tthis\tgroup\tof\tfriends\tgets\tso\tgood\tat\tbasketball\tthat\tother\tkids\tcannot compete.\tBut\tthat’s\tokay,\tbecause\tthe\tother\tkids\tspecialize\tin\tother\ttopics.\tAfter\ta\twhile,\tthe\tclass\tis\tfull\tof\tlittle\tspecialized\tgroups.",
      "content_length": 1997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 631,
      "content": "Index\n\nSymbols\n\n__call__(),\tStatic\tUnrolling\tThrough\tTime\n\nε-greedy\tpolicy,\tExploration\tPolicies,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nε-insensitive,\tSVM\tRegression\n\nχ\t2\ttest\t(see\tchi\tsquare\ttest)\n\nℓ\t0\tnorm,\tSelect\ta\tPerformance\tMeasure\n\nℓ\t1\tand\tℓ\t2\tregularization,\tℓ1\tand\tℓ2\tRegularization-ℓ1\tand\tℓ2\tRegularization\n\nℓ\t1\tnorm,\tSelect\ta\tPerformance\tMeasure,\tLasso\tRegression,\tDecision\tBoundaries,\tAdam Optimization,\tAvoiding\tOverfitting\tThrough\tRegularization\n\nℓ\t2\tnorm,\tSelect\ta\tPerformance\tMeasure,\tRidge\tRegression-Lasso\tRegression,\tDecision Boundaries,\tSoftmax\tRegression,\tAvoiding\tOverfitting\tThrough\tRegularization,\tMax-Norm Regularization\n\nℓ\tk\tnorm,\tSelect\ta\tPerformance\tMeasure\n\nℓ\t∞\tnorm,\tSelect\ta\tPerformance\tMeasure\n\nA\n\naccuracy,\tWhat\tIs\tMachine\tLearning?,\tMeasuring\tAccuracy\tUsing\tCross-Validation-Measuring Accuracy\tUsing\tCross-Validation\n\nactions,\tevaluating,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem-Evaluating\tActions: The\tCredit\tAssignment\tProblem\n\nactivation\tfunctions,\tMulti-Layer\tPerceptron\tand\tBackpropagation-Multi-Layer\tPerceptron and\tBackpropagation\n\nactive\tconstraints,\tSVM\tDual\tProblem\n\nactors,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nactual\tclass,\tConfusion\tMatrix",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 632,
      "content": "AdaBoost,\tAdaBoost-AdaBoost\n\nAdagrad,\tAdaGrad-AdaGrad\n\nAdam\toptimization,\tAdam\tOptimization-Adam\tOptimization,\tAdam\tOptimization\n\nadaptive\tlearning\trate,\tAdaGrad\n\nadaptive\tmoment\toptimization,\tAdam\tOptimization\n\nagents,\tLearning\tto\tOptimize\tRewards\n\nAlexNet\tarchitecture,\tAlexNet-AlexNet\n\nalgorithms\n\npreparing\tdata\tfor,\tPrepare\tthe\tData\tfor\tMachine\tLearning\tAlgorithms-Select\tand Train\ta\tModel\n\nAlphaGo,\tReinforcement\tLearning,\tIntroduction\tto\tArtificial\tNeural\tNetworks,\tReinforcement Learning,\tPolicy\tGradients\n\nAnaconda,\tCreate\tthe\tWorkspace\n\nanomaly\tdetection,\tUnsupervised\tlearning\n\nApple’s\tSiri,\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\napply_gradients(),\tGradient\tClipping,\tPolicy\tGradients\n\narea\tunder\tthe\tcurve\t(AUC),\tThe\tROC\tCurve\n\narray_split(),\tIncremental\tPCA\n\nartificial\tneural\tnetworks\t(ANNs),\tIntroduction\tto\tArtificial\tNeural\tNetworks-Exercises\n\nBoltzmann\tMachines,\tBoltzmann\tMachines-Boltzmann\tMachines\n\ndeep\tbelief\tnetworks\t(DBNs),\tDeep\tBelief\tNets-Deep\tBelief\tNets\n\nevolution\tof,\tFrom\tBiological\tto\tArtificial\tNeurons\n\nHopfield\tNetworks,\tHopfield\tNetworks-Hopfield\tNetworks\n\nhyperparameter\tfine-tuning,\tFine-Tuning\tNeural\tNetwork\tHyperparameters-Activation",
      "content_length": 1180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 633,
      "content": "Functions\n\noverview,\tIntroduction\tto\tArtificial\tNeural\tNetworks-From\tBiological\tto\tArtificial Neurons\n\nPerceptrons,\tThe\tPerceptron-Multi-Layer\tPerceptron\tand\tBackpropagation\n\nself-organizing\tmaps,\tSelf-Organizing\tMaps-Self-Organizing\tMaps\n\ntraining\ta\tDNN\twith\tTensorFlow,\tTraining\ta\tDNN\tUsing\tPlain\tTensorFlow-Using\tthe Neural\tNetwork\n\nartificial\tneuron,\tLogical\tComputations\twith\tNeurons\n\n(see\talso\tartificial\tneural\tnetwork\t(ANN))\n\nassign(),\tManually\tComputing\tthe\tGradients\n\nassociation\trule\tlearning,\tUnsupervised\tlearning\n\nassociative\tmemory\tnetworks,\tHopfield\tNetworks\n\nassumptions,\tchecking,\tCheck\tthe\tAssumptions\n\nasynchronous\tupdates,\tAsynchronous\tupdates-Asynchronous\tupdates\n\nasynchrous\tcommunication,\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues- PaddingFifoQueue\n\natrous_conv2d(),\tResNet\n\nattention\tmechanism,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nattributes,\tSupervised\tlearning,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure-Take\ta\tQuick\tLook at\tthe\tData\tStructure\n\n(see\talso\tdata\tstructure)\n\ncombinations\tof,\tExperimenting\twith\tAttribute\tCombinations-Experimenting\twith Attribute\tCombinations\n\npreprocessed,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\ntarget,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure",
      "content_length": 1226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 634,
      "content": "autodiff,\tUsing\tautodiff-Using\tautodiff,\tAutodiff-Reverse-Mode\tAutodiff\n\nforward-mode,\tForward-Mode\tAutodiff-Forward-Mode\tAutodiff\n\nmanual\tdifferentiation,\tManual\tDifferentiation\n\nnumerical\tdifferentiation,\tNumerical\tDifferentiation\n\nreverse-mode,\tReverse-Mode\tAutodiff-Reverse-Mode\tAutodiff\n\nsymbolic\tdifferentiation,\tSymbolic\tDifferentiation-Numerical\tDifferentiation\n\nautoencoders,\tAutoencoders-Exercises\n\nadversarial,\tOther\tAutoencoders\n\ncontractive,\tOther\tAutoencoders\n\ndenoising,\tDenoising\tAutoencoders-TensorFlow\tImplementation\n\nefficient\tdata\trepresentations,\tEfficient\tData\tRepresentations\n\ngenerative\tstochastic\tnetwork\t(GSN),\tOther\tAutoencoders\n\novercomplete,\tUnsupervised\tPretraining\tUsing\tStacked\tAutoencoders\n\nPCA\twith\tundercomplete\tlinear\tautoencoder,\tPerforming\tPCA\twith\tan\tUndercomplete Linear\tAutoencoder\n\nreconstructions,\tEfficient\tData\tRepresentations\n\nsparse,\tSparse\tAutoencoders-TensorFlow\tImplementation\n\nstacked,\tStacked\tAutoencoders-Unsupervised\tPretraining\tUsing\tStacked\tAutoencoders\n\nstacked\tconvolutional,\tOther\tAutoencoders\n\nundercomplete,\tEfficient\tData\tRepresentations\n\nvariational,\tVariational\tAutoencoders-Generating\tDigits\n\nvisualizing\tfeatures,\tVisualizing\tFeatures-Visualizing\tFeatures\n\nwinner-take-all\t(WTA),\tOther\tAutoencoders\n\nautomatic\tdifferentiating,\tUp\tand\tRunning\twith\tTensorFlow",
      "content_length": 1323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 635,
      "content": "B\n\nautonomous\tdriving\tsystems,\tRecurrent\tNeural\tNetworks\n\nAverage\tAbsolute\tDeviation,\tSelect\ta\tPerformance\tMeasure\n\naverage\tpooling\tlayer,\tPooling\tLayer\n\navg_pool(),\tPooling\tLayer\n\nbackpropagation,\tMulti-Layer\tPerceptron\tand\tBackpropagation-Multi-Layer\tPerceptron\tand Backpropagation,\tVanishing/Exploding\tGradients\tProblems,\tUnsupervised\tPretraining, Visualizing\tFeatures\n\nbackpropagation\tthrough\ttime\t(BPTT),\tTraining\tRNNs\n\nbagging\tand\tpasting,\tBagging\tand\tPasting-Out-of-Bag\tEvaluation\n\nout-of-bag\tevaluation,\tOut-of-Bag\tEvaluation-Out-of-Bag\tEvaluation\n\nin\tScikit-Learn,\tBagging\tand\tPasting\tin\tScikit-Learn-Bagging\tand\tPasting\tin\tScikit- Learn\n\nbandwidth\tsaturation,\tBandwidth\tsaturation-Bandwidth\tsaturation\n\nBasicLSTMCell,\tLSTM\tCell\n\nBasicRNNCell,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs-Distributing\ta\tDeep\tRNN Across\tMultiple\tGPUs\n\nBatch\tGradient\tDescent,\tBatch\tGradient\tDescent-Batch\tGradient\tDescent,\tLasso\tRegression\n\nbatch\tlearning,\tBatch\tlearning-Batch\tlearning\n\nBatch\tNormalization,\tBatch\tNormalization-Implementing\tBatch\tNormalization\twith TensorFlow,\tResNet\n\noperation\tsummary,\tBatch\tNormalization\n\nwith\tTensorFlow,\tImplementing\tBatch\tNormalization\twith\tTensorFlow-Implementing Batch\tNormalization\twith\tTensorFlow\n\nbatch(),\tOther\tconvenience\tfunctions\n\nbatch_join(),\tOther\tconvenience\tfunctions",
      "content_length": 1314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 636,
      "content": "C\n\nbatch_normalization(),\tImplementing\tBatch\tNormalization\twith\tTensorFlow-Implementing Batch\tNormalization\twith\tTensorFlow\n\nBellman\tOptimality\tEquation,\tMarkov\tDecision\tProcesses\n\nbetween-graph\treplication,\tIn-Graph\tVersus\tBetween-Graph\tReplication\n\nbias\tneurons,\tThe\tPerceptron\n\nbias\tterm,\tLinear\tRegression\n\nbias/variance\ttradeoff,\tLearning\tCurves\n\nbiases,\tConstruction\tPhase\n\nbinary\tclassifiers,\tTraining\ta\tBinary\tClassifier,\tLogistic\tRegression\n\nbiological\tneurons,\tFrom\tBiological\tto\tArtificial\tNeurons-Biological\tNeurons\n\nblack\tbox\tmodels,\tMaking\tPredictions\n\nblending,\tStacking-Exercises\n\nBoltzmann\tMachines,\tBoltzmann\tMachines-Boltzmann\tMachines\n\n(see\talso\trestricted\tBoltzman\tmachines\t(RBMs))\n\nboosting,\tBoosting-Gradient\tBoosting\n\nAdaBoost,\tAdaBoost-AdaBoost\n\nGradient\tBoosting,\tGradient\tBoosting-Gradient\tBoosting\n\nbootstrap\taggregation\t(see\tbagging)\n\nbootstrapping,\tGrid\tSearch,\tBagging\tand\tPasting,\tIntroduction\tto\tOpenAI\tGym,\tLearning\tto Play\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nbottleneck\tlayers,\tGoogLeNet\n\nbrew,\tStacking\n\nCaffe\tmodel\tzoo,\tModel\tZoos",
      "content_length": 1066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 637,
      "content": "CART\t(Classification\tand\tRegression\tTree)\talgorithm,\tMaking\tPredictions-The\tCART\tTraining Algorithm,\tRegression\n\ncategorical\tattributes,\tHandling\tText\tand\tCategorical\tAttributes-Handling\tText\tand Categorical\tAttributes\n\ncell\twrapper,\tTraining\tto\tPredict\tTime\tSeries\n\nchi\tsquare\ttest,\tRegularization\tHyperparameters\n\nclassification\tversus\tregression,\tSupervised\tlearning,\tMultioutput\tClassification\n\nclassifiers\n\nbinary,\tTraining\ta\tBinary\tClassifier\n\nerror\tanalysis,\tError\tAnalysis-Error\tAnalysis\n\nevaluating,\tMulticlass\tClassification\n\nMNIST\tdataset,\tMNIST-MNIST\n\nmulticlass,\tMulticlass\tClassification-Multiclass\tClassification\n\nmultilabel,\tMultilabel\tClassification-Multilabel\tClassification\n\nmultioutput,\tMultioutput\tClassification-Multioutput\tClassification\n\nperformance\tmeasures,\tPerformance\tMeasures-The\tROC\tCurve\n\nprecision\tof,\tConfusion\tMatrix\n\nvoting,\tVoting\tClassifiers-Voting\tClassifiers\n\nclip_by_value(),\tGradient\tClipping\n\nclosed-form\tequation,\tTraining\tModels,\tRidge\tRegression,\tTraining\tand\tCost\tFunction\n\ncluster\tspecification,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\nclustering\talgorithms,\tUnsupervised\tlearning\n\nclusters,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\ncoding\tspace,\tVariational\tAutoencoders",
      "content_length": 1221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 638,
      "content": "codings,\tAutoencoders\n\ncomplementary\tslackness\tcondition,\tSVM\tDual\tProblem\n\ncomponents_,\tUsing\tScikit-Learn\n\ncomputational\tcomplexity,\tComputational\tComplexity,\tComputational\tComplexity, Computational\tComplexity\n\ncompute_gradients(),\tGradient\tClipping,\tPolicy\tGradients\n\nconcat(),\tGoogLeNet\n\nconfig.gpu_options,\tManaging\tthe\tGPU\tRAM\n\nConfigProto,\tManaging\tthe\tGPU\tRAM\n\nconfusion\tmatrix,\tConfusion\tMatrix-Confusion\tMatrix,\tError\tAnalysis-Error\tAnalysis\n\nconnectionism,\tThe\tPerceptron\n\nconstrained\toptimization,\tTraining\tObjective,\tSVM\tDual\tProblem\n\nContrastive\tDivergence,\tRestricted\tBoltzmann\tMachines\n\ncontrol\tdependencies,\tControl\tDependencies\n\nconv1d(),\tResNet\n\nconv2d_transpose(),\tResNet\n\nconv3d(),\tResNet\n\nconvergence\trate,\tBatch\tGradient\tDescent\n\nconvex\tfunction,\tGradient\tDescent\n\nconvolution\tkernels,\tFilters,\tCNN\tArchitectures,\tGoogLeNet\n\nconvolutional\tneural\tnetworks\t(CNNs),\tConvolutional\tNeural\tNetworks-Exercises\n\narchitectures,\tCNN\tArchitectures-ResNet\n\nAlexNet,\tAlexNet-AlexNet\n\nGoogleNet,\tGoogLeNet-GoogLeNet",
      "content_length": 1024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 639,
      "content": "LeNet5,\tLeNet-5-LeNet-5\n\nResNet,\tResNet-ResNet\n\nconvolutional\tlayer,\tConvolutional\tLayer-Memory\tRequirements,\tGoogLeNet,\tResNet\n\nfeature\tmaps,\tStacking\tMultiple\tFeature\tMaps-TensorFlow\tImplementation\n\nfilters,\tFilters\n\nmemory\trequirement,\tMemory\tRequirements-Memory\tRequirements\n\nevolution\tof,\tThe\tArchitecture\tof\tthe\tVisual\tCortex\n\npooling\tlayer,\tPooling\tLayer-Pooling\tLayer\n\nTensorFlow\timplementation,\tTensorFlow\tImplementation-TensorFlow\tImplementation\n\nCoordinator\tclass,\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner- Multithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\ncorrelation\tcoefficient,\tLooking\tfor\tCorrelations-Looking\tfor\tCorrelations\n\ncorrelations,\tfinding,\tLooking\tfor\tCorrelations-Looking\tfor\tCorrelations\n\ncost\tfunction,\tModel-based\tlearning,\tSelect\ta\tPerformance\tMeasure\n\nin\tAdaBoost,\tAdaBoost\n\nin\tadagrad,\tAdaGrad\n\nin\tartificial\tneural\tnetworks,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI, Construction\tPhase-Construction\tPhase\n\nin\tautodiff,\tUsing\tautodiff\n\nin\tbatch\tnormalization,\tImplementing\tBatch\tNormalization\twith\tTensorFlow\n\ncross\tentropy,\tLeNet-5\n\ndeep\tQ-Learning,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nin\tElastic\tNet,\tElastic\tNet\n\nin\tGradient\tDescent,\tTraining\tModels,\tGradient\tDescent-Gradient\tDescent,\tBatch",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 640,
      "content": "Gradient\tDescent,\tBatch\tGradient\tDescent-Stochastic\tGradient\tDescent,\tGradient Boosting,\tVanishing/Exploding\tGradients\tProblems\n\nin\tLogistic\tRegression,\tTraining\tand\tCost\tFunction-Training\tand\tCost\tFunction\n\nin\tPG\talgorithms,\tPolicy\tGradients\n\nin\tvariational\tautoencoders,\tVariational\tAutoencoders\n\nin\tLasso\tRegression,\tLasso\tRegression-Lasso\tRegression\n\nin\tLinear\tRegression,\tThe\tNormal\tEquation,\tGradient\tDescent\n\nin\tMomentum\toptimization,\tMomentum\tOptimization-Nesterov\tAccelerated\tGradient\n\nin\tpretrained\tlayers\treuse,\tPretraining\ton\tan\tAuxiliary\tTask\n\nin\tridge\tregression,\tRidge\tRegression-Ridge\tRegression\n\nin\tRNNs,\tTraining\tRNNs,\tTraining\tto\tPredict\tTime\tSeries\n\nstale\tgradients\tand,\tAsynchronous\tupdates\n\ncreative\tsequences,\tCreative\tRNN\n\ncredit\tassignment\tproblem,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem-Evaluating Actions:\tThe\tCredit\tAssignment\tProblem\n\ncritics,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ncross\tentropy,\tSoftmax\tRegression-Softmax\tRegression,\tTraining\tan\tMLP\twith\tTensorFlow’s High-Level\tAPI,\tTensorFlow\tImplementation,\tPolicy\tGradients\n\ncross-validation,\tTesting\tand\tValidating,\tBetter\tEvaluation\tUsing\tCross-Validation-Better Evaluation\tUsing\tCross-Validation,\tMeasuring\tAccuracy\tUsing\tCross-Validation-Measuring Accuracy\tUsing\tCross-Validation\n\nCUDA\tlibrary,\tInstallation\n\ncuDNN\tlibrary,\tInstallation\n\ncurse\tof\tdimensionality,\tDimensionality\tReduction-The\tCurse\tof\tDimensionality\n\n(see\talso\tdimensionality\treduction)",
      "content_length": 1466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 641,
      "content": "D\n\ncustom\ttransformers,\tCustom\tTransformers-Custom\tTransformers\n\ndata,\tTesting\tand\tValidating\n\n(see\talso\ttest\tdata;\ttraining\tdata)\n\ncreating\tworkspace\tfor,\tGet\tthe\tData-Download\tthe\tData\n\ndownloading,\tDownload\tthe\tData-Download\tthe\tData\n\nfinding\tcorrelations\tin,\tLooking\tfor\tCorrelations-Looking\tfor\tCorrelations\n\nmaking\tassumptions\tabout,\tTesting\tand\tValidating\n\npreparing\tfor\tMachine\tLearning\talgorithms,\tPrepare\tthe\tData\tfor\tMachine\tLearning Algorithms-Select\tand\tTrain\ta\tModel\n\ntest-set\tcreation,\tCreate\ta\tTest\tSet-Create\ta\tTest\tSet\n\nworking\twith\treal\tdata,\tWorking\twith\tReal\tData\n\ndata\taugmentation,\tData\tAugmentation-Data\tAugmentation\n\ndata\tcleaning,\tData\tCleaning-Handling\tText\tand\tCategorical\tAttributes\n\ndata\tmining,\tWhy\tUse\tMachine\tLearning?\n\ndata\tparallelism,\tData\tParallelism-TensorFlow\timplementation\n\nasynchronous\tupdates,\tAsynchronous\tupdates-Asynchronous\tupdates\n\nbandwidth\tsaturation,\tBandwidth\tsaturation-Bandwidth\tsaturation\n\nsynchronous\tupdates,\tSynchronous\tupdates\n\nTensorFlow\timplementation,\tTensorFlow\timplementation\n\ndata\tpipeline,\tFrame\tthe\tProblem\n\ndata\tsnooping\tbias,\tCreate\ta\tTest\tSet\n\ndata\tstructure,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure-Take\ta\tQuick\tLook\tat\tthe\tData Structure",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 642,
      "content": "data\tvisualization,\tVisualizing\tGeographical\tData-Visualizing\tGeographical\tData\n\nDataFrame,\tData\tCleaning\n\ndataquest,\tOther\tResources\n\ndecision\tboundaries,\tDecision\tBoundaries-Decision\tBoundaries,\tSoftmax\tRegression,\tMaking Predictions\n\ndecision\tfunction,\tPrecision/Recall\tTradeoff,\tDecision\tFunction\tand\tPredictions-Decision Function\tand\tPredictions\n\nDecision\tStumps,\tAdaBoost\n\ndecision\tthreshold,\tPrecision/Recall\tTradeoff\n\nDecision\tTrees,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet-Better\tEvaluation\tUsing\tCross- Validation,\tDecision\tTrees-Exercises,\tEnsemble\tLearning\tand\tRandom\tForests\n\nbinary\ttrees,\tMaking\tPredictions\n\nclass\tprobability\testimates,\tEstimating\tClass\tProbabilities\n\ncomputational\tcomplexity,\tComputational\tComplexity\n\ndecision\tboundaries,\tMaking\tPredictions\n\nGINI\timpurity,\tGini\tImpurity\tor\tEntropy?\n\ninstability\twith,\tInstability-Instability\n\nnumbers\tof\tchildren,\tMaking\tPredictions\n\npredictions,\tMaking\tPredictions-Estimating\tClass\tProbabilities\n\nRandom\tForests\t(see\tRandom\tForests)\n\nregression\ttasks,\tRegression-Regression\n\nregularization\thyperparameters,\tRegularization\tHyperparameters-Regularization Hyperparameters\n\ntraining\tand\tvisualizing,\tTraining\tand\tVisualizing\ta\tDecision\tTree-Making\tPredictions\n\ndecoder,\tEfficient\tData\tRepresentations",
      "content_length": 1270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 643,
      "content": "deconvolutional\tlayer,\tResNet\n\ndeep\tautoencoders\t(see\tstacked\tautoencoders)\n\ndeep\tbelief\tnetworks\t(DBNs),\tSemisupervised\tlearning,\tDeep\tBelief\tNets-Deep\tBelief\tNets\n\nDeep\tLearning,\tReinforcement\tLearning\n\n(see\talso\tReinforcement\tLearning;\tTensorFlow)\n\nabout,\tThe\tMachine\tLearning\tTsunami,\tRoadmap\n\nlibraries,\tUp\tand\tRunning\twith\tTensorFlow-Up\tand\tRunning\twith\tTensorFlow\n\ndeep\tneural\tnetworks\t(DNNs),\tMulti-Layer\tPerceptron\tand\tBackpropagation,\tTraining\tDeep Neural\tNets-Exercises\n\n(see\talso\tMulti-Layer\tPerceptrons\t(MLP))\n\nfaster\toptimizers\tfor,\tFaster\tOptimizers-Learning\tRate\tScheduling\n\nregularization,\tAvoiding\tOverfitting\tThrough\tRegularization-Data\tAugmentation\n\nreusing\tpretrained\tlayers,\tReusing\tPretrained\tLayers-Pretraining\ton\tan\tAuxiliary\tTask\n\ntraining\tguidelines\toverview,\tPractical\tGuidelines\n\ntraining\twith\tTensorFlow,\tTraining\ta\tDNN\tUsing\tPlain\tTensorFlow-Using\tthe\tNeural Network\n\ntraining\twith\tTF.Learn,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nunstable\tgradients,\tVanishing/Exploding\tGradients\tProblems\n\nvanishing\tand\texploding\tgradients,\tTraining\tDeep\tNeural\tNets-Gradient\tClipping\n\nDeep\tQ-Learning,\tApproximate\tQ-Learning-Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ- Learning\n\nMs.\tPac\tMan\texample,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning- Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ndeep\tQ-network,\tApproximate\tQ-Learning\n\ndeep\tRNNs,\tDeep\tRNNs-The\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps",
      "content_length": 1439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 644,
      "content": "applying\tdropout,\tApplying\tDropout\n\ndistributing\tacross\tmultiple\tGPUs,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs\n\nlong\tsequence\tdifficulties,\tThe\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps\n\ntruncated\tbackpropagation\tthrough\ttime,\tThe\tDifficulty\tof\tTraining\tover\tMany\tTime Steps\n\nDeepMind,\tReinforcement\tLearning,\tIntroduction\tto\tArtificial\tNeural\tNetworks, Reinforcement\tLearning,\tApproximate\tQ-Learning\n\ndegrees\tof\tfreedom,\tOverfitting\tthe\tTraining\tData,\tLearning\tCurves\n\ndenoising\tautoencoders,\tDenoising\tAutoencoders-TensorFlow\tImplementation\n\ndense(),\tConstruction\tPhase,\tTying\tWeights\n\ndepth\tconcat\tlayer,\tGoogLeNet\n\ndepth\tradius,\tAlexNet\n\ndepthwise_conv2d(),\tResNet\n\ndequeue(),\tQueues\tof\ttuples\n\ndequeue_many(),\tQueues\tof\ttuples,\tPaddingFifoQueue\n\ndequeue_up_to(),\tClosing\ta\tqueue-PaddingFifoQueue\n\ndequeuing\tdata,\tDequeuing\tdata\n\ndescribe(),\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\ndevice\tblocks,\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\ndevice(),\tSimple\tplacement\n\ndimensionality\treduction,\tUnsupervised\tlearning,\tDimensionality\tReduction-Exercises, Autoencoders\n\napproaches\tto\n\nManifold\tLearning,\tManifold\tLearning",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 645,
      "content": "projection,\tProjection-Projection\n\nchoosing\tthe\tright\tnumber\tof\tdimensions,\tChoosing\tthe\tRight\tNumber\tof\tDimensions\n\ncurse\tof\tdimensionality,\tDimensionality\tReduction-The\tCurse\tof\tDimensionality\n\nand\tdata\tvisualization,\tDimensionality\tReduction\n\nIsomap,\tOther\tDimensionality\tReduction\tTechniques\n\nLLE\t(Locally\tLinear\tEmbedding),\tLLE-LLE\n\nMultidimensional\tScaling,\tOther\tDimensionality\tReduction\tTechniques-Other Dimensionality\tReduction\tTechniques\n\nPCA\t(Principal\tComponent\tAnalysis),\tPCA-Randomized\tPCA\n\nt-Distributed\tStochastic\tNeighbor\tEmbedding\t(t-SNE),\tOther\tDimensionality\tReduction Techniques\n\ndiscount\trate,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem\n\ndistributed\tcomputing,\tUp\tand\tRunning\twith\tTensorFlow\n\ndistributed\tsessions,\tSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers-Sharing\tState Across\tSessions\tUsing\tResource\tContainers\n\nDNNClassifier,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\ndrop(),\tPrepare\tthe\tData\tfor\tMachine\tLearning\tAlgorithms\n\ndropconnect,\tDropout\n\ndropna(),\tData\tCleaning\n\ndropout,\tNumber\tof\tNeurons\tper\tHidden\tLayer,\tApplying\tDropout\n\ndropout\trate,\tDropout\n\ndropout(),\tDropout\n\nDropoutWrapper,\tApplying\tDropout\n\nDRY\t(Don’t\tRepeat\tYourself),\tModularity",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 646,
      "content": "E\n\nDual\tAveraging,\tAdam\tOptimization\n\ndual\tnumbers,\tForward-Mode\tAutodiff\n\ndual\tproblem,\tThe\tDual\tProblem\n\nduality,\tSVM\tDual\tProblem\n\ndying\tReLUs,\tNonsaturating\tActivation\tFunctions\n\ndynamic\tplacements,\tDynamic\tplacement\tfunction\n\ndynamic\tplacer,\tPlacing\tOperations\ton\tDevices\n\nDynamic\tProgramming,\tMarkov\tDecision\tProcesses\n\ndynamic\tunrolling\tthrough\ttime,\tDynamic\tUnrolling\tThrough\tTime\n\ndynamic_rnn(),\tDynamic\tUnrolling\tThrough\tTime,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple GPUs,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nearly\tstopping,\tEarly\tStopping-Early\tStopping,\tGradient\tBoosting,\tNumber\tof\tNeurons\tper Hidden\tLayer,\tEarly\tStopping\n\nElastic\tNet,\tElastic\tNet\n\nembedded\tdevice\tblocks,\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\nEmbedded\tReber\tgrammars,\tExercises\n\nembeddings,\tWord\tEmbeddings-Word\tEmbeddings\n\nembedding_lookup(),\tWord\tEmbeddings\n\nencoder,\tEfficient\tData\tRepresentations\n\nEncoder–Decoder,\tInput\tand\tOutput\tSequences\n\nend-of-sequence\t(EOS)\ttoken,\tHandling\tVariable-Length\tOutput\tSequences\n\nenergy\tfunctions,\tHopfield\tNetworks\n\nenqueuing\tdata,\tEnqueuing\tdata",
      "content_length": 1100,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 647,
      "content": "Ensemble\tLearning,\tBetter\tEvaluation\tUsing\tCross-Validation,\tEnsemble\tMethods,\tEnsemble Learning\tand\tRandom\tForests-Exercises\n\nbagging\tand\tpasting,\tBagging\tand\tPasting-Out-of-Bag\tEvaluation\n\nboosting,\tBoosting-Gradient\tBoosting\n\nin-graph\tversus\tbetween-graph\treplication,\tIn-Graph\tVersus\tBetween-Graph Replication-In-Graph\tVersus\tBetween-Graph\tReplication\n\nRandom\tForests,\tRandom\tForests-Feature\tImportance\n\n(see\talso\tRandom\tForests)\n\nrandom\tpatches\tand\trandom\tsubspaces,\tRandom\tPatches\tand\tRandom\tSubspaces\n\nstacking,\tStacking-Stacking\n\nentropy\timpurity\tmeasure,\tGini\tImpurity\tor\tEntropy?\n\nenvironments,\tin\treinforcement\tlearning,\tLearning\tto\tOptimize\tRewards-Evaluating\tActions: The\tCredit\tAssignment\tProblem,\tExploration\tPolicies,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing Deep\tQ-Learning\n\nepisodes\t(in\tRL),\tIntroduction\tto\tOpenAI\tGym,\tEvaluating\tActions:\tThe\tCredit\tAssignment Problem-Policy\tGradients,\tPolicy\tGradients-Policy\tGradients,\tLearning\tto\tPlay\tMs.\tPac-Man Using\tDeep\tQ-Learning\n\nepochs,\tStochastic\tGradient\tDescent\n\nε-insensitive,\tSVM\tRegression\n\nequality\tcontraints,\tSVM\tDual\tProblem\n\nerror\tanalysis,\tError\tAnalysis-Error\tAnalysis\n\nestimators,\tData\tCleaning\n\nEuclidian\tnorm,\tSelect\ta\tPerformance\tMeasure\n\neval(),\tFeeding\tData\tto\tthe\tTraining\tAlgorithm\n\nevaluating\tmodels,\tTesting\tand\tValidating-Testing\tand\tValidating\n\nexplained\tvariance,\tChoosing\tthe\tRight\tNumber\tof\tDimensions",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 648,
      "content": "F\n\nexplained\tvariance\tratio,\tExplained\tVariance\tRatio\n\nexploding\tgradients,\tVanishing/Exploding\tGradients\tProblems\n\n(see\talso\tgradients,\tvanishing\tand\texploding)\n\nexploration\tpolicies,\tExploration\tPolicies\n\nexponential\tdecay,\tImplementing\tBatch\tNormalization\twith\tTensorFlow\n\nexponential\tlinear\tunit\t(ELU),\tNonsaturating\tActivation\tFunctions-Nonsaturating\tActivation Functions\n\nexponential\tscheduling,\tLearning\tRate\tScheduling\n\nExtra-Trees,\tExtra-Trees\n\nF-1\tscore,\tPrecision\tand\tRecall-Precision\tand\tRecall\n\nface-recognition,\tMultilabel\tClassification\n\nfake\tX\tserver,\tIntroduction\tto\tOpenAI\tGym\n\nfalse\tpositive\trate\t(FPR),\tThe\tROC\tCurve-The\tROC\tCurve\n\nfan-in,\tXavier\tand\tHe\tInitialization,\tXavier\tand\tHe\tInitialization\n\nfan-out,\tXavier\tand\tHe\tInitialization,\tXavier\tand\tHe\tInitialization\n\nfeature\tdetection,\tAutoencoders\n\nfeature\tengineering,\tIrrelevant\tFeatures\n\nfeature\textraction,\tUnsupervised\tlearning\n\nfeature\timportance,\tFeature\tImportance-Feature\tImportance\n\nfeature\tmaps,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters,\tFilters-TensorFlow Implementation,\tResNet\n\nfeature\tscaling,\tFeature\tScaling\n\nfeature\tselection,\tIrrelevant\tFeatures,\tGrid\tSearch,\tLasso\tRegression,\tFeature\tImportance,",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 649,
      "content": "Prepare\tthe\tData\n\nfeature\tspace,\tKernel\tPCA,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nfeature\tvector,\tSelect\ta\tPerformance\tMeasure,\tLinear\tRegression,\tUnder\tthe\tHood, Implementing\tGradient\tDescent\n\nfeatures,\tSupervised\tlearning\n\nFeatureUnion,\tTransformation\tPipelines\n\nfeedforward\tneural\tnetwork\t(FNN),\tMulti-Layer\tPerceptron\tand\tBackpropagation\n\nfeed_dict,\tFeeding\tData\tto\tthe\tTraining\tAlgorithm\n\nFIFOQueue,\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues,\tRandomShuffleQueue\n\nfillna(),\tData\tCleaning\n\nfirst-in\tfirst-out\t(FIFO)\tqueues,\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues\n\nfirst-order\tpartial\tderivatives\t(Jacobians),\tAdam\tOptimization\n\nfit(),\tData\tCleaning,\tTransformation\tPipelines,\tIncremental\tPCA\n\nfitness\tfunction,\tModel-based\tlearning\n\nfit_inverse_transform=,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nfit_transform(),\tData\tCleaning,\tTransformation\tPipelines\n\nfolds,\tBetter\tEvaluation\tUsing\tCross-Validation,\tMNIST,\tMeasuring\tAccuracy\tUsing\tCross- Validation-Measuring\tAccuracy\tUsing\tCross-Validation\n\nFollow\tThe\tRegularized\tLeader\t(FTRL),\tAdam\tOptimization\n\nforget\tgate,\tLSTM\tCell\n\nforward-mode\tautodiff,\tForward-Mode\tAutodiff-Forward-Mode\tAutodiff\n\nframing\ta\tproblem,\tFrame\tthe\tProblem-Frame\tthe\tProblem\n\nfrozen\tlayers,\tFreezing\tthe\tLower\tLayers-Caching\tthe\tFrozen\tLayers",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 650,
      "content": "G\n\nfunctools.partial(),\tImplementing\tBatch\tNormalization\twith\tTensorFlow,\tTensorFlow Implementation,\tVariational\tAutoencoders\n\ngame\tplay\t(see\treinforcement\tlearning)\n\ngamma\tvalue,\tGaussian\tRBF\tKernel\n\ngate\tcontrollers,\tLSTM\tCell\n\nGaussian\tdistribution,\tVariational\tAutoencoders,\tGenerating\tDigits\n\nGaussian\tRBF,\tAdding\tSimilarity\tFeatures\n\nGaussian\tRBF\tkernel,\tGaussian\tRBF\tKernel-Gaussian\tRBF\tKernel,\tKernelized\tSVM\n\ngeneralization\terror,\tTesting\tand\tValidating\n\ngeneralized\tLagrangian,\tSVM\tDual\tProblem-SVM\tDual\tProblem\n\ngenerative\tautoencoders,\tVariational\tAutoencoders\n\ngenerative\tmodels,\tAutoencoders,\tBoltzmann\tMachines\n\ngenetic\talgorithms,\tPolicy\tSearch\n\ngeodesic\tdistance,\tOther\tDimensionality\tReduction\tTechniques\n\nget_variable(),\tSharing\tVariables-Sharing\tVariables\n\nGINI\timpurity,\tMaking\tPredictions,\tGini\tImpurity\tor\tEntropy?\n\nglobal\taverage\tpooling,\tGoogLeNet\n\nglobal_step,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nglobal_variables_initializer(),\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\nGlorot\tinitialization,\tVanishing/Exploding\tGradients\tProblems-Xavier\tand\tHe\tInitialization\n\nGoogle,\tUp\tand\tRunning\twith\tTensorFlow\n\nGoogle\tImages,\tIntroduction\tto\tArtificial\tNeural\tNetworks",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 651,
      "content": "Google\tPhotos,\tSemisupervised\tlearning\n\nGoogleNet\tarchitecture,\tGoogLeNet-GoogLeNet\n\ngpu_options.per_process_gpu_memory_fraction,\tManaging\tthe\tGPU\tRAM\n\ngradient\tascent,\tPolicy\tSearch\n\nGradient\tBoosted\tRegression\tTrees\t(GBRT),\tGradient\tBoosting\n\nGradient\tBoosting,\tGradient\tBoosting-Gradient\tBoosting\n\nGradient\tDescent\t(GD),\tTraining\tModels,\tGradient\tDescent-Mini-batch\tGradient\tDescent, Online\tSVMs,\tTraining\tDeep\tNeural\tNets,\tMomentum\tOptimization,\tAdaGrad\n\nalgorithm\tcomparisons,\tMini-batch\tGradient\tDescent-Mini-batch\tGradient\tDescent\n\nautomatically\tcomputing\tgradients,\tUsing\tautodiff-Using\tautodiff\n\nBatch\tGD,\tBatch\tGradient\tDescent-Batch\tGradient\tDescent,\tLasso\tRegression\n\ndefining,\tGradient\tDescent\n\nlocal\tminimum\tversus\tglobal\tminimum,\tGradient\tDescent\n\nmanually\tcomputing\tgradients,\tManually\tComputing\tthe\tGradients\n\nMini-batch\tGD,\tMini-batch\tGradient\tDescent-Mini-batch\tGradient\tDescent,\tFeeding Data\tto\tthe\tTraining\tAlgorithm-Feeding\tData\tto\tthe\tTraining\tAlgorithm\n\noptimizer,\tUsing\tan\tOptimizer\n\nStochastic\tGD,\tStochastic\tGradient\tDescent-Stochastic\tGradient\tDescent,\tSoft\tMargin Classification\n\nwith\tTensorFlow,\tImplementing\tGradient\tDescent-Using\tan\tOptimizer\n\nGradient\tTree\tBoosting,\tGradient\tBoosting\n\nGradientDescentOptimizer,\tConstruction\tPhase\n\ngradients(),\tUsing\tautodiff\n\ngradients,\tvanishing\tand\texploding,\tTraining\tDeep\tNeural\tNets-Gradient\tClipping,\tThe Difficulty\tof\tTraining\tover\tMany\tTime\tSteps",
      "content_length": 1422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 652,
      "content": "H\n\nBatch\tNormalization,\tBatch\tNormalization-Implementing\tBatch\tNormalization\twith TensorFlow\n\nGlorot\tand\tHe\tinitialization,\tVanishing/Exploding\tGradients\tProblems-Xavier\tand\tHe Initialization\n\ngradient\tclipping,\tGradient\tClipping\n\nnonsaturating\tactivation\tfunctions,\tNonsaturating\tActivation\tFunctions-Nonsaturating Activation\tFunctions\n\ngraphviz,\tTraining\tand\tVisualizing\ta\tDecision\tTree\n\ngreedy\talgorithm,\tThe\tCART\tTraining\tAlgorithm\n\ngrid\tsearch,\tFine-Tune\tYour\tModel-Grid\tSearch,\tPolynomial\tKernel\n\ngroup(),\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nGRU\t(Gated\tRecurrent\tUnit)\tcell,\tGRU\tCell-GRU\tCell\n\nhailstone\tsequence,\tEfficient\tData\tRepresentations\n\nhard\tmargin\tclassification,\tSoft\tMargin\tClassification-Soft\tMargin\tClassification\n\nhard\tvoting\tclassifiers,\tVoting\tClassifiers-Voting\tClassifiers\n\nharmonic\tmean,\tPrecision\tand\tRecall\n\nHe\tinitialization,\tVanishing/Exploding\tGradients\tProblems-Xavier\tand\tHe\tInitialization\n\nHeaviside\tstep\tfunction,\tThe\tPerceptron\n\nHebb's\trule,\tThe\tPerceptron,\tHopfield\tNetworks\n\nHebbian\tlearning,\tThe\tPerceptron\n\nhidden\tlayers,\tMulti-Layer\tPerceptron\tand\tBackpropagation\n\nhierarchical\tclustering,\tUnsupervised\tlearning\n\nhinge\tloss\tfunction,\tOnline\tSVMs",
      "content_length": 1202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 653,
      "content": "I\n\nhistograms,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure-Take\ta\tQuick\tLook\tat\tthe\tData Structure\n\nhold-out\tsets,\tStacking\n\n(see\talso\tblenders)\n\nHopfield\tNetworks,\tHopfield\tNetworks-Hopfield\tNetworks\n\nhyperbolic\ttangent\t(htan\tactivation\tfunction),\tMulti-Layer\tPerceptron\tand\tBackpropagation, Activation\tFunctions,\tVanishing/Exploding\tGradients\tProblems,\tXavier\tand\tHe\tInitialization, Recurrent\tNeurons\n\nhyperparameters,\tOverfitting\tthe\tTraining\tData,\tCustom\tTransformers,\tGrid\tSearch-Grid Search,\tEvaluate\tYour\tSystem\ton\tthe\tTest\tSet,\tGradient\tDescent,\tPolynomial\tKernel, Computational\tComplexity,\tFine-Tuning\tNeural\tNetwork\tHyperparameters\n\n(see\talso\tneural\tnetwork\thyperparameters)\n\nhyperplane,\tDecision\tFunction\tand\tPredictions,\tManifold\tLearning-PCA,\tProjecting\tDown\tto\td Dimensions,\tOther\tDimensionality\tReduction\tTechniques\n\nhypothesis,\tSelect\ta\tPerformance\tMeasure\n\nmanifold,\tManifold\tLearning\n\nhypothesis\tboosting\t(see\tboosting)\n\nhypothesis\tfunction,\tLinear\tRegression\n\nhypothesis,\tnull,\tRegularization\tHyperparameters\n\nidentity\tmatrix,\tRidge\tRegression,\tQuadratic\tProgramming\n\nILSVRC\tImageNet\tchallenge,\tCNN\tArchitectures\n\nimage\tclassification,\tCNN\tArchitectures\n\nimpurity\tmeasures,\tMaking\tPredictions,\tGini\tImpurity\tor\tEntropy?\n\nin-graph\treplication,\tIn-Graph\tVersus\tBetween-Graph\tReplication\n\ninception\tmodules,\tGoogLeNet",
      "content_length": 1329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 654,
      "content": "Inception-v4,\tResNet\n\nincremental\tlearning,\tOnline\tlearning,\tIncremental\tPCA\n\ninequality\tconstraints,\tSVM\tDual\tProblem\n\ninference,\tModel-based\tlearning,\tExercises,\tMemory\tRequirements,\tAn\tEncoder–Decoder Network\tfor\tMachine\tTranslation\n\ninfo(),\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\ninformation\tgain,\tGini\tImpurity\tor\tEntropy?\n\ninformation\ttheory,\tGini\tImpurity\tor\tEntropy?\n\ninit\tnode,\tSaving\tand\tRestoring\tModels\n\ninput\tgate,\tLSTM\tCell\n\ninput\tneurons,\tThe\tPerceptron\n\ninput_keep_prob,\tApplying\tDropout\n\ninstance-based\tlearning,\tInstance-based\tlearning,\tModel-based\tlearning\n\nInteractiveSession,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\nintercept\tterm,\tLinear\tRegression\n\nInternal\tCovariate\tShift\tproblem,\tBatch\tNormalization\n\ninter_op_parallelism_threads,\tParallel\tExecution\n\nintra_op_parallelism_threads,\tParallel\tExecution\n\ninverse_transform(),\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nin_top_k(),\tConstruction\tPhase\n\nirreducible\terror,\tLearning\tCurves\n\nisolated\tenvironment,\tCreate\tthe\tWorkspace-Create\tthe\tWorkspace\n\nIsomap,\tOther\tDimensionality\tReduction\tTechniques",
      "content_length": 1094,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 655,
      "content": "J\n\nK\n\nL\n\njobs,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\njoin(),\tMultiple\tDevices\tAcross\tMultiple\tServers,\tMultithreaded\treaders\tusing\ta\tCoordinator and\ta\tQueueRunner\n\nJupyter,\tCreate\tthe\tWorkspace,\tCreate\tthe\tWorkspace,\tTake\ta\tQuick\tLook\tat\tthe\tData Structure\n\nK-fold\tcross-validation,\tBetter\tEvaluation\tUsing\tCross-Validation-Better\tEvaluation\tUsing Cross-Validation,\tMeasuring\tAccuracy\tUsing\tCross-Validation\n\nk-Nearest\tNeighbors,\tModel-based\tlearning,\tMultilabel\tClassification\n\nKarush–Kuhn–Tucker\t(KKT)\tconditions,\tSVM\tDual\tProblem\n\nkeep\tprobability,\tDropout\n\nKeras,\tUp\tand\tRunning\twith\tTensorFlow\n\nKernel\tPCA\t(kPCA),\tKernel\tPCA-Selecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nkernel\ttrick,\tPolynomial\tKernel,\tGaussian\tRBF\tKernel,\tThe\tDual\tProblem-Kernelized\tSVM, Kernel\tPCA\n\nkernelized\tSVM,\tKernelized\tSVM-Kernelized\tSVM\n\nkernels,\tPolynomial\tKernel-Gaussian\tRBF\tKernel,\tOperations\tand\tkernels\n\nKullback–Leibler\tdivergence,\tSoftmax\tRegression,\tSparse\tAutoencoders\n\nl1_l2_regularizer(),\tℓ1\tand\tℓ2\tRegularization\n\nLabelBinarizer,\tTransformation\tPipelines\n\nlabels,\tSupervised\tlearning,\tFrame\tthe\tProblem\n\nLagrange\tfunction,\tSVM\tDual\tProblem-SVM\tDual\tProblem\n\nLagrange\tmultiplier,\tSVM\tDual\tProblem",
      "content_length": 1197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 656,
      "content": "landmarks,\tAdding\tSimilarity\tFeatures-Adding\tSimilarity\tFeatures\n\nlarge\tmargin\tclassification,\tLinear\tSVM\tClassification-Linear\tSVM\tClassification\n\nLasso\tRegression,\tLasso\tRegression-Lasso\tRegression\n\nlatent\tloss,\tVariational\tAutoencoders\n\nlatent\tspace,\tVariational\tAutoencoders\n\nlaw\tof\tlarge\tnumbers,\tVoting\tClassifiers\n\nleaky\tReLU,\tNonsaturating\tActivation\tFunctions\n\nlearning\trate,\tOnline\tlearning,\tGradient\tDescent,\tBatch\tGradient\tDescent-Stochastic\tGradient Descent\n\nlearning\trate\tscheduling,\tStochastic\tGradient\tDescent,\tLearning\tRate\tScheduling-Learning Rate\tScheduling\n\nLeNet-5\tarchitecture,\tThe\tArchitecture\tof\tthe\tVisual\tCortex,\tLeNet-5-LeNet-5\n\nLevenshtein\tdistance,\tGaussian\tRBF\tKernel\n\nliblinear\tlibrary,\tComputational\tComplexity\n\nlibsvm\tlibrary,\tComputational\tComplexity\n\nLinear\tDiscriminant\tAnalysis\t(LDA),\tOther\tDimensionality\tReduction\tTechniques\n\nlinear\tmodels\n\nearly\tstopping,\tEarly\tStopping-Early\tStopping\n\nElastic\tNet,\tElastic\tNet\n\nLasso\tRegression,\tLasso\tRegression-Lasso\tRegression\n\nLinear\tRegression\t(see\tLinear\tRegression)\n\nregression\t(see\tLinear\tRegression)\n\nRidge\tRegression,\tRidge\tRegression-Ridge\tRegression,\tElastic\tNet\n\nSVM,\tLinear\tSVM\tClassification-Soft\tMargin\tClassification",
      "content_length": 1208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 657,
      "content": "Linear\tRegression,\tModel-based\tlearning,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet, Training\tModels-Mini-batch\tGradient\tDescent,\tElastic\tNet\n\ncomputational\tcomplexity,\tComputational\tComplexity\n\nGradient\tDescent\tin,\tGradient\tDescent-Mini-batch\tGradient\tDescent\n\nlearning\tcurves\tin,\tLearning\tCurves-Learning\tCurves\n\nNormal\tEquation,\tThe\tNormal\tEquation-Computational\tComplexity\n\nregularizing\tmodels\t(see\tregularization)\n\nusing\tStochastic\tGradient\tDescent\t(SGD),\tStochastic\tGradient\tDescent\n\nwith\tTensorFlow,\tLinear\tRegression\twith\tTensorFlow-Linear\tRegression\twith TensorFlow\n\nlinear\tSVM\tclassification,\tLinear\tSVM\tClassification-Soft\tMargin\tClassification\n\nlinear\tthreshold\tunits\t(LTUs),\tThe\tPerceptron\n\nLipschitz\tcontinuous,\tGradient\tDescent\n\nLLE\t(Locally\tLinear\tEmbedding),\tLLE-LLE\n\nload_sample_images(),\tTensorFlow\tImplementation\n\nlocal\treceptive\tfield,\tThe\tArchitecture\tof\tthe\tVisual\tCortex\n\nlocal\tresponse\tnormalization,\tAlexNet\n\nlocal\tsessions,\tSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers\n\nlocation\tinvariance,\tPooling\tLayer\n\nlog\tloss,\tTraining\tand\tCost\tFunction\n\nlogging\tplacements,\tLogging\tplacements-Logging\tplacements\n\nlogistic\tfunction,\tEstimating\tProbabilities\n\nLogistic\tRegression,\tSupervised\tlearning,\tLogistic\tRegression-Softmax\tRegression\n\ndecision\tboundaries,\tDecision\tBoundaries-Decision\tBoundaries",
      "content_length": 1329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 658,
      "content": "estimating\tprobablities,\tEstimating\tProbabilities-Estimating\tProbabilities\n\nSoftmax\tRegression\tmodel,\tSoftmax\tRegression-Softmax\tRegression\n\ntraining\tand\tcost\tfunction,\tTraining\tand\tCost\tFunction-Training\tand\tCost\tFunction\n\nlog_device_placement,\tLogging\tplacements\n\nLSTM\t(Long\tShort-Term\tMemory)\tcell,\tLSTM\tCell-GRU\tCell",
      "content_length": 320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 659,
      "content": "M\n\nmachine\tcontrol\t(see\treinforcement\tlearning)\n\nMachine\tLearning\n\nlarge-scale\tprojects\t(see\tTensorFlow)\n\nnotations,\tSelect\ta\tPerformance\tMeasure-Select\ta\tPerformance\tMeasure\n\nprocess\texample,\tEnd-to-End\tMachine\tLearning\tProject-Exercises\n\nproject\tchecklist,\tLook\tat\tthe\tBig\tPicture,\tMachine\tLearning\tProject\tChecklist- Launch!\n\nresources\ton,\tOther\tResources-Other\tResources\n\nuses\tfor,\tMachine\tLearning\tin\tYour\tProjects-Machine\tLearning\tin\tYour\tProjects\n\nMachine\tLearning\tbasics\n\nattributes,\tSupervised\tlearning\n\nchallenges,\tMain\tChallenges\tof\tMachine\tLearning-Stepping\tBack\n\nalgorithm\tproblems,\tOverfitting\tthe\tTraining\tData-Underfitting\tthe\tTraining Data\n\ntraining\tdata\tproblems,\tPoor-Quality\tData\n\ndefinition,\tWhat\tIs\tMachine\tLearning?\n\nfeatures,\tSupervised\tlearning\n\noverview,\tThe\tMachine\tLearning\tLandscape\n\nreasons\tfor\tusing,\tWhy\tUse\tMachine\tLearning?-Why\tUse\tMachine\tLearning?\n\nspam\tfilter\texample,\tWhat\tIs\tMachine\tLearning?-Why\tUse\tMachine\tLearning?\n\nsummary,\tStepping\tBack\n\ntesting\tand\tvalidating,\tTesting\tand\tValidating-Testing\tand\tValidating\n\ntypes\tof\tsystems,\tTypes\tof\tMachine\tLearning\tSystems-Model-based\tlearning\n\nbatch\tand\tonline\tlearning,\tBatch\tand\tOnline\tLearning-Online\tlearning",
      "content_length": 1196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 660,
      "content": "instance-based\tversus\tmodel-based\tlearning,\tInstance-Based\tVersus\tModel- Based\tLearning-Model-based\tlearning\n\nsupervised/unsupervised\tlearning,\tSupervised/Unsupervised\tLearning- Reinforcement\tLearning\n\nworkflow\texample,\tModel-based\tlearning-Model-based\tlearning\n\nmachine\ttranslation\t(see\tnatural\tlanguage\tprocessing\t(NLP))\n\nmake(),\tIntroduction\tto\tOpenAI\tGym\n\nManhattan\tnorm,\tSelect\ta\tPerformance\tMeasure\n\nmanifold\tassumption/hypothesis,\tManifold\tLearning\n\nManifold\tLearning,\tManifold\tLearning,\tLLE\n\n(see\talso\tLLE\t(Locally\tLinear\tEmbedding)\n\nMapReduce,\tFrame\tthe\tProblem\n\nmargin\tviolations,\tSoft\tMargin\tClassification\n\nMarkov\tchains,\tMarkov\tDecision\tProcesses\n\nMarkov\tdecision\tprocesses,\tMarkov\tDecision\tProcesses-Markov\tDecision\tProcesses\n\nmaster\tservice,\tThe\tMaster\tand\tWorker\tServices\n\nMatplotlib,\tCreate\tthe\tWorkspace,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure,\tThe\tROC Curve,\tError\tAnalysis\n\nmax\tmargin\tlearning,\tPretraining\ton\tan\tAuxiliary\tTask\n\nmax\tpooling\tlayer,\tPooling\tLayer\n\nmax-norm\tregularization,\tMax-Norm\tRegularization-Max-Norm\tRegularization\n\nmax_norm(),\tMax-Norm\tRegularization\n\nmax_norm_regularizer(),\tMax-Norm\tRegularization\n\nmax_pool(),\tPooling\tLayer",
      "content_length": 1169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 661,
      "content": "Mean\tAbsolute\tError\t(MAE),\tSelect\ta\tPerformance\tMeasure-Select\ta\tPerformance\tMeasure\n\nmean\tcoding,\tVariational\tAutoencoders\n\nMean\tSquare\tError\t(MSE),\tLinear\tRegression,\tManually\tComputing\tthe\tGradients,\tSparse Autoencoders\n\nmeasure\tof\tsimilarity,\tInstance-based\tlearning\n\nmemmap,\tIncremental\tPCA\n\nmemory\tcells,\tModel\tParallelism,\tMemory\tCells\n\nMercer's\ttheorem,\tKernelized\tSVM\n\nmeta\tlearner\t(see\tblending)\n\nmin-max\tscaling,\tFeature\tScaling\n\nMini-batch\tGradient\tDescent,\tMini-batch\tGradient\tDescent-Mini-batch\tGradient\tDescent, Training\tand\tCost\tFunction,\tFeeding\tData\tto\tthe\tTraining\tAlgorithm-Feeding\tData\tto\tthe Training\tAlgorithm\n\nmini-batches,\tOnline\tlearning\n\nminimize(),\tGradient\tClipping,\tFreezing\tthe\tLower\tLayers,\tPolicy\tGradients,\tLearning\tto\tPlay Ms.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nmin_after_dequeue,\tRandomShuffleQueue\n\nMNIST\tdataset,\tMNIST-MNIST\n\nmodel\tparallelism,\tModel\tParallelism-Model\tParallelism\n\nmodel\tparameters,\tGradient\tDescent,\tBatch\tGradient\tDescent,\tEarly\tStopping,\tUnder\tthe Hood,\tQuadratic\tProgramming,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession, Construction\tPhase,\tTraining\tRNNs\n\ndefining,\tModel-based\tlearning\n\nmodel\tselection,\tModel-based\tlearning\n\nmodel\tzoos,\tModel\tZoos",
      "content_length": 1214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 662,
      "content": "N\n\nmodel-based\tlearning,\tModel-based\tlearning-Model-based\tlearning\n\nmodels\n\nanalyzing,\tAnalyze\tthe\tBest\tModels\tand\tTheir\tErrors-Analyze\tthe\tBest\tModels\tand Their\tErrors\n\nevaluating\ton\ttest\tset,\tEvaluate\tYour\tSystem\ton\tthe\tTest\tSet-Evaluate\tYour\tSystem\ton the\tTest\tSet\n\nmoments,\tAdam\tOptimization\n\nMomentum\toptimization,\tMomentum\tOptimization-Momentum\tOptimization\n\nMonte\tCarlo\ttree\tsearch,\tPolicy\tGradients\n\nMulti-Layer\tPerceptrons\t(MLP),\tIntroduction\tto\tArtificial\tNeural\tNetworks,\tThe\tPerceptron- Multi-Layer\tPerceptron\tand\tBackpropagation,\tNeural\tNetwork\tPolicies\n\ntraining\twith\tTF.Learn,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nmulticlass\tclassifiers,\tMulticlass\tClassification-Multiclass\tClassification\n\nMultidimensional\tScaling\t(MDS),\tOther\tDimensionality\tReduction\tTechniques\n\nmultilabel\tclassifiers,\tMultilabel\tClassification-Multilabel\tClassification\n\nMultinomial\tLogistic\tRegression\t(see\tSoftmax\tRegression)\n\nmultinomial(),\tNeural\tNetwork\tPolicies\n\nmultioutput\tclassifiers,\tMultioutput\tClassification-Multioutput\tClassification\n\nMultiRNNCell,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs\n\nmultithreaded\treaders,\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner- Multithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\nmultivariate\tregression,\tFrame\tthe\tProblem\n\nnaive\tBayes\tclassifiers,\tMulticlass\tClassification\n\nname\tscopes,\tName\tScopes",
      "content_length": 1374,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 663,
      "content": "natural\tlanguage\tprocessing\t(NLP),\tRecurrent\tNeural\tNetworks,\tNatural\tLanguage Processing-An\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nencoder-decoder\tnetwork\tfor\tmachine\ttranslation,\tAn\tEncoder–Decoder\tNetwork\tfor Machine\tTranslation-An\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nTensorFlow\ttutorials,\tNatural\tLanguage\tProcessing,\tAn\tEncoder–Decoder\tNetwork\tfor Machine\tTranslation\n\nword\tembeddings,\tWord\tEmbeddings-Word\tEmbeddings\n\nNesterov\tAccelerated\tGradient\t(NAG),\tNesterov\tAccelerated\tGradient-Nesterov\tAccelerated Gradient\n\nNesterov\tmomentum\toptimization,\tNesterov\tAccelerated\tGradient-Nesterov\tAccelerated Gradient\n\nnetwork\ttopology,\tFine-Tuning\tNeural\tNetwork\tHyperparameters\n\nneural\tnetwork\thyperparameters,\tFine-Tuning\tNeural\tNetwork\tHyperparameters-Activation Functions\n\nactivation\tfunctions,\tActivation\tFunctions\n\nneurons\tper\thidden\tlayer,\tNumber\tof\tNeurons\tper\tHidden\tLayer\n\nnumber\tof\thidden\tlayers,\tNumber\tof\tHidden\tLayers-Number\tof\tHidden\tLayers\n\nneural\tnetwork\tpolicies,\tNeural\tNetwork\tPolicies-Neural\tNetwork\tPolicies\n\nneurons\n\nbiological,\tFrom\tBiological\tto\tArtificial\tNeurons-Biological\tNeurons\n\nlogical\tcomputations\twith,\tLogical\tComputations\twith\tNeurons\n\nneuron_layer(),\tConstruction\tPhase\n\nnext_batch(),\tExecution\tPhase\n\nNo\tFree\tLunch\ttheorem,\tTesting\tand\tValidating\n\nnode\tedges,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\nnonlinear\tdimensionality\treduction\t(NLDR),\tLLE",
      "content_length": 1425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 664,
      "content": "(see\talso\tKernel\tPCA;\tLLE\t(Locally\tLinear\tEmbedding))\n\nnonlinear\tSVM\tclassification,\tNonlinear\tSVM\tClassification-Computational\tComplexity\n\ncomputational\tcomplexity,\tComputational\tComplexity\n\nGaussian\tRBF\tkernel,\tGaussian\tRBF\tKernel-Gaussian\tRBF\tKernel\n\nwith\tpolynomial\tfeatures,\tNonlinear\tSVM\tClassification-Polynomial\tKernel\n\npolynomial\tkernel,\tPolynomial\tKernel-Polynomial\tKernel\n\nsimilarity\tfeatures,\tadding,\tAdding\tSimilarity\tFeatures-Adding\tSimilarity\tFeatures\n\nnonparametric\tmodels,\tRegularization\tHyperparameters\n\nnonresponse\tbias,\tNonrepresentative\tTraining\tData\n\nnonsaturating\tactivation\tfunctions,\tNonsaturating\tActivation\tFunctions-Nonsaturating Activation\tFunctions\n\nNormal\tEquation,\tThe\tNormal\tEquation-Computational\tComplexity\n\nnormalization,\tFeature\tScaling\n\nnormalized\texponential,\tSoftmax\tRegression\n\nnorms,\tSelect\ta\tPerformance\tMeasure\n\nnotations,\tSelect\ta\tPerformance\tMeasure-Select\ta\tPerformance\tMeasure\n\nNP-Complete\tproblems,\tThe\tCART\tTraining\tAlgorithm\n\nnull\thypothesis,\tRegularization\tHyperparameters\n\nnumerical\tdifferentiation,\tNumerical\tDifferentiation\n\nNumPy,\tCreate\tthe\tWorkspace\n\nNumPy\tarrays,\tHandling\tText\tand\tCategorical\tAttributes\n\nNVidia\tCompute\tCapability,\tInstallation\n\nnvidia-smi,\tManaging\tthe\tGPU\tRAM",
      "content_length": 1238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 665,
      "content": "O\n\nn_components,\tChoosing\tthe\tRight\tNumber\tof\tDimensions\n\nobservation\tspace,\tNeural\tNetwork\tPolicies\n\noff-policy\talgorithm,\tTemporal\tDifference\tLearning\tand\tQ-Learning\n\noffline\tlearning,\tBatch\tlearning\n\none-hot\tencoding,\tHandling\tText\tand\tCategorical\tAttributes\n\none-versus-all\t(OvA)\tstrategy,\tMulticlass\tClassification,\tSoftmax\tRegression,\tExercises\n\none-versus-one\t(OvO)\tstrategy,\tMulticlass\tClassification\n\nonline\tlearning,\tOnline\tlearning-Online\tlearning\n\nonline\tSVMs,\tOnline\tSVMs-Online\tSVMs\n\nOpenAI\tGym,\tIntroduction\tto\tOpenAI\tGym-Introduction\tto\tOpenAI\tGym\n\noperation_timeout_in_ms,\tIn-Graph\tVersus\tBetween-Graph\tReplication\n\nOptical\tCharacter\tRecognition\t(OCR),\tThe\tMachine\tLearning\tLandscape\n\noptimal\tstate\tvalue,\tMarkov\tDecision\tProcesses\n\noptimizers,\tFaster\tOptimizers-Learning\tRate\tScheduling\n\nAdaGrad,\tAdaGrad-AdaGrad\n\nAdam\toptimization,\tAdam\tOptimization-Adam\tOptimization,\tAdam\tOptimization\n\nGradient\tDescent\t(see\tGradient\tDescent\toptimizer)\n\nlearning\trate\tscheduling,\tLearning\tRate\tScheduling-Learning\tRate\tScheduling\n\nMomentum\toptimization,\tMomentum\tOptimization-Momentum\tOptimization\n\nNesterov\tAccelerated\tGradient\t(NAG),\tNesterov\tAccelerated\tGradient-Nesterov Accelerated\tGradient\n\nRMSProp,\tRMSProp\n\nout-of-bag\tevaluation,\tOut-of-Bag\tEvaluation-Out-of-Bag\tEvaluation",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 666,
      "content": "P\n\nout-of-core\tlearning,\tOnline\tlearning\n\nout-of-memory\t(OOM)\terrors,\tStatic\tUnrolling\tThrough\tTime\n\nout-of-sample\terror,\tTesting\tand\tValidating\n\nOutOfRangeError,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded\treaders using\ta\tCoordinator\tand\ta\tQueueRunner\n\noutput\tgate,\tLSTM\tCell\n\noutput\tlayer,\tMulti-Layer\tPerceptron\tand\tBackpropagation\n\nOutputProjectionWrapper,\tTraining\tto\tPredict\tTime\tSeries-Training\tto\tPredict\tTime\tSeries\n\noutput_keep_prob,\tApplying\tDropout\n\novercomplete\tautoencoder,\tUnsupervised\tPretraining\tUsing\tStacked\tAutoencoders\n\noverfitting,\tOverfitting\tthe\tTraining\tData-Overfitting\tthe\tTraining\tData,\tCreate\ta\tTest\tSet, Soft\tMargin\tClassification,\tGaussian\tRBF\tKernel,\tRegularization\tHyperparameters, Regression,\tNumber\tof\tNeurons\tper\tHidden\tLayer\n\navoiding\tthrough\tregularization,\tAvoiding\tOverfitting\tThrough\tRegularization-Data Augmentation\n\np-value,\tRegularization\tHyperparameters\n\nPaddingFIFOQueue,\tPaddingFifoQueue\n\nPandas,\tCreate\tthe\tWorkspace,\tDownload\tthe\tData\n\nscatter_matrix,\tLooking\tfor\tCorrelations-Looking\tfor\tCorrelations\n\nparallel\tdistributed\tcomputing,\tDistributing\tTensorFlow\tAcross\tDevices\tand\tServers-Exercises\n\ndata\tparallelism,\tData\tParallelism-TensorFlow\timplementation\n\nin-graph\tversus\tbetween-graph\treplication,\tIn-Graph\tVersus\tBetween-Graph Replication-Model\tParallelism\n\nmodel\tparallelism,\tModel\tParallelism-Model\tParallelism\n\nmultiple\tdevices\tacross\tmultiple\tservers,\tMultiple\tDevices\tAcross\tMultiple\tServers-",
      "content_length": 1472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 667,
      "content": "Other\tconvenience\tfunctions\n\nasynchronous\tcommunication\tusing\tqueues,\tAsynchronous\tCommunication\tUsing TensorFlow\tQueues-PaddingFifoQueue\n\nloading\ttraining\tdata,\tLoading\tData\tDirectly\tfrom\tthe\tGraph-Other\tconvenience functions\n\nmaster\tand\tworker\tservices,\tThe\tMaster\tand\tWorker\tServices\n\nopening\ta\tsession,\tOpening\ta\tSession\n\npinning\toperations\tacross\ttasks,\tPinning\tOperations\tAcross\tTasks\n\nsharding\tvariables,\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\nsharing\tstate\tacross\tsessions,\tSharing\tState\tAcross\tSessions\tUsing\tResource Containers-Sharing\tState\tAcross\tSessions\tUsing\tResource\tContainers\n\nmultiple\tdevices\ton\ta\tsingle\tmachine,\tMultiple\tDevices\ton\ta\tSingle\tMachine-Control Dependencies\n\ncontrol\tdependencies,\tControl\tDependencies\n\ninstallation,\tInstallation-Installation\n\nmanaging\tthe\tGPU\tRAM,\tManaging\tthe\tGPU\tRAM-Managing\tthe\tGPU\tRAM\n\nparallel\texecution,\tParallel\tExecution-Parallel\tExecution\n\nplacing\toperations\ton\tdevices,\tPlacing\tOperations\ton\tDevices-Soft\tplacement\n\none\tneural\tnetwork\tper\tdevice,\tOne\tNeural\tNetwork\tper\tDevice-One\tNeural\tNetwork per\tDevice\n\nparameter\tefficiency,\tNumber\tof\tHidden\tLayers\n\nparameter\tmatrix,\tSoftmax\tRegression\n\nparameter\tserver\t(ps),\tMultiple\tDevices\tAcross\tMultiple\tServers\n\nparameter\tspace,\tGradient\tDescent\n\nparameter\tvector,\tLinear\tRegression,\tGradient\tDescent,\tTraining\tand\tCost\tFunction,\tSoftmax Regression",
      "content_length": 1371,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 668,
      "content": "parametric\tmodels,\tRegularization\tHyperparameters\n\npartial\tderivative,\tBatch\tGradient\tDescent\n\npartial_fit(),\tIncremental\tPCA\n\nPearson's\tr,\tLooking\tfor\tCorrelations\n\npeephole\tconnections,\tPeephole\tConnections\n\npenalties\t(see\trewards,\tin\tRL)\n\npercentiles,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\nPerceptron\tconvergence\ttheorem,\tThe\tPerceptron\n\nPerceptrons,\tThe\tPerceptron-Multi-Layer\tPerceptron\tand\tBackpropagation\n\nversus\tLogistic\tRegression,\tThe\tPerceptron\n\ntraining,\tThe\tPerceptron-The\tPerceptron\n\nperformance\tmeasures,\tSelect\ta\tPerformance\tMeasure-Select\ta\tPerformance\tMeasure\n\nconfusion\tmatrix,\tConfusion\tMatrix-Confusion\tMatrix\n\ncross-validation,\tMeasuring\tAccuracy\tUsing\tCross-Validation-Measuring\tAccuracy Using\tCross-Validation\n\nprecision\tand\trecall,\tPrecision\tand\tRecall-Precision/Recall\tTradeoff\n\nROC\t(receiver\toperating\tcharacteristic)\tcurve,\tThe\tROC\tCurve-The\tROC\tCurve\n\nperformance\tscheduling,\tLearning\tRate\tScheduling\n\npermutation(),\tCreate\ta\tTest\tSet\n\nPG\talgorithms,\tPolicy\tGradients\n\nphoto-hosting\tservices,\tSemisupervised\tlearning\n\npinning\toperations,\tPinning\tOperations\tAcross\tTasks\n\npip,\tCreate\tthe\tWorkspace\n\nPipeline\tconstructor,\tTransformation\tPipelines-Select\tand\tTrain\ta\tModel",
      "content_length": 1201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 669,
      "content": "pipelines,\tFrame\tthe\tProblem\n\nplaceholder\tnodes,\tFeeding\tData\tto\tthe\tTraining\tAlgorithm\n\nplacers\t(see\tsimple\tplacer;\tdynamic\tplacer)\n\npolicy,\tPolicy\tSearch\n\npolicy\tgradients,\tPolicy\tSearch\t(see\tPG\talgorithms)\n\npolicy\tspace,\tPolicy\tSearch\n\npolynomial\tfeatures,\tadding,\tNonlinear\tSVM\tClassification-Polynomial\tKernel\n\npolynomial\tkernel,\tPolynomial\tKernel-Polynomial\tKernel,\tKernelized\tSVM\n\nPolynomial\tRegression,\tTraining\tModels,\tPolynomial\tRegression-Polynomial\tRegression\n\nlearning\tcurves\tin,\tLearning\tCurves-Learning\tCurves\n\npooling\tkernel,\tPooling\tLayer\n\npooling\tlayer,\tPooling\tLayer-Pooling\tLayer\n\npower\tscheduling,\tLearning\tRate\tScheduling\n\nprecision,\tConfusion\tMatrix\n\nprecision\tand\trecall,\tPrecision\tand\tRecall-Precision/Recall\tTradeoff\n\nF-1\tscore,\tPrecision\tand\tRecall-Precision\tand\tRecall\n\nprecision/recall\t(PR)\tcurve,\tThe\tROC\tCurve\n\nprecision/recall\ttradeoff,\tPrecision/Recall\tTradeoff-Precision/Recall\tTradeoff\n\npredetermined\tpiecewise\tconstant\tlearning\trate,\tLearning\tRate\tScheduling\n\npredict(),\tData\tCleaning\n\npredicted\tclass,\tConfusion\tMatrix\n\npredictions,\tConfusion\tMatrix-Confusion\tMatrix,\tDecision\tFunction\tand\tPredictions-Decision Function\tand\tPredictions,\tMaking\tPredictions-Estimating\tClass\tProbabilities",
      "content_length": 1223,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 670,
      "content": "predictors,\tSupervised\tlearning,\tData\tCleaning\n\npreloading\ttraining\tdata,\tPreload\tthe\tdata\tinto\ta\tvariable\n\nPReLU\t(parametric\tleaky\tReLU),\tNonsaturating\tActivation\tFunctions\n\npreprocessed\tattributes,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\npretrained\tlayers\treuse,\tReusing\tPretrained\tLayers-Pretraining\ton\tan\tAuxiliary\tTask\n\nauxiliary\ttask,\tPretraining\ton\tan\tAuxiliary\tTask-Pretraining\ton\tan\tAuxiliary\tTask\n\ncaching\tfrozen\tlayers,\tCaching\tthe\tFrozen\tLayers\n\nfreezing\tlower\tlayers,\tFreezing\tthe\tLower\tLayers\n\nmodel\tzoos,\tModel\tZoos\n\nother\tframeworks,\tReusing\tModels\tfrom\tOther\tFrameworks\n\nTensorFlow\tmodel,\tReusing\ta\tTensorFlow\tModel-Reusing\ta\tTensorFlow\tModel\n\nunsupervised\tpretraining,\tUnsupervised\tPretraining-Unsupervised\tPretraining\n\nupper\tlayers,\tTweaking,\tDropping,\tor\tReplacing\tthe\tUpper\tLayers\n\nPretty\tTensor,\tUp\tand\tRunning\twith\tTensorFlow\n\nprimal\tproblem,\tThe\tDual\tProblem\n\nprincipal\tcomponent,\tPrincipal\tComponents\n\nPrincipal\tComponent\tAnalysis\t(PCA),\tPCA-Randomized\tPCA\n\nexplained\tvariance\tratios,\tExplained\tVariance\tRatio\n\nfinding\tprincipal\tcomponents,\tPrincipal\tComponents-Principal\tComponents\n\nfor\tcompression,\tPCA\tfor\tCompression-Incremental\tPCA\n\nIncremental\tPCA,\tIncremental\tPCA-Randomized\tPCA\n\nKernel\tPCA\t(kPCA),\tKernel\tPCA-Selecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nprojecting\tdown\tto\td\tdimensions,\tProjecting\tDown\tto\td\tDimensions",
      "content_length": 1354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 671,
      "content": "Q\n\nRandomized\tPCA,\tRandomized\tPCA\n\nScikit\tLearn\tfor,\tUsing\tScikit-Learn\n\nvariance,\tpreserving,\tPreserving\tthe\tVariance-Preserving\tthe\tVariance\n\nprobabilistic\tautoencoders,\tVariational\tAutoencoders\n\nprobabilities,\testimating,\tEstimating\tProbabilities-Estimating\tProbabilities,\tEstimating\tClass Probabilities\n\nproducer\tfunctions,\tOther\tconvenience\tfunctions\n\nprojection,\tProjection-Projection\n\npropositional\tlogic,\tFrom\tBiological\tto\tArtificial\tNeurons\n\npruning,\tRegularization\tHyperparameters,\tSymbolic\tDifferentiation\n\nPython\n\nisolated\tenvironment\tin,\tCreate\tthe\tWorkspace-Create\tthe\tWorkspace\n\nnotebooks\tin,\tCreate\tthe\tWorkspace-Download\tthe\tData\n\npickle,\tBetter\tEvaluation\tUsing\tCross-Validation\n\npip,\tCreate\tthe\tWorkspace\n\nQ-Learning\talgorithm,\tTemporal\tDifference\tLearning\tand\tQ-Learning-Learning\tto\tPlay\tMs. Pac-Man\tUsing\tDeep\tQ-Learning\n\napproximate\tQ-Learning,\tApproximate\tQ-Learning\n\ndeep\tQ-Learning,\tApproximate\tQ-Learning-Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep Q-Learning\n\nQ-Value\tIteration\tAlgorithm,\tMarkov\tDecision\tProcesses\n\nQ-Values,\tMarkov\tDecision\tProcesses\n\nQuadratic\tProgramming\t(QP)\tProblems,\tQuadratic\tProgramming-Quadratic\tProgramming\n\nquantizing,\tBandwidth\tsaturation",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 672,
      "content": "R\n\nqueries\tper\tsecond\t(QPS),\tOne\tNeural\tNetwork\tper\tDevice\n\nQueueRunner,\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner-Multithreaded readers\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\nqueues,\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues-PaddingFifoQueue\n\nclosing,\tClosing\ta\tqueue\n\ndequeuing\tdata,\tDequeuing\tdata\n\nenqueuing\tdata,\tEnqueuing\tdata\n\nfirst-in\tfirst-out\t(FIFO),\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues\n\nof\ttuples,\tQueues\tof\ttuples\n\nPaddingFIFOQueue,\tPaddingFifoQueue\n\nRandomShuffleQueue,\tRandomShuffleQueue\n\nq_network(),\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nRadial\tBasis\tFunction\t(RBF),\tAdding\tSimilarity\tFeatures\n\nRandom\tForests,\tBetter\tEvaluation\tUsing\tCross-Validation-Grid\tSearch,\tMulticlass Classification,\tDecision\tTrees,\tInstability,\tEnsemble\tLearning\tand\tRandom\tForests,\tRandom Forests-Feature\tImportance\n\nExtra-Trees,\tExtra-Trees\n\nfeature\timportance,\tFeature\tImportance-Feature\tImportance\n\nrandom\tinitialization,\tGradient\tDescent,\tBatch\tGradient\tDescent,\tStochastic\tGradient Descent,\tVanishing/Exploding\tGradients\tProblems\n\nRandom\tPatches\tand\tRandom\tSubspaces,\tRandom\tPatches\tand\tRandom\tSubspaces\n\nrandomized\tleaky\tReLU\t(RReLU),\tNonsaturating\tActivation\tFunctions\n\nRandomized\tPCA,\tRandomized\tPCA\n\nrandomized\tsearch,\tRandomized\tSearch,\tFine-Tuning\tNeural\tNetwork\tHyperparameters",
      "content_length": 1338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 673,
      "content": "RandomShuffleQueue,\tRandomShuffleQueue,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe graph\n\nrandom_uniform(),\tManually\tComputing\tthe\tGradients\n\nreader\toperations,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph\n\nrecall,\tConfusion\tMatrix\n\nrecognition\tnetwork,\tEfficient\tData\tRepresentations\n\nreconstruction\terror,\tPCA\tfor\tCompression\n\nreconstruction\tloss,\tEfficient\tData\tRepresentations,\tTensorFlow\tImplementation,\tVariational Autoencoders\n\nreconstruction\tpre-image,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nreconstructions,\tEfficient\tData\tRepresentations\n\nrecurrent\tneural\tnetworks\t(RNNs),\tRecurrent\tNeural\tNetworks-Exercises\n\ndeep\tRNNs,\tDeep\tRNNs-The\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps\n\nexploration\tpolicies,\tExploration\tPolicies\n\nGRU\tcell,\tGRU\tCell-GRU\tCell\n\ninput\tand\toutput\tsequences,\tInput\tand\tOutput\tSequences-Input\tand\tOutput\tSequences\n\nLSTM\tcell,\tLSTM\tCell-GRU\tCell\n\nnatural\tlanguage\tprocessing\t(NLP),\tNatural\tLanguage\tProcessing-An\tEncoder–Decoder Network\tfor\tMachine\tTranslation\n\nin\tTensorFlow,\tBasic\tRNNs\tin\tTensorFlow-Handling\tVariable-Length\tOutput\tSequences\n\ndynamic\tunrolling\tthrough\ttime,\tDynamic\tUnrolling\tThrough\tTime\n\nstatic\tunrolling\tthrough\ttime,\tStatic\tUnrolling\tThrough\tTime-Static\tUnrolling Through\tTime\n\nvariable\tlength\tinput\tsequences,\tHandling\tVariable\tLength\tInput\tSequences\n\nvariable\tlength\toutput\tsequences,\tHandling\tVariable-Length\tOutput\tSequences",
      "content_length": 1395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 674,
      "content": "training,\tTraining\tRNNs-Creative\tRNN\n\nbackpropagation\tthrough\ttime\t(BPTT),\tTraining\tRNNs\n\ncreative\tsequences,\tCreative\tRNN\n\nsequence\tclassifiers,\tTraining\ta\tSequence\tClassifier-Training\ta\tSequence Classifier\n\ntime\tseries\tpredictions,\tTraining\tto\tPredict\tTime\tSeries-Training\tto\tPredict\tTime Series\n\nrecurrent\tneurons,\tRecurrent\tNeurons-Input\tand\tOutput\tSequences\n\nmemory\tcells,\tMemory\tCells\n\nreduce_mean(),\tConstruction\tPhase\n\nreduce_sum(),\tTensorFlow\tImplementation-TensorFlow\tImplementation,\tVariational Autoencoders,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nregression,\tSupervised\tlearning\n\nDecision\tTrees,\tRegression-Regression\n\nregression\tmodels\n\nlinear,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet\n\nregression\tversus\tclassification,\tMultioutput\tClassification\n\nregularization,\tOverfitting\tthe\tTraining\tData-Overfitting\tthe\tTraining\tData,\tTesting\tand Validating,\tRegularized\tLinear\tModels-Early\tStopping\n\ndata\taugmentation,\tData\tAugmentation-Data\tAugmentation\n\nDecision\tTrees,\tRegularization\tHyperparameters-Regularization\tHyperparameters\n\ndropout,\tDropout-Dropout\n\nearly\tstopping,\tEarly\tStopping-Early\tStopping,\tEarly\tStopping\n\nElastic\tNet,\tElastic\tNet\n\nLasso\tRegression,\tLasso\tRegression-Lasso\tRegression\n\nmax-norm,\tMax-Norm\tRegularization-Max-Norm\tRegularization",
      "content_length": 1281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 675,
      "content": "Ridge\tRegression,\tRidge\tRegression-Ridge\tRegression\n\nshrinkage,\tGradient\tBoosting\n\nℓ\t1\tand\tℓ\t2\tregularization,\tℓ1\tand\tℓ2\tRegularization-ℓ1\tand\tℓ2\tRegularization\n\nREINFORCE\talgorithms,\tPolicy\tGradients\n\nReinforcement\tLearning\t(RL),\tReinforcement\tLearning-Reinforcement\tLearning, Reinforcement\tLearning-Thank\tYou!\n\nactions,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem-Evaluating\tActions:\tThe Credit\tAssignment\tProblem\n\ncredit\tassignment\tproblem,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem- Evaluating\tActions:\tThe\tCredit\tAssignment\tProblem\n\ndiscount\trate,\tEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem\n\nexamples\tof,\tLearning\tto\tOptimize\tRewards\n\nMarkov\tdecision\tprocesses,\tMarkov\tDecision\tProcesses-Markov\tDecision\tProcesses\n\nneural\tnetwork\tpolicies,\tNeural\tNetwork\tPolicies-Neural\tNetwork\tPolicies\n\nOpenAI\tgym,\tIntroduction\tto\tOpenAI\tGym-Introduction\tto\tOpenAI\tGym\n\nPG\talgorithms,\tPolicy\tGradients-Policy\tGradients\n\npolicy\tsearch,\tPolicy\tSearch-Policy\tSearch\n\nQ-Learning\talgorithm,\tTemporal\tDifference\tLearning\tand\tQ-Learning-Learning\tto\tPlay Ms.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nrewards,\tlearning\tto\toptimize,\tLearning\tto\tOptimize\tRewards-Learning\tto\tOptimize Rewards\n\nTemporal\tDifference\t(TD)\tLearning,\tTemporal\tDifference\tLearning\tand\tQ-Learning- Temporal\tDifference\tLearning\tand\tQ-Learning\n\nReLU\t(rectified\tlinear\tunits),\tModularity-Modularity\n\nReLU\tactivation,\tResNet",
      "content_length": 1386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 676,
      "content": "ReLU\tfunction,\tMulti-Layer\tPerceptron\tand\tBackpropagation,\tActivation\tFunctions,\tXavier and\tHe\tInitialization-Nonsaturating\tActivation\tFunctions\n\nrelu(z),\tConstruction\tPhase\n\nrender(),\tIntroduction\tto\tOpenAI\tGym\n\nreplay\tmemory,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nreplica_device_setter(),\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\nrequest_stop(),\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\nreset(),\tIntroduction\tto\tOpenAI\tGym\n\nreset_default_graph(),\tManaging\tGraphs\n\nreshape(),\tTraining\tto\tPredict\tTime\tSeries\n\nresidual\terrors,\tGradient\tBoosting-Gradient\tBoosting\n\nresidual\tlearning,\tResNet\n\nresidual\tnetwork\t(ResNet),\tModel\tZoos,\tResNet-ResNet\n\nresidual\tunits,\tResNet\n\nResNet,\tResNet-ResNet\n\nresource\tcontainers,\tSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers-Sharing\tState Across\tSessions\tUsing\tResource\tContainers\n\nrestore(),\tSaving\tand\tRestoring\tModels\n\nrestricted\tBoltzmann\tmachines\t(RBMs),\tSemisupervised\tlearning,\tUnsupervised\tPretraining, Boltzmann\tMachines\n\nreuse_variables(),\tSharing\tVariables\n\nreverse-mode\tautodiff,\tReverse-Mode\tAutodiff-Reverse-Mode\tAutodiff\n\nrewards,\tin\tRL,\tLearning\tto\tOptimize\tRewards-Learning\tto\tOptimize\tRewards\n\nrgb_array,\tIntroduction\tto\tOpenAI\tGym",
      "content_length": 1240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 677,
      "content": "S\n\nRidge\tRegression,\tRidge\tRegression-Ridge\tRegression,\tElastic\tNet\n\nRMSProp,\tRMSProp\n\nROC\t(receiver\toperating\tcharacteristic)\tcurve,\tThe\tROC\tCurve-The\tROC\tCurve\n\nRoot\tMean\tSquare\tError\t(RMSE),\tSelect\ta\tPerformance\tMeasure-Select\ta\tPerformance Measure,\tLinear\tRegression\n\nRReLU\t(randomized\tleaky\tReLU),\tNonsaturating\tActivation\tFunctions\n\nrun(),\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession,\tIn-Graph\tVersus\tBetween-Graph Replication\n\nSampled\tSoftmax,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nsampling\tbias,\tNonrepresentative\tTraining\tData-Poor-Quality\tData,\tCreate\ta\tTest\tSet\n\nsampling\tnoise,\tNonrepresentative\tTraining\tData\n\nsave(),\tSaving\tand\tRestoring\tModels\n\nSaver\tnode,\tSaving\tand\tRestoring\tModels\n\nScikit\tFlow,\tUp\tand\tRunning\twith\tTensorFlow\n\nScikit-Learn,\tCreate\tthe\tWorkspace\n\nabout,\tObjective\tand\tApproach\n\nbagging\tand\tpasting\tin,\tBagging\tand\tPasting\tin\tScikit-Learn-Bagging\tand\tPasting\tin Scikit-Learn\n\nCART\talgorithm,\tMaking\tPredictions-The\tCART\tTraining\tAlgorithm,\tRegression\n\ncross-validation,\tBetter\tEvaluation\tUsing\tCross-Validation-Better\tEvaluation\tUsing Cross-Validation\n\ndesign\tprinciples,\tData\tCleaning-Data\tCleaning\n\nimputer,\tData\tCleaning-Handling\tText\tand\tCategorical\tAttributes\n\nLinearSVR\tclass,\tSVM\tRegression",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 678,
      "content": "MinMaxScaler,\tFeature\tScaling\n\nmin_\tand\tmax_\thyperparameters,\tRegularization\tHyperparameters\n\nPCA\timplementation,\tUsing\tScikit-Learn\n\nPerceptron\tclass,\tThe\tPerceptron\n\nPipeline\tconstructor,\tTransformation\tPipelines-Select\tand\tTrain\ta\tModel,\tNonlinear SVM\tClassification\n\nRandomized\tPCA,\tRandomized\tPCA\n\nRidge\tRegression\twith,\tRidge\tRegression\n\nSAMME,\tAdaBoost\n\nSGDClassifier,\tTraining\ta\tBinary\tClassifier,\tPrecision/Recall\tTradeoff-Precision/Recall Tradeoff,\tMulticlass\tClassification\n\nSGDRegressor,\tStochastic\tGradient\tDescent\n\nsklearn.base.BaseEstimator,\tCustom\tTransformers,\tTransformation\tPipelines, Measuring\tAccuracy\tUsing\tCross-Validation\n\nsklearn.base.clone(),\tMeasuring\tAccuracy\tUsing\tCross-Validation,\tEarly\tStopping\n\nsklearn.base.TransformerMixin,\tCustom\tTransformers,\tTransformation\tPipelines\n\nsklearn.datasets.fetch_california_housing(),\tLinear\tRegression\twith\tTensorFlow\n\nsklearn.datasets.fetch_mldata(),\tMNIST\n\nsklearn.datasets.load_iris(),\tDecision\tBoundaries,\tSoft\tMargin\tClassification,\tTraining and\tVisualizing\ta\tDecision\tTree,\tFeature\tImportance,\tThe\tPerceptron\n\nsklearn.datasets.load_sample_images(),\tTensorFlow\tImplementation-TensorFlow Implementation\n\nsklearn.datasets.make_moons(),\tNonlinear\tSVM\tClassification,\tExercises\n\nsklearn.decomposition.IncrementalPCA,\tIncremental\tPCA\n\nsklearn.decomposition.KernelPCA,\tKernel\tPCA-Selecting\ta\tKernel\tand\tTuning",
      "content_length": 1375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 679,
      "content": "Hyperparameters,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nsklearn.decomposition.PCA,\tUsing\tScikit-Learn\n\nsklearn.ensemble.AdaBoostClassifier,\tAdaBoost\n\nsklearn.ensemble.BaggingClassifier,\tBagging\tand\tPasting\tin\tScikit-Learn-Random Forests\n\nsklearn.ensemble.GradientBoostingRegressor,\tGradient\tBoosting,\tGradient\tBoosting- Gradient\tBoosting\n\nsklearn.ensemble.RandomForestClassifier,\tThe\tROC\tCurve,\tMulticlass\tClassification, Voting\tClassifiers\n\nsklearn.ensemble.RandomForestRegressor,\tBetter\tEvaluation\tUsing\tCross-Validation, Grid\tSearch-Analyze\tthe\tBest\tModels\tand\tTheir\tErrors,\tRandom\tForests-Extra-Trees, Gradient\tBoosting\n\nsklearn.ensemble.VotingClassifier,\tVoting\tClassifiers\n\nsklearn.externals.joblib,\tBetter\tEvaluation\tUsing\tCross-Validation\n\nsklearn.linear_model.ElasticNet,\tElastic\tNet\n\nsklearn.linear_model.Lasso,\tLasso\tRegression\n\nsklearn.linear_model.LinearRegression,\tModel-based\tlearning-Model-based\tlearning, Data\tCleaning,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet,\tThe\tNormal\tEquation, Mini-batch\tGradient\tDescent,\tPolynomial\tRegression,\tLearning\tCurves-Learning Curves\n\nsklearn.linear_model.LogisticRegression,\tDecision\tBoundaries,\tDecision\tBoundaries, Softmax\tRegression,\tVoting\tClassifiers,\tSelecting\ta\tKernel\tand\tTuning Hyperparameters\n\nsklearn.linear_model.Perceptron,\tThe\tPerceptron\n\nsklearn.linear_model.Ridge,\tRidge\tRegression\n\nsklearn.linear_model.SGDClassifier,\tTraining\ta\tBinary\tClassifier\n\nsklearn.linear_model.SGDRegressor,\tStochastic\tGradient\tDescent-Mini-batch\tGradient",
      "content_length": 1509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 680,
      "content": "Descent,\tRidge\tRegression,\tLasso\tRegression-Early\tStopping\n\nsklearn.manifold.LocallyLinearEmbedding,\tLLE-LLE\n\nsklearn.metrics.accuracy_score(),\tVoting\tClassifiers,\tOut-of-Bag\tEvaluation,\tTraining an\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nsklearn.metrics.confusion_matrix(),\tConfusion\tMatrix,\tError\tAnalysis\n\nsklearn.metrics.f1_score(),\tPrecision\tand\tRecall,\tMultilabel\tClassification\n\nsklearn.metrics.mean_squared_error(),\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet- Training\tand\tEvaluating\ton\tthe\tTraining\tSet,\tEvaluate\tYour\tSystem\ton\tthe\tTest\tSet, Learning\tCurves,\tEarly\tStopping,\tGradient\tBoosting-Gradient\tBoosting,\tSelecting\ta Kernel\tand\tTuning\tHyperparameters\n\nsklearn.metrics.precision_recall_curve(),\tPrecision/Recall\tTradeoff\n\nsklearn.metrics.precision_score(),\tPrecision\tand\tRecall,\tPrecision/Recall\tTradeoff\n\nsklearn.metrics.recall_score(),\tPrecision\tand\tRecall,\tPrecision/Recall\tTradeoff\n\nsklearn.metrics.roc_auc_score(),\tThe\tROC\tCurve-The\tROC\tCurve\n\nsklearn.metrics.roc_curve(),\tThe\tROC\tCurve-The\tROC\tCurve\n\nsklearn.model_selection.cross_val_predict(),\tConfusion\tMatrix,\tPrecision/Recall Tradeoff,\tThe\tROC\tCurve,\tError\tAnalysis,\tMultilabel\tClassification\n\nsklearn.model_selection.cross_val_score(),\tBetter\tEvaluation\tUsing\tCross-Validation- Better\tEvaluation\tUsing\tCross-Validation,\tMeasuring\tAccuracy\tUsing\tCross-Validation- Confusion\tMatrix\n\nsklearn.model_selection.GridSearchCV,\tGrid\tSearch-Randomized\tSearch,\tExercises, Error\tAnalysis,\tExercises,\tSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nsklearn.model_selection.StratifiedKFold,\tMeasuring\tAccuracy\tUsing\tCross-Validation\n\nsklearn.model_selection.StratifiedShuffleSplit,\tCreate\ta\tTest\tSet\n\nsklearn.model_selection.train_test_split(),\tCreate\ta\tTest\tSet,\tTraining\tand\tEvaluating on\tthe\tTraining\tSet,\tLearning\tCurves,\tExercises,\tGradient\tBoosting",
      "content_length": 1815,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 681,
      "content": "sklearn.multiclass.OneVsOneClassifier,\tMulticlass\tClassification\n\nsklearn.neighbors.KNeighborsClassifier,\tMultilabel\tClassification,\tExercises\n\nsklearn.neighbors.KNeighborsRegressor,\tModel-based\tlearning\n\nsklearn.pipeline.FeatureUnion,\tTransformation\tPipelines\n\nsklearn.pipeline.Pipeline,\tTransformation\tPipelines,\tLearning\tCurves,\tSoft\tMargin Classification-Nonlinear\tSVM\tClassification,\tSelecting\ta\tKernel\tand\tTuning Hyperparameters\n\nsklearn.preprocessing.Imputer,\tData\tCleaning,\tTransformation\tPipelines\n\nsklearn.preprocessing.LabelBinarizer,\tHandling\tText\tand\tCategorical\tAttributes, Transformation\tPipelines\n\nsklearn.preprocessing.LabelEncoder,\tHandling\tText\tand\tCategorical\tAttributes\n\nsklearn.preprocessing.OneHotEncoder,\tHandling\tText\tand\tCategorical\tAttributes\n\nsklearn.preprocessing.PolynomialFeatures,\tPolynomial\tRegression-Polynomial Regression,\tLearning\tCurves,\tRidge\tRegression,\tNonlinear\tSVM\tClassification\n\nsklearn.preprocessing.StandardScaler,\tFeature\tScaling-Transformation\tPipelines, Multiclass\tClassification,\tGradient\tDescent,\tRidge\tRegression,\tLinear\tSVM Classification,\tSoft\tMargin\tClassification-Polynomial\tKernel,\tGaussian\tRBF\tKernel, Implementing\tGradient\tDescent,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nsklearn.svm.LinearSVC,\tSoft\tMargin\tClassification-Nonlinear\tSVM\tClassification, Gaussian\tRBF\tKernel-Computational\tComplexity,\tSVM\tRegression,\tExercises\n\nsklearn.svm.LinearSVR,\tSVM\tRegression-SVM\tRegression\n\nsklearn.svm.SVC,\tSoft\tMargin\tClassification,\tPolynomial\tKernel,\tGaussian\tRBF\tKernel- Computational\tComplexity,\tSVM\tRegression,\tExercises,\tVoting\tClassifiers\n\nsklearn.svm.SVR,\tExercises,\tSVM\tRegression\n\nsklearn.tree.DecisionTreeClassifier,\tRegularization\tHyperparameters,\tExercises, Bagging\tand\tPasting\tin\tScikit-Learn-Out-of-Bag\tEvaluation,\tRandom\tForests, AdaBoost",
      "content_length": 1813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 682,
      "content": "sklearn.tree.DecisionTreeRegressor,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet, Decision\tTrees,\tRegression,\tGradient\tBoosting-Gradient\tBoosting\n\nsklearn.tree.export_graphviz(),\tTraining\tand\tVisualizing\ta\tDecision\tTree\n\nStandardScaler,\tGradient\tDescent,\tImplementing\tGradient\tDescent,\tTraining\tan\tMLP with\tTensorFlow’s\tHigh-Level\tAPI\n\nSVM\tclassification\tclasses,\tComputational\tComplexity\n\nTF.Learn,\tUp\tand\tRunning\twith\tTensorFlow\n\nuser\tguide,\tOther\tResources\n\nscore(),\tData\tCleaning\n\nsearch\tspace,\tRandomized\tSearch,\tFine-Tuning\tNeural\tNetwork\tHyperparameters\n\nsecond-order\tpartial\tderivatives\t(Hessians),\tAdam\tOptimization\n\nself-organizing\tmaps\t(SOMs),\tSelf-Organizing\tMaps-Self-Organizing\tMaps\n\nsemantic\thashing,\tExercises\n\nsemisupervised\tlearning,\tSemisupervised\tlearning\n\nsensitivity,\tConfusion\tMatrix,\tThe\tROC\tCurve\n\nsentiment\tanalysis,\tRecurrent\tNeural\tNetworks\n\nseparable_conv2d(),\tResNet\n\nsequences,\tRecurrent\tNeural\tNetworks\n\nsequence_length,\tHandling\tVariable\tLength\tInput\tSequences-Handling\tVariable-Length\tOutput Sequences,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nShannon's\tinformation\ttheory,\tGini\tImpurity\tor\tEntropy?\n\nshortcut\tconnections,\tResNet\n\nshow(),\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\nshow_graph(),\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard",
      "content_length": 1297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 683,
      "content": "shrinkage,\tGradient\tBoosting\n\nshuffle_batch(),\tOther\tconvenience\tfunctions\n\nshuffle_batch_join(),\tOther\tconvenience\tfunctions\n\nsigmoid\tfunction,\tEstimating\tProbabilities\n\nsigmoid_cross_entropy_with_logits(),\tTensorFlow\tImplementation\n\nsimilarity\tfunction,\tAdding\tSimilarity\tFeatures-Adding\tSimilarity\tFeatures\n\nsimulated\tannealing,\tStochastic\tGradient\tDescent\n\nsimulated\tenvironments,\tIntroduction\tto\tOpenAI\tGym\n\n(see\talso\tOpenAI\tGym)\n\nSingular\tValue\tDecomposition\t(SVD),\tPrincipal\tComponents\n\nskewed\tdatasets,\tMeasuring\tAccuracy\tUsing\tCross-Validation\n\nskip\tconnections,\tData\tAugmentation,\tResNet\n\nslack\tvariable,\tTraining\tObjective\n\nsmoothing\tterms,\tBatch\tNormalization,\tAdaGrad,\tAdam\tOptimization,\tVariational Autoencoders\n\nsoft\tmargin\tclassification,\tSoft\tMargin\tClassification-Soft\tMargin\tClassification\n\nsoft\tplacements,\tSoft\tplacement\n\nsoft\tvoting,\tVoting\tClassifiers\n\nsoftmax\tfunction,\tSoftmax\tRegression,\tMulti-Layer\tPerceptron\tand\tBackpropagation,\tTraining an\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nSoftmax\tRegression,\tSoftmax\tRegression-Softmax\tRegression\n\nsource\tops,\tLinear\tRegression\twith\tTensorFlow,\tParallel\tExecution\n\nspam\tfilters,\tThe\tMachine\tLearning\tLandscape-Why\tUse\tMachine\tLearning?,\tSupervised learning",
      "content_length": 1225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 684,
      "content": "sparse\tautoencoders,\tSparse\tAutoencoders-TensorFlow\tImplementation\n\nsparse\tmatrix,\tHandling\tText\tand\tCategorical\tAttributes\n\nsparse\tmodels,\tLasso\tRegression,\tAdam\tOptimization\n\nsparse_softmax_cross_entropy_with_logits(),\tConstruction\tPhase\n\nsparsity\tloss,\tSparse\tAutoencoders\n\nspecificity,\tThe\tROC\tCurve\n\nspeech\trecognition,\tWhy\tUse\tMachine\tLearning?\n\nspurious\tpatterns,\tHopfield\tNetworks\n\nstack(),\tStatic\tUnrolling\tThrough\tTime\n\nstacked\tautoencoders,\tStacked\tAutoencoders-Unsupervised\tPretraining\tUsing\tStacked Autoencoders\n\nTensorFlow\timplementation,\tTensorFlow\tImplementation\n\ntraining\tone-at-a-time,\tTraining\tOne\tAutoencoder\tat\ta\tTime-Training\tOne\tAutoencoder at\ta\tTime\n\ntying\tweights,\tTying\tWeights-Tying\tWeights\n\nunsupervised\tpretraining\twith,\tUnsupervised\tPretraining\tUsing\tStacked\tAutoencoders- Unsupervised\tPretraining\tUsing\tStacked\tAutoencoders\n\nvisualizing\tthe\treconstructions,\tVisualizing\tthe\tReconstructions-Visualizing\tthe Reconstructions\n\nstacked\tdenoising\tautoencoders,\tVisualizing\tFeatures,\tDenoising\tAutoencoders\n\nstacked\tdenoising\tencoders,\tDenoising\tAutoencoders\n\nstacked\tgeneralization\t(see\tstacking)\n\nstacking,\tStacking-Stacking\n\nstale\tgradients,\tAsynchronous\tupdates\n\nstandard\tcorrelation\tcoefficient,\tLooking\tfor\tCorrelations",
      "content_length": 1249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 685,
      "content": "standardization,\tFeature\tScaling\n\nStandardScaler,\tTransformation\tPipelines,\tImplementing\tGradient\tDescent,\tTraining\tan\tMLP with\tTensorFlow’s\tHigh-Level\tAPI\n\nstate-action\tvalues,\tMarkov\tDecision\tProcesses\n\nstates\ttensor,\tHandling\tVariable\tLength\tInput\tSequences\n\nstate_is_tuple,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs,\tLSTM\tCell\n\nstatic\tunrolling\tthrough\ttime,\tStatic\tUnrolling\tThrough\tTime-Static\tUnrolling\tThrough\tTime\n\nstatic_rnn(),\tStatic\tUnrolling\tThrough\tTime-Static\tUnrolling\tThrough\tTime,\tAn\tEncoder– Decoder\tNetwork\tfor\tMachine\tTranslation\n\nstationary\tpoint,\tSVM\tDual\tProblem-SVM\tDual\tProblem\n\nstatistical\tmode,\tBagging\tand\tPasting\n\nstatistical\tsignificance,\tRegularization\tHyperparameters\n\nstemming,\tExercises\n\nstep\tfunctions,\tThe\tPerceptron\n\nstep(),\tIntroduction\tto\tOpenAI\tGym\n\nStochastic\tGradient\tBoosting,\tGradient\tBoosting\n\nStochastic\tGradient\tDescent\t(SGD),\tStochastic\tGradient\tDescent-Stochastic\tGradient Descent,\tSoft\tMargin\tClassification,\tThe\tPerceptron\n\ntraining,\tTraining\tand\tCost\tFunction\n\nStochastic\tGradient\tDescent\t(SGD)\tclassifier,\tTraining\ta\tBinary\tClassifier,\tRidge\tRegression\n\nstochastic\tneurons,\tBoltzmann\tMachines\n\nstochastic\tpolicy,\tPolicy\tSearch\n\nstratified\tsampling,\tCreate\ta\tTest\tSet-Create\ta\tTest\tSet,\tMeasuring\tAccuracy\tUsing\tCross- Validation",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 686,
      "content": "stride,\tConvolutional\tLayer\n\nstring\tkernels,\tGaussian\tRBF\tKernel\n\nstring_input_producer(),\tOther\tconvenience\tfunctions\n\nstrong\tlearners,\tVoting\tClassifiers\n\nsubderivatives,\tOnline\tSVMs\n\nsubgradient\tvector,\tLasso\tRegression\n\nsubsample,\tGradient\tBoosting,\tPooling\tLayer\n\nsupervised\tlearning,\tSupervised/Unsupervised\tLearning-Supervised\tlearning\n\nSupport\tVector\tMachines\t(SVMs),\tMulticlass\tClassification,\tSupport\tVector\tMachines- Exercises\n\ndecision\tfunction\tand\tpredictions,\tDecision\tFunction\tand\tPredictions-Decision\tFunction and\tPredictions\n\ndual\tproblem,\tSVM\tDual\tProblem-SVM\tDual\tProblem\n\nkernelized\tSVM,\tKernelized\tSVM-Kernelized\tSVM\n\nlinear\tclassification,\tLinear\tSVM\tClassification-Soft\tMargin\tClassification\n\nmechanics\tof,\tUnder\tthe\tHood-Online\tSVMs\n\nnonlinear\tclassification,\tNonlinear\tSVM\tClassification-Computational\tComplexity\n\nonline\tSVMs,\tOnline\tSVMs-Online\tSVMs\n\nQuadratic\tProgramming\t(QP)\tproblems,\tQuadratic\tProgramming-Quadratic Programming\n\nSVM\tregression,\tSVM\tRegression-Online\tSVMs\n\nthe\tdual\tproblem,\tThe\tDual\tProblem\n\ntraining\tobjective,\tTraining\tObjective-Training\tObjective\n\nsupport\tvectors,\tLinear\tSVM\tClassification",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 687,
      "content": "svd(),\tPrincipal\tComponents\n\nsymbolic\tdifferentiation,\tUsing\tautodiff,\tSymbolic\tDifferentiation-Numerical\tDifferentiation\n\nsynchronous\tupdates,\tSynchronous\tupdates",
      "content_length": 163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 688,
      "content": "T\n\nt-Distributed\tStochastic\tNeighbor\tEmbedding\t(t-SNE),\tOther\tDimensionality\tReduction Techniques\n\ntail\theavy,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\ntarget\tattributes,\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\ntarget_weights,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\ntasks,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\nTemporal\tDifference\t(TD)\tLearning,\tTemporal\tDifference\tLearning\tand\tQ-Learning-Temporal Difference\tLearning\tand\tQ-Learning\n\ntensor\tprocessing\tunits\t(TPUs),\tInstallation\n\nTensorBoard,\tUp\tand\tRunning\twith\tTensorFlow\n\nTensorFlow,\tUp\tand\tRunning\twith\tTensorFlow-Exercises\n\nabout,\tObjective\tand\tApproach\n\nautodiff,\tUsing\tautodiff-Using\tautodiff,\tAutodiff-Reverse-Mode\tAutodiff\n\nBatch\tNormalization\twith,\tImplementing\tBatch\tNormalization\twith\tTensorFlow- Implementing\tBatch\tNormalization\twith\tTensorFlow\n\nconstruction\tphase,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\ncontrol\tdependencies,\tControl\tDependencies\n\nconvenience\tfunctions,\tOther\tconvenience\tfunctions\n\nconvolutional\tlayers,\tResNet\n\nconvolutional\tneural\tnetworks\tand,\tTensorFlow\tImplementation-TensorFlow Implementation\n\ndata\tparallelism\tand,\tTensorFlow\timplementation\n\ndenoising\tautoencoders,\tTensorFlow\tImplementation-TensorFlow\tImplementation\n\ndropout\twith,\tDropout",
      "content_length": 1264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 689,
      "content": "dynamic\tplacer,\tPlacing\tOperations\ton\tDevices\n\nexecution\tphase,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\nfeeding\tdata\tto\tthe\ttraining\talgorithm,\tFeeding\tData\tto\tthe\tTraining\tAlgorithm-Feeding Data\tto\tthe\tTraining\tAlgorithm\n\nGradient\tDescent\twith,\tImplementing\tGradient\tDescent-Using\tan\tOptimizer\n\ngraphs,\tmanaging,\tManaging\tGraphs\n\ninitial\tgraph\tcreation\tand\tsession\trun,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta Session-Creating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\ninstallation,\tInstallation\n\nl1\tand\tl2\tregularization\twith,\tℓ1\tand\tℓ2\tRegularization\n\nlearning\tschedules\tin,\tLearning\tRate\tScheduling\n\nLinear\tRegression\twith,\tLinear\tRegression\twith\tTensorFlow-Linear\tRegression\twith TensorFlow\n\nmax\tpooling\tlayer\tin,\tPooling\tLayer\n\nmax-norm\tregularization\twith,\tMax-Norm\tRegularization\n\nmodel\tzoo,\tModel\tZoos\n\nmodularity,\tModularity-Modularity\n\nMomentum\toptimization\tin,\tMomentum\tOptimization\n\nname\tscopes,\tName\tScopes\n\nneural\tnetwork\tpolicies,\tNeural\tNetwork\tPolicies\n\nNLP\ttutorials,\tNatural\tLanguage\tProcessing,\tAn\tEncoder–Decoder\tNetwork\tfor Machine\tTranslation\n\nnode\tvalue\tlifecycle,\tLifecycle\tof\ta\tNode\tValue\n\noperations\t(ops),\tLinear\tRegression\twith\tTensorFlow",
      "content_length": 1193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 690,
      "content": "optimizer,\tUsing\tan\tOptimizer\n\noverview,\tUp\tand\tRunning\twith\tTensorFlow-Up\tand\tRunning\twith\tTensorFlow\n\nparallel\tdistributed\tcomputing\t(see\tparallel\tdistributed\tcomputing\twith\tTensorFlow)\n\nPython\tAPI\n\nconstruction,\tConstruction\tPhase-Construction\tPhase\n\nexecution,\tExecution\tPhase\n\nusing\tthe\tneural\tnetwork,\tUsing\tthe\tNeural\tNetwork\n\nqueues\t(see\tqueues)\n\nreusing\tpretrained\tlayers,\tReusing\ta\tTensorFlow\tModel-Reusing\ta\tTensorFlow\tModel\n\nRNNs\tin,\tBasic\tRNNs\tin\tTensorFlow-Handling\tVariable-Length\tOutput\tSequences\n\n(see\talso\trecurrent\tneural\tnetworks\t(RNNs))\n\nsaving\tand\trestoring\tmodels,\tSaving\tand\tRestoring\tModels-Saving\tand\tRestoring Models\n\nsharing\tvariables,\tSharing\tVariables-Sharing\tVariables\n\nsimple\tplacer,\tPlacing\tOperations\ton\tDevices\n\nsparse\tautoencoders\twith,\tTensorFlow\tImplementation\n\nand\tstacked\tautoencoders,\tTensorFlow\tImplementation\n\nTensorBoard,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard- Visualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\ntf.abs(),\tℓ1\tand\tℓ2\tRegularization\n\ntf.add(),\tModularity,\tℓ1\tand\tℓ2\tRegularization\n\ntf.add_n(),\tModularity-Sharing\tVariables,\tSharing\tVariables-Sharing\tVariables\n\ntf.add_to_collection(),\tMax-Norm\tRegularization\n\ntf.assign(),\tManually\tComputing\tthe\tGradients,\tReusing\tModels\tfrom\tOther",
      "content_length": 1272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 691,
      "content": "Frameworks,\tMax-Norm\tRegularization-Max-Norm\tRegularization,\tChapter\t9:\tUp\tand Running\twith\tTensorFlow\n\ntf.bfloat16,\tBandwidth\tsaturation\n\ntf.bool,\tDropout\n\ntf.cast(),\tConstruction\tPhase,\tTraining\ta\tSequence\tClassifier\n\ntf.clip_by_norm(),\tMax-Norm\tRegularization-Max-Norm\tRegularization\n\ntf.clip_by_value(),\tGradient\tClipping\n\ntf.concat(),\tExercises,\tGoogLeNet,\tNeural\tNetwork\tPolicies,\tPolicy\tGradients\n\ntf.ConfigProto,\tManaging\tthe\tGPU\tRAM,\tLogging\tplacements-Soft\tplacement,\tIn- Graph\tVersus\tBetween-Graph\tReplication,\tChapter\t12:\tDistributing\tTensorFlow\tAcross Devices\tand\tServers\n\ntf.constant(),\tLifecycle\tof\ta\tNode\tValue-Manually\tComputing\tthe\tGradients,\tSimple placement-Dynamic\tplacement\tfunction,\tControl\tDependencies,\tOpening\ta\tSession- Pinning\tOperations\tAcross\tTasks\n\ntf.constant_initializer(),\tSharing\tVariables-Sharing\tVariables\n\ntf.container(),\tSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers-Asynchronous Communication\tUsing\tTensorFlow\tQueues,\tTensorFlow\timplementation-Exercises, Chapter\t9:\tUp\tand\tRunning\twith\tTensorFlow\n\ntf.contrib.layers.l1_regularizer(),\tℓ1\tand\tℓ2\tRegularization,\tMax-Norm\tRegularization\n\ntf.contrib.layers.l2_regularizer(),\tℓ1\tand\tℓ2\tRegularization,\tTensorFlow Implementation-Tying\tWeights\n\ntf.contrib.layers.variance_scaling_initializer(),\tXavier\tand\tHe\tInitialization-Xavier\tand He\tInitialization,\tTraining\ta\tSequence\tClassifier,\tTensorFlow\tImplementation-Tying Weights,\tVariational\tAutoencoders,\tNeural\tNetwork\tPolicies,\tPolicy\tGradients, Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.contrib.learn.DNNClassifier,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\ntf.contrib.learn.infer_real_valued_columns_from_input(),\tTraining\tan\tMLP\twith TensorFlow’s\tHigh-Level\tAPI",
      "content_length": 1728,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 692,
      "content": "tf.contrib.rnn.BasicLSTMCell,\tLSTM\tCell,\tPeephole\tConnections\n\ntf.contrib.rnn.BasicRNNCell,\tStatic\tUnrolling\tThrough\tTime-Dynamic\tUnrolling Through\tTime,\tTraining\ta\tSequence\tClassifier,\tTraining\tto\tPredict\tTime\tSeries-Training to\tPredict\tTime\tSeries,\tTraining\tto\tPredict\tTime\tSeries,\tDeep\tRNNs-Applying\tDropout, LSTM\tCell\n\ntf.contrib.rnn.DropoutWrapper,\tApplying\tDropout\n\ntf.contrib.rnn.GRUCell,\tGRU\tCell\n\ntf.contrib.rnn.LSTMCell,\tPeephole\tConnections\n\ntf.contrib.rnn.MultiRNNCell,\tDeep\tRNNs-Applying\tDropout\n\ntf.contrib.rnn.OutputProjectionWrapper,\tTraining\tto\tPredict\tTime\tSeries-Training\tto Predict\tTime\tSeries\n\ntf.contrib.rnn.RNNCell,\tDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs\n\ntf.contrib.rnn.static_rnn(),\tBasic\tRNNs\tin\tTensorFlow-Handling\tVariable\tLength\tInput Sequences,\tAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation-Exercises, Chapter\t14:\tRecurrent\tNeural\tNetworks-Chapter\t14:\tRecurrent\tNeural\tNetworks\n\ntf.contrib.slim\tmodule,\tUp\tand\tRunning\twith\tTensorFlow,\tExercises\n\ntf.contrib.slim.nets\tmodule\t(nets),\tExercises\n\ntf.control_dependencies(),\tControl\tDependencies\n\ntf.decode_csv(),\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded readers\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\ntf.device(),\tSimple\tplacement-Soft\tplacement,\tPinning\tOperations\tAcross\tTasks- Sharding\tVariables\tAcross\tMultiple\tParameter\tServers,\tDistributing\ta\tDeep\tRNN Across\tMultiple\tGPUs-Distributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs\n\ntf.exp(),\tVariational\tAutoencoders-Generating\tDigits\n\ntf.FIFOQueue,\tAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues,\tQueues\tof tuples-RandomShuffleQueue,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph, Multithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner",
      "content_length": 1703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 693,
      "content": "tf.float32,\tLinear\tRegression\twith\tTensorFlow,\tChapter\t9:\tUp\tand\tRunning\twith TensorFlow\n\ntf.get_collection(),\tReusing\ta\tTensorFlow\tModel-Freezing\tthe\tLower\tLayers,\tℓ1\tand\tℓ2 Regularization,\tMax-Norm\tRegularization,\tTensorFlow\tImplementation,\tLearning\tto Play\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.get_default_graph(),\tManaging\tGraphs,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves Using\tTensorBoard\n\ntf.get_default_session(),\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\ntf.get_variable(),\tSharing\tVariables-Sharing\tVariables,\tReusing\tModels\tfrom\tOther Frameworks,\tℓ1\tand\tℓ2\tRegularization\n\ntf.global_variables(),\tReusing\ta\tTensorFlow\tModel\n\ntf.global_variables_initializer(),\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession, Manually\tComputing\tthe\tGradients\n\ntf.gradients(),\tUsing\tautodiff\n\ntf.Graph,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession,\tManaging\tGraphs, Visualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard,\tLoading\tData\tDirectly from\tthe\tGraph,\tIn-Graph\tVersus\tBetween-Graph\tReplication\n\ntf.GraphKeys.GLOBAL_VARIABLES,\tReusing\ta\tTensorFlow\tModel-Freezing\tthe Lower\tLayers\n\ntf.GraphKeys.REGULARIZATION_LOSSES,\tℓ1\tand\tℓ2\tRegularization,\tTensorFlow Implementation\n\ntf.group(),\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.int32,\tOperations\tand\tkernels-Queues\tof\ttuples,\tReading\tthe\ttraining\tdata\tdirectly from\tthe\tgraph,\tHandling\tVariable\tLength\tInput\tSequences,\tTraining\ta\tSequence Classifier,\tWord\tEmbeddings,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.int64,\tConstruction\tPhase\n\ntf.InteractiveSession,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\ntf.layers.batch_normalization(),\tImplementing\tBatch\tNormalization\twith\tTensorFlow-",
      "content_length": 1702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 694,
      "content": "Implementing\tBatch\tNormalization\twith\tTensorFlow\n\ntf.layers.dense(),\tConstruction\tPhase\n\nTF.Learn,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\ntf.log(),\tTensorFlow\tImplementation,\tVariational\tAutoencoders,\tNeural\tNetwork Policies,\tPolicy\tGradients\n\ntf.matmul(),\tLinear\tRegression\twith\tTensorFlow-Manually\tComputing\tthe\tGradients, Modularity,\tConstruction\tPhase,\tBasic\tRNNs\tin\tTensorFlow,\tTying\tWeights,\tTraining One\tAutoencoder\tat\ta\tTime,\tTensorFlow\tImplementation,\tTensorFlow\tImplementation- TensorFlow\tImplementation\n\ntf.matrix_inverse(),\tLinear\tRegression\twith\tTensorFlow\n\ntf.maximum(),\tModularity,\tSharing\tVariables-Sharing\tVariables,\tNonsaturating Activation\tFunctions\n\ntf.multinomial(),\tNeural\tNetwork\tPolicies,\tPolicy\tGradients\n\ntf.name_scope(),\tName\tScopes,\tModularity-Sharing\tVariables,\tConstruction\tPhase, Construction\tPhase-Construction\tPhase,\tTraining\tOne\tAutoencoder\tat\ta\tTime-Training One\tAutoencoder\tat\ta\tTime\n\ntf.nn.conv2d(),\tTensorFlow\tImplementation-TensorFlow\tImplementation\n\ntf.nn.dynamic_rnn(),\tStatic\tUnrolling\tThrough\tTime-Dynamic\tUnrolling\tThrough\tTime, Training\ta\tSequence\tClassifier,\tTraining\tto\tPredict\tTime\tSeries,\tTraining\tto\tPredict Time\tSeries,\tDeep\tRNNs-Applying\tDropout,\tAn\tEncoder–Decoder\tNetwork\tfor Machine\tTranslation-Exercises,\tChapter\t14:\tRecurrent\tNeural\tNetworks-Chapter\t14: Recurrent\tNeural\tNetworks\n\ntf.nn.elu(),\tNonsaturating\tActivation\tFunctions,\tTensorFlow\tImplementation-Tying Weights,\tVariational\tAutoencoders,\tNeural\tNetwork\tPolicies,\tPolicy\tGradients\n\ntf.nn.embedding_lookup(),\tWord\tEmbeddings\n\ntf.nn.in_top_k(),\tConstruction\tPhase,\tTraining\ta\tSequence\tClassifier\n\ntf.nn.max_pool(),\tPooling\tLayer-Pooling\tLayer\n\ntf.nn.relu(),\tConstruction\tPhase,\tTraining\tto\tPredict\tTime\tSeries-Training\tto\tPredict",
      "content_length": 1753,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 695,
      "content": "Time\tSeries,\tTraining\tto\tPredict\tTime\tSeries,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing Deep\tQ-Learning\n\ntf.nn.sigmoid_cross_entropy_with_logits(),\tTensorFlow\tImplementation,\tGenerating Digits,\tPolicy\tGradients-Policy\tGradients\n\ntf.nn.sparse_softmax_cross_entropy_with_logits(),\tConstruction\tPhase-Construction Phase,\tTraining\ta\tSequence\tClassifier\n\ntf.one_hot(),\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.PaddingFIFOQueue,\tPaddingFifoQueue\n\ntf.placeholder(),\tFeeding\tData\tto\tthe\tTraining\tAlgorithm-Feeding\tData\tto\tthe\tTraining Algorithm,\tChapter\t9:\tUp\tand\tRunning\twith\tTensorFlow\n\ntf.placeholder_with_default(),\tTensorFlow\tImplementation\n\ntf.RandomShuffleQueue,\tRandomShuffleQueue,\tReading\tthe\ttraining\tdata\tdirectly\tfrom the\tgraph-Reading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded\treaders using\ta\tCoordinator\tand\ta\tQueueRunner-Other\tconvenience\tfunctions\n\ntf.random_normal(),\tModularity,\tBasic\tRNNs\tin\tTensorFlow,\tTensorFlow Implementation,\tVariational\tAutoencoders\n\ntf.random_uniform(),\tManually\tComputing\tthe\tGradients,\tSaving\tand\tRestoring\tModels, Word\tEmbeddings,\tChapter\t9:\tUp\tand\tRunning\twith\tTensorFlow\n\ntf.reduce_mean(),\tManually\tComputing\tthe\tGradients,\tName\tScopes,\tConstruction Phase-Construction\tPhase,\tℓ1\tand\tℓ2\tRegularization,\tTraining\ta\tSequence\tClassifier- Training\ta\tSequence\tClassifier,\tPerforming\tPCA\twith\tan\tUndercomplete\tLinear Autoencoder,\tTensorFlow\tImplementation,\tTraining\tOne\tAutoencoder\tat\ta\tTime, Training\tOne\tAutoencoder\tat\ta\tTime,\tTensorFlow\tImplementation,\tTensorFlow Implementation,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.reduce_sum(),\tℓ1\tand\tℓ2\tRegularization,\tTensorFlow\tImplementation-TensorFlow Implementation,\tVariational\tAutoencoders-Generating\tDigits,\tLearning\tto\tPlay\tMs. Pac-Man\tUsing\tDeep\tQ-Learning-Learning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ- Learning\n\ntf.reset_default_graph(),\tManaging\tGraphs",
      "content_length": 1875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 696,
      "content": "tf.reshape(),\tTraining\tto\tPredict\tTime\tSeries,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing Deep\tQ-Learning\n\ntf.RunOptions,\tIn-Graph\tVersus\tBetween-Graph\tReplication\n\ntf.Session,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession,\tChapter\t9:\tUp\tand Running\twith\tTensorFlow\n\ntf.shape(),\tTensorFlow\tImplementation,\tVariational\tAutoencoders\n\ntf.square(),\tManually\tComputing\tthe\tGradients,\tName\tScopes,\tTraining\tto\tPredict\tTime Series,\tPerforming\tPCA\twith\tan\tUndercomplete\tLinear\tAutoencoder,\tTensorFlow Implementation,\tTraining\tOne\tAutoencoder\tat\ta\tTime,\tTraining\tOne\tAutoencoder\tat\ta Time,\tTensorFlow\tImplementation,\tTensorFlow\tImplementation,\tVariational Autoencoders-Generating\tDigits,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ- Learning\n\ntf.stack(),\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded\treaders using\ta\tCoordinator\tand\ta\tQueueRunner,\tStatic\tUnrolling\tThrough\tTime\n\ntf.string,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded\treaders\tusing a\tCoordinator\tand\ta\tQueueRunner\n\ntf.summary.FileWriter,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard- Visualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\ntf.summary.scalar(),\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\ntf.tanh(),\tBasic\tRNNs\tin\tTensorFlow\n\ntf.TextLineReader,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph,\tMultithreaded readers\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\ntf.to_float(),\tPolicy\tGradients-Policy\tGradients\n\ntf.train.AdamOptimizer,\tAdam\tOptimization,\tAdam\tOptimization,\tTraining\ta\tSequence Classifier,\tTraining\tto\tPredict\tTime\tSeries,\tPerforming\tPCA\twith\tan\tUndercomplete Linear\tAutoencoder,\tTensorFlow\tImplementation-Tying\tWeights,\tTraining\tOne Autoencoder\tat\ta\tTime,\tTensorFlow\tImplementation,\tGenerating\tDigits,\tPolicy Gradients-Policy\tGradients,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.train.ClusterSpec,\tMultiple\tDevices\tAcross\tMultiple\tServers",
      "content_length": 1911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 697,
      "content": "tf.train.Coordinator,\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner- Multithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner\n\ntf.train.exponential_decay(),\tLearning\tRate\tScheduling\n\ntf.train.GradientDescentOptimizer,\tUsing\tan\tOptimizer,\tConstruction\tPhase,\tGradient Clipping,\tMomentum\tOptimization,\tAdam\tOptimization\n\ntf.train.MomentumOptimizer,\tUsing\tan\tOptimizer,\tMomentum\tOptimization-Nesterov Accelerated\tGradient,\tLearning\tRate\tScheduling,\tExercises,\tTensorFlow implementation,\tChapter\t10:\tIntroduction\tto\tArtificial\tNeural\tNetworks-Chapter\t11: Training\tDeep\tNeural\tNets\n\ntf.train.QueueRunner,\tMultithreaded\treaders\tusing\ta\tCoordinator\tand\ta\tQueueRunner- Other\tconvenience\tfunctions\n\ntf.train.replica_device_setter(),\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers- Sharing\tState\tAcross\tSessions\tUsing\tResource\tContainers\n\ntf.train.RMSPropOptimizer,\tRMSProp\n\ntf.train.Saver,\tSaving\tand\tRestoring\tModels-Saving\tand\tRestoring\tModels,\tConstruction Phase,\tExercises,\tApplying\tDropout,\tPolicy\tGradients,\tLearning\tto\tPlay\tMs.\tPac-Man Using\tDeep\tQ-Learning\n\ntf.train.Server,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\ntf.train.start_queue_runners(),\tOther\tconvenience\tfunctions\n\ntf.transpose(),\tLinear\tRegression\twith\tTensorFlow-Manually\tComputing\tthe\tGradients, Static\tUnrolling\tThrough\tTime,\tTying\tWeights\n\ntf.truncated_normal(),\tConstruction\tPhase\n\ntf.unstack(),\tStatic\tUnrolling\tThrough\tTime-Dynamic\tUnrolling\tThrough\tTime,\tTraining to\tPredict\tTime\tSeries,\tChapter\t14:\tRecurrent\tNeural\tNetworks\n\ntf.Variable,\tCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession,\tChapter\t9:\tUp\tand Running\twith\tTensorFlow\n\ntf.variable_scope(),\tSharing\tVariables-Sharing\tVariables,\tReusing\tModels\tfrom\tOther Frameworks,\tSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers,\tTraining\ta",
      "content_length": 1795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 698,
      "content": "Sequence\tClassifier,\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.zeros(),\tConstruction\tPhase,\tBasic\tRNNs\tin\tTensorFlow,\tTying\tWeights\n\ntruncated\tbackpropagation\tthrough\ttime,\tThe\tDifficulty\tof\tTraining\tover\tMany\tTime Steps\n\nvisualizing\tgraph\tand\ttraining\tcurves,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing TensorBoard-Visualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\nTensorFlow\tServing,\tOne\tNeural\tNetwork\tper\tDevice\n\ntensorflow.contrib,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\ntest\tset,\tTesting\tand\tValidating,\tCreate\ta\tTest\tSet-Create\ta\tTest\tSet,\tMNIST\n\ntesting\tand\tvalidating,\tTesting\tand\tValidating-Testing\tand\tValidating\n\ntext\tattributes,\tHandling\tText\tand\tCategorical\tAttributes-Handling\tText\tand\tCategorical Attributes\n\nTextLineReader,\tReading\tthe\ttraining\tdata\tdirectly\tfrom\tthe\tgraph\n\nTF-slim,\tUp\tand\tRunning\twith\tTensorFlow\n\ntf.layers.conv1d(),\tResNet\n\ntf.layers.conv2d(),\tLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\ntf.layers.conv2d_transpose(),\tResNet\n\ntf.layers.conv3d(),\tResNet\n\ntf.layers.dense(),\tXavier\tand\tHe\tInitialization,\tImplementing\tBatch\tNormalization\twith TensorFlow\n\ntf.layers.separable_conv2d(),\tResNet\n\nTF.Learn,\tUp\tand\tRunning\twith\tTensorFlow,\tTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level API\n\ntf.nn.atrous_conv2d(),\tResNet\n\ntf.nn.depthwise_conv2d(),\tResNet",
      "content_length": 1331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 699,
      "content": "thermal\tequilibrium,\tBoltzmann\tMachines\n\nthread\tpools\t(inter-op/intra-op,\tin\tTensorFlow,\tParallel\tExecution\n\nthreshold\tvariable,\tSharing\tVariables-Sharing\tVariables\n\nTikhonov\tregularization,\tRidge\tRegression\n\ntime\tseries\tdata,\tRecurrent\tNeural\tNetworks\n\ntoarray(),\tHandling\tText\tand\tCategorical\tAttributes\n\ntolerance\thyperparameter,\tComputational\tComplexity\n\ntraining,\tImplementing\tBatch\tNormalization\twith\tTensorFlow-Implementing\tBatch Normalization\twith\tTensorFlow,\tApplying\tDropout\n\ntraining\tdata,\tWhat\tIs\tMachine\tLearning?\n\ninsufficient\tquantities,\tInsufficient\tQuantity\tof\tTraining\tData\n\nirrelevant\tfeatures,\tIrrelevant\tFeatures\n\nloading,\tLoading\tData\tDirectly\tfrom\tthe\tGraph-Other\tconvenience\tfunctions\n\nnonrepresentative,\tNonrepresentative\tTraining\tData\n\noverfitting,\tOverfitting\tthe\tTraining\tData-Overfitting\tthe\tTraining\tData\n\npoor\tquality,\tPoor-Quality\tData\n\nunderfitting,\tUnderfitting\tthe\tTraining\tData\n\ntraining\tinstance,\tWhat\tIs\tMachine\tLearning?\n\ntraining\tmodels,\tModel-based\tlearning,\tTraining\tModels-Exercises\n\nlearning\tcurves\tin,\tLearning\tCurves-Learning\tCurves\n\nLinear\tRegression,\tTraining\tModels,\tLinear\tRegression-Mini-batch\tGradient\tDescent\n\nLogistic\tRegression,\tLogistic\tRegression-Softmax\tRegression\n\noverview,\tTraining\tModels-Training\tModels\n\nPolynomial\tRegression,\tTraining\tModels,\tPolynomial\tRegression-Polynomial\tRegression",
      "content_length": 1350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 700,
      "content": "U\n\ntraining\tobjectives,\tTraining\tObjective-Training\tObjective\n\ntraining\tset,\tWhat\tIs\tMachine\tLearning?,\tTesting\tand\tValidating,\tDiscover\tand\tVisualize\tthe Data\tto\tGain\tInsights,\tPrepare\tthe\tData\tfor\tMachine\tLearning\tAlgorithms,\tTraining\tand Evaluating\ton\tthe\tTraining\tSet-Training\tand\tEvaluating\ton\tthe\tTraining\tSet\n\ncost\tfunction\tof,\tTraining\tand\tCost\tFunction-Training\tand\tCost\tFunction\n\nshuffling,\tMNIST\n\ntransfer\tlearning,\tReusing\tPretrained\tLayers-Pretraining\ton\tan\tAuxiliary\tTask\n\n(see\talso\tpretrained\tlayers\treuse)\n\ntransform(),\tData\tCleaning,\tTransformation\tPipelines\n\ntransformation\tpipelines,\tTransformation\tPipelines-Select\tand\tTrain\ta\tModel\n\ntransformers,\tData\tCleaning\n\ntransformers,\tcustom,\tCustom\tTransformers-Custom\tTransformers\n\ntranspose(),\tStatic\tUnrolling\tThrough\tTime\n\ntrue\tnegative\trate\t(TNR),\tThe\tROC\tCurve\n\ntrue\tpositive\trate\t(TPR),\tConfusion\tMatrix,\tThe\tROC\tCurve\n\ntruncated\tbackpropagation\tthrough\ttime,\tThe\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps\n\ntuples,\tQueues\tof\ttuples\n\ntying\tweights,\tTying\tWeights\n\nunderfitting,\tUnderfitting\tthe\tTraining\tData,\tTraining\tand\tEvaluating\ton\tthe\tTraining\tSet, Gaussian\tRBF\tKernel\n\nunivariate\tregression,\tFrame\tthe\tProblem\n\nunstack(),\tStatic\tUnrolling\tThrough\tTime\n\nunsupervised\tlearning,\tUnsupervised\tlearning-Unsupervised\tlearning\n\nanomaly\tdetection,\tUnsupervised\tlearning",
      "content_length": 1339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 701,
      "content": "V\n\nassociation\trule\tlearning,\tUnsupervised\tlearning,\tUnsupervised\tlearning\n\nclustering,\tUnsupervised\tlearning\n\ndimensionality\treduction\talgorithm,\tUnsupervised\tlearning\n\nvisualization\talgorithms,\tUnsupervised\tlearning\n\nunsupervised\tpretraining,\tUnsupervised\tPretraining-Unsupervised\tPretraining,\tUnsupervised Pretraining\tUsing\tStacked\tAutoencoders-Unsupervised\tPretraining\tUsing\tStacked Autoencoders\n\nupsampling,\tResNet\n\nutility\tfunction,\tModel-based\tlearning\n\nvalidation\tset,\tTesting\tand\tValidating\n\nValue\tIteration,\tMarkov\tDecision\tProcesses\n\nvalue_counts(),\tTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\nvanishing\tgradients,\tVanishing/Exploding\tGradients\tProblems\n\n(see\talso\tgradients,\tvanishing\tand\texploding)\n\nvariables,\tsharing,\tSharing\tVariables-Sharing\tVariables\n\nvariable_scope(),\tSharing\tVariables-Sharing\tVariables\n\nvariance\n\nbias/variance\ttradeoff,\tLearning\tCurves\n\nvariance\tpreservation,\tPreserving\tthe\tVariance-Preserving\tthe\tVariance\n\nvariance_scaling_initializer(),\tXavier\tand\tHe\tInitialization\n\nvariational\tautoencoders,\tVariational\tAutoencoders-Generating\tDigits\n\nVGGNet,\tResNet\n\nvisual\tcortex,\tThe\tArchitecture\tof\tthe\tVisual\tCortex",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 702,
      "content": "W\n\nX\n\nY\n\nZ\n\nvisualization,\tVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard-Visualizing\tthe Graph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\nvisualization\talgorithms,\tUnsupervised\tlearning-Unsupervised\tlearning\n\nvoice\trecognition,\tConvolutional\tNeural\tNetworks\n\nvoting\tclassifiers,\tVoting\tClassifiers-Voting\tClassifiers\n\nwarmup\tphase,\tAsynchronous\tupdates\n\nweak\tlearners,\tVoting\tClassifiers\n\nweight-tying,\tTying\tWeights\n\nweights,\tConstruction\tPhase\n\nfreezing,\tFreezing\tthe\tLower\tLayers\n\nwhile_loop(),\tDynamic\tUnrolling\tThrough\tTime\n\nwhite\tbox\tmodels,\tMaking\tPredictions\n\nworker,\tMultiple\tDevices\tAcross\tMultiple\tServers\n\nworker\tservice,\tThe\tMaster\tand\tWorker\tServices\n\nworker_device,\tSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\nworkspace\tdirectory,\tGet\tthe\tData-Download\tthe\tData\n\nXavier\tinitialization,\tVanishing/Exploding\tGradients\tProblems-Xavier\tand\tHe\tInitialization\n\nYouTube,\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\nzero\tpadding,\tConvolutional\tLayer,\tTensorFlow\tImplementation",
      "content_length": 1003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 703,
      "content": "About\tthe\tAuthor\n\nAurélien\tGéron\tis\ta\tMachine\tLearning\tconsultant.\tA\tformer\tGoogler,\the\tled\tthe\tYouTube\tvideo classification\tteam\tfrom\t2013\tto\t2016.\tHe\twas\talso\ta\tfounder\tand\tCTO\tof\tWifirst\tfrom\t2002\tto\t2012,\ta leading\tWireless\tISP\tin\tFrance;\tand\ta\tfounder\tand\tCTO\tof\tPolyconseil\tin\t2001,\tthe\tfirm\tthat\tnow manages\tthe\telectric\tcar\tsharing\tservice\tAutolib’.\n\nBefore\tthis\the\tworked\tas\tan\tengineer\tin\ta\tvariety\tof\tdomains:\tfinance\t(JP\tMorgan\tand\tSociété\tGénérale), defense\t(Canada’s\tDOD),\tand\thealthcare\t(blood\ttransfusion).\tHe\tpublished\ta\tfew\ttechnical\tbooks\t(on C++,\tWiFi,\tand\tinternet\tarchitectures),\tand\twas\ta\tComputer\tScience\tlecturer\tin\ta\tFrench\tengineering school.\n\nA\tfew\tfun\tfacts:\the\ttaught\this\tthree\tchildren\tto\tcount\tin\tbinary\twith\ttheir\tfingers\t(up\tto\t1023),\the\tstudied microbiology\tand\tevolutionary\tgenetics\tbefore\tgoing\tinto\tsoftware\tengineering,\tand\this\tparachute\tdidn’t open\ton\tthe\tsecond\tjump.",
      "content_length": 908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 704,
      "content": "Colophon\n\nThe\tanimal\ton\tthe\tcover\tof\tHands-On\tMachine\tLearning\twith\tScikit-Learn\tand\tTensorFlow\tis\tthe\tfar eastern\tfire\tsalamander\t(Salamandra\tinfraimmaculata),\tan\tamphibian\tfound\tin\tthe\tMiddle\tEast.\tThey have\tblack\tskin\tfeaturing\tlarge\tyellow\tspots\ton\ttheir\tback\tand\thead.\tThese\tspots\tare\ta\twarning\tcoloration meant\tto\tkeep\tpredators\tat\tbay.\tFull-grown\tsalamanders\tcan\tbe\tover\ta\tfoot\tin\tlength.\n\nFar\teastern\tfire\tsalamanders\tlive\tin\tsubtropical\tshrubland\tand\tforests\tnear\trivers\tor\tother\tfreshwater bodies.\tThey\tspend\tmost\tof\ttheir\tlife\ton\tland,\tbut\tlay\ttheir\teggs\tin\tthe\twater.\tThey\tsubsist\tmostly\ton\ta\tdiet of\tinsects,\tworms,\tand\tsmall\tcrustaceans,\tbut\toccasionally\teat\tother\tsalamanders.\tMales\tof\tthe\tspecies have\tbeen\tknown\tto\tlive\tup\tto\t23\tyears,\twhile\tfemales\tcan\tlive\tup\tto\t21\tyears.\n\nAlthough\tnot\tyet\tendangered,\tthe\tfar\teastern\tfire\tsalamander\tpopulation\tis\tin\tdecline.\tPrimary\tthreats include\tdamming\tof\trivers\t(which\tdisrupts\tthe\tsalamander’s\tbreeding)\tand\tpollution.\tThey\tare\talso threatened\tby\tthe\trecent\tintroduction\tof\tpredatory\tfish,\tsuch\tas\tthe\tmosquitofish.\tThese\tfish\twere\tintended to\tcontrol\tthe\tmosquito\tpopulation,\tbut\tthey\talso\tfeed\ton\tyoung\tsalamanders.\n\nMany\tof\tthe\tanimals\ton\tO’Reilly\tcovers\tare\tendangered;\tall\tof\tthem\tare\timportant\tto\tthe\tworld.\tTo\tlearn more\tabout\thow\tyou\tcan\thelp,\tgo\tto\tanimals.oreilly.com.\n\nThe\tcover\timage\tis\tfrom\tWood’s\tIllustrated\tNatural\tHistory.\tThe\tcover\tfonts\tare\tURW\tTypewriter\tand Guardian\tSans.\tThe\ttext\tfont\tis\tAdobe\tMinion\tPro;\tthe\theading\tfont\tis\tAdobe\tMyriad\tCondensed;\tand\tthe code\tfont\tis\tDalton\tMaag’s\tUbuntu\tMono.",
      "content_length": 1581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 705,
      "content": "Preface\n\nThe\tMachine\tLearning\tTsunami\n\nMachine\tLearning\tin\tYour\tProjects\n\nObjective\tand\tApproach\n\nPrerequisites\n\nRoadmap\n\nOther\tResources\n\nConventions\tUsed\tin\tThis\tBook\n\nUsing\tCode\tExamples\n\nO’Reilly\tSafari\n\nHow\tto\tContact\tUs\n\nAcknowledgments\n\nI.\tThe\tFundamentals\tof\tMachine\tLearning\n\n1.\tThe\tMachine\tLearning\tLandscape\n\nWhat\tIs\tMachine\tLearning?\n\nWhy\tUse\tMachine\tLearning?\n\nTypes\tof\tMachine\tLearning\tSystems\n\nSupervised/Unsupervised\tLearning\n\nBatch\tand\tOnline\tLearning\n\nInstance-Based\tVersus\tModel-Based\tLearning\n\nMain\tChallenges\tof\tMachine\tLearning\n\nInsufficient\tQuantity\tof\tTraining\tData\n\nNonrepresentative\tTraining\tData\n\nPoor-Quality\tData\n\nIrrelevant\tFeatures\n\nOverfitting\tthe\tTraining\tData",
      "content_length": 693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 706,
      "content": "Underfitting\tthe\tTraining\tData\n\nStepping\tBack\n\nTesting\tand\tValidating\n\nExercises\n\n2.\tEnd-to-End\tMachine\tLearning\tProject\n\nWorking\twith\tReal\tData\n\nLook\tat\tthe\tBig\tPicture\n\nFrame\tthe\tProblem\n\nSelect\ta\tPerformance\tMeasure\n\nCheck\tthe\tAssumptions\n\nGet\tthe\tData\n\nCreate\tthe\tWorkspace\n\nDownload\tthe\tData\n\nTake\ta\tQuick\tLook\tat\tthe\tData\tStructure\n\nCreate\ta\tTest\tSet\n\nDiscover\tand\tVisualize\tthe\tData\tto\tGain\tInsights\n\nVisualizing\tGeographical\tData\n\nLooking\tfor\tCorrelations\n\nExperimenting\twith\tAttribute\tCombinations\n\nPrepare\tthe\tData\tfor\tMachine\tLearning\tAlgorithms\n\nData\tCleaning\n\nHandling\tText\tand\tCategorical\tAttributes\n\nCustom\tTransformers\n\nFeature\tScaling\n\nTransformation\tPipelines\n\nSelect\tand\tTrain\ta\tModel\n\nTraining\tand\tEvaluating\ton\tthe\tTraining\tSet",
      "content_length": 748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 707,
      "content": "Better\tEvaluation\tUsing\tCross-Validation\n\nFine-Tune\tYour\tModel\n\nGrid\tSearch\n\nRandomized\tSearch\n\nEnsemble\tMethods\n\nAnalyze\tthe\tBest\tModels\tand\tTheir\tErrors\n\nEvaluate\tYour\tSystem\ton\tthe\tTest\tSet\n\nLaunch,\tMonitor,\tand\tMaintain\tYour\tSystem\n\nTry\tIt\tOut!\n\nExercises\n\n3.\tClassification\n\nMNIST\n\nTraining\ta\tBinary\tClassifier\n\nPerformance\tMeasures\n\nMeasuring\tAccuracy\tUsing\tCross-Validation\n\nConfusion\tMatrix\n\nPrecision\tand\tRecall\n\nPrecision/Recall\tTradeoff\n\nThe\tROC\tCurve\n\nMulticlass\tClassification\n\nError\tAnalysis\n\nMultilabel\tClassification\n\nMultioutput\tClassification\n\nExercises\n\n4.\tTraining\tModels\n\nLinear\tRegression\n\nThe\tNormal\tEquation",
      "content_length": 631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 708,
      "content": "Computational\tComplexity\n\nGradient\tDescent\n\nBatch\tGradient\tDescent\n\nStochastic\tGradient\tDescent\n\nMini-batch\tGradient\tDescent\n\nPolynomial\tRegression\n\nLearning\tCurves\n\nRegularized\tLinear\tModels\n\nRidge\tRegression\n\nLasso\tRegression\n\nElastic\tNet\n\nEarly\tStopping\n\nLogistic\tRegression\n\nEstimating\tProbabilities\n\nTraining\tand\tCost\tFunction\n\nDecision\tBoundaries\n\nSoftmax\tRegression\n\nExercises\n\n5.\tSupport\tVector\tMachines\n\nLinear\tSVM\tClassification\n\nSoft\tMargin\tClassification\n\nNonlinear\tSVM\tClassification\n\nPolynomial\tKernel\n\nAdding\tSimilarity\tFeatures\n\nGaussian\tRBF\tKernel\n\nComputational\tComplexity\n\nSVM\tRegression",
      "content_length": 606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 709,
      "content": "Under\tthe\tHood\n\nDecision\tFunction\tand\tPredictions\n\nTraining\tObjective\n\nQuadratic\tProgramming\n\nThe\tDual\tProblem\n\nKernelized\tSVM\n\nOnline\tSVMs\n\nExercises\n\n6.\tDecision\tTrees\n\nTraining\tand\tVisualizing\ta\tDecision\tTree\n\nMaking\tPredictions\n\nEstimating\tClass\tProbabilities\n\nThe\tCART\tTraining\tAlgorithm\n\nComputational\tComplexity\n\nGini\tImpurity\tor\tEntropy?\n\nRegularization\tHyperparameters\n\nRegression\n\nInstability\n\nExercises\n\n7.\tEnsemble\tLearning\tand\tRandom\tForests\n\nVoting\tClassifiers\n\nBagging\tand\tPasting\n\nBagging\tand\tPasting\tin\tScikit-Learn\n\nOut-of-Bag\tEvaluation\n\nRandom\tPatches\tand\tRandom\tSubspaces\n\nRandom\tForests\n\nExtra-Trees",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 710,
      "content": "Feature\tImportance\n\nBoosting\n\nAdaBoost\n\nGradient\tBoosting\n\nStacking\n\nExercises\n\n8.\tDimensionality\tReduction\n\nThe\tCurse\tof\tDimensionality\n\nMain\tApproaches\tfor\tDimensionality\tReduction\n\nProjection\n\nManifold\tLearning\n\nPCA\n\nPreserving\tthe\tVariance\n\nPrincipal\tComponents\n\nProjecting\tDown\tto\td\tDimensions\n\nUsing\tScikit-Learn\n\nExplained\tVariance\tRatio\n\nChoosing\tthe\tRight\tNumber\tof\tDimensions\n\nPCA\tfor\tCompression\n\nIncremental\tPCA\n\nRandomized\tPCA\n\nKernel\tPCA\n\nSelecting\ta\tKernel\tand\tTuning\tHyperparameters\n\nLLE\n\nOther\tDimensionality\tReduction\tTechniques\n\nExercises\n\nII.\tNeural\tNetworks\tand\tDeep\tLearning",
      "content_length": 596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 711,
      "content": "9.\tUp\tand\tRunning\twith\tTensorFlow\n\nInstallation\n\nCreating\tYour\tFirst\tGraph\tand\tRunning\tIt\tin\ta\tSession\n\nManaging\tGraphs\n\nLifecycle\tof\ta\tNode\tValue\n\nLinear\tRegression\twith\tTensorFlow\n\nImplementing\tGradient\tDescent\n\nManually\tComputing\tthe\tGradients\n\nUsing\tautodiff\n\nUsing\tan\tOptimizer\n\nFeeding\tData\tto\tthe\tTraining\tAlgorithm\n\nSaving\tand\tRestoring\tModels\n\nVisualizing\tthe\tGraph\tand\tTraining\tCurves\tUsing\tTensorBoard\n\nName\tScopes\n\nModularity\n\nSharing\tVariables\n\nExercises\n\n10.\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\nFrom\tBiological\tto\tArtificial\tNeurons\n\nBiological\tNeurons\n\nLogical\tComputations\twith\tNeurons\n\nThe\tPerceptron\n\nMulti-Layer\tPerceptron\tand\tBackpropagation\n\nTraining\tan\tMLP\twith\tTensorFlow’s\tHigh-Level\tAPI\n\nTraining\ta\tDNN\tUsing\tPlain\tTensorFlow\n\nConstruction\tPhase\n\nExecution\tPhase",
      "content_length": 795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 712,
      "content": "Using\tthe\tNeural\tNetwork\n\nFine-Tuning\tNeural\tNetwork\tHyperparameters\n\nNumber\tof\tHidden\tLayers\n\nNumber\tof\tNeurons\tper\tHidden\tLayer\n\nActivation\tFunctions\n\nExercises\n\n11.\tTraining\tDeep\tNeural\tNets\n\nVanishing/Exploding\tGradients\tProblems\n\nXavier\tand\tHe\tInitialization\n\nNonsaturating\tActivation\tFunctions\n\nBatch\tNormalization\n\nGradient\tClipping\n\nReusing\tPretrained\tLayers\n\nReusing\ta\tTensorFlow\tModel\n\nReusing\tModels\tfrom\tOther\tFrameworks\n\nFreezing\tthe\tLower\tLayers\n\nCaching\tthe\tFrozen\tLayers\n\nTweaking,\tDropping,\tor\tReplacing\tthe\tUpper\tLayers\n\nModel\tZoos\n\nUnsupervised\tPretraining\n\nPretraining\ton\tan\tAuxiliary\tTask\n\nFaster\tOptimizers\n\nMomentum\tOptimization\n\nNesterov\tAccelerated\tGradient\n\nAdaGrad\n\nRMSProp\n\nAdam\tOptimization",
      "content_length": 719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 713,
      "content": "Learning\tRate\tScheduling\n\nAvoiding\tOverfitting\tThrough\tRegularization\n\nEarly\tStopping\n\nℓ1\tand\tℓ2\tRegularization\n\nDropout\n\nMax-Norm\tRegularization\n\nData\tAugmentation\n\nPractical\tGuidelines\n\nExercises\n\n12.\tDistributing\tTensorFlow\tAcross\tDevices\tand\tServers\n\nMultiple\tDevices\ton\ta\tSingle\tMachine\n\nInstallation\n\nManaging\tthe\tGPU\tRAM\n\nPlacing\tOperations\ton\tDevices\n\nParallel\tExecution\n\nControl\tDependencies\n\nMultiple\tDevices\tAcross\tMultiple\tServers\n\nOpening\ta\tSession\n\nThe\tMaster\tand\tWorker\tServices\n\nPinning\tOperations\tAcross\tTasks\n\nSharding\tVariables\tAcross\tMultiple\tParameter\tServers\n\nSharing\tState\tAcross\tSessions\tUsing\tResource\tContainers\n\nAsynchronous\tCommunication\tUsing\tTensorFlow\tQueues\n\nLoading\tData\tDirectly\tfrom\tthe\tGraph\n\nParallelizing\tNeural\tNetworks\ton\ta\tTensorFlow\tCluster\n\nOne\tNeural\tNetwork\tper\tDevice\n\nIn-Graph\tVersus\tBetween-Graph\tReplication",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 714,
      "content": "Model\tParallelism\n\nData\tParallelism\n\nExercises\n\n13.\tConvolutional\tNeural\tNetworks\n\nThe\tArchitecture\tof\tthe\tVisual\tCortex\n\nConvolutional\tLayer\n\nFilters\n\nStacking\tMultiple\tFeature\tMaps\n\nTensorFlow\tImplementation\n\nMemory\tRequirements\n\nPooling\tLayer\n\nCNN\tArchitectures\n\nLeNet-5\n\nAlexNet\n\nGoogLeNet\n\nResNet\n\nExercises\n\n14.\tRecurrent\tNeural\tNetworks\n\nRecurrent\tNeurons\n\nMemory\tCells\n\nInput\tand\tOutput\tSequences\n\nBasic\tRNNs\tin\tTensorFlow\n\nStatic\tUnrolling\tThrough\tTime\n\nDynamic\tUnrolling\tThrough\tTime\n\nHandling\tVariable\tLength\tInput\tSequences\n\nHandling\tVariable-Length\tOutput\tSequences\n\nTraining\tRNNs",
      "content_length": 593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 715,
      "content": "Training\ta\tSequence\tClassifier\n\nTraining\tto\tPredict\tTime\tSeries\n\nCreative\tRNN\n\nDeep\tRNNs\n\nDistributing\ta\tDeep\tRNN\tAcross\tMultiple\tGPUs\n\nApplying\tDropout\n\nThe\tDifficulty\tof\tTraining\tover\tMany\tTime\tSteps\n\nLSTM\tCell\n\nPeephole\tConnections\n\nGRU\tCell\n\nNatural\tLanguage\tProcessing\n\nWord\tEmbeddings\n\nAn\tEncoder–Decoder\tNetwork\tfor\tMachine\tTranslation\n\nExercises\n\n15.\tAutoencoders\n\nEfficient\tData\tRepresentations\n\nPerforming\tPCA\twith\tan\tUndercomplete\tLinear\tAutoencoder\n\nStacked\tAutoencoders\n\nTensorFlow\tImplementation\n\nTying\tWeights\n\nTraining\tOne\tAutoencoder\tat\ta\tTime\n\nVisualizing\tthe\tReconstructions\n\nVisualizing\tFeatures\n\nUnsupervised\tPretraining\tUsing\tStacked\tAutoencoders\n\nDenoising\tAutoencoders\n\nTensorFlow\tImplementation\n\nSparse\tAutoencoders",
      "content_length": 740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 716,
      "content": "TensorFlow\tImplementation\n\nVariational\tAutoencoders\n\nGenerating\tDigits\n\nOther\tAutoencoders\n\nExercises\n\n16.\tReinforcement\tLearning\n\nLearning\tto\tOptimize\tRewards\n\nPolicy\tSearch\n\nIntroduction\tto\tOpenAI\tGym\n\nNeural\tNetwork\tPolicies\n\nEvaluating\tActions:\tThe\tCredit\tAssignment\tProblem\n\nPolicy\tGradients\n\nMarkov\tDecision\tProcesses\n\nTemporal\tDifference\tLearning\tand\tQ-Learning\n\nExploration\tPolicies\n\nApproximate\tQ-Learning\n\nLearning\tto\tPlay\tMs.\tPac-Man\tUsing\tDeep\tQ-Learning\n\nExercises\n\nThank\tYou!\n\nA.\tExercise\tSolutions\n\nChapter\t1:\tThe\tMachine\tLearning\tLandscape\n\nChapter\t2:\tEnd-to-End\tMachine\tLearning\tProject\n\nChapter\t3:\tClassification\n\nChapter\t4:\tTraining\tModels\n\nChapter\t5:\tSupport\tVector\tMachines\n\nChapter\t6:\tDecision\tTrees",
      "content_length": 721,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 717,
      "content": "Chapter\t7:\tEnsemble\tLearning\tand\tRandom\tForests\n\nChapter\t8:\tDimensionality\tReduction\n\nChapter\t9:\tUp\tand\tRunning\twith\tTensorFlow\n\nChapter\t10:\tIntroduction\tto\tArtificial\tNeural\tNetworks\n\nChapter\t11:\tTraining\tDeep\tNeural\tNets\n\nChapter\t12:\tDistributing\tTensorFlow\tAcross\tDevices\tand\tServers\n\nChapter\t13:\tConvolutional\tNeural\tNetworks\n\nChapter\t14:\tRecurrent\tNeural\tNetworks\n\nChapter\t15:\tAutoencoders\n\nChapter\t16:\tReinforcement\tLearning\n\nB.\tMachine\tLearning\tProject\tChecklist\n\nFrame\tthe\tProblem\tand\tLook\tat\tthe\tBig\tPicture\n\nGet\tthe\tData\n\nExplore\tthe\tData\n\nPrepare\tthe\tData\n\nShort-List\tPromising\tModels\n\nFine-Tune\tthe\tSystem\n\nPresent\tYour\tSolution\n\nLaunch!\n\nC.\tSVM\tDual\tProblem\n\nD.\tAutodiff\n\nManual\tDifferentiation\n\nSymbolic\tDifferentiation\n\nNumerical\tDifferentiation\n\nForward-Mode\tAutodiff\n\nReverse-Mode\tAutodiff",
      "content_length": 806,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 718,
      "content": "E.\tOther\tPopular\tANN\tArchitectures\n\nHopfield\tNetworks\n\nBoltzmann\tMachines\n\nRestricted\tBoltzmann\tMachines\n\nDeep\tBelief\tNets\n\nSelf-Organizing\tMaps\n\nIndex",
      "content_length": 151,
      "extraction_method": "Unstructured"
    }
  ]
}