{
  "metadata": {
    "title": "AWS Cookbook - John Culkin Mike Zazon",
    "author": "John Culkin & Mike Zazon",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 489,
    "conversion_date": "2025-12-19T17:17:10.911356",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "AWS Cookbook - John Culkin Mike Zazon.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 3-10)",
      "start_page": 3,
      "end_page": 10,
      "detection_method": "topic_boundary",
      "content": "AWS Cookbook\n\nby John Culkin and Mike Zazon\n\nCopyright © 2022 Culkins Coffee Shop LLC and Mike Zazon. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nAcquisitions Editor: Jennifer Pollock Development Editor: Virginia Wilson Production Editor: Christopher Faucher Copyeditor: nSight, Inc. Proofreader: Sharon Wilkey Indexer: Ellen Troutman-Zaig Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Kate Dullea\n\nDecember 2021: First Edition\n\nRevision History for the First Edition\n\n2021-12-02: First Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492092605 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. AWS Cookbook, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nThe views expressed in this work are those of the authors and do not represent the publisher’s views. While the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-492-09260-5\n\n[LSI]\n\nDedication\n\nDedicated to my father, who taught me that a spreadsheet could be used for much more than totaling up columns.\n\n—John\n\nDedicated to my aunt, Judy Dunn. Thank you for the Tandy 1000 PC that sparked my fascination with computer programming and technology.\n\n—Mike\n\nForeword\n\nAs part of the Amazon Web Services (AWS) team since the beginning, I have been able to watch it grow in scale, richness, and complexity from a unique vantage point. Even after writing thousands of blog posts and millions of words, I learn something new and useful about AWS just about every day.\n\nWith well over two hundred services in production and more launching regularly, AWS could easily leave you feeling overwhelmed. In addition to tens of thousands of pages of official AWS documentation, bloggers, AWS Heroes, AWS Partners, and others have created innumerable pieces of content—including blog posts, videos, webinars, overviews, and code samples.\n\nWhile there’s no substitute for having a full and complete understanding of a particular AWS service, the reality is that you often simply need to solve a “point” problem. Even after you understand a service, remembering how to use it to solve that problem can be a challenge—at least it is for me.\n\nAnd that is where this cookbook comes in. Because of its broad selection of topics and carefully chosen recipes, I am confident that you will be able to quickly find one that addresses your immediate need and to put it into practice in short order. You can solve your problem, refresh your knowledge of that aspect of AWS, and move forward to create value for your customers!\n\nMy favorite aspect of this book is that it does not hand-wave past any of the details. Each recipe assumes that you start fresh and then helps you to cook up a perfectly seasoned solution. Nothing is left to chance, and you can use the recipes as is in most cases. The recipes also cover the all-important cleanup phase and ensure that you leave your AWS environment as you found it.\n\nWhere appropriate, the recipes use the AWS Cloud Development Kit (CDK) and include all of the necessary “moving parts.” The CDK provides a double benefit; in addition to helping you to move forward more quickly, these CDK elements can help you learn more about how to put infrastructure as code (IaC) into practice.\n\nMost cookbooks are designed to be browsed and savored, and this one is no exception. Flip through it, read an entire chapter, or use just a recipe or two, as you wish. I also recommend that you go through all of Chapter 1, just to make sure that your environment is set up and ready to go. Then, when you are presented with a problem to solve, find the appropriate recipe, put it into practice, and reap the benefits.\n\nJeff Barr\n\nVP and Chief Evangelist at AWS\n\nSeattle, WA\n\nNovember 2021\n\nPreface\n\nThe vast majority of workloads will go to the cloud. We’re just at the beginning—there’s so much more to happen.\n\nAndy Jassy\n\nCloud usage has been gaining traction with enterprises and small businesses over the last decade and continues to accelerate. Gartner said the worldwide infrastructure as a service (IaaS) public cloud services market grew 40.7% in 2020. The rapid growth of the cloud has led to a huge demand for cloud skills by many organizations. Many IT professionals understand the basic concepts of the cloud but want to become more comfortable working in the cloud. This gap between the supply and demand of cloud skills presents a significant opportunity for individuals to level up their career.\n\nThrough our combined 20+ years of cloud experience, we have had the benefit of working on Amazon Web Services (AWS) projects in many different roles. We have provided guidance to hundreds of developers on how and when to use AWS services. This has allowed us to understand the common challenges and easy wins of the cloud. We would like to share these lessons with you and give you a leg up for your own advancement. We wrote this book to share some of our knowledge and enable you to quickly acquire useful skills for working in the cloud. We hope that you will find yourself using this book as reference material for many years to come.\n\nWho This Book Is For This book is for developers, engineers, and architects of all levels, from beginner to expert. Beginners will learn cloud concepts and become comfortable working with cloud services. Experts will be able to examine code used to stand up recipe foundations, explore new services, and gain additional perspectives. If the plethora of cloud services and combinations seem overwhelming to you, then this book is for you. The recipes in this book aim to provide “Hello, World” proofs of concept and components of enterprise-grade applications. This will be accomplished using common use cases with guided walk-throughs of scenarios that you can directly apply to your current or future work. These curated and experience-building recipes are meant to demystify services and will immediately deliver value, regardless of your AWS experience level.\n\nWhat You Will Learn In addition to opening up new career opportunities, being able to harness the power of AWS will give you the ability to create powerful systems and applications that solve many interesting and demanding problems in our world today. Would you like to handle 60,000 cyber threats per second using AWS machine learning like Siemens does? Or reduce your organization’s on- premises footprint and expand its use of microservices like Capital One has? If so, the practical examples in this book will help expedite your learning by providing tangible examples showing how you can put the building blocks of AWS together to form practical solutions that address common scenarios. The on-demand consumption model, vast capacity, advanced capabilities, and global footprint of the cloud create new possibilities that need to be explored.\n\nThe Recipes We break the book into chapters that focus on general areas of technology (e.g., security, networking, artificial intelligence, etc.). The recipes contained within the chapters are bite-sized, self-contained, and easily consumable. Recipes vary in length and complexity. Each recipe has a problem statement, solution (with diagram), and discussion. Problem statements are tightly defined to avoid confusion. Solutions contain required preparation and steps to walk you through the work needed to accomplish the goal. When appropriate, explicit validation checks will be provided. We’ve also added extra challenges to the recipes to help you advance your learning if you wish to do so. Finally, we end each recipe with a short discussion to help you understand the solution and why it matters, suggestions to extend the solution, and ways to utilize it for real impact.\n\nNOTE\n\nTo keep your AWS bill low and keep your account tidy, each recipe has cleanup steps provided in the repositories\n\nassociated with the book.\n\nEach chapter has its own repository at https://github.com/awscookbook. The repository contains preparation steps for easy copying and pasting, required files, and infrastructure as code. We have also created GitHub templates for reporting bugs and suggesting new recipes. We encourage you to leverage GitHub to submit issues, create requests for new recipes, and submit your own pull requests. We will actively maintain the chapter repositories with updates for recipe steps and code in the README files of each recipe. Be sure to check these for any new or alternative approaches. We look forward to interacting with you on GitHub with new fun challenges and hints to assist you.\n\nSome recipes are “built from scratch,” and others include preparation steps to allow you to interact with common scenarios seen in the real world. We have provided code to enable you to easily deploy the prerequisites. For example, Recipe 6.5, assumes that you are a container developer creating an application deployment that requires an existing network stack. When prerequisites exist, they can be “pre-baked” with preparation steps using code provided in the repositories. When substantial preparation for a recipe is needed, you will use the AWS Cloud Development Kit (CDK), which is a fantastic tool for intelligently defining and declaring infrastructure. The majority of the recipes are CLI based; when appropriate, we use console walk-throughs including screenshots or descriptive text.\n\nNOTE\n\nThere are many ways to achieve similar outcomes on AWS; this book will not be an exhaustive list. Many factors will\n\ndictate the best overall solution for your use case. We have selected recipe topics to help you learn about AWS and make\n\nthe best choices for your specific needs.\n\nYou’ll find recipes for things like the following:\n\nRedacting personally identifiable information (PII) from text by using Amazon Comprehend\n\nAutomating password rotation for Amazon Relational Database Service (RDS) databases\n\nUsing VPC Reachability Analyzer to verify and troubleshoot network paths\n\nAlong with the recipes, we also provide short lines of code in the Appendix that will quickly accomplish valuable and routine tasks. We feel that these are great tidbits to add to your cloud toolbox.\n\nWARNING\n\nAWS has a free tier, but implementing recipes in this book could incur costs. We provide cleanup instructions, but you are\n\nresponsible for any costs in your account. We recommend checking out the Well-Architected Labs developed by AWS on\n\nexpenditure awareness and leveraging AWS Budgets actions to control costs.\n\nWhat You Will Need Here are the requirements to get started and some tips on where to find assistance:\n\nAWS account\n\nSetup instructions\n\nAn IAM user with console and programmatic access\n\nAdministrator privileges for your IAM user\n\nPersonal computer/laptop\n\nSoftware\n\nWeb browser (e.g., Microsoft Edge, Google Chrome, or Mozilla Firefox)\n\nTerminal with bash or Z shell (Zsh)\n\nGit\n\nInstall instructions\n\nHomebrew (optional but recommended to install other requirements)\n\nInstall instructions\n\nCode editor (e.g., VSCodium or AWS Cloud9)\n\nRecommended install: brew install --cask vscodium\n\nAWS CLI version 2 (2.1.26 or later)\n\nInstall guide\n\nRecommended install: brew install awscli@2\n\nPython 3.7.9 (and pip) or later\n\nExample install: brew install python@3.7\n\nAWS Cloud Development Kit version 2.0 or later\n\nGetting started guide\n\nRecommended install: brew install npm and npm i -g aws-cdk@next\n\nRecommended: Create a folder in your home directory called AWSCookbook. This will allow you to clone each chapter’s repository in one place:\n\nAWSCookbook:$ tree -L 1\n\n.\n\n├── AccountManagement\n\n├── ArtificialIntelligence\n\n├── BigData\n\n...\n\nNOTE\n\nAt the time of publishing, the AWS CDK has two versions: version 1 and version 2 (developer preview). The code we\n\nhave provided is written for version 2. You can find out more information about how to migrate to and install CDK\n\nversion 2 in this AWS CDK v2 article.\n\nGetting Started This section provides examples of techniques and approaches we perform throughout the book to make the recipe steps easier to follow. You can skip over these topics if you feel comfortable with them. You can always come back and reference this section.\n\nSetups\n\nIn addition to the installation of the prerequisites listed previously, you will need the following access.\n\nAWS account setup\n\nYou will need a user with administrative permissions. Some of the recipes require the ability to create AWS Identity and Access Management (IAM) resources. You can follow the AWS guide for creating your first IAM admin user and user group.\n\nGeneral workstation setup steps for CLI recipes\n\nWe have created a group of code repositories available at https://github.com/awscookbook. Create a folder called AWSCookbook in your home directory (or any place of your choosing) and cd there:\n\nmkdir ~/AWSCookbook && cd ~/AWSCookbook\n\nThis will give you a place to check out chapter repositories (e.g., Security):\n\ngit clone https://github.com/AWSCookbook/Security\n\nSet and export your default Region in your terminal:\n\nexport AWS_REGION=us-east-1\n\nTIP",
      "page_number": 3
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 11-18)",
      "start_page": 11,
      "end_page": 18,
      "detection_method": "topic_boundary",
      "content": "AWS offers many Regions across the world for cloud deployments. We’ll be using the us-east-1 Region for simplicity. As\n\nlong as the services are available, there is no reason these recipes won’t work in other Regions. AWS has a list of Regions\n\nand services.\n\nSet your AWS ACCOUNT_ID by parsing output from the aws sts get-caller-identity operation:\n\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity \\\n\n--query Account --output text)\n\nNOTE\n\nThe aws sts get-caller-identity operation “returns details about the IAM user or role whose credentials are used to call the operation.”\n\nValidate AWS Command Line Interface (AWS CLI) setup and access:\n\naws ec2 describe-instances\n\nIf you don’t have any EC2 instances deployed, you should see output similar to the following:\n\n{\n\n\"Reservations\": []\n\n}\n\nNOTE\n\nAWS CLI version 2 will by default send command output with multiple lines to less in your terminal. You can type q to exit. If you want to override this behavior, you can modify your ~/.aws/config file to remove this default functionality.\n\nTIP\n\nAWS CloudShell is a browser-based terminal that you can use to quickly create a terminal environment in your\n\nauthenticated AWS Console session to run AWS CLI commands from. By default, it uses the identity of your browser\n\nsession to interact with the AWS APIs. Many of the recipes can be run using CloudShell. You can use CloudShell to run\n\nrecipe steps, clean up commands, and other AWS CLI commands as your authenticated user, if you do not want to create a\n\nsession that you use in your own local terminal environment on your workstation.\n\nTechniques and Approaches Used in This Book\n\nThe next few sections will explain and give examples of some ways of using the CLI to help you with recipes.\n\nQuerying outputs, environment variables, and command substitution\n\nSometimes when subsequent commands depend on outputs from the command you are currently running. The AWS CLI provides the ability for client-side filtering of output. At times, we will set environment variables that contain these outputs by leveraging command substitution.\n\nWe’ll combine these three techniques to make things easier for you as you proceed through steps in the book. Here is an example:\n\nUse the AWS Security Token Service (AWS STS) to retrieve your IAM user (or role) Amazon Resource Name (ARN) with the AWS CLI:\n\naws sts get-caller-identity\n\nYou should see output similar to the following:\n\n{\n\n\"UserId\": \"EXAMPLE\",\n\n\"Account\": \"111111111111\",\n\n\"Arn\": \"arn:aws:iam::111111111111:user/UserName\"\n\n}\n\nAn example of querying for the ARN value and outputting it to the terminal follows:\n\naws sts get-caller-identity --query Arn --output text\n\nYou should see output similar to the following:\n\narn:aws:iam::111111111111:user/UserName\n\nQuery for the ARN value and set it as an environment variable using command substitution:\n\nPRINCIPAL_ARN=$(aws sts get-caller-identity --query Arn --output text)\n\nTo check the value of an environment variable, for example, you can echo it to the terminal:\n\necho $PRINCIPAL_ARN\n\nYou should see output similar to the following:\n\narn:aws:iam::111111111111:user/UserName\n\nTIP\n\nUsing the --dry-run flag is always a good idea when performing an operation that makes changes—for example, aws ec2 create-vpc --dry-run --cidr-block 10.10.0.0/16.\n\nReplacing values in provided template files\n\nWhere possible, to simplify the learning experience for you, we have provided template files in the chapter code repositories that you can use as a starting point as input to some of the commands you will run in recipe steps. For example, when you create an AWS CodeDeploy configuration in Recipe 6.5, we provide codedeploy-template.json with AWS_ACCOUNT_ID, PROD_LISTENER_ARN, and TEST_LISTENER_ARN placeholders in the JSON file. We expect you to replace these placeholder values and save the file as codedeploy.json.\n\nTo further simplify your experience, if you follow the steps exactly and save these to environment variables, you can use the sed command to replace the values. Where possible, we provide you a command to do this, such as this example from Chapter 6: Use the sed command to replace the values with the environment variables you exported with the helper.py script:\n\nsed -e \"s/AWS_ACCOUNT_ID/${AWS_ACCOUNT_ID}/g\" \\\n\ne \"s|PROD_LISTENER_ARN|${PROD_LISTENER_ARN}|g\" \\\n\ne \"s|TEST_LISTENER_ARN|${TEST_LISTENER_ARN}|g\" \\\n\ncodedeploy-template.json > codedeploy.json\n\nPasswords\n\nDuring some of the steps in the recipes, you will create passwords and temporarily save them as environment variables to use in subsequent steps. Make sure you unset the environment variables by following the cleanup steps when you complete the recipe. We use this approach for simplicity of understanding. A more secure method (such as the method in Recipe 1.8) should be used in production environments by leveraging AWS Secrets Manager.\n\nGeneration\n\nYou can use AWS Secrets Manager via the AWS CLI to generate passwords with specific requirements. An example from Chapter 4 looks like this:\n\nADMIN_PASSWORD=$(aws secretsmanager get-random-password \\\n\n--exclude-punctuation \\\n\n--password-length 41 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nUsage and storage\n\nIn production environments, you should use AWS Secrets Manager or AWS Systems Manager Parameter Store (using secure strings) with IAM policies to control who and what can access the secrets. For simplicity, some of the policies of passwords and secrets used in the recipes might not be as locked down from a policy perspective as you would want in a production environment. Be sure to always write your own IAM policies to control this behavior in practice.\n\nRandom suffixes\n\nWe generate a lot of random suffixes when we deal with global services like Amazon S3. These are needed because S3 bucket names need to be globally unique across the entire AWS customer base. Secrets Manager can be used via the CLI to generate a string that satisfies the naming convention and adds this random element to ensure all book readers can create resources and follow along using the same commands:\n\nRANDOM_STRING=$(aws secretsmanager get-random-password \\\n\n--exclude-punctuation --exclude-uppercase \\\n\n--password-length 6 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nYou can also use any other utilities to generate random strings. Some local tools may be preferred.\n\nAWS Cloud Development Kit and helper.py\n\nA good place to start is the “Getting started with the AWS CDK” guide. After you have CDK 2.0 installed, if this is the first time you are using the AWS CDK, you’ll need to bootstrap with the Region you are working on with the AWS CDK toolkit:\n\ncdk bootstrap aws://$AWS_ACCOUNT_ID/$AWS_REGION\n\nWe use the AWS CDK when needed throughout the book to give you the ability to deploy a consistent scenario that aligns with the problem statement you see in the recipe. You can also\n\nchoose to execute the recipe steps in your own existing environments, as long as you have the input variables required for the recipe steps. If things don’t work in your environment, you can stand up the provided environment and compare.\n\nThe CDK code we included in the repositories deploys resources using the AWS CloudFormation service, and we wrote output variables that you use in recipe steps. We created a Python script called helper.py which you can run in your terminal to take the CloudFormation output and set local variables to make the recipe steps easier to follow—in most cases, even copy and paste.\n\nAn example set of commands for deploying CDK code for a recipe after checking out the chapter repository for Chapter 4, looks like the following:\n\ncd 401-Creating-an-Aurora-Serverless-DB/cdk-AWS-Cookbook-401/\n\ntest -d .venv || python3 -m venv .venv\n\nsource .venv/bin/activate\n\npip install --upgrade pip setuptools wheel\n\npip install -r requirements.txt\n\ncdk deploy\n\nYou can easily copy and paste the preceding code from the root of the chapter repository (assuming you have Python, pip, and CDK installed as prerequisites) to deploy the scenario that the solution will address in the solution steps of the recipe.\n\nThe helper.py tool we created can then be run in your terminal after the cdk deploy is complete:\n\npython helper.py\n\nYou should see output that you can copy and paste into your terminal to set environment variables from the CDK CloudFormation stack outputs:\n\n$ python helper.py\n\nCopy and paste the commands below into your terminal\n\nROLE_NAME='cdk-aws-cookbook-108-InstanceSS1PK7LB631QYEF'\n\nINSTANCE_ID='random string here'\n\nNOTE\n\nFinally, a reminder that although we work for AWS, the opinions expressed in this book are our own.\n\nPut on your apron, and let’s get cooking with AWS!\n\nConventions Used in This Book The following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nConstant width italic\n\nShows text that should be replaced with user-supplied values or by values determined by context.\n\nTIP\n\nThis element signifies a tip or suggestion.\n\nNOTE\n\nThis element signifies a general note.\n\nWARNING\n\nThis element indicates a warning or caution.\n\nUsing Code Examples Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/awscookbook.\n\nIf you have a technical question or a problem using the code examples, please send email to bookquestions@oreilly.com.\n\nThis book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require permission.\n\nWe appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “AWS Cookbook by John Culkin and Mike Zazon (O’Reilly). Copyright 2022 Culkins Coffee Shop LLC and Mike Zazon, 978-1-492-09260-5.”\n\nIf you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.\n\nO’Reilly Online Learning\n\nNOTE\n\nFor more than 40 years, O’Reilly Media has provided technology and business training, knowledge, and insight to help\n\ncompanies succeed.\n\nOur unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\n\nHow to Contact Us Please address comments and questions concerning this book to the publisher:\n\nO’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax)\n\nWe have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/AWS-cookbook.\n\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\n\nFor news and information about our books and courses, visit http://oreilly.com.\n\nFind us on Facebook: http://facebook.com/oreilly\n\nFollow us on Twitter: http://twitter.com/oreillymedia\n\nWatch us on YouTube: http://youtube.com/oreillymedia\n\nAcknowledgments Thank you to Jeff Armstrong, author of Migrating to AWS, A Manager’s Guide for introducing us to O’Reilly.\n\nWe want to recognize the tech reviewers who helped get this book to where it is today. Their keen eyes, opinions, and technical prowess are greatly appreciated. Jess Males, Gaurav Raje, Jeff Barr, Paul Bayer, Neil Stewart, David Kheyman, Justin Domingus, Justin Garrison, Julian Pittas, Mark Wilkins, and Virginia Chu—thank you.\n\nThanks to the knowledgeable community at r/aws for always providing great insights and opinions.\n\nThank you to our production editor, Christopher Faucher, for getting the book in tip-top shape for release. Thanks also to our editor, Virginia Wilson, for taking the time to work with first-time authors during a pandemic. Your patience, suggestions, and guidance allowed us to complete this book and remain (somewhat) sane :-) Chapter 1. Security\n\n1.0 Introduction The average cost of a data breach in 2021 reached a new high of USD 4.24 million as reported by the IBM/Ponemon Institute Report. When you choose to run your applications in the cloud, you trust AWS to provide a secure infrastructure that runs cloud services so that you can focus on your own innovation and value-added activities.\n\nBut security in the cloud is a shared responsibility between you and AWS. You are responsible for the configuration of things like AWS Identity and Access Management (IAM) policies, Amazon EC2 security groups, and host based firewalls. In other words, the security of the hardware and software platform that make up the AWS cloud is an AWS responsibility. The security of software and configurations that you implement in your AWS account(s) are your responsibility.\n\nAs you deploy cloud resources in AWS and apply configuration, it is critical to understand the security settings required to maintain a secure environment. This chapter’s recipes include best",
      "page_number": 11
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 19-36)",
      "start_page": 19,
      "end_page": 36,
      "detection_method": "topic_boundary",
      "content": "practices and use cases focused on security. As security is a part of everything, you will use these recipes in conjunction with other recipes and chapters in this book. For example, you will see usage of AWS Systems Manager Session Manager used throughout the book when connecting to your EC2 instances. These foundational security recipes will give you the tools you need to build secure solutions on AWS.\n\nIn addition to the content in this chapter, so many great resources are available for you to dive deeper into security topics on AWS. “The Fundamentals of AWS Cloud Security”, presented at the 2019 AWS security conference re:Inforce, gives a great overview. A more advanced talk, “Encryption: It Was the Best of Controls, It Was the Worst of Controls”, from AWS re:Invent, explores encryption scenarios explained in detail.\n\nTIP\n\nAWS publishes a best practices guide for securing your account, and all AWS account holders should be familiar with the\n\nbest practices as they continue to evolve.\n\nWARNING\n\nWe cover important security topics in this chapter. It is not possible to cover every topic as the list of services and\n\nconfigurations (with respect to security on AWS) continues to grow and evolve. AWS keeps its Best Practices for Security,\n\nIdentity, and Compliance web page up-to-date.\n\nWorkstation Configuration\n\nYou will need a few things installed to be ready for the recipes in this chapter.\n\nGeneral setup\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Security\n\n1.1 Creating and Assuming an IAM Role for Developer Access\n\nProblem\n\nTo ensure that you are not always using administrative permissions, you need to create an IAM role for development use in your AWS account.\n\nSolution\n\nCreate a role using an IAM policy that will allow the role to be assumed later. Attach the AWS managed PowerUserAccess IAM policy to the role (see Figure 1-1).\n\nFigure 1-1. Create role, attach policy, and assume role\n\nSteps\n\n1.\n\nCreate a file named assume-role-policy-template.json with the following content. This will allow an IAM principal to assume the role you will create next (file provided in the repository):\n\n{\n\n2.\n\n3.\n\n4.\n\n5.\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"AWS\": \"PRINCIPAL_ARN\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nTIP\n\nIf you are using an IAM user, and you delete and re-create the IAM user, this policy will not continue\n\nto work because of the way that the IAM service helps mitigate the risk of privilege escalation. For\n\nmore information, see the Note in the IAM documentation about this.\n\nRetrieve the ARN for your user and set it as a variable:\n\nPRINCIPAL_ARN=$(aws sts get-caller-identity --query Arn --output text)\n\nUse the sed command to replace PRINCIPAL_ARN in the assume-role-policy- template.json file and generate the assume-role-policy.json file:\n\nsed -e \"s|PRINCIPAL_ARN|${PRINCIPAL_ARN}|g\" \\\n\nassume-role-policy-template.json > assume-role-policy.json\n\nCreate a role and specify the assume role policy file:\n\nROLE_ARN=$(aws iam create-role --role-name AWSCookbook101Role \\\n\n--assume-role-policy-document file://assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\nAttach the AWS managed PowerUserAccess policy to the role:\n\naws iam attach-role-policy --role-name AWSCookbook101Role \\\n\n--policy-arn arn:aws:iam::aws:policy/PowerUserAccess\n\nNOTE\n\nAWS provides access policies for common job functions for your convenience. These policies may be\n\na good starting point for you to delegate user access to your account for specific job functions;\n\nhowever, it is best to define a least-privilege policy for your own specific requirements for every\n\naccess need.\n\nValidation checks\n\nAssume the role:\n\naws sts assume-role --role-arn $ROLE_ARN \\\n\n--role-session-name AWSCookbook101\n\nYou should see output similar to the following:\n\n{\n\n\"Credentials\": {\n\n\"AccessKeyId\": \"<snip>\",\n\n\"SecretAccessKey\": \"<snip>\",\n\n\"SessionToken\": \"<snip>\",\n\n\"Expiration\": \"2021-09-12T23:34:56+00:00\"\n\n},\n\n\"AssumedRoleUser\": {\n\n\"AssumedRoleId\": \"EXAMPLE:AWSCookbook101\",\n\n\"Arn\": \"arn:aws:sts::11111111111:assumed-role/AWSCookbook101Role/AWSCookbook101\"\n\n}\n\n}\n\nTIP\n\nThe AssumeRole API returns a set of temporary credentials for a role session from the AWS Security Token Service (STS)\n\nto the caller as long as the permissions in the AssumeRole policy for the role allow. All IAM roles have an AssumeRole policy associated with them. You can use the output of this to configure the credentials for the AWS CLI; set the\n\nAccessKey, SecretAccessKey, and SessionToken as environment variables; and also assume the role in the AWS Console\n\nusing the Switch Role feature. When your applications need to make AWS API calls, the AWS SDK for your programming\n\nlanguage of choice handles this for them.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nUsing administrative access for routine development tasks is not a security best practice. Giving unneeded permissions can result in unauthorized actions being performed. Using the PowerUserAccess AWS managed policy for development purposes is a better alternative to start rather than using AdministratorAccess. Later, you should define your own customer managed policy granting only the specific actions for your needs. For example, if you need to log in often to check the status of your EC2 instances, you can create a read-only policy for this purpose and attach it to a role. Similarly, you can create a role for billing access and use it to access the AWS Billing console only. The more you practice using the principle of least privilege, the more security will become a natural part of what you do.\n\nYou used an IAM user in this recipe to perform the steps. If you are using an AWS account that leverages federation for access (e.g., a sandbox or development AWS account at your employer), you should use temporary credentials from the AWS STS rather than an IAM user. This type of access uses time-based tokens that expire after an amount of time, rather than “long-lived” credentials like access keys or passwords. When you performed the AssumeRole in the validation steps, you called the STS service for temporary credentials. To help with frequent AssumeRole operations, the AWS CLI supports named profiles that can automatically assume and refresh your temporary credentials for your role when you specify the role_arn parameter in the named profile.\n\nTIP\n\nYou can require multi-factor authentication (MFA) as a condition within the AssumeRole policies you create. This would allow the role to be assumed only by an identity that has been authenticated with MFA. For more information about\n\nrequiring MFA for AssumeRole, see the support document.\n\nSee Recipe 9.4 to create an alert when a root login occurs.\n\nTIP\n\nYou can grant cross-account access to your AWS resources. The resource you define in the policy in this recipe would\n\nreference the AWS account and principal within that account that you would like to delegate access to. You should always\n\nuse an ExternalID when enabling cross-account access. For more information, see the official tutorial for cross-account access.\n\nChallenge\n\nCreate additional IAM roles for each of the AWS managed policies for job functions (e.g., billing, database administrator, networking, etc.) 1.2 Generating a Least Privilege IAM Policy Based on Access Patterns\n\nProblem\n\nYou would like to implement least privilege access for your user and scope down the permissions to allow access to only the services, resources, and actions you need to use in your AWS account.\n\nSolution\n\nUse the IAM Access Analyzer in the IAM console to generate an IAM policy based on the CloudTrail activity in your AWS account, as shown in Figure 1-2.\n\nFigure 1-2. IAM Access Analyzer workflow\n\nPrerequisite\n\nSteps\n\n1.\n\n2.\n\n3.\n\nCloudTrail logging enabled for your account to a configured S3 bucket (see Recipe 9.3)\n\nNavigate to the IAM console and select your IAM role or IAM user that you would like to generate a policy for.\n\nOn the Permissions tab (the default active tab when viewing your principal), scroll to the bottom, expand the “Generate policy based on CloudTrail events” section, and click the “Generate policy” button.\n\nTIP\n\nFor a quick view of the AWS services accessed from your principal, click the Access Advisor tab and\n\nview the service list and access time. While the IAM Access Advisor is not as powerful as the Access\n\nAnalyzer, it can be helpful when auditing or troubleshooting IAM principals in your AWS account.\n\nSelect the time period of CloudTrail events you would like to evaluate, select your CloudTrail trail, choose your Region (or select “All regions”), and choose “Create and use a new service role.” IAM Access Analyzer will create a role for the service to use for read access to your trail that you selected. Finally, click “Generate policy.” See Figure 1-3 for an example.\n\n4.\n\n5.\n\nFigure 1-3. Generating a policy in the IAM Access Analyzer configuration\n\nNOTE\n\nThe role creation can take up to 30 seconds. Once the role is created, the policy generation will take an\n\namount of time depending on how much activity is in your CloudTrail trail.\n\nOnce the analyzer has completed, scroll to the bottom of the permissions tab and click “View generated policy,” as shown in Figure 1-4.\n\nFigure 1-4. Viewing the generated policy\n\nClick Next, and you will see a generated policy in JSON format that is based on the activity that your IAM principal has made. You can edit this policy in the interface if you wish to add additional permissions. Click Next again, choose a name, and you can deploy this generated policy as an IAM policy.\n\nYou should see a generated IAM policy in the IAM console similar to this:\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"access-analyzer:ListPolicyGenerations\",\n\n\"cloudtrail:DescribeTrails\",\n\n\"cloudtrail:LookupEvents\",\n\n\"iam:GetAccountPasswordPolicy\",\n\n\"iam:GetAccountSummary\",\n\n\"iam:GetServiceLastAccessedDetails\",\n\n\"iam:ListAccountAliases\",\n\n\"iam:ListGroups\",\n\n\"iam:ListMFADevices\",\n\n\"iam:ListUsers\",\n\n\"s3:ListAllMyBuckets\",\n\n\"sts:GetCallerIdentity\"\n\n],\n\n\"Resource\": \"*\"\n\n}, ...\n\n}\n\nValidation checks\n\nCreate a new IAM user or role and attach the newly created IAM policy to it. Perform an action granted by the policy to verify that the policy allows your IAM principal to perform the actions that you need it to.\n\nDiscussion\n\nYou should always seek to implement least privilege IAM policies when you are scoping them for your users and applications. Oftentimes, you might not know exactly what permissions you may need when you start. With IAM Access Analyzer, you can start by granting your users and applications a larger scope in a development environment, enable CloudTrail logging (Recipe 9.3), and then run IAM Access Analyzer after you have a window of time that provides a good representation of the usual activity (choose this time period in the Access Analyzer configuration as you did in step 3). The generated policy will contain all of the necessary permissions to allow your application or users to work as they did during that time period that you chose to analyze, helping you implement the principle of least privilege.\n\nNOTE\n\nYou should also be aware of the list of services that Access Analyzer supports.\n\nChallenge\n\nUse the IAM Policy Simulator (see Recipe 1.4) on the generated policy to verify that the policy contains the access you need.\n\n1.3 Enforcing IAM User Password Policies in Your AWS Account\n\nNOTE\n\nSpecial thanks to Gaurav Raje for his contribution to this recipe.\n\nProblem\n\nYour security policy requires that you must enforce a password policy for all the users within your AWS account. The password policy sets a 90-day expiration, and passwords must be made up of a minimum of 32 characters including lowercase and uppercase letters, numbers, and symbols.\n\nSolution\n\nSet a password policy for IAM users in your AWS account. Create an IAM group, an IAM user, and add the user to the group to verify that the policy is being enforced (see Figure 1-5).\n\nFigure 1-5. Using password policies with IAM users\n\nNOTE\n\nIf your organization has a central user directory, we recommend using identity federation to access your AWS accounts\n\nusing AWS Single Sign-On (SSO) rather than create individual IAM users and groups. Federation allows you to use an\n\nidentity provider (IdP) where you already maintain users and groups. AWS publishes a guide that explains federated\n\naccess configurations available. You can follow Recipe 9.6 to enable AWS SSO for your account even if you do not have\n\nan IdP available (AWS SSO provides a directory you can use by default).\n\nSteps\n\n1.\n\nSet an IAM password policy using the AWS CLI to require lowercase and uppercase letters, symbols, and numbers. The policy should indicate a minimum length of 32 characters, a maximum password age of 90 days, and password reuse prevented:\n\naws iam update-account-password-policy \\\n\n--minimum-password-length 32 \\\n\n--require-symbols \\\n\n--require-numbers \\\n\n2.\n\n3.\n\n4.\n\n--require-uppercase-characters \\\n\n--require-lowercase-characters \\\n\n--allow-users-to-change-password \\\n\n--max-password-age 90 \\\n\n--password-reuse-prevention true\n\nCreate an IAM group:\n\naws iam create-group --group-name AWSCookbook103Group\n\nYou should see output similar to the following:\n\n{\n\n\"Group\": {\n\n\"Path\": \"/\",\n\n\"GroupName\": \"AWSCookbook103Group\",\n\n\"GroupId\": \"<snip>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:group/AWSCookbook103Group\",\n\n\"CreateDate\": \"2021-11-06T19:26:01+00:00\"\n\n}\n\n}\n\nAttach the ReadOnlyAccess policy to the group:\n\naws iam attach-group-policy --group-name AWSCookbook103Group \\\n\n--policy-arn arn:aws:iam::aws:policy/AWSBillingReadOnlyAccess\n\nTIP\n\nIt is best to attach policies to groups and not directly to users. As the number of users grows, it is\n\neasier to use IAM groups to delegate permissions for manageability. This also helps to meet\n\ncompliance for standards like CIS Level 1.\n\nCreate an IAM user:\n\n5.\n\n6.\n\naws iam create-user --user-name awscookbook103user\n\nYou should see output similar to the following:\n\n{\n\n\"User\": {\n\n\"Path\": \"/\",\n\n\"UserName\": \"awscookbook103user\",\n\n\"UserId\": \"<snip>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:user/awscookbook103user\",\n\n\"CreateDate\": \"2021-11-06T21:01:47+00:00\"\n\n}\n\n}\n\nUse Secrets Manager to generate a password that conforms to your password policy:\n\nRANDOM_STRING=$(aws secretsmanager get-random-password \\\n\n--password-length 32 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nCreate a login profile for the user that specifies a password:\n\naws iam create-login-profile --user-name awscookbook103user \\\n\n--password $RANDOM_STRING\n\nYou should see output similar to the following:\n\n{\n\n\"LoginProfile\": {\n\n\"UserName\": \"awscookbook103user\",\n\n\"CreateDate\": \"2021-11-06T21:11:43+00:00\",\n\n\"PasswordResetRequired\": false\n\n}\n\n}\n\n7.\n\nAdd the user to the group you created for billing view-only access:\n\naws iam add-user-to-group --group-name AWSCookbook103Group \\\n\n--user-name awscookbook103user\n\nValidation checks\n\nVerify that the password policy you set is now active:\n\naws iam get-account-password-policy\n\nYou should see output similar to:\n\n{\n\n\"PasswordPolicy\": {\n\n\"MinimumPasswordLength\": 32,\n\n\"RequireSymbols\": true,\n\n\"RequireNumbers\": true,\n\n\"RequireUppercaseCharacters\": true,\n\n\"RequireLowercaseCharacters\": true,\n\n\"AllowUsersToChangePassword\": true,\n\n\"ExpirePasswords\": true,\n\n\"MaxPasswordAge\": 90,\n\n\"PasswordReusePrevention\": 1\n\n}\n\n}\n\nTry to create a new user by using the AWS CLI with a password that violates the password policy. AWS will not allow you to create such a user:\n\naws iam create-user --user-name awscookbook103user2\n\nUse Secrets Manager to generate a password that does not adhere to your password policy:\n\nRANDOM_STRING2=$(aws secretsmanager get-random-password \\\n\n--password-length 16 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nCreate a login profile for the user that specifies the password:\n\naws iam create-login-profile --user-name awscookbook103user2 \\\n\n--password $RANDOM_STRING2\n\nThis command should fail and you should see output similar to:\n\nAn error occurred (PasswordPolicyViolation) when calling the CreateLoginProfile\n\noperation: Password should have a minimum length of 32\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nFor users logging in with passwords, AWS allows administrators to enforce password policies to their accounts that conform to the security requirements of your organization. This way, administrators can ensure that individual users don’t compromise the security of the organization by choosing weak passwords or by not regularly changing their passwords.\n\nTIP\n\nMulti-factor authentication is encouraged for IAM users. You can use a software-based virtual MFA device or a hardware\n\ndevice for a second factor on IAM users. AWS keeps an updated list of supported devices.\n\nMulti-factor authentication is a great way to add another layer of security on top of existing password-based security. It combines “what you know” and “what you have”; so, in cases where your password might be exposed to a malicious third-party actor, they would still need the additional factor to authenticate.\n\nChallenge\n\nDownload the credential report to analyze the IAM users and the password ages in your account.\n\n1.4 Testing IAM Policies with the IAM Policy Simulator\n\nProblem\n\nYou have an IAM policy that you would like to put into use but would like to test its effectiveness first.\n\nSolution\n\nAttach an IAM policy to an IAM role and simulate actions with the IAM Policy Simulator, as shown in Figure 1-6.\n\nFigure 1-6. Simulating IAM policies attached to an IAM role\n\nSteps\n\n1.\n\nCreate a file called assume-role-policy.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n2.\n\n3.\n\n\"Principal\": {\n\n\"Service\": \"ec2.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate an IAM role using the assume-role-policy.json file:\n\naws iam create-role --assume-role-policy-document \\\n\nfile://assume-role-policy.json --role-name AWSCookbook104IamRole\n\nYou should see output similar to the following:\n\n{\n\n\"Role\": {\n\n\"Path\": \"/\",\n\n\"RoleName\": \"AWSCookbook104IamRole\",\n\n\"RoleId\": \"<<UniqueID>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:role/AWSCookbook104IamRole\",\n\n\"CreateDate\": \"2021-09-22T23:37:44+00:00\",\n\n\"AssumeRolePolicyDocument\": {\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n...\n\nAttach the IAM managed policy for AmazonEC2ReadOnlyAccess to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook104IamRole \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess\n\nTIP\n\nYou can find a list of all the actions, resources, and condition keys for EC2 in this AWS article. The IAM global condition\n\ncontext keys are also useful in authoring fine-grained policies.\n\nValidation checks\n\nSimulate the effect of the IAM policy you are using, testing several different types of actions on the EC2 service.\n\nTest the ec2:CreateInternetGateway action:\n\naws iam simulate-principal-policy \\\n\n--policy-source-arn arn:aws:iam::$AWS_ACCOUNT_ARN:role/AWSCookbook104IamRole \\\n\n--action-names ec2:CreateInternetGateway\n\nYou should see output similar to the following (note the EvalDecision):\n\n{\n\n\"EvaluationResults\": [\n\n{\n\n\"EvalActionName\": \"ec2:CreateInternetGateway\",\n\n\"EvalResourceName\": \"*\",\n\n\"EvalDecision\": \"implicitDeny\",\n\n\"MatchedStatements\": [],\n\n\"MissingContextValues\": []\n\n}\n\n]\n\n}\n\nNOTE\n\nSince you attached only the AWS managed AmazonEC2ReadOnlyAccess IAM policy to the role in this recipe, you will see an implicit deny for the CreateInternetGateway action. This is expected behavior. AmazonEC2ReadOnlyAccess does not grant any “create” capabilities for the EC2 service.\n\nTest the ec2:DescribeInstances action:\n\naws iam simulate-principal-policy \\\n\n--policy-source-arn arn:aws:iam::$AWS_ACCOUNT_ARN:role/AWSCookbook104IamRole \\\n\n--action-names ec2:DescribeInstances\n\nYou should see output similar to the following:",
      "page_number": 19
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 37-45)",
      "start_page": 37,
      "end_page": 45,
      "detection_method": "topic_boundary",
      "content": "{\n\n\"EvaluationResults\": [\n\n{\n\n\"EvalActionName\": \"ec2:DescribeInstances\",\n\n\"EvalResourceName\": \"*\",\n\n\"EvalDecision\": \"allowed\",\n\n\"MatchedStatements\": [\n\n{\n\n\"SourcePolicyId\": \"AmazonEC2ReadOnlyAccess\",\n\n\"SourcePolicyType\": \"IAM Policy\",\n\n\"StartPosition\": {\n\n\"Line\": 3,\n\n\"Column\": 17\n\n},\n\n\"EndPosition\": {\n\n\"Line\": 8,\n\n\"Column\": 6\n\n}\n\n}\n\n],\n\n\"MissingContextValues\": []\n\n}\n\n]\n\n}\n\nNOTE\n\nThe AmazonEC2ReadOnlyAccess policy allows read operations on the EC2 service, so the DescribeInstances operation succeeds when you simulate this action.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nIAM policies let you define permissions for managing access in AWS. Policies can be attached to principals that allow you to grant (or deny) permissions to resources, users, groups and services. It is always best to scope your policies to the minimal set of permissions required as a security best practice. The IAM Policy Simulator can be extremely helpful when designing and managing your own IAM policies for least-privileged access.\n\nThe IAM Policy Simulator also exposes a web interface you can use to test and troubleshoot IAM policies and understand their net effect with the policy you define. You can test all the\n\npolicies or a subset of policies that you have attached to users, groups, and roles.\n\nTIP\n\nThe IAM Policy Simulator can help you simulate the effect of the following:\n\nIdentity-based policies\n\nIAM permissions boundaries\n\nAWS Organizations service control policies (SCPs)\n\nResource-based policies\n\nAfter you review the Policy Simulator results, you can add additional statements to your policies that either solve your issue (from a troubleshooting standpoint) or attach newly created policies to users, groups, and roles with the confidence that the net effect of the policy was what you intended.\n\nNOTE\n\nTo help you easily build IAM policies from scratch, AWS provides the AWS Policy Generator.\n\nChallenge\n\nSimulate the effect of a permissions boundary on an IAM principal (see Recipe 1.5).\n\n1.5 Delegating IAM Administrative Capabilities Using Permissions Boundaries\n\nProblem\n\nYou need to grant team members the ability to deploy Lambda functions and create IAM roles for them. You need to limit the effective permissions of the IAM roles created so that they allow only actions needed by the function.\n\nSolution\n\nCreate a permissions boundary policy, create an IAM role for Lambda developers, create an IAM policy that specifies the boundary policy, and attach the policy to the role you created.\n\nFigure 1-7 illustrates the effective permissions of the identity-based policy with the permissions boundary.\n\nFigure 1-7. Effective permissions of identity-based policy with permissions boundary\n\nPrerequisite\n\nSteps\n\n1.\n\n2.\n\n3.\n\nAn IAM user or federated identity for your AWS account with administrative privileges (follow the AWS guide for creating your first IAM admin user and user group).\n\nCreate a file named assume-role-policy-template.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"AWS\": \"PRINCIPAL_ARN\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nRetrieve the ARN for your user and set it as a variable:\n\nPRINCIPAL_ARN=$(aws sts get-caller-identity --query Arn --output text)\n\nUse the sed command to replace PRINCIPAL_ARN in the assume-role-policy- template.json file that we provided in the repository and generate the assume-role-policy.json file:\n\nsed -e \"s|PRINCIPAL_ARN|${PRINCIPAL_ARN}|g\" \\\n\nassume-role-policy-template.json > assume-role-policy.json\n\nNOTE\n\nFor the purposes of this recipe, you set the allowed IAM principal to your own user (User 1). To test\n\ndelegated access, you would set the IAM principal to something else.\n\n4.\n\n5.\n\nCreate a role and specify the assume role policy file:\n\nROLE_ARN=$(aws iam create-role --role-name AWSCookbook105Role \\\n\n--assume-role-policy-document file://assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\nCreate a permissions boundary JSON file named boundary-template.json with the following content. This allows specific DynamoDB, S3, and CloudWatch Logs actions (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"CreateLogGroup\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": \"logs:CreateLogGroup\",\n\n\"Resource\": \"arn:aws:logs:*:AWS_ACCOUNT_ID:*\"\n\n},\n\n{\n\n\"Sid\": \"CreateLogStreamandEvents\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"logs:CreateLogStream\",\n\n\"logs:PutLogEvents\"\n\n],\n\n\"Resource\": \"arn:aws:logs:*:AWS_ACCOUNT_ID:*\"\n\n},\n\n{\n\n\"Sid\": \"DynamoDBPermissions\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"dynamodb:PutItem\",\n\n\"dynamodb:UpdateItem\",\n\n\"dynamodb:DeleteItem\"\n\n],\n\n\"Resource\": \"arn:aws:dynamodb:*:AWS_ACCOUNT_ID:table/AWSCookbook*\"\n\n},\n\n{\n\n\"Sid\": \"S3Permissions\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"s3:GetObject\",\n\n6.\n\n7.\n\n8.\n\n\"s3:PutObject\"\n\n],\n\n\"Resource\": \"arn:aws:s3:::AWSCookbook*/*\"\n\n}\n\n]\n\n}\n\nUse the sed command to replace AWS_ACCOUNT_ID in the boundary-policy- template.json file and generate the boundary-policy.json file:\n\nsed -e \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\nboundary-policy-template.json > boundary-policy.json\n\nCreate the permissions boundary policy by using the AWS CLI:\n\naws iam create-policy --policy-name AWSCookbook105PB \\\n\n--policy-document file://boundary-policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Policy\": {\n\n\"PolicyName\": \"AWSCookbook105PB\",\n\n\"PolicyId\": \"EXAMPLE\",\n\n\"Arn\": \"arn:aws:iam::111111111111:policy/AWSCookbook105PB\",\n\n\"Path\": \"/\",\n\n\"DefaultVersionId\": \"v1\",\n\n\"AttachmentCount\": 0,\n\n\"PermissionsBoundaryUsageCount\": 0,\n\n\"IsAttachable\": true,\n\n\"CreateDate\": \"2021-09-24T00:36:53+00:00\",\n\n\"UpdateDate\": \"2021-09-24T00:36:53+00:00\"\n\n}\n\n}\n\nCreate a policy file named policy-template.json for the role (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"DenyPBDelete\",\n\n\"Effect\": \"Deny\",\n\n\"Action\": \"iam:DeleteRolePermissionsBoundary\",\n\n\"Resource\": \"*\"\n\n},\n\n{\n\n\"Sid\": \"IAMRead\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"iam:Get*\",\n\n\"iam:List*\"\n\n],\n\n\"Resource\": \"*\"\n\n},\n\n{\n\n\"Sid\": \"IAMPolicies\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"iam:CreatePolicy\",\n\n\"iam:DeletePolicy\",\n\n\"iam:CreatePolicyVersion\",\n\n\"iam:DeletePolicyVersion\",\n\n\"iam:SetDefaultPolicyVersion\"\n\n],\n\n\"Resource\": \"arn:aws:iam::AWS_ACCOUNT_ID:policy/AWSCookbook*\"\n\n},\n\n{\n\n\"Sid\": \"IAMRolesWithBoundary\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"iam:CreateRole\",\n\n\"iam:DeleteRole\",\n\n\"iam:PutRolePolicy\",\n\n\"iam:DeleteRolePolicy\",\n\n\"iam:AttachRolePolicy\",\n\n\"iam:DetachRolePolicy\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:iam::AWS_ACCOUNT_ID:role/AWSCookbook*\"\n\n],\n\n\"Condition\": {\n\n\"StringEquals\": {\n\n\"iam:PermissionsBoundary\": \"arn:aws:iam::AWS_ACCOUNT_ID:policy/AWSCookbook105PB\"\n\n}\n\n}\n\n},\n\n{\n\n\"Sid\": \"ServerlessFullAccess\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"lambda:*\",\n\n\"logs:*\",\n\n\"dynamodb:*\",\n\n\"s3:*\"\n\n],\n\n\"Resource\": \"*\"\n\n},\n\n{\n\n\"Sid\": \"PassRole\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": \"iam:PassRole\",\n\n\"Resource\": \"arn:aws:iam::AWS_ACCOUNT_ID:role/AWSCookbook*\",\n\n\"Condition\": {\n\n\"StringLikeIfExists\": {\n\n\"iam:PassedToService\": \"lambda.amazonaws.com\"\n\n}\n\n}\n\n},\n\n{\n\n\"Sid\": \"ProtectPB\",\n\n\"Effect\": \"Deny\",\n\n\"Action\": [\n\n\"iam:CreatePolicyVersion\",\n\n\"iam:DeletePolicy\",\n\n\"iam:DeletePolicyVersion\",\n\n\"iam:SetDefaultPolicyVersion\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:iam::AWS_ACCOUNT_ID:policy/AWSCookbook105PB\",\n\n\"arn:aws:iam::AWS_ACCOUNT_ID:policy/AWSCookbook105Policy\"\n\n]\n\n}\n\n]\n\n}\n\nThis custom IAM policy has several statements working together, which define certain permissions for the solution to the problem statement:\n\n9.\n\n10.\n\nDenyPBDelete: Explicitly deny the ability to delete permissions boundaries from roles.\n\nIAMRead: Allow read-only IAM access to developers to ensure that the IAM console works.\n\nIAMPolicies: Allow the creation of IAM policies but force a naming convention prefix AWSCookbook*.\n\nIAMRolesWithBoundary: Allow the creation and deletion of IAM roles only if they contain the permissions boundary referenced.\n\nServerlessFullAccess: Allow developers to have full access to the AWS Lambda, Amazon DynamoDB, Amazon CloudWatch logs, and Amazon S3 services.\n\nPassRole: Allow developers to pass IAM roles to Lambda functions.\n\nProtectPB: Explicitly deny the ability to modify the permissions boundary that bound the roles they create.\n\nUse the sed command to replace AWS_ACCOUNT_ID in the policy- template.json file and generate the policy.json file:\n\nsed -e \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\npolicy-template.json > policy.json\n\nCreate the policy for developer access:\n\naws iam create-policy --policy-name AWSCookbook105Policy \\\n\n--policy-document file://policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Policy\": {\n\n\"PolicyName\": \"AWSCookbook105Policy\",\n\n\"PolicyId\": \"EXAMPLE\",",
      "page_number": 37
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 46-58)",
      "start_page": 46,
      "end_page": 58,
      "detection_method": "topic_boundary",
      "content": "\"Arn\": \"arn:aws:iam::11111111111:policy/AWSCookbook105Policy\",\n\n\"Path\": \"/\",\n\n\"DefaultVersionId\": \"v1\",\n\n\"AttachmentCount\": 0,\n\n\"PermissionsBoundaryUsageCount\": 0,\n\n\"IsAttachable\": true,\n\n\"CreateDate\": \"2021-09-24T00:37:13+00:00\",\n\n\"UpdateDate\": \"2021-09-24T00:37:13+00:00\"\n\n}\n\n}\n\n11.\n\nAttach the policy to the role you created in step 2:\n\naws iam attach-role-policy --policy-arn \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook105Policy \\\n\n--role-name AWSCookbook105Role\n\nValidation checks\n\nAssume the role you created and set the output to local variables for the AWS CLI:\n\ncreds=$(aws --output text sts assume-role --role-arn $ROLE_ARN \\\n\n--role-session-name \"AWSCookbook105\" | \\\n\ngrep CREDENTIALS | cut -d \" \" -f2,4,5)\n\nexport AWS_ACCESS_KEY_ID=$(echo $creds | cut -d \" \" -f2)\n\nexport AWS_SECRET_ACCESS_KEY=$(echo $creds | cut -d \" \" -f4)\n\nexport AWS_SESSION_TOKEN=$(echo $creds | cut -d \" \" -f5)\n\nTry to create an IAM role for a Lambda function, create an assume role policy for the Lambda service (lambda-assume-role-policy.json):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"lambda.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate the role, specifying the permissions boundary, which conforms to the role-naming standard specified in the policy:\n\nTEST_ROLE_1=$(aws iam create-role --role-name AWSCookbook105test1 \\\n\n--assume-role-policy-document \\\n\nfile://lambda-assume-role-policy.json \\\n\n--permissions-boundary \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook105PB \\\n\n--output text --query Role.Arn)\n\nAttach the managed AmazonDynamoDBFullAccess policy to the role:\n\naws iam attach-role-policy --role-name AWSCookbook105test1 \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\n\nAttach the managed CloudWatchFullAccess policy to the role:\n\naws iam attach-role-policy --role-name AWSCookbook105test1 \\\n\n--policy-arn arn:aws:iam::aws:policy/CloudWatchFullAccess\n\nNOTE\n\nEven though you attached AmazonDynamoDBFullAccess and CloudWatchFullAccess to the role, the effective permissions of the role are limited by the statements in the permissions boundary you created in step 3. Furthermore, even though you\n\nhave s3:GetObject and s3:PutObject defined in the boundary, you have not defined these in the role policy, so the function will not be able to make these calls until you create a policy that allows these actions. When you attach this role\n\nto a Lambda function, the Lambda function can perform only the actions allowed in the intersection of the permissions\n\nboundary and the role policy (see Figure 1-7).\n\nYou can now create a Lambda function specifying this role (AWSCookbook105test1) as the execution role to validate the DynamoDB and CloudWatch Logs permissions granted to the function. You can also test the results with the IAM Policy Simulator.\n\nYou used an AssumeRole and set environment variables to override your local terminal AWS profile to perform these validation checks. To ensure that you revert back to your original authenticated session on the command line, perform the perform the cleanup steps provided at the top of the README file in the repository.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nNOTE\n\nBe sure to delete your environment variables so that you can regain permissions needed for future recipes:\n\nunset AWS_ACCESS_KEY_ID\n\nunset AWS_SECRET_ACCESS_KEY\n\nunset AWS_SESSION_TOKEN\n\nDiscussion\n\nIn your quest to implement a least privilege access model for users and applications within AWS, you need to enable developers to create IAM roles that their applications can assume when they need to interact with other AWS services. For example, an AWS Lambda function that needs to access an Amazon DynamoDB table would need a role created to be able to perform operations against the table. As your team scales, instead of your team members coming to you every time they need a role created for a specific purpose, you can enable (but control) them with permissions boundaries, without giving up too much IAM access. The iam:PermissionsBoundary condition in the policy that grants the iam:CreateRole ensures that the roles created must always include the permissions boundary attached.\n\nPermissions boundaries act as a guardrail and limit privilege escalation. In other words, they limit the maximum effective permissions of an IAM principal created by a delegated administrator by defining what the roles created can do. As shown in Figure 1-7, they work in conjunction with the permissions policy (IAM policy) that is attached to an IAM principal (IAM user or role). This prevents the need to grant wide access to an administrator role, prevents privilege escalation, and helps you achieve least privilege access by allowing your team members to quickly iterate and create their own least-privileged roles for their applications.\n\nIn this recipe, you may have noticed that we used a naming convention of AWSCookbook* on the roles and policies referenced in the permissions boundary policy, which ensures the delegated principals can create roles and policies within this convention. This means that developers can create resources, pass only these roles to services, and also keep a standard naming convention. This is an ideal practice when implementing permissions boundaries. You can develop a naming convention for different teams, applications, and services so that these can all coexist within the\n\nsame account, yet have different boundaries applied to them based on their requirements, if necessary.\n\nAt minimum, you need to keep these four things in mind when building roles that implement permissions boundary guardrails to delegate IAM permissions to nonadministrators:\n\n1.\n\nAllow the creation of IAM customer managed policies: your users can create any policy they wish; they do not have an effect until they are attached to an IAM principal.\n\n2.\n\nAllow IAM role creation with a condition that a permissions boundary must be attached: force all roles created by your team members to include the permission boundary in the role creation.\n\n3.\n\nAllow attachment of policies, but only to roles that have a permissions boundary: do not let users modify existing roles that they may have access to.\n\n4.\n\nAllow iam:PassRole to AWS services that your users create roles for: your developers may need to create roles for Amazon EC2 and AWS Lambda, so give them the ability to pass only the roles they create to those services you define.\n\nTIP\n\nPermissions boundaries are a powerful, advanced IAM concept that can be challenging to understand. We recommend\n\nchecking out the talk by Brigid Johnson at AWS re:Inforce 2018 to see some real-world examples of IAM policies, roles,\n\nand permissions boundaries explained in a practical way.\n\nChallenge\n\nExtend the permissions boundary to allow roles created to publish to an SQS queue and SNS topic and adjust the policy for the role as well.\n\n1.6 Connecting to EC2 Instances Using AWS SSM Session Manager\n\nProblem\n\nYou have an EC2 instance in a private subnet and need to connect to the instance without using SSH over the internet.\n\nSolution\n\nCreate an IAM role, attach the AmazonSSMManagedInstanceCore policy, create an EC2 instance profile, attach the IAM role you created to the instance profile, associate the EC2 instance profile to an EC2 instance, and finally, run the aws ssm start-session command to connect to the instance. A logical flow of these steps is shown in Figure 1-8.\n\nFigure 1-8. Using Session Manager to connect to an EC2 instance\n\nPrerequisites\n\nAmazon Virtual Private Cloud (VPC) with isolated or private subnets and associated route tables\n\nRequired VPC endpoints for AWS Systems Manager\n\nAWS CLI v2 with the Session Manager plugin installed\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file named assume-role-policy.json with the following content (file provided in the repository):\n\n2.\n\n3.\n\n4.\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"ec2.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate an IAM role with the statement in the provided assume-role- policy.json file using this command:\n\nROLE_ARN=$(aws iam create-role --role-name AWSCookbook106SSMRole \\\n\n--assume-role-policy-document file://assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\nAttach the AmazonSSMManagedInstanceCore managed policy to the role so that the role allows access to AWS Systems Manager:\n\naws iam attach-role-policy --role-name AWSCookbook106SSMRole \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\n\nCreate an instance profile:\n\naws iam create-instance-profile \\\n\n--instance-profile-name AWSCookbook106InstanceProfile\n\nYou should see output similar to the following:\n\n{\n\n\"InstanceProfile\": {\n\n\"Path\": \"/\",\n\n5.\n\n6.\n\n7.\n\n\"InstanceProfileName\": \"AWSCookbook106InstanceProfile\",\n\n\"InstanceProfileId\": \"(RandomString\",\n\n\"Arn\": \"arn:aws:iam::111111111111:instance-\n\nprofile/AWSCookbook106InstanceProfile\",\n\n\"CreateDate\": \"2021-11-28T20:26:23+00:00\",\n\n\"Roles\": []\n\n}\n\n}\n\nAdd the role that you created to the instance profile:\n\naws iam add-role-to-instance-profile \\\n\n--role-name AWSCookbook106SSMRole \\\n\n--instance-profile-name AWSCookbook106InstanceProfile\n\nNOTE\n\nThe EC2 instance profile contains a role that you create. The instance profile association with an\n\ninstance allows it to define “who I am,” and the role defines “what I am permitted to do.” Both are\n\nrequired by IAM to allow an EC2 instance to communicate with other AWS services using the IAM\n\nservice. You can get a list of instance profiles in your account by running the aws iam list- instance-profiles AWS CLI command.\n\nQuery SSM for the latest Amazon Linux 2 AMI ID available in your Region and save it as an environment variable:\n\nAMI_ID=$(aws ssm get-parameters --names \\\n\n/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 \\\n\n--query 'Parameters[0].[Value]' --output text)\n\nLaunch an instance in one of your subnets that references the instance profile you created and also uses a Name tag that helps you identify the instance in the console:\n\nINSTANCE_ID=$(aws ec2 run-instances --image-id $AMI_ID \\\n\n--count 1 \\\n\n--instance-type t3.nano \\\n\n--iam-instance-profile Name=AWSCookbook106InstanceProfile \\\n\n--subnet-id $SUBNET_1 \\\n\n--security-group-ids $INSTANCE_SG \\\n\n--metadata-options \\\n\nHttpTokens=required,HttpPutResponseHopLimit=64,HttpEndpoint=enabled \\\n\n--tag-specifications \\\n\n'ResourceType=instance,Tags=[{Key=Name,Value=AWSCookbook106}]' \\\n\n'ResourceType=volume,Tags=[{Key=Name,Value=AWSCookbook106}]' \\\n\n--query Instances[0].InstanceId \\\n\n--output text)\n\nTIP\n\nEC2 instance metadata is a feature you can use within your EC2 instance to access information about\n\nyour EC2 instance over an HTTP endpoint from the instance itself. This is helpful for scripting and\n\nautomation via user data. You should always use the latest version of instance metadata. In step 7, you\n\ndid this by specifying the --metadata-options flag and providing the HttpTokens=required option that forces IMDSv2.\n\nValidation checks\n\nEnsure your EC2 instance has registered with SSM. Use the following command to check the status. This command should return the instance ID:\n\naws ssm describe-instance-information \\\n\n--filters Key=ResourceType,Values=EC2Instance \\\n\n--query \"InstanceInformationList[].InstanceId\" --output text\n\nConnect to the EC2 instance by using SSM Session Manager:\n\naws ssm start-session --target $INSTANCE_ID\n\nYou should now be connected to your instance and see a bash prompt. From the bash prompt, run a command to validate you are connected to your EC2 instance by querying the metadata service for an IMDSv2 token and using the token to query metadata for the instance profile associated with the instance:\n\nTOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds:\n\n21600\"`\n\ncurl -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/iam/info\n\nYou should see output similar to the following:\n\n{\n\n\"Code\" : \"Success\",\n\n\"LastUpdated\" : \"2021-09-23T16:03:25Z\",\n\n\"InstanceProfileArn\" : \"arn:aws:iam::111111111111:instance-profile/AWSCookbook106InstanceProfile\",\n\n\"InstanceProfileId\" : \"AIPAZVTINAMEXAMPLE\"\n\n}\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen you use AWS SSM Session Manager to connect to EC2 instances, you eliminate your dependency on Secure Shell (SSH) over the internet for command-line access to your instances. Once you configure Session Manager for your instances, you can instantly connect to a bash shell session on Linux or a PowerShell session for Windows systems.\n\nWARNING\n\nSSM can log all commands and their output during a session. You can set a preference to stop the logging of sensitive data\n\n(e.g., passwords) with this command:\n\nstty -echo; read passwd; stty echo;\n\nThere is more information in an AWS article about logging session activity.\n\nSession Manager works by communicating with the AWS Systems Manager (SSM) API endpoints within the AWS Region you are using over HTTPS (TCP port 443). The agent on your instance registers with the SSM service at boot time. No inbound security group rules are needed for Session Manager functionality. We recommend configuring VPC Endpoints for Session Manager to avoid the need for internet traffic and the cost of Network Address Translation (NAT) gateways.\n\nHere are some examples of the increased security posture Session Manager provides:\n\nNo internet-facing TCP ports need to be allowed in security groups associated with instances.\n\nYou can run instances in private (or isolated) subnets without exposing them directly to the internet and still access them for management duties.\n\nThere is no need to create, associate, and manage SSH keys with instances.\n\nThere is no need to manage user accounts and passwords on instances.\n\nYou can delegate access to manage EC2 instances using IAM roles.\n\nNOTE\n\nAny tool like SSM that provides such powerful capabilities must be carefully audited. AWS provides information about\n\nlocking down permissions for the SSM user, and more information about auditing session activity.\n\nChallenge\n\nView the logs for a session and create an alert whenever the rm command is executed.\n\n1.7 Encrypting EBS Volumes Using KMS Keys\n\nProblem\n\nYou need an encryption key for encrypting EBS volumes attached to your EC2 instances in a Region, and you need to rotate the key automatically every 365 days.\n\nSolution\n\nCreate a customer-managed KMS key (CMK), enable yearly rotation of the key, enable EC2 default encryption for EBS volumes in a Region, and specify the KMS key you created (shown in Figure 1-9).\n\nFigure 1-9. Create a customer-managed key, enable rotation, and set default encryption for EC2 using a customer-\n\nmanaged key\n\nSteps\n\n1.\n\nCreate a customer-managed KMS key and store the key ARN as a local variable:\n\nKMS_KEY_ID=$(aws kms create-key --description \"AWSCookbook107Key\" \\\n\n--output text --query KeyMetadata.KeyId)\n\n2.\n\nCreate a key alias to help you refer to the key in other steps:\n\naws kms create-alias --alias-name alias/AWSCookbook107Key \\\n\n--target-key-id $KMS_KEY_ID\n\n3.\n\nEnable automated rotation of the symmetric key material every 365 days:\n\naws kms enable-key-rotation --key-id $KMS_KEY_ID\n\n4.\n\nEnable EBS encryption by default for the EC2 service within your current Region:\n\naws ec2 enable-ebs-encryption-by-default\n\nYou should see output similar to the following:\n\n{\n\n\"EbsEncryptionByDefault\": true\n\n}\n\n5.\n\nUpdate the default KMS key used for default EBS encryption to your customer-managed key that you created in step 1:\n\naws ec2 modify-ebs-default-kms-key-id \\\n\n--kms-key-id alias/AWSCookbook107Key\n\nYou should see output similar to the following:\n\n{\n\n\"KmsKeyId\": \"arn:aws:kms:us-east-1:111111111111:key/1111111-aaaa-bbbb-222222222\"\n\n}\n\nValidation checks\n\nUse the AWS CLI to retrieve the default EBS encryption status for the EC2 service:\n\naws ec2 get-ebs-encryption-by-default\n\nYou should see output similar to the following:\n\n{\n\n\"EbsEncryptionByDefault\": true\n\n}\n\nRetrieve the KMS key ID used for default encryption:\n\naws ec2 get-ebs-default-kms-key-id\n\nYou should see output similar to the following:\n\n{\n\n\"KmsKeyId\": \"arn:aws:kms:us-east-1:1111111111:key/1111111-aaaa-3333-222222222c64b\"\n\n}\n\nCheck the automatic rotation status of the key you created:\n\naws kms get-key-rotation-status --key-id $KMS_KEY_ID\n\nYou should see output similar to the following:\n\n{\n\n\"KeyRotationEnabled\": true\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion",
      "page_number": 46
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 59-66)",
      "start_page": 59,
      "end_page": 66,
      "detection_method": "topic_boundary",
      "content": "When you are faced with the challenge of ensuring that all of your newly created EBS volumes are encrypted, the ebs-encryption-by-default option comes to the rescue. With this setting enabled, every EC2 instance you launch will by default have its EBS volumes encrypted with the specified KMS key. If you do not specify a KMS key, a default AWS-managed aws/ebs KMS key is created and used. If you need to manage the lifecycle of the key or have a requirement specifying that you or your organization must manage the key, customer-managed keys should be used.\n\nAutomatic key rotation on the KMS service simplifies your approach to key rotation and key lifecycle management.\n\nKMS is a flexible service you can use to implement a variety of data encryption strategies. It supports key policies that you can use to control who has access to the key. These key policies layer on top of your existing IAM policy strategy for added security. You can use KMS keys to encrypt many different types of data at rest within your AWS account, for example:\n\nAmazon S3\n\nAmazon EC2 EBS volumes\n\nAmazon RDS databases and clusters\n\nAmazon DynamoDB tables\n\nAmazon EFS volumes\n\nAmazon FSx file shares\n\nAnd many more\n\nChallenge 1\n\nChange the key policy on the KMS key to allow access to only your IAM principal and the EC2 service.\n\nChallenge 2\n\nCreate an EBS volume and verify that it is encrypted by using the aws ec2 describe-volumes command.\n\n1.8 Storing, Encrypting, and Accessing Passwords Using Secrets Manager\n\nProblem\n\nYou need to give your EC2 instance the ability to securely store and retrieve a database password for your application.\n\nSolution\n\nCreate a password, store the password in Secrets Manager, create an IAM Policy with access to the secret, and grant an EC2 instance profile access to the secret, as shown in Figure 1-10.\n\nFigure 1-10. Create a secret and retrieve it via the EC2 instance\n\nPrerequisites\n\nVPC with isolated subnets and associated route tables.\n\nEC2 instance deployed. You will need the ability to connect to this for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a secret using the AWS CLI:\n\nRANDOM_STRING=$(aws secretsmanager get-random-password \\\n\n--password-length 32 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\n2.\n\nStore it as a new secret in Secrets Manager:\n\nSECRET_ARN=$(aws secretsmanager \\\n\ncreate-secret --name AWSCookbook108/Secret1 \\\n\n--description \"AWSCookbook108 Secret 1\" \\\n\n3.\n\n4.\n\n5.\n\n--secret-string $RANDOM_STRING \\\n\n--output text \\\n\n--query ARN)\n\nCreate a file called secret-access-policy-template.json that references the secret you created. (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"secretsmanager:GetResourcePolicy\",\n\n\"secretsmanager:GetSecretValue\",\n\n\"secretsmanager:DescribeSecret\",\n\n\"secretsmanager:ListSecretVersionIds\"\n\n],\n\n\"Resource\": [\n\n\"SECRET_ARN\"\n\n]\n\n},\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": \"secretsmanager:ListSecrets\",\n\n\"Resource\": \"*\"\n\n}\n\n]\n\n}\n\nUse the sed command to replace SECRET_ARN in the secret-access-policy- template.json file and generate the secret-access-policy.json file:\n\nsed -e \"s|SECRET_ARN|$SECRET_ARN|g\" \\\n\nsecret-access-policy-template.json > secret-access-policy.json\n\nCreate the IAM policy for secret access:\n\naws iam create-policy --policy-name AWSCookbook108SecretAccess \\\n\n--policy-document file://secret-access-policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Policy\": {\n\n\"PolicyName\": \"AWSCookbook108SecretAccess\",\n\n\"PolicyId\": \"(Random String)\",\n\n\"Arn\": \"arn:aws:iam::1111111111:policy/AWSCookbook108SecretAccess\",\n\n\"Path\": \"/\",\n\n\"DefaultVersionId\": \"v1\",\n\n\"AttachmentCount\": 0,\n\n\"PermissionsBoundaryUsageCount\": 0,\n\n\"IsAttachable\": true,\n\n\"CreateDate\": \"2021-11-28T21:25:23+00:00\",\n\n\"UpdateDate\": \"2021-11-28T21:25:23+00:00\"\n\n}\n\n}\n\n6.\n\nGrant an EC2 instance ability to access the secret by adding the IAM policy you created to the EC2 instance profile’s currently attached IAM role:\n\naws iam attach-role-policy --policy-arn \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook108SecretAccess \\\n\n--role-name $ROLE_NAME\n\nValidation checks\n\nConnect to the EC2 instance:\n\naws ssm start-session --target $INSTANCE_ID\n\nSet and export your default region:\n\nexport AWS_DEFAULT_REGION=us-east-1\n\nRetrieve the secret from Secrets Manager from the EC2:\n\naws secretsmanager get-secret-value --secret-id AWSCookbook108/Secret1\n\nYou should see output similar to the following:\n\n{\n\n\"Name\": \"AWSCookbook108/Secret1\",\n\n\"VersionId\": \"<string>\",\n\n\"SecretString\": \"<secret value>\",\n\n\"VersionStages\": [\n\n\"AWSCURRENT\"\n\n],\n\n\"CreatedDate\": 1638221015.646,\n\n\"ARN\": \"arn:aws:secretsmanager:us-east-1:111111111111:secret:AWSCookbook108/Secret1-<suffix>\"\n\n}</suffix>\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nSecurely creating, storing, and managing the lifecycle of secrets, like API keys and database passwords, is a fundamental component to a strong security posture in the cloud. You can use Secrets Manager to implement a secrets management strategy that supports your security strategy. You can control who has access to what secrets using IAM policies to ensure the secrets you manage are accessible by only the necessary security principals.\n\nSince your EC2 instance uses an instance profile, you do not need to store any hard-coded credentials on the instance in order for it to access the secret. The access is granted via the IAM policy attached to the instance profile. Each time you (or your application) access the secret from the EC2 instance, temporary session credentials are obtained from the STS service to allow the get-secret-value API call to retrieve the secret. The AWS CLI automates this process of token retrieval when an EC2 instance profile is attached to your instance. You can also use the AWS SDK within your applications to achieve this functionality.\n\nSome additional benefits to using Secrets Manager include the following:\n\nEncrypting secrets with KMS keys that you create and manage\n\nAuditing access to secrets through CloudTrail\n\nAutomating secret rotation using Lambda\n\nGranting access to other users, roles, and services like EC2 and Lambda\n\nReplicating secrets to another Region for high availability and disaster recovery purposes\n\nChallenge\n\nConfigure a Lambda function to access the secret securely with an IAM role.\n\n1.9 Blocking Public Access for an S3 Bucket\n\nProblem\n\nYou have been alerted by your organization’s security team that an S3 bucket has been incorrectly configured and you need to block public access to it.\n\nSolution\n\nApply the Amazon S3 Block Public Access feature to your bucket, and then check the status with the Access Analyzer (see Figure 1-11).\n\nTIP\n\nAWS provides information on what is considered “public” in an article on S3 storage.\n\nFigure 1-11. Blocking public access to an S3 bucket\n\nPrerequisite\n\nS3 bucket with publicly available object(s)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate an Access Analyzer to use for validation of access:\n\n2.\n\n3.\n\nANALYZER_ARN=$(aws accessanalyzer create-analyzer \\\n\n--analyzer-name awscookbook109\\\n\n--type ACCOUNT \\\n\n--output text --query arn)\n\nPerform a scan of your S3 bucket with the Access Analyzer:\n\naws accessanalyzer start-resource-scan \\\n\n--analyzer-arn $ANALYZER_ARN \\\n\n--resource-arn arn:aws:s3:::awscookbook109-$RANDOM_STRING\n\nGet the results of the Access Analyzer scan (it may take about 30 seconds for the scan results to become available):\n\naws accessanalyzer get-analyzed-resource \\\n\n--analyzer-arn $ANALYZER_ARN \\\n\n--resource-arn arn:aws:s3:::awscookbook109-$RANDOM_STRING\n\nYou should see output similar to the following (note the isPublic value):\n\n{\n\n\"resource\": {\n\n\"actions\": [\n\n\"s3:GetObject\",\n\n\"s3:GetObjectVersion\"\n\n],\n\n\"analyzedAt\": \"2021-06-26T17:42:00.861000+00:00\",\n\n\"createdAt\": \"2021-06-26T17:42:00.861000+00:00\",\n\n\"isPublic\": true,\n\n\"resourceArn\": \"arn:aws:s3:::awscookbook109-<<string>>\",\n\n\"resourceOwnerAccount\": \"111111111111\",\n\n\"resourceType\": \"AWS::S3::Bucket\",\n\n\"sharedVia\": [\n\n\"POLICY\"\n\n],\n\n\"status\": \"ACTIVE\",\n\n\"updatedAt\": \"2021-06-26T17:42:00.861000+00:00\"\n\n}\n\n}",
      "page_number": 59
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 67-75)",
      "start_page": 67,
      "end_page": 75,
      "detection_method": "topic_boundary",
      "content": "4.\n\nSet the public access block for your bucket:\n\naws s3api put-public-access-block \\\n\n--bucket awscookbook109-$RANDOM_STRING \\\n\n--public-access-block-configuration \\\n\n\"BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPubli\n\ncBuckets=true\"\n\nNOTE\n\nSee the AWS article on the available PublicAccessBlock configuration properties.\n\nValidation checks\n\nPerform a scan of your S3 bucket:\n\naws accessanalyzer start-resource-scan \\\n\n--analyzer-arn $ANALYZER_ARN \\\n\n--resource-arn arn:aws:s3:::awscookbook109-$RANDOM_STRING\n\nGet the results of the Access Analyzer scan:\n\naws accessanalyzer get-analyzed-resource \\\n\n--analyzer-arn $ANALYZER_ARN \\\n\n--resource-arn arn:aws:s3:::awscookbook109-$RANDOM_STRING\n\nYou should see output similar to the following:\n\n{\n\n\"resource\": {\n\n\"analyzedAt\": \"2021-06-26T17:46:24.906000+00:00\",\n\n\"isPublic\": false,\n\n\"resourceArn\": \"arn:aws:s3:::awscookbook109-<<string>>\",\n\n\"resourceOwnerAccount\": \"111111111111\",\n\n\"resourceType\": \"AWS::S3::Bucket\"\n\n}\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nOne of the best things you can do to ensure data security in your AWS account is to always make certain that you apply the right security controls to your data. If you mark an object as public in your S3 bucket, it is accessible to anyone on the internet, since S3 serves objects using HTTP. One of the most common security misconfigurations that users make in the cloud is marking object(s) as public when that is not intended or required. To protect against misconfiguration of S3 objects, enabling BlockPublicAccess for your buckets is a great thing to do from a security standpoint.\n\nTIP\n\nYou can also set public block settings at your account level, which would include all S3 buckets in your account:\n\naws s3control put-public-access-block \\\n\n--public-access-block-configuration \\\n\nBlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true \\\n\n--account-id $AWS_ACCOUNT_ID\n\nYou can serve S3 content to internet users via HTTP and HTTPS while keeping your bucket private. Content delivery networking (CDN), like Amazon CloudFront, provides more secure, efficient, and cost-effective ways to achieve global static website hosting and still use S3 as your object source. To see an example of a CloudFront configuration that serves static content from an S3 bucket, see Recipe 1.10.\n\nChallenge\n\nDeploy a VPC endpoint for S3 within your VPC and create a bucket policy to restrict access to your S3 bucket through this endpoint only.\n\n1.10 Serving Web Content Securely from S3 with CloudFront\n\nProblem\n\nYou have nonpublic web content in S3 and want to configure CloudFront to serve the content.\n\nSolution\n\nCreate a CloudFront distribution and set the origin to your S3 bucket. Then configure an origin access identity (OAI) to require the bucket to be accessible only from CloudFront (see Figure 1- 12).\n\nFigure 1-12. CloudFront and S3\n\nPrerequisite\n\nS3 bucket with static web content\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a CloudFront OAI to reference in an S3 bucket policy:\n\nOAI=$(aws cloudfront create-cloud-front-origin-access-identity \\\n\n--cloud-front-origin-access-identity-config \\\n\nCallerReference=\"awscookbook\",Comment=\"AWSCookbook OAI\" \\\n\n--query CloudFrontOriginAccessIdentity.Id --output text)\n\n2.\n\nUse the sed command to replace the values in the distribution-config- template.json file with your CloudFront OAI and S3 bucket name:\n\nsed -e \"s/CLOUDFRONT_OAI/${OAI}/g\" \\\n\n3.\n\n4.\n\n5.\n\n6.\n\ne \"s|S3_BUCKET_NAME|awscookbook110-$RANDOM_STRING|g\" \\\n\ndistribution-template.json > distribution.json\n\nCreate a CloudFront distribution that uses the distribution configuration JSON file you just created:\n\nDISTRIBUTION_ID=$(aws cloudfront create-distribution \\\n\n--distribution-config file://distribution.json \\\n\n--query Distribution.Id --output text)\n\nThe distribution will take a few minutes to create; use this command to check the status. Wait until the status reaches “Deployed”:\n\naws cloudfront get-distribution --id $DISTRIBUTION_ID \\\n\n--output text --query Distribution.Status\n\nConfigure the S3 bucket policy to allow only requests from CloudFront by using a bucket policy like this (we have provided bucket-policy- template.json in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Id\": \"PolicyForCloudFrontPrivateContent\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity\n\nCLOUDFRONT_OAI\"\n\n},\n\n\"Action\": \"s3:GetObject\",\n\n\"Resource\": \"arn:aws:s3:::S3_BUCKET_NAME/*\"\n\n}\n\n]\n\n}\n\nUse the sed command to replace the values in the bucket-policy- template.json file with the CloudFront OAI and S3 bucket name:\n\nsed -e \"s/CLOUDFRONT_OAI/${OAI}/g\" \\\n\ne \"s|S3_BUCKET_NAME|awscookbook110-$RANDOM_STRING|g\" \\\n\nbucket-policy-template.json > bucket-policy.json\n\n7.\n\nApply the bucket policy to the S3 bucket with your static web content:\n\naws s3api put-bucket-policy --bucket awscookbook110-$RANDOM_STRING \\\n\n--policy file://bucket-policy.json\n\n8.\n\nGet the DOMAIN_NAME of the distribution you created:\n\nDOMAIN_NAME=$(aws cloudfront get-distribution --id $DISTRIBUTION_ID \\\n\n--query Distribution.DomainName --output text)\n\nValidation checks\n\nTry to access the S3 bucket directly using HTTPS to verify that the bucket does not serve content directly:\n\ncurl https://awscookbook110-$RANDOM_STRING.s3.$AWS_REGION.amazonaws.com/index.html\n\nYou should see output similar to the following:\n\n$ curl https://awscookbook110-$RANDOM_STRING.s3.$AWS_REGION.amazonaws.com/index.html\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<Error><Code>AccessDenied</Code><Message>Access\n\nDenied</Message><RequestId>0AKQD0EFJC9ZHPCC</RequestId>\n\n<HostId>gfld4qKp9A93G8ee7VPBFrXBZV1HE3jiOb3bNB54fP\n\nEPTihit/OyFh7hF2Nu4+Muv6JEc0ebLL4=</HostId></Error>\n\n110-Optimizing-S3-with-CloudFront:$\n\nUse curl to observe that your index.html file is served from the private S3 bucket through CloudFront:\n\ncurl $DOMAIN_NAME\n\nYou should see output similar to the following:\n\n$ curl $DOMAIN_NAME\n\nAWSCookbook\n\n$\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThis configuration allows you to keep the S3 bucket private and allows only the CloudFront distribution to be able to access objects in the bucket. You created an origin access identity and defined a bucket policy to allow only CloudFront access to your S3 content. This gives you a solid foundation to keep your S3 buckets secure with the additional protection of the CloudFront global CDN.\n\nThe protection that a CDN gives from a distributed-denial-of-service (DDoS) attack is worth noting, as the end user requests to your content are directed to a point of presence on the CloudFront network with the lowest latency. This also protects you from the costs of having a DDoS attack against static content hosted in an S3 bucket, as it is generally less expensive to serve requests out of CloudFront rather than S3 directly.\n\nBy default, CloudFront comes with an HTTPS certificate on the default hostname for your distribution that you use to secure traffic. With CloudFront, you can associate your own custom domain name, attach a custom certificate from Amazon Certificate Manager (ACM), redirect to HTTPS from HTTP, force HTTPS, customize cache behavior, invoke Lambda functions (Lambda @Edge), and more.\n\nChallenge\n\nAdd a geo restriction to your CloudFront distribution.\n\nChapter 2. Networking\n\n2.0 Introduction Many exciting topics, like computer vision, Internet of Things (IoT), and AI-enabled chat bots, dominate headlines. This causes traditional core technologies to be forgotten. While it’s great to have many new capabilities at your fingertips, it is important to realize that these technologies would not be possible without a strong foundation of reliable and secure connectivity. Data processing is useful only if the results are reliably delivered and accessible over a network. Containers are a fantastic application deployment method, but they provide the best experience for users when they are networked together.\n\nNetworking services and features within AWS are the backbone to almost all of the big services we cover in this book. AWS has many great features for you to connect what you want, where you want, and how you want. Gaining a better understanding of networking will allow you to have a better grasp of the cloud and therefore to be more comfortable using it.\n\nTIP\n\nKeeping up with new networking innovations at AWS requires continuous learning. Each year at AWS re:Invent, many\n\nnetwork services, features, and approaches are discussed.\n\nTwo suggested viewings of great networking talks from AWS re:Invent are Eric Brandwine’s “Another Day, Another\n\nBillion Packets” from 2015 and the annual “From One to Many: Evolving VPC Design” from 2019.\n\nIn this chapter, you will learn about essential cloud networking services and features. We will focus only on recipes that are realistic for you to accomplish in your personal account. Some advanced operations (e.g., AWS Direct Connect setup) are too dependent on external factors, so we left them out in order to focus on more easily accessible recipes and outcomes. While some recipes in this chapter may seem simple or short, they allow us to discuss important topics and concepts that are crucial to get right.\n\nWorkstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Networking\n\n2.1 Defining Your Private Virtual Network in the Cloud by Creating an Amazon VPC\n\nProblem\n\nYou need a network foundation to host cloud resources.\n\nSolution\n\nYou will create an Amazon Virtual Private Cloud (VPC) and configure a Classless Inter-Domain Routing (CIDR) block for it, as shown in Figure 2-1.\n\nFigure 2-1. VPC deployed in a Region\n\nSteps\n\n1.\n\nCreate a VPC with an IPv4 CIDR block. We will use 10.10.0.0/16 as the address range, but you can modify it based on your needs:\n\nVPC_ID=$(aws ec2 create-vpc --cidr-block 10.10.0.0/16 \\\n\n--tag-specifications\n\n'ResourceType=vpc,Tags=[{Key=Name,Value=AWSCookbook201}]' \\\n\n--output text --query Vpc.VpcId)\n\nNOTE\n\nWhen you are creating a VPC, the documentation states that the largest block size for VPC IPv4\n\nCIDRs is a /16 netmask (65,536 IP addresses). The smallest is a /28 netmask (16 IP addresses).\n\nValidation checks\n\nUse this command to verify that the VPC’s state is available:\n\naws ec2 describe-vpcs --vpc-ids $VPC_ID\n\nYou should see output similar to the following:\n\n{\n\n\"Vpcs\": [\n\n{\n\n\"CidrBlock\": \"10.10.0.0/16\",\n\n\"DhcpOptionsId\": \"dopt-<<snip>>\",\n\n\"State\": \"available\",\n\n\"VpcId\": \"vpc-<<snip>>\",\n\n\"OwnerId\": \"111111111111\",\n\n\"InstanceTenancy\": \"default\",\n\n\"CidrBlockAssociationSet\": [\n\n{\n\n\"AssociationId\": \"vpc-cidr-assoc-<<snip>>\",\n\n\"CidrBlock\": \"10.10.0.0/16\",\n\n\"CidrBlockState\": {\n\n\"State\": \"associated\"\n\n}\n\n}\n\n],\n\n\"IsDefault\": false,\n\n<<snip>>\n\n...",
      "page_number": 67
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 76-84)",
      "start_page": 76,
      "end_page": 84,
      "detection_method": "topic_boundary",
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWARNING\n\nHere are two important reasons for carefully selecting CIDR block(s) for your VPC:\n\nOnce a CIDR block is associated with a VPC, it can’t be modified (although it can be extended). If you wish\n\nto modify a CIDR block, it (and all resources within it) will need to be deleted and re-created.\n\nIf a VPC is connected to other networks by peering (see Recipe 2.11) or gateways (e.g., Transit and VPN),\n\nhaving overlapping IP ranges will cause unwanted problems.\n\nYou can add IPv4 space to the VPC by using the aws ec2 associate-vpc-cidr-block command to specify the additional IPv4 space. When IP space becomes scarce from usage and under-provisioning, it’s good to know that you don’t need to dedicate a large block to a VPC, especially if you aren’t sure if all of it will be utilized.\n\nHere is an example of associating an additional IPv4 CIDR block to your VPC:\n\naws ec2 associate-vpc-cidr-block \\\n\n--cidr-block 10.11.0.0/16 \\\n\n--vpc-id $VPC_ID\n\nNOTE\n\nIn addition to IPv4, VPC also supports IPv6. You can configure an Amazon-provided IPv6 CIDR block by specifying the\n\n--amazon-provided-ipv6-cidr-block option. Here is an example of creating a VPC with an IPv6 CIDR block:\n\naws ec2 create-vpc --cidr-block 10.10.0.0/16 \\\n\n--amazon-provided-ipv6-cidr-block \\\n\n--tag-specifications\n\n'ResourceType=vpc,Tags=[{Key=Name,Value=AWSCookbook201-IPv6}]'\n\nA VPC is a Regional construct in AWS. A Region is a geographical area, and Availability Zones are physical data centers that reside within a Region. Regions span all Availability Zones (AZs),\n\nwhich are groups of isolated physical data centers. The number of AZs per Region varies, but all Regions have at least three. For the most up-to-date information about AWS Regions and AZs, see this article on “Regions and Availability Zones”.\n\nNOTE\n\nPer the VPC user guide, the initial quota of IPv4 CIDR blocks per VPC is 5. This can be raised to 50. The allowable\n\nnumber of IPv6 CIDR blocks per VPC is 1.\n\nChallenge\n\nCreate another VPC with a different CIDR range.\n\n2.2 Creating a Network Tier with Subnets and a Route Table in a VPC\n\nProblem\n\nYou have a VPC and need to create a network layout consisting of individual IP spaces for segmentation and redundancy.\n\nSolution\n\nCreate a route table within your VPC. Create two subnets in separate Availability Zones in a VPC. Associate the route table with the subnets (see Figure 2-2).\n\nFigure 2-2. Isolated subnet tier and route table\n\nPrerequisite\n\nA VPC\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a route table. This will allow you to create customized traffic routes for subnets associated with it:\n\nROUTE_TABLE_ID=$(aws ec2 create-route-table --vpc-id $VPC_ID \\\n\n--tag-specifications \\\n\n2.\n\n3.\n\n4.\n\n'ResourceType=route-table,Tags=[{Key=Name,Value=AWSCookbook202}]' \\\n\n--output text --query RouteTable.RouteTableId)\n\nCreate two subnets, one in each AZ. This will define the address spaces for you to create resources for your VPC:\n\nSUBNET_ID_1=$(aws ec2 create-subnet --vpc-id $VPC_ID \\\n\n--cidr-block 10.10.0.0/24 --availability-zone ${AWS_REGION}a \\\n\n--tag-specifications \\\n\n'ResourceType=subnet,Tags=[{Key=Name,Value=AWSCookbook202a}]' \\\n\n--output text --query Subnet.SubnetId)\n\nSUBNET_ID_2=$(aws ec2 create-subnet --vpc-id $VPC_ID \\\n\n--cidr-block 10.10.1.0/24 --availability-zone ${AWS_REGION}b \\\n\n--tag-specifications \\\n\n'ResourceType=subnet,Tags=[{Key=Name,Value=AWSCookbook202b}]' \\\n\n--output text --query Subnet.SubnetId)\n\nNOTE\n\nIn the previous commands, the --availability-zone parameter uses an environment variable for your Region appended with lowercase a or b characters to indicate which logical AZ (e.g., us-east-1a) to provision each subnet. AWS states that these names are randomized per account to balance\n\nresources across AZs.\n\nIf you are using multiple AWS accounts and want to find Availability Zone IDs for a Region that are\n\nconsistent, run this command:\n\naws ec2 describe-availability-zones --region $AWS_REGION\n\nAssociate the route table with the two subnets:\n\naws ec2 associate-route-table \\\n\n--route-table-id $ROUTE_TABLE_ID --subnet-id $SUBNET_ID_1\n\naws ec2 associate-route-table \\\n\n--route-table-id $ROUTE_TABLE_ID --subnet-id $SUBNET_ID_2\n\nFor each command in step 3, you should see output similar to the following:\n\n{\n\n\"AssociationId\": \"rtbassoc-<<snip>>\",\n\n\"AssociationState\": {\n\n\"State\": \"associated\"\n\n}\n\n}\n\nValidation checks\n\nRetrieve the configuration of the subnets you created and verify that they are in the same VPC but different AZs:\n\naws ec2 describe-subnets --subnet-ids $SUBNET_ID_1\n\naws ec2 describe-subnets --subnet-ids $SUBNET_ID_2\n\nFor each describe-subnets command, you should see output similar to this:\n\n{\n\n\"Subnets\": [\n\n{\n\n\"AvailabilityZone\": \"us-east-1a\",\n\n\"AvailabilityZoneId\": \"use1-az6\",\n\n\"AvailableIpAddressCount\": 251,\n\n\"CidrBlock\": \"10.10.0.0/24\",\n\n\"DefaultForAz\": false,\n\n\"MapPublicIpOnLaunch\": false,\n\n\"MapCustomerOwnedIpOnLaunch\": false,\n\n\"State\": \"available\",\n\n\"SubnetId\": \"subnet-<<snip>>\",\n\n\"VpcId\": \"vpc-<<snip>>\",\n\n\"OwnerId\": \"111111111111\",\n\n\"AssignIpv6AddressOnCreation\": false,\n\n\"Ipv6CidrBlockAssociationSet\": [],\n\n<<snip>>\n\n...\n\nValidate that the route table you created is associated with the two subnets:\n\naws ec2 describe-route-tables --route-table-ids $ROUTE_TABLE_ID\n\nYou should see output similar to the following:\n\n{\n\n\"RouteTables\": [\n\n{\n\n\"Associations\": [\n\n{\n\n\"Main\": false,\n\n\"RouteTableAssociationId\": \"rtbassoc-<<snip>>\",\n\n\"RouteTableId\": \"rtb-<<snip>>\",\n\n\"SubnetId\": \"subnet-<<snip>>\",\n\n\"AssociationState\": {\n\n\"State\": \"associated\"\n\n}\n\n},\n\n{\n\n\"Main\": false,\n\n\"RouteTableAssociationId\": \"rtbassoc-<<snip>>\",\n\n\"RouteTableId\": \"rtb-<<snip>>\",\n\n\"SubnetId\": \"subnet-<<snip>>\",\n\n\"AssociationState\": {\n\n\"State\": \"associated\"\n\n}\n\n}\n\n<<snip>>\n\n...\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen designing a subnet strategy, you should choose subnet sizes that fit your current needs and account for your application’s future growth. Subnets are used for elastic network interface (ENI) placement for AWS resources. This means that a particular ENI lives within a single AZ.\n\nTIP\n\nYou may run into a case where routes overlap. AWS provides information on how priority is determined.\n\nAWS reserves the first four and last IP addresses of every subnet’s CIDR block for features and functionality when you create a subnet. These are not available for your use. Per the documentation, these are the reserved addresses in the case of your example:\n\n.0 Network address.\n\n.1 Reserved by AWS for the VPC router.\n\n.2 Reserved by AWS for the IP address of the DNS server. This is always set to the VPC network range plus two.\n\n.3 Reserved by AWS for future use.\n\n.255 Network broadcast address. Broadcast in a VPC is not supported.\n\nA subnet has one route table associated with it. Route tables can be associated with one or more subnets and direct traffic to a destination of your choosing (more on this with the NAT gateway, internet gateway, and transit gateway recipes later). Entries within route tables are called routes and are defined as pairs of Destinations and Targets. When you created the route table, a default local route that handles intra-VPC traffic was automatically added for you. You have the ability to create custom routes that fit your needs. For a complete list of targets available to use within route tables, see this support document.\n\nNOTE\n\nENIs receive an IP address from an AWS managed DHCP server within your VPC. The DHCP options set is automatically\n\nconfigured with defaults for assigning addresses within the subnets you define. For more information about DHCP option\n\nsets, and how to create your own DHCP option sets see this support document.\n\nWhen creating a VPC in a Region, it is a best practice to spread subnets across AZs in that network tier. The number of AZs differs per Region, but most have at least three. An example of this in practice would be that if you had a public tier and an isolated tier spread over two AZs, you would have a total of four subnets: 2 tiers × 2 subnets per tier (one per AZ); see Figure 2-3.\n\nFigure 2-3. Isolated and public subnet tiers and route tables\n\nChallenge\n\nCreate a second route table and associate it with $SUBNET_ID_2. Configuring route tables for each AZ is a common pattern.\n\n2.3 Connecting Your VPC to the Internet Using an Internet Gateway\n\nProblem\n\nYou have an existing EC2 instance in a subnet of a VPC. You need to provide the ability for this instance to directly reach clients on the internet.\n\nSolution\n\nYou will create an internet gateway and attach it to your VPC. Next you will modify the route table associated with the subnet where the EC2 instance lives. You will add a route that sends traffic from the subnets to the internet gateway. Finally, create an Elastic IP (EIP) and associate it with the instance, as shown in Figure 2-4.",
      "page_number": 76
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 85-94)",
      "start_page": 85,
      "end_page": 94,
      "detection_method": "topic_boundary",
      "content": "Prerequisites\n\nPreparation\n\nFigure 2-4. Public subnet tier, internet gateway, and route table\n\nVPC and subnets created in two AZs and associated route tables.\n\nEC2 instance deployed. You will need the ability to connect to this for testing.\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate an internet gateway (IGW):\n\nINET_GATEWAY_ID=$(aws ec2 create-internet-gateway \\\n\n--tag-specifications \\\n\n'ResourceType=internet-gateway,Tags=[{Key=Name,Value=AWSCookbook202}]' \\\n\n--output text --query InternetGateway.InternetGatewayId)\n\n2.\n\nAttach the internet gateway to the existing VPC:\n\naws ec2 attach-internet-gateway \\\n\n--internet-gateway-id $INET_GATEWAY_ID --vpc-id $VPC_ID\n\n3.\n\nIn each route table of your VPC, create a route that sets the default route destination to the internet gateway:\n\naws ec2 create-route --route-table-id $ROUTE_TABLE_ID_1 \\\n\n--destination-cidr-block 0.0.0.0/0 --gateway-id $INET_GATEWAY_ID\n\naws ec2 create-route --route-table-id $ROUTE_TABLE_ID_2 \\\n\n--destination-cidr-block 0.0.0.0/0 --gateway-id $INET_GATEWAY_ID\n\nFor each command in step 3, you should see output similar to the following:\n\n{\n\n\"Return\": true\n\n}\n\n4.\n\nCreate an EIP:\n\nALLOCATION_ID=$(aws ec2 allocate-address --domain vpc \\\n\n--output text --query AllocationId)\n\nNOTE\n\nAWS defines an Elastic IP address (EIP) as “a static IPv4 address designed for dynamic cloud\n\ncomputing. An EIP address is allocated to your AWS account and is yours until you release it.”\n\n5.\n\nAssociate the EIP with the existing EC2 instance:\n\naws ec2 associate-address \\\n\n--instance-id $INSTANCE_ID --allocation-id $ALLOCATION_ID\n\nYou should see output similar to the following:\n\n{\n\n\"AssociationId\": \"eipassoc-<<snip>>\"\n\n}\n\nValidation checks\n\nConnect to the EC2 instance using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nPing a host on the internet to test internet connectivity:\n\nping -c 4 homestarrunner.com\n\nYou should see output similar to the following:\n\nsh-4.2$ ping -c 4 homestarrunner.com\n\nPING homestarrunner.com (72.10.33.178) 56(84) bytes of data.\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=1 ttl=49 time=2.12 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=2 ttl=49 time=2.04 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=3 ttl=49 time=2.05 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=4 ttl=49 time=2.08 ms\n\n--- homestarrunner.com ping statistics ---\n\n4 packets transmitted, 4 received, 0% packet loss, time 3002ms\n\nrtt min/avg/max/mdev = 2.045/2.078/2.127/0.045 ms\n\nsh-4.2$\n\nTIP\n\nThe public IP is not part of the OS configuration. If you want to retrieve the public IP from the instance’s metadata, you\n\ncan use this command:\n\ncurl http://169.254.169.254/latest/meta-data/public-ipv4\n\nExit the Session Manager session:\n\nexit\n\nClean up\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThe route that you created in your route table entry sends all nonlocal traffic to the IGW that provides your VPC internet connectivity. Because you were working with an existing running EC2 instance, you needed to create an Elastic IP and associated it with the instance. These steps enabled internet communication to the instance without having to interact with it. There is an option to enable auto-assignment of public IPv4 addresses for newly launched instances in a subnet. However, if you utilize auto-assignment, the public IPs will change after each instance reboot. EIPs associated with an instance will not change after reboots.\n\nNOTE\n\nRoute tables give priority to the most specific route. AWS also allows you to create routes that are more specific than the\n\ndefault local route. This allows you to create very controlled network flows. More information about route priority can be\n\nfound in an AWS discussion.\n\nThe security group associated with your instance does not allow inbound access. If you would like to allow inbound internet access to an instance in a public subnet, you will have to configure a security group ingress rule for this.\n\nA subnet that has a route of 0.0.0.0/0 associated with an IGW is considered a public subnet. It is considered a security best practice to place instances only in this type of tier that require inbound access from the public internet. End-user-facing load balancers are commonly placed in public subnets. A public subnet would not be an ideal choice for an application server or a database. In these cases, you can create a private tier or an isolated tier to fit your needs with the appropriate routing and use a NAT gateway to direct that subnet traffic to the internet gateway only when outbound internet access is required.\n\nChallenge\n\nInstall a web server on the EC2 instance, modify the security group, and connect to the instance from your workstation. See Recipe 2.7 for an example of how to configure internet access for instances in private subnets using a load balancer.\n\n2.4 Using a NAT Gateway for Outbound Internet Access from Private Subnets\n\nProblem\n\nYou have public subnets in your VPC that have a route to an internet gateway. You want to leverage this setup to provide outbound-only internet access for an instance in private subnets.\n\nSolution\n\nCreate a NAT gateway in one of the public subnets. Then create an Elastic IP and associate it with the NAT gateway. In the route table associated with the private subnets, add a route for internet-bound traffic that targets the NAT gateway (see Figure 2-5).\n\nPrerequisites\n\nVPC with public subnets in two AZs and associated route tables.\n\nIsolated subnets created in two AZs (we will turn these into the private subnets) and associated route tables.\n\nTwo EC2 instances deployed in the isolated subnets. You will need the ability to connect to these for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\n2.\n\n3.\n\n4.\n\nFigure 2-5. Internet access for private subnets provided by NAT gateways\n\nCreate an Elastic IP to be used with the NAT gateway:\n\nALLOCATION_ID=$(aws ec2 allocate-address --domain vpc \\\n\n--output text --query AllocationId)\n\nCreate a NAT gateway within the public subnet of AZ1:\n\nNAT_GATEWAY_ID=$(aws ec2 create-nat-gateway \\\n\n--subnet-id $VPC_PUBLIC_SUBNET_1 \\\n\n--allocation-id $ALLOCATION_ID \\\n\n--output text --query NatGateway.NatGatewayId)\n\nThis will take a few moments for the state to become available; check the status:\n\naws ec2 describe-nat-gateways \\\n\n--nat-gateway-ids $NAT_GATEWAY_ID \\\n\n--output text --query NatGateways[0].State\n\nAdd a default route for 0.0.0.0/0 with a destination of the NAT gateway to both of the private tier’s route tables. This default route sends all traffic not matching a more specific route to the destination specified:\n\naws ec2 create-route --route-table-id $PRIVATE_RT_ID_1 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--nat-gateway-id $NAT_GATEWAY_ID\n\naws ec2 create-route --route-table-id $PRIVATE_RT_ID_2 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--nat-gateway-id $NAT_GATEWAY_ID\n\nFor each command in step 4, you should see output similar to the following:\n\n{\n\n\"Return\": true\n\n}\n\nValidation checks\n\nConnect to EC2 instance 1 by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID_1\n\nTest internet access with a ping:\n\nping -c 4 homestarrunner.com\n\nYou should see output similar to the following:\n\nsh-4.2$ ping -c 4 homestarrunner.com\n\nPING homestarrunner.com (72.10.33.178) 56(84) bytes of data.\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=1 ttl=47 time=2.95 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=2 ttl=47 time=2.16 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=3 ttl=47 time=2.13 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=4 ttl=47 time=2.13 ms\n\n--- homestarrunner.com ping statistics ---\n\n4 packets transmitted, 4 received, 0% packet loss, time 3003ms\n\nrtt min/avg/max/mdev = 2.134/2.348/2.958/0.356 ms\n\nsh-4.2$\n\nExit the Session Manager session:\n\nexit\n\n(Optional) Repeat the validation steps for EC2 instance 2.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThis architecture gives you a subnet tier that allows outbound access but does not permit direct inbound internet access to resources within it. One way to allow internet resources inbound access to services running on resources in private subnets is to use a load balancer in the public subnets. We’ll look more at that type of configuration in Recipe 2.7.\n\nThe EIP associated with your NAT gateway becomes the external IP address for all communication that goes through it. For example, if a vendor needed to add your IP to an allow list, the NAT gateway EIP would be the “source” IP address provided to the vendor. Your EIP will remain the same as long as you keep it provisioned within your account.\n\nTIP\n\nIf you created a VPC with IPv6 capability, you can also create an egress-only internet gateway to allow outbound internet\n\naccess for private subnets, as discussed in an AWS article.\n\nThis NAT gateway was provisioned within one AZ in your VPC. While this is a cost-effective way to achieve outbound internet access for your private subnets, for production and mission- critical applications, you should consider provisioning NAT gateways in each AZ to provide resiliency and reduce the amount of cross-AZ traffic. This would also require creating route tables for each of your private subnets so that you can direct the 0.0.0.0/0 traffic to the NAT gateway in that particular subnet’s AZ. See the challenge for this recipe.\n\nNOTE\n\nIf you have custom requirements or would like more granular control of your outbound routing for your NAT\n\nimplementation, you can create a NAT instance. For a comparison of NAT gateways and NAT instances, see this support\n\ndocument.\n\nChallenge\n\nCreate a second NAT gateway in the public subnet in AZ2. Then modify the default route in the route table associated with the private subnet in AZ2. Change the destination to the newly created NAT gateway.\n\n2.5 Granting Dynamic Access by Referencing Security Groups\n\nProblem\n\nYou have an application group that currently consists of two instances and need to allow Secure Shell (SSH) between them. This needs to be configured in a way to allow for future growth of the number of instances securely and easily.\n\nSolution\n\nWARNING\n\nA common misconception is that by merely associating the same security group to ENIs for multiple EC2 instances, it will\n\nallow communication between them (see Figure 2-6).\n\nFigure 2-6. Incorrect representation of two instances using the same security group\n\nIn this recipe, we will create a security group and an associate each to the ENIs of two EC2 instances. Finally, we will create an ingress rule that authorizes the security group to reach itself on TCP port 22 (see Figure 2-7).",
      "page_number": 85
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 95-102)",
      "start_page": 95,
      "end_page": 102,
      "detection_method": "topic_boundary",
      "content": "Prerequisites\n\nFigure 2-7. Correct visualization of the ENIs of two instances using the same security group\n\nVPC with a subnet and associated route table.\n\nTwo EC2 instances deployed in the subnet. You will need the ability to connect to these for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a new security group for the EC2 instances:\n\nSG_ID=$(aws ec2 create-security-group \\\n\n--group-name AWSCookbook205Sg \\\n\n--description \"Instance Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\n2.\n\nAttach the security group to instances 1 and 2:\n\naws ec2 modify-instance-attribute --instance-id $INSTANCE_ID_1 \\\n\n--groups $SG_ID\n\naws ec2 modify-instance-attribute --instance-id $INSTANCE_ID_2 \\\n\n--groups $SG_ID\n\nNOTE\n\nYou used the modify-instance-attribute command to attach a new security group to the ENIs of your\n\nEC2 instances. To list the security groups associated with the ENI of an EC2 instance, you can view\n\nthem in the EC2 console under the Security tab of the instance details or use this command (replacing\n\n$INSTANCE_ID_1 with your own instance ID):\n\naws ec2 describe-security-groups --group-ids \\\n\n$(aws ec2 describe-instances --instance-id $INSTANCE_ID_1 \\\n\n--query \"Reservations[].Instances[].SecurityGroups[].GroupId[]\" \\\n\n--output text) --output text\n\n3.\n\nAdd an ingress rule to the security group that allows access on TCP port 22 from itself:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 22 \\\n\n--source-group $SG_ID \\\n\n--group-id $SG_ID \\\n\nTIP\n\nYou can and should create descriptions for all your security group rules to indicate the intended\n\nfunctionality of the authorization.\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<snip>>\",\n\n\"GroupId\": \"sg-<<snip>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 22,\n\n\"ToPort\": 22,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<snip>>\"\n\n}\n\n}\n\n]\n\n}\n\nTIP\n\nThis type of security group rule is called a self-referencing rule. It allows access to the specific port\n\nfrom traffic originating from ENIs (not a static range of IPs) that have this same security group\n\nattached to them.\n\nValidation checks\n\nList the IP address for instance 2:\n\naws ec2 describe-instances --instance-ids $INSTANCE_ID_2 \\\n\n--output text \\\n\n--query Reservations[0].Instances[0].PrivateIpAddress\n\nConnect to your instance 1 by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID_1\n\nInstall the Ncat utility:\n\nsudo yum -y install nc\n\nTest SSH connectivity to instance 2 (use instance 2’s IP that you listed previously):\n\nnc -vz $INSTANCE_IP_2 22\n\nYou should see output similar to the following:\n\nNcat: Version 7.50 ( https://nmap.org/ncat )\n\nNcat: Connected to 10.10.0.48:22.\n\nNcat: 0 bytes sent, 0 bytes received in 0.01 seconds.\n\nsh-4.2$\n\nExit the Session Manager session:\n\nexit\n\n(Optional) Repeat the validation steps from instance 2 to instance 1.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThe on-demand nature of the cloud (e.g., autoscaling) presents an opportunity for elasticity. Network security mechanisms, like security group references, are suitable for that. Traditionally, network architects might authorize CIDR ranges within firewall configurations. This type of authorization is generally referred to as static references. This legacy practice doesn’t scale dynamically, as you may add or remove instances from your workloads.\n\nA security group acts as a stateful virtual firewall for ENIs. The default behavior for security groups is to implicitly block all ingress while allowing all egress. You can associate multiple security groups with an ENI. There is an initial quota of 5 security groups per ENI and 60 rules (inbound or outbound) per security group.\n\nYou can also specify CIDR notation for authorizations. For example, for an authorization intended to allow RDP access from your New York branch office, you would use the following:\n\naws ec2 authorize-security-group-ingress \\\n\n--group-id sg-1234567890abcdef0 \\\n\n--ip-permissions\n\nIpProtocol=tcp,FromPort=3389,ToPort=3389,IpRanges='[{CidrIp=XXX.XXX.XXX.XXX/24,Description=\"RDP access\n\nfrom NY office\"}]'\n\nWARNING\n\nRemember that security groups cannot be deleted if the following conditions are present:\n\nThey are currently attached to an ENI.\n\nThey are referenced by other security groups (including themselves).\n\nChallenge\n\nCreate a third EC2 instance; use the same security group. Test access to and from it. (Hint: in the repository.)\n\n2.6 Using VPC Reachability Analyzer to Verify and Troubleshoot Network Paths\n\nProblem\n\nYou have two EC2 instances deployed in isolated subnets. You need to troubleshoot SSH connectivity between them.\n\nSolution\n\nYou will create, analyze, and describe network insights by using the VPC Reachability Analyzer. Based on the results, you will add a rule to the security group of instance 2 that allows the SSH port (TCP port 22) from instance 1’s security group. Finally, you will rerun the VPC Reachability Analyzer and view the updated results (see Figure 2-8).\n\nFigure 2-8. VPC Reachability Analyzer\n\nPrerequisites\n\nVPC with isolated subnets in two AZs and associated route tables.\n\nTwo EC2 instances deployed in the isolated subnets. You will need the ability to connect to these for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\n2.\n\n3.\n\nCreate a network insights path specifying both of the EC2 instances you deployed and TCP port 22:\n\nINSIGHTS_PATH_ID=$(aws ec2 create-network-insights-path \\\n\n--source $INSTANCE_ID_1 --destination-port 22 \\\n\n--destination $INSTANCE_ID_2 --protocol tcp \\\n\n--output text --query NetworkInsightsPath.NetworkInsightsPathId)\n\nStart the network insights analysis between the two instances using the INSIGHTS_PATH_ID created in the previous step:\n\nANALYSIS_ID_1=$(aws ec2 start-network-insights-analysis \\\n\n--network-insights-path-id $INSIGHTS_PATH_ID --output text \\\n\n--query NetworkInsightsAnalysis.NetworkInsightsAnalysisId)\n\nWait a few seconds until the analysis is done running and then view the results:\n\naws ec2 describe-network-insights-analyses \\\n\n--network-insights-analysis-ids $ANALYSIS_ID_1\n\nYou should see output similar to the following (note the NetworkPathFound and ExplanationCode fields):\n\n{\n\n\"NetworkInsightsAnalyses\": [\n\n{\n\n\"NetworkInsightsAnalysisId\": \"nia-<<snip>\",\n\n\"NetworkInsightsAnalysisArn\": \"arn:aws:ec2:us-east-1:111111111111:network-\n\ninsights-analysis/nia-<<snip>\",\n\n\"NetworkInsightsPathId\": \"nip-<<snip>\",\n\n\"StartDate\": \"2020-12-22T02:12:36.836000+00:00\",\n\n\"Status\": \"succeeded\",\n\n\"NetworkPathFound\": false,\n\n\"Explanations\": [\n\n{\n\n4.\n\n5.\n\n\"Direction\": \"ingress\",\n\n\"ExplanationCode\": \"ENI_SG_RULES_MISMATCH\",\n\n\"NetworkInterface\": {\n\n\"Id\": \"eni-<<snip>\",\n\n\"Arn\": \"arn:aws:ec2:us-east-1:11111111111:network-interface/eni-<<snip>\"\n\n},\n\nUpdate the security group attached to instance 2. Add a rule to allow access from instance 1’s security group to TCP port 22 (SSH):\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 22 \\\n\n--source-group $INSTANCE_SG_ID_1 \\\n\n--group-id $INSTANCE_SG_ID_2\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<snip>>\",\n\n\"GroupId\": \"sg-<<snip>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 22,\n\n\"ToPort\": 22,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<snip>>\"\n\n}\n\n}\n\n]\n\n}\n\nRerun the network insights analysis. Use the same INSIGHTS_PATH_ID as you did previously:\n\nANALYSIS_ID_2=$(aws ec2 start-network-insights-analysis \\",
      "page_number": 95
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 103-112)",
      "start_page": 103,
      "end_page": 112,
      "detection_method": "topic_boundary",
      "content": "--network-insights-path-id $INSIGHTS_PATH_ID --output text \\\n\n--query NetworkInsightsAnalysis.NetworkInsightsAnalysisId)\n\n6.\n\nShow the results of the new analysis:\n\naws ec2 describe-network-insights-analyses \\\n\n--network-insights-analysis-ids $ANALYSIS_ID_2\n\nYou should see output similar to the following (note the NetworkPathFound field):\n\n{\n\n\"NetworkInsightsAnalyses\": [\n\n{\n\n\"NetworkInsightsAnalysisId\": \"nia-<<snip>>\",\n\n\"NetworkInsightsAnalysisArn\": \"arn:aws:ec2:us-east-1:111111111111:network-\n\ninsights-analysis/nia-<<snip>>\",\n\n\"NetworkInsightsPathId\": \"nip-<<snip>>\",\n\n\"StartDate\": \"2021-02-21T23:52:15.565000+00:00\",\n\n\"Status\": \"succeeded\",\n\n\"NetworkPathFound\": true,\n\n\"ForwardPathComponents\": [\n\n{\n\n\"SequenceNumber\": 1,\n\n\"Component\": {\n\n\"Id\": \"i-<<snip>>\",\n\n...\n\nValidation checks\n\nList the IP address for instance 2:\n\naws ec2 describe-instances --instance-ids $INSTANCE_ID_2 \\\n\n--output text \\\n\n--query Reservations[0].Instances[0].PrivateIpAddress\n\nConnect to your EC2 instance by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID_1\n\nInstall the Ncat utility:\n\nsudo yum -y install nc\n\nTest SSH connectivity to instance 2 (use instance 2’s IP that you listed previously):\n\nnc -vz $INSTANCE_IP_2 22\n\nYou should see output similar to the following:\n\nNcat: Version 7.50 ( https://nmap.org/ncat )\n\nNcat: Connected to 10.10.0.48:22.\n\nNcat: 0 bytes sent, 0 bytes received in 0.01 seconds.\n\nsh-4.2$\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nA network insights path is a definition of the connectivity you want to test. Initially, there wasn’t SSH connectivity between the instances because the security group on the destination (instance 2) did not allow access. After you updated the security group associated with instance 2 and reran an analysis, you were able to verify successful connectivity. Using the VPC Reachability Analyzer is an efficient capability for network troubleshooting and validating configuration in a “serverless” manner. It does not require you to provision infrastructure to analyze, verify, and troubleshoot network connectivity.\n\nNOTE\n\nVPC reachability has broad support of sources and destinations for resources within your VPCs. For a complete list of\n\nsupported sources and destinations, see this support document.\n\nVPC Reachability Analyzer provides explanation codes that describe the result of a network path analysis. In this recipe, you observed the code ENI_SG_RULES_MISMATCH that indicates that the security groups are not allowing traffic between the source and destination. The complete list of explanation codes is available in this documentation.\n\nChallenge\n\nAdd an internet gateway to your VPC and test access to that from an instance.\n\n2.7 Redirecting HTTP Traffic to HTTPS with an Application Load Balancer\n\nProblem\n\nYou have a containerized application running in a private subnet. Users on the internet need to access this application. To help secure the application, you would like to redirect all requests from HTTP to HTTPS.\n\nSolution\n\nCreate an Application Load Balancer (ALB). Next, create listeners on the ALB for ports 80 and 443, target groups for your containerized application, and listener rules. Configure the listener rules to send traffic to your target group, as shown in Figure 2-9. Finally, configure an action to redirect with an HTTP 301 response code, port 80 (HTTP) to port 443 (HTTPS) while preserving the URL in the request (see Figure 2-10).\n\nFigure 2-9. VPC with ALB serving internet traffic to containers in private subnets\n\nFigure 2-10. Redirecting HTTP to HTTPs with an ALB\n\nPrerequisites\n\nVPC with public subnets in two AZs and associated route tables.\n\nPrivate subnets created in two AZs and associated route tables.\n\nAn ECS cluster and container definition exposing a web application on port 80.\n\nA Fargate service that runs tasks on the ECS cluster.\n\nOpenSSL. (You can install this using brew install openssl or yum install openssl.)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a new private key to be used for the certificate:\n\nopenssl genrsa 2048 > my-private-key.pem\n\nYou should see output similar to the following:\n\nGenerating RSA private key, 2048 bit long modulus\n\n................................................................+++\n\n2.\n\n3.\n\n.........................................................................+++\n\ne is 65537 (0x10001)\n\nGenerate a self-signed certificate using OpenSSL CLI:\n\nopenssl req -new -x509 -nodes -sha256 -days 365 \\\n\nkey my-private-key.pem -outform PEM -out my-certificate.pem\n\nYou should see output similar to the following:\n\nYou are about to be asked to enter information that will be incorporated\n\ninto your certificate request.\n\nWhat you are about to enter is what is called a Distinguished Name or a DN.\n\nThere are quite a few fields but you can leave some blank\n\nFor some fields there will be a default value,\n\nIf you enter '.', the field will be left blank.\n\n-----\n\nCountry Name (2 letter code) []:US\n\nState or Province Name (full name) []:Pennsylvania\n\nLocality Name (eg, city) []:Scranton\n\nOrganization Name (eg, company) []:AWS Cookbook Inc\n\nOrganizational Unit Name (eg, section) []:Cloud Team\n\nCommon Name (eg, fully qualified host name) []:mytest.com\n\nEmail Address []:you@youremail.com\n\nNOTE\n\nYou are using a self-signed certificate for this recipe, which will throw a warning when you access the\n\nLoad Balancer DNS name in most browsers. You can generate a trusted certificate for your own DNS\n\nrecord by using AWS Certificate Manager (ACM).\n\nUpload the generated certificate into IAM:\n\nCERT_ARN=$(aws iam upload-server-certificate \\\n\n--server-certificate-name AWSCookbook207 \\\n\n--certificate-body file://my-certificate.pem \\\n\n--private-key file://my-private-key.pem \\\n\n--query ServerCertificateMetadata.Arn --output text)\n\n4.\n\n5.\n\n6.\n\nCreate a security group to use with the ALB that you will create later:\n\nALB_SG_ID=$(aws ec2 create-security-group --group-name Cookbook207SG \\\n\n--description \"ALB Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\nAdd rules to the security group to allow HTTP and HTTPS traffic from the world:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 443 \\\n\n--cidr '0.0.0.0/0' \\\n\n--group-id $ALB_SG_ID\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 80 \\\n\n--cidr '0.0.0.0/0' \\\n\n--group-id $ALB_SG_ID\n\nFor each command in step 5, you should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<snip>>\",\n\n\"GroupId\": \"sg-<<snip>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 80,\n\n\"ToPort\": 80,\n\n\"CidrIpv4\": \"0.0.0.0/0\"\n\n}\n\n]\n\n}\n\nAuthorize the container’s security group to allow ingress traffic from the ALB:\n\n7.\n\n8.\n\n9.\n\n10.\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 80 \\\n\n--source-group $ALB_SG_ID \\\n\n--group-id $APP_SG_ID\n\nCreate an ALB across the public subnets and assign it the previously created security group:\n\nLOAD_BALANCER_ARN=$(aws elbv2 create-load-balancer \\\n\n--name aws-cookbook207-alb \\\n\n--subnets $VPC_PUBLIC_SUBNETS --security-groups $ALB_SG_ID \\\n\n--scheme internet-facing \\\n\n--output text --query LoadBalancers[0].LoadBalancerArn)\n\nCreate a target group for the Load Balancer:\n\nTARGET_GROUP=$(aws elbv2 create-target-group \\\n\n--name aws-cookbook207-tg --vpc-id $VPC_ID \\\n\n--protocol HTTP --port 80 --target-type ip \\\n\n--query \"TargetGroups[0].TargetGroupArn\" \\\n\n--output text)\n\nGet the IP of the container that is running your application:\n\nTASK_ARN=$(aws ecs list-tasks --cluster $ECS_CLUSTER_NAME \\\n\n--output text --query taskArns)\n\nCONTAINER_IP=$(aws ecs describe-tasks --cluster $ECS_CLUSTER_NAME \\\n\n--task $TASK_ARN --output text \\\n\n--query tasks[0].attachments[0].details[4] | cut -f 2)\n\nRegister a container with the target group:\n\naws elbv2 register-targets --targets Id=$CONTAINER_IP \\\n\n--target-group-arn $TARGET_GROUP\n\n11.\n\n12.\n\nNOTE\n\nFor this recipe, you register an IP address of an ECS task within an ECS service with the load balancer\n\nthat you created. You can optionally associate an ECS service directly with an Application Load\n\nBalancer on ECS service creation. For more information, see this documentation.\n\nCreate an HTTPS listener on the ALB that uses the certificate you imported and forwards traffic to your target group:\n\nHTTPS_LISTENER_ARN=$(aws elbv2 create-listener \\\n\n--load-balancer-arn $LOAD_BALANCER_ARN \\\n\n--protocol HTTPS --port 443 \\\n\n--certificates CertificateArn=$CERT_ARN \\\n\n--default-actions Type=forward,TargetGroupArn=$TARGET_GROUP \\\n\n--output text --query Listeners[0].ListenerArn)\n\nAdd a rule for the listener on port 443 to forward traffic to the target group that you created:\n\naws elbv2 create-rule \\\n\n--listener-arn $HTTPS_LISTENER_ARN \\\n\n--priority 10 \\\n\n--conditions '{\"Field\":\"path-pattern\",\"PathPatternConfig\":{\"Values\":[\"/*\"]}}' \\\n\n--actions Type=forward,TargetGroupArn=$TARGET_GROUP\n\nYou should see output similar to the following:\n\n{\n\n\"Rules\": [\n\n{\n\n\"RuleArn\": \"arn:aws:elasticloadbalancing:us-east-1:111111111111:listener-\n\nrule/app/aws-cookbook207-alb/<<snip>>\",\n\n\"Priority\": \"10\",\n\n\"Conditions\": [\n\n{\n\n\"Field\": \"path-pattern\",\n\n\"Values\": [\n\n\"/*\"\n\n],\n\n13.\n\n\"PathPatternConfig\": {\n\n\"Values\": [\n\n\"/*\"\n\n]\n\n}\n\n}\n\n],\n\n\"Actions\": [\n\n{\n\n\"Type\": \"forward\",\n\n...\n\nCreate a redirect response for all HTTP traffic that sends a 301 response to the browser while preserving the full URL for the HTTPS redirect:\n\naws elbv2 create-listener --load-balancer-arn $LOAD_BALANCER_ARN \\\n\n--protocol HTTP --port 80 \\\n\n--default-actions \\\n\n\"Type=redirect,RedirectConfig={Protocol=HTTPS,Port=443,Host='#{host}',Query='#\n\n{query}',Path='/#{path}',\n\nStatusCode=HTTP_301}\"\n\nYou should see output similar to the following:\n\n{\n\n\"Listeners\": [\n\n{\n\n\"ListenerArn\": \"arn:aws:elasticloadbalancing:us-east-\n\n1:111111111111:listener/app/aws-cookbook207-alb/<<snip>>\",\n\n\"LoadBalancerArn\": \"arn:aws:elasticloadbalancing:us-east-\n\n1:111111111111:loadbalancer/app/aws-cookbook207-alb/<<snip>>\",\n\n\"Port\": 80,\n\n\"Protocol\": \"HTTP\",\n\n\"DefaultActions\": [\n\n{\n\n\"Type\": \"redirect\",\n\n\"RedirectConfig\": {\n\n\"Protocol\": \"HTTPS\",\n\n\"Port\": \"443\",\n\n\"Host\": \"#{host}\",\n\n\"Path\": \"/#{path}\",\n\n\"Query\": \"#{query}\",\n\n\"StatusCode\": \"HTTP_301\"\n\n}\n\n}\n\n...\n\n14.\n\nVerify the health of the targets:\n\naws elbv2 describe-target-health --target-group-arn $TARGET_GROUP \\\n\n--query TargetHealthDescriptions[*].TargetHealth.State\n\nYou should see output similar to this:\n\n[\n\n\"healthy\"\n\n]\n\nValidation checks\n\nGet the URL of the load balancer so that you can test it:\n\nLOAD_BALANCER_DNS=$(aws elbv2 describe-load-balancers \\\n\n--names aws-cookbook207-alb \\\n\n--output text --query LoadBalancers[0].DNSName)\n\nDisplay the URL and test it in your browser. You should notice that you end up at an HTTPS URL. You will most likely receive a warning from your browser because of the self-signed cert:\n\necho $LOAD_BALANCER_DNS\n\nOr test it from the command line.\n\ncURL the Load Balancer DNS over HTTP and observe the 301 code:\n\ncurl -v http://$LOAD_BALANCER_DNS",
      "page_number": 103
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 113-122)",
      "start_page": 113,
      "end_page": 122,
      "detection_method": "topic_boundary",
      "content": "cURL the Load Balancer DNS and specify to follow the redirect to HTTPS:\n\ncurl -vkL http://$LOAD_BALANCER_DNS\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen you added a 301 redirect rule for the port 80 listener, this allowed the ALB to instruct clients to follow the redirect to port 443 so that users of your application will be automatically redirected to HTTPS. The redirect rule also preserves the URL path in the original request.\n\nApplication Load Balancers operate on Layer 7 of the OSI model. The ALB documentation lists the available target types of EC2 instances, IP addresses, and Lambda functions. You can create internet-facing ALBs (when your VPC has an internet gateway attached) and internal ALBs for usage within your internal network only. The ALB provisions elastic network interfaces that have IP addresses within your chosen subnets to communicate with your services. ALBs continuously run health checks for members of your associated target groups that allow the ALB to detect healthy components of your application to route traffic to. ALBs are also a great layer to add in front of your applications for increased security since you can allow the targets to be accessed by only the load balancer—not by clients directly.\n\nAWS offers multiple types of load balancers for specific use cases. You should choose the load balancer that best fits your needs. For example, for high-performance Layer 4 load balancing with static IP address capability, you might consider Network Load Balancers, and for network virtual appliances (NVAs) like virtual firewalls and security appliances, you might consider Gateway Load Balancers. For more information on and a comparison of the types of load balancers available in AWS, see the support document.\n\nChallenge\n\nUpdate the SSL certificate with a new one.\n\n2.8 Simplifying Management of CIDRs in Security Groups with Prefix Lists\n\nProblem\n\nYou have two applications hosted in public subnets. The applications are hosted on instances with specific access requirements for each application. During normal operation, these applications need to be accessed from virtual desktops in another Region. However, you need to reach them from your home PC for a short period for testing.\n\nSolution\n\nUsing the AWS-provided IP address ranges list, create a managed prefix list that contains a list of CIDR ranges for WorkSpaces gateways in us-west-2 and associate it with each security group. Update the prefix list with your home IP for testing and then optionally remove it (see Figure 2- 11).\n\nPrerequisites\n\nVPC with public subnets in two AZs and associated route tables\n\nTwo EC2 instances in each public subnet running a web server on port 80\n\nTwo security groups, one associated with each EC2 instance\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\n2.\n\n3.\n\nFigure 2-11. Two applications in public subnets protected by security groups\n\nDownload the AWS IP address ranges JSON file:\n\ncurl -o ip-ranges.json https://ip-ranges.amazonaws.com/ip-ranges.json\n\nNOTE\n\nYou will need to install the jq utility if your workstation doesn’t already have it; for example, brew install jq.\n\nGenerate a list of the CIDR ranges for Amazon WorkSpaces gateways in us- west-2:\n\njq -r '.prefixes[] | select(.region==\"us-west-2\") |\n\nselect(.service==\"WORKSPACES_GATEWAYS\") | .ip_prefix' < ip-ranges.json\n\nTIP\n\nYou can find more information on AWS IP address ranges in their documentation.\n\nUse the IP ranges for Amazon WorkSpaces from ip-ranges.json to create a managed prefix list:\n\nPREFIX_LIST_ID=$(aws ec2 create-managed-prefix-list \\\n\n--address-family IPv4 \\\n\n--max-entries 15 \\\n\n--prefix-list-name allowed-us-east-1-cidrs \\\n\n--output text --query \"PrefixList.PrefixListId\" \\\n\n--entries\n\nCidr=44.234.54.0/23,Description=workspaces-us-west-2-cidr1\n\nCidr=54.244.46.0/23,Description=workspaces-us-west-2-cidr2)\n\n4.\n\n5.\n\nNOTE\n\nAt this point, your workstation should not be able to reach either of the instances. If you try one of\n\nthese commands, you will receive a “Connection timed out” error:\n\ncurl -m 2 $INSTANCE_IP_1\n\ncurl -m 2 $INSTANCE_IP_2\n\nGet your workstation’s public IPv4 address:\n\nMY_IP_4=$(curl myip4.com | tr -d ' ')\n\nUpdate your managed prefix list and add your workstation’s public IPv4 address (see Figure 2-12):\n\naws ec2 modify-managed-prefix-list \\\n\n--prefix-list-id $PREFIX_LIST_ID \\\n\n--current-version 1 \\\n\n--add-entries Cidr=${MY_IP_4}/32,Description=my-workstation-ip\n\nFigure 2-12. Security group rules referencing a prefix list\n\nTIP\n\nThere is an AWS-managed prefix list for S3.\n\nYou should see output similar to the following:\n\n{\n\n\"PrefixList\": {\n\n\"PrefixListId\": \"pl-013217b85144872d2\",\n\n\"AddressFamily\": \"IPv4\",\n\n\"State\": \"modify-in-progress\",\n\n\"PrefixListArn\": \"arn:aws:ec2:us-east-1:111111111111:prefix-list/pl-\n\n013217b85144872d2\",\n\n\"PrefixListName\": \"allowed-us-east-1-cidrs\",\n\n\"MaxEntries\": 10,\n\n\"Version\": 1,\n\n\"OwnerId\": \"111111111111\"\n\n}\n\n}\n\n6.\n\nFor each application’s security group, add an inbound rule that allows TCP port 80 access from the prefix list:\n\naws ec2 authorize-security-group-ingress \\\n\n--group-id $INSTANCE_SG_1 --ip-permissions \\\n\nIpProtocol=tcp,FromPort=80,ToPort=80,PrefixListIds=\"[{Description=http-from-\n\nprefix-list,PrefixListId=$PREFIX_LIST_ID}]\"\n\naws ec2 authorize-security-group-ingress \\\n\n--group-id $INSTANCE_SG_2 --ip-permissions \\\n\nIpProtocol=tcp,FromPort=80,ToPort=80,PrefixListIds=\"[{Description=http-from-\n\nprefix-list,PrefixListId=$PREFIX_LIST_ID}]\"\n\nTIP\n\nFind out where your managed list is used. This command is helpful for auditing where prefix lists are\n\nused throughout your AWS environments:\n\naws ec2 get-managed-prefix-list-associations \\\n\n--prefix-list-id $PREFIX_LIST_ID\n\nValidation checks\n\nTest access to both instances from your workstation’s PC:\n\ncurl -m 2 $INSTANCE_IP_1\n\ncurl -m 2 $INSTANCE_IP_2\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nIf you need to update the list of CIDR blocks allowing ingress communication to your instances, you can simply update the prefix list instead of the security group. This helps reduce the amount of maintenance overhead if you need to use this type of authorization across many security groups; you need to update the prefix list in only a single location rather than modify every security group authorization that requires this network security configuration. You can also use prefix lists for egress security group authorizations.\n\nPrefix lists can be associated with route tables; they are also useful for blackholing traffic (prohibiting access to a specific list of IP addresses and CIDR blocks) and can also simplify your route table configuration. For example, you could maintain a prefix list of branch office CIDR ranges and use them to implement your routing and security group authorizations, simplifying your management for network flow and security configuration. An example of associating a prefix list with a route looks like this:\n\naws ec2 create-route --route-table-id $Sub1RouteTableID \\\n\n--destination-prefix-list-id $PREFIX_LIST_ID \\\n\n--instance-id $INSTANCE_ID\n\nPrefix lists also provide a powerful versioning mechanism, allowing you to quickly roll back to previous known working states. If, for example, you updated a prefix list and found that the change broke some existing functionality, you can roll back to a previous version of a prefix list to restore previous functionality while you investigate the root cause of the error. If you decide to roll back to a previous version for some reason, first describe the prefix list to get the current version number:\n\naws ec2 describe-prefix-lists --prefix-list-ids $PREFIX_LIST_ID\n\nChallenge\n\nRevert the active version of the prefix list so that your workstation IP is removed and you can no longer access either application. (Hint: in the repository.) 2.9 Controlling Network Access to S3 from Your VPC Using VPC Endpoints\n\nProblem\n\nResources within your VPC should be able to access only a specific S3 bucket. Also, this S3 traffic should not traverse the internet for security reasons and to keep bandwidth costs low.\n\nSolution\n\nYou will create a gateway VPC endpoint for S3, associate it with a route table, and customize its policy document (see Figure 2-13).\n\nFigure 2-13. Controlling S3 access with gateway endpoints\n\nPrerequisites\n\nVPC with isolated subnets in two AZs and associated route tables\n\nOne EC2 instance in a public subnet that you can access for testing\n\nAn existing S3 bucket that you want to limit access to\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a gateway endpoint in your VPC and associate the endpoint with the isolated route tables:\n\nEND_POINT_ID=$(aws ec2 create-vpc-endpoint \\\n\n--vpc-id $VPC_ID \\\n\n--service-name com.amazonaws.$AWS_REGION.s3 \\\n\n--route-table-ids $RT_ID_1 $RT_ID_2 \\\n\n--query VpcEndpoint.VpcEndpointId --output text)\n\n2.\n\nCreate a template endpoint policy file called policy.json with the following content (included in the repository). This is used to limit access to only the S3 bucket that you created in the preparation steps:\n\n{\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"RestrictToOneBucket\",\n\n\"Principal\": \"*\",\n\n\"Action\": [\n\n\"s3:GetObject\",\n\n\"s3:PutObject\"\n\n],\n\n\"Effect\": \"Allow\",\n\n\"Resource\": [\"arn:aws:s3:::S3BucketName\",\n\n\"arn:aws:s3:::S3BucketName/*\"]\n\n}\n\n]\n\n}\n\n3.\n\nInsert your S3_BUCKET_NAME in the policy-template.json file:\n\nsed -e \"s/S3BucketName/${BUCKET_NAME}/g\" \\\n\npolicy-template.json > policy.json\n\n4.\n\nModify the endpoint’s policy document. Endpoint policies limit or restrict the resources that can be accessed through the VPC endpoint:\n\naws ec2 modify-vpc-endpoint \\\n\n--policy-document file://policy.json \\\n\n--vpc-endpoint-id $END_POINT_ID\n\nValidation checks\n\nOutput the name of your S3 Bucket so that you can refer to it when you are connected to your EC2 Instance:\n\necho $BUCKET_NAME\n\nConnect to the EC2 instance by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nSet the Region by grabbing the value from the instance’s metadata:\n\nexport AWS_DEFAULT_REGION=$(curl \\\n\n--silent http://169.254.169.254/latest/dynamic/instance-identity/document \\\n\n| awk -F'\"' ' /region/ {print $4}')\n\nRetrieve the allowed S3 bucket name:",
      "page_number": 113
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 123-137)",
      "start_page": 123,
      "end_page": 137,
      "detection_method": "topic_boundary",
      "content": "BUCKET=$(aws ssm get-parameters \\\n\n--names \"Cookbook209S3Bucket\" \\\n\n--query \"Parameters[*].Value\" --output text)\n\nTest access by trying to copy a file from your S3 bucket:\n\naws s3 cp s3://${BUCKET_NAME}/test_file /home/ssm-user/\n\nYou should see output similar to the following:\n\ndownload: s3://cdk-aws-cookbook-209-awscookbookrecipe20979239201-115xoj77fgxoh/test_file to\n\n./test_file\n\nNOTE\n\nThe following command is attempting to list a public S3 bucket. However, because of the endpoint policy we have\n\nconfigured, it is expected that this will fail.\n\nTry to list the contents of a public S3 bucket associated with the OpenStreetMap Foundation Public Dataset Initiative:\n\naws s3 ls s3://osm-pds/\n\nYou should see output similar to the following:\n\nAn error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nUsing an endpoint policy is a useful security implementation to restrict access to S3 buckets. This applies not only to S3 buckets owned by your account but also to all S3 buckets globally on AWS.\n\nTIP\n\nRecently, AWS announced support for S3 interface endpoints. However, it is worth noting that while these are great for\n\nsome use cases (e.g., when you want to control traffic with security groups), they are not ideal for this problem because of\n\nthe costs associated with interface endpoints.\n\nPer the VPC User Guide, Gateway VPC endpoints are free and used within your VPC’s route tables to keep traffic bound for AWS services within the AWS backbone network without traversing the network. This allows you to create VPCs that do not need internet gateways for applications that do not require them but need access to other AWS services like S3 and DynamoDB. All traffic bound for these services will be directed by the route table to the VPC endpoint rather than the public internet route, since the VPC endpoint route table entry is more specific than the default 0.0.0.0/0 route.\n\nS3 VPC endpoint policies leverage JSON policy documents that can be as fine-grained as your needs require. You can use conditionals, source IP addresses, VPC endpoint IDs, S3 bucket names, and more. For more information on the policy elements available, see the support document.\n\nChallenge\n\nModify the bucket policy for the S3 bucket to allow access only from the VPC endpoint that you created. For some tips on this, check out the S3 User Guide.\n\n2.10 Enabling Transitive Cross-VPC Connections Using Transit Gateway\n\nProblem\n\nYou need to implement transitive routing across all of your VPCs and share internet egress from a shared services VPC to your other VPCs to reduce the number of NAT gateways you have to deploy.\n\nSolution\n\nDeploy an AWS transit gateway (TGW) and configure transit gateway VPC attachments for all of your VPCs. Update your VPC route tables of each VPC to send all nonlocal traffic to the transit gateway and enable sharing of the NAT gateway in your shared services VPC for all of your spoke VPCs (see Figure 2-14).\n\nFigure 2-14. AWS transit gateway with three VPCs\n\nWARNING\n\nThe default initial quota of VPCs per Region per account is five. This solution will deploy three VPCs. If you already have\n\nmore than two VPCs, you can decide among four choices: deploy to a different Region, delete any existing VPCs that are\n\nno longer needed, use a test account, or request a quota increase.\n\nPrerequisites\n\nThree VPCs in the same Region with private and isolated subnet tiers\n\nInternet gateway attached to a VPC (VPC2 in our example)\n\nNAT gateway deployed in public subnets\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a transit gateway:\n\nTGW_ID=$(aws ec2 create-transit-gateway \\\n\n--description AWSCookbook210 \\\n\n—-\n\noptions=AmazonSideAsn=65010,AutoAcceptSharedAttachments=enable,DefaultRouteTable\n\nAssociation=enable,\\\n\nDefaultRouteTablePropagation=enable,VpnEcmpSupport=enable,DnsSupport=enable \\\n\n--output text --query TransitGateway.TransitGatewayId)\n\n2.\n\nWait until the transit gateway’s state has reached available. This may take several minutes:\n\naws ec2 describe-transit-gateways \\\n\n--transit-gateway-ids $TGW_ID \\\n\n--output text --query TransitGateways[0].State\n\n3.\n\n4.\n\n5.\n\n6.\n\nCreate a transit gateway attachment for VPC1:\n\nTGW_ATTACH_1=$(aws ec2 create-transit-gateway-vpc-attachment \\\n\n--transit-gateway-id $TGW_ID \\\n\n--vpc-id $VPC_ID_1 \\\n\n--subnet-ids $ATTACHMENT_SUBNETS_VPC_1 \\\n\n--query TransitGatewayVpcAttachment.TransitGatewayAttachmentId \\\n\n--output text)\n\nCreate a transit gateway attachment for VPC2:\n\nTGW_ATTACH_2=$(aws ec2 create-transit-gateway-vpc-attachment \\\n\n--transit-gateway-id $TGW_ID \\\n\n--vpc-id $VPC_ID_2 \\\n\n--subnet-ids $ATTACHMENT_SUBNETS_VPC_2 \\\n\n--query TransitGatewayVpcAttachment.TransitGatewayAttachmentId \\\n\n--output text)\n\nCreate a transit gateway attachment for VPC3:\n\nTGW_ATTACH_3=$(aws ec2 create-transit-gateway-vpc-attachment \\\n\n--transit-gateway-id $TGW_ID \\\n\n--vpc-id $VPC_ID_3 \\\n\n--subnet-ids $ATTACHMENT_SUBNETS_VPC_3 \\\n\n--query TransitGatewayVpcAttachment.TransitGatewayAttachmentId \\\n\n--output text)\n\nAdd routes for all private subnets in VPCs 1 and 3 to target the TGW for destinations of 0.0.0.0/0. This enables consolidated internet egress through the NAT gateway in VPC2 and transitive routing to other VPCs:\n\naws ec2 create-route --route-table-id $VPC_1_RT_ID_1 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--transit-gateway-id $TGW_ID\n\naws ec2 create-route --route-table-id $VPC_1_RT_ID_2 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--transit-gateway-id $TGW_ID\n\n7.\n\n8.\n\n9.\n\naws ec2 create-route --route-table-id $VPC_3_RT_ID_1 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--transit-gateway-id $TGW_ID\n\naws ec2 create-route --route-table-id $VPC_3_RT_ID_2 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--transit-gateway-id $TGW_ID\n\nNow add a route to your 10.10.0.0/24 supernet in the route tables associated with the private subnets of VPC2, pointing its destination to the transit gateway. This is more specific than the 0.0.0.0/0 destination that is already present and therefore takes higher priority in routing decisions. This directs traffic bound for VPCs 1, 2, and 3 to the TGW:\n\naws ec2 create-route --route-table-id $VPC_2_RT_ID_1 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\naws ec2 create-route --route-table-id $VPC_2_RT_ID_2 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\nQuery for the NAT gateways in use; we’ll need these to add routes to them for internet traffic:\n\nNAT_GW_ID_1=$(aws ec2 describe-nat-gateways \\\n\n--filter \"Name=subnet-id,Values=$VPC_2_PUBLIC_SUBNET_ID_1\" \\\n\n--output text --query NatGateways[*].NatGatewayId)\n\nNAT_GW_ID_2=$(aws ec2 describe-nat-gateways \\\n\n--filter \"Name=subnet-id,Values=$VPC_2_PUBLIC_SUBNET_ID_2\" \\\n\n--output text --query NatGateways[*].NatGatewayId)\n\nAdd a route for the attachment subnet in VPC2 to direct internet traffic to the NAT gateway:\n\naws ec2 create-route --route-table-id $VPC_2_ATTACH_RT_ID_1 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--nat-gateway-id $NAT_GW_ID_1\n\n10.\n\n11.\n\n12.\n\n13.\n\naws ec2 create-route --route-table-id $VPC_2_ATTACH_RT_ID_2 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--nat-gateway-id $NAT_GW_ID_2\n\nAdd a static route to the route tables associated with the public subnet in VPC2. This enables communication back to the TGW to allow sharing the NAT gateway with all attached VPCs:\n\naws ec2 create-route --route-table-id $VPC_2_PUBLIC_RT_ID_1 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\naws ec2 create-route --route-table-id $VPC_2_PUBLIC_RT_ID_2 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\nAdd a static route for the private subnets in VPC2 to allow communication back to the TGW attachments from VPC2 private subnets:\n\naws ec2 create-route --route-table-id $VPC_2_RT_ID_1 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\naws ec2 create-route --route-table-id $VPC_2_RT_ID_2 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\nGet the transit route table ID:\n\nTRAN_GW_RT=$(aws ec2 describe-transit-gateways \\\n\n--transit-gateway-ids $TGW_ID --output text \\\n\n--query TransitGateways[0].Options.AssociationDefaultRouteTableId)\n\nAdd a static route in the transit gateway route table for VPC2 (with the NAT gateways) to send all internet traffic over this path:\n\naws ec2 create-transit-gateway-route \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--transit-gateway-route-table-id $TRAN_GW_RT \\\n\n--transit-gateway-attachment-id $TGW_ATTACH_2\n\nValidation checks\n\nEnsure your EC2 Instance 1 has registered with SSM. To check the status use the following command, which should return the instance ID:\n\naws ssm describe-instance-information \\\n\n--filters Key=ResourceType,Values=EC2Instance \\\n\n--query \"InstanceInformationList[].InstanceId\" --output text\n\nConnect to your EC2 instance by using SSM Session Manager:\n\naws ssm start-session --target $INSTANCE_ID_1\n\nTest internet access:\n\nping -c 4 aws.amazon.com\n\nYou should see output similar to the following:\n\nPING dr49lng3n1n2s.cloudfront.net (99.86.187.73) 56(84) bytes of data.\n\n64 bytes from server-99-86-187-73.iad79.r.cloudfront.net (99.86.187.73): icmp_seq=1 ttl=238 time=3.44\n\nms\n\n64 bytes from server-99-86-187-73.iad79.r.cloudfront.net (99.86.187.73): icmp_seq=2 ttl=238 time=1.41\n\nms\n\n64 bytes from server-99-86-187-73.iad79.r.cloudfront.net (99.86.187.73): icmp_seq=3 ttl=238 time=1.43\n\nms\n\n64 bytes from server-99-86-187-73.iad79.r.cloudfront.net (99.86.187.73): icmp_seq=4 ttl=238 time=1.44\n\nms\n\n--- dr49lng3n1n2s.cloudfront.net ping statistics ---\n\n4 packets transmitted, 4 received, 0% packet loss, time 3004ms\n\nrtt min/avg/max/mdev = 1.411/1.934/3.449/0.875 ms\n\nsh-4.2$\n\nExit the Session Manager session:\n\nexit\n\nChallenge 1\n\nYou can limit which VPCs can access the internet through the NAT gateway in VPC2 by modifying the route tables. Try adding a more specific route of 10.10.0.0/24 instead of the 0.0.0.0/0 destination for VPC3 to see how you can customize the internet egress sharing.\n\nChallenge 2\n\nYou may not want to allow VPC1 and VPC3 to be able to communicate with each other. Try adding a new transit gateway route table, updating the attachments to accomplish this.\n\nChallenge 3\n\nIn the solution, you deployed three VPCs each of /26 subnet size within the 10.10.0.0/24 supernet. There is room for an additional /26 subnet. Try adding an additional VPC with a /26 CIDR with subnets and route tables; then attach it to the transit gateway.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nTransit gateways allow you to quickly implement a multi-VPC hub-and-spoke network topology for your network in AWS. You may have had to use many peering connections to achieve similar results or used third-party software on instances in a transit VPC architecture. Transit gateway also supports cross-region peering of transit gateways and cross-account sharing via Resource Access Manager (RAM).\n\nWhen you attached your VPCs to the transit gateway, you used subnets in each AZ for resiliency. You also used dedicated “attachment” subnets for the VPC attachments. You can attach the transit gateway to any subnet(s) within your VPC. Using a dedicated subnet for these attachments gives you flexibility to granularly define subnets you choose to route to the TGW. That is, if you attached the private subnet, it would always have a route to the TGW; this might not be intended based on your use case. In your case, you configured routes for your private subnets to send all traffic to the transit gateway which enabled sharing of the NAT gateway and internet gateway; this results in cost savings over having to deploy multiple NAT gateways (e.g., one for each VPC).\n\nYou can connect your on-premises network or any virtual network directly to a transit gateway, as it acts as a hub for all of your AWS network traffic. You can connect IPsec VPNs, Direct\n\nConnect (DX), and third-party network appliances to the transit gateway to extend your AWS network to non-AWS networks. This also allows you to consolidate VPN connections and/or Direct Connect connections by connecting one directly to the transit gateway to access all of your VPCs in a Region. Border Gateway Protocol (BGP) is supported by TGW over these types of network extensions for dynamic route updates in both directions.\n\nChallenge\n\nCreate a fourth VPC and attach your TGW to subnets in it. Allow it to use the existing NAT gateway to reach the internet.\n\n2.11 Peering Two VPCs Together for Inter-VPC Network Communication\n\nProblem\n\nYou need to enable two instances in separate VPCs to communicate with each other in a simple and cost-effective manner.\n\nSolution\n\nRequest a peering connection between two VPCs, accept the peering connection, update the route tables for each VPC subnet, and finally test the connection from one instance to another (see Figure 2-15).\n\nFigure 2-15. Communication between instances in peered VPCs\n\nPrerequisites\n\nTwo VPCs, each with isolated subnets in two AZs and associated route tables\n\nIn each VPC, one EC2 instance that you can access for testing\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a VPC peering connection to connect VPC1 to VPC2:\n\nVPC_PEERING_CONNECTION_ID=$(aws ec2 create-vpc-peering-connection \\\n\n2.\n\n3.\n\n4.\n\nValidation checks\n\nGet instance 2’s IP:\n\n--vpc-id $VPC_ID_1 --peer-vpc-id $VPC_ID_2 --output text \\\n\n--query VpcPeeringConnection.VpcPeeringConnectionId)\n\nAccept the peering connection:\n\naws ec2 accept-vpc-peering-connection \\\n\n--vpc-peering-connection-id $VPC_PEERING_CONNECTION_ID\n\nNOTE\n\nVPC peering connections can be established from one AWS account to a different AWS account. If\n\nyou choose to peer VPCs across AWS accounts, you need to ensure you have the correct IAM\n\nconfiguration to create and accept the peering connection within each account.\n\nIn the route tables associated with each subnet, add a route to direct traffic destined for the peered VPC’s CIDR range to the VPC_PEERING_CONNECTION_ID:\n\naws ec2 create-route --route-table-id $VPC_SUBNET_RT_ID_1 \\\n\n--destination-cidr-block $VPC_CIDR_2 \\\n\n--vpc-peering-connection-id $VPC_PEERING_CONNECTION_ID\n\naws ec2 create-route --route-table-id $VPC_SUBNET_RT_ID_2 \\\n\n--destination-cidr-block $VPC_CIDR_1 \\\n\n--vpc-peering-connection-id $VPC_PEERING_CONNECTION_ID\n\nAdd an ingress rule to instance 2’s security group that allows ICMPv4 access from instance 1’s security group:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol icmp --port -1 \\\n\n--source-group $INSTANCE_SG_1 \\\n\n--group-id $INSTANCE_SG_2\n\naws ec2 describe-instances --instance-ids $INSTANCE_ID_2\\\n\n--output text \\\n\n--query Reservations[0].Instances[0].PrivateIpAddress\n\nEnsure your EC2 instance 1 has registered with SSM. Use this command to check the status:\n\naws ssm describe-instance-information \\\n\n--filters Key=ResourceType,Values=EC2Instance \\\n\n--query \"InstanceInformationList[].InstanceId\" --output text\n\nConnect to your EC2 instance by using SSM Session Manager:\n\naws ssm start-session --target $INSTANCE_ID_1\n\nPing instance 2 from instance 1:\n\nping -c 4 <<INSTANCE_IP_2>>\n\nOutput:\n\nPING 10.20.0.242 (10.20.0.242) 56(84) bytes of data.\n\n64 bytes from 10.20.0.242: icmp_seq=1 ttl=255 time=0.232 ms\n\n64 bytes from 10.20.0.242: icmp_seq=2 ttl=255 time=0.300 ms\n\n64 bytes from 10.20.0.242: icmp_seq=3 ttl=255 time=0.186 ms\n\n64 bytes from 10.20.0.242: icmp_seq=4 ttl=255 time=0.183 ms\n\n--- 10.20.0.242 ping statistics ---\n\n4 packets transmitted, 4 received, 0% packet loss, time 3059ms\n\nrtt min/avg/max/mdev = 0.183/0.225/0.300/0.048 ms\n\nExit the Session Manager session:\n\nexit\n\nTIP\n\nYou can search for a security group ID in the VPC console to show all security groups that reference others. You can also\n\nrun the aws ec2 describe-security-group-references CLI command to accomplish this. This is helpful in gaining insight\n\ninto which security groups reference others. You can reference security groups in peered VPCs owned by other AWS\n\naccounts but not located in other Regions.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nVPC peering connections are nontransitive. Each VPC needs to peer with every other VPC that they need to communicate with. This type of connection is ideal when you might have a VPC hosting shared services that other VPCs need to access, while not having the “spoke” VPCs communicate with one another.\n\nIn addition to the peering connections, you need to configure the route tables associated with the VPC subnets to send traffic destined for the peered VPC’s CIDR to the peering connection (PCX). In other words, to enable VPC1 to be able to communicate with VPC2, the destination route must be present in VPC1 and the return route also must be present in VPC2.\n\nIf you were to add a third VPC to this recipe, and you needed all VPCs to be able to communicate with one another, you would need to peer that third VPC with the previous two and update all of the VPC route tables accordingly to allow for all of the VPCs to have communication with one another. As you continue to add more VPCs to a network architecture like this, you may notice that the number of peering connections and route table updates required begin to increase exponentially. Because of this, transit gateway is a better choice for transitive VPC communication using transit gateway route tables.\n\nYou can use VPC peering cross-account if needed, and you can also reference security groups in peered VPCs in a similar way of referencing security groups within a single VPC. This allows you to use the same type of strategy with how you manage security groups across your AWS environment when using VPC peering.\n\nWARNING\n\nConnecting VPCs together requires nonoverlapping CIDR ranges in order for routing to work normally. The VPC route\n\ntables must include a specific route directing traffic destined for the peered VPC to the peering connection.\n\nChallenge\n\nVPC peering connections can be established across AWS Regions. Connect a VPC in another Region to the VPC you deployed in the Region used for the recipe.\n\nChapter 3. Storage\n\n3.0 Introduction Many industries have put a heavy emphasis on cloud data storage technologies to help facilitate increasing demands of data. Many options are available for data storage to suit your needs, with seemingly infinite scale. Even with many new storage options available in the cloud, Amazon S3 remains a powerful, fundamental building block for so many use cases. It is amazing to think that it was released more than 15 years ago. Over time, many features have been added and new storage services launched. Multiple storage options are available to meet security requirements (e.g., key management service [KMS] encryption) while reducing costs (e.g., S3 Intelligent- Tiering). Ensuring that data is secured and available is a challenge that all developers and architects face.\n\nThe storage services available on AWS allow for integration with other AWS services to provide ways for developers and application architects who integrate with many AWS services. These services can also be used to replace legacy storage systems you run and operate with on- premises environments. For example:\n\nS3 can be used to automatically invoke Lambda functions on object operations like upload.\n\nEFS can be used with EC2 to replace existing shared file systems provided by Network File System (NFS) servers.\n\nFSx for Windows can be used to replace Windows-based file servers for your EC2 workloads.\n\nEBS replaces Fibre Channel and Internet Small Computer Systems Interface (iSCSI) targets by providing block devices, and it offers many throughput options to meet performance requirements.\n\nIn this chapter, you will use some of these services so that you can start building intelligent, scalable, and secure systems that have the potential to minimize costs and operational overhead.\n\nWorkstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Storage",
      "page_number": 123
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 138-145)",
      "start_page": 138,
      "end_page": 145,
      "detection_method": "topic_boundary",
      "content": "3.1 Using S3 Lifecycle Policies to Reduce Storage Costs\n\nProblem\n\nYou need to transition infrequently accessed objects to a more cost-effective storage tier without impacting performance or adding operational overhead.\n\nSolution\n\nCreate an S3 Lifecycle rule to transition objects to the S3 Infrequent Access (IA) storage class after a predefined time period of 30 days. Then apply this Lifecycle policy to your S3 bucket (see Figure 3-1).\n\nFigure 3-1. S3 Lifecycle rule configuration\n\nPrerequisite\n\nAn S3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a lifecycle-rule.json file (provided in the repository) to use as the Lifecycle policy that you will apply to your S3 bucket:\n\n{\n\n\"Rules\": [\n\n{\n\n\"ID\": \"Move all objects to Infrequent Access\",\n\n\"Prefix\": \"\",\n\n\"Status\": \"Enabled\",\n\n\"Transitions\": [\n\n{\n\n\"Date\": \"2015-11-10T00:00:00.000Z\",\n\n\"Days\": 30,\n\n\"StorageClass\": \"INFREQUENTLY_ACCESSED\"\n\n}\n\n]\n\n}\n\n]\n\n}\n\n2.\n\nApply the Lifecycle rule configuration:\n\naws s3api put-bucket-lifecycle-configuration \\\n\n--bucket awscookbook301-$RANDOM_STRING \\\n\n--lifecycle-configuration file://lifecycle-rule.json\n\nNOTE\n\nA Lifecycle rule helps automate the transition to a different storage class for some or all objects within\n\na bucket (prefixes, tags, and object names can all be used as filters for Lifecycle rules). For a complete\n\nlist of Lifecycle rule capabilities, see the documentation.\n\nValidation checks\n\nGet the Lifecycle configuration for your bucket:\n\naws s3api get-bucket-lifecycle-configuration \\\n\n--bucket awscookbook301-$RANDOM_STRING\n\n(Optional) Copy an object to the bucket:\n\naws s3 cp book_cover.png s3://awscookbook301-$RANDOM_STRING\n\nCheck the storage class for the object:\n\naws s3api list-objects-v2 --bucket awscookbook301-$RANDOM_STRING\n\nYou should see output similar to the following:\n\n{\n\n\"Contents\": [\n\n{\n\n\"Key\": \"book_cover.png\",\n\n\"LastModified\": \"2021-06-16T02:30:06+00:00\",\n\n\"ETag\": \"\\\"d...9\\\"\",\n\n\"Size\": 255549,\n\n\"StorageClass\": \"STANDARD\"\n\n}\n\n]\n\n}\n\nYou will see after 30 days that the storage class for the object is STANDARD_IA after running the same command.\n\nTIP\n\n“Days” in the Transition action must be greater than or equal to 30 for StorageClass STANDARD_IA. Other storage tiers allow for shorter transition times to meet your requirements. For a list of all of the storage classes available with transition\n\ntimes for Lifecycle rules, see the support document.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen you upload objects to an S3 bucket, if you do not specify the storage class, the default Standard storage class is used. Amazon S3 has multiple storage classes available that can be more cost-effective for long-term storage while also suiting your performance and resiliency requirements. If you cannot change your application to specify storage tiers for object uploads, Lifecycle rules can help automate the transition to your desired storage class. Lifecycle rules can be applied to some or all objects within a bucket with a filter.\n\nAs the name may imply, S3 Infrequent Access is a storage class that provides reduced cost (compared to the S3 Standard storage class) for data stored for objects that you rarely access. This provides the same level of redundancy for your data within a Region for a reduced cost, but the cost associated with accessing objects is slightly higher. If your data access patterns are unpredictable, and you would still like to optimize your S3 storage for cost, performance, and resiliency, take a look at S3 Intelligent-Tiering in the next recipe.\n\nChallenge 1\n\nConfigure the Lifecycle rule to apply only to objects based on object-level tags.\n\nChallenge 2\n\nConfigure the Lifecycle rule to transition objects to a Deep Archive.\n\n3.2 Using S3 Intelligent-Tiering Archive Policies to Automatically Archive S3 Objects\n\nProblem\n\nYou need to automatically transition infrequently accessed objects to a different archive storage class without impacting performance or adding operational overhead.\n\nSolution\n\nCreate a policy to automate the archival of S3 objects to the S3 Glacier archive based on access patterns for objects that are more than 90 days old. Apply it to your S3 bucket, as shown in Figure 3-2.\n\nFigure 3-2. S3 Intelligent-Tiering archive\n\nPrerequisite\n\nAn S3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file named tiering.json for the configuration (file provided in the repository):\n\n{\n\n\"Id\": \"awscookbook302\",\n\n\"Status\": \"Enabled\",\n\n\"Tierings\": [\n\n{\n\n\"Days\": 90,\n\n\"AccessTier\": \"ARCHIVE_ACCESS\"\n\n}\n\n]\n\n}\n\n2.\n\nApply the Intelligent-Tiering configuration:\n\naws s3api put-bucket-intelligent-tiering-configuration \\\n\n--bucket awscookbook302-$RANDOM_STRING \\\n\n--id awscookbook302 \\\n\n--intelligent-tiering-configuration \"$(cat tiering.json)\"\n\nWARNING\n\nEnsure that your use case and applications can support the increased retrieval time associated with the\n\nS3 Glacier archive storage tier. You can configure your application to use an expedited retrieval\n\nmechanism supported by S3 Glacier archive to decrease the retrieval time but increase cost. For a\n\ncomplete list of archive times and how to configure expedited access, please refer to the support\n\ndocument.\n\nValidation checks\n\nGet the configuration of Intelligent-Tiering for your bucket:\n\naws s3api get-bucket-intelligent-tiering-configuration \\\n\n--bucket awscookbook302-$RANDOM_STRING \\\n\n--id awscookbook302\n\nCopy an object to the bucket:\n\naws s3 cp ./book_cover.png s3://awscookbook302-$RANDOM_STRING\n\nCheck the storage class for the object:\n\naws s3api list-objects-v2 --bucket awscookbook302-$RANDOM_STRING\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAn S3 Intelligent-Tiering archive provides an automatic mechanism to transition “cool” (less frequently accessed) objects to an S3 Glacier archive. You can define the length of time required for an object to transition to the archive (between 90 and 730 days). This feature helps with meeting long-term retention requirements that you may have for compliance. The storage tiers available within S3 Intelligent-Tiering map directly to S3 tiers: Frequent Access\n\nOptimized for frequent access (S3 Standard)\n\nInfrequent Access\n\nOptimized for infrequent access (S3 Standard-IA)\n\nArchive Access\n\nArchive purposes (S3 Glacier)\n\nDeep Archive Access\n\nLong-term retention purposes (S3 Glacier Deep Archive)\n\nThis archive configuration is separate from the main S3 Intelligent-Tiering tier configuration that you place on objects, as this is a bucket-specific configuration. In the previous recipe, you configured a Lifecycle rule to configure all objects within a bucket to transition to the S3 Intelligent-Tiering storage tier. This recipe adds additional configuration to transition objects to S3 archive tiers based on your configuration. You can use either of these methods separately or both concurrently to meet your own requirements.\n\nNOTE\n\nS3 tiers are object-specific, which differs from the Intelligent-Tiering archive being bucket-specific. You can filter an\n\narchive configuration to apply only to certain prefixes, object tags, and object names if you wish to include or exclude\n\nobjects in a configuration. For more information, see the support document.\n\nChallenge 1\n\nConfigure the Intelligent-Tiering archive to send objects that are older than one year to the Glacier Deep Archive tier.\n\nChallenge 2\n\nConfigure the Intelligent-Tiering archive to use object-level tags and configure an object with a tag that matches your configuration.\n\n3.3 Replicating S3 Buckets to Meet Recovery Point Objectives\n\nProblem\n\nYour company’s data security policy mandates that objects be replicated within the same Region to meet a recovery point objective of 15 minutes.\n\nSolution\n\nFirst, create source and destination S3 buckets with versioning enabled. Then create an IAM role and attach an IAM policy that allows S3 to copy objects from the source to the destination bucket. Finally, create an S3 replication policy that references the IAM role, and apply that policy to the source bucket, as shown in Figure 3-3.",
      "page_number": 138
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 146-159)",
      "start_page": 146,
      "end_page": 159,
      "detection_method": "topic_boundary",
      "content": "Figure 3-3. S3 bucket replication\n\nPrerequisite\n\nAn S3 bucket with versioning enabled that you will use as your source\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\n2.\n\n3.\n\n4.\n\n5.\n\nCreate the destination S3 bucket:\n\naws s3api create-bucket --bucket awscookbook303-dst-$RANDOM_STRING\n\nEnable versioning for the destination S3 bucket:\n\naws s3api put-bucket-versioning \\\n\n--bucket awscookbook303-dst-$RANDOM_STRING \\\n\n--versioning-configuration Status=Enabled\n\nCreate a file named s3-assume-role-policy.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"s3.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate an IAM role using the statement in the provided assume-role- policy.json file using this command:\n\nROLE_ARN=$(aws iam create-role --role-name AWSCookbook303S3Role \\\n\n--assume-role-policy-document file://s3-assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\nCreate a file named s3-perms-policy-template.json with the following content (file provided in the repository) to allow S3 replication access to\n\n6.\n\nyour source and destination buckets:\n\n{\n\n\"Version\":\"2012-10-17\",\n\n\"Statement\":[\n\n{\n\n\"Effect\":\"Allow\",\n\n\"Action\":[\n\n\"s3:GetObjectVersionForReplication\",\n\n\"s3:GetObjectVersionAcl\",\n\n\"s3:GetObjectVersionTagging\"\n\n],\n\n\"Resource\":[\n\n\"arn:aws:s3:::SRCBUCKET/*\"\n\n]\n\n},\n\n{\n\n\"Effect\":\"Allow\",\n\n\"Action\":[\n\n\"s3:ListBucket\",\n\n\"s3:GetReplicationConfiguration\"\n\n],\n\n\"Resource\":[\n\n\"arn:aws:s3:::SRCBUCKET\"\n\n]\n\n},\n\n{\n\n\"Effect\":\"Allow\",\n\n\"Action\":[\n\n\"s3:ReplicateObject\",\n\n\"s3:ReplicateDelete\",\n\n\"s3:ReplicateTags\",\n\n\"s3:GetObjectVersionTagging\"\n\n],\n\n\"Resource\":\"arn:aws:s3:::DSTBUCKET/*\"\n\n}\n\n]\n\n}\n\nReplace the values for DSTBUCKET and SRCBUCKET in the file and save it as s3-perms-policy.json:\n\nsed -e \"s/DSTBUCKET/awscookbook303-dst-${RANDOM_STRING}/g\" \\\n\n7.\n\n8.\n\ne \"s|SRCBUCKET|awscookbook303-src-${RANDOM_STRING}|g\" \\\n\ns3-perms-policy-template.json > s3-perms-policy.json\n\nAttach the policy to the role you just created:\n\naws iam put-role-policy \\\n\n--role-name AWSCookbook303S3Role \\\n\n--policy-document file://s3-perms-policy.json \\\n\n--policy-name S3ReplicationPolicy\n\nCreate a file named s3-replication-template.json with the following content to configure a replication time of 15 minutes to your destination bucket:\n\n{\n\n\"Rules\": [\n\n{\n\n\"Status\": \"Enabled\",\n\n\"Filter\": {\n\n\"Prefix\": \"\"\n\n},\n\n\"Destination\": {\n\n\"Bucket\": \"arn:aws:s3:::DSTBUCKET\",\n\n\"Metrics\": {\n\n\"Status\": \"Enabled\",\n\n\"EventThreshold\": {\n\n\"Minutes\": 15\n\n}\n\n},\n\n\"ReplicationTime\": {\n\n\"Status\": \"Enabled\",\n\n\"Time\": {\n\n\"Minutes\": 15\n\n}\n\n}\n\n},\n\n\"DeleteMarkerReplication\": {\n\n\"Status\": \"Disabled\"\n\n},\n\n\"Priority\": 1\n\n}\n\n],\n\n\"Role\": \"ROLEARN\"\n\n}\n\n9.\n\nReplace the values for DSTBUCKET and ROLEARN in the file and save it as s3- replication.json:\n\nsed -e \"s|ROLEARN|${ROLE_ARN}|g\" \\\n\ne \"s|DSTBUCKET|awscookbook303-dst-${RANDOM_STRING}|g\" \\\n\ns3-replication-template.json > s3-replication.json\n\n10.\n\nConfigure the replication policy for the source S3 bucket:\n\naws s3api put-bucket-replication \\\n\n--replication-configuration file://s3-replication.json \\\n\n--bucket awscookbook303-src-${RANDOM_STRING}\n\nValidation checks\n\nView the replication configuration for the source bucket:\n\naws s3api get-bucket-replication \\\n\n--bucket awscookbook303-src-${RANDOM_STRING}\n\nCopy an object to the source bucket:\n\naws s3 cp ./book_cover.png s3://awscookbook303-src-$RANDOM_STRING\n\nView the replication status for the file that you uploaded to the source bucket:\n\naws s3api head-object --bucket awscookbook303-src-${RANDOM_STRING} \\\n\n--key book_cover.png\n\nYou should see output similar to the following:\n\n{\n\n\"AcceptRanges\": \"bytes\",\n\n\"LastModified\": \"2021-06-20T00:17:25+00:00\",\n\n\"ContentLength\": 255549,\n\n\"ETag\": \"\\\"d<<>>d\\\"\",\n\n\"VersionId\": \"I<>>X\",\n\n\"ContentType\": \"image/png\",\n\n\"Metadata\": {},\n\n\"ReplicationStatus\": \"PENDING\"\n\n}\n\nView the replication status after 15 minutes and confirm that ReplicationStatus is COMPLETED, similar to the following:\n\n{\n\n\"AcceptRanges\":\"bytes\",\n\n\"ContentType\":\"image/png\",\n\n\"LastModified\":\"2021-06-20T00:17:41+00:00\",\n\n\"ContentLength\":255549,\n\n\"ReplicationStatus\":\"COMPLETED\",\n\n\"VersionId\":\"I<>>X\",\n\n\"ETag\":\\\"d<<>>d\\\"\",\n\n\"Metadata\":{}\n\n}\n\nTIP\n\nYou can also view the replication metrics in the AWS Console.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nIf you are an engineer, developer, or architect working on AWS, there is a good chance you will end up using S3. You may have to implement some sort of replication on S3 for your applications; S3 offers two types of replication to meet your specific needs: Same-Region Replication (SRR) and Cross-Region Replication (CRR). The replication time is a configurable parameter of S3 Replication Time Control (S3 RTC) and is documented to meet a 15-minute recovery point objective (RPO) backed by a service level agreement (SLA).\n\nSRR uses an IAM role, a source and destination bucket, and a replication configuration that references the role and buckets. You use SRR in this recipe to configure a one-way replication; you can use SRR to facilitate many types of use cases:\n\nLog aggregation to a central bucket for indexing\n\nReplication of data between production and test environments\n\nData redundancy while retaining object metadata\n\nDesigning redundancy around data-sovereignty and compliance requirements\n\nBackup and archival purposes\n\nCRR uses a similar IAM role, a source and destination bucket, and a replication configuration that references the role and buckets. You can use CRR to extend the possibilities of what SRR enables:\n\nMeet requirements for data storage and archival across Regions\n\nLocate similar datasets closer to your regional compute and access needs to reduce latency\n\nNOTE\n\nS3 buckets that have versioning add markers to objects that you have deleted. Both types of S3 replication are able to\n\nreplicate delete markers to your target bucket if you choose. For more information, see the support document.\n\nChallenge 1\n\nCreate an S3 bucket in another Region and replicate the source bucket to that as well.\n\nChallenge 2\n\nYou can replicate specific paths and prefixes using a filter. Apply a filter so that only objects under a certain prefix (e.g., protected/) are replicated.\n\n3.4 Observing S3 Storage and Access Metrics Using Storage Lens\n\nProblem\n\nYou need to gain observability into the usage patterns of your S3 buckets.\n\nSolution\n\nConfigure S3 Storage Lens to provide observability and analytics about your S3 usage, as shown in Figure 3-4.\n\nFigure 3-4. Configuring S3 Storage Lens for S3 observability\n\nPrerequisite\n\nS3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\nNOTE\n\nPer the documentation: You can’t use your account’s root user credentials to view Amazon S3 Storage Lens dashboards.\n\n1.\n\nFrom the S3 console, select S3 Storage Lens from the navigation pane on the left.\n\n2.\n\nClick “Create dashboard.”\n\nNOTE\n\nAll AWS accounts have a default dashboard associated with them that shows the free metrics available\n\nthrough S3 Storage Lens. Enabling advanced metrics gives you deeper insights into your S3 usage and\n\nalso provides cost-savings recommendations you can take action on to optimize for cost. You can use\n\nthe default dashboard and/or create your own. The rest of these steps will show you how to create your\n\nown.\n\n3.\n\nGive your dashboard a name, as shown in Figure 3-5.\n\n4.\n\n5.\n\nFigure 3-5. S3 Storage Lens dashboard creation\n\nInclude all of your buckets and Regions (use the default values) for the “Dashboard scope” (see Figure 3-6).\n\nFigure 3-6. Dashboard scope\n\nEnable “Advanced metrics and recommendations,” keeping the default values, as shown in Figure 3-7.\n\n6.\n\nFigure 3-7. Selecting advanced metrics\n\nChoose the defaults for Export settings (no export).\n\nTIP\n\nYou can enable an automated export to periodically export your metrics to CSV and Apache Parquet\n\nformats and send them to an S3 bucket of your choice to run your own reports and visualizations.\n\n7.\n\nClick “Create dashboard,” and then view your dashboard from the dashboard selection.\n\nNOTE\n\nIt may take up to 48 hours for advanced metrics to begin accumulating for your usage and access\n\npatterns. In the meantime, you can view the default dashboard for the free metrics associated with\n\nyour S3 usage for all of your buckets in your account.\n\nValidation checks\n\nOpen the Storage Lens and view the dashboard that you configured. You should see metrics that correspond to your S3 usage. A sample is shown in Figure 3-8.\n\nFigure 3-8. Sample S3 Storage Lens dashboard\n\nTIP\n\nYou can drill down into “Cost efficiency” and “Data protection” metrics from the dashboard. After some time, you will be\n\nable to view historical data that allows you to take action on moving your objects to storage tiers that meet your needs for\n\ndata access patterns and availability requirements.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nS3 was one of the first AWS services, and as a result, many customers have been using S3 for a very long time. As customer storage usage grew exponentially, the ability to analyze what is being stored became a clear, desired capability. S3 Storage Lens gives you the ability to “see” into your S3 usage for your AWS accounts. Analyzing bucket usage, observing storage costs, and discovering anomalies (e.g., undeleted multipart upload fragments) are just a few of the many use cases S3 Storage Lens provides.\n\nWith Storage Lens, you can discover where your objects are being stored with a visual dashboard backed by a powerful analytics engine so that you can make adjustments to optimize for cost without impacting performance. You can also enable advanced metrics on your dashboard to gain deeper insights and cost-savings recommendations for your S3 buckets.\n\nNOTE\n\nS3 Storage Lens uses metrics to help you visualize your usage and activity. There are free metrics available and advanced\n\nmetrics that also give you recommendations on your usage. For more information about the different types of metrics and\n\ntheir associated costs, see the support document.\n\nChallenge 1\n\nUse Storage Lens findings to observe metrics and set an alert to continuously monitor your usage.\n\nChallenge 2\n\nCreate a new storage lens configuration that looks at only specific buckets.\n\n3.5 Configuring Application-Specific Access to S3 Buckets with S3 Access Points\n\nProblem\n\nYou have an S3 bucket and two applications. You need to grant read/write access to one of your applications and read-only access to another application. You do not want to use S3 bucket policies, as you expect to have to add additional applications with fine-grained security requirements in the future.\n\nSolution\n\nCreate two S3 access points and apply a policy granting the S3:PutObject and S3:GetObject actions to one of the access points and S3:GetObject to the other access point. Then, configure your application to use the respective access point DNS name (see Figure 3-9).\n\nFigure 3-9. S3 access points for two applications using the same S3 bucket\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nTwo EC2 instances deployed. You will need the ability to connect to test access.\n\nS3 bucket.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps",
      "page_number": 146
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 160-168)",
      "start_page": 160,
      "end_page": 168,
      "detection_method": "topic_boundary",
      "content": "1.\n\n2.\n\n3.\n\n4.\n\nIn your VPC, create an access point for Application 1:\n\naws s3control create-access-point --name cookbook305-app-1 \\\n\n--account-id $AWS_ACCOUNT_ID \\\n\n--bucket $BUCKET_NAME --vpc-configuration VpcId=$VPC_ID\n\nIn your VPC, create an access point for Application 2:\n\naws s3control create-access-point --name cookbook305-app-2 \\\n\n--account-id $AWS_ACCOUNT_ID \\\n\n--bucket $BUCKET_NAME --vpc-configuration VpcId=$VPC_ID\n\nCreate a file named app-1-policy-template.json with the access point policy for Application 1 to read/write with the following content (file provided in the repository):\n\n{\n\n\"Version\":\"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"AWS\": \"EC2_INSTANCE_PROFILE\"\n\n},\n\n\"Action\": [ACTIONS],\n\n\"Resource\":\n\n\"arn:aws:s3:AWS_REGION:AWS_ACCOUNT_ID:accesspoint/ACCESS_POINT_NAME/object/*\"\n\n}]\n\n}\n\nUse the sed command to replace the values in app-policy-template.json with your EC2_INSTANCE_PROFILE, AWS_REGION, AWS_ACCOUNT_ID, ACCESS_POINT_NAME, and ACTIONS values for Application 1:\n\nsed -e \"s/AWS_REGION/${AWS_REGION}/g\" \\\n\ne \"s|EC2_INSTANCE_PROFILE|${INSTANCE_ROLE_1}|g\" \\\n\ne \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\ne \"s|ACCESS_POINT_NAME|cookbook305-app-1|g\" \\\n\n5.\n\n6.\n\n7.\n\ne \"s|ACTIONS|\\\"s3:GetObject\\\",\\\"s3:PutObject\\\"|g\" \\\n\napp-policy-template.json > app-1-policy.json\n\nPut the policy you created on the access point for Application 1:\n\naws s3control put-access-point-policy --account-id $AWS_ACCOUNT_ID \\\n\n--name cookbook305-app-1 --policy file://app-1-policy.json\n\nUse the sed command to replace the values in app-policy-template.json with your EC2_INSTANCE_PROFILE, AWS_REGION, AWS_ACCOUNT_ID, ACCESS_POINT_NAME, and ACTIONS values for Application 2:\n\nsed -e \"s/AWS_REGION/${AWS_REGION}/g\" \\\n\ne \"s|EC2_INSTANCE_PROFILE|${INSTANCE_ROLE_2}|g\" \\\n\ne \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\ne \"s|ACCESS_POINT_NAME|cookbook305-app-2|g\" \\\n\ne \"s|ACTIONS|\\\"s3:GetObject\\\"|g\" \\\n\napp-policy-template.json > app-2-policy.json\n\nPut the policy you created on the access point for Application 2:\n\naws s3control put-access-point-policy --account-id $AWS_ACCOUNT_ID \\\n\n--name cookbook305-app-2 --policy file://app-2-policy.json\n\nNOTE\n\nYou can use specific access points with AWS SDK and CLI in a similar way. For example, the bucket\n\nname becomes the following for SDK usage: https://[access_point_name]-[accountID].s3-\n\naccesspoint.[region].amazonaws.com for URLs and arn:aws:s3:[region]:[accountID]: [access_point_name] as “bucket name” for CLI usage.\n\nHere is a CLI example:\n\naws s3api get-object --key object.zip \\\n\n--bucket \\\n\narn:aws:s3:us-east-1:111111111111:access_point_name object.zip\n\n8.\n\nFollow this guide to modifying the bucket policy so that you delegate control to the access points.\n\nValidation checks\n\nConnect to the EC2 instance 1 by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID_1\n\nSet your AWS account ID from the instance’s metadata:\n\nexport AWS_ACCOUNT_ID=$(curl --silent http://169.254.169.254/latest/dynamic/instance-identity/document\n\n\\\n\n| awk -F'\"' ' /accountId/ {print $4}')\n\nSet the Region by grabbing the value from the instance’s metadata:\n\nexport AWS_DEFAULT_REGION=$(curl --silent http://169.254.169.254/latest/dynamic/instance-\n\nidentity/document \\\n\n| awk -F'\"' ' /region/ {print $4}')\n\nTry to get an object from the S3 access point for Application 1:\n\naws s3api get-object --key Recipe305Test.txt \\\n\n--bucket arn:aws:s3:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:accesspoint/cookbook305-app-1 \\\n\n/tmp/Recipe305Test.txt\n\nWrite an object to the S3 access point for Application 1:\n\naws s3api put-object \\\n\n--bucket arn:aws:s3:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:accesspoint/cookbook305-app-1 \\\n\n--key motd.txt --body /etc/motd\n\nNOTE\n\nThese two commands work for Application 1 because you configured read/write access for this access point.\n\nDisconnect from EC2 instance 1:\n\nexit\n\nConnect to the EC2 instance 2 using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID_2\n\nSet your AWS account ID from the instance’s metadata:\n\nexport AWS_ACCOUNT_ID=$(curl --silent http://169.254.169.254/latest/dynamic/instance-identity/document\n\n\\\n\n| awk -F'\"' ' /accountId/ {print $4}')\n\nSet the Region by grabbing the value from the instance’s metadata:\n\nexport AWS_DEFAULT_REGION=$(curl --silent http://169.254.169.254/latest/dynamic/instance-\n\nidentity/document \\\n\n| awk -F'\"' ' /region/ {print $4}')\n\nTry to get an object from the S3 bucket:\n\naws s3api get-object --key Recipe305Test.txt \\\n\n--bucket \\\n\narn:aws:s3:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:accesspoint/cookbook305-app-2 \\\n\n/tmp/Recipe305Test.txt\n\nTry to put an object to the S3 bucket:\n\naws s3api put-object \\\n\n--bucket arn:aws:s3:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:accesspoint/cookbook305-app-2 \\\n\n--key motd2.txt --body /etc/motd\n\nNOTE\n\nThe first command works, and the second command fails, for Application 2 because you configured read-only access for\n\nthis access point.\n\nDisconnect from EC2 instance 2:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nS3 access points allow you to grant fine-grained access to specific principals, and they can be easier to manage than S3 bucket policies. In this recipe, you created two access points with different kinds of allowed actions and associated the access points with specific roles using access point IAM policies. You verified that only specific actions were granted to your EC2 instances when they were being used with the CLI and S3 access point.\n\nTo help you meet your security requirements, access points use IAM policies in a similar way that you use for other AWS services. You can also configure S3 Block Public Access on access points to ensure that no public access is ever granted by mistake to your S3 buckets (see Recipe 1.9). There is no additional cost for S3 access points.\n\nChallenge\n\nConfigure a third access point and specify access to a specific object or prefix only.\n\n3.6 Using Amazon S3 Bucket Keys with KMS to Encrypt Objects\n\nProblem\n\nYou need to encrypt S3 objects at rest with a Key Management Service (KMS) customer- managed key (CMK) and ensure that all objects within the bucket are encrypted with the KMS\n\nkey in a cost-effective manner.\n\nSolution\n\nCreate a KMS customer-managed key, configure your S3 bucket to use S3 bucket keys referencing your AWS KMS CMK, and configure an S3 bucket policy requiring KMS to be used for all S3:PutObject operations (see Figure 3-10).\n\nFigure 3-10. Encrypting objects in S3\n\nPrerequisite\n\nAn S3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a KMS key to use for your S3 bucket and store the key ID in an environment variable:\n\nKEY_ID=$(aws kms create-key \\\n\n--tags TagKey=Name,TagValue=AWSCookbook306Key \\\n\n--description \"AWSCookbook S3 CMK\" \\\n\n--query KeyMetadata.KeyId \\\n\n--output text)\n\n2.\n\nCreate an alias to reference your key:\n\naws kms create-alias \\\n\n--alias-name alias/awscookbook306 \\\n\n--target-key-id $KEY_ID\n\n3.\n\nConfigure the S3 bucket to use an S3 bucket key specifying your KMS key ID:\n\naws s3api put-bucket-encryption \\\n\n--bucket awscookbook306-$RANDOM_STRING \\\n\n--server-side-encryption-configuration '{\n\n\"Rules\": [\n\n{\n\n\"ApplyServerSideEncryptionByDefault\": {\n\n\"SSEAlgorithm\": \"aws:kms\",\n\n\"KMSMasterKeyID\": \"${KEY_ID}\"\n\n},\n\n\"BucketKeyEnabled\": true\n\n}\n\n]\n\n}'\n\n4.\n\nCreate a bucket policy template for the bucket to force encryption of all objects:\n\n{\n\n\"Version\":\"2012-10-17\",\n\n\"Id\":\"PutObjectPolicy\",\n\n\"Statement\":[{\n\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\n\n\"Effect\":\"Deny\",\n\n\"Principal\":\"*\",\n\n\"Action\":\"s3:PutObject\",\n\n\"Resource\":\"arn:aws:s3:::BUCKET_NAME/*\",\n\n\"Condition\":{\n\n\"StringNotEquals\":{\n\n\"s3:x-amz-server-side-encryption\":\"aws:kms\"\n\n}\n\n}\n\n}\n\n]\n\n}\n\n5.\n\nUse the sed command to replace the value in bucket-policy-template.json with your BUCKET_NAME:\n\nsed -e \"s|BUCKET_NAME|awscookbook306-${RANDOM_STRING}|g\" \\\n\nbucket-policy-template.json > bucket-policy.json\n\n6.\n\nApply the bucket policy to force encryption on all uploads:\n\naws s3api put-bucket-policy --bucket awscookbook306-$RANDOM_STRING \\\n\n--policy file://bucket-policy.json\n\nValidation checks\n\nUpload an object to the S3 bucket with encryption from the command line. You will see a successful upload:\n\naws s3 cp ./book_cover.png s3://awscookbook306-$RANDOM_STRING \\\n\n--sse aws:kms --sse-kms-key-id $KEY_ID\n\nNow, upload an object to the S3 bucket without encryption. You will notice that you receive a KMS.NotFoundException error on the command line. This indicates that the bucket policy you\n\nconfigured is working properly:\n\naws s3 cp ./book_cover.png s3://awscookbook306-$RANDOM_STRING\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen applying encryption to your S3 bucket, you could have chosen to use an AWS-managed CMK that Amazon S3 creates in your AWS account and manages for you. Like the customer- managed CMK, your AWS managed CMK is unique to your AWS account and Region. Only Amazon S3 has permission to use this CMK on your behalf. You can create, rotate, and disable auditable customer-managed CMKs from the AWS KMS Console. The S3 documentation provides a comprehensive explanation of the differences between the types of encryption supported on S3.\n\nWhen you encrypt your data, your data is protected, but you have to protect your encryption key. Envelope encryption is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key, as shown in Figure 3-11.\n\nFigure 3-11. S3 encryption process with KMS",
      "page_number": 160
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 169-177)",
      "start_page": 169,
      "end_page": 177,
      "detection_method": "topic_boundary",
      "content": "Challenge\n\nTo validate that you can rotate your keys without impacting your data, put an object into your bucket, trigger a rotation of the KMS CMK, and then get the object back out of the S3 bucket and confirm it can decrypt properly (see this AWS article for a hint).\n\n3.7 Creating and Restoring EC2 Backups to Another Region Using AWS Backup\n\nProblem\n\nYou need to create a backup of an instance and restore it in another Region.\n\nSolution\n\nCreate an on-demand backup with AWS Backup for your EC2 instance and restore the backup from the vault by using the AWS Console, as shown in Figure 3-12.\n\nFigure 3-12. Creating and restoring EC2 backups\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables\n\nEC2 instance deployed\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nNavigate to the AWS Backup console and select “Protected resources” from the lefthand navigation pane.\n\n2.\n\nClick the “Create an on-demand backup” and select your EC2 instance, choose defaults, and click “Create on-demand backup” (see Figure 3-13).\n\n3.\n\nFigure 3-13. Creating on-demand backup\n\nNOTE\n\nAWS Backup will create a role for its purposes that uses the AWS Backup Managed Policy to perform\n\nthe required actions for backups. You can also create your own custom role if you require. For more\n\ninformation, see this AWS document.\n\nThe backup starts in the backup jobs view, as shown in Figure 3-14.\n\nFigure 3-14. View of backup job running\n\nWait for the backup to complete in your account (this may take a few moments to reach the Completed status shown in Figure 3-15).\n\n4.\n\n5.\n\nFigure 3-15. View of backup job completed\n\nSelect the Default Backup vault for your current Region and view your image backup that you just completed.\n\nClick the backup recovery point that you just created and choose Copy, as shown in Figure 3-16.\n\n6.\n\nFigure 3-16. Copy recovery point\n\nSelect your destination Region, keep all defaults, and click Copy (shown in Figure 3-17). You will see the copy job enter Running status, as shown in Figure 3-18.\n\n7.\n\n8.\n\nFigure 3-17. Copy recovery point to a different AWS Region\n\nFigure 3-18. Copy recovery point running\n\nAfter the copy job has completed in a few minutes, in the AWS Console, select the destination Region from the drop-down Region selector (top right of the AWS console).\n\nSelect your default Backup vault and choose the backup you wish to restore, as shown in Figure 3-19.\n\n9.\n\nFigure 3-19. Restore recovery point\n\nUnder “Network settings,” select your instance type and VPC for your restore, and click “Restore backup.” An example of inputs is shown in Figure 3-20.\n\nFigure 3-20. Network settings for backup restoration\n\n10.\n\nYou can monitor the progress of the restore under the “Restore jobs” tab of the Jobs section in the console, shown in Figure 3-21.\n\nFigure 3-21. Recovery point restore job running\n\nValidation checks\n\nBrowse to the EC2 console to view your running instance in your destination Region. This EC2 instance is a copy of your original instance.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAWS Backup lets you manage and monitor the backups across the AWS services you use, from a single place. You can back up many AWS services and set backup policies for cloud resources in the AWS services that you use. You can also copy backups cross-Region within your account (or to other AWS accounts), which is what you explored in this recipe. EBS snapshots are an essential component of a backup strategy on AWS if you use the EC2 service to run instances with persistent data on them that you would like to protect. You can take snapshots of EBS volumes manually, write your own automation, automate them with Data Lifecycle Manager, or use AWS Backup.\n\nWhen you use AWS Backup to back up an EC2 instance, the service stores backups in a backup vault of your choice (a default backup vault is created if you do not have one), handles the process of building an Amazon Machine Image (AMI) which contains all of the configuration parameters, backed-up attached EBS volumes, and metadata. The service stores the entire bundle within the backup vault. This allows you to simply launch an EC2 instance from the AMI that was generated by the AWS Backup service to reduce the recovery time objective (RTO) associated with restoring an instance from a backup within your primary Region or another Region of your choice.\n\nChallenge\n\nConfigure an Automated Backup of the EC2 instance on a weekly schedule using backup plans, which copies the backed-up instances to a vault within another Region.\n\n3.8 Restoring a File from an EBS Snapshot\n\nProblem\n\nYou need to restore a file from an EBS snapshot that you have taken from a volume in your account.\n\nSolution\n\nCreate a volume from a snapshot, mount the volume from an EC2 instance, and copy the file to your instance volume (see Figure 3-22).",
      "page_number": 169
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 178-188)",
      "start_page": 178,
      "end_page": 188,
      "detection_method": "topic_boundary",
      "content": "Figure 3-22. Process flow for file restore from a snapshot\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nEC2 instance deployed. You will need the ability to connect to mount the EBS snapshot and restore a file.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nFind the EBS volume attached to your EC2 instance:\n\nORIG_VOLUME_ID=$(aws ec2 describe-volumes \\\n\n--filters Name=attachment.instance-id,Values=$INSTANCE_ID \\\n\n--output text \\\n\n--query Volumes[0].Attachments[0].VolumeId)\n\n2.\n\nTake a snapshot of the EBS volume (this will take a moment to complete):\n\nSNAPSHOT_ID=$(aws ec2 create-snapshot \\\n\n--volume-id $ORIG_VOLUME_ID \\\n\n--output text --query SnapshotId)\n\n3.\n\nCreate a volume from the snapshot and save the VOLUME_ID as an environment variable:\n\nSNAP_VOLUME_ID=$(aws ec2 create-volume \\\n\n--snapshot-id $SNAPSHOT_ID \\\n\n--size 8 \\\n\n--volume-type gp2 \\\n\n--availability-zone us-east-1a \\\n\n--output text --query VolumeId)\n\nValidation checks\n\nAttach the volume to the EC2 instance as /dev/sdf:\n\naws ec2 attach-volume --volume-id $SNAP_VOLUME_ID \\\n\n--instance-id $INSTANCE_ID --device /dev/sdf\n\nWait until the volume’s state has reached Attached:\n\naws ec2 describe-volumes \\\n\n--volume-ids $SNAP_VOLUME_ID\n\nConnect to the EC2 instance using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nRun the lsblk command to see volumes, and note the volume name that you attached (you will use this volume name in the subsequent step to mount the volume):\n\nlsblk\n\nYou should see output similar to this:\n\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT\n\nnvme0n1 259:0 0 8G 0 disk\n\n├─nvme0n1p1 259:1 0 8G 0 part /\n\n└─nvme0n1p128 259:2 0 1M 0 part\n\nnvme1n1 259:3 0 8G 0 disk\n\n├─nvme1n1p1 259:4 0 8G 0 part\n\n└─nvme1n1p128 259:5 0 1M 0 part\n\nCreate a folder to mount the attached disk:\n\nsudo mkdir /mnt/restore\n\nMount the volume you attached to the folder you created:\n\nsudo mount -t xfs -o nouuid /dev/nvme1n1p1 /mnt/restore\n\nNOTE\n\nThe XFS file uses universally unique identifiers (UUIDs) to identify filesystems. By default, a safety mechanism is in\n\nplace in the mount command to prevent you from mounting the same filesystem twice. Since you created a block-level\n\nsnapshot and created a volume from it, the mount command you used requires overriding this check to allow mounting a\n\nvolume with the same UUID using the -o nouuid parameter. For more information, consult the man page for mount.\n\nCopy the file(s) you need from the mounted volume to the local filesystem:\n\nsudo cp /mnt/restore/home/ec2-user/.bash_profile \\\n\n/tmp/.bash_profile.restored\n\nUnmount the volume:\n\nsudo umount /dev/nvme1n1p1\n\nLog out of the EC2 instance:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nEBS snapshots are an important part of a backup strategy within the EC2 service. If you run EC2 instances, snapshots enable you to restore an instance to a point in time when the snapshot was created. You can also create an EBS volume from a snapshot and attach it to a running instance, which you accomplished in this recipe. This is useful for site reliability engineering (SRE) teams, operations teams, and users who need to restore single files to a point in time to meet their needs.\n\nTIP\n\nEBS snapshots allow you to take snapshots of EBS volumes manually, write your own automation (like a Lambda\n\nfunction on a schedule), automate with Data Lifecycle Manager, or use AWS Backup (see Recipe 3.7) for a more\n\ncomprehensive solution to backing up and restoring EC2 instances.\n\nChallenge\n\nCreate an AMI from the snapshot you create and launch a new instance from the newly created AMI.\n\n3.9 Replicating Data Between EFS and S3 with DataSync\n\nProblem\n\nYou need to replicate files from Amazon S3 to Amazon EFS.\n\nSolution\n\nConfigure AWS DataSync with an S3 source and EFS target; then create a DataSync task and start the replication task, as shown in Figure 3-23.\n\nFigure 3-23. Replicating S3 Bucket and EFS file system with DataSync\n\nPrerequisites\n\nAn S3 bucket and EFS file system.\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nEC2 instance deployed with EFS file system attached. You will need the ability to connect to it for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate an IAM role using the statement in the provided assume-role- policy.json file with this command:\n\nS3_ROLE_ARN=$(aws iam create-role --role-name AWSCookbookS3LocationRole \\\n\n--assume-role-policy-document file://assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\n2.\n\nAttach the AmazonS3ReadOnlyAccess IAM managed policy to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbookS3LocationRole \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\n\n3.\n\nCreate a DataSync S3 location:\n\n4.\n\n5.\n\n6.\n\n7.\n\n8.\n\nS3_LOCATION_ARN=$(aws datasync create-location-s3 \\\n\n--s3-bucket-arn $BUCKET_ARN \\\n\n--s3-config BucketAccessRoleArn=$S3_ROLE_ARN \\\n\n--output text --query LocationArn)\n\nCreate an IAM role using the statement in the provided assume-role- policy.json file with this command:\n\nEFS_ROLE_ARN=$(aws iam create-role --role-name AWSCookbookEFSLocationRole \\\n\n--assume-role-policy-document file://assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\nAttach the AmazonElasticFileSystemClientReadWriteAccess IAM managed policy to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbookEFSLocationRole \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonElasticFileSystemClientFullAccess\n\nGet the ARN of the EFS file system:\n\nEFS_FILE_SYSTEM_ARN=$(aws efs describe-file-systems \\\n\n--file-system-id $EFS_ID \\\n\n--output text --query FileSystems[0].FileSystemArn)\n\nGet the ARN of the subnet:\n\nSUBNET_ARN=$(aws ec2 describe-subnets \\\n\n--subnet-ids $ISOLATED_SUBNET_1 \\\n\n--output text --query Subnets[0].SubnetArn)\n\nGet the ARN of the security group:\n\nSG_ARN=arn:aws:ec2:$AWS_REGION:$AWS_ACCOUNT_ID:security-group/$EFS_SG\n\n9.\n\nCreate a DataSync EFS location:\n\nEFS_LOCATION_ARN=$(aws datasync create-location-efs \\\n\n--efs-filesystem-arn $EFS_FILE_SYSTEM_ARN \\\n\n--ec2-config SubnetArn=$SUBNET_ARN,SecurityGroupArns=[$SG_ARN] \\\n\n--output text)\n\n10.\n\nCreate a DataSync task:\n\nTASK_ARN=$(aws datasync create-task \\\n\n--source-location-arn $S3_LOCATION_ARN \\\n\n--destination-location-arn $EFS_LOCATION_ARN \\\n\n--output text --query TaskArn)\n\n11.\n\nExecute the task:\n\naws datasync start-task-execution \\\n\n--task-arn $TASK_ARN\n\n12.\n\nEnsure the task has completed after a few seconds:\n\naws datasync list-task-executions \\\n\n--task-arn $TASK_ARN\n\nValidation checks\n\nEnsure your EC2 instance 1 has registered with SSM. Use this command to check the status. This command should return the instance ID:\n\naws ssm describe-instance-information \\\n\n--filters Key=ResourceType,Values=EC2Instance \\\n\n--query \"InstanceInformationList[].InstanceId\" --output text\n\nConnect to your EC2 instance using SSM Session Manager:\n\naws ssm start-session --target $INSTANCE_ID\n\nThe EC2 instance has the EFS volume mounted at /mnt/efs. You can browse to the directory and view that the S3-Test-Content.txt file has been replicated from your S3 bucket to your EFS volume, as shown in the sample output:\n\nsh-4.2$ cd /mnt/efs\n\nsh-4.2$ ls\n\nsh-4.2$ ls -al\n\ntotal 12\n\ndrwxr-xr-x 3 nfsnobody nfsnobody 6144 Jan 1 1970 .\n\ndrwxr-xr-x 3 root root 17 Sep 10 02:07 ..\n\ndrwx------ 2 root root 6144 Sep 10 03:27 .aws-datasync\n\nrwxr-xr-x 1 nfsnobody nfsnobody 30 Jan 1 1970 S3-Test-Content.txt\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou can use AWS DataSync for both on-demand and ongoing/automated file synchronization tasks across a variety of AWS services. DataSync preserves metadata for copied items and checks file integrity during the synchronization task, supporting retries if needed. This is useful if you are a developer or cloud engineer looking to move data among a variety of sources and targets without provisioning any infrastructure or writing your own scripts to accomplish the same task. In this recipe, you used it to synchronize data between S3 and EFS hosted in your AWS account, but you can also use it to synchronize data among your non-AWS servers if you have a VPN connection, Direct Connect, or among other AWS accounts using VPC peering or a transit gateway.\n\nNOTE\n\nAt the time of this writing, the minimum automated sync schedule interval you can set is one hour. You can find other\n\ndetails about DataSync in the user documentation.\n\nLike many AWS services, DataSync uses IAM roles to perform actions against S3 and EFS for you. You granted DataSync the ability to interact with S3 and EFS. DataSync provisions network interfaces in your VPC to connect to your EFS file shares and uses the AWS APIs to interact with S3. It encrypts traffic in transit using TLS and also supports encryption at rest using KMS should your security and compliance requirements mandate encryption at rest.\n\nChallenge 1\n\nSet up a DataSync task that excludes filenames in a certain folder (e.g., private-folder).\n\nChallenge 2\n\nSet up a scheduled DataSync task to replicate data from S3 to EFS on an hourly schedule.\n\nChapter 4. Databases\n\n4.0 Introduction You have a myriad of choices for using databases with AWS. Installing and running a database on EC2 provides you with the most choices of database engines and custom configurations, but brings about challenges like patching, backups, configuring high-availability, replication, and performance tuning. As noted on its product page, AWS offers managed database services that help address these challenges and cover a broad range of database types (relational, key- value/NoSQL, in-memory, document, wide column, graph, time series, ledger). When choosing a database type and data model, you must keep speed, volume, and access patterns in mind.\n\nThe managed database services on AWS integrate with many services to provide you additional functionality from security, operations, and development perspectives. In this chapter, you will explore Amazon Relational Database Service (RDS), NoSQL usage with Amazon DynamoDB, and the ways to migrate, secure, and operate these database types at scale. For example, you will learn how to integrate Secrets Manager with an RDS database to automatically rotate database user passwords. You will also learn how to leverage IAM authentication to reduce the application dependency on database passwords entirely, granting access to RDS through IAM permissions instead. You’ll explore autoscaling with DynamoDB and learn about why this is important from a cost and performance perspective.\n\nNOTE\n\nSome people think that Route 53 is a database but we disagree :-)\n\nNOTE\n\nSome database engines in the past have used certain terminology for replica configurations, default root user names,\n\nprimary tables, etc. We took care to use inclusive terminology throughout this chapter (and the whole book) wherever\n\npossible. We support the movement to use inclusive terminology in these commercial and open source database engines.\n\nWorkstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Databases\n\nWARNING\n\nDuring some of the steps in this chapter, you will create passwords and temporarily save them as environment variables to\n\nuse in subsequent steps. Make sure that you unset the environment variables by following the cleanup steps when you\n\ncomplete the recipe. We use this approach for simplicity of understanding. A more secure method (such as the method\n\nused in Recipe 1.8) should be used in production environments.\n\n4.1 Creating an Amazon Aurora Serverless PostgreSQL Database\n\nProblem\n\nYou have a web application that receives unpredictable requests that require storage in a relational database. You need a database solution that can scale with usage and be cost-effective. You would like to build a solution that has low operational overhead and must be compatible with your existing PostgreSQL-backed application.\n\nSolution\n\nConfigure and create an Aurora Serverless database cluster with a complex password. Then, apply a customized scaling configuration and enable automatic pause after inactivity. The scaling activity in response to the policy is shown in Figure 4-1.",
      "page_number": 178
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 189-196)",
      "start_page": 189,
      "end_page": 196,
      "detection_method": "topic_boundary",
      "content": "Figure 4-1. Aurora Serverless cluster scaling compute\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nEC2 instance deployed. You will need the ability to connect to this for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nUse AWS Secrets Manager to generate a complex password:\n\nADMIN_PASSWORD=$(aws secretsmanager get-random-password \\\n\n--exclude-punctuation \\\n\n--password-length 41 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\n2.\n\n3.\n\nNOTE\n\nWe are excluding punctuation characters from the password that we are creating because PostgreSQL\n\ndoes not support them. See the “Naming constraints in Amazon RDS” table.\n\nCreate a database subnet group specifying the VPC subnets to use for the cluster. Database subnet groups simplify the placement of RDS elastic network interfaces (ENIs):\n\naws rds create-db-subnet-group \\\n\n--db-subnet-group-name awscookbook401subnetgroup \\\n\n--db-subnet-group-description \"AWSCookbook401 subnet group\" \\\n\n--subnet-ids $SUBNET_ID_1 $SUBNET_ID_2\n\nYou should see output similar to the following:\n\n{\n\n\"DBSubnetGroup\": {\n\n\"DBSubnetGroupName\": \"awscookbook402subnetgroup\",\n\n\"DBSubnetGroupDescription\": \"AWSCookbook401 subnet group\",\n\n\"VpcId\": \"vpc-<<VPCID>>\",\n\n\"SubnetGroupStatus\": \"Complete\",\n\n\"Subnets\": [\n\n{\n\n\"SubnetIdentifier\": \"subnet-<<SUBNETID>>\",\n\n\"SubnetAvailabilityZone\": {\n\n\"Name\": \"us-east-1b\"\n\n},\n\n\"SubnetOutpost\": {},\n\n\"SubnetStatus\": \"Active\"\n\n},\n\n...\n\nCreate a VPC security group for the database:\n\nDB_SECURITY_GROUP_ID=$(aws ec2 create-security-group \\\n\n--group-name AWSCookbook401sg \\\n\n--description \"Aurora Serverless Security Group\" \\\n\n--vpc-id $VPC_ID --output text --query GroupId)\n\n4.\n\n5.\n\n6.\n\nCreate a database cluster, specifying an engine-mode of serverless:\n\naws rds create-db-cluster \\\n\n--db-cluster-identifier awscookbook401dbcluster \\\n\n--engine aurora-postgresql \\\n\n--engine-mode serverless \\\n\n--engine-version 10.14 \\\n\n--master-username dbadmin \\\n\n--master-user-password $ADMIN_PASSWORD \\\n\n--db-subnet-group-name awscookbook401subnetgroup \\\n\n--vpc-security-group-ids $DB_SECURITY_GROUP_ID\n\nYou should see output similar to the following:\n\n{\n\n\"DBCluster\": {\n\n\"AllocatedStorage\": 1,\n\n\"AvailabilityZones\": [\n\n\"us-east-1f\",\n\n\"us-east-1b\",\n\n\"us-east-1a\"\n\n],\n\n\"BackupRetentionPeriod\": 1,\n\n\"DBClusterIdentifier\": \"awscookbook401dbcluster\",\n\n\"DBClusterParameterGroup\": \"default.aurora-postgresql10\",\n\n\"DBSubnetGroup\": \"awscookbook401subnetgroup\",\n\n\"Status\": \"creating\",\n\n...\n\nWait for the Status to read available; this will take a few moments:\n\naws rds describe-db-clusters \\\n\n--db-cluster-identifier awscookbook401dbcluster \\\n\n--output text --query DBClusters[0].Status\n\nModify the database to automatically scale with new autoscaling capacity targets (8 min, 16 max) and enable AutoPause after five minutes of inactivity:\n\n7.\n\naws rds modify-db-cluster \\\n\n--db-cluster-identifier awscookbook401dbcluster --scaling-configuration \\\n\nMinCapacity=8,MaxCapacity=16,SecondsUntilAutoPause=300,TimeoutAction='ForceApply\n\nCapacityChange',AutoPause=true\n\nYou should see output similar to what you saw for step 4.\n\nNOTE\n\nIn practice, you may want to use a different AutoPause value. To determine what is appropriate for your use, evaluate your performance needs and Aurora pricing.\n\nWait at least five minutes and observe that the database’s capacity has scaled down to 0:\n\naws rds describe-db-clusters \\\n\n--db-cluster-identifier awscookbook401dbcluster \\\n\n--output text --query DBClusters[0].Capacity\n\nNOTE\n\nThe AutoPause feature automatically sets the capacity of the cluster to 0 after inactivity. When your database activity resumes (e.g., with a query or connection), the capacity value is automatically set to\n\nyour configured minimum scaling capacity value.\n\nGrant your EC2 instance’s security group access to the default PostgreSQL port:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 5432 \\\n\n--source-group $INSTANCE_SG \\\n\n--group-id $DB_SECURITY_GROUP_ID\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<ID>>\",\n\n\"GroupId\": \"sg-<<ID>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 5432,\n\n\"ToPort\": 5432,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<ID>>\"\n\n}\n\n}\n\n]\n\n}\n\nValidation checks\n\nList the endpoint for the RDS cluster:\n\naws rds describe-db-clusters \\\n\n--db-cluster-identifier awscookbook401dbcluster \\\n\n--output text --query DBClusters[0].Endpoint\n\nYou should see something similar to this:\n\nawscookbook401dbcluster.cluster-<<unique>>.us-east-1.rds.amazonaws.com\n\nRetrieve the password for your RDS cluster:\n\necho $ADMIN_PASSWORD\n\nConnect to the EC2 instance by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nInstall the PostgreSQL package so you can use the psql command to connect to the database:\n\nsudo yum -y install postgresql\n\nConnect to the database. This may take a moment as the database capacity is scaling up from 0. You’ll need to copy and paste the password (outputted previously):\n\npsql -h $HOST_NAME -U dbadmin -W -d postgres\n\nHere is an example of connecting to a database using the psql command:\n\nsh-4.2$ psql -h awscookbook401dbcluster.cluster-<<unique>>.us-east-1.rds.amazonaws.com -U dbadmin -W -\n\nd postgres\n\nPassword for user dbadmin:(paste in the password)\n\nQuit psql:\n\n\\q\n\nExit the Session Manager session:\n\nexit\n\nCheck the capacity of the cluster again to observe that the database has scaled up to the minimum value that you configured:\n\naws rds describe-db-clusters \\\n\n--db-cluster-identifier awscookbook401dbcluster \\\n\n--output text --query DBClusters[0].Capacity\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nTIP\n\nThe default behavior of deleting an RDS cluster is to take a final snapshot as a safety mechanism. We chose to skip this\n\nbehavior by adding the --skip-final-snapshot option to ensure you do not incur any costs for storing the snapshot in your AWS account. In a real-world scenario, you would likely want to retain the snapshot for a period of time in case you\n\nneeded to re-create the existing database from the snapshot.\n\nDiscussion\n\nThe cluster will automatically scale capacity to meet the needs of your usage. Setting MaxCapacity=16 limits the upper bound of your capacity to prevent runaway usage and unexpected costs. The cluster will set its capacity to 0 when no connection or activity is detected. This is triggered when the SecondsUntilAutoPause value is reached.\n\nWhen you enable AutoPause=true for your cluster, you pay for only the underlying storage during idle times. The default (and minimum) “inactivity period” is five minutes. Connecting to a paused cluster will cause the capacity to scale up to MinCapacity.\n\nWARNING\n\nNot all database engines and versions are available with the serverless engine. At the time of writing, the Aurora FAQ\n\nstates that Aurora Serverless is currently available for Aurora with MySQL 5.6 compatibility and for Aurora with\n\nPostgreSQL 10.7+ compatibility.\n\nThe user guide states that Aurora Serverless scaling is measured in capacity units (CUs) that correspond to compute and memory reserved for your cluster. This capability is a good fit for many workloads and use cases from development to batch-based workloads, and production workloads where traffic is unpredictable and costs associated with potential over-provisioning are a concern. By not needing to calculate baseline usage patterns, you can start developing quickly, and the cluster will automatically respond to the demand that your application requires.\n\nIf you currently use a “provisioned” capacity type cluster on Amazon RDS and would like to start using Aurora Serverless, you can snapshot your current database and restore it from within the AWS Console or from the command line to perform a migration. If your current database is not on RDS, you can use your database engine’s dump and restore features or use the AWS Database Migration Service (AWS DMS) to migrate to RDS.\n\nNOTE\n\nAt the time of this writing, Amazon Aurora Serverless v2 is in preview.\n\nThe user guide mentions that Aurora Serverless further builds on the existing Aurora platform, which replicates your database’s underlying storage six ways across three Availability Zones. While this replication is a benefit for resiliency, you should still use automated backups for your database to guard against operational errors. Aurora Serverless has automated backups enabled by default, and the backup retention can be increased up to 35 days if needed.\n\nNOTE\n\nPer the documentation, if your database cluster has been idle for more than seven days, the cluster will be backed up with\n\na snapshot. If this occurs, the database cluster is restored when there is a request to connect to it.\n\nChallenge\n\nChange the max capacity to 64 and idle time to 10 minutes for the database cluster.\n\nSee Also\n\nRecipe 4.2\n\nRecipe 4.7\n\n4.2 Using IAM Authentication with an RDS Database\n\nProblem\n\nYou have a server that connects to a database with a password and would like to instead use rotating temporary credentials.\n\nSolution\n\nFirst you will enable IAM authentication for your database. You will then configure the IAM permissions for the EC2 instance to use. Finally, create a new user on the database, retrieve the IAM authentication token, and verify connectivity (see Figure 4-2).",
      "page_number": 189
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 197-206)",
      "start_page": 197,
      "end_page": 206,
      "detection_method": "topic_boundary",
      "content": "Figure 4-2. IAM authentication from an EC2 instance to an RDS database\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nAn RDS MySQL instance.\n\nEC2 instance deployed. You will need the ability to connect to this for configuring MySQL and testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nEnable IAM database authentication on the RDS database instance:\n\naws rds modify-db-instance \\\n\n--db-instance-identifier $RDS_DATABASE_ID \\\n\n--enable-iam-database-authentication \\\n\n--apply-immediately\n\nYou should see output similar to the following:\n\n{\n\n2.\n\n3.\n\n\"DBInstance\": {\n\n\"DBInstanceIdentifier\": \"awscookbookrecipe402\",\n\n\"DBInstanceClass\": \"db.m5.large\",\n\n\"Engine\": \"mysql\",\n\n\"DBInstanceStatus\": \"available\",\n\n\"MasterUsername\": \"admin\",\n\n\"DBName\": \"AWSCookbookRecipe402\",\n\n\"Endpoint\": {\n\n\"Address\": \"awscookbookrecipe402.<<ID>>.us-east-1.rds.amazonaws.com\",\n\n\"Port\": 3306,\n\n\"HostedZoneId\": \"<<ID>>\"\n\n},\n\n...\n\nWARNING\n\nIAM database authentication is available for only the database engines listed in this AWS article.\n\nRetrieve the RDS database instance resource ID:\n\nDB_RESOURCE_ID=$(aws rds describe-db-instances \\\n\n--query \\\n\n'DBInstances[?DBName==`AWSCookbookRecipe402`].DbiResourceId' \\\n\n--output text)\n\nCreate a file called policy.json with the following content (a policy- template.json file is provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"rds-db:connect\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:rds-db:AWS_REGION:AWS_ACCOUNT_ID:dbuser:DBResourceId/db_user\"\n\n]\n\n}\n\n4.\n\n5.\n\n]\n\n}\n\nNOTE\n\nIn the preceding example, db_user must match the name of the user in the database that we would like to allow to connect.\n\nReplace the values in the template file by using the sed command with environment variables you have set:\n\nsed -e \"s/AWS_ACCOUNT_ID/${AWS_ACCOUNT_ID}/g\" \\\n\ne \"s|AWS_REGION|${AWS_REGION}|g\" \\\n\ne \"s|DBResourceId|${DB_RESOURCE_ID}|g\" \\\n\npolicy-template.json > policy.json\n\nCreate an IAM policy using the file you just created:\n\naws iam create-policy --policy-name AWSCookbook402EC2RDSPolicy \\\n\n--policy-document file://policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Policy\": {\n\n\"PolicyName\": \"AWSCookbook402EC2RDSPolicy\",\n\n\"PolicyId\": \"<<ID>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:policy/AWSCookbook402EC2RDSPolicy\",\n\n\"Path\": \"/\",\n\n\"DefaultVersionId\": \"v1\",\n\n\"AttachmentCount\": 0,\n\n\"PermissionsBoundaryUsageCount\": 0,\n\n\"IsAttachable\": true,\n\n\"CreateDate\": \"2021-09-21T21:18:54+00:00\",\n\n\"UpdateDate\": \"2021-09-21T21:18:54+00:00\"\n\n}\n\n}\n\n6.\n\n7.\n\n8.\n\n9.\n\n10.\n\nAttach the IAM policy AWSCookbook402EC2RDSPolicy to the IAM role that the EC2 is using:\n\naws iam attach-role-policy --role-name $INSTANCE_ROLE_NAME \\\n\n--policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook402EC2RDSPolicy\n\nRetrieve the RDS admin password from Secrets Manager:\n\nRDS_ADMIN_PASSWORD=$(aws secretsmanager get-secret-value \\\n\n--secret-id $RDS_SECRET_ARN \\\n\n--query SecretString | jq -r | jq .password | tr -d '\"')\n\nOutput text so that you can use it later when you connect to the EC2 instance.\n\nList the endpoint for the RDS cluster:\n\necho $RDS_ENDPOINT\n\nYou should see output similar to the following:\n\nawscookbookrecipe402.<<unique>>.us-east-1.rds.amazonaws.com\n\nList the password for the RDS cluster:\n\necho $RDS_ADMIN_PASSWORD\n\nConnect to the EC2 instance using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nInstall MySQL:\n\n11.\n\n12.\n\n13.\n\nsudo yum -y install mysql\n\nConnect to the database. You’ll need to copy and paste the password and hostname (outputted in steps 7 and 8):\n\nmysql -u admin -p$DB_ADMIN_PASSWORD -h $RDS_ENDPOINT\n\nYou should see output similar to the following:\n\nWelcome to the MariaDB monitor. Commands end with ; or \\g.\n\nYour MySQL connection id is 18\n\nServer version: 8.0.23 Source distribution\n\nCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMySQL [(none)]>\n\nNOTE\n\nIn the mysql command in step 11, there is no space between the -p flag and the first character of the password.\n\nCreate a new database user to associate with the IAM authentication:\n\nCREATE USER db_user@'%' IDENTIFIED WITH AWSAuthenticationPlugin as 'RDS';\n\nGRANT SELECT ON *.* TO 'db_user'@'%';\n\nFor both commands in step 12, you should see output similar to the following:\n\nQuery OK, 0 rows affected (0.01 sec)\n\nNow, exit the mysql prompt:\n\nquit\n\nValidation checks\n\nWhile still on the EC2 instance, download the RDS Root CA (certificate authority) file provided by Amazon from the rds-downloads S3 bucket:\n\nsudo wget https://s3.amazonaws.com/rds-downloads/rds-ca-2019-root.pem\n\nSet the Region by grabbing the value from the instance’s metadata:\n\nexport AWS_DEFAULT_REGION=$(curl --silent http://169.254.169.254/latest/dynamic/instance-\n\nidentity/document \\\n\n| awk -F'\"' ' /region/ {print $4}')\n\nGenerate the RDS authentication token and save it as a variable. You’ll need to copy and paste the hostname (outputted in step 8):\n\nTOKEN=\"$(aws rds generate-db-auth-token --hostname $RDS_ENDPOINT --port 3306 --username db_user)\"\n\nConnect to the database using the RDS authentication token with the new db_user. You’ll need to copy and paste the hostname (outputted in step 8):\n\nmysql --host=$RDS_ENDPOINT --port=3306 \\\n\n--ssl-ca=rds-ca-2019-root.pem \\\n\n--user=db_user --password=$TOKEN\n\nRun a SELECT query at the mysql prompt to verify that this user has the SELECT *.* grant that you applied:\n\nSELECT user FROM mysql.user;\n\nYou should see output similar to the following:\n\nMySQL [(none)]> SELECT user FROM mysql.user;\n\n+------------------+\n\n| user |\n\n+------------------+\n\n| admin |\n\n| db_user |\n\n| mysql.infoschema |\n\n| mysql.session |\n\n| mysql.sys |\n\n| rdsadmin |\n\n+------------------+\n\n6 rows in set (0.00 sec)\n\nExit the mysql prompt:\n\nquit\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nInstead of a password in your MySQL connection string, you retrieved and used a token associated with the EC2 instance’s IAM role. The documentation for IAM states that this token lasts for 15 minutes. If you install an application on this EC2 instance, the code can regularly refresh this token using the AWS SDK. There is no need to rotate passwords for your database user because the old token will be invalidated after 15 minutes.\n\nYou can create multiple database users associated with specific grants to allow your application to maintain different levels of access to your database. The grants happen within the database, not within the IAM permissions. IAM controls the db-connect action for the specific user. This IAM action allows the authentication token to be retrieved. That username is mapped from IAM to the GRANT(s) by using the same username within the database as in the policy.json file:\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"rds-db:connect\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:rds-db:AWS_REGION::dbuser:DBResourceId/db_user\"\n\n]\n\n}\n\n]\n\n}\n\nIn this recipe, you also enabled encryption in transit by specifying the SSL certificate bundle that you downloaded to the EC2 instance in your database connection command. This encrypts the connection between your application and your database. This is a good security practice and is often required for many compliance standards. The connection string you used to connect with the IAM authentication token indicated an SSL certificate as one of the connection parameters. The certificate authority bundle is available to download from AWS and use within your application.\n\nChallenge\n\nTry connecting to the database from a Lambda function using IAM authentication. We have provided a lambda_function.py file in the repository to get you started.\n\n4.3 Leveraging RDS Proxy for Database Connections from Lambda\n\nProblem\n\nYou have a serverless function that is accessing a relational database and you need to implement connection pooling to minimize the number of database connections and improve performance.\n\nSolution\n\nCreate an RDS Proxy, associate it with your RDS MySQL database, and configure your Lambda to connect to the proxy instead of accessing the database directly (see Figure 4-3).\n\nFigure 4-3. Lambda connection path to database via RDS Proxy\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables\n\nAn RDS MySQL instance\n\nA Lambda function that you would like to connect to your RDS database\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file called assume-role-policy.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n2.\n\n3.\n\n\"Service\": \"rds.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate an IAM role for the RDS Proxy using the assume-role-policy.json file:\n\naws iam create-role --assume-role-policy-document \\\n\nfile://assume-role-policy.json --role-name AWSCookbook403RDSProxy\n\nYou should see output similar to the following:\n\n{\n\n\"Role\": {\n\n\"Path\": \"/\",\n\n\"RoleName\": \"AWSCookbook403RDSProxy\",\n\n\"RoleId\": \"<<ID>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:role/AWSCookbook403RDSProxy\",\n\n\"CreateDate\": \"2021-09-21T22:33:57+00:00\",\n\n\"AssumeRolePolicyDocument\": {\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"rds.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\n}\n\n}\n\nCreate a security group to be used by the RDS Proxy:\n\nRDS_PROXY_SG_ID=$(aws ec2 create-security-group \\",
      "page_number": 197
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 207-221)",
      "start_page": 207,
      "end_page": 221,
      "detection_method": "topic_boundary",
      "content": "4.\n\n5.\n\n6.\n\n--group-name AWSCookbook403RDSProxySG \\\n\n--description \"Lambda Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\nCreate the RDS Proxy. This will take a few moments:\n\nRDS_PROXY_ENDPOINT_ARN=$(aws rds create-db-proxy \\\n\n--db-proxy-name $DB_NAME \\\n\n--engine-family MYSQL \\\n\n--auth '{\n\n\"AuthScheme\": \"SECRETS\",\n\n\"SecretArn\": \"'\"$RDS_SECRET_ARN\"'\",\n\n\"IAMAuth\": \"REQUIRED\"\n\n}' \\\n\n--role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook403RDSProxy \\\n\n--vpc-subnet-ids $ISOLATED_SUBNETS \\\n\n--vpc-security-group-ids $RDS_PROXY_SG_ID \\\n\n--require-tls --output text \\\n\n--query DBProxy.DBProxyArn)\n\nWait for the RDS Proxy to become available:\n\naws rds describe-db-proxies \\\n\n--db-proxy-name $DB_NAME \\\n\n--query DBProxies[0].Status \\\n\n--output text\n\nRetrieve the RDS_PROXY_ENDPOINT:\n\nRDS_PROXY_ENDPOINT=$(aws rds describe-db-proxies \\\n\n--db-proxy-name $DB_NAME \\\n\n--query DBProxies[0].Endpoint \\\n\n--output text)\n\nNext you need an IAM policy that allows the Lambda function to generate IAM authentication tokens. Create a file called template-policy.json with the following content (file provided in the repository):\n\n7.\n\n8.\n\n9.\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"rds-db:connect\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:rds-db:AWS_REGION:AWS_ACCOUNT_ID:dbuser:RDSProxyID/admin\"\n\n]\n\n}\n\n]\n\n}\n\nSeparate out the Proxy ID from the RDS Proxy endpoint ARN. The Proxy ID is required for configuring IAM policies in the following steps:\n\nRDS_PROXY_ID=$(echo $RDS_PROXY_ENDPOINT_ARN | awk -F: '{ print $7} ')\n\nReplace the values in the template file by using the sed command with environment variables you have set:\n\nsed -e \"s/AWS_ACCOUNT_ID/${AWS_ACCOUNT_ID}/g\" \\\n\ne \"s|AWS_REGION|${AWS_REGION}|g\" \\\n\ne \"s|RDSProxyID|${RDS_PROXY_ID}|g\" \\\n\npolicy-template.json > policy.json\n\nCreate an IAM policy using the file you just created:\n\naws iam create-policy --policy-name AWSCookbook403RdsIamPolicy \\\n\n--policy-document file://policy.json\n\nYou should see output similar to the following:\n\n{\n\n10.\n\n11.\n\n\"Policy\": {\n\n\"PolicyName\": \"AWSCookbook403RdsIamPolicy\",\n\n\"PolicyId\": \"<<Id>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:policy/AWSCookbook403RdsIamPolicy\",\n\n\"Path\": \"/\",\n\n\"DefaultVersionId\": \"v1\",\n\n\"AttachmentCount\": 0,\n\n\"PermissionsBoundaryUsageCount\": 0,\n\n\"IsAttachable\": true,\n\n\"CreateDate\": \"2021-09-21T22:50:24+00:00\",\n\n\"UpdateDate\": \"2021-09-21T22:50:24+00:00\"\n\n}\n\n}\n\nAttach the policy to the DBAppFunction Lambda function’s role:\n\naws iam attach-role-policy --role-name $DB_APP_FUNCTION_ROLE_NAME \\\n\n--policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook403RdsIamPolicy\n\nUse this command to check when the proxy enters the available status and then proceed:\n\naws rds describe-db-proxies --db-proxy-name $DB_NAME \\\n\n--query DBProxies[0].Status \\\n\n--output text\n\nAttach the SecretsManagerReadWrite policy to the RDS Proxy’s role:\n\naws iam attach-role-policy --role-name AWSCookbook403RDSProxy \\\n\n--policy-arn arn:aws:iam::aws:policy/SecretsManagerReadWrite\n\nTIP\n\nIn a production scenario, you would want to scope this permission down to the minimal secret\n\nresources that your application needs to access, rather than grant SecretsManagerReadWrite, which allows read/write for all secrets.\n\n12.\n\n13.\n\nAdd an ingress rule to the RDS instance’s security group that allows access on TCP port 3306 (the default MySQL engine TCP port) from the RDS Proxy security group:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 3306 \\\n\n--source-group $RDS_PROXY_SG_ID \\\n\n--group-id $RDS_SECURITY_GROUP\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<ID>>\",\n\n\"GroupId\": \"sg-<<ID>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 3306,\n\n\"ToPort\": 3306,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<ID>>\"\n\n}\n\n}\n\n]\n\n}\n\nNOTE\n\nSecurity groups can reference other security groups. Because of dynamic IP addresses within VPCs,\n\nthis is considered the best way to grant access without opening up your security group too wide. For\n\nmore information, see Recipe 2.5.\n\nRegister targets with the RDS Proxy:\n\naws rds register-db-proxy-targets \\\n\n14.\n\n--db-proxy-name $DB_NAME \\\n\n--db-instance-identifiers $RDS_DATABASE_ID\n\nYou should see output similar to the following:\n\n{\n\n\"DBProxyTargets\": [\n\n{\n\n\"Endpoint\": \"awscookbook403db.<<ID>>.us-east-1.rds.amazonaws.com\",\n\n\"RdsResourceId\": \"awscookbook403db\",\n\n\"Port\": 3306,\n\n\"Type\": \"RDS_INSTANCE\",\n\n\"TargetHealth\": {\n\n\"State\": \"REGISTERING\"\n\n}\n\n}\n\n]\n\n}\n\nCheck the status of the target registration with this command. Wait until the State reaches AVAILABLE:\n\naws rds describe-db-proxy-targets \\\n\n--db-proxy-name awscookbookrecipe403 \\\n\n--query Targets[0].TargetHealth.State \\\n\n--output text\n\nAdd an ingress rule to the RDS Proxy security group that allows access on TCP port 3306 from the Lambda App function’s security group:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 3306 \\\n\n--source-group $DB_APP_FUNCTION_SG_ID \\\n\n--group-id $RDS_PROXY_SG_ID\n\nYou should see output similar to the following:\n\n15.\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<ID>>\",\n\n\"GroupId\": \"sg-<<ID>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 3306,\n\n\"ToPort\": 3306,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<ID>>\"\n\n}\n\n}\n\n]\n\n}\n\nModify the Lambda function to now use the RDS Proxy endpoint as the DB_HOST, instead of connecting directly to the database:\n\naws lambda update-function-configuration \\\n\n--function-name $DB_APP_FUNCTION_NAME \\\n\n--environment Variables={DB_HOST=$RDS_PROXY_ENDPOINT}\n\nYou should see output similar to the following:\n\n{\n\n\"FunctionName\": \"cdk-aws-cookbook-403-LambdaApp<<ID>>\",\n\n\"FunctionArn\": \"arn:aws:lambda:us-east-1:111111111111:function:cdk-aws-cookbook-\n\n403-LambdaApp<<ID>>\",\n\n\"Runtime\": \"python3.8\",\n\n\"Role\": \"arn:aws:iam::111111111111:role/cdk-aws-cookbook-403-\n\nLambdaAppServiceRole<<ID>>\",\n\n\"Handler\": \"lambda_function.lambda_handler\",\n\n\"CodeSize\": 665,\n\n\"Description\": \"\",\n\n\"Timeout\": 600,\n\n\"MemorySize\": 1024,\n\n...\n\nValidation checks\n\nRun the Lambda function with this command to validate that the function can connect to RDS using your RDS Proxy:\n\naws lambda invoke \\\n\n--function-name $DB_APP_FUNCTION_NAME \\\n\nresponse.json && cat response.json\n\nYou should see output similar to the following:\n\n{\n\n\"StatusCode\": 200,\n\n\"ExecutedVersion\": \"$LATEST\"\n\n}\n\n\"Successfully connected to RDS via RDS Proxy!\"\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nConnection pooling is important to consider when you use Lambda with RDS. Since the function could be executed with a lot of concurrency and frequency depending on your application, the number of raw connections to your database can grow and impact performance. By using RDS Proxy to manage the connections to the database, fewer connections are needed to the actual database. This setup increases performance and efficiency.\n\nWithout RDS Proxy, a Lambda function might establish a new connection to the database each time the function is invoked. This behavior depends on the execution environment, runtimes (Python, NodeJS, Go, etc.), and the way you instantiate connections to the database from the function code. In cases with large amounts of function concurrency, this could result in large amounts of TCP connections to your database, reducing database performance and increasing latency. Per the documentation, RDS Proxy helps manage the connections from Lambda by managing them as a “pool,” so that as concurrency increases, RDS Proxy increases the actual connections to the database only as needed, offloading the TCP overhead to RDS Proxy.\n\nSSL encryption in transit is supported by RDS Proxy when you include the certificate bundle provided by AWS in your database connection string. RDS Proxy supports MySQL and PostgreSQL RDS databases. For a complete listing of all supported database engines and versions, see this support document.\n\nTIP\n\nYou can also architect to be efficient with short-lived database connections by leveraging the RDS Data API within your\n\napplication, which leverages a REST API exposed by Amazon RDS. For an example on the RDS Data API, see Recipe\n\n4.8.\n\nChallenge\n\nEnable enhanced logging for the RDS Proxy. This is useful for debugging.\n\n4.4 Encrypting the Storage of an Existing Amazon RDS for MySQL Database\n\nProblem\n\nYou need to encrypt the storage of an existing database.\n\nSolution\n\nCreate a read replica of your existing database, take a snapshot of the read replica, copy the snapshot to an encrypted snapshot, and restore the encrypted snapshot to a new encrypted database, as shown in Figure 4-4.\n\nFigure 4-4. Process of encrypting an RDS database using a snapshot\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables\n\nAn RDS MySQL instance with an RDS subnet group\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nVerify that the storage for the database is not encrypted:\n\naws rds describe-db-instances \\\n\n--db-instance-identifier $RDS_DATABASE_ID \\\n\n--query DBInstances[0].StorageEncrypted\n\nYou should see false outputted.\n\n2.\n\nCreate a KMS key to use to encrypt your database snapshot later. Store the key ID in an environment variable:\n\nKEY_ID=$(aws kms create-key \\\n\n--tags TagKey=Name,TagValue=AWSCookbook404RDS \\\n\n--description \"AWSCookbook RDS Key\" \\\n\n--query KeyMetadata.KeyId \\\n\n--output text)\n\n3.\n\nCreate an alias to easily reference the key that you created:\n\naws kms create-alias \\\n\n--alias-name alias/awscookbook404 \\\n\n--target-key-id $KEY_ID\n\n4.\n\nCreate a read replica of your existing unencrypted database:\n\naws rds create-db-instance-read-replica \\\n\n--db-instance-identifier awscookbook404db-rep \\\n\n5.\n\n--source-db-instance-identifier $RDS_DATABASE_ID \\\n\n--max-allocated-storage 10\n\nYou should see output similar to the following:\n\n{\n\n\"DBInstance\": {\n\n\"DBInstanceIdentifier\": \"awscookbook404db-rep\",\n\n\"DBInstanceClass\": \"db.m5.large\",\n\n\"Engine\": \"mysql\",\n\n\"DBInstanceStatus\": \"creating\",\n\n\"MasterUsername\": \"admin\",\n\n\"DBName\": \"AWSCookbookRecipe404\",\n\n\"AllocatedStorage\": 8,\n\n\"PreferredBackupWindow\": \"05:51-06:21\",\n\n\"BackupRetentionPeriod\": 0,\n\n\"DBSecurityGroups\": [],\n\n...\n\nNOTE\n\nBy creating a read replica, you allow the snapshot to be created from it and therefore not affect the\n\nperformance of the primary database.\n\nWait for the DBInstanceStatus to become “available”:\n\naws rds describe-db-instances \\\n\n--db-instance-identifier awscookbook404db-rep \\\n\n--output text --query DBInstances[0].DBInstanceStatus\n\nTake an unencrypted snapshot of your read replica:\n\naws rds create-db-snapshot \\\n\n--db-instance-identifier awscookbook404db-rep \\\n\n--db-snapshot-identifier awscookbook404-snapshot\n\nYou should see output similar to the following:\n\n6.\n\n{\n\n\"DBSnapshot\": {\n\n\"DBSnapshotIdentifier\": \"awscookbook404-snapshot\",\n\n\"DBInstanceIdentifier\": \"awscookbook404db-rep\",\n\n\"Engine\": \"mysql\",\n\n\"AllocatedStorage\": 8,\n\n\"Status\": \"creating\",\n\n\"Port\": 3306,\n\n\"AvailabilityZone\": \"us-east-1b\",\n\n\"VpcId\": \"vpc-<<ID>>\",\n\n\"InstanceCreateTime\": \"2021-09-21T22:46:07.785000+00:00\",\n\nWait for the Status of the snapshot to become available:\n\naws rds describe-db-snapshots \\\n\n--db-snapshot-identifier awscookbook404-snapshot \\\n\n--output text --query DBSnapshots[0].Status\n\nCopy the unencrypted snapshot to a new snapshot while encrypting by specifying your KMS key:\n\naws rds copy-db-snapshot \\\n\n--copy-tags \\\n\n--source-db-snapshot-identifier awscookbook404-snapshot \\\n\n--target-db-snapshot-identifier awscookbook404-snapshot-enc \\\n\n--kms-key-id alias/awscookbook404\n\nYou should see output similar to the following:\n\n{\n\n\"DBSnapshot\": {\n\n\"DBSnapshotIdentifier\": \"awscookbook404-snapshot-enc\",\n\n\"DBInstanceIdentifier\": \"awscookbook404db-rep\",\n\n\"Engine\": \"mysql\",\n\n\"AllocatedStorage\": 8,\n\n\"Status\": \"creating\",\n\n\"Port\": 3306,\n\n\"AvailabilityZone\": \"us-east-1b\",\n\n\"VpcId\": \"vpc-<<ID>>\",\n\n\"InstanceCreateTime\": \"2021-09-21T22:46:07.785000+00:00\",\n\n\"MasterUsername\": \"admin\",\n\n...\n\nTIP\n\nSpecifying a KMS key with the copy-snapshot command encrypts the copied snapshot. Restoring an encrypted snapshot to a new database results in an encrypted database.\n\nWait for the Status of the encrypted snapshot to become available:\n\naws rds describe-db-snapshots \\\n\n--db-snapshot-identifier awscookbook404-snapshot-enc \\\n\n--output text --query DBSnapshots[0].Status\n\n7.\n\nRestore the encrypted snapshot to a new RDS instance:\n\naws rds restore-db-instance-from-db-snapshot \\\n\n--db-subnet-group-name $RDS_SUBNET_GROUP \\\n\n--db-instance-identifier awscookbook404db-enc \\\n\n--db-snapshot-identifier awscookbook404-snapshot-enc\n\nYou should see output similar to the following:\n\n{\n\n\"DBInstance\": {\n\n\"DBInstanceIdentifier\": \"awscookbook404db-enc\",\n\n\"DBInstanceClass\": \"db.m5.large\",\n\n\"Engine\": \"mysql\",\n\n\"DBInstanceStatus\": \"creating\",\n\n\"MasterUsername\": \"admin\",\n\n\"DBName\": \"AWSCookbookRecipe404\",\n\n\"AllocatedStorage\": 8,\n\n...\n\nValidation checks\n\nWait for DBInstanceStatus to become available:\n\naws rds describe-db-instances \\\n\n--db-instance-identifier awscookbook404db-enc \\\n\n--output text --query DBInstances[0].DBInstanceStatus\n\nVerify that the storage is now encrypted:\n\naws rds describe-db-instances \\\n\n--db-instance-identifier awscookbook404db-enc \\\n\n--query DBInstances[0].StorageEncrypted\n\nYou should see true outputted.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen you complete the steps, you need to reconfigure your application to point to a new database endpoint hostname. To perform this with minimal downtime, you can configure a Route 53 DNS record that points to your database endpoint. Your application would be configured to use the DNS record. Then you would shift your database traffic over to the new encrypted database by updating the DNS record with the new database endpoint DNS.\n\nEncryption at rest is a security approach left up to end users in the AWS shared responsibility model, and often it is required to achieve or maintain compliance with regulatory standards. The encrypted snapshot you took could also be automatically copied to another Region, as well as exported to S3 for archival/backup purposes.\n\nChallenge\n\nCreate an RDS database from scratch that initially has encrypted storage and migrate your data from your existing database to the new database using AWS DMS, as shown in Recipe 4.7.\n\n4.5 Automating Password Rotation for RDS Databases\n\nProblem\n\nYou would like to implement automatic password rotation for a database user.\n\nSolution\n\nCreate a password and place it in AWS Secrets Manager. Configure a rotation interval for the secret containing the password. Finally, create a Lambda function using AWS-provided code, and configure the function to perform the password rotation. This configuration allows the password rotation automation to perform as shown in Figure 4-5.\n\nFigure 4-5. Secrets Manager Lambda function integration\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nMySQL RDS instance and EC2 instance deployed. You will need the ability to connect to these for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nUse AWS Secrets Manager to generate a password that meets RDS requirements:\n\nRDS_ADMIN_PASSWORD=$(aws secretsmanager get-random-password \\\n\n--exclude-punctuation \\\n\n--password-length 41 --require-each-included-type \\\n\n--output text --query RandomPassword)\n\nTIP\n\nYou can call the Secrets Manager GetRandomPassword API method to generate random strings of\n\ncharacters for various uses beyond password generation.\n\n2.\n\nChange the admin password for your RDS database to the one you just created:\n\naws rds modify-db-instance \\\n\n--db-instance-identifier $RDS_DATABASE_ID \\\n\n--master-user-password $RDS_ADMIN_PASSWORD \\\n\n--apply-immediately\n\nYou should see output similar to the following:\n\n{\n\n\"DBInstance\": {\n\n\"DBInstanceIdentifier\": \"awscookbook405db\",\n\n\"DBInstanceClass\": \"db.m5.large\",",
      "page_number": 207
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 222-231)",
      "start_page": 222,
      "end_page": 231,
      "detection_method": "topic_boundary",
      "content": "3.\n\n4.\n\n5.\n\n\"Engine\": \"mysql\",\n\n\"DBInstanceStatus\": \"available\",\n\n\"MasterUsername\": \"admin\",\n\n\"DBName\": \"AWSCookbookRecipe405\",\n\n...\n\nCreate a file with the following content called rdscreds-template.json (file provided in the repository):\n\n{\n\n\"username\": \"admin\",\n\n\"password\": \"PASSWORD\",\n\n\"engine\": \"mysql\",\n\n\"host\": \"HOST\",\n\n\"port\": 3306,\n\n\"dbname\": \"DBNAME\",\n\n\"dbInstanceIdentifier\": \"DBIDENTIFIER\"\n\n}\n\nUse sed to modify the values in rdscreds-template.json to create rdscreds.json:\n\nsed -e \"s/AWS_ACCOUNT_ID/${AWS_ACCOUNT_ID}/g\" \\\n\ne \"s|PASSWORD|${RDS_ADMIN_PASSWORD}|g\" \\\n\ne \"s|HOST|${RdsEndpoint}|g\" \\\n\ne \"s|DBNAME|${DbName}|g\" \\\n\ne \"s|DBIDENTIFIER|${RdsDatabaseId}|g\" \\\n\nrdscreds-template.json > rdscreds.json\n\nDownload code from the AWS Samples GitHub repository for the Rotation Lambda function:\n\nwget https://raw.githubusercontent.com/aws-samples/aws-secrets-manager-rotation-\n\nlambdas/master/SecretsManagerRDSMySQLRotationSingleUser/lambda_function.py\n\nNOTE\n\nAWS provides information and templates for different database rotation scenarios in this article.\n\n6.\n\n7.\n\n8.\n\nCompress the file containing the code:\n\nzip lambda_function.zip lambda_function.py\n\nYou should see output similar to the following:\n\nadding: lambda_function.py (deflated 76%)\n\nCreate a new security group for the Lambda function to use:\n\nLAMBDA_SG_ID=$(aws ec2 create-security-group \\\n\n--group-name AWSCookbook405LambdaSG \\\n\n--description \"Lambda Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\nAdd an ingress rule to the RDS instances security group that allows access on TCP port 3306 from the Lambda’s security group:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 3306 \\\n\n--source-group $LAMBDA_SG_ID \\\n\n--group-id $RDS_SECURITY_GROUP\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<ID>>\",\n\n\"GroupId\": \"sg-<<ID>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 3306,\n\n9.\n\n10.\n\n\"ToPort\": 3306,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<ID>>\"\n\n}\n\n}\n\n]\n\n}\n\nCreate a file named assume-role-policy.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"lambda.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate an IAM role with the statement in the provided assume-role- policy.json file using this command:\n\naws iam create-role --role-name AWSCookbook405Lambda \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Role\": {\n\n\"Path\": \"/\",\n\n\"RoleName\": \"AWSCookbook405Lambda\",\n\n\"RoleId\": \"<<ID>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:role/AWSCookbook405Lambda\",\n\n\"CreateDate\": \"2021-09-21T23:01:57+00:00\",\n\n\"AssumeRolePolicyDocument\": {\n\n11.\n\n12.\n\n13.\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n...\n\nAttach the IAM managed policy for AWSLambdaVPCAccess to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook405Lambda \\\n\n--policy-arn arn:aws:iam::aws:policy/service-\n\nrole/AWSLambdaVPCAccessExecutionRole\n\nAttach the IAM managed policy for SecretsManagerReadWrite to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook405Lambda \\\n\n--policy-arn arn:aws:iam::aws:policy/SecretsManagerReadWrite\n\nTIP\n\nThe IAM role that you associated with the Lambda function to rotate the password used the\n\nSecretsManagerReadWrite managed policy. In a production scenario, you would want to scope this down to limit which secrets the Lambda function can interact with.\n\nCreate the Lambda function to perform the secret rotation using the code:\n\nLAMBDA_ROTATE_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook405Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--environment Variables=\n\n{SECRETS_MANAGER_ENDPOINT=https://secretsmanager.$AWS_REGION.amazonaws.com} \\\n\n--layers $PyMysqlLambdaLayerArn \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook405Lambda \\\n\n--output text --query FunctionArn \\\n\n--vpc-config SubnetIds=${ISOLATED_SUBNETS},SecurityGroupIds=$LAMBDA_SG_ID)\n\n14.\n\n15.\n\n16.\n\nUse this command to determine when the Lambda function has entered the Active state:\n\naws lambda get-function --function-name $LAMBDA_ROTATE_ARN \\\n\n--output text --query Configuration.State\n\nAdd a permission to the Lambda function so that Secrets Manager can invoke it:\n\naws lambda add-permission --function-name $LAMBDA_ROTATE_ARN \\\n\n--action lambda:InvokeFunction --statement-id secretsmanager \\\n\n--principal secretsmanager.amazonaws.com\n\nYou should see output similar to the following:\n\n{\n\n\"Statement\": \"{\\\"Sid\\\":\\\"secretsmanager\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":\n\n{\\\"Service\\\":\\\"secretsmanager.amazonaws.com\\\"},\\\"Action\\\":\\\"lambda:InvokeFunctio\n\nn\\\",\\\"Resource\\\":\\\"arn:aws:lambda:us-east-\n\n1:111111111111:function:AWSCookbook405Lambda\\\"}\"\n\n}\n\nSet a unique suffix to use for the secret name to ensure you can reuse this pattern for additional automatic password rotations if desired:\n\nAWSCookbook405SecretName=AWSCookbook405Secret-$(aws secretsmanager \\\n\nget-random-password \\\n\n--exclude-punctuation \\\n\n--password-length 6 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nCreate a secret in Secrets Manager to store your admin password:\n\naws secretsmanager create-secret --name $AWSCookbook405SecretName \\\n\n17.\n\n18.\n\n--description \"My database secret created with the CLI\" \\\n\n--secret-string file://rdscreds.json\n\nYou should see output similar to the following:\n\n{\n\n\"ARN\": \"arn:aws:secretsmanager:us-east-\n\n1:1111111111111:secret:AWSCookbook405Secret-T4tErs-AlJcLn\",\n\n\"Name\": \"AWSCookbook405Secret-<<Random>>\",\n\n\"VersionId\": \"<<ID>>\"\n\n}\n\nSet up automatic rotation every 30 days and specify the Lambda function to perform rotation for the secret you just created:\n\naws secretsmanager rotate-secret \\\n\n--secret-id $AWSCookbook405SecretName \\\n\n--rotation-rules AutomaticallyAfterDays=30 \\\n\n--rotation-lambda-arn $LAMBDA_ROTATE_ARN\n\nYou should see output similar to the following:\n\n{\n\n\"ARN\": \"arn:aws:secretsmanager:us-east-\n\n1:1111111111111:secret:AWSCookbook405Secret-<<unique>>\",\n\n\"Name\": \"AWSCookbook405Secret-<<unique>>\",\n\n\"VersionId\": \"<<ID>>\"\n\n}\n\nNOTE\n\nThe rotate-secret command triggers an initial rotation of the password. You will trigger an extra rotation of the password in the next step to demonstrate how to perform rotations on demand.\n\nPerform another rotation of the secret:\n\naws secretsmanager rotate-secret --secret-id $AWSCookbook405SecretName\n\nYou should see output similar to the output from step 17. Notice that the V e r s i o n I d will be different from the last command indicating that the secret has been rotated.\n\nValidation checks\n\nRetrieve the RDS admin password from Secrets Manager:\n\nRDS_ADMIN_PASSWORD=$(aws secretsmanager get-secret-value --secret-id $AWSCookbook405SecretName --query\n\nSecretString | jq -r | jq .password | tr -d '\"')\n\nList the endpoint for the RDS cluster:\n\necho $RDS_ENDPOINT\n\nRetrieve the password for your RDS cluster:\n\necho $RDS_ADMIN_PASSWORD\n\nConnect to the EC2 instance by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nInstall the MySQL client:\n\nsudo yum -y install mysql\n\nConnect to the database to verify that the latest rotated password is working. You’ll need to copy and paste the password (outputted previously):\n\nmysql -u admin -p$password -h $hostname\n\nRun a SELECT statement on the mysql.user table to validate administrator permissions:\n\nSELECT user FROM mysql.user;\n\nYou should see output similar to the following:\n\n+------------------+\n\n| user |\n\n+------------------+\n\n| admin |\n\n| mysql.infoschema |\n\n| mysql.session |\n\n| mysql.sys |\n\n| rdsadmin |\n\n+------------------+\n\n5 rows in set (0.00 sec)\n\nExit from the mysql prompt:\n\nquit\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThe AWS-provided Lambda function stores the rotated password in Secrets Manager. You can then configure your application to retrieve secrets from Secrets Manager directly; or the Lambda function you configured to update the Secrets Manager values could also store the password in a\n\nsecure location of your choosing. You would need to grant the Lambda additional permissions to interact with the secure location you choose and add some code to store the new value there. This method could also be applied to rotate the passwords for nonadmin database user accounts by following the same steps after you have created the user(s) in your database.\n\nThe Lambda function you deployed is Python-based and connects to a MySQL engine- compatible database. The Lambda runtime environment does not have this library included by default, so you specified a Lambda layer with the aws lambda create-function command. This layer is required so that the PyMySQL library was available to the function in the Lambda runtime environment, and it was deployed for you as part of the preparation step when you ran cdk deploy.\n\nChallenge\n\nCreate another Lambda function and a separate IAM role. Grant this new function access to the same secret.\n\nSee Also\n\nRecipe 5.2\n\n4.6 Autoscaling DynamoDB Table Provisioned Capacity\n\nProblem\n\nYou have a DynamoDB database table with a low provisioned throughput. You realize that your application load is variable and you may need to scale up or scale down your provisioned throughput based on the variability of the incoming application load.\n\nSolution\n\nConfigure read and write scaling by setting a scaling target and a scaling policy for the read and write capacity of the DynamoDB table by using AWS application autoscaling, as shown in Figure 4-6.\n\nFigure 4-6. DynamoDB autoscaling configuration\n\nPrerequisite\n\nA DynamoDB table",
      "page_number": 222
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 232-241)",
      "start_page": 232,
      "end_page": 241,
      "detection_method": "topic_boundary",
      "content": "Preparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nNavigate to this recipe’s directory in the chapter repository:\n\ncd 406-Auto-Scaling-DynamoDB\n\n2.\n\nRegister a ReadCapacityUnits scaling target for the DynamoDB table:\n\naws application-autoscaling register-scalable-target \\\n\n--service-namespace dynamodb \\\n\n--resource-id \"table/AWSCookbook406\" \\\n\n--scalable-dimension \"dynamodb:table:ReadCapacityUnits\" \\\n\n--min-capacity 5 \\\n\n--max-capacity 10\n\n3.\n\nRegister a WriteCapacityUnits scaling target for the DynamoDB table:\n\naws application-autoscaling register-scalable-target \\\n\n--service-namespace dynamodb \\\n\n--resource-id \"table/AWSCookbook406\" \\\n\n--scalable-dimension \"dynamodb:table:WriteCapacityUnits\" \\\n\n--min-capacity 5 \\\n\n--max-capacity 10\n\n4.\n\nCreate a scaling policy JSON file for read capacity scaling (read-policy.json provided in the repository):\n\n{\n\n\"PredefinedMetricSpecification\": {\n\n\"PredefinedMetricType\": \"DynamoDBReadCapacityUtilization\"\n\n},\n\n\"ScaleOutCooldown\": 60,\n\n\"ScaleInCooldown\": 60,\n\n\"TargetValue\": 50.0\n\n}\n\n5.\n\n6.\n\n7.\n\nCreate a scaling policy JSON file for write capacity scaling (write- policy.json file provided in the repository):\n\n{\n\n\"PredefinedMetricSpecification\": {\n\n\"PredefinedMetricType\": \"DynamoDBWriteCapacityUtilization\"\n\n},\n\n\"ScaleOutCooldown\": 60,\n\n\"ScaleInCooldown\": 60,\n\n\"TargetValue\": 50.0\n\n}\n\nNOTE\n\nDynamoDB-provisioned capacity uses capacity units to define the read and write capacity of your\n\ntables. The target value that you set defines when to scale based on the current usage. Scaling\n\ncooldown parameters define, in seconds, how long to wait to scale again after a scaling operation has\n\ntaken place. For more information, see the API reference for autoscaling\n\nTargetTrackingScalingPolicyConfiguration.\n\nApply the read scaling policy to the table by using the read-policy.json file:\n\naws application-autoscaling put-scaling-policy \\\n\n--service-namespace dynamodb \\\n\n--resource-id \"table/AWSCookbook406\" \\\n\n--scalable-dimension \"dynamodb:table:ReadCapacityUnits\" \\\n\n--policy-name \"AWSCookbookReadScaling\" \\\n\n--policy-type \"TargetTrackingScaling\" \\\n\n--target-tracking-scaling-policy-configuration \\\n\nfile://read-policy.json\n\nApply the write scaling policy to the table using the write-policy.json file:\n\naws application-autoscaling put-scaling-policy \\\n\n--service-namespace dynamodb \\\n\n--resource-id \"table/AWSCookbook406\" \\\n\n--scalable-dimension \"dynamodb:table:WriteCapacityUnits\" \\\n\n--policy-name \"AWSCookbookWriteScaling\" \\\n\n--policy-type \"TargetTrackingScaling\" \\\n\n--target-tracking-scaling-policy-configuration \\\n\nfile://write-policy.json\n\nValidation checks\n\nYou can observe the autoscaling configuration for your table by selecting it in the DynamoDB console and looking under the “Additional settings” tab.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nTIP\n\nThese steps will autoscale read and write capacities independently for your DynamoDB table, which helps you achieve the\n\nlowest operating cost model for your application’s specific requirements.\n\nDynamoDB allows for two capacity modes: provisioned and on-demand. When using provisioned capacity mode, you are able to select the number of data reads and writes per second. The pricing guide notes that you are charged according to the capacity units you specify. Conversely, with on-demand capacity mode, you pay per request for the data reads and writes your application performs on your tables. In general, using on-demand mode can result in higher costs over provisioned mode for especially transactionally heavy applications.\n\nYou need to understand your application and usage patterns when selecting a provisioned capacity for your tables. If you set the capacity too low, you will experience slow database performance and your application could enter error and wait states, since the DynamoDB API will return ThrottlingException and ProvisionedThroughputExceededException responses to your application when these limits are met. If you set the capacity too high, you are paying for unneeded capacity. Enabling autoscaling allows you to define minimum and maximum target values by setting a scaling target, while also allowing you to define when the autoscaling trigger should go into effect for scaling up, and when it should begin to scale down your capacity. This allows you to optimize for both cost and performance while taking advantage of the DynamoDB service. To see a list of the scalable targets that you configured for your table, you can use the following command:\n\naws application-autoscaling describe-scalable-targets \\\n\n--service-namespace dynamodb \\\n\n--resource-id \"table/AWSCookbook406\"\n\nFor more information on DynamoDB capacities and how they are measured, see this support document.\n\nChallenge\n\nCreate a Lambda function that monitors the performance of your DynamoDB table, and then modify the autoscaling target minimums and maximums accordingly.\n\n4.7 Migrating Databases to Amazon RDS Using AWS DMS\n\nProblem\n\nYou need to move data from a source database to a target database.\n\nSolution\n\nConfigure the VPC security groups and IAM permissions to allow AWS Database Migration Service (DMS) connectivity to the databases. Then, configure the DMS endpoints for the source and target databases. Next, configure a DMS replication task. Finally, start the replication task. An architecture diagram of the solution is shown in Figure 4-7.\n\nFigure 4-7. DMS network diagram\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a security group for the replication instance:\n\nDMS_SG_ID=$(aws ec2 create-security-group \\\n\n--group-name AWSCookbook407DMSSG \\\n\n--description \"DMS Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\n2.\n\nGrant the DMS security group access to the source and target databases on TCP port 3306:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 3306 \\\n\n--source-group $DMS_SG_ID \\\n\n--group-id $SOURCE_RDS_SECURITY_GROUP\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 3306 \\\n\n--source-group $DMS_SG_ID \\\n\n--group-id $TARGET_RDS_SECURITY_GROUP\n\n3.\n\nCreate a role for DMS by using the assume-role-policy.json provided:\n\naws iam create-role --role-name dms-vpc-role \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\nWARNING\n\nThe DMS service requires an IAM role with a specific name and a specific policy. The command you\n\nran previously satisfies this requirement. You may also already have this role in your account if you\n\nhave used DMS previously. This command would result in an error if that is the case, and you can\n\nproceed with the next steps without concern.\n\n4.\n\nAttach the managed DMS policy to the role:\n\n5.\n\n6.\n\naws iam attach-role-policy --role-name dms-vpc-role --policy-arn \\\n\narn:aws:iam::aws:policy/service-role/AmazonDMSVPCManagementRole\n\nCreate a replication subnet group for the replication instance:\n\nREP_SUBNET_GROUP=$(aws dms create-replication-subnet-group \\\n\n--replication-subnet-group-identifier awscookbook407 \\\n\n--replication-subnet-group-description \"AWSCookbook407\" \\\n\n--subnet-ids $ISOLATED_SUBNETS \\\n\n--query ReplicationSubnetGroup.ReplicationSubnetGroupIdentifier \\\n\n--output text)\n\nCreate a replication instance and save the ARN in a variable:\n\nREP_INSTANCE_ARN=$(aws dms create-replication-instance \\\n\n--replication-instance-identifier awscookbook407 \\\n\n--no-publicly-accessible \\\n\n--replication-instance-class dms.t2.medium \\\n\n--vpc-security-group-ids $DMS_SG_ID \\\n\n--replication-subnet-group-identifier $REP_SUBNET_GROUP \\\n\n--allocated-storage 8 \\\n\n--query ReplicationInstance.ReplicationInstanceArn \\\n\n--output text)\n\nWait until the ReplicationInstanceStatus reaches available; check the status by using this command:\n\naws dms describe-replication-instances \\\n\n--filter=Name=replication-instance-id,Values=awscookbook407 \\\n\n--query ReplicationInstances[0].ReplicationInstanceStatus\n\nWARNING\n\nYou used the dms.t2.medium replication instance size for this example. You should choose an instance size appropriate to handle the amount of data you will be migrating. DMS transfers tables in parallel,\n\nso you will need a larger instance size for larger amounts of data. For more information, see this user\n\nguide document about best practices for DMS.\n\n7.\n\n8.\n\n9.\n\n10.\n\nRetrieve the source and target DB admin passwords from Secrets Manager and save to environment variables:\n\nRDS_SOURCE_PASSWORD=$(aws secretsmanager get-secret-value --secret-id\n\n$RDS_SOURCE_SECRET_NAME --query\n\nSecretString --output text | jq .password | tr -d '\"')\n\nRDS_TARGET_PASSWORD=$(aws secretsmanager get-secret-value --secret-id\n\n$RDS_TARGET_SECRET_NAME --query\n\nSecretString --output text | jq .password | tr -d '\"')\n\nCreate a source endpoint for DMS and save the ARN to a variable:\n\nSOURCE_ENDPOINT_ARN=$(aws dms create-endpoint \\\n\n--endpoint-identifier awscookbook407source \\\n\n--endpoint-type source --engine-name mysql \\\n\n--username admin --password $RDS_SOURCE_PASSWORD \\\n\n--server-name $SOURCE_RDS_ENDPOINT --port 3306 \\\n\n--query Endpoint.EndpointArn --output text)\n\nCreate a target endpoint for DMS and save the ARN to a variable:\n\nTARGET_ENDPOINT_ARN=$(aws dms create-endpoint \\\n\n--endpoint-identifier awscookbook407target \\\n\n--endpoint-type target --engine-name mysql \\\n\n--username admin --password $RDS_TARGET_PASSWORD \\\n\n--server-name $TARGET_RDS_ENDPOINT --port 3306 \\\n\n--query Endpoint.EndpointArn --output text)\n\nCreate your replication task:\n\nREPLICATION_TASK_ARN=$(aws dms create-replication-task \\\n\n--replication-task-identifier awscookbook-task \\\n\n--source-endpoint-arn $SOURCE_ENDPOINT_ARN \\\n\n--target-endpoint-arn $TARGET_ENDPOINT_ARN \\\n\n--replication-instance-arn $REP_INSTANCE_ARN \\\n\n--migration-type full-load \\\n\n--table-mappings file://table-mapping-all.json \\\n\n--query ReplicationTask.ReplicationTaskArn --output text)\n\nWait for the status to reach ready. To check the status of the replication task, use the following:\n\naws dms describe-replication-tasks \\\n\n--filters \"Name=replication-task-arn,Values=$REPLICATION_TASK_ARN\" \\\n\n--query \"ReplicationTasks[0].Status\"\n\n11.\n\nStart the replication task:\n\naws dms start-replication-task \\\n\n--replication-task-arn $REPLICATION_TASK_ARN \\\n\n--start-replication-task-type start-replication\n\nValidation checks\n\nMonitor the progress of the replication task:\n\naws dms describe-replication-tasks\n\nUse the AWS Console or the aws dms describe-replication-tasks operation to validate that your tables have been migrated:\n\naws dms describe-replication-tasks \\\n\n--query ReplicationTasks[0].ReplicationTaskStats\n\nYou can also view the status of the replication task in the DMS console.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nTIP\n\nYou could also run full-load-and-cdc to continuously replicate changes on the source to the destination to minimize your application downtime when you cut over to the new database.\n\nDMS comes with functionality to test source and destination endpoints from the replication instance. This is a handy feature to use when working with DMS to validate that you have the configuration correct before you start to run replication tasks. Testing connectivity from the replication instance to both of the endpoints you configured can be done through the DMS console or the command line with the following commands:\n\naws dms test-connection \\\n\n--replication-instance-arn $rep_instance_arn \\\n\n--endpoint-arn $source_endpoint_arn\n\naws dms test-connection \\\n\n--replication-instance-arn $rep_instance_arn \\\n\n--endpoint-arn $target_endpoint_arn\n\nThe test-connection operation takes a few moments to complete. You can check the status and the results of the operation by using this command:\n\naws dms describe-connections --filter \\\n\n\"Name=endpoint-arn,Values=$source_endpoint_arn,$target_endpoint_arn\"\n\nThe DMS service supports many types of source and target databases within your VPC, another AWS account, or databases hosted in a non-AWS environment. The service can also transform data for you if your source and destination are different types of databases by using additional configuration in the table-mappings.json file. For example, the data type of a column in an Oracle database may have a different format than the equivalent type in a PostgreSQL database. The AWS Schema Conversion Tool (SCT) can assist with identifying these necessary transforms, and also generate configuration files to use with DMS.\n\nChallenge\n\nEnable full load and ongoing replication to continuously replicate from one database to another.\n\n4.8 Enabling REST Access to Aurora Serverless Using RDS Data API",
      "page_number": 232
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 242-253)",
      "start_page": 242,
      "end_page": 253,
      "detection_method": "topic_boundary",
      "content": "Problem\n\nYou have a PostgreSQL database and would like to connect to it without having your application manage persistent database connections.\n\nSolution\n\nFirst, enable the Data API for your database and configure the IAM permissions for your EC2 instance. Then, test from both the CLI and RDS console. This allows your application to connect to your Aurora Serverless database, as shown in Figure 4-8.\n\nFigure 4-8. An application using the RDS Data API\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nPostgreSQL RDS instance and EC2 instance deployed. You will need the ability to connect to these for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nEnable the Data API on your Aurora Serverless cluster:\n\naws rds modify-db-cluster \\\n\n2.\n\n3.\n\n4.\n\n--db-cluster-identifier $CLUSTER_IDENTIFIER \\\n\n--enable-http-endpoint \\\n\n--apply-immediately\n\nEnsure that HttpEndpointEnabled is set to true:\n\naws rds describe-db-clusters \\\n\n--db-cluster-identifier $CLUSTER_IDENTIFIER \\\n\n--query DBClusters[0].HttpEndpointEnabled\n\nTest a command from your CLI:\n\naws rds-data execute-statement \\\n\n--secret-arn \"$SECRET_ARN\" \\\n\n--resource-arn \"$CLUSTER_ARN\" \\\n\n--database \"$DATABASE_NAME\" \\\n\n--sql \"select * from pg_user\" \\\n\n--output json\n\n(Optional) You can also test access via the AWS Console using the Amazon RDS Query Editor. First run these two commands from your terminal so you can copy and paste the values:\n\necho $SECRET_ARN\n\necho $DATABASE_NAME\n\nLog in to the AWS Console with admin permissions and go to the RDS console. On the lefthand sidebar menu, click Query Editor. Fill out the values and select “Connect to database,” as shown in Figure 4-9.\n\n5.\n\nFigure 4-9. Connect to database settings\n\nRun the same query and view the results below the Query Editor (see Figure 4-10):\n\n6.\n\nSELECT * from pg_user;\n\nFigure 4-10. RDS Query Editor\n\nConfigure your EC2 instance to use the Data API with your database cluster. Create a file called policy-template.json with the following content (file provided in the repository):\n\n{\n\n7.\n\n8.\n\n9.\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Action\": [\n\n\"rds-data:BatchExecuteStatement\",\n\n\"rds-data:BeginTransaction\",\n\n\"rds-data:CommitTransaction\",\n\n\"rds-data:ExecuteStatement\",\n\n\"rds-data:RollbackTransaction\"\n\n],\n\n\"Resource\": \"*\",\n\n\"Effect\": \"Allow\"\n\n},\n\n{\n\n\"Action\": [\n\n\"secretsmanager:GetSecretValue\",\n\n\"secretsmanager:DescribeSecret\"\n\n],\n\n\"Resource\": \"SecretArn\",\n\n\"Effect\": \"Allow\"\n\n}\n\n]\n\n}\n\nReplace the values in the template file by using the sed command with environment variables you have set:\n\nsed -e \"s/SecretArn/${SECRET_ARN}/g\" \\\n\npolicy-template.json > policy.json\n\nCreate an IAM policy by using the file you just created:\n\naws iam create-policy --policy-name AWSCookbook408RDSDataPolicy \\\n\n--policy-document file://policy.json\n\nAttach the IAM policy for AWSCookbook408RDSDataPolicy to your EC2 instance’s IAM role:\n\naws iam attach-role-policy --role-name $INSTANCE_ROLE_NAME \\\n\n--policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook408RDSDataPolicy\n\nValidation checks\n\nCreate and populate some SSM parameters to store values so that you can retrieve them from your EC2 instance:\n\naws ssm put-parameter \\\n\n--name \"Cookbook408DatabaseName\" \\\n\n--type \"String\" \\\n\n--value $DATABASE_NAME\n\naws ssm put-parameter \\\n\n--name \"Cookbook408ClusterArn\" \\\n\n--type \"String\" \\\n\n--value $CLUSTER_ARN\n\naws ssm put-parameter \\\n\n--name \"Cookbook408SecretArn\" \\\n\n--type \"String\" \\\n\n--value $SECRET_ARN\n\nConnect to the EC2 instance by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nSet the Region:\n\nexport AWS_DEFAULT_REGION=us-east-1\n\nRetrieve the SSM parameter values and set them to environment values:\n\nDatabaseName=$(aws ssm get-parameters \\\n\n--names \"Cookbook408DatabaseName\" \\\n\n--query \"Parameters[*].Value\" --output text)\n\nSecretArn=$(aws ssm get-parameters \\\n\n--names \"Cookbook408SecretArn\" \\\n\n--query \"Parameters[*].Value\" --output text)\n\nClusterArn=$(aws ssm get-parameters \\\n\n--names \"Cookbook408ClusterArn\" \\\n\n--query \"Parameters[*].Value\" --output text)\n\nRun a query against the database:\n\naws rds-data execute-statement \\\n\n--secret-arn \"$SecretArn\" \\\n\n--resource-arn \"$ClusterArn\" \\\n\n--database \"$DatabaseName\" \\\n\n--sql \"select * from pg_user\" \\\n\n--output json\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThe Data API exposes an HTTPS endpoint for usage with Aurora and uses IAM authentication to allow your application to execute SQL statements on your database over HTTPS instead of using classic TCP database connectivity.\n\nTIP\n\nPer the Aurora user guide, all calls to the Data API are synchronous, and the default timeout for a query is 45 seconds. If\n\nyour queries take longer than 45 seconds, you can use the continueAfterTimeout parameter to facilitate long-running queries.\n\nAs is the case with other AWS service APIs that use IAM authentication, all activities performed with the Data API are captured in CloudTrail to ensure an audit trail is present, which can help satisfy your security and audit requirements. You can control and delegate access to the Data API endpoint by using IAM policies associated with roles for your application. For example, if you wanted to grant your application the ability to only read from your database using the Data API, you could write a policy that omits the rds-data:CommitTransaction and rds- data:RollbackTransaction permissions.\n\nThe Query Editor within the RDS console provides a web-based means of access for executing SQL queries against your database. This is a convenient mechanism for developers and DBAs to quickly accomplish bespoke tasks. The same privileges that you assigned your EC2 instance in this recipe would need to be granted to your developer and DBA via IAM roles.\n\nChallenge\n\nCreate and deploy a Lambda function that has permissions to access the RDS Data API that you provisioned.\n\nChapter 5. Serverless\n\n5.0 Introduction The technology industry term serverless can sometimes lead to confusion in that servers are involved with the cloud services associated with this model of execution. The advantage is that end users do not need to worry about managing the underlying infrastructure and platform. The cloud provider (in this case, AWS) is responsible for all of the management, operating system updates, availability, capacity, and more.\n\nIn terms of “serverless” services available on AWS, many options are available to take advantage of the benefits. Here are some examples:\n\nAWS Lambda and Amazon Fargate for compute\n\nAmazon EventBridge, Amazon SNS, Amazon SQS, and Amazon API Gateway for application integration\n\nAmazon S3, Amazon DynamoDB, and Amazon Aurora Serverless for datastores\n\nThe main benefits of serverless on AWS are as follows:\n\nCost savings\n\nYou pay for only what you use.\n\nScalability\n\nScale up to what you need; scale down to save costs.\n\nLess management\n\nThere are no servers to deploy or systems to manage.\n\nFlexibility\n\nMany programming languages are supported in AWS Lambda.\n\nThe recipes in this chapter will enable you to explore several of the AWS services that fall under the “serverless” umbrella. You will find opportunities to extend the solutions with challenges and get experience with some new services that are leading the serverless industry trend.\n\nWorkstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Serverless\n\nChapter Prerequisites\n\nIAM role for Lambda function execution\n\nCreate a file named assume-role-policy.json with the following content (file provided in root of the chapter repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"lambda.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nNOTE\n\nA similar role is created automatically when you create a Lambda function in the AWS Management Console and select\n\n“Create a new role from AWS policy templates” for the execution role.\n\nCreate an IAM role with the statement in the provided assume-role-policy.json file using this command:\n\naws iam create-role --role-name AWSCookbookLambdaRole \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\nAttach the AWSLambdaBasicExecutionRole IAM managed policy to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbookLambdaRole \\\n\n--policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n\n5.1 Configuring an ALB to Invoke a Lambda Function\n\nProblem\n\nYou have a requirement that your entire web application must be exposed to the internet with a load balancer. Your application architecture includes serverless functions. You need a function to be able to respond to HTTP requests for specific URL paths.\n\nSolution\n\nGrant the Elastic Load Balancing service permission to invoke Lambda functions; then create a Lambda function. Create an ALB target group; then register the Lambda function with the target group. Associate the target group with the listener on your ALB. Finally, add a listener rule that directs traffic for the /function path to the Lambda function (see Figure 5-1).\n\nFigure 5-1. Lambda function invoked by ALB\n\nPrerequisites\n\nVPC with public subnets in two AZs and associated route tables\n\nAn Application Load Balancer that includes the following:\n\nAn associated security group that allows port 80 from the world\n\nA listener on port 80\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCompress the function code provided in this recipe’s directory in the repository. This code will be used for the Lambda function. You can modify this code if you wish, but we have provided code for you to use. For example, zip the code with the following command:",
      "page_number": 242
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 254-261)",
      "start_page": 254,
      "end_page": 261,
      "detection_method": "topic_boundary",
      "content": "2.\n\n3.\n\n4.\n\n5.\n\nzip lambda_function.zip lambda_function.py\n\nCreate a Lambda function that will respond to HTTP requests:\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook501Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole \\\n\n--output text --query FunctionArn)\n\nCreate an ALB target group with the target type set to lambda:\n\nTARGET_GROUP_ARN=$(aws elbv2 create-target-group \\\n\n--name awscookbook501tg \\\n\n--target-type lambda --output text \\\n\n--query TargetGroups[0].TargetGroupArn)\n\nUse the add-permission command to give the Elastic Load Balancing service permission to invoke your Lambda function:\n\naws lambda add-permission \\\n\n--function-name $LAMBDA_ARN \\\n\n--statement-id load-balancer \\\n\n--principal elasticloadbalancing.amazonaws.com \\\n\n--action lambda:InvokeFunction \\\n\n--source-arn $TARGET_GROUP_ARN\n\nUse the register-targets command to register the Lambda function as a target:\n\naws elbv2 register-targets \\\n\n--target-group-arn $TARGET_GROUP_ARN \\\n\n--targets Id=$LAMBDA_ARN\n\n6.\n\nModify the listener for your ALB on port 80; then create a rule that forwards traffic destined for the /function path to your target group:\n\nRULE_ARN=$(aws elbv2 create-rule \\\n\n--listener-arn $LISTENER_ARN --priority 10 \\\n\n--conditions Field=path-pattern,Values='/function' \\\n\n--actions Type=forward,TargetGroupArn=$TARGET_GROUP_ARN \\\n\n--output text --query Rules[0].RuleArn)\n\nValidation checks\n\nTest the invocation to verify that the Lambda function is invoked when requesting /function:\n\ncurl -v $LOAD_BALANCER_DNS/function\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nDevelopers and software architects can leverage Lambda functions to provide a programmatic response to some kind of event within a larger system. This type of compute is most often used when the functions are responsible for a small unit of work. AWS added the ability for Application Load Balancers to invoke Lambda functions in 2018.\n\nWhen an end user requests a specific URL path (configured by the developer) on an ALB, the ALB can pass the request to a Lambda function to handle the response. The ALB then receives the output from the function and hands the result back to the end user as an HTTP response. ALBs can have multiple paths and targets configured for a single load balancer, sending portions of traffic to specific targets (Lambda functions, containers, EC2 instances, etc.) ALBs also support routing to Lambda functions using header values. This simple architecture is extremely cost-effective and highly scalable.\n\nThis flexibility allows for a single ALB to handle all the traffic for your application and provides for a nice building block when architecting a system that needs to be exposed via HTTP/HTTPS.\n\nChallenge 1\n\nAdd a Fargate task that responds to another path on your ALB.\n\nChallenge 2\n\nTry using an Amazon API Gateway to play in front of your Lambda function.\n\n5.2 Packaging Libraries with Lambda Layers\n\nProblem\n\nYou have Python code using external libraries that you need to include with your serverless function deployments.\n\nSolution\n\nCreate a folder and use pip to install a Python package to the folder. Then, zip the folder and use the .zip file to create a Lambda layer that your function will leverage (see Figure 5-2).\n\nFigure 5-2. Lambda function layer creation and representation\n\nPrerequisite\n\nIAM role that allows Lambda functions to execute (provided in chapter prerequisites)\n\nSteps\n\n1.\n\nIn the root of this chapter’s repository, cd to the 502-Packaging-Libraries- with-Lambda-Layers directory and follow the subsequent steps:\n\ncd 502-Packaging-Libraries-with-Lambda-Layers/\n\n2.\n\n3.\n\n4.\n\n5.\n\nZip up lambda_function.py provided in the repository:\n\nzip lambda_function.zip lambda_function.py\n\nYou should see output similar to the following:\n\nupdating: lambda_function.py (deflated 49%)\n\nCreate a Lambda function that we will add a layer to:\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook502Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole \\\n\n--output text --query FunctionArn)\n\nCreate a directory for the layer contents:\n\nmkdir python\n\nUse pip to install the latest requests module to the directory:\n\npip install requests --target=\"./python\"\n\nYou should see output similar to the following:\n\nCollecting requests\n\nUsing cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n\nCollecting certifi>=2017.4.17\n\n6.\n\nUsing cached certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n\nCollecting idna<4,>=2.5\n\nUsing cached idna-3.2-py3-none-any.whl (59 kB)\n\nCollecting urllib3<1.27,>=1.21.1\n\nUsing cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n\nCollecting charset-normalizer~=2.0.0\n\nUsing cached charset_normalizer-2.0.6-py3-none-any.whl (37 kB)\n\nInstalling collected packages: urllib3, idna, charset-normalizer, certifi,\n\nrequests\n\nSuccessfully installed certifi-2021.5.30 charset-normalizer-2.0.6 idna-3.2\n\nrequests-2.26.0 urllib3-1.26.7\n\nZip the contents of the directory:\n\nzip -r requests-layer.zip ./python\n\nYou should see output similar to the following:\n\nadding: python/ (stored 0%)\n\nadding: python/bin/ (stored 0%)\n\nadding: python/bin/normalizer (deflated 28%)\n\nadding: python/requests-2.26.0.dist-info/ (stored 0%)\n\nadding: python/requests-2.26.0.dist-info/RECORD (deflated 55%)\n\nadding: python/requests-2.26.0.dist-info/LICENSE (deflated 65%)\n\nadding: python/requests-2.26.0.dist-info/WHEEL (deflated 14%)\n\nadding: python/requests-2.26.0.dist-info/top_level.txt (stored 0%)\n\nadding: python/requests-2.26.0.dist-info/REQUESTED (stored 0%)\n\nadding: python/requests-2.26.0.dist-info/INSTALLER (stored 0%)\n\nadding: python/requests-2.26.0.dist-info/METADATA (deflated 58%)\n\nNOTE\n\nLambda layers require a specific folder structure. You create this folder structure when you use pip to install the requests module to the Python folder within your directory:\n\n$ tree -L 1 python/\n\npython/\n\n├── bin\n\n├── certifi\n\n7.\n\n8.\n\n├── certifi-2020.12.5.dist-info\n\n├── chardet\n\n├── chardet-4.0.0.dist-info\n\n├── idna\n\n├── idna-2.10.dist-info\n\n├── requests\n\n├── requests-2.25.1.dist-info\n\n├── urllib3\n\n└── urllib3-1.26.4.dist-info\n\n11 directories, 0 files\n\nPublish the layer and set an environment variable to use in the next steps:\n\nLAYER_VERSION_ARN=$(aws lambda publish-layer-version \\\n\n--layer-name AWSCookbook502RequestsLayer \\\n\n--description \"Requests layer\" \\\n\n--license-info \"MIT\" \\\n\n--zip-file fileb://requests-layer.zip \\\n\n--compatible-runtimes python3.8 \\\n\n--output text --query LayerVersionArn)\n\nUpdate the Lambda to use the layer that you created:\n\naws lambda update-function-configuration \\\n\n--function-name AWSCookbook502Lambda \\\n\n--layers $LAYER_VERSION_ARN\n\nYou should see output similar to the following:\n\n{\n\n\"FunctionName\": \"AWSCookbook502Lambda\",\n\n\"FunctionArn\": \"arn:aws:lambda:us-east-\n\n1:111111111111:function:AWSCookbook502Lambda\",\n\n\"Runtime\": \"python3.8\",\n\n\"Role\": \"arn:aws:iam::111111111111:role/AWSCookbookLambdaRole\",\n\n\"Handler\": \"lambda_function.lambda_handler\",\n\n\"CodeSize\": 691,\n\n\"Description\": \"\",\n\n\"Timeout\": 3,\n\n\"MemorySize\": 128,\n\n...\n\nValidation checks\n\nTest the Lambda:\n\naws lambda invoke \\\n\n--function-name AWSCookbook502Lambda \\\n\nresponse.json && cat response.json\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nLambda layers can be used to extend the packages available within the default Lambda runtimes and also to provide your own custom runtimes for your functions. The default runtimes are associated with Amazon Linux. Custom runtimes can be developed on top of Amazon Linux to support your own programming language requirements. See this tutorial to publish a custom runtime.\n\nIn this recipe, you packaged the Python requests module as a layer and deployed a Python function that uses that module. Layers can be used by multiple functions, shared with other AWS accounts, and they can also be version-controlled so that you can deploy and test new versions of your layers without impacting existing versions that are being used for your functions.\n\nChallenge\n\nCreate another Lambda function that uses the same layer.",
      "page_number": 254
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 262-273)",
      "start_page": 262,
      "end_page": 273,
      "detection_method": "topic_boundary",
      "content": "5.3 Invoking Lambda Functions on a Schedule\n\nProblem\n\nYou need to run a serverless function once per minute.\n\nSolution\n\nAdd a permission to your Lambda function to allow the EventBridge service to invoke the function. Then configure an EventBridge rule using a schedule expression for one minute that targets your function (see Figure 5-3).\n\nFigure 5-3. EventBridge triggering a time-based invocation of a Lambda function\n\nPrerequisites\n\nLambda function that you want to trigger\n\nIAM role that allows Lambda functions to execute (provided in chapter prerequisites)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate an events rule with a scheduled expression with a rate of one minute:\n\nRULE_ARN=$(aws events put-rule --name \"EveryMinuteEvent\" \\\n\n--schedule-expression \"rate(1 minute)\")\n\n2.\n\n3.\n\nNOTE\n\nYou can use rate expressions and Cron formats for schedule expressions when defining a time-based\n\nevent rule. The Cron expression syntax for the rule you created would look like the following:\n\nRULE_ARN=$(aws events put-rule --name \"EveryMinuteEvent\" \\\n\n--schedule-expression \"cron(* * * * ? *)\")\n\nFor more information about schedule expressions, see this support document.\n\nAdd a permission to the Lambda function so that the EventBridge service can invoke it:\n\naws lambda add-permission --function-name $LAMBDA_ARN \\\n\n--action lambda:InvokeFunction --statement-id events \\\n\n--principal events.amazonaws.com\n\nYou should see output similar to the following:\n\n{\n\n\"Statement\": \"{\\\"Sid\\\":\\\"events\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":\n\n{\\\"Service\\\":\\\"events.amazonaws.com\\\"},\\\"Action\\\":\\\"lambda:InvokeFunction\\\",\\\"Re\n\nsource\\\":\\\"arn:aws:lambda:us-east-\n\n1:111111111111:function:AWSCookbook503Lambda\\\"}\"\n\n}\n\nAdd your Lambda function as a target for the rule that you created:\n\naws events put-targets --rule EveryMinuteEvent \\\n\n--targets \"Id\"=\"1\",\"Arn\"=\"$LAMBDA_ARN\"\n\nYou should see output similar to the following:\n\n{\n\n\"FailedEntryCount\": 0,\n\n\"FailedEntries\": []\n\n}\n\nNOTE\n\nThere are many available target options. For the latest list, check the documentation.\n\nValidation checks\n\nTail the CloudWatch Logs log group to observe the function invoked every 60 seconds:\n\naws logs tail \"/aws/lambda/AWSCookbook503Lambda\" --follow --since 10s\n\nNOTE\n\nYou may have to wait a few moments for the log group to be created. If the log group doesn’t exist, you will get the\n\nfollowing error:\n\nAn error occurred (ResourceNotFoundException) when calling the FilterLogEvents operation: The\n\nspecified log group does not exist.\n\nYou should see output similar to the following:\n\n$ $ aws logs tail \"/aws/lambda/AWSCookbook503Lambda\" --follow --since 10s\n\n2021-06-12T21:17:30.605000+00:00 2021/06/12/[$LATEST]4d1335bf8b0846938cb585871db38374 START RequestId:\n\n685481eb-9279-4007-854c-f99289bf9609 Version: $LATEST\n\n2021-06-12T21:17:30.607000+00:00 2021/06/12/[$LATEST]4d1335bf8b0846938cb585871db38374 AWS Cookbook\n\nLambda\n\nfunction run at 2021-06-12 21:17:30.607500\n\n2021-06-12T21:17:30.608000+00:00 2021/06/12/[$LATEST]4d1335bf8b0846938cb585871db38374 END RequestId:\n\n685481eb-9279-4007-854c-f99289bf9609\n\n2021-06-12T21:17:30.608000+00:00 2021/06/12/[$LATEST]4d1335bf8b0846938cb585871db38374 REPORT\n\nRequestId:\n\n685481eb-9279-4007-854c-f99289bf9609 Duration: 0.94 ms Billed Duration: 1 ms Memory Size: 128 MB\n\nMax Memory Used: 51 MB\n\n...\n\nYou can exit the tail session by pressing Ctrl-C.\n\nNOTE\n\nNotice that subsequent runs occur at one-minute increments from the time that you added the Lambda function as a target\n\nfor your event rule.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThere are many reasons you would want to run functions on a schedule:\n\nChecking stock prices\n\nChecking the weather\n\nStarting scheduled processes\n\nScheduling EC2 starting and stop\n\nBeing able to run serverless functions on a schedule without provisioning resources allows costs and management to be kept at a minimum. There are no servers to update, and you don’t need to pay for them when they are idle.\n\nChallenge\n\nPause and then enable the event rule. Here is a hint:\n\naws events disable-rule --name \"EveryMinuteEvent\"\n\naws events enable-rule --name \"EveryMinuteEvent\"\n\nNOTE\n\nEventBridge was formerly known as Amazon CloudWatch Events. EventBridge is now the preferred way to schedule\n\nevents and uses the same API as CloudWatch Events.\n\nWhen you need an AWS service to interact with another AWS service, you need to explicitly grant the permissions. In this case, EventBridge needs to be granted the permissions to invoke a Lambda function by using the aws lambda add-permission command.\n\n5.4 Configuring a Lambda Function to Access an EFS File System\n\nProblem\n\nYou have an existing network share that is accessible by servers, but you want to be able to process files on it with serverless functions.\n\nSolution\n\nYou will create a Lambda function and mount your EFS file system to it (see Figure 5-4).\n\nFigure 5-4. Lambda function accessing ENIs within the subnet of a VPC\n\nPrerequisites\n\nVPC with isolated subnets in two AZs and associated route tables\n\nEFS file system with content that you want to access\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a new security group for the Lambda function to use:\n\nLAMBDA_SG_ID=$(aws ec2 create-security-group \\\n\n--group-name AWSCookbook504LambdaSG \\\n\n--description \"Lambda Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\n2.\n\nAdd an ingress rule to the EFS file system’s security group that allows access on TCP port 2049 from the Lambda function’s security group:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 2049 \\\n\n--source-group $LAMBDA_SG_ID \\\n\n--group-id $EFS_SECURITY_GROUP\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-0f837d0b090ba38de\",\n\n\"GroupId\": \"sg-0867c2c4ca6f4ab83\",\n\n\"GroupOwnerId\": \"611652777867\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 2049,\n\n\"ToPort\": 2049,\n\n\"ReferencedGroupInfo\": {\n\n3.\n\n4.\n\n5.\n\n6.\n\n\"GroupId\": \"sg-0c71fc94eb6cd1ae3\"\n\n}\n\n}\n\n]\n\n}\n\nCreate an IAM role using the statement in the provided assume-role- policy.json file using this command:\n\naws iam create-role --role-name AWSCookbook504Role \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\nYou need to give your Lambda function the ability to execute within a VPC, so attach the IAM managed policy for AWSLambdaVPCAccessExecutionRole to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook504Role \\\n\n--policy-arn arn:aws:iam::aws:policy/service-\n\nrole/AWSLambdaVPCAccessExecutionRole\n\nZip up the lambda_function.py provided in the repository:\n\nzip lambda_function.zip lambda_function.py\n\nCreate a Lambda function specifying the ACCESS_POINT_ARN of the EFS file system:\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook504Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook504Role \\\n\n--file-system-configs Arn=\"$ACCESS_POINT_ARN\",LocalMountPath=\"/mnt/efs\" \\\n\n--output text --query FunctionArn \\\n\n--vpc-config SubnetIds=${ISOLATED_SUBNETS},SecurityGroupIds=${LAMBDA_SG_ID})\n\n7.\n\nUse this command to determine when the Lambda function has entered the active state (this may take a few moments):\n\naws lambda get-function --function-name $LAMBDA_ARN \\\n\n--output text --query Configuration.State\n\nValidation checks\n\nExecute the Lambda function to display the file’s contents:\n\naws lambda invoke \\\n\n--function-name $LAMBDA_ARN \\\n\nresponse.json && cat response.json\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAs a developer or software architect, you start to see the benefits of serverless technologies when you connect serverless compute to serverless persistent storage (filesystems and databases both). The compute and storage operational overhead is drastically reduced when using these types of services within applications. You can provision and scale your storage on demand while relying on AWS to manage the underlying infrastructure for you. While many applications will use an object-style storage service such as Amazon S3, others are best suited to a somewhat more traditional file-style storage service. Combining Lambda and EFS, as shown in this recipe, solves this problem with ease.\n\nBy integrating Amazon EFS with AWS Lambda, you can build solutions like the following:\n\nPersistent storage for applications\n\nMaintenance activities\n\nEvent-driven notifications\n\nEvent-driven file processing\n\nThe fully managed nature and the pay-per-use aspects of these services allow for the design, building, deployment, and operation of cost-effective and modern application architectures.\n\nChallenge 1\n\nCreate another Lambda function that has access to the same EFS file system.\n\nChallenge 2\n\nCreate a Lambda function that runs on a scheduled interval to detect if any files have been changed in the last 30 days.\n\nSee Also\n\nRecipe 5.9\n\n5.5 Running Trusted Code in Lambda Using AWS Signer\n\nProblem\n\nYou need to ensure that a serverless function deployed in your environment is running code from trusted sources. You need to verify the integrity of the code and have confidence that the code has not been modified after it has been signed.\n\nSolution\n\nCreate a signing profile and then start a signing job for your code by using AWS Signer. Finally, deploy a Lambda function that references your signing configuration and uses the signed code (see Figure 5-5).\n\nFigure 5-5. Signing process for Lambda function code\n\nPrerequisites\n\nS3 bucket with versioning enabled and source code copied to it\n\nS3 bucket for AWS Signer to use a destination\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nGet the version of the object in S3 that you will use. This is a zip of the code to be used in your Lambda function. You’ll need this when you start the signing job:\n\nOBJ_VER_ID=$(aws s3api list-object-versions \\\n\n--bucket awscookbook505-src-$RANDOM_STRING \\\n\n--prefix lambda_function.zip \\\n\n--output text --query Versions[0].VersionId)\n\n2.\n\nCreate a signing profile:\n\nSIGNING_PROFILE_ARN=$(aws signer put-signing-profile \\\n\n--profile-name AWSCookbook505_$RANDOM_STRING \\\n\n--platform AWSLambda-SHA384-ECDSA \\\n\n--output text --query arn)\n\nNOTE\n\nYou can find a list of the available signing platforms by running this command:\n\naws signer list-signing-platforms\n\n3.\n\nCreate a code-signing configuration for Lambda that refers to the signing profile:\n\nCODE_SIGNING_CONFIG_ARN=$(aws lambda create-code-signing-config \\\n\n4.\n\n--allowed-publishers SigningProfileVersionArns=$SIGNING_PROFILE_ARN \\\n\n--output text --query CodeSigningConfig.CodeSigningConfigArn)\n\nStart the signing job:\n\nSIGNING_JOB_ID=$(aws signer start-signing-job \\\n\n--source 's3={bucketName=awscookbook505-src-\n\n'\"${RANDOM_STRING}\"',key=lambda_function.zip,version='\"$OBJ_VER_ID\"'}' \\\n\n--destination 's3={bucketName=awscookbook505-dst-\n\n'\"${RANDOM_STRING}\"',prefix=signed-}' \\\n\n--profile-name AWSCookbook505_$RANDOM_STRING \\\n\n--output text --query jobId)\n\nWait a few moments and then verify that the signing job was successful:\n\naws signer list-signing-jobs --status Succeeded\n\nYou should see output similar to the following:\n\n{\n\n\"jobs\": [\n\n{\n\n\"jobId\": \"efd392ae-2503-4c78-963f-8f40a58d770f\",\n\n\"source\": {\n\n\"s3\": {\n\n\"bucketName\": \"awscookbook505-src-<<unique>>\",\n\n\"key\": \"lambda_function.zip\",\n\n\"version\": \"o.MffnpzjBmaBR1yzvoti0AnluovMtMf\"\n\n}\n\n},\n\n\"signedObject\": {\n\n\"s3\": {\n\n\"bucketName\": \"awscookbook505-dst-<<unique>>\",\n\n\"key\": \"signed-efd392ae-2503-4c78-963f-8f40a58d770f.zip\"\n\n}\n\n},\n\n\"signingMaterial\": {},\n\n\"createdAt\": \"2021-06-13T11:52:51-04:00\",\n\n\"status\": \"Succeeded\",\n\n...\n\n5.\n\nRetrieve the S3 object key of the resulting signed code:\n\nOBJECT_KEY=$(aws s3api list-objects-v2 \\\n\n--bucket awscookbook505-dst-$RANDOM_STRING \\\n\n--prefix 'signed-' \\\n\n--output text --query Contents[0].Key)\n\n6.\n\nCreate a Lambda function that uses the signed code:\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook505Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--code S3Bucket=awscookbook505-dst-$RANDOM_STRING,S3Key=$OBJECT_KEY \\\n\n--code-signing-config-arn $CODE_SIGNING_CONFIG_ARN \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole \\\n\n--output text --query FunctionArn)\n\n7.\n\nUse this command to determine when the Lambda function has entered the active state:\n\naws lambda get-function --function-name $LAMBDA_ARN \\\n\n--output text --query Configuration.State\n\nValidation checks\n\nView your Lambda function in that console. Notice that you can’t edit the code. You will see the message, “Your function has signed code and can’t be edited inline.”\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nSecurity-focused administrators and application developers can use this approach to implement a DevSecOps strategy by enforcing rules that allow only trusted code to be deployed in a given environment. By using AWS Signer, you can ensure that you are running only trusted code in",
      "page_number": 262
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 274-281)",
      "start_page": 274,
      "end_page": 281,
      "detection_method": "topic_boundary",
      "content": "your environments. This helps meet compliance requirements and increases the security posture of your application.\n\nBy using a digital signature generated by AWS Signer, your code is validated against a cryptographic fingerprint, and enforcement policies can be applied to restrict the deployment and execution of code. This capability paves the way to a strategic shift from “reactive” to “preventive” controls in your security and compliance governance.\n\nChallenge 1\n\nMake a modification to the source code, sign it, and update the Lambda function.\n\nChallenge 2\n\nYou can change your CodeSigningPolicies from Warn to Enforce—this will block deployments if validation checks of the signature aren’t successful. Deploy a function that leverages this capability to ensure you are running only signed code in your environment:\n\n\"CodeSigningPolicies\": {\n\n\"UntrustedArtifactOnDeployment\": \"Warn\"\n\n},\n\n5.6 Packaging Lambda Code in a Container Image\n\nProblem\n\nYou want to use your existing container-based development processes and tooling to package your serverless code.\n\nSolution\n\nCreate a Docker image and push it to an Amazon Elastic Container Registry (ECR) repository. Create a Lambda function with the package-type of Image and code that references an image URL in ECR (see Figure 5-6).\n\nFigure 5-6. Deploying Lambda code packaged in a Docker image\n\nPrerequisites\n\nECR repository\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\nDocker installed\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nGet ECR login information and pass to Docker:\n\naws ecr get-login-password | docker login --username AWS \\\n\n2.\n\n3.\n\n4.\n\n--password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n\nYou should see output similar to the following:\n\nLogin Succeeded\n\nCreate a file called app.py with the following code that you would like to execute in Lambda (file provided in the repository):\n\nimport sys\n\ndef handler(event, context):\n\nreturn 'Hello from the AWS Cookbook ' + sys.version + '!'\n\nCreate a file called Dockerfile with the following content that builds on an AWS-provided base image and references your Python code (file provided in the repository):\n\nFROM public.ecr.aws/lambda/python:3.8\n\nCOPY app.py ./\n\nCMD [\"app.handler\"]\n\nIn the folder where the Dockerfile and app.py files exist, build the container image. The process will take a few minutes to complete:\n\ndocker build -t aws-cookbook506-image .\n\nYou should see output similar to the following:\n\n[+] Building 19.1s (4/6)\n\n=> [internal] load build definition from Dockerfile\n\n0.0s\n\n=> => transferring dockerfile: 36B\n\n0.0s\n\n=> [internal] load .dockerignore\n\n5.\n\n6.\n\n7.\n\n0.0s\n\n=> => transferring context: 2B\n\n0.0s\n\n=> [internal] load metadata for public.ecr.aws/lambda/python:3.8\n\n2.2s\n\n=> [internal] load build context\n\n0.0s\n\n...\n\nAdd an additional tag to the image to allow it to be pushed to ECR:\n\ndocker tag \\\n\naws-cookbook506-image:latest \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-506repo:latest\n\nPush the image to the ECR repository. The process should take a few minutes to complete:\n\ndocker push \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-506repo:latest\n\nYou should see output similar to the following:\n\nThe push refers to repository [111111111111.dkr.ecr.us-east-1.amazonaws.com/aws-\n\ncookbook-506repo]\n\n5efc5a3f50dd: Pushed\n\na1f8e0568112: Pushing [=====> ] 10.3MB/98.4MB\n\nbcf453d1de13: Pushing [> ] 3.244MB/201.2MB\n\nf6ae2f36d5d7: Pushing [==============================> ] 4.998MB/8.204MB\n\n5959c8f9752b: Pushed\n\n3e5452c20c48: Pushed\n\n9c4b6b04eac3: Pushing [>\n\nCreate a Lambda function with the Docker image by specifying a --code value that is the ImageUri of the Docker image :\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook506Lambda \\\n\n--package-type \"Image\" \\\n\n--code ImageUri=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-\n\n506repo:latest \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole \\\n\n--output text --query FunctionArn)\n\nNOTE\n\nThe --runtime and --handler parameters are not necessary or supported when creating a function that uses a container image.\n\nUse this command to determine when the Lambda function has entered the active state:\n\naws lambda get-function --function-name $LAMBDA_ARN \\\n\n--output text --query Configuration.State\n\nValidation checks\n\nIn the AWS Console, navigate to the Lambda→Functions menu. Notice that the “Package type” for your function is Image.\n\nInvoke the function and view the response:\n\naws lambda invoke \\\n\n--function-name $LAMBDA_ARN response.json && cat response.json\n\nYou should see output similar to the following:\n\n{\n\n\"StatusCode\": 200,\n\n\"ExecutedVersion\": \"$LATEST\"\n\n}\n\n\"Hello from the AWS Cookbook 3.8.8 (default, Mar 8 2021, 20:13:42) \\n[GCC 7.3.1 20180712 (Red Hat\n\n7.3.1-12)]!\"\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nIf your application code is packaged in container images, AWS Lambda provides this ability to package your function code inside container images. This allows for alignment with existing build, test, package, and deploy pipelines that you may already be using. You can package application code up to 10 GB in size for your functions. You can use the base images that AWS provides or create your own images, as long as you include a runtime interface client that is required by the Lambda runtime environment.\n\nYou can store your container images on Amazon ECR, and your function must be in the same account as the ECR repository where you have stored your container image.\n\nChallenge\n\nUpdate the application code, create a new image, push it to ECR, and update the Lambda function.\n\n5.7 Automating CSV Import into DynamoDB from S3 with Lambda\n\nProblem\n\nYou need to load data from S3 into DynamoDB when files are uploaded to S3.\n\nSolution\n\nUse a Lambda function to load the S3 data into DynamoDB and configure an S3 notification specifying the Lambda function to trigger on S3:PutObject events (see Figure 5-7).\n\nFigure 5-7. Using a Lambda function to load data into a DynamoDB table\n\nSteps\n\n1.\n\n2.\n\n3.\n\n4.\n\n5.\n\nNavigate to this recipe’s directory in the chapter repository:\n\ncd 507-Importing-CSV-to-DynamoDB-from-S3\n\nCreate a DynamoDB table:\n\naws dynamodb create-table \\\n\n--table-name 'AWSCookbook507' \\\n\n--attribute-definitions 'AttributeName=UserID,AttributeType=S' \\\n\n--key-schema 'AttributeName=UserID,KeyType=HASH' \\\n\n--sse-specification 'Enabled=true,SSEType=KMS' \\\n\n--provisioned-throughput \\\n\n'ReadCapacityUnits=5,WriteCapacityUnits=5'\n\nSet a unique suffix to use for the S3 bucket name:\n\nRANDOM_STRING=$(aws secretsmanager get-random-password \\\n\n--exclude-punctuation --exclude-uppercase \\\n\n--password-length 6 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nCreate an S3 bucket:\n\naws s3api create-bucket --bucket awscookbook507-$RANDOM_STRING\n\nCreate a role for a Lambda function allowing S3 and DynamoDB usage (file provided in the repository):\n\naws iam create-role --role-name AWSCookbook507Lambda \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\n6.\n\n7.\n\n8.\n\n9.\n\n10.\n\nAttach the IAM managed policy for AmazonS3ReadOnlyAccess to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook507Lambda \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\n\nAttach the IAM managed policy for AmazonDynamoDBFullAccess to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook507Lambda \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\n\nNOTE\n\nIt is best to scope the Lambda function permission to the specific DynamoDB table resource rather\n\nthan AmazonDynamoDBFullAccess (we used it for simplicity here). See Recipe 1.2 for details on how to create a more narrowly scoped permission.\n\nAttach the AWSLambdaBasicExecutionRole IAM managed policy to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook507Lambda \\\n\n--policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n\nZip the function code:\n\nzip lambda_function.zip lambda_function.py\n\nCreate a Lambda function by using the provided code and specifying the code:\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook507Lambda \\\n\n--runtime python3.8 \\",
      "page_number": 274
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 282-292)",
      "start_page": 282,
      "end_page": 292,
      "detection_method": "topic_boundary",
      "content": "11.\n\n12.\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--environment Variables={bucket=awscookbook507-$RANDOM_STRING} \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook507Lambda \\\n\n--output text --query FunctionArn)\n\nGrant the S3 service invoke permissions for the Lambda function:\n\naws lambda add-permission --function-name $LAMBDA_ARN \\\n\n--action lambda:InvokeFunction --statement-id s3invoke \\\n\n--principal s3.amazonaws.com\n\nCreate a notification-template.json file to use as the event definition for automatically triggering the Lambda function when your file (sample_data.csv) is uploaded. A file you can use is provided in the repository:\n\n{\n\n\"LambdaFunctionConfigurations\": [\n\n{\n\n\"Id\": \"awscookbook507event\",\n\n\"LambdaFunctionArn\": \"LAMBDA_ARN\",\n\n\"Events\": [\n\n\"s3:ObjectCreated:*\"\n\n],\n\n\"Filter\": {\n\n\"Key\": {\n\n\"FilterRules\": [\n\n{\n\n\"Name\": \"prefix\",\n\n\"Value\": \"sample_data.csv\"\n\n}\n\n]\n\n}\n\n}\n\n}\n\n]\n\n}\n\n13.\n\nYou can use the sed command to replace the values in the provided notification-template.json file with the environment variables you have created:\n\nsed -e \"s/LAMBDA_ARN/${LAMBDA_ARN}/g\" \\\n\nnotification-template.json > notification.json\n\n14.\n\nConfigure the S3 bucket notification settings to trigger the Lambda function (one-liner config). NotificationConfiguration→LambdaConfigurations→Lambda ARN:\n\naws s3api put-bucket-notification-configuration \\\n\n--bucket awscookbook507-$RANDOM_STRING \\\n\n--notification-configuration file://notification.json\n\n15.\n\nUpload a file to S3 to trigger the import:\n\naws s3 cp ./sample_data.csv s3://awscookbook507-$RANDOM_STRING\n\nValidation checks\n\nView the results from your DynamoDB console, or use this CLI command to scan the table:\n\naws dynamodb scan --table-name AWSCookbook507\n\nNOTE\n\nOne of the great features of DynamoDB is that it provides AWS API endpoints for easy CRUD operations by your\n\napplication via the AWS SDK.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou can use AWS Lambda and Amazon DynamoDB together to build applications with massively scalable database persistence while minimizing the operational overhead required. Software architects and developers who are looking to build applications without worrying about server infrastructure may find it useful to use these two services together.\n\nWARNING\n\nAt the time of this writing, Lambda functions time out after 900 seconds. This could cause an issue with large CSV files or\n\nif the DynamoDB table does not have sufficient write capacity.\n\nEvent-driven applications are also an important concept in building modern cloud-native applications on AWS. When you created the notification.json file, you specified your Lambda function and the S3 bucket, as well as a key pattern to watch for uploads to trigger the Lambda function when an object is put into the bucket. Using event-driven architecture helps minimize the cost and complexity associated with running your applications because the function logic is run only when needed.\n\nChallenge 1\n\nAdd some new data to the sample_data.csv file, delete the file from your bucket, and re-upload the file to trigger the new import. Note that the existing data will remain, and the new data will be added.\n\nChallenge 2\n\nChange the S3 notification and the Lambda function to allow for other filenames to be used with the solution.\n\nChallenge 3\n\nCreate a fine-grained IAM policy for your Lambda function that scopes down the function’s granted access to the DynamoDB table.\n\n5.8 Reducing Lambda Startup Times with Provisioned Concurrency\n\nProblem\n\nYou need to ensure that a predetermined number (five) of invocations to your serverless function are as fast as possible. You need to eliminate any latency associated with cold starts.\n\nSolution\n\nCreate a Lambda function and set the function concurrency to 5 (see Figure 5-8).\n\nPrerequisite\n\nSteps\n\nFigure 5-8. Provisioned concurrency for a Lambda function\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\n1.\n\n2.\n\n3.\n\n4.\n\nCreate a file called lambda_function.py with the following content (file provided in the repository):\n\nfrom datetime import datetime\n\nimport time\n\ndef lambda_handler(event, context):\n\ntime.sleep(5)\n\nprint('AWS Cookbook Function run at {}'.format(str(datetime.now())))\n\nZip the function code:\n\nzip lambda_function.zip lambda_function.py\n\nCreate a Lambda function:\n\naws lambda create-function \\\n\n--function-name AWSCookbook508Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--timeout 20 \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole\n\nUse this command to determine when the Lambda function has entered the active state:\n\naws lambda get-function --function-name AWSCookbook509Lambda \\\n\n--output text --query Configuration.State\n\nNOTE\n\nLambda function aliases allow you to reference a specific version of a function. Each Lambda\n\nfunction can have one or more aliases. The initial alias is named LATEST.\n\n5.\n\nConfigure the provisioned concurrency for the Lambda function:\n\naws lambda put-provisioned-concurrency-config \\\n\n--function-name AWSCookbook508Lambda \\\n\n--qualifier LATEST \\\n\n--provisioned-concurrent-executions 5\n\nValidation checks\n\nInvoke the function six times in a row to see the limit hit:\n\naws lambda invoke --function-name AWSCookbook508Lambda response.json &\n\naws lambda invoke --function-name AWSCookbook508Lambda response.json\n\nNOTE\n\nThe Lambda function you deployed needs to be run in parallel to demonstrate the capability of the provisioned\n\nconcurrency feature. You may want to write a simple script which runs this command multiple times in parallel to prove\n\nthat the provisioned concurrency enabled.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYour code’s execution environment provisioning is handled for you by the Lambda service. This is a benefit of it being a fully managed service on AWS. Since the execution environment is provisioned on demand, a small amount of time is required to provision this environment for you. This is referred to as a cold start. Lambda keeps your execution environment provisioned (or “warm”) for a period of time so that if your function is invoked again, it will launch quickly. When you need your functions to respond quickly and achieve more concurrency, you can avoid the cold start and use provisioned concurrency to keep multiple copies of the execution environments “warm.”\n\nSome developers and software architects need to build solutions with time-sensitive requirements measured in milliseconds in microservice-based applications. When you use Lambda-provisioned concurrency, you minimize the amount of time your function needs to start up when it is invoked.\n\nChallenge 1\n\nConfigure an API Gateway in front of your Lambda function and use a tool like bees with machine guns or ApacheBench to simulate user load.\n\nChallenge 2\n\nConfigure application autoscaling to modify the provisioned concurrency for your Lambda function based on time and/or a performance metric (e.g., response time).\n\n5.9 Accessing VPC Resources with Lambda\n\nProblem\n\nYou need a serverless function to be able to access an ElastiCache cluster that has an endpoint in a VPC.\n\nSolution\n\nCreate a Lambda function that has a Redis client connection and package and specifies VPC subnets and security groups. Then create ElastiCache subnet groups and an ElastiCache cluster. Invoke the Lambda and pass the cluster endpoint to the function to test (see Figure 5-9).\n\nFigure 5-9. Lambda accessing an ElastiCache cluster in a VPC\n\nPrerequisite\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nYou need to give your Lambda function the ability to execute within a VPC, so attach the IAM managed policy for AWSLambdaVPCAccess to the IAM role (created in the chapter prerequisites):\n\naws iam attach-role-policy --role-name AWSCookbookLambdaRole \\\n\n--policy-arn arn:aws:iam::aws:policy/service-\n\nrole/AWSLambdaVPCAccessExecutionRole\n\n2.\n\nInstall the Redis Python package to the current directory from the Python Package Index (PyPI):\n\npip install redis -t .\n\n3.\n\nZip the function code:\n\nzip -r lambda_function.zip lambda_function.py redis*\n\n4.\n\nCreate a security group for the Lambda:\n\nLAMBDA_SG_ID=$(aws ec2 create-security-group \\\n\n--group-name Cookbook509LambdaSG \\\n\n--description \"Lambda Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\n5.\n\nCreate a Lambda function that will respond to HTTP requests:\n\n6.\n\n7.\n\n8.\n\nCleanup\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook509Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--role arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole \\\n\n--output text --query FunctionArn \\\n\n--vpc-config\n\nSubnetIds=${TRIMMED_ISOLATED_SUBNETS},SecurityGroupIds=${LAMBDA_SG_ID})\n\nCreate an ElastiCache subnet group:\n\naws elasticache create-cache-subnet-group \\\n\n--cache-subnet-group-name \"AWSCookbook509CacheSG\" \\\n\n--cache-subnet-group-description \"AWSCookbook509CacheSG\" \\\n\n--subnet-ids $ISOLATED_SUBNETS\n\nCreate an ElastiCache Redis cluster with one node:\n\naws elasticache create-cache-cluster \\\n\n--cache-cluster-id \"AWSCookbook509CacheCluster\" \\\n\n--cache-subnet-group-name AWSCookbook509CacheSG \\\n\n--engine redis \\\n\n--cache-node-type cache.t3.micro \\\n\n--num-cache-nodes 1\n\nWait for the cache cluster to be available.\n\nInvoke the function and view the response replacing HOSTNAME with the host name of your cluster:\n\naws lambda invoke \\\n\n--cli-binary-format raw-in-base64-out \\\n\n--function-name $LAMBDA_ARN \\\n\n--payload '{ \"hostname\": \"HOSTNAME\" }' \\\n\nresponse.json && cat response.json\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nBy default, Lambda functions do not have access to any VPC that you may have provisioned in your AWS environment. However, Lambda does support VPC connectivity by provisioning network interfaces in your VPC. ElastiCache requires compute nodes that have network interfaces in your VPC, so you need to configure Lambda within your VPC to allow it to access the ElastiCache nodes that you provision.\n\nThe compute memory that your function uses is not persistent if the function is invoked when the execution environment is spun down and restarted. If your application requires access to memory persistence (for example, in HTTP sessions), you can use the Amazon ElastiCache service to implement redis or memcached for session storage and key/value storage. These common solutions implement in-memory cache for fast read/write and allow you to scale horizontally with your application while maintaining memory persistence that your application requires.\n\nChallenge 1\n\nConfigure your Lambda function to read and write additional values to Amazon ElastiCache for Redis.",
      "page_number": 282
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 293-301)",
      "start_page": 293,
      "end_page": 301,
      "detection_method": "topic_boundary",
      "content": "Chapter 6. Containers\n\n6.0 Introduction A container, put simply, packages application code, binaries, configuration files, and libraries together into a single package, called a container image. By packaging everything together in this way, you can develop, test, and run applications with control and consistency. You can quickly start packaging up and testing containers that you build locally, while ensuring that the exact same runtime environment is present regardless of where it is running. This generally reduces the time it takes to build something and offer it to a wide audience, and ensures consistency whenever you deploy.\n\nContainers are wholly “contained” environments that leverage the underlying compute and memory capabilities on the host where they are running (your laptop, a server in a closet, or the cloud). Multiple containers can be run on the same host at once without conflicts. You can also have multiple containers running with the intention of them communicating with one another. Imagine that you have a frontend web application running as a container that accesses a container running a backend for your website, and you might want to run multiple instances of them at once to handle more traffic. Running multiple containers at once and ensuring they are always available can present some challenges, which is why you enlist the help of a container orchestrator. Popular orchestrators come in many flavors, but some of the common ones you may have heard of are Kubernetes and Docker Swarm.\n\nYou have options for running containers on AWS, such as Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Kubernetes Service (Amazon EKS) as container orchestrators, and Amazon Elastic Cloud Compute (Amazon EC2) for deployments with custom requirements. Both of the AWS container orchestrator services mentioned (Amazon ECS and Amazon EKS) can run workloads on Amazon EC2 or on the fully managed AWS Fargate compute engine. In other words, you can choose to control the underlying EC2 instance (or instances) responsible for running your containers on Amazon ECS and Amazon EKS, allowing some level of customization to your host, or you can use Fargate, which is fully managed by AWS—you don’t have to worry about instance management. You can even use ECS and EKS within your own datacenter using ECS Anywhere and EKS Anywhere. AWS provides a comprehensive listing of all up-to-date container services on its website.\n\nSome AWS services (AWS CodeDeploy, AWS CodePipeline, and Amazon Elastic Container Registry) can help streamline the development lifecycle and provide automation to your workflow. These integrate well with Amazon ECS and Amazon EKS. Some examples of AWS services that provide network capabilities are Amazon Virtual Private Cloud, AWS Elastic Load Balancing, AWS Cloud Map, and Amazon Route 53.You can address your logging and monitoring concerns with Amazon CloudWatch and the Amazon Managed Service for Prometheus. Fine-grained security capabilities can be provided by AWS Identity and Access Management (IAM) and AWS Key Management System (KMS). By following the recipes in this chapter, you will see how some of these services combine to meet your needs.\n\nWorkstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Containers\n\nChapter Prerequisites\n\nDocker installation and validation\n\nDocker Desktop is recommended for Windows and Mac users; Docker Linux Engine is recommended for Linux users. In the following recipes, you’ll use Docker to create a consistent working environment on your particular platform. Be sure to install the latest stable version of Docker for your OS.\n\nMacOS\n\n1.\n\nFollow instructions from Docker Desktop: https://docs.docker.com/docker- for-mac/install.\n\n2.\n\nRun the Docker Desktop application after installation.\n\nWindows\n\n1.\n\nFollow instructions from Docker Desktop: https://docs.docker.com/desktop/windows/install.\n\n2.\n\nRun the Docker Desktop application after installation.\n\nLinux\n\n1.\n\nFollow instructions from Docker Desktop: https://docs.docker.com/engine/install.\n\n2.\n\nStart the Docker daemon on your distribution:\n\nValidation the installation of Docker on your workstation with the following command:\n\ndocker --version\n\nYou should see output similar to the following:\n\nDocker version 19.03.13, build 4484c46d9d\n\nRun the docker images command to list images on your local machine:\n\nREPOSITORY TAG IMAGE ID CREATED SIZE\n\n6.1 Building, Tagging, and Pushing a Container Image to Amazon ECR\n\nProblem\n\nYou need a repository to store built and tagged container images.\n\nSolution\n\nFirst, you will create a repository in Amazon ECR. Next, you will create a Dockerfile and use it to build a Docker image. Finally, you will apply two tags to the container image and push them both to the newly created ECR repository. This process is illustrated in Figure 6-1.\n\nFigure 6-1. Solution workflow of build, tag, and push for container images\n\nSteps\n\n1.\n\nLog into the AWS Management Console and search for Elastic Container Registry. Click the “Create repository” button.\n\nGive your repository a name, keep all defaults (as shown in Figure 6-2), scroll to the bottom, and click “Create repository” again to finish.\n\nFigure 6-2. ECR repository creation\n\nYou now have a repository created on Amazon ECR that you can use to store container images. An example of the ECR console is shown in Figure 6-3.\n\nFigure 6-3. Screenshot of created ECR repository\n\nAlternatively, you can also create an ECR repository from the command line:\n\nREPO=aws-cookbook-repo && \\\n\naws ecr create-repository --repository-name $REPO\n\nYou should see output similar to the following:\n\n{\n\n2.\n\n3.\n\n4.\n\n\"repository\": {\n\n\"repositoryArn\": \"arn:aws:ecr:us-east-1:111111111111:repository/aws-cookbook-\n\nrepo\",\n\n\"registryId\": \"1111111111111\",\n\n\"repositoryName\": \"aws-cookbook-repo\",\n\n\"repositoryUri\": \"611652777867.dkr.ecr.us-east-1.amazonaws.com/aws-cookbook-\n\nrepo\",\n\n\"createdAt\": \"2021-10-02T19:57:56-04:00\",\n\n\"imageTagMutability\": \"MUTABLE\",\n\n\"imageScanningConfiguration\": {\n\n\"scanOnPush\": false\n\n},\n\n\"encryptionConfiguration\": {\n\n\"encryptionType\": \"AES256\"\n\n}\n\n}\n\n}\n\nCreate a simple Dockerfile:\n\necho FROM nginx:latest > Dockerfile\n\nNOTE\n\nThis command creates a Dockerfile that contains a single line instructing the Docker Engine to use the\n\nnginx:latest image as the base image. Since you use only the base image with no other lines in the Dockerfile, the resulting image is identical to the nginx:latest image. You could include some HTML files within this image by using the COPY and ADD Dockerfile directives.\n\nBuild and tag the image. This step may take a few moments as it downloads and combines the image layers:\n\ndocker build . -t \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:latest\n\nAdd an additional tag:\n\ndocker tag \\\n\n5.\n\n6.\n\nValidation checks\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:latest \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:1.0\n\nGet Docker login information:\n\naws ecr get-login-password | docker login --username AWS \\\n\n--password-stdin $AWS_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com\n\nHere is the output:\n\nLogin Succeeded\n\nNOTE\n\nAn authorization token needs to be provided each time an operation is executed against a private\n\nrepository. Tokens last for 12 hours, so the command you ran would need to be manually refreshed at\n\nthat interval on your command line. To help you with automating the task of obtaining authorization\n\ntokens, you can use the ECR Docker Credential Helper, available from the awslabs GitHub repository.\n\nPush each image tag to Amazon ECR:\n\ndocker push \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:latest\n\ndocker push \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:1.0\n\nNOTE\n\nYou will see “Layer already exists” for the image layer uploads on the second push. This is because\n\nthe image already exists in the ECR repository due to the first push, but this step is still required to add\n\nthe additional tag.\n\nNow you can view both of the tagged images in Amazon ECR from the console, as shown in Figure 6-4.\n\nFigure 6-4. Screenshot of the image with two tags\n\nAlternatively, you can use the AWS CLI to list the images:\n\naws ecr list-images --repository-name aws-cookbook-repo\n\nYou should see output similar to the following:\n\n{\n\n\"imageIds\": [\n\n{\n\n\"imageDigest\": \"sha256:99d0a53e3718cef59443558607d1e100b325d6a2b678cd2a48b05e5e22ffeb49\",",
      "page_number": 293
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 302-310)",
      "start_page": 302,
      "end_page": 310,
      "detection_method": "topic_boundary",
      "content": "\"imageTag\": \"1.0\"\n\n},\n\n{\n\n\"imageDigest\": \"sha256:99d0a53e3718cef59443558607d1e100b325d6a2b678cd2a48b05e5e22ffeb49\",\n\n\"imageTag\": \"latest\"\n\n}\n\n]\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nHaving a repository for your container images is an important foundational component of the application development process. A private repository for your container images that you control is a best practice to increase the security of your application development process. You can grant access to other AWS accounts, IAM entities, and AWS services with permissions for Amazon ECR. Now that you know how to create an ECR repository, you will be able to store your container images and use them with AWS services.\n\nNOTE\n\nAmazon ECR supports the popular Docker Image Manifest V2, Schema 2 and most recently Open Container Initiative\n\n(OCI) images. It can translate between these formats on pull. Legacy support is available for Manifest V2, Schema 1, and\n\nAmazon ECR can translate on the fly when interacting with legacy Docker client versions. The experience should be\n\nseamless for most Docker client versions in use today.\n\nContainer tagging allows you to version and keep track of your container images. You can apply multiple tags to an image, which can help you implement your versioning strategy and deployment process. For example, you may always refer to a “latest” tagged image in your dev environment, but your production environment can be locked to a specific version tag. The Docker CLI pushes tagged images to the repository and the tags can be used with pulls.\n\nChallenge\n\nModify the Dockerfile, build a new image, tag it with a new version number, and put it to ECR.\n\n6.2 Scanning Images for Security Vulnerabilities on Push to Amazon ECR\n\nProblem\n\nYou want to automatically scan your container images for security vulnerabilities each time you push to a repository.\n\nSolution\n\nEnable automatic image scanning on a repository in Amazon ECR, push an image, and observe the scan results, as shown in Figure 6-5.\n\nPrerequisite\n\nECR repository\n\nFigure 6-5. Container image scanning solution workflow\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nRather than building a new container image from a Dockerfile (as you did in Recipe 6.1), this time you are going to pull an old NGINX container image:\n\ndocker pull nginx:1.14.1\n\n2.\n\nOn the command line, apply the scanning configuration to the repository you created:\n\nREPO=aws-cookbook-repo && \\\n\naws ecr put-image-scanning-configuration \\\n\n--repository-name $REPO \\\n\n--image-scanning-configuration scanOnPush=true\n\n3.\n\nGet Docker login information:\n\naws ecr get-login-password | docker login --username AWS \\\n\n--password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n\n4.\n\nApply a tag to the image so that you can push it to the ECR repository:\n\ndocker tag nginx:1.14.1 \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:old\n\n5.\n\nPush the image:\n\ndocker push \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:old\n\nValidation checks\n\nShortly after the push is complete, you can examine the results of the security scan of the image in JSON format:\n\naws ecr describe-image-scan-findings \\\n\n--repository-name aws-cookbook-repo --image-id imageTag=old\n\nYou should see output similar to the following:\n\n{\n\n\"imageScanFindings\": {\n\n\"findings\": [\n\n{\n\n\"name\": \"CVE-2019-3462\",\n\n\"description\": \"Incorrect sanitation of the 302 redirect field in HTTP transport method of apt\n\nversions 1.4.8 and earlier can lead to content injection by a MITM attacker, potentially leading to\n\nremote code execution on the target machine.\",\n\n\"uri\": \"https://security-tracker.debian.org/tracker/CVE-2019-3462\",\n\n\"severity\": \"CRITICAL\",\n\n\"attributes\": [\n\n{\n\n\"key\": \"package_version\",\n\n\"value\": \"1.4.8\"\n\n},\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nTIP\n\nAmazon ECR has a safety mechanism built in that does not let you delete a repository containing images. If the repository\n\nis not empty and the delete-repository command is failing, you can bypass this check by adding --force to the delete-repository command.\n\nDiscussion\n\nThe Common Vulnerabilities and Exposures (CVEs) database from the open source Clair project is used by Amazon ECR for vulnerability scanning. You are provided a Common Vulnerability Scoring System (CVSS) score to indicate the severity of any detected vulnerabilities. This helps you detect and remediate vulnerabilities in your container image. You can configure alerts for newly discovered vulnerabilities in images by using Amazon EventBridge and Amazon Simple Notification Service (Amazon SNS).\n\nWARNING\n\nThe scanning feature does not continuously scan your images, so it is important to push your image versions routinely (or\n\ntrigger a manual scan).\n\nYou can retrieve the results of the last scan for an image at any time with the command used in the last step of this recipe. Furthermore, you can use these commands as part of an automated CI/CD process that may validate whether or not an image has a certain CVSS score before deploying.\n\nChallenge 1\n\nRemediate the vulnerability by updating the image with the latest NGINX container image.\n\nChallenge 2\n\nConfigure an SNS topic to send you an email when vulnerabilities are detected in your repository.\n\n6.3 Deploying a Container Using Amazon Lightsail\n\nProblem\n\nYou need to quickly deploy a container-based application and access it securely over the internet.\n\nSolution\n\nDeploy a plain NGINX container that listens on port 80 to Lightsail. Lightsail provides a way to quickly deploy applications to AWS. The workflow is shown in Figure 6-6.\n\nFigure 6-6. Amazon Lightsail serving a container image\n\nPrerequisite\n\nIn addition to Docker Desktop and the AWS CLI (version 2), you need to install the Lightsail Control plugin (lightsailctl) for the AWS CLI.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\nNOTE\n\nSeveral power levels are available for Lightsail, each of which is priced according to the amount of compute power your\n\ncontainer needs. We selected nano in this example. A list of power levels and associated costs is available in the Lightsail\n\npricing guide.\n\n1.\n\nOnce you have lightsailctl installed, create a new container service and give it a name, power parameter, and scale parameter:\n\naws lightsail create-container-service \\\n\n--service-name awscookbook --power nano --scale 1\n\nYou should see output similar to the following:\n\n{\n\n\"containerService\": {\n\n\"containerServiceName\": \"awscookbook\",\n\n\"arn\": \"arn:aws:lightsail:us-east-1:111111111111:ContainerService/124633d7-b625-\n\n48b2-b066-5826012904d5\",\n\n\"createdAt\": \"2020-11-15T10:10:55-05:00\",\n\n\"location\": {\n\n\"availabilityZone\": \"all\",\n\n\"regionName\": \"us-east-1\"\n\n},\n\n\"resourceType\": \"ContainerService\",\n\n\"tags\": [],\n\n\"power\": \"nano\",\n\n\"powerId\": \"nano-1\",\n\n\"state\": \"PENDING\",\n\n\"scale\": 1,\n\n\"isDisabled\": false,\n\n\"principalArn\": \"\",\n\n\"privateDomainName\": \"awscookbook.service.local\",\n\n\"url\": \"https://awscookbook.<<unique-id>>.us-east-1.cs.amazonlightsail.com/\"\n\n}\n\n}\n\n2.\n\n3.\n\n4.\n\nPull a plain NGINX container image to use that listens on port 80/TCP.\n\ndocker pull nginx\n\nUse the following command to ensure that the state of your container service has entered the READY state (this may take a few minutes):\n\naws lightsail get-container-services --service-name awscookbook\n\nWhen the container service is ready, push the container image to Lightsail:\n\naws lightsail push-container-image --service-name awscookbook \\\n\n--label awscookbook --image nginx\n\nYou should see output similar to the following:\n\n7b5417cae114: Pushed\n\nImage \"nginx\" registered.\n\nRefer to this image as \":awscookbook.awscookbook.1\" in deployments.\n\nNOTE\n\nYou can specify public image repositories or push your own image to your container service within\n\nAmazon Lightsail. Rather than using a private Amazon ECR location, your Lightsail images are kept\n\nwithin the Lightsail service. For more information, refer to the Lightsail documentation for image\n\nlocations.\n\nNow you will associate the image you pushed with the container service you created for deployment. Create a file with the following contents and save it as lightsail.json (file provided in the code repository):\n\n{\n\n\"serviceName\": \"awscookbook\",\n\n\"containers\": {\n\n\"awscookbook\": {\n\n\"image\": \":awscookbook.awscookbook.1\",\n\n\"ports\": {\n\n\"80\": \"HTTP\"\n\n}\n\n}\n\n},\n\n\"publicEndpoint\": {\n\n\"containerName\": \"awscookbook\",\n\n\"containerPort\": 80\n\n}\n\n}\n\n5.\n\nCreate the deployment:\n\naws lightsail create-container-service-deployment \\\n\n--service-name awscookbook --cli-input-json file://lightsail.json\n\nView your container service again and wait for the ACTIVE state. This may take a few minutes:\n\naws lightsail get-container-services --service-name awscookbook\n\nNote the endpoint URL at the end of the output.\n\nValidation checks\n\nNow, visit the endpoint URL in your browser, or use the curl on the command line (e.g., url: https://awscookbook.un94eb3cd7hgk.us-east-1.cs.amazonlightsail.com):\n\ncurl <<URL endpoint>>\n\nYou should see output similar to the following:\n\n...\n\n<h1>Welcome to nginx!</h1>\n\n...",
      "page_number": 302
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 311-319)",
      "start_page": 311,
      "end_page": 319,
      "detection_method": "topic_boundary",
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nLightsail manages the TLS certificate, load balancer, compute, and storage. It can also manage MySQL and PostgreSQL databases as part of your deployment if your application requires it. Lightsail performs routine health checks on your application and will automatically replace a container you deploy that may have become unresponsive for some reason. Changing the power and scale parameters in the lightsail create-container-service command will allow you to create services for demanding workloads.\n\nUsing this recipe, you could deploy any common container-based application (e.g., Wordpress) and have it served on the internet in a short period of time. You could even point a custom domain alias at your Lightsail deployment for an SEO-friendly URL.\n\nChallenge\n\nScale your service so that it will be able to handle more traffic.\n\n6.4 Deploying Containers Using AWS Copilot\n\nProblem\n\nYou need a way to use your existing Dockerfile to quickly deploy and manage a load balanced web service using best practices in a private network.\n\nSolution\n\nStarting with a Dockerfile, you can use AWS Copilot to quickly deploy an application using an architecture shown in Figure 6-7.\n\nPrerequisite\n\nFigure 6-7. AWS Copilot load balanced web service infrastructure\n\nAWS Copilot CLI\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCopilot requires an ECS service-linked role to allow Amazon ECS to perform actions on your behalf. This may already exist in your AWS account. To see if you have this role already, issue the following command:\n\naws iam list-roles --path-prefix /aws-service-role/ecs.amazonaws.com/\n\n(If the role is displayed, you can skip the following role-creation step.)\n\nCreate the ECS service-linked role if it does not exist:\n\naws iam create-service-linked-role --aws-service-name ecs.amazonaws.com\n\nNOTE\n\nIAM service-linked roles allow AWS services to securely interact with other AWS services on your\n\nbehalf. See the AWS article on using these roles.\n\n2.\n\ncd to this recipe’s directory in this chapter’s code repository:\n\ncd 604-Deploy-Container-With-Copilot-CLI\n\nNOTE\n\nYou could provide your own Dockerfile and content for this recipe. If you choose to use your own\n\ncontainer with this recipe, ensure that the container listens on port 80/TCP. Or configure the alternate\n\nport with the copilot init command.\n\n3.\n\nNow use AWS Copilot to deploy the sample NGINX Dockerfile to Amazon ECS:\n\ncopilot init --app web --name nginx --type 'Load Balanced Web Service' \\\n\n--dockerfile './Dockerfile' --port 80 --deploy\n\nNOTE\n\nIf you don’t specify any arguments to the copilot init command, it will walk you through a menu of options for your deployment.\n\nThe deployment will take a few moments. You can watch the progress of the deployment in your terminal.\n\nValidation checks\n\nAfter the deployment is complete, get information on the deployed service with this command:\n\ncopilot svc show\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nTIP\n\nCopilot exposes a command-line interface that simplifies deployments to Amazon ECS, AWS Fargate, and AWS App\n\nRunner. It helps streamline your development workflow and deployment lifecycle.\n\nThe copilot init command created a directory called copilot in your current working directory. You can view and customize the configuration by using the manifest.yml that is associated with your application.\n\nNOTE\n\nThe test environment is the default environment created. You can add additional environments to suit your needs and keep\n\nyour environments isolated from each other by using the copilot env init command.\n\nCopilot configures all of the required resources for hosting containers on Amazon ECS according to many best practices. Some examples are deploying to multiple Availability Zones), using subnet tiers to segment traffic, and using AWS KMS to encrypt.\n\nThe AWS Copilot commands can also be embedded in your CI/CD pipeline to perform automated deployments. In fact, Copilot can orchestrate the creation and management of a CI/CD pipeline for you with the copilot pipeline command. For all of the current supported features and examples, visit the AWS Copilot home page.\n\nChallenge\n\nReconfigure your Load Balanced Web Service to deploy to AWS App Runner instead of Amazon ECS.\n\n6.5 Updating Containers with Blue/Green Deployments\n\nProblem\n\nYou want to use a deployment strategy with your container-based application so you can update your application to the latest version without introducing downtime to customers, while also being able to easily roll back if the deployment was not successful.\n\nSolution\n\nUse AWS CodeDeploy to orchestrate your application deployments to Amazon ECS with the Blue/Green strategy, as shown in Figure 6-8.\n\nFigure 6-8. Blue/Green target group association\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nWith the CDK stack deployed, open a web browser and visit the LOAD_BALANCER_DNS address on TCP port 8080 that the CDK output displayed. You will see the “Blue” application running:\n\nE.g.:\n\nfirefox $LOAD_BALANCER_DNS:8080\n\nor\n\nopen http://$LOAD_BALANCER_DNS:8080\n\n2.\n\nCreate an IAM role with the statement in the provided assume-role- policy.json file using this command:\n\naws iam create-role --role-name ecsCodeDeployRole \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\n3.\n\nAttach the IAM managed policy for CodeDeployRoleForECS to the IAM role:\n\naws iam attach-role-policy --role-name ecsCodeDeployRole \\\n\n--policy-arn arn:aws:iam::aws:policy/AWSCodeDeployRoleForECS\n\n4.\n\nCreate a new ALB target group to use as the “Green” target group with CodeDeploy:\n\naws elbv2 create-target-group --name \"GreenTG\" --port 80 \\\n\n--protocol HTTP --vpc-id $VPC_ID --target-type ip\n\n5.\n\nCreate the CodeDeploy application:\n\naws deploy create-application --application-name awscookbook-605 \\\n\n6.\n\n7.\n\n8.\n\n9.\n\n--compute-platform ECS\n\nNOTE\n\nCodeDeploy requires some configuration. We provided a template file for you (codedeploy-\n\ntemplate.json) in this recipe’s directory of the code repository.\n\nUse the sed command to replace the values with the environment variables you exported with the helper.py script:\n\nsed -e \"s/AWS_ACCOUNT_ID/${AWS_ACCOUNT_ID}/g\" \\\n\ne \"s|PROD_LISTENER_ARN|${PROD_LISTENER_ARN}|g\" \\\n\ne \"s|TEST_LISTENER_ARN|${TEST_LISTENER_ARN}|g\" \\\n\ncodedeploy-template.json > codedeploy.json\n\nTIP\n\nsed (short for stream editor) is a great tool to use for text find-and-replace operations as well as other types of text manipulation in your terminal sessions and scripts. In this case, sed is used to replace values in a template file with values output from cdk deploy set as environment variables.\n\nNow, create a deployment group:\n\naws deploy create-deployment-group --cli-input-json file://codedeploy.json\n\nAppSpec-template.yaml contains information about the application you are going to update. The CDK preprovisioned a task definition you can use.\n\nUse the sed command to replace the value with the environment variable you exported with the helper.py script:\n\nsed -e \"s|FargateTaskGreenArn|${FARGATE_TASK_GREEN_ARN}|g\" \\\n\nappspec-template.yaml > appspec.yaml\n\nNow copy the AppSpec file to the S3 bucket created by the CDK deployment so that CodeDeploy can use it to update the application:",
      "page_number": 311
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 320-331)",
      "start_page": 320,
      "end_page": 331,
      "detection_method": "topic_boundary",
      "content": "10.\n\n11.\n\naws s3 cp ./appspec.yaml s3://$BUCKET_NAME\n\nOne final configuration file needs to be created; this contains the instructions about the deployment. Use sed to modify the S3 bucket used in the deployment-template.json file:\n\nsed -e \"s|S3BucketName|${BUCKET_NAME}|g\" \\\n\ndeployment-template.json > deployment.json\n\nNow create a deployment with the deployment configuration:\n\naws deploy create-deployment --cli-input-json file://deployment.json\n\nTo get the status of the deployment, observe the status in the AWS Console (Developer Tools→CodeDeploy→Deployments, and click the deployment ID). You should see CodeDeploy in progress with the deployment, as shown in Figure 6-9.\n\nFigure 6-9. Initial deployment status\n\nValidation checks\n\nOnce the replacement task is serving 100% of the traffic, you can visit the same URL where you previously observed the Blue application running, replaced with the Green version of the application.\n\nTIP\n\nYou may need to refresh your browser or clear your cache to see the updated Green application.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nCodeDeploy offers several deployment strategies—Canary, AllAtOnce, Blue/Green, etc.—and you can also create your own custom deployment strategies. You might customize the strategy to define a longer wait period for the cutover window or to define other conditions to be met before traffic switchover occurs. In the default Blue/Green strategy, CodeDeploy keeps your previous version of the application running for five minutes while all traffic is routed to the new version. If you notice that the new version is not behaving properly, you can quickly route traffic back to the original version since it is still running in a separate AWS Application Load Balancer target group.\n\nCodeDeploy uses ALB target groups to manage which application is considered “production.” When you deployed the initial stack with the AWS CDK, the V1 Blue containers were registered with a target group associated with port 8080 on the ALB. After you initiate the deployment of the new version, CodeDeploy starts a brand new version of the ECS service, associates it with the Green target group you created, and then gracefully shifts all traffic to the Green target group. The final result is the Green V2 containers now being served on port 8080 of the ALB. The previous target group is now ready to execute the next Blue/Green deployment.\n\nThis is a common pattern to utilize with CI/CD. Your previous version can quickly be reactivated with a seamless rollback. If no rollback is needed, the initial version (V1) is terminated, and you can repeat the processes the next time you deploy, putting V3 in the Blue target group, shifting traffic to it when you are ready. Using this strategy helps you minimize the impact to users of new application versions while allowing more frequent deployments.\n\nTIP\n\nDeployment conditions allow you to define deployment success criteria. You can use a combination of a custom\n\ndeployment strategy and a deployment condition to build automation tests into your CodeDeploy process. This would\n\nallow you to ensure that all of your tests run and pass before traffic is sent to your new deployment.\n\nChallenge\n\nTrigger a rollback to the original V1 container deployment and observe the results.\n\n6.6 Autoscaling Container Workloads on Amazon ECS\n\nProblem\n\nYou need to deploy a containerized service that scales out during times of heavy traffic to meet demand.\n\nSolution\n\nConfigure a CloudWatch alarm and scaling policy for an ECS service so that your service adds more containers when the CPU load increases, as shown in Figure 6-10.\n\nFigure 6-10. ECS service with a CloudWatch alarm and scaling policy\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nAccess the ECS service URL over the internet with the curl command (or your web browser) to verify the successful deployment:\n\ncurl -v -m 3 $LOAD_BALANCER_DNS\n\n2.\n\n3.\n\nUse verbose (-v) and three-second timeout (-m 3) to ensure you see the entire connection and have a timeout set. The following is an example command and output:\n\ncurl -v -m 3 http://AWSCookbook.us-east-1.elb.amazonaws.com:80/\n\nTrying 1.2.3.4...\n\nTCP_NODELAY set\n\nConnected to AWSCookbook.us-east-1.elb.amazonaws.com (1.2.3.4) port 80<\n\n> GET / HTTP/1.1\n\n> Host: AWSCookbook.us-east-1.elb.amazonaws.com:80\n\n> User-Agent: curl/7.64.1\n\n> Accept: */*\n\n>\n\n< HTTP/1.1 200\n\n< Content-Type: application/json\n\n< Content-Length: 318\n\n< Connection: keep-alive\n\n<\n\n{\n\n\"URL\":\"http://awscookbookloadtestloadbalancer-36821611.us-east-\n\n1.elb.amazonaws.com:80/\",\n\n\"ContainerLocalAddress\":\"10.192.2.179:8080\",\n\n\"ProcessingTimeTotalMilliseconds\":\"0\",\n\n\"LoadBalancerPrivateIP\":\"10.192.2.241\",\n\n\"ContainerHostname\":\"ip-10-192-2-179.ec2.internal\",\n\n\"CurrentTime\":\"1605724705176\"\n\n}\n\nClosing connection 0\n\nTIP\n\nRun this same curl command several times in a row, and you will notice the ContainerHostname and ContainerLocalAddress alternating between two addresses. This indicates that Amazon ECS is load balancing between the two containers you should expect to be running at all times, as defined by the\n\nECS service.\n\nYou will need to create a role for the autoscaling trigger to execute; this file is provided in this solution’s directory in the code repository:\n\naws iam create-role --role-name AWSCookbook606ECS \\\n\n4.\n\n5.\n\n6.\n\n7.\n\n--assume-role-policy-document file://task-execution-assume-role.json\n\nAttach the managed policy for autoscaling:\n\naws iam attach-role-policy --role-name AWSCookbook606ECS --policy-arn\n\narn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceAutoscaleRole\n\nRegister an autoscaling target:\n\naws application-autoscaling register-scalable-target \\\n\n--service-namespace ecs \\\n\n--scalable-dimension ecs:service:DesiredCount \\\n\n--resource-id service/$ECS_CLUSTER_NAME/AWSCookbook606 \\\n\n--min-capacity 2 \\\n\n--max-capacity 4\n\nSet up an autoscaling policy for the autoscaling target using the sample configuration file specifying a 50% average CPU target:\n\naws application-autoscaling put-scaling-policy --service-namespace ecs \\\n\n--scalable-dimension ecs:service:DesiredCount \\\n\n--resource-id service/$ECS_CLUSTER_NAME/AWSCookbook606 \\\n\n--policy-name cpu50-awscookbook-606 --policy-type TargetTrackingScaling \\\n\n--target-tracking-scaling-policy-configuration file://scaling-policy.json\n\nNow, to trigger a process within the container that simulates high CPU load, run the same curl command, appending cpu to the end of the service URL:\n\ncurl -v -m 3 $LOAD_BALANCER_DNS/cpu\n\nThis command will time out after three seconds, indicating that the container is running a CPU-intensive process as a result of visiting that URL (the ECS service you deployed with the CDK runs a CPU load generator that we provided to simulate high CPU usage). The following is an example command and output:\n\ncurl -v -m 3 http://AWSCookbookLoadtestLoadBalancer-36821611.us-east-\n\n1.elb.amazonaws.com:80/cpu\n\nTrying 52.4.148.24...\n\nTCP_NODELAY set\n\nConnected to AWSCookbookLoadtestLoadBalancer-36821611.us-east-\n\n1.elb.amazonaws.com (52.4.148.245) port 80 (#0)\n\n> GET /cpu HTTP/1.1\n\n> Host: AWSCookbookLoadtestLoadBalancer-36821611.us-east-1.elb.amazonaws.com:80\n\n> User-Agent: curl/7.64.1\n\n> Accept: */*\n\n>\n\nOperation timed out after 3002 milliseconds with 0 bytes received\n\nClosing connection 0\n\ncurl: (28) Operation timed out after 3002 milliseconds with 0 bytes received\n\nValidation checks\n\nWait approximately five minutes. Then log into the AWS Console, locate Elastic Container Service, go to the Clusters page, select the cluster deployed, and select the ECS service. Verify that the Desired Count has increased to 4, the maximum scaling value that you configured. You can click the Tasks tab to view four container tasks now running for your service.\n\nClick the Metrics tab to view the CPU usage for the service. You set the scaling target at 50% to trigger the autoscaling actions, adding two additional containers to the service as a result of high CPU usage. An example metrics graph is shown in Figure 6-11.\n\nFigure 6-11. ECS service metrics on the AWS Console\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAutosccaling is an important mechanism to implement to save costs associated with running your applications on AWS services. It allows your applications to provision their own resources as needed during times when load may increase, and it removes their own resources during times when the application may be idle. Note that whenever you have an AWS service doing something like this on your behalf, you have to specifically grant permission for services to execute these functions via IAM.\n\nThe underlying data that provides the metrics for such operations is contained in the CloudWatch metrics service. There are many data points and metrics that you can use for configuring autoscaling; some of the most common ones are listed here:\n\nNetwork I/O\n\nCPU usage\n\nMemory used\n\nNumber of transactions\n\nIn this recipe, you monitor the CPU usage metric on the ECS service. You set the metric at 50% and trigger the CPU load with a cURL call to the HTTP endpoint of the ECS service. Scaling metrics are dependent upon the type of applications you are running and the technologies you use to build them. As a best practice, you should observe your application metrics over a period of time to set a baseline before choosing metrics to implement autoscaling.\n\nChallenge\n\nReplace the provided sample CPU load application with your own containerized application and configure the target scaling policy to meet your needs.\n\n6.7 Launching a Fargate Container Task in Response to an Event\n\nProblem\n\nYou need to launch a container task to process incoming files.\n\nSolution\n\nUse Amazon EventBridge to trigger the launch of ECS container tasks on Fargate after a file is uploaded to S3, as shown in Figure 6-12.\n\nFigure 6-12. Flow of container EventBridge pattern\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nConfigure CloudTrail to log events on the S3 bucket:\n\naws cloudtrail put-event-selectors --trail-name $CLOUD_TRAIL_ARN --event-\n\nselectors \"[{ \\\"ReadWriteType\\\":\n\n\\\"WriteOnly\\\", \\\"IncludeManagementEvents\\\":false, \\\"DataResources\\\": [{\n\n\\\"Type\\\": \\\"AWS::S3::Object\\\",\n\n\\\"Values\\\": [\\\"arn:aws:s3:::$BUCKET_NAME/input/\\\"] }],\n\n\\\"ExcludeManagementEventSources\\\": [] }]\"\n\nNow create an assume-role policy JSON statement called policy1.json to use in the next step (this file is provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"events.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\n2.\n\nCreate the role and specify the policy1.json file:\n\naws iam create-role --role-name AWSCookbook607RuleRole \\\n\n--assume-role-policy-document file://policy1.json\n\n3.\n\nYou will also need a policy document with the following content called policy2.json (this file is provided in the repository):\n\n{\n\n4.\n\n5.\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"ecs:RunTask\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:ecs:*:*:task-definition/*\"\n\n]\n\n},\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": \"iam:PassRole\",\n\n\"Resource\": [\n\n\"*\"\n\n],\n\n\"Condition\": {\n\n\"StringLike\": {\n\n\"iam:PassedToService\": \"ecs-tasks.amazonaws.com\"\n\n}\n\n}\n\n}\n\n]\n\n}\n\nNow attach the IAM policy JSON you just created to the IAM role:\n\naws iam put-role-policy --role-name AWSCookbook607RuleRole \\\n\n--policy-name ECSRunTaskPermissionsForEvents \\\n\n--policy-document file://policy2.json\n\nCreate an EventBridge rule that monitors the S3 bucket for file uploads:\n\naws events put-rule --name \"AWSCookbookRule\" --role-arn\n\n\"arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook607RuleRole\" --event-pattern \"\n\n{\\\"source\\\":[\\\"aws.s3\\\"],\\\"detail-type\\\":[\\\"AWS API Call via\n\nCloudTrail\\\"],\\\"detail\\\":{\\\"eventSource\\\":[\\\"s3.amazonaws.com\\\"],\\\"eventName\\\":\n\n[\\\"CopyObject\\\",\\\"PutObject\\\",\\\"CompleteMultipartUpload\\\"],\\\"requestParameters\\\"\n\n:{\\\"bucketName\\\":[\\\"$BUCKET_NAME\\\"]}}}\"\n\n6.\n\n7.\n\n8.\n\n9.\n\nModify the value in targets-template.json and create a targets.json for use:\n\nsed -e \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\ne \"s|AWS_REGION|${AWS_REGION}|g\" \\\n\ne \"s|ECSClusterARN|${ECS_CLUSTER_ARN}|g\" \\\n\ne \"s|TaskDefinitionARN|${TASK_DEFINITION_ARN}|g\" \\\n\ne \"s|VPCPrivateSubnets|${VPC_PRIVATE_SUBNETS}|g\" \\\n\ne \"s|VPCDefaultSecurityGroup|${VPC_DEFAULT_SECURITY_GROUP}|g\" \\\n\ntargets-template.json > targets.json\n\nCreate a rule target that specifies the ECS cluster, ECS task definition, IAM role, and networking parameters. This specifies what the rule will trigger; in this case, launch a container on Fargate:\n\naws events put-targets --rule AWSCookbookRule \\\n\n--targets file://targets.json\n\nYou should see output similar to the following:\n\n{\n\n\"FailedEntryCount\": 0,\n\n\"FailedEntries\": []\n\n}\n\nCheck the S3 bucket to verify that it’s empty before we populate it:\n\naws s3 ls s3://$BUCKET_NAME/\n\nCopy the provided maze.jpg file to the S3 bucket. This will trigger the ECS task that launches a container with a Python library to process the file:\n\naws s3 cp maze.jpg s3://$BUCKET_NAME/input/maze.jpg\n\nThis will trigger an ECS task to process the image file. Quickly, check the task with the ecs list-tasks command. The task will run for about two to",
      "page_number": 320
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 332-339)",
      "start_page": 332,
      "end_page": 339,
      "detection_method": "topic_boundary",
      "content": "three minutes:\n\naws ecs list-tasks --cluster $ECS_CLUSTER_ARN\n\nYou should see output similar to the following:\n\n{\n\n\"taskArns\": [\n\n\"arn:aws:ecs:us-east-1:111111111111:task/cdk-aws-cookbook-607-\n\nAWSCookbookEcsCluster46494E6E-MX7kvtp1sYWZ/d86f16af55da56b5ca4874d6029\"\n\n]\n\n}\n\nValidation checks\n\nAfter a few minutes, observe the output directory created in the S3 bucket:\n\naws s3 ls s3://$BUCKET_NAME/output/\n\nDownload and view the output file:\n\naws s3 cp s3://$BUCKET_NAME/output/output.jpg ./output.jpg\n\nOpen output.jpg with a file viewer of your choice to view the file that was processed.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nEvent-driven architecture is an important approach to application and process design in the cloud. This type of design allows for removing long-running application workloads in favor of serverless architectures, which can be more resilient and easily scale to peaks of higher usage when needed. When there are no events to handle in your application, you generally do not pay much for compute resources (if at all), so potential cost savings is also a point to consider when choosing an application architecture.\n\nNOTE\n\nIt is common to use Lambda functions with S3 for event-driven architectures, but for longer-running data-processing jobs\n\nand computational jobs like this one, Fargate is a better choice because the runtime is essentially infinite, while the\n\nmaximum runtime for Lambda functions is limited.\n\nAmazon ECS can run tasks and services. Services are made up of tasks, and generally, are long- running in that a service keeps a specific set of tasks running. Tasks can be short-lived; a container may start, process some data, and then gracefully exit after the task is complete. This is what you have achieved in this solution: a task was launched in response to an S3 event signaling a new object, and the container read the object, processed the file, and exited.\n\nChallenge\n\nWhile EventBridge is a powerful solution that can be used to orchestrate many types of event- driven solutions, you can achieve similar functionality with S3’s triggers. Try to deploy and configure a Lambda function to be invoked directly from S3 events. Here is a hint.\n\n6.8 Capturing Logs from Containers Running on Amazon ECS\n\nProblem\n\nYou have an application running in a container and want to inspect the application logs.\n\nSolution\n\nSend the logs from the container to Amazon CloudWatch. By specifying the awslogs driver within an ECS task definition and providing an IAM role that allows the container to write to CloudWatch logs, you are able to stream container logs to a location within Amazon CloudWatch. A high-level view of this configuration and process is shown in Figure 6-13.\n\nFigure 6-13. Streaming container logs to CloudWatch\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file called task-execution-assume-role.json with the following content. The file is provided in the root of this recipe’s directory in the code repository.\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"\",\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"ecs-tasks.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\n2.\n\nCreate an IAM role using the statement in the preceding file:\n\naws iam create-role --role-name AWSCookbook608ECS \\\n\n--assume-role-policy-document file://task-execution-assume-role.json\n\n3.\n\nAttach the AWS managed IAM policy for ECS task execution to the IAM role that you just created:\n\naws iam attach-role-policy --role-name AWSCookbook608ECS --policy-arn\n\narn:aws:iam::aws:policy/service-\n\nrole/AmazonECSTaskExecutionRolePolicy\n\n4.\n\nCreate a log group in CloudWatch:\n\naws logs create-log-group --log-group-name AWSCookbook608ECS\n\n5.\n\nCreate a file called taskdef.json with the following content (a file is provided in this recipe’s directory in the code repository):\n\n{\n\n\"networkMode\": \"awsvpc\",\n\n\"containerDefinitions\": [\n\n{\n\n\"portMappings\": [\n\n{\n\n\"hostPort\": 80,\n\n\"containerPort\": 80,\n\n\"protocol\": \"tcp\"\n\n}\n\n],\n\n\"essential\": true,\n\n\"entryPoint\": [\n\n\"sh\",\n\n\"-c\"\n\n],\n\n\"logConfiguration\": {\n\n\"logDriver\": \"awslogs\",\n\n\"options\": {\n\n\"awslogs-group\": \"AWSCookbook608ECS\",\n\n\"awslogs-region\": \"us-east-1\",\n\n\"awslogs-stream-prefix\": \"LogStream\"\n\n}\n\n},\n\n\"name\": \"awscookbook608\",\n\n\"image\": \"httpd:2.4\",\n\n\"command\": [\n\n\"/bin/sh -c \\\"echo 'Hello AWS Cookbook Reader, this container is running on\n\nECS!' >\n\n/usr/local/apache2/htdocs/index.html && httpd-foreground\\\"\"\n\n]\n\n}\n\n],\n\n\"family\": \"awscookbook608\",\n\n\"requiresCompatibilities\": [\n\n\"FARGATE\"\n\n],\n\n\"cpu\": \"256\",\n\n\"memory\": \"512\"\n\n}\n\n6.\n\nNow that you have an IAM role and an ECS task definition configuration, you need to create the ECS task using the configuration and associate the IAM role:\n\naws ecs register-task-definition --execution-role-arn \\\n\n\"arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook608ECS\" \\\n\n--cli-input-json file://taskdef.json\n\n7.\n\nRun the ECS task on the ECS cluster that you created earlier in this recipe with the AWS CDK:\n\naws ecs run-task --cluster $ECS_CLUSTER_NAME \\\n\n--launch-type FARGATE --network-configuration\n\n\"awsvpcConfiguration={subnets=[$VPC_PUBLIC_SUBNETS],securityGroups=\n\n[$VPC_DEFAULT_SECURITY_GROUP],assign\n\nPublicIp=ENABLED}\" --task-definition awscookbook608\n\nValidation checks\n\nCheck the status of the task to make sure the task is running. First, find the Task’s Amazon Resource Name (ARN):\n\nTASK_ARNS=$(aws ecs list-tasks --cluster $ECS_CLUSTER_NAME \\\n\n--output text --query taskArns)\n\nThen use the task ARNs to check for the RUNNING state with the describe-tasks command output:\n\naws ecs describe-tasks --cluster $ECS_CLUSTER_NAME --tasks $TASK_ARNS\n\nAfter the task has reached the RUNNING state (approximately 15 seconds), use the following commands to view logs:\n\naws logs describe-log-streams --log-group-name AWSCookbook608ECS\n\nYou should see output similar to the following:\n\n{\n\n\"logStreams\": [\n\n{\n\n\"logStreamName\": \"LogStream/webserver/97635dab942e48d1bab11dbe88c8e5c3\",\n\n\"creationTime\": 1605584764184,\n\n\"firstEventTimestamp\": 1605584765067,\n\n\"lastEventTimestamp\": 1605584765067,\n\n\"lastIngestionTime\": 1605584894363,\n\n\"uploadSequenceToken\": \"49612420096740389364147985468451499506623702081936625922\",\n\n\"arn\": \"arn:aws:logs:us-east-1:111111111111:log-group:AWSCookbook608ECS:log-\n\nstream:LogStream/webserver/97635dab942e48d1bab11dbe88c8e5c3\",\n\n\"storedBytes\": 0\n\n}\n\n]\n\n}\n\nNote the logStreamName from the output and then run the get-log-events command:\n\naws logs get-log-events --log-group-name AWSCookbook608ECS \\\n\n--log-stream-name <<logStreamName>>\n\nYou should see output similar to the following:\n\n{\n\n\"events\": [\n\n{\n\n\"timestamp\": 1605590555566,\n\n\"message\": \"[Tue Nov 17 05:22:35.566054 2020] [mpm_event:notice] [pid 7:tid 140297116308608] AH00489:\n\nApache/2.4.46 (Unix) configured -- resuming normal operations\",\n\n\"ingestionTime\": 1605590559713\n\n},\n\n{\n\n\"timestamp\": 1605590555566,\n\n\"message\": \"[Tue Nov 17 05:22:35.566213 2020] [core:notice] [pid 7:tid 140297116308608] AH00094:\n\nCommand line: 'httpd -D FOREGROUND'\",\n\n\"ingestionTime\": 1605590559713\n\n}\n\n],\n\n\"nextForwardToken\": \"f/35805865872844590178623550035180924397996026459535048705\",\n\n\"nextBackwardToken\": \"b/35805865872844590178623550035180924397996026459535048704\"\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou made use of the awslogs driver and an IAM role, which allows the running task to write to a CloudWatch log group. This is a common pattern when working with containers on AWS, as you most likely need log output for troubleshooting and debugging your application. This configuration is handled by tools like Copilot since it is a common pattern, but when working with Amazon ECS directly, like defining and running a task, the configuration is critical for developers to know about.\n\nThe PID 1 process output to /dev/stdout and /dev/stderr is captured by the awslogs driver. In other words, the first process in the container is the only process logging to these streams. Be sure your application that you would like to see logs from is running with PID 1 inside of your container.\n\nIn order for most AWS services to communicate with one another, you must assign a role to them that allows the required level of permissions for the communication. This holds true when configuring logging to CloudWatch from a container ECS task; the container must have a role associated with it that allows the CloudWatchLogs operations via the awslogs logConfiguration driver:\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"logs:CreateLogGroup\",\n\n\"logs:CreateLogStream\",\n\n\"logs:PutLogEvents\",\n\n\"logs:DescribeLogStreams\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:logs:*:*:*\"\n\n]\n\n}\n\n]\n\n}\n\nCloudWatch logs allow for a central logging solution for many AWS services. When running multiple containers, it is important to be able to quickly locate logs for debugging purposes.\n\nChallenge\n\nYou can tail the logs of a log stream to give you a more real-time view of the logs that your application is generating. This can help in your development and troubleshooting activities. Try using the aws logs tail command with your log stream while generating some output for you to observe.",
      "page_number": 332
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 340-347)",
      "start_page": 340,
      "end_page": 347,
      "detection_method": "topic_boundary",
      "content": "Chapter 7. Big Data\n\n7.0 Introduction Data is sometimes referred to as “the new gold.” Many companies are leveraging data in new and exciting ways every day as available data science tools continue to improve. You can now mine troves of historical data quickly for insights and patterns by using modern analytics tools. You might not yet know the queries and analysis you need to run against the data, but tomorrow you might be faced with a challenge that could be supported by historical data analysis using new and emerging techniques. With the advent of cheaper data storage, many organizations and individuals opt to keep data rather than discard it so that they can run historical analysis to gain business insights, discover trends, train AI/ML models, and be ready to implement future technologies that can use the data.\n\nIn addition to the amount of data you might collect over time, you are also collecting a wider variety of data types and structures at an increasingly faster velocity. Imagine that you might deploy IoT devices to collect sensor data, and as you continue to deploy these over time, you need a way to capture and store the data in a scalable way. This can be structured, semistructured, and unstructured data with schemas that might be difficult to predict as new data sources are ingested. You need tools to be able to transform and analyze your diverse data.\n\nAn informative and succinct AWS re:Invent 2020 presentation by Francis Jayakumar, “An Introduction to Data Lakes and Analytics on AWS,” provides a high-level introduction to what is available on AWS for big data and analytics. We could have included so much in this chapter— enough to fill another book—but we will focus on foundational recipes for sending data to S3, discovering data on S3, and transforming data to give you examples of working with data on AWS.\n\nWorkstation Configuration\n\nYou will need a few things installed to be ready for the recipes in this chapter.\n\nGeneral setup\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/awscookbook/BigData\n\n7.1 Using a Kinesis Stream for Ingestion of Streaming Data\n\nProblem\n\nYou need a way to ingest streaming data for your applications.\n\nSolution\n\nCreate a Kinesis stream and verify that it is working by using the AWS CLI to put a record on the stream, as shown in Figure 7-1.\n\nFigure 7-1. Using a Kinesis stream for ingestion of streaming data\n\nSteps\n\n1.\n\nCreate a Kinesis stream:\n\naws kinesis create-stream --stream-name AWSCookbook701 --shard-count 1\n\nNOTE\n\nShards are an important concept to understand with Kinesis streams as you scale your stream to handle\n\nlarger velocities of incoming streaming data. Each shard can support up to five transactions per second\n\nfor reads, up to a maximum total data read rate of 2 MB per second. For writes, each shard can support\n\nup to 1,000 records per second, up to a maximum total data write rate of 1 MB per second (including\n\npartition keys). You can re-shard your stream at any time if you need to handle more data.\n\n2.\n\nConfirm that your stream is in ACTIVE state:\n\naws kinesis describe-stream-summary --stream-name AWSCookbook701\n\nYou should see output similar to the following:\n\n{\n\n\"StreamDescriptionSummary\": {\n\n\"StreamName\": \"AWSCookbook701\",\n\n\"StreamARN\": \"arn:aws:kinesis:us-east-1:111111111:stream/AWSCookbook701\",\n\n\"StreamStatus\": \"ACTIVE\",\n\n\"RetentionPeriodHours\": 24,\n\n\"StreamCreationTimestamp\": \"2021-10-12T17:12:06-04:00\",\n\n\"EnhancedMonitoring\": [\n\n{\n\n\"ShardLevelMetrics\": []\n\n}\n\n],\n\n\"EncryptionType\": \"NONE\",\n\n\"OpenShardCount\": 1,\n\n\"ConsumerCount\": 0\n\n}\n\n}\n\nValidation checks\n\nPut a record on the Kinesis stream:\n\naws kinesis put-record --stream-name AWSCookbook701 \\\n\n--partition-key 111 \\\n\n--cli-binary-format raw-in-base64-out \\\n\n--data={\\\"Data\\\":\\\"1\\\"}\n\nYou should see output similar to the following:\n\n{\n\n\"ShardId\": \"shardId-000000000000\",\n\n\"SequenceNumber\": \"49622914081337086513355510347442886426455090590105206786\"\n\n}\n\nGet the record from the Kinesis stream. Get the shard iterator and run the get-records command:\n\nSHARD_ITERATOR=$(aws kinesis get-shard-iterator \\\n\n--shard-id shardId-000000000000 \\\n\n--shard-iterator-type TRIM_HORIZON \\\n\n--stream-name AWSCookbook701 \\\n\n--query 'ShardIterator' \\\n\n--output text)\n\naws kinesis get-records --shard-iterator $SHARD_ITERATOR \\\n\n--query Records[0].Data --output text | base64 --decode\n\nYou should see output similar to the following:\n\n{\"Data\":\"1\"}\n\nNOTE\n\nThe data is base64 encoded that you published to the stream. You queried the command output for the data element within\n\nthe JSON object and piped the output to base64 --decode to validate that the record is what you published.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nStreaming data can come from a variety of sources. The sources putting records on streams are known as producers. Entities getting records from streams are known as consumers. In the case of streaming data, you are dealing with real-time information, and you may need to act on it immediately or store it for usage later (see Recipe 7.2). Some common producer examples to think about are as follows:\n\nReal-time financial market data\n\nIoT and sensor data\n\nEnd-user clickstream activity from web and mobile applications\n\nYou can use the Kinesis Producer Library (KPL) and Kinesis Client Library (KCL) within your application for your specific needs. When consuming data from Kinesis streams, you can configure your application to read records from the stream and respond, invoke Lambda functions directly from the stream, or even use a Kinesis Data Analytics application (powered by Apache Flink) directly within the Kinesis service.\n\nNOTE\n\nThe Kinesis service will scale automatically to meet your needs, but you need to be aware of the quotas and limits to\n\nensure that you do not exceed capacity with your shard configuration.\n\nChallenge\n\nAutomatically trigger a Lambda function to process incoming Kinesis data.\n\n7.2 Streaming Data to Amazon S3 Using Amazon Kinesis Data Firehose\n\nProblem\n\nYou need to deliver incoming streaming data to object storage.\n\nSolution\n\nCreate an S3 bucket, create a Kinesis stream, and configure Kinesis Data Firehose to deliver the stream data to the S3 bucket. The flow is shown in Figure 7-2.\n\nFigure 7-2. Streaming data to Amazon S3 using Amazon Kinesis Data Firehose\n\nPrerequisites\n\nKinesis stream\n\nS3 Bucket with a CSV file\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\n2.\n\nOpen the Kinesis Data Firehose console and click the “Create delivery stream” button; choose Amazon Kinesis Data Streams for the source and Amazon S3 for the destination, as shown in Figure 7-3.\n\nFigure 7-3. Choosing the source and destination in the Kinesis Data Firehose dialog\n\nFor Source settings, choose the Kinesis stream that you created in the preparation steps, as shown in Figure 7-4.\n\n3.\n\n4.\n\nFigure 7-4. Choosing a Kinesis data stream\n\nKeep the defaults (Disabled) for “Transform and convert records” options. For Destination settings, browse for and choose the S3 bucket that you created in the preparation steps as shown in Figure 7-5, and keep the defaults for the other options (disabled partitioning and no prefixes).\n\nFigure 7-5. Kinesis Data Firehose destination configuration\n\nUnder the Advanced settings section, confirm that the “Create or update IAM role” is selected. This will create an IAM role that Kinesis can use to\n\naccess the stream and S3 bucket, as shown in Figure 7-6.\n\nFigure 7-6. Creating an IAM role for the Kinesis Data Firehose service\n\nValidation checks\n\nYou can test delivery to the stream from within the Kinesis console. Click the Delivery streams link in the left navigation menu, choose the stream you created, expand the “Test with demo data” section, and click the “Start sending demo data” button. This will initiate sending sample data to your stream so you can verify that it is making it to your S3 bucket. A sample is shown in Figure 7-7.",
      "page_number": 340
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 348-355)",
      "start_page": 348,
      "end_page": 355,
      "detection_method": "topic_boundary",
      "content": "Figure 7-7. Sending test data through a Kinesis Data Firehose\n\nAfter a few minutes, you will see a folder structure and a file appear in your S3 bucket, similar to Figure 7-8.\n\nFigure 7-8. S3 destination with Kinesis stream data delivered\n\nIf you download and inspect the file, you will see output similar to the following:\n\n{\"CHANGE\":3.95,\"PRICE\":79.75,\"TICKER_SYMBOL\":\"SLW\",\"SECTOR\":\"ENERGY\"}\n\n{\"CHANGE\":7.27,\"PRICE\":96.37,\"TICKER_SYMBOL\":\"ALY\",\"SECTOR\":\"ENERGY\"}\n\n{\"CHANGE\":-5,\"PRICE\":81.74,\"TICKER_SYMBOL\":\"QXZ\",\"SECTOR\":\"HEALTHCARE\"}\n\n{\"CHANGE\":-0.6,\"PRICE\":98.4,\"TICKER_SYMBOL\":\"NFLX\",\"SECTOR\":\"TECHNOLOGY\"}\n\n{\"CHANGE\":-0.46,\"PRICE\":18.92,\"TICKER_SYMBOL\":\"PLM\",\"SECTOR\":\"FINANCIAL\"}\n\n{\"CHANGE\":4.09,\"PRICE\":100.46,\"TICKER_SYMBOL\":\"ALY\",\"SECTOR\":\"ENERGY\"}\n\n{\"CHANGE\":2.06,\"PRICE\":32.34,\"TICKER_SYMBOL\":\"PPL\",\"SECTOR\":\"HEALTHCARE\"}\n\n{\"CHANGE\":-2.99,\"PRICE\":38.98,\"TICKER_SYMBOL\":\"KFU\",\"SECTOR\":\"ENERGY\"}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAs you begin to ingest data from various sources, your application may be consuming or reacting to streaming data in real time. In some cases, you may want to store the data from the stream to query or process it later. You can use Kinesis Data Firehose to deliver data to object storage (S3), Amazon Redshift, OpenSearch, and many third-party endpoints. You can also connect multiple delivery streams to a single producer stream to deliver data if you have to support multiple delivery locations from your streams.\n\nNOTE\n\nKinesis Data Firehose scales automatically to handle the volume of data you need to deliver, meaning that you do not have\n\nto configure or provision additional resources if your data stream starts to receive large volumes of data. For more\n\ninformation on Kinesis Data Firehose features and capabilities, see the AWS documentation for Kinesis Data Firehose.\n\nIf you need to transform data before it ends up in the destination via Firehose, you can configure transformations. A transformation will automatically invoke a Lambda function as your streaming data queues up (see this Firehose article for buffer size information). This is useful when you have to adjust the schema of a record before delivery, sanitize data for long-term storage on the fly (e.g., remove personally identifiable information), or join the data with other sources before delivery. The transformation Lambda function you invoke must follow the convention specified by the Kinesis Data Firehose API. To see some examples of Lambda functions, go to the AWS Serverless Application Repository and search for “firehose.”\n\nChallenge\n\nConfigure a Firehose delivery with transformations to remove a field from the streaming data before delivery.\n\n7.3 Automatically Discovering Metadata with AWS Glue Crawlers\n\nProblem\n\nYou have CSV data files on object storage, and you would like to discover the schema and metadata about the files to use in further analysis and query operations.\n\nSolution\n\nCreate an AWS Glue database, follow the crawler configuration wizard to configure a crawler to scan your S3 bucket data, run the crawler, and inspect the resulting table, as shown in Figure 7-9.\n\nFigure 7-9. Automatically discover metadata with AWS Glue crawlers\n\nPrerequisite\n\nAn S3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nLog in to the AWS Console and navigate to the AWS Glue console, choose Databases from the left navigation menu and select “Add database,” as shown in Figure 7-10.\n\n2.\n\nFigure 7-10. Creating a database in the Glue data catalog\n\nGive your database a name (e.g., awscookbook703) and click Create. A sample dialog box is shown in Figure 7-11.\n\n3.\n\nFigure 7-11. Database name dialog box\n\nSelect Tables from the left navigation menu and choose “Add tables” → “Add tables using a crawler,” as shown in Figure 7-12.\n\n4.\n\nFigure 7-12. Adding a table to the Glue Data catalog\n\nFollow the “Add crawler” wizard. For the crawler source type, choose “Data stores,” crawl all folders, S3 as “Data store” (do not define a Connection), choose your S3 bucket and “data” folder in “Include path,” and do not choose a sample size. Choose to create an IAM role, and suffix it with AWSCookbook703. For a frequency, choose “Run on demand,” and select the database you created in step 2. Confirm the configuration on the “Review all steps” page and click Finish. An example review page is shown in Figure 7- 13.\n\n5.\n\nFigure 7-13. Review settings for Glue crawler\n\nFrom the left navigation menu, select Crawlers. Choose the crawler that you created in step 4 and click “Run crawler,” as shown in Figure 7-14.\n\nNOTE\n\nThe Glue crawler will take a few moments to run. Once it is complete, you can view the table\n\nproperties of the discovered schema and metadata.",
      "page_number": 348
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 356-363)",
      "start_page": 356,
      "end_page": 363,
      "detection_method": "topic_boundary",
      "content": "Figure 7-14. Glue crawler configuration summary\n\nValidation checks\n\nVerify the crawler configuration you created; you can use the AWS CLI or the Glue console. Note the LastCrawl status of SUCCEEDED:\n\naws glue get-crawler --name awscookbook703\n\nYou should see output similar to this:\n\n{\n\n\"Crawler\": {\n\n\"Name\": \"awscookbook703\",\n\n\"Role\": \"service-role/AWSGlueServiceRole-AWSCookbook703\",\n\n\"Targets\": {\n\n\"S3Targets\": [\n\n{\n\n\"Path\": \"s3://awscookbook704-<RANDOM_STRING>/data\",\n\n\"Exclusions\": []\n\n}\n\n],\n\n\"JdbcTargets\": [],\n\n\"MongoDBTargets\": [],\n\n\"DynamoDBTargets\": [],\n\n\"CatalogTargets\": []\n\n},\n\n\"DatabaseName\": \"awscookbook703\",\n\n\"Classifiers\": [],\n\n\"RecrawlPolicy\": {\n\n\"RecrawlBehavior\": \"CRAWL_EVERYTHING\"\n\n},\n\n\"SchemaChangePolicy\": {\n\n\"UpdateBehavior\": \"UPDATE_IN_DATABASE\",\n\n\"DeleteBehavior\": \"DEPRECATE_IN_DATABASE\"\n\n},\n\n\"LineageConfiguration\": {\n\n\"CrawlerLineageSettings\": \"DISABLE\"\n\n},\n\n\"State\": \"READY\",\n\n\"CrawlElapsedTime\": 0,\n\n\"CreationTime\": \"2021-10-12T12:45:18-04:00\",\n\n\"LastUpdated\": \"2021-10-12T12:45:18-04:00\",\n\n\"LastCrawl\": {\n\n\"Status\": \"SUCCEEDED\",\n\n\"LogGroup\": \"/aws-glue/crawlers\",\n\n\"LogStream\": \"awscookbook703\",\n\n\"MessagePrefix\": \"16e867b7-e972-4ceb-b318-8e78370949d8\",\n\n\"StartTime\": \"2021-10-12T12:54:19-04:00\"\n\n},\n\n\"Version\": 1\n\n}\n\n}\n\nTIP\n\nAWS Glue crawlers log information about their runs to Amazon CloudWatch logs. If you need to debug your crawlers’\n\nactivity, you can inspect the logs in the /aws-glue/crawlers log group.\n\nIn the Glue console, select the table that was created and click “View properties.” You can also run an AWS CLI command to output the JSON:\n\naws glue get-table --database-name awscookbook703 --name data\n\nThe JSON properties should look similar to this:\n\n{\n\n\"StorageDescriptor\": {\n\n\"cols\": {\n\n\"FieldSchema\": [\n\n{\n\n\"name\": \"title\",\n\n\"type\": \"string\",\n\n\"comment\": \"\"\n\n},\n\n{\n\n\"name\": \"other titles\",\n\n\"type\": \"string\",\n\n\"comment\": \"\"\n\n},\n\n{\n\n\"name\": \"bl record id\",\n\n\"type\": \"bigint\",\n\n\"comment\": \"\"\n\n}...<SNIP>...\n\n]\n\n},\n\n\"location\": \"s3://awscookbook703-<RANDOM_STRING>/data/\",\n\n\"inputFormat\": \"org.apache.hadoop.mapred.TextInputFormat\",\n\n\"outputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\n\n\"compressed\": \"false\",\n\n\"numBuckets\": \"-1\",\n\n\"SerDeInfo\": {\n\n\"name\": \"\",\n\n\"serializationLib\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\n\n\"parameters\": {\n\n\"field.delim\": \",\"\n\n}\n\n},\n\n\"bucketCols\": [],\n\n\"sortCols\": [],\n\n\"parameters\": {\n\n\"skip.header.line.count\": \"1\",\n\n\"sizeKey\": \"43017100\",\n\n\"objectCount\": \"1\",\n\n\"UPDATED_BY_CRAWLER\": \"awscookbook703\",\n\n\"CrawlerSchemaSerializerVersion\": \"1.0\",\n\n\"recordCount\": \"79367\",\n\n\"averageRecordSize\": \"542\",\n\n\"CrawlerSchemaDeserializerVersion\": \"1.0\",\n\n\"compressionType\": \"none\",\n\n\"classification\": \"csv\",\n\n\"columnsOrdered\": \"true\",\n\n\"areColumnsQuoted\": \"false\",\n\n\"delimiter\": \",\",\n\n\"typeOfData\": \"file\"\n\n},\n\n\"SkewedInfo\": {},\n\n\"storedAsSubDirectories\": \"false\"\n\n},\n\n\"parameters\": {\n\n\"skip.header.line.count\": \"1\",\n\n\"sizeKey\": \"43017100\",\n\n\"objectCount\": \"1\",\n\n\"UPDATED_BY_CRAWLER\": \"awscookbook703\",\n\n\"CrawlerSchemaSerializerVersion\": \"1.0\",\n\n\"recordCount\": \"79367\",\n\n\"averageRecordSize\": \"542\",\n\n\"CrawlerSchemaDeserializerVersion\": \"1.0\",\n\n\"compressionType\": \"none\",\n\n\"classification\": \"csv\",\n\n\"columnsOrdered\": \"true\",\n\n\"areColumnsQuoted\": \"false\",\n\n\"delimiter\": \",\",\n\n\"typeOfData\": \"file\"\n\n}\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen you start to ingest and store large amounts of data from various sources to object storage like S3, you may want to temporarily query the data in place without loading into an intermediate database. Since you may not always know the schema or metadata of the data, you need to know some basics about it, such as where the data resides, what the files and partitioning look like, whether it’s structured versus unstructured data, the size of data, and most importantly, the schema of the data. One specific feature of the AWS Glue service, Glue crawlers, allow you to discover metadata about the variety of data you have in storage. Crawlers connect to your data source (in this case, S3 bucket), scan the objects in the source, and populate a Glue Data Catalog database with tables associated with your data’s schema.\n\nNOTE\n\nIn addition to S3 bucket sources, you can use crawlers to scan Java Database Connectivity (JDBC) data stores and\n\nDynamoDB tables. In the case of a JDBC table, you will need to define a connection to allow Glue to use a network\n\nconnection to your JDBC source.\n\nChallenge\n\nConfigure your crawler to run on an interval so that your tables and metadata are automatically updated.\n\n7.4 Querying Files on S3 Using Amazon Athena\n\nProblem\n\nYou need to run a SQL query on CSV files stored on object storage without indexing them.\n\nSolution\n\nConfigure an Amazon Athena results S3 bucket location, create a Data Catalog database and table in the Athena Editor, and run a SQL query on the data in the S3 bucket, as shown in Figure 7-15.\n\nFigure 7-15. Query files on S3 using Amazon Athena\n\nPrerequisite\n\nS3 bucket with a CSV file containing data\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nLog into the AWS Console and go to the Athena console; you should see something similar to Figure 7-16.\n\nFigure 7-16. Athena console\n\n2.\n\nIn the Query Editor, click the Settings tab and configure a Query result location using the S3 bucket that you created and a prefix: s3://bucket/folder/object/. Click Manage, select the bucket, and click Choose. As an option, you can encrypt the results. See Figure 7-17 for an example.\n\n3.\n\nFigure 7-17. Athena results destination configuration\n\nBack in the Editor tab, run the following SQL statement to create a Data Catalog database:\n\nCREATE DATABASE `awscookbook704db`\n\nYou should see output similar to Figure 7-18.\n\n4.\n\nFigure 7-18. Database creation SQL statement\n\nRun a new statement in the Query Editor to create a table within the database that references the S3 bucket location of the data and the schema of the data. Be sure to replace BUCKET_NAME with the name of the bucket that you created:\n\nCREATE EXTERNAL TABLE IF NOT EXISTS default.`awscookbook704table`(\n\n`title` string,\n\n`other titles` string,\n\n`bl record id` bigint,\n\n`type of resource` string,\n\n`content type` string,\n\n`material type` string,\n\n`bnb number` string,\n\n`isbn` string,\n\n`name` string,\n\n`dates associated with name` string,\n\n`type of name` string,\n\n`role` string,\n\n`all names` string,\n\n`series title` string,\n\n`number within series` string,\n\n`country of publication` string,",
      "page_number": 356
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 364-372)",
      "start_page": 364,
      "end_page": 372,
      "detection_method": "topic_boundary",
      "content": "`place of publication` string,\n\n`publisher` string,\n\n`date of publication` string,\n\n`edition` string,\n\n`physical description` string,\n\n`dewey classification` string,\n\n`bl shelfmark` string,\n\n`topics` string,\n\n`genre` string,\n\n`languages` string,\n\n`notes` string)\n\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n\nWITH SERDEPROPERTIES (\n\n'serialization.format' = ',',\n\n'field.delim' = ','\n\n) LOCATION 's3://BUCKET_NAME/data/'\n\nTBLPROPERTIES ('has_encrypted_data'='false');\n\nTIP\n\nYou can use an AWS Glue crawler to crawl your data on S3 and keep databases, tables, and metadata\n\nup-to-date automatically. See Recipe 7.3 for an example of configuring a Glue crawler with this\n\nexample dataset.\n\nValidation checks\n\nOpen the Query Editor and run a query to list the rows where the title is “Marvel universe”:\n\nSELECT * FROM awscookbook704table WHERE title='\"Marvel universe\"' LIMIT 100\n\nYou should see output similar to Figure 7-19.\n\nFigure 7-19. Query results in the Athena console\n\nRun a SQL query selecting the top 100 rows for the sample dataset:\n\nSELECT * FROM awscookbook704table LIMIT 100;\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou may have had to use an extract, transform, load (ETL) process in the past to facilitate querying large amounts of data on demand. This process could have taken hours, if not days, if the volume of data was substantial. In the world of big data, the volume will only increase as time moves on, and you may not know what you need to query for when you initially start to ingest the data. This makes table schema design challenging. Chances are, as you ingest and collect data from various sources, you will be storing it in object storage. This is the data lake\n\nconcept realized. Being able to query text-based data in place within your data lake is a powerful concept that you can use Amazon Athena for. You can use standard SQL to run queries against data directly on object storage (Amazon S3).\n\nAthena requires knowing some basics about your data before you can run queries. This includes metadata, schema, and data locations. The concepts of tables, databases, and the Data Catalog are important to understand so that you can configure the Athena service to meet your needs. As you saw in Recipe 7.3, the Glue service can crawl your data to discover this information and keep it up-to-date so that you can always be ready to run queries against your data while it is stored in your data lake. You can use a Glue Data Catalog in the Athena service rather than defining your own schema to save you time.\n\nTIP\n\nAs you start to increase your usage of S3 as your central data repository (or in other words, a data lake), you may need to\n\nuse several services together and also start to apply certain levels of permission to allow other team members to interact\n\nwith and manage the data. AWS Lake Formation is a managed service that brings services like S3, Glue, and Athena\n\ntogether with a robust permissions engine that can meet your needs.\n\nNOTE\n\nThe Athena service will automatically scale for you, running queries in parallel for your large datasets, so you do not have\n\nto worry about provisioning any resources. For more information, see the Athena documentation.\n\nChallenge\n\nConfigure Athena to use the Glue Data Catalog with a Glue crawler for a source dataset on S3 that you do not have a predefined schema for.\n\n7.5 Transforming Data with AWS Glue DataBrew\n\nProblem\n\nYou have data stored in a CSV and need to convert all characters in a column to uppercase before further processing can occur.\n\nSolution\n\nStart with a sample project in Glue DataBrew using a sample CSV dataset. Wait for the session to initiate and apply an uppercase format operation on the name column of the sample set. Inspect the results (see Figure 7-20).\n\nFigure 7-20. Transforming data with AWS Glue DataBrew\n\nSteps\n\n1.\n\n2.\n\nIn the AWS Console, search for and navigate to the AWS Glue DataBrew console.\n\nClick “Create sample project,” select “Popular names for babies in 2020,” create a new IAM role, enter a role suffix of your choosing, and then click “Create project,” as shown in Figure 7-21. Your session will take a few moments to be prepared, and the status indicator should look similar to Figure 7-22.\n\n3.\n\nFigure 7-21. Creating a sample project in Glue DataBrew\n\nFigure 7-22. Preparing a session in Glue DataBrew\n\nWhen your session has been prepared, click FORMAT in the menu. Then from the drop-down menu, select “Change to uppercase,” as shown in Figure 7-23.\n\n4.\n\nFigure 7-23. Beginning to format a string to uppercase\n\nIn the righthand menu, set the “Source column” to “name” and then click Apply, as shown in Figure 7-24.\n\nFigure 7-24. Formatting column to uppercase interface\n\nValidation checks\n\nView the updated name column, as shown in Figure 7-25.\n\nFigure 7-25. Results of the uppercase recipe step\n\nFrom the ACTIONS menu in the top-right corner, select Download CSV, as shown in Figure 7- 26.\n\nFigure 7-26. Downloading CSV action\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen dealing with large amounts of heterogeneous data from various sources, you will find that you need to manipulate data in various ways to meet your use case. In the past, you would have to programmatically accomplish this using scripts, complicated ETL jobs, or third-party tools. You can use AWS DataBrew to streamline the tasks of formatting, cleaning, and extracting information from your data. You can perform complex joins and splits, and run custom functions within the DataBrew service using a visual interface.\n\nDataBrew is an example of a low-code development platform (LCDP) that enables you to quickly achieve outcomes that otherwise would require knowledge of more complicated development platforms. You use Glue DataBrew in your web browser to visually design recipe jobs to process your data, preview results, and automate your data-processing workflow. Once you have a recipe job saved, you can bring automation into your workflow by setting up an S3 trigger for your incoming data to have DataBrew process it and deliver it back to S3, the Glue Data Catalog, or a JDBC data source.",
      "page_number": 364
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 373-380)",
      "start_page": 373,
      "end_page": 380,
      "detection_method": "topic_boundary",
      "content": "Challenge\n\nUpload your own dataset. Create a job from the DataBrew console, configure it to deliver the results to S3 so you can use the job on demand or automatically with a trigger.\n\nChapter 8. AI/ML\n\n8.0 Introduction Machine learning (ML) and artificial intelligence (AI) are two of the hottest topics today. Scale provided by cloud computing and improved algorithms have enabled rapid advances in the abilities of computers to think like humans (aka “provide inferences”). Many mundane and boring tasks that required human intervention can now be automated because of AI.\n\nAI and ML can get complex very quickly. Volumes of text have been written about each. Recipes in this chapter will allow you to explore some of the easy-to-implement AI services provided by AWS and get started building your own models. While you are working through the recipes, try to think about other problematic areas in society that could be well served by these technologies. From supply chain predictive maintenance to song suggestions, the opportunities are endless.\n\nWe could have written 100 pages on this topic, but these recipes are great to get started, and you can iterate from there. If you are looking to dive deeper, we suggest you check out Data Science on AWS by Chris Fregly and Antje Barth (O’Reilly, 2021).\n\nWorkstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/ArtificialIntelligence\n\n8.1 Transcribing a Podcast\n\nProblem\n\nYou need to create a text transcription of an MP3-based audio, such as a podcast.\n\nSolution\n\nUse Amazon Transcribe to generate an English language transcription and save the results to an S3 bucket (see Figure 8-1).\n\nFigure 8-1. Using Amazon Transcribe with an MP3 file\n\nPrerequisites\n\nS3 bucket\n\njq CLI JSON processor\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\n2.\n\n3.\n\nDownload a podcast in MP3 format to upload to the S3 bucket:\n\ncurl https://d1le29qyzha1u4.cloudfront.net/AWS_Podcast_Episode_453.mp3 \\\n\no podcast.mp3\n\nYou should see output similar to the following:\n\n% Total % Received % Xferd Average Speed Time Time Time Current\n\nDload Upload Total Spent Left Speed\n\n100 29.8M 100 29.8M 0 0 4613k 0 0:00:06 0:00:06 --:--:-- 5003k\n\nNOTE\n\nYou can find a list of the file formats supported by Amazon Transcribe in the documentation.\n\nCopy the downloaded podcast to your S3 bucket:\n\naws s3 cp ./podcast.mp3 s3://awscookbook801-$RANDOM_STRING\n\nYou should see output similar to the following:\n\nupload: ./podcast.mp3 to s3://awscookbook801-<<unique>>/podcast.mp3\n\nStart a Transcribe transcription job using the AWS CLI:\n\naws transcribe start-transcription-job \\\n\n--language-code 'en-US' \\\n\n--media-format 'mp3' \\\n\n--transcription-job-name 'awscookbook-801' \\\n\n--media MediaFileUri=s3://awscookbook801-${RANDOM_STRING}/podcast.mp3 \\\n\n--output-bucket-name \"awscookbook801-${RANDOM_STRING}\"\n\nYou should see output similar to the following:\n\n{\n\n\"TranscriptionJob\": {\n\n\"TranscriptionJobName\": \"awscookbook-801\",\n\n\"TranscriptionJobStatus\": \"IN_PROGRESS\",\n\n\"LanguageCode\": \"en-US\",\n\n\"MediaFormat\": \"mp3\",\n\n\"Media\": {\n\n\"MediaFileUri\": \"s3://awscookbook801-<<unique>>/podcast.mp3\"\n\n},\n\n\"StartTime\": \"2021-09-21T22:02:13.312000-04:00\",\n\n\"CreationTime\": \"2021-09-21T22:02:13.273000-04:00\"\n\n}\n\n}\n\nCheck the status of the transcription job using the AWS CLI. Wait until the Transcription Job Status is COMPLETED. This should take a few minutes:\n\naws transcribe get-transcription-job \\\n\n--transcription-job-name awscookbook-801 \\\n\n--output text \\\n\n--query TranscriptionJob.TranscriptionJobStatus\n\nValidation checks\n\nDisplay the results of the Transcribe transcription job in your terminal:\n\naws s3 cp s3://awscookbook801-$RANDOM_STRING/awscookbook-801.json - \\\n\n| jq '.results.transcripts[0].transcript' --raw-output\n\nYou should see output similar to the following:\n\nThis is episode 453 of the US podcast released on June 11, 2021 podcast confirmed. Welcome to the\n\nofficial AWS podcast. Yeah. Mhm. Hello everyone and welcome back to another episode of a W. S. Launch.\n\nI'm Nicky, I'm your host. And today I am joined by Nathan Peck\n\n...\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nTranscribing audio is a great way to allow voice recordings to be processed and analyzed in different ways at scale. Having the ability to easily transcribe audio files without gathering a large dataset and training your own AI model will save you time and open up many possibilities. Converting audio to text also allows you to easily perform natural language processing (NLP) to gain additional insights from the piece of media.\n\nHere are some things to keep in mind when you are using Transcribe:\n\nIs the language supported?\n\nDo you need to identify speakers in a recording?\n\nDo you need to support streaming audio?\n\nChallenge\n\nAutomate the process to trigger when new objects are uploaded to your S3 bucket by using EventBridge.\n\n8.2 Converting Text to Speech\n\nProblem\n\nYou need to generate audio files from text descriptions of products. This audio will be incorporated in advertisements so must be as human-like and high quality as possible.\n\nSolution\n\nYou will use the neural engine of Amazon Polly to generate MP3s from the provided text (see Figure 8-2).\n\nFigure 8-2. Text to speech with Amazon Polly\n\nSteps\n\n1.\n\nCreate an initial sound file of text that you specify:\n\naws polly synthesize-speech \\\n\n--output-format mp3 \\\n\n--voice-id Joanna \\\n\n--text 'Acme products are of the very highest quality and lowest price.' \\\n\nproducts.mp3\n\nYou should see output similar to the following:\n\n{\n\n\"ContentType\": \"audio/mpeg\",\n\n2.\n\n3.\n\n4.\n\n5.\n\n6.\n\n\"RequestCharacters\": \"63\"\n\n}\n\nListen to the MP3. Here is an example on macOS CLI:\n\nafplay products.mp3\n\nChange to the neural engine:\n\naws polly synthesize-speech \\\n\n--output-format mp3 \\\n\n--voice-id Joanna \\\n\n--engine neural \\\n\n--text 'Acme products are of the very highest quality and lowest price.' \\\n\nproducts-neural.mp3\n\nYou will see similar output as you did in step 2.\n\nListen to the MP3. Here is an example on macOS CLI:\n\nafplay products-neural.mp3\n\nAdd some SSML tags to modify the speech speed:\n\naws polly synthesize-speech \\\n\n--output-format mp3 \\\n\n--voice-id Joanna \\\n\n--engine neural \\\n\n--text-type ssml \\\n\n--text '<speak>Acme products are of the very highest quality and <prosody\n\nrate=\"slow\">lowest price</prosody></speak>' \\\n\nproducts-neural-ssml.mp3\n\nYou will see similar output as you did in steps 2 and 3.\n\nListen to the MP3. Here is an example on macOS CLI:",
      "page_number": 373
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 381-390)",
      "start_page": 381,
      "end_page": 390,
      "detection_method": "topic_boundary",
      "content": "afplay products-neural-ssml.mp3\n\nNOTE\n\nA list of SSML tags supported by Polly is shown in this article.\n\nDiscussion\n\nHaving the ability to easily generate lifelike audio from text allows for many creative uses. You no longer need voice actors to sit and rehearse recordings. You can now easily incorporate audio into your application. This can improve your customers’ experience in many ways.\n\nWhen creating audio with Polly, you should experiment with different voices and SSML tags (you might even want to create your own voice). Many of Polly’s voices are available to create Amazon Alexa skills.\n\nChallenge\n\nCreate a pronunciation lexicon for Polly to use.\n\n8.3 Computer Vision Analysis of Form Data\n\nProblem\n\nYou have a document and need to extract responses from it so that you can digitally process them.\n\nSolution\n\nYou will install and use the Textractor tool provided by AWS to utilize the forms feature of Amazon Textract. This will pull the values from the form and associate them with their keys (e.g., Name). See Figure 8-3.\n\nFigure 8-3. Analyzing a document with Amazon Textract and the textractor tool\n\nPrerequisite\n\nS3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nIn the root of the Chapter 8 repository, cd to the 803-Computer-Vision- Analysis-of-Handwritten-Form-Data/ directory and follow the subsequent steps:\n\ncd 803-Computer-Vision-Analysis-of-Handwritten-Form-Data/\n\n2.\n\nCopy the provided registration_form.png file (or your own) to the S3 bucket you created:\n\naws s3 cp ./registration_form.png s3://awscookbook803-$RANDOM_STRING\n\nYou should see output similar to the following:\n\nupload: ./registration_form.png to s3://awscookbook803-\n\n<<unique>>/registration_form.png\n\n3.\n\nAnalyze the document with Textract and redirect the output to a file:\n\naws textract analyze-document \\\n\n--document '{\"S3Object\":{\"Bucket\":\"'\"awscookbook803-${RANDOM_STRING}\"'\",\n\n\"Name\":\"registration_form.png\"}}' \\\n\n--feature-types '[\"FORMS\"]' > output.json\n\n4.\n\nGet the Textractor tool from the aws-samples repository on GitHub:\n\nwget https://github.com/aws-samples/amazon-textract-\n\ntextractor/blob/master/zip/textractor.zip?raw=true -O textractor.zip\n\n5.\n\n6.\n\n7.\n\n8.\n\n9.\n\nTIP\n\nMany great AWS samples and tools are available on GitHub. It is a great place to check often for new\n\nideas and approaches.\n\nUnzip the archive:\n\nunzip textractor.zip\n\nCreate a Python virtual environment (if you don’t already have one created):\n\ntest -d .venv || python3 -m venv .venv\n\nActivate the newly created Python virtual environment:\n\nsource .venv/bin/activate\n\nInstall the required Python modules for Textractor:\n\npip install -r requirements.txt\n\nYou should see output similar to the following:\n\nCollecting tabulate\n\nDownloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n\nInstalling collected packages: tabulate\n\nSuccessfully installed tabulate-0.8.9\n\nInstall the boto3 module:\n\npip install boto3\n\n10.\n\nUse the tool to analyze the registration form and parse the output:\n\npython textractor.py \\\n\n--documents s3://awscookbook803-${RANDOM_STRING}/registration_form.png \\\n\n--text --forms\n\nYou should see output similar to the following:\n\n************************************************************\n\nTotal input documents: 1\n\n************************************************************\n\nTextracting Document # 1: registration_form.png\n\n===================================================\n\nCalling Textract...\n\nReceived Textract response...\n\nGenerating output...\n\nTotal Pages in Document: 1\n\nregistration_form.png textracted successfully.\n\n************************************************************\n\nSuccessfully textracted documents: 1\n\n************************************************************\n\nValidation checks\n\nCheck out the form data extracted and confidence values:\n\ncat registration_form-png-page-1-forms.csv | column -t -s,\n\nYou should see output similar to the following:\n\nKey KeyConfidence Value ValueConfidence\n\nAverage Score 97.0 285 97.0\n\nName: 96.5 Elwood Blues 96.5\n\nDate 94.5 2/9/2021 94.5\n\nTeam Name: 92.0 The Blues Brothers 92.0\n\nYears of Experience 91.5 10 91.5\n\nE-mail Address: 91.0 thebluesbrothers@theawscookbook.com 91.0\n\nSignature 90.5 Elwood Blues 90.5\n\nDate 89.0 2/9/2021 89.0\n\nNumber of Team Members 81.0 2 81.0\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAutomated document processing opens up many new avenues for innovation and efficiency for organizations. Instead of having to manually interpret fields and tables in a document, tools like Textract can be used to digitize and speed up the process. Now that you are able to associate data values and fields with each other, additional processing is able to happen effectively.\n\nChallenge\n\nPrint out the blank form provided in the repo. Fill in responses by hand. Take a photo/scan of the document and analyze it with Textract.\n\n8.4 Redacting PII from Text Using Comprehend\n\nProblem\n\nYou have a document with potential personally identifiable information (PII) in it. You would like to remove the PII before more processing of the document occurs.\n\nSolution\n\nCreate sample data and store it in an S3 bucket. Launch an Amazon Comprehend job to detect and redact PII entities. Finally, view the results (see Figure 8-4).\n\nFigure 8-4. Redacting PII data from a document with Amazon Comprehend\n\nPrerequisite\n\nS3 bucket with file for analysis and path for output\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file named assume-role-policy.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n2.\n\n3.\n\n4.\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"comprehend.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate a role for the Comprehend job to use to read and write data from S3:\n\naws iam create-role --role-name AWSCookbook804Comprehend \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Role\": {\n\n\"Path\": \"/\",\n\n\"RoleName\": \"AWSCookbook804Comprehend\",\n\n\"RoleId\": \"<<RoldID>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:role/AWSCookbook804Comprehend\",\n\n\"CreateDate\": \"2021-09-22T13:12:22+00:00\",\n\n\"AssumeRolePolicyDocument\": {\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n...\n\nAttach the IAM managed policy for AmazonS3FullAccess to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook804Comprehend \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n\nCreate some sample PII data using Faker (or by hand):\n\n5.\n\n6.\n\n7.\n\npip install faker\n\nfaker -r=10 profile > sample_data.txt\n\nCopy your sample data to the bucket (file provided in the repository)\n\naws s3 cp ./sample_data.txt s3://awscookbook804-$RANDOM_STRING\n\nYou should see output similar to the following:\n\nupload: ./sample_data.txt to s3://awscookbook804-<<unique>>/sample_data.txt\n\nCreate a start-pii-entities-detection-job with Comprehend:\n\nJOB_ID=$(aws comprehend start-pii-entities-detection-job \\\n\n--input-data-config S3Uri=\"s3://awscookbook804-$RANDOM_STRING/sample_data.txt\" \\\n\n--output-data-config S3Uri=\"s3://awscookbook804-$RANDOM_STRING/redacted_output/\"\n\n\\\n\n--mode \"ONLY_REDACTION\" \\\n\n--redaction-config\n\nPiiEntityTypes=\"BANK_ACCOUNT_NUMBER\",\"BANK_ROUTING\",\"CREDIT_DEBIT_NUMBER\",\n\n\"CREDIT_DEBIT_CVV\",\n\n\"CREDIT_DEBIT_EXPIRY\",\"PIN\",\"EMAIL\",\"ADDRESS\",\"NAME\",\"PHONE\",\"SSN\",MaskMode=\"REP\n\nLACE_WITH_PII_ENTITY_TYPE\" \\\n\n--data-access-role-arn\n\n\"arn:aws:iam::${AWS_ACCOUNT_ID}:role/AWSCookbook804Comprehend\" \\\n\n--job-name \"aws cookbook 804\" \\\n\n--language-code \"en\" \\\n\n--output text --query JobId)\n\nTIP\n\nYou can alternatively use the detect-pii-entities command if you are interested in the location of PII\n\ndata in a document. This is helpful if you need to process the PII in a certain way.\n\nMonitor the job until it is COMPLETED; this will take a few minutes:\n\naws comprehend describe-pii-entities-detection-job \\\n\n--job-id $JOB_ID\n\nYou should see output similar to the following:\n\n{\n\n\"PiiEntitiesDetectionJobProperties\": {\n\n\"JobId\": \"<<hash>>\",\n\n\"JobName\": \"aws cookbook 804\",\n\n\"JobStatus\": \"COMPLETED\",\n\n\"SubmitTime\": \"2021-06-29T18:35:14.701000-04:00\",\n\n\"EndTime\": \"2021-06-29T18:43:21.200000-04:00\",\n\n\"InputDataConfig\": {\n\n\"S3Uri\": \"s3://awscookbook804-<<string>>/sample_data.txt\",\n\n\"InputFormat\": \"ONE_DOC_PER_LINE\"\n\n},\n\n\"OutputDataConfig\": {\n\n\"S3Uri\": \"s3://awscookbook804-<<string>>/redacted_output/<<Account Id>>-PII-\n\n<<hash>>/output/\"\n\n},\n\nValidation checks\n\nWhen the job is complete, get the location of the outputted data in S3:\n\nS3_LOCATION=$(aws comprehend describe-pii-entities-detection-job \\\n\n--job-id $JOB_ID --output text \\\n\n--query PiiEntitiesDetectionJobProperties.OutputDataConfig.S3Uri)\n\nGet the output file from S3:\n\naws s3 cp ${S3_LOCATION}sample_data.txt.out .\n\nYou should see output similar to the following:\n\ndownload: s3://awscookbook804-<<unique>>/redacted_output/111111111111-PII-\n\ncb5991dd58105db185a4cc1906e38411/output/sample_data.txt.out to ./sample_data.txt.out",
      "page_number": 381
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 391-399)",
      "start_page": 391,
      "end_page": 399,
      "detection_method": "topic_boundary",
      "content": "View the output:\n\ncat sample_data.txt.out\n\nYou should see output similar to the following. Notice the PII has been redacted:\n\n{'job': 'Arts development officer', 'company': 'Vance Group', 'ssn': '[SSN]', 'residence':\n\n'[ADDRESS]\\[ADDRESS]', 'current_location': (Decimal('77.6093685'), Decimal('-90.497660')),\n\n'blood_group':\n\n'O-', 'website': ['http://cook.com/', 'http://washington.biz/', 'http://owens.net/',\n\n'http://www.benson.com/'], 'username': 'rrobinson', 'name': '[NAME]', 'sex': 'M', 'address':\n\n'[ADDRESS]\\[ADDRESS]', 'mail': '[EMAIL]', 'birthdate': datetime.date(1989, 10, 27)}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nPII is closely associated with many security and compliance standards that you may come across in your career responsibilities. Generally, if you are responsible for handling PII for your customers, you need to implement security mechanisms to ensure the safety of that data. Furthermore, you may need to detect and analyze the kind of PII you store. While Amazon Macie can do this at scale within your S3 buckets or data lake, you may want to detect PII within your application to implement your own checks and workflows. For example, you may have a user fill out a form and submit it, and then detect if they have accidentally disclosed specific types of PII that you are not allowed to store, and reject the upload.\n\nYou can leverage Amazon Comprehend to detect this type of information for you. When you use Comprehend, the predefined feature detection is backed by detection models that are trained using large datasets to ensure quality results.\n\nChallenge\n\nUse Comprehend to label the type of PII rather than just redacting it. (This article provides a hint.)\n\n8.5 Detecting Text in a Video\n\nProblem\n\nYou have a video and would like to extract any text from scenes in it for analysis.\n\nSolution\n\nUpload the video file to S3 and start a text detection job in Amazon Rekognition Video (see Figure 8-5).\n\nFigure 8-5. Using Rekognition Video to detect text in an MP4\n\nPrerequisite\n\nS3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCopy the provided sample_movie.mp4 file to the S3 bucket you created:\n\naws s3 cp ./sample_movie.mp4 s3://awscookbook805-$RANDOM_STRING\n\nYou should see output similar to the following:\n\nupload: ./sample_movie.mp4 to s3://awscookbook805-utonl0/sample_movie.mp4\n\nNOTE\n\nRegarding supported video formats, the Amazon Rekognition FAQs states, “Amazon Rekognition\n\nVideo supports H.264 files in MPEG-4 (.mp4) or MOV format. If your video files use a different\n\ncodec, you can transcode them into H.264 using AWS Elemental MediaConvert.”\n\n2.\n\nBegin the text detection job:\n\nJOB_ID=$(aws rekognition start-text-detection \\\n\n--video '{\"S3Object\":\n\n{\"Bucket\":\"'\"awscookbook805-$RANDOM_STRING\"'\",\"Name\":\"sample_movie.mp4\"}}' \\\n\n--output text --query JobId)\n\nWait until JobStatus changes from IN_PROGRESS to SUCCEEDED, and then the results will be displayed:\n\naws rekognition get-text-detection \\\n\n--job-id $JOB_ID\n\nYou should see text similar to the following:\n\n{\n\n\"JobStatus\": \"SUCCEEDED\",\n\n\"VideoMetadata\": {\n\n\"Codec\": \"h264\",\n\n\"DurationMillis\": 10051,\n\n\"Format\": \"QuickTime / MOV\",\n\n\"FrameRate\": 30.046607971191406,\n\n\"FrameHeight\": 240,\n\n\"FrameWidth\": 320,\n\n\"ColorRange\": \"LIMITED\"\n\n},\n\n...\n\nValidation checks\n\nRun the command again with this query to get the DetectedText values:\n\naws rekognition get-text-detection \\\n\n--job-id $JOB_ID \\\n\n--query 'TextDetections[*].TextDetection.DetectedText'\n\nYou should see text similar to the following:\n\n[\n\n\"COPYRIGHT, 1901\",\n\n\"THOMAB A. EDISON.\",\n\n\"PATENTED AuOUST 31ST. 1897\",\n\n\"COPYRIGHT,\",\n\n\"1901\",\n\n\"THOMAB\",\n\n\"A.\",\n\n\"EDISON.\",\n\n\"PATENTED\",\n\n\"AuOUST\",\n\n\"31ST.\",\n\n\"1897\",\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou used Rekognition to detect text within a video file and saved the output to a report accessible within the Rekognition service. The output of the text detection job contains time markers for the text detected, the detected text itself, and other features. You can also detect text within images, have batch processing jobs run on large sets of files, and detect other features within videos and images. This fully managed service allows you to use reliable detection models without having to train your own.\n\nTIP\n\nRekognition supports Custom Labels, a feature of the service that allows you to train specific models to recognize\n\nparticular features in video and images specific to your needs. You can accomplish this all within the Rekognition service\n\nitself for an end-to-end implementation. For more information, see the support document.\n\nYou can integrate Rekognition directly into your application or hardware by using the AWS SDK for a reliable feature and text detection mechanism.\n\nChallenge\n\nConfigure a text detection job to automatically start when a file is uploaded to a specific S3 bucket by using EventBridge.\n\n8.6 Physician Dictation Analysis Using Amazon Transcribe Medical and Comprehend Medical\n\nProblem\n\nYou need to build a solution that recognizes medical professional dictation audio files. The solution needs to be able to categorize things like protected health information (PHI) for further analysis.\n\nSolution\n\nUse Amazon Transcribe Medical to analyze your audio file. Then, use Amazon Comprehend Medical to generate the analysis of the physician’s speech in a medical context (see Figure 8-6).\n\nFigure 8-6. Using Transcribe Medical and Comprehend Medical with physician dictations\n\nPrerequisite\n\nAn audio file with speech (human or computer-generated) in it that contains medical jargon\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\n2.\n\n3.\n\nCreate a JSON file named awscookbook806-template.json with the parameters specifying your physician’s specialty, language, S3 bucket, and dictation audio file (file provided in the repository):\n\n{\n\n\"MedicalTranscriptionJobName\": \"aws-cookbook-806\",\n\n\"LanguageCode\": \"en-US\",\n\n\"Specialty\": \"PRIMARYCARE\",\n\n\"Type\": \"DICTATION\",\n\n\"OutputBucketName\":\"awscookbook806-RANDOM_STRING\",\n\n\"Media\": {\n\n\"MediaFileUri\": \"s3://awscookbook806-RANDOM_STRING/dictation.mp3\"\n\n}\n\n}\n\nUse the sed command to replace the values in the awscookbook806- template.json file with the RANDOM_STRING value for your S3 bucket:\n\nsed -e \"s/RANDOM_STRING/${RANDOM_STRING}/g\" \\\n\nawscookbook806-template.json > awscookbook806.json\n\nStart a medical transcription job using the JSON file that you created:\n\naws transcribe start-medical-transcription-job \\\n\n--cli-input-json file://awscookbook806.json\n\nYou should see output similar to the following:\n\n{\n\n\"MedicalTranscriptionJob\": {\n\n\"MedicalTranscriptionJobName\": \"aws-cookbook-806\",\n\n\"TranscriptionJobStatus\": \"IN_PROGRESS\",\n\n\"LanguageCode\": \"en-US\",\n\n\"Media\": {\n\n\"MediaFileUri\": \"s3://awscookbook806-<<unique>>/dictation.mp3\"\n\n},\n\n\"StartTime\": \"2021-07-14T20:24:58.012000-04:00\",\n\n\"CreationTime\": \"2021-07-14T20:24:57.979000-04:00\",\n\n4.\n\n5.\n\n\"Specialty\": \"PRIMARYCARE\",\n\n\"Type\": \"DICTATION\"\n\n}\n\n}\n\nCheck the status of the Transcribe Medical job. Wait until it is COMPLETED:\n\naws transcribe get-medical-transcription-job \\\n\n--medical-transcription-job-name aws-cookbook-806 \\\n\n--output text \\\n\n--query MedicalTranscriptionJob.TranscriptionJobStatus\n\nGet the output from your previous job by downloading the file from S3:\n\naws s3 cp s3://awscookbook806-${RANDOM_STRING}/medical/aws-cookbook-806.json \\\n\n./aws-cookbook-806.json\n\nYou should see output similar to the following:\n\ndownload: s3://awscookbook806-<<unique>>/medical/aws-cookbook-806.json to ./aws-\n\ncookbook-806.json\n\nDisplay the transcribed speech from the downloaded file:\n\ncat aws-cookbook-806.json | jq .results.transcripts\n\nYou should see output similar to the following:\n\n[\n\n{\n\n\"transcript\": \"patient jane doe experiencing symptoms of headache, administered\n\n200 mg ibuprofen\n\ntwice daily.\"\n\n}\n\n]\n\nNOTE\n\nThe resulting JSON file that you downloaded contains time markers for each word that was\n\ntranscribed. You can use this information within your application to provide additional context and\n\nfunctionality.\n\nValidation checks\n\nStart an entities detection job using the Comprehend Medical detect entities API. This will show the location of things like medical conditions and PHI:\n\naws comprehendmedical detect-entities-v2 \\\n\n--text \"$(cat aws-cookbook-806.json | jq .results.transcripts[0].transcript | tr -d '\"')\"\n\nYou should see output similar to the following:\n\n{\n\n\"Entities\": [\n\n{\n\n\"Id\": 4,\n\n\"BeginOffset\": 8,\n\n\"EndOffset\": 12,\n\n\"Score\": 0.8507962226867676,\n\n\"Text\": \"jane\",\n\n\"Category\": \"PROTECTED_HEALTH_INFORMATION\",\n\n\"Type\": \"NAME\",\n\n\"Traits\": []\n\n},\n\n...\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou were able to use Amazon Transcribe Medical and Amazon Comprehend Medical to take a physician’s dictation audio file, turn it into text, and then detect the medical-context entities within the audio file. The final result provided patient data, symptoms, and medication and dosage information, which can be extremely useful in building medical applications for patients and medical professionals. Comprehend Medical also provides batch analysis and feature\n\ndetection for multiple types of PHI and can determine dictation versus patient/physician conversation. With the advent of telemedicine, these powerful features can be used to provide immediate transcriptions to patients using app-based medical services to receive healthcare from medical professionals.\n\nNOTE\n\nBoth of these services you used for the solution are HIPAA compliant. You can confidently build solutions with these\n\nservices for medical use cases that need to conform with HIPAA compliance. For more information on AWS services that\n\nconform to compliance standards, see the Services in Scope web page.\n\nChallenge\n\nUse EventBridge to automate the processing of new objects uploaded to your S3 bucket.\n\n8.7 Determining Location of Text in an Image\n\nProblem\n\nYou need to determine in which quadrant of an image the text “AWS” appears.\n\nSolution\n\nWe’ll use Textract to analyze the image from an S3 bucket and then parse the output to calculate the location of the text (see Figure 8-7).\n\nFigure 8-7. Using Textractor to analyze output from Amazon Textract\n\nPrerequisite\n\nS3 bucket",
      "page_number": 391
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 400-408)",
      "start_page": 400,
      "end_page": 408,
      "detection_method": "topic_boundary",
      "content": "Preparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCopy the provided book_cover.png file to the S3 bucket you created:\n\naws s3 cp ./book_cover.png s3://awscookbook807-$RANDOM_STRING\n\nYou should see output similar to the following:\n\nupload: ./book_cover.png to s3://awscookbook807-<<unique>>/book_cover.png\n\n2.\n\nAnalyze the file with Textract and output the results to a file called output.json:\n\naws textract analyze-document \\\n\n--document '{\"S3Object\":\n\n{\"Bucket\":\"'\"awscookbook807-$RANDOM_STRING\"'\",\"Name\":\"book_cover.png\"}}' \\\n\n--feature-types '[\"TABLES\",\"FORMS\"]' > output.json\n\nValidation checks\n\nExamine the BoundingBox values for the Practical text to find the location:\n\njq '.Blocks[] | select(.Text == \"Practical\") | select(.BlockType == \"WORD\") | .Geometry.BoundingBox'\n\noutput.json\n\nIf the left and top values are both less than 0.5, the word Practical is located in the top left of the page (see Figure 8-8).\n\nYou should see output similar to the following:\n\n{\n\n\"Width\": 0.15338942408561707,\n\n\"Height\": 0.03961481899023056,\n\n\"Left\": 0.06334125995635986,\n\n\"Top\": 0.39024031162261963\n\n}\n\nFigure 8-8. Reference BoundingBox coordinate diagram\n\nNOTE\n\nPer the Textract Developer Guide, each BoundingBox property has a value between 0 and 1. The value is a ratio of the overall image width (applies to left and width) or height (applies to height and top). For example, if the input image is 700\n\n× 200 pixels, and the top-left coordinate of the bounding box is (350,50) pixels, the API returns a left value of 0.5\n\n(350/700) and a top value of 0.25 (50/200).\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou may have used traditional optical character recognition (OCR) software in the past when you have scanned physical documents and used the OCR output to get text-based output of the content. Amazon Textract takes this a step further using ML models for even more accurate text recognition with additional feature and context information you can use in your applications. You were able to determine the location of a string of text within an image by using this recipe. You could use similar functionality in Textract to forward certain portions of text from images (or scanned forms) to different teams for human review or automating sending certain parts of forms to different microservices in your application for processing or persisting. Application developers do not need to train a model before using Textract; Textract comes pretrained with many datasets to provide highly accurate character recognition.\n\nChallenge\n\nDetermine the location of two different words in an image and calculate the distance between them.\n\nChapter 9. Account Management\n\n9.0 Introduction While the previous chapters have focused on deploying and configuring resources inside your AWS account, we want to provide some recipes that show examples of what you can do at the whole account level.\n\nAs you continue to scale your AWS usage, you will find it useful to have tools and services available to ease the burden of management, especially if you start to add additional AWS accounts to your environment. We are seeing more and more people choose to use AWS accounts as “containers” for their specific applications. Some companies provide individual accounts for production and nonproduction workloads to business units within a company; some even set up “shared services” accounts to provide internal services to business units within a company to share common resources across the many accounts they may be managing. Your AWS account spans Regions to give you global capabilities, as shown in Figure 9-1.\n\nFigure 9-1. AWS account perspective\n\nWe like to think of the tools and services that AWS provides as building blocks that you can use to customize your cloud environment to meet your specific needs. Some of these can be used at an account level to give you more management capabilities over your environments as you scale (i.e., adding additional team members or adding additional AWS accounts). You can organize and consolidate billing for many accounts by using AWS Organizations and provide centralized\n\naccess management through AWS Single Sign-On, both of which you will explore in the recipes in this chapter, in addition to some recipes to help you configure and maintain a secure environment at the account level.\n\nTIP\n\nYou should always keep an eye on the AWS Prescriptive Guidance website for the most up-to-date guidance on account-\n\nlevel capabilities and recommendations.\n\nWorkstation Configuration\n\nYou will need a few things installed to be ready for the recipes in this chapter.\n\nGeneral setup\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/AccountManagement\n\n9.1 Using EC2 Global View for Account Resource Analysis\n\nProblem\n\nYou have been presented with an AWS account for a client. You need to export a CSV file containing all provisioned compute instances, disk volumes, and network resources across all Regions within an AWS account.\n\nSolution\n\nNavigate to EC2 Global View in the AWS Console. Export a CSV of resources in your account (see Figure 9-2).\n\nPrerequisite\n\nSteps\n\n1.\n\nFigure 9-2. Generating a CSV of resources with EC2 Global View\n\nAWS account with resources deployed\n\nIn the AWS Console, search for and then click EC2 Global View, as shown in Figure 9-3.\n\nFigure 9-3. Searching for Global View\n\n2.\n\nNavigate to the “Global search” menu and click Download CSV (see Figure 9-4).\n\nFigure 9-4. Downloading a CSV from Global View\n\nValidation checks\n\nOpen the downloaded CSV file in your favorite editor on your workstation. From there, you should be able to scroll through resources in your AWS account.\n\nDiscussion\n\nYou may find yourself often working in new AWS accounts as part of your job. If you are in a lab environment or a hackathon, using Global View is a great way for you to get an idea of what an AWS account contains. Before you begin to make changes in an account, it is important to take stock of what is already deployed across all Regions. Having this knowledge will allow you to ensure that you don’t mistakenly cause any outages.\n\nWhile you should use tools like AWS Config and the AWS Billing console for inspecting the configuration of resources inside your AWS account and keeping an eye on your bill, respectively, it is good to know that you can easily generate a list of EC2 and VPC resources. Routine CSV exports at different times can be used to provide a point-in-time snapshot.\n\nChallenge\n\nUse the Global search functionality to list all default VPC security groups. You can then double- check inbound rules for extraneous permissions.\n\n9.2 Modifying Tags for Many Resources at One Time with Tag Editor\n\nProblem\n\nYou need to add a tag to multiple resources in your AWS account where it doesn’t already exist.\n\nSolution\n\nLaunch the Tag Editor in the AWS Console. Find all resources that have an Environment key with nonexisting value. Add the tag (see Figure 9-5).\n\nFigure 9-5. Adding a tag to multiple resources\n\nPrerequisite\n\nAWS account with resources deployed and tagged\n\nSteps\n\n1.\n\nIn the AWS Console, search for Tag Editor and then click Resource Groups & Tag Editor, as shown in Figure 9-6.\n\n2.\n\nFigure 9-6. Searching for Tag Editor in the AWS Console\n\nFrom the lefthand menu, click Tag Editor under the Tagging heading (see Figure 9-7).",
      "page_number": 400
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 409-417)",
      "start_page": 409,
      "end_page": 417,
      "detection_method": "topic_boundary",
      "content": "3.\n\nFigure 9-7. Launching Tag Editor\n\nSet “Resource types” to “All supported resource types,” enter a Tag key named Environment, use the drop-down menu to select “(not tagged)” for the value, and then click “Search resources” (see Figure 9-8).\n\n4.\n\nFigure 9-8. Launching Tag Editor\n\nWait a few moments for the search to complete. Then view the results, select the resources that you wish to tag, and click “Manage tags of selected resources” (see Figure 9-9).\n\n5.\n\nFigure 9-9. Viewing search results in Tag Editor\n\nEnter a “Tag key” named Environment, enter a “Tag value” Dev, and click “Review and apply tag changes” (see Figure 9-10).\n\nFigure 9-10. Tagging multiple resources with Tag Editor\n\n6.\n\nIn the pop-up window, confirm your selection by clicking “Apply changes to all selected” (see Figure 9-11).\n\nFigure 9-11. Confirming changes to apply to resources in Tag Editor\n\nValidation checks\n\nUse the CLI to confirm that all EC2 instances have an Environment tag:\n\naws ec2 describe-instances \\\n\n--output text \\\n\n--query 'Reservations[].Instances[?!not_null(Tags[?Key == `Environment`].Value)] | [].[InstanceId]'\n\nYou should see no output. You can also repeat step 3 to confirm that all resources now have an Environment tag.\n\nDiscussion\n\nA tag consists of a key and a value and can be applied to many resources at the time of creation and almost all resources after creation. You should implement your own tagging strategy as early as possible in your cloud journey. This ensures that you can identify the growing number of resources you deploy over the lifespan of your AWS account. Tags are useful for auditing, billing, scheduling updates, delegating access to resources with specific tags, and so on.\n\nCost allocation tags can help you make sense of your AWS bill, to see exactly which resources are contributing to certain portions of your bill. In addition to the cost allocation report, you can filter by tags interactively via Cost Explorer in the AWS Billing console to analyze and visualize your costs.\n\nThe Tag Editor can help you search and perform batch updates for tags across your AWS resources. Say you might have deployed many resources but forgot to tag them all, or you have historically not used tags and would like to start. You can batch select many resources within the Tag Editor (across all Regions or within a selected set of Regions) and perform these updates to ease the burden of implementing a tagging strategy.\n\nWhile not an exhaustive list, here are some good tags to include as part of a tagging baseline:\n\nCreatedBy\n\nThe User or Identity that created the resource\n\nApplication\n\nThe service or application of which the resource is a component\n\nCostCenter\n\nUseful for billing identification and to help implement a chargeback model for shared AWS account usage\n\nCreationDate\n\nThe date the resource was created\n\nContact\n\nAn email address for the team or individual in case of any issues with the resource (also helpful for configuring automated alerts)\n\nMaintenanceWindow\n\nUseful for defining a window of time that the resource is allowed to not be available in case of patching, updates, or maintenance\n\nDeletionDate\n\nUseful for development or sandbox environments so that you know when it may be safe to delete a resource\n\nChallenge\n\nApply a baseline set of tags to your AWS resources and begin to enforce the tags with a tag policy by enabling AWS Organizations.\n\n9.3 Enabling CloudTrail Logging for Your AWS Account\n\nProblem\n\nYou just set up your AWS account and want to retain an audit log of all activity for all Regions in your account.\n\nSolution\n\nConfigure an S3 bucket with a bucket policy allowing CloudTrail to write events. Enable CloudTrail for all Regions in your account and configure CloudTrail to log all audit events to the S3 bucket, as shown in Figure 9-12.\n\nFigure 9-12. Turning on logging for CloudTrail\n\nPrerequisite\n\nS3 bucket for logging\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file called cloudtrail-s3policy-template.json (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"S3CloudTrail\",\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n\n\"Action\": \"s3:GetBucketAcl\",\n\n\"Resource\": \"arn:aws:s3:::BUCKET_NAME\"\n\n},\n\n{\n\n\"Sid\": \"S3CloudTrail\",\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n\n\"Action\": \"s3:PutObject\",\n\n\"Resource\": \"arn:aws:s3:::BUCKET_NAME/AWSLogs/AWS_ACCOUNT_ID/*\",\n\n\"Condition\": {\"StringEquals\": {\"s3:x-amz-acl\": \"bucket-owner-full-control\"}}\n\n}\n\n]\n\n}\n\n2.\n\nReplace the values in the cloudtrail-s3policy-template.json file with values from your deployment. Here is a way to do it quickly with sed:\n\nsed -e \"s/BUCKET_NAME/awscookbook903-$RANDOM_STRING/g\" \\\n\ne \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\ncloudtrail-s3policy-template.json > cloudtrail-s3policy.json\n\n3.\n\nAdd the S3 bucket policy to your S3 bucket:\n\naws s3api put-bucket-policy \\\n\n--bucket awscookbook903-$RANDOM_STRING \\\n\n--policy file://cloudtrail-s3policy.json\n\n4.\n\nEnable CloudTrail for all AWS Regions and configure the S3 bucket to send logs to the following:\n\naws cloudtrail create-trail --name AWSCookbook903Trail \\\n\n--s3-bucket-name awscookbook903-$RANDOM_STRING \\\n\n--is-multi-region-trail\n\n5.\n\nStart the logging of your CloudTrail trail:\n\naws cloudtrail start-logging --name AWSCookbook903Trail\n\nNOTE\n\nMore information can be found on AWS for the required S3 bucket policy for CloudTrail logging.\n\nValidation checks\n\nDescribe the trail:\n\naws cloudtrail describe-trails --trail-name-list AWSCookbook903Trail\n\nYou should see output similar to the following:\n\n{\n\n\"trailList\": [\n\n{\n\n\"Name\": \"AWSCookbook903Trail\",\n\n\"S3BucketName\": \"awscookbook903-<<string>>\",\n\n\"IncludeGlobalServiceEvents\": true,\n\n\"IsMultiRegionTrail\": true,\n\n\"HomeRegion\": \"us-east-1\",\n\n\"TrailARN\": \"arn:aws:cloudtrail:us-east-1:<<Account Id>>:trail/AWSCookbook903Trail\",\n\n\"LogFileValidationEnabled\": false,\n\n\"HasCustomEventSelectors\": false,\n\n\"HasInsightSelectors\": false,\n\n\"IsOrganizationTrail\": false\n\n}\n\n]\n\n}\n\nGet the trail status:\n\naws cloudtrail get-trail-status --name AWSCookbook903Trail\n\nYou should see output similar to the following:\n\n{\n\n\"IsLogging\": true,\n\n\"StartLoggingTime\": \"2021-06-28T21:22:56.308000-04:00\",\n\n\"LatestDeliveryAttemptTime\": \"\",\n\n\"LatestNotificationAttemptTime\": \"\",\n\n\"LatestNotificationAttemptSucceeded\": \"\",\n\n\"LatestDeliveryAttemptSucceeded\": \"\",\n\n\"TimeLoggingStarted\": \"2021-06-29T01:22:56Z\",\n\n\"TimeLoggingStopped\": \"\"\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou can use CloudTrail to log and continuously monitor all events in your AWS account. Specifically, events refer to all API activity against the AWS APIs. These include actions that you (and all authenticated entities) take in the console, via the command line, API activity from your applications, and other AWS services performing actions (automatic actions like autoscaling, EventBridge triggers, replication, etc.).\n\nTIP",
      "page_number": 409
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 418-430)",
      "start_page": 418,
      "end_page": 430,
      "detection_method": "topic_boundary",
      "content": "You can query your logs in place on S3 using Amazon Athena if you are looking for specific events, and you can index\n\nyour logs with Amazon OpenSearch to perform complex queries against your historical data.\n\nYou should use CloudTrail from a security standpoint, but you can also use it from an application debugging standpoint. Say you have an event-driven application that triggers a Lambda function when you upload files to an S3 bucket (see Recipe 5.7). If your IAM policy is incorrect for the Lambda function invocation, you will be able to see the Deny in the CloudTrail logs. This is helpful for application developers and architects who are building and designing event-driven applications powered by Amazon EventBridge.\n\nChallenge\n\nConfigure an organizational trail if you have an AWS Organization with multiple accounts. (See Recipe 9.6 to set up an AWS Organization.)\n\n9.4 Setting Up Email Alerts for Root Login\n\nProblem\n\nYou want to be notified by email when the root user logs into an AWS account.\n\nSolution\n\nCreate an SNS topic and subscribe to it. Then create an Amazon EventBridge rule with a pattern that searches for root logins and triggers the SNS topic, as shown in Figure 9-13.\n\nFigure 9-13. Logging and alerting for root logins\n\nPrerequisite\n\nAWS account with CloudTrail enabled (see Recipe 9.3)\n\nSteps\n\n1.\n\nCreate an SNS topic:\n\nTOPIC_ARN=$(aws sns create-topic \\\n\n--name root-login-notify-topic \\\n\n--output text --query TopicArn)\n\n2.\n\nSubscribe to the SNS topic. This will send a confirmation email to the address you specify:\n\naws sns subscribe \\\n\n--topic-arn $TOPIC_ARN \\\n\n--protocol email \\\n\n--notification-endpoint your-email@example.com\n\n3.\n\nLocate the confirmation email in your inbox that AWS sent and click “Confirm subscription.”\n\nIn the root of this chapter’s repository, cd to the 904-Setting-Up-Email-Alerts-for-Root-Login directory and follow the subsequent steps.\n\n4.\n\nNow create an assume-role policy JSON statement called assume-role- policy.json to use in the next step (this file is provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"events.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\n5.\n\n6.\n\n7.\n\n8.\n\nCreate the role and specify the assume-role-policy.json file:\n\naws iam create-role --role-name AWSCookbook904RuleRole \\\n\n--assume-role-policy-document \\\n\nfile://assume-role-policy.json\n\nNow attach the managed AmazonSNSFullAccess IAM policy to the IAM role:\n\naws iam attach-role-policy \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonSNSFullAccess \\\n\n--role-name AWSCookbook904RuleRole\n\nCreate a file called event-pattern.json for AWS Console sign-in events (file provided in the repository):\n\n{\n\n\"detail-type\": [\n\n\"AWS API Call via CloudTrail\",\n\n\"AWS Console Sign In via CloudTrail\"\n\n],\n\n\"detail\": {\n\n\"userIdentity\": {\n\n\"type\": [\n\n\"Root\"\n\n]\n\n}\n\n}\n\n}\n\nCreate an EventBridge rule that monitors the trail for root login activity:\n\naws events put-rule --name \"AWSCookbook904Rule\" \\\n\n--role-arn \"arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook904RuleRole\" --event-\n\npattern file://event-\n\npattern.json\n\n9.\n\nSet the target for your EventBridge rule to your SNS topic:\n\naws events put-targets --rule AWSCookbook904Rule \\\n\n--targets \"Id\"=\"1\",\"Arn\"=\"$TOPIC_ARN\"\n\nValidation checks\n\nLog in to your AWS account using your root account, wait a few minutes, and check your email for a message from SNS.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nSetting up an alert to notify you on root user sign-ins is a detective mechanism you can layer into your security strategy to keep aware of unwelcome activity within your account. This method is a cost-effective solution to monitor your AWS account for unintended access via the root user. Since the root user is the most powerful and privileged identity (and should be used only for specific infrequent tasks), it is important to know when it is accessed.\n\nNOTE\n\nIn some cases, the root user is needed to perform specific privileged actions and tasks within an AWS account. For a list of\n\nthese actions requiring the root user, refer to the AWS support document.\n\nChallenge\n\nAdd another EventBridge rule that notifies the same SNS topic when the root password is changed (here is a hint).\n\n9.5 Setting Up Multi-Factor Authentication for a Root User\n\nProblem\n\nYou need to enable multi-factor authentication for the root user of your AWS account.\n\nSolution\n\nLog in to your AWS account with your root user credentials. Navigate to the IAM console and enable multi-factor authentication using a U2F-compatible hardware device or a Time-Based One-Time Password (TOTP)–compliant virtual device (see Figure 9-14).\n\nFigure 9-14. Enabling MFA for the root user in your AWS account\n\nSteps\n\n1.\n\nLog in to the AWS Console by using the email address associated with your root user. You can reset the password for the root user by using the “Forgot password” link displayed after you enter the root user email address; click Next. The login dialog you should see is shown in Figure 9-15.\n\n2.\n\nFigure 9-15. Selecting the root user login option\n\nTIP\n\nSince you will log in with the root user infrequently, you do not need to store the password for the user\n\nonce you enable MFA. You can reset the password each time you need to access the account; take care\n\nto secure access to the mailbox associated with your root user.\n\nOnce you are logged in to the AWS Console, select My Security Credentials from the top right of the user interface, as shown in Figure 9-16.\n\nFigure 9-16. Navigating to the My Security Credentials menu\n\n3.\n\nExpand the “Multi-factor authentication” pane on the Your Security Credentials page within the IAM console and click the Activate MFA button. Choose the type of device you will use and click Continue (see Figure 9-17).\n\nWARNING\n\nIf you use a software-based password manager utility to store your virtual MFA device information, do\n\nnot store your root user password in that same password manager. If your password manager utility or\n\nvault is compromised, your second factor and password together give the ability to access your AWS\n\naccount. Similarly, the password information for your email account should not be stored in the same\n\nplace as your virtual MFA device, since the root user password-reset procedure can be performed\n\nsuccessfully with access to the mailbox associated with your root user.\n\n4.\n\nFigure 9-17. Selecting a virtual MFA device\n\nFollow the prompts to either display a QR code that you can scan with your virtual device or plug in your hardware token and follow the prompts. Once you enter the code, you will see a window indicating that you have completed the MFA setup (see Figure 9-18).\n\nFigure 9-18. Confirmation of MFA device setup\n\nTIP\n\nYou can print out a copy of the QR code displayed and keep this in a physically secure location as a\n\nbackup in case you lose access to your virtual MFA device.\n\nValidation checks\n\nSign out of your AWS account and sign back in with the root user. Enter the code generated by your virtual device (or plug in your hardware token) and complete the login process.\n\nCleanup\n\nYou should always keep MFA enabled for your root user. If you would like to disable the MFA device associated with your root user, follow the steps for deactivating the device.\n\nDiscussion\n\nThe root user is the most powerful and privileged identity of an AWS account. The username is an email address that you configure when you first establish your AWS account, and password- reset requests can be made from the AWS Console or by contacting AWS support. If your email account is compromised, a malicious actor could request a reset of your root account password and gain access to your account. The root user should always be protected with a second factor\n\nof authentication to prevent unauthorized access to your account. Since the root user is needed on only rare occasions, you should never use it for routine tasks.\n\nTIP\n\nYou need to enable IAM user and role access to the Billing console. You can follow the AWS steps to perform that action\n\nand further reduce your dependency on using the root user for tasks.\n\nIt is extremely important to configure an IAM user (or set up federated access using ADFS with IAM, SAML with IAM, or AWS Single Sign-On) rather than use the root user to log in to your AWS account for your routine usage. AWS recommends immediately creating an administrative IAM user as one of the first things you do when you open an AWS account, shortly after you enable MFA for the root user. For a scalable approach, you can follow Recipe 9.6 to enable and use AWS Single Sign-On for your own (and delegated) access requirements.\n\nChallenge\n\nConfigure MFA protection for other IAM roles (using trust policies) in your account (here is a hint).\n\n9.6 Setting Up AWS Organizations and AWS Single Sign-On\n\nProblem\n\nYou need a scalable way to centrally manage usernames and passwords to access your AWS accounts.\n\nSolution\n\nEnable AWS Organizations, configure AWS Single Sign-On, create a group, create a permission set, and create a user in the AWS SSO directory for you to use, as shown in Figure 9-19.\n\nSteps\n\n1.\n\nFigure 9-19. AWS SSO view of permissions and users\n\nNavigate to AWS Organizations in the AWS Console and click “Create an organization.”\n\nTIP\n\nAWS Organizations provides many features that enable you to work with multiple AWS accounts at\n\nscale. For more information, see the AWS Organizations documentation.\n\n2.\n\n3.\n\n4.\n\nThe organization creation will trigger an email to be sent to the email address associated with the root user of your account. Confirm the creation of the organization by clicking the “Verify your email address” button.\n\nNavigate to the AWS SSO console in your Region and click the Enable SSO button.\n\nNOTE\n\nThe initial configuration of AWS SSO uses a local directory for your users. You can federate to an\n\nexisting user directory you may have, like Active Directory or a SAML provider. For information on\n\nconfiguring federation, the user guide for AWS SSO provides details.\n\nGo to the Groups portion of the SSO console and click “Create group.” Enter the required Group name and click Create, as shown in Figure 9-20.",
      "page_number": 418
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 431-438)",
      "start_page": 431,
      "end_page": 438,
      "detection_method": "topic_boundary",
      "content": "5.\n\nFigure 9-20. Creating a group in AWS SSO\n\nGo to the Users portion of the SSO console and select “Add user.” Enter the information required in the fields, add the user to the group you created, and select Create. You can have the AWS SSO console generate the initial password for you or send an email to the address provided in the required “Email address” field. Assign the user to the group that you created in step 4 on the second page of the “Add user” wizard (see Figure 9-21).\n\n6.\n\nFigure 9-21. Creating a user in AWS SSO\n\nGo to the AWS Accounts portion of the SSO console (in the left navigation menu of the console), choose the “Permission sets” tab, and select “Create permission set.” Choose the “Use an existing job function policy” option and select PowerUserAccess (see Figure 9-22).\n\n7.\n\nFigure 9-22. Creating a permission set in AWS SSO\n\nTo assign the permission set to the group you created for your AWS account, click the AWS accounts link in the left navigation menu, choose the “AWS organization” tab, select your AWS account from the list of accounts, and click “Assign users.” Click the Groups tab and choose the group that you added your user to in step 4, as shown in Figure 9-23. Click “Next: Permission sets.”\n\n8.\n\nFigure 9-23. Adding a group to an assignment\n\nSelect the permission set you created in step 6 and click Finish (see Figure 9- 24).\n\nFigure 9-24. Assigning a permission set in AWS SSO\n\nNOTE\n\nWhen you click Finish, members of this group can access the AWS account you specified using with\n\npermissions of the PowerUserAccess IAM policy. AWS SSO provisions a role that can be assumed in your AWS account for this purpose. Do not modify this role via IAM in your AWS account, or the\n\npermission set will not be able to be used through AWS SSO.\n\nValidation checks\n\nGo to the URL provided on the AWS SSO console dashboard page and log in with the username and password of the user that you created in step 5. Choose either “Management console” or “Command line or programmatic access” to gain PowerUserAccess to your AWS account.\n\nTIP\n\nThe default URL is generated when you first enable AWS SSO. You can create a customized URL for your own purposes\n\nby clicking the Customize link next to the default URL in the SSO console.\n\nCleanup\n\nGo to the AWS SSO console and select Settings from the left navigation menu. Scroll down to the bottom of the settings page and select “Delete AWS SSO Configuration.” Confirm the deletion by selecting the checkboxes and clicking “Delete AWS SSO.”\n\nGo to AWS Organizations, select your organization, open the settings for your organization, and select “Delete organization.” You will need to verify the organization ID and confirm the deletion of the organization from the email address associated with the root user of your AWS account.\n\nDiscussion\n\nWhen you enable AWS Organizations, the account where you initially create your organization is known as the management account. While you enabled AWS Organizations so that you could use AWS Single Sign-On in this recipe, it primarily exists for governing, organizing, and managing AWS accounts at scale. Some account management capabilities that you might be interested in include, but are not limited to, these:\n\nConsolidating billing for multiple AWS accounts\n\nManaging group accounts using organizational units (OUs)\n\nApplying service control policies (SCPs) to individual accounts or OUs\n\nCentralizing policies for tagging and backups for all accounts within your organization\n\nSharing resources across accounts using Resource Access Manager (RAM)\n\nAWS Organizations and AWS SSO are free to enable and provide a scalable way for you to begin to manage AWS accounts and user access to your entire AWS environment. As a best practice, using the management account for management functions only (and not running workloads) is a pattern you should adopt. Creating specific AWS accounts for your production and nonproduction workloads that are members of your AWS Organization helps you isolate your workloads and reduce blast radius, delegate access, manage your billing, and so on.\n\nTIP\n\nThese concepts begin to define the concept of a landing zone. Rather than build your own, you can use AWS Control\n\nTower to configure a fully managed landing zone. There are advantages of using Control Tower when you plan to scale\n\nyour AWS usage beyond just a few accounts.\n\nAWS Single Sign-On provides a secure and scalable way to manage user access to your AWS accounts. You can integrate with an external identity provider (IdP) or use the built-in directory within AWS SSO, depending on how many users you need to manage and if you already have an external user directory.\n\nWhen you successfully authenticate with AWS SSO, you are presented with a choice of access levels defined by permission sets. Permission sets use an IAM policy definition that SSO uses to manage a role, which you can assume upon successful login. AWS provides permission sets within SSO that align with common job functions, but you can also create your own custom permission sets by writing IAM policy statements and saving them as a permission set. A temporary session is created once you choose your access level into an account. You can use the session within the AWS Console, or via the command line with the temporary access key ID, secret access key, and session token variables, or by using the AWS CLI v2 to authenticate with SSO from the command line. You can adjust the length of the session duration within the AWS SSO console.\n\nSecurity around the access to your AWS environments should be your top priority, and as you scale the number of users and accounts, and level of access delegated, this becomes a challenge. AWS SSO gives you a mechanism to implement security at scale for your AWS environments. Since the session you initiate with your account via AWS SSO is temporary, you do not need to create long-lived IAM access keys to use in your command-line environment, which is one less secret to have to rotate and manage. You can also use multi-factor authentication with AWS SSO and require MFA for login.\n\nChallenge 1\n\nConnect AWS SSO to an external identity provider for an existing IdP-like Active Directory or Okta.\n\nChallenge 2\n\nApply a service control policy (SCP) to limit the Regions you can use within your AWS account.\n\nAppendix. Fast Fixes\n\nThese useful small bits of code will help you save time and get the most out of AWS.\n\nSet your AWS_ACCOUNT_ID to a bash variable:\n\nexport AWS_ACCOUNT_ID=$(aws sts get-caller-identity \\\n\n--query Account --output text)\n\nGet the most recently created CloudWatch log group name:\n\naws logs describe-log-groups --output=yaml \\\n\n--query 'reverse(sort_by(logGroups,&creationTime))[:1].{Name:logGroupName}'\n\nTail the logs for the CloudWatch group:\n\naws logs tail <<LOGGROUPNAME>> --follow --since 10s\n\nDelete all log groups that match a text pattern and prompt yes/no for confirmation:\n\naws logs describe-log-groups | \\\n\njq \".logGroups[].logGroupName\" | grep -i <<pattern>> | \\\n\nxargs -p -I % aws logs delete-log-group --log-group-name %\n\nStop all running instances for your current working Region (H/T: Curtis Rissi):\n\naws ec2 stop-instances \\\n\n--instance-ids $(aws ec2 describe-instances \\\n\n--filters \"Name=instance-state-name,Values=running\" --query \"Reservations[].Instances[].[InstanceId]\"\n\n--output text | tr '\\n' ' ')\n\nDetermine the user making CLI calls:",
      "page_number": 431
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 439-454)",
      "start_page": 439,
      "end_page": 454,
      "detection_method": "topic_boundary",
      "content": "aws sts get-caller-identity --query UserId --output text\n\nGenerate YAML input for your CLI command and use it:\n\naws ec2 create-vpc --generate-cli-skeleton yaml-input > input.yaml\n\n#Edit input.yaml - at a minimum modify CidrBlock, DryRun, ResourceType, and Tags\n\naws ec2 create-vpc --cli-input-yaml file://input.yaml\n\nList the AWS Region names and endpoints in a table format:\n\naws ec2 describe-regions --output table\n\nFind interface VPC endpoints for the Region you are currently using:\n\naws ec2 describe-vpc-endpoint-services \\\n\n--query ServiceDetails[*].ServiceName\n\nPopulate data into a DynamoDB table:\n\naws ddb put table_name '[{key1: value1}, {key2: value2}]'\n\nDetermine the current supported versions for a particular database engine (e.g., aurora- postgresql):\n\naws rds describe-db-engine-versions --engine aurora-postgresql \\\n\n--query \"DBEngineVersions[].EngineVersion\"\n\nDelete network interfaces associated with a security group and prompt for each delete (answer yes/no to delete or skip):\n\naws ec2 describe-network-interfaces \\\n\n--filters Name=group-id,Values=$SecurityGroup \\\n\n--query NetworkInterfaces[*].NetworkInterfaceId \\\n\n--output text | tr '\\t' '\\n' | xargs -p -I % \\\n\naws ec2 delete-network-interface --network-interface-id %\n\nFind your default VPC (if you have one) for a Region:\n\naws ec2 describe-vpcs --vpc-ids \\\n\n--query 'Vpcs[?IsDefault==`true`]'\n\nEnable encryption by default for new EBS volumes in a Region:\n\naws ec2 enable-ebs-encryption-by-default\n\nList all AWS Regions:\n\naws ssm get-parameters-by-path \\\n\n--path /aws/service/global-infrastructure/regions \\\n\n--output text --query Parameters[*].Name | tr \"\\t\" \"\\n\"\n\nList all AWS services:\n\naws ssm get-parameters-by-path \\\n\n--path /aws/service/global-infrastructure/services \\\n\n--output text --query Parameters[*].Name \\\n\n| tr \"\\t\" \"\\n\" | awk -F \"/\" '{ print $6 }'\n\nList all services available in a region (e.g., us-east-1):\n\naws ssm get-parameters-by-path \\\n\n--path /aws/service/global-infrastructure/regions/us-east-1/services \\\n\n--output text --query Parameters[*].Name | tr \"\\t\" \"\\n\" \\\n\n| awk -F \"/\" '{ print $8 }'\n\nList all Regions that have a particular service available (e.g., SNS):\n\naws ssm get-parameters-by-path \\\n\n--path /aws/service/global-infrastructure/services/sns/regions \\\n\n--output text --query Parameters[*].Value | tr \"\\t\" \"\\n\"\n\nCreate a presigned URL for an object in S3 that expires in a week:\n\naws s3 presign s3://<<BucketName>>/<<FileName>> \\\n\n--expires-in 604800\n\nFind Availability Zone IDs for a Region that are consistent across accounts:\n\naws ec2 describe-availability-zones --region $AWS_REGION\n\nSet the Region by grabbing the value from an EC2 instance’s metadata:\n\nexport AWS_DEFAULT_REGION=$(curl --silent http://169.254.169.254/latest/dynamic/instance-\n\nidentity/document \\\n\n| awk -F'\"' ' /region/ {print $4}')\n\nIndex\n\nA\n\nAccess Analyzer\n\nlist of services supported, Discussion using to generate IAM policy based on CloudTrail activity, Solution\n\naccess points (S3), configuring application-specific access to buckets with, Problem- Challenge access policies for common job functions, Steps creating policy for secret access, Steps\n\naccount management, Introduction-Challenge 2\n\nenabling CloudTrail logging for AWS account, Problem-Challenge modifying tags for many resources at one time with Tag Editor, Problem-Challenge setting AWS ACCOUNT ID to bash variable, Fast Fixes setting up AWS Organization and AWS Single Sign-On, Problem-Challenge 2 setting up email alerts for root login, Problem-Challenge setting up multi-factor authentication for root user in, Problem-Challenge using EC2 Global View for account resource analysis, Problem-Challenge\n\nadministrative access for routine development tasks, not recommended, Discussion administrative capabilities, delegating for IAM using permissions boundaries, Problem- Challenge administrative IAM user, Discussion AI/ML, Introduction-Challenge\n\ncomputer vision analysis of form data, Problem-Challenge converting text to speech, Problem-Challenge detecting text in a video, Problem-Challenge determining location of text in an image, Problem-Challenge physician dictation analysis, Problem-Discussion redacting PII from text using Comprehend, Problem-Challenge transcribing a podcast, Problem-Challenge\n\nALBs (see Application Load Balancers) alerts (email) for root login, setting up, Problem-Challenge Amazon Certificate Manager (ACM), Discussion Amazon Cloud Watch (see Cloud Watch) Amazon Comprehend, using to redact PII from text, Problem Amazon ECS (see ECS)\n\nAmazon Elastic Cloud Compute (see EC2) Amazon Elastic Kubernetes Service (Amazon EKS), Introduction\n\n(see also EKS)\n\nAmazon Lightsail (see Lightsail, deploying containers with) Amazon Machine Image (AMI), AWS Backup handling process of building, Discussion Amazon Managed Service for Prometheus, Introduction Amazon OpenSearch, Discussion Amazon Polly, converting text to speech, Problem-Challenge Amazon Rekognition Video, using to detect text in a video, Problem-Challenge Amazon Textract and textractor tool\n\ndetermining location of text in an image, Problem-Challenge using for computer vision analysis of form data, Solution-Challenge\n\nAmazon Transcribe Medical and Amazon Comprehend Medical, using to analyze physician dictation, Problem-Discussion Amazon Transcribe, using with MP3 file, Solution-Challenge AmazonDynamoDBFullAccess policy, Validation checks, Steps AmazonEC2ReadOnlyAccess policy, Steps AmazonS3ReadOnlyAccess policy, Steps AmazonSSMManagedInstanceCore policy, Solution, Steps Application Load Balancers (ALBs)\n\nconfiguring ALB to invoke Lambda function, Problem-Challenge 2 creating new ALB target group to use as Green target with CodeDeploy, Steps redirecting HTTP traffic to HTTPS with, Problem-Challenge\n\nArchive Access storage tier, Discussion ARN (Amazon Resource Names)\n\nfinding for ECS task, Validation checks retrieving ARN for user, Steps\n\nartificial intelligence (see AI/ML) AssumeRole API, Validation checks AssumeRole policy, Validation checks assuming a role, Validation checks Athena service\n\nquerying files on S3 with, Problem-Challenge querying logs in place on S3 with, Discussion\n\naudio, transcribing to text, Problem-Challenge auditing session activity, Discussion Aurora Serverless\n\ncreating PostgreSQL database, Problem-Challenge enabling REST access to, using RDS Data API, Problem-Challenge\n\nauthentication\n\nIAM, using with RDS database, Problem-Challenge\n\nmulti-factor, Discussion\n\nrequiring in AssumeRole policies, Discussion setting up for root user, Problem-Challenge\n\nauthorization tokens, Steps AutoPause, Steps Availability Zones (AZs), Discussion\n\nENI in, Discussion finding AZ IDs for Region that is consistent across accounts, Fast Fixes NAT gateways in, Discussion subnets in a VPC, spreading across AZs, Discussion\n\navailable state for VPC, verifying, Validation checks AWS ACCOUNT ID, setting to bash variable, Fast Fixes AWS account management (see account management) AWS Backup, Discussion\n\ncreating and restoring EC2 backups to another Region using, Problem-Challenge\n\nAWS Backup Managed Policy, Steps AWS CLI\n\nautomating process of token retrieval, Discussion configuring credentials for, Validation checks determining user making calls, Fast Fixes generating Yaml input for command, Fast Fixes installing Lightsail Control plugin for, Prerequisite starting Transcribe transcription job, Steps\n\nAWS Cloud Map, Introduction AWS CodeDeploy, using to orchestrate application deployments to ECS, Problem- Challenge AWS Config, Discussion AWS Control Tower, Discussion AWS Copilot, deploying containers with, Problem-Challenge AWS Database Migration Service (AWS DMS), Discussion AWS Glue crawlers, automatically discovering metadata with, Problem-Challenge AWS KMS CMK, configuring S3 buckets to use keys referencing, Solution AWS Lake Formation, Discussion AWS Organizations, Introduction AWS Organizations and Single Sign-On, setting up, Problem-Challenge 2 AWS Policy Generator, Discussion AWS Schema Conversion Tool, Discussion AWS SDK, Discussion\n\nusing specific access points with, Steps\n\nAWS Security Token Service (see Security Token Service)\n\nAWS security topics, resources for, Introduction AWS services, listing all, Fast Fixes AWS Signer, using to run trusted code in Lambda function, Problem-Problem AWS Single Sign-On, Solution, Introduction, Discussion\n\nsetting up AWS Organizations and, Problem-Challenge 2\n\nAWS SSM Session Manager\n\nconnecting to EC2 instance in subnet of VPC via, Validation checks connecting to EC2 instances with, Problem-Challenge\n\nAWS Systems Manager (SSM) API, Discussion AWS-managed (aws/ebs) KMS keys, Discussion AWSLambdaBasicExecutionRole policy, IAM role for Lambda function execution, Steps AWSLambdaVPCAccess IAM policy, Steps\n\nB\n\nbackups\n\nautomated in Aurora Serverless, Discussion creating and restoring EC2 backups to another Region using AWS Backup, Problem- Challenge\n\nbig data, Introduction-Challenge\n\nautomatically discovering metadata with AWS Glue crawlers, Problem-Challenge querying files on S3 using Amazon Athena, Problem-Challenge streaming data to S3 using Kinesis Data Firehose, Problem-Challenge transforming data with AWS Glue DataBrew, Problem-Challenge using Kinesis Stream for ingestion of streaming data, Problem-Challenge workstation configuration, Workstation Configuration\n\nBlock Public Access feature (S3), Solution blue/green deployments, updating containers with, Problem-Challenge Border Gateway Protocol (BGP), Discussion bucket policy template to enforce encryption on all objects, Steps\n\nC\n\ncapacity for databases, Steps\n\nAurora Serverless scaling measured in capacity units, Discussion automatically scaling capacity targets, Steps autoscaling DynamoDB table provisioned capacity, Problem-Challenge checking for RDS cluster, Validation checks DynamoDB provisioned capacity , use of capacity units, Steps scaled down to 0, Steps setting at upper limit, Discussion\n\nwhen database activity resumes, Steps\n\nCDNs (content delivery networks), Discussion certificate authority (CA), RDS Root CA, Validation checks certificates\n\nCloudFront HTTPS certificate on default hostname , Discussion creating for HTTPS, Steps SSL, Discussion\n\nCI/CD pipelines\n\nAWS Copilot commands embedded in, Discussion blue/green deployments pattern, Discussion\n\nCIDR (classless inter-domain routing) blocks, Solution choosing carefully for your VPC, Discussion creating VPC with IPv6 CIDR block, Discussion nonoverlapping ranges when connecting VPCs, Discussion quotas for IPv4 and IPv6, Discussion simplifying management of CIDRs in security groups with prefix lists, Problem- Challenge specifying CIDR notation for authorizations, Discussion\n\nCLI (see AWS CLI) Cloud Map, Introduction CloudFront, Discussion\n\nserving web content securely from S3 with, Problem-Challenge\n\nCloudTrail logging, Prerequisite\n\nconfiguring to log events on S3 bucket, Steps enabling, Steps enabling for your AWS account, Problem-Challenge\n\nCloudWatch, Steps\n\nconfiguring alarm and scaling policy for ECS service, Solution getting most recently created log group name, Fast Fixes Glue crawlers logging information to, Validation checks metrics service, Discussion streaming container logs to, Solution-Challenge tailing log group to observe Lambda function invoked, Validation checks tailing logs for CloudWatch group, Fast Fixes\n\nCloudWatch Events (see EventBridge) CloudWatchFullAccess policy, Validation checks clusters\n\ncreating database cluster with engine mode of serverless, Steps database, checking capacity of, Validation checks deleting RDS cluster, Cleanup\n\nCMK (see customer-managed KMS key) code signing configuration, Steps CodeDeploy, using to orchestrate application deployments to ECS, Solution-Challenge CodeDeployRoleForECS policy, Steps CodeSigningPolicies, changing, Challenge 2 Common Vulnerabilities Scoring System (CVSS), Discussion Comprehend, using to redact PII from text, Problem-Challenge computer vision analysis of form data, Problem-Challenge concurrency (provisioned), reducing Lambda startup times with, Problem-Challenge 2 connection pooling, leveraging RDS Proxy for database connections from Lambda, Problem-Challenge consumers and producers (streaming data), Discussion containers, Introduction-Challenge\n\nautoscaling container workloads on ECS, Problem-Challenge building, tagging and pushing container image to Amazon ECR, Problem-Challenge capturing logs from containers running on ECS, Problem-Challenge deploying using Amazon Lightsail, Problem-Challenge deploying using AWS Copilot, Problem-Challenge launching Fargate container task in response to an event, Problem-Challenge networked together, Introduction orchestrators, Introduction packaging Lambda code in container image, Problem prerequisite, installing Docker, Docker installation and validation scanning images for security vulnerabilities on push to ECR, Problem-Challenge 2 updating with blue/green deployments, Problem-Challenge\n\nControl Tower, Discussion Copilot, deploying containers with, Problem-Challenge cost allocation tags, Discussion CreateInternetGateway action (EC2), testing, Validation checks credentials\n\ntemporary, for a role, returned by AssumeRole API, Validation checks temporary, from AWS STS instead of IAM user, Discussion\n\ncross-account access to AWS resources, Discussion Cross-Region Replication (CRR), Discussion CSV files\n\ngenerating and exporting CSV of resources with EC2 Global View, Problem-Challenge transforming data in, using Glue DataBrew, Problem-Challenge\n\ncustomer-managed KMS key, Solution, Problem, Discussion CVSS (Common Vulnerability Scoring System), Discussion\n\nD\n\ndashboard, creating for S3 Storage Lens, Steps-Discussion data, Introduction\n\n(see also big data)\n\nData Catalog\n\ncreating in Athena, Steps using Glue Data Catalog in Athena service, Discussion\n\ndata lakes, Discussion Data Lifecycle Manager, Discussion\n\nautomating EBS snapshots with, Discussion\n\ndatabase engines, determining current supported versions for, Fast Fixes Database Migration Service (see DMS) databases, Introduction-Challenge, Discussion\n\nautomating password rotation for RDS databases, Problem-Challenge autoscaling DynamoDB table provisioned capacity, Problem-Challenge AWS managed database services, Introduction creating Amazon Aurora serverless PostgreSQL database, Problem-Challenge enabling REST access to Aurora Serverless using RDS Data API, Problem-Challenge encrypting storage of existing RDS for MySQL database, Problem-Challenge leveraging RDS Proxy for database connections from Lambda, Problem-Challenge migrating to Amazon RDS using DMS, Problem-Challenge using IAM authentication with RDS database, Problem-Challenge\n\nDataSync, replicating data between EFS and S3 with, Problem-Challenge 2 Days in Transition action, Validation checks db-connect action, Discussion DBAppFunction Lambda function’s role, Steps DBInstanceStatus, Steps DBProxyTargets, Steps Deep Archive Access storage tier, Discussion deploying containers\n\nupdating containers with blue/green deployments, Problem-Challenge using Amazon Lightsail, Problem-Challenge using AWS Copilot, Problem-Challenge\n\nDescribeInstances action (EC2), testing, Validation checks Destination S3 bucket, Steps DHCP server (AWS-managed), options set, Discussion DMS (Database Migration Service), Discussion\n\nmigrating databases to Amazon RDS using DMS, Problem-Challenge\n\ndms.t2.medium replication instance size, Steps Docker\n\ncreating a Dockerfile, Steps installation and validation, Docker installation and validation\n\nDocker container image, pushing to ECR repository, Problem-Challenge Docker Desktop, Docker installation and validation Docker Linux Engine, Docker installation and validation Docker Swarm, Introduction DynamoDB\n\nautomating CSV import from S3 with Lambda, Problem-Challenge 3 autoscaling DynamoDB table provisioned capacity, Problem-Challenge populating data into a table, Fast Fixes using Glue crawlers to scan tables, Discussion\n\nE\n\nEBS snapshots, Discussion\n\nrestoring a file from, Problem-Challenge\n\nEBS volumes\n\nenabling encryption by default for new volumes in Region, Fast Fixes encrypting using KMS keys, Problem-Challenge 2\n\nebs-encryption-by-default option, Discussion EC2, Introduction\n\nconnecting to instances using AWS SSM Session Manager, Problem-Challenge creating and restoring EC2 backups to another Region using AWS Backup, Problem- Challenge granting instance the ability to access a secret, Steps instance in subnet of a VPC, connecting to the internet, Problem retrieving secret from Secrets Manager, Validation checks storing, encrypting, and accessing passwords with Secrets Manager, Problem- Challenge using EC2 Global View for account resource analysis, Problem-Challenge\n\nECR (Elastic Container Registry), Introduction\n\ncontainer images supported, Discussion deleting repositories containing images, Cleanup Docker Credential Helper, Steps pushing container image to, Solution-Challenge, Problem-Challenge scanning container images for vulnerabilities on push to ECR, Problem-Challenge 2 storage of container images on, Discussion\n\nECS (Elastic Container Service), Introduction\n\nautoscaling container workloads on, Problem-Challenge capturing logs from containers running on, Problem-Challenge Copilot requirement for ECS service-linked role, Steps\n\nusing EventBridge to trigger launch of container tasks on Fargate, Solution-Challenge\n\nECSRunTaskPermissionsForEvents policy, Steps EFS\n\nconfiguring Lambda function to access, Problem-See Also replicating data between EFS and S3 with DataSync, Problem-Challenge 2\n\nEIP (Elastic IP address), Steps\n\nassociated with NAT gatway, Discussion creating and associating to EC2 instance, Solution creating for use with NAT gateway, Steps\n\nElastic Cloud Compute (see EC2) Elastic Container Registry (see ECR) Elastic Container Service (Amazon ECS) (see ECS) Elastic Load Balancing (ELB) service, Introduction\n\ngiving permission to invoke Lambda functions, Solution\n\nelastic network interfaces (see ENIs) ElastiCache, accessing cluster having endpoint on a VPC, Problem-Challenge 1 email alerts for root login, setting up, Problem-Challenge encryption\n\nenabling by default for new EBS volumes in a Region, Fast Fixes encrypting EBS volumes using KMS keys, Problem-Challenge 2 encrypting storage of existing RDS for MySQL database, Problem-Challenge SSL encryption in transit, support by RDS Proxy, Discussion traffic in transit and at rest using TLS with DataSync, Discussion using S3 bucket keys with KMS to encrypt objects, Problem-Challenge\n\nendpoint policies (S3 VPC), Steps engine-mode of serverless, Steps ENIs (elastic network interfaces)\n\ndatabase subnet groups simplifying palcement of RDS ENIs, Steps of two EC2 instances, security group associated with, Problem security group acting as stateful virtual firewall for, Discussion subnets used for placement of, Discussion\n\nEvalDecision, Validation checks event-driven applications, Discussion, Discussion EventBridge\n\ninvoking a Lambda function on a schedule, Solution-Challenge rule searching for root logins and triggering SNS topic, Solution-Challenge using to trigger launch of ECS container tasks on Fargate, Solution-Challenge\n\nevents in your AWS account, monitoring with CloudTrail, Discussion\n\nF\n\nFargate, Prerequisites, Introduction\n\nlaunching Fargate container task in response to an event, Problem-Challenge\n\nfast fixes, Fast Fixes-Fast Fixes federation\n\nAWS accounts leveraging, Discussion identity, Solution\n\nform data, computer vision analysis of, Problem-Challenge Frequent Access storage tier, Discussion full-load-and-cdc, Discussion\n\nG\n\ngateway endpoint in your VPC, creating and associating with route tables, Steps Gateway Load Balancers, Discussion Gateway VPC endpoints, Discussion get-secret-value API call, Discussion Glacier archive (S3), automating archival of S3 objects to, Solution global condition context keys (IAM), Steps Glue service\n\nautomatically discovering metadata with Glue crawlers, Problem-Challenge\n\ncrawler configuration summary, Steps creating Glue Data Catalog database, Steps\n\ntransforming data with Glue DataBrew, Problem-Challenge\n\ngroups (IAM), Steps\n\nH\n\nHIPAA compliance for PHI, Discussion HTTP\n\nHTTP 301 response, Steps security group rules allowing HTTP traffic, Steps\n\nHTTPS\n\nredirecting HTTP traffic to with application load balancer, Problem-Challenge Session Manager, communicating with AWS Systems Manager (SSM) via, Discussion\n\nI\n\nIAM (Identity and Access Management)\n\naccess keys, AWS SSO and, Discussion access point policies, Steps, Discussion creating and assuming role for developer access, Problem-Challenge creating policy for secret access, Steps, Steps\n\ncreating role for Kinesis Data Firehose, Steps creating role for RDS Proxy, Steps delegating administrative capabilities using permissions boundaries, Problem- Challenge enabling IAM user and role access to Billing console, Discussion generating least privilege IAM policy based on access patterns, Problem-Challenge role allowing S3 to copy objects from source to destination bucket, Steps role for Lambda function execution, IAM role for Lambda function execution service-linked roles, Steps setting up multi-factor authentication for root user in AWS account, Steps testing policies with IAM Policy Simulator, Problem-Challenge user password policies, enforcing in AWS account, Enforcing IAM User Password Policies in Your AWS Account-Challenge using IAM authentication with RDS database, Problem-Challenge\n\nIAM Access Analyzer (see Access Analyzer) identity federation, using to access AWS accounts, Solution IGW (see internet gateway) Infrequent Access storage class, Solution, Discussion, Discussion instance metadata (EC2), Steps instance profiles (EC2), Steps\n\ncreating and associating profile allowing access to secret, Solution\n\nIntelligent-Tiering archive policies, using to automatically archive S3 objects, Problem- Challenge 2 internet gateway\n\negress-only, for private subnets on VPC with IPv6 capability, Discussion using to connect VPC to the internet, Problem-Challenge\n\ninternet, using NAT gateway for outbound access from private subnets, Problem-Challenge IP addresses\n\nAWS-provided ranges list, Solution Elastic IP address in IPv4, Steps for ENIs, Discussion option to auto-assign on newly launched EC2 instances in a subnet, Discussion retrieving public IP from EC2 instance's metadata, Validation checks\n\nIPv4 CIDR blocks, Steps\n\nadditional, associating with your VPC, Discussion quota for, Discussion\n\nIPv6 CIDR blocks\n\nconfiguring for Amazon VPC , Discussion quota, Discussion\n\nJ\n\nJava Database Connectivity (JDBC) data stores, using Glue crawlers to scan, Discussion jq utility, Steps, Prerequisites\n\nK\n\nKey Management Service (KMS), Introduction\n\ncreating KMS key to encrypt database snapshot, Steps encrypting EBS volumes using KMS keys, Problem-Challenge 2 specifying key for encyrypting RDS database snapshot, Steps using with S3 bucket keys to encrypt objects, Problem-Challenge\n\nkey rotation, automatic, on KMS service, Discussion Kinesis Client Library (KCL), Discussion Kinesis Data Analytics, Discussion Kinesis Data Firehose, streaming data to S3 with, Problem-Challenge Kinesis Producer Library (KPL), Discussion Kinesis Stream, using for ingestion of streaming data, Problem-Challenge KMS (see Key Management Service) KMS.NotFoundException error, Validation checks Kubernetes, Introduction\n\nL\n\nLambda functions\n\naccessing VPC resources with, Problem-Challenge 1 configuring application load balancer to invoke, Problem-Challenge 2 configuring to access EFS file system, Problem-See Also connection to RDS database, leveraging RDS Proxy for, Problem-Challenge IAM role for execution, IAM role for Lambda function execution integrating function with Secrets Manager to rotate RDS database passwords, Problem-Challenge invoking on a schedule, Problem-Challenge packaging Lambda code in container image, Problem-Challenge packaging libraries with Lambda Layers, Problem-Challenge reducing startup times with provisioned concurrency, Problem running trusted code in, using AWS Signer, Problem-Problem time out after 900 seconds, Discussion transforming data with, Discussion using EventBridge instead of for long-running jobs, Discussion\n\nlanding zone, Discussion least privilege access\n\nimplementing based on access patterns, Problem-Challenge\n\nprinciple of least privilege, Discussion\n\nlibraries, packaging with Lambda Layers, Problem-Challenge lifecycle policies (S3), using to reduce storage costs, Problem-Challenge 2 Lightsail, deploying containers with, Problem-Challenge Linux, installing Docker on, Linux load balancers\n\nredirecting HTTP traffic to HTTPS with application load balancer, Problem-Challenge types other than ALB offered by AWS, Discussion\n\nlogging\n\ncapturing logs from containers running on ECS, Problem-Challenge CloudTrail, enabling for your account, Prerequisite deleting all log groups matching text pattern, Fast Fixes enabling CloudTrail logging for AWS account, Problem-Challenge Glue crawlers automatically logging information to CloudWatch Logs, Validation checks\n\nlogin profile, creating for a user, Steps low-code development platforms (LCDPs), Discussion\n\nM\n\nmachine learning (ML) (see AI/ML) MacOS, installing Docke Desktop on, MacOS management account, Discussion metadata, Discussion\n\nautomatically discovering with AWS Glue crawlers, Problem-Challenge EC2 instance, retrieving public IP from, Validation checks EC2 instances, Steps\n\nmetrics\n\nautoscaling metrics on CloudWatch, Discussion ECS service, on AWS Console, Validation checks observing for S3 storage using Storage Lens, Discussion\n\nmigration of databases\n\nmigrating databases to Amazon RDS using DMS, Problem-Challenge provisioned capacity type on RDS to Aurora Serverless, Discussion\n\nmounting and unmounting EBS volumes, Validation checks MP3-based audio, transcribing to text, Problem-Challenge multi-factor authentication (MFA), Discussion, Discussion\n\nsetting up for root user in your AWS account, Problem-Challenge\n\nMySQL\n\nRDS instance, encrypting storage of, Solution-Challenge RDS instance, leveraging RDS Proxy for database connections from Lambda, Problem",
      "page_number": 439
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 455-482)",
      "start_page": 455,
      "end_page": 482,
      "detection_method": "topic_boundary",
      "content": "RDS instance, using IAM authentication with, Problem-Challenge\n\nN\n\nNAT gateway\n\nsharing of, enabled by Transit Gateway, Steps using for outbound internet access from private subnets, Problem-Challenge\n\nNcat utility, Validation checks network insights path, creating for EC2 instances, Steps Network Load Balancers, Discussion networking, Introduction-Challenge\n\nAWS services providing, Introduction connecting VPC to the internet using internet gateway, Problem-Challenge controlling network access to S3 from VPC using VPC endpoints, Problem-Challenge creating network tier with subnets and route table in a VPC, Problem-Challenge defining private virtual network in the cloud with Amazon VPC, Problem-Challenge deleting network interfaces associated with a security group, Fast Fixes enabling transitive cross-VPC connections using Transit Gateway, Problem-Challenge granting dynamic access by referencing security groups, Problem-Challenge innovations at AWS, resources on, Introduction peering VPCs together for inter-VPC network communication, Problem-Challenge redirecting HTTP traffic to HTTPS with application load balancer, Problem-Challenge simplifying management of CIDRs in security groups with prefix lists, Problem- Challenge using NAT gateway for outbound internet access from private subnets, Problem- Challenge using VPC Reachability Analyzer to verify and troubleshoot network paths, Problem- Challenge\n\nNGINX containers\n\ndeploying using Amazon Lightsail, Solution-Challenge nginx:latest image, Steps\n\nnotifications (S3), Steps\n\nO\n\nOpenSearch, Discussion OpenSSL CLI, generating self-signed certificate with, Steps orchestrators (container), Introduction Organizations (AWS), setting up, Introduction, Problem-Challenge 2 origin access identity (OAI)\n\nconfiguring to require S3bucket to be accessible only from CloudFront, Solution creating for CloudFront to reference S3 bucket polity, Steps\n\nOS X CLI, listening to MP3 file on, Steps\n\nP\n\npasswords\n\nautomating rotation for RDS databases, Problem-Challenge complex, generating with Secrets Manager, Steps enforcing IAM user password policies in AWS account, Enforcing IAM User Password Policies in Your AWS Account-Challenge storing, encrypting, and accessing with Secrets Manager, Problem-Challenge\n\npeering VPCs for inter-VPC network communication, Problem-Challenge permissions\n\nlocking down for SSM users, Discussion permission sets, Discussion\n\npermissions boundaries, using to delegate IAM administrative capabilities, Problem- Challenge PHI (protected health information), categorizing for further analysis, Problem-Discussion physician dictation analysis using Transcribe Medical and Comprehend Medical, Problem- Discussion PII (personally identifiable information), redacting from text using Comprehend, Problem- Challenge Policy Simulator (IAM), testing IAM policies with, Problem-Challenge Polly service, converting text to speech, Problem-Challenge PostgreSQL\n\nAurora Serverless database, allowing REST access using RDS Data API, Problem- Challenge creating Amazon Aurora serverless PostgreSQL database, Problem-Challenge\n\nPostgreSQL package, installing, Validation checks PowerUserAccess IAM policy, Solution, Discussion\n\nattaching to role, Steps\n\nprefix lists in security groups, managing CIDRs with, Problem-Challenge principle of least privilege, Discussion, Discussion\n\n(see also least privilege access)\n\nprivilege escalation, IAM service mitigating risk of, Steps producers and consumers (streaming data), Discussion Prometheus, Amazon Managed Service for, Introduction protected health information (PHI), categorizing for further analysis, Problem-Discussion provisioned capacity for DynamoDB table, autoscaling, Problem-Challenge provisioned concurrency, reducing Lambda startup times with, Problem-Challenge 2 Proxy ID, Steps public access, blocking for S3 bucket, Problem-Challenge\n\nQ\n\nQuery Editor in RDS Console, Steps, Discussion quotas and limits (Kinesis service), Discussion\n\nR\n\nRDS Data API\n\nshort-lived database connections, Discussion using to enable REST access to Aurora Serverless, Problem-Challenge\n\nrds-data:CommitTransaction permission, Discussion rds-data:RollbackTransaction permission, Discussion Reachability Analyzer (VPC), using to verify and troubleshoot network paths, Problem- Challenge ReadCapacityUnits scaling target, Steps recovery point (backup), Steps recovery point objectives, replicating S3 buckets to meet, Problem-Challenge 2 recovery time objective (RTO), decreasing for EC2 instance using AWS Backups, Discussion redirect response for all HTTP traffic to HTTPS, Steps Redis Python package, installing, Steps Redis, Lambda function with Redis client, accessing VPC resources, Solution-Challenge 1 Regions\n\ncreating and restoring EC2 backups to another Region using AWS Backup, Problem- Challenge creating VPC in, spreading subnets across Availability Zones, Discussion listing all, Fast Fixes listing all that have a particular service, Fast Fixes listing Region names and endpoints in table, Fast Fixes setting by grabbing value from EC2 instance metadata, Fast Fixes stopping all running instances for current working Region, Fast Fixes for VPCs, Discussion\n\nRekognition Video, using to detect text in a video, Problem Relational Database Service (RDS), Introduction\n\nautomating password rotation for RDS databases, Problem-Challenge deleting RDS cluster, Cleanup leveraging RDS Proxy for database connections from Lambda, Problem-Challenge migrating databases to, using DMS, Problem-Challenge naming constraints, Steps using IAM authentication with RDS database, Problem-Challenge\n\nreplication\n\nAurora Serverless databases, Discussion DMS replication tasks, Solution-Challenge replicating data between EFS and S3 with DataSync, Problem-Challenge 2 replicating S3 buckets to meet recovery point objectives, Problem-Challenge 2\n\nReplicationStatus, Validation checks Resource Access Manager, Discussion REST\n\nenabling REST access to Aurora Serverless using RDS Data API, Problem-Challenge REST API exposed by RDS, Discussion\n\nrestores\n\nrestoring a file from an EBS snapshot, Problem restoring EC2 backups to another Region using AWS Backup, Steps\n\nroles\n\ncreating, Steps retrieving for a user, Steps\n\nroot user\n\nemail alerts for root login, setting up, Problem-Challenge setting up multi-factor authentication for, Problem-Challenge\n\nrotate-secret command (Secrets Manager), Steps Route 53 DNS records, Discussion route tables\n\nassociated with each subnet, route to direct traffic for peered VPCs to peering connection, Steps associating gateway endpoint in VPC with, Steps creating network tier with subnets and route table in a VPC, Problem-Challenge prefix lists associated wtih, Discussion priority given to most specific route, Discussion\n\nrouting, Solution\n\n(see also CIDR blocks) for Transit Gateway, Steps\n\nruntime interface client, Discussion\n\nS\n\nS3\n\nautomatically archiving S3 objects using Intelligent-Tiering, Problem-Challenge 1 AWS support for S3 interface endpoints, Discussion blocking public access for a bucket, Problem-Challenge configuring application-specific access to buckets with S3 access points, Problem- Challenge controlling network access to S3 from VPC using VPC endpoints, Problem-Challenge\n\ncreating presigned URL for object that expires in a week, Fast Fixes CSV import into DynamoDB from S3 with Lambda, Problem-Challenge 3 lifecycle policies, using to reduce storage costs, Problem-Challenge 2 observing storage and access metrics using Storage Lens, Problem-Challenge 2 querying files using Amazon Athena, Problem-Challenge replicating buckets to meet recovery point objectives, Problem-Challenge 2 replicating files from S3 to EFS using DataSync, Problem-Challenge 2 serving web content securely from using CloudFront, Problem-Challenge streaming data to, using Kinesis Data Firehose, Problem-Challenge using bucket keys with KMS to encrypt objects, Problem-Challenge\n\nS3:GetObject action, Solution S3:PutObject action, Solution Same-Region Replication (SRR), Discussion scaling\n\nautoscaling by Kinesis Data Firehose, Discussion autoscaling capacity for database, Steps autoscaling container workloads on ECS, Problem-Challenge autoscaling DynamoDB table provisioned capacity, Problem-Challenge\n\nSchema Conversion Tool (SCT), Discussion schemas, Discussion secrets\n\ncreating using AWS CLI, Steps SECRET_ARN role, replacing, Steps storing, encrypting, and accessing passwords with Secrets Manager, Problem- Challenge\n\nSecrets Manager\n\ncreating and storing passwork in, Solution generating passwords with, Steps using to generate complex password, Steps using with Lambda function to automatically rotate RDS database passwords, Solution-Challenge\n\nSecretsManagerReadWrite policy, Steps, Steps security, Introduction-Challenge\n\nadministrative access, Discussion blocking public access for S3 bucket, Problem-Challenge encryption at rest, Discussion endpoint policy to restrict access to S3 buckets, Discussion fine-grained security capabilities on AWS, Introduction IAM role, creating and assuming for developer access, Problem-Challenge scanning container images for vulnerabilities on push to ECR, Problem-Challenge 2 security topics on AWS, Introduction\n\nserving web content securely from S3 with CloudFront, Problem-Challenge trusted code, running in Lambda, Discussion workstation configuration, Workstation Configuration\n\nsecurity groups\n\nCIDR management using prefix lists, Problem-Challenge creating VPC security group for database, Steps deleting network interfaces associated with, Fast Fixes EC2 instances's security group allowing ingress traffic from ALB, Steps granting dynamic access to EC2 instances by referencing, Problem-Challenge for RDS Proxy, Steps\n\nrule allowing access on TCP port 3306 for Lambda APP function security group, Steps\n\nRDS MySQL database instance, ingress rule allowing access on TCP port 3306, Steps referencing in peered VPCs, Discussion referencing other security groups, Steps rules to allow HTTP and HTTPS traffic, Steps updating rule to allow access SSH access between instances, Steps\n\nSecurity Token Service (STS), Validation checks, Discussion self-referencing rule (security group), Steps serverless, Introduction-Challenge 1\n\naccessing VPC resources with Lambda, Problem-Challenge 1 automating CSV import into DynamoDB from S3 with Lambda, Problem-Challenge 3 benefits of, on AWS, Introduction configuring ALB to invoke Lambda function, Problem-Challenge 2 configuring Lambda function to access EFS, Problem-See Also packaging Lambda code in container image, Problem-Challenge packaging libraries with Lambda Layers, Problem-Challenge prerequisite, IAM role for Lambda function execution, IAM role for Lambda function execution reducing Lambda startup times with provisioned concurrency, Problem-Challenge 2 running trusted code in Lambda using AWS Signer, Problem-Problem services available on AWS, Introduction\n\nSession Manager (see SSM Session Manager) shards, Steps signing process for Lambda function code, Solution Single Sign-On (SSO), Solution, Introduction, Discussion\n\nsetting up AWS Organizations and SSO, Problem-Challenge 2\n\nsnapshots\n\nencrypting storage of RDS database using, Solution-Challenge skipping final snapshot when deleting RDS cluster, Cleanup\n\nSNS topic, creating and subscribing to it, Solution-Challenge Source S3 bucket, configuring replication policy for, Steps SQL query, running on files stored in S3, Problem-Challenge SSH, allowing between EC2 instances, Problem-Challenge SSL\n\ncertificate, creating, Steps OpenSSL, Prerequisites\n\nSSM Session Manager\n\nconnecting to EC2 instance in subnet of VPC via, Validation checks connecting to EC2 instances with, Problem-Challenge\n\nSSML (Speech Synthesis Markup Language) tags, Steps SSO (see Single Sign-On) Standard storage class, Discussion storage, Introduction-Challenge 2\n\ncreating and restoring EC2 backups to another Region using AWS Backup, Problem- Challenge ElastiCache service implementing redis or memcached for, Discussion encrypting storage of existing RDS for MySQL database, Problem-Challenge restoring a file from an EBS snapshot, Problem-Challenge using Intelligent-Tiering archive policies to automatically archive S3 objects, Problem- Challenge 2 using S3 lifecycle policies to reduce storage costs, Problem-Challenge 2\n\nStorage Lens, observing S3 storage and access metrics with, Problem-Challenge 2 streaming data\n\nto S3 using Kinesis Data Firehose, Problem-Challenge using Kinesis Stream for ingestion of, Problem-Challenge\n\nsubnets\n\ncreating database subnet group, Steps creating network tier with subnets and route table in a VPC, Problem-Challenge outbound-only internet access for an EC2 instance in private subnets, Problem- Challenge\n\nSwitch Role feature, Validation checks\n\nT\n\ntables, Discussion Tag Editor, using to modify tags for many resources at one time, Problem-Challenge tagging containers, Steps, Discussion tags for AWS resources, Discussion target groups for Load Balancer, Steps text in an image, determining location of, Problem-Challenge\n\ntext to speech conversion, Problem-Challenge text, detecting in a video, Problem Textractor tool\n\nanalyzing output from Amazon Textract to determine location of text in an image, Problem-Challenge using for computer vision analysis of form data, Problem-Challenge\n\ntokens (authentication), Discussion\n\nIAM policy allowing Lambda function to generate, Steps\n\ntokens (authorization), Steps Transcribe Medical and Comprehend Medical, using for physician dictation analysis, Problem-Discussion transcribing a podcast, Problem-Challenge transformations\n\nLambda functions for, Discussion transforming data with AWS Glue DataBrew, Problem-Challenge\n\nTransit Gateway, enabling transitive cross-VPC connections, Problem-Challenge\n\nU\n\nuniversally unique identifiers (UUIDs), Validation checks user password policies, enforcing in AWS account, Enforcing IAM User Password Policies in Your AWS Account-Challenge\n\nV\n\nvideo, detecting text in, Problem-Challenge VPC (Virtual Private Cloud), Introduction\n\naccessing VPC resources with Lambda, Problem-Challenge 1 connecting to internet using internet gateway, Problem-Challenge controlling network access to S3 from VPC using VPC endpoints, Problem-Challenge creating, Problem-Challenge creating network tier with subnets and route table in a VPC, Problem default, finding for a Region, Fast Fixes enabling transitive cross-VPC connections using Transit Gateway, Problem-Challenge finding interface VPC endpoints for current Region, Fast Fixes peering two VPCs together for inter-VPC network communication, Problem-Challenge Reachability Analyzer, using to verify and troubleshoot network paths, Problem- Challenge using NAT gateway for outbound internet access from private subnets, Problem- Challenge\n\nVPN Endpoints for Session Manager, Discussion\n\nW\n\nWindows, installing Docker Desktop on, Windows WorkSpaces gateways, list of CIDR ranges for, Solution\n\nY\n\nYaml input for CLI command, Fast Fixes\n\nAbout the Authors\n\nJohn Culkin is a senior solutions architect at AWS. He holds all current AWS certifications. Previously, he was a principal cloud architect lead at Cloudreach, where he led the delivery of cloud solutions aligned with organizational needs. A lifelong student of technology, he now focuses on creating transformative business solutions that utilize cloud services.\n\nMike Zazon is a senior cloud architect at AWS, focused on helping enterprise customers modernize their businesses. He previously held roles as a cloud software developer, software engineer, software architect, IT manager, and data center architect. His passion for technology education blossomed while serving in some of these roles within an engineering research university setting.\n\nColophon\n\nThe animal on the cover of AWS Cookbook is the northern goshawk (Accipiter gentilis). A powerful predator, the northern goshawk belongs to the Accipitridae family (part of the “true hawk” subfamily) and can be found in the temperate parts of the Northern Hemisphere. The northern goshawk is the only species in the genus Accipiter living in coniferous and mixed forests in both Eurasia and North America, generally restricting itself to relatively open wooded areas or along the edges of a forest. It’s a migratory bird that ventures south during the winter.\n\nThe northern goshawk has relatively short, broad wings and a long tail to enable maneuverability within its forest habitat. For its species, it has a comparatively sizable beak, robust and fairly short legs, and thick talons. It has a blue-gray back and a brownish-gray or white underside with dark barring. These birds tend to show clinal variation in color, which means that goshawks further north are paler than those in warmer areas.\n\nThe northern goshawk is amazingly fast, and catches its prey by putting on short bursts of flight, often twisting among branches and crashing through thickets in its intensity. It’s a quiet predator that hunts by perching at mid-level heights and attacking quickly when it spots prey. The diet often consists of medium-sized birds, small mammals including squirrels and rabbits, small rodents, snakes, and insects.\n\nThis hawk lays around 2–4 bluish white eggs and is very territorial and protective of its nest, diving at intruders (including humans) and sometimes drawing blood. Young goshawks are usually looked after by the female, who remains with them most of the time while the male’s primary responsibility is to bring food to the nest. Goshawks typically mate for life and hence they function as a partnership with most things.\n\nGoshawks have a long and noble history. In the Middle Ages, only the nobility were permitted to fly goshawks for falconry. Ancient European falconry literature refers to goshawks as a yeoman’s bird or the cook’s bird because of their utility as a hunting partner catching edible prey. Currently, the northern goshawk’s conservation status is that of least concern as the population remains stable. However, it may falter with increased deforestation, which is a loss of habitat for these mighty birds. Many of the animals on O’Reilly covers are endangered; all of them are important to the world.\n\nThe cover illustration is by Karen Montgomery, based on a black and white engraving from British Birds. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n\nTable of Contents\n\nForeword Preface\n\nWho This Book Is For What You Will Learn The Recipes What You Will Need Getting Started Conventions Used in This Book Using Code Examples O’Reilly Online Learning How to Contact Us Acknowledgments\n\n1. Security\n\n1.0. Introduction 1.1. Creating and Assuming an IAM Role for Developer Access 1.2. Generating a Least Privilege IAM Policy Based on Access Patterns 1.3. Enforcing IAM User Password Policies in Your AWS Account 1.4. Testing IAM Policies with the IAM Policy Simulator 1.5. Delegating IAM Administrative Capabilities Using Permissions Boundaries 1.6. Connecting to EC2 Instances Using AWS SSM Session Manager 1.7. Encrypting EBS Volumes Using KMS Keys 1.8. Storing, Encrypting, and Accessing Passwords Using Secrets Manager 1.9. Blocking Public Access for an S3 Bucket 1.10. Serving Web Content Securely from S3 with CloudFront\n\n2. Networking\n\n2.0. Introduction 2.1. Defining Your Private Virtual Network in the Cloud by Creating an Amazon VPC 2.2. Creating a Network Tier with Subnets and a Route Table in a VPC 2.3. Connecting Your VPC to the Internet Using an Internet Gateway 2.4. Using a NAT Gateway for Outbound Internet Access from Private Subnets 2.5. Granting Dynamic Access by Referencing Security Groups 2.6. Using VPC Reachability Analyzer to Verify and Troubleshoot Network Paths 2.7. Redirecting HTTP Traffic to HTTPS with an Application Load Balancer 2.8. Simplifying Management of CIDRs in Security Groups with Prefix Lists 2.9. Controlling Network Access to S3 from Your VPC Using VPC Endpoints 2.10. Enabling Transitive Cross-VPC Connections Using Transit Gateway 2.11. Peering Two VPCs Together for Inter-VPC Network Communication\n\n3. Storage\n\n3.0. Introduction 3.1. Using S3 Lifecycle Policies to Reduce Storage Costs 3.2. Using S3 Intelligent-Tiering Archive Policies to Automatically Archive S3 Objects 3.3. Replicating S3 Buckets to Meet Recovery Point Objectives 3.4. Observing S3 Storage and Access Metrics Using Storage Lens 3.5. Configuring Application-Specific Access to S3 Buckets with S3 Access Points\n\n3.6. Using Amazon S3 Bucket Keys with KMS to Encrypt Objects 3.7. Creating and Restoring EC2 Backups to Another Region Using AWS Backup 3.8. Restoring a File from an EBS Snapshot 3.9. Replicating Data Between EFS and S3 with DataSync\n\n4. Databases\n\n4.0. Introduction 4.1. Creating an Amazon Aurora Serverless PostgreSQL Database 4.2. Using IAM Authentication with an RDS Database 4.3. Leveraging RDS Proxy for Database Connections from Lambda 4.4. Encrypting the Storage of an Existing Amazon RDS for MySQL Database 4.5. Automating Password Rotation for RDS Databases 4.6. Autoscaling DynamoDB Table Provisioned Capacity 4.7. Migrating Databases to Amazon RDS Using AWS DMS 4.8. Enabling REST Access to Aurora Serverless Using RDS Data API\n\n5. Serverless\n\n5.0. Introduction 5.1. Configuring an ALB to Invoke a Lambda Function 5.2. Packaging Libraries with Lambda Layers 5.3. Invoking Lambda Functions on a Schedule 5.4. Configuring a Lambda Function to Access an EFS File System 5.5. Running Trusted Code in Lambda Using AWS Signer 5.6. Packaging Lambda Code in a Container Image 5.7. Automating CSV Import into DynamoDB from S3 with Lambda 5.8. Reducing Lambda Startup Times with Provisioned Concurrency 5.9. Accessing VPC Resources with Lambda\n\n6. Containers\n\n6.0. Introduction 6.1. Building, Tagging, and Pushing a Container Image to Amazon ECR 6.2. Scanning Images for Security Vulnerabilities on Push to Amazon ECR 6.3. Deploying a Container Using Amazon Lightsail 6.4. Deploying Containers Using AWS Copilot 6.5. Updating Containers with Blue/Green Deployments 6.6. Autoscaling Container Workloads on Amazon ECS 6.7. Launching a Fargate Container Task in Response to an Event 6.8. Capturing Logs from Containers Running on Amazon ECS\n\n7. Big Data\n\n7.0. Introduction 7.1. Using a Kinesis Stream for Ingestion of Streaming Data 7.2. Streaming Data to Amazon S3 Using Amazon Kinesis Data Firehose 7.3. Automatically Discovering Metadata with AWS Glue Crawlers 7.4. Querying Files on S3 Using Amazon Athena 7.5. Transforming Data with AWS Glue DataBrew\n\n8. AI/ML\n\n8.0. Introduction 8.1. Transcribing a Podcast 8.2. Converting Text to Speech 8.3. Computer Vision Analysis of Form Data\n\n8.4. Redacting PII from Text Using Comprehend 8.5. Detecting Text in a Video 8.6. Physician Dictation Analysis Using Amazon Transcribe Medical and Comprehend Medical 8.7. Determining Location of Text in an Image\n\n9. Account Management\n\n9.0. Introduction 9.1. Using EC2 Global View for Account Resource Analysis 9.2. Modifying Tags for Many Resources at One Time with Tag Editor 9.3. Enabling CloudTrail Logging for Your AWS Account 9.4. Setting Up Email Alerts for Root Login 9.5. Setting Up Multi-Factor Authentication for a Root User 9.6. Setting Up AWS Organizations and AWS Single Sign-On\n\nFast Fixes Index About the Authors",
      "page_number": 455
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 483-488)",
      "start_page": 483,
      "end_page": 488,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 483
    }
  ],
  "pages": [
    {
      "page_number": 3,
      "content": "Praise for AWS Cookbook\n\nThere’s something sorely missing from official AWS documentation: a sense of reality. Most of us aren’t attempting to win points for collecting as many AWS services as we can; we’re trying to complete a task somewhere that isn’t a whiteboard. AWS Cookbook speaks to real users with a collection of recipes and pragmatic examples we can all benefit from.\n\nCorey Quinn, Chief Cloud Economist, The Duckbill Group\n\nInside you’ll find a great deal of information on typical AWS use cases plus a reference implementation that’s easy to follow. If you like to learn AWS concepts in a practice-driven, example-based, hands-on manner, I highly recommend this book.\n\nGaurav Raje, author of Security and Microservice Architecture on AWS\n\nI’ve never read a book packed so densely with ninja level tips and tricks for AWS; it’s the book I wish I had five years ago. If you use AWS day to day, you need this in your toolkit, not only for the things it contains but also for the inspiration it provides. In my view, it’s the best AWS book there is.\n\nAdrian Cantrill, AWS Trainer, learn.cantrill.io\n\nPutting AWS into practice with hands-on experience is the difference between cloud literacy and cloud fluency. AWS Cookbook serves up practical scenarios for working in the cloud to help individuals level up their career.\n\nDrew Firment, AWS Community Hero and Head Enterprise Strategist, Pluralsight",
      "content_length": 1408,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "AWS Cookbook Recipes for Success on AWS\n\nJohn Culkin and Mike Zazon",
      "content_length": 67,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "AWS Cookbook\n\nby John Culkin and Mike Zazon\n\nCopyright © 2022 Culkins Coffee Shop LLC and Mike Zazon. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nAcquisitions Editor: Jennifer Pollock Development Editor: Virginia Wilson Production Editor: Christopher Faucher Copyeditor: nSight, Inc. Proofreader: Sharon Wilkey Indexer: Ellen Troutman-Zaig Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Kate Dullea\n\nDecember 2021: First Edition\n\nRevision History for the First Edition\n\n2021-12-02: First Release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492092605 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. AWS Cookbook, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n\nThe views expressed in this work are those of the authors and do not represent the publisher’s views. While the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n\n978-1-492-09260-5\n\n[LSI]",
      "content_length": 1965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Dedication\n\nDedicated to my father, who taught me that a spreadsheet could be used for much more than totaling up columns.\n\n—John\n\nDedicated to my aunt, Judy Dunn. Thank you for the Tandy 1000 PC that sparked my fascination with computer programming and technology.\n\n—Mike",
      "content_length": 272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Foreword\n\nAs part of the Amazon Web Services (AWS) team since the beginning, I have been able to watch it grow in scale, richness, and complexity from a unique vantage point. Even after writing thousands of blog posts and millions of words, I learn something new and useful about AWS just about every day.\n\nWith well over two hundred services in production and more launching regularly, AWS could easily leave you feeling overwhelmed. In addition to tens of thousands of pages of official AWS documentation, bloggers, AWS Heroes, AWS Partners, and others have created innumerable pieces of content—including blog posts, videos, webinars, overviews, and code samples.\n\nWhile there’s no substitute for having a full and complete understanding of a particular AWS service, the reality is that you often simply need to solve a “point” problem. Even after you understand a service, remembering how to use it to solve that problem can be a challenge—at least it is for me.\n\nAnd that is where this cookbook comes in. Because of its broad selection of topics and carefully chosen recipes, I am confident that you will be able to quickly find one that addresses your immediate need and to put it into practice in short order. You can solve your problem, refresh your knowledge of that aspect of AWS, and move forward to create value for your customers!\n\nMy favorite aspect of this book is that it does not hand-wave past any of the details. Each recipe assumes that you start fresh and then helps you to cook up a perfectly seasoned solution. Nothing is left to chance, and you can use the recipes as is in most cases. The recipes also cover the all-important cleanup phase and ensure that you leave your AWS environment as you found it.\n\nWhere appropriate, the recipes use the AWS Cloud Development Kit (CDK) and include all of the necessary “moving parts.” The CDK provides a double benefit; in addition to helping you to move forward more quickly, these CDK elements can help you learn more about how to put infrastructure as code (IaC) into practice.\n\nMost cookbooks are designed to be browsed and savored, and this one is no exception. Flip through it, read an entire chapter, or use just a recipe or two, as you wish. I also recommend that you go through all of Chapter 1, just to make sure that your environment is set up and ready to go. Then, when you are presented with a problem to solve, find the appropriate recipe, put it into practice, and reap the benefits.\n\nJeff Barr\n\nVP and Chief Evangelist at AWS\n\nSeattle, WA\n\nNovember 2021",
      "content_length": 2535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Preface\n\nThe vast majority of workloads will go to the cloud. We’re just at the beginning—there’s so much more to happen.\n\nAndy Jassy\n\nCloud usage has been gaining traction with enterprises and small businesses over the last decade and continues to accelerate. Gartner said the worldwide infrastructure as a service (IaaS) public cloud services market grew 40.7% in 2020. The rapid growth of the cloud has led to a huge demand for cloud skills by many organizations. Many IT professionals understand the basic concepts of the cloud but want to become more comfortable working in the cloud. This gap between the supply and demand of cloud skills presents a significant opportunity for individuals to level up their career.\n\nThrough our combined 20+ years of cloud experience, we have had the benefit of working on Amazon Web Services (AWS) projects in many different roles. We have provided guidance to hundreds of developers on how and when to use AWS services. This has allowed us to understand the common challenges and easy wins of the cloud. We would like to share these lessons with you and give you a leg up for your own advancement. We wrote this book to share some of our knowledge and enable you to quickly acquire useful skills for working in the cloud. We hope that you will find yourself using this book as reference material for many years to come.",
      "content_length": 1361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Who This Book Is For This book is for developers, engineers, and architects of all levels, from beginner to expert. Beginners will learn cloud concepts and become comfortable working with cloud services. Experts will be able to examine code used to stand up recipe foundations, explore new services, and gain additional perspectives. If the plethora of cloud services and combinations seem overwhelming to you, then this book is for you. The recipes in this book aim to provide “Hello, World” proofs of concept and components of enterprise-grade applications. This will be accomplished using common use cases with guided walk-throughs of scenarios that you can directly apply to your current or future work. These curated and experience-building recipes are meant to demystify services and will immediately deliver value, regardless of your AWS experience level.\n\nWhat You Will Learn In addition to opening up new career opportunities, being able to harness the power of AWS will give you the ability to create powerful systems and applications that solve many interesting and demanding problems in our world today. Would you like to handle 60,000 cyber threats per second using AWS machine learning like Siemens does? Or reduce your organization’s on- premises footprint and expand its use of microservices like Capital One has? If so, the practical examples in this book will help expedite your learning by providing tangible examples showing how you can put the building blocks of AWS together to form practical solutions that address common scenarios. The on-demand consumption model, vast capacity, advanced capabilities, and global footprint of the cloud create new possibilities that need to be explored.\n\nThe Recipes We break the book into chapters that focus on general areas of technology (e.g., security, networking, artificial intelligence, etc.). The recipes contained within the chapters are bite-sized, self-contained, and easily consumable. Recipes vary in length and complexity. Each recipe has a problem statement, solution (with diagram), and discussion. Problem statements are tightly defined to avoid confusion. Solutions contain required preparation and steps to walk you through the work needed to accomplish the goal. When appropriate, explicit validation checks will be provided. We’ve also added extra challenges to the recipes to help you advance your learning if you wish to do so. Finally, we end each recipe with a short discussion to help you understand the solution and why it matters, suggestions to extend the solution, and ways to utilize it for real impact.\n\nNOTE\n\nTo keep your AWS bill low and keep your account tidy, each recipe has cleanup steps provided in the repositories\n\nassociated with the book.",
      "content_length": 2740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Each chapter has its own repository at https://github.com/awscookbook. The repository contains preparation steps for easy copying and pasting, required files, and infrastructure as code. We have also created GitHub templates for reporting bugs and suggesting new recipes. We encourage you to leverage GitHub to submit issues, create requests for new recipes, and submit your own pull requests. We will actively maintain the chapter repositories with updates for recipe steps and code in the README files of each recipe. Be sure to check these for any new or alternative approaches. We look forward to interacting with you on GitHub with new fun challenges and hints to assist you.\n\nSome recipes are “built from scratch,” and others include preparation steps to allow you to interact with common scenarios seen in the real world. We have provided code to enable you to easily deploy the prerequisites. For example, Recipe 6.5, assumes that you are a container developer creating an application deployment that requires an existing network stack. When prerequisites exist, they can be “pre-baked” with preparation steps using code provided in the repositories. When substantial preparation for a recipe is needed, you will use the AWS Cloud Development Kit (CDK), which is a fantastic tool for intelligently defining and declaring infrastructure. The majority of the recipes are CLI based; when appropriate, we use console walk-throughs including screenshots or descriptive text.\n\nNOTE\n\nThere are many ways to achieve similar outcomes on AWS; this book will not be an exhaustive list. Many factors will\n\ndictate the best overall solution for your use case. We have selected recipe topics to help you learn about AWS and make\n\nthe best choices for your specific needs.\n\nYou’ll find recipes for things like the following:\n\nRedacting personally identifiable information (PII) from text by using Amazon Comprehend\n\nAutomating password rotation for Amazon Relational Database Service (RDS) databases\n\nUsing VPC Reachability Analyzer to verify and troubleshoot network paths\n\nAlong with the recipes, we also provide short lines of code in the Appendix that will quickly accomplish valuable and routine tasks. We feel that these are great tidbits to add to your cloud toolbox.\n\nWARNING\n\nAWS has a free tier, but implementing recipes in this book could incur costs. We provide cleanup instructions, but you are\n\nresponsible for any costs in your account. We recommend checking out the Well-Architected Labs developed by AWS on\n\nexpenditure awareness and leveraging AWS Budgets actions to control costs.",
      "content_length": 2592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "What You Will Need Here are the requirements to get started and some tips on where to find assistance:\n\nAWS account\n\nSetup instructions\n\nAn IAM user with console and programmatic access\n\nAdministrator privileges for your IAM user\n\nPersonal computer/laptop\n\nSoftware\n\nWeb browser (e.g., Microsoft Edge, Google Chrome, or Mozilla Firefox)\n\nTerminal with bash or Z shell (Zsh)\n\nGit\n\nInstall instructions\n\nHomebrew (optional but recommended to install other requirements)\n\nInstall instructions\n\nCode editor (e.g., VSCodium or AWS Cloud9)\n\nRecommended install: brew install --cask vscodium\n\nAWS CLI version 2 (2.1.26 or later)\n\nInstall guide\n\nRecommended install: brew install awscli@2\n\nPython 3.7.9 (and pip) or later\n\nExample install: brew install python@3.7\n\nAWS Cloud Development Kit version 2.0 or later\n\nGetting started guide\n\nRecommended install: brew install npm and npm i -g aws-cdk@next\n\nRecommended: Create a folder in your home directory called AWSCookbook. This will allow you to clone each chapter’s repository in one place:\n\nAWSCookbook:$ tree -L 1\n\n.\n\n├── AccountManagement\n\n├── ArtificialIntelligence\n\n├── BigData\n\n...",
      "content_length": 1130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "NOTE\n\nAt the time of publishing, the AWS CDK has two versions: version 1 and version 2 (developer preview). The code we\n\nhave provided is written for version 2. You can find out more information about how to migrate to and install CDK\n\nversion 2 in this AWS CDK v2 article.\n\nGetting Started This section provides examples of techniques and approaches we perform throughout the book to make the recipe steps easier to follow. You can skip over these topics if you feel comfortable with them. You can always come back and reference this section.\n\nSetups\n\nIn addition to the installation of the prerequisites listed previously, you will need the following access.\n\nAWS account setup\n\nYou will need a user with administrative permissions. Some of the recipes require the ability to create AWS Identity and Access Management (IAM) resources. You can follow the AWS guide for creating your first IAM admin user and user group.\n\nGeneral workstation setup steps for CLI recipes\n\nWe have created a group of code repositories available at https://github.com/awscookbook. Create a folder called AWSCookbook in your home directory (or any place of your choosing) and cd there:\n\nmkdir ~/AWSCookbook && cd ~/AWSCookbook\n\nThis will give you a place to check out chapter repositories (e.g., Security):\n\ngit clone https://github.com/AWSCookbook/Security\n\nSet and export your default Region in your terminal:\n\nexport AWS_REGION=us-east-1\n\nTIP",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "AWS offers many Regions across the world for cloud deployments. We’ll be using the us-east-1 Region for simplicity. As\n\nlong as the services are available, there is no reason these recipes won’t work in other Regions. AWS has a list of Regions\n\nand services.\n\nSet your AWS ACCOUNT_ID by parsing output from the aws sts get-caller-identity operation:\n\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity \\\n\n--query Account --output text)\n\nNOTE\n\nThe aws sts get-caller-identity operation “returns details about the IAM user or role whose credentials are used to call the operation.”\n\nValidate AWS Command Line Interface (AWS CLI) setup and access:\n\naws ec2 describe-instances\n\nIf you don’t have any EC2 instances deployed, you should see output similar to the following:\n\n{\n\n\"Reservations\": []\n\n}\n\nNOTE\n\nAWS CLI version 2 will by default send command output with multiple lines to less in your terminal. You can type q to exit. If you want to override this behavior, you can modify your ~/.aws/config file to remove this default functionality.\n\nTIP\n\nAWS CloudShell is a browser-based terminal that you can use to quickly create a terminal environment in your\n\nauthenticated AWS Console session to run AWS CLI commands from. By default, it uses the identity of your browser\n\nsession to interact with the AWS APIs. Many of the recipes can be run using CloudShell. You can use CloudShell to run",
      "content_length": 1382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "recipe steps, clean up commands, and other AWS CLI commands as your authenticated user, if you do not want to create a\n\nsession that you use in your own local terminal environment on your workstation.\n\nTechniques and Approaches Used in This Book\n\nThe next few sections will explain and give examples of some ways of using the CLI to help you with recipes.\n\nQuerying outputs, environment variables, and command substitution\n\nSometimes when subsequent commands depend on outputs from the command you are currently running. The AWS CLI provides the ability for client-side filtering of output. At times, we will set environment variables that contain these outputs by leveraging command substitution.\n\nWe’ll combine these three techniques to make things easier for you as you proceed through steps in the book. Here is an example:\n\nUse the AWS Security Token Service (AWS STS) to retrieve your IAM user (or role) Amazon Resource Name (ARN) with the AWS CLI:\n\naws sts get-caller-identity\n\nYou should see output similar to the following:\n\n{\n\n\"UserId\": \"EXAMPLE\",\n\n\"Account\": \"111111111111\",\n\n\"Arn\": \"arn:aws:iam::111111111111:user/UserName\"\n\n}\n\nAn example of querying for the ARN value and outputting it to the terminal follows:\n\naws sts get-caller-identity --query Arn --output text\n\nYou should see output similar to the following:\n\narn:aws:iam::111111111111:user/UserName\n\nQuery for the ARN value and set it as an environment variable using command substitution:",
      "content_length": 1459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "PRINCIPAL_ARN=$(aws sts get-caller-identity --query Arn --output text)\n\nTo check the value of an environment variable, for example, you can echo it to the terminal:\n\necho $PRINCIPAL_ARN\n\nYou should see output similar to the following:\n\narn:aws:iam::111111111111:user/UserName\n\nTIP\n\nUsing the --dry-run flag is always a good idea when performing an operation that makes changes—for example, aws ec2 create-vpc --dry-run --cidr-block 10.10.0.0/16.\n\nReplacing values in provided template files\n\nWhere possible, to simplify the learning experience for you, we have provided template files in the chapter code repositories that you can use as a starting point as input to some of the commands you will run in recipe steps. For example, when you create an AWS CodeDeploy configuration in Recipe 6.5, we provide codedeploy-template.json with AWS_ACCOUNT_ID, PROD_LISTENER_ARN, and TEST_LISTENER_ARN placeholders in the JSON file. We expect you to replace these placeholder values and save the file as codedeploy.json.\n\nTo further simplify your experience, if you follow the steps exactly and save these to environment variables, you can use the sed command to replace the values. Where possible, we provide you a command to do this, such as this example from Chapter 6: Use the sed command to replace the values with the environment variables you exported with the helper.py script:\n\nsed -e \"s/AWS_ACCOUNT_ID/${AWS_ACCOUNT_ID}/g\" \\\n\ne \"s|PROD_LISTENER_ARN|${PROD_LISTENER_ARN}|g\" \\\n\ne \"s|TEST_LISTENER_ARN|${TEST_LISTENER_ARN}|g\" \\\n\ncodedeploy-template.json > codedeploy.json\n\nPasswords\n\nDuring some of the steps in the recipes, you will create passwords and temporarily save them as environment variables to use in subsequent steps. Make sure you unset the environment variables by following the cleanup steps when you complete the recipe. We use this approach for simplicity of understanding. A more secure method (such as the method in Recipe 1.8) should be used in production environments by leveraging AWS Secrets Manager.",
      "content_length": 2020,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Generation\n\nYou can use AWS Secrets Manager via the AWS CLI to generate passwords with specific requirements. An example from Chapter 4 looks like this:\n\nADMIN_PASSWORD=$(aws secretsmanager get-random-password \\\n\n--exclude-punctuation \\\n\n--password-length 41 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nUsage and storage\n\nIn production environments, you should use AWS Secrets Manager or AWS Systems Manager Parameter Store (using secure strings) with IAM policies to control who and what can access the secrets. For simplicity, some of the policies of passwords and secrets used in the recipes might not be as locked down from a policy perspective as you would want in a production environment. Be sure to always write your own IAM policies to control this behavior in practice.\n\nRandom suffixes\n\nWe generate a lot of random suffixes when we deal with global services like Amazon S3. These are needed because S3 bucket names need to be globally unique across the entire AWS customer base. Secrets Manager can be used via the CLI to generate a string that satisfies the naming convention and adds this random element to ensure all book readers can create resources and follow along using the same commands:\n\nRANDOM_STRING=$(aws secretsmanager get-random-password \\\n\n--exclude-punctuation --exclude-uppercase \\\n\n--password-length 6 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nYou can also use any other utilities to generate random strings. Some local tools may be preferred.\n\nAWS Cloud Development Kit and helper.py\n\nA good place to start is the “Getting started with the AWS CDK” guide. After you have CDK 2.0 installed, if this is the first time you are using the AWS CDK, you’ll need to bootstrap with the Region you are working on with the AWS CDK toolkit:\n\ncdk bootstrap aws://$AWS_ACCOUNT_ID/$AWS_REGION\n\nWe use the AWS CDK when needed throughout the book to give you the ability to deploy a consistent scenario that aligns with the problem statement you see in the recipe. You can also",
      "content_length": 2052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "choose to execute the recipe steps in your own existing environments, as long as you have the input variables required for the recipe steps. If things don’t work in your environment, you can stand up the provided environment and compare.\n\nThe CDK code we included in the repositories deploys resources using the AWS CloudFormation service, and we wrote output variables that you use in recipe steps. We created a Python script called helper.py which you can run in your terminal to take the CloudFormation output and set local variables to make the recipe steps easier to follow—in most cases, even copy and paste.\n\nAn example set of commands for deploying CDK code for a recipe after checking out the chapter repository for Chapter 4, looks like the following:\n\ncd 401-Creating-an-Aurora-Serverless-DB/cdk-AWS-Cookbook-401/\n\ntest -d .venv || python3 -m venv .venv\n\nsource .venv/bin/activate\n\npip install --upgrade pip setuptools wheel\n\npip install -r requirements.txt\n\ncdk deploy\n\nYou can easily copy and paste the preceding code from the root of the chapter repository (assuming you have Python, pip, and CDK installed as prerequisites) to deploy the scenario that the solution will address in the solution steps of the recipe.\n\nThe helper.py tool we created can then be run in your terminal after the cdk deploy is complete:\n\npython helper.py\n\nYou should see output that you can copy and paste into your terminal to set environment variables from the CDK CloudFormation stack outputs:\n\n$ python helper.py\n\nCopy and paste the commands below into your terminal\n\nROLE_NAME='cdk-aws-cookbook-108-InstanceSS1PK7LB631QYEF'\n\nINSTANCE_ID='random string here'\n\nNOTE\n\nFinally, a reminder that although we work for AWS, the opinions expressed in this book are our own.\n\nPut on your apron, and let’s get cooking with AWS!",
      "content_length": 1812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Conventions Used in This Book The following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.\n\nConstant width bold\n\nShows commands or other text that should be typed literally by the user.\n\nConstant width italic\n\nShows text that should be replaced with user-supplied values or by values determined by context.\n\nTIP\n\nThis element signifies a tip or suggestion.\n\nNOTE\n\nThis element signifies a general note.\n\nWARNING\n\nThis element indicates a warning or caution.\n\nUsing Code Examples Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/awscookbook.",
      "content_length": 891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "If you have a technical question or a problem using the code examples, please send email to bookquestions@oreilly.com.\n\nThis book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require permission.\n\nWe appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “AWS Cookbook by John Culkin and Mike Zazon (O’Reilly). Copyright 2022 Culkins Coffee Shop LLC and Mike Zazon, 978-1-492-09260-5.”\n\nIf you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.\n\nO’Reilly Online Learning\n\nNOTE\n\nFor more than 40 years, O’Reilly Media has provided technology and business training, knowledge, and insight to help\n\ncompanies succeed.\n\nOur unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, visit http://oreilly.com.\n\nHow to Contact Us Please address comments and questions concerning this book to the publisher:\n\nO’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax)",
      "content_length": 2066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/AWS-cookbook.\n\nEmail bookquestions@oreilly.com to comment or ask technical questions about this book.\n\nFor news and information about our books and courses, visit http://oreilly.com.\n\nFind us on Facebook: http://facebook.com/oreilly\n\nFollow us on Twitter: http://twitter.com/oreillymedia\n\nWatch us on YouTube: http://youtube.com/oreillymedia\n\nAcknowledgments Thank you to Jeff Armstrong, author of Migrating to AWS, A Manager’s Guide for introducing us to O’Reilly.\n\nWe want to recognize the tech reviewers who helped get this book to where it is today. Their keen eyes, opinions, and technical prowess are greatly appreciated. Jess Males, Gaurav Raje, Jeff Barr, Paul Bayer, Neil Stewart, David Kheyman, Justin Domingus, Justin Garrison, Julian Pittas, Mark Wilkins, and Virginia Chu—thank you.\n\nThanks to the knowledgeable community at r/aws for always providing great insights and opinions.\n\nThank you to our production editor, Christopher Faucher, for getting the book in tip-top shape for release. Thanks also to our editor, Virginia Wilson, for taking the time to work with first-time authors during a pandemic. Your patience, suggestions, and guidance allowed us to complete this book and remain (somewhat) sane :-) Chapter 1. Security\n\n1.0 Introduction The average cost of a data breach in 2021 reached a new high of USD 4.24 million as reported by the IBM/Ponemon Institute Report. When you choose to run your applications in the cloud, you trust AWS to provide a secure infrastructure that runs cloud services so that you can focus on your own innovation and value-added activities.\n\nBut security in the cloud is a shared responsibility between you and AWS. You are responsible for the configuration of things like AWS Identity and Access Management (IAM) policies, Amazon EC2 security groups, and host based firewalls. In other words, the security of the hardware and software platform that make up the AWS cloud is an AWS responsibility. The security of software and configurations that you implement in your AWS account(s) are your responsibility.\n\nAs you deploy cloud resources in AWS and apply configuration, it is critical to understand the security settings required to maintain a secure environment. This chapter’s recipes include best",
      "content_length": 2396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "practices and use cases focused on security. As security is a part of everything, you will use these recipes in conjunction with other recipes and chapters in this book. For example, you will see usage of AWS Systems Manager Session Manager used throughout the book when connecting to your EC2 instances. These foundational security recipes will give you the tools you need to build secure solutions on AWS.\n\nIn addition to the content in this chapter, so many great resources are available for you to dive deeper into security topics on AWS. “The Fundamentals of AWS Cloud Security”, presented at the 2019 AWS security conference re:Inforce, gives a great overview. A more advanced talk, “Encryption: It Was the Best of Controls, It Was the Worst of Controls”, from AWS re:Invent, explores encryption scenarios explained in detail.\n\nTIP\n\nAWS publishes a best practices guide for securing your account, and all AWS account holders should be familiar with the\n\nbest practices as they continue to evolve.\n\nWARNING\n\nWe cover important security topics in this chapter. It is not possible to cover every topic as the list of services and\n\nconfigurations (with respect to security on AWS) continues to grow and evolve. AWS keeps its Best Practices for Security,\n\nIdentity, and Compliance web page up-to-date.\n\nWorkstation Configuration\n\nYou will need a few things installed to be ready for the recipes in this chapter.\n\nGeneral setup\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Security\n\n1.1 Creating and Assuming an IAM Role for Developer Access\n\nProblem",
      "content_length": 1728,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "To ensure that you are not always using administrative permissions, you need to create an IAM role for development use in your AWS account.\n\nSolution\n\nCreate a role using an IAM policy that will allow the role to be assumed later. Attach the AWS managed PowerUserAccess IAM policy to the role (see Figure 1-1).\n\nFigure 1-1. Create role, attach policy, and assume role\n\nSteps\n\n1.\n\nCreate a file named assume-role-policy-template.json with the following content. This will allow an IAM principal to assume the role you will create next (file provided in the repository):\n\n{",
      "content_length": 571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "2.\n\n3.\n\n4.\n\n5.\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"AWS\": \"PRINCIPAL_ARN\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nTIP\n\nIf you are using an IAM user, and you delete and re-create the IAM user, this policy will not continue\n\nto work because of the way that the IAM service helps mitigate the risk of privilege escalation. For\n\nmore information, see the Note in the IAM documentation about this.\n\nRetrieve the ARN for your user and set it as a variable:\n\nPRINCIPAL_ARN=$(aws sts get-caller-identity --query Arn --output text)\n\nUse the sed command to replace PRINCIPAL_ARN in the assume-role-policy- template.json file and generate the assume-role-policy.json file:\n\nsed -e \"s|PRINCIPAL_ARN|${PRINCIPAL_ARN}|g\" \\\n\nassume-role-policy-template.json > assume-role-policy.json\n\nCreate a role and specify the assume role policy file:\n\nROLE_ARN=$(aws iam create-role --role-name AWSCookbook101Role \\\n\n--assume-role-policy-document file://assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\nAttach the AWS managed PowerUserAccess policy to the role:",
      "content_length": 1094,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "aws iam attach-role-policy --role-name AWSCookbook101Role \\\n\n--policy-arn arn:aws:iam::aws:policy/PowerUserAccess\n\nNOTE\n\nAWS provides access policies for common job functions for your convenience. These policies may be\n\na good starting point for you to delegate user access to your account for specific job functions;\n\nhowever, it is best to define a least-privilege policy for your own specific requirements for every\n\naccess need.\n\nValidation checks\n\nAssume the role:\n\naws sts assume-role --role-arn $ROLE_ARN \\\n\n--role-session-name AWSCookbook101\n\nYou should see output similar to the following:\n\n{\n\n\"Credentials\": {\n\n\"AccessKeyId\": \"<snip>\",\n\n\"SecretAccessKey\": \"<snip>\",\n\n\"SessionToken\": \"<snip>\",\n\n\"Expiration\": \"2021-09-12T23:34:56+00:00\"\n\n},\n\n\"AssumedRoleUser\": {\n\n\"AssumedRoleId\": \"EXAMPLE:AWSCookbook101\",\n\n\"Arn\": \"arn:aws:sts::11111111111:assumed-role/AWSCookbook101Role/AWSCookbook101\"\n\n}\n\n}\n\nTIP\n\nThe AssumeRole API returns a set of temporary credentials for a role session from the AWS Security Token Service (STS)\n\nto the caller as long as the permissions in the AssumeRole policy for the role allow. All IAM roles have an AssumeRole policy associated with them. You can use the output of this to configure the credentials for the AWS CLI; set the\n\nAccessKey, SecretAccessKey, and SessionToken as environment variables; and also assume the role in the AWS Console",
      "content_length": 1378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "using the Switch Role feature. When your applications need to make AWS API calls, the AWS SDK for your programming\n\nlanguage of choice handles this for them.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nUsing administrative access for routine development tasks is not a security best practice. Giving unneeded permissions can result in unauthorized actions being performed. Using the PowerUserAccess AWS managed policy for development purposes is a better alternative to start rather than using AdministratorAccess. Later, you should define your own customer managed policy granting only the specific actions for your needs. For example, if you need to log in often to check the status of your EC2 instances, you can create a read-only policy for this purpose and attach it to a role. Similarly, you can create a role for billing access and use it to access the AWS Billing console only. The more you practice using the principle of least privilege, the more security will become a natural part of what you do.\n\nYou used an IAM user in this recipe to perform the steps. If you are using an AWS account that leverages federation for access (e.g., a sandbox or development AWS account at your employer), you should use temporary credentials from the AWS STS rather than an IAM user. This type of access uses time-based tokens that expire after an amount of time, rather than “long-lived” credentials like access keys or passwords. When you performed the AssumeRole in the validation steps, you called the STS service for temporary credentials. To help with frequent AssumeRole operations, the AWS CLI supports named profiles that can automatically assume and refresh your temporary credentials for your role when you specify the role_arn parameter in the named profile.\n\nTIP\n\nYou can require multi-factor authentication (MFA) as a condition within the AssumeRole policies you create. This would allow the role to be assumed only by an identity that has been authenticated with MFA. For more information about\n\nrequiring MFA for AssumeRole, see the support document.\n\nSee Recipe 9.4 to create an alert when a root login occurs.\n\nTIP\n\nYou can grant cross-account access to your AWS resources. The resource you define in the policy in this recipe would\n\nreference the AWS account and principal within that account that you would like to delegate access to. You should always",
      "content_length": 2420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "use an ExternalID when enabling cross-account access. For more information, see the official tutorial for cross-account access.\n\nChallenge\n\nCreate additional IAM roles for each of the AWS managed policies for job functions (e.g., billing, database administrator, networking, etc.) 1.2 Generating a Least Privilege IAM Policy Based on Access Patterns\n\nProblem\n\nYou would like to implement least privilege access for your user and scope down the permissions to allow access to only the services, resources, and actions you need to use in your AWS account.\n\nSolution\n\nUse the IAM Access Analyzer in the IAM console to generate an IAM policy based on the CloudTrail activity in your AWS account, as shown in Figure 1-2.\n\nFigure 1-2. IAM Access Analyzer workflow",
      "content_length": 757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "Prerequisite\n\nSteps\n\n1.\n\n2.\n\n3.\n\nCloudTrail logging enabled for your account to a configured S3 bucket (see Recipe 9.3)\n\nNavigate to the IAM console and select your IAM role or IAM user that you would like to generate a policy for.\n\nOn the Permissions tab (the default active tab when viewing your principal), scroll to the bottom, expand the “Generate policy based on CloudTrail events” section, and click the “Generate policy” button.\n\nTIP\n\nFor a quick view of the AWS services accessed from your principal, click the Access Advisor tab and\n\nview the service list and access time. While the IAM Access Advisor is not as powerful as the Access\n\nAnalyzer, it can be helpful when auditing or troubleshooting IAM principals in your AWS account.\n\nSelect the time period of CloudTrail events you would like to evaluate, select your CloudTrail trail, choose your Region (or select “All regions”), and choose “Create and use a new service role.” IAM Access Analyzer will create a role for the service to use for read access to your trail that you selected. Finally, click “Generate policy.” See Figure 1-3 for an example.",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "4.\n\n5.\n\nFigure 1-3. Generating a policy in the IAM Access Analyzer configuration\n\nNOTE\n\nThe role creation can take up to 30 seconds. Once the role is created, the policy generation will take an\n\namount of time depending on how much activity is in your CloudTrail trail.\n\nOnce the analyzer has completed, scroll to the bottom of the permissions tab and click “View generated policy,” as shown in Figure 1-4.\n\nFigure 1-4. Viewing the generated policy\n\nClick Next, and you will see a generated policy in JSON format that is based on the activity that your IAM principal has made. You can edit this policy in the interface if you wish to add additional permissions. Click Next again, choose a name, and you can deploy this generated policy as an IAM policy.\n\nYou should see a generated IAM policy in the IAM console similar to this:\n\n{",
      "content_length": 831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"access-analyzer:ListPolicyGenerations\",\n\n\"cloudtrail:DescribeTrails\",\n\n\"cloudtrail:LookupEvents\",\n\n\"iam:GetAccountPasswordPolicy\",\n\n\"iam:GetAccountSummary\",\n\n\"iam:GetServiceLastAccessedDetails\",\n\n\"iam:ListAccountAliases\",\n\n\"iam:ListGroups\",\n\n\"iam:ListMFADevices\",\n\n\"iam:ListUsers\",\n\n\"s3:ListAllMyBuckets\",\n\n\"sts:GetCallerIdentity\"\n\n],\n\n\"Resource\": \"*\"\n\n}, ...\n\n}\n\nValidation checks\n\nCreate a new IAM user or role and attach the newly created IAM policy to it. Perform an action granted by the policy to verify that the policy allows your IAM principal to perform the actions that you need it to.\n\nDiscussion\n\nYou should always seek to implement least privilege IAM policies when you are scoping them for your users and applications. Oftentimes, you might not know exactly what permissions you may need when you start. With IAM Access Analyzer, you can start by granting your users and applications a larger scope in a development environment, enable CloudTrail logging (Recipe 9.3), and then run IAM Access Analyzer after you have a window of time that provides a good representation of the usual activity (choose this time period in the Access Analyzer configuration as you did in step 3). The generated policy will contain all of the necessary permissions to allow your application or users to work as they did during that time period that you chose to analyze, helping you implement the principle of least privilege.\n\nNOTE\n\nYou should also be aware of the list of services that Access Analyzer supports.",
      "content_length": 1585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Challenge\n\nUse the IAM Policy Simulator (see Recipe 1.4) on the generated policy to verify that the policy contains the access you need.\n\n1.3 Enforcing IAM User Password Policies in Your AWS Account\n\nNOTE\n\nSpecial thanks to Gaurav Raje for his contribution to this recipe.\n\nProblem\n\nYour security policy requires that you must enforce a password policy for all the users within your AWS account. The password policy sets a 90-day expiration, and passwords must be made up of a minimum of 32 characters including lowercase and uppercase letters, numbers, and symbols.\n\nSolution\n\nSet a password policy for IAM users in your AWS account. Create an IAM group, an IAM user, and add the user to the group to verify that the policy is being enforced (see Figure 1-5).",
      "content_length": 760,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Figure 1-5. Using password policies with IAM users\n\nNOTE\n\nIf your organization has a central user directory, we recommend using identity federation to access your AWS accounts\n\nusing AWS Single Sign-On (SSO) rather than create individual IAM users and groups. Federation allows you to use an\n\nidentity provider (IdP) where you already maintain users and groups. AWS publishes a guide that explains federated\n\naccess configurations available. You can follow Recipe 9.6 to enable AWS SSO for your account even if you do not have\n\nan IdP available (AWS SSO provides a directory you can use by default).\n\nSteps\n\n1.\n\nSet an IAM password policy using the AWS CLI to require lowercase and uppercase letters, symbols, and numbers. The policy should indicate a minimum length of 32 characters, a maximum password age of 90 days, and password reuse prevented:\n\naws iam update-account-password-policy \\\n\n--minimum-password-length 32 \\\n\n--require-symbols \\\n\n--require-numbers \\",
      "content_length": 965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "2.\n\n3.\n\n4.\n\n--require-uppercase-characters \\\n\n--require-lowercase-characters \\\n\n--allow-users-to-change-password \\\n\n--max-password-age 90 \\\n\n--password-reuse-prevention true\n\nCreate an IAM group:\n\naws iam create-group --group-name AWSCookbook103Group\n\nYou should see output similar to the following:\n\n{\n\n\"Group\": {\n\n\"Path\": \"/\",\n\n\"GroupName\": \"AWSCookbook103Group\",\n\n\"GroupId\": \"<snip>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:group/AWSCookbook103Group\",\n\n\"CreateDate\": \"2021-11-06T19:26:01+00:00\"\n\n}\n\n}\n\nAttach the ReadOnlyAccess policy to the group:\n\naws iam attach-group-policy --group-name AWSCookbook103Group \\\n\n--policy-arn arn:aws:iam::aws:policy/AWSBillingReadOnlyAccess\n\nTIP\n\nIt is best to attach policies to groups and not directly to users. As the number of users grows, it is\n\neasier to use IAM groups to delegate permissions for manageability. This also helps to meet\n\ncompliance for standards like CIS Level 1.\n\nCreate an IAM user:",
      "content_length": 941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "5.\n\n6.\n\naws iam create-user --user-name awscookbook103user\n\nYou should see output similar to the following:\n\n{\n\n\"User\": {\n\n\"Path\": \"/\",\n\n\"UserName\": \"awscookbook103user\",\n\n\"UserId\": \"<snip>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:user/awscookbook103user\",\n\n\"CreateDate\": \"2021-11-06T21:01:47+00:00\"\n\n}\n\n}\n\nUse Secrets Manager to generate a password that conforms to your password policy:\n\nRANDOM_STRING=$(aws secretsmanager get-random-password \\\n\n--password-length 32 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nCreate a login profile for the user that specifies a password:\n\naws iam create-login-profile --user-name awscookbook103user \\\n\n--password $RANDOM_STRING\n\nYou should see output similar to the following:\n\n{\n\n\"LoginProfile\": {\n\n\"UserName\": \"awscookbook103user\",\n\n\"CreateDate\": \"2021-11-06T21:11:43+00:00\",\n\n\"PasswordResetRequired\": false\n\n}\n\n}",
      "content_length": 879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "7.\n\nAdd the user to the group you created for billing view-only access:\n\naws iam add-user-to-group --group-name AWSCookbook103Group \\\n\n--user-name awscookbook103user\n\nValidation checks\n\nVerify that the password policy you set is now active:\n\naws iam get-account-password-policy\n\nYou should see output similar to:\n\n{\n\n\"PasswordPolicy\": {\n\n\"MinimumPasswordLength\": 32,\n\n\"RequireSymbols\": true,\n\n\"RequireNumbers\": true,\n\n\"RequireUppercaseCharacters\": true,\n\n\"RequireLowercaseCharacters\": true,\n\n\"AllowUsersToChangePassword\": true,\n\n\"ExpirePasswords\": true,\n\n\"MaxPasswordAge\": 90,\n\n\"PasswordReusePrevention\": 1\n\n}\n\n}\n\nTry to create a new user by using the AWS CLI with a password that violates the password policy. AWS will not allow you to create such a user:\n\naws iam create-user --user-name awscookbook103user2\n\nUse Secrets Manager to generate a password that does not adhere to your password policy:\n\nRANDOM_STRING2=$(aws secretsmanager get-random-password \\\n\n--password-length 16 --require-each-included-type \\",
      "content_length": 1011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "--output text \\\n\n--query RandomPassword)\n\nCreate a login profile for the user that specifies the password:\n\naws iam create-login-profile --user-name awscookbook103user2 \\\n\n--password $RANDOM_STRING2\n\nThis command should fail and you should see output similar to:\n\nAn error occurred (PasswordPolicyViolation) when calling the CreateLoginProfile\n\noperation: Password should have a minimum length of 32\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nFor users logging in with passwords, AWS allows administrators to enforce password policies to their accounts that conform to the security requirements of your organization. This way, administrators can ensure that individual users don’t compromise the security of the organization by choosing weak passwords or by not regularly changing their passwords.\n\nTIP\n\nMulti-factor authentication is encouraged for IAM users. You can use a software-based virtual MFA device or a hardware\n\ndevice for a second factor on IAM users. AWS keeps an updated list of supported devices.\n\nMulti-factor authentication is a great way to add another layer of security on top of existing password-based security. It combines “what you know” and “what you have”; so, in cases where your password might be exposed to a malicious third-party actor, they would still need the additional factor to authenticate.\n\nChallenge\n\nDownload the credential report to analyze the IAM users and the password ages in your account.\n\n1.4 Testing IAM Policies with the IAM Policy Simulator",
      "content_length": 1543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "Problem\n\nYou have an IAM policy that you would like to put into use but would like to test its effectiveness first.\n\nSolution\n\nAttach an IAM policy to an IAM role and simulate actions with the IAM Policy Simulator, as shown in Figure 1-6.\n\nFigure 1-6. Simulating IAM policies attached to an IAM role\n\nSteps\n\n1.\n\nCreate a file called assume-role-policy.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",",
      "content_length": 486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "2.\n\n3.\n\n\"Principal\": {\n\n\"Service\": \"ec2.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate an IAM role using the assume-role-policy.json file:\n\naws iam create-role --assume-role-policy-document \\\n\nfile://assume-role-policy.json --role-name AWSCookbook104IamRole\n\nYou should see output similar to the following:\n\n{\n\n\"Role\": {\n\n\"Path\": \"/\",\n\n\"RoleName\": \"AWSCookbook104IamRole\",\n\n\"RoleId\": \"<<UniqueID>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:role/AWSCookbook104IamRole\",\n\n\"CreateDate\": \"2021-09-22T23:37:44+00:00\",\n\n\"AssumeRolePolicyDocument\": {\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n...\n\nAttach the IAM managed policy for AmazonEC2ReadOnlyAccess to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook104IamRole \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonEC2ReadOnlyAccess\n\nTIP\n\nYou can find a list of all the actions, resources, and condition keys for EC2 in this AWS article. The IAM global condition\n\ncontext keys are also useful in authoring fine-grained policies.",
      "content_length": 1000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Validation checks\n\nSimulate the effect of the IAM policy you are using, testing several different types of actions on the EC2 service.\n\nTest the ec2:CreateInternetGateway action:\n\naws iam simulate-principal-policy \\\n\n--policy-source-arn arn:aws:iam::$AWS_ACCOUNT_ARN:role/AWSCookbook104IamRole \\\n\n--action-names ec2:CreateInternetGateway\n\nYou should see output similar to the following (note the EvalDecision):\n\n{\n\n\"EvaluationResults\": [\n\n{\n\n\"EvalActionName\": \"ec2:CreateInternetGateway\",\n\n\"EvalResourceName\": \"*\",\n\n\"EvalDecision\": \"implicitDeny\",\n\n\"MatchedStatements\": [],\n\n\"MissingContextValues\": []\n\n}\n\n]\n\n}\n\nNOTE\n\nSince you attached only the AWS managed AmazonEC2ReadOnlyAccess IAM policy to the role in this recipe, you will see an implicit deny for the CreateInternetGateway action. This is expected behavior. AmazonEC2ReadOnlyAccess does not grant any “create” capabilities for the EC2 service.\n\nTest the ec2:DescribeInstances action:\n\naws iam simulate-principal-policy \\\n\n--policy-source-arn arn:aws:iam::$AWS_ACCOUNT_ARN:role/AWSCookbook104IamRole \\\n\n--action-names ec2:DescribeInstances\n\nYou should see output similar to the following:",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "{\n\n\"EvaluationResults\": [\n\n{\n\n\"EvalActionName\": \"ec2:DescribeInstances\",\n\n\"EvalResourceName\": \"*\",\n\n\"EvalDecision\": \"allowed\",\n\n\"MatchedStatements\": [\n\n{\n\n\"SourcePolicyId\": \"AmazonEC2ReadOnlyAccess\",\n\n\"SourcePolicyType\": \"IAM Policy\",\n\n\"StartPosition\": {\n\n\"Line\": 3,\n\n\"Column\": 17\n\n},\n\n\"EndPosition\": {\n\n\"Line\": 8,\n\n\"Column\": 6\n\n}\n\n}\n\n],\n\n\"MissingContextValues\": []\n\n}\n\n]\n\n}\n\nNOTE\n\nThe AmazonEC2ReadOnlyAccess policy allows read operations on the EC2 service, so the DescribeInstances operation succeeds when you simulate this action.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nIAM policies let you define permissions for managing access in AWS. Policies can be attached to principals that allow you to grant (or deny) permissions to resources, users, groups and services. It is always best to scope your policies to the minimal set of permissions required as a security best practice. The IAM Policy Simulator can be extremely helpful when designing and managing your own IAM policies for least-privileged access.\n\nThe IAM Policy Simulator also exposes a web interface you can use to test and troubleshoot IAM policies and understand their net effect with the policy you define. You can test all the",
      "content_length": 1253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "policies or a subset of policies that you have attached to users, groups, and roles.\n\nTIP\n\nThe IAM Policy Simulator can help you simulate the effect of the following:\n\nIdentity-based policies\n\nIAM permissions boundaries\n\nAWS Organizations service control policies (SCPs)\n\nResource-based policies\n\nAfter you review the Policy Simulator results, you can add additional statements to your policies that either solve your issue (from a troubleshooting standpoint) or attach newly created policies to users, groups, and roles with the confidence that the net effect of the policy was what you intended.\n\nNOTE\n\nTo help you easily build IAM policies from scratch, AWS provides the AWS Policy Generator.\n\nChallenge\n\nSimulate the effect of a permissions boundary on an IAM principal (see Recipe 1.5).\n\n1.5 Delegating IAM Administrative Capabilities Using Permissions Boundaries\n\nProblem\n\nYou need to grant team members the ability to deploy Lambda functions and create IAM roles for them. You need to limit the effective permissions of the IAM roles created so that they allow only actions needed by the function.\n\nSolution\n\nCreate a permissions boundary policy, create an IAM role for Lambda developers, create an IAM policy that specifies the boundary policy, and attach the policy to the role you created.",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Figure 1-7 illustrates the effective permissions of the identity-based policy with the permissions boundary.\n\nFigure 1-7. Effective permissions of identity-based policy with permissions boundary",
      "content_length": 194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Prerequisite\n\nSteps\n\n1.\n\n2.\n\n3.\n\nAn IAM user or federated identity for your AWS account with administrative privileges (follow the AWS guide for creating your first IAM admin user and user group).\n\nCreate a file named assume-role-policy-template.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"AWS\": \"PRINCIPAL_ARN\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nRetrieve the ARN for your user and set it as a variable:\n\nPRINCIPAL_ARN=$(aws sts get-caller-identity --query Arn --output text)\n\nUse the sed command to replace PRINCIPAL_ARN in the assume-role-policy- template.json file that we provided in the repository and generate the assume-role-policy.json file:\n\nsed -e \"s|PRINCIPAL_ARN|${PRINCIPAL_ARN}|g\" \\\n\nassume-role-policy-template.json > assume-role-policy.json\n\nNOTE\n\nFor the purposes of this recipe, you set the allowed IAM principal to your own user (User 1). To test\n\ndelegated access, you would set the IAM principal to something else.",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "4.\n\n5.\n\nCreate a role and specify the assume role policy file:\n\nROLE_ARN=$(aws iam create-role --role-name AWSCookbook105Role \\\n\n--assume-role-policy-document file://assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\nCreate a permissions boundary JSON file named boundary-template.json with the following content. This allows specific DynamoDB, S3, and CloudWatch Logs actions (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"CreateLogGroup\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": \"logs:CreateLogGroup\",\n\n\"Resource\": \"arn:aws:logs:*:AWS_ACCOUNT_ID:*\"\n\n},\n\n{\n\n\"Sid\": \"CreateLogStreamandEvents\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"logs:CreateLogStream\",\n\n\"logs:PutLogEvents\"\n\n],\n\n\"Resource\": \"arn:aws:logs:*:AWS_ACCOUNT_ID:*\"\n\n},\n\n{\n\n\"Sid\": \"DynamoDBPermissions\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"dynamodb:PutItem\",\n\n\"dynamodb:UpdateItem\",\n\n\"dynamodb:DeleteItem\"\n\n],\n\n\"Resource\": \"arn:aws:dynamodb:*:AWS_ACCOUNT_ID:table/AWSCookbook*\"\n\n},\n\n{\n\n\"Sid\": \"S3Permissions\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"s3:GetObject\",",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "6.\n\n7.\n\n8.\n\n\"s3:PutObject\"\n\n],\n\n\"Resource\": \"arn:aws:s3:::AWSCookbook*/*\"\n\n}\n\n]\n\n}\n\nUse the sed command to replace AWS_ACCOUNT_ID in the boundary-policy- template.json file and generate the boundary-policy.json file:\n\nsed -e \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\nboundary-policy-template.json > boundary-policy.json\n\nCreate the permissions boundary policy by using the AWS CLI:\n\naws iam create-policy --policy-name AWSCookbook105PB \\\n\n--policy-document file://boundary-policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Policy\": {\n\n\"PolicyName\": \"AWSCookbook105PB\",\n\n\"PolicyId\": \"EXAMPLE\",\n\n\"Arn\": \"arn:aws:iam::111111111111:policy/AWSCookbook105PB\",\n\n\"Path\": \"/\",\n\n\"DefaultVersionId\": \"v1\",\n\n\"AttachmentCount\": 0,\n\n\"PermissionsBoundaryUsageCount\": 0,\n\n\"IsAttachable\": true,\n\n\"CreateDate\": \"2021-09-24T00:36:53+00:00\",\n\n\"UpdateDate\": \"2021-09-24T00:36:53+00:00\"\n\n}\n\n}\n\nCreate a policy file named policy-template.json for the role (file provided in the repository):",
      "content_length": 983,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"DenyPBDelete\",\n\n\"Effect\": \"Deny\",\n\n\"Action\": \"iam:DeleteRolePermissionsBoundary\",\n\n\"Resource\": \"*\"\n\n},\n\n{\n\n\"Sid\": \"IAMRead\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"iam:Get*\",\n\n\"iam:List*\"\n\n],\n\n\"Resource\": \"*\"\n\n},\n\n{\n\n\"Sid\": \"IAMPolicies\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"iam:CreatePolicy\",\n\n\"iam:DeletePolicy\",\n\n\"iam:CreatePolicyVersion\",\n\n\"iam:DeletePolicyVersion\",\n\n\"iam:SetDefaultPolicyVersion\"\n\n],\n\n\"Resource\": \"arn:aws:iam::AWS_ACCOUNT_ID:policy/AWSCookbook*\"\n\n},\n\n{\n\n\"Sid\": \"IAMRolesWithBoundary\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"iam:CreateRole\",\n\n\"iam:DeleteRole\",\n\n\"iam:PutRolePolicy\",\n\n\"iam:DeleteRolePolicy\",\n\n\"iam:AttachRolePolicy\",\n\n\"iam:DetachRolePolicy\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:iam::AWS_ACCOUNT_ID:role/AWSCookbook*\"\n\n],\n\n\"Condition\": {\n\n\"StringEquals\": {",
      "content_length": 832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "\"iam:PermissionsBoundary\": \"arn:aws:iam::AWS_ACCOUNT_ID:policy/AWSCookbook105PB\"\n\n}\n\n}\n\n},\n\n{\n\n\"Sid\": \"ServerlessFullAccess\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"lambda:*\",\n\n\"logs:*\",\n\n\"dynamodb:*\",\n\n\"s3:*\"\n\n],\n\n\"Resource\": \"*\"\n\n},\n\n{\n\n\"Sid\": \"PassRole\",\n\n\"Effect\": \"Allow\",\n\n\"Action\": \"iam:PassRole\",\n\n\"Resource\": \"arn:aws:iam::AWS_ACCOUNT_ID:role/AWSCookbook*\",\n\n\"Condition\": {\n\n\"StringLikeIfExists\": {\n\n\"iam:PassedToService\": \"lambda.amazonaws.com\"\n\n}\n\n}\n\n},\n\n{\n\n\"Sid\": \"ProtectPB\",\n\n\"Effect\": \"Deny\",\n\n\"Action\": [\n\n\"iam:CreatePolicyVersion\",\n\n\"iam:DeletePolicy\",\n\n\"iam:DeletePolicyVersion\",\n\n\"iam:SetDefaultPolicyVersion\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:iam::AWS_ACCOUNT_ID:policy/AWSCookbook105PB\",\n\n\"arn:aws:iam::AWS_ACCOUNT_ID:policy/AWSCookbook105Policy\"\n\n]\n\n}\n\n]\n\n}\n\nThis custom IAM policy has several statements working together, which define certain permissions for the solution to the problem statement:",
      "content_length": 911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "9.\n\n10.\n\nDenyPBDelete: Explicitly deny the ability to delete permissions boundaries from roles.\n\nIAMRead: Allow read-only IAM access to developers to ensure that the IAM console works.\n\nIAMPolicies: Allow the creation of IAM policies but force a naming convention prefix AWSCookbook*.\n\nIAMRolesWithBoundary: Allow the creation and deletion of IAM roles only if they contain the permissions boundary referenced.\n\nServerlessFullAccess: Allow developers to have full access to the AWS Lambda, Amazon DynamoDB, Amazon CloudWatch logs, and Amazon S3 services.\n\nPassRole: Allow developers to pass IAM roles to Lambda functions.\n\nProtectPB: Explicitly deny the ability to modify the permissions boundary that bound the roles they create.\n\nUse the sed command to replace AWS_ACCOUNT_ID in the policy- template.json file and generate the policy.json file:\n\nsed -e \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\npolicy-template.json > policy.json\n\nCreate the policy for developer access:\n\naws iam create-policy --policy-name AWSCookbook105Policy \\\n\n--policy-document file://policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Policy\": {\n\n\"PolicyName\": \"AWSCookbook105Policy\",\n\n\"PolicyId\": \"EXAMPLE\",",
      "content_length": 1198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "\"Arn\": \"arn:aws:iam::11111111111:policy/AWSCookbook105Policy\",\n\n\"Path\": \"/\",\n\n\"DefaultVersionId\": \"v1\",\n\n\"AttachmentCount\": 0,\n\n\"PermissionsBoundaryUsageCount\": 0,\n\n\"IsAttachable\": true,\n\n\"CreateDate\": \"2021-09-24T00:37:13+00:00\",\n\n\"UpdateDate\": \"2021-09-24T00:37:13+00:00\"\n\n}\n\n}\n\n11.\n\nAttach the policy to the role you created in step 2:\n\naws iam attach-role-policy --policy-arn \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook105Policy \\\n\n--role-name AWSCookbook105Role\n\nValidation checks\n\nAssume the role you created and set the output to local variables for the AWS CLI:\n\ncreds=$(aws --output text sts assume-role --role-arn $ROLE_ARN \\\n\n--role-session-name \"AWSCookbook105\" | \\\n\ngrep CREDENTIALS | cut -d \" \" -f2,4,5)\n\nexport AWS_ACCESS_KEY_ID=$(echo $creds | cut -d \" \" -f2)\n\nexport AWS_SECRET_ACCESS_KEY=$(echo $creds | cut -d \" \" -f4)\n\nexport AWS_SESSION_TOKEN=$(echo $creds | cut -d \" \" -f5)\n\nTry to create an IAM role for a Lambda function, create an assume role policy for the Lambda service (lambda-assume-role-policy.json):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"lambda.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"",
      "content_length": 1189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "}\n\n]\n\n}\n\nCreate the role, specifying the permissions boundary, which conforms to the role-naming standard specified in the policy:\n\nTEST_ROLE_1=$(aws iam create-role --role-name AWSCookbook105test1 \\\n\n--assume-role-policy-document \\\n\nfile://lambda-assume-role-policy.json \\\n\n--permissions-boundary \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook105PB \\\n\n--output text --query Role.Arn)\n\nAttach the managed AmazonDynamoDBFullAccess policy to the role:\n\naws iam attach-role-policy --role-name AWSCookbook105test1 \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\n\nAttach the managed CloudWatchFullAccess policy to the role:\n\naws iam attach-role-policy --role-name AWSCookbook105test1 \\\n\n--policy-arn arn:aws:iam::aws:policy/CloudWatchFullAccess\n\nNOTE\n\nEven though you attached AmazonDynamoDBFullAccess and CloudWatchFullAccess to the role, the effective permissions of the role are limited by the statements in the permissions boundary you created in step 3. Furthermore, even though you\n\nhave s3:GetObject and s3:PutObject defined in the boundary, you have not defined these in the role policy, so the function will not be able to make these calls until you create a policy that allows these actions. When you attach this role\n\nto a Lambda function, the Lambda function can perform only the actions allowed in the intersection of the permissions\n\nboundary and the role policy (see Figure 1-7).\n\nYou can now create a Lambda function specifying this role (AWSCookbook105test1) as the execution role to validate the DynamoDB and CloudWatch Logs permissions granted to the function. You can also test the results with the IAM Policy Simulator.",
      "content_length": 1656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "You used an AssumeRole and set environment variables to override your local terminal AWS profile to perform these validation checks. To ensure that you revert back to your original authenticated session on the command line, perform the perform the cleanup steps provided at the top of the README file in the repository.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nNOTE\n\nBe sure to delete your environment variables so that you can regain permissions needed for future recipes:\n\nunset AWS_ACCESS_KEY_ID\n\nunset AWS_SECRET_ACCESS_KEY\n\nunset AWS_SESSION_TOKEN\n\nDiscussion\n\nIn your quest to implement a least privilege access model for users and applications within AWS, you need to enable developers to create IAM roles that their applications can assume when they need to interact with other AWS services. For example, an AWS Lambda function that needs to access an Amazon DynamoDB table would need a role created to be able to perform operations against the table. As your team scales, instead of your team members coming to you every time they need a role created for a specific purpose, you can enable (but control) them with permissions boundaries, without giving up too much IAM access. The iam:PermissionsBoundary condition in the policy that grants the iam:CreateRole ensures that the roles created must always include the permissions boundary attached.\n\nPermissions boundaries act as a guardrail and limit privilege escalation. In other words, they limit the maximum effective permissions of an IAM principal created by a delegated administrator by defining what the roles created can do. As shown in Figure 1-7, they work in conjunction with the permissions policy (IAM policy) that is attached to an IAM principal (IAM user or role). This prevents the need to grant wide access to an administrator role, prevents privilege escalation, and helps you achieve least privilege access by allowing your team members to quickly iterate and create their own least-privileged roles for their applications.\n\nIn this recipe, you may have noticed that we used a naming convention of AWSCookbook* on the roles and policies referenced in the permissions boundary policy, which ensures the delegated principals can create roles and policies within this convention. This means that developers can create resources, pass only these roles to services, and also keep a standard naming convention. This is an ideal practice when implementing permissions boundaries. You can develop a naming convention for different teams, applications, and services so that these can all coexist within the",
      "content_length": 2617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "same account, yet have different boundaries applied to them based on their requirements, if necessary.\n\nAt minimum, you need to keep these four things in mind when building roles that implement permissions boundary guardrails to delegate IAM permissions to nonadministrators:\n\n1.\n\nAllow the creation of IAM customer managed policies: your users can create any policy they wish; they do not have an effect until they are attached to an IAM principal.\n\n2.\n\nAllow IAM role creation with a condition that a permissions boundary must be attached: force all roles created by your team members to include the permission boundary in the role creation.\n\n3.\n\nAllow attachment of policies, but only to roles that have a permissions boundary: do not let users modify existing roles that they may have access to.\n\n4.\n\nAllow iam:PassRole to AWS services that your users create roles for: your developers may need to create roles for Amazon EC2 and AWS Lambda, so give them the ability to pass only the roles they create to those services you define.\n\nTIP\n\nPermissions boundaries are a powerful, advanced IAM concept that can be challenging to understand. We recommend\n\nchecking out the talk by Brigid Johnson at AWS re:Inforce 2018 to see some real-world examples of IAM policies, roles,\n\nand permissions boundaries explained in a practical way.\n\nChallenge\n\nExtend the permissions boundary to allow roles created to publish to an SQS queue and SNS topic and adjust the policy for the role as well.\n\n1.6 Connecting to EC2 Instances Using AWS SSM Session Manager\n\nProblem\n\nYou have an EC2 instance in a private subnet and need to connect to the instance without using SSH over the internet.\n\nSolution",
      "content_length": 1684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Create an IAM role, attach the AmazonSSMManagedInstanceCore policy, create an EC2 instance profile, attach the IAM role you created to the instance profile, associate the EC2 instance profile to an EC2 instance, and finally, run the aws ssm start-session command to connect to the instance. A logical flow of these steps is shown in Figure 1-8.\n\nFigure 1-8. Using Session Manager to connect to an EC2 instance\n\nPrerequisites\n\nAmazon Virtual Private Cloud (VPC) with isolated or private subnets and associated route tables\n\nRequired VPC endpoints for AWS Systems Manager\n\nAWS CLI v2 with the Session Manager plugin installed\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file named assume-role-policy.json with the following content (file provided in the repository):",
      "content_length": 828,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "2.\n\n3.\n\n4.\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"ec2.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate an IAM role with the statement in the provided assume-role- policy.json file using this command:\n\nROLE_ARN=$(aws iam create-role --role-name AWSCookbook106SSMRole \\\n\n--assume-role-policy-document file://assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\nAttach the AmazonSSMManagedInstanceCore managed policy to the role so that the role allows access to AWS Systems Manager:\n\naws iam attach-role-policy --role-name AWSCookbook106SSMRole \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\n\nCreate an instance profile:\n\naws iam create-instance-profile \\\n\n--instance-profile-name AWSCookbook106InstanceProfile\n\nYou should see output similar to the following:\n\n{\n\n\"InstanceProfile\": {\n\n\"Path\": \"/\",",
      "content_length": 898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "5.\n\n6.\n\n7.\n\n\"InstanceProfileName\": \"AWSCookbook106InstanceProfile\",\n\n\"InstanceProfileId\": \"(RandomString\",\n\n\"Arn\": \"arn:aws:iam::111111111111:instance-\n\nprofile/AWSCookbook106InstanceProfile\",\n\n\"CreateDate\": \"2021-11-28T20:26:23+00:00\",\n\n\"Roles\": []\n\n}\n\n}\n\nAdd the role that you created to the instance profile:\n\naws iam add-role-to-instance-profile \\\n\n--role-name AWSCookbook106SSMRole \\\n\n--instance-profile-name AWSCookbook106InstanceProfile\n\nNOTE\n\nThe EC2 instance profile contains a role that you create. The instance profile association with an\n\ninstance allows it to define “who I am,” and the role defines “what I am permitted to do.” Both are\n\nrequired by IAM to allow an EC2 instance to communicate with other AWS services using the IAM\n\nservice. You can get a list of instance profiles in your account by running the aws iam list- instance-profiles AWS CLI command.\n\nQuery SSM for the latest Amazon Linux 2 AMI ID available in your Region and save it as an environment variable:\n\nAMI_ID=$(aws ssm get-parameters --names \\\n\n/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 \\\n\n--query 'Parameters[0].[Value]' --output text)\n\nLaunch an instance in one of your subnets that references the instance profile you created and also uses a Name tag that helps you identify the instance in the console:\n\nINSTANCE_ID=$(aws ec2 run-instances --image-id $AMI_ID \\\n\n--count 1 \\\n\n--instance-type t3.nano \\\n\n--iam-instance-profile Name=AWSCookbook106InstanceProfile \\",
      "content_length": 1473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "--subnet-id $SUBNET_1 \\\n\n--security-group-ids $INSTANCE_SG \\\n\n--metadata-options \\\n\nHttpTokens=required,HttpPutResponseHopLimit=64,HttpEndpoint=enabled \\\n\n--tag-specifications \\\n\n'ResourceType=instance,Tags=[{Key=Name,Value=AWSCookbook106}]' \\\n\n'ResourceType=volume,Tags=[{Key=Name,Value=AWSCookbook106}]' \\\n\n--query Instances[0].InstanceId \\\n\n--output text)\n\nTIP\n\nEC2 instance metadata is a feature you can use within your EC2 instance to access information about\n\nyour EC2 instance over an HTTP endpoint from the instance itself. This is helpful for scripting and\n\nautomation via user data. You should always use the latest version of instance metadata. In step 7, you\n\ndid this by specifying the --metadata-options flag and providing the HttpTokens=required option that forces IMDSv2.\n\nValidation checks\n\nEnsure your EC2 instance has registered with SSM. Use the following command to check the status. This command should return the instance ID:\n\naws ssm describe-instance-information \\\n\n--filters Key=ResourceType,Values=EC2Instance \\\n\n--query \"InstanceInformationList[].InstanceId\" --output text\n\nConnect to the EC2 instance by using SSM Session Manager:\n\naws ssm start-session --target $INSTANCE_ID\n\nYou should now be connected to your instance and see a bash prompt. From the bash prompt, run a command to validate you are connected to your EC2 instance by querying the metadata service for an IMDSv2 token and using the token to query metadata for the instance profile associated with the instance:\n\nTOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds:",
      "content_length": 1610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "21600\"`\n\ncurl -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/iam/info\n\nYou should see output similar to the following:\n\n{\n\n\"Code\" : \"Success\",\n\n\"LastUpdated\" : \"2021-09-23T16:03:25Z\",\n\n\"InstanceProfileArn\" : \"arn:aws:iam::111111111111:instance-profile/AWSCookbook106InstanceProfile\",\n\n\"InstanceProfileId\" : \"AIPAZVTINAMEXAMPLE\"\n\n}\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen you use AWS SSM Session Manager to connect to EC2 instances, you eliminate your dependency on Secure Shell (SSH) over the internet for command-line access to your instances. Once you configure Session Manager for your instances, you can instantly connect to a bash shell session on Linux or a PowerShell session for Windows systems.\n\nWARNING\n\nSSM can log all commands and their output during a session. You can set a preference to stop the logging of sensitive data\n\n(e.g., passwords) with this command:\n\nstty -echo; read passwd; stty echo;\n\nThere is more information in an AWS article about logging session activity.",
      "content_length": 1121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Session Manager works by communicating with the AWS Systems Manager (SSM) API endpoints within the AWS Region you are using over HTTPS (TCP port 443). The agent on your instance registers with the SSM service at boot time. No inbound security group rules are needed for Session Manager functionality. We recommend configuring VPC Endpoints for Session Manager to avoid the need for internet traffic and the cost of Network Address Translation (NAT) gateways.\n\nHere are some examples of the increased security posture Session Manager provides:\n\nNo internet-facing TCP ports need to be allowed in security groups associated with instances.\n\nYou can run instances in private (or isolated) subnets without exposing them directly to the internet and still access them for management duties.\n\nThere is no need to create, associate, and manage SSH keys with instances.\n\nThere is no need to manage user accounts and passwords on instances.\n\nYou can delegate access to manage EC2 instances using IAM roles.\n\nNOTE\n\nAny tool like SSM that provides such powerful capabilities must be carefully audited. AWS provides information about\n\nlocking down permissions for the SSM user, and more information about auditing session activity.\n\nChallenge\n\nView the logs for a session and create an alert whenever the rm command is executed.",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "1.7 Encrypting EBS Volumes Using KMS Keys\n\nProblem\n\nYou need an encryption key for encrypting EBS volumes attached to your EC2 instances in a Region, and you need to rotate the key automatically every 365 days.\n\nSolution\n\nCreate a customer-managed KMS key (CMK), enable yearly rotation of the key, enable EC2 default encryption for EBS volumes in a Region, and specify the KMS key you created (shown in Figure 1-9).\n\nFigure 1-9. Create a customer-managed key, enable rotation, and set default encryption for EC2 using a customer-\n\nmanaged key\n\nSteps\n\n1.\n\nCreate a customer-managed KMS key and store the key ARN as a local variable:\n\nKMS_KEY_ID=$(aws kms create-key --description \"AWSCookbook107Key\" \\\n\n--output text --query KeyMetadata.KeyId)\n\n2.\n\nCreate a key alias to help you refer to the key in other steps:\n\naws kms create-alias --alias-name alias/AWSCookbook107Key \\",
      "content_length": 872,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "--target-key-id $KMS_KEY_ID\n\n3.\n\nEnable automated rotation of the symmetric key material every 365 days:\n\naws kms enable-key-rotation --key-id $KMS_KEY_ID\n\n4.\n\nEnable EBS encryption by default for the EC2 service within your current Region:\n\naws ec2 enable-ebs-encryption-by-default\n\nYou should see output similar to the following:\n\n{\n\n\"EbsEncryptionByDefault\": true\n\n}\n\n5.\n\nUpdate the default KMS key used for default EBS encryption to your customer-managed key that you created in step 1:\n\naws ec2 modify-ebs-default-kms-key-id \\\n\n--kms-key-id alias/AWSCookbook107Key\n\nYou should see output similar to the following:\n\n{\n\n\"KmsKeyId\": \"arn:aws:kms:us-east-1:111111111111:key/1111111-aaaa-bbbb-222222222\"\n\n}\n\nValidation checks\n\nUse the AWS CLI to retrieve the default EBS encryption status for the EC2 service:",
      "content_length": 809,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "aws ec2 get-ebs-encryption-by-default\n\nYou should see output similar to the following:\n\n{\n\n\"EbsEncryptionByDefault\": true\n\n}\n\nRetrieve the KMS key ID used for default encryption:\n\naws ec2 get-ebs-default-kms-key-id\n\nYou should see output similar to the following:\n\n{\n\n\"KmsKeyId\": \"arn:aws:kms:us-east-1:1111111111:key/1111111-aaaa-3333-222222222c64b\"\n\n}\n\nCheck the automatic rotation status of the key you created:\n\naws kms get-key-rotation-status --key-id $KMS_KEY_ID\n\nYou should see output similar to the following:\n\n{\n\n\"KeyRotationEnabled\": true\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion",
      "content_length": 646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "When you are faced with the challenge of ensuring that all of your newly created EBS volumes are encrypted, the ebs-encryption-by-default option comes to the rescue. With this setting enabled, every EC2 instance you launch will by default have its EBS volumes encrypted with the specified KMS key. If you do not specify a KMS key, a default AWS-managed aws/ebs KMS key is created and used. If you need to manage the lifecycle of the key or have a requirement specifying that you or your organization must manage the key, customer-managed keys should be used.\n\nAutomatic key rotation on the KMS service simplifies your approach to key rotation and key lifecycle management.\n\nKMS is a flexible service you can use to implement a variety of data encryption strategies. It supports key policies that you can use to control who has access to the key. These key policies layer on top of your existing IAM policy strategy for added security. You can use KMS keys to encrypt many different types of data at rest within your AWS account, for example:\n\nAmazon S3\n\nAmazon EC2 EBS volumes\n\nAmazon RDS databases and clusters\n\nAmazon DynamoDB tables\n\nAmazon EFS volumes\n\nAmazon FSx file shares\n\nAnd many more\n\nChallenge 1\n\nChange the key policy on the KMS key to allow access to only your IAM principal and the EC2 service.\n\nChallenge 2\n\nCreate an EBS volume and verify that it is encrypted by using the aws ec2 describe-volumes command.\n\n1.8 Storing, Encrypting, and Accessing Passwords Using Secrets Manager\n\nProblem\n\nYou need to give your EC2 instance the ability to securely store and retrieve a database password for your application.\n\nSolution\n\nCreate a password, store the password in Secrets Manager, create an IAM Policy with access to the secret, and grant an EC2 instance profile access to the secret, as shown in Figure 1-10.",
      "content_length": 1823,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Figure 1-10. Create a secret and retrieve it via the EC2 instance\n\nPrerequisites\n\nVPC with isolated subnets and associated route tables.\n\nEC2 instance deployed. You will need the ability to connect to this for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a secret using the AWS CLI:\n\nRANDOM_STRING=$(aws secretsmanager get-random-password \\\n\n--password-length 32 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\n2.\n\nStore it as a new secret in Secrets Manager:\n\nSECRET_ARN=$(aws secretsmanager \\\n\ncreate-secret --name AWSCookbook108/Secret1 \\\n\n--description \"AWSCookbook108 Secret 1\" \\",
      "content_length": 680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "3.\n\n4.\n\n5.\n\n--secret-string $RANDOM_STRING \\\n\n--output text \\\n\n--query ARN)\n\nCreate a file called secret-access-policy-template.json that references the secret you created. (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"secretsmanager:GetResourcePolicy\",\n\n\"secretsmanager:GetSecretValue\",\n\n\"secretsmanager:DescribeSecret\",\n\n\"secretsmanager:ListSecretVersionIds\"\n\n],\n\n\"Resource\": [\n\n\"SECRET_ARN\"\n\n]\n\n},\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": \"secretsmanager:ListSecrets\",\n\n\"Resource\": \"*\"\n\n}\n\n]\n\n}\n\nUse the sed command to replace SECRET_ARN in the secret-access-policy- template.json file and generate the secret-access-policy.json file:\n\nsed -e \"s|SECRET_ARN|$SECRET_ARN|g\" \\\n\nsecret-access-policy-template.json > secret-access-policy.json\n\nCreate the IAM policy for secret access:\n\naws iam create-policy --policy-name AWSCookbook108SecretAccess \\\n\n--policy-document file://secret-access-policy.json",
      "content_length": 965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "You should see output similar to the following:\n\n{\n\n\"Policy\": {\n\n\"PolicyName\": \"AWSCookbook108SecretAccess\",\n\n\"PolicyId\": \"(Random String)\",\n\n\"Arn\": \"arn:aws:iam::1111111111:policy/AWSCookbook108SecretAccess\",\n\n\"Path\": \"/\",\n\n\"DefaultVersionId\": \"v1\",\n\n\"AttachmentCount\": 0,\n\n\"PermissionsBoundaryUsageCount\": 0,\n\n\"IsAttachable\": true,\n\n\"CreateDate\": \"2021-11-28T21:25:23+00:00\",\n\n\"UpdateDate\": \"2021-11-28T21:25:23+00:00\"\n\n}\n\n}\n\n6.\n\nGrant an EC2 instance ability to access the secret by adding the IAM policy you created to the EC2 instance profile’s currently attached IAM role:\n\naws iam attach-role-policy --policy-arn \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook108SecretAccess \\\n\n--role-name $ROLE_NAME\n\nValidation checks\n\nConnect to the EC2 instance:\n\naws ssm start-session --target $INSTANCE_ID\n\nSet and export your default region:\n\nexport AWS_DEFAULT_REGION=us-east-1\n\nRetrieve the secret from Secrets Manager from the EC2:",
      "content_length": 935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "aws secretsmanager get-secret-value --secret-id AWSCookbook108/Secret1\n\nYou should see output similar to the following:\n\n{\n\n\"Name\": \"AWSCookbook108/Secret1\",\n\n\"VersionId\": \"<string>\",\n\n\"SecretString\": \"<secret value>\",\n\n\"VersionStages\": [\n\n\"AWSCURRENT\"\n\n],\n\n\"CreatedDate\": 1638221015.646,\n\n\"ARN\": \"arn:aws:secretsmanager:us-east-1:111111111111:secret:AWSCookbook108/Secret1-<suffix>\"\n\n}</suffix>\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nSecurely creating, storing, and managing the lifecycle of secrets, like API keys and database passwords, is a fundamental component to a strong security posture in the cloud. You can use Secrets Manager to implement a secrets management strategy that supports your security strategy. You can control who has access to what secrets using IAM policies to ensure the secrets you manage are accessible by only the necessary security principals.\n\nSince your EC2 instance uses an instance profile, you do not need to store any hard-coded credentials on the instance in order for it to access the secret. The access is granted via the IAM policy attached to the instance profile. Each time you (or your application) access the secret from the EC2 instance, temporary session credentials are obtained from the STS service to allow the get-secret-value API call to retrieve the secret. The AWS CLI automates this process of token retrieval when an EC2 instance profile is attached to your instance. You can also use the AWS SDK within your applications to achieve this functionality.\n\nSome additional benefits to using Secrets Manager include the following:\n\nEncrypting secrets with KMS keys that you create and manage",
      "content_length": 1742,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Auditing access to secrets through CloudTrail\n\nAutomating secret rotation using Lambda\n\nGranting access to other users, roles, and services like EC2 and Lambda\n\nReplicating secrets to another Region for high availability and disaster recovery purposes\n\nChallenge\n\nConfigure a Lambda function to access the secret securely with an IAM role.\n\n1.9 Blocking Public Access for an S3 Bucket\n\nProblem\n\nYou have been alerted by your organization’s security team that an S3 bucket has been incorrectly configured and you need to block public access to it.\n\nSolution\n\nApply the Amazon S3 Block Public Access feature to your bucket, and then check the status with the Access Analyzer (see Figure 1-11).\n\nTIP\n\nAWS provides information on what is considered “public” in an article on S3 storage.",
      "content_length": 782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Figure 1-11. Blocking public access to an S3 bucket\n\nPrerequisite\n\nS3 bucket with publicly available object(s)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate an Access Analyzer to use for validation of access:",
      "content_length": 268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "2.\n\n3.\n\nANALYZER_ARN=$(aws accessanalyzer create-analyzer \\\n\n--analyzer-name awscookbook109\\\n\n--type ACCOUNT \\\n\n--output text --query arn)\n\nPerform a scan of your S3 bucket with the Access Analyzer:\n\naws accessanalyzer start-resource-scan \\\n\n--analyzer-arn $ANALYZER_ARN \\\n\n--resource-arn arn:aws:s3:::awscookbook109-$RANDOM_STRING\n\nGet the results of the Access Analyzer scan (it may take about 30 seconds for the scan results to become available):\n\naws accessanalyzer get-analyzed-resource \\\n\n--analyzer-arn $ANALYZER_ARN \\\n\n--resource-arn arn:aws:s3:::awscookbook109-$RANDOM_STRING\n\nYou should see output similar to the following (note the isPublic value):\n\n{\n\n\"resource\": {\n\n\"actions\": [\n\n\"s3:GetObject\",\n\n\"s3:GetObjectVersion\"\n\n],\n\n\"analyzedAt\": \"2021-06-26T17:42:00.861000+00:00\",\n\n\"createdAt\": \"2021-06-26T17:42:00.861000+00:00\",\n\n\"isPublic\": true,\n\n\"resourceArn\": \"arn:aws:s3:::awscookbook109-<<string>>\",\n\n\"resourceOwnerAccount\": \"111111111111\",\n\n\"resourceType\": \"AWS::S3::Bucket\",\n\n\"sharedVia\": [\n\n\"POLICY\"\n\n],\n\n\"status\": \"ACTIVE\",\n\n\"updatedAt\": \"2021-06-26T17:42:00.861000+00:00\"\n\n}\n\n}",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "4.\n\nSet the public access block for your bucket:\n\naws s3api put-public-access-block \\\n\n--bucket awscookbook109-$RANDOM_STRING \\\n\n--public-access-block-configuration \\\n\n\"BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPubli\n\ncBuckets=true\"\n\nNOTE\n\nSee the AWS article on the available PublicAccessBlock configuration properties.\n\nValidation checks\n\nPerform a scan of your S3 bucket:\n\naws accessanalyzer start-resource-scan \\\n\n--analyzer-arn $ANALYZER_ARN \\\n\n--resource-arn arn:aws:s3:::awscookbook109-$RANDOM_STRING\n\nGet the results of the Access Analyzer scan:\n\naws accessanalyzer get-analyzed-resource \\\n\n--analyzer-arn $ANALYZER_ARN \\\n\n--resource-arn arn:aws:s3:::awscookbook109-$RANDOM_STRING\n\nYou should see output similar to the following:\n\n{\n\n\"resource\": {\n\n\"analyzedAt\": \"2021-06-26T17:46:24.906000+00:00\",\n\n\"isPublic\": false,\n\n\"resourceArn\": \"arn:aws:s3:::awscookbook109-<<string>>\",\n\n\"resourceOwnerAccount\": \"111111111111\",\n\n\"resourceType\": \"AWS::S3::Bucket\"",
      "content_length": 992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "}\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nOne of the best things you can do to ensure data security in your AWS account is to always make certain that you apply the right security controls to your data. If you mark an object as public in your S3 bucket, it is accessible to anyone on the internet, since S3 serves objects using HTTP. One of the most common security misconfigurations that users make in the cloud is marking object(s) as public when that is not intended or required. To protect against misconfiguration of S3 objects, enabling BlockPublicAccess for your buckets is a great thing to do from a security standpoint.\n\nTIP\n\nYou can also set public block settings at your account level, which would include all S3 buckets in your account:\n\naws s3control put-public-access-block \\\n\n--public-access-block-configuration \\\n\nBlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true \\\n\n--account-id $AWS_ACCOUNT_ID\n\nYou can serve S3 content to internet users via HTTP and HTTPS while keeping your bucket private. Content delivery networking (CDN), like Amazon CloudFront, provides more secure, efficient, and cost-effective ways to achieve global static website hosting and still use S3 as your object source. To see an example of a CloudFront configuration that serves static content from an S3 bucket, see Recipe 1.10.\n\nChallenge\n\nDeploy a VPC endpoint for S3 within your VPC and create a bucket policy to restrict access to your S3 bucket through this endpoint only.\n\n1.10 Serving Web Content Securely from S3 with CloudFront\n\nProblem\n\nYou have nonpublic web content in S3 and want to configure CloudFront to serve the content.",
      "content_length": 1729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Solution\n\nCreate a CloudFront distribution and set the origin to your S3 bucket. Then configure an origin access identity (OAI) to require the bucket to be accessible only from CloudFront (see Figure 1- 12).\n\nFigure 1-12. CloudFront and S3\n\nPrerequisite\n\nS3 bucket with static web content\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a CloudFront OAI to reference in an S3 bucket policy:\n\nOAI=$(aws cloudfront create-cloud-front-origin-access-identity \\\n\n--cloud-front-origin-access-identity-config \\\n\nCallerReference=\"awscookbook\",Comment=\"AWSCookbook OAI\" \\\n\n--query CloudFrontOriginAccessIdentity.Id --output text)\n\n2.\n\nUse the sed command to replace the values in the distribution-config- template.json file with your CloudFront OAI and S3 bucket name:\n\nsed -e \"s/CLOUDFRONT_OAI/${OAI}/g\" \\",
      "content_length": 855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "3.\n\n4.\n\n5.\n\n6.\n\ne \"s|S3_BUCKET_NAME|awscookbook110-$RANDOM_STRING|g\" \\\n\ndistribution-template.json > distribution.json\n\nCreate a CloudFront distribution that uses the distribution configuration JSON file you just created:\n\nDISTRIBUTION_ID=$(aws cloudfront create-distribution \\\n\n--distribution-config file://distribution.json \\\n\n--query Distribution.Id --output text)\n\nThe distribution will take a few minutes to create; use this command to check the status. Wait until the status reaches “Deployed”:\n\naws cloudfront get-distribution --id $DISTRIBUTION_ID \\\n\n--output text --query Distribution.Status\n\nConfigure the S3 bucket policy to allow only requests from CloudFront by using a bucket policy like this (we have provided bucket-policy- template.json in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Id\": \"PolicyForCloudFrontPrivateContent\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity\n\nCLOUDFRONT_OAI\"\n\n},\n\n\"Action\": \"s3:GetObject\",\n\n\"Resource\": \"arn:aws:s3:::S3_BUCKET_NAME/*\"\n\n}\n\n]\n\n}\n\nUse the sed command to replace the values in the bucket-policy- template.json file with the CloudFront OAI and S3 bucket name:",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "sed -e \"s/CLOUDFRONT_OAI/${OAI}/g\" \\\n\ne \"s|S3_BUCKET_NAME|awscookbook110-$RANDOM_STRING|g\" \\\n\nbucket-policy-template.json > bucket-policy.json\n\n7.\n\nApply the bucket policy to the S3 bucket with your static web content:\n\naws s3api put-bucket-policy --bucket awscookbook110-$RANDOM_STRING \\\n\n--policy file://bucket-policy.json\n\n8.\n\nGet the DOMAIN_NAME of the distribution you created:\n\nDOMAIN_NAME=$(aws cloudfront get-distribution --id $DISTRIBUTION_ID \\\n\n--query Distribution.DomainName --output text)\n\nValidation checks\n\nTry to access the S3 bucket directly using HTTPS to verify that the bucket does not serve content directly:\n\ncurl https://awscookbook110-$RANDOM_STRING.s3.$AWS_REGION.amazonaws.com/index.html\n\nYou should see output similar to the following:\n\n$ curl https://awscookbook110-$RANDOM_STRING.s3.$AWS_REGION.amazonaws.com/index.html\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<Error><Code>AccessDenied</Code><Message>Access\n\nDenied</Message><RequestId>0AKQD0EFJC9ZHPCC</RequestId>\n\n<HostId>gfld4qKp9A93G8ee7VPBFrXBZV1HE3jiOb3bNB54fP\n\nEPTihit/OyFh7hF2Nu4+Muv6JEc0ebLL4=</HostId></Error>\n\n110-Optimizing-S3-with-CloudFront:$\n\nUse curl to observe that your index.html file is served from the private S3 bucket through CloudFront:",
      "content_length": 1240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "curl $DOMAIN_NAME\n\nYou should see output similar to the following:\n\n$ curl $DOMAIN_NAME\n\nAWSCookbook\n\n$\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThis configuration allows you to keep the S3 bucket private and allows only the CloudFront distribution to be able to access objects in the bucket. You created an origin access identity and defined a bucket policy to allow only CloudFront access to your S3 content. This gives you a solid foundation to keep your S3 buckets secure with the additional protection of the CloudFront global CDN.\n\nThe protection that a CDN gives from a distributed-denial-of-service (DDoS) attack is worth noting, as the end user requests to your content are directed to a point of presence on the CloudFront network with the lowest latency. This also protects you from the costs of having a DDoS attack against static content hosted in an S3 bucket, as it is generally less expensive to serve requests out of CloudFront rather than S3 directly.\n\nBy default, CloudFront comes with an HTTPS certificate on the default hostname for your distribution that you use to secure traffic. With CloudFront, you can associate your own custom domain name, attach a custom certificate from Amazon Certificate Manager (ACM), redirect to HTTPS from HTTP, force HTTPS, customize cache behavior, invoke Lambda functions (Lambda @Edge), and more.\n\nChallenge\n\nAdd a geo restriction to your CloudFront distribution.",
      "content_length": 1474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "Chapter 2. Networking\n\n2.0 Introduction Many exciting topics, like computer vision, Internet of Things (IoT), and AI-enabled chat bots, dominate headlines. This causes traditional core technologies to be forgotten. While it’s great to have many new capabilities at your fingertips, it is important to realize that these technologies would not be possible without a strong foundation of reliable and secure connectivity. Data processing is useful only if the results are reliably delivered and accessible over a network. Containers are a fantastic application deployment method, but they provide the best experience for users when they are networked together.\n\nNetworking services and features within AWS are the backbone to almost all of the big services we cover in this book. AWS has many great features for you to connect what you want, where you want, and how you want. Gaining a better understanding of networking will allow you to have a better grasp of the cloud and therefore to be more comfortable using it.\n\nTIP\n\nKeeping up with new networking innovations at AWS requires continuous learning. Each year at AWS re:Invent, many\n\nnetwork services, features, and approaches are discussed.\n\nTwo suggested viewings of great networking talks from AWS re:Invent are Eric Brandwine’s “Another Day, Another\n\nBillion Packets” from 2015 and the annual “From One to Many: Evolving VPC Design” from 2019.\n\nIn this chapter, you will learn about essential cloud networking services and features. We will focus only on recipes that are realistic for you to accomplish in your personal account. Some advanced operations (e.g., AWS Direct Connect setup) are too dependent on external factors, so we left them out in order to focus on more easily accessible recipes and outcomes. While some recipes in this chapter may seem simple or short, they allow us to discuss important topics and concepts that are crucial to get right.\n\nWorkstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Networking",
      "content_length": 2177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "2.1 Defining Your Private Virtual Network in the Cloud by Creating an Amazon VPC\n\nProblem\n\nYou need a network foundation to host cloud resources.\n\nSolution\n\nYou will create an Amazon Virtual Private Cloud (VPC) and configure a Classless Inter-Domain Routing (CIDR) block for it, as shown in Figure 2-1.\n\nFigure 2-1. VPC deployed in a Region\n\nSteps\n\n1.\n\nCreate a VPC with an IPv4 CIDR block. We will use 10.10.0.0/16 as the address range, but you can modify it based on your needs:",
      "content_length": 480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "VPC_ID=$(aws ec2 create-vpc --cidr-block 10.10.0.0/16 \\\n\n--tag-specifications\n\n'ResourceType=vpc,Tags=[{Key=Name,Value=AWSCookbook201}]' \\\n\n--output text --query Vpc.VpcId)\n\nNOTE\n\nWhen you are creating a VPC, the documentation states that the largest block size for VPC IPv4\n\nCIDRs is a /16 netmask (65,536 IP addresses). The smallest is a /28 netmask (16 IP addresses).\n\nValidation checks\n\nUse this command to verify that the VPC’s state is available:\n\naws ec2 describe-vpcs --vpc-ids $VPC_ID\n\nYou should see output similar to the following:\n\n{\n\n\"Vpcs\": [\n\n{\n\n\"CidrBlock\": \"10.10.0.0/16\",\n\n\"DhcpOptionsId\": \"dopt-<<snip>>\",\n\n\"State\": \"available\",\n\n\"VpcId\": \"vpc-<<snip>>\",\n\n\"OwnerId\": \"111111111111\",\n\n\"InstanceTenancy\": \"default\",\n\n\"CidrBlockAssociationSet\": [\n\n{\n\n\"AssociationId\": \"vpc-cidr-assoc-<<snip>>\",\n\n\"CidrBlock\": \"10.10.0.0/16\",\n\n\"CidrBlockState\": {\n\n\"State\": \"associated\"\n\n}\n\n}\n\n],\n\n\"IsDefault\": false,\n\n<<snip>>\n\n...",
      "content_length": 930,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWARNING\n\nHere are two important reasons for carefully selecting CIDR block(s) for your VPC:\n\nOnce a CIDR block is associated with a VPC, it can’t be modified (although it can be extended). If you wish\n\nto modify a CIDR block, it (and all resources within it) will need to be deleted and re-created.\n\nIf a VPC is connected to other networks by peering (see Recipe 2.11) or gateways (e.g., Transit and VPN),\n\nhaving overlapping IP ranges will cause unwanted problems.\n\nYou can add IPv4 space to the VPC by using the aws ec2 associate-vpc-cidr-block command to specify the additional IPv4 space. When IP space becomes scarce from usage and under-provisioning, it’s good to know that you don’t need to dedicate a large block to a VPC, especially if you aren’t sure if all of it will be utilized.\n\nHere is an example of associating an additional IPv4 CIDR block to your VPC:\n\naws ec2 associate-vpc-cidr-block \\\n\n--cidr-block 10.11.0.0/16 \\\n\n--vpc-id $VPC_ID\n\nNOTE\n\nIn addition to IPv4, VPC also supports IPv6. You can configure an Amazon-provided IPv6 CIDR block by specifying the\n\n--amazon-provided-ipv6-cidr-block option. Here is an example of creating a VPC with an IPv6 CIDR block:\n\naws ec2 create-vpc --cidr-block 10.10.0.0/16 \\\n\n--amazon-provided-ipv6-cidr-block \\\n\n--tag-specifications\n\n'ResourceType=vpc,Tags=[{Key=Name,Value=AWSCookbook201-IPv6}]'\n\nA VPC is a Regional construct in AWS. A Region is a geographical area, and Availability Zones are physical data centers that reside within a Region. Regions span all Availability Zones (AZs),",
      "content_length": 1639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "which are groups of isolated physical data centers. The number of AZs per Region varies, but all Regions have at least three. For the most up-to-date information about AWS Regions and AZs, see this article on “Regions and Availability Zones”.\n\nNOTE\n\nPer the VPC user guide, the initial quota of IPv4 CIDR blocks per VPC is 5. This can be raised to 50. The allowable\n\nnumber of IPv6 CIDR blocks per VPC is 1.\n\nChallenge\n\nCreate another VPC with a different CIDR range.\n\n2.2 Creating a Network Tier with Subnets and a Route Table in a VPC\n\nProblem\n\nYou have a VPC and need to create a network layout consisting of individual IP spaces for segmentation and redundancy.\n\nSolution\n\nCreate a route table within your VPC. Create two subnets in separate Availability Zones in a VPC. Associate the route table with the subnets (see Figure 2-2).",
      "content_length": 835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Figure 2-2. Isolated subnet tier and route table\n\nPrerequisite\n\nA VPC\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a route table. This will allow you to create customized traffic routes for subnets associated with it:\n\nROUTE_TABLE_ID=$(aws ec2 create-route-table --vpc-id $VPC_ID \\\n\n--tag-specifications \\",
      "content_length": 366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "2.\n\n3.\n\n4.\n\n'ResourceType=route-table,Tags=[{Key=Name,Value=AWSCookbook202}]' \\\n\n--output text --query RouteTable.RouteTableId)\n\nCreate two subnets, one in each AZ. This will define the address spaces for you to create resources for your VPC:\n\nSUBNET_ID_1=$(aws ec2 create-subnet --vpc-id $VPC_ID \\\n\n--cidr-block 10.10.0.0/24 --availability-zone ${AWS_REGION}a \\\n\n--tag-specifications \\\n\n'ResourceType=subnet,Tags=[{Key=Name,Value=AWSCookbook202a}]' \\\n\n--output text --query Subnet.SubnetId)\n\nSUBNET_ID_2=$(aws ec2 create-subnet --vpc-id $VPC_ID \\\n\n--cidr-block 10.10.1.0/24 --availability-zone ${AWS_REGION}b \\\n\n--tag-specifications \\\n\n'ResourceType=subnet,Tags=[{Key=Name,Value=AWSCookbook202b}]' \\\n\n--output text --query Subnet.SubnetId)\n\nNOTE\n\nIn the previous commands, the --availability-zone parameter uses an environment variable for your Region appended with lowercase a or b characters to indicate which logical AZ (e.g., us-east-1a) to provision each subnet. AWS states that these names are randomized per account to balance\n\nresources across AZs.\n\nIf you are using multiple AWS accounts and want to find Availability Zone IDs for a Region that are\n\nconsistent, run this command:\n\naws ec2 describe-availability-zones --region $AWS_REGION\n\nAssociate the route table with the two subnets:\n\naws ec2 associate-route-table \\\n\n--route-table-id $ROUTE_TABLE_ID --subnet-id $SUBNET_ID_1\n\naws ec2 associate-route-table \\\n\n--route-table-id $ROUTE_TABLE_ID --subnet-id $SUBNET_ID_2\n\nFor each command in step 3, you should see output similar to the following:",
      "content_length": 1557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "{\n\n\"AssociationId\": \"rtbassoc-<<snip>>\",\n\n\"AssociationState\": {\n\n\"State\": \"associated\"\n\n}\n\n}\n\nValidation checks\n\nRetrieve the configuration of the subnets you created and verify that they are in the same VPC but different AZs:\n\naws ec2 describe-subnets --subnet-ids $SUBNET_ID_1\n\naws ec2 describe-subnets --subnet-ids $SUBNET_ID_2\n\nFor each describe-subnets command, you should see output similar to this:\n\n{\n\n\"Subnets\": [\n\n{\n\n\"AvailabilityZone\": \"us-east-1a\",\n\n\"AvailabilityZoneId\": \"use1-az6\",\n\n\"AvailableIpAddressCount\": 251,\n\n\"CidrBlock\": \"10.10.0.0/24\",\n\n\"DefaultForAz\": false,\n\n\"MapPublicIpOnLaunch\": false,\n\n\"MapCustomerOwnedIpOnLaunch\": false,\n\n\"State\": \"available\",\n\n\"SubnetId\": \"subnet-<<snip>>\",\n\n\"VpcId\": \"vpc-<<snip>>\",\n\n\"OwnerId\": \"111111111111\",\n\n\"AssignIpv6AddressOnCreation\": false,\n\n\"Ipv6CidrBlockAssociationSet\": [],\n\n<<snip>>\n\n...\n\nValidate that the route table you created is associated with the two subnets:\n\naws ec2 describe-route-tables --route-table-ids $ROUTE_TABLE_ID",
      "content_length": 994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "You should see output similar to the following:\n\n{\n\n\"RouteTables\": [\n\n{\n\n\"Associations\": [\n\n{\n\n\"Main\": false,\n\n\"RouteTableAssociationId\": \"rtbassoc-<<snip>>\",\n\n\"RouteTableId\": \"rtb-<<snip>>\",\n\n\"SubnetId\": \"subnet-<<snip>>\",\n\n\"AssociationState\": {\n\n\"State\": \"associated\"\n\n}\n\n},\n\n{\n\n\"Main\": false,\n\n\"RouteTableAssociationId\": \"rtbassoc-<<snip>>\",\n\n\"RouteTableId\": \"rtb-<<snip>>\",\n\n\"SubnetId\": \"subnet-<<snip>>\",\n\n\"AssociationState\": {\n\n\"State\": \"associated\"\n\n}\n\n}\n\n<<snip>>\n\n...\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen designing a subnet strategy, you should choose subnet sizes that fit your current needs and account for your application’s future growth. Subnets are used for elastic network interface (ENI) placement for AWS resources. This means that a particular ENI lives within a single AZ.\n\nTIP\n\nYou may run into a case where routes overlap. AWS provides information on how priority is determined.",
      "content_length": 964,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "AWS reserves the first four and last IP addresses of every subnet’s CIDR block for features and functionality when you create a subnet. These are not available for your use. Per the documentation, these are the reserved addresses in the case of your example:\n\n.0 Network address.\n\n.1 Reserved by AWS for the VPC router.\n\n.2 Reserved by AWS for the IP address of the DNS server. This is always set to the VPC network range plus two.\n\n.3 Reserved by AWS for future use.\n\n.255 Network broadcast address. Broadcast in a VPC is not supported.\n\nA subnet has one route table associated with it. Route tables can be associated with one or more subnets and direct traffic to a destination of your choosing (more on this with the NAT gateway, internet gateway, and transit gateway recipes later). Entries within route tables are called routes and are defined as pairs of Destinations and Targets. When you created the route table, a default local route that handles intra-VPC traffic was automatically added for you. You have the ability to create custom routes that fit your needs. For a complete list of targets available to use within route tables, see this support document.\n\nNOTE\n\nENIs receive an IP address from an AWS managed DHCP server within your VPC. The DHCP options set is automatically\n\nconfigured with defaults for assigning addresses within the subnets you define. For more information about DHCP option\n\nsets, and how to create your own DHCP option sets see this support document.\n\nWhen creating a VPC in a Region, it is a best practice to spread subnets across AZs in that network tier. The number of AZs differs per Region, but most have at least three. An example of this in practice would be that if you had a public tier and an isolated tier spread over two AZs, you would have a total of four subnets: 2 tiers × 2 subnets per tier (one per AZ); see Figure 2-3.",
      "content_length": 1873,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Figure 2-3. Isolated and public subnet tiers and route tables",
      "content_length": 61,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "Challenge\n\nCreate a second route table and associate it with $SUBNET_ID_2. Configuring route tables for each AZ is a common pattern.\n\n2.3 Connecting Your VPC to the Internet Using an Internet Gateway\n\nProblem\n\nYou have an existing EC2 instance in a subnet of a VPC. You need to provide the ability for this instance to directly reach clients on the internet.\n\nSolution\n\nYou will create an internet gateway and attach it to your VPC. Next you will modify the route table associated with the subnet where the EC2 instance lives. You will add a route that sends traffic from the subnets to the internet gateway. Finally, create an Elastic IP (EIP) and associate it with the instance, as shown in Figure 2-4.",
      "content_length": 704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Prerequisites\n\nPreparation\n\nFigure 2-4. Public subnet tier, internet gateway, and route table\n\nVPC and subnets created in two AZs and associated route tables.\n\nEC2 instance deployed. You will need the ability to connect to this for testing.",
      "content_length": 240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "Follow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate an internet gateway (IGW):\n\nINET_GATEWAY_ID=$(aws ec2 create-internet-gateway \\\n\n--tag-specifications \\\n\n'ResourceType=internet-gateway,Tags=[{Key=Name,Value=AWSCookbook202}]' \\\n\n--output text --query InternetGateway.InternetGatewayId)\n\n2.\n\nAttach the internet gateway to the existing VPC:\n\naws ec2 attach-internet-gateway \\\n\n--internet-gateway-id $INET_GATEWAY_ID --vpc-id $VPC_ID\n\n3.\n\nIn each route table of your VPC, create a route that sets the default route destination to the internet gateway:\n\naws ec2 create-route --route-table-id $ROUTE_TABLE_ID_1 \\\n\n--destination-cidr-block 0.0.0.0/0 --gateway-id $INET_GATEWAY_ID\n\naws ec2 create-route --route-table-id $ROUTE_TABLE_ID_2 \\\n\n--destination-cidr-block 0.0.0.0/0 --gateway-id $INET_GATEWAY_ID\n\nFor each command in step 3, you should see output similar to the following:\n\n{\n\n\"Return\": true\n\n}\n\n4.\n\nCreate an EIP:\n\nALLOCATION_ID=$(aws ec2 allocate-address --domain vpc \\\n\n--output text --query AllocationId)",
      "content_length": 1054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "NOTE\n\nAWS defines an Elastic IP address (EIP) as “a static IPv4 address designed for dynamic cloud\n\ncomputing. An EIP address is allocated to your AWS account and is yours until you release it.”\n\n5.\n\nAssociate the EIP with the existing EC2 instance:\n\naws ec2 associate-address \\\n\n--instance-id $INSTANCE_ID --allocation-id $ALLOCATION_ID\n\nYou should see output similar to the following:\n\n{\n\n\"AssociationId\": \"eipassoc-<<snip>>\"\n\n}\n\nValidation checks\n\nConnect to the EC2 instance using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nPing a host on the internet to test internet connectivity:\n\nping -c 4 homestarrunner.com\n\nYou should see output similar to the following:\n\nsh-4.2$ ping -c 4 homestarrunner.com\n\nPING homestarrunner.com (72.10.33.178) 56(84) bytes of data.\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=1 ttl=49 time=2.12 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=2 ttl=49 time=2.04 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=3 ttl=49 time=2.05 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=4 ttl=49 time=2.08 ms\n\n--- homestarrunner.com ping statistics ---",
      "content_length": 1174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "4 packets transmitted, 4 received, 0% packet loss, time 3002ms\n\nrtt min/avg/max/mdev = 2.045/2.078/2.127/0.045 ms\n\nsh-4.2$\n\nTIP\n\nThe public IP is not part of the OS configuration. If you want to retrieve the public IP from the instance’s metadata, you\n\ncan use this command:\n\ncurl http://169.254.169.254/latest/meta-data/public-ipv4\n\nExit the Session Manager session:\n\nexit\n\nClean up\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThe route that you created in your route table entry sends all nonlocal traffic to the IGW that provides your VPC internet connectivity. Because you were working with an existing running EC2 instance, you needed to create an Elastic IP and associated it with the instance. These steps enabled internet communication to the instance without having to interact with it. There is an option to enable auto-assignment of public IPv4 addresses for newly launched instances in a subnet. However, if you utilize auto-assignment, the public IPs will change after each instance reboot. EIPs associated with an instance will not change after reboots.\n\nNOTE\n\nRoute tables give priority to the most specific route. AWS also allows you to create routes that are more specific than the\n\ndefault local route. This allows you to create very controlled network flows. More information about route priority can be\n\nfound in an AWS discussion.\n\nThe security group associated with your instance does not allow inbound access. If you would like to allow inbound internet access to an instance in a public subnet, you will have to configure a security group ingress rule for this.",
      "content_length": 1628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "A subnet that has a route of 0.0.0.0/0 associated with an IGW is considered a public subnet. It is considered a security best practice to place instances only in this type of tier that require inbound access from the public internet. End-user-facing load balancers are commonly placed in public subnets. A public subnet would not be an ideal choice for an application server or a database. In these cases, you can create a private tier or an isolated tier to fit your needs with the appropriate routing and use a NAT gateway to direct that subnet traffic to the internet gateway only when outbound internet access is required.\n\nChallenge\n\nInstall a web server on the EC2 instance, modify the security group, and connect to the instance from your workstation. See Recipe 2.7 for an example of how to configure internet access for instances in private subnets using a load balancer.\n\n2.4 Using a NAT Gateway for Outbound Internet Access from Private Subnets\n\nProblem\n\nYou have public subnets in your VPC that have a route to an internet gateway. You want to leverage this setup to provide outbound-only internet access for an instance in private subnets.\n\nSolution\n\nCreate a NAT gateway in one of the public subnets. Then create an Elastic IP and associate it with the NAT gateway. In the route table associated with the private subnets, add a route for internet-bound traffic that targets the NAT gateway (see Figure 2-5).\n\nPrerequisites\n\nVPC with public subnets in two AZs and associated route tables.\n\nIsolated subnets created in two AZs (we will turn these into the private subnets) and associated route tables.\n\nTwo EC2 instances deployed in the isolated subnets. You will need the ability to connect to these for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.",
      "content_length": 1812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Steps\n\n1.\n\n2.\n\n3.\n\n4.\n\nFigure 2-5. Internet access for private subnets provided by NAT gateways\n\nCreate an Elastic IP to be used with the NAT gateway:\n\nALLOCATION_ID=$(aws ec2 allocate-address --domain vpc \\\n\n--output text --query AllocationId)\n\nCreate a NAT gateway within the public subnet of AZ1:\n\nNAT_GATEWAY_ID=$(aws ec2 create-nat-gateway \\\n\n--subnet-id $VPC_PUBLIC_SUBNET_1 \\\n\n--allocation-id $ALLOCATION_ID \\\n\n--output text --query NatGateway.NatGatewayId)\n\nThis will take a few moments for the state to become available; check the status:\n\naws ec2 describe-nat-gateways \\\n\n--nat-gateway-ids $NAT_GATEWAY_ID \\\n\n--output text --query NatGateways[0].State\n\nAdd a default route for 0.0.0.0/0 with a destination of the NAT gateway to both of the private tier’s route tables. This default route sends all traffic not matching a more specific route to the destination specified:\n\naws ec2 create-route --route-table-id $PRIVATE_RT_ID_1 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--nat-gateway-id $NAT_GATEWAY_ID\n\naws ec2 create-route --route-table-id $PRIVATE_RT_ID_2 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--nat-gateway-id $NAT_GATEWAY_ID",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "For each command in step 4, you should see output similar to the following:\n\n{\n\n\"Return\": true\n\n}\n\nValidation checks\n\nConnect to EC2 instance 1 by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID_1\n\nTest internet access with a ping:\n\nping -c 4 homestarrunner.com\n\nYou should see output similar to the following:\n\nsh-4.2$ ping -c 4 homestarrunner.com\n\nPING homestarrunner.com (72.10.33.178) 56(84) bytes of data.\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=1 ttl=47 time=2.95 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=2 ttl=47 time=2.16 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=3 ttl=47 time=2.13 ms\n\n64 bytes from homestarrunner.com (72.10.33.178): icmp_seq=4 ttl=47 time=2.13 ms\n\n--- homestarrunner.com ping statistics ---\n\n4 packets transmitted, 4 received, 0% packet loss, time 3003ms\n\nrtt min/avg/max/mdev = 2.134/2.348/2.958/0.356 ms\n\nsh-4.2$\n\nExit the Session Manager session:\n\nexit",
      "content_length": 984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "(Optional) Repeat the validation steps for EC2 instance 2.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThis architecture gives you a subnet tier that allows outbound access but does not permit direct inbound internet access to resources within it. One way to allow internet resources inbound access to services running on resources in private subnets is to use a load balancer in the public subnets. We’ll look more at that type of configuration in Recipe 2.7.\n\nThe EIP associated with your NAT gateway becomes the external IP address for all communication that goes through it. For example, if a vendor needed to add your IP to an allow list, the NAT gateway EIP would be the “source” IP address provided to the vendor. Your EIP will remain the same as long as you keep it provisioned within your account.\n\nTIP\n\nIf you created a VPC with IPv6 capability, you can also create an egress-only internet gateway to allow outbound internet\n\naccess for private subnets, as discussed in an AWS article.\n\nThis NAT gateway was provisioned within one AZ in your VPC. While this is a cost-effective way to achieve outbound internet access for your private subnets, for production and mission- critical applications, you should consider provisioning NAT gateways in each AZ to provide resiliency and reduce the amount of cross-AZ traffic. This would also require creating route tables for each of your private subnets so that you can direct the 0.0.0.0/0 traffic to the NAT gateway in that particular subnet’s AZ. See the challenge for this recipe.\n\nNOTE\n\nIf you have custom requirements or would like more granular control of your outbound routing for your NAT\n\nimplementation, you can create a NAT instance. For a comparison of NAT gateways and NAT instances, see this support\n\ndocument.\n\nChallenge\n\nCreate a second NAT gateway in the public subnet in AZ2. Then modify the default route in the route table associated with the private subnet in AZ2. Change the destination to the newly created NAT gateway.",
      "content_length": 2047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "2.5 Granting Dynamic Access by Referencing Security Groups\n\nProblem\n\nYou have an application group that currently consists of two instances and need to allow Secure Shell (SSH) between them. This needs to be configured in a way to allow for future growth of the number of instances securely and easily.\n\nSolution\n\nWARNING\n\nA common misconception is that by merely associating the same security group to ENIs for multiple EC2 instances, it will\n\nallow communication between them (see Figure 2-6).",
      "content_length": 495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Figure 2-6. Incorrect representation of two instances using the same security group\n\nIn this recipe, we will create a security group and an associate each to the ENIs of two EC2 instances. Finally, we will create an ingress rule that authorizes the security group to reach itself on TCP port 22 (see Figure 2-7).",
      "content_length": 312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "Prerequisites\n\nFigure 2-7. Correct visualization of the ENIs of two instances using the same security group",
      "content_length": 107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "VPC with a subnet and associated route table.\n\nTwo EC2 instances deployed in the subnet. You will need the ability to connect to these for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a new security group for the EC2 instances:\n\nSG_ID=$(aws ec2 create-security-group \\\n\n--group-name AWSCookbook205Sg \\\n\n--description \"Instance Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\n2.\n\nAttach the security group to instances 1 and 2:\n\naws ec2 modify-instance-attribute --instance-id $INSTANCE_ID_1 \\\n\n--groups $SG_ID\n\naws ec2 modify-instance-attribute --instance-id $INSTANCE_ID_2 \\\n\n--groups $SG_ID\n\nNOTE\n\nYou used the modify-instance-attribute command to attach a new security group to the ENIs of your\n\nEC2 instances. To list the security groups associated with the ENI of an EC2 instance, you can view\n\nthem in the EC2 console under the Security tab of the instance details or use this command (replacing\n\n$INSTANCE_ID_1 with your own instance ID):\n\naws ec2 describe-security-groups --group-ids \\\n\n$(aws ec2 describe-instances --instance-id $INSTANCE_ID_1 \\\n\n--query \"Reservations[].Instances[].SecurityGroups[].GroupId[]\" \\\n\n--output text) --output text\n\n3.\n\nAdd an ingress rule to the security group that allows access on TCP port 22 from itself:",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "aws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 22 \\\n\n--source-group $SG_ID \\\n\n--group-id $SG_ID \\\n\nTIP\n\nYou can and should create descriptions for all your security group rules to indicate the intended\n\nfunctionality of the authorization.\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<snip>>\",\n\n\"GroupId\": \"sg-<<snip>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 22,\n\n\"ToPort\": 22,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<snip>>\"\n\n}\n\n}\n\n]\n\n}\n\nTIP\n\nThis type of security group rule is called a self-referencing rule. It allows access to the specific port\n\nfrom traffic originating from ENIs (not a static range of IPs) that have this same security group\n\nattached to them.\n\nValidation checks\n\nList the IP address for instance 2:",
      "content_length": 878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "aws ec2 describe-instances --instance-ids $INSTANCE_ID_2 \\\n\n--output text \\\n\n--query Reservations[0].Instances[0].PrivateIpAddress\n\nConnect to your instance 1 by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID_1\n\nInstall the Ncat utility:\n\nsudo yum -y install nc\n\nTest SSH connectivity to instance 2 (use instance 2’s IP that you listed previously):\n\nnc -vz $INSTANCE_IP_2 22\n\nYou should see output similar to the following:\n\nNcat: Version 7.50 ( https://nmap.org/ncat )\n\nNcat: Connected to 10.10.0.48:22.\n\nNcat: 0 bytes sent, 0 bytes received in 0.01 seconds.\n\nsh-4.2$\n\nExit the Session Manager session:\n\nexit\n\n(Optional) Repeat the validation steps from instance 2 to instance 1.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.",
      "content_length": 805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Discussion\n\nThe on-demand nature of the cloud (e.g., autoscaling) presents an opportunity for elasticity. Network security mechanisms, like security group references, are suitable for that. Traditionally, network architects might authorize CIDR ranges within firewall configurations. This type of authorization is generally referred to as static references. This legacy practice doesn’t scale dynamically, as you may add or remove instances from your workloads.\n\nA security group acts as a stateful virtual firewall for ENIs. The default behavior for security groups is to implicitly block all ingress while allowing all egress. You can associate multiple security groups with an ENI. There is an initial quota of 5 security groups per ENI and 60 rules (inbound or outbound) per security group.\n\nYou can also specify CIDR notation for authorizations. For example, for an authorization intended to allow RDP access from your New York branch office, you would use the following:\n\naws ec2 authorize-security-group-ingress \\\n\n--group-id sg-1234567890abcdef0 \\\n\n--ip-permissions\n\nIpProtocol=tcp,FromPort=3389,ToPort=3389,IpRanges='[{CidrIp=XXX.XXX.XXX.XXX/24,Description=\"RDP access\n\nfrom NY office\"}]'\n\nWARNING\n\nRemember that security groups cannot be deleted if the following conditions are present:\n\nThey are currently attached to an ENI.\n\nThey are referenced by other security groups (including themselves).\n\nChallenge\n\nCreate a third EC2 instance; use the same security group. Test access to and from it. (Hint: in the repository.)\n\n2.6 Using VPC Reachability Analyzer to Verify and Troubleshoot Network Paths\n\nProblem\n\nYou have two EC2 instances deployed in isolated subnets. You need to troubleshoot SSH connectivity between them.",
      "content_length": 1732,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Solution\n\nYou will create, analyze, and describe network insights by using the VPC Reachability Analyzer. Based on the results, you will add a rule to the security group of instance 2 that allows the SSH port (TCP port 22) from instance 1’s security group. Finally, you will rerun the VPC Reachability Analyzer and view the updated results (see Figure 2-8).\n\nFigure 2-8. VPC Reachability Analyzer\n\nPrerequisites\n\nVPC with isolated subnets in two AZs and associated route tables.\n\nTwo EC2 instances deployed in the isolated subnets. You will need the ability to connect to these for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.",
      "content_length": 677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "Steps\n\n1.\n\n2.\n\n3.\n\nCreate a network insights path specifying both of the EC2 instances you deployed and TCP port 22:\n\nINSIGHTS_PATH_ID=$(aws ec2 create-network-insights-path \\\n\n--source $INSTANCE_ID_1 --destination-port 22 \\\n\n--destination $INSTANCE_ID_2 --protocol tcp \\\n\n--output text --query NetworkInsightsPath.NetworkInsightsPathId)\n\nStart the network insights analysis between the two instances using the INSIGHTS_PATH_ID created in the previous step:\n\nANALYSIS_ID_1=$(aws ec2 start-network-insights-analysis \\\n\n--network-insights-path-id $INSIGHTS_PATH_ID --output text \\\n\n--query NetworkInsightsAnalysis.NetworkInsightsAnalysisId)\n\nWait a few seconds until the analysis is done running and then view the results:\n\naws ec2 describe-network-insights-analyses \\\n\n--network-insights-analysis-ids $ANALYSIS_ID_1\n\nYou should see output similar to the following (note the NetworkPathFound and ExplanationCode fields):\n\n{\n\n\"NetworkInsightsAnalyses\": [\n\n{\n\n\"NetworkInsightsAnalysisId\": \"nia-<<snip>\",\n\n\"NetworkInsightsAnalysisArn\": \"arn:aws:ec2:us-east-1:111111111111:network-\n\ninsights-analysis/nia-<<snip>\",\n\n\"NetworkInsightsPathId\": \"nip-<<snip>\",\n\n\"StartDate\": \"2020-12-22T02:12:36.836000+00:00\",\n\n\"Status\": \"succeeded\",\n\n\"NetworkPathFound\": false,\n\n\"Explanations\": [\n\n{",
      "content_length": 1273,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "4.\n\n5.\n\n\"Direction\": \"ingress\",\n\n\"ExplanationCode\": \"ENI_SG_RULES_MISMATCH\",\n\n\"NetworkInterface\": {\n\n\"Id\": \"eni-<<snip>\",\n\n\"Arn\": \"arn:aws:ec2:us-east-1:11111111111:network-interface/eni-<<snip>\"\n\n},\n\nUpdate the security group attached to instance 2. Add a rule to allow access from instance 1’s security group to TCP port 22 (SSH):\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 22 \\\n\n--source-group $INSTANCE_SG_ID_1 \\\n\n--group-id $INSTANCE_SG_ID_2\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<snip>>\",\n\n\"GroupId\": \"sg-<<snip>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 22,\n\n\"ToPort\": 22,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<snip>>\"\n\n}\n\n}\n\n]\n\n}\n\nRerun the network insights analysis. Use the same INSIGHTS_PATH_ID as you did previously:\n\nANALYSIS_ID_2=$(aws ec2 start-network-insights-analysis \\",
      "content_length": 955,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "--network-insights-path-id $INSIGHTS_PATH_ID --output text \\\n\n--query NetworkInsightsAnalysis.NetworkInsightsAnalysisId)\n\n6.\n\nShow the results of the new analysis:\n\naws ec2 describe-network-insights-analyses \\\n\n--network-insights-analysis-ids $ANALYSIS_ID_2\n\nYou should see output similar to the following (note the NetworkPathFound field):\n\n{\n\n\"NetworkInsightsAnalyses\": [\n\n{\n\n\"NetworkInsightsAnalysisId\": \"nia-<<snip>>\",\n\n\"NetworkInsightsAnalysisArn\": \"arn:aws:ec2:us-east-1:111111111111:network-\n\ninsights-analysis/nia-<<snip>>\",\n\n\"NetworkInsightsPathId\": \"nip-<<snip>>\",\n\n\"StartDate\": \"2021-02-21T23:52:15.565000+00:00\",\n\n\"Status\": \"succeeded\",\n\n\"NetworkPathFound\": true,\n\n\"ForwardPathComponents\": [\n\n{\n\n\"SequenceNumber\": 1,\n\n\"Component\": {\n\n\"Id\": \"i-<<snip>>\",\n\n...\n\nValidation checks\n\nList the IP address for instance 2:\n\naws ec2 describe-instances --instance-ids $INSTANCE_ID_2 \\\n\n--output text \\\n\n--query Reservations[0].Instances[0].PrivateIpAddress\n\nConnect to your EC2 instance by using SSM Session Manager (see Recipe 1.6):",
      "content_length": 1035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "aws ssm start-session --target $INSTANCE_ID_1\n\nInstall the Ncat utility:\n\nsudo yum -y install nc\n\nTest SSH connectivity to instance 2 (use instance 2’s IP that you listed previously):\n\nnc -vz $INSTANCE_IP_2 22\n\nYou should see output similar to the following:\n\nNcat: Version 7.50 ( https://nmap.org/ncat )\n\nNcat: Connected to 10.10.0.48:22.\n\nNcat: 0 bytes sent, 0 bytes received in 0.01 seconds.\n\nsh-4.2$\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nA network insights path is a definition of the connectivity you want to test. Initially, there wasn’t SSH connectivity between the instances because the security group on the destination (instance 2) did not allow access. After you updated the security group associated with instance 2 and reran an analysis, you were able to verify successful connectivity. Using the VPC Reachability Analyzer is an efficient capability for network troubleshooting and validating configuration in a “serverless” manner. It does not require you to provision infrastructure to analyze, verify, and troubleshoot network connectivity.",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "NOTE\n\nVPC reachability has broad support of sources and destinations for resources within your VPCs. For a complete list of\n\nsupported sources and destinations, see this support document.\n\nVPC Reachability Analyzer provides explanation codes that describe the result of a network path analysis. In this recipe, you observed the code ENI_SG_RULES_MISMATCH that indicates that the security groups are not allowing traffic between the source and destination. The complete list of explanation codes is available in this documentation.\n\nChallenge\n\nAdd an internet gateway to your VPC and test access to that from an instance.\n\n2.7 Redirecting HTTP Traffic to HTTPS with an Application Load Balancer\n\nProblem\n\nYou have a containerized application running in a private subnet. Users on the internet need to access this application. To help secure the application, you would like to redirect all requests from HTTP to HTTPS.\n\nSolution\n\nCreate an Application Load Balancer (ALB). Next, create listeners on the ALB for ports 80 and 443, target groups for your containerized application, and listener rules. Configure the listener rules to send traffic to your target group, as shown in Figure 2-9. Finally, configure an action to redirect with an HTTP 301 response code, port 80 (HTTP) to port 443 (HTTPS) while preserving the URL in the request (see Figure 2-10).",
      "content_length": 1354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Figure 2-9. VPC with ALB serving internet traffic to containers in private subnets\n\nFigure 2-10. Redirecting HTTP to HTTPs with an ALB\n\nPrerequisites\n\nVPC with public subnets in two AZs and associated route tables.\n\nPrivate subnets created in two AZs and associated route tables.\n\nAn ECS cluster and container definition exposing a web application on port 80.\n\nA Fargate service that runs tasks on the ECS cluster.\n\nOpenSSL. (You can install this using brew install openssl or yum install openssl.)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a new private key to be used for the certificate:\n\nopenssl genrsa 2048 > my-private-key.pem\n\nYou should see output similar to the following:\n\nGenerating RSA private key, 2048 bit long modulus\n\n................................................................+++",
      "content_length": 865,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "2.\n\n3.\n\n.........................................................................+++\n\ne is 65537 (0x10001)\n\nGenerate a self-signed certificate using OpenSSL CLI:\n\nopenssl req -new -x509 -nodes -sha256 -days 365 \\\n\nkey my-private-key.pem -outform PEM -out my-certificate.pem\n\nYou should see output similar to the following:\n\nYou are about to be asked to enter information that will be incorporated\n\ninto your certificate request.\n\nWhat you are about to enter is what is called a Distinguished Name or a DN.\n\nThere are quite a few fields but you can leave some blank\n\nFor some fields there will be a default value,\n\nIf you enter '.', the field will be left blank.\n\n-----\n\nCountry Name (2 letter code) []:US\n\nState or Province Name (full name) []:Pennsylvania\n\nLocality Name (eg, city) []:Scranton\n\nOrganization Name (eg, company) []:AWS Cookbook Inc\n\nOrganizational Unit Name (eg, section) []:Cloud Team\n\nCommon Name (eg, fully qualified host name) []:mytest.com\n\nEmail Address []:you@youremail.com\n\nNOTE\n\nYou are using a self-signed certificate for this recipe, which will throw a warning when you access the\n\nLoad Balancer DNS name in most browsers. You can generate a trusted certificate for your own DNS\n\nrecord by using AWS Certificate Manager (ACM).\n\nUpload the generated certificate into IAM:\n\nCERT_ARN=$(aws iam upload-server-certificate \\\n\n--server-certificate-name AWSCookbook207 \\\n\n--certificate-body file://my-certificate.pem \\\n\n--private-key file://my-private-key.pem \\\n\n--query ServerCertificateMetadata.Arn --output text)",
      "content_length": 1534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "4.\n\n5.\n\n6.\n\nCreate a security group to use with the ALB that you will create later:\n\nALB_SG_ID=$(aws ec2 create-security-group --group-name Cookbook207SG \\\n\n--description \"ALB Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\nAdd rules to the security group to allow HTTP and HTTPS traffic from the world:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 443 \\\n\n--cidr '0.0.0.0/0' \\\n\n--group-id $ALB_SG_ID\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 80 \\\n\n--cidr '0.0.0.0/0' \\\n\n--group-id $ALB_SG_ID\n\nFor each command in step 5, you should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<snip>>\",\n\n\"GroupId\": \"sg-<<snip>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 80,\n\n\"ToPort\": 80,\n\n\"CidrIpv4\": \"0.0.0.0/0\"\n\n}\n\n]\n\n}\n\nAuthorize the container’s security group to allow ingress traffic from the ALB:",
      "content_length": 972,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "7.\n\n8.\n\n9.\n\n10.\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 80 \\\n\n--source-group $ALB_SG_ID \\\n\n--group-id $APP_SG_ID\n\nCreate an ALB across the public subnets and assign it the previously created security group:\n\nLOAD_BALANCER_ARN=$(aws elbv2 create-load-balancer \\\n\n--name aws-cookbook207-alb \\\n\n--subnets $VPC_PUBLIC_SUBNETS --security-groups $ALB_SG_ID \\\n\n--scheme internet-facing \\\n\n--output text --query LoadBalancers[0].LoadBalancerArn)\n\nCreate a target group for the Load Balancer:\n\nTARGET_GROUP=$(aws elbv2 create-target-group \\\n\n--name aws-cookbook207-tg --vpc-id $VPC_ID \\\n\n--protocol HTTP --port 80 --target-type ip \\\n\n--query \"TargetGroups[0].TargetGroupArn\" \\\n\n--output text)\n\nGet the IP of the container that is running your application:\n\nTASK_ARN=$(aws ecs list-tasks --cluster $ECS_CLUSTER_NAME \\\n\n--output text --query taskArns)\n\nCONTAINER_IP=$(aws ecs describe-tasks --cluster $ECS_CLUSTER_NAME \\\n\n--task $TASK_ARN --output text \\\n\n--query tasks[0].attachments[0].details[4] | cut -f 2)\n\nRegister a container with the target group:\n\naws elbv2 register-targets --targets Id=$CONTAINER_IP \\\n\n--target-group-arn $TARGET_GROUP",
      "content_length": 1162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "11.\n\n12.\n\nNOTE\n\nFor this recipe, you register an IP address of an ECS task within an ECS service with the load balancer\n\nthat you created. You can optionally associate an ECS service directly with an Application Load\n\nBalancer on ECS service creation. For more information, see this documentation.\n\nCreate an HTTPS listener on the ALB that uses the certificate you imported and forwards traffic to your target group:\n\nHTTPS_LISTENER_ARN=$(aws elbv2 create-listener \\\n\n--load-balancer-arn $LOAD_BALANCER_ARN \\\n\n--protocol HTTPS --port 443 \\\n\n--certificates CertificateArn=$CERT_ARN \\\n\n--default-actions Type=forward,TargetGroupArn=$TARGET_GROUP \\\n\n--output text --query Listeners[0].ListenerArn)\n\nAdd a rule for the listener on port 443 to forward traffic to the target group that you created:\n\naws elbv2 create-rule \\\n\n--listener-arn $HTTPS_LISTENER_ARN \\\n\n--priority 10 \\\n\n--conditions '{\"Field\":\"path-pattern\",\"PathPatternConfig\":{\"Values\":[\"/*\"]}}' \\\n\n--actions Type=forward,TargetGroupArn=$TARGET_GROUP\n\nYou should see output similar to the following:\n\n{\n\n\"Rules\": [\n\n{\n\n\"RuleArn\": \"arn:aws:elasticloadbalancing:us-east-1:111111111111:listener-\n\nrule/app/aws-cookbook207-alb/<<snip>>\",\n\n\"Priority\": \"10\",\n\n\"Conditions\": [\n\n{\n\n\"Field\": \"path-pattern\",\n\n\"Values\": [\n\n\"/*\"\n\n],",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "13.\n\n\"PathPatternConfig\": {\n\n\"Values\": [\n\n\"/*\"\n\n]\n\n}\n\n}\n\n],\n\n\"Actions\": [\n\n{\n\n\"Type\": \"forward\",\n\n...\n\nCreate a redirect response for all HTTP traffic that sends a 301 response to the browser while preserving the full URL for the HTTPS redirect:\n\naws elbv2 create-listener --load-balancer-arn $LOAD_BALANCER_ARN \\\n\n--protocol HTTP --port 80 \\\n\n--default-actions \\\n\n\"Type=redirect,RedirectConfig={Protocol=HTTPS,Port=443,Host='#{host}',Query='#\n\n{query}',Path='/#{path}',\n\nStatusCode=HTTP_301}\"\n\nYou should see output similar to the following:\n\n{\n\n\"Listeners\": [\n\n{\n\n\"ListenerArn\": \"arn:aws:elasticloadbalancing:us-east-\n\n1:111111111111:listener/app/aws-cookbook207-alb/<<snip>>\",\n\n\"LoadBalancerArn\": \"arn:aws:elasticloadbalancing:us-east-\n\n1:111111111111:loadbalancer/app/aws-cookbook207-alb/<<snip>>\",\n\n\"Port\": 80,\n\n\"Protocol\": \"HTTP\",\n\n\"DefaultActions\": [\n\n{\n\n\"Type\": \"redirect\",\n\n\"RedirectConfig\": {\n\n\"Protocol\": \"HTTPS\",\n\n\"Port\": \"443\",\n\n\"Host\": \"#{host}\",\n\n\"Path\": \"/#{path}\",\n\n\"Query\": \"#{query}\",",
      "content_length": 1003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "\"StatusCode\": \"HTTP_301\"\n\n}\n\n}\n\n...\n\n14.\n\nVerify the health of the targets:\n\naws elbv2 describe-target-health --target-group-arn $TARGET_GROUP \\\n\n--query TargetHealthDescriptions[*].TargetHealth.State\n\nYou should see output similar to this:\n\n[\n\n\"healthy\"\n\n]\n\nValidation checks\n\nGet the URL of the load balancer so that you can test it:\n\nLOAD_BALANCER_DNS=$(aws elbv2 describe-load-balancers \\\n\n--names aws-cookbook207-alb \\\n\n--output text --query LoadBalancers[0].DNSName)\n\nDisplay the URL and test it in your browser. You should notice that you end up at an HTTPS URL. You will most likely receive a warning from your browser because of the self-signed cert:\n\necho $LOAD_BALANCER_DNS\n\nOr test it from the command line.\n\ncURL the Load Balancer DNS over HTTP and observe the 301 code:\n\ncurl -v http://$LOAD_BALANCER_DNS",
      "content_length": 818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "cURL the Load Balancer DNS and specify to follow the redirect to HTTPS:\n\ncurl -vkL http://$LOAD_BALANCER_DNS\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen you added a 301 redirect rule for the port 80 listener, this allowed the ALB to instruct clients to follow the redirect to port 443 so that users of your application will be automatically redirected to HTTPS. The redirect rule also preserves the URL path in the original request.\n\nApplication Load Balancers operate on Layer 7 of the OSI model. The ALB documentation lists the available target types of EC2 instances, IP addresses, and Lambda functions. You can create internet-facing ALBs (when your VPC has an internet gateway attached) and internal ALBs for usage within your internal network only. The ALB provisions elastic network interfaces that have IP addresses within your chosen subnets to communicate with your services. ALBs continuously run health checks for members of your associated target groups that allow the ALB to detect healthy components of your application to route traffic to. ALBs are also a great layer to add in front of your applications for increased security since you can allow the targets to be accessed by only the load balancer—not by clients directly.\n\nAWS offers multiple types of load balancers for specific use cases. You should choose the load balancer that best fits your needs. For example, for high-performance Layer 4 load balancing with static IP address capability, you might consider Network Load Balancers, and for network virtual appliances (NVAs) like virtual firewalls and security appliances, you might consider Gateway Load Balancers. For more information on and a comparison of the types of load balancers available in AWS, see the support document.\n\nChallenge\n\nUpdate the SSL certificate with a new one.\n\n2.8 Simplifying Management of CIDRs in Security Groups with Prefix Lists\n\nProblem\n\nYou have two applications hosted in public subnets. The applications are hosted on instances with specific access requirements for each application. During normal operation, these applications need to be accessed from virtual desktops in another Region. However, you need to reach them from your home PC for a short period for testing.",
      "content_length": 2289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Solution\n\nUsing the AWS-provided IP address ranges list, create a managed prefix list that contains a list of CIDR ranges for WorkSpaces gateways in us-west-2 and associate it with each security group. Update the prefix list with your home IP for testing and then optionally remove it (see Figure 2- 11).\n\nPrerequisites\n\nVPC with public subnets in two AZs and associated route tables\n\nTwo EC2 instances in each public subnet running a web server on port 80\n\nTwo security groups, one associated with each EC2 instance\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.",
      "content_length": 603,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "Steps\n\n1.\n\n2.\n\n3.\n\nFigure 2-11. Two applications in public subnets protected by security groups\n\nDownload the AWS IP address ranges JSON file:\n\ncurl -o ip-ranges.json https://ip-ranges.amazonaws.com/ip-ranges.json\n\nNOTE\n\nYou will need to install the jq utility if your workstation doesn’t already have it; for example, brew install jq.\n\nGenerate a list of the CIDR ranges for Amazon WorkSpaces gateways in us- west-2:\n\njq -r '.prefixes[] | select(.region==\"us-west-2\") |\n\nselect(.service==\"WORKSPACES_GATEWAYS\") | .ip_prefix' < ip-ranges.json\n\nTIP\n\nYou can find more information on AWS IP address ranges in their documentation.\n\nUse the IP ranges for Amazon WorkSpaces from ip-ranges.json to create a managed prefix list:\n\nPREFIX_LIST_ID=$(aws ec2 create-managed-prefix-list \\\n\n--address-family IPv4 \\\n\n--max-entries 15 \\\n\n--prefix-list-name allowed-us-east-1-cidrs \\\n\n--output text --query \"PrefixList.PrefixListId\" \\\n\n--entries\n\nCidr=44.234.54.0/23,Description=workspaces-us-west-2-cidr1\n\nCidr=54.244.46.0/23,Description=workspaces-us-west-2-cidr2)",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "4.\n\n5.\n\nNOTE\n\nAt this point, your workstation should not be able to reach either of the instances. If you try one of\n\nthese commands, you will receive a “Connection timed out” error:\n\ncurl -m 2 $INSTANCE_IP_1\n\ncurl -m 2 $INSTANCE_IP_2\n\nGet your workstation’s public IPv4 address:\n\nMY_IP_4=$(curl myip4.com | tr -d ' ')\n\nUpdate your managed prefix list and add your workstation’s public IPv4 address (see Figure 2-12):\n\naws ec2 modify-managed-prefix-list \\\n\n--prefix-list-id $PREFIX_LIST_ID \\\n\n--current-version 1 \\\n\n--add-entries Cidr=${MY_IP_4}/32,Description=my-workstation-ip",
      "content_length": 578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "Figure 2-12. Security group rules referencing a prefix list\n\nTIP\n\nThere is an AWS-managed prefix list for S3.\n\nYou should see output similar to the following:\n\n{\n\n\"PrefixList\": {\n\n\"PrefixListId\": \"pl-013217b85144872d2\",\n\n\"AddressFamily\": \"IPv4\",\n\n\"State\": \"modify-in-progress\",\n\n\"PrefixListArn\": \"arn:aws:ec2:us-east-1:111111111111:prefix-list/pl-\n\n013217b85144872d2\",\n\n\"PrefixListName\": \"allowed-us-east-1-cidrs\",",
      "content_length": 414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "\"MaxEntries\": 10,\n\n\"Version\": 1,\n\n\"OwnerId\": \"111111111111\"\n\n}\n\n}\n\n6.\n\nFor each application’s security group, add an inbound rule that allows TCP port 80 access from the prefix list:\n\naws ec2 authorize-security-group-ingress \\\n\n--group-id $INSTANCE_SG_1 --ip-permissions \\\n\nIpProtocol=tcp,FromPort=80,ToPort=80,PrefixListIds=\"[{Description=http-from-\n\nprefix-list,PrefixListId=$PREFIX_LIST_ID}]\"\n\naws ec2 authorize-security-group-ingress \\\n\n--group-id $INSTANCE_SG_2 --ip-permissions \\\n\nIpProtocol=tcp,FromPort=80,ToPort=80,PrefixListIds=\"[{Description=http-from-\n\nprefix-list,PrefixListId=$PREFIX_LIST_ID}]\"\n\nTIP\n\nFind out where your managed list is used. This command is helpful for auditing where prefix lists are\n\nused throughout your AWS environments:\n\naws ec2 get-managed-prefix-list-associations \\\n\n--prefix-list-id $PREFIX_LIST_ID\n\nValidation checks\n\nTest access to both instances from your workstation’s PC:\n\ncurl -m 2 $INSTANCE_IP_1\n\ncurl -m 2 $INSTANCE_IP_2\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Discussion\n\nIf you need to update the list of CIDR blocks allowing ingress communication to your instances, you can simply update the prefix list instead of the security group. This helps reduce the amount of maintenance overhead if you need to use this type of authorization across many security groups; you need to update the prefix list in only a single location rather than modify every security group authorization that requires this network security configuration. You can also use prefix lists for egress security group authorizations.\n\nPrefix lists can be associated with route tables; they are also useful for blackholing traffic (prohibiting access to a specific list of IP addresses and CIDR blocks) and can also simplify your route table configuration. For example, you could maintain a prefix list of branch office CIDR ranges and use them to implement your routing and security group authorizations, simplifying your management for network flow and security configuration. An example of associating a prefix list with a route looks like this:\n\naws ec2 create-route --route-table-id $Sub1RouteTableID \\\n\n--destination-prefix-list-id $PREFIX_LIST_ID \\\n\n--instance-id $INSTANCE_ID\n\nPrefix lists also provide a powerful versioning mechanism, allowing you to quickly roll back to previous known working states. If, for example, you updated a prefix list and found that the change broke some existing functionality, you can roll back to a previous version of a prefix list to restore previous functionality while you investigate the root cause of the error. If you decide to roll back to a previous version for some reason, first describe the prefix list to get the current version number:\n\naws ec2 describe-prefix-lists --prefix-list-ids $PREFIX_LIST_ID\n\nChallenge\n\nRevert the active version of the prefix list so that your workstation IP is removed and you can no longer access either application. (Hint: in the repository.) 2.9 Controlling Network Access to S3 from Your VPC Using VPC Endpoints\n\nProblem\n\nResources within your VPC should be able to access only a specific S3 bucket. Also, this S3 traffic should not traverse the internet for security reasons and to keep bandwidth costs low.\n\nSolution",
      "content_length": 2212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "You will create a gateway VPC endpoint for S3, associate it with a route table, and customize its policy document (see Figure 2-13).",
      "content_length": 132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Figure 2-13. Controlling S3 access with gateway endpoints\n\nPrerequisites\n\nVPC with isolated subnets in two AZs and associated route tables\n\nOne EC2 instance in a public subnet that you can access for testing\n\nAn existing S3 bucket that you want to limit access to\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a gateway endpoint in your VPC and associate the endpoint with the isolated route tables:\n\nEND_POINT_ID=$(aws ec2 create-vpc-endpoint \\\n\n--vpc-id $VPC_ID \\\n\n--service-name com.amazonaws.$AWS_REGION.s3 \\\n\n--route-table-ids $RT_ID_1 $RT_ID_2 \\\n\n--query VpcEndpoint.VpcEndpointId --output text)\n\n2.\n\nCreate a template endpoint policy file called policy.json with the following content (included in the repository). This is used to limit access to only the S3 bucket that you created in the preparation steps:\n\n{\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"RestrictToOneBucket\",\n\n\"Principal\": \"*\",\n\n\"Action\": [\n\n\"s3:GetObject\",\n\n\"s3:PutObject\"\n\n],\n\n\"Effect\": \"Allow\",\n\n\"Resource\": [\"arn:aws:s3:::S3BucketName\",\n\n\"arn:aws:s3:::S3BucketName/*\"]\n\n}",
      "content_length": 1095,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "]\n\n}\n\n3.\n\nInsert your S3_BUCKET_NAME in the policy-template.json file:\n\nsed -e \"s/S3BucketName/${BUCKET_NAME}/g\" \\\n\npolicy-template.json > policy.json\n\n4.\n\nModify the endpoint’s policy document. Endpoint policies limit or restrict the resources that can be accessed through the VPC endpoint:\n\naws ec2 modify-vpc-endpoint \\\n\n--policy-document file://policy.json \\\n\n--vpc-endpoint-id $END_POINT_ID\n\nValidation checks\n\nOutput the name of your S3 Bucket so that you can refer to it when you are connected to your EC2 Instance:\n\necho $BUCKET_NAME\n\nConnect to the EC2 instance by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nSet the Region by grabbing the value from the instance’s metadata:\n\nexport AWS_DEFAULT_REGION=$(curl \\\n\n--silent http://169.254.169.254/latest/dynamic/instance-identity/document \\\n\n| awk -F'\"' ' /region/ {print $4}')\n\nRetrieve the allowed S3 bucket name:",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "BUCKET=$(aws ssm get-parameters \\\n\n--names \"Cookbook209S3Bucket\" \\\n\n--query \"Parameters[*].Value\" --output text)\n\nTest access by trying to copy a file from your S3 bucket:\n\naws s3 cp s3://${BUCKET_NAME}/test_file /home/ssm-user/\n\nYou should see output similar to the following:\n\ndownload: s3://cdk-aws-cookbook-209-awscookbookrecipe20979239201-115xoj77fgxoh/test_file to\n\n./test_file\n\nNOTE\n\nThe following command is attempting to list a public S3 bucket. However, because of the endpoint policy we have\n\nconfigured, it is expected that this will fail.\n\nTry to list the contents of a public S3 bucket associated with the OpenStreetMap Foundation Public Dataset Initiative:\n\naws s3 ls s3://osm-pds/\n\nYou should see output similar to the following:\n\nAn error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied\n\nExit the Session Manager session:\n\nexit",
      "content_length": 876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nUsing an endpoint policy is a useful security implementation to restrict access to S3 buckets. This applies not only to S3 buckets owned by your account but also to all S3 buckets globally on AWS.\n\nTIP\n\nRecently, AWS announced support for S3 interface endpoints. However, it is worth noting that while these are great for\n\nsome use cases (e.g., when you want to control traffic with security groups), they are not ideal for this problem because of\n\nthe costs associated with interface endpoints.\n\nPer the VPC User Guide, Gateway VPC endpoints are free and used within your VPC’s route tables to keep traffic bound for AWS services within the AWS backbone network without traversing the network. This allows you to create VPCs that do not need internet gateways for applications that do not require them but need access to other AWS services like S3 and DynamoDB. All traffic bound for these services will be directed by the route table to the VPC endpoint rather than the public internet route, since the VPC endpoint route table entry is more specific than the default 0.0.0.0/0 route.\n\nS3 VPC endpoint policies leverage JSON policy documents that can be as fine-grained as your needs require. You can use conditionals, source IP addresses, VPC endpoint IDs, S3 bucket names, and more. For more information on the policy elements available, see the support document.\n\nChallenge\n\nModify the bucket policy for the S3 bucket to allow access only from the VPC endpoint that you created. For some tips on this, check out the S3 User Guide.\n\n2.10 Enabling Transitive Cross-VPC Connections Using Transit Gateway\n\nProblem\n\nYou need to implement transitive routing across all of your VPCs and share internet egress from a shared services VPC to your other VPCs to reduce the number of NAT gateways you have to deploy.\n\nSolution",
      "content_length": 1914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Deploy an AWS transit gateway (TGW) and configure transit gateway VPC attachments for all of your VPCs. Update your VPC route tables of each VPC to send all nonlocal traffic to the transit gateway and enable sharing of the NAT gateway in your shared services VPC for all of your spoke VPCs (see Figure 2-14).",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Figure 2-14. AWS transit gateway with three VPCs\n\nWARNING\n\nThe default initial quota of VPCs per Region per account is five. This solution will deploy three VPCs. If you already have\n\nmore than two VPCs, you can decide among four choices: deploy to a different Region, delete any existing VPCs that are\n\nno longer needed, use a test account, or request a quota increase.\n\nPrerequisites\n\nThree VPCs in the same Region with private and isolated subnet tiers\n\nInternet gateway attached to a VPC (VPC2 in our example)\n\nNAT gateway deployed in public subnets\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a transit gateway:\n\nTGW_ID=$(aws ec2 create-transit-gateway \\\n\n--description AWSCookbook210 \\\n\n—-\n\noptions=AmazonSideAsn=65010,AutoAcceptSharedAttachments=enable,DefaultRouteTable\n\nAssociation=enable,\\\n\nDefaultRouteTablePropagation=enable,VpnEcmpSupport=enable,DnsSupport=enable \\\n\n--output text --query TransitGateway.TransitGatewayId)\n\n2.\n\nWait until the transit gateway’s state has reached available. This may take several minutes:\n\naws ec2 describe-transit-gateways \\\n\n--transit-gateway-ids $TGW_ID \\\n\n--output text --query TransitGateways[0].State",
      "content_length": 1212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "3.\n\n4.\n\n5.\n\n6.\n\nCreate a transit gateway attachment for VPC1:\n\nTGW_ATTACH_1=$(aws ec2 create-transit-gateway-vpc-attachment \\\n\n--transit-gateway-id $TGW_ID \\\n\n--vpc-id $VPC_ID_1 \\\n\n--subnet-ids $ATTACHMENT_SUBNETS_VPC_1 \\\n\n--query TransitGatewayVpcAttachment.TransitGatewayAttachmentId \\\n\n--output text)\n\nCreate a transit gateway attachment for VPC2:\n\nTGW_ATTACH_2=$(aws ec2 create-transit-gateway-vpc-attachment \\\n\n--transit-gateway-id $TGW_ID \\\n\n--vpc-id $VPC_ID_2 \\\n\n--subnet-ids $ATTACHMENT_SUBNETS_VPC_2 \\\n\n--query TransitGatewayVpcAttachment.TransitGatewayAttachmentId \\\n\n--output text)\n\nCreate a transit gateway attachment for VPC3:\n\nTGW_ATTACH_3=$(aws ec2 create-transit-gateway-vpc-attachment \\\n\n--transit-gateway-id $TGW_ID \\\n\n--vpc-id $VPC_ID_3 \\\n\n--subnet-ids $ATTACHMENT_SUBNETS_VPC_3 \\\n\n--query TransitGatewayVpcAttachment.TransitGatewayAttachmentId \\\n\n--output text)\n\nAdd routes for all private subnets in VPCs 1 and 3 to target the TGW for destinations of 0.0.0.0/0. This enables consolidated internet egress through the NAT gateway in VPC2 and transitive routing to other VPCs:\n\naws ec2 create-route --route-table-id $VPC_1_RT_ID_1 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--transit-gateway-id $TGW_ID\n\naws ec2 create-route --route-table-id $VPC_1_RT_ID_2 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--transit-gateway-id $TGW_ID",
      "content_length": 1342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "7.\n\n8.\n\n9.\n\naws ec2 create-route --route-table-id $VPC_3_RT_ID_1 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--transit-gateway-id $TGW_ID\n\naws ec2 create-route --route-table-id $VPC_3_RT_ID_2 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--transit-gateway-id $TGW_ID\n\nNow add a route to your 10.10.0.0/24 supernet in the route tables associated with the private subnets of VPC2, pointing its destination to the transit gateway. This is more specific than the 0.0.0.0/0 destination that is already present and therefore takes higher priority in routing decisions. This directs traffic bound for VPCs 1, 2, and 3 to the TGW:\n\naws ec2 create-route --route-table-id $VPC_2_RT_ID_1 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\naws ec2 create-route --route-table-id $VPC_2_RT_ID_2 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\nQuery for the NAT gateways in use; we’ll need these to add routes to them for internet traffic:\n\nNAT_GW_ID_1=$(aws ec2 describe-nat-gateways \\\n\n--filter \"Name=subnet-id,Values=$VPC_2_PUBLIC_SUBNET_ID_1\" \\\n\n--output text --query NatGateways[*].NatGatewayId)\n\nNAT_GW_ID_2=$(aws ec2 describe-nat-gateways \\\n\n--filter \"Name=subnet-id,Values=$VPC_2_PUBLIC_SUBNET_ID_2\" \\\n\n--output text --query NatGateways[*].NatGatewayId)\n\nAdd a route for the attachment subnet in VPC2 to direct internet traffic to the NAT gateway:\n\naws ec2 create-route --route-table-id $VPC_2_ATTACH_RT_ID_1 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--nat-gateway-id $NAT_GW_ID_1",
      "content_length": 1513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "10.\n\n11.\n\n12.\n\n13.\n\naws ec2 create-route --route-table-id $VPC_2_ATTACH_RT_ID_2 \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--nat-gateway-id $NAT_GW_ID_2\n\nAdd a static route to the route tables associated with the public subnet in VPC2. This enables communication back to the TGW to allow sharing the NAT gateway with all attached VPCs:\n\naws ec2 create-route --route-table-id $VPC_2_PUBLIC_RT_ID_1 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\naws ec2 create-route --route-table-id $VPC_2_PUBLIC_RT_ID_2 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\nAdd a static route for the private subnets in VPC2 to allow communication back to the TGW attachments from VPC2 private subnets:\n\naws ec2 create-route --route-table-id $VPC_2_RT_ID_1 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\naws ec2 create-route --route-table-id $VPC_2_RT_ID_2 \\\n\n--destination-cidr-block 10.10.0.0/24 \\\n\n--transit-gateway-id $TGW_ID\n\nGet the transit route table ID:\n\nTRAN_GW_RT=$(aws ec2 describe-transit-gateways \\\n\n--transit-gateway-ids $TGW_ID --output text \\\n\n--query TransitGateways[0].Options.AssociationDefaultRouteTableId)\n\nAdd a static route in the transit gateway route table for VPC2 (with the NAT gateways) to send all internet traffic over this path:",
      "content_length": 1316,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "aws ec2 create-transit-gateway-route \\\n\n--destination-cidr-block 0.0.0.0/0 \\\n\n--transit-gateway-route-table-id $TRAN_GW_RT \\\n\n--transit-gateway-attachment-id $TGW_ATTACH_2\n\nValidation checks\n\nEnsure your EC2 Instance 1 has registered with SSM. To check the status use the following command, which should return the instance ID:\n\naws ssm describe-instance-information \\\n\n--filters Key=ResourceType,Values=EC2Instance \\\n\n--query \"InstanceInformationList[].InstanceId\" --output text\n\nConnect to your EC2 instance by using SSM Session Manager:\n\naws ssm start-session --target $INSTANCE_ID_1\n\nTest internet access:\n\nping -c 4 aws.amazon.com\n\nYou should see output similar to the following:\n\nPING dr49lng3n1n2s.cloudfront.net (99.86.187.73) 56(84) bytes of data.\n\n64 bytes from server-99-86-187-73.iad79.r.cloudfront.net (99.86.187.73): icmp_seq=1 ttl=238 time=3.44\n\nms\n\n64 bytes from server-99-86-187-73.iad79.r.cloudfront.net (99.86.187.73): icmp_seq=2 ttl=238 time=1.41\n\nms\n\n64 bytes from server-99-86-187-73.iad79.r.cloudfront.net (99.86.187.73): icmp_seq=3 ttl=238 time=1.43\n\nms\n\n64 bytes from server-99-86-187-73.iad79.r.cloudfront.net (99.86.187.73): icmp_seq=4 ttl=238 time=1.44\n\nms\n\n--- dr49lng3n1n2s.cloudfront.net ping statistics ---\n\n4 packets transmitted, 4 received, 0% packet loss, time 3004ms",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "rtt min/avg/max/mdev = 1.411/1.934/3.449/0.875 ms\n\nsh-4.2$\n\nExit the Session Manager session:\n\nexit\n\nChallenge 1\n\nYou can limit which VPCs can access the internet through the NAT gateway in VPC2 by modifying the route tables. Try adding a more specific route of 10.10.0.0/24 instead of the 0.0.0.0/0 destination for VPC3 to see how you can customize the internet egress sharing.\n\nChallenge 2\n\nYou may not want to allow VPC1 and VPC3 to be able to communicate with each other. Try adding a new transit gateway route table, updating the attachments to accomplish this.\n\nChallenge 3\n\nIn the solution, you deployed three VPCs each of /26 subnet size within the 10.10.0.0/24 supernet. There is room for an additional /26 subnet. Try adding an additional VPC with a /26 CIDR with subnets and route tables; then attach it to the transit gateway.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nTransit gateways allow you to quickly implement a multi-VPC hub-and-spoke network topology for your network in AWS. You may have had to use many peering connections to achieve similar results or used third-party software on instances in a transit VPC architecture. Transit gateway also supports cross-region peering of transit gateways and cross-account sharing via Resource Access Manager (RAM).\n\nWhen you attached your VPCs to the transit gateway, you used subnets in each AZ for resiliency. You also used dedicated “attachment” subnets for the VPC attachments. You can attach the transit gateway to any subnet(s) within your VPC. Using a dedicated subnet for these attachments gives you flexibility to granularly define subnets you choose to route to the TGW. That is, if you attached the private subnet, it would always have a route to the TGW; this might not be intended based on your use case. In your case, you configured routes for your private subnets to send all traffic to the transit gateway which enabled sharing of the NAT gateway and internet gateway; this results in cost savings over having to deploy multiple NAT gateways (e.g., one for each VPC).\n\nYou can connect your on-premises network or any virtual network directly to a transit gateway, as it acts as a hub for all of your AWS network traffic. You can connect IPsec VPNs, Direct",
      "content_length": 2288,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Connect (DX), and third-party network appliances to the transit gateway to extend your AWS network to non-AWS networks. This also allows you to consolidate VPN connections and/or Direct Connect connections by connecting one directly to the transit gateway to access all of your VPCs in a Region. Border Gateway Protocol (BGP) is supported by TGW over these types of network extensions for dynamic route updates in both directions.\n\nChallenge\n\nCreate a fourth VPC and attach your TGW to subnets in it. Allow it to use the existing NAT gateway to reach the internet.\n\n2.11 Peering Two VPCs Together for Inter-VPC Network Communication\n\nProblem\n\nYou need to enable two instances in separate VPCs to communicate with each other in a simple and cost-effective manner.\n\nSolution\n\nRequest a peering connection between two VPCs, accept the peering connection, update the route tables for each VPC subnet, and finally test the connection from one instance to another (see Figure 2-15).",
      "content_length": 976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "Figure 2-15. Communication between instances in peered VPCs\n\nPrerequisites\n\nTwo VPCs, each with isolated subnets in two AZs and associated route tables\n\nIn each VPC, one EC2 instance that you can access for testing\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a VPC peering connection to connect VPC1 to VPC2:\n\nVPC_PEERING_CONNECTION_ID=$(aws ec2 create-vpc-peering-connection \\",
      "content_length": 439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "2.\n\n3.\n\n4.\n\nValidation checks\n\nGet instance 2’s IP:\n\n--vpc-id $VPC_ID_1 --peer-vpc-id $VPC_ID_2 --output text \\\n\n--query VpcPeeringConnection.VpcPeeringConnectionId)\n\nAccept the peering connection:\n\naws ec2 accept-vpc-peering-connection \\\n\n--vpc-peering-connection-id $VPC_PEERING_CONNECTION_ID\n\nNOTE\n\nVPC peering connections can be established from one AWS account to a different AWS account. If\n\nyou choose to peer VPCs across AWS accounts, you need to ensure you have the correct IAM\n\nconfiguration to create and accept the peering connection within each account.\n\nIn the route tables associated with each subnet, add a route to direct traffic destined for the peered VPC’s CIDR range to the VPC_PEERING_CONNECTION_ID:\n\naws ec2 create-route --route-table-id $VPC_SUBNET_RT_ID_1 \\\n\n--destination-cidr-block $VPC_CIDR_2 \\\n\n--vpc-peering-connection-id $VPC_PEERING_CONNECTION_ID\n\naws ec2 create-route --route-table-id $VPC_SUBNET_RT_ID_2 \\\n\n--destination-cidr-block $VPC_CIDR_1 \\\n\n--vpc-peering-connection-id $VPC_PEERING_CONNECTION_ID\n\nAdd an ingress rule to instance 2’s security group that allows ICMPv4 access from instance 1’s security group:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol icmp --port -1 \\\n\n--source-group $INSTANCE_SG_1 \\\n\n--group-id $INSTANCE_SG_2",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "aws ec2 describe-instances --instance-ids $INSTANCE_ID_2\\\n\n--output text \\\n\n--query Reservations[0].Instances[0].PrivateIpAddress\n\nEnsure your EC2 instance 1 has registered with SSM. Use this command to check the status:\n\naws ssm describe-instance-information \\\n\n--filters Key=ResourceType,Values=EC2Instance \\\n\n--query \"InstanceInformationList[].InstanceId\" --output text\n\nConnect to your EC2 instance by using SSM Session Manager:\n\naws ssm start-session --target $INSTANCE_ID_1\n\nPing instance 2 from instance 1:\n\nping -c 4 <<INSTANCE_IP_2>>\n\nOutput:\n\nPING 10.20.0.242 (10.20.0.242) 56(84) bytes of data.\n\n64 bytes from 10.20.0.242: icmp_seq=1 ttl=255 time=0.232 ms\n\n64 bytes from 10.20.0.242: icmp_seq=2 ttl=255 time=0.300 ms\n\n64 bytes from 10.20.0.242: icmp_seq=3 ttl=255 time=0.186 ms\n\n64 bytes from 10.20.0.242: icmp_seq=4 ttl=255 time=0.183 ms\n\n--- 10.20.0.242 ping statistics ---\n\n4 packets transmitted, 4 received, 0% packet loss, time 3059ms\n\nrtt min/avg/max/mdev = 0.183/0.225/0.300/0.048 ms\n\nExit the Session Manager session:\n\nexit\n\nTIP",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "You can search for a security group ID in the VPC console to show all security groups that reference others. You can also\n\nrun the aws ec2 describe-security-group-references CLI command to accomplish this. This is helpful in gaining insight\n\ninto which security groups reference others. You can reference security groups in peered VPCs owned by other AWS\n\naccounts but not located in other Regions.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nVPC peering connections are nontransitive. Each VPC needs to peer with every other VPC that they need to communicate with. This type of connection is ideal when you might have a VPC hosting shared services that other VPCs need to access, while not having the “spoke” VPCs communicate with one another.\n\nIn addition to the peering connections, you need to configure the route tables associated with the VPC subnets to send traffic destined for the peered VPC’s CIDR to the peering connection (PCX). In other words, to enable VPC1 to be able to communicate with VPC2, the destination route must be present in VPC1 and the return route also must be present in VPC2.\n\nIf you were to add a third VPC to this recipe, and you needed all VPCs to be able to communicate with one another, you would need to peer that third VPC with the previous two and update all of the VPC route tables accordingly to allow for all of the VPCs to have communication with one another. As you continue to add more VPCs to a network architecture like this, you may notice that the number of peering connections and route table updates required begin to increase exponentially. Because of this, transit gateway is a better choice for transitive VPC communication using transit gateway route tables.\n\nYou can use VPC peering cross-account if needed, and you can also reference security groups in peered VPCs in a similar way of referencing security groups within a single VPC. This allows you to use the same type of strategy with how you manage security groups across your AWS environment when using VPC peering.\n\nWARNING\n\nConnecting VPCs together requires nonoverlapping CIDR ranges in order for routing to work normally. The VPC route\n\ntables must include a specific route directing traffic destined for the peered VPC to the peering connection.\n\nChallenge\n\nVPC peering connections can be established across AWS Regions. Connect a VPC in another Region to the VPC you deployed in the Region used for the recipe.",
      "content_length": 2479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Chapter 3. Storage\n\n3.0 Introduction Many industries have put a heavy emphasis on cloud data storage technologies to help facilitate increasing demands of data. Many options are available for data storage to suit your needs, with seemingly infinite scale. Even with many new storage options available in the cloud, Amazon S3 remains a powerful, fundamental building block for so many use cases. It is amazing to think that it was released more than 15 years ago. Over time, many features have been added and new storage services launched. Multiple storage options are available to meet security requirements (e.g., key management service [KMS] encryption) while reducing costs (e.g., S3 Intelligent- Tiering). Ensuring that data is secured and available is a challenge that all developers and architects face.\n\nThe storage services available on AWS allow for integration with other AWS services to provide ways for developers and application architects who integrate with many AWS services. These services can also be used to replace legacy storage systems you run and operate with on- premises environments. For example:\n\nS3 can be used to automatically invoke Lambda functions on object operations like upload.\n\nEFS can be used with EC2 to replace existing shared file systems provided by Network File System (NFS) servers.\n\nFSx for Windows can be used to replace Windows-based file servers for your EC2 workloads.\n\nEBS replaces Fibre Channel and Internet Small Computer Systems Interface (iSCSI) targets by providing block devices, and it offers many throughput options to meet performance requirements.\n\nIn this chapter, you will use some of these services so that you can start building intelligent, scalable, and secure systems that have the potential to minimize costs and operational overhead.\n\nWorkstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Storage",
      "content_length": 2059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "3.1 Using S3 Lifecycle Policies to Reduce Storage Costs\n\nProblem\n\nYou need to transition infrequently accessed objects to a more cost-effective storage tier without impacting performance or adding operational overhead.\n\nSolution\n\nCreate an S3 Lifecycle rule to transition objects to the S3 Infrequent Access (IA) storage class after a predefined time period of 30 days. Then apply this Lifecycle policy to your S3 bucket (see Figure 3-1).\n\nFigure 3-1. S3 Lifecycle rule configuration\n\nPrerequisite",
      "content_length": 497,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "An S3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a lifecycle-rule.json file (provided in the repository) to use as the Lifecycle policy that you will apply to your S3 bucket:\n\n{\n\n\"Rules\": [\n\n{\n\n\"ID\": \"Move all objects to Infrequent Access\",\n\n\"Prefix\": \"\",\n\n\"Status\": \"Enabled\",\n\n\"Transitions\": [\n\n{\n\n\"Date\": \"2015-11-10T00:00:00.000Z\",\n\n\"Days\": 30,\n\n\"StorageClass\": \"INFREQUENTLY_ACCESSED\"\n\n}\n\n]\n\n}\n\n]\n\n}\n\n2.\n\nApply the Lifecycle rule configuration:\n\naws s3api put-bucket-lifecycle-configuration \\\n\n--bucket awscookbook301-$RANDOM_STRING \\\n\n--lifecycle-configuration file://lifecycle-rule.json\n\nNOTE\n\nA Lifecycle rule helps automate the transition to a different storage class for some or all objects within\n\na bucket (prefixes, tags, and object names can all be used as filters for Lifecycle rules). For a complete\n\nlist of Lifecycle rule capabilities, see the documentation.",
      "content_length": 946,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Validation checks\n\nGet the Lifecycle configuration for your bucket:\n\naws s3api get-bucket-lifecycle-configuration \\\n\n--bucket awscookbook301-$RANDOM_STRING\n\n(Optional) Copy an object to the bucket:\n\naws s3 cp book_cover.png s3://awscookbook301-$RANDOM_STRING\n\nCheck the storage class for the object:\n\naws s3api list-objects-v2 --bucket awscookbook301-$RANDOM_STRING\n\nYou should see output similar to the following:\n\n{\n\n\"Contents\": [\n\n{\n\n\"Key\": \"book_cover.png\",\n\n\"LastModified\": \"2021-06-16T02:30:06+00:00\",\n\n\"ETag\": \"\\\"d...9\\\"\",\n\n\"Size\": 255549,\n\n\"StorageClass\": \"STANDARD\"\n\n}\n\n]\n\n}\n\nYou will see after 30 days that the storage class for the object is STANDARD_IA after running the same command.\n\nTIP\n\n“Days” in the Transition action must be greater than or equal to 30 for StorageClass STANDARD_IA. Other storage tiers allow for shorter transition times to meet your requirements. For a list of all of the storage classes available with transition\n\ntimes for Lifecycle rules, see the support document.",
      "content_length": 1003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen you upload objects to an S3 bucket, if you do not specify the storage class, the default Standard storage class is used. Amazon S3 has multiple storage classes available that can be more cost-effective for long-term storage while also suiting your performance and resiliency requirements. If you cannot change your application to specify storage tiers for object uploads, Lifecycle rules can help automate the transition to your desired storage class. Lifecycle rules can be applied to some or all objects within a bucket with a filter.\n\nAs the name may imply, S3 Infrequent Access is a storage class that provides reduced cost (compared to the S3 Standard storage class) for data stored for objects that you rarely access. This provides the same level of redundancy for your data within a Region for a reduced cost, but the cost associated with accessing objects is slightly higher. If your data access patterns are unpredictable, and you would still like to optimize your S3 storage for cost, performance, and resiliency, take a look at S3 Intelligent-Tiering in the next recipe.\n\nChallenge 1\n\nConfigure the Lifecycle rule to apply only to objects based on object-level tags.\n\nChallenge 2\n\nConfigure the Lifecycle rule to transition objects to a Deep Archive.\n\n3.2 Using S3 Intelligent-Tiering Archive Policies to Automatically Archive S3 Objects\n\nProblem\n\nYou need to automatically transition infrequently accessed objects to a different archive storage class without impacting performance or adding operational overhead.\n\nSolution\n\nCreate a policy to automate the archival of S3 objects to the S3 Glacier archive based on access patterns for objects that are more than 90 days old. Apply it to your S3 bucket, as shown in Figure 3-2.",
      "content_length": 1837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Figure 3-2. S3 Intelligent-Tiering archive\n\nPrerequisite\n\nAn S3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps",
      "content_length": 164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "1.\n\nCreate a file named tiering.json for the configuration (file provided in the repository):\n\n{\n\n\"Id\": \"awscookbook302\",\n\n\"Status\": \"Enabled\",\n\n\"Tierings\": [\n\n{\n\n\"Days\": 90,\n\n\"AccessTier\": \"ARCHIVE_ACCESS\"\n\n}\n\n]\n\n}\n\n2.\n\nApply the Intelligent-Tiering configuration:\n\naws s3api put-bucket-intelligent-tiering-configuration \\\n\n--bucket awscookbook302-$RANDOM_STRING \\\n\n--id awscookbook302 \\\n\n--intelligent-tiering-configuration \"$(cat tiering.json)\"\n\nWARNING\n\nEnsure that your use case and applications can support the increased retrieval time associated with the\n\nS3 Glacier archive storage tier. You can configure your application to use an expedited retrieval\n\nmechanism supported by S3 Glacier archive to decrease the retrieval time but increase cost. For a\n\ncomplete list of archive times and how to configure expedited access, please refer to the support\n\ndocument.\n\nValidation checks\n\nGet the configuration of Intelligent-Tiering for your bucket:\n\naws s3api get-bucket-intelligent-tiering-configuration \\\n\n--bucket awscookbook302-$RANDOM_STRING \\\n\n--id awscookbook302\n\nCopy an object to the bucket:",
      "content_length": 1103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "aws s3 cp ./book_cover.png s3://awscookbook302-$RANDOM_STRING\n\nCheck the storage class for the object:\n\naws s3api list-objects-v2 --bucket awscookbook302-$RANDOM_STRING\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAn S3 Intelligent-Tiering archive provides an automatic mechanism to transition “cool” (less frequently accessed) objects to an S3 Glacier archive. You can define the length of time required for an object to transition to the archive (between 90 and 730 days). This feature helps with meeting long-term retention requirements that you may have for compliance. The storage tiers available within S3 Intelligent-Tiering map directly to S3 tiers: Frequent Access\n\nOptimized for frequent access (S3 Standard)\n\nInfrequent Access\n\nOptimized for infrequent access (S3 Standard-IA)\n\nArchive Access\n\nArchive purposes (S3 Glacier)\n\nDeep Archive Access\n\nLong-term retention purposes (S3 Glacier Deep Archive)\n\nThis archive configuration is separate from the main S3 Intelligent-Tiering tier configuration that you place on objects, as this is a bucket-specific configuration. In the previous recipe, you configured a Lifecycle rule to configure all objects within a bucket to transition to the S3 Intelligent-Tiering storage tier. This recipe adds additional configuration to transition objects to S3 archive tiers based on your configuration. You can use either of these methods separately or both concurrently to meet your own requirements.\n\nNOTE\n\nS3 tiers are object-specific, which differs from the Intelligent-Tiering archive being bucket-specific. You can filter an\n\narchive configuration to apply only to certain prefixes, object tags, and object names if you wish to include or exclude\n\nobjects in a configuration. For more information, see the support document.",
      "content_length": 1824,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Challenge 1\n\nConfigure the Intelligent-Tiering archive to send objects that are older than one year to the Glacier Deep Archive tier.\n\nChallenge 2\n\nConfigure the Intelligent-Tiering archive to use object-level tags and configure an object with a tag that matches your configuration.\n\n3.3 Replicating S3 Buckets to Meet Recovery Point Objectives\n\nProblem\n\nYour company’s data security policy mandates that objects be replicated within the same Region to meet a recovery point objective of 15 minutes.\n\nSolution\n\nFirst, create source and destination S3 buckets with versioning enabled. Then create an IAM role and attach an IAM policy that allows S3 to copy objects from the source to the destination bucket. Finally, create an S3 replication policy that references the IAM role, and apply that policy to the source bucket, as shown in Figure 3-3.",
      "content_length": 845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Figure 3-3. S3 bucket replication\n\nPrerequisite\n\nAn S3 bucket with versioning enabled that you will use as your source\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.",
      "content_length": 205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "Steps\n\n1.\n\n2.\n\n3.\n\n4.\n\n5.\n\nCreate the destination S3 bucket:\n\naws s3api create-bucket --bucket awscookbook303-dst-$RANDOM_STRING\n\nEnable versioning for the destination S3 bucket:\n\naws s3api put-bucket-versioning \\\n\n--bucket awscookbook303-dst-$RANDOM_STRING \\\n\n--versioning-configuration Status=Enabled\n\nCreate a file named s3-assume-role-policy.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"s3.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate an IAM role using the statement in the provided assume-role- policy.json file using this command:\n\nROLE_ARN=$(aws iam create-role --role-name AWSCookbook303S3Role \\\n\n--assume-role-policy-document file://s3-assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\nCreate a file named s3-perms-policy-template.json with the following content (file provided in the repository) to allow S3 replication access to",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "6.\n\nyour source and destination buckets:\n\n{\n\n\"Version\":\"2012-10-17\",\n\n\"Statement\":[\n\n{\n\n\"Effect\":\"Allow\",\n\n\"Action\":[\n\n\"s3:GetObjectVersionForReplication\",\n\n\"s3:GetObjectVersionAcl\",\n\n\"s3:GetObjectVersionTagging\"\n\n],\n\n\"Resource\":[\n\n\"arn:aws:s3:::SRCBUCKET/*\"\n\n]\n\n},\n\n{\n\n\"Effect\":\"Allow\",\n\n\"Action\":[\n\n\"s3:ListBucket\",\n\n\"s3:GetReplicationConfiguration\"\n\n],\n\n\"Resource\":[\n\n\"arn:aws:s3:::SRCBUCKET\"\n\n]\n\n},\n\n{\n\n\"Effect\":\"Allow\",\n\n\"Action\":[\n\n\"s3:ReplicateObject\",\n\n\"s3:ReplicateDelete\",\n\n\"s3:ReplicateTags\",\n\n\"s3:GetObjectVersionTagging\"\n\n],\n\n\"Resource\":\"arn:aws:s3:::DSTBUCKET/*\"\n\n}\n\n]\n\n}\n\nReplace the values for DSTBUCKET and SRCBUCKET in the file and save it as s3-perms-policy.json:\n\nsed -e \"s/DSTBUCKET/awscookbook303-dst-${RANDOM_STRING}/g\" \\",
      "content_length": 744,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "7.\n\n8.\n\ne \"s|SRCBUCKET|awscookbook303-src-${RANDOM_STRING}|g\" \\\n\ns3-perms-policy-template.json > s3-perms-policy.json\n\nAttach the policy to the role you just created:\n\naws iam put-role-policy \\\n\n--role-name AWSCookbook303S3Role \\\n\n--policy-document file://s3-perms-policy.json \\\n\n--policy-name S3ReplicationPolicy\n\nCreate a file named s3-replication-template.json with the following content to configure a replication time of 15 minutes to your destination bucket:\n\n{\n\n\"Rules\": [\n\n{\n\n\"Status\": \"Enabled\",\n\n\"Filter\": {\n\n\"Prefix\": \"\"\n\n},\n\n\"Destination\": {\n\n\"Bucket\": \"arn:aws:s3:::DSTBUCKET\",\n\n\"Metrics\": {\n\n\"Status\": \"Enabled\",\n\n\"EventThreshold\": {\n\n\"Minutes\": 15\n\n}\n\n},\n\n\"ReplicationTime\": {\n\n\"Status\": \"Enabled\",\n\n\"Time\": {\n\n\"Minutes\": 15\n\n}\n\n}\n\n},\n\n\"DeleteMarkerReplication\": {\n\n\"Status\": \"Disabled\"\n\n},\n\n\"Priority\": 1\n\n}\n\n],",
      "content_length": 827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "\"Role\": \"ROLEARN\"\n\n}\n\n9.\n\nReplace the values for DSTBUCKET and ROLEARN in the file and save it as s3- replication.json:\n\nsed -e \"s|ROLEARN|${ROLE_ARN}|g\" \\\n\ne \"s|DSTBUCKET|awscookbook303-dst-${RANDOM_STRING}|g\" \\\n\ns3-replication-template.json > s3-replication.json\n\n10.\n\nConfigure the replication policy for the source S3 bucket:\n\naws s3api put-bucket-replication \\\n\n--replication-configuration file://s3-replication.json \\\n\n--bucket awscookbook303-src-${RANDOM_STRING}\n\nValidation checks\n\nView the replication configuration for the source bucket:\n\naws s3api get-bucket-replication \\\n\n--bucket awscookbook303-src-${RANDOM_STRING}\n\nCopy an object to the source bucket:\n\naws s3 cp ./book_cover.png s3://awscookbook303-src-$RANDOM_STRING\n\nView the replication status for the file that you uploaded to the source bucket:\n\naws s3api head-object --bucket awscookbook303-src-${RANDOM_STRING} \\\n\n--key book_cover.png\n\nYou should see output similar to the following:",
      "content_length": 957,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "{\n\n\"AcceptRanges\": \"bytes\",\n\n\"LastModified\": \"2021-06-20T00:17:25+00:00\",\n\n\"ContentLength\": 255549,\n\n\"ETag\": \"\\\"d<<>>d\\\"\",\n\n\"VersionId\": \"I<>>X\",\n\n\"ContentType\": \"image/png\",\n\n\"Metadata\": {},\n\n\"ReplicationStatus\": \"PENDING\"\n\n}\n\nView the replication status after 15 minutes and confirm that ReplicationStatus is COMPLETED, similar to the following:\n\n{\n\n\"AcceptRanges\":\"bytes\",\n\n\"ContentType\":\"image/png\",\n\n\"LastModified\":\"2021-06-20T00:17:41+00:00\",\n\n\"ContentLength\":255549,\n\n\"ReplicationStatus\":\"COMPLETED\",\n\n\"VersionId\":\"I<>>X\",\n\n\"ETag\":\\\"d<<>>d\\\"\",\n\n\"Metadata\":{}\n\n}\n\nTIP\n\nYou can also view the replication metrics in the AWS Console.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nIf you are an engineer, developer, or architect working on AWS, there is a good chance you will end up using S3. You may have to implement some sort of replication on S3 for your applications; S3 offers two types of replication to meet your specific needs: Same-Region Replication (SRR) and Cross-Region Replication (CRR). The replication time is a configurable parameter of S3 Replication Time Control (S3 RTC) and is documented to meet a 15-minute recovery point objective (RPO) backed by a service level agreement (SLA).",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "SRR uses an IAM role, a source and destination bucket, and a replication configuration that references the role and buckets. You use SRR in this recipe to configure a one-way replication; you can use SRR to facilitate many types of use cases:\n\nLog aggregation to a central bucket for indexing\n\nReplication of data between production and test environments\n\nData redundancy while retaining object metadata\n\nDesigning redundancy around data-sovereignty and compliance requirements\n\nBackup and archival purposes\n\nCRR uses a similar IAM role, a source and destination bucket, and a replication configuration that references the role and buckets. You can use CRR to extend the possibilities of what SRR enables:\n\nMeet requirements for data storage and archival across Regions\n\nLocate similar datasets closer to your regional compute and access needs to reduce latency\n\nNOTE\n\nS3 buckets that have versioning add markers to objects that you have deleted. Both types of S3 replication are able to\n\nreplicate delete markers to your target bucket if you choose. For more information, see the support document.\n\nChallenge 1\n\nCreate an S3 bucket in another Region and replicate the source bucket to that as well.\n\nChallenge 2\n\nYou can replicate specific paths and prefixes using a filter. Apply a filter so that only objects under a certain prefix (e.g., protected/) are replicated.\n\n3.4 Observing S3 Storage and Access Metrics Using Storage Lens\n\nProblem\n\nYou need to gain observability into the usage patterns of your S3 buckets.\n\nSolution",
      "content_length": 1528,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "Configure S3 Storage Lens to provide observability and analytics about your S3 usage, as shown in Figure 3-4.\n\nFigure 3-4. Configuring S3 Storage Lens for S3 observability\n\nPrerequisite\n\nS3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\nNOTE\n\nPer the documentation: You can’t use your account’s root user credentials to view Amazon S3 Storage Lens dashboards.\n\n1.\n\nFrom the S3 console, select S3 Storage Lens from the navigation pane on the left.\n\n2.\n\nClick “Create dashboard.”\n\nNOTE\n\nAll AWS accounts have a default dashboard associated with them that shows the free metrics available\n\nthrough S3 Storage Lens. Enabling advanced metrics gives you deeper insights into your S3 usage and\n\nalso provides cost-savings recommendations you can take action on to optimize for cost. You can use\n\nthe default dashboard and/or create your own. The rest of these steps will show you how to create your\n\nown.",
      "content_length": 952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "3.\n\nGive your dashboard a name, as shown in Figure 3-5.",
      "content_length": 55,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "4.\n\n5.\n\nFigure 3-5. S3 Storage Lens dashboard creation\n\nInclude all of your buckets and Regions (use the default values) for the “Dashboard scope” (see Figure 3-6).\n\nFigure 3-6. Dashboard scope\n\nEnable “Advanced metrics and recommendations,” keeping the default values, as shown in Figure 3-7.",
      "content_length": 293,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "6.\n\nFigure 3-7. Selecting advanced metrics\n\nChoose the defaults for Export settings (no export).\n\nTIP\n\nYou can enable an automated export to periodically export your metrics to CSV and Apache Parquet\n\nformats and send them to an S3 bucket of your choice to run your own reports and visualizations.",
      "content_length": 297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "7.\n\nClick “Create dashboard,” and then view your dashboard from the dashboard selection.\n\nNOTE\n\nIt may take up to 48 hours for advanced metrics to begin accumulating for your usage and access\n\npatterns. In the meantime, you can view the default dashboard for the free metrics associated with\n\nyour S3 usage for all of your buckets in your account.\n\nValidation checks\n\nOpen the Storage Lens and view the dashboard that you configured. You should see metrics that correspond to your S3 usage. A sample is shown in Figure 3-8.\n\nFigure 3-8. Sample S3 Storage Lens dashboard",
      "content_length": 569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "TIP\n\nYou can drill down into “Cost efficiency” and “Data protection” metrics from the dashboard. After some time, you will be\n\nable to view historical data that allows you to take action on moving your objects to storage tiers that meet your needs for\n\ndata access patterns and availability requirements.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nS3 was one of the first AWS services, and as a result, many customers have been using S3 for a very long time. As customer storage usage grew exponentially, the ability to analyze what is being stored became a clear, desired capability. S3 Storage Lens gives you the ability to “see” into your S3 usage for your AWS accounts. Analyzing bucket usage, observing storage costs, and discovering anomalies (e.g., undeleted multipart upload fragments) are just a few of the many use cases S3 Storage Lens provides.\n\nWith Storage Lens, you can discover where your objects are being stored with a visual dashboard backed by a powerful analytics engine so that you can make adjustments to optimize for cost without impacting performance. You can also enable advanced metrics on your dashboard to gain deeper insights and cost-savings recommendations for your S3 buckets.\n\nNOTE\n\nS3 Storage Lens uses metrics to help you visualize your usage and activity. There are free metrics available and advanced\n\nmetrics that also give you recommendations on your usage. For more information about the different types of metrics and\n\ntheir associated costs, see the support document.\n\nChallenge 1\n\nUse Storage Lens findings to observe metrics and set an alert to continuously monitor your usage.\n\nChallenge 2\n\nCreate a new storage lens configuration that looks at only specific buckets.\n\n3.5 Configuring Application-Specific Access to S3 Buckets with S3 Access Points",
      "content_length": 1847,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Problem\n\nYou have an S3 bucket and two applications. You need to grant read/write access to one of your applications and read-only access to another application. You do not want to use S3 bucket policies, as you expect to have to add additional applications with fine-grained security requirements in the future.\n\nSolution\n\nCreate two S3 access points and apply a policy granting the S3:PutObject and S3:GetObject actions to one of the access points and S3:GetObject to the other access point. Then, configure your application to use the respective access point DNS name (see Figure 3-9).\n\nFigure 3-9. S3 access points for two applications using the same S3 bucket\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nTwo EC2 instances deployed. You will need the ability to connect to test access.\n\nS3 bucket.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps",
      "content_length": 942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "1.\n\n2.\n\n3.\n\n4.\n\nIn your VPC, create an access point for Application 1:\n\naws s3control create-access-point --name cookbook305-app-1 \\\n\n--account-id $AWS_ACCOUNT_ID \\\n\n--bucket $BUCKET_NAME --vpc-configuration VpcId=$VPC_ID\n\nIn your VPC, create an access point for Application 2:\n\naws s3control create-access-point --name cookbook305-app-2 \\\n\n--account-id $AWS_ACCOUNT_ID \\\n\n--bucket $BUCKET_NAME --vpc-configuration VpcId=$VPC_ID\n\nCreate a file named app-1-policy-template.json with the access point policy for Application 1 to read/write with the following content (file provided in the repository):\n\n{\n\n\"Version\":\"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"AWS\": \"EC2_INSTANCE_PROFILE\"\n\n},\n\n\"Action\": [ACTIONS],\n\n\"Resource\":\n\n\"arn:aws:s3:AWS_REGION:AWS_ACCOUNT_ID:accesspoint/ACCESS_POINT_NAME/object/*\"\n\n}]\n\n}\n\nUse the sed command to replace the values in app-policy-template.json with your EC2_INSTANCE_PROFILE, AWS_REGION, AWS_ACCOUNT_ID, ACCESS_POINT_NAME, and ACTIONS values for Application 1:\n\nsed -e \"s/AWS_REGION/${AWS_REGION}/g\" \\\n\ne \"s|EC2_INSTANCE_PROFILE|${INSTANCE_ROLE_1}|g\" \\\n\ne \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\ne \"s|ACCESS_POINT_NAME|cookbook305-app-1|g\" \\",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "5.\n\n6.\n\n7.\n\ne \"s|ACTIONS|\\\"s3:GetObject\\\",\\\"s3:PutObject\\\"|g\" \\\n\napp-policy-template.json > app-1-policy.json\n\nPut the policy you created on the access point for Application 1:\n\naws s3control put-access-point-policy --account-id $AWS_ACCOUNT_ID \\\n\n--name cookbook305-app-1 --policy file://app-1-policy.json\n\nUse the sed command to replace the values in app-policy-template.json with your EC2_INSTANCE_PROFILE, AWS_REGION, AWS_ACCOUNT_ID, ACCESS_POINT_NAME, and ACTIONS values for Application 2:\n\nsed -e \"s/AWS_REGION/${AWS_REGION}/g\" \\\n\ne \"s|EC2_INSTANCE_PROFILE|${INSTANCE_ROLE_2}|g\" \\\n\ne \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\ne \"s|ACCESS_POINT_NAME|cookbook305-app-2|g\" \\\n\ne \"s|ACTIONS|\\\"s3:GetObject\\\"|g\" \\\n\napp-policy-template.json > app-2-policy.json\n\nPut the policy you created on the access point for Application 2:\n\naws s3control put-access-point-policy --account-id $AWS_ACCOUNT_ID \\\n\n--name cookbook305-app-2 --policy file://app-2-policy.json\n\nNOTE\n\nYou can use specific access points with AWS SDK and CLI in a similar way. For example, the bucket\n\nname becomes the following for SDK usage: https://[access_point_name]-[accountID].s3-\n\naccesspoint.[region].amazonaws.com for URLs and arn:aws:s3:[region]:[accountID]: [access_point_name] as “bucket name” for CLI usage.\n\nHere is a CLI example:\n\naws s3api get-object --key object.zip \\\n\n--bucket \\\n\narn:aws:s3:us-east-1:111111111111:access_point_name object.zip",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "8.\n\nFollow this guide to modifying the bucket policy so that you delegate control to the access points.\n\nValidation checks\n\nConnect to the EC2 instance 1 by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID_1\n\nSet your AWS account ID from the instance’s metadata:\n\nexport AWS_ACCOUNT_ID=$(curl --silent http://169.254.169.254/latest/dynamic/instance-identity/document\n\n\\\n\n| awk -F'\"' ' /accountId/ {print $4}')\n\nSet the Region by grabbing the value from the instance’s metadata:\n\nexport AWS_DEFAULT_REGION=$(curl --silent http://169.254.169.254/latest/dynamic/instance-\n\nidentity/document \\\n\n| awk -F'\"' ' /region/ {print $4}')\n\nTry to get an object from the S3 access point for Application 1:\n\naws s3api get-object --key Recipe305Test.txt \\\n\n--bucket arn:aws:s3:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:accesspoint/cookbook305-app-1 \\\n\n/tmp/Recipe305Test.txt\n\nWrite an object to the S3 access point for Application 1:\n\naws s3api put-object \\\n\n--bucket arn:aws:s3:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:accesspoint/cookbook305-app-1 \\\n\n--key motd.txt --body /etc/motd\n\nNOTE",
      "content_length": 1104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "These two commands work for Application 1 because you configured read/write access for this access point.\n\nDisconnect from EC2 instance 1:\n\nexit\n\nConnect to the EC2 instance 2 using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID_2\n\nSet your AWS account ID from the instance’s metadata:\n\nexport AWS_ACCOUNT_ID=$(curl --silent http://169.254.169.254/latest/dynamic/instance-identity/document\n\n\\\n\n| awk -F'\"' ' /accountId/ {print $4}')\n\nSet the Region by grabbing the value from the instance’s metadata:\n\nexport AWS_DEFAULT_REGION=$(curl --silent http://169.254.169.254/latest/dynamic/instance-\n\nidentity/document \\\n\n| awk -F'\"' ' /region/ {print $4}')\n\nTry to get an object from the S3 bucket:\n\naws s3api get-object --key Recipe305Test.txt \\\n\n--bucket \\\n\narn:aws:s3:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:accesspoint/cookbook305-app-2 \\\n\n/tmp/Recipe305Test.txt\n\nTry to put an object to the S3 bucket:",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "aws s3api put-object \\\n\n--bucket arn:aws:s3:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:accesspoint/cookbook305-app-2 \\\n\n--key motd2.txt --body /etc/motd\n\nNOTE\n\nThe first command works, and the second command fails, for Application 2 because you configured read-only access for\n\nthis access point.\n\nDisconnect from EC2 instance 2:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nS3 access points allow you to grant fine-grained access to specific principals, and they can be easier to manage than S3 bucket policies. In this recipe, you created two access points with different kinds of allowed actions and associated the access points with specific roles using access point IAM policies. You verified that only specific actions were granted to your EC2 instances when they were being used with the CLI and S3 access point.\n\nTo help you meet your security requirements, access points use IAM policies in a similar way that you use for other AWS services. You can also configure S3 Block Public Access on access points to ensure that no public access is ever granted by mistake to your S3 buckets (see Recipe 1.9). There is no additional cost for S3 access points.\n\nChallenge\n\nConfigure a third access point and specify access to a specific object or prefix only.\n\n3.6 Using Amazon S3 Bucket Keys with KMS to Encrypt Objects\n\nProblem\n\nYou need to encrypt S3 objects at rest with a Key Management Service (KMS) customer- managed key (CMK) and ensure that all objects within the bucket are encrypted with the KMS",
      "content_length": 1555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "key in a cost-effective manner.\n\nSolution\n\nCreate a KMS customer-managed key, configure your S3 bucket to use S3 bucket keys referencing your AWS KMS CMK, and configure an S3 bucket policy requiring KMS to be used for all S3:PutObject operations (see Figure 3-10).\n\nFigure 3-10. Encrypting objects in S3\n\nPrerequisite\n\nAn S3 bucket\n\nPreparation",
      "content_length": 344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "Follow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a KMS key to use for your S3 bucket and store the key ID in an environment variable:\n\nKEY_ID=$(aws kms create-key \\\n\n--tags TagKey=Name,TagValue=AWSCookbook306Key \\\n\n--description \"AWSCookbook S3 CMK\" \\\n\n--query KeyMetadata.KeyId \\\n\n--output text)\n\n2.\n\nCreate an alias to reference your key:\n\naws kms create-alias \\\n\n--alias-name alias/awscookbook306 \\\n\n--target-key-id $KEY_ID\n\n3.\n\nConfigure the S3 bucket to use an S3 bucket key specifying your KMS key ID:\n\naws s3api put-bucket-encryption \\\n\n--bucket awscookbook306-$RANDOM_STRING \\\n\n--server-side-encryption-configuration '{\n\n\"Rules\": [\n\n{\n\n\"ApplyServerSideEncryptionByDefault\": {\n\n\"SSEAlgorithm\": \"aws:kms\",\n\n\"KMSMasterKeyID\": \"${KEY_ID}\"\n\n},\n\n\"BucketKeyEnabled\": true\n\n}\n\n]\n\n}'\n\n4.\n\nCreate a bucket policy template for the bucket to force encryption of all objects:",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "{\n\n\"Version\":\"2012-10-17\",\n\n\"Id\":\"PutObjectPolicy\",\n\n\"Statement\":[{\n\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\n\n\"Effect\":\"Deny\",\n\n\"Principal\":\"*\",\n\n\"Action\":\"s3:PutObject\",\n\n\"Resource\":\"arn:aws:s3:::BUCKET_NAME/*\",\n\n\"Condition\":{\n\n\"StringNotEquals\":{\n\n\"s3:x-amz-server-side-encryption\":\"aws:kms\"\n\n}\n\n}\n\n}\n\n]\n\n}\n\n5.\n\nUse the sed command to replace the value in bucket-policy-template.json with your BUCKET_NAME:\n\nsed -e \"s|BUCKET_NAME|awscookbook306-${RANDOM_STRING}|g\" \\\n\nbucket-policy-template.json > bucket-policy.json\n\n6.\n\nApply the bucket policy to force encryption on all uploads:\n\naws s3api put-bucket-policy --bucket awscookbook306-$RANDOM_STRING \\\n\n--policy file://bucket-policy.json\n\nValidation checks\n\nUpload an object to the S3 bucket with encryption from the command line. You will see a successful upload:\n\naws s3 cp ./book_cover.png s3://awscookbook306-$RANDOM_STRING \\\n\n--sse aws:kms --sse-kms-key-id $KEY_ID\n\nNow, upload an object to the S3 bucket without encryption. You will notice that you receive a KMS.NotFoundException error on the command line. This indicates that the bucket policy you",
      "content_length": 1105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "configured is working properly:\n\naws s3 cp ./book_cover.png s3://awscookbook306-$RANDOM_STRING\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen applying encryption to your S3 bucket, you could have chosen to use an AWS-managed CMK that Amazon S3 creates in your AWS account and manages for you. Like the customer- managed CMK, your AWS managed CMK is unique to your AWS account and Region. Only Amazon S3 has permission to use this CMK on your behalf. You can create, rotate, and disable auditable customer-managed CMKs from the AWS KMS Console. The S3 documentation provides a comprehensive explanation of the differences between the types of encryption supported on S3.\n\nWhen you encrypt your data, your data is protected, but you have to protect your encryption key. Envelope encryption is the practice of encrypting plaintext data with a data key, and then encrypting the data key under another key, as shown in Figure 3-11.\n\nFigure 3-11. S3 encryption process with KMS",
      "content_length": 1025,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Challenge\n\nTo validate that you can rotate your keys without impacting your data, put an object into your bucket, trigger a rotation of the KMS CMK, and then get the object back out of the S3 bucket and confirm it can decrypt properly (see this AWS article for a hint).\n\n3.7 Creating and Restoring EC2 Backups to Another Region Using AWS Backup\n\nProblem\n\nYou need to create a backup of an instance and restore it in another Region.\n\nSolution\n\nCreate an on-demand backup with AWS Backup for your EC2 instance and restore the backup from the vault by using the AWS Console, as shown in Figure 3-12.\n\nFigure 3-12. Creating and restoring EC2 backups\n\nPrerequisites",
      "content_length": 660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "VPC with isolated subnets created in two AZs and associated route tables\n\nEC2 instance deployed\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nNavigate to the AWS Backup console and select “Protected resources” from the lefthand navigation pane.\n\n2.\n\nClick the “Create an on-demand backup” and select your EC2 instance, choose defaults, and click “Create on-demand backup” (see Figure 3-13).",
      "content_length": 443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "3.\n\nFigure 3-13. Creating on-demand backup\n\nNOTE\n\nAWS Backup will create a role for its purposes that uses the AWS Backup Managed Policy to perform\n\nthe required actions for backups. You can also create your own custom role if you require. For more\n\ninformation, see this AWS document.\n\nThe backup starts in the backup jobs view, as shown in Figure 3-14.\n\nFigure 3-14. View of backup job running\n\nWait for the backup to complete in your account (this may take a few moments to reach the Completed status shown in Figure 3-15).",
      "content_length": 526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "4.\n\n5.\n\nFigure 3-15. View of backup job completed\n\nSelect the Default Backup vault for your current Region and view your image backup that you just completed.\n\nClick the backup recovery point that you just created and choose Copy, as shown in Figure 3-16.",
      "content_length": 255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "6.\n\nFigure 3-16. Copy recovery point\n\nSelect your destination Region, keep all defaults, and click Copy (shown in Figure 3-17). You will see the copy job enter Running status, as shown in Figure 3-18.",
      "content_length": 200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "7.\n\n8.\n\nFigure 3-17. Copy recovery point to a different AWS Region\n\nFigure 3-18. Copy recovery point running\n\nAfter the copy job has completed in a few minutes, in the AWS Console, select the destination Region from the drop-down Region selector (top right of the AWS console).\n\nSelect your default Backup vault and choose the backup you wish to restore, as shown in Figure 3-19.",
      "content_length": 379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "9.\n\nFigure 3-19. Restore recovery point\n\nUnder “Network settings,” select your instance type and VPC for your restore, and click “Restore backup.” An example of inputs is shown in Figure 3-20.",
      "content_length": 192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Figure 3-20. Network settings for backup restoration\n\n10.\n\nYou can monitor the progress of the restore under the “Restore jobs” tab of the Jobs section in the console, shown in Figure 3-21.\n\nFigure 3-21. Recovery point restore job running\n\nValidation checks\n\nBrowse to the EC2 console to view your running instance in your destination Region. This EC2 instance is a copy of your original instance.",
      "content_length": 397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAWS Backup lets you manage and monitor the backups across the AWS services you use, from a single place. You can back up many AWS services and set backup policies for cloud resources in the AWS services that you use. You can also copy backups cross-Region within your account (or to other AWS accounts), which is what you explored in this recipe. EBS snapshots are an essential component of a backup strategy on AWS if you use the EC2 service to run instances with persistent data on them that you would like to protect. You can take snapshots of EBS volumes manually, write your own automation, automate them with Data Lifecycle Manager, or use AWS Backup.\n\nWhen you use AWS Backup to back up an EC2 instance, the service stores backups in a backup vault of your choice (a default backup vault is created if you do not have one), handles the process of building an Amazon Machine Image (AMI) which contains all of the configuration parameters, backed-up attached EBS volumes, and metadata. The service stores the entire bundle within the backup vault. This allows you to simply launch an EC2 instance from the AMI that was generated by the AWS Backup service to reduce the recovery time objective (RTO) associated with restoring an instance from a backup within your primary Region or another Region of your choice.\n\nChallenge\n\nConfigure an Automated Backup of the EC2 instance on a weekly schedule using backup plans, which copies the backed-up instances to a vault within another Region.\n\n3.8 Restoring a File from an EBS Snapshot\n\nProblem\n\nYou need to restore a file from an EBS snapshot that you have taken from a volume in your account.\n\nSolution\n\nCreate a volume from a snapshot, mount the volume from an EC2 instance, and copy the file to your instance volume (see Figure 3-22).",
      "content_length": 1881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Figure 3-22. Process flow for file restore from a snapshot\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nEC2 instance deployed. You will need the ability to connect to mount the EBS snapshot and restore a file.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nFind the EBS volume attached to your EC2 instance:\n\nORIG_VOLUME_ID=$(aws ec2 describe-volumes \\\n\n--filters Name=attachment.instance-id,Values=$INSTANCE_ID \\\n\n--output text \\\n\n--query Volumes[0].Attachments[0].VolumeId)\n\n2.\n\nTake a snapshot of the EBS volume (this will take a moment to complete):\n\nSNAPSHOT_ID=$(aws ec2 create-snapshot \\\n\n--volume-id $ORIG_VOLUME_ID \\\n\n--output text --query SnapshotId)",
      "content_length": 758,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "3.\n\nCreate a volume from the snapshot and save the VOLUME_ID as an environment variable:\n\nSNAP_VOLUME_ID=$(aws ec2 create-volume \\\n\n--snapshot-id $SNAPSHOT_ID \\\n\n--size 8 \\\n\n--volume-type gp2 \\\n\n--availability-zone us-east-1a \\\n\n--output text --query VolumeId)\n\nValidation checks\n\nAttach the volume to the EC2 instance as /dev/sdf:\n\naws ec2 attach-volume --volume-id $SNAP_VOLUME_ID \\\n\n--instance-id $INSTANCE_ID --device /dev/sdf\n\nWait until the volume’s state has reached Attached:\n\naws ec2 describe-volumes \\\n\n--volume-ids $SNAP_VOLUME_ID\n\nConnect to the EC2 instance using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nRun the lsblk command to see volumes, and note the volume name that you attached (you will use this volume name in the subsequent step to mount the volume):\n\nlsblk\n\nYou should see output similar to this:\n\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT",
      "content_length": 905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "nvme0n1 259:0 0 8G 0 disk\n\n├─nvme0n1p1 259:1 0 8G 0 part /\n\n└─nvme0n1p128 259:2 0 1M 0 part\n\nnvme1n1 259:3 0 8G 0 disk\n\n├─nvme1n1p1 259:4 0 8G 0 part\n\n└─nvme1n1p128 259:5 0 1M 0 part\n\nCreate a folder to mount the attached disk:\n\nsudo mkdir /mnt/restore\n\nMount the volume you attached to the folder you created:\n\nsudo mount -t xfs -o nouuid /dev/nvme1n1p1 /mnt/restore\n\nNOTE\n\nThe XFS file uses universally unique identifiers (UUIDs) to identify filesystems. By default, a safety mechanism is in\n\nplace in the mount command to prevent you from mounting the same filesystem twice. Since you created a block-level\n\nsnapshot and created a volume from it, the mount command you used requires overriding this check to allow mounting a\n\nvolume with the same UUID using the -o nouuid parameter. For more information, consult the man page for mount.\n\nCopy the file(s) you need from the mounted volume to the local filesystem:\n\nsudo cp /mnt/restore/home/ec2-user/.bash_profile \\\n\n/tmp/.bash_profile.restored\n\nUnmount the volume:\n\nsudo umount /dev/nvme1n1p1\n\nLog out of the EC2 instance:",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "exit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nEBS snapshots are an important part of a backup strategy within the EC2 service. If you run EC2 instances, snapshots enable you to restore an instance to a point in time when the snapshot was created. You can also create an EBS volume from a snapshot and attach it to a running instance, which you accomplished in this recipe. This is useful for site reliability engineering (SRE) teams, operations teams, and users who need to restore single files to a point in time to meet their needs.\n\nTIP\n\nEBS snapshots allow you to take snapshots of EBS volumes manually, write your own automation (like a Lambda\n\nfunction on a schedule), automate with Data Lifecycle Manager, or use AWS Backup (see Recipe 3.7) for a more\n\ncomprehensive solution to backing up and restoring EC2 instances.\n\nChallenge\n\nCreate an AMI from the snapshot you create and launch a new instance from the newly created AMI.\n\n3.9 Replicating Data Between EFS and S3 with DataSync\n\nProblem\n\nYou need to replicate files from Amazon S3 to Amazon EFS.\n\nSolution\n\nConfigure AWS DataSync with an S3 source and EFS target; then create a DataSync task and start the replication task, as shown in Figure 3-23.",
      "content_length": 1265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "Figure 3-23. Replicating S3 Bucket and EFS file system with DataSync\n\nPrerequisites\n\nAn S3 bucket and EFS file system.\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nEC2 instance deployed with EFS file system attached. You will need the ability to connect to it for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate an IAM role using the statement in the provided assume-role- policy.json file with this command:\n\nS3_ROLE_ARN=$(aws iam create-role --role-name AWSCookbookS3LocationRole \\\n\n--assume-role-policy-document file://assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\n2.\n\nAttach the AmazonS3ReadOnlyAccess IAM managed policy to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbookS3LocationRole \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\n\n3.\n\nCreate a DataSync S3 location:",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "4.\n\n5.\n\n6.\n\n7.\n\n8.\n\nS3_LOCATION_ARN=$(aws datasync create-location-s3 \\\n\n--s3-bucket-arn $BUCKET_ARN \\\n\n--s3-config BucketAccessRoleArn=$S3_ROLE_ARN \\\n\n--output text --query LocationArn)\n\nCreate an IAM role using the statement in the provided assume-role- policy.json file with this command:\n\nEFS_ROLE_ARN=$(aws iam create-role --role-name AWSCookbookEFSLocationRole \\\n\n--assume-role-policy-document file://assume-role-policy.json \\\n\n--output text --query Role.Arn)\n\nAttach the AmazonElasticFileSystemClientReadWriteAccess IAM managed policy to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbookEFSLocationRole \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonElasticFileSystemClientFullAccess\n\nGet the ARN of the EFS file system:\n\nEFS_FILE_SYSTEM_ARN=$(aws efs describe-file-systems \\\n\n--file-system-id $EFS_ID \\\n\n--output text --query FileSystems[0].FileSystemArn)\n\nGet the ARN of the subnet:\n\nSUBNET_ARN=$(aws ec2 describe-subnets \\\n\n--subnet-ids $ISOLATED_SUBNET_1 \\\n\n--output text --query Subnets[0].SubnetArn)\n\nGet the ARN of the security group:\n\nSG_ARN=arn:aws:ec2:$AWS_REGION:$AWS_ACCOUNT_ID:security-group/$EFS_SG",
      "content_length": 1134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "9.\n\nCreate a DataSync EFS location:\n\nEFS_LOCATION_ARN=$(aws datasync create-location-efs \\\n\n--efs-filesystem-arn $EFS_FILE_SYSTEM_ARN \\\n\n--ec2-config SubnetArn=$SUBNET_ARN,SecurityGroupArns=[$SG_ARN] \\\n\n--output text)\n\n10.\n\nCreate a DataSync task:\n\nTASK_ARN=$(aws datasync create-task \\\n\n--source-location-arn $S3_LOCATION_ARN \\\n\n--destination-location-arn $EFS_LOCATION_ARN \\\n\n--output text --query TaskArn)\n\n11.\n\nExecute the task:\n\naws datasync start-task-execution \\\n\n--task-arn $TASK_ARN\n\n12.\n\nEnsure the task has completed after a few seconds:\n\naws datasync list-task-executions \\\n\n--task-arn $TASK_ARN\n\nValidation checks\n\nEnsure your EC2 instance 1 has registered with SSM. Use this command to check the status. This command should return the instance ID:\n\naws ssm describe-instance-information \\\n\n--filters Key=ResourceType,Values=EC2Instance \\\n\n--query \"InstanceInformationList[].InstanceId\" --output text\n\nConnect to your EC2 instance using SSM Session Manager:",
      "content_length": 970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "aws ssm start-session --target $INSTANCE_ID\n\nThe EC2 instance has the EFS volume mounted at /mnt/efs. You can browse to the directory and view that the S3-Test-Content.txt file has been replicated from your S3 bucket to your EFS volume, as shown in the sample output:\n\nsh-4.2$ cd /mnt/efs\n\nsh-4.2$ ls\n\nsh-4.2$ ls -al\n\ntotal 12\n\ndrwxr-xr-x 3 nfsnobody nfsnobody 6144 Jan 1 1970 .\n\ndrwxr-xr-x 3 root root 17 Sep 10 02:07 ..\n\ndrwx------ 2 root root 6144 Sep 10 03:27 .aws-datasync\n\nrwxr-xr-x 1 nfsnobody nfsnobody 30 Jan 1 1970 S3-Test-Content.txt\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou can use AWS DataSync for both on-demand and ongoing/automated file synchronization tasks across a variety of AWS services. DataSync preserves metadata for copied items and checks file integrity during the synchronization task, supporting retries if needed. This is useful if you are a developer or cloud engineer looking to move data among a variety of sources and targets without provisioning any infrastructure or writing your own scripts to accomplish the same task. In this recipe, you used it to synchronize data between S3 and EFS hosted in your AWS account, but you can also use it to synchronize data among your non-AWS servers if you have a VPN connection, Direct Connect, or among other AWS accounts using VPC peering or a transit gateway.\n\nNOTE",
      "content_length": 1441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "At the time of this writing, the minimum automated sync schedule interval you can set is one hour. You can find other\n\ndetails about DataSync in the user documentation.\n\nLike many AWS services, DataSync uses IAM roles to perform actions against S3 and EFS for you. You granted DataSync the ability to interact with S3 and EFS. DataSync provisions network interfaces in your VPC to connect to your EFS file shares and uses the AWS APIs to interact with S3. It encrypts traffic in transit using TLS and also supports encryption at rest using KMS should your security and compliance requirements mandate encryption at rest.\n\nChallenge 1\n\nSet up a DataSync task that excludes filenames in a certain folder (e.g., private-folder).\n\nChallenge 2\n\nSet up a scheduled DataSync task to replicate data from S3 to EFS on an hourly schedule.",
      "content_length": 828,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Chapter 4. Databases\n\n4.0 Introduction You have a myriad of choices for using databases with AWS. Installing and running a database on EC2 provides you with the most choices of database engines and custom configurations, but brings about challenges like patching, backups, configuring high-availability, replication, and performance tuning. As noted on its product page, AWS offers managed database services that help address these challenges and cover a broad range of database types (relational, key- value/NoSQL, in-memory, document, wide column, graph, time series, ledger). When choosing a database type and data model, you must keep speed, volume, and access patterns in mind.\n\nThe managed database services on AWS integrate with many services to provide you additional functionality from security, operations, and development perspectives. In this chapter, you will explore Amazon Relational Database Service (RDS), NoSQL usage with Amazon DynamoDB, and the ways to migrate, secure, and operate these database types at scale. For example, you will learn how to integrate Secrets Manager with an RDS database to automatically rotate database user passwords. You will also learn how to leverage IAM authentication to reduce the application dependency on database passwords entirely, granting access to RDS through IAM permissions instead. You’ll explore autoscaling with DynamoDB and learn about why this is important from a cost and performance perspective.\n\nNOTE\n\nSome people think that Route 53 is a database but we disagree :-)\n\nNOTE\n\nSome database engines in the past have used certain terminology for replica configurations, default root user names,\n\nprimary tables, etc. We took care to use inclusive terminology throughout this chapter (and the whole book) wherever\n\npossible. We support the movement to use inclusive terminology in these commercial and open source database engines.\n\nWorkstation Configuration",
      "content_length": 1923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Follow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Databases\n\nWARNING\n\nDuring some of the steps in this chapter, you will create passwords and temporarily save them as environment variables to\n\nuse in subsequent steps. Make sure that you unset the environment variables by following the cleanup steps when you\n\ncomplete the recipe. We use this approach for simplicity of understanding. A more secure method (such as the method\n\nused in Recipe 1.8) should be used in production environments.\n\n4.1 Creating an Amazon Aurora Serverless PostgreSQL Database\n\nProblem\n\nYou have a web application that receives unpredictable requests that require storage in a relational database. You need a database solution that can scale with usage and be cost-effective. You would like to build a solution that has low operational overhead and must be compatible with your existing PostgreSQL-backed application.\n\nSolution\n\nConfigure and create an Aurora Serverless database cluster with a complex password. Then, apply a customized scaling configuration and enable automatic pause after inactivity. The scaling activity in response to the policy is shown in Figure 4-1.",
      "content_length": 1322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Figure 4-1. Aurora Serverless cluster scaling compute\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nEC2 instance deployed. You will need the ability to connect to this for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nUse AWS Secrets Manager to generate a complex password:\n\nADMIN_PASSWORD=$(aws secretsmanager get-random-password \\\n\n--exclude-punctuation \\\n\n--password-length 41 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)",
      "content_length": 559,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "2.\n\n3.\n\nNOTE\n\nWe are excluding punctuation characters from the password that we are creating because PostgreSQL\n\ndoes not support them. See the “Naming constraints in Amazon RDS” table.\n\nCreate a database subnet group specifying the VPC subnets to use for the cluster. Database subnet groups simplify the placement of RDS elastic network interfaces (ENIs):\n\naws rds create-db-subnet-group \\\n\n--db-subnet-group-name awscookbook401subnetgroup \\\n\n--db-subnet-group-description \"AWSCookbook401 subnet group\" \\\n\n--subnet-ids $SUBNET_ID_1 $SUBNET_ID_2\n\nYou should see output similar to the following:\n\n{\n\n\"DBSubnetGroup\": {\n\n\"DBSubnetGroupName\": \"awscookbook402subnetgroup\",\n\n\"DBSubnetGroupDescription\": \"AWSCookbook401 subnet group\",\n\n\"VpcId\": \"vpc-<<VPCID>>\",\n\n\"SubnetGroupStatus\": \"Complete\",\n\n\"Subnets\": [\n\n{\n\n\"SubnetIdentifier\": \"subnet-<<SUBNETID>>\",\n\n\"SubnetAvailabilityZone\": {\n\n\"Name\": \"us-east-1b\"\n\n},\n\n\"SubnetOutpost\": {},\n\n\"SubnetStatus\": \"Active\"\n\n},\n\n...\n\nCreate a VPC security group for the database:\n\nDB_SECURITY_GROUP_ID=$(aws ec2 create-security-group \\\n\n--group-name AWSCookbook401sg \\\n\n--description \"Aurora Serverless Security Group\" \\\n\n--vpc-id $VPC_ID --output text --query GroupId)",
      "content_length": 1199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "4.\n\n5.\n\n6.\n\nCreate a database cluster, specifying an engine-mode of serverless:\n\naws rds create-db-cluster \\\n\n--db-cluster-identifier awscookbook401dbcluster \\\n\n--engine aurora-postgresql \\\n\n--engine-mode serverless \\\n\n--engine-version 10.14 \\\n\n--master-username dbadmin \\\n\n--master-user-password $ADMIN_PASSWORD \\\n\n--db-subnet-group-name awscookbook401subnetgroup \\\n\n--vpc-security-group-ids $DB_SECURITY_GROUP_ID\n\nYou should see output similar to the following:\n\n{\n\n\"DBCluster\": {\n\n\"AllocatedStorage\": 1,\n\n\"AvailabilityZones\": [\n\n\"us-east-1f\",\n\n\"us-east-1b\",\n\n\"us-east-1a\"\n\n],\n\n\"BackupRetentionPeriod\": 1,\n\n\"DBClusterIdentifier\": \"awscookbook401dbcluster\",\n\n\"DBClusterParameterGroup\": \"default.aurora-postgresql10\",\n\n\"DBSubnetGroup\": \"awscookbook401subnetgroup\",\n\n\"Status\": \"creating\",\n\n...\n\nWait for the Status to read available; this will take a few moments:\n\naws rds describe-db-clusters \\\n\n--db-cluster-identifier awscookbook401dbcluster \\\n\n--output text --query DBClusters[0].Status\n\nModify the database to automatically scale with new autoscaling capacity targets (8 min, 16 max) and enable AutoPause after five minutes of inactivity:",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "7.\n\naws rds modify-db-cluster \\\n\n--db-cluster-identifier awscookbook401dbcluster --scaling-configuration \\\n\nMinCapacity=8,MaxCapacity=16,SecondsUntilAutoPause=300,TimeoutAction='ForceApply\n\nCapacityChange',AutoPause=true\n\nYou should see output similar to what you saw for step 4.\n\nNOTE\n\nIn practice, you may want to use a different AutoPause value. To determine what is appropriate for your use, evaluate your performance needs and Aurora pricing.\n\nWait at least five minutes and observe that the database’s capacity has scaled down to 0:\n\naws rds describe-db-clusters \\\n\n--db-cluster-identifier awscookbook401dbcluster \\\n\n--output text --query DBClusters[0].Capacity\n\nNOTE\n\nThe AutoPause feature automatically sets the capacity of the cluster to 0 after inactivity. When your database activity resumes (e.g., with a query or connection), the capacity value is automatically set to\n\nyour configured minimum scaling capacity value.\n\nGrant your EC2 instance’s security group access to the default PostgreSQL port:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 5432 \\\n\n--source-group $INSTANCE_SG \\\n\n--group-id $DB_SECURITY_GROUP_ID\n\nYou should see output similar to the following:",
      "content_length": 1199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<ID>>\",\n\n\"GroupId\": \"sg-<<ID>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 5432,\n\n\"ToPort\": 5432,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<ID>>\"\n\n}\n\n}\n\n]\n\n}\n\nValidation checks\n\nList the endpoint for the RDS cluster:\n\naws rds describe-db-clusters \\\n\n--db-cluster-identifier awscookbook401dbcluster \\\n\n--output text --query DBClusters[0].Endpoint\n\nYou should see something similar to this:\n\nawscookbook401dbcluster.cluster-<<unique>>.us-east-1.rds.amazonaws.com\n\nRetrieve the password for your RDS cluster:\n\necho $ADMIN_PASSWORD\n\nConnect to the EC2 instance by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID",
      "content_length": 773,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Install the PostgreSQL package so you can use the psql command to connect to the database:\n\nsudo yum -y install postgresql\n\nConnect to the database. This may take a moment as the database capacity is scaling up from 0. You’ll need to copy and paste the password (outputted previously):\n\npsql -h $HOST_NAME -U dbadmin -W -d postgres\n\nHere is an example of connecting to a database using the psql command:\n\nsh-4.2$ psql -h awscookbook401dbcluster.cluster-<<unique>>.us-east-1.rds.amazonaws.com -U dbadmin -W -\n\nd postgres\n\nPassword for user dbadmin:(paste in the password)\n\nQuit psql:\n\n\\q\n\nExit the Session Manager session:\n\nexit\n\nCheck the capacity of the cluster again to observe that the database has scaled up to the minimum value that you configured:\n\naws rds describe-db-clusters \\\n\n--db-cluster-identifier awscookbook401dbcluster \\\n\n--output text --query DBClusters[0].Capacity\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.",
      "content_length": 965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "TIP\n\nThe default behavior of deleting an RDS cluster is to take a final snapshot as a safety mechanism. We chose to skip this\n\nbehavior by adding the --skip-final-snapshot option to ensure you do not incur any costs for storing the snapshot in your AWS account. In a real-world scenario, you would likely want to retain the snapshot for a period of time in case you\n\nneeded to re-create the existing database from the snapshot.\n\nDiscussion\n\nThe cluster will automatically scale capacity to meet the needs of your usage. Setting MaxCapacity=16 limits the upper bound of your capacity to prevent runaway usage and unexpected costs. The cluster will set its capacity to 0 when no connection or activity is detected. This is triggered when the SecondsUntilAutoPause value is reached.\n\nWhen you enable AutoPause=true for your cluster, you pay for only the underlying storage during idle times. The default (and minimum) “inactivity period” is five minutes. Connecting to a paused cluster will cause the capacity to scale up to MinCapacity.\n\nWARNING\n\nNot all database engines and versions are available with the serverless engine. At the time of writing, the Aurora FAQ\n\nstates that Aurora Serverless is currently available for Aurora with MySQL 5.6 compatibility and for Aurora with\n\nPostgreSQL 10.7+ compatibility.\n\nThe user guide states that Aurora Serverless scaling is measured in capacity units (CUs) that correspond to compute and memory reserved for your cluster. This capability is a good fit for many workloads and use cases from development to batch-based workloads, and production workloads where traffic is unpredictable and costs associated with potential over-provisioning are a concern. By not needing to calculate baseline usage patterns, you can start developing quickly, and the cluster will automatically respond to the demand that your application requires.\n\nIf you currently use a “provisioned” capacity type cluster on Amazon RDS and would like to start using Aurora Serverless, you can snapshot your current database and restore it from within the AWS Console or from the command line to perform a migration. If your current database is not on RDS, you can use your database engine’s dump and restore features or use the AWS Database Migration Service (AWS DMS) to migrate to RDS.\n\nNOTE\n\nAt the time of this writing, Amazon Aurora Serverless v2 is in preview.",
      "content_length": 2377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "The user guide mentions that Aurora Serverless further builds on the existing Aurora platform, which replicates your database’s underlying storage six ways across three Availability Zones. While this replication is a benefit for resiliency, you should still use automated backups for your database to guard against operational errors. Aurora Serverless has automated backups enabled by default, and the backup retention can be increased up to 35 days if needed.\n\nNOTE\n\nPer the documentation, if your database cluster has been idle for more than seven days, the cluster will be backed up with\n\na snapshot. If this occurs, the database cluster is restored when there is a request to connect to it.\n\nChallenge\n\nChange the max capacity to 64 and idle time to 10 minutes for the database cluster.\n\nSee Also\n\nRecipe 4.2\n\nRecipe 4.7\n\n4.2 Using IAM Authentication with an RDS Database\n\nProblem\n\nYou have a server that connects to a database with a password and would like to instead use rotating temporary credentials.\n\nSolution\n\nFirst you will enable IAM authentication for your database. You will then configure the IAM permissions for the EC2 instance to use. Finally, create a new user on the database, retrieve the IAM authentication token, and verify connectivity (see Figure 4-2).",
      "content_length": 1279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "Figure 4-2. IAM authentication from an EC2 instance to an RDS database\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nAn RDS MySQL instance.\n\nEC2 instance deployed. You will need the ability to connect to this for configuring MySQL and testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nEnable IAM database authentication on the RDS database instance:\n\naws rds modify-db-instance \\\n\n--db-instance-identifier $RDS_DATABASE_ID \\\n\n--enable-iam-database-authentication \\\n\n--apply-immediately\n\nYou should see output similar to the following:\n\n{",
      "content_length": 640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "2.\n\n3.\n\n\"DBInstance\": {\n\n\"DBInstanceIdentifier\": \"awscookbookrecipe402\",\n\n\"DBInstanceClass\": \"db.m5.large\",\n\n\"Engine\": \"mysql\",\n\n\"DBInstanceStatus\": \"available\",\n\n\"MasterUsername\": \"admin\",\n\n\"DBName\": \"AWSCookbookRecipe402\",\n\n\"Endpoint\": {\n\n\"Address\": \"awscookbookrecipe402.<<ID>>.us-east-1.rds.amazonaws.com\",\n\n\"Port\": 3306,\n\n\"HostedZoneId\": \"<<ID>>\"\n\n},\n\n...\n\nWARNING\n\nIAM database authentication is available for only the database engines listed in this AWS article.\n\nRetrieve the RDS database instance resource ID:\n\nDB_RESOURCE_ID=$(aws rds describe-db-instances \\\n\n--query \\\n\n'DBInstances[?DBName==`AWSCookbookRecipe402`].DbiResourceId' \\\n\n--output text)\n\nCreate a file called policy.json with the following content (a policy- template.json file is provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"rds-db:connect\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:rds-db:AWS_REGION:AWS_ACCOUNT_ID:dbuser:DBResourceId/db_user\"\n\n]\n\n}",
      "content_length": 978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "4.\n\n5.\n\n]\n\n}\n\nNOTE\n\nIn the preceding example, db_user must match the name of the user in the database that we would like to allow to connect.\n\nReplace the values in the template file by using the sed command with environment variables you have set:\n\nsed -e \"s/AWS_ACCOUNT_ID/${AWS_ACCOUNT_ID}/g\" \\\n\ne \"s|AWS_REGION|${AWS_REGION}|g\" \\\n\ne \"s|DBResourceId|${DB_RESOURCE_ID}|g\" \\\n\npolicy-template.json > policy.json\n\nCreate an IAM policy using the file you just created:\n\naws iam create-policy --policy-name AWSCookbook402EC2RDSPolicy \\\n\n--policy-document file://policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Policy\": {\n\n\"PolicyName\": \"AWSCookbook402EC2RDSPolicy\",\n\n\"PolicyId\": \"<<ID>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:policy/AWSCookbook402EC2RDSPolicy\",\n\n\"Path\": \"/\",\n\n\"DefaultVersionId\": \"v1\",\n\n\"AttachmentCount\": 0,\n\n\"PermissionsBoundaryUsageCount\": 0,\n\n\"IsAttachable\": true,\n\n\"CreateDate\": \"2021-09-21T21:18:54+00:00\",\n\n\"UpdateDate\": \"2021-09-21T21:18:54+00:00\"\n\n}\n\n}",
      "content_length": 991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "6.\n\n7.\n\n8.\n\n9.\n\n10.\n\nAttach the IAM policy AWSCookbook402EC2RDSPolicy to the IAM role that the EC2 is using:\n\naws iam attach-role-policy --role-name $INSTANCE_ROLE_NAME \\\n\n--policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook402EC2RDSPolicy\n\nRetrieve the RDS admin password from Secrets Manager:\n\nRDS_ADMIN_PASSWORD=$(aws secretsmanager get-secret-value \\\n\n--secret-id $RDS_SECRET_ARN \\\n\n--query SecretString | jq -r | jq .password | tr -d '\"')\n\nOutput text so that you can use it later when you connect to the EC2 instance.\n\nList the endpoint for the RDS cluster:\n\necho $RDS_ENDPOINT\n\nYou should see output similar to the following:\n\nawscookbookrecipe402.<<unique>>.us-east-1.rds.amazonaws.com\n\nList the password for the RDS cluster:\n\necho $RDS_ADMIN_PASSWORD\n\nConnect to the EC2 instance using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nInstall MySQL:",
      "content_length": 901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "11.\n\n12.\n\n13.\n\nsudo yum -y install mysql\n\nConnect to the database. You’ll need to copy and paste the password and hostname (outputted in steps 7 and 8):\n\nmysql -u admin -p$DB_ADMIN_PASSWORD -h $RDS_ENDPOINT\n\nYou should see output similar to the following:\n\nWelcome to the MariaDB monitor. Commands end with ; or \\g.\n\nYour MySQL connection id is 18\n\nServer version: 8.0.23 Source distribution\n\nCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nMySQL [(none)]>\n\nNOTE\n\nIn the mysql command in step 11, there is no space between the -p flag and the first character of the password.\n\nCreate a new database user to associate with the IAM authentication:\n\nCREATE USER db_user@'%' IDENTIFIED WITH AWSAuthenticationPlugin as 'RDS';\n\nGRANT SELECT ON *.* TO 'db_user'@'%';\n\nFor both commands in step 12, you should see output similar to the following:\n\nQuery OK, 0 rows affected (0.01 sec)\n\nNow, exit the mysql prompt:",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "quit\n\nValidation checks\n\nWhile still on the EC2 instance, download the RDS Root CA (certificate authority) file provided by Amazon from the rds-downloads S3 bucket:\n\nsudo wget https://s3.amazonaws.com/rds-downloads/rds-ca-2019-root.pem\n\nSet the Region by grabbing the value from the instance’s metadata:\n\nexport AWS_DEFAULT_REGION=$(curl --silent http://169.254.169.254/latest/dynamic/instance-\n\nidentity/document \\\n\n| awk -F'\"' ' /region/ {print $4}')\n\nGenerate the RDS authentication token and save it as a variable. You’ll need to copy and paste the hostname (outputted in step 8):\n\nTOKEN=\"$(aws rds generate-db-auth-token --hostname $RDS_ENDPOINT --port 3306 --username db_user)\"\n\nConnect to the database using the RDS authentication token with the new db_user. You’ll need to copy and paste the hostname (outputted in step 8):\n\nmysql --host=$RDS_ENDPOINT --port=3306 \\\n\n--ssl-ca=rds-ca-2019-root.pem \\\n\n--user=db_user --password=$TOKEN\n\nRun a SELECT query at the mysql prompt to verify that this user has the SELECT *.* grant that you applied:\n\nSELECT user FROM mysql.user;\n\nYou should see output similar to the following:",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "MySQL [(none)]> SELECT user FROM mysql.user;\n\n+------------------+\n\n| user |\n\n+------------------+\n\n| admin |\n\n| db_user |\n\n| mysql.infoschema |\n\n| mysql.session |\n\n| mysql.sys |\n\n| rdsadmin |\n\n+------------------+\n\n6 rows in set (0.00 sec)\n\nExit the mysql prompt:\n\nquit\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nInstead of a password in your MySQL connection string, you retrieved and used a token associated with the EC2 instance’s IAM role. The documentation for IAM states that this token lasts for 15 minutes. If you install an application on this EC2 instance, the code can regularly refresh this token using the AWS SDK. There is no need to rotate passwords for your database user because the old token will be invalidated after 15 minutes.\n\nYou can create multiple database users associated with specific grants to allow your application to maintain different levels of access to your database. The grants happen within the database, not within the IAM permissions. IAM controls the db-connect action for the specific user. This IAM action allows the authentication token to be retrieved. That username is mapped from IAM to the GRANT(s) by using the same username within the database as in the policy.json file:\n\n{",
      "content_length": 1318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"rds-db:connect\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:rds-db:AWS_REGION::dbuser:DBResourceId/db_user\"\n\n]\n\n}\n\n]\n\n}\n\nIn this recipe, you also enabled encryption in transit by specifying the SSL certificate bundle that you downloaded to the EC2 instance in your database connection command. This encrypts the connection between your application and your database. This is a good security practice and is often required for many compliance standards. The connection string you used to connect with the IAM authentication token indicated an SSL certificate as one of the connection parameters. The certificate authority bundle is available to download from AWS and use within your application.\n\nChallenge\n\nTry connecting to the database from a Lambda function using IAM authentication. We have provided a lambda_function.py file in the repository to get you started.\n\n4.3 Leveraging RDS Proxy for Database Connections from Lambda\n\nProblem\n\nYou have a serverless function that is accessing a relational database and you need to implement connection pooling to minimize the number of database connections and improve performance.\n\nSolution\n\nCreate an RDS Proxy, associate it with your RDS MySQL database, and configure your Lambda to connect to the proxy instead of accessing the database directly (see Figure 4-3).",
      "content_length": 1378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "Figure 4-3. Lambda connection path to database via RDS Proxy\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables\n\nAn RDS MySQL instance\n\nA Lambda function that you would like to connect to your RDS database\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file called assume-role-policy.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "2.\n\n3.\n\n\"Service\": \"rds.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate an IAM role for the RDS Proxy using the assume-role-policy.json file:\n\naws iam create-role --assume-role-policy-document \\\n\nfile://assume-role-policy.json --role-name AWSCookbook403RDSProxy\n\nYou should see output similar to the following:\n\n{\n\n\"Role\": {\n\n\"Path\": \"/\",\n\n\"RoleName\": \"AWSCookbook403RDSProxy\",\n\n\"RoleId\": \"<<ID>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:role/AWSCookbook403RDSProxy\",\n\n\"CreateDate\": \"2021-09-21T22:33:57+00:00\",\n\n\"AssumeRolePolicyDocument\": {\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"rds.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\n}\n\n}\n\nCreate a security group to be used by the RDS Proxy:\n\nRDS_PROXY_SG_ID=$(aws ec2 create-security-group \\",
      "content_length": 819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "4.\n\n5.\n\n6.\n\n--group-name AWSCookbook403RDSProxySG \\\n\n--description \"Lambda Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\nCreate the RDS Proxy. This will take a few moments:\n\nRDS_PROXY_ENDPOINT_ARN=$(aws rds create-db-proxy \\\n\n--db-proxy-name $DB_NAME \\\n\n--engine-family MYSQL \\\n\n--auth '{\n\n\"AuthScheme\": \"SECRETS\",\n\n\"SecretArn\": \"'\"$RDS_SECRET_ARN\"'\",\n\n\"IAMAuth\": \"REQUIRED\"\n\n}' \\\n\n--role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook403RDSProxy \\\n\n--vpc-subnet-ids $ISOLATED_SUBNETS \\\n\n--vpc-security-group-ids $RDS_PROXY_SG_ID \\\n\n--require-tls --output text \\\n\n--query DBProxy.DBProxyArn)\n\nWait for the RDS Proxy to become available:\n\naws rds describe-db-proxies \\\n\n--db-proxy-name $DB_NAME \\\n\n--query DBProxies[0].Status \\\n\n--output text\n\nRetrieve the RDS_PROXY_ENDPOINT:\n\nRDS_PROXY_ENDPOINT=$(aws rds describe-db-proxies \\\n\n--db-proxy-name $DB_NAME \\\n\n--query DBProxies[0].Endpoint \\\n\n--output text)\n\nNext you need an IAM policy that allows the Lambda function to generate IAM authentication tokens. Create a file called template-policy.json with the following content (file provided in the repository):",
      "content_length": 1133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "7.\n\n8.\n\n9.\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"rds-db:connect\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:rds-db:AWS_REGION:AWS_ACCOUNT_ID:dbuser:RDSProxyID/admin\"\n\n]\n\n}\n\n]\n\n}\n\nSeparate out the Proxy ID from the RDS Proxy endpoint ARN. The Proxy ID is required for configuring IAM policies in the following steps:\n\nRDS_PROXY_ID=$(echo $RDS_PROXY_ENDPOINT_ARN | awk -F: '{ print $7} ')\n\nReplace the values in the template file by using the sed command with environment variables you have set:\n\nsed -e \"s/AWS_ACCOUNT_ID/${AWS_ACCOUNT_ID}/g\" \\\n\ne \"s|AWS_REGION|${AWS_REGION}|g\" \\\n\ne \"s|RDSProxyID|${RDS_PROXY_ID}|g\" \\\n\npolicy-template.json > policy.json\n\nCreate an IAM policy using the file you just created:\n\naws iam create-policy --policy-name AWSCookbook403RdsIamPolicy \\\n\n--policy-document file://policy.json\n\nYou should see output similar to the following:\n\n{",
      "content_length": 894,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "10.\n\n11.\n\n\"Policy\": {\n\n\"PolicyName\": \"AWSCookbook403RdsIamPolicy\",\n\n\"PolicyId\": \"<<Id>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:policy/AWSCookbook403RdsIamPolicy\",\n\n\"Path\": \"/\",\n\n\"DefaultVersionId\": \"v1\",\n\n\"AttachmentCount\": 0,\n\n\"PermissionsBoundaryUsageCount\": 0,\n\n\"IsAttachable\": true,\n\n\"CreateDate\": \"2021-09-21T22:50:24+00:00\",\n\n\"UpdateDate\": \"2021-09-21T22:50:24+00:00\"\n\n}\n\n}\n\nAttach the policy to the DBAppFunction Lambda function’s role:\n\naws iam attach-role-policy --role-name $DB_APP_FUNCTION_ROLE_NAME \\\n\n--policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook403RdsIamPolicy\n\nUse this command to check when the proxy enters the available status and then proceed:\n\naws rds describe-db-proxies --db-proxy-name $DB_NAME \\\n\n--query DBProxies[0].Status \\\n\n--output text\n\nAttach the SecretsManagerReadWrite policy to the RDS Proxy’s role:\n\naws iam attach-role-policy --role-name AWSCookbook403RDSProxy \\\n\n--policy-arn arn:aws:iam::aws:policy/SecretsManagerReadWrite\n\nTIP\n\nIn a production scenario, you would want to scope this permission down to the minimal secret\n\nresources that your application needs to access, rather than grant SecretsManagerReadWrite, which allows read/write for all secrets.",
      "content_length": 1205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "12.\n\n13.\n\nAdd an ingress rule to the RDS instance’s security group that allows access on TCP port 3306 (the default MySQL engine TCP port) from the RDS Proxy security group:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 3306 \\\n\n--source-group $RDS_PROXY_SG_ID \\\n\n--group-id $RDS_SECURITY_GROUP\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<ID>>\",\n\n\"GroupId\": \"sg-<<ID>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 3306,\n\n\"ToPort\": 3306,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<ID>>\"\n\n}\n\n}\n\n]\n\n}\n\nNOTE\n\nSecurity groups can reference other security groups. Because of dynamic IP addresses within VPCs,\n\nthis is considered the best way to grant access without opening up your security group too wide. For\n\nmore information, see Recipe 2.5.\n\nRegister targets with the RDS Proxy:\n\naws rds register-db-proxy-targets \\",
      "content_length": 964,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "14.\n\n--db-proxy-name $DB_NAME \\\n\n--db-instance-identifiers $RDS_DATABASE_ID\n\nYou should see output similar to the following:\n\n{\n\n\"DBProxyTargets\": [\n\n{\n\n\"Endpoint\": \"awscookbook403db.<<ID>>.us-east-1.rds.amazonaws.com\",\n\n\"RdsResourceId\": \"awscookbook403db\",\n\n\"Port\": 3306,\n\n\"Type\": \"RDS_INSTANCE\",\n\n\"TargetHealth\": {\n\n\"State\": \"REGISTERING\"\n\n}\n\n}\n\n]\n\n}\n\nCheck the status of the target registration with this command. Wait until the State reaches AVAILABLE:\n\naws rds describe-db-proxy-targets \\\n\n--db-proxy-name awscookbookrecipe403 \\\n\n--query Targets[0].TargetHealth.State \\\n\n--output text\n\nAdd an ingress rule to the RDS Proxy security group that allows access on TCP port 3306 from the Lambda App function’s security group:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 3306 \\\n\n--source-group $DB_APP_FUNCTION_SG_ID \\\n\n--group-id $RDS_PROXY_SG_ID\n\nYou should see output similar to the following:",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "15.\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<ID>>\",\n\n\"GroupId\": \"sg-<<ID>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 3306,\n\n\"ToPort\": 3306,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<ID>>\"\n\n}\n\n}\n\n]\n\n}\n\nModify the Lambda function to now use the RDS Proxy endpoint as the DB_HOST, instead of connecting directly to the database:\n\naws lambda update-function-configuration \\\n\n--function-name $DB_APP_FUNCTION_NAME \\\n\n--environment Variables={DB_HOST=$RDS_PROXY_ENDPOINT}\n\nYou should see output similar to the following:\n\n{\n\n\"FunctionName\": \"cdk-aws-cookbook-403-LambdaApp<<ID>>\",\n\n\"FunctionArn\": \"arn:aws:lambda:us-east-1:111111111111:function:cdk-aws-cookbook-\n\n403-LambdaApp<<ID>>\",\n\n\"Runtime\": \"python3.8\",\n\n\"Role\": \"arn:aws:iam::111111111111:role/cdk-aws-cookbook-403-\n\nLambdaAppServiceRole<<ID>>\",\n\n\"Handler\": \"lambda_function.lambda_handler\",\n\n\"CodeSize\": 665,\n\n\"Description\": \"\",\n\n\"Timeout\": 600,\n\n\"MemorySize\": 1024,\n\n...",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Validation checks\n\nRun the Lambda function with this command to validate that the function can connect to RDS using your RDS Proxy:\n\naws lambda invoke \\\n\n--function-name $DB_APP_FUNCTION_NAME \\\n\nresponse.json && cat response.json\n\nYou should see output similar to the following:\n\n{\n\n\"StatusCode\": 200,\n\n\"ExecutedVersion\": \"$LATEST\"\n\n}\n\n\"Successfully connected to RDS via RDS Proxy!\"\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nConnection pooling is important to consider when you use Lambda with RDS. Since the function could be executed with a lot of concurrency and frequency depending on your application, the number of raw connections to your database can grow and impact performance. By using RDS Proxy to manage the connections to the database, fewer connections are needed to the actual database. This setup increases performance and efficiency.\n\nWithout RDS Proxy, a Lambda function might establish a new connection to the database each time the function is invoked. This behavior depends on the execution environment, runtimes (Python, NodeJS, Go, etc.), and the way you instantiate connections to the database from the function code. In cases with large amounts of function concurrency, this could result in large amounts of TCP connections to your database, reducing database performance and increasing latency. Per the documentation, RDS Proxy helps manage the connections from Lambda by managing them as a “pool,” so that as concurrency increases, RDS Proxy increases the actual connections to the database only as needed, offloading the TCP overhead to RDS Proxy.\n\nSSL encryption in transit is supported by RDS Proxy when you include the certificate bundle provided by AWS in your database connection string. RDS Proxy supports MySQL and PostgreSQL RDS databases. For a complete listing of all supported database engines and versions, see this support document.",
      "content_length": 1927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "TIP\n\nYou can also architect to be efficient with short-lived database connections by leveraging the RDS Data API within your\n\napplication, which leverages a REST API exposed by Amazon RDS. For an example on the RDS Data API, see Recipe\n\n4.8.\n\nChallenge\n\nEnable enhanced logging for the RDS Proxy. This is useful for debugging.\n\n4.4 Encrypting the Storage of an Existing Amazon RDS for MySQL Database\n\nProblem\n\nYou need to encrypt the storage of an existing database.\n\nSolution\n\nCreate a read replica of your existing database, take a snapshot of the read replica, copy the snapshot to an encrypted snapshot, and restore the encrypted snapshot to a new encrypted database, as shown in Figure 4-4.\n\nFigure 4-4. Process of encrypting an RDS database using a snapshot",
      "content_length": 763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "Prerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables\n\nAn RDS MySQL instance with an RDS subnet group\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nVerify that the storage for the database is not encrypted:\n\naws rds describe-db-instances \\\n\n--db-instance-identifier $RDS_DATABASE_ID \\\n\n--query DBInstances[0].StorageEncrypted\n\nYou should see false outputted.\n\n2.\n\nCreate a KMS key to use to encrypt your database snapshot later. Store the key ID in an environment variable:\n\nKEY_ID=$(aws kms create-key \\\n\n--tags TagKey=Name,TagValue=AWSCookbook404RDS \\\n\n--description \"AWSCookbook RDS Key\" \\\n\n--query KeyMetadata.KeyId \\\n\n--output text)\n\n3.\n\nCreate an alias to easily reference the key that you created:\n\naws kms create-alias \\\n\n--alias-name alias/awscookbook404 \\\n\n--target-key-id $KEY_ID\n\n4.\n\nCreate a read replica of your existing unencrypted database:\n\naws rds create-db-instance-read-replica \\\n\n--db-instance-identifier awscookbook404db-rep \\",
      "content_length": 1035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "5.\n\n--source-db-instance-identifier $RDS_DATABASE_ID \\\n\n--max-allocated-storage 10\n\nYou should see output similar to the following:\n\n{\n\n\"DBInstance\": {\n\n\"DBInstanceIdentifier\": \"awscookbook404db-rep\",\n\n\"DBInstanceClass\": \"db.m5.large\",\n\n\"Engine\": \"mysql\",\n\n\"DBInstanceStatus\": \"creating\",\n\n\"MasterUsername\": \"admin\",\n\n\"DBName\": \"AWSCookbookRecipe404\",\n\n\"AllocatedStorage\": 8,\n\n\"PreferredBackupWindow\": \"05:51-06:21\",\n\n\"BackupRetentionPeriod\": 0,\n\n\"DBSecurityGroups\": [],\n\n...\n\nNOTE\n\nBy creating a read replica, you allow the snapshot to be created from it and therefore not affect the\n\nperformance of the primary database.\n\nWait for the DBInstanceStatus to become “available”:\n\naws rds describe-db-instances \\\n\n--db-instance-identifier awscookbook404db-rep \\\n\n--output text --query DBInstances[0].DBInstanceStatus\n\nTake an unencrypted snapshot of your read replica:\n\naws rds create-db-snapshot \\\n\n--db-instance-identifier awscookbook404db-rep \\\n\n--db-snapshot-identifier awscookbook404-snapshot\n\nYou should see output similar to the following:",
      "content_length": 1043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "6.\n\n{\n\n\"DBSnapshot\": {\n\n\"DBSnapshotIdentifier\": \"awscookbook404-snapshot\",\n\n\"DBInstanceIdentifier\": \"awscookbook404db-rep\",\n\n\"Engine\": \"mysql\",\n\n\"AllocatedStorage\": 8,\n\n\"Status\": \"creating\",\n\n\"Port\": 3306,\n\n\"AvailabilityZone\": \"us-east-1b\",\n\n\"VpcId\": \"vpc-<<ID>>\",\n\n\"InstanceCreateTime\": \"2021-09-21T22:46:07.785000+00:00\",\n\nWait for the Status of the snapshot to become available:\n\naws rds describe-db-snapshots \\\n\n--db-snapshot-identifier awscookbook404-snapshot \\\n\n--output text --query DBSnapshots[0].Status\n\nCopy the unencrypted snapshot to a new snapshot while encrypting by specifying your KMS key:\n\naws rds copy-db-snapshot \\\n\n--copy-tags \\\n\n--source-db-snapshot-identifier awscookbook404-snapshot \\\n\n--target-db-snapshot-identifier awscookbook404-snapshot-enc \\\n\n--kms-key-id alias/awscookbook404\n\nYou should see output similar to the following:\n\n{\n\n\"DBSnapshot\": {\n\n\"DBSnapshotIdentifier\": \"awscookbook404-snapshot-enc\",\n\n\"DBInstanceIdentifier\": \"awscookbook404db-rep\",\n\n\"Engine\": \"mysql\",\n\n\"AllocatedStorage\": 8,\n\n\"Status\": \"creating\",\n\n\"Port\": 3306,\n\n\"AvailabilityZone\": \"us-east-1b\",\n\n\"VpcId\": \"vpc-<<ID>>\",",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "\"InstanceCreateTime\": \"2021-09-21T22:46:07.785000+00:00\",\n\n\"MasterUsername\": \"admin\",\n\n...\n\nTIP\n\nSpecifying a KMS key with the copy-snapshot command encrypts the copied snapshot. Restoring an encrypted snapshot to a new database results in an encrypted database.\n\nWait for the Status of the encrypted snapshot to become available:\n\naws rds describe-db-snapshots \\\n\n--db-snapshot-identifier awscookbook404-snapshot-enc \\\n\n--output text --query DBSnapshots[0].Status\n\n7.\n\nRestore the encrypted snapshot to a new RDS instance:\n\naws rds restore-db-instance-from-db-snapshot \\\n\n--db-subnet-group-name $RDS_SUBNET_GROUP \\\n\n--db-instance-identifier awscookbook404db-enc \\\n\n--db-snapshot-identifier awscookbook404-snapshot-enc\n\nYou should see output similar to the following:\n\n{\n\n\"DBInstance\": {\n\n\"DBInstanceIdentifier\": \"awscookbook404db-enc\",\n\n\"DBInstanceClass\": \"db.m5.large\",\n\n\"Engine\": \"mysql\",\n\n\"DBInstanceStatus\": \"creating\",\n\n\"MasterUsername\": \"admin\",\n\n\"DBName\": \"AWSCookbookRecipe404\",\n\n\"AllocatedStorage\": 8,\n\n...\n\nValidation checks\n\nWait for DBInstanceStatus to become available:",
      "content_length": 1083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "aws rds describe-db-instances \\\n\n--db-instance-identifier awscookbook404db-enc \\\n\n--output text --query DBInstances[0].DBInstanceStatus\n\nVerify that the storage is now encrypted:\n\naws rds describe-db-instances \\\n\n--db-instance-identifier awscookbook404db-enc \\\n\n--query DBInstances[0].StorageEncrypted\n\nYou should see true outputted.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen you complete the steps, you need to reconfigure your application to point to a new database endpoint hostname. To perform this with minimal downtime, you can configure a Route 53 DNS record that points to your database endpoint. Your application would be configured to use the DNS record. Then you would shift your database traffic over to the new encrypted database by updating the DNS record with the new database endpoint DNS.\n\nEncryption at rest is a security approach left up to end users in the AWS shared responsibility model, and often it is required to achieve or maintain compliance with regulatory standards. The encrypted snapshot you took could also be automatically copied to another Region, as well as exported to S3 for archival/backup purposes.\n\nChallenge\n\nCreate an RDS database from scratch that initially has encrypted storage and migrate your data from your existing database to the new database using AWS DMS, as shown in Recipe 4.7.",
      "content_length": 1390,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "4.5 Automating Password Rotation for RDS Databases\n\nProblem\n\nYou would like to implement automatic password rotation for a database user.\n\nSolution\n\nCreate a password and place it in AWS Secrets Manager. Configure a rotation interval for the secret containing the password. Finally, create a Lambda function using AWS-provided code, and configure the function to perform the password rotation. This configuration allows the password rotation automation to perform as shown in Figure 4-5.\n\nFigure 4-5. Secrets Manager Lambda function integration",
      "content_length": 544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "Prerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nMySQL RDS instance and EC2 instance deployed. You will need the ability to connect to these for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nUse AWS Secrets Manager to generate a password that meets RDS requirements:\n\nRDS_ADMIN_PASSWORD=$(aws secretsmanager get-random-password \\\n\n--exclude-punctuation \\\n\n--password-length 41 --require-each-included-type \\\n\n--output text --query RandomPassword)\n\nTIP\n\nYou can call the Secrets Manager GetRandomPassword API method to generate random strings of\n\ncharacters for various uses beyond password generation.\n\n2.\n\nChange the admin password for your RDS database to the one you just created:\n\naws rds modify-db-instance \\\n\n--db-instance-identifier $RDS_DATABASE_ID \\\n\n--master-user-password $RDS_ADMIN_PASSWORD \\\n\n--apply-immediately\n\nYou should see output similar to the following:\n\n{\n\n\"DBInstance\": {\n\n\"DBInstanceIdentifier\": \"awscookbook405db\",\n\n\"DBInstanceClass\": \"db.m5.large\",",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "3.\n\n4.\n\n5.\n\n\"Engine\": \"mysql\",\n\n\"DBInstanceStatus\": \"available\",\n\n\"MasterUsername\": \"admin\",\n\n\"DBName\": \"AWSCookbookRecipe405\",\n\n...\n\nCreate a file with the following content called rdscreds-template.json (file provided in the repository):\n\n{\n\n\"username\": \"admin\",\n\n\"password\": \"PASSWORD\",\n\n\"engine\": \"mysql\",\n\n\"host\": \"HOST\",\n\n\"port\": 3306,\n\n\"dbname\": \"DBNAME\",\n\n\"dbInstanceIdentifier\": \"DBIDENTIFIER\"\n\n}\n\nUse sed to modify the values in rdscreds-template.json to create rdscreds.json:\n\nsed -e \"s/AWS_ACCOUNT_ID/${AWS_ACCOUNT_ID}/g\" \\\n\ne \"s|PASSWORD|${RDS_ADMIN_PASSWORD}|g\" \\\n\ne \"s|HOST|${RdsEndpoint}|g\" \\\n\ne \"s|DBNAME|${DbName}|g\" \\\n\ne \"s|DBIDENTIFIER|${RdsDatabaseId}|g\" \\\n\nrdscreds-template.json > rdscreds.json\n\nDownload code from the AWS Samples GitHub repository for the Rotation Lambda function:\n\nwget https://raw.githubusercontent.com/aws-samples/aws-secrets-manager-rotation-\n\nlambdas/master/SecretsManagerRDSMySQLRotationSingleUser/lambda_function.py\n\nNOTE\n\nAWS provides information and templates for different database rotation scenarios in this article.",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "6.\n\n7.\n\n8.\n\nCompress the file containing the code:\n\nzip lambda_function.zip lambda_function.py\n\nYou should see output similar to the following:\n\nadding: lambda_function.py (deflated 76%)\n\nCreate a new security group for the Lambda function to use:\n\nLAMBDA_SG_ID=$(aws ec2 create-security-group \\\n\n--group-name AWSCookbook405LambdaSG \\\n\n--description \"Lambda Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\nAdd an ingress rule to the RDS instances security group that allows access on TCP port 3306 from the Lambda’s security group:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 3306 \\\n\n--source-group $LAMBDA_SG_ID \\\n\n--group-id $RDS_SECURITY_GROUP\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-<<ID>>\",\n\n\"GroupId\": \"sg-<<ID>>\",\n\n\"GroupOwnerId\": \"111111111111\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 3306,",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "9.\n\n10.\n\n\"ToPort\": 3306,\n\n\"ReferencedGroupInfo\": {\n\n\"GroupId\": \"sg-<<ID>>\"\n\n}\n\n}\n\n]\n\n}\n\nCreate a file named assume-role-policy.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"lambda.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate an IAM role with the statement in the provided assume-role- policy.json file using this command:\n\naws iam create-role --role-name AWSCookbook405Lambda \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Role\": {\n\n\"Path\": \"/\",\n\n\"RoleName\": \"AWSCookbook405Lambda\",\n\n\"RoleId\": \"<<ID>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:role/AWSCookbook405Lambda\",\n\n\"CreateDate\": \"2021-09-21T23:01:57+00:00\",\n\n\"AssumeRolePolicyDocument\": {",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "11.\n\n12.\n\n13.\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n...\n\nAttach the IAM managed policy for AWSLambdaVPCAccess to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook405Lambda \\\n\n--policy-arn arn:aws:iam::aws:policy/service-\n\nrole/AWSLambdaVPCAccessExecutionRole\n\nAttach the IAM managed policy for SecretsManagerReadWrite to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook405Lambda \\\n\n--policy-arn arn:aws:iam::aws:policy/SecretsManagerReadWrite\n\nTIP\n\nThe IAM role that you associated with the Lambda function to rotate the password used the\n\nSecretsManagerReadWrite managed policy. In a production scenario, you would want to scope this down to limit which secrets the Lambda function can interact with.\n\nCreate the Lambda function to perform the secret rotation using the code:\n\nLAMBDA_ROTATE_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook405Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--environment Variables=\n\n{SECRETS_MANAGER_ENDPOINT=https://secretsmanager.$AWS_REGION.amazonaws.com} \\\n\n--layers $PyMysqlLambdaLayerArn \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook405Lambda \\\n\n--output text --query FunctionArn \\\n\n--vpc-config SubnetIds=${ISOLATED_SUBNETS},SecurityGroupIds=$LAMBDA_SG_ID)",
      "content_length": 1368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "14.\n\n15.\n\n16.\n\nUse this command to determine when the Lambda function has entered the Active state:\n\naws lambda get-function --function-name $LAMBDA_ROTATE_ARN \\\n\n--output text --query Configuration.State\n\nAdd a permission to the Lambda function so that Secrets Manager can invoke it:\n\naws lambda add-permission --function-name $LAMBDA_ROTATE_ARN \\\n\n--action lambda:InvokeFunction --statement-id secretsmanager \\\n\n--principal secretsmanager.amazonaws.com\n\nYou should see output similar to the following:\n\n{\n\n\"Statement\": \"{\\\"Sid\\\":\\\"secretsmanager\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":\n\n{\\\"Service\\\":\\\"secretsmanager.amazonaws.com\\\"},\\\"Action\\\":\\\"lambda:InvokeFunctio\n\nn\\\",\\\"Resource\\\":\\\"arn:aws:lambda:us-east-\n\n1:111111111111:function:AWSCookbook405Lambda\\\"}\"\n\n}\n\nSet a unique suffix to use for the secret name to ensure you can reuse this pattern for additional automatic password rotations if desired:\n\nAWSCookbook405SecretName=AWSCookbook405Secret-$(aws secretsmanager \\\n\nget-random-password \\\n\n--exclude-punctuation \\\n\n--password-length 6 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nCreate a secret in Secrets Manager to store your admin password:\n\naws secretsmanager create-secret --name $AWSCookbook405SecretName \\",
      "content_length": 1252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "17.\n\n18.\n\n--description \"My database secret created with the CLI\" \\\n\n--secret-string file://rdscreds.json\n\nYou should see output similar to the following:\n\n{\n\n\"ARN\": \"arn:aws:secretsmanager:us-east-\n\n1:1111111111111:secret:AWSCookbook405Secret-T4tErs-AlJcLn\",\n\n\"Name\": \"AWSCookbook405Secret-<<Random>>\",\n\n\"VersionId\": \"<<ID>>\"\n\n}\n\nSet up automatic rotation every 30 days and specify the Lambda function to perform rotation for the secret you just created:\n\naws secretsmanager rotate-secret \\\n\n--secret-id $AWSCookbook405SecretName \\\n\n--rotation-rules AutomaticallyAfterDays=30 \\\n\n--rotation-lambda-arn $LAMBDA_ROTATE_ARN\n\nYou should see output similar to the following:\n\n{\n\n\"ARN\": \"arn:aws:secretsmanager:us-east-\n\n1:1111111111111:secret:AWSCookbook405Secret-<<unique>>\",\n\n\"Name\": \"AWSCookbook405Secret-<<unique>>\",\n\n\"VersionId\": \"<<ID>>\"\n\n}\n\nNOTE\n\nThe rotate-secret command triggers an initial rotation of the password. You will trigger an extra rotation of the password in the next step to demonstrate how to perform rotations on demand.\n\nPerform another rotation of the secret:",
      "content_length": 1080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "aws secretsmanager rotate-secret --secret-id $AWSCookbook405SecretName\n\nYou should see output similar to the output from step 17. Notice that the V e r s i o n I d will be different from the last command indicating that the secret has been rotated.\n\nValidation checks\n\nRetrieve the RDS admin password from Secrets Manager:\n\nRDS_ADMIN_PASSWORD=$(aws secretsmanager get-secret-value --secret-id $AWSCookbook405SecretName --query\n\nSecretString | jq -r | jq .password | tr -d '\"')\n\nList the endpoint for the RDS cluster:\n\necho $RDS_ENDPOINT\n\nRetrieve the password for your RDS cluster:\n\necho $RDS_ADMIN_PASSWORD\n\nConnect to the EC2 instance by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nInstall the MySQL client:\n\nsudo yum -y install mysql\n\nConnect to the database to verify that the latest rotated password is working. You’ll need to copy and paste the password (outputted previously):",
      "content_length": 929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "mysql -u admin -p$password -h $hostname\n\nRun a SELECT statement on the mysql.user table to validate administrator permissions:\n\nSELECT user FROM mysql.user;\n\nYou should see output similar to the following:\n\n+------------------+\n\n| user |\n\n+------------------+\n\n| admin |\n\n| mysql.infoschema |\n\n| mysql.session |\n\n| mysql.sys |\n\n| rdsadmin |\n\n+------------------+\n\n5 rows in set (0.00 sec)\n\nExit from the mysql prompt:\n\nquit\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThe AWS-provided Lambda function stores the rotated password in Secrets Manager. You can then configure your application to retrieve secrets from Secrets Manager directly; or the Lambda function you configured to update the Secrets Manager values could also store the password in a",
      "content_length": 842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "secure location of your choosing. You would need to grant the Lambda additional permissions to interact with the secure location you choose and add some code to store the new value there. This method could also be applied to rotate the passwords for nonadmin database user accounts by following the same steps after you have created the user(s) in your database.\n\nThe Lambda function you deployed is Python-based and connects to a MySQL engine- compatible database. The Lambda runtime environment does not have this library included by default, so you specified a Lambda layer with the aws lambda create-function command. This layer is required so that the PyMySQL library was available to the function in the Lambda runtime environment, and it was deployed for you as part of the preparation step when you ran cdk deploy.\n\nChallenge\n\nCreate another Lambda function and a separate IAM role. Grant this new function access to the same secret.\n\nSee Also\n\nRecipe 5.2",
      "content_length": 963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "4.6 Autoscaling DynamoDB Table Provisioned Capacity\n\nProblem\n\nYou have a DynamoDB database table with a low provisioned throughput. You realize that your application load is variable and you may need to scale up or scale down your provisioned throughput based on the variability of the incoming application load.\n\nSolution\n\nConfigure read and write scaling by setting a scaling target and a scaling policy for the read and write capacity of the DynamoDB table by using AWS application autoscaling, as shown in Figure 4-6.\n\nFigure 4-6. DynamoDB autoscaling configuration\n\nPrerequisite\n\nA DynamoDB table",
      "content_length": 601,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "Preparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nNavigate to this recipe’s directory in the chapter repository:\n\ncd 406-Auto-Scaling-DynamoDB\n\n2.\n\nRegister a ReadCapacityUnits scaling target for the DynamoDB table:\n\naws application-autoscaling register-scalable-target \\\n\n--service-namespace dynamodb \\\n\n--resource-id \"table/AWSCookbook406\" \\\n\n--scalable-dimension \"dynamodb:table:ReadCapacityUnits\" \\\n\n--min-capacity 5 \\\n\n--max-capacity 10\n\n3.\n\nRegister a WriteCapacityUnits scaling target for the DynamoDB table:\n\naws application-autoscaling register-scalable-target \\\n\n--service-namespace dynamodb \\\n\n--resource-id \"table/AWSCookbook406\" \\\n\n--scalable-dimension \"dynamodb:table:WriteCapacityUnits\" \\\n\n--min-capacity 5 \\\n\n--max-capacity 10\n\n4.\n\nCreate a scaling policy JSON file for read capacity scaling (read-policy.json provided in the repository):\n\n{\n\n\"PredefinedMetricSpecification\": {\n\n\"PredefinedMetricType\": \"DynamoDBReadCapacityUtilization\"\n\n},\n\n\"ScaleOutCooldown\": 60,\n\n\"ScaleInCooldown\": 60,\n\n\"TargetValue\": 50.0\n\n}",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "5.\n\n6.\n\n7.\n\nCreate a scaling policy JSON file for write capacity scaling (write- policy.json file provided in the repository):\n\n{\n\n\"PredefinedMetricSpecification\": {\n\n\"PredefinedMetricType\": \"DynamoDBWriteCapacityUtilization\"\n\n},\n\n\"ScaleOutCooldown\": 60,\n\n\"ScaleInCooldown\": 60,\n\n\"TargetValue\": 50.0\n\n}\n\nNOTE\n\nDynamoDB-provisioned capacity uses capacity units to define the read and write capacity of your\n\ntables. The target value that you set defines when to scale based on the current usage. Scaling\n\ncooldown parameters define, in seconds, how long to wait to scale again after a scaling operation has\n\ntaken place. For more information, see the API reference for autoscaling\n\nTargetTrackingScalingPolicyConfiguration.\n\nApply the read scaling policy to the table by using the read-policy.json file:\n\naws application-autoscaling put-scaling-policy \\\n\n--service-namespace dynamodb \\\n\n--resource-id \"table/AWSCookbook406\" \\\n\n--scalable-dimension \"dynamodb:table:ReadCapacityUnits\" \\\n\n--policy-name \"AWSCookbookReadScaling\" \\\n\n--policy-type \"TargetTrackingScaling\" \\\n\n--target-tracking-scaling-policy-configuration \\\n\nfile://read-policy.json\n\nApply the write scaling policy to the table using the write-policy.json file:\n\naws application-autoscaling put-scaling-policy \\\n\n--service-namespace dynamodb \\\n\n--resource-id \"table/AWSCookbook406\" \\\n\n--scalable-dimension \"dynamodb:table:WriteCapacityUnits\" \\\n\n--policy-name \"AWSCookbookWriteScaling\" \\\n\n--policy-type \"TargetTrackingScaling\" \\",
      "content_length": 1486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "--target-tracking-scaling-policy-configuration \\\n\nfile://write-policy.json\n\nValidation checks\n\nYou can observe the autoscaling configuration for your table by selecting it in the DynamoDB console and looking under the “Additional settings” tab.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nTIP\n\nThese steps will autoscale read and write capacities independently for your DynamoDB table, which helps you achieve the\n\nlowest operating cost model for your application’s specific requirements.\n\nDynamoDB allows for two capacity modes: provisioned and on-demand. When using provisioned capacity mode, you are able to select the number of data reads and writes per second. The pricing guide notes that you are charged according to the capacity units you specify. Conversely, with on-demand capacity mode, you pay per request for the data reads and writes your application performs on your tables. In general, using on-demand mode can result in higher costs over provisioned mode for especially transactionally heavy applications.\n\nYou need to understand your application and usage patterns when selecting a provisioned capacity for your tables. If you set the capacity too low, you will experience slow database performance and your application could enter error and wait states, since the DynamoDB API will return ThrottlingException and ProvisionedThroughputExceededException responses to your application when these limits are met. If you set the capacity too high, you are paying for unneeded capacity. Enabling autoscaling allows you to define minimum and maximum target values by setting a scaling target, while also allowing you to define when the autoscaling trigger should go into effect for scaling up, and when it should begin to scale down your capacity. This allows you to optimize for both cost and performance while taking advantage of the DynamoDB service. To see a list of the scalable targets that you configured for your table, you can use the following command:\n\naws application-autoscaling describe-scalable-targets \\\n\n--service-namespace dynamodb \\\n\n--resource-id \"table/AWSCookbook406\"",
      "content_length": 2153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "For more information on DynamoDB capacities and how they are measured, see this support document.\n\nChallenge\n\nCreate a Lambda function that monitors the performance of your DynamoDB table, and then modify the autoscaling target minimums and maximums accordingly.\n\n4.7 Migrating Databases to Amazon RDS Using AWS DMS\n\nProblem\n\nYou need to move data from a source database to a target database.\n\nSolution\n\nConfigure the VPC security groups and IAM permissions to allow AWS Database Migration Service (DMS) connectivity to the databases. Then, configure the DMS endpoints for the source and target databases. Next, configure a DMS replication task. Finally, start the replication task. An architecture diagram of the solution is shown in Figure 4-7.",
      "content_length": 746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "Figure 4-7. DMS network diagram",
      "content_length": 31,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "Preparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a security group for the replication instance:\n\nDMS_SG_ID=$(aws ec2 create-security-group \\\n\n--group-name AWSCookbook407DMSSG \\\n\n--description \"DMS Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\n2.\n\nGrant the DMS security group access to the source and target databases on TCP port 3306:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 3306 \\\n\n--source-group $DMS_SG_ID \\\n\n--group-id $SOURCE_RDS_SECURITY_GROUP\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 3306 \\\n\n--source-group $DMS_SG_ID \\\n\n--group-id $TARGET_RDS_SECURITY_GROUP\n\n3.\n\nCreate a role for DMS by using the assume-role-policy.json provided:\n\naws iam create-role --role-name dms-vpc-role \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\nWARNING\n\nThe DMS service requires an IAM role with a specific name and a specific policy. The command you\n\nran previously satisfies this requirement. You may also already have this role in your account if you\n\nhave used DMS previously. This command would result in an error if that is the case, and you can\n\nproceed with the next steps without concern.\n\n4.\n\nAttach the managed DMS policy to the role:",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "5.\n\n6.\n\naws iam attach-role-policy --role-name dms-vpc-role --policy-arn \\\n\narn:aws:iam::aws:policy/service-role/AmazonDMSVPCManagementRole\n\nCreate a replication subnet group for the replication instance:\n\nREP_SUBNET_GROUP=$(aws dms create-replication-subnet-group \\\n\n--replication-subnet-group-identifier awscookbook407 \\\n\n--replication-subnet-group-description \"AWSCookbook407\" \\\n\n--subnet-ids $ISOLATED_SUBNETS \\\n\n--query ReplicationSubnetGroup.ReplicationSubnetGroupIdentifier \\\n\n--output text)\n\nCreate a replication instance and save the ARN in a variable:\n\nREP_INSTANCE_ARN=$(aws dms create-replication-instance \\\n\n--replication-instance-identifier awscookbook407 \\\n\n--no-publicly-accessible \\\n\n--replication-instance-class dms.t2.medium \\\n\n--vpc-security-group-ids $DMS_SG_ID \\\n\n--replication-subnet-group-identifier $REP_SUBNET_GROUP \\\n\n--allocated-storage 8 \\\n\n--query ReplicationInstance.ReplicationInstanceArn \\\n\n--output text)\n\nWait until the ReplicationInstanceStatus reaches available; check the status by using this command:\n\naws dms describe-replication-instances \\\n\n--filter=Name=replication-instance-id,Values=awscookbook407 \\\n\n--query ReplicationInstances[0].ReplicationInstanceStatus\n\nWARNING\n\nYou used the dms.t2.medium replication instance size for this example. You should choose an instance size appropriate to handle the amount of data you will be migrating. DMS transfers tables in parallel,\n\nso you will need a larger instance size for larger amounts of data. For more information, see this user\n\nguide document about best practices for DMS.",
      "content_length": 1568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "7.\n\n8.\n\n9.\n\n10.\n\nRetrieve the source and target DB admin passwords from Secrets Manager and save to environment variables:\n\nRDS_SOURCE_PASSWORD=$(aws secretsmanager get-secret-value --secret-id\n\n$RDS_SOURCE_SECRET_NAME --query\n\nSecretString --output text | jq .password | tr -d '\"')\n\nRDS_TARGET_PASSWORD=$(aws secretsmanager get-secret-value --secret-id\n\n$RDS_TARGET_SECRET_NAME --query\n\nSecretString --output text | jq .password | tr -d '\"')\n\nCreate a source endpoint for DMS and save the ARN to a variable:\n\nSOURCE_ENDPOINT_ARN=$(aws dms create-endpoint \\\n\n--endpoint-identifier awscookbook407source \\\n\n--endpoint-type source --engine-name mysql \\\n\n--username admin --password $RDS_SOURCE_PASSWORD \\\n\n--server-name $SOURCE_RDS_ENDPOINT --port 3306 \\\n\n--query Endpoint.EndpointArn --output text)\n\nCreate a target endpoint for DMS and save the ARN to a variable:\n\nTARGET_ENDPOINT_ARN=$(aws dms create-endpoint \\\n\n--endpoint-identifier awscookbook407target \\\n\n--endpoint-type target --engine-name mysql \\\n\n--username admin --password $RDS_TARGET_PASSWORD \\\n\n--server-name $TARGET_RDS_ENDPOINT --port 3306 \\\n\n--query Endpoint.EndpointArn --output text)\n\nCreate your replication task:\n\nREPLICATION_TASK_ARN=$(aws dms create-replication-task \\\n\n--replication-task-identifier awscookbook-task \\\n\n--source-endpoint-arn $SOURCE_ENDPOINT_ARN \\\n\n--target-endpoint-arn $TARGET_ENDPOINT_ARN \\\n\n--replication-instance-arn $REP_INSTANCE_ARN \\\n\n--migration-type full-load \\",
      "content_length": 1459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "--table-mappings file://table-mapping-all.json \\\n\n--query ReplicationTask.ReplicationTaskArn --output text)\n\nWait for the status to reach ready. To check the status of the replication task, use the following:\n\naws dms describe-replication-tasks \\\n\n--filters \"Name=replication-task-arn,Values=$REPLICATION_TASK_ARN\" \\\n\n--query \"ReplicationTasks[0].Status\"\n\n11.\n\nStart the replication task:\n\naws dms start-replication-task \\\n\n--replication-task-arn $REPLICATION_TASK_ARN \\\n\n--start-replication-task-type start-replication\n\nValidation checks\n\nMonitor the progress of the replication task:\n\naws dms describe-replication-tasks\n\nUse the AWS Console or the aws dms describe-replication-tasks operation to validate that your tables have been migrated:\n\naws dms describe-replication-tasks \\\n\n--query ReplicationTasks[0].ReplicationTaskStats\n\nYou can also view the status of the replication task in the DMS console.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion",
      "content_length": 1000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "TIP\n\nYou could also run full-load-and-cdc to continuously replicate changes on the source to the destination to minimize your application downtime when you cut over to the new database.\n\nDMS comes with functionality to test source and destination endpoints from the replication instance. This is a handy feature to use when working with DMS to validate that you have the configuration correct before you start to run replication tasks. Testing connectivity from the replication instance to both of the endpoints you configured can be done through the DMS console or the command line with the following commands:\n\naws dms test-connection \\\n\n--replication-instance-arn $rep_instance_arn \\\n\n--endpoint-arn $source_endpoint_arn\n\naws dms test-connection \\\n\n--replication-instance-arn $rep_instance_arn \\\n\n--endpoint-arn $target_endpoint_arn\n\nThe test-connection operation takes a few moments to complete. You can check the status and the results of the operation by using this command:\n\naws dms describe-connections --filter \\\n\n\"Name=endpoint-arn,Values=$source_endpoint_arn,$target_endpoint_arn\"\n\nThe DMS service supports many types of source and target databases within your VPC, another AWS account, or databases hosted in a non-AWS environment. The service can also transform data for you if your source and destination are different types of databases by using additional configuration in the table-mappings.json file. For example, the data type of a column in an Oracle database may have a different format than the equivalent type in a PostgreSQL database. The AWS Schema Conversion Tool (SCT) can assist with identifying these necessary transforms, and also generate configuration files to use with DMS.\n\nChallenge\n\nEnable full load and ongoing replication to continuously replicate from one database to another.\n\n4.8 Enabling REST Access to Aurora Serverless Using RDS Data API",
      "content_length": 1881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "Problem\n\nYou have a PostgreSQL database and would like to connect to it without having your application manage persistent database connections.\n\nSolution\n\nFirst, enable the Data API for your database and configure the IAM permissions for your EC2 instance. Then, test from both the CLI and RDS console. This allows your application to connect to your Aurora Serverless database, as shown in Figure 4-8.\n\nFigure 4-8. An application using the RDS Data API\n\nPrerequisites\n\nVPC with isolated subnets created in two AZs and associated route tables.\n\nPostgreSQL RDS instance and EC2 instance deployed. You will need the ability to connect to these for testing.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nEnable the Data API on your Aurora Serverless cluster:\n\naws rds modify-db-cluster \\",
      "content_length": 837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "2.\n\n3.\n\n4.\n\n--db-cluster-identifier $CLUSTER_IDENTIFIER \\\n\n--enable-http-endpoint \\\n\n--apply-immediately\n\nEnsure that HttpEndpointEnabled is set to true:\n\naws rds describe-db-clusters \\\n\n--db-cluster-identifier $CLUSTER_IDENTIFIER \\\n\n--query DBClusters[0].HttpEndpointEnabled\n\nTest a command from your CLI:\n\naws rds-data execute-statement \\\n\n--secret-arn \"$SECRET_ARN\" \\\n\n--resource-arn \"$CLUSTER_ARN\" \\\n\n--database \"$DATABASE_NAME\" \\\n\n--sql \"select * from pg_user\" \\\n\n--output json\n\n(Optional) You can also test access via the AWS Console using the Amazon RDS Query Editor. First run these two commands from your terminal so you can copy and paste the values:\n\necho $SECRET_ARN\n\necho $DATABASE_NAME\n\nLog in to the AWS Console with admin permissions and go to the RDS console. On the lefthand sidebar menu, click Query Editor. Fill out the values and select “Connect to database,” as shown in Figure 4-9.",
      "content_length": 904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "5.\n\nFigure 4-9. Connect to database settings\n\nRun the same query and view the results below the Query Editor (see Figure 4-10):",
      "content_length": 127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "6.\n\nSELECT * from pg_user;\n\nFigure 4-10. RDS Query Editor\n\nConfigure your EC2 instance to use the Data API with your database cluster. Create a file called policy-template.json with the following content (file provided in the repository):\n\n{",
      "content_length": 241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "7.\n\n8.\n\n9.\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Action\": [\n\n\"rds-data:BatchExecuteStatement\",\n\n\"rds-data:BeginTransaction\",\n\n\"rds-data:CommitTransaction\",\n\n\"rds-data:ExecuteStatement\",\n\n\"rds-data:RollbackTransaction\"\n\n],\n\n\"Resource\": \"*\",\n\n\"Effect\": \"Allow\"\n\n},\n\n{\n\n\"Action\": [\n\n\"secretsmanager:GetSecretValue\",\n\n\"secretsmanager:DescribeSecret\"\n\n],\n\n\"Resource\": \"SecretArn\",\n\n\"Effect\": \"Allow\"\n\n}\n\n]\n\n}\n\nReplace the values in the template file by using the sed command with environment variables you have set:\n\nsed -e \"s/SecretArn/${SECRET_ARN}/g\" \\\n\npolicy-template.json > policy.json\n\nCreate an IAM policy by using the file you just created:\n\naws iam create-policy --policy-name AWSCookbook408RDSDataPolicy \\\n\n--policy-document file://policy.json\n\nAttach the IAM policy for AWSCookbook408RDSDataPolicy to your EC2 instance’s IAM role:\n\naws iam attach-role-policy --role-name $INSTANCE_ROLE_NAME \\\n\n--policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/AWSCookbook408RDSDataPolicy",
      "content_length": 986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "Validation checks\n\nCreate and populate some SSM parameters to store values so that you can retrieve them from your EC2 instance:\n\naws ssm put-parameter \\\n\n--name \"Cookbook408DatabaseName\" \\\n\n--type \"String\" \\\n\n--value $DATABASE_NAME\n\naws ssm put-parameter \\\n\n--name \"Cookbook408ClusterArn\" \\\n\n--type \"String\" \\\n\n--value $CLUSTER_ARN\n\naws ssm put-parameter \\\n\n--name \"Cookbook408SecretArn\" \\\n\n--type \"String\" \\\n\n--value $SECRET_ARN\n\nConnect to the EC2 instance by using SSM Session Manager (see Recipe 1.6):\n\naws ssm start-session --target $INSTANCE_ID\n\nSet the Region:\n\nexport AWS_DEFAULT_REGION=us-east-1\n\nRetrieve the SSM parameter values and set them to environment values:\n\nDatabaseName=$(aws ssm get-parameters \\\n\n--names \"Cookbook408DatabaseName\" \\\n\n--query \"Parameters[*].Value\" --output text)\n\nSecretArn=$(aws ssm get-parameters \\\n\n--names \"Cookbook408SecretArn\" \\\n\n--query \"Parameters[*].Value\" --output text)\n\nClusterArn=$(aws ssm get-parameters \\",
      "content_length": 957,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "--names \"Cookbook408ClusterArn\" \\\n\n--query \"Parameters[*].Value\" --output text)\n\nRun a query against the database:\n\naws rds-data execute-statement \\\n\n--secret-arn \"$SecretArn\" \\\n\n--resource-arn \"$ClusterArn\" \\\n\n--database \"$DatabaseName\" \\\n\n--sql \"select * from pg_user\" \\\n\n--output json\n\nExit the Session Manager session:\n\nexit\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThe Data API exposes an HTTPS endpoint for usage with Aurora and uses IAM authentication to allow your application to execute SQL statements on your database over HTTPS instead of using classic TCP database connectivity.\n\nTIP\n\nPer the Aurora user guide, all calls to the Data API are synchronous, and the default timeout for a query is 45 seconds. If\n\nyour queries take longer than 45 seconds, you can use the continueAfterTimeout parameter to facilitate long-running queries.\n\nAs is the case with other AWS service APIs that use IAM authentication, all activities performed with the Data API are captured in CloudTrail to ensure an audit trail is present, which can help satisfy your security and audit requirements. You can control and delegate access to the Data API endpoint by using IAM policies associated with roles for your application. For example, if you wanted to grant your application the ability to only read from your database using the Data API, you could write a policy that omits the rds-data:CommitTransaction and rds- data:RollbackTransaction permissions.",
      "content_length": 1500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "The Query Editor within the RDS console provides a web-based means of access for executing SQL queries against your database. This is a convenient mechanism for developers and DBAs to quickly accomplish bespoke tasks. The same privileges that you assigned your EC2 instance in this recipe would need to be granted to your developer and DBA via IAM roles.\n\nChallenge\n\nCreate and deploy a Lambda function that has permissions to access the RDS Data API that you provisioned.",
      "content_length": 472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Chapter 5. Serverless\n\n5.0 Introduction The technology industry term serverless can sometimes lead to confusion in that servers are involved with the cloud services associated with this model of execution. The advantage is that end users do not need to worry about managing the underlying infrastructure and platform. The cloud provider (in this case, AWS) is responsible for all of the management, operating system updates, availability, capacity, and more.\n\nIn terms of “serverless” services available on AWS, many options are available to take advantage of the benefits. Here are some examples:\n\nAWS Lambda and Amazon Fargate for compute\n\nAmazon EventBridge, Amazon SNS, Amazon SQS, and Amazon API Gateway for application integration\n\nAmazon S3, Amazon DynamoDB, and Amazon Aurora Serverless for datastores\n\nThe main benefits of serverless on AWS are as follows:\n\nCost savings\n\nYou pay for only what you use.\n\nScalability\n\nScale up to what you need; scale down to save costs.\n\nLess management\n\nThere are no servers to deploy or systems to manage.\n\nFlexibility\n\nMany programming languages are supported in AWS Lambda.\n\nThe recipes in this chapter will enable you to explore several of the AWS services that fall under the “serverless” umbrella. You will find opportunities to extend the solutions with challenges and get experience with some new services that are leading the serverless industry trend.\n\nWorkstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:",
      "content_length": 1612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "git clone https://github.com/AWSCookbook/Serverless",
      "content_length": 51,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "Chapter Prerequisites\n\nIAM role for Lambda function execution\n\nCreate a file named assume-role-policy.json with the following content (file provided in root of the chapter repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"lambda.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nNOTE\n\nA similar role is created automatically when you create a Lambda function in the AWS Management Console and select\n\n“Create a new role from AWS policy templates” for the execution role.\n\nCreate an IAM role with the statement in the provided assume-role-policy.json file using this command:\n\naws iam create-role --role-name AWSCookbookLambdaRole \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\nAttach the AWSLambdaBasicExecutionRole IAM managed policy to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbookLambdaRole \\\n\n--policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n\n5.1 Configuring an ALB to Invoke a Lambda Function",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Problem\n\nYou have a requirement that your entire web application must be exposed to the internet with a load balancer. Your application architecture includes serverless functions. You need a function to be able to respond to HTTP requests for specific URL paths.\n\nSolution\n\nGrant the Elastic Load Balancing service permission to invoke Lambda functions; then create a Lambda function. Create an ALB target group; then register the Lambda function with the target group. Associate the target group with the listener on your ALB. Finally, add a listener rule that directs traffic for the /function path to the Lambda function (see Figure 5-1).\n\nFigure 5-1. Lambda function invoked by ALB\n\nPrerequisites\n\nVPC with public subnets in two AZs and associated route tables\n\nAn Application Load Balancer that includes the following:\n\nAn associated security group that allows port 80 from the world\n\nA listener on port 80\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCompress the function code provided in this recipe’s directory in the repository. This code will be used for the Lambda function. You can modify this code if you wish, but we have provided code for you to use. For example, zip the code with the following command:",
      "content_length": 1363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "2.\n\n3.\n\n4.\n\n5.\n\nzip lambda_function.zip lambda_function.py\n\nCreate a Lambda function that will respond to HTTP requests:\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook501Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole \\\n\n--output text --query FunctionArn)\n\nCreate an ALB target group with the target type set to lambda:\n\nTARGET_GROUP_ARN=$(aws elbv2 create-target-group \\\n\n--name awscookbook501tg \\\n\n--target-type lambda --output text \\\n\n--query TargetGroups[0].TargetGroupArn)\n\nUse the add-permission command to give the Elastic Load Balancing service permission to invoke your Lambda function:\n\naws lambda add-permission \\\n\n--function-name $LAMBDA_ARN \\\n\n--statement-id load-balancer \\\n\n--principal elasticloadbalancing.amazonaws.com \\\n\n--action lambda:InvokeFunction \\\n\n--source-arn $TARGET_GROUP_ARN\n\nUse the register-targets command to register the Lambda function as a target:\n\naws elbv2 register-targets \\",
      "content_length": 1108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "--target-group-arn $TARGET_GROUP_ARN \\\n\n--targets Id=$LAMBDA_ARN\n\n6.\n\nModify the listener for your ALB on port 80; then create a rule that forwards traffic destined for the /function path to your target group:\n\nRULE_ARN=$(aws elbv2 create-rule \\\n\n--listener-arn $LISTENER_ARN --priority 10 \\\n\n--conditions Field=path-pattern,Values='/function' \\\n\n--actions Type=forward,TargetGroupArn=$TARGET_GROUP_ARN \\\n\n--output text --query Rules[0].RuleArn)\n\nValidation checks\n\nTest the invocation to verify that the Lambda function is invoked when requesting /function:\n\ncurl -v $LOAD_BALANCER_DNS/function\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nDevelopers and software architects can leverage Lambda functions to provide a programmatic response to some kind of event within a larger system. This type of compute is most often used when the functions are responsible for a small unit of work. AWS added the ability for Application Load Balancers to invoke Lambda functions in 2018.\n\nWhen an end user requests a specific URL path (configured by the developer) on an ALB, the ALB can pass the request to a Lambda function to handle the response. The ALB then receives the output from the function and hands the result back to the end user as an HTTP response. ALBs can have multiple paths and targets configured for a single load balancer, sending portions of traffic to specific targets (Lambda functions, containers, EC2 instances, etc.) ALBs also support routing to Lambda functions using header values. This simple architecture is extremely cost-effective and highly scalable.\n\nThis flexibility allows for a single ALB to handle all the traffic for your application and provides for a nice building block when architecting a system that needs to be exposed via HTTP/HTTPS.\n\nChallenge 1\n\nAdd a Fargate task that responds to another path on your ALB.\n\nChallenge 2",
      "content_length": 1909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "Try using an Amazon API Gateway to play in front of your Lambda function.",
      "content_length": 73,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "5.2 Packaging Libraries with Lambda Layers\n\nProblem\n\nYou have Python code using external libraries that you need to include with your serverless function deployments.\n\nSolution\n\nCreate a folder and use pip to install a Python package to the folder. Then, zip the folder and use the .zip file to create a Lambda layer that your function will leverage (see Figure 5-2).\n\nFigure 5-2. Lambda function layer creation and representation\n\nPrerequisite\n\nIAM role that allows Lambda functions to execute (provided in chapter prerequisites)\n\nSteps\n\n1.\n\nIn the root of this chapter’s repository, cd to the 502-Packaging-Libraries- with-Lambda-Layers directory and follow the subsequent steps:\n\ncd 502-Packaging-Libraries-with-Lambda-Layers/",
      "content_length": 729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "2.\n\n3.\n\n4.\n\n5.\n\nZip up lambda_function.py provided in the repository:\n\nzip lambda_function.zip lambda_function.py\n\nYou should see output similar to the following:\n\nupdating: lambda_function.py (deflated 49%)\n\nCreate a Lambda function that we will add a layer to:\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook502Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole \\\n\n--output text --query FunctionArn)\n\nCreate a directory for the layer contents:\n\nmkdir python\n\nUse pip to install the latest requests module to the directory:\n\npip install requests --target=\"./python\"\n\nYou should see output similar to the following:\n\nCollecting requests\n\nUsing cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n\nCollecting certifi>=2017.4.17",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "6.\n\nUsing cached certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n\nCollecting idna<4,>=2.5\n\nUsing cached idna-3.2-py3-none-any.whl (59 kB)\n\nCollecting urllib3<1.27,>=1.21.1\n\nUsing cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n\nCollecting charset-normalizer~=2.0.0\n\nUsing cached charset_normalizer-2.0.6-py3-none-any.whl (37 kB)\n\nInstalling collected packages: urllib3, idna, charset-normalizer, certifi,\n\nrequests\n\nSuccessfully installed certifi-2021.5.30 charset-normalizer-2.0.6 idna-3.2\n\nrequests-2.26.0 urllib3-1.26.7\n\nZip the contents of the directory:\n\nzip -r requests-layer.zip ./python\n\nYou should see output similar to the following:\n\nadding: python/ (stored 0%)\n\nadding: python/bin/ (stored 0%)\n\nadding: python/bin/normalizer (deflated 28%)\n\nadding: python/requests-2.26.0.dist-info/ (stored 0%)\n\nadding: python/requests-2.26.0.dist-info/RECORD (deflated 55%)\n\nadding: python/requests-2.26.0.dist-info/LICENSE (deflated 65%)\n\nadding: python/requests-2.26.0.dist-info/WHEEL (deflated 14%)\n\nadding: python/requests-2.26.0.dist-info/top_level.txt (stored 0%)\n\nadding: python/requests-2.26.0.dist-info/REQUESTED (stored 0%)\n\nadding: python/requests-2.26.0.dist-info/INSTALLER (stored 0%)\n\nadding: python/requests-2.26.0.dist-info/METADATA (deflated 58%)\n\nNOTE\n\nLambda layers require a specific folder structure. You create this folder structure when you use pip to install the requests module to the Python folder within your directory:\n\n$ tree -L 1 python/\n\npython/\n\n├── bin\n\n├── certifi",
      "content_length": 1499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "7.\n\n8.\n\n├── certifi-2020.12.5.dist-info\n\n├── chardet\n\n├── chardet-4.0.0.dist-info\n\n├── idna\n\n├── idna-2.10.dist-info\n\n├── requests\n\n├── requests-2.25.1.dist-info\n\n├── urllib3\n\n└── urllib3-1.26.4.dist-info\n\n11 directories, 0 files\n\nPublish the layer and set an environment variable to use in the next steps:\n\nLAYER_VERSION_ARN=$(aws lambda publish-layer-version \\\n\n--layer-name AWSCookbook502RequestsLayer \\\n\n--description \"Requests layer\" \\\n\n--license-info \"MIT\" \\\n\n--zip-file fileb://requests-layer.zip \\\n\n--compatible-runtimes python3.8 \\\n\n--output text --query LayerVersionArn)\n\nUpdate the Lambda to use the layer that you created:\n\naws lambda update-function-configuration \\\n\n--function-name AWSCookbook502Lambda \\\n\n--layers $LAYER_VERSION_ARN\n\nYou should see output similar to the following:\n\n{\n\n\"FunctionName\": \"AWSCookbook502Lambda\",\n\n\"FunctionArn\": \"arn:aws:lambda:us-east-\n\n1:111111111111:function:AWSCookbook502Lambda\",\n\n\"Runtime\": \"python3.8\",\n\n\"Role\": \"arn:aws:iam::111111111111:role/AWSCookbookLambdaRole\",\n\n\"Handler\": \"lambda_function.lambda_handler\",\n\n\"CodeSize\": 691,\n\n\"Description\": \"\",",
      "content_length": 1103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "\"Timeout\": 3,\n\n\"MemorySize\": 128,\n\n...\n\nValidation checks\n\nTest the Lambda:\n\naws lambda invoke \\\n\n--function-name AWSCookbook502Lambda \\\n\nresponse.json && cat response.json\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nLambda layers can be used to extend the packages available within the default Lambda runtimes and also to provide your own custom runtimes for your functions. The default runtimes are associated with Amazon Linux. Custom runtimes can be developed on top of Amazon Linux to support your own programming language requirements. See this tutorial to publish a custom runtime.\n\nIn this recipe, you packaged the Python requests module as a layer and deployed a Python function that uses that module. Layers can be used by multiple functions, shared with other AWS accounts, and they can also be version-controlled so that you can deploy and test new versions of your layers without impacting existing versions that are being used for your functions.\n\nChallenge\n\nCreate another Lambda function that uses the same layer.",
      "content_length": 1081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "5.3 Invoking Lambda Functions on a Schedule\n\nProblem\n\nYou need to run a serverless function once per minute.\n\nSolution\n\nAdd a permission to your Lambda function to allow the EventBridge service to invoke the function. Then configure an EventBridge rule using a schedule expression for one minute that targets your function (see Figure 5-3).\n\nFigure 5-3. EventBridge triggering a time-based invocation of a Lambda function\n\nPrerequisites\n\nLambda function that you want to trigger\n\nIAM role that allows Lambda functions to execute (provided in chapter prerequisites)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate an events rule with a scheduled expression with a rate of one minute:\n\nRULE_ARN=$(aws events put-rule --name \"EveryMinuteEvent\" \\\n\n--schedule-expression \"rate(1 minute)\")",
      "content_length": 841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "2.\n\n3.\n\nNOTE\n\nYou can use rate expressions and Cron formats for schedule expressions when defining a time-based\n\nevent rule. The Cron expression syntax for the rule you created would look like the following:\n\nRULE_ARN=$(aws events put-rule --name \"EveryMinuteEvent\" \\\n\n--schedule-expression \"cron(* * * * ? *)\")\n\nFor more information about schedule expressions, see this support document.\n\nAdd a permission to the Lambda function so that the EventBridge service can invoke it:\n\naws lambda add-permission --function-name $LAMBDA_ARN \\\n\n--action lambda:InvokeFunction --statement-id events \\\n\n--principal events.amazonaws.com\n\nYou should see output similar to the following:\n\n{\n\n\"Statement\": \"{\\\"Sid\\\":\\\"events\\\",\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":\n\n{\\\"Service\\\":\\\"events.amazonaws.com\\\"},\\\"Action\\\":\\\"lambda:InvokeFunction\\\",\\\"Re\n\nsource\\\":\\\"arn:aws:lambda:us-east-\n\n1:111111111111:function:AWSCookbook503Lambda\\\"}\"\n\n}\n\nAdd your Lambda function as a target for the rule that you created:\n\naws events put-targets --rule EveryMinuteEvent \\\n\n--targets \"Id\"=\"1\",\"Arn\"=\"$LAMBDA_ARN\"\n\nYou should see output similar to the following:\n\n{\n\n\"FailedEntryCount\": 0,",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "\"FailedEntries\": []\n\n}\n\nNOTE\n\nThere are many available target options. For the latest list, check the documentation.\n\nValidation checks\n\nTail the CloudWatch Logs log group to observe the function invoked every 60 seconds:\n\naws logs tail \"/aws/lambda/AWSCookbook503Lambda\" --follow --since 10s\n\nNOTE\n\nYou may have to wait a few moments for the log group to be created. If the log group doesn’t exist, you will get the\n\nfollowing error:\n\nAn error occurred (ResourceNotFoundException) when calling the FilterLogEvents operation: The\n\nspecified log group does not exist.\n\nYou should see output similar to the following:\n\n$ $ aws logs tail \"/aws/lambda/AWSCookbook503Lambda\" --follow --since 10s\n\n2021-06-12T21:17:30.605000+00:00 2021/06/12/[$LATEST]4d1335bf8b0846938cb585871db38374 START RequestId:\n\n685481eb-9279-4007-854c-f99289bf9609 Version: $LATEST\n\n2021-06-12T21:17:30.607000+00:00 2021/06/12/[$LATEST]4d1335bf8b0846938cb585871db38374 AWS Cookbook\n\nLambda\n\nfunction run at 2021-06-12 21:17:30.607500\n\n2021-06-12T21:17:30.608000+00:00 2021/06/12/[$LATEST]4d1335bf8b0846938cb585871db38374 END RequestId:\n\n685481eb-9279-4007-854c-f99289bf9609\n\n2021-06-12T21:17:30.608000+00:00 2021/06/12/[$LATEST]4d1335bf8b0846938cb585871db38374 REPORT\n\nRequestId:\n\n685481eb-9279-4007-854c-f99289bf9609 Duration: 0.94 ms Billed Duration: 1 ms Memory Size: 128 MB\n\nMax Memory Used: 51 MB\n\n...",
      "content_length": 1374,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "You can exit the tail session by pressing Ctrl-C.\n\nNOTE\n\nNotice that subsequent runs occur at one-minute increments from the time that you added the Lambda function as a target\n\nfor your event rule.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nThere are many reasons you would want to run functions on a schedule:\n\nChecking stock prices\n\nChecking the weather\n\nStarting scheduled processes\n\nScheduling EC2 starting and stop\n\nBeing able to run serverless functions on a schedule without provisioning resources allows costs and management to be kept at a minimum. There are no servers to update, and you don’t need to pay for them when they are idle.\n\nChallenge\n\nPause and then enable the event rule. Here is a hint:\n\naws events disable-rule --name \"EveryMinuteEvent\"\n\naws events enable-rule --name \"EveryMinuteEvent\"\n\nNOTE\n\nEventBridge was formerly known as Amazon CloudWatch Events. EventBridge is now the preferred way to schedule\n\nevents and uses the same API as CloudWatch Events.\n\nWhen you need an AWS service to interact with another AWS service, you need to explicitly grant the permissions. In this case, EventBridge needs to be granted the permissions to invoke a Lambda function by using the aws lambda add-permission command.",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "5.4 Configuring a Lambda Function to Access an EFS File System\n\nProblem\n\nYou have an existing network share that is accessible by servers, but you want to be able to process files on it with serverless functions.\n\nSolution\n\nYou will create a Lambda function and mount your EFS file system to it (see Figure 5-4).\n\nFigure 5-4. Lambda function accessing ENIs within the subnet of a VPC\n\nPrerequisites",
      "content_length": 398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "VPC with isolated subnets in two AZs and associated route tables\n\nEFS file system with content that you want to access\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a new security group for the Lambda function to use:\n\nLAMBDA_SG_ID=$(aws ec2 create-security-group \\\n\n--group-name AWSCookbook504LambdaSG \\\n\n--description \"Lambda Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\n2.\n\nAdd an ingress rule to the EFS file system’s security group that allows access on TCP port 2049 from the Lambda function’s security group:\n\naws ec2 authorize-security-group-ingress \\\n\n--protocol tcp --port 2049 \\\n\n--source-group $LAMBDA_SG_ID \\\n\n--group-id $EFS_SECURITY_GROUP\n\nYou should see output similar to the following:\n\n{\n\n\"Return\": true,\n\n\"SecurityGroupRules\": [\n\n{\n\n\"SecurityGroupRuleId\": \"sgr-0f837d0b090ba38de\",\n\n\"GroupId\": \"sg-0867c2c4ca6f4ab83\",\n\n\"GroupOwnerId\": \"611652777867\",\n\n\"IsEgress\": false,\n\n\"IpProtocol\": \"tcp\",\n\n\"FromPort\": 2049,\n\n\"ToPort\": 2049,\n\n\"ReferencedGroupInfo\": {",
      "content_length": 1144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "3.\n\n4.\n\n5.\n\n6.\n\n\"GroupId\": \"sg-0c71fc94eb6cd1ae3\"\n\n}\n\n}\n\n]\n\n}\n\nCreate an IAM role using the statement in the provided assume-role- policy.json file using this command:\n\naws iam create-role --role-name AWSCookbook504Role \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\nYou need to give your Lambda function the ability to execute within a VPC, so attach the IAM managed policy for AWSLambdaVPCAccessExecutionRole to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook504Role \\\n\n--policy-arn arn:aws:iam::aws:policy/service-\n\nrole/AWSLambdaVPCAccessExecutionRole\n\nZip up the lambda_function.py provided in the repository:\n\nzip lambda_function.zip lambda_function.py\n\nCreate a Lambda function specifying the ACCESS_POINT_ARN of the EFS file system:\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook504Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook504Role \\\n\n--file-system-configs Arn=\"$ACCESS_POINT_ARN\",LocalMountPath=\"/mnt/efs\" \\",
      "content_length": 1143,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "--output text --query FunctionArn \\\n\n--vpc-config SubnetIds=${ISOLATED_SUBNETS},SecurityGroupIds=${LAMBDA_SG_ID})\n\n7.\n\nUse this command to determine when the Lambda function has entered the active state (this may take a few moments):\n\naws lambda get-function --function-name $LAMBDA_ARN \\\n\n--output text --query Configuration.State\n\nValidation checks\n\nExecute the Lambda function to display the file’s contents:\n\naws lambda invoke \\\n\n--function-name $LAMBDA_ARN \\\n\nresponse.json && cat response.json\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAs a developer or software architect, you start to see the benefits of serverless technologies when you connect serverless compute to serverless persistent storage (filesystems and databases both). The compute and storage operational overhead is drastically reduced when using these types of services within applications. You can provision and scale your storage on demand while relying on AWS to manage the underlying infrastructure for you. While many applications will use an object-style storage service such as Amazon S3, others are best suited to a somewhat more traditional file-style storage service. Combining Lambda and EFS, as shown in this recipe, solves this problem with ease.\n\nBy integrating Amazon EFS with AWS Lambda, you can build solutions like the following:\n\nPersistent storage for applications\n\nMaintenance activities\n\nEvent-driven notifications\n\nEvent-driven file processing\n\nThe fully managed nature and the pay-per-use aspects of these services allow for the design, building, deployment, and operation of cost-effective and modern application architectures.",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "Challenge 1\n\nCreate another Lambda function that has access to the same EFS file system.\n\nChallenge 2\n\nCreate a Lambda function that runs on a scheduled interval to detect if any files have been changed in the last 30 days.\n\nSee Also\n\nRecipe 5.9\n\n5.5 Running Trusted Code in Lambda Using AWS Signer\n\nProblem\n\nYou need to ensure that a serverless function deployed in your environment is running code from trusted sources. You need to verify the integrity of the code and have confidence that the code has not been modified after it has been signed.\n\nSolution\n\nCreate a signing profile and then start a signing job for your code by using AWS Signer. Finally, deploy a Lambda function that references your signing configuration and uses the signed code (see Figure 5-5).\n\nFigure 5-5. Signing process for Lambda function code\n\nPrerequisites\n\nS3 bucket with versioning enabled and source code copied to it",
      "content_length": 901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "S3 bucket for AWS Signer to use a destination\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nGet the version of the object in S3 that you will use. This is a zip of the code to be used in your Lambda function. You’ll need this when you start the signing job:\n\nOBJ_VER_ID=$(aws s3api list-object-versions \\\n\n--bucket awscookbook505-src-$RANDOM_STRING \\\n\n--prefix lambda_function.zip \\\n\n--output text --query Versions[0].VersionId)\n\n2.\n\nCreate a signing profile:\n\nSIGNING_PROFILE_ARN=$(aws signer put-signing-profile \\\n\n--profile-name AWSCookbook505_$RANDOM_STRING \\\n\n--platform AWSLambda-SHA384-ECDSA \\\n\n--output text --query arn)\n\nNOTE\n\nYou can find a list of the available signing platforms by running this command:\n\naws signer list-signing-platforms\n\n3.\n\nCreate a code-signing configuration for Lambda that refers to the signing profile:\n\nCODE_SIGNING_CONFIG_ARN=$(aws lambda create-code-signing-config \\",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "4.\n\n--allowed-publishers SigningProfileVersionArns=$SIGNING_PROFILE_ARN \\\n\n--output text --query CodeSigningConfig.CodeSigningConfigArn)\n\nStart the signing job:\n\nSIGNING_JOB_ID=$(aws signer start-signing-job \\\n\n--source 's3={bucketName=awscookbook505-src-\n\n'\"${RANDOM_STRING}\"',key=lambda_function.zip,version='\"$OBJ_VER_ID\"'}' \\\n\n--destination 's3={bucketName=awscookbook505-dst-\n\n'\"${RANDOM_STRING}\"',prefix=signed-}' \\\n\n--profile-name AWSCookbook505_$RANDOM_STRING \\\n\n--output text --query jobId)\n\nWait a few moments and then verify that the signing job was successful:\n\naws signer list-signing-jobs --status Succeeded\n\nYou should see output similar to the following:\n\n{\n\n\"jobs\": [\n\n{\n\n\"jobId\": \"efd392ae-2503-4c78-963f-8f40a58d770f\",\n\n\"source\": {\n\n\"s3\": {\n\n\"bucketName\": \"awscookbook505-src-<<unique>>\",\n\n\"key\": \"lambda_function.zip\",\n\n\"version\": \"o.MffnpzjBmaBR1yzvoti0AnluovMtMf\"\n\n}\n\n},\n\n\"signedObject\": {\n\n\"s3\": {\n\n\"bucketName\": \"awscookbook505-dst-<<unique>>\",\n\n\"key\": \"signed-efd392ae-2503-4c78-963f-8f40a58d770f.zip\"\n\n}\n\n},\n\n\"signingMaterial\": {},\n\n\"createdAt\": \"2021-06-13T11:52:51-04:00\",\n\n\"status\": \"Succeeded\",\n\n...",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "5.\n\nRetrieve the S3 object key of the resulting signed code:\n\nOBJECT_KEY=$(aws s3api list-objects-v2 \\\n\n--bucket awscookbook505-dst-$RANDOM_STRING \\\n\n--prefix 'signed-' \\\n\n--output text --query Contents[0].Key)\n\n6.\n\nCreate a Lambda function that uses the signed code:\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook505Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--code S3Bucket=awscookbook505-dst-$RANDOM_STRING,S3Key=$OBJECT_KEY \\\n\n--code-signing-config-arn $CODE_SIGNING_CONFIG_ARN \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole \\\n\n--output text --query FunctionArn)\n\n7.\n\nUse this command to determine when the Lambda function has entered the active state:\n\naws lambda get-function --function-name $LAMBDA_ARN \\\n\n--output text --query Configuration.State\n\nValidation checks\n\nView your Lambda function in that console. Notice that you can’t edit the code. You will see the message, “Your function has signed code and can’t be edited inline.”\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nSecurity-focused administrators and application developers can use this approach to implement a DevSecOps strategy by enforcing rules that allow only trusted code to be deployed in a given environment. By using AWS Signer, you can ensure that you are running only trusted code in",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "your environments. This helps meet compliance requirements and increases the security posture of your application.\n\nBy using a digital signature generated by AWS Signer, your code is validated against a cryptographic fingerprint, and enforcement policies can be applied to restrict the deployment and execution of code. This capability paves the way to a strategic shift from “reactive” to “preventive” controls in your security and compliance governance.\n\nChallenge 1\n\nMake a modification to the source code, sign it, and update the Lambda function.\n\nChallenge 2\n\nYou can change your CodeSigningPolicies from Warn to Enforce—this will block deployments if validation checks of the signature aren’t successful. Deploy a function that leverages this capability to ensure you are running only signed code in your environment:\n\n\"CodeSigningPolicies\": {\n\n\"UntrustedArtifactOnDeployment\": \"Warn\"\n\n},\n\n5.6 Packaging Lambda Code in a Container Image\n\nProblem\n\nYou want to use your existing container-based development processes and tooling to package your serverless code.\n\nSolution\n\nCreate a Docker image and push it to an Amazon Elastic Container Registry (ECR) repository. Create a Lambda function with the package-type of Image and code that references an image URL in ECR (see Figure 5-6).",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "Figure 5-6. Deploying Lambda code packaged in a Docker image\n\nPrerequisites\n\nECR repository\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\nDocker installed\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nGet ECR login information and pass to Docker:\n\naws ecr get-login-password | docker login --username AWS \\",
      "content_length": 403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "2.\n\n3.\n\n4.\n\n--password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n\nYou should see output similar to the following:\n\nLogin Succeeded\n\nCreate a file called app.py with the following code that you would like to execute in Lambda (file provided in the repository):\n\nimport sys\n\ndef handler(event, context):\n\nreturn 'Hello from the AWS Cookbook ' + sys.version + '!'\n\nCreate a file called Dockerfile with the following content that builds on an AWS-provided base image and references your Python code (file provided in the repository):\n\nFROM public.ecr.aws/lambda/python:3.8\n\nCOPY app.py ./\n\nCMD [\"app.handler\"]\n\nIn the folder where the Dockerfile and app.py files exist, build the container image. The process will take a few minutes to complete:\n\ndocker build -t aws-cookbook506-image .\n\nYou should see output similar to the following:\n\n[+] Building 19.1s (4/6)\n\n=> [internal] load build definition from Dockerfile\n\n0.0s\n\n=> => transferring dockerfile: 36B\n\n0.0s\n\n=> [internal] load .dockerignore",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "5.\n\n6.\n\n7.\n\n0.0s\n\n=> => transferring context: 2B\n\n0.0s\n\n=> [internal] load metadata for public.ecr.aws/lambda/python:3.8\n\n2.2s\n\n=> [internal] load build context\n\n0.0s\n\n...\n\nAdd an additional tag to the image to allow it to be pushed to ECR:\n\ndocker tag \\\n\naws-cookbook506-image:latest \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-506repo:latest\n\nPush the image to the ECR repository. The process should take a few minutes to complete:\n\ndocker push \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-506repo:latest\n\nYou should see output similar to the following:\n\nThe push refers to repository [111111111111.dkr.ecr.us-east-1.amazonaws.com/aws-\n\ncookbook-506repo]\n\n5efc5a3f50dd: Pushed\n\na1f8e0568112: Pushing [=====> ] 10.3MB/98.4MB\n\nbcf453d1de13: Pushing [> ] 3.244MB/201.2MB\n\nf6ae2f36d5d7: Pushing [==============================> ] 4.998MB/8.204MB\n\n5959c8f9752b: Pushed\n\n3e5452c20c48: Pushed\n\n9c4b6b04eac3: Pushing [>\n\nCreate a Lambda function with the Docker image by specifying a --code value that is the ImageUri of the Docker image :\n\nLAMBDA_ARN=$(aws lambda create-function \\",
      "content_length": 1119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "--function-name AWSCookbook506Lambda \\\n\n--package-type \"Image\" \\\n\n--code ImageUri=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-\n\n506repo:latest \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole \\\n\n--output text --query FunctionArn)\n\nNOTE\n\nThe --runtime and --handler parameters are not necessary or supported when creating a function that uses a container image.\n\nUse this command to determine when the Lambda function has entered the active state:\n\naws lambda get-function --function-name $LAMBDA_ARN \\\n\n--output text --query Configuration.State\n\nValidation checks\n\nIn the AWS Console, navigate to the Lambda→Functions menu. Notice that the “Package type” for your function is Image.\n\nInvoke the function and view the response:\n\naws lambda invoke \\\n\n--function-name $LAMBDA_ARN response.json && cat response.json\n\nYou should see output similar to the following:\n\n{\n\n\"StatusCode\": 200,\n\n\"ExecutedVersion\": \"$LATEST\"\n\n}\n\n\"Hello from the AWS Cookbook 3.8.8 (default, Mar 8 2021, 20:13:42) \\n[GCC 7.3.1 20180712 (Red Hat\n\n7.3.1-12)]!\"",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nIf your application code is packaged in container images, AWS Lambda provides this ability to package your function code inside container images. This allows for alignment with existing build, test, package, and deploy pipelines that you may already be using. You can package application code up to 10 GB in size for your functions. You can use the base images that AWS provides or create your own images, as long as you include a runtime interface client that is required by the Lambda runtime environment.\n\nYou can store your container images on Amazon ECR, and your function must be in the same account as the ECR repository where you have stored your container image.\n\nChallenge\n\nUpdate the application code, create a new image, push it to ECR, and update the Lambda function.\n\n5.7 Automating CSV Import into DynamoDB from S3 with Lambda\n\nProblem\n\nYou need to load data from S3 into DynamoDB when files are uploaded to S3.\n\nSolution\n\nUse a Lambda function to load the S3 data into DynamoDB and configure an S3 notification specifying the Lambda function to trigger on S3:PutObject events (see Figure 5-7).\n\nFigure 5-7. Using a Lambda function to load data into a DynamoDB table",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "Steps\n\n1.\n\n2.\n\n3.\n\n4.\n\n5.\n\nNavigate to this recipe’s directory in the chapter repository:\n\ncd 507-Importing-CSV-to-DynamoDB-from-S3\n\nCreate a DynamoDB table:\n\naws dynamodb create-table \\\n\n--table-name 'AWSCookbook507' \\\n\n--attribute-definitions 'AttributeName=UserID,AttributeType=S' \\\n\n--key-schema 'AttributeName=UserID,KeyType=HASH' \\\n\n--sse-specification 'Enabled=true,SSEType=KMS' \\\n\n--provisioned-throughput \\\n\n'ReadCapacityUnits=5,WriteCapacityUnits=5'\n\nSet a unique suffix to use for the S3 bucket name:\n\nRANDOM_STRING=$(aws secretsmanager get-random-password \\\n\n--exclude-punctuation --exclude-uppercase \\\n\n--password-length 6 --require-each-included-type \\\n\n--output text \\\n\n--query RandomPassword)\n\nCreate an S3 bucket:\n\naws s3api create-bucket --bucket awscookbook507-$RANDOM_STRING\n\nCreate a role for a Lambda function allowing S3 and DynamoDB usage (file provided in the repository):\n\naws iam create-role --role-name AWSCookbook507Lambda \\\n\n--assume-role-policy-document file://assume-role-policy.json",
      "content_length": 1015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "6.\n\n7.\n\n8.\n\n9.\n\n10.\n\nAttach the IAM managed policy for AmazonS3ReadOnlyAccess to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook507Lambda \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\n\nAttach the IAM managed policy for AmazonDynamoDBFullAccess to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook507Lambda \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess\n\nNOTE\n\nIt is best to scope the Lambda function permission to the specific DynamoDB table resource rather\n\nthan AmazonDynamoDBFullAccess (we used it for simplicity here). See Recipe 1.2 for details on how to create a more narrowly scoped permission.\n\nAttach the AWSLambdaBasicExecutionRole IAM managed policy to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook507Lambda \\\n\n--policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n\nZip the function code:\n\nzip lambda_function.zip lambda_function.py\n\nCreate a Lambda function by using the provided code and specifying the code:\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook507Lambda \\\n\n--runtime python3.8 \\",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "11.\n\n12.\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--environment Variables={bucket=awscookbook507-$RANDOM_STRING} \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook507Lambda \\\n\n--output text --query FunctionArn)\n\nGrant the S3 service invoke permissions for the Lambda function:\n\naws lambda add-permission --function-name $LAMBDA_ARN \\\n\n--action lambda:InvokeFunction --statement-id s3invoke \\\n\n--principal s3.amazonaws.com\n\nCreate a notification-template.json file to use as the event definition for automatically triggering the Lambda function when your file (sample_data.csv) is uploaded. A file you can use is provided in the repository:\n\n{\n\n\"LambdaFunctionConfigurations\": [\n\n{\n\n\"Id\": \"awscookbook507event\",\n\n\"LambdaFunctionArn\": \"LAMBDA_ARN\",\n\n\"Events\": [\n\n\"s3:ObjectCreated:*\"\n\n],\n\n\"Filter\": {\n\n\"Key\": {\n\n\"FilterRules\": [\n\n{\n\n\"Name\": \"prefix\",\n\n\"Value\": \"sample_data.csv\"\n\n}\n\n]\n\n}\n\n}\n\n}\n\n]\n\n}",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "13.\n\nYou can use the sed command to replace the values in the provided notification-template.json file with the environment variables you have created:\n\nsed -e \"s/LAMBDA_ARN/${LAMBDA_ARN}/g\" \\\n\nnotification-template.json > notification.json\n\n14.\n\nConfigure the S3 bucket notification settings to trigger the Lambda function (one-liner config). NotificationConfiguration→LambdaConfigurations→Lambda ARN:\n\naws s3api put-bucket-notification-configuration \\\n\n--bucket awscookbook507-$RANDOM_STRING \\\n\n--notification-configuration file://notification.json\n\n15.\n\nUpload a file to S3 to trigger the import:\n\naws s3 cp ./sample_data.csv s3://awscookbook507-$RANDOM_STRING\n\nValidation checks\n\nView the results from your DynamoDB console, or use this CLI command to scan the table:\n\naws dynamodb scan --table-name AWSCookbook507\n\nNOTE\n\nOne of the great features of DynamoDB is that it provides AWS API endpoints for easy CRUD operations by your\n\napplication via the AWS SDK.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion",
      "content_length": 1059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "You can use AWS Lambda and Amazon DynamoDB together to build applications with massively scalable database persistence while minimizing the operational overhead required. Software architects and developers who are looking to build applications without worrying about server infrastructure may find it useful to use these two services together.\n\nWARNING\n\nAt the time of this writing, Lambda functions time out after 900 seconds. This could cause an issue with large CSV files or\n\nif the DynamoDB table does not have sufficient write capacity.\n\nEvent-driven applications are also an important concept in building modern cloud-native applications on AWS. When you created the notification.json file, you specified your Lambda function and the S3 bucket, as well as a key pattern to watch for uploads to trigger the Lambda function when an object is put into the bucket. Using event-driven architecture helps minimize the cost and complexity associated with running your applications because the function logic is run only when needed.\n\nChallenge 1\n\nAdd some new data to the sample_data.csv file, delete the file from your bucket, and re-upload the file to trigger the new import. Note that the existing data will remain, and the new data will be added.\n\nChallenge 2\n\nChange the S3 notification and the Lambda function to allow for other filenames to be used with the solution.\n\nChallenge 3\n\nCreate a fine-grained IAM policy for your Lambda function that scopes down the function’s granted access to the DynamoDB table.\n\n5.8 Reducing Lambda Startup Times with Provisioned Concurrency\n\nProblem\n\nYou need to ensure that a predetermined number (five) of invocations to your serverless function are as fast as possible. You need to eliminate any latency associated with cold starts.\n\nSolution\n\nCreate a Lambda function and set the function concurrency to 5 (see Figure 5-8).",
      "content_length": 1866,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Prerequisite\n\nSteps\n\nFigure 5-8. Provisioned concurrency for a Lambda function\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)",
      "content_length": 167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "1.\n\n2.\n\n3.\n\n4.\n\nCreate a file called lambda_function.py with the following content (file provided in the repository):\n\nfrom datetime import datetime\n\nimport time\n\ndef lambda_handler(event, context):\n\ntime.sleep(5)\n\nprint('AWS Cookbook Function run at {}'.format(str(datetime.now())))\n\nZip the function code:\n\nzip lambda_function.zip lambda_function.py\n\nCreate a Lambda function:\n\naws lambda create-function \\\n\n--function-name AWSCookbook508Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--timeout 20 \\\n\n--role \\\n\narn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole\n\nUse this command to determine when the Lambda function has entered the active state:\n\naws lambda get-function --function-name AWSCookbook509Lambda \\\n\n--output text --query Configuration.State\n\nNOTE\n\nLambda function aliases allow you to reference a specific version of a function. Each Lambda\n\nfunction can have one or more aliases. The initial alias is named LATEST.",
      "content_length": 1042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "5.\n\nConfigure the provisioned concurrency for the Lambda function:\n\naws lambda put-provisioned-concurrency-config \\\n\n--function-name AWSCookbook508Lambda \\\n\n--qualifier LATEST \\\n\n--provisioned-concurrent-executions 5\n\nValidation checks\n\nInvoke the function six times in a row to see the limit hit:\n\naws lambda invoke --function-name AWSCookbook508Lambda response.json &\n\naws lambda invoke --function-name AWSCookbook508Lambda response.json\n\nNOTE\n\nThe Lambda function you deployed needs to be run in parallel to demonstrate the capability of the provisioned\n\nconcurrency feature. You may want to write a simple script which runs this command multiple times in parallel to prove\n\nthat the provisioned concurrency enabled.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYour code’s execution environment provisioning is handled for you by the Lambda service. This is a benefit of it being a fully managed service on AWS. Since the execution environment is provisioned on demand, a small amount of time is required to provision this environment for you. This is referred to as a cold start. Lambda keeps your execution environment provisioned (or “warm”) for a period of time so that if your function is invoked again, it will launch quickly. When you need your functions to respond quickly and achieve more concurrency, you can avoid the cold start and use provisioned concurrency to keep multiple copies of the execution environments “warm.”\n\nSome developers and software architects need to build solutions with time-sensitive requirements measured in milliseconds in microservice-based applications. When you use Lambda-provisioned concurrency, you minimize the amount of time your function needs to start up when it is invoked.\n\nChallenge 1",
      "content_length": 1789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "Configure an API Gateway in front of your Lambda function and use a tool like bees with machine guns or ApacheBench to simulate user load.\n\nChallenge 2\n\nConfigure application autoscaling to modify the provisioned concurrency for your Lambda function based on time and/or a performance metric (e.g., response time).",
      "content_length": 314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "5.9 Accessing VPC Resources with Lambda\n\nProblem\n\nYou need a serverless function to be able to access an ElastiCache cluster that has an endpoint in a VPC.\n\nSolution\n\nCreate a Lambda function that has a Redis client connection and package and specifies VPC subnets and security groups. Then create ElastiCache subnet groups and an ElastiCache cluster. Invoke the Lambda and pass the cluster endpoint to the function to test (see Figure 5-9).\n\nFigure 5-9. Lambda accessing an ElastiCache cluster in a VPC",
      "content_length": 503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "Prerequisite\n\nIAM role that allows the Lambda function to execute (provided in chapter prerequisites)\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nYou need to give your Lambda function the ability to execute within a VPC, so attach the IAM managed policy for AWSLambdaVPCAccess to the IAM role (created in the chapter prerequisites):\n\naws iam attach-role-policy --role-name AWSCookbookLambdaRole \\\n\n--policy-arn arn:aws:iam::aws:policy/service-\n\nrole/AWSLambdaVPCAccessExecutionRole\n\n2.\n\nInstall the Redis Python package to the current directory from the Python Package Index (PyPI):\n\npip install redis -t .\n\n3.\n\nZip the function code:\n\nzip -r lambda_function.zip lambda_function.py redis*\n\n4.\n\nCreate a security group for the Lambda:\n\nLAMBDA_SG_ID=$(aws ec2 create-security-group \\\n\n--group-name Cookbook509LambdaSG \\\n\n--description \"Lambda Security Group\" --vpc-id $VPC_ID \\\n\n--output text --query GroupId)\n\n5.\n\nCreate a Lambda function that will respond to HTTP requests:",
      "content_length": 1028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "6.\n\n7.\n\n8.\n\nCleanup\n\nLAMBDA_ARN=$(aws lambda create-function \\\n\n--function-name AWSCookbook509Lambda \\\n\n--runtime python3.8 \\\n\n--package-type \"Zip\" \\\n\n--zip-file fileb://lambda_function.zip \\\n\n--handler lambda_function.lambda_handler --publish \\\n\n--role arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbookLambdaRole \\\n\n--output text --query FunctionArn \\\n\n--vpc-config\n\nSubnetIds=${TRIMMED_ISOLATED_SUBNETS},SecurityGroupIds=${LAMBDA_SG_ID})\n\nCreate an ElastiCache subnet group:\n\naws elasticache create-cache-subnet-group \\\n\n--cache-subnet-group-name \"AWSCookbook509CacheSG\" \\\n\n--cache-subnet-group-description \"AWSCookbook509CacheSG\" \\\n\n--subnet-ids $ISOLATED_SUBNETS\n\nCreate an ElastiCache Redis cluster with one node:\n\naws elasticache create-cache-cluster \\\n\n--cache-cluster-id \"AWSCookbook509CacheCluster\" \\\n\n--cache-subnet-group-name AWSCookbook509CacheSG \\\n\n--engine redis \\\n\n--cache-node-type cache.t3.micro \\\n\n--num-cache-nodes 1\n\nWait for the cache cluster to be available.\n\nInvoke the function and view the response replacing HOSTNAME with the host name of your cluster:\n\naws lambda invoke \\\n\n--cli-binary-format raw-in-base64-out \\\n\n--function-name $LAMBDA_ARN \\\n\n--payload '{ \"hostname\": \"HOSTNAME\" }' \\\n\nresponse.json && cat response.json",
      "content_length": 1245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "Follow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nBy default, Lambda functions do not have access to any VPC that you may have provisioned in your AWS environment. However, Lambda does support VPC connectivity by provisioning network interfaces in your VPC. ElastiCache requires compute nodes that have network interfaces in your VPC, so you need to configure Lambda within your VPC to allow it to access the ElastiCache nodes that you provision.\n\nThe compute memory that your function uses is not persistent if the function is invoked when the execution environment is spun down and restarted. If your application requires access to memory persistence (for example, in HTTP sessions), you can use the Amazon ElastiCache service to implement redis or memcached for session storage and key/value storage. These common solutions implement in-memory cache for fast read/write and allow you to scale horizontally with your application while maintaining memory persistence that your application requires.\n\nChallenge 1\n\nConfigure your Lambda function to read and write additional values to Amazon ElastiCache for Redis.",
      "content_length": 1149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "Chapter 6. Containers\n\n6.0 Introduction A container, put simply, packages application code, binaries, configuration files, and libraries together into a single package, called a container image. By packaging everything together in this way, you can develop, test, and run applications with control and consistency. You can quickly start packaging up and testing containers that you build locally, while ensuring that the exact same runtime environment is present regardless of where it is running. This generally reduces the time it takes to build something and offer it to a wide audience, and ensures consistency whenever you deploy.\n\nContainers are wholly “contained” environments that leverage the underlying compute and memory capabilities on the host where they are running (your laptop, a server in a closet, or the cloud). Multiple containers can be run on the same host at once without conflicts. You can also have multiple containers running with the intention of them communicating with one another. Imagine that you have a frontend web application running as a container that accesses a container running a backend for your website, and you might want to run multiple instances of them at once to handle more traffic. Running multiple containers at once and ensuring they are always available can present some challenges, which is why you enlist the help of a container orchestrator. Popular orchestrators come in many flavors, but some of the common ones you may have heard of are Kubernetes and Docker Swarm.\n\nYou have options for running containers on AWS, such as Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Kubernetes Service (Amazon EKS) as container orchestrators, and Amazon Elastic Cloud Compute (Amazon EC2) for deployments with custom requirements. Both of the AWS container orchestrator services mentioned (Amazon ECS and Amazon EKS) can run workloads on Amazon EC2 or on the fully managed AWS Fargate compute engine. In other words, you can choose to control the underlying EC2 instance (or instances) responsible for running your containers on Amazon ECS and Amazon EKS, allowing some level of customization to your host, or you can use Fargate, which is fully managed by AWS—you don’t have to worry about instance management. You can even use ECS and EKS within your own datacenter using ECS Anywhere and EKS Anywhere. AWS provides a comprehensive listing of all up-to-date container services on its website.\n\nSome AWS services (AWS CodeDeploy, AWS CodePipeline, and Amazon Elastic Container Registry) can help streamline the development lifecycle and provide automation to your workflow. These integrate well with Amazon ECS and Amazon EKS. Some examples of AWS services that provide network capabilities are Amazon Virtual Private Cloud, AWS Elastic Load Balancing, AWS Cloud Map, and Amazon Route 53.You can address your logging and monitoring concerns with Amazon CloudWatch and the Amazon Managed Service for Prometheus. Fine-grained security capabilities can be provided by AWS Identity and Access Management (IAM) and AWS Key Management System (KMS). By following the recipes in this chapter, you will see how some of these services combine to meet your needs.",
      "content_length": 3215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "Workstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/Containers",
      "content_length": 259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "Chapter Prerequisites\n\nDocker installation and validation\n\nDocker Desktop is recommended for Windows and Mac users; Docker Linux Engine is recommended for Linux users. In the following recipes, you’ll use Docker to create a consistent working environment on your particular platform. Be sure to install the latest stable version of Docker for your OS.\n\nMacOS\n\n1.\n\nFollow instructions from Docker Desktop: https://docs.docker.com/docker- for-mac/install.\n\n2.\n\nRun the Docker Desktop application after installation.\n\nWindows\n\n1.\n\nFollow instructions from Docker Desktop: https://docs.docker.com/desktop/windows/install.\n\n2.\n\nRun the Docker Desktop application after installation.\n\nLinux\n\n1.\n\nFollow instructions from Docker Desktop: https://docs.docker.com/engine/install.\n\n2.\n\nStart the Docker daemon on your distribution:\n\nValidation the installation of Docker on your workstation with the following command:\n\ndocker --version\n\nYou should see output similar to the following:\n\nDocker version 19.03.13, build 4484c46d9d\n\nRun the docker images command to list images on your local machine:\n\nREPOSITORY TAG IMAGE ID CREATED SIZE",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "6.1 Building, Tagging, and Pushing a Container Image to Amazon ECR\n\nProblem\n\nYou need a repository to store built and tagged container images.\n\nSolution\n\nFirst, you will create a repository in Amazon ECR. Next, you will create a Dockerfile and use it to build a Docker image. Finally, you will apply two tags to the container image and push them both to the newly created ECR repository. This process is illustrated in Figure 6-1.\n\nFigure 6-1. Solution workflow of build, tag, and push for container images\n\nSteps\n\n1.\n\nLog into the AWS Management Console and search for Elastic Container Registry. Click the “Create repository” button.",
      "content_length": 635,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "Give your repository a name, keep all defaults (as shown in Figure 6-2), scroll to the bottom, and click “Create repository” again to finish.",
      "content_length": 141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "Figure 6-2. ECR repository creation\n\nYou now have a repository created on Amazon ECR that you can use to store container images. An example of the ECR console is shown in Figure 6-3.\n\nFigure 6-3. Screenshot of created ECR repository\n\nAlternatively, you can also create an ECR repository from the command line:\n\nREPO=aws-cookbook-repo && \\\n\naws ecr create-repository --repository-name $REPO\n\nYou should see output similar to the following:\n\n{",
      "content_length": 441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "2.\n\n3.\n\n4.\n\n\"repository\": {\n\n\"repositoryArn\": \"arn:aws:ecr:us-east-1:111111111111:repository/aws-cookbook-\n\nrepo\",\n\n\"registryId\": \"1111111111111\",\n\n\"repositoryName\": \"aws-cookbook-repo\",\n\n\"repositoryUri\": \"611652777867.dkr.ecr.us-east-1.amazonaws.com/aws-cookbook-\n\nrepo\",\n\n\"createdAt\": \"2021-10-02T19:57:56-04:00\",\n\n\"imageTagMutability\": \"MUTABLE\",\n\n\"imageScanningConfiguration\": {\n\n\"scanOnPush\": false\n\n},\n\n\"encryptionConfiguration\": {\n\n\"encryptionType\": \"AES256\"\n\n}\n\n}\n\n}\n\nCreate a simple Dockerfile:\n\necho FROM nginx:latest > Dockerfile\n\nNOTE\n\nThis command creates a Dockerfile that contains a single line instructing the Docker Engine to use the\n\nnginx:latest image as the base image. Since you use only the base image with no other lines in the Dockerfile, the resulting image is identical to the nginx:latest image. You could include some HTML files within this image by using the COPY and ADD Dockerfile directives.\n\nBuild and tag the image. This step may take a few moments as it downloads and combines the image layers:\n\ndocker build . -t \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:latest\n\nAdd an additional tag:\n\ndocker tag \\",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "5.\n\n6.\n\nValidation checks\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:latest \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:1.0\n\nGet Docker login information:\n\naws ecr get-login-password | docker login --username AWS \\\n\n--password-stdin $AWS_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com\n\nHere is the output:\n\nLogin Succeeded\n\nNOTE\n\nAn authorization token needs to be provided each time an operation is executed against a private\n\nrepository. Tokens last for 12 hours, so the command you ran would need to be manually refreshed at\n\nthat interval on your command line. To help you with automating the task of obtaining authorization\n\ntokens, you can use the ECR Docker Credential Helper, available from the awslabs GitHub repository.\n\nPush each image tag to Amazon ECR:\n\ndocker push \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:latest\n\ndocker push \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:1.0\n\nNOTE\n\nYou will see “Layer already exists” for the image layer uploads on the second push. This is because\n\nthe image already exists in the ECR repository due to the first push, but this step is still required to add\n\nthe additional tag.",
      "content_length": 1231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "Now you can view both of the tagged images in Amazon ECR from the console, as shown in Figure 6-4.\n\nFigure 6-4. Screenshot of the image with two tags\n\nAlternatively, you can use the AWS CLI to list the images:\n\naws ecr list-images --repository-name aws-cookbook-repo\n\nYou should see output similar to the following:\n\n{\n\n\"imageIds\": [\n\n{\n\n\"imageDigest\": \"sha256:99d0a53e3718cef59443558607d1e100b325d6a2b678cd2a48b05e5e22ffeb49\",",
      "content_length": 427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "\"imageTag\": \"1.0\"\n\n},\n\n{\n\n\"imageDigest\": \"sha256:99d0a53e3718cef59443558607d1e100b325d6a2b678cd2a48b05e5e22ffeb49\",\n\n\"imageTag\": \"latest\"\n\n}\n\n]\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nHaving a repository for your container images is an important foundational component of the application development process. A private repository for your container images that you control is a best practice to increase the security of your application development process. You can grant access to other AWS accounts, IAM entities, and AWS services with permissions for Amazon ECR. Now that you know how to create an ECR repository, you will be able to store your container images and use them with AWS services.\n\nNOTE\n\nAmazon ECR supports the popular Docker Image Manifest V2, Schema 2 and most recently Open Container Initiative\n\n(OCI) images. It can translate between these formats on pull. Legacy support is available for Manifest V2, Schema 1, and\n\nAmazon ECR can translate on the fly when interacting with legacy Docker client versions. The experience should be\n\nseamless for most Docker client versions in use today.\n\nContainer tagging allows you to version and keep track of your container images. You can apply multiple tags to an image, which can help you implement your versioning strategy and deployment process. For example, you may always refer to a “latest” tagged image in your dev environment, but your production environment can be locked to a specific version tag. The Docker CLI pushes tagged images to the repository and the tags can be used with pulls.\n\nChallenge\n\nModify the Dockerfile, build a new image, tag it with a new version number, and put it to ECR.\n\n6.2 Scanning Images for Security Vulnerabilities on Push to Amazon ECR",
      "content_length": 1796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "Problem\n\nYou want to automatically scan your container images for security vulnerabilities each time you push to a repository.\n\nSolution\n\nEnable automatic image scanning on a repository in Amazon ECR, push an image, and observe the scan results, as shown in Figure 6-5.\n\nPrerequisite\n\nECR repository\n\nFigure 6-5. Container image scanning solution workflow\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps",
      "content_length": 449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "1.\n\nRather than building a new container image from a Dockerfile (as you did in Recipe 6.1), this time you are going to pull an old NGINX container image:\n\ndocker pull nginx:1.14.1\n\n2.\n\nOn the command line, apply the scanning configuration to the repository you created:\n\nREPO=aws-cookbook-repo && \\\n\naws ecr put-image-scanning-configuration \\\n\n--repository-name $REPO \\\n\n--image-scanning-configuration scanOnPush=true\n\n3.\n\nGet Docker login information:\n\naws ecr get-login-password | docker login --username AWS \\\n\n--password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n\n4.\n\nApply a tag to the image so that you can push it to the ECR repository:\n\ndocker tag nginx:1.14.1 \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:old\n\n5.\n\nPush the image:\n\ndocker push \\\n\n$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/aws-cookbook-repo:old\n\nValidation checks\n\nShortly after the push is complete, you can examine the results of the security scan of the image in JSON format:\n\naws ecr describe-image-scan-findings \\",
      "content_length": 1041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "--repository-name aws-cookbook-repo --image-id imageTag=old\n\nYou should see output similar to the following:\n\n{\n\n\"imageScanFindings\": {\n\n\"findings\": [\n\n{\n\n\"name\": \"CVE-2019-3462\",\n\n\"description\": \"Incorrect sanitation of the 302 redirect field in HTTP transport method of apt\n\nversions 1.4.8 and earlier can lead to content injection by a MITM attacker, potentially leading to\n\nremote code execution on the target machine.\",\n\n\"uri\": \"https://security-tracker.debian.org/tracker/CVE-2019-3462\",\n\n\"severity\": \"CRITICAL\",\n\n\"attributes\": [\n\n{\n\n\"key\": \"package_version\",\n\n\"value\": \"1.4.8\"\n\n},\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nTIP\n\nAmazon ECR has a safety mechanism built in that does not let you delete a repository containing images. If the repository\n\nis not empty and the delete-repository command is failing, you can bypass this check by adding --force to the delete-repository command.\n\nDiscussion\n\nThe Common Vulnerabilities and Exposures (CVEs) database from the open source Clair project is used by Amazon ECR for vulnerability scanning. You are provided a Common Vulnerability Scoring System (CVSS) score to indicate the severity of any detected vulnerabilities. This helps you detect and remediate vulnerabilities in your container image. You can configure alerts for newly discovered vulnerabilities in images by using Amazon EventBridge and Amazon Simple Notification Service (Amazon SNS).",
      "content_length": 1447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "WARNING\n\nThe scanning feature does not continuously scan your images, so it is important to push your image versions routinely (or\n\ntrigger a manual scan).\n\nYou can retrieve the results of the last scan for an image at any time with the command used in the last step of this recipe. Furthermore, you can use these commands as part of an automated CI/CD process that may validate whether or not an image has a certain CVSS score before deploying.\n\nChallenge 1\n\nRemediate the vulnerability by updating the image with the latest NGINX container image.\n\nChallenge 2\n\nConfigure an SNS topic to send you an email when vulnerabilities are detected in your repository.",
      "content_length": 660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "6.3 Deploying a Container Using Amazon Lightsail\n\nProblem\n\nYou need to quickly deploy a container-based application and access it securely over the internet.\n\nSolution\n\nDeploy a plain NGINX container that listens on port 80 to Lightsail. Lightsail provides a way to quickly deploy applications to AWS. The workflow is shown in Figure 6-6.\n\nFigure 6-6. Amazon Lightsail serving a container image\n\nPrerequisite\n\nIn addition to Docker Desktop and the AWS CLI (version 2), you need to install the Lightsail Control plugin (lightsailctl) for the AWS CLI.\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.",
      "content_length": 636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "Steps\n\nNOTE\n\nSeveral power levels are available for Lightsail, each of which is priced according to the amount of compute power your\n\ncontainer needs. We selected nano in this example. A list of power levels and associated costs is available in the Lightsail\n\npricing guide.\n\n1.\n\nOnce you have lightsailctl installed, create a new container service and give it a name, power parameter, and scale parameter:\n\naws lightsail create-container-service \\\n\n--service-name awscookbook --power nano --scale 1\n\nYou should see output similar to the following:\n\n{\n\n\"containerService\": {\n\n\"containerServiceName\": \"awscookbook\",\n\n\"arn\": \"arn:aws:lightsail:us-east-1:111111111111:ContainerService/124633d7-b625-\n\n48b2-b066-5826012904d5\",\n\n\"createdAt\": \"2020-11-15T10:10:55-05:00\",\n\n\"location\": {\n\n\"availabilityZone\": \"all\",\n\n\"regionName\": \"us-east-1\"\n\n},\n\n\"resourceType\": \"ContainerService\",\n\n\"tags\": [],\n\n\"power\": \"nano\",\n\n\"powerId\": \"nano-1\",\n\n\"state\": \"PENDING\",\n\n\"scale\": 1,\n\n\"isDisabled\": false,\n\n\"principalArn\": \"\",\n\n\"privateDomainName\": \"awscookbook.service.local\",\n\n\"url\": \"https://awscookbook.<<unique-id>>.us-east-1.cs.amazonlightsail.com/\"\n\n}\n\n}",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "2.\n\n3.\n\n4.\n\nPull a plain NGINX container image to use that listens on port 80/TCP.\n\ndocker pull nginx\n\nUse the following command to ensure that the state of your container service has entered the READY state (this may take a few minutes):\n\naws lightsail get-container-services --service-name awscookbook\n\nWhen the container service is ready, push the container image to Lightsail:\n\naws lightsail push-container-image --service-name awscookbook \\\n\n--label awscookbook --image nginx\n\nYou should see output similar to the following:\n\n7b5417cae114: Pushed\n\nImage \"nginx\" registered.\n\nRefer to this image as \":awscookbook.awscookbook.1\" in deployments.\n\nNOTE\n\nYou can specify public image repositories or push your own image to your container service within\n\nAmazon Lightsail. Rather than using a private Amazon ECR location, your Lightsail images are kept\n\nwithin the Lightsail service. For more information, refer to the Lightsail documentation for image\n\nlocations.\n\nNow you will associate the image you pushed with the container service you created for deployment. Create a file with the following contents and save it as lightsail.json (file provided in the code repository):\n\n{\n\n\"serviceName\": \"awscookbook\",\n\n\"containers\": {",
      "content_length": 1226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "\"awscookbook\": {\n\n\"image\": \":awscookbook.awscookbook.1\",\n\n\"ports\": {\n\n\"80\": \"HTTP\"\n\n}\n\n}\n\n},\n\n\"publicEndpoint\": {\n\n\"containerName\": \"awscookbook\",\n\n\"containerPort\": 80\n\n}\n\n}\n\n5.\n\nCreate the deployment:\n\naws lightsail create-container-service-deployment \\\n\n--service-name awscookbook --cli-input-json file://lightsail.json\n\nView your container service again and wait for the ACTIVE state. This may take a few minutes:\n\naws lightsail get-container-services --service-name awscookbook\n\nNote the endpoint URL at the end of the output.\n\nValidation checks\n\nNow, visit the endpoint URL in your browser, or use the curl on the command line (e.g., url: https://awscookbook.un94eb3cd7hgk.us-east-1.cs.amazonlightsail.com):\n\ncurl <<URL endpoint>>\n\nYou should see output similar to the following:\n\n...\n\n<h1>Welcome to nginx!</h1>\n\n...",
      "content_length": 822,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nLightsail manages the TLS certificate, load balancer, compute, and storage. It can also manage MySQL and PostgreSQL databases as part of your deployment if your application requires it. Lightsail performs routine health checks on your application and will automatically replace a container you deploy that may have become unresponsive for some reason. Changing the power and scale parameters in the lightsail create-container-service command will allow you to create services for demanding workloads.\n\nUsing this recipe, you could deploy any common container-based application (e.g., Wordpress) and have it served on the internet in a short period of time. You could even point a custom domain alias at your Lightsail deployment for an SEO-friendly URL.\n\nChallenge\n\nScale your service so that it will be able to handle more traffic.",
      "content_length": 927,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "6.4 Deploying Containers Using AWS Copilot\n\nProblem\n\nYou need a way to use your existing Dockerfile to quickly deploy and manage a load balanced web service using best practices in a private network.\n\nSolution\n\nStarting with a Dockerfile, you can use AWS Copilot to quickly deploy an application using an architecture shown in Figure 6-7.",
      "content_length": 338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "Prerequisite\n\nFigure 6-7. AWS Copilot load balanced web service infrastructure\n\nAWS Copilot CLI",
      "content_length": 95,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "Preparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCopilot requires an ECS service-linked role to allow Amazon ECS to perform actions on your behalf. This may already exist in your AWS account. To see if you have this role already, issue the following command:\n\naws iam list-roles --path-prefix /aws-service-role/ecs.amazonaws.com/\n\n(If the role is displayed, you can skip the following role-creation step.)\n\nCreate the ECS service-linked role if it does not exist:\n\naws iam create-service-linked-role --aws-service-name ecs.amazonaws.com\n\nNOTE\n\nIAM service-linked roles allow AWS services to securely interact with other AWS services on your\n\nbehalf. See the AWS article on using these roles.\n\n2.\n\ncd to this recipe’s directory in this chapter’s code repository:\n\ncd 604-Deploy-Container-With-Copilot-CLI\n\nNOTE\n\nYou could provide your own Dockerfile and content for this recipe. If you choose to use your own\n\ncontainer with this recipe, ensure that the container listens on port 80/TCP. Or configure the alternate\n\nport with the copilot init command.\n\n3.\n\nNow use AWS Copilot to deploy the sample NGINX Dockerfile to Amazon ECS:",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "copilot init --app web --name nginx --type 'Load Balanced Web Service' \\\n\n--dockerfile './Dockerfile' --port 80 --deploy\n\nNOTE\n\nIf you don’t specify any arguments to the copilot init command, it will walk you through a menu of options for your deployment.\n\nThe deployment will take a few moments. You can watch the progress of the deployment in your terminal.\n\nValidation checks\n\nAfter the deployment is complete, get information on the deployed service with this command:\n\ncopilot svc show\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nTIP\n\nCopilot exposes a command-line interface that simplifies deployments to Amazon ECS, AWS Fargate, and AWS App\n\nRunner. It helps streamline your development workflow and deployment lifecycle.\n\nThe copilot init command created a directory called copilot in your current working directory. You can view and customize the configuration by using the manifest.yml that is associated with your application.\n\nNOTE\n\nThe test environment is the default environment created. You can add additional environments to suit your needs and keep\n\nyour environments isolated from each other by using the copilot env init command.",
      "content_length": 1201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "Copilot configures all of the required resources for hosting containers on Amazon ECS according to many best practices. Some examples are deploying to multiple Availability Zones), using subnet tiers to segment traffic, and using AWS KMS to encrypt.\n\nThe AWS Copilot commands can also be embedded in your CI/CD pipeline to perform automated deployments. In fact, Copilot can orchestrate the creation and management of a CI/CD pipeline for you with the copilot pipeline command. For all of the current supported features and examples, visit the AWS Copilot home page.\n\nChallenge\n\nReconfigure your Load Balanced Web Service to deploy to AWS App Runner instead of Amazon ECS.\n\n6.5 Updating Containers with Blue/Green Deployments\n\nProblem\n\nYou want to use a deployment strategy with your container-based application so you can update your application to the latest version without introducing downtime to customers, while also being able to easily roll back if the deployment was not successful.\n\nSolution\n\nUse AWS CodeDeploy to orchestrate your application deployments to Amazon ECS with the Blue/Green strategy, as shown in Figure 6-8.",
      "content_length": 1133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "Figure 6-8. Blue/Green target group association",
      "content_length": 47,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "Preparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nWith the CDK stack deployed, open a web browser and visit the LOAD_BALANCER_DNS address on TCP port 8080 that the CDK output displayed. You will see the “Blue” application running:\n\nE.g.:\n\nfirefox $LOAD_BALANCER_DNS:8080\n\nor\n\nopen http://$LOAD_BALANCER_DNS:8080\n\n2.\n\nCreate an IAM role with the statement in the provided assume-role- policy.json file using this command:\n\naws iam create-role --role-name ecsCodeDeployRole \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\n3.\n\nAttach the IAM managed policy for CodeDeployRoleForECS to the IAM role:\n\naws iam attach-role-policy --role-name ecsCodeDeployRole \\\n\n--policy-arn arn:aws:iam::aws:policy/AWSCodeDeployRoleForECS\n\n4.\n\nCreate a new ALB target group to use as the “Green” target group with CodeDeploy:\n\naws elbv2 create-target-group --name \"GreenTG\" --port 80 \\\n\n--protocol HTTP --vpc-id $VPC_ID --target-type ip\n\n5.\n\nCreate the CodeDeploy application:\n\naws deploy create-application --application-name awscookbook-605 \\",
      "content_length": 1088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "6.\n\n7.\n\n8.\n\n9.\n\n--compute-platform ECS\n\nNOTE\n\nCodeDeploy requires some configuration. We provided a template file for you (codedeploy-\n\ntemplate.json) in this recipe’s directory of the code repository.\n\nUse the sed command to replace the values with the environment variables you exported with the helper.py script:\n\nsed -e \"s/AWS_ACCOUNT_ID/${AWS_ACCOUNT_ID}/g\" \\\n\ne \"s|PROD_LISTENER_ARN|${PROD_LISTENER_ARN}|g\" \\\n\ne \"s|TEST_LISTENER_ARN|${TEST_LISTENER_ARN}|g\" \\\n\ncodedeploy-template.json > codedeploy.json\n\nTIP\n\nsed (short for stream editor) is a great tool to use for text find-and-replace operations as well as other types of text manipulation in your terminal sessions and scripts. In this case, sed is used to replace values in a template file with values output from cdk deploy set as environment variables.\n\nNow, create a deployment group:\n\naws deploy create-deployment-group --cli-input-json file://codedeploy.json\n\nAppSpec-template.yaml contains information about the application you are going to update. The CDK preprovisioned a task definition you can use.\n\nUse the sed command to replace the value with the environment variable you exported with the helper.py script:\n\nsed -e \"s|FargateTaskGreenArn|${FARGATE_TASK_GREEN_ARN}|g\" \\\n\nappspec-template.yaml > appspec.yaml\n\nNow copy the AppSpec file to the S3 bucket created by the CDK deployment so that CodeDeploy can use it to update the application:",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "10.\n\n11.\n\naws s3 cp ./appspec.yaml s3://$BUCKET_NAME\n\nOne final configuration file needs to be created; this contains the instructions about the deployment. Use sed to modify the S3 bucket used in the deployment-template.json file:\n\nsed -e \"s|S3BucketName|${BUCKET_NAME}|g\" \\\n\ndeployment-template.json > deployment.json\n\nNow create a deployment with the deployment configuration:\n\naws deploy create-deployment --cli-input-json file://deployment.json\n\nTo get the status of the deployment, observe the status in the AWS Console (Developer Tools→CodeDeploy→Deployments, and click the deployment ID). You should see CodeDeploy in progress with the deployment, as shown in Figure 6-9.",
      "content_length": 679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Figure 6-9. Initial deployment status\n\nValidation checks\n\nOnce the replacement task is serving 100% of the traffic, you can visit the same URL where you previously observed the Blue application running, replaced with the Green version of the application.",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "TIP\n\nYou may need to refresh your browser or clear your cache to see the updated Green application.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nCodeDeploy offers several deployment strategies—Canary, AllAtOnce, Blue/Green, etc.—and you can also create your own custom deployment strategies. You might customize the strategy to define a longer wait period for the cutover window or to define other conditions to be met before traffic switchover occurs. In the default Blue/Green strategy, CodeDeploy keeps your previous version of the application running for five minutes while all traffic is routed to the new version. If you notice that the new version is not behaving properly, you can quickly route traffic back to the original version since it is still running in a separate AWS Application Load Balancer target group.\n\nCodeDeploy uses ALB target groups to manage which application is considered “production.” When you deployed the initial stack with the AWS CDK, the V1 Blue containers were registered with a target group associated with port 8080 on the ALB. After you initiate the deployment of the new version, CodeDeploy starts a brand new version of the ECS service, associates it with the Green target group you created, and then gracefully shifts all traffic to the Green target group. The final result is the Green V2 containers now being served on port 8080 of the ALB. The previous target group is now ready to execute the next Blue/Green deployment.\n\nThis is a common pattern to utilize with CI/CD. Your previous version can quickly be reactivated with a seamless rollback. If no rollback is needed, the initial version (V1) is terminated, and you can repeat the processes the next time you deploy, putting V3 in the Blue target group, shifting traffic to it when you are ready. Using this strategy helps you minimize the impact to users of new application versions while allowing more frequent deployments.\n\nTIP\n\nDeployment conditions allow you to define deployment success criteria. You can use a combination of a custom\n\ndeployment strategy and a deployment condition to build automation tests into your CodeDeploy process. This would\n\nallow you to ensure that all of your tests run and pass before traffic is sent to your new deployment.\n\nChallenge\n\nTrigger a rollback to the original V1 container deployment and observe the results.",
      "content_length": 2405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "6.6 Autoscaling Container Workloads on Amazon ECS\n\nProblem\n\nYou need to deploy a containerized service that scales out during times of heavy traffic to meet demand.\n\nSolution\n\nConfigure a CloudWatch alarm and scaling policy for an ECS service so that your service adds more containers when the CPU load increases, as shown in Figure 6-10.\n\nFigure 6-10. ECS service with a CloudWatch alarm and scaling policy\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nAccess the ECS service URL over the internet with the curl command (or your web browser) to verify the successful deployment:\n\ncurl -v -m 3 $LOAD_BALANCER_DNS",
      "content_length": 665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "2.\n\n3.\n\nUse verbose (-v) and three-second timeout (-m 3) to ensure you see the entire connection and have a timeout set. The following is an example command and output:\n\ncurl -v -m 3 http://AWSCookbook.us-east-1.elb.amazonaws.com:80/\n\nTrying 1.2.3.4...\n\nTCP_NODELAY set\n\nConnected to AWSCookbook.us-east-1.elb.amazonaws.com (1.2.3.4) port 80<\n\n> GET / HTTP/1.1\n\n> Host: AWSCookbook.us-east-1.elb.amazonaws.com:80\n\n> User-Agent: curl/7.64.1\n\n> Accept: */*\n\n>\n\n< HTTP/1.1 200\n\n< Content-Type: application/json\n\n< Content-Length: 318\n\n< Connection: keep-alive\n\n<\n\n{\n\n\"URL\":\"http://awscookbookloadtestloadbalancer-36821611.us-east-\n\n1.elb.amazonaws.com:80/\",\n\n\"ContainerLocalAddress\":\"10.192.2.179:8080\",\n\n\"ProcessingTimeTotalMilliseconds\":\"0\",\n\n\"LoadBalancerPrivateIP\":\"10.192.2.241\",\n\n\"ContainerHostname\":\"ip-10-192-2-179.ec2.internal\",\n\n\"CurrentTime\":\"1605724705176\"\n\n}\n\nClosing connection 0\n\nTIP\n\nRun this same curl command several times in a row, and you will notice the ContainerHostname and ContainerLocalAddress alternating between two addresses. This indicates that Amazon ECS is load balancing between the two containers you should expect to be running at all times, as defined by the\n\nECS service.\n\nYou will need to create a role for the autoscaling trigger to execute; this file is provided in this solution’s directory in the code repository:\n\naws iam create-role --role-name AWSCookbook606ECS \\",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "4.\n\n5.\n\n6.\n\n7.\n\n--assume-role-policy-document file://task-execution-assume-role.json\n\nAttach the managed policy for autoscaling:\n\naws iam attach-role-policy --role-name AWSCookbook606ECS --policy-arn\n\narn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceAutoscaleRole\n\nRegister an autoscaling target:\n\naws application-autoscaling register-scalable-target \\\n\n--service-namespace ecs \\\n\n--scalable-dimension ecs:service:DesiredCount \\\n\n--resource-id service/$ECS_CLUSTER_NAME/AWSCookbook606 \\\n\n--min-capacity 2 \\\n\n--max-capacity 4\n\nSet up an autoscaling policy for the autoscaling target using the sample configuration file specifying a 50% average CPU target:\n\naws application-autoscaling put-scaling-policy --service-namespace ecs \\\n\n--scalable-dimension ecs:service:DesiredCount \\\n\n--resource-id service/$ECS_CLUSTER_NAME/AWSCookbook606 \\\n\n--policy-name cpu50-awscookbook-606 --policy-type TargetTrackingScaling \\\n\n--target-tracking-scaling-policy-configuration file://scaling-policy.json\n\nNow, to trigger a process within the container that simulates high CPU load, run the same curl command, appending cpu to the end of the service URL:\n\ncurl -v -m 3 $LOAD_BALANCER_DNS/cpu\n\nThis command will time out after three seconds, indicating that the container is running a CPU-intensive process as a result of visiting that URL (the ECS service you deployed with the CDK runs a CPU load generator that we provided to simulate high CPU usage). The following is an example command and output:",
      "content_length": 1495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "curl -v -m 3 http://AWSCookbookLoadtestLoadBalancer-36821611.us-east-\n\n1.elb.amazonaws.com:80/cpu\n\nTrying 52.4.148.24...\n\nTCP_NODELAY set\n\nConnected to AWSCookbookLoadtestLoadBalancer-36821611.us-east-\n\n1.elb.amazonaws.com (52.4.148.245) port 80 (#0)\n\n> GET /cpu HTTP/1.1\n\n> Host: AWSCookbookLoadtestLoadBalancer-36821611.us-east-1.elb.amazonaws.com:80\n\n> User-Agent: curl/7.64.1\n\n> Accept: */*\n\n>\n\nOperation timed out after 3002 milliseconds with 0 bytes received\n\nClosing connection 0\n\ncurl: (28) Operation timed out after 3002 milliseconds with 0 bytes received\n\nValidation checks\n\nWait approximately five minutes. Then log into the AWS Console, locate Elastic Container Service, go to the Clusters page, select the cluster deployed, and select the ECS service. Verify that the Desired Count has increased to 4, the maximum scaling value that you configured. You can click the Tasks tab to view four container tasks now running for your service.\n\nClick the Metrics tab to view the CPU usage for the service. You set the scaling target at 50% to trigger the autoscaling actions, adding two additional containers to the service as a result of high CPU usage. An example metrics graph is shown in Figure 6-11.",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "Figure 6-11. ECS service metrics on the AWS Console\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAutosccaling is an important mechanism to implement to save costs associated with running your applications on AWS services. It allows your applications to provision their own resources as needed during times when load may increase, and it removes their own resources during times when the application may be idle. Note that whenever you have an AWS service doing something like this on your behalf, you have to specifically grant permission for services to execute these functions via IAM.\n\nThe underlying data that provides the metrics for such operations is contained in the CloudWatch metrics service. There are many data points and metrics that you can use for configuring autoscaling; some of the most common ones are listed here:",
      "content_length": 884,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "Network I/O\n\nCPU usage\n\nMemory used\n\nNumber of transactions\n\nIn this recipe, you monitor the CPU usage metric on the ECS service. You set the metric at 50% and trigger the CPU load with a cURL call to the HTTP endpoint of the ECS service. Scaling metrics are dependent upon the type of applications you are running and the technologies you use to build them. As a best practice, you should observe your application metrics over a period of time to set a baseline before choosing metrics to implement autoscaling.\n\nChallenge\n\nReplace the provided sample CPU load application with your own containerized application and configure the target scaling policy to meet your needs.\n\n6.7 Launching a Fargate Container Task in Response to an Event\n\nProblem\n\nYou need to launch a container task to process incoming files.\n\nSolution\n\nUse Amazon EventBridge to trigger the launch of ECS container tasks on Fargate after a file is uploaded to S3, as shown in Figure 6-12.\n\nFigure 6-12. Flow of container EventBridge pattern\n\nPreparation",
      "content_length": 1022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "Follow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nConfigure CloudTrail to log events on the S3 bucket:\n\naws cloudtrail put-event-selectors --trail-name $CLOUD_TRAIL_ARN --event-\n\nselectors \"[{ \\\"ReadWriteType\\\":\n\n\\\"WriteOnly\\\", \\\"IncludeManagementEvents\\\":false, \\\"DataResources\\\": [{\n\n\\\"Type\\\": \\\"AWS::S3::Object\\\",\n\n\\\"Values\\\": [\\\"arn:aws:s3:::$BUCKET_NAME/input/\\\"] }],\n\n\\\"ExcludeManagementEventSources\\\": [] }]\"\n\nNow create an assume-role policy JSON statement called policy1.json to use in the next step (this file is provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"events.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\n2.\n\nCreate the role and specify the policy1.json file:\n\naws iam create-role --role-name AWSCookbook607RuleRole \\\n\n--assume-role-policy-document file://policy1.json\n\n3.\n\nYou will also need a policy document with the following content called policy2.json (this file is provided in the repository):\n\n{",
      "content_length": 1046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "4.\n\n5.\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"ecs:RunTask\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:ecs:*:*:task-definition/*\"\n\n]\n\n},\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": \"iam:PassRole\",\n\n\"Resource\": [\n\n\"*\"\n\n],\n\n\"Condition\": {\n\n\"StringLike\": {\n\n\"iam:PassedToService\": \"ecs-tasks.amazonaws.com\"\n\n}\n\n}\n\n}\n\n]\n\n}\n\nNow attach the IAM policy JSON you just created to the IAM role:\n\naws iam put-role-policy --role-name AWSCookbook607RuleRole \\\n\n--policy-name ECSRunTaskPermissionsForEvents \\\n\n--policy-document file://policy2.json\n\nCreate an EventBridge rule that monitors the S3 bucket for file uploads:\n\naws events put-rule --name \"AWSCookbookRule\" --role-arn\n\n\"arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook607RuleRole\" --event-pattern \"\n\n{\\\"source\\\":[\\\"aws.s3\\\"],\\\"detail-type\\\":[\\\"AWS API Call via\n\nCloudTrail\\\"],\\\"detail\\\":{\\\"eventSource\\\":[\\\"s3.amazonaws.com\\\"],\\\"eventName\\\":\n\n[\\\"CopyObject\\\",\\\"PutObject\\\",\\\"CompleteMultipartUpload\\\"],\\\"requestParameters\\\"\n\n:{\\\"bucketName\\\":[\\\"$BUCKET_NAME\\\"]}}}\"",
      "content_length": 1024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "6.\n\n7.\n\n8.\n\n9.\n\nModify the value in targets-template.json and create a targets.json for use:\n\nsed -e \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\ne \"s|AWS_REGION|${AWS_REGION}|g\" \\\n\ne \"s|ECSClusterARN|${ECS_CLUSTER_ARN}|g\" \\\n\ne \"s|TaskDefinitionARN|${TASK_DEFINITION_ARN}|g\" \\\n\ne \"s|VPCPrivateSubnets|${VPC_PRIVATE_SUBNETS}|g\" \\\n\ne \"s|VPCDefaultSecurityGroup|${VPC_DEFAULT_SECURITY_GROUP}|g\" \\\n\ntargets-template.json > targets.json\n\nCreate a rule target that specifies the ECS cluster, ECS task definition, IAM role, and networking parameters. This specifies what the rule will trigger; in this case, launch a container on Fargate:\n\naws events put-targets --rule AWSCookbookRule \\\n\n--targets file://targets.json\n\nYou should see output similar to the following:\n\n{\n\n\"FailedEntryCount\": 0,\n\n\"FailedEntries\": []\n\n}\n\nCheck the S3 bucket to verify that it’s empty before we populate it:\n\naws s3 ls s3://$BUCKET_NAME/\n\nCopy the provided maze.jpg file to the S3 bucket. This will trigger the ECS task that launches a container with a Python library to process the file:\n\naws s3 cp maze.jpg s3://$BUCKET_NAME/input/maze.jpg\n\nThis will trigger an ECS task to process the image file. Quickly, check the task with the ecs list-tasks command. The task will run for about two to",
      "content_length": 1262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "three minutes:\n\naws ecs list-tasks --cluster $ECS_CLUSTER_ARN\n\nYou should see output similar to the following:\n\n{\n\n\"taskArns\": [\n\n\"arn:aws:ecs:us-east-1:111111111111:task/cdk-aws-cookbook-607-\n\nAWSCookbookEcsCluster46494E6E-MX7kvtp1sYWZ/d86f16af55da56b5ca4874d6029\"\n\n]\n\n}\n\nValidation checks\n\nAfter a few minutes, observe the output directory created in the S3 bucket:\n\naws s3 ls s3://$BUCKET_NAME/output/\n\nDownload and view the output file:\n\naws s3 cp s3://$BUCKET_NAME/output/output.jpg ./output.jpg\n\nOpen output.jpg with a file viewer of your choice to view the file that was processed.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nEvent-driven architecture is an important approach to application and process design in the cloud. This type of design allows for removing long-running application workloads in favor of serverless architectures, which can be more resilient and easily scale to peaks of higher usage when needed. When there are no events to handle in your application, you generally do not pay much for compute resources (if at all), so potential cost savings is also a point to consider when choosing an application architecture.",
      "content_length": 1197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "NOTE\n\nIt is common to use Lambda functions with S3 for event-driven architectures, but for longer-running data-processing jobs\n\nand computational jobs like this one, Fargate is a better choice because the runtime is essentially infinite, while the\n\nmaximum runtime for Lambda functions is limited.\n\nAmazon ECS can run tasks and services. Services are made up of tasks, and generally, are long- running in that a service keeps a specific set of tasks running. Tasks can be short-lived; a container may start, process some data, and then gracefully exit after the task is complete. This is what you have achieved in this solution: a task was launched in response to an S3 event signaling a new object, and the container read the object, processed the file, and exited.\n\nChallenge\n\nWhile EventBridge is a powerful solution that can be used to orchestrate many types of event- driven solutions, you can achieve similar functionality with S3’s triggers. Try to deploy and configure a Lambda function to be invoked directly from S3 events. Here is a hint.\n\n6.8 Capturing Logs from Containers Running on Amazon ECS\n\nProblem\n\nYou have an application running in a container and want to inspect the application logs.\n\nSolution\n\nSend the logs from the container to Amazon CloudWatch. By specifying the awslogs driver within an ECS task definition and providing an IAM role that allows the container to write to CloudWatch logs, you are able to stream container logs to a location within Amazon CloudWatch. A high-level view of this configuration and process is shown in Figure 6-13.\n\nFigure 6-13. Streaming container logs to CloudWatch",
      "content_length": 1624,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "Preparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file called task-execution-assume-role.json with the following content. The file is provided in the root of this recipe’s directory in the code repository.\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"\",\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"ecs-tasks.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\n2.\n\nCreate an IAM role using the statement in the preceding file:\n\naws iam create-role --role-name AWSCookbook608ECS \\\n\n--assume-role-policy-document file://task-execution-assume-role.json\n\n3.\n\nAttach the AWS managed IAM policy for ECS task execution to the IAM role that you just created:\n\naws iam attach-role-policy --role-name AWSCookbook608ECS --policy-arn\n\narn:aws:iam::aws:policy/service-\n\nrole/AmazonECSTaskExecutionRolePolicy\n\n4.\n\nCreate a log group in CloudWatch:\n\naws logs create-log-group --log-group-name AWSCookbook608ECS",
      "content_length": 973,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "5.\n\nCreate a file called taskdef.json with the following content (a file is provided in this recipe’s directory in the code repository):\n\n{\n\n\"networkMode\": \"awsvpc\",\n\n\"containerDefinitions\": [\n\n{\n\n\"portMappings\": [\n\n{\n\n\"hostPort\": 80,\n\n\"containerPort\": 80,\n\n\"protocol\": \"tcp\"\n\n}\n\n],\n\n\"essential\": true,\n\n\"entryPoint\": [\n\n\"sh\",\n\n\"-c\"\n\n],\n\n\"logConfiguration\": {\n\n\"logDriver\": \"awslogs\",\n\n\"options\": {\n\n\"awslogs-group\": \"AWSCookbook608ECS\",\n\n\"awslogs-region\": \"us-east-1\",\n\n\"awslogs-stream-prefix\": \"LogStream\"\n\n}\n\n},\n\n\"name\": \"awscookbook608\",\n\n\"image\": \"httpd:2.4\",\n\n\"command\": [\n\n\"/bin/sh -c \\\"echo 'Hello AWS Cookbook Reader, this container is running on\n\nECS!' >\n\n/usr/local/apache2/htdocs/index.html && httpd-foreground\\\"\"\n\n]\n\n}\n\n],\n\n\"family\": \"awscookbook608\",\n\n\"requiresCompatibilities\": [\n\n\"FARGATE\"\n\n],\n\n\"cpu\": \"256\",\n\n\"memory\": \"512\"\n\n}",
      "content_length": 844,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "6.\n\nNow that you have an IAM role and an ECS task definition configuration, you need to create the ECS task using the configuration and associate the IAM role:\n\naws ecs register-task-definition --execution-role-arn \\\n\n\"arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook608ECS\" \\\n\n--cli-input-json file://taskdef.json\n\n7.\n\nRun the ECS task on the ECS cluster that you created earlier in this recipe with the AWS CDK:\n\naws ecs run-task --cluster $ECS_CLUSTER_NAME \\\n\n--launch-type FARGATE --network-configuration\n\n\"awsvpcConfiguration={subnets=[$VPC_PUBLIC_SUBNETS],securityGroups=\n\n[$VPC_DEFAULT_SECURITY_GROUP],assign\n\nPublicIp=ENABLED}\" --task-definition awscookbook608\n\nValidation checks\n\nCheck the status of the task to make sure the task is running. First, find the Task’s Amazon Resource Name (ARN):\n\nTASK_ARNS=$(aws ecs list-tasks --cluster $ECS_CLUSTER_NAME \\\n\n--output text --query taskArns)\n\nThen use the task ARNs to check for the RUNNING state with the describe-tasks command output:\n\naws ecs describe-tasks --cluster $ECS_CLUSTER_NAME --tasks $TASK_ARNS\n\nAfter the task has reached the RUNNING state (approximately 15 seconds), use the following commands to view logs:\n\naws logs describe-log-streams --log-group-name AWSCookbook608ECS",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "You should see output similar to the following:\n\n{\n\n\"logStreams\": [\n\n{\n\n\"logStreamName\": \"LogStream/webserver/97635dab942e48d1bab11dbe88c8e5c3\",\n\n\"creationTime\": 1605584764184,\n\n\"firstEventTimestamp\": 1605584765067,\n\n\"lastEventTimestamp\": 1605584765067,\n\n\"lastIngestionTime\": 1605584894363,\n\n\"uploadSequenceToken\": \"49612420096740389364147985468451499506623702081936625922\",\n\n\"arn\": \"arn:aws:logs:us-east-1:111111111111:log-group:AWSCookbook608ECS:log-\n\nstream:LogStream/webserver/97635dab942e48d1bab11dbe88c8e5c3\",\n\n\"storedBytes\": 0\n\n}\n\n]\n\n}\n\nNote the logStreamName from the output and then run the get-log-events command:\n\naws logs get-log-events --log-group-name AWSCookbook608ECS \\\n\n--log-stream-name <<logStreamName>>\n\nYou should see output similar to the following:\n\n{\n\n\"events\": [\n\n{\n\n\"timestamp\": 1605590555566,\n\n\"message\": \"[Tue Nov 17 05:22:35.566054 2020] [mpm_event:notice] [pid 7:tid 140297116308608] AH00489:\n\nApache/2.4.46 (Unix) configured -- resuming normal operations\",\n\n\"ingestionTime\": 1605590559713\n\n},\n\n{\n\n\"timestamp\": 1605590555566,\n\n\"message\": \"[Tue Nov 17 05:22:35.566213 2020] [core:notice] [pid 7:tid 140297116308608] AH00094:\n\nCommand line: 'httpd -D FOREGROUND'\",\n\n\"ingestionTime\": 1605590559713\n\n}\n\n],",
      "content_length": 1231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "\"nextForwardToken\": \"f/35805865872844590178623550035180924397996026459535048705\",\n\n\"nextBackwardToken\": \"b/35805865872844590178623550035180924397996026459535048704\"\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou made use of the awslogs driver and an IAM role, which allows the running task to write to a CloudWatch log group. This is a common pattern when working with containers on AWS, as you most likely need log output for troubleshooting and debugging your application. This configuration is handled by tools like Copilot since it is a common pattern, but when working with Amazon ECS directly, like defining and running a task, the configuration is critical for developers to know about.\n\nThe PID 1 process output to /dev/stdout and /dev/stderr is captured by the awslogs driver. In other words, the first process in the container is the only process logging to these streams. Be sure your application that you would like to see logs from is running with PID 1 inside of your container.\n\nIn order for most AWS services to communicate with one another, you must assign a role to them that allows the required level of permissions for the communication. This holds true when configuring logging to CloudWatch from a container ECS task; the container must have a role associated with it that allows the CloudWatchLogs operations via the awslogs logConfiguration driver:\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"logs:CreateLogGroup\",\n\n\"logs:CreateLogStream\",\n\n\"logs:PutLogEvents\",\n\n\"logs:DescribeLogStreams\"\n\n],\n\n\"Resource\": [\n\n\"arn:aws:logs:*:*:*\"\n\n]\n\n}\n\n]\n\n}",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "CloudWatch logs allow for a central logging solution for many AWS services. When running multiple containers, it is important to be able to quickly locate logs for debugging purposes.\n\nChallenge\n\nYou can tail the logs of a log stream to give you a more real-time view of the logs that your application is generating. This can help in your development and troubleshooting activities. Try using the aws logs tail command with your log stream while generating some output for you to observe.",
      "content_length": 488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "Chapter 7. Big Data\n\n7.0 Introduction Data is sometimes referred to as “the new gold.” Many companies are leveraging data in new and exciting ways every day as available data science tools continue to improve. You can now mine troves of historical data quickly for insights and patterns by using modern analytics tools. You might not yet know the queries and analysis you need to run against the data, but tomorrow you might be faced with a challenge that could be supported by historical data analysis using new and emerging techniques. With the advent of cheaper data storage, many organizations and individuals opt to keep data rather than discard it so that they can run historical analysis to gain business insights, discover trends, train AI/ML models, and be ready to implement future technologies that can use the data.\n\nIn addition to the amount of data you might collect over time, you are also collecting a wider variety of data types and structures at an increasingly faster velocity. Imagine that you might deploy IoT devices to collect sensor data, and as you continue to deploy these over time, you need a way to capture and store the data in a scalable way. This can be structured, semistructured, and unstructured data with schemas that might be difficult to predict as new data sources are ingested. You need tools to be able to transform and analyze your diverse data.\n\nAn informative and succinct AWS re:Invent 2020 presentation by Francis Jayakumar, “An Introduction to Data Lakes and Analytics on AWS,” provides a high-level introduction to what is available on AWS for big data and analytics. We could have included so much in this chapter— enough to fill another book—but we will focus on foundational recipes for sending data to S3, discovering data on S3, and transforming data to give you examples of working with data on AWS.\n\nWorkstation Configuration\n\nYou will need a few things installed to be ready for the recipes in this chapter.\n\nGeneral setup\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/awscookbook/BigData\n\n7.1 Using a Kinesis Stream for Ingestion of Streaming Data",
      "content_length": 2269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "Problem\n\nYou need a way to ingest streaming data for your applications.\n\nSolution\n\nCreate a Kinesis stream and verify that it is working by using the AWS CLI to put a record on the stream, as shown in Figure 7-1.\n\nFigure 7-1. Using a Kinesis stream for ingestion of streaming data\n\nSteps\n\n1.\n\nCreate a Kinesis stream:\n\naws kinesis create-stream --stream-name AWSCookbook701 --shard-count 1\n\nNOTE\n\nShards are an important concept to understand with Kinesis streams as you scale your stream to handle\n\nlarger velocities of incoming streaming data. Each shard can support up to five transactions per second\n\nfor reads, up to a maximum total data read rate of 2 MB per second. For writes, each shard can support\n\nup to 1,000 records per second, up to a maximum total data write rate of 1 MB per second (including\n\npartition keys). You can re-shard your stream at any time if you need to handle more data.\n\n2.\n\nConfirm that your stream is in ACTIVE state:\n\naws kinesis describe-stream-summary --stream-name AWSCookbook701",
      "content_length": 1016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "You should see output similar to the following:\n\n{\n\n\"StreamDescriptionSummary\": {\n\n\"StreamName\": \"AWSCookbook701\",\n\n\"StreamARN\": \"arn:aws:kinesis:us-east-1:111111111:stream/AWSCookbook701\",\n\n\"StreamStatus\": \"ACTIVE\",\n\n\"RetentionPeriodHours\": 24,\n\n\"StreamCreationTimestamp\": \"2021-10-12T17:12:06-04:00\",\n\n\"EnhancedMonitoring\": [\n\n{\n\n\"ShardLevelMetrics\": []\n\n}\n\n],\n\n\"EncryptionType\": \"NONE\",\n\n\"OpenShardCount\": 1,\n\n\"ConsumerCount\": 0\n\n}\n\n}\n\nValidation checks\n\nPut a record on the Kinesis stream:\n\naws kinesis put-record --stream-name AWSCookbook701 \\\n\n--partition-key 111 \\\n\n--cli-binary-format raw-in-base64-out \\\n\n--data={\\\"Data\\\":\\\"1\\\"}\n\nYou should see output similar to the following:\n\n{\n\n\"ShardId\": \"shardId-000000000000\",\n\n\"SequenceNumber\": \"49622914081337086513355510347442886426455090590105206786\"\n\n}\n\nGet the record from the Kinesis stream. Get the shard iterator and run the get-records command:",
      "content_length": 903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "SHARD_ITERATOR=$(aws kinesis get-shard-iterator \\\n\n--shard-id shardId-000000000000 \\\n\n--shard-iterator-type TRIM_HORIZON \\\n\n--stream-name AWSCookbook701 \\\n\n--query 'ShardIterator' \\\n\n--output text)\n\naws kinesis get-records --shard-iterator $SHARD_ITERATOR \\\n\n--query Records[0].Data --output text | base64 --decode\n\nYou should see output similar to the following:\n\n{\"Data\":\"1\"}\n\nNOTE\n\nThe data is base64 encoded that you published to the stream. You queried the command output for the data element within\n\nthe JSON object and piped the output to base64 --decode to validate that the record is what you published.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nStreaming data can come from a variety of sources. The sources putting records on streams are known as producers. Entities getting records from streams are known as consumers. In the case of streaming data, you are dealing with real-time information, and you may need to act on it immediately or store it for usage later (see Recipe 7.2). Some common producer examples to think about are as follows:\n\nReal-time financial market data\n\nIoT and sensor data\n\nEnd-user clickstream activity from web and mobile applications\n\nYou can use the Kinesis Producer Library (KPL) and Kinesis Client Library (KCL) within your application for your specific needs. When consuming data from Kinesis streams, you can configure your application to read records from the stream and respond, invoke Lambda functions directly from the stream, or even use a Kinesis Data Analytics application (powered by Apache Flink) directly within the Kinesis service.",
      "content_length": 1640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "NOTE\n\nThe Kinesis service will scale automatically to meet your needs, but you need to be aware of the quotas and limits to\n\nensure that you do not exceed capacity with your shard configuration.\n\nChallenge\n\nAutomatically trigger a Lambda function to process incoming Kinesis data.\n\n7.2 Streaming Data to Amazon S3 Using Amazon Kinesis Data Firehose\n\nProblem\n\nYou need to deliver incoming streaming data to object storage.\n\nSolution\n\nCreate an S3 bucket, create a Kinesis stream, and configure Kinesis Data Firehose to deliver the stream data to the S3 bucket. The flow is shown in Figure 7-2.\n\nFigure 7-2. Streaming data to Amazon S3 using Amazon Kinesis Data Firehose\n\nPrerequisites\n\nKinesis stream\n\nS3 Bucket with a CSV file\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.",
      "content_length": 813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "Steps\n\n1.\n\n2.\n\nOpen the Kinesis Data Firehose console and click the “Create delivery stream” button; choose Amazon Kinesis Data Streams for the source and Amazon S3 for the destination, as shown in Figure 7-3.\n\nFigure 7-3. Choosing the source and destination in the Kinesis Data Firehose dialog\n\nFor Source settings, choose the Kinesis stream that you created in the preparation steps, as shown in Figure 7-4.",
      "content_length": 409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "3.\n\n4.\n\nFigure 7-4. Choosing a Kinesis data stream\n\nKeep the defaults (Disabled) for “Transform and convert records” options. For Destination settings, browse for and choose the S3 bucket that you created in the preparation steps as shown in Figure 7-5, and keep the defaults for the other options (disabled partitioning and no prefixes).\n\nFigure 7-5. Kinesis Data Firehose destination configuration\n\nUnder the Advanced settings section, confirm that the “Create or update IAM role” is selected. This will create an IAM role that Kinesis can use to",
      "content_length": 548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "access the stream and S3 bucket, as shown in Figure 7-6.\n\nFigure 7-6. Creating an IAM role for the Kinesis Data Firehose service\n\nValidation checks\n\nYou can test delivery to the stream from within the Kinesis console. Click the Delivery streams link in the left navigation menu, choose the stream you created, expand the “Test with demo data” section, and click the “Start sending demo data” button. This will initiate sending sample data to your stream so you can verify that it is making it to your S3 bucket. A sample is shown in Figure 7-7.",
      "content_length": 544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "Figure 7-7. Sending test data through a Kinesis Data Firehose\n\nAfter a few minutes, you will see a folder structure and a file appear in your S3 bucket, similar to Figure 7-8.",
      "content_length": 175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "Figure 7-8. S3 destination with Kinesis stream data delivered\n\nIf you download and inspect the file, you will see output similar to the following:\n\n{\"CHANGE\":3.95,\"PRICE\":79.75,\"TICKER_SYMBOL\":\"SLW\",\"SECTOR\":\"ENERGY\"}\n\n{\"CHANGE\":7.27,\"PRICE\":96.37,\"TICKER_SYMBOL\":\"ALY\",\"SECTOR\":\"ENERGY\"}\n\n{\"CHANGE\":-5,\"PRICE\":81.74,\"TICKER_SYMBOL\":\"QXZ\",\"SECTOR\":\"HEALTHCARE\"}\n\n{\"CHANGE\":-0.6,\"PRICE\":98.4,\"TICKER_SYMBOL\":\"NFLX\",\"SECTOR\":\"TECHNOLOGY\"}",
      "content_length": 436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "{\"CHANGE\":-0.46,\"PRICE\":18.92,\"TICKER_SYMBOL\":\"PLM\",\"SECTOR\":\"FINANCIAL\"}\n\n{\"CHANGE\":4.09,\"PRICE\":100.46,\"TICKER_SYMBOL\":\"ALY\",\"SECTOR\":\"ENERGY\"}\n\n{\"CHANGE\":2.06,\"PRICE\":32.34,\"TICKER_SYMBOL\":\"PPL\",\"SECTOR\":\"HEALTHCARE\"}\n\n{\"CHANGE\":-2.99,\"PRICE\":38.98,\"TICKER_SYMBOL\":\"KFU\",\"SECTOR\":\"ENERGY\"}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAs you begin to ingest data from various sources, your application may be consuming or reacting to streaming data in real time. In some cases, you may want to store the data from the stream to query or process it later. You can use Kinesis Data Firehose to deliver data to object storage (S3), Amazon Redshift, OpenSearch, and many third-party endpoints. You can also connect multiple delivery streams to a single producer stream to deliver data if you have to support multiple delivery locations from your streams.\n\nNOTE\n\nKinesis Data Firehose scales automatically to handle the volume of data you need to deliver, meaning that you do not have\n\nto configure or provision additional resources if your data stream starts to receive large volumes of data. For more\n\ninformation on Kinesis Data Firehose features and capabilities, see the AWS documentation for Kinesis Data Firehose.\n\nIf you need to transform data before it ends up in the destination via Firehose, you can configure transformations. A transformation will automatically invoke a Lambda function as your streaming data queues up (see this Firehose article for buffer size information). This is useful when you have to adjust the schema of a record before delivery, sanitize data for long-term storage on the fly (e.g., remove personally identifiable information), or join the data with other sources before delivery. The transformation Lambda function you invoke must follow the convention specified by the Kinesis Data Firehose API. To see some examples of Lambda functions, go to the AWS Serverless Application Repository and search for “firehose.”\n\nChallenge\n\nConfigure a Firehose delivery with transformations to remove a field from the streaming data before delivery.\n\n7.3 Automatically Discovering Metadata with AWS Glue Crawlers\n\nProblem",
      "content_length": 2196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "You have CSV data files on object storage, and you would like to discover the schema and metadata about the files to use in further analysis and query operations.\n\nSolution\n\nCreate an AWS Glue database, follow the crawler configuration wizard to configure a crawler to scan your S3 bucket data, run the crawler, and inspect the resulting table, as shown in Figure 7-9.\n\nFigure 7-9. Automatically discover metadata with AWS Glue crawlers\n\nPrerequisite\n\nAn S3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nLog in to the AWS Console and navigate to the AWS Glue console, choose Databases from the left navigation menu and select “Add database,” as shown in Figure 7-10.",
      "content_length": 726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "2.\n\nFigure 7-10. Creating a database in the Glue data catalog\n\nGive your database a name (e.g., awscookbook703) and click Create. A sample dialog box is shown in Figure 7-11.",
      "content_length": 174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "3.\n\nFigure 7-11. Database name dialog box\n\nSelect Tables from the left navigation menu and choose “Add tables” → “Add tables using a crawler,” as shown in Figure 7-12.",
      "content_length": 167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "4.\n\nFigure 7-12. Adding a table to the Glue Data catalog\n\nFollow the “Add crawler” wizard. For the crawler source type, choose “Data stores,” crawl all folders, S3 as “Data store” (do not define a Connection), choose your S3 bucket and “data” folder in “Include path,” and do not choose a sample size. Choose to create an IAM role, and suffix it with AWSCookbook703. For a frequency, choose “Run on demand,” and select the database you created in step 2. Confirm the configuration on the “Review all steps” page and click Finish. An example review page is shown in Figure 7- 13.",
      "content_length": 578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "5.\n\nFigure 7-13. Review settings for Glue crawler\n\nFrom the left navigation menu, select Crawlers. Choose the crawler that you created in step 4 and click “Run crawler,” as shown in Figure 7-14.\n\nNOTE\n\nThe Glue crawler will take a few moments to run. Once it is complete, you can view the table\n\nproperties of the discovered schema and metadata.",
      "content_length": 345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "Figure 7-14. Glue crawler configuration summary\n\nValidation checks\n\nVerify the crawler configuration you created; you can use the AWS CLI or the Glue console. Note the LastCrawl status of SUCCEEDED:",
      "content_length": 198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "aws glue get-crawler --name awscookbook703\n\nYou should see output similar to this:\n\n{\n\n\"Crawler\": {\n\n\"Name\": \"awscookbook703\",\n\n\"Role\": \"service-role/AWSGlueServiceRole-AWSCookbook703\",\n\n\"Targets\": {\n\n\"S3Targets\": [\n\n{\n\n\"Path\": \"s3://awscookbook704-<RANDOM_STRING>/data\",\n\n\"Exclusions\": []\n\n}\n\n],\n\n\"JdbcTargets\": [],\n\n\"MongoDBTargets\": [],\n\n\"DynamoDBTargets\": [],\n\n\"CatalogTargets\": []\n\n},\n\n\"DatabaseName\": \"awscookbook703\",\n\n\"Classifiers\": [],\n\n\"RecrawlPolicy\": {\n\n\"RecrawlBehavior\": \"CRAWL_EVERYTHING\"\n\n},\n\n\"SchemaChangePolicy\": {\n\n\"UpdateBehavior\": \"UPDATE_IN_DATABASE\",\n\n\"DeleteBehavior\": \"DEPRECATE_IN_DATABASE\"\n\n},\n\n\"LineageConfiguration\": {\n\n\"CrawlerLineageSettings\": \"DISABLE\"\n\n},\n\n\"State\": \"READY\",\n\n\"CrawlElapsedTime\": 0,\n\n\"CreationTime\": \"2021-10-12T12:45:18-04:00\",\n\n\"LastUpdated\": \"2021-10-12T12:45:18-04:00\",\n\n\"LastCrawl\": {\n\n\"Status\": \"SUCCEEDED\",\n\n\"LogGroup\": \"/aws-glue/crawlers\",\n\n\"LogStream\": \"awscookbook703\",\n\n\"MessagePrefix\": \"16e867b7-e972-4ceb-b318-8e78370949d8\",\n\n\"StartTime\": \"2021-10-12T12:54:19-04:00\"\n\n},\n\n\"Version\": 1",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "}\n\n}\n\nTIP\n\nAWS Glue crawlers log information about their runs to Amazon CloudWatch logs. If you need to debug your crawlers’\n\nactivity, you can inspect the logs in the /aws-glue/crawlers log group.\n\nIn the Glue console, select the table that was created and click “View properties.” You can also run an AWS CLI command to output the JSON:\n\naws glue get-table --database-name awscookbook703 --name data\n\nThe JSON properties should look similar to this:\n\n{\n\n\"StorageDescriptor\": {\n\n\"cols\": {\n\n\"FieldSchema\": [\n\n{\n\n\"name\": \"title\",\n\n\"type\": \"string\",\n\n\"comment\": \"\"\n\n},\n\n{\n\n\"name\": \"other titles\",\n\n\"type\": \"string\",\n\n\"comment\": \"\"\n\n},\n\n{\n\n\"name\": \"bl record id\",\n\n\"type\": \"bigint\",\n\n\"comment\": \"\"\n\n}...<SNIP>...\n\n]\n\n},\n\n\"location\": \"s3://awscookbook703-<RANDOM_STRING>/data/\",\n\n\"inputFormat\": \"org.apache.hadoop.mapred.TextInputFormat\",\n\n\"outputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\n\n\"compressed\": \"false\",",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "\"numBuckets\": \"-1\",\n\n\"SerDeInfo\": {\n\n\"name\": \"\",\n\n\"serializationLib\": \"org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\",\n\n\"parameters\": {\n\n\"field.delim\": \",\"\n\n}\n\n},\n\n\"bucketCols\": [],\n\n\"sortCols\": [],\n\n\"parameters\": {\n\n\"skip.header.line.count\": \"1\",\n\n\"sizeKey\": \"43017100\",\n\n\"objectCount\": \"1\",\n\n\"UPDATED_BY_CRAWLER\": \"awscookbook703\",\n\n\"CrawlerSchemaSerializerVersion\": \"1.0\",\n\n\"recordCount\": \"79367\",\n\n\"averageRecordSize\": \"542\",\n\n\"CrawlerSchemaDeserializerVersion\": \"1.0\",\n\n\"compressionType\": \"none\",\n\n\"classification\": \"csv\",\n\n\"columnsOrdered\": \"true\",\n\n\"areColumnsQuoted\": \"false\",\n\n\"delimiter\": \",\",\n\n\"typeOfData\": \"file\"\n\n},\n\n\"SkewedInfo\": {},\n\n\"storedAsSubDirectories\": \"false\"\n\n},\n\n\"parameters\": {\n\n\"skip.header.line.count\": \"1\",\n\n\"sizeKey\": \"43017100\",\n\n\"objectCount\": \"1\",\n\n\"UPDATED_BY_CRAWLER\": \"awscookbook703\",\n\n\"CrawlerSchemaSerializerVersion\": \"1.0\",\n\n\"recordCount\": \"79367\",\n\n\"averageRecordSize\": \"542\",\n\n\"CrawlerSchemaDeserializerVersion\": \"1.0\",\n\n\"compressionType\": \"none\",\n\n\"classification\": \"csv\",\n\n\"columnsOrdered\": \"true\",\n\n\"areColumnsQuoted\": \"false\",\n\n\"delimiter\": \",\",\n\n\"typeOfData\": \"file\"\n\n}\n\n}",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen you start to ingest and store large amounts of data from various sources to object storage like S3, you may want to temporarily query the data in place without loading into an intermediate database. Since you may not always know the schema or metadata of the data, you need to know some basics about it, such as where the data resides, what the files and partitioning look like, whether it’s structured versus unstructured data, the size of data, and most importantly, the schema of the data. One specific feature of the AWS Glue service, Glue crawlers, allow you to discover metadata about the variety of data you have in storage. Crawlers connect to your data source (in this case, S3 bucket), scan the objects in the source, and populate a Glue Data Catalog database with tables associated with your data’s schema.\n\nNOTE\n\nIn addition to S3 bucket sources, you can use crawlers to scan Java Database Connectivity (JDBC) data stores and\n\nDynamoDB tables. In the case of a JDBC table, you will need to define a connection to allow Glue to use a network\n\nconnection to your JDBC source.\n\nChallenge\n\nConfigure your crawler to run on an interval so that your tables and metadata are automatically updated.\n\n7.4 Querying Files on S3 Using Amazon Athena\n\nProblem\n\nYou need to run a SQL query on CSV files stored on object storage without indexing them.\n\nSolution\n\nConfigure an Amazon Athena results S3 bucket location, create a Data Catalog database and table in the Athena Editor, and run a SQL query on the data in the S3 bucket, as shown in Figure 7-15.",
      "content_length": 1651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "Figure 7-15. Query files on S3 using Amazon Athena\n\nPrerequisite\n\nS3 bucket with a CSV file containing data\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nLog into the AWS Console and go to the Athena console; you should see something similar to Figure 7-16.\n\nFigure 7-16. Athena console\n\n2.\n\nIn the Query Editor, click the Settings tab and configure a Query result location using the S3 bucket that you created and a prefix: s3://bucket/folder/object/. Click Manage, select the bucket, and click Choose. As an option, you can encrypt the results. See Figure 7-17 for an example.",
      "content_length": 631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "3.\n\nFigure 7-17. Athena results destination configuration\n\nBack in the Editor tab, run the following SQL statement to create a Data Catalog database:\n\nCREATE DATABASE `awscookbook704db`\n\nYou should see output similar to Figure 7-18.",
      "content_length": 232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "4.\n\nFigure 7-18. Database creation SQL statement\n\nRun a new statement in the Query Editor to create a table within the database that references the S3 bucket location of the data and the schema of the data. Be sure to replace BUCKET_NAME with the name of the bucket that you created:\n\nCREATE EXTERNAL TABLE IF NOT EXISTS default.`awscookbook704table`(\n\n`title` string,\n\n`other titles` string,\n\n`bl record id` bigint,\n\n`type of resource` string,\n\n`content type` string,\n\n`material type` string,\n\n`bnb number` string,\n\n`isbn` string,\n\n`name` string,\n\n`dates associated with name` string,\n\n`type of name` string,\n\n`role` string,\n\n`all names` string,\n\n`series title` string,\n\n`number within series` string,\n\n`country of publication` string,",
      "content_length": 736,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "`place of publication` string,\n\n`publisher` string,\n\n`date of publication` string,\n\n`edition` string,\n\n`physical description` string,\n\n`dewey classification` string,\n\n`bl shelfmark` string,\n\n`topics` string,\n\n`genre` string,\n\n`languages` string,\n\n`notes` string)\n\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n\nWITH SERDEPROPERTIES (\n\n'serialization.format' = ',',\n\n'field.delim' = ','\n\n) LOCATION 's3://BUCKET_NAME/data/'\n\nTBLPROPERTIES ('has_encrypted_data'='false');\n\nTIP\n\nYou can use an AWS Glue crawler to crawl your data on S3 and keep databases, tables, and metadata\n\nup-to-date automatically. See Recipe 7.3 for an example of configuring a Glue crawler with this\n\nexample dataset.\n\nValidation checks\n\nOpen the Query Editor and run a query to list the rows where the title is “Marvel universe”:\n\nSELECT * FROM awscookbook704table WHERE title='\"Marvel universe\"' LIMIT 100\n\nYou should see output similar to Figure 7-19.",
      "content_length": 949,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "Figure 7-19. Query results in the Athena console\n\nRun a SQL query selecting the top 100 rows for the sample dataset:\n\nSELECT * FROM awscookbook704table LIMIT 100;\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou may have had to use an extract, transform, load (ETL) process in the past to facilitate querying large amounts of data on demand. This process could have taken hours, if not days, if the volume of data was substantial. In the world of big data, the volume will only increase as time moves on, and you may not know what you need to query for when you initially start to ingest the data. This makes table schema design challenging. Chances are, as you ingest and collect data from various sources, you will be storing it in object storage. This is the data lake",
      "content_length": 822,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "concept realized. Being able to query text-based data in place within your data lake is a powerful concept that you can use Amazon Athena for. You can use standard SQL to run queries against data directly on object storage (Amazon S3).\n\nAthena requires knowing some basics about your data before you can run queries. This includes metadata, schema, and data locations. The concepts of tables, databases, and the Data Catalog are important to understand so that you can configure the Athena service to meet your needs. As you saw in Recipe 7.3, the Glue service can crawl your data to discover this information and keep it up-to-date so that you can always be ready to run queries against your data while it is stored in your data lake. You can use a Glue Data Catalog in the Athena service rather than defining your own schema to save you time.\n\nTIP\n\nAs you start to increase your usage of S3 as your central data repository (or in other words, a data lake), you may need to\n\nuse several services together and also start to apply certain levels of permission to allow other team members to interact\n\nwith and manage the data. AWS Lake Formation is a managed service that brings services like S3, Glue, and Athena\n\ntogether with a robust permissions engine that can meet your needs.\n\nNOTE\n\nThe Athena service will automatically scale for you, running queries in parallel for your large datasets, so you do not have\n\nto worry about provisioning any resources. For more information, see the Athena documentation.\n\nChallenge\n\nConfigure Athena to use the Glue Data Catalog with a Glue crawler for a source dataset on S3 that you do not have a predefined schema for.",
      "content_length": 1660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "7.5 Transforming Data with AWS Glue DataBrew\n\nProblem\n\nYou have data stored in a CSV and need to convert all characters in a column to uppercase before further processing can occur.\n\nSolution\n\nStart with a sample project in Glue DataBrew using a sample CSV dataset. Wait for the session to initiate and apply an uppercase format operation on the name column of the sample set. Inspect the results (see Figure 7-20).\n\nFigure 7-20. Transforming data with AWS Glue DataBrew\n\nSteps",
      "content_length": 477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "1.\n\n2.\n\nIn the AWS Console, search for and navigate to the AWS Glue DataBrew console.\n\nClick “Create sample project,” select “Popular names for babies in 2020,” create a new IAM role, enter a role suffix of your choosing, and then click “Create project,” as shown in Figure 7-21. Your session will take a few moments to be prepared, and the status indicator should look similar to Figure 7-22.",
      "content_length": 393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "3.\n\nFigure 7-21. Creating a sample project in Glue DataBrew\n\nFigure 7-22. Preparing a session in Glue DataBrew\n\nWhen your session has been prepared, click FORMAT in the menu. Then from the drop-down menu, select “Change to uppercase,” as shown in Figure 7-23.",
      "content_length": 259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "4.\n\nFigure 7-23. Beginning to format a string to uppercase\n\nIn the righthand menu, set the “Source column” to “name” and then click Apply, as shown in Figure 7-24.",
      "content_length": 163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "Figure 7-24. Formatting column to uppercase interface\n\nValidation checks\n\nView the updated name column, as shown in Figure 7-25.\n\nFigure 7-25. Results of the uppercase recipe step\n\nFrom the ACTIONS menu in the top-right corner, select Download CSV, as shown in Figure 7- 26.",
      "content_length": 274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "Figure 7-26. Downloading CSV action\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nWhen dealing with large amounts of heterogeneous data from various sources, you will find that you need to manipulate data in various ways to meet your use case. In the past, you would have to programmatically accomplish this using scripts, complicated ETL jobs, or third-party tools. You can use AWS DataBrew to streamline the tasks of formatting, cleaning, and extracting information from your data. You can perform complex joins and splits, and run custom functions within the DataBrew service using a visual interface.\n\nDataBrew is an example of a low-code development platform (LCDP) that enables you to quickly achieve outcomes that otherwise would require knowledge of more complicated development platforms. You use Glue DataBrew in your web browser to visually design recipe jobs to process your data, preview results, and automate your data-processing workflow. Once you have a recipe job saved, you can bring automation into your workflow by setting up an S3 trigger for your incoming data to have DataBrew process it and deliver it back to S3, the Glue Data Catalog, or a JDBC data source.",
      "content_length": 1233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "Challenge\n\nUpload your own dataset. Create a job from the DataBrew console, configure it to deliver the results to S3 so you can use the job on demand or automatically with a trigger.",
      "content_length": 183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "Chapter 8. AI/ML\n\n8.0 Introduction Machine learning (ML) and artificial intelligence (AI) are two of the hottest topics today. Scale provided by cloud computing and improved algorithms have enabled rapid advances in the abilities of computers to think like humans (aka “provide inferences”). Many mundane and boring tasks that required human intervention can now be automated because of AI.\n\nAI and ML can get complex very quickly. Volumes of text have been written about each. Recipes in this chapter will allow you to explore some of the easy-to-implement AI services provided by AWS and get started building your own models. While you are working through the recipes, try to think about other problematic areas in society that could be well served by these technologies. From supply chain predictive maintenance to song suggestions, the opportunities are endless.\n\nWe could have written 100 pages on this topic, but these recipes are great to get started, and you can iterate from there. If you are looking to dive deeper, we suggest you check out Data Science on AWS by Chris Fregly and Antje Barth (O’Reilly, 2021).\n\nWorkstation Configuration\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/ArtificialIntelligence",
      "content_length": 1393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "8.1 Transcribing a Podcast\n\nProblem\n\nYou need to create a text transcription of an MP3-based audio, such as a podcast.\n\nSolution\n\nUse Amazon Transcribe to generate an English language transcription and save the results to an S3 bucket (see Figure 8-1).\n\nFigure 8-1. Using Amazon Transcribe with an MP3 file\n\nPrerequisites\n\nS3 bucket\n\njq CLI JSON processor\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.",
      "content_length": 442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "Steps\n\n1.\n\n2.\n\n3.\n\nDownload a podcast in MP3 format to upload to the S3 bucket:\n\ncurl https://d1le29qyzha1u4.cloudfront.net/AWS_Podcast_Episode_453.mp3 \\\n\no podcast.mp3\n\nYou should see output similar to the following:\n\n% Total % Received % Xferd Average Speed Time Time Time Current\n\nDload Upload Total Spent Left Speed\n\n100 29.8M 100 29.8M 0 0 4613k 0 0:00:06 0:00:06 --:--:-- 5003k\n\nNOTE\n\nYou can find a list of the file formats supported by Amazon Transcribe in the documentation.\n\nCopy the downloaded podcast to your S3 bucket:\n\naws s3 cp ./podcast.mp3 s3://awscookbook801-$RANDOM_STRING\n\nYou should see output similar to the following:\n\nupload: ./podcast.mp3 to s3://awscookbook801-<<unique>>/podcast.mp3\n\nStart a Transcribe transcription job using the AWS CLI:\n\naws transcribe start-transcription-job \\\n\n--language-code 'en-US' \\\n\n--media-format 'mp3' \\\n\n--transcription-job-name 'awscookbook-801' \\\n\n--media MediaFileUri=s3://awscookbook801-${RANDOM_STRING}/podcast.mp3 \\\n\n--output-bucket-name \"awscookbook801-${RANDOM_STRING}\"",
      "content_length": 1034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "You should see output similar to the following:\n\n{\n\n\"TranscriptionJob\": {\n\n\"TranscriptionJobName\": \"awscookbook-801\",\n\n\"TranscriptionJobStatus\": \"IN_PROGRESS\",\n\n\"LanguageCode\": \"en-US\",\n\n\"MediaFormat\": \"mp3\",\n\n\"Media\": {\n\n\"MediaFileUri\": \"s3://awscookbook801-<<unique>>/podcast.mp3\"\n\n},\n\n\"StartTime\": \"2021-09-21T22:02:13.312000-04:00\",\n\n\"CreationTime\": \"2021-09-21T22:02:13.273000-04:00\"\n\n}\n\n}\n\nCheck the status of the transcription job using the AWS CLI. Wait until the Transcription Job Status is COMPLETED. This should take a few minutes:\n\naws transcribe get-transcription-job \\\n\n--transcription-job-name awscookbook-801 \\\n\n--output text \\\n\n--query TranscriptionJob.TranscriptionJobStatus\n\nValidation checks\n\nDisplay the results of the Transcribe transcription job in your terminal:\n\naws s3 cp s3://awscookbook801-$RANDOM_STRING/awscookbook-801.json - \\\n\n| jq '.results.transcripts[0].transcript' --raw-output\n\nYou should see output similar to the following:\n\nThis is episode 453 of the US podcast released on June 11, 2021 podcast confirmed. Welcome to the\n\nofficial AWS podcast. Yeah. Mhm. Hello everyone and welcome back to another episode of a W. S. Launch.\n\nI'm Nicky, I'm your host. And today I am joined by Nathan Peck\n\n...",
      "content_length": 1234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nTranscribing audio is a great way to allow voice recordings to be processed and analyzed in different ways at scale. Having the ability to easily transcribe audio files without gathering a large dataset and training your own AI model will save you time and open up many possibilities. Converting audio to text also allows you to easily perform natural language processing (NLP) to gain additional insights from the piece of media.\n\nHere are some things to keep in mind when you are using Transcribe:\n\nIs the language supported?\n\nDo you need to identify speakers in a recording?\n\nDo you need to support streaming audio?\n\nChallenge\n\nAutomate the process to trigger when new objects are uploaded to your S3 bucket by using EventBridge.",
      "content_length": 827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "8.2 Converting Text to Speech\n\nProblem\n\nYou need to generate audio files from text descriptions of products. This audio will be incorporated in advertisements so must be as human-like and high quality as possible.\n\nSolution\n\nYou will use the neural engine of Amazon Polly to generate MP3s from the provided text (see Figure 8-2).\n\nFigure 8-2. Text to speech with Amazon Polly\n\nSteps\n\n1.\n\nCreate an initial sound file of text that you specify:\n\naws polly synthesize-speech \\\n\n--output-format mp3 \\\n\n--voice-id Joanna \\\n\n--text 'Acme products are of the very highest quality and lowest price.' \\\n\nproducts.mp3\n\nYou should see output similar to the following:\n\n{\n\n\"ContentType\": \"audio/mpeg\",",
      "content_length": 689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "2.\n\n3.\n\n4.\n\n5.\n\n6.\n\n\"RequestCharacters\": \"63\"\n\n}\n\nListen to the MP3. Here is an example on macOS CLI:\n\nafplay products.mp3\n\nChange to the neural engine:\n\naws polly synthesize-speech \\\n\n--output-format mp3 \\\n\n--voice-id Joanna \\\n\n--engine neural \\\n\n--text 'Acme products are of the very highest quality and lowest price.' \\\n\nproducts-neural.mp3\n\nYou will see similar output as you did in step 2.\n\nListen to the MP3. Here is an example on macOS CLI:\n\nafplay products-neural.mp3\n\nAdd some SSML tags to modify the speech speed:\n\naws polly synthesize-speech \\\n\n--output-format mp3 \\\n\n--voice-id Joanna \\\n\n--engine neural \\\n\n--text-type ssml \\\n\n--text '<speak>Acme products are of the very highest quality and <prosody\n\nrate=\"slow\">lowest price</prosody></speak>' \\\n\nproducts-neural-ssml.mp3\n\nYou will see similar output as you did in steps 2 and 3.\n\nListen to the MP3. Here is an example on macOS CLI:",
      "content_length": 896,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "afplay products-neural-ssml.mp3\n\nNOTE\n\nA list of SSML tags supported by Polly is shown in this article.\n\nDiscussion\n\nHaving the ability to easily generate lifelike audio from text allows for many creative uses. You no longer need voice actors to sit and rehearse recordings. You can now easily incorporate audio into your application. This can improve your customers’ experience in many ways.\n\nWhen creating audio with Polly, you should experiment with different voices and SSML tags (you might even want to create your own voice). Many of Polly’s voices are available to create Amazon Alexa skills.\n\nChallenge\n\nCreate a pronunciation lexicon for Polly to use.",
      "content_length": 660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "8.3 Computer Vision Analysis of Form Data\n\nProblem\n\nYou have a document and need to extract responses from it so that you can digitally process them.\n\nSolution\n\nYou will install and use the Textractor tool provided by AWS to utilize the forms feature of Amazon Textract. This will pull the values from the form and associate them with their keys (e.g., Name). See Figure 8-3.\n\nFigure 8-3. Analyzing a document with Amazon Textract and the textractor tool\n\nPrerequisite\n\nS3 bucket",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "Preparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nIn the root of the Chapter 8 repository, cd to the 803-Computer-Vision- Analysis-of-Handwritten-Form-Data/ directory and follow the subsequent steps:\n\ncd 803-Computer-Vision-Analysis-of-Handwritten-Form-Data/\n\n2.\n\nCopy the provided registration_form.png file (or your own) to the S3 bucket you created:\n\naws s3 cp ./registration_form.png s3://awscookbook803-$RANDOM_STRING\n\nYou should see output similar to the following:\n\nupload: ./registration_form.png to s3://awscookbook803-\n\n<<unique>>/registration_form.png\n\n3.\n\nAnalyze the document with Textract and redirect the output to a file:\n\naws textract analyze-document \\\n\n--document '{\"S3Object\":{\"Bucket\":\"'\"awscookbook803-${RANDOM_STRING}\"'\",\n\n\"Name\":\"registration_form.png\"}}' \\\n\n--feature-types '[\"FORMS\"]' > output.json\n\n4.\n\nGet the Textractor tool from the aws-samples repository on GitHub:\n\nwget https://github.com/aws-samples/amazon-textract-\n\ntextractor/blob/master/zip/textractor.zip?raw=true -O textractor.zip",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "5.\n\n6.\n\n7.\n\n8.\n\n9.\n\nTIP\n\nMany great AWS samples and tools are available on GitHub. It is a great place to check often for new\n\nideas and approaches.\n\nUnzip the archive:\n\nunzip textractor.zip\n\nCreate a Python virtual environment (if you don’t already have one created):\n\ntest -d .venv || python3 -m venv .venv\n\nActivate the newly created Python virtual environment:\n\nsource .venv/bin/activate\n\nInstall the required Python modules for Textractor:\n\npip install -r requirements.txt\n\nYou should see output similar to the following:\n\nCollecting tabulate\n\nDownloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n\nInstalling collected packages: tabulate\n\nSuccessfully installed tabulate-0.8.9\n\nInstall the boto3 module:\n\npip install boto3",
      "content_length": 726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "10.\n\nUse the tool to analyze the registration form and parse the output:\n\npython textractor.py \\\n\n--documents s3://awscookbook803-${RANDOM_STRING}/registration_form.png \\\n\n--text --forms\n\nYou should see output similar to the following:\n\n************************************************************\n\nTotal input documents: 1\n\n************************************************************\n\nTextracting Document # 1: registration_form.png\n\n===================================================\n\nCalling Textract...\n\nReceived Textract response...\n\nGenerating output...\n\nTotal Pages in Document: 1\n\nregistration_form.png textracted successfully.\n\n************************************************************\n\nSuccessfully textracted documents: 1\n\n************************************************************\n\nValidation checks\n\nCheck out the form data extracted and confidence values:\n\ncat registration_form-png-page-1-forms.csv | column -t -s,\n\nYou should see output similar to the following:\n\nKey KeyConfidence Value ValueConfidence\n\nAverage Score 97.0 285 97.0\n\nName: 96.5 Elwood Blues 96.5\n\nDate 94.5 2/9/2021 94.5\n\nTeam Name: 92.0 The Blues Brothers 92.0",
      "content_length": 1151,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "Years of Experience 91.5 10 91.5\n\nE-mail Address: 91.0 thebluesbrothers@theawscookbook.com 91.0\n\nSignature 90.5 Elwood Blues 90.5\n\nDate 89.0 2/9/2021 89.0\n\nNumber of Team Members 81.0 2 81.0\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nAutomated document processing opens up many new avenues for innovation and efficiency for organizations. Instead of having to manually interpret fields and tables in a document, tools like Textract can be used to digitize and speed up the process. Now that you are able to associate data values and fields with each other, additional processing is able to happen effectively.\n\nChallenge\n\nPrint out the blank form provided in the repo. Fill in responses by hand. Take a photo/scan of the document and analyze it with Textract.",
      "content_length": 812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "8.4 Redacting PII from Text Using Comprehend\n\nProblem\n\nYou have a document with potential personally identifiable information (PII) in it. You would like to remove the PII before more processing of the document occurs.\n\nSolution\n\nCreate sample data and store it in an S3 bucket. Launch an Amazon Comprehend job to detect and redact PII entities. Finally, view the results (see Figure 8-4).\n\nFigure 8-4. Redacting PII data from a document with Amazon Comprehend\n\nPrerequisite\n\nS3 bucket with file for analysis and path for output\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file named assume-role-policy.json with the following content (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [",
      "content_length": 778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "2.\n\n3.\n\n4.\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"comprehend.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}\n\nCreate a role for the Comprehend job to use to read and write data from S3:\n\naws iam create-role --role-name AWSCookbook804Comprehend \\\n\n--assume-role-policy-document file://assume-role-policy.json\n\nYou should see output similar to the following:\n\n{\n\n\"Role\": {\n\n\"Path\": \"/\",\n\n\"RoleName\": \"AWSCookbook804Comprehend\",\n\n\"RoleId\": \"<<RoldID>>\",\n\n\"Arn\": \"arn:aws:iam::111111111111:role/AWSCookbook804Comprehend\",\n\n\"CreateDate\": \"2021-09-22T13:12:22+00:00\",\n\n\"AssumeRolePolicyDocument\": {\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n...\n\nAttach the IAM managed policy for AmazonS3FullAccess to the IAM role:\n\naws iam attach-role-policy --role-name AWSCookbook804Comprehend \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n\nCreate some sample PII data using Faker (or by hand):",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "5.\n\n6.\n\n7.\n\npip install faker\n\nfaker -r=10 profile > sample_data.txt\n\nCopy your sample data to the bucket (file provided in the repository)\n\naws s3 cp ./sample_data.txt s3://awscookbook804-$RANDOM_STRING\n\nYou should see output similar to the following:\n\nupload: ./sample_data.txt to s3://awscookbook804-<<unique>>/sample_data.txt\n\nCreate a start-pii-entities-detection-job with Comprehend:\n\nJOB_ID=$(aws comprehend start-pii-entities-detection-job \\\n\n--input-data-config S3Uri=\"s3://awscookbook804-$RANDOM_STRING/sample_data.txt\" \\\n\n--output-data-config S3Uri=\"s3://awscookbook804-$RANDOM_STRING/redacted_output/\"\n\n\\\n\n--mode \"ONLY_REDACTION\" \\\n\n--redaction-config\n\nPiiEntityTypes=\"BANK_ACCOUNT_NUMBER\",\"BANK_ROUTING\",\"CREDIT_DEBIT_NUMBER\",\n\n\"CREDIT_DEBIT_CVV\",\n\n\"CREDIT_DEBIT_EXPIRY\",\"PIN\",\"EMAIL\",\"ADDRESS\",\"NAME\",\"PHONE\",\"SSN\",MaskMode=\"REP\n\nLACE_WITH_PII_ENTITY_TYPE\" \\\n\n--data-access-role-arn\n\n\"arn:aws:iam::${AWS_ACCOUNT_ID}:role/AWSCookbook804Comprehend\" \\\n\n--job-name \"aws cookbook 804\" \\\n\n--language-code \"en\" \\\n\n--output text --query JobId)\n\nTIP\n\nYou can alternatively use the detect-pii-entities command if you are interested in the location of PII\n\ndata in a document. This is helpful if you need to process the PII in a certain way.\n\nMonitor the job until it is COMPLETED; this will take a few minutes:",
      "content_length": 1314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "aws comprehend describe-pii-entities-detection-job \\\n\n--job-id $JOB_ID\n\nYou should see output similar to the following:\n\n{\n\n\"PiiEntitiesDetectionJobProperties\": {\n\n\"JobId\": \"<<hash>>\",\n\n\"JobName\": \"aws cookbook 804\",\n\n\"JobStatus\": \"COMPLETED\",\n\n\"SubmitTime\": \"2021-06-29T18:35:14.701000-04:00\",\n\n\"EndTime\": \"2021-06-29T18:43:21.200000-04:00\",\n\n\"InputDataConfig\": {\n\n\"S3Uri\": \"s3://awscookbook804-<<string>>/sample_data.txt\",\n\n\"InputFormat\": \"ONE_DOC_PER_LINE\"\n\n},\n\n\"OutputDataConfig\": {\n\n\"S3Uri\": \"s3://awscookbook804-<<string>>/redacted_output/<<Account Id>>-PII-\n\n<<hash>>/output/\"\n\n},\n\nValidation checks\n\nWhen the job is complete, get the location of the outputted data in S3:\n\nS3_LOCATION=$(aws comprehend describe-pii-entities-detection-job \\\n\n--job-id $JOB_ID --output text \\\n\n--query PiiEntitiesDetectionJobProperties.OutputDataConfig.S3Uri)\n\nGet the output file from S3:\n\naws s3 cp ${S3_LOCATION}sample_data.txt.out .\n\nYou should see output similar to the following:\n\ndownload: s3://awscookbook804-<<unique>>/redacted_output/111111111111-PII-\n\ncb5991dd58105db185a4cc1906e38411/output/sample_data.txt.out to ./sample_data.txt.out",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "View the output:\n\ncat sample_data.txt.out\n\nYou should see output similar to the following. Notice the PII has been redacted:\n\n{'job': 'Arts development officer', 'company': 'Vance Group', 'ssn': '[SSN]', 'residence':\n\n'[ADDRESS]\\[ADDRESS]', 'current_location': (Decimal('77.6093685'), Decimal('-90.497660')),\n\n'blood_group':\n\n'O-', 'website': ['http://cook.com/', 'http://washington.biz/', 'http://owens.net/',\n\n'http://www.benson.com/'], 'username': 'rrobinson', 'name': '[NAME]', 'sex': 'M', 'address':\n\n'[ADDRESS]\\[ADDRESS]', 'mail': '[EMAIL]', 'birthdate': datetime.date(1989, 10, 27)}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nPII is closely associated with many security and compliance standards that you may come across in your career responsibilities. Generally, if you are responsible for handling PII for your customers, you need to implement security mechanisms to ensure the safety of that data. Furthermore, you may need to detect and analyze the kind of PII you store. While Amazon Macie can do this at scale within your S3 buckets or data lake, you may want to detect PII within your application to implement your own checks and workflows. For example, you may have a user fill out a form and submit it, and then detect if they have accidentally disclosed specific types of PII that you are not allowed to store, and reject the upload.\n\nYou can leverage Amazon Comprehend to detect this type of information for you. When you use Comprehend, the predefined feature detection is backed by detection models that are trained using large datasets to ensure quality results.\n\nChallenge\n\nUse Comprehend to label the type of PII rather than just redacting it. (This article provides a hint.)",
      "content_length": 1752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "8.5 Detecting Text in a Video\n\nProblem\n\nYou have a video and would like to extract any text from scenes in it for analysis.\n\nSolution\n\nUpload the video file to S3 and start a text detection job in Amazon Rekognition Video (see Figure 8-5).\n\nFigure 8-5. Using Rekognition Video to detect text in an MP4\n\nPrerequisite\n\nS3 bucket\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCopy the provided sample_movie.mp4 file to the S3 bucket you created:\n\naws s3 cp ./sample_movie.mp4 s3://awscookbook805-$RANDOM_STRING\n\nYou should see output similar to the following:\n\nupload: ./sample_movie.mp4 to s3://awscookbook805-utonl0/sample_movie.mp4",
      "content_length": 684,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "NOTE\n\nRegarding supported video formats, the Amazon Rekognition FAQs states, “Amazon Rekognition\n\nVideo supports H.264 files in MPEG-4 (.mp4) or MOV format. If your video files use a different\n\ncodec, you can transcode them into H.264 using AWS Elemental MediaConvert.”\n\n2.\n\nBegin the text detection job:\n\nJOB_ID=$(aws rekognition start-text-detection \\\n\n--video '{\"S3Object\":\n\n{\"Bucket\":\"'\"awscookbook805-$RANDOM_STRING\"'\",\"Name\":\"sample_movie.mp4\"}}' \\\n\n--output text --query JobId)\n\nWait until JobStatus changes from IN_PROGRESS to SUCCEEDED, and then the results will be displayed:\n\naws rekognition get-text-detection \\\n\n--job-id $JOB_ID\n\nYou should see text similar to the following:\n\n{\n\n\"JobStatus\": \"SUCCEEDED\",\n\n\"VideoMetadata\": {\n\n\"Codec\": \"h264\",\n\n\"DurationMillis\": 10051,\n\n\"Format\": \"QuickTime / MOV\",\n\n\"FrameRate\": 30.046607971191406,\n\n\"FrameHeight\": 240,\n\n\"FrameWidth\": 320,\n\n\"ColorRange\": \"LIMITED\"\n\n},\n\n...\n\nValidation checks\n\nRun the command again with this query to get the DetectedText values:",
      "content_length": 1011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "aws rekognition get-text-detection \\\n\n--job-id $JOB_ID \\\n\n--query 'TextDetections[*].TextDetection.DetectedText'\n\nYou should see text similar to the following:\n\n[\n\n\"COPYRIGHT, 1901\",\n\n\"THOMAB A. EDISON.\",\n\n\"PATENTED AuOUST 31ST. 1897\",\n\n\"COPYRIGHT,\",\n\n\"1901\",\n\n\"THOMAB\",\n\n\"A.\",\n\n\"EDISON.\",\n\n\"PATENTED\",\n\n\"AuOUST\",\n\n\"31ST.\",\n\n\"1897\",\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou used Rekognition to detect text within a video file and saved the output to a report accessible within the Rekognition service. The output of the text detection job contains time markers for the text detected, the detected text itself, and other features. You can also detect text within images, have batch processing jobs run on large sets of files, and detect other features within videos and images. This fully managed service allows you to use reliable detection models without having to train your own.\n\nTIP\n\nRekognition supports Custom Labels, a feature of the service that allows you to train specific models to recognize\n\nparticular features in video and images specific to your needs. You can accomplish this all within the Rekognition service\n\nitself for an end-to-end implementation. For more information, see the support document.\n\nYou can integrate Rekognition directly into your application or hardware by using the AWS SDK for a reliable feature and text detection mechanism.",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "Challenge\n\nConfigure a text detection job to automatically start when a file is uploaded to a specific S3 bucket by using EventBridge.\n\n8.6 Physician Dictation Analysis Using Amazon Transcribe Medical and Comprehend Medical\n\nProblem\n\nYou need to build a solution that recognizes medical professional dictation audio files. The solution needs to be able to categorize things like protected health information (PHI) for further analysis.\n\nSolution\n\nUse Amazon Transcribe Medical to analyze your audio file. Then, use Amazon Comprehend Medical to generate the analysis of the physician’s speech in a medical context (see Figure 8-6).\n\nFigure 8-6. Using Transcribe Medical and Comprehend Medical with physician dictations\n\nPrerequisite\n\nAn audio file with speech (human or computer-generated) in it that contains medical jargon\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps",
      "content_length": 917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "1.\n\n2.\n\n3.\n\nCreate a JSON file named awscookbook806-template.json with the parameters specifying your physician’s specialty, language, S3 bucket, and dictation audio file (file provided in the repository):\n\n{\n\n\"MedicalTranscriptionJobName\": \"aws-cookbook-806\",\n\n\"LanguageCode\": \"en-US\",\n\n\"Specialty\": \"PRIMARYCARE\",\n\n\"Type\": \"DICTATION\",\n\n\"OutputBucketName\":\"awscookbook806-RANDOM_STRING\",\n\n\"Media\": {\n\n\"MediaFileUri\": \"s3://awscookbook806-RANDOM_STRING/dictation.mp3\"\n\n}\n\n}\n\nUse the sed command to replace the values in the awscookbook806- template.json file with the RANDOM_STRING value for your S3 bucket:\n\nsed -e \"s/RANDOM_STRING/${RANDOM_STRING}/g\" \\\n\nawscookbook806-template.json > awscookbook806.json\n\nStart a medical transcription job using the JSON file that you created:\n\naws transcribe start-medical-transcription-job \\\n\n--cli-input-json file://awscookbook806.json\n\nYou should see output similar to the following:\n\n{\n\n\"MedicalTranscriptionJob\": {\n\n\"MedicalTranscriptionJobName\": \"aws-cookbook-806\",\n\n\"TranscriptionJobStatus\": \"IN_PROGRESS\",\n\n\"LanguageCode\": \"en-US\",\n\n\"Media\": {\n\n\"MediaFileUri\": \"s3://awscookbook806-<<unique>>/dictation.mp3\"\n\n},\n\n\"StartTime\": \"2021-07-14T20:24:58.012000-04:00\",\n\n\"CreationTime\": \"2021-07-14T20:24:57.979000-04:00\",",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "4.\n\n5.\n\n\"Specialty\": \"PRIMARYCARE\",\n\n\"Type\": \"DICTATION\"\n\n}\n\n}\n\nCheck the status of the Transcribe Medical job. Wait until it is COMPLETED:\n\naws transcribe get-medical-transcription-job \\\n\n--medical-transcription-job-name aws-cookbook-806 \\\n\n--output text \\\n\n--query MedicalTranscriptionJob.TranscriptionJobStatus\n\nGet the output from your previous job by downloading the file from S3:\n\naws s3 cp s3://awscookbook806-${RANDOM_STRING}/medical/aws-cookbook-806.json \\\n\n./aws-cookbook-806.json\n\nYou should see output similar to the following:\n\ndownload: s3://awscookbook806-<<unique>>/medical/aws-cookbook-806.json to ./aws-\n\ncookbook-806.json\n\nDisplay the transcribed speech from the downloaded file:\n\ncat aws-cookbook-806.json | jq .results.transcripts\n\nYou should see output similar to the following:\n\n[\n\n{\n\n\"transcript\": \"patient jane doe experiencing symptoms of headache, administered\n\n200 mg ibuprofen\n\ntwice daily.\"\n\n}\n\n]",
      "content_length": 926,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "NOTE\n\nThe resulting JSON file that you downloaded contains time markers for each word that was\n\ntranscribed. You can use this information within your application to provide additional context and\n\nfunctionality.\n\nValidation checks\n\nStart an entities detection job using the Comprehend Medical detect entities API. This will show the location of things like medical conditions and PHI:\n\naws comprehendmedical detect-entities-v2 \\\n\n--text \"$(cat aws-cookbook-806.json | jq .results.transcripts[0].transcript | tr -d '\"')\"\n\nYou should see output similar to the following:\n\n{\n\n\"Entities\": [\n\n{\n\n\"Id\": 4,\n\n\"BeginOffset\": 8,\n\n\"EndOffset\": 12,\n\n\"Score\": 0.8507962226867676,\n\n\"Text\": \"jane\",\n\n\"Category\": \"PROTECTED_HEALTH_INFORMATION\",\n\n\"Type\": \"NAME\",\n\n\"Traits\": []\n\n},\n\n...\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou were able to use Amazon Transcribe Medical and Amazon Comprehend Medical to take a physician’s dictation audio file, turn it into text, and then detect the medical-context entities within the audio file. The final result provided patient data, symptoms, and medication and dosage information, which can be extremely useful in building medical applications for patients and medical professionals. Comprehend Medical also provides batch analysis and feature",
      "content_length": 1324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "detection for multiple types of PHI and can determine dictation versus patient/physician conversation. With the advent of telemedicine, these powerful features can be used to provide immediate transcriptions to patients using app-based medical services to receive healthcare from medical professionals.\n\nNOTE\n\nBoth of these services you used for the solution are HIPAA compliant. You can confidently build solutions with these\n\nservices for medical use cases that need to conform with HIPAA compliance. For more information on AWS services that\n\nconform to compliance standards, see the Services in Scope web page.\n\nChallenge\n\nUse EventBridge to automate the processing of new objects uploaded to your S3 bucket.\n\n8.7 Determining Location of Text in an Image\n\nProblem\n\nYou need to determine in which quadrant of an image the text “AWS” appears.\n\nSolution\n\nWe’ll use Textract to analyze the image from an S3 bucket and then parse the output to calculate the location of the text (see Figure 8-7).\n\nFigure 8-7. Using Textractor to analyze output from Amazon Textract\n\nPrerequisite\n\nS3 bucket",
      "content_length": 1089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "Preparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCopy the provided book_cover.png file to the S3 bucket you created:\n\naws s3 cp ./book_cover.png s3://awscookbook807-$RANDOM_STRING\n\nYou should see output similar to the following:\n\nupload: ./book_cover.png to s3://awscookbook807-<<unique>>/book_cover.png\n\n2.\n\nAnalyze the file with Textract and output the results to a file called output.json:\n\naws textract analyze-document \\\n\n--document '{\"S3Object\":\n\n{\"Bucket\":\"'\"awscookbook807-$RANDOM_STRING\"'\",\"Name\":\"book_cover.png\"}}' \\\n\n--feature-types '[\"TABLES\",\"FORMS\"]' > output.json\n\nValidation checks\n\nExamine the BoundingBox values for the Practical text to find the location:\n\njq '.Blocks[] | select(.Text == \"Practical\") | select(.BlockType == \"WORD\") | .Geometry.BoundingBox'\n\noutput.json\n\nIf the left and top values are both less than 0.5, the word Practical is located in the top left of the page (see Figure 8-8).\n\nYou should see output similar to the following:\n\n{\n\n\"Width\": 0.15338942408561707,\n\n\"Height\": 0.03961481899023056,",
      "content_length": 1082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "\"Left\": 0.06334125995635986,\n\n\"Top\": 0.39024031162261963\n\n}\n\nFigure 8-8. Reference BoundingBox coordinate diagram\n\nNOTE\n\nPer the Textract Developer Guide, each BoundingBox property has a value between 0 and 1. The value is a ratio of the overall image width (applies to left and width) or height (applies to height and top). For example, if the input image is 700\n\n× 200 pixels, and the top-left coordinate of the bounding box is (350,50) pixels, the API returns a left value of 0.5\n\n(350/700) and a top value of 0.25 (50/200).",
      "content_length": 527,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "Cleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou may have used traditional optical character recognition (OCR) software in the past when you have scanned physical documents and used the OCR output to get text-based output of the content. Amazon Textract takes this a step further using ML models for even more accurate text recognition with additional feature and context information you can use in your applications. You were able to determine the location of a string of text within an image by using this recipe. You could use similar functionality in Textract to forward certain portions of text from images (or scanned forms) to different teams for human review or automating sending certain parts of forms to different microservices in your application for processing or persisting. Application developers do not need to train a model before using Textract; Textract comes pretrained with many datasets to provide highly accurate character recognition.\n\nChallenge\n\nDetermine the location of two different words in an image and calculate the distance between them.",
      "content_length": 1119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "Chapter 9. Account Management\n\n9.0 Introduction While the previous chapters have focused on deploying and configuring resources inside your AWS account, we want to provide some recipes that show examples of what you can do at the whole account level.\n\nAs you continue to scale your AWS usage, you will find it useful to have tools and services available to ease the burden of management, especially if you start to add additional AWS accounts to your environment. We are seeing more and more people choose to use AWS accounts as “containers” for their specific applications. Some companies provide individual accounts for production and nonproduction workloads to business units within a company; some even set up “shared services” accounts to provide internal services to business units within a company to share common resources across the many accounts they may be managing. Your AWS account spans Regions to give you global capabilities, as shown in Figure 9-1.\n\nFigure 9-1. AWS account perspective\n\nWe like to think of the tools and services that AWS provides as building blocks that you can use to customize your cloud environment to meet your specific needs. Some of these can be used at an account level to give you more management capabilities over your environments as you scale (i.e., adding additional team members or adding additional AWS accounts). You can organize and consolidate billing for many accounts by using AWS Organizations and provide centralized",
      "content_length": 1472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "access management through AWS Single Sign-On, both of which you will explore in the recipes in this chapter, in addition to some recipes to help you configure and maintain a secure environment at the account level.\n\nTIP\n\nYou should always keep an eye on the AWS Prescriptive Guidance website for the most up-to-date guidance on account-\n\nlevel capabilities and recommendations.\n\nWorkstation Configuration\n\nYou will need a few things installed to be ready for the recipes in this chapter.\n\nGeneral setup\n\nFollow the “General workstation setup steps for CLI recipes” to validate your configuration and set up the required environment variables. Then, clone the chapter code repository:\n\ngit clone https://github.com/AWSCookbook/AccountManagement\n\n9.1 Using EC2 Global View for Account Resource Analysis\n\nProblem\n\nYou have been presented with an AWS account for a client. You need to export a CSV file containing all provisioned compute instances, disk volumes, and network resources across all Regions within an AWS account.\n\nSolution\n\nNavigate to EC2 Global View in the AWS Console. Export a CSV of resources in your account (see Figure 9-2).",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "Prerequisite\n\nSteps\n\n1.\n\nFigure 9-2. Generating a CSV of resources with EC2 Global View\n\nAWS account with resources deployed\n\nIn the AWS Console, search for and then click EC2 Global View, as shown in Figure 9-3.\n\nFigure 9-3. Searching for Global View",
      "content_length": 251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "2.\n\nNavigate to the “Global search” menu and click Download CSV (see Figure 9-4).\n\nFigure 9-4. Downloading a CSV from Global View\n\nValidation checks\n\nOpen the downloaded CSV file in your favorite editor on your workstation. From there, you should be able to scroll through resources in your AWS account.\n\nDiscussion\n\nYou may find yourself often working in new AWS accounts as part of your job. If you are in a lab environment or a hackathon, using Global View is a great way for you to get an idea of what an AWS account contains. Before you begin to make changes in an account, it is important to take stock of what is already deployed across all Regions. Having this knowledge will allow you to ensure that you don’t mistakenly cause any outages.\n\nWhile you should use tools like AWS Config and the AWS Billing console for inspecting the configuration of resources inside your AWS account and keeping an eye on your bill, respectively, it is good to know that you can easily generate a list of EC2 and VPC resources. Routine CSV exports at different times can be used to provide a point-in-time snapshot.\n\nChallenge\n\nUse the Global search functionality to list all default VPC security groups. You can then double- check inbound rules for extraneous permissions.\n\n9.2 Modifying Tags for Many Resources at One Time with Tag Editor",
      "content_length": 1331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "Problem\n\nYou need to add a tag to multiple resources in your AWS account where it doesn’t already exist.\n\nSolution\n\nLaunch the Tag Editor in the AWS Console. Find all resources that have an Environment key with nonexisting value. Add the tag (see Figure 9-5).\n\nFigure 9-5. Adding a tag to multiple resources\n\nPrerequisite\n\nAWS account with resources deployed and tagged\n\nSteps\n\n1.\n\nIn the AWS Console, search for Tag Editor and then click Resource Groups & Tag Editor, as shown in Figure 9-6.",
      "content_length": 492,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "2.\n\nFigure 9-6. Searching for Tag Editor in the AWS Console\n\nFrom the lefthand menu, click Tag Editor under the Tagging heading (see Figure 9-7).",
      "content_length": 145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "3.\n\nFigure 9-7. Launching Tag Editor\n\nSet “Resource types” to “All supported resource types,” enter a Tag key named Environment, use the drop-down menu to select “(not tagged)” for the value, and then click “Search resources” (see Figure 9-8).",
      "content_length": 243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "4.\n\nFigure 9-8. Launching Tag Editor\n\nWait a few moments for the search to complete. Then view the results, select the resources that you wish to tag, and click “Manage tags of selected resources” (see Figure 9-9).",
      "content_length": 214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "5.\n\nFigure 9-9. Viewing search results in Tag Editor\n\nEnter a “Tag key” named Environment, enter a “Tag value” Dev, and click “Review and apply tag changes” (see Figure 9-10).\n\nFigure 9-10. Tagging multiple resources with Tag Editor",
      "content_length": 232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "6.\n\nIn the pop-up window, confirm your selection by clicking “Apply changes to all selected” (see Figure 9-11).\n\nFigure 9-11. Confirming changes to apply to resources in Tag Editor\n\nValidation checks\n\nUse the CLI to confirm that all EC2 instances have an Environment tag:\n\naws ec2 describe-instances \\\n\n--output text \\\n\n--query 'Reservations[].Instances[?!not_null(Tags[?Key == `Environment`].Value)] | [].[InstanceId]'\n\nYou should see no output. You can also repeat step 3 to confirm that all resources now have an Environment tag.\n\nDiscussion",
      "content_length": 544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "A tag consists of a key and a value and can be applied to many resources at the time of creation and almost all resources after creation. You should implement your own tagging strategy as early as possible in your cloud journey. This ensures that you can identify the growing number of resources you deploy over the lifespan of your AWS account. Tags are useful for auditing, billing, scheduling updates, delegating access to resources with specific tags, and so on.\n\nCost allocation tags can help you make sense of your AWS bill, to see exactly which resources are contributing to certain portions of your bill. In addition to the cost allocation report, you can filter by tags interactively via Cost Explorer in the AWS Billing console to analyze and visualize your costs.\n\nThe Tag Editor can help you search and perform batch updates for tags across your AWS resources. Say you might have deployed many resources but forgot to tag them all, or you have historically not used tags and would like to start. You can batch select many resources within the Tag Editor (across all Regions or within a selected set of Regions) and perform these updates to ease the burden of implementing a tagging strategy.\n\nWhile not an exhaustive list, here are some good tags to include as part of a tagging baseline:\n\nCreatedBy\n\nThe User or Identity that created the resource\n\nApplication\n\nThe service or application of which the resource is a component\n\nCostCenter\n\nUseful for billing identification and to help implement a chargeback model for shared AWS account usage\n\nCreationDate\n\nThe date the resource was created\n\nContact\n\nAn email address for the team or individual in case of any issues with the resource (also helpful for configuring automated alerts)\n\nMaintenanceWindow\n\nUseful for defining a window of time that the resource is allowed to not be available in case of patching, updates, or maintenance\n\nDeletionDate\n\nUseful for development or sandbox environments so that you know when it may be safe to delete a resource\n\nChallenge\n\nApply a baseline set of tags to your AWS resources and begin to enforce the tags with a tag policy by enabling AWS Organizations.",
      "content_length": 2158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "9.3 Enabling CloudTrail Logging for Your AWS Account\n\nProblem\n\nYou just set up your AWS account and want to retain an audit log of all activity for all Regions in your account.\n\nSolution\n\nConfigure an S3 bucket with a bucket policy allowing CloudTrail to write events. Enable CloudTrail for all Regions in your account and configure CloudTrail to log all audit events to the S3 bucket, as shown in Figure 9-12.\n\nFigure 9-12. Turning on logging for CloudTrail",
      "content_length": 458,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "Prerequisite\n\nS3 bucket for logging\n\nPreparation\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nSteps\n\n1.\n\nCreate a file called cloudtrail-s3policy-template.json (file provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Sid\": \"S3CloudTrail\",\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n\n\"Action\": \"s3:GetBucketAcl\",\n\n\"Resource\": \"arn:aws:s3:::BUCKET_NAME\"\n\n},\n\n{\n\n\"Sid\": \"S3CloudTrail\",\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n\n\"Action\": \"s3:PutObject\",\n\n\"Resource\": \"arn:aws:s3:::BUCKET_NAME/AWSLogs/AWS_ACCOUNT_ID/*\",\n\n\"Condition\": {\"StringEquals\": {\"s3:x-amz-acl\": \"bucket-owner-full-control\"}}\n\n}\n\n]\n\n}\n\n2.\n\nReplace the values in the cloudtrail-s3policy-template.json file with values from your deployment. Here is a way to do it quickly with sed:\n\nsed -e \"s/BUCKET_NAME/awscookbook903-$RANDOM_STRING/g\" \\\n\ne \"s|AWS_ACCOUNT_ID|${AWS_ACCOUNT_ID}|g\" \\\n\ncloudtrail-s3policy-template.json > cloudtrail-s3policy.json\n\n3.\n\nAdd the S3 bucket policy to your S3 bucket:",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "aws s3api put-bucket-policy \\\n\n--bucket awscookbook903-$RANDOM_STRING \\\n\n--policy file://cloudtrail-s3policy.json\n\n4.\n\nEnable CloudTrail for all AWS Regions and configure the S3 bucket to send logs to the following:\n\naws cloudtrail create-trail --name AWSCookbook903Trail \\\n\n--s3-bucket-name awscookbook903-$RANDOM_STRING \\\n\n--is-multi-region-trail\n\n5.\n\nStart the logging of your CloudTrail trail:\n\naws cloudtrail start-logging --name AWSCookbook903Trail\n\nNOTE\n\nMore information can be found on AWS for the required S3 bucket policy for CloudTrail logging.\n\nValidation checks\n\nDescribe the trail:\n\naws cloudtrail describe-trails --trail-name-list AWSCookbook903Trail\n\nYou should see output similar to the following:\n\n{\n\n\"trailList\": [\n\n{\n\n\"Name\": \"AWSCookbook903Trail\",\n\n\"S3BucketName\": \"awscookbook903-<<string>>\",\n\n\"IncludeGlobalServiceEvents\": true,\n\n\"IsMultiRegionTrail\": true,",
      "content_length": 881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "\"HomeRegion\": \"us-east-1\",\n\n\"TrailARN\": \"arn:aws:cloudtrail:us-east-1:<<Account Id>>:trail/AWSCookbook903Trail\",\n\n\"LogFileValidationEnabled\": false,\n\n\"HasCustomEventSelectors\": false,\n\n\"HasInsightSelectors\": false,\n\n\"IsOrganizationTrail\": false\n\n}\n\n]\n\n}\n\nGet the trail status:\n\naws cloudtrail get-trail-status --name AWSCookbook903Trail\n\nYou should see output similar to the following:\n\n{\n\n\"IsLogging\": true,\n\n\"StartLoggingTime\": \"2021-06-28T21:22:56.308000-04:00\",\n\n\"LatestDeliveryAttemptTime\": \"\",\n\n\"LatestNotificationAttemptTime\": \"\",\n\n\"LatestNotificationAttemptSucceeded\": \"\",\n\n\"LatestDeliveryAttemptSucceeded\": \"\",\n\n\"TimeLoggingStarted\": \"2021-06-29T01:22:56Z\",\n\n\"TimeLoggingStopped\": \"\"\n\n}\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nYou can use CloudTrail to log and continuously monitor all events in your AWS account. Specifically, events refer to all API activity against the AWS APIs. These include actions that you (and all authenticated entities) take in the console, via the command line, API activity from your applications, and other AWS services performing actions (automatic actions like autoscaling, EventBridge triggers, replication, etc.).\n\nTIP",
      "content_length": 1217,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "You can query your logs in place on S3 using Amazon Athena if you are looking for specific events, and you can index\n\nyour logs with Amazon OpenSearch to perform complex queries against your historical data.\n\nYou should use CloudTrail from a security standpoint, but you can also use it from an application debugging standpoint. Say you have an event-driven application that triggers a Lambda function when you upload files to an S3 bucket (see Recipe 5.7). If your IAM policy is incorrect for the Lambda function invocation, you will be able to see the Deny in the CloudTrail logs. This is helpful for application developers and architects who are building and designing event-driven applications powered by Amazon EventBridge.\n\nChallenge\n\nConfigure an organizational trail if you have an AWS Organization with multiple accounts. (See Recipe 9.6 to set up an AWS Organization.)\n\n9.4 Setting Up Email Alerts for Root Login\n\nProblem\n\nYou want to be notified by email when the root user logs into an AWS account.\n\nSolution\n\nCreate an SNS topic and subscribe to it. Then create an Amazon EventBridge rule with a pattern that searches for root logins and triggers the SNS topic, as shown in Figure 9-13.\n\nFigure 9-13. Logging and alerting for root logins\n\nPrerequisite",
      "content_length": 1264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "AWS account with CloudTrail enabled (see Recipe 9.3)\n\nSteps\n\n1.\n\nCreate an SNS topic:\n\nTOPIC_ARN=$(aws sns create-topic \\\n\n--name root-login-notify-topic \\\n\n--output text --query TopicArn)\n\n2.\n\nSubscribe to the SNS topic. This will send a confirmation email to the address you specify:\n\naws sns subscribe \\\n\n--topic-arn $TOPIC_ARN \\\n\n--protocol email \\\n\n--notification-endpoint your-email@example.com\n\n3.\n\nLocate the confirmation email in your inbox that AWS sent and click “Confirm subscription.”\n\nIn the root of this chapter’s repository, cd to the 904-Setting-Up-Email-Alerts-for-Root-Login directory and follow the subsequent steps.\n\n4.\n\nNow create an assume-role policy JSON statement called assume-role- policy.json to use in the next step (this file is provided in the repository):\n\n{\n\n\"Version\": \"2012-10-17\",\n\n\"Statement\": [\n\n{\n\n\"Effect\": \"Allow\",\n\n\"Principal\": {\n\n\"Service\": \"events.amazonaws.com\"\n\n},\n\n\"Action\": \"sts:AssumeRole\"\n\n}\n\n]\n\n}",
      "content_length": 948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "5.\n\n6.\n\n7.\n\n8.\n\nCreate the role and specify the assume-role-policy.json file:\n\naws iam create-role --role-name AWSCookbook904RuleRole \\\n\n--assume-role-policy-document \\\n\nfile://assume-role-policy.json\n\nNow attach the managed AmazonSNSFullAccess IAM policy to the IAM role:\n\naws iam attach-role-policy \\\n\n--policy-arn arn:aws:iam::aws:policy/AmazonSNSFullAccess \\\n\n--role-name AWSCookbook904RuleRole\n\nCreate a file called event-pattern.json for AWS Console sign-in events (file provided in the repository):\n\n{\n\n\"detail-type\": [\n\n\"AWS API Call via CloudTrail\",\n\n\"AWS Console Sign In via CloudTrail\"\n\n],\n\n\"detail\": {\n\n\"userIdentity\": {\n\n\"type\": [\n\n\"Root\"\n\n]\n\n}\n\n}\n\n}\n\nCreate an EventBridge rule that monitors the trail for root login activity:\n\naws events put-rule --name \"AWSCookbook904Rule\" \\\n\n--role-arn \"arn:aws:iam::$AWS_ACCOUNT_ID:role/AWSCookbook904RuleRole\" --event-\n\npattern file://event-\n\npattern.json",
      "content_length": 908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "9.\n\nSet the target for your EventBridge rule to your SNS topic:\n\naws events put-targets --rule AWSCookbook904Rule \\\n\n--targets \"Id\"=\"1\",\"Arn\"=\"$TOPIC_ARN\"\n\nValidation checks\n\nLog in to your AWS account using your root account, wait a few minutes, and check your email for a message from SNS.\n\nCleanup\n\nFollow the steps in this recipe’s folder in the chapter code repository.\n\nDiscussion\n\nSetting up an alert to notify you on root user sign-ins is a detective mechanism you can layer into your security strategy to keep aware of unwelcome activity within your account. This method is a cost-effective solution to monitor your AWS account for unintended access via the root user. Since the root user is the most powerful and privileged identity (and should be used only for specific infrequent tasks), it is important to know when it is accessed.\n\nNOTE\n\nIn some cases, the root user is needed to perform specific privileged actions and tasks within an AWS account. For a list of\n\nthese actions requiring the root user, refer to the AWS support document.\n\nChallenge\n\nAdd another EventBridge rule that notifies the same SNS topic when the root password is changed (here is a hint).\n\n9.5 Setting Up Multi-Factor Authentication for a Root User\n\nProblem\n\nYou need to enable multi-factor authentication for the root user of your AWS account.\n\nSolution",
      "content_length": 1343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "Log in to your AWS account with your root user credentials. Navigate to the IAM console and enable multi-factor authentication using a U2F-compatible hardware device or a Time-Based One-Time Password (TOTP)–compliant virtual device (see Figure 9-14).\n\nFigure 9-14. Enabling MFA for the root user in your AWS account\n\nSteps\n\n1.\n\nLog in to the AWS Console by using the email address associated with your root user. You can reset the password for the root user by using the “Forgot password” link displayed after you enter the root user email address; click Next. The login dialog you should see is shown in Figure 9-15.",
      "content_length": 617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "2.\n\nFigure 9-15. Selecting the root user login option\n\nTIP\n\nSince you will log in with the root user infrequently, you do not need to store the password for the user\n\nonce you enable MFA. You can reset the password each time you need to access the account; take care\n\nto secure access to the mailbox associated with your root user.\n\nOnce you are logged in to the AWS Console, select My Security Credentials from the top right of the user interface, as shown in Figure 9-16.",
      "content_length": 473,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "Figure 9-16. Navigating to the My Security Credentials menu",
      "content_length": 59,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "3.\n\nExpand the “Multi-factor authentication” pane on the Your Security Credentials page within the IAM console and click the Activate MFA button. Choose the type of device you will use and click Continue (see Figure 9-17).\n\nWARNING\n\nIf you use a software-based password manager utility to store your virtual MFA device information, do\n\nnot store your root user password in that same password manager. If your password manager utility or\n\nvault is compromised, your second factor and password together give the ability to access your AWS\n\naccount. Similarly, the password information for your email account should not be stored in the same\n\nplace as your virtual MFA device, since the root user password-reset procedure can be performed\n\nsuccessfully with access to the mailbox associated with your root user.",
      "content_length": 808,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "4.\n\nFigure 9-17. Selecting a virtual MFA device\n\nFollow the prompts to either display a QR code that you can scan with your virtual device or plug in your hardware token and follow the prompts. Once you enter the code, you will see a window indicating that you have completed the MFA setup (see Figure 9-18).",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "Figure 9-18. Confirmation of MFA device setup\n\nTIP\n\nYou can print out a copy of the QR code displayed and keep this in a physically secure location as a\n\nbackup in case you lose access to your virtual MFA device.\n\nValidation checks\n\nSign out of your AWS account and sign back in with the root user. Enter the code generated by your virtual device (or plug in your hardware token) and complete the login process.\n\nCleanup\n\nYou should always keep MFA enabled for your root user. If you would like to disable the MFA device associated with your root user, follow the steps for deactivating the device.\n\nDiscussion\n\nThe root user is the most powerful and privileged identity of an AWS account. The username is an email address that you configure when you first establish your AWS account, and password- reset requests can be made from the AWS Console or by contacting AWS support. If your email account is compromised, a malicious actor could request a reset of your root account password and gain access to your account. The root user should always be protected with a second factor",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "of authentication to prevent unauthorized access to your account. Since the root user is needed on only rare occasions, you should never use it for routine tasks.\n\nTIP\n\nYou need to enable IAM user and role access to the Billing console. You can follow the AWS steps to perform that action\n\nand further reduce your dependency on using the root user for tasks.\n\nIt is extremely important to configure an IAM user (or set up federated access using ADFS with IAM, SAML with IAM, or AWS Single Sign-On) rather than use the root user to log in to your AWS account for your routine usage. AWS recommends immediately creating an administrative IAM user as one of the first things you do when you open an AWS account, shortly after you enable MFA for the root user. For a scalable approach, you can follow Recipe 9.6 to enable and use AWS Single Sign-On for your own (and delegated) access requirements.\n\nChallenge\n\nConfigure MFA protection for other IAM roles (using trust policies) in your account (here is a hint).\n\n9.6 Setting Up AWS Organizations and AWS Single Sign-On\n\nProblem\n\nYou need a scalable way to centrally manage usernames and passwords to access your AWS accounts.\n\nSolution\n\nEnable AWS Organizations, configure AWS Single Sign-On, create a group, create a permission set, and create a user in the AWS SSO directory for you to use, as shown in Figure 9-19.",
      "content_length": 1364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "Steps\n\n1.\n\nFigure 9-19. AWS SSO view of permissions and users\n\nNavigate to AWS Organizations in the AWS Console and click “Create an organization.”\n\nTIP\n\nAWS Organizations provides many features that enable you to work with multiple AWS accounts at\n\nscale. For more information, see the AWS Organizations documentation.",
      "content_length": 319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "2.\n\n3.\n\n4.\n\nThe organization creation will trigger an email to be sent to the email address associated with the root user of your account. Confirm the creation of the organization by clicking the “Verify your email address” button.\n\nNavigate to the AWS SSO console in your Region and click the Enable SSO button.\n\nNOTE\n\nThe initial configuration of AWS SSO uses a local directory for your users. You can federate to an\n\nexisting user directory you may have, like Active Directory or a SAML provider. For information on\n\nconfiguring federation, the user guide for AWS SSO provides details.\n\nGo to the Groups portion of the SSO console and click “Create group.” Enter the required Group name and click Create, as shown in Figure 9-20.",
      "content_length": 732,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "5.\n\nFigure 9-20. Creating a group in AWS SSO\n\nGo to the Users portion of the SSO console and select “Add user.” Enter the information required in the fields, add the user to the group you created, and select Create. You can have the AWS SSO console generate the initial password for you or send an email to the address provided in the required “Email address” field. Assign the user to the group that you created in step 4 on the second page of the “Add user” wizard (see Figure 9-21).",
      "content_length": 485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "6.\n\nFigure 9-21. Creating a user in AWS SSO\n\nGo to the AWS Accounts portion of the SSO console (in the left navigation menu of the console), choose the “Permission sets” tab, and select “Create permission set.” Choose the “Use an existing job function policy” option and select PowerUserAccess (see Figure 9-22).",
      "content_length": 312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "7.\n\nFigure 9-22. Creating a permission set in AWS SSO\n\nTo assign the permission set to the group you created for your AWS account, click the AWS accounts link in the left navigation menu, choose the “AWS organization” tab, select your AWS account from the list of accounts, and click “Assign users.” Click the Groups tab and choose the group that you added your user to in step 4, as shown in Figure 9-23. Click “Next: Permission sets.”",
      "content_length": 436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "8.\n\nFigure 9-23. Adding a group to an assignment\n\nSelect the permission set you created in step 6 and click Finish (see Figure 9- 24).",
      "content_length": 134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "Figure 9-24. Assigning a permission set in AWS SSO\n\nNOTE\n\nWhen you click Finish, members of this group can access the AWS account you specified using with\n\npermissions of the PowerUserAccess IAM policy. AWS SSO provisions a role that can be assumed in your AWS account for this purpose. Do not modify this role via IAM in your AWS account, or the\n\npermission set will not be able to be used through AWS SSO.\n\nValidation checks\n\nGo to the URL provided on the AWS SSO console dashboard page and log in with the username and password of the user that you created in step 5. Choose either “Management console” or “Command line or programmatic access” to gain PowerUserAccess to your AWS account.\n\nTIP\n\nThe default URL is generated when you first enable AWS SSO. You can create a customized URL for your own purposes\n\nby clicking the Customize link next to the default URL in the SSO console.",
      "content_length": 887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "Cleanup\n\nGo to the AWS SSO console and select Settings from the left navigation menu. Scroll down to the bottom of the settings page and select “Delete AWS SSO Configuration.” Confirm the deletion by selecting the checkboxes and clicking “Delete AWS SSO.”\n\nGo to AWS Organizations, select your organization, open the settings for your organization, and select “Delete organization.” You will need to verify the organization ID and confirm the deletion of the organization from the email address associated with the root user of your AWS account.\n\nDiscussion\n\nWhen you enable AWS Organizations, the account where you initially create your organization is known as the management account. While you enabled AWS Organizations so that you could use AWS Single Sign-On in this recipe, it primarily exists for governing, organizing, and managing AWS accounts at scale. Some account management capabilities that you might be interested in include, but are not limited to, these:\n\nConsolidating billing for multiple AWS accounts\n\nManaging group accounts using organizational units (OUs)\n\nApplying service control policies (SCPs) to individual accounts or OUs\n\nCentralizing policies for tagging and backups for all accounts within your organization\n\nSharing resources across accounts using Resource Access Manager (RAM)\n\nAWS Organizations and AWS SSO are free to enable and provide a scalable way for you to begin to manage AWS accounts and user access to your entire AWS environment. As a best practice, using the management account for management functions only (and not running workloads) is a pattern you should adopt. Creating specific AWS accounts for your production and nonproduction workloads that are members of your AWS Organization helps you isolate your workloads and reduce blast radius, delegate access, manage your billing, and so on.\n\nTIP\n\nThese concepts begin to define the concept of a landing zone. Rather than build your own, you can use AWS Control\n\nTower to configure a fully managed landing zone. There are advantages of using Control Tower when you plan to scale\n\nyour AWS usage beyond just a few accounts.\n\nAWS Single Sign-On provides a secure and scalable way to manage user access to your AWS accounts. You can integrate with an external identity provider (IdP) or use the built-in directory within AWS SSO, depending on how many users you need to manage and if you already have an external user directory.",
      "content_length": 2425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "When you successfully authenticate with AWS SSO, you are presented with a choice of access levels defined by permission sets. Permission sets use an IAM policy definition that SSO uses to manage a role, which you can assume upon successful login. AWS provides permission sets within SSO that align with common job functions, but you can also create your own custom permission sets by writing IAM policy statements and saving them as a permission set. A temporary session is created once you choose your access level into an account. You can use the session within the AWS Console, or via the command line with the temporary access key ID, secret access key, and session token variables, or by using the AWS CLI v2 to authenticate with SSO from the command line. You can adjust the length of the session duration within the AWS SSO console.\n\nSecurity around the access to your AWS environments should be your top priority, and as you scale the number of users and accounts, and level of access delegated, this becomes a challenge. AWS SSO gives you a mechanism to implement security at scale for your AWS environments. Since the session you initiate with your account via AWS SSO is temporary, you do not need to create long-lived IAM access keys to use in your command-line environment, which is one less secret to have to rotate and manage. You can also use multi-factor authentication with AWS SSO and require MFA for login.\n\nChallenge 1\n\nConnect AWS SSO to an external identity provider for an existing IdP-like Active Directory or Okta.\n\nChallenge 2\n\nApply a service control policy (SCP) to limit the Regions you can use within your AWS account.",
      "content_length": 1649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "Appendix. Fast Fixes\n\nThese useful small bits of code will help you save time and get the most out of AWS.\n\nSet your AWS_ACCOUNT_ID to a bash variable:\n\nexport AWS_ACCOUNT_ID=$(aws sts get-caller-identity \\\n\n--query Account --output text)\n\nGet the most recently created CloudWatch log group name:\n\naws logs describe-log-groups --output=yaml \\\n\n--query 'reverse(sort_by(logGroups,&creationTime))[:1].{Name:logGroupName}'\n\nTail the logs for the CloudWatch group:\n\naws logs tail <<LOGGROUPNAME>> --follow --since 10s\n\nDelete all log groups that match a text pattern and prompt yes/no for confirmation:\n\naws logs describe-log-groups | \\\n\njq \".logGroups[].logGroupName\" | grep -i <<pattern>> | \\\n\nxargs -p -I % aws logs delete-log-group --log-group-name %\n\nStop all running instances for your current working Region (H/T: Curtis Rissi):\n\naws ec2 stop-instances \\\n\n--instance-ids $(aws ec2 describe-instances \\\n\n--filters \"Name=instance-state-name,Values=running\" --query \"Reservations[].Instances[].[InstanceId]\"\n\n--output text | tr '\\n' ' ')\n\nDetermine the user making CLI calls:",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "aws sts get-caller-identity --query UserId --output text\n\nGenerate YAML input for your CLI command and use it:\n\naws ec2 create-vpc --generate-cli-skeleton yaml-input > input.yaml\n\n#Edit input.yaml - at a minimum modify CidrBlock, DryRun, ResourceType, and Tags\n\naws ec2 create-vpc --cli-input-yaml file://input.yaml\n\nList the AWS Region names and endpoints in a table format:\n\naws ec2 describe-regions --output table\n\nFind interface VPC endpoints for the Region you are currently using:\n\naws ec2 describe-vpc-endpoint-services \\\n\n--query ServiceDetails[*].ServiceName\n\nPopulate data into a DynamoDB table:\n\naws ddb put table_name '[{key1: value1}, {key2: value2}]'\n\nDetermine the current supported versions for a particular database engine (e.g., aurora- postgresql):\n\naws rds describe-db-engine-versions --engine aurora-postgresql \\\n\n--query \"DBEngineVersions[].EngineVersion\"\n\nDelete network interfaces associated with a security group and prompt for each delete (answer yes/no to delete or skip):\n\naws ec2 describe-network-interfaces \\",
      "content_length": 1038,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "--filters Name=group-id,Values=$SecurityGroup \\\n\n--query NetworkInterfaces[*].NetworkInterfaceId \\\n\n--output text | tr '\\t' '\\n' | xargs -p -I % \\\n\naws ec2 delete-network-interface --network-interface-id %\n\nFind your default VPC (if you have one) for a Region:\n\naws ec2 describe-vpcs --vpc-ids \\\n\n--query 'Vpcs[?IsDefault==`true`]'\n\nEnable encryption by default for new EBS volumes in a Region:\n\naws ec2 enable-ebs-encryption-by-default\n\nList all AWS Regions:\n\naws ssm get-parameters-by-path \\\n\n--path /aws/service/global-infrastructure/regions \\\n\n--output text --query Parameters[*].Name | tr \"\\t\" \"\\n\"\n\nList all AWS services:\n\naws ssm get-parameters-by-path \\\n\n--path /aws/service/global-infrastructure/services \\\n\n--output text --query Parameters[*].Name \\\n\n| tr \"\\t\" \"\\n\" | awk -F \"/\" '{ print $6 }'\n\nList all services available in a region (e.g., us-east-1):\n\naws ssm get-parameters-by-path \\\n\n--path /aws/service/global-infrastructure/regions/us-east-1/services \\\n\n--output text --query Parameters[*].Name | tr \"\\t\" \"\\n\" \\\n\n| awk -F \"/\" '{ print $8 }'",
      "content_length": 1057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "List all Regions that have a particular service available (e.g., SNS):\n\naws ssm get-parameters-by-path \\\n\n--path /aws/service/global-infrastructure/services/sns/regions \\\n\n--output text --query Parameters[*].Value | tr \"\\t\" \"\\n\"\n\nCreate a presigned URL for an object in S3 that expires in a week:\n\naws s3 presign s3://<<BucketName>>/<<FileName>> \\\n\n--expires-in 604800\n\nFind Availability Zone IDs for a Region that are consistent across accounts:\n\naws ec2 describe-availability-zones --region $AWS_REGION\n\nSet the Region by grabbing the value from an EC2 instance’s metadata:\n\nexport AWS_DEFAULT_REGION=$(curl --silent http://169.254.169.254/latest/dynamic/instance-\n\nidentity/document \\\n\n| awk -F'\"' ' /region/ {print $4}')",
      "content_length": 724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "Index\n\nA\n\nAccess Analyzer\n\nlist of services supported, Discussion using to generate IAM policy based on CloudTrail activity, Solution\n\naccess points (S3), configuring application-specific access to buckets with, Problem- Challenge access policies for common job functions, Steps creating policy for secret access, Steps\n\naccount management, Introduction-Challenge 2\n\nenabling CloudTrail logging for AWS account, Problem-Challenge modifying tags for many resources at one time with Tag Editor, Problem-Challenge setting AWS ACCOUNT ID to bash variable, Fast Fixes setting up AWS Organization and AWS Single Sign-On, Problem-Challenge 2 setting up email alerts for root login, Problem-Challenge setting up multi-factor authentication for root user in, Problem-Challenge using EC2 Global View for account resource analysis, Problem-Challenge\n\nadministrative access for routine development tasks, not recommended, Discussion administrative capabilities, delegating for IAM using permissions boundaries, Problem- Challenge administrative IAM user, Discussion AI/ML, Introduction-Challenge\n\ncomputer vision analysis of form data, Problem-Challenge converting text to speech, Problem-Challenge detecting text in a video, Problem-Challenge determining location of text in an image, Problem-Challenge physician dictation analysis, Problem-Discussion redacting PII from text using Comprehend, Problem-Challenge transcribing a podcast, Problem-Challenge\n\nALBs (see Application Load Balancers) alerts (email) for root login, setting up, Problem-Challenge Amazon Certificate Manager (ACM), Discussion Amazon Cloud Watch (see Cloud Watch) Amazon Comprehend, using to redact PII from text, Problem Amazon ECS (see ECS)",
      "content_length": 1703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "Amazon Elastic Cloud Compute (see EC2) Amazon Elastic Kubernetes Service (Amazon EKS), Introduction\n\n(see also EKS)\n\nAmazon Lightsail (see Lightsail, deploying containers with) Amazon Machine Image (AMI), AWS Backup handling process of building, Discussion Amazon Managed Service for Prometheus, Introduction Amazon OpenSearch, Discussion Amazon Polly, converting text to speech, Problem-Challenge Amazon Rekognition Video, using to detect text in a video, Problem-Challenge Amazon Textract and textractor tool\n\ndetermining location of text in an image, Problem-Challenge using for computer vision analysis of form data, Solution-Challenge\n\nAmazon Transcribe Medical and Amazon Comprehend Medical, using to analyze physician dictation, Problem-Discussion Amazon Transcribe, using with MP3 file, Solution-Challenge AmazonDynamoDBFullAccess policy, Validation checks, Steps AmazonEC2ReadOnlyAccess policy, Steps AmazonS3ReadOnlyAccess policy, Steps AmazonSSMManagedInstanceCore policy, Solution, Steps Application Load Balancers (ALBs)\n\nconfiguring ALB to invoke Lambda function, Problem-Challenge 2 creating new ALB target group to use as Green target with CodeDeploy, Steps redirecting HTTP traffic to HTTPS with, Problem-Challenge\n\nArchive Access storage tier, Discussion ARN (Amazon Resource Names)\n\nfinding for ECS task, Validation checks retrieving ARN for user, Steps\n\nartificial intelligence (see AI/ML) AssumeRole API, Validation checks AssumeRole policy, Validation checks assuming a role, Validation checks Athena service\n\nquerying files on S3 with, Problem-Challenge querying logs in place on S3 with, Discussion\n\naudio, transcribing to text, Problem-Challenge auditing session activity, Discussion Aurora Serverless\n\ncreating PostgreSQL database, Problem-Challenge enabling REST access to, using RDS Data API, Problem-Challenge\n\nauthentication\n\nIAM, using with RDS database, Problem-Challenge",
      "content_length": 1903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "multi-factor, Discussion\n\nrequiring in AssumeRole policies, Discussion setting up for root user, Problem-Challenge\n\nauthorization tokens, Steps AutoPause, Steps Availability Zones (AZs), Discussion\n\nENI in, Discussion finding AZ IDs for Region that is consistent across accounts, Fast Fixes NAT gateways in, Discussion subnets in a VPC, spreading across AZs, Discussion\n\navailable state for VPC, verifying, Validation checks AWS ACCOUNT ID, setting to bash variable, Fast Fixes AWS account management (see account management) AWS Backup, Discussion\n\ncreating and restoring EC2 backups to another Region using, Problem-Challenge\n\nAWS Backup Managed Policy, Steps AWS CLI\n\nautomating process of token retrieval, Discussion configuring credentials for, Validation checks determining user making calls, Fast Fixes generating Yaml input for command, Fast Fixes installing Lightsail Control plugin for, Prerequisite starting Transcribe transcription job, Steps\n\nAWS Cloud Map, Introduction AWS CodeDeploy, using to orchestrate application deployments to ECS, Problem- Challenge AWS Config, Discussion AWS Control Tower, Discussion AWS Copilot, deploying containers with, Problem-Challenge AWS Database Migration Service (AWS DMS), Discussion AWS Glue crawlers, automatically discovering metadata with, Problem-Challenge AWS KMS CMK, configuring S3 buckets to use keys referencing, Solution AWS Lake Formation, Discussion AWS Organizations, Introduction AWS Organizations and Single Sign-On, setting up, Problem-Challenge 2 AWS Policy Generator, Discussion AWS Schema Conversion Tool, Discussion AWS SDK, Discussion\n\nusing specific access points with, Steps\n\nAWS Security Token Service (see Security Token Service)",
      "content_length": 1707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "AWS security topics, resources for, Introduction AWS services, listing all, Fast Fixes AWS Signer, using to run trusted code in Lambda function, Problem-Problem AWS Single Sign-On, Solution, Introduction, Discussion\n\nsetting up AWS Organizations and, Problem-Challenge 2\n\nAWS SSM Session Manager\n\nconnecting to EC2 instance in subnet of VPC via, Validation checks connecting to EC2 instances with, Problem-Challenge\n\nAWS Systems Manager (SSM) API, Discussion AWS-managed (aws/ebs) KMS keys, Discussion AWSLambdaBasicExecutionRole policy, IAM role for Lambda function execution, Steps AWSLambdaVPCAccess IAM policy, Steps\n\nB\n\nbackups\n\nautomated in Aurora Serverless, Discussion creating and restoring EC2 backups to another Region using AWS Backup, Problem- Challenge\n\nbig data, Introduction-Challenge\n\nautomatically discovering metadata with AWS Glue crawlers, Problem-Challenge querying files on S3 using Amazon Athena, Problem-Challenge streaming data to S3 using Kinesis Data Firehose, Problem-Challenge transforming data with AWS Glue DataBrew, Problem-Challenge using Kinesis Stream for ingestion of streaming data, Problem-Challenge workstation configuration, Workstation Configuration\n\nBlock Public Access feature (S3), Solution blue/green deployments, updating containers with, Problem-Challenge Border Gateway Protocol (BGP), Discussion bucket policy template to enforce encryption on all objects, Steps\n\nC\n\ncapacity for databases, Steps\n\nAurora Serverless scaling measured in capacity units, Discussion automatically scaling capacity targets, Steps autoscaling DynamoDB table provisioned capacity, Problem-Challenge checking for RDS cluster, Validation checks DynamoDB provisioned capacity , use of capacity units, Steps scaled down to 0, Steps setting at upper limit, Discussion",
      "content_length": 1789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "when database activity resumes, Steps\n\nCDNs (content delivery networks), Discussion certificate authority (CA), RDS Root CA, Validation checks certificates\n\nCloudFront HTTPS certificate on default hostname , Discussion creating for HTTPS, Steps SSL, Discussion\n\nCI/CD pipelines\n\nAWS Copilot commands embedded in, Discussion blue/green deployments pattern, Discussion\n\nCIDR (classless inter-domain routing) blocks, Solution choosing carefully for your VPC, Discussion creating VPC with IPv6 CIDR block, Discussion nonoverlapping ranges when connecting VPCs, Discussion quotas for IPv4 and IPv6, Discussion simplifying management of CIDRs in security groups with prefix lists, Problem- Challenge specifying CIDR notation for authorizations, Discussion\n\nCLI (see AWS CLI) Cloud Map, Introduction CloudFront, Discussion\n\nserving web content securely from S3 with, Problem-Challenge\n\nCloudTrail logging, Prerequisite\n\nconfiguring to log events on S3 bucket, Steps enabling, Steps enabling for your AWS account, Problem-Challenge\n\nCloudWatch, Steps\n\nconfiguring alarm and scaling policy for ECS service, Solution getting most recently created log group name, Fast Fixes Glue crawlers logging information to, Validation checks metrics service, Discussion streaming container logs to, Solution-Challenge tailing log group to observe Lambda function invoked, Validation checks tailing logs for CloudWatch group, Fast Fixes\n\nCloudWatch Events (see EventBridge) CloudWatchFullAccess policy, Validation checks clusters\n\ncreating database cluster with engine mode of serverless, Steps database, checking capacity of, Validation checks deleting RDS cluster, Cleanup",
      "content_length": 1651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "CMK (see customer-managed KMS key) code signing configuration, Steps CodeDeploy, using to orchestrate application deployments to ECS, Solution-Challenge CodeDeployRoleForECS policy, Steps CodeSigningPolicies, changing, Challenge 2 Common Vulnerabilities Scoring System (CVSS), Discussion Comprehend, using to redact PII from text, Problem-Challenge computer vision analysis of form data, Problem-Challenge concurrency (provisioned), reducing Lambda startup times with, Problem-Challenge 2 connection pooling, leveraging RDS Proxy for database connections from Lambda, Problem-Challenge consumers and producers (streaming data), Discussion containers, Introduction-Challenge\n\nautoscaling container workloads on ECS, Problem-Challenge building, tagging and pushing container image to Amazon ECR, Problem-Challenge capturing logs from containers running on ECS, Problem-Challenge deploying using Amazon Lightsail, Problem-Challenge deploying using AWS Copilot, Problem-Challenge launching Fargate container task in response to an event, Problem-Challenge networked together, Introduction orchestrators, Introduction packaging Lambda code in container image, Problem prerequisite, installing Docker, Docker installation and validation scanning images for security vulnerabilities on push to ECR, Problem-Challenge 2 updating with blue/green deployments, Problem-Challenge\n\nControl Tower, Discussion Copilot, deploying containers with, Problem-Challenge cost allocation tags, Discussion CreateInternetGateway action (EC2), testing, Validation checks credentials\n\ntemporary, for a role, returned by AssumeRole API, Validation checks temporary, from AWS STS instead of IAM user, Discussion\n\ncross-account access to AWS resources, Discussion Cross-Region Replication (CRR), Discussion CSV files\n\ngenerating and exporting CSV of resources with EC2 Global View, Problem-Challenge transforming data in, using Glue DataBrew, Problem-Challenge\n\ncustomer-managed KMS key, Solution, Problem, Discussion CVSS (Common Vulnerability Scoring System), Discussion\n\nD",
      "content_length": 2045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "dashboard, creating for S3 Storage Lens, Steps-Discussion data, Introduction\n\n(see also big data)\n\nData Catalog\n\ncreating in Athena, Steps using Glue Data Catalog in Athena service, Discussion\n\ndata lakes, Discussion Data Lifecycle Manager, Discussion\n\nautomating EBS snapshots with, Discussion\n\ndatabase engines, determining current supported versions for, Fast Fixes Database Migration Service (see DMS) databases, Introduction-Challenge, Discussion\n\nautomating password rotation for RDS databases, Problem-Challenge autoscaling DynamoDB table provisioned capacity, Problem-Challenge AWS managed database services, Introduction creating Amazon Aurora serverless PostgreSQL database, Problem-Challenge enabling REST access to Aurora Serverless using RDS Data API, Problem-Challenge encrypting storage of existing RDS for MySQL database, Problem-Challenge leveraging RDS Proxy for database connections from Lambda, Problem-Challenge migrating to Amazon RDS using DMS, Problem-Challenge using IAM authentication with RDS database, Problem-Challenge\n\nDataSync, replicating data between EFS and S3 with, Problem-Challenge 2 Days in Transition action, Validation checks db-connect action, Discussion DBAppFunction Lambda function’s role, Steps DBInstanceStatus, Steps DBProxyTargets, Steps Deep Archive Access storage tier, Discussion deploying containers\n\nupdating containers with blue/green deployments, Problem-Challenge using Amazon Lightsail, Problem-Challenge using AWS Copilot, Problem-Challenge\n\nDescribeInstances action (EC2), testing, Validation checks Destination S3 bucket, Steps DHCP server (AWS-managed), options set, Discussion DMS (Database Migration Service), Discussion\n\nmigrating databases to Amazon RDS using DMS, Problem-Challenge\n\ndms.t2.medium replication instance size, Steps Docker",
      "content_length": 1802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 469,
      "content": "creating a Dockerfile, Steps installation and validation, Docker installation and validation\n\nDocker container image, pushing to ECR repository, Problem-Challenge Docker Desktop, Docker installation and validation Docker Linux Engine, Docker installation and validation Docker Swarm, Introduction DynamoDB\n\nautomating CSV import from S3 with Lambda, Problem-Challenge 3 autoscaling DynamoDB table provisioned capacity, Problem-Challenge populating data into a table, Fast Fixes using Glue crawlers to scan tables, Discussion\n\nE\n\nEBS snapshots, Discussion\n\nrestoring a file from, Problem-Challenge\n\nEBS volumes\n\nenabling encryption by default for new volumes in Region, Fast Fixes encrypting using KMS keys, Problem-Challenge 2\n\nebs-encryption-by-default option, Discussion EC2, Introduction\n\nconnecting to instances using AWS SSM Session Manager, Problem-Challenge creating and restoring EC2 backups to another Region using AWS Backup, Problem- Challenge granting instance the ability to access a secret, Steps instance in subnet of a VPC, connecting to the internet, Problem retrieving secret from Secrets Manager, Validation checks storing, encrypting, and accessing passwords with Secrets Manager, Problem- Challenge using EC2 Global View for account resource analysis, Problem-Challenge\n\nECR (Elastic Container Registry), Introduction\n\ncontainer images supported, Discussion deleting repositories containing images, Cleanup Docker Credential Helper, Steps pushing container image to, Solution-Challenge, Problem-Challenge scanning container images for vulnerabilities on push to ECR, Problem-Challenge 2 storage of container images on, Discussion\n\nECS (Elastic Container Service), Introduction\n\nautoscaling container workloads on, Problem-Challenge capturing logs from containers running on, Problem-Challenge Copilot requirement for ECS service-linked role, Steps",
      "content_length": 1868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "using EventBridge to trigger launch of container tasks on Fargate, Solution-Challenge\n\nECSRunTaskPermissionsForEvents policy, Steps EFS\n\nconfiguring Lambda function to access, Problem-See Also replicating data between EFS and S3 with DataSync, Problem-Challenge 2\n\nEIP (Elastic IP address), Steps\n\nassociated with NAT gatway, Discussion creating and associating to EC2 instance, Solution creating for use with NAT gateway, Steps\n\nElastic Cloud Compute (see EC2) Elastic Container Registry (see ECR) Elastic Container Service (Amazon ECS) (see ECS) Elastic Load Balancing (ELB) service, Introduction\n\ngiving permission to invoke Lambda functions, Solution\n\nelastic network interfaces (see ENIs) ElastiCache, accessing cluster having endpoint on a VPC, Problem-Challenge 1 email alerts for root login, setting up, Problem-Challenge encryption\n\nenabling by default for new EBS volumes in a Region, Fast Fixes encrypting EBS volumes using KMS keys, Problem-Challenge 2 encrypting storage of existing RDS for MySQL database, Problem-Challenge SSL encryption in transit, support by RDS Proxy, Discussion traffic in transit and at rest using TLS with DataSync, Discussion using S3 bucket keys with KMS to encrypt objects, Problem-Challenge\n\nendpoint policies (S3 VPC), Steps engine-mode of serverless, Steps ENIs (elastic network interfaces)\n\ndatabase subnet groups simplifying palcement of RDS ENIs, Steps of two EC2 instances, security group associated with, Problem security group acting as stateful virtual firewall for, Discussion subnets used for placement of, Discussion\n\nEvalDecision, Validation checks event-driven applications, Discussion, Discussion EventBridge\n\ninvoking a Lambda function on a schedule, Solution-Challenge rule searching for root logins and triggering SNS topic, Solution-Challenge using to trigger launch of ECS container tasks on Fargate, Solution-Challenge\n\nevents in your AWS account, monitoring with CloudTrail, Discussion\n\nF",
      "content_length": 1952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "Fargate, Prerequisites, Introduction\n\nlaunching Fargate container task in response to an event, Problem-Challenge\n\nfast fixes, Fast Fixes-Fast Fixes federation\n\nAWS accounts leveraging, Discussion identity, Solution\n\nform data, computer vision analysis of, Problem-Challenge Frequent Access storage tier, Discussion full-load-and-cdc, Discussion\n\nG\n\ngateway endpoint in your VPC, creating and associating with route tables, Steps Gateway Load Balancers, Discussion Gateway VPC endpoints, Discussion get-secret-value API call, Discussion Glacier archive (S3), automating archival of S3 objects to, Solution global condition context keys (IAM), Steps Glue service\n\nautomatically discovering metadata with Glue crawlers, Problem-Challenge\n\ncrawler configuration summary, Steps creating Glue Data Catalog database, Steps\n\ntransforming data with Glue DataBrew, Problem-Challenge\n\ngroups (IAM), Steps\n\nH\n\nHIPAA compliance for PHI, Discussion HTTP\n\nHTTP 301 response, Steps security group rules allowing HTTP traffic, Steps\n\nHTTPS\n\nredirecting HTTP traffic to with application load balancer, Problem-Challenge Session Manager, communicating with AWS Systems Manager (SSM) via, Discussion\n\nI\n\nIAM (Identity and Access Management)\n\naccess keys, AWS SSO and, Discussion access point policies, Steps, Discussion creating and assuming role for developer access, Problem-Challenge creating policy for secret access, Steps, Steps",
      "content_length": 1415,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "creating role for Kinesis Data Firehose, Steps creating role for RDS Proxy, Steps delegating administrative capabilities using permissions boundaries, Problem- Challenge enabling IAM user and role access to Billing console, Discussion generating least privilege IAM policy based on access patterns, Problem-Challenge role allowing S3 to copy objects from source to destination bucket, Steps role for Lambda function execution, IAM role for Lambda function execution service-linked roles, Steps setting up multi-factor authentication for root user in AWS account, Steps testing policies with IAM Policy Simulator, Problem-Challenge user password policies, enforcing in AWS account, Enforcing IAM User Password Policies in Your AWS Account-Challenge using IAM authentication with RDS database, Problem-Challenge\n\nIAM Access Analyzer (see Access Analyzer) identity federation, using to access AWS accounts, Solution IGW (see internet gateway) Infrequent Access storage class, Solution, Discussion, Discussion instance metadata (EC2), Steps instance profiles (EC2), Steps\n\ncreating and associating profile allowing access to secret, Solution\n\nIntelligent-Tiering archive policies, using to automatically archive S3 objects, Problem- Challenge 2 internet gateway\n\negress-only, for private subnets on VPC with IPv6 capability, Discussion using to connect VPC to the internet, Problem-Challenge\n\ninternet, using NAT gateway for outbound access from private subnets, Problem-Challenge IP addresses\n\nAWS-provided ranges list, Solution Elastic IP address in IPv4, Steps for ENIs, Discussion option to auto-assign on newly launched EC2 instances in a subnet, Discussion retrieving public IP from EC2 instance's metadata, Validation checks\n\nIPv4 CIDR blocks, Steps\n\nadditional, associating with your VPC, Discussion quota for, Discussion\n\nIPv6 CIDR blocks\n\nconfiguring for Amazon VPC , Discussion quota, Discussion\n\nJ",
      "content_length": 1905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "Java Database Connectivity (JDBC) data stores, using Glue crawlers to scan, Discussion jq utility, Steps, Prerequisites\n\nK\n\nKey Management Service (KMS), Introduction\n\ncreating KMS key to encrypt database snapshot, Steps encrypting EBS volumes using KMS keys, Problem-Challenge 2 specifying key for encyrypting RDS database snapshot, Steps using with S3 bucket keys to encrypt objects, Problem-Challenge\n\nkey rotation, automatic, on KMS service, Discussion Kinesis Client Library (KCL), Discussion Kinesis Data Analytics, Discussion Kinesis Data Firehose, streaming data to S3 with, Problem-Challenge Kinesis Producer Library (KPL), Discussion Kinesis Stream, using for ingestion of streaming data, Problem-Challenge KMS (see Key Management Service) KMS.NotFoundException error, Validation checks Kubernetes, Introduction\n\nL\n\nLambda functions\n\naccessing VPC resources with, Problem-Challenge 1 configuring application load balancer to invoke, Problem-Challenge 2 configuring to access EFS file system, Problem-See Also connection to RDS database, leveraging RDS Proxy for, Problem-Challenge IAM role for execution, IAM role for Lambda function execution integrating function with Secrets Manager to rotate RDS database passwords, Problem-Challenge invoking on a schedule, Problem-Challenge packaging Lambda code in container image, Problem-Challenge packaging libraries with Lambda Layers, Problem-Challenge reducing startup times with provisioned concurrency, Problem running trusted code in, using AWS Signer, Problem-Problem time out after 900 seconds, Discussion transforming data with, Discussion using EventBridge instead of for long-running jobs, Discussion\n\nlanding zone, Discussion least privilege access\n\nimplementing based on access patterns, Problem-Challenge",
      "content_length": 1771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "principle of least privilege, Discussion\n\nlibraries, packaging with Lambda Layers, Problem-Challenge lifecycle policies (S3), using to reduce storage costs, Problem-Challenge 2 Lightsail, deploying containers with, Problem-Challenge Linux, installing Docker on, Linux load balancers\n\nredirecting HTTP traffic to HTTPS with application load balancer, Problem-Challenge types other than ALB offered by AWS, Discussion\n\nlogging\n\ncapturing logs from containers running on ECS, Problem-Challenge CloudTrail, enabling for your account, Prerequisite deleting all log groups matching text pattern, Fast Fixes enabling CloudTrail logging for AWS account, Problem-Challenge Glue crawlers automatically logging information to CloudWatch Logs, Validation checks\n\nlogin profile, creating for a user, Steps low-code development platforms (LCDPs), Discussion\n\nM\n\nmachine learning (ML) (see AI/ML) MacOS, installing Docke Desktop on, MacOS management account, Discussion metadata, Discussion\n\nautomatically discovering with AWS Glue crawlers, Problem-Challenge EC2 instance, retrieving public IP from, Validation checks EC2 instances, Steps\n\nmetrics\n\nautoscaling metrics on CloudWatch, Discussion ECS service, on AWS Console, Validation checks observing for S3 storage using Storage Lens, Discussion\n\nmigration of databases\n\nmigrating databases to Amazon RDS using DMS, Problem-Challenge provisioned capacity type on RDS to Aurora Serverless, Discussion\n\nmounting and unmounting EBS volumes, Validation checks MP3-based audio, transcribing to text, Problem-Challenge multi-factor authentication (MFA), Discussion, Discussion\n\nsetting up for root user in your AWS account, Problem-Challenge\n\nMySQL\n\nRDS instance, encrypting storage of, Solution-Challenge RDS instance, leveraging RDS Proxy for database connections from Lambda, Problem",
      "content_length": 1818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "RDS instance, using IAM authentication with, Problem-Challenge\n\nN\n\nNAT gateway\n\nsharing of, enabled by Transit Gateway, Steps using for outbound internet access from private subnets, Problem-Challenge\n\nNcat utility, Validation checks network insights path, creating for EC2 instances, Steps Network Load Balancers, Discussion networking, Introduction-Challenge\n\nAWS services providing, Introduction connecting VPC to the internet using internet gateway, Problem-Challenge controlling network access to S3 from VPC using VPC endpoints, Problem-Challenge creating network tier with subnets and route table in a VPC, Problem-Challenge defining private virtual network in the cloud with Amazon VPC, Problem-Challenge deleting network interfaces associated with a security group, Fast Fixes enabling transitive cross-VPC connections using Transit Gateway, Problem-Challenge granting dynamic access by referencing security groups, Problem-Challenge innovations at AWS, resources on, Introduction peering VPCs together for inter-VPC network communication, Problem-Challenge redirecting HTTP traffic to HTTPS with application load balancer, Problem-Challenge simplifying management of CIDRs in security groups with prefix lists, Problem- Challenge using NAT gateway for outbound internet access from private subnets, Problem- Challenge using VPC Reachability Analyzer to verify and troubleshoot network paths, Problem- Challenge\n\nNGINX containers\n\ndeploying using Amazon Lightsail, Solution-Challenge nginx:latest image, Steps\n\nnotifications (S3), Steps\n\nO\n\nOpenSearch, Discussion OpenSSL CLI, generating self-signed certificate with, Steps orchestrators (container), Introduction Organizations (AWS), setting up, Introduction, Problem-Challenge 2 origin access identity (OAI)\n\nconfiguring to require S3bucket to be accessible only from CloudFront, Solution creating for CloudFront to reference S3 bucket polity, Steps",
      "content_length": 1910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": "OS X CLI, listening to MP3 file on, Steps\n\nP\n\npasswords\n\nautomating rotation for RDS databases, Problem-Challenge complex, generating with Secrets Manager, Steps enforcing IAM user password policies in AWS account, Enforcing IAM User Password Policies in Your AWS Account-Challenge storing, encrypting, and accessing with Secrets Manager, Problem-Challenge\n\npeering VPCs for inter-VPC network communication, Problem-Challenge permissions\n\nlocking down for SSM users, Discussion permission sets, Discussion\n\npermissions boundaries, using to delegate IAM administrative capabilities, Problem- Challenge PHI (protected health information), categorizing for further analysis, Problem-Discussion physician dictation analysis using Transcribe Medical and Comprehend Medical, Problem- Discussion PII (personally identifiable information), redacting from text using Comprehend, Problem- Challenge Policy Simulator (IAM), testing IAM policies with, Problem-Challenge Polly service, converting text to speech, Problem-Challenge PostgreSQL\n\nAurora Serverless database, allowing REST access using RDS Data API, Problem- Challenge creating Amazon Aurora serverless PostgreSQL database, Problem-Challenge\n\nPostgreSQL package, installing, Validation checks PowerUserAccess IAM policy, Solution, Discussion\n\nattaching to role, Steps\n\nprefix lists in security groups, managing CIDRs with, Problem-Challenge principle of least privilege, Discussion, Discussion\n\n(see also least privilege access)\n\nprivilege escalation, IAM service mitigating risk of, Steps producers and consumers (streaming data), Discussion Prometheus, Amazon Managed Service for, Introduction protected health information (PHI), categorizing for further analysis, Problem-Discussion provisioned capacity for DynamoDB table, autoscaling, Problem-Challenge provisioned concurrency, reducing Lambda startup times with, Problem-Challenge 2 Proxy ID, Steps public access, blocking for S3 bucket, Problem-Challenge",
      "content_length": 1960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "Q\n\nQuery Editor in RDS Console, Steps, Discussion quotas and limits (Kinesis service), Discussion\n\nR\n\nRDS Data API\n\nshort-lived database connections, Discussion using to enable REST access to Aurora Serverless, Problem-Challenge\n\nrds-data:CommitTransaction permission, Discussion rds-data:RollbackTransaction permission, Discussion Reachability Analyzer (VPC), using to verify and troubleshoot network paths, Problem- Challenge ReadCapacityUnits scaling target, Steps recovery point (backup), Steps recovery point objectives, replicating S3 buckets to meet, Problem-Challenge 2 recovery time objective (RTO), decreasing for EC2 instance using AWS Backups, Discussion redirect response for all HTTP traffic to HTTPS, Steps Redis Python package, installing, Steps Redis, Lambda function with Redis client, accessing VPC resources, Solution-Challenge 1 Regions\n\ncreating and restoring EC2 backups to another Region using AWS Backup, Problem- Challenge creating VPC in, spreading subnets across Availability Zones, Discussion listing all, Fast Fixes listing all that have a particular service, Fast Fixes listing Region names and endpoints in table, Fast Fixes setting by grabbing value from EC2 instance metadata, Fast Fixes stopping all running instances for current working Region, Fast Fixes for VPCs, Discussion\n\nRekognition Video, using to detect text in a video, Problem Relational Database Service (RDS), Introduction\n\nautomating password rotation for RDS databases, Problem-Challenge deleting RDS cluster, Cleanup leveraging RDS Proxy for database connections from Lambda, Problem-Challenge migrating databases to, using DMS, Problem-Challenge naming constraints, Steps using IAM authentication with RDS database, Problem-Challenge\n\nreplication",
      "content_length": 1749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "Aurora Serverless databases, Discussion DMS replication tasks, Solution-Challenge replicating data between EFS and S3 with DataSync, Problem-Challenge 2 replicating S3 buckets to meet recovery point objectives, Problem-Challenge 2\n\nReplicationStatus, Validation checks Resource Access Manager, Discussion REST\n\nenabling REST access to Aurora Serverless using RDS Data API, Problem-Challenge REST API exposed by RDS, Discussion\n\nrestores\n\nrestoring a file from an EBS snapshot, Problem restoring EC2 backups to another Region using AWS Backup, Steps\n\nroles\n\ncreating, Steps retrieving for a user, Steps\n\nroot user\n\nemail alerts for root login, setting up, Problem-Challenge setting up multi-factor authentication for, Problem-Challenge\n\nrotate-secret command (Secrets Manager), Steps Route 53 DNS records, Discussion route tables\n\nassociated with each subnet, route to direct traffic for peered VPCs to peering connection, Steps associating gateway endpoint in VPC with, Steps creating network tier with subnets and route table in a VPC, Problem-Challenge prefix lists associated wtih, Discussion priority given to most specific route, Discussion\n\nrouting, Solution\n\n(see also CIDR blocks) for Transit Gateway, Steps\n\nruntime interface client, Discussion\n\nS\n\nS3\n\nautomatically archiving S3 objects using Intelligent-Tiering, Problem-Challenge 1 AWS support for S3 interface endpoints, Discussion blocking public access for a bucket, Problem-Challenge configuring application-specific access to buckets with S3 access points, Problem- Challenge controlling network access to S3 from VPC using VPC endpoints, Problem-Challenge",
      "content_length": 1623,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "creating presigned URL for object that expires in a week, Fast Fixes CSV import into DynamoDB from S3 with Lambda, Problem-Challenge 3 lifecycle policies, using to reduce storage costs, Problem-Challenge 2 observing storage and access metrics using Storage Lens, Problem-Challenge 2 querying files using Amazon Athena, Problem-Challenge replicating buckets to meet recovery point objectives, Problem-Challenge 2 replicating files from S3 to EFS using DataSync, Problem-Challenge 2 serving web content securely from using CloudFront, Problem-Challenge streaming data to, using Kinesis Data Firehose, Problem-Challenge using bucket keys with KMS to encrypt objects, Problem-Challenge\n\nS3:GetObject action, Solution S3:PutObject action, Solution Same-Region Replication (SRR), Discussion scaling\n\nautoscaling by Kinesis Data Firehose, Discussion autoscaling capacity for database, Steps autoscaling container workloads on ECS, Problem-Challenge autoscaling DynamoDB table provisioned capacity, Problem-Challenge\n\nSchema Conversion Tool (SCT), Discussion schemas, Discussion secrets\n\ncreating using AWS CLI, Steps SECRET_ARN role, replacing, Steps storing, encrypting, and accessing passwords with Secrets Manager, Problem- Challenge\n\nSecrets Manager\n\ncreating and storing passwork in, Solution generating passwords with, Steps using to generate complex password, Steps using with Lambda function to automatically rotate RDS database passwords, Solution-Challenge\n\nSecretsManagerReadWrite policy, Steps, Steps security, Introduction-Challenge\n\nadministrative access, Discussion blocking public access for S3 bucket, Problem-Challenge encryption at rest, Discussion endpoint policy to restrict access to S3 buckets, Discussion fine-grained security capabilities on AWS, Introduction IAM role, creating and assuming for developer access, Problem-Challenge scanning container images for vulnerabilities on push to ECR, Problem-Challenge 2 security topics on AWS, Introduction",
      "content_length": 1968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "serving web content securely from S3 with CloudFront, Problem-Challenge trusted code, running in Lambda, Discussion workstation configuration, Workstation Configuration\n\nsecurity groups\n\nCIDR management using prefix lists, Problem-Challenge creating VPC security group for database, Steps deleting network interfaces associated with, Fast Fixes EC2 instances's security group allowing ingress traffic from ALB, Steps granting dynamic access to EC2 instances by referencing, Problem-Challenge for RDS Proxy, Steps\n\nrule allowing access on TCP port 3306 for Lambda APP function security group, Steps\n\nRDS MySQL database instance, ingress rule allowing access on TCP port 3306, Steps referencing in peered VPCs, Discussion referencing other security groups, Steps rules to allow HTTP and HTTPS traffic, Steps updating rule to allow access SSH access between instances, Steps\n\nSecurity Token Service (STS), Validation checks, Discussion self-referencing rule (security group), Steps serverless, Introduction-Challenge 1\n\naccessing VPC resources with Lambda, Problem-Challenge 1 automating CSV import into DynamoDB from S3 with Lambda, Problem-Challenge 3 benefits of, on AWS, Introduction configuring ALB to invoke Lambda function, Problem-Challenge 2 configuring Lambda function to access EFS, Problem-See Also packaging Lambda code in container image, Problem-Challenge packaging libraries with Lambda Layers, Problem-Challenge prerequisite, IAM role for Lambda function execution, IAM role for Lambda function execution reducing Lambda startup times with provisioned concurrency, Problem-Challenge 2 running trusted code in Lambda using AWS Signer, Problem-Problem services available on AWS, Introduction\n\nSession Manager (see SSM Session Manager) shards, Steps signing process for Lambda function code, Solution Single Sign-On (SSO), Solution, Introduction, Discussion\n\nsetting up AWS Organizations and SSO, Problem-Challenge 2\n\nsnapshots\n\nencrypting storage of RDS database using, Solution-Challenge skipping final snapshot when deleting RDS cluster, Cleanup",
      "content_length": 2059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "SNS topic, creating and subscribing to it, Solution-Challenge Source S3 bucket, configuring replication policy for, Steps SQL query, running on files stored in S3, Problem-Challenge SSH, allowing between EC2 instances, Problem-Challenge SSL\n\ncertificate, creating, Steps OpenSSL, Prerequisites\n\nSSM Session Manager\n\nconnecting to EC2 instance in subnet of VPC via, Validation checks connecting to EC2 instances with, Problem-Challenge\n\nSSML (Speech Synthesis Markup Language) tags, Steps SSO (see Single Sign-On) Standard storage class, Discussion storage, Introduction-Challenge 2\n\ncreating and restoring EC2 backups to another Region using AWS Backup, Problem- Challenge ElastiCache service implementing redis or memcached for, Discussion encrypting storage of existing RDS for MySQL database, Problem-Challenge restoring a file from an EBS snapshot, Problem-Challenge using Intelligent-Tiering archive policies to automatically archive S3 objects, Problem- Challenge 2 using S3 lifecycle policies to reduce storage costs, Problem-Challenge 2\n\nStorage Lens, observing S3 storage and access metrics with, Problem-Challenge 2 streaming data\n\nto S3 using Kinesis Data Firehose, Problem-Challenge using Kinesis Stream for ingestion of, Problem-Challenge\n\nsubnets\n\ncreating database subnet group, Steps creating network tier with subnets and route table in a VPC, Problem-Challenge outbound-only internet access for an EC2 instance in private subnets, Problem- Challenge\n\nSwitch Role feature, Validation checks\n\nT\n\ntables, Discussion Tag Editor, using to modify tags for many resources at one time, Problem-Challenge tagging containers, Steps, Discussion tags for AWS resources, Discussion target groups for Load Balancer, Steps text in an image, determining location of, Problem-Challenge",
      "content_length": 1786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "text to speech conversion, Problem-Challenge text, detecting in a video, Problem Textractor tool\n\nanalyzing output from Amazon Textract to determine location of text in an image, Problem-Challenge using for computer vision analysis of form data, Problem-Challenge\n\ntokens (authentication), Discussion\n\nIAM policy allowing Lambda function to generate, Steps\n\ntokens (authorization), Steps Transcribe Medical and Comprehend Medical, using for physician dictation analysis, Problem-Discussion transcribing a podcast, Problem-Challenge transformations\n\nLambda functions for, Discussion transforming data with AWS Glue DataBrew, Problem-Challenge\n\nTransit Gateway, enabling transitive cross-VPC connections, Problem-Challenge\n\nU\n\nuniversally unique identifiers (UUIDs), Validation checks user password policies, enforcing in AWS account, Enforcing IAM User Password Policies in Your AWS Account-Challenge\n\nV\n\nvideo, detecting text in, Problem-Challenge VPC (Virtual Private Cloud), Introduction\n\naccessing VPC resources with Lambda, Problem-Challenge 1 connecting to internet using internet gateway, Problem-Challenge controlling network access to S3 from VPC using VPC endpoints, Problem-Challenge creating, Problem-Challenge creating network tier with subnets and route table in a VPC, Problem default, finding for a Region, Fast Fixes enabling transitive cross-VPC connections using Transit Gateway, Problem-Challenge finding interface VPC endpoints for current Region, Fast Fixes peering two VPCs together for inter-VPC network communication, Problem-Challenge Reachability Analyzer, using to verify and troubleshoot network paths, Problem- Challenge using NAT gateway for outbound internet access from private subnets, Problem- Challenge\n\nVPN Endpoints for Session Manager, Discussion",
      "content_length": 1784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "W\n\nWindows, installing Docker Desktop on, Windows WorkSpaces gateways, list of CIDR ranges for, Solution\n\nY\n\nYaml input for CLI command, Fast Fixes",
      "content_length": 147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "About the Authors\n\nJohn Culkin is a senior solutions architect at AWS. He holds all current AWS certifications. Previously, he was a principal cloud architect lead at Cloudreach, where he led the delivery of cloud solutions aligned with organizational needs. A lifelong student of technology, he now focuses on creating transformative business solutions that utilize cloud services.\n\nMike Zazon is a senior cloud architect at AWS, focused on helping enterprise customers modernize their businesses. He previously held roles as a cloud software developer, software engineer, software architect, IT manager, and data center architect. His passion for technology education blossomed while serving in some of these roles within an engineering research university setting.",
      "content_length": 767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "Colophon\n\nThe animal on the cover of AWS Cookbook is the northern goshawk (Accipiter gentilis). A powerful predator, the northern goshawk belongs to the Accipitridae family (part of the “true hawk” subfamily) and can be found in the temperate parts of the Northern Hemisphere. The northern goshawk is the only species in the genus Accipiter living in coniferous and mixed forests in both Eurasia and North America, generally restricting itself to relatively open wooded areas or along the edges of a forest. It’s a migratory bird that ventures south during the winter.\n\nThe northern goshawk has relatively short, broad wings and a long tail to enable maneuverability within its forest habitat. For its species, it has a comparatively sizable beak, robust and fairly short legs, and thick talons. It has a blue-gray back and a brownish-gray or white underside with dark barring. These birds tend to show clinal variation in color, which means that goshawks further north are paler than those in warmer areas.\n\nThe northern goshawk is amazingly fast, and catches its prey by putting on short bursts of flight, often twisting among branches and crashing through thickets in its intensity. It’s a quiet predator that hunts by perching at mid-level heights and attacking quickly when it spots prey. The diet often consists of medium-sized birds, small mammals including squirrels and rabbits, small rodents, snakes, and insects.\n\nThis hawk lays around 2–4 bluish white eggs and is very territorial and protective of its nest, diving at intruders (including humans) and sometimes drawing blood. Young goshawks are usually looked after by the female, who remains with them most of the time while the male’s primary responsibility is to bring food to the nest. Goshawks typically mate for life and hence they function as a partnership with most things.\n\nGoshawks have a long and noble history. In the Middle Ages, only the nobility were permitted to fly goshawks for falconry. Ancient European falconry literature refers to goshawks as a yeoman’s bird or the cook’s bird because of their utility as a hunting partner catching edible prey. Currently, the northern goshawk’s conservation status is that of least concern as the population remains stable. However, it may falter with increased deforestation, which is a loss of habitat for these mighty birds. Many of the animals on O’Reilly covers are endangered; all of them are important to the world.\n\nThe cover illustration is by Karen Montgomery, based on a black and white engraving from British Birds. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
      "content_length": 2729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "Table of Contents\n\nForeword Preface\n\nWho This Book Is For What You Will Learn The Recipes What You Will Need Getting Started Conventions Used in This Book Using Code Examples O’Reilly Online Learning How to Contact Us Acknowledgments\n\n1. Security\n\n1.0. Introduction 1.1. Creating and Assuming an IAM Role for Developer Access 1.2. Generating a Least Privilege IAM Policy Based on Access Patterns 1.3. Enforcing IAM User Password Policies in Your AWS Account 1.4. Testing IAM Policies with the IAM Policy Simulator 1.5. Delegating IAM Administrative Capabilities Using Permissions Boundaries 1.6. Connecting to EC2 Instances Using AWS SSM Session Manager 1.7. Encrypting EBS Volumes Using KMS Keys 1.8. Storing, Encrypting, and Accessing Passwords Using Secrets Manager 1.9. Blocking Public Access for an S3 Bucket 1.10. Serving Web Content Securely from S3 with CloudFront\n\n2. Networking\n\n2.0. Introduction 2.1. Defining Your Private Virtual Network in the Cloud by Creating an Amazon VPC 2.2. Creating a Network Tier with Subnets and a Route Table in a VPC 2.3. Connecting Your VPC to the Internet Using an Internet Gateway 2.4. Using a NAT Gateway for Outbound Internet Access from Private Subnets 2.5. Granting Dynamic Access by Referencing Security Groups 2.6. Using VPC Reachability Analyzer to Verify and Troubleshoot Network Paths 2.7. Redirecting HTTP Traffic to HTTPS with an Application Load Balancer 2.8. Simplifying Management of CIDRs in Security Groups with Prefix Lists 2.9. Controlling Network Access to S3 from Your VPC Using VPC Endpoints 2.10. Enabling Transitive Cross-VPC Connections Using Transit Gateway 2.11. Peering Two VPCs Together for Inter-VPC Network Communication\n\n3. Storage\n\n3.0. Introduction 3.1. Using S3 Lifecycle Policies to Reduce Storage Costs 3.2. Using S3 Intelligent-Tiering Archive Policies to Automatically Archive S3 Objects 3.3. Replicating S3 Buckets to Meet Recovery Point Objectives 3.4. Observing S3 Storage and Access Metrics Using Storage Lens 3.5. Configuring Application-Specific Access to S3 Buckets with S3 Access Points",
      "content_length": 2076,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "3.6. Using Amazon S3 Bucket Keys with KMS to Encrypt Objects 3.7. Creating and Restoring EC2 Backups to Another Region Using AWS Backup 3.8. Restoring a File from an EBS Snapshot 3.9. Replicating Data Between EFS and S3 with DataSync\n\n4. Databases\n\n4.0. Introduction 4.1. Creating an Amazon Aurora Serverless PostgreSQL Database 4.2. Using IAM Authentication with an RDS Database 4.3. Leveraging RDS Proxy for Database Connections from Lambda 4.4. Encrypting the Storage of an Existing Amazon RDS for MySQL Database 4.5. Automating Password Rotation for RDS Databases 4.6. Autoscaling DynamoDB Table Provisioned Capacity 4.7. Migrating Databases to Amazon RDS Using AWS DMS 4.8. Enabling REST Access to Aurora Serverless Using RDS Data API\n\n5. Serverless\n\n5.0. Introduction 5.1. Configuring an ALB to Invoke a Lambda Function 5.2. Packaging Libraries with Lambda Layers 5.3. Invoking Lambda Functions on a Schedule 5.4. Configuring a Lambda Function to Access an EFS File System 5.5. Running Trusted Code in Lambda Using AWS Signer 5.6. Packaging Lambda Code in a Container Image 5.7. Automating CSV Import into DynamoDB from S3 with Lambda 5.8. Reducing Lambda Startup Times with Provisioned Concurrency 5.9. Accessing VPC Resources with Lambda\n\n6. Containers\n\n6.0. Introduction 6.1. Building, Tagging, and Pushing a Container Image to Amazon ECR 6.2. Scanning Images for Security Vulnerabilities on Push to Amazon ECR 6.3. Deploying a Container Using Amazon Lightsail 6.4. Deploying Containers Using AWS Copilot 6.5. Updating Containers with Blue/Green Deployments 6.6. Autoscaling Container Workloads on Amazon ECS 6.7. Launching a Fargate Container Task in Response to an Event 6.8. Capturing Logs from Containers Running on Amazon ECS\n\n7. Big Data\n\n7.0. Introduction 7.1. Using a Kinesis Stream for Ingestion of Streaming Data 7.2. Streaming Data to Amazon S3 Using Amazon Kinesis Data Firehose 7.3. Automatically Discovering Metadata with AWS Glue Crawlers 7.4. Querying Files on S3 Using Amazon Athena 7.5. Transforming Data with AWS Glue DataBrew\n\n8. AI/ML\n\n8.0. Introduction 8.1. Transcribing a Podcast 8.2. Converting Text to Speech 8.3. Computer Vision Analysis of Form Data",
      "content_length": 2185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "8.4. Redacting PII from Text Using Comprehend 8.5. Detecting Text in a Video 8.6. Physician Dictation Analysis Using Amazon Transcribe Medical and Comprehend Medical 8.7. Determining Location of Text in an Image\n\n9. Account Management\n\n9.0. Introduction 9.1. Using EC2 Global View for Account Resource Analysis 9.2. Modifying Tags for Many Resources at One Time with Tag Editor 9.3. Enabling CloudTrail Logging for Your AWS Account 9.4. Setting Up Email Alerts for Root Login 9.5. Setting Up Multi-Factor Authentication for a Root User 9.6. Setting Up AWS Organizations and AWS Single Sign-On\n\nFast Fixes Index About the Authors",
      "content_length": 628,
      "extraction_method": "Unstructured"
    }
  ]
}