{
  "metadata": {
    "title": "Kafka Streams in Action",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 282,
    "conversion_date": "2025-12-19T17:31:26.897393",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Kafka Streams in Action.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-9)",
      "start_page": 1,
      "end_page": 9,
      "detection_method": "topic_boundary",
      "content": "M A N N I N G\nWilliam P. Bejeck Jr.\nForeword by Neha Narkhede                                  \nReal-time apps and \nmicroservices with the \nKafka Streams API\n\n\nPatterns\nMasking\nSource\nElectronics\nsink\nCafe\nsink\nPatterns\nsink\nPurchases\nsink\nRewards\nBranch\nprocessor\nFiltering\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nRewards\nsink\nSelect-key\nprocessor\n \n\n\nKafka Streams\nin Action\nREAL-TIME APPS AND MICROSERVICES\nWITH THE KAFKA STREAMS API\nWILLIAM P. BEJECK JR.\nFOREWORD BY NEHA NARKHEDE\nM A N N I N G\nSHELTER ISLAND\n\n\nFor online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2018 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nManning Publications Co.\nAcquisitions editor: Michael Stephens\n20 Baldwin Road\nDevelopment editor: Frances Lefkowitz\nPO Box 761\nTechnical development editors: Alain Couniot, John Hyaduck\nShelter Island, NY 11964\nReview editor: Aleksandar Dragosavljevic´\nProject manager: David Novak\nCopy editors: Andy Carroll, Tiffany Taylor\nProofreader: Katie Tennant\nTechnical proofreader: Valentin Crettaz\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781617294471\nPrinted in the United States of America\n1 2 3 4 5 6 7 8 9 10 – DP – 23 22 21 20 19 18\n \n\n\niii\nbrief contents\nPART 1\nGETTING STARTED WITH KAFKA STREAMS ..................... 1\n1\n■\nWelcome to Kafka Streams 3\n2\n■\nKafka quickly 22\nPART 2\nKAFKA STREAMS DEVELOPMENT ...................................55\n3\n■\nDeveloping Kafka Streams 57\n4\n■\nStreams and state 84\n5\n■\nThe KTable API 117\n6\n■\nThe Processor API 145\nPART 3\nADMINISTERING KAFKA STREAMS ..............................173\n7\n■\nMonitoring and performance 175\n8\n■\nTesting a Kafka Streams application 199\nPART 4\nADVANCED CONCEPTS WITH KAFKA STREAMS .............215\n9\n■\nAdvanced applications with Kafka Streams 217\n \n\n\nv\ncontents\nforeword\nxi\npreface\nxiii\nacknowledgments\nxiv\nabout this book\nxv\nabout the author\nxix\nabout the cover illustration\nxx\nPART 1\nGETTING STARTED WITH KAFKA STREAMS ....................1\n1 \nWelcome to Kafka Streams\n3\n1.1\nThe big data movement, and how it changed \nthe programming landscape\n4\nThe genesis of big data\n4\n■Important concepts from \nMapReduce\n5\n■Batch processing is not enough\n8\n1.2\nIntroducing stream processing\n8\nWhen to use stream processing, and when not to use it\n9\n1.3\nHandling a purchase transaction\n10\nWeighing the stream-processing option\n10\n■Deconstructing the \nrequirements into a graph\n11\n \n\n\nCONTENTS\nvi\n1.4\nChanging perspective on a purchase transaction\n12\nSource node\n12\n■Credit card masking node\n12\nPatterns node\n13\n■Rewards node\n13\n■Storage node\n13\n1.5\nKafka Streams as a graph of processing nodes\n15\n1.6\nApplying Kafka Streams to the purchase \ntransaction flow\n16\nDefining the source\n16\n■The first processor: masking credit card \nnumbers\n17\n■The second processor: purchase patterns\n18\nThe third processor: customer rewards\n19\n■The fourth \nprocessor—writing purchase records\n20\n2 \nKafka quickly\n22\n2.1\nThe data problem\n23\n2.2\nUsing Kafka to handle data\n23\nZMart’s original data platform\n23\n■A Kafka sales transaction \ndata hub\n24\n2.3\nKafka architecture\n25\nKafka is a message broker\n26\n■Kafka is a log\n27\nHow logs work in Kafka\n27\n■Kafka and partitions\n28\nPartitions group data by key\n29\n■Writing a custom \npartitioner\n30\n■Specifying a custom partitioner\n31\nDetermining the correct number of partitions\n32\nThe distributed log\n32\n■ZooKeeper: leaders, followers, \nand replication\n33\n■Apache ZooKeeper\n33\n■Electing \na controller\n34\n■Replication\n34\n■Controller \nresponsibilities\n35\n■Log management\n37\nDeleting logs\n37\n■Compacting logs\n38\n2.4\nSending messages with producers\n40\nProducer properties\n42\n■Specifying partitions and \ntimestamps\n42\n■Specifying a partition\n43\nTimestamps in Kafka\n43\n2.5\nReading messages with consumers\n44\nManaging offsets\n44\n■Automatic offset commits\n46\nManual offset commits\n46\n■Creating the consumer\n47\nConsumers and partitions\n47\n■Rebalancing\n47\nFiner-grained consumer assignment\n48\n■Consumer example\n48\n2.6\nInstalling and running Kafka\n49\nKafka local configuration\n49\n■Running Kafka\n50\nSending your first message\n52\n \n\n\nCONTENTS\nvii\nPART 2\nKAFKA STREAMS DEVELOPMENT ........................ 55\n3 \nDeveloping Kafka Streams\n57\n3.1\nThe Streams Processor API\n58\n3.2\nHello World for Kafka Streams\n58\nCreating the topology for the Yelling App\n59\n■Kafka Streams \nconfiguration\n63\n■Serde creation\n63\n3.3\nWorking with customer data\n65\nConstructing a topology\n66\n■Creating a custom Serde\n72\n3.4\nInteractive development\n74\n3.5\nNext steps\n76\nNew requirements\n76\n■Writing records outside of Kafka\n81\n4 \nStreams and state\n84\n4.1\nThinking of events\n85\nStreams need state\n86\n4.2\nApplying stateful operations to Kafka Streams\n86\nThe transformValues processor\n87\n■Stateful customer \nrewards\n88\n■Initializing the value transformer\n90\nMapping the Purchase object to a RewardAccumulator \nusing state\n90\n■Updating the rewards processor\n94\n4.3\nUsing state stores for lookups and previously \nseen data\n96\nData locality\n96\n■Failure recovery and fault tolerance\n97\nUsing state stores in Kafka Streams\n98\n■Additional key/value \nstore suppliers\n99\n■StateStore fault tolerance\n99\n■Configuring \nchangelog topics\n99\n4.4\nJoining streams for added insight\n100\nData setup\n102\n■Generating keys containing customer \nIDs to perform joins\n103\n■Constructing the join\n104\nOther join options\n109\n4.5\nTimestamps in Kafka Streams\n110\nProvided TimestampExtractor implementations\n112\nWallclockTimestampExtractor\n113\n■Custom \nTimestampExtractor\n114\n■Specifying a \nTimestampExtractor\n115\n \n\n\nCONTENTS\nviii\n5 \nThe KTable API\n117\n5.1\nThe relationship between streams and tables\n118\nThe record stream\n118\n■Updates to records or the changelog\n119\nEvent streams vs. update streams\n122\n5.2\nRecord updates and KTable configuration\n123\nSetting cache buffering size\n124\n■Setting the commit \ninterval\n125\n5.3\nAggregations and windowing operations\n126\nAggregating share volume by industry\n127\n■Windowing \noperations\n132\n■Joining KStreams and KTables\n139\nGlobalKTables\n140\n■Queryable state\n143\n6 \nThe Processor API\n145\n6.1\nThe trade-offs of higher-level abstractions vs. \nmore control\n146\n6.2\nWorking with sources, processors, and sinks to create a \ntopology\n146\nAdding a source node\n147\n■Adding a processor node\n148\nAdding a sink node\n151\n6.3\nDigging deeper into the Processor API with a stock analysis \nprocessor\n152\nThe stock-performance processor application\n153\n■The process() \nmethod\n157\n■The punctuator execution\n158\n6.4\nThe co-group processor\n159\nBuilding the co-grouping processor\n161\n6.5\nIntegrating the Processor API and the \nKafka Streams API\n170\nPART 3\nADMINISTERING KAFKA STREAMS .....................173\n7 \nMonitoring and performance\n175\n7.1\nBasic Kafka monitoring\n176\nMeasuring consumer and producer performance\n176\nChecking for consumer lag\n178\n■Intercepting the producer \nand consumer\n179\n7.2\nApplication metrics\n182\nMetrics configuration\n184\n■How to hook into the collected \nmetrics\n185\n■Using JMX\n185\n■Viewing metrics\n189\n \n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 10-18)",
      "start_page": 10,
      "end_page": 18,
      "detection_method": "topic_boundary",
      "content": "CONTENTS\nix\n7.3\nMore Kafka Streams debugging techniques\n191\nViewing a representation of the application\n191\n■Getting \nnotification on various states of the application\n192\nUsing the StateListener\n193\n■State restore listener\n195\nUncaught exception handler\n198\n8 \nTesting a Kafka Streams application\n199\n8.1\nTesting a topology\n201\nBuilding the test\n202\n■Testing a state store in the topology\n204\nTesting processors and transformers\n205\n8.2\nIntegration testing\n208\nBuilding an integration test\n209\nPART 4\nADVANCED CONCEPTS WITH KAFKA STREAMS.... 215\n9 \nAdvanced applications with Kafka Streams\n217\n9.1\nIntegrating Kafka with other data sources\n218\nUsing Kafka Connect to integrate data\n219\n■Setting up \nKafka Connect\n219\n■Transforming data\n222\n9.2\nKicking your database to the curb\n226\nHow interactive queries work\n228\n■Distributing state stores\n229\nSetting up and discovering a distributed state store\n230\n■Coding \ninteractive queries\n232\n■Inside the query server\n234\n9.3\nKSQL\n237\nKSQL streams and tables\n238\n■KSQL architecture\n238\nInstalling and running KSQL\n240\n■Creating a KSQL \nstream\n241\n■Writing a KSQL query\n242\n■Creating \na KSQL table\n243\n■Configuring KSQL\n244\nappendix A\nAdditional configuration information\n245\nappendix B\nExactly once semantics\n251\nindex\n253\n \n\n\nxi\nforeword\nI believe that architectures centered around real-time event streams and stream pro-\ncessing will become ubiquitous in the years ahead. Technically sophisticated compa-\nnies like Netflix, Uber, Goldman Sachs, Bloomberg, and others have built out this type\nof large, event-streaming platform operating at massive scale. It’s a bold claim, but I\nthink the emergence of stream processing and the event-driven architecture will have\nas big an impact on how companies make use of data as relational databases did.\n Event thinking and building event-driven applications oriented around stream pro-\ncessing require a mind shift if you are coming from the world of request/response–style\napplications and relational databases. That’s where Kafka Streams in Action comes in.\n Stream processing entails a fundamental move away from command thinking\ntoward event thinking—a change that enables responsive, event-driven, extensible, flex-\nible, real-time applications. In business, event thinking opens organizations to real-\ntime, context-sensitive decision making and operations. In technology, event thinking\ncan produce more autonomous and decoupled software applications and, conse-\nquently, elastically scalable and extensible systems.\n In both cases, the ultimate benefit is greater agility—for the business and for the\nbusiness-facilitating technology. Applying event thinking to an entire organization is\nthe foundation of the event-driven architecture. And stream processing is the technol-\nogy that enables this transformation.\n Kafka Streams is the native Apache Kafka stream-processing library for building\nevent-driven applications in Java. Applications that use Kafka Streams can do sophisti-\ncated transformations on data streams that are automatically made fault tolerant and\n \n\n\nFOREWORD\nxii\nare transparently and elastically distributed over the instances of the application.\nSince its initial release in the 0.10 version of Apache Kafka in 2016, many companies\nhave put Kafka Streams into production, including Pinterest, The New York Times, Rabo-\nbank, LINE, and many more.\n Our goal with Kafka Streams and KSQL is to make stream processing simple\nenough that it can be a natural way of building event-driven applications that respond\nto events, not just a heavyweight framework for processing big data. In our model, the\nprimary entity isn’t the processing code: it’s the streams of data in Kafka.\n Kafka Streams in Action is a great way to learn about Kafka Streams, and to learn how\nit is a key enabler of event-driven applications. I hope you enjoy reading this book as\nmuch as I have!\n—NEHA NARKHEDE\nCofounder and CTO at Confluent, Cocreator of Apache Kafka\n \n\n\nxiii\npreface\nDuring my time as a software developer, I’ve had the good fortune to work with cur-\nrent software on exciting projects. I started out doing a mix of client-side and backend\nwork; but I found I preferred to work solely on the backend, so I made my home\nthere. As time went on, I transitioned to working on distributed systems, beginning\nwith Hadoop (then in its pre-1.0 release). Fast-forward to a new project, and I had an\nopportunity to use Kafka. My initial impression was how simple Kafka was to work\nwith; it also brought a lot of power and flexibility. I found more and more ways to inte-\ngrate Kafka into delivering project data. Writing producers and consumers was straight-\nforward, and Kafka improved the quality of our system.\n Then I learned about Kafka Streams. I immediately realized, “Why do I need\nanother processing cluster to read from Kafka, just to write back to it?” As I looked\nthrough the API, I found everything I needed for stream processing: joins, map val-\nues, reduce, and group-by. More important, the approach to adding state was superior\nto anything I had worked with up to that point.\n I’ve always had a passion for explaining concepts to other people in a way that is\nstraightforward and easy to understand. When the opportunity came to write about\nKafka Streams, I knew it would be hard work but worth it. I’m hopeful the hard work\nwill pay off in this book by demonstrating that Kafka Streams is a simple but elegant\nand powerful way to perform stream processing.\n \n\n\nxiv\nacknowledgments\nFirst and foremost, I’d like to thank my wife Beth and acknowledge all the support I\nreceived from her during this process. Writing a book is a time-consuming task, and\nwithout her encouragement, this book never would have happened. Beth, you are fan-\ntastic, and I’m very grateful to have you as my wife. I’d also like to acknowledge my\nchildren, who put up with Dad sitting in his office all day on most weekends and\naccepted the vague answer “Soon” when they asked when I’d be finished writing.\n Next, I thank Guozhang Wang, Matthias Sax, Damian Guy, and Eno Thereska, the\ncore developers of Kafka Streams. Without their brilliant insights and hard work,\nthere would be no Kafka Streams, and I wouldn’t have had the chance to write about\nthis game-changing tool.\n I thank my editor at Manning, Frances Lefkowitz, whose expert guidance and\ninfinite patience made writing a book almost fun. I also thank John Hyaduck for his\nspot-on technical feedback, and Valentin Crettaz, the technical proofer, for his excel-\nlent work reviewing the code. Additionally, I thank the reviewers for their hard work\nand invaluable feedback in making the quality of this book better for all readers:\nAlexander Koutmos, Bojan Djurkovic, Dylan Scott, Hamish Dickson, James Frohnhofer,\nJim Manthely, Jose San Leandro, Kerry Koitzsch, László Hegedüs, Matt Belanger,\nMichele Adduci, Nicholas Whitehead, Ricardo Jorge Pereira Mano, Robin Coe, Sumant\nTambe, and Venkata Marrapu.\n Finally, I’d like to acknowledge all the Kafka developers for building such high-\nquality software, especially Jay Kreps, Neha Narkhede, and Jun Rao—not just for start-\ning Kafka in the first place, but also for founding Confluent, a great and inspiring\nplace to work.\n \n\n\nxv\nabout this book\nI wrote Kafka Streams in Action to teach you how to get started with Kafka Streams and,\nto a lesser extent, how to work with stream processing in general. My approach to writ-\ning this book is a pair-programming perspective; I imagine myself sitting next to you\nas you write the code and learn the API. You’ll start by building a simple application,\nand you’ll layer on more features as you go deeper into Kafka Streams. You’ll learn\nabout testing and monitoring and, finally, wrap things up by developing an advanced\nKafka Streams application.\nWho should read this book\nKafka Streams in Action is for any developer wishing to get into stream processing.\nWhile not strictly required, knowledge of distributed programming will be helpful in\nunderstanding Kafka and Kafka Streams. Knowledge of Kafka itself is useful but not\nrequired; I’ll teach you what you need to know. Experienced Kafka developers, as well\nas those new to Kafka, will learn how to develop compelling stream-processing appli-\ncations with Kafka Streams. Intermediate-to-advanced Java developers who are famil-\niar with topics like serialization will learn how to use their skills to build a Kafka\nStreams application. The book’s source code is written in Java 8 and makes extensive\nuse of Java 8 lambda syntax, so experience with lambdas (even from another lan-\nguage) will be helpful.\n \n\n\nABOUT THIS BOOK\nxvi\nHow this book is organized: a roadmap\nThis book has four parts spread over nine chapters. Part 1 introduces a mental model\nof Kafka Streams to show you the big-picture view of how it works. These chapters also\nprovide the basics of Kafka, for those who need them or want a review:\n■\nChapter 1 provides some history of how and why stream processing became\nnecessary for handling real-time data at scale. It also presents the mental model\nof Kafka Streams. I don’t go over any code but rather describe how Kafka\nStreams works.\n■\nChapter 2 is a primer for developers who are new to Kafka. Those with more\nexperience with Kafka can skip this chapter and get right into Kafka Streams.\nPart 2 moves on to Kafka Streams, starting with the basics of the API and continuing\nto the more complex features:\n■\nChapter 3 presents a Hello World application and then presents a more realistic\nexample: developing an application for a fictional retailer, including advanced\nfeatures.\n■\nChapter 4 discusses state and explains how it’s sometimes required for stream-\ning applications. You’ll learn about state store implementations and how to per-\nform joins in Kafka Streams.\n■\nChapter 5 explores the duality of tables and streams, and introduces a new con-\ncept: the KTable. Whereas a KStream is a stream of events, a KTable is a stream\nof related events or an update stream.\n■\nChapter 6 goes into the low-level Processor API. Up to this point, you’ve been\nworking with the high-level DSL, but here you’ll learn how to use the Processor\nAPI when you need to write customized parts of an application.\nPart 3 moves on from developing Kafka Streams applications to managing Kafka\nStreams:\n■\nChapter 7 explains how to test a Kafka Streams application. You’ll learn how to\ntest an entire topology, unit-test a single processor, and use an embedded Kafka\nbroker for integration tests.\n■\nChapter 8 covers how to monitor your Kafka Streams application, both to see\nhow long it takes to process records and to locate potential processing bottle-\nnecks.\nPart 4 is the capstone of the book, where you’ll delve into advanced application devel-\nopment with Kafka Streams:\n■\nChapter 9 covers integrating existing data sources into Kafka Streams using\nKafka Connect. You’ll learn to include database tables in a streaming applica-\ntion. Then, you’ll see how to use interactive queries to provide visualization\nand dashboard applications while data is flowing through Kafka Streams, with-\nout the need for relational databases. The chapter also introduces KSQL,\n \n\n\nABOUT THIS BOOK\nxvii\nwhich you can use to run continuous queries over Kafka without writing any\ncode, by using SQL.\nAbout the code\nThis book contains many examples of source code both in numbered listings and\ninline with normal text. In both cases, source code is formatted in a fixed-width font\nlike this to separate it from ordinary text. \n In many cases, the original source code has been reformatted; we’ve added line\nbreaks and reworked indentation to accommodate the available page space in the\nbook. In rare cases, even this was not enough, and listings include line-continuation\nmarkers (➥). Additionally, comments in the source code have often been removed\nfrom the listings when the code is described in the text. Code annotations accompany\nmany of the listings, highlighting important concepts.\n Finally, it’s important to note that many of the code examples aren’t meant to\nstand on their own: they’re excerpts containing only the most relevant parts of what is\ncurrently under discussion. You’ll find all the examples from the book in the accom-\npanying source code in their complete form. Source code for the book’s examples is\navailable from GitHub at https://github.com/bbejeck/kafka-streams-in-action and\nthe publisher’s website at www.manning.com/books/kafka-streams-in-action.\n The source code for the book is an all-encompassing project using the build tool\nGradle (https://gradle.org). You can import the project into either IntelliJ or Eclipse\nusing the appropriate commands. Full instructions for using and navigating the source\ncode can be found in the accompanying README.md file.\nBook forum\nPurchase of Kafka Streams in Action includes free access to a private web forum run by\nManning Publications where you can make comments about the book, ask technical\nquestions, and receive help from the author and from other users. To access the\nforum, go to https://forums.manning.com/forums/kafka-streams-in-action. You can\nalso learn more about Manning’s forums and the rules of conduct at https://forums\n.manning.com/forums/about.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe author, whose contribution to the forum remains voluntary (and unpaid). We sug-\ngest you try asking him some challenging questions lest his interest stray! The forum\nand the archives of previous discussions will be accessible from the publisher’s website\nas long as the book is in print.\n \n\n\nABOUT THIS BOOK\nxviii\nOther online resources\n■\nApache Kafka documentation: https://kafka.apache.org\n■\nConfluent documentation: https://docs.confluent.io/current\n■\nKafka Streams documentation: https://docs.confluent.io/current/streams/index\n.html#kafka-streams\n■\nKSQL documentation: https://docs.confluent.io/current/ksql.html#ksql\n \n",
      "page_number": 10
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 19-27)",
      "start_page": 19,
      "end_page": 27,
      "detection_method": "topic_boundary",
      "content": "xix\nabout the author\nBill Bejeck, a contributor to Kafka, works at Confluent on the\nKafka Streams team. He has worked in software development for\nmore than 15 years, including 8 years focused exclusively on the\nbackend, specifically, handling large volumes of data; and on\ningestion teams, using Kafka to improve data flow to downstream\ncustomers. Bill is the author of Getting Started with Google Guava\n(Packt Publishing, 2013) and a regular blogger at “Random\nThoughts on Coding” (http://codingjunkie.net).\n \n\n\nxx\nabout the cover illustration\nThe figure on the cover of Kafka Streams in Action is captioned “Habit of a Turkish Gen-\ntleman in 1700.” The illustration is taken from Thomas Jefferys’ A Collection of the Dresses\nof Different Nations, Ancient and Modern (four volumes), London, published between\n1757 and 1772. The title page states that these are hand-colored copperplate engrav-\nings, heightened with gum arabic. Thomas Jefferys (1719–1771) was called “Geogra-\npher to King George III.” He was an English cartographer who was the leading map\nsupplier of his day. He engraved and printed maps for government and other official\nbodies and produced a wide range of commercial maps and atlases, especially of North\nAmerica. His work as a map maker sparked an interest in local dress customs of the\nlands he surveyed and mapped, which are brilliantly displayed in this collection.\n Fascination with faraway lands and travel for pleasure were relatively new phenom-\nena in the late eighteenth century, and collections such as this one were popular,\nintroducing both the tourist as well as the armchair traveler to the inhabitants of\nother countries. The diversity of the drawings in Jefferys’ volumes speaks vividly of the\nuniqueness and individuality of the world’s nations some 200 years ago. Dress codes\nhave changed since then, and the diversity by region and country, so rich at the time,\nhas faded away. It is now often hard to tell the inhabitant of one continent from\nanother. Perhaps we have traded a cultural and visual diversity for a more varied per-\nsonal life—certainly, a more varied and interesting intellectual and technical life.\n At a time when it is hard to tell one computer book from another, Manning celebrates\nthe inventiveness and initiative of the computer business with book covers based on the\nrich diversity of regional life of two centuries ago, brought back to life by Jefferys’ pictures.\n \n\n\nPart 1\nGetting started\nwith Kafka Streams\nIn part 1 of this book, we’ll discuss the big data era: how it began with the\nneed to process large amounts of data and eventually progressed to stream pro-\ncessing—processing data as it becomes available. We’ll also discuss what Kafka\nStreams is, and I’ll show you a mental model of how it works without any code so\nyou can focus on the big picture. We’ll also briefly cover Kafka to get you up to\nspeed on how to work with it.\n \n\n\n3\nWelcome to Kafka Streams\nIn this book, you’ll learn how to use Kafka Streams to solve your streaming applica-\ntion needs. From basic extract, transform, and load (ETL) to complex stateful\ntransformations to joining records, we’ll cover the components of Kafka Streams so\nyou can solve these kinds of challenges in your streaming applications.\n Before we dive into Kafka Streams, we’ll briefly explore the history of big data\nprocessing. As we identify problems and solutions, you’ll clearly see how the need\nfor Kafka, and then Kafka Streams, evolved. Let’s look at how the big data era got\nstarted and what led to the Kafka Streams solution.\nThis chapter covers\nUnderstanding how the big data movement \nchanged the programming landscape\nGetting to know how stream processing works \nand why we need it\nIntroducing Kafka Streams\nLooking at the problems solved by Kafka Streams\n \n\n\n4\nCHAPTER 1\nWelcome to Kafka Streams\n1.1\nThe big data movement, and how it changed \nthe programming landscape\nThe modern programming landscape has exploded with big data frameworks and\ntechnologies. Sure, client-side development has undergone transformations of its\nown, and the number of mobile device applications has exploded as well. But no mat-\nter how big the mobile device market gets or how client-side technologies evolve,\nthere’s one constant: we need to process more and more data every day. As the\namount of data grows, the need to analyze and take advantage of the benefits of that\ndata grows at the same rate.\n But having the ability to process large quantities of data in bulk (batch processing)\nisn’t always enough. Increasingly, organizations are finding that they need to process\ndata as it becomes available (stream processing). Kafka Streams, a cutting-edge approach\nto stream processing, is a library that allows you to perform per-event processing of\nrecords. Per-event processing means you process each record as soon as it’s avail-\nable—no grouping of data into small batches (microbatching) is required.\nNOTE\nWhen the need to process data as it arrives became more and more\napparent, a new strategy was developed: microbatching. As the name implies,\nmicrobatching is nothing more than batch processing, but with smaller quan-\ntities of data. By reducing the size of the batch, microbatching can sometimes\nproduce results more quickly; but microbatching is still batch processing,\nalthough at faster intervals. It doesn’t give you real per-event processing.\n1.1.1\nThe genesis of big data\nThe internet started to have a real impact on our daily lives in the mid-1990s. Since\nthen, the connectivity provided by the web has given us unparalleled access to infor-\nmation and the ability to communicate instantly with anyone, anywhere in the world.\nAn unexpected byproduct of all this connectivity emerged: the generation of massive\namounts of data.\n For our purposes, I’ll say that the big data era officially began in 1998, the year\nSergey Brin and Larry Page formed Google. Brin and Page developed a new way of\nranking web pages for searches: the PageRank algorithm. At a very high level, the Page-\nRank algorithm rates a website by counting the number and quality of links pointing\nto it. The assumption is that the more important or relevant a web page is, the more\nsites will refer to it.\n Figure 1.1 offers a graphical representation of the PageRank algorithm:\nSite A is the most important, because it has the most references pointing to it.\nSite B is somewhat important. Although it doesn’t have as many references, an\nimportant site does point to it.\nSite C is less important than A or B. More references are pointing to site C than\nsite B, but the quality of those references is lower.\nThe sites at the bottom (D through I) have no references pointing to them.\nThis makes them the least valuable.\n \n\n\n5\nThe big data movement, and how it changed the programming landscape\nThe figure is an oversimplification of the PageRank algorithm, but it gives you the\nbasic idea of how the algorithm works.\n At the time, PageRank was a revolutionary approach. Previously, searches on the web\nwere more likely to use Boolean logic to return results. If a website contained all or most\nof the terms you were looking for, that website was in the search results, regardless of the\nquality of the content. But running the PageRank algorithm on all internet content\nrequired a new approach—the traditional approaches to working with data took too\nlong. For Google to survive and grow, it needed to index all that content quickly\n(“quickly” being a relative term) and present quality results to the public.\n Google developed another revolutionary approach for processing all that data: the\nMapReduce paradigm. Not only did MapReduce enable Google to do the work it\nneeded to as a company, it inadvertently spawned an entire new industry in computing. \n1.1.2\nImportant concepts from MapReduce\nThe map and reduce functions weren’t new concepts when Google developed Map-\nReduce. What was unique about Google’s approach was applying those simple con-\ncepts at a massive scale across many machines.\n At its heart, MapReduce has roots in functional programming. A map function\ntakes some input and maps that input into something else without changing the origi-\nnal value. Here’s a simple example in Java 8, where a LocalDate object is mapped into\na String message, while the original LocalDate object is left unmodified:\nFunction<LocalDate, String> addDate =\n(date) -> \"The Day of the week is \" + date.getDayOfWeek();\nSite A\nSite B\nSite C\nSite D\nSite E\nSite F\nSite G\nSite H\nSite I\nFigure 1.1\nThe PageRank algorithm in action. The circles represent websites, and the larger \nones represent sites with more links pointing to them from other sites.\n \n\n\n6\nCHAPTER 1\nWelcome to Kafka Streams\nAlthough simple, this short example is sufficient for demonstrating what a map func-\ntion does.\n On the other hand, a reduce function takes a number of parameters and reduces\nthem down to a singular, or at least smaller, value. A good example of that is adding\ntogether all the values in a collection of numbers.\n To perform a reduction on a collection of numbers, you first provide an initial\nstarting value. In this case, we’ll use 0 (the identity value for addition). The next step\nis adding the seed value to the first number in the list. You then add the result of that\nfirst addition to the second number in the list. The function repeats this process until\nit reaches the last value, producing a single number.\n Here are the steps to reduce a List<Integer> containing the values 1, 2, and 3:\n0 + 1 = 1    \n1 + 2 = 3           \n3 + 3 = 6        \nAs you can see, a reduce function collapses results together to form smaller results. As\nin the map function, the original list of numbers is left unchanged.\n The following example shows an implementation of a simple reduce function\nusing a Java 8 lambda:\nList<Integer> numbers = Arrays.asList(1, 2, 3);\nint sum = numbers.reduce(0, (i, j) -> i + j );\nThe main topic of this book is not MapReduce, so we’ll stop our background discus-\nsion here. But some of the key concepts introduced by the MapReduce paradigm\n(later implemented in Hadoop, the original open source version based on Google’s\nMapReduce white paper) come into play in Kafka Streams:\nHow to distribute data across a cluster to achieve scale in processing\nThe use of key/value pairs and partitions to group distributed data together\nInstead of avoiding failure, embracing failure by using replication\nThe following sections look at these concepts in general terms. Pay attention, because\nyou’ll see them coming up again and again in the book.\nDISTRIBUTING DATA ACROSS A CLUSTER TO ACHIEVE SCALE IN PROCESSING\nWorking with 5 TB (5,000 GB) of data could be overwhelming for one machine. But if\nyou can split up the data and involve more machines, so each is processing a manage-\nable amount, your problem is minimized. Table 1.1 illustrates this clearly.\n As you can see from the table, you may start out with an unwieldy amount of data\nto process, but by spreading the load across more servers, you eliminate the difficulty\nAdds the seed value \nto the first number\nTakes the result from \nstep 1 and adds it to \nthe second number \nin the list\nAdds the sum of step 2 \nto the third number\n \n\n\n7\nThe big data movement, and how it changed the programming landscape\nof processing the data. The 1 GB of data in the last line of the table is something a lap-\ntop could easily handle.\n This is the first key concept to understand about MapReduce: by spreading the\nload across a cluster of machines, you can turn an overwhelming amount of data into\na manageable amount. \nUSING KEY/VALUE PAIRS AND PARTITIONS TO GROUP DISTRIBUTED DATA\nThe key/value pair is a simple data structure with powerful implications. In the previ-\nous section, you saw the value of spreading a massive amount of data over a cluster of\nmachines. Distributing your data solves the processing problem, but now you have the\nproblem of collecting the distributed data back together.\n To regroup distributed data, you can use the keys from the key/value pairs to par-\ntition the data. The term partition implies grouping, but I don’t mean grouping by\nidentical keys, but rather by keys that have the same hash code. To split data into par-\ntitions by key, you can use the following formula:\nint partition = key.hashCode() % numberOfPartitions;\nFigure 1.2 shows how you could apply a hashing function to take results from Olympic\nevents stored on separate servers and group them on partitions for different events.\nTable 1.1\nHow splitting up 5 TB improves processing throughput\nNumber of machines\nAmount of data processed per server\n10\n500 GB\n100\n50 GB\n1000\n5 GB\n5000\n1 GB\nswimming, result_1\nsprinting, result_3\nswimming, result_3\nsprinting, result_2\nswimming, result_2\nsprinting, result_1\nSwim results partition\nSprint results partition\nPartition = key.hashCode % 2\nFigure 1.2\nGrouping records by key on partitions. Even though the records start out on separate \nservers, they end up in the appropriate partitions.\n \n\n\n8\nCHAPTER 1\nWelcome to Kafka Streams\nAll the data is stored as key/value pairs. In the image below the key is the name of the\nevent, and the value is a result for an individual athlete.\n Partitioning is an important concept, and you’ll see detailed examples in later\nchapters. \nEMBRACING FAILURE BY USING REPLICATION\nAnother key component of Google’s MapReduce is the Google File System (GFS). Just\nas Hadoop is the open source implementation of MapReduce, Hadoop File System\n(HDFS) is the open source implementation of GFS.\n At a very high level, both GFS and HDFS split data into blocks and distribute\nthose blocks across a cluster. But the essential part of GFS/HDFS is the approach to\nserver and disk failure. Instead of trying to prevent failure, the framework embraces\nfailure by replicating blocks of data across the cluster (by default, the replication\nfactor is 3).\n By replicating data blocks on different servers, you no longer have to worry about\ndisk failures or even complete server failures causing a halt in production. Replication\nof data is crucial for giving distributed applications fault tolerance, which is essential\nfor a distributed application to be successful. You’ll see later how partitions and repli-\ncation work in Kafka Streams. \n1.1.3\nBatch processing is not enough\nHadoop caught on with the computing world like wildfire. It allowed people to pro-\ncess vast amounts of data and have fault tolerance while using commodity hardware\n(cost savings). But Hadoop/MapReduce is a batch-oriented process, which means you\ncollect large amounts of data, process it, and then store the output for later use. Batch\nprocessing is a perfect fit for something like PageRank because you can’t make deter-\nminations of what resources are valuable across the entire internet by watching user\nclicks in real time.\n But business also came under increasing pressure to respond to important ques-\ntions more quickly, such as these:\nWhat is trending right now?\nHow many invalid login attempts have there been in the last 10 minutes?\nHow is our recently released feature being utilized by the user base?\nIt was apparent that another solution was needed, and that solution emerged as stream\nprocessing. \n1.2\nIntroducing stream processing\nThere are varying definitions of stream processing. In this book, I define stream process-\ning as working with data as it’s arriving in your system. The definition can be further\nrefined to say that stream processing is the ability to work with an infinite stream of\ndata with continuous computation, as it flows, with no need to collect or store the data\nto act on it.\n \n",
      "page_number": 19
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 28-36)",
      "start_page": 28,
      "end_page": 36,
      "detection_method": "topic_boundary",
      "content": "9\nIntroducing stream processing\n Figure 1.3 represents a stream of data, with each circle on the line representing\ndata at a point in time. Data is continuously flowing, as data in stream processing is\nunbounded.\nWho needs to use stream processing? Anyone who needs quick feedback from an\nobservable event. Let’s look at some examples.\n1.2.1\nWhen to use stream processing, and when not to use it\nLike any technical solution, stream processing isn’t a one-size-fits-all solution. The\nneed to quickly respond to or report on incoming data is a good use case for stream\nprocessing. Here are a few examples:\nCredit card fraud—A credit card owner may not notice a card has been stolen,\nbut by reviewing purchases as they happen against established patterns (loca-\ntion, general spending habits), you may be able to detect a stolen credit card\nand alert the owner.\nIntrusion detection—Analyzing application log files after a breach has occurred\nmay be helpful to prevent future attacks or to improve security, but the ability to\nmonitor aberrant behavior in real time is critical.\nA large race, such as the New York City Marathon—Almost all runners will have a\nchip on their shoe, and when runners pass sensors along the course, you can\nuse that information to track the runners’ positions. By using the sensor data,\nyou can determine the leaders, spot potential cheating, and detect whether a\nrunner is potentially having problems.\nThe financial industry—The ability to track market prices and direction in real\ntime is essential for brokers and consumers to make effective decisions about\nwhen to sell or buy.\nOn the other hand, stream processing isn’t a solution for all problem domains. To\neffectively make forecasts of future behavior, for example, you need to use a large\namount of data over time to eliminate anomalies and identify patterns and trends.\nHere the focus is on analyzing data over time, rather than just the most current data:\nEconomic forecasting—Information is collected on many variables over an extended\nperiod of time in an attempt to make an accurate forecast, such as trends in\ninterest rates for the housing market.\nSchool curriculum changes—Only after one or two testing cycles can school adminis-\ntrators measure whether curriculum changes are achieving their goals.\nFigure 1.3\nThis marble diagram is a simple representation of stream processing. Each circle represents \nsome information or an event occurring at a particular point in time. The number of events is unbounded \nand moves continually from left to right.\n \n\n\n10\nCHAPTER 1\nWelcome to Kafka Streams\nHere are the key points to remember: If you need to report on or take action immedi-\nately as data arrives, stream processing is a good approach. If you need to perform\nin-depth analysis or are compiling a large repository of data for later analysis, a stream-\nprocessing approach may not be a good fit. Let’s now walk through a concrete exam-\nple of stream processing. \n1.3\nHandling a purchase transaction\nLet’s start by applying a general stream-processing approach to a retail sales example.\nThen we’ll look at how you can use Kafka Streams to implement the stream-processing\napplication.\n Suppose Jane Doe is on her way home from work and remembers she needs tooth-\npaste. She stops at a ZMart, goes in to pick up the toothpaste, and heads to the check-\nout to pay. The cashier asks Jane if she’s a member of the ZClub and scans her\nmembership card, so Jane’s membership info is now part of the purchase transaction.\n When the total is rung up, Jane hands the cashier her debit card. The cashier\nswipes the card and gives Jane the receipt. As Jane is walking out of the store, she\nchecks her email, and there’s a message from ZMart thanking her for her patronage,\nwith various coupons for discounts on Jane’s next visit.\n This transaction is a normal occurrence that a customer wouldn’t give a second\nthought to, but you’ll have recognized it for what it is: a wealth of information that can\nhelp ZMart run more efficiently and serve customers better. Let’s go back in time a lit-\ntle, to see how this transaction became a reality.\n1.3.1\nWeighing the stream-processing option\nSuppose you’re the lead developer for ZMart’s streaming-data team. ZMart is a big-\nbox retail store with several locations across the country. ZMart does great business,\nwith total sales for any given year upwards of $1 billion. You’d like to start mining the\ndata from your company’s transactions to make the business more efficient. You know\nyou have a tremendous amount of sales data to work with, so whatever technology you\nimplement will need to be able to work fast and scale to handle this volume of data.\n You decide to use stream processing because there are business decisions and\nopportunities that you can take advantage of as each transaction occurs. After data is\ngathered, there’s no reason to wait for hours to make decisions. You get together with\nmanagement and your team and come up with the following four primary require-\nments for the stream-processing initiative to succeed:\nPrivacy—First and foremost, ZMart values its relationship with its customers.\nWith all of today’s privacy concerns, your first goal is to protect customers’ pri-\nvacy, and protecting their credit card numbers is the highest priority. However\nyou use the transaction information, customer credit card information should\nnever be at risk of exposure.\nCustomer rewards—A new customer-rewards program is in place, with customers\nearning bonus points based on the amount of money they spend on certain\n \n\n\n11\nHandling a purchase transaction\nitems. The goal is to notify customers quickly, once they’ve received a reward—\nyou want them back in the store! Again, appropriate monitoring of activity is\nrequired here. Remember how Jane received an email immediately after leav-\ning the store? That’s the kind of exposure you want for the company.\nSales data—ZMart would like to refine its advertising and sales strategy. The\ncompany wants to track purchases by region to figure out which items are more\npopular in certain parts of the country. The goal is to target sales and specials\nfor best-selling items in a given area of the country.\nStorage—All purchase records need to be saved in an off-site storage center for\nhistorical and ad hoc analysis.\nThese requirements are straightforward enough on their own, but how would you go\nabout implementing them against a single purchase transaction like Jane Doe’s?\n1.3.2\nDeconstructing the requirements into a graph\nLooking at the preceding requirements, you can quickly recast them in a directed acyclic\ngraph (DAG). The point where the customer completes the transaction at the register\nis the source node for the entire graph. ZMart’s requirements become the child nodes\nof the main source node (figure 1.4).\nNext, you need to determine how to map a purchase transaction to the require-\nments graph. \nPatterns\nMasking\nRewards\nPurchase\nStorage\nFigure 1.4\nThe business \nrequirements for the streaming \napplication presented as a \ndirected acyclic graph. Each \nvertex represents a requirement, \nand the edges show the flow of \ndata through the graph.\n \n\n\n12\nCHAPTER 1\nWelcome to Kafka Streams\n1.4\nChanging perspective on a purchase transaction\nIn this section, we’ll walk through the steps of a purchase and see how it relates, at a\nhigh level, to the requirements graph from figure 1.4. In the next section, we’ll look at\nhow to apply Kafka Streams to this process.\n1.4.1\nSource node\nThe graph’s source node (figure 1.5) is where the application consumes the purchase\ntransaction. This node is the source of the sales transaction information that will flow\nthrough the graph. \n1.4.2\nCredit card masking node\nThe child node of the graph source is where the credit card masking takes place (fig-\nure 1.6). This is the first vertex or node in the graph that represents the business\nrequirements, and it’s the only node that receives the raw sales data from the source\nnode, effectively making this node the source for all other nodes connected to it.\nFor the credit card masking operation, you make a copy of the data and then convert\nall the digits of the credit card number to an x, except the last four digits. The data\nflowing through the rest of the graph will have the credit card field converted to the\nxxxx-xxxx-xxxx-1122 format. \nThe point of purchase is the source or\nparent node for the entire graph.\nPurchase\nFigure 1.5\nThe simple start for the sales \ntransaction graph. This node is the source of \nraw sales transaction information that will \nflow through the graph.\nCredit card numbers are masked\nhere for security purposes.\nMasking\nPurchase\nFigure 1.6\nThe first node in the graph that \nrepresents the business requirements. This \nnode is responsible for masking credit card \nnumbers and is the only node that receives \nthe raw sales data from the source node, \neffectively making it the source for all other \nnodes connected to it.\n \n\n\n13\nChanging perspective on a purchase transaction\n1.4.3\nPatterns node\nThe patterns node (figure 1.7) extracts the relevant information to establish where\ncustomers purchase products throughout the country. Instead of making a copy of the\ndata, the patterns node will retrieve the item, date, and ZIP code for the purchase and\ncreate a new object containing those fields. \n1.4.4\nRewards node\nThe next child node in the process is the rewards accumulator (figure 1.8). ZMart has\na customer rewards program that gives customers points for purchases made in the\nstore. This node’s role is to extract the dollar amount spent and the client’s ID and\ncreate a new object containing those two fields. \n1.4.5\nStorage node\nThe final child node writes the purchase data out to a NoSQL data store for further\nanalysis (figure 1.9).\n We’ve now tracked the example purchase transaction through ZMart’s graph of\nrequirements. Let’s see how you can use Kafka Streams to convert this graph into a\nfunctional streaming application. \nData is extracted here for\ndetermining purchase patterns.\nPatterns\nMasking\nPurchase\nFigure 1.7\nThe patterns node consumes purchase information from the \nmasking node and converts it into a record showing when a customer \npurchased an item and the ZIP code where the customer completed the \ntransaction.\n \n\n\n14\nCHAPTER 1\nWelcome to Kafka Streams\nData is pulled from the transaction here for\nuse in calculating customer rewards.\nPatterns\nMasking\nRewards\nPurchase\nFigure 1.8\nThe rewards node is \nresponsible for consuming sales records \nfrom the masking node and converting \nthem into records containing the total of \nthe purchase and the customer ID.\nPurchase is stored here to\nbe available for further\nad hoc analysis.\nPatterns\nMasking\nRewards\nPurchase\nStorage\nFigure 1.9\nThe storage node consumes records from the masking node as well. \nThese records aren’t converted into any other format but are stored in a NoSQL \ndata store for ad hoc analysis later.\n \n\n\n15\nKafka Streams as a graph of processing nodes\n1.5\nKafka Streams as a graph of processing nodes\nKafka Streams is a library that allows you to perform per-event processing of records.\nYou can use it to work on data as it arrives, without grouping data in microbatches.\nYou process each record as soon as it’s available.\n Most of ZMart’s goals are time sensitive, in that you want to take action as soon as\npossible. Preferably, you’ll be able to collect information as events occur. Additionally,\nthere are several ZMart locations across the country, so you’ll need all the transaction\nrecords to funnel into a single flow or stream of data for analysis. For these reasons,\nKafka Streams is a perfect fit. Kafka Streams allows you to process records as they\narrive and gives you the low-latency processing you require.\n In Kafka Streams, you define a topology of processing nodes (I’ll use the terms pro-\ncessor and node interchangeably). One or more nodes will have as source Kafka topic(s),\nand you can add additional nodes, which are considered child nodes (if you aren’t\nfamiliar with what a Kafka topic is, don’t worry—I'll explain in detail in chapter 2). Each\nchild node can define other child nodes. Each processing node performs its assigned\ntask and then forwards the record to each of its child nodes. This process of perform-\ning work and then forwarding data to any child nodes continues until every child\nnode has executed its function.\n Does this process sound familiar? It should, because you similarly transformed\nZMart’s business requirements into a graph of processing nodes. Traversing a graph is\nhow Kafka Streams works—it’s a DAG or topology of processing nodes.\n You start with a source or parent node, which has one or more children. Data\nalways flows from the parent to the child nodes, never from child to parent. Each\nchild node, in turn, can define child nodes of its own, and so on.\n Records flow through the graph in a depth-first manner. This approach has signifi-\ncant implications: each record (a key/value pair) is processed in full by the entire\ngraph before another record is forwarded through the topology. Because each record\nis processed depth-first through the whole DAG, there’s no need to have backpressure\nbuilt into Kafka Streams.\nDEFINITION\nThere are varying definitions of backpressure, but here I define it\nas the need to restrict the flow of data by buffering or using a blocking mech-\nanism. Backpressure is necessary when a source is producing data faster than a\nsink can receive and process that data.\nBy being able to connect or chain together multiple processors, you can quickly build\nup complex processing logic, while at the same time keeping each component rela-\ntively straightforward. It’s in this composition of processors that Kafka Streams’ power\nand complexity come into play.\nDEFINITION\nA topology is the way you arrange the parts of an entire system and\nconnect them with each other. When I say Kafka Streams has a topology, I’m\nreferring to transforming data by running through one or more processors.\n \n\n\n16\nCHAPTER 1\nWelcome to Kafka Streams\n1.6\nApplying Kafka Streams to the purchase \ntransaction flow\nLet’s build a processing graph again, but this time we’ll create a Kafka Streams pro-\ngram. To refresh your memory, figure 1.4 shows the requirements graph for ZMart’s\nbusiness requirements. Remember, the vertexes are processing nodes that handle\ndata, and the edges show the flow of data.\n Although you’ll be building a Kafka Streams program as you build your new graph,\nyou’ll still be taking a relatively high-level approach. Some details will be left out. We’ll\ngo into more detail later in the book when we look at the actual code.\n The Kafka Streams program will consume records, and when it does, you’ll convert\nthe raw records into Purchase objects. These pieces of information will make up a\nPurchase object:\nZMart customer ID (scanned from the member card)\nTotal dollar amount spent\nItem(s) purchased\nZIP code of the store where the purchase took place\nDate and time of the transaction\nDebit or credit card number\n1.6.1\nDefining the source\nThe first step in any Kafka Streams program is to establish a source for the stream.\nThe source could be any of the following:\nA single topic\nMultiple topics in a comma-separated list\nA regex that can match one or more topics\nIn this case, it will be a single topic named transactions. If any of these Kafka terms\nare unfamiliar to you, remember—they’ll be explained in chapter 2.\n It’s important to note that to Kafka, the Kafka Streams program looks like any\nother combination of consumers and producers. Any number of applications could\nKafka Streams and Kafka\nAs you might have guessed from the name, Kafka Streams runs on top of Kafka. In\nthis introductory chapter, you don’t need to know about Kafka, because we’re focus-\ning more how Kafka Streams works conceptually. A few Kafka-specific terms may be\nmentioned, but for the most part, we’ll be concentrating on the stream-processing\naspects of Kafka Streams.\nIf you’re new to Kafka or are unfamiliar with it, you’ll learn what you need to know\nabout Kafka in chapter 2. Knowledge of Kafka is essential for working effectively with\nKafka Streams. \n \n\n\n17\nApplying Kafka Streams to the purchase transaction flow\nbe reading from the same topic in conjunction with your streaming program. Figure 1.10\nrepresents the source node in the topology. \n1.6.2\nThe first processor: masking credit card numbers\nNow that you have a source defined, you can start creating processors that will work\non the data. Your first goal is to mask the credit card numbers recorded in the incom-\ning purchase records. The first processor will convert credit card numbers from some-\nthing like 1234-5678-9123-2233 to xxxx-xxxx-xxxx-2233.\n The KStream.mapValues method will perform the masking represented in fig-\nure 1.11. It will return a new KStream instance with values masked as specified by a\nValueMapper. This particular KStream instance will be the parent processor for any\nother processors you define.\nCREATING PROCESSOR TOPOLOGIES\nEach time you create a new KStream instance by using a transformation method,\nyou’re in essence building a new processor that’s connected to the other processors\nalready created. By composing processors, you can use Kafka Streams to create com-\nplex data flows elegantly.\n It’s important to note that calling a method that returns a new KStream instance\ndoesn’t cause the original instance to stop consuming messages. A transforming method\nSource\nFigure 1.10\nThe source node: a Kafka topic\nChild node of the source node\nSource node consuming message from\nthe Kafka transaction topic\nSource\nMasking\nFigure 1.11\nThe masking processor is a \nchild of the main source node. It receives \nall the raw sales transactions and emits \nnew records with the credit card number \nmasked.\n \n",
      "page_number": 28
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 37-44)",
      "start_page": 37,
      "end_page": 44,
      "detection_method": "topic_boundary",
      "content": "18\nCHAPTER 1\nWelcome to Kafka Streams\ncreates a new processor and adds it to the existing processor topology. The updated\ntopology is then used as a parameter to create the next KStream instance, which starts\nreceiving messages from the point of its creation.\n It’s very likely that you’ll build new KStream instances to perform additional trans-\nformations while retaining the original stream for its original purpose. You’ll work\nwith an example of this when you define the second and third processors.\n It’s possible to have a ValueMapper convert an incoming value to an entirely new\ntype, but in this case it will return an updated copy of the Purchase object. Using a\nmapper to update an object is a pattern you’ll see frequently.\n You should now have a clear image of how you can build up your processor pipe-\nline to transform and output data.\n1.6.3\nThe second processor: purchase patterns\nThe next processor to create is one that can capture information necessary for deter-\nmining purchase patterns in different regions of the country (figure 1.12). To do this,\nyou’ll add a child-processing node to the first processor (KStream) you created. The\nfirst processor produces Purchase objects with the credit card number masked.\n The purchase-patterns processor receives a Purchase object from its parent node\nand maps the object to a new PurchasePattern object. The mapping process extracts\nHere the Purchase object is “mapped”\nto a PurchasePatterns object.\nThe child processor node of the\npatterns processor has a child node\nthat writes the PurchasePatterns object\nout to the patterns topic. The\nformat is JSON.\npatterns\ntopic\nPatterns\nMasking\nSource\nFigure 1.12\nThe purchase-pattern processor takes Purchase objects and converts \nthem into PurchasePattern objects containing the items purchased and the ZIP \ncode where the transaction took place. A new processor takes records from the \npatterns processor and writes them out to a Kafka topic.\n \n\n\n19\nApplying Kafka Streams to the purchase transaction flow\nthe item purchased (toothpaste, for example) and the ZIP code it was bought in and\nuses that information to create the PurchasePattern object. We’ll go over exactly how\nthis mapping process occurs in chapter 3.\n Next, the purchase-patterns processor adds a child processor node that receives\nthe new PurchasePattern object and writes it out to a Kafka topic named patterns.\nThe PurchasePattern object is converted to some form of transferable data when it’s\nwritten to the topic. Other applications can then consume this information and use it\nto determine inventory levels as well as purchasing trends in a given area. \n1.6.4\nThe third processor: customer rewards\nThe third processor will extract information for the customer rewards program (fig-\nure 1.13). This processor is also a child node of the original processor. It receives the\nPurchase objects and maps them to another type: the RewardAccumulator object.\n The customer rewards processor also adds a child-processing node to write the\nRewardAccumulator object out to a Kafka topic, rewards. By consuming records from\nthe rewards topic, other applications can determine rewards for ZMart customers and\nproduce, for example, the email that Jane Doe received. \nHere the Purchase object is “mapped”\nto a RewardAccumulator object.\nThe child processor node of the\nRewards processor has a child node\nthat writes the RewardAccumulator\nobject out to the rewards topic.\nThe format is JSON.\npatterns\ntopic\nPatterns\nMasking\nSource\nRewards\nrewards\ntopic\nFigure 1.13\nThe customer rewards processor is responsible for transforming Purchase objects \ninto a RewardAccumulator object containing the customer ID, date, and dollar amount of the \ntransaction. A child processor writes the Rewards objects to another Kafka topic.\n \n\n\n20\nCHAPTER 1\nWelcome to Kafka Streams\n1.6.5\nThe fourth processor—writing purchase records\nThe last processor is shown in figure 1.14. This is the third child node of the masking\nprocessor node, and it writes the entire masked purchase record out to a topic called\npurchases. This topic will be used to feed a NoSQL storage application that will con-\nsume the records as they come in. These records will be used for later analysis.\nAs you can see, the first processor, which masks the credit card number, feeds three\nother processors: two that further refine or transform the data, and one that writes the\nmasked results to a topic for further use by other consumers. By using Kafka Streams,\nyou can build up a powerful processing graph of connected nodes to perform stream\nprocessing on your incoming data. \nSummary\nKafka Streams is a graph of processing nodes that combine to provide powerful\nand complex stream processing.\nBatch processing is powerful, but it’s not enough to satisfy real-time needs for\nworking with data.\nThis last processor writes out\nthe purchase transaction as\nJSON to the purchases topic,\nwhich is consumed by a NoSQL\nstorage engine.\npatterns\ntopic\nPatterns\nMasking\nSource\nRewards\nrewards\ntopic\npurchases\ntopic\nFigure 1.14\nThe final processor is responsible for writing out the entire Purchase object to \nanother Kafka topic. The consumer for this topic will store the results in a NoSQL store such as \nMongoDB.\n \n\n\n21\nSummary\nDistributing data, key/value pairs, partitioning, and data replication are critical\nfor distributed applications.\nTo understand Kafka Streams, you should know some Kafka. For those who don’t\nknow Kafka, we’ll cover the essentials in chapter 2:\nInstalling Kafka and sending a message\nExploring Kafka’s architecture and what a distributed log is\nUnderstanding topics and how they’re used in Kafka\nUnderstanding how producers and consumers work and how to write them\neffectively\nIf you’re already comfortable with Kafka, feel free to go straight to chapter 3, where\nwe’ll build a Kafka Streams application based on the example discussed in this chapter.\n \n\n\n22\nKafka quickly\nAlthough this is a book about Kafka Streams, it’s impossible to explore Kafka\nStreams without discussing Kafka. After all, Kafka Streams is a library that runs on\nKafka.\n Kafka Streams is designed very well, so it’s possible to get up and running with\nlittle or no Kafka experience, but your progress and ability to fine-tune Kafka will\nbe limited. Having a good fundamental knowledge of Kafka is essential to get the\nmost out of Kafka Streams.\nNOTE\nThis chapter is for developers who are interested in getting started\nwith Kafka Streams but have little or no experience with Kafka itself. If you\nhave a good working knowledge of Kafka, feel free to skip this chapter and\nproceed directly to chapter 3.\nThis chapter covers\nExamining the Kafka architecture\nSending messages with producers\nReading messages with consumers\nInstalling and running Kafka\n \n\n\n23\nUsing Kafka to handle data\nKafka is too large a topic to cover in its entirety in one chapter. I’ll cover enough to\ngive you a good understanding how Kafka works and a few of the core configuration\nsettings you’ll need to know. For in-depth coverage of Kafka, take a look at Kafka in\nAction by Dylan Scott (Manning, 2018).\n2.1\nThe data problem\nOrganizations today are swimming in data. Internet companies, financial businesses,\nand large retailers are better positioned now than ever to use this data, both to serve\ntheir customers better and to find more efficient ways of conducting business. (We’re\ngoing to take a positive outlook on this situation and assume only good intentions\nwhen looking at customer data.)\n Let’s consider the various requirements you’d like to have in the ZMart data-\nmanagement solution:\nYou need a way to send data to a central storage quickly.\nBecause machines frequently fail, you also need the ability to have your data\nreplicated, so those inevitable failures don’t cause downtime and data loss.\nYou need the potential to scale to any number of consumers of data without\nhaving to keep track of different applications. You need to make the data avail-\nable to anyone in an organization, but not have to keep track of who has and\nhas not viewed the data.\n2.2\nUsing Kafka to handle data\nIn chapter 1, you were introduced to the large retail company ZMart. At that point,\nZMart wanted a streaming platform to use the company’s sales data in order to offer\nbetter customer service and improve sales overall. But six months before that, ZMart\nwas looking to get a handle on its data situation. ZMart had a custom solution that ini-\ntially worked well but had become unmanageable for reasons you’ll soon see.\n2.2.1\nZMart’s original data platform\nOriginally, ZMart was a small company that had retail sales data flowing into its system\nfrom separate applications. This worked fine initially, but over time it became evident\nthat a new approach would be needed. Data from sales in one department is not of\ninterest only to that department. Several parts of the company are interested, and\neach part has a different take on what’s important and how they want the data struc-\ntured. Figure 2.1 shows ZMart’s original data platform.\n Over time, ZMart continued to grow by acquiring other companies and expanding\nits offerings in existing stores. With each addition, the connections between applica-\ntions become more complicated. What started out as a handful of applications com-\nmunicating with each other turned into a veritable pile of spaghetti. As you can see in\nfigure 2.2, even with just three applications, the number of connections is cumber-\nsome and confusing. You can see how adding new applications will make this data\narchitecture unmanageable over time. \n \n\n\n24\nCHAPTER 2\nKafka quickly\n2.2.2\nA Kafka sales transaction data hub\nA solution to ZMart’s problem is to create one intake process to hold all transaction\ndata—a transaction data hub. This transaction data hub should be stateless, accepting\nSales\nAuditing\nMarketing\nFigure 2.1\nThe original data architecture for ZMart was simple enough to \nhave information flowing to and from each source of information.\nSales\nAcquired company A\nAuditing\nMarketing\nAcquired company C\nAcquired company B\nCustomer info\nFigure 2.2\nWith more applications being added over time, connecting all these information sources has \nbecome complex.\n \n\n\n25\nKafka architecture\ntransaction data and storing it in such a fashion that any consuming application can\npull the information it needs. It will be up to the consuming application to keep track\nof what it’s seen. The transaction data hub will only know how long it’s been holding\nany transaction data, and when that data should be rolled off or deleted.\n In case you haven’t guessed it yet, we have the perfect use case here for Kafka.\nKafka is a fault-tolerant, robust publish/subscribe system. A single Kafka node is called\na broker, and multiple Kafka servers make up a cluster. Kafka stores messages written by\nproducers in topics. Consumers subscribe to topics and contact Kafka to see if messages\nare available in those subscribed topics. Figure 2.3 shows how you can envision Kafka\nas the sales transaction data hub.\nYou’ve seen an overview of Kafka from 50,000 feet. We’ll take a closer look in the fol-\nlowing sections. \n2.3\nKafka architecture\nIn the next several subsections, we’ll look at the key parts of Kafka’s architecture and\nat how Kafka works. If you’re interested in kicking the tires on Kafka sooner rather\nthan later, skip ahead to section 2.6, on installing and running Kafka. After you’ve got\nit installed, come back here to continue learning about Kafka.\nSales\nWith Kafka as a sales transaction\ndata hub, the architecture becomes\nmuch simpler. Each application only\nneeds to know how to read/write to\nKafka. Adding or removing\napplications has no effect on other\napplications in the data processing.\nAcquired company A\nMarketing\nAcquired company C\nAcquired company B\nCustomer info\nAuditing\nKafka\nFigure 2.3\nUsing Kafka as a sales transaction hub simplifies the ZMart data architecture \nsignificantly. Now each machine doesn’t need to know about every other source of \ninformation. All they need to know is how to read from and write to Kafka.\n \n",
      "page_number": 37
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 45-53)",
      "start_page": 45,
      "end_page": 53,
      "detection_method": "topic_boundary",
      "content": "26\nCHAPTER 2\nKafka quickly\n2.3.1\nKafka is a message broker\nEarlier, I stated that Kafka is a publish/subscribe system, but it would be more precise\nto say that Kafka acts as a message broker. A broker is an intermediary that brings\ntogether two parties that don’t necessarily know each other for a mutually beneficial\nexchange or deal. Figure 2.4 shows the evolution of the ZMart data infrastructure.\nThe producers and consumers have been added to show how the individual parts\ncommunicate with Kafka. They don’t communicate directly with each other.\nKafka stores messages in topics and retrieves messages from topics. There’s no direct\nconnection between the producers and the consumers of the messages. Additionally,\nKafka doesn’t keep any state regarding the producers or consumers. It acts solely as a\nmessage clearinghouse.\n The underlying technology of a Kafka topic is a log, which is a file that Kafka\nappends incoming records to. To help manage the load of messages coming into a\ntopic, Kafka uses partitions. We discussed partitions in chapter 1, and you may recall\nthat one use of partitions is to bring data located on different machines together on\nthe same server. We’ll discuss partitions in detail shortly. \nSales\nIn this simpliﬁed view of Kafka,\nwe’re assuming a cluster is installed.\nAll output is sent from a producer, and\ninput is consumed by a consumer.\nAcquired company A\nConsumer\nMarketing\nAcquired company C\nAcquired company B\nCustomer info\nAuditing\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nKafka\nZooKeeper\nA cluster of ZooKeeper nodes communicates\nwith Kafka to maintain topic info and keep\ntrack of brokers in the cluster.\nFigure 2.4\nKafka is a message broker. Producers send messages to Kafka, and those messages \nare stored and made available to consumers via subscriptions to topics.\n \n\n\n27\nKafka architecture\n2.3.2\nKafka is a log\nThe mechanism underlying Kafka is the log. Most software engineers are familiar with\nlogs that track what an application’s doing. If you’re having performance issues or\nerrors in your application, the first place to check is the application logs. But that’s a\ndifferent sort of log. In the context of Kafka (or any other distributed system), a log is\n“an append-only, totally ordered sequence of records ordered by time.”1\n Figure 2.5 shows what a log looks like. An application appends records to the end\nof the log as they arrive. Records have an implied ordering by time, even though there\nmight not be a timestamp associated with each record, because the earliest records\nare to the left and the last record to arrive is at the right end.\nLogs are a simple data abstraction with powerful implications. If you have records in\norder with respect to time, resolving conflicts or determining which update to apply\nto different machines becomes straightforward: the latest record wins.\n Topics in Kafka are logs that are segregated by topic name. You could almost\nthink of topics as labeled logs. If the log is replicated among a cluster of machines,\nand a single machine goes down, it’s easy to bring that server back up: just replay\nthe log file. The ability to recover from failure is precisely the role of a distributed\ncommit log.\n We’ve only scratched the surface of a very deep topic when it comes to distributed\napplications and data consistency, but what you’ve seen so far should give you a basic\nunderstanding of what’s going on under the covers with Kafka.\n2.3.3\nHow logs work in Kafka\nWhen you install Kafka, one of the configuration settings is log.dir, which specifies\nwhere Kafka stores log data. Each topic maps to a subdirectory under the specified log\ndirectory. There will be as many subdirectories as there are topic partitions, with a for-\nmat of partition-name_partition-number (I’ll cover partitions in the next section). Inside\n1 Jay Kreps, “The Log: What Every Software Engineer Should Know About Real-time Data’s Unifying Abstrac-\ntion,” http://mng.bz/eE3w.\nFirst record to arrive\nLatest record to arrive\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFigure 2.5\nA log is a file where incoming records are appended—each \nnewly arrived record is placed immediately after the last record received. \nThis process orders the records in the file by time.\n \n\n\n28\nCHAPTER 2\nKafka quickly\neach directory is the log file where incoming messages are appended. Once the log\nfiles reach a certain size (either a number of records or size on disk), or when a con-\nfigured time difference between message timestamps is reached, the log file is\n“rolled,” and Kafka appends incoming messages to a new log (see figure 2.6).\nYou can see that logs and topics are highly connected concepts. You could say that a\ntopic is a log, or that it represents a log. The topic name gives you a good handle on\nwhich log the messages sent to Kafka via producers will be stored in. Now that we’ve cov-\nered the concept of logs, let’s discuss another fundamental concept in Kafka: partitions. \n2.3.4\nKafka and partitions\nPartitions are a critical part of Kafka’s design. They’re essential for performance, and\nthey guarantee that data with the same keys will be sent to the same consumer and in\norder. Figure 2.7 shows how partitions work.\nThe logs directory is conﬁgured in the root at /logs.\n/logs\n/logs/topicA_0               topicA has one partition.\n/logs/topicB_0               topicB has three partitions.\n/logs/topicB_1\n/logs/topicB_2\nFigure 2.6\nThe logs directory is the base \nstorage for messages. Each directory under \n/logs represents a topic partition. Filenames \nwithin the directory start with the name of the \ntopic, followed by an underscore, which is \nfollowed by a partition number.\nPartition 2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPartition 1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPartition 0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThe numbers shown in the\nrectangles are the offsets\nfor the messages.\nAs messages or records come in,\nthey are written to a partition\n(assigned by producer) and\nappended in time order to\nthe end of the log.\nTopic with partitions\nData comes into a single\ntopic but is placed into\nindividual partitions either\n(0, 1, or 2). Because\nthere are no keys with\nthese messages,\npartitions are assigned\nin round-robin fashion.\nEach partition is in strictly\nincreasing order, but there’s\nno order across partitions.\nFigure 2.7\nKafka uses partitions to achieve high throughput and spread the messages for an \nindividual topic across several machines in the cluster.\n \n\n\n29\nKafka architecture\nPartitioning a topic essentially splits the data forwarded to a topic across parallel\nstreams, and it’s key to how Kafka achieves its tremendous throughput. I explained\nthat a topic is a distributed log; each partition is similarly a log unto itself and follows\nthe same rules. Kafka appends each incoming message to the end of the log, and all\nmessages are strictly time-ordered. Each message has an offset number assigned to it.\nThe order of messages across partitions isn’t guaranteed, but the order of messages\nwithin each partition is guaranteed.\n Partitioning serves another purpose, aside from increasing throughput. It allows\ntopic messages to be spread across several machines so that the capacity of a given\ntopic isn’t limited to the available disk space on one server.\n Now let’s look at another critical role partitions play: ensuring messages with the\nsame keys end up together. \n2.3.5\nPartitions group data by key\nKafka works with data in key/value pairs. If the keys are null, the Kafka producer will\nwrite records to partitions chosen in a round-robin fashion. Figure 2.8 shows how par-\ntition assignment operates with non-null keys.\nIf the keys aren’t null, Kafka uses the following formula (shown in pseudocode) to\ndetermine which partition to send the key/value pair to:\nHashCode.(key) % number of partitions\nBy using a deterministic approach to select a partition, records with the same key will\nalways be sent to the same partition and in order. The default partitioner uses this\nPartition 1\nPartition 0\nhashCode(barBytes) % 2 = 1\nhashCode(fooBytes) % 2 = 0\nOnce the partition is determined, the message\nis appended to the appropriate log.\nIncoming messages:\n{foo, message data}\n{bar, message data}\nare used to determine\npartition the\nMessage keys\nwhich\nThese\nmessage should go to.\nkeys are not null.\nThe bytes of the key are used to calculate the hash.\nFigure 2.8\n“foo” is sent to partition 0, and \n“bar” is sent to partition 1. You obtain the \npartition by hashing the bytes of the key, \nmodulus the number of partitions.\n \n\n\n30\nCHAPTER 2\nKafka quickly\napproach; if you need a different strategy for selecting partitions, you can provide a\ncustom partitioner. \n2.3.6\nWriting a custom partitioner\nWhy would you want to write a custom partitioner? Of the several possible reasons,\nwe’ll look at one simple case here—the use of composite keys.\n Suppose you have purchase data flowing into Kafka, and the keys contain two val-\nues: a customer ID and a transaction date. But you need to group values by cus-\ntomer ID, so taking a hash of the customer ID and the purchase date won’t work. In\nthis case, you’ll need to write a custom partitioner that knows which part of the com-\nposite key determines which partition to use. For example, the composite key found\nin src/main/java/bbejeck/model/PurchaseKey.java (source code can be found on\nthe book’s website here: https://manning.com/books/kafka-streams-in-action) is\nshown in the following listing.\npublic class PurchaseKey {\nprivate String customerId;\nprivate Date transactionDate;\npublic PurchaseKey(String customerId, Date transactionDate) {\nthis.customerId = customerId;\nthis.transactionDate = transactionDate;\n}\npublic String getCustomerId() {\nreturn customerId;\n}\npublic Date getTransactionDate() {\nreturn transactionDate;\n}\n}\nWhen it comes to partitioning, you need to ensure that all transactions for a particular\ncustomer go to the same partition, but using the key in its entirety won’t enable this to\nhappen. Because purchases happen on many dates, including the date will result in\ndifferent key values for a single customer, placing the transactions across random par-\ntitions. You need to ensure you send all transactions with the same customer ID to the\nsame partition. The only way to do that is to only use the customer ID when determin-\ning the partition.\n The following example custom partitioner does what’s required. PurchaseKey-\nPartitioner (from src/main/java/bbejeck/chapter_2/partitioner/PurchaseKey-\nPartitioner.java) extracts the customer ID from the key to determine which partition\nto use.\nListing 2.1\nPurchaseKey composite key\n \n\n\n31\nKafka architecture\npublic class PurchaseKeyPartitioner extends DefaultPartitioner {\n@Override\npublic int partition(String topic, Object key,\nbyte[] keyBytes, Object value,\nbyte[] valueBytes, Cluster cluster) {\n        Object newKey = null;\nif (key != null) {                                  \nPurchaseKey purchaseKey = (PurchaseKey) key;\nnewKey = purchaseKey.getCustomerId();\nkeyBytes = ((String) newKey).getBytes();         \n}\nreturn super.partition(topic, newKey, keyBytes, value, \n   valueBytes, cluster);                                       \n}\n}\nThis custom partitioner extends DefaultPartitioner. You could implement the\nPartitioner interface directly, but there’s existing logic in DefaultPartitioner that\nwe’re using in this case.\n Keep in mind that when creating a custom partitioner, you aren’t limited to using\nonly the key. Using the value alone, or the value in combination with the key, is valid\nas well.\nNOTE\nThe Kafka API provides a Partitioner interface that you can use to\nwrite a custom partitioner. We won’t be covering writing a partitioner from\nscratch, but the principles are the same as those in the listing 2.2.\nYou’ve just seen how to construct a custom partitioner. Next, let’s wire up the parti-\ntioner with Kafka. \n2.3.7\nSpecifying a custom partitioner\nNow that you’ve written a custom partitioner, you need to tell Kafka you want to use it\ninstead of the default partitioner. Although we haven’t covered producers yet, you\nspecify a different partitioner when configuring the Kafka producer:\npartitioner.class=bbejeck_2.partitioner.PurchaseKeyPartitioner\nBy setting a partitioner per producer instance, you’re free to use any partitioner class\nfor any producer. We’ll go over producer configuration in detail when we cover using\nKafka producers.\nWARNING\nYou must exercise some caution when choosing the keys you use\nand when selecting parts of a key/value pair to partition on. Make sure the\nkey you choose has a fair distribution across all of your data. Otherwise, you’ll\nListing 2.2\nPurchaseKeyPartitioner custom partitioner\nIf the key isn’t \nnull, extracts \nthe customer ID\nSets the key \nbytes to the \nnew value\nReturns the partition with the\nupdated key, delegating to\nthe superclass\n \n\n\n32\nCHAPTER 2\nKafka quickly\nend up with a data-skew problem, because most of your data will be located\non just a few of your partitions. \n2.3.8\nDetermining the correct number of partitions\nChoosing the number of partitions to use when creating a topic is part art and part sci-\nence. One of the key considerations is the amount of data flowing into a given topic.\nMore data implies more partitions for higher throughput. But as with anything in life,\nthere are trade-offs.\n Increasing the number of partitions increases the number of TCP connections and\nopen file handles. Additionally, how long it takes to process an incoming record in a\nconsumer will also determine throughput. If you have heavyweight processing in your\nconsumer, adding more partitions may help, but ultimately the slower processing will\nhinder performance.2\n2.3.9\nThe distributed log\nWe’ve discussed the concepts of logs and partitioned topics. Let’s take a minute to\nlook at those two concepts together to demonstrate distributed logs.\n So far, we’ve focused on logs and topics on one Kafka server or broker, but typically\na Kafka production cluster environment includes several machines. I’ve intentionally\nkept the discussion centered on a single node, as it’s easier to understand the con-\ncepts when considering one node. But in practice, you’ll always be working with a clus-\nter of machines in Kafka.\n When a topic is partitioned, Kafka doesn’t allocate those partitions on one machine—\nKafka spreads them across several machines in the cluster. As Kafka appends records\nto a log, Kafka is distributing those records across several machines by partition. In fig-\nure 2.9, you can see this process in action.\n Let’s walk through a quick example using figure 2.9 as a guide. For this example,\nwe’ll assume one topic and null keys, so the producer assigns partitions in a round-\nrobin manner.\n The producer sends its first message to partition 0 on Kafka broker 1, the second\nmessage to partition 1 on Kafka broker 1, and the third message to partition 2 on\nKafka broker 2. When the producer sends its sixth message, it goes to partition 5 on\nKafka broker 3, and the next message starts over, going to partition 0 on Kafka bro-\nker 1. Message distribution continues in this manner, spreading message traffic across\nall nodes in the Kafka cluster.\n Although storing data remotely may sound risky, because a server can go down,\nKafka offers data redundancy. Data is replicated to one or more machines in the\ncluster as you write to one broker in Kafka (we’ll cover replication in an upcoming\nsection). \n2 Jun Rao, “How to Choose the Number of Topics/Partitions in a Kafka Cluster?” http://mng.bz/4C03.\n \n\n\n33\nKafka architecture\n2.3.10 ZooKeeper: leaders, followers, and replication\nSo far, we’ve discussed the role topics play in Kafka, and how and why topics are parti-\ntioned. You’ve seen that partitions aren’t all located on one machine but are spread\nout on brokers throughout the cluster. Now it’s time to look at how Kafka provides\ndata availability in the face of machine failures.\n Kafka has the notion of leader and follower brokers. In Kafka, for each topic parti-\ntion, one broker is chosen as the leader for the other brokers (the followers). One of\nthe chief duties of the leader is to assign replication of topic partitions to the follower\nbrokers. Just as Kafka allocates partitions for a topic across the cluster, Kafka also\nreplicates the partitions across machines. Before we go into the details of how leaders,\nfollowers, and replication work, we need to discuss the technology Kafka uses to\nachieve this.\n2.3.11 Apache ZooKeeper\nIf you’re a complete Kafka newbie, you may be asking yourself, “Why are we talking\nabout Apache ZooKeeper in a Kafka book?” Apache ZooKeeper is integral to Kafka’s\nThis topic has 6 partitions on a 3-node Kafka\ncluster. The log for the topic is spread across\nthe 3 nodes. Only the leader brokers for the\ntopic partitions are shown here.\nIf no keys are associated with the\nmessages, partitions are chosen\nin a round-robin fashion. Otherwise,\npartitions are determined by the hash\nof the key, modulus the number of partitions.\nPartition 1\nPartition 0\nPartition 2\nPartition 3\nPartition 4\nPartition 5\nKafka broker 1\nKafka broker 2\nKafka broker 3\nProducer\nFigure 2.9\nA producer writes messages to partitions of a topic. If no key is associated with the \nmessage, the producer chooses a partition in a round-robin fashion. Otherwise, the hash of the key, \nmodulus the number of partitions is used.\n \n\n\n34\nCHAPTER 2\nKafka quickly\narchitecture, and it’s ZooKeeper that enables Kafka to have leader brokers and to do\nsuch things as track the replication of topics (https://zookeeper.apache.org):\nZooKeeper is a centralized service for maintaining configuration information, naming,\nproviding distributed synchronization, and providing group services. All of these kinds of\nservices are used in some form or another by distributed applications.\nGiven that Kafka is a distributed application, it should start to be clear how ZooKeeper\nis involved in Kafka’s architecture. For this discussion, we’ll only consider Kafka instal-\nlations where there are two or more Kafka servers installed.\n In a Kafka cluster, one of the brokers is “elected” as the controller. We covered parti-\ntions in the previous section and discussed how Kafka spreads partitions across differ-\nent machines in the cluster. Topic partitions have a leader and follower(s) (the level\nof replication determines the degree of replication). When producing messages,\nKafka sends the record to the broker that is the leader for the record’s partition. \n2.3.12 Electing a controller\nKafka uses ZooKeeper to elect the controller broker. Discussing the consensus algo-\nrithms involved is way beyond the scope of this book, so we’ll take the 50,000-foot view\nand just state that ZooKeeper elects a broker from the cluster to be the controller.\n If the controlling broker fails or becomes unavailable for any reason, ZooKeeper\nelects a new controller from a set of brokers that are considered to be caught up with\nthe leader (an in-sync replica [ISR]). The brokers that make up this set are dynamic,\nand ZooKeeper recognizes only brokers in this set for election as leader.3\n2.3.13 Replication\nKafka replicates records among brokers to ensure data availability, should a broker in\nthe cluster fail. You can set the level of replication for each topic (as you saw in our\nprevious example of publishing and consuming) or for all topics in the cluster. Fig-\nure 2.10 demonstrates the replication flow between brokers.\n The Kafka replication process is straightforward. Brokers that follow a topic parti-\ntion consume messages from the topic-partition leader and append those records to\ntheir log. As discussed in the previous section, follower brokers that are caught up\nwith their leader broker are considered to be ISRs. ISR brokers are eligible to be\nelected leader, should the current leader fail or become unavailable.4 \n \n \n \n3 Kafka documentation, “Replicated Logs: Quorums, ISRs, and State Machines (Oh my!),” http://kafka.apache\n.org/documentation/#design_replicatedlog.\n4 Kafka documentation, “Replication,” http://kafka.apache.org/documentation/#replication.\n \n",
      "page_number": 45
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 54-61)",
      "start_page": 54,
      "end_page": 61,
      "detection_method": "topic_boundary",
      "content": "35\nKafka architecture\n2.3.14 Controller responsibilities\nThe controller broker is responsible for setting up leader/follower relationships for\nall partitions of a topic. If a Kafka node dies or is unresponsive (to ZooKeeper heart-\nbeats), all of its assigned partitions (both leader and follower) are reassigned by the\ncontroller broker. Figure 2.11 illustrates a controller broker in action.5\n The figure shows a simple failure scenario. In step 1, the controller broker detects\nthat broker 3 isn’t available. In step 2, the controller broker reassigns the leadership\nof the partition on broker 3 to broker 2.\n \n \n \n \n \n \n5 Some of the information in this section came from answers given by Gwen Shapira, “What is the actual role\nof Zookeeper in Kafka? What benefits will I miss out on if I don’t use Zookeeper and Kafka together?” on\nQuora at http://mng.bz/25Sy.\nThe topic foo has 2 partitions and a replication\nlevel of 3. Dashed lines between partitions point\nto the leader of the given partition. Producers\nwrite records to the leader of a partition, and\nthe followers read from the leader.\nBroker 1 is the leader for partition 0 and\nis a follower for partition 1 on broker 3.\nfoo topic partition 1\nKafka broker 1\nfoo topic partition 0\nfoo topic partition 0\nKafka broker 2\nfoo topic partition 1\nfoo topic partition 1\nKafka broker 3\nfoo topic partition 0\nBroker 2 is a follower for partition 0 on broker\n1 and a follower for partition 1 on broker 3.\nBroker 3 is a follower for partition 0 on\nbroker 1 and the leader for partition 1.\nFigure 2.10\nBrokers 1 and 3 are leaders for one topic partition and followers for another, whereas \nbroker 2 is a follower only. Follower brokers copy data from the leader broker.\n \n\n\n36\nCHAPTER 2\nKafka quickly\nZooKeeper is also involved in the following aspects of Kafka operations:\nCluster membership—Joining a cluster and maintaining membership in a cluster.\nIf a broker becomes unavailable, ZooKeeper removes the broker from cluster\nmembership.\nTopic configuration—Keeping track of the topics in a cluster, which broker is the\nleader for a topic, how many partitions there are for a topic, and any specific\nconfiguration overrides for a topic.\nAccess control—Identifying who can read from and write to particular topics.\nYou’ve now seen why Kafka has a dependency on Apache ZooKeeper. It’s ZooKeeper\nthat enables Kafka to have a leader broker with followers. The head broker has the\ncritical role of assigning topic partitions for replication to the followers, as well as reas-\nsigning them when a member broker fails. \nThe topic foo has 2 partitions and a replication\nlevel of 3. These are the initial leaders and\nfollowers:\nBroker 1 leader partition 0, follower partition 1\nBroker 2 follower partition 0, follower partition 1\nBroker 3 follower partition 0, leader partition 1\nBroker 3 has become unresponsive.\nStep 2: The controller has reassigned the\nleadership of partition\nfrom broker 3 to\n1\nbroker 2. All records for partition\nwill go\n1\nto broker 2, and broker\nwill now consume\n1\nmessages for partition\nfrom broker 2.\n1\nStep\n: As the leader, broker\n1\n1\nhas detected that broker 3\nhas failed.\nfoo topic partition 1\nKafka broker 1\nfoo topic partition 0\nfoo topic partition 0\nKafka broker 2\nfoo topic partition 1\nfoo topic partition 1\nKafka broker 3\nfoo topic partition 0\nFigure 2.11\nThe controller broker is responsible for assigning other brokers to be the leader broker \nfor some topics/partitions and followers for other topics/partitions. When a broker becomes \nunavailable, the controller broker will reassign the failed broker’s assignments to other brokers \nin the cluster.\n \n\n\n37\nKafka architecture\n2.3.15 Log management\nWe’ve covered appending messages, but we haven’t talked about how logs are man-\naged as they continue to grow. The amount of space on spinning disks in a cluster is a\nfinite resource, so it’s important for Kafka to remove messages over time. When it\ncomes to removing old data in Kafka, there are two approaches: the traditional log-\ndeletion approach, and compaction. \n2.3.16 Deleting logs\nThe log-deletion strategy is a two-phased approach: first, the logs are rolled into seg-\nments, and then the oldest segments are deleted. To manage the increasing size of the\nlogs, Kafka rolls them into segments. The timing of log rolling is based on time-\nstamps embedded in the messages. Kafka rolls a log when a new message arrives,\nand its timestamp is greater than the timestamp of the first message in the log plus\nthe log.roll.ms configuration value. At that point, the log is rolled and a new seg-\nment is created as the new active log. The previous active segment is still used to\nretrieve messages for consumers.\n Log rolling is a configuration setting you can specify when setting up a Kafka bro-\nker.6 There are two options for log rolling:\n\nlog.roll.ms—This is the primary configuration, but there’s no default value.\n\nlog.roll.hours—This is the secondary configuration, which is only used if\nlog.role.ms isn’t set. It defaults to 168 hours.\nOver time, the number of segments will continue to grow, and older segments will need\nto be deleted to make room for incoming data. To handle the deletion, you can specify\nhow long to retain the segments. Figure 2.12 illustrates the process of log rolling.\n Like log rolling, the removal of segments is based on timestamps in the messages\nand not just the clock time or time when the file was last modified. Log-segment\ndeletion is based on the largest timestamp in the log. Here are three settings, listed\nin order of priority, meaning that configurations earlier in the list override the later\nentries:\n\nlog.retention.ms—How long to keep a log file in milliseconds\n\nlog.retention.minutes—How long to keep a log file in minutes\n\nlog.retention.hours—Log file retention in hours\nI present these settings based on the assumption of high-volume topics, where you’re\nguaranteed to reach the maximum file size in a given time period. Another configura-\ntion setting, log.retention.bytes, could be specified with a longer rolling-time\nthreshold to keep down I/O operations. Finally, to guard against the case of a signifi-\ncant spike in volume when there are relatively large roll settings, the log.segment\n.bytes setting governs how large an individual log segment can be.\n6 Kafka documentation, “Broker Configs,” http://kafka.apache.org/documentation/#brokerconfigs.\n \n\n\n38\nCHAPTER 2\nKafka quickly\nThe deletion of logs works well for non-keyed records, or records that stand alone.\nBut if you have keyed data and expected updates, there’s another method that will suit\nyour needs better. \n2.3.17 Compacting logs\nConsider the case where you have keyed data, and you’re receiving updates for that\ndata over time, meaning a new record with the same key will update the previous\nvalue. For example, a stock ticker symbol could be the key, and the price per share\nwould be the regularly updated value. Imagine you’re using that information to dis-\nplay stock values, and you have a crash or restart—you need to be able to start back up\nwith the latest data for each key.7\n If you use the deletion policy, a segment could be removed between the last update\nand the application’s crash or restart. You wouldn’t have all the records on startup. It\nwould be better to retain the last known value for a given key, treating the next record\nwith the same key as you would an update to a database table.\n Updating records by key is the behavior that compacted topics (logs) deliver.\nInstead of taking a coarse-grained approach and deleting entire segments based on\ntime or size, compaction is more fine-grained and deletes old records per key in a log.\nAt a very high level, a log cleaner (a pool of threads) runs in the background, recopying\n7 Kafka documentation, “Log Compaction,” http://kafka.apache.org/documentation/#compaction.\nThis segment log has\nbeen deleted.\nOlder log segment ﬁles that\nhave been rolled. The bottom\nsegment is still in use.\nThe records are appended\nto this current log.\nFigure 2.12\nOn the left are the \ncurrent log segments. On the upper \nright is a deleted log segment, and \nthe one below it is a recently rolled \nsegment still in use.\n \n\n\n39\nKafka architecture\nlog-segment files and removing records if there’s an occurrence later in the log with\nthe same key. Figure 2.13 illustrates how log compaction retains the most recent mes-\nsage for each key.\nThis approach guarantees that the last record for a given key is in the log. You can\nspecify log retention per topic, so it’s entirely possible to have some topics using time-\nbased retention and other topics using compaction.\n By default, the log cleaner is enabled. To use compaction for a topic, you’ll need to\nset the log.cleanup.policy=compact property when creating the topic.\n Compaction is used in Kafka Streams when using state stores, but you won’t be cre-\nating those logs/topics yourself—the framework handles that task. Nevertheless, it’s\nimportant to understand how compaction works. Log compaction is a broad subject,\nand we’ve only touched on it here. For more information, see the Kafka documenta-\ntion: http://kafka.apache.org/documentation/#compaction.\nNOTE\nWith a cleanup.policy of compact, you might wonder how you can\nremove a record from the log. With a compacted topic, deletion provides a\nnull value for the given key, setting a tombstone marker. Any key with a null\nvalue ensures that any prior record with the same key is removed, and the\ntombstone marker itself is removed after a period of time.\nThe key takeaway from this section is that if you have independent, standalone events\nor messages, use log deletion. If you have updates to events or messages, you’ll want to\nuse log compaction.\n We’ve spent a good deal of time covering how Kafka handles data internally. Now\nit’s time to move outside of Kafka and discuss how we can send messages to Kafka with\nproducers and read messages from Kafka with consumers. \nBefore compaction\nAfter compaction\nOffset\nValue\nKey\nOffset\nValue\nKey\n10\nA\nfoo\n11\nB\nbar\n12\nC\nbaz\n13\nD\nfoo\n13\nD\nfoo\n14\nE\nbaz\n15\nF\nboo\n16\nG\nfoo\n17\nH\nbaz\n11\nB\nbar\n15\nF\nboo\n16\nG\nfoo\n17\nH\nbaz\nFigure 2.13\nOn the left is a log before compaction—you’ll notice duplicate keys with different \nvalues that are updates for the given key. On the right is the log after compaction—the latest value \nfor each key is retained, but the log is smaller in size.\n \n\n\n40\nCHAPTER 2\nKafka quickly\n2.4\nSending messages with producers\nGoing back to ZMart’s need for a centralized sales transaction data hub, let’s look at\nhow you’ll send purchase transactions into Kafka. In Kafka, the producer is the client\nused for sending messages. Figure 2.14 revisits ZMart’s data architecture with the pro-\nducers highlighted, to emphasize where they fit into the data flow.\nAlthough ZMart has a lot of sales transactions, we’re going to consider the purchase of\na single item for now: a book costing $10.99. When the customer completes the sales\ntransaction, the information is converted into a key/value pair and sent to Kafka via a\nproducer.\n The key is the customer ID, 123447777, and the value is in JSON format:\n\"{\\\"item\\\":\\\"book\\\",\\\"price\\\":10.99}\". (I’ve escaped the double quotes so the\nJSON can be represented as a string literal in Java.) With the data in this format, you\ncan use a producer to send the data to the Kafka cluster. The following example can\nbe found in src/main/java/bbejeck.chapter_2/producer/SimpleProducer.java.\n \n \nSales\nAcquired company A\nConsumer\nMarketing\nAcquired company C\nAcquired company B\nCustomer info\nAuditing\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nKafka\nZooKeeper\nIn this simpliﬁed view of Kafka,\nwe’re assuming a cluster is installed.\nAll output is sent from a producer, and\ninput is consumed by a consumer.\nA cluster of ZooKeeper nodes communicates\nwith Kafka to maintain topic info and keep\ntrack of brokers in the cluster.\nFigure 2.14\nProducers are used to send messages to Kafka. Producers don’t know which \nconsumer will read the messages or when.\n \n\n\n41\nSending messages with producers\nProperties properties = new Properties();\nproperties.put(\"bootstrap.servers\", \"localhost:9092\");\nproperties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.Strin\ngSerializer\");\nproperties.put(\"value.serializer\",\n➥ \"org.apache.kafka.common.serialization.StringSerializer\");\nproperties.put(\"acks\", \"1\");\nproperties.put(\"retries\", \"3\");\nproperties.put(\"compression.type\", \"snappy\");\nproperties.put(\"partitioner.class\",\n➥ PurchaseKeyPartitioner.class.getName());         \nPurchaseKey key = new PurchaseKey(\"12334568\", new Date());\ntry(Producer<PurchaseKey, String> producer =  \n➥ new KafkaProducer<>(properties)) {                                                \nProducerRecord<PurchaseKey, String>\nrecord =  \n➥ new ProducerRecord<>(\"transactions\", key, \"{\\\"item\\\":\\\"book\\\",\n\\\"price\\\":10.99}\");\nCallback callback = (metadata, exception) -> {\nif (exception != null) {\nSystem.out.println(\"Encountered exception \"   \n➥ + exception);                                              \n}\n};\nFuture<RecordMetadata> sendFuture =  \n➥ producer.send(record, callback);    \n}\nKafka producers are thread-safe. All sends to Kafka are asynchronous—Producer\n.send returns immediately once the producer places the record in an internal buffer.\nThe buffer sends records in batches. Depending on your configuration, there could\nbe some blocking if you attempt to send a message while a producer’s buffer is full.\n The Producer.send method depicted here takes a Callback instance. Once the\nleader broker acknowledges the record, the producer fires the Callback.onComplete\nmethod. Only one of the arguments will be non-null in the Callback.onComplete\nmethod. In this case, you’re only concerned with printing out the stacktrace in the\nevent of error, so you check if the exception object is non-null. The returned Future\nyields a RecordMetadata object once the server acknowledges the record.\nDEFINITION\nIn listing 2.3, the Producer.send method returns a Future object.\nA Future object represents the result of an asynchronous operation. More\nimportant, a Future gives you the option to lazily retrieve asynchronous results\ninstead of waiting for their completion. For more information on futures, see\nthe Java documentation for “Interface Future<V>”: http:// mng.bz/0JK2.\nListing 2.3\nSimpleProducer example\nProperties for \nconfiguring a \nproducer\nCreates the \nKafkaProducer\nInstantiates\nthe Producer-\nRecord\nBuilds a \ncallback\nSends the record and sets the \nreturned Future to a variable\n \n\n\n42\nCHAPTER 2\nKafka quickly\n2.4.1\nProducer properties\nWhen you created the KafkaProducer instance, you passed a java.util.Properties\nparameter containing the configuration for the producer. The configuration of a\nKafkaProducer isn’t complicated, but there are key properties to consider when set-\nting it up. These settings are where you’d specify a custom partitioner, for example.\nThere are too many properties to cover here, so we’ll just look at the ones used in list-\ning 2.3:\nBootstrap servers—bootstrap.servers is a comma-separated list of host:port\nvalues. Eventually the producer will use all the brokers in the cluster; this list is\nused for initially connecting to the cluster.\nSerialization—key.serializer and value.serializer instruct Kafka how to\nconvert the keys and values into byte arrays. Internally, Kafka uses byte arrays\nfor keys and values, so you need to provide Kafka with the correct serializers to\nconvert objects to byte arrays before them sending across the wire.\n\nacks—acks specifies the minimum number of acknowledgments from a bro-\nker that the producer will wait for before considering a record send completed.\nValid values for acks are all, 0, and 1. With a value of all, the producer will\nwait for a broker to receive confirmation that all followers have committed\nthe record. When set to 1, the broker writes the record to its log but doesn’t\nwait for any followers to acknowledge committing the record. A value of 0\nmeans the producer won’t wait for any acknowledgments—this is mostly\nfire-and-forget.\nRetries—If sending a batch results in a failure, retries specifies the number of\ntimes to attempt to resend. If record order is important, you should consider\nsetting max.in.flight.requests.per.connection to 1 to prevent the scenario\nof a second batch being sent successfully before a failed record being sent as\nthe result a retry.\nCompression type—compression.type specifies what compression algorithm to\napply, if any. If set, compression.type instructs the producer to compress a\nbatch before sending. Note that it’s the entire batch that’s compressed, not\nindividual records.\nPartitioner class—partitioner.class specifies the name of the class implement-\ning the Partitioner interface. The partitioner.class is related to our earlier\ndiscussion of custom partitioners discussion in section 2.3.7.\nFor more information about producer configuration, see the Kafka documentation:\nhttp://kafka.apache.org/documentation/#producerconfigs. \n2.4.2\nSpecifying partitions and timestamps\nWhen you create a ProducerRecord, you have the option of specifying a partition, a\ntimestamp, or both. When you instantiated the ProducerRecord in listing 2.3, you\n \n",
      "page_number": 54
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 62-70)",
      "start_page": 62,
      "end_page": 70,
      "detection_method": "topic_boundary",
      "content": "43\nSending messages with producers\nused one of four overloaded constructors. Other constructors allow for setting a parti-\ntion and timestamp, or just a partition:\nProducerRecord(String topic, Integer partition, String key, String value)\nProducerRecord(String topic, Integer partition,\nLong timestamp, String key,\nString value)\n2.4.3\nSpecifying a partition\nIn section 2.3.5, we discussed the importance of partitions in Kafka. We also discussed\nhow the DefaultPartitioner works and how you can supply a custom partitioner.\nWhy would you explicitly set the partition? There are a variety of business reasons why\nyou might do so. Here’s one example.\n Suppose you have keyed data coming in, but it doesn’t matter which partition the\nrecords go to, because the consumers have logic to handle any value that the key\nmight contain. Additionally, the distribution of the keys might not be even, and you\nwant to ensure that all partitions receive roughly the same amount of data. Here’s a\nrough implementation that would do this.\nAtomicInteger partitionIndex = new AtomicInteger(0); \nint currentPartition = Math.abs(partitionIndex.getAndIncrement()) % \n➥ numberPartitions;                                                \nProducerRecord<String, String> record =\n➥ new ProducerRecord<>(\"topic\", currentPartition, \"key\", \"value\");\nHere, you use the Math.abs call, so you don’t have to keep track of the value of the\ninteger if it goes beyond Integer.MAX_VALUE.\nDEFINITION\nAtomicInteger belongs to the java.util.concurrent.atomic pack-\nage, which contains classes that support lock-free, thread-safe operations on\nsingle variables. For more information, see the Java documentation for the\njava.util.concurrent.atomic package: http://mng.bz/PQ2q. \n2.4.4\nTimestamps in Kafka\nKafka version 0.10 added timestamps to records. You set the timestamp when you cre-\nate a ProducerRecord via this overloaded constructor call:\nProducerRecord(String topic, Integer partition,\n➥ Long timestamp, K key, V value)\nIf you don’t set a timestamp, the producer will (using the current clock time) before\nsending the record to the Kafka broker. Timestamps are also affected by the\nListing 2.4\nManually setting the partition\nCreates an AtomicInteger \ninstance variable\nGets the current partition and\nuses it as a parameter\n \n\n\n44\nCHAPTER 2\nKafka quickly\nlog.message.timestamp.type broker configuration setting, which can be set to either\nCreateTime (the default) or LogAppendTime. Like many other broker settings, the value\nconfigured on the broker applies to all topics as a default value, but when you create a\ntopic, you can specify a different value for that topic. If you specify LogAppendTime\nand the topic doesn’t override the broker’s configuration, the broker will overwrite\nthe timestamp with the current time when it appends the record to the log. Other-\nwise, the timestamp from ProducerRecord is used.\n Why would you choose one setting over another? LogAppendTime is considered to\nbe “processing time,” and CreateTime is considered to be “event time.” Which you\nshould use depends on your business requirements. You’ll need to decide whether\nyou need to know when Kafka processed the record, or when the actual event\noccurred. In later chapters, you’ll see the important role timestamps have in con-\ntrolling data flow in Kafka Streams. \n2.5\nReading messages with consumers\nYou’ve seen how producers work; now it’s time to look at consumers in Kafka. Suppose\nyou’re building a prototype application to show the latest ZMart sales statistics. For\nthis example, you’ll consume the message you sent in the previous producer example.\nBecause this prototype is in its earliest stages, all you’ll do at this point is consume the\nmessage and print the information to the console.\nNOTE\nBecause the version of Kafka Streams covered in the book requires\nKafka version 0.10.2 or higher, we’ll only discuss the new consumer that was\npart of the Kafka 0.9 release.\nKafkaConsumer is the client you’ll use to consume messages from Kafka. The Kafka-\nConsumer class is straightforward to use, but there are a few operational consider-\nations to take into account. Figure 2.15 shows the ZMart architecture, highlighting\nwhere consumers play a role in the data flow.\n2.5.1\nManaging offsets\nKafkaProducer is essentially stateless, but KafkaConsumer manages some state by peri-\nodically committing the offsets of messages consumed from Kafka. Offsets uniquely\nidentify messages and represent the starting positions of messages in the log. Consum-\ners periodically need to commit the offsets of messages they have received.\n Committing an offset has two implications for a consumer:\nCommitting implies the consumer has fully processed the message.\nCommitting also represents the starting point for that consumer in the case of\nfailure or a restart.\n \n\n\n45\nReading messages with consumers\nIf you have a new consumer instance or some failure has occurred, and the last com-\nmitted offset isn’t available, where the consumer starts from will depend on your con-\nfiguration:\n\nauto.offset.reset=\"earliest\"—Messages will be retrieved starting at the\nearliest available offset. Any messages that haven’t yet been removed by the log-\nmanagement process will be retrieved.\n\nauto.offset.reset=\"latest\"—Messages will be retrieved from the latest off-\nset, essentially only consuming messages from the point of joining the cluster.\n\nauto.offset.reset=\"none\"—No reset strategy is specified. The broker throws\nan exception to the consumer.\nIn figure 2.16, you can see the impact of choosing an auto.offset.reset setting. By\nselecting earliest, you receive messages starting at offset 1. If you choose latest,\nyou’ll get a message starting at offset 11.\n Next, we need to discuss the options for committing offsets. You can do this either\nautomatically or manually.\nSales\nIn this simpliﬁed view of Kafka,\nwe’re assuming a cluster is installed.\nAll output is sent from a producer, and\ninput is consumed by a consumer.\nAcquired company A\nConsumer\nMarketing\nAcquired company C\nAcquired company B\nCustomer info\nAuditing\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nKafka\nZooKeeper\nA cluster of ZooKeeper nodes communicates\nwith Kafka to maintain topic info and keep\ntrack of brokers in the cluster.\nFigure 2.15\nThese are the consumers that read messages from Kafka. Just as producers have \nno knowledge of the consumers, consumers read messages from Kafka with no knowledge of who \nproduced the messages.\n \n\n\n46\nCHAPTER 2\nKafka quickly\n2.5.2\nAutomatic offset commits\nAutomatic offset commits are enabled by default, and they’re represented by the\nenable.auto.commit property. There’s a companion configuration option, auto\n.commit.interval.ms, which specifies how often the consumer will commit offsets\n(the default value is 5 seconds). You should take care when adjusting this value. If it’s\ntoo small, it will increase network traffic; if it’s too large, it could result in the con-\nsumer receiving large amounts of repeated data in the event of a failure or restart. \n2.5.3\nManual offset commits\nThere are two types of manually committed offsets—synchronous and asynchronous.\nThese are the synchronous commits:\nconsumer.commitSync()\nconsumer.commitSync(Map<TopicPartition, OffsetAndMetadata>)\nThe no-arg commitSync() method blocks until all offsets returned from the last retrieval\n(poll) succeed. This call applies to all subscribed topics and partitions. The other ver-\nsion takes a Map<TopicPartition, OffsetAndMetadata> parameter, and it commits\nonly the offsets, partitions, and topics specified in the map.\n There are analogous consumer.commitAsync() methods that are completely asyn-\nchronous and return immediately. One of the overloaded methods accepts no argu-\nments, and two of the consumer.commitAsync methods have an option to provide an\nOffsetCommitCallback object, which is called when the commit has concluded either\nsuccessfully or with an error. Providing a callback instance allows for asynchronous\nprocessing and error handling. The advantage of using manual commits is that it gives\nyou direct control over when a record is considered processed. \nA conﬁg setting of “earliest”\nmeans messages starting\nfrom offset 0 will be sent to\nthe consumer.\nA conﬁg setting of “latest”\nmeans the consumer will\nget the next message when\nit is appended to the log.\nTen messages have been sent to a topic.\nA new consumer starts up, so it doesn’t\nhave a last offset committed.\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\nFigure 2.16\nA graphical representation of setting auto.offset.reset to earliest versus \nlatest. A setting of earliest will give you all messages not yet deleted; latest means you’ll \nwait for the next available message to arrive.\n \n\n\n47\nReading messages with consumers\n2.5.4\nCreating the consumer\nCreating a consumer is similar to creating a producer. You supply a configuration in\nthe form of a Java java.util.Properties object, and you get back a KafkaConsumer\ninstance. This instance then subscribes to topics from a supplied list of topic names or\nby specifying a regular expression. Typically, you’ll run the consumer in a loop, where\nyou poll for a period specified in milliseconds.\n A ConsumerRecords<K, V> object is the result of the polling. ConsumerRecords\nimplements the Iterable interface, and each call to next() returns a Consumer-\nRecord object containing metadata about the message, in addition to the actual key\nand value.\n After you’ve exhausted all of the ConsumerRecord objects returned from the last call\nto poll, you return to the top of the loop, polling again for the specified period. In\npractice, consumers are expected to run indefinitely in this manner, unless an error\noccurs or the application needs to be shut down and restarted (this is where committed\noffsets come into play—on reboot, the consumer will pick up where it left off). \n2.5.5\nConsumers and partitions\nYou’ll generally want multiple consumer instances—one for each partition of a topic.\nIt’s possible to have one consumer read from multiple partitions, but it’s not uncom-\nmon to have a thread pool with as many threads as there are partitions, and with each\nthread running a consumer that’s assigned to one partition.\n This consumer-per-partition pattern maximizes throughput, but if you spread your\nconsumers across multiple applications or machines, the total thread count across all\ninstances shouldn’t exceed the total number of partitions in the topic. Any threads in\nexcess of the total partition count will be idle. If a consumer fails, the leader broker\nassigns its partitions to another active consumer.\nNOTE\nThis example shows a consumer subscribing to one topic, but this is for\ndemonstration purposes only. You can subscribe a consumer to an arbitrary\nnumber of topics.\nThe leader broker assigns topic partitions to all available consumers with the same\ngroup.id. The group.id is a configuration setting that identifies the consumer as\nbelonging to a consumer group—that way, consumers don’t need to reside on the same\nmachine. In fact, it’s probably preferable to have your consumers spread out across a\nfew machines. That way, in the case of one machine failing, the leader broker can\nassign topic partitions to consumers on good machines. \n2.5.6\nRebalancing\nThe process of adding and removing topic-partition assignments to consumers\ndescribed in the previous section is called rebalancing. Topic-partition assignments to a\nconsumer aren’t static—they’re dynamic. As you add consumers with the same group\nID, some of the current topic-partition assignments are taken from active consumers\n \n\n\n48\nCHAPTER 2\nKafka quickly\nand given to the new consumers. This reassignment process continues until every par-\ntition has been assigned to a consumer that’s reading data.\n After that equilibrium point, any additional consumers will remain idle. When\nconsumers leave the group for whatever reason, their topic-partition assignments are\nreassigned to other consumers. \n2.5.7\nFiner-grained consumer assignment\nIn section 2.5.5, I described the use of a thread pool and subscribing multiple con-\nsumers (in the same consumer group) to the same topics. Although Kafka will balance\nthe load of topic-partitions across all consumers, the assignment of the topic and par-\ntition isn’t deterministic. You won’t know what topic-partition pairings each consumer\nwill receive.\n KafkaConsumer has methods that allow you to subscribe to a particular topic and\npartition:\nTopicPartition fooTopicPartition_0 = new TopicPartition(\"foo\", 0);\nTopicPartition barTopicPartition_0 = new TopicPartition(\"bar\", 0);\nconsumer.assign(Arrays.asList(fooTopicPartition_0, barTopicPartition_0));\nThere are trade-offs to consider when using manual topic-partition assignment:\nFailures won’t result in topic partitions being reassigned, even for consumers\nwith the same group ID. Any changes in assignments will require another call to\nconsumer.assign.\nThe group specified by the consumer is used for committing, but because each\nconsumer will be acting independently, it’s a good idea to give each consumer a\nunique group ID. \n2.5.8\nConsumer example\nHere’s the consumer code for the ZMart prototype that consumes transactions and\nprints them to the console. You can find it in src/main/java/bbejeck.chapter_2/con-\nsumer/ThreadedConsumerExample.java.\npublic void startConsuming() {\nexecutorService = Executors.newFixedThreadPool(numberPartitions);\nProperties properties = getConsumerProps();\nfor (int i = 0; i < numberPartitions; i++) {\nRunnable consumerThread = getConsumerThread(properties);\nexecutorService.submit(consumerThread);\n}\n}\nprivate Runnable getConsumerThread(Properties properties) {\nreturn () -> {\nListing 2.5\nThreadedConsumerExample example\nBuilds a\nconsumer\nthread\n \n\n\n49\nInstalling and running Kafka\nConsumer<String, String> consumer = null;\ntry {\nconsumer = new KafkaConsumer<>(properties);\nconsumer.subscribe(Collections.singletonList(  \n➥ \"test-topic\"));                                                                \nwhile (!doneConsuming) {\nConsumerRecords<String, String> records =  \n➥ consumer.poll(5000);                                                         \nfor (ConsumerRecord<String, String> record : records) {\nString message = String.format(\"Consumed: key =\n➥ %s value = %s with offset = %d partition = %d\",\nrecord.key(), record.value(),\nrecord.offset(), record.partition());\nSystem.out.println(message);                           \n}\n}\n} catch (Exception e) {\ne.printStackTrace();\n} finally {\nif (consumer != null) {\nconsumer.close();  \n}\n}\n};\n}\nThis example leaves out other sections of the class for clarity—it won’t stand on its\nown. You can find the full example in this chapter’s source code. \n2.6\nInstalling and running Kafka\nAs I write this, Kafka 1.0.0 is the most recent version. Because Kafka is a Scala project,\neach release comes in two versions: one for Scala 2.11 and another for Scala 2.12. I use\nthe 2.12 Scala version of Kafka in this book. Although you can download the release,\nthe book’s source code includes a binary distribution of Kafka that will work with\nKafka Streams as demonstrated and described in this book. To install Kafka, extract\nthe .tgz file found in the book’s source code repo (source code can be found on the\nbook’s website here: https://manning.com/books/kafka-streams-in-action), to some-\nwhere in the libs folder on your machine.\nNOTE\nThe binary distribution of Kafka includes Apache ZooKeeper, so no\nextra installation work is required.\n2.6.1\nKafka local configuration\nRunning Kafka locally on your machine requires minimal configuration if you accept\nthe default values. By default, Kafka uses port 9092, and ZooKeeper uses port 2181.\nAssuming you have no applications already using those ports, you’re all set.\n Kafka writes its logs to /tmp/kafka-logs, and ZooKeeper uses /tmp/zookeeper\nfor its log storage. Depending on your machine, you may need to change permission\nSubscribes \nto the topic\nPolls for 5 \nseconds\nPrints a \nformatted \nmessage\nCloses the consumer—\nwill leak resources \notherwise\n \n\n\n50\nCHAPTER 2\nKafka quickly\nor ownership of those directories or to modify the location where you want to write\nthe logs.\n To change the Kafka logs directory, cd into <kafka-install-dir>/config and open the\nserver.properties file. Find the log.dirs entry, and change the value to what you’d\nrather use. In the same directory, open the zookeeper.properties file and change the\ndataDir entry.\n We’ll look at configuring Kafka in detail later in this book, but that’s all the configu-\nration you need to do for now. Keep in mind that these “logs” are the actual data used\nby Kafka and ZooKeeper, and not application-level logs that track the application’s\nbehavior. The application logs are found in the <kafka-install-dir>/logs directory. \n2.6.2\nRunning Kafka\nKafka is simple to get started. Because ZooKeeper is essential for the Kafka cluster to\nfunction properly (ZooKeeper determines the leader broker, holds topic information,\nperforms health checks on cluster members, and so on), you’ll need to start Zoo-\nKeeper before starting Kafka.\nNOTE\nFrom now on, all directory references assume you’re working in your\nKafka installation directory. If you’re using a Windows machine, the directory\nis <kafka-install-dir>/bin/windows.\nSTARTING ZOOKEEPER\nTo start ZooKeeper, open a command prompt and enter the following command:\nbin/zookeeper-server-start.sh\nconfig/zookeeper.properties\nYou’ll see a lot of information run by on the screen, and it should end up looking\nsomething like figure 2.17. \nSTARTING KAFKA\nTo start Kafka, open another command prompt and type this command:\nbin/Kafka-server-start.sh\nconfig/server.properties\nAgain, you’ll see text scroll by on the screen. When Kafka has fully started, you should\nsee something similar to figure 2.18.\nTIP\nZooKeeper is essential for Kafka to run, so it’s important to reverse the\norder when shutting down: stop Kafka first, and then stop ZooKeeper. To\nstop Kafka, you can press Ctrl-C from the terminal Kafka is running in, or run\nkafka-server-stop.sh from another terminal. The same goes for Zoo-\nKeeper, except the shutdown script is zookeeper-server-stop.sh. \n \n\n\n51\nInstalling and running Kafka\nFigure 2.17\nOutput visible on the console when ZooKeeper starts up\nFigure 2.18\nOutput from Kafka when starting up\n \n",
      "page_number": 62
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 71-84)",
      "start_page": 71,
      "end_page": 84,
      "detection_method": "topic_boundary",
      "content": "52\nCHAPTER 2\nKafka quickly\n2.6.3\nSending your first message\nNow that you have Kafka up and running, it’s time to use Kafka for what it’s meant to\ndo: sending and receiving messages. But before you send a message, you’ll need to\ndefine a topic for a producer to send a message to.\nYOUR FIRST TOPIC\nCreating a topic in Kafka is simple. It’s just a matter of running a script with some con-\nfiguration parameters. The configuration is easy, but the settings you provide have\nbroad performance implications.\n By default, Kafka is configured to autocreate topics, meaning that if you attempt to\nsend to or read from a nonexistent topic, the Kafka broker will create one for you\n(using default configurations in the server.properties file). It’s rarely a good practice\nto rely on the broker to create topics, even in development, because the first pro-\nduce/consume attempt will fail, as it takes time for the metadata about the topic’s\nexistence to propagate. Be sure to always proactively to create your topics.\nCREATING A TOPIC\nTo create a topic, you need to run the kafka-topics.sh script. Open a terminal window\nand run this command:\nbin/kafka-topics.sh --create --topic first-topic --replication-factor 1\n➥ --partitions 1 --zookeeper localhost:2181\nWhen the script executes, you should see something similar to figure 2.19 in your\nterminal.\nMost of the configuration flags in the previous command are obvious, but let’s quickly\nreview two of them:\n\nreplication-factor—This flag determines how many copies of the message\nthe leader broker distributes in the cluster. In this case, with a replication factor\nof 1, no copies will be made. Just the original message will reside in Kafka. A\nreplication factor of 1 is fine for a quick demo or prototype, but in practice\nyou’ll almost always want a replication factor of 2 or 3 to provide data availabil-\nity in the case of machine failures.\nFigure 2.19\nThese are the results from creating a topic. It’s important to create your topics ahead of time so you \ncan supply topic-specific configurations. Otherwise, autocreated topics will use default configuration or the \nconfiguration in the server.properties file.\n \n\n\n53\nInstalling and running Kafka\n\npartitions—This flag specifies the number of partitions that the topic will use.\nAgain, just one partition is fine here, but if you have greater load, you’ll certainly\nwant more partitions. Determining the correct number of partitions is not an\nexact science.\nSENDING A MESSAGE\nSending a message in Kafka generally involves writing a producer client, but Kafka\nalso comes with a handy script called kafka-console-producer that allows you to\nsend a message from a terminal window. We’ll use the console producer in this exam-\nple, but we’ve covered how to use the KafkaProducer in section 2.4.1 of this chapter.\n To send your first message, run the following command (also shown in figure 2.20):\n# command assumes running from bin directory\n./kafka-console-producer.sh --topic first-topic --broker-list localhost:9092\nThere are several options for configuring the console producer, but for now we’ll only\nuse the required ones: the topic to send the message to, and a list of Kafka brokers to\nconnect to (in this case, just the one on your local machine).\n Starting a console producer is a “blocking script,” so after executing the preceding\ncommand, you enter some text and press Enter. You can send as many messages as you\nlike, but for our demo purposes you can type a single message, “the quick brown fox\njumped over the lazy dog,” press Enter, and then press Ctrl-C to exit the producer.\nREADING A MESSAGE\nKafka also provides a console consumer for reading messages from the command line.\nThe console consumer is similar to the console producer: once started, it will keep\nreading messages from the topic until the script is stopped by you (with Ctrl-C).\n To launch the console consumer, run this command:\nbin/kafka-console-consumer.sh --topic first-topic\n➥ --bootstrap-server localhost:9092 --from-beginning\nAfter starting the console consumer, you should see something like figure 2.21 in your\nterminal.\n The --from-beginning parameter specifies that you’ll receive any message not\ndeleted from that topic. The console consumer won’t have any committed offsets, so if\nyou didn’t have the --from-beginning setting, you’d only get messages sent after the\nconsole consumer had started.\nFigure 2.20\nThe console producer is a great tool for quickly testing your configuration and ensuring \nend-to-end functionality.\n \n\n\n54\nCHAPTER 2\nKafka quickly\nYou’ve just completed a whirlwind tour of Kafka and produced and consumed your\nfirst message. If you haven’t read the first part of this chapter, it’s time to go back to\nthe beginning this chapter to learn the details of how Kafka works!\nSummary\nKafka is a message broker that receives messages and stores them in a way that\nmakes it easy and fast to respond to consumer requests. Messages are never\npushed out to consumers, and message retention in Kafka is entirely indepen-\ndent of when and how often messages are consumed.\nKafka uses partitions for achieving high throughput and to provide a means for\ngrouping messages with the same keys in order.\nProducers are used for sending messages to Kafka.\nNull keys mean round-robin partition assignment; otherwise, the producer uses\nthe hash of the key, modulus the number of partitions, for partition assignment.\nConsumers are what you use to read messages from Kafka.\nConsumers that are part of a consumer group are given topic-partition alloca-\ntions in an attempt to distribute messages evenly.\nIn the next chapter, we’ll start looking at Kafka Streams with a concrete example from\nthe world of retail sales. Although Kafka Streams will handle the creation of all con-\nsumer and producer instances, you should be able to see the concepts we introduced\nhere come into play.\nFigure 2.21\nThe console consumer is a handy tool for quickly getting a feel for whether data is flowing and if \nmessages contain the expected information.\n \n\n\nPart 2\nKafka Streams development\nThis part of the book builds on the previous part and puts the mental model\nof Kafka Streams into action as you develop your first Kafka Streams applica-\ntion. Once you’ve gotten your feet wet, we’ll walk through the significant Kafka\nStreams APIs.\n You’ll learn about providing state to a streaming application and how to use\nstate for performing joins, much like the joins you perform when running SQL\nqueries. Then we’ll move on to a new abstraction from Kafka Streams: the KTable\nAPI. This part of the book begins with the high-level DSL, but we’ll wrap up by\ndiscussing the lower-level Processor API and how you can use it to make Kafka\nStreams do pretty much anything you need it to do.\n \n\n\n57\nDeveloping Kafka Streams\nIn chapter 1, you learned about the Kafka Streams library. You learned about build-\ning a topology of processing nodes, or a graph that transforms data as it’s streaming\ninto Kafka. In this chapter, you’ll learn how to create this processing topology with\nthe Kafka Streams API.\n The Kafka Streams API is what you’ll use to build Kafka Streams applications.\nYou’ll learn how to assemble Kafka Streams applications; but, more important,\nyou’ll gain a deeper understanding of how the components work together and how\nthey can be used to achieve your stream-processing goals.\nThis chapter covers\nIntroducing the Kafka Streams API\nBuilding Hello World for Kafka Streams\nExploring the ZMart Kafka Streams application \nin depth\nSplitting an incoming stream into multiple \nstreams\n \n\n\n58\nCHAPTER 3\nDeveloping Kafka Streams\n3.1\nThe Streams Processor API\nThe Kafka Streams DSL is the high-level API that enables you to build Kafka Streams\napplications quickly. The high-level API is very well thought out, and there are meth-\nods to handle most stream-processing needs out of the box, so you can create a sophis-\nticated stream-processing program without much effort. At the heart of the high-level\nAPI is the KStream object, which represents the streaming key/value pair records.\n Most of the methods in the Kafka Streams DSL return a reference to a KStream\nobject, allowing for a fluent interface style of programming. Additionally, a good\npercentage of the KStream methods accept types consisting of single-method inter-\nfaces allowing for the use of Java 8 lambda expressions. Taking these factors into\naccount, you can imagine the simplicity and ease with which you can build a Kafka\nStreams program.\n Back in 2005, Martin Fowler and Eric Evans developed the concept of the fluent\ninterface—an interface where the return value of a method call is the same instance\nthat originally called the method (https://martinfowler.com/bliki/FluentInterface\n.html). This approach is useful when constructing objects with several parameters,\nsuch as \nPerson.builder().firstName(\"Beth\").withLastName(\"Smith\").with-\nOccupation(\"CEO\"). In Kafka Streams, there is one small but important difference:\nthe returned KStream object is a new instance, not the same instance that made the\noriginal method call.\n There’s also a lower-level API, the Processor API, which isn’t as succinct as the\nKafka Streams DSL but allows for more control. We’ll cover the Processor API in chap-\nter 6. With that introduction out of the way, let’s dive into the requisite Hello World\nprogram for Kafka Streams. \n3.2\nHello World for Kafka Streams\nFor the first Kafka Streams example, we’ll deviate from the problem outlined in chap-\nter 1 to a simpler use case. This will get off the ground quickly so you can see how\nKafka Streams works. We’ll get back to the problem from chapter 1 later in section 3.1.1\nfor a more realistic, concrete example.\n Your first program will be a toy application that takes incoming messages and con-\nverts them to uppercase characters, effectively yelling at anyone who reads the mes-\nsage. You’ll call this the Yelling App.\n Before diving into the code, let’s take a look at the processing topology you’ll assem-\nble for this application. You’ll follow the same pattern as in chapter 1, where you built\nup a processing graph topology with each node in the graph having a particular func-\ntion. The main difference is that this graph will be simpler, as you can see in figure 3.1.\n As you can see, you’re building a simple processing graph—so simple that it resem-\nbles a linked list of nodes more than the typical tree-like structure of a graph. But\nthere’s enough here to give you strong clues about what to expect in the code. There\nwill be a source node, a processor node transforming incoming text to uppercase, and\na sink processor writing results out to a topic.\n \n\n\n59\nHello World for Kafka Streams\nThis is a trivial example, but the code shown here is representative of what you’ll see in\nother Kafka Streams programs. In most of the examples, you’ll see a similar structure:\n1\nDefine the configuration items.\n2\nCreate Serde instances, either custom or predefined.\n3\nBuild the processor topology.\n4\nCreate and start the KStream.\nWhen we get into the more advanced examples, the principal difference will be in the\ncomplexity of the processor topology. With that in mind, it’s time to build your first\napplication.\n3.2.1\nCreating the topology for the Yelling App\nThe first step to creating any Kafka Streams application is to create a source node.\nThe source node is responsible for consuming the records, from a topic, that will flow\nthrough the application. Figure 3.2 highlights the source node in the graph.\n The following line of code creates the source, or parent, node of the graph.\nKStream<String, String> simpleFirstStream = builder.stream(\"src-topic\",\n➥ Consumed.with(stringSerde, stringSerde));\nThe simpleFirstStreamKStream instance is set to consume messages written to the\nsrc-topic topic. In addition to specifying the topic name, you also provide Serde\nListing 3.1\nDefining the source for the stream\nUpperCase\nprocessor\nsrc-topic\nSource\nprocessor\nSink\nprocessor\nout-topic\nHere the source processor will consume\nmessages that will be fed into the\nprocessing topology.\nThe UpperCase processor simply uppercases all\nincoming text. It’s important to note that the\ncopy of the original message is what gets\nuppercased, but the original value is unchanged.\nThe terminal processor here takes\nthe uppercase text from the\nprevious processor and writes\nit out to a topic.\nFigure 3.1\nGraph (topology) of the Yelling App\n \n\n\n60\nCHAPTER 3\nDeveloping Kafka Streams\nobjects (via a Consumed instance) for deserializing the records from Kafka. You’ll use\nthe Consumed class for any optional parameters whenever you create a source node in\nKafka Streams.\n You now have a source node for your application, but you need to attach a process-\ning node to make use of the data, as shown in figure 3.3. The code used to attach the\nUpperCase\nprocessor\nsrc-topic\nSource\nprocessor\nSink\nprocessor\nout-topic\nFigure 3.2\nCreating the source node \nof the Yelling App\nUpperCase\nprocessor\nsrc-topic\nSource\nprocessor\nSink\nprocessor\nout-topic\nFigure 3.3\nAdding the uppercase \nprocessor to the Yelling App\n \n\n\n61\nHello World for Kafka Streams\nprocessor (a child node of the source node) is shown in the following listing. With this\nline, you create another KStream instance that’s a child node of the parent node.\nKStream<String, String> upperCasedStream =\n➥ simpleFirstStream.mapValues(String::toUpperCase);\nBy calling the KStream.mapValues function, you’re creating a new processing node\nwhose inputs are the results of going through the mapValues call.\n It’s important to remember that you shouldn’t modify the original value in the Value-\nMapper provided to mapValues. The upperCasedStream instance receives transformed\ncopies of the initial value from the simpleFirstStream.mapValues call. In this case,\nit’s uppercase text.\n The mapValues() method takes an instance of the ValueMapper<V, V1> interface.\nThe ValueMapper interface defines only one method, ValueMapper.apply, making it\nan ideal candidate for using a Java 8 lambda expression. This is what you’ve done here\nwith String::toUpperCase, which is a method reference, an even shorter form of a\nJava 8 lambda expression.\nNOTE\nMany Java 8 tutorials are available for lambda expressions and method\nreferences. Good starting points can be found in Oracle’s Java documenta-\ntion: “Lambda Expressions” (http://mng.bz/J0Xm) and “Method References”\n(http://mng.bz/BaDW).\nYou could have used the form s  s.toUpperCase(), but because toUpperCase is an\ninstance method on the String class, you can use a method reference.\n Using lambda expressions instead of concrete implementations is a pattern\nyou’ll see over and over with the Streams Processor API in this book. Because most\nof the methods expect types that are single method interfaces, you can easily use\nJava 8 lambdas.\n So far, your Kafka Streams application is consuming records and transforming\nthem to uppercase. The final step is to add a sink processor that writes the results out\nto a topic. Figure 3.4 shows where you are in the construction of the topology.\n The following code line adds the last processor in the graph.\nupperCasedStream.to(\"out-topic\", Produced.with(stringSerde, stringSerde));\nThe KStream.to method creates a sink-processing node in the topology. Sink proces-\nsors write records back out to Kafka. This sink node takes records from the upper-\nCasedStream processor and writes them to a topic named out-topic. Again, you\nprovide Serde instances, this time for serializing records written to a Kafka topic. But\nin this case, you use a Produced instance, which provides optional parameters for cre-\nating a sink node in Kafka Streams.\nListing 3.2\nMapping incoming text to uppercase\nListing 3.3\nCreating a sink node\n \n\n\n62\nCHAPTER 3\nDeveloping Kafka Streams\nNOTE\nYou don’t always have to provide Serde objects to either the Consumed\nor Produced objects. If you don’t, the application will use the serializer/dese-\nrializer listed in the configuration. Additionally, with the Consumed and\nProduced classes, you can specify a Serde for either the key or value only.\nThe preceding example uses three lines to build the topology:\nKStream<String,String> simpleFirstStream =\n➥ builder.stream(\"src-topic\", Consumed.with(stringSerde, stringSerde));\nKStream<String, String> upperCasedStream =\n➥ simpleFirstStream.mapValues(String::toUpperCase);\nupperCasedStream.to(\"out-topic\", Produced.with(stringSerde, stringSerde));\nEach step is on an individual line to demonstrate the different stages of the building\nprocess. But all methods in the KStream API that don’t create terminal nodes (meth-\nods with a return type of void) return a new KStream instance, which allows you to use\nthe fluent interface style of programming mentioned earlier. To demonstrate this\nidea, here’s another way you could construct the Yelling App topology:\nbuilder.stream(\"src-topic\", Consumed.with(stringSerde, stringSerde))\n➥ .mapValues(String::toUpperCase)\n➥ .to(\"out-topic\", Produced.with(stringSerde, stringSerde));\nThis shortens the program from three lines to one without losing any clarity or pur-\npose. From this point forward, all the examples will be written using the fluent inter-\nface style unless doing so causes the clarity of the program to suffer.\nUpperCase\nprocessor\nsrc-topic\nSource\nprocessor\nSink\nprocessor\nout-topic\nFigure 3.4\nAdding a processor for \nwriting the Yelling App results\n \n\n\n63\nHello World for Kafka Streams\n You’ve built your first Kafka Streams topology, but we glossed over the important\nsteps of configuration and Serde creation. We’ll look at those now. \n3.2.2\nKafka Streams configuration\nAlthough Kafka Streams is highly configurable, with several properties you can\nadjust for your specific needs, the first example uses only two configuration settings,\nAPPLICATION_ID_CONFIG and BOOTSTRAP_SERVERS_CONFIG:\nprops.put(StreamsConfig.APPLICATION_ID_CONFIG, \"yelling_app_id\");\nprops.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\nBoth settings are required because no default values are provided. Attempting to start\na Kafka Streams program without these two properties defined will result in a Config-\nException being thrown.\n The StreamsConfig.APPLICATION_ID_CONFIG property identifies your Kafka Streams\napplication, and it must be a unique value for the entire cluster. It also serves as a\ndefault value for the client ID prefix and group ID parameters if you don’t set either\nvalue. The client ID prefix is the user-defined value that uniquely identifies clients\nconnecting to Kafka. The group ID is used to manage the membership of a group of\nconsumers reading from the same topic, ensuring that all consumers in the group can\neffectively read subscribed topics.\n The StreamsConfig.BOOTSTRAP_SERVERS_CONFIG property can be a single host-\nname:port pair or multiple hostname:port comma-separated pairs. The value of this\nsetting points the Kafka Streams application to the locaction of the Kafka cluster. We’ll\ncover several more configuration items as we explore more examples in the book. \n3.2.3\nSerde creation\nIn Kafka Streams, the Serdes class provides convenience methods for creating Serde\ninstances, as shown here:\nSerde<String> stringSerde = Serdes.String();\nThis line is where you create the Serde instance required for serialization/deserializa-\ntion using the Serdes class. Here, you create a variable to reference the Serde for\nrepeated use in the topology. The Serdes class provides default implementations for\nthe following types:\nString\nByte array\nLong\nInteger\nDouble\nImplementations of the Serde interface are extremely useful because they contain the\nserializer and deserializer, which keeps you from having to specify four parameters\n \n\n\n64\nCHAPTER 3\nDeveloping Kafka Streams\n(key serializer, value serializer, key deserializer, and value deserializer) every time you\nneed to provide a Serde in a KStream method. In an upcoming example, you’ll create a\nSerde implementation to handle serialization/deserialization of more-complex types.\n Let’s take a look at the whole program you just put together. You can find the\nsource in src/main/java/bbejeck/chapter_3/KafkaStreamsYellingApp.java (source\ncode can be found on the book’s website here: https://manning.com/books/kafka-\nstreams-in-action).\npublic class KafkaStreamsYellingApp {\npublic static void main(String[] args) {\nProperties props = new Properties();\nprops.put(StreamsConfig.APPLICATION_ID_CONFIG, \"yelling_app_id\");  \nprops.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\nStreamsConfig streamingConfig = new StreamsConfig(props);   \nSerde<String> stringSerde = Serdes.String();  \nStreamsBuilder builder = new StreamsBuilder();   \nKStream<String, String> simpleFirstStream = builder.stream(\"src-topic\",  \n➥ Consumed.with(stringSerde, stringSerde));                             \nKStream<String, String> upperCasedStream =             \n➥ simpleFirstStream.mapValues(String::toUpperCase);   \nupperCasedStream.to( \"out-topic\",                    \n➥ Produced.with(stringSerde, stringSerde));         \nKafkaStreams kafkaStreams = new KafkaStreams(builder.build(),streamsConfig);\nkafkaStreams.start();     \nThread.sleep(35000);\nLOG.info(\"Shutting down the Yelling APP now\");\nkafkaStreams.close();\n}\n}\nYou’ve now constructed your first Kafka Streams application. Let’s quickly review the\nsteps involved, as it’s a general pattern you’ll see in most of your Kafka Streams\napplications:\n1\nCreate a StreamsConfig instance.\n2\nCreate a Serde object.\nListing 3.4\nHello World: the Yelling App\nProperties for configuring\nthe Kafka Streams program\nCreates the\nStreamsConfig with\nthe given properties\nCreates the\nSerdes used\nto serialize/\ndeserialize\nkeys and\nvalues\nCreates the StreamsBuilder \ninstance used to construct \nthe processor topology\nCreates the\nactual stream\nwith a source\ntopic to read\nfrom (the\nparent node\nin the graph)\nA processor using a Java 8 \nmethod handle (the first \nchild node in the graph)\nWrites the transformed \noutput to another topic \n(the sink node in the graph)\nKicks off the Kafka \nStreams threads\n \n\n\n65\nWorking with customer data\n3\nConstruct a processing topology.\n4\nStart the Kafka Streams program.\nApart from the general construction of a Kafka Streams application, a key takeaway here\nis to use lambda expressions whenever possible, to make your programs more concise.\n We’ll now move on to a more complex example that will allow us to explore more\nof the Streams Processor API. The example will be new, but the scenario is one you’re\nalready familiar with: ZMart data-processing goals. \n3.3\nWorking with customer data\nIn chapter 1, we discussed ZMart’s new requirements for processing customer data,\nintended to help ZMart do business more efficiently. We demonstrated how you could\nbuild a topology of processors that would work on purchase records as they come stream-\ning in from transactions in ZMart stores. Figure 3.5 shows the completed graph again.\nLet’s briefly review the requirements for the streaming program, which will also serve\nas a good description of what the program will do:\nAll records need to have credit card numbers protected, in this case by masking\nthe first 12 digits.\nYou need to extract the items purchased and the ZIP code to determine pur-\nchase patterns. This data will be written out to a topic.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFigure 3.5\nTopology for ZMart Kafka Streams program\n \n\n\n66\nCHAPTER 3\nDeveloping Kafka Streams\nYou need to capture the customer’s ZMart member number and the amount\nspent and write this information to a topic. Consumers of the topic will use this\ndata to determine rewards.\nYou need to write the entire transaction out to topic, which will be consumed by\na storage engine for ad hoc analysis.\nAs in the Yelling App, you’ll combine the fluent interface approach with Java 8 lamb-\ndas when building the application. Although it’s sometimes clear that the return type\nof a method call is a KStream object, other times it may not be. Keep in mind that the\nmajority of the methods in the KStream API return new KStream instances. Now, let’s\nbuild a streaming application that will satisfy ZMart’s business requirements.\n3.3.1\nConstructing a topology\nLet’s dive into building the processing topology. To help make the connection between\nthe code you’ll create here and the processing topology graph from chapter 1, I’ll high-\nlight the part of the graph that you’re currently working on.\nBUILDING THE SOURCE NODE\nYou’ll start by building the source node and first processor of the topology by chaining\ntwo calls to the KStream API together (highlighted in figure 3.6). It should be fairly obvi-\nous by now what the role of the origin node is. The first processor in the topology will be\nresponsible for masking credit card numbers to protect customer privacy.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nSource node consuming messages from\nthe Kafka transactions topic\nSecond node does the masking\nof credit card numbers\nFigure 3.6\nThe source processor consumes from a Kafka topic, and it feeds the \nmasking processor exclusively, making it the source for the rest of the topology.\n \n",
      "page_number": 71
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 85-93)",
      "start_page": 85,
      "end_page": 93,
      "detection_method": "topic_boundary",
      "content": "67\nWorking with customer data\nKStream<String,Purchase> purchaseKStream =\n➥ streamsBuilder.stream(\"transactions\",\n➥ Consumed.with(stringSerde, purchaseSerde))\n➥ .mapValues(p -> Purchase.builder(p).maskCreditCard().build());\nYou create the source node with a call to the StreamsBuilder.stream method using a\ndefault String serde, a custom serde for Purchase objects, and the name of the topic\nthat’s the source of the messages for the stream. In this case, you only specify one\ntopic, but you could have provided a comma-separated list of names or a regular\nexpression to match topic names instead.\n In this listing 3.5, you provide Serdes with a Consumed instance, but you could have\nleft that out and only provided the topic name and relied on the default Serdes pro-\nvided via configuration parameters.\n The next immediate call is to the KStream.mapValues method, taking a ValueMap-\nper<V, V1> instance as a parameter. Value mappers take a single parameter of one\ntype (a Purchase object, in this case) and map that object to a to a new value, possibly\nof another type. In this example, KStream.mapValues returns an object of the same\ntype (Purchase), but with a masked credit card number.\n Note that when using the KStream.mapValues method, the original key is unchanged\nand isn’t factored into mapping a new value. If you wanted to generate a new key/value\npair or include the key in producing a new value, you’d use the KStream.map method\nthat takes a KeyValueMapper<K, V, KeyValue<K1, V1>> instance. \nHINTS ABOUT FUNCTIONAL PROGRAMMING\nAn important concept to keep in mind with the map and mapValues functions is that\nthey’re expected to operate without side effects, meaning the functions don’t modify\nthe object or value presented as a parameter. This is because of the functional pro-\ngramming aspects in the KStream API. Functional programming is a deep topic, and a\nfull discussion is beyond the scope of this book, but we’ll briefly look at two central\nprinciples of functional programming here.\n The first principle is avoiding state modification. If an object requires a change or\nupdate, you pass the object to a function, and a copy or entirely new instance is made,\ncontaining the desired changes or updates. In listing 3.5, the lambda passed to\nKStream.mapValues is used to update the Purchase object with a masked credit card\nnumber. The credit card field on the original Purchase object is left unchanged.\n The second principle is building complex operations by composing several smaller\nsingle-purpose functions together. The composition of functions is a pattern you’ll\nfrequently see when working with the KStream API.\nDEFINITION\nFor the purposes of this book, I define functional programming as a\nprogramming approach in which functions are first-class objects. Further-\nmore, functions are expected to avoid creating side effects, such as modifying\nstate or mutable objects. \nListing 3.5\nBuilding the source node and first processor\n \n\n\n68\nCHAPTER 3\nDeveloping Kafka Streams\nBUILDING THE SECOND PROCESSOR\nNow you’ll build the second processor, responsible for extracting pattern data from a\ntopic, which ZMart can use to determine purchase patterns in regions of the country.\nYou’ll also add a sink node responsible for writing the pattern data to a Kafka topic.\nThe construction of these is demonstrated in figure 3.7.\nIn listing 3.6, you can see the purchaseKStream processor using the familiar mapValues\ncall to create a new KStream instance. This new KStream will start to receive Purchase-\nPattern objects created as a result of the mapValues call.\nKStream<String, PurchasePattern> patternKStream =\n➥ purchaseKStream.mapValues(purchase ->\n➥ PurchasePattern.builder(purchase).build());\npatternKStream.to(\"patterns\",\n➥ Produced.with(stringSerde,purchasePatternSerde));\nListing 3.6\nSecond processor and a sink node that writes to Kafka\nAgain you see two nodes in the graph, but\nwith the ﬂuent style of programming in\nKafka Streams, sometimes it’s easy to\noverlook the fact that you’re creating\ntwo nodes.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nThis is the fourth node overall,\nbut it does no processing. This\nnode writes PurchasePattern\nout to a topic.\nSecond processing node\n(third node overall) builds\nthe PurchasePattern object\nFigure 3.7\nThe second processor builds purchase-pattern information. The sink node writes the \nPurchasePattern object out to a Kafka topic.\n \n\n\n69\nWorking with customer data\nHere, you declare a variable to hold the reference of the new KStream instance,\nbecause you’ll use it to print the results of the stream to the console with a print call.\nThis is very useful during development and for debugging. The purchase-patterns\nprocessor forwards the records it receives to a child node of its own, defined by the\nmethod call KStream.to, writing to the patterns topic. Note the use of a Produced\nobject to provide the previously built Serde.\n The KStream.to method is a mirror image of the KStream.source method.\nInstead of setting a source for the topology to read from, the KStream.to method\ndefines a sink node that’s used to write the data from a KStream instance to a Kafka\ntopic. The KStream.to method also provides overloaded versions in which you can\nleave out the Produced parameter and use the default Serdes defined in the configu-\nration. One of the optional parameters you can set with the Produced class is Stream-\nPartitioner, which we’ll discuss next. \nBUILDING THE THIRD PROCESSOR\nThe third processor in the topology is the customer rewards accumulator node shown\nin figure 3.8, which will let ZMart track purchases made by members of their pre-\nferred customer club. The rewards accumulator sends data to a topic consumed by\napplications at ZMart HQ to determine rewards when customers complete purchases.\nThe rewards\nprocessor builds a\nRewards object and\npasses the object to a\nsink processor, which\nserializes and writes the\nobject out to a topic.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFigure 3.8\nThe third processor creates the RewardAccumulator object from the \npurchase data. The terminal node writes the results out to a Kafka topic.\n \n\n\n70\nCHAPTER 3\nDeveloping Kafka Streams\nKStream<String, RewardAccumulator> rewardsKStream =\n➥ purchaseKStream.mapValues(purchase ->\n➥ RewardAccumulator.builder(purchase).build());\nrewardsKStream.to(\"rewards\",\n➥ Produced.with(stringSerde,rewardAccumulatorSerde));\nYou build the rewards accumulator processor using what should be by now a familiar\npattern: creating a new KStream instance that maps the raw purchase data contained\nin the record to a new object type. You also attach a sink node to the rewards accumu-\nlator so the results of the rewards KStream can be written to a topic and used for deter-\nmining customer reward levels. \nBUILDING THE LAST PROCESSOR\nFinally, you’ll take the first KStream you created, purchaseKStream, and attach a sink\nnode to write out the raw purchase records (with credit cards masked, of course) to a\ntopic called purchases. The purchases topic will be used to feed into a NoSQL store\nsuch as Cassandra (http://cassandra.apache.org/), Presto (https://prestodb.io/), or\nElastic Search (www.elastic.co/webinars/getting-started-elasticsearch) to perform ad\nhoc analysis. Figure 3.9 shows the final processor.\npurchaseKStream.to(\"purchases\", Produced.with(stringSerde, purchaseSerde));\nListing 3.7\nThird processor and a terminal node that writes to Kafka\nListing 3.8\nFinal processor\nThe ﬁnal processor, a sink\nprocessor to be precise, writes\nthe purchase data out to\na topic with the credit card\ninformation still masked.\nPatterns\nMasking\nSource\nRewards\nRewards\nsink\nRewards\nsink\nPurchases\nsink\nFigure 3.9\nThe last node writes out the entire purchase transaction to a topic whose \nconsumer is a NoSQL data store.\n \n\n\n71\nWorking with customer data\nNow that you’ve built the application piece by piece, let’s look at the entire applica-\ntion (src/main/java/bbejeck/chapter_3/ZMartKafkaStreamsApp.java). You’ll quickly\nnotice it’s more complicated than the previous Hello World (the Yelling App) example.\npublic class ZMartKafkaStreamsApp {\npublic static void main(String[] args) {\n// some details left out for clarity\nStreamsConfig streamsConfig = new StreamsConfig(getProperties());\nJsonSerializer<Purchase> purchaseJsonSerializer = new\n➥ JsonSerializer<>();\nJsonDeserializer<Purchase> purchaseJsonDeserializer =  \n➥ new JsonDeserializer<>(Purchase.class);                     \nSerde<Purchase> purchaseSerde =\n➥ Serdes.serdeFrom(purchaseJsonSerializer, purchaseJsonDeserializer);\n//Other Serdes left out for clarity\nSerde<String> stringSerde = Serdes.String();\nStreamsBuilder streamsBuilder = new StreamsBuilder();\nKStream<String,Purchase> purchaseKStream =                \n➥ streamsBuilder.stream(\"transactions\",                          \n➥ Consumed.with(stringSerde, purchaseSerde))                     \n➥ .mapValues(p -> Purchase.builder(p).maskCreditCard().build()); \nKStream<String, PurchasePattern> patternKStream =   \n➥ purchaseKStream.mapValues(purchase ->                    \n➥ PurchasePattern.builder(purchase).build());              \npatternKStream.to(\"patterns\",\n➥ Produced.with(stringSerde,purchasePatternSerde));\nKStream<String, RewardAccumulator> rewardsKStream =  \n➥ purchaseKStream.mapValues(purchase ->                     \n➥ RewardAccumulator.builder(purchase).build());             \nrewardsKStream.to(\"rewards\",\n➥ Produced.with(stringSerde,rewardAccumulatorSerde));\npurchaseKStream.to(\"purchases\",         \n➥ Produced.with(stringSerde,purchaseSerde));   \nKafkaStreams kafkaStreams =\n➥ new KafkaStreams(streamsBuilder.build(),streamsConfig);\nkafkaStreams.start();\n}\nListing 3.9\nZMart customer purchase KStream program\nCreates the Serde; the \ndata format is JSON.\nBuilds the \nsource and first \nprocessor\nBuilds the \nPurchasePattern \nprocessor\nBuilds the \nRewardAccumula\ntor processor\nBuilds the storage sink, the topic \nused by the storage consumer\n \n\n\n72\nCHAPTER 3\nDeveloping Kafka Streams\nNOTE\nI’ve left out some details in listing 3.9 for clarity. The code examples in\nthe book aren’t necessarily meant to stand on their own. The source code\nthat accompanies this book provides the full examples.\nAs you can see, this example is a little more involved than the Yelling App, but it has a\nsimilar flow. Specifically, you still performed the following steps:\nCreate a StreamsConfig instance.\nBuild one or more Serde instances.\nConstruct the processing topology.\nAssemble all the components and start the Kafka Streams program.\nIn this application, I’ve mentioned using a Serde, but I haven’t explained why or how\nyou create them. Let’s take some time now to discuss the role of the Serde in a Kafka\nStreams application. \n3.3.2\nCreating a custom Serde\nKafka transfers data in byte array format. Because the data format is JSON, you need\nto tell Kafka how to convert an object first into JSON and then into a byte array when\nit sends data to a topic. Conversely, you need to specify how to convert consumed byte\narrays into JSON, and then into the object type your processors will use. This conver-\nsion of data to and from different formats is why you need a Serde. Some serdes are\nprovided out of the box by the Kafka client dependency, (String, Long, Integer, and\nso on), but you’ll need to create custom serdes for other objects.\n In the first example, the Yelling App, you only needed a serializer/deserializer for\nstrings, and an implementation is provided by the Serdes.String() factory method.\nIn the ZMart example, however, you need to create custom Serde instances, because\nthe types of the objects are arbitrary. We’ll look at what’s involved in building a Serde\nfor the Purchase class. We won’t cover the other Serde instances, because they follow\nthe same pattern, just with different types.\n Building a Serde requires implementations of the Deserializer<T> and Serial-\nizer<T> interfaces. We’ll use the implementations in listings 3.10 and 3.11 through-\nout the examples. Also, you’ll use the Gson library from Google to convert objects to\nand from JSON. Here’s the serializer, which you can find in src/main/java/bbejeck/\nutil/serializer/JsonSerializer.java.\npublic class JsonSerializer<T> implements Serializer<T> {\nprivate Gson gson = new Gson();      \n@Override\npublic void configure(Map<String, ?> map, boolean b) {\n}\nListing 3.10\nGeneric serializer\nCreates the \nGson object\n \n\n\n73\nWorking with customer data\n@Override\npublic byte[] serialize(String topic, T t) {\nreturn gson.toJson(t).getBytes(Charset.forName(\"UTF-8\"));  \n}\n@Override\npublic void close() {\n}\n}\nFor serialization, you first convert an object to JSON, and then get the bytes from the\nstring. To handle the conversions from and to JSON, the example uses Gson (https://\ngithub.com/google/gson).\n For the deserializing process, you take different steps: create a new string from a\nbyte array, and then use Gson to convert the JSON string into a Java object. This\ngeneric deserializer can be found in src/main/java/bbejeck/util/serializer/Json-\nDeserializer.java.\npublic class JsonDeserializer<T> implements Deserializer<T> {\nprivate Gson gson = new Gson();                   \nprivate Class<T> deserializedClass;          \npublic JsonDeserializer(Class<T> deserializedClass) {\nthis.deserializedClass = deserializedClass;\n}\npublic JsonDeserializer() {\n}\n@Override\n@SuppressWarnings(\"unchecked\")\npublic void configure(Map<String, ?> map, boolean b) {\nif(deserializedClass == null) {\ndeserializedClass = (Class<T>) map.get(\"serializedClass\");\n}\n}\n@Override\npublic T deserialize(String s, byte[] bytes) {\nif(bytes == null){\nreturn null;\n}\nreturn gson.fromJson(new String(bytes),deserializedClass);  \n}\nListing 3.11\nGeneric deserializer\nSerializes an\nobject to bytes\nCreates the \nGson object\nInstance variable of \nClass to deserialize\nDeserializes bytes to an\ninstance of expected Class\n \n\n\n74\nCHAPTER 3\nDeveloping Kafka Streams\n@Override\npublic void close() {\n}\n}\nNow, let’s go back to the following lines from listing 3.9:\nJsonDeserializer<Purchase> purchaseJsonDeserializer =  \n➥ new JsonDeserializer<>(Purchase.class);             \nJsonSerializer<Purchase> purchaseJsonSerializer =     \n➥ new JsonSerializer<>();                            \nSerde<Purchase> purchaseSerde =\n➥ Serdes.serdeFrom(purchaseJsonSerializer,purchaseJsonDeserializer);   \nAs you can see, a Serde object is useful because it serves as a container for the serial-\nizer and deserializer for a given object.\n We’ve covered a lot of ground so far in developing a Kafka Streams application. We\nstill have much more to cover, but let’s pause for a moment and talk about the devel-\nopment process itself and how you can make life easier for yourself while developing a\nKafka Streams application. \n3.4\nInteractive development\nYou’ve built the graph to process purchase records from ZMart in a streaming fashion,\nand you have three processors that write out to individual topics. During development\nit would certainly be possible to have a console consumer running to view results, but\nit would be good to have a more convenient solution, like the ability to watch data\nflowing through the topology in the console, as shown in figure 3.10.\n There’s a method on the KStream interface that can be useful during develop-\nment: the KStream.print method, which takes an instance of the Printed<K, V> class.\nCreates the Deserializer \nfor the Purchase class\nCreates the Serializer \nfor the Purchase class\nCreates the Serde for\nPurchase objects\nFigure 3.10\nA great tool while you’re developing is the capacity to print the data that’s output from each node to \nthe console. To enable printing to the console, just replace any of the to methods with a call to print.\n \n\n\n75\nInteractive development\nPrinted provides two static methods allowing you print to stdout, Printed.toSys-\nOut(), or to write results to a file, Printed.toFile(filePath).\n Additionally, you can label your printed results by chaining the withLabel()\nmethod, allowing you to print an initial header with the records. This is useful when\nyou’re dealing with results from different processors. It’s important that your objects\nprovide a meaningful toString implementation to create useful results when printing\nyour stream either to the console or a file.\n Finally, if you don’t want to use toString, or you want to customize how Kafka\nStreams prints records, there’s the Printed.withKeyValueMapper method, which\ntakes a KeyValueMapper instance so you can format your records in any way you want.\nThe same caveat I mentioned earlier—that you shouldn’t modify the original\nrecords—applies here as well.\n In this book, I focus on printing records to the console for all examples. Here are\nsome examples of using KStream.print in listing 3.11:\npatternKStream.print(Printed.<String, PurchasePattern>toSysOut()   \n➥ .withLabel(\"patterns\"));                                        \nrewardsKStream.print(Printed.<String, RewardAccumulator>toSysOut()  \n➥ .withLabel(\"rewards\"));                                          \npurchaseKStream.print(Printed.<String, Purchase>toSysOut()   \n➥ .withLabel(\"purchases\"));                                 \nLet’s take a quick look at the output you’ll see on the screen (figure 3.11) and how it can\nhelp you during development. With printing enabled, you can run the Kafka Streams\napplication directly from your IDE as you make changes, stop and start the application,\nand confirm that the output is what you expect. This is no substitute for unit and integra-\ntion tests, but viewing streaming results directly as you develop is a great tool.\nSets up to print the PurchasePattern\ntransformation to the console\nSets up to print the RewardAccumulator \ntransformation to the console\nPrints the purchase \ndata to the console\nName(s) given to the print\nstatement, helpful to make\nthis the same as the topic\nThe values for the records. Note that these are\nJSON strings and the Purchase, PurchasePattern,\nand RewardAccumulator objects deﬁned toString\nmethods to get this rendering on the console.\nThe keys for the records,\nwhich are null in this case\nNote the masked\ncredit card number!\nFigure 3.11\nThis a detailed view of the data on the screen. With printing to the console enabled, you’ll quickly \nsee if your processors are working correctly.\n \n",
      "page_number": 85
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 94-101)",
      "start_page": 94,
      "end_page": 101,
      "detection_method": "topic_boundary",
      "content": "76\nCHAPTER 3\nDeveloping Kafka Streams\nOne downside of using the print() method is that it creates a terminal node, mean-\ning you can’t embed it in a chain of processors. You need to have a separate statement.\nHowever, there’s also the KStream.peek method, which takes a ForeachAction\ninstance as a parameter and returns a new KStream instance. The ForeachAction\ninterface has one method, apply(), which has a return type of void, so nothing from\nKStream.peek is forwarded downstream, making it ideal for operations like printing.\nYou can embed it in a chain of processors without the need for a separate print state-\nment. You’ll see the KStream.peek method used in this manner in other examples in\nthe book. \n3.5\nNext steps\nAt this point, you have your Kafka Streams purchase-analysis program running well.\nOther applications have also been developed to consume the messages written to the\npatterns, rewards, and purchases topics, and the results for ZMart have been good.\nBut alas, no good deed goes unpunished. Now that the ZMart executives can see what\nyour streaming program can provide, a slew of new requirements come your way.\n3.5.1\nNew requirements\nYou now have new requirements for each of the three categories of results you’re pro-\nducing. The good news is that you’ll still use the same source data. You’re being asked\nto refine, and in some cases further break down, the data you’re providing. The new\nrequirements may be able to be applied to current topics, or they may require you to\ncreate entirely new topics:\nPurchases under a certain dollar amount need to be filtered out. Upper man-\nagement isn’t much interested in the small purchases for general daily articles.\nZMart has expanded and has bought an electronics chain and a popular coffee\nhouse chain. All purchases from these new stores will flow through the stream-\ning application you’ve set up. You need to send the purchases from these new\nsubsidiaries to their topics.\nThe NoSQL solution you’ve chosen stores items in key/value format. Although\nKafka also uses key/value pairs, the records coming into your Kafka cluster\ndon’t have keys defined. You need to generate a key for each record before the\ntopology forwards it to the purchases topic.\nMore requirements will inevitably come your way, but you can start to work on the cur-\nrent set of new requirements now. If you look through the KStream API, you’ll be\nrelieved to see that there are several methods already defined that will make fulfilling\nthese new demands easy.\nNOTE\nFrom this point forward, all code examples are pared down to the\nessentials to maximize clarity. Unless there’s something new to introduce, you\ncan assume that the configuration and setup code remain the same. These\ntruncated examples aren’t meant to stand alone—the full code listing for this\n \n\n\n77\nNext steps\nexample can be found in src/main/java/bbejeck/chapter_3/ZMartKafka-\nStreamsAdvancedReqsApp.java.\nFILTERING PURCHASES\nLet’s start with filtering out purchases that don’t reach the minimum threshold. To\nremove low-dollar purchases, you’ll need to insert a filter-processing node between\nthe KStream instance and the sink node. You’ll update the processor topology graph\nas shown in figure 3.12.\nYou can use the KStream method, which takes a Predicate<K,V> instance as a param-\neter. Although you’re chaining method calls together here, you’re creating a new pro-\ncessing node in the topology.\nKStream<Long, Purchase> filteredKStream =\n➥ purchaseKStream((key, purchase) ->\n➥ purchase.getPrice() > 5.00).selectKey(purchaseDateAsKey);\nThis code filters purchases that are less than $5.00 and selects the purchase date as a\nlong value for a key.\n The Predicate interface has one method defined, test(), which takes two parame-\nters—the key and the value—although, at this point, you only need to use the value.\nAgain, you can use a Java 8 lambda in place of a concrete type defined in the KStream API.\nListing 3.12\nFiltering on KStream\nThe ﬁltering processor will\nonly allow records through that\nmatch the given predicate—in\nthis case, purchases over a\ncertain dollar amount.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFiltering\nprocessor\nFigure 3.12\nYou’re placing a processor between the masking processor and the terminal \nnode that writes to Kafka. This filtering processor will drop purchases under a given dollar \namount.\n \n\n\n78\nCHAPTER 3\nDeveloping Kafka Streams\nDEFINITION\nIf you’re familiar with functional programming, you should feel\nright at home with the Predicate interface. If the term predicate is new to you,\nit’s nothing more than a given statement, such as x < 100. An object either\nmatches the predicate statement or doesn’t.\nAdditionally, you want to use the purchase timestamp as a key, so you use the select-\nKey processor, which uses the KeyValueMapper mentioned in section 3.4 to extract the\npurchase date as a long value. I cover details about selecting the key in the section\n“Generating a key.”\n A mirror-image function, KStreamNot, performs the same filtering functionality\nbut in reverse. Only records that don’t match the given predicate are processed further\nin the topology.\nSPLITTING/BRANCHING THE STREAM\nNow you need to split the stream of purchases into separate streams that can write to\ndifferent topics. Fortunately, the KStream.branch method is perfect. The KStream\n.branch method takes an arbitrary number of Predicate instances and returns an\narray of KStream instances. The size of the returned array matches the number of\npredicates supplied in the call.\n In the previous change, you modified an existing leaf on the processing topology.\nWith this requirement to branch the stream, you’ll create brand-new leaf nodes on\nthe graph of processing nodes, as shown in figure 3.13.\nThe KStream.branch method takes an array of\npredicates and returns an array containing an equal\nnumber of KStream instances, each one accepting\nrecords matching the corresponding predicate.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFiltering\nprocessor\nProcessor for records\nmatching predicate at\nindex 0\nProcessor for records\nmatching predicate at\nindex 1\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nFigure 3.13\nThe branch processor splits the stream into two: one stream consists of purchases from \nthe cafe, and the other stream contains purchases from the electronics store.\n \n\n\n79\nNext steps\nAs records from the original stream flow through the branch processor, each record is\nmatched against the supplied predicates in the order that they’re provided. The pro-\ncessor assigns records to a stream on the first match; no attempts are made to match\nadditional predicates.\n The branch processor drops records if they don’t match any of the given predi-\ncates. The order of the streams in the returned array matches the order of the predi-\ncates provided to the branch() method. A separate topic for each department may\nnot be the only approach, but we’ll stick with this for now. It satisfies the requirement,\nand it can be revisited later.\nPredicate<String, Purchase> isCoffee =                   \n➥ (key, purchase) ->                                    \n➥ purchase.getDepartment().equalsIgnoreCase(\"coffee\");  \nPredicate<String, Purchase> isElectronics =\n➥ (key, purchase) ->\n➥ purchase.getDepartment().equalsIgnoreCase(\"electronics\");\nint coffee = 0;       \nint electronics = 1;\nKStream<String, Purchase>[] kstreamByDept =          \n➥ purchaseKStream.branch(isCoffee, isElectronics);  \nkstreamByDept[coffee].to( \"coffee\", \nProduced.with(stringSerde, purchaseSerde));\nkstreamByDept[electronics].to(\"electronics\",     \n➥ Produced.with(stringSerde, purchaseSerde));   \nWARNING\nThe example in listing 3.13 sends records to several different top-\nics. Although Kafka can be configured to automatically create topics when it\nattempts to produce or consume for the first time from nonexistent topics,\nit’s not a good idea to rely on this mechanism. If you rely on autocreating top-\nics, the topics are configured with default values from the server.config prop-\nerties file, which may or may not be the settings you need. You should always\nthink about what topics you’ll need, the level of partitions, and the replica-\ntion factor ahead of time, and create them before running your Kafka\nStreams application.\nIn listing 3.13, you define the predicates ahead of time, because passing four lambda\nexpression parameters would be a little unwieldy. The indices of the returned array\nare also labeled, to maximize readability.\n This example demonstrates the power and flexibility of Kafka Streams. You’ve\nbeen able to take the original stream of purchase transactions and split them into four\nstreams with very few lines of code. Also, you’re starting to build up a more complex\nprocessing topology, all while reusing the same source processor.\nListing 3.13\nSplitting the stream\nCreates the \npredicates as \nJava 8 lambdas\nLabels the expected indices \nof the returned array\nCalls branch to split \nthe original stream \ninto two streams\nWrites the results of each \nstream out to a topic\n \n\n\n80\nCHAPTER 3\nDeveloping Kafka Streams\nSo far, so good. You’ve met two of the three new requirements with ease. Now it’s time\nto implement the last additional requirement, generating a key for the purchase\nrecord to be stored.\nGENERATING A KEY\nKafka messages are in key/value pairs, so all records flowing through a Kafka Streams\napplication are key/value pairs as well. But there’s no requirement stating that keys\ncan’t be null. In practice, if there’s no need for a particular key, having a null key will\nreduce the overall amount of data that travels the network. All the records flowing\ninto the ZMart Kafka Streams application have null keys.\n That’s been fine, until you realize that your NoSQL storage solution stores data in\nkey/value format. You need a way to create a key from the Purchase data before it gets\nwritten out to the purchases topic. You certainly could use KStream.map to generate a\nkey and return a new key/value pair (where only the key would be new), but there’s a\nmore succinct KStream.selectKey method that returns a new KStream instance that\nproduces records with a new key (possibly a different type) and the same value. This\nchange to the processor topology is similar to filtering, in that you add a processing\nnode between the filter and the sink processor, shown in figure 3.14.\nKeyValueMapper<String, Purchase, Long> purchaseDateAsKey =  \n➥ (key, purchase) -> purchase.getPurchaseDate().getTime(); \nKStream<Long, Purchase> filteredKStream =                     \n➥ purchaseKStream((key, purchase) ->                         \n➥ purchase.getPrice() > 5.00).selectKey(purchaseDateAsKey);  \nfilteredKStream.print(Printed.<Long, Purchase>            \n➥ toSysOut().withLabel(\"purchases\"));  \nfilteredKStream.to(\"purchases\",                    \n➥ Produced.with(Serdes.Long(),purchaseSerde));    \nTo create the new key, you take the purchase date and convert it to a long. Although you\ncould pass a lambda expression, it’s assigned to a variable here to help with readability.\nSplitting vs. partitioning streams\nAlthough splitting and partitioning may seem like similar ideas, they’re unrelated in\nKafka and Kafka Streams. Splitting a stream with the KStream.branch method\nresults in creating one or more streams that could ultimately send records to another\ntopic. Partitioning is how Kafka distributes messages for one topic across servers,\nand aside from configuration tuning, it’s the principal means of achieving high\nthroughput in Kafka.\nListing 3.14\nGenerating a new key\nThe KeyValueMapper \nextracts the purchase \ndate and converts to \na long.\nFilters out purchases \nand selects the key in \none statement\nPrints the results \nto the console\nMaterializes the results \nto a Kafka topic\n \n\n\n81\nNext steps\nAlso, note that you need to change the serde type used in the KStream.to method,\nbecause you’ve changed the type of the key.\nThis is a simple example of mapping to a new key. Later, in another example, you’ll\nselect keys to enable joining separate streams. Also, all the examples up until this\npoint have been stateless, but there are several options for stateful transformations as\nwell, which you’ll see a little later on.\n3.5.2\nWriting records outside of Kafka\nThe security department at ZMart has approached you. Apparently, in one of the\nstores, there’s a suspicion of fraud. There have been reports that a store manager is\nentering invalid discount codes for purchases. Security isn’t sure what’s going on, but\nthey’re asking for your help.\n The security folks don’t want this information to go into a topic. You talk to them\nabout securing Kafka, about access controls, and about how you can lock down access\nto a topic, but the security folks are standing firm. These records need to go into a\nrelational database where they have full control. You sense this is a fight you can’t win,\nso you relent and resolve to get this task done as requested.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nAdd the select-key\nprocessor here after the\nﬁltering, as you only need to\ngenerate keys for records\nthat will be written out to\nthe purchases topic.\nFiltering\nprocessor\nSelect-key\nprocessor\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nFigure 3.14\nThe NoSQL data store will use the purchase date as a key for the data it stores. The new select-\nKey processor will extract the purchase date to be used as a key, right before you write the data to Kafka.\n \n\n\n82\nCHAPTER 3\nDeveloping Kafka Streams\nFOREACH ACTIONS\nThe first thing you need to do is create a new KStream that filters results down to a sin-\ngle employee ID. Even though you have a large amount of data flowing through your\ntopology, this filter will reduce the volume to a tiny amount.\n Here, you’ll use KStream with a predicate that looks to match a specific employee\nID. This filter will be completely separate from the previous filter, and it’ll be attached\nto the source KStream instance. Although it’s entirely possible to chain filters, you\nwon’t do that here; you want full access to the data in the stream for this filter.\nNext, you’ll use a KStream.foreach method, as shown in figure 3.15. KStream.foreach\ntakes a ForeachAction<K, V> instance, and it’s another example of a terminal node.\nIt’s a simple processor that uses the provided ForeachAction instance to perform an\naction on each record it receives.\nForeachAction<String, Purchase> purchaseForeachAction = (key, purchase) ->\n➥ SecurityDBService.saveRecord(purchase.getPurchaseDate(),\n➥ purchase.getEmployeeId(), purchase.getItemPurchased());\nListing 3.15\nForeach operations\nThis ﬁlter will only forward records where the\nemployee ID matches the given predicate.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nAfter records are forwarded to the Foreach processor,\nthe value of each record is written to an external database.\nPurchase-\nprice\nprocessor\nSelect-key\nprocessor\nBranch\nprocessor\nEmployee\nID\nprocessor\nForeach-\nValue\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nFigure 3.15\nTo write purchases involving a given employee outside of the Kafka Streams application, you’ll first \nadd a filter processor to extract purchases by employee ID, and then you’ll use a foreach operator to write \neach record to an external relational database.\n \n\n\n83\nSummary\npurchaseKStream.filter((key, purchase) ->\n➥ purchase.getEmployeeId()\n➥ .equals(\"source code has 000000\"))\n➥ .foreach(purchaseForeachAction);\nForeachAction uses a Java 8 lambda (again), and it’s stored in a variable, purchase-\nForeachAction. This requires an extra line of code, but the clarity gained by doing so\nmore than makes up for it. On the next line, another KStream instance sends the fil-\ntered results to the ForeachAction defined directly above it.\n Note that KStream.foreach is stateless. If you need state to perform some action\nfor each record, you can use the KStream.process method. The KStream.process\nmethod will be discussed in the next chapter when you add state to a Kafka Streams\napplication.\n If you step back and look at what you’ve accomplished so far, it’s pretty impressive,\nconsidering the amount of code written. Don’t get too comfortable, though, because\nupper management at ZMart has taken notice of your productivity. More changes and\nrefinements to the purchase-streaming analysis program are coming. \nSummary\nYou can use the KStream.mapValues function to map incoming record values to\nnew values, possibly of a different type. You also learned that these mapping\nchanges shouldn’t modify the original objects. Another method, KStream.map,\nperforms the same action but can be used to map both the key and the value to\nsomething new.\nA predicate is a statement that accepts an object as a parameter and returns\ntrue or false depending on whether that object matches a given condition.\nYou used predicates in the filter function to prevent records that didn’t match a\ngiven predicate from being forwarded in the topology.\nThe KStream.branch method uses predicates to split records into new streams\nwhen a record matches a given predicate. The processor assigns a record to a\nstream on the first match and drops unmatched records.\nYou can modify an existing key or create a new one using the KStream.select-\nKey method.\nIn the next chapter, we’ll start to look at state, the required properties for using state\nwith a steaming application, and why you might need to add state at all. Then you’ll add\nstate to a KStream application, first by using stateful versions of KStream methods you’ve\nseen in this chapter (KStream.mapValues()). For a more advanced example, you’ll per-\nform joins between two different streams of purchases to help ZMart improve customer\nservice.\n \n",
      "page_number": 94
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 102-114)",
      "start_page": 102,
      "end_page": 114,
      "detection_method": "topic_boundary",
      "content": "84\nStreams and state\nIn the last chapter, we dove headfirst into the Kafka Streams DSL and built a pro-\ncessing topology to handle streaming requirements from purchases at ZMart loca-\ntions. Although you built a nontrivial processing topology, it was one dimensional\nin that all transformations and operations were stateless. You considered each\ntransaction in isolation, without any regard to other events occurring at the same\ntime or within certain time boundaries, either before or after the transaction. Also,\nyou only dealt with individual streams, ignoring any possibility of gaining addi-\ntional insight by joining streams together.\n In this chapter, you’ll extract the maximum amount of information from the\nKafka Streams application. To get this level of information, you’ll need to use state.\nState is nothing more than the ability to recall information you’ve seen before and\nconnect it to current information. You can utilize state in different ways. We’ll look\nThis chapter covers\nApplying stateful operations to Kafka Streams\nUsing state stores for lookups and remembering \npreviously seen data\nJoining streams for added insight\nHow time and timestamps drive Kafka Streams\n \n\n\n85\nThinking of events\nat one example when we explore the stateful operations, such as the accumulation of\nvalues, provided by the Kafka Streams DSL.\n Another example of state we’ll discuss is the joining of streams. Joining streams is\nclosely related to the joins performed in database operations, such as joining records\nfrom the employee and department tables to generate a report on who staffs which\ndepartments in a company.\n We’ll also define what the state needs to look like and what the requirements are\nfor using state when we discuss state stores in Kafka Streams. Finally, we’ll weigh the\nimportance of timestamps and look at how they can help you work with stateful opera-\ntions, such as ensuring you only work with events occurring within a given time frame\nor helping you work with data arriving out of order.\n4.1\nThinking of events\nWhen it comes to event processing, events sometimes require no further information\nor context. At other times, an event on its own may be understood in a literal sense,\nbut without some added context, you might miss the significance of what is occurring;\nyou might think of the event in a whole new light, given some additional information.\n An example of an event that doesn’t require additional information is the\nattempted use of a stolen credit card. The transaction is canceled immediately once\nthe stolen card’s use is detected. You don’t need any additional information to make\nthat decision.\n But sometimes a singular event won’t give you enough information to make a deci-\nsion. Consider a series of stock purchases by three individual investors within a short\nperiod. On the face of it, there’s nothing about the purchases of XYZ Pharmaceutical\nstock, shown in figure 4.1, that would give you pause. Investors buying shares of the\nsame stock is something that happens every day on Wall Street.\nNow let’s add some context. Within a short period of the individual stock purchases,\nXYZ Pharmaceutical announced government approval for a new drug, which sent the\nstock price to historic highs. Additionally, those three investors had close ties to XYZ\nPharmaceutical. Now the transactions, shown in figure 4.2, can be viewed in a whole\nnew light.\nTimeline\n9:30 a.m.\n9:50 a.m.\n10:30 a.m.\n10,000 shares of XYZ\nPharmaceutical\npurchased\n12,000 shares of XYZ\nPharmaceutical\npurchased\n15,000 shares of XYZ\nPharmaceutical\npurchased\nFigure 4.1\nStock transactions without any extra information don’t look like anything \nout of the ordinary.\n \n\n\n86\nCHAPTER 4\nStreams and state\nThe timing of these purchases and the information release raises some questions.\nWere these investors leaked information ahead of time? Or do the transactions repre-\nsent one investor with inside information trying to cover their tracks?\n4.1.1\nStreams need state\nThe preceding fictional scenario illustrates something that most of us already know\ninstinctively. Sometimes it’s easy to reason about what’s going on, but usually you need\nsome context to make good decisions. When it comes to stream processing, we call\nthat added context state.\n At first glance, the notions of state and stream processing may seem to be at odds\nwith each other. Stream processing implies a constant flow of discrete events that\ndon’t have much to do with each other and need to be dealt with as they occur. The\nnotion of state might evoke images of a static resource, such as a database table.\n In actuality, you can view these as one and the same. But the rate of change in a\nstream is potentially much faster and more frequent than in a database table.1\n You don’t always need state to work with streaming data. In some cases, you may\nhave discrete events or records that carry enough information to be valuable on their\nown. But more often than not, the incoming stream of data will need enrichment\nfrom some sort of store, either using information from events that arrived before, or\njoining related events with events from different streams. \n4.2\nApplying stateful operations to Kafka Streams\nIn this section, we’ll look at how you can add a stateful operation to an existing state-\nless one to improve the information collected by our application. You’re going to\nmodify the original topology from chapter 3, shown in figure 4.3 to refresh your\nmemory.\n In this topology, you produced a stream of purchase-transaction events. One of the\nprocessing nodes in the topology calculated reward points for customers based on the\n1 Jay Kreps, “Why Local State Is a Fundamental Primitive in Stream Processing,” http://mng.bz/sfoI.\nTimeline\n9:30 a.m.\n9:50 a.m.\n10:30 a.m.\n10,000 shares of XYZ\nPharmaceutical\npurchased\n12,000 shares of XYZ\nPharmaceutical\npurchased\n15,000 shares of XYZ\nPharmaceutical\npurchased\n11:00 a.m.\nFDA announces approval\nof experimental drug\ndeveloped by XYZ\nPharmaceutical. Stock\nprice soars 30%.\nFigure 4.2\nWhen you add some additional context about the timing of the stock purchases, you’ll \nsee them in an entirely new light.\n \n\n\n87\nApplying stateful operations to Kafka Streams\namount of the sale. But in that processor, you just calculated the total number of\npoints for the single transaction and forwarded the results.\n If you added some state to the processor, you could keep track of the cumulative\nnumber of reward points. Then, the consuming application at ZMart would need to\ncheck the total and send out a reward if needed.\n Now that you have a basic idea of how state can be useful in Kafka Streams (or\nany other streaming application), let’s look at some concrete examples. You’ll start\nwith transforming the stateless rewards processor into a stateful processor using\ntransformValues. You’ll keep track of the total bonus points achieved so far and the\namount of time between purchases, to provide more information to downstream\nconsumers.\n4.2.1\nThe transformValues processor\nThe most basic of the stateful functions is KStream.transformValues. Figure 4.4 illus-\ntrates how the KStream.transformValues() method operates.\n This method is semantically the same as KStream.mapValues(), with a few excep-\ntions. One difference is that transformValues has access to a StateStore instance to\naccomplish its task. The other difference is its ability to schedule operations to occur\nat regular intervals via a punctuate() method. The punctuate() method will be dis-\ncussed in detail when we cover the Processor API in chapter 6. \nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nSource node consuming messages from\nthe Kafka transactions topic\nSecond node does the\nmasking of credit\ncard numbers\nFigure 4.3\nHere’s another look at the topology from chapter 3.\n \n\n\n88\nCHAPTER 4\nStreams and state\n4.2.2\nStateful customer rewards\nThe rewards processor from the chapter 3 topology (see figure 4.3) for ZMart extracts\ninformation for customers belonging to ZMart’s rewards program. Initially, the rewards\nprocessor used the KStream.mapValues() method to map the incoming Purchase\nobject into a RewardAccumulator object.\n The RewardAccumulator object originally consisted of just two fields, the customer\nID and the purchase total for the transaction. Now, the requirements have changed\nsome, and points are being associated with the ZMart rewards program:\npublic class RewardAccumulator {\nprivate String customerId;     \nprivate double purchaseTotal;      \nprivate int currentRewardPoints;  \n//details left out for clarity\n}\nWhereas before, an application read from the rewards topic and calculated customer\nachievements, now management wants the point system to be maintained and calcu-\nlated by the streaming application. Additionally, you need to capture the amount of\ntime between the customer’s current and last purchase.\nTransaction object representing\ncustomer purchase in store\nRetrieves state by key\nand uses previously seen\ndata to update object\nTakes current data plus\nprevious data to create\nupdated state stored by key\ntransformValues forwards\ntransformed object to next\nprocessor in stream\nRecords ﬂowing\ninto transform\ntransformValues uses\nlocal state to perform\ntransformation\nCustomer ID\nTotal reward points\nDays since last purchase\n…………...\ntransformValues\nprocessor\nDate\nItems purchased\nCustomer ID\nStore ID\n………...\nLocal state\nIn-memory\nkey/value store\nFigure 4.4\nThe transformValues processor uses information stored in local state to \nupdate incoming records. In this case, the customer ID is the key used to retrieve and store \nthe state for a given record.\nCustomer ID\nTotal dollar amount \nof purchase\nCurrent number of \nreward points\n \n\n\n89\nApplying stateful operations to Kafka Streams\n When the application reads records from the rewards topic, the consuming appli-\ncation will only need to check whether the total points are above the threshold to dis-\ntribute an award. To meet this new goal, you can add the totalRewardPoints and\ndaysFromLastPurchase fields to the RewardAccumulator object, and use the local\nstate to keep track of accumulated points and the last date of purchase. Here’s the\nrefactored RewardAccumulator code (found in src/main/java/bbejeck/model/Reward-\nAccumulator.java; source code can be found on the book’s website here: https://\nmanning.com/books/kafka-streams-in-action) needed to support these changes.\npublic class RewardAccumulator {\nprivate String customerId;\nprivate double purchaseTotal;\nprivate int currentRewardPoints;\nprivate int daysFromLastPurchase;\nprivate long totalRewardPoints;        \n//details left out for clarity\n}\nThe updated rules for the purchase program are simple. The customer earns a point\nper dollar, and transaction totals are rounded down to the nearest dollar. The overall\nstructure of the topology won’t change, but the rewards-processing node will change\nfrom using the KStream.mapValues() method to using KStream.transformValues().\nSemantically, these two methods operate the same way, in that you still map the\nPurchase object into a RewardAccumulator object. The difference lies in the ability to\nuse local state to perform the transformation.\n Specifically, you’ll take two main two steps:\nInitialize the value transformer.\nMap the Purchase object to a RewardAccumulator using state.\nThe KStream.transformValues() method takes a ValueTransformerSupplier<V, R>\nobject, which supplies an instance of the ValueTransformer<V, R> interface. Your imple-\nmentation of the ValueTransformer will be PurchaseRewardTransformer<Purchase,\nRewardAccumulator>. For the sake of clarity, I won’t reproduce the entire class here in\nthe text. Instead, we’ll walk through the important methods for the example applica-\ntion. Also note that these code snippets aren’t meant to stand alone, and some details\nwill be left out for clarity. The full code can be found in the chapter source code\n(found on the book’s website here: https://manning.com/books/kafka-streams-in-\naction). Let’s move on and initialize the processor. \nListing 4.1\nRefactored RewardAccumulator object\nField added for \ntracking total \npoints\n \n\n\n90\nCHAPTER 4\nStreams and state\n4.2.3\nInitializing the value transformer\nThe first step is to set up or create any instance variables in the transformer init()\nmethod. In the init() method, you retrieve the state store created when building the\nprocessing topology (we’ll cover how you add the state store in section 4.3.3).\nprivate KeyValueStore<String, Integer> stateStore;      \nprivate final String storeName;\nprivate ProcessorContext context;\npublic void init(ProcessorContext context) {\nthis.context = context;                  \nstateStore = (KeyValueStore)               \n➥ this.context.getStateStore(storeName);      \n}\nInside the transformer class, you cast to a KeyValueStore type. You’re not concerned\nwith the implementation inside the transformer at this point, just that you can retrieve\nvalues by key (more on state store implementation types in the next section).\n There are other methods (such as punctuate() and close()) not listed here that\nbelong to the ValueTransformer interface. We’ll discuss punctuate() and close()\nwhen we discuss the Processor API in chapter 6. \n4.2.4\nMapping the Purchase object to a RewardAccumulator \nusing state\nNow that you’ve initialized the processor, you can move on to transforming a Purchase\nobject using state. A few simple steps for performing the transformation are as follows:\n1\nCheck for points accumulated so far by customer ID.\n2\nSum the points for the current transaction and present the total.\n3\nSet the reward points on the RewardAccumulator to the new total amount.\n4\nSave the new total points by customer ID in the local state store.\npublic RewardAccumulator transform(Purchase value) {\nRewardAccumulator rewardAccumulator =\n➥ RewardAccumulator.builder(value).build();  \nInteger accumulatedSoFar =\n➥ stateStore.get(rewardAccumulator.getCustomerId());\nif (accumulatedSoFar != null) {\nrewardAccumulator.addRewardPoints(accumulatedSoFar);    \n}\nListing 4.2\ninit() method\nListing 4.3\nTransforming Purchase using state\nInstance variables\nSets a local reference \nto ProcessorContext\nRetrieves the StateStore instance \nby storeName variable. storeName \nis set in the constructor.\nBuilds the Reward-\nAccumulator object \nfrom Purchase\nRetrieves the latest \ncount by customer ID\nIf an accumulated number exists,\nadds it to the current total\n \n\n\n91\nApplying stateful operations to Kafka Streams\nstateStore.put(rewardAccumulator.getCustomerId(),\nrewardAccumulator.getTotalRewardPoints());   \nreturn rewardAccumulator;   \n}\nIn the transform() method, you first map a Purchase object into the RewardAccumu-\nlator—this is the same operation used in the mapValues() method. In the next few\nlines, the state gets involved in the transformation process. You do a lookup by key\n(customer ID) and add any points accumulated so far to the points from the current\npurchase. Then, you place the new total in the state store until it’s needed again.\n All that’s left is to update the rewards processor. But before you do, you need to\nconsider the fact that you’re accessing all sales by customer ID. Gathering information\nper sale for a given customer implies that all transactions for that customer are on the\nsame partition. But because the transactions come into the application without a key,\nthe producer assigns the transactions to partitions in a round-robin fashion. We cov-\nered round-robin partition assignment in chapter 2, but it’s worth reviewing it here\nagain—see figure 4.5.\n You’ll have an issue here (unless you’re using topics with only one partition).\nBecause the key isn’t populated, round-robin assignment means the transactions for a\ngiven customer won’t land on the same partition.\nStores the new \ntotal points in \nstateStore\nReturns the new \naccumulated \nrewards points\nBecause the keys are null, partition\nassignment starts at 0 and increases\nby 1 for each message up to 5. Then\nthe partition assignment starts over\nat 0 again.\nPartition 1\nPartition 0\nPartition 2\nPartition 3\nPartition 4\nPartition 5\nKafka broker 1\nKafka broker 2\nKafka broker 3\nProducer\nProducer\n0\n5\n4\n3\n2\n1\nFigure 4.5\nA Kafka producer distributes records evenly (round-robin) when the keys are null.\n \n\n\n92\nCHAPTER 4\nStreams and state\nPlacing customer transactions with the same ID on the same partition is important,\nbecause you need to look up records by ID in the state store. Otherwise, you’ll have\ncustomers with the same ID spread across different partitions, requiring you to look\nup the same customer in multiple state stores. (This statement could be interpreted to\nmean that each partition has its own state store, but that’s not the case. Partitions are\nassigned to a StreamTask, and each StreamTask has its own state store.)\n The way to solve this problem is to repartition the data by customer ID. We’ll look\nat how to do this next.\nREPARTITIONING THE DATA\nFirst, let’s have a general discussion on how repartitioning works (see figure 4.6). To\nrepartition records, first you may modify or change the key on the original record,\nand then you write out the record to a new topic. Next, you consume those records\nagain; but as a result of repartitioning, those records may come from different parti-\ntions than they were in originally.\nAlthough, in this simple example, you replaced the null key with a concrete value,\nrepartitioning need not always change the key. By using StreamPartitioner (http://\nmng.bz/9Z8A), you can apply just about any partition strategy you can think of, such\nThe keys are originally null, so distribution is done round-robin,\nresulting in records with the same ID across different partitions.\nNow with the key populated, all\nrecords with the identical ID land\nthe same partition.\nOriginal topic\nRepartition topic\n(null, {“id”:”5”, “info”:”123”} )\n(null, {“id”:”4”, “info”:”abc”} )\nnull, {“id”:”5”, “info”:”456”} )\n(null, {“id”:”4”, “info”:”def”} )\n(“4”, {“id”:”4”, “info”:”def”} )\n(“4”, {“id”:”4”, “info”:”abc”} )\n(“5”, {“id”:”5”, “info”:”456”} )\n(“5”, {“id”:”5”, “info”:”123”} )\nPartition 0\nPartition 1\nPartition 0\nPartition 1\nFor repartitioning, set the ID\nﬁeld as the key, and then write\nthe records to a topic.\nFigure 4.6\nRepartitioning: changing the original key to move records to a different partition\n \n\n\n93\nApplying stateful operations to Kafka Streams\nas partitioning on the value or part of the value instead of the key. In the next section,\nwe’ll demonstrate using StreamPartitioner in Kafka Streams.\nREPARTITIONING IN KAFKA STREAMS\nRepartitioning in Kafka Streams is easily accomplished by using the KStream.through()\nmethod, as illustrated in figure 4.7. The KStream.through() method creates an inter-\nmediate topic, and the current KStream instance will start writing records to that topic.\nA new KStream instance is returned from the through() method call, using the same\nintermediate topic for its source. This way, the data is seamlessly repartitioned.\nUnder the covers, Kafka Streams creates a sink and source node. The sink node is a\nchild processor of the calling KStream instance, and the new KStream instance uses\nthe new source node for its source of records. You could write the same type of sub-\ntopology yourself using the DSL, but using the KStream.through() method is more\nconvenient.\n If you’ve modified or changed keys and you don’t need a custom partition strategy,\nyou can rely on the DefaultPartitioner of the internal Kafka Streams Kafka-\nProducer to handle the partitioning. But if you’d like to apply your own partitioning\napproach, you can use StreamPartitioner. You’ll do just that in the next example.\n The code for using the KStream.through method is shown in the following list-\ning. In this example, KStream.through() takes two parameters: the topic name and\na Produced instance that provides the key Serde, the value Serde, and a Stream-\nPartitioner. Note that if you want to use the default key and value Serde instances\nand have no need for a custom partitioning strategy, there’s a version of KStream\n.through where you only provide the topic name.\nRewardsStreamPartitioner streamPartitioner =\n➥ new RewardsStreamPartitioner();           \nListing 4.4\nUsing the KStream.through method\nThe returned KStream instance immediately\nstarts to consume from the intermediate topic.\nOriginal KStream node\nmaking the “through” call\nWrites to topic\nReads from topic\nIntermediate topic\n(created by the application)\nFigure 4.7\nWriting out to an \nintermediate topic and then \nreading from it in a new \nKStream instance\nInstantiates the concrete \nStreamPartitioner instance\n \n\n\n94\nCHAPTER 4\nStreams and state\nKStream<String, Purchase> transByCustomerStream =\n➥ purchaseKStream.through(\"customer_transactions\",\nProduced.with(stringSerde,\npurchaseSerde,\nstreamPartitioner));      \nHere, you’ve instantiated a RewardsStreamPartitioner. Let’s take a quick look at how\nit works as well as demonstrate how to create a StreamPartitioner. \nUSING A STREAMPARTITIONER\nTypically, the partition assignment is calculated by taking the hash of an object, modu-\nlus the number of partitions. In this case, you want to use the customer ID found in\nthe Purchase object so that all data for a given customer ends up in the same state\nstore. The following listing shows the StreamPartitioner implementation (found in\nsrc/main/java/bbejeck/chapter_4/partitioner/RewardsStreamPartitioner.java).\npublic class RewardsStreamPartitioner implements\n➥ StreamPartitioner<String, Purchase> {\n@Override\npublic Integer partition(String key,\nPurchase value,\nint numPartitions) {\nreturn value.getCustomerId().hashCode() % numPartitions;   \n}\n}\nNotice that you haven’t generated a new key. You’re using a property of the value to\ndetermine the correct partition. The key point to take away from this quick detour is\nthat when you’re using state to update and modify records, it’s necessary for those\nrecords to be on the same partition.\nWARNING\nDon’t mistake this simple repartitioning demonstration for some-\nthing you can be cavalier with. Although repartitioning is sometimes neces-\nsary, it comes at the cost of duplicating your data and incurs processing\noverhead. My advice is to use mapValues(), transformValues(), or flatMap-\nValues() operations whenever possible, because map(), transform(), and\nflatMap() can trigger automatic repartitioning. It’s best to use repartitioning\nlogic sparingly.\nNow, let’s get back to making changes in the rewards processor node to support state-\nful transformation. \n4.2.5\nUpdating the rewards processor\nUp to this point, you’ve created a new processing node that writes purchase objects\nout to a topic, partitioned by customer ID. This new topic will also be the source for\nyour soon-to-be-updated rewards processor. You did this to ensure that all purchases\nListing 4.5\nRewardsStreamPartitioner\nCreates a new \nKStream with the \nKStream.through \nmethod\nDetermines the\npartition by\ncustomer ID\n \n\n\n95\nApplying stateful operations to Kafka Streams\nfor a given customer are written to the same partition; hence, you’ll use the same state\nstore for all purchases by a given customer. Figure 4.8 shows the updated processing\ntopology with the new through processor between the credit card–masking node (the\nsource for all purchase transactions) and the rewards processor.\nNow, you’ll use the new Stream instance (created by the KStream.through() method)\nto update the rewards processor and use the stateful transform approach with the fol-\nlowing code.\nKStream<String, RewardAccumulator> statefulRewardAccumulator =\n➥ transByCustomerStream.transformValues(() ->\n➥ new PurchaseRewardTransformer(rewardsStateStoreName),\nrewardsStateStoreName);  \nstatefulRewardAccumulator.to(\"rewards\",\nProduced.with(stringSerde,\nrewardAccumulatorSerde));  \nThe KStream.transformValues method takes a ValueTransformerSupplier<V, R>\ninstance, which is provided via a Java 8 lambda expression.\nListing 4.6\nChanging the rewards processor to use stateful transformation\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nThe through processor\nrepartitions data with\nnew keys.\nPatterns\nSource\nThrough\nprocessor\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFiltering\nprocessor\nIn-memory key/value store\nused by the Rewards transforms\nvalues for state processing.\nThe rewards-processor\nnode is updated to use\na stateful TransformValues\nprocessor.\nRewards\nprocessor\nLocal state\nstore\nMasking\nFigure 4.8\nThe new through processor ensures that you send purchases to partitions by customer ID, \nallowing the rewards processor to make the right updates using local state.\nUses a stateful \ntransformation\nWrites the \nresults out \nto a topic\n \n\n\n96\nCHAPTER 4\nStreams and state\n In this section, you’ve added stateful processing to a stateless node. By adding state\nto the processor, ZMart can take action sooner after a customer makes a reward-quali-\nfying purchase. You’ve seen how to use a state store and the benefits using one pro-\nvides, but we’ve glossed over important details that you’ll need to understand about\nhow state can impact your applications. With this in mind, the next section will discuss\nwhich type of state store to use, what requirements you’ll need to make state efficient,\nand how you can add the state stores to a Kafka Streams program. \n4.3\nUsing state stores for lookups and previously \nseen data\nIn this section, we’ll look at the essentials of using state stores in Kafka Streams and the\nkey factors related to using state in streaming applications in general. This will enable\nyou to make practical choices when using state in your Kafka Streams applications.\n So far, we’ve discussed the need for using state with streams, and you’ve seen an\nexample of one of the more basic stateful operations available in Kafka Streams.\nBefore we get into more detail about using state stores in Kafka Streams, let’s briefly\nlook at two important attributes of state: data locality and failure recovery.\n4.3.1\nData locality\nData locality is critical for performance. Although key lookups are typically very fast,\nthe latency introduced by using remote storage becomes a bottleneck when you’re\nworking at scale.\n Figure 4.9 illustrates the principle behind data locality. The dashed line represents a\nnetwork call to retrieve data from a remote database. The solid line depicts a call to an\nServer\nProcessor\nLocal data\nstore\nData locality\nRemote data\nstore\nThe processor needs to\ncommunicate with a remote\nstore: the data must travel\nfurther and is subject to\nnetwork availability.\nThe processor communicates\nwith a local store, which could\nbe in memory or off-heap: the\ndata travels a shorter distance,\nand there’s no reliance on\nnetwork availability.\nFigure 4.9\nData locality is necessary for stream processing.\n \n",
      "page_number": 102
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 115-122)",
      "start_page": 115,
      "end_page": 122,
      "detection_method": "topic_boundary",
      "content": "97\nUsing state stores for lookups and previously seen data\nin-memory data store located on the same server. As you can see, making a call to get\ndata locally is more efficient than making a call across a network to a remote database.\n The key point here isn’t the degree of latency per record retrieval, which may be\nminimal. The important factor is that you’ll potentially process millions or billions of\nrecords through a streaming application. When multiplied by a factor that large, even\na small degree of network latency can have a huge impact.\n Data locality also implies that the store is local to each processing node, and\nthere’s no sharing across processes or threads. This way, if a process fails, it shouldn’t\nhave an impact on the other stream-processing processes or threads.\n The key point here is that although streaming applications will sometimes require\nstate, it should be local to where the processing occurs. Each server or node in the\napplication should have an individual data store. \n4.3.2\nFailure recovery and fault tolerance\nApplication failure is inevitable, especially when it comes to distributed applications.\nWe need to shift our focus from preventing failure to recovering quickly from failure,\nor even from restarts.\n Figure 4.10 depicts the principles of data locality and fault tolerance. Each proces-\nsor has its local data store, and a changelog topic is used to back up the state store.\nProcess 2\nProcessor\nLocal data\nstore\nLocal data\nstore\nProcess 1\nFault tolerance and failure recovery:\ntwo Kafka Streams processes running on the same server\nBecause each process has its own local state store and a shared-nothing\narchitecture, if either process fails, the other process will be unaffected.\nAlso, each store has its keys/values replicated to a topic, which is used\nto recover values lost when a process fails or restarts.\nData in state\nstore backed\nup in topic\nTopic used\nas changelog\nfor store\nTopic used\nas changelog\nfor store\nData in state\nstore backed\nup in topic\nProcessor\nFigure 4.10\nThe ability to recover from failure is important for stream-processing applications. Kafka \nStreams persists data from the local in-memory stores to an internal topic, so when you resume \noperations after a failure or a restart, the data is repopulated.\n \n\n\n98\nCHAPTER 4\nStreams and state\nBacking up a state store with a topic may seem expensive, but there are a couple of\nmitigating factors at play: a KafkaProducer sends records in batches, and by default,\nrecords are cached. It’s only on cache flush that Kafka Streams writes records to the\nstore, so only the latest record for a given key is persisted. We’ll discuss this caching\nmechanism with state stores in more detail in chapter 5.\n The state stores provided by Kafka Streams meet both the locality and fault-toler-\nance requirements. They’re local to the defined processors and don’t share access\nacross processes or threads. State stores also use topics for backup and quick recovery.\n We’ve now covered the requirements for using state with a streaming application.\nThe next step is to look at how you can enable the use of state in a Kafka Streams\napplication. \n4.3.3\nUsing state stores in Kafka Streams\nAdding a state store is a simple matter of creating a StoreSupplier instance with one\nof the static factory methods on the Stores class. There are two additional classes for\ncustomizing the state store: the Materialized and StoreBuilder classes. Which one\nyou’ll use depends on how you add the store to the topology. If you use the high-level\nDSL, you’ll typically use the Materialized class; when you work with the lower-level\nProcessor API, you’ll use the StoreBuilder.\n Even though the current example uses the high-level DSL, you add a state store to\na Transformer, which provides Processor API semantics. So, you’ll use the StoreBuilder\nfor state store customization.\nString rewardsStateStoreName = \"rewardsPointsStore\";\nKeyValueBytesStoreSupplier storeSupplier =\n➥ Stores.inMemoryKeyValueStore(rewardsStateStoreName);  \nStoreBuilder<KeyValueStore<String, Integer>> storeBuilder =\n➥ Stores.keyValueStoreBuilder(storeSupplier,\nSerdes.String(),\nSerdes.Integer());    \nbuilder.addStateStore(storeBuilder);  \nYou first create a StoreSupplier that provides an in-memory key/value store. Then,\nyou provide the StoreSupplier as a parameter to create a StoreBuilder, additionally\nspecifying String keys and Integer values. Finally, you add the StateStore to the\ntopology by providing the StoreBuilder to the StreamsBuilder.\n Here, you’ve created an in-memory key/value store with String keys and Integer\nvalues, and you’ve added the store to the application with the StreamsBuilder\n.addStateStore method. As a result, you can now use the state in your processors by\nusing the name rewardsStateStoreName created above, for the state store.\nListing 4.7\nAdding a state store\nCreates the \nStateStore \nsupplier\nCreates the StoreBuilder \nand specifies the key \nand value types\nAdds the state store \nto the topology\n \n\n\n99\nUsing state stores for lookups and previously seen data\n You’ve now seen an example of building an in-memory state store, but you have\noptions for creating different types of StateStore instances. Let’s look at those options.\n4.3.4\nAdditional key/value store suppliers\nIn addition to the Stores.inMemoryKeyValueStore method, you can use these other\nstatic factory methods for producing store suppliers:\n\nStores.persistentKeyValueStore\n\nStores.lruMap\n\nStores.persistentWindowStore\n\nStores.persistentSessionStore\nIt’s worth noting that all persistent StateStore instances provide local storage using\nRocksDB (http://rocksdb.org).\n Before we move on from state stores, I’d like to cover two other important aspects\nof Kafka Streams state stores: how fault tolerance is provided with changelog topics,\nand how you can configure changelog topics. \n4.3.5\nStateStore fault tolerance\nAll the StateStoreSupplier types have logging enabled as a default. Logging, in this\ncontext, means a Kafka topic used as a changelog to back up the values in the store\nand provide fault tolerance.\n For example, suppose you lost a machine running Kafka Streams. Once you recov-\nered your server and restarted your Kafka Streams application, the state stores for that\ninstance would be restored to their original contents (the last committed offset in the\nchangelog before crashing).\n This logging can be disabled when using the Stores factory with the disable-\nLogging() method. But you shouldn’t disabling logging without serious consideration,\nbecause doing so removes fault tolerance from the state stores and eliminates their\nability to recover after a crash. \n4.3.6\nConfiguring changelog topics\nThe changelogs for state stores are configurable via the withLoggingEnabled(Map\n<String, String> config) method. You can use any configuration parameters avail-\nable for topics in the map. The configuration of changelogs for state stores is import-\nant when building a Kafka Streams application. But keep in mind that you never need\nto create changelog topics—Kafka Streams handles changelog topic creation for you.\nNOTE\nState store changelogs are compacted topics, which we discussed in\nchapter 2. As you may recall, the delete semantics require a null value for a\nkey, so if you want to remove a record from a state store permanently, you’ll\nneed to do a put(key, null) operation.\nWith Kafka topics, the default setting for data retention for a log segment is one week,\nand the size is unlimited. Depending on your volume of data, this might be acceptable,\n \n\n\n100\nCHAPTER 4\nStreams and state\nbut there’s a good chance you’ll want to adjust those settings. Additionally, the default\ncleanup policy is delete.\n Let’s first take a look at how you can configure your changelog topic to have a\nretention size of 10 GB and a retention time of 2 days.\nMap<String, String> changeLogConfigs = new HashMap<>();\nchangeLogConfigs.put(\"retention.ms\",\"172800000\" );\nchangeLogConfigs.put(\"retention.bytes\", \"10000000000\");\n// to use with a StoreBuilder\nstoreBuilder.withLoggingEnabled(changeLogConfigs);\n// to use with Materialized\nMaterialized.as(Stores.inMemoryKeyValueStore(\"foo\")\n.withLoggingEnabled(changeLogConfigs));\nIn chapter 2, we discussed compacted topics offered by Kafka. To refresh your mem-\nory: compacted topics use a different approach to cleaning a topic. Instead of deleting\nlog segments by size or time, log segments are compacted by keeping only the latest\nrecord for each key—older records with the same key are deleted. By default, Kafka\nStreams creates changelog topics with a delete policy of compact.\n But if you have a changelog topic with a lot of unique keys, compaction might not\nbe enough, as the size of the log segment will keep growing. In that case, the solution\nis simple. You specify a cleanup policy of delete and compact.\nMap<String, String> changeLogConfigs = new HashMap<>();\nchangeLogConfigs.put(\"retention.ms\",\"172800000\" );\nchangeLogConfigs.put(\"retention.bytes\", \"10000000000\");\nchangeLogConfigs.put(\"cleanup.policy\", \"compact,delete\");\nNow your changelog topic will be kept at a reasonable size even with unique keys. This\nhas been a brief section on topic configuration; appendix A provides more informa-\ntion about changelog topics and internal topic configuration.\n You’ve been introduced to the basics of stateful operations and state stores. You’ve\nlearned about the in-memory and persistent state stores Kafka Streams provides and\nhow you can include them in your streaming applications. You’ve also learned about\nthe importance of data locality and fault tolerance when using state in a streaming\napplication. Let’s move on to joining streams. \n4.4\nJoining streams for added insight\nAs we discussed earlier in the chapter, streams need state when events in the stream\ndon’t stand alone. Sometimes the state or context you need is another stream. In this\nListing 4.8\nSetting changelog properties\nListing 4.9\nSetting a cleanup policy\n \n\n\n101\nJoining streams for added insight\nsection, you’ll take different events from two streams with the same key, and combine\nthem to form a new event.\n The best way to learn about joining streams is to look at a concrete example, so\nwe’ll return to the ZMart scenario. As you’ll recall, ZMart opened a new line of stores\nthat carried electronics and related merchandise (CDs, DVDs, smart phones, and so\non). Trying a new approach, ZMart has partnered with a national coffee house and\nhas embedded a cafe in each store.\n In chapter 3, you were asked to separate the purchase transactions in those stores\ninto two distinct streams. Figure 4.11 shows the topology for this requirement.\nThis approach of embedding the cafe has been a big success for ZMart, and the com-\npany would like to see this trend continue, so they’ve decided to start a new program.\nZMart wants to keep traffic in the electronics store high by offering coupons for the\ncafe (hoping that increased traffic leads to additional purchases).\n ZMart wants to identify customers who have bought coffee and made a purchase in\nthe electronics store and give them coupons almost immediately after their second\ntransaction (see figure 4.12). ZMart intends to see if it can generate some sort of Pav-\nlovian response in their customers.\nThe KStream.branch method takes an array of\npredicates and returns an array containing an equal\nnumber of KStream instances, each one accepting\nrecords matching the corresponding predicate.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFiltering\nprocessor\nProcessor for records\nmatching predicate at\nindex 0\nProcessor for records\nmatching predicate at\nindex 1\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nFigure 4.11\nThe branch processor, and where it stands in the overall topology\n \n\n\n102\nCHAPTER 4\nStreams and state\nTo determine when to issue a coupon, you’ll need to join the sales from the cafe with\nthe sales in the electronics store. Joining streams is relatively straightforward in terms\nof the code you need to write. Let’s start by setting up the data you need to process for\ndoing joins.\n4.4.1\nData setup\nFirst, let’s take another look at the portion of the topology responsible for branching\nthe streams (figure 4.13). In addition, let’s review the code used to implement the\nDetermining free coffee coupons\nIssue coupon for free\ncoffee drink.\nCafe\npurchase\nElectronics\nstore\npurchase\nCafe\npurchase\nElectronics\nstore\npurchase\n20-minute interval\n20-minute interval\nFigure 4.12\nPurchase records with timestamps within 20 minutes of each other are joined by \ncustomer ID and used to issue a reward to the customer—a free coffee drink, in this case.\nProcessor for records\nmatching predicate at\nindex 0\nThis processor contains the\narray of predicates and returns\nan equal number of KStream\ninstances, accepting records that\nmatch the given predicate.\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nProcessor for records\nmatching predicate at\nindex 1\nFigure 4.13\nTo perform a join, you need more than one stream. The branch processor \ntakes care of this by creating two streams: one containing cafe purchases and the other \ncontaining electronics purchases.\n \n\n\n103\nJoining streams for added insight\nbranching requirement (found in src/main/java/bbejeck/chapter_3/ZMartKafka-\nStreamsAdvancedReqsApp.java).\nPredicate<String, Purchase> coffeePurchase = (key, purchase) ->\n➥ purchase.getDepartment().equalsIgnoreCase(\"coffee\");\nPredicate<String, Purchase> electronicPurchase = (key, purchase) ->\n➥ purchase.getDepartment().equalsIgnoreCase(\"electronics\");     \nfinal int COFFEE_PURCHASE = 0;\nfinal int ELECTRONICS_PURCHASE = 1;     \nKStream<String, Purchase>[] branchedTransactions =\n➥ transactionStream.branch(coffeePurchase, electronicPurchase);  \nThis code shows how to perform branching: you use predicates to match incoming\nrecords into an array of KStream instances. The order of matching is the same as the\nposition of KStream objects in the array. The branching process drops any record not\nmatching any predicate.\n Although you have two streams to join, there’s an additional step to perform.\nRemember that purchase records come into the Kafka Streams application with no\nkeys. As a result, you need to add another processor to generate a key containing the\ncustomer ID. You need populated keys because that’s what you’ll use to join the\nrecords together. \n4.4.2\nGenerating keys containing customer IDs to perform joins\nTo generate a key, you select the customer ID from the purchase data in the stream.\nTo do so, you need to update the original KStream instance (transactionStream) and\ncreate another processing node between it and the branch node. This is shown in the fol-\nlowing code (found in src/main/java/bbejeck/chapter_4/KafkaStreamsJoinsApp.java).\nKStream<String, Purchase>[] branchesStream =\n➥ transactionStream.selectKey((k,v)->\n➥ v.getCustomerId()).branch(coffeePurchase, electronicPurchase);   \nFigure 4.14 shows an updated view of the processing topology based on listing 4.11.\nYou’ve seen before that changing the key may require you to repartition the data.\nThat’s true in this example as well, so why isn’t there a repartitioning step?\n In Kafka Streams, whenever you invoke a method that could result in generating a\nnew key (selectKey, map, or transform), an internal Boolean flag is set to true, indi-\ncating that the new KStream instance requires repartitioning. With this Boolean flag\nset, if you perform a join, reduce, or aggregation operation, the repartitioning is han-\ndled for you automatically.\nListing 4.10\nBranching into two streams\nListing 4.11\nGenerating new keys\nDefines the \npredicates \nfor matching \nrecords\nUses labeled integers for \nclarity when accessing the \ncorresponding array\nCreates the \nbranched \nstream\nInserts the selectKey\nprocessing node\n \n\n\n104\nCHAPTER 4\nStreams and state\nIn this example, you perform a selectKey() operation on the transactionStream, so\nthe resulting KStream is flagged for repartitioning. Additionally, you immediately per-\nform a branching operation, so each KStream resulting from the branch() call is\nflagged for repartitioning as well.\nNOTE\nIn the example, you repartition by the key only. But there may be times\nwhen you either don’t want to use the key or want to use some combination\nof the key and value. In these cases, you can use the StreamPartitioner<K,\nV> interface, as you saw in the example in listing 4.5 in the section, “Using a\nStreamPartitioner.”\nNow that you have two separate streams with populated keys, you’re ready for the next\nstep: joining the streams by key. \n4.4.3\nConstructing the join\nThe next step is to perform the actual join. You’ll take the two branched streams and\njoin them with the KStream.join() method. The topology is shown in figure 4.15.\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nThis processor is inserted to extract the\ncustomer ID from the transaction object to\nbe used for the key. This sets up the join\nbetween the two types of purchases.\nProcessor for records\nmatching predicate at\nindex 0\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFiltering\nprocessor\nSelect-key\nprocessor\nProcessor for records\nmatching predicate at\nindex 1\nFigure 4.14\nYou need to remap the key/value purchase records into records where the key contains the \ncustomer ID. Fortunately, you can extract the customer ID from the Purchase object.\n \n",
      "page_number": 115
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 123-132)",
      "start_page": 123,
      "end_page": 132,
      "detection_method": "topic_boundary",
      "content": "105\nJoining streams for added insight\nJOINING PURCHASE RECORDS\nTo create the joined record, you need to create an instance of a ValueJoiner<V1, V2,\nR>. ValueJoiner takes two objects, which may or may not be of the same type, and it\nreturns a single object, possibly of a different type. In this case, ValueJoiner takes two\nPurchase objects and returns a CorrelatedPurchase object. Let’s take a look at the\ncode (found in src/main/java/bbejeck/chapter_4/joiner/PurchaseJoiner.java).\npublic class PurchaseJoiner\n➥ implements ValueJoiner<Purchase, Purchase, CorrelatedPurchase> {\n@Override\npublic CorrelatedPurchase apply(Purchase purchase, Purchase purchase2) {\nCorrelatedPurchase.Builder builder =\n➥ CorrelatedPurchase.newBuilder();           \nDate purchaseDate =\n➥ purchase != null ? purchase.getPurchaseDate() : null;\nDouble price = purchase != null ? purchase.getPrice() : 0.0;\nListing 4.12\nValueJoiner implementation\nPatterns\nMasking\nSource\nJoin\nprocessor\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nSelect-key\nprocessor\nBranch\nprocessor\nJoin results\nFiltering\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nLocal state\nstore\nTransform\nvalues\nLocal state\nstore\nLocal state\nstore\nThe join is actually a series of\nprocessors used to complete\nthe join. Because that code is\ninternal to Kafka Streams, you\nabstract the join away and\nrepresent it as one “logical”\njoin processor.\nRecords from\nboth processors\nﬂow into the\njoin processor.\nTwo state stores are used for\njoins: one to hold the keys\nfor the main stream, and the\nother to look for matches in\nthe other stream.\nThrough\nprocessor\nFigure 4.15\nIn the updated topology, both the cafe and electronics processors forward their records to the join \nprocessor. The join processor uses two state stores to search for matches for a record in the other stream.\nInstantiates \nthe builder\n \n\n\n106\nCHAPTER 4\nStreams and state\nString itemPurchased =\n➥ purchase != null ? purchase.getItemPurchased() : null;  \nDate otherPurchaseDate =\n➥ otherPurchase != null ? otherPurchase.getPurchaseDate() : null;\nDouble otherPrice =\n➥ otherPurchase != null ? otherPurchase.getPrice() : 0.0;\nString otherItemPurchased =\n➥ otherPurchase != null ? otherPurchase.getItemPurchased() : null; \nList<String> purchasedItems = new ArrayList<>();\nif (itemPurchased != null) {\npurchasedItems.add(itemPurchased);\n}\nif (otherItemPurchased != null) {\npurchasedItems.add(otherItemPurchased);\n}\nString customerId =\n➥ purchase != null ? purchase.getCustomerId() : null;\nString otherCustomerId =\n➥ otherPurchase != null ? otherPurchase.getCustomerId() : null;\nbuilder.withCustomerId(customerId != null ? customerId : otherCustome\nrId)\n.withFirstPurchaseDate(purchaseDate)\n.withSecondPurchaseDate(otherPurchaseDate)\n.withItemsPurchased(purchasedItems)\n.withTotalAmount(price + otherPrice);\nreturn builder.build();      \n}\n}\nTo create the CorrelatedPurchase object, you extract some information from each\nPurchase object. Because of the number of items you need to construct the new\nobject, you use the builder pattern, which makes the code clearer and eliminates any\nerrors due to misplaced parameters. Additionally, the PurchaseJoiner checks for null\nvalues with both of the provided Purchase objects, so it can be used for inner, outer,\nand left-outer joins. We’ll discuss the different join options in section 4.4.4. For now,\nwe’ll move on to implementing the join between streams. \nIMPLEMENTING THE JOIN\nYou’ve seen how to merge records resulting from the join between streams, so let’s\nmove on to calling the actual KStream.join method (found in src/main/java/bbejeck/\nchapter_4/KafkaStreamsJoinsApp.java).\nHandles a null \nPurchase in the case \nof an outer join\nHandles a null\nPurchase in the\ncase of a left\nouter join\nReturns the new \nCorrelatedPurchase \nobject\n \n\n\n107\nJoining streams for added insight\nKStream<String, Purchase> coffeeStream =\n➥ branchesStream[COFFEE_PURCHASE];       \nKStream<String, Purchase> electronicsStream =\n➥ branchesStream[ELECTRONICS_PURCHASE];\nValueJoiner<Purchase, Purchase, CorrelatedPurchase> purchaseJoiner =\n➥ new PurchaseJoiner();                                \nJoinWindows twentyMinuteWindow =\nJoinWindows.of(60 * 1000 * 20);\nKStream<String, CorrelatedPurchase> joinedKStream =\n➥ coffeeStream.join(electronicsStream,         \npurchaseJoiner,\ntwentyMinuteWindow,\nJoined.with(stringSerde,\npurchaseSerde,\npurchaseSerde));     \njoinedKStream.print(\"joinedStream\");    \nYou supply four parameters to the KStream.join method:\n\nelectronicsStream—The stream of electronic purchases to join with.\n\npurchaseJoiner—An implementation of the ValueJoiner<V1, V2, R> inter-\nface. ValueJoiner accepts two values (not necessarily of the same type). The\nValueJoiner.apply method performs the implementation-specific logic and\nreturns a (possibly new) object of type R (maybe a whole new type). In this exam-\nple, purchaseJoiner will add some relevant information from both Purchase\nobjects, and it will return a CorrelatedPurchase object.\n\ntwentyMinuteWindow—A JoinWindows instance. The JoinWindows.of method\nspecifies a maximum time difference between the two values to be included in\nthe join. In this case, the timestamps must be within 20 minutes of each other.\nA Joined instance—Provides optional parameters for performing joins. In this\ncase, it’s the key and the value Serde for the calling stream, and the value Serde\nfor the secondary stream. You only have one key Serde because, when joining\nrecords, keys must be of the same type.\nNOTE\nSerdes are required for joins because join participants are materialized\nin windowed state stores. This example provides only one Serde for the key,\nbecause both sides of the join must have a key of the same type.\nYou’ve specified that the purchases need to be within 20 minutes of each other, but no\norder is implied. As long as the timestamps are within 20 minutes of each other, the\njoin will occur.\n Two additional JoinWindows() methods are available, which you can use to specify\nthe order of events:\nListing 4.13\nUsing the join() method\nExtracts the \nbranched streams\nValueJoiner instance used\nto perform the join\nCalls the join method, \ntriggering automatic \nrepartitioning of \ncoffeeStream and \nelectronicsStream\nConstructs the join\nPrints the join results \nto the console\n \n\n\n108\nCHAPTER 4\nStreams and state\n\nJoinWindows.after—streamA.join(streamB,...,twentyMinuteWindow.after\n(5000)....) This specifies that the timestamp of the streamB record is at most\n5 seconds after the timestamp of the streamA record. The starting time bound-\nary of the window is unchanged.\n\nJoinWindows.before—streamA.join(streamB,...,twentyMinuteWindow\n.before(5000),...) This specifies that the timestamp of the streamB record is\nat most 5 seconds before the timestamp of the streamA record. The ending time\nboundary of the window is unchanged.\nWith both the before() and after() methods, the time difference is expressed in\nmilliseconds. The timespans used for the join are an example of sliding windows. We’ll\nlook at windowing operations in detail in the next chapter.\nNOTE\nIn listing 4.13, you’re relying on the actual timestamps of the transac-\ntion, not timestamps set by Kafka. In order to use the timestamps embedded\nin the transaction, you specify a custom timestamp extractor by setting Streams-\nConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG to use Transaction-\nTimestampExtractor.class.\nYou’ve now constructed a joined stream: electronics purchases made within 20 min-\nutes of a coffee purchase will result in a coupon for a free drink on the customer’s\nnext visit to ZMart.\n Before we go any further, I’d like to take a minute to explain an important require-\nment for joining data—co-partitioning. \nCO-PARTITIONING\nIn order to perform a join in Kafka Streams, you need to ensure that all join partici-\npants are co-partitioned, meaning that they have the same number of partitions and are\nkeyed by the same type. As a result, when you call the join() method in listing 4.13,\nboth KStream instances will be checked to see if a repartition is required.\nNOTE\nGlobalKTable instances don’t require repartitioning when involved in\na join.\nIn section 4.4.2, you use the selectKey() method on the transactionStream and\nimmediately branched on the returned KStreams. Because the selectKey() method\nmodifies the key, both coffeeStream and electronicsStream require repartitioning.\nIt’s worth repeating that repartitioning is necessary because you need to ensure that\nidentical keys are written to the same partition. This repartitioning is handled auto-\nmatically. Additionally, when you start your Kafka Streams application, topics involved\nin a join are checked to make sure they have the same number of partitions; if any\nmismatches are found, a TopologyBuilderException is thrown. It’s the developer’s\nresponsibility to ensure the keys involved in a join are of the same type.\n Co-partitioning also requires all Kafka producers to use the same partitioning\nclass when writing to Kafka Streams source topics. Likewise, you need to use the same\n \n\n\n109\nJoining streams for added insight\nStreamPartitioner for any operations writing Kafka Streams sink topics via the\nKStream.to() method. If you stick with the default partitioning strategies, you won’t\nneed to worry about partitioning strategies.\n Let’s continue on with joins and look at the other options available to you. \n4.4.4\nOther join options\nThe join in listing 4.13 is an inner join. With an inner join, if either record isn’t pres-\nent, the join doesn’t occur, and you don’t emit a CorrelatedPurchase object. There\nare other options that don’t require both records. These are useful if you need infor-\nmation even when the desired record for joining isn’t available.\nOUTER JOINS\nOuter joins always output a record, but the forwarded join record may not include\nboth of the events specified by the join. If either side of the join isn’t present when\nthe time window expires, an outer join sends the record that’s available down-\nstream. Of course, if both events are present within the window, the issued record\ncontains both events.\n For example, if you wanted to use an outer join in listing 4.13, you’d do so like this:\ncoffeeStream.outerJoin(electronicsStream,..)\nFigure 4.16 demonstrates the three possible outcomes of the outer join. \nOnly the calling stream’s event is available\nin the time window, so that’s the only record\nincluded.\nBoth streams’ events are available in the\ntime window, so both are included in the\njoin record.\nOnly the other stream’s event is available\nin the time window, so nothing is sent\ndownstream.\nJoin time window\n(Coffee purchase, null)\n(Coffee purchase, electronics purchase)\n(Null, electronics purchase)\nElectronics\npurchase\nCoffee\npurchase\nCoffee\npurchase\nElectronics\npurchase\nFigure 4.16\nThree outcomes are possible with outer joins: only the calling stream’s event, both \nevents, and only the other stream’s event.\n \n\n\n110\nCHAPTER 4\nStreams and state\nLEFT-OUTER JOIN\nThe records sent downstream from a left-outer join are similar to an outer join, with\none exception. When the only event available in the join window is from the other\nstream, there’s no output at all. If you wanted to use a left-outer join in listing 4.13,\nyou’d do so like this: \ncoffeeStream.leftJoin(electronicsStream..) \nFigure 4.17 shows the outcomes of the left-outer join.\nWe’ve now covered joining streams, but there’s one concept that deserves a more\ndetailed discussion: timestamps and the impact they have on your Kafka Streams\napplication. In the join example, you specified a maximum time difference between\nevents of 20 minutes. In this case, it’s the time between purchases, but how you set or\nextract these timestamps wasn’t specified. Let’s take a closer look at that. \n4.5\nTimestamps in Kafka Streams\nSection 2.4.4 discussed timestamps in Kafka records. In this section, we’ll discuss the\nuse of timestamps in Kafka Streams. Timestamps play a role in key areas of Kafka\nStreams functionality:\nJoining streams\nUpdating a changelog (KTable API)\nDeciding when the Processor.punctuate() method is triggered (Processor API)\nOnly the calling stream’s event is available\nin the time window, so that’s the only record\nincluded.\nBoth streams’ events are available in the\ntime window, so both are included in the\njoin record.\nOnly the other stream’s event is available\nin the time window, so nothing is sent\ndownstream.\nJoin time window\n(Coffee purchase, null)\n(Coffee purchase, electronics purchase)\nNo output\nElectronics\npurchase\nCoffee\npurchase\nCoffee\npurchase\nElectronics\npurchase\nFigure 4.17\nThree outcomes are possible with the left-outer join, but there’s no output if only \nthe other stream’s record is available.\n \n\n\n111\nTimestamps in Kafka Streams\nWe haven’t covered the KTable or Processor APIs yet, but that’s OK. You don’t need\nthem to understand this section.\n In stream processing, you can group timestamps into three categories, as shown in\nfigure 4.18:\nEvent time—A timestamp set when the event occurred, usually embedded in the\nobject used to represent the event. For our purposes, we’ll consider the time-\nstamp set when the ProducerRecord is created as the event time as well.\nTimestamp embedded in data object at time of event, or\ntimestamp set in ProducerRecord by a Kafka producer\nTimestamp set at time record is appended to log (topic)\nTimestamp generated at the moment when record\nis consumed, ignoring timestamp embedded in data\nobject and ConsumerRecord\nEvent time\nIngest time\nProcessing time\nOr\nValue\nRecord\nTimestamp\nRecord\nValue\nTimestamp\nValue\nRecord\nTimestamp\nTimestamp generated\nwhen record is consumed\n(wall-clock time)\nTimestamp\nKafka producer\nKafka broker\nKafka Streams\nSome event\ntimestamp\nFigure 4.18\nThere are three categories of timestamps in Kafka Streams: event \ntime, ingestion time, and processing time.\n \n\n\n112\nCHAPTER 4\nStreams and state\nIngestion time—A timestamp set when the data first enters the data processing\npipeline. You can consider the timestamp set by the Kafka broker (assuming a\nconfiguration setting of LogAppendTime) to be ingestion time.\nProcessing time—A timestamp set when the data or event record first starts to flow\nthrough a processing pipeline.\nYou’ll see in this section how the Kafka Streams API supports all three types of process-\ning timestamps.\nNOTE\nSo far, we’ve had an implicit assumption that clients and brokers are\nlocated in the same time zone, but that might not always be the case. When\nusing timestamps, it’s safest to normalize the times using the UTC time zone,\neliminating any confusion over which brokers and clients are using which\ntime zones.\nWe’ll consider three cases of timestamp-processing semantics:\nA timestamp embedded in the actual event or message object (event-time\nsemantics)\nUsing the timestamp set in the record metadata when creating the ProducerRe-\ncord (event-time semantics)\nUsing the current timestamp (current local time) when the Kafka Streams\napplication ingests the record (processing-time semantics)\nFor event-time semantics, using the timestamp placed in the metadata by the Producer-\nRecord is sufficient. But there may be cases when you have different needs. Consider\nthese examples:\nYou’re sending messages to Kafka with events that have timestamps recorded in\nthe message objects. There’s some lag time in when these event objects are\nmade available to the Kafka producer, so you want to consider only the embed-\nded timestamp.\nYou want to consider the time when your Kafka Streams application consumes\nrecords as opposed to using the timestamps of the records.\nTo enable different processing semantics, Kafka Stream provides a TimestampExtractor\ninterface with one abstract and four concrete implementations. If you need to work\nwith timestamps embedded in the record values, you’ll need to create a custom Time-\nstampExtractor implementation. Let’s briefly look at the included implementations\nand implement a custom TimestampExtractor.\n4.5.1\nProvided TimestampExtractor implementations\nAlmost all of the provided TimestampExtractor implementations work with time-\nstamps set by the producer or broker in the message metadata, thus providing either\nevent-time processing semantics (timestamp set by the producer) or log-append-time\n \n\n\n113\nTimestamps in Kafka Streams\nprocessing semantics (timestamp set by the broker). Figure 4.19 demonstrates pulling\nthe timestamp from the ConsumerRecord object.\nAlthough you’re assuming the default configuration setting of CreateTime for the\ntimestamp, bear in mind that if you were to use LogAppendTime, this would return\nthe timestamp value for when the Kafka broker appended the record to the log.\nExtractRecordMetadataTimestamp is an abstract class that provides the core function-\nality for extracting the metadata timestamp from the ConsumerRecord. Most of the\nconcrete implementations extend this class. Implementors override the abstract\nmethod, ExtractRecordMetadataTimestamp.onInvalidTimestamp, to handle invalid\ntimestamps (when the timestamp is less than 0).\n Here’s a list of classes that extend the ExtractRecordMetadataTimestamp class:\n\nFailOnInvalidTimestamp—Throws an exception in the case of an invalid time-\nstamp.\n\nLogAndSkipOnInvalidTimestamp—Returns the invalid timestamp and logs a\nwarning message that the record will be discarded due to the invalid timestamp.\n\nUsePreviousTimeOnInvalidTimestamp—In the case of an invalid timestamp,\nthe last valid extracted timestamp is returned.\nWe’ve covered the event-time timestamp extractors, but there’s one more provided\ntimestamp extractor to cover. \n4.5.2\nWallclockTimestampExtractor\nWallclockTimestampExtractor provides process-time semantics and doesn’t extract\nany timestamps. Instead, it returns the time in milliseconds by calling the System\n.currentTimeMillis() method.\n That’s it for the provided timestamp extractors. Next, we’ll look at how you can\ncreate a custom version. \nTimestamp\nConsumer timestamp extractor\nretrieves timestamp set by\nKafka producer or broker\nDotted rectangle represents\nConsumerRecord metadata\nKey\nValue\nEntire enclosing rectangle represents\na ConsumerRecord object\nFigure 4.19\nTimestamps in the ConsumerRecord object: either the \nproducer or broker set this timestamp, depending on your configuration.\n \n\n\n114\nCHAPTER 4\nStreams and state\n4.5.3\nCustom TimestampExtractor\nTo work with timestamps (or calculate one) in the value object from the Consumer-\nRecord, you’ll need a custom extractor that implements the TimestampExtractor\ninterface. Figure 4.20 depicts using the timestamp embedded in the value object ver-\nsus one set by Kafka (either producer or broker).\nHere’s an example of a TimestampExtractor implementation (found in src/main/\njava/bbejeck/chapter_4/timestamp_extractor/TransactionTimestampExtractor.java),\nalso used in the join example from listing 4.12 in the section “Implementing the Join”\n(although not shown in the text, because it’s a configuration parameter).\npublic class TransactionTimestampExtractor implements TimestampExtractor {\n@Override\npublic long extract(ConsumerRecord<Object, Object> record,\n➥ long previousTimestamp) {\nPurchase purchaseTransaction = (Purchase) record.value();   \nreturn purchaseTransaction.getPurchaseDate().getTime();   \n}\n}\nIn the join example, you used a custom TimestampExtractor because you wanted to\nuse the timestamps of the actual purchase time. This approach allows you to join the\nrecords even if there are delays in delivery or out-of-order arrivals.\nListing 4.14\nCustom TimestampExtractor\nTimestamp\nConsumerRecord metadata\nCustom TimestampExtractor knows where\nto pull the timestamp from the value in a\nConsumerRecord object\nKey\nValue\nEntire enclosing\nrectangle represents a\nConsumerRecord object\nRecord in JSON format\n{ “recordType” = “purchase”,\n“amount” = 500.00,\n“timestamp” = 1502041889179 }\nFigure 4.20\nA custom TimestampExtractor provides a timestamp based \non the value contained in the ConsumerRecord. This timestamp could be an \nexisting value or one calculated from properties contained in the value object.\nRetrieves the Purchase object from\nthe key/value pair sent to Kafka\nReturns the timestamp\nrecorded at the point of sale\n \n",
      "page_number": 123
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 133-142)",
      "start_page": 133,
      "end_page": 142,
      "detection_method": "topic_boundary",
      "content": "115\nSummary\nWARNING\nWhen you create a custom TimestampExtractor, take care not to\nget too clever. Log retention and log rolling are timestamp based, and the\ntimestamp provided by the extractor may become the message timestamp\nused by changelogs and output topics downstream. \n4.5.4\nSpecifying a TimestampExtractor\nNow that we’ve discussed how timestamp extractors work, let’s tell the application\nwhich one to use. You have two choices for specifying timestamp extractors.\n The first option is to set a global timestamp extractor, specified in the properties\nwhen setting up your Kafka Streams application. If no property is set, the default set-\nting is FailOnInvalidTimestamp.class. For example, the following code would con-\nfigure the TransactionTimestampExtractor via properties when setting up the\napplication:\nprops.put(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG,\n➥ TransactionTimestampExtractor.class);\nThe second option is to provide a TimestampExtractor instance via a Consumed object:\nConsumed.with(Serdes.String(), purchaseSerde)\n.withTimestampExtractor(new TransactionTimestampExtractor()))\nThe advantage of doing this is that you have one TimestampExtractor per input\nsource, whereas the other option requires you to handle records from different topics\nin one TimestampExtractor instance.\n We’ve come to the end of our discussion of timestamp usage. In the coming chap-\nters, you’ll run into situations where the difference between timestamps drives some\naction, such as flushing the cache of a KTable. I don’t expect you remember all three\ntypes of timestamp extractors, but it’s vital to understand that timestamps are an\nimportant part of how Kafka and Kafka Streams function. \nSummary\nStream processing needs state. Sometimes events can stand on their own, but\nusually you’ll need additional information to make good decisions.\nKafka Streams provides useful abstractions for stateful transformations, includ-\ning joins.\nState stores in Kafka Streams provide the type of state required for stream pro-\ncessing: data locality and fault tolerance.\nTimestamps control the flow of data in Kafka Streams. The choice of timestamp\nsources needs careful consideration.\nIn the next chapter, we’ll continue exploring state in streams with more-significant\noperations, like aggregations and grouping. We’ll also explore the KTable API. Whereas\n \n\n\n116\nCHAPTER 4\nStreams and state\nthe KStream API concerns itself with individual discrete records, a KTable is an imple-\nmentation of a changelog, where records with the same key are considered updates.\nWe’ll also discuss joins between KStream and KTable instances. Finally, we’ll explore\none of the most exciting developments in Kafka Streams: queryable state. Queryable\nstate allows you to directly observe the state of your stream, without having to material-\nize the information by reading data from a topic in an external application.\n \n\n\n117\nThe KTable API\nSo far, we’ve covered the KStream API and adding state to a Kafka Streams applica-\ntion. In this chapter, we’re going to look deeper into adding state. Along the way,\nyou’ll be introduced to a new abstraction, the KTable.\n In discussing the KStream API, we’ve talked about individual events or an event\nstream. In the original ZMart example, when Jane Doe made a purchase, you con-\nsidered the purchase to be an individual event. You didn’t keep track of how many\npurchases Jane made, or how often. In the context of a database, the purchase\nevent stream could be considered a series of inserts. Because each record is new\nand unrelated to any other record, you could continually insert them into a table.\nThis chapter covers\nDefining the relationship between streams \nand tables\nUpdating records, and the KTable abstraction\nAggregations, and windowing and joining \nKStreams and KTables\nGlobal KTables\nQueryable state stores\n \n\n\n118\nCHAPTER 5\nThe KTable API\n Now let’s add a primary key (customer ID) to each purchase event. You have a\nseries of related purchase events or updates for Jane Doe. Because you’re using a pri-\nmary key, each purchase is updated with respect to Jane’s purchase activity. Treating\nan event stream as inserts, and events with keys as updates, is how you’ll define the\nrelationship between streams and tables.\n In this chapter, we’ll cover the relationship between streams and tables in more\ndepth. This relationship is important, as it will help you understand how the KTable\noperates.\n Second, we’ll discuss the KTable. The KTable API is necessary because it’s designed\nto work with updates to records. We need the ability to work with updates for opera-\ntions like aggregations and counts. We touched on updates in chapter 4 when intro-\nducing stateful transformations; in section 4.2.5, you updated the rewards processor\nto keep track of customer purchases.\n Third, we’ll get into windowing operations. Windowing is the process of bucketing\ndata for a given period. For example, how many purchases have there been over the\npast hour, updated every ten minutes? Windowing allows you to gather data in\nchunks, as opposed to having an unbounded collection.\nNOTE\nWindowing and bucketing are somewhat synonymous terms. Both oper-\nate by placing information into smaller chunks or categories. Windowing\nimplies categorizing by time, but the result of either operation is the same.\nOur final topic in this chapter will be queryable state stores. Queryable state stores are\nan exciting feature of Kafka Streams: they allow you to run direct queries against state\nstores. In other words, you can view stateful data without having to consume it from a\nKafka topic or read it from a database. Let’s move on to our first topic.\n5.1\nThe relationship between streams and tables\nIn chapter 1, I defined a stream as an infinite sequence of events. That wording is\npretty generic, so let’s narrow it down with a specific example.\n5.1.1\nThe record stream\nSuppose you want to view a series of stock price updates. You can recast the generic\nmarble diagram from chapter 1 to look like figure 5.1. You can see that each stock\nprice quote is a discrete event, and they aren’t related to each other. Even if the same\ncompany accounts for many price quotes, you’re only looking at them one at a time.\nThis view of events is how the KStream works—it’s a stream of records.\n Now, let’s see how this concept ties into database tables. Look at the simple stock\nquote table in figure 5.2.\nNOTE\nTo keep our discussion straightforward, we’ll assume a key to be a sin-\ngular value.\n \n\n\n119\nThe relationship between streams and tables\nNext, let’s take another look at the record stream. Because each record stands on its\nown, the stream represents inserts into a table. Figure 5.3 combines the two concepts\nto illustrate this point.\n What’s important here is that you can view a stream of events in the same light as\ninserts into a table, which can help give you a deeper understanding of using streams\nfor working with events. The next step is to consider the case where events in the\nstream are related to one another. \n5.1.2\nUpdates to records or the changelog\nLet’s take the same stream of customer transactions, but now track activity over time.\nIf you add a key of customer ID, the purchase events can be related to each other, and\nyou’ll have an update stream as opposed to an event stream.\n If you consider the stream of events as a log, you can consider this stream of\nupdates as a changelog. Figure 5.4 demonstrates this concept.\nCompany AAVF\nAmount $100.57\nTS 12:14:35 1/20/17\nCompany APPL\nAmount $203.77\nTS 12:15:57 1/20/17\nCompany FRLS\nAmount $40.27\nTS 12:18:41 1/20/17\nCompany AMEX\nAmount $57.17\nTS 12:20:38 1/20/17\nImagine that you are observing a stock ticker displaying updated share prices in real time.\nEach circle on the line represents a publicly traded stock’s share price adjusting to\nmarket forces.\nTime\nFigure 5.1\nA marble diagram for an unbounded stream of stock quotes\nThe rows from table above can be recast as key/value pairs.\nFor example, the ﬁrst row in the table can be converted\nto this key/value pair:\n{key:{stockid: 235588}, value:{ts:32225544289, price: 05.36}}\n1\n1\nABVF\n105.36\n32225544289\nAPPL\n333.66\n32225544254\nStock_ID\nShare_Price\nTimestamp\nKey\nValue\nFigure 5.2\nA simple database \ntable represents stock prices for \ncompanies. There’s a key column, \nand the other columns contain \nvalues. You can consider this a \nkey/value pair if you lump the \nother columns into a “value” \ncontainer.\n \n\n\n120\nCHAPTER 5\nThe KTable API\nStock_ID AMEX\nShare $105.36\nTS 148907726274\nStock_ID RLPX\nShare $203.77\nTS 148907726589\nStock_ID AMEX\nAmount $107.05\nTS 1489077288531\nStock_ID RLPX\nAmount $201.57\nTS 1148907736628\nThis shows the relationship between events and inserts into a database. Even though it’s\nstock prices for two companies, it counts as four events because we’re considering\neach item on the stream as a singular event.\nAs a result, each event is an insert, and we increment the key by one for each insert into the table.\nWith that in mind, each event is a new, independent record or insert into a database table.\nAMEX\n105.36\n148907726274\nStock_ID\nShare_Price\nTimestamp\nRLPX\n203.77\n148907726589\nAMEX\n107.05\n1489077288531\nRLPX\n201.57\n148907736628\n1\nKey\n2\n3\n4\nFigure 5.3\nA stream of individual events compares to inserts into a database table. You could \nsimilarly imagine streaming each row from the table.\nThe previous records\nfor these stocks have\nbeen overwritten with\nupdates.\nLatest records from\nevent stream\nIf you use the stock ID as a primary key, subsequent events with the same key are updates\nin a changelog. In this case, you only have two records, one per company. Although more\nrecords can arrive for the same companies, the records won’t accumulate.\nAMEX\n105.36\n148907726274\nStock_ID\nShare_Price\nTimestamp\nRLPX\n203.77\n148907726589\nAMEX\n107.05\n1489077288531\nRLPX\n201.57\n148907736628\nStock_ID AMEX\nShare $105.36\nTS 148907726274\nStock_ID RLPX\nShare $203.77\nTS 148907726589\nStock_ID AMEX\nAmount $107.05\nTS 1489077288531\nStock_ID RLPX\nAmount $201.57\nTS 1148907736628\nFigure 5.4\nIn a changelog, each incoming record overwrites the previous one with the same key. \nWith a record stream, you’d have a total of four events, but in the case of updates or a changelog, you \nhave only two.\n \n\n\n121\nThe relationship between streams and tables\nHere, you can see the relationship between a stream of updates and a database table.\nBoth a log and a changelog represent incoming records appended to the end of a file.\nIn a log, you see all the records; but in a changelog, you only keep the latest record for\nany given key.\nNOTE\nWith both a log and a changelog, records are appended to the end of\nthe file as they come in. The distinction between the two is that in a log, you\nwant to see all records, but in a changelog, you only want the latest record for\neach key.\nTo trim a log while maintaining the latest records per key, you can use log compac-\ntion, which we discussed in chapter 2. You can see the impact of compacting a log in\nfigure 5.5. Because you only care about the latest values, you can remove older\nkey/value pairs.1\nYou’re already familiar with event streams from working with KStreams. For a chan-\ngelog or stream of updates, we’ll use an abstraction known as the KTable. Now that\nwe’ve established the relationship between streams and tables, the next step is to com-\npare an event stream to an update stream. \n1 This section derived information from Jay Kreps’s “Introducing Kafka Streams: Stream Processing Made Sim-\nple” (http://mng.bz/49HO) and “The Log: What Every Software Engineer Should Know About Real-time\nData’s Unifying Abstraction” (http://mng.bz/eE3w).\nBefore compaction\nAfter compaction\nOffset\nValue\nKey\nOffset\nValue\nKey\n10\nA\nfoo\n11\nB\nbar\n12\nC\nbaz\n13\nD\nfoo\n13\nD\nfoo\n14\nE\nbaz\n15\nF\nboo\n16\nG\nfoo\n17\nH\nbaz\n11\nB\nbar\n15\nF\nboo\n16\nG\nfoo\n17\nH\nbaz\nFigure 5.5\nOn the left is a log before compaction—you’ll notice duplicate keys with different \nvalues, which are updates. On the right is the log after compaction—you keep the latest value for \neach key, but the log is smaller in size.\n \n\n\n122\nCHAPTER 5\nThe KTable API\n5.1.3\nEvent streams vs. update streams\nWe’ll use the KStream and the KTable to drive our comparison of event streams versus\nupdate streams. We’ll do this by running a simple stock ticker application that writes\nthe current share price for three (fictitious!) companies. It will produce three itera-\ntions of stock quotes for a total of nine records. A KStream and a KTable will read the\nrecords and write them to the console via the print() method.\n Figure 5.6 shows the results of running the application. As you can see, the\nKStream printed all nine records. We’d expect the KStream to behave this way because\nit views each record individually. In contrast, the KTable printed only three records,\nbecause the KTable views records as updates to previous ones.\nNOTE\nFigure 5.6 demonstrates how a KTable works with updates. I made an\nimplicit assumption that I’ll make explicit here. When working with a KTable,\nyour records must have populated keys in the key/value pairs. Having a key is\nessential for the KTable to work, just as you can’t update a record in a data-\nbase table without having the key.\nFrom the KTable’s point of view, it didn’t receive nine individual records. The KTable\nreceived three original records and two rounds of updates, and it only printed the last\nHere are all three\nevents/records\nfor the KStream.\nHere is the last update\nrecord for the KTable.\nAs expected, the values for the last KStream\nevent and KTable update are the same.\nA simple stock ticker for three ﬁctitious companies with a data generator producing\nthree updates for the stocks. The KStream printed all records as they were received.\nThe KTable only printed the last batch of records because they were the latest\nupdates for the given stock symbol.\nFigure 5.6\nKTable versus KStream printing messages with the same keys\n \n\n\n123\nRecord updates and KTable configuration\nround of updates. Notice that the KTable records are the same as the last three\nrecords published by the KStream. We’ll discuss the mechanisms of how the KTable\nemits only the updates in the next section.\n Here’s the program for printing stock ticker results to the console (found in\nsrc/main/java/bbejeck/chapter_5/KStreamVsKTableExample.java; source code can\nbe found on the book’s website here: https://manning.com/books/kafka-streams-in-\naction).\nKTable<String, StockTickerData> stockTickerTable =\n➥ builder.table(STOCK_TICKER_TABLE_TOPIC);  \nKStream<String, StockTickerData> stockTickerStream =\n➥ builder.stream(STOCK_TICKER_STREAM_TOPIC);   \nstockTickerTable.toStream()\n➥ .print(Printed.<String, StockTickerData>toSysOut()\n➥ .withLabel(\"Stocks-KTable\"));                        \nstockTickerStream\n➥ .print(Printed.<String, StockTickerData>toSysOut()\n➥ .withLabel(\"Stocks-KStream\"));                       \nThe takeaway here is that records in a stream with the same keys are updates, not new\nrecords in themselves. A stream of updates is the main concept behind the KTable.\n You’ve now seen the KTable in action, so let’s discuss the mechanisms behind its\nfunctionality. \n5.2\nRecord updates and KTable configuration\nTo figure out how the KTable functions, we should ask the following two questions:\nWhere are records stored?\nHow does a KTable make the determination to emit records?\nListing 5.1\nKTable and KStream printing to the console\nUsing default serdes\nIn creating the KTable and KStream, you didn’t specify any serdes to use. The same\nis true with both calls to the print() method. You were able to do this because you\nregistered a default serdes in the configuration. like so:\nprops.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,\n➥ Serdes.String().getClass().getName());\nprops.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,\n➥ StreamsSerdes.StockTickerSerde().getClass().getName());\nIf you used different types, you’d need to provide serdes in the overloaded methods\nfor reading or writing records.\nCreates the \nKTable instance\nCreates the \nKStream instance\nKTable prints results \nto the console\nKStream prints results \nto the console\n \n\n\n124\nCHAPTER 5\nThe KTable API\nAs we get into aggregation and reducing operations, the answers to these questions are\nnecessary. For example, when performing an aggregation, you’ll want to see updated\ncounts, but you probably won’t want an update each time the count increments by one.\n To answer the first question, let’s look at the line that creates the KTable:\nbuilder.table(STOCK_TICKER_TABLE_TOPIC);\nWith this simple statement, the StreamsBuilder creates a KTable instance and simul-\ntaneously, under the covers, creates a StateStore for tracking the state of stream, thus\ncreating an update stream. The StateStore created by this approach has an internal\nname and won’t be available for interactive queries.\n There’s an overloaded version of StreamsBuilder.table that accepts a Material-\nized instance, allowing you to customize the type of store and provide a name to\nmake it available for querying. We’ll discuss interactive queries later in this chapter.\n That gives us the answer to our first question: the KTable uses the local state inte-\ngrated with Kafka Streams for storage. (We covered state stores in section 4.3.)\n Now let’s move on to the next question: what determines when the KTable emits\nupdates to downstream processors? To answer this question, we need to consider a few\nfactors:\nThe number of records flowing into the application. Higher data rates will tend\nto increase the rate of emitting updated records.\nHow many distinct keys are in the data. Again, a greater number of distinct keys\nmay lead to more updates being sent downstream.\nThe configuration parameters cache.max.bytes.buffering and commit\n.interval.ms.\nFrom this list, we’ll only cover what we can control: configuration parameters. First,\nlet’s look at the cache.max.bytes.buffering configuration.\n5.2.1\nSetting cache buffering size\nThe KTable cache serves to deduplicate updates to records with the same key. This\ndeduplication allows child nodes to receive only the most recent update instead of all\nupdates, reducing the amount of data processed. Additionally, only the most recent\nupdate is placed in the state store, which can amount to significant performance\nimprovements when using persistent state stores.\n Figure 5.7 illustrates the cache operation. As you can see, with caching enabled,\nnot all the record updates get forwarded downstream. The cache keeps only the latest\nrecord for any given key.\nNOTE\nA Kafka Streams application is a topology or graph of connected nodes\n(processors). Any given node may have one or more child nodes, unless it’s a\nterminal processor. Once a processor has finished working with a record, it\nforwards the record “downstream” to its child nodes.\n \n",
      "page_number": 133
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 143-154)",
      "start_page": 143,
      "end_page": 154,
      "detection_method": "topic_boundary",
      "content": "125\nRecord updates and KTable configuration\nBecause the KTable represents a changelog of events in a stream, you’ll expect to\nwork with only the latest update at any given point. Using the cache enforces this\nbehavior. If you wanted to process all records in the stream, you’d use an event stream,\nthe KStream, covered earlier.\n A larger cache will reduce the number of updates emitted. Additionally, caching\nreduces the amount of data written to disk by persistent stores (RocksDB), and if log-\nging is enabled, the number of records sent to the changelog topic for any store.\n Cache size is controlled by the cache.max.bytes.buffering setting, which speci-\nfies the amount of memory allocated for the record cache. The amount of memory\nspecified is divided evenly across the number of stream threads. (The number of\nstream threads is specified by the StreamsConfig.NUM_STREAM_THREADS_CONFIG set-\nting, with 1 being the default.)\nWARNING\nTo turn off caching, you can set cache.max.bytes.buffering to 0.\nBut this setting will result in every KTable update being sent downstream,\neffectively turning your changelog stream into an event stream. Also, no cach-\ning means more I/O, as persistent stores will now write each update to disk\ninstead of only writing the latest update. \n5.2.2\nSetting the commit interval\nThe other setting is the commit.interval.ms parameter. The commit interval speci-\nfies how often (in milliseconds) the state of a processor should be saved. When the\nstate of the processor is saved (committing), it forces a cache flush, sending the latest\nupdated, deduplicated records downstream.\n In the full caching workflow (figure 5.8), you can see two forces at play when it\ncomes to sending records downstream. Either a commit or the cache reaching its\nmaximum size will send records downstream. Conversely, disabling the cache will send\nall records downstream, including duplicate keys. Generally speaking, it’s best to have\ncaching enabled when using a KTable.\n33.56\nNDLE\n105.36\nYERB\n105.24\nYERB\n105.36\nYERB\nIncoming stock ticker record\nCache\nAs records come in, they are also\nplaced in the cache, with new records\nreplacing older ones.\nFigure 5.7\nKTable caching deduplicates updates \nto records with the same key, preventing a flood of \nconsecutive updates to child nodes of the KTable \nin the topology.\n \n\n\n126\nCHAPTER 5\nThe KTable API\nAs you can see, we need to strike a balance between cache size and commit time. A\nlarge cache with a small commit time will still result in frequent updates. A longer\ncommit interval could lead to fewer updates (depending on the memory settings)\nbecause cache evictions occur to free-up space. There are no hard rules here—only\ntrial and error will determine what works best for you. It’s best to start with the default\nvalues of 30 seconds (commit time) and 10 MB (cache size). The key thing to remem-\nber is that the rate of updated records sent from a KTable is configurable.\n Next, let’s take a look at how you can use the KTable in your applications. \n5.3\nAggregations and windowing operations\nIn this section, we’ll move on to cover some of the most potent parts of Kafka Streams.\nSo far, we’ve looked at several aspects of Kafka Streams:\nHow to set up a processing topology\nHow to use state in your streaming application\nHow to perform joins between streams\nThe difference between an event stream (KStream) and an update stream\n(KTable)\nBoth records with the key of\nYERB were stored ﬁrst and\nthen forwarded.\nWith caching disabled (setting cache.max.bytes.buffering= 0),\nincoming updates are immediately written to the state store\nand sent downstream.\nDue to caching, this update was never\nstored or sent downstream; it was\nautomatically deduped by the cache.\nEither when the commit time or the max-cache size is reached,\nrecords are ﬂushed from the cache and then written to state\nstore and forwarded downstream. Notice all records are not\nstored or forwarded due to deduping by the cache.\nIncoming stock ticker record\n105.36\nYERB\nCaching enabled?\nYes\n105.36\nYERB\n33.56\nNDLE\n105.36\nYERB\n33.56\nNDLE\n105.36\nYERB\n105.24\nYERB\n33.56\nNDLE\n105.36\nYERB\n105.24\nYERB\nNo\n33.56\nNDLE\n105.36\nYERB\nState store\nFigure 5.8\nFull caching workflow: if caching is enabled, records are deduped and sent downstream \non cache flush or commit.\n \n\n\n127\nAggregations and windowing operations\nIn the examples that follow, we’ll tie all these elements together. Additionally, I’ll\nintroduce windowing, another powerful tool in streaming applications. The first exam-\nple is a straightforward aggregation.\n5.3.1\nAggregating share volume by industry\nAggregation and grouping are necessary tools when you’re working with streaming\ndata. Reviewing single records as they arrive is often not enough. To gain any insight,\nyou’ll need grouping and combining of some sort.\n In this example, you’ll take on the role of being a day trader, and you’ll track the\nshare volume of companies across a list of selected industries. In particular, you’re\ninterested in the top five companies (by share volume) in each industry.\n To do this aggregation, a few steps will be required to set up the data in the correct\nformat. At a high level, these are the steps:\n1\nCreate a source from a topic publishing raw stock-trade information.\nYou’ll need to\nmap a StockTransaction object into a ShareVolume object. The reason for per-\nforming this mapping step is simple: the StockTransaction object contains\nmetadata about the trade, but you only want the volume of shares involved in\nthe trade.\n2\nGroup ShareVolume by its ticker symbol.\nOnce it’s grouped by symbol, you can\nreduce it to a rolling total of share volume. I should note here that calling\nKStream.groupBy returns a KGroupedStream instance. Then, calling KGrouped-\nStream.reduce is what will get you to a KTable instance.\nLet’s pause for a minute and look at figure 5.9, which shows what you’ve built so far.\nThis topology is something you’re familiar with by now.\n Now, let’s look at the code behind the topology (found in src/main/java/bbejeck/\nchapter_5/AggregationsAndReducingExample.java).\n \n \n \nWhat is the KGroupedStream?\nWhen you use KStream.groupBy or KStream.groupByKey, the returned instance is\na KGroupedStream. The KGroupedStream is an intermediate representation of the\nevent stream after grouping by keys and is never meant for you to work with directly.\nInstead, the KGroupedStream is required to perform aggregation operations, which\nalways result in a KTable. Because the aggregate operations produce a KTable and\nuse a state store, not all updates may end up being forwarded downstream.\nThere’s an analogous KGroupedTable resulting from the KTable.groupBy method,\nwhich is the intermediate representation of the update stream regrouped by key.\n \n\n\n128\nCHAPTER 5\nThe KTable API\nKTable<String, ShareVolume> shareVolume =\n➥ builder.stream(STOCK_TRANSACTIONS_TOPIC,\nConsumed.with(stringSerde, stockTransactionSerde)\n➥ .withOffsetResetPolicy(EARLIEST))\n                           \n➥ .mapValues(st -> ShareVolume.newBuilder(st).build())              \n➥ .groupBy((k, v) -> v.getSymbol(),\nSerialized.with(stringSerde, shareVolumeSerde))  \n➥ .reduce(ShareVolume::reduce);  \nThis code is concise and squeezes a lot of power into a few lines. If you look at the first\nparameter of the builder.stream method, you’ll see something new: the AutoOffset-\nReset.EARLIEST enum (there’s also a LATEST) that you set with the Consumed\n.withOffsetResetPolicy method. This enum allows you to specify the offset-reset\nstrategy for each KStream or KTable. Using the enum takes precedence over the off-\nset-reset setting in the streams configuration.\nListing 5.2\nSource for the map-reduce of stock transactions\nKTable<String, ShareVolume>\nMapValues\nprocessor\nReducing\nprocessor\nstock-\ntransactions\ntopic\nSource\nprocessor\nGroup-by\nprocessor\nReduces ShareVolume objects\ndown to a single ShareVolume\ninstance with a rolling update\nof total share volume. The ﬁnal\nresult of this process is a\nKTable<String, ShareVolume>\ninstance.\nMaps StockTransaction\nobjects to ShareVolume objects\nConsumes from\nstock-transactions topic\nGroups ShareVolume\nobjects by stock\nticker symbol\nFigure 5.9\nMapping and reducing StockTransaction objects into ShareVolume objects and \nthen reducing to a rolling total\nThe source processor\nconsumes from a topic.\nMaps StockTransaction \nobjects to ShareVolume \nobjects\nGroups the\nShareVolume\nobjects by their\nstock ticker symbol\nReduces \nShareVolume \nobjects to contain \na rolling aggregate \nof share volume\n \n\n\n129\nAggregations and windowing operations\nIt’s clear what mapValues and groupBy are doing, but let’s look into the sum() method\n(found in src/main/java/bbejeck/model/ShareVolume.java).\npublic static ShareVolume sum(ShareVolume csv1, ShareVolume csv2) {\nBuilder builder = newBuilder(csv1);                      \nbuilder.shares = csv1.shares + csv2.shares;         \nreturn builder.build();           \n}\nNOTE\nYou’ve seen the builder pattern in use earlier in the book, but it’s used\nhere in a somewhat different context. In this example, you’re using the\nbuilder to make a copy of an object and update a field without modifying the\noriginal object.\nThe ShareVolume.sum method gives you the rolling total of share volume, and the\noutcome of the entire processing chain is a KTable<String, ShareVolume> object.\nNow, you can see the role of the KTable. As ShareVolume objects come in, the associ-\nated KTable keeps the most recent update. It’s important to remember that every\nupdate is reflected in the preceding shareVolumeKTable, but each update is not nec-\nessarily emitted.\nNOTE\nWhy do a reduce instead of an aggregation? Although reducing is a\nform of aggregation, a reduce operation will yield the same type of object. An\naggregation also sums results, but it could return a different type of object.\nNext, you’ll take the KTable and use it to perform a top-five aggregation (by share vol-\nume) summary. The steps you’ll take here are similar to the steps for the first aggregation:\n1\nPerform another groupBy operation to group the individual ShareVolume objects\nby industry.\nGroupByKey vs. GroupBy\nKStream has two methods for grouping records: GroupByKey and GroupBy. Both\nreturn a KGroupedTable, so you might wonder what the difference is and when you\nshould use which one.\nThe GroupByKey method is for when your KStream already has non-null keys. More\nimportantly, the “needs repartitioning” flag is never set.\nThe GroupBy method assumes you’ve modified the key for the grouping, so the repar-\ntition flag is set to true. After calling GroupBy, joins, aggregations, and the like will\nresult in automatic repartitioning.\nThe bottom line is that you should prefer GroupByKey over GroupBy whenever possible.\nListing 5.3\nThe ShareVolume.sum method\nUses a Builder \nfor a copy \nconstructor\nSets the number of\nshares to the total of\nboth ShareVolume\nobjects\nCalls build and returns a\nnew ShareVolume object\n \n\n\n130\nCHAPTER 5\nThe KTable API\n2\nStart to add the ShareVolume objects. This time, the aggregation object is a fixed-\nsize priority queue. The fixed-size queue keeps only the top-five companies by\nshare volume.\n3\nMap the queue into a string, reporting the top-five stocks per industry by share\nvolume.\n4\nWrite out the string result to a topic.\nFigure 5.10 shows a topology graph of the data flow. As you can see, this second round\nof processing is straightforward.\nNow that you have a clear picture of the structure of this second round of processing,\nit’s time to look at the source code (found in src/main/java/bbejeck/chapter_5/\nAggregationsAndReducingExample.java).\nComparator<ShareVolume> comparator =\n➥ (sv1, sv2) -> sv2.getShares() - sv1.getShares()\nFixedSizePriorityQueue<ShareVolume> fixedQueue =\n➥ new FixedSizePriorityQueue<>(comparator, 5);\nshareVolume.groupBy((k, v) -> KeyValue.pair(v.getIndustry(), v),\n➥ Serialized.with(stringSerde, shareVolumeSerde))               \n.aggregate(() -> fixedQueue,\n         \n(k, v, agg) -> agg.add(v),\n       \n(k, v, agg) -> agg.remove(v),\n     \nMaterialized.with(stringSerde, fixedSizePriorityQueueSerde))  \nListing 5.4\nKTable groupBy and aggregation\nAggregate\nprocessor\nGroup-by\nprocessor\nKTable<String, ShareVolume>\nThe grouped records are\naggregated into a Top-N\nqueue—in this case N=5.\nThe queue contents are\nmapped to a string\nin this format.\n) YERB:2 7,934\n1\n1\n2) OCHK: 47,074, and so on\n1\nGroups the reduced\nShareVolume objects\nby industry\nResults are\nwritten out to\na topic.\nMapValues\nprocessor\nTo topic\nFigure 5.10\nTopology for grouping by industry, aggregating by top five, mapping the top-five queue to \na string, and writing out the string to a topic\nGroups by industry and\nprovides required serdes\nThe Aggregate initializer is\nan instance of the\nFixedSizePriorityQueue\nclass (for demonstration\npurposes only!).\nAggregate\nadder adds\nnew updates\nAggregate remover \nremoves old updates\nSerde for the aggregator\n \n\n\n131\nAggregations and windowing operations\n.mapValues(valueMapper)                                    \n.toStream().peek((k, v) ->\n➥ LOG.info(\"Stock volume by industry {} {}\", k, v))         \n.to(\"stock-volume-by-company\", Produced.with(stringSerde,\n➥ stringSerde));      \nIn this initializer, there’s a fixedQueue variable. This is a custom object that wraps a\njava.util.TreeSet, which is used to keep track of the top N results in decreasing\norder of share volume.\n You’ve seen the groupBy and mapValues calls before, so we won’t go over them\nagain (you call the KTable.toStream method, as KTable.print is deprecated). But\nyou haven’t seen the KTable version of aggregate before, so let’s take a minute to dis-\ncuss it.\n As you’ll recall, what makes a KTable unique is that records with the same key are\nupdates. The KTable replaces the old record with the new one. Aggregation works in\nthe same manner. It aggregates the most recent records with the same key. As a record\narrives, you add it to the FixedSizePriorityQueue using the adder method (the\nsecond parameter in the aggregate call), but if another record exists with the same\nkey, you remove the old record with the subtractor (the third parameter in the\naggregate call).\n What this means is that your aggregator, FixedSizePriorityQueue, doesn’t aggre-\ngate all values with the same key. Instead, it keeps a running tally of the top N stocks\nthat have the largest volume. Each record coming in has the total volume of shares\ntraded so far. Your KTable will show you which companies have the top number of\nshares traded at the moment; you don’t want a running aggregation of each update.\n You’ve now learned how to do two important things:\nGroup values in a KTable by a common key\nPerform useful operations like reducing and aggregation with those grouped\nvalues\nThe ability to perform these operations is important when you’re trying to make sense\nof your data, or to determine what your data is telling you, as it flows through your\nKafka Streams application.\n We’ve also brought together some of the key concepts discussed earlier in the\nbook. In chapter 4, you learned about the importance of having fault-tolerant, local\nstate for streaming applications. The first example in this chapter showed why local\nstate is so important—it allows you to keep track of what you’ve seen. Having local\naccess avoids network latency, making your application more robust and performant.\n Any time you execute a reduction or aggregation operation, you provide the name\nof a state store. Reduction and aggregation operations return a KTable instance, and\nthe KTable uses the state store to replace older results with the newer ones. As you’ve\nValueMapper \ninstance converts \naggregator to a \nstring that’s used \nfor reporting\nCalls toStream() to log the\nresults (to the console) via\nthe peak method\nWrites the results to\nthe stock-volume-by-\ncompany topic\n \n\n\n132\nCHAPTER 5\nThe KTable API\nseen, not every update gets forwarded downstream, and that’s important because you\nperform aggregation operations to gather summary information. If you didn’t use\nlocal state, the KTable would forward every aggregation or reduction result.\n Next, we’ll look at how you can perform aggregation-like operations over distinct\nperiods of time, a process called windowing. \n5.3.2\nWindowing operations\nIn the previous section, we looked at a “rolling” reduction and aggregation. The appli-\ncation performed a continuous reduction of share volume and then a top-five aggre-\ngation of shares traded in the stock market.\n Sometimes, you’ll want an ongoing aggregation and reduction of results like this.\nAt other times, you’ll only want to perform operations for a given time range. For\nexample, how many stock transactions have involved a particular company in the last\n10 minutes? How many users have clicked to view a new advertisement in the last 15\nminutes? An application may perform these operations many times, but the results\nmay only be for a defined period or window of time.\nCOUNTING STOCK TRANSACTIONS BY CUSTOMER\nIn the next example, you’ll track stock transactions for a handful of traders. These\ncould be large institutional traders or financially savvy individuals.\n There are two likely reasons for doing this tracking. One reason is that you may\nwant to see where the market leaders are buying and selling. If these big hitters or\nsavvy investors see an opportunity in the market, you’ll follow the same strategy. The\nother reason is that you may want to identify any indications of insider trading. You’ll\nwant to look into the timing of large spikes in trading and correlate them with signifi-\ncant news releases.\n Here are the steps to do this tracking:\n1\nCreate a stream to read from the stock-transactions topic.\n2\nGroup incoming records by the customer ID and stock ticker symbol. The\ngroupBy call returns a KGroupedStream instance.\n3\nUse the KGroupedStream.windowedBy method to return a windowed stream, so\nyou can perform some sort of windowed aggregation. Depending on the win-\ndow type provided, you’ll get either a TimeWindowedKStream or a Session-\nWindowedKStream in return.\n4\nPerform a count for the aggregation operation. The windowing stream deter-\nmines whether the record is included in the count.\n5\nWrite the results to a topic, or print the results to the console during development.\nThe topology for this application is straightforward, but it’s helpful to have a mental\npicture of the structure. Take a look at figure 5.11.\n Next, let’s look at the windowing functionality and corresponding code. \n \n \n\n\n133\nAggregations and windowing operations\nWINDOW TYPES\nIn Kafka Streams, three types of windows are available:\nSession windows\nTumbling windows\nSliding/hopping windows\nWhich type you choose depends on your business requirements. The tumbling and\nhopping windows are time bound, whereas session windows are more about user activ-\nity; the length of the session(s) is determined solely by how active the user is. A key\npoint to keep in mind for all windows is that they’re based on the timestamps in the\nrecords and not wall-clock time.\n Next, you’ll implement the topology with each of the window types. We’ll only look\nat the full code in the first example. Other than changing the type of window opera-\ntion, everything will remain the same for the other windows. \nSESSION WINDOWS\nSession windows are very different from other windows. Session windows aren’t bound\nstrictly by time as much as by user activity (or the activity of anything you want to\ntrack). You delineate session windows by a period of inactivity.\n Figure 5.12 shows how you can view session windows. The smaller session will be\nmerged with the session on the left. But the session on the right will be a new ses-\nsion because it follows a large inactivity gap. Session windows are based on user\nstock-\ntransactions\ntopic\nSource\nprocessor\nSink/Print\nprocessor\nGroup-by\nprocessor\nCount\nprocessor\nGroups StockTransactions by\ncustomer ID and stock ticker\nsymbol. These are encapsulated in\na TransactionSummary object.\nCounts number of transactions by\nTransactionSummary (customer ID and stock\nsymbol). You’ll use a windowed approach to\ncontain the counts. The window could be\na tumbling, hopping, or session window.\nConsumes from\nstock-transactions topic\nYou’ll write out the results to\na topic (or print to the console\nduring development).\nThe ﬁnal object returned from the count processor is a\nKTable<Windowed<TransactionSummary>, Long>.\nFigure 5.11\nCounting windows topology\n \n\n\n134\nCHAPTER 5\nThe KTable API\nactivity, but they use timestamps in the records to determine which session a record\nbelongs to. \nUSING SESSION WINDOWS TO TRACK STOCK TRANSACTIONS\nLet’s use session windows to capture the stock transactions. The following code (found\nin src/main/java/bbejeck/chapter_5/CountingWindowingAndKTableJoinExample\n.java) shows how to implement the session windows.\nSerde<String> stringSerde = Serdes.String();\nSerde<StockTransaction> transactionSerde =\n➥ StreamsSerdes.StockTransactionSerde();\nSerde<TransactionSummary> transactionKeySerde =\n➥ StreamsSerdes.TransactionSummarySerde();\nlong twentySeconds = 1000 * 20;\nlong fifteenMinutes = 1000 * 60 * 15;\nKTable<Windowed<TransactionSummary>, Long>\n➥ customerTransactionCounts =\n             \n➥ builder.stream(STOCK_TRANSACTIONS_TOPIC, Consumed.with(stringSerde,\n➥ transactionSerde)\n.withOffsetResetPolicy(LATEST))           \n.groupBy((noKey, transaction) ->\n➥ TransactionSummary.from(transaction),                    \n➥ Serialized.with(transactionKeySerde, transactionSerde))\n.windowedBy(SessionWindows.with(twentySeconds).\n➥ until(fifteenMinutes)).count();        \nListing 5.5\nTracking stock transactions with session windows\n350 600\nInactivity gaps\nThis inactivity gap is large, so new events\ngo into a separate session.\nThere’s a small inactivity gap here, so\nthese sessions would probably be merged\ninto one larger session.\nSession windows are different because they aren’t strictly bound by time but\nrepresent periods of activity. Speciﬁed inactivity gaps demarcate the sessions.\n100 200 500 400\n100 200 500 400\n100 200 500 400\n350 600\nFigure 5.12\nSession windows separated by a small inactivity gap are combined to form \na new, larger session.\nKTable resulting \nfrom the groupBy \nand count calls\nCreates the stream from the\nSTOCK_TRANSACTIONS_TOPIC\n(a String constant). Uses the\noffset-reset-strategy enum of\nLATEST for this stream.\nGroups records by \ncustomer ID and stock \nsymbol, which are stored in \nthe TransactionSummary \nobject.\nWindows the groups with SessionWindow with an\ninactivity time of 20 seconds and a retention time of 15\nminutes. Then performs an aggregation as a count.\n \n\n\n135\nAggregations and windowing operations\ncustomerTransactionCounts.toStream()\n➥ .print(Printed.<Windowed<TransactionSummary>, Long>toSysOut()\n➥ .withLabel(\"Customer Transactions Counts\"));       \nYou’ve seen most of the operations specified in this topology before, so we don’t need\nto cover them again. But there are a couple of new items, and we’ll discuss those now.\n Any time you do a groupBy operation, you’ll typically perform some sort of aggre-\ngation operation (aggregate, reduce, or count). You can perform a cumulative aggre-\ngation where previous results continue to build up, or you can perform windowed\naggregations where records are combined for the specified time of the window.\n The code in listing 5.5 does a count over session windows. Figure 5.13 breaks\nit down.\nWith the call to windowedBy(SessionWindows.with(twentySeconds).until(fifteen-\nMinutes)), you create a session window with an inactivity gap of 20 seconds and a\nretention period of 15 minutes. An inactivity time of 20 seconds means the applica-\ntion includes any record arriving within 20 seconds of the current session’s ending or\nstart time within the current (active) session.\n You then specify the aggregation operation to perform count, in this case, on the\nsession window. If an incoming record falls outside the inactivity gap (on either side of\nthe timestamp), the application creates a new session. The retention period maintains\nthe session for the specified amount of time and allows for late-arriving data that’s out-\nside the inactivity period of a session but can still be merged. Additionally, as sessions\nare combined, the newly created session uses the earliest timestamp and latest time-\nstamp for the start and end of the new session, respectively.\n Let’s walk through a few records from the count method to see sessions in action:\nsee table 5.1.\nTable 5.1\nSessioning table with a 20-second inactivity gap\nArrival order\nKey\nTimestamp\n1\n{123-345-654,FFBE}\n00:00:00\n2\n{123-345-654,FFBE}\n00:00:15\nConverts the KTable output to a KStream\nand prints the result to the console\nThe with call creates\nthe inactivity gap of\n20 seconds.\nThe until method creates\nthe retention period—\n5 minutes, in this case.\n1\nSessionWindows.with(twentySeconds).until(ﬁfteenMinutes)\nFigure 5.13\nCreating session windows with inactivity periods and retention\n \n\n\n136\nCHAPTER 5\nThe KTable API\nAs records come in, you look for existing sessions with the same key, with ending times\nless than current timestamp – inactivity gap, and with starting times greater than\ncurrent timestamp + inactivity gap. With that in mind, here’s how the four records\nin table 5.15.1 end up being merged into a single session:\n1\nRecord 1 is first, so start and end are 00:00:00.\n2\nRecord 2 arrives, and you look for sessions with an earliest ending of 23:59:55\nand a latest start of 00:00:35. You find record 1, so you merge sessions 1 and 2.\nYou keep the session 1 start time (earliest) and the session 2 end time (latest),\nso you have one session starting at 00:00:00 and ending at 00:00:15.\n3\nRecord 3 arrives, and you look for sessions between 00:00:30 and 00:01:10 and\nfind none. You add a second session for key 123-345-654, FFBE starting and\nending at 00:00:50.\n4\nRecord 4 arrives, and you search for sessions between 23:59:45 and 00:00:25.\nThis time you find both sessions 1 and 2. All three are merged into one session\nwith a start time of 00:00:00 and an end time of 00:00:15.\nThere are a couple of key points to remember from this section:\nSessions are not a fixed-size window. Rather, the size of a session is driven by the\namount of activity within a given time frame.\nTimestamps in the data determine whether an event fits into an existing session\nor falls into an inactivity gap.\nNow, we’ll move on to the next windowing option, tumbling windows. \nTUMBLING WINDOWS\nFixed or tumbling windows capture events within a given period. Imagine you want to\ncapture all stock transactions for a company every 20 seconds, so you collect every\nevent for that time. After the 20-second period is over, your window will “tumble” to a\nnew 20-second observation period. Figure 5.14 shows this situation.\n As you can see, each event that has come in for the last 20 seconds is included in\nthe window. A new window is created after the specified time.\n Here’s how you could use tumbling windows to capture stock transactions every 20\nseconds (found in src/main/java/bbejeck/chapter_5/CountingWindowingAndKtable-\nJoinExample.java).\n \n \n \n3\n{123-345-654,FFBE}\n00:00:50\n4\n{123-345-654,FFBE}\n00:00:05\nTable 5.1\nSessioning table with a 20-second inactivity gap (continued)\nArrival order\nKey\nTimestamp\n \n",
      "page_number": 143
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 155-162)",
      "start_page": 155,
      "end_page": 162,
      "detection_method": "topic_boundary",
      "content": "137\nAggregations and windowing operations\nKTable<Windowed<TransactionSummary>, Long> customerTransactionCounts =\n➥ builder.stream(STOCK_TRANSACTIONS_TOPIC, Consumed.with(stringSerde,\ntransactionSerde)\n➥ .withOffsetResetPolicy(LATEST))\n.groupBy((noKey, transaction) -> TransactionSummary.from(transaction),\n➥ Serialized.with(transactionKeySerde, transactionSerde))\n.windowedBy(TimeWindows.of(twentySeconds)).count();        \nWith this minor change of the TimeWindows.of call, you can use a tumbling window.\nThis example doesn’t include the until() method. By not specifying the duration of\nthe window, you’ll get the default retention of 24 hours.\n Finally, we’ll move on to the last of the windowing options: hopping windows. \nSLIDING OR HOPPING WINDOWS\nSliding or hopping windows are like tumbling windows but with a small difference. A\nsliding window doesn’t wait the entire time before launching another window to pro-\ncess recent events. Sliding windows perform a new calculation after waiting for an\ninterval smaller than the duration of the entire window.\n To illustrate how hopping windows differ from tumbling windows, let’s recast the\nstock transaction count example. You still want to count the number of transactions,\nbut you don’t want to wait the entire period before you update the count. Instead,\nyou’ll update the count at smaller intervals. For example, you’ll still count the number\nof transactions every 20 seconds, but you’ll update the count every 5 seconds, as\nshown in figure 5.15. You now have three result windows with overlapping data.\nListing 5.6\nUsing tumbling windows to count user transactions\n100 200 500 400\n350 600 50 2500\nThe box on the left is the ﬁrst 20-second window. After 20 seconds, it “tumbles”\nover or updates to capture events in a new 20-second period.\nThere is no overlapping of events. The ﬁrst event window contains [ 00, 200, 500, 400]\n1\nand the second event window contains [350, 600, 50, 2500].\nNext 20-second period\nInitial 20-second period\nThe current time period “tumbles” (represented by the dashed box)\ninto the next time period completely with no overlap.\nFigure 5.14\nTumbling windows reset after a fixed period.\nSpecifies a tumbling window of 20 seconds\n \n\n\n138\nCHAPTER 5\nThe KTable API\nHere’s how to specify hopping windows with code (found in src/main/java/bbejeck/\nchapter_5/CountingWindowingAndKtableJoinExample.java).\nKTable<Windowed<TransactionSummary>, Long> customerTransactionCounts =\n➥ builder.stream(STOCK_TRANSACTIONS_TOPIC, Consumed.with(stringSerde,\n➥ transactionSerde)\n➥ .withOffsetResetPolicy(LATEST))\n.groupBy((noKey, transaction) -> TransactionSummary.from(transaction),\n➥ Serialized.with(transactionKeySerde, transactionSerde))\n.windowedBy(TimeWindows.of(twentySeconds)\n➥ .advanceBy(fiveSeconds).until(fifteenMinutes)).count();  \nWith the addition of the advanceBy() method, you can convert a tumbling window to\na hopping window. This example specifies a retention time of 15 minutes.\nNOTE\nYou’ll notice in all the windowing examples presented here that the\nonly code that changes is the windowedBy call. Instead of having four nearly\nidentical example classes in the sample code, I’ve included four different win-\ndowing lines in the src/main/java/bbejeck/chapter_5/CountingWindowing-\nAndKtableJoinExample.java file. To see a different windowing operation in\naction, comment out the current windowing operation and uncomment the\none you want to execute.\nYou’ve now seen how to put your aggregation results into time windows. In particular,\nI want you to remember three things from this section:\nSession windows aren’t fixed by time but are driven by user activity.\nTumbling windows give you a set picture of events within the specified time frame.\nHopping windows are of fixed length, but they’re frequently updated and can\ncontain overlapping records in each window.\nListing 5.7\nSpecifying hopping windows to count transactions\n100 200 500 400\n350 600 50 2500\nThe box on the left is the ﬁrst 20-second window, but the window “slides” over\nor updates after 5 seconds to start a new window. Now you see an overlapping\nof events. Window\ncontains [ 00, 200, 500, 400], window 2 contains [500,\n1\n1\n400, 350, 600], and window 3 is [350, 600, 50, 2500].\nFigure 5.15\nSliding windows update frequently and may contain \noverlapping data.\nUses a hopping \nwindow of 20 \nseconds, advancing \nevery 5 seconds\n \n\n\n139\nAggregations and windowing operations\nNext, we’ll look at how to convert a KTable back into a KStream to perform a join. \n5.3.3\nJoining KStreams and KTables\nIn chapter 4, we discussed joining two KStreams. Now, you’re going to join a KTable\nand a KStream. The reason to do this is simple. KStreams are record streams, and\nKTables are streams of record updates, but sometimes you may need to add some\nadditional context to your record stream with the updates from a KTable.\n Let’s take the stock transaction counts and join them with financial news from rel-\nevant industry sectors. Here are the steps to make this happen with the existing code:\n1\nConvert the KTable of stock transaction counts into a KStream where you\nchange the key to the industry of the count by ticker symbol.\n2\nCreate a KTable that reads from a topic of financial news. The new KTable will\nbe categorized by industry.\n3\nJoin the news updates with the stock transaction counts by industry.\nWith these steps laid out, let’s walk through how you can accomplish these tasks.\nCONVERTING THE KTABLE TO A KSTREAM\nTo do the KTable-to-KStream conversion, you’ll take the following steps:\n1\nCall the KTable.toStream() method.\n2\nUse the KStream.map call to change the key to the industry name, and extract\nthe TransactionSummary object from the Windowed instance.\nThese steps are chained together in the following manner (found in src/main/java/\nbbejeck/chapter_5/CountingWindowingAndKtableJoinExample.java).\nKStream<String, TransactionSummary> countStream =\n➥ customerTransactionCounts.toStream().map((window, count) -> {   \nTransactionSummary transactionSummary = window.key();\n     \nString newKey = transactionSummary.getIndustry();\n         \ntransactionSummary.setSummaryCount(count);\n   \nreturn KeyValue.pair(newKey, transactionSummary);\n  \n});\nBecause you’re performing a KStream.map operation, repartitioning for the returned\nKStream instance is done automatically when it’s used in a join.\n Now that you have the conversion process completed, the next step is to create the\nKTable to read the financial news. \nListing 5.8\nConverting a KTable to a KStream\nCalls toStream, immediately\nfollowed by the map call\nExtracts the TransactionSummary \nobject from the Windowed instance\nSets the key to the industry \nsegment of the stock purchase\nTakes the count value \nfrom the aggregation \nand places it in the \nTransactionSummary \nobject\nReturns the new\nKeyValue pair for\nthe KStream\n \n\n\n140\nCHAPTER 5\nThe KTable API\nCREATING THE FINANCIAL NEWS KTABLE\nFortunately, creating the KTable involves just one line of code (found in src/main/\njava/bbejeck/chapter_5/CountingWindowingAndKtableJoinExample.java).\nKTable<String, String> financialNews =\n➥ builder.table( \"financial-news\", Consumed.with(EARLIEST));    \nIt’s worth noting here that you don’t need to provide any serdes because the configu-\nration is using string serdes. Also, by using the enum EARLIEST, you populate the table\nwith records on startup.\n Now, we’ll move on to the last step, setting up the join. \nJOINING NEWS UPDATES WITH TRANSACTION COUNTS\nSetting up the join is very simple. You’ll use a left join, in case there’s no financial\nnews for the industry involved in the transaction (found in src/main/java/bbe-\njeck/chapter_5/CountingWindowingAndKtableJoinExample.java).\nValueJoiner<TransactionSummary, String, String> valueJoiner =\n➥ (txnct, news) ->\n➥ String.format(\"%d shares purchased %s related news [%s]\",\n➥ txnct.getSummaryCount(), txnct.getStockTicker(), news);     \nKStream<String,String> joined =\n➥ countStream.leftJoin(financialNews, valueJoiner,\n➥ Joined.with(stringSerde, transactionKeySerde, stringSerde));   \njoined.print(Printed.<String, String>toSysOut()\n➥ .withLabel(\"Transactions and News\"));      \nThe leftJoin statement is straightforward. Unlike the joins in chapter 4, you don’t\nprovide a JoinWindow because, when performing a KStream-to-KTable join, there’s\nonly one record per key in the KTable. The join is unrelated to time; the record is\neither present in the KTable or not. The key point here is that you can use KTables to\nprovide less-frequently updated lookup data to enrich your KStream counterparts.\n Next, we’ll look at a more efficient way to enhance KStream events. \n5.3.4\nGlobalKTables\nWe’ve established the need to enrich or add context to our event streams. You’ve also\nseen joins between two KStreams in chapter 4, and the previous section demonstrated\nListing 5.9\nKTable for financial news\nListing 5.10\nSetting up the join between the KStream and KTable\nCreates the KTable with the EARLIEST\nenum, topic financial-news\nValueJoiner \ncombines the \nvalues from the \njoin result.\nThe leftJoin statement for the\ncountStream KStream and the\nfinancial news KTable, providing\nserdes with a Joined instance\nPrints results to the console (in \nproduction this would be written to a \ntopic with a to(\"topic-name\") call)\n \n\n\n141\nAggregations and windowing operations\na join between a KStream and a KTable. In all of these cases, when you map the keys to\na new type or value, the stream needs to be repartitioned. Sometimes you’ll do the\nrepartitioning explicitly yourself, and other times Kafka Streams will do it automati-\ncally. Repartitioning makes sense, because the keys have been changed and will end\nup on new partitions or the join won’t happen (this was discussed in the chapter 4 sec-\ntion, “Repartitioning the data”).\nREPARTITIONING HAS A COST\nRepartitioning isn’t free. There’s additional overhead in this process: creating inter-\nmediate topics, storing duplicate data in another topic, and increased latency due to\nwriting to and reading from another topic. Additionally, if you want to join on more\nthan one facet or dimension, you’ll need to chain joins, map the records with new\nkeys, and repeat the repartitioning process. \nJOINING WITH SMALLER DATASETS\nIn some cases, the lookup data you want to join against will be relatively small, and\nentire copies of the lookup data could fit locally on each node. For situations where\nthe lookup data is reasonably small, Kafka Streams provides the GlobalKTable.\n GlobalKTables are unique because the application replicates all the data to each\nnode. Because the entirety of the data is on each node, the event stream doesn’t need\nto be partitioned by the key of the lookup data in order to make it available to all par-\ntitions. GlobalKTables also allow you to do non-key joins. Let’s revisit one of the previ-\nous examples to demonstrate this capability. \nJOINING KSTREAMS WITH GLOBALKTABLES\nIn section 5.3.2, you performed a windowed aggregation of stock transactions per cus-\ntomer. The output of the aggregation looked like this:\n{customerId='074-09-3705', stockTicker='GUTM'}, 17\n{customerId='037-34-5184', stockTicker='CORK'}, 16\nAlthough this output accomplished the goal, it would have more impact if you could\nsee the client’s name and the full company name. You could perform regular joins to\nfill out the customer and company names, but you’d need to do two key mappings\nand repartitioning. With GlobalKTables, you can avoid the expense of those opera-\ntions. To accomplish this, you’ll use the countStream from the following listing\n(found in src/main/java/bbejeck/chapter_5/GlobalKTableExample.java) and join it\nagainst two GlobalKTables.\nKStream<String, TransactionSummary> countStream =\nbuilder.stream( STOCK_TRANSACTIONS_TOPIC,\n➥ Consumed.with(stringSerde, transactionSerde)\n➥ .withOffsetResetPolicy(LATEST))\n.groupBy((noKey, transaction) ->\n➥ TransactionSummary.from(transaction),\nListing 5.11\nAggregating stock transactions using session windows\n \n\n\n142\nCHAPTER 5\nThe KTable API\n➥ Serialized.with(transactionSummarySerde, transactionSerde))\n.windowedBy(SessionWindows.with(twentySeconds)).count()\n.toStream().map(transactionMapper);\nWe covered this previously, so we won’t review it again here. But note that the code in\nthe toStream().map function is abstracted into a function object instead of having an\nin-line lambda, for readability purposes.\n The next step is to define two GlobalKTable instances (found in src/main/java/\nbbejeck/chapter_5/GlobalKTableExample.java).\nGlobalKTable<String, String> publicCompanies =\n➥ builder.globalTable(COMPANIES.topicName());  \nGlobalKTable<String, String> clients =\n➥ builder.globalTable(CLIENTS.topicName());      \nNote that the topic names are defined using enums.\n Now that the components are in place, you need to construct the join (found in\nsrc/main/java/bbejeck/chapter_5/GlobalKTableExample.java).\ncountStream.leftJoin(publicCompanies, (key, txn) ->\n➥ txn.getStockTicker(),TransactionSummary::withCompanyName)   \n.leftJoin(clients, (key, txn) ->\n➥ txn.getCustomerId(), TransactionSummary::withCustomerName)    \n.print(Printed.<String, TransactionSummary>toSysOut()\n➥ .withLabel(\"Resolved Transaction Summaries\"));   \nAlthough there are two joins here, they’re chained together because you don’t use\nany of the results alone. You print the results at the end of the entire operation.\n If you run the join operation, you’ll now get results like this:\n{customer='Barney, Smith' company=\"Exxon\", transactions= 17}\nThe facts haven’t changed, but these results are clearer for reading.\n Including chapter 4, you’ve seen several types of joins in action. They’re listed in\ntable 5.2. This table represents the state of join options as of Kafka Streams 1.0.0, but\nthis may change in future releases.\nListing 5.12\nDefining the GlobalKTables for lookup data\nListing 5.13\nJoining a KStream with two GlobalKTables\nThe publicCompanies lookup is \nfor finding companies by their \nstock ticker symbol.\nThe clients lookup is for getting \ncustomer names by customer ID.\nSets up the leftJoin with the publicCompanies table,\nkeys by stock ticker symbol, and returns the\ntransactionSummary with the company name added\nSets up the leftJoin with the clients table, keys by\ncustomer ID, and returns the transactionSummary\nwith the customer named added\nPrints the results \nout to the console\n \n\n\n143\nAggregations and windowing operations\nIn conclusion, the key thing to remember is that you can combine event streams\n(KStream) and update streams (KTable), using local state. Additionally, when the\nlookup data is of a manageable size, you can use a GlobalKTable. GlobalKTables rep-\nlicate all partitions to each node in the Kafka Streams application, making all data\navailable, regardless of which partition the key maps to.\n Next, we’ll look at a capability of Kafka Streams that allows you to observe changes\nto state without having to consume data from a Kafka topic. \n5.3.5\nQueryable state\nYou’ve performed several operations involving state, and you’ve always printed the\nresults to the console (for development) or written them to a topic (for production).\nWhen you write the results to a topic, you need to use a Kafka consumer to view the\nresults.\n Reading the data from these topics could be considered a form of materialized\nviews. For our purposes, we can use Wikipedia’s definition of a materialized view: “a\ndatabase object that contains the results of a query. For example, it may be a local\ncopy of data located remotely, or may be a subset of the rows and/or columns of a\ntable or join result, or may be a summary using an aggregate function” (https://en\n.wikipedia.org/wiki/Materialized_view).\n Kafka Streams also offers interactive queries from state stores, giving you the ability to\nread these materialized views directly. It’s important to note that querying state stores\nis a read-only operation. By making the queries read-only, you don’t have to worry\nabout creating an inconsistent state while the application continues to process data.\n The impact of making the state stores directly queryable is significant. It means\nyou can create dashboard applications without having to consume the data from a\nKafka consumer first. There are also some gains in efficiency resulting from not writ-\ning the data out again:\nBecause the data is local, you can access it quickly.\nYou avoid duplicating data by not copying it to an external store.2\nTable 5.2\nKafka Streams joins\nLeft join\nInner join\nOuter join\nKStream-KStream\nKStream-KStream\nKStream-KStream\nKStream-KTable\nKStream-KTable\nN/A\nKTable-KTable\nKTable-KTable\nKTable-KTable\nKStream-GlobalKTable\nKStream-GlobaKTable\nN/A\n2 For more details, see Eno Thereska, “Unifying Stream Processing and Interactive Queries in Apache Kafka,”\nhttp://mng.bz/dh1H.\n \n\n\n144\nCHAPTER 5\nThe KTable API\nThe main thing I want you to remember here is that you can query state from the\napplication directly. I can’t stress enough the power this feature gives you. Instead of\nconsuming from Kafka and storing records in a database to feed your application, you\ncan directly query the state stores for the same results. The impact of direct queries on\nstate stores means less code (no consumer) and less software (no need for a database\ntable to store results).\n We’ve covered a lot in this chapter, so we’ll stop here in our discussion of interac-\ntive queries on state stores. But fear not: in chapter 9, you’ll build a simple dashboard\napplication with interactive queries. It will use some of the examples from this and\nprevious chapters to demonstrate interactive queries and how you can add them to\nyour Kafka Streams applications. \nSummary\n\nKStreams represent event streams that are comparable to inserts into a data-\nbase. KTables are update streams and are more akin to updates to a database.\nThe size of a KTable doesn’t continue to grow; older records are replaced with\nnewer records.\n\nKTables are essential for performing aggregation operations.\nYou can place your aggregated data into time buckets with windowing operations.\nGlobalKTables give you lookup data across the entire application, regardless of\nthe partitions.\nYou can perform joins with KStreams, KTables, and GlobalKTables.\nSo far, we’ve focused on the high-level KStreams DSL to build Kafka Streams applica-\ntions. Although the high-level approach gives nice, concise programs, everything is a\ntrade-off. By working with the KStreams DSL, you relinquish a level of control and\ngain more-concise code. In the next chapter, we’ll cover the low-level Processor API\nand make different trade-offs. You won’t have the conciseness of the applications\nyou’ve built so far, but you’ll gain the ability to create virtually any kind of processor\nyou need.\n \n",
      "page_number": 155
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 163-171)",
      "start_page": 163,
      "end_page": 171,
      "detection_method": "topic_boundary",
      "content": "145\nThe Processor API\nUp to this point in the book, we’ve been working with the high-level Kafka Streams\nAPI. It’s a DSL that allows developers to create robust applications with minimal\ncode. The ability to quickly put together processing topologies is an important fea-\nture of the Kafka Streams DSL. It allows you to iterate quickly to flesh out ideas for\nworking on your data without getting bogged down in the intricate setup details\nthat some other frameworks may need.\n But at some point, even when working with the best of tools, you’ll come up\nagainst one of those one-off situations: a problem that requires you to deviate from\nThis chapter covers\nEvaluating higher-level abstractions versus \nmore control\nWorking with sources, processors, and sinks to \ncreate a topology\nDigging deeper into the Processor API with a \nstock analysis processor\nCreating a co-grouping processor\nIntegrating the Processor API and the Kafka \nStreams API\n \n\n\n146\nCHAPTER 6\nThe Processor API\nthe traditional path. Whatever the particular case may be, you need a way to dig down\nand write some code that just isn’t possible with a higher-level abstraction.\n6.1\nThe trade-offs of higher-level abstractions vs. \nmore control\nA classic example of trading off higher-level abstractions versus gaining more control\nis using object-relational mapping (ORM) frameworks. A good ORM framework maps\nyour domain objects to database tables and creates the correct SQL queries for you at\nruntime. When you have simple-to-moderate SQL operations (simple SELECT or JOIN\nstatements), using the ORM framework saves you a lot of time. But no matter how\ngood the ORM framework is, there will inevitably be those few queries (very complex\njoins, SELECT statements with nested subselect statements) that just don’t work the way\nyou want. You need to write raw SQL to get the information from the database in the\nformat you need. You can see the trade-off between a higher-level abstraction versus\nmore programmatic control here. Often, you’ll be able to mix the raw SQL with the\nhigher-level mappings provided with the framework.\n This chapter is about those times when you want to do stream processing in a way\nthat the Kafka Streams DSL doesn’t make easy. For example, you’ve seen from work-\ning with the KTable API that the framework controls the timing of forwarding records\ndownstream. You may find yourself in a situation where you want explicit control over\nwhen a record is sent. You might be tracking trades on Wall Street, and you only want\nto forward records when a stock crosses a particular price threshold. To gain this type\nof control, you can use the Processor API. What the Processor API lacks in ease of\ndevelopment, it makes up for in power. You can write custom processors to do almost\nanything you want.\n In this chapter, you’ll learn how to use the Processor API to handle situations\nlike these:\nSchedule actions to occur at regular intervals (either based on timestamps in\nthe records or wall-clock time).\nGain full control over when records are sent downstream.\nForward records to specific child nodes.\nCreate functionality that doesn’t exist in the Kafka Streams API (you’ll see an\nexample of this when we build a co-grouping processor).\nFirst, let’s look at how to use the Processor API by developing a topology step by step. \n6.2\nWorking with sources, processors, and sinks to create \na topology\nLet’s say you’re the owner of a successful brewery (Pops Hops) with several locations.\nYou’ve recently expanded your business to accept online orders from distributors,\nincluding international sales to Europe. You want to route orders within the company\nbased on whether the order is domestic or international, converting any European\nsales from British pounds or euros to US dollars.\n \n\n\n147\nWorking with sources, processors, and sinks to create a topology\n If you were to sketch out the flow of operation, it would look something like fig-\nure 6.1. In building this example, you’ll see how the Processor API gives you the flexi-\nbility to select specific child nodes when forwarding records. Let’s start by creating a\nsource node\n6.2.1\nAdding a source node\nThe first step in constructing a topology is establishing the source nodes. The follow-\ning listing (found in src/main/java/bbejeck/chapter_6/PopsHopsApplication.java)\nsets the data source for the new topology.\ntopology.addSource(LATEST,\n      \npurchaseSourceNodeName,\nnew UsePreviousTimeOnInvalidTimestamp(),\nstringDeserializer,\nbeerPurchaseDeserializer,\nTopics.POPS_HOPS_PURCHASES.topicName())  \nIn the Topology.addSource() method, there are some parameters you didn’t use in\nthe DSL. First, you name the source node. When you used the Kafka Streams DSL,\nyou didn’t need to pass in a name because the KStream instance generated a name\nfor the node. But when you use the Processor API, you need to provide the names of\nthe nodes in the topology. The node name is used to wire up a child node to a par-\nent node.\nListing 6.1\nCreating the beer application source node\nSource node\ndomestic-sales sink\ninternational-sales sink\nBeer-purchase processor\nFigure 6.1\nBeer sales distribution pipeline\nSpecifies the offset \nreset to use\nSpecifies the name \nof this node\nSpecifies the\nTimestampExtractor\nto use for this\nsource\nSets the key deserializer\nSets the value \ndeserializer\nSpecifies the name of the\ntopic to consume data from\n \n\n\n148\nCHAPTER 6\nThe Processor API\n Next, you specify the timestamp extractor to use with this source. In section 4.5.1,\nwe discussed the different timestamp extractors available to use for each stream\nsource. Here, you’re using the UsePreviousTimeOnInvalidTimestamp class; all other\nsources in the application will use the default FailOnInvalidTimestamp class.\n Next, you provide a key deserializer and a value deserializer, which represents\nanother departure from the Kafka Streams DSL. In the DSL, you supplied Serde\ninstances when creating source or sink nodes. The Serde itself contains a serializer\nand deserializer, and the Kafka Streams DSL uses the appropriate one, depending on\nwhether you’re going from object to byte array, or from byte array to object. Because\nthe Processor API is a lower-level abstraction, you directly provide a deserializer when\ncreating a source node and a serializer when creating a sink node. Finally, you provide\nthe name of the source topic.\n Let’s next look at how you’ll work with purchase records coming into the application. \n6.2.2\nAdding a processor node\nNow, you’ll add a processor to work with the records coming in from the source node\n(found in src/main/java/bbejeck/chapter_6/PopsHopsApplication.java).\nBeerPurchaseProcessor beerProcessor =\n➥ new BeerPurchaseProcessor(domesticSalesSink, internationalSalesSink);\ntopology.addSource(LATEST,\npurchaseSourceNodeName,\nnew UsePreviousTimeOnInvalidTimestamp(),\nstringDeserializer,\nbeerPurchaseDeserializer,\nTopics.POPS_HOPS_PURCHASES.topicName())\n.addProcessor(purchaseProcessor,\n() -> beerProcessor,\npurchaseSourceNodeName);\nThis code uses the fluent interface pattern for constructing the topology. The differ-\nence from the Kafka Streams API lies in the return type. With the Kafka Streams API,\nevery call on a KStream operator returns a new KStream or KTable instance. In the\nProcessor API, each call to Topology returns the same Topology instance.\n In the second annotation, you pass in the processor instantiated on the first line of\nthe code example. The Topology.addProcessor method takes an instance of a\nProcessorSupplier interface for the second parameter, but because the Processor-\nSupplier is a single-method interface, you can replace it with a lambda expression.\n The key point in this section is that the third parameter, purchaseSourceNode-\nName, of the addProcessor() method is the same as the second parameter of the\naddSource() method, as illustrated in figure 6.2. This establishes the parent-child\nrelationship between nodes. The parent-child relationship, in turn, determines how\nListing 6.2\nAdding a processor node\nNames the \nprocessor node\nAdds the processor\ndefined above\nSpecifies the name of the \nparent node or nodes\n \n\n\n149\nWorking with sources, processors, and sinks to create a topology\nrecords move from one processor to the next in a Kafka Streams application. Figure 6.3\nreviews what you’ve built so far.\nLet’s take a second to discuss the BeerPurchaseProcessor, created in listing 6.1, func-\ntions. The processor has two responsibilities:\nConvert international sales amounts (in euros) to US dollars.\nBased on the origin of the sale (domestic or international), route the record to\nthe appropriate sink node.\nAll of this takes place in the process() method. To quickly summarize, here’s what\nthe process() method does:\n1\nCheck the currency type. If it’s not in dollars, convert it to dollars.\n2\nIf it’s a non-domestic sale, forward the updated record to the international-\nsales topic.\n3\nOtherwise, forward the record directly to the domestic-sales topic.\nHere’s the code for this processor (found in src/main/java/bbejeck/chapter_6/\nprocessor/BearPurchaseProcessor.java).\nFigure 6.2\nWiring up parent and child nodes in the Processor API\nbuilder.addSource(LATEST,\n,\npurchaseSourceNodeName\nnew UsePreviousTimeOnInvalidTimestamp()\nstringDeserializer,\nbeerPurchaseDeserializer,\n\"pops-hops-purchases\");\nbuilder.addProcessor(purchaseProcessor,\n() -> beerProcessor,\n);\npurchaseSourceNodeName\nThe\nof the source node (above) is used\nname\nfor the\nof the processing node\nparent name\n(below). This establishes the parent-child\nrelationship, which directs data ﬂow\nin Kafka Streams.\nname\nparent name\nFigure 6.3\nThe Processor API topology so far, \nincluding node names and parent names\nSource node\nBeer-purchase processor\nname = \"beer-purchase-source\"\nname = \"purchase-processor\"\nparent = \"beer-purchase-source\"\n \n\n\n150\nCHAPTER 6\nThe Processor API\npublic class BeerPurchaseProcessor extends\n➥ AbstractProcessor<String, BeerPurchase> {\nprivate String domesticSalesNode;\nprivate String internationalSalesNode;\npublic BeerPurchaseProcessor(String domesticSalesNode,\nString internationalSalesNode) {\nthis.domesticSalesNode = domesticSalesNode;\nthis.internationalSalesNode = internationalSalesNode;\n}\n@Override\npublic void process(String key, BeerPurchase beerPurchase) {\nCurrency transactionCurrency = beerPurchase.getCurrency();\nif (transactionCurrency != DOLLARS) {\nBeerPurchase dollarBeerPurchase;\nBeerPurchase.Builder builder =\n➥ BeerPurchase.newBuilder(beerPurchase);\ndouble internationalSaleAmount = beerPurchase.getTotalSale();\nString pattern = \"###.##\";\nDecimalFormat decimalFormat = new DecimalFormat(pattern);\nbuilder.currency(DOLLARS);\nbuilder.totalSale(Double.parseDouble(decimalFormat.\n➥ format(transactionCurrency\n➥ .convertToDollars(internationalSaleAmount))));\ndollarBeerPurchase = builder.build();\ncontext().forward(key,\n➥ dollarBeerPurchase, internationalSalesNode);\n} else {\ncontext().forward(key, beerPurchase, domesticSalesNode);\n}\n}\n}\nThis example extends AbstractProcessor, a class with overrides for Processor inter-\nface methods, except for the process() method. The Processor.process() method\nis where you perform actions on the records flowing through the topology.\nNOTE\nThe Processor interface provides the init(), process(), punctuate(),\nand close() methods. The Processor is the main driver of any application\nlogic that works with records in your streaming application. In the examples,\nyou’ll mostly use the AbstractProcessor class, so you’ll only override the\nmethods you want. The AbstractProcessor class initializes the Processor-\nContext for you, so if you don’t need to do any setup in your class, you don’t\nneed to override the init() method.\nListing 6.3\nBeerPurchaseProcessor\nSets the \nnames for \ndifferent \nnodes to \nforward \nrecords to\nThe process() method,\nwhere the action takes place\nConverts \ninternational \nsales to US dollars\nUses the ProcessorContext (returned from\nthe context() method) and forwards\nrecords to the international child node\nSends\nrecords for\ndomestic\nsales to the\ndomestic\nchild node\n \n\n\n151\nWorking with sources, processors, and sinks to create a topology\nThe last few lines of listing 6.3 demonstrate the main point of this example—the abil-\nity to forward records to specific child nodes. The context() method in these lines\nretrieves a reference to the ProcessorContext object for this processor. All processors\nin a topology receive a reference to the ProcessorContext via the init() method,\nwhich is executed by the StreamTask when initializing the topology.\n Now that you’ve seen how you can process records, the next step is to connect a\nsink node (topic) so you can write records back to Kafka. \n6.2.3\nAdding a sink node\nBy now, you probably have a good feel for the flow of using the Processor API. To add\na source, you used addSource, and for adding a processor, you used addProcessor. As\nyou might imagine, you’ll use the addSink() method to wire up a sink node (topic) to\na processor node. Figure 6.4 shows the updated topology.\nYou can update the topology you’re building by adding sink nodes in the code now\n(found in src/main/java/bbejeck/chapter_6/PopsHopsApplication.java).\ntopology.addSource(LATEST,\npurchaseSourceNodeName,\nnew UsePreviousTimeOnInvalidTimestamp(),\nstringDeserializer,\nbeerPurchaseDeserializer,\nTopics.POPS_HOPS_PURCHASES.topicName())\n.addProcessor(purchaseProcessor,\nListing 6.4\nAdding a sink node\nFigure 6.4\nCompleting the topology by adding sink nodes\nSource node\nDomestic sales sink\nInternational sales sink\nBeer-purchase processor\nname = purchaseSourceNodeName\nname = domesticSalesSink\nparent = purchaseProcessor\nname = purchaseProcessor\nparent = purchaseSourceNodeName\nname = internationalSalesSink\nparent = purchaseProcessor\nNote that the two sink nodes\nhere have the same parent.\n \n\n\n152\nCHAPTER 6\nThe Processor API\n() -> beerProcessor,\npurchaseSourceNodeName)\n.addSink(internationalSalesSink,\n\"international-sales\",\nstringSerializer,\nbeerPurchaseSerializer,\npurchaseProcessor)\n.addSink(domesticSalesSink,\n\"domestic-sales\",\nstringSerializer,\nbeerPurchaseSerializer,\npurchaseProcessor);\nIn this listing, you add two sink nodes, one for dollars and another for euros. Depend-\ning on the currency of the transaction, you’ll write the records out to the appropri-\nate topic.\n The key point to notice when adding two sink nodes here is that both have the\nsame parent name. By supplying the same parent name to both sink nodes, you’ve\nwired both of them to your processor (as shown in figure 6.4).\n You’ve seen in this first example how you can wire topologies together and forward\nrecords to specific child nodes. Although the Processor API is a little more verbose\nthan the Kafka Streams API, it’s still easy to construct topologies. The next example\nwill explore more of the flexibility the Processor API provides. \n6.3\nDigging deeper into the Processor API with a stock \nanalysis processor\nYou’ll now return to the world of finance and put on your day trading hat. As a day\ntrader, you want to analyze how stock prices are changing with the intent of picking\nthe best time to buy and sell. The goal is to take advantage of market fluctuations and\nmake a quick profit. We’ll consider a few key indicators, hoping they’ll indicate when\nyou should make a move.\n This is the list of requirements:\nShow the current value of the stock.\nIndicate whether the price per share is trending up or down.\nInclude the total share volume so far, and whether the volume is trending up\nor down.\nOnly send records downstream for stocks displaying 2% trending (up or down).\nCollect a minimum of 20 samples for a given stock before performing any cal-\nculations.\nLet’s walk through how you might handle this analysis manually. Figure 6.5 shows the\nsort of decision tree you’ll want to create to help make decisions.\nName of \nthe sink\nThe topic this \nsink represents\nSerializer\nfor the key\nSerializer for the value\nParent node \nfor this sink\nName of the sink\nThe topic this \nsink represents\nSerializer for\nthe value\nParent node\nfor this sink\n \n\n\n153\nDigging deeper into the Processor API with a stock analysis processor\nThere are a handful of calculations you’ll need to perform for your analysis. Addition-\nally, you’ll use these calculation results to determine if and when you should forward\nrecords downstream.\n This restriction on sending records means you can’t rely on the standard mecha-\nnisms of commit time or cache flushes to handle the flow for you, which rules out\nusing the Kafka Streams API. It goes without saying that you’ll also require state, so\nyou can keep track of changes over time. What you need here is the ability to write a\ncustom processor. Let’s look at the solution to the problem.\n6.3.1\nThe stock-performance processor application\nHere’s the topology for the stock-performance application (found in src/main/java/\nbbejeck/chapter_6/StockPerformanceApplication.java).\nTopology topology = new Topology();\nString stocksStateStore = \"stock-performance-store\";\ndouble differentialThreshold = 0.02;\nFor demo purposes only\nI’m pretty sure it goes without saying, but I’ll state the obvious anyway: these stock\nprice evaluations are for demonstration purposes only. Please don’t infer any real\nmarket-forecasting ability from this example. This model bears no similarity to a real-\nlife approach and is presented only to demonstrate a more complex processing situ-\nation. I’m certainly not a day trader!\nListing 6.5\nStock-performance application with custom processor\nFigure 6.5\nStock trend updates\nYes\nNo\nHold until conditions change.\nOver the last X number of trades, has the price\nor volume of shares increased/decreased\nby more than 2%?\nThe current status of stock XXYY\nSymbol: XXYY; Share price: $10.79; Total volume: 5,123,987\nIf the price and/or volume is increasing, sell;\nif the price and/or volume is decreasing, buy.\nSets the percentage \ndifferential for forwarding \nstock information\n \n",
      "page_number": 163
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 172-181)",
      "start_page": 172,
      "end_page": 181,
      "detection_method": "topic_boundary",
      "content": "154\nCHAPTER 6\nThe Processor API\nKeyValueBytesStoreSupplier storeSupplier =\n➥ Stores.inMemoryKeyValueStore(stocksStateStore);\nStoreBuilder<KeyValueStore<String, StockPerformance>> storeBuilder\n➥ = Stores.keyValueStoreBuilder(\n➥ storeSupplier, Serdes.String(), stockPerformanceSerde);\ntopology.addSource(\"stocks-source\",\nstringDeserializer,\nstockTransactionDeserializer,\n\"stock-transactions\")\n.addProcessor(\"stocks-processor\",\n➥ () -> new StockPerformanceProcessor(\n➥ stocksStateStore, differentialThreshold), \"stocks-source\")\n.addStateStore(storeBuilder,\"stocks-processor\")\n.addSink(\"stocks-sink\",\n\"stock-performance\",\nstringSerializer,\nstockPerformanceSerializer,\n\"stocks-processor\");\nThis topology has the same flow as the previous example, so we’ll focus on the new\nfeatures in the processor. In the previous example, you don’t have any setup to do, so\nyou rely on the AbstractProcessor.init method to initialize the ProcessorContext\nobject. In this example, however, you need to use a state store, and you also want to\nschedule when you emit records, instead of forwarding records each time you\nreceive them.\n Let’s look first at the init() method in the processor (found in src/main/java/\nbbejeck/chapter_6/processor/StockPerformanceProcessor.java).\n@Override\npublic void init(ProcessorContext processorContext) {\nsuper.init(processorContext);\nkeyValueStore =\n➥ (KeyValueStore) context().getStateStore(stateStoreName);\nStockPerformancePunctuator punctuator =\n➥ new StockPerformancePunctuator(differentialThreshold,\ncontext(),\nkeyValueStore);\ncontext().schedule(10000, PunctuationType.WALL_CLOCK_TIME,\n➥ punctuator);\n}\n}\nFirst, you need to initialize the AbstractProcessor with the ProcessorContext, so\nyou call the init() method on the superclass. Next, you grab a reference to the state\nstore you created in the topology. All you need to do here is set the state store to a\nListing 6.6\ninit() method tasks\nCreates\nan in-\nmemory\nkey/value\nstate store\nCreates the \nStoreBuilder \nto place in the \ntopology\n Adds the \nprocessor to \nthe topology\nAdds the \nstate store \nto the stocks \nprocessor\nAdds a sink for\nwriting results out,\nalthough you could\nuse a printing sink\nas well\nInitializes \nProcessorContext via \nthe AbstractProcessor \nsuperclass\nRetrieving state \nstore created when \nbuilding topology\nInitializing the \nPunctuator to handle \nthe scheduled \nprocessing\nSchedules Punctuator.punctuate() to \nbe called every 10 seconds\n \n\n\n155\nDigging deeper into the Processor API with a stock analysis processor\nvariable for use later in the processor. Listing 6.5 also introduces a Punctuator, an\ninterface that’s a callback to handle scheduled execution of processor logic but encap-\nsulated in the Punctuator.punctuate method.\nTIP\nThe ProcessorContext.schedule(long, PunctuationType, Punctuator)\nmethod returns a type of Cancellable, allowing you to cancel a punctuation\nand manage more-advanced scenarios, like those found in the “Punctuate\nUse Cases” discussion (http://mng.bz/YSKF). I don’t have examples or a dis-\ncussion here, but I present some examples in src/main/java/bbejeck/chap-\nter_6/cancellation.\nIn the last line of listing 6.5, you use the ProcessorContext to schedule the Punctu-\nator to execute every 10 seconds. The second parameter, PunctuationType.WALL\n_CLOCK_TIME, specifies that you want to call Punctuator.punctuate every 10 sec-\nonds based on WALL_CLOCK_TIME. Your other option is to specify Punctuation-\nType.STREAM_TIME, which means the execution of Punctuator.punctuate is still\nscheduled every 10 seconds but driven by the time elapsed according to timestamps\nin the data. Let’s take a moment to discuss the difference between these two\nPunctuationType settings.\nPUNCTUATION SEMANTICS\nLet’s start our conversation on punctuation semantics with STREAM_TIME, because it\nrequires a little more explanation. Figure 6.6 illustrates the concept of stream time.\nLet’s walk through some details to gain a deeper understanding of how the schedule is\ndetermined (note that some of the Kafka Stream internals are not shown):\n1\nThe StreamTask extracts the smallest timestamp from the PartitionGroup. The\nPartitionGroup is a set of partitions for a given StreamThread, and it contains\nall timestamp information for all partitions in the group.\n2\nDuring the processing of records, the StreamThread iterates over its Stream-\nTask object, and each task will end up calling punctuate for each of its proces-\nsors that are eligible for punctuation. Recall that you collect a minimum of 20\ntrades before you examine an individual stock’s performance.\n3\nIf the timestamp from the last execution of punctuate (plus the scheduled\ntime) is less than or equal to the extracted timestamp from the Partition-\nGroup, then Kafka Streams calls that processor’s punctuate() method.\nThe key point here is that the application advances timestamps via the Timestamp-\nExtractor, so punctuate() calls are consistent only if data arrives at a constant rate. If\nyour flow of data is sporadic, the punctuate() method won’t get executed at the regu-\nlarly scheduled intervals.\n With PunctuationType.WALL_CLOCK_TIME, on the other hand, the execution of\nPunctuator.punctuate is more predictable, as it uses wall-clock time. Note that sys-\ntem-time semantics is best effort—wall-clock time is advanced in the polling interval,\nand the granularity is dependent on how long it takes to complete a polling cycle. So,\n \n\n\n156\nCHAPTER 6\nThe Processor API\nwith the example in listing 6.6, you can expect the punctuation activity to be executed\ncloser to every 10 seconds, regardless of data activity.\n Which approach you choose to use is entirely dependent on your needs. If you\nneed some activity performed on a regular basis, regardless of data flow, using system\ntime is probably the best bet. On the other hand, if you only need calculations per-\nformed on incoming data, and some lag time between executions is acceptable, try\nstream-time semantics.\nNOTE\nBefore Kafka 0.11.0, punctuation involved the ProcessorContext\n.schedule(long time) method, which in turn called the Processor.punctuate\nmethod at the scheduled interval. This approach only worked on stream-\ntime semantics, and both methods are now deprecated. I mention depre-\ncated methods in this book, but I only use the latest punctuation methods\nin the examples.\nNow that we’ve covered scheduling and punctuation, let’s move on to handling incom-\ning records. \nFigure 6.6\nPunctuation scheduling using STREAM_TIME\nPartition A\nPartition B\nBecause partition A has the smallest timestamp, it’s chosen ﬁrst:\n1) process called with record A\n2) process called with record B\nNow partition B has the smallest timestamp:\n3) process called with record C\n4) process called with record D\nSwitch back to partition A, which has the smallest timestamp again:\n5) process called with record E\n6) punctuate called because time elapsed from timestamps is 5 seconds\n7) process called with record F\nFinally, switch back to partition B:\n8) process called with record G\n9) punctuate called again as 5 more seconds have elapsed, according to the timestamps\nA:1\nB:2\nE:5\nF:6\nC:3\nD:4\nG:10\nIn the two partitions below, the letter represents the record, and the number is\nthe timestamp. For this example, we’ll assume that\nis scheduled to\npunctuate\nrun every ﬁve seconds.\n \n\n\n157\nDigging deeper into the Processor API with a stock analysis processor\n6.3.2\nThe process() method\nThe process() method is where you’ll perform all of your calculations to evaluate\nstock performance. There are several steps to take when you receive a record:\n1\nCheck the state store to see if you have a corresponding StockPerformance\nobject for the record’s stock ticker symbol.\n2\nIf the store doesn’t contain the StockPerformance object, one is created. Then,\nthe StockPerfomance instance adds the current share price and share volume\nand updates your calculations.\n3\nStart performing calculations once you hit 20 transactions for any given stock.\nAlthough financial analysis is beyond the scope of this book, we should take a minute\nto look at the calculations. For both the share price and volume, you’re going to per-\nform a simple moving average (SMA). In the financial-trading world, SMAs are used\nto calculate the average for datasets of size N.\n For this example, you’ll set N to 20. Setting a maximum size means that as new\ntrades come in, you collect the share price and number of shares traded for the first\n20 transactions. Once you hit that threshold, you remove the oldest value and add the\nlatest one. Using the SMA, you get a rolling average of stock price and volume over\nthe last 20 trades. It’s important to note you won’t have to recalculate the entire\namount as new values come in.\n Figure 6.7 provides a high-level walk-through of the process() method, illustrat-\ning what you’d do if you were to perform these steps manually. The process()\nmethod is where you’ll perform all the calculations.\nNow, let’s look at the code that makes up the process() method (found in src/main/\njava/bbejeck/chapter_6/processor/StockPerformanceProcessor.java).\nFigure 6.7\nStock analysis process() method walk-through\n1) Price: $10.79, Number shares: 5,000\n2) Price: $11.79, Number shares: 7,000\n20) Price: $12.05, Number shares: 8,000\nAs stocks come in, you keep a rolling average of share price\nand volume of shares over the last 20 trades. You also\nrecord the timestamp of the last update.\nBefore you have 20 trades, you take the average of the number\nof trades you’ve collected so far.\n1) Price: $10.79, Number shares: 5,000\n2) Price: $11.79, Number shares: 7,000\n20) Price: $12.05, Number shares: 8,000\n21) Price: $11.75, Number shares: 6,500\n22) Price: $11.95, Number shares: 7,300\nAfter you hit 20 trades, you drop the oldest trade and add\nthe newest one. You also update the rolling average by\nremoving the old value from the average.\n \n\n\n158\nCHAPTER 6\nThe Processor API\n@Override\npublic void process(String symbol, StockTransaction transaction) {\nStockPerformance stockPerformance = keyValueStore.get(symbol);\nif (stockPerformance == null) {\nstockPerformance = new StockPerformance();\n}\nstockPerformance.updatePriceStats(transaction.getSharePrice());\nstockPerformance.updateVolumeStats(transaction.getShares());\nstockPerformance.setLastUpdateSent(Instant.now());\nkeyValueStore.put(symbol, stockPerformance);\n}\nIn the process() method, you take the latest share price and the number of shares\ninvolved in the transaction and add them to the StockPerformance object. Notice that\nall details of how you perform the updates are abstracted inside the StockPerformance\nobject. Keeping most of the business logic out of the processor is a good idea—we’ll\ncome back to that point when we cover testing in chapter 8.\n There are two key calculations: determining the moving average, and calculating\nthe differential of stock price/volume from the current average. You don’t want to cal-\nculate an average until you’ve collected data from 20 transactions, so you defer doing\nanything until the processor receives 20 trades. When you have data from 20 trades\nfor an individual stock, you calculate your first average. Then, you take the current\nvalue of the stock price or a number of shares and divide by the moving average, con-\nverting the result to a percentage.\nNOTE\nIf you want to see the calculations, the StockPerformance code can be\nfound in src/main/java/bejeck/model/StockPerformance.java.\nIn the Processor example in listing 6.3, once you worked your way through the\nprocess() method, you forwarded the records downstream. In this case, you store\nthe final results in the state store and leave the forwarding of records to the\nPunctuator.punctuate method. \n6.3.3\nThe punctuator execution\nWe’ve already discussed the punctuation semantics and scheduling, so let’s jump\nstraight into the code for the Punctuator.punctuate method (found in src/main/\njava/bejeck/chapter_6/processor/punctuator/StockPerformancePunctuator.java).\n \nListing 6.7\nprocess() implementation\nRetrieves \nprevious \nperformance \nstats, \npossibly null\nCreates a new StockPerformance \nobject if one isn’t in the state store\nUpdates\nthe price\nstatistics\nfor this\nstock\nUpdates the \nvolume statistics \nfor this stock\nSets the\ntimestamp\nof the last\nupdate\nPlaces the updated StockPerformance\nobject into the state store\n \n\n\n159\nThe co-group processor\n@Override\npublic void punctuate(long timestamp) {\nKeyValueIterator<String, StockPerformance> performanceIterator =\n➥ keyValueStore.all();\nwhile (performanceIterator.hasNext()) {\nKeyValue<String, StockPerformance> keyValue =\n➥ performanceIterator.next();\nString key = keyValue.key;\nStockPerformance stockPerformance = keyValue.value;\nif (stockPerformance != null) {\nif (stockPerformance.priceDifferential()\n➥ >= differentialThreshold ||\nstockPerformance.volumeDifferential()\n➥ >= differentialThreshold) {\ncontext.forward(key, stockPerformance);\n}\n}\n}\n}\nThe procedure in the Punctuator.punctuate method is simple. You iterate over the\nkey/value pairs in the state store, and if the value has crossed over the predefined\nthreshold, you forward the record downstream.\n An important concept to remember here is that, whereas before you relied on a\ncombination of committing or cache flushing to forward records, now you define the\nterms for when records get forwarded. Additionally, even though you expect to exe-\ncute this code every 10 seconds, that doesn’t guarantee you’ll emit records. They must\nmeet the differential threshold. Also note that the Processor.process and Punctuator\n.punctuate methods aren’t called concurrently.\nNOTE\nAlthough we’re demonstrating access to a state store, it’s a good time\nto review Kafka Streams’ architecture and go over a few key points. Each\nStreamTask has its own copy of a local state store, and StreamThread objects\ndon’t share tasks or data. As records make their way through the topology,\neach node is visited in a depth-first manner, meaning there’s never concur-\nrent access to state stores from any given processor.\nThis example has given you an excellent introduction to writing a custom processor,\nbut you can take writing custom processors a bit further by adding a new data struc-\nture and an entirely new way of aggregating data that doesn’t currently exist in the\nAPI. With this in mind, we’ll move on to adding a co-group processor. \n6.4\nThe co-group processor\nBack in chapter 4, we discussed joins between two streams; specifically, we joined pur-\nchases from different departments within a given time frame to promote business. You\nListing 6.8\nPunctuation code\nRetrieves the iterator \nto go over all key values \nin the state store\nChecks the threshold \nfor the current stock\nIf you’ve met or \nexceeded the \nthreshold, forwards \nthe record\n \n\n\n160\nCHAPTER 6\nThe Processor API\ncan use joins to bring together records that have the same key and that arrive in the\nsame time window. With joins, there’s an implied one-to-one mapping of records from\nstream A to stream B. Figure 6.8 depicts this relationship.\nNow, let’s imagine that you want to do a similar type of analysis, but instead of using a\none-to-one join by key, you want two collections of data joined by a common key, a co-\ngrouping of data. Suppose you’re the manager of a popular online day-trading applica-\ntion. Day traders use your application for several hours a day—sometimes the entire\ntime the stock market is open. One of the metrics your application tracks is the notion\nof events. You’ve defined an event as being when a user clicks on a ticker symbol to\nread more about a company and its financial outlook. You want to do some deeper\nanalysis between those user clicks in the application and the purchase of stocks by\nusers. You want course-grained results, comparing multiple clicks and purchases to\ndetermine some overall pattern. What you need is a tuple with two collections of each\nevent type by the company trading symbol, as shown in figure 6.9.\nFigure 6.8\nRecords A and B joined by a common key\nRecords A and B are individually joined by a key\nand combined to produce a single joined record.\nAB:1 AB:2 AB:3 AB:4 AB:5\nCafe purchases\nStream A key/value pairs\nA:1 A:2 A:3 A:4 A:5\nElectronics purchases\nStream B key/value pairs\nB:1 B:2 B:3 B:4 B:5\nFor this example, you assume the window time lines up to\ngive you only one-to-one joins so that each record uniquely\nmatches up with only one other record.\nJoins\nRecords A (click events from the day-trading application)\nand B (purchase transactions) are co-grouped by key\n(stock symbol) and produce a key/value pair where the key\nis K, and the value is Tuple, containing a collection of click\nevents and a collection of stock transactions.\nK, Tuple ( [A1, A2, A3, A4, A5], [B1, B2, B3, B4, B5] )\nClickEvents key/value pairs\nA:1 A:2 A:3 A:4 A:5\nStockPurchase key/value pairs\nB:1 B:2 B:3 B:4 B:5\nFor this example, each collection is populated with what’s available at the\ntime punctuate is called. There might be an empty collection at any point.\nFigure 6.9\nOutput of a key with a tuple containing two collections of data—\na co-grouped result\n \n\n\n161\nThe co-group processor\nYour goal is to combine snapshots of click events and stock transactions for a given\ncompany, every N seconds, but you aren’t waiting for records from either stream to\narrive. When the specified amount of time goes by, you want a co-grouping of click\nevents and stock transactions by company ticker symbol. If either type of event isn’t\npresent, one of the collections in the tuple will be empty. If you’re familiar with\nApache Spark or Apache Flink, this functionality is similar to the PairRDDFunctions\n.cogroup method (http://mng.bz/LaD4) and the CoGroupDataSet class (http://mng\n.bz/FH9m), respectively. Let’s walk through the steps you’ll take in constructing this\nprocessor.\n6.4.1\nBuilding the co-grouping processor\nTo create the co-grouping processor, you need to tie a few pieces together:\n1\nDefine two topics (stock-transactions, events).\n2\nAdd two processors to consume records from the topics.\n3\nAdd a third processor to act as an aggregator/co-grouping for the two preced-\ning processors.\n4\nAdd a state store for the aggregating processor to keep the state for both events.\n5\nAdd a sink node to write the results to (and/or a printing processor to print\nresults to console).\nNow, lets walk through the steps to put this processor together.\nDEFINING THE SOURCE NODES\nYou’re already familiar with the first step, creating the source nodes. This time, you’ll\ncreate two source nodes to support reading both the click event stream and the stock-\ntransactions stream. To keep track of where we are in the topology, we’ll build on fig-\nure 6.10. The code for creating the source nodes is shown in the following listing\n(found in src/main/java/bbejeck/chapter_6/CoGroupingApplication.java).\n//I’ve left out configuration and (de)serializer creation for clarity.\ntopology.addSource(\"Txn-Source\",\nstringDeserializer,\nstockTransactionDeserializer,\n\"stock-transactions\")\n.addSource(\"Events-Source\",\nstringDeserializer,\nclickEventDeserializer,\n\"events\")\nWith the sources for the topology in place, let’s move on to the next step. \nListing 6.9\nSource nodes for co-grouping processor\nSource node for the \nstock-transactions topic\nSource node for \nthe events topic\n \n\n\n162\nCHAPTER 6\nThe Processor API\nADDING THE PROCESSOR NODES\nNow, you’ll add the workhorses of the topology, the processors. Figure 6.11 shows the\nupdated topology graph. Here’s the code for adding these new processors (found in\nsrc/main/java/bbejeck/chapter_6/CoGroupingApplication.java).\n.addProcessor(\"Txn-Processor\",\nStockTransactionProcessor::new,\n\"Txn-Source\")\n.addProcessor(\"Events-Processor\",\nClickEventProcessor::new,\n\"Events-Source\")\n.addProcessor(\"CoGrouping-Processor\",\nCogroupingProcessor::new,\n\"Txn-Processor\",\n\"Events-Processor\")\nIn the first two lines, the parent names are the names of source nodes reading from\nthe stock-transactions and events topics, respectively. The third processor has the\nnames of both processors given as parent nodes. This means both processors will feed\nthe aggregation processor.\nListing 6.10\nThe processor nodes\nFigure 6.10\nCo-grouping \nsource nodes\ntransaction-source topic\nClick events source node\nAdds the \nStockTransactionProcessor\nAdds the \nClickEventProcessor\nAdds the CogroupingProcessor, which \nis a child node of both processors\nFigure 6.11\nAdding processor nodes\ntransaction-source topic\nClick event source node\nTxn processor\nEvents processor\nCo-grouping processor\nBoth of those send records to\nthe co-grouping processor.\nThe source nodes send\nrecords to the Txn processor\nand events processor.\n \n\n\n163\nThe co-group processor\nFor the ProcessorSupplier instances, you’re again taking a Java 8 shortcut. This time,\nyou’ve shortened the form, even more, to use a method handle: in this case, a con-\nstructor call to create the associated processor.\nTIP\nWith single-method no-arg interfaces in Java 8, you can use a lambda in\nthe form of ()-> doSomething. But because the ProcessorSupplier’s only role\nis to return a (possibly new each time) Processor object, you can shorten the\nform even more to use a method handle for the constructor of the Processor\ntype. Note that this only applies for no-arg constructors.\nLet’s look at why you’ve set up the processors in this manner. This example is an\naggregation operation, and the roles of the StockTransactionProcessor and Click-\nEventProcessor are to wrap their respective objects into smaller aggregate objects\nand then forward them to another processor for a total aggregation. Both the Stock-\nTransactionProcessor and the ClickEventProcessor perform the smaller aggregation,\nand they forward their records to the CogroupingProcessor. The CogroupingProcessor\nthen performs the co-grouping and forwards the results at regular intervals (intervals\ndriven by timestamps) to an output topic.\n The following listing shows the code for the processors (found in src/main/java/\nbbejeck/chapter_6/processor/cogrouping/StockTransactionProcessor.java).\npublic class StockTransactionProcessor extends\n➥ AbstractProcessor<String, StockTransaction> {\n@Override\n@SuppressWarnings(\"unchecked\")\npublic void init(ProcessorContext context) {\nsuper.init(context);\n}\n@Override\npublic void process(String key, StockTransaction value) {\nif (key != null) {\nTuple<ClickEvent, StockTransaction> tuple =\n➥ Tuple.of(null, value);\ncontext().forward(key, tuple);\n}\n}\n}\nAs you can see, StockTransactionProcessor adds the StockTransaction to the\naggregator (the Tuple) and forwards the record.\nNOTE\nThe Tuple<L, R> shown in listing 6.11 is a custom object for examples\nin this book. You can find it in src/main/java/bbejeck/util/collection/\nTuple.java.\nListing 6.11\nStockTransactionProcessor\nCreates an \naggregate \nobject with the \nStockTransaction\nForwards the tuple to the \nCogroupingProcessor\n \n",
      "page_number": 172
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 182-191)",
      "start_page": 182,
      "end_page": 191,
      "detection_method": "topic_boundary",
      "content": "164\nCHAPTER 6\nThe Processor API\nNow, let’s look at the ClickEventProcessor code (found in src/main/java/bbejeck/\nchapter_6/processor/cogrouping/ClickEventProcessor.java).\npublic class ClickEventProcessor extends\n➥ AbstractProcessor<String, ClickEvent> {\n@Override\n@SuppressWarnings(\"unchecked\")\npublic void init(ProcessorContext context) {\nsuper.init(context);\n}\n@Override\npublic void process(String key, ClickEvent clickEvent) {\nif (key != null) {\nTuple<ClickEvent, StockTransaction> tuple =\n➥ Tuple.of(clickEvent, null);\ncontext().forward(key, tuple);\n}\n}\n}\nAs you can see, the ClickEventProcessor adds the ClickEvent to the Tuple aggrega-\ntor, much like the previous listing.\n To complete the picture of how to perform the aggregation, we need to look at the\nCogroupingProcessor code. It’s more involved, so we’ll examine each method in\nturn, starting with CogroupingProcessor.init() (found in src/main/java/bbejeck/\nchapter_6/processor/cogrouping/AggregatingProcessor.java).\npublic class CogroupingProcessor extends\n➥ AbstractProcessor<String, Tuple<ClickEvent,StockTransaction>> {\nprivate KeyValueStore<String,\n➥ Tuple<List<ClickEvent>,List<StockTransaction>>> tupleStore;\npublic static final\nString TUPLE_STORE_NAME = \"tupleCoGroupStore\";\n@Override\n@SuppressWarnings(\"unchecked\")\npublic void init(ProcessorContext context) {\nsuper.init(context);\ntupleStore = (KeyValueStore)\n➥ context().getStateStore(TUPLE_STORE_NAME);\nCogroupingPunctuator punctuator =\n➥ new CogroupingPunctuator(tupleStore, context());\ncontext().schedule(15000L, STREAM_TIME, punctuator);\n}\nListing 6.12\nClickEventProcessor\nListing 6.13\nThe CogroupingProcessorinit() method\nAdds the \nClickEvent \nto the initial \naggregator \nobject\nForwards the tuple to the \nCogroupingProcessor\nRetrieves the configured \nstate store\nCreates a Punctuator \ninstance, Cogrouping-\nPunctuator, which handles \nall scheduled calls\nSchedules a call to the Punctuator.punctuate() method every 15 seconds\n \n\n\n165\nThe co-group processor\nAs you might expect, the init() method handles the details of setting up the class.\nYou grab the state store configured in the main application and save it in a variable for\nuse later on. You create the CogroupingPunctuator to handle the scheduled punctua-\ntion calls.\nListing 6.13 schedules punctuate for every 15 seconds. Because you’re using the\nPunctuationType.STREAM_TIME semantics, the timestamps in the arriving data drive\nthe calls to punctuate. Remember that if the flow of data isn’t relatively constant, you\nmay have more than 15 seconds between calls to Punctuator.punctuate.\nNOTE\nYou’ll recall from our previous discussion of punctuate semantics that\nyou have two choices: PunctuationType.STREAM_TIME and Punctuation-\nType.WALL_CLOCK_TIME. Listing 6.13 uses STREAM_TIME semantics. There’s\nan additional processor example showing WALL_CLOCK_TIME semantics in\nsrc/main/ava/bbejeck/chapter_6/processor/cogrouping/CogroupingSystem-\nTimeProcessor.java, so you can observe the differences in performance and\nbehavior.\nNext, let’s look at how the CogroupingProcessor performs one of its main tasks in\nthe process() method (found in src/main/java/bbejeck/chapter_6/processor/\ncogrouping/CogroupingProcessor.java).\n@Override\npublic void process(String key,\n➥ Tuple<ClickEvent, StockTransaction> value) {\nTuple<List<ClickEvent>, List<StockTransaction>> cogroupedTuple\n➥ = tupleStore.get(key);\nif (cogroupedTuple == null) {\ncogroupedTuple =\n➥ Tuple.of(new ArrayList<>(), new ArrayList<>());\n}\nif (value._1 != null) {\ncogroupedTuple._1.add(value._1);\n}\nMethod handles for Punctuator\nYou can specify a method handle for the Punctuator instance. To do so, declare a\nmethod in the processor that accepts a single parameter of type long and a void\nreturn type. Then, schedule punctuation like this:\ncontext().schedule(15000L, STREAM_TIME, this::myPunctuationMethod);\nFor an example of this, look in src/main/java/bbejeck/chapter_6/processor/\ncogrouping/CogroupingMethodHandleProcessor.java.\nListing 6.14\nThe CogroupingProcessorprocess() method\nInitializes the total \naggregation if it \ndoesn’t exist yet\nIf the ClickEvent is \nnot null, adds it to the \nlist of click events\n \n\n\n166\nCHAPTER 6\nThe Processor API\nif (value._2 != null) {\ncogroupedTuple._2.add(value._2);\n}\ntupleStore.put(key, cogroupedTuple);\n}\n}\nAs you process incoming smaller aggregates of your overall co-grouping, the first is\nstep checking if you have an instance in your state store already. If you don’t, you cre-\nate a Tuple with empty collections of ClickEvent and StockTransaction.\n Next, you check the incoming smaller aggregation, and if either a ClickEvent or\nStockTransaction is present, you add it to the overall aggregation. The last step in\nthe process() method is putting the Tuple back into the store, updating your aggre-\ngation total.\nNOTE\nAlthough you have two processors forwarding records to one processor\nand accessing one state store, you don’t have to be concerned about concur-\nrency issues. Remember, parent processors forward records to child proces-\nsors in a depth-first manner, so each parent processor serially calls the child\nprocessor. Additionally, Kafka Streams only uses one thread per task, so there\nare never any concurrency access issues.\nThe next step is to look at how punctuation is handled (found in src/main/java/\nbbejeck/chapter_6/processor/cogrouping/CogroupingPunctuator.java). You use the\nupdated API, so we won’t look at the Processor.punctuate call, which is deprecated.\n// leaving out class declaration and constructor for clarity\n@Override\npublic void punctuate(long timestamp) {\nKeyValueIterator<String, Tuple<List<ClickEvent>,\n➥ List<StockTransaction>>> iterator = tupleStore.all();\nwhile (iterator.hasNext()) {\nKeyValue<String, Tuple<List<ClickEvent>, List<StockTransaction>>>\n➥ cogrouping = iterator.next();\n// if either list contains values forward results\nif (cogrouping.value != null &&\n➥ (!cogrouping.value._1.isEmpty() ||\n➥ !cogrouping.value._2.isEmpty())) {\nList<ClickEvent> clickEvents =\n➥ new ArrayList<>(cogrouping.value._1);\nList<StockTransaction> stockTransactions =\n➥ new ArrayList<>(cogrouping.value._2);\nListing 6.15\nThe CogroupingPunctuator.punctuate() method\nIf the StockTransaction \nis not null, adds it to the list \nof stock transactions\nPlaces the updated aggregation \ninto the state store\nGets iterator of \nall co-groupings \nin the store\nRetrieves the next \nco-grouping\nEnsures that the value is not null, and \nthat either collection contains data\nMakes defensive \ncopies of co-grouped \ncollections\n \n\n\n167\nThe co-group processor\ncontext.forward(cogrouping.key,\n➥ Tuple.of(clickEvents, stockTransactions));\ncogrouped.value._1.clear();\ncogrouped.value._2.clear();\ntupleStore.put(cogrouped.key, cogrouped.value);\n}\n}\niterator.close();\n}\nDuring each punctuate call, you retrieve all of the stored records in a KeyValue-\nIterator, and you start to pull out each co-grouped result contained in the iterator.\nThen, you make defensive copies of the collections, create a new co-grouped Tuple,\nand forward it downstream. In this case, you send the co-grouped results to a sink\nnode. Finally, you remove the current co-grouped results and store the tuple back in\nthe store, ready for the next round of records to arrive.\n Now that we’ve covered the co-grouping functionality, let’s complete building the\ntopology. \nADDING THE STATE STORE\nAs you’ve seen, the ability to perform aggregations in a Kafka streaming application\nrequires having state. You’ll need to add a state store to the CogroupingProcessor for\nit to function properly. Figure 6.12 shows the updated topology.\nForwards the key and \naggregated co-grouping\nPuts the cleared-out\ntuple back into the store\nFigure 6.12\nAdding a state store to the co-grouping processor in the topology\nTransaction source node\nClick event source node\nTxn processor\nEvents processor\nCo-grouping processor\nBoth of those send records to\nthe co-grouping processor.\nState store used by co-grouping processor\nThe source nodes send\nrecords to the Txn processor\nand events processor.\n \n\n\n168\nCHAPTER 6\nThe Processor API\nNow, let’s look at the code for adding the state store (found in src/main/java/bbejeck/\nchapter_6/CoGroupingApplication.java).\n// this comes earlier in source code, included here for context\nMap<String, String> changeLogConfigs = new HashMap<>();\nchangeLogConfigs.put(\"retention.ms\",\"120000\" );\n            \nchangeLogConfigs.put(\"cleanup.policy\", \"compact,delete\");\nKeyValueBytesStoreSupplier storeSupplier =\n➥ Stores.persistentKeyValueStore(TUPLE_STORE_NAME);\nStoreBuilder<KeyValueStore<String,\n➥ Tuple<List<ClickEvent>, List<StockTransaction>>>> storeBuilder =\nStores.keyValueStoreBuilder(storeSupplier,\nSerdes.String(),\neventPerformanceTuple)\n➥ .withLoggingEnabled(changeLogConfigs);\n.addStateStore(storeBuilder, \"CoGrouping-Processor\")\nHere, you add a persistent state store. This is a persistent store, because you might get\ninfrequent updates for some keys. With the in-memory and LRU-based stores, infre-\nquently used keys and values might eventually be removed, and here you’ll want the\nability to retrieve information for any key you’ve worked with before.\nTIP\nThe first three lines in listing 6.16 create specific configurations for the\nstate store to keep the changelog at a manageable size. Remember: you can\nconfigure changelog topics with any valid topic configuration.\nThis code is straightforward. One point to notice, though, is that the CoGrouping-\nProcessor is specified as the only processor that can access this store.\n You now have one step left to complete the topology: the ability to read the results\nof the co-grouping. \nADDING THE SINK NODE\nFor the co-grouping topology to be of use, you need to write the data out to a topic (or\nthe console). Let’s update the topology one more time, as shown in figure 6.13.\nNOTE\nIn several examples, I talk about adding a sink node, but in the source\ncode there’s a sink that writes to the console; the sink that writes to a topic is\ncommented out. For development purposes, I use the sink node writing to a\ntopic and to stdout interchangeably.\nListing 6.16\nAdding a state store node\nSpecifies how long to keep records, and\nuses compaction and delete for cleanup\nCreates the store supplier for \na persistent store (RocksDB)\nCreates the \nstore builder\nAdds the changelog configs \nto the store builder\nAdds the store to the \ntopology with the name \nof the processor that \nwill access the store\n \n\n\n169\nThe co-group processor\nNow, the co-grouped aggregation results are written out to a topic for use in further\nanalysis. Here’s the code (found in src/main/java/bbejeck/chapter_6/CoGrouping-\nApplication.java).\n.addSink(\"Tuple-Sink\",\n\"cogrouped-results\",\nstringSerializer,\ntupleSerializer,\n\"CoGrouping-Processor\");\ntopology.addProcessor(\"Print\",\nnew KStreamPrinter(\"Co-Grouping\"),\n\"CoGrouping-Processor\");\nIn this final piece of the topology, you add a sink node, as a child of the CoGrouping-\nProcessor, that writes the co-grouping results out to a topic. Listing 6.17 also adds an\nadditional processor for printing results to the console during development—it’s also\na child node of the CoGrouping-Processor. Remember that with the Processor API,\nthe order in which you define nodes doesn’t establish a parent-child relationship. The\nparent-child relationship is determined by providing the names of previously defined\nprocessors.\nListing 6.17\nThe sink node and a printing processor\nFigure 6.13\nAdding a sink node completes the co-grouping topology\nSink node\nCo-grouping processor\nEvents processor\nClick event source node\nTransaction source node\nTxn processor\nState store used by the\nco-grouping processor\nThe sink node writes \nthe co-grouped tuples \nout to a topic.\nThis processor prints \nresults to stdout for use \nduring development.\n \n\n\n170\nCHAPTER 6\nThe Processor API\n You’ve now built the co-grouping processor. The key point I want you to remember\nfrom this section is that, although it involves more code, using the Processor API gives\nyou the flexibility to create virtually any kind of streaming topology you need.\n Let’s wrap up this chapter with a look at how you can integrate some Processor API\nfunctionality into a KStreams application. \n6.5\nIntegrating the Processor API and the \nKafka Streams API\nSo far, our coverage of the Kafka Streams and the Processor APIs has been separate,\nbut that’s not to say that you can’t combine approaches. Why would you want to mix\nthe two approaches?\n Let’s say you’ve used both the KStream and Processor APIs for a while. You’ve\ncome to prefer the KStream approach, but you want to include some of your previ-\nously defined processors in a KStream application, because they provide some of the\nlower-level control you need.\n The Kafka Streams API offers three methods that allow you to plug in functionality\nbuilt using the Processor API: KStream.process, KStream.transform, and KStream\n.transformValues. You already have some experience with this approach because you\nworked with the ValueTransformer in section 4.2.2.\n The KStream.process method creates a terminal node, whereas the KStream\n.transform (or KStream.transformValues) method returns a new KStream instance\nallowing you to continue adding processors to that node. Note also that the trans-\nform methods are stateful, so you also provide a state store name when using them.\nBecause KStream.process results in a terminal node, you’ll usually want to use either\nKStream.transform or KStream.transformValues.\n From there, you can replace your Processor with a Transformer instance. The\nmain difference between the two interfaces is that the Processor’s main action method\nis process(), which has a void return, whereas the Transformer uses transform() and\nexpects a return type of R. Both offer the same punctuation semantics.\n In most cases, replacing a Processor is a matter of taking the logic from the\nProcessor.process method and placing it in the Transformer.transform method.\nYou’ll need to account for returning a value, but returning null and forwarding results\nwith ProcessorContext.forward is an option.\nTIP\nThe transformer returns a value: in this case, it returns a null, which is fil-\ntered out, and you use the ProcessorContext.forward method to send mul-\ntiple values downstream. If you wanted to return multiple values instead,\nyou’d return a List<KeyValue<K,V>> and then attach a flatMap or flat-\nMapValues to send individual records downstream. An example of this can be\nfound in src/main/java/bbejeck/chapter_6/StockPerformanceStreamsAnd-\nProcessorMultipleValuesApplication.java. To complete the replacement of a\nProcessor instance, you’d plug in the Transformer (or ValueTransformer)\ninstance using the KStream.transform or KStream.transformValues method.\n \n\n\n171\nSummary\nA great example of combining the KStream and Processor APIs can be found in\nsrc/main/java/bbejeck/chapter_6/StockPerformanceStreamsAndProcessorApplication\n.java. I didn’t present that example here because the logic is, for the most part, identi-\ncal to the StockPerformanceApplication example from section 6.3.1. You can look it\nup if you’re interested. Additionally, you’ll find a Processor API version of the original\nZMart application in src/main/java/bbejeck/chapter_6/ZMartProcessorApp.java. \nSummary\nThe Processor API gives you more flexibility at the cost of more code.\nAlthough the Processor API is more verbose than the Kafka Streams API, it’s\nstill easy to use, and the Processor API is what the Kafka Streams API, itself, uses\nunder the covers.\nWhen faced with a decision about which API to use, consider using the Kafka\nStreams API and integrating lower-level methods (process(), transform(),\ntransformValues()) when needed.\nAt this point in the book, we’ve covered how you can build applications with Kafka\nStreams. Our next step is to look at how you can optimally configure these applica-\ntions, monitor them for maximum performance, and spot potential issues.\n \n\n\nPart 3\nAdministering\nKafka Streams\nIn these chapters, we’ll shift focus to how you can measure the performance\nof your Kafka Streams application. Additionally, you’ll learn how to monitor and\ntest your Kafka Streams code so you know it’s working as expected and will grace-\nfully handle errors.\n \n\n\n175\nMonitoring and\nperformance\nSo far, you’ve learned how to build a Kafka Streams application from the bottom\nup. You’ve worked with the high-level Kafka Streams DSL, and you’ve seen the\npower of using a declarative API. You’ve also learned about the Processor API and\nhave seen how you can give up some convenience to gain more control in writing\nyour streaming applications.\n It’s now time to change gears a bit. You’re going to put on your forensic investi-\ngator hat and dig into your application from a different perspective. Your focus is\ngoing to shift from how you get things to work to what is going on. In some respects,\nthe initial building of an application is the most comfortable part. Getting the\napplication to run successfully, scale correctly, and work properly is always the more\nsignificant challenge. Despite your best efforts, there’s almost always a situation you\ndidn’t account for.\nThis chapter covers\nLooking at basic Kafka monitoring\nIntercepting messages\nMeasuring performance\nObserving the state of the application\n \n",
      "page_number": 182
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 192-203)",
      "start_page": 192,
      "end_page": 203,
      "detection_method": "topic_boundary",
      "content": "176\nCHAPTER 7\nMonitoring and performance\n In this chapter, you’ll learn how to check the running state of your Kafka Streams\napplication. You’ll see how you can measure the performance of the application in\norder to spot performance bottlenecks. You’ll also see techniques you can use to\nnotify you about various states of the application and to view the structure of the\ntopology. You’ll learn what metrics are available, how you can collect them, and how\nyou can observe the collected metrics as the application is running. Let’s start with\nmonitoring a Kafka Streams application.\n7.1\nBasic Kafka monitoring\nBecause the Kafka Streams API is a part of Kafka, it goes without saying that monitoring\nyour application will require some monitoring of Kafka as well. Full-blown surveillance\nof a Kafka cluster is a big topic, so we’ll limit our discussion of Kafka performance to\nwhere the two meet—we’ll talk about monitoring Kafka consumers and producers.\nMore information on monitoring a Kafka cluster can be found in the documentation\n(https://kafka.apache.org/documentation/#monitoring).\nNOTE\nOne thing I should note here is that to measure Kafka Streams perfor-\nmance, we also need to measure Kafka itself. At times, some of our coverage\nof performance will edge over to the Kafka side of things. But because this is a\nbook on Kafka Streams, we’ll focus on Kafka Streams.\n7.1.1\nMeasuring consumer and producer performance\nFor our discussion of consumer and producer performance, let’s start by looking at\nfigure 7.1, which depicts one of the fundamental performance concerns for a pro-\nducer and consumer. As you can see, producer and consumer performance are very\nsimilar in that both are concerned with throughput. But where we put the emphasis is\njust different enough that they can be considered two sides of the same coin.\nFor producers, we care mostly about how fast the producer is sending messages to the\nbroker. Obviously, the higher the throughput, the better.\n For consumers, we’re also concerned with performance, or how fast we can read\nmessages from a broker. But there’s another way to measure consumer performance:\nFigure 7.1\nKafka producer and consumer performance concerns, writing to and reading \nfrom a broker\nProducer\nConsumer\nMB per second consumed\nRecords per second consumed\nMB per second produced\nRecords per second produced\nKafka broker\n \n\n\n177\nBasic Kafka monitoring\nconsumer lag. Take a look at figure 7.2. This measures producer and consumer through-\nput with a slightly different focus.\nYou can see that we care about how much and how fast our producers can publish to a\nbroker, and we simultaneously care about how quickly our consumers can read those\nmessages from the broker. The difference between how fast the producers place records\non the broker and when consumers read those messages is called consumer lag.\n Figure 7.3 illustrates that consumer lag is the difference between the last commit-\nted offset from the consumer and the last offset from a message written to the broker.\nThere’s bound to be some lag from the consumer, but ideally the consumer will catch\nup, or at least have a consistent lag rather than a gradually increasing lag\nFigure 7.2\nKafka producer and consumer performance revisited\nProducer\nConsumer\nHow fast are records consumed\nfrom topic A?\nHow fast are records being\nproduced to topic A?\nKafka broker\nDoes the consumer keep up with the rate of records coming from the producer?\nFigure 7.3\nConsumer lag is the difference in offsets committed by the consumer and offsets \nwritten by the producer\nProducer\nConsumer\nLast message consumed\nat offset 994\nLast offset produced\nLast offset consumed\nThe difference between the most recent offset produced and the last\noffset consumed (from the same topic) is known as consumer lag.\nIn this case, the consumer lags behind the producer by six records.\nMost recent message produced\nhas an offset of 1000\n995\n994\n996\n997\n998\n999\n1000\n1000\n994\n \n\n\n178\nCHAPTER 7\nMonitoring and performance\nNow that we’ve defined our performance parameters for producers and consumers,\nlet’s see how we can monitor them for performance and troubleshooting issues. \n7.1.2\nChecking for consumer lag\nTo check for consumer lag, Kafka provides a convenient command-line tool, kafka-\nconsumer-groups.sh, found in the <kafka-install-dir>/bin directory. The script has a\nfew options, but here we’ll focus on the list and describe options. These two\noptions will give you the information you need about consumer group performance.\n First, use the list command to find all active consumer groups. Figure 7.4 shows\nthe results of running this command.\n<kafka-install-dir>/bin/kafka-consumer-groups.sh \\\n--bootstrap-server localhost:9092 \\\n--list\nWith this information, you can choose a consumer group name and run the following\ncommand:\n<kafka-install-dir>/bin/kafka-consumer-groups.sh \\\n--bootstrap-server localhost:9092 \\\n--group <GROUP-NAME> \\\n--describe\nFigure 7.5 shows the results: the status of how this consumer is performing.\nThese results show that you have a small consumer lag. Having a consumer lag isn’t\nalways indicative of a problem—consumers read messages in batches and won’t\nretrieve another batch until they’re finished processing the current batch. Processing\nthe records takes time, so a little lag is not entirely surprising.\nFigure 7.4\nListing available consumer groups from the command line\nFigure 7.5\nStatus of a consumer group\nNumber of messages read = 3\nNumber of messages sent to topic =\n0\n1\n10 (messages sent) – 3 (messages read) = 7 (lag, or records behind)\n \n\n\n179\nBasic Kafka monitoring\n A small lag or one that stays constant is OK, but a lag that continues to grow over\ntime is an indication you’ll need to give your consumer more resources. For example,\nyou might need to increase the partition count and hence increase the number of\nthreads consuming from the topic. Or maybe your processing after reading the mes-\nsage is too heavyweight. After consuming a message, you could hand it off to an async\nqueue, where another thread can pick up the message and do the processing.\n In this section, you’ve learned how to determine how quickly a consumer is read-\ning messages from a broker. Next, we’ll dig a little deeper into observing behavior for\ndebugging purposes—you’ll see how to intercept what the producers are sending and\nconsumers are receiving before your Kafka Streams application sends or consumes\nrecords. \n7.1.3\nIntercepting the producer and consumer\nEarly in 2016, Kafka Improvement Proposal 42 (KIP-42) introduced the ability to\nmonitor or “intercept” information on client (consumer and producer) behavior. The\ngoal of the KIP was to provide “the ability to quickly deploy tools to observe, measure,\nand monitor Kafka client behavior, down to the message level.”1\n Although interceptors aren’t typically your first line for debugging, they can prove\nuseful in observing the behavior of your Kafka streaming application, and they’re a\nvaluable addition to your toolbox. An excellent example of using an interceptor (pro-\nducer) is using one to keep track of the message offsets your Kafka Streams applica-\ntion is producing back to Kafka.\nNOTE\nBecause Kafka Streams can consume or produce any number of key\nand value types, the internal Consumer and Producer are configured to work\nwith byte[] keys and byte[] values; hence, they always handle unserialized\ndata. Serialized data means you can’t inspect messages without an extra dese-\nrialization/serialization step.\nLet’s get started by discussing the consumer interceptor.\nCONSUMER INTERCEPTOR\nThe consumer interceptor gives you two access points to intercept. The first is Consumer-\nInterceptor.onConsume(), which reads ConsumerRecords between the point where\nthey’re retrieved from the broker, and before the messages are returned from the\nConsumer.poll() method. The following pseudocode will give you an idea of where\nthe consumer interceptor is doing its work:\nConsumerRecords<String, String> poll(long timeout) {\nConsumerRecords<String, String> consumerRecords =\n➥ ...consuming records\nreturn interceptors.onConsume(consumerRecords);\n1 Apache Kafka, “KIP-42: Add Producer and Consumer Interceptors,” http://mng.bz/g8oX.\nFetches new records \nfrom the broker\nRuns records through the \ninterceptor chain and \nreturns the results\n \n\n\n180\nCHAPTER 7\nMonitoring and performance\nAlthough this pseudocode bears no resemblance to the actual KafkaConsumer code, it\nillustrates the point. Interceptors accept the ConsumerRecords returned from the bro-\nker inside the Consumer.poll() method and have the opportunity to perform any\noperation, including filtering or modification, before the KafkaConsumer returns the\nrecords from the poll method.\n ConsumerInterceptors are specified via ConsumerConfig.INTERCEPTOR_CLASSES\n_CONFIG with a Collection of one or more ConsumerInterceptor implementor\nclasses. Multiple interceptors are chained together and executed in the order speci-\nfied in the configuration.\n A ConsumerInterceptor accepts and returns a ConsumerRecords instance. If there\nare multiple interceptors, the returned ConsumerRecords from one interceptor serves\nas the input parameter for the next interceptor in the chain. Thus, any modifications\nmade by one interceptor are propagated to the next interceptor in the chain.\n Exception handling is an important consideration when chaining multiple inter-\nceptors together. If an Exception occurs in an interceptor, it logs the error, but it\ndoesn’t short-circuit the chain. Thus, ConsumerRecords continues to work its way\nthrough the remaining interceptors.\n For example, suppose you have three interceptors: A, B, and C. All three modify\nthe records and rely on changes made by the previous interceptor in the chain. But if\ninterceptor A encounters an error, the ConsumerRecords object continues to intercep-\ntors B and C, but without the expected modifications, rendering the results from the\ninterceptor chain invalid. For this reason, it’s best not to have an interceptor rely on\nConsumerRecords modified by a previous interceptor in the chain.\n The second interception point is the ConsumerInterceptor.onCommit() method.\nAfter the consumer commits its offsets to the broker, the broker returns a Map<Topic-\nPartition, OffsetAndMetadata> containing information with the topic, partition,\nand committed offsets, along with associated metadata (time of commit, and so on).\nThe commit information can be useful for tracking purposes. Here’s an example of a\nsimple ConsumerInterceptor used for logging purposes (found in src/main/java/\nbbejeck/chapter_7/interceptors/StockTransactionConsumerInterceptor.java).\npublic class StockTransactionConsumerInterceptor implements\n➥ ConsumerInterceptor<Object, Object> {\n// some details left out for clarity\nprivate static final Logger LOG =\n➥ LoggerFactory.getLogger(StockTransactionConsumerInterceptor.class);\npublic StockTransactionConsumerInterceptor() {\nLOG.info(\"Built StockTransactionConsumerInterceptor\");\n}\n@Override\npublic ConsumerRecords<Object, Object>\n➥ (ConsumerRecords<Object, Object> consumerRecords) {\nListing 7.1\nLogging consumer interceptor\n \n\n\n181\nBasic Kafka monitoring\nLOG.info(\"Intercepted ConsumerRecords {}\",\nbuildMessage(consumerRecords.iterator()));\nreturn consumerRecords;\n}\n@Override\npublic void onCommit(Map<TopicPartition, OffsetAndMetadata> map) {\nLOG.info(\"Commit information {}\",\nmap);\n}\nNow let’s cover the producing side of intercepting.\nPRODUCER INTERCEPTOR\nThe ProducerInterceptor works similarly and has two access points: Producer-\nInterceptor.onSend() and ProducerInterceptor.onAcknowledgement(). With the\nonSend method, the interceptor can perform any action, including mutating the\nProducerRecord. Each producer interceptor in the chain receives the returned object\nfrom the previous interceptor.\n Exception handling is the same as on the consumer side, so the same caveats apply\nhere as well. The ProducerInterceptor.onAcknowledgement() method is called when\nthe broker acknowledges the record. If sending the record fails, onAcknowledgement\nis called at that point as well.\n Here’s a simple logging ProducerInterceptor example (found in src/main/java/\nbbejeck/chapter_7/interceptors/ZMartProducerInterceptor.java).\npublic class ZMartProducerInterceptor implements\n➥ ProducerInterceptor<Object, Object> {\n// some details left out for clarity\nprivate static final Logger LOG =\n➥ LoggerFactory.getLogger(ZMartProducerInterceptor.class);\n@Override\npublic ProducerRecord<Object, Object> onSend(ProducerRecord<Object,\n➥ Object> record) {\nLOG.info(\"ProducerRecord being sent out {} \", record);\nreturn record;\n}\n@Override\npublic void onAcknowledgement(RecordMetadata metadata,Exception exception) {\nif (exception != null) {\nLOG.warn(\"Exception encountered producing record {}\",\n➥ exception);\n} else {\nLOG.info(\"record has been acknowledged {} \", metadata);\n}\n}\nListing 7.2\nLogging producer interceptor\nLogs the consumer records and metadata\nbefore the records are processed\nLogs the commit information once\nthe Kafka Streams consumer\ncommits offsets to the broker\nLogs right before \nthe message is sent \nto the broker\nLogs broker \nacknowledgement \nor whether error \noccurred (broker-\nside) during the \nproduce phase\n \n\n\n182\nCHAPTER 7\nMonitoring and performance\nThe ProducerInterceptor is specified with ProducerConfig.INTERCEPTOR_CLASSES\n_CONFIG and takes a Collection of one or more ProducerInterceptor classes.\nTIP\nWhen configuring interceptors in a Kafka Streams application, you need\nto prefix the consumer and producer interceptors’ property names with\nprops.put(StreamsConfig.consumerPrefix(ConsumerConfig.INTERCEPTOR\n_CLASSES_CONFIG) and StreamsConfig.producerPrefix(ProducerConfig\n.INTERCEPTOR_CLASSES_CONFIG), respectively.\nIf you want to see the interceptors in action, src/main/java/bbejeck/chapter_7/\nStockPerformanceStreamsAndProcessorMetricsApplication.java uses a consumer inter-\nceptor, and src/main/java/bbejeck/chapter_7/ZMartKafkaStreamsAd-vancedReqs-\nMetricsApp.java uses a producer interceptor. Both classes include the configuration\nrequired for using interceptors.\n As a side note, because interceptors work on every record in the Kafka Streams\napplication, the output of the logging interceptors is significant. The interceptor\nresults are output to consumer_interceptor.log and producer_interceptor.log, found\nin the logs directory at the base of the source code installation.\n We’ve spent some time looking at metrics on consumer performance and how you\ncan intercept records coming into and out of a Kafka Streams application. But this\ninformation is coarse grained and outside of a Kafka Streams application. Let’s now go\ninside a Kafka Streams application and see what’s going on under the hood. The next\nstep is to measure performance inside the topology by gathering metrics. \n7.2\nApplication metrics\nWhen it comes to measuring the performance of an application, you can get a sense\nof how long it takes to process one record, and measuring end-to-end latency is\nundoubtedly a good indicator of overall performance. But if you want to improve per-\nformance, you’ll need to know exactly where things are slowing down.\n Measuring performance is essential for streaming applications. The mere fact\nthat you’re using a streaming application implies you want to process data or infor-\nmation as it becomes available. It stands to reason that if your business needs dictate\na streaming solution, you’ll want the most efficient and correct streaming process you\ncan get.\n Before we discuss the actual metrics we’ll focus on, let’s revisit one of the applica-\ntions you built in chapter 3, the advanced ZMart application. That app is a good can-\ndidate for metrics tracking because there are several processing nodes, so we’ll use\nthat topology for this example. Figure 7.6 shows the topology you created.\n \n \n \n \n \n\n\n183\nApplication metrics\nKeeping the ZMart topology in mind, let’s take a look at the categories of metrics:\nThread metrics\n– Average time for commits, poll, process operations\n– Tasks created per second, tasked closed per second\nTask metrics\n– Average number of commits per second\n– Average commit time\nProcessor node metrics\n– Average and max processing time\n– Average number of process operations per second\n– Forward rate\nState store metrics\n– Average execution time for put, get, and flush operations\n– Average number put, get, and flush operations per second\nNote that this isn’t an exhaustive list of the possible metrics. I’ve chosen these because\nthey offer excellent coverage of the most common performance scenarios. You can\nfind a full list on the Confluent website: http://mng.bz/4bcA.\nFigure 7.6\nZMart advanced application topology with a lot of nodes\nPatterns\nMasking\nSource\nElectronics\nsink\nCafe\nsink\nPatterns\nsink\nPurchases\nsink\nRewards\nBranch\nprocessor\nFiltering\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nRewards\nsink\nSelect-key\nprocessor\n \n\n\n184\nCHAPTER 7\nMonitoring and performance\n Now that we have what we’re going to measure, let’s look at how to capture the\ninformation.\n7.2.1\nMetrics configuration\nKafka Streams already provides the mechanism for collecting performance metrics.\nFor the most part, you just need to provide some configuration values. Because the\ncollection of metrics does incur a performance cost, there are two levels, INFO and\nDEBUG. An individual metric may not be that expensive on its own, but when you con-\nsider that some metrics may involve every record flowing through the Kafka Streams\napplication, you can see how the impact on performance can add up.\n The metric levels are used like logging levels. When you’re troubleshooting an\nissue or observing how the application behaves, you’ll want more information, so you\ncan use the DEBUG level. Other times, you don’t need all the information, so you can\nuse the INFO level.\n Typically, you won’t want to use DEBUG in production, as the cost of performance\nwould be too high. Each of the previously listed metrics are available at different levels,\nas shown in table 7.1. As you can see, thread metrics are available at any level, whereas\nthe rest of the metric categories are only collected when using the DEBUG level.\nYou set the level when you set the configuration of your Kafka Streams application.\nThat setting has been there all along with your other application configurations;\nyou’ve accepted the default settings up to this point. The default level of metrics col-\nlection is INFO.\n Let’s update the configs in the advanced ZMart application and turn on the collec-\ntion of all metrics (src/main/java/bbejeck/chapter_7/ZMartKafkaStreamsAdvanced-\nReqsMetricsApp.java).\nprivate static Properties getProperties() {\nProperties props = new Properties();\nprops.put(StreamsConfig.CLIENT_ID_CONFIG,\n➥ \"metrics-client-id\");\nTable 7.1\nMetrics availability by levels\nMetrics category\nDEBUG\nINFO\nThread\nx\nx\nTask\nx\nProcessor node\nx\nState store\nx\nRecord cache\nx\nListing 7.3\nUpdating the configs for DEBUG metrics\nClient ID\n \n\n\n185\nApplication metrics\nprops.put(ConsumerConfig.GROUP_ID_CONFIG,\n➥ \"metrics-group-id\");\nprops.put(StreamsConfig.APPLICATION_ID_CONFIG,\n➥ \"metrics-app-id\");\nprops.put(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG,\n➥ \"DEBUG\");\nprops.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,\n➥ \"localhost:9092\");\nreturn props;\n}\nYou’ve now enabled the collection and recording of DEBUG-level metrics. The key\npoints I want you to remember from this section is there are built-in metrics to mea-\nsure the full scope of a Kafka Steams application, and that you should carefully con-\nsider the performance impact before turning on metrics collection at the DEBUG level.\n Now that we’ve discussed what metrics are available and how they’re collected, the\nnext step is to observe the collected metrics. \n7.2.2\nHow to hook into the collected metrics\nThe metrics in a Kafka Streams application are collected and distributed to metrics\nreporters. As you might have guessed, Kafka Streams provides a default reporter via\nJava Management Extensions (JMX).\n Once you’ve enabled collecting metrics at the DEBUG level, you have nothing left to\ndo but observe them. One thing to keep in mind is that JMX only works with live run-\nning applications, so the metrics we’ll look at will be when the application is running.\nTIP\nYou can also access metrics programmatically. For an example of pro-\ngrammatic metrics access, take a look at src/main/java/bbejeck/chapter_7/\nStockPerformanceStreamsAndProcessorMetricsApplication.java.\nYou’re likely familiar with using JMX or have at least heard of it. In the next section,\nI’ll provide a brief overview on how to get started using JMX, but if you’re an experi-\nenced JMX user, feel free to skip this next section. \n7.2.3\nUsing JMX\nJMX is a standard way of viewing the behavior of programs running on the Java VM.\nYou can also use JMX to see how the Java Virtual Machine (Java VM) is performing. In\na nutshell, JMX gives you the infrastructure to expose parts of your running program.\n Fortunately, you won’t need to write any code to do this monitoring. You’ll just\nconnect either Java VisualVM (http://mng.bz/euif), JConsole (http://mng.bz/Ea71),\nor Java Mission Control (http://mng.bz/0r5B).\n \n \n \nGroup ID\nApplication ID\nSets the metrics \nrecording level \nto DEBUG\nSets the connection \nfor the brokers\n \n\n\n186\nCHAPTER 7\nMonitoring and performance\nTIP\nJava Mission Control (JMC) is powerful and can be a great tool for moni-\ntoring, but it requires a commercial license for use in production. Because\nJMC ships with the JDK, you can start JMC directly from the command line\nwith the command jmc (assuming the JDK bin directory is on your path).\nAdditionally, you’ll need to add these flags when starting your Kafka stream-\ning application: -XX:+UnlockCommercialFeatures -XX:+FlightRecorder.\nAs JConsole is the most straightforward approach, we’ll start with it for now.\nSTARTING JCONSOLE\nJConsole ships with the JDK, so if you’ve got Java installed, you already have JConsole.\nStarting JConsole is as simple as running jconsole from a command prompt (assum-\ning Java is on your path). Once it’s started, a GUI will come up, as shown in figure 7.7.\nOnce JConsole is up, the next step is to use it to look at some metric data!\nSTARTING TO MONITOR A RUNNING PROGRAM\nIf you look at the center of the JConsole GUI, you’ll see a New Connection dialog box.\nFigure 7.8 shows the starting point for JConsole. For now, we’re only concerned with\nthe Java processes listed in the Local Process section.\nNOTE\nYou can use JConsole to monitor remote applications, and you can\nsecure access to JMX. You can see the Remote Process, Username, and Pass-\nword text boxes in figure 7.8. In this book, however, we’ll limit our discussion\nto local access during development. The internet is full of instructions on\nremote and secure JConsole access, and Oracle’s documentation is a great\nstarting point (http://mng.bz/Ea71).\nWhat is JMX?\nOracle says the following in “Lesson: Overview of the JMX Technology” http://mng\n.bz/Ej29):\nThe Java Management Extensions (JMX) technology is a standard part of the Java\nPlatform, Standard Edition (Java SE platform), added to the platform in the 5.0\nrelease.\nThe JMX technology provides a simple, standard way of managing resources such\nas applications, devices, and services. Because the JMX technology is dynamic, you\ncan use it to monitor and control resources as they are created, installed, and imple-\nmented. You can also use the JMX technology to monitor and manage the Java Vir-\ntual Machine (Java VM).\nThe JMX specification defines the architecture, design patterns, APIs, and services\nin the Java programming language for management and monitoring of applications\nand networks.\n \n\n\n187\nApplication metrics\nFigure 7.7\nJConsole start menu\nFigure 7.8\nJConsole connect to program\nSelect Connection > New Connection to\nrefresh the Local Process dialog below.\nSelect the application\nprocess from the\ndialog here.\n \n",
      "page_number": 192
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 204-212)",
      "start_page": 204,
      "end_page": 212,
      "detection_method": "topic_boundary",
      "content": "188\nCHAPTER 7\nMonitoring and performance\nIf you haven’t already started your Kafka Streams application, do so now. Then, to get\nyour application to show up in the Local Process window, click Connection > New\nConnection (as in figure 7.8). The processes listed under Local Process should refresh,\nand you’ll see your Kafka Streams application. Double-click the Kafka Streams applica-\ntion process.\n Chances are that after you double-click the program you want to connect to, you’ll\nbe greeted with a warning similar to the one in figure 7.9. Because you’re on your\nlocal machine, you can click the Insecure Connection button.\nNow you’re all set to look at the metrics being collected by your Kafka Streams appli-\ncation. The next step is to look at the information available.\nWARNING\nYou’re using an insecure connection for development on a local\nmachine. In practice, you should always secure access to any remote services\naccessing the internal state of your application. \nVIEWING THE INFORMATION\nOnce you’re connected, you’ll see a GUI screen looking something like figure 7.10.\nJConsole offers several handy options for peeking inside the internals of running\napplications.\n Of the Overview, Memory, Threads, Classes, VM Summary, and MBeans tabs, you’re\nonly going use the MBeans tab. MBeans contain the collected statistics about the per-\nformance of your Kafka Streams program. The other tabs provide relevant informa-\ntion, but it’s info that relates more to overall application health and how the program\nFigure 7.9\nJConsole connect warning, no SSL\nYou’re running on your local development\nmachine, so select this button.\n \n\n\n189\nApplication metrics\nis utilizing resources. The metrics collected in the MBeans contain information about\nthe internal performance of the topology.\n That’s the end of your introduction to using JConsole. The next step is to start\nviewing the recorded metrics for the topology. \n7.2.4\nViewing metrics\nFigure 7.11 shows how you can view some metrics via JConsole while running the\nZMart application (src/main/java/bbejeck/chapter_7/ZMartKafkaStreamsAdvanced-\nReqsMetricsApp.java). As you can see, you can drill down to all processors and nodes\nin the topology to view performance (either throughput or latency).\nTIP\nBecause JMX only works with running applications, some of the example\napplications in src/main/java/bbejeck/chapter_7 will run continuously so\nthat you can play with the metrics. As a result, you’ll need to explicitly stop\nthem either in the IDE or by pressing Ctrl-C from the command line.\nFigure 7.10\nJConsole started\nSelect the MBeans tab. You could use the other\ntabs to perform other monitoring tasks, but you\nwant to look at your collected metrics.\nYou’re going to use the metrics found\nunder the kafka.streams folder.\nHere, you can see a good selection\nof MBeans for different metrics.\n \n\n\n190\nCHAPTER 7\nMonitoring and performance\nFigure 7.11 shows the process-rate metric, which tells you the average number of\nrecords processed per millisecond. If you look at the upper right, under Attribute\nValue, you can see that the process rate averages 3.537 records per millisecond (3,537\nrecords per second). Additionally, as discussed earlier, you can see the producer and\nconsumer metrics from JConsole.\nTIP\nAlthough the provided metrics are comprehensive, there may be cases\nwhere you want custom metrics. This is a low-level detail and probably not a\nFigure 7.11\nJConsole metrics for ZMart\nHere’s the\nprocess-rate value.\nAll the nodes in the topology\nwith one of the branch\nprocessors expanded\n \n\n\n191\nMore Kafka Streams debugging techniques\nvery common use case, so we won’t walk through an example in detail. But\nyou can look at the StockPerformanceMetricsTransformer.init method for\nan example of how you can add a custom metric and the StockPerformance-\nMetricsTransformer.transform method for an example of how you can uti-\nlize it. The StockPerformanceMetricsTransformer is found in src/main/\njava/bbejeck/chapter_7/transformer/StockPerformanceMetrics-Transformer\n.java.\nNow that you’ve seen how to view Kafka Streams metrics, let’s move on to other useful\ntechniques for observing what’s going on in an application. \n7.3\nMore Kafka Streams debugging techniques\nWe’ll now look at some more ways you can observe and debug Kafka streaming appli-\ncations. The previous section was more about performance; the techniques we’ll look\nat in this section focus on getting notified about various states of the application and\nviewing the structure of the topology.\n7.3.1\nViewing a representation of the application\nAfter your application is up and running, you might run into situations where you\nneed to debug it. You might like to get a second pair of eyes on the job, but for what-\never reason, you can’t share the code. Or you’d like to see the TopicPartition\nassigned to the tasks of the application.\n The Topology.describe() method provides general information on the structure\nof the application. It prints out information regarding the structure of the program,\nincluding any internal topics created to support repartitioning. Figure 7.12 displays\nFigure 7.12\nDisplaying node names, associated child nodes, and other info\nStreamsTask taskId: 0_0\nProcessorTopology:\nTxn-Source:\ntopics: [stock-transactions]\nchildren: [Txn-Processor]\nTxn-Processor:\nchildren: [CoGrouping-Processor]\nCoGrouping-Processor:\nstates: [tupleCoGroupStore]\nchildren: [Tuple-Sink, Print]\nTuple-Sink:\ntopic: cogrouped-results\nPrint:\nEvents-Source:\ntopics: [events]\nchildren: [Events-Processor]\nEvents-Processor:\nchildren: [CoGrouping-Processor]\nCoGrouping-Processor:\nstates: [tupleCoGroupStore]\nchildren: [Tuple-Sink, Print]\nTuple-Sink:\ntopic: cogrouped-results\nPrint:\nProcessor name\nAssociated state store\nChild nodes\nProcessor name\nTopic name\nThe taskId\n \n\n\n192\nCHAPTER 7\nMonitoring and performance\nthe results of calling describe on the CoGroupingListeningExampleApplication\nfrom chapter 7 (src/main/java/bbejeck/chapter_7/CoGroupingListeningExample-\nApplication.java).\n As you can see, the Topology.describe() method prints out a nice, concise\nview of the application structure. Notice that the CoGroupingListeningExample-\nApplication used the Processor API, so all the nodes in the topology have the names\nyou chose. With the Kafka Streams API, the names of the nodes are a little more\ngeneric:\nKSTREAM-SOURCE-0000000000:\ntopics:\n[transactions]\nchildren:\n[KSTREAM-MAPVALUES-0000000001]\nTIP\nWhen you’re using the Kafka Streams DSL API, you don’t directly use\nthe Topology class, but it’s easily accessed. If you want to print the physical\ntopology of the application, use the StreamsBuilder.build() method, which\nreturns a Topology object, and then you can call Topology.describe() as\nyou just saw.\nGetting information about the StreamThread objects, which shows runtime informa-\ntion, in your application can be useful as well. To access the StreamThread info, use\nthe KafkaStreams.localThreadsMetadata() method. \n7.3.2\nGetting notification on various states of the application\nWhen you start your Kafka Streams application, it doesn’t automatically begin process-\ning data—some coordination has to happen first. The consumer needs to fetch metadata\nand subscription information; the application needs to start StreamThread instances and\nassign TopicPartitions to StreamTasks.\n This process of assigning or redistributing tasks (workload) is called rebalancing.\nRebalancing means Kafka Streams can autoscale up or down. This is a crucial strength—\nyou can add new application instances while an existing application is already running,\nand the rebalancing process will redistribute the workload.\n For example, suppose you have a Kafka Streams application with two source topics,\nand each topic has two partitions, resulting in four TopicPartition objects needing\nassignment. You initially start the application with one thread. Kafka Streams deter-\nmines the number of tasks to create by taking the max partition size among all input\ntopics. In this case, each topic has two partitions, so the max is two, and you end up\nwith two tasks. The rebalance process then assigns the two tasks two TopicPartition\nobjects each.\n After running the app for a little while, you decide you want to process records\nmore quickly. All you need to do is start another version of the application with the\nsame application ID, and the rebalance process will distribute the load across the new\napplication thread, resulting in the two tasks being assigned across both threads.\n \n\n\n193\nMore Kafka Streams debugging techniques\nYou’ve just doubled the scale of your application while the original version is still run-\nning—there’s no need to shut the initial application down.\n Other causes of rebalancing include another Kafka Streams instance (with the same\napplication ID) starting or stopping, adding partitions to a topic, or, in the case of a\nregex-defined source node, adding or removing topics matching the regex pattern.\n During the rebalance phase, external interaction temporarily pauses until the\napplication has completed the assignment of topic partitions to stream tasks, so you’d\nlike to be aware of this point in the lifecycle of the application. For example, the que-\nryable state stores are unavailable, so you’d like to be able to restrict requests to view\nthe contents of the store until the stores are available again.\n But how can you check whether your other applications are going through a rebal-\nance? Fortunately, Kafka Streams provides just such a mechanism, the StateListener,\nwhich we’ll look at next. \n7.3.3\nUsing the StateListener\nA Kafka Streams application can be in one of six states at any point in time. Figure 7.13\nshows the possible valid states for a Kafka Streams application. As you can see, there\nare a few state-change scenarios we could discuss, but we’re going to focus on the tran-\nsition between running and rebalancing. The transition between these two states is\nthe most frequent and has the most impact on performance, because during the\nrebalancing phase no processing occurs.\nTo capture these state changes, you’ll use the KafkaStreams.setStateListener\nmethod, which takes an instance of the StateListener interface. It’s a single-method\nFigure 7.13\nPossible states of a Kafka Streams application\nCreated\nRebalancing\nRunning\nError\nPending shutdown\nNot running\nSix states of a Kafka Streams application\nThe arrows show the\ndirections of valid\ntransitions.\nOnly the pending-shutdown state\ncan go into the not-running state.\nOnly the running and\nrebalancing states can\ngo into an error state.\n \n\n\n194\nCHAPTER 7\nMonitoring and performance\ninterface, so you can use Java 8 lambda syntax, as follows (found in src/main/java/\nbbejeck/chapter_7/ZMartKafkaStreamsAdvancedReqsMetricsApp.java).\nKafkaStreams.StateListener stateListener = (newState, oldState) -> {\nif (newState == KafkaStreams.State.RUNNING &&\n➥ oldState == KafkaStreams.State.REBALANCING) {\nLOG.info(\"Application has gone from REBALANCING to RUNNING \");\nLOG.info(\"Topology Layout {}\",\n➥ streamsBuilder.build().describe());\n}\n};\nTIP\nListing 7.4, running ZMartKafkaStreamsAdvancedReqsMetricsApp.java,\ninvolves viewing JMX metrics and the state-transition notification, so I’ve\nturned off printing the streaming results to the console. You’re writing solely\nto Kafka topics. When you run the app, you should see the listener output in\nthe console.\nFor your first StateListener implementation, you’ll log the state change to the con-\nsole. In section 7.3.1, when we discussed printing the topology structure, I spoke of\nneeding to wait until the application has completed rebalancing. That’s what you do in\nthe listing 7.4: print out the structure once all tasks and assignments are complete.\n Let’s take this example a little further and show how to signal when the application\nis going into a rebalancing state. You can update your code to handle this additional\nstate transition as follows (found in src/main/java/bbejeck/chapter_7/ZMartKafka-\nStreamsAdvancedReqsMetricsApp.java).\nKafkaStreams.StateListener stateListener = (newState, oldState) -> {\nif (newState == KafkaStreams.State.RUNNING &&\n➥ oldState == KafkaStreams.State.REBALANCING) {\nLOG.info(\"Application has gone from REBALANCING to RUNNING \");\nLOG.info(\"Topology Layout {}\", streamsBuilder.build().describe());\n}\nif (newState == KafkaStreams.State.REBALANCING) {\nLOG.info(\"Application is entering REBALANCING phase\");\n}\n};\nEven though you’re using simple logging statements, it should be evident how you can\nadd more-sophisticated logic to handle the state changes within your application.\nListing 7.4\nAdding a state listener\nListing 7.5\nUpdating the state listener when REBALANCING\nChecks that you’re\ntransitioning from\nREBALANCING to\nRUNNING\nPrints out the structure\nof the topology\nAdds an action when entering\nthe rebalancing phase\n \n\n\n195\nMore Kafka Streams debugging techniques\nNOTE\nBecause Kafka Streams is a library and not a framework, you can run a\nsingle instance on one server. If you do run multiple applications on different\nmachines, you’ll only see results from state changes on your local machine.\nThe critical point of this section is that you can hook into the current state of your\nKafka Streams application, making it less of a black-box operation.\n Next, we’ll look at rebalancing in a little more in depth. Although the ability to\nautomatically rebalance the workload is a strength of Kafka Streams, you’ll likely want\nto keep the number of rebalances to a minimum. When a rebalance occurs, you’re\nnot processing data, and you’d like to have your application processing data as much\nas possible. \n7.3.4\nState restore listener\nYou learned in chapter 4 about state stores and the importance of having your state\nstores backed up, in case of a failure. In Kafka Streams, we use topics frequently called\nchangelogs as the backup for the state stores.\n The changelog records the updates to the state store as the update occurs. When a\nKafka Streams application fails, or you restart it, the state store can recover from the\nlocal state files, as shown in figure 7.14.\nIn some circumstances, however, you may need to do a full recovery of state store from\nthe changelog, such as if you’re running your Kafka Streams application in a stateless\nenvironment like Mesos, or if you encounter a severe failure and the files on local disk\nFigure 7.14\nRestoring a state store from clean start/recovery\nProcessor\nState\nstore\nRecords in the changelog are\nconsumed on startup and used\nto restore the state store.\nOn a clean startup with no persisted\nlocal state, the state is fully restored from\nthe backing topic or changelog.\nBacking changelog\nState store restoration\n \n\n\n196\nCHAPTER 7\nMonitoring and performance\nare wiped out. Depending on the amount of data you have to restore, this restoration\nprocess could take a non-trivial amount of time.\n During this recovery period, any state stores you have exposed for querying are\nunavailable, so it would be nice to get an idea of how long this restoration process is\nlikely to take and how progress is coming along. Additionally, if you have a custom\nstate store, you’d like notification of when the restore is starting and ending so you\ncan do any necessary setup or teardown tasks.\n The StateRestoreListener interface, much like the StateListener, allows notifi-\ncation of what’s going on inside the application. StateRestoreListener has three meth-\nods: onRestoreStart, onBatchRestored, and onRestoreEnd. The KafkaStreams.set-\nGlobalRestoreListener method is used to specify the global restore listener to use.\nNOTE\nThe provided StateRestoreListener is shared application-wide and is\nexpected to be stateless. If you need to keep track of any state in the listener,\nyou’ll need to provide the synchronization.\nLet’s walk through the listener code to get an idea of how this notification process can\nwork. We’ll start with the declaration of the variable and the onRestoreStart method\n(found in src/main/java/bbejeck/chapter_7/restore/LoggingStateRestoreListener\n.java).\npublic class LoggingStateRestoreListener implements StateRestoreListener {\nprivate static final Logger LOG =\n➥ LoggerFactory.getLogger(LoggingStateRestoreListener.class);\nprivate final Map<TopicPartition, Long> totalToRestore =\n➥ new ConcurrentHashMap<>();\nprivate final Map<TopicPartition, Long> restoredSoFar =\n➥ new ConcurrentHashMap<>();\n@Override\npublic void onRestoreStart(TopicPartition topicPartition,\n➥ String store, long start, long end) {\nlong toRestore = end - start;\ntotalToRestore.put(topicPartition, toRestore);\nLOG.info(\"Starting restoration for {} on topic-partition {}\n➥ total to restore {}\", store, topicPartition, toRestore);\n}\n// other methods left out for clarity covered below\n}\nYour first steps are to create two ConcurrentHashMap instances for keeping track of\nrestoration progress. In the onRestoreStart method, you store the total number of\nrecords you need to restore and log the fact that you’re starting.\nListing 7.6\nA logging restore listener\nCreates Concurrent-\nHashMap instances \nfor keeping track of \nrestore progress\nStores the total \namount to restore \nfor the given \nTopicPartition\n \n",
      "page_number": 204
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 213-220)",
      "start_page": 213,
      "end_page": 220,
      "detection_method": "topic_boundary",
      "content": "197\nMore Kafka Streams debugging techniques\n Next, let’s move on to the code that handles each batch restored (found in\nsrc/main/java/bbejeck/chapter_7/restore/LoggingStateRestoreListener.java).\n@Override\npublic void onBatchRestored(TopicPartition topicPartition,\n➥ String store, long start, long batchCompleted) {\nNumberFormat formatter = new DecimalFormat(\"#.##\");\nlong currentProgress = batchCompleted +\n➥ restoredSoFar.getOrDefault(topicPartition, 0L);\ndouble percentComplete =\n➥ (double) currentProgress / totalToRestore.get(topicPartition);\nLOG.info(\"Completed {} for {}% of total restoration for {} on {}\",\nbatchCompleted,\n➥ formatter.format(percentComplete * 100.00),\n➥ store, topicPartition);\nrestoredSoFar.put(topicPartition, currentProgress);\n}\nThe restoration process uses an internal consumer to read the changelog topic, so it\nfollows that the application restores records in batches from each consumer.poll()\nmethod call. As a consequence, the maximum size of any batch will be equal to the\nmax.poll.records setting.\n The onBatchRestored method is called after the restore process has loaded the lat-\nest batch into the state store. First, you add the size of the current batch to the accumu-\nlated restore count. Then, you calculate the percentage of restoration completed and\nlog the results. Finally, you store the new total number of records, computed earlier.\n The last step we’ll cover is when the restoration process completes (found in\nsrc/main/java/bbejeck/chapter_7/restore/LoggingStateRestoreListener.java).\n@Override\npublic void onRestoreEnd(TopicPartition topicPartition,\n➥ String store, long totalRestored) {\nLOG.info(\"Restoration completed for {} on\n➥ topic-partition {}\", store, topicPartition);\nrestoredSoFar.put(topicPartition, 0L);\n}\nOnce the application completes the recovery process, you make one final call to the\nlistener with the total number of records restored. In this example, you log the fin-\nished state and update the full restoration count map to 0.\n Finally, you can use the LoggingStateRestoreListener in your application as\nfollows (found in src/main/java/bbejeck/chapter_7/CoGroupingListeningExample-\nApplication.java).\nListing 7.7\nHandling onBatchRestored\nListing 7.8\nMethod called when restoration is completed\nCalculates the total \nnumber of records \nrestored\nDetermines the \npercentage of \nrestoration \ncompleted\nLogs the percent \nrestored\nStores the\nnumber of\nrecords\nrestored\nso far\nKeeps track of restore progress \nfor a TopicPartition\n \n\n\n198\nCHAPTER 7\nMonitoring and performance\nkafkaStreams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\nThis is an example of using a StateRestoreListener. In chapter 9, you’ll see an\nexample that includes a graphical representation of the restore progress.\nTIP\nTo view the log file generated by running the CoGroupingListening-\nExampleApplication example, look for a log file named logs/state_restore_\nlistener.log in the root directory where you installed the source code. \n7.3.5\nUncaught exception handler\nI think it’s fair to say that every developer, from time to time, has encountered an\nunaccounted-for Exception and the big stack trace in the console/log as your pro-\ngram suddenly quits. Although this situation doesn’t quite fit into a “monitoring”\nexample, the ability to get a notification and handle any cleanup when the unex-\npected occurs is good practice. Kafka Streams provides KafkaStreams.setUncaught-\nExceptionHandler for dealing with these unexpected errors (found in src/main/\njava/bbejeck/chapter_7/CoGroupingListeningExampleApplication.java)\nkafkaStreams.setUncaughtExceptionHandler((thread, exception) -> {\nCONSOLE_LOG.info(\"Thread [\" + thread + \"]\n➥ encountered [\" + exception.getMessage() +\"]\");\n});\nThis is definitely a bare-bones implementation, but it serves to demonstrate where you\ncan set a hook for dealing with unexpected errors, either by logging the error as\nshown here or by performing any required cleanup and shutting down the streams\napplication.\n That wraps up our coverage of monitoring Kafka Streams applications. \nSummary\nTo monitor Kafka Streams, you’ll need to look at the Kafka brokers as well.\nYou should enable metrics reporting from time to time to see the how the per-\nformance of the application is doing.\nPeeking under the hood is required, and sometimes you’ll need to go to lower\nlevels and use command-line tools included with Java, such as jstack (thread\ndumps) and jmap/jhat (for heap dumps) to understand what your application\nis doing.\nIn this chapter, we focused on observing behavior. In the next chapter, we’ll shift our\nfocus to making sure the application handles errors consistently and adequately. We’ll\nalso make sure that it provides expected behavior by doing regular testing.\nListing 7.9\nSpecifying the global restore listener\nListing 7.10\nBasic uncaught exception handler\n \n\n\n199\nTesting a Kafka Streams\napplication\nSo far, we’ve covered the essential building blocks for creating a Kafka Streams\napplication. But there’s one crucial part of application development I’ve left out\nuntil now: how to test your application. One of the critical concepts we’ll focus on\nis placing your business logic in standalone classes that are entirely independent of\na Kafka Streams application, because that makes your logic much more accessible\nto test. I expect you’re aware of the importance of testing, but we’ll review my top\ntwo reasons for why testing is just as necessary as the development process itself.\n First, as you develop your code, you’re creating an implicit contract of what you\nand others can expect about how the code will execute. The only way to prove that\nthe code works is by testing thoroughly, so you’ll use testing to provide a good\nThis chapter covers\nTesting a topology\nTesting individual processors and transformers\nIntegration testing with an embedded Kafka \ncluster\n \n\n\n200\nCHAPTER 8\nTesting a Kafka Streams application\nbreadth of possible inputs and scenarios to make sure your code works appropriately\nunder reasonable circumstances.\n The second reason you need an excellent suite of tests is to deal with the inevitable\nchanges in software. Having a good set of tests gives you immediate feedback when\nyour new code has broken the expected set of behaviors. Additionally, when you do a\nmajor refactor or add new functionality, passing tests give you a level of confidence\nabout the impact of your changes (provided you have good tests).\n Even once you understand the importance of testing, testing a Kafka Streams\napplication isn’t always straightforward. You still have the option of running a simple\ntopology and observing the results, but there’s one drawback with that approach.\nYou’ll want a suite of repeatable tests that you can run at any time, and as part of a build,\nso you’d like the ability to test your applications without a Kafka cluster and Zoo-\nKeeper ensemble.\n That’s what we’ll cover in this chapter. First, you’ll see how you can test a topology\nwithout Kafka running, so you can see the entire topology working from a unit test.\nYou’ll also learn how to test a processor or transformer independently and mock any\nrequired dependencies.\nNOTE\nYou likely have experience testing with mock objects, but if not, Wiki-\npedia’s article provides a good introduction: https://en.wikipedia.org/wiki/\nMock_object.\nAlthough unit testing is critical for repeatability and quick feedback, integration\ntesting is important as well, because sometimes you’ll need to see the moving parts\nof your application in action. For example, consider the case of rebalancing, an\nessential part of a Kafka Streams application. Getting rebalancing to work in a unit\ntest is nearly impossible. Table 8.1 summarizes the differences between unit and inte-\ngration testing.\nYou need to trigger an actual rebalance under realistic conditions to test it. For those\nsituations, you’ll need to run with a live instance of a Kafka cluster. But you don’t want\nto rely on having an external cluster set up, so we’ll look at how you can use an\nembedded Kafka and ZooKeeper for integration testing.\nTable 8.1\nTesting approaches\nTest type\nPurpose\nTesting speed\nLevel of use\nUnit\nTesting individual parts of \nfunctionality in isolation\nFast\nLarge majority\nIntegration\nTesting integration points \nbetween whole systems\nLonger to run\nSmall minority\n \n\n\n201\nTesting a topology\n8.1\nTesting a topology\nThe first topology we built, in chapter 3, was relatively complicated. Figure 8.1 shows it\nagain, to refresh your memory.\nThe processing logic is pretty straightforward, but as you can see from the structure, it\nhas several nodes. It also has one key thing that will help demonstrate testing: it takes\none input—an initial purchase—and it performs several transformations. This will\nmake testing somewhat easy, in that you only need to supply a single purchase value,\nand you can confirm that all the appropriate transformations take place.\nTIP\nIn most cases, you’ll want to have your logic in separate classes so you can\nunit test the business logic separately from the topology. In the case of the\nZMart topology, most of the logic is simple and represented as Java 8 lambda\nexpressions, so in this case you’ll test the topology flow.\nYou’ll want a repeatable standalone test, so you’ll use the ProcessorTopologyTest-\nDriver, which will allow you to write such a test without needing Kafka to run the test.\nRemember, the ability to test your topology without a live Kafka instance makes testing\nfaster and more lightweight, leading to a quicker development cycle. Also note that\nthe ProcessorTopologyTestDriver is a generic testing framework that tests the indi-\nvidual Kafka Streams Topology objects you’ve created.\nFigure 8.1\nInitial complete topology for ZMart Kafka Streams program\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nThis topology takes a single input from\nthe source node and performs several\ntransformations, making this a great\ndemonstration of testing.\n \n\n\n202\nCHAPTER 8\nTesting a Kafka Streams application\nTIP\nIf you write your own projects using Kafka and Kafka Streams testing\ncode, it’s best if you use the all the dependencies that come with the sample\ncode.\nWhen you initially built this topology, you did it entirely within the ZMartKafka-\nStreamsApp.main method, which was fine for quick development at the time, but it\ndoesn’t lend itself to being testable. What you’ll do now is refactor the topology into a\nstandalone class, which will enable you to test the topology.\n The logic isn’t changing, and you’ll move the code as is, so I won’t show the con-\nversion here. Instead, I’ll point you to src/main/java/bbejeck/chapter_8/ZMart-\nTopology.java, where you can view it if you choose.\n With the code transitioned, let’s get on with constructing a test.\n8.1.1\nBuilding the test\nLet’s move on to building a unit test for the ZMart topology. You can use a standard\nJUnit test, and you’ll have some setup work to do before running the test (found in\nsrc/main/java/bbejeck/chapter_8/ZMartTopologyTest.java).\n \n \nUsing Kafka Streams’ testing utilities\nTo use Kafka Streams’ testing utilities, you’ll need to update your build.gradle file\nwith the following:\ntestCompile\ngroup:'org.apache.kafka', name:'kafka-streams',\n➥ version:'1.0.0', classifier:'test'\ntestCompile\ngroup:'org.apache.kafka', name:'kafka-clients',\n➥ version:'1.0.0', classifier:'test'\nIf you’re using Maven, use this code:\n<dependency>\n<groupId>org.apache.kafka</groupId>\n<artifactId>kafka-streams</artifactId>\n<version>1.0.0</version>\n<scope>test</scope>\n<classifier>test</classifier>\n</dependency>\n<dependency>\n<groupId>org.apache.kafka</groupId>\n<artifactId>kafka-clients</artifactId>\n<version>1.0.0</version>\n<scope>test</scope>\n<classifier>test</classifier>\n</dependency>\n \n\n\n203\nTesting a topology\n@Before\npublic\nvoid setUp() {\n// properties construction left out for clarity\nStreamsConfig streamsConfig = new StreamsConfig(props);\nTopology topology = ZMartTopology.build();\ntopologyTestDriver =\n➥ new ProcessorTopologyTestDriver(streamsConfig, topology);\n}\nThe critical point of listing 8.1 is the creation of the ProcessorTopologyTestDriver,\nwhich you’ll use in the following listing when you run your test (found in src/main/\njava/bbejeck/chapter_8/ZMartTopologyTest.java).\n@Test\npublic void testZMartTopology() {\n// serde creation left out for clarity\nPurchase purchase = DataGenerator.generatePurchase();\ntopologyTestDriver.process(\"transactions\",\nnull,\npurchase,\nstringSerde.serializer(),\npurchaseSerde.serializer());\nProducerRecord<String, Purchase> record =\n➥ topologyTestDriver.readOutput(\"purchases\",\nstringSerde.deserializer(),\npurchaseSerde.deserializer());\nPurchase expectedPurchase =\n➥ Purchase.builder(purchase).maskCreditCard().build();\nassertThat(record.value(), equalTo(expectedPurchase));\nThere are two critical sections in listing 8.2. Starting with topologyTestDriver\n.process, you feed a record into the transactions topic, because it’s the source for\nthe entire topology. With the topology loading completed, you can verify that the cor-\nrect actions have taken place. In the following line, using the topologyTestDriver\n.readOutput method, you read the record from the purchases topic, with one of the\nsink nodes defined in the topology. In the second-to-last line, you create the expected\noutput record, and on the final line, you assert that the results are what you expect.\nListing 8.1\nSetup method for topology test\nListing 8.2\nTesting the topology\nRefactored ZMart \ntopology: now you can \nget the topology from \nthe method call.\nCreates the ProcessorTopologyTestDriver\nCreates a test object; \nreuses the generation \ncode from running \nthe topology\nSends an initial record \ninto the topology\nReads a record from \nthe purchases topic\nConverts the \ntest object to \nthe expected \nformat\nVerifies that the record from the \ntopology matches the expected record\n \n\n\n204\nCHAPTER 8\nTesting a Kafka Streams application\n There are two other sink nodes in the topology, so let’s complete the test by verify-\ning you get the correct output from them (found src/test/java/bbejeck/chapter_8/\nZMartTopologyTest.java).\n@Test\npublic void testZMartTopology() {\n// continuing test from the previous section\nRewardAccumulator expectedRewardAccumulator =\n➥ RewardAccumulator.builder(expectedPurchase).build();\nProducerRecord<String, RewardAccumulator> accumulatorProducerRecord =\n➥ topologyTestDriver.readOutput(\"rewards\",\nstringSerde.deserializer(),\nrewardAccumulatorSerde.deserializer());\nassertThat(accumulatorProducerRecord.value(),\n➥ equalTo(expectedRewardAccumulator));\nPurchasePattern expectedPurchasePattern =\n➥ PurchasePattern.builder(expectedPurchase).build();\nProducerRecord<String, PurchasePattern> purchasePatternProducerRecord =\n➥ topologyTestDriver.readOutput(\"patterns\",\nstringSerde.deserializer(),\npurchasePatternSerde.deserializer());\nassertThat(purchasePatternProducerRecord.value(),\n➥ equalTo(expectedPurchasePattern));\n}\nAs you add another processing node to the test, you’ll see the same pattern as in list-\ning 8.3. You read records from each topic and verify your expectations with an assert\nstatement. The critical point to keep in mind with this test is that you now have a\nrepeatable test running a record through your entire topology, without the overhead\nof running Kafka.\n The ProcessorTopologyTestDriver also supports testing topologies with a state\nstore, so let’s look at how you’d accomplish that. \n8.1.2\nTesting a state store in the topology\nTo demonstrate testing a state store, you’ll refactor another class, StockPerformance-\nStreamsAndProcessorApplication, to have the Topology returned from a method\ncall. You’ll find the class in src/main/java/bbejeck/chapter_8/StockPerformance-\nStreamsProcessorTopology.java. I haven’t made any changes to the logic, so we won’t\nreview it here.\nListing 8.3\nTesting the rest of the topology\nReads a\nrecord from\nthe rewards\ntopic\nVerifies the rewards topic \noutput matches expectations\nReads a record from \nthe patterns topic\nVerifies the patterns topic \noutput matches expectations\n \n",
      "page_number": 213
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 221-231)",
      "start_page": 221,
      "end_page": 231,
      "detection_method": "topic_boundary",
      "content": "205\nTesting a topology\n The test setup is the same as in the previous test, so I’ll limit my explanations to the\nparts that are new (src/test/java/bbejeck/chapter_8/StockPerformanceStreams-\nProcessorTopologyTest.java).\nStockTransaction stockTransaction =\n➥ DataGenerator.generateStockTransaction();\ntopologyTestDriver.process(\"stock-transactions\",\nstockTransaction.getSymbol(),\nstockTransaction,\nstringSerde.serializer(),\nstockTransactionSerde.serializer());\nKeyValueStore<String, StockPerformance> store =\n➥ topologyTestDriver.getKeyValueStore(\"stock-performance-store\");\nassertThat(store.get(stockTransaction.getSymbol()),\n➥ notNullValue());\nAs you can see, the last assert line quickly verifies that your code is using the state\nstore as expected. You’ve seen the ProcessorTopologyTestDriver in action, and\nyou’ve seen how you can achieve end-to-end testing of a topology. The topologies you\ntest can be very simple, with one processing node, or very complex, consisting of sev-\neral sub-topologies. Even though you’re doing this testing without a Kafka broker,\nmake no mistake: this is a full test of the topology that will exercise all parts, including\nserializing and deserializing records.\n You’ve seen how you can do end-to-end testing of a topology. But you’ll also want\nto test the internal logic of your Processor and Transformer objects. Testing an\nentire topology is great, but verifying the behavior inside each class requires a more\nfine-grained approach, which we’ll cover next. \n8.1.3\nTesting processors and transformers\nTo verify the behavior inside a single class requires a true unit test, where there’s only\none class under test. Writing a unit test for a Processor or Transformer shouldn’t be\nvery challenging, but remember that both classes have a dependency on the Processor-\nContext for obtaining any state stores and scheduling punctuation actions.\n You don’t want to create a real ProcessorContext object, but rather a stand-in you\ncan use for testing purposes: a mock object. When it comes to using a mock object,\nyou can follow two paths.\n One option is to use a mock object framework such as Mockito (http://site\n.mockito.org) to generate mock objects in your test. Another option is to use the\nMockProcessorContext object found in the same test library as ProcessorTopology-\nTestDriver. Which one you use will depend on how you need to use them.\nListing 8.4\nTesting the state store\nGenerates a \ntest record\nProcesses the record \nwith the test driver\nRetrieves the\nstate store from\nthe test topology\nAsserts the store contains \nthe expected value\n \n\n\n206\nCHAPTER 8\nTesting a Kafka Streams application\n If you need the mock object strictly as a placeholder for a real dependency, con-\ncrete mocks (mocks not created from a framework) are a good choice. But if you want\nto verify the parameters passed to a mock, the value returned, or any other behavior,\nusing a mock object generated by a framework is a good choice. Mock object frame-\nworks (like Mockito) come with a rich API for setting expectations and verifying\nbehavior, saving you development time and speeding up your testing process.\n In listing 8.5, you’ll use both types of mock objects. You’ll use the Mockito frame-\nwork to create the ProcessorContext mock, because you want to verify parameters\nduring the init call as well as validate that you’re forwarding the expected values\nfrom the punctuate() method. You’ll also use a custom mock object for the key/value\nstore, which you’ll see in action as we step through the code example.\n In this listing, you’ll test a Processor, using mock objects. You’ll start with a test\nfor the AggregatingMethodHandleProcessor named AggregatingMethodHandle-\nProcessorTest, located in src/test/java/bbejeck_chapter6/processor/cogrouping/.\nFirst, you want to verify the parameters used in the init method (see src/test/java/\nbbejeck/chapter_6/AggregatingMethodHandleProcessorTest.java).\n// some details left out for clarity\nprivate ProcessorContext processorContext =\n➥ mock(ProcessorContext.class);\nprivate MockKeyValueStore<String, Tuple<List<ClickEvent>,\n➥ List<StockTransaction>>> keyValueStore =\n➥ new MockKeyValueStore<>();\nprivate AggregatingMethodHandleProcessor processor =\n➥ new AggregatingMethodHandleProcessor();\n@Test\n@DisplayName(\"Processor should initialize correctly\")\npublic void testInitializeCorrectly() {\nprocessor.init(processorContext);\nverify(processorContext).schedule(eq(15000L), eq(STREAM_TIME),\n➥isA(Punctuator.class));\nverify(processorContext).getStateStore(TUPLE_STORE_NAME);\n}\nThis first test is a simple one: you call the init method on the processor under test\nwith the mocked ProcessorContext. You then validate the parameters used to sched-\nule the punctuate method, and that the state store is retrieved.\n Next, let’s test the punctuate method to verify that the records are forwarded as\nexpected (found in src/test/java/bbejeck/chapter_6/AggregatingMethodHandle-\nProcessorTest.java).\n \nListing 8.5\nTesting the init method\nMocks the ProcessorContext \nwith Mockito\nA mock \nKeyValueStore \nobject\nThe class under test\nCalls the init method on the \nprocessor, triggering method \ncalls on ProcessorContext\nVerifies the parameters for the \nProcessorContext.schedule method\nVerifies retrieving\nthe state store\n \n\n\n207\nTesting a topology\n@Test\n@DisplayName(\"Punctuate should forward records\")\npublic void testPunctuateProcess(){\nwhen(processorContext.getStateStore(TUPLE_STORE_NAME))\n.thenReturn(keyValueStore);\nprocessor.init(processorContext);\nprocessor.process(\"ABC\", Tuple.of(clickEvent, null));\nprocessor.process(\"ABC\", Tuple.of(null, transaction));\nTuple<List<ClickEvent>,List<StockTransaction>> tuple =\n➥ keyValueStore.innerStore().get(\"ABC\");\nList<ClickEvent> clickEvents = new ArrayList<>(tuple._1);\nList<StockTransaction> stockTransactions = new ArrayList<>(tuple._2);\nprocessor.cogroup(124722348947L);\nverify(processorContext).forward(\"ABC\",\n➥ Tuple.of(clickEvents, stockTransactions));\nassertThat(tuple._1.size(), equalTo(0));\nassertThat(tuple._2.size(), equalTo(0));\n}\nThis test is a little more involved, and it utilizes a mix of mock and real behavior. Let’s\ntake a brief walk through the test.\n The first line specifies the behavior for the mock ProcessorContext to return the\nstubbed-out KeyValueStore when the ProcessorContext.getStateStore method is\ncalled. This line alone is an interesting mix of generated mock versus a stubbed-out\nmock object.\n I could easily have used Mockito to generate a mock KeyValueStore, but I chose not\nto for two reasons. First, a generated mock returning another generated mock seems a\nbit unnatural (in my opinion). Second, you want to verify and use the stored values in the\nKeyValueStore during the test instead of setting expectations with a canned response.\n The next three lines, starting with processor.init, run the processor through its\nusual steps: first initializing and then processing records. The fourth step is where hav-\ning a working KeyValueStore is important. Because the KeyValueStore is a simple\nstub, you use a java.util.HashMap underneath for the actual storage. In the three\nlines after setting processor expectations, you retrieve the contents from the store\nplaced there by the process() method calls. You create new ArrayList objects with\nthe contents of the Tuple (again, this is a custom class developed for the sample code\nin this book) pulled from the state store by the provided key.\n Next, you drive the punctuate method of the processor. Because this is a unit test,\nyou don’t need to test how time is advanced—that would constitute testing the Kafka\nStreams API itself, which you don’t want here. Your goal is to verify the behavior of the\nmethod you defined as your Punctuator (via a method reference).\nListing 8.6\nTesting the punctuate method\nSets mock behavior to \nreturn a KeyValueStore \nwhen called\nCalls init\nmethod on\nprocessor\nProcesses a ClickEvent \nand a StockTransaction\nExtracts the entries put \ninto the state store in \nthe process method\nCalls the\nco-group\nmethod,\nwhich is\nthe method\nused to\nschedule\npunctuate\nValidates that the \nProcessorContext forwards \nthe expected records\nValidates that the collections \nwithin the tuple are cleared out\n \n\n\n208\nCHAPTER 8\nTesting a Kafka Streams application\n Now, you verify the main point of the test: that the expected key and value are for-\nwarded downstream via the ProcessorContext.forward method. This portion of the\ntest demonstrates the usefulness of a generated mock object. Using the Mockito\nframework, you just need to tell the mock to expect a forward call with the given key\nand value, and verify that the test executed the code precisely in this manner. Finally,\nyou verify that the processor cleared out the collections of ClickEvent and Stock-\nTransaction objects after forwarding them downstream.\n As you can see from this test, you can isolate the class under test with a mix of gen-\nerated and stubbed-out mock objects. As I stated earlier in this chapter, the bulk of the\ntesting in your Kafka Streams API application should be unit tests on your business\nlogic and on any individual Processor or Transformer objects. Kafka Streams itself is\nthoroughly tested, so you’ll want to focus your efforts on new, untested code.\n You probably won’t want to wait to deploy your application to see how it interacts\nwith a Kafka cluster. You’ll want to sanity check your code, which will require integra-\ntion testing. Let’s look at how you can locally test against a real Kafka broker. \n8.2\nIntegration testing\nSo far, you’ve seen how you can test an entire topology or an individual component in\na unit test. For the most part, these types of tests are best, as they’re quick to run, and\nthey validate specific parts of your code.\n But there are times where you’ll need to test all the working parts together, end to\nend: in other words, an integration test. Usually, an integration test is required when\nyou have some functionality that can’t be covered in a unit test. \n For an example, let’s go back to our very first application, the Yelling App. Because\nyou created the topology so long ago, take another look at it in figure 8.2.\nUpperCase\nprocessor\nsrc-topic\nSource\nprocessor\nSink\nprocessor\nout-topic\nFigure 8.2\nAnother look at the \nYelling App topology\n \n\n\n209\nIntegration testing\nSuppose you’ve decided to change the source from a single named topic to any topic\nmatching a regex:\nyell-[A-Za-z0-9-]\nAs an example, you want to confirm that if a topic matching the pattern yell-at-\neveryone is added while your application is deployed and running, you’ll start read-\ning information from that newly added topic.\n You won’t update the original Yelling App, since it’s so small. Instead you’ll use the\nfollowing modified version directly in the test (found in src/java/bbejeck/chapter_3/\nKafkaStreamsYellingIntegrationTest.java).\nstreamsBuilder.<String,String>stream(Pattern.compile(\"yell.*\"))\n.mapValues(String::toUpperCase)\n.to(OUT_TOPIC);\nBecause you add topics at the Kafka broker level, the only real way to test whether\nyour application picks up a newly created topic is to add one while your application is\nrunning. Running this scenario is all but impossible with a unit test. But does this\nmean you need to deploy your updated app to test it?\n Fortunately, the answer is no. You can use the embedded Kafka cluster available with\nKafka test libraries.\n By using the embedded Kafka cluster, you can run an integration test requiring a\nKafka cluster on your machine at any point, either individually or as part of your\nentire battery of tests. This speeds up your development cycle. (I use the term embed-\nded here to refer to running a large application like Kafka or ZooKeeper in local\nstandalone mode, or “embedding” it in an existing application.) Let’s move on to\nbuilding an integration test.\n8.2.1\nBuilding an integration test\nThe first step to using the embedded Kafka server requires you to add three more test-\ning dependencies—scala-library-2.12.4.jar, kafka_2.12-1.0.0-test.jar, and kafka_2.12-\n1.0.0.jar—to your build.gradle or pom.xml file. We’ve already covered the syntax for\nproviding a test JAR in section 8.1, so I won’t repeat it here.\n While it may seem like the number of dependencies is starting to increase, remem-\nber that anything you add here is a testing dependency. Testing dependencies aren’t\npackaged up and deployed with your application code; hence, they won’t affect the\nsize of your final application.\n Now that you’ve added the required dependencies, let’s start defining the integra-\ntion test with an embedded Kafka broker. You’ll use a standard JUnit approach for\ncreating the integration test.\nListing 8.7\nUpdating the Yelling App\nSubscribes \nto any topic \nstarting with \n\"yell\"\nConverts all text\nto uppercase\nWrites out to topic, \nor yells at people!\n \n\n\n210\nCHAPTER 8\nTesting a Kafka Streams application\nADDING THE EMBEDDEDKAFKACLUSTER\nAdding the embedded Kafka broker to the test is a matter of adding one line, as\nshown in the following listing (found in src/java/bbejeck/chapter_3/KafkaStreams-\nYellingIntegrationTest.java).\nprivate static final int NUM_BROKERS = 1;\n@ClassRule\npublic static final EmbeddedKafkaCluster EMBEDDED_KAFKA\n➥= new EmbeddedKafkaCluster(NUM_BROKERS);\nIn the second line listing 8.8, you create the EmbeddedKafkaCluster instance that\nserves as the cluster for running the tests in the class. The key point in this example is\nthe @ClassRule annotation. A full description of testing frameworks and JUnit is\nbeyond the scope of this book, but I’ll take a minute here to explain the importance\nof @ClassRule and how it drives the test. \nJUNIT RULES\nJUnit introduced the concept of rules to apply some common logic JUnit tests. Here’s\na brief definition, from https://github.com/junit-team/junit4/wiki/Rules#rules:\n“Rules allow very flexible addition or redefinition of the behavior of each test method\nin a test class.”\n JUnit provides three types of rules, and the EmbeddedKafkaCluster class uses the\nExternalResource rules (https://github.com/junit-team/junit4/wiki/Rules#exter-\nnalresource-rules). You use ExternalResource rules for setting up and tearing down\nexternal resources, such as the EmbeddedKafkaCluster needed for a test.\n JUnit provides the ExternalResource class, which has two no-op methods,\nbefore() and after(). Any class extending the ExternalResource must override the\nbefore() and after() methods for setting up and tearing down the external resource\nneeded for testing.\n Rules provide an excellent abstraction for using external resources in your tests.\nAfter you create your class extending ExternalResource, all you need to do is create a\nvariable in your test and use the @Rule or @ClassRule annotation, and all the setup\nand teardown methods will be executed automatically.\n The difference between @Rule and @ClassRule is how often before() and\nafter() are called. The @Rule annotation executes before() and after() methods\nfor each individual test in the class. @ClassRule executes the before() and after()\nmethods once; before() is executed prior to any test execution, and after() is called\nwhen the last test in the class completes. Setting up an EmbeddedKafkaCluster is rela-\ntively resource intensive, so it makes sense that you’ll only want to set it up once per\ntest class.\nListing 8.8\nAdding the embedded Kafka broker\nDefines the number \nof brokers\nThe JUnit ClassRule \nannotation\nCreates an instance of the \nEmbeddedKafkaCluster\n \n\n\n211\nIntegration testing\n Let’s get back to building an integration test. You’ve created an EmbeddedKafka-\nCluster, so the next step is to create any topics you’ll initially need. \nCREATING TOPICS\nNow that your embedded Kafka cluster is available, you can use it to create topics, as\nfollows (src/java/bbejeck/chapter_3/KafkaStreamsYellingIntegrationTest.java).\n@BeforeClass\npublic static void setUpAll() throws Exception {\nEMBEDDED_KAFKA.createTopic(YELL_A_TOPIC);\nEMBEDDED_KAFKA.createTopic(OUT_TOPIC);\n}\nCreating topics for the test is something you’ll want to do only once for all tests, so you\ncan use a @BeforeClass annotation, which creates the required topics before the\nexecution of any tests. For this test, you only need topics with a single partition and\na replication factor of 1, so you can use the convenience method EmbeddedKafka-\nCluster.createTopic(String name). If you needed more than one partition, a repli-\ncation factor greater than 1 requires configurations different from the defaults. For\nthat, you can use one of the following overloaded createTopic methods:\n\nEmbeddedKafkaCluster.createTopic(String topic, int partitions, int\nreplication)\n\nEmbeddedKafkaCluster.createTopic(String topic, int partitions, int\nreplication, Properties topicConfig)\nWith all the pieces in place for the embedded Kafka cluster to run, let’s move on to\ntesting the topology with the embedded broker. \nTESTING THE TOPOLOGY\nAll the pieces are in place. Now you can follow these steps to execute the integra-\ntion test:\n1\nStart the Kafka Streams application.\n2\nWrite some records to the source topic and assert the correct results.\n3\nCreate a new topic matching your pattern.\n4\nWrite some additional records to the newly created topic and assert the cor-\nrect results.\nLet’s start with the first two parts of the test (found in src/java/bbejeck/chapter_3/\nKafkaStreamsYellingIntegrationTest.java).\n// some setup code left out for clarity\nkafkaStreams = new KafkaStreams(streamsBuilder.build(), streamsConfig);\nListing 8.9\nCreating the topics for testing\nListing 8.10\nStarting the application and asserting the first set of values\nBeforeClass annotation\nCreates the first \nsource topic\nCreates the \noutput topic\n \n\n\n212\nCHAPTER 8\nTesting a Kafka Streams application\nkafkaStreams.start();\nList<String> valuesToSendList =\n➥ Arrays.asList(\"this\", \"should\", \"yell\", \"at\", \"you\");\nList<String> expectedValuesList =\n➥ valuesToSendList.stream()\n.map(String::toUpperCase)\n.collect(Collectors.toList());\nIntegrationTestUtils.produceValuesSynchronously(YELL_A_TOPIC,\nvaluesToSendList,\nproducerConfig,\nmockTime);\nint expectedNumberOfRecords = 5;\nList<String> actualValues =\n➥ IntegrationTestUtils.waitUntilMinValuesRecordsReceived(\n➥ consumerConfig, OUT_TOPIC, expectedNumberOfRecords);\nassertThat(actualValues, equalTo(expectedValuesList));\nThis portion of the test is pretty standard testing code. You “seed” your streaming\napplication by writing records to the source topic. The streaming application is\nalready running, so it consumes, processes, and writes out records as part of its stan-\ndard processing. To verify that the application is performing as you expect, the test\nconsumes records from the sink-node topic and compares the expected values to the\nactual values.\n Toward the end of listing 8.10 are two static utility methods, IntegrationTest-\nUtils.produceValuesSynchronously and IntegrationTestUtils.waitUntilMin-\nValuesRecordsReceived, making the construction of this integration test much more\nmanageable. These producing and consuming utility methods are part of kafka-\nstreams-test.jar. Let’s discuss these methods briefly. \nPRODUCING AND CONSUMING RECORDS IN A TEST\nThe IntegrationTestUtils.produceValuesSynchronously method creates a Producer-\nRecord for each item in the collection with a null key. This method is synchronous, as\nit takes the resulting Future<RecordMetadata> from the Producer.send call and\nimmediately calls Future.get(), which blocks until the produce request returns.\nBecause this method is sending records synchronously, you know the records are avail-\nable for consuming once the method returns. Another method, IntegrationTest-\nUtils.produceKeyValuesSynchronously, takes a collection of KeyValue<K,V> if you\nwant to specify a value for the key.\n For consuming records in listing 8.10, you use the IntegrationTestUtils.wait-\nUntilMinValuesRecordsReceived method. As you can probably guess from the name,\nthis method will attempt to consume the expected number of records from the given\ntopic. By default, this method will wait up to 30 seconds, and if the expected number\nof records has not been consumed, an AssertionError is thrown, failing the test.\nStarts the Kafka \nStreams application\nSpecifies the list \nof values to send\nCreates the list of \nexpected values\nProduces the values \nto embedded Kafka\nConsumes \nrecords \nfrom Kafka\nAsserts the values \nread are equal to \nthe expected values\n \n\n\n213\nIntegration testing\n If you need to work with the consumed KeyValue instead of just the value, there’s\nthe IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived method, which\nworks in the same manner but returns a Collection of KeyValue results. Additionally,\nthere are overloaded versions of the consuming utility, where you can specify a custom\namount of time to wait via a parameter of type long.\n Now, let’s finish describing the test. \nDYNAMICALLY ADDING A TOPIC\nYou’re at the point in the test where you want to test the dynamic behavior that you\nneed a live Kafka broker for. The previous portion of the test was done to verify the\nstarting point. Now, you’re going to use the EmbeddedKafkaCluster to create a new\ntopic, and you’ll test that the application consumes from the new topic and processes\nrecords as expected (found in src/java/bbejeck/chapter_3/KafkaStreamsYellingInte-\ngrationTest.java).\nEMBEDDED_KAFKA.createTopic(YELL_B_TOPIC);\nvaluesToSendList = Arrays.asList(\"yell\", \"at\", \"you\", \"too\");\nexpectedValuesList = valuesToSendList.stream()\n.map(String::toUpperCase)\n.collect(Collectors.toList());\nIntegrationTestUtils.produceValuesSynchronously(YELL_B_TOPIC,\nvaluesToSendList,\nproducerConfig,\nmockTime);\nexpectedNumberOfRecords = 4;\nList<String> actualValues =\n➥IntegrationTestUtils.waitUntilMinValuesRecordsReceived(\n➥consumerConfig, OUT_TOPIC, expectedNumberOfRecords);\nassertThat(actualValues, equalTo(expectedValuesList));\nYou create a new topic matching the pattern for the source node of the streaming\napplication. After that, you go through the same steps of populating the new topic\nwith data, and consuming records from the topic backing the sink node of the stream-\ning application. At the end of the test, you verify that the consumed results match the\nexpected results.\n You can run this test from inside your IDE, and you should see a successful result.\nYou’ve completed your first integration test!\n You won’t want to use an integration test for everything, because unit tests are eas-\nier to write and maintain. But integration tests can be indispensable when the only\nway to verify the behavior of your code is working with a live Kafka broker.\nListing 8.11\nStarting the application and asserting values\nCreates the \nnew topic\nSpecifies a new list \nof values to send\nCreates the \nexpected \nvalues\nProduces the values to the source \ntopic of the streaming application\nConsumes the \nresults of the \nstreaming \napplication\nAsserts that the \nexpected results match \nthe actual results\n \n\n\n214\nCHAPTER 8\nTesting a Kafka Streams application\nNOTE\nIt may be tempting to make all your tests using the EmbeddedKafka-\nCluster, but it’s best if you don’t. If you run the sample integration test you\njust built, you’ll notice that it takes much longer to run than the unit tests.\nThe few extra seconds taken by one test might not seem like much, but when\nyou multiply that time by several hundred or a thousand or more tests, the\ntime it takes to run your test suite is substantial. Additionally, you should\nalways try to keep your tests small and focused on one specific piece of func-\ntionality instead of exercising all parts of the application chain. \nSummary\nStrive to keep business logic in standalone classes that are entirely independent\nof your Kafka Streams application. This makes them easy to unit test.\nIt’s useful to have at least one test using ProcessorTopologyTestDriver to test\nyour topology from end to end. This type of test doesn’t use a Kafka broker, so\nit’s fast, and you can see end-to-end results.\nFor testing individual Processor or Transformer instances, try to use a mock\nobject framework only when you need to verify the behavior of some class in the\nKafka Streams API.\nIntegration tests with the EmbeddedKafkaCluster should be used sparingly, and\nonly when you have interactive behavior that can only be verified with a live,\nrunning Kafka broker.\nIt’s been a fun journey, and you’ve learned quite a lot about the Kafka Streams API\nand how you can use it to handle your data-processing needs. So to conclude your\nlearning path, we’ll now switch gears from student to master. The next and final chap-\nter of this book will be a capstone project based on everything you’ve learned so far,\nand in some cases extending out to write custom code that isn’t in the Kafka Streams\nAPI. The result will be a live, end-to-end application using the core functionality pre-\nsented in this book.\n \n\n\nPart 4\nAdvanced concepts\nwith Kafka Streams\nIn this final part, you’ll take everything you’ve learned and apply it to build-\ning advanced applications. You’ll integrate Kafka Streams with Kafka Connect so\nthat you can stream data even if it’s being written to a relational database. Then,\nyou’ll learn how to use the power of interactive queries to display—in real time—\ninformation your application is building, directly from Kafka Streams, without\nneeding an external tool. Finally, you’ll learn about KSQL, a new tool intro-\nduced by Confluent (the company founded by the original developers of Kafka\nat LinkedIn), and how you can write SQL statements and run continuous que-\nries on data coming into Kafka.\n \n",
      "page_number": 221
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 232-247)",
      "start_page": 232,
      "end_page": 247,
      "detection_method": "topic_boundary",
      "content": "217\nAdvanced applications\nwith Kafka Streams\nYou’ve come a long way in your journey to learn how to use Kafka Streams. We’ve\ncovered a lot of ground, and now you should know how to build streaming applica-\ntions. Up to this point, you’ve included the core functionality of Kafka Streams, but\nthere’s much more you can do. In this chapter, you’ll use what you’ve learned to\nbuild two advanced applications that will allow you to work in real-world situations.\n For example, in many organizations, when you bring in new technology, it must\nmesh with legacy technology or processes. It’s not uncommon to see database\ntables as the main sink for incoming data. You learned in chapter 5 that tables are\nstreams, so you know you should be able to treat database tables as streams of data.\n The first advanced application we’ll look at in this chapter will “convert” a\nphysical database into a streaming application by integrating Kafka Connect with\nKafka Streams. Kafka Connect will listen for new insertions into the table(s) and\nThis chapter covers\nIntegrating outside data into Kafka Streams with \nKafka Connect\nKicking your database to the curb with interactive \nqueries\nKSQL continuous queries in Kafka\n \n\n\n218\nCHAPTER 9\nAdvanced applications with Kafka Streams\nwrite those records to a topic in Kafka. This same topic will serve as the source for\nthe Kafka Streams application so that you can turn your database table into a stream-\ning application.\n When you’re working with legacy applications, even if data is captured in real time,\nit’s typical to dump the data into a database to serve as the data source for dashboard\napplications. In this chapter’s second advanced application, you’ll see how to use\ninteractive queries that expose the Kafka Streams state stores for direct queries. A\ndashboard application can then pull directly from the state stores and show data as it’s\nflowing through the streaming application, eliminating the need for a database.\n We’ll wrap up our coverage of advanced features by looking at a powerful new\nKafka feature: KSQL. KSQL lets you write long-running SQL queries on data coming\ninto Kafka. KSQL gives you all the power of Kafka Streams combined with the ease of\nwriting a SQL query. When you use KSQL, it uses Kafka Streams under the covers to\nget the job done.\n9.1\nIntegrating Kafka with other data sources\nFor the first advanced example application, let’s suppose you work at an established\nfinancial services firm, Big Short Equity (BSE). BSE wants to migrate its legacy data\noperations into the modern era, and that plan includes using Kafka. The migration is\npart-way through, and you’re tasked with updating the company’s analytics. The goal\nis to display the latest equities transactions and associated information in real time,\nand Kafka Streams is the perfect fit.\n BSE offers funds focused on different areas of the financial market. The company\nrecords fund transactions in real time in a relational database. Eventually, BSE plans\nto write transactions directly into Kafka, but in the short term, the database is the sys-\ntem of record.\n Given that incoming data is fed into a relational database, how can you bridge the\ngap between the database and your emerging Kafka Streams application? The answer\nis to use Kafka Connect (https://kafka.apache.org/documentation/#connect), a frame-\nwork that’s included with Apache Kafka and that integrates Kafka with other systems.\nOnce Kafka has the data, you’re no longer concerned about the location of the source\ndata; you just point your Kafka Streams application at the source topic as you’ve done\nwith other Kafka Streams applications.\nNOTE\nWhen you use Kafka Connect to bring in data from other sources, the\nintegration point is a Kafka topic. This means any application using Kafka-\nConsumer can use the imported data. Because this is a book about Kafka\nStreams, I emphasize how to integrate with a Kafka Streams application.\nFigure 9.1 shows how this integration between the database and Kafka works. In this\ncase, you’ll use Kafka Connect to monitor a database table and stream updates into a\nKafka topic, which is the source of your financial analysis application.\n \n\n\n219\nIntegrating Kafka with other data sources\nTIP\nBecause this is a book on Kafka Streams, this section is a whirlwind tour\nof Kafka Connect. For more in-depth information, see the Apache Kafka doc-\numentation (https://kafka.apache.org/documentation/#connect) and Kafka\nConnect Quick Start (https://docs.confluent.io/current/connect/quickstart\n.html).\n9.1.1\nUsing Kafka Connect to integrate data\nKafka Connect is explicitly designed for streaming data from other systems into Kafka\nand for streaming data from Kafka into another data-storage application such as\nMongoDB (www.mongodb.com) or Elasticsearch (www.elastic.co). With Kafka Con-\nnect, it’s possible to import entire databases into Kafka, or other data such as perfor-\nmance metrics.\n Kafka Connect uses specific connectors to interact with outside data sources. Several\nconnectors are available (www.confluent.io/product/connectors), many developed\nby the connector community, making integration between Kafka and almost any other\nstorage system possible. If there isn’t a connector available for your purposes, you can\nimplement a connector of your own (https://docs.confluent.io/current/connect/\ndevguide.html). \n9.1.2\nSetting up Kafka Connect\nKafka Connect runs in two flavors: distributed mode and standalone mode. For most\nproduction environments, running in distributed mode makes sense, because you can\nKafka Connect\nKafka broker\nDatabase\nTopic\nKafka Streams\nKafka Connect reads from the conﬁgured table and writes\nthe data to a topic(s) named\n+\n. The import\npreﬁx\ntablename\nprocess runs continuously, by executing select statements to\npull in newly inserted records.\nThe Kafka Streams application now\nprocesses the incoming data.\nBecause the import process is\ncontinuous, you are essentially stream\nprocessing a database table.\nConnect uses conﬁgured connectors\nto perform data integration.\npreﬁx\ntablename\nFigure 9.1\nKafka Connect integrating a database table and Kafka Streams\n \n\n\n220\nCHAPTER 9\nAdvanced applications with Kafka Streams\ntake advantage of the parallelism and fault tolerance available when you run multiple\nConnect instances. I’m assuming you’ll run the examples on your local machine, so\neverything is configured in standalone mode.\n The connectors that Kafka Connect uses to interact with outside data sources\ncome in two types: source connectors and sink connectors. Figure 9.2 illustrates how\nKafka Connect uses both types. As you can see, source connectors bring data into\nKafka, and sink connectors receive data from Kafka for use by another system.\nFor this example, you’ll use the Kafka JDBC connector (https://github.com/conflu-\nentinc/kafka-connect-jdbc). It’s available on GitHub and also packaged with the book’s\nsource code distribution as a convenience (https://manning.com/books/kafka-streams-\nin-action).\n When using Kafka Connect, you’ll need to do some minor configuration to Kafka\nConnect itself and to the individual connector you’re using to import or export data.\nFirst, let’s look at the configuration parameters you’ll work with for Kafka Connect:\n\nbootstrap.servers—Comma-separated list of the Kafka broker(s) used by\nConnect.\n\nkey.converter—Class of the converter that controls serialization of the key\nfrom Connect format to the format written to Kafka.\n\nvalue.converter—Class of the converter that controls serialization of the\nvalue from Connect format to the format written to Kafka. For this example,\nyou’ll use the built-in org.apache.kafka.connect.json.JsonConverter.\nKafka cluster\nKafka Connect\nKafka cluster\nKafka Connect\nConnect uses sink connectors\nto push data from Kafka\nto external data sources\n(database, ﬁlesystem, and so on).\nConnect uses source connectors\nto pull from external data sources\n(database, ﬁlesystem, and so\non) to push data to Kafka.\nFigure 9.2\nKafka Connect source and sink connectors\n \n\n\n221\nIntegrating Kafka with other data sources\n\nvalue.converter.schemas.enable—true or false, specifying whether Con-\nnect should include the schema for the value. In this example, you’ll set this\nvalue to false; I explain why in the next section.\n\nplugin.path—Tells Connect the location of a connector and its dependencies.\nThis location can be a single, top-level directory containing an uber JAR file or\nmultiple JAR files. You can also provide several paths in a comma-separated list\nof locations.\n\noffset.storage.file.filename—File containing the offsets stored by the Con-\nnect consumer.\nYou’ll also need to provide some configuration for the JDBC connector. Let’s review\nthose settings:\n\nname—Name of the connector.\n\nconnector.class—Class of the connector.\n\ntasks.max—The maximum number of tasks used by this connector.\n\nconnection.url—URL used to connect to the database.\n\nmode—Method the JDBC source connector uses to detect changes.\n\nincrementing.column.name—Name of the column tracked for detecting\nchanges.\n\ntopic.prefix—Connect writes each table to a topic named topic.prefix+\ntable name.\nMost of these configurations are straightforward, but we need to discuss two of\nthem—mode and incrementing.column.name—in a little more detail, because they\nplay an active role in how the connector runs. The JDBC source connector uses mode\nto detect which rows it needs to load. The example uses the incrementing setting,\nwhich relies on an auto-incrementing column where each insert increments the col-\numn value by 1. By tracking an incrementing column, you’ll only pull in new records;\nupdates will go unnoticed. Your Kafka Streams application is only concerned with\npulling in the latest equities-product purchases, so this setting is ideal. incrementing\n.column.name is the name of the column containing the auto-incrementing value.\nTIP\nThe source code for the book contains the nearly completed configura-\ntion for both Connect and the JDBC connector. The config files are located\nin the src/main/resources directory of the source code distribution (https://\nmanning.com/books/kafka-streams-in-action). You’ll need to provide some\ninformation about the path where you’ve extracted the source code reposi-\ntory. Be sure to look at the README.md file for full instructions.\nThis brings us to the end of our overview of Kafka Connect and the JDBC source con-\nnector. We have one more integration point to cover, which we’ll discuss in the next\nsection.\n \n\n\n222\nCHAPTER 9\nAdvanced applications with Kafka Streams\nNOTE\nYou can find more information about the JDBC source connector in\nthe Confluent documentation (http://mng.bz/01vh). Additionally, there are\nother incremental query modes you should look over (http://mng.bz/0pjP). \n9.1.3\nTransforming data\nBefore getting this new assignment, you had already developed a Kafka Streams appli-\ncation using similar data. As a result, you have an existing model and Serde objects\n(using Gson underneath for JSON serialization and deserialization). To keep your\ndevelopment velocity high, you’d prefer not to write any new code to support working\nwith Connect. As you’ll see in the next section, you’ll be able to import data from Con-\nnect seamlessly.\nTIP\nGson (https://github.com/google/gson) is an Apache licensed library\ndeveloped by Google for the serialization and deserialization of Java objects\ninto and from JSON. You can learn more from the user guide: http://mng\n.bz/JqV2.\nTo enable this seamless integration, you’ll need to make some minor additional con-\nfiguration changes to your JDBC connector’s properties. Before you do, let’s revisit\nsection 9.1.2, where we discussed configuration settings. Specifically, I said you’d use\norg.apache.kafka.connect.json.JsonConverter with schemas disabled; hence, the\nvalue is converted into a simple JSON format.\n Although JSON is what you want to consume in your Kafka Streams application,\nthere are two issues. First, when converting the data into JSON format, the column\nnames are used for the names of fields in the converted JSON. The names are all in an\nabbreviated BSE format that has no meaning outside the organization, so when your\nGson serde is converted from JSON to the expected model object, all the fields are\nnull because the names don’t match.\n Second, the date and time are stored in the database as a timestamp, as expected.\nBut the provided Gson serde hasn’t defined a custom TypeAdapter (http://mng\n.bz/inzB) for the Date type, so all dates need to be a String formatted like this: yyyy-\nMM-dd’T’HH:mm:ss.SSS-0400. Fortunately, Kafka Connect provides a mechanism that\nallows you to handle these two issues with ease.\n Kafka Connect has the concept of Transformations that let you perform lightweight\ntransformations before Connect writes data to Kafka. Figure 9.3 shows where this\ntransformation process takes place.\n In this example, you’ll use two built-in transformations: TimestampConverter\nand ReplaceField. As I mentioned previously, to use these transformations, you\nneed to add the following configuration lines to the connector-jdbc.properties file\n(see src/main/resources/conf/connector-jdbc.properties).\n \n \n \n \n \n\n\n223\nIntegrating Kafka with other data sources\ntransforms=ConvertDate,Rename\n \ntransforms.ConvertDate.type=\n➥ org.apache.kafka.connect.transforms.TimestampConverter$Value  \ntransforms.ConvertDate.field=TXNTS\n  \ntransforms.ConvertDate.target.type=string\n  \ntransforms.ConvertDate.format=yyyy-MM-dd'T'HH:mm:ss.SSS-0400  \ntransforms.Rename.type=\n➥ org.apache.kafka.connect.transforms.ReplaceField$Value\ntransforms.Rename.renames=SMBL:symbol, SCTR:sector,....   \nThese properties are relatively self descriptive, so we won’t spend much time on them.\nAs you can see, they provide you with exactly what you need for your Kafka Streams\napplication to successfully deserialize messages imported into Kafka by Connect and\nthe JDBC connector.\n With all the Connect pieces in place, completing the integration between the data-\nbase table and your Kafka Streams application is just a matter of using a topic with the\nprefix specified in the connector-jdbc.properties file (found in src/main/java/bbejeck/\nchapter_9/StockCountsStreamsConnectIntegrationApplication.java).\n \n \n \nListing 9.1\nJDBC connector properties\nSYMB\nSCTR\nINDSTY\nOriginal column names\nConnect transforms\nSYMB:symbol, SCTR:sector, INDSTY:industry,...\n{\"symbol\":\"xxxx\", \"sector\":\"xxxxx\", \"industry\":\"xxxxx\",...}\nTransforms take the original\ncolumn names and map them\nto the new conﬁgured names.\nThe JSON message sent to Kafka\ntopics now has the expected ﬁeld\nnames needed for deserialization.\nFigure 9.3\nTransforming the names of the columns to match expected field names\nAliases for the \ntransformers\nType for the\nConvertDate alias\nDate field to convert\nOutput \ntype of the \nconverted \ndate field\nFormat of \nthe date\nType for the \nRename alias\nList of column names (truncated for\nclarity) to replace. The format is\nOriginal:Replacement.\n \n\n\n224\nCHAPTER 9\nAdvanced applications with Kafka Streams\nSerde<StockTransaction> stockTransactionSerde =\n➥ StreamsSerdes.StockTransactionSerde();     \nStreamsBuilder builder = new StreamsBuilder();\nbuilder.stream(\"dbTxnTRANSACTIONS\",\n➥ Consumed.with(stringSerde,stockTransactionSerde))   \n.peek((k, v)->\n➥ LOG.info(\"transactions from database key {}value {}\", k, v));   \nAt this point, you’re stream processing records from a database table in Kafka\nStreams, but there’s more to do. You’re streaming stock-transaction data—to do any\nanalysis, you need to group the transactions by their stock ticker symbol.\n You’ve seen how to select a key and repartition the records, but it’s more efficient\nif the records come into Kafka keyed; that way, your Kafka Streams application can\nskip the repartitioning step, which saves processing time and disk space. Let’s revisit\nthe configuration for Kafka Connect.\n First, you can add a ValueToKey transformer that takes a list of field names in the\nvalue to extract and use for the key. Update the connector-jdbc.properties file as\nshown here (src/main/resources/conf/connector-jdbc.properties).\ntransforms=ConvertDate,Rename,ExtractKey\n    \ntransforms.ConvertDate.type=\n➥ org.apache.kafka.connect.transforms.TimestampConverter$Value\ntransforms.ConvertDate.field=TXNTS\ntransforms.ConvertDate.target.type=string\ntransforms.ConvertDate.format=yyyy-MM-dd'T'HH:mm:ss.SSS-0400\ntransforms.Rename.type=\n➥ org.apache.kafka.connect.transforms.ReplaceField$Value\ntransforms.Rename.renames=SMBL:symbol, SCTR:sector,....\ntransforms.ExtractKey.type=\n➥ org.apache.kafka.connect.transforms.ValueToKey      \ntransforms.ExtractKey.fields=symbol\n      \nYou add the ExtractKey transform and give Connect the name of the transformer\nclass: ValueToKey. You also provide the name of the field to use for the key: symbol.\nThis could consist of multiple comma-separated values, but in this case, you need only\none value. Note that you use the renamed version for the field, because this trans-\nformer is executed after the Rename transformer.\n The result of the ExtractKey field is a struct of one value. But you only want the\nvalue contained in the struct for the key—the stock ticker symbol. For this operation,\nyou’ll add a FlattenStruct transform to pull the ticker symbol out by itself (see\nsrc/main/resources/conf/connector-jdbc.properties).\nListing 9.2\nKafka Streams source topic populated with data from Connect\nListing 9.3\nUpdated JDBC connector properties\nSerde for the \nStockTransaction object\nUses the topic Connect \nwrites to as the source \nfor the stream\nPrints messages out to the console\nAdds the \nExtractKey \ntransform\nSpecifies the \nclass name of \nthe ExtractKey \ntransform\nLists the field(s) to \nextract for the key\n \n\n\n225\nIntegrating Kafka with other data sources\ntransforms=ConvertDate,Rename,ExtractKey,FlattenStruct\n    \ntransforms.ConvertDate.type=\n➥ org.apache.kafka.connect.transforms.TimestampConverter$Value\ntransforms.ConvertDate.field=TXNTS\ntransforms.ConvertDate.target.type=string\ntransforms.ConvertDate.format=yyyy-MM-dd'T'HH:mm:ss.SSS-0400\ntransforms.Rename.type=\n➥ org.apache.kafka.connect.transforms.ReplaceField$Value\ntransforms.Rename.renames=SMBL:symbol, SCTR:sector,....\ntransforms.ExtractKey.type=org.apache.kafka.connect.transforms.ValueToKey\ntransforms.ExtractKey.fields=symbol\ntransforms.FlattenStruct.type=\n➥ org.apache.kafka.connect.transforms.ExtractField$Key     \ntransforms.FlattenStruct.field=symbol\n  \nYou add the final transform (FlattenStruct) and specify the ExtractField$Key class,\nwhich is used by Connect to extract the named field and only include that field in the\nresults (in this case, the key). Finally, you provide the name of the field (symbol),\nwhich is the same as in the previous transform; this makes sense, because that’s the\nfield used to create the key struct.\n With just a few extra lines of configuration, you can expand the previous Kafka\nStreams application to do more-advanced operations without the need to select a key\nand do the repartitioning step (found in src/main/java/bbejeck/chapter_9/Stock-\nCountsStreamsConnectIntegrationApplication.java).\nSerde<StockTransaction> stockTransactionSerde =\n➥ StreamsSerdes.StockTransactionSerde();\nStreamsBuilder builder = new StreamsBuilder();\nbuilder.stream(\"dbTxnTRANSACTIONS\",\n➥ Consumed.with(stringSerde, stockTransactionSerde))\n.peek((k, v)->\n➥ LOG.info(\"transactions from database key {}value {}\", k, v))\n.groupByKey(\n➥ Serialized.with(stringSerde,stockTransactionSerde))\n     \n.aggregate(()-> 0L,(symb, stockTxn, numShares) ->\n➥ numShares + stockTxn.getShares(),                    \nMaterialized.with(stringSerde, longSerde)).toStream(\n)\n.peek((k,v) -> LOG.info(\"Aggregated stock sales for {} {}\",k, v))\n.to( \"stock-counts\", Produced.with(stringSerde, longSerde));\nBecause the data is coming in keyed, you can use groupByKey, which won’t set the\nautomatic repartitioning flag. From the group-by operation, you can directly go into\nan aggregation without performing a repartitioning step, which is important for per-\nformance reasons. The README.md file included with the source code contains\nListing 9.4\nAdding a transform\nListing 9.5\nProcessing transactions from a table in Kafka Streams via Connect\nAdds \nthe last \ntransform\nSpecifies the class \nfor the transform \n(ExtractField$Key)\nName of the field \nto pull out\nGroups by key\nPerforms an \naggregation \nof the total \nnumber of \nshares sold\n \n\n\n226\nCHAPTER 9\nAdvanced applications with Kafka Streams\ninstructions for running an embedded H2 database (www.h2database.com/html/\nmain.html) and Kafka Connect to produce data for the dbTxnTRANSACTIONS topic to\nrun the streaming application.\nTIP\nAlthough it might seem tempting to use Transformations to perform all\nthe work when importing data into Kafka via Connect, remember that trans-\nformations are meant to be lightweight. For any transformations beyond the\nsimple ones shown in the examples, it’s better to pull the data into Kafka and\nthen use Kafka Streams to do the heavy transformational work.\nNow that you’ve seen how to use Kafka Connect to get data into Kafka for processing\nwith Kafka Streams, let’s turn our attention to how you can visualize the state of data\nin real time. \n9.2\nKicking your database to the curb\nIn chapter 4, you learned how to add local state to a Kafka Streams application. Stream-\ning applications need to use state to perform operations like aggregations, reduces,\nand joins. Unless your streaming application works exclusively with individual records,\nhaving local state is required.\n Going back to the requirements from BSE, you’ve developed a Kafka Streams\napplication that captures three categories of equity transactions:\nTotal transactions by market sector\nCustomer purchases of shares per session\nTotal number of shares traded by stock symbol, in tumbling windows of\n10 seconds\nIn the examples so far, you’ve either reviewed the results in the console or read them\nfrom a sink topic. Viewing data in the console is suitable for development, but the con-\nsole isn’t the best medium for displaying results. To do any analytic work or quickly\nunderstand what’s going on, a dashboard application is a better medium to use.\n In this section, you’ll see how you can use interactive queries in Kafka Streams to\ndevelop a dashboard application for viewing analytics, without the need for a relational\ndatabase to hold the state. You’ll populate the dashboard application directly from\nKafka Streams, as the data streams. Hence, the app will continually update naturally.\n In a typical architecture, data that’s captured and operated on is pushed out to a\nrelational database for viewing. Figure 9.4 shows this setup: pre-Kafka Streams, you\ningested data with Kafka, fed an analysis engine, and then pushed the data to a rela-\ntional database table used by a dashboard application.\n If you add Kafka Streams, using local state, the architecture changes slightly, as\nshown in figure 9.5. You can simplify the structure significantly by removing an entire\ncluster (not to mention that the deployment is much more manageable). Kafka\nStreams still writes back to Kafka, and the database is still the primary consumer of the\ntransformed data.\n \n\n\n227\nKicking your database to the curb\nIn chapter 5, I talked about interactive queries. Let’s revisit the definition briefly:\ninteractive queries let you directly view the data in the state store without having to con-\nsume the data from Kafka. In other words, the stream becomes the database as well.\n Because a picture is worth a thousand words, let’s take another look at figure 9.5,\nbut adjusted in figure 9.6 to use interactive queries.\nKafka\nProcessing cluster\nDatabase\nWeb browser/REST\nservice\nThe processing\ncluster pushes results\nout to a database.\nAn external browser using\na REST service connects\nto the data store to view\nthe processing results.\nThe processing cluster\ningests data from Kafka.\nFigure 9.4\nArchitecture of applications viewing processed data prior to Kafka \nStreams\nKafka\nKafka Streams\nWeb browser/REST\nservice\nDatabase\nExternal\napplication\nYou simplify things, removing the need\nfor a separate cluster, by running\na Kafka Streams application.\nAn external app\nconsumes the results\nand writes them\nto a database.\nFigure 9.5\nArchitecture with Kafka Streams and state added\n \n\n\n228\nCHAPTER 9\nAdvanced applications with Kafka Streams\nThe idea demonstrated here is simple but powerful. While state stores hold the state\nof the stream, Kafka Streams provides read-only access from outside the streaming\napplication via a RESTful interface. It’s worth stating again how powerful this con-\ncept is; you can view the running state of the stream without the need for an exter-\nnal database.\n Now that you have an understanding of the impact of interactive queries, let’s walk\nthrough how they work.\n9.2.1\nHow interactive queries work\nFor interactive queries to work, Kafka Streams exposes state stores in a read-only wrap-\nper. It’s important to understand that while Kafka Streams makes the stores available\nfor queries, there’s no updating or modifying the state store in any way. Kafka Streams\nexposes state stores from the KafkaStreams.store method.\n Here’s how this method works:\nReadOnlyWindowStore readOnlyStore =\n➥ kafkaStreams.store(storeName, QueryableStoreTypes.windowStore());\nThis example retrieves a WindowStore. In addition, QueryableStoreTypes provides\ntwo other types:\n\nQueryableStoreTypes.sessionStore()\n\nQueryableStoreTypes.keyValueStore()\nOnce you have a reference to the read-only store, it’s just a matter of exposing the\nstore to a service (a RESTful service, for example) for users to query the state of the\nKafka\nKafka Streams\nWeb browser/REST\nservice\nState store\nNow, you really simplify your architecture,\nbecause you remove the processing\ncluster\nthe database.\nand\nThe Kafka Streams application consumes\nfrom the broker, and local state captures\nthe current state of the stream.\nThe REST service now connects directly\nto the local state store, retrieving live\nresults from the stream.\nand\nFigure 9.6\nArchitecture using interactive queries\n \n\n\n229\nKicking your database to the curb\nstreaming data. But retrieving the state store is only part of the picture. The state store\nextracted here will only contain keys contained in the local store.\nNOTE\nRemember, Kafka Streams assigns a state store per task, and as long as\nyou use the same application ID, a Kafka Streams application can consist of\nmultiple instances. Additionally, those instances need not all be located on\nthe same host. Thus, it’s possible that the state store you query may contain\nonly a subset of all the keys; other state stores (with the same name, located\non other machine[s]) may contain another subset of the keys.\nLet’s use the analytics listed earlier to make this concept clear. \n9.2.2\nDistributing state stores\nConsider the first analytic: aggregating stock trades per market sector. Because you’re\ndoing an aggregation, state stores come into play. You want to expose the state stores\nto provide a real-time view of the number of trades per sector, to gain insight into\nwhich segment of the market is seeing the most action at the moment.\n Stock market activity generates significant data volume, but I’ll only discuss using\ntwo partitions, to keep the details of the example clear. Additionally, let’s say you’re\nrunning two single-threaded instances on two separate machines, located in the same\ndata center. Because of Kafka Streams’ automatic load balancing, each application will\nhave one task processing the data from each partition of the input topic.\n Figure 9.7 shows the distribution of tasks and state stores. As you can see, instance\nA handles all records on partition 0, and instance B handles all records on partition 1.\nStreams app A\nStreams app B\nState store\nState store\nThe assigned task for Kafka\nStreams application instance B\nis TopicPartition T .1\nProcesses messages\non partition 0\nProcesses messages\non partition 1\nThe assigned task for Kafka\nStreams application instance A\nis TopicPartition T0.\nKafka topic T with\ntwo partitions\nFigure 9.7\nTask and state store distribution\n \n\n\n230\nCHAPTER 9\nAdvanced applications with Kafka Streams\nFigure 9.8 illustrates what happens when you have two records with the keys \"Energy\"\nand \"Finance\".\n\"Energy\":\"100000\" lands in the state store on instance A, and \"Finance\":\"110000\"\nends up in the state store on instance B. Going back to the example of exposing the\nstate store for queries, you can clearly see that if you expose the state store on instance\nA to a web service or any external querying, you can only retrieve the value for the key\n\"Energy\".\n What’s the solution to this issue? You certainly don’t want to set up an individual\nweb service to query each instance—that approach won’t scale. Fortunately, you don’t\nhave to: the solution is as simple as setting a configuration. \n9.2.3\nSetting up and discovering a distributed state store\nTo enable interactive queries, you need to set the StreamsConfig.APPLICATION\n_SERVER_CONFIG parameter. It consists of the hostname of the Kafka Streams applica-\ntion and the port that a query service will listen on, in hostname:port format.\n When a Kafka Streams instance receives a query for a given key, you’ll need to find\nout whether the key is contained in the local store. More important, if it’s not local,\nyou’ll want to find out which instance contains the key and query against that store.\n Several methods on the KafkaStreams object allow for retrieving information for\nall running instances with the same application ID and defining the APPLICATION\n_SERVER_CONFIG. Table 9.1 lists the method names and descriptions.\n You can use KafkaStreams.allMetadata to obtain information for all instances\nthat are eligible for interactive queries. I find that KafkaStreams.allMetadataForKey\nis the method I use most when writing interactive queries.\nStreams app A\nStreams app B\nState store\nState store\nPartition 0\nPartition 1\n{\"Energy\":\" 00000\"} is written\n1\nto partition 0 and stored in the\nstate store in app instance A.\n{\"Finance\":\"\n0000\"}  is written\n11\nto partition\nand stored in the\n1\nstate store in app instance B.\nFigure 9.8\nKey and value distribution in state stores\n \n\n\n231\nKicking your database to the curb\nNext, let’s take another look at the key/value distribution across Kafka Streams\ninstances, adding the sequence of checking for the \"Finance\" key, which is found and\nreturned from another instance (see figure 9.9). Each Kafka Streams instance has a\nlightweight embedded server listening to the port specified in APPLICATION_SERVER\n_CONFIG.\nIt’s important to point out that you’ll need to query only one of the Kafka Streams\ninstances, and which one you choose doesn’t matter (assuming you’ve configured the\napplication correctly). By using an RPC mechanism and metadata-retrieval methods,\nif the instance you’ve queried doesn’t contain the data you’re looking for, the queried\nKafka Streams instance will find where it’s located, pull the results, and return the\nresults to the original query.\nTable 9.1\nMethods for retrieving store metadata\nName\nParameter(s)\nUsage\nallMetadata\nN/A\nAll instances, some possibly remote\nallMetadataForStore\nStore name\nAll instances (some remote) contain-\ning the named store\nallMetadataForKey\nKey, Serializer\nAll instances (some remote) with the \nstore containing the key\nallMetadataForKey\nKey, StreamPartitioner\nAll instances (some remote) with the \nstore containing the key\nStreams app A\nHost = hostA:4567\nStreams app B\nHost = hostB:4568\n{\"Energy\":\"100000\"}\nState store\nState store\n{\"Finance\":\"110000\"}\nA query comes into application instance A\nfor the key \"Finance\". Using metadata,\ninstance A is able to determine that the\nkey \"Finance\" is located on instance B.\nThe value is retrieved from\nthe state store on instance B and\nreturned to satisfy the original\nrequest on instance A.\nInstance A uses host and port info\nfrom metadata to retrieve the value\nfor \"Finance\" from instance B.\nFigure 9.9\nKey and value query-and-discovery process\n \n\n\n232\nCHAPTER 9\nAdvanced applications with Kafka Streams\n You can see this in action by tracing the flow of the calls in figure 9.9. Instance A\ndoesn’t contain the key \"Finance\" but discovers that instance B does contain the key.\nSo, A issues a call to the embedded server on B, which retrieves the data and returns\nthe result to the original caller.\nNOTE\nInteractive queries will work on a single node out of the box, but an\nRPC mechanism isn’t provided—you have to implement your own. This sec-\ntion offers one possible solution, but you’re free to implement your own pro-\ncess, and I’m sure many of you will come up with something better. A great\nexample of another RPC implementation is located in the Confluent kafka-\nstreams-examples GitHub repo: http://mng.bz/Ogo3.\nLet’s move on to see interactive queries in action. \n9.2.4\nCoding interactive queries\nThe application you’ll write for interactive queries will look very similar to the other\napps you’ve written so far, with a couple of small changes. The first difference is that\nyou need to pass in two arguments when launching the Kafka Streams application: the\nhostname and the port the embedded service will listen to (found in src/main/java/\nbbejeck/chapter_9/StockPerformanceInteractiveQueryApplication.java).\npublic static void main(String[] args) throws Exception {\nif(args.length < 2){\nLOG.error(\"Need to specify host and port\");\nSystem.exit(1);\n}\nString host = args[0];\nint port = Integer.parseInt(args[1]);\nfinal HostInfo hostInfo = new HostInfo(host, port);\n   \nProperties properties = getProperties();\nproperties.put(\n➥ StreamsConfig.APPLICATION_SERVER_CONFIG,host+\":\"+port);   \n// other details left out for clarity\nUntil this point, you’ve fired up the application without a second thought. Now, you\nneed to provide two arguments (the host and the port), but this change has minimal\nimpact.\n You also embed the local server for performing actual queries: for this implemen-\ntation, I’ve chosen to use the Spark web server (http://sparkjava.com). (Not that\nSpark—this is a book about Kafka Streams, after all!) My motivation for going with the\nSpark web server is its small footprint, its convention-over-configuration approach,\nand the fact that it’s purpose-built for microservices—and a microservice is what you\nListing 9.6\nSetting the hostname and port\nCreates a \nHostInfo object \nfor later use in \nthe application\nSets the config \nfor enabling \ninteractive \nqueries\n \n",
      "page_number": 232
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 248-258)",
      "start_page": 248,
      "end_page": 258,
      "detection_method": "topic_boundary",
      "content": "233\nKicking your database to the curb\ncan provide by using interactive queries. If the Spark web server isn’t to your liking,\nfeel free to replace it with another web server.\nNOTE\nI think most readers will be familiar with the term microservice, but\nhere’s the best definition I’ve seen, from http://microservices.io: “Microser-\nvices—also known as the microservice architecture—is an architectural style\nthat structures an application as a collection of loosely coupled services,\nwhich implement business capabilities. The microservice architecture enables\nthe continuous delivery/deployment of large, complex applications. It also\nenables an organization to evolve its technology stack.”\nNow, let’s look at the point in the code where you embed the Spark server, and some\nof the supporting code used to manage it (found in src/main/java/bbejeck/chapter_9/\nStockPerformanceInteractiveQueryApplication.java).\n// details left out for clarity\nKafkaStreams kafkaStreams = new KafkaStreams(builder.build(), streamsConfig);\nInteractiveQueryServer queryServer =\n➥ new InteractiveQueryServer(kafkaStreams, hostInfo);    \nqueryServer.init();\nkafkaStreams.setStateListener(((newState, oldState) -> {\n \nif (newState == KafkaStreams.State.RUNNING && oldState ==\n➥ KafkaStreams.State.REBALANCING) {\nLOG.info(\"Setting the query server to ready\");\nqueryServer.setReady(true);\n                \n} else if (newState != KafkaStreams.State.RUNNING) {\nLOG.info(\"State not RUNNING, disabling the query server\");\nqueryServer.setReady(false);\n}\n}));\nkafkaStreams.setUncaughtExceptionHandler((t, e) -> {\nLOG.error(\"Thread {} had a fatal error {}\", t, e, e);\nshutdown(kafkaStreams, queryServer);\n  \n});\nRuntime.getRuntime().addShutdownHook(new Thread(() -> {\nshutdown(kafkaStreams, queryServer);\n \n}));\nIn this code, you create an instance of InteractiveQueryServer, which is a wrapper\nclass containing the Spark web server and the code to manage the web service calls\nand start and stop the web server.\nListing 9.7\nInitializing the web server and setting its status\nCreates the embedded\nweb server (actually a\nwrapper class)\nAdds a StateListener to only enable \nqueries to state stores until ready\nEnables queries to \nstate stores once \nthe Kafka Streams \napplication is in a \nRUNNING state. \nQueries are \ndisabled if the state \nisn’t RUNNING.\nSets an Uncaught-\nExceptionHandler to \nlog unexpected errors \nand close everything \ndown\nAdds a shutdown \nhook to close \neverything down \nwhen the application \nexits normally\n \n\n\n234\nCHAPTER 9\nAdvanced applications with Kafka Streams\n Chapter 7 discussed using a StateListener for notifications about various states of\na Kafka Streams application. Here you can see an efficient use of this listener. Recall\nthat when running an interactive query, you need to use an instance of StreamsMeta-\ndata to determine whether the data for the given key is local to the instance process-\ning the query. You set the state of the query server to true, allowing access to the\nmetadata needed for queries only if the application is in the RUNNING state.\n A key point to keep in mind is that the metadata returned is a snapshot of the\nmakeup of the Kafka Streams application. At any point in time, you can scale the\napplication up (or down). When this occurs (or when any other qualifying event takes\nplace, such as adding topics with a regex source node), the Kafka Streams application\ngoes through a rebalancing phase and may change partition assignments. In this case,\nyou’re allowing queries only in the RUNNING state, but feel free to use whatever strategy\nyou think is appropriate.\n Next is another example of a concept covered in chapter 7: setting an Uncaught-\nExceptionHandler. In this case, you log the error and shut down the application and\nthe query server. Because this application runs indefinitely, you add a shutdown hook\nto close everything down once you stop the demo.\n Now that you’ve seen how to instantiate and start the service, let’s move on to the\ncode for running the query server. \n9.2.5\nInside the query server\nWhen implementing your RESTful service, the first step is to map URL paths to the\ncorrect methods to execute (found in src/main/java/bbejeck/webserver/Interactive-\nQueryServer.java).\npublic void init() {\nLOG.info(\"Started the Interactive Query Web server\");\nget(\"/kv/:store\", (req, res) ->\nready ?\n➥ fetchAllFromKeyValueStore(req.params()) :\n➥ STORES_NOT_ACCESSIBLE);                \nget(\"/session/:store/:key\", (req, res) -> ready ?\n➥ fetchFromSessionStore(req.params()) :\n➥ STORES_NOT_ACCESSIBLE);                    \nget(\"/window/:store/:key\", (req, res) -> ready ?\n➥ fetchFromWindowStore(req.params()) :\n➥ STORES_NOT_ACCESSIBLE);                   \nget(\"/window/:store/:key/:from/:to\",(req, res) -> ready ?\n➥ fetchFromWindowStore(req.params()) :\n➥ STORES_NOT_ACCESSIBLE);          \n}\nThis code highlights the decision to go with the Spark web server: you can concisely\nmap URLs to a Java 8 lambda expression to handle the request. These mappings are\nListing 9.8\nMapping URL paths to methods\nMapping to retrieve \nall values from a plain \nkey/value store\nMapping to return all \nsessions (from a session \nstore) for a given key\nMapping for a \nwindow store with \nno times specified\nMapping for a window store \nwith from and to times\n \n\n\n235\nKicking your database to the curb\nstraightforward, but notice that you map the retrieval from the window store twice. To\nretrieve values from a window store, you need to provide a from time and a to time.\n In the URL mappings, notice the check for the ready Boolean value. This value is\nset in StateListener. If ready evaluates to false, you don’t attempt to process the\nrequest, and you return a message that the stores aren’t currently accessible. This\nmakes sense because a window store is segmented by time, with the segment size estab-\nlished when you create the store. (We covered windowing in section 5.3.2.) But I’m\ncheating here and offering you a method that accepts only a key and a store and pro-\nvides default from and to times that we’ll explore in the next example.\nNOTE\nThere’s a proposal (KIP-205, http://mng.bz/lI9Y) to extend ReadOnly-\nWindowStore to provide an all() method that retrieves all time segments by\nkey, alleviating the need to specify from and to times. This functionality hasn’t\nbeen implemented yet but should be included in a future release.\nAs an example of how the interactive query service works, let’s walk through retrieving\nfrom a windowed store. Although we’ll only look at one example, the source code\ncontains instructions to run all types of queries.\nCHECKING FOR THE STATE STORE LOCATION\nYou’ll remember that you need to collect various metrics on BSE’s securities sales to\nprovide stock-transaction data analysis. You decide to first track sales of individual\nstocks, keeping a running total over 10-second windows to identify stocks that may\ntrend up or down.\n You’ll use the following mapping to walk through the example, from reviewing the\nrequest to returning the response:\nget(\"/window/:store/:key\", (req, res) -> ready ?\n➥ fetchFromWindowStore(req.params()) : STORES_NOT_ACCESSIBLE);\nTo help you keep your place in the query process, let’s use figure 9.9 as a roadmap.\nYou’ll start by sending an HTTP get request http://localhost:4567/window/Number-\nSharesPerPeriod/XXXX, where XXXX represents the ticker symbol for a given stock\n(found in src/main/java/bbejeck/webserver/InteractiveQueryServer.java).\nprivate String fetchFromWindowStore(Map<String, String> params) {\nString store = params.get(STORE_PARAM);\nString key = params.get(KEY_PARAM);\nString fromStr = params.get(FROM_PARAM);\nString toStr = params.get(TO_PARAM);\n  \nHostInfo storeHostInfo = getHostInfo(store, key);   \nif(storeHostInfo.host().equals(\"unknown\")){\n  \nreturn STORES_NOT_ACCESSIBLE;\n}\nListing 9.9\nMapping the request and checking for the key location\nExtracts the request \nparameters\nGets the HostInfo \nfor the key\nIf the hostname is \n\"unknown\", returns an \nappropriate message\n \n\n\n236\nCHAPTER 9\nAdvanced applications with Kafka Streams\nif(dataNotLocal(storeHostInfo)){\n   \nLOG.info(\"{} located in state store on another instance\", key);\nreturn fetchRemote(storeHostInfo,\"window\", params);\n}\nThe request is mapped to the fetchFromWindowStore method. The first step is to pull\nout the store name and key (stock symbol) from the request-parameters map. You\nfetch the HostInfo object for the key in the request, and you use the hostname to\ndetermine whether the key is located on this instance or a remote one.\n Next, you check whether the Kafka Streams instance is (re)initializing, which is\nindicated by the host() method returning \"unknown\". If so, you stop processing the\nrequest and return a \"not accessible\" message.\n Finally, you check whether the hostname matches the hostname for the current\ninstance. If the hostname doesn’t match, you get the data from the instance contain-\ning the key and return the results.\n Next, let’s look at how you retrieve and format the results (found in src/main/\njava/bbejeck/webserver/InteractiveQueryServer.java).\nInstant instant = Instant.now();\nlong now = instant.toEpochMilli();\n  \nlong from =\nfromStr !=\n➥ null ? Long.parseLong(fromStr) : now - 60000;         \nlong to =\ntoStr != null ? Long.parseLong(toStr) : now;   \nList<Integer> results = new ArrayList<>();\nReadOnlyWindowStore<String, Integer> readOnlyWindowStore =\n➥ kafkaStreams.store(store,\n➥ QueryableStoreTypes.windowStore());    \ntry(WindowStoreIterator<Integer> iterator =\n➥ readOnlyWindowStore.fetch(key, from , to)){\n  \nwhile (iterator.hasNext()) {\nresults.add(iterator.next().value);  \n}\n}\nreturn gson.toJson(results);\n  \nI mentioned earlier that you’ll cheat on the window store query if the from and to\nparameters aren’t provided in the query. If the user doesn’t specify a range, by default\nyou return the last minute of results from the window store. Because you’ve defined a\nwindow of 10 seconds, you’ll return six-windowed results. After you fetch the window\nsegments from the store, you iterate over them, building a response that indicates the\nnumber of shares purchased for each 10-second interval over the last minute. \nListing 9.10\nRetrieving and formatting the results\nChecks whether the returned hostname\nmatches the host of this instance\nGets the current \ntime in milliseconds\nSets the window segment \nstart time or, if not \nprovided, the time as \nof one minute ago\nSets the window \nsegment ending time \nor, if not provided, \nthe current time\nRetrieves the ReadOnlyWindowStore\nFetches the window segments\nBuilds up the response\nConverts the results to JSON \nand returns to the requestor\n \n\n\n237\nKSQL\nRUNNING THE INTERACTIVE QUERY EXAMPLE\nTo observe the results of this example, you need to run three commands:\n\n./gradlew runProducerInteractiveQueries produces the data needed for the\nexamples.\n\n./gradlew runInteractiveQueryApplicationOne starts a Kafka Streams appli-\ncation with HostInfo using port 4567.\n\n./gradlew runInteractiveQueryApplicationTwo starts a Kafka Streams appli-\ncation with HostInfo using port 4568.\nThen, point your browser to http://localhost:4568/window/NumberSharesPerPeriod/\nAEBB. Click Refresh a few time to see different results. Here’s a static list of company\nsymbols for this example: AEBB, VABC, ALBC, EABC, BWBC, BNBC, MASH, BARX, WNBC,\nWKRP. \nRUNNING A DASHBOARD APPLICATION FOR INTERACTIVE QUERIES\nA better example is a mini-dashboard web application that updates automatically (via\nAjax) and displays the results from four different Kafka Streams aggregation opera-\ntions. By running the commands listed in the previous subsection, you have every-\nthing set up; point your browser to localhost:4568/iq or localhost:4567/iq to run the\ndashboard application. By going to either instance, you’ll see how Kafka Stream’s\ninteractive queries handle getting results from all instances with the same application\nID. Look in the README file in the source code for full instructions on how to set up\nand start the dashboard application.\n As you can see from observing the web application, you can view live results of the\nstream in a dashboard-like application. Previously, this type of application required a\nrelational database; but here, Kafka Streams provides the information as needed.\n We’ve wrapped up our coverage of interactive queries. Let’s move on to KSQL: an\nexciting new tool that Confluent (the company founded by the original developers of\nKafka at LinkedIn) recently released, which allows you to specify long-running queries\nagainst records streaming into Kafka without code, but using SQL. \n9.3\nKSQL\nImagine you’re working with business analysts at BSE. The analysts are interested in\nyour ability to quickly write applications in Kafka Streams to perform real-time data\nanalysis. This interest puts you in a bind.\n You want to work with the analysts and write applications for their requests, but\nyou also have your normal workload—the additional work makes it hard to keep up\nwith everything. The analysts understand the added work they’re creating, but they\ncan’t write code, so they depend on you to write their analytics.\n The analysts are experts on working with relational databases and thus are com-\nfortable with SQL queries. If there were some way to give the analysts a SQL layer over\nKafka Streams, everyone’s productivity would increase. Well, now there is.\n \n\n\n238\nCHAPTER 9\nAdvanced applications with Kafka Streams\n In August 2017, Confluent unveiled a powerful new tool for stream processing:\nKSQL (https://github.com/confluentinc/ksql#-ksql). KSQL is a streaming SQL engine\nfor Apache Kafka, providing an interactive SQL interface that you can use to write\npowerful stream-processing queries without writing code. KSQL is especially adept at\nfraud detection and real-time applications.\nNOTE\nKSQL is a big topic and could take a chapter or two if not an entire\nbook on its own. So, the coverage here will be concise. Fortunately, you’ve\nalready learned the core concepts underpinning KSQL, because it uses Kafka\nStreams under the covers. For more information, see the KSQL documenta-\ntion (http://mng.bz/zw3F).\nKSQL provides scalable, distributed stream processing, including aggregations, joins,\nwindowing, and more. Additionally, unlike SQL run against a database or a batch-\nprocessing system, the results of a KSQL query are continuous. Before we dive into\nwriting streaming queries, let’s take a minute to review some fundamental concepts\nof KSQL.\n9.3.1\nKSQL streams and tables\nSection 5.1.3 discussed the concept of an event stream versus an update stream. An event\nstream is an unbounded stream of individual independent events, where an update or\nrecord stream is a stream of updates to previous records with the same key.\n KSQL has a similar concept of querying from a Stream or a Table. A Stream is an\ninfinite series of immutable events or facts, but with a query on a Table, the facts are\nupdatable or can even be deleted.\n Although some of the terminology is different, the concepts are pretty much the\nsame. If you’re comfortable with Kafka Streams, you’ll feel right at home with KSQL. \n9.3.2\nKSQL architecture\nKSQL uses Kafka Streams under the covers to build and fetch the results of queries.\nKSQL is made up of two components: a CLI and a server. Users of standard SQL tools\nsuch as MySQL, Oracle, and even Hive will feel right at home with the CLI when writ-\ning queries in KSQL. Best of all, KSQL is open source (Apache 2.0 licensed).\n The CLI is also the client connecting to the KSQL server. The KSQL server is\nresponsible for processing the queries and retrieving data from Kafka as well as writ-\ning results into Kafka.\n KSQL runs in two modes: standalone, which is useful for prototyping and develop-\nment; and distributed, which of course is how you’d use KSQL when working in a more\nrealistic-size data environment. Figure 9.10 shows how KSQL works in local mode. As\nyou can see, the KSQL CLI, REST server, and KSQL engine are all located on the\nsame JVM, which is ideal when running on your laptop.\n Now, let’s look at KSQL in distributed mode; see figure 9.11. The KSQL CLI is by\nitself, and it will connect to one of the remote KSQL servers (we’ll cover starting and\n \n\n\n239\nKSQL\nconnections in the next section). A key point is that although you only explicitly con-\nnect to one of the remote KSQL servers, all servers pointing to the same Kafka cluster\nwill share in the workload of the submitted query.\nKafka\nKSQL CLI\nREST\ninterface\nKSQL\nengine\nJVM\nThe KSQL CLI and KSQL engine\nare in one JVM locally.\nFigure 9.10\nKSQL in local mode\nKafka\nKSQL CLI\nREST interface\nKSQL engine\nREST interface\nKSQL engine\nREST interface\nKSQL engine\nEach of the\nKSQL engines is in a\nseparate JVM.\nFigure 9.11\nKSQL in distributed mode\n \n\n\n240\nCHAPTER 9\nAdvanced applications with Kafka Streams\nNote that the KSQL servers are using Kafka Streams to execute the queries. This\nmeans that if you need more processing power, you can stand up another KSQL\nserver, even during live operations (just like you can spin up another Kafka Streams\napplication). The opposite case works just as well: if you have excess capacity, you can\nstop any number of KSQL servers, with the assumption that you’ll leave at least one\nserver operational. Otherwise, your queries will stop running!\n Next, let’s see how you get KSQL installed and running. \n9.3.3\nInstalling and running KSQL\nTo install KSQL, you’ll clone the KSQL repo with the command git clone\ngit@github.com:confluentinc/ksql.git and then cd into the ksql directory and\nexecute mvn clean package to build the entire KSQL project. If you don’t have git\ninstalled or don’t want to build from source, you can download the KSQL release\nfrom http://mng.bz/765U.\nTIP\nKSQL is an Apache Maven–based project, so you’ll need Maven installed\nto build KSQL. If you don’t have Maven installed and you’re on a Mac and\nhave Homebrew installed, run brew install maven. Otherwise, you can head\nover to https://maven.apache.org/download.cgi and download Maven directly;\ninstallation instructions are at https://maven.apache.org/install.html.\nMake sure you’re in the base directory of the KSQL project before going any further.\nThe next step is to start KSQL in local mode:\n./bin/ksql-cli local\nNote that you’ll be using KSQL in local mode for all the examples, but we’ll still cover\nhow to run KSQL in distributed mode.\n After running the previous command, you should see something like figure 9.12 in\nyour console. Congratulations—you’ve successfully installed and launched KSQL!\nNext, let’s start writing some queries. \nFigure 9.12\nKSQL successful launch result\n \n\n\n241\nKSQL\n9.3.4\nCreating a KSQL stream\nGetting back to your work at BSE, you’ve been approached by an analyst who is inter-\nested in one of the applications you’ve written and would like to make some tweaks to\nit. But instead of this request resulting in more work, you spin up a KSQL console and\nturn the analyst loose to reconstruct your application as a SQL statement!\n The example you’re going to convert is the last windowed stream from the interac-\ntive queries example found in src/main/java/bbejeck/chapter_9/StockPerformance-\nInteractiveQueryApplication.java, lines 96-103. In that application, you track the number\nof shares sold every 10 seconds, by company ticker symbol.\n You already have the topic defined (the topic maps to a database table) and a\nmodel object, StockTransaction, where the fields on the object map to columns in\na table. Even though the topic is defined, you need to register this information with\nKSQL by using a CREATE STREAM statement in src/main/resources/ksql/create_\nstream.txt.\nCREATE STREAM stock_txn_stream (symbol VARCHAR, sector VARCHAR, \\  \nindustry VARCHAR, shares BIGINT, sharePrice DOUBLE, \\\n      \ncustomerId VARCHAR, transactionTimestamp STRING, purchase BOOLEAN) \\\nWITH (VALUE_FORMAT = 'JSON', KAFKA_TOPIC = 'stock-transactions');  \nWith this one statement, you create a KSQL Streams instance that you can issue\nqueries against. The WITH clause has two required parameters: VALUE_FORMAT, tell-\ning KSQL the format of the data, and KAFKA_TOPIC, telling KSQL where to pull the\ndata from. There are two additional parameters you can use in the WITH clause\nwhen creating a stream. The first is TIMESTAMP, which associates the message time-\nstamp with a column in the KSQL stream. Operations requiring a timestamp, such\nas windowing, will use this column to process the record. The other parameter is\nKEY, which associates the key of the message with a column on the defined stream.\nIn this case, the message key for the stock-transactions topic matches the symbol\nfield in the JSON value, so you don’t need to specify the key. Had this not been the\ncase, you would have needed to map the key to a named column, because you\nalways need a key to perform grouping operations. You’ll see this when you execute\nthe stream SQL.\nTIP\nThe KSQL command list topics; shows a list of topics on the broker\nthe KSQL CLI is pointing to and whether the topics are registered.\nListing 9.11\nCreating a stream\nCREATE STREAM statement to create a\nstream named stock_txn_stream\nRegisters the fields of the\nStockTransaction object\nas columns\nSpecifies the data format and the Kafka \ntopic serving as the source of the stream \n(both required parameters)\n \n\n\n242\nCHAPTER 9\nAdvanced applications with Kafka Streams\nYou can view all streams and verify that KSQL created the new stream as expected with\nthe following commands:\nshow streams;\ndescribe stock_txn_stream;\nThe results are shown in figure 9.13. Notice that KSQL inserted two extra columns:\nROWTIME and ROWKEY. The ROWTIME column is the timestamp placed on the message\n(either from the producer or by the broker), and ROWKEY is the key (if any) of the\nmessage.\nNow, let’s run the query on this stream.\nNOTE\nYou’ll need to run ./gradlew runProducerInteractiveQueries to\nprovide data for the KSQL examples \n9.3.5\nWriting a KSQL query\nThe SQL query for performing the stock analysis is as follows:\nSELECT symbol, sum(shares) FROM stock_txn_stream\n➥ WINDOW TUMBLING (SIZE 10 SECONDS) GROUP BY symbol;\nRun this query, and you’ll see results similar\nto those shown in figure 9.14. The column\non the left is the ticker symbol, and the num-\nber is the number of shares traded for that\nsymbol over the last 10 seconds. With this\nquery, you specify a tumbling window of 10\nseconds, but KSQL supports session and hopping windows, as well, as we discussed in\nsection 5.3.2.\nFigure 9.13\nListing streams, and \ndescribing your newly created stream\nFigure 9.14\nResults of \nthe tumbling window query\n \n\n\n243\nKSQL\n You’ve built a streaming application without writing any code—quite an achieve-\nment. For a comparison, let’s look at the corresponding application written in the\nKafka Streams API.\nKStream<String, StockTransaction> stockTransactionKStream =\n➥ builder.stream(MockDataProducer.STOCK_TRANSACTIONS_TOPIC,\nConsumed.with(stringSerde, stockTransactionSerde)\n.withOffsetResetPolicy(Topology.AutoOffsetReset.EARLIEST));\nAggregator<String, StockTransaction, Integer> sharesAggregator =\n➥ (k, v, i) -> v.getShares() + i;\nstockTransactionKStream.groupByKey()\n.windowedBy(TimeWindows.of(10000))\n.aggregate(() -> 0, sharesAggregator,\nMaterialized.<String, Integer,\nWindowStore<Bytes,\nbyte[]>>as(\"NumberSharesPerPeriod\")\n.withKeySerde(stringSerde)\n.withValueSerde(Serdes.Integer()))\n.toStream().\n➥ peek((k,v)->LOG.info(\"key is {} value is{}\", k, v));\nEven though the Kafka Streams API is concise, the equivalent you wrote in KSQL is a\none-liner query. Before we wrap up our coverage of KSQL, let’s discuss some additional\nfeatures of KSQL. \n9.3.6\nCreating a KSQL table\nSo far, we’ve demonstrated creating a KSQL stream. Now, let’s see how to create a\nKSQL table, using the stock-transactions topic as the source, for familiarity (found\nin src/main/resources/ksql/create_table.txt).\nCREATE TABLE stock_txn_table (symbol VARCHAR, sector VARCHAR, \\\nindustry VARCHAR, shares BIGINT, \\\nsharePrice DOUBLE, \\\ncustomerId VARCHAR, transactionTimestamp \\\nSTRING, purchase BOOLEAN) \\\nWITH (KEY='symbol', VALUE_FORMAT = 'JSON', \\\nKAFKA_TOPIC = 'stock-transactions');\nOnce you’ve created the table, you can execute queries against it. Keep in mind that\nthe table will contain updates for each transaction by symbol, because the stock-\ntransactions topic is keyed by the ticker symbol.\nListing 9.12\nStock analysis application written in Kafka Streams\nListing 9.13\nCreating a KSQL table\n \n",
      "page_number": 248
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 259-266)",
      "start_page": 259,
      "end_page": 266,
      "detection_method": "topic_boundary",
      "content": "244\nCHAPTER 9\nAdvanced applications with Kafka Streams\n A useful experiment is to pick a ticker symbol from the streaming stock-perfor-\nmance query, and then run the following queries in the KSQL console, and notice the\ndifference in output:\nselect * from stock_txn_stream\nwhere symbol='CCLU';\nselect * from stock_txn_table where symbol='CCLU';\nThe first query produces several results, because it’s a stream of individual events. But\nthe table query returns far fewer results (one record, when I ran the experiment).\nThese results are the expected behavior, because a table represents updates to facts,\nwhereas a stream represents a series of unbounded events. \n9.3.7\nConfiguring KSQL\nKSQL offers the familiar SQL syntax and the ability to write powerful streaming appli-\ncations quickly, but you may have noticed the lack of configuration. This is not to say\nyou can’t configure KSQL. You’re free to override any settings as needed, and any of\nthe stream, consumer, and producer configs that you can set for a Kafka Streams\napplication are available. To view the properties that are currently set, run the show\nproperties; command.\n As an example of setting a property, here’s how you can change auto.offset\n.reset to earliest:\nSET 'auto.offset.reset'='earliest';\nThis is the approach you use to set any property in the KSQL shell. But if you need to\nset several configurations, typing each one into the console isn’t convenient. Instead,\nyou can specify a configuration file on startup:\n./bin/ksql-cli local --properties-file /path/to/configs.properties\nThis has been a quick tour of KSQL, but I hope you can see the power and flexibility it\ngives you for creating streaming applications on Kafka. \nSummary\nBy using Kafka Connect, you can incorporate other data sources into your\nKafka Streams applications.\nInteractive queries are a potent tool: they allow you to see data in a stream as it\nflows through your Kafka Streams application, without the need for a relational\ndatabase.\nThe KSQL language lets you quickly build powerful streaming applications\nwithout code. KSQL promises to deliver the power and flexibility of Kafka\nStreams to workers who aren’t developers.\n \n\n\n245\nappendix A\nAdditional configuration\ninformation\nThis appendix covers common and not-so-common configuration options for a\nKafka Streams application. During the course of the book, you’ve seen several exam-\nples of configuring a Kafka Streams application, but the configurations usually\nincluded only the required (application ID, bootstrap servers) and a handful of other\nconfigs (key and value serdes). In this appendix, I’ll show you some other settings\nthat, although not required, will help you keep your Kafka Streams applications run-\nning smoothly. These options will be presented in somewhat of a cookbook fashion.\nLimiting the number of rebalances on startup\nWhen starting up a Kafka Streams application, if you have multiple instances, the\nfirst instance gets all the topic partitions assigned from the GroupCoordinator on\nthe broker. If you start another instance, a rebalance occurs, removing current\nTopicPartition assignments and reassigning all TopicPartitions across both\nKafka Streams instances. This process is repeated until you’ve started all Kafka\nStreams applications that have the same application ID.\n This is normal operation for a Kafka Streams application. But during a rebal-\nance, processing of records is paused until the rebalance is completed; thus, you’d\nlike to limit the number of rebalances when starting up, if possible.\n With the release of Kafka 0.11.0, a new broker configuration, group.initial\n.rebalance.delay.ms, was introduced. This configuration delays the initial consumer\nrebalance from the GroupCoordinator when a new consumer joins the group by the\namount specified in the group.initial.rebalance.delay.ms configuration. The\ndefault setting is 3 seconds. As other consumers join the group, the rebalance is con-\ntinually delayed by the configured amount (up to a limit of max.poll.interval.ms).\n \n\n\n246\nAPPENDIX A\nAdditional configuration information\nThis benefits Kafka Streams because as you start new instances, the rebalance is\ndelayed until all instances have come online (assuming you’re starting them up one\nafter another). For example, if you start four instances with the appropriate rebal-\nance-delay setting, you should have only one rebalance after all four instances come\nonline—meaning you’ll start processing data more quickly. \nResilience to broker outages\nTo keep your Kafka Streams application resilient in the face of broker failures, here\nare some recommended settings (see listing A.1):\n■\nSet Producer.NUM_RETRIES to Integer.MAX_VALUE.\n■\nSet Producer.REQUEST_TIMEOUT to 305000 (5 minutes).\n■\nSet Producer.BLOCK_MS_CONFIG to Integer.MAX_VALUE.\n■\nSet Consumer.MAX_POLL_CONFIG to Integer.MAX_VALUE.\nProperties props = new Properties();\nprops.put(StreamsConfig.producerPrefix(\n➥ ProducerConfig.RETRIES_CONFIG), Integer.MAX_VALUE);\nprops.put(StreamsConfig.producerPrefix(\n➥ ProducerConfig.MAX_BLOCK_MS_CONFIG), Integer.MAX_VALUE);\nprops.put(StreamsConfig.REQUEST_TIMEOUT_MS_CONFIG, 305000);\nprops.put(StreamsConfig.consumerPrefix(\n➥ ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG), Integer.MAX_VALUE);\nSetting these values should help ensure that if all brokers in the Kafka cluster go\ndown, your Kafka Streams application will stay up and be ready to resume working\nonce the brokers are back online. \nHandling deserialization errors\nKafka works with byte arrays for keys and values, and you need to deserialize the keys and\nvalues to work with them. This is why you need to provide serdes for all source and sink\nprocessors. It wouldn’t be unexpected to have some malformed data during record pro-\ncessing. Kafka Streams provides the default.deserialization.exception.handler\nand StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG\nconfigurations to specify how you want to handle these deserialization errors.\n The default setting is org.apache.kafka.streams.errors.LogAndFailException-\nHandler, which, as the name implies, logs the error. Your Kafka Streams application\ninstance will fail (shut down) due to this deserialization exception. Another class,\norg.apache.kafka.streams.errors.LogAndContinueExceptionHandler, logs the error,\nbut your Kafka Streams application will continue to run.\n You can implement your own deserialization exception handler by creating a class\nimplementing the DeserializationExceptionHandler interface.\n \nListing A.1\nSetting properties for resilience to broker outages\n \n\n\n247\nScaling up your application\nProperties props = new Properties();\nprops.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_\n➥ CONFIG, LogAndContinueExceptionHandler.class);\nI only show how to set the LogAndContinueExceptionHandler handler, because the\nlog-and-fail version is the default setting. \nScaling up your application\nIn all the examples in the book, the Kafka Streams applications run with one stream\nthread. That’s fine for development, but in practice you’ll most likely need to run with\nmore than one stream thread. The question is how many threads and how many Kafka\nStreams instances to use. There are no concrete answers, because only you know your\ncircumstances well enough to address those questions, but we can go over some basic\ncalculations to give you a good idea.\n You’ll remember from chapter 3 that Kafka Streams creates a StreamTask per par-\ntition of the input topic(s). For our first example, we’ll consider a single input topic\nwith 12 partitions, to keep the discussion straightforward.\n With 12 input partitions, Kafka Streams creates 12 tasks. For the moment, let’s\nassume you want to have 1 task per thread. You could have 1 instance with 12 threads,\nbut that approach has a drawback: if the machine hosting your Kafka Streams applica-\ntion were to go down, all stream processing would stop.\n But if you start instances with 4 threads each, then each instance will process 4\ninput partitions. The benefit of this approach is that if one of the Kafka Streams\ninstances goes down, a rebalance will be triggered, and the 4 tasks from the non-\nrunning instance will be assigned to the other 2 instances; thus, the remaining appli-\ncations will process 6 tasks each. Additionally, when the stopped instance resumes\nrunning, another rebalance will occur, and all 3 instances will go back to processing 4\ntasks.\n One important consideration is that when determining the number of tasks to cre-\nate, Kafka Streams takes the maximum number of partitions from all input topics. If\nyou have 1 topic with 12 partitions, you end up with 12 tasks; but if the number of\nsource topics is 4, with 3 partitions each, you’ll have 3 tasks, each of which is responsi-\nble for processing 4 partitions.\n Keep in mind that any stream threads beyond the number of tasks will be idle.\nGoing back to the example of 3 Kafka Streams instances, if you stand up a fourth\ninstance of 4 threads, then after rebalancing you’ll have 4 idle stream threads among\nyour applications (16 threads, but only 12 tasks).\n This is a key component of Kafka Streams that I mentioned earlier in the book.\nThis dynamic scaling up or down doesn’t involve taking your application offline—it\nhappens automatically. This feature is helpful because if you have an uneven flow of\nListing A.2\nSetting a deserialization handler\n \n\n\n248\nAPPENDIX A\nAdditional configuration information\ndata into the application, you can spin up additional instances to handle the load and\nthen take some offline when the volume drops off.\n Do you always want a single thread per task? Maybe, but it’s hard to say, because it\ndepends on the demands of your application. \nRocksDB configuration\nFor stateful operations, Kafka Streams uses RocksDB (http://rocksdb.org) under the\ncovers as the persistence mechanism. RocksDB is a fast, highly configurable key/value\nstore. There are too many options to make specific recommendations here, but Kafka\nStreams provides a way to override the default settings with the RocksDBConfigSetter\ninterface.\n To set custom RocksDB settings, create a class implementing the RocksDBConfig-\nSetter interface, and then provide the class name when configuring your Kafka\nStreams application via the StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG\nsetting. To get an idea of what you can adjust with RocksDB, I encourage you to read\nthe RocksDB Tuning Guide at http://mng.bz/I88k. \nCreating repartitioning topics ahead of time\nIn Kafka Streams, any time you perform an operation that may potentially change the\nmap key—transform or groupBy, for example—an internal flag is set in the Streams-\nBuilder class, indicating that repartitioning will be required. Now, performing a map\nor transform won’t automatically force the creation of a repartitioning topic and the\nrepartitioning operation; but as soon as you add an operation using the updated key, a\nrepartitioning operation will be triggered.\n Although this is a required step (covered in chapter 4), in some cases, it’s better to\nrepartition the data yourself ahead of time. Consider the following (abbreviated)\nexample:\nKStream<String, String> mappedStream =\n➥ streamsBuilder.stream(\"inputTopic\").map(....);  \nKTable<Windowed<String>, Long> ktable1 =\n➥ mappedStream.groupByKey().windowedBy...count() \nKTable<Windowed<String>, Long> ktable2 =\n➥ mappedStream.groupByKey().windowedBy...count() \nKTable<Windowed<String>, Long> ktable3 =\n➥ mappedStream.groupByKey().windowedBy...count() \nHere, you map the original stream to create a new key to group by. You want to per-\nform three counts with three different windowing options—a legitimate use case. But\nbecause you mapped to a new key, each windowed count operation creates a new\nrepartition topic. Again, the need for a repartition topic makes sense due to the\nchanged key, but having three repartition topics duplicates data when you need only\none repartition topic.\nMaps the original input \nstream to create a new key\nWindowed \ncount option 1\nWindowed \ncount option 2\nWindowed \ncount option 3\n \n\n\n249\nConfiguring internal topics\n The solution to this issue is simple: after your map call, you immediately use a\nthrough operation to partition the data. Then, the subsequent groupByKey calls won’t\ntrigger repartitioning, because the groupByKey operator does not set the repartition-\nneeded flag. Here’s the revised code:\nKStream<String, String> mappedStream =\n➥ streamsBuilder.stream(\"inputTopic\").map(....).through(...);  \nBy adding the through processor and repartitioning manually, you have one reparti-\ntion topic instead of three. \nConfiguring internal topics\nWhen building a topology, depending on the processors you add, Kafka Streams may\ncreate several internal topics. These internal topics can be changelogs for backing up\nstate stores, or repartition topics. Depending on your data volume, these internal top-\nics can consume a large amount of space. Additionally, even though changelog topics\nare by default created with a cleanup policy of \"compact\", if you have many unique\nkeys, these compacted topics can grow in size. With this in mind, it’s a good idea to\nconfigure your internal topics to keep their size manageable.\n You have two options for managing internal topics. First, you can provide configs\ndirectly when creating state stores, using either StoreBuilder.withLoggingEnabled\nor Materialized.withLoggingEnabled. Which method you use depends on how\nyou create the state store. Both methods take a Map<String, String> containing the\ntopic properties. You can see an example in src/main/java/bbejeck/chapter_7/\nCoGroupingListeningExampleApplication.\n The other option for managing internal topics is to provide configurations for\nthem when configuring your Kafka Streams application:\nProperties props = new Properties();\n// other properties set here\nprops.put(StreamsConfig.topicPrefix(\"retention.bytes\"), 1024 * 1024);\nprops.put(StreamsConfig.topicPrefix(\"retention.ms\"), 3600000);\nWhen using the StreamsConfig.topicPrefix approach, the provided settings are\napplied globally to all internal topics. Any topic settings provided when creating a\nstate store will take precedence over the settings provided with StreamsConfig.\n I can’t give you much advice regarding what settings to use, because that depends\non your particular use case. But keep in mind that the default size of a topic is unlim-\nited and the default retention time is one week, so you should adjust the retention\n.bytes and retention.ms settings. In addition, for changelogs backing state stores\nwith many unique keys, you can set cleanup.policy to compact, delete to ensure that\nthe topic size stays manageable. \nMaps the original \ninput stream to \ncreate a new key, \nand repartitions\n \n\n\n250\nAPPENDIX A\nAdditional configuration information\nResetting your Kafka Streams application\nAt some point, you may need to start a Kafka Streams application over and reprocess\ndata, either in development or after a code update. To do this, Kafka Streams pro-\nvides a kafka-streams-application-reset.sh script in the bin directory of the Kafka\ninstallation.\n The script has one required parameter: the application ID of the Kafka Streams\napplication. The script offers several options, but in a nutshell, it can reset input top-\nics to the earliest available offset, reset intermediate topics to the latest offset, and\ndelete any internal topics. Note that you’ll need to call KafkaStreams.cleanUp the\nnext time you start your application, to delete any local state from previous runs. \nCleaning up local state\nChapter 4 discussed how Kafka Streams stores local state per task on the local filesys-\ntem. During development or testing, or when migrating to a new instance, you may\nwant to clean out all previous local state.\n To clean up any previous state, you can use KafkaStreams.cleanUp either before\nyou call KafkaStreams.start or after KafkaStreams.stop. Using the cleanUp method\nat any other time will result in an error. \n \n\n\n251\nappendix B\nExactly once semantics\nKafka achieved a major milestone with the release of version 0.11.0: exactly once\nsemantics. Prior to this release of Kafka, the delivery semantics of Kafka could have\nbeen described as at-least-once or at-most-once, depending on the producer.\n In the case of at-least-once delivery, a broker can persist a message but experi-\nence an error before sending the acknowledgment back to the producer, assuming\nthe producer is configured with asks=\"all\" and times out waiting for the acknowl-\nedgment. If the producer is configured with retries greater than zero, it will resend\nthe message, unaware that the previous message was successfully persisted. In this\nscenario (although rare), a duplicate message is delivered to consumers—hence,\nthe phrase at least once.\n For the at-most-once condition, consider the case where a producer is config-\nured with retries set to zero. In the previous example, the message in question\nwould be delivered only once, because there are no retries. But if the broker\nexperiences an error before it can persist the message, the message won’t be sent.\nIn this case, you’ve traded receiving all messages for not receiving any duplicate\nmessages.\n With exactly once semantics, even in situations where a producer resends a mes-\nsage that was previously persisted to a topic, consumers will receive the message\nexactly once. To enable transactions or exactly once processing with a Kafka-\nProducer, you add a configuration transactional.id and a couple of method calls,\nas shown in the following example. Otherwise, producing messages with transactions\nlooks familiar. Note that this excerpt doesn’t stand alone—it’s provided to highlight\nwhat’s required to produce and consume messages with the transactional API:\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\nprops.put(\"transactional.id\", \"transactional-id\");\n \n",
      "page_number": 259
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 267-274)",
      "start_page": 267,
      "end_page": 274,
      "detection_method": "topic_boundary",
      "content": "252\nAPPENDIX B\nExactly once semantics\nProducer<String, String> producer =\n➥ new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());\nproducer.initTransactions();  \ntry {\n// called right before sending any records\nproducer.beginTransaction();\n...sending some messages\n// when done sending, commit the transaction\nproducer.commitTransaction();\n} catch (ProducerFencedException | OutOfOrderSequenceException |\n➥ AuthorizationException e) {\nproducer.close();   \n} catch (KafkaException e) {\nproducer.abortTransaction();     \n}\nTo use KafkaConsumer with transactions, you need to add only one configuration:\nprops.put(\"isolation.level\", \"read_committed\");\nIn read_committed mode, KafkaConsumer only reads successfully committed transac-\ntional messages. The default setting is read_uncommitted, which returns all messages.\nNon-transactional messages are always retrieved in either configuration setting.\n The impact of exactly once semantics is a big win for Kafka Streams. With exactly\nonce, or transactions, you’re guaranteed to process records through a topology\nexactly once.\n To enable exactly once processing with Kafka Streams, set StreamsConfig.PROCESSING\n_GUARANTEE_CONFIG to exactly_once. The default setting for PROCESSING_GUARANTEE\n_CONFIG is at_least_once, or non-transactional processing. With that simple configu-\nration setting, Kafka Streams handles all required steps for performing transactional\nprocessing.\n This has been a quick overview of Kafka’s transactional API. For more information,\ncheck out the following resources:\n■\nDylan Scott, Kafka in Action (Manning, forthcoming), www.manning.com/books/\nkafka-in-action\n■\nNeha Narkhede, “Exactly-once Semantics Are Possible: Here’s How Kafka Does\nIt,” Confluent, June 30, 2017, http://mng.bz/t9rO\n■\nApurva Mehta and Jason Gustafson, “Transactions in Apache Kafka,” Confluent,\nNovember 17, 2017, http://mng.bz/YKqf\n■\nGuozhang Wang, “Enabling Exactly-Once in Kafka Streams,” Confluent, Decem-\nber 13, 2017, http://mng.bz/2A32\nWhen setting transactional.id, \nyou need to call this method \nbefore any others.\nThe only option for any of the \nnon-recoverable exceptions is \nto close the producer.\nFor any other exception, \nabort and retry.\n \n\n\n253\nindex\nA\nabstractions, higher-level vs. \nmore control 146\nAbstractProcessor class 150\naccess control 36\nadder() method 131\naddProcessor() method 148\naddSource() method 148\nadvanceBy() method 138\nadvanced applications with \nKafka Streams\nintegrating Kafka with other \ndata sources 218–226\nKafka Connect 219–222\ntransforming data 222–226\ninteractive queries 226–237\ncoding 232–234\noverview 228–229\nquery server 234–237\nstate stores 230–232\nKSQL 237–244\narchitecture 238–240\nconfiguring 244\ninstalling and running 240\nqueries 242–243\nstreams 241–242\ntables 243–244\nAggregatingMethodHandle-\nProcessor 206\nAggregatingMethodHandle-\nProcessorTest 206\naggregations 126–144\nGlobalKTables 140–143\njoining KStreams with\n141–143\njoining with smaller \ndatasets 141\nrepartitioning, cost of 141\njoining KStreams and \nKTables 139–140\nconverting KTable to \nKStream 139\ncreating financial news \nKTable 140\njoining news updates with \ntransaction counts 140\nQueryable state 143–144\nshare volume by \nindustry 127–132\napplication metrics 182–191\ncollected metrics 185\nmetrics configuration\n184–185\nusing JMX 185–189\nJConsole, starting 186\nmonitoring running \nprogram 186–188\nviewing information\n188–189\nviewing metrics 189–191\narchitecture, Kafka 25–39\ncontroller\nelecting 34\nresponsibilities of 35–36\nlogs 27–28\ncompacting 38–39\ndeleting 37–38\ndistributed 32\nmanagement of 37\nmessage broker 26\npartitions 28–29\ndetermining correct num-\nber of 32\ngrouping data by key 29–30\nspecifying custom \npartitioner 31–32\nwriting custom \npartitioner 30–31\nreplication 34\nZooKeeper 33–34\nat-least-once condition 251\nat-most-once condition 251\nAtomicInteger 43\nautomatic offset commits 46\nB\nbackpressure 15\nbatch processing 4\nbatch processing, inadequacy \nfor 8\nbig data 4–8\nbatch processing, inadequacy \nfor 8\ngenesis of 4–5\nMapReduce 5–8\ndistributing data across \ncluster to achieve scale \nin processing 6–7\nembracing failure by using \nreplication 8\nusing key/value pairs and \npartitions 7–8\nbootstrap servers 42\nbranch() method 79\nbroker 25\nbroker outages 246\n \n\n\nINDEX\n254\nbucketing 118\nbuilder.stream method 128\nC\ncache operation 124\nCallback.onComplete \nmethod 41\nCassandra 70\nchangelog, updates to\n119–121\n@ClassRule annotation 210\ncleaning local state 250\nClickEventProcessor 164\nclose() method 90, 150\ncluster 25\ncluster membership 36\nco-group processor 159–170\nprocessor nodes, adding\n162–167\nsink node, adding 168–170\nsource nodes, defining 161\nstate store, adding 167–168\nCoGrouping-Processor 169\ncommitSync() method 46\ncompression type 42\nConcurrentHashMap 196\nconfiguration\ncleaning local state 250\ncreating repartitioning topics \nahead of time 248–249\nhandling deserialization \nerrors 246–247\ninternal topics 249\nlimiting number of rebalances \non startup 245–246\nlocal 49–50\nresetting Kafka Streams \napplication 250\nresilience to broker \noutages 246\nRocksDB 248\nscaling up application\n247–248\nConsumed class 60\nconsumer group 47\nconsumer lag 177\nconsumers 25\nconsumers, reading messages \nwith 44–49\nconsumer example 48–49\ncreating consumer 47\nfiner-grained consumer \nassignment 48\noffsets 44–46\npartitions and consumers 47\nrebalancing 47–48\ncontroller\nelecting 34\nresponsibilities of 35–36\nco-partitioning 108–109\ncount() method 135\ncredit cards\nmasking node 12\nmasking numbers 17–19\ncustom partitioner\nspecifying 31–32\nwriting 30–31\ncustomer data 65–74\nconstructing topology 66–72\nbuilding source node\n66–67\nfunctional programming \nhints 67\nlast processor 70–72\nsecond processor 68–69\nthird processor 69–70\ncreating custom Serde 72–74\ncustomer IDs, keys \ncontaining 103–104\ncustomer rewards 10, 19, 88–89\nD\ndata locality 96–97\nDEBUG level 184\ndebugging Kafka Streams\n191–198\ngetting notification on vari-\nous states of \napplication 192–193\nState restore listener\n195–198\nuncaught exception \nhandler 198\nusing StateListener 193–195\nviewing representation of \napplication 191–192\nDefaultPartitioner 31\ndeserialization errors 246–247\nDeserializationExceptionHan-\ndler interface 246\ndeveloping Kafka Streams\ncustomer data 65–74\nconstructing topology\n66–72\ncreating custom Serde\n72–74\nHello World 58–65\nconfiguration 63\nSerde creation 63–65\ntopology for Yelling \nApp 59–63\ninteractive development\n74–76\nnext steps 76–83\nStreams Processor API 58\ndisableLogging() method 99\ndistributed logs 32\nE\neconomic forecasting 9\nelectronicsStream \nparameter 107\nembedded Kafka cluster 209\nEmbeddedKafkaCluster 210, \n214\nETL (extract, transform, load) 3\nevent streams, vs. update \nstreams 122–123\nevent time 111\nExtractKey 224\nExtractRecordMetadataTime-\nstamp class 113\nF\nFailOnInvalidTimestamp \nclass 113\nfailure\nembracing by using \nreplication 8\nrecovery from 97–98\nfault tolerance 97–98\nfetchFromWindowStore \nmethod 236\nfixedQueue variable 131\nFixedSizePriorityQueue 131\nFlattenStruct 225\nfluent interface 58\nforeach actions 82–83\nForeachAction interface 76, 83\nfraud 9\nfrom-beginning parameter 53\nfunctional programming 67\nG\nGlobalKTables 140–143\njoining KStreams with\n141–143\njoining with smaller \ndatasets 141\nrepartitioning, cost of 141\n \n\n\nINDEX\n255\ngraph of processing nodes\n15–16\nGroupBy method 129\ngroupBy operation 135\nGroupByKey method 129\ngroupByKey operator 249\nGroupCoordinator 245\ngrouping data, by key 29–30\ngrouping records 7\nGson 73\nH\nhashing function 7\nHello World 58–65\nconfiguration 63\nSerde creation 63–65\ntopology for Yelling App\n59–63\nhigher-level abstractions vs. \nmore control 146\nhopping windows 133, 137–139\nI\nIDs, customer, keys \ncontaining 103–104\nINFO level 184\ningestion time 112\ninit() method 90, 151\ninstalling\nKafka 49–50\nKSQL 240\ninteractive development 74–76\ninteractive queries 143, 226–237\ncoding 232–234\ninside query server 234–237\noverview 228–229\nstate stores 229–232\nInteractiveQueryServer \nclass 233\ninternal topics, configuring 249\nintrusion detection 9\nISR (in-sync replica) 34\nJ\nJava VisualVM 185\nJConsole, starting 186\nJMX, application metrics \nusing 185–189\nJConsole, starting 186\nmonitoring running \nprogram 186–188\nviewing information 188–189\njoin() method 105\njoined parameter 107\njoining\nKafka Streams 100–110\nco-partitioning 108–109\ndata setup 102–103\nimplementation 106–108\nkeys containing customer \nIDs 103–104\nleft-outer join 110\nouter joins 109\npurchase records 105–106\nKStreams and KTables\n139–140\nconverting KTable to \nKStream 139\ncreating financial news \nKTable 140\njoining news updates with \ntransaction counts 140\nJoinWindows.after method 108\nJoinWindows.before \nmethod 108\nJoinWindows.of method 107\nJUnit rules 210–211\nK\nKafka\ninstalling, local \nconfiguration 49–50\nstarting 50\nSee also architecture, Kafka\nKafka Connect 218–222\nKafka Streams 15–16\nAPI, integrating Processor API \nwith 170–171\ndebugging techniques\n191–198\ngetting notification on \nvarious states of \napplication 192–193\nState restore listener\n195–198\nuncaught exception \nhandler 198\nusing StateListener\n193–195\nviewing representation of \napplication 191–192\nevents 85–86\njoining 100–110\nco-partitioning 108–109\ndata setup 102–103\nimplementation 106–108\nkeys containing customer \nIDs 103–104\nleft-outer join 110\nouter joins 109\npurchase records 105–106\nstate and 86\nstate stores\nconfiguring changelog \ntopics 99–100\nusing for lookups and \npreviously seen \ndata 96–100\nstateful operations, applying \nto 86–96\nmapping Purchase object to \nRewardAccumulator \nusing state 90–94\nstateful customer \nrewards 88–89\ntransformValues \nprocessor 87\nupdating rewards \nprocessor 94–96\nvalue transformer, \ninitializing 90\nTimestampExtractor\ncustom 114–115\nprovided implementations\n112–113\nspecifying 115\nWallclockTimestamp-\nExtractor 113\nSee also advanced applications \nwith Kafka Streams; devel-\noping Kafka Streams; pur-\nchase transaction; stream \nprocessing\nKafka Streams application\nresetting 250\ntesting\nintegration testing\n208–214\ntopology 201–208\nKafkaConsumer 252\nKafkaProducer 42, 251\nkey/value pairs 7–8, 15\nkey/value store suppliers 99\nkeys\ncontaining customer \nIDs 103–104\ngrouping data by 29–30\nKeyValueIterator 167\nKGroupedStream 127\nKGroupedStream.windowedBy \nmethod 132\n \n\n\nINDEX\n256\nKSQL 237–244\narchitecture 238–240\nconfiguring 244\ninstalling and running 240\nqueries 242–243\nstreams 238, 241–242\ntables 238, 243–244\nKStream.mapValues \nfunction 61, 83\nKStreamNot 78\nKStreams, joining to \nKTables 139–140\nconverting KTable to \nKStream 139\ncreating financial news \nKTable 140\njoining news updates with \ntransaction counts 140\nKStream.through() method\n93\nKStream.to() method 109\nKTable API\naggregations 126–144\nGlobalKTables 140–143\njoining KStreams and \nKTables 139–140\nQueryable state 143–144\nshare volume by industry\n127–132\nrecord updates and KTable \nconfiguration 123–126\ncache buffering size\n124–125\ncommit interval 125–126\nstreams and tables, \nrelationship between\n118–123\nevent streams vs. update \nstreams 122–123\nrecord stream 118–119\nupdates to records or \nchangelog 119–121\nwindowing operations\n132–139\ncounting stock transactions \nby customer 132\nsession windows 133–134\nsession windows, using to \ntrack stock \ntransactions 134–136\nsliding or hopping \nwindows 137–139\ntumbling windows\n136–137\nwindow types 133\nKTables, joining to \nKStreams 139–140\nconverting KTable to \nKStream 139\ncreating financial news \nKTable 140\njoining news updates with \ntransaction counts 140\nKTable.toStream() method 139\nL\nleft-outer join 110\nlist command 178\nlocal configuration 49–50\nlocal state, cleaning 250\nlog rolling 37\nLogAndSkipOnInvalidTime-\nstamp class 113\nLoggingStateRestoreListener\n197\nlogs 27–28\ncompacting 38–39\ndeleting 37–38\ndistributed 32\nmanagement of 37\nM\nmanual offset commits 46\nMapReduce 5–8\ndistributing data across cluster \nto achieve scale in \nprocessing 6–7\nembracing failure by using \nreplication 8\nusing key/value pairs and \npartitions 7–8\nmapValues method 89, 94\nMaterialized class 98\nmaterialized views 143\nmessage broker 26\nmessages\nreading 53–54\nreading with consumers\n44–49\nconsumer example 48–49\ncreating consumer 47\nfiner-grained consumer \nassignment 48\noffsets 44–46\npartitions and \nconsumers 47\nrebalancing 47–48\nsending first message 52–54\nsending with producers\n40–44\npartitions, specifying\n42–43\nproducer properties 42\ntimestamps 43–44\nmetrics. See application metrics\nmicrobatching 4\nmicroservices 233\nMockito 205\nmonitoring and performance\napplication metrics 182–191\ncollected metrics 185\nmetrics configuration\n184–185\nusing JMX 185–189\nviewing metrics 189–191\nbasic monitoring 176–182\nconsumer and producer \nperformance, \nmeasuring 176–178\nconsumer lag, checking \nfor 178–179\nproducer and consumer, \nintercepting\n179–182\nKafka Streams, debugging \ntechniques 191–198\nN\nnext() method 47\nnull value 39\nO\nonBatchRestored method 197\nonRestoreStart method 196\nORM (object-relational \nmapping) 146\nouter joins 109\nP\nPageRank algorithm 5\nPairRDDFunctions.cogroup \nmethod 161\npartitioner class 42\nPartitioner interface 31\npartitioning streams 79\npartitions 7, 28–29\nconsumers and 47\ndetermining correct number \nof 32\ngrouping data by key 29–30\n \n\n\nINDEX\n257\npartitions (continued)\nspecifying 42–43\nspecifying custom partitioner\n31–32\nwriting custom partitioner\n30–31\npartitions flag 53\npatterns node 13\nperformance. See monitoring \nand performance\nprint() method 76, 122–123\nPrinted.toSysOut() method 75\nPrinted.withKeyValueMapper \nmethod 75\nprivacy 10\nprocess() method 149, 157–158\nprocessing nodes, graph of\n15–16\nprocessing time 112\nProcessor API\nco-group processor 159–170\nprocessor nodes, adding\n162–167\nsink node, adding 168–170\nsource nodes, defining 161\nstate store, adding\n167–168\nhigher-level abstractions vs. \nmore control 146\nintegrating with Kafka \nStreams API 170–171\nstock analysis processor\n152–159\nprocess( ) method\n157–158\npunctuator execution\n158–159\nstock-performance proces-\nsor application\n153–156\ntopology 146–152\nprocessor node 148–151\nsink node 151–152\nsource node 147–148\nprocessor node, Processor \nAPI 148–151\nProcessorContext object 154\nProcessor.punctuate method\n110\nProcessorSupplier() method\n148\nProcessorTopologyTestDriver\n201, 214\nProducerInterceptor 182\nproducers 25\nproducers, sending messages \nwith 40–44\npartitions, specifying 42–43\nproducer properties 42\ntimestamps 43–44\npunctuate() method 87, 150, \n155, 206\nPurchase object 90–94\npurchase patterns 18–19\npurchase records\ngeneral discussion 105–106\nwriting 20\npurchase transaction\nchanging perspective on\n12–13\ncredit card masking node 12\npatterns node 13\nrewards node 13\nsource node 12\nstorage node 13\nflow of, applying to Kafka \nStreams to 16–20\ncustomer rewards 19\nmasking credit card \nnumbers 17–19\npurchase patterns 18–19\nsource, defining 16–17\nwriting purchase \nrecords 20\nhandling 10–11\ndeconstructing require-\nments into graph 11\nweighing stream-processing \noption 10–11\npurchaseJoiner parameter 107\nPurchaseKeyPartitioner 30\nPurchasePattern object 18\nQ\nqueries\ninteractive 226–237\ncoding 232–234\ninside query server 234–237\noverview 228–229\nstate stores 229–232\nKSQL 242–243\nQueryable state 143–144\nR\nreading messages, with \nconsumers 44–49\nconsumer example 48–49\ncreating consumer 47\nfiner-grained consumer \nassignment 48\noffsets 44–46\npartitions and consumers 47\nrebalancing 47–48\nReadOnlyWindowStore 235\nrebalances, limiting number of \non startup 245–246\nrecord stream 118–119\nrecords, updates to 119–121\nrepartitioning\ncreating topics ahead of \ntime 248–249\ngeneral discussion 92–94\nrepeatable tests 200\nReplaceField 222\nreplication 8, 34\nreplication-factor flag 52\nresetting Kafka Streams \napplication 250\nretries 42\nRewardAccumulator object 19, \n88\nRewardAccumulator, mapping \nto Purchase object using \nstate 90–94\nrewards node 13\nrewards processor, updating\n94–96\nRocksDB 99, 248\nS\nsales data 11\nsales transaction data hub\n24–25\nscaling up application\n247–248\nSELECT statements 146\nselectKey method 83, 108\nsending messages, with \nproducers 40–44\npartitions, specifying 42–43\nproducer properties 42\ntimestamps 43–44\nSerde class 60, 63–65\nSerde.String() method 72\nserialization 42\nsession windows 133–134\nShareVolume 127\nShareVolume.sum method 129\nsimpleFirstStream 59\nsink node, Processor API\n151–152\nsliding windows 133, 137–139\n \n\n\nINDEX\n258\nSMA (simple moving average)\n157\nsource, defining 16–17\nsplitting streams 79\nstate 86\nstate stores\nconfiguring changelog \ntopics 99–100\nusing for lookups and \npreviously seen data\n96–100\nstateful operations, applying \nto Kafka Streams 86–96\nmapping Purchase object to \nRewardAccumulator \nusing state 90–94\nstateful customer \nrewards 88–89\ntransformValues \nprocessor 87\nupdating rewards \nprocessor 94–96\nvalue transformer, \ninitializing 90\nState restore listener 195–198\nStateListener 193–195\nStateRestoreListener 196\nStateStore fault tolerance 99\nstock analysis processor\n152–159\nprocess() method 157–158\npunctuator execution\n158–159\nstock-performance processor \napplication 153–156\nstorage 11\nstorage node 13\nStoreBuilder class 98\nstream processing 4\noverview 8–10\nwhen to use and when not \nto 9–10\nStreamPartitioner 94\nStreams Processor API 58\nStreamsBuilder class 248\nStreamsBuilder.stream \nmethod 67\nStreamsConfig.APPLICATION\n_ID_CONFIG property 63\nStreamsConfig.BOOTSTRAP\n_SERVERS_CONFIG \nproperty 63\nStreamTask 92, 155, 247\nStreamThread 155\nsum() method 129\nT\ntables\nKSQL 243–244\nstreams and 118–123\nevent streams vs. update \nstreams 122–123\nrecord stream 118–119\nupdates to records or \nchangelog 119–121\ntest() method 77\ntesting Kafka Streams applica-\ntion\nintegration testing 208–214\ndynamically adding topic\n213–214\nEmbeddedKafkaCluster, \nadding 210\nJUnit rules 210–211\nproducing and consuming \nrecords in test 212–213\ntesting topology 211–212\ntopics, creating 211\ntopology 201–208\nbuilding test 202–204\nprocessors and \ntransformers 205–208\nstate store in 204–205\nTimestampConverter 222\nTimestampExtractor 110–115\ncustom 114–115\nprovided implementations\n112–113\nspecifying 115\nWallclockTimestampExtractor\n113\nTimestampExtractor \ninterface 112\ntimestamps 43–44\ntopic configuration 36\nTopicPartition 245\ntopics 25, 27\ntopology 15\nTopologyBuilderException 108\nTopology.describe() \nmethod 191\ntopologyTestDriver.readOutput \nmethod 203\ntoStream().map function 142\ntoUpperCase() method 61\nTransactionSummary object 139\ntransform() method 91\ntransformValues processor 87\ntransformValues() method 87, \n89, 171\ntumbling windows 133, 136–137\ntwentyMinuteWindow \nparameter 107\nU\nuntil() method 137\nupdate streams, vs. event \nstreams 122–123\nupdates\nto changelog 119–121\nto records 119–121, 123–126\nUsePreviousTimeOnInvalid-\nTimestamp class 113, 148\nV\nvalue transformer, initializing\n90\nValueMapper interface 18, 61\nW\nWallclockTimestampExtractor\n113\nwindowing 118\nwindowing operations 132–139\ncounting stock transactions by \ncustomer 132\nsession windows 133–134\nsliding or hopping \nwindows 137–139\ntumbling windows 136–137\nwindow types 133\nwithLabel() method 75\nY\nYelling App, topology for 59–63\nZ\nZMart’s original data \nplatform 23\nZooKeeper 33–34, 50\n \n\n\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nThis topology takes a single input from\nthe source node and performs several\ntransformations, making this a great\ndemonstration of testing.\n \n",
      "page_number": 267
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 275-282)",
      "start_page": 275,
      "end_page": 282,
      "detection_method": "topic_boundary",
      "content": "William P. Bejeck Jr.  \nN\not all stream-based applications require a dedicated \nprocessing cluster. The lightweight Kafka Streams \nlibrary provides exactly the power and simplicity you \nneed for message handling in microservices and real-time \nevent processing. With the Kafka Streams API, you ﬁ lter and \ntransform data streams with just Kafka and your application.\nKafka Streams in Action teaches you to implement stream \nprocessing within the Kafka platform. In this easy-to-follow \nbook, you’ll explore real-world examples to collect, trans-\nform, and aggregate data, work with multiple processors, and \nhandle real-time events. You’ll even dive into streaming SQL \nwith KSQL! Practical to the very end, it ﬁ nishes with testing \nand operational aspects, such as monitoring and debugging. \nWhat’s Inside\n● Using the KStream API\n● Filtering, transforming, and splitting data\n● Working with the Processor API\n● Integrating with external systems\nAssumes some experience with distributed systems. No \nknowledge of Kafka or streaming applications required.\nBill Bejeck is a Kafka Streams contributor and Conﬂ uent \nengineer with over 15 years of software development \nexperience.\nTo download their free eBook in PDF, ePub, and Kindle formats, owners \nof this book should visit manning.com/books/kafka-streams-in-action\n$44.99 / Can $59.99  [INCLUDING eBOOK]\nKafka Streams IN ACTION\nSOFTWARE DEVELOPMENT\nM A N N I N G\n“\nA great way to learn \nabout Kafka Streams and \nhow it is a key enabler of \nevent-driven applications.”\n \n—From the Foreword by \nNeha Narkhede\nCocreator of Apache Kafka\n“\nA comprehensive guide \nto Kafka Streams—from \nintroduction to production!”\n \n—Bojan Djurkovic, Cvent\n“\nBridges the gap between \nmessage brokering and real-\n time streaming analytics.”\n—Jim Mantheiy Jr.\nNext Century \n“\nValuable both as an \nintroduction to streams \nas well as an ongoing \n  reference.”\n \n—Robin Coe, TD Bank\nSee first page\n",
      "page_number": 275
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "M A N N I N G\nWilliam P. Bejeck Jr.\nForeword by Neha Narkhede                                  \nReal-time apps and \nmicroservices with the \nKafka Streams API\n",
      "content_length": 158,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": "Patterns\nMasking\nSource\nElectronics\nsink\nCafe\nsink\nPatterns\nsink\nPurchases\nsink\nRewards\nBranch\nprocessor\nFiltering\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nRewards\nsink\nSelect-key\nprocessor\n \n",
      "content_length": 198,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Kafka Streams\nin Action\nREAL-TIME APPS AND MICROSERVICES\nWITH THE KAFKA STREAMS API\nWILLIAM P. BEJECK JR.\nFOREWORD BY NEHA NARKHEDE\nM A N N I N G\nSHELTER ISLAND\n",
      "content_length": 161,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "For online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2018 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nManning Publications Co.\nAcquisitions editor: Michael Stephens\n20 Baldwin Road\nDevelopment editor: Frances Lefkowitz\nPO Box 761\nTechnical development editors: Alain Couniot, John Hyaduck\nShelter Island, NY 11964\nReview editor: Aleksandar Dragosavljevic´\nProject manager: David Novak\nCopy editors: Andy Carroll, Tiffany Taylor\nProofreader: Katie Tennant\nTechnical proofreader: Valentin Crettaz\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781617294471\nPrinted in the United States of America\n1 2 3 4 5 6 7 8 9 10 – DP – 23 22 21 20 19 18\n \n",
      "content_length": 1847,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "iii\nbrief contents\nPART 1\nGETTING STARTED WITH KAFKA STREAMS ..................... 1\n1\n■\nWelcome to Kafka Streams 3\n2\n■\nKafka quickly 22\nPART 2\nKAFKA STREAMS DEVELOPMENT ...................................55\n3\n■\nDeveloping Kafka Streams 57\n4\n■\nStreams and state 84\n5\n■\nThe KTable API 117\n6\n■\nThe Processor API 145\nPART 3\nADMINISTERING KAFKA STREAMS ..............................173\n7\n■\nMonitoring and performance 175\n8\n■\nTesting a Kafka Streams application 199\nPART 4\nADVANCED CONCEPTS WITH KAFKA STREAMS .............215\n9\n■\nAdvanced applications with Kafka Streams 217\n \n",
      "content_length": 574,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "v\ncontents\nforeword\nxi\npreface\nxiii\nacknowledgments\nxiv\nabout this book\nxv\nabout the author\nxix\nabout the cover illustration\nxx\nPART 1\nGETTING STARTED WITH KAFKA STREAMS ....................1\n1 \nWelcome to Kafka Streams\n3\n1.1\nThe big data movement, and how it changed \nthe programming landscape\n4\nThe genesis of big data\n4\n■Important concepts from \nMapReduce\n5\n■Batch processing is not enough\n8\n1.2\nIntroducing stream processing\n8\nWhen to use stream processing, and when not to use it\n9\n1.3\nHandling a purchase transaction\n10\nWeighing the stream-processing option\n10\n■Deconstructing the \nrequirements into a graph\n11\n \n",
      "content_length": 619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "CONTENTS\nvi\n1.4\nChanging perspective on a purchase transaction\n12\nSource node\n12\n■Credit card masking node\n12\nPatterns node\n13\n■Rewards node\n13\n■Storage node\n13\n1.5\nKafka Streams as a graph of processing nodes\n15\n1.6\nApplying Kafka Streams to the purchase \ntransaction flow\n16\nDefining the source\n16\n■The first processor: masking credit card \nnumbers\n17\n■The second processor: purchase patterns\n18\nThe third processor: customer rewards\n19\n■The fourth \nprocessor—writing purchase records\n20\n2 \nKafka quickly\n22\n2.1\nThe data problem\n23\n2.2\nUsing Kafka to handle data\n23\nZMart’s original data platform\n23\n■A Kafka sales transaction \ndata hub\n24\n2.3\nKafka architecture\n25\nKafka is a message broker\n26\n■Kafka is a log\n27\nHow logs work in Kafka\n27\n■Kafka and partitions\n28\nPartitions group data by key\n29\n■Writing a custom \npartitioner\n30\n■Specifying a custom partitioner\n31\nDetermining the correct number of partitions\n32\nThe distributed log\n32\n■ZooKeeper: leaders, followers, \nand replication\n33\n■Apache ZooKeeper\n33\n■Electing \na controller\n34\n■Replication\n34\n■Controller \nresponsibilities\n35\n■Log management\n37\nDeleting logs\n37\n■Compacting logs\n38\n2.4\nSending messages with producers\n40\nProducer properties\n42\n■Specifying partitions and \ntimestamps\n42\n■Specifying a partition\n43\nTimestamps in Kafka\n43\n2.5\nReading messages with consumers\n44\nManaging offsets\n44\n■Automatic offset commits\n46\nManual offset commits\n46\n■Creating the consumer\n47\nConsumers and partitions\n47\n■Rebalancing\n47\nFiner-grained consumer assignment\n48\n■Consumer example\n48\n2.6\nInstalling and running Kafka\n49\nKafka local configuration\n49\n■Running Kafka\n50\nSending your first message\n52\n \n",
      "content_length": 1655,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "CONTENTS\nvii\nPART 2\nKAFKA STREAMS DEVELOPMENT ........................ 55\n3 \nDeveloping Kafka Streams\n57\n3.1\nThe Streams Processor API\n58\n3.2\nHello World for Kafka Streams\n58\nCreating the topology for the Yelling App\n59\n■Kafka Streams \nconfiguration\n63\n■Serde creation\n63\n3.3\nWorking with customer data\n65\nConstructing a topology\n66\n■Creating a custom Serde\n72\n3.4\nInteractive development\n74\n3.5\nNext steps\n76\nNew requirements\n76\n■Writing records outside of Kafka\n81\n4 \nStreams and state\n84\n4.1\nThinking of events\n85\nStreams need state\n86\n4.2\nApplying stateful operations to Kafka Streams\n86\nThe transformValues processor\n87\n■Stateful customer \nrewards\n88\n■Initializing the value transformer\n90\nMapping the Purchase object to a RewardAccumulator \nusing state\n90\n■Updating the rewards processor\n94\n4.3\nUsing state stores for lookups and previously \nseen data\n96\nData locality\n96\n■Failure recovery and fault tolerance\n97\nUsing state stores in Kafka Streams\n98\n■Additional key/value \nstore suppliers\n99\n■StateStore fault tolerance\n99\n■Configuring \nchangelog topics\n99\n4.4\nJoining streams for added insight\n100\nData setup\n102\n■Generating keys containing customer \nIDs to perform joins\n103\n■Constructing the join\n104\nOther join options\n109\n4.5\nTimestamps in Kafka Streams\n110\nProvided TimestampExtractor implementations\n112\nWallclockTimestampExtractor\n113\n■Custom \nTimestampExtractor\n114\n■Specifying a \nTimestampExtractor\n115\n \n",
      "content_length": 1423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "CONTENTS\nviii\n5 \nThe KTable API\n117\n5.1\nThe relationship between streams and tables\n118\nThe record stream\n118\n■Updates to records or the changelog\n119\nEvent streams vs. update streams\n122\n5.2\nRecord updates and KTable configuration\n123\nSetting cache buffering size\n124\n■Setting the commit \ninterval\n125\n5.3\nAggregations and windowing operations\n126\nAggregating share volume by industry\n127\n■Windowing \noperations\n132\n■Joining KStreams and KTables\n139\nGlobalKTables\n140\n■Queryable state\n143\n6 \nThe Processor API\n145\n6.1\nThe trade-offs of higher-level abstractions vs. \nmore control\n146\n6.2\nWorking with sources, processors, and sinks to create a \ntopology\n146\nAdding a source node\n147\n■Adding a processor node\n148\nAdding a sink node\n151\n6.3\nDigging deeper into the Processor API with a stock analysis \nprocessor\n152\nThe stock-performance processor application\n153\n■The process() \nmethod\n157\n■The punctuator execution\n158\n6.4\nThe co-group processor\n159\nBuilding the co-grouping processor\n161\n6.5\nIntegrating the Processor API and the \nKafka Streams API\n170\nPART 3\nADMINISTERING KAFKA STREAMS .....................173\n7 \nMonitoring and performance\n175\n7.1\nBasic Kafka monitoring\n176\nMeasuring consumer and producer performance\n176\nChecking for consumer lag\n178\n■Intercepting the producer \nand consumer\n179\n7.2\nApplication metrics\n182\nMetrics configuration\n184\n■How to hook into the collected \nmetrics\n185\n■Using JMX\n185\n■Viewing metrics\n189\n \n",
      "content_length": 1440,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "CONTENTS\nix\n7.3\nMore Kafka Streams debugging techniques\n191\nViewing a representation of the application\n191\n■Getting \nnotification on various states of the application\n192\nUsing the StateListener\n193\n■State restore listener\n195\nUncaught exception handler\n198\n8 \nTesting a Kafka Streams application\n199\n8.1\nTesting a topology\n201\nBuilding the test\n202\n■Testing a state store in the topology\n204\nTesting processors and transformers\n205\n8.2\nIntegration testing\n208\nBuilding an integration test\n209\nPART 4\nADVANCED CONCEPTS WITH KAFKA STREAMS.... 215\n9 \nAdvanced applications with Kafka Streams\n217\n9.1\nIntegrating Kafka with other data sources\n218\nUsing Kafka Connect to integrate data\n219\n■Setting up \nKafka Connect\n219\n■Transforming data\n222\n9.2\nKicking your database to the curb\n226\nHow interactive queries work\n228\n■Distributing state stores\n229\nSetting up and discovering a distributed state store\n230\n■Coding \ninteractive queries\n232\n■Inside the query server\n234\n9.3\nKSQL\n237\nKSQL streams and tables\n238\n■KSQL architecture\n238\nInstalling and running KSQL\n240\n■Creating a KSQL \nstream\n241\n■Writing a KSQL query\n242\n■Creating \na KSQL table\n243\n■Configuring KSQL\n244\nappendix A\nAdditional configuration information\n245\nappendix B\nExactly once semantics\n251\nindex\n253\n \n",
      "content_length": 1269,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "xi\nforeword\nI believe that architectures centered around real-time event streams and stream pro-\ncessing will become ubiquitous in the years ahead. Technically sophisticated compa-\nnies like Netflix, Uber, Goldman Sachs, Bloomberg, and others have built out this type\nof large, event-streaming platform operating at massive scale. It’s a bold claim, but I\nthink the emergence of stream processing and the event-driven architecture will have\nas big an impact on how companies make use of data as relational databases did.\n Event thinking and building event-driven applications oriented around stream pro-\ncessing require a mind shift if you are coming from the world of request/response–style\napplications and relational databases. That’s where Kafka Streams in Action comes in.\n Stream processing entails a fundamental move away from command thinking\ntoward event thinking—a change that enables responsive, event-driven, extensible, flex-\nible, real-time applications. In business, event thinking opens organizations to real-\ntime, context-sensitive decision making and operations. In technology, event thinking\ncan produce more autonomous and decoupled software applications and, conse-\nquently, elastically scalable and extensible systems.\n In both cases, the ultimate benefit is greater agility—for the business and for the\nbusiness-facilitating technology. Applying event thinking to an entire organization is\nthe foundation of the event-driven architecture. And stream processing is the technol-\nogy that enables this transformation.\n Kafka Streams is the native Apache Kafka stream-processing library for building\nevent-driven applications in Java. Applications that use Kafka Streams can do sophisti-\ncated transformations on data streams that are automatically made fault tolerant and\n \n",
      "content_length": 1795,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "FOREWORD\nxii\nare transparently and elastically distributed over the instances of the application.\nSince its initial release in the 0.10 version of Apache Kafka in 2016, many companies\nhave put Kafka Streams into production, including Pinterest, The New York Times, Rabo-\nbank, LINE, and many more.\n Our goal with Kafka Streams and KSQL is to make stream processing simple\nenough that it can be a natural way of building event-driven applications that respond\nto events, not just a heavyweight framework for processing big data. In our model, the\nprimary entity isn’t the processing code: it’s the streams of data in Kafka.\n Kafka Streams in Action is a great way to learn about Kafka Streams, and to learn how\nit is a key enabler of event-driven applications. I hope you enjoy reading this book as\nmuch as I have!\n—NEHA NARKHEDE\nCofounder and CTO at Confluent, Cocreator of Apache Kafka\n \n",
      "content_length": 889,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "xiii\npreface\nDuring my time as a software developer, I’ve had the good fortune to work with cur-\nrent software on exciting projects. I started out doing a mix of client-side and backend\nwork; but I found I preferred to work solely on the backend, so I made my home\nthere. As time went on, I transitioned to working on distributed systems, beginning\nwith Hadoop (then in its pre-1.0 release). Fast-forward to a new project, and I had an\nopportunity to use Kafka. My initial impression was how simple Kafka was to work\nwith; it also brought a lot of power and flexibility. I found more and more ways to inte-\ngrate Kafka into delivering project data. Writing producers and consumers was straight-\nforward, and Kafka improved the quality of our system.\n Then I learned about Kafka Streams. I immediately realized, “Why do I need\nanother processing cluster to read from Kafka, just to write back to it?” As I looked\nthrough the API, I found everything I needed for stream processing: joins, map val-\nues, reduce, and group-by. More important, the approach to adding state was superior\nto anything I had worked with up to that point.\n I’ve always had a passion for explaining concepts to other people in a way that is\nstraightforward and easy to understand. When the opportunity came to write about\nKafka Streams, I knew it would be hard work but worth it. I’m hopeful the hard work\nwill pay off in this book by demonstrating that Kafka Streams is a simple but elegant\nand powerful way to perform stream processing.\n \n",
      "content_length": 1513,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "xiv\nacknowledgments\nFirst and foremost, I’d like to thank my wife Beth and acknowledge all the support I\nreceived from her during this process. Writing a book is a time-consuming task, and\nwithout her encouragement, this book never would have happened. Beth, you are fan-\ntastic, and I’m very grateful to have you as my wife. I’d also like to acknowledge my\nchildren, who put up with Dad sitting in his office all day on most weekends and\naccepted the vague answer “Soon” when they asked when I’d be finished writing.\n Next, I thank Guozhang Wang, Matthias Sax, Damian Guy, and Eno Thereska, the\ncore developers of Kafka Streams. Without their brilliant insights and hard work,\nthere would be no Kafka Streams, and I wouldn’t have had the chance to write about\nthis game-changing tool.\n I thank my editor at Manning, Frances Lefkowitz, whose expert guidance and\ninfinite patience made writing a book almost fun. I also thank John Hyaduck for his\nspot-on technical feedback, and Valentin Crettaz, the technical proofer, for his excel-\nlent work reviewing the code. Additionally, I thank the reviewers for their hard work\nand invaluable feedback in making the quality of this book better for all readers:\nAlexander Koutmos, Bojan Djurkovic, Dylan Scott, Hamish Dickson, James Frohnhofer,\nJim Manthely, Jose San Leandro, Kerry Koitzsch, László Hegedüs, Matt Belanger,\nMichele Adduci, Nicholas Whitehead, Ricardo Jorge Pereira Mano, Robin Coe, Sumant\nTambe, and Venkata Marrapu.\n Finally, I’d like to acknowledge all the Kafka developers for building such high-\nquality software, especially Jay Kreps, Neha Narkhede, and Jun Rao—not just for start-\ning Kafka in the first place, but also for founding Confluent, a great and inspiring\nplace to work.\n \n",
      "content_length": 1747,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "xv\nabout this book\nI wrote Kafka Streams in Action to teach you how to get started with Kafka Streams and,\nto a lesser extent, how to work with stream processing in general. My approach to writ-\ning this book is a pair-programming perspective; I imagine myself sitting next to you\nas you write the code and learn the API. You’ll start by building a simple application,\nand you’ll layer on more features as you go deeper into Kafka Streams. You’ll learn\nabout testing and monitoring and, finally, wrap things up by developing an advanced\nKafka Streams application.\nWho should read this book\nKafka Streams in Action is for any developer wishing to get into stream processing.\nWhile not strictly required, knowledge of distributed programming will be helpful in\nunderstanding Kafka and Kafka Streams. Knowledge of Kafka itself is useful but not\nrequired; I’ll teach you what you need to know. Experienced Kafka developers, as well\nas those new to Kafka, will learn how to develop compelling stream-processing appli-\ncations with Kafka Streams. Intermediate-to-advanced Java developers who are famil-\niar with topics like serialization will learn how to use their skills to build a Kafka\nStreams application. The book’s source code is written in Java 8 and makes extensive\nuse of Java 8 lambda syntax, so experience with lambdas (even from another lan-\nguage) will be helpful.\n \n",
      "content_length": 1375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "ABOUT THIS BOOK\nxvi\nHow this book is organized: a roadmap\nThis book has four parts spread over nine chapters. Part 1 introduces a mental model\nof Kafka Streams to show you the big-picture view of how it works. These chapters also\nprovide the basics of Kafka, for those who need them or want a review:\n■\nChapter 1 provides some history of how and why stream processing became\nnecessary for handling real-time data at scale. It also presents the mental model\nof Kafka Streams. I don’t go over any code but rather describe how Kafka\nStreams works.\n■\nChapter 2 is a primer for developers who are new to Kafka. Those with more\nexperience with Kafka can skip this chapter and get right into Kafka Streams.\nPart 2 moves on to Kafka Streams, starting with the basics of the API and continuing\nto the more complex features:\n■\nChapter 3 presents a Hello World application and then presents a more realistic\nexample: developing an application for a fictional retailer, including advanced\nfeatures.\n■\nChapter 4 discusses state and explains how it’s sometimes required for stream-\ning applications. You’ll learn about state store implementations and how to per-\nform joins in Kafka Streams.\n■\nChapter 5 explores the duality of tables and streams, and introduces a new con-\ncept: the KTable. Whereas a KStream is a stream of events, a KTable is a stream\nof related events or an update stream.\n■\nChapter 6 goes into the low-level Processor API. Up to this point, you’ve been\nworking with the high-level DSL, but here you’ll learn how to use the Processor\nAPI when you need to write customized parts of an application.\nPart 3 moves on from developing Kafka Streams applications to managing Kafka\nStreams:\n■\nChapter 7 explains how to test a Kafka Streams application. You’ll learn how to\ntest an entire topology, unit-test a single processor, and use an embedded Kafka\nbroker for integration tests.\n■\nChapter 8 covers how to monitor your Kafka Streams application, both to see\nhow long it takes to process records and to locate potential processing bottle-\nnecks.\nPart 4 is the capstone of the book, where you’ll delve into advanced application devel-\nopment with Kafka Streams:\n■\nChapter 9 covers integrating existing data sources into Kafka Streams using\nKafka Connect. You’ll learn to include database tables in a streaming applica-\ntion. Then, you’ll see how to use interactive queries to provide visualization\nand dashboard applications while data is flowing through Kafka Streams, with-\nout the need for relational databases. The chapter also introduces KSQL,\n \n",
      "content_length": 2551,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "ABOUT THIS BOOK\nxvii\nwhich you can use to run continuous queries over Kafka without writing any\ncode, by using SQL.\nAbout the code\nThis book contains many examples of source code both in numbered listings and\ninline with normal text. In both cases, source code is formatted in a fixed-width font\nlike this to separate it from ordinary text. \n In many cases, the original source code has been reformatted; we’ve added line\nbreaks and reworked indentation to accommodate the available page space in the\nbook. In rare cases, even this was not enough, and listings include line-continuation\nmarkers (➥). Additionally, comments in the source code have often been removed\nfrom the listings when the code is described in the text. Code annotations accompany\nmany of the listings, highlighting important concepts.\n Finally, it’s important to note that many of the code examples aren’t meant to\nstand on their own: they’re excerpts containing only the most relevant parts of what is\ncurrently under discussion. You’ll find all the examples from the book in the accom-\npanying source code in their complete form. Source code for the book’s examples is\navailable from GitHub at https://github.com/bbejeck/kafka-streams-in-action and\nthe publisher’s website at www.manning.com/books/kafka-streams-in-action.\n The source code for the book is an all-encompassing project using the build tool\nGradle (https://gradle.org). You can import the project into either IntelliJ or Eclipse\nusing the appropriate commands. Full instructions for using and navigating the source\ncode can be found in the accompanying README.md file.\nBook forum\nPurchase of Kafka Streams in Action includes free access to a private web forum run by\nManning Publications where you can make comments about the book, ask technical\nquestions, and receive help from the author and from other users. To access the\nforum, go to https://forums.manning.com/forums/kafka-streams-in-action. You can\nalso learn more about Manning’s forums and the rules of conduct at https://forums\n.manning.com/forums/about.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe author, whose contribution to the forum remains voluntary (and unpaid). We sug-\ngest you try asking him some challenging questions lest his interest stray! The forum\nand the archives of previous discussions will be accessible from the publisher’s website\nas long as the book is in print.\n \n",
      "content_length": 2589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "ABOUT THIS BOOK\nxviii\nOther online resources\n■\nApache Kafka documentation: https://kafka.apache.org\n■\nConfluent documentation: https://docs.confluent.io/current\n■\nKafka Streams documentation: https://docs.confluent.io/current/streams/index\n.html#kafka-streams\n■\nKSQL documentation: https://docs.confluent.io/current/ksql.html#ksql\n \n",
      "content_length": 333,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "xix\nabout the author\nBill Bejeck, a contributor to Kafka, works at Confluent on the\nKafka Streams team. He has worked in software development for\nmore than 15 years, including 8 years focused exclusively on the\nbackend, specifically, handling large volumes of data; and on\ningestion teams, using Kafka to improve data flow to downstream\ncustomers. Bill is the author of Getting Started with Google Guava\n(Packt Publishing, 2013) and a regular blogger at “Random\nThoughts on Coding” (http://codingjunkie.net).\n \n",
      "content_length": 511,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "xx\nabout the cover illustration\nThe figure on the cover of Kafka Streams in Action is captioned “Habit of a Turkish Gen-\ntleman in 1700.” The illustration is taken from Thomas Jefferys’ A Collection of the Dresses\nof Different Nations, Ancient and Modern (four volumes), London, published between\n1757 and 1772. The title page states that these are hand-colored copperplate engrav-\nings, heightened with gum arabic. Thomas Jefferys (1719–1771) was called “Geogra-\npher to King George III.” He was an English cartographer who was the leading map\nsupplier of his day. He engraved and printed maps for government and other official\nbodies and produced a wide range of commercial maps and atlases, especially of North\nAmerica. His work as a map maker sparked an interest in local dress customs of the\nlands he surveyed and mapped, which are brilliantly displayed in this collection.\n Fascination with faraway lands and travel for pleasure were relatively new phenom-\nena in the late eighteenth century, and collections such as this one were popular,\nintroducing both the tourist as well as the armchair traveler to the inhabitants of\nother countries. The diversity of the drawings in Jefferys’ volumes speaks vividly of the\nuniqueness and individuality of the world’s nations some 200 years ago. Dress codes\nhave changed since then, and the diversity by region and country, so rich at the time,\nhas faded away. It is now often hard to tell the inhabitant of one continent from\nanother. Perhaps we have traded a cultural and visual diversity for a more varied per-\nsonal life—certainly, a more varied and interesting intellectual and technical life.\n At a time when it is hard to tell one computer book from another, Manning celebrates\nthe inventiveness and initiative of the computer business with book covers based on the\nrich diversity of regional life of two centuries ago, brought back to life by Jefferys’ pictures.\n \n",
      "content_length": 1919,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "Part 1\nGetting started\nwith Kafka Streams\nIn part 1 of this book, we’ll discuss the big data era: how it began with the\nneed to process large amounts of data and eventually progressed to stream pro-\ncessing—processing data as it becomes available. We’ll also discuss what Kafka\nStreams is, and I’ll show you a mental model of how it works without any code so\nyou can focus on the big picture. We’ll also briefly cover Kafka to get you up to\nspeed on how to work with it.\n \n",
      "content_length": 473,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "3\nWelcome to Kafka Streams\nIn this book, you’ll learn how to use Kafka Streams to solve your streaming applica-\ntion needs. From basic extract, transform, and load (ETL) to complex stateful\ntransformations to joining records, we’ll cover the components of Kafka Streams so\nyou can solve these kinds of challenges in your streaming applications.\n Before we dive into Kafka Streams, we’ll briefly explore the history of big data\nprocessing. As we identify problems and solutions, you’ll clearly see how the need\nfor Kafka, and then Kafka Streams, evolved. Let’s look at how the big data era got\nstarted and what led to the Kafka Streams solution.\nThis chapter covers\nUnderstanding how the big data movement \nchanged the programming landscape\nGetting to know how stream processing works \nand why we need it\nIntroducing Kafka Streams\nLooking at the problems solved by Kafka Streams\n \n",
      "content_length": 884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "4\nCHAPTER 1\nWelcome to Kafka Streams\n1.1\nThe big data movement, and how it changed \nthe programming landscape\nThe modern programming landscape has exploded with big data frameworks and\ntechnologies. Sure, client-side development has undergone transformations of its\nown, and the number of mobile device applications has exploded as well. But no mat-\nter how big the mobile device market gets or how client-side technologies evolve,\nthere’s one constant: we need to process more and more data every day. As the\namount of data grows, the need to analyze and take advantage of the benefits of that\ndata grows at the same rate.\n But having the ability to process large quantities of data in bulk (batch processing)\nisn’t always enough. Increasingly, organizations are finding that they need to process\ndata as it becomes available (stream processing). Kafka Streams, a cutting-edge approach\nto stream processing, is a library that allows you to perform per-event processing of\nrecords. Per-event processing means you process each record as soon as it’s avail-\nable—no grouping of data into small batches (microbatching) is required.\nNOTE\nWhen the need to process data as it arrives became more and more\napparent, a new strategy was developed: microbatching. As the name implies,\nmicrobatching is nothing more than batch processing, but with smaller quan-\ntities of data. By reducing the size of the batch, microbatching can sometimes\nproduce results more quickly; but microbatching is still batch processing,\nalthough at faster intervals. It doesn’t give you real per-event processing.\n1.1.1\nThe genesis of big data\nThe internet started to have a real impact on our daily lives in the mid-1990s. Since\nthen, the connectivity provided by the web has given us unparalleled access to infor-\nmation and the ability to communicate instantly with anyone, anywhere in the world.\nAn unexpected byproduct of all this connectivity emerged: the generation of massive\namounts of data.\n For our purposes, I’ll say that the big data era officially began in 1998, the year\nSergey Brin and Larry Page formed Google. Brin and Page developed a new way of\nranking web pages for searches: the PageRank algorithm. At a very high level, the Page-\nRank algorithm rates a website by counting the number and quality of links pointing\nto it. The assumption is that the more important or relevant a web page is, the more\nsites will refer to it.\n Figure 1.1 offers a graphical representation of the PageRank algorithm:\nSite A is the most important, because it has the most references pointing to it.\nSite B is somewhat important. Although it doesn’t have as many references, an\nimportant site does point to it.\nSite C is less important than A or B. More references are pointing to site C than\nsite B, but the quality of those references is lower.\nThe sites at the bottom (D through I) have no references pointing to them.\nThis makes them the least valuable.\n \n",
      "content_length": 2932,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "5\nThe big data movement, and how it changed the programming landscape\nThe figure is an oversimplification of the PageRank algorithm, but it gives you the\nbasic idea of how the algorithm works.\n At the time, PageRank was a revolutionary approach. Previously, searches on the web\nwere more likely to use Boolean logic to return results. If a website contained all or most\nof the terms you were looking for, that website was in the search results, regardless of the\nquality of the content. But running the PageRank algorithm on all internet content\nrequired a new approach—the traditional approaches to working with data took too\nlong. For Google to survive and grow, it needed to index all that content quickly\n(“quickly” being a relative term) and present quality results to the public.\n Google developed another revolutionary approach for processing all that data: the\nMapReduce paradigm. Not only did MapReduce enable Google to do the work it\nneeded to as a company, it inadvertently spawned an entire new industry in computing. \n1.1.2\nImportant concepts from MapReduce\nThe map and reduce functions weren’t new concepts when Google developed Map-\nReduce. What was unique about Google’s approach was applying those simple con-\ncepts at a massive scale across many machines.\n At its heart, MapReduce has roots in functional programming. A map function\ntakes some input and maps that input into something else without changing the origi-\nnal value. Here’s a simple example in Java 8, where a LocalDate object is mapped into\na String message, while the original LocalDate object is left unmodified:\nFunction<LocalDate, String> addDate =\n(date) -> \"The Day of the week is \" + date.getDayOfWeek();\nSite A\nSite B\nSite C\nSite D\nSite E\nSite F\nSite G\nSite H\nSite I\nFigure 1.1\nThe PageRank algorithm in action. The circles represent websites, and the larger \nones represent sites with more links pointing to them from other sites.\n \n",
      "content_length": 1923,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "6\nCHAPTER 1\nWelcome to Kafka Streams\nAlthough simple, this short example is sufficient for demonstrating what a map func-\ntion does.\n On the other hand, a reduce function takes a number of parameters and reduces\nthem down to a singular, or at least smaller, value. A good example of that is adding\ntogether all the values in a collection of numbers.\n To perform a reduction on a collection of numbers, you first provide an initial\nstarting value. In this case, we’ll use 0 (the identity value for addition). The next step\nis adding the seed value to the first number in the list. You then add the result of that\nfirst addition to the second number in the list. The function repeats this process until\nit reaches the last value, producing a single number.\n Here are the steps to reduce a List<Integer> containing the values 1, 2, and 3:\n0 + 1 = 1    \n1 + 2 = 3           \n3 + 3 = 6        \nAs you can see, a reduce function collapses results together to form smaller results. As\nin the map function, the original list of numbers is left unchanged.\n The following example shows an implementation of a simple reduce function\nusing a Java 8 lambda:\nList<Integer> numbers = Arrays.asList(1, 2, 3);\nint sum = numbers.reduce(0, (i, j) -> i + j );\nThe main topic of this book is not MapReduce, so we’ll stop our background discus-\nsion here. But some of the key concepts introduced by the MapReduce paradigm\n(later implemented in Hadoop, the original open source version based on Google’s\nMapReduce white paper) come into play in Kafka Streams:\nHow to distribute data across a cluster to achieve scale in processing\nThe use of key/value pairs and partitions to group distributed data together\nInstead of avoiding failure, embracing failure by using replication\nThe following sections look at these concepts in general terms. Pay attention, because\nyou’ll see them coming up again and again in the book.\nDISTRIBUTING DATA ACROSS A CLUSTER TO ACHIEVE SCALE IN PROCESSING\nWorking with 5 TB (5,000 GB) of data could be overwhelming for one machine. But if\nyou can split up the data and involve more machines, so each is processing a manage-\nable amount, your problem is minimized. Table 1.1 illustrates this clearly.\n As you can see from the table, you may start out with an unwieldy amount of data\nto process, but by spreading the load across more servers, you eliminate the difficulty\nAdds the seed value \nto the first number\nTakes the result from \nstep 1 and adds it to \nthe second number \nin the list\nAdds the sum of step 2 \nto the third number\n \n",
      "content_length": 2542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "7\nThe big data movement, and how it changed the programming landscape\nof processing the data. The 1 GB of data in the last line of the table is something a lap-\ntop could easily handle.\n This is the first key concept to understand about MapReduce: by spreading the\nload across a cluster of machines, you can turn an overwhelming amount of data into\na manageable amount. \nUSING KEY/VALUE PAIRS AND PARTITIONS TO GROUP DISTRIBUTED DATA\nThe key/value pair is a simple data structure with powerful implications. In the previ-\nous section, you saw the value of spreading a massive amount of data over a cluster of\nmachines. Distributing your data solves the processing problem, but now you have the\nproblem of collecting the distributed data back together.\n To regroup distributed data, you can use the keys from the key/value pairs to par-\ntition the data. The term partition implies grouping, but I don’t mean grouping by\nidentical keys, but rather by keys that have the same hash code. To split data into par-\ntitions by key, you can use the following formula:\nint partition = key.hashCode() % numberOfPartitions;\nFigure 1.2 shows how you could apply a hashing function to take results from Olympic\nevents stored on separate servers and group them on partitions for different events.\nTable 1.1\nHow splitting up 5 TB improves processing throughput\nNumber of machines\nAmount of data processed per server\n10\n500 GB\n100\n50 GB\n1000\n5 GB\n5000\n1 GB\nswimming, result_1\nsprinting, result_3\nswimming, result_3\nsprinting, result_2\nswimming, result_2\nsprinting, result_1\nSwim results partition\nSprint results partition\nPartition = key.hashCode % 2\nFigure 1.2\nGrouping records by key on partitions. Even though the records start out on separate \nservers, they end up in the appropriate partitions.\n \n",
      "content_length": 1785,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "8\nCHAPTER 1\nWelcome to Kafka Streams\nAll the data is stored as key/value pairs. In the image below the key is the name of the\nevent, and the value is a result for an individual athlete.\n Partitioning is an important concept, and you’ll see detailed examples in later\nchapters. \nEMBRACING FAILURE BY USING REPLICATION\nAnother key component of Google’s MapReduce is the Google File System (GFS). Just\nas Hadoop is the open source implementation of MapReduce, Hadoop File System\n(HDFS) is the open source implementation of GFS.\n At a very high level, both GFS and HDFS split data into blocks and distribute\nthose blocks across a cluster. But the essential part of GFS/HDFS is the approach to\nserver and disk failure. Instead of trying to prevent failure, the framework embraces\nfailure by replicating blocks of data across the cluster (by default, the replication\nfactor is 3).\n By replicating data blocks on different servers, you no longer have to worry about\ndisk failures or even complete server failures causing a halt in production. Replication\nof data is crucial for giving distributed applications fault tolerance, which is essential\nfor a distributed application to be successful. You’ll see later how partitions and repli-\ncation work in Kafka Streams. \n1.1.3\nBatch processing is not enough\nHadoop caught on with the computing world like wildfire. It allowed people to pro-\ncess vast amounts of data and have fault tolerance while using commodity hardware\n(cost savings). But Hadoop/MapReduce is a batch-oriented process, which means you\ncollect large amounts of data, process it, and then store the output for later use. Batch\nprocessing is a perfect fit for something like PageRank because you can’t make deter-\nminations of what resources are valuable across the entire internet by watching user\nclicks in real time.\n But business also came under increasing pressure to respond to important ques-\ntions more quickly, such as these:\nWhat is trending right now?\nHow many invalid login attempts have there been in the last 10 minutes?\nHow is our recently released feature being utilized by the user base?\nIt was apparent that another solution was needed, and that solution emerged as stream\nprocessing. \n1.2\nIntroducing stream processing\nThere are varying definitions of stream processing. In this book, I define stream process-\ning as working with data as it’s arriving in your system. The definition can be further\nrefined to say that stream processing is the ability to work with an infinite stream of\ndata with continuous computation, as it flows, with no need to collect or store the data\nto act on it.\n \n",
      "content_length": 2620,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "9\nIntroducing stream processing\n Figure 1.3 represents a stream of data, with each circle on the line representing\ndata at a point in time. Data is continuously flowing, as data in stream processing is\nunbounded.\nWho needs to use stream processing? Anyone who needs quick feedback from an\nobservable event. Let’s look at some examples.\n1.2.1\nWhen to use stream processing, and when not to use it\nLike any technical solution, stream processing isn’t a one-size-fits-all solution. The\nneed to quickly respond to or report on incoming data is a good use case for stream\nprocessing. Here are a few examples:\nCredit card fraud—A credit card owner may not notice a card has been stolen,\nbut by reviewing purchases as they happen against established patterns (loca-\ntion, general spending habits), you may be able to detect a stolen credit card\nand alert the owner.\nIntrusion detection—Analyzing application log files after a breach has occurred\nmay be helpful to prevent future attacks or to improve security, but the ability to\nmonitor aberrant behavior in real time is critical.\nA large race, such as the New York City Marathon—Almost all runners will have a\nchip on their shoe, and when runners pass sensors along the course, you can\nuse that information to track the runners’ positions. By using the sensor data,\nyou can determine the leaders, spot potential cheating, and detect whether a\nrunner is potentially having problems.\nThe financial industry—The ability to track market prices and direction in real\ntime is essential for brokers and consumers to make effective decisions about\nwhen to sell or buy.\nOn the other hand, stream processing isn’t a solution for all problem domains. To\neffectively make forecasts of future behavior, for example, you need to use a large\namount of data over time to eliminate anomalies and identify patterns and trends.\nHere the focus is on analyzing data over time, rather than just the most current data:\nEconomic forecasting—Information is collected on many variables over an extended\nperiod of time in an attempt to make an accurate forecast, such as trends in\ninterest rates for the housing market.\nSchool curriculum changes—Only after one or two testing cycles can school adminis-\ntrators measure whether curriculum changes are achieving their goals.\nFigure 1.3\nThis marble diagram is a simple representation of stream processing. Each circle represents \nsome information or an event occurring at a particular point in time. The number of events is unbounded \nand moves continually from left to right.\n \n",
      "content_length": 2550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "10\nCHAPTER 1\nWelcome to Kafka Streams\nHere are the key points to remember: If you need to report on or take action immedi-\nately as data arrives, stream processing is a good approach. If you need to perform\nin-depth analysis or are compiling a large repository of data for later analysis, a stream-\nprocessing approach may not be a good fit. Let’s now walk through a concrete exam-\nple of stream processing. \n1.3\nHandling a purchase transaction\nLet’s start by applying a general stream-processing approach to a retail sales example.\nThen we’ll look at how you can use Kafka Streams to implement the stream-processing\napplication.\n Suppose Jane Doe is on her way home from work and remembers she needs tooth-\npaste. She stops at a ZMart, goes in to pick up the toothpaste, and heads to the check-\nout to pay. The cashier asks Jane if she’s a member of the ZClub and scans her\nmembership card, so Jane’s membership info is now part of the purchase transaction.\n When the total is rung up, Jane hands the cashier her debit card. The cashier\nswipes the card and gives Jane the receipt. As Jane is walking out of the store, she\nchecks her email, and there’s a message from ZMart thanking her for her patronage,\nwith various coupons for discounts on Jane’s next visit.\n This transaction is a normal occurrence that a customer wouldn’t give a second\nthought to, but you’ll have recognized it for what it is: a wealth of information that can\nhelp ZMart run more efficiently and serve customers better. Let’s go back in time a lit-\ntle, to see how this transaction became a reality.\n1.3.1\nWeighing the stream-processing option\nSuppose you’re the lead developer for ZMart’s streaming-data team. ZMart is a big-\nbox retail store with several locations across the country. ZMart does great business,\nwith total sales for any given year upwards of $1 billion. You’d like to start mining the\ndata from your company’s transactions to make the business more efficient. You know\nyou have a tremendous amount of sales data to work with, so whatever technology you\nimplement will need to be able to work fast and scale to handle this volume of data.\n You decide to use stream processing because there are business decisions and\nopportunities that you can take advantage of as each transaction occurs. After data is\ngathered, there’s no reason to wait for hours to make decisions. You get together with\nmanagement and your team and come up with the following four primary require-\nments for the stream-processing initiative to succeed:\nPrivacy—First and foremost, ZMart values its relationship with its customers.\nWith all of today’s privacy concerns, your first goal is to protect customers’ pri-\nvacy, and protecting their credit card numbers is the highest priority. However\nyou use the transaction information, customer credit card information should\nnever be at risk of exposure.\nCustomer rewards—A new customer-rewards program is in place, with customers\nearning bonus points based on the amount of money they spend on certain\n \n",
      "content_length": 3017,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "11\nHandling a purchase transaction\nitems. The goal is to notify customers quickly, once they’ve received a reward—\nyou want them back in the store! Again, appropriate monitoring of activity is\nrequired here. Remember how Jane received an email immediately after leav-\ning the store? That’s the kind of exposure you want for the company.\nSales data—ZMart would like to refine its advertising and sales strategy. The\ncompany wants to track purchases by region to figure out which items are more\npopular in certain parts of the country. The goal is to target sales and specials\nfor best-selling items in a given area of the country.\nStorage—All purchase records need to be saved in an off-site storage center for\nhistorical and ad hoc analysis.\nThese requirements are straightforward enough on their own, but how would you go\nabout implementing them against a single purchase transaction like Jane Doe’s?\n1.3.2\nDeconstructing the requirements into a graph\nLooking at the preceding requirements, you can quickly recast them in a directed acyclic\ngraph (DAG). The point where the customer completes the transaction at the register\nis the source node for the entire graph. ZMart’s requirements become the child nodes\nof the main source node (figure 1.4).\nNext, you need to determine how to map a purchase transaction to the require-\nments graph. \nPatterns\nMasking\nRewards\nPurchase\nStorage\nFigure 1.4\nThe business \nrequirements for the streaming \napplication presented as a \ndirected acyclic graph. Each \nvertex represents a requirement, \nand the edges show the flow of \ndata through the graph.\n \n",
      "content_length": 1592,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "12\nCHAPTER 1\nWelcome to Kafka Streams\n1.4\nChanging perspective on a purchase transaction\nIn this section, we’ll walk through the steps of a purchase and see how it relates, at a\nhigh level, to the requirements graph from figure 1.4. In the next section, we’ll look at\nhow to apply Kafka Streams to this process.\n1.4.1\nSource node\nThe graph’s source node (figure 1.5) is where the application consumes the purchase\ntransaction. This node is the source of the sales transaction information that will flow\nthrough the graph. \n1.4.2\nCredit card masking node\nThe child node of the graph source is where the credit card masking takes place (fig-\nure 1.6). This is the first vertex or node in the graph that represents the business\nrequirements, and it’s the only node that receives the raw sales data from the source\nnode, effectively making this node the source for all other nodes connected to it.\nFor the credit card masking operation, you make a copy of the data and then convert\nall the digits of the credit card number to an x, except the last four digits. The data\nflowing through the rest of the graph will have the credit card field converted to the\nxxxx-xxxx-xxxx-1122 format. \nThe point of purchase is the source or\nparent node for the entire graph.\nPurchase\nFigure 1.5\nThe simple start for the sales \ntransaction graph. This node is the source of \nraw sales transaction information that will \nflow through the graph.\nCredit card numbers are masked\nhere for security purposes.\nMasking\nPurchase\nFigure 1.6\nThe first node in the graph that \nrepresents the business requirements. This \nnode is responsible for masking credit card \nnumbers and is the only node that receives \nthe raw sales data from the source node, \neffectively making it the source for all other \nnodes connected to it.\n \n",
      "content_length": 1792,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "13\nChanging perspective on a purchase transaction\n1.4.3\nPatterns node\nThe patterns node (figure 1.7) extracts the relevant information to establish where\ncustomers purchase products throughout the country. Instead of making a copy of the\ndata, the patterns node will retrieve the item, date, and ZIP code for the purchase and\ncreate a new object containing those fields. \n1.4.4\nRewards node\nThe next child node in the process is the rewards accumulator (figure 1.8). ZMart has\na customer rewards program that gives customers points for purchases made in the\nstore. This node’s role is to extract the dollar amount spent and the client’s ID and\ncreate a new object containing those two fields. \n1.4.5\nStorage node\nThe final child node writes the purchase data out to a NoSQL data store for further\nanalysis (figure 1.9).\n We’ve now tracked the example purchase transaction through ZMart’s graph of\nrequirements. Let’s see how you can use Kafka Streams to convert this graph into a\nfunctional streaming application. \nData is extracted here for\ndetermining purchase patterns.\nPatterns\nMasking\nPurchase\nFigure 1.7\nThe patterns node consumes purchase information from the \nmasking node and converts it into a record showing when a customer \npurchased an item and the ZIP code where the customer completed the \ntransaction.\n \n",
      "content_length": 1320,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "14\nCHAPTER 1\nWelcome to Kafka Streams\nData is pulled from the transaction here for\nuse in calculating customer rewards.\nPatterns\nMasking\nRewards\nPurchase\nFigure 1.8\nThe rewards node is \nresponsible for consuming sales records \nfrom the masking node and converting \nthem into records containing the total of \nthe purchase and the customer ID.\nPurchase is stored here to\nbe available for further\nad hoc analysis.\nPatterns\nMasking\nRewards\nPurchase\nStorage\nFigure 1.9\nThe storage node consumes records from the masking node as well. \nThese records aren’t converted into any other format but are stored in a NoSQL \ndata store for ad hoc analysis later.\n \n",
      "content_length": 650,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "15\nKafka Streams as a graph of processing nodes\n1.5\nKafka Streams as a graph of processing nodes\nKafka Streams is a library that allows you to perform per-event processing of records.\nYou can use it to work on data as it arrives, without grouping data in microbatches.\nYou process each record as soon as it’s available.\n Most of ZMart’s goals are time sensitive, in that you want to take action as soon as\npossible. Preferably, you’ll be able to collect information as events occur. Additionally,\nthere are several ZMart locations across the country, so you’ll need all the transaction\nrecords to funnel into a single flow or stream of data for analysis. For these reasons,\nKafka Streams is a perfect fit. Kafka Streams allows you to process records as they\narrive and gives you the low-latency processing you require.\n In Kafka Streams, you define a topology of processing nodes (I’ll use the terms pro-\ncessor and node interchangeably). One or more nodes will have as source Kafka topic(s),\nand you can add additional nodes, which are considered child nodes (if you aren’t\nfamiliar with what a Kafka topic is, don’t worry—I'll explain in detail in chapter 2). Each\nchild node can define other child nodes. Each processing node performs its assigned\ntask and then forwards the record to each of its child nodes. This process of perform-\ning work and then forwarding data to any child nodes continues until every child\nnode has executed its function.\n Does this process sound familiar? It should, because you similarly transformed\nZMart’s business requirements into a graph of processing nodes. Traversing a graph is\nhow Kafka Streams works—it’s a DAG or topology of processing nodes.\n You start with a source or parent node, which has one or more children. Data\nalways flows from the parent to the child nodes, never from child to parent. Each\nchild node, in turn, can define child nodes of its own, and so on.\n Records flow through the graph in a depth-first manner. This approach has signifi-\ncant implications: each record (a key/value pair) is processed in full by the entire\ngraph before another record is forwarded through the topology. Because each record\nis processed depth-first through the whole DAG, there’s no need to have backpressure\nbuilt into Kafka Streams.\nDEFINITION\nThere are varying definitions of backpressure, but here I define it\nas the need to restrict the flow of data by buffering or using a blocking mech-\nanism. Backpressure is necessary when a source is producing data faster than a\nsink can receive and process that data.\nBy being able to connect or chain together multiple processors, you can quickly build\nup complex processing logic, while at the same time keeping each component rela-\ntively straightforward. It’s in this composition of processors that Kafka Streams’ power\nand complexity come into play.\nDEFINITION\nA topology is the way you arrange the parts of an entire system and\nconnect them with each other. When I say Kafka Streams has a topology, I’m\nreferring to transforming data by running through one or more processors.\n \n",
      "content_length": 3070,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "16\nCHAPTER 1\nWelcome to Kafka Streams\n1.6\nApplying Kafka Streams to the purchase \ntransaction flow\nLet’s build a processing graph again, but this time we’ll create a Kafka Streams pro-\ngram. To refresh your memory, figure 1.4 shows the requirements graph for ZMart’s\nbusiness requirements. Remember, the vertexes are processing nodes that handle\ndata, and the edges show the flow of data.\n Although you’ll be building a Kafka Streams program as you build your new graph,\nyou’ll still be taking a relatively high-level approach. Some details will be left out. We’ll\ngo into more detail later in the book when we look at the actual code.\n The Kafka Streams program will consume records, and when it does, you’ll convert\nthe raw records into Purchase objects. These pieces of information will make up a\nPurchase object:\nZMart customer ID (scanned from the member card)\nTotal dollar amount spent\nItem(s) purchased\nZIP code of the store where the purchase took place\nDate and time of the transaction\nDebit or credit card number\n1.6.1\nDefining the source\nThe first step in any Kafka Streams program is to establish a source for the stream.\nThe source could be any of the following:\nA single topic\nMultiple topics in a comma-separated list\nA regex that can match one or more topics\nIn this case, it will be a single topic named transactions. If any of these Kafka terms\nare unfamiliar to you, remember—they’ll be explained in chapter 2.\n It’s important to note that to Kafka, the Kafka Streams program looks like any\nother combination of consumers and producers. Any number of applications could\nKafka Streams and Kafka\nAs you might have guessed from the name, Kafka Streams runs on top of Kafka. In\nthis introductory chapter, you don’t need to know about Kafka, because we’re focus-\ning more how Kafka Streams works conceptually. A few Kafka-specific terms may be\nmentioned, but for the most part, we’ll be concentrating on the stream-processing\naspects of Kafka Streams.\nIf you’re new to Kafka or are unfamiliar with it, you’ll learn what you need to know\nabout Kafka in chapter 2. Knowledge of Kafka is essential for working effectively with\nKafka Streams. \n \n",
      "content_length": 2165,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "17\nApplying Kafka Streams to the purchase transaction flow\nbe reading from the same topic in conjunction with your streaming program. Figure 1.10\nrepresents the source node in the topology. \n1.6.2\nThe first processor: masking credit card numbers\nNow that you have a source defined, you can start creating processors that will work\non the data. Your first goal is to mask the credit card numbers recorded in the incom-\ning purchase records. The first processor will convert credit card numbers from some-\nthing like 1234-5678-9123-2233 to xxxx-xxxx-xxxx-2233.\n The KStream.mapValues method will perform the masking represented in fig-\nure 1.11. It will return a new KStream instance with values masked as specified by a\nValueMapper. This particular KStream instance will be the parent processor for any\nother processors you define.\nCREATING PROCESSOR TOPOLOGIES\nEach time you create a new KStream instance by using a transformation method,\nyou’re in essence building a new processor that’s connected to the other processors\nalready created. By composing processors, you can use Kafka Streams to create com-\nplex data flows elegantly.\n It’s important to note that calling a method that returns a new KStream instance\ndoesn’t cause the original instance to stop consuming messages. A transforming method\nSource\nFigure 1.10\nThe source node: a Kafka topic\nChild node of the source node\nSource node consuming message from\nthe Kafka transaction topic\nSource\nMasking\nFigure 1.11\nThe masking processor is a \nchild of the main source node. It receives \nall the raw sales transactions and emits \nnew records with the credit card number \nmasked.\n \n",
      "content_length": 1636,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "18\nCHAPTER 1\nWelcome to Kafka Streams\ncreates a new processor and adds it to the existing processor topology. The updated\ntopology is then used as a parameter to create the next KStream instance, which starts\nreceiving messages from the point of its creation.\n It’s very likely that you’ll build new KStream instances to perform additional trans-\nformations while retaining the original stream for its original purpose. You’ll work\nwith an example of this when you define the second and third processors.\n It’s possible to have a ValueMapper convert an incoming value to an entirely new\ntype, but in this case it will return an updated copy of the Purchase object. Using a\nmapper to update an object is a pattern you’ll see frequently.\n You should now have a clear image of how you can build up your processor pipe-\nline to transform and output data.\n1.6.3\nThe second processor: purchase patterns\nThe next processor to create is one that can capture information necessary for deter-\nmining purchase patterns in different regions of the country (figure 1.12). To do this,\nyou’ll add a child-processing node to the first processor (KStream) you created. The\nfirst processor produces Purchase objects with the credit card number masked.\n The purchase-patterns processor receives a Purchase object from its parent node\nand maps the object to a new PurchasePattern object. The mapping process extracts\nHere the Purchase object is “mapped”\nto a PurchasePatterns object.\nThe child processor node of the\npatterns processor has a child node\nthat writes the PurchasePatterns object\nout to the patterns topic. The\nformat is JSON.\npatterns\ntopic\nPatterns\nMasking\nSource\nFigure 1.12\nThe purchase-pattern processor takes Purchase objects and converts \nthem into PurchasePattern objects containing the items purchased and the ZIP \ncode where the transaction took place. A new processor takes records from the \npatterns processor and writes them out to a Kafka topic.\n \n",
      "content_length": 1954,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "19\nApplying Kafka Streams to the purchase transaction flow\nthe item purchased (toothpaste, for example) and the ZIP code it was bought in and\nuses that information to create the PurchasePattern object. We’ll go over exactly how\nthis mapping process occurs in chapter 3.\n Next, the purchase-patterns processor adds a child processor node that receives\nthe new PurchasePattern object and writes it out to a Kafka topic named patterns.\nThe PurchasePattern object is converted to some form of transferable data when it’s\nwritten to the topic. Other applications can then consume this information and use it\nto determine inventory levels as well as purchasing trends in a given area. \n1.6.4\nThe third processor: customer rewards\nThe third processor will extract information for the customer rewards program (fig-\nure 1.13). This processor is also a child node of the original processor. It receives the\nPurchase objects and maps them to another type: the RewardAccumulator object.\n The customer rewards processor also adds a child-processing node to write the\nRewardAccumulator object out to a Kafka topic, rewards. By consuming records from\nthe rewards topic, other applications can determine rewards for ZMart customers and\nproduce, for example, the email that Jane Doe received. \nHere the Purchase object is “mapped”\nto a RewardAccumulator object.\nThe child processor node of the\nRewards processor has a child node\nthat writes the RewardAccumulator\nobject out to the rewards topic.\nThe format is JSON.\npatterns\ntopic\nPatterns\nMasking\nSource\nRewards\nrewards\ntopic\nFigure 1.13\nThe customer rewards processor is responsible for transforming Purchase objects \ninto a RewardAccumulator object containing the customer ID, date, and dollar amount of the \ntransaction. A child processor writes the Rewards objects to another Kafka topic.\n \n",
      "content_length": 1830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "20\nCHAPTER 1\nWelcome to Kafka Streams\n1.6.5\nThe fourth processor—writing purchase records\nThe last processor is shown in figure 1.14. This is the third child node of the masking\nprocessor node, and it writes the entire masked purchase record out to a topic called\npurchases. This topic will be used to feed a NoSQL storage application that will con-\nsume the records as they come in. These records will be used for later analysis.\nAs you can see, the first processor, which masks the credit card number, feeds three\nother processors: two that further refine or transform the data, and one that writes the\nmasked results to a topic for further use by other consumers. By using Kafka Streams,\nyou can build up a powerful processing graph of connected nodes to perform stream\nprocessing on your incoming data. \nSummary\nKafka Streams is a graph of processing nodes that combine to provide powerful\nand complex stream processing.\nBatch processing is powerful, but it’s not enough to satisfy real-time needs for\nworking with data.\nThis last processor writes out\nthe purchase transaction as\nJSON to the purchases topic,\nwhich is consumed by a NoSQL\nstorage engine.\npatterns\ntopic\nPatterns\nMasking\nSource\nRewards\nrewards\ntopic\npurchases\ntopic\nFigure 1.14\nThe final processor is responsible for writing out the entire Purchase object to \nanother Kafka topic. The consumer for this topic will store the results in a NoSQL store such as \nMongoDB.\n \n",
      "content_length": 1440,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "21\nSummary\nDistributing data, key/value pairs, partitioning, and data replication are critical\nfor distributed applications.\nTo understand Kafka Streams, you should know some Kafka. For those who don’t\nknow Kafka, we’ll cover the essentials in chapter 2:\nInstalling Kafka and sending a message\nExploring Kafka’s architecture and what a distributed log is\nUnderstanding topics and how they’re used in Kafka\nUnderstanding how producers and consumers work and how to write them\neffectively\nIf you’re already comfortable with Kafka, feel free to go straight to chapter 3, where\nwe’ll build a Kafka Streams application based on the example discussed in this chapter.\n \n",
      "content_length": 669,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "22\nKafka quickly\nAlthough this is a book about Kafka Streams, it’s impossible to explore Kafka\nStreams without discussing Kafka. After all, Kafka Streams is a library that runs on\nKafka.\n Kafka Streams is designed very well, so it’s possible to get up and running with\nlittle or no Kafka experience, but your progress and ability to fine-tune Kafka will\nbe limited. Having a good fundamental knowledge of Kafka is essential to get the\nmost out of Kafka Streams.\nNOTE\nThis chapter is for developers who are interested in getting started\nwith Kafka Streams but have little or no experience with Kafka itself. If you\nhave a good working knowledge of Kafka, feel free to skip this chapter and\nproceed directly to chapter 3.\nThis chapter covers\nExamining the Kafka architecture\nSending messages with producers\nReading messages with consumers\nInstalling and running Kafka\n \n",
      "content_length": 872,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "23\nUsing Kafka to handle data\nKafka is too large a topic to cover in its entirety in one chapter. I’ll cover enough to\ngive you a good understanding how Kafka works and a few of the core configuration\nsettings you’ll need to know. For in-depth coverage of Kafka, take a look at Kafka in\nAction by Dylan Scott (Manning, 2018).\n2.1\nThe data problem\nOrganizations today are swimming in data. Internet companies, financial businesses,\nand large retailers are better positioned now than ever to use this data, both to serve\ntheir customers better and to find more efficient ways of conducting business. (We’re\ngoing to take a positive outlook on this situation and assume only good intentions\nwhen looking at customer data.)\n Let’s consider the various requirements you’d like to have in the ZMart data-\nmanagement solution:\nYou need a way to send data to a central storage quickly.\nBecause machines frequently fail, you also need the ability to have your data\nreplicated, so those inevitable failures don’t cause downtime and data loss.\nYou need the potential to scale to any number of consumers of data without\nhaving to keep track of different applications. You need to make the data avail-\nable to anyone in an organization, but not have to keep track of who has and\nhas not viewed the data.\n2.2\nUsing Kafka to handle data\nIn chapter 1, you were introduced to the large retail company ZMart. At that point,\nZMart wanted a streaming platform to use the company’s sales data in order to offer\nbetter customer service and improve sales overall. But six months before that, ZMart\nwas looking to get a handle on its data situation. ZMart had a custom solution that ini-\ntially worked well but had become unmanageable for reasons you’ll soon see.\n2.2.1\nZMart’s original data platform\nOriginally, ZMart was a small company that had retail sales data flowing into its system\nfrom separate applications. This worked fine initially, but over time it became evident\nthat a new approach would be needed. Data from sales in one department is not of\ninterest only to that department. Several parts of the company are interested, and\neach part has a different take on what’s important and how they want the data struc-\ntured. Figure 2.1 shows ZMart’s original data platform.\n Over time, ZMart continued to grow by acquiring other companies and expanding\nits offerings in existing stores. With each addition, the connections between applica-\ntions become more complicated. What started out as a handful of applications com-\nmunicating with each other turned into a veritable pile of spaghetti. As you can see in\nfigure 2.2, even with just three applications, the number of connections is cumber-\nsome and confusing. You can see how adding new applications will make this data\narchitecture unmanageable over time. \n \n",
      "content_length": 2802,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "24\nCHAPTER 2\nKafka quickly\n2.2.2\nA Kafka sales transaction data hub\nA solution to ZMart’s problem is to create one intake process to hold all transaction\ndata—a transaction data hub. This transaction data hub should be stateless, accepting\nSales\nAuditing\nMarketing\nFigure 2.1\nThe original data architecture for ZMart was simple enough to \nhave information flowing to and from each source of information.\nSales\nAcquired company A\nAuditing\nMarketing\nAcquired company C\nAcquired company B\nCustomer info\nFigure 2.2\nWith more applications being added over time, connecting all these information sources has \nbecome complex.\n \n",
      "content_length": 621,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "25\nKafka architecture\ntransaction data and storing it in such a fashion that any consuming application can\npull the information it needs. It will be up to the consuming application to keep track\nof what it’s seen. The transaction data hub will only know how long it’s been holding\nany transaction data, and when that data should be rolled off or deleted.\n In case you haven’t guessed it yet, we have the perfect use case here for Kafka.\nKafka is a fault-tolerant, robust publish/subscribe system. A single Kafka node is called\na broker, and multiple Kafka servers make up a cluster. Kafka stores messages written by\nproducers in topics. Consumers subscribe to topics and contact Kafka to see if messages\nare available in those subscribed topics. Figure 2.3 shows how you can envision Kafka\nas the sales transaction data hub.\nYou’ve seen an overview of Kafka from 50,000 feet. We’ll take a closer look in the fol-\nlowing sections. \n2.3\nKafka architecture\nIn the next several subsections, we’ll look at the key parts of Kafka’s architecture and\nat how Kafka works. If you’re interested in kicking the tires on Kafka sooner rather\nthan later, skip ahead to section 2.6, on installing and running Kafka. After you’ve got\nit installed, come back here to continue learning about Kafka.\nSales\nWith Kafka as a sales transaction\ndata hub, the architecture becomes\nmuch simpler. Each application only\nneeds to know how to read/write to\nKafka. Adding or removing\napplications has no effect on other\napplications in the data processing.\nAcquired company A\nMarketing\nAcquired company C\nAcquired company B\nCustomer info\nAuditing\nKafka\nFigure 2.3\nUsing Kafka as a sales transaction hub simplifies the ZMart data architecture \nsignificantly. Now each machine doesn’t need to know about every other source of \ninformation. All they need to know is how to read from and write to Kafka.\n \n",
      "content_length": 1870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "26\nCHAPTER 2\nKafka quickly\n2.3.1\nKafka is a message broker\nEarlier, I stated that Kafka is a publish/subscribe system, but it would be more precise\nto say that Kafka acts as a message broker. A broker is an intermediary that brings\ntogether two parties that don’t necessarily know each other for a mutually beneficial\nexchange or deal. Figure 2.4 shows the evolution of the ZMart data infrastructure.\nThe producers and consumers have been added to show how the individual parts\ncommunicate with Kafka. They don’t communicate directly with each other.\nKafka stores messages in topics and retrieves messages from topics. There’s no direct\nconnection between the producers and the consumers of the messages. Additionally,\nKafka doesn’t keep any state regarding the producers or consumers. It acts solely as a\nmessage clearinghouse.\n The underlying technology of a Kafka topic is a log, which is a file that Kafka\nappends incoming records to. To help manage the load of messages coming into a\ntopic, Kafka uses partitions. We discussed partitions in chapter 1, and you may recall\nthat one use of partitions is to bring data located on different machines together on\nthe same server. We’ll discuss partitions in detail shortly. \nSales\nIn this simpliﬁed view of Kafka,\nwe’re assuming a cluster is installed.\nAll output is sent from a producer, and\ninput is consumed by a consumer.\nAcquired company A\nConsumer\nMarketing\nAcquired company C\nAcquired company B\nCustomer info\nAuditing\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nKafka\nZooKeeper\nA cluster of ZooKeeper nodes communicates\nwith Kafka to maintain topic info and keep\ntrack of brokers in the cluster.\nFigure 2.4\nKafka is a message broker. Producers send messages to Kafka, and those messages \nare stored and made available to consumers via subscriptions to topics.\n \n",
      "content_length": 1891,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "27\nKafka architecture\n2.3.2\nKafka is a log\nThe mechanism underlying Kafka is the log. Most software engineers are familiar with\nlogs that track what an application’s doing. If you’re having performance issues or\nerrors in your application, the first place to check is the application logs. But that’s a\ndifferent sort of log. In the context of Kafka (or any other distributed system), a log is\n“an append-only, totally ordered sequence of records ordered by time.”1\n Figure 2.5 shows what a log looks like. An application appends records to the end\nof the log as they arrive. Records have an implied ordering by time, even though there\nmight not be a timestamp associated with each record, because the earliest records\nare to the left and the last record to arrive is at the right end.\nLogs are a simple data abstraction with powerful implications. If you have records in\norder with respect to time, resolving conflicts or determining which update to apply\nto different machines becomes straightforward: the latest record wins.\n Topics in Kafka are logs that are segregated by topic name. You could almost\nthink of topics as labeled logs. If the log is replicated among a cluster of machines,\nand a single machine goes down, it’s easy to bring that server back up: just replay\nthe log file. The ability to recover from failure is precisely the role of a distributed\ncommit log.\n We’ve only scratched the surface of a very deep topic when it comes to distributed\napplications and data consistency, but what you’ve seen so far should give you a basic\nunderstanding of what’s going on under the covers with Kafka.\n2.3.3\nHow logs work in Kafka\nWhen you install Kafka, one of the configuration settings is log.dir, which specifies\nwhere Kafka stores log data. Each topic maps to a subdirectory under the specified log\ndirectory. There will be as many subdirectories as there are topic partitions, with a for-\nmat of partition-name_partition-number (I’ll cover partitions in the next section). Inside\n1 Jay Kreps, “The Log: What Every Software Engineer Should Know About Real-time Data’s Unifying Abstrac-\ntion,” http://mng.bz/eE3w.\nFirst record to arrive\nLatest record to arrive\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFigure 2.5\nA log is a file where incoming records are appended—each \nnewly arrived record is placed immediately after the last record received. \nThis process orders the records in the file by time.\n \n",
      "content_length": 2394,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "28\nCHAPTER 2\nKafka quickly\neach directory is the log file where incoming messages are appended. Once the log\nfiles reach a certain size (either a number of records or size on disk), or when a con-\nfigured time difference between message timestamps is reached, the log file is\n“rolled,” and Kafka appends incoming messages to a new log (see figure 2.6).\nYou can see that logs and topics are highly connected concepts. You could say that a\ntopic is a log, or that it represents a log. The topic name gives you a good handle on\nwhich log the messages sent to Kafka via producers will be stored in. Now that we’ve cov-\nered the concept of logs, let’s discuss another fundamental concept in Kafka: partitions. \n2.3.4\nKafka and partitions\nPartitions are a critical part of Kafka’s design. They’re essential for performance, and\nthey guarantee that data with the same keys will be sent to the same consumer and in\norder. Figure 2.7 shows how partitions work.\nThe logs directory is conﬁgured in the root at /logs.\n/logs\n/logs/topicA_0               topicA has one partition.\n/logs/topicB_0               topicB has three partitions.\n/logs/topicB_1\n/logs/topicB_2\nFigure 2.6\nThe logs directory is the base \nstorage for messages. Each directory under \n/logs represents a topic partition. Filenames \nwithin the directory start with the name of the \ntopic, followed by an underscore, which is \nfollowed by a partition number.\nPartition 2\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPartition 1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPartition 0\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThe numbers shown in the\nrectangles are the offsets\nfor the messages.\nAs messages or records come in,\nthey are written to a partition\n(assigned by producer) and\nappended in time order to\nthe end of the log.\nTopic with partitions\nData comes into a single\ntopic but is placed into\nindividual partitions either\n(0, 1, or 2). Because\nthere are no keys with\nthese messages,\npartitions are assigned\nin round-robin fashion.\nEach partition is in strictly\nincreasing order, but there’s\nno order across partitions.\nFigure 2.7\nKafka uses partitions to achieve high throughput and spread the messages for an \nindividual topic across several machines in the cluster.\n \n",
      "content_length": 2166,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "29\nKafka architecture\nPartitioning a topic essentially splits the data forwarded to a topic across parallel\nstreams, and it’s key to how Kafka achieves its tremendous throughput. I explained\nthat a topic is a distributed log; each partition is similarly a log unto itself and follows\nthe same rules. Kafka appends each incoming message to the end of the log, and all\nmessages are strictly time-ordered. Each message has an offset number assigned to it.\nThe order of messages across partitions isn’t guaranteed, but the order of messages\nwithin each partition is guaranteed.\n Partitioning serves another purpose, aside from increasing throughput. It allows\ntopic messages to be spread across several machines so that the capacity of a given\ntopic isn’t limited to the available disk space on one server.\n Now let’s look at another critical role partitions play: ensuring messages with the\nsame keys end up together. \n2.3.5\nPartitions group data by key\nKafka works with data in key/value pairs. If the keys are null, the Kafka producer will\nwrite records to partitions chosen in a round-robin fashion. Figure 2.8 shows how par-\ntition assignment operates with non-null keys.\nIf the keys aren’t null, Kafka uses the following formula (shown in pseudocode) to\ndetermine which partition to send the key/value pair to:\nHashCode.(key) % number of partitions\nBy using a deterministic approach to select a partition, records with the same key will\nalways be sent to the same partition and in order. The default partitioner uses this\nPartition 1\nPartition 0\nhashCode(barBytes) % 2 = 1\nhashCode(fooBytes) % 2 = 0\nOnce the partition is determined, the message\nis appended to the appropriate log.\nIncoming messages:\n{foo, message data}\n{bar, message data}\nare used to determine\npartition the\nMessage keys\nwhich\nThese\nmessage should go to.\nkeys are not null.\nThe bytes of the key are used to calculate the hash.\nFigure 2.8\n“foo” is sent to partition 0, and \n“bar” is sent to partition 1. You obtain the \npartition by hashing the bytes of the key, \nmodulus the number of partitions.\n \n",
      "content_length": 2070,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "30\nCHAPTER 2\nKafka quickly\napproach; if you need a different strategy for selecting partitions, you can provide a\ncustom partitioner. \n2.3.6\nWriting a custom partitioner\nWhy would you want to write a custom partitioner? Of the several possible reasons,\nwe’ll look at one simple case here—the use of composite keys.\n Suppose you have purchase data flowing into Kafka, and the keys contain two val-\nues: a customer ID and a transaction date. But you need to group values by cus-\ntomer ID, so taking a hash of the customer ID and the purchase date won’t work. In\nthis case, you’ll need to write a custom partitioner that knows which part of the com-\nposite key determines which partition to use. For example, the composite key found\nin src/main/java/bbejeck/model/PurchaseKey.java (source code can be found on\nthe book’s website here: https://manning.com/books/kafka-streams-in-action) is\nshown in the following listing.\npublic class PurchaseKey {\nprivate String customerId;\nprivate Date transactionDate;\npublic PurchaseKey(String customerId, Date transactionDate) {\nthis.customerId = customerId;\nthis.transactionDate = transactionDate;\n}\npublic String getCustomerId() {\nreturn customerId;\n}\npublic Date getTransactionDate() {\nreturn transactionDate;\n}\n}\nWhen it comes to partitioning, you need to ensure that all transactions for a particular\ncustomer go to the same partition, but using the key in its entirety won’t enable this to\nhappen. Because purchases happen on many dates, including the date will result in\ndifferent key values for a single customer, placing the transactions across random par-\ntitions. You need to ensure you send all transactions with the same customer ID to the\nsame partition. The only way to do that is to only use the customer ID when determin-\ning the partition.\n The following example custom partitioner does what’s required. PurchaseKey-\nPartitioner (from src/main/java/bbejeck/chapter_2/partitioner/PurchaseKey-\nPartitioner.java) extracts the customer ID from the key to determine which partition\nto use.\nListing 2.1\nPurchaseKey composite key\n \n",
      "content_length": 2078,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "31\nKafka architecture\npublic class PurchaseKeyPartitioner extends DefaultPartitioner {\n@Override\npublic int partition(String topic, Object key,\nbyte[] keyBytes, Object value,\nbyte[] valueBytes, Cluster cluster) {\n        Object newKey = null;\nif (key != null) {                                  \nPurchaseKey purchaseKey = (PurchaseKey) key;\nnewKey = purchaseKey.getCustomerId();\nkeyBytes = ((String) newKey).getBytes();         \n}\nreturn super.partition(topic, newKey, keyBytes, value, \n   valueBytes, cluster);                                       \n}\n}\nThis custom partitioner extends DefaultPartitioner. You could implement the\nPartitioner interface directly, but there’s existing logic in DefaultPartitioner that\nwe’re using in this case.\n Keep in mind that when creating a custom partitioner, you aren’t limited to using\nonly the key. Using the value alone, or the value in combination with the key, is valid\nas well.\nNOTE\nThe Kafka API provides a Partitioner interface that you can use to\nwrite a custom partitioner. We won’t be covering writing a partitioner from\nscratch, but the principles are the same as those in the listing 2.2.\nYou’ve just seen how to construct a custom partitioner. Next, let’s wire up the parti-\ntioner with Kafka. \n2.3.7\nSpecifying a custom partitioner\nNow that you’ve written a custom partitioner, you need to tell Kafka you want to use it\ninstead of the default partitioner. Although we haven’t covered producers yet, you\nspecify a different partitioner when configuring the Kafka producer:\npartitioner.class=bbejeck_2.partitioner.PurchaseKeyPartitioner\nBy setting a partitioner per producer instance, you’re free to use any partitioner class\nfor any producer. We’ll go over producer configuration in detail when we cover using\nKafka producers.\nWARNING\nYou must exercise some caution when choosing the keys you use\nand when selecting parts of a key/value pair to partition on. Make sure the\nkey you choose has a fair distribution across all of your data. Otherwise, you’ll\nListing 2.2\nPurchaseKeyPartitioner custom partitioner\nIf the key isn’t \nnull, extracts \nthe customer ID\nSets the key \nbytes to the \nnew value\nReturns the partition with the\nupdated key, delegating to\nthe superclass\n \n",
      "content_length": 2225,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "32\nCHAPTER 2\nKafka quickly\nend up with a data-skew problem, because most of your data will be located\non just a few of your partitions. \n2.3.8\nDetermining the correct number of partitions\nChoosing the number of partitions to use when creating a topic is part art and part sci-\nence. One of the key considerations is the amount of data flowing into a given topic.\nMore data implies more partitions for higher throughput. But as with anything in life,\nthere are trade-offs.\n Increasing the number of partitions increases the number of TCP connections and\nopen file handles. Additionally, how long it takes to process an incoming record in a\nconsumer will also determine throughput. If you have heavyweight processing in your\nconsumer, adding more partitions may help, but ultimately the slower processing will\nhinder performance.2\n2.3.9\nThe distributed log\nWe’ve discussed the concepts of logs and partitioned topics. Let’s take a minute to\nlook at those two concepts together to demonstrate distributed logs.\n So far, we’ve focused on logs and topics on one Kafka server or broker, but typically\na Kafka production cluster environment includes several machines. I’ve intentionally\nkept the discussion centered on a single node, as it’s easier to understand the con-\ncepts when considering one node. But in practice, you’ll always be working with a clus-\nter of machines in Kafka.\n When a topic is partitioned, Kafka doesn’t allocate those partitions on one machine—\nKafka spreads them across several machines in the cluster. As Kafka appends records\nto a log, Kafka is distributing those records across several machines by partition. In fig-\nure 2.9, you can see this process in action.\n Let’s walk through a quick example using figure 2.9 as a guide. For this example,\nwe’ll assume one topic and null keys, so the producer assigns partitions in a round-\nrobin manner.\n The producer sends its first message to partition 0 on Kafka broker 1, the second\nmessage to partition 1 on Kafka broker 1, and the third message to partition 2 on\nKafka broker 2. When the producer sends its sixth message, it goes to partition 5 on\nKafka broker 3, and the next message starts over, going to partition 0 on Kafka bro-\nker 1. Message distribution continues in this manner, spreading message traffic across\nall nodes in the Kafka cluster.\n Although storing data remotely may sound risky, because a server can go down,\nKafka offers data redundancy. Data is replicated to one or more machines in the\ncluster as you write to one broker in Kafka (we’ll cover replication in an upcoming\nsection). \n2 Jun Rao, “How to Choose the Number of Topics/Partitions in a Kafka Cluster?” http://mng.bz/4C03.\n \n",
      "content_length": 2677,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "33\nKafka architecture\n2.3.10 ZooKeeper: leaders, followers, and replication\nSo far, we’ve discussed the role topics play in Kafka, and how and why topics are parti-\ntioned. You’ve seen that partitions aren’t all located on one machine but are spread\nout on brokers throughout the cluster. Now it’s time to look at how Kafka provides\ndata availability in the face of machine failures.\n Kafka has the notion of leader and follower brokers. In Kafka, for each topic parti-\ntion, one broker is chosen as the leader for the other brokers (the followers). One of\nthe chief duties of the leader is to assign replication of topic partitions to the follower\nbrokers. Just as Kafka allocates partitions for a topic across the cluster, Kafka also\nreplicates the partitions across machines. Before we go into the details of how leaders,\nfollowers, and replication work, we need to discuss the technology Kafka uses to\nachieve this.\n2.3.11 Apache ZooKeeper\nIf you’re a complete Kafka newbie, you may be asking yourself, “Why are we talking\nabout Apache ZooKeeper in a Kafka book?” Apache ZooKeeper is integral to Kafka’s\nThis topic has 6 partitions on a 3-node Kafka\ncluster. The log for the topic is spread across\nthe 3 nodes. Only the leader brokers for the\ntopic partitions are shown here.\nIf no keys are associated with the\nmessages, partitions are chosen\nin a round-robin fashion. Otherwise,\npartitions are determined by the hash\nof the key, modulus the number of partitions.\nPartition 1\nPartition 0\nPartition 2\nPartition 3\nPartition 4\nPartition 5\nKafka broker 1\nKafka broker 2\nKafka broker 3\nProducer\nFigure 2.9\nA producer writes messages to partitions of a topic. If no key is associated with the \nmessage, the producer chooses a partition in a round-robin fashion. Otherwise, the hash of the key, \nmodulus the number of partitions is used.\n \n",
      "content_length": 1837,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "34\nCHAPTER 2\nKafka quickly\narchitecture, and it’s ZooKeeper that enables Kafka to have leader brokers and to do\nsuch things as track the replication of topics (https://zookeeper.apache.org):\nZooKeeper is a centralized service for maintaining configuration information, naming,\nproviding distributed synchronization, and providing group services. All of these kinds of\nservices are used in some form or another by distributed applications.\nGiven that Kafka is a distributed application, it should start to be clear how ZooKeeper\nis involved in Kafka’s architecture. For this discussion, we’ll only consider Kafka instal-\nlations where there are two or more Kafka servers installed.\n In a Kafka cluster, one of the brokers is “elected” as the controller. We covered parti-\ntions in the previous section and discussed how Kafka spreads partitions across differ-\nent machines in the cluster. Topic partitions have a leader and follower(s) (the level\nof replication determines the degree of replication). When producing messages,\nKafka sends the record to the broker that is the leader for the record’s partition. \n2.3.12 Electing a controller\nKafka uses ZooKeeper to elect the controller broker. Discussing the consensus algo-\nrithms involved is way beyond the scope of this book, so we’ll take the 50,000-foot view\nand just state that ZooKeeper elects a broker from the cluster to be the controller.\n If the controlling broker fails or becomes unavailable for any reason, ZooKeeper\nelects a new controller from a set of brokers that are considered to be caught up with\nthe leader (an in-sync replica [ISR]). The brokers that make up this set are dynamic,\nand ZooKeeper recognizes only brokers in this set for election as leader.3\n2.3.13 Replication\nKafka replicates records among brokers to ensure data availability, should a broker in\nthe cluster fail. You can set the level of replication for each topic (as you saw in our\nprevious example of publishing and consuming) or for all topics in the cluster. Fig-\nure 2.10 demonstrates the replication flow between brokers.\n The Kafka replication process is straightforward. Brokers that follow a topic parti-\ntion consume messages from the topic-partition leader and append those records to\ntheir log. As discussed in the previous section, follower brokers that are caught up\nwith their leader broker are considered to be ISRs. ISR brokers are eligible to be\nelected leader, should the current leader fail or become unavailable.4 \n \n \n \n3 Kafka documentation, “Replicated Logs: Quorums, ISRs, and State Machines (Oh my!),” http://kafka.apache\n.org/documentation/#design_replicatedlog.\n4 Kafka documentation, “Replication,” http://kafka.apache.org/documentation/#replication.\n \n",
      "content_length": 2721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "35\nKafka architecture\n2.3.14 Controller responsibilities\nThe controller broker is responsible for setting up leader/follower relationships for\nall partitions of a topic. If a Kafka node dies or is unresponsive (to ZooKeeper heart-\nbeats), all of its assigned partitions (both leader and follower) are reassigned by the\ncontroller broker. Figure 2.11 illustrates a controller broker in action.5\n The figure shows a simple failure scenario. In step 1, the controller broker detects\nthat broker 3 isn’t available. In step 2, the controller broker reassigns the leadership\nof the partition on broker 3 to broker 2.\n \n \n \n \n \n \n5 Some of the information in this section came from answers given by Gwen Shapira, “What is the actual role\nof Zookeeper in Kafka? What benefits will I miss out on if I don’t use Zookeeper and Kafka together?” on\nQuora at http://mng.bz/25Sy.\nThe topic foo has 2 partitions and a replication\nlevel of 3. Dashed lines between partitions point\nto the leader of the given partition. Producers\nwrite records to the leader of a partition, and\nthe followers read from the leader.\nBroker 1 is the leader for partition 0 and\nis a follower for partition 1 on broker 3.\nfoo topic partition 1\nKafka broker 1\nfoo topic partition 0\nfoo topic partition 0\nKafka broker 2\nfoo topic partition 1\nfoo topic partition 1\nKafka broker 3\nfoo topic partition 0\nBroker 2 is a follower for partition 0 on broker\n1 and a follower for partition 1 on broker 3.\nBroker 3 is a follower for partition 0 on\nbroker 1 and the leader for partition 1.\nFigure 2.10\nBrokers 1 and 3 are leaders for one topic partition and followers for another, whereas \nbroker 2 is a follower only. Follower brokers copy data from the leader broker.\n \n",
      "content_length": 1719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "36\nCHAPTER 2\nKafka quickly\nZooKeeper is also involved in the following aspects of Kafka operations:\nCluster membership—Joining a cluster and maintaining membership in a cluster.\nIf a broker becomes unavailable, ZooKeeper removes the broker from cluster\nmembership.\nTopic configuration—Keeping track of the topics in a cluster, which broker is the\nleader for a topic, how many partitions there are for a topic, and any specific\nconfiguration overrides for a topic.\nAccess control—Identifying who can read from and write to particular topics.\nYou’ve now seen why Kafka has a dependency on Apache ZooKeeper. It’s ZooKeeper\nthat enables Kafka to have a leader broker with followers. The head broker has the\ncritical role of assigning topic partitions for replication to the followers, as well as reas-\nsigning them when a member broker fails. \nThe topic foo has 2 partitions and a replication\nlevel of 3. These are the initial leaders and\nfollowers:\nBroker 1 leader partition 0, follower partition 1\nBroker 2 follower partition 0, follower partition 1\nBroker 3 follower partition 0, leader partition 1\nBroker 3 has become unresponsive.\nStep 2: The controller has reassigned the\nleadership of partition\nfrom broker 3 to\n1\nbroker 2. All records for partition\nwill go\n1\nto broker 2, and broker\nwill now consume\n1\nmessages for partition\nfrom broker 2.\n1\nStep\n: As the leader, broker\n1\n1\nhas detected that broker 3\nhas failed.\nfoo topic partition 1\nKafka broker 1\nfoo topic partition 0\nfoo topic partition 0\nKafka broker 2\nfoo topic partition 1\nfoo topic partition 1\nKafka broker 3\nfoo topic partition 0\nFigure 2.11\nThe controller broker is responsible for assigning other brokers to be the leader broker \nfor some topics/partitions and followers for other topics/partitions. When a broker becomes \nunavailable, the controller broker will reassign the failed broker’s assignments to other brokers \nin the cluster.\n \n",
      "content_length": 1910,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "37\nKafka architecture\n2.3.15 Log management\nWe’ve covered appending messages, but we haven’t talked about how logs are man-\naged as they continue to grow. The amount of space on spinning disks in a cluster is a\nfinite resource, so it’s important for Kafka to remove messages over time. When it\ncomes to removing old data in Kafka, there are two approaches: the traditional log-\ndeletion approach, and compaction. \n2.3.16 Deleting logs\nThe log-deletion strategy is a two-phased approach: first, the logs are rolled into seg-\nments, and then the oldest segments are deleted. To manage the increasing size of the\nlogs, Kafka rolls them into segments. The timing of log rolling is based on time-\nstamps embedded in the messages. Kafka rolls a log when a new message arrives,\nand its timestamp is greater than the timestamp of the first message in the log plus\nthe log.roll.ms configuration value. At that point, the log is rolled and a new seg-\nment is created as the new active log. The previous active segment is still used to\nretrieve messages for consumers.\n Log rolling is a configuration setting you can specify when setting up a Kafka bro-\nker.6 There are two options for log rolling:\n\nlog.roll.ms—This is the primary configuration, but there’s no default value.\n\nlog.roll.hours—This is the secondary configuration, which is only used if\nlog.role.ms isn’t set. It defaults to 168 hours.\nOver time, the number of segments will continue to grow, and older segments will need\nto be deleted to make room for incoming data. To handle the deletion, you can specify\nhow long to retain the segments. Figure 2.12 illustrates the process of log rolling.\n Like log rolling, the removal of segments is based on timestamps in the messages\nand not just the clock time or time when the file was last modified. Log-segment\ndeletion is based on the largest timestamp in the log. Here are three settings, listed\nin order of priority, meaning that configurations earlier in the list override the later\nentries:\n\nlog.retention.ms—How long to keep a log file in milliseconds\n\nlog.retention.minutes—How long to keep a log file in minutes\n\nlog.retention.hours—Log file retention in hours\nI present these settings based on the assumption of high-volume topics, where you’re\nguaranteed to reach the maximum file size in a given time period. Another configura-\ntion setting, log.retention.bytes, could be specified with a longer rolling-time\nthreshold to keep down I/O operations. Finally, to guard against the case of a signifi-\ncant spike in volume when there are relatively large roll settings, the log.segment\n.bytes setting governs how large an individual log segment can be.\n6 Kafka documentation, “Broker Configs,” http://kafka.apache.org/documentation/#brokerconfigs.\n \n",
      "content_length": 2760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "38\nCHAPTER 2\nKafka quickly\nThe deletion of logs works well for non-keyed records, or records that stand alone.\nBut if you have keyed data and expected updates, there’s another method that will suit\nyour needs better. \n2.3.17 Compacting logs\nConsider the case where you have keyed data, and you’re receiving updates for that\ndata over time, meaning a new record with the same key will update the previous\nvalue. For example, a stock ticker symbol could be the key, and the price per share\nwould be the regularly updated value. Imagine you’re using that information to dis-\nplay stock values, and you have a crash or restart—you need to be able to start back up\nwith the latest data for each key.7\n If you use the deletion policy, a segment could be removed between the last update\nand the application’s crash or restart. You wouldn’t have all the records on startup. It\nwould be better to retain the last known value for a given key, treating the next record\nwith the same key as you would an update to a database table.\n Updating records by key is the behavior that compacted topics (logs) deliver.\nInstead of taking a coarse-grained approach and deleting entire segments based on\ntime or size, compaction is more fine-grained and deletes old records per key in a log.\nAt a very high level, a log cleaner (a pool of threads) runs in the background, recopying\n7 Kafka documentation, “Log Compaction,” http://kafka.apache.org/documentation/#compaction.\nThis segment log has\nbeen deleted.\nOlder log segment ﬁles that\nhave been rolled. The bottom\nsegment is still in use.\nThe records are appended\nto this current log.\nFigure 2.12\nOn the left are the \ncurrent log segments. On the upper \nright is a deleted log segment, and \nthe one below it is a recently rolled \nsegment still in use.\n \n",
      "content_length": 1783,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "39\nKafka architecture\nlog-segment files and removing records if there’s an occurrence later in the log with\nthe same key. Figure 2.13 illustrates how log compaction retains the most recent mes-\nsage for each key.\nThis approach guarantees that the last record for a given key is in the log. You can\nspecify log retention per topic, so it’s entirely possible to have some topics using time-\nbased retention and other topics using compaction.\n By default, the log cleaner is enabled. To use compaction for a topic, you’ll need to\nset the log.cleanup.policy=compact property when creating the topic.\n Compaction is used in Kafka Streams when using state stores, but you won’t be cre-\nating those logs/topics yourself—the framework handles that task. Nevertheless, it’s\nimportant to understand how compaction works. Log compaction is a broad subject,\nand we’ve only touched on it here. For more information, see the Kafka documenta-\ntion: http://kafka.apache.org/documentation/#compaction.\nNOTE\nWith a cleanup.policy of compact, you might wonder how you can\nremove a record from the log. With a compacted topic, deletion provides a\nnull value for the given key, setting a tombstone marker. Any key with a null\nvalue ensures that any prior record with the same key is removed, and the\ntombstone marker itself is removed after a period of time.\nThe key takeaway from this section is that if you have independent, standalone events\nor messages, use log deletion. If you have updates to events or messages, you’ll want to\nuse log compaction.\n We’ve spent a good deal of time covering how Kafka handles data internally. Now\nit’s time to move outside of Kafka and discuss how we can send messages to Kafka with\nproducers and read messages from Kafka with consumers. \nBefore compaction\nAfter compaction\nOffset\nValue\nKey\nOffset\nValue\nKey\n10\nA\nfoo\n11\nB\nbar\n12\nC\nbaz\n13\nD\nfoo\n13\nD\nfoo\n14\nE\nbaz\n15\nF\nboo\n16\nG\nfoo\n17\nH\nbaz\n11\nB\nbar\n15\nF\nboo\n16\nG\nfoo\n17\nH\nbaz\nFigure 2.13\nOn the left is a log before compaction—you’ll notice duplicate keys with different \nvalues that are updates for the given key. On the right is the log after compaction—the latest value \nfor each key is retained, but the log is smaller in size.\n \n",
      "content_length": 2200,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "40\nCHAPTER 2\nKafka quickly\n2.4\nSending messages with producers\nGoing back to ZMart’s need for a centralized sales transaction data hub, let’s look at\nhow you’ll send purchase transactions into Kafka. In Kafka, the producer is the client\nused for sending messages. Figure 2.14 revisits ZMart’s data architecture with the pro-\nducers highlighted, to emphasize where they fit into the data flow.\nAlthough ZMart has a lot of sales transactions, we’re going to consider the purchase of\na single item for now: a book costing $10.99. When the customer completes the sales\ntransaction, the information is converted into a key/value pair and sent to Kafka via a\nproducer.\n The key is the customer ID, 123447777, and the value is in JSON format:\n\"{\\\"item\\\":\\\"book\\\",\\\"price\\\":10.99}\". (I’ve escaped the double quotes so the\nJSON can be represented as a string literal in Java.) With the data in this format, you\ncan use a producer to send the data to the Kafka cluster. The following example can\nbe found in src/main/java/bbejeck.chapter_2/producer/SimpleProducer.java.\n \n \nSales\nAcquired company A\nConsumer\nMarketing\nAcquired company C\nAcquired company B\nCustomer info\nAuditing\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nKafka\nZooKeeper\nIn this simpliﬁed view of Kafka,\nwe’re assuming a cluster is installed.\nAll output is sent from a producer, and\ninput is consumed by a consumer.\nA cluster of ZooKeeper nodes communicates\nwith Kafka to maintain topic info and keep\ntrack of brokers in the cluster.\nFigure 2.14\nProducers are used to send messages to Kafka. Producers don’t know which \nconsumer will read the messages or when.\n \n",
      "content_length": 1694,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "41\nSending messages with producers\nProperties properties = new Properties();\nproperties.put(\"bootstrap.servers\", \"localhost:9092\");\nproperties.put(\"key.serializer\", \"org.apache.kafka.common.serialization.Strin\ngSerializer\");\nproperties.put(\"value.serializer\",\n➥ \"org.apache.kafka.common.serialization.StringSerializer\");\nproperties.put(\"acks\", \"1\");\nproperties.put(\"retries\", \"3\");\nproperties.put(\"compression.type\", \"snappy\");\nproperties.put(\"partitioner.class\",\n➥ PurchaseKeyPartitioner.class.getName());         \nPurchaseKey key = new PurchaseKey(\"12334568\", new Date());\ntry(Producer<PurchaseKey, String> producer =  \n➥ new KafkaProducer<>(properties)) {                                                \nProducerRecord<PurchaseKey, String>\nrecord =  \n➥ new ProducerRecord<>(\"transactions\", key, \"{\\\"item\\\":\\\"book\\\",\n\\\"price\\\":10.99}\");\nCallback callback = (metadata, exception) -> {\nif (exception != null) {\nSystem.out.println(\"Encountered exception \"   \n➥ + exception);                                              \n}\n};\nFuture<RecordMetadata> sendFuture =  \n➥ producer.send(record, callback);    \n}\nKafka producers are thread-safe. All sends to Kafka are asynchronous—Producer\n.send returns immediately once the producer places the record in an internal buffer.\nThe buffer sends records in batches. Depending on your configuration, there could\nbe some blocking if you attempt to send a message while a producer’s buffer is full.\n The Producer.send method depicted here takes a Callback instance. Once the\nleader broker acknowledges the record, the producer fires the Callback.onComplete\nmethod. Only one of the arguments will be non-null in the Callback.onComplete\nmethod. In this case, you’re only concerned with printing out the stacktrace in the\nevent of error, so you check if the exception object is non-null. The returned Future\nyields a RecordMetadata object once the server acknowledges the record.\nDEFINITION\nIn listing 2.3, the Producer.send method returns a Future object.\nA Future object represents the result of an asynchronous operation. More\nimportant, a Future gives you the option to lazily retrieve asynchronous results\ninstead of waiting for their completion. For more information on futures, see\nthe Java documentation for “Interface Future<V>”: http:// mng.bz/0JK2.\nListing 2.3\nSimpleProducer example\nProperties for \nconfiguring a \nproducer\nCreates the \nKafkaProducer\nInstantiates\nthe Producer-\nRecord\nBuilds a \ncallback\nSends the record and sets the \nreturned Future to a variable\n \n",
      "content_length": 2510,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "42\nCHAPTER 2\nKafka quickly\n2.4.1\nProducer properties\nWhen you created the KafkaProducer instance, you passed a java.util.Properties\nparameter containing the configuration for the producer. The configuration of a\nKafkaProducer isn’t complicated, but there are key properties to consider when set-\nting it up. These settings are where you’d specify a custom partitioner, for example.\nThere are too many properties to cover here, so we’ll just look at the ones used in list-\ning 2.3:\nBootstrap servers—bootstrap.servers is a comma-separated list of host:port\nvalues. Eventually the producer will use all the brokers in the cluster; this list is\nused for initially connecting to the cluster.\nSerialization—key.serializer and value.serializer instruct Kafka how to\nconvert the keys and values into byte arrays. Internally, Kafka uses byte arrays\nfor keys and values, so you need to provide Kafka with the correct serializers to\nconvert objects to byte arrays before them sending across the wire.\n\nacks—acks specifies the minimum number of acknowledgments from a bro-\nker that the producer will wait for before considering a record send completed.\nValid values for acks are all, 0, and 1. With a value of all, the producer will\nwait for a broker to receive confirmation that all followers have committed\nthe record. When set to 1, the broker writes the record to its log but doesn’t\nwait for any followers to acknowledge committing the record. A value of 0\nmeans the producer won’t wait for any acknowledgments—this is mostly\nfire-and-forget.\nRetries—If sending a batch results in a failure, retries specifies the number of\ntimes to attempt to resend. If record order is important, you should consider\nsetting max.in.flight.requests.per.connection to 1 to prevent the scenario\nof a second batch being sent successfully before a failed record being sent as\nthe result a retry.\nCompression type—compression.type specifies what compression algorithm to\napply, if any. If set, compression.type instructs the producer to compress a\nbatch before sending. Note that it’s the entire batch that’s compressed, not\nindividual records.\nPartitioner class—partitioner.class specifies the name of the class implement-\ning the Partitioner interface. The partitioner.class is related to our earlier\ndiscussion of custom partitioners discussion in section 2.3.7.\nFor more information about producer configuration, see the Kafka documentation:\nhttp://kafka.apache.org/documentation/#producerconfigs. \n2.4.2\nSpecifying partitions and timestamps\nWhen you create a ProducerRecord, you have the option of specifying a partition, a\ntimestamp, or both. When you instantiated the ProducerRecord in listing 2.3, you\n \n",
      "content_length": 2691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "43\nSending messages with producers\nused one of four overloaded constructors. Other constructors allow for setting a parti-\ntion and timestamp, or just a partition:\nProducerRecord(String topic, Integer partition, String key, String value)\nProducerRecord(String topic, Integer partition,\nLong timestamp, String key,\nString value)\n2.4.3\nSpecifying a partition\nIn section 2.3.5, we discussed the importance of partitions in Kafka. We also discussed\nhow the DefaultPartitioner works and how you can supply a custom partitioner.\nWhy would you explicitly set the partition? There are a variety of business reasons why\nyou might do so. Here’s one example.\n Suppose you have keyed data coming in, but it doesn’t matter which partition the\nrecords go to, because the consumers have logic to handle any value that the key\nmight contain. Additionally, the distribution of the keys might not be even, and you\nwant to ensure that all partitions receive roughly the same amount of data. Here’s a\nrough implementation that would do this.\nAtomicInteger partitionIndex = new AtomicInteger(0); \nint currentPartition = Math.abs(partitionIndex.getAndIncrement()) % \n➥ numberPartitions;                                                \nProducerRecord<String, String> record =\n➥ new ProducerRecord<>(\"topic\", currentPartition, \"key\", \"value\");\nHere, you use the Math.abs call, so you don’t have to keep track of the value of the\ninteger if it goes beyond Integer.MAX_VALUE.\nDEFINITION\nAtomicInteger belongs to the java.util.concurrent.atomic pack-\nage, which contains classes that support lock-free, thread-safe operations on\nsingle variables. For more information, see the Java documentation for the\njava.util.concurrent.atomic package: http://mng.bz/PQ2q. \n2.4.4\nTimestamps in Kafka\nKafka version 0.10 added timestamps to records. You set the timestamp when you cre-\nate a ProducerRecord via this overloaded constructor call:\nProducerRecord(String topic, Integer partition,\n➥ Long timestamp, K key, V value)\nIf you don’t set a timestamp, the producer will (using the current clock time) before\nsending the record to the Kafka broker. Timestamps are also affected by the\nListing 2.4\nManually setting the partition\nCreates an AtomicInteger \ninstance variable\nGets the current partition and\nuses it as a parameter\n \n",
      "content_length": 2291,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "44\nCHAPTER 2\nKafka quickly\nlog.message.timestamp.type broker configuration setting, which can be set to either\nCreateTime (the default) or LogAppendTime. Like many other broker settings, the value\nconfigured on the broker applies to all topics as a default value, but when you create a\ntopic, you can specify a different value for that topic. If you specify LogAppendTime\nand the topic doesn’t override the broker’s configuration, the broker will overwrite\nthe timestamp with the current time when it appends the record to the log. Other-\nwise, the timestamp from ProducerRecord is used.\n Why would you choose one setting over another? LogAppendTime is considered to\nbe “processing time,” and CreateTime is considered to be “event time.” Which you\nshould use depends on your business requirements. You’ll need to decide whether\nyou need to know when Kafka processed the record, or when the actual event\noccurred. In later chapters, you’ll see the important role timestamps have in con-\ntrolling data flow in Kafka Streams. \n2.5\nReading messages with consumers\nYou’ve seen how producers work; now it’s time to look at consumers in Kafka. Suppose\nyou’re building a prototype application to show the latest ZMart sales statistics. For\nthis example, you’ll consume the message you sent in the previous producer example.\nBecause this prototype is in its earliest stages, all you’ll do at this point is consume the\nmessage and print the information to the console.\nNOTE\nBecause the version of Kafka Streams covered in the book requires\nKafka version 0.10.2 or higher, we’ll only discuss the new consumer that was\npart of the Kafka 0.9 release.\nKafkaConsumer is the client you’ll use to consume messages from Kafka. The Kafka-\nConsumer class is straightforward to use, but there are a few operational consider-\nations to take into account. Figure 2.15 shows the ZMart architecture, highlighting\nwhere consumers play a role in the data flow.\n2.5.1\nManaging offsets\nKafkaProducer is essentially stateless, but KafkaConsumer manages some state by peri-\nodically committing the offsets of messages consumed from Kafka. Offsets uniquely\nidentify messages and represent the starting positions of messages in the log. Consum-\ners periodically need to commit the offsets of messages they have received.\n Committing an offset has two implications for a consumer:\nCommitting implies the consumer has fully processed the message.\nCommitting also represents the starting point for that consumer in the case of\nfailure or a restart.\n \n",
      "content_length": 2517,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "45\nReading messages with consumers\nIf you have a new consumer instance or some failure has occurred, and the last com-\nmitted offset isn’t available, where the consumer starts from will depend on your con-\nfiguration:\n\nauto.offset.reset=\"earliest\"—Messages will be retrieved starting at the\nearliest available offset. Any messages that haven’t yet been removed by the log-\nmanagement process will be retrieved.\n\nauto.offset.reset=\"latest\"—Messages will be retrieved from the latest off-\nset, essentially only consuming messages from the point of joining the cluster.\n\nauto.offset.reset=\"none\"—No reset strategy is specified. The broker throws\nan exception to the consumer.\nIn figure 2.16, you can see the impact of choosing an auto.offset.reset setting. By\nselecting earliest, you receive messages starting at offset 1. If you choose latest,\nyou’ll get a message starting at offset 11.\n Next, we need to discuss the options for committing offsets. You can do this either\nautomatically or manually.\nSales\nIn this simpliﬁed view of Kafka,\nwe’re assuming a cluster is installed.\nAll output is sent from a producer, and\ninput is consumed by a consumer.\nAcquired company A\nConsumer\nMarketing\nAcquired company C\nAcquired company B\nCustomer info\nAuditing\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nConsumer\nProducer\nKafka\nZooKeeper\nA cluster of ZooKeeper nodes communicates\nwith Kafka to maintain topic info and keep\ntrack of brokers in the cluster.\nFigure 2.15\nThese are the consumers that read messages from Kafka. Just as producers have \nno knowledge of the consumers, consumers read messages from Kafka with no knowledge of who \nproduced the messages.\n \n",
      "content_length": 1710,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "46\nCHAPTER 2\nKafka quickly\n2.5.2\nAutomatic offset commits\nAutomatic offset commits are enabled by default, and they’re represented by the\nenable.auto.commit property. There’s a companion configuration option, auto\n.commit.interval.ms, which specifies how often the consumer will commit offsets\n(the default value is 5 seconds). You should take care when adjusting this value. If it’s\ntoo small, it will increase network traffic; if it’s too large, it could result in the con-\nsumer receiving large amounts of repeated data in the event of a failure or restart. \n2.5.3\nManual offset commits\nThere are two types of manually committed offsets—synchronous and asynchronous.\nThese are the synchronous commits:\nconsumer.commitSync()\nconsumer.commitSync(Map<TopicPartition, OffsetAndMetadata>)\nThe no-arg commitSync() method blocks until all offsets returned from the last retrieval\n(poll) succeed. This call applies to all subscribed topics and partitions. The other ver-\nsion takes a Map<TopicPartition, OffsetAndMetadata> parameter, and it commits\nonly the offsets, partitions, and topics specified in the map.\n There are analogous consumer.commitAsync() methods that are completely asyn-\nchronous and return immediately. One of the overloaded methods accepts no argu-\nments, and two of the consumer.commitAsync methods have an option to provide an\nOffsetCommitCallback object, which is called when the commit has concluded either\nsuccessfully or with an error. Providing a callback instance allows for asynchronous\nprocessing and error handling. The advantage of using manual commits is that it gives\nyou direct control over when a record is considered processed. \nA conﬁg setting of “earliest”\nmeans messages starting\nfrom offset 0 will be sent to\nthe consumer.\nA conﬁg setting of “latest”\nmeans the consumer will\nget the next message when\nit is appended to the log.\nTen messages have been sent to a topic.\nA new consumer starts up, so it doesn’t\nhave a last offset committed.\n0\n9\n8\n7\n6\n5\n4\n3\n2\n1\nFigure 2.16\nA graphical representation of setting auto.offset.reset to earliest versus \nlatest. A setting of earliest will give you all messages not yet deleted; latest means you’ll \nwait for the next available message to arrive.\n \n",
      "content_length": 2227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "47\nReading messages with consumers\n2.5.4\nCreating the consumer\nCreating a consumer is similar to creating a producer. You supply a configuration in\nthe form of a Java java.util.Properties object, and you get back a KafkaConsumer\ninstance. This instance then subscribes to topics from a supplied list of topic names or\nby specifying a regular expression. Typically, you’ll run the consumer in a loop, where\nyou poll for a period specified in milliseconds.\n A ConsumerRecords<K, V> object is the result of the polling. ConsumerRecords\nimplements the Iterable interface, and each call to next() returns a Consumer-\nRecord object containing metadata about the message, in addition to the actual key\nand value.\n After you’ve exhausted all of the ConsumerRecord objects returned from the last call\nto poll, you return to the top of the loop, polling again for the specified period. In\npractice, consumers are expected to run indefinitely in this manner, unless an error\noccurs or the application needs to be shut down and restarted (this is where committed\noffsets come into play—on reboot, the consumer will pick up where it left off). \n2.5.5\nConsumers and partitions\nYou’ll generally want multiple consumer instances—one for each partition of a topic.\nIt’s possible to have one consumer read from multiple partitions, but it’s not uncom-\nmon to have a thread pool with as many threads as there are partitions, and with each\nthread running a consumer that’s assigned to one partition.\n This consumer-per-partition pattern maximizes throughput, but if you spread your\nconsumers across multiple applications or machines, the total thread count across all\ninstances shouldn’t exceed the total number of partitions in the topic. Any threads in\nexcess of the total partition count will be idle. If a consumer fails, the leader broker\nassigns its partitions to another active consumer.\nNOTE\nThis example shows a consumer subscribing to one topic, but this is for\ndemonstration purposes only. You can subscribe a consumer to an arbitrary\nnumber of topics.\nThe leader broker assigns topic partitions to all available consumers with the same\ngroup.id. The group.id is a configuration setting that identifies the consumer as\nbelonging to a consumer group—that way, consumers don’t need to reside on the same\nmachine. In fact, it’s probably preferable to have your consumers spread out across a\nfew machines. That way, in the case of one machine failing, the leader broker can\nassign topic partitions to consumers on good machines. \n2.5.6\nRebalancing\nThe process of adding and removing topic-partition assignments to consumers\ndescribed in the previous section is called rebalancing. Topic-partition assignments to a\nconsumer aren’t static—they’re dynamic. As you add consumers with the same group\nID, some of the current topic-partition assignments are taken from active consumers\n \n",
      "content_length": 2868,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "48\nCHAPTER 2\nKafka quickly\nand given to the new consumers. This reassignment process continues until every par-\ntition has been assigned to a consumer that’s reading data.\n After that equilibrium point, any additional consumers will remain idle. When\nconsumers leave the group for whatever reason, their topic-partition assignments are\nreassigned to other consumers. \n2.5.7\nFiner-grained consumer assignment\nIn section 2.5.5, I described the use of a thread pool and subscribing multiple con-\nsumers (in the same consumer group) to the same topics. Although Kafka will balance\nthe load of topic-partitions across all consumers, the assignment of the topic and par-\ntition isn’t deterministic. You won’t know what topic-partition pairings each consumer\nwill receive.\n KafkaConsumer has methods that allow you to subscribe to a particular topic and\npartition:\nTopicPartition fooTopicPartition_0 = new TopicPartition(\"foo\", 0);\nTopicPartition barTopicPartition_0 = new TopicPartition(\"bar\", 0);\nconsumer.assign(Arrays.asList(fooTopicPartition_0, barTopicPartition_0));\nThere are trade-offs to consider when using manual topic-partition assignment:\nFailures won’t result in topic partitions being reassigned, even for consumers\nwith the same group ID. Any changes in assignments will require another call to\nconsumer.assign.\nThe group specified by the consumer is used for committing, but because each\nconsumer will be acting independently, it’s a good idea to give each consumer a\nunique group ID. \n2.5.8\nConsumer example\nHere’s the consumer code for the ZMart prototype that consumes transactions and\nprints them to the console. You can find it in src/main/java/bbejeck.chapter_2/con-\nsumer/ThreadedConsumerExample.java.\npublic void startConsuming() {\nexecutorService = Executors.newFixedThreadPool(numberPartitions);\nProperties properties = getConsumerProps();\nfor (int i = 0; i < numberPartitions; i++) {\nRunnable consumerThread = getConsumerThread(properties);\nexecutorService.submit(consumerThread);\n}\n}\nprivate Runnable getConsumerThread(Properties properties) {\nreturn () -> {\nListing 2.5\nThreadedConsumerExample example\nBuilds a\nconsumer\nthread\n \n",
      "content_length": 2154,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "49\nInstalling and running Kafka\nConsumer<String, String> consumer = null;\ntry {\nconsumer = new KafkaConsumer<>(properties);\nconsumer.subscribe(Collections.singletonList(  \n➥ \"test-topic\"));                                                                \nwhile (!doneConsuming) {\nConsumerRecords<String, String> records =  \n➥ consumer.poll(5000);                                                         \nfor (ConsumerRecord<String, String> record : records) {\nString message = String.format(\"Consumed: key =\n➥ %s value = %s with offset = %d partition = %d\",\nrecord.key(), record.value(),\nrecord.offset(), record.partition());\nSystem.out.println(message);                           \n}\n}\n} catch (Exception e) {\ne.printStackTrace();\n} finally {\nif (consumer != null) {\nconsumer.close();  \n}\n}\n};\n}\nThis example leaves out other sections of the class for clarity—it won’t stand on its\nown. You can find the full example in this chapter’s source code. \n2.6\nInstalling and running Kafka\nAs I write this, Kafka 1.0.0 is the most recent version. Because Kafka is a Scala project,\neach release comes in two versions: one for Scala 2.11 and another for Scala 2.12. I use\nthe 2.12 Scala version of Kafka in this book. Although you can download the release,\nthe book’s source code includes a binary distribution of Kafka that will work with\nKafka Streams as demonstrated and described in this book. To install Kafka, extract\nthe .tgz file found in the book’s source code repo (source code can be found on the\nbook’s website here: https://manning.com/books/kafka-streams-in-action), to some-\nwhere in the libs folder on your machine.\nNOTE\nThe binary distribution of Kafka includes Apache ZooKeeper, so no\nextra installation work is required.\n2.6.1\nKafka local configuration\nRunning Kafka locally on your machine requires minimal configuration if you accept\nthe default values. By default, Kafka uses port 9092, and ZooKeeper uses port 2181.\nAssuming you have no applications already using those ports, you’re all set.\n Kafka writes its logs to /tmp/kafka-logs, and ZooKeeper uses /tmp/zookeeper\nfor its log storage. Depending on your machine, you may need to change permission\nSubscribes \nto the topic\nPolls for 5 \nseconds\nPrints a \nformatted \nmessage\nCloses the consumer—\nwill leak resources \notherwise\n \n",
      "content_length": 2293,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "50\nCHAPTER 2\nKafka quickly\nor ownership of those directories or to modify the location where you want to write\nthe logs.\n To change the Kafka logs directory, cd into <kafka-install-dir>/config and open the\nserver.properties file. Find the log.dirs entry, and change the value to what you’d\nrather use. In the same directory, open the zookeeper.properties file and change the\ndataDir entry.\n We’ll look at configuring Kafka in detail later in this book, but that’s all the configu-\nration you need to do for now. Keep in mind that these “logs” are the actual data used\nby Kafka and ZooKeeper, and not application-level logs that track the application’s\nbehavior. The application logs are found in the <kafka-install-dir>/logs directory. \n2.6.2\nRunning Kafka\nKafka is simple to get started. Because ZooKeeper is essential for the Kafka cluster to\nfunction properly (ZooKeeper determines the leader broker, holds topic information,\nperforms health checks on cluster members, and so on), you’ll need to start Zoo-\nKeeper before starting Kafka.\nNOTE\nFrom now on, all directory references assume you’re working in your\nKafka installation directory. If you’re using a Windows machine, the directory\nis <kafka-install-dir>/bin/windows.\nSTARTING ZOOKEEPER\nTo start ZooKeeper, open a command prompt and enter the following command:\nbin/zookeeper-server-start.sh\nconfig/zookeeper.properties\nYou’ll see a lot of information run by on the screen, and it should end up looking\nsomething like figure 2.17. \nSTARTING KAFKA\nTo start Kafka, open another command prompt and type this command:\nbin/Kafka-server-start.sh\nconfig/server.properties\nAgain, you’ll see text scroll by on the screen. When Kafka has fully started, you should\nsee something similar to figure 2.18.\nTIP\nZooKeeper is essential for Kafka to run, so it’s important to reverse the\norder when shutting down: stop Kafka first, and then stop ZooKeeper. To\nstop Kafka, you can press Ctrl-C from the terminal Kafka is running in, or run\nkafka-server-stop.sh from another terminal. The same goes for Zoo-\nKeeper, except the shutdown script is zookeeper-server-stop.sh. \n \n",
      "content_length": 2115,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "51\nInstalling and running Kafka\nFigure 2.17\nOutput visible on the console when ZooKeeper starts up\nFigure 2.18\nOutput from Kafka when starting up\n \n",
      "content_length": 148,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "52\nCHAPTER 2\nKafka quickly\n2.6.3\nSending your first message\nNow that you have Kafka up and running, it’s time to use Kafka for what it’s meant to\ndo: sending and receiving messages. But before you send a message, you’ll need to\ndefine a topic for a producer to send a message to.\nYOUR FIRST TOPIC\nCreating a topic in Kafka is simple. It’s just a matter of running a script with some con-\nfiguration parameters. The configuration is easy, but the settings you provide have\nbroad performance implications.\n By default, Kafka is configured to autocreate topics, meaning that if you attempt to\nsend to or read from a nonexistent topic, the Kafka broker will create one for you\n(using default configurations in the server.properties file). It’s rarely a good practice\nto rely on the broker to create topics, even in development, because the first pro-\nduce/consume attempt will fail, as it takes time for the metadata about the topic’s\nexistence to propagate. Be sure to always proactively to create your topics.\nCREATING A TOPIC\nTo create a topic, you need to run the kafka-topics.sh script. Open a terminal window\nand run this command:\nbin/kafka-topics.sh --create --topic first-topic --replication-factor 1\n➥ --partitions 1 --zookeeper localhost:2181\nWhen the script executes, you should see something similar to figure 2.19 in your\nterminal.\nMost of the configuration flags in the previous command are obvious, but let’s quickly\nreview two of them:\n\nreplication-factor—This flag determines how many copies of the message\nthe leader broker distributes in the cluster. In this case, with a replication factor\nof 1, no copies will be made. Just the original message will reside in Kafka. A\nreplication factor of 1 is fine for a quick demo or prototype, but in practice\nyou’ll almost always want a replication factor of 2 or 3 to provide data availabil-\nity in the case of machine failures.\nFigure 2.19\nThese are the results from creating a topic. It’s important to create your topics ahead of time so you \ncan supply topic-specific configurations. Otherwise, autocreated topics will use default configuration or the \nconfiguration in the server.properties file.\n \n",
      "content_length": 2161,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "53\nInstalling and running Kafka\n\npartitions—This flag specifies the number of partitions that the topic will use.\nAgain, just one partition is fine here, but if you have greater load, you’ll certainly\nwant more partitions. Determining the correct number of partitions is not an\nexact science.\nSENDING A MESSAGE\nSending a message in Kafka generally involves writing a producer client, but Kafka\nalso comes with a handy script called kafka-console-producer that allows you to\nsend a message from a terminal window. We’ll use the console producer in this exam-\nple, but we’ve covered how to use the KafkaProducer in section 2.4.1 of this chapter.\n To send your first message, run the following command (also shown in figure 2.20):\n# command assumes running from bin directory\n./kafka-console-producer.sh --topic first-topic --broker-list localhost:9092\nThere are several options for configuring the console producer, but for now we’ll only\nuse the required ones: the topic to send the message to, and a list of Kafka brokers to\nconnect to (in this case, just the one on your local machine).\n Starting a console producer is a “blocking script,” so after executing the preceding\ncommand, you enter some text and press Enter. You can send as many messages as you\nlike, but for our demo purposes you can type a single message, “the quick brown fox\njumped over the lazy dog,” press Enter, and then press Ctrl-C to exit the producer.\nREADING A MESSAGE\nKafka also provides a console consumer for reading messages from the command line.\nThe console consumer is similar to the console producer: once started, it will keep\nreading messages from the topic until the script is stopped by you (with Ctrl-C).\n To launch the console consumer, run this command:\nbin/kafka-console-consumer.sh --topic first-topic\n➥ --bootstrap-server localhost:9092 --from-beginning\nAfter starting the console consumer, you should see something like figure 2.21 in your\nterminal.\n The --from-beginning parameter specifies that you’ll receive any message not\ndeleted from that topic. The console consumer won’t have any committed offsets, so if\nyou didn’t have the --from-beginning setting, you’d only get messages sent after the\nconsole consumer had started.\nFigure 2.20\nThe console producer is a great tool for quickly testing your configuration and ensuring \nend-to-end functionality.\n \n",
      "content_length": 2353,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "54\nCHAPTER 2\nKafka quickly\nYou’ve just completed a whirlwind tour of Kafka and produced and consumed your\nfirst message. If you haven’t read the first part of this chapter, it’s time to go back to\nthe beginning this chapter to learn the details of how Kafka works!\nSummary\nKafka is a message broker that receives messages and stores them in a way that\nmakes it easy and fast to respond to consumer requests. Messages are never\npushed out to consumers, and message retention in Kafka is entirely indepen-\ndent of when and how often messages are consumed.\nKafka uses partitions for achieving high throughput and to provide a means for\ngrouping messages with the same keys in order.\nProducers are used for sending messages to Kafka.\nNull keys mean round-robin partition assignment; otherwise, the producer uses\nthe hash of the key, modulus the number of partitions, for partition assignment.\nConsumers are what you use to read messages from Kafka.\nConsumers that are part of a consumer group are given topic-partition alloca-\ntions in an attempt to distribute messages evenly.\nIn the next chapter, we’ll start looking at Kafka Streams with a concrete example from\nthe world of retail sales. Although Kafka Streams will handle the creation of all con-\nsumer and producer instances, you should be able to see the concepts we introduced\nhere come into play.\nFigure 2.21\nThe console consumer is a handy tool for quickly getting a feel for whether data is flowing and if \nmessages contain the expected information.\n \n",
      "content_length": 1515,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "Part 2\nKafka Streams development\nThis part of the book builds on the previous part and puts the mental model\nof Kafka Streams into action as you develop your first Kafka Streams applica-\ntion. Once you’ve gotten your feet wet, we’ll walk through the significant Kafka\nStreams APIs.\n You’ll learn about providing state to a streaming application and how to use\nstate for performing joins, much like the joins you perform when running SQL\nqueries. Then we’ll move on to a new abstraction from Kafka Streams: the KTable\nAPI. This part of the book begins with the high-level DSL, but we’ll wrap up by\ndiscussing the lower-level Processor API and how you can use it to make Kafka\nStreams do pretty much anything you need it to do.\n \n",
      "content_length": 728,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "57\nDeveloping Kafka Streams\nIn chapter 1, you learned about the Kafka Streams library. You learned about build-\ning a topology of processing nodes, or a graph that transforms data as it’s streaming\ninto Kafka. In this chapter, you’ll learn how to create this processing topology with\nthe Kafka Streams API.\n The Kafka Streams API is what you’ll use to build Kafka Streams applications.\nYou’ll learn how to assemble Kafka Streams applications; but, more important,\nyou’ll gain a deeper understanding of how the components work together and how\nthey can be used to achieve your stream-processing goals.\nThis chapter covers\nIntroducing the Kafka Streams API\nBuilding Hello World for Kafka Streams\nExploring the ZMart Kafka Streams application \nin depth\nSplitting an incoming stream into multiple \nstreams\n \n",
      "content_length": 808,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "58\nCHAPTER 3\nDeveloping Kafka Streams\n3.1\nThe Streams Processor API\nThe Kafka Streams DSL is the high-level API that enables you to build Kafka Streams\napplications quickly. The high-level API is very well thought out, and there are meth-\nods to handle most stream-processing needs out of the box, so you can create a sophis-\nticated stream-processing program without much effort. At the heart of the high-level\nAPI is the KStream object, which represents the streaming key/value pair records.\n Most of the methods in the Kafka Streams DSL return a reference to a KStream\nobject, allowing for a fluent interface style of programming. Additionally, a good\npercentage of the KStream methods accept types consisting of single-method inter-\nfaces allowing for the use of Java 8 lambda expressions. Taking these factors into\naccount, you can imagine the simplicity and ease with which you can build a Kafka\nStreams program.\n Back in 2005, Martin Fowler and Eric Evans developed the concept of the fluent\ninterface—an interface where the return value of a method call is the same instance\nthat originally called the method (https://martinfowler.com/bliki/FluentInterface\n.html). This approach is useful when constructing objects with several parameters,\nsuch as \nPerson.builder().firstName(\"Beth\").withLastName(\"Smith\").with-\nOccupation(\"CEO\"). In Kafka Streams, there is one small but important difference:\nthe returned KStream object is a new instance, not the same instance that made the\noriginal method call.\n There’s also a lower-level API, the Processor API, which isn’t as succinct as the\nKafka Streams DSL but allows for more control. We’ll cover the Processor API in chap-\nter 6. With that introduction out of the way, let’s dive into the requisite Hello World\nprogram for Kafka Streams. \n3.2\nHello World for Kafka Streams\nFor the first Kafka Streams example, we’ll deviate from the problem outlined in chap-\nter 1 to a simpler use case. This will get off the ground quickly so you can see how\nKafka Streams works. We’ll get back to the problem from chapter 1 later in section 3.1.1\nfor a more realistic, concrete example.\n Your first program will be a toy application that takes incoming messages and con-\nverts them to uppercase characters, effectively yelling at anyone who reads the mes-\nsage. You’ll call this the Yelling App.\n Before diving into the code, let’s take a look at the processing topology you’ll assem-\nble for this application. You’ll follow the same pattern as in chapter 1, where you built\nup a processing graph topology with each node in the graph having a particular func-\ntion. The main difference is that this graph will be simpler, as you can see in figure 3.1.\n As you can see, you’re building a simple processing graph—so simple that it resem-\nbles a linked list of nodes more than the typical tree-like structure of a graph. But\nthere’s enough here to give you strong clues about what to expect in the code. There\nwill be a source node, a processor node transforming incoming text to uppercase, and\na sink processor writing results out to a topic.\n \n",
      "content_length": 3082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "59\nHello World for Kafka Streams\nThis is a trivial example, but the code shown here is representative of what you’ll see in\nother Kafka Streams programs. In most of the examples, you’ll see a similar structure:\n1\nDefine the configuration items.\n2\nCreate Serde instances, either custom or predefined.\n3\nBuild the processor topology.\n4\nCreate and start the KStream.\nWhen we get into the more advanced examples, the principal difference will be in the\ncomplexity of the processor topology. With that in mind, it’s time to build your first\napplication.\n3.2.1\nCreating the topology for the Yelling App\nThe first step to creating any Kafka Streams application is to create a source node.\nThe source node is responsible for consuming the records, from a topic, that will flow\nthrough the application. Figure 3.2 highlights the source node in the graph.\n The following line of code creates the source, or parent, node of the graph.\nKStream<String, String> simpleFirstStream = builder.stream(\"src-topic\",\n➥ Consumed.with(stringSerde, stringSerde));\nThe simpleFirstStreamKStream instance is set to consume messages written to the\nsrc-topic topic. In addition to specifying the topic name, you also provide Serde\nListing 3.1\nDefining the source for the stream\nUpperCase\nprocessor\nsrc-topic\nSource\nprocessor\nSink\nprocessor\nout-topic\nHere the source processor will consume\nmessages that will be fed into the\nprocessing topology.\nThe UpperCase processor simply uppercases all\nincoming text. It’s important to note that the\ncopy of the original message is what gets\nuppercased, but the original value is unchanged.\nThe terminal processor here takes\nthe uppercase text from the\nprevious processor and writes\nit out to a topic.\nFigure 3.1\nGraph (topology) of the Yelling App\n \n",
      "content_length": 1760,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "60\nCHAPTER 3\nDeveloping Kafka Streams\nobjects (via a Consumed instance) for deserializing the records from Kafka. You’ll use\nthe Consumed class for any optional parameters whenever you create a source node in\nKafka Streams.\n You now have a source node for your application, but you need to attach a process-\ning node to make use of the data, as shown in figure 3.3. The code used to attach the\nUpperCase\nprocessor\nsrc-topic\nSource\nprocessor\nSink\nprocessor\nout-topic\nFigure 3.2\nCreating the source node \nof the Yelling App\nUpperCase\nprocessor\nsrc-topic\nSource\nprocessor\nSink\nprocessor\nout-topic\nFigure 3.3\nAdding the uppercase \nprocessor to the Yelling App\n \n",
      "content_length": 658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "61\nHello World for Kafka Streams\nprocessor (a child node of the source node) is shown in the following listing. With this\nline, you create another KStream instance that’s a child node of the parent node.\nKStream<String, String> upperCasedStream =\n➥ simpleFirstStream.mapValues(String::toUpperCase);\nBy calling the KStream.mapValues function, you’re creating a new processing node\nwhose inputs are the results of going through the mapValues call.\n It’s important to remember that you shouldn’t modify the original value in the Value-\nMapper provided to mapValues. The upperCasedStream instance receives transformed\ncopies of the initial value from the simpleFirstStream.mapValues call. In this case,\nit’s uppercase text.\n The mapValues() method takes an instance of the ValueMapper<V, V1> interface.\nThe ValueMapper interface defines only one method, ValueMapper.apply, making it\nan ideal candidate for using a Java 8 lambda expression. This is what you’ve done here\nwith String::toUpperCase, which is a method reference, an even shorter form of a\nJava 8 lambda expression.\nNOTE\nMany Java 8 tutorials are available for lambda expressions and method\nreferences. Good starting points can be found in Oracle’s Java documenta-\ntion: “Lambda Expressions” (http://mng.bz/J0Xm) and “Method References”\n(http://mng.bz/BaDW).\nYou could have used the form s  s.toUpperCase(), but because toUpperCase is an\ninstance method on the String class, you can use a method reference.\n Using lambda expressions instead of concrete implementations is a pattern\nyou’ll see over and over with the Streams Processor API in this book. Because most\nof the methods expect types that are single method interfaces, you can easily use\nJava 8 lambdas.\n So far, your Kafka Streams application is consuming records and transforming\nthem to uppercase. The final step is to add a sink processor that writes the results out\nto a topic. Figure 3.4 shows where you are in the construction of the topology.\n The following code line adds the last processor in the graph.\nupperCasedStream.to(\"out-topic\", Produced.with(stringSerde, stringSerde));\nThe KStream.to method creates a sink-processing node in the topology. Sink proces-\nsors write records back out to Kafka. This sink node takes records from the upper-\nCasedStream processor and writes them to a topic named out-topic. Again, you\nprovide Serde instances, this time for serializing records written to a Kafka topic. But\nin this case, you use a Produced instance, which provides optional parameters for cre-\nating a sink node in Kafka Streams.\nListing 3.2\nMapping incoming text to uppercase\nListing 3.3\nCreating a sink node\n \n",
      "content_length": 2643,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "62\nCHAPTER 3\nDeveloping Kafka Streams\nNOTE\nYou don’t always have to provide Serde objects to either the Consumed\nor Produced objects. If you don’t, the application will use the serializer/dese-\nrializer listed in the configuration. Additionally, with the Consumed and\nProduced classes, you can specify a Serde for either the key or value only.\nThe preceding example uses three lines to build the topology:\nKStream<String,String> simpleFirstStream =\n➥ builder.stream(\"src-topic\", Consumed.with(stringSerde, stringSerde));\nKStream<String, String> upperCasedStream =\n➥ simpleFirstStream.mapValues(String::toUpperCase);\nupperCasedStream.to(\"out-topic\", Produced.with(stringSerde, stringSerde));\nEach step is on an individual line to demonstrate the different stages of the building\nprocess. But all methods in the KStream API that don’t create terminal nodes (meth-\nods with a return type of void) return a new KStream instance, which allows you to use\nthe fluent interface style of programming mentioned earlier. To demonstrate this\nidea, here’s another way you could construct the Yelling App topology:\nbuilder.stream(\"src-topic\", Consumed.with(stringSerde, stringSerde))\n➥ .mapValues(String::toUpperCase)\n➥ .to(\"out-topic\", Produced.with(stringSerde, stringSerde));\nThis shortens the program from three lines to one without losing any clarity or pur-\npose. From this point forward, all the examples will be written using the fluent inter-\nface style unless doing so causes the clarity of the program to suffer.\nUpperCase\nprocessor\nsrc-topic\nSource\nprocessor\nSink\nprocessor\nout-topic\nFigure 3.4\nAdding a processor for \nwriting the Yelling App results\n \n",
      "content_length": 1651,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "63\nHello World for Kafka Streams\n You’ve built your first Kafka Streams topology, but we glossed over the important\nsteps of configuration and Serde creation. We’ll look at those now. \n3.2.2\nKafka Streams configuration\nAlthough Kafka Streams is highly configurable, with several properties you can\nadjust for your specific needs, the first example uses only two configuration settings,\nAPPLICATION_ID_CONFIG and BOOTSTRAP_SERVERS_CONFIG:\nprops.put(StreamsConfig.APPLICATION_ID_CONFIG, \"yelling_app_id\");\nprops.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\nBoth settings are required because no default values are provided. Attempting to start\na Kafka Streams program without these two properties defined will result in a Config-\nException being thrown.\n The StreamsConfig.APPLICATION_ID_CONFIG property identifies your Kafka Streams\napplication, and it must be a unique value for the entire cluster. It also serves as a\ndefault value for the client ID prefix and group ID parameters if you don’t set either\nvalue. The client ID prefix is the user-defined value that uniquely identifies clients\nconnecting to Kafka. The group ID is used to manage the membership of a group of\nconsumers reading from the same topic, ensuring that all consumers in the group can\neffectively read subscribed topics.\n The StreamsConfig.BOOTSTRAP_SERVERS_CONFIG property can be a single host-\nname:port pair or multiple hostname:port comma-separated pairs. The value of this\nsetting points the Kafka Streams application to the locaction of the Kafka cluster. We’ll\ncover several more configuration items as we explore more examples in the book. \n3.2.3\nSerde creation\nIn Kafka Streams, the Serdes class provides convenience methods for creating Serde\ninstances, as shown here:\nSerde<String> stringSerde = Serdes.String();\nThis line is where you create the Serde instance required for serialization/deserializa-\ntion using the Serdes class. Here, you create a variable to reference the Serde for\nrepeated use in the topology. The Serdes class provides default implementations for\nthe following types:\nString\nByte array\nLong\nInteger\nDouble\nImplementations of the Serde interface are extremely useful because they contain the\nserializer and deserializer, which keeps you from having to specify four parameters\n \n",
      "content_length": 2307,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "64\nCHAPTER 3\nDeveloping Kafka Streams\n(key serializer, value serializer, key deserializer, and value deserializer) every time you\nneed to provide a Serde in a KStream method. In an upcoming example, you’ll create a\nSerde implementation to handle serialization/deserialization of more-complex types.\n Let’s take a look at the whole program you just put together. You can find the\nsource in src/main/java/bbejeck/chapter_3/KafkaStreamsYellingApp.java (source\ncode can be found on the book’s website here: https://manning.com/books/kafka-\nstreams-in-action).\npublic class KafkaStreamsYellingApp {\npublic static void main(String[] args) {\nProperties props = new Properties();\nprops.put(StreamsConfig.APPLICATION_ID_CONFIG, \"yelling_app_id\");  \nprops.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\nStreamsConfig streamingConfig = new StreamsConfig(props);   \nSerde<String> stringSerde = Serdes.String();  \nStreamsBuilder builder = new StreamsBuilder();   \nKStream<String, String> simpleFirstStream = builder.stream(\"src-topic\",  \n➥ Consumed.with(stringSerde, stringSerde));                             \nKStream<String, String> upperCasedStream =             \n➥ simpleFirstStream.mapValues(String::toUpperCase);   \nupperCasedStream.to( \"out-topic\",                    \n➥ Produced.with(stringSerde, stringSerde));         \nKafkaStreams kafkaStreams = new KafkaStreams(builder.build(),streamsConfig);\nkafkaStreams.start();     \nThread.sleep(35000);\nLOG.info(\"Shutting down the Yelling APP now\");\nkafkaStreams.close();\n}\n}\nYou’ve now constructed your first Kafka Streams application. Let’s quickly review the\nsteps involved, as it’s a general pattern you’ll see in most of your Kafka Streams\napplications:\n1\nCreate a StreamsConfig instance.\n2\nCreate a Serde object.\nListing 3.4\nHello World: the Yelling App\nProperties for configuring\nthe Kafka Streams program\nCreates the\nStreamsConfig with\nthe given properties\nCreates the\nSerdes used\nto serialize/\ndeserialize\nkeys and\nvalues\nCreates the StreamsBuilder \ninstance used to construct \nthe processor topology\nCreates the\nactual stream\nwith a source\ntopic to read\nfrom (the\nparent node\nin the graph)\nA processor using a Java 8 \nmethod handle (the first \nchild node in the graph)\nWrites the transformed \noutput to another topic \n(the sink node in the graph)\nKicks off the Kafka \nStreams threads\n \n",
      "content_length": 2350,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "65\nWorking with customer data\n3\nConstruct a processing topology.\n4\nStart the Kafka Streams program.\nApart from the general construction of a Kafka Streams application, a key takeaway here\nis to use lambda expressions whenever possible, to make your programs more concise.\n We’ll now move on to a more complex example that will allow us to explore more\nof the Streams Processor API. The example will be new, but the scenario is one you’re\nalready familiar with: ZMart data-processing goals. \n3.3\nWorking with customer data\nIn chapter 1, we discussed ZMart’s new requirements for processing customer data,\nintended to help ZMart do business more efficiently. We demonstrated how you could\nbuild a topology of processors that would work on purchase records as they come stream-\ning in from transactions in ZMart stores. Figure 3.5 shows the completed graph again.\nLet’s briefly review the requirements for the streaming program, which will also serve\nas a good description of what the program will do:\nAll records need to have credit card numbers protected, in this case by masking\nthe first 12 digits.\nYou need to extract the items purchased and the ZIP code to determine pur-\nchase patterns. This data will be written out to a topic.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFigure 3.5\nTopology for ZMart Kafka Streams program\n \n",
      "content_length": 1363,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "66\nCHAPTER 3\nDeveloping Kafka Streams\nYou need to capture the customer’s ZMart member number and the amount\nspent and write this information to a topic. Consumers of the topic will use this\ndata to determine rewards.\nYou need to write the entire transaction out to topic, which will be consumed by\na storage engine for ad hoc analysis.\nAs in the Yelling App, you’ll combine the fluent interface approach with Java 8 lamb-\ndas when building the application. Although it’s sometimes clear that the return type\nof a method call is a KStream object, other times it may not be. Keep in mind that the\nmajority of the methods in the KStream API return new KStream instances. Now, let’s\nbuild a streaming application that will satisfy ZMart’s business requirements.\n3.3.1\nConstructing a topology\nLet’s dive into building the processing topology. To help make the connection between\nthe code you’ll create here and the processing topology graph from chapter 1, I’ll high-\nlight the part of the graph that you’re currently working on.\nBUILDING THE SOURCE NODE\nYou’ll start by building the source node and first processor of the topology by chaining\ntwo calls to the KStream API together (highlighted in figure 3.6). It should be fairly obvi-\nous by now what the role of the origin node is. The first processor in the topology will be\nresponsible for masking credit card numbers to protect customer privacy.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nSource node consuming messages from\nthe Kafka transactions topic\nSecond node does the masking\nof credit card numbers\nFigure 3.6\nThe source processor consumes from a Kafka topic, and it feeds the \nmasking processor exclusively, making it the source for the rest of the topology.\n \n",
      "content_length": 1753,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "67\nWorking with customer data\nKStream<String,Purchase> purchaseKStream =\n➥ streamsBuilder.stream(\"transactions\",\n➥ Consumed.with(stringSerde, purchaseSerde))\n➥ .mapValues(p -> Purchase.builder(p).maskCreditCard().build());\nYou create the source node with a call to the StreamsBuilder.stream method using a\ndefault String serde, a custom serde for Purchase objects, and the name of the topic\nthat’s the source of the messages for the stream. In this case, you only specify one\ntopic, but you could have provided a comma-separated list of names or a regular\nexpression to match topic names instead.\n In this listing 3.5, you provide Serdes with a Consumed instance, but you could have\nleft that out and only provided the topic name and relied on the default Serdes pro-\nvided via configuration parameters.\n The next immediate call is to the KStream.mapValues method, taking a ValueMap-\nper<V, V1> instance as a parameter. Value mappers take a single parameter of one\ntype (a Purchase object, in this case) and map that object to a to a new value, possibly\nof another type. In this example, KStream.mapValues returns an object of the same\ntype (Purchase), but with a masked credit card number.\n Note that when using the KStream.mapValues method, the original key is unchanged\nand isn’t factored into mapping a new value. If you wanted to generate a new key/value\npair or include the key in producing a new value, you’d use the KStream.map method\nthat takes a KeyValueMapper<K, V, KeyValue<K1, V1>> instance. \nHINTS ABOUT FUNCTIONAL PROGRAMMING\nAn important concept to keep in mind with the map and mapValues functions is that\nthey’re expected to operate without side effects, meaning the functions don’t modify\nthe object or value presented as a parameter. This is because of the functional pro-\ngramming aspects in the KStream API. Functional programming is a deep topic, and a\nfull discussion is beyond the scope of this book, but we’ll briefly look at two central\nprinciples of functional programming here.\n The first principle is avoiding state modification. If an object requires a change or\nupdate, you pass the object to a function, and a copy or entirely new instance is made,\ncontaining the desired changes or updates. In listing 3.5, the lambda passed to\nKStream.mapValues is used to update the Purchase object with a masked credit card\nnumber. The credit card field on the original Purchase object is left unchanged.\n The second principle is building complex operations by composing several smaller\nsingle-purpose functions together. The composition of functions is a pattern you’ll\nfrequently see when working with the KStream API.\nDEFINITION\nFor the purposes of this book, I define functional programming as a\nprogramming approach in which functions are first-class objects. Further-\nmore, functions are expected to avoid creating side effects, such as modifying\nstate or mutable objects. \nListing 3.5\nBuilding the source node and first processor\n \n",
      "content_length": 2959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "68\nCHAPTER 3\nDeveloping Kafka Streams\nBUILDING THE SECOND PROCESSOR\nNow you’ll build the second processor, responsible for extracting pattern data from a\ntopic, which ZMart can use to determine purchase patterns in regions of the country.\nYou’ll also add a sink node responsible for writing the pattern data to a Kafka topic.\nThe construction of these is demonstrated in figure 3.7.\nIn listing 3.6, you can see the purchaseKStream processor using the familiar mapValues\ncall to create a new KStream instance. This new KStream will start to receive Purchase-\nPattern objects created as a result of the mapValues call.\nKStream<String, PurchasePattern> patternKStream =\n➥ purchaseKStream.mapValues(purchase ->\n➥ PurchasePattern.builder(purchase).build());\npatternKStream.to(\"patterns\",\n➥ Produced.with(stringSerde,purchasePatternSerde));\nListing 3.6\nSecond processor and a sink node that writes to Kafka\nAgain you see two nodes in the graph, but\nwith the ﬂuent style of programming in\nKafka Streams, sometimes it’s easy to\noverlook the fact that you’re creating\ntwo nodes.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nThis is the fourth node overall,\nbut it does no processing. This\nnode writes PurchasePattern\nout to a topic.\nSecond processing node\n(third node overall) builds\nthe PurchasePattern object\nFigure 3.7\nThe second processor builds purchase-pattern information. The sink node writes the \nPurchasePattern object out to a Kafka topic.\n \n",
      "content_length": 1473,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "69\nWorking with customer data\nHere, you declare a variable to hold the reference of the new KStream instance,\nbecause you’ll use it to print the results of the stream to the console with a print call.\nThis is very useful during development and for debugging. The purchase-patterns\nprocessor forwards the records it receives to a child node of its own, defined by the\nmethod call KStream.to, writing to the patterns topic. Note the use of a Produced\nobject to provide the previously built Serde.\n The KStream.to method is a mirror image of the KStream.source method.\nInstead of setting a source for the topology to read from, the KStream.to method\ndefines a sink node that’s used to write the data from a KStream instance to a Kafka\ntopic. The KStream.to method also provides overloaded versions in which you can\nleave out the Produced parameter and use the default Serdes defined in the configu-\nration. One of the optional parameters you can set with the Produced class is Stream-\nPartitioner, which we’ll discuss next. \nBUILDING THE THIRD PROCESSOR\nThe third processor in the topology is the customer rewards accumulator node shown\nin figure 3.8, which will let ZMart track purchases made by members of their pre-\nferred customer club. The rewards accumulator sends data to a topic consumed by\napplications at ZMart HQ to determine rewards when customers complete purchases.\nThe rewards\nprocessor builds a\nRewards object and\npasses the object to a\nsink processor, which\nserializes and writes the\nobject out to a topic.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFigure 3.8\nThe third processor creates the RewardAccumulator object from the \npurchase data. The terminal node writes the results out to a Kafka topic.\n \n",
      "content_length": 1749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "70\nCHAPTER 3\nDeveloping Kafka Streams\nKStream<String, RewardAccumulator> rewardsKStream =\n➥ purchaseKStream.mapValues(purchase ->\n➥ RewardAccumulator.builder(purchase).build());\nrewardsKStream.to(\"rewards\",\n➥ Produced.with(stringSerde,rewardAccumulatorSerde));\nYou build the rewards accumulator processor using what should be by now a familiar\npattern: creating a new KStream instance that maps the raw purchase data contained\nin the record to a new object type. You also attach a sink node to the rewards accumu-\nlator so the results of the rewards KStream can be written to a topic and used for deter-\nmining customer reward levels. \nBUILDING THE LAST PROCESSOR\nFinally, you’ll take the first KStream you created, purchaseKStream, and attach a sink\nnode to write out the raw purchase records (with credit cards masked, of course) to a\ntopic called purchases. The purchases topic will be used to feed into a NoSQL store\nsuch as Cassandra (http://cassandra.apache.org/), Presto (https://prestodb.io/), or\nElastic Search (www.elastic.co/webinars/getting-started-elasticsearch) to perform ad\nhoc analysis. Figure 3.9 shows the final processor.\npurchaseKStream.to(\"purchases\", Produced.with(stringSerde, purchaseSerde));\nListing 3.7\nThird processor and a terminal node that writes to Kafka\nListing 3.8\nFinal processor\nThe ﬁnal processor, a sink\nprocessor to be precise, writes\nthe purchase data out to\na topic with the credit card\ninformation still masked.\nPatterns\nMasking\nSource\nRewards\nRewards\nsink\nRewards\nsink\nPurchases\nsink\nFigure 3.9\nThe last node writes out the entire purchase transaction to a topic whose \nconsumer is a NoSQL data store.\n \n",
      "content_length": 1647,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "71\nWorking with customer data\nNow that you’ve built the application piece by piece, let’s look at the entire applica-\ntion (src/main/java/bbejeck/chapter_3/ZMartKafkaStreamsApp.java). You’ll quickly\nnotice it’s more complicated than the previous Hello World (the Yelling App) example.\npublic class ZMartKafkaStreamsApp {\npublic static void main(String[] args) {\n// some details left out for clarity\nStreamsConfig streamsConfig = new StreamsConfig(getProperties());\nJsonSerializer<Purchase> purchaseJsonSerializer = new\n➥ JsonSerializer<>();\nJsonDeserializer<Purchase> purchaseJsonDeserializer =  \n➥ new JsonDeserializer<>(Purchase.class);                     \nSerde<Purchase> purchaseSerde =\n➥ Serdes.serdeFrom(purchaseJsonSerializer, purchaseJsonDeserializer);\n//Other Serdes left out for clarity\nSerde<String> stringSerde = Serdes.String();\nStreamsBuilder streamsBuilder = new StreamsBuilder();\nKStream<String,Purchase> purchaseKStream =                \n➥ streamsBuilder.stream(\"transactions\",                          \n➥ Consumed.with(stringSerde, purchaseSerde))                     \n➥ .mapValues(p -> Purchase.builder(p).maskCreditCard().build()); \nKStream<String, PurchasePattern> patternKStream =   \n➥ purchaseKStream.mapValues(purchase ->                    \n➥ PurchasePattern.builder(purchase).build());              \npatternKStream.to(\"patterns\",\n➥ Produced.with(stringSerde,purchasePatternSerde));\nKStream<String, RewardAccumulator> rewardsKStream =  \n➥ purchaseKStream.mapValues(purchase ->                     \n➥ RewardAccumulator.builder(purchase).build());             \nrewardsKStream.to(\"rewards\",\n➥ Produced.with(stringSerde,rewardAccumulatorSerde));\npurchaseKStream.to(\"purchases\",         \n➥ Produced.with(stringSerde,purchaseSerde));   \nKafkaStreams kafkaStreams =\n➥ new KafkaStreams(streamsBuilder.build(),streamsConfig);\nkafkaStreams.start();\n}\nListing 3.9\nZMart customer purchase KStream program\nCreates the Serde; the \ndata format is JSON.\nBuilds the \nsource and first \nprocessor\nBuilds the \nPurchasePattern \nprocessor\nBuilds the \nRewardAccumula\ntor processor\nBuilds the storage sink, the topic \nused by the storage consumer\n \n",
      "content_length": 2151,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "72\nCHAPTER 3\nDeveloping Kafka Streams\nNOTE\nI’ve left out some details in listing 3.9 for clarity. The code examples in\nthe book aren’t necessarily meant to stand on their own. The source code\nthat accompanies this book provides the full examples.\nAs you can see, this example is a little more involved than the Yelling App, but it has a\nsimilar flow. Specifically, you still performed the following steps:\nCreate a StreamsConfig instance.\nBuild one or more Serde instances.\nConstruct the processing topology.\nAssemble all the components and start the Kafka Streams program.\nIn this application, I’ve mentioned using a Serde, but I haven’t explained why or how\nyou create them. Let’s take some time now to discuss the role of the Serde in a Kafka\nStreams application. \n3.3.2\nCreating a custom Serde\nKafka transfers data in byte array format. Because the data format is JSON, you need\nto tell Kafka how to convert an object first into JSON and then into a byte array when\nit sends data to a topic. Conversely, you need to specify how to convert consumed byte\narrays into JSON, and then into the object type your processors will use. This conver-\nsion of data to and from different formats is why you need a Serde. Some serdes are\nprovided out of the box by the Kafka client dependency, (String, Long, Integer, and\nso on), but you’ll need to create custom serdes for other objects.\n In the first example, the Yelling App, you only needed a serializer/deserializer for\nstrings, and an implementation is provided by the Serdes.String() factory method.\nIn the ZMart example, however, you need to create custom Serde instances, because\nthe types of the objects are arbitrary. We’ll look at what’s involved in building a Serde\nfor the Purchase class. We won’t cover the other Serde instances, because they follow\nthe same pattern, just with different types.\n Building a Serde requires implementations of the Deserializer<T> and Serial-\nizer<T> interfaces. We’ll use the implementations in listings 3.10 and 3.11 through-\nout the examples. Also, you’ll use the Gson library from Google to convert objects to\nand from JSON. Here’s the serializer, which you can find in src/main/java/bbejeck/\nutil/serializer/JsonSerializer.java.\npublic class JsonSerializer<T> implements Serializer<T> {\nprivate Gson gson = new Gson();      \n@Override\npublic void configure(Map<String, ?> map, boolean b) {\n}\nListing 3.10\nGeneric serializer\nCreates the \nGson object\n \n",
      "content_length": 2445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "73\nWorking with customer data\n@Override\npublic byte[] serialize(String topic, T t) {\nreturn gson.toJson(t).getBytes(Charset.forName(\"UTF-8\"));  \n}\n@Override\npublic void close() {\n}\n}\nFor serialization, you first convert an object to JSON, and then get the bytes from the\nstring. To handle the conversions from and to JSON, the example uses Gson (https://\ngithub.com/google/gson).\n For the deserializing process, you take different steps: create a new string from a\nbyte array, and then use Gson to convert the JSON string into a Java object. This\ngeneric deserializer can be found in src/main/java/bbejeck/util/serializer/Json-\nDeserializer.java.\npublic class JsonDeserializer<T> implements Deserializer<T> {\nprivate Gson gson = new Gson();                   \nprivate Class<T> deserializedClass;          \npublic JsonDeserializer(Class<T> deserializedClass) {\nthis.deserializedClass = deserializedClass;\n}\npublic JsonDeserializer() {\n}\n@Override\n@SuppressWarnings(\"unchecked\")\npublic void configure(Map<String, ?> map, boolean b) {\nif(deserializedClass == null) {\ndeserializedClass = (Class<T>) map.get(\"serializedClass\");\n}\n}\n@Override\npublic T deserialize(String s, byte[] bytes) {\nif(bytes == null){\nreturn null;\n}\nreturn gson.fromJson(new String(bytes),deserializedClass);  \n}\nListing 3.11\nGeneric deserializer\nSerializes an\nobject to bytes\nCreates the \nGson object\nInstance variable of \nClass to deserialize\nDeserializes bytes to an\ninstance of expected Class\n \n",
      "content_length": 1467,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "74\nCHAPTER 3\nDeveloping Kafka Streams\n@Override\npublic void close() {\n}\n}\nNow, let’s go back to the following lines from listing 3.9:\nJsonDeserializer<Purchase> purchaseJsonDeserializer =  \n➥ new JsonDeserializer<>(Purchase.class);             \nJsonSerializer<Purchase> purchaseJsonSerializer =     \n➥ new JsonSerializer<>();                            \nSerde<Purchase> purchaseSerde =\n➥ Serdes.serdeFrom(purchaseJsonSerializer,purchaseJsonDeserializer);   \nAs you can see, a Serde object is useful because it serves as a container for the serial-\nizer and deserializer for a given object.\n We’ve covered a lot of ground so far in developing a Kafka Streams application. We\nstill have much more to cover, but let’s pause for a moment and talk about the devel-\nopment process itself and how you can make life easier for yourself while developing a\nKafka Streams application. \n3.4\nInteractive development\nYou’ve built the graph to process purchase records from ZMart in a streaming fashion,\nand you have three processors that write out to individual topics. During development\nit would certainly be possible to have a console consumer running to view results, but\nit would be good to have a more convenient solution, like the ability to watch data\nflowing through the topology in the console, as shown in figure 3.10.\n There’s a method on the KStream interface that can be useful during develop-\nment: the KStream.print method, which takes an instance of the Printed<K, V> class.\nCreates the Deserializer \nfor the Purchase class\nCreates the Serializer \nfor the Purchase class\nCreates the Serde for\nPurchase objects\nFigure 3.10\nA great tool while you’re developing is the capacity to print the data that’s output from each node to \nthe console. To enable printing to the console, just replace any of the to methods with a call to print.\n \n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "75\nInteractive development\nPrinted provides two static methods allowing you print to stdout, Printed.toSys-\nOut(), or to write results to a file, Printed.toFile(filePath).\n Additionally, you can label your printed results by chaining the withLabel()\nmethod, allowing you to print an initial header with the records. This is useful when\nyou’re dealing with results from different processors. It’s important that your objects\nprovide a meaningful toString implementation to create useful results when printing\nyour stream either to the console or a file.\n Finally, if you don’t want to use toString, or you want to customize how Kafka\nStreams prints records, there’s the Printed.withKeyValueMapper method, which\ntakes a KeyValueMapper instance so you can format your records in any way you want.\nThe same caveat I mentioned earlier—that you shouldn’t modify the original\nrecords—applies here as well.\n In this book, I focus on printing records to the console for all examples. Here are\nsome examples of using KStream.print in listing 3.11:\npatternKStream.print(Printed.<String, PurchasePattern>toSysOut()   \n➥ .withLabel(\"patterns\"));                                        \nrewardsKStream.print(Printed.<String, RewardAccumulator>toSysOut()  \n➥ .withLabel(\"rewards\"));                                          \npurchaseKStream.print(Printed.<String, Purchase>toSysOut()   \n➥ .withLabel(\"purchases\"));                                 \nLet’s take a quick look at the output you’ll see on the screen (figure 3.11) and how it can\nhelp you during development. With printing enabled, you can run the Kafka Streams\napplication directly from your IDE as you make changes, stop and start the application,\nand confirm that the output is what you expect. This is no substitute for unit and integra-\ntion tests, but viewing streaming results directly as you develop is a great tool.\nSets up to print the PurchasePattern\ntransformation to the console\nSets up to print the RewardAccumulator \ntransformation to the console\nPrints the purchase \ndata to the console\nName(s) given to the print\nstatement, helpful to make\nthis the same as the topic\nThe values for the records. Note that these are\nJSON strings and the Purchase, PurchasePattern,\nand RewardAccumulator objects deﬁned toString\nmethods to get this rendering on the console.\nThe keys for the records,\nwhich are null in this case\nNote the masked\ncredit card number!\nFigure 3.11\nThis a detailed view of the data on the screen. With printing to the console enabled, you’ll quickly \nsee if your processors are working correctly.\n \n",
      "content_length": 2569,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "76\nCHAPTER 3\nDeveloping Kafka Streams\nOne downside of using the print() method is that it creates a terminal node, mean-\ning you can’t embed it in a chain of processors. You need to have a separate statement.\nHowever, there’s also the KStream.peek method, which takes a ForeachAction\ninstance as a parameter and returns a new KStream instance. The ForeachAction\ninterface has one method, apply(), which has a return type of void, so nothing from\nKStream.peek is forwarded downstream, making it ideal for operations like printing.\nYou can embed it in a chain of processors without the need for a separate print state-\nment. You’ll see the KStream.peek method used in this manner in other examples in\nthe book. \n3.5\nNext steps\nAt this point, you have your Kafka Streams purchase-analysis program running well.\nOther applications have also been developed to consume the messages written to the\npatterns, rewards, and purchases topics, and the results for ZMart have been good.\nBut alas, no good deed goes unpunished. Now that the ZMart executives can see what\nyour streaming program can provide, a slew of new requirements come your way.\n3.5.1\nNew requirements\nYou now have new requirements for each of the three categories of results you’re pro-\nducing. The good news is that you’ll still use the same source data. You’re being asked\nto refine, and in some cases further break down, the data you’re providing. The new\nrequirements may be able to be applied to current topics, or they may require you to\ncreate entirely new topics:\nPurchases under a certain dollar amount need to be filtered out. Upper man-\nagement isn’t much interested in the small purchases for general daily articles.\nZMart has expanded and has bought an electronics chain and a popular coffee\nhouse chain. All purchases from these new stores will flow through the stream-\ning application you’ve set up. You need to send the purchases from these new\nsubsidiaries to their topics.\nThe NoSQL solution you’ve chosen stores items in key/value format. Although\nKafka also uses key/value pairs, the records coming into your Kafka cluster\ndon’t have keys defined. You need to generate a key for each record before the\ntopology forwards it to the purchases topic.\nMore requirements will inevitably come your way, but you can start to work on the cur-\nrent set of new requirements now. If you look through the KStream API, you’ll be\nrelieved to see that there are several methods already defined that will make fulfilling\nthese new demands easy.\nNOTE\nFrom this point forward, all code examples are pared down to the\nessentials to maximize clarity. Unless there’s something new to introduce, you\ncan assume that the configuration and setup code remain the same. These\ntruncated examples aren’t meant to stand alone—the full code listing for this\n \n",
      "content_length": 2809,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "77\nNext steps\nexample can be found in src/main/java/bbejeck/chapter_3/ZMartKafka-\nStreamsAdvancedReqsApp.java.\nFILTERING PURCHASES\nLet’s start with filtering out purchases that don’t reach the minimum threshold. To\nremove low-dollar purchases, you’ll need to insert a filter-processing node between\nthe KStream instance and the sink node. You’ll update the processor topology graph\nas shown in figure 3.12.\nYou can use the KStream method, which takes a Predicate<K,V> instance as a param-\neter. Although you’re chaining method calls together here, you’re creating a new pro-\ncessing node in the topology.\nKStream<Long, Purchase> filteredKStream =\n➥ purchaseKStream((key, purchase) ->\n➥ purchase.getPrice() > 5.00).selectKey(purchaseDateAsKey);\nThis code filters purchases that are less than $5.00 and selects the purchase date as a\nlong value for a key.\n The Predicate interface has one method defined, test(), which takes two parame-\nters—the key and the value—although, at this point, you only need to use the value.\nAgain, you can use a Java 8 lambda in place of a concrete type defined in the KStream API.\nListing 3.12\nFiltering on KStream\nThe ﬁltering processor will\nonly allow records through that\nmatch the given predicate—in\nthis case, purchases over a\ncertain dollar amount.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFiltering\nprocessor\nFigure 3.12\nYou’re placing a processor between the masking processor and the terminal \nnode that writes to Kafka. This filtering processor will drop purchases under a given dollar \namount.\n \n",
      "content_length": 1569,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "78\nCHAPTER 3\nDeveloping Kafka Streams\nDEFINITION\nIf you’re familiar with functional programming, you should feel\nright at home with the Predicate interface. If the term predicate is new to you,\nit’s nothing more than a given statement, such as x < 100. An object either\nmatches the predicate statement or doesn’t.\nAdditionally, you want to use the purchase timestamp as a key, so you use the select-\nKey processor, which uses the KeyValueMapper mentioned in section 3.4 to extract the\npurchase date as a long value. I cover details about selecting the key in the section\n“Generating a key.”\n A mirror-image function, KStreamNot, performs the same filtering functionality\nbut in reverse. Only records that don’t match the given predicate are processed further\nin the topology.\nSPLITTING/BRANCHING THE STREAM\nNow you need to split the stream of purchases into separate streams that can write to\ndifferent topics. Fortunately, the KStream.branch method is perfect. The KStream\n.branch method takes an arbitrary number of Predicate instances and returns an\narray of KStream instances. The size of the returned array matches the number of\npredicates supplied in the call.\n In the previous change, you modified an existing leaf on the processing topology.\nWith this requirement to branch the stream, you’ll create brand-new leaf nodes on\nthe graph of processing nodes, as shown in figure 3.13.\nThe KStream.branch method takes an array of\npredicates and returns an array containing an equal\nnumber of KStream instances, each one accepting\nrecords matching the corresponding predicate.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFiltering\nprocessor\nProcessor for records\nmatching predicate at\nindex 0\nProcessor for records\nmatching predicate at\nindex 1\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nFigure 3.13\nThe branch processor splits the stream into two: one stream consists of purchases from \nthe cafe, and the other stream contains purchases from the electronics store.\n \n",
      "content_length": 2037,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "79\nNext steps\nAs records from the original stream flow through the branch processor, each record is\nmatched against the supplied predicates in the order that they’re provided. The pro-\ncessor assigns records to a stream on the first match; no attempts are made to match\nadditional predicates.\n The branch processor drops records if they don’t match any of the given predi-\ncates. The order of the streams in the returned array matches the order of the predi-\ncates provided to the branch() method. A separate topic for each department may\nnot be the only approach, but we’ll stick with this for now. It satisfies the requirement,\nand it can be revisited later.\nPredicate<String, Purchase> isCoffee =                   \n➥ (key, purchase) ->                                    \n➥ purchase.getDepartment().equalsIgnoreCase(\"coffee\");  \nPredicate<String, Purchase> isElectronics =\n➥ (key, purchase) ->\n➥ purchase.getDepartment().equalsIgnoreCase(\"electronics\");\nint coffee = 0;       \nint electronics = 1;\nKStream<String, Purchase>[] kstreamByDept =          \n➥ purchaseKStream.branch(isCoffee, isElectronics);  \nkstreamByDept[coffee].to( \"coffee\", \nProduced.with(stringSerde, purchaseSerde));\nkstreamByDept[electronics].to(\"electronics\",     \n➥ Produced.with(stringSerde, purchaseSerde));   \nWARNING\nThe example in listing 3.13 sends records to several different top-\nics. Although Kafka can be configured to automatically create topics when it\nattempts to produce or consume for the first time from nonexistent topics,\nit’s not a good idea to rely on this mechanism. If you rely on autocreating top-\nics, the topics are configured with default values from the server.config prop-\nerties file, which may or may not be the settings you need. You should always\nthink about what topics you’ll need, the level of partitions, and the replica-\ntion factor ahead of time, and create them before running your Kafka\nStreams application.\nIn listing 3.13, you define the predicates ahead of time, because passing four lambda\nexpression parameters would be a little unwieldy. The indices of the returned array\nare also labeled, to maximize readability.\n This example demonstrates the power and flexibility of Kafka Streams. You’ve\nbeen able to take the original stream of purchase transactions and split them into four\nstreams with very few lines of code. Also, you’re starting to build up a more complex\nprocessing topology, all while reusing the same source processor.\nListing 3.13\nSplitting the stream\nCreates the \npredicates as \nJava 8 lambdas\nLabels the expected indices \nof the returned array\nCalls branch to split \nthe original stream \ninto two streams\nWrites the results of each \nstream out to a topic\n \n",
      "content_length": 2697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "80\nCHAPTER 3\nDeveloping Kafka Streams\nSo far, so good. You’ve met two of the three new requirements with ease. Now it’s time\nto implement the last additional requirement, generating a key for the purchase\nrecord to be stored.\nGENERATING A KEY\nKafka messages are in key/value pairs, so all records flowing through a Kafka Streams\napplication are key/value pairs as well. But there’s no requirement stating that keys\ncan’t be null. In practice, if there’s no need for a particular key, having a null key will\nreduce the overall amount of data that travels the network. All the records flowing\ninto the ZMart Kafka Streams application have null keys.\n That’s been fine, until you realize that your NoSQL storage solution stores data in\nkey/value format. You need a way to create a key from the Purchase data before it gets\nwritten out to the purchases topic. You certainly could use KStream.map to generate a\nkey and return a new key/value pair (where only the key would be new), but there’s a\nmore succinct KStream.selectKey method that returns a new KStream instance that\nproduces records with a new key (possibly a different type) and the same value. This\nchange to the processor topology is similar to filtering, in that you add a processing\nnode between the filter and the sink processor, shown in figure 3.14.\nKeyValueMapper<String, Purchase, Long> purchaseDateAsKey =  \n➥ (key, purchase) -> purchase.getPurchaseDate().getTime(); \nKStream<Long, Purchase> filteredKStream =                     \n➥ purchaseKStream((key, purchase) ->                         \n➥ purchase.getPrice() > 5.00).selectKey(purchaseDateAsKey);  \nfilteredKStream.print(Printed.<Long, Purchase>            \n➥ toSysOut().withLabel(\"purchases\"));  \nfilteredKStream.to(\"purchases\",                    \n➥ Produced.with(Serdes.Long(),purchaseSerde));    \nTo create the new key, you take the purchase date and convert it to a long. Although you\ncould pass a lambda expression, it’s assigned to a variable here to help with readability.\nSplitting vs. partitioning streams\nAlthough splitting and partitioning may seem like similar ideas, they’re unrelated in\nKafka and Kafka Streams. Splitting a stream with the KStream.branch method\nresults in creating one or more streams that could ultimately send records to another\ntopic. Partitioning is how Kafka distributes messages for one topic across servers,\nand aside from configuration tuning, it’s the principal means of achieving high\nthroughput in Kafka.\nListing 3.14\nGenerating a new key\nThe KeyValueMapper \nextracts the purchase \ndate and converts to \na long.\nFilters out purchases \nand selects the key in \none statement\nPrints the results \nto the console\nMaterializes the results \nto a Kafka topic\n \n",
      "content_length": 2718,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "81\nNext steps\nAlso, note that you need to change the serde type used in the KStream.to method,\nbecause you’ve changed the type of the key.\nThis is a simple example of mapping to a new key. Later, in another example, you’ll\nselect keys to enable joining separate streams. Also, all the examples up until this\npoint have been stateless, but there are several options for stateful transformations as\nwell, which you’ll see a little later on.\n3.5.2\nWriting records outside of Kafka\nThe security department at ZMart has approached you. Apparently, in one of the\nstores, there’s a suspicion of fraud. There have been reports that a store manager is\nentering invalid discount codes for purchases. Security isn’t sure what’s going on, but\nthey’re asking for your help.\n The security folks don’t want this information to go into a topic. You talk to them\nabout securing Kafka, about access controls, and about how you can lock down access\nto a topic, but the security folks are standing firm. These records need to go into a\nrelational database where they have full control. You sense this is a fight you can’t win,\nso you relent and resolve to get this task done as requested.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nAdd the select-key\nprocessor here after the\nﬁltering, as you only need to\ngenerate keys for records\nthat will be written out to\nthe purchases topic.\nFiltering\nprocessor\nSelect-key\nprocessor\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nFigure 3.14\nThe NoSQL data store will use the purchase date as a key for the data it stores. The new select-\nKey processor will extract the purchase date to be used as a key, right before you write the data to Kafka.\n \n",
      "content_length": 1733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "82\nCHAPTER 3\nDeveloping Kafka Streams\nFOREACH ACTIONS\nThe first thing you need to do is create a new KStream that filters results down to a sin-\ngle employee ID. Even though you have a large amount of data flowing through your\ntopology, this filter will reduce the volume to a tiny amount.\n Here, you’ll use KStream with a predicate that looks to match a specific employee\nID. This filter will be completely separate from the previous filter, and it’ll be attached\nto the source KStream instance. Although it’s entirely possible to chain filters, you\nwon’t do that here; you want full access to the data in the stream for this filter.\nNext, you’ll use a KStream.foreach method, as shown in figure 3.15. KStream.foreach\ntakes a ForeachAction<K, V> instance, and it’s another example of a terminal node.\nIt’s a simple processor that uses the provided ForeachAction instance to perform an\naction on each record it receives.\nForeachAction<String, Purchase> purchaseForeachAction = (key, purchase) ->\n➥ SecurityDBService.saveRecord(purchase.getPurchaseDate(),\n➥ purchase.getEmployeeId(), purchase.getItemPurchased());\nListing 3.15\nForeach operations\nThis ﬁlter will only forward records where the\nemployee ID matches the given predicate.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nAfter records are forwarded to the Foreach processor,\nthe value of each record is written to an external database.\nPurchase-\nprice\nprocessor\nSelect-key\nprocessor\nBranch\nprocessor\nEmployee\nID\nprocessor\nForeach-\nValue\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nFigure 3.15\nTo write purchases involving a given employee outside of the Kafka Streams application, you’ll first \nadd a filter processor to extract purchases by employee ID, and then you’ll use a foreach operator to write \neach record to an external relational database.\n \n",
      "content_length": 1870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "83\nSummary\npurchaseKStream.filter((key, purchase) ->\n➥ purchase.getEmployeeId()\n➥ .equals(\"source code has 000000\"))\n➥ .foreach(purchaseForeachAction);\nForeachAction uses a Java 8 lambda (again), and it’s stored in a variable, purchase-\nForeachAction. This requires an extra line of code, but the clarity gained by doing so\nmore than makes up for it. On the next line, another KStream instance sends the fil-\ntered results to the ForeachAction defined directly above it.\n Note that KStream.foreach is stateless. If you need state to perform some action\nfor each record, you can use the KStream.process method. The KStream.process\nmethod will be discussed in the next chapter when you add state to a Kafka Streams\napplication.\n If you step back and look at what you’ve accomplished so far, it’s pretty impressive,\nconsidering the amount of code written. Don’t get too comfortable, though, because\nupper management at ZMart has taken notice of your productivity. More changes and\nrefinements to the purchase-streaming analysis program are coming. \nSummary\nYou can use the KStream.mapValues function to map incoming record values to\nnew values, possibly of a different type. You also learned that these mapping\nchanges shouldn’t modify the original objects. Another method, KStream.map,\nperforms the same action but can be used to map both the key and the value to\nsomething new.\nA predicate is a statement that accepts an object as a parameter and returns\ntrue or false depending on whether that object matches a given condition.\nYou used predicates in the filter function to prevent records that didn’t match a\ngiven predicate from being forwarded in the topology.\nThe KStream.branch method uses predicates to split records into new streams\nwhen a record matches a given predicate. The processor assigns a record to a\nstream on the first match and drops unmatched records.\nYou can modify an existing key or create a new one using the KStream.select-\nKey method.\nIn the next chapter, we’ll start to look at state, the required properties for using state\nwith a steaming application, and why you might need to add state at all. Then you’ll add\nstate to a KStream application, first by using stateful versions of KStream methods you’ve\nseen in this chapter (KStream.mapValues()). For a more advanced example, you’ll per-\nform joins between two different streams of purchases to help ZMart improve customer\nservice.\n \n",
      "content_length": 2417,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "84\nStreams and state\nIn the last chapter, we dove headfirst into the Kafka Streams DSL and built a pro-\ncessing topology to handle streaming requirements from purchases at ZMart loca-\ntions. Although you built a nontrivial processing topology, it was one dimensional\nin that all transformations and operations were stateless. You considered each\ntransaction in isolation, without any regard to other events occurring at the same\ntime or within certain time boundaries, either before or after the transaction. Also,\nyou only dealt with individual streams, ignoring any possibility of gaining addi-\ntional insight by joining streams together.\n In this chapter, you’ll extract the maximum amount of information from the\nKafka Streams application. To get this level of information, you’ll need to use state.\nState is nothing more than the ability to recall information you’ve seen before and\nconnect it to current information. You can utilize state in different ways. We’ll look\nThis chapter covers\nApplying stateful operations to Kafka Streams\nUsing state stores for lookups and remembering \npreviously seen data\nJoining streams for added insight\nHow time and timestamps drive Kafka Streams\n \n",
      "content_length": 1194,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "85\nThinking of events\nat one example when we explore the stateful operations, such as the accumulation of\nvalues, provided by the Kafka Streams DSL.\n Another example of state we’ll discuss is the joining of streams. Joining streams is\nclosely related to the joins performed in database operations, such as joining records\nfrom the employee and department tables to generate a report on who staffs which\ndepartments in a company.\n We’ll also define what the state needs to look like and what the requirements are\nfor using state when we discuss state stores in Kafka Streams. Finally, we’ll weigh the\nimportance of timestamps and look at how they can help you work with stateful opera-\ntions, such as ensuring you only work with events occurring within a given time frame\nor helping you work with data arriving out of order.\n4.1\nThinking of events\nWhen it comes to event processing, events sometimes require no further information\nor context. At other times, an event on its own may be understood in a literal sense,\nbut without some added context, you might miss the significance of what is occurring;\nyou might think of the event in a whole new light, given some additional information.\n An example of an event that doesn’t require additional information is the\nattempted use of a stolen credit card. The transaction is canceled immediately once\nthe stolen card’s use is detected. You don’t need any additional information to make\nthat decision.\n But sometimes a singular event won’t give you enough information to make a deci-\nsion. Consider a series of stock purchases by three individual investors within a short\nperiod. On the face of it, there’s nothing about the purchases of XYZ Pharmaceutical\nstock, shown in figure 4.1, that would give you pause. Investors buying shares of the\nsame stock is something that happens every day on Wall Street.\nNow let’s add some context. Within a short period of the individual stock purchases,\nXYZ Pharmaceutical announced government approval for a new drug, which sent the\nstock price to historic highs. Additionally, those three investors had close ties to XYZ\nPharmaceutical. Now the transactions, shown in figure 4.2, can be viewed in a whole\nnew light.\nTimeline\n9:30 a.m.\n9:50 a.m.\n10:30 a.m.\n10,000 shares of XYZ\nPharmaceutical\npurchased\n12,000 shares of XYZ\nPharmaceutical\npurchased\n15,000 shares of XYZ\nPharmaceutical\npurchased\nFigure 4.1\nStock transactions without any extra information don’t look like anything \nout of the ordinary.\n \n",
      "content_length": 2487,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "86\nCHAPTER 4\nStreams and state\nThe timing of these purchases and the information release raises some questions.\nWere these investors leaked information ahead of time? Or do the transactions repre-\nsent one investor with inside information trying to cover their tracks?\n4.1.1\nStreams need state\nThe preceding fictional scenario illustrates something that most of us already know\ninstinctively. Sometimes it’s easy to reason about what’s going on, but usually you need\nsome context to make good decisions. When it comes to stream processing, we call\nthat added context state.\n At first glance, the notions of state and stream processing may seem to be at odds\nwith each other. Stream processing implies a constant flow of discrete events that\ndon’t have much to do with each other and need to be dealt with as they occur. The\nnotion of state might evoke images of a static resource, such as a database table.\n In actuality, you can view these as one and the same. But the rate of change in a\nstream is potentially much faster and more frequent than in a database table.1\n You don’t always need state to work with streaming data. In some cases, you may\nhave discrete events or records that carry enough information to be valuable on their\nown. But more often than not, the incoming stream of data will need enrichment\nfrom some sort of store, either using information from events that arrived before, or\njoining related events with events from different streams. \n4.2\nApplying stateful operations to Kafka Streams\nIn this section, we’ll look at how you can add a stateful operation to an existing state-\nless one to improve the information collected by our application. You’re going to\nmodify the original topology from chapter 3, shown in figure 4.3 to refresh your\nmemory.\n In this topology, you produced a stream of purchase-transaction events. One of the\nprocessing nodes in the topology calculated reward points for customers based on the\n1 Jay Kreps, “Why Local State Is a Fundamental Primitive in Stream Processing,” http://mng.bz/sfoI.\nTimeline\n9:30 a.m.\n9:50 a.m.\n10:30 a.m.\n10,000 shares of XYZ\nPharmaceutical\npurchased\n12,000 shares of XYZ\nPharmaceutical\npurchased\n15,000 shares of XYZ\nPharmaceutical\npurchased\n11:00 a.m.\nFDA announces approval\nof experimental drug\ndeveloped by XYZ\nPharmaceutical. Stock\nprice soars 30%.\nFigure 4.2\nWhen you add some additional context about the timing of the stock purchases, you’ll \nsee them in an entirely new light.\n \n",
      "content_length": 2464,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "87\nApplying stateful operations to Kafka Streams\namount of the sale. But in that processor, you just calculated the total number of\npoints for the single transaction and forwarded the results.\n If you added some state to the processor, you could keep track of the cumulative\nnumber of reward points. Then, the consuming application at ZMart would need to\ncheck the total and send out a reward if needed.\n Now that you have a basic idea of how state can be useful in Kafka Streams (or\nany other streaming application), let’s look at some concrete examples. You’ll start\nwith transforming the stateless rewards processor into a stateful processor using\ntransformValues. You’ll keep track of the total bonus points achieved so far and the\namount of time between purchases, to provide more information to downstream\nconsumers.\n4.2.1\nThe transformValues processor\nThe most basic of the stateful functions is KStream.transformValues. Figure 4.4 illus-\ntrates how the KStream.transformValues() method operates.\n This method is semantically the same as KStream.mapValues(), with a few excep-\ntions. One difference is that transformValues has access to a StateStore instance to\naccomplish its task. The other difference is its ability to schedule operations to occur\nat regular intervals via a punctuate() method. The punctuate() method will be dis-\ncussed in detail when we cover the Processor API in chapter 6. \nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nSource node consuming messages from\nthe Kafka transactions topic\nSecond node does the\nmasking of credit\ncard numbers\nFigure 4.3\nHere’s another look at the topology from chapter 3.\n \n",
      "content_length": 1661,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "88\nCHAPTER 4\nStreams and state\n4.2.2\nStateful customer rewards\nThe rewards processor from the chapter 3 topology (see figure 4.3) for ZMart extracts\ninformation for customers belonging to ZMart’s rewards program. Initially, the rewards\nprocessor used the KStream.mapValues() method to map the incoming Purchase\nobject into a RewardAccumulator object.\n The RewardAccumulator object originally consisted of just two fields, the customer\nID and the purchase total for the transaction. Now, the requirements have changed\nsome, and points are being associated with the ZMart rewards program:\npublic class RewardAccumulator {\nprivate String customerId;     \nprivate double purchaseTotal;      \nprivate int currentRewardPoints;  \n//details left out for clarity\n}\nWhereas before, an application read from the rewards topic and calculated customer\nachievements, now management wants the point system to be maintained and calcu-\nlated by the streaming application. Additionally, you need to capture the amount of\ntime between the customer’s current and last purchase.\nTransaction object representing\ncustomer purchase in store\nRetrieves state by key\nand uses previously seen\ndata to update object\nTakes current data plus\nprevious data to create\nupdated state stored by key\ntransformValues forwards\ntransformed object to next\nprocessor in stream\nRecords ﬂowing\ninto transform\ntransformValues uses\nlocal state to perform\ntransformation\nCustomer ID\nTotal reward points\nDays since last purchase\n…………...\ntransformValues\nprocessor\nDate\nItems purchased\nCustomer ID\nStore ID\n………...\nLocal state\nIn-memory\nkey/value store\nFigure 4.4\nThe transformValues processor uses information stored in local state to \nupdate incoming records. In this case, the customer ID is the key used to retrieve and store \nthe state for a given record.\nCustomer ID\nTotal dollar amount \nof purchase\nCurrent number of \nreward points\n \n",
      "content_length": 1890,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "89\nApplying stateful operations to Kafka Streams\n When the application reads records from the rewards topic, the consuming appli-\ncation will only need to check whether the total points are above the threshold to dis-\ntribute an award. To meet this new goal, you can add the totalRewardPoints and\ndaysFromLastPurchase fields to the RewardAccumulator object, and use the local\nstate to keep track of accumulated points and the last date of purchase. Here’s the\nrefactored RewardAccumulator code (found in src/main/java/bbejeck/model/Reward-\nAccumulator.java; source code can be found on the book’s website here: https://\nmanning.com/books/kafka-streams-in-action) needed to support these changes.\npublic class RewardAccumulator {\nprivate String customerId;\nprivate double purchaseTotal;\nprivate int currentRewardPoints;\nprivate int daysFromLastPurchase;\nprivate long totalRewardPoints;        \n//details left out for clarity\n}\nThe updated rules for the purchase program are simple. The customer earns a point\nper dollar, and transaction totals are rounded down to the nearest dollar. The overall\nstructure of the topology won’t change, but the rewards-processing node will change\nfrom using the KStream.mapValues() method to using KStream.transformValues().\nSemantically, these two methods operate the same way, in that you still map the\nPurchase object into a RewardAccumulator object. The difference lies in the ability to\nuse local state to perform the transformation.\n Specifically, you’ll take two main two steps:\nInitialize the value transformer.\nMap the Purchase object to a RewardAccumulator using state.\nThe KStream.transformValues() method takes a ValueTransformerSupplier<V, R>\nobject, which supplies an instance of the ValueTransformer<V, R> interface. Your imple-\nmentation of the ValueTransformer will be PurchaseRewardTransformer<Purchase,\nRewardAccumulator>. For the sake of clarity, I won’t reproduce the entire class here in\nthe text. Instead, we’ll walk through the important methods for the example applica-\ntion. Also note that these code snippets aren’t meant to stand alone, and some details\nwill be left out for clarity. The full code can be found in the chapter source code\n(found on the book’s website here: https://manning.com/books/kafka-streams-in-\naction). Let’s move on and initialize the processor. \nListing 4.1\nRefactored RewardAccumulator object\nField added for \ntracking total \npoints\n \n",
      "content_length": 2423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "90\nCHAPTER 4\nStreams and state\n4.2.3\nInitializing the value transformer\nThe first step is to set up or create any instance variables in the transformer init()\nmethod. In the init() method, you retrieve the state store created when building the\nprocessing topology (we’ll cover how you add the state store in section 4.3.3).\nprivate KeyValueStore<String, Integer> stateStore;      \nprivate final String storeName;\nprivate ProcessorContext context;\npublic void init(ProcessorContext context) {\nthis.context = context;                  \nstateStore = (KeyValueStore)               \n➥ this.context.getStateStore(storeName);      \n}\nInside the transformer class, you cast to a KeyValueStore type. You’re not concerned\nwith the implementation inside the transformer at this point, just that you can retrieve\nvalues by key (more on state store implementation types in the next section).\n There are other methods (such as punctuate() and close()) not listed here that\nbelong to the ValueTransformer interface. We’ll discuss punctuate() and close()\nwhen we discuss the Processor API in chapter 6. \n4.2.4\nMapping the Purchase object to a RewardAccumulator \nusing state\nNow that you’ve initialized the processor, you can move on to transforming a Purchase\nobject using state. A few simple steps for performing the transformation are as follows:\n1\nCheck for points accumulated so far by customer ID.\n2\nSum the points for the current transaction and present the total.\n3\nSet the reward points on the RewardAccumulator to the new total amount.\n4\nSave the new total points by customer ID in the local state store.\npublic RewardAccumulator transform(Purchase value) {\nRewardAccumulator rewardAccumulator =\n➥ RewardAccumulator.builder(value).build();  \nInteger accumulatedSoFar =\n➥ stateStore.get(rewardAccumulator.getCustomerId());\nif (accumulatedSoFar != null) {\nrewardAccumulator.addRewardPoints(accumulatedSoFar);    \n}\nListing 4.2\ninit() method\nListing 4.3\nTransforming Purchase using state\nInstance variables\nSets a local reference \nto ProcessorContext\nRetrieves the StateStore instance \nby storeName variable. storeName \nis set in the constructor.\nBuilds the Reward-\nAccumulator object \nfrom Purchase\nRetrieves the latest \ncount by customer ID\nIf an accumulated number exists,\nadds it to the current total\n \n",
      "content_length": 2297,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "91\nApplying stateful operations to Kafka Streams\nstateStore.put(rewardAccumulator.getCustomerId(),\nrewardAccumulator.getTotalRewardPoints());   \nreturn rewardAccumulator;   \n}\nIn the transform() method, you first map a Purchase object into the RewardAccumu-\nlator—this is the same operation used in the mapValues() method. In the next few\nlines, the state gets involved in the transformation process. You do a lookup by key\n(customer ID) and add any points accumulated so far to the points from the current\npurchase. Then, you place the new total in the state store until it’s needed again.\n All that’s left is to update the rewards processor. But before you do, you need to\nconsider the fact that you’re accessing all sales by customer ID. Gathering information\nper sale for a given customer implies that all transactions for that customer are on the\nsame partition. But because the transactions come into the application without a key,\nthe producer assigns the transactions to partitions in a round-robin fashion. We cov-\nered round-robin partition assignment in chapter 2, but it’s worth reviewing it here\nagain—see figure 4.5.\n You’ll have an issue here (unless you’re using topics with only one partition).\nBecause the key isn’t populated, round-robin assignment means the transactions for a\ngiven customer won’t land on the same partition.\nStores the new \ntotal points in \nstateStore\nReturns the new \naccumulated \nrewards points\nBecause the keys are null, partition\nassignment starts at 0 and increases\nby 1 for each message up to 5. Then\nthe partition assignment starts over\nat 0 again.\nPartition 1\nPartition 0\nPartition 2\nPartition 3\nPartition 4\nPartition 5\nKafka broker 1\nKafka broker 2\nKafka broker 3\nProducer\nProducer\n0\n5\n4\n3\n2\n1\nFigure 4.5\nA Kafka producer distributes records evenly (round-robin) when the keys are null.\n \n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "92\nCHAPTER 4\nStreams and state\nPlacing customer transactions with the same ID on the same partition is important,\nbecause you need to look up records by ID in the state store. Otherwise, you’ll have\ncustomers with the same ID spread across different partitions, requiring you to look\nup the same customer in multiple state stores. (This statement could be interpreted to\nmean that each partition has its own state store, but that’s not the case. Partitions are\nassigned to a StreamTask, and each StreamTask has its own state store.)\n The way to solve this problem is to repartition the data by customer ID. We’ll look\nat how to do this next.\nREPARTITIONING THE DATA\nFirst, let’s have a general discussion on how repartitioning works (see figure 4.6). To\nrepartition records, first you may modify or change the key on the original record,\nand then you write out the record to a new topic. Next, you consume those records\nagain; but as a result of repartitioning, those records may come from different parti-\ntions than they were in originally.\nAlthough, in this simple example, you replaced the null key with a concrete value,\nrepartitioning need not always change the key. By using StreamPartitioner (http://\nmng.bz/9Z8A), you can apply just about any partition strategy you can think of, such\nThe keys are originally null, so distribution is done round-robin,\nresulting in records with the same ID across different partitions.\nNow with the key populated, all\nrecords with the identical ID land\nthe same partition.\nOriginal topic\nRepartition topic\n(null, {“id”:”5”, “info”:”123”} )\n(null, {“id”:”4”, “info”:”abc”} )\nnull, {“id”:”5”, “info”:”456”} )\n(null, {“id”:”4”, “info”:”def”} )\n(“4”, {“id”:”4”, “info”:”def”} )\n(“4”, {“id”:”4”, “info”:”abc”} )\n(“5”, {“id”:”5”, “info”:”456”} )\n(“5”, {“id”:”5”, “info”:”123”} )\nPartition 0\nPartition 1\nPartition 0\nPartition 1\nFor repartitioning, set the ID\nﬁeld as the key, and then write\nthe records to a topic.\nFigure 4.6\nRepartitioning: changing the original key to move records to a different partition\n \n",
      "content_length": 2046,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "93\nApplying stateful operations to Kafka Streams\nas partitioning on the value or part of the value instead of the key. In the next section,\nwe’ll demonstrate using StreamPartitioner in Kafka Streams.\nREPARTITIONING IN KAFKA STREAMS\nRepartitioning in Kafka Streams is easily accomplished by using the KStream.through()\nmethod, as illustrated in figure 4.7. The KStream.through() method creates an inter-\nmediate topic, and the current KStream instance will start writing records to that topic.\nA new KStream instance is returned from the through() method call, using the same\nintermediate topic for its source. This way, the data is seamlessly repartitioned.\nUnder the covers, Kafka Streams creates a sink and source node. The sink node is a\nchild processor of the calling KStream instance, and the new KStream instance uses\nthe new source node for its source of records. You could write the same type of sub-\ntopology yourself using the DSL, but using the KStream.through() method is more\nconvenient.\n If you’ve modified or changed keys and you don’t need a custom partition strategy,\nyou can rely on the DefaultPartitioner of the internal Kafka Streams Kafka-\nProducer to handle the partitioning. But if you’d like to apply your own partitioning\napproach, you can use StreamPartitioner. You’ll do just that in the next example.\n The code for using the KStream.through method is shown in the following list-\ning. In this example, KStream.through() takes two parameters: the topic name and\na Produced instance that provides the key Serde, the value Serde, and a Stream-\nPartitioner. Note that if you want to use the default key and value Serde instances\nand have no need for a custom partitioning strategy, there’s a version of KStream\n.through where you only provide the topic name.\nRewardsStreamPartitioner streamPartitioner =\n➥ new RewardsStreamPartitioner();           \nListing 4.4\nUsing the KStream.through method\nThe returned KStream instance immediately\nstarts to consume from the intermediate topic.\nOriginal KStream node\nmaking the “through” call\nWrites to topic\nReads from topic\nIntermediate topic\n(created by the application)\nFigure 4.7\nWriting out to an \nintermediate topic and then \nreading from it in a new \nKStream instance\nInstantiates the concrete \nStreamPartitioner instance\n \n",
      "content_length": 2294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "94\nCHAPTER 4\nStreams and state\nKStream<String, Purchase> transByCustomerStream =\n➥ purchaseKStream.through(\"customer_transactions\",\nProduced.with(stringSerde,\npurchaseSerde,\nstreamPartitioner));      \nHere, you’ve instantiated a RewardsStreamPartitioner. Let’s take a quick look at how\nit works as well as demonstrate how to create a StreamPartitioner. \nUSING A STREAMPARTITIONER\nTypically, the partition assignment is calculated by taking the hash of an object, modu-\nlus the number of partitions. In this case, you want to use the customer ID found in\nthe Purchase object so that all data for a given customer ends up in the same state\nstore. The following listing shows the StreamPartitioner implementation (found in\nsrc/main/java/bbejeck/chapter_4/partitioner/RewardsStreamPartitioner.java).\npublic class RewardsStreamPartitioner implements\n➥ StreamPartitioner<String, Purchase> {\n@Override\npublic Integer partition(String key,\nPurchase value,\nint numPartitions) {\nreturn value.getCustomerId().hashCode() % numPartitions;   \n}\n}\nNotice that you haven’t generated a new key. You’re using a property of the value to\ndetermine the correct partition. The key point to take away from this quick detour is\nthat when you’re using state to update and modify records, it’s necessary for those\nrecords to be on the same partition.\nWARNING\nDon’t mistake this simple repartitioning demonstration for some-\nthing you can be cavalier with. Although repartitioning is sometimes neces-\nsary, it comes at the cost of duplicating your data and incurs processing\noverhead. My advice is to use mapValues(), transformValues(), or flatMap-\nValues() operations whenever possible, because map(), transform(), and\nflatMap() can trigger automatic repartitioning. It’s best to use repartitioning\nlogic sparingly.\nNow, let’s get back to making changes in the rewards processor node to support state-\nful transformation. \n4.2.5\nUpdating the rewards processor\nUp to this point, you’ve created a new processing node that writes purchase objects\nout to a topic, partitioned by customer ID. This new topic will also be the source for\nyour soon-to-be-updated rewards processor. You did this to ensure that all purchases\nListing 4.5\nRewardsStreamPartitioner\nCreates a new \nKStream with the \nKStream.through \nmethod\nDetermines the\npartition by\ncustomer ID\n \n",
      "content_length": 2326,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "95\nApplying stateful operations to Kafka Streams\nfor a given customer are written to the same partition; hence, you’ll use the same state\nstore for all purchases by a given customer. Figure 4.8 shows the updated processing\ntopology with the new through processor between the credit card–masking node (the\nsource for all purchase transactions) and the rewards processor.\nNow, you’ll use the new Stream instance (created by the KStream.through() method)\nto update the rewards processor and use the stateful transform approach with the fol-\nlowing code.\nKStream<String, RewardAccumulator> statefulRewardAccumulator =\n➥ transByCustomerStream.transformValues(() ->\n➥ new PurchaseRewardTransformer(rewardsStateStoreName),\nrewardsStateStoreName);  \nstatefulRewardAccumulator.to(\"rewards\",\nProduced.with(stringSerde,\nrewardAccumulatorSerde));  \nThe KStream.transformValues method takes a ValueTransformerSupplier<V, R>\ninstance, which is provided via a Java 8 lambda expression.\nListing 4.6\nChanging the rewards processor to use stateful transformation\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nThe through processor\nrepartitions data with\nnew keys.\nPatterns\nSource\nThrough\nprocessor\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFiltering\nprocessor\nIn-memory key/value store\nused by the Rewards transforms\nvalues for state processing.\nThe rewards-processor\nnode is updated to use\na stateful TransformValues\nprocessor.\nRewards\nprocessor\nLocal state\nstore\nMasking\nFigure 4.8\nThe new through processor ensures that you send purchases to partitions by customer ID, \nallowing the rewards processor to make the right updates using local state.\nUses a stateful \ntransformation\nWrites the \nresults out \nto a topic\n \n",
      "content_length": 1736,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "96\nCHAPTER 4\nStreams and state\n In this section, you’ve added stateful processing to a stateless node. By adding state\nto the processor, ZMart can take action sooner after a customer makes a reward-quali-\nfying purchase. You’ve seen how to use a state store and the benefits using one pro-\nvides, but we’ve glossed over important details that you’ll need to understand about\nhow state can impact your applications. With this in mind, the next section will discuss\nwhich type of state store to use, what requirements you’ll need to make state efficient,\nand how you can add the state stores to a Kafka Streams program. \n4.3\nUsing state stores for lookups and previously \nseen data\nIn this section, we’ll look at the essentials of using state stores in Kafka Streams and the\nkey factors related to using state in streaming applications in general. This will enable\nyou to make practical choices when using state in your Kafka Streams applications.\n So far, we’ve discussed the need for using state with streams, and you’ve seen an\nexample of one of the more basic stateful operations available in Kafka Streams.\nBefore we get into more detail about using state stores in Kafka Streams, let’s briefly\nlook at two important attributes of state: data locality and failure recovery.\n4.3.1\nData locality\nData locality is critical for performance. Although key lookups are typically very fast,\nthe latency introduced by using remote storage becomes a bottleneck when you’re\nworking at scale.\n Figure 4.9 illustrates the principle behind data locality. The dashed line represents a\nnetwork call to retrieve data from a remote database. The solid line depicts a call to an\nServer\nProcessor\nLocal data\nstore\nData locality\nRemote data\nstore\nThe processor needs to\ncommunicate with a remote\nstore: the data must travel\nfurther and is subject to\nnetwork availability.\nThe processor communicates\nwith a local store, which could\nbe in memory or off-heap: the\ndata travels a shorter distance,\nand there’s no reliance on\nnetwork availability.\nFigure 4.9\nData locality is necessary for stream processing.\n \n",
      "content_length": 2088,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "97\nUsing state stores for lookups and previously seen data\nin-memory data store located on the same server. As you can see, making a call to get\ndata locally is more efficient than making a call across a network to a remote database.\n The key point here isn’t the degree of latency per record retrieval, which may be\nminimal. The important factor is that you’ll potentially process millions or billions of\nrecords through a streaming application. When multiplied by a factor that large, even\na small degree of network latency can have a huge impact.\n Data locality also implies that the store is local to each processing node, and\nthere’s no sharing across processes or threads. This way, if a process fails, it shouldn’t\nhave an impact on the other stream-processing processes or threads.\n The key point here is that although streaming applications will sometimes require\nstate, it should be local to where the processing occurs. Each server or node in the\napplication should have an individual data store. \n4.3.2\nFailure recovery and fault tolerance\nApplication failure is inevitable, especially when it comes to distributed applications.\nWe need to shift our focus from preventing failure to recovering quickly from failure,\nor even from restarts.\n Figure 4.10 depicts the principles of data locality and fault tolerance. Each proces-\nsor has its local data store, and a changelog topic is used to back up the state store.\nProcess 2\nProcessor\nLocal data\nstore\nLocal data\nstore\nProcess 1\nFault tolerance and failure recovery:\ntwo Kafka Streams processes running on the same server\nBecause each process has its own local state store and a shared-nothing\narchitecture, if either process fails, the other process will be unaffected.\nAlso, each store has its keys/values replicated to a topic, which is used\nto recover values lost when a process fails or restarts.\nData in state\nstore backed\nup in topic\nTopic used\nas changelog\nfor store\nTopic used\nas changelog\nfor store\nData in state\nstore backed\nup in topic\nProcessor\nFigure 4.10\nThe ability to recover from failure is important for stream-processing applications. Kafka \nStreams persists data from the local in-memory stores to an internal topic, so when you resume \noperations after a failure or a restart, the data is repopulated.\n \n",
      "content_length": 2287,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "98\nCHAPTER 4\nStreams and state\nBacking up a state store with a topic may seem expensive, but there are a couple of\nmitigating factors at play: a KafkaProducer sends records in batches, and by default,\nrecords are cached. It’s only on cache flush that Kafka Streams writes records to the\nstore, so only the latest record for a given key is persisted. We’ll discuss this caching\nmechanism with state stores in more detail in chapter 5.\n The state stores provided by Kafka Streams meet both the locality and fault-toler-\nance requirements. They’re local to the defined processors and don’t share access\nacross processes or threads. State stores also use topics for backup and quick recovery.\n We’ve now covered the requirements for using state with a streaming application.\nThe next step is to look at how you can enable the use of state in a Kafka Streams\napplication. \n4.3.3\nUsing state stores in Kafka Streams\nAdding a state store is a simple matter of creating a StoreSupplier instance with one\nof the static factory methods on the Stores class. There are two additional classes for\ncustomizing the state store: the Materialized and StoreBuilder classes. Which one\nyou’ll use depends on how you add the store to the topology. If you use the high-level\nDSL, you’ll typically use the Materialized class; when you work with the lower-level\nProcessor API, you’ll use the StoreBuilder.\n Even though the current example uses the high-level DSL, you add a state store to\na Transformer, which provides Processor API semantics. So, you’ll use the StoreBuilder\nfor state store customization.\nString rewardsStateStoreName = \"rewardsPointsStore\";\nKeyValueBytesStoreSupplier storeSupplier =\n➥ Stores.inMemoryKeyValueStore(rewardsStateStoreName);  \nStoreBuilder<KeyValueStore<String, Integer>> storeBuilder =\n➥ Stores.keyValueStoreBuilder(storeSupplier,\nSerdes.String(),\nSerdes.Integer());    \nbuilder.addStateStore(storeBuilder);  \nYou first create a StoreSupplier that provides an in-memory key/value store. Then,\nyou provide the StoreSupplier as a parameter to create a StoreBuilder, additionally\nspecifying String keys and Integer values. Finally, you add the StateStore to the\ntopology by providing the StoreBuilder to the StreamsBuilder.\n Here, you’ve created an in-memory key/value store with String keys and Integer\nvalues, and you’ve added the store to the application with the StreamsBuilder\n.addStateStore method. As a result, you can now use the state in your processors by\nusing the name rewardsStateStoreName created above, for the state store.\nListing 4.7\nAdding a state store\nCreates the \nStateStore \nsupplier\nCreates the StoreBuilder \nand specifies the key \nand value types\nAdds the state store \nto the topology\n \n",
      "content_length": 2718,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "99\nUsing state stores for lookups and previously seen data\n You’ve now seen an example of building an in-memory state store, but you have\noptions for creating different types of StateStore instances. Let’s look at those options.\n4.3.4\nAdditional key/value store suppliers\nIn addition to the Stores.inMemoryKeyValueStore method, you can use these other\nstatic factory methods for producing store suppliers:\n\nStores.persistentKeyValueStore\n\nStores.lruMap\n\nStores.persistentWindowStore\n\nStores.persistentSessionStore\nIt’s worth noting that all persistent StateStore instances provide local storage using\nRocksDB (http://rocksdb.org).\n Before we move on from state stores, I’d like to cover two other important aspects\nof Kafka Streams state stores: how fault tolerance is provided with changelog topics,\nand how you can configure changelog topics. \n4.3.5\nStateStore fault tolerance\nAll the StateStoreSupplier types have logging enabled as a default. Logging, in this\ncontext, means a Kafka topic used as a changelog to back up the values in the store\nand provide fault tolerance.\n For example, suppose you lost a machine running Kafka Streams. Once you recov-\nered your server and restarted your Kafka Streams application, the state stores for that\ninstance would be restored to their original contents (the last committed offset in the\nchangelog before crashing).\n This logging can be disabled when using the Stores factory with the disable-\nLogging() method. But you shouldn’t disabling logging without serious consideration,\nbecause doing so removes fault tolerance from the state stores and eliminates their\nability to recover after a crash. \n4.3.6\nConfiguring changelog topics\nThe changelogs for state stores are configurable via the withLoggingEnabled(Map\n<String, String> config) method. You can use any configuration parameters avail-\nable for topics in the map. The configuration of changelogs for state stores is import-\nant when building a Kafka Streams application. But keep in mind that you never need\nto create changelog topics—Kafka Streams handles changelog topic creation for you.\nNOTE\nState store changelogs are compacted topics, which we discussed in\nchapter 2. As you may recall, the delete semantics require a null value for a\nkey, so if you want to remove a record from a state store permanently, you’ll\nneed to do a put(key, null) operation.\nWith Kafka topics, the default setting for data retention for a log segment is one week,\nand the size is unlimited. Depending on your volume of data, this might be acceptable,\n \n",
      "content_length": 2544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "100\nCHAPTER 4\nStreams and state\nbut there’s a good chance you’ll want to adjust those settings. Additionally, the default\ncleanup policy is delete.\n Let’s first take a look at how you can configure your changelog topic to have a\nretention size of 10 GB and a retention time of 2 days.\nMap<String, String> changeLogConfigs = new HashMap<>();\nchangeLogConfigs.put(\"retention.ms\",\"172800000\" );\nchangeLogConfigs.put(\"retention.bytes\", \"10000000000\");\n// to use with a StoreBuilder\nstoreBuilder.withLoggingEnabled(changeLogConfigs);\n// to use with Materialized\nMaterialized.as(Stores.inMemoryKeyValueStore(\"foo\")\n.withLoggingEnabled(changeLogConfigs));\nIn chapter 2, we discussed compacted topics offered by Kafka. To refresh your mem-\nory: compacted topics use a different approach to cleaning a topic. Instead of deleting\nlog segments by size or time, log segments are compacted by keeping only the latest\nrecord for each key—older records with the same key are deleted. By default, Kafka\nStreams creates changelog topics with a delete policy of compact.\n But if you have a changelog topic with a lot of unique keys, compaction might not\nbe enough, as the size of the log segment will keep growing. In that case, the solution\nis simple. You specify a cleanup policy of delete and compact.\nMap<String, String> changeLogConfigs = new HashMap<>();\nchangeLogConfigs.put(\"retention.ms\",\"172800000\" );\nchangeLogConfigs.put(\"retention.bytes\", \"10000000000\");\nchangeLogConfigs.put(\"cleanup.policy\", \"compact,delete\");\nNow your changelog topic will be kept at a reasonable size even with unique keys. This\nhas been a brief section on topic configuration; appendix A provides more informa-\ntion about changelog topics and internal topic configuration.\n You’ve been introduced to the basics of stateful operations and state stores. You’ve\nlearned about the in-memory and persistent state stores Kafka Streams provides and\nhow you can include them in your streaming applications. You’ve also learned about\nthe importance of data locality and fault tolerance when using state in a streaming\napplication. Let’s move on to joining streams. \n4.4\nJoining streams for added insight\nAs we discussed earlier in the chapter, streams need state when events in the stream\ndon’t stand alone. Sometimes the state or context you need is another stream. In this\nListing 4.8\nSetting changelog properties\nListing 4.9\nSetting a cleanup policy\n \n",
      "content_length": 2413,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "101\nJoining streams for added insight\nsection, you’ll take different events from two streams with the same key, and combine\nthem to form a new event.\n The best way to learn about joining streams is to look at a concrete example, so\nwe’ll return to the ZMart scenario. As you’ll recall, ZMart opened a new line of stores\nthat carried electronics and related merchandise (CDs, DVDs, smart phones, and so\non). Trying a new approach, ZMart has partnered with a national coffee house and\nhas embedded a cafe in each store.\n In chapter 3, you were asked to separate the purchase transactions in those stores\ninto two distinct streams. Figure 4.11 shows the topology for this requirement.\nThis approach of embedding the cafe has been a big success for ZMart, and the com-\npany would like to see this trend continue, so they’ve decided to start a new program.\nZMart wants to keep traffic in the electronics store high by offering coupons for the\ncafe (hoping that increased traffic leads to additional purchases).\n ZMart wants to identify customers who have bought coffee and made a purchase in\nthe electronics store and give them coupons almost immediately after their second\ntransaction (see figure 4.12). ZMart intends to see if it can generate some sort of Pav-\nlovian response in their customers.\nThe KStream.branch method takes an array of\npredicates and returns an array containing an equal\nnumber of KStream instances, each one accepting\nrecords matching the corresponding predicate.\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFiltering\nprocessor\nProcessor for records\nmatching predicate at\nindex 0\nProcessor for records\nmatching predicate at\nindex 1\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nFigure 4.11\nThe branch processor, and where it stands in the overall topology\n \n",
      "content_length": 1843,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "102\nCHAPTER 4\nStreams and state\nTo determine when to issue a coupon, you’ll need to join the sales from the cafe with\nthe sales in the electronics store. Joining streams is relatively straightforward in terms\nof the code you need to write. Let’s start by setting up the data you need to process for\ndoing joins.\n4.4.1\nData setup\nFirst, let’s take another look at the portion of the topology responsible for branching\nthe streams (figure 4.13). In addition, let’s review the code used to implement the\nDetermining free coffee coupons\nIssue coupon for free\ncoffee drink.\nCafe\npurchase\nElectronics\nstore\npurchase\nCafe\npurchase\nElectronics\nstore\npurchase\n20-minute interval\n20-minute interval\nFigure 4.12\nPurchase records with timestamps within 20 minutes of each other are joined by \ncustomer ID and used to issue a reward to the customer—a free coffee drink, in this case.\nProcessor for records\nmatching predicate at\nindex 0\nThis processor contains the\narray of predicates and returns\nan equal number of KStream\ninstances, accepting records that\nmatch the given predicate.\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nProcessor for records\nmatching predicate at\nindex 1\nFigure 4.13\nTo perform a join, you need more than one stream. The branch processor \ntakes care of this by creating two streams: one containing cafe purchases and the other \ncontaining electronics purchases.\n \n",
      "content_length": 1413,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "103\nJoining streams for added insight\nbranching requirement (found in src/main/java/bbejeck/chapter_3/ZMartKafka-\nStreamsAdvancedReqsApp.java).\nPredicate<String, Purchase> coffeePurchase = (key, purchase) ->\n➥ purchase.getDepartment().equalsIgnoreCase(\"coffee\");\nPredicate<String, Purchase> electronicPurchase = (key, purchase) ->\n➥ purchase.getDepartment().equalsIgnoreCase(\"electronics\");     \nfinal int COFFEE_PURCHASE = 0;\nfinal int ELECTRONICS_PURCHASE = 1;     \nKStream<String, Purchase>[] branchedTransactions =\n➥ transactionStream.branch(coffeePurchase, electronicPurchase);  \nThis code shows how to perform branching: you use predicates to match incoming\nrecords into an array of KStream instances. The order of matching is the same as the\nposition of KStream objects in the array. The branching process drops any record not\nmatching any predicate.\n Although you have two streams to join, there’s an additional step to perform.\nRemember that purchase records come into the Kafka Streams application with no\nkeys. As a result, you need to add another processor to generate a key containing the\ncustomer ID. You need populated keys because that’s what you’ll use to join the\nrecords together. \n4.4.2\nGenerating keys containing customer IDs to perform joins\nTo generate a key, you select the customer ID from the purchase data in the stream.\nTo do so, you need to update the original KStream instance (transactionStream) and\ncreate another processing node between it and the branch node. This is shown in the fol-\nlowing code (found in src/main/java/bbejeck/chapter_4/KafkaStreamsJoinsApp.java).\nKStream<String, Purchase>[] branchesStream =\n➥ transactionStream.selectKey((k,v)->\n➥ v.getCustomerId()).branch(coffeePurchase, electronicPurchase);   \nFigure 4.14 shows an updated view of the processing topology based on listing 4.11.\nYou’ve seen before that changing the key may require you to repartition the data.\nThat’s true in this example as well, so why isn’t there a repartitioning step?\n In Kafka Streams, whenever you invoke a method that could result in generating a\nnew key (selectKey, map, or transform), an internal Boolean flag is set to true, indi-\ncating that the new KStream instance requires repartitioning. With this Boolean flag\nset, if you perform a join, reduce, or aggregation operation, the repartitioning is han-\ndled for you automatically.\nListing 4.10\nBranching into two streams\nListing 4.11\nGenerating new keys\nDefines the \npredicates \nfor matching \nrecords\nUses labeled integers for \nclarity when accessing the \ncorresponding array\nCreates the \nbranched \nstream\nInserts the selectKey\nprocessing node\n \n",
      "content_length": 2634,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "104\nCHAPTER 4\nStreams and state\nIn this example, you perform a selectKey() operation on the transactionStream, so\nthe resulting KStream is flagged for repartitioning. Additionally, you immediately per-\nform a branching operation, so each KStream resulting from the branch() call is\nflagged for repartitioning as well.\nNOTE\nIn the example, you repartition by the key only. But there may be times\nwhen you either don’t want to use the key or want to use some combination\nof the key and value. In these cases, you can use the StreamPartitioner<K,\nV> interface, as you saw in the example in listing 4.5 in the section, “Using a\nStreamPartitioner.”\nNow that you have two separate streams with populated keys, you’re ready for the next\nstep: joining the streams by key. \n4.4.3\nConstructing the join\nThe next step is to perform the actual join. You’ll take the two branched streams and\njoin them with the KStream.join() method. The topology is shown in figure 4.15.\nBranch\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nCafe\nsink\nElectronics\nsink\nThis processor is inserted to extract the\ncustomer ID from the transaction object to\nbe used for the key. This sets up the join\nbetween the two types of purchases.\nProcessor for records\nmatching predicate at\nindex 0\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nFiltering\nprocessor\nSelect-key\nprocessor\nProcessor for records\nmatching predicate at\nindex 1\nFigure 4.14\nYou need to remap the key/value purchase records into records where the key contains the \ncustomer ID. Fortunately, you can extract the customer ID from the Purchase object.\n \n",
      "content_length": 1611,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "105\nJoining streams for added insight\nJOINING PURCHASE RECORDS\nTo create the joined record, you need to create an instance of a ValueJoiner<V1, V2,\nR>. ValueJoiner takes two objects, which may or may not be of the same type, and it\nreturns a single object, possibly of a different type. In this case, ValueJoiner takes two\nPurchase objects and returns a CorrelatedPurchase object. Let’s take a look at the\ncode (found in src/main/java/bbejeck/chapter_4/joiner/PurchaseJoiner.java).\npublic class PurchaseJoiner\n➥ implements ValueJoiner<Purchase, Purchase, CorrelatedPurchase> {\n@Override\npublic CorrelatedPurchase apply(Purchase purchase, Purchase purchase2) {\nCorrelatedPurchase.Builder builder =\n➥ CorrelatedPurchase.newBuilder();           \nDate purchaseDate =\n➥ purchase != null ? purchase.getPurchaseDate() : null;\nDouble price = purchase != null ? purchase.getPrice() : 0.0;\nListing 4.12\nValueJoiner implementation\nPatterns\nMasking\nSource\nJoin\nprocessor\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nSelect-key\nprocessor\nBranch\nprocessor\nJoin results\nFiltering\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nLocal state\nstore\nTransform\nvalues\nLocal state\nstore\nLocal state\nstore\nThe join is actually a series of\nprocessors used to complete\nthe join. Because that code is\ninternal to Kafka Streams, you\nabstract the join away and\nrepresent it as one “logical”\njoin processor.\nRecords from\nboth processors\nﬂow into the\njoin processor.\nTwo state stores are used for\njoins: one to hold the keys\nfor the main stream, and the\nother to look for matches in\nthe other stream.\nThrough\nprocessor\nFigure 4.15\nIn the updated topology, both the cafe and electronics processors forward their records to the join \nprocessor. The join processor uses two state stores to search for matches for a record in the other stream.\nInstantiates \nthe builder\n \n",
      "content_length": 1835,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "106\nCHAPTER 4\nStreams and state\nString itemPurchased =\n➥ purchase != null ? purchase.getItemPurchased() : null;  \nDate otherPurchaseDate =\n➥ otherPurchase != null ? otherPurchase.getPurchaseDate() : null;\nDouble otherPrice =\n➥ otherPurchase != null ? otherPurchase.getPrice() : 0.0;\nString otherItemPurchased =\n➥ otherPurchase != null ? otherPurchase.getItemPurchased() : null; \nList<String> purchasedItems = new ArrayList<>();\nif (itemPurchased != null) {\npurchasedItems.add(itemPurchased);\n}\nif (otherItemPurchased != null) {\npurchasedItems.add(otherItemPurchased);\n}\nString customerId =\n➥ purchase != null ? purchase.getCustomerId() : null;\nString otherCustomerId =\n➥ otherPurchase != null ? otherPurchase.getCustomerId() : null;\nbuilder.withCustomerId(customerId != null ? customerId : otherCustome\nrId)\n.withFirstPurchaseDate(purchaseDate)\n.withSecondPurchaseDate(otherPurchaseDate)\n.withItemsPurchased(purchasedItems)\n.withTotalAmount(price + otherPrice);\nreturn builder.build();      \n}\n}\nTo create the CorrelatedPurchase object, you extract some information from each\nPurchase object. Because of the number of items you need to construct the new\nobject, you use the builder pattern, which makes the code clearer and eliminates any\nerrors due to misplaced parameters. Additionally, the PurchaseJoiner checks for null\nvalues with both of the provided Purchase objects, so it can be used for inner, outer,\nand left-outer joins. We’ll discuss the different join options in section 4.4.4. For now,\nwe’ll move on to implementing the join between streams. \nIMPLEMENTING THE JOIN\nYou’ve seen how to merge records resulting from the join between streams, so let’s\nmove on to calling the actual KStream.join method (found in src/main/java/bbejeck/\nchapter_4/KafkaStreamsJoinsApp.java).\nHandles a null \nPurchase in the case \nof an outer join\nHandles a null\nPurchase in the\ncase of a left\nouter join\nReturns the new \nCorrelatedPurchase \nobject\n \n",
      "content_length": 1942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "107\nJoining streams for added insight\nKStream<String, Purchase> coffeeStream =\n➥ branchesStream[COFFEE_PURCHASE];       \nKStream<String, Purchase> electronicsStream =\n➥ branchesStream[ELECTRONICS_PURCHASE];\nValueJoiner<Purchase, Purchase, CorrelatedPurchase> purchaseJoiner =\n➥ new PurchaseJoiner();                                \nJoinWindows twentyMinuteWindow =\nJoinWindows.of(60 * 1000 * 20);\nKStream<String, CorrelatedPurchase> joinedKStream =\n➥ coffeeStream.join(electronicsStream,         \npurchaseJoiner,\ntwentyMinuteWindow,\nJoined.with(stringSerde,\npurchaseSerde,\npurchaseSerde));     \njoinedKStream.print(\"joinedStream\");    \nYou supply four parameters to the KStream.join method:\n\nelectronicsStream—The stream of electronic purchases to join with.\n\npurchaseJoiner—An implementation of the ValueJoiner<V1, V2, R> inter-\nface. ValueJoiner accepts two values (not necessarily of the same type). The\nValueJoiner.apply method performs the implementation-specific logic and\nreturns a (possibly new) object of type R (maybe a whole new type). In this exam-\nple, purchaseJoiner will add some relevant information from both Purchase\nobjects, and it will return a CorrelatedPurchase object.\n\ntwentyMinuteWindow—A JoinWindows instance. The JoinWindows.of method\nspecifies a maximum time difference between the two values to be included in\nthe join. In this case, the timestamps must be within 20 minutes of each other.\nA Joined instance—Provides optional parameters for performing joins. In this\ncase, it’s the key and the value Serde for the calling stream, and the value Serde\nfor the secondary stream. You only have one key Serde because, when joining\nrecords, keys must be of the same type.\nNOTE\nSerdes are required for joins because join participants are materialized\nin windowed state stores. This example provides only one Serde for the key,\nbecause both sides of the join must have a key of the same type.\nYou’ve specified that the purchases need to be within 20 minutes of each other, but no\norder is implied. As long as the timestamps are within 20 minutes of each other, the\njoin will occur.\n Two additional JoinWindows() methods are available, which you can use to specify\nthe order of events:\nListing 4.13\nUsing the join() method\nExtracts the \nbranched streams\nValueJoiner instance used\nto perform the join\nCalls the join method, \ntriggering automatic \nrepartitioning of \ncoffeeStream and \nelectronicsStream\nConstructs the join\nPrints the join results \nto the console\n \n",
      "content_length": 2487,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "108\nCHAPTER 4\nStreams and state\n\nJoinWindows.after—streamA.join(streamB,...,twentyMinuteWindow.after\n(5000)....) This specifies that the timestamp of the streamB record is at most\n5 seconds after the timestamp of the streamA record. The starting time bound-\nary of the window is unchanged.\n\nJoinWindows.before—streamA.join(streamB,...,twentyMinuteWindow\n.before(5000),...) This specifies that the timestamp of the streamB record is\nat most 5 seconds before the timestamp of the streamA record. The ending time\nboundary of the window is unchanged.\nWith both the before() and after() methods, the time difference is expressed in\nmilliseconds. The timespans used for the join are an example of sliding windows. We’ll\nlook at windowing operations in detail in the next chapter.\nNOTE\nIn listing 4.13, you’re relying on the actual timestamps of the transac-\ntion, not timestamps set by Kafka. In order to use the timestamps embedded\nin the transaction, you specify a custom timestamp extractor by setting Streams-\nConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG to use Transaction-\nTimestampExtractor.class.\nYou’ve now constructed a joined stream: electronics purchases made within 20 min-\nutes of a coffee purchase will result in a coupon for a free drink on the customer’s\nnext visit to ZMart.\n Before we go any further, I’d like to take a minute to explain an important require-\nment for joining data—co-partitioning. \nCO-PARTITIONING\nIn order to perform a join in Kafka Streams, you need to ensure that all join partici-\npants are co-partitioned, meaning that they have the same number of partitions and are\nkeyed by the same type. As a result, when you call the join() method in listing 4.13,\nboth KStream instances will be checked to see if a repartition is required.\nNOTE\nGlobalKTable instances don’t require repartitioning when involved in\na join.\nIn section 4.4.2, you use the selectKey() method on the transactionStream and\nimmediately branched on the returned KStreams. Because the selectKey() method\nmodifies the key, both coffeeStream and electronicsStream require repartitioning.\nIt’s worth repeating that repartitioning is necessary because you need to ensure that\nidentical keys are written to the same partition. This repartitioning is handled auto-\nmatically. Additionally, when you start your Kafka Streams application, topics involved\nin a join are checked to make sure they have the same number of partitions; if any\nmismatches are found, a TopologyBuilderException is thrown. It’s the developer’s\nresponsibility to ensure the keys involved in a join are of the same type.\n Co-partitioning also requires all Kafka producers to use the same partitioning\nclass when writing to Kafka Streams source topics. Likewise, you need to use the same\n \n",
      "content_length": 2758,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "109\nJoining streams for added insight\nStreamPartitioner for any operations writing Kafka Streams sink topics via the\nKStream.to() method. If you stick with the default partitioning strategies, you won’t\nneed to worry about partitioning strategies.\n Let’s continue on with joins and look at the other options available to you. \n4.4.4\nOther join options\nThe join in listing 4.13 is an inner join. With an inner join, if either record isn’t pres-\nent, the join doesn’t occur, and you don’t emit a CorrelatedPurchase object. There\nare other options that don’t require both records. These are useful if you need infor-\nmation even when the desired record for joining isn’t available.\nOUTER JOINS\nOuter joins always output a record, but the forwarded join record may not include\nboth of the events specified by the join. If either side of the join isn’t present when\nthe time window expires, an outer join sends the record that’s available down-\nstream. Of course, if both events are present within the window, the issued record\ncontains both events.\n For example, if you wanted to use an outer join in listing 4.13, you’d do so like this:\ncoffeeStream.outerJoin(electronicsStream,..)\nFigure 4.16 demonstrates the three possible outcomes of the outer join. \nOnly the calling stream’s event is available\nin the time window, so that’s the only record\nincluded.\nBoth streams’ events are available in the\ntime window, so both are included in the\njoin record.\nOnly the other stream’s event is available\nin the time window, so nothing is sent\ndownstream.\nJoin time window\n(Coffee purchase, null)\n(Coffee purchase, electronics purchase)\n(Null, electronics purchase)\nElectronics\npurchase\nCoffee\npurchase\nCoffee\npurchase\nElectronics\npurchase\nFigure 4.16\nThree outcomes are possible with outer joins: only the calling stream’s event, both \nevents, and only the other stream’s event.\n \n",
      "content_length": 1869,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "110\nCHAPTER 4\nStreams and state\nLEFT-OUTER JOIN\nThe records sent downstream from a left-outer join are similar to an outer join, with\none exception. When the only event available in the join window is from the other\nstream, there’s no output at all. If you wanted to use a left-outer join in listing 4.13,\nyou’d do so like this: \ncoffeeStream.leftJoin(electronicsStream..) \nFigure 4.17 shows the outcomes of the left-outer join.\nWe’ve now covered joining streams, but there’s one concept that deserves a more\ndetailed discussion: timestamps and the impact they have on your Kafka Streams\napplication. In the join example, you specified a maximum time difference between\nevents of 20 minutes. In this case, it’s the time between purchases, but how you set or\nextract these timestamps wasn’t specified. Let’s take a closer look at that. \n4.5\nTimestamps in Kafka Streams\nSection 2.4.4 discussed timestamps in Kafka records. In this section, we’ll discuss the\nuse of timestamps in Kafka Streams. Timestamps play a role in key areas of Kafka\nStreams functionality:\nJoining streams\nUpdating a changelog (KTable API)\nDeciding when the Processor.punctuate() method is triggered (Processor API)\nOnly the calling stream’s event is available\nin the time window, so that’s the only record\nincluded.\nBoth streams’ events are available in the\ntime window, so both are included in the\njoin record.\nOnly the other stream’s event is available\nin the time window, so nothing is sent\ndownstream.\nJoin time window\n(Coffee purchase, null)\n(Coffee purchase, electronics purchase)\nNo output\nElectronics\npurchase\nCoffee\npurchase\nCoffee\npurchase\nElectronics\npurchase\nFigure 4.17\nThree outcomes are possible with the left-outer join, but there’s no output if only \nthe other stream’s record is available.\n \n",
      "content_length": 1784,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "111\nTimestamps in Kafka Streams\nWe haven’t covered the KTable or Processor APIs yet, but that’s OK. You don’t need\nthem to understand this section.\n In stream processing, you can group timestamps into three categories, as shown in\nfigure 4.18:\nEvent time—A timestamp set when the event occurred, usually embedded in the\nobject used to represent the event. For our purposes, we’ll consider the time-\nstamp set when the ProducerRecord is created as the event time as well.\nTimestamp embedded in data object at time of event, or\ntimestamp set in ProducerRecord by a Kafka producer\nTimestamp set at time record is appended to log (topic)\nTimestamp generated at the moment when record\nis consumed, ignoring timestamp embedded in data\nobject and ConsumerRecord\nEvent time\nIngest time\nProcessing time\nOr\nValue\nRecord\nTimestamp\nRecord\nValue\nTimestamp\nValue\nRecord\nTimestamp\nTimestamp generated\nwhen record is consumed\n(wall-clock time)\nTimestamp\nKafka producer\nKafka broker\nKafka Streams\nSome event\ntimestamp\nFigure 4.18\nThere are three categories of timestamps in Kafka Streams: event \ntime, ingestion time, and processing time.\n \n",
      "content_length": 1125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "112\nCHAPTER 4\nStreams and state\nIngestion time—A timestamp set when the data first enters the data processing\npipeline. You can consider the timestamp set by the Kafka broker (assuming a\nconfiguration setting of LogAppendTime) to be ingestion time.\nProcessing time—A timestamp set when the data or event record first starts to flow\nthrough a processing pipeline.\nYou’ll see in this section how the Kafka Streams API supports all three types of process-\ning timestamps.\nNOTE\nSo far, we’ve had an implicit assumption that clients and brokers are\nlocated in the same time zone, but that might not always be the case. When\nusing timestamps, it’s safest to normalize the times using the UTC time zone,\neliminating any confusion over which brokers and clients are using which\ntime zones.\nWe’ll consider three cases of timestamp-processing semantics:\nA timestamp embedded in the actual event or message object (event-time\nsemantics)\nUsing the timestamp set in the record metadata when creating the ProducerRe-\ncord (event-time semantics)\nUsing the current timestamp (current local time) when the Kafka Streams\napplication ingests the record (processing-time semantics)\nFor event-time semantics, using the timestamp placed in the metadata by the Producer-\nRecord is sufficient. But there may be cases when you have different needs. Consider\nthese examples:\nYou’re sending messages to Kafka with events that have timestamps recorded in\nthe message objects. There’s some lag time in when these event objects are\nmade available to the Kafka producer, so you want to consider only the embed-\nded timestamp.\nYou want to consider the time when your Kafka Streams application consumes\nrecords as opposed to using the timestamps of the records.\nTo enable different processing semantics, Kafka Stream provides a TimestampExtractor\ninterface with one abstract and four concrete implementations. If you need to work\nwith timestamps embedded in the record values, you’ll need to create a custom Time-\nstampExtractor implementation. Let’s briefly look at the included implementations\nand implement a custom TimestampExtractor.\n4.5.1\nProvided TimestampExtractor implementations\nAlmost all of the provided TimestampExtractor implementations work with time-\nstamps set by the producer or broker in the message metadata, thus providing either\nevent-time processing semantics (timestamp set by the producer) or log-append-time\n \n",
      "content_length": 2410,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "113\nTimestamps in Kafka Streams\nprocessing semantics (timestamp set by the broker). Figure 4.19 demonstrates pulling\nthe timestamp from the ConsumerRecord object.\nAlthough you’re assuming the default configuration setting of CreateTime for the\ntimestamp, bear in mind that if you were to use LogAppendTime, this would return\nthe timestamp value for when the Kafka broker appended the record to the log.\nExtractRecordMetadataTimestamp is an abstract class that provides the core function-\nality for extracting the metadata timestamp from the ConsumerRecord. Most of the\nconcrete implementations extend this class. Implementors override the abstract\nmethod, ExtractRecordMetadataTimestamp.onInvalidTimestamp, to handle invalid\ntimestamps (when the timestamp is less than 0).\n Here’s a list of classes that extend the ExtractRecordMetadataTimestamp class:\n\nFailOnInvalidTimestamp—Throws an exception in the case of an invalid time-\nstamp.\n\nLogAndSkipOnInvalidTimestamp—Returns the invalid timestamp and logs a\nwarning message that the record will be discarded due to the invalid timestamp.\n\nUsePreviousTimeOnInvalidTimestamp—In the case of an invalid timestamp,\nthe last valid extracted timestamp is returned.\nWe’ve covered the event-time timestamp extractors, but there’s one more provided\ntimestamp extractor to cover. \n4.5.2\nWallclockTimestampExtractor\nWallclockTimestampExtractor provides process-time semantics and doesn’t extract\nany timestamps. Instead, it returns the time in milliseconds by calling the System\n.currentTimeMillis() method.\n That’s it for the provided timestamp extractors. Next, we’ll look at how you can\ncreate a custom version. \nTimestamp\nConsumer timestamp extractor\nretrieves timestamp set by\nKafka producer or broker\nDotted rectangle represents\nConsumerRecord metadata\nKey\nValue\nEntire enclosing rectangle represents\na ConsumerRecord object\nFigure 4.19\nTimestamps in the ConsumerRecord object: either the \nproducer or broker set this timestamp, depending on your configuration.\n \n",
      "content_length": 2010,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "114\nCHAPTER 4\nStreams and state\n4.5.3\nCustom TimestampExtractor\nTo work with timestamps (or calculate one) in the value object from the Consumer-\nRecord, you’ll need a custom extractor that implements the TimestampExtractor\ninterface. Figure 4.20 depicts using the timestamp embedded in the value object ver-\nsus one set by Kafka (either producer or broker).\nHere’s an example of a TimestampExtractor implementation (found in src/main/\njava/bbejeck/chapter_4/timestamp_extractor/TransactionTimestampExtractor.java),\nalso used in the join example from listing 4.12 in the section “Implementing the Join”\n(although not shown in the text, because it’s a configuration parameter).\npublic class TransactionTimestampExtractor implements TimestampExtractor {\n@Override\npublic long extract(ConsumerRecord<Object, Object> record,\n➥ long previousTimestamp) {\nPurchase purchaseTransaction = (Purchase) record.value();   \nreturn purchaseTransaction.getPurchaseDate().getTime();   \n}\n}\nIn the join example, you used a custom TimestampExtractor because you wanted to\nuse the timestamps of the actual purchase time. This approach allows you to join the\nrecords even if there are delays in delivery or out-of-order arrivals.\nListing 4.14\nCustom TimestampExtractor\nTimestamp\nConsumerRecord metadata\nCustom TimestampExtractor knows where\nto pull the timestamp from the value in a\nConsumerRecord object\nKey\nValue\nEntire enclosing\nrectangle represents a\nConsumerRecord object\nRecord in JSON format\n{ “recordType” = “purchase”,\n“amount” = 500.00,\n“timestamp” = 1502041889179 }\nFigure 4.20\nA custom TimestampExtractor provides a timestamp based \non the value contained in the ConsumerRecord. This timestamp could be an \nexisting value or one calculated from properties contained in the value object.\nRetrieves the Purchase object from\nthe key/value pair sent to Kafka\nReturns the timestamp\nrecorded at the point of sale\n \n",
      "content_length": 1900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "115\nSummary\nWARNING\nWhen you create a custom TimestampExtractor, take care not to\nget too clever. Log retention and log rolling are timestamp based, and the\ntimestamp provided by the extractor may become the message timestamp\nused by changelogs and output topics downstream. \n4.5.4\nSpecifying a TimestampExtractor\nNow that we’ve discussed how timestamp extractors work, let’s tell the application\nwhich one to use. You have two choices for specifying timestamp extractors.\n The first option is to set a global timestamp extractor, specified in the properties\nwhen setting up your Kafka Streams application. If no property is set, the default set-\nting is FailOnInvalidTimestamp.class. For example, the following code would con-\nfigure the TransactionTimestampExtractor via properties when setting up the\napplication:\nprops.put(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG,\n➥ TransactionTimestampExtractor.class);\nThe second option is to provide a TimestampExtractor instance via a Consumed object:\nConsumed.with(Serdes.String(), purchaseSerde)\n.withTimestampExtractor(new TransactionTimestampExtractor()))\nThe advantage of doing this is that you have one TimestampExtractor per input\nsource, whereas the other option requires you to handle records from different topics\nin one TimestampExtractor instance.\n We’ve come to the end of our discussion of timestamp usage. In the coming chap-\nters, you’ll run into situations where the difference between timestamps drives some\naction, such as flushing the cache of a KTable. I don’t expect you remember all three\ntypes of timestamp extractors, but it’s vital to understand that timestamps are an\nimportant part of how Kafka and Kafka Streams function. \nSummary\nStream processing needs state. Sometimes events can stand on their own, but\nusually you’ll need additional information to make good decisions.\nKafka Streams provides useful abstractions for stateful transformations, includ-\ning joins.\nState stores in Kafka Streams provide the type of state required for stream pro-\ncessing: data locality and fault tolerance.\nTimestamps control the flow of data in Kafka Streams. The choice of timestamp\nsources needs careful consideration.\nIn the next chapter, we’ll continue exploring state in streams with more-significant\noperations, like aggregations and grouping. We’ll also explore the KTable API. Whereas\n \n",
      "content_length": 2369,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "116\nCHAPTER 4\nStreams and state\nthe KStream API concerns itself with individual discrete records, a KTable is an imple-\nmentation of a changelog, where records with the same key are considered updates.\nWe’ll also discuss joins between KStream and KTable instances. Finally, we’ll explore\none of the most exciting developments in Kafka Streams: queryable state. Queryable\nstate allows you to directly observe the state of your stream, without having to material-\nize the information by reading data from a topic in an external application.\n \n",
      "content_length": 541,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "117\nThe KTable API\nSo far, we’ve covered the KStream API and adding state to a Kafka Streams applica-\ntion. In this chapter, we’re going to look deeper into adding state. Along the way,\nyou’ll be introduced to a new abstraction, the KTable.\n In discussing the KStream API, we’ve talked about individual events or an event\nstream. In the original ZMart example, when Jane Doe made a purchase, you con-\nsidered the purchase to be an individual event. You didn’t keep track of how many\npurchases Jane made, or how often. In the context of a database, the purchase\nevent stream could be considered a series of inserts. Because each record is new\nand unrelated to any other record, you could continually insert them into a table.\nThis chapter covers\nDefining the relationship between streams \nand tables\nUpdating records, and the KTable abstraction\nAggregations, and windowing and joining \nKStreams and KTables\nGlobal KTables\nQueryable state stores\n \n",
      "content_length": 951,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "118\nCHAPTER 5\nThe KTable API\n Now let’s add a primary key (customer ID) to each purchase event. You have a\nseries of related purchase events or updates for Jane Doe. Because you’re using a pri-\nmary key, each purchase is updated with respect to Jane’s purchase activity. Treating\nan event stream as inserts, and events with keys as updates, is how you’ll define the\nrelationship between streams and tables.\n In this chapter, we’ll cover the relationship between streams and tables in more\ndepth. This relationship is important, as it will help you understand how the KTable\noperates.\n Second, we’ll discuss the KTable. The KTable API is necessary because it’s designed\nto work with updates to records. We need the ability to work with updates for opera-\ntions like aggregations and counts. We touched on updates in chapter 4 when intro-\nducing stateful transformations; in section 4.2.5, you updated the rewards processor\nto keep track of customer purchases.\n Third, we’ll get into windowing operations. Windowing is the process of bucketing\ndata for a given period. For example, how many purchases have there been over the\npast hour, updated every ten minutes? Windowing allows you to gather data in\nchunks, as opposed to having an unbounded collection.\nNOTE\nWindowing and bucketing are somewhat synonymous terms. Both oper-\nate by placing information into smaller chunks or categories. Windowing\nimplies categorizing by time, but the result of either operation is the same.\nOur final topic in this chapter will be queryable state stores. Queryable state stores are\nan exciting feature of Kafka Streams: they allow you to run direct queries against state\nstores. In other words, you can view stateful data without having to consume it from a\nKafka topic or read it from a database. Let’s move on to our first topic.\n5.1\nThe relationship between streams and tables\nIn chapter 1, I defined a stream as an infinite sequence of events. That wording is\npretty generic, so let’s narrow it down with a specific example.\n5.1.1\nThe record stream\nSuppose you want to view a series of stock price updates. You can recast the generic\nmarble diagram from chapter 1 to look like figure 5.1. You can see that each stock\nprice quote is a discrete event, and they aren’t related to each other. Even if the same\ncompany accounts for many price quotes, you’re only looking at them one at a time.\nThis view of events is how the KStream works—it’s a stream of records.\n Now, let’s see how this concept ties into database tables. Look at the simple stock\nquote table in figure 5.2.\nNOTE\nTo keep our discussion straightforward, we’ll assume a key to be a sin-\ngular value.\n \n",
      "content_length": 2653,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "119\nThe relationship between streams and tables\nNext, let’s take another look at the record stream. Because each record stands on its\nown, the stream represents inserts into a table. Figure 5.3 combines the two concepts\nto illustrate this point.\n What’s important here is that you can view a stream of events in the same light as\ninserts into a table, which can help give you a deeper understanding of using streams\nfor working with events. The next step is to consider the case where events in the\nstream are related to one another. \n5.1.2\nUpdates to records or the changelog\nLet’s take the same stream of customer transactions, but now track activity over time.\nIf you add a key of customer ID, the purchase events can be related to each other, and\nyou’ll have an update stream as opposed to an event stream.\n If you consider the stream of events as a log, you can consider this stream of\nupdates as a changelog. Figure 5.4 demonstrates this concept.\nCompany AAVF\nAmount $100.57\nTS 12:14:35 1/20/17\nCompany APPL\nAmount $203.77\nTS 12:15:57 1/20/17\nCompany FRLS\nAmount $40.27\nTS 12:18:41 1/20/17\nCompany AMEX\nAmount $57.17\nTS 12:20:38 1/20/17\nImagine that you are observing a stock ticker displaying updated share prices in real time.\nEach circle on the line represents a publicly traded stock’s share price adjusting to\nmarket forces.\nTime\nFigure 5.1\nA marble diagram for an unbounded stream of stock quotes\nThe rows from table above can be recast as key/value pairs.\nFor example, the ﬁrst row in the table can be converted\nto this key/value pair:\n{key:{stockid: 235588}, value:{ts:32225544289, price: 05.36}}\n1\n1\nABVF\n105.36\n32225544289\nAPPL\n333.66\n32225544254\nStock_ID\nShare_Price\nTimestamp\nKey\nValue\nFigure 5.2\nA simple database \ntable represents stock prices for \ncompanies. There’s a key column, \nand the other columns contain \nvalues. You can consider this a \nkey/value pair if you lump the \nother columns into a “value” \ncontainer.\n \n",
      "content_length": 1942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "120\nCHAPTER 5\nThe KTable API\nStock_ID AMEX\nShare $105.36\nTS 148907726274\nStock_ID RLPX\nShare $203.77\nTS 148907726589\nStock_ID AMEX\nAmount $107.05\nTS 1489077288531\nStock_ID RLPX\nAmount $201.57\nTS 1148907736628\nThis shows the relationship between events and inserts into a database. Even though it’s\nstock prices for two companies, it counts as four events because we’re considering\neach item on the stream as a singular event.\nAs a result, each event is an insert, and we increment the key by one for each insert into the table.\nWith that in mind, each event is a new, independent record or insert into a database table.\nAMEX\n105.36\n148907726274\nStock_ID\nShare_Price\nTimestamp\nRLPX\n203.77\n148907726589\nAMEX\n107.05\n1489077288531\nRLPX\n201.57\n148907736628\n1\nKey\n2\n3\n4\nFigure 5.3\nA stream of individual events compares to inserts into a database table. You could \nsimilarly imagine streaming each row from the table.\nThe previous records\nfor these stocks have\nbeen overwritten with\nupdates.\nLatest records from\nevent stream\nIf you use the stock ID as a primary key, subsequent events with the same key are updates\nin a changelog. In this case, you only have two records, one per company. Although more\nrecords can arrive for the same companies, the records won’t accumulate.\nAMEX\n105.36\n148907726274\nStock_ID\nShare_Price\nTimestamp\nRLPX\n203.77\n148907726589\nAMEX\n107.05\n1489077288531\nRLPX\n201.57\n148907736628\nStock_ID AMEX\nShare $105.36\nTS 148907726274\nStock_ID RLPX\nShare $203.77\nTS 148907726589\nStock_ID AMEX\nAmount $107.05\nTS 1489077288531\nStock_ID RLPX\nAmount $201.57\nTS 1148907736628\nFigure 5.4\nIn a changelog, each incoming record overwrites the previous one with the same key. \nWith a record stream, you’d have a total of four events, but in the case of updates or a changelog, you \nhave only two.\n \n",
      "content_length": 1800,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "121\nThe relationship between streams and tables\nHere, you can see the relationship between a stream of updates and a database table.\nBoth a log and a changelog represent incoming records appended to the end of a file.\nIn a log, you see all the records; but in a changelog, you only keep the latest record for\nany given key.\nNOTE\nWith both a log and a changelog, records are appended to the end of\nthe file as they come in. The distinction between the two is that in a log, you\nwant to see all records, but in a changelog, you only want the latest record for\neach key.\nTo trim a log while maintaining the latest records per key, you can use log compac-\ntion, which we discussed in chapter 2. You can see the impact of compacting a log in\nfigure 5.5. Because you only care about the latest values, you can remove older\nkey/value pairs.1\nYou’re already familiar with event streams from working with KStreams. For a chan-\ngelog or stream of updates, we’ll use an abstraction known as the KTable. Now that\nwe’ve established the relationship between streams and tables, the next step is to com-\npare an event stream to an update stream. \n1 This section derived information from Jay Kreps’s “Introducing Kafka Streams: Stream Processing Made Sim-\nple” (http://mng.bz/49HO) and “The Log: What Every Software Engineer Should Know About Real-time\nData’s Unifying Abstraction” (http://mng.bz/eE3w).\nBefore compaction\nAfter compaction\nOffset\nValue\nKey\nOffset\nValue\nKey\n10\nA\nfoo\n11\nB\nbar\n12\nC\nbaz\n13\nD\nfoo\n13\nD\nfoo\n14\nE\nbaz\n15\nF\nboo\n16\nG\nfoo\n17\nH\nbaz\n11\nB\nbar\n15\nF\nboo\n16\nG\nfoo\n17\nH\nbaz\nFigure 5.5\nOn the left is a log before compaction—you’ll notice duplicate keys with different \nvalues, which are updates. On the right is the log after compaction—you keep the latest value for \neach key, but the log is smaller in size.\n \n",
      "content_length": 1812,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "122\nCHAPTER 5\nThe KTable API\n5.1.3\nEvent streams vs. update streams\nWe’ll use the KStream and the KTable to drive our comparison of event streams versus\nupdate streams. We’ll do this by running a simple stock ticker application that writes\nthe current share price for three (fictitious!) companies. It will produce three itera-\ntions of stock quotes for a total of nine records. A KStream and a KTable will read the\nrecords and write them to the console via the print() method.\n Figure 5.6 shows the results of running the application. As you can see, the\nKStream printed all nine records. We’d expect the KStream to behave this way because\nit views each record individually. In contrast, the KTable printed only three records,\nbecause the KTable views records as updates to previous ones.\nNOTE\nFigure 5.6 demonstrates how a KTable works with updates. I made an\nimplicit assumption that I’ll make explicit here. When working with a KTable,\nyour records must have populated keys in the key/value pairs. Having a key is\nessential for the KTable to work, just as you can’t update a record in a data-\nbase table without having the key.\nFrom the KTable’s point of view, it didn’t receive nine individual records. The KTable\nreceived three original records and two rounds of updates, and it only printed the last\nHere are all three\nevents/records\nfor the KStream.\nHere is the last update\nrecord for the KTable.\nAs expected, the values for the last KStream\nevent and KTable update are the same.\nA simple stock ticker for three ﬁctitious companies with a data generator producing\nthree updates for the stocks. The KStream printed all records as they were received.\nThe KTable only printed the last batch of records because they were the latest\nupdates for the given stock symbol.\nFigure 5.6\nKTable versus KStream printing messages with the same keys\n \n",
      "content_length": 1844,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "123\nRecord updates and KTable configuration\nround of updates. Notice that the KTable records are the same as the last three\nrecords published by the KStream. We’ll discuss the mechanisms of how the KTable\nemits only the updates in the next section.\n Here’s the program for printing stock ticker results to the console (found in\nsrc/main/java/bbejeck/chapter_5/KStreamVsKTableExample.java; source code can\nbe found on the book’s website here: https://manning.com/books/kafka-streams-in-\naction).\nKTable<String, StockTickerData> stockTickerTable =\n➥ builder.table(STOCK_TICKER_TABLE_TOPIC);  \nKStream<String, StockTickerData> stockTickerStream =\n➥ builder.stream(STOCK_TICKER_STREAM_TOPIC);   \nstockTickerTable.toStream()\n➥ .print(Printed.<String, StockTickerData>toSysOut()\n➥ .withLabel(\"Stocks-KTable\"));                        \nstockTickerStream\n➥ .print(Printed.<String, StockTickerData>toSysOut()\n➥ .withLabel(\"Stocks-KStream\"));                       \nThe takeaway here is that records in a stream with the same keys are updates, not new\nrecords in themselves. A stream of updates is the main concept behind the KTable.\n You’ve now seen the KTable in action, so let’s discuss the mechanisms behind its\nfunctionality. \n5.2\nRecord updates and KTable configuration\nTo figure out how the KTable functions, we should ask the following two questions:\nWhere are records stored?\nHow does a KTable make the determination to emit records?\nListing 5.1\nKTable and KStream printing to the console\nUsing default serdes\nIn creating the KTable and KStream, you didn’t specify any serdes to use. The same\nis true with both calls to the print() method. You were able to do this because you\nregistered a default serdes in the configuration. like so:\nprops.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,\n➥ Serdes.String().getClass().getName());\nprops.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,\n➥ StreamsSerdes.StockTickerSerde().getClass().getName());\nIf you used different types, you’d need to provide serdes in the overloaded methods\nfor reading or writing records.\nCreates the \nKTable instance\nCreates the \nKStream instance\nKTable prints results \nto the console\nKStream prints results \nto the console\n \n",
      "content_length": 2204,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "124\nCHAPTER 5\nThe KTable API\nAs we get into aggregation and reducing operations, the answers to these questions are\nnecessary. For example, when performing an aggregation, you’ll want to see updated\ncounts, but you probably won’t want an update each time the count increments by one.\n To answer the first question, let’s look at the line that creates the KTable:\nbuilder.table(STOCK_TICKER_TABLE_TOPIC);\nWith this simple statement, the StreamsBuilder creates a KTable instance and simul-\ntaneously, under the covers, creates a StateStore for tracking the state of stream, thus\ncreating an update stream. The StateStore created by this approach has an internal\nname and won’t be available for interactive queries.\n There’s an overloaded version of StreamsBuilder.table that accepts a Material-\nized instance, allowing you to customize the type of store and provide a name to\nmake it available for querying. We’ll discuss interactive queries later in this chapter.\n That gives us the answer to our first question: the KTable uses the local state inte-\ngrated with Kafka Streams for storage. (We covered state stores in section 4.3.)\n Now let’s move on to the next question: what determines when the KTable emits\nupdates to downstream processors? To answer this question, we need to consider a few\nfactors:\nThe number of records flowing into the application. Higher data rates will tend\nto increase the rate of emitting updated records.\nHow many distinct keys are in the data. Again, a greater number of distinct keys\nmay lead to more updates being sent downstream.\nThe configuration parameters cache.max.bytes.buffering and commit\n.interval.ms.\nFrom this list, we’ll only cover what we can control: configuration parameters. First,\nlet’s look at the cache.max.bytes.buffering configuration.\n5.2.1\nSetting cache buffering size\nThe KTable cache serves to deduplicate updates to records with the same key. This\ndeduplication allows child nodes to receive only the most recent update instead of all\nupdates, reducing the amount of data processed. Additionally, only the most recent\nupdate is placed in the state store, which can amount to significant performance\nimprovements when using persistent state stores.\n Figure 5.7 illustrates the cache operation. As you can see, with caching enabled,\nnot all the record updates get forwarded downstream. The cache keeps only the latest\nrecord for any given key.\nNOTE\nA Kafka Streams application is a topology or graph of connected nodes\n(processors). Any given node may have one or more child nodes, unless it’s a\nterminal processor. Once a processor has finished working with a record, it\nforwards the record “downstream” to its child nodes.\n \n",
      "content_length": 2686,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "125\nRecord updates and KTable configuration\nBecause the KTable represents a changelog of events in a stream, you’ll expect to\nwork with only the latest update at any given point. Using the cache enforces this\nbehavior. If you wanted to process all records in the stream, you’d use an event stream,\nthe KStream, covered earlier.\n A larger cache will reduce the number of updates emitted. Additionally, caching\nreduces the amount of data written to disk by persistent stores (RocksDB), and if log-\nging is enabled, the number of records sent to the changelog topic for any store.\n Cache size is controlled by the cache.max.bytes.buffering setting, which speci-\nfies the amount of memory allocated for the record cache. The amount of memory\nspecified is divided evenly across the number of stream threads. (The number of\nstream threads is specified by the StreamsConfig.NUM_STREAM_THREADS_CONFIG set-\nting, with 1 being the default.)\nWARNING\nTo turn off caching, you can set cache.max.bytes.buffering to 0.\nBut this setting will result in every KTable update being sent downstream,\neffectively turning your changelog stream into an event stream. Also, no cach-\ning means more I/O, as persistent stores will now write each update to disk\ninstead of only writing the latest update. \n5.2.2\nSetting the commit interval\nThe other setting is the commit.interval.ms parameter. The commit interval speci-\nfies how often (in milliseconds) the state of a processor should be saved. When the\nstate of the processor is saved (committing), it forces a cache flush, sending the latest\nupdated, deduplicated records downstream.\n In the full caching workflow (figure 5.8), you can see two forces at play when it\ncomes to sending records downstream. Either a commit or the cache reaching its\nmaximum size will send records downstream. Conversely, disabling the cache will send\nall records downstream, including duplicate keys. Generally speaking, it’s best to have\ncaching enabled when using a KTable.\n33.56\nNDLE\n105.36\nYERB\n105.24\nYERB\n105.36\nYERB\nIncoming stock ticker record\nCache\nAs records come in, they are also\nplaced in the cache, with new records\nreplacing older ones.\nFigure 5.7\nKTable caching deduplicates updates \nto records with the same key, preventing a flood of \nconsecutive updates to child nodes of the KTable \nin the topology.\n \n",
      "content_length": 2328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "126\nCHAPTER 5\nThe KTable API\nAs you can see, we need to strike a balance between cache size and commit time. A\nlarge cache with a small commit time will still result in frequent updates. A longer\ncommit interval could lead to fewer updates (depending on the memory settings)\nbecause cache evictions occur to free-up space. There are no hard rules here—only\ntrial and error will determine what works best for you. It’s best to start with the default\nvalues of 30 seconds (commit time) and 10 MB (cache size). The key thing to remem-\nber is that the rate of updated records sent from a KTable is configurable.\n Next, let’s take a look at how you can use the KTable in your applications. \n5.3\nAggregations and windowing operations\nIn this section, we’ll move on to cover some of the most potent parts of Kafka Streams.\nSo far, we’ve looked at several aspects of Kafka Streams:\nHow to set up a processing topology\nHow to use state in your streaming application\nHow to perform joins between streams\nThe difference between an event stream (KStream) and an update stream\n(KTable)\nBoth records with the key of\nYERB were stored ﬁrst and\nthen forwarded.\nWith caching disabled (setting cache.max.bytes.buffering= 0),\nincoming updates are immediately written to the state store\nand sent downstream.\nDue to caching, this update was never\nstored or sent downstream; it was\nautomatically deduped by the cache.\nEither when the commit time or the max-cache size is reached,\nrecords are ﬂushed from the cache and then written to state\nstore and forwarded downstream. Notice all records are not\nstored or forwarded due to deduping by the cache.\nIncoming stock ticker record\n105.36\nYERB\nCaching enabled?\nYes\n105.36\nYERB\n33.56\nNDLE\n105.36\nYERB\n33.56\nNDLE\n105.36\nYERB\n105.24\nYERB\n33.56\nNDLE\n105.36\nYERB\n105.24\nYERB\nNo\n33.56\nNDLE\n105.36\nYERB\nState store\nFigure 5.8\nFull caching workflow: if caching is enabled, records are deduped and sent downstream \non cache flush or commit.\n \n",
      "content_length": 1961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "127\nAggregations and windowing operations\nIn the examples that follow, we’ll tie all these elements together. Additionally, I’ll\nintroduce windowing, another powerful tool in streaming applications. The first exam-\nple is a straightforward aggregation.\n5.3.1\nAggregating share volume by industry\nAggregation and grouping are necessary tools when you’re working with streaming\ndata. Reviewing single records as they arrive is often not enough. To gain any insight,\nyou’ll need grouping and combining of some sort.\n In this example, you’ll take on the role of being a day trader, and you’ll track the\nshare volume of companies across a list of selected industries. In particular, you’re\ninterested in the top five companies (by share volume) in each industry.\n To do this aggregation, a few steps will be required to set up the data in the correct\nformat. At a high level, these are the steps:\n1\nCreate a source from a topic publishing raw stock-trade information.\nYou’ll need to\nmap a StockTransaction object into a ShareVolume object. The reason for per-\nforming this mapping step is simple: the StockTransaction object contains\nmetadata about the trade, but you only want the volume of shares involved in\nthe trade.\n2\nGroup ShareVolume by its ticker symbol.\nOnce it’s grouped by symbol, you can\nreduce it to a rolling total of share volume. I should note here that calling\nKStream.groupBy returns a KGroupedStream instance. Then, calling KGrouped-\nStream.reduce is what will get you to a KTable instance.\nLet’s pause for a minute and look at figure 5.9, which shows what you’ve built so far.\nThis topology is something you’re familiar with by now.\n Now, let’s look at the code behind the topology (found in src/main/java/bbejeck/\nchapter_5/AggregationsAndReducingExample.java).\n \n \n \nWhat is the KGroupedStream?\nWhen you use KStream.groupBy or KStream.groupByKey, the returned instance is\na KGroupedStream. The KGroupedStream is an intermediate representation of the\nevent stream after grouping by keys and is never meant for you to work with directly.\nInstead, the KGroupedStream is required to perform aggregation operations, which\nalways result in a KTable. Because the aggregate operations produce a KTable and\nuse a state store, not all updates may end up being forwarded downstream.\nThere’s an analogous KGroupedTable resulting from the KTable.groupBy method,\nwhich is the intermediate representation of the update stream regrouped by key.\n \n",
      "content_length": 2449,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "128\nCHAPTER 5\nThe KTable API\nKTable<String, ShareVolume> shareVolume =\n➥ builder.stream(STOCK_TRANSACTIONS_TOPIC,\nConsumed.with(stringSerde, stockTransactionSerde)\n➥ .withOffsetResetPolicy(EARLIEST))\n                           \n➥ .mapValues(st -> ShareVolume.newBuilder(st).build())              \n➥ .groupBy((k, v) -> v.getSymbol(),\nSerialized.with(stringSerde, shareVolumeSerde))  \n➥ .reduce(ShareVolume::reduce);  \nThis code is concise and squeezes a lot of power into a few lines. If you look at the first\nparameter of the builder.stream method, you’ll see something new: the AutoOffset-\nReset.EARLIEST enum (there’s also a LATEST) that you set with the Consumed\n.withOffsetResetPolicy method. This enum allows you to specify the offset-reset\nstrategy for each KStream or KTable. Using the enum takes precedence over the off-\nset-reset setting in the streams configuration.\nListing 5.2\nSource for the map-reduce of stock transactions\nKTable<String, ShareVolume>\nMapValues\nprocessor\nReducing\nprocessor\nstock-\ntransactions\ntopic\nSource\nprocessor\nGroup-by\nprocessor\nReduces ShareVolume objects\ndown to a single ShareVolume\ninstance with a rolling update\nof total share volume. The ﬁnal\nresult of this process is a\nKTable<String, ShareVolume>\ninstance.\nMaps StockTransaction\nobjects to ShareVolume objects\nConsumes from\nstock-transactions topic\nGroups ShareVolume\nobjects by stock\nticker symbol\nFigure 5.9\nMapping and reducing StockTransaction objects into ShareVolume objects and \nthen reducing to a rolling total\nThe source processor\nconsumes from a topic.\nMaps StockTransaction \nobjects to ShareVolume \nobjects\nGroups the\nShareVolume\nobjects by their\nstock ticker symbol\nReduces \nShareVolume \nobjects to contain \na rolling aggregate \nof share volume\n \n",
      "content_length": 1754,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "129\nAggregations and windowing operations\nIt’s clear what mapValues and groupBy are doing, but let’s look into the sum() method\n(found in src/main/java/bbejeck/model/ShareVolume.java).\npublic static ShareVolume sum(ShareVolume csv1, ShareVolume csv2) {\nBuilder builder = newBuilder(csv1);                      \nbuilder.shares = csv1.shares + csv2.shares;         \nreturn builder.build();           \n}\nNOTE\nYou’ve seen the builder pattern in use earlier in the book, but it’s used\nhere in a somewhat different context. In this example, you’re using the\nbuilder to make a copy of an object and update a field without modifying the\noriginal object.\nThe ShareVolume.sum method gives you the rolling total of share volume, and the\noutcome of the entire processing chain is a KTable<String, ShareVolume> object.\nNow, you can see the role of the KTable. As ShareVolume objects come in, the associ-\nated KTable keeps the most recent update. It’s important to remember that every\nupdate is reflected in the preceding shareVolumeKTable, but each update is not nec-\nessarily emitted.\nNOTE\nWhy do a reduce instead of an aggregation? Although reducing is a\nform of aggregation, a reduce operation will yield the same type of object. An\naggregation also sums results, but it could return a different type of object.\nNext, you’ll take the KTable and use it to perform a top-five aggregation (by share vol-\nume) summary. The steps you’ll take here are similar to the steps for the first aggregation:\n1\nPerform another groupBy operation to group the individual ShareVolume objects\nby industry.\nGroupByKey vs. GroupBy\nKStream has two methods for grouping records: GroupByKey and GroupBy. Both\nreturn a KGroupedTable, so you might wonder what the difference is and when you\nshould use which one.\nThe GroupByKey method is for when your KStream already has non-null keys. More\nimportantly, the “needs repartitioning” flag is never set.\nThe GroupBy method assumes you’ve modified the key for the grouping, so the repar-\ntition flag is set to true. After calling GroupBy, joins, aggregations, and the like will\nresult in automatic repartitioning.\nThe bottom line is that you should prefer GroupByKey over GroupBy whenever possible.\nListing 5.3\nThe ShareVolume.sum method\nUses a Builder \nfor a copy \nconstructor\nSets the number of\nshares to the total of\nboth ShareVolume\nobjects\nCalls build and returns a\nnew ShareVolume object\n \n",
      "content_length": 2406,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "130\nCHAPTER 5\nThe KTable API\n2\nStart to add the ShareVolume objects. This time, the aggregation object is a fixed-\nsize priority queue. The fixed-size queue keeps only the top-five companies by\nshare volume.\n3\nMap the queue into a string, reporting the top-five stocks per industry by share\nvolume.\n4\nWrite out the string result to a topic.\nFigure 5.10 shows a topology graph of the data flow. As you can see, this second round\nof processing is straightforward.\nNow that you have a clear picture of the structure of this second round of processing,\nit’s time to look at the source code (found in src/main/java/bbejeck/chapter_5/\nAggregationsAndReducingExample.java).\nComparator<ShareVolume> comparator =\n➥ (sv1, sv2) -> sv2.getShares() - sv1.getShares()\nFixedSizePriorityQueue<ShareVolume> fixedQueue =\n➥ new FixedSizePriorityQueue<>(comparator, 5);\nshareVolume.groupBy((k, v) -> KeyValue.pair(v.getIndustry(), v),\n➥ Serialized.with(stringSerde, shareVolumeSerde))               \n.aggregate(() -> fixedQueue,\n         \n(k, v, agg) -> agg.add(v),\n       \n(k, v, agg) -> agg.remove(v),\n     \nMaterialized.with(stringSerde, fixedSizePriorityQueueSerde))  \nListing 5.4\nKTable groupBy and aggregation\nAggregate\nprocessor\nGroup-by\nprocessor\nKTable<String, ShareVolume>\nThe grouped records are\naggregated into a Top-N\nqueue—in this case N=5.\nThe queue contents are\nmapped to a string\nin this format.\n) YERB:2 7,934\n1\n1\n2) OCHK: 47,074, and so on\n1\nGroups the reduced\nShareVolume objects\nby industry\nResults are\nwritten out to\na topic.\nMapValues\nprocessor\nTo topic\nFigure 5.10\nTopology for grouping by industry, aggregating by top five, mapping the top-five queue to \na string, and writing out the string to a topic\nGroups by industry and\nprovides required serdes\nThe Aggregate initializer is\nan instance of the\nFixedSizePriorityQueue\nclass (for demonstration\npurposes only!).\nAggregate\nadder adds\nnew updates\nAggregate remover \nremoves old updates\nSerde for the aggregator\n \n",
      "content_length": 1968,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "131\nAggregations and windowing operations\n.mapValues(valueMapper)                                    \n.toStream().peek((k, v) ->\n➥ LOG.info(\"Stock volume by industry {} {}\", k, v))         \n.to(\"stock-volume-by-company\", Produced.with(stringSerde,\n➥ stringSerde));      \nIn this initializer, there’s a fixedQueue variable. This is a custom object that wraps a\njava.util.TreeSet, which is used to keep track of the top N results in decreasing\norder of share volume.\n You’ve seen the groupBy and mapValues calls before, so we won’t go over them\nagain (you call the KTable.toStream method, as KTable.print is deprecated). But\nyou haven’t seen the KTable version of aggregate before, so let’s take a minute to dis-\ncuss it.\n As you’ll recall, what makes a KTable unique is that records with the same key are\nupdates. The KTable replaces the old record with the new one. Aggregation works in\nthe same manner. It aggregates the most recent records with the same key. As a record\narrives, you add it to the FixedSizePriorityQueue using the adder method (the\nsecond parameter in the aggregate call), but if another record exists with the same\nkey, you remove the old record with the subtractor (the third parameter in the\naggregate call).\n What this means is that your aggregator, FixedSizePriorityQueue, doesn’t aggre-\ngate all values with the same key. Instead, it keeps a running tally of the top N stocks\nthat have the largest volume. Each record coming in has the total volume of shares\ntraded so far. Your KTable will show you which companies have the top number of\nshares traded at the moment; you don’t want a running aggregation of each update.\n You’ve now learned how to do two important things:\nGroup values in a KTable by a common key\nPerform useful operations like reducing and aggregation with those grouped\nvalues\nThe ability to perform these operations is important when you’re trying to make sense\nof your data, or to determine what your data is telling you, as it flows through your\nKafka Streams application.\n We’ve also brought together some of the key concepts discussed earlier in the\nbook. In chapter 4, you learned about the importance of having fault-tolerant, local\nstate for streaming applications. The first example in this chapter showed why local\nstate is so important—it allows you to keep track of what you’ve seen. Having local\naccess avoids network latency, making your application more robust and performant.\n Any time you execute a reduction or aggregation operation, you provide the name\nof a state store. Reduction and aggregation operations return a KTable instance, and\nthe KTable uses the state store to replace older results with the newer ones. As you’ve\nValueMapper \ninstance converts \naggregator to a \nstring that’s used \nfor reporting\nCalls toStream() to log the\nresults (to the console) via\nthe peak method\nWrites the results to\nthe stock-volume-by-\ncompany topic\n \n",
      "content_length": 2907,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "132\nCHAPTER 5\nThe KTable API\nseen, not every update gets forwarded downstream, and that’s important because you\nperform aggregation operations to gather summary information. If you didn’t use\nlocal state, the KTable would forward every aggregation or reduction result.\n Next, we’ll look at how you can perform aggregation-like operations over distinct\nperiods of time, a process called windowing. \n5.3.2\nWindowing operations\nIn the previous section, we looked at a “rolling” reduction and aggregation. The appli-\ncation performed a continuous reduction of share volume and then a top-five aggre-\ngation of shares traded in the stock market.\n Sometimes, you’ll want an ongoing aggregation and reduction of results like this.\nAt other times, you’ll only want to perform operations for a given time range. For\nexample, how many stock transactions have involved a particular company in the last\n10 minutes? How many users have clicked to view a new advertisement in the last 15\nminutes? An application may perform these operations many times, but the results\nmay only be for a defined period or window of time.\nCOUNTING STOCK TRANSACTIONS BY CUSTOMER\nIn the next example, you’ll track stock transactions for a handful of traders. These\ncould be large institutional traders or financially savvy individuals.\n There are two likely reasons for doing this tracking. One reason is that you may\nwant to see where the market leaders are buying and selling. If these big hitters or\nsavvy investors see an opportunity in the market, you’ll follow the same strategy. The\nother reason is that you may want to identify any indications of insider trading. You’ll\nwant to look into the timing of large spikes in trading and correlate them with signifi-\ncant news releases.\n Here are the steps to do this tracking:\n1\nCreate a stream to read from the stock-transactions topic.\n2\nGroup incoming records by the customer ID and stock ticker symbol. The\ngroupBy call returns a KGroupedStream instance.\n3\nUse the KGroupedStream.windowedBy method to return a windowed stream, so\nyou can perform some sort of windowed aggregation. Depending on the win-\ndow type provided, you’ll get either a TimeWindowedKStream or a Session-\nWindowedKStream in return.\n4\nPerform a count for the aggregation operation. The windowing stream deter-\nmines whether the record is included in the count.\n5\nWrite the results to a topic, or print the results to the console during development.\nThe topology for this application is straightforward, but it’s helpful to have a mental\npicture of the structure. Take a look at figure 5.11.\n Next, let’s look at the windowing functionality and corresponding code. \n \n \n",
      "content_length": 2662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "133\nAggregations and windowing operations\nWINDOW TYPES\nIn Kafka Streams, three types of windows are available:\nSession windows\nTumbling windows\nSliding/hopping windows\nWhich type you choose depends on your business requirements. The tumbling and\nhopping windows are time bound, whereas session windows are more about user activ-\nity; the length of the session(s) is determined solely by how active the user is. A key\npoint to keep in mind for all windows is that they’re based on the timestamps in the\nrecords and not wall-clock time.\n Next, you’ll implement the topology with each of the window types. We’ll only look\nat the full code in the first example. Other than changing the type of window opera-\ntion, everything will remain the same for the other windows. \nSESSION WINDOWS\nSession windows are very different from other windows. Session windows aren’t bound\nstrictly by time as much as by user activity (or the activity of anything you want to\ntrack). You delineate session windows by a period of inactivity.\n Figure 5.12 shows how you can view session windows. The smaller session will be\nmerged with the session on the left. But the session on the right will be a new ses-\nsion because it follows a large inactivity gap. Session windows are based on user\nstock-\ntransactions\ntopic\nSource\nprocessor\nSink/Print\nprocessor\nGroup-by\nprocessor\nCount\nprocessor\nGroups StockTransactions by\ncustomer ID and stock ticker\nsymbol. These are encapsulated in\na TransactionSummary object.\nCounts number of transactions by\nTransactionSummary (customer ID and stock\nsymbol). You’ll use a windowed approach to\ncontain the counts. The window could be\na tumbling, hopping, or session window.\nConsumes from\nstock-transactions topic\nYou’ll write out the results to\na topic (or print to the console\nduring development).\nThe ﬁnal object returned from the count processor is a\nKTable<Windowed<TransactionSummary>, Long>.\nFigure 5.11\nCounting windows topology\n \n",
      "content_length": 1949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "134\nCHAPTER 5\nThe KTable API\nactivity, but they use timestamps in the records to determine which session a record\nbelongs to. \nUSING SESSION WINDOWS TO TRACK STOCK TRANSACTIONS\nLet’s use session windows to capture the stock transactions. The following code (found\nin src/main/java/bbejeck/chapter_5/CountingWindowingAndKTableJoinExample\n.java) shows how to implement the session windows.\nSerde<String> stringSerde = Serdes.String();\nSerde<StockTransaction> transactionSerde =\n➥ StreamsSerdes.StockTransactionSerde();\nSerde<TransactionSummary> transactionKeySerde =\n➥ StreamsSerdes.TransactionSummarySerde();\nlong twentySeconds = 1000 * 20;\nlong fifteenMinutes = 1000 * 60 * 15;\nKTable<Windowed<TransactionSummary>, Long>\n➥ customerTransactionCounts =\n             \n➥ builder.stream(STOCK_TRANSACTIONS_TOPIC, Consumed.with(stringSerde,\n➥ transactionSerde)\n.withOffsetResetPolicy(LATEST))           \n.groupBy((noKey, transaction) ->\n➥ TransactionSummary.from(transaction),                    \n➥ Serialized.with(transactionKeySerde, transactionSerde))\n.windowedBy(SessionWindows.with(twentySeconds).\n➥ until(fifteenMinutes)).count();        \nListing 5.5\nTracking stock transactions with session windows\n350 600\nInactivity gaps\nThis inactivity gap is large, so new events\ngo into a separate session.\nThere’s a small inactivity gap here, so\nthese sessions would probably be merged\ninto one larger session.\nSession windows are different because they aren’t strictly bound by time but\nrepresent periods of activity. Speciﬁed inactivity gaps demarcate the sessions.\n100 200 500 400\n100 200 500 400\n100 200 500 400\n350 600\nFigure 5.12\nSession windows separated by a small inactivity gap are combined to form \na new, larger session.\nKTable resulting \nfrom the groupBy \nand count calls\nCreates the stream from the\nSTOCK_TRANSACTIONS_TOPIC\n(a String constant). Uses the\noffset-reset-strategy enum of\nLATEST for this stream.\nGroups records by \ncustomer ID and stock \nsymbol, which are stored in \nthe TransactionSummary \nobject.\nWindows the groups with SessionWindow with an\ninactivity time of 20 seconds and a retention time of 15\nminutes. Then performs an aggregation as a count.\n \n",
      "content_length": 2170,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "135\nAggregations and windowing operations\ncustomerTransactionCounts.toStream()\n➥ .print(Printed.<Windowed<TransactionSummary>, Long>toSysOut()\n➥ .withLabel(\"Customer Transactions Counts\"));       \nYou’ve seen most of the operations specified in this topology before, so we don’t need\nto cover them again. But there are a couple of new items, and we’ll discuss those now.\n Any time you do a groupBy operation, you’ll typically perform some sort of aggre-\ngation operation (aggregate, reduce, or count). You can perform a cumulative aggre-\ngation where previous results continue to build up, or you can perform windowed\naggregations where records are combined for the specified time of the window.\n The code in listing 5.5 does a count over session windows. Figure 5.13 breaks\nit down.\nWith the call to windowedBy(SessionWindows.with(twentySeconds).until(fifteen-\nMinutes)), you create a session window with an inactivity gap of 20 seconds and a\nretention period of 15 minutes. An inactivity time of 20 seconds means the applica-\ntion includes any record arriving within 20 seconds of the current session’s ending or\nstart time within the current (active) session.\n You then specify the aggregation operation to perform count, in this case, on the\nsession window. If an incoming record falls outside the inactivity gap (on either side of\nthe timestamp), the application creates a new session. The retention period maintains\nthe session for the specified amount of time and allows for late-arriving data that’s out-\nside the inactivity period of a session but can still be merged. Additionally, as sessions\nare combined, the newly created session uses the earliest timestamp and latest time-\nstamp for the start and end of the new session, respectively.\n Let’s walk through a few records from the count method to see sessions in action:\nsee table 5.1.\nTable 5.1\nSessioning table with a 20-second inactivity gap\nArrival order\nKey\nTimestamp\n1\n{123-345-654,FFBE}\n00:00:00\n2\n{123-345-654,FFBE}\n00:00:15\nConverts the KTable output to a KStream\nand prints the result to the console\nThe with call creates\nthe inactivity gap of\n20 seconds.\nThe until method creates\nthe retention period—\n5 minutes, in this case.\n1\nSessionWindows.with(twentySeconds).until(ﬁfteenMinutes)\nFigure 5.13\nCreating session windows with inactivity periods and retention\n \n",
      "content_length": 2336,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "136\nCHAPTER 5\nThe KTable API\nAs records come in, you look for existing sessions with the same key, with ending times\nless than current timestamp – inactivity gap, and with starting times greater than\ncurrent timestamp + inactivity gap. With that in mind, here’s how the four records\nin table 5.15.1 end up being merged into a single session:\n1\nRecord 1 is first, so start and end are 00:00:00.\n2\nRecord 2 arrives, and you look for sessions with an earliest ending of 23:59:55\nand a latest start of 00:00:35. You find record 1, so you merge sessions 1 and 2.\nYou keep the session 1 start time (earliest) and the session 2 end time (latest),\nso you have one session starting at 00:00:00 and ending at 00:00:15.\n3\nRecord 3 arrives, and you look for sessions between 00:00:30 and 00:01:10 and\nfind none. You add a second session for key 123-345-654, FFBE starting and\nending at 00:00:50.\n4\nRecord 4 arrives, and you search for sessions between 23:59:45 and 00:00:25.\nThis time you find both sessions 1 and 2. All three are merged into one session\nwith a start time of 00:00:00 and an end time of 00:00:15.\nThere are a couple of key points to remember from this section:\nSessions are not a fixed-size window. Rather, the size of a session is driven by the\namount of activity within a given time frame.\nTimestamps in the data determine whether an event fits into an existing session\nor falls into an inactivity gap.\nNow, we’ll move on to the next windowing option, tumbling windows. \nTUMBLING WINDOWS\nFixed or tumbling windows capture events within a given period. Imagine you want to\ncapture all stock transactions for a company every 20 seconds, so you collect every\nevent for that time. After the 20-second period is over, your window will “tumble” to a\nnew 20-second observation period. Figure 5.14 shows this situation.\n As you can see, each event that has come in for the last 20 seconds is included in\nthe window. A new window is created after the specified time.\n Here’s how you could use tumbling windows to capture stock transactions every 20\nseconds (found in src/main/java/bbejeck/chapter_5/CountingWindowingAndKtable-\nJoinExample.java).\n \n \n \n3\n{123-345-654,FFBE}\n00:00:50\n4\n{123-345-654,FFBE}\n00:00:05\nTable 5.1\nSessioning table with a 20-second inactivity gap (continued)\nArrival order\nKey\nTimestamp\n \n",
      "content_length": 2313,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "137\nAggregations and windowing operations\nKTable<Windowed<TransactionSummary>, Long> customerTransactionCounts =\n➥ builder.stream(STOCK_TRANSACTIONS_TOPIC, Consumed.with(stringSerde,\ntransactionSerde)\n➥ .withOffsetResetPolicy(LATEST))\n.groupBy((noKey, transaction) -> TransactionSummary.from(transaction),\n➥ Serialized.with(transactionKeySerde, transactionSerde))\n.windowedBy(TimeWindows.of(twentySeconds)).count();        \nWith this minor change of the TimeWindows.of call, you can use a tumbling window.\nThis example doesn’t include the until() method. By not specifying the duration of\nthe window, you’ll get the default retention of 24 hours.\n Finally, we’ll move on to the last of the windowing options: hopping windows. \nSLIDING OR HOPPING WINDOWS\nSliding or hopping windows are like tumbling windows but with a small difference. A\nsliding window doesn’t wait the entire time before launching another window to pro-\ncess recent events. Sliding windows perform a new calculation after waiting for an\ninterval smaller than the duration of the entire window.\n To illustrate how hopping windows differ from tumbling windows, let’s recast the\nstock transaction count example. You still want to count the number of transactions,\nbut you don’t want to wait the entire period before you update the count. Instead,\nyou’ll update the count at smaller intervals. For example, you’ll still count the number\nof transactions every 20 seconds, but you’ll update the count every 5 seconds, as\nshown in figure 5.15. You now have three result windows with overlapping data.\nListing 5.6\nUsing tumbling windows to count user transactions\n100 200 500 400\n350 600 50 2500\nThe box on the left is the ﬁrst 20-second window. After 20 seconds, it “tumbles”\nover or updates to capture events in a new 20-second period.\nThere is no overlapping of events. The ﬁrst event window contains [ 00, 200, 500, 400]\n1\nand the second event window contains [350, 600, 50, 2500].\nNext 20-second period\nInitial 20-second period\nThe current time period “tumbles” (represented by the dashed box)\ninto the next time period completely with no overlap.\nFigure 5.14\nTumbling windows reset after a fixed period.\nSpecifies a tumbling window of 20 seconds\n \n",
      "content_length": 2214,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "138\nCHAPTER 5\nThe KTable API\nHere’s how to specify hopping windows with code (found in src/main/java/bbejeck/\nchapter_5/CountingWindowingAndKtableJoinExample.java).\nKTable<Windowed<TransactionSummary>, Long> customerTransactionCounts =\n➥ builder.stream(STOCK_TRANSACTIONS_TOPIC, Consumed.with(stringSerde,\n➥ transactionSerde)\n➥ .withOffsetResetPolicy(LATEST))\n.groupBy((noKey, transaction) -> TransactionSummary.from(transaction),\n➥ Serialized.with(transactionKeySerde, transactionSerde))\n.windowedBy(TimeWindows.of(twentySeconds)\n➥ .advanceBy(fiveSeconds).until(fifteenMinutes)).count();  \nWith the addition of the advanceBy() method, you can convert a tumbling window to\na hopping window. This example specifies a retention time of 15 minutes.\nNOTE\nYou’ll notice in all the windowing examples presented here that the\nonly code that changes is the windowedBy call. Instead of having four nearly\nidentical example classes in the sample code, I’ve included four different win-\ndowing lines in the src/main/java/bbejeck/chapter_5/CountingWindowing-\nAndKtableJoinExample.java file. To see a different windowing operation in\naction, comment out the current windowing operation and uncomment the\none you want to execute.\nYou’ve now seen how to put your aggregation results into time windows. In particular,\nI want you to remember three things from this section:\nSession windows aren’t fixed by time but are driven by user activity.\nTumbling windows give you a set picture of events within the specified time frame.\nHopping windows are of fixed length, but they’re frequently updated and can\ncontain overlapping records in each window.\nListing 5.7\nSpecifying hopping windows to count transactions\n100 200 500 400\n350 600 50 2500\nThe box on the left is the ﬁrst 20-second window, but the window “slides” over\nor updates after 5 seconds to start a new window. Now you see an overlapping\nof events. Window\ncontains [ 00, 200, 500, 400], window 2 contains [500,\n1\n1\n400, 350, 600], and window 3 is [350, 600, 50, 2500].\nFigure 5.15\nSliding windows update frequently and may contain \noverlapping data.\nUses a hopping \nwindow of 20 \nseconds, advancing \nevery 5 seconds\n \n",
      "content_length": 2162,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "139\nAggregations and windowing operations\nNext, we’ll look at how to convert a KTable back into a KStream to perform a join. \n5.3.3\nJoining KStreams and KTables\nIn chapter 4, we discussed joining two KStreams. Now, you’re going to join a KTable\nand a KStream. The reason to do this is simple. KStreams are record streams, and\nKTables are streams of record updates, but sometimes you may need to add some\nadditional context to your record stream with the updates from a KTable.\n Let’s take the stock transaction counts and join them with financial news from rel-\nevant industry sectors. Here are the steps to make this happen with the existing code:\n1\nConvert the KTable of stock transaction counts into a KStream where you\nchange the key to the industry of the count by ticker symbol.\n2\nCreate a KTable that reads from a topic of financial news. The new KTable will\nbe categorized by industry.\n3\nJoin the news updates with the stock transaction counts by industry.\nWith these steps laid out, let’s walk through how you can accomplish these tasks.\nCONVERTING THE KTABLE TO A KSTREAM\nTo do the KTable-to-KStream conversion, you’ll take the following steps:\n1\nCall the KTable.toStream() method.\n2\nUse the KStream.map call to change the key to the industry name, and extract\nthe TransactionSummary object from the Windowed instance.\nThese steps are chained together in the following manner (found in src/main/java/\nbbejeck/chapter_5/CountingWindowingAndKtableJoinExample.java).\nKStream<String, TransactionSummary> countStream =\n➥ customerTransactionCounts.toStream().map((window, count) -> {   \nTransactionSummary transactionSummary = window.key();\n     \nString newKey = transactionSummary.getIndustry();\n         \ntransactionSummary.setSummaryCount(count);\n   \nreturn KeyValue.pair(newKey, transactionSummary);\n  \n});\nBecause you’re performing a KStream.map operation, repartitioning for the returned\nKStream instance is done automatically when it’s used in a join.\n Now that you have the conversion process completed, the next step is to create the\nKTable to read the financial news. \nListing 5.8\nConverting a KTable to a KStream\nCalls toStream, immediately\nfollowed by the map call\nExtracts the TransactionSummary \nobject from the Windowed instance\nSets the key to the industry \nsegment of the stock purchase\nTakes the count value \nfrom the aggregation \nand places it in the \nTransactionSummary \nobject\nReturns the new\nKeyValue pair for\nthe KStream\n \n",
      "content_length": 2450,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "140\nCHAPTER 5\nThe KTable API\nCREATING THE FINANCIAL NEWS KTABLE\nFortunately, creating the KTable involves just one line of code (found in src/main/\njava/bbejeck/chapter_5/CountingWindowingAndKtableJoinExample.java).\nKTable<String, String> financialNews =\n➥ builder.table( \"financial-news\", Consumed.with(EARLIEST));    \nIt’s worth noting here that you don’t need to provide any serdes because the configu-\nration is using string serdes. Also, by using the enum EARLIEST, you populate the table\nwith records on startup.\n Now, we’ll move on to the last step, setting up the join. \nJOINING NEWS UPDATES WITH TRANSACTION COUNTS\nSetting up the join is very simple. You’ll use a left join, in case there’s no financial\nnews for the industry involved in the transaction (found in src/main/java/bbe-\njeck/chapter_5/CountingWindowingAndKtableJoinExample.java).\nValueJoiner<TransactionSummary, String, String> valueJoiner =\n➥ (txnct, news) ->\n➥ String.format(\"%d shares purchased %s related news [%s]\",\n➥ txnct.getSummaryCount(), txnct.getStockTicker(), news);     \nKStream<String,String> joined =\n➥ countStream.leftJoin(financialNews, valueJoiner,\n➥ Joined.with(stringSerde, transactionKeySerde, stringSerde));   \njoined.print(Printed.<String, String>toSysOut()\n➥ .withLabel(\"Transactions and News\"));      \nThe leftJoin statement is straightforward. Unlike the joins in chapter 4, you don’t\nprovide a JoinWindow because, when performing a KStream-to-KTable join, there’s\nonly one record per key in the KTable. The join is unrelated to time; the record is\neither present in the KTable or not. The key point here is that you can use KTables to\nprovide less-frequently updated lookup data to enrich your KStream counterparts.\n Next, we’ll look at a more efficient way to enhance KStream events. \n5.3.4\nGlobalKTables\nWe’ve established the need to enrich or add context to our event streams. You’ve also\nseen joins between two KStreams in chapter 4, and the previous section demonstrated\nListing 5.9\nKTable for financial news\nListing 5.10\nSetting up the join between the KStream and KTable\nCreates the KTable with the EARLIEST\nenum, topic financial-news\nValueJoiner \ncombines the \nvalues from the \njoin result.\nThe leftJoin statement for the\ncountStream KStream and the\nfinancial news KTable, providing\nserdes with a Joined instance\nPrints results to the console (in \nproduction this would be written to a \ntopic with a to(\"topic-name\") call)\n \n",
      "content_length": 2432,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "141\nAggregations and windowing operations\na join between a KStream and a KTable. In all of these cases, when you map the keys to\na new type or value, the stream needs to be repartitioned. Sometimes you’ll do the\nrepartitioning explicitly yourself, and other times Kafka Streams will do it automati-\ncally. Repartitioning makes sense, because the keys have been changed and will end\nup on new partitions or the join won’t happen (this was discussed in the chapter 4 sec-\ntion, “Repartitioning the data”).\nREPARTITIONING HAS A COST\nRepartitioning isn’t free. There’s additional overhead in this process: creating inter-\nmediate topics, storing duplicate data in another topic, and increased latency due to\nwriting to and reading from another topic. Additionally, if you want to join on more\nthan one facet or dimension, you’ll need to chain joins, map the records with new\nkeys, and repeat the repartitioning process. \nJOINING WITH SMALLER DATASETS\nIn some cases, the lookup data you want to join against will be relatively small, and\nentire copies of the lookup data could fit locally on each node. For situations where\nthe lookup data is reasonably small, Kafka Streams provides the GlobalKTable.\n GlobalKTables are unique because the application replicates all the data to each\nnode. Because the entirety of the data is on each node, the event stream doesn’t need\nto be partitioned by the key of the lookup data in order to make it available to all par-\ntitions. GlobalKTables also allow you to do non-key joins. Let’s revisit one of the previ-\nous examples to demonstrate this capability. \nJOINING KSTREAMS WITH GLOBALKTABLES\nIn section 5.3.2, you performed a windowed aggregation of stock transactions per cus-\ntomer. The output of the aggregation looked like this:\n{customerId='074-09-3705', stockTicker='GUTM'}, 17\n{customerId='037-34-5184', stockTicker='CORK'}, 16\nAlthough this output accomplished the goal, it would have more impact if you could\nsee the client’s name and the full company name. You could perform regular joins to\nfill out the customer and company names, but you’d need to do two key mappings\nand repartitioning. With GlobalKTables, you can avoid the expense of those opera-\ntions. To accomplish this, you’ll use the countStream from the following listing\n(found in src/main/java/bbejeck/chapter_5/GlobalKTableExample.java) and join it\nagainst two GlobalKTables.\nKStream<String, TransactionSummary> countStream =\nbuilder.stream( STOCK_TRANSACTIONS_TOPIC,\n➥ Consumed.with(stringSerde, transactionSerde)\n➥ .withOffsetResetPolicy(LATEST))\n.groupBy((noKey, transaction) ->\n➥ TransactionSummary.from(transaction),\nListing 5.11\nAggregating stock transactions using session windows\n \n",
      "content_length": 2701,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "142\nCHAPTER 5\nThe KTable API\n➥ Serialized.with(transactionSummarySerde, transactionSerde))\n.windowedBy(SessionWindows.with(twentySeconds)).count()\n.toStream().map(transactionMapper);\nWe covered this previously, so we won’t review it again here. But note that the code in\nthe toStream().map function is abstracted into a function object instead of having an\nin-line lambda, for readability purposes.\n The next step is to define two GlobalKTable instances (found in src/main/java/\nbbejeck/chapter_5/GlobalKTableExample.java).\nGlobalKTable<String, String> publicCompanies =\n➥ builder.globalTable(COMPANIES.topicName());  \nGlobalKTable<String, String> clients =\n➥ builder.globalTable(CLIENTS.topicName());      \nNote that the topic names are defined using enums.\n Now that the components are in place, you need to construct the join (found in\nsrc/main/java/bbejeck/chapter_5/GlobalKTableExample.java).\ncountStream.leftJoin(publicCompanies, (key, txn) ->\n➥ txn.getStockTicker(),TransactionSummary::withCompanyName)   \n.leftJoin(clients, (key, txn) ->\n➥ txn.getCustomerId(), TransactionSummary::withCustomerName)    \n.print(Printed.<String, TransactionSummary>toSysOut()\n➥ .withLabel(\"Resolved Transaction Summaries\"));   \nAlthough there are two joins here, they’re chained together because you don’t use\nany of the results alone. You print the results at the end of the entire operation.\n If you run the join operation, you’ll now get results like this:\n{customer='Barney, Smith' company=\"Exxon\", transactions= 17}\nThe facts haven’t changed, but these results are clearer for reading.\n Including chapter 4, you’ve seen several types of joins in action. They’re listed in\ntable 5.2. This table represents the state of join options as of Kafka Streams 1.0.0, but\nthis may change in future releases.\nListing 5.12\nDefining the GlobalKTables for lookup data\nListing 5.13\nJoining a KStream with two GlobalKTables\nThe publicCompanies lookup is \nfor finding companies by their \nstock ticker symbol.\nThe clients lookup is for getting \ncustomer names by customer ID.\nSets up the leftJoin with the publicCompanies table,\nkeys by stock ticker symbol, and returns the\ntransactionSummary with the company name added\nSets up the leftJoin with the clients table, keys by\ncustomer ID, and returns the transactionSummary\nwith the customer named added\nPrints the results \nout to the console\n \n",
      "content_length": 2369,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "143\nAggregations and windowing operations\nIn conclusion, the key thing to remember is that you can combine event streams\n(KStream) and update streams (KTable), using local state. Additionally, when the\nlookup data is of a manageable size, you can use a GlobalKTable. GlobalKTables rep-\nlicate all partitions to each node in the Kafka Streams application, making all data\navailable, regardless of which partition the key maps to.\n Next, we’ll look at a capability of Kafka Streams that allows you to observe changes\nto state without having to consume data from a Kafka topic. \n5.3.5\nQueryable state\nYou’ve performed several operations involving state, and you’ve always printed the\nresults to the console (for development) or written them to a topic (for production).\nWhen you write the results to a topic, you need to use a Kafka consumer to view the\nresults.\n Reading the data from these topics could be considered a form of materialized\nviews. For our purposes, we can use Wikipedia’s definition of a materialized view: “a\ndatabase object that contains the results of a query. For example, it may be a local\ncopy of data located remotely, or may be a subset of the rows and/or columns of a\ntable or join result, or may be a summary using an aggregate function” (https://en\n.wikipedia.org/wiki/Materialized_view).\n Kafka Streams also offers interactive queries from state stores, giving you the ability to\nread these materialized views directly. It’s important to note that querying state stores\nis a read-only operation. By making the queries read-only, you don’t have to worry\nabout creating an inconsistent state while the application continues to process data.\n The impact of making the state stores directly queryable is significant. It means\nyou can create dashboard applications without having to consume the data from a\nKafka consumer first. There are also some gains in efficiency resulting from not writ-\ning the data out again:\nBecause the data is local, you can access it quickly.\nYou avoid duplicating data by not copying it to an external store.2\nTable 5.2\nKafka Streams joins\nLeft join\nInner join\nOuter join\nKStream-KStream\nKStream-KStream\nKStream-KStream\nKStream-KTable\nKStream-KTable\nN/A\nKTable-KTable\nKTable-KTable\nKTable-KTable\nKStream-GlobalKTable\nKStream-GlobaKTable\nN/A\n2 For more details, see Eno Thereska, “Unifying Stream Processing and Interactive Queries in Apache Kafka,”\nhttp://mng.bz/dh1H.\n \n",
      "content_length": 2425,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "144\nCHAPTER 5\nThe KTable API\nThe main thing I want you to remember here is that you can query state from the\napplication directly. I can’t stress enough the power this feature gives you. Instead of\nconsuming from Kafka and storing records in a database to feed your application, you\ncan directly query the state stores for the same results. The impact of direct queries on\nstate stores means less code (no consumer) and less software (no need for a database\ntable to store results).\n We’ve covered a lot in this chapter, so we’ll stop here in our discussion of interac-\ntive queries on state stores. But fear not: in chapter 9, you’ll build a simple dashboard\napplication with interactive queries. It will use some of the examples from this and\nprevious chapters to demonstrate interactive queries and how you can add them to\nyour Kafka Streams applications. \nSummary\n\nKStreams represent event streams that are comparable to inserts into a data-\nbase. KTables are update streams and are more akin to updates to a database.\nThe size of a KTable doesn’t continue to grow; older records are replaced with\nnewer records.\n\nKTables are essential for performing aggregation operations.\nYou can place your aggregated data into time buckets with windowing operations.\nGlobalKTables give you lookup data across the entire application, regardless of\nthe partitions.\nYou can perform joins with KStreams, KTables, and GlobalKTables.\nSo far, we’ve focused on the high-level KStreams DSL to build Kafka Streams applica-\ntions. Although the high-level approach gives nice, concise programs, everything is a\ntrade-off. By working with the KStreams DSL, you relinquish a level of control and\ngain more-concise code. In the next chapter, we’ll cover the low-level Processor API\nand make different trade-offs. You won’t have the conciseness of the applications\nyou’ve built so far, but you’ll gain the ability to create virtually any kind of processor\nyou need.\n \n",
      "content_length": 1949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "145\nThe Processor API\nUp to this point in the book, we’ve been working with the high-level Kafka Streams\nAPI. It’s a DSL that allows developers to create robust applications with minimal\ncode. The ability to quickly put together processing topologies is an important fea-\nture of the Kafka Streams DSL. It allows you to iterate quickly to flesh out ideas for\nworking on your data without getting bogged down in the intricate setup details\nthat some other frameworks may need.\n But at some point, even when working with the best of tools, you’ll come up\nagainst one of those one-off situations: a problem that requires you to deviate from\nThis chapter covers\nEvaluating higher-level abstractions versus \nmore control\nWorking with sources, processors, and sinks to \ncreate a topology\nDigging deeper into the Processor API with a \nstock analysis processor\nCreating a co-grouping processor\nIntegrating the Processor API and the Kafka \nStreams API\n \n",
      "content_length": 950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "146\nCHAPTER 6\nThe Processor API\nthe traditional path. Whatever the particular case may be, you need a way to dig down\nand write some code that just isn’t possible with a higher-level abstraction.\n6.1\nThe trade-offs of higher-level abstractions vs. \nmore control\nA classic example of trading off higher-level abstractions versus gaining more control\nis using object-relational mapping (ORM) frameworks. A good ORM framework maps\nyour domain objects to database tables and creates the correct SQL queries for you at\nruntime. When you have simple-to-moderate SQL operations (simple SELECT or JOIN\nstatements), using the ORM framework saves you a lot of time. But no matter how\ngood the ORM framework is, there will inevitably be those few queries (very complex\njoins, SELECT statements with nested subselect statements) that just don’t work the way\nyou want. You need to write raw SQL to get the information from the database in the\nformat you need. You can see the trade-off between a higher-level abstraction versus\nmore programmatic control here. Often, you’ll be able to mix the raw SQL with the\nhigher-level mappings provided with the framework.\n This chapter is about those times when you want to do stream processing in a way\nthat the Kafka Streams DSL doesn’t make easy. For example, you’ve seen from work-\ning with the KTable API that the framework controls the timing of forwarding records\ndownstream. You may find yourself in a situation where you want explicit control over\nwhen a record is sent. You might be tracking trades on Wall Street, and you only want\nto forward records when a stock crosses a particular price threshold. To gain this type\nof control, you can use the Processor API. What the Processor API lacks in ease of\ndevelopment, it makes up for in power. You can write custom processors to do almost\nanything you want.\n In this chapter, you’ll learn how to use the Processor API to handle situations\nlike these:\nSchedule actions to occur at regular intervals (either based on timestamps in\nthe records or wall-clock time).\nGain full control over when records are sent downstream.\nForward records to specific child nodes.\nCreate functionality that doesn’t exist in the Kafka Streams API (you’ll see an\nexample of this when we build a co-grouping processor).\nFirst, let’s look at how to use the Processor API by developing a topology step by step. \n6.2\nWorking with sources, processors, and sinks to create \na topology\nLet’s say you’re the owner of a successful brewery (Pops Hops) with several locations.\nYou’ve recently expanded your business to accept online orders from distributors,\nincluding international sales to Europe. You want to route orders within the company\nbased on whether the order is domestic or international, converting any European\nsales from British pounds or euros to US dollars.\n \n",
      "content_length": 2832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "147\nWorking with sources, processors, and sinks to create a topology\n If you were to sketch out the flow of operation, it would look something like fig-\nure 6.1. In building this example, you’ll see how the Processor API gives you the flexi-\nbility to select specific child nodes when forwarding records. Let’s start by creating a\nsource node\n6.2.1\nAdding a source node\nThe first step in constructing a topology is establishing the source nodes. The follow-\ning listing (found in src/main/java/bbejeck/chapter_6/PopsHopsApplication.java)\nsets the data source for the new topology.\ntopology.addSource(LATEST,\n      \npurchaseSourceNodeName,\nnew UsePreviousTimeOnInvalidTimestamp(),\nstringDeserializer,\nbeerPurchaseDeserializer,\nTopics.POPS_HOPS_PURCHASES.topicName())  \nIn the Topology.addSource() method, there are some parameters you didn’t use in\nthe DSL. First, you name the source node. When you used the Kafka Streams DSL,\nyou didn’t need to pass in a name because the KStream instance generated a name\nfor the node. But when you use the Processor API, you need to provide the names of\nthe nodes in the topology. The node name is used to wire up a child node to a par-\nent node.\nListing 6.1\nCreating the beer application source node\nSource node\ndomestic-sales sink\ninternational-sales sink\nBeer-purchase processor\nFigure 6.1\nBeer sales distribution pipeline\nSpecifies the offset \nreset to use\nSpecifies the name \nof this node\nSpecifies the\nTimestampExtractor\nto use for this\nsource\nSets the key deserializer\nSets the value \ndeserializer\nSpecifies the name of the\ntopic to consume data from\n \n",
      "content_length": 1596,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "148\nCHAPTER 6\nThe Processor API\n Next, you specify the timestamp extractor to use with this source. In section 4.5.1,\nwe discussed the different timestamp extractors available to use for each stream\nsource. Here, you’re using the UsePreviousTimeOnInvalidTimestamp class; all other\nsources in the application will use the default FailOnInvalidTimestamp class.\n Next, you provide a key deserializer and a value deserializer, which represents\nanother departure from the Kafka Streams DSL. In the DSL, you supplied Serde\ninstances when creating source or sink nodes. The Serde itself contains a serializer\nand deserializer, and the Kafka Streams DSL uses the appropriate one, depending on\nwhether you’re going from object to byte array, or from byte array to object. Because\nthe Processor API is a lower-level abstraction, you directly provide a deserializer when\ncreating a source node and a serializer when creating a sink node. Finally, you provide\nthe name of the source topic.\n Let’s next look at how you’ll work with purchase records coming into the application. \n6.2.2\nAdding a processor node\nNow, you’ll add a processor to work with the records coming in from the source node\n(found in src/main/java/bbejeck/chapter_6/PopsHopsApplication.java).\nBeerPurchaseProcessor beerProcessor =\n➥ new BeerPurchaseProcessor(domesticSalesSink, internationalSalesSink);\ntopology.addSource(LATEST,\npurchaseSourceNodeName,\nnew UsePreviousTimeOnInvalidTimestamp(),\nstringDeserializer,\nbeerPurchaseDeserializer,\nTopics.POPS_HOPS_PURCHASES.topicName())\n.addProcessor(purchaseProcessor,\n() -> beerProcessor,\npurchaseSourceNodeName);\nThis code uses the fluent interface pattern for constructing the topology. The differ-\nence from the Kafka Streams API lies in the return type. With the Kafka Streams API,\nevery call on a KStream operator returns a new KStream or KTable instance. In the\nProcessor API, each call to Topology returns the same Topology instance.\n In the second annotation, you pass in the processor instantiated on the first line of\nthe code example. The Topology.addProcessor method takes an instance of a\nProcessorSupplier interface for the second parameter, but because the Processor-\nSupplier is a single-method interface, you can replace it with a lambda expression.\n The key point in this section is that the third parameter, purchaseSourceNode-\nName, of the addProcessor() method is the same as the second parameter of the\naddSource() method, as illustrated in figure 6.2. This establishes the parent-child\nrelationship between nodes. The parent-child relationship, in turn, determines how\nListing 6.2\nAdding a processor node\nNames the \nprocessor node\nAdds the processor\ndefined above\nSpecifies the name of the \nparent node or nodes\n \n",
      "content_length": 2739,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "149\nWorking with sources, processors, and sinks to create a topology\nrecords move from one processor to the next in a Kafka Streams application. Figure 6.3\nreviews what you’ve built so far.\nLet’s take a second to discuss the BeerPurchaseProcessor, created in listing 6.1, func-\ntions. The processor has two responsibilities:\nConvert international sales amounts (in euros) to US dollars.\nBased on the origin of the sale (domestic or international), route the record to\nthe appropriate sink node.\nAll of this takes place in the process() method. To quickly summarize, here’s what\nthe process() method does:\n1\nCheck the currency type. If it’s not in dollars, convert it to dollars.\n2\nIf it’s a non-domestic sale, forward the updated record to the international-\nsales topic.\n3\nOtherwise, forward the record directly to the domestic-sales topic.\nHere’s the code for this processor (found in src/main/java/bbejeck/chapter_6/\nprocessor/BearPurchaseProcessor.java).\nFigure 6.2\nWiring up parent and child nodes in the Processor API\nbuilder.addSource(LATEST,\n,\npurchaseSourceNodeName\nnew UsePreviousTimeOnInvalidTimestamp()\nstringDeserializer,\nbeerPurchaseDeserializer,\n\"pops-hops-purchases\");\nbuilder.addProcessor(purchaseProcessor,\n() -> beerProcessor,\n);\npurchaseSourceNodeName\nThe\nof the source node (above) is used\nname\nfor the\nof the processing node\nparent name\n(below). This establishes the parent-child\nrelationship, which directs data ﬂow\nin Kafka Streams.\nname\nparent name\nFigure 6.3\nThe Processor API topology so far, \nincluding node names and parent names\nSource node\nBeer-purchase processor\nname = \"beer-purchase-source\"\nname = \"purchase-processor\"\nparent = \"beer-purchase-source\"\n \n",
      "content_length": 1689,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "150\nCHAPTER 6\nThe Processor API\npublic class BeerPurchaseProcessor extends\n➥ AbstractProcessor<String, BeerPurchase> {\nprivate String domesticSalesNode;\nprivate String internationalSalesNode;\npublic BeerPurchaseProcessor(String domesticSalesNode,\nString internationalSalesNode) {\nthis.domesticSalesNode = domesticSalesNode;\nthis.internationalSalesNode = internationalSalesNode;\n}\n@Override\npublic void process(String key, BeerPurchase beerPurchase) {\nCurrency transactionCurrency = beerPurchase.getCurrency();\nif (transactionCurrency != DOLLARS) {\nBeerPurchase dollarBeerPurchase;\nBeerPurchase.Builder builder =\n➥ BeerPurchase.newBuilder(beerPurchase);\ndouble internationalSaleAmount = beerPurchase.getTotalSale();\nString pattern = \"###.##\";\nDecimalFormat decimalFormat = new DecimalFormat(pattern);\nbuilder.currency(DOLLARS);\nbuilder.totalSale(Double.parseDouble(decimalFormat.\n➥ format(transactionCurrency\n➥ .convertToDollars(internationalSaleAmount))));\ndollarBeerPurchase = builder.build();\ncontext().forward(key,\n➥ dollarBeerPurchase, internationalSalesNode);\n} else {\ncontext().forward(key, beerPurchase, domesticSalesNode);\n}\n}\n}\nThis example extends AbstractProcessor, a class with overrides for Processor inter-\nface methods, except for the process() method. The Processor.process() method\nis where you perform actions on the records flowing through the topology.\nNOTE\nThe Processor interface provides the init(), process(), punctuate(),\nand close() methods. The Processor is the main driver of any application\nlogic that works with records in your streaming application. In the examples,\nyou’ll mostly use the AbstractProcessor class, so you’ll only override the\nmethods you want. The AbstractProcessor class initializes the Processor-\nContext for you, so if you don’t need to do any setup in your class, you don’t\nneed to override the init() method.\nListing 6.3\nBeerPurchaseProcessor\nSets the \nnames for \ndifferent \nnodes to \nforward \nrecords to\nThe process() method,\nwhere the action takes place\nConverts \ninternational \nsales to US dollars\nUses the ProcessorContext (returned from\nthe context() method) and forwards\nrecords to the international child node\nSends\nrecords for\ndomestic\nsales to the\ndomestic\nchild node\n \n",
      "content_length": 2231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "151\nWorking with sources, processors, and sinks to create a topology\nThe last few lines of listing 6.3 demonstrate the main point of this example—the abil-\nity to forward records to specific child nodes. The context() method in these lines\nretrieves a reference to the ProcessorContext object for this processor. All processors\nin a topology receive a reference to the ProcessorContext via the init() method,\nwhich is executed by the StreamTask when initializing the topology.\n Now that you’ve seen how you can process records, the next step is to connect a\nsink node (topic) so you can write records back to Kafka. \n6.2.3\nAdding a sink node\nBy now, you probably have a good feel for the flow of using the Processor API. To add\na source, you used addSource, and for adding a processor, you used addProcessor. As\nyou might imagine, you’ll use the addSink() method to wire up a sink node (topic) to\na processor node. Figure 6.4 shows the updated topology.\nYou can update the topology you’re building by adding sink nodes in the code now\n(found in src/main/java/bbejeck/chapter_6/PopsHopsApplication.java).\ntopology.addSource(LATEST,\npurchaseSourceNodeName,\nnew UsePreviousTimeOnInvalidTimestamp(),\nstringDeserializer,\nbeerPurchaseDeserializer,\nTopics.POPS_HOPS_PURCHASES.topicName())\n.addProcessor(purchaseProcessor,\nListing 6.4\nAdding a sink node\nFigure 6.4\nCompleting the topology by adding sink nodes\nSource node\nDomestic sales sink\nInternational sales sink\nBeer-purchase processor\nname = purchaseSourceNodeName\nname = domesticSalesSink\nparent = purchaseProcessor\nname = purchaseProcessor\nparent = purchaseSourceNodeName\nname = internationalSalesSink\nparent = purchaseProcessor\nNote that the two sink nodes\nhere have the same parent.\n \n",
      "content_length": 1737,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "152\nCHAPTER 6\nThe Processor API\n() -> beerProcessor,\npurchaseSourceNodeName)\n.addSink(internationalSalesSink,\n\"international-sales\",\nstringSerializer,\nbeerPurchaseSerializer,\npurchaseProcessor)\n.addSink(domesticSalesSink,\n\"domestic-sales\",\nstringSerializer,\nbeerPurchaseSerializer,\npurchaseProcessor);\nIn this listing, you add two sink nodes, one for dollars and another for euros. Depend-\ning on the currency of the transaction, you’ll write the records out to the appropri-\nate topic.\n The key point to notice when adding two sink nodes here is that both have the\nsame parent name. By supplying the same parent name to both sink nodes, you’ve\nwired both of them to your processor (as shown in figure 6.4).\n You’ve seen in this first example how you can wire topologies together and forward\nrecords to specific child nodes. Although the Processor API is a little more verbose\nthan the Kafka Streams API, it’s still easy to construct topologies. The next example\nwill explore more of the flexibility the Processor API provides. \n6.3\nDigging deeper into the Processor API with a stock \nanalysis processor\nYou’ll now return to the world of finance and put on your day trading hat. As a day\ntrader, you want to analyze how stock prices are changing with the intent of picking\nthe best time to buy and sell. The goal is to take advantage of market fluctuations and\nmake a quick profit. We’ll consider a few key indicators, hoping they’ll indicate when\nyou should make a move.\n This is the list of requirements:\nShow the current value of the stock.\nIndicate whether the price per share is trending up or down.\nInclude the total share volume so far, and whether the volume is trending up\nor down.\nOnly send records downstream for stocks displaying 2% trending (up or down).\nCollect a minimum of 20 samples for a given stock before performing any cal-\nculations.\nLet’s walk through how you might handle this analysis manually. Figure 6.5 shows the\nsort of decision tree you’ll want to create to help make decisions.\nName of \nthe sink\nThe topic this \nsink represents\nSerializer\nfor the key\nSerializer for the value\nParent node \nfor this sink\nName of the sink\nThe topic this \nsink represents\nSerializer for\nthe value\nParent node\nfor this sink\n \n",
      "content_length": 2241,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "153\nDigging deeper into the Processor API with a stock analysis processor\nThere are a handful of calculations you’ll need to perform for your analysis. Addition-\nally, you’ll use these calculation results to determine if and when you should forward\nrecords downstream.\n This restriction on sending records means you can’t rely on the standard mecha-\nnisms of commit time or cache flushes to handle the flow for you, which rules out\nusing the Kafka Streams API. It goes without saying that you’ll also require state, so\nyou can keep track of changes over time. What you need here is the ability to write a\ncustom processor. Let’s look at the solution to the problem.\n6.3.1\nThe stock-performance processor application\nHere’s the topology for the stock-performance application (found in src/main/java/\nbbejeck/chapter_6/StockPerformanceApplication.java).\nTopology topology = new Topology();\nString stocksStateStore = \"stock-performance-store\";\ndouble differentialThreshold = 0.02;\nFor demo purposes only\nI’m pretty sure it goes without saying, but I’ll state the obvious anyway: these stock\nprice evaluations are for demonstration purposes only. Please don’t infer any real\nmarket-forecasting ability from this example. This model bears no similarity to a real-\nlife approach and is presented only to demonstrate a more complex processing situ-\nation. I’m certainly not a day trader!\nListing 6.5\nStock-performance application with custom processor\nFigure 6.5\nStock trend updates\nYes\nNo\nHold until conditions change.\nOver the last X number of trades, has the price\nor volume of shares increased/decreased\nby more than 2%?\nThe current status of stock XXYY\nSymbol: XXYY; Share price: $10.79; Total volume: 5,123,987\nIf the price and/or volume is increasing, sell;\nif the price and/or volume is decreasing, buy.\nSets the percentage \ndifferential for forwarding \nstock information\n \n",
      "content_length": 1875,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "154\nCHAPTER 6\nThe Processor API\nKeyValueBytesStoreSupplier storeSupplier =\n➥ Stores.inMemoryKeyValueStore(stocksStateStore);\nStoreBuilder<KeyValueStore<String, StockPerformance>> storeBuilder\n➥ = Stores.keyValueStoreBuilder(\n➥ storeSupplier, Serdes.String(), stockPerformanceSerde);\ntopology.addSource(\"stocks-source\",\nstringDeserializer,\nstockTransactionDeserializer,\n\"stock-transactions\")\n.addProcessor(\"stocks-processor\",\n➥ () -> new StockPerformanceProcessor(\n➥ stocksStateStore, differentialThreshold), \"stocks-source\")\n.addStateStore(storeBuilder,\"stocks-processor\")\n.addSink(\"stocks-sink\",\n\"stock-performance\",\nstringSerializer,\nstockPerformanceSerializer,\n\"stocks-processor\");\nThis topology has the same flow as the previous example, so we’ll focus on the new\nfeatures in the processor. In the previous example, you don’t have any setup to do, so\nyou rely on the AbstractProcessor.init method to initialize the ProcessorContext\nobject. In this example, however, you need to use a state store, and you also want to\nschedule when you emit records, instead of forwarding records each time you\nreceive them.\n Let’s look first at the init() method in the processor (found in src/main/java/\nbbejeck/chapter_6/processor/StockPerformanceProcessor.java).\n@Override\npublic void init(ProcessorContext processorContext) {\nsuper.init(processorContext);\nkeyValueStore =\n➥ (KeyValueStore) context().getStateStore(stateStoreName);\nStockPerformancePunctuator punctuator =\n➥ new StockPerformancePunctuator(differentialThreshold,\ncontext(),\nkeyValueStore);\ncontext().schedule(10000, PunctuationType.WALL_CLOCK_TIME,\n➥ punctuator);\n}\n}\nFirst, you need to initialize the AbstractProcessor with the ProcessorContext, so\nyou call the init() method on the superclass. Next, you grab a reference to the state\nstore you created in the topology. All you need to do here is set the state store to a\nListing 6.6\ninit() method tasks\nCreates\nan in-\nmemory\nkey/value\nstate store\nCreates the \nStoreBuilder \nto place in the \ntopology\n Adds the \nprocessor to \nthe topology\nAdds the \nstate store \nto the stocks \nprocessor\nAdds a sink for\nwriting results out,\nalthough you could\nuse a printing sink\nas well\nInitializes \nProcessorContext via \nthe AbstractProcessor \nsuperclass\nRetrieving state \nstore created when \nbuilding topology\nInitializing the \nPunctuator to handle \nthe scheduled \nprocessing\nSchedules Punctuator.punctuate() to \nbe called every 10 seconds\n \n",
      "content_length": 2435,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "155\nDigging deeper into the Processor API with a stock analysis processor\nvariable for use later in the processor. Listing 6.5 also introduces a Punctuator, an\ninterface that’s a callback to handle scheduled execution of processor logic but encap-\nsulated in the Punctuator.punctuate method.\nTIP\nThe ProcessorContext.schedule(long, PunctuationType, Punctuator)\nmethod returns a type of Cancellable, allowing you to cancel a punctuation\nand manage more-advanced scenarios, like those found in the “Punctuate\nUse Cases” discussion (http://mng.bz/YSKF). I don’t have examples or a dis-\ncussion here, but I present some examples in src/main/java/bbejeck/chap-\nter_6/cancellation.\nIn the last line of listing 6.5, you use the ProcessorContext to schedule the Punctu-\nator to execute every 10 seconds. The second parameter, PunctuationType.WALL\n_CLOCK_TIME, specifies that you want to call Punctuator.punctuate every 10 sec-\nonds based on WALL_CLOCK_TIME. Your other option is to specify Punctuation-\nType.STREAM_TIME, which means the execution of Punctuator.punctuate is still\nscheduled every 10 seconds but driven by the time elapsed according to timestamps\nin the data. Let’s take a moment to discuss the difference between these two\nPunctuationType settings.\nPUNCTUATION SEMANTICS\nLet’s start our conversation on punctuation semantics with STREAM_TIME, because it\nrequires a little more explanation. Figure 6.6 illustrates the concept of stream time.\nLet’s walk through some details to gain a deeper understanding of how the schedule is\ndetermined (note that some of the Kafka Stream internals are not shown):\n1\nThe StreamTask extracts the smallest timestamp from the PartitionGroup. The\nPartitionGroup is a set of partitions for a given StreamThread, and it contains\nall timestamp information for all partitions in the group.\n2\nDuring the processing of records, the StreamThread iterates over its Stream-\nTask object, and each task will end up calling punctuate for each of its proces-\nsors that are eligible for punctuation. Recall that you collect a minimum of 20\ntrades before you examine an individual stock’s performance.\n3\nIf the timestamp from the last execution of punctuate (plus the scheduled\ntime) is less than or equal to the extracted timestamp from the Partition-\nGroup, then Kafka Streams calls that processor’s punctuate() method.\nThe key point here is that the application advances timestamps via the Timestamp-\nExtractor, so punctuate() calls are consistent only if data arrives at a constant rate. If\nyour flow of data is sporadic, the punctuate() method won’t get executed at the regu-\nlarly scheduled intervals.\n With PunctuationType.WALL_CLOCK_TIME, on the other hand, the execution of\nPunctuator.punctuate is more predictable, as it uses wall-clock time. Note that sys-\ntem-time semantics is best effort—wall-clock time is advanced in the polling interval,\nand the granularity is dependent on how long it takes to complete a polling cycle. So,\n \n",
      "content_length": 2968,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "156\nCHAPTER 6\nThe Processor API\nwith the example in listing 6.6, you can expect the punctuation activity to be executed\ncloser to every 10 seconds, regardless of data activity.\n Which approach you choose to use is entirely dependent on your needs. If you\nneed some activity performed on a regular basis, regardless of data flow, using system\ntime is probably the best bet. On the other hand, if you only need calculations per-\nformed on incoming data, and some lag time between executions is acceptable, try\nstream-time semantics.\nNOTE\nBefore Kafka 0.11.0, punctuation involved the ProcessorContext\n.schedule(long time) method, which in turn called the Processor.punctuate\nmethod at the scheduled interval. This approach only worked on stream-\ntime semantics, and both methods are now deprecated. I mention depre-\ncated methods in this book, but I only use the latest punctuation methods\nin the examples.\nNow that we’ve covered scheduling and punctuation, let’s move on to handling incom-\ning records. \nFigure 6.6\nPunctuation scheduling using STREAM_TIME\nPartition A\nPartition B\nBecause partition A has the smallest timestamp, it’s chosen ﬁrst:\n1) process called with record A\n2) process called with record B\nNow partition B has the smallest timestamp:\n3) process called with record C\n4) process called with record D\nSwitch back to partition A, which has the smallest timestamp again:\n5) process called with record E\n6) punctuate called because time elapsed from timestamps is 5 seconds\n7) process called with record F\nFinally, switch back to partition B:\n8) process called with record G\n9) punctuate called again as 5 more seconds have elapsed, according to the timestamps\nA:1\nB:2\nE:5\nF:6\nC:3\nD:4\nG:10\nIn the two partitions below, the letter represents the record, and the number is\nthe timestamp. For this example, we’ll assume that\nis scheduled to\npunctuate\nrun every ﬁve seconds.\n \n",
      "content_length": 1886,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "157\nDigging deeper into the Processor API with a stock analysis processor\n6.3.2\nThe process() method\nThe process() method is where you’ll perform all of your calculations to evaluate\nstock performance. There are several steps to take when you receive a record:\n1\nCheck the state store to see if you have a corresponding StockPerformance\nobject for the record’s stock ticker symbol.\n2\nIf the store doesn’t contain the StockPerformance object, one is created. Then,\nthe StockPerfomance instance adds the current share price and share volume\nand updates your calculations.\n3\nStart performing calculations once you hit 20 transactions for any given stock.\nAlthough financial analysis is beyond the scope of this book, we should take a minute\nto look at the calculations. For both the share price and volume, you’re going to per-\nform a simple moving average (SMA). In the financial-trading world, SMAs are used\nto calculate the average for datasets of size N.\n For this example, you’ll set N to 20. Setting a maximum size means that as new\ntrades come in, you collect the share price and number of shares traded for the first\n20 transactions. Once you hit that threshold, you remove the oldest value and add the\nlatest one. Using the SMA, you get a rolling average of stock price and volume over\nthe last 20 trades. It’s important to note you won’t have to recalculate the entire\namount as new values come in.\n Figure 6.7 provides a high-level walk-through of the process() method, illustrat-\ning what you’d do if you were to perform these steps manually. The process()\nmethod is where you’ll perform all the calculations.\nNow, let’s look at the code that makes up the process() method (found in src/main/\njava/bbejeck/chapter_6/processor/StockPerformanceProcessor.java).\nFigure 6.7\nStock analysis process() method walk-through\n1) Price: $10.79, Number shares: 5,000\n2) Price: $11.79, Number shares: 7,000\n20) Price: $12.05, Number shares: 8,000\nAs stocks come in, you keep a rolling average of share price\nand volume of shares over the last 20 trades. You also\nrecord the timestamp of the last update.\nBefore you have 20 trades, you take the average of the number\nof trades you’ve collected so far.\n1) Price: $10.79, Number shares: 5,000\n2) Price: $11.79, Number shares: 7,000\n20) Price: $12.05, Number shares: 8,000\n21) Price: $11.75, Number shares: 6,500\n22) Price: $11.95, Number shares: 7,300\nAfter you hit 20 trades, you drop the oldest trade and add\nthe newest one. You also update the rolling average by\nremoving the old value from the average.\n \n",
      "content_length": 2551,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "158\nCHAPTER 6\nThe Processor API\n@Override\npublic void process(String symbol, StockTransaction transaction) {\nStockPerformance stockPerformance = keyValueStore.get(symbol);\nif (stockPerformance == null) {\nstockPerformance = new StockPerformance();\n}\nstockPerformance.updatePriceStats(transaction.getSharePrice());\nstockPerformance.updateVolumeStats(transaction.getShares());\nstockPerformance.setLastUpdateSent(Instant.now());\nkeyValueStore.put(symbol, stockPerformance);\n}\nIn the process() method, you take the latest share price and the number of shares\ninvolved in the transaction and add them to the StockPerformance object. Notice that\nall details of how you perform the updates are abstracted inside the StockPerformance\nobject. Keeping most of the business logic out of the processor is a good idea—we’ll\ncome back to that point when we cover testing in chapter 8.\n There are two key calculations: determining the moving average, and calculating\nthe differential of stock price/volume from the current average. You don’t want to cal-\nculate an average until you’ve collected data from 20 transactions, so you defer doing\nanything until the processor receives 20 trades. When you have data from 20 trades\nfor an individual stock, you calculate your first average. Then, you take the current\nvalue of the stock price or a number of shares and divide by the moving average, con-\nverting the result to a percentage.\nNOTE\nIf you want to see the calculations, the StockPerformance code can be\nfound in src/main/java/bejeck/model/StockPerformance.java.\nIn the Processor example in listing 6.3, once you worked your way through the\nprocess() method, you forwarded the records downstream. In this case, you store\nthe final results in the state store and leave the forwarding of records to the\nPunctuator.punctuate method. \n6.3.3\nThe punctuator execution\nWe’ve already discussed the punctuation semantics and scheduling, so let’s jump\nstraight into the code for the Punctuator.punctuate method (found in src/main/\njava/bejeck/chapter_6/processor/punctuator/StockPerformancePunctuator.java).\n \nListing 6.7\nprocess() implementation\nRetrieves \nprevious \nperformance \nstats, \npossibly null\nCreates a new StockPerformance \nobject if one isn’t in the state store\nUpdates\nthe price\nstatistics\nfor this\nstock\nUpdates the \nvolume statistics \nfor this stock\nSets the\ntimestamp\nof the last\nupdate\nPlaces the updated StockPerformance\nobject into the state store\n \n",
      "content_length": 2447,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "159\nThe co-group processor\n@Override\npublic void punctuate(long timestamp) {\nKeyValueIterator<String, StockPerformance> performanceIterator =\n➥ keyValueStore.all();\nwhile (performanceIterator.hasNext()) {\nKeyValue<String, StockPerformance> keyValue =\n➥ performanceIterator.next();\nString key = keyValue.key;\nStockPerformance stockPerformance = keyValue.value;\nif (stockPerformance != null) {\nif (stockPerformance.priceDifferential()\n➥ >= differentialThreshold ||\nstockPerformance.volumeDifferential()\n➥ >= differentialThreshold) {\ncontext.forward(key, stockPerformance);\n}\n}\n}\n}\nThe procedure in the Punctuator.punctuate method is simple. You iterate over the\nkey/value pairs in the state store, and if the value has crossed over the predefined\nthreshold, you forward the record downstream.\n An important concept to remember here is that, whereas before you relied on a\ncombination of committing or cache flushing to forward records, now you define the\nterms for when records get forwarded. Additionally, even though you expect to exe-\ncute this code every 10 seconds, that doesn’t guarantee you’ll emit records. They must\nmeet the differential threshold. Also note that the Processor.process and Punctuator\n.punctuate methods aren’t called concurrently.\nNOTE\nAlthough we’re demonstrating access to a state store, it’s a good time\nto review Kafka Streams’ architecture and go over a few key points. Each\nStreamTask has its own copy of a local state store, and StreamThread objects\ndon’t share tasks or data. As records make their way through the topology,\neach node is visited in a depth-first manner, meaning there’s never concur-\nrent access to state stores from any given processor.\nThis example has given you an excellent introduction to writing a custom processor,\nbut you can take writing custom processors a bit further by adding a new data struc-\nture and an entirely new way of aggregating data that doesn’t currently exist in the\nAPI. With this in mind, we’ll move on to adding a co-group processor. \n6.4\nThe co-group processor\nBack in chapter 4, we discussed joins between two streams; specifically, we joined pur-\nchases from different departments within a given time frame to promote business. You\nListing 6.8\nPunctuation code\nRetrieves the iterator \nto go over all key values \nin the state store\nChecks the threshold \nfor the current stock\nIf you’ve met or \nexceeded the \nthreshold, forwards \nthe record\n \n",
      "content_length": 2420,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "160\nCHAPTER 6\nThe Processor API\ncan use joins to bring together records that have the same key and that arrive in the\nsame time window. With joins, there’s an implied one-to-one mapping of records from\nstream A to stream B. Figure 6.8 depicts this relationship.\nNow, let’s imagine that you want to do a similar type of analysis, but instead of using a\none-to-one join by key, you want two collections of data joined by a common key, a co-\ngrouping of data. Suppose you’re the manager of a popular online day-trading applica-\ntion. Day traders use your application for several hours a day—sometimes the entire\ntime the stock market is open. One of the metrics your application tracks is the notion\nof events. You’ve defined an event as being when a user clicks on a ticker symbol to\nread more about a company and its financial outlook. You want to do some deeper\nanalysis between those user clicks in the application and the purchase of stocks by\nusers. You want course-grained results, comparing multiple clicks and purchases to\ndetermine some overall pattern. What you need is a tuple with two collections of each\nevent type by the company trading symbol, as shown in figure 6.9.\nFigure 6.8\nRecords A and B joined by a common key\nRecords A and B are individually joined by a key\nand combined to produce a single joined record.\nAB:1 AB:2 AB:3 AB:4 AB:5\nCafe purchases\nStream A key/value pairs\nA:1 A:2 A:3 A:4 A:5\nElectronics purchases\nStream B key/value pairs\nB:1 B:2 B:3 B:4 B:5\nFor this example, you assume the window time lines up to\ngive you only one-to-one joins so that each record uniquely\nmatches up with only one other record.\nJoins\nRecords A (click events from the day-trading application)\nand B (purchase transactions) are co-grouped by key\n(stock symbol) and produce a key/value pair where the key\nis K, and the value is Tuple, containing a collection of click\nevents and a collection of stock transactions.\nK, Tuple ( [A1, A2, A3, A4, A5], [B1, B2, B3, B4, B5] )\nClickEvents key/value pairs\nA:1 A:2 A:3 A:4 A:5\nStockPurchase key/value pairs\nB:1 B:2 B:3 B:4 B:5\nFor this example, each collection is populated with what’s available at the\ntime punctuate is called. There might be an empty collection at any point.\nFigure 6.9\nOutput of a key with a tuple containing two collections of data—\na co-grouped result\n \n",
      "content_length": 2323,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "161\nThe co-group processor\nYour goal is to combine snapshots of click events and stock transactions for a given\ncompany, every N seconds, but you aren’t waiting for records from either stream to\narrive. When the specified amount of time goes by, you want a co-grouping of click\nevents and stock transactions by company ticker symbol. If either type of event isn’t\npresent, one of the collections in the tuple will be empty. If you’re familiar with\nApache Spark or Apache Flink, this functionality is similar to the PairRDDFunctions\n.cogroup method (http://mng.bz/LaD4) and the CoGroupDataSet class (http://mng\n.bz/FH9m), respectively. Let’s walk through the steps you’ll take in constructing this\nprocessor.\n6.4.1\nBuilding the co-grouping processor\nTo create the co-grouping processor, you need to tie a few pieces together:\n1\nDefine two topics (stock-transactions, events).\n2\nAdd two processors to consume records from the topics.\n3\nAdd a third processor to act as an aggregator/co-grouping for the two preced-\ning processors.\n4\nAdd a state store for the aggregating processor to keep the state for both events.\n5\nAdd a sink node to write the results to (and/or a printing processor to print\nresults to console).\nNow, lets walk through the steps to put this processor together.\nDEFINING THE SOURCE NODES\nYou’re already familiar with the first step, creating the source nodes. This time, you’ll\ncreate two source nodes to support reading both the click event stream and the stock-\ntransactions stream. To keep track of where we are in the topology, we’ll build on fig-\nure 6.10. The code for creating the source nodes is shown in the following listing\n(found in src/main/java/bbejeck/chapter_6/CoGroupingApplication.java).\n//I’ve left out configuration and (de)serializer creation for clarity.\ntopology.addSource(\"Txn-Source\",\nstringDeserializer,\nstockTransactionDeserializer,\n\"stock-transactions\")\n.addSource(\"Events-Source\",\nstringDeserializer,\nclickEventDeserializer,\n\"events\")\nWith the sources for the topology in place, let’s move on to the next step. \nListing 6.9\nSource nodes for co-grouping processor\nSource node for the \nstock-transactions topic\nSource node for \nthe events topic\n \n",
      "content_length": 2191,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "162\nCHAPTER 6\nThe Processor API\nADDING THE PROCESSOR NODES\nNow, you’ll add the workhorses of the topology, the processors. Figure 6.11 shows the\nupdated topology graph. Here’s the code for adding these new processors (found in\nsrc/main/java/bbejeck/chapter_6/CoGroupingApplication.java).\n.addProcessor(\"Txn-Processor\",\nStockTransactionProcessor::new,\n\"Txn-Source\")\n.addProcessor(\"Events-Processor\",\nClickEventProcessor::new,\n\"Events-Source\")\n.addProcessor(\"CoGrouping-Processor\",\nCogroupingProcessor::new,\n\"Txn-Processor\",\n\"Events-Processor\")\nIn the first two lines, the parent names are the names of source nodes reading from\nthe stock-transactions and events topics, respectively. The third processor has the\nnames of both processors given as parent nodes. This means both processors will feed\nthe aggregation processor.\nListing 6.10\nThe processor nodes\nFigure 6.10\nCo-grouping \nsource nodes\ntransaction-source topic\nClick events source node\nAdds the \nStockTransactionProcessor\nAdds the \nClickEventProcessor\nAdds the CogroupingProcessor, which \nis a child node of both processors\nFigure 6.11\nAdding processor nodes\ntransaction-source topic\nClick event source node\nTxn processor\nEvents processor\nCo-grouping processor\nBoth of those send records to\nthe co-grouping processor.\nThe source nodes send\nrecords to the Txn processor\nand events processor.\n \n",
      "content_length": 1351,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "163\nThe co-group processor\nFor the ProcessorSupplier instances, you’re again taking a Java 8 shortcut. This time,\nyou’ve shortened the form, even more, to use a method handle: in this case, a con-\nstructor call to create the associated processor.\nTIP\nWith single-method no-arg interfaces in Java 8, you can use a lambda in\nthe form of ()-> doSomething. But because the ProcessorSupplier’s only role\nis to return a (possibly new each time) Processor object, you can shorten the\nform even more to use a method handle for the constructor of the Processor\ntype. Note that this only applies for no-arg constructors.\nLet’s look at why you’ve set up the processors in this manner. This example is an\naggregation operation, and the roles of the StockTransactionProcessor and Click-\nEventProcessor are to wrap their respective objects into smaller aggregate objects\nand then forward them to another processor for a total aggregation. Both the Stock-\nTransactionProcessor and the ClickEventProcessor perform the smaller aggregation,\nand they forward their records to the CogroupingProcessor. The CogroupingProcessor\nthen performs the co-grouping and forwards the results at regular intervals (intervals\ndriven by timestamps) to an output topic.\n The following listing shows the code for the processors (found in src/main/java/\nbbejeck/chapter_6/processor/cogrouping/StockTransactionProcessor.java).\npublic class StockTransactionProcessor extends\n➥ AbstractProcessor<String, StockTransaction> {\n@Override\n@SuppressWarnings(\"unchecked\")\npublic void init(ProcessorContext context) {\nsuper.init(context);\n}\n@Override\npublic void process(String key, StockTransaction value) {\nif (key != null) {\nTuple<ClickEvent, StockTransaction> tuple =\n➥ Tuple.of(null, value);\ncontext().forward(key, tuple);\n}\n}\n}\nAs you can see, StockTransactionProcessor adds the StockTransaction to the\naggregator (the Tuple) and forwards the record.\nNOTE\nThe Tuple<L, R> shown in listing 6.11 is a custom object for examples\nin this book. You can find it in src/main/java/bbejeck/util/collection/\nTuple.java.\nListing 6.11\nStockTransactionProcessor\nCreates an \naggregate \nobject with the \nStockTransaction\nForwards the tuple to the \nCogroupingProcessor\n \n",
      "content_length": 2213,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "164\nCHAPTER 6\nThe Processor API\nNow, let’s look at the ClickEventProcessor code (found in src/main/java/bbejeck/\nchapter_6/processor/cogrouping/ClickEventProcessor.java).\npublic class ClickEventProcessor extends\n➥ AbstractProcessor<String, ClickEvent> {\n@Override\n@SuppressWarnings(\"unchecked\")\npublic void init(ProcessorContext context) {\nsuper.init(context);\n}\n@Override\npublic void process(String key, ClickEvent clickEvent) {\nif (key != null) {\nTuple<ClickEvent, StockTransaction> tuple =\n➥ Tuple.of(clickEvent, null);\ncontext().forward(key, tuple);\n}\n}\n}\nAs you can see, the ClickEventProcessor adds the ClickEvent to the Tuple aggrega-\ntor, much like the previous listing.\n To complete the picture of how to perform the aggregation, we need to look at the\nCogroupingProcessor code. It’s more involved, so we’ll examine each method in\nturn, starting with CogroupingProcessor.init() (found in src/main/java/bbejeck/\nchapter_6/processor/cogrouping/AggregatingProcessor.java).\npublic class CogroupingProcessor extends\n➥ AbstractProcessor<String, Tuple<ClickEvent,StockTransaction>> {\nprivate KeyValueStore<String,\n➥ Tuple<List<ClickEvent>,List<StockTransaction>>> tupleStore;\npublic static final\nString TUPLE_STORE_NAME = \"tupleCoGroupStore\";\n@Override\n@SuppressWarnings(\"unchecked\")\npublic void init(ProcessorContext context) {\nsuper.init(context);\ntupleStore = (KeyValueStore)\n➥ context().getStateStore(TUPLE_STORE_NAME);\nCogroupingPunctuator punctuator =\n➥ new CogroupingPunctuator(tupleStore, context());\ncontext().schedule(15000L, STREAM_TIME, punctuator);\n}\nListing 6.12\nClickEventProcessor\nListing 6.13\nThe CogroupingProcessorinit() method\nAdds the \nClickEvent \nto the initial \naggregator \nobject\nForwards the tuple to the \nCogroupingProcessor\nRetrieves the configured \nstate store\nCreates a Punctuator \ninstance, Cogrouping-\nPunctuator, which handles \nall scheduled calls\nSchedules a call to the Punctuator.punctuate() method every 15 seconds\n \n",
      "content_length": 1955,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "165\nThe co-group processor\nAs you might expect, the init() method handles the details of setting up the class.\nYou grab the state store configured in the main application and save it in a variable for\nuse later on. You create the CogroupingPunctuator to handle the scheduled punctua-\ntion calls.\nListing 6.13 schedules punctuate for every 15 seconds. Because you’re using the\nPunctuationType.STREAM_TIME semantics, the timestamps in the arriving data drive\nthe calls to punctuate. Remember that if the flow of data isn’t relatively constant, you\nmay have more than 15 seconds between calls to Punctuator.punctuate.\nNOTE\nYou’ll recall from our previous discussion of punctuate semantics that\nyou have two choices: PunctuationType.STREAM_TIME and Punctuation-\nType.WALL_CLOCK_TIME. Listing 6.13 uses STREAM_TIME semantics. There’s\nan additional processor example showing WALL_CLOCK_TIME semantics in\nsrc/main/ava/bbejeck/chapter_6/processor/cogrouping/CogroupingSystem-\nTimeProcessor.java, so you can observe the differences in performance and\nbehavior.\nNext, let’s look at how the CogroupingProcessor performs one of its main tasks in\nthe process() method (found in src/main/java/bbejeck/chapter_6/processor/\ncogrouping/CogroupingProcessor.java).\n@Override\npublic void process(String key,\n➥ Tuple<ClickEvent, StockTransaction> value) {\nTuple<List<ClickEvent>, List<StockTransaction>> cogroupedTuple\n➥ = tupleStore.get(key);\nif (cogroupedTuple == null) {\ncogroupedTuple =\n➥ Tuple.of(new ArrayList<>(), new ArrayList<>());\n}\nif (value._1 != null) {\ncogroupedTuple._1.add(value._1);\n}\nMethod handles for Punctuator\nYou can specify a method handle for the Punctuator instance. To do so, declare a\nmethod in the processor that accepts a single parameter of type long and a void\nreturn type. Then, schedule punctuation like this:\ncontext().schedule(15000L, STREAM_TIME, this::myPunctuationMethod);\nFor an example of this, look in src/main/java/bbejeck/chapter_6/processor/\ncogrouping/CogroupingMethodHandleProcessor.java.\nListing 6.14\nThe CogroupingProcessorprocess() method\nInitializes the total \naggregation if it \ndoesn’t exist yet\nIf the ClickEvent is \nnot null, adds it to the \nlist of click events\n \n",
      "content_length": 2199,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "166\nCHAPTER 6\nThe Processor API\nif (value._2 != null) {\ncogroupedTuple._2.add(value._2);\n}\ntupleStore.put(key, cogroupedTuple);\n}\n}\nAs you process incoming smaller aggregates of your overall co-grouping, the first is\nstep checking if you have an instance in your state store already. If you don’t, you cre-\nate a Tuple with empty collections of ClickEvent and StockTransaction.\n Next, you check the incoming smaller aggregation, and if either a ClickEvent or\nStockTransaction is present, you add it to the overall aggregation. The last step in\nthe process() method is putting the Tuple back into the store, updating your aggre-\ngation total.\nNOTE\nAlthough you have two processors forwarding records to one processor\nand accessing one state store, you don’t have to be concerned about concur-\nrency issues. Remember, parent processors forward records to child proces-\nsors in a depth-first manner, so each parent processor serially calls the child\nprocessor. Additionally, Kafka Streams only uses one thread per task, so there\nare never any concurrency access issues.\nThe next step is to look at how punctuation is handled (found in src/main/java/\nbbejeck/chapter_6/processor/cogrouping/CogroupingPunctuator.java). You use the\nupdated API, so we won’t look at the Processor.punctuate call, which is deprecated.\n// leaving out class declaration and constructor for clarity\n@Override\npublic void punctuate(long timestamp) {\nKeyValueIterator<String, Tuple<List<ClickEvent>,\n➥ List<StockTransaction>>> iterator = tupleStore.all();\nwhile (iterator.hasNext()) {\nKeyValue<String, Tuple<List<ClickEvent>, List<StockTransaction>>>\n➥ cogrouping = iterator.next();\n// if either list contains values forward results\nif (cogrouping.value != null &&\n➥ (!cogrouping.value._1.isEmpty() ||\n➥ !cogrouping.value._2.isEmpty())) {\nList<ClickEvent> clickEvents =\n➥ new ArrayList<>(cogrouping.value._1);\nList<StockTransaction> stockTransactions =\n➥ new ArrayList<>(cogrouping.value._2);\nListing 6.15\nThe CogroupingPunctuator.punctuate() method\nIf the StockTransaction \nis not null, adds it to the list \nof stock transactions\nPlaces the updated aggregation \ninto the state store\nGets iterator of \nall co-groupings \nin the store\nRetrieves the next \nco-grouping\nEnsures that the value is not null, and \nthat either collection contains data\nMakes defensive \ncopies of co-grouped \ncollections\n \n",
      "content_length": 2366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "167\nThe co-group processor\ncontext.forward(cogrouping.key,\n➥ Tuple.of(clickEvents, stockTransactions));\ncogrouped.value._1.clear();\ncogrouped.value._2.clear();\ntupleStore.put(cogrouped.key, cogrouped.value);\n}\n}\niterator.close();\n}\nDuring each punctuate call, you retrieve all of the stored records in a KeyValue-\nIterator, and you start to pull out each co-grouped result contained in the iterator.\nThen, you make defensive copies of the collections, create a new co-grouped Tuple,\nand forward it downstream. In this case, you send the co-grouped results to a sink\nnode. Finally, you remove the current co-grouped results and store the tuple back in\nthe store, ready for the next round of records to arrive.\n Now that we’ve covered the co-grouping functionality, let’s complete building the\ntopology. \nADDING THE STATE STORE\nAs you’ve seen, the ability to perform aggregations in a Kafka streaming application\nrequires having state. You’ll need to add a state store to the CogroupingProcessor for\nit to function properly. Figure 6.12 shows the updated topology.\nForwards the key and \naggregated co-grouping\nPuts the cleared-out\ntuple back into the store\nFigure 6.12\nAdding a state store to the co-grouping processor in the topology\nTransaction source node\nClick event source node\nTxn processor\nEvents processor\nCo-grouping processor\nBoth of those send records to\nthe co-grouping processor.\nState store used by co-grouping processor\nThe source nodes send\nrecords to the Txn processor\nand events processor.\n \n",
      "content_length": 1508,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "168\nCHAPTER 6\nThe Processor API\nNow, let’s look at the code for adding the state store (found in src/main/java/bbejeck/\nchapter_6/CoGroupingApplication.java).\n// this comes earlier in source code, included here for context\nMap<String, String> changeLogConfigs = new HashMap<>();\nchangeLogConfigs.put(\"retention.ms\",\"120000\" );\n            \nchangeLogConfigs.put(\"cleanup.policy\", \"compact,delete\");\nKeyValueBytesStoreSupplier storeSupplier =\n➥ Stores.persistentKeyValueStore(TUPLE_STORE_NAME);\nStoreBuilder<KeyValueStore<String,\n➥ Tuple<List<ClickEvent>, List<StockTransaction>>>> storeBuilder =\nStores.keyValueStoreBuilder(storeSupplier,\nSerdes.String(),\neventPerformanceTuple)\n➥ .withLoggingEnabled(changeLogConfigs);\n.addStateStore(storeBuilder, \"CoGrouping-Processor\")\nHere, you add a persistent state store. This is a persistent store, because you might get\ninfrequent updates for some keys. With the in-memory and LRU-based stores, infre-\nquently used keys and values might eventually be removed, and here you’ll want the\nability to retrieve information for any key you’ve worked with before.\nTIP\nThe first three lines in listing 6.16 create specific configurations for the\nstate store to keep the changelog at a manageable size. Remember: you can\nconfigure changelog topics with any valid topic configuration.\nThis code is straightforward. One point to notice, though, is that the CoGrouping-\nProcessor is specified as the only processor that can access this store.\n You now have one step left to complete the topology: the ability to read the results\nof the co-grouping. \nADDING THE SINK NODE\nFor the co-grouping topology to be of use, you need to write the data out to a topic (or\nthe console). Let’s update the topology one more time, as shown in figure 6.13.\nNOTE\nIn several examples, I talk about adding a sink node, but in the source\ncode there’s a sink that writes to the console; the sink that writes to a topic is\ncommented out. For development purposes, I use the sink node writing to a\ntopic and to stdout interchangeably.\nListing 6.16\nAdding a state store node\nSpecifies how long to keep records, and\nuses compaction and delete for cleanup\nCreates the store supplier for \na persistent store (RocksDB)\nCreates the \nstore builder\nAdds the changelog configs \nto the store builder\nAdds the store to the \ntopology with the name \nof the processor that \nwill access the store\n \n",
      "content_length": 2389,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "169\nThe co-group processor\nNow, the co-grouped aggregation results are written out to a topic for use in further\nanalysis. Here’s the code (found in src/main/java/bbejeck/chapter_6/CoGrouping-\nApplication.java).\n.addSink(\"Tuple-Sink\",\n\"cogrouped-results\",\nstringSerializer,\ntupleSerializer,\n\"CoGrouping-Processor\");\ntopology.addProcessor(\"Print\",\nnew KStreamPrinter(\"Co-Grouping\"),\n\"CoGrouping-Processor\");\nIn this final piece of the topology, you add a sink node, as a child of the CoGrouping-\nProcessor, that writes the co-grouping results out to a topic. Listing 6.17 also adds an\nadditional processor for printing results to the console during development—it’s also\na child node of the CoGrouping-Processor. Remember that with the Processor API,\nthe order in which you define nodes doesn’t establish a parent-child relationship. The\nparent-child relationship is determined by providing the names of previously defined\nprocessors.\nListing 6.17\nThe sink node and a printing processor\nFigure 6.13\nAdding a sink node completes the co-grouping topology\nSink node\nCo-grouping processor\nEvents processor\nClick event source node\nTransaction source node\nTxn processor\nState store used by the\nco-grouping processor\nThe sink node writes \nthe co-grouped tuples \nout to a topic.\nThis processor prints \nresults to stdout for use \nduring development.\n \n",
      "content_length": 1342,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "170\nCHAPTER 6\nThe Processor API\n You’ve now built the co-grouping processor. The key point I want you to remember\nfrom this section is that, although it involves more code, using the Processor API gives\nyou the flexibility to create virtually any kind of streaming topology you need.\n Let’s wrap up this chapter with a look at how you can integrate some Processor API\nfunctionality into a KStreams application. \n6.5\nIntegrating the Processor API and the \nKafka Streams API\nSo far, our coverage of the Kafka Streams and the Processor APIs has been separate,\nbut that’s not to say that you can’t combine approaches. Why would you want to mix\nthe two approaches?\n Let’s say you’ve used both the KStream and Processor APIs for a while. You’ve\ncome to prefer the KStream approach, but you want to include some of your previ-\nously defined processors in a KStream application, because they provide some of the\nlower-level control you need.\n The Kafka Streams API offers three methods that allow you to plug in functionality\nbuilt using the Processor API: KStream.process, KStream.transform, and KStream\n.transformValues. You already have some experience with this approach because you\nworked with the ValueTransformer in section 4.2.2.\n The KStream.process method creates a terminal node, whereas the KStream\n.transform (or KStream.transformValues) method returns a new KStream instance\nallowing you to continue adding processors to that node. Note also that the trans-\nform methods are stateful, so you also provide a state store name when using them.\nBecause KStream.process results in a terminal node, you’ll usually want to use either\nKStream.transform or KStream.transformValues.\n From there, you can replace your Processor with a Transformer instance. The\nmain difference between the two interfaces is that the Processor’s main action method\nis process(), which has a void return, whereas the Transformer uses transform() and\nexpects a return type of R. Both offer the same punctuation semantics.\n In most cases, replacing a Processor is a matter of taking the logic from the\nProcessor.process method and placing it in the Transformer.transform method.\nYou’ll need to account for returning a value, but returning null and forwarding results\nwith ProcessorContext.forward is an option.\nTIP\nThe transformer returns a value: in this case, it returns a null, which is fil-\ntered out, and you use the ProcessorContext.forward method to send mul-\ntiple values downstream. If you wanted to return multiple values instead,\nyou’d return a List<KeyValue<K,V>> and then attach a flatMap or flat-\nMapValues to send individual records downstream. An example of this can be\nfound in src/main/java/bbejeck/chapter_6/StockPerformanceStreamsAnd-\nProcessorMultipleValuesApplication.java. To complete the replacement of a\nProcessor instance, you’d plug in the Transformer (or ValueTransformer)\ninstance using the KStream.transform or KStream.transformValues method.\n \n",
      "content_length": 2949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "171\nSummary\nA great example of combining the KStream and Processor APIs can be found in\nsrc/main/java/bbejeck/chapter_6/StockPerformanceStreamsAndProcessorApplication\n.java. I didn’t present that example here because the logic is, for the most part, identi-\ncal to the StockPerformanceApplication example from section 6.3.1. You can look it\nup if you’re interested. Additionally, you’ll find a Processor API version of the original\nZMart application in src/main/java/bbejeck/chapter_6/ZMartProcessorApp.java. \nSummary\nThe Processor API gives you more flexibility at the cost of more code.\nAlthough the Processor API is more verbose than the Kafka Streams API, it’s\nstill easy to use, and the Processor API is what the Kafka Streams API, itself, uses\nunder the covers.\nWhen faced with a decision about which API to use, consider using the Kafka\nStreams API and integrating lower-level methods (process(), transform(),\ntransformValues()) when needed.\nAt this point in the book, we’ve covered how you can build applications with Kafka\nStreams. Our next step is to look at how you can optimally configure these applica-\ntions, monitor them for maximum performance, and spot potential issues.\n \n",
      "content_length": 1193,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "Part 3\nAdministering\nKafka Streams\nIn these chapters, we’ll shift focus to how you can measure the performance\nof your Kafka Streams application. Additionally, you’ll learn how to monitor and\ntest your Kafka Streams code so you know it’s working as expected and will grace-\nfully handle errors.\n \n",
      "content_length": 297,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "175\nMonitoring and\nperformance\nSo far, you’ve learned how to build a Kafka Streams application from the bottom\nup. You’ve worked with the high-level Kafka Streams DSL, and you’ve seen the\npower of using a declarative API. You’ve also learned about the Processor API and\nhave seen how you can give up some convenience to gain more control in writing\nyour streaming applications.\n It’s now time to change gears a bit. You’re going to put on your forensic investi-\ngator hat and dig into your application from a different perspective. Your focus is\ngoing to shift from how you get things to work to what is going on. In some respects,\nthe initial building of an application is the most comfortable part. Getting the\napplication to run successfully, scale correctly, and work properly is always the more\nsignificant challenge. Despite your best efforts, there’s almost always a situation you\ndidn’t account for.\nThis chapter covers\nLooking at basic Kafka monitoring\nIntercepting messages\nMeasuring performance\nObserving the state of the application\n \n",
      "content_length": 1051,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "176\nCHAPTER 7\nMonitoring and performance\n In this chapter, you’ll learn how to check the running state of your Kafka Streams\napplication. You’ll see how you can measure the performance of the application in\norder to spot performance bottlenecks. You’ll also see techniques you can use to\nnotify you about various states of the application and to view the structure of the\ntopology. You’ll learn what metrics are available, how you can collect them, and how\nyou can observe the collected metrics as the application is running. Let’s start with\nmonitoring a Kafka Streams application.\n7.1\nBasic Kafka monitoring\nBecause the Kafka Streams API is a part of Kafka, it goes without saying that monitoring\nyour application will require some monitoring of Kafka as well. Full-blown surveillance\nof a Kafka cluster is a big topic, so we’ll limit our discussion of Kafka performance to\nwhere the two meet—we’ll talk about monitoring Kafka consumers and producers.\nMore information on monitoring a Kafka cluster can be found in the documentation\n(https://kafka.apache.org/documentation/#monitoring).\nNOTE\nOne thing I should note here is that to measure Kafka Streams perfor-\nmance, we also need to measure Kafka itself. At times, some of our coverage\nof performance will edge over to the Kafka side of things. But because this is a\nbook on Kafka Streams, we’ll focus on Kafka Streams.\n7.1.1\nMeasuring consumer and producer performance\nFor our discussion of consumer and producer performance, let’s start by looking at\nfigure 7.1, which depicts one of the fundamental performance concerns for a pro-\nducer and consumer. As you can see, producer and consumer performance are very\nsimilar in that both are concerned with throughput. But where we put the emphasis is\njust different enough that they can be considered two sides of the same coin.\nFor producers, we care mostly about how fast the producer is sending messages to the\nbroker. Obviously, the higher the throughput, the better.\n For consumers, we’re also concerned with performance, or how fast we can read\nmessages from a broker. But there’s another way to measure consumer performance:\nFigure 7.1\nKafka producer and consumer performance concerns, writing to and reading \nfrom a broker\nProducer\nConsumer\nMB per second consumed\nRecords per second consumed\nMB per second produced\nRecords per second produced\nKafka broker\n \n",
      "content_length": 2367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "177\nBasic Kafka monitoring\nconsumer lag. Take a look at figure 7.2. This measures producer and consumer through-\nput with a slightly different focus.\nYou can see that we care about how much and how fast our producers can publish to a\nbroker, and we simultaneously care about how quickly our consumers can read those\nmessages from the broker. The difference between how fast the producers place records\non the broker and when consumers read those messages is called consumer lag.\n Figure 7.3 illustrates that consumer lag is the difference between the last commit-\nted offset from the consumer and the last offset from a message written to the broker.\nThere’s bound to be some lag from the consumer, but ideally the consumer will catch\nup, or at least have a consistent lag rather than a gradually increasing lag\nFigure 7.2\nKafka producer and consumer performance revisited\nProducer\nConsumer\nHow fast are records consumed\nfrom topic A?\nHow fast are records being\nproduced to topic A?\nKafka broker\nDoes the consumer keep up with the rate of records coming from the producer?\nFigure 7.3\nConsumer lag is the difference in offsets committed by the consumer and offsets \nwritten by the producer\nProducer\nConsumer\nLast message consumed\nat offset 994\nLast offset produced\nLast offset consumed\nThe difference between the most recent offset produced and the last\noffset consumed (from the same topic) is known as consumer lag.\nIn this case, the consumer lags behind the producer by six records.\nMost recent message produced\nhas an offset of 1000\n995\n994\n996\n997\n998\n999\n1000\n1000\n994\n \n",
      "content_length": 1576,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "178\nCHAPTER 7\nMonitoring and performance\nNow that we’ve defined our performance parameters for producers and consumers,\nlet’s see how we can monitor them for performance and troubleshooting issues. \n7.1.2\nChecking for consumer lag\nTo check for consumer lag, Kafka provides a convenient command-line tool, kafka-\nconsumer-groups.sh, found in the <kafka-install-dir>/bin directory. The script has a\nfew options, but here we’ll focus on the list and describe options. These two\noptions will give you the information you need about consumer group performance.\n First, use the list command to find all active consumer groups. Figure 7.4 shows\nthe results of running this command.\n<kafka-install-dir>/bin/kafka-consumer-groups.sh \\\n--bootstrap-server localhost:9092 \\\n--list\nWith this information, you can choose a consumer group name and run the following\ncommand:\n<kafka-install-dir>/bin/kafka-consumer-groups.sh \\\n--bootstrap-server localhost:9092 \\\n--group <GROUP-NAME> \\\n--describe\nFigure 7.5 shows the results: the status of how this consumer is performing.\nThese results show that you have a small consumer lag. Having a consumer lag isn’t\nalways indicative of a problem—consumers read messages in batches and won’t\nretrieve another batch until they’re finished processing the current batch. Processing\nthe records takes time, so a little lag is not entirely surprising.\nFigure 7.4\nListing available consumer groups from the command line\nFigure 7.5\nStatus of a consumer group\nNumber of messages read = 3\nNumber of messages sent to topic =\n0\n1\n10 (messages sent) – 3 (messages read) = 7 (lag, or records behind)\n \n",
      "content_length": 1614,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "179\nBasic Kafka monitoring\n A small lag or one that stays constant is OK, but a lag that continues to grow over\ntime is an indication you’ll need to give your consumer more resources. For example,\nyou might need to increase the partition count and hence increase the number of\nthreads consuming from the topic. Or maybe your processing after reading the mes-\nsage is too heavyweight. After consuming a message, you could hand it off to an async\nqueue, where another thread can pick up the message and do the processing.\n In this section, you’ve learned how to determine how quickly a consumer is read-\ning messages from a broker. Next, we’ll dig a little deeper into observing behavior for\ndebugging purposes—you’ll see how to intercept what the producers are sending and\nconsumers are receiving before your Kafka Streams application sends or consumes\nrecords. \n7.1.3\nIntercepting the producer and consumer\nEarly in 2016, Kafka Improvement Proposal 42 (KIP-42) introduced the ability to\nmonitor or “intercept” information on client (consumer and producer) behavior. The\ngoal of the KIP was to provide “the ability to quickly deploy tools to observe, measure,\nand monitor Kafka client behavior, down to the message level.”1\n Although interceptors aren’t typically your first line for debugging, they can prove\nuseful in observing the behavior of your Kafka streaming application, and they’re a\nvaluable addition to your toolbox. An excellent example of using an interceptor (pro-\nducer) is using one to keep track of the message offsets your Kafka Streams applica-\ntion is producing back to Kafka.\nNOTE\nBecause Kafka Streams can consume or produce any number of key\nand value types, the internal Consumer and Producer are configured to work\nwith byte[] keys and byte[] values; hence, they always handle unserialized\ndata. Serialized data means you can’t inspect messages without an extra dese-\nrialization/serialization step.\nLet’s get started by discussing the consumer interceptor.\nCONSUMER INTERCEPTOR\nThe consumer interceptor gives you two access points to intercept. The first is Consumer-\nInterceptor.onConsume(), which reads ConsumerRecords between the point where\nthey’re retrieved from the broker, and before the messages are returned from the\nConsumer.poll() method. The following pseudocode will give you an idea of where\nthe consumer interceptor is doing its work:\nConsumerRecords<String, String> poll(long timeout) {\nConsumerRecords<String, String> consumerRecords =\n➥ ...consuming records\nreturn interceptors.onConsume(consumerRecords);\n1 Apache Kafka, “KIP-42: Add Producer and Consumer Interceptors,” http://mng.bz/g8oX.\nFetches new records \nfrom the broker\nRuns records through the \ninterceptor chain and \nreturns the results\n \n",
      "content_length": 2744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "180\nCHAPTER 7\nMonitoring and performance\nAlthough this pseudocode bears no resemblance to the actual KafkaConsumer code, it\nillustrates the point. Interceptors accept the ConsumerRecords returned from the bro-\nker inside the Consumer.poll() method and have the opportunity to perform any\noperation, including filtering or modification, before the KafkaConsumer returns the\nrecords from the poll method.\n ConsumerInterceptors are specified via ConsumerConfig.INTERCEPTOR_CLASSES\n_CONFIG with a Collection of one or more ConsumerInterceptor implementor\nclasses. Multiple interceptors are chained together and executed in the order speci-\nfied in the configuration.\n A ConsumerInterceptor accepts and returns a ConsumerRecords instance. If there\nare multiple interceptors, the returned ConsumerRecords from one interceptor serves\nas the input parameter for the next interceptor in the chain. Thus, any modifications\nmade by one interceptor are propagated to the next interceptor in the chain.\n Exception handling is an important consideration when chaining multiple inter-\nceptors together. If an Exception occurs in an interceptor, it logs the error, but it\ndoesn’t short-circuit the chain. Thus, ConsumerRecords continues to work its way\nthrough the remaining interceptors.\n For example, suppose you have three interceptors: A, B, and C. All three modify\nthe records and rely on changes made by the previous interceptor in the chain. But if\ninterceptor A encounters an error, the ConsumerRecords object continues to intercep-\ntors B and C, but without the expected modifications, rendering the results from the\ninterceptor chain invalid. For this reason, it’s best not to have an interceptor rely on\nConsumerRecords modified by a previous interceptor in the chain.\n The second interception point is the ConsumerInterceptor.onCommit() method.\nAfter the consumer commits its offsets to the broker, the broker returns a Map<Topic-\nPartition, OffsetAndMetadata> containing information with the topic, partition,\nand committed offsets, along with associated metadata (time of commit, and so on).\nThe commit information can be useful for tracking purposes. Here’s an example of a\nsimple ConsumerInterceptor used for logging purposes (found in src/main/java/\nbbejeck/chapter_7/interceptors/StockTransactionConsumerInterceptor.java).\npublic class StockTransactionConsumerInterceptor implements\n➥ ConsumerInterceptor<Object, Object> {\n// some details left out for clarity\nprivate static final Logger LOG =\n➥ LoggerFactory.getLogger(StockTransactionConsumerInterceptor.class);\npublic StockTransactionConsumerInterceptor() {\nLOG.info(\"Built StockTransactionConsumerInterceptor\");\n}\n@Override\npublic ConsumerRecords<Object, Object>\n➥ (ConsumerRecords<Object, Object> consumerRecords) {\nListing 7.1\nLogging consumer interceptor\n \n",
      "content_length": 2816,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "181\nBasic Kafka monitoring\nLOG.info(\"Intercepted ConsumerRecords {}\",\nbuildMessage(consumerRecords.iterator()));\nreturn consumerRecords;\n}\n@Override\npublic void onCommit(Map<TopicPartition, OffsetAndMetadata> map) {\nLOG.info(\"Commit information {}\",\nmap);\n}\nNow let’s cover the producing side of intercepting.\nPRODUCER INTERCEPTOR\nThe ProducerInterceptor works similarly and has two access points: Producer-\nInterceptor.onSend() and ProducerInterceptor.onAcknowledgement(). With the\nonSend method, the interceptor can perform any action, including mutating the\nProducerRecord. Each producer interceptor in the chain receives the returned object\nfrom the previous interceptor.\n Exception handling is the same as on the consumer side, so the same caveats apply\nhere as well. The ProducerInterceptor.onAcknowledgement() method is called when\nthe broker acknowledges the record. If sending the record fails, onAcknowledgement\nis called at that point as well.\n Here’s a simple logging ProducerInterceptor example (found in src/main/java/\nbbejeck/chapter_7/interceptors/ZMartProducerInterceptor.java).\npublic class ZMartProducerInterceptor implements\n➥ ProducerInterceptor<Object, Object> {\n// some details left out for clarity\nprivate static final Logger LOG =\n➥ LoggerFactory.getLogger(ZMartProducerInterceptor.class);\n@Override\npublic ProducerRecord<Object, Object> onSend(ProducerRecord<Object,\n➥ Object> record) {\nLOG.info(\"ProducerRecord being sent out {} \", record);\nreturn record;\n}\n@Override\npublic void onAcknowledgement(RecordMetadata metadata,Exception exception) {\nif (exception != null) {\nLOG.warn(\"Exception encountered producing record {}\",\n➥ exception);\n} else {\nLOG.info(\"record has been acknowledged {} \", metadata);\n}\n}\nListing 7.2\nLogging producer interceptor\nLogs the consumer records and metadata\nbefore the records are processed\nLogs the commit information once\nthe Kafka Streams consumer\ncommits offsets to the broker\nLogs right before \nthe message is sent \nto the broker\nLogs broker \nacknowledgement \nor whether error \noccurred (broker-\nside) during the \nproduce phase\n \n",
      "content_length": 2091,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "182\nCHAPTER 7\nMonitoring and performance\nThe ProducerInterceptor is specified with ProducerConfig.INTERCEPTOR_CLASSES\n_CONFIG and takes a Collection of one or more ProducerInterceptor classes.\nTIP\nWhen configuring interceptors in a Kafka Streams application, you need\nto prefix the consumer and producer interceptors’ property names with\nprops.put(StreamsConfig.consumerPrefix(ConsumerConfig.INTERCEPTOR\n_CLASSES_CONFIG) and StreamsConfig.producerPrefix(ProducerConfig\n.INTERCEPTOR_CLASSES_CONFIG), respectively.\nIf you want to see the interceptors in action, src/main/java/bbejeck/chapter_7/\nStockPerformanceStreamsAndProcessorMetricsApplication.java uses a consumer inter-\nceptor, and src/main/java/bbejeck/chapter_7/ZMartKafkaStreamsAd-vancedReqs-\nMetricsApp.java uses a producer interceptor. Both classes include the configuration\nrequired for using interceptors.\n As a side note, because interceptors work on every record in the Kafka Streams\napplication, the output of the logging interceptors is significant. The interceptor\nresults are output to consumer_interceptor.log and producer_interceptor.log, found\nin the logs directory at the base of the source code installation.\n We’ve spent some time looking at metrics on consumer performance and how you\ncan intercept records coming into and out of a Kafka Streams application. But this\ninformation is coarse grained and outside of a Kafka Streams application. Let’s now go\ninside a Kafka Streams application and see what’s going on under the hood. The next\nstep is to measure performance inside the topology by gathering metrics. \n7.2\nApplication metrics\nWhen it comes to measuring the performance of an application, you can get a sense\nof how long it takes to process one record, and measuring end-to-end latency is\nundoubtedly a good indicator of overall performance. But if you want to improve per-\nformance, you’ll need to know exactly where things are slowing down.\n Measuring performance is essential for streaming applications. The mere fact\nthat you’re using a streaming application implies you want to process data or infor-\nmation as it becomes available. It stands to reason that if your business needs dictate\na streaming solution, you’ll want the most efficient and correct streaming process you\ncan get.\n Before we discuss the actual metrics we’ll focus on, let’s revisit one of the applica-\ntions you built in chapter 3, the advanced ZMart application. That app is a good can-\ndidate for metrics tracking because there are several processing nodes, so we’ll use\nthat topology for this example. Figure 7.6 shows the topology you created.\n \n \n \n \n \n",
      "content_length": 2619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "183\nApplication metrics\nKeeping the ZMart topology in mind, let’s take a look at the categories of metrics:\nThread metrics\n– Average time for commits, poll, process operations\n– Tasks created per second, tasked closed per second\nTask metrics\n– Average number of commits per second\n– Average commit time\nProcessor node metrics\n– Average and max processing time\n– Average number of process operations per second\n– Forward rate\nState store metrics\n– Average execution time for put, get, and flush operations\n– Average number put, get, and flush operations per second\nNote that this isn’t an exhaustive list of the possible metrics. I’ve chosen these because\nthey offer excellent coverage of the most common performance scenarios. You can\nfind a full list on the Confluent website: http://mng.bz/4bcA.\nFigure 7.6\nZMart advanced application topology with a lot of nodes\nPatterns\nMasking\nSource\nElectronics\nsink\nCafe\nsink\nPatterns\nsink\nPurchases\nsink\nRewards\nBranch\nprocessor\nFiltering\nprocessor\nCafe\nprocessor\nElectronics\nprocessor\nRewards\nsink\nSelect-key\nprocessor\n \n",
      "content_length": 1067,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "184\nCHAPTER 7\nMonitoring and performance\n Now that we have what we’re going to measure, let’s look at how to capture the\ninformation.\n7.2.1\nMetrics configuration\nKafka Streams already provides the mechanism for collecting performance metrics.\nFor the most part, you just need to provide some configuration values. Because the\ncollection of metrics does incur a performance cost, there are two levels, INFO and\nDEBUG. An individual metric may not be that expensive on its own, but when you con-\nsider that some metrics may involve every record flowing through the Kafka Streams\napplication, you can see how the impact on performance can add up.\n The metric levels are used like logging levels. When you’re troubleshooting an\nissue or observing how the application behaves, you’ll want more information, so you\ncan use the DEBUG level. Other times, you don’t need all the information, so you can\nuse the INFO level.\n Typically, you won’t want to use DEBUG in production, as the cost of performance\nwould be too high. Each of the previously listed metrics are available at different levels,\nas shown in table 7.1. As you can see, thread metrics are available at any level, whereas\nthe rest of the metric categories are only collected when using the DEBUG level.\nYou set the level when you set the configuration of your Kafka Streams application.\nThat setting has been there all along with your other application configurations;\nyou’ve accepted the default settings up to this point. The default level of metrics col-\nlection is INFO.\n Let’s update the configs in the advanced ZMart application and turn on the collec-\ntion of all metrics (src/main/java/bbejeck/chapter_7/ZMartKafkaStreamsAdvanced-\nReqsMetricsApp.java).\nprivate static Properties getProperties() {\nProperties props = new Properties();\nprops.put(StreamsConfig.CLIENT_ID_CONFIG,\n➥ \"metrics-client-id\");\nTable 7.1\nMetrics availability by levels\nMetrics category\nDEBUG\nINFO\nThread\nx\nx\nTask\nx\nProcessor node\nx\nState store\nx\nRecord cache\nx\nListing 7.3\nUpdating the configs for DEBUG metrics\nClient ID\n \n",
      "content_length": 2060,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "185\nApplication metrics\nprops.put(ConsumerConfig.GROUP_ID_CONFIG,\n➥ \"metrics-group-id\");\nprops.put(StreamsConfig.APPLICATION_ID_CONFIG,\n➥ \"metrics-app-id\");\nprops.put(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG,\n➥ \"DEBUG\");\nprops.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,\n➥ \"localhost:9092\");\nreturn props;\n}\nYou’ve now enabled the collection and recording of DEBUG-level metrics. The key\npoints I want you to remember from this section is there are built-in metrics to mea-\nsure the full scope of a Kafka Steams application, and that you should carefully con-\nsider the performance impact before turning on metrics collection at the DEBUG level.\n Now that we’ve discussed what metrics are available and how they’re collected, the\nnext step is to observe the collected metrics. \n7.2.2\nHow to hook into the collected metrics\nThe metrics in a Kafka Streams application are collected and distributed to metrics\nreporters. As you might have guessed, Kafka Streams provides a default reporter via\nJava Management Extensions (JMX).\n Once you’ve enabled collecting metrics at the DEBUG level, you have nothing left to\ndo but observe them. One thing to keep in mind is that JMX only works with live run-\nning applications, so the metrics we’ll look at will be when the application is running.\nTIP\nYou can also access metrics programmatically. For an example of pro-\ngrammatic metrics access, take a look at src/main/java/bbejeck/chapter_7/\nStockPerformanceStreamsAndProcessorMetricsApplication.java.\nYou’re likely familiar with using JMX or have at least heard of it. In the next section,\nI’ll provide a brief overview on how to get started using JMX, but if you’re an experi-\nenced JMX user, feel free to skip this next section. \n7.2.3\nUsing JMX\nJMX is a standard way of viewing the behavior of programs running on the Java VM.\nYou can also use JMX to see how the Java Virtual Machine (Java VM) is performing. In\na nutshell, JMX gives you the infrastructure to expose parts of your running program.\n Fortunately, you won’t need to write any code to do this monitoring. You’ll just\nconnect either Java VisualVM (http://mng.bz/euif), JConsole (http://mng.bz/Ea71),\nor Java Mission Control (http://mng.bz/0r5B).\n \n \n \nGroup ID\nApplication ID\nSets the metrics \nrecording level \nto DEBUG\nSets the connection \nfor the brokers\n \n",
      "content_length": 2318,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "186\nCHAPTER 7\nMonitoring and performance\nTIP\nJava Mission Control (JMC) is powerful and can be a great tool for moni-\ntoring, but it requires a commercial license for use in production. Because\nJMC ships with the JDK, you can start JMC directly from the command line\nwith the command jmc (assuming the JDK bin directory is on your path).\nAdditionally, you’ll need to add these flags when starting your Kafka stream-\ning application: -XX:+UnlockCommercialFeatures -XX:+FlightRecorder.\nAs JConsole is the most straightforward approach, we’ll start with it for now.\nSTARTING JCONSOLE\nJConsole ships with the JDK, so if you’ve got Java installed, you already have JConsole.\nStarting JConsole is as simple as running jconsole from a command prompt (assum-\ning Java is on your path). Once it’s started, a GUI will come up, as shown in figure 7.7.\nOnce JConsole is up, the next step is to use it to look at some metric data!\nSTARTING TO MONITOR A RUNNING PROGRAM\nIf you look at the center of the JConsole GUI, you’ll see a New Connection dialog box.\nFigure 7.8 shows the starting point for JConsole. For now, we’re only concerned with\nthe Java processes listed in the Local Process section.\nNOTE\nYou can use JConsole to monitor remote applications, and you can\nsecure access to JMX. You can see the Remote Process, Username, and Pass-\nword text boxes in figure 7.8. In this book, however, we’ll limit our discussion\nto local access during development. The internet is full of instructions on\nremote and secure JConsole access, and Oracle’s documentation is a great\nstarting point (http://mng.bz/Ea71).\nWhat is JMX?\nOracle says the following in “Lesson: Overview of the JMX Technology” http://mng\n.bz/Ej29):\nThe Java Management Extensions (JMX) technology is a standard part of the Java\nPlatform, Standard Edition (Java SE platform), added to the platform in the 5.0\nrelease.\nThe JMX technology provides a simple, standard way of managing resources such\nas applications, devices, and services. Because the JMX technology is dynamic, you\ncan use it to monitor and control resources as they are created, installed, and imple-\nmented. You can also use the JMX technology to monitor and manage the Java Vir-\ntual Machine (Java VM).\nThe JMX specification defines the architecture, design patterns, APIs, and services\nin the Java programming language for management and monitoring of applications\nand networks.\n \n",
      "content_length": 2399,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "187\nApplication metrics\nFigure 7.7\nJConsole start menu\nFigure 7.8\nJConsole connect to program\nSelect Connection > New Connection to\nrefresh the Local Process dialog below.\nSelect the application\nprocess from the\ndialog here.\n \n",
      "content_length": 227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "188\nCHAPTER 7\nMonitoring and performance\nIf you haven’t already started your Kafka Streams application, do so now. Then, to get\nyour application to show up in the Local Process window, click Connection > New\nConnection (as in figure 7.8). The processes listed under Local Process should refresh,\nand you’ll see your Kafka Streams application. Double-click the Kafka Streams applica-\ntion process.\n Chances are that after you double-click the program you want to connect to, you’ll\nbe greeted with a warning similar to the one in figure 7.9. Because you’re on your\nlocal machine, you can click the Insecure Connection button.\nNow you’re all set to look at the metrics being collected by your Kafka Streams appli-\ncation. The next step is to look at the information available.\nWARNING\nYou’re using an insecure connection for development on a local\nmachine. In practice, you should always secure access to any remote services\naccessing the internal state of your application. \nVIEWING THE INFORMATION\nOnce you’re connected, you’ll see a GUI screen looking something like figure 7.10.\nJConsole offers several handy options for peeking inside the internals of running\napplications.\n Of the Overview, Memory, Threads, Classes, VM Summary, and MBeans tabs, you’re\nonly going use the MBeans tab. MBeans contain the collected statistics about the per-\nformance of your Kafka Streams program. The other tabs provide relevant informa-\ntion, but it’s info that relates more to overall application health and how the program\nFigure 7.9\nJConsole connect warning, no SSL\nYou’re running on your local development\nmachine, so select this button.\n \n",
      "content_length": 1631,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "189\nApplication metrics\nis utilizing resources. The metrics collected in the MBeans contain information about\nthe internal performance of the topology.\n That’s the end of your introduction to using JConsole. The next step is to start\nviewing the recorded metrics for the topology. \n7.2.4\nViewing metrics\nFigure 7.11 shows how you can view some metrics via JConsole while running the\nZMart application (src/main/java/bbejeck/chapter_7/ZMartKafkaStreamsAdvanced-\nReqsMetricsApp.java). As you can see, you can drill down to all processors and nodes\nin the topology to view performance (either throughput or latency).\nTIP\nBecause JMX only works with running applications, some of the example\napplications in src/main/java/bbejeck/chapter_7 will run continuously so\nthat you can play with the metrics. As a result, you’ll need to explicitly stop\nthem either in the IDE or by pressing Ctrl-C from the command line.\nFigure 7.10\nJConsole started\nSelect the MBeans tab. You could use the other\ntabs to perform other monitoring tasks, but you\nwant to look at your collected metrics.\nYou’re going to use the metrics found\nunder the kafka.streams folder.\nHere, you can see a good selection\nof MBeans for different metrics.\n \n",
      "content_length": 1213,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "190\nCHAPTER 7\nMonitoring and performance\nFigure 7.11 shows the process-rate metric, which tells you the average number of\nrecords processed per millisecond. If you look at the upper right, under Attribute\nValue, you can see that the process rate averages 3.537 records per millisecond (3,537\nrecords per second). Additionally, as discussed earlier, you can see the producer and\nconsumer metrics from JConsole.\nTIP\nAlthough the provided metrics are comprehensive, there may be cases\nwhere you want custom metrics. This is a low-level detail and probably not a\nFigure 7.11\nJConsole metrics for ZMart\nHere’s the\nprocess-rate value.\nAll the nodes in the topology\nwith one of the branch\nprocessors expanded\n \n",
      "content_length": 704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "191\nMore Kafka Streams debugging techniques\nvery common use case, so we won’t walk through an example in detail. But\nyou can look at the StockPerformanceMetricsTransformer.init method for\nan example of how you can add a custom metric and the StockPerformance-\nMetricsTransformer.transform method for an example of how you can uti-\nlize it. The StockPerformanceMetricsTransformer is found in src/main/\njava/bbejeck/chapter_7/transformer/StockPerformanceMetrics-Transformer\n.java.\nNow that you’ve seen how to view Kafka Streams metrics, let’s move on to other useful\ntechniques for observing what’s going on in an application. \n7.3\nMore Kafka Streams debugging techniques\nWe’ll now look at some more ways you can observe and debug Kafka streaming appli-\ncations. The previous section was more about performance; the techniques we’ll look\nat in this section focus on getting notified about various states of the application and\nviewing the structure of the topology.\n7.3.1\nViewing a representation of the application\nAfter your application is up and running, you might run into situations where you\nneed to debug it. You might like to get a second pair of eyes on the job, but for what-\never reason, you can’t share the code. Or you’d like to see the TopicPartition\nassigned to the tasks of the application.\n The Topology.describe() method provides general information on the structure\nof the application. It prints out information regarding the structure of the program,\nincluding any internal topics created to support repartitioning. Figure 7.12 displays\nFigure 7.12\nDisplaying node names, associated child nodes, and other info\nStreamsTask taskId: 0_0\nProcessorTopology:\nTxn-Source:\ntopics: [stock-transactions]\nchildren: [Txn-Processor]\nTxn-Processor:\nchildren: [CoGrouping-Processor]\nCoGrouping-Processor:\nstates: [tupleCoGroupStore]\nchildren: [Tuple-Sink, Print]\nTuple-Sink:\ntopic: cogrouped-results\nPrint:\nEvents-Source:\ntopics: [events]\nchildren: [Events-Processor]\nEvents-Processor:\nchildren: [CoGrouping-Processor]\nCoGrouping-Processor:\nstates: [tupleCoGroupStore]\nchildren: [Tuple-Sink, Print]\nTuple-Sink:\ntopic: cogrouped-results\nPrint:\nProcessor name\nAssociated state store\nChild nodes\nProcessor name\nTopic name\nThe taskId\n \n",
      "content_length": 2236,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "192\nCHAPTER 7\nMonitoring and performance\nthe results of calling describe on the CoGroupingListeningExampleApplication\nfrom chapter 7 (src/main/java/bbejeck/chapter_7/CoGroupingListeningExample-\nApplication.java).\n As you can see, the Topology.describe() method prints out a nice, concise\nview of the application structure. Notice that the CoGroupingListeningExample-\nApplication used the Processor API, so all the nodes in the topology have the names\nyou chose. With the Kafka Streams API, the names of the nodes are a little more\ngeneric:\nKSTREAM-SOURCE-0000000000:\ntopics:\n[transactions]\nchildren:\n[KSTREAM-MAPVALUES-0000000001]\nTIP\nWhen you’re using the Kafka Streams DSL API, you don’t directly use\nthe Topology class, but it’s easily accessed. If you want to print the physical\ntopology of the application, use the StreamsBuilder.build() method, which\nreturns a Topology object, and then you can call Topology.describe() as\nyou just saw.\nGetting information about the StreamThread objects, which shows runtime informa-\ntion, in your application can be useful as well. To access the StreamThread info, use\nthe KafkaStreams.localThreadsMetadata() method. \n7.3.2\nGetting notification on various states of the application\nWhen you start your Kafka Streams application, it doesn’t automatically begin process-\ning data—some coordination has to happen first. The consumer needs to fetch metadata\nand subscription information; the application needs to start StreamThread instances and\nassign TopicPartitions to StreamTasks.\n This process of assigning or redistributing tasks (workload) is called rebalancing.\nRebalancing means Kafka Streams can autoscale up or down. This is a crucial strength—\nyou can add new application instances while an existing application is already running,\nand the rebalancing process will redistribute the workload.\n For example, suppose you have a Kafka Streams application with two source topics,\nand each topic has two partitions, resulting in four TopicPartition objects needing\nassignment. You initially start the application with one thread. Kafka Streams deter-\nmines the number of tasks to create by taking the max partition size among all input\ntopics. In this case, each topic has two partitions, so the max is two, and you end up\nwith two tasks. The rebalance process then assigns the two tasks two TopicPartition\nobjects each.\n After running the app for a little while, you decide you want to process records\nmore quickly. All you need to do is start another version of the application with the\nsame application ID, and the rebalance process will distribute the load across the new\napplication thread, resulting in the two tasks being assigned across both threads.\n \n",
      "content_length": 2704,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "193\nMore Kafka Streams debugging techniques\nYou’ve just doubled the scale of your application while the original version is still run-\nning—there’s no need to shut the initial application down.\n Other causes of rebalancing include another Kafka Streams instance (with the same\napplication ID) starting or stopping, adding partitions to a topic, or, in the case of a\nregex-defined source node, adding or removing topics matching the regex pattern.\n During the rebalance phase, external interaction temporarily pauses until the\napplication has completed the assignment of topic partitions to stream tasks, so you’d\nlike to be aware of this point in the lifecycle of the application. For example, the que-\nryable state stores are unavailable, so you’d like to be able to restrict requests to view\nthe contents of the store until the stores are available again.\n But how can you check whether your other applications are going through a rebal-\nance? Fortunately, Kafka Streams provides just such a mechanism, the StateListener,\nwhich we’ll look at next. \n7.3.3\nUsing the StateListener\nA Kafka Streams application can be in one of six states at any point in time. Figure 7.13\nshows the possible valid states for a Kafka Streams application. As you can see, there\nare a few state-change scenarios we could discuss, but we’re going to focus on the tran-\nsition between running and rebalancing. The transition between these two states is\nthe most frequent and has the most impact on performance, because during the\nrebalancing phase no processing occurs.\nTo capture these state changes, you’ll use the KafkaStreams.setStateListener\nmethod, which takes an instance of the StateListener interface. It’s a single-method\nFigure 7.13\nPossible states of a Kafka Streams application\nCreated\nRebalancing\nRunning\nError\nPending shutdown\nNot running\nSix states of a Kafka Streams application\nThe arrows show the\ndirections of valid\ntransitions.\nOnly the pending-shutdown state\ncan go into the not-running state.\nOnly the running and\nrebalancing states can\ngo into an error state.\n \n",
      "content_length": 2063,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "194\nCHAPTER 7\nMonitoring and performance\ninterface, so you can use Java 8 lambda syntax, as follows (found in src/main/java/\nbbejeck/chapter_7/ZMartKafkaStreamsAdvancedReqsMetricsApp.java).\nKafkaStreams.StateListener stateListener = (newState, oldState) -> {\nif (newState == KafkaStreams.State.RUNNING &&\n➥ oldState == KafkaStreams.State.REBALANCING) {\nLOG.info(\"Application has gone from REBALANCING to RUNNING \");\nLOG.info(\"Topology Layout {}\",\n➥ streamsBuilder.build().describe());\n}\n};\nTIP\nListing 7.4, running ZMartKafkaStreamsAdvancedReqsMetricsApp.java,\ninvolves viewing JMX metrics and the state-transition notification, so I’ve\nturned off printing the streaming results to the console. You’re writing solely\nto Kafka topics. When you run the app, you should see the listener output in\nthe console.\nFor your first StateListener implementation, you’ll log the state change to the con-\nsole. In section 7.3.1, when we discussed printing the topology structure, I spoke of\nneeding to wait until the application has completed rebalancing. That’s what you do in\nthe listing 7.4: print out the structure once all tasks and assignments are complete.\n Let’s take this example a little further and show how to signal when the application\nis going into a rebalancing state. You can update your code to handle this additional\nstate transition as follows (found in src/main/java/bbejeck/chapter_7/ZMartKafka-\nStreamsAdvancedReqsMetricsApp.java).\nKafkaStreams.StateListener stateListener = (newState, oldState) -> {\nif (newState == KafkaStreams.State.RUNNING &&\n➥ oldState == KafkaStreams.State.REBALANCING) {\nLOG.info(\"Application has gone from REBALANCING to RUNNING \");\nLOG.info(\"Topology Layout {}\", streamsBuilder.build().describe());\n}\nif (newState == KafkaStreams.State.REBALANCING) {\nLOG.info(\"Application is entering REBALANCING phase\");\n}\n};\nEven though you’re using simple logging statements, it should be evident how you can\nadd more-sophisticated logic to handle the state changes within your application.\nListing 7.4\nAdding a state listener\nListing 7.5\nUpdating the state listener when REBALANCING\nChecks that you’re\ntransitioning from\nREBALANCING to\nRUNNING\nPrints out the structure\nof the topology\nAdds an action when entering\nthe rebalancing phase\n \n",
      "content_length": 2262,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "195\nMore Kafka Streams debugging techniques\nNOTE\nBecause Kafka Streams is a library and not a framework, you can run a\nsingle instance on one server. If you do run multiple applications on different\nmachines, you’ll only see results from state changes on your local machine.\nThe critical point of this section is that you can hook into the current state of your\nKafka Streams application, making it less of a black-box operation.\n Next, we’ll look at rebalancing in a little more in depth. Although the ability to\nautomatically rebalance the workload is a strength of Kafka Streams, you’ll likely want\nto keep the number of rebalances to a minimum. When a rebalance occurs, you’re\nnot processing data, and you’d like to have your application processing data as much\nas possible. \n7.3.4\nState restore listener\nYou learned in chapter 4 about state stores and the importance of having your state\nstores backed up, in case of a failure. In Kafka Streams, we use topics frequently called\nchangelogs as the backup for the state stores.\n The changelog records the updates to the state store as the update occurs. When a\nKafka Streams application fails, or you restart it, the state store can recover from the\nlocal state files, as shown in figure 7.14.\nIn some circumstances, however, you may need to do a full recovery of state store from\nthe changelog, such as if you’re running your Kafka Streams application in a stateless\nenvironment like Mesos, or if you encounter a severe failure and the files on local disk\nFigure 7.14\nRestoring a state store from clean start/recovery\nProcessor\nState\nstore\nRecords in the changelog are\nconsumed on startup and used\nto restore the state store.\nOn a clean startup with no persisted\nlocal state, the state is fully restored from\nthe backing topic or changelog.\nBacking changelog\nState store restoration\n \n",
      "content_length": 1838,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "196\nCHAPTER 7\nMonitoring and performance\nare wiped out. Depending on the amount of data you have to restore, this restoration\nprocess could take a non-trivial amount of time.\n During this recovery period, any state stores you have exposed for querying are\nunavailable, so it would be nice to get an idea of how long this restoration process is\nlikely to take and how progress is coming along. Additionally, if you have a custom\nstate store, you’d like notification of when the restore is starting and ending so you\ncan do any necessary setup or teardown tasks.\n The StateRestoreListener interface, much like the StateListener, allows notifi-\ncation of what’s going on inside the application. StateRestoreListener has three meth-\nods: onRestoreStart, onBatchRestored, and onRestoreEnd. The KafkaStreams.set-\nGlobalRestoreListener method is used to specify the global restore listener to use.\nNOTE\nThe provided StateRestoreListener is shared application-wide and is\nexpected to be stateless. If you need to keep track of any state in the listener,\nyou’ll need to provide the synchronization.\nLet’s walk through the listener code to get an idea of how this notification process can\nwork. We’ll start with the declaration of the variable and the onRestoreStart method\n(found in src/main/java/bbejeck/chapter_7/restore/LoggingStateRestoreListener\n.java).\npublic class LoggingStateRestoreListener implements StateRestoreListener {\nprivate static final Logger LOG =\n➥ LoggerFactory.getLogger(LoggingStateRestoreListener.class);\nprivate final Map<TopicPartition, Long> totalToRestore =\n➥ new ConcurrentHashMap<>();\nprivate final Map<TopicPartition, Long> restoredSoFar =\n➥ new ConcurrentHashMap<>();\n@Override\npublic void onRestoreStart(TopicPartition topicPartition,\n➥ String store, long start, long end) {\nlong toRestore = end - start;\ntotalToRestore.put(topicPartition, toRestore);\nLOG.info(\"Starting restoration for {} on topic-partition {}\n➥ total to restore {}\", store, topicPartition, toRestore);\n}\n// other methods left out for clarity covered below\n}\nYour first steps are to create two ConcurrentHashMap instances for keeping track of\nrestoration progress. In the onRestoreStart method, you store the total number of\nrecords you need to restore and log the fact that you’re starting.\nListing 7.6\nA logging restore listener\nCreates Concurrent-\nHashMap instances \nfor keeping track of \nrestore progress\nStores the total \namount to restore \nfor the given \nTopicPartition\n \n",
      "content_length": 2471,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "197\nMore Kafka Streams debugging techniques\n Next, let’s move on to the code that handles each batch restored (found in\nsrc/main/java/bbejeck/chapter_7/restore/LoggingStateRestoreListener.java).\n@Override\npublic void onBatchRestored(TopicPartition topicPartition,\n➥ String store, long start, long batchCompleted) {\nNumberFormat formatter = new DecimalFormat(\"#.##\");\nlong currentProgress = batchCompleted +\n➥ restoredSoFar.getOrDefault(topicPartition, 0L);\ndouble percentComplete =\n➥ (double) currentProgress / totalToRestore.get(topicPartition);\nLOG.info(\"Completed {} for {}% of total restoration for {} on {}\",\nbatchCompleted,\n➥ formatter.format(percentComplete * 100.00),\n➥ store, topicPartition);\nrestoredSoFar.put(topicPartition, currentProgress);\n}\nThe restoration process uses an internal consumer to read the changelog topic, so it\nfollows that the application restores records in batches from each consumer.poll()\nmethod call. As a consequence, the maximum size of any batch will be equal to the\nmax.poll.records setting.\n The onBatchRestored method is called after the restore process has loaded the lat-\nest batch into the state store. First, you add the size of the current batch to the accumu-\nlated restore count. Then, you calculate the percentage of restoration completed and\nlog the results. Finally, you store the new total number of records, computed earlier.\n The last step we’ll cover is when the restoration process completes (found in\nsrc/main/java/bbejeck/chapter_7/restore/LoggingStateRestoreListener.java).\n@Override\npublic void onRestoreEnd(TopicPartition topicPartition,\n➥ String store, long totalRestored) {\nLOG.info(\"Restoration completed for {} on\n➥ topic-partition {}\", store, topicPartition);\nrestoredSoFar.put(topicPartition, 0L);\n}\nOnce the application completes the recovery process, you make one final call to the\nlistener with the total number of records restored. In this example, you log the fin-\nished state and update the full restoration count map to 0.\n Finally, you can use the LoggingStateRestoreListener in your application as\nfollows (found in src/main/java/bbejeck/chapter_7/CoGroupingListeningExample-\nApplication.java).\nListing 7.7\nHandling onBatchRestored\nListing 7.8\nMethod called when restoration is completed\nCalculates the total \nnumber of records \nrestored\nDetermines the \npercentage of \nrestoration \ncompleted\nLogs the percent \nrestored\nStores the\nnumber of\nrecords\nrestored\nso far\nKeeps track of restore progress \nfor a TopicPartition\n \n",
      "content_length": 2497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "198\nCHAPTER 7\nMonitoring and performance\nkafkaStreams.setGlobalStateRestoreListener(new LoggingStateRestoreListener());\nThis is an example of using a StateRestoreListener. In chapter 9, you’ll see an\nexample that includes a graphical representation of the restore progress.\nTIP\nTo view the log file generated by running the CoGroupingListening-\nExampleApplication example, look for a log file named logs/state_restore_\nlistener.log in the root directory where you installed the source code. \n7.3.5\nUncaught exception handler\nI think it’s fair to say that every developer, from time to time, has encountered an\nunaccounted-for Exception and the big stack trace in the console/log as your pro-\ngram suddenly quits. Although this situation doesn’t quite fit into a “monitoring”\nexample, the ability to get a notification and handle any cleanup when the unex-\npected occurs is good practice. Kafka Streams provides KafkaStreams.setUncaught-\nExceptionHandler for dealing with these unexpected errors (found in src/main/\njava/bbejeck/chapter_7/CoGroupingListeningExampleApplication.java)\nkafkaStreams.setUncaughtExceptionHandler((thread, exception) -> {\nCONSOLE_LOG.info(\"Thread [\" + thread + \"]\n➥ encountered [\" + exception.getMessage() +\"]\");\n});\nThis is definitely a bare-bones implementation, but it serves to demonstrate where you\ncan set a hook for dealing with unexpected errors, either by logging the error as\nshown here or by performing any required cleanup and shutting down the streams\napplication.\n That wraps up our coverage of monitoring Kafka Streams applications. \nSummary\nTo monitor Kafka Streams, you’ll need to look at the Kafka brokers as well.\nYou should enable metrics reporting from time to time to see the how the per-\nformance of the application is doing.\nPeeking under the hood is required, and sometimes you’ll need to go to lower\nlevels and use command-line tools included with Java, such as jstack (thread\ndumps) and jmap/jhat (for heap dumps) to understand what your application\nis doing.\nIn this chapter, we focused on observing behavior. In the next chapter, we’ll shift our\nfocus to making sure the application handles errors consistently and adequately. We’ll\nalso make sure that it provides expected behavior by doing regular testing.\nListing 7.9\nSpecifying the global restore listener\nListing 7.10\nBasic uncaught exception handler\n \n",
      "content_length": 2366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "199\nTesting a Kafka Streams\napplication\nSo far, we’ve covered the essential building blocks for creating a Kafka Streams\napplication. But there’s one crucial part of application development I’ve left out\nuntil now: how to test your application. One of the critical concepts we’ll focus on\nis placing your business logic in standalone classes that are entirely independent of\na Kafka Streams application, because that makes your logic much more accessible\nto test. I expect you’re aware of the importance of testing, but we’ll review my top\ntwo reasons for why testing is just as necessary as the development process itself.\n First, as you develop your code, you’re creating an implicit contract of what you\nand others can expect about how the code will execute. The only way to prove that\nthe code works is by testing thoroughly, so you’ll use testing to provide a good\nThis chapter covers\nTesting a topology\nTesting individual processors and transformers\nIntegration testing with an embedded Kafka \ncluster\n \n",
      "content_length": 1013,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "200\nCHAPTER 8\nTesting a Kafka Streams application\nbreadth of possible inputs and scenarios to make sure your code works appropriately\nunder reasonable circumstances.\n The second reason you need an excellent suite of tests is to deal with the inevitable\nchanges in software. Having a good set of tests gives you immediate feedback when\nyour new code has broken the expected set of behaviors. Additionally, when you do a\nmajor refactor or add new functionality, passing tests give you a level of confidence\nabout the impact of your changes (provided you have good tests).\n Even once you understand the importance of testing, testing a Kafka Streams\napplication isn’t always straightforward. You still have the option of running a simple\ntopology and observing the results, but there’s one drawback with that approach.\nYou’ll want a suite of repeatable tests that you can run at any time, and as part of a build,\nso you’d like the ability to test your applications without a Kafka cluster and Zoo-\nKeeper ensemble.\n That’s what we’ll cover in this chapter. First, you’ll see how you can test a topology\nwithout Kafka running, so you can see the entire topology working from a unit test.\nYou’ll also learn how to test a processor or transformer independently and mock any\nrequired dependencies.\nNOTE\nYou likely have experience testing with mock objects, but if not, Wiki-\npedia’s article provides a good introduction: https://en.wikipedia.org/wiki/\nMock_object.\nAlthough unit testing is critical for repeatability and quick feedback, integration\ntesting is important as well, because sometimes you’ll need to see the moving parts\nof your application in action. For example, consider the case of rebalancing, an\nessential part of a Kafka Streams application. Getting rebalancing to work in a unit\ntest is nearly impossible. Table 8.1 summarizes the differences between unit and inte-\ngration testing.\nYou need to trigger an actual rebalance under realistic conditions to test it. For those\nsituations, you’ll need to run with a live instance of a Kafka cluster. But you don’t want\nto rely on having an external cluster set up, so we’ll look at how you can use an\nembedded Kafka and ZooKeeper for integration testing.\nTable 8.1\nTesting approaches\nTest type\nPurpose\nTesting speed\nLevel of use\nUnit\nTesting individual parts of \nfunctionality in isolation\nFast\nLarge majority\nIntegration\nTesting integration points \nbetween whole systems\nLonger to run\nSmall minority\n \n",
      "content_length": 2460,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "201\nTesting a topology\n8.1\nTesting a topology\nThe first topology we built, in chapter 3, was relatively complicated. Figure 8.1 shows it\nagain, to refresh your memory.\nThe processing logic is pretty straightforward, but as you can see from the structure, it\nhas several nodes. It also has one key thing that will help demonstrate testing: it takes\none input—an initial purchase—and it performs several transformations. This will\nmake testing somewhat easy, in that you only need to supply a single purchase value,\nand you can confirm that all the appropriate transformations take place.\nTIP\nIn most cases, you’ll want to have your logic in separate classes so you can\nunit test the business logic separately from the topology. In the case of the\nZMart topology, most of the logic is simple and represented as Java 8 lambda\nexpressions, so in this case you’ll test the topology flow.\nYou’ll want a repeatable standalone test, so you’ll use the ProcessorTopologyTest-\nDriver, which will allow you to write such a test without needing Kafka to run the test.\nRemember, the ability to test your topology without a live Kafka instance makes testing\nfaster and more lightweight, leading to a quicker development cycle. Also note that\nthe ProcessorTopologyTestDriver is a generic testing framework that tests the indi-\nvidual Kafka Streams Topology objects you’ve created.\nFigure 8.1\nInitial complete topology for ZMart Kafka Streams program\nPatterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nThis topology takes a single input from\nthe source node and performs several\ntransformations, making this a great\ndemonstration of testing.\n \n",
      "content_length": 1650,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "202\nCHAPTER 8\nTesting a Kafka Streams application\nTIP\nIf you write your own projects using Kafka and Kafka Streams testing\ncode, it’s best if you use the all the dependencies that come with the sample\ncode.\nWhen you initially built this topology, you did it entirely within the ZMartKafka-\nStreamsApp.main method, which was fine for quick development at the time, but it\ndoesn’t lend itself to being testable. What you’ll do now is refactor the topology into a\nstandalone class, which will enable you to test the topology.\n The logic isn’t changing, and you’ll move the code as is, so I won’t show the con-\nversion here. Instead, I’ll point you to src/main/java/bbejeck/chapter_8/ZMart-\nTopology.java, where you can view it if you choose.\n With the code transitioned, let’s get on with constructing a test.\n8.1.1\nBuilding the test\nLet’s move on to building a unit test for the ZMart topology. You can use a standard\nJUnit test, and you’ll have some setup work to do before running the test (found in\nsrc/main/java/bbejeck/chapter_8/ZMartTopologyTest.java).\n \n \nUsing Kafka Streams’ testing utilities\nTo use Kafka Streams’ testing utilities, you’ll need to update your build.gradle file\nwith the following:\ntestCompile\ngroup:'org.apache.kafka', name:'kafka-streams',\n➥ version:'1.0.0', classifier:'test'\ntestCompile\ngroup:'org.apache.kafka', name:'kafka-clients',\n➥ version:'1.0.0', classifier:'test'\nIf you’re using Maven, use this code:\n<dependency>\n<groupId>org.apache.kafka</groupId>\n<artifactId>kafka-streams</artifactId>\n<version>1.0.0</version>\n<scope>test</scope>\n<classifier>test</classifier>\n</dependency>\n<dependency>\n<groupId>org.apache.kafka</groupId>\n<artifactId>kafka-clients</artifactId>\n<version>1.0.0</version>\n<scope>test</scope>\n<classifier>test</classifier>\n</dependency>\n \n",
      "content_length": 1794,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "203\nTesting a topology\n@Before\npublic\nvoid setUp() {\n// properties construction left out for clarity\nStreamsConfig streamsConfig = new StreamsConfig(props);\nTopology topology = ZMartTopology.build();\ntopologyTestDriver =\n➥ new ProcessorTopologyTestDriver(streamsConfig, topology);\n}\nThe critical point of listing 8.1 is the creation of the ProcessorTopologyTestDriver,\nwhich you’ll use in the following listing when you run your test (found in src/main/\njava/bbejeck/chapter_8/ZMartTopologyTest.java).\n@Test\npublic void testZMartTopology() {\n// serde creation left out for clarity\nPurchase purchase = DataGenerator.generatePurchase();\ntopologyTestDriver.process(\"transactions\",\nnull,\npurchase,\nstringSerde.serializer(),\npurchaseSerde.serializer());\nProducerRecord<String, Purchase> record =\n➥ topologyTestDriver.readOutput(\"purchases\",\nstringSerde.deserializer(),\npurchaseSerde.deserializer());\nPurchase expectedPurchase =\n➥ Purchase.builder(purchase).maskCreditCard().build();\nassertThat(record.value(), equalTo(expectedPurchase));\nThere are two critical sections in listing 8.2. Starting with topologyTestDriver\n.process, you feed a record into the transactions topic, because it’s the source for\nthe entire topology. With the topology loading completed, you can verify that the cor-\nrect actions have taken place. In the following line, using the topologyTestDriver\n.readOutput method, you read the record from the purchases topic, with one of the\nsink nodes defined in the topology. In the second-to-last line, you create the expected\noutput record, and on the final line, you assert that the results are what you expect.\nListing 8.1\nSetup method for topology test\nListing 8.2\nTesting the topology\nRefactored ZMart \ntopology: now you can \nget the topology from \nthe method call.\nCreates the ProcessorTopologyTestDriver\nCreates a test object; \nreuses the generation \ncode from running \nthe topology\nSends an initial record \ninto the topology\nReads a record from \nthe purchases topic\nConverts the \ntest object to \nthe expected \nformat\nVerifies that the record from the \ntopology matches the expected record\n \n",
      "content_length": 2111,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "204\nCHAPTER 8\nTesting a Kafka Streams application\n There are two other sink nodes in the topology, so let’s complete the test by verify-\ning you get the correct output from them (found src/test/java/bbejeck/chapter_8/\nZMartTopologyTest.java).\n@Test\npublic void testZMartTopology() {\n// continuing test from the previous section\nRewardAccumulator expectedRewardAccumulator =\n➥ RewardAccumulator.builder(expectedPurchase).build();\nProducerRecord<String, RewardAccumulator> accumulatorProducerRecord =\n➥ topologyTestDriver.readOutput(\"rewards\",\nstringSerde.deserializer(),\nrewardAccumulatorSerde.deserializer());\nassertThat(accumulatorProducerRecord.value(),\n➥ equalTo(expectedRewardAccumulator));\nPurchasePattern expectedPurchasePattern =\n➥ PurchasePattern.builder(expectedPurchase).build();\nProducerRecord<String, PurchasePattern> purchasePatternProducerRecord =\n➥ topologyTestDriver.readOutput(\"patterns\",\nstringSerde.deserializer(),\npurchasePatternSerde.deserializer());\nassertThat(purchasePatternProducerRecord.value(),\n➥ equalTo(expectedPurchasePattern));\n}\nAs you add another processing node to the test, you’ll see the same pattern as in list-\ning 8.3. You read records from each topic and verify your expectations with an assert\nstatement. The critical point to keep in mind with this test is that you now have a\nrepeatable test running a record through your entire topology, without the overhead\nof running Kafka.\n The ProcessorTopologyTestDriver also supports testing topologies with a state\nstore, so let’s look at how you’d accomplish that. \n8.1.2\nTesting a state store in the topology\nTo demonstrate testing a state store, you’ll refactor another class, StockPerformance-\nStreamsAndProcessorApplication, to have the Topology returned from a method\ncall. You’ll find the class in src/main/java/bbejeck/chapter_8/StockPerformance-\nStreamsProcessorTopology.java. I haven’t made any changes to the logic, so we won’t\nreview it here.\nListing 8.3\nTesting the rest of the topology\nReads a\nrecord from\nthe rewards\ntopic\nVerifies the rewards topic \noutput matches expectations\nReads a record from \nthe patterns topic\nVerifies the patterns topic \noutput matches expectations\n \n",
      "content_length": 2178,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "205\nTesting a topology\n The test setup is the same as in the previous test, so I’ll limit my explanations to the\nparts that are new (src/test/java/bbejeck/chapter_8/StockPerformanceStreams-\nProcessorTopologyTest.java).\nStockTransaction stockTransaction =\n➥ DataGenerator.generateStockTransaction();\ntopologyTestDriver.process(\"stock-transactions\",\nstockTransaction.getSymbol(),\nstockTransaction,\nstringSerde.serializer(),\nstockTransactionSerde.serializer());\nKeyValueStore<String, StockPerformance> store =\n➥ topologyTestDriver.getKeyValueStore(\"stock-performance-store\");\nassertThat(store.get(stockTransaction.getSymbol()),\n➥ notNullValue());\nAs you can see, the last assert line quickly verifies that your code is using the state\nstore as expected. You’ve seen the ProcessorTopologyTestDriver in action, and\nyou’ve seen how you can achieve end-to-end testing of a topology. The topologies you\ntest can be very simple, with one processing node, or very complex, consisting of sev-\neral sub-topologies. Even though you’re doing this testing without a Kafka broker,\nmake no mistake: this is a full test of the topology that will exercise all parts, including\nserializing and deserializing records.\n You’ve seen how you can do end-to-end testing of a topology. But you’ll also want\nto test the internal logic of your Processor and Transformer objects. Testing an\nentire topology is great, but verifying the behavior inside each class requires a more\nfine-grained approach, which we’ll cover next. \n8.1.3\nTesting processors and transformers\nTo verify the behavior inside a single class requires a true unit test, where there’s only\none class under test. Writing a unit test for a Processor or Transformer shouldn’t be\nvery challenging, but remember that both classes have a dependency on the Processor-\nContext for obtaining any state stores and scheduling punctuation actions.\n You don’t want to create a real ProcessorContext object, but rather a stand-in you\ncan use for testing purposes: a mock object. When it comes to using a mock object,\nyou can follow two paths.\n One option is to use a mock object framework such as Mockito (http://site\n.mockito.org) to generate mock objects in your test. Another option is to use the\nMockProcessorContext object found in the same test library as ProcessorTopology-\nTestDriver. Which one you use will depend on how you need to use them.\nListing 8.4\nTesting the state store\nGenerates a \ntest record\nProcesses the record \nwith the test driver\nRetrieves the\nstate store from\nthe test topology\nAsserts the store contains \nthe expected value\n \n",
      "content_length": 2579,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "206\nCHAPTER 8\nTesting a Kafka Streams application\n If you need the mock object strictly as a placeholder for a real dependency, con-\ncrete mocks (mocks not created from a framework) are a good choice. But if you want\nto verify the parameters passed to a mock, the value returned, or any other behavior,\nusing a mock object generated by a framework is a good choice. Mock object frame-\nworks (like Mockito) come with a rich API for setting expectations and verifying\nbehavior, saving you development time and speeding up your testing process.\n In listing 8.5, you’ll use both types of mock objects. You’ll use the Mockito frame-\nwork to create the ProcessorContext mock, because you want to verify parameters\nduring the init call as well as validate that you’re forwarding the expected values\nfrom the punctuate() method. You’ll also use a custom mock object for the key/value\nstore, which you’ll see in action as we step through the code example.\n In this listing, you’ll test a Processor, using mock objects. You’ll start with a test\nfor the AggregatingMethodHandleProcessor named AggregatingMethodHandle-\nProcessorTest, located in src/test/java/bbejeck_chapter6/processor/cogrouping/.\nFirst, you want to verify the parameters used in the init method (see src/test/java/\nbbejeck/chapter_6/AggregatingMethodHandleProcessorTest.java).\n// some details left out for clarity\nprivate ProcessorContext processorContext =\n➥ mock(ProcessorContext.class);\nprivate MockKeyValueStore<String, Tuple<List<ClickEvent>,\n➥ List<StockTransaction>>> keyValueStore =\n➥ new MockKeyValueStore<>();\nprivate AggregatingMethodHandleProcessor processor =\n➥ new AggregatingMethodHandleProcessor();\n@Test\n@DisplayName(\"Processor should initialize correctly\")\npublic void testInitializeCorrectly() {\nprocessor.init(processorContext);\nverify(processorContext).schedule(eq(15000L), eq(STREAM_TIME),\n➥isA(Punctuator.class));\nverify(processorContext).getStateStore(TUPLE_STORE_NAME);\n}\nThis first test is a simple one: you call the init method on the processor under test\nwith the mocked ProcessorContext. You then validate the parameters used to sched-\nule the punctuate method, and that the state store is retrieved.\n Next, let’s test the punctuate method to verify that the records are forwarded as\nexpected (found in src/test/java/bbejeck/chapter_6/AggregatingMethodHandle-\nProcessorTest.java).\n \nListing 8.5\nTesting the init method\nMocks the ProcessorContext \nwith Mockito\nA mock \nKeyValueStore \nobject\nThe class under test\nCalls the init method on the \nprocessor, triggering method \ncalls on ProcessorContext\nVerifies the parameters for the \nProcessorContext.schedule method\nVerifies retrieving\nthe state store\n \n",
      "content_length": 2687,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "207\nTesting a topology\n@Test\n@DisplayName(\"Punctuate should forward records\")\npublic void testPunctuateProcess(){\nwhen(processorContext.getStateStore(TUPLE_STORE_NAME))\n.thenReturn(keyValueStore);\nprocessor.init(processorContext);\nprocessor.process(\"ABC\", Tuple.of(clickEvent, null));\nprocessor.process(\"ABC\", Tuple.of(null, transaction));\nTuple<List<ClickEvent>,List<StockTransaction>> tuple =\n➥ keyValueStore.innerStore().get(\"ABC\");\nList<ClickEvent> clickEvents = new ArrayList<>(tuple._1);\nList<StockTransaction> stockTransactions = new ArrayList<>(tuple._2);\nprocessor.cogroup(124722348947L);\nverify(processorContext).forward(\"ABC\",\n➥ Tuple.of(clickEvents, stockTransactions));\nassertThat(tuple._1.size(), equalTo(0));\nassertThat(tuple._2.size(), equalTo(0));\n}\nThis test is a little more involved, and it utilizes a mix of mock and real behavior. Let’s\ntake a brief walk through the test.\n The first line specifies the behavior for the mock ProcessorContext to return the\nstubbed-out KeyValueStore when the ProcessorContext.getStateStore method is\ncalled. This line alone is an interesting mix of generated mock versus a stubbed-out\nmock object.\n I could easily have used Mockito to generate a mock KeyValueStore, but I chose not\nto for two reasons. First, a generated mock returning another generated mock seems a\nbit unnatural (in my opinion). Second, you want to verify and use the stored values in the\nKeyValueStore during the test instead of setting expectations with a canned response.\n The next three lines, starting with processor.init, run the processor through its\nusual steps: first initializing and then processing records. The fourth step is where hav-\ning a working KeyValueStore is important. Because the KeyValueStore is a simple\nstub, you use a java.util.HashMap underneath for the actual storage. In the three\nlines after setting processor expectations, you retrieve the contents from the store\nplaced there by the process() method calls. You create new ArrayList objects with\nthe contents of the Tuple (again, this is a custom class developed for the sample code\nin this book) pulled from the state store by the provided key.\n Next, you drive the punctuate method of the processor. Because this is a unit test,\nyou don’t need to test how time is advanced—that would constitute testing the Kafka\nStreams API itself, which you don’t want here. Your goal is to verify the behavior of the\nmethod you defined as your Punctuator (via a method reference).\nListing 8.6\nTesting the punctuate method\nSets mock behavior to \nreturn a KeyValueStore \nwhen called\nCalls init\nmethod on\nprocessor\nProcesses a ClickEvent \nand a StockTransaction\nExtracts the entries put \ninto the state store in \nthe process method\nCalls the\nco-group\nmethod,\nwhich is\nthe method\nused to\nschedule\npunctuate\nValidates that the \nProcessorContext forwards \nthe expected records\nValidates that the collections \nwithin the tuple are cleared out\n \n",
      "content_length": 2931,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "208\nCHAPTER 8\nTesting a Kafka Streams application\n Now, you verify the main point of the test: that the expected key and value are for-\nwarded downstream via the ProcessorContext.forward method. This portion of the\ntest demonstrates the usefulness of a generated mock object. Using the Mockito\nframework, you just need to tell the mock to expect a forward call with the given key\nand value, and verify that the test executed the code precisely in this manner. Finally,\nyou verify that the processor cleared out the collections of ClickEvent and Stock-\nTransaction objects after forwarding them downstream.\n As you can see from this test, you can isolate the class under test with a mix of gen-\nerated and stubbed-out mock objects. As I stated earlier in this chapter, the bulk of the\ntesting in your Kafka Streams API application should be unit tests on your business\nlogic and on any individual Processor or Transformer objects. Kafka Streams itself is\nthoroughly tested, so you’ll want to focus your efforts on new, untested code.\n You probably won’t want to wait to deploy your application to see how it interacts\nwith a Kafka cluster. You’ll want to sanity check your code, which will require integra-\ntion testing. Let’s look at how you can locally test against a real Kafka broker. \n8.2\nIntegration testing\nSo far, you’ve seen how you can test an entire topology or an individual component in\na unit test. For the most part, these types of tests are best, as they’re quick to run, and\nthey validate specific parts of your code.\n But there are times where you’ll need to test all the working parts together, end to\nend: in other words, an integration test. Usually, an integration test is required when\nyou have some functionality that can’t be covered in a unit test. \n For an example, let’s go back to our very first application, the Yelling App. Because\nyou created the topology so long ago, take another look at it in figure 8.2.\nUpperCase\nprocessor\nsrc-topic\nSource\nprocessor\nSink\nprocessor\nout-topic\nFigure 8.2\nAnother look at the \nYelling App topology\n \n",
      "content_length": 2066,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "209\nIntegration testing\nSuppose you’ve decided to change the source from a single named topic to any topic\nmatching a regex:\nyell-[A-Za-z0-9-]\nAs an example, you want to confirm that if a topic matching the pattern yell-at-\neveryone is added while your application is deployed and running, you’ll start read-\ning information from that newly added topic.\n You won’t update the original Yelling App, since it’s so small. Instead you’ll use the\nfollowing modified version directly in the test (found in src/java/bbejeck/chapter_3/\nKafkaStreamsYellingIntegrationTest.java).\nstreamsBuilder.<String,String>stream(Pattern.compile(\"yell.*\"))\n.mapValues(String::toUpperCase)\n.to(OUT_TOPIC);\nBecause you add topics at the Kafka broker level, the only real way to test whether\nyour application picks up a newly created topic is to add one while your application is\nrunning. Running this scenario is all but impossible with a unit test. But does this\nmean you need to deploy your updated app to test it?\n Fortunately, the answer is no. You can use the embedded Kafka cluster available with\nKafka test libraries.\n By using the embedded Kafka cluster, you can run an integration test requiring a\nKafka cluster on your machine at any point, either individually or as part of your\nentire battery of tests. This speeds up your development cycle. (I use the term embed-\nded here to refer to running a large application like Kafka or ZooKeeper in local\nstandalone mode, or “embedding” it in an existing application.) Let’s move on to\nbuilding an integration test.\n8.2.1\nBuilding an integration test\nThe first step to using the embedded Kafka server requires you to add three more test-\ning dependencies—scala-library-2.12.4.jar, kafka_2.12-1.0.0-test.jar, and kafka_2.12-\n1.0.0.jar—to your build.gradle or pom.xml file. We’ve already covered the syntax for\nproviding a test JAR in section 8.1, so I won’t repeat it here.\n While it may seem like the number of dependencies is starting to increase, remem-\nber that anything you add here is a testing dependency. Testing dependencies aren’t\npackaged up and deployed with your application code; hence, they won’t affect the\nsize of your final application.\n Now that you’ve added the required dependencies, let’s start defining the integra-\ntion test with an embedded Kafka broker. You’ll use a standard JUnit approach for\ncreating the integration test.\nListing 8.7\nUpdating the Yelling App\nSubscribes \nto any topic \nstarting with \n\"yell\"\nConverts all text\nto uppercase\nWrites out to topic, \nor yells at people!\n \n",
      "content_length": 2540,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "210\nCHAPTER 8\nTesting a Kafka Streams application\nADDING THE EMBEDDEDKAFKACLUSTER\nAdding the embedded Kafka broker to the test is a matter of adding one line, as\nshown in the following listing (found in src/java/bbejeck/chapter_3/KafkaStreams-\nYellingIntegrationTest.java).\nprivate static final int NUM_BROKERS = 1;\n@ClassRule\npublic static final EmbeddedKafkaCluster EMBEDDED_KAFKA\n➥= new EmbeddedKafkaCluster(NUM_BROKERS);\nIn the second line listing 8.8, you create the EmbeddedKafkaCluster instance that\nserves as the cluster for running the tests in the class. The key point in this example is\nthe @ClassRule annotation. A full description of testing frameworks and JUnit is\nbeyond the scope of this book, but I’ll take a minute here to explain the importance\nof @ClassRule and how it drives the test. \nJUNIT RULES\nJUnit introduced the concept of rules to apply some common logic JUnit tests. Here’s\na brief definition, from https://github.com/junit-team/junit4/wiki/Rules#rules:\n“Rules allow very flexible addition or redefinition of the behavior of each test method\nin a test class.”\n JUnit provides three types of rules, and the EmbeddedKafkaCluster class uses the\nExternalResource rules (https://github.com/junit-team/junit4/wiki/Rules#exter-\nnalresource-rules). You use ExternalResource rules for setting up and tearing down\nexternal resources, such as the EmbeddedKafkaCluster needed for a test.\n JUnit provides the ExternalResource class, which has two no-op methods,\nbefore() and after(). Any class extending the ExternalResource must override the\nbefore() and after() methods for setting up and tearing down the external resource\nneeded for testing.\n Rules provide an excellent abstraction for using external resources in your tests.\nAfter you create your class extending ExternalResource, all you need to do is create a\nvariable in your test and use the @Rule or @ClassRule annotation, and all the setup\nand teardown methods will be executed automatically.\n The difference between @Rule and @ClassRule is how often before() and\nafter() are called. The @Rule annotation executes before() and after() methods\nfor each individual test in the class. @ClassRule executes the before() and after()\nmethods once; before() is executed prior to any test execution, and after() is called\nwhen the last test in the class completes. Setting up an EmbeddedKafkaCluster is rela-\ntively resource intensive, so it makes sense that you’ll only want to set it up once per\ntest class.\nListing 8.8\nAdding the embedded Kafka broker\nDefines the number \nof brokers\nThe JUnit ClassRule \nannotation\nCreates an instance of the \nEmbeddedKafkaCluster\n \n",
      "content_length": 2638,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "211\nIntegration testing\n Let’s get back to building an integration test. You’ve created an EmbeddedKafka-\nCluster, so the next step is to create any topics you’ll initially need. \nCREATING TOPICS\nNow that your embedded Kafka cluster is available, you can use it to create topics, as\nfollows (src/java/bbejeck/chapter_3/KafkaStreamsYellingIntegrationTest.java).\n@BeforeClass\npublic static void setUpAll() throws Exception {\nEMBEDDED_KAFKA.createTopic(YELL_A_TOPIC);\nEMBEDDED_KAFKA.createTopic(OUT_TOPIC);\n}\nCreating topics for the test is something you’ll want to do only once for all tests, so you\ncan use a @BeforeClass annotation, which creates the required topics before the\nexecution of any tests. For this test, you only need topics with a single partition and\na replication factor of 1, so you can use the convenience method EmbeddedKafka-\nCluster.createTopic(String name). If you needed more than one partition, a repli-\ncation factor greater than 1 requires configurations different from the defaults. For\nthat, you can use one of the following overloaded createTopic methods:\n\nEmbeddedKafkaCluster.createTopic(String topic, int partitions, int\nreplication)\n\nEmbeddedKafkaCluster.createTopic(String topic, int partitions, int\nreplication, Properties topicConfig)\nWith all the pieces in place for the embedded Kafka cluster to run, let’s move on to\ntesting the topology with the embedded broker. \nTESTING THE TOPOLOGY\nAll the pieces are in place. Now you can follow these steps to execute the integra-\ntion test:\n1\nStart the Kafka Streams application.\n2\nWrite some records to the source topic and assert the correct results.\n3\nCreate a new topic matching your pattern.\n4\nWrite some additional records to the newly created topic and assert the cor-\nrect results.\nLet’s start with the first two parts of the test (found in src/java/bbejeck/chapter_3/\nKafkaStreamsYellingIntegrationTest.java).\n// some setup code left out for clarity\nkafkaStreams = new KafkaStreams(streamsBuilder.build(), streamsConfig);\nListing 8.9\nCreating the topics for testing\nListing 8.10\nStarting the application and asserting the first set of values\nBeforeClass annotation\nCreates the first \nsource topic\nCreates the \noutput topic\n \n",
      "content_length": 2215,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "212\nCHAPTER 8\nTesting a Kafka Streams application\nkafkaStreams.start();\nList<String> valuesToSendList =\n➥ Arrays.asList(\"this\", \"should\", \"yell\", \"at\", \"you\");\nList<String> expectedValuesList =\n➥ valuesToSendList.stream()\n.map(String::toUpperCase)\n.collect(Collectors.toList());\nIntegrationTestUtils.produceValuesSynchronously(YELL_A_TOPIC,\nvaluesToSendList,\nproducerConfig,\nmockTime);\nint expectedNumberOfRecords = 5;\nList<String> actualValues =\n➥ IntegrationTestUtils.waitUntilMinValuesRecordsReceived(\n➥ consumerConfig, OUT_TOPIC, expectedNumberOfRecords);\nassertThat(actualValues, equalTo(expectedValuesList));\nThis portion of the test is pretty standard testing code. You “seed” your streaming\napplication by writing records to the source topic. The streaming application is\nalready running, so it consumes, processes, and writes out records as part of its stan-\ndard processing. To verify that the application is performing as you expect, the test\nconsumes records from the sink-node topic and compares the expected values to the\nactual values.\n Toward the end of listing 8.10 are two static utility methods, IntegrationTest-\nUtils.produceValuesSynchronously and IntegrationTestUtils.waitUntilMin-\nValuesRecordsReceived, making the construction of this integration test much more\nmanageable. These producing and consuming utility methods are part of kafka-\nstreams-test.jar. Let’s discuss these methods briefly. \nPRODUCING AND CONSUMING RECORDS IN A TEST\nThe IntegrationTestUtils.produceValuesSynchronously method creates a Producer-\nRecord for each item in the collection with a null key. This method is synchronous, as\nit takes the resulting Future<RecordMetadata> from the Producer.send call and\nimmediately calls Future.get(), which blocks until the produce request returns.\nBecause this method is sending records synchronously, you know the records are avail-\nable for consuming once the method returns. Another method, IntegrationTest-\nUtils.produceKeyValuesSynchronously, takes a collection of KeyValue<K,V> if you\nwant to specify a value for the key.\n For consuming records in listing 8.10, you use the IntegrationTestUtils.wait-\nUntilMinValuesRecordsReceived method. As you can probably guess from the name,\nthis method will attempt to consume the expected number of records from the given\ntopic. By default, this method will wait up to 30 seconds, and if the expected number\nof records has not been consumed, an AssertionError is thrown, failing the test.\nStarts the Kafka \nStreams application\nSpecifies the list \nof values to send\nCreates the list of \nexpected values\nProduces the values \nto embedded Kafka\nConsumes \nrecords \nfrom Kafka\nAsserts the values \nread are equal to \nthe expected values\n \n",
      "content_length": 2715,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "213\nIntegration testing\n If you need to work with the consumed KeyValue instead of just the value, there’s\nthe IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived method, which\nworks in the same manner but returns a Collection of KeyValue results. Additionally,\nthere are overloaded versions of the consuming utility, where you can specify a custom\namount of time to wait via a parameter of type long.\n Now, let’s finish describing the test. \nDYNAMICALLY ADDING A TOPIC\nYou’re at the point in the test where you want to test the dynamic behavior that you\nneed a live Kafka broker for. The previous portion of the test was done to verify the\nstarting point. Now, you’re going to use the EmbeddedKafkaCluster to create a new\ntopic, and you’ll test that the application consumes from the new topic and processes\nrecords as expected (found in src/java/bbejeck/chapter_3/KafkaStreamsYellingInte-\ngrationTest.java).\nEMBEDDED_KAFKA.createTopic(YELL_B_TOPIC);\nvaluesToSendList = Arrays.asList(\"yell\", \"at\", \"you\", \"too\");\nexpectedValuesList = valuesToSendList.stream()\n.map(String::toUpperCase)\n.collect(Collectors.toList());\nIntegrationTestUtils.produceValuesSynchronously(YELL_B_TOPIC,\nvaluesToSendList,\nproducerConfig,\nmockTime);\nexpectedNumberOfRecords = 4;\nList<String> actualValues =\n➥IntegrationTestUtils.waitUntilMinValuesRecordsReceived(\n➥consumerConfig, OUT_TOPIC, expectedNumberOfRecords);\nassertThat(actualValues, equalTo(expectedValuesList));\nYou create a new topic matching the pattern for the source node of the streaming\napplication. After that, you go through the same steps of populating the new topic\nwith data, and consuming records from the topic backing the sink node of the stream-\ning application. At the end of the test, you verify that the consumed results match the\nexpected results.\n You can run this test from inside your IDE, and you should see a successful result.\nYou’ve completed your first integration test!\n You won’t want to use an integration test for everything, because unit tests are eas-\nier to write and maintain. But integration tests can be indispensable when the only\nway to verify the behavior of your code is working with a live Kafka broker.\nListing 8.11\nStarting the application and asserting values\nCreates the \nnew topic\nSpecifies a new list \nof values to send\nCreates the \nexpected \nvalues\nProduces the values to the source \ntopic of the streaming application\nConsumes the \nresults of the \nstreaming \napplication\nAsserts that the \nexpected results match \nthe actual results\n \n",
      "content_length": 2525,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "214\nCHAPTER 8\nTesting a Kafka Streams application\nNOTE\nIt may be tempting to make all your tests using the EmbeddedKafka-\nCluster, but it’s best if you don’t. If you run the sample integration test you\njust built, you’ll notice that it takes much longer to run than the unit tests.\nThe few extra seconds taken by one test might not seem like much, but when\nyou multiply that time by several hundred or a thousand or more tests, the\ntime it takes to run your test suite is substantial. Additionally, you should\nalways try to keep your tests small and focused on one specific piece of func-\ntionality instead of exercising all parts of the application chain. \nSummary\nStrive to keep business logic in standalone classes that are entirely independent\nof your Kafka Streams application. This makes them easy to unit test.\nIt’s useful to have at least one test using ProcessorTopologyTestDriver to test\nyour topology from end to end. This type of test doesn’t use a Kafka broker, so\nit’s fast, and you can see end-to-end results.\nFor testing individual Processor or Transformer instances, try to use a mock\nobject framework only when you need to verify the behavior of some class in the\nKafka Streams API.\nIntegration tests with the EmbeddedKafkaCluster should be used sparingly, and\nonly when you have interactive behavior that can only be verified with a live,\nrunning Kafka broker.\nIt’s been a fun journey, and you’ve learned quite a lot about the Kafka Streams API\nand how you can use it to handle your data-processing needs. So to conclude your\nlearning path, we’ll now switch gears from student to master. The next and final chap-\nter of this book will be a capstone project based on everything you’ve learned so far,\nand in some cases extending out to write custom code that isn’t in the Kafka Streams\nAPI. The result will be a live, end-to-end application using the core functionality pre-\nsented in this book.\n \n",
      "content_length": 1920,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "Part 4\nAdvanced concepts\nwith Kafka Streams\nIn this final part, you’ll take everything you’ve learned and apply it to build-\ning advanced applications. You’ll integrate Kafka Streams with Kafka Connect so\nthat you can stream data even if it’s being written to a relational database. Then,\nyou’ll learn how to use the power of interactive queries to display—in real time—\ninformation your application is building, directly from Kafka Streams, without\nneeding an external tool. Finally, you’ll learn about KSQL, a new tool intro-\nduced by Confluent (the company founded by the original developers of Kafka\nat LinkedIn), and how you can write SQL statements and run continuous que-\nries on data coming into Kafka.\n \n",
      "content_length": 713,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "217\nAdvanced applications\nwith Kafka Streams\nYou’ve come a long way in your journey to learn how to use Kafka Streams. We’ve\ncovered a lot of ground, and now you should know how to build streaming applica-\ntions. Up to this point, you’ve included the core functionality of Kafka Streams, but\nthere’s much more you can do. In this chapter, you’ll use what you’ve learned to\nbuild two advanced applications that will allow you to work in real-world situations.\n For example, in many organizations, when you bring in new technology, it must\nmesh with legacy technology or processes. It’s not uncommon to see database\ntables as the main sink for incoming data. You learned in chapter 5 that tables are\nstreams, so you know you should be able to treat database tables as streams of data.\n The first advanced application we’ll look at in this chapter will “convert” a\nphysical database into a streaming application by integrating Kafka Connect with\nKafka Streams. Kafka Connect will listen for new insertions into the table(s) and\nThis chapter covers\nIntegrating outside data into Kafka Streams with \nKafka Connect\nKicking your database to the curb with interactive \nqueries\nKSQL continuous queries in Kafka\n \n",
      "content_length": 1207,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "218\nCHAPTER 9\nAdvanced applications with Kafka Streams\nwrite those records to a topic in Kafka. This same topic will serve as the source for\nthe Kafka Streams application so that you can turn your database table into a stream-\ning application.\n When you’re working with legacy applications, even if data is captured in real time,\nit’s typical to dump the data into a database to serve as the data source for dashboard\napplications. In this chapter’s second advanced application, you’ll see how to use\ninteractive queries that expose the Kafka Streams state stores for direct queries. A\ndashboard application can then pull directly from the state stores and show data as it’s\nflowing through the streaming application, eliminating the need for a database.\n We’ll wrap up our coverage of advanced features by looking at a powerful new\nKafka feature: KSQL. KSQL lets you write long-running SQL queries on data coming\ninto Kafka. KSQL gives you all the power of Kafka Streams combined with the ease of\nwriting a SQL query. When you use KSQL, it uses Kafka Streams under the covers to\nget the job done.\n9.1\nIntegrating Kafka with other data sources\nFor the first advanced example application, let’s suppose you work at an established\nfinancial services firm, Big Short Equity (BSE). BSE wants to migrate its legacy data\noperations into the modern era, and that plan includes using Kafka. The migration is\npart-way through, and you’re tasked with updating the company’s analytics. The goal\nis to display the latest equities transactions and associated information in real time,\nand Kafka Streams is the perfect fit.\n BSE offers funds focused on different areas of the financial market. The company\nrecords fund transactions in real time in a relational database. Eventually, BSE plans\nto write transactions directly into Kafka, but in the short term, the database is the sys-\ntem of record.\n Given that incoming data is fed into a relational database, how can you bridge the\ngap between the database and your emerging Kafka Streams application? The answer\nis to use Kafka Connect (https://kafka.apache.org/documentation/#connect), a frame-\nwork that’s included with Apache Kafka and that integrates Kafka with other systems.\nOnce Kafka has the data, you’re no longer concerned about the location of the source\ndata; you just point your Kafka Streams application at the source topic as you’ve done\nwith other Kafka Streams applications.\nNOTE\nWhen you use Kafka Connect to bring in data from other sources, the\nintegration point is a Kafka topic. This means any application using Kafka-\nConsumer can use the imported data. Because this is a book about Kafka\nStreams, I emphasize how to integrate with a Kafka Streams application.\nFigure 9.1 shows how this integration between the database and Kafka works. In this\ncase, you’ll use Kafka Connect to monitor a database table and stream updates into a\nKafka topic, which is the source of your financial analysis application.\n \n",
      "content_length": 2966,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "219\nIntegrating Kafka with other data sources\nTIP\nBecause this is a book on Kafka Streams, this section is a whirlwind tour\nof Kafka Connect. For more in-depth information, see the Apache Kafka doc-\numentation (https://kafka.apache.org/documentation/#connect) and Kafka\nConnect Quick Start (https://docs.confluent.io/current/connect/quickstart\n.html).\n9.1.1\nUsing Kafka Connect to integrate data\nKafka Connect is explicitly designed for streaming data from other systems into Kafka\nand for streaming data from Kafka into another data-storage application such as\nMongoDB (www.mongodb.com) or Elasticsearch (www.elastic.co). With Kafka Con-\nnect, it’s possible to import entire databases into Kafka, or other data such as perfor-\nmance metrics.\n Kafka Connect uses specific connectors to interact with outside data sources. Several\nconnectors are available (www.confluent.io/product/connectors), many developed\nby the connector community, making integration between Kafka and almost any other\nstorage system possible. If there isn’t a connector available for your purposes, you can\nimplement a connector of your own (https://docs.confluent.io/current/connect/\ndevguide.html). \n9.1.2\nSetting up Kafka Connect\nKafka Connect runs in two flavors: distributed mode and standalone mode. For most\nproduction environments, running in distributed mode makes sense, because you can\nKafka Connect\nKafka broker\nDatabase\nTopic\nKafka Streams\nKafka Connect reads from the conﬁgured table and writes\nthe data to a topic(s) named\n+\n. The import\npreﬁx\ntablename\nprocess runs continuously, by executing select statements to\npull in newly inserted records.\nThe Kafka Streams application now\nprocesses the incoming data.\nBecause the import process is\ncontinuous, you are essentially stream\nprocessing a database table.\nConnect uses conﬁgured connectors\nto perform data integration.\npreﬁx\ntablename\nFigure 9.1\nKafka Connect integrating a database table and Kafka Streams\n \n",
      "content_length": 1949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "220\nCHAPTER 9\nAdvanced applications with Kafka Streams\ntake advantage of the parallelism and fault tolerance available when you run multiple\nConnect instances. I’m assuming you’ll run the examples on your local machine, so\neverything is configured in standalone mode.\n The connectors that Kafka Connect uses to interact with outside data sources\ncome in two types: source connectors and sink connectors. Figure 9.2 illustrates how\nKafka Connect uses both types. As you can see, source connectors bring data into\nKafka, and sink connectors receive data from Kafka for use by another system.\nFor this example, you’ll use the Kafka JDBC connector (https://github.com/conflu-\nentinc/kafka-connect-jdbc). It’s available on GitHub and also packaged with the book’s\nsource code distribution as a convenience (https://manning.com/books/kafka-streams-\nin-action).\n When using Kafka Connect, you’ll need to do some minor configuration to Kafka\nConnect itself and to the individual connector you’re using to import or export data.\nFirst, let’s look at the configuration parameters you’ll work with for Kafka Connect:\n\nbootstrap.servers—Comma-separated list of the Kafka broker(s) used by\nConnect.\n\nkey.converter—Class of the converter that controls serialization of the key\nfrom Connect format to the format written to Kafka.\n\nvalue.converter—Class of the converter that controls serialization of the\nvalue from Connect format to the format written to Kafka. For this example,\nyou’ll use the built-in org.apache.kafka.connect.json.JsonConverter.\nKafka cluster\nKafka Connect\nKafka cluster\nKafka Connect\nConnect uses sink connectors\nto push data from Kafka\nto external data sources\n(database, ﬁlesystem, and so on).\nConnect uses source connectors\nto pull from external data sources\n(database, ﬁlesystem, and so\non) to push data to Kafka.\nFigure 9.2\nKafka Connect source and sink connectors\n \n",
      "content_length": 1882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "221\nIntegrating Kafka with other data sources\n\nvalue.converter.schemas.enable—true or false, specifying whether Con-\nnect should include the schema for the value. In this example, you’ll set this\nvalue to false; I explain why in the next section.\n\nplugin.path—Tells Connect the location of a connector and its dependencies.\nThis location can be a single, top-level directory containing an uber JAR file or\nmultiple JAR files. You can also provide several paths in a comma-separated list\nof locations.\n\noffset.storage.file.filename—File containing the offsets stored by the Con-\nnect consumer.\nYou’ll also need to provide some configuration for the JDBC connector. Let’s review\nthose settings:\n\nname—Name of the connector.\n\nconnector.class—Class of the connector.\n\ntasks.max—The maximum number of tasks used by this connector.\n\nconnection.url—URL used to connect to the database.\n\nmode—Method the JDBC source connector uses to detect changes.\n\nincrementing.column.name—Name of the column tracked for detecting\nchanges.\n\ntopic.prefix—Connect writes each table to a topic named topic.prefix+\ntable name.\nMost of these configurations are straightforward, but we need to discuss two of\nthem—mode and incrementing.column.name—in a little more detail, because they\nplay an active role in how the connector runs. The JDBC source connector uses mode\nto detect which rows it needs to load. The example uses the incrementing setting,\nwhich relies on an auto-incrementing column where each insert increments the col-\numn value by 1. By tracking an incrementing column, you’ll only pull in new records;\nupdates will go unnoticed. Your Kafka Streams application is only concerned with\npulling in the latest equities-product purchases, so this setting is ideal. incrementing\n.column.name is the name of the column containing the auto-incrementing value.\nTIP\nThe source code for the book contains the nearly completed configura-\ntion for both Connect and the JDBC connector. The config files are located\nin the src/main/resources directory of the source code distribution (https://\nmanning.com/books/kafka-streams-in-action). You’ll need to provide some\ninformation about the path where you’ve extracted the source code reposi-\ntory. Be sure to look at the README.md file for full instructions.\nThis brings us to the end of our overview of Kafka Connect and the JDBC source con-\nnector. We have one more integration point to cover, which we’ll discuss in the next\nsection.\n \n",
      "content_length": 2469,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "222\nCHAPTER 9\nAdvanced applications with Kafka Streams\nNOTE\nYou can find more information about the JDBC source connector in\nthe Confluent documentation (http://mng.bz/01vh). Additionally, there are\nother incremental query modes you should look over (http://mng.bz/0pjP). \n9.1.3\nTransforming data\nBefore getting this new assignment, you had already developed a Kafka Streams appli-\ncation using similar data. As a result, you have an existing model and Serde objects\n(using Gson underneath for JSON serialization and deserialization). To keep your\ndevelopment velocity high, you’d prefer not to write any new code to support working\nwith Connect. As you’ll see in the next section, you’ll be able to import data from Con-\nnect seamlessly.\nTIP\nGson (https://github.com/google/gson) is an Apache licensed library\ndeveloped by Google for the serialization and deserialization of Java objects\ninto and from JSON. You can learn more from the user guide: http://mng\n.bz/JqV2.\nTo enable this seamless integration, you’ll need to make some minor additional con-\nfiguration changes to your JDBC connector’s properties. Before you do, let’s revisit\nsection 9.1.2, where we discussed configuration settings. Specifically, I said you’d use\norg.apache.kafka.connect.json.JsonConverter with schemas disabled; hence, the\nvalue is converted into a simple JSON format.\n Although JSON is what you want to consume in your Kafka Streams application,\nthere are two issues. First, when converting the data into JSON format, the column\nnames are used for the names of fields in the converted JSON. The names are all in an\nabbreviated BSE format that has no meaning outside the organization, so when your\nGson serde is converted from JSON to the expected model object, all the fields are\nnull because the names don’t match.\n Second, the date and time are stored in the database as a timestamp, as expected.\nBut the provided Gson serde hasn’t defined a custom TypeAdapter (http://mng\n.bz/inzB) for the Date type, so all dates need to be a String formatted like this: yyyy-\nMM-dd’T’HH:mm:ss.SSS-0400. Fortunately, Kafka Connect provides a mechanism that\nallows you to handle these two issues with ease.\n Kafka Connect has the concept of Transformations that let you perform lightweight\ntransformations before Connect writes data to Kafka. Figure 9.3 shows where this\ntransformation process takes place.\n In this example, you’ll use two built-in transformations: TimestampConverter\nand ReplaceField. As I mentioned previously, to use these transformations, you\nneed to add the following configuration lines to the connector-jdbc.properties file\n(see src/main/resources/conf/connector-jdbc.properties).\n \n \n \n \n \n",
      "content_length": 2685,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "223\nIntegrating Kafka with other data sources\ntransforms=ConvertDate,Rename\n \ntransforms.ConvertDate.type=\n➥ org.apache.kafka.connect.transforms.TimestampConverter$Value  \ntransforms.ConvertDate.field=TXNTS\n  \ntransforms.ConvertDate.target.type=string\n  \ntransforms.ConvertDate.format=yyyy-MM-dd'T'HH:mm:ss.SSS-0400  \ntransforms.Rename.type=\n➥ org.apache.kafka.connect.transforms.ReplaceField$Value\ntransforms.Rename.renames=SMBL:symbol, SCTR:sector,....   \nThese properties are relatively self descriptive, so we won’t spend much time on them.\nAs you can see, they provide you with exactly what you need for your Kafka Streams\napplication to successfully deserialize messages imported into Kafka by Connect and\nthe JDBC connector.\n With all the Connect pieces in place, completing the integration between the data-\nbase table and your Kafka Streams application is just a matter of using a topic with the\nprefix specified in the connector-jdbc.properties file (found in src/main/java/bbejeck/\nchapter_9/StockCountsStreamsConnectIntegrationApplication.java).\n \n \n \nListing 9.1\nJDBC connector properties\nSYMB\nSCTR\nINDSTY\nOriginal column names\nConnect transforms\nSYMB:symbol, SCTR:sector, INDSTY:industry,...\n{\"symbol\":\"xxxx\", \"sector\":\"xxxxx\", \"industry\":\"xxxxx\",...}\nTransforms take the original\ncolumn names and map them\nto the new conﬁgured names.\nThe JSON message sent to Kafka\ntopics now has the expected ﬁeld\nnames needed for deserialization.\nFigure 9.3\nTransforming the names of the columns to match expected field names\nAliases for the \ntransformers\nType for the\nConvertDate alias\nDate field to convert\nOutput \ntype of the \nconverted \ndate field\nFormat of \nthe date\nType for the \nRename alias\nList of column names (truncated for\nclarity) to replace. The format is\nOriginal:Replacement.\n \n",
      "content_length": 1794,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "224\nCHAPTER 9\nAdvanced applications with Kafka Streams\nSerde<StockTransaction> stockTransactionSerde =\n➥ StreamsSerdes.StockTransactionSerde();     \nStreamsBuilder builder = new StreamsBuilder();\nbuilder.stream(\"dbTxnTRANSACTIONS\",\n➥ Consumed.with(stringSerde,stockTransactionSerde))   \n.peek((k, v)->\n➥ LOG.info(\"transactions from database key {}value {}\", k, v));   \nAt this point, you’re stream processing records from a database table in Kafka\nStreams, but there’s more to do. You’re streaming stock-transaction data—to do any\nanalysis, you need to group the transactions by their stock ticker symbol.\n You’ve seen how to select a key and repartition the records, but it’s more efficient\nif the records come into Kafka keyed; that way, your Kafka Streams application can\nskip the repartitioning step, which saves processing time and disk space. Let’s revisit\nthe configuration for Kafka Connect.\n First, you can add a ValueToKey transformer that takes a list of field names in the\nvalue to extract and use for the key. Update the connector-jdbc.properties file as\nshown here (src/main/resources/conf/connector-jdbc.properties).\ntransforms=ConvertDate,Rename,ExtractKey\n    \ntransforms.ConvertDate.type=\n➥ org.apache.kafka.connect.transforms.TimestampConverter$Value\ntransforms.ConvertDate.field=TXNTS\ntransforms.ConvertDate.target.type=string\ntransforms.ConvertDate.format=yyyy-MM-dd'T'HH:mm:ss.SSS-0400\ntransforms.Rename.type=\n➥ org.apache.kafka.connect.transforms.ReplaceField$Value\ntransforms.Rename.renames=SMBL:symbol, SCTR:sector,....\ntransforms.ExtractKey.type=\n➥ org.apache.kafka.connect.transforms.ValueToKey      \ntransforms.ExtractKey.fields=symbol\n      \nYou add the ExtractKey transform and give Connect the name of the transformer\nclass: ValueToKey. You also provide the name of the field to use for the key: symbol.\nThis could consist of multiple comma-separated values, but in this case, you need only\none value. Note that you use the renamed version for the field, because this trans-\nformer is executed after the Rename transformer.\n The result of the ExtractKey field is a struct of one value. But you only want the\nvalue contained in the struct for the key—the stock ticker symbol. For this operation,\nyou’ll add a FlattenStruct transform to pull the ticker symbol out by itself (see\nsrc/main/resources/conf/connector-jdbc.properties).\nListing 9.2\nKafka Streams source topic populated with data from Connect\nListing 9.3\nUpdated JDBC connector properties\nSerde for the \nStockTransaction object\nUses the topic Connect \nwrites to as the source \nfor the stream\nPrints messages out to the console\nAdds the \nExtractKey \ntransform\nSpecifies the \nclass name of \nthe ExtractKey \ntransform\nLists the field(s) to \nextract for the key\n \n",
      "content_length": 2749,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "225\nIntegrating Kafka with other data sources\ntransforms=ConvertDate,Rename,ExtractKey,FlattenStruct\n    \ntransforms.ConvertDate.type=\n➥ org.apache.kafka.connect.transforms.TimestampConverter$Value\ntransforms.ConvertDate.field=TXNTS\ntransforms.ConvertDate.target.type=string\ntransforms.ConvertDate.format=yyyy-MM-dd'T'HH:mm:ss.SSS-0400\ntransforms.Rename.type=\n➥ org.apache.kafka.connect.transforms.ReplaceField$Value\ntransforms.Rename.renames=SMBL:symbol, SCTR:sector,....\ntransforms.ExtractKey.type=org.apache.kafka.connect.transforms.ValueToKey\ntransforms.ExtractKey.fields=symbol\ntransforms.FlattenStruct.type=\n➥ org.apache.kafka.connect.transforms.ExtractField$Key     \ntransforms.FlattenStruct.field=symbol\n  \nYou add the final transform (FlattenStruct) and specify the ExtractField$Key class,\nwhich is used by Connect to extract the named field and only include that field in the\nresults (in this case, the key). Finally, you provide the name of the field (symbol),\nwhich is the same as in the previous transform; this makes sense, because that’s the\nfield used to create the key struct.\n With just a few extra lines of configuration, you can expand the previous Kafka\nStreams application to do more-advanced operations without the need to select a key\nand do the repartitioning step (found in src/main/java/bbejeck/chapter_9/Stock-\nCountsStreamsConnectIntegrationApplication.java).\nSerde<StockTransaction> stockTransactionSerde =\n➥ StreamsSerdes.StockTransactionSerde();\nStreamsBuilder builder = new StreamsBuilder();\nbuilder.stream(\"dbTxnTRANSACTIONS\",\n➥ Consumed.with(stringSerde, stockTransactionSerde))\n.peek((k, v)->\n➥ LOG.info(\"transactions from database key {}value {}\", k, v))\n.groupByKey(\n➥ Serialized.with(stringSerde,stockTransactionSerde))\n     \n.aggregate(()-> 0L,(symb, stockTxn, numShares) ->\n➥ numShares + stockTxn.getShares(),                    \nMaterialized.with(stringSerde, longSerde)).toStream(\n)\n.peek((k,v) -> LOG.info(\"Aggregated stock sales for {} {}\",k, v))\n.to( \"stock-counts\", Produced.with(stringSerde, longSerde));\nBecause the data is coming in keyed, you can use groupByKey, which won’t set the\nautomatic repartitioning flag. From the group-by operation, you can directly go into\nan aggregation without performing a repartitioning step, which is important for per-\nformance reasons. The README.md file included with the source code contains\nListing 9.4\nAdding a transform\nListing 9.5\nProcessing transactions from a table in Kafka Streams via Connect\nAdds \nthe last \ntransform\nSpecifies the class \nfor the transform \n(ExtractField$Key)\nName of the field \nto pull out\nGroups by key\nPerforms an \naggregation \nof the total \nnumber of \nshares sold\n \n",
      "content_length": 2684,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "226\nCHAPTER 9\nAdvanced applications with Kafka Streams\ninstructions for running an embedded H2 database (www.h2database.com/html/\nmain.html) and Kafka Connect to produce data for the dbTxnTRANSACTIONS topic to\nrun the streaming application.\nTIP\nAlthough it might seem tempting to use Transformations to perform all\nthe work when importing data into Kafka via Connect, remember that trans-\nformations are meant to be lightweight. For any transformations beyond the\nsimple ones shown in the examples, it’s better to pull the data into Kafka and\nthen use Kafka Streams to do the heavy transformational work.\nNow that you’ve seen how to use Kafka Connect to get data into Kafka for processing\nwith Kafka Streams, let’s turn our attention to how you can visualize the state of data\nin real time. \n9.2\nKicking your database to the curb\nIn chapter 4, you learned how to add local state to a Kafka Streams application. Stream-\ning applications need to use state to perform operations like aggregations, reduces,\nand joins. Unless your streaming application works exclusively with individual records,\nhaving local state is required.\n Going back to the requirements from BSE, you’ve developed a Kafka Streams\napplication that captures three categories of equity transactions:\nTotal transactions by market sector\nCustomer purchases of shares per session\nTotal number of shares traded by stock symbol, in tumbling windows of\n10 seconds\nIn the examples so far, you’ve either reviewed the results in the console or read them\nfrom a sink topic. Viewing data in the console is suitable for development, but the con-\nsole isn’t the best medium for displaying results. To do any analytic work or quickly\nunderstand what’s going on, a dashboard application is a better medium to use.\n In this section, you’ll see how you can use interactive queries in Kafka Streams to\ndevelop a dashboard application for viewing analytics, without the need for a relational\ndatabase to hold the state. You’ll populate the dashboard application directly from\nKafka Streams, as the data streams. Hence, the app will continually update naturally.\n In a typical architecture, data that’s captured and operated on is pushed out to a\nrelational database for viewing. Figure 9.4 shows this setup: pre-Kafka Streams, you\ningested data with Kafka, fed an analysis engine, and then pushed the data to a rela-\ntional database table used by a dashboard application.\n If you add Kafka Streams, using local state, the architecture changes slightly, as\nshown in figure 9.5. You can simplify the structure significantly by removing an entire\ncluster (not to mention that the deployment is much more manageable). Kafka\nStreams still writes back to Kafka, and the database is still the primary consumer of the\ntransformed data.\n \n",
      "content_length": 2780,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "227\nKicking your database to the curb\nIn chapter 5, I talked about interactive queries. Let’s revisit the definition briefly:\ninteractive queries let you directly view the data in the state store without having to con-\nsume the data from Kafka. In other words, the stream becomes the database as well.\n Because a picture is worth a thousand words, let’s take another look at figure 9.5,\nbut adjusted in figure 9.6 to use interactive queries.\nKafka\nProcessing cluster\nDatabase\nWeb browser/REST\nservice\nThe processing\ncluster pushes results\nout to a database.\nAn external browser using\na REST service connects\nto the data store to view\nthe processing results.\nThe processing cluster\ningests data from Kafka.\nFigure 9.4\nArchitecture of applications viewing processed data prior to Kafka \nStreams\nKafka\nKafka Streams\nWeb browser/REST\nservice\nDatabase\nExternal\napplication\nYou simplify things, removing the need\nfor a separate cluster, by running\na Kafka Streams application.\nAn external app\nconsumes the results\nand writes them\nto a database.\nFigure 9.5\nArchitecture with Kafka Streams and state added\n \n",
      "content_length": 1100,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "228\nCHAPTER 9\nAdvanced applications with Kafka Streams\nThe idea demonstrated here is simple but powerful. While state stores hold the state\nof the stream, Kafka Streams provides read-only access from outside the streaming\napplication via a RESTful interface. It’s worth stating again how powerful this con-\ncept is; you can view the running state of the stream without the need for an exter-\nnal database.\n Now that you have an understanding of the impact of interactive queries, let’s walk\nthrough how they work.\n9.2.1\nHow interactive queries work\nFor interactive queries to work, Kafka Streams exposes state stores in a read-only wrap-\nper. It’s important to understand that while Kafka Streams makes the stores available\nfor queries, there’s no updating or modifying the state store in any way. Kafka Streams\nexposes state stores from the KafkaStreams.store method.\n Here’s how this method works:\nReadOnlyWindowStore readOnlyStore =\n➥ kafkaStreams.store(storeName, QueryableStoreTypes.windowStore());\nThis example retrieves a WindowStore. In addition, QueryableStoreTypes provides\ntwo other types:\n\nQueryableStoreTypes.sessionStore()\n\nQueryableStoreTypes.keyValueStore()\nOnce you have a reference to the read-only store, it’s just a matter of exposing the\nstore to a service (a RESTful service, for example) for users to query the state of the\nKafka\nKafka Streams\nWeb browser/REST\nservice\nState store\nNow, you really simplify your architecture,\nbecause you remove the processing\ncluster\nthe database.\nand\nThe Kafka Streams application consumes\nfrom the broker, and local state captures\nthe current state of the stream.\nThe REST service now connects directly\nto the local state store, retrieving live\nresults from the stream.\nand\nFigure 9.6\nArchitecture using interactive queries\n \n",
      "content_length": 1786,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "229\nKicking your database to the curb\nstreaming data. But retrieving the state store is only part of the picture. The state store\nextracted here will only contain keys contained in the local store.\nNOTE\nRemember, Kafka Streams assigns a state store per task, and as long as\nyou use the same application ID, a Kafka Streams application can consist of\nmultiple instances. Additionally, those instances need not all be located on\nthe same host. Thus, it’s possible that the state store you query may contain\nonly a subset of all the keys; other state stores (with the same name, located\non other machine[s]) may contain another subset of the keys.\nLet’s use the analytics listed earlier to make this concept clear. \n9.2.2\nDistributing state stores\nConsider the first analytic: aggregating stock trades per market sector. Because you’re\ndoing an aggregation, state stores come into play. You want to expose the state stores\nto provide a real-time view of the number of trades per sector, to gain insight into\nwhich segment of the market is seeing the most action at the moment.\n Stock market activity generates significant data volume, but I’ll only discuss using\ntwo partitions, to keep the details of the example clear. Additionally, let’s say you’re\nrunning two single-threaded instances on two separate machines, located in the same\ndata center. Because of Kafka Streams’ automatic load balancing, each application will\nhave one task processing the data from each partition of the input topic.\n Figure 9.7 shows the distribution of tasks and state stores. As you can see, instance\nA handles all records on partition 0, and instance B handles all records on partition 1.\nStreams app A\nStreams app B\nState store\nState store\nThe assigned task for Kafka\nStreams application instance B\nis TopicPartition T .1\nProcesses messages\non partition 0\nProcesses messages\non partition 1\nThe assigned task for Kafka\nStreams application instance A\nis TopicPartition T0.\nKafka topic T with\ntwo partitions\nFigure 9.7\nTask and state store distribution\n \n",
      "content_length": 2034,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "230\nCHAPTER 9\nAdvanced applications with Kafka Streams\nFigure 9.8 illustrates what happens when you have two records with the keys \"Energy\"\nand \"Finance\".\n\"Energy\":\"100000\" lands in the state store on instance A, and \"Finance\":\"110000\"\nends up in the state store on instance B. Going back to the example of exposing the\nstate store for queries, you can clearly see that if you expose the state store on instance\nA to a web service or any external querying, you can only retrieve the value for the key\n\"Energy\".\n What’s the solution to this issue? You certainly don’t want to set up an individual\nweb service to query each instance—that approach won’t scale. Fortunately, you don’t\nhave to: the solution is as simple as setting a configuration. \n9.2.3\nSetting up and discovering a distributed state store\nTo enable interactive queries, you need to set the StreamsConfig.APPLICATION\n_SERVER_CONFIG parameter. It consists of the hostname of the Kafka Streams applica-\ntion and the port that a query service will listen on, in hostname:port format.\n When a Kafka Streams instance receives a query for a given key, you’ll need to find\nout whether the key is contained in the local store. More important, if it’s not local,\nyou’ll want to find out which instance contains the key and query against that store.\n Several methods on the KafkaStreams object allow for retrieving information for\nall running instances with the same application ID and defining the APPLICATION\n_SERVER_CONFIG. Table 9.1 lists the method names and descriptions.\n You can use KafkaStreams.allMetadata to obtain information for all instances\nthat are eligible for interactive queries. I find that KafkaStreams.allMetadataForKey\nis the method I use most when writing interactive queries.\nStreams app A\nStreams app B\nState store\nState store\nPartition 0\nPartition 1\n{\"Energy\":\" 00000\"} is written\n1\nto partition 0 and stored in the\nstate store in app instance A.\n{\"Finance\":\"\n0000\"}  is written\n11\nto partition\nand stored in the\n1\nstate store in app instance B.\nFigure 9.8\nKey and value distribution in state stores\n \n",
      "content_length": 2083,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "231\nKicking your database to the curb\nNext, let’s take another look at the key/value distribution across Kafka Streams\ninstances, adding the sequence of checking for the \"Finance\" key, which is found and\nreturned from another instance (see figure 9.9). Each Kafka Streams instance has a\nlightweight embedded server listening to the port specified in APPLICATION_SERVER\n_CONFIG.\nIt’s important to point out that you’ll need to query only one of the Kafka Streams\ninstances, and which one you choose doesn’t matter (assuming you’ve configured the\napplication correctly). By using an RPC mechanism and metadata-retrieval methods,\nif the instance you’ve queried doesn’t contain the data you’re looking for, the queried\nKafka Streams instance will find where it’s located, pull the results, and return the\nresults to the original query.\nTable 9.1\nMethods for retrieving store metadata\nName\nParameter(s)\nUsage\nallMetadata\nN/A\nAll instances, some possibly remote\nallMetadataForStore\nStore name\nAll instances (some remote) contain-\ning the named store\nallMetadataForKey\nKey, Serializer\nAll instances (some remote) with the \nstore containing the key\nallMetadataForKey\nKey, StreamPartitioner\nAll instances (some remote) with the \nstore containing the key\nStreams app A\nHost = hostA:4567\nStreams app B\nHost = hostB:4568\n{\"Energy\":\"100000\"}\nState store\nState store\n{\"Finance\":\"110000\"}\nA query comes into application instance A\nfor the key \"Finance\". Using metadata,\ninstance A is able to determine that the\nkey \"Finance\" is located on instance B.\nThe value is retrieved from\nthe state store on instance B and\nreturned to satisfy the original\nrequest on instance A.\nInstance A uses host and port info\nfrom metadata to retrieve the value\nfor \"Finance\" from instance B.\nFigure 9.9\nKey and value query-and-discovery process\n \n",
      "content_length": 1811,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "232\nCHAPTER 9\nAdvanced applications with Kafka Streams\n You can see this in action by tracing the flow of the calls in figure 9.9. Instance A\ndoesn’t contain the key \"Finance\" but discovers that instance B does contain the key.\nSo, A issues a call to the embedded server on B, which retrieves the data and returns\nthe result to the original caller.\nNOTE\nInteractive queries will work on a single node out of the box, but an\nRPC mechanism isn’t provided—you have to implement your own. This sec-\ntion offers one possible solution, but you’re free to implement your own pro-\ncess, and I’m sure many of you will come up with something better. A great\nexample of another RPC implementation is located in the Confluent kafka-\nstreams-examples GitHub repo: http://mng.bz/Ogo3.\nLet’s move on to see interactive queries in action. \n9.2.4\nCoding interactive queries\nThe application you’ll write for interactive queries will look very similar to the other\napps you’ve written so far, with a couple of small changes. The first difference is that\nyou need to pass in two arguments when launching the Kafka Streams application: the\nhostname and the port the embedded service will listen to (found in src/main/java/\nbbejeck/chapter_9/StockPerformanceInteractiveQueryApplication.java).\npublic static void main(String[] args) throws Exception {\nif(args.length < 2){\nLOG.error(\"Need to specify host and port\");\nSystem.exit(1);\n}\nString host = args[0];\nint port = Integer.parseInt(args[1]);\nfinal HostInfo hostInfo = new HostInfo(host, port);\n   \nProperties properties = getProperties();\nproperties.put(\n➥ StreamsConfig.APPLICATION_SERVER_CONFIG,host+\":\"+port);   \n// other details left out for clarity\nUntil this point, you’ve fired up the application without a second thought. Now, you\nneed to provide two arguments (the host and the port), but this change has minimal\nimpact.\n You also embed the local server for performing actual queries: for this implemen-\ntation, I’ve chosen to use the Spark web server (http://sparkjava.com). (Not that\nSpark—this is a book about Kafka Streams, after all!) My motivation for going with the\nSpark web server is its small footprint, its convention-over-configuration approach,\nand the fact that it’s purpose-built for microservices—and a microservice is what you\nListing 9.6\nSetting the hostname and port\nCreates a \nHostInfo object \nfor later use in \nthe application\nSets the config \nfor enabling \ninteractive \nqueries\n \n",
      "content_length": 2442,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "233\nKicking your database to the curb\ncan provide by using interactive queries. If the Spark web server isn’t to your liking,\nfeel free to replace it with another web server.\nNOTE\nI think most readers will be familiar with the term microservice, but\nhere’s the best definition I’ve seen, from http://microservices.io: “Microser-\nvices—also known as the microservice architecture—is an architectural style\nthat structures an application as a collection of loosely coupled services,\nwhich implement business capabilities. The microservice architecture enables\nthe continuous delivery/deployment of large, complex applications. It also\nenables an organization to evolve its technology stack.”\nNow, let’s look at the point in the code where you embed the Spark server, and some\nof the supporting code used to manage it (found in src/main/java/bbejeck/chapter_9/\nStockPerformanceInteractiveQueryApplication.java).\n// details left out for clarity\nKafkaStreams kafkaStreams = new KafkaStreams(builder.build(), streamsConfig);\nInteractiveQueryServer queryServer =\n➥ new InteractiveQueryServer(kafkaStreams, hostInfo);    \nqueryServer.init();\nkafkaStreams.setStateListener(((newState, oldState) -> {\n \nif (newState == KafkaStreams.State.RUNNING && oldState ==\n➥ KafkaStreams.State.REBALANCING) {\nLOG.info(\"Setting the query server to ready\");\nqueryServer.setReady(true);\n                \n} else if (newState != KafkaStreams.State.RUNNING) {\nLOG.info(\"State not RUNNING, disabling the query server\");\nqueryServer.setReady(false);\n}\n}));\nkafkaStreams.setUncaughtExceptionHandler((t, e) -> {\nLOG.error(\"Thread {} had a fatal error {}\", t, e, e);\nshutdown(kafkaStreams, queryServer);\n  \n});\nRuntime.getRuntime().addShutdownHook(new Thread(() -> {\nshutdown(kafkaStreams, queryServer);\n \n}));\nIn this code, you create an instance of InteractiveQueryServer, which is a wrapper\nclass containing the Spark web server and the code to manage the web service calls\nand start and stop the web server.\nListing 9.7\nInitializing the web server and setting its status\nCreates the embedded\nweb server (actually a\nwrapper class)\nAdds a StateListener to only enable \nqueries to state stores until ready\nEnables queries to \nstate stores once \nthe Kafka Streams \napplication is in a \nRUNNING state. \nQueries are \ndisabled if the state \nisn’t RUNNING.\nSets an Uncaught-\nExceptionHandler to \nlog unexpected errors \nand close everything \ndown\nAdds a shutdown \nhook to close \neverything down \nwhen the application \nexits normally\n \n",
      "content_length": 2497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "234\nCHAPTER 9\nAdvanced applications with Kafka Streams\n Chapter 7 discussed using a StateListener for notifications about various states of\na Kafka Streams application. Here you can see an efficient use of this listener. Recall\nthat when running an interactive query, you need to use an instance of StreamsMeta-\ndata to determine whether the data for the given key is local to the instance process-\ning the query. You set the state of the query server to true, allowing access to the\nmetadata needed for queries only if the application is in the RUNNING state.\n A key point to keep in mind is that the metadata returned is a snapshot of the\nmakeup of the Kafka Streams application. At any point in time, you can scale the\napplication up (or down). When this occurs (or when any other qualifying event takes\nplace, such as adding topics with a regex source node), the Kafka Streams application\ngoes through a rebalancing phase and may change partition assignments. In this case,\nyou’re allowing queries only in the RUNNING state, but feel free to use whatever strategy\nyou think is appropriate.\n Next is another example of a concept covered in chapter 7: setting an Uncaught-\nExceptionHandler. In this case, you log the error and shut down the application and\nthe query server. Because this application runs indefinitely, you add a shutdown hook\nto close everything down once you stop the demo.\n Now that you’ve seen how to instantiate and start the service, let’s move on to the\ncode for running the query server. \n9.2.5\nInside the query server\nWhen implementing your RESTful service, the first step is to map URL paths to the\ncorrect methods to execute (found in src/main/java/bbejeck/webserver/Interactive-\nQueryServer.java).\npublic void init() {\nLOG.info(\"Started the Interactive Query Web server\");\nget(\"/kv/:store\", (req, res) ->\nready ?\n➥ fetchAllFromKeyValueStore(req.params()) :\n➥ STORES_NOT_ACCESSIBLE);                \nget(\"/session/:store/:key\", (req, res) -> ready ?\n➥ fetchFromSessionStore(req.params()) :\n➥ STORES_NOT_ACCESSIBLE);                    \nget(\"/window/:store/:key\", (req, res) -> ready ?\n➥ fetchFromWindowStore(req.params()) :\n➥ STORES_NOT_ACCESSIBLE);                   \nget(\"/window/:store/:key/:from/:to\",(req, res) -> ready ?\n➥ fetchFromWindowStore(req.params()) :\n➥ STORES_NOT_ACCESSIBLE);          \n}\nThis code highlights the decision to go with the Spark web server: you can concisely\nmap URLs to a Java 8 lambda expression to handle the request. These mappings are\nListing 9.8\nMapping URL paths to methods\nMapping to retrieve \nall values from a plain \nkey/value store\nMapping to return all \nsessions (from a session \nstore) for a given key\nMapping for a \nwindow store with \nno times specified\nMapping for a window store \nwith from and to times\n \n",
      "content_length": 2780,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "235\nKicking your database to the curb\nstraightforward, but notice that you map the retrieval from the window store twice. To\nretrieve values from a window store, you need to provide a from time and a to time.\n In the URL mappings, notice the check for the ready Boolean value. This value is\nset in StateListener. If ready evaluates to false, you don’t attempt to process the\nrequest, and you return a message that the stores aren’t currently accessible. This\nmakes sense because a window store is segmented by time, with the segment size estab-\nlished when you create the store. (We covered windowing in section 5.3.2.) But I’m\ncheating here and offering you a method that accepts only a key and a store and pro-\nvides default from and to times that we’ll explore in the next example.\nNOTE\nThere’s a proposal (KIP-205, http://mng.bz/lI9Y) to extend ReadOnly-\nWindowStore to provide an all() method that retrieves all time segments by\nkey, alleviating the need to specify from and to times. This functionality hasn’t\nbeen implemented yet but should be included in a future release.\nAs an example of how the interactive query service works, let’s walk through retrieving\nfrom a windowed store. Although we’ll only look at one example, the source code\ncontains instructions to run all types of queries.\nCHECKING FOR THE STATE STORE LOCATION\nYou’ll remember that you need to collect various metrics on BSE’s securities sales to\nprovide stock-transaction data analysis. You decide to first track sales of individual\nstocks, keeping a running total over 10-second windows to identify stocks that may\ntrend up or down.\n You’ll use the following mapping to walk through the example, from reviewing the\nrequest to returning the response:\nget(\"/window/:store/:key\", (req, res) -> ready ?\n➥ fetchFromWindowStore(req.params()) : STORES_NOT_ACCESSIBLE);\nTo help you keep your place in the query process, let’s use figure 9.9 as a roadmap.\nYou’ll start by sending an HTTP get request http://localhost:4567/window/Number-\nSharesPerPeriod/XXXX, where XXXX represents the ticker symbol for a given stock\n(found in src/main/java/bbejeck/webserver/InteractiveQueryServer.java).\nprivate String fetchFromWindowStore(Map<String, String> params) {\nString store = params.get(STORE_PARAM);\nString key = params.get(KEY_PARAM);\nString fromStr = params.get(FROM_PARAM);\nString toStr = params.get(TO_PARAM);\n  \nHostInfo storeHostInfo = getHostInfo(store, key);   \nif(storeHostInfo.host().equals(\"unknown\")){\n  \nreturn STORES_NOT_ACCESSIBLE;\n}\nListing 9.9\nMapping the request and checking for the key location\nExtracts the request \nparameters\nGets the HostInfo \nfor the key\nIf the hostname is \n\"unknown\", returns an \nappropriate message\n \n",
      "content_length": 2709,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "236\nCHAPTER 9\nAdvanced applications with Kafka Streams\nif(dataNotLocal(storeHostInfo)){\n   \nLOG.info(\"{} located in state store on another instance\", key);\nreturn fetchRemote(storeHostInfo,\"window\", params);\n}\nThe request is mapped to the fetchFromWindowStore method. The first step is to pull\nout the store name and key (stock symbol) from the request-parameters map. You\nfetch the HostInfo object for the key in the request, and you use the hostname to\ndetermine whether the key is located on this instance or a remote one.\n Next, you check whether the Kafka Streams instance is (re)initializing, which is\nindicated by the host() method returning \"unknown\". If so, you stop processing the\nrequest and return a \"not accessible\" message.\n Finally, you check whether the hostname matches the hostname for the current\ninstance. If the hostname doesn’t match, you get the data from the instance contain-\ning the key and return the results.\n Next, let’s look at how you retrieve and format the results (found in src/main/\njava/bbejeck/webserver/InteractiveQueryServer.java).\nInstant instant = Instant.now();\nlong now = instant.toEpochMilli();\n  \nlong from =\nfromStr !=\n➥ null ? Long.parseLong(fromStr) : now - 60000;         \nlong to =\ntoStr != null ? Long.parseLong(toStr) : now;   \nList<Integer> results = new ArrayList<>();\nReadOnlyWindowStore<String, Integer> readOnlyWindowStore =\n➥ kafkaStreams.store(store,\n➥ QueryableStoreTypes.windowStore());    \ntry(WindowStoreIterator<Integer> iterator =\n➥ readOnlyWindowStore.fetch(key, from , to)){\n  \nwhile (iterator.hasNext()) {\nresults.add(iterator.next().value);  \n}\n}\nreturn gson.toJson(results);\n  \nI mentioned earlier that you’ll cheat on the window store query if the from and to\nparameters aren’t provided in the query. If the user doesn’t specify a range, by default\nyou return the last minute of results from the window store. Because you’ve defined a\nwindow of 10 seconds, you’ll return six-windowed results. After you fetch the window\nsegments from the store, you iterate over them, building a response that indicates the\nnumber of shares purchased for each 10-second interval over the last minute. \nListing 9.10\nRetrieving and formatting the results\nChecks whether the returned hostname\nmatches the host of this instance\nGets the current \ntime in milliseconds\nSets the window segment \nstart time or, if not \nprovided, the time as \nof one minute ago\nSets the window \nsegment ending time \nor, if not provided, \nthe current time\nRetrieves the ReadOnlyWindowStore\nFetches the window segments\nBuilds up the response\nConverts the results to JSON \nand returns to the requestor\n \n",
      "content_length": 2629,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "237\nKSQL\nRUNNING THE INTERACTIVE QUERY EXAMPLE\nTo observe the results of this example, you need to run three commands:\n\n./gradlew runProducerInteractiveQueries produces the data needed for the\nexamples.\n\n./gradlew runInteractiveQueryApplicationOne starts a Kafka Streams appli-\ncation with HostInfo using port 4567.\n\n./gradlew runInteractiveQueryApplicationTwo starts a Kafka Streams appli-\ncation with HostInfo using port 4568.\nThen, point your browser to http://localhost:4568/window/NumberSharesPerPeriod/\nAEBB. Click Refresh a few time to see different results. Here’s a static list of company\nsymbols for this example: AEBB, VABC, ALBC, EABC, BWBC, BNBC, MASH, BARX, WNBC,\nWKRP. \nRUNNING A DASHBOARD APPLICATION FOR INTERACTIVE QUERIES\nA better example is a mini-dashboard web application that updates automatically (via\nAjax) and displays the results from four different Kafka Streams aggregation opera-\ntions. By running the commands listed in the previous subsection, you have every-\nthing set up; point your browser to localhost:4568/iq or localhost:4567/iq to run the\ndashboard application. By going to either instance, you’ll see how Kafka Stream’s\ninteractive queries handle getting results from all instances with the same application\nID. Look in the README file in the source code for full instructions on how to set up\nand start the dashboard application.\n As you can see from observing the web application, you can view live results of the\nstream in a dashboard-like application. Previously, this type of application required a\nrelational database; but here, Kafka Streams provides the information as needed.\n We’ve wrapped up our coverage of interactive queries. Let’s move on to KSQL: an\nexciting new tool that Confluent (the company founded by the original developers of\nKafka at LinkedIn) recently released, which allows you to specify long-running queries\nagainst records streaming into Kafka without code, but using SQL. \n9.3\nKSQL\nImagine you’re working with business analysts at BSE. The analysts are interested in\nyour ability to quickly write applications in Kafka Streams to perform real-time data\nanalysis. This interest puts you in a bind.\n You want to work with the analysts and write applications for their requests, but\nyou also have your normal workload—the additional work makes it hard to keep up\nwith everything. The analysts understand the added work they’re creating, but they\ncan’t write code, so they depend on you to write their analytics.\n The analysts are experts on working with relational databases and thus are com-\nfortable with SQL queries. If there were some way to give the analysts a SQL layer over\nKafka Streams, everyone’s productivity would increase. Well, now there is.\n \n",
      "content_length": 2729,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "238\nCHAPTER 9\nAdvanced applications with Kafka Streams\n In August 2017, Confluent unveiled a powerful new tool for stream processing:\nKSQL (https://github.com/confluentinc/ksql#-ksql). KSQL is a streaming SQL engine\nfor Apache Kafka, providing an interactive SQL interface that you can use to write\npowerful stream-processing queries without writing code. KSQL is especially adept at\nfraud detection and real-time applications.\nNOTE\nKSQL is a big topic and could take a chapter or two if not an entire\nbook on its own. So, the coverage here will be concise. Fortunately, you’ve\nalready learned the core concepts underpinning KSQL, because it uses Kafka\nStreams under the covers. For more information, see the KSQL documenta-\ntion (http://mng.bz/zw3F).\nKSQL provides scalable, distributed stream processing, including aggregations, joins,\nwindowing, and more. Additionally, unlike SQL run against a database or a batch-\nprocessing system, the results of a KSQL query are continuous. Before we dive into\nwriting streaming queries, let’s take a minute to review some fundamental concepts\nof KSQL.\n9.3.1\nKSQL streams and tables\nSection 5.1.3 discussed the concept of an event stream versus an update stream. An event\nstream is an unbounded stream of individual independent events, where an update or\nrecord stream is a stream of updates to previous records with the same key.\n KSQL has a similar concept of querying from a Stream or a Table. A Stream is an\ninfinite series of immutable events or facts, but with a query on a Table, the facts are\nupdatable or can even be deleted.\n Although some of the terminology is different, the concepts are pretty much the\nsame. If you’re comfortable with Kafka Streams, you’ll feel right at home with KSQL. \n9.3.2\nKSQL architecture\nKSQL uses Kafka Streams under the covers to build and fetch the results of queries.\nKSQL is made up of two components: a CLI and a server. Users of standard SQL tools\nsuch as MySQL, Oracle, and even Hive will feel right at home with the CLI when writ-\ning queries in KSQL. Best of all, KSQL is open source (Apache 2.0 licensed).\n The CLI is also the client connecting to the KSQL server. The KSQL server is\nresponsible for processing the queries and retrieving data from Kafka as well as writ-\ning results into Kafka.\n KSQL runs in two modes: standalone, which is useful for prototyping and develop-\nment; and distributed, which of course is how you’d use KSQL when working in a more\nrealistic-size data environment. Figure 9.10 shows how KSQL works in local mode. As\nyou can see, the KSQL CLI, REST server, and KSQL engine are all located on the\nsame JVM, which is ideal when running on your laptop.\n Now, let’s look at KSQL in distributed mode; see figure 9.11. The KSQL CLI is by\nitself, and it will connect to one of the remote KSQL servers (we’ll cover starting and\n \n",
      "content_length": 2840,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "239\nKSQL\nconnections in the next section). A key point is that although you only explicitly con-\nnect to one of the remote KSQL servers, all servers pointing to the same Kafka cluster\nwill share in the workload of the submitted query.\nKafka\nKSQL CLI\nREST\ninterface\nKSQL\nengine\nJVM\nThe KSQL CLI and KSQL engine\nare in one JVM locally.\nFigure 9.10\nKSQL in local mode\nKafka\nKSQL CLI\nREST interface\nKSQL engine\nREST interface\nKSQL engine\nREST interface\nKSQL engine\nEach of the\nKSQL engines is in a\nseparate JVM.\nFigure 9.11\nKSQL in distributed mode\n \n",
      "content_length": 547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "240\nCHAPTER 9\nAdvanced applications with Kafka Streams\nNote that the KSQL servers are using Kafka Streams to execute the queries. This\nmeans that if you need more processing power, you can stand up another KSQL\nserver, even during live operations (just like you can spin up another Kafka Streams\napplication). The opposite case works just as well: if you have excess capacity, you can\nstop any number of KSQL servers, with the assumption that you’ll leave at least one\nserver operational. Otherwise, your queries will stop running!\n Next, let’s see how you get KSQL installed and running. \n9.3.3\nInstalling and running KSQL\nTo install KSQL, you’ll clone the KSQL repo with the command git clone\ngit@github.com:confluentinc/ksql.git and then cd into the ksql directory and\nexecute mvn clean package to build the entire KSQL project. If you don’t have git\ninstalled or don’t want to build from source, you can download the KSQL release\nfrom http://mng.bz/765U.\nTIP\nKSQL is an Apache Maven–based project, so you’ll need Maven installed\nto build KSQL. If you don’t have Maven installed and you’re on a Mac and\nhave Homebrew installed, run brew install maven. Otherwise, you can head\nover to https://maven.apache.org/download.cgi and download Maven directly;\ninstallation instructions are at https://maven.apache.org/install.html.\nMake sure you’re in the base directory of the KSQL project before going any further.\nThe next step is to start KSQL in local mode:\n./bin/ksql-cli local\nNote that you’ll be using KSQL in local mode for all the examples, but we’ll still cover\nhow to run KSQL in distributed mode.\n After running the previous command, you should see something like figure 9.12 in\nyour console. Congratulations—you’ve successfully installed and launched KSQL!\nNext, let’s start writing some queries. \nFigure 9.12\nKSQL successful launch result\n \n",
      "content_length": 1850,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "241\nKSQL\n9.3.4\nCreating a KSQL stream\nGetting back to your work at BSE, you’ve been approached by an analyst who is inter-\nested in one of the applications you’ve written and would like to make some tweaks to\nit. But instead of this request resulting in more work, you spin up a KSQL console and\nturn the analyst loose to reconstruct your application as a SQL statement!\n The example you’re going to convert is the last windowed stream from the interac-\ntive queries example found in src/main/java/bbejeck/chapter_9/StockPerformance-\nInteractiveQueryApplication.java, lines 96-103. In that application, you track the number\nof shares sold every 10 seconds, by company ticker symbol.\n You already have the topic defined (the topic maps to a database table) and a\nmodel object, StockTransaction, where the fields on the object map to columns in\na table. Even though the topic is defined, you need to register this information with\nKSQL by using a CREATE STREAM statement in src/main/resources/ksql/create_\nstream.txt.\nCREATE STREAM stock_txn_stream (symbol VARCHAR, sector VARCHAR, \\  \nindustry VARCHAR, shares BIGINT, sharePrice DOUBLE, \\\n      \ncustomerId VARCHAR, transactionTimestamp STRING, purchase BOOLEAN) \\\nWITH (VALUE_FORMAT = 'JSON', KAFKA_TOPIC = 'stock-transactions');  \nWith this one statement, you create a KSQL Streams instance that you can issue\nqueries against. The WITH clause has two required parameters: VALUE_FORMAT, tell-\ning KSQL the format of the data, and KAFKA_TOPIC, telling KSQL where to pull the\ndata from. There are two additional parameters you can use in the WITH clause\nwhen creating a stream. The first is TIMESTAMP, which associates the message time-\nstamp with a column in the KSQL stream. Operations requiring a timestamp, such\nas windowing, will use this column to process the record. The other parameter is\nKEY, which associates the key of the message with a column on the defined stream.\nIn this case, the message key for the stock-transactions topic matches the symbol\nfield in the JSON value, so you don’t need to specify the key. Had this not been the\ncase, you would have needed to map the key to a named column, because you\nalways need a key to perform grouping operations. You’ll see this when you execute\nthe stream SQL.\nTIP\nThe KSQL command list topics; shows a list of topics on the broker\nthe KSQL CLI is pointing to and whether the topics are registered.\nListing 9.11\nCreating a stream\nCREATE STREAM statement to create a\nstream named stock_txn_stream\nRegisters the fields of the\nStockTransaction object\nas columns\nSpecifies the data format and the Kafka \ntopic serving as the source of the stream \n(both required parameters)\n \n",
      "content_length": 2678,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "242\nCHAPTER 9\nAdvanced applications with Kafka Streams\nYou can view all streams and verify that KSQL created the new stream as expected with\nthe following commands:\nshow streams;\ndescribe stock_txn_stream;\nThe results are shown in figure 9.13. Notice that KSQL inserted two extra columns:\nROWTIME and ROWKEY. The ROWTIME column is the timestamp placed on the message\n(either from the producer or by the broker), and ROWKEY is the key (if any) of the\nmessage.\nNow, let’s run the query on this stream.\nNOTE\nYou’ll need to run ./gradlew runProducerInteractiveQueries to\nprovide data for the KSQL examples \n9.3.5\nWriting a KSQL query\nThe SQL query for performing the stock analysis is as follows:\nSELECT symbol, sum(shares) FROM stock_txn_stream\n➥ WINDOW TUMBLING (SIZE 10 SECONDS) GROUP BY symbol;\nRun this query, and you’ll see results similar\nto those shown in figure 9.14. The column\non the left is the ticker symbol, and the num-\nber is the number of shares traded for that\nsymbol over the last 10 seconds. With this\nquery, you specify a tumbling window of 10\nseconds, but KSQL supports session and hopping windows, as well, as we discussed in\nsection 5.3.2.\nFigure 9.13\nListing streams, and \ndescribing your newly created stream\nFigure 9.14\nResults of \nthe tumbling window query\n \n",
      "content_length": 1283,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "243\nKSQL\n You’ve built a streaming application without writing any code—quite an achieve-\nment. For a comparison, let’s look at the corresponding application written in the\nKafka Streams API.\nKStream<String, StockTransaction> stockTransactionKStream =\n➥ builder.stream(MockDataProducer.STOCK_TRANSACTIONS_TOPIC,\nConsumed.with(stringSerde, stockTransactionSerde)\n.withOffsetResetPolicy(Topology.AutoOffsetReset.EARLIEST));\nAggregator<String, StockTransaction, Integer> sharesAggregator =\n➥ (k, v, i) -> v.getShares() + i;\nstockTransactionKStream.groupByKey()\n.windowedBy(TimeWindows.of(10000))\n.aggregate(() -> 0, sharesAggregator,\nMaterialized.<String, Integer,\nWindowStore<Bytes,\nbyte[]>>as(\"NumberSharesPerPeriod\")\n.withKeySerde(stringSerde)\n.withValueSerde(Serdes.Integer()))\n.toStream().\n➥ peek((k,v)->LOG.info(\"key is {} value is{}\", k, v));\nEven though the Kafka Streams API is concise, the equivalent you wrote in KSQL is a\none-liner query. Before we wrap up our coverage of KSQL, let’s discuss some additional\nfeatures of KSQL. \n9.3.6\nCreating a KSQL table\nSo far, we’ve demonstrated creating a KSQL stream. Now, let’s see how to create a\nKSQL table, using the stock-transactions topic as the source, for familiarity (found\nin src/main/resources/ksql/create_table.txt).\nCREATE TABLE stock_txn_table (symbol VARCHAR, sector VARCHAR, \\\nindustry VARCHAR, shares BIGINT, \\\nsharePrice DOUBLE, \\\ncustomerId VARCHAR, transactionTimestamp \\\nSTRING, purchase BOOLEAN) \\\nWITH (KEY='symbol', VALUE_FORMAT = 'JSON', \\\nKAFKA_TOPIC = 'stock-transactions');\nOnce you’ve created the table, you can execute queries against it. Keep in mind that\nthe table will contain updates for each transaction by symbol, because the stock-\ntransactions topic is keyed by the ticker symbol.\nListing 9.12\nStock analysis application written in Kafka Streams\nListing 9.13\nCreating a KSQL table\n \n",
      "content_length": 1870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "244\nCHAPTER 9\nAdvanced applications with Kafka Streams\n A useful experiment is to pick a ticker symbol from the streaming stock-perfor-\nmance query, and then run the following queries in the KSQL console, and notice the\ndifference in output:\nselect * from stock_txn_stream\nwhere symbol='CCLU';\nselect * from stock_txn_table where symbol='CCLU';\nThe first query produces several results, because it’s a stream of individual events. But\nthe table query returns far fewer results (one record, when I ran the experiment).\nThese results are the expected behavior, because a table represents updates to facts,\nwhereas a stream represents a series of unbounded events. \n9.3.7\nConfiguring KSQL\nKSQL offers the familiar SQL syntax and the ability to write powerful streaming appli-\ncations quickly, but you may have noticed the lack of configuration. This is not to say\nyou can’t configure KSQL. You’re free to override any settings as needed, and any of\nthe stream, consumer, and producer configs that you can set for a Kafka Streams\napplication are available. To view the properties that are currently set, run the show\nproperties; command.\n As an example of setting a property, here’s how you can change auto.offset\n.reset to earliest:\nSET 'auto.offset.reset'='earliest';\nThis is the approach you use to set any property in the KSQL shell. But if you need to\nset several configurations, typing each one into the console isn’t convenient. Instead,\nyou can specify a configuration file on startup:\n./bin/ksql-cli local --properties-file /path/to/configs.properties\nThis has been a quick tour of KSQL, but I hope you can see the power and flexibility it\ngives you for creating streaming applications on Kafka. \nSummary\nBy using Kafka Connect, you can incorporate other data sources into your\nKafka Streams applications.\nInteractive queries are a potent tool: they allow you to see data in a stream as it\nflows through your Kafka Streams application, without the need for a relational\ndatabase.\nThe KSQL language lets you quickly build powerful streaming applications\nwithout code. KSQL promises to deliver the power and flexibility of Kafka\nStreams to workers who aren’t developers.\n \n",
      "content_length": 2179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "245\nappendix A\nAdditional configuration\ninformation\nThis appendix covers common and not-so-common configuration options for a\nKafka Streams application. During the course of the book, you’ve seen several exam-\nples of configuring a Kafka Streams application, but the configurations usually\nincluded only the required (application ID, bootstrap servers) and a handful of other\nconfigs (key and value serdes). In this appendix, I’ll show you some other settings\nthat, although not required, will help you keep your Kafka Streams applications run-\nning smoothly. These options will be presented in somewhat of a cookbook fashion.\nLimiting the number of rebalances on startup\nWhen starting up a Kafka Streams application, if you have multiple instances, the\nfirst instance gets all the topic partitions assigned from the GroupCoordinator on\nthe broker. If you start another instance, a rebalance occurs, removing current\nTopicPartition assignments and reassigning all TopicPartitions across both\nKafka Streams instances. This process is repeated until you’ve started all Kafka\nStreams applications that have the same application ID.\n This is normal operation for a Kafka Streams application. But during a rebal-\nance, processing of records is paused until the rebalance is completed; thus, you’d\nlike to limit the number of rebalances when starting up, if possible.\n With the release of Kafka 0.11.0, a new broker configuration, group.initial\n.rebalance.delay.ms, was introduced. This configuration delays the initial consumer\nrebalance from the GroupCoordinator when a new consumer joins the group by the\namount specified in the group.initial.rebalance.delay.ms configuration. The\ndefault setting is 3 seconds. As other consumers join the group, the rebalance is con-\ntinually delayed by the configured amount (up to a limit of max.poll.interval.ms).\n \n",
      "content_length": 1850,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "246\nAPPENDIX A\nAdditional configuration information\nThis benefits Kafka Streams because as you start new instances, the rebalance is\ndelayed until all instances have come online (assuming you’re starting them up one\nafter another). For example, if you start four instances with the appropriate rebal-\nance-delay setting, you should have only one rebalance after all four instances come\nonline—meaning you’ll start processing data more quickly. \nResilience to broker outages\nTo keep your Kafka Streams application resilient in the face of broker failures, here\nare some recommended settings (see listing A.1):\n■\nSet Producer.NUM_RETRIES to Integer.MAX_VALUE.\n■\nSet Producer.REQUEST_TIMEOUT to 305000 (5 minutes).\n■\nSet Producer.BLOCK_MS_CONFIG to Integer.MAX_VALUE.\n■\nSet Consumer.MAX_POLL_CONFIG to Integer.MAX_VALUE.\nProperties props = new Properties();\nprops.put(StreamsConfig.producerPrefix(\n➥ ProducerConfig.RETRIES_CONFIG), Integer.MAX_VALUE);\nprops.put(StreamsConfig.producerPrefix(\n➥ ProducerConfig.MAX_BLOCK_MS_CONFIG), Integer.MAX_VALUE);\nprops.put(StreamsConfig.REQUEST_TIMEOUT_MS_CONFIG, 305000);\nprops.put(StreamsConfig.consumerPrefix(\n➥ ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG), Integer.MAX_VALUE);\nSetting these values should help ensure that if all brokers in the Kafka cluster go\ndown, your Kafka Streams application will stay up and be ready to resume working\nonce the brokers are back online. \nHandling deserialization errors\nKafka works with byte arrays for keys and values, and you need to deserialize the keys and\nvalues to work with them. This is why you need to provide serdes for all source and sink\nprocessors. It wouldn’t be unexpected to have some malformed data during record pro-\ncessing. Kafka Streams provides the default.deserialization.exception.handler\nand StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG\nconfigurations to specify how you want to handle these deserialization errors.\n The default setting is org.apache.kafka.streams.errors.LogAndFailException-\nHandler, which, as the name implies, logs the error. Your Kafka Streams application\ninstance will fail (shut down) due to this deserialization exception. Another class,\norg.apache.kafka.streams.errors.LogAndContinueExceptionHandler, logs the error,\nbut your Kafka Streams application will continue to run.\n You can implement your own deserialization exception handler by creating a class\nimplementing the DeserializationExceptionHandler interface.\n \nListing A.1\nSetting properties for resilience to broker outages\n \n",
      "content_length": 2535,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "247\nScaling up your application\nProperties props = new Properties();\nprops.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_\n➥ CONFIG, LogAndContinueExceptionHandler.class);\nI only show how to set the LogAndContinueExceptionHandler handler, because the\nlog-and-fail version is the default setting. \nScaling up your application\nIn all the examples in the book, the Kafka Streams applications run with one stream\nthread. That’s fine for development, but in practice you’ll most likely need to run with\nmore than one stream thread. The question is how many threads and how many Kafka\nStreams instances to use. There are no concrete answers, because only you know your\ncircumstances well enough to address those questions, but we can go over some basic\ncalculations to give you a good idea.\n You’ll remember from chapter 3 that Kafka Streams creates a StreamTask per par-\ntition of the input topic(s). For our first example, we’ll consider a single input topic\nwith 12 partitions, to keep the discussion straightforward.\n With 12 input partitions, Kafka Streams creates 12 tasks. For the moment, let’s\nassume you want to have 1 task per thread. You could have 1 instance with 12 threads,\nbut that approach has a drawback: if the machine hosting your Kafka Streams applica-\ntion were to go down, all stream processing would stop.\n But if you start instances with 4 threads each, then each instance will process 4\ninput partitions. The benefit of this approach is that if one of the Kafka Streams\ninstances goes down, a rebalance will be triggered, and the 4 tasks from the non-\nrunning instance will be assigned to the other 2 instances; thus, the remaining appli-\ncations will process 6 tasks each. Additionally, when the stopped instance resumes\nrunning, another rebalance will occur, and all 3 instances will go back to processing 4\ntasks.\n One important consideration is that when determining the number of tasks to cre-\nate, Kafka Streams takes the maximum number of partitions from all input topics. If\nyou have 1 topic with 12 partitions, you end up with 12 tasks; but if the number of\nsource topics is 4, with 3 partitions each, you’ll have 3 tasks, each of which is responsi-\nble for processing 4 partitions.\n Keep in mind that any stream threads beyond the number of tasks will be idle.\nGoing back to the example of 3 Kafka Streams instances, if you stand up a fourth\ninstance of 4 threads, then after rebalancing you’ll have 4 idle stream threads among\nyour applications (16 threads, but only 12 tasks).\n This is a key component of Kafka Streams that I mentioned earlier in the book.\nThis dynamic scaling up or down doesn’t involve taking your application offline—it\nhappens automatically. This feature is helpful because if you have an uneven flow of\nListing A.2\nSetting a deserialization handler\n \n",
      "content_length": 2823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "248\nAPPENDIX A\nAdditional configuration information\ndata into the application, you can spin up additional instances to handle the load and\nthen take some offline when the volume drops off.\n Do you always want a single thread per task? Maybe, but it’s hard to say, because it\ndepends on the demands of your application. \nRocksDB configuration\nFor stateful operations, Kafka Streams uses RocksDB (http://rocksdb.org) under the\ncovers as the persistence mechanism. RocksDB is a fast, highly configurable key/value\nstore. There are too many options to make specific recommendations here, but Kafka\nStreams provides a way to override the default settings with the RocksDBConfigSetter\ninterface.\n To set custom RocksDB settings, create a class implementing the RocksDBConfig-\nSetter interface, and then provide the class name when configuring your Kafka\nStreams application via the StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG\nsetting. To get an idea of what you can adjust with RocksDB, I encourage you to read\nthe RocksDB Tuning Guide at http://mng.bz/I88k. \nCreating repartitioning topics ahead of time\nIn Kafka Streams, any time you perform an operation that may potentially change the\nmap key—transform or groupBy, for example—an internal flag is set in the Streams-\nBuilder class, indicating that repartitioning will be required. Now, performing a map\nor transform won’t automatically force the creation of a repartitioning topic and the\nrepartitioning operation; but as soon as you add an operation using the updated key, a\nrepartitioning operation will be triggered.\n Although this is a required step (covered in chapter 4), in some cases, it’s better to\nrepartition the data yourself ahead of time. Consider the following (abbreviated)\nexample:\nKStream<String, String> mappedStream =\n➥ streamsBuilder.stream(\"inputTopic\").map(....);  \nKTable<Windowed<String>, Long> ktable1 =\n➥ mappedStream.groupByKey().windowedBy...count() \nKTable<Windowed<String>, Long> ktable2 =\n➥ mappedStream.groupByKey().windowedBy...count() \nKTable<Windowed<String>, Long> ktable3 =\n➥ mappedStream.groupByKey().windowedBy...count() \nHere, you map the original stream to create a new key to group by. You want to per-\nform three counts with three different windowing options—a legitimate use case. But\nbecause you mapped to a new key, each windowed count operation creates a new\nrepartition topic. Again, the need for a repartition topic makes sense due to the\nchanged key, but having three repartition topics duplicates data when you need only\none repartition topic.\nMaps the original input \nstream to create a new key\nWindowed \ncount option 1\nWindowed \ncount option 2\nWindowed \ncount option 3\n \n",
      "content_length": 2678,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "249\nConfiguring internal topics\n The solution to this issue is simple: after your map call, you immediately use a\nthrough operation to partition the data. Then, the subsequent groupByKey calls won’t\ntrigger repartitioning, because the groupByKey operator does not set the repartition-\nneeded flag. Here’s the revised code:\nKStream<String, String> mappedStream =\n➥ streamsBuilder.stream(\"inputTopic\").map(....).through(...);  \nBy adding the through processor and repartitioning manually, you have one reparti-\ntion topic instead of three. \nConfiguring internal topics\nWhen building a topology, depending on the processors you add, Kafka Streams may\ncreate several internal topics. These internal topics can be changelogs for backing up\nstate stores, or repartition topics. Depending on your data volume, these internal top-\nics can consume a large amount of space. Additionally, even though changelog topics\nare by default created with a cleanup policy of \"compact\", if you have many unique\nkeys, these compacted topics can grow in size. With this in mind, it’s a good idea to\nconfigure your internal topics to keep their size manageable.\n You have two options for managing internal topics. First, you can provide configs\ndirectly when creating state stores, using either StoreBuilder.withLoggingEnabled\nor Materialized.withLoggingEnabled. Which method you use depends on how\nyou create the state store. Both methods take a Map<String, String> containing the\ntopic properties. You can see an example in src/main/java/bbejeck/chapter_7/\nCoGroupingListeningExampleApplication.\n The other option for managing internal topics is to provide configurations for\nthem when configuring your Kafka Streams application:\nProperties props = new Properties();\n// other properties set here\nprops.put(StreamsConfig.topicPrefix(\"retention.bytes\"), 1024 * 1024);\nprops.put(StreamsConfig.topicPrefix(\"retention.ms\"), 3600000);\nWhen using the StreamsConfig.topicPrefix approach, the provided settings are\napplied globally to all internal topics. Any topic settings provided when creating a\nstate store will take precedence over the settings provided with StreamsConfig.\n I can’t give you much advice regarding what settings to use, because that depends\non your particular use case. But keep in mind that the default size of a topic is unlim-\nited and the default retention time is one week, so you should adjust the retention\n.bytes and retention.ms settings. In addition, for changelogs backing state stores\nwith many unique keys, you can set cleanup.policy to compact, delete to ensure that\nthe topic size stays manageable. \nMaps the original \ninput stream to \ncreate a new key, \nand repartitions\n \n",
      "content_length": 2680,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "250\nAPPENDIX A\nAdditional configuration information\nResetting your Kafka Streams application\nAt some point, you may need to start a Kafka Streams application over and reprocess\ndata, either in development or after a code update. To do this, Kafka Streams pro-\nvides a kafka-streams-application-reset.sh script in the bin directory of the Kafka\ninstallation.\n The script has one required parameter: the application ID of the Kafka Streams\napplication. The script offers several options, but in a nutshell, it can reset input top-\nics to the earliest available offset, reset intermediate topics to the latest offset, and\ndelete any internal topics. Note that you’ll need to call KafkaStreams.cleanUp the\nnext time you start your application, to delete any local state from previous runs. \nCleaning up local state\nChapter 4 discussed how Kafka Streams stores local state per task on the local filesys-\ntem. During development or testing, or when migrating to a new instance, you may\nwant to clean out all previous local state.\n To clean up any previous state, you can use KafkaStreams.cleanUp either before\nyou call KafkaStreams.start or after KafkaStreams.stop. Using the cleanUp method\nat any other time will result in an error. \n \n",
      "content_length": 1231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "251\nappendix B\nExactly once semantics\nKafka achieved a major milestone with the release of version 0.11.0: exactly once\nsemantics. Prior to this release of Kafka, the delivery semantics of Kafka could have\nbeen described as at-least-once or at-most-once, depending on the producer.\n In the case of at-least-once delivery, a broker can persist a message but experi-\nence an error before sending the acknowledgment back to the producer, assuming\nthe producer is configured with asks=\"all\" and times out waiting for the acknowl-\nedgment. If the producer is configured with retries greater than zero, it will resend\nthe message, unaware that the previous message was successfully persisted. In this\nscenario (although rare), a duplicate message is delivered to consumers—hence,\nthe phrase at least once.\n For the at-most-once condition, consider the case where a producer is config-\nured with retries set to zero. In the previous example, the message in question\nwould be delivered only once, because there are no retries. But if the broker\nexperiences an error before it can persist the message, the message won’t be sent.\nIn this case, you’ve traded receiving all messages for not receiving any duplicate\nmessages.\n With exactly once semantics, even in situations where a producer resends a mes-\nsage that was previously persisted to a topic, consumers will receive the message\nexactly once. To enable transactions or exactly once processing with a Kafka-\nProducer, you add a configuration transactional.id and a couple of method calls,\nas shown in the following example. Otherwise, producing messages with transactions\nlooks familiar. Note that this excerpt doesn’t stand alone—it’s provided to highlight\nwhat’s required to produce and consume messages with the transactional API:\nProperties props = new Properties();\nprops.put(\"bootstrap.servers\", \"localhost:9092\");\nprops.put(\"transactional.id\", \"transactional-id\");\n \n",
      "content_length": 1920,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "252\nAPPENDIX B\nExactly once semantics\nProducer<String, String> producer =\n➥ new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());\nproducer.initTransactions();  \ntry {\n// called right before sending any records\nproducer.beginTransaction();\n...sending some messages\n// when done sending, commit the transaction\nproducer.commitTransaction();\n} catch (ProducerFencedException | OutOfOrderSequenceException |\n➥ AuthorizationException e) {\nproducer.close();   \n} catch (KafkaException e) {\nproducer.abortTransaction();     \n}\nTo use KafkaConsumer with transactions, you need to add only one configuration:\nprops.put(\"isolation.level\", \"read_committed\");\nIn read_committed mode, KafkaConsumer only reads successfully committed transac-\ntional messages. The default setting is read_uncommitted, which returns all messages.\nNon-transactional messages are always retrieved in either configuration setting.\n The impact of exactly once semantics is a big win for Kafka Streams. With exactly\nonce, or transactions, you’re guaranteed to process records through a topology\nexactly once.\n To enable exactly once processing with Kafka Streams, set StreamsConfig.PROCESSING\n_GUARANTEE_CONFIG to exactly_once. The default setting for PROCESSING_GUARANTEE\n_CONFIG is at_least_once, or non-transactional processing. With that simple configu-\nration setting, Kafka Streams handles all required steps for performing transactional\nprocessing.\n This has been a quick overview of Kafka’s transactional API. For more information,\ncheck out the following resources:\n■\nDylan Scott, Kafka in Action (Manning, forthcoming), www.manning.com/books/\nkafka-in-action\n■\nNeha Narkhede, “Exactly-once Semantics Are Possible: Here’s How Kafka Does\nIt,” Confluent, June 30, 2017, http://mng.bz/t9rO\n■\nApurva Mehta and Jason Gustafson, “Transactions in Apache Kafka,” Confluent,\nNovember 17, 2017, http://mng.bz/YKqf\n■\nGuozhang Wang, “Enabling Exactly-Once in Kafka Streams,” Confluent, Decem-\nber 13, 2017, http://mng.bz/2A32\nWhen setting transactional.id, \nyou need to call this method \nbefore any others.\nThe only option for any of the \nnon-recoverable exceptions is \nto close the producer.\nFor any other exception, \nabort and retry.\n \n",
      "content_length": 2220,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "253\nindex\nA\nabstractions, higher-level vs. \nmore control 146\nAbstractProcessor class 150\naccess control 36\nadder() method 131\naddProcessor() method 148\naddSource() method 148\nadvanceBy() method 138\nadvanced applications with \nKafka Streams\nintegrating Kafka with other \ndata sources 218–226\nKafka Connect 219–222\ntransforming data 222–226\ninteractive queries 226–237\ncoding 232–234\noverview 228–229\nquery server 234–237\nstate stores 230–232\nKSQL 237–244\narchitecture 238–240\nconfiguring 244\ninstalling and running 240\nqueries 242–243\nstreams 241–242\ntables 243–244\nAggregatingMethodHandle-\nProcessor 206\nAggregatingMethodHandle-\nProcessorTest 206\naggregations 126–144\nGlobalKTables 140–143\njoining KStreams with\n141–143\njoining with smaller \ndatasets 141\nrepartitioning, cost of 141\njoining KStreams and \nKTables 139–140\nconverting KTable to \nKStream 139\ncreating financial news \nKTable 140\njoining news updates with \ntransaction counts 140\nQueryable state 143–144\nshare volume by \nindustry 127–132\napplication metrics 182–191\ncollected metrics 185\nmetrics configuration\n184–185\nusing JMX 185–189\nJConsole, starting 186\nmonitoring running \nprogram 186–188\nviewing information\n188–189\nviewing metrics 189–191\narchitecture, Kafka 25–39\ncontroller\nelecting 34\nresponsibilities of 35–36\nlogs 27–28\ncompacting 38–39\ndeleting 37–38\ndistributed 32\nmanagement of 37\nmessage broker 26\npartitions 28–29\ndetermining correct num-\nber of 32\ngrouping data by key 29–30\nspecifying custom \npartitioner 31–32\nwriting custom \npartitioner 30–31\nreplication 34\nZooKeeper 33–34\nat-least-once condition 251\nat-most-once condition 251\nAtomicInteger 43\nautomatic offset commits 46\nB\nbackpressure 15\nbatch processing 4\nbatch processing, inadequacy \nfor 8\nbig data 4–8\nbatch processing, inadequacy \nfor 8\ngenesis of 4–5\nMapReduce 5–8\ndistributing data across \ncluster to achieve scale \nin processing 6–7\nembracing failure by using \nreplication 8\nusing key/value pairs and \npartitions 7–8\nbootstrap servers 42\nbranch() method 79\nbroker 25\nbroker outages 246\n \n",
      "content_length": 2033,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "INDEX\n254\nbucketing 118\nbuilder.stream method 128\nC\ncache operation 124\nCallback.onComplete \nmethod 41\nCassandra 70\nchangelog, updates to\n119–121\n@ClassRule annotation 210\ncleaning local state 250\nClickEventProcessor 164\nclose() method 90, 150\ncluster 25\ncluster membership 36\nco-group processor 159–170\nprocessor nodes, adding\n162–167\nsink node, adding 168–170\nsource nodes, defining 161\nstate store, adding 167–168\nCoGrouping-Processor 169\ncommitSync() method 46\ncompression type 42\nConcurrentHashMap 196\nconfiguration\ncleaning local state 250\ncreating repartitioning topics \nahead of time 248–249\nhandling deserialization \nerrors 246–247\ninternal topics 249\nlimiting number of rebalances \non startup 245–246\nlocal 49–50\nresetting Kafka Streams \napplication 250\nresilience to broker \noutages 246\nRocksDB 248\nscaling up application\n247–248\nConsumed class 60\nconsumer group 47\nconsumer lag 177\nconsumers 25\nconsumers, reading messages \nwith 44–49\nconsumer example 48–49\ncreating consumer 47\nfiner-grained consumer \nassignment 48\noffsets 44–46\npartitions and consumers 47\nrebalancing 47–48\ncontroller\nelecting 34\nresponsibilities of 35–36\nco-partitioning 108–109\ncount() method 135\ncredit cards\nmasking node 12\nmasking numbers 17–19\ncustom partitioner\nspecifying 31–32\nwriting 30–31\ncustomer data 65–74\nconstructing topology 66–72\nbuilding source node\n66–67\nfunctional programming \nhints 67\nlast processor 70–72\nsecond processor 68–69\nthird processor 69–70\ncreating custom Serde 72–74\ncustomer IDs, keys \ncontaining 103–104\ncustomer rewards 10, 19, 88–89\nD\ndata locality 96–97\nDEBUG level 184\ndebugging Kafka Streams\n191–198\ngetting notification on vari-\nous states of \napplication 192–193\nState restore listener\n195–198\nuncaught exception \nhandler 198\nusing StateListener 193–195\nviewing representation of \napplication 191–192\nDefaultPartitioner 31\ndeserialization errors 246–247\nDeserializationExceptionHan-\ndler interface 246\ndeveloping Kafka Streams\ncustomer data 65–74\nconstructing topology\n66–72\ncreating custom Serde\n72–74\nHello World 58–65\nconfiguration 63\nSerde creation 63–65\ntopology for Yelling \nApp 59–63\ninteractive development\n74–76\nnext steps 76–83\nStreams Processor API 58\ndisableLogging() method 99\ndistributed logs 32\nE\neconomic forecasting 9\nelectronicsStream \nparameter 107\nembedded Kafka cluster 209\nEmbeddedKafkaCluster 210, \n214\nETL (extract, transform, load) 3\nevent streams, vs. update \nstreams 122–123\nevent time 111\nExtractKey 224\nExtractRecordMetadataTime-\nstamp class 113\nF\nFailOnInvalidTimestamp \nclass 113\nfailure\nembracing by using \nreplication 8\nrecovery from 97–98\nfault tolerance 97–98\nfetchFromWindowStore \nmethod 236\nfixedQueue variable 131\nFixedSizePriorityQueue 131\nFlattenStruct 225\nfluent interface 58\nforeach actions 82–83\nForeachAction interface 76, 83\nfraud 9\nfrom-beginning parameter 53\nfunctional programming 67\nG\nGlobalKTables 140–143\njoining KStreams with\n141–143\njoining with smaller \ndatasets 141\nrepartitioning, cost of 141\n \n",
      "content_length": 2977,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "INDEX\n255\ngraph of processing nodes\n15–16\nGroupBy method 129\ngroupBy operation 135\nGroupByKey method 129\ngroupByKey operator 249\nGroupCoordinator 245\ngrouping data, by key 29–30\ngrouping records 7\nGson 73\nH\nhashing function 7\nHello World 58–65\nconfiguration 63\nSerde creation 63–65\ntopology for Yelling App\n59–63\nhigher-level abstractions vs. \nmore control 146\nhopping windows 133, 137–139\nI\nIDs, customer, keys \ncontaining 103–104\nINFO level 184\ningestion time 112\ninit() method 90, 151\ninstalling\nKafka 49–50\nKSQL 240\ninteractive development 74–76\ninteractive queries 143, 226–237\ncoding 232–234\ninside query server 234–237\noverview 228–229\nstate stores 229–232\nInteractiveQueryServer \nclass 233\ninternal topics, configuring 249\nintrusion detection 9\nISR (in-sync replica) 34\nJ\nJava VisualVM 185\nJConsole, starting 186\nJMX, application metrics \nusing 185–189\nJConsole, starting 186\nmonitoring running \nprogram 186–188\nviewing information 188–189\njoin() method 105\njoined parameter 107\njoining\nKafka Streams 100–110\nco-partitioning 108–109\ndata setup 102–103\nimplementation 106–108\nkeys containing customer \nIDs 103–104\nleft-outer join 110\nouter joins 109\npurchase records 105–106\nKStreams and KTables\n139–140\nconverting KTable to \nKStream 139\ncreating financial news \nKTable 140\njoining news updates with \ntransaction counts 140\nJoinWindows.after method 108\nJoinWindows.before \nmethod 108\nJoinWindows.of method 107\nJUnit rules 210–211\nK\nKafka\ninstalling, local \nconfiguration 49–50\nstarting 50\nSee also architecture, Kafka\nKafka Connect 218–222\nKafka Streams 15–16\nAPI, integrating Processor API \nwith 170–171\ndebugging techniques\n191–198\ngetting notification on \nvarious states of \napplication 192–193\nState restore listener\n195–198\nuncaught exception \nhandler 198\nusing StateListener\n193–195\nviewing representation of \napplication 191–192\nevents 85–86\njoining 100–110\nco-partitioning 108–109\ndata setup 102–103\nimplementation 106–108\nkeys containing customer \nIDs 103–104\nleft-outer join 110\nouter joins 109\npurchase records 105–106\nstate and 86\nstate stores\nconfiguring changelog \ntopics 99–100\nusing for lookups and \npreviously seen \ndata 96–100\nstateful operations, applying \nto 86–96\nmapping Purchase object to \nRewardAccumulator \nusing state 90–94\nstateful customer \nrewards 88–89\ntransformValues \nprocessor 87\nupdating rewards \nprocessor 94–96\nvalue transformer, \ninitializing 90\nTimestampExtractor\ncustom 114–115\nprovided implementations\n112–113\nspecifying 115\nWallclockTimestamp-\nExtractor 113\nSee also advanced applications \nwith Kafka Streams; devel-\noping Kafka Streams; pur-\nchase transaction; stream \nprocessing\nKafka Streams application\nresetting 250\ntesting\nintegration testing\n208–214\ntopology 201–208\nKafkaConsumer 252\nKafkaProducer 42, 251\nkey/value pairs 7–8, 15\nkey/value store suppliers 99\nkeys\ncontaining customer \nIDs 103–104\ngrouping data by 29–30\nKeyValueIterator 167\nKGroupedStream 127\nKGroupedStream.windowedBy \nmethod 132\n \n",
      "content_length": 2956,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "INDEX\n256\nKSQL 237–244\narchitecture 238–240\nconfiguring 244\ninstalling and running 240\nqueries 242–243\nstreams 238, 241–242\ntables 238, 243–244\nKStream.mapValues \nfunction 61, 83\nKStreamNot 78\nKStreams, joining to \nKTables 139–140\nconverting KTable to \nKStream 139\ncreating financial news \nKTable 140\njoining news updates with \ntransaction counts 140\nKStream.through() method\n93\nKStream.to() method 109\nKTable API\naggregations 126–144\nGlobalKTables 140–143\njoining KStreams and \nKTables 139–140\nQueryable state 143–144\nshare volume by industry\n127–132\nrecord updates and KTable \nconfiguration 123–126\ncache buffering size\n124–125\ncommit interval 125–126\nstreams and tables, \nrelationship between\n118–123\nevent streams vs. update \nstreams 122–123\nrecord stream 118–119\nupdates to records or \nchangelog 119–121\nwindowing operations\n132–139\ncounting stock transactions \nby customer 132\nsession windows 133–134\nsession windows, using to \ntrack stock \ntransactions 134–136\nsliding or hopping \nwindows 137–139\ntumbling windows\n136–137\nwindow types 133\nKTables, joining to \nKStreams 139–140\nconverting KTable to \nKStream 139\ncreating financial news \nKTable 140\njoining news updates with \ntransaction counts 140\nKTable.toStream() method 139\nL\nleft-outer join 110\nlist command 178\nlocal configuration 49–50\nlocal state, cleaning 250\nlog rolling 37\nLogAndSkipOnInvalidTime-\nstamp class 113\nLoggingStateRestoreListener\n197\nlogs 27–28\ncompacting 38–39\ndeleting 37–38\ndistributed 32\nmanagement of 37\nM\nmanual offset commits 46\nMapReduce 5–8\ndistributing data across cluster \nto achieve scale in \nprocessing 6–7\nembracing failure by using \nreplication 8\nusing key/value pairs and \npartitions 7–8\nmapValues method 89, 94\nMaterialized class 98\nmaterialized views 143\nmessage broker 26\nmessages\nreading 53–54\nreading with consumers\n44–49\nconsumer example 48–49\ncreating consumer 47\nfiner-grained consumer \nassignment 48\noffsets 44–46\npartitions and \nconsumers 47\nrebalancing 47–48\nsending first message 52–54\nsending with producers\n40–44\npartitions, specifying\n42–43\nproducer properties 42\ntimestamps 43–44\nmetrics. See application metrics\nmicrobatching 4\nmicroservices 233\nMockito 205\nmonitoring and performance\napplication metrics 182–191\ncollected metrics 185\nmetrics configuration\n184–185\nusing JMX 185–189\nviewing metrics 189–191\nbasic monitoring 176–182\nconsumer and producer \nperformance, \nmeasuring 176–178\nconsumer lag, checking \nfor 178–179\nproducer and consumer, \nintercepting\n179–182\nKafka Streams, debugging \ntechniques 191–198\nN\nnext() method 47\nnull value 39\nO\nonBatchRestored method 197\nonRestoreStart method 196\nORM (object-relational \nmapping) 146\nouter joins 109\nP\nPageRank algorithm 5\nPairRDDFunctions.cogroup \nmethod 161\npartitioner class 42\nPartitioner interface 31\npartitioning streams 79\npartitions 7, 28–29\nconsumers and 47\ndetermining correct number \nof 32\ngrouping data by key 29–30\n \n",
      "content_length": 2895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "INDEX\n257\npartitions (continued)\nspecifying 42–43\nspecifying custom partitioner\n31–32\nwriting custom partitioner\n30–31\npartitions flag 53\npatterns node 13\nperformance. See monitoring \nand performance\nprint() method 76, 122–123\nPrinted.toSysOut() method 75\nPrinted.withKeyValueMapper \nmethod 75\nprivacy 10\nprocess() method 149, 157–158\nprocessing nodes, graph of\n15–16\nprocessing time 112\nProcessor API\nco-group processor 159–170\nprocessor nodes, adding\n162–167\nsink node, adding 168–170\nsource nodes, defining 161\nstate store, adding\n167–168\nhigher-level abstractions vs. \nmore control 146\nintegrating with Kafka \nStreams API 170–171\nstock analysis processor\n152–159\nprocess( ) method\n157–158\npunctuator execution\n158–159\nstock-performance proces-\nsor application\n153–156\ntopology 146–152\nprocessor node 148–151\nsink node 151–152\nsource node 147–148\nprocessor node, Processor \nAPI 148–151\nProcessorContext object 154\nProcessor.punctuate method\n110\nProcessorSupplier() method\n148\nProcessorTopologyTestDriver\n201, 214\nProducerInterceptor 182\nproducers 25\nproducers, sending messages \nwith 40–44\npartitions, specifying 42–43\nproducer properties 42\ntimestamps 43–44\npunctuate() method 87, 150, \n155, 206\nPurchase object 90–94\npurchase patterns 18–19\npurchase records\ngeneral discussion 105–106\nwriting 20\npurchase transaction\nchanging perspective on\n12–13\ncredit card masking node 12\npatterns node 13\nrewards node 13\nsource node 12\nstorage node 13\nflow of, applying to Kafka \nStreams to 16–20\ncustomer rewards 19\nmasking credit card \nnumbers 17–19\npurchase patterns 18–19\nsource, defining 16–17\nwriting purchase \nrecords 20\nhandling 10–11\ndeconstructing require-\nments into graph 11\nweighing stream-processing \noption 10–11\npurchaseJoiner parameter 107\nPurchaseKeyPartitioner 30\nPurchasePattern object 18\nQ\nqueries\ninteractive 226–237\ncoding 232–234\ninside query server 234–237\noverview 228–229\nstate stores 229–232\nKSQL 242–243\nQueryable state 143–144\nR\nreading messages, with \nconsumers 44–49\nconsumer example 48–49\ncreating consumer 47\nfiner-grained consumer \nassignment 48\noffsets 44–46\npartitions and consumers 47\nrebalancing 47–48\nReadOnlyWindowStore 235\nrebalances, limiting number of \non startup 245–246\nrecord stream 118–119\nrecords, updates to 119–121\nrepartitioning\ncreating topics ahead of \ntime 248–249\ngeneral discussion 92–94\nrepeatable tests 200\nReplaceField 222\nreplication 8, 34\nreplication-factor flag 52\nresetting Kafka Streams \napplication 250\nretries 42\nRewardAccumulator object 19, \n88\nRewardAccumulator, mapping \nto Purchase object using \nstate 90–94\nrewards node 13\nrewards processor, updating\n94–96\nRocksDB 99, 248\nS\nsales data 11\nsales transaction data hub\n24–25\nscaling up application\n247–248\nSELECT statements 146\nselectKey method 83, 108\nsending messages, with \nproducers 40–44\npartitions, specifying 42–43\nproducer properties 42\ntimestamps 43–44\nSerde class 60, 63–65\nSerde.String() method 72\nserialization 42\nsession windows 133–134\nShareVolume 127\nShareVolume.sum method 129\nsimpleFirstStream 59\nsink node, Processor API\n151–152\nsliding windows 133, 137–139\n \n",
      "content_length": 3089,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "INDEX\n258\nSMA (simple moving average)\n157\nsource, defining 16–17\nsplitting streams 79\nstate 86\nstate stores\nconfiguring changelog \ntopics 99–100\nusing for lookups and \npreviously seen data\n96–100\nstateful operations, applying \nto Kafka Streams 86–96\nmapping Purchase object to \nRewardAccumulator \nusing state 90–94\nstateful customer \nrewards 88–89\ntransformValues \nprocessor 87\nupdating rewards \nprocessor 94–96\nvalue transformer, \ninitializing 90\nState restore listener 195–198\nStateListener 193–195\nStateRestoreListener 196\nStateStore fault tolerance 99\nstock analysis processor\n152–159\nprocess() method 157–158\npunctuator execution\n158–159\nstock-performance processor \napplication 153–156\nstorage 11\nstorage node 13\nStoreBuilder class 98\nstream processing 4\noverview 8–10\nwhen to use and when not \nto 9–10\nStreamPartitioner 94\nStreams Processor API 58\nStreamsBuilder class 248\nStreamsBuilder.stream \nmethod 67\nStreamsConfig.APPLICATION\n_ID_CONFIG property 63\nStreamsConfig.BOOTSTRAP\n_SERVERS_CONFIG \nproperty 63\nStreamTask 92, 155, 247\nStreamThread 155\nsum() method 129\nT\ntables\nKSQL 243–244\nstreams and 118–123\nevent streams vs. update \nstreams 122–123\nrecord stream 118–119\nupdates to records or \nchangelog 119–121\ntest() method 77\ntesting Kafka Streams applica-\ntion\nintegration testing 208–214\ndynamically adding topic\n213–214\nEmbeddedKafkaCluster, \nadding 210\nJUnit rules 210–211\nproducing and consuming \nrecords in test 212–213\ntesting topology 211–212\ntopics, creating 211\ntopology 201–208\nbuilding test 202–204\nprocessors and \ntransformers 205–208\nstate store in 204–205\nTimestampConverter 222\nTimestampExtractor 110–115\ncustom 114–115\nprovided implementations\n112–113\nspecifying 115\nWallclockTimestampExtractor\n113\nTimestampExtractor \ninterface 112\ntimestamps 43–44\ntopic configuration 36\nTopicPartition 245\ntopics 25, 27\ntopology 15\nTopologyBuilderException 108\nTopology.describe() \nmethod 191\ntopologyTestDriver.readOutput \nmethod 203\ntoStream().map function 142\ntoUpperCase() method 61\nTransactionSummary object 139\ntransform() method 91\ntransformValues processor 87\ntransformValues() method 87, \n89, 171\ntumbling windows 133, 136–137\ntwentyMinuteWindow \nparameter 107\nU\nuntil() method 137\nupdate streams, vs. event \nstreams 122–123\nupdates\nto changelog 119–121\nto records 119–121, 123–126\nUsePreviousTimeOnInvalid-\nTimestamp class 113, 148\nV\nvalue transformer, initializing\n90\nValueMapper interface 18, 61\nW\nWallclockTimestampExtractor\n113\nwindowing 118\nwindowing operations 132–139\ncounting stock transactions by \ncustomer 132\nsession windows 133–134\nsliding or hopping \nwindows 137–139\ntumbling windows 136–137\nwindow types 133\nwithLabel() method 75\nY\nYelling App, topology for 59–63\nZ\nZMart’s original data \nplatform 23\nZooKeeper 33–34, 50\n \n",
      "content_length": 2761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "Patterns\nMasking\nSource\nRewards\nPatterns\nsink\nRewards\nsink\nPurchases\nsink\nThis topology takes a single input from\nthe source node and performs several\ntransformations, making this a great\ndemonstration of testing.\n \n",
      "content_length": 216,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "William P. Bejeck Jr.  \nN\not all stream-based applications require a dedicated \nprocessing cluster. The lightweight Kafka Streams \nlibrary provides exactly the power and simplicity you \nneed for message handling in microservices and real-time \nevent processing. With the Kafka Streams API, you ﬁ lter and \ntransform data streams with just Kafka and your application.\nKafka Streams in Action teaches you to implement stream \nprocessing within the Kafka platform. In this easy-to-follow \nbook, you’ll explore real-world examples to collect, trans-\nform, and aggregate data, work with multiple processors, and \nhandle real-time events. You’ll even dive into streaming SQL \nwith KSQL! Practical to the very end, it ﬁ nishes with testing \nand operational aspects, such as monitoring and debugging. \nWhat’s Inside\n● Using the KStream API\n● Filtering, transforming, and splitting data\n● Working with the Processor API\n● Integrating with external systems\nAssumes some experience with distributed systems. No \nknowledge of Kafka or streaming applications required.\nBill Bejeck is a Kafka Streams contributor and Conﬂ uent \nengineer with over 15 years of software development \nexperience.\nTo download their free eBook in PDF, ePub, and Kindle formats, owners \nof this book should visit manning.com/books/kafka-streams-in-action\n$44.99 / Can $59.99  [INCLUDING eBOOK]\nKafka Streams IN ACTION\nSOFTWARE DEVELOPMENT\nM A N N I N G\n“\nA great way to learn \nabout Kafka Streams and \nhow it is a key enabler of \nevent-driven applications.”\n \n—From the Foreword by \nNeha Narkhede\nCocreator of Apache Kafka\n“\nA comprehensive guide \nto Kafka Streams—from \nintroduction to production!”\n \n—Bojan Djurkovic, Cvent\n“\nBridges the gap between \nmessage brokering and real-\n time streaming analytics.”\n—Jim Mantheiy Jr.\nNext Century \n“\nValuable both as an \nintroduction to streams \nas well as an ongoing \n  reference.”\n \n—Robin Coe, TD Bank\nSee first page\n",
      "content_length": 1926,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}