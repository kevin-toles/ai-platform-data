{
  "metadata": {
    "title": "Designing Cloud Data Platforms",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 337,
    "conversion_date": "2025-12-25T18:13:00.847896",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Designing Cloud Data Platforms.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Introducing the data platform",
      "start_page": 24,
      "end_page": 41,
      "detection_method": "regex_chapter",
      "content": "5\nData warehouses struggle with data variety, volume, and velocity\n1.2.2\nVolume \nData volume is everyone’s problem. In today’s internet-enabled world, even a small\norganization may need to process and analyze terabytes of data. IT departments are\nregularly being asked to corral more and more data. Clickstreams of user activity from\nwebsites, social media data, third-party data sets, and machine-generated data from\nIoT sensors all produce high-volume data sets that businesses often need to access. \nIn a traditional data warehouse (figure 1.3), storage and processing are coupled\ntogether, significantly limiting scalability and flexibility. To accommodate a surge in\ndata volume in traditional relational data warehouses, bigger servers with more disk,\nRAM, and CPU to process the data must be purchased and installed. This approach is\nslow and very expensive, because you can’t get storage without compute, and buying\nmore servers to increase storage means that you are likely paying for compute that you\nmight not need, or vice versa. Storage appliances evolved as a solution to this problem\nbut did not eliminate the challenges of easily scaling compute and storage at a cost-\neffective ratio. The bottom line is that in a traditional data warehouse design, process-\ning large volumes of data is available only to organizations with significant IT budgets. \n1.2.3\nVelocity\nData velocity, the speed at which data arrives into your data system and is processed,\nmight not be a problem for you today, but with analytics going real-time, it’s just a\nquestion of when, not if. With the increasing proliferation of sensors, streaming data\nis becoming commonplace. In addition to the growing need to ingest and process\nstreaming data, there’s increasing demand to produce analytics in as close to real-time\nas possible.\n Traditional data warehouses are batch-oriented: take nightly data, load it into a\nstaging area, apply business logic, and load your fact and dimension tables. This\nmeans that your data and analytics are delayed until these processes are completed for\nall new data in a batch. Streaming data is available more quickly but forces you to deal\nwith each data point separately as it comes in. This doesn’t work in a data warehouse\nand requires a whole new infrastructure to deliver data over the network, buffer it in\nmemory, provide reliability of computation, etc. \nData\nsources\nRDBMS data warehouse\nStorage\nSQL\nProcessing\nETL\ntool\nFigure 1.3\nIn traditional \ndata warehouses, storage \nand processing are coupled.\n\n\n6\nCHAPTER 1\nIntroducing the data platform\n1.2.4\nAll the V’s at once\nThe emergence of artificial intelligence and its popular subset, machine learning, cre-\nates a trifecta of V’s. When data scientists become users of your data systems, volume\nand variety challenges come into play all at once. Machine learning models love\ndata—lots and lots of it (i.e., volume). Models developed by data scientists usually\nrequire access not just to the organized, curated data in the data warehouse, but also\nto the raw source-file data of all types that’s typically not brought into the data ware-\nhouse (i.e., variety). Their models are compute intensive, and when run against data\nin a data warehouse, put enormous performance pressure on the system, especially\nwhen they run against data arriving in near-real time (velocity). With current data\nwarehouse architectures, these models often take hours or even days to run. They also\nimpact warehouse performance for all other users while they’re running. Finding a\nway to give data scientists access to high-volume, high-variety data will allow you to cap-\nitalize on the promise of advanced analytics while reducing its impact on other users\nand, if done correctly, it can keep costs lower.\n1.3\nData lakes to the rescue?\nA data lake, as defined by TechTarget’s WhatIs.com is “A storage repository that holds\na vast amount of raw data in its native format until it is needed.” Gartner Research adds\na bit more context in its definition: “A collection of storage instances of various data\nassets additional to the originating data sources. These assets are stored in a near-exact\n(or even exact) copy of the source format. As a result, the data lake is an unintegrated,\nnon-subject-oriented collection of data.” \n The concept of a data lake evolved from these megatrends mentioned previously,\nas organizations desperately needed a way to deal with increasing numbers of data for-\nmats and growing volumes and velocities of data that traditional data warehouses\ncouldn’t handle. The data lake was to be the place where you could bring any data you\nwant, from different sources, structured, unstructured, semistructured, or binary. It\nwas the place where you could store and process all your data in a scalable manner.\n After the introduction of Apache Hadoop in 2006, data lakes became synonymous with\nthe ecosystem of open source software utilities, known simply as “Hadoop,” that provided\na software framework for distributed storage and processing of big data using a network\nof many computers to solve problems involving massive amounts of data and computa-\ntion. While most would argue that Hadoop is more than a data lake, it did address some\nof the variety, velocity, and volume challenges discussed earlier in this chapter:\nVariety—Hadoop’s ability to do schema on read (versus the data warehouse’s\nschema on write) meant that any file in any format could be immediately stored\non the system, and processing could take place later. Unlike data warehouses,\nwhere processing could only be done on the structured data in the data ware-\nhouse, processing in Hadoop could be done on any data type.\n\n\n7\nAlong came the cloud\nVolume—Unlike the expensive, specialized hardware often required for ware-\nhouses, Hadoop systems took advantage of distributed processing and storage\nacross less expensive commodity hardware that could be added in smaller incre-\nments as needed. This made storage less expensive, and the distributed nature\nof processing made it easier and faster to do processing because the workload\ncould be split among many servers. \nVelocity—When it came to streaming and real-time processing, ingesting and\nstoring streaming data was easy and inexpensive on Hadoop. It was also possi-\nble, with the help of some custom code, to do real-time processing on Hadoop\nusing products such as Hive or MapReduce or, more recently, Spark.\nHadoop’s ability to cost-effectively store and process huge amounts of data in its native\nformat was a step in the right direction towards handing variety, volume, and velocity\nof today’s data estate, and for almost a decade, it was the de facto standard for data\nlakes in the data center. \n But Hadoop did have shortcomings:\nIt is a complex system with many integrated components that run on hardware\nin a data center. This makes it difficult to maintain and requires a team of\nhighly skilled support engineers to keep the system secure and operational.\nIt isn’t easy for users who want to access the data. Its unstructured approach to\nstorage, while more flexible than the very structured and curated data ware-\nhouse, is often too difficult for business users to make sense of.\nFrom a developer perspective, its use of an “open” toolset makes it very flexible,\nbut its lack of cohesiveness makes it challenging to use. For example, you can\ninstall any language, library, or utility onto a Hadoop framework to process\ndata, but you would have to know all those languages and libraries instead of\nusing a generic interface such as SQL.\nStorage and compute are not separate, meaning that while the same hardware\ncan be used for both storage and compute, it can only be deployed effectively in\na static ratio. This limits its flexibility and cost-effectiveness.\nAdding hardware to scale the system often takes months, resulting in a cluster\nthat is either chronically over or underutilized. \nInevitably a better answer came along—one that had the benefits of Hadoop, elimi-\nnated its shortcomings, and brought even more flexibility to designers of data systems.\nAlong came the cloud.\n1.4\nAlong came the cloud\nThe advent of the public cloud, with its on-demand storage, compute resource provi-\nsioning, and pay-per-usage pricing model, allowed data lake design to move beyond\nthe limitations of Hadoop. The public cloud allowed the data lake to include more\nflexibility in design and scalability and be more cost effective while drastically reduc-\ning the amount of support required. \n\n\n8\nCHAPTER 1\nIntroducing the data platform\n Data warehouses and data lakes have moved to the cloud and are increasingly\noffered as a platform as a service (PaaS), defined by Wikipedia as “a category of cloud\ncomputing services that provides a platform allowing customers to develop, run, and\nmanage applications without the complexity of building and maintaining the infra-\nstructure typically associated with developing and launching an app.” Using PaaS\nallows organizations to take advantage of additional flexibility and cost-effective scal-\nability. There’s also a new generation of data processing frameworks available only in\nthe cloud that combine scalability with support for modern programming languages\nand integrate well into the overall cloud paradigm.\n The advent of the public cloud changed everything when it came to analytics data\nsystems. It allowed data lake design to move beyond the limitations of Hadoop and\nallowed for the creation of a combined data lake and data warehouse solution that\nwent far beyond what was available on premises.\n The cloud brought so many things, but topping the list were the following:\nElastic resources —Whether you’re talking storage or compute, you can get either\nfrom your favorite cloud vendor: the amount of that resource is allocated to you\nexactly as you need it; and it grows and shrinks as your needs change—automat-\nically or by request. \nModularity—Storage and compute are separate in a cloud world. No longer do\nyou have to buy both when you need only one, which optimizes your invest-\nment.\nPay per use—Nothing is more irksome than paying for something you aren’t\nusing. In a cloud world, you only pay for what you use so you no longer have to\ninvest in overprovisioned systems in anticipation of future demand. \nCloud turns capital investment, capital budgets, and capital amortization into opera-\ntional expense—This is tied to pay per use. Compute and storage resources are\nnow utilities rather than owned infrastructure.\nManaged services are the norm—In an on-premises world, human resources are\nneeded for the operation, support, and updating of a data system. In a cloud\nworld, much of these functions are done by the cloud provider and are\nincluded in the use of the services.\nInstant availability—Ordering and deploying a new server can take months.\nOrdering and deploying a cloud service takes minutes.\nA new generation of cloud-only processing frameworks—There’s a new generation of\ndata processing frameworks available only in the cloud that combine scalability\nwith support for modern programming languages and integrate well into the\noverall cloud paradigm.\nFaster feature introduction—Data warehouses have moved to the cloud and are\nincreasingly offered as PaaS, allowing organizations to take instant advantage of\nnew features.\nLet’s look at an example: Amazon Web Services (AWS) EMR.\n\n\n9\nCloud, data lakes, and data warehouses: The emergence of cloud data platforms\n AWS EMR is a cloud data platform for processing data using open source tools. It is\noffered as a managed service from AWS and allows you to run Hadoop and Spark jobs\non AWS. All you need to do to create a new cluster is to specify how many virtual\nmachines you need and what type of machines you want. You also need to provide a\nlist of software you want to install on the cluster, and AWS will do the rest for you. In\nseveral minutes you have a fully functional cluster up and running. Compare that to\nmonths of planning, procuring, deploying, and configuring an on-premises Hadoop\ncluster! Additionally, AWS EMR allows you to store data on AWS S3 and process the\ndata on an AWS EMR cluster without permanently storing any data on AWS EMR\nmachines. This unlocks a lot of flexibility in the number of clusters you can run and\ntheir configuration and allows you to create ephemeral clusters that can be disposed\nof once their job is done. \n1.5\nCloud, data lakes, and data warehouses: The emergence \nof cloud data platforms\nThe argument for a data lake is tied to the dramatic increases in variety, volume, and\nvelocity of today’s analytic data, along with the limitations of traditional data ware-\nhouses to accommodate these increases. We’ve described how a data warehouse alone\nstruggles to cost-effectively accommodate the variety of data that IT must make avail-\nable. It’s also more expensive and complicated to store and process these growing vol-\numes and velocities of data in a data warehouse, instead of in a combination of a data\nlake and a data warehouse.\n A data lake easily and cost-effectively handles an almost unlimited variety, volume,\nand velocity of data. The caveat is that it’s not usually organized in a way that’s useful\nto most users—business users in particular. Much of the data in a data lake is also\nungoverned, which presents other challenges. It may be that in the future a modern\ndata lake will completely replace the data warehouse, but for now, based on what we\nsee in all our customer environments, a data lake is almost always coupled with a data\nwarehouse. The data warehouse serves as the primary governed data consumption\npoint for business users, while direct user access to the largely ungoverned data in a\ndata lake is typically reserved for data exploration either by advanced users, such as\ndata scientists, or other systems.\n Until recently, the data warehouse and/or associated ETL tools are where the\nmajority of data processing took place. But today that processing can occur in the data\nlake itself, moving performance-impacting processing from the more expensive data\nwarehouse to the less expensive data lake. This also provides for new forms of process-\ning, such as streaming, as well as the more traditional batch processing supported by\ndata warehouses.\n While the distinction between a data lake and data warehouse continues to blur,\nthey each have distinct roles to play in the design of a modern analytics platform. There\nare many good reasons to consider a data lake in addition to a cloud data warehouse\ninstead of simply choosing one or the other. A data lake can help balance your users’\n\n\n10\nCHAPTER 1\nIntroducing the data platform\ndesire for immediate access to all the data against the organization’s need to ensure\ndata is properly governed in the warehouse. \n The bottom line is that the combination of new processing technologies available\nin the cloud, a cloud data warehouse, and a cloud data lake enable you to take better\nadvantage of the modularity, flexibility, and elasticity offered in the cloud to meet the\nneeds of the broadest number of use cases. The resulting solution is a modern data\nplatform: cost effective, flexible, and capable of ingesting, integrating, transforming,\nand managing all the V’s to facilitate analytics outcomes. \n The resulting analytics data platform can be far more capable than anything the\ndata center can possibly provide. Designing a cloud data platform to take advantage of\nnew technologies and cloud services to address the needs of the new data consumers\nis the subject of this book.\n1.6\nBuilding blocks of a cloud data platform\nThe purpose of a data platform is to ingest, store, process, and make data available for\nanalysis no matter which type of data comes in—and in the most cost-efficient manner\npossible. To achieve this, well-designed data platforms use a loosely coupled architec-\nture where each layer is responsible for a specific function and interacts with other lay-\ners via their well-defined APIs. The foundational building blocks of a data platform\nare ingestion, storage, processing, and serving layers, as illustrated in figure 1.4.\n1.6.1\nIngestion layer\nThe ingestion layer is all about getting data into the data platform. It’s responsible for\nreaching out to various data sources such as relational or NoSQL databases, file stor-\nage, or internal or third-party APIs, and extracting data from them. With the prolifer-\nation of different data sources that organizations want to feed their analytics, this layer\nmust be very flexible. To this end, the ingestion layer is often implemented using a\nvariety of open source or commercial tools, each specialized to a specific data type. \nIngest\nStorage\nProcessing\nServing\nCloud data\nwarehouse\nAPIs\nData export\nFigure 1.4\nWell-designed data platforms use a loosely coupled architecture where each layer is \nresponsible for a specific function.\n\n\n11\nBuilding blocks of a cloud data platform\n One of the most important characteristics of a data platform’s ingestion layer is\nthat this layer should not modify and transform incoming data in any way. This is to\nmake sure that the raw, unprocessed data is always available in the lake for data lin-\neage tracking and reprocessing.\n1.6.2\nStorage layer\nOnce we’ve acquired the data from the source, it must be stored. This is where data\nlake storage comes into play. An important characteristic of a data lake storage system\nis that it must be scalable and inexpensive, so as to accommodate the vast amounts\nand velocity of data being produced today. The scalability requirement is also driven\nby the need to store all incoming data in its raw format, as well as the results of differ-\nent data transformations or experiments that data lake users apply to the data.\n A standard way to obtain scalable storage in a data center is to use a large disk array\nor Network-Attached Storage. These enterprise-level solutions provide access to large\nvolumes of storage, but have two key drawbacks: they’re usually expensive, and they\ntypically come with a predefined capacity. This means you must buy more devices to\nget more storage.\n Given these factors, it’s not surprising that flexible storage was one of the first ser-\nvices offered by cloud vendors. Cloud storage doesn’t impose any restrictions on the\ntypes of files you can upload—you’ve got free rein to bring in text files like CSV or\nJSON and binary files like Avro, Parquet, images, or video—just about anything can be\nstored in the data lake. This ability to store any file format is an important foundation\nof a data lake because it allows you to store raw, unprocessed data and delay its pro-\ncessing until later.\n For users who have worked with Network-Attached Storage or Hadoop Distributed\nFile System (HDFS), cloud storage may look and feel very similar to one of those sys-\ntems. But there are some important differences:\nCloud storage is fully managed by a cloud provider. This means you don’t need\nto worry about maintenance, software or hardware upgrades, etc.\nCloud storage is elastic. This means cloud vendors will only allocate the amount\nof storage you need, growing or shrinking the volume as requirements dictate.\nYou no longer need to overprovision storage system capacity in anticipation of\nfuture demand.\nYou only pay for the capacity you use.\nThere are no compute resources directly associated with cloud storage. From\nan end-user perspective, there are no virtual machines attached to cloud stor-\nage—this means large volumes of data can be stored without having to take on\nidle compute capacity. When the time comes to process the data, you can easily\nprovision the required compute resources on demand.\nToday, every major cloud provider offers a cloud storage service—and for good rea-\nson. As data flows through the data lake, cloud storage becomes a central component.\n\n\n12\nCHAPTER 1\nIntroducing the data platform\nRaw data is stored in cloud storage and awaits processing, the processing layer saves\nthe results back to cloud storage, and users access either raw or processed data in an\nad hoc fashion.\n1.6.3\nProcessing layer\nAfter data has been saved to cloud storage in its original form, it can now be processed\nto make it more useful. The processing of data is arguably the most interesting part of\nbuilding a data lake. While the data lake’s design makes it possible to perform analysis\ndirectly on the raw data, this may not be the most productive and efficient method.\nUsually, data is transformed to some degree to make it more user-friendly for analysts,\ndata scientists, and others.\n There are several technologies and frameworks available for implementing a pro-\ncessing layer in the cloud data lake, unlike traditional data warehouses, which typically\nlimited you to a SQL engine provided by your database vendor. However, while SQL is\na great query language, it is not a particularly robust programming language. For exam-\nple, it’s difficult to extract common data-cleaning steps into a separate, reusable\nlibrary in pure SQL, simply because it lacks many of the abstraction and modularity\nfeatures of modern programming languages such as Java, Scala, or Python. SQL also\ndoesn’t support unit or integration testing. It’s very difficult to make iterative data\ntransformations or data-cleaning code without good test coverage. Despite these lim-\nitations, SQL is still widely used in data lakes for analyzing data, and in fact many of\nthe data service components provide a SQL interface. \n Another limitation of SQL—in this case, not the language itself, but its implemen-\ntation in RDBMs—is that all data processing must happen inside the database engine.\nThis limits the amount of computational resources available for data processing tasks\nto how many CPU, RAM, or disks are available in a single database server. Even if\nyou’re not processing extremely large data volumes, you may need to process the\nsame data multiple times to satisfy different data transformation or data governance\nrequirements. Having a data processing framework that can scale to handle any\namount of data, along with cloud compute resources you can tap into anytime, makes\nsolving this problem possible. \n Several data processing frameworks have been developed that combine scalability\nwith support for modern programming languages and integrate well into the overall\ncloud paradigm. Most notable among these are\nApache Spark\nApache Beam\nApache Flink\nThere are other, more specialized frameworks out there, but this book will focus on\nthese three. At a high level, each one allows you to write data transformation, valida-\ntion, or cleaning tasks using one of the modern programming languages (usually Java,\nScala, or Python). These frameworks then read the data from scalable cloud storage,\n\n\n13\nBuilding blocks of a cloud data platform\nsplit it into smaller chunks (if the data volume requires it), and finally process these\nchunks using flexible cloud compute resources. \n It’s also important, when thinking about data processing in the data lake, to keep\nin mind the distinction between batch and stream processing. Figure 1.5 shows that\nthe ingestion layer saves data to cloud storage, with the processing layer reading data\nfrom this storage and saving results back to it. \nThis approach works very well for batch processing because while cloud storage is\ninexpensive and scalable, it’s not particularly fast. Reading and writing data can take\nminutes even for moderate volumes of data. More and more use cases now require sig-\nnificantly lower processing times (seconds or less) and are generally solved with\nstream-based data processing. In this case, also shown in the preceding diagram, the\ningestion layer must bypass cloud storage and send data directly to the processing\nlayer. Cloud storage is then used as an archive where data is periodically dumped but\nisn’t used when processing all that streaming data. \n Processing data in the data platform typically includes several distinct steps includ-\ning schema management, data validation, data cleaning, and the production of data\nproducts. We’ll cover these steps in greater detail in chapter 5.\n1.6.4\nServing layer\nThe goal of the serving layer is to prepare data for consumption by end users, be they\npeople or other systems. The increasing demands from a variety of users in most\norganizations who need faster access to more data is a huge IT challenge in that these\nIngest\nStorage\nProcessing\nServing\nCloud data\nwarehouse\nAPIs\nData export\nStream\nExploration, data science experiments\nIn streaming mode, data ingestion\nbypasses storage, and data is sent\ndirectly to the processing layer. \nIn batch mode, data is\nsaved to storage first,\nthen processed.\nFigure 1.5\nProcessing differs between batch and streaming data.\n\n\n14\nCHAPTER 1\nIntroducing the data platform\nusers often have different (or even no) technology backgrounds. They also typically\nhave different preferences as to which tools they want to use to access and analyze data.\n Business users often want access to reports and dashboards with rich self-service\ncapabilities. The popularity of this use case is such that when we talk about data plat-\nforms, we almost always design them to include a data warehouse. \n Power users and analysts want to run ad hoc SQL queries and get responses in\nseconds. Data scientists and developers want to use the programming languages\nthey’re most comfortable with to prototype new data transformations or build\nmachine learning models and share the results with other team members. Ultimately,\nyou’ll typically have to use different, specialized technologies for different access tasks.\nBut the good news is that the cloud makes it easy for them to coexist in a single\narchitecture. For example, for fast SQL access, you can load data from the lake into a\ncloud data warehouse. \n To provide data lake access to other applications, you can load data from the lake\ninto a fast key/value or document store and point the application to that. And for\ndata science and engineering teams, a cloud data lake provides an environment where\nthey can work with the data directly in cloud storage by using a processing framework\nsuch as Spark, Beam, or Flink. Some cloud vendors also support managed notebook\nenvironments such as Jupyter Notebook or Apache Zeppelin. Teams can use these\nnotebooks to build a collaborative environment where they can share the results of\ntheir experiments along with performing code reviews and other activities.\n The main benefit of the cloud, in this case, is that several of these technologies are\noffered as platform as a service (PaaS), which shifts the operations and support of these\nfunctions to the cloud provider. Many of these services are also offered through a pay-\nas-you-go pricing model, making them more accessible for organizations of any size.\n1.7\nHow the cloud data platform deals with the three V’s\nThe following sections explain how variety, volume, and velocity work with cloud\nplatforms.\n1.7.1\nVariety\nA cloud data platform is well positioned to adapt to all this data variety because of its\nlayered design. The data platform’s ingestion layer can be implemented as a collec-\ntion of tools, each dealing with a specific source system or data type. Or it can be\nimplemented as a single ingestion application with a plug-and-play design that allows\nyou to add and remove support for different source systems as required. For example,\nKafka Connect and Apache NiFi are examples of plug-and-play ingestion layers that\nadapt to different data types. At the storage layer, cloud storage can accept data in any\nformat because it’s a generic file system—meaning you can store JSON, CSV, video,\naudit data, or any other data type. There are no data type limits associated with cloud\nstorage, which means you can introduce new types of data easily.\n Finally, using a modern data processing framework such as Apache Spark or Beam\nmeans you’re no longer confined by the limitations of the SQL programming language.\n\n\n15\nHow the cloud data platform deals with the three V’s\nUnlike SQL, in Spark you can easily use existing libraries for parsing and processing\npopular file formats or implement a parser yourself if there’s no support for it today.\n1.7.2\nVolume\nThe cloud provides tools that can store, process, and analyze lots of data without a\nlarge, upfront investment in hardware, software, and support. The separation of stor-\nage and compute and pay-as-you-use pricing in the cloud data platform makes han-\ndling large data volumes in the cloud easier and less expensive. Cloud storage is\nelastic, the amount of storage grows and shrinks as you need it, and the many tiers of\npricing for different types of storage (both hot and cold) means you pay only for what\nyou need in terms of both capacity and accessibility.\n On the compute side, processing large volumes of data is also best done in the\ncloud and outside the data warehouse. You’ll likely need a lot of compute capacity to\nclean and validate all this data, and it’s unlikely you’ll be running these jobs continu-\nously, so you can take advantage of the elasticity of the cloud to provision a required\ncluster on demand and destroy it after processing is complete. By running these jobs\nin the data platform but outside the data warehouse, you also won’t negatively impact\nthe performance of the data warehouse for users, and you might also save a substan-\ntial amount of money because the processing will use data from less expensive storage.\n While cloud storage is almost always the least expensive way to store raw data, pro-\ncessed data in a data warehouse is the de facto standard for business users, and the\nsame elasticity applies to cloud data warehouses offered by Google, AWS, and Micro-\nsoft. Cloud data warehouse services such as Google BigQuery, AWS Redshift, and\nAzure Synapse provide either an easy way to scale warehouse capacity up and down on\ndemand, or, like Google BigQuery, introduce the concept of paying only for the\nresources a particular query has consumed. With cloud data lakes, processing large\nvolumes of data is available to budgets of almost any size. These cloud data ware-\nhouses couple on-demand scaling with an almost endless array of pricing options that\ncan fit any budget.\n1.7.3\nVelocity\nThink about running a predictive model to recommend a next best offer (NBO) to a\nuser on your website. A cloud data lake allows the incorporation of streaming data\ningestion and analytics alongside more traditional business intelligence needs such as\ndashboards and reporting. Most modern data processing frameworks have robust sup-\nport for real-time processing, allowing you to bypass the relatively slow cloud storage\nlayer and have your ingestion layer send streaming data directly to the processing layer.\n With elastic cloud compute resources, there’s no longer any need to share real-time\nworkloads with your batch workloads—you can have dedicated processing clusters for\neach use case, or even for different jobs, if needed. The processing layer can then send\ndata to different destinations: to a fast key/value store to be consumed by an\napplication, to cloud storage for archiving purposes, or to a cloud warehouse for\nreporting and ad hoc analytics.\n\n\n16\nCHAPTER 1\nIntroducing the data platform\n When data scientists become users of your data systems, volume and variety chal-\nlenges come into play all at once. Machine learning models love data—lots and lots of\nit (i.e., volume). Models developed by data scientists usually require access not just to\nthe organized, curated data in the data warehouse, but also to the raw, source-file data\nof all types that’s typically not brought into the data warehouse (i.e., variety). Their\nmodels are compute intensive, and when run against data in a data warehouse, put\nenormous performance pressure on the system. With current data warehouse archi-\ntectures, these models often take hours or even days to run. They also impact ware-\nhouse performance for all other users while they’re running. Giving data scientists\naccess to the high-volume, high-variety data in the data lake makes everyone happier\nand will likely keep costs lower.\n1.7.4\nTwo more V’s \nVeracity and value are two other V’s that should factor into your choice of a data plat-\nform over just a data warehouse. Turning data into value only happens when your data\nusers, be they people, models, or other systems, get timely access to data and use it\neffectively.\n The beauty of a data lake is that you can now give people access to more data. The\ndownside, though, is that you’re also providing access to data that’s not necessarily as\nclean and organized and well governed as it tends to be in a data warehouse. The\nveracity or correctness of the data is a major consideration of any big data project, and\nwhile data governance is a topic big enough for a book of its own, many big data proj-\nects balance the need for data governance (ensuring veracity) with the need for access\nto more data to drive value. This can be accomplished by using the data platform not\njust as a source of raw data to produce governed data sets for the data warehouse, but\nas an ungoverned or lightly governed data repository where users can explore the\nentirety of their data, knowing that it hasn’t yet been blessed for corporate reporting.\nWe increasingly see data governance as an iterative, more Agile process when data\nlakes are involved—where once the exploratory phase is complete and models appear\nto produce a good output, the data moves into the data warehouse, where it becomes\npart of a governed dataset.\n1.8\nCommon use cases\nUnderstanding the various use cases for data platforms is important when you design\nand plan your own data platform. Without this context, you risk winding up with a\ndata swamp that doesn’t actually deliver real business value.\n One of the most common data platform use cases is driven by the need for a 360-\ndegree view of an organization’s customers. Customers engage with or talk about\norganizations in many ways using many different systems, from social media to\ne-commerce to online chats to call center conversations and more. The data from these\nengagements is both structured and unstructured, originates from many different\nsources, and is of varying degrees of quality, volume, and velocity. Integrating all these\n\n\n17\nCommon use cases\ntouchpoints into a single view of the customer opens the doors to a plethora of improved\nbusiness outcomes—an improved customer experience as they interact with different\nparts of the business, better personalization in marketing, dynamic pricing, reduced\nchurn, improved cross-selling, and much more.\n A second common use case for a data lake is IoT, where data from machines and\nsensors can be combined to create operational insights and efficiencies ranging from\nproactively predicting equipment failures on a factory floor or in the field, to monitor-\ning skier (a person who moves over snow on skis) performance and location via RFID\ntags. The data from sensors tends to be very high volume and with a high degree of\nuncertainty, which makes it well suited for a data lake. Traditional data warehouses\ndon’t just struggle with this data; the sheer amount of data produced in an IoT use case\nmakes a traditional data warehouse-based project extremely expensive and shifts the\nbalance away from a good return on that investment for all but a small number of cases.\n The emergence of advanced analytics using machine learning and AI has also\ndriven the adoption of data lakes, because these techniques require the processing of\nlarge data sets—often much larger than can be cost-effectively stored or processed in a\ndata warehouse. In this regard, the data lake, with its ability to cost-effectively store\nalmost unlimited amounts of raw data, is a data scientist’s dream come true. And the\nability to process that data without impacting performance for other analytic consum-\ners is another big benefit. \nSummary\nPressure from the business to get more accurate insights faster, cheaper, and\nwith an increasing level of confidence is growing along with the volume, veloc-\nity, and variety of data needed to produce these insights. All of this is putting\nimmense pressure on traditional data warehouses and paving the way for the\nemergence of a new solution.\nTraditional data warehouses or data lakes alone can’t meet today’s rapidly\nchanging data requirements, but when combined with new cloud services and\nprocessing frameworks available only in the cloud, they create a powerful and\nflexible analytics data platform that addresses a wide range of use cases and data\nconsumers.\nData platform design revolves around concepts of flexibility and cost effectiveness.\nPublic cloud, with its on-demand storage, compute resources provisioning, and\npay-per-usage pricing model, fits the data platform design perfectly. \nWell-designed data platforms use a loosely coupled architecture where each\nlayer is responsible for a specific function. The foundational building blocks of\na data platform are individual layers designed for ingestion, storage, processing,\nand serving. \nKey use cases for data platforms include a 360-degree view of an organization’s\ncustomers, IoT, and machine learning.\n\n\n18\nWhy a data platform\n and not just a data\n warehouse\nWe’ve covered what a data platform is (as a reminder, it is “a cloud-native platform\ncapable of cost-effectively ingesting, integrating, transforming, and managing an\nalmost unlimited amount of data of any type data in order to facilitate analytics\noutcomes”), what drives the need for a data platform, and how changes in data will\nshape your data platform. Now we will explore in more detail why a cloud data\nThis chapter covers\nAnswering “Why a data platform?” and “Why build in \nthe cloud?”\nComparing data platform to data warehouse–only \nsolutions\nProcessing differences in structured and semi-\nstructured data\nComparing cloud costs for data warehouse and data \nplatform\n\n\n19\nCloud data platforms and cloud data warehouses: The practical aspects\nplatform provides more capabilities as opposed to a data warehouse–only architecture.\nIn this chapter, we will equip you with the knowledge necessary to make a solid argument\nfor a data platform and will walk you through several examples demonstrating the\ndifference between the two approaches (data warehouse–only and data platform). \n While designing the best data platform is what we want you to be able to do when\nyou’ve finished this book, we also know from experience that knowing the “why”\nbehind your data platform project will not only help you make better decisions along\nthe way, but you’ll also be ready to justify why it makes sense from a business perspec-\ntive to embark on a cloud data platform project. This chapter will equip you with solid\nbusiness and technical reasons for choosing a data platform so you’ll be ready for that\nmoment when someone asks, “Why are you doing this?”\n We’ll use a simple but common analytics use case to demonstrate how a data ware-\nhouse solution compares to a data platform example, introducing some of the key dif-\nferences between the two options. \n We will start by describing two potential architectures: one that is centered around a\ncloud data warehouse only and another one that uses broader design principles to define\na data platform. Then we will walk through examples showing how to load and work with\nthe data in both solutions. We will specifically focus on what happens to the data platform\npipelines when there are changes to source data structure and look at how data platform\narchitecture can help you analyze semistructured data at scale. Because similar outcomes\ncan be achieved by directly ingesting data into the cloud data warehouse, we’ll also walk\nthrough loading and working with the same data in a data warehouse alone.\n We’ll also explore the main difference between delivering and analyzing data in\nthe traditional warehouse versus a data platform environment. You will see how each\nsolution deals with changes to the source schema and how they work with large vol-\numes of semistructured data, such as JSON. We will also compare the cost and perfor-\nmance characteristics of each.\n By the end of this chapter, you’ll understand how a data platform compares to\nmeeting the same business goals with a data warehouse.\n2.1\nCloud data platforms and cloud data warehouses: \nThe practical aspects\nIn this section, we’ll use an example cloud analytics challenge to illustrate the differ-\nences between data platforms and data warehouse architectures. We’ll also introduce\nAzure as the cloud platform for this example and describe an Azure cloud data ware-\nhouse and an Azure data platform architecture.\n Imagine that we’ve been tasked with building a small reporting solution for our\norganization. The Marketing department in our organization has data from their\nemail marketing campaigns that is stored in a relational database—let’s assume it’s a\nMySQL RDBMS for this scenario. They also have clickstream data that captures all\nwebsite user activity that is then stored in a CSV file that is available to us via an inter-\nnal SFTP server. \n\n\n20\nCHAPTER 2\nWhy a data platform and not just a data warehouse\n Our main analytical goal is to combine campaign data with the clickstream data to\nidentify users who landed on our website using links from specific email marketing\ncampaigns and the parts of the site they visited. In our example, Marketing wants this\nanalysis done repeatedly, so our data pipelines must bring data into our cloud envi-\nronment on a regular basis and must be resilient to changes in the upstream source.\nWe would also like our solution to be performant and cost efficient. As usual, the\ndeadline for this is “yesterday.”\n To illustrate the differences between a data platform approach to dealing with the\nfirst three V’s of data (volume, variety, and velocity) versus a more traditional ware-\nhouse approach, let’s consider two simplified implementations: (1) a data platform\nwith a data warehouse and (2) a traditional data warehouse. We will use Azure as our\ncloud platform of choice for these examples. We could equally have used similar\nexamples on AWS or Google Cloud Platform, but Azure allows us to easily emulate a\ntraditional warehouse with Azure Synapse. Azure Synapse is a fully managed and scal-\nable warehousing solution from Microsoft, based on a very popular MS SQL Server\ndatabase engine. This way, one of our example architectures will be very close to what\nyou might see in an on-premises data warehouse setup.\n2.1.1\nA closer look at the data sources\nOur simplified email marketing campaign data consists of a single table, as shown in\nfigure 2.1. The table includes the unique identifier of the campaign (campaign_id), a\nlist of target email addresses the campaign was sent to (email), a unique code\nincluded in a link to our website for each specific user (unique_code), and a date\nwhen the campaign was sent (send_date). Real marketing automation systems are, of\ncourse, more complex and include many different tables, but this is good enough for\nour purpose.\nClickstream data with its lack of fixed schema is semistructured data derived from web\napplication logs and includes details about the pages visited, information about the\nvisitor’s browser and operating system, session identifiers, etc. \nNOTE\nIn general terms, semistructured data is any data that doesn’t fit nicely\ninto a relational model. This means that semistructured data cannot be repre-\nsented as a flat table with columns and rows, where each cell contains a value\nof a certain type: integers, dates, strings, etc. JSON documents are a common\nexample of semistructured data.\ncampaign_id\nemail\nunique_code\nsend_date\ncampaign_1\nuser1@example.com\n12342\n2019-08-02\ncampaign_2\nuser2@example.com\n34563\n2019-03-05\nFigure 2.1\nExample marketing campaign table\n\n\n21\nCloud data platforms and cloud data warehouses: The practical aspects\nThese logs will look different depending on the application that was used to generate\nthem. For our example, we will use a simplified representation of the clickstream log,\nshown in figure 2.2, that only includes the details we need for our use case. \n For our scenario, we will assume that clickstream log is a large (hundreds of GBs) text\nfile in CSV format that includes three columns: a UNIX timestamp of when the event\nhappened (timestamp); a content column containing the details about the page URL,\nunique visitor identifier, and browser info (content_json); and other details (other_\ndetails). One thing that you will notice about this example is that the content_json\ncolumn in our hypothetical CSV file is a JSON document with many nested fields. This\nis a common layout for this type of data and will require extra steps to process. \n Our task is illustrated in figure 2.3. It is to design a cloud data platform capable of\nintegrating these two data sources in a performant and cost-efficient manner and to\nmake this integrated data available to the Marketing team for analysis.\n Our goal here is not only to describe two different cloud architectures, but to high-\nlight the important differences between the two, focusing on what happens to the data\ntimestamp\ncontent_json\nother_details\n1565395200 {\nurl:   \n\"https://example.com/campaigns/landing?co\nde=12342\",\nuser_id: 254,\nbrowser_info: {…}\n}\n...\n1565396381 {\nurl:   \"https://example.com/products\",\nuser_id: 254,\nbrowser_info: {…}\n}\n...\nFigure 2.2\nExample clickstream data\nFigure 2.3\nCloud analytics platform problem statement\nClickstream logs\n(CSV, 100s GBs)\nCloud environment\nMySQL\n?\nWe need a scalable, cost-efficient\nsolution that will allow us to\nperform SQL analytics on the\ncombined data.\nUsers\nreports, dashboards\n\n\n22\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nplatform pipelines when there are changes to source data structure and how the two\narchitectures analyze semistructured data at scale. In the next section, we will walk you\nthrough the solution to our analytics problem (design a cloud data platform capable of\nintegrating these two data sources in a performant and cost-efficient manner and make\nthis integrated data available to the marketing team for analysis) using a cloud data\nwarehouse architecture and then a cloud data platform architecture. \n2.1.2\nAn example cloud data warehouse–only architecture\nA cloud data warehouse architecture is quite similar to a traditional enterprise data\nwarehouse solution. Figure 2.4 shows how the center of this architecture is a relational\ndata warehouse that is responsible for storing, processing, and serving data to the end\nusers. There is also an extract, transform, load (ETL) process that loads the data from\nthe sources (clickstream data via CSV files and email campaign data from a MySQL\ndatabase) into the warehouse. \nOur example cloud data warehouse–only architecture consists of two PaaS services run-\nning on Azure: Azure Data Factory and Azure SQL Data Warehouse (Azure Synapse).\nNOTE\nIn all of our examples we will use platform-as-a-service (PaaS) offerings\nwhere possible. PaaS enables one of the most powerful promises of the\ncloud—getting platforms up and running in minutes, not days.\nAzure Data Factory is a fully managed PaaS ETL service that allows you to create pipe-\nlines by connecting to various data sources, ingesting data, performing basic transfor-\nmations such as uncompressing files or changing file formats, and loading data into a\ntarget system for processing and serving. In our example cloud data warehouse–only\narchitecture, we will use Data Factory to read email campaign data from a MySQL\nFigure 2.4\nExample cloud data warehouse-only architecture on Azure.\nClickstream logs\n(CSV, 100s GBs)\non SFTP server\nAzure Data\nFactory service\nAzure Data\nFactory service\nAzure Synapse\nMarketing\ncampaigns\ndata\nData Factory\nMySQL\nconnection\nData Factory\nSFTP\nconnection\nData Factory\nAzure Synapse\nsink\nData storage and\nprocessing\nIngest\nStore|Process|Serve\nMySQL\nAzure\n",
      "page_number": 24
    },
    {
      "number": 2,
      "title": "Why a data platform and not just a data warehouse",
      "start_page": 42,
      "end_page": 59,
      "detection_method": "regex_chapter",
      "content": "23\nCloud data platforms and cloud data warehouses: The practical aspects\ntable as well as to fetch files containing clickstream data from an SFTP server. We will\nalso use Data Factory to load data into Azure Synapse.\n Our example warehouse, Azure Synapse, is a fully managed warehouse service\nbased on MS SQL Server technology. Fully managed in this case means that you don’t\nneed to install, configure, and manage the database server yourself. Instead, you need\nonly choose how much computational and storage capacity you require, and Azure will\ntake care of the rest. While there are certain limitations to fully managed PaaS offerings\nsuch as Azure Synapse and Azure Data Factory, they make it very easy for people who\nare not MS SQL Server experts to implement architectures such as our cloud data\nwarehouse–only architecture and to program relatively complex pipelines quickly. \n In the next section, we will describe an alternative architecture—a cloud data plat-\nform design that provides more flexibility than a data warehouse design. \n2.1.3\nAn example cloud data platform architecture\nA cloud data platform architecture is inspired by the concept of a data lake and is in\nfact a combination of a data lake and a data warehouse created for the age of cloud. A\ncloud data platform consists of several layers, each responsible for a particular aspect\nof the data pipeline: ingestion, storage, processing, and serving. Let’s look at our\nexample cloud data platform architecture, which can be seen in figure 2.5.\nOur cloud data platform architecture consists of these Azure PaaS services:\nAzure Data Factory\nAzure Blob Storage\nAzure Synapse\nAzure Databricks\nFigure 2.5\nExample cloud data platform architecture\nClickstream logs\n(CSV, 100s\nGBs) on SFTP\nserver\nAzure Data\nFactory service\nAzure Data\nFactory service\nIngest\nAzure Blob\nStorage\nStore\nAzure\nDatabricks\nProcess\nAzure \nSynapse\nServe\nMarketing\ncampaigns\ndata\nData Factory\nMySQL\nconnection\nData Factory\nSFTP\nconnection\nData Factory\nBlob Storage\nsink\nRead/write\nWrite\nMySQL\nAzure\n\n\n24\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nWhile they may look similar (both use Azure Data Factory for ingestion and both use\nAzure Synapse for serving), there are several key differences between a cloud data\nwarehouse–only architecture and cloud data platform architecture. In the cloud data\nplatform, while we are using Azure Data Factory to connect and extract data from the\nsource systems instead of loading it directly into the warehouse, we will save the source\ndata into a landing area on Azure Blob storage (often known as “the lake”). This\nallows us to preserve the original data format and helps with data variety challenges as\nwell as provides other benefits.\n Once the data has landed in Azure Blob Storage, we’ll use Apache Spark running\non the Azure Databricks managed service (PaaS) to process it. As with all PaaS ser-\nvices, we get simple setup and ongoing management, allowing us to create new Spark\nclusters without needing to manually install and configure any software. It also pro-\nvides an easy-to-use notebook environment where you can execute Spark commands\nagainst the data in the lake and see the results right away, without having to compile\nand submit Spark programs to the cluster.\n While Spark and other distributed data processing frameworks can help you pro-\ncess various data formats and almost infinitely large data volumes, these tools aren’t\nwell suited for serving what is sometimes called interactive queries. Interactive in this\ncase means that the query response is usually expected within seconds or less, not\nminutes. For these use cases, a well-designed relational warehouse can typically pro-\nvide a faster query performance than can Spark. Also there are many off-the-shelf\nreporting and BI tools that integrate much better with an RDBMS database than with\na distributed system such as Spark and are easier to use for less technical users.\n2.2\nIngesting data\nThis section covers how to load data into Azure Synapse and an Azure data platform\nusing Azure Data Factory. We’ll also look at what happens to ingestion pipelines where\nthe source schema changes.\n Using a managed service like Azure Data Factory makes creating a pipeline to get\ndata into the data platform or data warehouse a relatively easy task. Data Factory\nExercise 2.1\nIn our examples in this section, which of the following is the main difference between\na cloud data warehouse architecture and a data platform architecture?\n1\nData platform uses only serverless technologies.\n2\nData platform uses Azure functions for data ingestion.\n3\nData warehouses can connect to the data sources directly to perform the\ningestion.\n4\nData platform adds a “data lake” layer to offload data processing from the\ndata warehouse.\n\n\n25\nIngesting data\ncomes with a set of built-in connectors to various sources, allows for basic data trans-\nformations, and supports saving data to the most popular destinations. \n There are, however, fundamental differences between how a data ingestion pipe-\nline works in a cloud data platform versus a cloud data warehouse–only implementa-\ntion. In this section, we will highlight these differences. \n2.2.1\nIngesting data directly into Azure Synapse\nAn Azure Data Factory pipeline consists of several key components: (1) linked ser-\nvices, (2) input and output data sets, and (3) activities. Figure 2.6 shows how these\ncomponents work together to load data from a MySQL table into Azure Synapse.\nA linked service describes a connection to a specific data source (in this case, MySQL)\nor data sink (in this case, Azure Synapse). These services would include the location of\nthe data source, credentials that will be used to connect to it, etc. A data set is a spe-\ncific object in the connected service. It could be a table in the MySQL database that\nwe want to read or a destination table in Azure Synapse. An important property of a\ndata set is a schema. For Data Factory to be able to load data from MySQL into Azure\nSynapse, it needs to know the schema of the source and destination tables. This infor-\nmation is required up front, meaning the schema must be available to the pipeline\nbefore the pipeline can be executed. To be more specific, the schema for the input\ndata source can be inferred by the Data Factory automatically, but the output schema,\nand especially the mapping of the input to output schemas, must be provided. The\nData Factory UI provides a quick way to fetch schema from data sources, but if you are\nbuilding pipeline automation using the Data Factory API to construct pipelines, then\nyou need to provide the schema yourself. In the example in figure 2.6, the MySQL\nMySQL linked \nservice\nCampaigns\ninput data set\nCampaigns \noutput data \nset\nAzure SQL data\nwarehouse\nlinked service\nCopy\nactivity\nInput data set schema\ncampaign_id: varchar\nemail: varchar\nunique_code: integer\nsend_data: date\nOutput data set schema\n1. Data Factory will connect to\n    a MySQL source, fetch table\n    schemas, and create an input\n    data set.\n2. Data Factory will convert an input\n    data set into a corresponding output\n    data set based on a data sink type.\n3. Data Factory\n    creates a new table\n    in Azure SQL DW\n    based on the output\n    data set schema.\ncampaign_id: varchar\nemail: varchar\nunique_code: integer\nsend_data: date\nFigure 2.6\nAzure Data Factory ingestion pipeline for Azure Synapse\n\n\n26\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nsource schema and the Azure Synapse schema are similar, but in other cases this may\nnot be true because of data types mismatch, etc. \n2.2.2\nIngesting data into an Azure data platform\nOur data platform architecture takes a different approach to data ingestion (figure\n2.7). In a data platform architecture, the primary destination for the data is Azure Blob\nstorage, which, for this use case, can be thought of as an infinitely scalable filesystem. \nThe main difference between this ingestion pipeline and the previous one is that\nAzure Blob Storage Data Factory service doesn’t require a schema to be specified up\nfront. In our use case, each ingestion from MySQL is saved as a text file in Azure Blob\nStorage without concern for the source columns and data types. Our cloud data plat-\nform design has an extra layer of processing data, which will be implemented using\nApache Spark running on the Azure Databricks platform to convert source text files\ninto more efficient binary formats. This way we can combine the flexibility of text files\non Azure Blob Storage with the efficiency of binary formats.\n In the data platform design, the fact that you are no longer required to manually\nprovide output schema and its mapping to the source schema is important for two rea-\nsons: (1) a typical relational database can contain hundreds of tables, which translates\ninto a lot of manual effort and increased chance of errors, and (2) it is highly resilient\nto change, the subject of the next section. \n2.2.3\nManaging changes in upstream data sources\nSource datasets are never static. Developers who support our marketing campaign\nmanagement software will be constantly adding new features. This can result in new\ncolumns added to the source table and/or columns being renamed or deleted. Build-\ning data ingestion and processing pipelines that deal with these types of changes is\none of the most important tasks for a data architect or a data engineer. \nMySQL linked\nservice\nCampaigns\ninput data set\nCampaigns\noutput data\nset\nAzure Blob\nStorage linked\nservice\nCopy\nactivity\ncampaign_id: varchar\nemail: varchar\nunique_code: integer\nsend_data: date\nInput data set schema\n2. Data Factory will convert an input\n    data set into a corresponding\n    output data set based on a data\n    sink type.\n3. Data Factory creates a new text\n    file on Azure Blob Storage. No\n    schema is required for this\n    operation.\n1. Data Factory will connect to a\n    MySQL source, fetch table\n    schemas, and create an input\n    data set.\nFigure 2.7\nAzure Data Factory ingestion pipeline for a cloud data platform\n\n\n27\nIngesting data\nLet’s imagine that a new version of our email marketing software introduced a new\ncolumn called “country” in the source data set. What will happen to our ingestion\npipeline in our data warehouse–only architecture? Figure 2.8 explains.\n Our data warehouse–only Data Factory pipeline requires both input and output\nschemas, so, without intervention, a change in the source schema means our two sche-\nmas will be out of sync. Data Factory maps input to output columns by position\nbecause it supports column renaming in the output. This means the next time the\ningestion pipeline runs, inserts into Azure Synapse will fail, because it will expect an\ninteger unique_code column where a varchar region column will arrive from source.\nAn operator will need to go and adjust the output data set schema manually and\nrestart the pipeline. We will discuss schema management in great detail in chapter 8. \nNOTE\nData Factory, like any generic ETL overlay tool, will allow you to copy\ndata from a variety of sources into a variety of destinations. Some of these des-\ntinations, such as Azure Blob Storage, don’t care about input schema, but\nothers, such as databases and Azure Synapse, require a strict schema to be\ndefined up front. While you can alter the destination schema and add new\ncolumns, the behavior of this operation will depend on the type of destina-\ntion. Some databases will lock the full table for the duration of the schema\nchange, making it completely unavailable to end users. In other cases, space\ndepending on data size in the table, a schema adjustment operation can take\nhours to run. There is no single way to deal with schema changes in the multi-\ntude of data destinations in a unified way, so Data Factory and other ETL\ntools delegate this responsibility to the platform operators. \nResilience to the upstream schema changes is one of the benefits of the Data Platform\narchitecture over a data warehouse–only approach. As shown in figure 2.9, in a data\nMySQL linked\nservice\nCampaigns\ninput data set\nCampaigns\noutput data\nset\nAzure SQL\nDW linked\nservice\nCopy\nactivity\ncampaign_id: varchar\nemail: varchar\nregion: varchar\nunique_code: integer\nsend_data: date\nInput data set schema\ncampaign_id: varchar\nemail: varchar\nunique_code: integer\nsend_data: date\nOutput data set schema\nA new column was added to the source MySQL\ntable. Input data set schema is adjusted\nautomatically, but output data set is not.\nFigure 2.8\nUpstream schema changes break our data warehouse ingestion pipeline.\n\n\n28\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nplatform implementation, if the source schema changes, since the output destination\nis a file on Azure Blob Storage, the output schema is not required, and the ingestion\npipeline will simply create a new file with a new schema and continue working. \n While a data platform ingestion pipeline will continue to work as expected if the\nupstream schema changes, there are other problems that come with source schema\nchanges. At some point along the way, a consumer of the data, either an analytics user\nor a scheduled job, will need to work with the changed data, and to work with it they\nwill need to know that a new column was added. We will look into approaches on deal-\ning with schema changes in the data platform in chapter 8.\n2.3\nProcessing data\nNow that we’ve covered some of the key ideas involved with ingestion, let’s consider\nprocessing—specifically, how processing data to answer our analytics problem differs\nwhen using SQL running on a data warehouse versus using Apache Spark on a cloud\ndata platform. We’ll discuss the pros and cons of both approaches.\nMySQL linked\nservice\nCampaigns\ninput data set\nCampaigns\noutput data\nset\nAzure Blob\nStorage linked\nservice\nCopy\nactivity\ncampaign_id: varchar\nemail: varchar\nregion: varchar\nunique_code: integer\nsend_data: date\nInput data set schema (inferred)\nNo output schema means the\npipeline will continue to work\nas expected if the upstream\nschema changes.\nFigure 2.9\nData platform ingestion pipeline is resilient to upstream schema changes.\nExercise 2.2\nOur example data platform architecture is more resilient to the changes in the\nupstream data sources because (choose one)\n1\nIt uses Apache Spark for data processing, which utilizes Resilient Distributed\nDatasets (RDDs).\n2\nIt saves data into Azure Blob Storage first, which doesn’t require strict schema\ndefinition.\n3\nIt uses Azure Blob Storage as a primary data store, which provides extra\nredundancy.\n4\nIt uses Azure Functions for data ingestion, which provides extra flexibility.\n\n\n29\nProcessing data\n We saw in the previous section that ingesting data into the data warehouse and\ndata platform require different approaches. The differences don’t stop there. In this\nsection, we will explore how processing data to answer analytical questions is different\nin the two systems. \n Let’s recall the two data sources we are using as an example use case. First, we have\nthe marketing campaign table in figure 2.10, with the following columns.\nWe also have clickstream data from our website that comes to us in the following semi-\nstructured format (figure 2.11).\nWhile we have data split into individual columns, we also have a content_json column\nthat contains a complex JSON document with multiple attributes and nested values. \n To demonstrate how the approach to working with such data in the data ware-\nhouse and data platform environments differs, let’s consider the following request for\ninformation from our Marketing team: When users land on our website from cam-\npaign X, what other pages do they visit? Our example data set may be artificial, but\nthis type of request is a common one. \n2.3.1\nProcessing data in the warehouse\nSince we are using Azure Synapse as the destination for our data source in the ware-\nhouse-only design, we will need to work with two relational tables that we have loaded\nusing our ingestion process—one for clickstream data and one for email marketing\ncampaign data. Let’s assume tables in our Azure Synapse are called campaigns and\nclicks for campaigns information and clickstream data, correspondingly. A campaigns\ncampaign_id\nemail\nunique_code\nsend_date\ncampaign_1\nuser1@example.com\n12342\n2019-08-02\ncampaign_2\nuser2@example.com\n34563\n2019-03-05\nFigure 2.10\nExample marketing campaigns table\ntimestamp\ncontent_json\nother_details\n1565395200 {\nurl:   \n\"https://example.com/campaigns/landing?co\nde=12342\",\nuser_id: 254,\nbrowser_info: {…}\n}\n...\n1565396381 {\nurl:   \"https://example.com/products\",\nuser_id: 254,\nbrowser_info: {…}\n}\n...\nFigure 2.11\nExample clickstream data\n\n\n30\nCHAPTER 2\nWhy a data platform and not just a data warehouse\ntable is a straightforward mapping of our source table to the destination table since\nboth data sources are relational in nature. Azure Synapse will contain the same col-\numns. What about clickstream data and its nested JSON documents? There is cur-\nrently no dedicated data types in Azure Synapse to represent JSON (note that some\ncloud data warehouses such as Google BigQuery have native support for JSON data\nstructures), but you can store JSON documents in standard text columns and use a\nbuilt-in JSON_VALUE function to access specific attributes inside the document. \n The following listing is an example of an Azure Synapse query that can answer our\nMarketing request.\nSELECT\n  DISTINCT SUBSTRING(\n    JSON_VALUE(CL.content_json, '$.url'),\n    1,\n    CHARINDEX('?', JSON_VALUE(CL.content_json, '$.url'))\n  ) as landing_url, \n  SUBSTRING(\n    JSON_VALUE(CL1.content_json, '$.url'),\n    1,\n    CHARINDEX('?', JSON_VALUE(CL1.content_json, '$.url'))\n  ) as follow_url\nFROM\n  clicks CL\n  JOIN campaigns CM ON CM.unique_code = SUBSTRING(\n    JSON_VALUE(CL.content_json, '$.url'),\n    CHARINDEX(\n      'code =',\n      JSON_VALUE(CL.content_json, '$.url') + 5),\n      LEN(JSON_VALUE(CL.content_json, '$.url')\n    )\n  )    \n  LEFT JOIN (\n    SELECT\n      JSON_VALUE(CL_INNER.content_json, '$.user_id') as user_id    \n    FROM\n      clicks CL_INNER\n  ) AS CL1 ON JSON_VALUE(CL.content_json, '$.user_id') = CL1.user_id\nWHERE\n  CM.campaign_id = 'campaign_1'\nThis query joins two tables in the data warehouse using a unique_code column from\nthe campaigns table and a portion of the URL from the clicks table that contains this\nunique code for our landing page. Notice that we need to use a complex string pars-\ning construct in the first join to extract the campaign code from the url attribute of\nthe JSON document. Then we join a subquery that will allow us to find all other pages\nthat the same user has visited using a user_id porting of the url. Again, we need to use\ncomplex string parsing to extract values we want.\nListing 2.1\nAzure Synapse query\nExtracts a URL portion only from a \ncontent_json.url attribute by finding the \n“?” character in the URL and extracting \nall preceding characters\nExtracts campaign code from content_json.url \nattribute and uses it in a JOIN\nExtracts a user_id\nattribute from the\ncontent_json JSON\ndocument\n\n\n31\nProcessing data\n If you have been working with relational technologies for some time, this query\nmost likely is not the most complex SQL you have seen. What makes it hard to read\nand understand is all the extra logic that is needed to parse and extract value from a\nJSON document. There are ways to make this query simpler—by preparsing the\nincoming data as a part of the ETL pipeline or by implementing custom user-defined\nfunctions (UDFs) to make the query more readable. Both approaches have their lim-\nitations. Preparsing may introduce significant ETL processing times and, subse-\nquently, costs, if your data volumes are large—and clickstream data tends to get very\nbig over time. Implementing custom user-defined functions requires you to have a\nseparate development process to maintain and deploy UDFs to each instance of Azure\nSynapse that you need to work with. \n Besides being not very readable, this SQL also suffers from an issue that is com-\nmon to any SQL-based pipeline—the difficulty of testing it. Our string-parsing logic is\ncomplicated and depends on certain characters in the URL to be in specific places so\nwe can extract parameters like unique_code or user_id. It’s very easy to make a mis-\ntake in one of those expressions or to run into a number of edge cases that will break\nthe logic. Lack of tests means that you will end up relying on your users to find these\nissues for you. Not the best way to build trust with your user community. \n Another challenge with running this type of SQL on Azure Synapse is that you\ncan’t really use a number of performance optimizations that make Azure Synapse a\ngreat warehouse. Azure Synapse uses a columnar storage that allows columns that usu-\nally contain numbers or short text to be compressed and require less reads from disk\nwhile running a query. With JSON values you lose this optimization because you never\nwork with a document as a whole, but rather need to parse it and access individual\nattributes. This negatively impacts query performance, especially as data size grows.\nYou can read more about various cloud data warehouse features in chapter 9. \n2.3.2\nProcessing data in the data platform\nCloud offers multiple ways to process data at any scale outside of the data warehouse\nusing a number of distributed data processing engines. Apache Spark is one of the\nmost popular and widely adopted distributed data processing engines. All cloud ven-\ndors offer some sort of a managed service to run Spark jobs without having to worry\nabout cluster deployment and configuration. Azure offers a managed environment\nfor Spark that is based on Databricks (https://databricks.com/)—a commercial offer-\ning from the team that created Apache Spark.\n In our example data platform implementation, Spark running on an Azure Data-\nbricks platform is responsible for all the data processing. Using Apache Spark gives\nyou a choice of either using SQL to process your data or using a general purpose lan-\nguage such as Python or Scala to write more flexible, readable, and testable programs.\nThis way you can pick the best API that fits your needs. You can use SQL for simple\nreports and ad hoc analytics since it’s quick to write and experiment with. When it\ncomes to code that you are planning to maintain long term or that must be modular\nand testable, then you can use a Python (see listing 2.2) or Scala API. \n\n\n32\nCHAPTER 2\nWhy a data platform and not just a data warehouse\n Our example analytics request can be written in Spark as follows:\nfrom pyspark.sql.functions import from_json, substring_index\ndef get_full_url(json_column):   \n    # extract full URL value from a JSON Column\n    url_full = from_json(json_column, “url STRING”)\n    return url_full\n    \ndef extract_url(json_column):\n    url_full = get_full_url(json_column)\n    url = substring_index(url_full, “?”, 1)  \n    return url\ndef extract_campaign_code(json_column):\n    url_full = get_full_url(json_column)\n    code = substring_index(url_full, “?”, -1)\n    return substring_index(code, “=”, -1)\ncampaigns_df = … # Use either Spark SQL or Spark Python API to get the \n➥ Dataframe\nclicks_df = … # Use either Spark SQL or Spark Python API to get the Dataframe\nresult_df = campaigns_df.join(...)\nWe have omitted some code here that performs the actual join since it’s very similar to\nwhat you have seen in our SQL example and in fact can be written in SQL as well.\nWhat we would like to demonstrate here is that extracting campaign code has been\nfactored out into a separate Python function. This function contains comments that\nmake the code easier to understand. You can also use standard Python testing capabil-\nities to write a comprehensive suite of tests to validate all the edge cases. If you need to\nuse this URL parsing logic in other places in your pipeline, you can easily put it into its\nown library together with other common functions. \n While having more modular and testable code may not sound like a huge advan-\ntage of a data platform over a data warehouse, it is extremely important, in our experi-\nence. Once your platform grows to several dozen use cases with several data engineers\nworking on it, maintaining a code base that is well organized and understood by every-\none is critical to your success. \nListing 2.2\nExample Apache Spark implementation using Python API\nSplits URL parsing logic into small \nfunctions that are easy to test\nExtracts URL portion \n(without parameters) but \ntaking a substring from the first \ncharacter to the “?” character\nExtracts campaign code portion of the \nURL by taking a substring from the \nlast character in the string to the “?” \ncharacter\nReturns the unique code portion of a “code=XYZ” string\nExercise 2.3\nUsing Spark instead of SQL for complex data processing gives you which key advantage?\n1\nSpark allows you to write modular and reusable code.\n2\nSpark is always much faster than SQL.\n3\nSpark uses machine learning to make complex code simpler.\n\n\n33\nAccessing data\n2.4\nAccessing data\nIn this section, we’ll review the tools that are available for end users to access data in\nthe data warehouse and data platform.\n Our data warehouse–only and data platform architectures offer different ways for\nend users to access the data. In the previous section, we described an example analyti-\ncal use case that can be implemented either as a pure SQL pipeline in Azure Synapse\nor as a Spark job running on the data platform. Both approaches will produce a new\ndata set, containing all pages a particular user visits after landing on a specific market-\ning campaign page. \n This new data set might be of interest in different ways to different users inside\nyour organization. For example:\nMarketing team—Often business users who want to consume the data in a dash-\nboard that lists the top 10 pages visited for each campaign\nData analyst—A power user, who may need to slice and dice data in multiple dif-\nferent ways\nData scientist—An uber power user who may need to categorize users into differ-\nent profiles based on pages visited\nDifferent users will want to consume data in different ways. Reporting and dashboard-\ning tools like Power BI are often preferred by business users and work best with rela-\ntional technologies using a SQL interface. For this use case, Azure Synapse will\nprovide the simplest integration and best performance. \n For many power users or data analysts, SQL is the primary analytical instrument.\nThese users will also benefit from having the resulting data set in Azure Synapse. On\nthe other hand, using a service such as Azure Databricks can allow them to easily run\nSpark SQL even if they aren’t familiar with Spark—bringing them processing perfor-\nmance at scale. \n Data science use cases will require access to raw clickstream data, not just the\nresulting data set described in the previous section. For example, your data science\nteam may be working on a model that categorizes users into several common arche-\ntypes based on their behavior. For this model to work properly, it needs to take into\naccount as many details as possible about how users are browsing your website. The\nmost accurate categorizations are often created using data that might not be the most\nobvious choice to humans, so maximizing the data available to the models used by\ndata scientists is important, and aggregated and cleaned up data that is usually loaded\ninto a warehouse does not contain enough of these details. That’s why tools such as\nSpark and access to files on Azure Blob Storage will be required for data scientists to\ndo their job most efficiently.\n While a data warehouse alone won’t satisfy all these groups of users, a cloud data\nplatform that also includes a data warehouse can. In a cloud data platform, the data\nwarehouse can be used as a destination for your resulting data sets, and your raw data\ncan be made available in storage.\n\n\n34\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nThis flexibility is demonstrated in the architecture we first looked at in section 2.1.3,\nshown again in figure 2.12. \n We will explore this idea in greater detail in chapter 3. \n2.5\nCloud cost considerations\nThis section covers cloud cost control options for the data warehouse and data\nplatform.\n Our comparison of data warehouse and data platform architecture would not be\ncomplete if we didn’t talk about cloud costs. After all, you are likely to get questions\nabout cloud costs soon after launching your cloud platform—usually after the first\nmonth, when the first bills come in :). \n It is difficult to compare specific costs for different services and implementations\neven within a single cloud provider, not to mention across different providers. You are\nlikely to find that many supporting services, such as cloud storage, are relatively inex-\npensive, and services that require heavy computation will drive the bulk of the costs.\nDetermining which of the services you are considering for your architecture are the\nbiggest cost drivers is a good first step. \n Once you know which service or services are contributing the most cost, you\nshould check whether this service supports truly elastic scaling. The idea of an elastic\nservice is that you can use as much of it as you need for only as long as you need it, and\nthen scale it down when you don’t need it, so your overall cost is optimized. Many\ncloud services would claim that they can do this, but you as an architect for your plat-\nform must be able to validate these claims and understand any trade-offs.\n For our cloud data warehouse–only example, the service that drives most of the cost\nis Azure Synapse. This is not surprising since it does all the processing in this design.\nFigure 2.12\nThe data warehouse becomes just another component in a data platform architecture.\nClickstream logs\n(CSV, 100s GBs)\non SFTP server\nAzure Data\nFactory pipeline\nIngest\nAzure Blob\nStorage\nStore\nAzure\nDatabricks\nProcess\nAzure \nSynapse\nServe\nAzure Data\nFactory pipeline\nData scientists will\nuse raw files on Blob\nStorage and run Spark\njobs on Azure Databricks.\nDashboarding tools\nand power users will\nbenefit from SQL access\nusing Azure Synapse.\nMySQL\nAzure\n\n\n35\nCloud cost considerations\nWhile Azure Synapse can be scaled up and down in terms of processing capacity using\nthe Azure API, scaling takes time (tens of minutes sometimes) and during that time,\nthe entire warehouse is unavailable. So does Azure Synapse support truly elastic scal-\ning? Not exactly. Scaling Azure Synapse is something you can schedule to do every\nnight if your users don’t require 24/7 access to the data, but it is not something you\nwould be able to do multiple times a day, because that would be very disruptive.\nAnother challenge is that you can’t create multiple instances of Azure Synapse that can\nwork with the same data without having to copy data from one instance to another,\nwhich would take time as well. \n In our cloud data platform design, we have Azure Blob Storage as our primary\nstorage, and we use Spark running on Azure Databricks to do the processing. Data-\nbricks will copy the data you need to process to the virtual machines running Spark\nand will store it in memory. This copy does add some overhead when launching new\njobs, but the benefit is that you can create multiple Spark clusters working with the\nsame data. Those clusters can be of different sizes for different processing needs and\ncan be terminated when not used to save cost. From an elasticity perspective, Azure\nDatabricks provides better cost control options than Azure Synapse. It’s not\nuncommon to see a dedicated cluster be provisioned just to run a heavy SparkSQL\nquery and then be torn down.\n There are, of course, a lot more nuances when it comes to cloud costs, and we will\ntalk more about this important topic in chapter 11.\n To summarize the discussion around data warehouse and data platform design, we\nhave put together table 2.1 to outline use cases where you might want to pick one\ndesign over another. \nTable 2.1\nUse cases for a data warehouse–only vs. a data platform\nData warehouse–only design\nData platform design\nYou only have a relational data source.\nYou have multiple data sources with structured and \nsemistructured data.\nYou have control over your source data and a \nprocess in place to manage schema changes.\nYour want to ingest and use data from multiple data \nsources, i.e., spreadsheets or SaaS products over \nwhich you don’t have full control.\nYour use cases are constrained to BI reports \nand interactive SQL queries.\nYou want to be able to use your data for machine \nlearning and data science use cases in addition to \ntraditional BI and data analytics.\nYou have a limited community of data users.\nYou have more and more users in your organization \nrequiring access to the data to do their job.\nYour data volumes are small enough to justify \nthe cost of storing and processing all data \ninside a cloud data warehouse.\nYou want to optimize your cloud costs by using different \ncloud services for storing and processing data.\n\n\n36\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nSummary\nPracticing using an RDBMS data source and a data file that contains a JSON\ndocument will give you a good understanding of the challenges associated with\nthe variety of data that is common in a modern analytic platform.\nOne of the most obvious differences between a data platform and a data\nwarehouse–only implementation is how they handle schema changes. In a data\nplatform, unlike with a data warehouse–only design, there is no need to provide\na schema for the incoming data, making it much easier to handle schema\nchanges in a data platform than in a data warehouse. \nA second key difference between a data platform and a data warehouse–only\ndesign is where processing takes place. SQL processing in a data warehouse may\nseem simple, but a lack of available testing frameworks, scalability complexity,\nand restrictions on performance optimization may become problems as data\nvariety and volume increase.\nIn a data platform, distributed data processing engines such as Spark offer huge\nflexibility when dealing with large, semistructured data sets because Spark can\nsplit the file into smaller chunks and use multiple parallel tasks to process each\nchunk. This provides the scalability we are looking for in a big data system.\nData platforms that also include a data warehouse bring maximum flexibility\nfor users, allowing power users access to the data in the lake via Spark jobs as\nwell as SQL-based access to data in the data warehouse, while less technical\nusers can enjoy access using a plethora of commercial tools that can connect via\nSQL to the data warehouse.\nUsing PaaS services in your analytic platform design will minimize ongoing sup-\nport costs and time and make setup much faster.\nThe clear separation between storage and compute in a data platform design\nbrings cost flexibility because storage and compute are charged for separately\nin a cloud data platform architecture, and each one can be optimized for cost\nindependently of the other.\nThe combination of a data warehouse and data platform in the same architec-\nture brings better performance, more options for user access, and lower costs\nthan a data warehouse alone.\n2.6\nExercise answers\nExercise 2.1:\n 4—Data platform adds a “data lake” layer to offload data processing from the\nwarehouse.\nExercise 2.2:\n 2—It saves data into Azure Blob Storage first, which doesn’t require strict schema\ndefinition.\nExercise 2.3:\n 1—Spark allows you to write modular and reusable code.\n\n\n37\nGetting bigger and\n leveraging the Big 3:\n Amazon, Microsoft\n Azure, and Google\nChapter 2 covered setting up a simple data platform made up of a data lake and a\ndata warehouse in the cloud, with simple batch pipelines to ingest data. It also laid\nout the pros and cons of a data lake versus a data warehouse versus a combination\nof the two to produce the best analysis outcomes.\nThis chapter covers\nDesigning a flexible and scalable six-layer data \nplatform architecture\nUnderstanding how layers support both batch and \nstreaming data\nEnsuring the right foundational components for \neasier management\nImplementing a modern cloud data platform in \nAWS, Google, or Azure\n\n\n38\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n In this chapter, we’ll build on the data platform architecture concepts introduced in\nchapters 1 and 2, and we’ll layer on top of those some of the critical and more advanced\nfunctionality needed for most data platforms today. Without this added layer of sophis-\ntication, your data platform would work, but it wouldn’t scale easily, nor would it meet\nthe growing data velocity challenges discussed in chapter 1. It would also be limited in\nterms of the types of data consumers (people and systems who consume the data from\nthe platform) it supports, as they too are growing in both numbers and variety.\n We will take a deeper dive into a more complex cloud data platform architecture,\nexploring which functional layers exist in modern platform architectures and the\nroles they play. We’ll also introduce the concepts of fast/slow storage, streaming ver-\nsuss batch, metadata management, ETL overlays, and data consumers. \n In a world that has new tools and services being announced almost daily, we’ll also\nlook at some of the tools and services that are available to you to bring into your data\nplatform planning. We’ll map the functional layers to existing tools and services avail-\nable from the three major public cloud providers—Amazon, Google, and Microsoft—\nand then we’ll take a look at some of the tooling that is independent of cloud provid-\ners, including both open source and commercial offerings. \n We know that choosing a cloud provider is a decision that hinges on far more than\nwhat services they offer, and the truth is that you can build and operate a great data\nplatform on any of the three cloud providers discussed here. Our recommendations\non tools are intended to help narrow your search and free up some of your time to\nstart working on your design.\nNOTE\nThere is an ongoing debate about the trade-offs between using cloud\nvendor–specific services versus trying to build a platform that is cloud vendor\nindependent. Going cloud vendor–specific and using PaaS keeps support\ncosts low but makes migrating your platform from one cloud vendor to\nanother challenging. Using non-vendor-specific services, such as open source\nsoftware, brings portability but increases the management burden. There is\nno easy answer, but to date our experience has been that the benefits of going\ncloud vendor–specific and using PaaS outweighs the benefits of being cloud\nvendor independent.\nThat’s a lot in one chapter, but this is where your data platform design gets really\ninteresting. \n3.1\nCloud data platform layered architecture\nWe’ll start by reminding you of the simple high-level data platform architecture we\nintroduced in chapter 1; then we’ll build on that design to introduce you to the six\nlayers of our more complex data platform, which coordinate work across multiple lay-\ners: a data ingestion layer, a data processing layer, a metadata layer, a serving layer, and\ntwo overlay layers (the orchestration and ETL layers).\n We’ll discuss what they do and why, along with some tips that we’ve learned by\nimplementing these in real life. We’ll also talk about why it’s a good idea from an\narchitecture perspective for these layers to be “loosely coupled,” or separate layers\n\n\n39\nCloud data platform layered architecture\nthat communicate through a well-defined interface and don’t depend on the internal\nimplementation of a specific layer. \n In chapter 1 we introduced a very high-level architecture of a data platform. It had\nfour layers (ingestion, storage, processing, and serving) and looked like figure 3.1.\nAlso in chapter 1, we discussed each of these architecture components and what their\ncore functions are, so if you skipped that bit you might want to go back and review\nthem because, in this chapter, we will expand on this data platform architecture and\nprovide more details about specific functional components. Figure 3.2 shows a more\nsophisticated data platform architecture that builds on our simpler version from chap-\nter 1 and expands it into six layers.\n These layers are as follows:\nIn the ingestion layer (1), we’re showing a distinction between batch and\nstreaming ingest.\nIn our storage layer (2), we’re introducing the concept of slow and fast storage\noptions.\nIn our processing layer (3), we discuss how it will work with batch and stream-\ning data, fast and slow storage.\nWe’ve added a new metadata layer (4) to enhance our processing layer.\nWe’ve expanded the serving layer (5) to go beyond a data warehouse to include\nother data consumers.\nWe’ve added an overlay layer (6) for ETL and/or orchestration.\nA layer is a functional component that performs a specific task in the data platform\nsystem. In practical terms, a layer is either a cloud service, an open source or commer-\ncial tool, or an application component that you have implemented yourself. Very\noften, it’s a combination of several such components. Let’s go through each of these\nlayers in more detail.\nIngest\nStorage\nProcessing\nServing\nCloud data\nwarehouse\nAPIs\nData export\nStream\nExploration, data science experiments\nFigure 3.1\nThe four-layer high-level data platform architecture introduced in chapter 1\n\n\n40\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n3.1.1\nData ingestion layer\nThe name of this layer is self-explanatory—it is responsible for connecting to the\nsource systems and bringing data into the data platform. There is, of course, much\nmore happening in this layer (see figure 3.3). \nFigure 3.2\nCloud data platform six-layered architecture\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nOperational\nmetadata\nThe ingestion layer connects to\nsource systems and brings data\ninto the data lake, preserving\noriginal data formats.\nBuild your ingestion\nlayer to support\nbatch and stream\ningests as first-class\ncitizens for maximum\nflexibility.\nBatch\ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.3\nThe data ingestion layer connects to the source system and brings data into the data \nplatform.\n",
      "page_number": 42
    },
    {
      "number": 3,
      "title": "Getting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google",
      "start_page": 60,
      "end_page": 101,
      "detection_method": "regex_chapter",
      "content": "41\nCloud data platform layered architecture\nThe data ingestion layer should be able to perform the following tasks:\nSecurely connect to a wide variety of data sources—in streaming and/or batch\nmodes.\nTransfer data from the source to the data platform without applying significant\nchanges to the data itself or its format. Preserving raw data in the lake is import-\nant for cases where data needs to be reprocessed later, without having to reach\nout to the source again.\nRegister statistics and ingestion status in the metadata repository. For example,\nit’s important to know how much data has been ingested either in a given batch\nor within a specific time frame if it’s a streaming data source. \nYou can see in figure 3.4 that our architecture diagram has both batch and streaming\ningestion coming into the ingestion layer.\nYou may be hearing that the data processing world is moving (or has already moved,\ndepending on who you talk to) to data streaming and real-time solutions. While we\nagree that this is the direction the industry is heading, we also can’t ignore the current\nreality. In our experience, there are many existing data stores today that only support\nbatch data access. This is especially true for third-party data sources that are often\ndelivered as files on FTP in CSV, JSON, or XML format or other systems with batch-\nonly access patterns. \n For the data sources you control, such as your operational RDBMS, it is possible to\nimplement a full streaming solution. It’s significantly harder, if not impossible, to\nachieve the same with third-party sources. And because third-party sources often com-\nprise a large portion of all the data that needs to be brought into the data platform,\nboth batch and streaming ingestion are likely to be part of your data platform design.\nUnlike with a lambda\narchitecture, here\nstreaming and batch\ndata go through\ndifferent pipelines.\nUse streaming\ndata for one-\nevent-at-a-time\ndata access. \nUse batch for CSV,\nJSON, and XML \nfiles on FTP \nservers, and systems \nwith batch-only \naccess patterns. \nBatch\ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.4\nThe ingestion layer should support streaming and batch ingestion.\n\n\n42\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n We believe that it’s sound architectural thinking to build your data ingestion layer\nto support both batch and streaming ingestion as first-class citizens. This means using\nappropriate tools for different types of data sources. For example, instead of creating\na true streaming capability, you could ingest streaming data as a series of small\nbatches, but you will give up the ability to perform real-time analytics in the future.\nOur design should prevent such technical debt. This way, you will always be able to\ningest any data source, no matter where it comes from. And this is one of the most\nimportant characteristics of a data platform. \nNOTE\nWe often hear from companies that their data platform must be “real-\ntime,” but we’ve learned that it’s important to unpack what real-time means\nwhen it comes to analytics. In our experience, there are often two different\ninterpretations: making data available for analysis as soon as it’s produced at the\nsource (i.e., real-time ingestion) or immediately analyzing and taking action on\ndata that has been ingested in real time (i.e., real-time analytics). Fraud detec-\ntion and real-time recommendation systems are good examples of real-time\nanalytics. Most of our customers actually don’t need real-time analytics, at least\nnot yet. They simply want to ensure that the data they use to produce insights\nis up to date, meaning as current as a few minutes or hours ago, even though\nthe report or dashboard may only be looked at periodically. Given that real-time\nanalytics is much more complex than real-time ingestion, it’s often worth\nexploring user needs in detail to fully understand how to best architect your\ndata platform.\nSome of you may look at the cloud data platform architecture diagram in this chapter,\nsee two ingestion paths, one for batch and one for stream, and ask if this is an exam-\nple of a lambda architecture.\nNOTE\nFor those of you who are not familiar with lambda architecture, we rec-\nommend this great resource on the topic: http://lambda-architecture.net/.\nIn a nutshell, a lambda architecture suggests that, in order to provide accurate analytical\nresults combined with low-latency analytical results, a data platform must support both\nbatch and streaming data processing paths. The difference between lambda architec-\nture and the cloud data platform architecture described here is that in lambda, the same\ndata goes through two different pipelines, and in our data platform architecture, batch\ngoes through one pipeline and streaming through a different one.\n The lambda architecture was conceived in the early days of Hadoop implementations,\nwhen it was impossible to build a fully resilient and accurate real-time pipeline. The fast\npath provided low-latency results, but due to limitations of some of the streaming frame-\nworks available in Hadoop-based platforms, these results were not always 100% accurate.\nTo reconcile the potential differences in results, the same data is pushed through a batch\nlayer and reconciled to produce a completely correct result at a later stage. \n Today with cloud services such as Cloud Dataflow from Google or open source\nsolutions such as Kafka Streams, these limitations have largely been overcome. Other\n\n\n43\nCloud data platform layered architecture\nframeworks such as Spark Streaming have also made significant improvements to\naccuracy and handling failures. So in our proposed cloud data platform architecture,\nbatch and stream ingestions paths are actually intended for completely different data\nsources. If a data source doesn’t support real-time data access, or the nature of the\ndata is such that it arrives only periodically, it’s easier and more efficient to push this\ndata through the batch path. Other sources that support one-event-at-a-time data\naccess, such as streaming, should go through the real-time layer. \n Delivering data into the lake efficiently and reliably requires a data ingestion layer\nto have four important properties:\nPluggable architecture—New types of data sources are added all the time. It’s\nunrealistic to expect that connectors for every data source will be available in\nthe ingestion tool or service that you choose. Make sure that your data inges-\ntion layer allows you to add new connector types without significant effort.\nScalability—The data ingestion layer should be able to handle large volumes of data\nand scale beyond a single computer capacity. You may not need all this scale today,\nbut you should always plan ahead and choose solutions that will not require you to\ncompletely revamp your data ingestion layer when you need to grow. \nHigh availability—The data ingestion layer should be able to handle the failure\nof individual components, such as disk, network, or full virtual machine failures\nand still be able to deliver data to the data platform. \nObservability—The data ingestion layer should expose critical metrics, like data\nthroughput and latency, to external monitoring tools. Most of these metrics\nshould be stored in the central metadata repository that we will discuss later in\nthis chapter. Some of the more technical metrics, such as memory or CPU or\ndisk utilization, might be exposed to the monitoring tools directly. It’s import-\nant to make sure that the data ingestion layer doesn’t act as a black box if you\nwant visibility into the movement of your data into the data platform. This is\nimportant for monitoring and troubleshooting purposes.\nLater in this chapter we will explore which services and tools, both open source and\ncommercial, are available for a data ingestion layer implementation on all three cloud\nproviders. Not all of the tools will satisfy all of the requirements, and it will be import-\nant to understand the trade-offs a data platform architect will have to accept when\nchoosing a particular solution. \nExercise 3.1\nYou need to plan for both batch and real-time ingestion because (choose one)\n1\nReal-time tools are complex and hard to manage.\n2\nBatch ingestion is the only way to get accurate data.\n3\nSome data sources don’t support real-time ingestion.\n4\nYou need two ingestion mechanisms for redundancy.\n\n\n44\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n3.1.2\nFast and slow storage\nBecause the data ingestion layer usually doesn’t store any data itself, though it may use\ntransient cache, once data is passed through the ingestion layer, it must be stored reli-\nably. The storage layer in the data platform architecture is responsible for persisting\ndata for long-term consumption. It has two types of storage—fast and slow, as shown in\nfigure 3.5.\nWe will be using the terms slow and fast storage throughout this book to differentiate\nbetween cloud storage services that are optimized for larger files (tens of MBs and\nmore) and those optimized for storing smaller bits of data (KBs, typically), but with\nmuch higher performance characteristics. Such systems are also sometimes referred\nto as message buses; distributed logs are queues with persistence. Fast and slow here is\nnot a reference to specific hardware characteristics, such as the difference between\nHDD and SSD drives, but rather the characteristics of the storage software design and\nuse cases it is targeted for. As another example, frameworks that allow you to process\ndata in real-time (Cloud Dataflow, Kafka Streams, and so on) are tied to a specific stor-\nage system. So if you want to do real-time, you will need to work directly with the fast\nstorage layer.\n The storage layer in the data platform must perform the following tasks:\nStore data for both the long term and short term\nMake data available for consumption either in batch or streaming modes\nSlow storage\nis your archive\nand area for\nbatch data. It\nis often used\nfor permanent\ndata retention.\nFast storage is a\nmessage bus for\ndata that is coming\nin streams, message by\nmessage. It typically has\ndata expiry policies.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.5\nThe data storage layer persists data for consumption using fast and slow storage.\n\n\n45\nCloud data platform layered architecture\nOne of the benefits of cloud is that storage is so inexpensive that storing data for years\nor even decades becomes feasible. Having all that data available gives you so many\noptions with the ability to repurpose it for new analytic use cases, such as machine\nlearning at the top of the list.\n In our cloud data platform architecture, the data storage layer is split into two dis-\ntinct components—slow and fast storage. Slow storage is your main storage for archive\nand persistent data. This is where data will be stored for days, months, and often years\nor even decades. In a cloud environment, this type of storage is available as a service\nfrom the cloud vendors as an object store that allows you to cost effectively store all\nkinds of data and support fast reading of large volumes of data.\n The main benefit of using an object store for long-term storage is that in the cloud\nyou don’t have any compute associated directly with the storage. For example, you\ndon’t need to provision and pay for a new virtual machine if you want to increase the\ncapacity of your object store. Cloud vendors grow and shrink the capacity of your stor-\nage in response to the actual data you upload or delete. This makes this type of stor-\nage very cost efficient. \n The main downside of object stores is that they don’t support low-latency access.\nThis means that for streaming data, which operates on a single message or a single\ndata point at a time, an object store will not provide the necessary response time.\nThere is a difference between uploading a 1TB file with JSON data to the object store\n(batch) or trying to upload the same volume as a billion single JSON documents, one\nat a time (streaming). \n For streaming use cases, a cloud data platform requires a different type of storage.\nWe call it “fast” storage because it can accommodate low-latency read/write opera-\ntions on a single message. Most associate storage of this type with Apache Kafka, but\nthere are also services from cloud vendors that have similar characteristics. We will\nexplore what those are in more detail later in this chapter. \n Fast storage brings the low latency required for streaming data ingestion, but it\nusually means that some compute capacity is associated with the storage itself. For\nexample, in a Kafka cluster, you will need to add new machines with RAM, CPU, and\ndisk if you want to increase your fast storage capacity. This means that the cost of fast\ncloud storage is significantly higher than the cost of slow storage. In practice, you\nwould configure a data retention policy, where your fast storage only stores a certain\namount of data (one day, one week, or one month, depending on your data volumes).\nThe data will then be transferred to a permanent location on the slow storage and\npurged from the fast storage per the policy. \n The storage layer should have the following properties:\nReliable—Both slow and fast storage should be able to persist data in the face of\nvarious failures.\nScalable—You should be able to add extra storage capacity with minimal effort.\n\n\n46\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nPerformant—You should be able to read large volumes of data with high enough\nthroughput from slow storage or read/write single messages with low latency to\nfast storage. \nCost efficient—You should be able to apply a data retention policy to optimize\nstorage mix to optimize costs.\n3.1.3\nProcessing layer\nThe processing layer, highlighted in figure 3.6, is the heart of the data platform imple-\nmentation. This is where all the required business logic is applied and all the data val-\nidations and data transformations take place. The processing layer also plays an\nimportant role in providing ad hoc access to the data in the data platform.\nThe processing layer should be able to perform the following tasks:\nRead data in batch or streaming modes from storage and apply various types of\nbusiness logic\nExercise 3.2\nWhy do you need two types of storage in your data platform?\n1\nTo provide redundancy in case one storage type fails\n2\nTo support both batch and real-time processing\n3\nTo optimize cloud costs\n4\nTo support both data science and business intelligence use cases\nA distributed data\nprocessing engine\nis a must. You might\nneed separate ones\nfor batch and for\nreal time.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.6\nThe processing layer is where business logic is applied and all data validations \nand data transformations take place, as well as providing ad hoc access to data.\n\n\n47\nCloud data platform layered architecture\nSave data back to storage for data analysts and data scientists to access\nDeliver streaming data outputs to consumers, usually other systems\nThe processing layer is responsible for reading data from storage, applying some cal-\nculations on it, and then saving it back to storage for further consumption. This layer\nshould be able to work with both slow and fast data storage. This means that the ser-\nvices or frameworks that we choose to implement this layer should have support for\nboth batch processing of files stored in slow storage as well as one-message-at-a-time\nprocessing from the fast storage. \n Today, there are open source frameworks and cloud services that allow you to pro-\ncess data from both fast and slow storage at the same time. A good example is an open\nsource Apache Beam project and service from Google called Cloud Dataflow, which\nprovides a managed platform-as-a-service execution environment for Apache Beam\njobs. Apache Beam supports both batch and real-time processing models within the\nsame framework. Generally, a layer in the data platform doesn’t have to be imple-\nmented using a single cloud service or software product. Often, you will find that\nusing specialized solutions for each batch and stream processing will give you better\nresults than using a single multipurpose tool. \n The processing layer should have the following properties:\nScale beyond a single computer. Data processing framework or cloud service\nshould be able to work efficiently with data sizes ranging from megabytes to\nterabytes or petabytes. \nSupport both batch and real-time streaming models. Sometimes it makes sense\nto use two different tools for this. \nSupport most popular programming languages, such as Python, Java, or Scala.\nProvide a SQL interface. This is more a “nice to have” requirement. A lot of\nanalytics, especially in the ad hoc scenario, is done using SQL. Frameworks that\nsupport SQL will significantly increase the productivity of your analysts, data sci-\nentists, or data engineers. \n3.1.4\nTechnical metadata layer\nTechnical metadata, as opposed to business metadata, typically includes but isn’t lim-\nited to schema information from the data sources; status of the ingestion; transforma-\ntion pipelines such as success, failure, error rates, and so on; statistics about ingested\nand processed data, like row counts; and lineage information for data transformation\npipelines. As shown in figure 3.7, the metadata layer is central to the data platform\nand is kept in a metadata store.\n A data platform metadata store performs the following tasks:\nStores information about the activity status of different data platform layers\nProvides an interface for layers to fetch, add, and update metadata in the store\n\n\n48\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nThis technical metadata is very important for automation, monitoring and alerting,\nand developers’ productivity. Since our data platform design consists of multiple lay-\ners that sometimes don’t communicate directly with other layers, we need to have a\nrepository that stores the state of these layers. This allows, for example, the data pro-\ncessing layer to know which data is now available for processing by checking the meta-\ndata layer instead of trying to communicate with the ingestion layer directly. This\nallows us to decouple different layers from each other, reducing the complexities asso-\nciated with interdependencies. \n Another type of metadata that you might be familiar with is business metadata, which\nis usually represented by a data catalog that stores information about what data actually\nmeans from a business perspective; for example, what this specific column in this data\nsource represents. Business metadata is an important component of an overall data\nstrategy because it allows for easier data discovery and communication. Business meta-\ndata stores and data catalogs are well represented by multiple third-party products and\ncan be plugged into the layered data platform design as another layer. Going into details\nof business metadata solutions is outside of the scope of this book.\n A data platform metadata store should have the following properties:\nScalable—There can be hundreds (or sometimes thousands!) of individual tasks\nrunning in the data platform environment. A metadata layer must be able to\nscale to provide fast responses to all of the tasks.\nHighly available—The metadata layer can become a single point of failure in\nyour data platform pipelines. If the processing layer needs to fetch information\nabout which data is available for processing from the metadata layer and the\nTechnical metadata typically\nincludes schema information,\nstatus of the ingestion and\ntransformation pipelines,\nstatistics about ingested and\nprocessed data, and lineage\ninformation. \nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.7\nThe metadata layer stores information about the status of the data platform layers, \nneeded for automation, monitoring and alerting, and developer productivity.\n\n\n49\nCloud data platform layered architecture\nmetadata service is not responding, the processing pipeline will either fail or\nget stuck. This can trigger cascading failures if there are other pipelines\ndepending on the one that failed.\nExtendable—There are no strict rules on which metadata you should store in this\nlayer. We will explore the most common items like schema and pipeline statis-\ntics in upcoming chapters. You may also find that often you want to store some\nbusiness-specific information in the metadata layer, such as how many rows with\na certain column value we have. Your metadata layer should allow you to easily\nstore this extra information. \nTechnical metadata management in the data platform is a relatively new topic. There\nare few existing solutions that can fulfill the tasks described here. For example, a Con-\nfluent Schema Registry allows you to store, fetch, and update schema definitions, but\nit doesn’t allow you to store any other types of metadata. Some metadata layer roles\ncan be performed by various ETL overlay services such as Amazon Glue. We will talk\nmore about the pros and cons of these types of tools in the later sections of this chap-\nter. As it stands today, you will likely need a combination of different tools and services\nto implement a fully functional technical metadata layer. \n3.1.5\nThe serving layer and data consumers\nThe serving layer delivers the output of analytics processing to the various data consumers.\n As shown in figure 3.8, the serving layer is responsible for:\nServing data to consumers who expect a relational data structure and full SQL\nsupport via a data warehouse\nServing data to consumers who want to access data from storage without going\nthrough a data warehouse\nFigure 3.8\nThe serving layer delivers the output of analytics processing to data consumers.\nConsumers are both people and\nsystems. Some may need to\nconsume streams directly, some\nwant raw data, and others want\norganized, curated data via the\ndata warehouse.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\n\n\n50\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nIn chapters 1 and 2, we discussed why in most cases a cloud data platform doesn’t\nreplace the need for a data warehouse as a data consumption option. Data warehouses\nprovide a data access point for data consumers that require full SQL support and\nexpect data to be presented in a relational format. Such data consumers may include\nvarious existing dashboarding and business intelligence applications, but also data\nanalysts or power business users familiar with SQL. Figure 3.9 shows the data ware-\nhouse as the access point for these consumers.\nA serving layer will almost always include a data warehouse, which should have the fol-\nlowing properties:\nScalable and reliable—A cloud data warehouse should work efficiently with both\nlarge and small data sets and scale beyond the capacity of a single computer. It\nalso should be able to continue to serve data in the face of inevitable failures or\nindividual components.\nNoOps—Preferably, a cloud warehouse should require as little tuning or opera-\ntional maintenance as possible.\nElastic cost model—Another highly desirable property of the cloud warehouse is\nthe ability to scale both up and down in response to the load. In many tradi-\ntional BI workloads, the data warehouse is used mostly during the business days\nand may experience only a small portion of the load during off hours. A cloud\ncost model should be able to reflect this. \nIn a modern data architecture, while it is very likely you will have data consumers who\nwill want a relational data structure and SQL for data access, increasingly, other data\naccess languages are gaining popularity. \nA cloud data warehouse\nis a destination for\ncurated data sets. It’s\nrole is to provide results\nof the queries back to\nusers—fast.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.9\nA data warehouse is usually included in the serving layer.\n\n\n51\nCloud data platform layered architecture\nSome data consumers will require direct access to the data in the lake, as shown in fig-\nure 3.10.\n Usually data science, data exploration, and experimentation use cases fall into this\ncategory. Direct access to the data in the lake unlocks the ability to work with the raw,\nunprocessed data. It also moves experimentation workloads outside of a warehouse to\navoid performance impacts. Your data warehouse can be serving critical business\nreports and dashboards, and you don’t want it to suddenly slow down because a data\nscientist decided to read the last 10 years of data from it. There are multiple ways to\nprovide direct access to the data in the lake. Some cloud providers, as discussed in the\nnext sections of this chapter, provide a SQL engine that can run queries directly on\nthe files in cloud storage. In other cases, you may use Spark SQL to achieve the same\ngoal. Finally, it’s not uncommon, especially for data science workloads, to just copy\nrequired files from the data platform into experimentation environments like note-\nbooks or dedicated data science VMs.\nNOTE\nWhile it is possible to have a data platform without a data warehouse, it\nis likely that every business has business users who will want access to the data\nin the lake, and these business users are best served via a data warehouse as\nopposed to direct access to the lake. As such, when we talk about data plat-\nforms, we always assume it will feed at the very least a data warehouse for data\nconsumption.\nData consumers aren’t always humans (see figure 3.11). The results of real-time\nanalytics that are being calculated as the data is received a single message at a time are\nrarely intended to be consumed by a human. No one spends their day watching\nReading raw files from\nstorage is often desired\nby data scientists using\nML applications.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.10\nDirect data lake access allows consumers to bypass the serving layer and work \ndirectly with raw, unprocessed data.\n\n\n52\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nmetrics on a dashboard change every second. Outputs from a real-time analytics\npipeline are usually consumed by other applications like marketing activation systems,\nsuch as ecommerce recommendation systems that decide which item to recommend\nto a user while they are shopping, or ad bidding systems where the balance between\nad relevance and cost changes in milliseconds. \nSuch programmatic data consumers require a dedicated API to consume data from\nthe lake in real-time, often using built-in APIs that are often available in the real-time\ndata processing engine of your choice. Alternatively, you can implement a separate\nAPI layer to allow multiple programmatic consumers access to real-time data using the\nsame interface. This separate API layer approach scales better when you have multiple\nprogrammatic data consumers, but it also requires significantly more engineering\neffort to implement and maintain. \nOutputs from a real-\ntime analytics pipeline\nare usually consumed\nby other applications,\nnot people.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.11\nConsumers of real-time data are usually other applications, not humans.\nExercise 3.3\nWhy does a data platform need multiple ways to provide access to the data?\n1\nTo support consumers with different needs and requirements\n2\nTo maximize data throughput\n3\nTo provide failover capabilities in case of outages\n4\nTo improve data quality\n\n\n53\nCloud data platform layered architecture\n3.1.6\nOrchestration and ETL overlay layers\nThere are two components of our cloud data platform architecture that require spe-\ncial consideration. Or rather, they require a slightly different approach to thinking\nabout them. They are the orchestration and ETL overlay layers, highlighted in figure\n3.12. The reason these layers require special treatment is because, in many cloud data\nplatform implementations, the responsibilities of these layers are spread across many\ndifferent tools.\nORCHESTRATION LAYER\nIn a cloud data platform architecture, the orchestration layer (see figure 3.13) is\nresponsible for the following tasks:\nCoordinate multiple data processing jobs according to a dependency graph (a\nlist of dependencies for each data processing job that includes which sources\nare required for each job and whether a job depends on other jobs)\nHandle job failures and retries\nAs we have now seen, a modern cloud data platform architecture includes multiple\nloosely coupled layers that communicate with each other via a metadata layer. The\nmissing piece in this design is a component that can coordinate work across multiple\nlayers. While the metadata layer acts as a repository for various status information and\nstatistics about the data pipelines, the orchestration layer is an action-oriented compo-\nnent. It’s main function is to allow data engineers to construct complex data flows,\nwith multiple interdependencies. \n Imagine the following scenario. Your organization is a retailer that sells goods both\nonline and at brick-and-mortar stores. You want to be able to compare top-selling\nproducts online and at the offline stores. To do that, you need to bring product infor-\nmation data from an enterprise resource planning (ERP) system; sales from the\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.12\nOrchestration and ETL overlay layers usually have responsibilities \nthat are spread across many different tools.\n\n\n54\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\noffline stores are provided on a regular basis by a third-party point-of-sales (POS) pro-\nvider; and online stores sales are available in real-time as clickstream data. \n To produce a top-sellers comparison report, we need to create two data transfor-\nmation jobs. The first job will combine product information with POS sales data. The\nsecond job will use clickstream data and combine it with the output of the first job to\nproduce a comparison data set (figure 3.14). \n As you can see from this example, jobs 1 and 2 can’t just run independently from\neach other. All three sources can be delivering the latest data at very different sched-\nules. For example, POS and products data can only be available once a day, while\nclickstream is a real-time data source. If we don’t coordinate the two data transforma-\ntion jobs somehow, our final data product may have incorrect or incomplete results.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nThe orchestration layer is\nresponsible for coordinating\nmultiple data processing jobs\nand handling job failures\nand retries.\nFigure 3.13\nThe Orchestration overlay allows data engineers to construct complex data flows \nwith multiple inter-dependencies.\nFigure 3.14\nExample of \njobs and data dependency \ngraph\nProducts\nPOS\nClickstream\nJob\n1\nJob\n2\nFinal data product\nThe first data\ntransformation job\ncombines product\ndata with POS\nsales data.\nThe second job \ncombines the\noutput of the first\njob with online sales.\n\n\n55\nCloud data platform layered architecture\nThere are several different ways to address this challenge. One approach is to com-\nbine jobs 1 and 2 into one job and schedule it in such a way that it only runs when the\nlatest data becomes available from all three sources (you can use the metadata layer\nfor this!). This approach sounds straightforward, but what if you need to add more\nsteps to this job? Or what happens if job 2 is a common task that should be shared\nacross multiple different jobs? As the complexity of your data pipeline grows, develop-\ning and maintaining monolithic data processing jobs will become a challenge. Com-\nbining the two jobs has all the downside of a monolithic design: hard to make changes\nto specific components, hard to test, and a challenge for different teams to collabo-\nrate on.\n An alternative approach is to coordinate jobs using an external orchestration\nmechanism (figure 3.15).\nAn orchestration layer, shown in figure 3.15, is responsible for coordinating multiple\njobs based on when required input data is available from an external source, or when\nan upstream dependency is met, such as job 1 needs to complete before job 2 can\nstart. In this case, job implementations remain independent of each other. When they\nare independent, they can be developed, tested, and changed separately, and the\norchestration layer maintains what’s called a dependency graph—a list of dependencies\nfor each data processing job that includes which sources are required for each job and\nwhether a job depends on other jobs. A dependency graph need only be changed\nwhen the logical flow of the data is changed, for example, when a new step in process-\ning is introduced. It doesn’t have to be changed when an implementation of a certain\nstep changes. \nProducts\nPOS\nClickstream\nJob\n1\nJob\n2\nFinal data product\nOrchestration layer\nTrigger job 2 when job 1 completes and we have\naccumulated a full day of clickstream data.\nTrigger job 1 when products\nPOS data arrives.\nFigure 3.15\nAn orchestration layer coordinates multiple jobs while allowing job \nimplementations to remain independent of each other.\n\n\n56\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n In a large data platform implementation, the dependency graph can contain hun-\ndreds and sometimes thousands of dependencies. In such implementations, there are\nusually multiple teams involved in developing and maintaining the data processing\npipelines. The logical separation of the jobs and the dependency graph makes it eas-\nier for these teams to make changes to parts of the system, without having to impact\nthe larger data platform. \n An orchestration layer should have the following properties:\nScalability—It should be able to grow from a handful to thousands of tasks and\nhandle large dependency graphs efficiently.\nHigh availability—If the orchestration layer is down or unresponsive, your data\nprocessing jobs will not run.\nMaintainability—It should have a dependency graph that is easy to describe and\nmaintain.\nTransparency—It should provide visibility into job statuses, history of execution,\nand other observability metrics. This is important for monitoring and debug-\nging purposes.\nThere are several implementations of the orchestration layer available today. One of\nthe most popular is Apache Airflow, an open source job scheduler and orchestration\nmechanism. Airflow satisfies most of the properties listed in this section and is avail-\nable as a cloud service on Google Cloud Platform, called Cloud Composer. There are\nother tools such as Azkaban and Oozie that can serve the same purpose, but both have\nbeen created specifically as job orchestration tools for Hadoop and don’t fit into flexi-\nble cloud environments as well as Airflow does.\n When it comes to native cloud services, different cloud providers approach the\norchestration problem differently. As mentioned, Google adopted Airflow and made\nit available as a managed service, simplifying the operational aspect of managing the\norchestration layer. Amazon and Microsoft include some orchestration features into\ntheir ETL tools overlay products. \nETL TOOLS OVERLAY\nAn ETL tools overlay, highlighted in figure 3.16, is a product or a suite of products\nwhose main purpose is to make the implementation and maintenance of cloud data\npipelines easier. These products absorb some of the responsibilities of the various data\nplatform architecture layers and provide a simplified mechanism to develop and man-\nage specific implementations. Usually these tools have a user interface and allow for\ndata pipelines to be developed and deployed with little or no code. \n ETL overlay tools are usually responsible for\nAdding and configuring data ingestion from multiple sources (ingestion layer)\nCreating data processing pipelines (processing layer)\nStoring some of the metadata about the pipelines (metadata layer)\nCoordinating multiple jobs (orchestration layer)\n\n\n57\nCloud data platform layered architecture\nAs you can see, an ETL overlay tool can implement almost all layers in the cloud data\nplatform architecture. \n Does this mean that you can implement a data platform using a single tool and not\nworry about implementing and managing separate layers yourself? The answer, per-\nhaps unsurprisingly, is that “it depends.” The main question you have to keep in mind\nwhen deciding to fully rely on an ETL overlay service from a cloud vendor (or a simi-\nlar third-party solution) is, “How easy it is to extend it?” Is it possible to add new com-\nponents for data ingestion? Do you have to do data processing using only ETL service\nfacilities, or can you call external data processing components? Finally, it’s important\nto understand whether you can integrate other third-party services or open source\ntools with this ETL service. \n The reason these questions are important is because no system is static. You may\nfind that using an ETL service is a great way to get you started and can provide signifi-\ncant time/cost savings. As your data platform grows, you may find yourself in a situa-\ntion where the ETL service or tool doesn’t allow you to easily implement some needed\nfunctionality. If this ETL service doesn’t provide any options for extending its func-\ntionality or integrating with other solutions, then your only choice is to build a work-\naround that bypasses the ETL layer completely. \n In our experience, at some point these workarounds will become as complex as\nthe initial solution itself and you will end up with what we fondly call a “spaghetti\narchitecture.” We certainly don’t like speaking poorly of one of our favorite meals, but\na spaghetti architecture is the result of different components of the system becoming\nmore and more tangled together, making it harder to maintain. In a spaghetti archi-\ntecture, workarounds exist not because they fit into the overall design but because\nthey have to compensate for ETL service limitations. \nFigure 3.16\nAn ETL tools overlay can be used to implement many of the layer functions in a cloud \ndata platform architecture.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nThe ETL tools overlay is a\nproduct or a suite of products\nwhose main purpose is to\nmake the implementation and\nmaintenance of data pipelines\neasier.\nOperational\nmetadata\n\n\n58\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n An ETL overlay should have the following properties:\nExtensibility—It should be possible to add your own components to the system.\nIntegrations—It’s important for modern ETL tools to be able to delegate some\nof the tasks to external systems. For example, if all data processing is done\ninside the ETL service using a black-box engine, then you will not be able to\naddress any of the limitations or idiosyncrasies of this engine.\nAutomation maturity—Many ETL solutions offer a no-code-required, UI-driven\nexperience. This is great for rapid prototyping, but when thinking about a pro-\nduction implementation, consider how this tool will fit into your organization’s\ncontinuous integration/continuous delivery practices. For example, how will a\nchange to a pipeline be tested and validated automatically, before being promoted\nto a production environment? If the ETL tool doesn’t have an API or an easy-to-\nuse configuration language, then your automation options will be very limited.\nCloud architecture fit—The ETL tools market is very mature, but many open\nsource and commercial tools were built in the age of on-premises solutions and\nmonolithic data warehouses. While all of them offer some form of cloud inte-\ngration, you need to carefully evaluate whether this particular solution allows\nyou to utilize cloud capabilities to their fullest extent. For example, some of the\nexisting ETL tools are only capable of processing data using SQL running in\nthe warehouse, while others use their own processing engines, which are less\nscalable and robust than Spark or Beam.\nWhen it comes to existing ETL overlay solutions, there are many available options.\nLater in this chapter, we will take a look at the cloud services AWS Glue, Azure Data\nFactory, and Google Cloud Data Fusion. \n As for ETL solutions that are not part of cloud vendor services, there are many\nthird-party solutions to choose from. Talend is one of the more popular solutions\ntoday. It uses an “open core” model, meaning that the core functionality of Talend is\nopen source and free to use for development and prototyping. When it comes to\nusing Talend for production workloads, a proper commercial license is required.\nInformatica is another ETL tool popular among larger organizations. This book will\nfocus on cloud-native ETL overlay solutions as well as free open source components.\nCommercial or open-core ETL products are outside of the scope of this book, but\nthere is a lot of existing literature available on the topic.\nExercise 3.4\nWhat is the role of the orchestration layer in the data platform architecture?\n1\nTo allow users to find data easily\n2\nTo optimize performance of different data platform components\n3\nTo manage dependencies between different data processing jobs\n4\nTo provide a UI for data engineers to create new data processing jobs\n\n\n59\nThe importance of layers in a data platform architecture\n3.2\nThe importance of layers in a data platform architecture\nOne of the key conceptual models used in a cloud data platform architecture is the\nidea of a layer. Each layer plays a very specific role, and it’s a good idea from an archi-\ntecture perspective to isolate layers from each other as much as possible. \n Let’s take another look at our data architecture and its layers in figure 3.17.\nYou can see that there is a clear separation between the layers. In the case of ingestion\nand processing, while you could implement both ingestion and processing using a sin-\ngle service, tool, or application, it’s not the best decision as it will cause problems\ndown the road. \n For example, you could use Apache Spark for both data processing and data inges-\ntion, because, while Spark is really good at processing, it also comes with a set of con-\nnectors to external systems such as relational databases and the like. Using Spark for\nboth ingestion and processing combines two functionally different layers into one.\nHere are some of the things to consider if you decide to go this route:\nAre you getting the best possible functionality for data ingestion and data pro-\ncessing using Spark? Yes, Spark can perform data ingestion, but is it functionally\nthe best solution? Is it easy to extend if you will need to bring in a data source,\nfor which there is no existing Spark connector? \nIn larger organizations, different teams or different developers can be\nresponsible for data ingestion and data processing. In fact, you would probably\nwant to encourage a more centralized and controlled data ingestion process,\nbut enable more self-service data transformation practices. This will allow data\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.17\nFor maximum flexibility, functional layers in the data platform should be separate but \nloosely coupled.\n\n\n60\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nconsumers to shape data to their preferences and needs. If you combine\ningestion and processing together, you will lose this flexibility.\nWhat if you decide to replace Apache Spark with a data processing solution that\nsuits your needs better? If your ingestion and processing are combined, you will\nneed to reimplement both of these functions. If they are separate, you can keep\ningestion going and gradually replace processing with another tool.\nIf we translate this idea of separate functional layers into software development terms,\nwe will say that functional layers in the data platform should be loosely coupled. This\nmeans that separate layers must communicate through a well-defined interface but\nshould not depend on the internal implementation of a specific layer. This approach\nbrings significant flexibility to how you can mix and match different cloud services or\ntools to achieve your goals. \n Cloud implementations are notorious for being in a state of constant change:\nthere are always new services being released by cloud vendors or new projects avail-\nable from the open source community. A data platform architecture based on loosely\ncoupled layers allows you to respond to these changes with the least possible impact\non the overall data platform structure. \n3.3\nMapping cloud data platform layers to specific tools \nWhen it comes to selecting services and tools that you can use in your data platform\ndesign planning, you have various options—from PaaS (platform as a service) to\nserverless to open source and SaaS (software as a service) offerings. \n In the next several sections of this chapter, we will map different data platform layers\nto specific services and tools that are available in three major cloud environments: AWS,\nGoogle Cloud, and Azure. There are always multiple options for each layer. There is no\none-size-fits-all solution, and specific implementations will depend on many factors,\nsuch as your organization’s skills, budgets, timelines, and analytical needs. \n When talking about specific implementation details, we will follow the following\npriorities:\n1\nCloud-native platform-as-a-service solutions from AWS, Google, and Microsoft\n2\nServerless solutions\n3\nOpen source solutions\n4\nCommercial and third-party SaaS offerings\nThere are trade-offs among the different options—most often the trade-off between con-\ntrol, flexibility, and effort to support versus portability among cloud and on-premises plat-\nforms. You can see how each option is rated along a trade-off continuum in figure 3.18.\n We will always start with an overview of fully managed solutions from cloud vendors for\neach of the data platform layers, if such a solution exists. While there are pros and cons for\neach type of solution, PaaS solutions usually offer an environment where you don’t need\nto spend time on the mundane tasks associated with managing your own servers, making\nsure that versions of different libraries actually work together, and so on. These solutions\nalso automate many time-consuming tasks such as managing connections to external\n\n\n61\nMapping cloud data platform layers to specific tools\nsystems or keeping track of what data has been already ingested. This can significantly\nimprove the productivity of the team or a person working on the data platform project.\nPaaS solutions are also a big area of investment from cloud vendors, so there are always new\nfeatures and improvements being released. On the other hand, PaaS solutions are usually\nthe most limiting when it comes to extensibility. It’s often difficult or even completely\nimpossible to introduce your own modules or libraries, add new connectors, and so on.\n Next on our list of recommended solutions are serverless solutions. In a nutshell,\ncloud serverless solutions allow you to execute custom application code without hav-\ning to manage your own servers or worry about scalability and fault tolerance. These\nsolutions offer all the benefits of a managed cloud environment, but are more flexible\nsince you can write your own code. Today several different serverless services exist on\ndifferent clouds and typically include data processing services and short-lived light-\nweight cloud functions. \n We will also provide an overview of existing open source solutions that can be used\nto implement various data platform layers. These solutions usually provide the most\nflexibility, including portability across cloud vendors, but usually require you to provi-\nsion and manage your own cloud infrastructure. This is still a very viable approach if\nthe benefits of the open source solution outweigh the need to provision, monitor, and\nmaintain required VMs. Since cloud makes automation a first-class citizen, it’s possible\nto achieve significant levels of infrastructure stability with a very small team of engi-\nneers using open source solutions. \n When it comes to commercial or third-party SaaS offerings, we will only briefly\nmention existing products if they have a significant market share. There are dozens of\nproducts in the data management and ETL space out there, and the landscape of this\nindustry is changing very rapidly. We recommend commercial SaaS solutions when\nspecial functionality not available as PaaS or services is required or when open source\nalternatives are not mature. SaaS is also a good option when the company has already\nmade a significant investment in a particular product, including the teams using the\nproducts, and wants to protect that investment.\n The reality of a cloud data platform implementation is that you will most likely\nneed to mix and match several solutions. That’s why a loosely coupled layered archi-\ntecture is so important. In the following sections, we will provide an overview of tools\nMore control and flexibility\nMore work to support\nVery portable\nLess control and flexibility\nLess work to support\nNot very portable\nPaaS\nServerless\nThird-party\nSaaS\nOpen\nsource\nFigure 3.18\nKey trade-offs among data platform implementation component options\n\n\n62\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\navailable on AWS, Google Cloud, and Azure and will try to provide an evaluation of\nthese solutions against the desired properties that we have outlined in each section of\nthis chapter.\nNOTE\nToday we are seeing an increasing interest in multicloud solutions,\nwhere an organization decides to utilize components across different cloud\nvendors. Sometimes this is done to reduce the risk of vendor lock-in, but\nmore and more often it is done to utilize best-in-class products that each\ncloud has to offer. For example, we have seen cases where an organization\nwas performing the bulk of their analytics on AWS but decided to implement\ntheir machine learning use cases on Google Cloud. A layered cloud data plat-\nform design allows you to not only mix and match products and services\nwithin one provider, but also build successful multicloud solutions.\n3.3.1\nAWS\nAWS is the oldest player in the relatively new public cloud market and offers a wide selec-\ntion of products both in fully managed platform as a service and flexible infrastructure\nas a service (IaaS) space. In this section, we will go over specific AWS components that\ncan be used to implement various data platform layers, as shown in figure 3.19. \nBATCH DATA INGESTION \nFor batch data ingestion, AWS offers two fully managed services. AWS Glue can be\nused as your ingestion mechanism. Currently, Glue supports ingesting files only from\nAWS S3 storage or reading data from a database using a JDBC connection. External\nAPIs and NoSQL databases are not supported. \nFigure 3.19\nAWS services for cloud data platforms\nBatch \ndata\nStreaming\ndata\nAWS S3 & AWS Athena\nSlow storage/direct data lake access\nAWS Kinesis\nFast storage\nAWS Glue\nData\nCatalog\nOperational metadata\nAWS\nRedshift\ndata\nwarehouse\nAWS Kinesis Data Analytics\nReal-time processing and analytics\nDynamoDB\n+ API\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nAWS Glue\nETL tools overlay\nAWS Glue, AWS Step Functions\nOrchestration overlay\nAWS EMR\nBatch processing and analytics\nAWS Kinesis,\nAWS Kinesis\nFirehose\nIngestion\nAWS Glue,\nAWS DMS,\nAWS Lambda\n\n\n63\nMapping cloud data platform layers to specific tools\n Another option for batch data ingestion is AWS Database Migration Service. This\nservice allows you to perform historical and ongoing data migration from an on-\npremises relational database into different AWS destinations. While this service is\nprimarily targeted to migrating your operational databases into AWS-managed database\nservices, you can use it to ingest data into your data platform by specifying S3 as a\ndestination. Additionally, AWS DMS supports change data capture (CDC—defined by\nWikipedia as “a set of software design patterns used to determine and track the data that\nhas changed so that action can be taken using the changed data”) for ongoing data\ningestion from MS SQL Server, MySQL, and Oracle. Unless you require CDC, then it’s\npreferable to do all ingestion using Glue. This provides you with a single service where\nyou can monitor ingestion status, configure error handling, alerting, and so on. It also\nallows you to simplify job scheduling and coordination.\n To ingest data from sources that are currently not supported by AWS Glue or DMS,\nyou can implement and run your own ingestion code using a serverless AWS Lambda\nenvironment. This, of course, means that you will need to develop, test, and maintain\nthe ingestion code yourself. \nSTREAMING DATA INGESTION\nFor data sources that produce data one message at a time and require streaming\ningestion, AWS offers the AWS Kinesis service. Kinesis acts as a message bus by storing\nmessages from source systems such as CDC tools, clickstream collection systems such as\nSnowplow, or custom applications and allowing different consumers to read those\nmessages. Kinesis itself is just a fast data transport service. This means that you will need\nto write code that will actually publish messages from your data source into Kinesis\nyourself. There are existing prebuilt Kinesis connectors, but only for a few AWS-specific\ndata sources such as DynamoDB or Redshift. AWS also offers a number of prebuilt\nconsumers and transformations for Kinesis called Kinesis Firehose, which allows you to\nread messages from Kinesis, change the data format (for example, converting JSON\nmessages to Parquet), and save data to various destinations like S3 or Redshift. With\nFirehose, you can quickly configure a data ingestion pipeline that reads JSON messages\nfrom Kinesis, converts them to Parquet (more on this later), and saves them to S3 for\nfurther processing.\n If you need more advanced capabilities for handling incoming streaming data, you\ncan also use the AWS Glue streaming feature. Glue streaming runs on Spark Struc-\ntured Streaming and allows you to ingest and process data from Kinesis and Kafka. \n As an alternative to Kinesis, you can use AWS Managed Streaming for Apache\nKafka (MSK). MSK is a fully managed Kafka cluster that you can use as you would nor-\nmally use a standalone Kafka cluster. MSK provides full compatibility with the existing\nKafka producer and consumer libraries. This option is particularly useful if you are\nmigrating an existing real-time pipeline based on Kafka into AWS.\nDATA PLATFORM STORAGE \nAWS S3 is a great choice for implementing scalable and cost-efficient data platform stor-\nage. It offers unlimited scalability and high data durability guarantees. AWS has different\n\n\n64\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\ntiers for S3 with different data access latency and cost properties. For example, you can\nuse a slower, less expensive tier for your archival data or data that is not accessed often\nand a lower-latency, faster, more expensive tier when response time is important. \nBATCH DATA PROCESSING\nWe have mentioned before that Apache Spark is one of the most popular distributed\ndata processing frameworks. AWS offers a service called Elastic MapReduce (EMR)\nthat allows you (despite the name!) to execute not only legacy MapReduce jobs, but to\nalso run Apache Spark jobs. \n EMR was originally created to help AWS customers migrate their on-premises\nHadoop workloads into the cloud. It allows you to specify the number of machines\nand types of machines you want in your cluster, and then AWS takes care of provision-\ning and configuring them. EMR can work with both data that is stored locally on clus-\nter machines and data stored on S3. \n For our data platform architecture, we will only use data that is stored on S3. This\nallows us to keep the EMR cluster only for the duration of specific jobs, and then auto-\nmatically destroy the cluster once the job is completed. This type of elastic resource\nusage is one of the primary methods of cloud cost management.\nREAL-TIME DATA PROCESSING AND ANALYTICS\nWhile Apache Spark is great for batch and micro-batch data processing, some modern\napplications and analytics use cases require a more real-time approach. Real-time data\nanalytics means that messages are processed one at a time instead of batching them\nin larger groups. AWS Kinesis Data Analytics allows you to build real-time, data-\nprocessing applications that read data from AWS Kinesis. Kinesis Data Analytics also\nhas support for ad hoc querying of live data streams using SQL, which makes real-time\nanalytics available to people without programming experience. If you are using AWS\nMSK, then you can also use the Kafka Streams libraries to implement your real-time\nprocessing applications.\nCLOUD WAREHOUSE\nOne of the flagship products in the AWS data analytics space is AWS Redshift, the first\nwarehousing solution designed specifically for cloud. Redshift uses a massively parallel\nprocessing (MPP) architecture, meaning data is distributed among multiple nodes in\nthe Redshift cluster. This allows you to add more capacity to the cluster by adding\nmore nodes. Redshift is also tightly integrated with S3 and Kinesis, which makes it easy\nto load processed data in batch or streaming modes. \n Redshift Spectrum allows you to query external tables that are located on S3 with-\nout having to load them into the warehouse first. You still need to create the external\ntables in Redshift and define their schemas, but once that is done, you can query\nthose tables using the Spectrum engine, with most of the processing being done out-\nside of the data warehouse. Keep in mind that Spectrum performance is worse than\nnative Redshift tables because data needs to be moved from S3 into the Spectrum\nengine for processing each time you run a query.\n\n\n65\nMapping cloud data platform layers to specific tools\nDIRECT DATA PLATFORM ACCESS\nTo access data in the data platform directly, AWS has a service called Athena. Athena\nallows you to author a SQL query that will be executed in parallel on multiple\nmachines, reading data in S3 and returning the result back to the client. The main\nbenefit of Athena is that AWS provisions machines required for a specific query on the\nfly, meaning that you don’t need to maintain (and pay for) a fleet of permanent VMs.\nAthena charges per amount of data processed in each query, making it a cost-efficient\nsolution for ad hoc analytics, which tend to be done on less predictable schedules. \nETL OVERLAY AND METADATA REPOSITORY\nAWS Glue can be used not only to ingest data from different sources into your data\nplatform, but also to create and execute data transformation pipelines. Glue is actu-\nally based on Apache Spark and tries to simplify the process of developing Spark jobs.\nIt provides templates for most common data transformation pipelines on AWS and\nhas flexible schema support, tracking which data has been ingested for incremental\nloads. For example, Glue has prebuilt templates that allow you to transform a com-\nplex nested JSON structure into a set of relational tables for loading into AWS Red-\nshift. Glue can simplify the process of creating multiple Spark data pipelines, though\nat a cost—making the pipeline code less portable—since Glue uses Spark add-ons that\nare not available in standard Apache Spark distributions. \n Additionally, Glue maintains a Data Catalog that contains schemas for all the data\nsets that you have on S3 storage. Data Catalog uses an automatic discovery process,\nwhere AWS Glue periodically scans data you have on S3 to keep the catalog up to date.\nGlue also maintains a number of statistics about pipeline execution, such as number\nof rows and bytes processed, and so on. These metrics can be used for pipeline moni-\ntoring and troubleshooting. \nORCHESTRATION LAYER\nAWS Glue supports scheduling ETL jobs and allows you to configure dependencies\nbetween different jobs for more complex workflows. Glue scheduling capabilities are\nlimited to the jobs implemented in the Glue overlay. If you are using multiple services\nfor data ingestion and processing like DMS or lambda functions, then you can use\nAWS Step Functions to build workflows that span multiple services.\n Another AWS orchestration option is Data Pipeline. Data Pipeline is focused on\nscheduling and executing data transfers from one system to another. For example,\nyou can schedule periodic loads of files on S3 into Redshift or run a data transforma-\ntion job on EMR. Data Pipeline supports a limited number of systems it can copy data\nto and from. Data Pipeline somewhat overlaps with Glue in functionality but is\nfocused on predefined out-of-the-box actions that are not extendable.   \nDATA CONSUMERS\nAWS supports different types of data consumers. Applications that support SQL via\nJDBC/ODBC drivers can connect to either Redshift or Athena to execute SQL state-\nments on those systems. This includes tools such as Tableau, Looker, Excel, and other\noff-the-shelf tools. AWS also has web interfaces where data analysts can run ad hoc\nanalysis without having to install drives on their local system.\n\n\n66\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n If you need to provide data or the results of real-time analytics to applications that\nrequire low-latency response times, then using JDBC/ODBC connectors to the ware-\nhouse or data platform is not the ideal solution. These systems have inherent latency\nand are designed for large-scale data analysis as opposed to fast data access. For low-\nlatency use cases, you can deliver data from your real-time layer into an AWS\nkey/value store called DynamoDB. DynamoDB offers fast data access (especially when\ncombined with a caching mechanism such as DynamoDB Accelerator). Your applica-\ntions can access DynamoDB directly, or you can build an API layer if you want to con-\ntrol how data is exposed. Other options for fast data access include using managed\nrelational database services from Amazon, such as AWS RDS or AWS Aurora.\n3.3.2\nGoogle Cloud \nGoogle Cloud is a relatively new player in the public cloud space. Google Cloud offers\ncapabilities similar to AWS and Azure infrastructure-as-a-service components, includ-\ning virtual machines, networking, and storage. Google Cloud stands out when it\ncomes to data processing and analytics services. Many Google Cloud tools have been\ndeveloped and used internally at Google for many years. This means they have been\ntested at scale and under high load. On the other hand, these tools were designed to\nsolve Google-specific problems and are only now being retrofitted to match broader\nmarket needs. In this section, we will provide an overview of Google Cloud tools that\nyou can use to implement a cloud data platform as shown in figure 3.20.\nFigure 3.20\nGoogle Cloud services for a cloud data platform\nBatch \ndata\nStreaming\ndata\nGoogle Cloud Storage\nSlow storage/direct data lake access\nCloud Pub/Sub\nFast storage\nData Fusion,\nData Catalog\nOperational metadata\nBigQuery\ndata\nwarehouse\nCloud Dataflow\nReal-time processing and analytics\nCloud Bigtable \n+ API\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nCloud Data Fusion\nETL tools overlay\nGoogle Cloud Composer\nOrchestration overlay\nDataproc, Cloud Dataflow\nBatch processing and analytics\nCloud Pub/Sub,\nDataflow\nIngestion\nCloud Data Fusion,\nCloud\nFunctions,\nBigQuery\nData Transfer\nServices\n\n\n67\nMapping cloud data platform layers to specific tools\nBATCH DATA INGESTION\nGoogle Cloud offers several services to perform batch data ingestion—Cloud Data\nFusion, Cloud Functions, and BigQuery Data Transfer Service. \n Cloud Data Fusion is an ETL overlay service that allows users to construct data inges-\ntion and data processing pipelines using a UI editor and then execute these pipelines\nusing different data processing engines such as Dataproc and (in the future) Cloud\nDataflow. Cloud Data Fusion supports the ingestion of data from relational databases\nusing JDBC connectors as well as the ingestion of files from Google Cloud Storage.\nCloud Data Fusion also has connectors to ingest files from FTP and AWS S3. Unlike\nother managed ETL services, Cloud Data Fusion is based on an open source project\ncalled CDAP (https://cdap.io/). This means you can implement plugins for various\ndata sources yourself and not be constrained by what’s provided out of the box.\n As with AWS Lambda, Google Cloud also provides a serverless execution environ-\nment for custom code called Cloud Functions. Cloud Functions allow you to imple-\nment ingestions from sources that are not currently supported by Cloud Data Fusion\nor BigQuery Data Transfer Service. As Cloud Functions limit how long each function\ncan run before it’s terminated by Google Cloud, Cloud Functions isn’t well suited to\nlarge data ingestion use cases. At the time of writing, the time limit is 9 minutes. \n BigQuery Data Transfer Service is another viable choice for ingesting data into the\ndata warehouse part of your data platform, in this case, Google’s BigQuery. BigQuery\nData Transfer Service allows you to ingest data directly into BigQuery from selected\nGoogle-owned and -operated SaaS sources like Google Analytics, Google AdWords,\nYouTube statistics, and so on. BigQuery Data Transfer Service also supports ingesting\ndata from hundreds of other SaaS providers through a partnership with the data inte-\ngration SaaS company called Fivetran (https://fivetran.com/). Google Cloud offers\nservice provisioning for Fivetran connectors via the Google Cloud web console and\nunified billing, but the integration service itself is provided by Fivetran. \n The downside of using BigQuery Data Transfer Service is that data goes directly\ninto the warehouse, and as we have discussed previously, this limits you in the ways\ndata can be accessed and processed later. If your data analytics use cases require the\ningestion of data from many different SaaS providers, such as Google Analytics, Sales-\nforce, and others, then the simplicity associated with ingesting data using Transfer Ser-\nvice directly into the warehouse may outweigh the bigger architectural considerations. \n BigQuery Data Transfer Service is also expanding to support ingestion from rela-\ntional databases, similar to AWS’s Database Migration Services. Currently, only Tera-\ndata as a source RDBMS is supported. In this case, BigQuery Data Transfer Service\nactually saves data to Google Cloud Storage first, which makes it better suited for the\ncloud data platform architecture. \nSTREAMING DATA INGESTION\nCloud Pub/Sub service provides a fast message bus for data that needs to be ingested\nin a streaming fashion. Pub/Sub is similar in functionality to AWS Kinesis, but cur-\nrently supports larger message sizes (1 MB in AWS Kinesis and 10 MB in Pub/Sub).\n\n\n68\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nCloud Pub/Sub is just a message storage and delivery service—it doesn’t offer any pre-\nbuilt connectors or data transformations. You will need to develop the code that will\npublish and consume messages from Pub/Sub. Pub/Sub provides integrations with\nGoogle Cloud Dataflow for real-time data processing and analytics and with Cloud\nFunctions. \nDATA PLATFORM STORAGE\nGoogle Cloud Storage is a primary scalable and cost efficient storage offering on Goo-\ngle Cloud. Google Cloud Storage supports multiple storage tiers that vary in data\naccess speed and cost. Google Cloud Storage also integrates with many of the Google\nCloud data processing services such as Dataproc, Cloud Dataflow, and BigQuery.\nBATCH DATA PROCESSING\nGoogle Cloud offers two different ways to process data at scale in batch mode: Data-\nproc and Cloud Dataflow.\n Dataproc allows you to launch a fully configured Spark/Hadoop cluster and exe-\ncute Apache Spark jobs on it. These clusters don’t need to store any data locally and\nthey can be ephemeral—meaning that if all data is stored on Google Cloud Storage, a\nDataproc cluster is only required for the duration of the data transformation job, sav-\ning you money. \n Another Google Cloud service that can be used to process data from Cloud Stor-\nage is Cloud Dataflow. Cloud Dataflow is a fully managed execution environment for\nthe Apache Beam data processing framework. Cloud Dataflow can automatically\nadjust the compute resources required for your job depending on how much data you\nneed to process. Think of Apache Beam as an alternative to Apache Spark, and, like\nSpark, it’s an open source framework for distributed data processing. \n The main difference between Beam (Cloud Dataflow) and Spark (Dataproc) is\nthat Beam offers the same programming model for both batch and real-time data pro-\ncessing. On the other hand, Spark is a more mature technology that has been tested\nin multiple production environments. \nREAL-TIME DATA PROCESSING AND ANALYTICS\nThe primary cloud-native method of performing real-time data processing or analytics\non Google Cloud is to use Cloud Pub/Sub in conjunction with Apache Beam jobs run-\nning on the Google Cloud Dataflow service. Beam provides robust support for real-\ntime pipelines, including windows, triggers, dealing with late-arriving messages, and\nso on. Cloud Dataflow currently supports Java and Python Beam jobs. There’s no sup-\nport for SQL yet, but expect it to be added in future releases. \n As an alternative to Cloud Dataflow and Apache Beam for real-time processing and\nanalytics is Spark Streaming running on a Dataproc cluster. The Spark Streaming\napproach to real-time data processing is usually called micro-batching. Spark Stream-\ning doesn’t operate on one message at a time; instead, it combines incoming messages\ninto small groups (usually a few seconds long) and processes those micro-batches all at\nonce. Choosing between Apache Beam and Spark Streaming as your real-time data\n\n\n69\nMapping cloud data platform layers to specific tools\nprocessing engine on Google Cloud usually depends on whether you have existing\ninvestments into Apache Spark. This may include skills that your team has or an existing\ncode base. Google is making significant investments into its Cloud Dataflow + Beam\ncombo, so for new development this may be a better choice long term. Additionally,\nBeam provides richer semantics when it comes to real-time data processing. So if most\nof your pipelines are or will be real-time, then Beam would be a good choice.\nCLOUD WAREHOUSE\nBigQuery is Google’s offering in the managed cloud data warehouse space. It is a dis-\ntributed data warehouse with several unique properties—automatic compute capacity\nmanagement and robust support for complex data types. Where other cloud ware-\nhouses will require you to specify how many nodes you want in your cluster and what\ntypes of nodes you need up front, BigQuery manages the compute capacity for you\nautomatically. For each query you issue, BigQuery will decide how much processing\npower you need and allocate only the resources required. BigQuery offers a per-query\nbilling model, where you only pay for the volume of data each query has to process.\nThis works really well for low-volume analytics workloads or ad hoc data exploration\nuse cases, but it can make it difficult to predict or estimate BigQuery costs. BigQuery\nalso has robust support for complex data types such as arrays and nested data struc-\ntures, making it a great choice if your data sources are JSON-based. \nDIRECT DATA PLATFORM ACCESS\nThere is currently no dedicated service from Google Cloud to directly access data in\nthe lake. BigQuery supports external tables, which allows you to create tables that are\nphysically stored on Google Cloud Storage, without having to load the data into Big-\nQuery first. BigQuery also allows you to create temporary external tables that only\nexist for the duration of your session. Temporary external tables are well suited for ad-\nhoc data exploration on the lake. The limitations of using BigQuery as a data platform\naccess mechanism is that currently you need to provide a schema for each external\ntable. This can be a big barrier for ad hoc analysis, since the schema is often not\nknown at this stage. An alternative way to work with data in Google Cloud Storage\ndirectly is to provision a temporary Dataproc cluster and use Spark SQL to query data\nin the lake. Spark can infer schema for most of the popular file types automatically,\nmaking data discovery easier. \nETL OVERLAY AND METADATA REPOSITORY\nCloud Data Fusion is a managed ETL service available on Google Cloud. Cloud Data\nFusion allows data engineers to construct data processing and analytics pipelines\nusing a UI editor and then have those pipelines translated into one of the data pro-\ncessing frameworks to be executed at scale on Google Cloud. Currently, only Apache\nSpark running on Dataproc is supported, with Apache Beam planned for future\nreleases. The main benefit of using an ETL overlay such as Cloud Data Fusion is that it\nprovides mechanisms for people to search for existing data sets and immediately see\nwhich pipelines and transformations affect them. This allows you to perform a quick\n\n\n70\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nimpact analysis to understand what data will be affected if a given pipeline is changed.\nCloud Data Fusion also tracks a number of statistics about pipeline execution, such as\nthe number of rows processed, timing of different stages, and so on. This information\ncan be used for monitoring and debugging purposes. \nORCHESTRATION LAYER\nCloud Composer is a fully managed service for complex job orchestration. It is based\non a popular Apache Airflow project and can execute existing Airflow jobs without\nany modifications. Airflow allows you to author jobs that consist of multiple steps. For\nexample, steps could be these: read a file from Google Cloud Storage, launch a Cloud\nDataflow job to process it, and send a notification upon success or failure. Airflow also\nsupports dependencies between jobs and allows you to rerun either separate steps or\nfull jobs on demand. Cloud Composer makes managing an Airflow environment eas-\nier because provisioning the required virtual machines, installing and configuring\nsoftware, and so on, are part of the service.\nDATA CONSUMERS\nBigQuery doesn’t currently have native support for JDBC/ODBC drivers, but these\ndrivers are available for free from a third party called Simba Technologies. BigQuery\nnative data access is all done via a REST API because BigQuery acts more like a global\nSaaS than a typical database. JDBC/ODBC drivers from Simba act as a bridge between\nthe JDBC/ODBC API and the BigQuery REST API. \n As with any translation from one protocol to another, there are limitations, primarily\naround response latency and total throughput. These drivers may not be suitable for\napplications that require low-latency response or that need to extract large (tens of GBs)\nof data from BigQuery. Fortunately, a number of existing BI and reporting tools are\nimplementing native BigQuery support, eliminating the need for a JDBC/ODBC driver.\n You should check that the reporting or BI tools you plan to use with your Google\nCloud data platform offer support for BigQuery. When it comes to data consumers\nwho need real-time data access, Google Cloud offers a fast key/value store called\nCloud Bigtable that can be used as a caching mechanism. As with AWS, you will need\nto implement and maintain application code that will load results from your real-time\npipelines into Cloud Bigtable and then either build a custom API layer on top of\nCloud Bigtable or use the Cloud Bigtable API directly in your applications.  \n3.3.3\nAzure\nAzure is a public cloud service from Microsoft, who have a long and very successful\nhistory of implementing data-related products associated with their flagship MS SQL\nServer RDBMS. Azure offers a range of services based on MS SQL Server, but they also\nhave a set of completely new cloud-native products, including Data Factory, Cosmos\nDB, and others. In this section, we will provide an overview of Azure tools that you can\nuse to implement a cloud data platform as shown in figure 3.21.\n\n\n71\nMapping cloud data platform layers to specific tools\nBATCH DATA INGESTION\nAzure Data Factory is an ETL overlay service that supports batch ingestion. Currently,\nData Factory supports a wide range of RDBMS products, ingesting flat files from FTP,\nAzure Blob Storage, S3, and Google Cloud Storage; NoSQL databases such as Cassan-\ndra and MongoDB; and a growing number of connectors for external SaaS platforms\nsuch as Salesforce and Marketo. Data Factory also integrates with most Azure data\nproducts like Cosmos DB, Azure SQL Database, and many others. Compared to AWS\nand Google ETL overlays, Data Factory offers the biggest library of connectors and\nsupports some extensibility in the form of a generic HTTP connector. You can use this\nconnector to implement your own ingestion pipelines for REST APIs (internal or\nexternal) that may not be currently supported out of the box. As with other cloud pro-\nviders, Azure has support for a serverless execution environment called Azure Func-\ntions that can be used to implement your own ingestion mechanisms in one of the two\ncurrently supported languages: Java and Python.\nSTREAMING DATA INGESTION\nAzure Event Hubs is a service that allows you to send and receive data from streaming\nsources. It is similar in functionality to AWS Kinesis and Google Cloud Pub/Sub. A\nunique feature of Event Hubs is its compatibility with Apache Kafka API. We will talk\nabout Kafka later in this chapter, but API compatibility means that if you have existing\ninvestments or skills with Kafka, it will be easier to migrate to Event Hubs. Event Hubs\nCapture is a supporting service to Event Hubs that allows you to save messages from\nEvents Hubs into various Azure services, such as Azure Blob Storage or Azure Synapse. \nFigure 3.21\nAzure services for a cloud data platform\nBatch \ndata\nStreaming\ndata\nAzure Data Lake Storage\nSlow storage/direct data lake access\nAzure Event Hubs\nFast storage\nAzure Data Factory,\nAzure Data Catalog\nOperational metadata\nAzure\nSynapse\ndata\nwarehouse\nAzure Stream Analytics\nReal-time processing and analytics\nCosmos\nDB +API\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nData Factory\nETL tools overlay\nAzure Data Factory\nOrchestration overlay\nAzure Databricks\nBatch processing and analytics\nAzure Event\nHubs\nIngestion\nAzure Data\nFactory,\nAzure\nFunctions\n\n\n72\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nDATA PLATFORM STORAGE\nSimilar to other major cloud providers, Azure offers a scalable and cost-efficient data\nstorage service called Azure Blob Storage. On top of this service, Azure has imple-\nmented a new service offering called Azure Data Lake Storage. This service provides\nseveral improvements over regular Azure Blob Storage, especially when it comes to\nperformance of large-scale data processing jobs. \nBATCH DATA PROCESSING\nAzure doesn’t offer services like AWS EMR or Google Cloud Dataproc. Instead, they\nhave partnered with a company called Databricks to offer a flexible environment to\nexecute Apache Spark jobs. Databricks was founded by the original creators of Apache\nSpark and currently offers its managed Spark environments on both Azure and AWS.\nAzure Databricks provides seamless integration with the rest of the Azure ecosystem. It\nhas connectors to work with data in Azure Data Lake Storage, read and write data to\nAzure Warehouse, etc. \nREAL-TIME DATA PROCESSING AND ANALYTICS\nAzure Stream Analytics is a service that allows you to perform data transformations\nand analysis on messages from Event Hubs. Stream Analytics uses a SQL-like language\nto do this. You can save the results of the Stream Analytics job to a number of sup-\nported destinations. Currently this includes a new Event Hubs destination, Azure Data\nLake Storage, a SQL database, and Cosmos DB. Since Event Hubs is compatible with\nKafka, you can also implement your own Kafka Streams real-time pipelines or migrate\nexisting ones. \nCLOUD DATA WAREHOUSE\nAzure Synapse is a scalable, cloud-native warehousing service. It’s built on top of\nproven MS SQL Server technology and offers several compelling features. First, Azure\nSynapse separates storage from compute resources. When creating a new Azure Syn-\napse warehouse, you need only specify how much computational capacity you will\nneed, as storage is adjusted automatically based on your needs. This means that you\ncan scale your Azure Synapse data warehouse compute resources up and down on\ndemand while keeping the data on the storage intact. For example, you can switch to\na lower computer tier during off hours or on weekends to reduce cost. The Azure Syn-\napse model is a hybrid between AWS Redshift, where data has to be stored locally on\nthe cluster machines, and BigQuery, where Google Cloud manages both compute and\nstorage capacity for you. In Azure Synapse, you have full control over how much com-\npute you need, and storage is managed for you. Since Azure Synapse is built on top of\nrelational database technology, it offers robust SQL support and is compatible with all\nthe tools that have JDBC/ODBC driver support. With the recent updates, Azure Syn-\napse also supports storing and processing JSON data. \nDIRECT DATA PLATFORM ACCESS\nA recommended way to access and process data stored in Azure Blob Storage is to use\nan Azure Databricks platform. Databricks is a third-party company that provides a data\n\n\n73\nMapping cloud data platform layers to specific tools\nprocessing platform based on Apache Spark. Databricks significantly simplifies creat-\ning and managing Spark clusters and focuses on providing a collaborative environ-\nment for multiple teams to work on the same data sets. You can use Spark SQL to issue\nSQL queries to work directly on the data in Blob Storage, or you can use native Spark\nAPIs. Databricks allows you to easily create multiple independent clusters that have\naccess to the same data sets. This simplifies resource management and allows fine tun-\ning your processing clusters to fit a specific task. \n What’s unique about the Azure Databricks service is that it’s natively integrated\ninto the Azure platform and looks and feels just like another Azure service. There are\nAzure APIs that allow you to create and manage Databricks workspaces, and Azure\nprovides out-of-the-box integration with other data services like Data Factory, Azure\nSynapse, and others.\nETL OVERLAY AND METADATA REPOSITORY\nAzure Data Factory is an ETL service that you can use to ingest data from sources, pro-\ncess it, and then save data to various destinations. Data Factory provides a UI where\nyou can construct, execute, and monitor your pipeline, but it also has very robust API\nsupport, which makes it possible to automate pipeline creation and deployment. This\nis an important feature that allows your ETL pipeline to be made a part of the contin-\nuous integration/continuous deployment (CI/CD) process. \n Data Factory currently has limited support for data transformations that work out\nof the box—mostly converting files from one format to another. To allow users to per-\nform more complex transformations, Data Factory provides hooks into Azure Data-\nbricks. This way you can use Data Factory to ingest data from the source and then\nexecute a complex Spark transformation job using an Azure Databricks hook. When it\ncomes to pipeline-specific metadata, Azure Data Factory captures and allows you to\nmonitor metrics like pipeline success/failure, duration, and a few others. Currently,\nthe number of available metrics is quite limited and doesn’t allow you to capture\nevents like schema changes, data volume changes, and so on.\n Google Cloud Data Catalog is a separate service that is more focused on business\nmetadata (data that adds business context to other data, providing information\nauthored by business people and/or used by business people) and data discovery\nfunctionality. It integrates with various Azure and external sources and allows you to\ncreate a searchable catalog of existing data assets. Additionally, it can perform basic\ndata profiling (total number of rows, number of distinct or empty values in each col-\numn, and so on) for sources that support SQL access. \nORCHESTRATION LAYER\nAzure Data Factory provides pipeline-scheduling capabilities and also supports com-\nplex pipeline dependencies. For example, you can create a chain of pipelines, where\nas one pipeline finishes successfully, it triggers another pipeline, and so on. \n\n\n74\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nDATA CONSUMERS\nAzure Synapse provides full support for SQL consumers, including JDBC/ODBC driv-\ners. This means you can easily connect your favorite reporting client, BI, or SQL to it.\nAzure Databricks also offers JDBC/ODBC connectivity for reporting tools, but to use it,\nyou must ensure that there is an Azure Databricks cluster capable of processing incom-\ning queries that is always on. This approach may be expensive. For application consum-\ners who require more real-time direct access to the data platform, Azure offers a fast\ndocument-oriented database called Cosmos DB. You can save the results of your real-\ntime analytics there and then either connect your applications directly to Cosmos DB\nusing client libraries or build an API layer around it for more controlled data access.\n3.4\nOpen source and commercial alternatives \nThere are scenarios where services provided by public cloud vendors will not meet\nyour specific requirements. Over the years, we have seen the following cases where a\ncloud-native services may not be the best fit:\nFunctionality limitations—Cloud-native services are evolving quickly, but some of\nthem are really new and may not have all the features you need.\nCost—Depending on your data volumes, the cloud vendor pricing model may\nresult in costs that are not reasonable for your use case.\nPortability—There is an increasing demand for organizations to implement\nmulticloud solutions, either to avoid vendor lock-in or to utilize the best ser-\nvices from each provider.\nIn this section, we will give you an overview of some of the open source or commercial\nsoftware alternatives. Not all layers of a cloud data platform have a valid open source\nor commercial software alternative. For example, deploying and managing your own\ndistributed warehouse in the cloud will always be either cost prohibitive or not perfor-\nmant enough (or both), even though open source solutions like Apache Druid exist.\n3.4.1\nBatch data ingestion\nData ingestion is a major component of any data platform, whether it’s a cloud data\nplatform or a traditional on-premises warehouse. That’s why there are lots of tools,\nboth open source and third-party commercial offerings, that can play this role. \n Apache NiFi is one of the popular open source solutions that allows you to connect\nto various data sources and bring data into your cloud data platform. NiFi uses a plug-\ngable architecture that allows you to create new connectors using Java, but also comes\nwith a large library of existing connectors. Talend is another popular ETL solution\nthat you can use to implement a data ingestion layer. Talend uses an open-core model,\nwhere basic functionality is free and open source, but enterprise-level features require\na commercial license. Talend is not just an ingestion tool, and it makes more sense to\nuse its whole ecosystem of solutions that include data profiling, scheduling, and so on.\n\n\n75\nOpen source and commercial alternatives\n There are also many existing third-party SaaS solutions that specialize in bringing\ndata from various sources into your cloud environment. Alooma (acquired by Google\nin 2019) and Fivetran are two examples of such services. These SaaS services usually\nprovide a very rich set of connectors and additional features such as monitoring or\nlightweight transformation of data. The limitations of using SaaS providers for data\ningestion include having to send your data through a third party, which may not\nalways be acceptable from a security perspective. Also, these tools specialize in writing\ndata directly into the warehouse, making it challenging to integrate them into a flexi-\nble data platform architecture.\n3.4.2\nStreaming data ingestion and real-time analytics\nWe often see fast message store, streaming data ingestion, and real-time data process-\ning implemented using open source solutions instead of cloud-native services. This is\nbecause Apache Kafka is a leading open source solution in this space. Kafka offers a\nfast message bus for your streaming data sources, but also has a Kafka Connect com-\nponent that allows you to easily ingest data from various sources into Kafka. It also\ncomes with support for Kafka Streams—a way for you to implement real-time data pro-\ncessing or analytics applications. The reasons to choose Kafka instead of a cloud\nnative solution are performance, feature set richness, and existing expertise. If you\nhave existing investments into Kafka or have very specific performance requirements,\nyou may consider implementing your own streaming solution in the cloud using this\ntechnology. The downside, as with any open source solution, is that you will need to\ninvest in managing your Kafka cluster.\n3.4.3\nOrchestration layer\nApache Airflow is a popular open source job orchestration tool. It allows you to con-\nstruct complex job dependencies, and provides logging, alerting, and retry mecha-\nnisms. The Google Cloud Composer orchestration service is based on Apache Airflow\ntechnology. The benefit of using Airflow over a cloud service is flexibility because Air-\nflow job configurations are created using the Python programming language, which\nallows you to create dynamic job definitions. For example, you can create an Airflow\njob that can change its behavior based on external configuration or reach out to an\nexternal service to fetch configuration parameters.\nSummary\nA good modern data platform architecture will have six layers—a data ingestion\nlayer, a data processing layer, a metadata layer, a serving layer, and two overlay\nlayers—the orchestration and ETL layers—which coordinate work across multi-\nple layers.\nEach layer in a data platform architecture plays a very specific role, and it’s a\ngood idea from an architecture perspective for these layers to be “loosely cou-\npled,” or separate layers that communicate through a well-defined interface and\n\n\n76\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\ndon’t depend on the internal implementation of a specific layer. This will allow\nyou to more easily mix and match different cloud services or tools to achieve\nyour goals and respond to changes in available services while minimizing data\nplatform impact.\nWhile the data processing world is moving to data streaming and real-time solu-\ntions, it’s good architectural thinking to build your data ingestion layer to sup-\nport both batch and streaming ingestion as a first-class citizens. This way, you\nwill always be able to ingest any data source, no matter where it comes from. \nThe data ingestion layer should be able to securely connect to a wide variety of\ndata sources, in streaming and/or batch modes, transfer data from the source\nto the data platform without applying significant changes to the data itself or\nit’s format, and register statistics and ingestion status in the metadata reposi-\ntory. Focus on ensuring your data ingestion layer has a pluggable architecture,\nscalability, high availability, and observability. \nThe storage layer in the data platform must store data for both the long term\nand short term and make data available for consumption either in batch or\nstreaming modes. Focus on designing your storage layer to be reliable, scalable,\nperformant, and cost efficient. \nThe processing layer is the heart of the data platform implementation and\nshould be able to read data in batch or streaming modes from storage, apply\nvarious types of business logic, and provide a way for data analysts and data sci-\nentists to work with data in the data platform in an interactive fashion.\nDesign your data processing layer to also provide direct data platform access.\nConsider using a distributed SQL engine that can work directly on files in the\ndata platform, a distributed data processing engine API, or directly reading files\nfrom the lake. \nA data platform metadata store stores information about the activity status of\ndifferent data platform layers and provides an interface for layers to fetch, add,\nand update metadata in the store. It’s very important for automation, monitor-\ning and alerting, and developer productivity. It also allows us to decouple differ-\nent layers from each other, reducing the complexities associated with\ninterdependencies. \nTechnical metadata management in the data platform is a relatively new topic.\nThere are few existing solutions that can fulfill all the tasks needed, so as it\nstands today, you will likely need a combination of different tools and services to\nimplement a fully functional technical metadata layer. \nThe serving layer delivers the output of analytics processing to the various data\nconsumers. Business users typically use SQL-based tools to access data via the\ndata warehouse. Programmatic data consumers require a dedicated API to con-\nsume data from the lake, usually in real-time.\n\n\n77\nExercise answers\nThe orchestration layer coordinates multiple data processing jobs according to\nthe dependency graph and handles job failures and retries. It should be scal-\nable, highly available, maintainable, and transparent.\nAn ETL overlay is a product or a suite of products whose main purpose is to\nmake the implementation and maintenance of cloud data pipelines easier.\nThese products absorb some of the responsibilities of the various data platform\narchitecture layers and provide a simplified mechanism to develop and manage\nspecific implementations. \nETL overlay tools work across the various layers as they are responsible for add-\ning and configuring data ingestions from multiple sources (ingestion layer),\ncreating data processing pipelines (processing layer), storing some of the meta-\ndata about the pipelines (metadata layer), and coordinating multiple jobs\n(orchestration layer).\nThe reality of a cloud data platform implementation is that you will most likely\nneed to mix and match several tools, and turning to PaaS services in your lay-\nered architecture will bring you a balance of agility and flexibility with mini-\nmum maintenance. Picking which tools to use requires you to evaluate your\nspecific requirements against features, ease of use, scalability, and more.\nIn an AWS cloud, consider using Glue, DMS, Kinesis, Kinesis Firehose, Lambda,\nGoogle Cloud Data Catalog, Step Functions, S3, EMR, Kinesis Data Analytics,\nRedshift, Athena, and DynamoDB. \nIn a Google cloud, consider using Cloud Pub/Sub, Cloud Dataflow, Cloud Data\nFusion, Cloud Functions, BigQuery, BigQuery Data Transfer Service, Cloud\nComposer, Dataproc, Cloud Catalog, and Cloud Storage.\nIn an Azure cloud, consider using Data Factory, Event Hubs, Azure Functions,\nDatabricks, Data Catalog, Stream Analytics, Data Lake Analytics, Data Lake Stor-\nage, Azure Synapse, and Cosmos DB.\n3.5\nExercise answers\nExercise 3.1:\n 3—Some data sources don’t support real-time ingestion.\nExercise 3.2:\n 2—To support both batch and real-time processing.\nExercise 3.3:\n 1—To support consumers with different needs and requirements.\nExercise 3.4:\n 3—To manage dependencies between different data processing jobs.\n\n\n78\nGetting data\n into the platform\nIf you’ve read the chapters up to this point, you’re able to architect a good, layered\ndata lake. Now it’s time to start diving into a few of these layers in much greater\ndetail.\nThis chapter covers\nUnderstanding databases, files, APIs, and streams \nIngesting data from RDBMSs using SQL versus \nchange data capture\nParsing and ingesting data from various file formats\nDeveloping strategies to deal with source schema \nchanges\nDesigning an ingestion pipeline to handle the \nchallenges of data streams\nBuilding an ingestion pipeline for SaaS data\nImplementing quality control and monitoring in your \ningestion pipeline\nDiscussing network and security considerations for \ncloud data ingestion\n\n\n79\nDatabases, files, APIs, and streams\n In this chapter, we’ll focus on the ingestion layer. Before you can start using your\ncloud data platform to produce outcomes using traditional or advanced analytics or\nreports, you will need to populate it with data. One of the key characteristics of a data\nplatform is its ability to ingest and store data of all types in its native format. This vari-\nety does present challenges, so we’ll walk through the most popular data types—\nRDBMs, files, APIs, and streams—and help you understand how they are different\nfrom the perspective of ingestion. We’ll also touch on the networking and security\nconsiderations that apply regardless of the data source to be ingested.\n By the end of the chapter, you’ll be able to choose the most appropriate ingestion\nmethod for the various types of data sources and use cases and design a robust and\nappropriate ingestion process for each. You’ll also be able to guide developers in iden-\ntifying and dealing with common ingestion challenges, including mapping data types,\nautomation, and data volatility. Finally, you’ll be able to anticipate and work around\nthe most common gotchas when ingesting data.\n4.1\nDatabases, files, APIs, and streams\nThe importance and complexity of establishing reliable data ingestion pipelines are\noften overlooked. \n The complexity comes from the fact that a modern data platform must support\ningestion from a variety of different data sources (remember the three V’s from chap-\nter 1?), often at high velocities and in a consistent, uniform way. In this chapter, we will\ndo an overview of the most common types of data sources we see today and provide\nsome guidance on how to establish a robust ingestion process for each of them. We will\nspecifically focus on databases (relational and NoSQL), files, APIs, and streams. \nNOTE\nIf you are experienced in the use of traditional data warehouses, this\nchapter might cover material you already know, but because data platforms\nare increasingly being designed by data engineers who don’t necessarily come\nfrom a traditional data warehousing world, we felt it was important to include\nthis detail for their benefit.\nEach of these source types have their own unique characteristics. For example, rela-\ntional databases always have data types associated with each column and are organized\ninto tables. Flat CSV files also have a table-like format, but have no data type informa-\ntion attached to columns, so you need to figure out how to deal with this in your plat-\nform. Figure 4.1 illustrates the most common considerations for each data source type.\n This next section will walk through these data sources, exploring the attributes of\neach that will most impact your ability to ingest them into your data platform and\nsome key things to keep in mind as you prepare to ingest data from these sources. In\nthe following sections of this chapter, we will do a deeper dive into each of these data\nsource types and outline specific guidelines and gotchas about how to design and\nimplement a data platform ingestion layer for each of them.\n\n\n80\nCHAPTER 4\nGetting data into the platform\n4.1.1\nRelational databases\nRelational databases (referred to as RDBMSs) are one of the most common data\nsources to be integrated into a data platform. Today, RDBMSs power most of the\napplications responsible for business operations and contain valuable information\nabout core business transactions such as payments, orders, bookings, etc. Data in\nRDBMSs is organized into tables, each containing one or more columns. Columns\nhave clear data types (such as strings, integers, dates, etc.) associated with them, and\nRDBMSs takes care of making sure that data with the wrong type doesn’t get inserted\ninto the database. Data in relational databases is typically highly normalized, meaning\ndata entities are split into multiple tables that can be joined using a common key at\nquery time. As such, it’s not unusual to have databases with hundreds or even\nthousands of different tables in them. Finally, because of the operational nature of\nthese databases, data in thesm changes all the time: new orders arrive, get changed,\ncanceled, or delivered.\n What do these RDBMS properties mean when it comes to implementing a data\ningestion process into a common data platform? Here are some of the most important\nconsiderations:\nMapping data types—Column data types from the source database must be\nmapped into the destination cloud warehouse. Unfortunately, each RDBMS\nand cloud vendor has their own set of supported data types. While many types\noverlap, such as strings, integers, and dates, there are always data types that are\neither unique to a particular vendor or behave differently in different data-\nbases. For example, TIMESTAMP types can have different precisions (microsec-\nond, nanosecond, etc.), while DATE types can expect dates to be formatted a\ncertain way, etc.\nAutomation—Since RDBMSs are often composed of hundreds of different tables,\nyour ingestion process must be highly configurable and automated. No one will\nCloud data platform\nRDBMS\nFiles\nSaaS\nAPIs\nData type mapping,\nhundreds of tables,\nneed to capture state\nchanges, volatile data\nMultiple file\nformats, no schema\nenforcement, no data\ntype information\nNo single API\nstandard, no data\ntype information,\ncontinuous API\nchanges\nNo single message\nformat, no data\ntype information,\npotential duplicates,\nhigh volume\nFigure 4.1\nCommon data source types\n\n\n81\nDatabases, files, APIs, and streams\nhave time to manually configure ingestions for 600 different tables. Even if you\nhave time to do it manually, you will most likely make mistakes along the way and\nend up with an ingestion process that is brittle and nonreproducible.\nVolatility—Data in RDBMSs is typically highly volatile—businesses are never\nstatic and a lot of the business operations are captured in the database. For\nexample, if you are dealing with a large e-commerce site, you will see hundreds\nor thousands of orders being placed, processed, edited, or cancelled every sec-\nond. This in turn will result in data changes that affect dozens of tables. Your\ningestion process must be able to deal with constantly changing data, capture\ndata state at a given point in time, and deliver it into the data platform, where it\ncan be processed and analyzed further.\n4.1.2\nFiles\nFiles are another very common data source for your data platform. These are usually\ntext or binary files that are delivered to the target system through FTP protocol or\nplaced on cloud storage such as Google Cloud Storage (GCS), S3, or Azure Blob Stor-\nage, where ingestion processes pick them up. It’s worth mentioning that we have seen\nsome more exotic ways of delivering data into a data platform, such as sending files as\nattachments to a dedicated email address. We strongly advise against such an\napproach for security and reliability reasons. \n Despite their simplicity, files are actually really tricky to deal with from an ingestion\nstandpoint. First of all, they come in many different formats. The most popular text\nformats are CSV, JSON, and XML. Binary formats are slightly less common and may\ninclude Avro or Protocol Buffers (protobuf). Text file formats don’t include any col-\numn type information and don’t usually enforce any constraints on the data or file\nstructure itself. This means there is no guarantee that files you receive will follow a\nconsistent structure even for the same source. Your ingestion process must therefore\nbe very resilient to change and must be able to deal with many different edge cases. \n Here are some considerations when building a file-based ingestion process:\nParsing different file formats—You will need to parse different file formats: CSV,\nJSON, XML, Avro, etc. In the case of text file formats, there may be no guaran-\ntee that whoever produced the file followed the same conventions that your\nparser expects.\nDealing with schema changes—Unlike with RDBMSs, adding a new column to a\nCSV file or a new attribute to a JSON document is easy for data producers, and\nas such our experience shows that schema changes cases are very common in\nfile-based data sources. Your ingestion process needs to be able to deal with this. \nSnapshots and multiple files—As opposed to highly volatile RDBMS data, files usu-\nally represent a snapshot of some data set in time. A typical flow for a file-based\ndata source is that data is extracted from some other system, saved as a text file,\nand then delivered to a destination. A single file can either represent a full\nsnapshot of the source system (e.g., all orders) or a data increment (e.g., new\n\n\n82\nCHAPTER 4\nGetting data into the platform\norders since yesterday). Also any given ingestion batch can either be delivered\nas a single file or multiple files. Your ingestion process must take all of these\noptions into account.\n4.1.3\nSaaS data via API\nIt’s almost impossible to find a business that doesn’t rely on SaaS products for some of\ntheir business operations. SaaS products such as Salesforce or Marketo host some of a\nbusiness’s most important data sets. Combining customer data (Salesforce), and mar-\nketing campaign data (Marketo) with the information about business transactions\n(RDBMS) is a common desired outcome of a unified data platform. Most SaaS compa-\nnies allow you to extract your data using a REST API. Usually this is an HTTP request\nthat returns some data in a JSON format. You may also have an option to download\ndata as a flat CSV file, but since APIs provide more flexibility in the way data can be\naccessed or filtered out, they are a preferred way to extract data from SaaS systems. \n There are multiple challenges associated with using SaaS APIs. Each SaaS provider\ndevelops their own unique way of exposing data to external data consumers. For\nexample, one provider may one have a single API endpoint that provides you with all\nthe data you have in the system. Another provider may have an API endpoint for each\nobject in their system, such as Customer, Contract, Vendor, etc. Some providers allow\nyou to fetch data for a specified time frame, while others may either provide you with\na full snapshot of data or only give you the last couple of days worth of data. This list\ngoes on and on. This lack of standardization for data access and the resulting variety\nof API access methods makes ingesting data from SaaS into a data platform a challeng-\ning task. Here are some of the considerations you need to keep in mind when design-\ning an ingestion process for SaaS:\nIf your organization uses multiple SaaS solutions, then you need to implement\ndifferent data ingestion pipelines for each of those and constantly update them\nto keep up with the provider-introduced changes. \nVery few SaaS provider APIs have any data type information available. From the\nperspective of data types, ingesting data from APIs is similar to dealing with\nJSON files—it’s up to the data pipeline implementation to do the necessary\ndata type and schema validations. \nThere is also no standard for full versus incremental data load when it comes to\nSaaS APIs. You will need to adjust your pipeline for each provider.\n4.1.4\nStreams\nA more recent addition to the list of data source types that a data platform architect\nmust take into account are streams. Data streams usually represent events that happen\nat a given point in time. For example, a popular data stream for web applications is\nclickstream data, where each time a visitor clicks on an object on a web page, this\naction is captured as an event with various different attributes (user IP address,\nbrowser type, etc.). A series of these actions combined with the time of the event form\n",
      "page_number": 60
    },
    {
      "number": 4,
      "title": "Getting data into the platform",
      "start_page": 102,
      "end_page": 149,
      "detection_method": "regex_chapter",
      "content": "83\nIngesting data from relational databases\na clickstream, which can represent many different activities and business operations.\nPlacing and removing items in a shopping cart of an e-commerce site can be repre-\nsented as a sequence of corresponding events—or as a stream—as can depositing and\nwithdrawing funds from a bank account. While RDBMSs represent the current state of\nthe system—for example, items in the shopping cart at the time of the checkout or\ncurrent bank account balance at a point in time—event streams represent the\nsequence of actions that occurred prior to arriving at this current state. This informa-\ntion is extremely valuable for analytic purposes because it can provide insights into\nuser motivation, system optimization opportunities, and so on.\n The concept of event streams is not new, but several technological advances now\nmake capturing and storing events scalable, reliable, and cost effective. Apache Kafka\n(an open source stream-processing software platform developed by LinkedIn,\ndonated to Apache Software Foundation, and written in Scala and Java) is the most\npopular open source project created specifically for this purpose. Cloud-native\nstreaming services are available from AWS (Kinesis), Google Cloud Pub/Sub, and\nAzure (Event Hubs) for integrating streaming data from cloud-native applications.\nConsiderations for ingestion pipeline design are the same across both open source\nand cloud service options and include\nIn data streams systems, there is a message format restriction. All messages are\nstored as an array of bytes and can encode data as a JSON document, an Avro\nmessage, or any other format. Your ingestion pipeline must be able to decode\nthe messages back into a consumable format. \nTo deliver data reliably and at scale, streaming data systems allow the same mes-\nsages to be consumed multiple times. This means that the data ingestion pipe-\nline must be able to deal efficiently with duplicate data.\nMessages in streaming data systems are immutable. Once written by the pro-\nducer, they cannot be changed, but a new version of the same message can be\ndelivered later, so your data platform may need to be able to resolve multiple\nversions of the same message.\nStreaming data is typically high-volume data. Your ingestion pipeline must be\nable to scale accordingly. \nIn the following sections of this chapter, we will do a deeper dive into each of these\ndata source types and outline specific guidelines and gotchas about how to design and\nimplement a data platform ingestion layer for each of them.\n4.2\nIngesting data from relational databases\nIn this section, we will discuss different ways to ingest data from RDBMSs. There are\ntwo primary ways to establish an ongoing ingestion process from databases: using a\nSQL interface and using change data capture (CDC) techniques. We will start with\nusing a SQL interface, since it’s a more common method of ingesting data, and we\nwill discuss two methods of ingestion using a SQL interface: full-table versus incre-\nmental table ingestion. \n\n\n84\nCHAPTER 4\nGetting data into the platform\n4.2.1\nIngesting data from RDBMSs using a SQL interface \nAll relational databases support SQL. The most basic way to extract some data from a\ndatabase is to run a SQL query that looks something like this:\nSELECT * FROM some_table;\nThis query will give you all columns and all rows from a given table. You can then save\nthese results into a flat file (CSV or Avro), for example, in a landing area of your data\nplatform, and there you have a basic RDBMS ingestion process. \n Of course, there is much more involved in making this ingestion process work con-\nsistently and well. To explain, let’s take a look at figure 4.2.\nThere are several important things we need to figure out before we can implement an\ningestion process like this. First of all, we need an application of some sort to actually\nrun those SQL queries against RDBMSs and then save the result to the data platform.\nIn chapter 3 we provided some examples of different cloud-native or open source\ntools that can be used for ingestion. For this specific use case, ingesting data from\nRDBMSs, it’s important that the tool is capable of\nExecuting a SQL statement against different kinds of RDBMSs\nSaving results in one or more formats into a cloud storage\nAllowing specifying filtering conditions and other parameters for the SQL\nquery\nfield1\nfield2\nfield3\nfield1\nfield2\nfield3\nRDBMS Table1\nRDBMS Table2\nIngestion\napplication\nCloud data platform\nSQL\nSQL\nSELECT * FROM Table1\nSample query for data selection \nAn  ingestion application runs SQL\nqueries against the RDBMS and then\nsaves the result to the data platform. \nFigure 4.2\nRDBMS ingestion process\n\n\n85\nIngesting data from relational databases\nIf we look at this list from another angle, we will see a few things. First of all, we need\nsome kind of connector that allows our application to read data from RDBMSs. Luck-\nily for us, RDBMSs have been around for decades, and there are database connectors\n(or drivers) available for just about every RDBMS vendor that can be used with most\npopular programming languages. \n Next, if you look carefully at the diagram you will see that in the RDBMS ingestion\nprocess, data will need to travel through at least three different levels, as shown in fig-\nure 4.3.\nOn level 1, data is stored using native RDBMS types. Then, when a SQL query is exe-\ncuted by an ingestion application, these native RDBMS data types will be converted\ninto a data type supported by the specific programming language in which the inges-\ntion application is implemented (level 2). Finally, when the ingestion application\nsaves data into the landing area of the cloud data platform, data types must be con-\nverted once again into the data types that your chosen file format supports (level 3).\nSince data type mapping is not always one to one, and given that during the ingestion\nprocess from RDBMSs, data types change at least twice, data types mapping is import-\nant. We will talk more about this later in this chapter. \n Finally, if all your ingestion application can do is execute SELECT * FROM\nsome_table, then you are limited to doing only full-table ingestion. In many scenarios,\nthis is not enough, and a more sophisticated way of identifying and ingesting new or\nchanged data is required. \nfield1\nfield2\nfield3\nfield1\nfield2\nfield3\nTable1\nTable2\nIngestion\napplication\nCloud data platform\nLanding\nProcessing\nWarehouse\nSQL\nSQL\nSELECT * FROM Table1\nWhen a SQL query is\nexecuted by an ingestion\napplication, native RDBMS\ndata types are converted\ninto a data type supported\nby the ingestion application\n(level 2).\nWhen the ingestion application\nsaves data into the landing area\nof the cloud data platform, data\ntypes are converted into the\ndata types that your chosen file\nformat supports (level 3).\nData is stored in the\nRDBMS using native\nRDBMS types (level 1). \nFigure 4.3\nDifferent levels of an ingestion process\n\n\n86\nCHAPTER 4\nGetting data into the platform\n4.2.2\nFull-table ingestion\nTypically, RDBMS power applications are responsible for business operations, and\ndata in the RDBMSs changes all the time: new rows are added, existing rows are\nupdated, and some rows are deleted. It is unusual to see an application that mostly\nworks with static data. When designing a data ingestion pipeline for our cloud data\nplatform, we need to decide how to deal with this constantly changing data.\nNOTE\nThere is a difference between which data is important in an opera-\ntional database and which is important in an analytical data platform. Opera-\ntional databases are usually concerned with the question of “What is the\ncurrent state of some item?” It could be which items are in the shopping cart\nright now, what is the user’s account balance, or how many green gems did a\nplayer collect in the current game? Analytical data platforms are usually con-\ncerned with the question “How did a given item change over time?” In which\norder did the customer add items into the shopping cart? Did they add some\nitems that were later removed? To be able to answer these questions, analyti-\ncal data platforms need to store data differently than operational databases. \nImagine that we are building a cloud data platform for some kind of online service.\nThis service allows users to sign up for a trial account, and after the trial period\nexpires, users can either buy a premium subscription or cancel their account. Our\nchallenge is to design a pipeline that will allow us to capture not only the user status at\na point in time, but also how it evolves over time.\n Let’s dive in. In our scenario, our service uses a relational database as a backend.\nAs new users sign up and existing users change their subscription status, data in our\noperational database may look something like figure 4.4.\n We start with two users, one with a premium subscription and one recently joining\nfor a trial (A). After some time, a new user signs up for a trial, and row with user_id=3\nis added to our table (B). Next, user with user_id=2 decides to switch to a premium\nsubscription (C). Finally, because we don’t have an analytics data platform built yet,\nwe couldn’t identify which incentives to provide for the new trial user and market\nthose incentives to them. So this user decides to cancel their account, and their entry\nis deleted from the table (D). \n These events can be happening in any time frame. All these changes can happen\nwithin hours, minutes, or days. What’s important here is that we started with a table\nwith two rows in it and we ended up with two rows, but the data in these rows is now\ndifferent, and there are important things that happened in between. For example, for\nanalytics purposes, the fact that they canceled their account is important, but in an\nRDBMS, this data would be lost when the entry is deleted from the table. To make\nsure we have all the data we need to do our analysis, we need to design an ingestion\npipeline into our data platform that will allow us to capture not only the data at a\npoint in time, but also how it evolves over time.\n\n\n87\nIngesting data from relational databases\nOne way to address this problem is to create an ingestion pipeline that performs a\nperiodic full-table ingestion from an RDBMS—the simplest though limited approach\nto doing this. Basically, a full-table ingestion pipeline performs the following steps:\n1\nStart the pipeline on a given schedule.\n2\nExecute a SQL query—SELECT * FROM some_table—against the source database.\n3\nSave the results into the cloud data platform storage.\n4\nLoad data into the cloud warehouse.\nWith this full-table ingestion strategy, we read the entire table every time the pipeline\nruns. Let’s assume we run this pipeline once a day for four days. Using our previous\nexample containing users-subscription status, we would end up with four different\nsnapshots of our table in the cloud data platform storage, as shown in figure 4.5.\n Each day we extract all rows from the source database and land this snapshot in a\nfolder in our cloud data platform storage. We will talk more about organizing the data\nin cloud storage in chapter 5, but for now let’s assume we just save the snapshot in the\nfolder that uses the ingestion date as its name. So now we have four snapshots saved in\nUSER_ID\nSTATUS\nJOINED_DATE\n1\nPREMIUM\n2018-03-27\n2\nTRIAL\n2019-05-01\nUSER_ID\nSTATUS\nJOINED_DATE\n1\nPREMIUM\n2018-03-27\n2\nTRIAL\n2019-05-01\n3\nTRIAL\n2019-05-04\nnew row added\nUSER_ID\nSTATUS\nJOINED_DATE\n1\nPREMIUM\n2018-03-27\n2\nPREMIUM\n2019-05-01\n3\nTRIAL\n2019-05-04\nrow updated\nUSER_ID\nSTATUS\nJOINED_DATE\n1\nPREMIUM\n2018-03-27\n2\nPREMIUM\n2019-05-01\nrow deleted\nA. We start with two users,\none with a premium subscription\nand one recently joining for a trial. \nB. A new user signs up for a trial,\nand a row with user_id=3 is\nadded to our table.\nC. A user with user_id=2 decides\nto switch to a premium subscription,\nand a row is updated.\nD. A user decides to cancel their\naccount, and their entry is\ndeleted from the table. \nFigure 4.4\nData is always changing in an operational database.\n\n\n88\nCHAPTER 4\nGetting data into the platform\ndifferent folders in the storage. In our cloud warehouse, we have two options. We can\nstack these snapshots one on top of the other, as shown in figure 4.5, resulting in one\nlong table. Or we can keep only the latest snapshot in the warehouse. As we discussed\npreviously, keeping only the latest state of the data usually is not acceptable for analyt-\nical use cases, because you will lose a lot of details of how data arrived at this state. \n In our example, if we only keep the latest snapshot in the warehouse, then a data\nanalyst looking into our warehouse for user subscription data will only see what is\nshown in figure 4.6.\nBy looking at this data, we will not be able to see the whole story behind it: users join-\ning, switching to premium or dropping off in between, etc. This is a lot of useful infor-\nmation to lose. A preferred way is to add snapshots on top of one another in the\nwarehouse, as shown in figure 4.7.\n So, basically, we end up with a warehouse table where we have all the rows from\neach snapshot. Note that we have also added a new column in the warehouse that\ndoesn’t exist in the source data: INGEST_DATE. This allows us to distinguish between\ndifferent snapshots, because each of them will have its own ingestion date. There are\nsome other system columns that we recommend adding to each of the ingested tables\nin the cloud data platform, and we will talk more about those in chapter 5.\n \n \n \nA\nB\nC\nD\nA\nB\nC\nD\nSource\ndatabase\nCloud data platform storage folders\nCloud data warehouse\n2019-05-03\n2019-05-04\n2019-05-05\n2019-05-06\nWith full-table ingestion, the entire table from the source database is\nread every time the pipeline runs. Running the pipeline once a day for\nfour days will result in four different snapshots of our table in the cloud \ndata platform storage and data warehouse. \nFigure 4.5\nFull-table snapshots on cloud storage and in the warehouse\nUSER_ID\nSTATUS\nJOINED_DATE\n1\nPREMIUM\n2018-03-27\n2\nPREMIUM\n2019-05-01\nD\nFigure 4.6\nLatest users-\nsubscriptions table snapshot.\n\n\n89\nIngesting data from relational databases\nThis table gives us much more information about how data has changed over time\nthan just storing the latest snapshots, but this data model has its flaws as well. For\nexample, it is easy to write a SQL query that will show you all changes to a particular\nuser, in this case user_id = 2, over time:\nSELECT * FROM subscriptions WHERE user_id = 2 \nThis would give you the history of how the user first signed up as a trial user and then\nswitched to a premium subscription. But if you repeat the same query for user_id = 3,\nthen it would seem that they signed up for a trial and continued with the trial sub-\nscription, when in fact they cancelled their trial and deleted their account. So one of\nthe flaws of this design is that it doesn’t track deleted rows in any way. These rows just\ndisappear from the last full-table snapshot.\n The next issue to deal with is duplication of data. Let’s say you want to count how\nmany premium subscribers you have. If you run a query like this\nSELECT COUNT(*) FROM subscriptions WHERE status=”PREMIUM”\nyour answer would be wrong because you would be counting the number of rows in\neach snapshot. Instead, you need to always remember to limit such queries to a spe-\ncific ingestion date. This way you can get the number of premium subscribers on a\ngiven day. Anyone working with this table in the warehouse must remember that this\ntable is a stack of full source table snapshots taken on different days and then must\nadjust their queries accordingly.\nUSER_ID\nSTATUS\nJOINED_DATE INGEST_DATE\n1\nPREMIUM\n2018-03-27\n2019-05-01\n2\nTRIAL\n2019-05-01\n2019-05-01\n1\nPREMIUM\n2018-03-27\n2019-05-04\n2\nTRIAL\n2019-05-01\n2019-05-04\n3\nTRIAL\n2019-05-04\n2019-05-04\n1\nPREMIUM\n2018-03-27\n2019-05-05\n2\nPREMIUM\n2019-05-01\n2019-05-05\n3\nTRIAL\n2019-05-04\n2019-05-05\n1\nPREMIUM\n2018-03-27\n2019-05-06\n2\nPREMIUM\n2019-05-01\n2019-05-06\nA\nD\nC\nB\nFigure 4.7\nFull-table \nsnapshots appended into \na single warehouse table\n\n\n90\nCHAPTER 4\nGetting data into the platform\n All these issues can be easily overcome by building new data sets in the processing\nlayer that provide the required representation of data. The table structure shown in\nfigure 4.7 is a good fit for data ingestion purposes. It preserves data change history\nand is easy to implement, because we always do a full-table snapshot. You can imple-\nment any number of representations of this data using the processing layer of the\ncloud data platform or by creating a view in the warehouse, depending on the data\nsize and transformation complexity. Here are some examples of derived data sets that\nyou may want to implement based on raw source data:\nLatest version of each row—This is similar to storing only the last snapshot. Some\ndata users may only be interested in how data looked at the last ingestion. You\ncan implement this as a view in the warehouse and still have all the change his-\ntory preserved.\nIdentify deleted rows by comparing the last snapshot with a previous one—You can cre-\nate a transformation that will identify that user_id=3 existed in snapshot C, but\nnot in snapshot D, and mark it as a deleted row. Such a derived data set can be\nvery useful for analytics purposes. \nCompact raw data set—You can create a derived data set that preserves the his-\ntory of data changes but eliminates complete duplicates. For example,\nuser_id=1 hasn’t changed since snapshot A, so you can store only one row for\nthis user. User_id=2 has the same data in snapshots A and B, and after upgrad-\ning to PREMIUM in snapshot C, it doesn’t change in D. This means you need\nonly store two rows for this user. \nYou can see now how data platform layer design allows you to store raw data in a con-\nvenient way and preserve as many details about data changes as you can, while imple-\nmenting multiple derived data sets using the transformation layer. All the previous\nexamples are very typical for any business use case that describes how a certain entity\nchanges over time. All of them can be implemented in SQL, and we leave it as an exer-\ncise for the reader to do that.\n While full-table ingestion is easy to implement (simply execute a full SELECT state-\nment against the required tables on a regular basis) if the source table is large, the\nprocess will become inefficient. Doing a full-table extract of a large table (tens of GBs\nor more) will put an extra load on the source database server. If the source database is\nlocated on premises then, depending on the network bandwidths you have between\nthe data center and specific cloud region, transferring large data volumes into the\ncloud will take time—sometimes significant amounts of time. Second, while the cloud\nprovides almost unlimited scalability for storage, it comes with a cost. If your source\ntable is hundreds of GBs, and you do a full snapshot daily, then in a year this table\nalone will be responsible for about 36 TB of data. To address these issues, you can\ndesign your ingestion pipeline to ingest only data that is new or that has changed\nsince the last ingestion. This process is called incremental ingestion, and we will look into\nit next. \n\n\n91\nIngesting data from relational databases\n4.2.3\nIncremental table ingestion \nOn a high level, the idea behind incremental ingestion is simple—instead of pulling the\nwhole table on every ingest, we will pull only new rows and rows that have changed since\nthe last ingestion. This can significantly reduce storage needs and data transfer times.\nThere are several challenges with this approach, though. First, how do we know which\nrows are new and which rows have changed at the source? Second, how do we reliably\ntrack which data we have already ingested into our data platform? Let’s unpack this. \n A common way to identify which rows have been added or updated in an RDBMS is\nto use a dedicated column in every table that contains a timestamp of when the row was\nlast changed. For new rows, this column will contain the date and time of when this row\nwas inserted, and for updated rows, it will contain the last time this column was\nmodified. Figure 4.8 shows how this would look in our example subscriptions table.\nExercise 4.1\nThere are two main issues with performing full-table ingestion. What are they?\n1\nIt’s more complex to implement than other ingestion methods.\n2\nIt’s hard to identify rows that were deleted.\n3\nIt’s impossible to see how a given row changed over time.\n4\nIt causes too much duplicate data to be stored in the platform.\nFigure 4.8\nTracking row changes with the LAST_MODIFIED column\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nTRIAL\n2019-05-01\n2019-05-01 17:01:00\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nTRIAL\n2019-05-01\n2019-05-01 17:01:00\n3\nTRIAL\n2019-05-04\n2019-05-04 09:05:39\nNew row added\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nPREMIUM\n2019-05-01\n2019-05-05 08:12:00\n3\nTRIAL\n2019-05-04\n2019-05-04 09:05:39\nRow updated\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2019-03-27 13:57:03\n2\nPREMIUM\n2019-05-01\n2019-05-05 08:12:00\nRow deleted\nA\nD\nC\nB\n\n\n92\nCHAPTER 4\nGetting data into the platform\nIn our example, ingestion is happening daily, so\nfor each ingestion, we must keep track of the\nmaximum value of the LAST_MODIFIED col-\numn at the time of that ingestion (figure 4.9).\n Now to implement our incremental ingestion\nprocess, we must adjust the SQL query that we run\nto extract data from the source. Instead of pulling\nall rows, it will pull rows that have LAST_\nMODIFIED timestamp in the source table greater\nthan the MAX(LAST_MODIFIED) timestamp in\nthe data platform that we have recorded during\nthe previous ingestion. So during ingestion B, the query we need to run will look like this:\nSELECT * FROM subscriptions WHERE LAST_MODIFIED > “2019-05-01 17:01:00”\nOnly one row satisfies this condition at ingestion B, and only one row will be pulled\ninto our data platform. Now you must record the MAX(LAST_MODIFIED) value\nfrom ingestion B and repeat the process the next day. This will give you a full history\nof changes for each row in your warehouse, without the unnecessary duplication that\nyou would have to deal with if you were implementing a full-table ingestion. \n This approach obviously requires a LAST_MODIFIED column to be present in each\ntable for each incremental ingestion process. Fortunately, many RDBMSs make it easy\nto automatically track change timestamps for each row by providing an ability to specify\nthe default value of the column to be current time. This way, you don’t need to make\nany changes to your application code other than to update the table definition. \nNOTE\nWe strongly recommend using RDBMS capabilities to track last-modified\ntimestamps, if they exist. It’s possible to implement a similar process on the\napplication side, but this approach is error prone. One common issue we see\nwhen using timestamps created by the application is that sometimes bugs are\nintroduced in the application where rows get the last modified timestamp that\nis earlier than the current system time. From the perspective of ingestion, it\nmakes it appear that a row has been modified in the past and therefore no longer\nneeds to be ingested. Last-modified column values must always be increasing\nand never be less than the current MAX value for this column. \nAnother best practice is to track the previous MAX value of the last-modified time-\nstamp column so we can use it as a filtering condition for our ingestion SQL query.\nThe most recent value of the timestamp column is often referred to as the highest\nwatermark in ETL literature. In our layered cloud data platform architecture shown in\nfigure 4.10, we have a dedicated component to store technical metadata about our\npipelines. This is where we recommend storing things like max values of last modified\ntimestamps for incremental ingestion.\nINGESTION MAX( LAST_MODIFIED)\nA\n2019-05-01 17:01:00\nB\nC\nD\n2019-05-04 09:05:39\n2019-05-05 08:12:00\n2019-05-05 08:12:00\nFigure 4.9\nTracking max LAST_ \nMODIFIED values\n\n\n93\nIngesting data from relational databases\nThe actual implementation of this metadata repository will depend on the cloud ven-\ndor you choose for your data platform. Some of the tools we mentioned in chapter 3,\nsuch as AWS Glue, have a built-in way to track such watermarks. You can always imple-\nment a basic metadata repository by using a managed RDBMS service in the cloud,\nsuch as Cloud SQL on Google Cloud or Azure SQL Database, as your metadata repos-\nitory, implemented as a simple table that tracks max ingested timestamp for each of\nthe source tables, as shown in figure 4.11.\nUsing this metadata table, your ingestion process can then dynamically construct SQL\nqueries to perform the correct incremental injections for each of the tables. Dealing\nwith a simple MySQL or PostgreSQL is easy, but as the number of pipelines grows and\nthe number of different statistics you want to track includes more than high ingestion\nwatermarks, you may want to implement an API layer on top of this metadata reposi-\ntory to provide a consistent way to work with the data in it. \nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 4.10\nUse the cloud data platform operational metadata component to store max values for \nlast-modified timestamps.\nDATABASE\nTABLE_NAME\nWATERMARK_COLUMN\nMAX_INGESTED_VALUE\nmy_service_db\nsubscriptions\nlast_modified\n2019-05-05 08:12:00\nmy_service_db\nusers\nupdated_ts\n2019-05-04 12:23:13\nsales_db\ncontracts\nlast_modified\n2019-05-01 17:02:45\nFigure 4.11\nUsing a relational table as a metadata repository to track highest watermarks for each \nsource table\n\n\n94\nCHAPTER 4\nGetting data into the platform\n An incremental ingestion process helps address some of the challenges with full-\ntable ingestions—you need only bring in new and changed data, and you can avoid\nlarge data set transfers. However, an incremental ingestion process still doesn’t\naddress some of the fundamental problems with a SQL-based ingestion process. The\nfirst is one we already mentioned in the context of full-table ingestion. If you only read\ndata that currently exists in a source table, you will miss deleted rows. Unless your\napplication is designed specifically to not delete rows and rather mark them as deleted\nusing a special column, then you can only infer deleted rows by comparing previous\nsnapshots and finding missing rows. This is extra development work in the processing\nlayer that you may want to avoid. \n A second issue is that if your data changes often, even the incremental load process\nwill be limited by how many state changes between ingestions it can capture. Let’s say\nyou ingest data incrementally every hour. If rows in the source table changed multiple\ntimes during this hour, you will not get all the history of changes, and you will only see\nthe state of each row at the beginning of each ingestion period. You can ingest more\nfrequently, but you may still not be able to capture every single change of each row,\nbecause in a busy system, rows can be inserted and updated thousands of times per\nsecond. A reasonable maximum frequency of a SQL-based incremental ingestion\nwould be every few minutes because each ingestion has to execute a query against the\nsource database server, introducing extra load. For a busy system, this extra ingestion\nload is usually not acceptable. To see how these challenges can be addressed, we will\nlook into one more way to ingest data from RDBMSs: change data capture.\n4.2.4\nChange data capture (CDC)\nEvery production RDBMS writes row changes into a log. While different vendors\nname them differently—redo logs, transaction logs, binary logs—change data capture\nas an ingestion mechanism involves parsing these logs using a CDC application and\nsending a stream of changes to the target storage system from the log file, as shown in\nfigure 4.12.\n A CDC application parses log files, producing a stream of events: one event for\neach row change, which is sent to a cloud data platform using fast storage as an initial\nExercise 4.2\nWhich of the following is a prerequisite to implement incremental ingestion from an\nRDBMS?\n1\nYou need to migrate your source database into the cloud.\n2\nYou need a database administrator to give your specific permissions.\n3\nYou need to use the latest version of Apache Spark in your data platform.\n4\nYou need to have a LAST_MODIFIED column in each of the source tables.\n\n\n95\nIngesting data from relational databases\nlanding area. CDC applications are sometimes available from RDBMS vendors—for\nexample, Oracle GoldenGate—or are implemented as third-party applications, such\nas the open source project Debezium. These third-party applications can be either\nexternal to the cloud data platform or can be run as a cloud-native service as a part of\nthe data ingestion layer, such as AWS Database Migration Service. \n Unlike SQL-based ingestion, CDC allows you to capture all changes that happen to\nindividual rows. This includes deleted rows, which makes CDC a more robust way to\ningest data from an RDBMS. It also reduces the load on the source database relative to\nusing SQL-based ingestion, and it allows you to implement real-time analytics use\ncases with an RDBMS source. The trade-offs are that you may add licensing costs for a\nCDC application, and CDC is more complex to implement because it requires a real-\ntime infrastructure. \n Why does CDC ingestion require a real-time infrastructure with fast storage and a\nstreaming ingestion path? RDBMSs usually only retain a certain amount of data in\nthese logs. The retention period can vary from a few days to a few hours, depending\non how much traffic the source database handles. This means we need to transfer the\nrow change events into our cloud data platform, where we have almost unlimited stor-\nage and can retain as much change history as we need, as soon as possible, before this\nchange event is purged from the RDBMS log. \n To understand how a CDC event stream can be represented in the cloud data plat-\nform, let’s take a look at an example. We will use the same subscriptions table example\nwe used in the previous section (figure 4.13).\n We have three row changes in this table: a new row is added, an existing row is\nmodified, and an existing row is deleted. For our SQL-based ingestion example, we\nassumed that data changed only once within a couple of days (see the LAST_MODIFIED\ncolumn), but you can imagine that for a system with lots of users and traffic, multiple\nchanges like this will happen within seconds.\n \nSource\nRDBMS\nInsert\nUpdate\nDelete\nInsert\nUpdate\nDelete\n...\nCDC\napplication\nCloud data platform\nFast storage\n2. A CDC application parses\n    database log files, producing\n    a stream of events, one for\n    each row change.  \n1. A Production\n    RDBMS writes\n    row changes\n    into a log file.\n3. This stream of changes is sent to the\n    cloud data platform using fast storage\n    as an initial landing area.\nFigure 4.12\nCDC ingestion flow\n\n\n96\nCHAPTER 4\nGetting data into the platform\nThere is no standard on how RDBMSs capture row changes in the logs, and different\nCDC applications parse and format the data differently. We will demonstrate a generic\nway of how changes can be captured by a CDC application and how they can be repre-\nsented in the target data platform. In general, each change is represented by a mes-\nsage that contains information about what the state of the row was before and after\nthe change, with some additional information. \n Figure 4.14 is an example of a change message for a new row. This message is a\nJSON document, which is a common way to represent change message data in differ-\nent CDC applications. The key idea behind CDC is that you not only get the current\nstate of the row, but also the previous version of this row. In our example, these are\nrepresented by before and after attributes of the message. Another important aspect\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nTRIAL\n2019-05-01\n2019-05-01 17:01:00\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nTRIAL\n2019-05-01\n2019-05-01 17:01:00\n3\nTRIAL\n2019-05-04\n2019-05-04 09:05:39\nNew row added\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nPREMIUM\n2019-05-01\n2019-05-05 08:12:00\n3\nTRIAL\n2019-05-04\n2019-05-04 09:05:39\nRow updated\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2019-03-27 13:57:03\n2\nPREMIUM\n2019-05-01\n2019-05-05 08:12:00\nRow deleted\nA\nD\nC\nB\nFigure 4.13\nSubscription table changes example\n{\nmessage: {\nbefore: null,\nafter: {\nuser_id: 3,\nstatus: \"TRIAL\",\njoined_date: \"2019-05-04\",\nlast_modified: \"2019-05-04 09:05:39\"\n},\noperation: \"INSERT\",\ntable_name: \"subscriptions\"\n}\n}\nFigure 4.14\nExample of \na CDC message for a new \nrow\n\n\n97\nIngesting data from relational databases\nof the CDC message is that it usually contains a type of the operation: INSERT, UPDATE,\nor DELETE. In our example, the type of the operation is INSERT and thus, the before\nattribute is empty, because this is a new row and it didn’t exist before. The after attri-\nbute contains all columns for the new row and their values. Typically, CDC messages\ncontain lots of additional information, such as the name of the database server, exact\ntimestamp when a message was written to the RDBMS log, and time when it was\nextracted by the CDC application and other metadata. We have omitted most of this\nextra information in our example for brevity.\n Let’s take a look at how an UPDATE operation is represented in a CDC message (fig-\nure 4.15).\nIn this example, the user with id=2 has switched from a trial subscription to a pre-\nmium one, and once the row for this user has been updated in the source RDBMS\nsubscriptions table, our CDC application captures the state of this row before and\nafter the change. Finally, we will leave constructing a similar message for a DELETE\noperation as an exercise for the reader. \nNOTE\nINSERT, UPDATE, and DELETE operations are not the only ones that\ncan be captured by a CDC process. Many RDBMSs include information\nabout the schema changes in their logs as well. These schema changes (add-\ning new columns, deleting existing ones, etc.) will be included in a CDC\nmessage in a similar before/after pattern, where before represents a table\nschema before the change and after would represent the current status of\nthe schema. We will discuss more about schema management in the data\nplatform in chapter 6.\nNow let’s imagine that a CDC application has delivered data to our cloud data platform.\nThe next question is, what kind of processing do we need to apply to this data so we can\nstart using it in our warehouse? When it comes to representing CDC data in the ware-\nhouse, we end up with a model similar to incremental ingestion described earlier in this\nchapter. You will have a table in the warehouse that represents a full history of changes\n{\nmessage: {\nuser_id: 2,\nstatus: \"TRIAL\",\njoined_date: \"2019-05-01\",\nlast_modified: \"2019-05-01 17:01:00\"\nbefore: {\n},\nafter: {\nuser_id: 2,\nstatus: \"PREMIUM\",\njoined_date: \"2019-05-01\",\nlast_modified: \"2019-05-05 08:12:00\"\n},\noperation: \"UPDATE\",\ntable_name: \"subscriptions\"\n}\n}\nFigure 4.15\nExample \nof a CDC message for \nan UPDATE operation\n\n\n98\nCHAPTER 4\nGetting data into the platform\nfor each row, but without any gaps that you might see if you use SQL-based incremental\ningestion. There are a number of preprocessing steps that you might need to imple-\nment to get CDC data to this state:\n1\nDepending on the cloud platform of your choice, you might need to unpack\nthe CDC JSON document into a flat structure. Currently, only Google BigQuery\nsupports nested data types, which allows you to store the CDC messages as they\nare. If you are using AWS Redshift or Azure Cloud Warehouse, you will need to\nflatten all nested attributes into a single table.\n2\nEven if your warehouse supports nested data types, the way a CDC message is\nconstructed by a CDC application is not the most efficient and easy to use when\nit comes to warehouses. Since we are always appending new CDC messages to\nthe warehouse table, we are not really interested in the before state of the row,\nbecause we already have it in the warehouse from previous ingestions (it is still\nuseful to store the before attribute in the raw data archive in the cloud stor-\nage). We also usually don’t need any extra metadata associated with the CDC\nmessage in the warehouse. This means that you will need to filter out some of\nthe attributes before loading them into the warehouse. \n3\nSimilar to our incremental ingestion scenario, you might want to have a version\nof the table in the warehouse that only contains the latest version of each row,\nas opposed to having a full history of changes for each row. We recommend\nimplementing this as a view in the warehouse itself or a new transformation that\ncompacts multiple versions, and write them to a separate dedicated table. \n4.2.5\nCDC vendors overview\nAs we mentioned before, different RDBMS vendors implement a CDC process differ-\nently and have different CDC applications available. In this section, we provide a brief\noverview of CDC-specific options for the more popular RDBMSs: Oracle, MySQL, MS\nSQL Server, and PostgreSQL.\nORACLE\nOracle RDBMS captures changes to the rows in its redo logs. These logs are used for\nguaranteeing the reliability of data in the database itself. Oracle also provides a solution\ncalled Oracle GoldenGate that can extract changes from the redo stream and replicate\nit to the destination system. GoldenGate is able to play the role of a CDC application and\nExercise 4.3\nWhat is the main benefit of using CDC over SQL-based data ingestion?\n1\nCDC is easier to implement.\n2\nCDC is designed specifically for cloud.\n3\nCDC includes all changes to a given row.\n4\nCDC makes data warehouse table design simpler.\n\n\n99\nIngesting data from relational databases\ncan also stream CDC messages to various Big Data platforms via GoldenGate Big Data\nAdapters. GoldenGate is proprietary software and requires the purchase of an addi-\ntional software license.\n An alternative to using GoldenGate for Oracle CDC is Debezium, an open source\nproject that acts as a CDC application and publishes messages to Kafka. While Debe-\nzium itself is free and open source, it does require the Oracle XStream API, which\nrequires a GoldenGate license to be purchased. It should be noted that Kafka does not\nperform transactional consistency because it is essentially a message-queuing system.\n Oracle also comes with a bundled tool called LogMiner that can be used to extract\ndata from redo logs. While primarily designed for data exploration and debugging pur-\nposes, for DBAs there are a number of third-party applications that utilize LogMiner for\nCDC purposes: for example, AWS Database Migration Service uses LogMiner. LogMiner\ngenerally is not considered to be a 100% reliable way to deliver a CDC stream because\nit struggles with data consistency because transaction commits, and more importantly,\nrollbacks may occur after the capture of DML, and this must be accounted for.\n Another possibly lower cost replication product is SharePlex from Quest (https://\nwww.quest.com/products/shareplex/). This tool can be used in a way similar to Ora-\ncle GoldenGate to provide CDC operations.\nMYSQL\nMySQL is an open source RDBMS. MySQL uses what it calls a “binary log,” where it\nrecords all row changes. A binary log is used primarily for replication purposes. You\ncan configure a second MySQL server to continuously replicate changes from the pri-\nmary server via a binary log. Existing CDC tools for MySQL utilize this capability to\ncapture row change events and ship those to other systems. \n MySQL doesn’t come with any built-in tool or a CDC application, but since it is an\nopen source database, there are a number of available third-party CDC applications\nthat work with it. Debezium is one of them, but there are also MySQL CDC plugins\navailable for ETL tools, such as Apache NiFi. \nMS SQL SERVER\nMicrosoft SQL Server has had built-in CDC capabilities since the 2016 version. SQL\nServer, like all other RDBMSs, has a log, called transaction log, where all row changes\nare recorded for reliability purposes. MS SQL Server has a built-in capability that can\nextract change events from this log and populate a special “change table.” A change\ntable is a regular table in the database that contains row change history for a particu-\nlar table. CDC applications can then access this change table using a standard SQL\ninterface and extract changes.\n Since SQL Server exposes it’s change events as a regular database table that can be\naccessed with SQL, implementing a CDC application becomes a relatively simple task.\nA number of existing third-party and cloud applications support SQL Server CDC,\nsuch as AWS Database Migration Service, Debezium, and Apache NiFi.\n There is a downside to using CDC with SQL Server. If you have hundreds of tables\nfor which you want to capture change events, SQL Server will need to create and\n\n\n100\nCHAPTER 4\nGetting data into the platform\nmaintain a change capture table for each of them. Depending on how busy your pri-\nmary database is, it can introduce additional load on an already overloaded machine.\nYou can always choose to combine which tables require CDC ingestion (ones where\nyou need to have a full history of changes) and which can be implemented using full\nor incremental SQL-based ingestions. \nPOSTGRESQL\nPostgreSQL is another very popular open source RDBMS. Since version 9.4, Postgre-\nSQL has support for an “output plugin,” which can decode row change messages from\na PostgreSQL transaction log and publish them as either Protobuf or JSON messages.\nThis approach is similar to the MySQL approach, but PostgreSQL simplifies CDC\napplications by taking care of reading and parsing the log using the output plugin.\nDebezium and AWS Database Migration Service both support consuming the output\nof this plugin and streaming change events to the target destinations. \n4.2.6\nData type conversion\nLet’s take another look at our RDBMS ingestion diagram, which shows three levels\nwhere each level is a different software system: a relational database engine, an inges-\ntion application that reads data from an RDBMS, and a destination warehouse in a\ncloud data platform. Each of these systems stores and represents data types differently.\nWhen designing an RDBMS ingestion pipeline, you must consider how data types for\nthese three levels will map to each other (figure 4.16). \n First you should take an audit of what data types exist in your source database. Then\nyou should do an analysis of the data types supported by your destination cloud ware-\nhouse and whether there are any types that are supported in your source database that\nfield1\nfield2\nfield3\nfield1\nfield2\nfield3\nTable1\nTable2\nIngestion\napplication\nCloud data platform\nLanding\nProcessing\nWarehouse\nSQL\nSQL\nSELECT * FROM Table1\nWhen a SQL query is\nexecuted by an ingestion\napplication, native RDBMS\ndata types are converted\ninto a data type supported\nby the ingestion application\n(level 2).\nWhen the ingestion application\nsaves data into the landing area\nof the cloud data platform, data\ntypes are converted into the\ndata types that your chosen file\nformat supports (level 3).\nData is stored in the\nRDBMS using native\nRDBMS types (level 1). \nFigure 4.16\nDifferent levels of an RDBMS ingestion process\n\n\n101\nIngesting data from relational databases\nare not supported in your destination warehouse, or whether some types have different\ncharacteristics (such as TIMESTAMP precision) in the source and destination. \n Most likely you will find some inconsistencies. Typically, relational databases pro-\nvide a wider range of data type support that may not be available in the cloud data\nwarehouse. For example, MySQL supports five different types for integer values:\nTINYINT, SMALLINT, MEDIUMINT, INT, and BIGINT. All these types are represented by dif-\nferent numbers of bytes in the database storage and thus can store numbers up to a\ncertain size. \n If we check which integer types are supported in Google BigQuery (as an exam-\nple), we will see that there is only one—INT64—that is equivalent to BIGINT in MySQL\nin terms of how big of an integer value it can store. Such differences are caused by the\nfact that RDBMS data types need to be optimized for storage and performance. It’s\nbetter to use a small-size data type if your expected range of values fits into it. It will\nsave you disk space and improve performance of your queries. Warehouses, on the\nother end, are optimized for scanning large volumes of data and are less concerned\nwith having granular data type support. \n The important thing to keep in mind when doing your data type analysis is\nwhether there is a data type in your source system that can store a wider range of val-\nues than your destination warehouse. If that’s the case, then there is a chance of data\nloss during ingestion—some values in the source database may be too big for a corre-\nsponding data type in the warehouse. The reverse situation is not a problem. For\nexample, BigQuery doesn’t have support for the SMALLINT data type, but all MySQL\nSMALLINT values (up to 64535) will fit into the INT64 BigQuery data type (up to\n9,223,372,036,854,775,807). \nNOTE Cloud vendors are constantly releasing new versions of their services,\nincluding their cloud warehouse offerings. Check the release notes when a\nnew version is available to see if a new data type support has been added. \nIf that’s the case and you have data types without direct equivalents in the cloud\nwarehouse, then you will need to use some workarounds. Usually you can use TEXT or\nBYTES data types in the warehouse to store some sort of a representation of the source\ndatabase type and that perform required calculations in the data platform processing\nlayer. For example, a point with coordinates (12, 23) can be stored in a MySQL\ndatabase using a native geospatial type, but if your destination warehouse is AWS\nRedshift, then you can store this information as a TEXT type that looks something like\nthis: POINT(12, 23). This allows you to preserve the information, but of course you\nwill need to write custom data processing code in your data platform processing layer\nto perform any calculations on this data, such as checking if the point lies within a\nrectangular area. But where does the conversion from a MySQL native type to a text\nrepresentation happen? The answer is, in the ingestion application (level 2 of fig-\nure 4.16). \n\n\n102\nCHAPTER 4\nGetting data into the platform\n Ingestion application is software that sits between the source database and the des-\ntination warehouse. Depending on which programming language they are imple-\nmented in and what type RDBMS drivers they use to access the source database,\ningestion applications will have their own set of data types that will be used to map\nsource types to destination types. \n If you are using an off-the-shelf ETL tool for data ingestion, then you are limited\nby whatever data type support it offers. In this case, in addition to doing the source\nversus destination comparison of supported data types, you also need to do an analysis\nof what data types are supported by your ingestion application. \n To summarize this section, here is a sequence of steps that you need to perform to\nevaluate your RDBMS ingestion pipeline from a data type support perspective:\n1\nPrepare a list of data types that your source RDBMS supports. It may be tempt-\ning to limit your analysis to the subset of data types you currently use. We sug-\ngest performing a full analysis on all supported data types to avoid issues in the\nfuture, when your application development team decides to use a new type. \n2\nPrepare a list of data types that your cloud data warehouse supports. Identify\ndifferences between the two.\n3\nIdentify data types that have no direct equivalent in your destination warehouse,\nbut will not case data loss. For example, various sizes of integer data types will\nalways fit into a bigger integer data type size that most warehouses provide.\n4\nIdentify types that do not have equivalents and can cause data to be lost on\ningestion: for example, geospatial types, JSON data types, etc. For each type,\nidentify if a workaround is possible: for example, storing some representation\nof the unsupported data type as text and writing custom data processing code\nto perform analysis. \n5\nIf a workaround is not possible, or if your primary analytics use cases revolve\naround these unsupported data types, then you may consider choosing a differ-\nent cloud warehouse vendor that has the data types you need. For example, if\nyour primary analytics use cases are working with geospatial data, then AWS\nRedshift will not be the best fit for you. \n6\nIf you are using an off-the-shelf ETL tool or a cloud service for data ingestion,\ndo an analysis of which data types are supported there. We always recommend\ndoing several proof of concepts to find out how different data types are sup-\nported, since documentation for these tools may not always be clear.\n7\nIf you are developing an ingestion application yourself, list data types that are\nsupported by the database driver you are planning to use. A popular choice is\nusing a JDBC driver, which has its own set of data types. For JDBC drivers, check\nyour RDBMS vendor documentation to see how source database types map to\nJDBC types. \n\n\n103\nIngesting data from relational databases\n4.2.7\nIngesting data from NoSQL databases\nRDBMSs are still the most popular database backends for most applications today. Over\nthe last several years, we have also seen the significant rise in popularity of NoSQL data-\nbases. NoSQL is a generic term for databases that, as one of their design principles, have\nchosen to sacrifice some of the RDBMS properties (transactions, durability, or others)\nin favor of being able to support a large volume of operations, scale easily by creating\nclusters of machines, or use a more flexible document-oriented data model. \n The challenge with building an ingestion pipeline for a NoSQL database is that\nthere is no single standard on how data can be extracted from them and in which for-\nmat it will be presented to the ingestion applications. The very name NoSQL implies\nthat typically SQL as a data access language is not supported, and every NoSQL data-\nbase vendor has their own set of APIs to access the data. \n We can still outline some of the most common ways to ingest data from NoSQL\ndatabases into our cloud data platform:\nUse an existing commercial or SaaS product for ingestion from a NoSQL data-\nbase. This is the path of least resistance if using such a product fits into your\ntechnology landscape and budget. Vendors who sell data ingestion tools usually\nhave a rich set of various connectors to NoSQL databases.\nImplement a dedicated ingestion application for your NoSQL vendor. You will\nneed to develop an ingestion application that uses client libraries specific to\nyour NoSQL database. This approach gives you the most flexibility because you\ncan use all the features your database has. You can also implement full or incre-\nmental ingestion using guidelines we described earlier in this chapter.\nUse a change data capture plugin, if available. Some of the popular NoSQL ven-\ndors have change data capture plugins available. For example, there is a Debe-\nzium connector for MongoDB that captures all changes done to the database\nand writes them as a stream to Kafka\nUse export tools provided by your NoSQL database. Most of the databases come\nwith tools that allow users to export data into a text format (CSV or JSON, usu-\nally) for backup or migration purposes. You can schedule these tools to run\nperiodically and then build the ingestion pipeline to only work with resulting\ntext files. This approach will simplify your ingestion pipeline, but you might be\nlimited to doing only full exports if your NoSQL database doesn’t support\nincremental data extracts. \nLet’s take a look at which ingestion options are available for some of the most popular\nNoSQL databases that are on the market today.\nMONGODB\nMongoDB is a document-oriented NoSQL database. Document-oriented means that\ninstead of using a table concept with rows and columns, MongoDB stores data in a for-\nmat that is very similar to JSON format. Each document can have nested attributes,\nwhich makes it possible to express dependencies between different entities, such as\n\n\n104\nCHAPTER 4\nGetting data into the platform\nusers and their orders, for example, in a single document, instead of having two rela-\ntional tables that later need to be joined together. \n MongoDB has client libraries for all popular programming languages that you can\nuse to implement your own ingestion application. There is also a data export\ntool called mongoexport (https://docs.mongodb.com/manual/reference/program/\nmongoexport/) that allows you to export data from MongoDB into CSV or JSON files.\nMongoexport supports a custom query to extract data, which means you can imple-\nment an incremental export process if you have a timestamp of when the document\nhas been last modified in your MongoDB collection. \n There is also a CDC plugin for the open source project Debezium that will capture\nall changes like adding, updating, and deleting a new document as a message stream\nand publish it to Kafka (https://debezium.io/docs/connectors/mongodb/). \nCASSANDRA\nApache Cassandra is an open source, highly scalable database that uses a hybrid\nbetween a key/value and columnar data model. Similar to key/value stores, columns\nin Cassandra do not have specific types, but they can be organized into column fami-\nlies for speeding up data access to columns that are frequently accessed together.\n Cassandra supports CQL—Cassandra Query Language—for accessing data. You\ncan use CQL commands or client libraries for various programming languages to\nimplement a dedicated ingestion application. CQL also has a COPY command that\nallows you to export a table as a CSV file. This only supports full-table exports. \n Cassandra has a built-in CDC feature that saves a log of all changes into a dedi-\ncated directory on disk. While the format of this log is open source and documented,\nthere are currently no widely used CDC applications that can read this log and publish\nchange event messages to Kafka or some other system. \n4.2.8\nCapturing important metadata for RDBMS or NoSQL ingestion pipelines\nIn a production environment, data ingestion pipelines are rarely executed just once;\nthey typically ingest data continuously to keep the data in the data platform current.\nTo make sure your data ingestion pipeline is working correctly and delivering accu-\nrate results, you will need to implement a number of data quality checks and monitor-\ning alerts to know when things don’t go as expected. We will talk more about data\nquality and monitoring in further chapters, but in this section, we will outline some of\nthe important statistics that you should capture in your ingestion pipeline so you can\nimplement quality control and monitoring later. As we described in chapter 3 and is\nshown in figure 4.17, the destination for these metrics is a metadata repository in your\ncloud data platform. \n There is some basic information that you would want to capture for every database\ningestion pipeline, including\nThe source database server name (and IP address, if possible) \nThe database name (or schema name, depending on the database vendor) \nThe source table name \n\n\n105\nIngesting data from relational databases\nIf you are ingesting from multiple RDBMS and NoSQL databases, it’s a good idea to\nalso store the type of the database as well. This information will help you to perform\nsome basic data lineage operations and is very helpful in debugging and troubleshoot-\ning any ingestion problems. \n When it comes to batch data ingestion from a database (RDBMS or NoSQL), one\nof the most important metrics to capture is the number of rows ingested per table for\neach ingestion. This metric is applicable for both full-table ingests and incremental\ningests, but you should distinguish between the two in your metrics-capturing process.\nThis metric is very important because it allows you to later implement two critical\nmonitoring checks:\nA check that validates that all ingested data made it way into the destination\nwarehouse or other destination system. \nA check that detects anomalies in the ingestion process. For example, a sudden\nincrease or decrease in the number of ingested rows can indicate issues with\neither the data source or the pipeline itself. \nAnother important metric for batch ingestion is the duration of each ingest. If you\ncapture the start and end timestamps for each ingestion, then you can implement a\nservice level agreement (SLA) type of monitoring. SLA monitoring allows you to know\nwhen the whole pipeline, or parts of it, takes either longer than a specified value or\nlonger than an average duration. This way you can detect issues with the pipeline (or\nsource system) as they happen. \n If you are using an existing ETL tool or service to do the data ingestion from the\ndatabase, then you need to carefully evaluate which metrics your tool captures and\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 4.17\nOperational metrics are captured in a metadata repository.\n\n\n106\nCHAPTER 4\nGetting data into the platform\nwhether these metrics can later be used by external monitoring tools. If you are imple-\nmenting your own pipeline metadata repository, which, as mentioned earlier in this\nchapter, can be a simple database itself, then figure 4.18 is an example of an ingestion\nstatistics table for batch ingestion.\n In this example, we are storing some basic information about the source system, the\nname of the database and table that we are ingesting from, type of operation (full or\nincremental), number of rows ingested, start/end timestamps for this particular ingest,\nas well as the duration of the ingest in seconds. Please note that we would normally rec-\nommend more descriptive column names for statistics tables, such as duration_\nseconds instead of just duration, but we have to be mindful about the page width here. \n For streaming, CDC ingestion the metrics would be slightly different. The basic\ninformation about the source server and table name will be the same, but since data\narrives continuously, we can no longer store metrics about a particular ingest batch.\nInstead, for streaming a CDC ingestion pipeline, we need to store statistics for a spe-\ncific time window. For example, we can calculate how many rows we are ingesting\nevery five minutes. The duration of the window itself will depend on your alerting and\nmonitoring needs. The faster you are expected to react to issues in the pipeline, the\nsmaller the observation window should be. \n In addition to the number of rows ingested in a CDC pipeline, it is a good idea to\nstore the number of inserts, updates, and deletes separately. CDC usually provides this\ninformation out of the box, and having these metrics will allow you to construct more\nprecise data quality and pipeline-monitoring checks. Figure 4.19 shows how a metrics\ntable in the metadata repository might look for a CDC pipeline. (Note that we have\nomitted the common fields like server and IP for brevity.)\nserver\nIP\ndb_type database\ntable\nop_type\nrows\nstart_ts\nend_ts\nduration\nprod1\n10.12.13.4 MySQL\nusers_db\nusers\nfull\n50432\n2019-05-02\n12:03:15\n2019-05-02\n12:15:01\n706\nprod1\n10.12.13.4 MySQL\nusers_db\nsubscrib\nincr\n642\n2019-05-02\n08:27:43\n2019-05-02\n08:28:00\n17\nprod2\n10.12.23.4 MongoDB marketing\ncampaign\nfull\n429\n2019-05-02\n09:48:00\n2019-05-02\n09:48:53\n53\nFigure 4.18\nExample of pipeline metrics for a database batch ingestion pipeline\nFigure 4.19\nExample of pipeline metrics for a database CDC ingestion pipeline\nop_type\nstart_ts\nend_ts\nduration\ninserts\nupdates\ndeletes\ncdc\n2019-05-02\n12:00:00\n2019-05-02\n12:01:00\n60\n10\n129\n2\ncdc\n2019-05-02\n12:01:00\n2019-05-02\n12:02:00\n60\n7\n100\n1\n\n\n107\nIngesting data from files\nHere, we are splitting a single row count metric into separate metrics for inserts,\nupdates, and delete operations. Also start_ts, end_ts, and duration no longer repre-\nsent how long it took to ingest these rows. Instead, they identify the boundaries and\nthe duration of the observation window we are using to calculate these metrics. \n Finally, we need to mention that another important metric for an ingestion pipeline\nis whether there were any schema changes in the given ingestion batch or an observa-\ntion window for streams. Knowing that the source schema changed allows us to build all\nkinds of alerting and automation for resolving some of the issues that a schema change\ncan cause. We will talk about schema management in great detail in chapter 6.\n4.3\nIngesting data from files\nUsing files to deliver data is probably one of the oldest ETL pipeline types. Various sys-\ntems support exporting data into text files like CSV or JSON, making this type of data\nexchange relatively simple to implement. Files are also a popular way to exchange\ndata with third parties, because they provide a nice separation between the source and\ndestination systems. You can save source data into a file and then load it into a num-\nber of different systems without having to establish direct connectivity between the\ntwo, minimizing security risks and the possibility of a negative performance impact. \n While it looks like files are the simplest way to exchange data, implementing a\nrobust file-based data ingestion pipeline is not straightforward. Text file formats such\nas CSV suffer from a lack of format enforcing—a system that produces a file may use a\ndifferent set of rules to format CSV files than the system consuming the data. File for-\nmats such as JSON have a stricter format but still lack any way to enforce schema or\ndata types. \nNOTE\nThere is a formal specification (https://tools.ietf.org/html/rfc4180)\nto which all CSV tools should adhere, but our experience indicates that\nupstream data producers often choose to implement their own way of writing\nCSV files and don’t always follow the standard. You will need to build your\ningestion pipeline in a defensive way to compensate for this possibility. \nWe will be discussing differences in various file formats in this chapter, in our cloud\ndata platform design; parsing the files into data elements is not the responsibility of\nan ingestion layer. Instead, it is done in the processing layer, and we will talk more\nabout it in chapter 5.\n There are two primary ways to deliver files into a cloud data platform. The first one\nis to use a standard File Transfer Protocol (FTP) or SFTP, a more secure version of\nFTP. FTP is a popular protocol supported by many ETL tools. It requires a dedicated\nserver to host files and to allow various clients to connect to it using username and\npassword as an authentication option.\n As shown in figure 4.20, FTP servers are often hosted on premises while an inges-\ntion application, either an existing ETL tool or cloud ETL service, connects to the\nFTP server and pulls the required files from it, depositing them into cloud storage on\nour data platform.\n\n\n108\nCHAPTER 4\nGetting data into the platform\nThere are several limitations to this approach. First, you will need to establish secure\nnetwork connectivity between the FTP server and your cloud data platform. Second,\nthe FTP server usually relies on local storage to store the files. This means that you\nneed to plan your FTP storage accordingly if you are planning to ingest large volumes\nof data. \n An alternative approach that we see gradually replacing the traditional FTP file\nexchange method is the use of cloud storage instead of an FTP server. This is shown in\nfigure 4.21.\nFTP server\nLocal storage\nFiles\nIngestion\napplication\nCloud data platform storage\nOn premises\nCloud\nFTP servers are often\nhosted on premises. \nAn ingestion application, either an existing\nETL tool or cloud ETL service, connects to the\nFTP server and pulls the required files, depositing\nthem into cloud storage on our data platform.\nFigure 4.20\nIngesting files from an on-premises FTP server\nCloud storage\nFiles\nIngestion\napplication\nCloud data platform storage\nCloud\nCloud\nFiles are saved to cloud\nstorage instead of FTP. \nAn ingestion application simply copies files\nfrom one cloud storage to another. Storage\nfor incoming files and the cloud data platform\nare often hosted on different clouds. \nFigure 4.21\nIngesting files from cloud storage\n\n\n109\nIngesting data from files\nHere, files are saved to a cloud storage, and an ingestion app just copies files from one\ncloud storage to another. The benefits of cloud storage over FTP are elastic storage;\nuse of secure file transfer mechanisms provided by cloud vendors that make network\nconfiguration simpler; security options such as temporary access keys that expire after\nsome time; and data access auditing features. \n Storage for incoming files and the cloud data platform are often hosted on differ-\nent clouds. Keep in mind that there is a cost for data transfer between cloud provid-\ners—all cloud vendors charge for copying data out of their environment. The benefits\nof cloud storage instead of FTP are that you get elastic storage and don’t need to\nworry about storage size. Also, using the secure mechanism of file transfer that cloud\nvendors provide makes network configuration much simpler. Usually this involves a\nsecure access key of some kind. Cloud storage also provides other security options\nsuch as creating a temporary access key that expires after some time; as well as data\naccess auditing features. \n4.3.1\nTracking ingested files\nUnlike ingesting data from RDBMSs, with files there are fewer concerns about\nwhether we are dealing with full or incremental ingestion. A file on FTP or a cloud\nstorage is an immutable data set—once the source system finishes writing to it, it\ndoesn’t change. This means that your ingestion pipeline should not be concerned\nwith tracking which specific items for a file have already been ingested and which have\nnot. But what needs to be tracked carefully is which files have already been ingested\ninto the cloud data platform and which have not. \n One of the methods of tracking which files have been ingested is to structure your\nfolders on FTP or cloud storage to have “incoming” and “processed” folders, as shown\nin figure 4.22. \nFigure 4.22\nUsing incoming and processed folders for tracking ingested files\n1. To track which files have been\n    ingested, create “incoming”\n    and “processed” folders on \n    FTP or cloud storage. \nStorage\nfiles\nIngestion\napplication\n3. An ingestion application reads all files from\n    the incoming folder, then saves them to the\n    landing area of the cloud data platform. \n2. The source system saves new\n    files into the incoming folder.\nsales_4\nProcessed\nsales_1\nsales_2\nsales_3\nsales_4\nIncoming\nsales_1\nsales_2\nsales_3\nsales_4\nCloud data platform storage\n4. After the file has been successfully saved to the data platform,\n    the ingestion application copies it into the processed folder on\n    the source system and deletes it from the incoming folder.\n\n\n110\nCHAPTER 4\nGetting data into the platform\nIn this approach, you would create two folders on your source FTP or cloud storage\nsystem—one called “incoming” and one called “processed.” The source system saves\nnew files into the incoming folder. An ingestion application reads all files from the\nincoming folder, then saves them to the landing area of the cloud data platform. After\na file has been successfully saved to the data platform, the ingestion application copies\nit into the processed folder on the source system and deletes it from the incoming\nfolder. You don’t need to store files in the processed folder long term, but we found it\nuseful to keep files there for a few days (depending on your ingestion schedule),\nbecause it is helpful in debugging possible ingestion issues. You can have a scheduled\nprocess that periodically cleans up the processed-folder contents.\n While this may appear to be a complex sequence of steps, there are several major\nbenefits to this approach. First, your ingestion app doesn’t need to actually track\nwhich files have been processed and which have not. It can read all files from the\nincoming folder, knowing that only new files will exist in this folder. Second, it is very\neasy to replay the ingestion of certain files just by copying them from the processed\nfolder into the incoming folder and letting the ingestion app follow the regular\nsequence. Note that for this approach to work correctly, it’s very important to make\nsure the ingestion application properly handles any relevant exceptions. For example,\nif a copy to the landing area of the cloud data platform has failed for some reason,\nfiles should not be copied into the processed folder but rather left in the incoming\nfolder for the next retry. \n Generally, we would recommend this two-folder approach for any file-based inges-\ntion pipelines. Unfortunately, there are cases where it isn’t possible. For example, if\nthe source system already uses a certain folder organization for the files on the FTP or\ncloud storage, or if you don’t have permissions to move files from one folder to\nanother on the source system (common when ingesting files from a third party), then\na different approach is needed. \n Here is a possible different approach. Each file, either stored on FTP or a cloud\nstorage, has a set of metadata stored with it. One of the attributes of a file is a time-\nstamp of when the file was last modified. We can use this timestamp to identify only\nnew files that have been added after a certain time. The approach we use here is very\nsimilar to the incremental ingestion with RDBMSs. The idea is to track the maximum\nlast modified timestamp of all files we have seen so far and only ingest files with the\ntimestamp greater than that high watermark. The ingestion application will need to\nwork together with the data platform metadata repository to store and fetch the high\nwatermark value, as shown in figure 4.23.\n Since, unlike with an RDBMS, FTP and cloud storage don’t support a query lan-\nguage, the first step in the process is to list all available files on the source storage\ntogether with the last modified timestamp value for each file (1). Next, the ingestion\napplication needs to fetch current high watermark value from the metadata repository\n(2). It then can use this value to filter the list of files to only those where the last modified\ntimestamp is greater than the current watermark (3). Finally, the ingestion application\n\n\n111\nIngesting data from files\nneeds to find the maximum timestamp among the remaining files and save it back into\nthe metadata repository as the new watermark (4).\n This approach is generally more complex than the two-folders approach. It’s worth\nmentioning that many existing ETL tools can implement the process shown in figure\n4.23, so you don’t need to worry about getting all the steps right. For example Apache\nNiFi has ListS3, ListAzureBlobStorage, ListGCSBucket, and ListSFTP processors that\nhandle the high watermark and file-filtering processes. The issue with ETL tools in\nthis case is that they usually store the high watermark in some kind of internal reposi-\ntory that may not be flexible enough for all your pipeline metadata needs. You will\nalso need to make sure that the ETL tool repository is backed up on a regular basis,\nbecause losing the information about the high watermark means you can no longer\ntell which files were already ingested. Another downside of this approach is that if you\nare dealing with thousands of files, then just the operation of listing all files in the\nsource system and filtering them out can become very slow. This is especially true for\ncloud storage. \n A slight variation of the previous approaches to track ingested files is to organize\nfolders on FTP or cloud storage using names that represent the date and time at\nwhich a given file was uploaded. For example:\n/ftp/inventory_data/incoming/2019/05/28/sales_1_081232\nHere we can see that the directory structure on the FTP contains the year, month, and\nday when a file has been uploaded, and the timestamp is included in the file name. Hav-\ning such a directory structure allows you to narrow down listing all files to only listing\nfiles from a folder that represents the most recent date. For example, if your ingestion\nsales_1\n2019-05-28 08:12:31\nsales_2\n2019-05-28 08:13:31\nsales_3\n2019-05-29 14:23:07\nsales_4\n2019-05-29 17:13:43\nMetadata\nrepository\ncurrent high\nwatermark =\n2019-05-28\n08:13:31\nIngestion application\nSource system\n3. Filter files where last\n    modified timestamp >\n    2019-05-28 08:13:31\nsales_3 2019-05-29 14:23:07\nsales_4 2019-05-29 17:13:43\n1. List all available files on the\n    source storage together with\n    last modified timestamp value\n    for each file.\n2. Fetch current high\n    watermark from the\n    metadata repository.\n4. Find the maximum\n    timestamp among\n    the remaining files\n    and save it back\n    into the metadata\n    repository as the\n    new watermark.\nFigure 4.23\nTracking the last modified timestamp of files\n\n\n112\nCHAPTER 4\nGetting data into the platform\napplication runs on 2019-05-28, then it can only list files from a corresponding directory.\nThis can help improve the performance when the number of files on the source systems\ngrows significantly. \n It’s worth noting that using a timestamp tracking approach makes replaying the\ningestion of a certain file or files more complicated. You will need to adjust the water-\nmark value in the metadata repository to force the process to pick up specific files\nagain, and it may not be possible to reprocess a single file if multiple files have the\nsame timestamp value.\nNOTE\nIf you are dealing with cloud storage as a source system for your files, it’s\nworth checking which file copy tools are available from your specific cloud\nvendor. For example, you can use the gsutil rsync tool on Google Cloud to\nsynchronize files between two Google Cloud Storage destinations, or between\nGoogle Cloud Storage and a local file system, or even between Google Cloud\nStorage and S3. This tool can keep track of which files were added to the\ndestination and only copy new files for you. Similar tools exist for other cloud\nvendors, such as blobxfer (https://github.com/Azure/blobxfer) on Azure\nand s3 sync on AWS. Using these tools is convenient if you are doing a large\ninitial transfer of data into your cloud data platform. \nKeep in mind that these tools don’t really store any information about which\nfiles have been ingested and which not, but rather compare the list and the\nchecksums of files on source and destination to identify new or changed files.\nThis means that you can’t easily reset the high watermark to replay ingestion of\ncertain files, for example. \n4.3.2\nCapturing file ingestion metadata\nAs with an RDBMS ingestion process, it is important to capture statistics and other\nmetadata about the file-based ingestion pipeline. Unlike RDBMSs, though, we will not\nbe capturing the number of rows in each file during the ingestion process. Row count\nis very important for files as well as databases, but instead of doing it during ingestion,\nwe need to do it during processing. \n The main reason for doing it during processing is scalability. When reading data\nfrom RDBMSs, you usually can get the number of rows fetched from a table “for free,”\nmeaning this information is available to you via a client library, or you can send a SQL\nExercise 4.4\nWhen would you choose tracking ingested files in the Metadata layer instead of using\nan “incoming” and “processed” folder structure?\n1\nWhen a source system already has a predefined folder structure\n2\nWhen you are dealing with lots of small files\n3\nWhen you are dealing with lots of large files\n4\nWhen you are concerned about pipeline performance\n\n\n113\nIngesting data from files\nquery to get this number from the database itself. The client (your ingestion applica-\ntion) doesn’t need to do any extra processing to get the row count. \n With files, the situation is different. If you need to count the number of individual\nrows in files, you first of all need to know exactly which file format you are dealing with;\nthen implement a specific parsing functionality in your ingestion application; and,\nfinally, split the files into rows and count the rows in your ingestion application code.\nThis approach won’t scale for large files, because ingestion applications are usually not\ndistributed in nature, meaning they run on a single virtual machine and cannot split\nthe large files into smaller chunks and process each chunk on a separate VM. We have\nseen ETL tools choke trying to parse a CSV file that was tens of GBs in size. \n So we will leave capturing row counts of files to the processing layer, using a distrib-\nuted data processing engine to easily scale to any file size. We will talk more about this\nin chapter 5. \n Some of the statistics and metadata we recommend capturing for the file ingestion\npipeline are similar to what we discussed previously for RDBMSs. It’s important to\nhave some kind of name for the source system that will help you to identify where the\nfile came from. For example, you can give your FTP servers or source cloud storage\nsome descriptive names such as “inventory_ftp” or “demographics_s3.” Knowing and\nstoring the type of the files for this particular source is also useful, since that can later\nbe used by the processing layer to know which parsing library to use to read data from\nthis source. The size of the file and the duration of the ingestion are also useful met-\nrics that you can utilize later to detect any anomalies in file size (files that are too big\nor too small) and/or ingestion time SLAs. \n Finally, as we mentioned before, sometimes there is useful information encoded in\nthe file name itself. It could be the specific time the file was generated, the name of\nthe source system that produced the file, and the like. We have also seen some other\ninformation, such as the name of a state or province, included in the file name, indi-\ncating that we should only inspect data for this geography in the file. Preserving the\nfull file name in the metadata repository can help when debugging data issues. We\nactually recommend storing the full path to the file on the source system, since some-\ntimes directory structure contains useful information as well. \n Figure 4.24 summarizes common metadata that you should consider for a file\ningestion pipeline.\nsource_name\nfile_type\nstart_ts\nend_ts\nduration\nfile_name\nfull_path\nfile_size\ninventory_ftp\nCSV\n2019-05-02\n07:00:00\n2019-05-02\n07:12:00\n720\ninventory_CA.csv\ndata/incoming/inventory/2019/05/01/\n268435456\ndemographics_s3\nJSON\n2019-05-05\n12:00:00\n2019-05-05\n12:02:00\n120\ndem_full.json\ns3://share/demographics/latest\n524288000\nFigure 4.24\nExample metadata for a file ingestion pipeline\n\n\n114\nCHAPTER 4\nGetting data into the platform\n4.4\nIngesting data from streams\nStreams are becoming an increasingly popular way to exchange data between multiple\nsoftware systems, with Apache Kafka becoming the de facto standard for a message\ndelivery system. This means that as data platform designers we need to account for\nKafka or a similar system as one of the possible data sources. This section will focus\nprimarily on Kafka, but there other message delivery systems that you may see as a\ndata source for your platform. All major cloud vendors have their own services with\nsimilar properties: Google Cloud Pub/Sub, Azure Event Hubs, and AWS Kinesis. The\nchallenges and solutions that we will describe in this section will be generally applica-\nble to all such systems. \n When it comes to ingesting data from a stream, we usually see two main scenarios.\nThe first one is streaming, or real-time, ingestion. In this scenario, what is important\nfrom the end user’s perspective is that data be ingested into the cloud data platform,\nincluding into the warehouse, as fast as possible, but this data is then used for analysis\nin an ad hoc and not necessarily real-time manner. \n The second scenario is when, in addition to ingesting data in real time, you also\nneed to perform some non-trivial computations and analytics on the data as it comes\nin. This is called real-time analytics, as opposed to real-time ingestion. This chapter\nwill focus on the real-time ingestion scenario, and real-time analytics will be covered in\ngreat detail in chapter 6. \n Ingesting data from a streaming system is quite different from dealing with batch\ningestion from an RDBMS or a file. The main difference comes from the fact that\ninstead of receiving multiple data elements together (rows from RDBMS, JSON docu-\nments from a file, etc.), we receive messages one by one from a message bus as they\nare written by an upstream application. One exception is a CDC ingestion pipeline,\nwhich in reality is a streaming ingestion pipeline.\n If we are receiving messages one by one, we can no longer just save them as files to\nour cloud storage landing area—this would mean creating a file for each message!\nStreaming systems usually receive messages at a very high rate, so saving them directly\nto cloud storage wouldn’t work from a performance perspective. This is where the fast\nstorage from our data platform architecture comes into play.\n Let’s imagine that your application development team is already using Kafka as a\nmessage exchange platform for their microservices. This Kafka cluster is either\ndeployed on premises or in the cloud (not necessarily on the same cloud as your data\nplatform). Figure 4.25 shows how your data platform could be architected to do\nstreaming ingestion using Kafka.\n Applications that write data to Kafka are typically called producers (1). We need an\ningestion application that is capable of reading a stream of messages from Kafka and\npublishing it to our data platform fast storage (2). This step of a stream ingestion pro-\ncess can be implemented in a few different ways. Kafka itself comes with a component\ncalled Kafka Connect (https://kafka.apache.org/documentation/#connect) that\nallows you to read data from different sources and publish to Kafka (source) and also\n\n\n115\nIngesting data from streams\nread from Kafka and publish to a destination (sink). You can use Kafka Connect with\nGoogle Cloud Pub/Sub, AWS Kinesis, or Azure Event Hubs as sinks and in doing so\nestablish replication of messages from your source Kafka to your cloud data platform\nmessage bus (3). This option is the easiest to implement, but it does require changes\nto the Kafka configuration itself. In some cases, you may not be able to do it because\npeople responsible for the existing Kafka cluster might be busy with other work, there\nmight be hardware limitations in your current data center, etc.\n An alternative to this approach is to create a dedicated application that will read\nmessages from Kafka and publish them to your cloud data platform. This is relatively\neasy to implement because Kafka has libraries for most of the popular programming\nlanguages. We recommend finding an out-of-the-box solution to replicate data from\nKafka into the cloud data platform fast storage because, while it easy to implement a\nsimple consumer application for Kafka, you will still need to deal with error handling,\nproper logging strategies, and scaling your application to multiple machines if you are\ndealing with high-volume data. Some of the existing ETL tools support Kafka as a\nsource, so it’s worth checking what is available to you. On Google Cloud Platform you\ncan also use Cloud Dataflow to implement this step in the ingestion process. With\nCloud Dataflow, you can balance having full control of the implementation details,\nwhile getting out-of-the-box scalability, logging, and error handling. \n Once we have data in the fast storage of our cloud data platform, we need to deliver\nit into our cloud data warehouse as well as into our slow storage for archiving and other\nFigure 4.25\nIngesting streaming data into the data platform\nIngestion\napplication\nCloud data\nwarehouse\nFast storage\nSlow storage\nGCP \nPubSub\nAWS \nKinesis\nAzure \nEvent Hubs\nIngestion\napplication\nGoogle\nCloud Storage\nAWS S3\nAzure Blob\nStorage\n2. An ingestion application \n    reads a stream of messages \n    from Kafka and publishes it \n    to data platform fast storage.\n4. Read messages from the fast \n    store and write them to the\n    cloud warehouse and cloud \n    storage using cloud-native \n    services like Azure Stream\n    Analytics, Google Dataflow, \n    or AWS Kinesis Data Firehose.\n1. Producers are applications\n    that write data to Kafka.\n3. Kafka Connect allows you to read data from different sources and publish\n    to Kafka (source) and also read from Kafka and publish to a destination\n    (sink). Use Kafka Connect with Cloud Pub/Sub, AWS Kinesis, or Azure Event\n    Hubs as sinks to replicate messages from your source Kafka to your cloud data\n    platform message bus. \n\n\n116\nCHAPTER 4\nGetting data into the platform\npurposes (4). For this we need another application that will read messages from fast\nstore and write them to the cloud warehouse and cloud storage. Fortunately, this can be\nmostly handled by cloud-native services. On Azure, for example, you can use Azure\nStream Analytics, which can read messages from Azure Event Hubs and write them into\nAzure SQL Warehouse. On Google Cloud Platform, you can use Cloud Dataflow to\nread messages from Cloud Pub/Sub and write them into BigQuery. On AWS, similar\nfunctionality can be implemented with Kinesis Data Firehose. Kinesis Data Firehose\nwill read messages from AWS Kinesis and write them into a Redshift warehouse. While\nit is possible to implement similar applications yourself using existing libraries, we\nstrongly recommend using one of these native services—they provide better integra-\ntion with corresponding message bus systems and various cloud destinations. \nNOTE\nCurrently only Google BigQuery supports actual real-time ingestions\nwhen it comes to cloud warehouses. While both AWS Redshift and Azure SQL\nWarehouse support inserting rows one by one, they still need to batch multi-\nple inserts together to get reasonable performance. This batching is usually\nconfigurable and is easy to tweak using one of the services previously men-\ntioned. We will discuss more about the differences between different ware-\nhouses in chapter 9.\nSome cloud services (Azure Stream Analytics, Google Cloud Dataflow, and AWS Kine-\nsis Data Firehose) can be used to save messages to the regular cloud storage. We need\nto keep in mind that cloud storage is not optimized for dealing with lots of small files,\nso if we were to write each message as a separate file, the performance of our pipeline\nwould be really bad. Instead, a common approach is to batch messages together and\nwrite them to storage as a single large file. We recommend keeping the size of the files\nat several hundreds of MBs or more. This may not always be possible if your ingestion\nstream is low volume. You will need to find a balance between the resulting size of the\nfile and the time it takes to accumulate that volume of messages. \nNOTE\nAs mentioned before, AWS and Azure Cloud Warehouse have to batch\nmessages before inserting them into tables. The batch size doesn’t have to be\nthe same as the batch size for writing files to the cloud storage. Usually you\nwould want to keep the batch size for the warehouse as small as performance\npermits to get the latest data available to the users in near real time. Batch\nsize for the cloud storage would be bigger to get larger resulting files. \nIn some cases, you will not have an existing Kafka service to read messages from in\nreal time. For example, imagine your development team is implementing a brand-new\napplication, and they want to be able to push messages into a cloud data platform for\nfurther analysis and archival. In this case, we can make the fast storage from our cloud\ndata platform available to this application. \n The process is very similar to what we have described previously, except we don’t\nneed an ETL tool or a dedicated ingestion application that will simply transport mes-\nsages from Kafka into our cloud data platform. Figure 4.26 shows the steps involved.\n\n\n117\nIngesting data from streams\nInstead of an ETL tool or a dedicated ingestion application that transports messages\nfrom Kafka into our cloud data platform, we will expose our fast storage to the appli-\ncations that need to publish messages (1). These applications will need to implement\ncustom code that will write the messages to the fast storage using a client library that is\nprovided by the cloud vendor. \n Once data is published into the fast storage, the process is the same—use one of\nthe cloud services to write data into the cloud warehouse and cloud storage (2). \n4.4.1\nDifferences between batch and streaming ingestion\nThere are certain things that are simpler in the streaming ingestion pipelines than in\nbatch ones and vice versa. We would like to outline some of the key differences in this\nsection. \n One thing that you don’t need to worry about in the streaming ingestion is tracking\nwhich data you have already ingested and processed and which data is new. If you recall\nour discussion in the previous sections, such tracking is critical for RDBMSs and file\ningestion. With streaming pipelines this is less of a concern. Tracking which messages\nhave been consumed and which have not is a common feature for a message bus system\nsuch as Kafka or any of the similar streaming cloud services. Our ingestion application\nwill keep reading messages as they come in, and it will periodically checkpoint the offset\nthat it has last processed back to Kafka itself. The offset is a sequence number for each\nmessage. This allows our ingestion application to easily recover from crashes or\nplanned outages—it will read the latest processed offset from Kafka and will continue\nreading messages from this offset forward. Similar capabilities exist for cloud services\nlike Google Cloud Pub/Sub, Azure Event Hubs, and AWS Kinesis. \n This offset tracking approach has an unfortunate side effect. If our ingestion\napplication needs to write the latest processed offset back to Kafka for every processed\nmessage, we will significantly impact the performance of our pipeline, because we will\nnow need to perform one write operation for every read operation. In real applications,\nFigure 4.26\nIngesting streaming data directly into the cloud data platform\nCloud data\nwarehouse\nFast storage\nSlow storage\nIngestion\napplication\n2. Use GCP PubSub,\n    AWS Kinesis or\n    Azure Event Hubs\n    to write data into\n    the cloud warehouse\n    and cloud storage.\n1. To ingest streaming data\n   directly into the cloud\n   data platform, expose fast\n   storage to the applications\n   that will write messages to\n   fast storage.\nGCP \nPub/Sub\nAWS \nKinesis\nAzure \nEvent Hubs\nGoogle\nCloud Storage\nAWS S3\nAzure Blob\nStorage\n\n\n118\nCHAPTER 4\nGetting data into the platform\noffsets are usually saved periodically in configurable time intervals. This means that if\nyour ingestion application crashes, or if there is a problem with Kafka itself, then when\nthe ingestion application is back online, it will read some of the messages it has already\nprocessed. This situation happens in production systems more often than you\nmay think. \nNOTE\nFor more information about Kafka, including implementation details\nand various strategies to implement applications using Kafka, we recommend\nhttps://www.manning.com/books/kafka-in-action.\nAs such, your streaming ingestion pipeline needs to be able to deal with duplicate\nmessages. Some streaming data processing systems, such as Google Cloud Dataflow,\nhave built-in functionality that allows you to deduplicate incoming data. In other\ncases, you will need to implement a specific step in the processing layer of your cloud\ndata platform to deduplicate incoming data. Usually this is achieved by having a\nunique identifier included with every message. We will talk more about deduplicating\ndata in chapter 5. One thing to keep in mind is that to implement an efficient dedu-\nplication strategy, each message should have a unique identifier of some sort. \n Another important consideration for building a streaming pipeline is the poten-\ntially large volume of incoming messages. If you are using an off-the-shelf ETL tool or\na custom application to implement step 1 of the streaming ingestion pipeline, which\nis moving messages from Kafka into the data platform fast storage, then you need to\nevaluate how scalable this approach is. If your solution is only capable of processing\nincoming streaming messages on a single virtual machine, then it’s very likely that you\nwill hit scalability limits. To make sure the first step in the pipeline doesn’t become a\nbottleneck, you need to use one of the distributed message processing systems, such as\nKafka Connect, Spark Streaming, or Apache Beam. \n Unlike batch sources, such as files or databases, where data is usually stored for the\nlong term in message bus systems, messages are usually purged on a regular basis. Rea-\nsons for this are usually related to the volume of incoming messages; a reliable mes-\nsage bus needs to replicate messages to multiple machines, so we are potentially\nlooking at lots of storage and compute resources to store these messages over a long\nperiod of time. It’s not uncommon for messages in Kafka to have an expiration period\nof about a week or less. This is, of course, configurable and depends on the volume of\nmessages and the size of the cluster. In cloud systems, such as Cloud Pub/Sub, for\nexample, messages can be purged automatically once a consumer application\nacknowledges receiving it. \n These expiration policies make reprocessing streaming data a more complicated\ntask than reprocessing data from batch sources. To implement a robust reprocessing\npipeline, we will need to utilize archive data that we have saved to cloud storage as one\nof the steps of the ingestion pipeline. But since our downstream transformations\nexpect a stream and not a file, we will need to implement a step that reads a file,\nbreaks it down into individual messages, and pushes those messages through the\n\n\n119\nIngesting data from streams\nstreaming pipeline again. This way new incoming messages and reprocessed messages\nwill be treated the same. \n4.4.2\nCapturing streaming pipeline metadata\nAs you may have realized by now, streaming data has two representations in our cloud\ndata platform design. The first one is a real-time stream that is handled by our plat-\nform message one at a time and is used for real-time ingestion into the warehouse,\nreal-time analytics, or both. When it comes to capturing pipeline metadata for stream-\ning, these two representations need to be treated differently. \n Important metadata for streaming pipelines is similar to what we have discussed in\nthe CDC section earlier in this chapter. To be able to quickly assess the health of the\npipeline, we need to measure how many messages we processed over a fixed period of\ntime. This time window will depend on your particular scenario and could be anything\nfrom a few minutes to hours. Consider your overall message volume to decide which\ntime window for metrics collection works best. If you only receive a few messages every\nminute, then a larger metrics window, such as an hour, will give you a better baseline\nthat you can implement your monitoring from. For higher-volume streams, you will\nneed to use a smaller window to make sure you don’t miss any spikes or drops that may\nindicate a pipeline issue or an upstream application problem. In fact, we have seen\nexamples in real cloud data platforms where a monitoring check for the volume of mes-\nsages in a streaming pipeline detected an outage of the upstream application, before\nthe application itself detected it—the volume of messages that the pipeline observed\ndropped lower than a baseline, and an alert was triggered immediately. \n In figure 4.27 we have two streaming sources: one is a clickstream from our web\napplication and another some sort of IoT sensor. As you can see, different sources can\nuse different time windows for capturing message throughput. \nAs we have mentioned previously, a streaming pipeline will also flush batches of mes-\nsages to the regular cloud storage periodically. The time window for this operation will\ntypically be bigger than the one used for capturing statistics for the streaming pipe-\nline—we need to choose it in such a way that we don’t end up with lots of small files. \n In figure 4.28 we are writing batches of messages into Google Cloud Storage (GCS)\nand assume that messages arrive in JSON format. We use a larger time window for a\nstream with less volume and a smaller window for a higher throughput stream. We also\nassume message size is big enough to result in files that are at least a hundred MBs. \nsource_name\nstart_ts\nend_ts\nduration\nmessages\nweb_events\n2019-05-02 12:00:00\n2019-05-02 12:05:00\n300\n1038\nsensor1\n2019-05-02 12:00:00\n2019-05-02 12:01:00\n60\n7800\nFigure 4.27\nMetadata for a streaming pipeline\n\n\n120\nCHAPTER 4\nGetting data into the platform\nNote that we are saving the files into a dedicated archive bucket on Google Cloud\nStorage and using a date-partitioned folder structure. We will talk more about data\norganization on cloud storage in chapter 5.\n4.5\nIngesting data from SaaS applications\nSaaS applications are an increasingly popular data source for all types of analytics\napplications. It’s hard to find a business today that doesn’t use a SaaS CRM or a mar-\nketing management solution. Extracting data from SaaS sources, though, comes with\nits own set of unique challenges. \n Developers of SaaS applications will never give you direct access to the underlying\ndatabases, for security and scalability reasons. This leaves us with two common ways to\nextract data: APIs and file exports. File exports are less and less common these days,\nbecause APIs provide more flexibility for consumers and more control for the SaaS\napplication owners.\n Most of the SaaS applications today provide a REST API over HTTP. This means that\nyou as an API consumer can make a simple call to an HTTP or HTTPS URL, provide\nsome parameters that this API requires, and get a response back from the SaaS appli-\ncation. Responses are usually wrapped in JSON format, but XML is also used sometimes.\n Figure 4.29 shows the steps involved in ingesting data via a REST API over HTTP.\nAs with all other ingestion scenarios described in this chapter, we need to have some\nkind of ingestion application that will be responsible for interacting with the SaaS\napplication and saving results back to our cloud data platform. First of all, this inges-\ntion application needs to authenticate somehow with the SaaS side to prove that it is\nallowed to access the data behind the API(1). Different SaaS providers use different\nauthentication methods: username/password combinations or authentication tokens,\nand today using the OAuth protocol is an increasingly popular authentication mecha-\nnism for web applications (https://oauth.net). You will need to deal with whichever\nmethod the SaaS provider uses.\n Once the ingestion application has proven that it has permission to access the data,\nit can start making calls to the API to fetch data (2). These calls are HTTP calls to spe-\ncific URLs on the SaaS side, sometimes with parameters that specify which specific\nobjects to fetch, etc. In figure 4.29, we are fetching full customer data from some hypo-\nthetical SaaS service. The design of the URLs (or API endpoints) is specific to each\nsource_name\npath\nstart_ts\nend_ts\nduration\nmessages\nweb_events\ngs://archive/web_events/2019/06/08/webe\nvents_01.json\n2019-05-02\n12:00:00\n2019-05-02\n12:15:00\n900\n35231\nsensor1\ngs://archive/sensors/sensor1/2019/06/08/s\nensor1_05.json\n2019-05-02\n12:00:00\n2019-05-02\n12:05:00\n300\n32265\nFigure 4.28\nMetadata for streaming pipeline archive on cloud storage\n\n\n121\nIngesting data from SaaS applications\nSaaS provider, so you will need to study the API documentation to understand which\nendpoints exist, what data they expose, and which parameters they accept. \n Finally, when the ingestion application receives a response back from the SaaS ser-\nvice, it needs to save the data into the cloud storage for further processing (3). Typi-\ncally, a web API would return data as a JSON document or a collection of documents. \n It may look like building an ingestion pipeline for a SaaS application is simple.\nAfter all, we just need to make a series of HTTP calls and save resulting documents to\ncloud storage. All programming languages have libraries for making parameterized\nHTTP calls, and many of the cloud or third-party ETL tools have components to\naccomplish the same task. In reality, there are many challenges with implementing a\nrobust SaaS ingestion pipeline. \n4.5.1\nNo standard approach to API design\nThere currently are no standards on how SaaS APIs are designed. Each provider\ncomes up with their own set of endpoints and required parameters. For example, in\none SaaS application you may be able to fetch a full list of existing customers, together\nwith all the details on each customer. Another provider may decide to not allow full\ndata exports (to prevent load on their systems) and will give you two endpoints\ninstead: one for fetching only customer unique identifiers and another for fetching\ndetailed information for a given customer identifier. This makes an ingestion pipeline\nfor a specific SaaS application tailored to the design SaaS developers decided to go\nwith. If you need to deal with several SaaS sources, then you will need to build an\ningestion pipeline that can understand dozens of API endpoints. \nIngestion\napplication\nSaaS\napplication\nCloud data platform\nSlow storage\nThe ingestion application is responsible for interacting with the SaaS\napplication and saving results back to the cloud data platform.\n2. Once the ingestion application has proven that it has\n    permission to access the data, it can start making calls to the\n    API to fetch data using HTTP calls to specific URLs on the \n    SaaS side—GET https://some-saas-app/data/customers/all.\n1. The ingestion application authenticates\n    with the SaaS application to prove that\n    it is allowed to access the data behind\n    the API.\n3. When the ingestion application\n    receives a response back from\n    the SaaS service, it will save the\n    data into the cloud storage for\n    further processing.\nFigure 4.29\nExample of an API request to a SaaS application\n\n\n122\nCHAPTER 4\nGetting data into the platform\n4.5.2\nNo standard way to deal with full vs. incremental data exports\nUnlike ingesting data from databases, there is no common way to implement full or\nincremental exports from a SaaS source. Again, you are at the mercy of the SaaS API\ndevelopers. Some SaaS systems we have worked with provide no incremental ingestion\ncapabilities, because there is no API to tell which objects inside SaaS have changed.\nOthers may only provide an incremental data export API. Mature platforms, such as\nSalesforce, would typically give you two options. One is an API endpoint to fetch a list\nof all unique identifiers for a given object and then a second API to fetch details about\na particular object using an identifier as an input parameter. Another option is a dedi-\ncated API to fetch identifiers only for new and updated objects using start and stop\ntimestamps as input parameters. You then call a “fetch details” endpoint with these ids\nand implement an incremental ingestion pipeline. \n4.5.3\nResulting data is typically highly nested JSON\nJSON is a prevalent data exchange format for web applications, so it’s not surprising\nthat most of the SaaS APIs return JSON data. When it comes to performing analytics\ntasks and specifically loading SaaS data into a cloud data warehouse, there are several\nthings to consider. First of all, not all cloud warehouses support storing and querying\nnested data. Currently, only Google BigQuery and Azure Cloud Warehouse have sup-\nport for JSON-like data. If you are planning to use Redshift, then you will need to\nimplement an additional transformation process, which will unnest JSON documents\ninto separate tables. Implementing such a process may be required even if your ware-\nhouse does support nested data structures. From our experience, many analysts find it\nhard to navigate a complex JSON document with several levels of nesting and rather\nprefer these documents to be broken out into multiple “flat tables” that are easier for\na person to reason about. What’s easy for a computer program to generate is not\nalways easy for a human to work with. \n Given the challenges, our recommendation when implementing an ingestion pro-\ncess for a SaaS application is to carefully evaluate the amount of development (and\nfuture maintenance) you will need to incur. If you are only dealing with one SaaS\napplication, which is rare these days, and the API of that application is relatively\nmature, then you may decide to implement your own ingestion application. \n If, on the other hand, you are dealing with multiple SaaS sources and envision the\nnumber of these sources to increase over time, then we would recommend looking at\noff-the-shelf products and services that can help you with integrating SaaS data into\nyour cloud data platform. There are a number of SaaS services that offer extracting\ndata from other applications and saving them to the destination of your choice. One\nof the good examples of such a service is Fivetran (https://fivetran.com/), which sup-\nports most of the popular SaaS applications on the market and can save extracted data\ninto the cloud platform of your choice. For some SaaS services, it also can unpack\nnested JSON structures into a set of relational tables that can be easily queried by data\nanalysts and BI tools. \n\n\n123\nNetwork and security considerations for data ingestion into the cloud\n When it comes to capturing metadata and statistics for the SaaS data ingestion\npipelines, similar rules as for RDBMS and files apply. Today, most SaaS applications\nonly provide access to data in batch mode. This means that you would want to capture\nat least the following metadata items:\nName of the SaaS source\nName of the specific object in this source (customers, contracts, etc.)\nIngestion start and end times\nNumber of objects fetched\n4.6\nNetwork and security considerations for data ingestion \ninto the cloud\nWhen implementing data ingestion pipelines for your cloud data platform, one of the\nthings you will need to plan for is how your data platform will actually connect to the\ndata source. You may have sources that live on the on-premises network, or you may\nhave sources deployed into a different cloud. While going into the details of the cloud\nnetworking is beyond the scope of this book, we will outline several of the most com-\nmon scenarios that we have seen in real platforms. \n4.6.1\nConnecting other networks to your cloud data platform\nA very common scenario is to have sources that are deployed either on premises or in\nanother cloud. “Another cloud” here could mean a different cloud provider than the\none you are using for your data platform, or a different project with the same cloud\nprovider. For your ingestion pipelines to work, there must be some way for your inges-\ntion application to connect to these data sources. It’s very unlikely that you would\nhave a database that has direct access to the internet. This means that some kind of\nsecure connection between cloud and on-premises resources must be established. All\nthree major cloud providers offer a way for you to deploy your cloud resources into\nwhat’s commonly referred to as a virtual private cloud or VPC. Azure calls it a virtual\nnetwork, but we will use VPC terminology for simplicity. \n What a VPC allows you to do is to restrict access to your cloud resources using a vir-\ntual network construct. If you deploy components of your cloud data platform into a\nVPC, then only resources inside that VPC can communicate with each other. You can\nbreak down your VPC into different subnets to even further limit which resources in\nyour data platform can communicate with each other. For example, it’s a good prac-\ntice to isolate your ingestion layer into its own subnet, because the ingestion layer\nalways needs to deal with external resources. This is an oversimplification, of course,\nbecause there is more to cloud networking configuration than we can describe in this\nbook. For the purpose of this section, let’s assume that your cloud data platform is\ndeployed into its own VPC and has a dedicated subnet for ingestion components, as\nshown in figure 4.30. \n \n \n\n\n124\nCHAPTER 4\nGetting data into the platform\nIn cases when data sources and the data platform itself are deployed on two different\nnetworks, one of the ways to establish connectivity between the two is to use a VPN\ngateway. All cloud vendors have some version of a VPN gateway service that can be\ndeployed into your VPC and connected to a gateway on premises or on the cloud.\nUsing a VPN connection makes it safe to transfer data over the internet, since all data\nis encrypted in-flight. \nNOTE\nIf you are planning to transfer large volumes (hundreds of GBs per\nday) of data from an on-premises location to the cloud, then it’s worth\nexploring if a direct connection can be used. All cloud providers offer a ser-\nvice that allows using a dedicated connection between certain connections\nand their cloud: AWS Direct Connect, Azure ExpressRoute, and Google\nCloud Interconnect. \nIf you are using some of the cloud providers’ PaaS solutions, such as Azure Event\nHubs or Google Cloud Storage as data sources, then you need to understand how\nthose sources are deployed. Often, these services are deployed as global services,\nmeaning they don’t belong to your VPC. In this case, you will need to rely on cloud\nprovider authentication and encryption capabilities to establish a secure connection\nbetween the sources and your data platform. Today, we see a toward for cloud vendors\nmaking these services deployable into a customer VPC to provide additional control\nand security. For example, Azure Blob Storage access can be limited to a certain\nvirtual network. If you need to use such a service as a data source, then the same\npattern we discussed is applicable—you need to establish VPN connectivity between\nthe two networks. \n If you are dealing with SaaS sources, then connectivity will be established over the\ninternet. Today all major SaaS providers provide APIs over HTTPS, meaning that com-\nmunication between the SaaS application and your ingestion application will be\nencrypted. Since SaaS applications are globally available, there is no need to establish\na dedicated network connectivity. \nIngestion\nsubnet\nRest of the\nplatform\nresources\nCloud data platform VPC\nVPN\nGateway\nVPN\nGateway\nRDBMS\nFTP\nOn-premises network\nInternet\nFigure 4.30\nConnecting a cloud data platform to the on-premises network\n\n\n125\nNetwork and security considerations for data ingestion into the cloud\nSummary\nIngesting data is more complex than many expect as the modern data platform\nmust support ingestion from a variety of different data sources (typically\nRDBMSs, files, SaaS via API, and streams), often at high velocities and in a con-\nsistent, uniform way across different source data types.\nWhen ingesting from an RDBMS, key considerations include the need to map\ncolumn data types from the source database into the cloud data warehouse–\nsupported data types, the need to automate your ingestion process to reduce\nmanual effort and to improve accuracy, and the need to deal with constantly\nchanging source data. \nThere are two primary ways to establish an ongoing ingestion process from data-\nbases: using a SQL interface (full-table ingestion that can be slow and costly, and\nincremental table ingestion that can be more performant and cost effective but\nmay still not be as timely as is needed) or using change data capture techniques,\nwhich is more complicated and may add cost but addresses the challenges associ-\nated with performance at scale. CDC solutions are available from all major RDBMS\nvendors, as open source, and increasingly as cloud services.\nWhen ingesting from files, key considerations include the need to parse different\nfile formats such as CSV, JSON, XML, Avro, etc.; the need to deal with source\nschema changes, which are common in files; and the need to deal with snapshots\nor data increments that can be delivered as either a single file or multiple files.\nYour ingestion process must be very resilient to change and must be able to deal\nwith many different edge cases. \nMost SaaS data can be accessed using a REST API, but a lack of standardization\nfor data access and the resulting variety of API access methods requires these\nkey considerations when building an ingestion pipeline for SaaS data: the need\nto implement and manage different pipelines for each source; the need to do\ndata type validation, as most SaaS APIs have minimal available data type infor-\nmation, and the need to adjust your pipeline for incremental versus full data\nloads, depending on what each provider supports.\nFor data streams, your ingestion pipeline must be able to decode messages into\na consumable format; deal effectively with duplicate data because streaming data\nsystems allow the same messages to be consumed multiple times; resolve multiple\nversions of the same message; and scale to accommodate high-volume data. \nTo implement ingestion pipeline quality control and monitoring, you will want\nto capture at least these important statistics in your metadata layer: the name of\nthe source database server (and IP address if possible); the name of the data-\nbase (or schema name depending on the database vendor) and the name of the\nsource table; the number of rows ingested per table for each ingestion (if batch\ningestion from a database); the duration of each ingest (start and end time-\nstamps); and how many messages processed over a fixed period of time (if\nstreaming data).\n\n\n126\nCHAPTER 4\nGetting data into the platform\n4.7\nExercise answers\nExercise 4.1:\n 2—It’s hard to identify rows that were deleted.\n 4—It causes too much duplicate data to be stored in the platform.\nExercise 4.2:\n 4—You need to have a last_modified column in each of the source tables.\nExercise 4.3:\n 3—CDC includes all changes to a given row.\nExercise 4.4:\n 1—When a source system already has a predefined folder structure.\n\n\n127\nOrganizing\n and processing data\nWe will introduce a number of concepts, such as the difference between common\ndata processing steps (such as file format conversion, deduplication, and schema\nmanagement) versus custom business logic (such as the rules each company\nchooses to apply to transform their data for a unique use case).\nThis chapter covers\nOrganizing and processing data in your cloud data \nplatform\nUnderstanding the different stages of data processing\nDiscussing the rationale for separating storage from \ncompute \nOrganizing data in cloud storage and designing a data flow\nImplementing common data processing patterns \nChoosing the right file formats for archive, staging, and \nproduction\nCreating a single parameter-driven pipeline with common \ndata transformations \n\n\n128\nCHAPTER 5\nOrganizing and processing data\n We will walk through how to organize your data in storage, following the data jour-\nney through landing, archiving, staging, and production areas. We’ll explain the impor-\ntance of using batch identifiers to make it simpler to trace the data journey through the\nstorage areas and the warehouse and make debugging and lineage tracking easier.\n We will talk about the use of different file formats for the different storage areas\nand the importance of standardizing on binary formats in staging and production for\ncompression, performance, and common schemas.\n Last, we’ll explain how we can scale our common data processing by designing\nflexible and configurable pipelines, using orchestration.\n As covered in chapter 3, the processing layer, highlighted in figure 5.1, is the heart\nof the data platform implementation. This is where all the required business logic is\napplied and all the data validations and data transformations take place. The process-\ning layer also plays an important role in providing ad hoc access to the data in the data\nplatform.\nThe processing layer, shown in figure 5.1, is responsible for reading data from storage,\ntransforming it, and then saving it back to storage for further consumption.\nTransformations can include the implementation of common data cleanup steps, such\nas ensuring all date fields follow the same format, or the implementation of specific\nbusiness logic, such as joining two datasets together to produce the data needed for a\nspecific report. This layer should be able to work with both slow and fast data storage.\nThis means that the services or frameworks that we choose to implement in this layer\nshould have support for both batch processing of files stored in the slow storage as well\nas “one message at a time” or streaming processing from fast storage. This chapter will\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 5.1\nThe processing layer applies business logic to data—transforming and validating it.\n\n\n129\nProcessing as a separate layer in the data platform\ndescribe processing principles that can be applied to both batch and streaming data,\nbut will focus mostly on batch processing. We will go deep into streaming data in\nchapter 6.\n The processing layer should be able to perform the following tasks: read data in\nbatch or streaming modes from storage, apply various types of business logic, and pro-\nvide a way for data analysts and data scientists to work with data in the data platform in\nan interactive fashion.\n5.1\nProcessing as a separate layer in the data platform\nWe’ve talked at length about the benefits of separating storage from compute in our\ndata platform architecture, so we won’t go over that again, but we will touch on an\nongoing debate we see happening over and over again—whether you should do com-\npute in the lake portion of the data platform or in the data warehouse. While propo-\nnents of using SQL in the data warehouse to apply business logic readily agree that\nthis violates the principles of a layered design, they point to other reasons why they\nthink this is a good idea. We felt it was worth sharing their view because you are likely\nto run into it when you propose your layered design.\n Separating storage from compute is a key tenet of a layered cloud data platform\ndesign. It brings scalability, cost savings, flexibility, and maintainability. But established\nways of operating are slow to change—traditional data warehouse architects may advo-\ncate for processing in the data warehouse, while modern cloud platform design dic-\ntates that processing should happen outside the data warehouse.\n Table 5.1 summarizes the pros and cons of each position—doing processing in the\ndata warehouse using SQL versus doing processing in the lake using a framework like\nSpark.\nNOTE\nSpark is available as a managed service from all three of the public\ncloud vendors. In Microsoft Azure, it is Azure Databricks; in Google Cloud, it\nis Dataproc; and in AWS, it is EMR (Elastic MapReduce).\nTable 5.1\nProcessing data in the warehouse versus in the lake\nProcessing data in the data lake (Spark)\nProcessing data in the data warehouse \n(SQL)\nFlexibility\nProcessing done in the data lake brings \nadditional flexibility because outputs can \nbe used not just for data served in the \ndata warehouse but also for data that can \nbe delivered to or consumed by other \nusers and/or systems.\nOutputs of data processing are typically \nrestricted to use in the data warehouse. \nDeveloper \nproductivity\nOnce trained, developers will appreciate \nthe power and flexibility of Spark with its \nsophisticated testing frameworks and \nlibraries to accelerate code delivery.\nWhile not designed as a programming lan-\nguage, SQL’s popularity means that find-\ning people who know it is relatively easy, \nso using SQL instead of learning Spark \ncan mean a shorter time to value.\n\n\n130\nCHAPTER 5\nOrganizing and processing data\nCreating a modern data platform is a significant change for any organization, and\nthere are times when the need to minimize change, i.e., use the more popular SQL\nfor processing, may outweigh the more “purist view” of using new frameworks such as\nSpark. SQL talent is readily available and is likely already in your organization. But as\nyour platform scales, the challenges with using SQL for processing in the data ware-\nhouse will continue to mount.\n In our experience, doing processing in the data warehouse is one of those things\nthat “seemed like a good idea at the time.” It is likely to get you to a solution fairly\nquickly, and for small platform solutions it might be fine for the mid to long term, but\nif you want to take full advantage of the flexibility of a cloud data platform, it’s not the\nbest solution. You will want to do your processing outside of the data warehouse for all\nthe reasons stated.\nData \ngovernance\nProcessing data as close to the source as \npossible supports more consistent use of \ntransformed data across different sinks \nand reduces the risk of multiple people \ntransforming data in the sink and defining \ndata differently.\nProcessing data in the data warehouse \ncan support a data governance program, \nbut if processing is also done in the data \nlake, conflicts on data definitions may \nemerge.\nCross-platform\nportability\nSpark produces fully portable code that is \nindependent of the cloud vendor. Chang-\ning from one data warehouse to another is \neasier if transformations don’t also need \nto be changed. No migration and minimal \ntesting would be involved.\nTransforms done in ANSI-SQL are sup-\nported by all of the major cloud provider’s \ndata warehouse offerings and are porta-\nble, provided that no extra cloud vendor–\nspecific add-ons have been added. Work \nwill be involved to migrate and test code.\nPerformance\nNo amount of processing will impact data \nwarehouse users when processing is \ndone outside the data warehouse. \nMost modern cloud data warehouses pro-\nvide great performance out of the box, but \nsome may suffer as processing load \nincreases.\nSpeed of \nprocessing\nReal-time analytics is always possible.\nReal-time analytics is possible in some \ncloud data warehouses but involves \nmultiple steps and/or products.\nCost\nWhen cloud data warehouse vendors \ncharge for processing, it will be much less \nexpensive to do processing in the lake.\nDepending on the data warehouse \nselected and the commercial terms asso-\nciated with it, processing in the data ware-\nhouse can be expensive.\nReusability\nReusable functions and modules are read-\nily available in Spark. All processing jobs \nare available, not just to deliver data to \nthe cloud data warehouse but also to \ndeliver processed data to other destina-\ntions, an increasingly popular use of cloud \ndata platforms.\nWhen available in cloud data warehouses, \nstored procedures and functions can pro-\nvide reusable code.\nTable 5.1\nProcessing data in the warehouse versus in the lake (continued)\nProcessing data in the data lake (Spark)\nProcessing data in the data warehouse \n(SQL)\n",
      "page_number": 102
    },
    {
      "number": 5,
      "title": "Organizing and processing data",
      "start_page": 150,
      "end_page": 179,
      "detection_method": "regex_chapter",
      "content": "131\nData processing stages\n We have seen blended solutions. For example, when the first use case for a data\nplatform is to replace a traditional data warehouse, repurposing existing SQL code\ncan be faster than redoing transforms in Spark. Once the data warehouse has been\nmigrated, the transform jobs can be moved to the data lake.\n For the purposes of this chapter, we will assume you are designing a cloud data\nplatform with storage and compute clearly separated.\n5.2\nData processing stages\nWhen thinking about processing data in a cloud data platform, it’s a good idea to visu-\nalize the data flowing through several stages. At each stage, we apply some data trans-\nformation and validation logic. This increases the “usefulness” of data as it is\ntransformed from raw and unrefined data coming from the data source to well-\ndefined and validated data products that can be used for analysis or made available to\nother data consumers. \n Each stage in the diagram in figure 5.2 consists of two things: a storage component\nwhere data for the stage is stored long term (raw data area, staging area, and produc-\ntion area), and a data processing component that reads data from storage, applies\nsome processing rules, and saves data to the next storage area. This data processing\ncomponent is implemented as a job using a distributed data processing framework,\nsuch as Spark. Different jobs are coordinated together using an orchestration layer in\nour data platform. \nRaw incoming data\nDeduplication\nStaging area: data ready for consumption\nEnterprise-level\nquality check\nEnterprise-level\ndata transformation\nValidation rule 1\nValidation rule 2\nValidation rule 3\nValidation rule 4\nTransformation job 1\nTransformation job 2\nTransformation job 3\nData product 2\nData product 1\nStandardize file formats and resolve schema differences\nProduction area: contains final data products\nCommon data\nprocessing steps\nBusiness\nlogic−specific\nprocessing\nsteps\nFigure 5.2\nData flows through different stages in the platform with processing applied at different \nsteps.\n\n\n132\nCHAPTER 5\nOrganizing and processing data\nData processing tasks can typically be divided into two broad categories: common data\nprocessing steps and business logic–specific steps. Common data processing steps are\nsteps that you apply to all data coming in from all data sources. Examples would be\nconverting file formats to a single, unified standard and making sure schema differ-\nences between incoming data and existing data are resolved. Common data process-\ning steps can also be jobs that deduplicate data and apply standard quality checks. An\nexample of this could be ensuring that all fields containing zip codes are valid or that\nall dates are formatted using the same pattern. \n In addition to common processing steps, each analytics use case will require its\nown set of transformations and validations that are specific to the use case. For exam-\nple, if you are preparing a data set for a marketing campaign efficiency report, you\nmight want to include only campaigns that produce a certain number of impressions.\nOne of the key benefits of the cloud data platform is that you can implement and exe-\ncute these custom validations and transformations for hundreds of potential reports,\neach running in its own isolated environment. In this new world, there is no need to\nworry about sharing compute or storage resources.\n In this section of the chapter, we will focus on how to plan and design two common\ntransformations steps: file format conversion and data deduplication. We will talk\nmore about schema management in chapter 8. But first, we’ll start with describing\nhow to organize your cloud storage and implement a data flow to support the data\njourney through multiple stages. This chapter focuses on batch data processing, but\nwe will explore real-time processing and analytics and how they differ from the batch\napproach in chapter 6.\n5.3\nOrganizing your cloud storage \nIt may not sound like it’s important, but having a set of consistent, clear principles on\nhow to organize your data in cloud storage is very important. It will allow you to build\nstandardized pipelines that follow the same design with regard to where to read data\nfrom and where to write data to. This standardization will make it much easier for you\nto manage your pipelines at scale. It will also help your data users search for data in\nthe storage and understand exactly where to find the data they need. \n Over the course of having implemented multiple cloud data platforms for compa-\nnies in a wide range of industries, we have arrived at a storage organization pattern\nthat satisfies most of the use cases. Figure 5.3 will walk you through it.\n \n \n \n \n \n \n \n\n\n133\nOrganizing your cloud storage\n1\nLanding area—When data arrives from the ingestion layer, it is saved into a land-\ning area where all incoming raw data resides until it gets processed. This land-\ning area is a transient area, meaning that data is not stored there for the long\nterm.\n2\nStaging area—Next, raw data goes through a set of common transformations:\nmaking sure it conforms to existing schemas for this data source, converting it\ninto a common Avro binary format, and applying any organization-level data\nquality checks. Once these steps have been successfully applied, data is saved\ninto a staging area. For end users and data processing jobs, data in a staging\narea satisfies some basic quality requirements, there are no major issues with it,\nand it is ready to be used. \n1. Data arriving from the ingestion layer is saved into a landing area where all incoming raw data resides \nuntil it gets processed. Note that the ingestion layer is the only layer that can write to the landing area.\n2. Next, raw data goes through a set of common transformations, and then is saved into a staging area.\n3. Raw data is copied from the landing area into an archive area to be used for reprocessing, debugging \npipelines, and testing any new pipeline code. \n4. Data transformation jobs read data from the staging area, apply necessary business, logic and save data \ninto the production area. \n5. An optional “pass-through” job copies data from staging to production and then into the cloud warehouse \nas an exact replica of the incoming raw data to help debug issues with the business logic of other jobs.\n6. Different jobs read data from the staging area and produce data sets to be used for reporting or other \nanalytical purposes. These derived data sets are saved in a dedicated location in the production area and \nloaded into the cloud warehouse.\n7. Each step of the flow must deal with failures, saving data into a failed area of the storage and allowing \ndata engineers to debug the issues. Once addressed, data can be reprocessed by copying it back into \nthe landing area.  \nLanding\nStaging\nFailed\nArchive\nProduction\nData product 1\nData product 2\nData product 3\nCloud data\nwarehouse\nFigure 5.3\nOrganizing data optimally on storage requires a number of steps\n\n\n134\nCHAPTER 5\nOrganizing and processing data\n3\nArchive area—After data is processed and saved into a staging area, raw data\nfrom the landing area should be copied into the archive area. This is an import-\nant step, because it will allow us to reprocess any given batch of data by simply\ncopying it from the archive back into the landing area and letting the pipelines\ndo their job. Data in the archive area can also be used for debugging pipeline\nissues and for testing new pipeline code. Data is only copied into the archive\narea once it has successfully made it to the staging area. There is a separate and\nimportant step in this flow to deal with any kind of failures.\n4\nProduction area—Data transformation jobs read data from the staging area,\napply the desired business logic, and save the transformed data into the produc-\ntion area. At this point, we also convert data from Avro format into Parquet for-\nmat, which is more suitable for analytics use cases. We will talk more about file\nformats and their differences later in this chapter. \n5\nPass-through job—Often viewed as a special case, a “pass-through” job copies data\nfrom staging to production (in Parquet format) and then into the cloud data\nwarehouse without applying any business logic. This job is optional, but having\na data set in the data warehouse and production area that is an exact replica of\nthe incoming raw data can be extremely helpful when debugging any issues\nwith the business logic of other jobs.\n6\nCloud data warehouse and production area—You would typically have many differ-\nent jobs that read data from the staging area and produce data sets to be used\nfor reporting or other analytical purposes. These derived data sets should be\nsaved in a dedicated location in the production area and also loaded into the\ncloud warehouse. \n7\nFailed area—Building a robust data pipeline means you need to be able to deal\nwith all kinds of errors and failures. There might be bugs in the pipeline code,\ncloud resources may fail, or incoming data may not satisfy common data-quality\nrules. After data has been saved into the landing area, each step of the flow\nmust deal with failures by saving data into a failed storage area. This will allow\ndata engineers to more easily debug the issues and find the data that caused\nthem. Once an issue is fixed, assuming it’s a code issue and not a data issue,\ndata can be reprocessed by copying it from the failed area into the landing area. \n5.3.1\nCloud storage containers and folders\nIn the data flow described previously, we referred to different stages of the flow as\n“areas.” Containers and folders are important concepts that you will need to under-\nstand to better organize your data and implement these areas in cloud storage. Differ-\nent cloud vendors use different names for these. AWS and Google Cloud refer to\ncontainers as “buckets,” and Azure uses the actual container name. The term folder is\nused universally across these three providers. \n From a hierarchy perspective, you create a cloud storage container and then\nupload files into specific folders in that container. Each container can host multiple\n\n\n135\nOrganizing your cloud storage\nfolders. Containers have different properties that you can configure. While different\nvendors have different configuration options, the two most common properties you\nwill need to set up for your cloud containers are\nAccess and security—Most vendors allow you to control who can access files on\nstorage and what operations they are allowed to perform at the container level. \nContainer storage tier—Cloud vendors offer different storage tiers with different\nprice/performance characteristics. We will refer to those as hot, cold, and\narchive. The hot storage tier provides the fastest read/write operations, but also\nhas the highest cost for storing data long term. Cold and archive tiers are slower\nbut allow you to store large volumes of data long term for a much lower cost\nthan the hot tier. \nIn our data flow, each area—landing, staging, archive, production, and failed—is\nimplemented as a separate container in cloud storage. Container access security and\nstorage tiers can be configured as shown in table 5.2.\nFOLDER-NAMING CONVENTIONS\nUse folders to further organize data inside containers into a logical structure. Differ-\nent containers in our data platform will have slightly different folder structures, but\nbefore we describe them, we need to introduce some of the common elements that\nwill allow you to organize data and data pipelines in the platform in a logical manner: \nTable 5.2\nConfiguring container access security and storage tiers\nContainer\nPermissions\nStorage tier\nLanding\nOnly ingestion-layer applications are allowed to write to \nthis container. Scheduled pipelines can read data, and \ndata engineers supporting the platform have read/ \nwrite access. Data consumers don’t have access.\nHot. Reads and writes are \nhappening frequently. \nStaging\nScheduled pipelines can read/write data, and data engi-\nneers supporting the platform have read/write access. \nSelected data consumers have read-only access.\nHot. Reads and writes are \nhappening frequently.\nProduction\nScheduled pipelines can read/write data, and data \nengineers supporting the platform have read/write \naccess. Consumers of Parquet formatted data will have \nread-only access.\nHot. Reads and writes are \nhappening frequently.\nArchive\nScheduled pipelines can write data, and data engi-\nneers supporting the platform have read/write access. \nA dedicated data reprocessing pipeline has read-only \naccess. Very few selected data consumers have read-\nonly access. \nCold or archive. Depending on \ndata volume, you may store your \nmore recent data in the cold \narchive container and older data \nin the archive container.\nFailed\nScheduled pipelines can write data, and data engi-\nneers supporting the platform have read/write access. \nA dedicated data reprocessing pipeline has read-only \naccess. Data consumers don’t have access.\nHot. Reads and writes are \nhappening frequently. \n\n\n136\nCHAPTER 5\nOrganizing and processing data\nNamespace—The highest level in our hierarchy, namespaces are used to logically\ngroup multiple pipelines together. In larger organizations that deal with hun-\ndreds of pipelines, a department name or specific initiative can be used as a\nnamespace. For example, you can have a Sales namespace for data and pipe-\nlines used in sales-related reporting or a ProductX namespace that will contain\nall data and pipelines related to a specific product. In smaller organizations, we\nfind that a single namespace with an organization name is enough. It’s worth\nnoting that if you want to provide access to the data in different namespaces to\ndifferent groups of users, then creating a separate storage container per name-\nspace is a better option since it’s easier to assign permissions to containers.\nPipeline name—Each data pipeline should have a name that reflects its purpose\nand is visible in the pipeline logs as well as in storage folders created by the\npipeline. You will have some common pipelines—ones that operate on all data\nin the platform. For example, you will have a pipeline that takes data from the\nlanding area, applies common processing steps, and saves data into the staging\narea. You will also have one for archiving data. Name these pipelines so you can\neasily identify their function. \nData source name—As discussed in the previous chapter, the ingestion layer will\nassign a name to each data source you bring into the platform. This source\nname will be saved in the metadata repository, but it should also be included in\ncloud storage folder names so users and pipelines can easily identify where this\ndata is coming from.\nBatchId—This is a unique identifier for any batch of data that is saved into a\nlanding area. Since the only layer that is allowed to write data to the landing\narea is the ingestion layer, it is the responsibility of the ingestion application to\ngenerate this identifier. A common choice for this type of an identifier is a Uni-\nversally Unique Identifier (UUID). Many existing ETL tools allow you to gener-\nate a UUID that you can then use in your ingestion pipelines. Another good\nchoice for a BatchId is a Universally Unique Lexicographically Sortable Identi-\nfier, or ULID (https://github.com/ulid/spec). A ULID is shorter than a UUID\nand has a great sortability property. If you use a ULID as your BatchId, then\nnewer batches will always be on top of a sorter list, and you can always tell which\nbatch is older just by comparing two ULIDs. \nNow that we have identified all the common elements of a data pipeline, let’s take a\nlook at how to structure our folders in the cloud storage containers. \nLANDING CONTAINER\nA landing container will have the following folder structure:\nlanding/NAMESPACE/PIPELINE/SOURCE_NAME/BATCH_ID/\nHere “landing” is the container name, and the rest of the path is the folder structure.\nItems in bold are variables that will be set by the ingestion layer. The difference between\nPIPELINE and SOURCE_NAME is that a single ingestion pipeline can handle multiple\n\n\n137\nOrganizing your cloud storage\nsources. For example, when ingesting multiple tables from a single database in a\nRDBMS, PIPELINE could be something similar to my_database_ingest_pipeline, and\nSOURCE could be just the name of the table. We will assume the ingestion layer is using\na ULID as a batch identifier and that a single company-wide namespace is used. We will\ncall this namespace ETL for brevity. \n This shows how folders in the landing container might look for two ingestion\npipelines: \n/landing/ETL/sales_oracle_ingest/customers/01DFTQ028FX89YDFAXREPJTR94\n/landing/ETL/sales_oracle_ingest/contracts/01DFTQB596HG2R2CN2QS6EJGBQ\n/landing/ETL/marketing_ftp_ingest/campaigns/01DFTQCWAYDPW141VYNMCHSE3\nEach container has a folder structure that reflects its namespace, pipeline, and source.\nA ULID is used as the batch identifier.\n Here we can see that there are two pipelines. One pipeline ingests data from an\nOracle sales database and brings in two tables: customers and contracts. Another pipe-\nline brings in marketing data from an FTP server. It’s a good idea to make pipeline\nnames short, but descriptive, so someone looking at a folder in storage can under-\nstand where the data is coming from without referring to documentation or a meta-\ndata repository. In this example, each data source in the pipeline folder has one batch\nof data. BatchId is a folder itself and can contain multiple files that were produced by\nan ingestion application for a single ingestion: a full copy of a table or an incremental\nportion of the table, depending on the type of ingestion you are using. A landing area\ntypically only contains the most recent batches, since the data flow we described previ-\nously will move data into staging for further processing. If you see multiple batches pil-\ning up in the landing area, it could indicate that your downstream processing is not\nworking or is slow.\nSTAGING CONTAINER\nStaging container folder structure is similar to landing, but since we are planning to\nstore data in the staging area long term, data should be organized by time. A common\napproach to organizing by time is known as ingestion time-based partitioning, where we\nput batches into the folders that encode the time when each batch was ingested. It’s\nbetter to see this using the following example.\n This shows how to use time partitioning to organize data by ingestion time in a\nstaging container:\n/staging/ETL/sales_oracle_ingest/customers/year=2019/month=07/day=03/01DFT\n➥ Q028FX89YDFAXREPJTR94\n/staging/ETL/sales_oracle_ingest/contracts/year=2019/month=07/day=03/01DFT\n➥ QB596HG2R2CN2QS6EJGBQ\n/staging/ETL/marketing_ftp_ingest/campaigns/year=2019/month=06/day=01/01D\n➥ FTQCWAYDPW141VYNMCHSE3\nFor each pipeline and source folder, we will introduce three additional folders: year,\nmonth, and day. If multiple batches arrive in a single day, they are placed in the same\n\n\n138\nCHAPTER 5\nOrganizing and processing data\nfolder. And if you are using a ULID as in the previous example, folders can be sorted,\nand newer batches will always end up on top of the list in a cloud portal web UI or any\nother program you use to access data in storage. The naming convention year=YYYY/\nmonth=MM/day=DD comes from Hadoop and is supported by many distributed\nprocessing engines, including Spark. If you read the whole /staging/ETL/sales_\noracle_ingest/customers/ folder, Spark will recognize the time-partitioning structure\nand will automatically add year, month, and day columns in your data set. This way you\ncan easily filter the data you need. \nNOTE If you are ingesting data frequently, perhaps several times an hour, then\nyou may want to add another folder layer with hour=hh to minimize the num-\nber of batches your jobs need to read if you are only interested in the most\nrecent ones.\nArchived and failed containers will follow the same folder structure as staging.\n Production containers will have the same structure as staging, except here new\npipelines may be introduced; for example, if you have a job that combines data from a\ncontract data source and a campaign data source to produce a marketing report. The\nfollowing example shows that some data sets in the production container can be a\nresult of a data transformation, such as joining two data sets together:\n/production/ETL/sales_oracle_ingest/customers/year=2019/month=07/day=03/01\n➥ DFTQ028FX89YDFAXREPJTR94\n/production/ETL/sales_oracle_ingest/contracts/year=2019/month=07/day=03/01\n➥ DFTQB596HG2R2CN2QS6EJGBQ\n/production/ETL/marketing_ftp_ingest/campaigns/year=2019/month=06/day=01/\n➥ 01DFTQCWAYDPW141VYNMCHSE3\n/production/ETL/marketing_report_job/marketing_report/year=2019/month=7/\n➥ day=3/01DFXA98BGBACGSTH5J63B3ZCZ\nHere we can see a new pipeline called marketing_report_job. It’s a good idea to name\njobs in a way that reflects their origin. In this case, we can see the pipeline is not an\ningestion pipeline, but rather a data transformation pipeline. For data transformation\npipelines, there is rarely a single data source. Most often these types of pipelines read\ndata from multiple sources and produce a new data set. It’s unreasonable to encode\nthe names of all the sources required for the data transformation pipeline in a folder\nname because there could be dozens of them. Instead, a recommended approach is to\ncreate a new “derived” source; in our example, we call it marketing_report. Then you\nregister the information about this derived source in the metadata repository where\nyou can expand on which sources are required to create this derived data set. In our\nexample, that would be contracts and campaigns. Also, note that time partitioning\nhere is not the ingestion time, but rather the time this particular transformation job\nwas executed. \n\n\n139\nOrganizing your cloud storage\nORGANIZING STREAMING DATA\nIn a data platform architecture, streaming data lives in two different places. For pro-\ncessing that requires real-time response, we have fast storage, and for archiving and\nreprocessing, we have regular cloud storage. Organizing data in fast storage is differ-\nent from what we previously discussed. In message-oriented systems, such as Kafka,\nCloud Pub/Sub, and others, data is usually organized by topics that represent collec-\ntions of individual messages. There are no concepts of containers, folders, or storage\ntiers. We’ll talk more about how data is organized in fast storage in chapter 6. \n When it comes to saving streaming data to regular storage for archiving purposes,\nwe can apply the same storage organization pattern as we used for batch data in the pre-\nvious section. Imagine that we have a clickstream pipeline that we use for real-time\ningestion into our cloud warehouse. We have performed an initial data assessment and\nwe know that we receive about 100 MB of data per minute. We also know (from chapter\n4) that regular cloud storage is optimized for larger files, so we decide to flush click-\nstream data from fast storage to slow storage every 15 minutes. This flushing process will\nsave data to a landing area of our cloud storage, just like any other batch process would,\nand will follow the same folder-naming convention. The following example shows how\nstreaming data is archived into cloud storage by flushing data from the real-time layer\ninto the landing container. Each flush is assigned a unique batch id:\n/landing/ETL/clickstream_ingest/clicks/01DH3XE2MHJBG6ZF4QKK6RF2Q9\n/landing/ETL/clickstream_ingest/clicks/01DH3XFWJVCSK5TDYWATXNDHJ1\n/landing/ETL/clickstream_ingest/clicks/01DH3XG81SKYD30YV8EBP82M0K\nHere we are also using ULID as our unique batch identifier, and you can see that we have\nthree different batches that have been flushed from fast storage into regular storage. \n The remaining journey of this streaming data through different storage areas\nwould be exactly the same as for batch data: data will be converted to a unified file for-\nmat, cleaned up if necessary, and saved to the staging area; and raw data will be saved\nto an archive area. After that, data will be converted into Parquet format and saved to\na production area where it can be used for other batch data processing jobs or for ad\nhoc analysis. The only difference between streaming and batch data in this case is that\nwe will not be loading these batches into our cloud warehouse, because this data\nshould have been loaded by our real-time pipeline already. \nExercise 5.1\nWhy do you need to follow a naming convention for your cloud storage folders?\n1\nThis is how cloud providers expects you to name resources.\n2\nThis keeps the pipeline code consistent.\n3\nThis is a limitation of Apache Spark.\n4\nThis improves pipeline performance.\n\n\n140\nCHAPTER 5\nOrganizing and processing data\n5.4\nCommon data processing steps\nData processing pipelines in your platform will be divided into common data process-\ning pipelines and custom business logic pipelines. In this section, we will talk about\nwhat data transformations are usually implemented as common processing steps. We\nwill specifically look at\nFile format conversion\nData deduplication\nData quality checks\n5.4.1\nFile format conversion\nAs you learned in chapter 4, depending on the source, data can arrive into the plat-\nform in different formats, including CSV, JSON, XML files, or a custom binary format.\nOne of the core properties of a data lake is its ability to store and provide access to\ndata in different formats, so you might be wondering why we don’t just store data as is\nin our storage layer—a traditional data lake approach. \n Let’s consider what our data transformation and analytics pipelines would look\nlike in a traditional data lake. In a data lake, we’d push the responsibility of dealing\nwith different data formats to each of the pipelines individually. For example, if you\nare building a pipeline to produce a certain report, the first step in your pipeline\nwould be to read a file, figure out which format it is, and which columns and data\ntypes it contains, and only then apply the required business logic. This might be rea-\nsonable if you only have one or two pipelines, but once the number of pipeline\nincreases, this approach won’t scale because you will need to duplicate the file-parsing\nlogic in every single pipeline. If the file format changes or a new column is added, you\nwill need to update and test a lot of code. Leaving the original file formats unchanged\nalso makes data exploration much more complicated. Every person who wants to\naccess data will need to figure out how to read the files first.\n Our modern data platform design suggests a more organized and structured\napproach to this problem. We will still retain data in its original format and save it into\nthe archive area, but one of the first transformations performed on all incoming data\nis to convert it into a single unified file format. Actually, we will be using two different\nfile formats, as described in the previous section. We will use Apache Avro (https://\navro.apache.org/) for our staging area and Apache Parquet (https://parquet\n.apache.org/) in our production area.\nAVRO AND PARQUET FILE FORMATS \nBoth Avro and Parquet are binary file formats. Unlike CSV, JSON, and XML, which\nare text formats, Avro and Parquet are not stored in a human-readable format and\nrequire a special program to decode and encode actual data. While there are many\ndifferent binary file formats that are being used in the data space today, Avro and Par-\nquet are two of the most popular. \n\n\n141\nCommon data processing steps\n Binary file formats offer several advantages over text-based file formats. First,\nbinary formats take significantly less space on disk because of different optimizations\nthey can apply during data encoding. Both Avro and Parquet include column type\ninformation that allows for much better file compression. We have seen reductions of\nup to 10 times the original data size by going from a text-based file format to a com-\npressed binary format. Smaller file sizes not only reduce your cloud storage costs, but\nalso significantly speed up your data processing pipelines. \n A second advantage of binary file formats is that they enforce the use of a certain\nschema for all files. This means that before saving any data in Avro or Parquet format,\nyou must define which columns and column types exist in your data set. In Avro file\nformat, this schema is actually embedded into every single file, so any program or data\npipeline that reads these files will be automatically aware of all column names and\ntheir types. Schema and file format standardization definitely requires extra develop-\nment and maintenance efforts in comparison with just storing all data as is, but in our\nexperience, this effort pays for itself many times over when you need to deal with more\nthan just a handful of pipelines or have to expose data in the platform to different data\nconsumers. We will talk about schema management in more detail in chapter 8.\n Why do we need both Avro and Parquet formats? To answer this question, we need\nto discuss differences between row-oriented and column-oriented file formats. Most\npeople have experience working with row-oriented file formats, where all information\nfor a single data row is saved into a continuous file block. CSV format is the simplest\nexample of a row-oriented file format: rows are stored one after another, separated by\na newline character, as shown figure 5.4.\nWhen computer programs read files from storage, they don’t really do it byte by byte.\nFor performance reasons, they read a whole block at once. The block size depends on\nthe storage and file system parameters. In row-oriented file formats, values for col-\numns that belong to a single row are written one after another, as shown in figure 5.4.\nTo read the whole block from a file, we would get data for multiple rows. To read the\nwhole file, you will only need to perform M read operations, assuming your file con-\nsists of M blocks. This is quite efficient if your goal is to read all columns for all rows\nColumn 1, Column 2, Column 3,...\nFile\nRow 1\nRow 2\nRow 3\nRow 4\nRow N-1\nRow N\nFile block 1\nFile block M\n...\nColumn 1, Column 2, Column 3,...\nColumn 1, Column 2, Column 3,...\nColumn 1, Column 2, Column 3,...\nColumn 1, Column 2, Column 3,...\nColumn 1, Column 2, Column 3,...\nFigure 5.4\nIn a row-oriented file \nformat layout, information for a \nsingle data row is saved into a \ncontinuous file block.\n\n\n142\nCHAPTER 5\nOrganizing and processing data\nfrom a file. Row-oriented files are useful when your goal is to read all columns for all\nrows in a file and perform some operations on them. \n For typical analytics workloads, many of the queries are various aggregations on cer-\ntain columns with different grouping and filtering conditions. For example, if we want\nto count how many users with the status “premium” joined last month, then we only\nneed data from the user-status column and the user-joined-date column to answer this\nquestion. If you have dozens of columns in your hypothetical data set, reading all of\nthem just to use two is a waste of resources. This is where column-oriented or columnar\nfile formats come into play. As shown in figure 5.5, in columnar file formats, values for\na single column are stored one after another, even if they belong to different rows.\nBy reading a single block, you can get all values for a single column across all rows. For\nexample, if you want to get a total sum of column 3 from figure 5.5, then you only\nneed to read block 2 from the file and can safely ignore everything else. Columnar file\nformats provide much better performance when it comes to analytical workloads,\nwhere only certain columns are required to answer the question. This, of course, is a\nsimplified representation of a columnar format. In reality, you will have more than\njust a handful of rows in your files, and values for a single column will span multiple\nblocks, but the key idea is that these values will be arranged in continuous blocks\ninside a file. \n Another benefit of a columnar format is that since values for a single column are\nusually of one type (numbers, strings, dates, and the like), you can get a much better\ncompression ratio with columnar formats than with row formats, where values of dif-\nferent types are mixed in a single continuous block. \n Avro is a row-oriented file format. It provides support for primitive and complex\ndata types, including nested types. Additionally, Avro embedded the schema as a part\nof every file, which makes it easy for programs that work with Avro files to quickly get\nall column definitions and their types. Avro also supports schema evolution rules,\nmeaning that if you make schema changes that are backward compatible, then you\ncan always use the latest version of the Avro schema to read all previous Avro files,\neven if the schema changed over time. The simplest example of a schema evolution is\nColumn 1, Column 1, Column 1\nColumn 2, Column 2, Column 2\nFile\nColumn 3, Column 3, Column 3\nColumn 4, Column 4, Column 4\nFile block 1\nFile block 2\n...\nRow 1\nRow 2\nRow 3\nFigure 5.5\nIn columnar file formats, \nvalues for a single column are stored \none after another, even if they belong \nto different rows.\n\n\n143\nCommon data processing steps\nadding new columns to a data set. We will talk more about schema management and\nAvro features in chapter 8. \n All these properties make Avro a great choice for the staging area, which is primar-\nily used as a source for downstream transformations or ad hoc data exploration use\ncases. Being a row-oriented file format, Avro is not as efficient for analytics use cases as\na columnar file format. That’s why we are using Parquet as a file format for our pro-\nduction area. \n Parquet is a column file format with support for primitive and complex data types.\nIt provides fast access to individual columns in a data set, without having to read the\nwhole data set, which significantly improves performance of analytics queries. Parquet\nalso compresses very well, which helps with reducing storage footprint. All three\nmajor cloud warehouses—AWS Redshift, Google BigQuery, and Azure SQL Data\nWarehouse—have native support for Parquet, which makes loading data from the pro-\nduction area into the warehouse seamless.\nUSING SPARK TO CONVERT FILE FORMATS\nHow do we actually implement converting files from original formats into Avro and\nParquet? Because we are using Apache Spark as our distributed data processing frame-\nwork, this operation is quite simple. \n To work with the Avro file format in Spark (see listing 5.1), you need an external\nAvro library (https://github.com/databricks/spark-avro). Both Google Cloud Datap-\nroc and Azure Databricks services have a preinstalled version of the library available,\nand for the AWS EMR service, you need to explicitly specify external libraries at clus-\nter creation time. \nNOTE\nIf you are using Spark 2.4.0 or later, there is no need for an external\nAvro library, because support for Avro was added to Spark itself. Check which\nversion of Spark your cloud provider supports.\nNOTE\nYou need to make sure your Dataproc cluster has permissions to read\nand write data to the appropriate GCS buckets.\n \nExercise 5.2\nWhat is the benefit of storing data in both Avro and Parquet formats?\n1\nThis reduces the cloud costs.\n2\nThis increases platform reliability.\n3\nThis addresses different data access patterns in the staging and production\nareas.\n4\nThis makes data more portable between different cloud providers.\n\n\n144\nCHAPTER 5\nOrganizing and processing data\nimport datetime\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder ... # we omit Spark session creation for brevity\nnamespace = “ETL” \npipeline_name = “click_stream_ingest”\nsource_name = “clicks”\nbatch_id = “01DH3XE2MHJBG6ZF4QKK6RF2Q9”\ncurrent_date = datetime.datetime.now()\nin_path = f“gs://landing/{namespace}/{pipeline_name}/{source_name}/{batch_id}/*”\nout_path = f”gs://staging/{namespace}/{pipeline_name}/{source_name}/year=\n➥ {current_date.year}/month={current_date.month}/day={current_date.day}/\n➥ {batch_id}”\nclicks_df = spark.read.json(in_path)   \nclicks_df = spark.write.format(“avro”).save(out_path)\nNOTE\nIn our Spark code examples in this chapter, we assume data is stored\non Google Cloud Storage (GCS). If you are using AWS S3 or Azure Blob Stor-\nage with Spark, your path’s prefixes will be different.\nIn listing 5.1, we assume we want to read one of the incoming clickstream batches in\nJSON format and save them into a staging area in Avro format. First, we predefine\nsome variables that make up the path on GCS to our landing data. Note that we are\nusing a Python datetime library to get the current year, month, and day, so we can use\nit as a part of the staging area path. The actual Spark code is just the last two lines of\nthis snippet. First, we read JSON files from the input path, and then, we save Avro files\nto the output path. \n There are many oversimplifications in listing 5.1. First of all, we omitted the Spark\nsession creation and destruction details. We also didn’t include any error handling\nthat must happen for read/write operations. You can find these details in the Spark\ndocumentation. \n There are several things that are important to highlight about this code example.\nFirst, notice that we have hardcoded things such as namespace, pipeline name, source\nname, and batch id. In a real data processing application, you would make these val-\nues parameters that your pipeline code accepts. This way you can reuse the same pipe-\nline code for many different sources. We will be talking about how to make pipelines\nmore generic later in this chapter. \n Second, we haven’t really mentioned what the source JSON file schema is and how\nAvro knows which columns and types it includes. The reason we didn’t need to do that\nis because of Spark’s feature called schema inference. Spark can understand common\nfile formats and try to figure out which columns have which types automatically. This\nListing 5.1\nReading a JSON file from landing and saving it to staging in the Avro format\nDefines configuration variables \nfor our example pipeline\nInputs and outputs GCS paths\nfollowing our preferred folder\nstructure\nSpark has native methods for reading \nJSON data and inferring its schema.\nSaves data in Avro storage \nusing inferred schema from \nthe previous step\n\n\n145\nCommon data processing steps\nis an extremely helpful feature that simplifies a lot of data transformation code. For\nnow, please keep in mind that just relying on schema inference features is not enough\nfor more complex use cases. We will discuss how to use Spark schema inference\ntogether with schema evolution rules in chapter 8.\n5.4.2\nData deduplication\nData deduplication is a large and important topic. The problems that data deduplica-\ntion deals with can be broadly categorized into the following two challenges:\nAre two similar entries in your data set representing the same logical entities?\nFor example, are the John Smith and Jonathan Smith entries in your customer\ndata referring to the same person or two different persons? To deal with these\nissues, a separate set of techniques and tooling has been created over the years,\ncommonly referred to as master data management (MDM) tools. \nHow can you enforce that certain attributes in your data set are unique; for\nexample, making sure you don’t have two records with the same payment trans-\naction_id in your payments data set?\nDiscussing MDM tools and approaches is outside of the scope of this book, and you\ncan refer to existing books and materials on the topic. In this section, we will focus on\nthe problem of enforcing uniqueness on certain data since this is the problem that\nmost data platform implementations need to deal with. \n If you are familiar with how RDBMSs work, then you might be wondering, what is\nthe big deal about enforcing uniqueness? After all, relational databases hava had sup-\nport for primary and unique keys for decades. There are two main issues with unique-\nness in cloud data platforms:\nUnreliable data sources or ingestion replays.\nLack of uniqueness enforcement in existing cloud warehouses. Because of their\ndistributed nature, existing cloud warehouses don’t support constraints such as\nunique indexes or foreign keys. We will talk more about cloud warehouses fea-\ntures in chapter 9.\nFigure 5.6 demonstrates how duplicate rows can end up in the cloud warehouse even\nif a source like an RDBMS provides uniqueness guarantees.\n In this example, we have a unique key defined for the user_id column in the source\ndatabase. This makes sure that there will be no more than one row with the same\nuser_id. During normal operations, we would get an exact copy of the data from the\nsource database in our data platform, which would make sure the user_id column has\nunique values in the cloud warehouse as well; but normal operations shouldn’t be what\nwe plan for. Working with the cloud or with any complex system means planning for\nvarious types of failures—from cloud resource failures to operator errors and pipeline\ncode bugs. If there is a catastrophic metadata repository failure (catastrophic meaning\nthere are no backups to restore data from, and the like) or a data engineer mistakenly\n\n\n146\nCHAPTER 5\nOrganizing and processing data\ndecided to re-ingest previously ingested data, then there are no safeguards against\nduplicate data getting into the platform. \nNOTE\nIn chapter 4 we described several ingestion scenarios where capturing\nthe “history of changes” for a single row would result in intentional duplicates\nfor columns that otherwise could be unique in the source data. Since this is\nan intentional outcome, there is no need to deduplicate this data on inges-\ntion. If some reports require certain columns to have a unique value in this\ncase, the jobs producing these reports will be responsible for implementing\nthis logic. \nSimilar data duplication problems can happen when ingesting data from Kafka or\nother similar message busses and even flat files, but for different reasons. When read-\ning data from Kafka, you need to keep in mind that Kafka provides no guarantee that\nthe same message will be read only once by an ingestion application. Kafka rebalanc-\ning operations, individual node failures, or failures on the ingestion side can cause\nthe same message to be read multiple times, resulting in duplicates. \nNOTE\nStarting with Kafka version 0.11, it is possible to configure both appli-\ncations producing messages and applications consuming messages to avoid\nduplication and guarantee “exactly once processing.” This requires changes on\nboth sides of data delivery: producer and consumer. In data platform use cases,\nthis might not always be an option; applications producing data to Kafka may\nbe controlled by different teams or different organizations altogether. \nRDBMS\nCloud data platform\nIngestion\napplication\nMetadata\nrepository\nMetadata\nrepository\nIngest row: user_id=123,\nuser_name=John Smith \nRecord that\nuser_id=123 has\nbeen ingested\nRDBMS\nCloud data platform\nIngesti\nIngestion\napplication\nCloud\nwarehouse\nCloud\nwarehouse\nIngest row: user_id=123,\nuser_name=John Smith \nRow from previous ingest:\nuser_id=123,\nuser_name=John Smith\nDuplicate was added: \nuser_id=123,\nuser_name=John Smith\nHas a\nuniqueness\nconstraint\non user_id\ncolumn\nSome kind of catastrophic failure\nhappens to the metadata\nrepository or a data engineer\ntriggers full ingest by mistake.\nNow has row: \nuser_id=123,\nuser_name=John Smith\nStill has only\none row with\nuser_id=123\nLost the original watermark record\nor was forced to overwrite \nFigure 5.6\nFailures can result in duplicates even from reliable sources.\n\n\n147\nCommon data processing steps\nIngestion pipelines based on files are also prone to duplicates. While files themselves\nare usually immutable, failures either on the side that delivers files or on the data plat-\nform ingestion side can cause a file to be delivered or consumed multiple times. \nDEDUPLICATING DATA IN SPARK\nIn this section, we will show how to use Apache Spark to deduplicate incoming data to\naddress the issues described. Before we look at the code examples, let’s understand\nthe difference between global and “within the batch” deduplication scenarios as out-\nlined in figure 5.7.\nIn the first scenario, we are only concerned about deduplicating data within a single\nincoming batch that is stored in the landing area. We assume that all data that is\nalready in the platform and is stored in staging, production, and the warehouse is\nduplicate-free. This scenario is common when ingesting data from Kafka in batch\nmode or flat-file ingestion from an unreliable source (usually a third party or an appli-\ncation that cannot provide uniqueness guarantees). \n It is very simple to implement a deduplication code for this scenario using Spark.\nLet’s imagine in the following listing that we are ingesting CSV files containing user\ndata from an unreliable source, meaning each incoming file may contain duplicate\nuser_id values. \nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder ... # we omit Spark session creation for brevity\nnamespace = “ETL”    \npipeline_name = “users_csv_ingest”\nListing 5.2\nUsing Spark dropDuplicates to remove duplicate rows in CSV file\nuser_id\nemail\n5\nuser5@example.com\n6\nuser6@example.com\n5\nuser5@example.com\nuser_id\nemail\n1\nuser1@example.com\n2\nuser2@example.com\n3\nuser3@example.com\n4\nuser4@example.com\nAn incoming batch in the landing area \ncontains duplicate entries for user_id=5.\nData that already exists in the data\nplatform has no duplicates.\nLanding area\nStaging/production area\nFigure 5.7\nIncoming batch deduplication scope assumes duplicates are contained within a single \nbatch.\nDefines configuration variables for our example pipeline\n\n\n148\nCHAPTER 5\nOrganizing and processing data\nsource_name = “users”\nbatch_id = “01DH3XE2MHJBG6ZF4QKK6RF2Q9”\nin_path = f“gs://landing/{namespace}/{pipeline_name}/{source_name}/{batch_id}/*”\nusers_df = spark.read.format(“csv”).load(in_path)  \nusers_deduplicate_df = users_df.dropDuplicates([“user_id”])\nThis Python Spark listing shows that we can read the CSV file from the landing area,\nand use the dropDuplicates function to remove all rows with duplicate user_id values\nfrom the Spark Dataframe. We can then proceed with converting the deduplicated\ndata set into Avro/Parquet, or similar. The dropDuplicates function takes a list of col-\numn names to use for deduplication, so you can use a combination of multiple col-\numns to enforce uniqueness. If you don’t specify column names, dropDuplicates will\nuse all columns for deduplication. \n Deduplication within a single incoming batch is easy to implement and is efficient\nfrom a performance point of view, because it doesn’t require us to join multiple data\nsets to identify duplicates. It is also limited in its ability to prevent duplicates from\nappearing in the data platform. For example, you might not have duplicates in your\ncurrent batch, but adding this batch to existing production data may result in dupli-\ncates, as demonstrated in figure 5.8.\n In this example, we don’t have any duplicate data in the incoming batch, so if we\nonly applied incoming data deduplication, we would miss the fact that user_id=3\nalready exists in the staging and production areas. This scenario is more complicated\nto solve, but it is also a more prevalent one. Our earlier example with a RDBMS inges-\ntion failure or an operator error, as well as flat files that could be sent multiple times\nby mistake, would fall into this category. \nInput and output GCS paths follow\nour preferred folder structure.\nSpark has built-in \nsupport for reading \nCSV files.\nUses dropDuplicates Spark data frame method \nto remove rows with duplicate user_id values\nuser_id\nemail\n5\nuser5@example.com\n6\nuser6@example.com\n3\nuser3@example.com\nuser_id\nemail\n1\nuser1@example.com\n2\nuser2@example.com\n3\nuser3@example.com\n4\nuser4@example.com\nAn incoming batch in the landing \narea doesn’t contain any duplicates.\nCombining incoming data with existing\nproduction data will result in duplicates\nfor user_id=3.\nLanding area\nStaging/production area\nFigure 5.8\nGlobal deduplications scope means we need to look for duplicates in an incoming batch \nas well as in existing data.\n\n\n149\nCommon data processing steps\nHow can we deduplicate data globally? If you are familiar with SQL and relational\noperations, then you might already see the solution: we need to join incoming data to\nthe existing data and deduplicate the resulting data set. Fortunately, Spark supports\nSQL, so it’s easy to express this logic, as shown in the following listing.\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder ... # we omit Spark session creation for brevity\nnamespace = “ETL”\npipeline_name = “users_csv_ingest”\nsource_name = “users”\nbatch_id = “01DH3XE2MHJBG6ZF4QKK6RF2Q9”\nin_path = f“gs://landing/{namespace}/{pipeline_name}/{source_name}/{batch_id}/*”\nstaging_path = f”gs://staging/{namespace}/{pipeline_name}/{source_name}/*”    \nincoming_users_df = spark.read.format(“csv”).load(in_path)    \nstaging_users_df = spark.read.format(“avro”).load(staging_path)\nincoming_users_df.createOrReplaceTempView(“incomgin_users”) \nstaging.users_df.createOrReplaceTempView(“staging_users”)\nusers_deduplicate_df = \\   \nspark.sql(“SELECT * FROM incoming_users u1 LEFT JOIN staging_users u2 ON \n➥ u1.user_id =  u2.user_id WHERE u2.user_id IS NULL”) \nIn this listing, we are reading both incoming batch and existing Avro data from the\nstaging area into two separate Spark data frames and then using Spark SQL to pro-\nduce a third resulting users_deduplicate_df data frame that will contain only rows\nfrom the incoming data frame that do not already exist in the staging data frame. Our\npipeline then can take this resulting data frame, convert it to Avro, and append it to\nthe existing staging data. \n So when should you use batch-scope deduplication versus global scope deduplica-\ntion? Batch-scope deduplication will only solve for simple use cases, like duplication\nwithin incoming flat files. If you want to avoid duplicates with a 100% guarantee, you\nreally should be doing both. The challenge is that as your data volumes grow, global\nscope deduplication will require more and more computational resources, because\nyou will need to join an ever-growing staging data set. This may or may not present a\nproblem, depending on your data volume and cloud costs that you can afford for the\ndata processing cluster. There are some optimization techniques you can use to make\nglobal deduplication require fewer resources. Note that in listing 5.3, we are reading all\ndata from the staging area. Assuming you followed the year/month/day partitioning\nListing 5.3\nDeduplicating data globally via a join\nWe will be reading ALL data from\nthe staging area for this source.\nReads both \nincoming and \nexisting data in \nstaging\nRegisters \ntemporary tables \nfor SQL operations\nSelects all data from the incoming batch, where a user_id \nfrom the incoming batch doesn’t occur in the existing data set\n\n\n150\nCHAPTER 5\nOrganizing and processing data\nstructure for your staging area, you can limit the scope of global deduplication to, for\nexample, the current year, month, or week. This will improve performance, but will\nincrease the risk of duplicates: if you receive a duplicate row that you have received a\nyear ago, or similar. You need to carefully evaluate the business logic and the nature of\nthe source data before you can safely limit the global deduplication scope.\n5.4.3\nData quality checks\nTwo of the most common concerns we heard from organizations that have adopted a\nstandard data lake approach is that they can’t always trust the data in the lake and that\ndifferent data sources have very different levels of quality. These concerns are easy to\nunderstand. Since data lake design in itself doesn’t offer any level of control over data\nthat is being ingested, the quality of data completely depends on the source. This is\nnot good enough for most use cases. Data users want reassurance that the data they\nwork with conforms to at least some basic set of standards.\n Interestingly enough, this problem is less prevalent in the traditional relational\nwarehouse. Relational databases, as we know, have a strict schema, which often\nincludes constraints on the length of certain columns, their type, and sometimes even\nadditional business logic, restricting what type of data can be saved to a table. \n In our cloud data platform design, a warehouse is a destination for the processed\ndata and is not used for ingestion. So we can’t use built-in warehouse controls. Also, as\nwe have seen before, existing cloud warehouses often lack column-level constraints\nthat you might find in traditional databases.\n To address this problem, we can implement required quality checks as one of the\nsteps in our data processing pipeline. In the previous section, we described\napproaches to data deduplication. Data deduplication can be considered one of the\nrequired quality checks. Here are some other common checks that we have seen:\nLength of values for certain columns should be within predefined range.\nNumeric values should be within a reasonable range: for example, no negative\nsalary values.\nExercise 5.3\nWhat is the main trade-off between batch-scope deduplication and global deduplication?\n1\nBatch-scope deduplication is much faster than global but will occasionally\nmiss duplicates within the batch.\n2\nBatch-scope deduplication is easier to implement but it doesn’t perform as\nwell as global deduplication.\n3\nGlobal deduplication is easier to implement, but it doesn’t provide full dedu-\nplication guarantees.  \n4\nBatch-scope deduplication is much faster, but it can’t find duplicates outside\nof the incoming batch.\n\n\n151\nCommon data processing steps\nCertain columns must never contain empty values. The definition of what\n“empty” means can also be different for different columns.\nValues must conform to specific patterns: for example, the email column\nshould contain valid email addresses. \nThese types of checks are really easy to implement in Spark and so are among the steps\nin our data pipeline. We can use the Spark filter function to filter out rows from the\ndata frames that do not satisfy our requirements. We have omitted the initial pipeline\nconfiguration code in this example, because it’s the same as in previous listings:\nusers_df = spark.read.format(“csv”).load(in_path) \nbad_user_rows = users_df.filter(“length(email) > 100 OR username IS NULL”)    \nusers_df = users_df.subtract(bad_user_rows) \nIn this code snippet, you can see how you can use an OR condition to filter out rows\nthat do not satisfy some predefined criteria. In this case, we don’t want rows with email\naddresses that are longer than 100 characters and rows where the username column is\nempty. Note that we save these rows into a bad_user_rows data frame so we can later\nsave them into a failed area in our platform, in case we later want to understand what\nhappened to these rows. \n We also use the Spark subtract function to remove bad rows from our original\ndata set. Our pipeline code can then proceed as usual with the users_df and have a\nseparate functionality for how to deal with bad rows. \nNOTE\nRemoving “bad” rows to improve the overall quality of the data should\nbe done with caution. Lots of data sets come from highly normalized rela-\ntional data sources. For example, you may have separate orders, order items,\nand customers data sets. If you delete an order because it doesn’t conform to\na certain data quality check, you will end up with orphaned order items that\nno longer link to any order. In such cases, you may decide to just alert a data\nengineer about the issue but let the data flow to the platform without\nchanges. You can also implement a more sophisticated data quality check that\ntreats an order and all related items as one unit and either allows all related\nitems through or fails all of them.\nWhile this example is very simple, it does demonstrate the overall approach to imple-\nmenting data quality checks in Spark. Other considerations about data quality flow\nthat you may need to make are the following:\nChecking criticality—From the data user’s perspective, not all data quality issues\nhave the same level of criticality. For example, an empty username column may\nnot break any existing business processes, but it is something that data users may\nUses the Spark built-in method \nto read data from a CSV file\nCreates a new Spark data frame\nobject by filtering out rows with\nemails that are too long or where\nthe username field is empty\nRemoves bad rows from the original data \nframe by using the Spark subtract method\n\n\n152\nCHAPTER 5\nOrganizing and processing data\nwant to be informed about. On the other hand, a negative value in the salary col-\numn may break an existing report. Such data must never enter the platform.\nAlerting on data quality issues—You may want to send alerts to data engineering\nor selected data consumers in the case of data quality issues. \nRemoving bad rows or failing the whole batch—In some cases, if a certain number of\nrows in the incoming data set don’t pass quality checks, you can decide to not\ningest a batch altogether and move it to the failed directory for further investi-\ngation. This is a common scenario when the incoming batch represents a sum-\nmary of some state, for example, the inventory summary for the previous week.\nProcessing a partial summary could be worse than not processing anything at all. \nNow that we have looked at various steps in our common data processing pipeline, a\nnatural question is, how do we scale this approach to hundreds of different data\nsources? The code examples we showed so far only worked for a single source. Copy-\ning and pasting code snippets for each data source you want to process is obviously\nnot going to work. This brings us to the next section of this chapter, in which we will\ndiscuss how to design a flexible and configurable data pipeline. \n5.5\nConfigurable pipelines\nAs we mentioned earlier in this chapter, shifting from a “free for all” data lake model\nto a more organized data platform approach allows us to unify how data is organized\non storage, how it moves from one stage to another, and what data consumers should\nexpect by looking at data at any stage. It also allows us to standardize some of the com-\nmon transformation steps into a single, highly configurable pipeline. \n We now know that for each data source that we ingest into the incoming area, we\nwould need to at least do file format conversion, deduplication, and some basic data\nquality checks. Because we know the exact folder structure, we can create a single\npipeline that will accept parameters, such as pipeline name and data source name,\nand perform common data transformation steps. This pipeline will be responsible for\nprocessing all incoming data sources, but it will be called with different parameters for\ndifferent sources. Figure 5.9 demonstrates how such a pipeline can be constructed.\n A common data processing pipeline will consist of several modules responsible for\ndifferent aspects of the pipeline. You can implement this pipeline by separating your\nmodules into separate jobs and then using an orchestration layer to execute them one\nafter another, or have a single processing job with several different functions, each\nresponsible for one step in the transformation pipeline. Choosing which one is best\nwill depend on which data processing engine you are using and/or your development\nteam preferences. Regardless of how you choose to implement your pipeline, each\nmodule should accept a configuration telling the module which data source to pro-\ncess, where on storage the source is located, what the source schema is, which col-\numns to use for deduplication, and the like.\n A recommended place to store such configurations is the metadata repository.\nThis way you have a central place with all the configurations and a unified mechanism\n\n\n153\nConfigurable pipelines\nto fetch a configuration for your data source using the metadata API. If you are deal-\ning with only a handful of pipelines, then you can choose to store configurations in a\ntext file in a dedicated location in cloud storage. \n The only missing piece in this flow is a component that will actually launch the\ndata processing job with a required configuration. This component is the orchestra-\ntion layer in our cloud data platform architecture, highlighted in figure 5.10.\nThe orchestration layer is the glue that holds together the entire pipeline. It needs to\nmonitor the landing area in storage for new data batches. Once a new batch is\ndetected, it will extract the pipeline name, data source name, and other needed\nparameters by using folder-naming conventions that we described earlier. Depending\nIncoming data\nFile format conversion module\nDeduplication module\nData quality checks module\nStaging data\nMetadata API\nPipeline name\nSource name\nSchema\nColumns to deduplicate\nQuality check rules\nQuality check severity\nProvides configuration\nto the pipeline\nExample of\nconfiguration\nparameters\nCommon data transformation pipeline\nFigure 5.9\nAll common \ndata processing steps can be \ncombined into a single pipeline \nthat accepts configuration \nfrom the metadata API. \nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nThe orchestration layer is\nresponsible for coordinating\nmultiple data-processing jobs\nand handling job failures\nand retries.\nFigure 5.10\nThe orchestration layer in a data platform architecture\n\n\n154\nCHAPTER 5\nOrganizing and processing data\non the orchestration mechanism you choose, you will either need to implement cus-\ntom code that will periodically check for new data to arrive and then use existing trig-\ngers in tools such as Apache Airflow; or, you can use cloud provider built-in\nnotification mechanisms. With these bits of information, the orchestration layer will\nthen fetch a more complete configuration for the data processing job and launch it.\nFigure 5.11 walks you through this process step by step.\nBecause common processing steps for different data sources are typically independent\nof one another, we can run multiple common data transformation jobs in parallel.\nThis will allow us to process potentially hundreds of different data sources in a timely\nmanner. \nSummary\nThe processing layer is the heart of the data platform implementation. This is\nwhere all the required business logic is applied and all the data validations and\ndata transformations take place.\nProcessing can be done in the data warehouse, but if your system is critical to\nthe enterprise, and you want it to scale and be managed and available for the\nlong term, doing your processing in the lake will give you better outcomes.\nIn the processing steps, data flows through several stages, and at each stage data\ntransformation and validation logic is applied, thus increasing the “usefulness”\nof data as it is transformed from raw and unrefined data coming from the data\nsource to well-defined and validated data products that can be used for analysis\nor made available to other data consumers. \nData processing tasks typically can be divided into two broad categories—com-\nmon data processing steps and business logic–specific steps. Common data pro-\ncessing steps are steps that you apply to all data coming in from all data sources\nOrchestration tool\nMetadata\nrepository\nStorage landing area\nCommon data processing job\nAn orchestration tool\nmonitors the storage\nlanding area for\nnew data.\nFetch a pipeline configuration for an\nincoming data for a given data source.\nLaunch a data processing\njob and provide a\nconfiguration to it.\nFigure 5.11\nThe orchestration layer is responsible for providing a configuration \nto the data processing job.\n\n\n155\nExercise answers\nor jobs that deduplicate data and apply standard quality checks that are\nrequired for all data. In addition to common processing steps, each analytics\nuse case will require its own set of transformations and validations that are spe-\ncific to the use case.\nHaving a set of consistent, clear principles on how to organize your data on\ncloud storage is very important because it will allow you to build pipelines that\nfollow the same design with regard to where to read data from and where to\nwrite data to. It will also help your data users search for data in storage and\nunderstand exactly where to find the data they need. \nA common data processing pipeline will consist of several modules responsible\nfor different aspects of the pipeline. You can implement this pipeline by sepa-\nrating your modules into separate jobs and then using an Orchestration layer to\nexecute them one after another, or you can have a single processing job with\nseveral different functions in it, each responsible for a step in the transforma-\ntion pipeline. \nConverting file formats to binary file formats (AVRO and Parquet) offers sev-\neral advantages over using text-based file formats. First, binary formats take sig-\nnificantly less space on disk because of different optimizations they can apply\nduring data encoding; and second, they enforce the use of a certain schema for\nall files, making scalability much easier.\nWhen it comes to deduplication, a common data processing step, enforcing\nuniqueness is especially important in cloud data platforms with their typically\nunreliable data sources, or ingestion replays and lack of uniqueness enforce-\nment in existing cloud warehouses. \nBecause a data lake design in itself doesn’t offer any level of control over data\nthat is being ingested, and the quality of data depends entirely on the source, a\ngood practice is to implement required quality checks as one of the steps in a\ndata processing pipeline.\n5.6\nExercise answers\nExercise 5.1:\n 2—This keeps the pipeline code consistent.\nExercise 5.2:\n 3—This addresses different data access patterns in the staging and production\nareas.\nExercise 5.3:\n 4—Batch-scope deduplication is much faster, but it can’t find duplicates outside of\nthe incoming batch.\n\n\n156\nReal-time data\n processing and analytics\nIn this chapter, we’ll help you get a clear understanding of real-time or streaming\ndata—one of the most popular features of a modern data platform.\n We’ll cover the difference between real-time ingestion and real-time processing\nand walk through some examples of when to use one or both, showing different\ndata platform designs.\nThis chapter covers\nDefining real-time processing and real-time \nanalytics\nOrganizing data in fast storage\nUnderstanding typical real-time data \ntransformation scenarios \nOrganizing data for real-time use\nTranslating common data transformations into \nreal-time processing\nComparing real-time processing services \n\n\n157\nReal-time ingestion vs. real-time processing\n We’ll also go deeper into how streaming data is organized—with producers, con-\nsumers, messages, partitions, and offsets. Then we’ll walk through some typical real-\ntime data transformation use cases, with particular attention on dealing with data\ndeduplication, file format conversion, real-time data quality checks, and combining\nbatch and real-time data.\n Last, each cloud vendor provides a pair of related services for real-time process-\ning—one that implements real-time storage and maps to the fast storage layer in our\narchitecture, and another that implements the real-time processing. We will look at\nAWS Kinesis Data Streams and Kinesis Data Analytics, Google Cloud’s Pub/Sub and\nCloud Dataflow, and Azure Event Hubs and Azure Stream Analytics.\n6.1\nReal-time ingestion vs. real-time processing\nAs we discussed in chapter 3, the processing layer, highlighted in figure 6.1, is the\nheart of the data platform implementation. This is where all the required business\nlogic is applied and all the data validations and data transformations take place. The\nprocessing layer also plays an important role in providing ad hoc access to the data in\nthe data platform.\nSo far in this book, we’ve used data processing and analytics scenarios focused on\nbatch data processing. In these scenarios, we assumed that data can be extracted from\nthe source system on regular intervals or that it naturally arrives in the form of files\nthat need to be processed. \nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 6.1\nThe processing layer is where business logic is applied and all data validations and data \ntransformations take place, as well as where ad hoc access to data is provided.\n\n\n158\nCHAPTER 6\nReal-time data processing and analytics\n Batch is not the only way data can be delivered and analyzed in our cloud data plat-\nform. You may have already heard the term “real-time data processing,” and in this\nchapter, we will explore this form of processing and its common use cases. Let’s start\nwith some definitions and use cases.\n When people use the terms “real-time” or “streaming” in the context of a data plat-\nform, it can mean different things to different people, and it is relevant in two layers\nof a data platform—the ingestion layer and the processing layer.\n Real-time or streaming ingestion takes place when you have a pipeline that streams\ndata, one message at a time, from a source into a destination such as storage or the\ndata warehouse or both. While the term real-time processing isn’t clearly defined any-\nwhere, much of the available product documentation, blog posts, and books use the\nterm to refer to straightforward data transformations applied to streaming data.\nExamples of these data transformations include converting a date field from one date\nformat to another, or more complex data cleanup use cases, such as making sure all\naddress fields follow the same format. \n The term “real-time data analytics,” on the other hand, is usually reserved for the\napplication of complex computations on streaming data. A good example might be cal-\nculating the probability of a certain event happening based on previous events. While\nthese differences may matter in some cases, going forward we will refer to all real-time\ndata processing and real-time data analytics use cases as “real-time processing.”\n Real-time ingestion can take place without using real-time processing, but real-\ntime processing typically requires real-time ingestion. Whether you need one or both\ndepends on the use case.\n Let’s look at two use cases in table 6.1—one can be satisfied by ingesting data in real\ntime into the data warehouse, and the other will require real-time processing done in\na separate system. The difference comes down to who the final consumer of the data is. \nIn our first use case, the final data consumer is a human analyst looking at a sales\ndashboard produced using data in a data warehouse. They are asking for “real-time”\nbut they aren’t sitting there refreshing the dashboard continuously and acting on\nsecond-by-second changes. It is likely that what they really want is to ensure that the\ndashboard can be refreshed when they want to see it, and that the data they see in the\ndashboard is up to date and reflects the state of things “as of now.” \nTable 6.1\nComparing the “real-time” needs of two different use cases\nUse case\nHuman analyst looking at a dashboard\nGaming application reacting to \nchanges in player behavior\nWhat “real-time” \nmeans to them\nData is refreshed when requested, and data \nreflects the state of things “as of now.”\nData is processed and delivered \nwith sub-second turnaround times.\nWhat they need in \na data platform\nReal-time ingestion to a data warehouse.\nReal-time ingestion and real-time \nprocessing.\n\n\n159\nReal-time ingestion vs. real-time processing\n To meet this “real-time” requirement, we can develop pipelines that deliver data to\nthe data warehouse in real time. It is important to note that even though the data is\narriving continuously into the data warehouse, data warehouses don’t process data in\nreal time. While dashboards can be updated as often as users want, it can take more\nthan a few seconds for a typical dashboard to refresh. The exact timing of data refresh\nrequired may vary, but as a rule, when the analysis is done by human consumers,\nbringing data into the cloud warehouse in real time and letting the data warehouse\nperform the analysis with a response time of seconds or minutes is usually an accept-\nable trade-off between performance and data platform architecture complexity. \n In this use case, data is delivered in real time (i.e., streaming) into the cloud data\nwarehouse, but it is not being consumed in real time or even close to real time. To a\nuser who is accustomed to seeing their data refresh once a day, a 15-minute refresh\nmight be considered “real time” to them. They may even call this a real-time dash-\nboard. Having the data delivered to the data warehouse in real time may well give\nthem exactly what they want without any real-time processing involved.\n When your business users say they want “real time,” take the time to explore what they\nmean. If the real-time requirement is to make current data available for analysis any\ntime, but the analysis itself happens in an ad hoc manner—i.e., as scheduled reports or\ndashboard refreshes requested by the user—then save yourself some extra work and cost\nand implement real-time ingestion without real-time processing. \n In our second use case, people aren’t involved. In online gaming, for example,\ndata collected from player engagement is to be used to change the game behavior\nitself. Obviously, this has to happen quickly, because you can’t wait seconds to react to\nsomething a player does and then change the game behavior. Unlike people, the\ngame is capable of reacting to ongoing instant changes, so real-time ingestion coupled\nwith real-time processing makes sense. \n So if the end data consumer is an application that needs to perform actions based\non incoming data, it’s a good indicator that ingestion and processing should be imple-\nmented, and you will need a real-time data processing system for this use case.\n Let’s summarize. These two use cases describe two different flavors of real-time\ndata processing. Our dashboard use case is an example of real-time data ingestion\n(sometimes called streaming ingestion or just data streaming) without real-time pro-\ncessing. If the only requirement for real time is that you have to make data available\nfor analysis as fast as possible, but the analysis itself happens in an ad hoc manner,\nthen real-time ingestion is what you should be implementing. If, on the other hand,\nthe requirement is to have the analysis itself done in real time to be passed on to\nanother system for action, then real-time ingestion and real-time processing will be\nrequired.\n Note that in the first scenario, you may still need to perform some data prepara-\ntion before it is made available in the warehouse using real-time processing engines,\nbut we will explore that later in this chapter.\n\n\n160\nCHAPTER 6\nReal-time data processing and analytics\n6.2\nUse cases for real-time data processing\nIn this section, we will take our two use cases and work through the data platform\ndesign considerations for each one.\n6.2.1\nRetail use case: Real-time ingestion\nIn this first use case, imagine that your company is a retailer that operates brick-and-\nmortar stores as well as an online store. Your old point-of-sale (POS) system in the\nphysical stores is only capable of delivering sales data once a day as a CSV file, while\nyour online store is capable of delivering each sales transaction as it occurs. These\ntransactions are available for analysis within seconds of visitors to the website clicking\nthe “Purchase” button. In this scenario, you would have two dashboards: one for phys-\nical stores with data updated once per day and one for the online store with data\nupdated throughout the day. \n The business users at your company want to visualize daily sales in a dashboard, but\ncombining data from offline and online stores on a single dashboard creates a lot of\nconfusion because the data from the physical stores arrives once a day, while online\nstore data is delivered constantly throughout the day. \n Before the POS upgrade, we had two different pipelines: one batch pipeline that\nprocessed files delivered daily from the POS system and one real-time pipeline that\nprocessed online sales transactions. Both are supported by respective layers in our\ncloud data platform architecture, with the serving layer being a cloud data warehouse\nwhere data can be accessed by reporting tools. \n But, as you can see in figure 6.2, combining two data sources with different data\nrefresh rates into a single dashboard will not give your business a consistent overall\nview of daily sales. \nBefore POS upgrade:\nWeb\nPOS\n1. Data from online sales transactions are streamed\n    in real time into fast storage and delivered in real \n    time into the cloud data warehouse. Real-time \n    data is also archived in slow storage.\n2. Data from the store POS system is\n    ingested into storage once per day,\n    where it is processed and delivered\n    to the cloud data warehouse.\n3. Data from both sources is available in \n    the data warehouse, but the different\n    timing of delivery means that two \n    separate dashboards are required.\nFast storage\nBatch archiving\nprocess\nSlow storage\nReal-time\nprocessing\nBatch\nprocessing\nCloud data\nwarehouse\nOnline sales\ndashboard\nOffline sales\ndashboard\nFigure 6.2\nCombining batch and real-time delivery of data into a data warehouse is possible but may result \nin limitations when displaying the data.\n",
      "page_number": 150
    },
    {
      "number": 6,
      "title": "Real-time data processing and analytics",
      "start_page": 180,
      "end_page": 219,
      "detection_method": "regex_chapter",
      "content": "161\nUse cases for real-time data processing\nLuckily, your company has decided to upgrade the old POS system to a newer version\nthat supports sending sales transaction data in real time. Figure 6.3 demonstrates what\nwill change in your cloud data platform when POS data becomes a streaming data\nsource.\n After the POS upgrade, we can deliver both POS data and online data via the real-\ntime layer, eliminating the timing discrepancy. We can now combine two separate\ndashboards into one. Notice that we are still archiving real-time data sets into slow\nstorage, as discussed in chapter 4. \n So in this scenario, our pipeline is delivering data from both data sources in real\ntime into the data warehouse. Dashboards can be updated as often as users want, but\nit can take more than a few seconds for a typical dashboard to refresh—not real time\nby our definition. So while data is being delivered in real time into the cloud data\nwarehouse, it is not being consumed in real time or even close to real time, but users can\nbe confident that the data they are looking at is up to date and reflects the state of the\nthings “as of now.” \n6.2.2\nOnline gaming use case: Real-time ingestion and real-time processing\nNow, let’s take a look at another example. Imagine that after successfully implement-\ning that POS migration to real-time data processing at your retail employer, you\nreceived an offer to join an online gaming company. This company wants to make one\nof its flagship games more sophisticated by adding more interactive elements where\nthe environment and other players can react to actions that are performed in the\ngame. Now that you are equipped with the experience of working with real-time data\nprocessing from your previous job, you think that you can apply the same pattern\nhere. Figure 6.4 shows the first draft of the architecture you have in mind.\n This architecture looks similar to our retail example, with only one data source—\nin this case the game application—which is not part of the data platform, but it does\nAfter POS upgrade:\nWeb\nPOS\n1. Data from both online sales transactions and the store POS system are\n    streamed real time into fast storage and delivered in real time into the\n    cloud data warehouse. Real-time data is also archived in slow storage.\n2. Data from both sources is available in the \n    data warehouse AND can be combined in \n    a single unified sales dashboard as there\n    is no longer any timing discrepancy.\nFast storage\nBatch archiving\nprocess\nSlow storage\nReal-time\nprocessing\nCloud data\nwarehouse\nUnified sales\ndashboard\nFigure 6.3\nWhen data is streamed from different sources, it can be delivered in real time to the data \nwarehouse and combined into a single unified report or dashboard.\n\n\n162\nCHAPTER 6\nReal-time data processing and analytics\nproduce data in real time. In fact, it plays a dual role, acting as both a data source and\na data consumer.\n Players use their devices (mobile devices, game consoles, or PCs) to interact with\nthe game. These interactions are sent to the game backend application (which in real-\nity is usually composed of many different microservices) as game events. These events\nare just pieces of data with information about what happened, when, where, etc.\nThese events flow through into the data platform in our real-time layer and end up in\nthe cloud data warehouse where, instead of reporting tools connecting to our ware-\nhouse, we’ve got the game backend application running some complex SQL state-\nments to make decisions on how the game environment for this particular player\nneeds to be adjusted. For example, you may need to calculate the probability of a cer-\ntain type of monster appearing in front of the player at a given point in time. \n This design looks good on paper, but as shown in figure 6.5, if you try to implement\nit, you will soon discover a significant limitation. Even though you can deliver data to the\ncloud warehouse in real time (limitations for specific cloud warehouses are discussed in\nchapter 9), there is no guarantee of how quickly a query will produce results. Data\nwarehouses are designed to provide reasonable performance when dealing with very\nBatch archiving\nprocess\nGame\nbackend\napplication\nFast storage\nReal-time\nprocessing\nCloud data\nwarehouse\nPerform an analysis\nof player actions\nusing SQL.\nAdjust the game\nenvironment based\non the analysis.\nGame events\nIn-game\nactions\nPlayer’s device\nSlow storage\nFigure 6.4\nFirst draft of \nthe real-time processing \narchitecture for an online \ngaming use case\nBatch archiving\nprocess\nGame\nbackend\napplication\nFast storage\nReal-time\nprocessing\nCloud data\nwarehouse\nIn today’s cloud data warehouses,\ntypical response times, even for a\nrelatively simple query, are measured\nin seconds and sometimes minutes, \nwhich is not good enough to adjust \nthe game in real time.\nGame events\nPlayer’s device\nIn-game\nactions\nSlow storage\nFigure 6.5\nUsing a \ndata warehouse for \nreal-time processing \nwon’t satisfy a need \nfor sub-second \nprocessing.\n\n\n163\nUse cases for real-time data processing\nlarge volumes of data, but they are not optimized for fast response. In today’s cloud data\nwarehouses, typical response times, even for a relatively simple query, are measured in\nseconds and sometimes minutes. Unless your online game is a game of chess, response\ntimes of tens of seconds or a couple of minutes won’t be acceptable, especially for an\ninteractive game. We need to look at a different solution. \n Modern cloud-based, real-time processing systems can not only perform basic data\ntransformations such as changing date formats or filtering messages that satisfy a cer-\ntain condition but can also perform complex computations and analytics. Some sup-\nport SQL so you can perform similar types of analysis in the real-time processing\nsystem that you would do in a cloud data warehouse. Later in this chapter, we will\nexplore the different cloud real-time processing systems, but for now, let’s assume that\nthe calculation that we originally planned to take place in the cloud data warehouse\ncan also take place in the real-time system. Figure 6.6 shows the second iteration of\nour architecture.\nIn this architecture, a game backend application submits one or more real-time pro-\ncessing jobs that run constantly in the real-time processing system, adjusting the calcu-\nlations with every new incoming message from the fast storage. One of the most\nimportant differences between submitting a query to the warehouse and submitting a\nquery to a real-time processing job is that a data warehouse starts reading and process-\ning data only after it has received a SQL query from an application and often reads large\nportions of data each time it runs; while a real-time job is always running and doesn’t\nneed to read large portions of data every time a calculation needs to be adjusted. \n It is also important to note that, unlike data warehouses, real-time processing sys-\ntems are not designed as data-serving endpoints. This means that your real-time pro-\ncessing job must save results to another system, one that can provide very low-latency\naccess to data. Usually a key/value NoSQL database or in-memory cache is used for\nBatch archiving\nprocess\nGame\nbackend\napplication\nFast storage\nReal-time\nprocessing\nCloud data\nwarehouse\nSubmit a real-time\nprocessing job to\nperform required\ncalculations.\nWe still need the data\nwarehouse for ad hoc\nanalytics.\nSave result of the\ncalculations into\nfast data store, like\nin-memory cache,\nNoSQL, or an RDBMS.\nFetch results of the analysis\nfrom the fast data store.\nAdjust the game\nenvironment\nbased on the\nanalysis.\nGame events\nPlayer’s\ndevice\nIn-game\nactions\nSlow storage\nFigure 6.6\nTo reduce response latency, real-time calculations should take place in a real-time \nprocessing system with a fast data store used to store the results.\n\n\n164\nCHAPTER 6\nReal-time data processing and analytics\nthis purpose, but we have also seen relational databases used successfully for storing\nthe results of a real-time calculation. Now our game backend can fetch the result of\nthe real-time data analysis with a latency of a few seconds or less. This is far more\nacceptable for our use case. \n Another common scenario for a real-time processing system is to save the results\nback into fast store, thus creating new streams of data based on incoming streams. We\nwill explore this scenario later in this chapter when we talk about common real-time\ndata transformations. \n6.2.3\nSummary of real-time ingestion vs. real-time processing\nSo, let’s revisit the differences between our retail and online gaming examples when it\ncomes to real-time processing requirements. \n In our retail example, where analysis is done by human consumers, bringing data\ninto the cloud warehouse in real time and letting the data warehouse perform the\nanalysis with a response time of seconds or minutes is an acceptable trade-off between\nperformance and data platform architecture complexity.\n In our online gaming scenario, where decisions are made by other programs and\nthese decisions need to happen fast, we can’t rely on the data warehouse response time\nfor processing. This use case is an example of a complete end-to-end, real-time, data\nprocessing implementation where data needs to be ingested into the warehouse in real\ntime, but we also need to perform some complex data processing and make the results\navailable to an application using a low-latency data store. This scenario requires addi-\ntional infrastructure, monitoring, and ongoing maintenance. You will need to make\nsure that your low-latency data store is highly available and provides optimal perfor-\nmance. Various cloud services definitely make this task easier than implementing a sim-\nilar data store on premises, but you will still need to balance performance and cost of\nyour low-latency data store as well as plan properly for outages and the like. \n6.3\nWhen should you use real-time ingestion and/or \nreal-time processing?\nNow that we know the differences between real-time ingestion and real-time process-\ning, let’s try to answer the question that we have been asked countless times when\nworking on various data platform implementations: can we do this in real time? Usu-\nally this question is asked in relation to either an existing use case or a new use case\nthat business wants to implement. Often there is a report of some sort that runs once\nper day and business users are interested in making it real time. \n It’s your job as a data platform architect to decipher the actual requirement here and\napply the right real-time processing approach. We, of course, will help you with that. \n If the end users are mostly concerned with data “freshness,” then implementing a\nreal-time ingestion process for the required data sources will probably satisfy that\nrequirement. In general, we suggest using real-time data ingestion whenever possible\nand reserve batch layer ingestion only for sources that don’t support real time or when\n\n\n165\nWhen should you use real-time ingestion and/or real-time processing?\ndata is naturally produced in batch. Real-time ingestion has multiple benefits. For\nexample, real-time ingestion requires less orchestration, such as monitoring if new files\nhave arrived or not. For relational databases, real-time ingestion layers make it possible\nto use a powerful change data capture (CDC) mechanism, which we described in chap-\nter 4. CDC allows you to capture all kinds of changes that happen in the database\n(inserts, updates, and deletes) and does this at a lowest possible granularity, unlike peri-\nodic batch snapshots that will miss changes happening “in between.” Real time is also\nbecoming a standard of data delivery today with more and more source systems making\ndata available as streams of messages. We described some benefits of a real-time change\ndata capture process in chapter 4. Today users expect data to “be there” as soon as a\nchange happens in a source system, so even if you don’t have a formal requirement for\nreal-time ingestion, your users won’t complain. Keep in mind the possibility of confu-\nsion if you mix batch and real-time data in reports and analytics. If your dashboard has\none section that is up to the second, and another where data is only refreshed once a\nday, there is a very real risk that some of the data consumers will not realize this, and the\nnext thing you know, they’ll be reporting that the system is “broken.” \nHINT\nIt’s often better to separate reports where data freshness varies. \nNOTE\nThere are other use cases for combining real-time and batch data,\nsuch as data enrichment, that we will explore later in this chapter.\nAnother benefit of standardizing your cloud data platform around real-time ingestion\nis the fact that today there is no industry standard system that works efficiently with\nboth batch and real-time data. The Google Cloud Dataflow service (using the Apache\nBeam API) is an example of such a system, but it is specific to Google Cloud. Apache\nSpark, which we used as an example in previous chapters, is a great batch processing\nsystem that also supports real-time processing using the Spark Streaming API, but it\ndoes so by using a micro-batching technique. Micro-batching means buffering incom-\ning real-time data into several seconds or longer intervals and then processing all\nbuffered data at once. This approach may not be suitable for use cases where true low-\nlatency response is required. Lack of industry standards here means that if you are\nusing both batch and real-time layers, most likely you will need to use two completely\ndifferent systems. \n In our experience, the vast majority of use cases where end users say they want data\nprocessing in real time can be satisfied by real-time ingestion. Most end users just want\ntimely data delivery into the cloud warehouse, so they can run their queries without\nworrying about data staleness problems. \n There is, of course, a set of problems that can’t be solved by real-time ingestion.\nOur earlier online gaming scenario is a good example of such a problem. But let’s\nlook at a simple use case first. Imagine that there is a report that runs a set of queries\nagainst a data warehouse once a day and then packages the results into an PDF docu-\nment and emails it to end users. Users of this report are asking you whether this\n\n\n166\nCHAPTER 6\nReal-time data processing and analytics\nreport can be “made real time.” Now, real-time ingestion is required here, but it is not\nenough—in the end, the report is still scheduled to run once a day. \n In many cases, what we have seen in our practice is that going from daily to hourly\nreport delivery is “real time” for the end users. Sometimes it is improving from hours\nto every 15 minutes, but the idea is the same. If data is delivered into the platform in\nreal time, then it’s easy to run reports more frequently, because you can get extra\nprocessing capacity for the cloud data warehouse or offload certain reports from the\nwarehouse into the data lake altogether. Remember the “who is the consumer” rule.\nHuman data consumers would rarely make any use of reports that are refreshed\nin seconds. \nHINT\nWhen assessing the needs of business users, don’t simply accept the\nword “real-time” from them. Instead, explore what they really need for data\ntimeliness.\nFinally, there are legitimate use cases where additional real-time processing infrastruc-\nture is required. If the end data consumer is an application that needs to perform\nsome actions based on incoming data, it’s a good indicator that ingestion and process-\ning needs to happen using a real-time data processing system. There are enough\nexamples of such use cases from different industries: various recommendation systems\nthat suggest content to the users on websites and mobile apps need to have a low-\nlatency response time; monitoring and alerting systems that detect anomalies and take\ncorresponding actions based on incoming data; fraud detection in the online pay-\nment systems, etc. As we will see later in this chapter, real-time processing comes with\nsome unique challenges and restrictions. We encourage you to carefully analyze the\nuse case before deciding what type of real-time processing is needed. Table 6.2 lists\nsome factors that might help you.\nAnother question that comes up often is whether it is easy to convert a process from\nbatch to real time. In our experience, the answer is no, because batch in real time today\nrequires different technologies, so conversion means complete code rewrite, bringing\nin new pieces of infrastructure, etc. It is easier to move from batch ingestion to real-\ntime ingestion than to convert a complex batch-processing job to real-time processing.\nChanging the ingestion type only affects a single layer in our architecture and, if your\nTable 6.2\nFactors influencing processing decisions\nUse case\nIngestion\nProcessing\nDashboards\nReal time\nBatch\nIn-game actions\nReal time\nReal time\nRecommendation engines\nReal time\nReal time\nFraud detection\nReal time\nReal time\n\n\n167\nOrganizing data for real-time use\nsource system can support both batch and real-time delivery, there may be existing\ntools that you can utilize: for example, switching from batch extracts from an RDBMS\nto real-time ingestion using CDC tools. We recommend investing more time into plan-\nning around real-time use cases and choosing the required approach early in the sys-\ntem design to avoid costly rewrites.\n6.4\nOrganizing data for real-time use\nIn chapter 5 we introduced folder and file layouts for cloud storage that can be used\nto organize data for efficient data processing. One of the key benefits of following a\nstandard layout is that you can create a configurable ETL pipeline that can be used to\nexecute data processing steps that are common for all the data sources. A standard lay-\nout also helps users of your data platform, who need direct access to the data lake\nlayer, to easily navigate between different data sets. \n In this section, we will describe how this layout translates from files and folders in\nthe batch scenario to real-time storage and processing. Before we can do that, we need\nto introduce several important real-time storage and processing concepts. \n6.4.1\nThe anatomy of fast storage\nBefore we dive into describing how storage for the real-time systems works, we need to\nnote that we will use Apache Kafka as our main example. Later in this chapter, we will\nlook into cloud-specific services for AWS, Azure, and Google Cloud, but we can’t use\nany of them to describe the common concepts. As these are proprietary systems, we\ndon’t really have significant insights into exactly how those services work or what\nunderlying technologies they use. Because Apache Kafka is the most popular open\nsource system for real-time data ingestion and processing, and many cloud services\nseem to at least adopt similar concepts and terminology, we will base our discussions\nin this section on Kafka. When talking about specific cloud services, we will highlight\nterminology or behavior that is different from one that Kafka uses. \n Batch systems work with files, and files consist of individual rows that contain data.\nReal-time systems operate on the level of an individual row or a message. A message is\nbasically a piece of data that can be written and then read from the real-time storage.\nExercise 6.1\nYou are building a social mobile app for runners, which should be able to notify a user\nabout how their friend performed on a particular part of the route and suggest they\npick up the pace or congratulate them on a job well done. Which data ingestion and\nprocessing should you choose for this use case?\n1\nReal-time data ingestion and batch processing\n2\nReal-time data ingestion and real-time processing\n3\nBatch ingestion and batch processing\n\n\n168\nCHAPTER 6\nReal-time data processing and analytics\nYou can think of a single row in a relational database as being a single message or a\nsingle row in a text log file. A single JSON document with attribute names and their\nvalues is a good example of a message. JSON also supports arrays of documents, but for\nreal-time systems, a message would typically be a single document from that array. In\nreal-time systems, messages are typically very small in comparison to batch files. We are\ntalking about several KBs to 1MB per message. Messages are organized into topics, which\nare similar to folders on a file system. \n Messages are written into real-time storage by producers and are read and processed\nby consumers. Both producers and consumers in this context are applications of some\nsort. As seen in figure 6.7, producers write messages into topics (1), and consumers\nread from topics (2). \nSo far, this doesn’t sound too different from a batch system where you can have file\nproducers saving data to the storage, and file consumers, such as an ETL pipeline,\nreading those files. But there are some fundamental differences in how producers can\nexchange data with multiple consumers.\n In this real-time processing pipeline, we have a single producer, which is our inges-\ntion application, reading new, updated, and deleted rows from some RDBMS. There is\na single topic in our real-time storage and two different consumer applications. One\n(consumer Y) is a data transformation pipeline that performs common data transfor-\nmations and saves processed messages to a different topic. Consumer Y is acting both\nas a consumer and as a producer, which is not uncommon in real-time processing sys-\ntems. Another consumer, consumer X, is a real-time analytics job of some sort. Maybe\nit’s a machine-learning application, because otherwise, why would it read raw data that\nhasn’t been processed by consumer Y yet? ;)\n \nConsumer Y saves\nprocessed messages\nto another topic.\n1. Producers\n    add new\n    messages to\n    the topic.\n2. Consumers\n    read from\n    topics.\nProducer\nTopic\nMessage 1\nMessage 2\nMessage n\nMessage n+1\n...\nConsumer Y\nConsumer X\nFigure 6.7\nProducers write messages into topics. Consumers, which are applications, read from \ntopics.\n\n\n169\nOrganizing data for real-time use\n Messages in real-time storage systems are immutable. This means once a producer\nhas written a message to the topic, it cannot change or delete it. As shown in figure\n6.8, when a message is written to the storage, it is assigned an offset (1)—an always-\nincreasing number that plays an important role in real-time processing systems.\nOffsets are used to send acknowledgements back to the producers, confirming that\ntheir message has been successfully saved (2). This way, producers know they can send\na new message. Consumers also track offsets by sending the offset of the latest pro-\ncessed message back to the real-time system (3).\nThe offset/acknowledgement mechanism is used for providing reliability. If a con-\nsumer fails (which it eventually will), it can easily resume processing where it left off\nby checking what was the last message offset it processed because producer offsets\noffer protection from failures in the real-time storage system. If a producer didn’t\nreceive an acknowledgement that a certain message has been successfully saved, it may\ndecide to retry saving the same message again.\n Offsets also provide a way for us to monitor the performance of various consumer\napplications. In our example and shown in figure 6.9, we can tell that consumer Y is\nahead of consumer X because it is processing messages with greater offset numbers. If\nwe notice that consumer Y is falling behind consumer X, this may indicate problems\nwith the consumer application code, network issues, etc. \n Hopefully, by now you will start to realize the benefits of real-time storage and pro-\ncessing systems when compared to file-based storage. In file-based storage, a file is a sin-\ngle unit that your data processing jobs operate on. You usually read a file as a whole and\n3. Consumers acknowledge messages\n    they have processed by saving the\n    offset number back to the storage.\n1. Each message in a topic is\n    assigned a sequential number,\n    called an offset.\nProducer\nTopic\nMessage 1\nMessage 2\nMessage n\nMessage n+1\n...\nConsumer Y\nConsumer X\n2. Offsets are used to provide an\n    acknowledgement to a producer\n    that a message was successfully\n    written.\nFigure 6.8\nReal-time storage systems provide a mechanism for tracking which messages have been \nwritten and which have been consumed by which application using a mechanism of offsets and \nacknowledgements.\n\n\n170\nCHAPTER 6\nReal-time data processing and analytics\nprocess it in one shot. While there are benefits to this approach (see our deduplication\ndiscussion later in this chapter) some things are more difficult, such as reprocessing\nonly some of the data from a file. Real-time systems offer not only the ability to process\none message at a time with low latency, but they also provide additional mechanisms for\nreliability and message tracking. This makes coordination between different processing\njobs easier, because now you can tell exactly which data the job is processing right now.\nFast storage is not only fast, it is also smart storage, meaning that a real-time system does\nquite a bit more than just saving bytes to disks. \n6.4.2\nHow does fast storage scale?\nTypically, we would not dive deeper into specific storage and processing systems archi-\ntecture, since this book is focused on architecture principles and ideas. For real-time\nsystems though, understanding how these systems work internally is important for\nunderstanding why certain architecture decisions have to be made or what issues you\nshould expect down the road and how to deal with them. \n One related question is, how do real-time storage and processing systems scale?\nData volume and velocity often come hand in hand. Real-time systems that deal with\ncollecting and processing data from sensors of some sort, or processing clickstream\ndata from a website, need to deal with very large data volumes and guarantee low\nlatency processing at the same time. How do they achieve this?\n Obviously, real-time systems (or any modern data processing systems, really) can’t\nrun on just a single physical or virtual machine. They have to be distributed and run\non a cluster of machines to provide the scalability we require today. Figure 6.10 shows\nhow data and processing are distributed to multiple machines in a real-time system.\n \n1. This consumer is our common \n    data transformation pipeline. \n    It is currently ahead of consumer X.\nProducer\nTopic\nMessage 1\nMessage 2\nMessage n\nMessage n+1\n...\nConsumer Y\nConsumer X\n2. Consumer X, in this example, \n    is performing some complex\n    calculations in real time. It is\n    reading a message with an\n    offset.\nFigure 6.9\nOffsets also provide a way for us to monitor the performance of various \nconsumer applications.\n\n\n171\nOrganizing data for real-time use\nLike many other distributed systems, real-time storage systems split messages in a\ntopic into multiple partitions. Each message in a topic belongs only to a single parti-\ntion. These partitions are physically stored on different machines. Real-time systems\ndecide where to place a partition in order to maximize data availability and perfor-\nmance. Each machine in a cluster can host multiple active partitions and multiple\ncopies of other partitions. In figure 6.10, we show only a single active partition per\nmachine and a copy of the other two partitions. “Active,” in this context, means that\nthis machine processes actual producer and consumer requests. Copies of other par-\ntitions are needed to guarantee data availability. If one of the machines fails, another\nmachine holding a copy of that partition will start serving requests for that partition. \n By splitting data into multiple partitions and placing those partitions on different\nmachines, real-time systems make sure that even a very active topic with potentially\nhundreds of thousands of messages being produced and consumed doesn’t over-\nwhelm a single machine.\nNOTE\nWhy not save each message into a single file on a cloud storage instead\nof using a real-time system? You can achieve an impressive performance writ-\ning and reading a single file on a cloud storage. For example, AWS documen-\ntation says you can read a “small” file from S3 with 100–200 ms latency. On\nthe other hand, Apache Kafka in one of the LinkedIn benchmarks achieved\n14 ms latency for writing and reading a message. While milliseconds latency\nPartition 1\nPartition 2\nProducers and\nconsumers interact\nwith a real-time\nsystem without\nknowing on which\nspecific machines\ndata resides. It’s the\nrole of the system to\nroute them correctly.\nEach partition \nis stored on a\nseparate machine.\nEach topic is split into several\npartitions. Each partition\ncontains only a portion of \nall messages.\nEach machine also stores \ncopies of other partitions, so \nif a single machine fails, data \nis always available elsewhere.\nMachine 3\nMachine 2\nMachine 1\nTopic\nPartition 3\nPartition 1\nCopy of\npartitions 2\nand 3\nPartition 2\nPartition 3\nCopy of\npartitions 1\nand 3\nCopy of\npartitions 1\nand 2\nFigure 6.10\nReal-time systems scale by splitting messages into multiple partitions and distributing those \npartitions across multiple machines. Actual data placement is hidden from producers and consumers.\n\n\n172\nCHAPTER 6\nReal-time data processing and analytics\nmay not be critical to your workload, things get different when you need to\nwork with a large number of small files on cloud storage. We are talking about\nhundreds of thousands or millions of small files. Cloud storage is not\ndesigned for scanning a large number of files. Latencies will spike when try-\ning to get a list of all files that you need to process. Processing engines such as\nApache Spark will have a hard time working with many small files, because\nthey are designed to deal with fewer, bigger files. We described this problem\nwhen talking about flushing real-time data to cloud storage for archiving in\nchapter 4. \nAs you can see, there are a lot of moving pieces, even in this simplified example. Real-\ntime systems abstract the internal mechanics from the producers and consumers, so\nend users don’t need to worry about how exactly the data is distributed across differ-\nent machines. Understanding these details is important, because it helps to under-\nstand different failure scenarios that will affect real-time data processing. We will look\ninto different failure scenarios and ways they affect your data processing pipelines\nlater in this chapter.\n To conclude our tour of real-time storage system internals, we would like to recom-\nmend additional reading for those who want to dive deeper into details, since we had\nto make a number of simplifications. A great place to start is the seminal blog post “The\nLog: What every software engineer should know about real-time data’s unifying abstrac-\ntion” by Jay Kreps, one of the creators of Apache Kafka, at http://mng.bz/WdVa.\n6.4.3\nOrganizing data in the real-time storage\nIn chapter 5 we talked about the importance of having a standard layout for files and\nfolders in the cloud storage. A standard structure makes it easy for people to navigate\nbetween different data sets and also makes it possible to implement a common data\nprocessing pipeline. The same principles apply to the real-time storage and real-time\nprocessing. Let’s take a look at figure 6.11, which will walk you through the different\nstages of data flow, as introduced in chapter 5.\n \n \n \nExercise 6.2\nWhat is the purpose of offset tracking in a real-time system?\n1\nTo provide a reliable way for consumers to resume processing after a crash or\na restart\n2\nTo improve performance\n3\nTo provide scalability of the real-time system\n4\nAll of the above\n\n\n173\nOrganizing data for real-time use\n1\nData arriving from the ingestion layer is saved into a landing area where all\nincoming raw data resides until it gets processed. Note that the ingestion layer\nis the only layer that can write to the landing area.\n2\nNext, raw data goes through a set of common transformations, and then is\nsaved into a staging area. \n3\nRaw data is copied from the landing area into the archive area to be used for re-\nprocessing, debugging pipelines, and testing any new pipeline code.\n4\nData transformation jobs read data from the staging area, apply necessary busi-\nness logic, and save data into the production area. \n5\nAn optional “pass-through” job copies data from staging to production and\nthen into the cloud warehouse as an exact replica of the incoming raw data to\nhelp debug issues with the business logic of other jobs.\nFailed\nLanding\nArchive\nStaging\nProduction\nData product 1\nData product 2\nData product 3\nCloud data\nwarehouse\nFast\ndata store\n1. Data arriving from the ingestion layer is saved into a landing area.\n3. Raw data is copied from the landing area into archive area to be used for reprocessing, \ndebugging pipelines, and testing any new pipeline code.\n2. Next, raw data goes through a set of common transformations, and then is saved into a staging area.\n4. Data transformation jobs read data from the staging area, apply necessary business logic, \nand save data into the production area.\n5. An optional “pass-through” job copies data from staging to production and then into the \ncloud warehouse to help debug issues.\n6. Different jobs read data from the staging area and produce data sets to be used for \nreporting or other analytical purposes.\n7. Each step of the flow must deal with failures, saving data into a failed area of the storage.\nFigure 6.11\nDifferent stages that real-time data flows through\n\n\n174\nCHAPTER 6\nReal-time data processing and analytics\n6\nDifferent jobs read data from the staging area and produce data sets to be used\nfor reporting or other analytical purposes. These derived data sets are saved in a\ndedicated location in the production area and loaded into the cloud ware-\nhouse. In real-time processing, these data sets can also be saved into a fast data\nstore (cache, RDBMS, NoSQL) for low-latency access.\n7\nEach step of the flow must deal with failures, saving data into a failed area of the\nstorage and allowing data engineers to debug the issues. Once addressed, data\ncan be reprocessed by copying it back into the landing area. \nAt this level of detail, the only difference between batch and real time is that real time\ncan be optionally saved into a fast data store if you need low-latency access to the pro-\ncessing results. Other than that, stages are similar to batch processing:\nRaw data is saved as is into the landing area.\nAfter applying common data processing steps, real-time data goes into a staging\narea.\nReal-time data is archived to regular cloud storage.\nVarious data transformation jobs that implement business logic read data from\nthe staging area and save the result back into the production area.\nIn case of failures, either in data processing jobs or data quality checks, data is\ncopied into a failed area for further investigation. \nIn the batch processing scenario, all these different areas are implemented as folders\non cloud storage with files inside those folders. We are using an agreed-upon naming\nconvention for files and folders, which allows us to construct a generic data processing\nand orchestration pipeline. In the real-time processing system, we don’t have a notion\nof folders and files; instead, we have individual messages that contain data and topics\nthat provide logical grouping of these messages.\n We could use topics instead of folders and containers on the regular cloud storage\nand assume that landing, staging, production, and failure are just different topics in\nthe real-time storage system, but details are always more nuanced than that. Let’s use\nour retail example to look further into these details. Let’s assume that after upgrading\nour POS system to a newer version that supports real-time data delivery, we can start\nreceiving individual messages from the three main tables in the RDBMSs that are used\nby the POS: orders, customers, and payments. Maybe our POS vendor has imple-\nmented a nifty change data capture (CDC) connector for Debezium and started send-\ning all the new, updated, and deleted rows from these three tables into our real-time\ningestion system. \n In chapter 4 we showed a couple of examples of what a CDC message looks like.\nFor example, the following listing shows how a new entry in the POS orders table will\nresult in the following (simplified) message.\n \n \n \n\n\n175\nOrganizing data for real-time use\n{\nmessage: {\n    before: null,\n    after: { \n               order_id: 3,\n               type: “PURCHASE”,\n                      order_total: 37.45,  \n               store_id: 2432,\n               last_modified: “2019-05-04 09:05:39”\n},\noperation: “INSERT”,    \ntable_name: “orders”    \n      }\n}\nFor now we can think of a message as a JSON document with certain attributes. Some\nof these attributes will be present in every CDC message. These common attributes,\nsuch as operation, table_name, before, and after, provide some additional data about\nthe message itself. For example, a table_name attribute contains the name of the source\ntable and will dictate what kinds of attributes will be present in the message body (the\nafter field). These attributes will match the source table schema where each attribute\nwill represent a certain column from the source table. Keep in mind that this is just an\nexample, and different ingestion systems may produce messages of different structures.\n We will have similar messages for the other tables in the POS database: customers\nand payments. Should we save all messages to a single landing topic, or should we cre-\nate separate topics for each of the message types: orders, payments, and customers? \n From a technical point of view, nothing really prevents us from creating a separate\ntopic per message type. We would then have topics such as landing_orders, landing_\npayments, and landing_customers, each containing only a certain message type.\nThere are certain limits that cloud providers impose on the number of topics that you\ncan create. For example, Google Cloud’s Pub/Sub real-time system allows 10,000 top-\nics per project. This is probably enough for most organizations, but is this the optimal\nway to organize the real-time data? \n As illustrated in figure 6.12, the challenge with having a topic for each specific\nsource is that even in a small system, one RDBMS source can have hundreds of tables\nand, thus, we will need a separate landing, staging, production, etc., topic for each of\nthem. This doesn’t help with data discovery and also means that you will need a sepa-\nrate, real-time, data processing job for each topic because, typically, real-time systems\nallow a single consumer to read from a single topic. We learned in chapter 5 that\nthere are great benefits to having a single configurable data transformation pipeline\nthat applies all common data transformations to all incoming data sources. \nNOTE\nSome cloud real-time services, like Google Cloud Pub/Sub, for example,\nallow a single real-time data consumer to read from multiple different topics.\nListing 6.1\nExample of a CDC message\nThis is the body of the \nmessage; it contains \nthe relevant data \nabout the order.\nIn CDC systems like Debezium, \nthe operation attribute of the \nmessage specifies whether this \nis a new entry in the source \ndatabase, an update to an \nexisting entry, or a deletion of \nan existing entry.\nAnother useful attribute of this CDC message is the name of the \nsource table. Knowing the name of the table, we can expect certain \nattributes to be present in the message body.\n\n\n176\nCHAPTER 6\nReal-time data processing and analytics\nAn alternative to saving data to different landing topics is to use a single topic, but use\na message attribute to identify what type of data this message contains. In this\napproach, we use the fact that for a real-time data processing job to do anything with\nthe data, it needs to look into the message structure itself. Figure 6.13 shows a single-\ntopic approach that uses a common data transformation job.\nPOS\nRDBMS\nIngestion\nlayer\nlanding_orders\nlanding_customers\nlanding_payments\nOrders\ntransformation\njob\nCustomers\ntransformation\njob\nPayments\ntransformation\njob\nstaging_orders\nstaging_customers\nstaging_payments\nThe real-time\ningestion layer\nuses table names\nto save messages\nto specific topic.\nReal-time\nstorage has a\nseparate topic\nper table.\nSeparate topics\nrequire separate\nreal-time data\ntransformation\njobs.\nThis requires separate \ntopics for thestaging\narea, etc.\nFigure 6.12\nOrganizing real-time storage using one topic per each ingestion object is possible but will \nlead to an explosion of topics as your cloud data platform grows.\n{\nmessage: {\n \nbefore: null,\n \nafter: {\n \n \n \norder_id: 3,\n \n \n \ntype:\n \n \n \n\"PURCHASE\",\n \n \n \norder_total: 37.45,\n \n \n \nstore_id: 2432,\n \n \n \nlast_modified: \"2019-05-04\"\n \n \n},\n \noperation: \"INSERT\",\n \ntable_name: \"orders\"\n \n}\n}\nPOS\nRDBMS\nIngestion\nlayer\nLanding\nCommon\ntransformation\njob\nStaging\nThe real-time\ningestion layer\nsaves all messages\nto a single topic.\nReal-time storage\nhas a single topic\nfor all tables.\nWe can use a common\ndata transformation\njob that uses\ntable_name attribute\nto decide which\ntransformations\nto apply.\nThis allows us to\nmaintain fewer\ndownstream\ntopics.\nFigure 6.13\nWe can use a single landing topic and rely on message attributes to apply required data \ntransformation logic without having to run multiple jobs.\n\n\n177\nOrganizing data for real-time use\nThis approach plays well with our idea of a single configurable pipeline for data trans-\nformations. For example, you can have a library of common data transformations for\nspecific message types and use the table_name attribute to call an appropriate library.\nThis reduces the number of active, real-time, data processing jobs and makes the solu-\ntion easier to monitor and maintain. It also reduces the number of downstream topics\nin the staging and production areas. \n Does this mean that a single-topic approach is always the preferred one? As you\nhave probably learned by now, the answer is that it depends on several different fac-\ntors. Generally, you should try and maintain a single landing topic with a common\ndata processing job, but if you are dealing with multiple real-time sources, especially\nthird-party ones, you may find yourself in a situation where message structures are\nvery different; or there are really no common transformations that are applicable to\nall data sources; or some sources may not provide an attribute that you can use in a\ncommon data transformation job to decide which transformations to actually apply. In\nthis case, you will need to use different landing topics, which you can postfix with the\nsource name, for example: landing_pos_rdbms, landing_web_clickstream, etc. \n Another scenario where you may need to use separate topics for different sources\nis if you start running into performance issues related to the cloud service limits and\nquotas. It’s typical for a cloud provider to import various limits and quotas on a ser-\nvice, so a single customer with a heavy load doesn’t impact other customers. All cloud\nvendors impose limits on how much data can be written to and read from a single\ntopic. If one of your sources starts to reach these limits, then your only option is to\nsplit different sources into separate topics, or, in some cases, even split a single source\nif it produces a lot of high-velocity data. \n Finally, sometimes there is a need to split data into multiple topics to implement\ncertain organizational restrictions. For example, in large organizations, certain data\ncan be processed only by a certain team and no one else outside this team can work\nwith a given real-time data set. In this case, as shown in figure 6.14, you can have a sin-\ngle real-time job, which reads from a landing topic and, depending on certain mes-\nsage attributes, saves data to a corresponding staging topic. \n Since none of the existing real-time storage and processing systems allows you to\ncontrol access to individual messages, splitting messages from different sensitive\ndomains into separate topics and controlling access to those topics is the only way to\nachieve a required security posture. \n All existing cloud real-time systems make it very simple to read a message from one\ntopic and save it to another topic, with minimal code. A job that inspects a message’s\nattributes and, based on those attributes, decides in which downstream topic to save\nthis message is a very common pattern in the real-time systems. Another example of\nthis pattern is real-time data quality checks, where you can inspect a message and\ndecide whether it should be saved into a staging topic or into a failed topic. Yet\nanother example is saving the results of a real-time job into an appropriate cloud data\nwarehouse table (if your cloud warehouse supports real-time ingestion) using a table-\nname attribute from the message. \n\n\n178\nCHAPTER 6\nReal-time data processing and analytics\n6.5\nCommon data transformations in real time\nIn chapter 5 we described some of the common data transformation steps that you will\nlikely need to implement in a batch-processing pipeline. This includes file format con-\nversion, data deduplication, and data quality checks. In this section, we will describe\nhow to implement them in a real-time processing system. We will also look into addi-\ntional transformations that we see often: combining real-time and batch data together.\nReal-time systems make some of these transformations simpler to implement and oth-\ners more complicated, so let’s dig in! \n6.5.1\nCauses of duplicates in real-time systems\nWe will start our discussion about common data processing steps in the real-time sys-\ntem with data deduplication. We talked about data deduplication in batch pipelines in\nchapter 5, and if you recall, it wasn’t that complicated. If you had a column in your\ndata set that could be used to uniquely identify a row, then you could use Spark built-\nin functions to easily deduplicate data.\n Things get more complicated in real-time systems. Our experience shows that\nwhen organizations move from batch pipelines to real time or implement brand-new,\nreal-time pipelines, the increased presence of duplicates in the final data sets puzzles\nmany data engineers and data analysts. The key challenge here is that now we need to\ndeal with two types of duplicates:\nPOS\nRDBMS\nHR\nRDBMS\nIngestion\nlayer\nLanding\nCommon\nrouting job\nAccess to these cloud resources \nis highly restricted.\nOnly members \nof the finance data\nengineering team\nhave access to this\ntopic.\nOnly members of \nthe HR data engineering\nteam have access to\nthis topic.\nWe use a routing job that doesn’t\nchange data, but only saves it to an\nappropriate topic based on certain\nmessage attributes.\nReal-time\ningestion saves\ndata from\nmultiple\nsources into a\nsingle topic.\nlanding_hr\nlanding_sales\nFigure 6.14\nTo control access and permissions to certain data, you need to split it into multiple \ntopics using a routing job.\n\n\n179\nCommon data transformations in real time\nDuplicate data introduced at the source\nDuplicates introduced as a part of real-time systems’ failure/recovery processes \nDuplicates introduced at the source are basically multiple copies of the same row of\ndata that for one reason or another happened in the source system. It could be a\nhuman operator copying/pasting data in an Excel spreadsheet that was later exported\nas a CSV file and loaded into our data platform. It could also be caused by an issue in\na relational database schema design where unique constraints are not enforced, allow-\ning duplicate data to be added and then ingested into our platform. There are many\ndifferent possible scenarios. What’s common about them is that they all happen out-\nside of our data platform, and we have little to no control over them. \n In batch pipelines, duplicates introduced at source—such as a duplicate row\ninserted into a source table by mistake or the same file delivered multiple times to an\nFTP server—are typically the only types of duplicates that you need to worry about.\nWhen it comes to the real-time system, a new type of data duplication problem is\nintroduced. Real-time systems need to balance many different parameters. They need\nto provide low-latency response, scale with increasing data volume, and provide guar-\nantees that data written to such a system is durably stored and will not disappear if one\nor more components of the real-time system fail. Because satisfying all these require-\nments at once is difficult (if at all possible), all existing real-time processing systems\nhave to compromise on certain characteristics to provide expected behaviors in the\nface of one or more components of the system failing. Losing data in the case of indi-\nvidual machine failures is usually not an option, so real-time systems introduce retries\nthat, in their turn, can cause data duplicates. \n During normal operations, real-time systems send an acknowledgement to the data\nproducers that their messages were successfully written to disk. This way, a producer\nknows that it’s safe to proceed to writing the next message. This flow is shown in fig-\nure 6.15.\n In distributed systems, and especially in the cloud, normal operations are not the\nnorm anymore. Real-time systems run on dozens or hundreds of machines, each with\nits own network, disks, and so on. Any of these components can and will fail all the time.\nActive partition 1\nCopy partition 1\n2. When a message is\n    successfully saved to disk\n    on machine 1, it sends an\n    acknowledgement to the\n    producer. The producer can\n    now move on to writing the\n    next message.\nMachine 2\n3. The message is\n    replicated to\n    machine 2\n    for redundancy.\nMachine 1\n1. A producer writes a \n    message to a partition.\nFigure 6.15\nDuring \nnormal operations, a \nreal-time system sends \nan acknowledgement to \nthe producer when data \nis successfully written \nto disk.\n\n\n180\nCHAPTER 6\nReal-time data processing and analytics\nFor example, imagine that a producer is writing a message, but an underlying machine\nthat hosts that partition is experiencing some sort of a network problem. As shown in\nfigure 6.16, the message is written to the disk on that machine, but the acknowledge-\nment cannot be sent to the producer because the network is not available. \n The real-time system now thinks that that machine is down and tells another\nmachine in the cluster to pick up serving this partition. Our producer doesn’t receive\nan acknowledgement that a message was written successfully and tries to write the\nsame message again. This time it succeeds, but the message is written to a different\nmachine. Now the network issue is resolved, and the machine that originally hosted\nthe partition is back online. We know the data was written to the disk, so we have a sit-\nuation where a single message was written twice to the same partition. There is now a\nduplicate message that we will need to deal with somehow during processing. This sce-\nnario may sound very complicated and unlikely to happen, but even in a small distrib-\nuted system with several machines, these types of failures happen more often that one\nmight anticipate. \n Consumers are also susceptible to these types of issues. Consumers are even more\nlikely to experience different types of failures, just because there are usually more data\nconsumers than there are data producers. As shown in figure 6.17, if a consumer\nreads a message, and then the consumer application crashes, or a machine experi-\nences a network problem similar to that described previously, the offset won’t make it\nback to the storage system. When the consumer restarts, it will reread and reprocess\nthe same message, causing a duplicate. \n3. The producer retries \n    writing the same message  \n    to a machine that it knows\n    has a copy of the partition.\n5. The producer is not\n    aware of the duplication\n    and moves on to the\n    next message to write.\nThe active\npartition for\nthis message\nkey is on\nmachine 1.\n2. A network issue between\n    the real-time system and\n    the producer makes it\n    impossible to send an\n    acknowledgement, but\n    the message is already saved\n    to disk on machine 1.\n4. This time, the\n    acknowledgement\n    goes through\n    without issues,\n    but message 1 is\n    now stored twice.\nActive\npartition on\nmachine 1\nSend\nacknowledgement\nfor message 1\nActive\npartition on\nmachine 2\nSend\nacknowledgement\nfor message 1\nActive\npartition on\nmachine 3\n1. A producer writes\n    a message to a\n    partition.\nReal-time\nsystem\nProducer\nOK\n???\nWrite\nmessage 1\nWrite\nmessage 1\nWrite\nmessage 2\nFigure 6.16\nA network issue between a real-time system and a data producer will prevent an \nacknowledgement being sent. If a producer retries writing the same message, it may result in \nduplicates.\n\n\n181\nCommon data transformations in real time\nYou may think that a consumer can minimize these types of problems if it acknowl-\nedges every single message it processes and thus minimizes the probability of failures.\nIn reality, most real-time systems acknowledge processed messages in bulk every sev-\neral seconds. Acknowledging every message is very expensive in terms of performance\nand will not scale to the volumes that are expected from such systems today.\nNOTE\nSome real-time cloud services such as AWS Kinesis use an external\ndata store to checkpoint which consumer processed which messages. This\ndata store (DynamoDB, in the case of AWS) is not a part of the real-time sys-\ntem, but it doesn’t eliminate the problem described previously. A consumer\ncan crash before it saves offsets of processed messages to DynamoDB.\n6.5.2\nDeduplicating data in real-time systems\nNow that we know what causes data duplicates in real-time systems, let’s explore what\nsolutions can be implemented to solve this. As with any non-trivial technical problem,\nthere is no silver bullet. In chapter 5 we showed how to deduplicate data in a batch\npipeline using a column that has a unique ID. Can we use a similar deduplication\nmethod for a real-time pipeline? \n What makes deduplication in the batch pipeline relatively simple is the fact that the\nbatch pipeline can “see” all the data that exists at any given point in time. For example,\nit is an easy task to find all duplicate rows in a file, because once a file is written to cloud\nstorage, it doesn’t change. So all we have to do is go row by row, find rows that have the\nsame unique ID value and filter them out, and save the result to a new file. The same\napproach, to some extent, can be applied to relational databases and NoSQL stores. \n In real-time systems, we don’t have the ability to “see” all the data, because data is\nbeing continuously added to the system. And while messages that have been written to\nthe real-time system cannot be changed, new data arrives all the time. Readers that\nhave a relational database background may argue that data is constantly added and\n4. After the \n    consumer restarts, \n    it asks a real-\n    time system \n    what the last\n    checkpoint was.\n5. The consumer \n    reads message 1 \n    again, resulting \n    in duplicate\n    processing.\n3. Before the consumer\n    can acknowledge\n    that it has\n    processed 1 and\n    2, it crashes.\n2. The consumer\n    reads and\n    processes\n    message 2.\n1. The consumer\n    reads and\n    processes\n    message 1.\nReal-time\nsystem\nConsumer\nRead and\nprocess\nmessage 1\nRead and\nprocess\nmessage 1\nRead and\nprocess\nmessage 2\nAcknowledge\nmessages 1\nand 2\nGet last\nprocessed\nmessage\nMessage 1\ndata\nCheckpoint:\nmessage 1\nMessage 1\ndata\nMessage 2\ndata\nCheckpoint\nFigure 6.17\nIf a consumer crashes before it can acknowledge that a certain message has been processed \nback to the real-time system, then after it is restarted, it will process the same message again.\n\n\n182\nCHAPTER 6\nReal-time data processing and analytics\nchanged in a database as well, but relational databases have the ability to read a cur-\nrent snapshot of the data using a mechanism called transaction isolation. This allows a\ndata reader to operate on data that was “frozen in time” and, from that perspective,\nthe data acts like a static file on cloud storage. \n As illustrated in figure 6.18, real-time systems distributed across potentially hun-\ndreds of machines can’t provide the ability for data consumers to read a snapshot of\nall data at any given point in time. All this makes creating real-time data deduplication\njobs challenging, but it doesn’t mean it’s impossible. \nWe will now take a look at some of the available options. \n One of the important concepts in real-time processing is a time window. Each\nmessage that is written to the real-time system gets assigned a timestamp that indicates\nwhen exactly the message arrived. Using this timestamp—or, if your messages have\nsome sort of event time from the source system, you can use that as well—you can group\nmessages together into various time windows. For example, you can group messages\nthat arrived in the last five minutes, in the last hour, etc. Such grouping is very\nimportant for various real-time analytics applications, for example, calculating an\nhourly total sales amount from an online store. There are different types of time\nwindows that can be used for different types of analytics, such as sliding and tumbling\nwindows. Tumbling windows allow you to slice a data stream into equal non-overlapping\ntime segments. For example, if every 30 seconds you want to know how many users have\nvisited your website, you can use a 30-second tumbling window. Sliding windows also\nsegment data streams, but unlike tumbling windows, sliding windows can overlap.\nThere are other types of windows, but discussing these concepts is outside of the scope\nof this book. \n Since windowing allows us to group messages together using a timestamp, we can\nnow detect duplicates inside the group, as shown in figure 6.19.\nusers.csv\n{user_id:1,name:“Alice”}\nA file has a boundary and\ndoesn’t change. This makes it\neasy to implement a program\nthat will find duplicate rows.\nA data producer is\nabout to send a\nduplicate message.\nReal-time systems don’t provide the capability to read a “snapshot”\nof all existing data. There is also no clear boundary for the data\nbecause new data is added all the time.\nA data consumer works\nwith a single message\nat a time.\n{user_id:3, name:“John”} {user_id:2, name:“Bob”} {user_id:1,name:“Alice”}\nuser_id name\nAlice\nBob\nAlice\n1\n2\n1\nFigure 6.18\nReal-time systems are unbounded and don't provide the capability to read a snapshot of all existing \ndata at once. This makes detecting duplicates a challenging task.\n\n\n183\nCommon data transformations in real time\nFor example, we can use a 10-minute window to group together messages that were\nadded in the last 10 minutes. Now we have a boundary around the data and can easily\ndetect duplicates inside this window. Hopefully, you are already seeing some of the\nissues with this approach. The key question here is, how big should your window be?\nData is being added to the real-time system continuously, so, looking at our previous\nexample, if a new duplicate message arrives at 10:47, it will not fit into the existing\nwindow and a duplicate will be allowed through. \n Time windows can’t have an infinite size. They essentially cache incoming data\nusing real-time processing system resources and a big enough window can overwhelm\neven a big enough machine. Cloud providers always have some restrictions and limita-\ntions when it comes to using windows. AWS Kinesis Data Analytics, for example, rec-\nommends limiting time windows to a maximum of one hour. Google Cloud Dataflow\nhas built-in mechanisms to detect data duplications, but with the limitation of a 10-\nminute window. \n Using time windows for data deduplication will never provide a 100% guarantee\nthat there will be no duplicates in your real-time data sets. In many cases, this is not\ngood enough. Is there a better way to catch duplicates in an unbounded data stream? \n One common way to implement effective deduplication in a real-time system is to\ncache unique IDs in a fast data store of some sort and then have the data consumer first\ncheck this cache to see if a specific ID has been processed yet. If a message with a given\nID has been processed before, there will be an entry with this ID in the cache and the\ndata consumer can safely ignore this message as a duplicate, as depicted in figure 6.20.\n This idea works very well, but there are some new challenges that it introduces.\nFirst of all, the data store you use needs to be very fast. A lookup time for a given ID\nshould be measured in milliseconds. This way, your real-time processing is not slowed\n{user_id:1,name:“Alice”}\n{user_id:3, name:“John”}\n{user_id:2, name:“Bob”}\n{user_id:1,name:“Alice”}\nA data consumer uses a 10-minute\nwindow to group messages\ntogether. This allows easily finding\nduplicates within a window.\n10:45\n10:40\n10:37\n10:34\n10:32\n{user_id:4,name:“Helen”} {user_id:1,name:“Alice”}\n{user_id:3, name:“John”}\n{user_id:2, name:“Bob”}\n{user_id:1,name:“Alice”}\nFigure 6.19\nUsing time windows, we can find duplicate messages within a window.\n\n\n184\nCHAPTER 6\nReal-time data processing and analytics\ndown by extra checks. Fortunately, most modern key/value stores and relational data-\nbases have no issues in providing this type of performance, especially when it comes to\nchecking a single ID.\n Depending on the data volumes that you are dealing with in your real-time pro-\ncessing pipeline, you may find that storing all unique IDs requires too many resources\nfrom the data store, and it either becomes too expensive to maintain or doesn’t guar-\nantee required performance. In our experience, you need to be dealing with data vol-\numes comparable to today’s internet giants such as Facebook, Google, or Amazon to\nstart seeing issues with storing all unique IDs. Because you only need to store an ID\nvalue and not the full message, it’s easy to calculate how big the cache data store will\nbe if you have an idea of how many unique IDs your system needs to be able to handle.\nFor example, if you are using 16-byte UUIDs as your unique identifiers, you only need\n~15 GB of storage to store a billion of them. This is far below the capacity that a mod-\nern database can handle. \n One of the biggest challenges that prevents people from adopting this approach is\nthe fact that it does introduce a separate data store. This data store not only needs to\nbe highly performant, but also needs to be highly available and avoid any type of data\nloss. If your ID data store is down, your whole real-time processing pipeline will halt. If\nyou had to implement something like this in a data center, the engineering and oper-\national overhead could be significant. \n Fortunately, we are only concerned with cloud data platforms in this book. Today\nall cloud vendors provide fully managed services for both fast key/value stores (Azure\nCosmos DB, Google Cloud Bigtable, AWS DynamoDB) and various types of managed\nrelational database services. It’s relatively easy to configure these services to be highly\navailable, and they require minimal operational overhead. \n{user_id:1,name:“Alice”}\n{user_id:3, name:“John”}\n{user_id:2, name:“Bob”}\n{user_id:1,name:“Alice”}\nA data consumer first checks if an ID already exists\nin the cache. If it doesn’t, it processes the message\nand saves it’s ID to the cache. If the ID already exists,\nthe message is marked as a duplicate and is not\nprocessed or saved.\n{user_id:4,name:“Helen”} {user_id:1,name:“Alice”}\n{user_id:3, name:“John”}\n{user_id:2, name:“Bob”}\n{user_id:1,name:“Alice”}\nuser_id=1,\nuser_id=2,\nuser_id=3\nA fast data store is like\na key/value store, and\nan RDBMS can be used\nto cache unique IDs.\nFigure 6.20\nIf we can store all unique IDs that we have already processed in a separate data store, then we can \ncheck if a given message has been processed before.\n\n\n185\nCommon data transformations in real time\n There is another approach to data deduplication in real time. This approach calls\nfor not performing any data deduplication in real time and instead relying on a desti-\nnation data warehouse (which allows for reading a snapshot of data at a given point in\ntime). This may sound like a naive approach to the problem, given the complexities\nwe have just described, but it works well if your primary use case is real-time data\ningestion and not necessarily real-time data analytics. \n If you are only concerned with delivering real-time data into the warehouse as fast\nas possible and then allow end users to perform ad hoc or scheduled analytics on the\nwarehouse, then from the perspective of implementation complexity, it is easier to\nallow duplicates through in the real-time pipeline. Then, produce a deduplicated set\nby either running a batch deduplication job in the data lake layer or performing simi-\nlar operations in the data warehouse itself. \n This approach basically pushes the problem of detecting duplicates from an\nunbounded real-time stream to a bounded system such as a batch pipeline or a cloud\ndata warehouse (table 6.3). The main limitation of this approach, besides the fact that\nit only works for real-time data ingestion, is that if you are dealing with large data sets,\nrunning a batch deduplication job can take a long time or require significant com-\npute resources. In this case, you can do a side-by-side cost comparison of the solution\nthat caches unique IDs versus the batch deduplication approach.  \n \n \nTable 6.3\nComparing ways to eliminate duplicates in real-time streaming\nNeed\nConsider\nPossible negatives\nPerform real-time analytics \nand make sure data is dedu-\nplicated at the beginning of \nthe pipeline.\nUse windowing to group messages \ntogether using a timestamp, and detect \nduplicates inside the group.\nTime windows have to be of a fixed \nsize. Cloud providers always have some \nrestrictions and limitations on the size \nof windows. There is always a \npossibility that a duplicate will arrive \noutside of a given window, so this \napproach doesn’t guarantee full \ndeduplication. \nCache unique IDs in a fast data store and \nhave the data consumer first check this \ncache to see if a specific ID has been pro-\ncessed. If a message with a given ID has \nbeen processed before, there will be an \nentry with this ID in the cache, and the \ndata consumer can safely ignore this \nmessage as a duplicate.\nRequires a very fast data store to avoid \nslowing your real-time processing with \nextra checks.\nStoring all unique IDs may become too \nexpensive to maintain or performance \nmay suffer.\nIntroduces a separate data store.\nDeliver real-time data into \nthe warehouse as fast as \npossible, and then allow end \nusers to perform ad hoc or \nscheduled analytics on the \nwarehouse.\nAllow duplicates to move through the \nreal-time pipeline, and then produce a \ndeduplicated set by either running a \nbatch deduplication job in the data lake \nlayer or performing similar operations in \nthe data warehouse\nRunning a batch deduplication job can \ntake a long time or require significant \ncompute resources.\n\n\n186\nCHAPTER 6\nReal-time data processing and analytics\n6.5.3\nConverting message formats in real-time pipelines\nIn chapter 5 we described converting files from the original format to a binary format\nsuch as Avro and Parquet. We suggested that Avro be used as a primary file format for\nthe staging area, where multiple data processing jobs may need to access it. Parquet,\nbeing a column-oriented store, is better suited for production storage areas, where\ncomplex analytics and fast performance is required. \n Do the same ideas apply to real-time systems? What would be an equivalent of a file\nformat in this case? Real-time systems don’t operate with files. Instead, they operate\nwith individual messages. Producers write messages to the real-time system and con-\nsumers read those messages. All existing cloud real-time systems are agnostic to the\nmessage content. For a system itself, a message is just a collection of bytes, and it’s up\nto data producers and consumers to agree on how to interpret those bytes into some-\nthing meaningful.\n Making sure that a data producer and a data consumer agree on the given message\nschema is very important in the real-time system. While batch-processing frameworks\nsuch as Spark can scan large volumes of data and try to infer schema, in real-time sys-\ntems, the processing context is typically a single message, and inferring schema can-\nnot be efficiently implemented. We will discuss schema management and ways for\nproducers and consumers to agree on message schemas in detail in chapters 7 and 8. \n In this chapter, we will focus on the performance aspect of different message for-\nmats. We often see developers choosing JSON as a format for their messages. It is not\nsurprising, given the format’s popularity and the fact that it’s easy to implement.\nBesides the fact that JSON doesn’t provide any schema-management capabilities, this\nformat is less than optimal from a performance point of view.\n We saw earlier in this chapter that real-time systems are distributed systems that\nactively send data over the network; data producers and data consumers can be\nlocated on the opposite sides of the globe; and data replication between different\nmachines also happens over the network. To get optimal performance given these fac-\ntors, each individual message size should be as small as possible. JSON as a message\nformat doesn’t make keeping message size small an easy task. The format is very ver-\nbose and has to include attribute names and values in plain text. For large messages\nwith several dozens of attributes or complex nested data structures, this overhead\nExercise 6.3\nWhat is the primary cause of duplicate data in real-time systems?\n1\nReal-time storage is fast, but unreliable. Sometimes data needs to be written\ntwice to ensure persistence.\n2\nAll duplicates are caused by bugs in the producer’s code.\n3\nDuplicates can be caused by consumers’ or producers’ failures as well as\nfailures in the underlying components, such as the network. \n\n\n187\nCommon data transformations in real time\nbecomes significant. Just to be clear, we are still discussing message sizes in the kilo-\nbytes range, but when multiplied by a number of messages, these sizes will become sig-\nnificant. You can use compression to reduce the size of JSON messages, but\ncompression algorithms perform better when they have a lot of “similar” data to com-\npress. Compressing a single message will require more processing and may not pro-\nvide expected size reduction.\n Using a binary format for real-time messages, such as Apache Avro, provides the\nsame benefits as batch pipelines. Binary formats are smaller in size to corresponding\nplain-text formats. Formats such as Avro also allow you to send an Avro-encoded mes-\nsage without sending a schema alongside them. This minimizes the message size even\nfurther, but requires a metadata layer to store the schema itself, so data consumers can\ndecode the binary data. We will talk about options for implementing this schema\nexchange mechanism in chapter 7. \n Do column-oriented formats such as Parquet provide any benefits in real-time\nsystems? The answer here is no. Column-oriented formats only provide optimizations\nfor workloads where you need to scan several columns from a large data set. Real-time\nsystems mostly operate on one message at a time and won’t benefit from column-\noriented optimizations.\n6.5.4\nReal-time data quality checks\nImplementing data quality checks that inspect a single message at a time in the real-\ntime system is a relatively straightforward task. We need to first define the set of rules\nthat would allow us to determine whether a message is “good” or “bad.” This is actu-\nally the hard part! If you are dealing with many different data sources that are con-\ntrolled by different teams, then coming up with a common set of rules is a challenging\norganizational task. \n Once you have the rules, you can use the message-routing job idea that we\ndescribed earlier in this chapter. You will implement a real-time processing job that\nwill inspect all incoming data from a landing topic, apply the rules and, based on the\nresult, publish the message either into a downstream staging topic or into a failed\ntopic, as shown in figure 6.21. In this example, we have a check for data in a source\nORDERS that says that an order_total value cannot be negative.\n The way you will actually implement the checks will depend on the cloud real-time\nsystem that you are using for your data platform. Many support a SQL-like language to\nanalyze the content of a given message, which makes it easy to implement checks that\noperate on a message-attribute level. We will look into various cloud real-time systems\nand their capabilities later in this chapter. \n Not all data quality checks can be implemented by just looking at a single message\nat a time. For example, in our retail system, we may want to implement a check that\nverifies that no more than 10% of all orders that happened in the last hour have a\nCANCELLED type. Maybe in our case, this usually indicates an issue with POS termi-\nnals or network issues. \n\n\n188\nCHAPTER 6\nReal-time data processing and analytics\nObviously, we can’t implement something like that just by looking at each individual\nmessage. If you recall our earlier discussion about data deduplication, this is where\ntime-based windows come into play again. You can use windowing functions that your\nreal-time system supports to group all messages from the “orders” source that came in\nthe last hour. Then you can count all orders in that window and compare it to orders\nthat have a CANCELLED type. \n Limitations similar to those in our data deduplication discussion apply. Windows\ncan’t be of an indefinite size, and usually cloud providers impose much stricter limita-\ntions on windows sizes. If your quality check requires looking at the last week, month,\nor year of data, you should consider implementing it in the batch-processing layer by\nusing the data that has been saved to the cloud storage from the real-time system.\n6.5.5\nCombining batch and real-time data\nIt’s unlikely that your organization will only need to deal with real-time data. Existing\nlegacy systems or third-party data sources often only allow you to extract data in batch\nmode in the form of files. A natural question that arises is, what if we need to combine\na batch data source with a real-time stream? \n Going back to our POS example, imagine that we have detailed information about\nall offline stores extracted from an existing Enterprise Resource Planning (ERP) sys-\ntem as a simple CSV file. This data doesn’t change often, and there is little reason to\n{\nsource: \"ORDERS\",\norder_id: 3,\ntype: \"PURCHASE\",\norder_total: -78,\nstore_id: 2435\n}\nMessages that don’t pass the\ncheck are saved to the failed\ntopic.\nA data quality job\nfetches required rules\nusing the message\nsource name.\nAn operational metadata\nlayer stores all data\nquality check rules.\nPOS\nRDBMS\nIngestion\nlayer\nLanding\nData\nquality job\nStaging\nFailed\nOperational\nmetadata\nlayer\nFigure 6.21\nReal-time data quality checks utilize the metadata layer to fetch the right checks for a \ngiven message. Messages that do not pass the check are saved to a failed topic.\n\n\n189\nCommon data transformations in real time\ningest it as a real-time stream. Our real-time stream from the POS system only includes\nstore_id—a unique number identifying each store. Our business users would like to\nsee full store information alongside POS data, so we need to figure out a way to com-\nbine the two data sets. \n Our store information CSV file is processed by the batch-processing layer and is\nstored in the cloud storage. We can adjust our real-time processing job for the POS\nsales transactions to read this file when the job starts, cache it in memory, and then\nuse it as a lookup dictionary to match store_id with expanded store information. This\nis shown in figure 6.22.\nThe main limitation of this approach is that the batch data set has to be small enough\nto fit into the memory of the machine on which the real-time processing job runs.\nReal-time jobs usually run on many virtual machines in parallel to deal with the scale\nof the incoming data stream. This means that batch data has to be copied into mem-\nory on each of those machines to be available for lookups.\n With existing cloud real-time services, it’s sometimes hard to tell what the exact\ncharacteristics are of the virtual machines your job is running on. Usually, batch data\nsets used for lookups are in the several-MBs size range, which would easily fit into\nThe message that comes out\nof a POS system doesn’t\ncontain any information\nabout the store, except\na store_id.\nA data enrichment\njob can now\ncombine two data\nsets using the store_id\nattribute.\nThe data enrichment job can read and\ncache the store information data set.\nCloud storage\ncontains another\ndata set with\ndetailed store\ninformation that\nis ingested from\nan FTP server\nevery day.\n{\nsource: \"ORDERS\",\norder_id: 3,\ntype: \"PURCHASE\",\norder_total: -78,\nstore_id: 2435\n}\n{store_id:2435, address:\"Barrington st…\", …},\n{store_id:2436, address:\"Agricola st..\", …},\n{store_id:2437, address:\"Lower Water st,..\" ,...}\nCloud storage layer\nData\nenrichment\njob\nProduction\nStaging\n{\nsource: \"ORDERS\",\norder_id: 3,\ntype: \"PURCHASE\",\norder_total: -78,\nstore_id: 2435,\nstore_location:\n\"Barrington st,...\"\n}\nFigure 6.22\nReal-time processing jobs can use batch data from cloud storage as a lookup dictionary to \nenrich the real-time stream.\n\n\n190\nCHAPTER 6\nReal-time data processing and analytics\nmemory in most configurations. You will need to refer to specific real-time cloud ser-\nvice documentation to understand their exact limitations. \n If a batch data set is too big to fit into memory, then you can always fall back to per-\nforming the join of the two data sets either in the data lake using batch-processing\ntools such as Spark or in the cloud warehouse by providing the end users a pre-joined\ntable or a view. This approach, of course, works only if your primary use case is real-\ntime data ingestion. \n6.6\nCloud services for real-time data processing\nSo far we have been describing generic real-time processing concepts that can be\napplied to pretty much any real-time system that exists today. In this section, we give a\nbrief overview of what real-time services are offered by three major public cloud pro-\nviders: AWS, Azure, and Google Cloud. We should warn you that cloud vendors\nrelease new features and make changes to existing systems every six months or so,\nmeaning that some of the behaviors that we will describe in this section may change in\nthe future. We encourage you to always refer to cloud vendor documentation for the\nup-to-date details. \n Each cloud vendor provides a pair of related services for the real-time processing:\none that implements real-time storage and maps to the fast storage layer in our archi-\ntecture and another one that implements the real-time processing. These services are\nusually tightly integrated and, in most cases, it makes sense to use them together (see\ntable 6.4). In some cases it may be possible to use a cloud vendor’s real-time process-\ning services with an existing real-time storage service such as Kafka. \n6.6.1\nAWS real-time processing services\nAWS offers two services in the real-time processing space: Kinesis Data Streams is a\nreal-time storage service, and Kinesis Data Analytics is a real-time processing service.\n Kinesis Data Streams consists of separate Data Streams, which are equivalent to\ntopics, in our terminology. Each Data Stream contains Data Records. These are\nindividual messages that producers write to the stream. Data Stream Applications, or\nconsumers, are applications that read messages from Data Streams. Data Streams\nApplications utilize Kinesis Client Library (currently available for Java, Node.js, .NET,\nPython, and Ruby languages). The client library hides a lot of complexity such as\ntracking sequence numbers of each processed Data Record and storing them in\nTable 6.4\nCloud services for real-time storage and processing\nReal-time storage\nReal-time processing\nAWS\nKinesis Data Streams\nKinesis Data Analytics\nGoogle Cloud\nPub/Sub\nDataflow\nAzure\nEvent Hubs\nStream Analytics\n\n\n191\nCloud services for real-time data processing\nDynamoDB, an AWS fast key/value store. This protects consumers from reprocessing\nlarge volumes of data in case of a crash, but all the limitations that we have described\nin this chapter still apply—if you save the processed record sequence numbers for each\nrecord, you will quickly run into performance limitations. If you only checkpoint for a\nbatch of records, there is always a possibility of processing the same record twice in case\nof failures. \n AWS also provides a service called Kinesis Data Firehouse, which is a managed\nData Streams Application. Firehouse is used to read data from Data Streams and save\nto a multitude of destination systems, or sinks. It is often used as a data ingestion\nmechanism, and we talked about in chapter 4. \n Kinesis Data Streams has a concept of shards (partitions, in our terminology). You\nneed to specify how many shards you want for a particular Data Stream. Shards are\nthe main unit of scalability for Kinesis Data Streams, with one shard only supporting a\nlimited throughput. Currently, each shard supports 1 MB/sec of data writes to the\nshard and 2 MB/sec of data reads from the shard. Kinesis Data Streams also supports\na resharding operation—you can increase or decrease the number of shards in\na stream.\n Kinesis Data Streams doesn’t impose any limitations on the format of the Data\nRecord, but like many other services, there is a limit on the Data Record size. At the\ntime of this writing, it is 1 MB. As with many other real-time cloud services, Kinesis\nData Streams has limitations on how long a Data Record can be stored in a Data\nStream. The default is a 24-hour retention period. This means that if you haven’t pro-\ncessed a record within this time, it will be purged and no longer available to you. You\ncan increase the retention period to a maximum of seven days, at an additional cost.\nAs you can see, fast storage cannot be used for archival purposes. That is why our\narchitectures suggest you save real-time data to a long-term regular cloud storage. \n Kinesis Data Streams Analytics is a real-time processing engine that allows you to\nsubmit real-time data processing jobs that will read the data from Kinesis Data\nStreams. Data Streams Analytics is a fully managed service—you don’t need to provi-\nsion and configure virtual machines to run your real-time job. All this is taken care of\nby the service itself. \n Kinesis Data Streams Analytics supports two APIs: SQL and Java. SQL allows you to\nuse a familiar SQL syntax to run interactive and ad hoc queries against your real-time\ndata streams. The SQL API only supports messages in CSV or JSON formats, which is a\nsignificant limitation because for performance and schema-management reasons, you\nwant to use a binary format for messages. \n The Java API is based on a popular open source, real-time processing framework\ncalled Apache Flink and provides more flexibility in terms of message formats and\ncontrol over job behavior, such as how often to checkpoint processed messages back\nto Kinesis  and the like.\n Kinesis Data Streams Analytics doesn’t provide any built-in mechanism for data\ndeduplication, and AWS’s recommendation is to cache unique message IDs and look\n\n\n192\nCHAPTER 6\nReal-time data processing and analytics\nthem up during processing. We described this approach earlier in the chapter.\nAnother deduplication recommendation from AWS is to rely on data destinations to\nperform the deduplication. As we mentioned earlier, this approach works only if your\nprimary use case is real-time data ingestion. \n6.6.2\nGoogle Cloud real-time processing services\nGoogle Cloud’s approach to the real-time processing is slightly different from other\ncloud providers. Google Cloud provides a more managed experience when it comes\nto the real-time storage and more advanced real-time processing capabilities in com-\nparison to what we see in other cloud providers today.\n Google Cloud’s real-time storage service is called Cloud Pub/Sub (publish and\nsubscribe). Cloud Pub/Sub uses the concepts of topics and subscriptions. Topics are\nused as a namespace to group messages together. Topics in Cloud Pub/Sub have less\nimportance than Data Streams in AWS Kinesis or topics in Apache Kafka, because\nCloud Pub/Sub doesn’t explicitly partition data into topics. Most likely, Google Cloud\ndoes partitioning behind the scenes, but from a data producer’s perspective, it is com-\npletely transparent. Producers in Cloud Pub/Sub just write messages to a topic with-\nout having to specify a partitioning key or worry about how many partitions a topic\nshould have.\n Cloud Pub/Sub data consumers need to have a subscription attached to one or\nmore topics to start receiving data. A single topic can have multiple subscriptions if you\nhave multiple different data consumers. You can also have a subscription that combines\ndata from two or more topics. This capability doesn’t exist in other cloud services and\nopens up interesting possibilities when implementing the message-routing job that we\ndescribed previously. \n Subscriptions are also a mechanism for Cloud Pub/Sub to scale. One subscription\nprovides 1 MB/sec data ingestion and 2 MB/sec data reads. Individual message size\nhas a maximum size limit of 10 MB, and maximum data retention is seven days. \n Cloud Pub/Sub doesn’t require data consumers to checkpoint message offsets,\nwhich simplifies consumer code. Since there are no explicit offsets, Cloud Pub/Sub\nhas to introduce different concepts for consumers to be able to replay messages. Cloud\nPub/Sub supports creating snapshots of a subscription, which becomes like a frozen\ncopy of that subscription at a given point in time. Data consumers can replay all mes-\nsages that were not consumed at the time a snapshot was created and all new messages\nthat have arrived since then. There is an extra cost to store snapshots, and also there is\na limit of 5,000 snapshots per Google Cloud project. This limits snapshots’ usability for\nfailure recovery, because you can’t snapshot too frequently. Google Cloud suggests\nusing snapshots mostly for planned outages, like creating a snapshot before you roll\nout an update to your real-time processing code. In addition to snapshots, data con-\nsumers can start consuming messages from a given timestamp, but this method is not\nprecise and most likely will result in the same messages being processed twice. \n\n\n193\nCloud services for real-time data processing\n Cloud Pub/Sub client libraries are simpler in comparison to AWS, because there\nare fewer configuration options and no need to perform checkpoints. Currently,\nCloud Pub/Sub client libraries are available for C#, Go, Java, Node.js, PHP, Python,\nand Ruby. \n Google Cloud’s real-time data processing service is called Cloud Dataflow. It’s a\nfully managed service that uses the open source Apache Beam API. What’s interesting\nabout Cloud Dataflow is that it supports both batch and real-time processing. This\nmakes it possible to unify two layers in our cloud architecture to use the same process-\ning engine. Cloud Dataflow supports SQL as well as Java and Python. SQL only sup-\nports messages in JSON format and provides less flexibility than Python and Java, but\ncan be useful for ad hoc analytics or simple ETL pipelines.\n Cloud Dataflow has support for various data sources and data sinks (destinations). It\ncan read from Cloud Pub/Sub, files on Google Cloud Storage, and other sources. Data\nflow can write to different Cloud Pub/Sub topics, Google BigQuery, or files on GCS. It\nis possible to implement your own data sources and sinks using the Apache Beam API.\n Cloud Dataflow has built-in capabilities to deduplicate data that comes from Cloud\nPub/Sub. Cloud Dataflow handles any internal Cloud Pub/Sub failures that may\nresult in data duplicates automatically. It can also deal with duplicates that were\ncaused by data producers sending the same message multiple times, if a message has a\nunique ID. The latter deduplication capabilities will only work if a duplicate message\narrives within a 10-minute window. For other scenarios, it is possible to implement dif-\nferent deduplication strategies that we described earlier in this chapter. \n6.6.3\nAzure real-time processing services\nAzure Event Hubs is a real-time storage service with some interesting characteristics.\nWhat’s interesting about Event Hubs is that it supports several protocols for data pro-\nducers: AMQP, Kafka, and custom HTTPS. Both AMQP and Kafka are open industry\nstandards with a number of clients that support these protocols available in open\nsource format. For an end user, this means fewer rewrites of their data producing or\nconsuming applications during cloud migrations if you are already using AMQP or\nKafka. \n The Event Hubs service consists of multiple namespaces that host multiple hubs\n(in the AMQP case) or Kafka topics. Event Hubs uses explicit data partitioning, and\neach topic can have between 2 and 32 (you can get more after talking to Azure sup-\nport) partitions. You need to configure the number of partitions up front and, unlike\nAWS Kinesis, this number cannot be changed later, so some careful planning is\nrequired. Partitions are the main scalability units in Event Hubs. Each partition gets a\nsingle throughput unit that includes 1 MB/sec or 1,000 messages/sec ingest rate and\n2 MB/sec or 4,096 messages/sec data consumption rate. You can prepurchase up to\n20 throughput units per Namespace, which Event Hubs will automatically assign\namong all existing partitions. This way, you can make sure to provide guaranteed per-\nformance for your real-time system. Event Hubs has a message size limit of 1 MB.\n\n\n194\nCHAPTER 6\nReal-time data processing and analytics\n Like AWS Kinesis or Kafka, Event Hubs exposes an offset number for each message\nin a partition. This number can be used to replay messages from the topic. Keep in\nmind that data retention for Event Hubs is seven days maximum. What’s different in\nEvent Hubs is that data consumers are responsible for storing offsets and managing\nthe checkpoint process themselves. In comparison, AWS does this by using a Dyna-\nmoDB key/value store, and Kafka stores checkpoint offsets in Kafka itself. Event Hubs\nclient libraries come with support for an Azure Blob Storage offset store, but if you\nwant to use a faster store, you need to implement it yourself or look for open source\noptions. As with other real-time storages, the frequency of checkpoints for data con-\nsumers is a trade-off between reliability and performance. Event Hubs client libraries\nare available for .NET and Python, but open source implementations for other lan-\nguages exist as well. \n Event Hubs has a supporting service called Event Hubs Capture that allows you to\nsave data from the real-time storage into Azure Blob Storage on a regular basis. This is\nsimilar to Kinesis Firehose, but only supports Azure Blob Storage currently. \n When it comes to real-time data processing, Azure provides the Azure Stream\nAnalytics service. This is a fully managed service, similar in this sense to what we have\nseen in AWS and Google Cloud. Azure Stream Analytics only supports SQL syntax and\ndoesn’t offer an advanced API in any other language. It is possible to extend SQL\nqueries with user-defined functions in C# and JavaScript. Azure Stream Analytics also\ncomes with lots of SQL extensions geared towards real-time analytics, such as\nwindowing support, window joins, lookups in dictionary data sets, etc. Azure Stream\nAnalytics supports a number of destinations it can write results to, including Azure\nSynapse, Azure SQL Database, and others. \n If you need more controls over your real-time processing jobs or want to use more\nsophisticated code organization patterns than what SQL offers, you can use the\nApache Spark Streaming API (remember the caveat about micro-batching from ear-\nlier in this chapter). The Azure Databricks service provides full integration with Event\nHubs as well and can be used to host your Spark jobs in a managed environment. \n Event Hubs and Azure Stream Analytics don’t provide any built-in mechanisms for\ndata deduplication. Azure’s recommendation is to use a destination, like Azure Syn-\napse, to perform deduplication. \nSummary\nThe processing layer is the heart of the data platform implementation, the layer\nwhere all the required business logic is applied, and the layer where all data val-\nidations and data transformations take place.\nWhen people use the terms “real-time” or “streaming” in the context of a data\nplatform, it can mean different things to different people, and it is relevant in\ntwo layers of a data platform—the ingestion layer and the processing layer. Real-\ntime or streaming ingestion takes place when you have a pipeline that streams\ndata, one message at a time, from a source into a destination such as storage or\n\n\n195\nExercise answers\nthe data warehouse or both. The terms “real-time data analytics” or “real-time\nprocessing,” on the other hand, are usually reserved for the application of com-\nplex computations on streaming data.\nChoosing between real-time ingestion, real-time processing, or both depends\non the requirements. If the only requirement for real time is that you must\nmake data available for analysis as fast as possible, but the analysis itself happens\nin an ad hoc manner, then real-time ingestion is what you should be imple-\nmenting. If, on the other hand, the requirement is to have the analysis itself\ndone in real time to be passed on to another system for action, then real-time\ningestion and real-time processing will be required.\nWhile batch systems work with files, and files, in their turn, consist of individual\nrows that contain data, real-time systems operate on the level of an individual\nrow or a message. A message is basically a piece of data that can be written and\nthen read from the real-time storage. Messages are organized into topics, which\nare similar to folders on a file system. Messages are written into the real-time\nstorage by producers and are read and processed by consumers. Both producers\nand consumers, in this context, are applications of some sort. Producers write\nmessages into topics, and consumers read from topics.\nReal-time systems (or any modern data processing system, really) can’t just run\non a single physical or virtual machine. They have to be distributed and run on\na cluster of machines to provide the scalability we require today.\nReal-time processing comes with some unique challenges and restrictions. Data\nduplicates are commonplace, and while there are several methods of dealing\nwith them, all of them have negative implications, from additional costs to per-\nformance impacts.\nWhen it comes to common message formats, using a binary format such as\nApache Avro for real-time messages provides the same benefits as batch pipe-\nlines. Binary formats are smaller in size than corresponding plain-text formats.\nFormats such as Avro also allow you to send an Avro-encoded message without\nsending a schema alongside them. This minimizes the message size even fur-\nther but requires a metadata layer to store the schema itself, so data consumers\ncan decode the binary data. \nThe Apache Kafka real-time system is a very popular, open source option for\ntooling, but each of the major public cloud providers also have real-time pro-\ncessing services.\n6.7\nExercise answers\nExercise 6.1:\n 2—Real-time data ingestion and real-time processing. You need to ingest data from\na user’s phone in real time and also have a real-time pipeline that compares users to\ntheir friends. Even a couple of minutes of delay will likely make the data inaccurate.\n\n\n196\nCHAPTER 6\nReal-time data processing and analytics\nExercise 6.2:\n 1—To provide a reliable way for consumers to resume processing after a crash or a\nrestart. \nExercise 6.3:\n 3—Duplicates can be caused by a consumer's or a producer’s failure, as well as fail-\nures in the underlying components, such as the network. Failure of producers can\ncause the same data to be written two (or more) times. Consumer failure can cause\nthe same data to be processed more than once. Network failures between producers\nand consumers can cause issues with saving the offset of a message and cause dupli-\ncate data processing. \n\n\n197\nMetadata\n layer architecture\nIn this chapter, we’ll help you get a clear understanding of what we mean by data plat-\nform internal metadata and why it is important to the operation of a data platform.\n We’ll cover the difference between configuration and activity metadata and how\neach can be used, using examples of a data platform with growing complexity. We\nThis chapter covers\nUnderstanding data platform technical metadata \nvs. business metadata\nLeveraging metadata to simplify data platform \nmanagement\nArchitecting the optimal metadata layer\nDesigning a metadata model with multiple \ndomains\nUnderstanding metadata layer implementation \noptions\nEvaluating commercial and open source \nmetadata options\n\n\n198\nCHAPTER 7\nMetadata layer architecture\nwill show why the metadata layer should become the primary interface for data engi-\nneers and advanced data users.\n We will describe a generic metadata model with four main domains—pipeline\nmetadata, data quality checks, pipeline activity, and schema registry—a model that we\nhave found to work across different organizations, focusing on the aspects of meta-\ndata that we found to be more or less universal. \n After the conversation about how to structure your cloud data platform metadata,\nwhich main metadata entities you should have, and what some of the common attributes\nof these entities are, we will explore how to implement it in a real project, focusing on\nthree metadata layer implementation options of increasing sophistication, starting with\nthe simplest option and providing advice on when each one might be needed.\n Last, we will do a high-level overview of existing open source and cloud services\naround metadata and how they differ from the model we have described.\n7.1\nWhat we mean by metadata\nSimply put, metadata is “a set of data that describes and gives information about other\ndata.” In the case of data platforms and data management, metadata is information\nthat helps us manage our own data better. There are two types of metadata used in\ndata platforms: business metadata and data platform internal metadata, sometimes\ncalled pipeline metadata.\n7.1.1\nBusiness metadata \nTypically, when people talk about metadata in the context of data management they\nusually mean business metadata. Business metadata is information, or “tags” that\ndescribe the organizational source of the data (Sales, HR, Engineering), the owner of\nthe data, the date and time of creation, file size, purpose of the data, rating of the qual-\nity of the data set, etc. Metadata is data that is not in the data itself, and it becomes even\nmore important when files are merged, because context or “assumptions” may be lost\nin the merger. For example, our US sales data may come from an RDBMS, and our\nCanadian sales data may come from a CSV file. These data sets might not have a sepa-\nrate column that describes the country where the sales transaction happened because\nthe applications just assumed that the context for this RDBMS or a CSV file is US or\nCanadian, respectively. But when placed into a single data platform, this context no lon-\nger exists, so we need to add some metadata to be able to distinguish between the two.\n Business metadata’s primary role is to facilitate data discovery for end users. When\na business user wants to find specific data sets or reports, they don’t want to search\nthrough hundreds of existing data sets: they want to search by typing something like\n“Sales Q1 2020 Canada+US quality=high” and get only the data sets and reports that\nhave these tags assigned. \n For many years in the data management domain, business metadata was the only\nmetadata that people were interested in. This is not surprising, because business\nmetadata and the tools that help manage it allow end users to work more efficiently,\n\n\n199\nTaking advantage of pipeline metadata\nfacilitating data use. With the advent of data platforms, the number of available data\nsets has increased exponentially, and the challenges associated with data discovery\nand data classification continue to this day.\n Addressing the business metadata management challenge is an array of tools\ndesigned to help organizations implement data tagging and data discovery. Alation,\nCollibra, and Informatica are just some of the commercial metadata software products\nthat specifically address these challenges. In addition, many of the ETL overlay tools,\nincluding Talend and Informatica, come with metadata management capabilities,\noften referred to as a Data Catalog. And of course, the public cloud providers also\noffer services in this space, including Google Cloud Data Catalog, Azure Data Catalog,\nand AWS Glue Data Catalog, which is part of the Glue ETL service. \n7.1.2\nData platform internal metadata or “pipeline metadata”\nThere is another type of metadata that is critical to a well-functioning data platform\nbut doesn’t get as much attention as business metadata. This metadata we call data\nplatform metadata or pipeline metadata. It describes the data pipelines themselves,\nhelping us understand which data sources a pipeline connects to and providing\ndetailed information about these data sources. Among other things, it can also tell us\nwhen the pipeline ran successfully and, if it failed, what error was associated with the\nfailure. It can also tell us things like who introduced a new data quality check that sud-\ndenly caused all the data to be marked as bad.\n This metadata is fundamental to automation, monitoring, configuration manage-\nment, and other operations. Without it, any data platform that has more than a cou-\nple of data sources and active pipelines will quickly become a nightmare to manage\nand operate. \n In this chapter and throughout this book we will be focusing on pipeline metadata,\nthough not because it is more important than business metadata. Both play important\nroles, but they do so at different levels. Data platform internal metadata is more\naligned with the needs of data engineering and data operations domains that are\nfocused on making the platform run smoothly. And while there are many existing\ntools and products and sufficient information available for business metadata manage-\nment, there is little available for data platform internal metadata. \n7.2\nTaking advantage of pipeline metadata\nLet’s imagine a simple, but typical, evolution of a data platform. Our example\nplatform starts with just a single batch data source—a file uploaded into an FTP\nserver. Our pipeline is a simple pass-through pipeline that applies some data quality\nchecks and ingests data into a cloud data warehouse. Figure 7.1 shows this simple one-\npipeline platform.\n This example may appear overly simplified, but we have seen many data platforms\nthat start with simple use cases like this. When you have a single pipeline that does\ningestion, some data quality checks, and maybe some basic data formatting, there is\n\n\n200\nCHAPTER 7\nMetadata layer architecture\nlittle need for pipeline configuration or pipeline metadata. Everything is in one place\nand is contained in a single code base. It’s easy to update a source location or adjust\nthe source or destination data locations—by updating a single ingestion job. \nNOTE\nWhile in the previous example the pipeline code requires little config-\nuration, you still need to configure the platform infrastructure, such as cloud\nstorage account, data warehouse, etc. You could also collect and use infra-\nstructure metadata, but we are excluding it from pipeline metadata because\nthe infrastructure tends to be stable, while pipeline code usually evolves rap-\nidly—driving the need for metadata. Cloud infrastructure configuration can\nalso utilize many infrastructure-as-code options, which are outside of the\nscope of this book.\nThis data platform is also very easy to monitor and operate for all the same reasons. If\nsomething fails and data doesn’t get into the data warehouse, there is only one place\nto look for issues: that one single pipeline. \n Soon, as is typical in real life, we need to extend our example platform by adding\nanother source. Let’s say it’s another file, but this time on AWS S3 storage. Figure 7.2\nshows how our original data platform changes with the addition of a second source.\nCloud data platform\nA single pipeline\nhandles ingestion,\nbasic data cleanup,\nand loading data\ninto the warehouse.\nCloud data\nwarehouse\n1\nFTP\nFigure 7.1\nA simple one-pipeline \nplatform doesn’t require much \nconfiguration because all logic is \nstored in the ingestion pipeline itself.\nCloud data platform\nPipeline that handles ingestion,\nbasic data cleanup, and loading\ndata into the warehouse\nfor the FTP source\nWhen adding a new source, do we duplicate\nthe first pipeline or try to extend the existing\npipeline to support a new data source?\nCloud data\nwarehouse\n2\n1\nAWS S3\nFTP\nFigure 7.2\nAdding a new \ndata source (2) introduces \noptions—to either create a \nnew pipeline to support the \nnew source or to add more \nlogic into the existing \npipeline.\n",
      "page_number": 180
    },
    {
      "number": 7,
      "title": "Metadata layer architecture",
      "start_page": 220,
      "end_page": 251,
      "detection_method": "regex_chapter",
      "content": "201\nTaking advantage of pipeline metadata\nAdding another data source such as a batch file forces us to make some tough deci-\nsions. If we want to keep the simplicity of having a single ingestion pipeline, then it\nwould suggest we add more logic into the existing pipeline code that would allow it to\nhandle connecting to AWS S3 storage, copying data over, performing data validations\nthat are relevant to this specific data source, etc. This can work, but what happens\nwhen you add more and more data sources? Some of those additional data sources\nmight be very different from the batch files, and they could even be real-time change\ndata capture streams from a database. This will cause your pipeline code to become\ncomplex and harder to maintain. \n Monitoring and operating this still is simple, but an extended data pipeline starts to\nget tricky. If data from AWS didn’t come through in time, what caused the delay? The\ndelay could be an issue with this specific source, or it could be a faulty connection to\nFTP. It’s difficult to tell the root cause of the problem even in this simple case.\n An alternative approach to adding a second data source could be to duplicate the\noriginal ingestion pipeline and adjust it to work with the new AWS S3 source. In this\ncase, you will need to deal with a different set of issues. You will end up with code\nduplication, which will make changes to your pipelines quite challenging. Think\nabout a scenario where you want to adjust one of the data quality checks because a\nbusiness requirement has changed. You will need to make the adjustment in two\nplaces instead of one. \n Throughout this book, we’ve advocated the use of a “configurable pipeline.” This\nmeans using the same pipeline code to handle data ingestion from similar source types\n(files or databases) or performing common data transformation and data quality check\ntasks. Instead of creating separate unique pipelines for these tasks, with names and loca-\ntions of the data sources hardcoded into them, we need to abstract pipeline configura-\ntion from the pipeline code. This approach will help you deal with the almost inevitable\never-increasing complexity, and it is flexible enough to handle change. Instead of cre-\nating a new pipeline or adding extra logic into an existing pipeline, we just need to\nupdate the pipeline configuration without touching the pipeline code itself. \n If we take our example data platform further, and as shown in figure 7.3, add sev-\neral data transformation pipelines that first apply some transformations on each\nsource separately (numbers 3 and 4) and then join the two sources together before\nloading them into the data warehouse (number 5), you can see the need to better\nmanage emerging complexity by separating actual pipeline implementations from\npipeline configurations—or metadata.\n Adding a metadata layer that stores information about what each pipeline actually\ndoes, where it should read data from, where it should save outputs to, etc., and com-\nbining this with configuration files allows you to make changes to the pipeline’s\nbehavior easily. You can now simply update a specific configuration file in the meta-\ndata layer instead of making changes to the pipeline code. This makes it much easier\nto scale your data platform because instead of updating the actual pipeline code, you\ncan simply update a specific configuration in the metadata layer. \n\n\n202\nCHAPTER 7\nMetadata layer architecture\nNOTE\nKeep in mind that as seen in chapter 4, implementation details of\ningestion pipelines for different types of data sources can vary significantly.\nThis means that you will most likely end up with different ingestion pipelines\nfor different source types: RDBMSs, flat files, real-time streams, etc. The meta-\ndata layer can still be used to store configuration for all these pipelines, pro-\nviding a unified interface for data platform operators.\nStoring pipeline configurations is not the only role that the metadata layer plays. In\nfact, one of the existing implementations for this layer by tech company LinkedIn is\ncalled “LinkedIn Datahub Project.” This gives you a hint of what responsibilities the\nmetadata layer has. It stores information about where data is coming from, where it is\nsaved into, and how it is being processed: how long it takes to process each step,\nwhether there are any issues with processing, and tons of other useful details. \n In our architecture the metadata layer has three major functions:\nIt is a central storage for all pipeline configurations.\nIt serves as a mechanism to track each pipeline’s execution status and various\nstatistics about pipelines (activity).\nIt serves as a schema repository.\nWe will talk in detail about schema repositories in chapter 8, so this chapter will focus\non the first two items and how to maximize your use of metadata. \n Simply put, the metadata layer should become the primary interface for data engi-\nneers and advanced data users to use to interact with the data platform. Following are\nsome examples of how pipeline metadata can be useful to users of a data platform.\nCloud data platform\nCloud data\nwarehouse\n2\n1\nAWS S3\n3\n4\n5\n● Which sources need to be ingested?\n● How do we connect to them?\n● Which quality checks need to be\n applied to which sources?\n● Which transformations require which\n data as inputs?\n● Where do transformations save their\n outputs?\nAs the number of pipeline increases, we\nneed to abstract pipeline configurations\nfrom pipeline implementations.\nIngestion\nand data\nquality\nData\ntransformations\nMetadata layer\nFTP\nFigure 7.3\nAdding more complexity, i.e., more sources and more transformations, drives the \nneed for a new layer to hold pipeline configurations.\n\n\n203\nMetadata model\n If a data engineer needs to update the location of a source file on an FTP server,\nthey can update a configuration for this source in the metadata layer instead of\ndirectly updating the pipeline code. \n If a data engineer receives an alert saying that a specific transformation pipeline\nfailed (activity), they should not have to dig through dozens of log files located on dif-\nferent machines; they should be able to request the latest pipeline status from the\nmetadata layer, which should have detailed status information. The alerting mecha-\nnism itself can be built on top of this layer. Failures, data ingestion rates, number of\nduplicates in the real-time stream—all this information is recorded in the metadata\nlayer and allows you to build all types of monitoring and alerting. \n Data users who would like to make the most of the data lake capabilities and\nmaybe even construct their own transformation pipelines should not wonder where a\nparticular pipeline gets its inputs from and where it stores its outputs. All this data is in\none place—the metadata layer.\n7.3\nMetadata model\nOne of the key challenges you will face is that there are no industry standards for a\ngood metadata model. If you are designing an operational database using relational\ntechnology, you will find a lot of available information about how to best organize data\nin your tables, pros and cons of different approaches, etc. Nothing like this exists in\nthe metadata domain today. There are works by some of the large internet companies\nsuch as LinkedIn and Lyft that are available in open source today and which we will\ndiscuss later in this chapter, but even those available metadata implementations are\nstrongly tied to the specifics of a particular company’s data engineering approach,\nbusiness domain, etc. None of these are universal enough to be easily applied by many\norganizations. \n In this section, we will describe a generic metadata model, one that we have found\nto work across different organizations. The model is intended to be flexible, which\nmeans we will only describe aspects of metadata that we have found to be more or less\nuniversal. Once you get the general idea about how to organize your data platform\nmetadata, you will need to decide whether you need to extend this model to fit your\nunique organizational needs.\nExercise 7.1\nWhich of the following is not a function of the metadata layer in our design?\n1\nProvide schema repository capabilities for all data sources.\n2\nProvide easy data search capabilities to end users.\n3\nProvide pipeline configuration storage capabilities.\n4\nProvide pipeline activity-tracking capabilities.\n\n\n204\nCHAPTER 7\nMetadata layer architecture\n7.3.1\nMetadata domains\nWe will group different metadata items into four main domains: Pipeline Metadata\n(configuration), Pipeline Activities (activity), Data Quality Checks (configuration),\nand Schema Registry (configuration). Figure 7.4 shows these domains and the rela-\ntionship among them.\nPipeline Metadata contains information about all existing data sources and data desti-\nnations as well as ingestion and transformation pipelines that are configured in the\nplatform. Data sources and data destinations store their schemas in the Schema Regis-\ntry, which we will cover in detail in chapter 8. Additionally, both ingestion and transfor-\nmation pipelines can apply different data quality checks, and information about these\nchecks is stored in the Data Quality Checks domain. Finally, each pipeline’s execution\nis tracked in the Pipeline Activities domain, including success or failure status, dura-\ntions, and other statistics such as the amount of data read/written, etc. If you think\nabout data platform pipelines as applications, then Pipeline Metadata is your applica-\ntion configuration and Pipeline Activities are your application log files and metrics.\nPIPELINE METADATA\nPipeline metadata is at the core of our architecture. It contains information about\nwhere data is coming from and how exactly it is being processed. It is easier to under-\nstand the key components of the Pipeline Metadata domain if we look at an example. \n Imagine that we are building a data platform to ingest and process data from two\nsources containing sales data (one a file and the other an RDBMS) and one source\ncontaining HR data (an API). We’d like to keep sales and HR data separate in our\nplatform so it is easier to control access permissions to this data.\n Figure 7.5 shows a model for the pipeline metadata associated with our example. At\nthe top level of the pipeline metadata is a Namespace. Namespaces can be used to log-\nically separate pipelines, data sources, and other elements of our data platform. In fig-\nure 7.5 we have two namespaces: Sales and HR. Each namespace has its own data sources\nand pipelines. Namespaces can participate in the naming of the pipelines themselves,\nData Quality\nChecks\nPipeline Activities\nSchema Registry\nStores information\nabout schemas for\nall data sources\nand data\ndestinations\nPipeline\nMetadata\nCaptures information and\nstatistics about pipeline\nexecution\nCentral storage for all pipeline configurations\nFigure 7.4\nMain \nmetadata domains and \ntheir interrelationships\n\n\n205\nMetadata model\nnaming of the folders in the cloud storage, topics in the real-time systems, etc. This\nallows you to assign permissions to the cloud resources based on the namespace. \n In our metadata data model, a namespace has only a few required attributes:\nID—A unique identifier for the namespace\nName—A short name that can participate in the naming of the cloud resources\nDescription—A more detailed description of what this namespace is about\nCreated Timestamp—Date and time of when the namespace was created\nLast Modified Timestamp—Date and time of when the namespace was last modified\nA Pipeline metadata object describes all your ingestion and transformation pipelines.\nFor example, we can have a “Ingest Sales RDBMS” pipeline or “Sales quarterly report”\npipeline. Pipelines take data sources as an input and produce data destinations as an\noutput. In our example metadata model, we have outputs of the “Ingest sales RDBMS”\npipeline used as an inputs to the “Sales quarterly report” pipeline (figure 7.6).\nNOTE While figure 7.6 shows that the “Sales quarterly report” pipeline uses\nwarehouse tables as an input, it doesn’t necessarily mean the pipeline physi-\ncally will read data from the cloud data warehouse. It’s definitely an option\nyou can use when creating your transformation pipelines, but a data destina-\ntion in the cloud platform usually has at least two representations: a table in\nthe warehouse and a collection of files in the cloud storage. We talked about\npros and cons of using a data warehouse and data lake for data processing in\nchapter 5. \nPipeline: Ingest sales\nRDBMS\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\ntable 4\nNamespace: Sales\nNamespace: HR\nPipeline: Ingest ﬁle\nPipeline: Ingest API\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nFigure 7.5\nPipeline metadata layout for a sample data platform with two domains: \nSales and HR\n\n\n206\nCHAPTER 7\nMetadata layer architecture\nThe Pipeline metadata object has the following attributes:\nID—Unique identifier for this pipeline.\nName—A short name that can participate in naming of the cloud resources.\nDescription—Provides more information about purpose of this pipeline.\nType—Is this pipeline an ingestion or transformation pipeline? \nVelocity—Is this pipeline batch or a real-time pipeline?\nSources and Destinations—This is a mapping of which data sources should be\nsaved into which data destinations. Sources and Destinations are separate\nobjects in our metadata model, and this attribute can include only Source and\nDestination IDs. For ingestion pipelines, there is usually a one-to-one mapping\nof sources to destinations. For transformation pipelines, usually multiple\nsources are used as inputs and one data output is produced.\nData Quality Checks IDs—A list of Data Quality Checks that need to be applied to\nall sources and destinations in this pipeline. Our model also supports adding\nchecks for a specific Data Source/Destination. Pipelines can combine multiple\nsources together. This allows pipelines to perform data quality checks on a\njoined data set, which is not possible when looking at each individual source\nseparately.\nCreated Timestamp—Date and time of when the pipeline was first created.\nPipeline: Ingest sales\nRDBMS\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\ntable 4\nNamespace: Sales\nNamespace: HR\nPipeline: Ingest ﬁle\nPipeline: Ingest API\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nAn output of \none pipeline (i.e.,\nWarehouse table\n1 from the Ingest\nsales RDBMS\npipeline) (1) ...\n...can be used as \nan input to another\npipeline, i.e., the Sales\nquarterly report\npipeline. (2)\nFigure 7.6\nFrom a pipeline perspective, data sources and data destinations are interchangeable. An \noutput of one pipeline can be used as an input to another pipeline.\n\n\n207\nMetadata model\nLast Modified Timestamp—Date and time of when the pipeline was last modified.\nThese two timestamps are very useful for troubleshooting because they give you\nan idea of when the pipeline was first created and when it was changed. \nConnectivity Details—Ingestion pipelines need to know how to reach the data\nsources. For example, they need to know the hostname/IP address of the\nsource RDBMS, FTP server, Kafka cluster, etc. \nNOTE\nNever store sensitive information like usernames and passwords for\nthe source systems in your metadata layer. Use dedicated cloud services like\nAzure Key Vault, AWS Secrets Manager, or Google Cloud Secrets Manager.\nA Source is a collection of data that we want to bring into the platform. What usually\nseparates one source from another is it’s schema. For example, a table in an RDBMS is\na single source, a topic in a real-time system is a source, and a collection of CSV files\nwith the same schema on an FTP server is a source. \n A Source metadata object has the following attributes:\nID—Unique identifier for this source.\nName—Can be a table name, a topic name, or a file name.\nSchema ID—A link to the Schema Registry that contains the schema for this spe-\ncific source.\nData Quality Check IDs—A list of IDs of the checks that we want to be applied to\nthis source.\nType—What type of a source this is. This value can be “table”, “file”, “real-time\ntopic”, etc.\nCreated Timestamp—Date and time of when this source was first added.\nLast Modified Timestamp—Date and time of when this source metadata was last\nupdated.\nA pipeline journey concludes with a Destination. In our cloud data platform architec-\nture, this can be a table in the cloud data warehouse, a topic in the real-time system, a\nkey/value store for fast application access, etc. There are some implied destinations as\nwell, which we don’t need to describe explicitly in our metadata layer. For example,\neach pipeline stores data in the cloud storage by design. This allows you to create\nchains of transformation pipelines, where the output of one pipeline serves as an\ninput to another one. In this case, some of the pipelines will not have explicit destina-\ntions attached to them. \n A Destination object is similar to the Source object, except a list of Destination\ntypes will be different:\nID—Unique identifier for this destination.\nName—Can be a table name, a topic name, or a collection in a key/value store.\nSchema ID—A link to the Schema Registry that contains the schema for this spe-\ncific destination. Keep in mind that source and destination schemas may not\nnecessarily be identical. \n\n\n208\nCHAPTER 7\nMetadata layer architecture\nData Quality Check IDs—A list of IDs of the checks that we want to be applied to\nthis destination.\nType—What type of a destination this is. This value can be “warehouse table”,\n“real-time topic”, etc.\nCreated Timestamp—Date and time of when this destination was first added.\nLast Modified Timestamp—Date and time of when this destination metadata was\nlast updated.\nDATA QUALITY CHECKS\nData quality checks involve the application of business rules to data in the platform.\nTheir purpose is to identify data that doesn’t fit within specified parameters. Figure\n7.7 illustrates how data quality checks can be applied to pipelines (Ingest file and\nIngest API) or data sources (employees API) across multiple namespaces (in this case\nHR and Sales).\nThere are typically two types of data quality checks: proactive and retrospective. Proac-\ntive data quality checks happen in the ingestion pipelines and are used to prevent data\nthat doesn’t pass the test from being added to the data platform. These checks usually\noperate on a column or a single row level. Retrospective checks, on the other hand, ana-\nlyze existing archive data, making sure that data still maintains logical integrity and con-\nsistency. You can think of proactive checks as filters that prevent “bad” data from getting\nPipeline: Ingest sales\nRDBMS\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\ntable 4\nNamespace: Sales\nNamespace: HR\nData\nquality\nchecks\nPipeline: Ingest ﬁle\nPipeline: Ingest API\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nFigure 7.7\nData quality checks involve the application of business rules to pipelines and/or data \nsources and are stored as configurations in metadata.\n\n\n209\nMetadata model\ninto the platform. Retroactive checks are more like audit reports that make sure that\nnew data combined with existing data doesn’t break any organizational quality rules.\nFigure 7.8 demonstrates differences between proactive and retrospective checks.\n Proactive checks are typically used to make sure data in a certain column follows a\npredefined format or to check that certain values never appear in columns. For exam-\nple, a birth-date column cannot have negative values, and first- and last-name columns\ncannot be empty. Because proactive checks happen during ingestion, they usually\ndon’t have access to data that already exists in the platform and therefore can’t join\nmultiple data sources together. This is not purely a technical limitation. For example,\nin chapter 4 we described batch deduplication pipelines that do require joining\nincoming data to existing data to find duplicates. You would want to limit heavy oper-\nations such as joining two large data sets together during ingestion because this will\nslow down ingestion pipelines significantly. \n Retrospective checks are more flexible, because they can run periodically on their\nown schedule. Retroactive checks are really just data transformations pipelines that\nPipeline: Ingest sales\nRDBMS\nDestination:\nWarehouse\ntable 4\nQA pipeline:\nEmployee count\nNamespace: Sales\nPipeline: Ingest ﬁle\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\nemployees table\nData source:\nEmployees\nAPI\nNamespace: HR\nData\nquality\nchecks\nPipeline: Ingest API\n1. Proactive: Make sure all date fields in this \npipeline follow the same format.\nProactive: Make sure all employees have \nnon-empty first- and last-name fields.\n2. Retrospective: Make sure departments \nwith no employees don’t exist.\nFigure 7.8\nProactive checks (1) happen during ingestion and can be attached to an ingestion pipeline \nor to individual data sources. Retrospective checks (2) are separate pipelines, similar to transformation \npipelines, except that they don’t modify any data.\n\n\n210\nCHAPTER 7\nMetadata layer architecture\ndon’t change any data. They can produce new data sets, like data quality reports, for\nexample. Retrospective checks can access all historical data and join multiple differ-\nent data sources to make sure that data is still consistent. For example, we can have a\nretrospective check that joins an employees data set with a departments data set to\nmake sure there are no departments without employees in the data. Retrospective\nchecks would typically produce a periodic report that is sent out to the data owners.\nThey can then decide on an appropriate action. This may include cleaning up data at\nthe source or making changes in the data platform itself.\n From the metadata perspective, both proactive and retrospective data checks are\njust collections of different rules that need to be applied to the data. Data quality\nchecks, metadata attributes, and structure will depend on how you decide to imple-\nment quality checks in your pipelines. There are many different options and existing\nproducts that can be used for data quality control. For example, you can implement a\ncheck as an SQL query and store this query together with a description in the Data\nQuality metadata. In some cases, we have seen organizations implement their own\ndomain-specific language (DSL) to configure all kinds of quality checks. They would\nthen store these DSL configurations in the metadata repository and fetch the ones\nthat are required for a given pipeline execution. \n Here are some common attributes that you should include in the Data Quality\nmetadata:\nID—Unique data quality check identifier. \nName—A descriptive name of the data quality check.\nSeverity—Different data quality issues have different criticality. Some data must\nnever enter the data platform, for example, negative numbers in the salary col-\numn of an employees table. Such data may break existing reports or down-\nstream pipelines and must be quarantined for further investigation. Other\nissues may not be as critical and should not prevent data from getting ingested,\nbut a data engineer needs to be notified when such issues happen, so they can\ninvestigate the root cause. We usually see three types of data quality check sever-\nity: “info”, “warning”, and “critical”. “Info” means the issue gets recorded in the\nactivities metadata, but no alert is raised and data is allowed to be ingested.\n“Warning” meaning the data is still allowed in, but an alert is raised, and “criti-\ncal” means data that doesn’t pass this check will not be ingested. \nRule—This attribute contains the logic for the check. As described earlier, it can\nbe an SQL query to run or a more sophisticated DSL configuration. \nCreated Timestamp—Date and time of when this check was added.\nLast Modified Timestamp—Date and time of when this check was last updated.\nPIPELINE ACTIVITY METADATA\nIn the previous section, we described pipeline configuration metadata, which includes\ninformation about how data flows are configured. Unlike pipeline configuration\nmetadata, pipeline activity metadata captures valuable information about the data\nflow as it is being executed. \n\n\n211\nMetadata model\n Pipeline activity metadata captures data describing successful pipeline execution so\nwe can then use this information to identify what a potentially problematic execution\nlooks like. This matters because pipelines are not static. You typically don’t perform a\ndata ingestion or particular transformation just once. Batch ingestion and transforma-\ntion pipelines are scheduled to run once or multiple times per day. Real-time pipelines\ndon’t have a particular start/stop time—they run continuously all the time. \n Things can change from one pipeline execution to another. A database may\nbecome unavailable for some period of time, or a daily file is misplaced by a third\nparty and doesn’t arrive at its expected location. As shown in figure 7.9, pipeline activ-\nity captures information about every pipeline’s status (successful, failed) as well as var-\nious statistics about pipeline execution, such as number of rows processed, pipeline\nexecution time, etc. This applies for both batch and streaming pipelines.\nWe store this information on an ongoing basis from the first time the pipeline is\ndeployed and typically don’t ever delete it. This archive of historical information\nabout the pipeline behavior has proven to be extremely useful for troubleshooting\nvarious operational issues, and it also gives you a way to analyze the behavior of the\nplatform overall. \n Pipeline activity metadata can capture a lot of different attributes, but here we will\nonly describe some common ones that we have found to be useful for many different\norganizations. When thinking about pipeline activity metadata, it is useful to visualize\nPipeline: Ingest sales\nRDBMS\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\ntable 4\nPipeline\nactivities\nNamespace: Sales\nNamespace: HR\nPipeline: Ingest ﬁle\nPipeline: Ingest API\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nPipeline activity captures\nstatistics and other helpful\ninformation about every single\npipeline run or a periodic\nsnapshot of a pipeline in a\nreal-time scenario.\nFigure 7.9\nPipeline activity captures statistics and other helpful information about every single \npipeline run or a periodic snapshot of a pipeline in a real-time scenario.\n\n\n212\nCHAPTER 7\nMetadata layer architecture\nit as a log file or table in a database. We will talk about some of the actual implementa-\ntion options later in this chapter.\n Here are some of the attributes that you want to capture about pipeline activities:\nActivity ID—Unique activity identifier. A unique ID must be generated for each\nentry in the activity metadata.\nPipeline ID—ID of the pipeline for which metadata is captured. This allows us to\nget detailed information about the pipeline from the pipeline metadata.\nStart Time—Timestamp of when the pipeline has started execution.\nStop Time—Timestamp of when the pipeline finished.\nStatus—Success or failure or other statuses that you may be interested in.\nError Message—Details of what exactly went wrong if the pipeline has failed. This\nsaves tremendous amounts of time sifting through various log files to find an\nerror. Since this error message is for the whole pipeline, it can include multiple\nerrors in case there were failures caused by individual rows. \nSource and Destination IDs—Since pipelines can read from many different sources\nand write to many different destinations, we need to know exactly which pair of\nSources and Destinations we are recording information for. For the ingestion\npipeline, this would typically be one Source and one Destination. For transfor-\nmations, this attribute can include multiple Sources and a single Destination.\nRows Read—How many rows the pipeline read from the Source.\nRows Written—How many rows the pipeline wrote to the Destination. For inges-\ntion, you typically would want this to match Rows Read, but if you are doing\ndata deduplication or filtering some data out based on data quality checks, this\nmight not be the case.\nBytes Read—Amount of data read from the Source. This is useful when dealing\nwith files to make sure the entire file was processed.\nBytes Written—Amount of data written to the Destination. Usually it will not\nmatch Bytes Read because of file format conversion, but it can be used for extra\nmonitoring, like making sure this value is not 0 if Bytes Read is not 0.\nExtra—This is a family of attributes that can include the batch id and the full\npath on cloud store where a batch pipeline saved data to, or real-time topic data\nwas written to, time windows for real-time pipelines, etc. \nExercise 7.2\nWhich of the following extra attributes should not be included in pipeline activities?\n1\nBatchId for the batch pipelines\n2\nWarnings produced when parsing incoming JSON, CSV, or XML files\n3\nList of all fields in the current data source\n4\nFull cloud storage paths for the input/output data\n\n\n213\nMetadata layer implementation options\nSince pipeline activity metadata is used for both batch and real-time pipelines, some\nattributes will have dual meaning. For example, Start Time and Stop Time attributes\nwork well for batch pipelines because they have clear start and stop times. For real-time\npipelines, on the other hand, you can use a predefined time window to capture these\nstatistics. You can decide to get real-time stats every five minutes, and then the Start\nTime and Stop Time for this pipeline will be a five-minute timeframe. This also applies\nto other time-dependent attributes, such as Rows Read, Rows Written, etc. It’s a good\nidea to align this timeframe with how often you flush real-time data into cloud storage\nfor archival purposes. This way, from the statistics-collection perspective, you are break-\ning down the real-time pipeline into a series of batch pipeline executions. \n To highlight the importance of collecting this information about your pipelines,\nhere are some questions you can ask of this data:\nWhat was the last successful execution of a given pipeline? Here you will need\nto find an entry in the activities metadata with Status being equal both to “suc-\ncess” and latest Start Time. \nWhat is the average duration for a given pipeline execution? This applies to\nbatch pipelines only, because for real-time pipelines, you will have a fixed time\nwindow in which statistics are collected. To answer this question, you will need\nto aggregate the difference between Start Time and Stop Time for a given Pipe-\nline ID where Status is equal to “success”. You want to exclude failed executions\nfrom this calculation.\nWhat is the average number of rows this pipeline processes? This question is\nuseful to ask when you want to establish a baseline of what looks normal for a\ngiven pipeline. This can be used for both batch and real-time pipelines, and to\nanswer this question, you need to aggregate the Rows Read and Rows Written\nmetrics for a given Pipeline ID where Status is equal to “success”.\nHow much data are we ingesting from a given RDBMS table every day? Here you\ncan do similar aggregation of Rows Read, but this time not by Pipeline ID but by\na specific Source ID and only take into account executions that were successful. \nHopefully this gives you some ideas of how pipeline activities can be used for getting\nvisibility into what your pipelines are doing. Not only can this data be used for ad hoc\nexploration and troubleshooting, it is also very easy to use these calculations for your\ncloud data platform monitoring to make sure you get alerted when a certain metric\nbehaves abnormally.\n7.4\nMetadata layer implementation options\nIn the previous sections of this chapter, we described how to structure your cloud data\nplatform metadata, which main metadata entities you should have, and what some of\nthe common attributes of these entities are. Once you know how your metadata\nshould look and what it is used for, the next logical question is, “How do you imple-\nment it in a real project?”\n Some disappointing news first. As mentioned at the beginning of this chapter,\nthere are no industry-standard open source or even commercial products that you can\n\n\n214\nCHAPTER 7\nMetadata layer architecture\nuse for your metadata layer. There are some solutions that we will review later in this\nchapter that may work for your use case out of the box, but our experience shows that\nthese solutions almost always require significant effort to adjust them to meet the spe-\ncific organization’s needs. \n This means that to have a metadata layer that satisfies your requirements, you will\nneed to build this yourself. This do-it-yourself approach may discourage some of you\nright away. We all know that building a brand-new software component from scratch\nrequires lots of time and effort. The metadata layer is an important component of the\ncloud data platform—without it, pipelines won’t know where to read data from and\nwhere they should store the results, and platform operators will be left clueless about\nwhat’s actually going on in the platform they support. \n All that said, don’t despair! Our experience shows that while most of the really effi-\ncient metadata layer implementations are implemented from scratch, there is a com-\nplexity scale where most simple implementations don’t require a lot of effort and will\nwork just fine for smaller teams or platforms with only a handful of data sources and\npipelines. At the other end of this spectrum are implementations that are more flexi-\nble and sophisticated and are capable of supporting hundreds of data sources, thou-\nsands of pipelines, and multiple data engineering teams. In this section, we will walk\nyou through three metadata layer implementation options of increasing sophistica-\ntion, starting with the simplest option. \n7.4.1\nMetadata layer as a collection of configuration files\nIn the previous section, we talked about two types of metadata. The first is metadata that\ndescribes pipeline configurations. This includes Namespaces, Pipelines, Data Sources,\nDestinations, the Schema Registry, and Data Quality Checks. And then there are Pipeline\nActivities that capture information about what happened during pipeline execution.\n One of the simplest ways to implement pipeline metadata is with configuration\nfiles. In the application development world, configuration files are widely used for\nstoring various application settings. We can adopt the same approach for our data\nplatform. We can represent Namespaces, Pipelines, Data Sources, Destinations, and\nData Quality Checks as separate configuration files, leaving the Schema Registry\nimplementation options to the next chapter. These configuration files can be in any\npopular format, including JSON, YAML, or any other format that your organization is\nmost comfortable with. We will use YAML for examples in this section. \n The following listing is an example of how a Namespace configuration file may look.\n---\nnamespaces: \nsales:   \n  id: 1234 \n  description: This namespace contains data from sales data sources\nListing 7.1\nExample namespace.yaml configuration file\nAll existing Namespaces \nare stored in a single file.\nThis is the name \nattribute.\nAll other namespace \nattributes are nested \nkey/value pairs.\n\n\n215\nMetadata layer implementation options\n  created_at: 2020-03-10 08:17:52\n  modified_at: 2020-03-15 14:23:05\nhr: \n  id: 1235\n  description: This namespace contains sensitive data from HR data sources.\n  created_at: 2020-03-01 10:08:40\n  modified_at: 2020-03-01 10:08:40\nThis configuration file is easy to read for humans, and it can be easily parsed by the\nactual pipeline code using any of the available YAML libraries for your language of\nchoice. You can now see how all other pipeline metadata domains like Pipelines, Data\nSources, etc., can be represented as configuration files. \n A big benefit of this approach is that configuration files are just text files and in the\nend can be stored together with other code in your code repository. Your code ver-\nsioning tool, such as Git, Mercurial, SVN, or any other, will also allow you to track all\nchanges to these configuration files the same way you track changes to your code. \n Figure 7.10 illustrates how configuration files can be used together with cloud log-\naggregation services to implement the metadata layer. Metadata configuration files are\nstored in the code repository and are versioned as the rest of your code. You can use\nthe same code repository you use for your pipeline code, or you can have a dedicated\nrepository just for the configuration files. Every time a change to the configuration\npipelines:\n - ingest_rdbms:\n  \nid: 1\n  \ntype: ingest\n  \n...\n - sales_report:\n  \nid: 2\n  \ntype: transform\n  \n...\nCode\nrepository\nCI/CD\nCode repository\nstores metadata as\nplain-text\nconfiguration files:\nJSON, YAML, etc.\nExample pipeline\nconfiguration\nfile in YAML\nformat\nConfiguration files\nare saved to the\ncloud storage\nusing an existing \nCI/CD process.\nPipelines save their activity\nmetadata into text log files in a\ndedicated cloud storage container,\nwhich will be ingested by a Cloud\nLog Aggregation service.\nIngestion and\ntransformation\npipelines read\nthese configuration\nfiles from cloud\nstorage.\nTransformation\npipelines\nCloud storage\nfor logs\nCloud storage\nIngestion\npipelines\nCloud Log\nAggregation\nservices\nFigure 7.10\nConfiguration files are stored in the code repository and are pushed to cloud storage \nusing the existing CI/CD process whenever a new configuration change is made. Pipelines use these \nconfiguration files to adjust their behavior. Pipelines use Cloud Log Aggregation services to save \nactivity metadata.\n\n\n216\nCHAPTER 7\nMetadata layer architecture\nfiles is ready to be released, your continuous integration/continuous delivery (CI/CD)\nprocess will copy the latest version of the files from the code repository into a dedi-\ncated location on cloud storage. Describing a CI/CD pipeline is outside the scope of\nthis book, but in the simplest scenario, this can be a script that a data engineer evokes\nafter making changes to the configuration code, or it can be a more automated\napproach using one of the many existing CI/CD tools like Jenkins, etc.\n The location of the configuration files on the cloud storage is up to you. You can\ncreate a separate storage container for this or just use a folder in the same container\nyou use for actual data.\n Your batch ingestion and transformation pipelines will then read these configura-\ntion files when they start and adjust their behavior accordingly. For real-time pipe-\nlines, you will need to implement periodic polling for the new version of config files\nsince those pipelines run continuously. \n Configuration files work well for the metadata that describes pipeline behavior but\nwhat about Pipeline Activities metadata? Unlike configuration data, activities are not\nstatic. Your pipelines will need to constantly add new entries to the activity metadata.\nAgain, there is an analogy to be made here to application development. Application\nlog files have been around for a long time and are a common way to capture “applica-\ntion activities” as a text file that you append new lines to. Log files are easy to imple-\nment, and most general-purpose programming languages have libraries to make that\nprocess even easier. \n There is one challenge with log files, though. It’s really hard to analyze them with-\nout specialized tools. Imagine that you have several dozen pipelines, each writing\ninformation about their status into a separate log file. You would have a hard time try-\ning to find the bit of information you want in those log files if you were to go through\nthem manually. There are dedicated tools, both commercial and open source, that\nsolve this problem, and we are lucky to be implementing our data platform in the\ncloud, because each cloud provider has a service that solves this problem without you\nhaving to install any additional tools. It’s worth mentioning that it is a good idea to fol-\nlow structured logging principles for your pipeline logs. Logs that follow a certain struc-\nture and clearly delineate between different attributes will be much easier to search\nand analyze in future.\n In figure 7.10 we call these services Cloud Log Aggregation Services, and their pri-\nmary function is to periodically take the log files that your pipelines generate, parse\nthem, and give you a UI to do all types of exploration and analytics. You can search for\nspecific keywords, filter log files for given timeframes, cross-reference, etc. On Azure,\nyou can use Azure Monitor together with Log Analytics; on Google Cloud, you can use\nCloud Logging; and AWS offers an Elasticsearch service for the same purpose. \n Going into details of these services is outside the scope of this book, but at a high\nlevel, the idea is that your pipelines will produce a text log file that includes all the\nattributes that we have described in the Pipeline Activities section. These log files will\nbe stored in a dedicated cloud storage container and then ingested automatically by\n\n\n217\nMetadata layer implementation options\nthe corresponding Cloud Log Aggregation service. You will then use the cloud ven-\ndor’s specific service UI or tools to run queries against your pipeline logs.\n7.4.2\nMetadata database\nStoring all pipeline metadata as configuration files works for smaller data platforms,\nbut once your cloud data platform grows, it will become more and more challenging\nfor a data engineer to find the information they need in those files. Imagine what will\nhappen once you get to several hundreds of data sources and dozens of pipelines,\nwith a couple of data quality checks assigned to each pipeline. In this scenario, you\nwill have configuration files with potentially thousands of entries. You can generate\nmany parts of those config files automatically using inputs. For example, given a list of\ntable names in a database, you can write a script to generate data source configuration\nfiles in your preferred format. However, daily platform operations will be more\nchallenging.\n Let’s say one of your many data quality checks suddenly starts failing on many dif-\nferent data sources. You will want to find all data sources that have this check attached\nto them. You can’t really run a query against your configuration files, and sifting\nthrough thousands of entries in the files manually is not going to work well either. To\nsolve this problem, you need to load your configuration files into a data store that sup-\nports some kind of query language to make these types of operations possible. Figure\n7.11 introduces a metadata database (1), which is a cloud database that will store your\nconfiguration files from the code repository in a more structured way.\n This metadata layer implementation looks very similar to the previous one, except\nconfiguration files are not stored in the cloud storage but are parsed and loaded into\npipelines:\n - ingest_rdbms:\n  \nid: 1\n  \ntype: ingest\n  \n...\n - sales_report:\n  \nid: 2\n  \ntype: transform\n  \n...\nCode\nrepository\nCI/CD\nMetadata\ndatabase\nConfiguration files are\nparsed and loaded into\ntables in the metadata\ndatabase instead of in\ncloud storage.\nTransformation\npipelines\nCloud storage\nfor logs\nIngestion\npipelines\nCloud Log\nAggregation\nservices\nFigure 7.11\nStoring configuration files in a \ndatabase instead of text files on cloud storage \ngives you the ability to run queries against \nthese configurations.\n\n\n218\nCHAPTER 7\nMetadata layer architecture\na database. This database can be either a relational database or a document-oriented\nkey/value store. The latter is often a better fit for configuration data, because the\nstructure of your metadata will be evolving, and it is easier to implement schema\nchanges in a document data store than it is in a relational database. Each major cloud\nprovider has fully managed services for relational databases and key/value stores. We\nhave seen successful implementations of this approach using Google Cloud Datastore,\nAzure Cosmos DB, and AWS DynamoDB.\n In figure 7.11 you will see that we still maintain and update text-based configura-\ntion files in our code repository. We do this because we must treat metadata as code\nand make sure that we have a version history of all changes. Some changes to the pipe-\nline code may require making changes to the structure of your metadata, such as\nintroducing new attributes, etc. In this case, you will want to synchronize the release of\nthe new versions of pipelines and configuration updates, and storing both in a code\nrepository (not necessarily the same one) will help. \n In order to load new versions of the configuration data into the metadata database,\nyou will need to implement a tool that can parse the configuration file format that you\nhave and then add, update, or delete specific entries. The database itself will be struc-\ntured to follow your metadata domain design. Each metadata type, such as Pipeline or\nNamespace, will become a separate table in the metadata database. The tool that is\nresponsible for loading configuration files into the database will need to check the\ncurrent data in the file and compare it with what’s in the database and then either add\na new entry, update an existing one, or delete an entry that no longer exists in the\ndatabase. \n Ingestion and transformation pipelines will read directly from the metadata data-\nbase and will require fewer changes, when compared to doing the same with configu-\nration files. Instead of going through hundreds of entries in the configuration file\nwhen they need to troubleshoot an issue, data engineers will now use a query lan-\nguage that the database provides. Usually it’s either SQL or a SQL-like language that\nmost data engineers are familiar with. All major cloud vendors also provide a basic UI\nfor these databases where you can run a query and explore the results, making work-\ning with metadata a much more pleasant experience. \n7.4.3\nMetadata API\nStoring metadata in a dedicated database is an approach that will work for most small\nand medium-size teams, but there are limits to its scalability. If your organization\ngrows beyond a single team developing and maintaining the cloud data platform, you\nwill find it challenging as multiple teams try to work with the metadata database\ndirectly. The problem here is the same one you will see in any large application devel-\nopment project. If you have two or more teams that need to work with the metadata,\nlet’s say building their own pipelines or tools, then those pipelines and tools become\ntoo tightly coupled with the structure of the database itself. If you need to change\nsome of the metadata entities or add new ones, then multiple teams get affected. And\n\n\n219\nMetadata layer implementation options\nanyone who has had to coordinate a large update of a monolithic application across\nmultiple teams knows it’s not a fun exercise. \n The solution for such large-scale data platforms is to introduce an API layer that\nhides the internals of the metadata database. The most common implementation of\nthe metadata API is REST, which provides teams with a way to make HTTP requests to\nthe service to add new metadata items, retrieve existing ones, and update or delete\nentries. APIs can be versioned, which makes rolling out changes to the underlying\ndatabase structure simpler. Figure 7.12 shows how previous metadata layer implemen-\ntations can be extended to include APIs (2). The metadata API abstracts the internal\nstructure of the metadata database from the pipelines, tools, and end users. This\nallows us to make changes to the database structure without major impact to those\npipelines and tools.\nDiscussing how to implement a cloud-based REST API service is outside the scope of\nthis book. Cloud providers offer different services that allow you to deploy such ser-\nvices in a fully managed environment. For more details on how to implement REST\nAPI services, feel free to check out another Manning publication such as The Design of\nWeb APIs (Arnaud Lauret, 2019).\n From the metadata flow perspective, the only major change is that instead of mak-\ning a connection to the database and writing data directly to it, the CI/CD pipeline or\nany other automation tools that you develop will make an HTTP call to the corre-\nsponding metadata API endpoint to make the necessary changes. Pipelines will also\nneed to switch to using HTTP calls instead of direct database connections. \npipelines:\n - ingest_rdbms:\n  \nid: 1\n  \ntype: ingest\n  \n...\n - sales_report:\n  \nid: 2\n  \ntype: transform\n  \n...\nCode\nrepository\nCI/CD\nMetadata\ndatabase\nMetadata API\nservice\nThis service provides \nAPIs to fetch, add, \nupdate, and delete \nmetadata.\nTransformation\npipelines\nCloud storage\nfor logs\nIngestion\npipelines\nCloud Log\nAggregation\nservices\nFigure 7.12\nThe metadata API \nabstracts the internal structure of \nthe metadata database from the \npipelines, tools, and end users.\n\n\n220\nCHAPTER 7\nMetadata layer architecture\n It’s very important to carefully assess on which level of maturity and scale your\ncloud data platform implementation is. It will help you to choose the right implemen-\ntation approach. We recommend that you choose the simplest implementation that\nsatisfies your requirements today and evolve the solution as your requirements\nchange. It might be tempting to jump right into the API implementation, which will\ntake significant engineering efforts to implement correctly, and neglect the needs of\nactual data users. \n Table 7.1 provides a summary of which implementation option is a better fit for\ndata platforms and teams of different sizes.\nIn our experience it’s always better to start with a simpler option and then gradually\nevolve your metadata architecture. None of the implementation options that we have\noutlined require you to completely rewrite to move from option 1 to option 2, for\nexample. Adding a database instead of working directly with the configuration files\ndoesn’t require you to change the structure of the configuration files, for example.\nThis makes it possible to move incrementally from one option to the next.\n7.5\nOverview of existing solutions\nAs mentioned before, the topic of collecting and maintaining pipeline metadata is not\nvery widely discussed in the industry today. Cloud vendors focus more on business\nmetadata tools that allow end users to catalog and search various cloud data sets.\nThese are useful services, but not for describing pipeline configuration. Large compa-\nnies with mature data platforms almost always end up implementing some kind of\nTable 7.1\nChoose an appropriate metadata implementation option for the data platform and data\nengineering team sizes.\nMetadata implementation option\nNumber of\ndata sources\nSize of the data\nengineering team\nOption 1, using plain configuration files\n1–5\n1–3\nOption 2, using a database\n5–10\n3–5\nOption 3, using a database with an API layer on top of it\n10 or more\n5 or more, multiple teams\nExercise 7.3\nWhat is the main benefit of using a metadata database with an API layer?\n1\nIt’s the simplest option to implement.\n2\nIt provides a common interface for multiple teams to interact with the \nmetadata.\n3\nIt allows you to store more metadata entities than other options.\n4\nIt provides faster performance than other options.\n\n\n221\nOverview of existing solutions\nmetadata layer solution that works for them, especially with some of these implemen-\ntations available as open source projects. In this section, we will provide a high-level\noverview of existing cloud services around metadata and how they differ from the\nmodel we have described, and we will also look into some of the existing open source\npipeline metadata projects. \n7.5.1\nCloud metadata services\nAcross AWS, Azure, and Google Cloud, there are several services that at least partially\nfit into the metadata layer model that we have described in this chapter. As much as\nwe would love to see a cloud-native service that is flexible enough to fully solve the\nproblems associated with data platform pipeline metadata, unfortunately, it doesn’t\nexist today. Some of the cloud services are close, but others are too focused on the\ndata discovery part of the problem. \n Among existing cloud services, AWS Glue Data Catalog is most aligned with the\nidea of the metadata layer described in this chapter. Glue Data Catalog stores informa-\ntion about data sources and data destinations as well as information about pipeline\nexecution and other statistics. It has an API that can be used to fetch and modify data\nin the catalog and also plays the Schema Registry role. \n Some of the most interesting features of AWS Glue Data Catalog are crawlers. Crawl-\ners are scheduled processes that connect to your data sources, scan them, and add meta-\ndata about the new tables of files that they discover since the previous run. Crawlers also\nperform schema discovery for each data source, which they store in Data Catalog. This\napproach fits nicely with our Pipeline Metadata/Schema Registry domains.\n AWS Glue Data Catalog also stores metadata about ETL jobs that you run, including\nhigh watermarks for database tables, latest processed files for file-based sources, etc.\nThese are called job bookmarks in Glue terminology. In addition to job bookmarks, Glue\nalso stores various statistics about jobs, such as the number of rows processed, etc.\n One major limitation of Glue Data Catalog is flexibility. AWS Glue Data Catalog is\nnot a standalone service but a component of the AWS Glue ETL service. This means\nthat in order to use Data Catalog properly, you need to author and schedule all your\nETL jobs in Glue. If all your pipelines are batch jobs, and you don’t have data sources\nthat Glue doesn’t support, such as REST APIs, you can implement most of the meta-\ndata and data processing layers in Glue. \n If you need to bring in a wider variety of sources that are currently not supported\nby Glue, you will need to implement a different way to store metadata. This splits your\nmetadata into Glue and non-Glue parts, which becomes a problem from a mainte-\nnance and operations perspective.\n With Azure and Google Cloud, existing offerings are more centered around busi-\nness metadata and data discovery. Azure Data Catalog allows users to register the loca-\ntion of a data source, for example, a table in a database: enter a description and\ndocumentation for each column. This can be either done manually or through an\nAPI. Once most existing data sources are added to the Data Catalog, data users can\n\n\n222\nCHAPTER 7\nMetadata layer architecture\neasily search for them using the catalog UI. This data discovery aspect of a data plat-\nform is really important if you want to increase the self-service capabilities of your plat-\nform. Essentially, it solves the problem of “only our data engineers know where the\nactual data is” and empowers end users to do more with the data. Azure Data Catalog\ndoesn’t fit the role of the pipeline metadata storage as described in this chapter. It\ndoesn’t capture a lot of the attributes that are required by ETL jobs and generally was\ndesigned to be used by people who consume data rather than those who program data\ningestion and transformation jobs. \n Google Cloud Data Catalog is very similar to Azure’s Data Catalog. It too is\ndesigned to serve as a central hub for data users to search for data they are interested\nin, but it can’t be used as the metadata layer that we have described, because it doesn’t\ncapture a lot of the pipeline-specific metadata attributes. \n To better understand where business-oriented Data Catalog services such as Goo-\ngle Cloud Data Catalog and Azure Data Catalog fit in our data platform design and\nhow they are different from the metadata layer we have been talking about in this\nchapter, let’s use figure 7.13 to look back at the platform layers diagram from chapter\n3, and add a data catalog between the data platform and the data consumers to show\nhow it helps data consumers find and access data.\n Figure 7.13 shows that a Data Catalog (the way it is implemented on Azure and\nGoogle Cloud) is a separate component that sits between the cloud data platform and\ndata consumers. It’s main role is to simplify data access and discovery for human\nusers, not to help with automation and monitoring of the data ingestion and transfor-\nmation pipelines. \nOrchestration overlay\nFast storage\nReal-time analytics\nIngestion\nProcessing\nOperational \nmetadata\nData\nwarehouse\nDirect data lake access\nSlow storage\nETL tools overlay\nData\nCatalog\nData\nconsumers\nData\nconsumers\nStreaming\ndata\nBatch\ndata\nFigure 7.13\nData Catalog sits between the data platform and data consumers and is primarily used to \nallow end users to use the platform in a self-service manner without knowing technical implementation \ndetails of the platform itself.\n\n\n223\nOverview of existing solutions\n7.5.2\nOpen source metadata layer implementations\nThere are several existing open source projects that we would like to mention here\nthat can, to some extent, be used as a basis for metadata layer implementations.\nBefore we go over some of the projects, it’s worth mentioning that in our experience\nnone of them can be used as a drop-in metadata layer implementation, but depend-\ning on your use case and engineering capabilities, your organization may either find\nthe provided functionality useful or you can use existing code as a foundation for a\nmetadata layer that is tailored to your needs.\n First in our list is Apache Atlas (https://atlas.apache.org/). Atlas combines two\nmain components. It acts as a central metadata storage with an API on top of it, and it\nprovides a UI for end users to do data discovery. So on a high level, Atlas does what\nexisting cloud data catalog services do today, plus it has a flexible metadata system that\ncan be used for storing pipeline information. \n When it comes to pipeline metadata, Atlas provides a very flexible type/entity sys-\ntem. Types in Atlas terminology are metadata items that we have described previously\nin the chapter. For example, a Data Source or Data Destination in our model can be\ndescribed as an Atlas Type. Types are basically collections of attributes. Atlas has a rich\ntype system that allows you to not only define attributes of primitive types like num-\nbers or string, but also arrays of values (useful for things like attaching data quality\nchecks to specific pipelines or sources) and references, which allow you to link one\ntype to another. If you go back to our metadata model, shown again in figure 7.14, you\nwill see that we use references extensively to link metadata items together, such as link-\ning a data source and destination to a specific pipeline. \nPipeline: Ingest sales\nRDBMS\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\ntable 4\nNamespace: Sales\nNamespace: HR\nPipeline: Ingest ﬁle\nPipeline: Ingest API\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nFigure 7.14\nApache Atlas supports linking of one \ntype to another using references. This works well for \nour metadata model that uses references to link data \nsources to pipelines, data quality checks to data \nsources, namespaces to pipelines, etc.\n\n\n224\nCHAPTER 7\nMetadata layer architecture\nApache Atlas Entity is a concrete representation of a type. Where type describes which\nattributes, let’s say, a pipeline has, an entity describes a specific pipeline. If you are\nfamiliar with object-oriented programming concepts, you will notice that types are\nsimilar to classes and entities are objects. \n Apache Atlas allows you to describe types and specific entities using either a REST\nAPI or a built-in UI. The fact that the type/entity system is flexible and allows you to\ndefine any types and relationships between them that you want means that you can\nuse Atlas to implement the metadata model that we have described in this chapter.\nBecause a second part of Atlas provides capabilities for adding, searching, and propa-\ngating business metadata, it is a two-in-one product, which is useful for both data plat-\nform automation and data discovery/self-service purposes. \n The major limitation of Atlas is that it was created for working with the Hadoop plat-\nform. This means that a lot of its out-of-the-box functionality is tied to specific Hadoop\ncomponents that we don’t really have in the cloud data platform design. For example,\nAtlas has some built-in data sources from which it can import metadata automatically\n(as opposed to creating types and entities using an API or UI). These sources include\nHadoop or Hadoop ecosystem components like Hive, HBase and others.\n Another downside of Apache Atlas is that it requires HBase and Solr components\nto store and index/search metadata, respectively. Both HBase and Solr are open\nsource projects as well, meaning you can download and configure them anywhere,\nincluding in the cloud. Using open source instead of PaaS options goes against our\nprinciple of using as many cloud managed services as possible as it can add a signifi-\ncant operational overhead. If you don’t have a lot of experience in managing an\nHBase installation in production, you probably don’t want to start now. \n To summarize, Apache Atlas offers a very flexible type/entity system that can be\nused to implement any metadata model, including one that we have described in this\nchapter. It also provides business metadata editing and searching capabilities, similar\nto existing cloud data catalog tools. On the other hand, Atlas only provides Hadoop-\nspecific plugins for metadata discovery out of the box and requires some relatively\ncomplex infrastructure to run. We hope that the Apache Atlas community will at some\npoint make this project more cloud native and allow using cloud services instead of\nHBase and Solr. Or maybe it will be your organization that will contribute these\nchanges back to the project. It is open source, after all. :)\n Another open source project in the metadata management space is DataHub\n(https://github.com/linkedin/datahub), developed by LinkedIn’s engineering team.\nAs you can imagine, LinkedIn has a highly mature data platform and metadata\nmanagement design and implementation because of the amount of data they need to\ndeal with and the size of their data engineering, data user community. DataHub is\nsimilar to Apache Atlas because it not only provides capabilities to add information\nabout data sources, their location, etc., but also provides data discovery capabilities for\nend users. \n\n\n225\nOverview of existing solutions\n DataHub allows you to create your own metadata entities, so you can use it to imple-\nment the metadata model that we have described in this chapter. It’s worth mentioning\nthat DataHub documentation is rather sparse at the moment, which makes it harder to\nquickly adopt this project for your needs. DataHub has drawbacks similar to Apache\nAtlas—it requires some extra infrastructure to be deployed in order to function prop-\nerly. DataHub requires a Kafka cluster in order to collect metadata from various data\nsources. It uses a MySQL database as a primary metadata store and uses Elasticsearch\nand Neo4j graph databases to provide search capabilities to data users. While all these\ntechnologies are open source, it will require significant operational overhead in order\nto deploy and manage all of them in a production environment. \n Finally, we would like to mention the Marquez project (https://github.com/\nMarquezProject/marquez), developed by WeWork engineers. Marquez is different\nfrom Apache Atlas and DataHub because it doesn’t provide the Data Catalog func-\ntionality. It’s main purpose is to collect and track information about data sources and\nthe pipelines that process those data sources. This information is then used to allow\nend users to visualize and search through data lineage information. Data lineage is like\na family tree for data. For each data set in a data platform, data lineage provides infor-\nmation about which pipelines and data sources produced it all the way through to the\noriginal set of data that served as an origin for this data set. \n While data lineage functionality is very useful, it is still mostly used for data discov-\nery purposes. Marquez doesn’t have (at least currently) the functionality to imple-\nment the full metadata model that we have described. For example, you can’t create\nnew metadata entities besides the ones that are already available. Marquez’s metadata\nmodel is very simple and consists only of Data Sets and Job (pipeline) entities. Each\nJob is linked to the actual pipeline code version, and each Data Set is linked to one or\nmore Jobs, allowing you to track data lineage. This functionality can be adopted for\nactivity tracking in our metadata model. Marquez provides libraries for Java and\nPython to allow Jobs to register their progress in a Marquez data store. This can also\nbe used to produce pipeline logs that will be sent to a Cloud Log Aggregation service,\nbut this will require you to make changes to the Marquez code itself. \n From an operations and infrastructure perspective, Marquez is relatively simple\nand only requires a PostgreSQL database as it’s main metadata storage. It should be\npossible to use one of the cloud PostgreSQL managed services to minimize the opera-\ntional impact. \nSummary\nThere are two types of metadata used in data platforms: business metadata and\ndata platform internal metadata, sometimes called pipeline metadata. While\nbusiness metadata is intended for use by business users, data platform metadata\nor pipeline metadata describes the data pipelines themselves. \nThis data platform metadata is fundamental to automation, monitoring, config-\nuration management, and other operations. Without it, any data platform that\n\n\n226\nCHAPTER 7\nMetadata layer architecture\nhas more than a couple of data sources and active pipelines will quickly become\na nightmare to manage and operate. \nThe metadata layer has three primary functions:\n– It is a central storage for all pipeline configurations.\n– It serves as a mechanism to track each pipeline’s execution status and various\nstatistics about pipelines.\n– It serves as a schema repository.\nThere are two types of pipeline metadata. The first is metadata that describes\npipeline configurations (Configuration) and the other is metadata that cap-\ntures information about what happened during pipeline execution (Activity).\nMetadata items can be grouped into four main domains: \n– Pipeline Metadata (Configuration)—Information about all existing data sources\nand data destinations, as well as ingestion and transformation pipelines\n– Pipeline Activities (Activity)—Information about pipeline success or failure sta-\ntus, durations, and various other statistics, such as the amount of data\nread/written\n– Data Quality Checks (Configuration)—Information about the quality checks\napplied to ingestion and transformation pipelines\n– Schema Registry (Configuration)—To be covered in chapter 8\nData engineers can update a configuration in the metadata layer instead of\ndirectly updating the pipeline code, which makes it easier to scale the platform.\nThey can investigate pipeline failures by requesting the latest pipeline status\nfrom the metadata layer. Data users who would like to make the most of the\ndata platform capabilities can easily see where a particular pipeline gets its\ninputs from and where it stores its outputs.\nThe metadata layer should become the primary unified interface for data engi-\nneers and advanced data users to interact with the data platform.\nThere are different approaches to architecting a metadata layer—from simply\nstoring all configuration files in cloud storage to using a dedicated database,\nwith or without a REST API. We recommend that you choose the simplest\nimplementation that satisfies your requirements today and evolve the solution\nas your requirements change. \nThere are currently no industry standards for a good pipeline metadata model,\nand most available software and cloud services don’t fully address the require-\nments described in this chapter. That said, Amazon, Google, and Azure all offer\nmetadata services. There are also several, also limited, open source metadata\nlayer implementations (Apache Atlas, DataHub, and Marquez, for example)\nthat might help accelerate your development activities. \n\n\n227\nExercise answers\n7.6\nExercise answers\nExercise 7.1:\n 2—Provide easy data search capabilities to end users.\nExercise 7.2:\n 3—List of all fields in the current data source.\nExercise 7.3:\n 2—It provides a common interface for multiple teams to interact with the\nmetadata.\n\n\n228\nSchema management\nIn this chapter, we will tackle the age-old problem of managing schema changes in\na data system introduced when source data changes, exploring how the increase in\nusage of third-party data sources—i.e., SaaS—and the growing use of streaming\ndata add to the challenge. \nThis chapter covers\nManaging schema changes in a cloud data platform\nUnderstanding schema-on-read vs. an active a schema-\nmanagement approach\nEvaluating when to use schema-as-a-contract vs. a smart-\npipeline approach\nUsing Spark to infer schemas in batch mode\nImplementing a Schema Registry as part of a Metadata layer\nUsing operational metadata to manage schema changes \nBuilding resilient data pipelines to manage schema changes \nautomatically\nManaging schema changes with backward and forward \ncompatibility\nManaging schema changes through to the data warehouse \nconsumption layer\n\n\n229\nWhy schema management\n We will discuss how our cloud data platform design can be used to address these chal-\nlenges—starting with leveraging the Schema Registry domain in the Metadata layer\nintroduced in chapter 7 and tackling different approaches to updating schemas in the\nRegistry—from “do nothing and wait till something breaks” to schema-as-a-contract and\nsmart pipelines.\n Because our end goal is to be able to maintain potentially hundreds of existing\ndata transformation pipelines and reports and introduce as little disturbance into the\ndownstream data consumers as possible, we will also discuss backward- and forward-\ncompatible schema changes and their potential impacts on different types of schema\nchanges.\n We’ll also discuss how to implement a Schema Registry as part of the cloud data\nplatform. This will include using Avro as a common schema format, inferring schema\nfrom incoming data and where to store the schema. We will also review the option to\nuse PaaS data catalog offerings from AWS, Azure, and Google, and three different\noptions for implementing a Schema Registry using a database with an API layer.\n Last, as the primary way users will consume the data is via a data warehouse, we will\nwalk through what happens when the schema for these data sets changes and which\npart of the data platform should be responsible for keeping data warehouse table\nschemas up to date. \n8.1\nWhy schema management\nDealing with schemas of the input data sources is a problem as old as data warehouses\nthemselves. Traditional data warehouses are based on relational technology, meaning\nthat the structure of the data—it’s column names, their types, and their order—must\nbe known up front before any data gets loaded. Any changes to the data source\nschema, such as adding new columns, must be carefully planned, so the destination\nschemas and ETL pipelines can be adjusted to accommodate this change. \n With traditional relational systems, the process of making a schema change often\ntakes hours to complete, because the data warehouse must reorganize existing data to\nfit the new schema. In an organization that operates proactively, any changes to\nsource data that require a schema change will trigger a “change request” for a data\nwarehouse schema update. It’s not uncommon in large enterprises for the schema\nupdate to take weeks or even months of planning. \n Some, usually smaller, organizations choose a different approach—they do noth-\ning and wait for things to break. In this case, upstream changes to the data sources\nhappen ad hoc, leaving the data engineering team to fix ETL pipelines when they\nbreak because of the schema change. While the proactive option is sometimes oner-\nous, the wait-and-see option can result in significant user dissatisfaction as they are typ-\nically the people who notice and report problems with the data after a schema\nchange. Regardless of how an organization chooses to deal with schema changes, they\ncan’t be ignored, and they require a significant level of manual intervention. The next\nsection will describe the required intervention in more detail.\n\n\n230\nCHAPTER 8\nSchema management\n8.1.1\nSchema changes in a traditional data warehouse architecture\nLet’s take a look at a simple example in figure 8.1. We have a typical traditional data\nwarehouse architecture with a single file-based data source. In a traditional data ware-\nhouse, data from the source file is always loaded into the landing table first. In data\nwarehouse terminology, a landing table is a table that is only used for ingesting new\ndata from the source, before any data transformations are applied to the data. The\nlanding table mimics the schema of the source data set to make ingestion code simpler. \nAs you can see in the figure, a landing table must have the same column names, such\nas transaction_amount and transaction_date. If we change the schema of the\nsource file and change some of the column names (for example, transaction_\namount changes to transaction_total), the ingestion process will break on the next\nrun, as shown in figure 8.2.\n At this point a data engineer gets involved and fixes the landing table definition,\nand the process is resumed until the next time a schema change happens. \nstore_id\ntransaction_date\ntransaction_amount\nstore_id\ntransaction_date\ntransaction_amount\nData source\nfile\nDW\n1. Data from the source file is\n    loaded into a landing table\n    before any transformations\n    are applied.\nData\nsource file\nschema\n2. The schema of\n    the landing\n    zone must\n    match the\n    schema of the\n    source data.\nFigure 8.1\nTo load data into a traditional data warehouse landing table, it’s schema \nmust match the source file schema.\nstore_id\ntransaction_date\ntransaction_total\nstore_id\ntransaction_date\ntransaction_amount\nData source\nfile\nDW\nIf the landing table schema\ndoesn’t match the data source\nschema, the load will fail.\nUpdated\ndata source\nfile schema\nLanding\ntable\nschema\nFigure 8.2\nIngestion breaks because of the upstream data source schema \nchange.\n\n\n231\nWhy schema management\n8.1.2\nSchema-on-read approach\nWhen Hadoop came on the scene as a data analytics solution, it introduced the concept\nof “schema-on-read.” The idea here is simple. Hadoop comes with its own file system\ncalled Hadoop Distributed File System, or HDFS. Instead of using landing tables that\nmust have all columns and their types defined up front in the data warehouse, in\nHadoop, you can simplify your ingestion process by saving files as is into HDFS. This pro-\ncess is resilient to the upstream schema changes, because HDFS, much like any other\nfile system, doesn’t care about the internal structure of the file itself. The “on-read” part\ncomes into play when you start processing the data from HDFS. Let’s see how our pre-\nvious example can be implemented on a Hadoop cluster. Figure 8.3 also adds a simple\ndata transformation step in SQL—calculating total sales for all stores for a given day.\nAs you can see, the ingestion process is now independent of the actual file schema.\nBut if you look closely at the SQL statement that represents our simple ETL pipeline,\nyou will see that it still has to reference specific column names. This means that the\nschema-on-read approach just pushes the problem of schema changes further down\nthe pipeline—from ingestion to the data transformation pipeline itself. Figure 8.4\ndemonstrates that while a column rename in the upstream file doesn’t break the\ningestions, it will break the ETL.\n If we start exploring this example even further, then we will arrive at a point where\ndata needs to be loaded into the data warehouse or any other data store to allow end\nusers to access it. These data stores may also need a schema to be defined and will be\naffected by the data source schema changes. At this point, you may be asking, why\ndoes Hadoop have anything to do with our cloud data platform design? It’s because\nthe cloud storage that we use as a landing area in our data platform architecture acts\nlike HDFS—it’s a distributed file storage that doesn’t have any notion of the schema.\nSELECT SUM(transaction_amount),\ntransaction_date\nFROM landing_table\nGROUP BY transation_date\nINSERT INTO result_table ...\nData source\nfile\nHadoop cluster\nHDFS\nSpark\nSQL\n2. Data source files\n    are saved as is\n    in HDFS.\n1. Expected\n   file schema\n3. Note that ETL\n    code references\n    specific column\n    names.\nstore_id\ntransaction_date\ntransaction_amount\nFigure 8.3\nIn Hadoop clusters, incoming data is saved as files on HDFS without the need to check for \nits schema first.\n\n\n232\nCHAPTER 8\nSchema management\nBecause of this, many data architects adopt a schema-on-read approach in their cloud\ndata platform. \n As you can hopefully see by now, the schema-on-read approach solves the ingestion\npart of the data pipeline, making it easy to land new data into the storage layer of our\ndata platform without worrying about the schema. But it doesn’t solve the schema-\nmanagement problem. Once you’ve ingested your data, you will inevitably need to\nperform data transformations and load data into other systems. That’s when it\nbecomes apparent that you need to know the schema. \n In the next section, we will explore alternatives to the schema-on-read approach. \n8.2\nSchema-management approaches\nIn a cloud data platform, the Metadata layer, introduced in chapter 7, has an import-\nant role to play in managing schema changes more easily than in traditional data\nwarehouse architectures. As shown in figure 8.5, one of the four Metadata layer\ndomains is the Schema Registry. \nstore_id\ntransaction_date\ntransaction_total\nSELECT SUM(transaction_amount),\ntransaction_date\nFROM landing_table\nGROUP BY transation_date\nINSERT INTO result_table ...\nData source\nfile\nHadoop cluster\nHDFS\nSpark\nSQL\n2. When the source\n    schema changes,\n    ingestion still\n    works.\n1. Updated\n   file schema\n3. However, ETL will\n    break as it is\n    expecting a different\n    column name, i.e.,\n    transaction_amount.\nFigure 8.4\nThe schema-on-read approach pushes the problem of schema management further down the \npipeline to the data transformation step.\nExercise 8.1\nWhich of the following best describes the schema-on-read approach?\n1\nSchema-on-read requires an upfront definition of the schema in the data platform.\n2\nSchema-on-read automatically adjusts schema definitions used in the data\nprocessing pipelines.\n3\nSchema-on-read allows you to ingest data with any schema into the data plat-\nform, but you still need to maintain the up-to-date schema for data processing.\n4\nSchema-on-read provides you with a central repository that stores schemas\nfor all your data sources.\n",
      "page_number": 220
    },
    {
      "number": 8,
      "title": "Schema management",
      "start_page": 252,
      "end_page": 283,
      "detection_method": "regex_chapter",
      "content": "233\nSchema-management approaches\nThe Schema Registry is a repository for schemas. It contains all versions of all schemas\nfor all data sources. Data transformation pipelines or people who need to know the\nschema for a particular data source can fetch the latest version from the Registry.\nThey can also explore all previous versions of the schema to understand how a partic-\nular data source has evolved over time. \n But how do the schemas get into the Registry, and who is responsible for updating\nthe schema versions when something changes? \n8.2.1\nSchema as a contract\nAs mentioned in the previous section, if we agree that the “do nothing and wait for things\nto break” approach, often used in conjunction with schema-on-read, is not the best\napproach, there are two alternative methods to proactively handling schema changes:\ntreating schema as a contract and performing schema management in the platform.\n The first approach, treating schema as a contract, is intended to make application\ndevelopers responsible for the schema management of the data that their application\nproduces. This approach says that a schema is a contract between application develop-\ners and data consumers, be it the data platforms, other applications, microservices,\netc. In this method, shown in figure 8.6, application developers publish schemas for\nall the data their application produces into a central repository, the Schema Registry.\nConsumers fetch and use the latest schema version from the same Schema Registry.\n As a part of this contract, only backward-compatible schema changes are allowed.\nBackward compatible means that existing data consumers can use the latest version of\nthe schema to process all existing data, including the data that was produced before\nthe schema change happened. For example, adding a new column to the schema is a\nbackward-compatible change because you can still read the old data using the new\nschema and use default values for recently added columns. On the other hand,\nrenaming a column is not a backward-compatible change. We will discuss schema\ncompatibility in more detail later in this chapter.\nPipeline metadata\nPipeline activity\nData quality\nchecks\nSchema Registry\nThe Schema Registry\ncontains all versions\nof all schemas for all\ndata sources and\ndata destinations.\nFigure 8.5\nThe Schema Registry is a component of the Metadata Layer that is used to store \ninformation about schemas for all data sources and destinations.\n\n\n234\nCHAPTER 8\nSchema management\nThe approach of treating a schema as a contract between application developers and\nthe downstream data consumers is ideal because it provides a clear separation of\nduties and decouples data producers from data consumers. It’s easy to add new data\nconsumers because we can rely on the schema in the Registry to understand the data\nstructure. But this approach requires two things for a successful implementation: first,\nit requires a high level of maturity in your development processes, and second, it\nrequires a clear data owner for third-party data sources.\n Let’s unpack these two requirements. In order to implement a schema-as-a-contract\napproach, it is the developer’s responsibility to ensure that all schema changes are\ncompatible. As such, you’ll need to make sure that your development practices can\nsupport that. This means not only disciplined developers who actually follow the process\nand publish schema changes to the Registry, but also automated checks and guardrails\nto make sure these changes are backward compatible and won’t break downstream\nconsumers. These assurances require a high level of automation and a mature testing\ninfrastructure. It also requires a high degree of automation when it comes to your\ncontinuous integration/continuous delivery (CI/CD) processes. If developers need to\nperform a time-consuming and complex multistep protocol to deploy their code\nchanges into production, there is a high probability that schema-management steps\nwon’t be followed consistently. \n The second requirement for a schema-as-a-contract approach is to have a data\nowner for all third-party data sources. This includes all of the SaaS solutions that your\norganization is using and any other data you receive from vendors, partners, etc. Since\nyour organization doesn’t own the SaaS application that produces this data, it’s a\ntough sell to make any given development team responsible for managing schemas\nIn a schema-as-a-contract approach,\napplication developers publish schemas\nfor all the data their application\nproduces into a central Registry.\nConsumers fetch the \nlatest versions of schemas \nfrom the Registry.\nData\ntransformation\npipelines\nApplication C\nOther data\nconsumers\nSchema\nRegistry\nRDBMS\nAWS S3\nApplication B\nMetadata layer\nApplication A\nFigure 8.6\nSchema as a contract between application teams producing data and teams consuming \nthe data\n\n\n235\nSchema-management approaches\nfor these data sources. It’s hard to take responsibility for schema changes when you\ndon’t have any control over the application.\n This level of development processes maturity is of course achievable, but in prac-\ntice we found that parts of the organizations that are tasked with designing and imple-\nmenting a data platform usually don’t have any control over the organization’s overall\ndevelopment practices and standards. This means that a data platform implementa-\ntion has to assume that the organization may have different levels of development pro-\ncesses maturity. Another caveat is that a data platform creation is often the first time\nthat an organization attempts to integrate all of its existing data sources. This means\nthat prior to the data platform implementation efforts, schema management was\nnever really a problem. This puts data architects and data engineers in charge of the\nschema-management problem, which often leads to behaviors that we described previ-\nously: rely on schema-on-read for ingestion and fix pipelines as they break because of\nthe schema changes.\n8.2.2\nSchema management in the data platform\nIf we think of the “schema on read/wait for things to break” approach (no control) to\nbe on the one end of a spectrum where we don’t do any schema management up\nfront, controlling the schema at the data sources or schema-as-a-contract (full con-\ntrol) is at the other end.\n In our practice, we have found that an approach to schema management that\nworks well for a wide variety of organizations is somewhere in between the “no con-\ntrol” and “full control” spectrum. In this middle ground, schema management is the\nresponsibility of the data platform owners. There are two reasons why we suggest this:\nSince a data platform is an integration point of internal and third-party data\nsources, it’s the only place where schemas can be managed centrally. \nData transformation pipelines are often the first to break when they encounter\nan incompatible schema change. This makes a data platform a logical place to\nhost a central schema repository and be responsible for maintaining the sche-\nmas up to date. \nNOTE\nThere is also a hybrid approach where schemas for some or all of the\ninternal data sources are managed by the development team, and the data\nplatform takes care of all third-party data sources. This mixed approach is\ncommon when your data sources are real-time. We will talk more about this\nlater in the chapter. \nPerforming schema management in the data platform itself allows you to realize the\nfollowing benefits:\nResilient ETL. For example, you will be able to detect and ideally adjust to\nschema changes before your ETL pipelines fail.\nAn up-to-date schema catalog that contains schema details, which is important\nfor data discovery and self-service use cases. \n\n\n236\nCHAPTER 8\nSchema management\nA history of schema changes for any given data set not only makes it possible to\nwork with archived data in your data platform because it tracks changes such as\nthe timing of the introduction or deletion of a column. It also simplifies pipe-\nline debugging and troubleshooting because you know exactly how the schema\nchanges over time.\nLet’s take a look at how schema management can be done inside the data platform\nitself. For this we need to take another look at the simplified data platform architec-\nture from chapter 7. Imagine that we have a platform that ingests data from two data\nsources: one is an RDBMS, and the other is flat files from AWS S3. We then combine\nthese two data sources and publish the output into the data warehouse. Figure 8.7\nshows the simplified architecture.\nIn our experience, schema-management steps should be implemented as the first step\nin the common data transformation pipeline. In figure 8.7, step 1 is the data ingestion\nlayer that lands data as is in the platform. Step 2 is a common data transformation\npipeline, and step 3 is a custom data transformation that joins two data sources\ntogether. Step 3 produces a new data set and should also maintain the schema for it.\nIn the next section, we will talk about a common schema-management module that\ncan be used as a part of any data transformation pipeline.\nSCHEMA-MANAGEMENT MODULE\nIn chapter 5 we described the typical steps performed by a common data transforma-\ntion pipeline: data format conversion, deduplication, and data quality checks. Now we\nneed to add one more step to this list: schema management. Figure 8.8 adds a new\nschema-management module to our Configurable Pipeline concept from chapter 5.\nCloud data platform\nMetadata layer\nSchema\nRegistry\nIn each of the transformations \nthat take place in steps 2 and 3, \nthe schema is registered or\nupdated.\nCloud data\nwarehouse\nData\ntransformations\n1\n1\n2\n2\n3\nData Ingestion\nIn step 1, data is ingested\nas is into the platform.\nRDBMS\nAWS S3\nFigure 8.7\nSchema management can be done as a part of the data platform itself.\n\n\n237\nSchema-management approaches\nThe schema-management module performs the following steps. First, it checks\nwhether the schema for this data source exists in the Schema Registry. If it doesn’t\nexist, this means we haven’t seen this data source before. In this scenario, the schema-\nmanagement module will perform the following steps:\n1\nInfer the schema from the incoming data (more on this later in this chapter).\n2\nRegister Version 1 of the schema in the Registry for this data source.\nIf the schema already exists, then the steps are slightly different:\n1\nFetch the current version of the schema from the Registry.\n2\nInfer the schema from the incoming data.\n3\nCompare the inferred schema to the current schema in the Schema Registry, and\ncreate a new schema version that combines the old and new definitions in a back-\nward-compatible way (more on this later in this chapter).\n4\nPublish the new schema version into the Registry for other pipelines to use.\nOne step that is common in both scenarios is the “infer schema” step. Let’s unpack\nwhat that means. Throughout this book, we use Apache Spark as our framework for\ndata transformations. Spark comes with a feature called schema inference. This means\nthat Spark can read a batch of data and try to automatically come up with a schema\ndefinition that matches this data. This works well on flat CSV files as well as highly\nnested JSON data as well. \nSCHEMA INFERENCE IN APACHE SPARK\nLet’s take a look at an example. The following listing is a sample JSON document that\nwe created using https://www.json-generator.com/.\nTypical steps performed by a\ncommon data transformation\npipeline should also include\nschema management.\nIncoming data\nPipeline name\nSource name\nSchema\nColumns to deduplicate\nQuality check rules\nQuality check severity\nThe schema-\nmanagement\nmodule\nmaintains\nschemas in\nthe Metadata\nlayer.\nMetadata\nlayer\nData format conversion module\nDeduplication module\nData quality checks module\nSchema-management module\nStaging data\nFigure 8.8\nBy adding a schema-management module to our common data transformation pipeline, \nwe can perform all schema-management tasks as a part of initial data transformations.\n\n\n238\nCHAPTER 8\nSchema management\n[\n  {\n    \"_id\": \"5f084f4ba8de96c3a6df5f1e\",\n    \"index\": 0,\n    \"guid\": \"d776db8c-90a4-4cc7-a136-35e09e8d7fb5\",\n    \"isActive\": false,\n    \"balance\": \"$1,702.05\",\n    \"picture\": \"http://placehold.it/32x32\",\n    \"age\": 23,\n    \"eyeColor\": \"blue\",\n    \"name\": \"Doyle Page\",\n    \"gender\": \"male\",\n    \"company\": \"STELAECOR\",\n    \"email\": \"doylepage@stelaecor.com\",\n    \"phone\": \"+1 (826) 572-2118\",\n    \"address\": \"190 Coventry Road, Riverton, South Dakota, 2701\",\n    \"about\": \"Et Lorem Lorem in aliqua irure nulla nostrud laborum veniam. \nAute cillum occaecat ad non velit eiusmod culpa id. Mollit veniam ut \nmollit consequat dolore Lorem aute voluptate ea aliquip sint anim labore \neu. Aliqua qui cillum proident ad.\\r\\n\",\n    \"registered\": \"2014-11-08T02:38:13 +04:00\",\n    \"latitude\": 60.913309,\n    \"longitude\": -81.07079,\n    \"tags\": [\n      \"velit\",\n      \"duis\",\n      \"et\",\n      \"deserunt\",\n      \"velit\",\n      \"incididunt\",\n      \"Lorem\"\n    ],\n    \"friends\": [\n      {\n        \"id\": 0,\n        \"name\": \"Terrell Donaldson\"\n      },\n      {\n        \"id\": 1,\n        \"name\": \"Freida Brooks\"\n      },\n      {\n        \"id\": 2,\n        \"name\": \"Lisa Cole\"\n      }\n    ],\n    \"favoriteFruit\": \"strawberry\"\n  } ]\nThis represents a profile of some fictitious person and contains a number of nested fields\nsuch as tags and friends. There are 21 fields in this sample document, and describing\nListing 8.1\nA sample JSON document with nested attributes\n\n\n239\nSchema-management approaches\na schema manually for even this simple example would be rather time consuming. In real\napplications, you might have hundreds of different attributes to deal with. \nNOTE\nWe have formatted the sample JSON document to make it more read-\nable, but for Spark to be able to parse it, each separate item in the document\nmust be a single line in a file. \nLuckily for us, Spark can infer the schema from this document automatically. In the\nfollowing listing, we use Spark Shell, an interactive command-line tool that allows us\nto enter Spark commands and see their outputs right away, without having to compile\nthe full program. Spark commands start with the scala> prompt and are followed by\nthe output. \nscala> val df = spark.read.json(\"/data/sample.json\")    \ndf: org.apache.spark.sql.DataFrame = [_id: string, about: string ... 19 more \n➥ fields]\nscala> df.printSchema    \nroot\n |-- _id: string (nullable = true)\n |-- about: string (nullable = true)\n |-- address: string (nullable = true)\n |-- age: long (nullable = true)    \n |-- balance: string (nullable = true)\n |-- company: string (nullable = true)\n |-- email: string (nullable = true)\n |-- eyeColor: string (nullable = true)\n |-- favoriteFruit: string (nullable = true)\n |-- friends: array (nullable = true)    \n |    |-- element: struct (containsNull = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- guid: string (nullable = true)\n |-- index: long (nullable = true)\n |-- isActive: boolean (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- name: string (nullable = true)\n |-- phone: string (nullable = true)\n |-- picture: string (nullable = true)\n |-- registered: string (nullable = true)\n |-- tags: array (nullable = true)\n |    |-- element: string (containsNull = true)\nNOTE\nIn this example, we are using the Spark Scala API, but it’s Python\nequivalent will only have minor syntactic differences.\nListing 8.2\nUsing the Spark Scala API to read JSON and display the inferred schema\nReads JSON document\nfrom a local file into a\nSpark DataFrame\nShows the \ninferred schema\nSpark identifies the types of the \ncolumns based on the actual data.\nfriends attribute is inferred \ncorrectly as an array.\n\n\n240\nCHAPTER 8\nSchema management\nAs you can see, Spark did a great job of correctly identifying the column names and\ntheir types from the actual JSON data. Of course, our example only contained one\ndocument, but the same approach works for multiple documents as well. There are\nseveral important things to keep in mind when using Spark’s schema inference:\nSpark uses a sample of all records to infer schema. For example, if you have a\nmillion JSON documents in the incoming batch, by default Spark will only use a\nsample of 1,000 documents to infer schema. This is done to improve inference\nperformance. If your documents have widely varying structures, then there is a\nhigh probability that the inferred schema will not match all of the documents\nin the batch. We recommend either significantly increasing the sample size (by\nsetting a sampleSize option in the read function) or setting your sample size to\nthe whole batch if your batches are small enough or performance is not a con-\ncern. You will need to experiment with your data to identify the sweet spot\nbetween the schema accuracy and performance for each of your data sources.\nFor example, data that comes from an RDBMS will always have the same\nschema for all rows in a given table, so a smaller sample size will work just fine.\nSpark relies on column names from your data files. For a JSON document,\nSpark will use the actual attribute names. For CSVs, you must include a header\nline with column names; otherwise, Spark will name columns like c0_, c1_, etc. \nIf you have an attribute that has different data types in different documents,\nthen Spark will try to use a generalized type that matches all values. For example,\nif in half of your documents an attribute “age” is a number, but in the other half\nit’s a string, then Spark will use a string type, because numbers can be always\nconverted to string, but not vice versa. In some cases, Spark will not be able to\nreconcile types at all if, for example, your age attribute is a number in some doc-\numents and a nested structure in others. In such cases, Spark will place rows it\ncan’t convert into a special field called _corrupt_record, where you can inspect\nthem and work with the data source owner to either split those attributes into\ntwo different ones or use the same data type in all the documents.\nThe schema that we have shown is using Spark internal types and is specific to the\nSpark framework itself. We could take that schema and save it as is in the Registry as is,\nbut because we are designing a data platform that can support a wide variety of tools,\nwe will take the approach of converting the Spark schema into an Avro schema before\nsaving it into the Registry. We have talked about the Avro file format in the previous\nchapters and will further discuss Avro schemas later in this chapter. \n Spark schema inference is a really powerful feature, and Spark itself is widely sup-\nported by the cloud providers. AWS Glue, for example, relies on Spark schema infer-\nences and adds new features on top of it. Azure Databricks and Azure Synapse services\nuse Apache Spark as a primary data transformation framework as well. But there are\ncases where you will not be able to implement schema-management steps entirely\ninside a Spark pipeline. \n\n\n241\nSchema-management approaches\n The first scenario is if your data processing framework of choice doesn’t support\nschema inference. For example, Apache Beam, which is used by Google’s Cloud Data-\nflow service, doesn’t support schema inference from data. This means that you need\nto maintain schemas manually, outside the pipeline. The second scenario is a real-\ntime data pipeline. \nSCHEMA MANAGEMENT IN REAL-TIME PIPELINES\nThe challenge with schema inference in real-time pipelines is that schema inference\nrequires you to look at a statistically significant amount of data to decide which\nschema this data has. Remember that Spark uses a default sample size of 1000 rows to\ninfer schema for a given batch of data. In a real-time data pipeline, our processing is\nconstrained to looking at a single message at a time. We could infer the schema for a\nsingle message, but since there is no guarantee that the next message will have the\nsame schema, we will have to somehow reconcile schemas for each individual mes-\nsage. This is a computationally expensive process and will result in an extremely large\nnumber of schema versions. \n Another challenge with schemas in a real-time pipeline is that in order to achieve\nbetter performance, developers are using binary formats like Protobuf or Avro to min-\nimize the size of each individual message. To even further reduce the size of each mes-\nsage, developers often remove the Avro schema definition from the message itself (in\nmany cases, schema size can be larger than the actual message size). In this case, a\nmessage is just an array of bytes in the real-time storage like Kafka, and the schema\ncannot be inferred from it. In these cases, the schema must be saved into the Registry\nand maintained by the application development teams. \n The good news is that the schema-inference approach and the manual schema-\nmanagement approach can be easily mixed together. Batch data sources can use a\nschema-inference approach, and real-time sources can rely on the manual schema\nmanagement, where development teams are responsible for publishing schemas into\nthe Registry and updating schema versions in the case of changes.\n8.2.3\nMonitoring schema changes\nTaking steps to build a resilient data pipeline that can deal with most of the schema\nchanges automatically is important if you don’t want to spend your days fixing broken\npipelines all the time. But it is also important to have an alerting mechanism in place\nto let you know when schema changes happen. \n The problem with the schema changes is that, while it is possible to build a pipe-\nline that will keep working when the structure of the data changes, certain changes\ncan cause logical errors in the downstream reports or data products. A common sce-\nnario is when a column gets deleted or renamed at the source. It is possible to build\ndata ingestion and data transformation pipelines that will use default values instead of\nmissing columns. We will talk about how this can be achieved in section 8.4, “Schema\nevolution scenarios,” but if we have a report that our business users rely on that\n\n\n242\nCHAPTER 8\nSchema management\nexpects a certain column to be present, and it starts to receive only default values all of\na sudden, the logic of this report will be broken. \n If we go back to our earlier example where we renamed transaction_amount to\ntransaction_total column and assume that we have built a resilient pipeline that uses a\ndefault value of “0” when it can’t find a transaction_amount column, then at some\npoint our report that calculates total sales per day will start showing zero sales. This is\nclearly a logic error, because the data is there; it’s just in a different column.\n As you can see, there are certain cases where we can’t automate dealing with\nschema changes, and what we need is an alerting mechanism that will let us know that\na schema change can cause issues with the downstream pipelines. If we don’t have this\nmechanism in place, our business users will discover these issues for us, and this will\nsignificantly erode the trust they have in the data platform and the quality of data in it. \n In chapter 7, we discussed the different domains in our Metadata layer. One of\nthese domains is Pipeline Activities. Pipeline Activities captures information about\nwhat happened during a pipeline execution: how much data did the pipeline read,\nwere there any errors, etc. Capturing schema change events as a Pipeline Activity\nbecomes really important if we want to monitor for these events. Figure 8.9 shows how\na common data transformation pipeline can register schema change events in the\nMetadata layer.\n Our common schema-management module can detect the schema changes and\npublish new versions of the schema into the Registry (part of the Metadata layer). It\ncan also record that the change happened into the Pipeline Activities domain of the\nCommon data transformation pipeline\nOther common modules\nSchema-management module\nProvides\nconfiguration to\nthe pipeline\nStaging data\nIncoming data\nMetadata\nlayer\nThe schema-management\nmodule maintains\nschemas in the\nMetadata layer.\nThe schema-management\nmodule saves schema\nchange events into the\nPipeline Activities area\nof the Metadata layer.\nActivities are sent to a cloud Log\nAggregation service, where they\ncan be analyzed and monitored\nfor specific events.\nCloud Log\nAggregation\nservices\nFigure 8.9\nThe schema-management module publishes information about schema change \nevents into the Pipeline Activities area of the Metadata layer, which ends up in a cloud Log \nAggregation service.\n\n\n243\nSchema Registry Implementation\nMetadata layer. From an implementation perspective, this can be a log file that gets\naggregated into one of the cloud log management solutions. There you can analyze\nthis data and build alerts that will notify the team responsible for managing the data\nplatform about a schema change.\n If your data platform is small and has only a handful of pipelines and reports, you\ncan easily identify the reports that can be affected by the schema changes and warn\nthe end users that the data in the report might not be valid, and you need to do some\nmaintenance on the pipelines. This is a much better user experience than users dis-\ncovering issues on their own and telling your team the data is wrong. Some logical\nissues can be rather subtle and not detected by the end users at all. \n As your data platform grows and you add more and more pipelines and reports,\nmanually figuring out which reports are affected can become time consuming or even\nimpossible. In this case, you can utilize pipeline configurations from the Metadata\nlayer. If you recall from chapter 7, pipeline configurations contain information about\nwhich data sources are used by which data transformation pipelines. If you know\nwhich data sources are affected by the schema change, you can easily identify all\ndownstream transformations and reports that are affected.    \n8.3\nSchema Registry Implementation\nBefore we talk about options to implement a Schema Registry as a part of your cloud\ndata platform, we need to discuss how to actually represent and store schemas. As you\nprobably realized by now, a “Schema Registry” is not a common concept in the data-\nprocessing world. Relational databases use schemas, but each vendor has its own ways\nto describe table schemas using different types, etc. CSV and JSON files only include\nattribute names and don’t include any type information for them, so their schemas\nare only partially represented. To be able to work with data that comes from a multi-\ntude of different sources, we need a schema that includes attribute names, their types,\nand default values. \n8.3.1\nApache Avro schemas\nIn chapter 5 we talked about using Apache Avro (https://avro.apache.org/) as a com-\nmon file format for all the data in our data platform. Avro describes data using its own\nschema, and because we are trying to standardize on a single data format, it makes\nsense for our platform to adopt Avro schemas as a common schema format. Avro sche-\nmas support most common primitive types: strings, integers, float, null, etc. They also\nsupport complex types such as records, arrays, and enums. This makes it possible to\nuse Avro to describe all kinds of data sources—from data that arrived from RDBMSs\nand mostly uses primitive types to various JSON documents that use complex nested\nattributes. \n The following listing is an example of an Avro schema definition for the sample\nJSON document that we used previously.\n \n\n\n244\nCHAPTER 8\nSchema management\n{\n   \"type\":\"record\",\n   \"name\":\"sampleUserProfile\",\n   \"fields\":[\n      { \"name\":\"_id\", \"type\":[\"string\",\"null\"]},  \n      { \"name\":\"about\", \"type\":[\"string\",\"null\"]},\n      { \"name\":\"address\", \"type\":[ \"string\",\"null\"]},\n      { \"name\":\"age\", \"type\":[\"long\", \"null\"]},\n      … \n      {\n         \"name\":\"friends\",\n         \"type\":[{\"type\":\"array\",\"items\":[ \n                  {\n                     \"type\":\"record\", \n                     \"name\":\"friends\",\n                     \"namespace\":\"sampleFriendsRecord\",\n                     \"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},   \n                               {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]\n                  },\n                  \"null\"\n               ]\n            },\n            \"null\"\n         ]\n      },\n      …\nAvro schema definitions are human readable, which means you can create these sche-\nmas manually for some data sources where schema inference can’t be used. \n As we discussed in section 8.2.2, if you are using schema inference in Spark, you\nneed to convert the Spark schema into an Avro schema somehow. Spark provides a\nconvenient method in the Scala API to do that, as shown in the following listing.\nimport org.apache.spark.sql.avro.SchemaConverters\nval df = spark.read.json(\"/data/sample.json\")   \nval avroSchema = SchemaConverters.toAvroType(df.schema, false, \n➥ \"sampleUserProfile\")  \navroSchema: org.apache.avro.Schema = {\"type\":\"record\",\"name\":\n➥ \"sampleUserProfile\",\"fields\":[{\"name\":\"_id\",\"type\":[\"string\",\"null\"]} …\nAs you can see from this example, the output of the toAvroType method is of\norg.apache.avro.Schema type, and it looks exactly like the example Avro schema we\nListing 8.3\nAvro schema definition for our sample JSON document\nListing 8.4\nConverting Spark schema into Avro schema\nAvro schema definitions can \ncontain columns with primitive \ntypes such as string, integers, etc.\nThe “friends” attribute in this \nexample is an array, meaning it \ncan contain multiple values.\nEach item in the\n“friends” array is of\ntype record. This is\nused to describe\nnested values.\nEach item in the “friends” array\ncontains a record with two\nattributes of primitive types.\nImports helper object to \nperform schema conversions\nReads a sample JSON document \ninto a Spark dataFrame\nUses a “toAvroType” method to convert \na Spark schema into an Avro schema\n\n\n245\nSchema Registry Implementation\npreviously showed. We have omitted the full schema output in this listing for brevity, but\nif you try this using a Spark shell yourself, you will see the full Avro schema definition.\n Beside serving as a common format for schema definitions, Avro also supports\nschema evolution. Schemas for the data sources are always changing (otherwise, we\nwouldn’t be having this conversation), so being able to reflect how schema changed\nover time is very helpful. We will talk in detail about schema evolution examples later\nin this chapter. \n Once we have obtained the Avro schema, the next thing to do is to store it some-\nwhere other pipelines, human operators, or monitoring tools can fetch it from. That\nplace is a Schema Registry. A Schema Registry essentially is a database that allows you to\nstore, fetch, and update schema definitions. As we saw in the previous examples, the\nAvro schema definition is just a text describing attributes, their types, etc. Actually, the\nAvro schema definition itself is a valid JSON document. This means any database that\nis capable of storing JSON data would work as a Schema Registry implementation.\n8.3.2\nExisting Schema Registry implementations\nBefore we start talking about how you can implement your own Schema Registry in\nthe cloud, we need to take a look at existing solutions. When talking about ways to\nstore pipeline metadata in chapter 7, we discussed the following cloud services that we\nhave put into a broad category of “data catalogs”:\nAWS Glue Data Catalog\nAzure Data Catalog\nGoogle Cloud Data Catalog\nAll these services offer some capabilities to store the schema of various data sources.\nIn our experience, we have found all of them to be quite limiting when it comes to the\nability for pipeline developers to publish, version, and retrieve schemas. Both Azure\nData Catalog and Google Cloud Data Catalog are focused on automatic discovery of\nexisting data sources to provide end users with a search interface for data discovery.\nFor example, Azure Data Catalog allows you to register not only data sources, but also\nexisting reports, which makes it particularly useful as a data discovery tool. What’s\nmissing from these data catalog solutions is the ability to version schemas and to use a\ncommon schema format such as Avro. AWS, Azure, and Google Cloud do offer APIs to\nupdate schemas, but there is no versioning information, and each data catalog solu-\ntion uses its own way to represent schemas. This means that as a data developer you\nwill need to convert Spark schemas into Avro, which you will still need to do if you\nwant a compact binary format to store files in the cloud storage and then convert Avro\ninto a specific schema representation expected by the data catalog tool.\n Out of these three data catalog solutions, AWS Glue Data Catalog comes the clos-\nest to being able to serve as a Schema Registry in our design. It supports discovering\nschemas from common file formats such as JSON and Avro and updating schemas via\nan API. It doesn’t support (at least in the current version) retrieving old versions of\n\n\n246\nCHAPTER 8\nSchema management\nthe schemas, though. As mentioned in previous chapters, AWS Glue Data Catalog\nworks well if you decide to implement the whole transformation layer using the AWS\nGlue service. If you want to use just the Data Catalog portion of it and implement the\npipelines yourself using Apache Spark or any other data processing framework, then\nthe limitations become quite apparent. \n If we look into existing open source solutions for Schema Registry we, unfortu-\nnately, won’t find too many. Confluent Schema Registry (http://mng.bz/7V5m) is the\nonly solution that we came across in the past that ticks all the boxes when it comes to\nthe Schema Registry features. It supports versioning and the Avro schema format, has\na proper API, and even supports schema evolution rules to ensure that changes to the\nschema don’t break existing pipelines. \n One big challenge with the Confluent Schema Registry is that it requires Kafka to\nwork. It was developed specifically for real-time processing use cases, and while you\ncan use its API to register schemas for any type of data source, you do need to have a\nKafka cluster up and running. So if you don’t yet have a real-time component in your\ndata platform, or you are using a cloud-specific real-time storage service such as AWS\nKinesis or Google Cloud Pub/Sub, then you will not be able to use this tool. You\nshould be able to use Confluent Schema Registry with the Azure Events Hub because\nit provides an API that is compatible with Kafka.\n It is also important to note that Confluent Schema Registry is distributed under\nConfluent’s own Community License, which is different from most common open\nsource licenses such as Apache v2 or MIT. \n8.3.3\nSchema Registry as part of a Metadata layer\nIn chapter 7 we talked about options for implementing a Metadata layer that will host\npipeline configurations, pipeline activity information, and data source schemas. We\nhave described three solutions that are gradually increasing in complexity and sup-\nported features:\nUse a plain-text pipeline configuration file and a code repository to version it.\nUse a key/value or a relational database to store pipeline configs and other\nmetadata.\nAdd a REST API layer on top of the database to provide a uniform interface for\nall tools in the platform.\nBecause a Schema Registry is a logical part of the Metadata layer, it makes sense for us\nto use the same approach for implementing it as we described in chapter 7. We have\nalready mentioned that a Schema Registry is really just a database for schemas and\ntheir versions. We will not repeat here the same three options that we described here,\nbut we’ll focus on the last one, which has a database and an API layer on top of it. You\ncan still use just a database without an API layer if the number of tools and teams inter-\nacting with the Schema Registry are low. Using text files and a code repository to store\nschemas, similar to the simplest option for pipeline configuration, will not work here,\n\n\n247\nSchema Registry Implementation\nbecause in our design, schemas are updated automatically by the pipelines themselves.\nFigure 8.10 shows how different tools will interact with the Schema Registry.\n When it comes to the actual database, the same key/value services that we men-\ntioned in chapter 7 will work for the Registry database as well: Azure CosmosDB, Goo-\ngle Cloud Datastore, or AWS DynamoDB. In fact, when implementing a Schema\nRegistry in the past, we have often had the same database for the pipeline metadata\nand the Schema Registry. Sometimes, you may need to use separate instances of Cos-\nmos DB, Datastore, or DynamoDB for the pipeline metadata and the schemas. For\nexample, if you are using a hybrid scenario where some of that data source’s schemas\nare managed by the data platform, and some are managed by the application teams,\nyou may want the application team to only have permissions to access the schema\ndata, but not the pipeline configuration data. Fortunately, the cloud makes it easy to\ncreate multiple instances of these data stores and configure granular access to them. \n Schema Registry operations, whether performed directly on the database or imple-\nmented via the API layer, can be summarized like this:\nFetch the current version for a given data source.\nCreate a new schema for a data source if the current version doesn’t exist. This\nis used to register new data sources.\nAdd a new version of the schema for an existing data source.\nIn chapter 7 we described which attributes different entities in the Metadata layer,\nsuch as Namespaces, Pipelines, Data Sources, etc., should have. Now we can extend\nthis list with attributes that you will need to store in the Schema Registry:\nID—Identifier for the schema. This ID is linked back to the Sources and Desti-\nnation entities in the Metadata layer.\nIngestion pipelines\nregister schemas for\nnew data sources and\nupdate schemas for\nexisting sources via a\ncommon schema-\nmanagement module.\nApplication teams can\nuse the same API to\nregister and maintain\nschemas for data their\napplications produce.\nTransformation\npipelines read\nschemas for the data\nsources they need\nand register schemas\nfor the new datasets\nthey produce.\nMonitoring tools can\nperiodically check for\nnew versions of the\nschema and produce\nalerts or reports.\nIngestion\npipelines\nTransformation\npipelines\nSchema\nRegistry DB\nSchema Registry\nAPI service\nMonitoring tools\nApplication\nteams\nFigure 8.10\nA Schema Registry with an API layer on top of it can be used both by pipelines \nthat are internal to the data platform as well as external teams and tools.\n\n\n248\nCHAPTER 8\nSchema management\nVersion—A number indicating versions of this schema. Together with the ID\nattribute, this forms a unique schema key.\nSchema—Text attribute that stores the Avro schema definition.\nCreated Timestamp—Date and time of when this schema was first created.\nLast Updated Timestamp—Date and time of when this schema was last updated.\nNote that here ID is not unique, because you can have multiple versions of the same\nschema. As we discussed previously, schemas of the data sources we are dealing with are\nconstantly changing. We need to capture each modification of the schema so we can\nwork with data that was produced with that version of the schema, but also it is import-\nant to know how the schema changed over time for debugging and troubleshooting\npurposes. We don’t assign a new ID to each schema version because then we would\nneed to go and update all Sources and Destinations entities in the Metadata layer.\n8.4\nSchema evolution scenarios\nNow that we know how to infer schema from the incoming data and how to store new\nversions of it in the Schema Registry, we need to discuss what are the most common\nscenarios for schema evolution. Schema evolution is a commonly used term to\ndescribe how programs that process data deal with changes to structure of the data. In\nthe case of the data platform, we need to understand how our data pipelines will work\n(or not work!) when the schema of a certain data source changes. \n Here are the most common examples of schema changes:\nAdding a new column\nDeleting an existing column\nRenaming a column\nChanging the column’s type\nWhen talking about schema evolution, it is important to keep the larger context in\nmind. Our goal is not only to be able to read the data with the new schema, but also to\nmaintain potentially hundreds of existing data transformation pipelines and reports\nand introduce as little disturbance into the downstream data consumers as possible.\n There are two types of schema changes: backward compatible and forward com-\npatible. Backward-compatible schema changes mean that if our data transformation\npipelines use the latest version of the schema from the Registry, they should be able to\nread all data that is already stored in the platform even if that existing data was written\nusing older versions of the schema. This process is illustrated in figure 8.11.\n In this example, we are dealing with a single data source that has an initial schema\n(V1) that consists of two columns. Let’s imagine our ingestion pipelines have worked\nfor some time, and we have already stored some archive data that uses V1 schema.\nNow at some point, a new column was added to this data source and we have created a\nV2 of the schema and started writing data using this version.\n\n\n249\nSchema evolution scenarios\nIn chapter 5 we introduced two types of data processing pipelines: a common data\nprocessing pipeline and a custom data processing pipeline. As a quick reminder, a\ncommon data processing pipeline is concerned with the following:\nFile format conversion\nData deduplication\nData quality checks\nIn this and the next example we assume that the data processing pipeline is a com-\nmon one, because it needs to be able to deal with different schemas coming out of\ndata sources but doesn’t really need to apply any customer business logic. We will talk\nabout the custom data processing pipelines and how they can deal with the schema\nchanges later in this chapter.\n Let’s also assume that our data processing pipeline automatically picks up the lat-\nest version of the schema for each source from the Registry. What will happen if we\nneed to reprocess data written with the V1 schema using the V2 schema?\n8.4.1\nSchema compatibility rules\nThis is where schema backward-compatibility rules come into play. Avro format\ndefines several rules that make schemas backward compatible. In our example, if a\nnew column that we have added in the V2 of the schema, column_3 has a default value\ndefined, then this schema change is backward compatible. If we read archive data that\nwas written using the V1 schema, that doesn’t have column_3 present, using the V2\nBackward-compatible schema changes mean\nthat if our data transformation pipelines use\nthe latest version of the schema from the\nRegistry (V2), they should be able to read all\ndata that is already stored in the platform\neven if that existing data was written using\nolder versions of the schema.\nThe data ingestion layer\nalways writes data with\nthe latest version because\nit reads it directly from\nthe data source.\nCommon data\nprocessing pipeline\n(using V2 schema)\nSchema V2\nSchema V1\nVersion 1:\ncolumn_1\ncolumn_2\nSchema\nRegistry\nVersion 2:\ncolumn_1\ncolumn_2\ncolumn_3\nData with schema\nV1\nData with schema\nV1\nData with schema\nV2\nData ingestion layer\nFigure 8.11\nBackward-compatible schema changes mean that you can use schema V2 to process \nthe data that was written by schema V1.\n\n\n250\nCHAPTER 8\nSchema management\nschema, then Avro will automatically use a default value for column_3. Often an empty\nor \"null\" value is used as a default, but it can be any value that matches the column\ntype. If you recall, our example Avro schema for a user profile JSON document, then\nyou can see how default values are used in Avro: { \"name\":\"age\", \"type\":[\"long\",\n\"null\"]}. Here we define a column called \"age\" that is of \"long\" type and has a\ndefault value of \"null\".\n Another case for schema evolution supported by Avro is a forward-compatibility\nscenario. A schema change is forward compatible if you can use an older version of\nthe schema to process data that was written with the newer schema. In our previous\nexample, our data processing pipeline always used the latest version of the schema\nfrom the Registry. Let’s take a look in figure 8.12 at what will happen if our pipeline\nwill use the current version of the schema to process the new incoming data.\nLike in the previous example, we add a new column in the V2 version of the schema\nand have some archive data written with the V1 version, but our pipeline keeps using\nthe V1 schema instead of immediately switching to V2. As you can imagine, the pipe-\nline can reprocess the archive data without any issues because it’s using the same\nschema as the data was written with. What will happen if our pipeline tries to process a\nnew incoming batch that was written with a new column in it? \n In Avro, adding a column to the schema is a forward-compatible change. Our data\nprocessing pipeline that uses the V1 schema will simply ignore any new columns that\nwere added and keep reading the data as if the new columns were not there. This\ncompatibility feature of Avro helps to keep existing pipelines running when faced\nwith schema changes and lets the data engineering team adjust the pipelines to start\nusing new schemas at a later time.\nA schema change is\nforward compatible if you\ncan use an older version\nof the schema to process\ndata that was written\nwith the newer schema.\nCommon data\nprocessing pipeline\n(using V1 schema)\nVersion 1:\ncolumn_1\ncolumn_2\nSchema\nRegistry\nVersion 2:\ncolumn_1\ncolumn_2\ncolumn_3\nData with schema\nV1\nData with schema\nV1\nData with schema\nV2\nData ingestion layer\nFigure 8.12\nForward-compatible schema changes mean that you can use schema V1 to process \nthe data that was written with schema 2.\n\n\n251\nSchema evolution scenarios\nNOTE\nAs discussed earlier in this chapter, monitoring for schema changes\nusing a Pipeline Activities log is very important. While your pipelines can\nignore new columns and keep working until you make the necessary adjust-\nments, some of the data users might expect the new columns to be added as\nsoon as possible. It’s always a good idea to be one step ahead of your users\nand let them know that you are aware of the change and provide an estimate\non when the new column will become available in the downstream data sets.\nWe have talked about adding and deleting columns so far. Another common type of\nschema change is renaming an existing column. You can probably guess by now that\nrenaming behaves like adding a new column with the new name and deleting a col-\numn with the old name. In Avro, backward- and forward-compatibility rules for\nrenaming would be the same as combined rules for adding and deleting columns. If\nyou rename a column that has a default value, then this change is both forward and\nbackward compatible. If you rename a column that doesn’t have a default value, then\nthis change is neither backward nor forward compatible.\n The last type of schema change is changing the column type. Out of the box, Avro\nsupports “promoting” certain data types to other compatible data types. The key\nrequirement here is not to lose any data. For example, Avro can promote an int type\nto long, float, and double types, but not the other way around. This is because if you\ntry converting a long, which is a 64-bit integer, to an int, which is a 32-bit integer, you\nmay end up with values that don’t fit into an int type. You can find the full list of which\nAvro data types can be automatically promoted to other data types here: http://\nmng.bz/mgRP.\n There are other scenarios for the data types conversions that Avro doesn’t support\nautomatically (meaning without you having to write any code), but that can be rela-\ntively easily implemented in the common schema-management module that we have\ntalked about previously. For example, any number can be represented as a string (the\nreverse is not true) and any single data point can be represented as an array of one\nelement. There are more examples, and depending on your environments and types\nof the schema changes that you see most frequently, you can decide to implement\nadditional data type conversions. In our experience, it’s a good idea to stick to the\nautomatic data type conversions that Avro provides, because implementing custom\ntype conversion will add complexity to downstream data processing pipelines.\n8.4.2\nSchema evolution and data transformation pipelines  \nSchema changes can have a significant impact on the downstream data processing\npipelines and reports or other types of analytics that users perform on your data plat-\nform. Now that we know about Avro schema compatibility rules, we can discuss what\nthis means for the data transformation pipelines.\n Previously in our schema compatibility examples, we have said the pipeline in\nquestion is a common data transformation pipeline, which includes steps that are\ngeneric for all data sources: file format conversion, deduplication, etc. Such pipelines,\n\n\n252\nCHAPTER 8\nSchema management\nof course, need to be able to read the latest and archive data (in case of reprocessing)\nand need to be able to deal with the schema changes. But they don’t perform any\nbusiness logic that requires specific columns to be present or these columns to have\nspecific types. This means it’s easier to build a resilient common data transformation\npipeline because it is not affected by schema changes as much.\nNOTE\nAn exception here is data deduplication on a particular column. If\nthat column gets deleted, the deduplication process will need to be adjusted.\nThings are more complicated for the data transformation pipelines that actually\nimplement business logic. Let’s modify our previous example and use a data transfor-\nmation pipeline that uses a simple aggregation in figure 8.13.\nIn this example, we have a data transformation pipeline that calculates a sum of\ncolumn_2 values for each unique column_1 value. Let’s say in the V2 of the schema a\nnew column_3 was added, and column_2 was removed. Or column_2 was renamed to\ncolumn_3, which would result in the same schema. What happens to our pipeline then? \n If our data transformation pipeline switches to using the latest version of the\nschema, it will fail. New incoming data batches will not contain column_2, and the\npipeline will produce an error. If the data transformation pipeline sticks to using the\nprevious version of the schema, then results will depend on whether column_2 has a\ndefault value defined. If it does, the pipeline will keep working because it will use the\ndefault value in place of the missing column. If column_2 doesn’t have a default value,\nthe pipeline will fail with an error, because column_2 is declared in the V1 of the\nschema, but is missing from the new data, and there is no default value to use. \nIn this scenario when\ncolumns are deleted\nand a new one is added,\nthe data transformation\npipeline must keep\nusing V1 of the schema\nto keep working.\nThe data ingestion layer always\nwrites data with the latest\nversion because it reads it\ndirectly from the data source.\nSELECT\nSUM(column_2)\nFROM data\nGROUP BY column_1\nVersion 1:\ncolumn_1\ncolumn_2\n(default:0)\nSchema\nRegistry\nVersion 2:\ncolumn_1\ncolumn_2\ncolumn_3\nData with schema\nV1\nData with schema\nV1\nData with schema\nV2\nData ingestion layer\nFigure 8.13\nData transformation pipelines that perform actual calculations on specific columns \nhave to keep using previous versions of the schema to keep them from breaking.\n\n\n253\nSchema evolution scenarios\n Table 8.1 summarizes the types of the schema changes in relation to the forward\nand backward compatibility. \nAs you can see in the preceding table, adding a new column with a default value is the\nsafest schema change operation. Your existing pipelines will keep working and will\nignore the new column until you make changes to the pipeline’s logic. Deleting a col-\numn that has a default value is also a mostly safe operation. You will need to make sure\nthat your transformation pipeline uses the previous version of the schema though.\nRenaming a column is only safe when a column has a default value. Otherwise, it has\nthe same properties as adding a column without a default value and deleting a col-\numn without a default value. Finally, changing column type, as described earlier,\ndepends on the actual types. \n Should pipelines switch to the latest version of the schema or keep using the previ-\nous version until an engineer makes the switch? Based on table 8.1 and our experi-\nence, it’s better to keep using the previous version of the schema and only switch to\nthe new version once all required changes are made to the pipelines. In this case, your\npipelines will continue working for any schema changes that are forward compatible.\nIf you can negotiate with the owners of the data sources and have them agree to only\nmake changes that are safe (meaning forward compatible), then you will end up with\na resilient pipeline. \nNOTE\nWhen using the schema inference feature of Spark, all columns get a\ndefault value of “null” (empty). This reduces the chances of pipelines break-\ning due to the incompatible schema changes. You may need to update default\nvalues for certain columns manually to better reflect business logic in your\npipelines.\nTable 8.1\nForward and backward compatibility associated with different types of schema changes\nSchema change\nBackward compatibility\nForward compatibility\nSafe for transformations\nAdding a column with \na default value\nYes\nYes\nYes\nAdding a column with-\nout a default value\nNo\nYes\nYes\nDeleting a column \nwith a default value\nYes\nYes\nYes, if you use the previ-\nous schema version\nDeleting a column \nwithout a default value\nYes\nNo\nNo\nRenaming a column \nwith a default value\nYes\nYes\nYes, if you use the previ-\nous schema version\nRenaming a column \nwithout a default value\nNo\nNo\nNo\nChange column type\nSometimes\nSometimes\nSometimes\n\n\n254\nCHAPTER 8\nSchema management\nLOGICAL ERRORS IN THE DATA TRANSFORMATION PIPELINES DUE TO SCHEMA CHANGES\nIt’s important to understand that when we talk about building pipelines that are resil-\nient to the schema changes, we mean pipelines that keep running and don’t fail with\nerrors when the schema changes. Certain schema changes can impact the business\nlogic in your data transformation pipelines and produce incorrect results. To illustrate\nthis problem, let’s take a look at our previous example, but now instead of abstract\ncolumn_1 and column_2, use a retail scenario. Let’s say we are ingesting a table from\nan RDBMS that contains daily sales for a number of different stores in our retail chain\nempire. We have a data transformation pipeline running that generates a report with\na total amount of sales per each unique store_id. At some point, the application devel-\nopment team decides to change the name of the column that contains daily sales\nnumbers. Figure 8.14 shows a schema change example.\nLet’s say we have implemented all the best practices of building a resilient pipeline: we\nare using Avro as our file format, we have a Schema Registry, and we make sure our\npipelines keep using previous versions of the schema until we update them. In this\ncase, our daily_sales column in the V1 schema has a default value of NULL (empty\nvalue). The schema changes, and all new incoming data has a V2 schema where the\ndaily_sales column is deleted and a total_day_sales column is added. Our simple\ntransformation pipeline will keep working, because when it reads new data it will use a\nNULL value for the daily_sales column. But our report will start showing NULL for\n1. The schema changes, and all new\n    incoming data have a V2 schema where\n    the daily_sales column is deleted and\n    a total_day_sales column is added.\n2. Our simple transformation pipeline will keep working, because when it reads\n    new data it will use a NULL value for the daily_sales column. But our report\n    will start showing NULL for the sum_sales because in SQL if you add a NULL\n    value to a non-NULL value the result is NULL.\nVersion 1:\nstore_id\ndaily_sales\n[default NULL]\nVersion 2:\nstore_id\ndaily_sales\ntotal_day_sales\nSchema\nRegistry\nSELECT\nSUM(daily_sales)as sum_sales\nFROM store_sales\nGROUP BY store_id\n1\n2\n3\nNULL\nNULL\nNULL\nstore_id\nsum_sales\n100\n250\n385\n1\n2\n3\nstore_id\nsum_sales\nFigure 8.14\nRenaming the daily_sales column can cause incorrect or at least unexpected \nreport results.\n\n\n255\nSchema evolution and data warehouses\nthe sum_sales because in SQL if you add a NULL value to a non-NULL value, the\nresult is NULL. Your business users will be really surprised to see a report that shows\nempty values for total sales. Using “0” as a default value will prevent us from seeing\nNULLs in the sum_sales column, but it will look like no new sales are happening\nbecause existing numbers will be unchanged even as new data arrives. \n As you can see, even if our pipelines keep working, the business logic in them may\nbe broken when the schema changes. Unfortunately, there is no simple solution to\nthis problem. As discussed previously, it is important to have some alerting in place to\nlet you know that schema changes happened so you can review existing pipelines and\nmake adjustments when necessary and inform your users that certain reports may not\nbe correct in the meantime.\n8.5\nSchema evolution and data warehouses\nSo far we have talked about how to handle schema changes in our data transformation\npipelines, but in our data platform architecture, the primary way users will consume\nthe data is via a data warehouse. As we load new incoming data or results of the trans-\nformations into the data warehouse tables, we also need to think about what will hap-\npen when the schema for these datasets changes. \n There are differences between how different cloud data warehouses behave when\nit comes to schema changes, and we will talk about this later, but there are also differ-\nences in approaches to schema management between data transformation pipelines\nand data warehouses. \n Our data transformation pipelines deal mostly with files (unless it’s a real-time\npipeline). When using Avro as a file format, we get the benefit of having the schema\ndefinition that is stored together with each file. This means we can have files with dif-\nferent schema versions stored in our cloud storage and then have our pipeline rely on\nthe compatibility rules to reconcile these different versions of schemas. \n Data warehouses, including the ones that are developed by the cloud vendors,\nwork differently. Data warehouses store all their data in tables, and these tables must\nExercise 8.2\nWhat makes a schema change backward compatible?\n1\nA schema change is backward compatible if it only includes adding new columns.\n2\nA schema change is backward compatible if all schema versions use the\nsame column names.\n3\nA schema change is backward compatible if you can use the latest version of\nthe schema to read the data that was written using the previous versions of\nthe schema.\n4\nA schema change is backward compatible if you can take the oldest schema\nversion and use it to read the data that was written using more recent ver-\nsions of the schema.\n\n\n256\nCHAPTER 8\nSchema management\nhave a defined schema. There can’t be multiple versions of the schema for the same\ntable. This has two consequences for the schema evolution scenarios:\nAs the schema of the data sources or data products changes, we need to adjust\ncorresponding table schemas in the data warehouse. \nData warehouse table schemas must accumulate changes over time and not\napply irreversible changes.\nExisting data warehouses don’t integrate with external Schema Registries. This means\nthat while you will use schema inference and store new versions of the schemas in the\nRegistry, for your data pipelines to work you will need to make corresponding changes\nto the data warehouse table schemas separately. For example, if a new column is\nadded to some data source and you want this column to be available in the data ware-\nhouse, you will need to add a corresponding column to the data warehouse. \n You also need to treat schema changes to the data warehouse tables differently\nwhen it comes to changes like column deletion or column renames. If a column gets\ndeleted from a source table, you don’t want to delete it from the data warehouse table\nbecause data warehouse tables contain all data, including historical data. If we delete\nthe column from the data warehouse, we lose the data for this column. This means\nthat changes to the data warehouse schema should be accumulative—we can add new\ncolumns or change column types, but we should not delete columns.\nNOTE\nThis is not a problem with data that we store in cloud storage, because\nschema changes only affect new incoming data. We don’t modify existing\narchive data when a column is deleted.\nBut which part of the data platform should be responsible for keeping data warehouse\ntable schemas up to date? Obviously, we don’t want to do it manually. Previously, we\ntalked about creating a schema-management module that will be used as a part of the\ncommon data transformation pipeline. The same module can be used to manage the\ndata warehouse schema changes. \n Because we have previous and current versions of the schema in the Schema Regis-\ntry, we can create an automated way to generate necessary SQL statements to update\nthe data warehouse table definitions. Let’s take another look at our example of a col-\numn rename. Figure 8.15 shows how the schema-management module can be used to\nmaintain data warehouse table definitions.\n In this example, column_2 gets renamed into column_3. In terms of schema\nchange, this can also be represented as deleting column_2 and creating a new\ncolumn_3. Remember that we don’t want to delete existing columns from a data ware-\nhouse table because there might be historical data in that column that is still valuable.\nWe omit the deletion portion and generate a SQL statement that will just add a new\ncolumn_3 to the table. \n Automating schema changes in the data warehouse means that you will need to\nwrite custom code in the schema-management module that will map Avro data types\ninto the data types of the data warehouse of your choice. Schema change commands\n\n\n257\nSchema evolution and data warehouses\nfor different data warehouses may have different syntax, so you will need to take that\ninto account as well. \n Sometimes you may want to simplify the schema-management process for the\ntables in the data warehouse, and instead of adjusting existing tables on the go, just\ndelete the existing table and create a new one with a new schema and load all the\ndata, including historical data from the cloud storage. This works well for small tables\nthat don’t take a lot of time to rebuild.\n It’s also important to understand how the cloud data warehouse of your choice\nbehaves when you change existing table schemas. Many existing data warehouses will\nmake the table unavailable for any queries while changing the schema. This means\nthat your reports or users who run queries against the data warehouse will have to wait\nfor the change to complete. Depending on the size of the table and complexity of the\nchange, it can take minutes or even hours to run. In the next section, we will take a\nbrief look at schema-management features of AWS Redshift, Google Cloud BigQuery,\nand Azure Synapse. \n8.5.1\nSchema-management features of cloud data warehouses\nImplementations of existing cloud data warehouses differ significantly and offer dif-\nferent ways to deal with schema changes. Unfortunately, cloud vendors rarely publish\nenough details about the internal workings of their data warehouses for us to fully\nunderstand how schema changes are implemented under the covers. We can only\nmake certain assumptions based on the existing documentation. \nSchema Registry\nCommon data transformation pipeline\nVersion 1:\ncolumn_1\ncolumn_2\nVersion 2:\ncolumn_1\ncolumn_2\ncolumn_3\nCommon modules\nThe schema management module\nALTER TABLE some_table\nADD COLUMN column_3...\nCloud\nwarehouse\nCompare V1 and V2 schemas\nand create a SQL statement\nto update the warehouse table.\nFigure 8.15\nThe schema-management module can compare V1 and V2 schemas and \nautomatically generate necessary SQL commands to update data warehouse table definitions.\n\n\n258\nCHAPTER 8\nSchema management\n Both AWS Redshift and Azure Synapse have roots in traditional relational technol-\nogies. Redshift was originally based on PostgreSQL RDBMS, and Azure Synapse is\nbased on the Parallel Data Warehouse technology from Microsoft. This means that\nwhen it comes to schema management, both AWS Redshift and Azure Synapse have\nproperties similar to what you would expect from relational technologies.\n First of all, you need to define the schema of the tables up front before you can\nload any data to it. AWS Redshift supports loading data directly from Avro files, but it\ndoesn’t offer any automated schema inference features, even though the schema, as\nwe know, is embedded into each Avro file. Azure Synapse at the time of this writing\nonly supports loading data from CSV, ORC, and Parquet files. It also doesn’t offer any\nautomated schema inference tools, and you will need to write some conversion tools\nto map Avro/Parquet schemas into Azure Synapse table schemas. \n Once the initial table is created, you need to keep it up to date based on the prin-\nciples that we have described previously. Both AWS Redshift and Azure Synapse sup-\nport SQL ALTER TABLE commands that allow you to modify existing tables by adding\nnew columns, deleting existing columns, or changing column types. Keep in mind\nthat the ALTER TABLE command locks the table and makes it unavailable for read\nand write queries. This means that if your schema modification takes a long time, the\ntable will be offline for any data consumer who tries to access it. Fortunately, both\nRedshift and Synapse are columnar data warehouses, so adding and removing col-\numns is a fast operation even on large data sets. Changing column types will result in a\ndata conversion operation and can take a long time depending on your data size. \n Google Cloud BigQuery takes a different approach to the data warehouse architec-\nture and schema management. BigQuery is not based on a relational technology, which\nhas some pros and cons. A great feature of BigQuery is the fact that it can automatically\ninfer schema from certain file formats, including Avro, Parquet, and JSON. This means\nthat you don’t need to predefine table schemas before loading data into them.\n When it comes to schema evolution, BigQuery only supports adding new columns\nto existing tables. This is also done automatically based on the schema in existing files\n(but can be done manually via an API or command-line tools that BigQuery provides).\nThis means that if you add a column to a data source and to the new incoming Avro files\nin the data platform, when loading this new data into the data warehouse, BigQuery will\nrecognize the new column and will add it to the table automatically. This makes schema\nchange automation very simple if you are only dealing with adding new columns. \n On the other hand, BigQuery doesn’t support SQL ALTER TABLE commands and\ncan’t rename, delete, or change the type of existing columns. The only workaround\nthat is available for this scenario is basically to create a new table with the desired\nschema and load data from the original table into this new table and then drop the\noriginal table. This approach works fine on small tables, but for large tables it can take\na long time and you can incur significant BigQuery costs based on the size of the table\nthat you are re-creating, because BigQuery charges you based on the volume of data\nthat you read.\n\n\n259\nSchema evolution and data warehouses\nSummary\nTraditional data warehouses are based on relational technology, meaning that\nthe structure of the data, it’s column names, their types, and their order must be\nknown up front before any data gets loaded. Any changes to the data source\nschema, such as adding new columns, must be carefully planned so the destina-\ntion schemas and ETL pipelines can be adjusted to accommodate this change.\nUnplanned changes will result in broken ETL jobs and unhappy data consumers. \nThe most common examples of schema changes include adding a new column,\ndeleting an existing column, renaming a column, and changing the column’s\ndata type.\nManaging schema changes proactively is possible when you have control over\nthe application that is producing the data, but it becomes much more challeng-\ning if you start ingesting and using data outside of your control, such as data\nfrom third-party SaaS companies—a common use case for data platforms. \nYou can also perform schema management in the data platform, which can\nallow you to (1) build resilient ETLs by detecting and ideally adjusting to\nschema changes before your ETL pipelines fail, (2) maintain an up to date\nschema catalog with schema details that can also be used for data discovery and\nself-service, and (3) have a history of schema changes for any given data to work\nwith archived data in your data platform and simplify pipeline debugging and\ntroubleshooting.\nIn a well-architected cloud data platform, the Metadata layer has a Schema Reg-\nistry that contains all versions of all schemas for all data sources. Data transfor-\nmation pipelines or people who need to know the schema for a particular data\nsource can fetch the latest version from the Registry and even explore all previ-\nous versions of the schema to understand how a particular data source has\nevolved over time. \nIn a cloud data platform, we can add schema management to the other typical\nsteps (data format conversion, deduplication, data quality checks) performed\nby a data transformation pipeline. The schema-management module in a pipe-\nline checks if the schema for this data source already exists in the Schema Reg-\nistry. If it doesn’t, the module will infer the schema from the incoming data and\nregister it as Version 1 of the schema in the Registry.\nIf the schema already exists, the module will fetch the current version of the\nschema from the Registry, infer the schema from the incoming data, compare\nthe inferred schema to the current schema in the Schema Registry, and create a\nnew schema version that combines old and new definitions in a backward-\ncompatible way, and publish the new schema version into the Registry for other\npipelines to use.\nWhile schema inference works well in a batch environment, it’s not feasible in a\nreal-time data pipeline because, while we could infer the schema for a single\n\n\n260\nCHAPTER 8\nSchema management\nmessage, there is no guarantee that the next message will have the same schema.\nReconciling schemas for each individual message would be both computationally\nexpensive and would result in an extremely large number of schema versions. \nBatch data sources can use a schema-inference approach, and real-time sources\ncan rely on the manual schema management where development teams are\nresponsible for publishing schemas into the Registry and updating schema ver-\nsions in case of changes.\nThere are certain cases where we can’t automate dealing with schema changes,\nand what we need is an alerting mechanism that will let us know that a schema\nchange can cause issues with the downstream pipelines. Pipeline configuration\nfrom the Metadata layer contains information about which data sources are\nused by which data transformation pipelines, so if you know which data source\nwas affected by the schema change, you can easily identify all downstream trans-\nformations and reports that are affected. \nExisting solutions for implementing a Schema Registry are available (AWS Glue\nData Catalog, Azure Data Catalog, and Google Cloud Data Catalog) but limited\nin functionality. Other options with gradually increasing in complexity and sup-\nported features include (1) a plain-text pipeline configuration file and a code\nrepository to version it, (2) a key/value or a relational database to store pipe-\nline configs and other metadata, and (3) adding a REST API layer on top of the\ndatabase to provide a uniform interface for all tools in the platform.\n8.6\nExercise answers\nExercise 8.1:\n 3—While schema-on-read allows you to ingest data without considering schema,\nyou still need to make sure your data processing pipelines use relevant schema defini-\ntions.\nExercise 8.2:\n 3—Backward-compatible schemas allow you to always use the most recent versions\nof the schema to read all data that exists in the data platform for a given data source.\n\n\n261\nData access and security\nIn this chapter we acknowledge that the primary reason for developing a data plat-\nform is to cost effectively and securely make data available to data consumers—at\nscale. While throughout this book we have assumed that your data platform will\ninclude a data warehouse to support users who access data via business intelligence\n(BI) tools or by running SQL queries directly, this isn’t the only way data will be\naccessed.\n Increasingly, raw data in storage is also being accessed by users, especially data\nscientists. And, increasingly, applications want access to data in storage as well. The\nlayered design we’ve discussed throughout this book makes it easy to support a vari-\nety of data consumers.\nThis chapter covers\nAppreciating how data from the data platform is \nconsumed \nComparing cloud-native data warehouse offerings \nUsing cloud-native services for data access \npatterns for applications\nSimplifying the machine learning lifecycle \nUnderstanding the basics of a cloud security model\n\n\n262\nCHAPTER 9\nData access and security\n Because the data warehouse is the most popular way to access data for reporting or\nad hoc data analysis, we will review existing cloud data warehousing platform-as-a-service\noptions and highlight their key differences and similarities. We will discuss how to\nenable your applications to become data driven by providing them with data access via\nfast data stores such as cloud RDBMSs or key/value services. We will also cover ways to\nenable your data science team with access to the large amounts of data they need to\ndevelop robust machine learning models. \n With the significant proliferation of different types of data consumers and differ-\nent parts of the data platform that they can access, it’s very important to consider how\nto provide this access and in a secure manner. We will cover some of the fundamentals\nof cloud security at the end of this chapter.\n9.1\nDifferent types of data consumers\nData platforms exist to serve data to data consumers. There is little reason to build a\nsophisticated architecture to ingest, process, and store data if no one can access it or if\ngetting access to the data is a cumbersome process. Data warehouse–centric solutions\noffer only one way to access the data—connect to the warehouse with your favorite BI\nor reporting tools, or run SQL queries directly. This process is simple and familiar to\nmany users, but it is also very limited. \n Today, more and different types of data consumers need fast, secure, and reliable\naccess to the data, and a single point of data access via the warehouse can’t satisfy\nthem all. We can generalize different types of data consumers into two categories:\nHuman users—Many people will need to run reports using BI tools or run SQL\nqueries against the data platform, but other users, such as data science teams,\nmay need direct access to the raw data files to run their experiments on.\nApplications—Modern data analytics is not only about helping businesses to\nmake decisions based on data. All types of applications are becoming “data\ndriven,” meaning that applications utilize data analytics approaches to enhance\nthe end-user experience. This includes various machine learning applications\n(ML) like recommending products that we think the customer might be inter-\nested in or predicting when some factory equipment should be maintained\nbefore it develops a problem.\nData warehouses alone can’t satisfy the needs of all these consumers. While data ware-\nhouses remain the primary way to access data for BI tools and direct SQL access, appli-\ncations rarely connect to the warehouse directly. Applications usually need much\nfaster response time than even modern cloud-based data warehouses can provide. You\nalso need to keep the cloud costs in mind all the time. If previously the community of\ndata users was limited to only a few people in your organization, with applications now\nrequiring data access, this community is extended to potentially thousands or millions\nof users all over the world. Many of the cloud services that we will be talking about in\nthis chapter have a consumption billing model, where you will be charged for the vol-\nume of data you push through the system, or in some cases, per individual query. \n\n\n263\nCloud data warehouses\n Luckily, our cloud data platform design takes that into account. The layered archi-\ntecture utilizes different technologies and different storage types that can address the\nneeds of any data consumer. Figure 9.1 shows how different data consumers can\naccess data from different layers of our platform. \n9.2\nCloud data warehouses\nWe started this book with a comparison of two different architectures: one in which a\ndata warehouse is a center of the data processing and data serving universe, and one\nwhere a data warehouse is just another component in a more flexible layered data\nplatform. Nevertheless, data warehouses remain the most common way to access the\nresults of the data processing pipelines. There are multiple reasons for this. First of\nall, data warehouses have full support of the SQL language standard. SQL remains the\nmost popular data access and data manipulation language out there. The popular BI\ntools are all SQL-based, and for many power data users, writing a SQL query is even\neasier and faster than using a reporting or BI tool. \nOrchestration overlay\nFast storage\nReal-time processing and analytics\nOperational \nmetadata\nBatch processing and analytics\nSlow storage/direct data lake access\nETL tools overlay\nData\nwarehouse\nIngestion\nStreaming\ndata\nBatch\ndata\nRDBMS\nKey/value stores\nCache\nBI tools\nAd hoc SQL\nML libraries\nML collaboration\ntools\nHuman\nusers\nData\nscience\nteams\nApps\nHuman\nusers\nFigure 9.1\nCloud data platform architecture allows different data consumers to use layers that are most \nsuitable for their data access patterns.\nExercise 9.1\nWhy does it matter that there are different types of users of a cloud data platform?\n1\nThey will all be competing for the same data.\n2\nThey are likely to want different ways to access data. \n3\nThere is no difference between any users of a cloud data platform.\n\n\n264\nCHAPTER 9\nData access and security\nNOTE\nWhile most cloud data warehouses support the SQL ANSI standard,\nthey also introduce multiple extensions to the language, so queries written for\none warehouse may not necessarily work in another.\nMost of the existing cloud warehouses are relational by nature (with the exception of\nGoogle BigQuery). This means that existing BI, reporting, and other tools that sim-\nplify working with data in a traditional data warehouse are easy to use with a cloud\ndata warehouse. This compatibility is very important, because unless you are building\na brand-new data platform and don’t have any existing or legacy reporting in your\norganizations, you will need to give your users tools that they are familiar with.\n In the following sections, we will give a brief overview of the cloud data warehouse\nofferings for the three major cloud providers: AWS, Azure, and Google.\n9.2.1\nAWS Redshift\nAWS Redshift is a distributed relational cloud data warehouse. Let’s unpack what this\nactually means. Distributed means that Redshift can distribute large data sets that you\nhave to multiple machines (nodes) and run queries on those data sets in parallel, uti-\nlizing CPU and memory of multiple computers. Relational means that at its core Red-\nshift is based on a relational technology. Redshift has roots in an open source\nPostgreSQL database, and you might notice some similarities in commands and\nbehavior if you are familiar with PostgreSQL. Finally, cloud data warehouse means\nthat a lot of the management operations on the warehouse are performed by AWS\nand are hidden from the end user. Managing any distributed system yourself is a\ndaunting task since you need to think about how to replicate data between different\nnodes, what to do in case of network issues, etc.\n Figure 9.2 shows a high-level architecture of a Redshift cluster.\nNOTE\nDiagrams in these sections are high level and do not depict the actual\narchitecture of the underlying service. They are based on available documen-\ntation from the cloud vendors, which doesn’t necessarily go into implementa-\ntion details very deeply.\nA Redshift cluster consists of multiple nodes. A leader node is responsible for accept-\ning all connections from clients, parsing queries and distributing queries to the corre-\nsponding nodes. A leader is also responsible for deciding which node will get the new\ndata as it is added into the warehouse. We will talk about different ways Redshift dis-\ntributes data among nodes later in this section. A leader is also responsible for certain\nqueries that can’t be distributed. These include system operations such as getting a list\nof tables, checking user permissions, etc.\n Each compute node gets a portion of the whole data set that is stored in the ware-\nhouse. Our diagram shows a leader node and three compute nodes, but the cluster size\ncan be larger than that. Internally, Redshift actually subdivides each compute node into\n“slices” where each slice gets a portion of node compute and storage capacity. This\narchitecture makes it possible to scale a Redshift cluster by adding new compute nodes.\n",
      "page_number": 252
    },
    {
      "number": 9,
      "title": "Data access and security",
      "start_page": 284,
      "end_page": 311,
      "detection_method": "regex_chapter",
      "content": "265\nCloud data warehouses\nAfter a new node is added, Redshift needs to rebalance the data slices and copy some\nof them to the new node. This process runs in the background but will negatively\nimpact query performance. You need to plan scaling operations carefully to avoid\nimpact on existing users.\n You can control how data slices are distributed to different compute nodes when\ncreating a Redshift table. To do this you need to specify a DISTSTYLE property of the\ntable. It can take the following values:\n\nALL—This places a copy of the table into each of the compute nodes. This dis-\ntribution style is useful for small tables that are frequently joined with other,\nlarger tables. Having a copy of the table on a compute node avoids slow net-\nwork transfers.\n\nEVEN—The table is evenly distributed across all compute nodes, so each node\ngets approximately the same number of rows. \n\nKEY—Distributes a table by a given column. You need to specify a column to use\nwith a DISTKEY property. All rows with the same key will be allocated to the\nsame compute nodes. If you have two large tables that are frequently joined\ntogether, distributing them using the same KEY will significantly improve perfor-\nmance because rows with common keys will be on the same compute node and\nwill not have to be transferred over the network. \n\nAUTO—Starts with the ALL distribution style and switches to EVEN automatically\nas the table size increases.\nSQL queries from clients\nA leader node is responsible for\naccepting all connections from\nclients, parsing queries, and\ndistributing queries to the\ncorresponding nodes as well as\ndeciding which node will get the\nnew data as it is added into the\nwarehouse.\nCompute nodes store and \nprocess a portion of the \nwhole data set. Each node\nhas its own storage.\nA Redshift cluster is \nmade of multiple nodes.\nLeader node\nStorage\nStorage\nStorage\nStorage\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nCompute\nnode\nCompute\nnode\nCompute\nnode\nFigure 9.2\nAn AWS Redshift cluster consists of a leader node with multiple compute nodes \nthat are used to distribute data and workload among themselves. Each node has its own \nstorage attached to it.\n\n\n266\nCHAPTER 9\nData access and security\nSetting a proper table distribution style is the most important performance optimiza-\ntion technique in Redshift.\nNOTE\nPlease refer to AWS documentation for specifics on how to configure\ntable distribution styles and best practices around that.\nAdditionally, you can specify a SORT KEY for a table to force Redshift to physically\norder a table by a given column. This will improve performance for queries that\nrequire data to be sorted in some way, for example, by a date column.\n Redshift is a columnar data warehouse, meaning data on a disk is organized by col-\numns instead of rows. We described the differences between row-oriented and colum-\nnar formats in chapter 5. Columnar storage allows Redshift to use various\ncompression algorithms for different columns. For example, you can specify a “byte-\ndictionary” encoding for a column that has a small number of unique values in a\ntable, such as a column that stores a country name or a status column with only a few\npossible values. This will significantly reduce the data size on disk and improve query\nperformance. Keep in mind that Redshift compute nodes have fixed storage, memory,\nand CPU capacity, and while you can add new nodes to the system, you need to keep\noverall system cost in mind at the same time. Instead, you should try to optimize the\nsize of your data by providing a relevant encoding for specific columns. Redshift sup-\nports a number of different encodings, and you can find detailed explanations of\nwhen to use them in the Redshift documentation.\n Redshift has relational roots, which is reflected in the data types it supports. Red-\nshift only supports what is called “primitive data types” such as integers, strings, dates,\netc. It doesn’t support arrays or nested data structures. This makes working with JSON-\nstyle data challenging because you need to store these values as strings and rely on\nJSON-parsing functions to extract necessary values from them. This approach works for\nsmall data sets, but if your data is mostly JSON documents, then if you are storing it as\na string column in Redshift, you are not utilizing any of the optimizations we have\ntalked about, such as distributing tables by key or using specific encoding for columns. \n Redshift also offers a feature called Spectrum that allows you to query data that is\nstored in AWS S3 storage directly. To use Spectrum, you need to create a Redshift\ntable that is marked as “external” and specify a path to the data on S3. Spectrum que-\nries would typically be slower than queries against tables that are stored on Redshift\ncompute nodes because you don’t benefit from Redshift optimizations that we have\ndescribed previously. In our data platform architecture, you can use Spectrum for data\nexploration or for offloading certain workloads from the data warehouse. Figure 9.3\nshows how Spectrum fits into Redshift’s distributed architecture.\n In our design, we store all the new and archive data in the S3 layer, which makes it\neasy to query it using Spectrum. You can use Spectrum to run queries against this data\nin S3 and even join it to the data that is already stored in Redshift. This is useful if you\nare dealing with a new data set and need to do some data exploration before deciding\nwhether you need to bring it into the data warehouse or what the right Redshift table\nstructure for this data should be.\n\n\n267\nCloud data warehouses\nImagine that you have an established data platform with a Redshift warehouse. Your\ncompany uses the warehouse to create various reports and dashboards for sales data.\nNow you have obtained new data sources that contain improved demographics data\nfor your customers, and you think it will help your business users to make better deci-\nsions, but you are not a hundred percent sure it is useful. You can load this new data-\nset into Redshift and ask your end users to run some experimental queries against it,\nbut this will consume warehouse storage and compute resources. If your warehouse is\nvery busy, this new data set may even require you to add new nodes to the Redshift\ncluster. Instead, you can create an external table in Redshift and use Spectrum to run\nthese experimental queries outside of Redshift and easily discard the dataset if your\nusers don’t find it helpful. This way, the impact on the warehouse is minimal. \n Keep in mind that using Spectrum still requires you to create external tables in\nRedshift, and while you are not physically copying data to the warehouse at this point,\ntable definitions can pollute your curated warehouse design, especially if you allow\nmultiple people to create external tables. We recommend you group external tables\ninto a dedicated database and periodically clean up table definitions that are no lon-\nger needed. \n You can also use Spectrum to offload some processing from your Redshift data\nwarehouse. For example, if you have a large table that you need to query, loading it\ninto Redshift would require adding more compute nodes, which will increase your\nwarehouse cost. If query performance is not a concern, you can choose to not load\nthis table into Redshift from S3 and instead query it directly using Spectrum.\n In our data platform architecture, Spectrum is definitely an optional feature,\nbecause you can achieve the same results by running Spark jobs or Spark SQL queries\nSpectrum allows you to query\ndata that is stored in AWS S3\nstorage directly.\nAWS Spectrum on-demand compute\ncapacity\nData lake on AWS S3\nQueries to external\ntables are offloaded\nto Spectrum.\nSQL queries from clients\nLeader node\nCompute\nnode\nCompute\nnode\nCompute\nnode\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nStorage\nStorage\nStorage\nStorage\nFigure 9.3\nAWS Spectrum can be used to query data that is stored in the data lake without \ncopying it to the warehouse storage first. Spectrum can also offload some processing from your \nwarehouse by using an on-demand compute model.\n\n\n268\nCHAPTER 9\nData access and security\ndirectly on the data in the S3 storage. It all comes down to whether your end users are\ncomfortable with Spark and Spark-specific tools or whether they prefer to only inter-\nact with the warehouse.\n9.2.2\nAzure Synapse\nAzure Synapse is a distributed cloud data warehouse offering from Microsoft. It is\nbased on the Microsoft Parallel Data Warehouse product and is based on a relational\ntechnology. This makes Azure Synapse, much like AWS Redshift, easily compatible\nwith various existing reporting and BI tools that expect a relational database as their\nsource of data. \n The high-level architecture of Azure Synapse is similar to Redshift, but with a few\nkey distinctions. Figure 9.4 shows Azure Synapse architecture.\nSimilar to AWS Redshift, Synapse clusters consist of a control node and compute\nnodes. The control node accepts connections from clients, parses and validates the\nincoming queries, and sends those queries to compute nodes to execute. One major\ndifference between Synapse and Redshift is that Synapse separates the storage layer\nfrom the compute layer. Synapse splits all your data into 60 data distributions, and each\ndata distribution is attached to a particular compute node, but data is not stored on the\ncompute nodes themselves. This design makes scaling the cluster a simple and fast\noperation. You can add new compute nodes or remove compute nodes, and Synapse\nSQL queries from clients\nControl node\nCompute\nnode\nCompute\nnode\nCompute\nnode\nData\ndistribution 1\nData\ndistribution 2\nData\ndistribution 60\nAzure Storage\nIn Synapse, as in Redshift,\ncontrol nodes accept\nconnections from clients, parse\nand validate the incoming\nqueries, and send those queries\nto compute nodes to execute.\nUnlike with Azure Synapse, \ndata is stored independently\nof compute nodes, which\nmakes adding and removing\nnodes simpler.\nFigure 9.4\nAn Azure Synapse cluster consists of a control node and compute nodes, with data \nstorage being completely separate from the compute layer.\n\n\n269\nCloud data warehouses\nwill only need to adjust how data distributions are mapped to compute nodes, without\nhaving to actually copy any data around. \n It’s important to note that, currently, scaling a Synapse cluster is an offline opera-\ntion, meaning the cluster will need to complete or cancel any running queries and will\nnot accept any new queries while the scaling operation is underway. This makes Syn-\napse not truly elastically scalable and forces you to plan your scaling operations care-\nfully in order not to impact your users.\n Another property of this design is that you can pause all your computer tasks com-\npletely and resume them later without having to move data anywhere. If your ware-\nhouse doesn’t need to be online 24/7, you can pause it at the end of day and resume\nfirst thing in the morning. Or you can pause it on the weekends. This will save a signif-\nicant amount of your cloud costs because Synapse charges you for compute and stor-\nage separately, and compute is much more expensive than storage.\n Azure Synapse doesn’t allow you to directly specify the type and the number of\ncompute nodes in the cluster. Instead, you need to specify the overall cluster capacity\nusing Data Warehouse Units (DWUs). DWU represents a combination of CPU and\nmemory capacity. In our experience, trying to configure the right number of DWUs is\na bit of a trial-and-error process where you need to run your workload on different-\nsize clusters and arrive at a capacity that gives you the optimal price/performance\ncombination. \n Similar to Redshift, you can specify how a Synapse table will be split into multiple\ndistributions. There are three options that are available:\n\nHASH—This requires a column name to be specified. All rows with the same\nvalue for this column will be placed into the same data distribution. \n\nROUND ROBIN—The table will be split into equal chunks and spread across all\ndata distributions.\n\nREPLICATE—A copy of the table will be placed into each data distribution.\nAs you can see, table distribution options are similar to those in AWS Redshift. Setting\nthe right distribution method for a table is the main performance optimization tech-\nnique, so you will need to spend some time planning this and taking into account the\nmost common ways the end users will query the data in the warehouse. You can also\nforce Synapse to order the data in the table using specific columns during the table\ncreation. This will speed up queries that expect data to be sorted in a particular way.\nNOTE\nPlease refer to Azure documentation for more details and best prac-\ntices around Synapse table design.\nAzure Synapse uses a columnar data store and applies compression to each column\nseparately. Unlike with Redshift, you can’t specify a compression algorithm for each\ncolumn, and you have to rely on Synapse to make the best choice for you. \n Synapse only supports primitive data types (integers, dates, strings, etc.) and\ndoesn’t support arrays or nested data structures. It provides a number of built-in JSON\n\n\n270\nCHAPTER 9\nData access and security\nparsing functions that you can use to read JSON data from a string column, parse it,\nand access specific attributes inside the JSON document. Similar to Redshift, this\napproach doesn’t utilize many of the columnar optimizations and will not provide you\nwith optimal performance. \n When talking about AWS Redshift, we have discussed a Spectrum feature that\nallows you to query data directly from S3 storage without having to load the data into\nthe warehouse first. Azure Synapse offers similar features by introducing the notion of\npools. Currently there are three types of Azure Synapse pools: SQL pool, SQL on-\ndemand pool, and Spark pool. \n The architecture we described earlier with provisioned capacity, data distributions,\nand compute nodes applies to SQL pool only. This is your traditional cloud data ware-\nhouse module where you need to load data into the warehouse table first before you\ncan query it. With the SQL on-demand pool, you can query Parquet, CSV, or JSON\ndata directly from the Azure Blob Storage in a serverless fashion. When you run a\nquery in the SQL on-demand pool, Azure will provision the required compute node to\nprocess your query and destroy the compute node after your query is finished. This is\nuseful for data exploration use cases or for offloading certain workloads from your\nmain SQL pool. \n In addition to SQL on-demand, Azure Synapse also supports Apache Spark pools.\nThis allows you to run Spark jobs against data in the Blob Storage using the same Syn-\napse interface. Spark pools are not completely ephemeral like SQL on-demand pools\nand require a minimum of three nodes to be available all the time. Spark pools sup-\nport autoscaling, which means extra nodes can be provisioned by Azure to process\nyour job, and then the cluster will be scaled down to its original configuration.\n9.2.3\nGoogle BigQuery\nAmong the three major cloud providers, the Google cloud data warehouse offering\ndefinitely stands out. Unlike Redshift and Synapse, BigQuery is not based on any exist-\ning relational technology and was developed for internal Google purposes before\nGoogle Cloud existed. This heritage provides BigQuery with some unique properties. \n First of all, BigQuery (see figure 9.5) is closer to a fully managed service than Red-\nshift or Synapse. While AWS and Azure take care of the many maintenance and opera-\ntional tasks that are required to make a distributed data warehouse work, you still\nneed to worry about choosing the right type of nodes (in AWS) and planning how\nmuch capacity your cluster will need (both in AWS and Azure). BigQuery doesn’t\nrequire any of that. The cluster capacity is provisioned by Google Cloud on the fly for\nyou and can be different from query to query. BigQuery uses very large pools of hard-\nware resources (tens of thousands of nodes) that are shared between different Google\nCloud customers. This allows BigQuery to allocate required processing power (slots,\nin Google Cloud terminology) on a per-query basis, without having to provision new\nmachines for it (which takes time). For the end user’s perspective, the result is a truly\nelastically scalable data warehouse. \n\n\n271\nCloud data warehouses\nBigQuery uses a tree-like architecture with an additional layer of “intermediate serv-\ners” that sit between the root node and the leaf servers. Intermediate servers are\nresponsible for sending work to specific leaf nodes, collect results from leaf nodes,\nperforming aggregations, etc. Having a three-tier architecture instead of a two-tier\narchitecture like we saw in Redshift and Synapse allows BigQuery to scale to a much\nlarger number of nodes in the cluster.\n Another important property of BigQuery architecture is that in order to be truly\nelastically scalable, it can’t rely on data locality. Data locality in distributed systems\nmeans that a node that processes data should have the fastest access to the data it is\nprocessing as possible. Having local storage such as a hard drive or SSD directly on the\ncompute node is the fastest option, but it is really expensive to move data from one\nnode to another because you need to wait for the data to copy. Another option is to\nattach a network storage of some sort to the compute node so it can read the data and\nthen reattach it to another node if you need to rebalance your cluster. This is faster\nthan a full copy, but still takes time. BigQuery solves this problem by using a Google\ninternal network, which provides high enough throughput and low enough latency\nfor the data locality not to matter as much. This means that leaf nodes in BigQuery\ncan read the data from the storage and not worry much about whether data is local or\ndirectly attached to the node.\n BigQuery is based on an internal Google data processing system called Dremel,\nand Dremel was developed to allow users to analyze very large volumes of various log\nfiles. Log files, as you can imagine, don’t have a relational structure with tables and\nSQL queries/API calls\nfrom clients\nRoot\nserver\nIntermediate\nserver\nIntermediate\nserver\nLeaf\nserver\nLeaf\nserver\nLeaf\nserver\nGoogle’s uber-fast network\neliminates the need to rely\non data locality.\nBigQuery Data Transfer Service \nuses a tree-like architecture with \nan additional layer of “intermediate \nservers” that sit between the root \nnode and the leaf servers and are\nresponsible for sending work to \nspecific leaf nodes, collect results from \nleaf nodes, performing aggregations etc.\nThis three-tier architecture allows \nBigQuery Data Transfer Service\nto scale to a large number of\nnodes in the cluster.\nServers in BigQuery\nData Transfer Service\nare shared across\nmultiple customers.\nData is chunked\ninto shards and\nstored in a\ndistributed file\nstorage.\nFigure 9.5\nBigQuery uses large pools of shared resources to provide elastic scalability. Storage and \ncompute are separate, and the fast internal Google network allows not to worry about data and compute \nnodes being “close.”\n\n\n272\nCHAPTER 9\nData access and security\nkeys that can be used to join those tables together. Also, log files can store a wide vari-\nety of data, including nested data structures such as JSON documents. BigQuery is the\nonly cloud data warehouse among the three major providers that has a native support\nfor nested data types. BigQuery supports arrays of values as well as JSON data with\nnested attributes. Support is native, meaning that BigQuery actually stores each JSON\nattribute as a separate column and not a full document as a string value, as with Red-\nshift or Synapse. This significantly improves your query performance and makes que-\nries much easier to read and write.\n Finally, BigQuery uses a pay-per-use pricing model where you only pay for the\namount of data your query has actually processed. This is different from other ware-\nhouses where you pay per provisioned capacity, no matter how many queries you run\non the system. BigQuery also charges separately for the storage you use, but those\ncosts are usually negligible in comparison to the compute costs. This pricing model\nhas its pros and cons. For small organizations that don’t have a large volume of analyti-\ncal queries, BigQuery can be extremely cost efficient in comparison with other cloud\nproviders or traditional data warehouses, but as your data and the number of queries\nincreases, it becomes very hard to predict the total cost of BigQuery. To do this accu-\nrately, you need to know every single query that will be executed and how much data\nthis query will read. \nNOTE\nGoogle Cloud offers BigQuery flat-rate pricing for enterprise custom-\ners, which makes costs much more predictable.\nBigQuery also doesn’t offer as many tuning options as Redshift and AWS. You can’t choose\nhow your tables are distributed across the nodes, because, if you recall, in BigQuery, this\nconcept doesn’t exist. You also can’t control the compression algorithms and have to rely\non internal BigQuery columnar storage to optimize the data for you. A couple of import-\nant tuning options for BigQuery are data partitioning and clustering. \n Data partitioning physically splits that data in the BigQuery storage using a particu-\nlar column value. For example, you can partition your table using a month value of a\ndata column. If you do that, then if you only query data for a given month, BigQuery\nwill read only data in that month’s partition. This is much faster and much cheaper\nthan reading a full table and then filtering out data only for a given month. Partition-\ning in BigQuery is important because it is both a performance optimization and a\ncost-control mechanism. \n Clustering of BigQuery tables physically organizes and sorts data in a table or a par-\ntition using one or more columns. This is a performance optimization technique and\nwill speed up queries that often require data to be sorted or aggregated by a certain\ncolumn. You can combine partitioning and clustering together.\n One of the side effects of BigQuery’s non-relational nature is that it is not as seam-\nlessly compatible with existing data visualization and reporting tools as Redshift or Syn-\napse. The primary way of BigQuery accepting requests from clients and sending data\nback is via a REST API. There is an existing third-party implementation of a\nJDBC/ODBC driver for BigQuery, but it still does the translation to REST API calls\n\n\n273\nCloud data warehouses\ninstead of using a native network protocol. This may cause performance issues if you are\ntrying to query large amounts of data from BigQuery into a BI tool. Another compati-\nbility challenge is that many of the existing reporting and BI tools expect data to be in\na relational format with multiple tables and joins between them. These tools may not\nyet understand how to natively work with nested JSON data and arrays. Given the\nincreasing popularity of BigQuery, we expect more and more BI vendors will work out\nthese compatibility issues eventually, but if you have an existing reporting tool that your\norganization uses extensively, it’s a good idea to check its compatibility with BigQuery.\n BigQuery supports external tables, which allows you to query data from Google\nCloud Storage and other data sources such as Cloud Bigtable or Cloud SQL. Unlike\nRedshift and Synapse, BigQuery doesn’t allocate any additional capacity for these que-\nries and uses the same slots available to you for regular BigQuery workloads. The pric-\ning is also the same for external tables as it is for internal BigQuery tables, so while you\ncan use BigQuery to query data directly in your data lake, you need to be cognizant of\nthe cost and other workloads that you run on the data warehouse. \n It is also worth mentioning a recently (2020) introduced feature called BigQuery\nOmni. Omni allows you to deploy BigQuery software on other cloud providers’ VMs,\nsuch as AWS or Azure. You still get all the features of BigQuery, but now you don’t\nneed to copy the data from AWS or Azure into Google Cloud, which can be very slow\nand very expensive. You will still need a Google BigQuery deployment to act as an end-\npoint that accepts queries from clients, provides the UI, etc., but you will be able to\ncreate external tables that actually reside on AWS S3 and Azure Blob Storage.\n This is a new feature, and we haven’t seen it used in real-life scenarios yet. Obvi-\nously, you will not be able to get the same scale of tens of thousands of nodes in Omni\nas you would get natively in BigQuery, but the benefits of not having to move data\nfrom one cloud provider to another may outweigh the limitations for your use case.\n9.2.4\nChoosing the right data warehouse\nAs you can see, different cloud data warehouses offer different sets of features, and a\nnatural question is, which one is the best for your use case? In our experience, the\nchoice of a cloud data warehouse is almost never a separate decision and is always tied\nto choosing a cloud provider. For large organizations, using different cloud vendors\nfor different parts of their infrastructure makes sense, because this protects them\nfrom vendor lock-in and gives them an upper hand in negotiating the best possible\ndeal with each provider. For smaller organizations, a multicloud approach has too\nmuch overhead and extra engineering cost to be truly feasible.\n We have seen use cases in the past where an organization would have its applica-\ntion estate deployed on one cloud provider and their data analytics and machine\nlearning workloads on another provider, but in such cases you need to be cognizant of\nthe data transfer costs between providers. \n In most cases, the choice of the cloud provider will dictate which data warehouse\nyou will use for your cloud data platform. Table 9.1 summarizes some of the key ware-\nhouse features of AWS, Azure, and Google Cloud.\n\n\n274\nCHAPTER 9\nData access and security\n  \n9.3\nApplication data access\nOver the last several years, there has been an increasing trend for applications to\nbecome more and more data driven. Websites now provide recommendations on\nthings users might like or find useful, provide more personalized experience, etc.\nApplications like these may require access to data that was previously only available in\nthe data warehouse and used exclusively for reporting or ad hoc analysis.\n It is, of course, possible to provide your applications with direct access to the cloud\ndata warehouse, but there are several major challenges with this. First of all, cloud ware-\nhouses are all designed to provide reasonable and consistent query performance over\nlarge data sets, but they were not designed to provide low-latency access. A typical query\nexecution time in a cloud data warehouse is seconds to minutes, while most applica-\ntions require significantly faster response time, usually measured in milliseconds. \nNOTE\nIn this section, we will use “application” to describe a wide spectrum of\ndifferent use cases, but all of these use cases will have the following require-\nments in common: exposed to internet or a large user community inside your\norganizations, are interactive, require fast response time for the data stores,\nand rely on data produced by the data platform. \nAnother challenge with applications is that cloud data warehouses can easily deal with\nhundreds of concurrent queries, which is usually enough for reporting and analytics\nTable 9.1\nComparing key data warehouse features\nAWS Redshift\nAzure Synapse\nGoogle BigQuery\nBased on a relational \ntechnology?\nYes\nYes\nNo\nSupport for nested \ndata structures?\nLimited (via JSON \nparsing functions)\nLimited (via JSON \nparsing functions)\nNative support\nHow does it scale \nup/down? \nManually\nManually\nAutomatically\nPay per use or pay per \nprovisioned capacity?\nPay per provisioned \ncapacity\nPay per provisioned \ncapacity\nPay per use (with option to pay \nper provisioned capacity)\nExercise 9.2\nWhich of AWS Redshift, Azure Synapse, or Google BigQuery are not based on relation-\nship technology?\n1\nRedshift\n2\nSynapse\n3\nBigQuery\n4\nAll of the above\n\n\n275\nApplication data access\nuse cases even at very large organizations. Applications, on the other hand, often are\nexposed to tens or hundreds of thousands of the end users and thus have much\nhigher concurrency requirements. Popular applications connected directly to the\nwarehouse can easily consume all available capacity. \n There is also a security concern with having internet-facing applications connect-\ning to the data warehouse. Data warehouses may contain sensitive data access that\nmust be restricted to internal users only. This often leads to cloud data warehouses\nbeing deployed inside cloud virtual networks to restrict who can access the data on the\nnetwork level. If you expose your data warehouse for application access and applica-\ntion becomes compromised, this potentially can lead to data exfiltration. \n There are several alternatives that can be used as a data store for applications and\nthat don’t have the limitations that we have outlined. Using a dedicated data store for\napplications will not only provide faster data retrieval times and handle concurrent\nrequests but will also allow you to only load data required for a particular application\ninstead of providing access to the data warehouse.\nIn this section, we will do an overview of services that major cloud vendors provide for\napplication data stores. This includes the following:\nRelational databases\nKey/value data stores\nFull-text search systems\nIn-memory caches\n9.3.1\nCloud relational databases\nRelational databases are a tried-and-true way to provide applications with a fast and\nreliable data store. Relational data modelling is well understood and can be very flexi-\nble when done correctly. Cloud-based relational databases are also a great choice if\nyou are migrating an existing application into the cloud. Today, every major cloud\nprovider has at least one service that offers a managed relational database instance.\nThese managed services take care of many of the day-to-day tasks that previously you\nExercise 9.3\nWhich of the following are reasons for not connecting your applications directly to your\ndata warehouse?\n1\nThey have high concurrency requirements and could potentially consume all\navailable capacity.\n2\nThey need lower latency than the data warehouse can provide.\n3\nIf your application becomes compromised, it could potentially lead to data\nexfiltration. \n4\nAll of the above.\n\n\n276\nCHAPTER 9\nData access and security\nwould have to deal with yourself. This includes backup automation, replication, oper-\nating system patching, updates, etc. Because we are only loading specific data sets that\nhave been preprocessed in the data platform into these data stores, the scalability\nrequirements for application data stores are typically significantly smaller than for the\ndata platform itself. \n AWS offers Relational Database Service (RDS) for PostgreSQL, MySQL, MariaDB,\nOracle, and SQL Server. These would perform well with little tuning and optimiza-\ntions on data sets that are under 1 TB in size. It is possible to scale relational databases\nto much larger data sets, but that would require extra planning and work. For large\ndata sets and applications that need to scale both read and write operations, or server\nusers in multiple geographical regions, AWS offers Aurora, a distributed database ser-\nvice, that is compatible with MySQL and PostgreSQL.\n Google has a service called Google Cloud SQL that offers managed MySQL, Post-\ngreSQL, and SQL Server databases. Google Cloud  Spanner is a large-scale distributed\ndatabase that offers seamless replication into multiple geographical regions. Spanner\nautomatically makes all data available in different geographies without the drawback\nof eventual consistency that are common in key/value data stores.\n Azure’s managed RDBMS offering is an Azure SQL Database that supports\nMySQL, PostgreSQL, and SQL Server. Because SQL Server is a Microsoft product,\nAzure offers multiple options for a cloud version of its flagship database product. You\ncan choose between a fully managed Azure SQL Database or a hybrid Managed\nInstance service. Managed Instance offers more control over the virtual machine on\nwhich the SQL Server runs and also is more compatible with the on-premises versions\nof the SQL Server. For large-scale applications, Azure has a HyperScale version of\nAzure SQL Database. HyperScale is only available for the SQL Server version.\n9.3.2\nCloud key/value data stores\nKey/value or NoSQL data stores became a popular alternative to relational databases\nbecause of their simpler data model and the fact that they are easier to scale. While\ndifferent NoSQL data stores have slightly different data models, the general idea is\nthat you have a unique identifier for the row (the “key”), and you have a number of\ncolumns attached to the key (“values”). Such data stores provide really low latency for\nfetching or writing values for one or more keys. Key/value data stores are popular for\ngreenfield application development because they make it easier to iterate and make\nchanges to the schema. \n AWS’s primary key/value service is DynamoDB. One of its key characteristics is\nconsistent performance at any scale. DynamoDB offers two pricing models: pay-per-\nuser or pay for preprovisioned capacity. The pay-per-use approach is typically cheaper\nfor light or spiky workloads. \n Google Cloud has two services in the key/value category: Datastore and Cloud Big-\ntable. Google Cloud Datastore is similar to DynamoDB in its data model but offers\nonly a pay-per-use model, which can get expensive if you are performing lots of read\n\n\n277\nApplication data access\nand write operations. Cloud Bigtable offers a simpler data model than Google Cloud\nDatastore; for example, there is no notion of data types for columns in Cloud Big-\ntable, and all columns are represented as arrays of bytes. This makes it possible to\nwrite pretty much any values into Cloud Bigtable, but it can be challenging for appli-\ncation development because you will need to track data types in the application code.\nCloud Bigtable scales to much larger volumes than Google Cloud Datastore, but its\npricing model only has a pay for preprovisioned capacity option. Cloud Bigtable clus-\nters have a minimum-of-three-nodes requirement, which makes it not a cost-efficient\noption for smaller scale applications. One of the popular use cases for Cloud Bigtable\nis migrating existing Apache HBase applications from on-premises Hadoop clusters\ninto Google Cloud. Apache HBase and Cloud Bigtable are compatible on the API\nlevel, which means you can port your existing Apache HBase applications to Cloud\nBigtable with minimal changes. \n Azure’s key/value data store is Cosmos DB. Its unique feature is support for multiple\npopular APIs. Cosmos DB can be configured to support MongoDB, Cassandra, SQL,\nand graph APIs. You need to specify which API you want to use before creating a Cosmos\nDB database. Being able to use MongoDB or Cassandra client libraries with Cosmos DB\nmakes porting existing applications a much simpler process.\n9.3.3\nFull-text search services\nVery often the data that your applications need to deal with is not nicely structured\ntables, with numeric metrics such as sales amount or various counts that you query and\naggregate. If your application needs to deal with text data and provide search function-\nality, then taking a look at full-text search data stores is a good idea. For example, your\napplication may allow users to search through hotel room descriptions or product\ndescriptions and return entries that don’t match the search request 100% but rather\nfind entries very similar to the search request using the natural language semantics.\n Apache Solr and Elasticsearch are two popular, full-text-search data stores. Both\nare based on an open source search library called Apache Lucene. From an applica-\ntion developer perspective, a full-text search data store allows you to load JSON docu-\nments into the data store and then perform search operations on that data. Search\ndata stores can index all attributes in your document or you can specify which attri-\nbutes should be indexed. You get different search options depending on the type of\nthe attribute in your document. For example, for attributes that store numeric values,\nyou can use equals, less than, greater than, etc., search qualifiers in your queries. For\ntext attributes, a search data store will allow you to search on exact text match, find\nsimilar entries, recommend entries for autocompletion, etc. Full-text search data\nstores don’t support joins of different documents, so you need to make sure your data\nis organized in such a way that doesn’t require joins.\n AWS offers CloudSearch, a managed service that provides full-text search capabili-\nties. Azure’s full-text search service is called Azure Search and provides features simi-\nlar to CloudSearch. Google Cloud, surprisingly, doesn’t offer a full-text search\n\n\n278\nCHAPTER 9\nData access and security\nmanaged service at the time of this writing. There is a product called Google Search,\nbut it is focused on providing companies with capabilities to index and search their\ninternal Word documents, emails, etc., and is not used as a data store for applications.\n9.3.4\nIn-memory cache\nOpen source solutions such as Redis and Memcached are used to provide sub-millisecond\nlatencies for application data access. Redis and Memcached store data in memory and\nthus can provide much faster response times than a relational or key/value database.\nBecause there is no data persistence, these data stores are used as caches, meaning that\na relational or key/value data store still acts as a source of truth, and data in a cache can\nbe reloaded from a persistent database in the case of a crash or any other issues. The key\nidea behind using an in-memory cache is that your application first tries to find the data\nit needs in the cache, and if it’s not there, or if it is stale, applications can reach into a per-\nsistent data store. \n You can use an in-memory cache to provide a very fast access to the data from the\ndata platform to your applications. The data platform in this case will serve as a per-\nsistent layer, and you can create a process that will load and refresh data in the cache\non a regular basis. Caches have a simple data model, similar to key/value stores, and\ndon’t support complex queries and joins. \n AWS provides a managed service for both Memcached and Redis called Elasti-\nCache. Google only supports Memcached in its Memorystore service, and Azure offers\na managed Azure Cache for Redis service.\n As you can see, there are multiple options for data stores that can be used by appli-\ncations. Often, the choice of the particular data store is driven by application develop-\ners, their experience with various solutions, etc. It’s a good idea for you as a data\nplatform designer and architect to understand different access patterns and cloud ser-\nvices that can be used to implement them.\n In the next section, we will look at yet another data access pattern that we see more\nand more often in the real-life projects. We will talk about machine learning on the\ncloud data platform.\n9.4\nMachine learning on the data platform\nMachine learning (ML) workloads have some unique properties and characteristics.\nDeveloping an ML model requires data scientists to understand what kind of data\ntheir model will need to deal with and then run a number of experiments to choose\nthe best suitable algorithm or an existing library that will fit the purpose. Then a\nmodel needs to be trained, which means its parameters are adjusted in multiple itera-\ntions using a training data set until a model accuracy is within acceptable limits. Once\na model is trained, it needs to be validated to make sure that the model produces\ngood results on data other than the training data set. And finally, the model can start\nserving results to the end users by accepting new data, applying calculations, and pro-\nducing results. \n\n\n279\nMachine learning on the data platform\n This process requires data science teams to have access to large volumes of data so\nthey can understand what kind of data they are dealing with and what kind of prob-\nlems can be solved with this data. It also requires access to significant compute capac-\nity to run the training process, which includes rerunning training steps hundreds or\nthousands of times. We also need to have a simple way to split data into training and\nvalidation data sets that doesn’t require a lot of manual effort from the data science\nteam. Communication and collaboration between team members during this process\nis also very important. If each data science team member experiments with data on\ntheir own computers, it can be hard to reconcile or share these results with other\nteam members. \n These requirements make a cloud data platform an ideal place for the machine-\nlearning workloads. The data platform already stores all data that is available to your\norganization, including archives of historical data and access to both raw data that is\ningested from the source systems as is and preprocessed data that has been cleaned up\naccording to organizational standards. \n A cloud data platform also provides multiple different ways to access the data:\nSQL, Apache Spark, direct file access, etc. This is important because it allows you to\nuse the ML tools and libraries that may have different requirements for working with\ndata. If your platform only provides one way to access data (say, SQL queries on the\ndata warehouse), it may limit the data science team in the selection of tools and push\nthem to develop models on their computers where they have more control over the\nenvironment.\n The data processing layer of the cloud data platform offers a scalable compute\nplatform that can be used to train models on much larger datasets than would be pos-\nsible on a personal computer or a dedicated VM. Today, all cloud vendors offer access\nto VMs with powerful graphics processing units (GPUs) that can be used to signifi-\ncantly speed up the ML model training process.\n Cloud vendors also realize the importance of seamless collaboration for the model\ndevelopment process and offer a number of tools that allow data scientists to run,\nshare, and discuss the results of their experiments with the other team members or\nstakeholders. These tools are easy to integrate with the cloud data platform and pro-\nvide a cloud environment that checks all the boxes:\nHas access to all organizations data\nProvides a scalable computational environment\nOffers experimentation and collaboration tools \n9.4.1\nMachine learning model lifecycle on a cloud data platform\nA typical machine learning lifecycle has the following steps:\nIngest and prepare data sets\nTrain/validate model loop\nDeploy the model to production to serve results to the end users\n\n\n280\nCHAPTER 9\nData access and security\nFirst, we will take a look at how a typical ML process can be implemented without a\ncommon data platform. Figure 9.6 shows what this process might look like.\n The first thing a data scientist does is try to ingest (copy) the data from the existing\ndata sources into their own computer where they can run the rest of the ML process.\nThen they need to perform some data cleanup steps and convert data from multiple\nformats into a single format that their tools of choice will understand. Then a data sci-\nentist needs to run some exploratory analysis on the data sets just to understand what\nkind of data they contain, checking if there are any outliers in the data or any other\nproperties that may impact the model training process. \n According to multiple studies, these first two steps in the ML lifecycle—ingesting\nand cleaning up data—take up to 80% of all the time data scientists spend on the\nmodel development process. This is not surprising, because as we have seen in previ-\nous chapters, ingesting and preparing data for consumption is an involved process. \n Next, data scientists perform the train/validate loop. To train a model of their\nchoice, they need to split the data they now have into two parts: training data set and\nvalidation data set. Then they run the model training process on the training data set.\nThe training process itself is iterative, meaning an ML model will need to read and\nprocess the training data set multiple times, gradually adjusting its parameters to pro-\nduce results with a required accuracy. This process is computationally expensive, espe-\ncially if you are dealing with large data sets. Running it on a single computer can take\na long time.\nNOTE\nWe are making some simplifications when describing the ML lifecycle.\nThere are many more details involved in the process, such as labeling data\nsets for the supervised learning, etc. \nOnce the training process has finished, the data scientist needs to apply the model to\nthe validation data sets to make sure the model still produces accurate results on the\nHow a typical ML process might\nbe implemented without a\ncommon data platform\nData sources\nIngest data\nPersonal\ncomputer\nClean up and\nexplore data sets\nTraining\ndata set\nValidation\ndata set\nTrain/validate\nloop\nShare the\ntrained model\nData scientist\nFigure 9.6\nTypical ML lifecycle without a cloud data platform\n\n\n281\nMachine learning on the data platform\ndata other than training data. This helps data scientists avoid a problem called overfit-\nting, where the model’s parameters fit the training data set very well but produce inac-\ncurate results on any other data. The way data is split into training and validation data\nsets plays a big role here because you want your validation data set to be as representa-\ntive as possible of data that the model will need to deal with in production. \n Once the training process is complete, the data scientist needs to share the model\nwith their peers or hand it over to the Operations team to actually deploy it into pro-\nduction. One of the challenges with this step is that often code developed on a personal\ncomputer doesn’t integrate very well (if at all) with the production environment. For\nexample, it may not have all the logging, error handling, or monitoring features that\nare required to run an application in production. The fact that data scientists in this\nprocess are isolated from the actual production or testing environments forces the\nOperations or DevOps teams to make significant changes to the model code, for exam-\nple, adding proper logging and error handling, before it can be deployed. \n Now let’s take a look how some of these steps can be simplified by using a cloud\ndata platform as a central place to develop ML models. Figure 9.7 shows how an ML\nlifecycle might look on a shared platform.\n As we mentioned before, data scientists may spend a lot of time doing work that is\nnot directly related to building and training ML models, such as figuring out how to\ningest data from a particular data source or converting all data sets to a common for-\nmat. With a cloud data platform, this part of the work is taken care of by ingestion and\ncommon data transformation steps. \nIn a data platform,\ningestion and data\ntransformation are\nexisting processes.\nData in existing cloud storage or a data warehouse is available for\ndata exploration and training/validating data, existing batch-\nprocessing frameworks can be used to train models, and multiple\npeople can collaborate on the same data sets.\nMachine learning\nprocess steps can\nbe simplified by\nusing a cloud data\nplatform as a\ncentral place to\ndevelop ML models.\nFast storage\nReal-time processing and analytics\nData scientist\nIngestion\nStreaming\ndata\nBatch\ndata\nExplore\nTrain/validate\nShare\nBatch processing and analytics\nSlow storage/direct data lake access\nOperational\nmetadata\nData\nwarehouse\nFigure 9.7\nBy using a cloud data platform as a central place to develop and test ML models, many of the time-\nconsuming tasks, such as data ingestion and cleanup, are significantly simplified.\n\n\n282\nCHAPTER 9\nData access and security\nData scientists can use existing cloud storage or a data warehouse to browse and run\nexploratory analytics without having to bring data to their own computers. When it\ncomes to the train/validate loop, data scientists can use existing archive data as large-\nscale training data sets and then use more recent incoming data as a validation data\nset. They can also choose to split data in some other way by creating a dedicated area\non the cloud storage and copying required data sets there. Cloud storage is scalable\nand cheap, which allows the data science teams to experiment and slice and dice data\nin any way they want. \n We already mentioned that a model training process is computationally expensive,\nand here data scientists have a choice to either use existing batch-processing frame-\nworks such as Apache Spark (which supports a number of ML models out of the box)\nto train their models in a scalable way or use dedicated cloud virtual machines with\nlarge amounts of memory and GPUs, because some models cannot be trained in a dis-\ntributed way. Using processing in the cloud brings scalability and elasticity, which also\nminimizes any performance impact on other users.\n Because every team member in the data science team is working on the same plat-\nform, it is now possible to not only share final results of the work, but for multiple peo-\nple to collaborate on the same data sets and the same splits of test and validation data\nsets, which significantly shortens the feedback cycle and improves productivity. Being\n“close” to real data also makes it possible to test final models on real data volumes\nbefore publishing it into production. This reduces the number of issues people run\ninto because their locally developed model was not ready for production deployment.\nIn the next section, we will look into some of the ML collaboration and productivity\ntools that cloud providers offer.\n9.4.2\nML cloud collaboration tools\nShared cloud environments, such as a data platform, are perfect places for data scien-\ntists to collaborate and experiment during the model development. All three major\ncloud vendors offer tools that make this collaboration process even simpler and, as\nyou can imagine, integrate with the rest of the cloud components.\nExercise 9.4\nWhich of the following is not a benefit of running your machine learning in a cloud\ndata platform?\n1\nMultiple people can collaborate on the same data sets, which significantly\nshortens the feedback cycle and improves productivity.\n2\nData scientists can use existing cloud storage or a data warehouse to browse\nand run exploratory analytics without having to bring data to their own\ncomputers.\n3\nCloud data platform ingestion and common data transformation steps elimi-\nnate much of the manual effort being done by data scientists.\n4\nRunning models at scale is slow, but it will work in the end.\n\n\n283\nBusiness intelligence and reporting tools\n Azure ML is a service from Microsoft that offers an end-to-end model lifecycle\nfrom exploration up to production deployment. Azure ML provides a visual editor to\ncreate models or allows data scientists to write code in one of the popular program-\nming languages like Python or R. Azure ML integrates with most of the other Azure\ndata services such as Azure Blob Storage and Azure Synapse. Azure ML can also help\nwith operationalizing your models by deploying them as a web service or batch or real-\ntime data processing pipelines. \n AWS has a service called SageMaker, which offers an integrated development envi-\nronment (IDE) for model development and training. It has support for notebooks\nwhere data scientists can run experiments and share results with their peers. Sage-\nMaker notebooks can be connected to an existing Elastic MapReduce (EMR) cluster\nand use existing cloud data platform compute capacity to run the model training and\nvalidation. \n Google Cloud offers an AI Platform that is a collection of services that can simplify\ndevelopment of ML models and their deployment into production. AI Platform sup-\nports many of the popular ML libraries and frameworks like TensorFlow and Keras.\nYou can use notebooks for collaborations and experiments, but currently there is no\nintegration of these notebooks with existing Spark clusters.    \n9.5\nBusiness intelligence and reporting tools\nBI and reporting tools accessing data via the data warehouse are the most common\nways to consume data from the data platform. These traditional analytics use cases are\noften the initial driving force for a data platform to bring together all of the organiza-\ntion’s data, while applications and machine learning uses cases tend to come later. \n9.5.1\nTraditional BI tools and cloud data platform integration\nVarious types of BI tools have been around for as long as traditional data warehousing\ntechnologies, and it is not surprising that all of the existing tools on the market today\ncan easily integrate with relational cloud data warehouses, such as AWS Redshift, Goo-\ngle BigQuery, or Azure Synapse. The relational nature (here meaning that data is\norganized into tables that can be joined together) of the underlying data warehouse is\nimportant to the design of many BI tools because these tools assume the following:\nData has a flat structure (e.g., no nested data types).\nData can be sliced and diced in many different ways; for example, to provide\nusers with a drill-down functionality where you can go from a high-level aggre-\ngated report into lower-level details for a specific metric or a calculation.\nIn the early days of cloud data platforms, many BI tools struggled with BigQuery inte-\ngration because it didn’t use a fully relational model. For example, where a BI tool\nmight expect user data and user address details to live into two separate relational\ntables, BigQuery was better optimized for one table with address data being nested\ninside the user table. Many BI tools didn’t understand how to work with nested data\ntypes and couldn’t process arrays or JSON data. \n\n\n284\nCHAPTER 9\nData access and security\n This has changed in the recent years, and many major players in the BI space, such\nas Tableau, now have full support for BigQuery. But if your organization is using a\nniche BI solution, and you are planning on adopting Google Cloud for your data plat-\nform, make sure that your reporting solution will work properly with BigQuery out of\nthe box. \n Often people ask us if they can connect their BI tools directly to the data lake layer\nin their data platform. While the answer is “yes, you can,” it is not the solution we usually\nrecommend. Many modern BI and reporting tools today support Apache Spark and its\nSQL API, so technically your BI tools can query data directly from the data lake and pro-\nvide you with visualization and data exploration capabilities. The problem with this\napproach is that users expect BI tools to be interactive and responsive. It’s a frustrating\nexperience for data analysis or business users to wait tens of seconds or minutes for a\nSpark SQL query to finish. Cloud data warehouses are much better suited to provide\nthis interactive experience, and that’s one of the reasons we advocate for data ware-\nhouses to become a standard component in the data platform design. \n9.5.2\nUsing Excel as a BI tool\nOne of the most popular data processing and analytics tools on the planet is Excel,\nwhether you like it or not. A lot of business users are very proficient with it and can\nbuild complex analysis and visualizations in it. So a natural question we get asked a lot\nis whether you can use Excel or a similar tool to connect to the data warehouse. In\nfact, you can definitely do that since Excel has support for JDBC/ODBC connectors\nand can connect to any cloud data warehouse. Also, on Google Cloud, you can use\nGoogle Sheets (an alternative to Excel) to seamlessly connect to BigQuery. \n There are a few things to keep in mind when deciding to use Excel as an analytics\ntool against the data warehouse. First of all, Excel running on user’s computers can only\nhandle a small portion of the data sizes that a BI solution running on a dedicated server\ncan deal with. If your users routinely work with data sets that have millions of rows,\nExcel might not be the best choice from a performance point of view. Another issue\nthat is generally applicable to any tool that runs on the end-user computer versus a cen-\ntralized solution is security. We will talk more about protecting data in your cloud data\nplatform in the next section, but overall, providing multiple users with direct access to\nyour data warehouse from their computers is not a recommended security practice. \n9.5.3\nBI tools that are external to the cloud provider\nSo far we have talked about a cloud data platform as a solution that you build with a\ncombination of a single cloud provider’s tools and services and open source solutions.\nWith BI and reporting tools, where you run them doesn’t matter as much. It’s com-\nmon to see BI tools that are deployed on premises to connect to the cloud data ware-\nhouses, or reporting solutions implemented on one cloud provider connect to the\nwarehouses on another provider. The adoption rate and internal experience with a\nparticular tool is much more important in this case than the technical details. One\n\n\n285\nData security\nthing to keep in mind if you are deploying a cross-cloud solution is data egress costs.\nAll cloud providers charge you for getting data out of the cloud over the network, and\nthese costs can be substantial if your data volumes are large. Usually this is not a prob-\nlem with the reporting solutions because the queries run inside the warehouse and\nonly return a small result set, but you need to analyze your use cases carefully to avoid\ncloud billing surprises. \n In addition to establishing BI solutions and various SaaS offerings, cloud vendors\nalso provide reporting and visualization tools. Azure Power BI is probably one of the\nmost popular ones, and we often see Power BI being used with data platforms\ndeployed on clouds other than Azure. AWS offers a QuickSight solution, and Google\nhas a DataStudio and Looker BI platform.\n9.6\nData security\nData security and privacy should be on top of the priority list for any organization, no\nmatter whether they have their data platform built on premises or in the cloud. The\ncloud data platform design we have discussed in this book offers a great deal of flexi-\nbility and opens up multiple ways to connect to the platform, from data warehouse to\nmachine learning tools and applications. This proliferation of different types of data\nconsumers and modular platform design that requires multiple layers to communi-\ncate with each other requires a thorough approach to data security. Security is a very\nbroad and complex topic that warrants a book of its own. In this section, we will high-\nlight some of the key items and ideas on how to secure your cloud data platform.\n Cloud providers offer a flexible way to manage access to the cloud resources. You\nneed to design a security model that is easy to manage and maintain over time. One of\nthe common issues with cloud resource security is providing ad hoc access to individ-\nual users as the need arises. This approach makes it hard to get a clear picture of who\nhas access to what at any given point in time. \n9.6.1\nUsers, groups, and roles\nAll cloud providers have a notion of users, which includes human users and applica-\ntions or other cloud services and groups. You can either provide access to the cloud\nresources to individual users or applications, or you can arrange users into logical\ngroups and provide access to resources to the group instead. For example, you can\nhave a group called “Data Platform Operators” which would have permissions to pro-\nvision new resources in the cloud and reconfigure or delete cloud resources. Then\nyou can add the members of your Data Engineering or Operations team to this group.\nYou can also have separate groups for your data scientists and data users, and those\ngroups would have more restricted permissions to only read existing data and write\ndata to a dedicated sandbox area on the cloud storage. Data science teams may also\nneed permissions to provision new compute resources for training ML models. \n It is easier to manage permissions on the group level because you will typically have\nfewer groups than you have users. If someone leaves your organization, or if a new\n\n\n286\nCHAPTER 9\nData access and security\nmember joins the team, it’s just a matter of adding them to the right group, and they\nwill get access to the cloud resources they need. This significantly simplifies permis-\nsions management.\n It is also a good idea to spend some time and understand what kind of permissions\nusers need to do their jobs. Instead of assigning a very wide set of permissions to cloud\nresources, try to follow the principle of the least privilege. This principle says that you\nonly assign a bare minimum of permissions to a given group of users that is required\nfor them to do their jobs efficiently. While it is easier to give everyone administrative\nprivileges on the cloud resources, this can potentially lead to security issues or cloud\nresource mismanagement. \n9.6.2\nCredentials and configuration management\nLeaking cloud resource credentials such as passwords or secret keys is one of the most\ncommon security problems. Make sure you use cloud-specific key management solu-\ntions to store passwords and other secrets, and never hardcode passwords in your con-\nfiguration files or applications code. Many cloud services today offer a way to\nauthenticate to one another that doesn’t require a password or a secret key. Azure\nActive Directory authentication is a good example. Use cloud-native authentication\nwhenever it’s available. Minimizing the usage of passwords and other shared secrets\nreduces the risk of exposure.\n Another common problem with data security in the cloud is accidentally opening\nup access to your cloud data storage to the public internet. Cloud providers have an\noption to configure storage that doesn’t require any type of authentication and is pub-\nlicly available, for example, to host static websites. Having the same setting acciden-\ntally applied to your data lake storage would have disastrous consequences since\nanyone on the internet will be able to access the data. Use the infrastructure-as-code\napproach to make sure you know exactly which settings are being applied to your\ncloud resources, and audit them periodically. Cloud vendors also offer a notion of a\npolicy that allows you to describe which resources and which configurations are\nallowed for your organization. For example, you can create a policy that prohibits\npublicly available cloud storage in your organization.\n9.6.3\nData encryption\nAll cloud vendors encrypt all the data that is stored in the cloud storage or in cloud\ndata warehouses when it is actually saved into the disks or SSDs in the vendor’s data\ncenters. This encryption, while important, only prevents you from scenarios where\nsomeone got access to a physical infrastructure of your cloud vendor. This is not\nimpossible, but it is a highly unlikely event. In addition to the cloud vendor-provided\nencryption, you may want to encrypt specific sensitive data, such as personally identifi-\nable information, in your data platform. Encrypting specific columns in your data sets\nand then making sure that only a specific group of users have access to the decryption\nkeys is a good way to limit data exposure.\n\n\n287\nData security\n9.6.4\nNetwork boundaries\nMany cloud services, especially the PaaS solutions, by default are accessible from any\ncomputer on the internet. Accessible here means that they have a public IP address,\nbut it doesn’t mean anyone can actually access them without having a password or\nproviding any other authentication method. You need to work closely with your net-\nwork and security team to understand where your data consumers are located and\nwhich networks they will use to access the data platform. This way you can restrict\naccess to your cloud resources on the network level, which, in addition to other best\npractices covered in this section, will significantly reduce the probability of a security\nincident on your data platform.   \nSummary\nThe primary reason for developing a data platform is to cost effectively and\nsecurely make data available to data consumers—at scale. \nData consumers can be human users running reports or SQL queries against a\ndata warehouse or accessing raw data files for model development. They can\nalso be applications.\nEach public cloud provider offers a data warehouse as a service. Azure has Syn-\napse, Google Cloud has BigQuery, and Amazon has Redshift. They are similar\nin functionality, but there are some differences to be considered.\nWhile data warehouses remain the primary way to access data for BI tools and\ndirect SQL access, applications usually need a much faster response time than\neven modern cloud-based data warehouses can provide. \nUsing a dedicated data store for applications will provide faster data retrieval\ntimes and let you handle concurrent requests, but you should be aware of the\nsecurity risks associated with providing applications access to the data warehouse.\nThe proliferation of different types of data consumers and modular platform\ndesign that needs multiple layers to communicate with each other requires a\nthorough approach to data security. At a minimum, consider the use of users,\ngroups, and roles; the use of credentials and configuration management; data\nencryption; and using network boundaries to restrict access to cloud resources.\nApplication data store options include relational databases, key/value data\nstores, full-text search systems, and in-memory caches. Each cloud vendor offers\nthese as managed services.\nData scientists access and use data differently than do report users or applica-\ntions. They need access to large volumes of data, significant compute capacity\nto run their models, and a simple way to split data into training and validation\ndata sets. Luckily, a well-designed data platform stores all data, including raw\ndata and preprocessed data, and offers multiple different ways to access the\ndata: SQL, Apache Spark, direct file access, etc., to avoid limiting the data sci-\nence team in the selection of tools.\n\n\n288\nCHAPTER 9\nData access and security\nAll three major cloud vendors offer tools that make collaboration among data\nscientists simpler. Azure provides ML, while AWS and Google Cloud offer Sage-\nMaker and AI Platform, respectively. They also offer BI solutions for reporting\nand visualizing data in the data warehouse Azure has Power BI, AWS has Quick-\nSight, and Google Cloud has Data Studio and the Looker BI platform.\n9.7\nExercise Answers\nExercise 9.1:\n 2—They are likely to want different ways to access data. \nExercise 9.2:\n 3—BigQuery.\nExercise 9.3:\n 4—All of the above.\nExercise 9.4:\n 4—Running models at scale is slow, but it will work in the end.\n\n\n289\nFueling business value\n with data platforms\nWe wanted to wrap up this book with a chapter on how your carefully designed\ncloud data platform will be used in a typical enterprise, and in doing so point out\nwhy we recommended some of the things we did. We’ll also talk about some of the\nnon-technical parts of an analytics project because a data platform doesn’t exist in a\nvacuum; it is only a single ingredient of an organization’s quest to become truly\ndata driven and enabled. Knowing how your platform delivers business value in the\nshort and long term is important because it can impact design, and understanding\nwhat factors other than technology can influence use of a data platform can help\nThis chapter covers\nUnderstanding how the data platform contributes \nto business value\nDeveloping a data strategy for your organization\nAssessing the analytics maturity of your \norganization\nAnticipating and responding to potential “data-\nplatform stoppers”\n\n\n290\nCHAPTER 10\nFueling business value with data platforms\nyou ensure that your platform becomes an integral and valued part of your organiza-\ntion’s assets. \nNOTE\nWe want to thank Pythian (www.pythian.com) for the use of the dia-\ngrams and frameworks used in this chapter.\n10.1\nWhy you need a data strategy\nLet’s start with what’s driving the need for a cloud data platform. At the highest level,\nthe business drivers that lead to the need for a data platform are most often a desire to\n(1) gain operational efficiencies, (2) grow revenues, (3) improve the customer experi-\nence, (4) drive innovation, and (5) improve compliance—or some combination of all\nfive. It is incredibly important that an organization understand what they are driving\ntoward as a business, because only then can you develop a data strategy that is focused\non producing business outcomes. At the end of the day, your data platform will be\njudged by how well it contributes to these business outcomes. \n Before you can leap from desired business outcomes to a cloud data platform, you\nneed a data strategy to guide you. Like many terms in our industry, data strategy can\nmean different things to different people, so while there is no commonly accepted\ndefinition of a data strategy, here are a few for consideration:\nAn approach that “aligns and prioritizes data and analytics activities with key\norganizational priorities, goals and objectives.” (Micheline Casey, CDO LLC)\n“A coherent strategy for organizing, governing, analyzing, and deploying an\norganization’s information assets that can be applied across industries and lev-\nels of data maturity.” (DalleMule and Davenport, Harvard Business Review)\n“Intentional action & prioritization plan to harness and integrate data, to create\nand disseminate information/intelligence, to advance a business mission.”\n(Braden J. Hosch, Stony Brook University)\nAs shown in figure 10.1, the purpose of a data strategy is to connect the dots between\nan organization’s data and its business goals, establishing how data and analytics can\nsupport the organization’s business goals and priorities. \nCommon\nbusiness\ngoals\nA data strategy\nconnects data to\nthe business goals.\nGrow\nrevenues\nGain\noperational\nefficiencies\nImprove\ncustomer\nexperiences\nDrive\ninnovation\nImprove\ncompliance\nData strategy\nData\nFigure 10.1\nThe data strategy outlines how data can serve the organization’s business goals.\n\n\n291\nThe analytics maturity journey\nHere are a few examples of high-level data strategy statements. Of course, a full data\nstrategy includes all of the ways you will use data and technology and change processes\nand engage people to realize the vision, but that would be far beyond the scope of this\nchapter:\nIf an organization wants to empower innovation across the organization, one\nstrategy to do this might be to make self-service analytics an accepted part of the\norganization’s behavior. From this strategy comes the need for a data platform\nthat facilitates access to well-curated data for different types of users. \nIf an organization’s goal is to win more business by delivering better pricing\nproposals faster than their competitors, a data platform that is optimized for\nrapid processing of data supported by machine learning for delivery of recom-\nmended pricing options is important. \nFor a financial services or retail organization whose primary business goal is to\nimprove the customer experience to grow revenues, a data platform that is opti-\nmized to predict customer propensity to buy and deliver data about those seg-\nments to a marketing automation system would make sense. \nIn a different industry, such as with the gaming example in chapter 6, if the\nbusiness goal is to keep users engaged in a game for as long as possible to maxi-\nmize in-game purchases or ad views, a data strategy that supports this would\ninclude a data platform that is optimized to use data in real time to adjust the\ngame to suit the individual playing.\nFor a mining organization whose primary goal is to reduce operational costs, a\ndata platform can support this goal by optimizing for capturing data from\nsensors on trucks, combining it with other data, and then predicting when\nmaintenance is needed before the trucks break down miles away from a service\ncenter.\n10.2\nThe analytics maturity journey\nNobody can wave a magic wand and deliver operational efficiencies, revenue growth,\nimproved customer experiences, or innovation overnight—becoming truly data-\nenabled is a multiyear journey that starts in different places for different organizations\nor even teams within organizations. A typical analytics maturity journey often looks\nlike figure 10.2, which shows an evolution from using data to get insights on “what is\nand was” (SEE) to using data to predict what to do next (PREDICT) to using data to\ndrive other applications (DO) to using analytics to feed application development\n(CREATE). While the picture doesn’t show it, this process is almost always iterative\nwith many loops in each stage. Let’s break down each of these in more detail.\n \n \n \n \n\n\n292\nCHAPTER 10\nFueling business value with data platforms\n10.2.1 SEE: Getting insights from data\nAs shown in figure 10.3, most businesses start with a desire to “see” their business—\nusing reports and dashboards to gain insights into what is happening today compared\nto what happened in the past. \nIn a traditional data warehouse world, specialized report creators take requests from\nthe business and produce static reports that are delivered to the requestor. This often\ntake time—sometimes weeks can go by before the business user gets the report they\nrequested, and as anyone who’s ever produced a report knows all too well, the user\nthen immediately wants changes. Another drawback, of course, is that traditional data\nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nAn analytics\nmaturity\njourney is\ndefined in the\ndata strategy.\nFigure 10.2\nA typical analytics journey starts with “seeing” data and can evolve into creating \nnew products using data.\nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nA desire to get\ninsights from data\nis often the first\nstep in an analytics\nmaturity journey.\nFigure 10.3\nMost organizations start by bringing data together to allow the business to “see” the \nentirety of the business.\n",
      "page_number": 284
    },
    {
      "number": 10,
      "title": "Fueling business value with data platforms",
      "start_page": 312,
      "end_page": 337,
      "detection_method": "regex_chapter",
      "content": "293\nThe analytics maturity journey\nwarehouses are often limited in terms of the data stored in them—while financial data\nis almost always included, data from the growing number of SaaS systems is often\nexcluded. This means that the business user is limited in their analysis to the data in\nthe data warehouse, or they are forced to scrape/export data from different places\nand integrate it themselves into a spreadsheet. Getting a true view of the business\nacross multiple data sources using traditional methods is very difficult, and to many\nbusiness users, it’s an impossible task.\n In a modern data world, these insights are driven by self-service analytics, where the\nbusiness is empowered to use their tools of choice to access any and all data they need\nfor exploration—we call this Bring Your Own Analytics (or BYOA). The argument in\nfavor of BYOA is that different users have different levels of knowledge and comfort in\nusing various tools to analyze data. In some cases, a user may want to only look at a pre-\ncreated dashboard and maybe select a few parameters to drill down into the data\ncontained in the dashboard. In another case, a power user may be comfortable with a\nproduct such as Looker and may want access to a data set or data sets to do their own\nexploration and analytics. Most of these insights are created using off-the-shelf SQL-\nbased tools accessing data in the data warehouse as discussed in the last chapter, but what\nis driving BYOA is the recent introduction of a variety of off-the-shelf options and the\nfact that as people join the organization, they may come with preexisting knowledge of\nthese tools, which means they can be productively analyzing data far more quickly than\nif they have to learn new tools. A data platform that is optimized to allow different people\nto use different tools is one that maximizes the use of insights across the organization\nwhile ensuring compliance with company or government policies for data access.\n And of course, the fact that your data platform is capable of ingesting, integrating,\nand preparing data from an almost unlimited number of data sources means that the\ndata you are giving users is rich and broad and easily consumed from a single location.\nEliminating data silos and manual data integration will result in happy business users.\n10.2.2 PREDICT: Using data to predict what to do \nAs shown in figure 10.4, once business users have visibility into the present and past, they\noften look to more advanced analytics such as machine learning to look at data in a dif-\nferent way, predicting what they should do next to reach their business goals (PREDICT).\n Machine learning models are developed by accessing data in both the data ware-\nhouse and in storage (the lake). The key to effective machine learning models is\ndata—they need lots and lots of data. While cloud data warehouses are increasingly\noffering integrated machine learning features, a well-designed data platform won’t\nstore raw data in the warehouse; it will store it outside the data warehouse where it is\nprocessed and transformed into the clean aggregated data that is aligned with the\nneeds of most business users wishing to produce insights.\n This means that the data science team will want to access data in storage (or the lake\nportion of the data platform). Recognizing that different users have different data needs\nis a core tenet of good platform design—luckily, if you’ve read this book, you’ll have\ndesigned a platform that can support both insight generation and machine learning.\n\n\n294\nCHAPTER 10\nFueling business value with data platforms\nOur example mining organization, whose primary goal is to reduce operational costs,\nmight use machine learning to predict when maintenance is needed before the trucks\nbreak down miles away from a service center. Our example retail organization, whose\nprimary business goal is to improve the customer experience to grow revenues, might\nuse machine learning to predict which group of customers is most likely to respond to\na particular offer or set of offers so they can target that group, delivering the right\noffer to the right customer, which enhances the customer experience. It should be\nnoted that machine learning is only one form of advanced analytics that could apply\nto these use cases, but of these types of analytics, it is certainly the most talked about\nthese days.\n10.2.3 DO: Making your analytics actionable\nOnce data has been explored, either in the SEE or PREDICT phases, the outputs of\nthat exploration can be automatically delivered to external systems for action (DO),\nshown in figure 10.5. \n This could be a segment of customer data that a business user feels would respond\nbest to a particular marketing message that then gets delivered to a marketing automa-\ntion system, or it can be a machine learning model, such as a recommendation engine,\nwhere the output is integrated into an e-commerce system. When selective data is moved\nfrom the data platform to other systems, we call this orchestration—picture a conductor\nmaking all of the different players in an orchestra work together in harmony.\n Making data actionable, and making those actions automated, really changes the\nnature of your data platform into a mission-critical system. It’s one thing when a sys-\ntem that produces reports and insights responds slowly or is unavailable for periods of\ntime. But a system that is driving recommendations on an e-commerce platform or\ndelivering near-real-time personalized marketing offers, or that is helping optimize\nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nMachine learning\nand other advanced\nanalytics can produce\npredictions, telling\nthe business what\nthey “should” do,\nnot just what is\ntoday.\nFigure 10.4\nUsing advanced analytics can help an organization move from exploring the past and \npresent into predicting what to do in the future.\n\n\n295\nThe analytics maturity journey\nadvertising spend by deciding what segments should get which ads and when, simply\nmust be performant and available.\n10.2.4 CREATE: Going beyond analytics into products\nIt’s sometimes easy to overlook the fact that all this data you’ve collected has another\nuse not typically associated with analytics. In the CREATE stage (figure 10.6), data that\nmay have been gathered with analytics in mind is made available to developers to\ndrive the creation of new products. That same analytics data can also be a data source\nfor applications, as outlined in chapter 9. \nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nData that moves from \nthe data platform to \nanother system creates \nactionable data.\nFigure 10.5\nWhen data flows from the data platform into other systems automatically, it can drive \nfurther actions across the organization.\nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nData in the data\nplatform can also\nbe used to produce \nproducts.\nFigure 10.6\nEnterprise data platforms can also be a source of data for application development.\n\n\n296\nCHAPTER 10\nFueling business value with data platforms\nHere are a few examples of how organizations moved to create new products from\ntheir analytics data.\n A bank that started out wanting to improve their customer experience by helping\ntheir agents anticipate and react to customers likely to churn realizes that this same\ndata could be used to enhance their mobile apps. \n A security software company’s service is watching for and reacting to intrusion\nattempts before there is any impact. Their value is preventing attacks, so when they are\nsuccessful, nothing is visible to their clients. While their clients don’t experience any\nintrusions, they also can’t “see” all the prevented attacks, so the software company uses\nthe data from the prevented intrusions to create a dashboard exposing all of the\nthwarted attacks. Their customers can suddenly see the value of the service.\n A postal service is able to take data about parcel deliveries—what type of goods,\ndelivered to what neighborhoods at what frequency—and turn it into a value-added\nrevenue-generating service for other suppliers. This new product shows suppliers data\nthat helps them to understand shopping trends by neighborhood, enabling them to\nfocus their marketing efforts on more lucrative neighborhoods. \n10.3\nThe data platform: The engine that powers analytics maturity\nIn almost every customer engagement, we hear that the data our customers want to\nuse is stuck in silos, a big barrier to making it usable. In some cases with traditional\ndata warehouses, adding new data sources, especially data sources that include\nunstructured data, is challenging, and analytic response times on traditional data\nwarehouses can often be slow. In other cases, analytics are being performed on data\nfrom each silo individually, giving insights that only tell part of the story. In still other\ncases, manual extraction of data is followed by manual integration of data sets, an\nexpensive and cumbersome process that must be repeated each time updated data is\navailable. For more advanced organizations that are developing machine learning\nmodels, most of the work is being done in small data sets with little ability to scale and\nintegrate these models into a production environment.\n The foundation for every modern analytics program is a cloud data platform where\ndata of all types can be ingested and processed at any speed and delivered to users and\nsystems alike in a manageable, scalable, cost-effective way. This can be seen in figure 10.7.\n Taking time to develop a data strategy and truly understand how the business will\nbe using data will help ensure that you build a data platform that can (1) support the\nvariety of data that will be needed and (2) support the expected range of users as well.\nFollowing the guidance of this book will also ensure that your data platform is flexible\nenough to grow as your organization moves along in its maturity journey.\n It’s also important to note that this journey is not always linear—we’ve seen organi-\nzations develop data platforms exclusively to support making predictions, for exam-\nple. What is more common is that different groups in the company are at different\nstages, which means that a strong platform design is even more important as you deal\nsimultaneously with different data consumers who all have different needs.\n\n\n297\nPlatform project stoppers\nOne thing is certain—without a strong data foundation, none of this is possible.\n10.4\nPlatform project stoppers\nThe success of a data platform project goes well beyond what we’ve talked about in\nthis book. You can design the most flexible, scalable, secure platform in the world, but\nif the business doesn’t buy into its outputs, it will all be in vain. Aside from technology,\nkeys to project success include (1) the ability to deliver value quickly and iteratively,\n(2) managing the change that a data platform introduces to the organization and\nensuring that people actually use the platform, (3) delivering quality data so that users\ntrust the data they are accessing, (4) recognizing and acting on the fact that the plat-\nform impact extends beyond the platform itself—into the hands of the users them-\nselves, and (5) continuously improving the platform from the perspective of evolving\ncloud costs and keeping pace with users’ evolving needs and expectations.\n10.4.1 Time does indeed kill \nGone are the days when the CFO was happy to sign off on a major project investment\neven though everyone agreed it would take years before the organization would be  able\nto derive value from it. More and more we are seeing pressure from the business to\ndeliver value faster—almost always in months, not years. Designing and implementing\nan enterprise data platform is not a simple task, and to do it well is a complex under-\ntaking that will empower new uses of data across the enterprise. This analytics journey\nwill definitely evolve over years, not months, so reconciling the CFO’s or CEO’s need\nfor rapid “payback” against the complexity and importance of an enterprise data plat-\nform isn’t easy. We’ve seen projects die before they even get started because of this.\n In our experience, the best way to address this concern before it becomes a prob-\nlem is to develop your platform with the end goal in mind. We’ve talked at length\nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nA cloud data\nplatform is the\nfoundation for all\nstages of analytics\nmaturity.\nFigure 10.7\nAn analytics journey isn’t possible without a foundation of clean, integrated, organized \ndata.\n\n\n298\nCHAPTER 10\nFueling business value with data platforms\nabout designing for the long term—this can be coupled with implementing for the\nshort term. What we mean is that you should work with the business users to find a\ncompelling use case, one where the users can articulate what the business benefit of\ntheir use of data would be, and hopefully one that isn’t too complicated. Start with\nthat one use case, work with the business to truly understand what they need from\ndata, and then work backward to the data that is needed to answer their question or\nfuel their analysis. Ingest, transform, and model only the data they need—in our expe-\nrience, that is usually two to three data sources.\n Once the first use case is showing value, move on to the next one and the next one,\nand soon you’ll have exercised your engineering processes to the point where more\nand more use cases can be delivered in parallel. The end result is demonstrable busi-\nness value sooner, which will help you gain support for ongoing investment in your\nplatform.\n The alternative approach is to start at the data source and ingest every possible\ndata source into the platform, even before you know how or even if the data will be\nneeded. Yes, this is the proverbial “data swamp.” It takes more time before you’ll see\nthe data being used, and the effort needed to make sense of the data in the swamp will\nbe much greater,\n10.4.2 User adoption \nFor most organizations, moving from a traditional report-producing operating mode\nto one of user self-service is a major change. And change is always difficult. Most users\ndon’t like change—they are busy, they have their current methods of doing analysis\nand reporting, they hoard their data, they don’t want to take the time to learn some-\nthing new—the excuses are almost infinite.\n Grassroots adoption of new business models, such as those that a data platform\nencourages, is critical to successful adoption. If nobody uses your platform, it can’t\never be considered a success.\n There are some things you can do to help. In the previous section, we talked about\nfinding the early use cases. Inevitably, in the organization you will have the “early\nadopters” (they are the ones asking for more data), the “blockers” (they are the ones\nexpressing skepticism that this will ever work and often seeding doubt in the minds of\nothers), the “chickens” (those who are simply afraid to try anything new) and “the\navoiders” (those who don’t want anything to do with anything new).\n Converting all of these different types of people into avid users and supporters of\nyour data platform can be difficult at times, but we’ve seen a few effective techniques\nthat you might want to consider:\nMake sure the first users are both early adopters and influential employees.\nEarly adopters are most likely to collaborate with you to get results faster, and if\nyou pick a use case that has exposure to senior management, it is more likely\nthat the use case outcomes will make their way into a C-suite presentation. That\nwill help demonstrate the business value of the data platform.\n\n\n299\nPlatform project stoppers\nIf you are brave enough, shortly after helping an early adopter group, offer to\nsolve a problem being experienced by a blocker. “Why invite pain?” you might\nbe asking. If you can convert a blocker into a fan and get them to start talking\nabout how awesome their data use is, others in the company will know that even\nthe toughest critics have been silenced. And that is great internal marketing.\nThe chickens need lots of attention—lots of lots of training and coaching will be\nrequired to get this group comfortable with using data differently. Encourage\nand support this training and any centers of excellence that might be set up.\nThe avoiders will come along eventually, and it’s OK if they take a bit longer—\nby then there will be lots of “experts” in the company to help them. This last\npoint, encouraging use and expertise across the organization, is important, and\nit will be much easier if you have an executive sponsor—someone in the C-suite\nwho totally buys into the need for organizational transformation using data.\nThis can be the Chief Data Officer, the CIO, the CMO, or even the CEO, but\nwithout this kind of support, adoption will be slower. An effective use of your\nexecutive sponsor might be to create visibility for uses of data both at the C-level\nand across the company. Disney did this by creating a contest where data users\ncould show off their output to the entire company and be rewarded for their\nefforts and the value they created. This showed the company that data was\nimportant, and it also created experts and gave them rewards and additional vis-\nibility. Many of these champions went on to become data coaches, which in turn\naccelerated data and analytics maturity across the organization. \n10.4.3 User trust and the need for data governance\nThe only thing worse than not seeing anyone use your data platform is when you have\nusers but they don’t trust the data it produces.\n Anyone who has found themselves presenting data to a group of people when some-\none points out that the number(s) you are showing are simply not correct, knows that\nit’s the moment you lose the room. The minute anyone sees inaccurate data is the\nmoment that they start mistrusting all future data. From that moment onward, regain-\ning user trust must become the most important thing the extended data team works on.\n At this point, the platform team is likely to point out that they made sure that data\nmade it to the data warehouse, and it was at that point that the business started using\nit, so it’s not the platform’s fault. That may be true, but if you want your platform to be\nused for the foreseeable future, you have to extend your thinking of the data platform\nto include the final destination of your data even if you don’t have control over it. This\nis where data governance comes in. \n While data governance is a broad topic that goes beyond data quality, efforts to\nimprove data quality are an integral part of most governance programs, and measure-\nments of data quality levels are the most widely used data governance metrics.\n Examples of metrics on data quality include percentages of the correct entries in\ndata sets, required data fields that are filled in, and data values that match across\n\n\n300\nCHAPTER 10\nFueling business value with data platforms\ndifferent systems, plus other measurements of attributes such as data accuracy,\ncompleteness, consistency, and integrity. In chapter 5 we talked about how to\nimplement the rules that are the output of data quality metrics definitions, but we\ndidn’t talk about how these rules came to be. That is usually the work of a group of\npeople from across the organization, in particular, data stewards who are the business\nowners of the data. The designers of the data platform must become a part of the data\ngovernance process—understanding what the business needs from a data quality\nperspective is critical to implementing the necessary changes to the platform.\n Ensuring data quality is more than just applying business rules as data quality checks\non pipelines, it’s also what happens when the data fails the quality check. The goal in\ntaking action is to ensure that (1) the consumers of that “failed” data are aware that the\ndata product is not to be trusted until the issue can be resolved, and (2) that the people\nwho can fix the problems are aware of the need for action, as quickly as possible. And\nyes, that involves going beyond the confines of the data platform.\n10.4.4 Operating in a platform silo\nSilos are a pervasive fact of life in organizations. You know this or you wouldn’t be\nworking so hard to break down the data silos that are stopping your organization from\nfully using their data. But silos are sneaky, and sometimes we see them in the IT orga-\nnization as well.\n Think of the reach of your data platform—it extends from outside your company\nas you reach into SaaS data sources and ingest them, all the way to the dashboard that\nyou didn’t have any part in designing. It can also extend into other systems—into the\nwebsite where the output of a machine learning algorithm is embedded or into the\nmarketing automation system where a segment of users extracted from your data plat-\nform was used as an input to an email marketing campaign. Where exactly does the\ndata platform start and stop?\n It’s too easy for an IT department to take the easy way out—to say that the platform\nstarts when the data comes into the platform and ends when it’s delivered as a data\nset. But we know that this is a short-sighted approach that will lead to many of the\nother problems we’ve identified in this chapter. A data platform is a system with a long\nreach—think of it as having tentacles that reach into many other systems and into all\nparts of the organization.\n Accepting responsibility for the end-to-end system can be the role of IT, but in our\nexperience should be done with the support of the users so that data ownership can\nbe moved to the business, so that the ongoing evolution of data quality rules is driven\nby the business, so that the necessary SLAs for data pipelines are defined mutually by\nthe business and IT. While IT can take ownership of implementation and support, the\nway in which the platform must operate and be measured must be a shared responsi-\nbility. Encourage the creation of multifunctional teams who care about the full system,\nand be an active participant on these teams to share the workload and to create\nshared ownership of a continuously evolving and successful platform. \n\n\n301\nPlatform project stoppers\n10.4.5 The dollar dance\nA second potential platform project stopper is cost. Understanding the dollar dance is\ncritical to project success. Here’s what we see happening over and over again. The\norganization (rightly) gets sold on the potential of cloud, not just for its agility and\navailability of tools and its opportunity to reduce support costs but also on the promise\nof eliminating large capital expenditures for hardware and especially on the promise\nof only having to pay as the system is used. The data platform project is approved, the\ndesign is done to meet all the requirements, it’s implemented well, and because\nyou’ve also implemented a strong data governance practice, data quality is high and\nusers are consuming data and driving better business outcomes. What could possibly\nbe wrong with that scenario, you ask?\n What happens next, usually within a few months of the solution being in use, is\nthat the CFO starts taking a closer look at the cloud consumption invoices, which\n(understandably) are growing as more data is being ingested, stored, and used. Inevi-\ntably, the CFO will come to the IT team to ask what can be done to slow the increase.\nGetting ahead of this request is important. From an operating perspective, investing\nin FinOps (the ongoing analysis and optimization of cloud consumption costs) is\nimportant. If you can understand the trends and take action to optimize costs before\nthey become excessive, your CFO will thank you, and one can argue that by avoiding\nunnecessary costs, you are creating a form of job security as more money will be avail-\nable for ongoing development. \n The dollar dance has two sides—the top side of the dollar dance is the value the\nplatform brings to the business. The more that can be quantified the better. Perhaps\nyou can encourage the business to share the benefits—how much more revenue came\nin because they were able to do more targeted marketing, or how much was saved\nwhen the algorithm running against all the data in your platform reduced machine\nmaintenance costs by predicting failures in advance. The bottom side of the dollar\ndance is the cost side—how to make sure that you aren’t spending anything that\ndoesn’t have to be spent. While FinOps is often not the responsibility of the data plat-\nform designer, there are times when changes to the platform design can realize big\ncost savings.\n In some cases, there isn’t much to be done—the design may be optimal, users may\nbe consuming data efficiently, and it may even be possible to show how the value that\nis being delivered to the business far exceeds the cloud costs. In other cases, there may\nin fact be something that can be done—but not without a good understanding of how\ncloud consumption costs work coupled with an understanding of what the design\ntrade-offs might be.\n Let’s consider a real-world example. A large telecommunications company built a\ndata platform on Google to capture IoT data from their systems. It was a lot of data\nspanning many years. Best practices suggest that the processing of this data should be\ndone outside of the data warehouse, in this case using Spark. There were many good\nreasons for this design decision, as we discussed in chapter 2, including flexibility,\n\n\n302\nCHAPTER 10\nFueling business value with data platforms\ndeveloper productivity, data governance, cross-platform portability, performance, and\nspeed. However, it turned out that the design team was unaware that the business had\na plan with Google that gave them unlimited use of BigQuery for a fixed fee per\nmonth. Had this not been the case, doing the processing outside the data warehouse\nwould have been the absolute best decision, but it turned out that by moving the pro-\ncessing into BigQuery, the company could save more than $600K per year. So while it\nmight not have been the perfect design change, the savings were so significant that it\nwas considered to be the right decision.\n Our point with these stories about possible platform stoppers is that the data plat-\nform doesn’t exist in a vacuum—it is a critical part of a much larger ecosystem. The\nneed for platform designers to work with the business can’t be underestimated as the\nmore its designers are connected to the business, the more likely it is that the platform\nproject will be a success—not just technically but also business-wise.\nSummary\nThe business outcomes often tied to a data platform can include a desire to (1)\ngain operational efficiencies, (2) grow revenues, (3) improve the customer\nexperience, (4) drive innovation, and (5) improve compliance—or some com-\nbination of all five. \nThe purpose of a data strategy is to connect the dots between an organization’s\ndata and its business goals, establishing how data and analytics can support the\norganization’s business goals and priorities. \nA typical analytics maturity journey shows an evolution from using data to get\ninsights on “what is and was” (SEE) to using data to predict what to do next\n(PREDICT) to using data to drive other applications (DO) to using analytics to\nfeed application development (CREATE). It is defined in the data strategy. A\ncloud data platform is the foundation for all stages of analytics maturity.\nA desire to get insights from data is often the first step in an analytics maturity\njourney. These insights are driven by self-service analytics, where the business is\nempowered to use their tools of choice to access any and all data they need for\nexploration—we call this Bring Your Own Analytics (or BYOA).\nOnce business users have visibility into the present and past, they often look to\nmore advanced analytics such as machine learning to look at data in a different\nway, predicting what they should do next to reach their business goals.\nData that moves from the data platform to another system creates actionable\ndata and converts your platform into a mission-critical system. Data in the data\nplatform can also be used to produce new products.\nThe success of a data platform project requires not just a good design, but also\nhaving the business buy into its outputs. Aside from technology, keys to project\nsuccess include (1) the ability to deliver value quickly and iteratively, (2) manag-\ning the change that a data platform introduces to the organization and ensur-\ning that people actually use the platform, (3) delivering quality data so that\n\n\n303\nPlatform project stoppers\nusers trust the data they are accessing, (4) recognizing and acting on the fact\nthat the platform impact extends beyond the platform itself—into the hands of\nthe users themselves, and (5) continuously improving the platform from the\nperspective of ongoing cloud costs.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n305\nindex\nA\naccess and security property 135\naccessing data 33–288\nbusiness intelligence (BI) tools 283–285\nexternal cloud provider tools 284–285\ntraditional tools and cloud data-platform \nintegration 283–284\nusing Excel as 284\ncloud data warehouses 263–274\nAWS Redshift 264–268\nAzure Synapse 268–270\nchoosing right data warehouse 273–274\nGoogle BigQuery 270–273\nfor applications 274–278\ncloud key/value data stores 276–277\ncloud relational databases 275–276\nfull text search services 277–278\nin-memory cache 278\nmachine learning (ML) on data platform\n278–283\ncloud collaboration tools 282–283\nmodel lifecycle on cloud data platform\n279–282\ntypes of data consumers 262–263\nActivity ID attribute 212\nALL value 265\nAmazon Web Services see AWS (Amazon Web \nServices)\nanalytics maturity 291–296\ndata platform powering 296–297\ngetting insights from data 292–293\ngoing beyond analytics into products 295–296\nmaking analytics actionable 294–295\nusing data to predict what to do 293–294\nApache Atlas 223\nApache Avro schemas 243–245\nApache Cassandra database 104\nApache Spark, schema inference in 237–241\nAPI, ingesting data via 82\napplication data access 274–278\ncloud key/value data stores 276–277\ncloud relational databases 275–276\nfull text search services 277–278\nin-memory cache 278\narchive area 134\narchive container 138\nAUTO value 265\nautomation 80\nautomation maturity property 58\nAvro file format 140–143\nAWS (Amazon Web Services)\nmapping layers to 62–66\nbatch data ingestion 62–63\nbatch data processing 64\ncloud warehouse 64\ndata consumers 65–66\ndata platform storage 63–64\ndirect data platform access 65\nETL overlay and metadata repository 65\norchestration layer 65\nreal-time data processing and analytics 64\nstreaming data ingestion 63\nreal-time processing services 190–192\nAWS Glue Data Catalog 221\nAWS Redshift 264–268\nAzure\ningesting data into 26\nmapping layers to 70–74\nbatch data ingestion 71\nbatch data processing 72\ncloud data warehouse 72\n\n\nINDEX\n306\nAzure (continued)\ndata consumers 74\ndata platform storage 72\ndirect data platform access 72\nETL overlay and metadata repository 73\norchestration layer 73\nreal-time data processing and analytics 72\nstreaming data ingestion 71\nreal-time processing services 193–194\nAzure Data Catalog 221\nAzure Stream Analytics 194\nAzure Synapse\naccessing data, cloud data warehouses 268–270\ningesting data from data platforms 25–26\nB\nbackward-compatible schema 233\nbatch data\ningestion\nAWS 62–63\nAzure 71\ncombining real-time data and 188–190\ndifferences between streaming ingestion \nand 117–119\nGoogle Cloud 67\nopen source and commercial alternatives\n74–75\nprocessing\nAWS 64\nAzure 72\nGoogle Cloud 68\nBatchId 136\nBI (business intelligence) tools 283–285\nexternal cloud provider tools 284–285\ntraditional tools and cloud data-platform \nintegration 283–284\nusing Excel as 284\nBring Your Own Analytics (BYOA) 293\nbusiness metadata 198–199\nbusiness value and data platforms 303\nanalytics maturity 291–296\ndata platform powering 296–297\ngetting insights from data 292\ngoing beyond analytics into products 295–296\nmaking analytics actionable 294–295\nusing data to predict what to do 293–294\ndata strategy 290–291\nplatform project stoppers 297, 301–302\ncost 302\noperating in platform silos 300\ntime 297–298\nuser adoption 298–299\nuser trust and need for data governance\n299–300\nBYOA (Bring Your Own Analytics) 293\nBytes Read attribute 212\nBytes Written attribute 212\nC\nCassandra database 104\nCassandra Query Language (CQL) 104\nCDC (change data capture) 165, 174\nRDBMS ingestion flow 94–98\nvendors 98–100\nMicrosoft (MS) SQL SERVER 99–100\nMySQL 99\nOracle 98–99\nPostgreSQL 100\nCI/CD (continuous integration/continuous \ndeployment) 73, 216, 234\nCloud architecture fit property 58\ncloud costs considerations 34–35\ncloud data platforms 49, 77\narchitecture example 23–24\nemergence of 9–10\nlayered architecture 10–14, 38–58\nETL tools overlay 56–58\nfast and slow storage layer 44–46\nimportance of 59–60\ningestion layer 10–11, 40–43\norchestration layer 53–56\nprocessing layer 12–13, 46–47\nserving layer 13–14, 52\nstorage layer 11–12\ntechnical metadata layer 47–49\nmachine learning (ML)\ncollaboration tools 282–283\nmodel lifecycle 279–282\nmapping layers to tools 60–70, 74\nAWS 62–66\nAzure 70–74\nGoogle Cloud 66\nopen source and commercial alternatives 74–75\nbatch data ingestion 74–75\norchestration layer 75\nvariety 14–15\nvelocity 15–16\nvolume 15\ncloud data warehouses\naccessing data 263–274\nAWS Redshift 264–268\nAzure Synapse 268–270\nchoosing right data warehouse 273–274\nGoogle BigQuery 270–273\narchitecture example 22–23\nAWS 64\nAzure 72\n\n\nINDEX\n307\ncloud data warehouses (continued)\ndata platforms vs. 19–24\ncloud data platform architecture \nexample 23–24\ncloud data warehouse-only architecture \nexample 22–23\ndata sources 20–22\nGoogle Cloud 69\nschema management features of 257–258\ncloud data-platform integration 283–284\nCloud Dataflow 193\ncloud key/value data stores 276–277\ncloud metadata services 221–222\nCloud Pub/Sub storage service 192\ncloud relational databases 275–276\ncloud services for real-time data processing and \nanalytics 190–194\nAWS 190–192\nAzure 193–194\nGoogle Cloud 192–193\ncloud storage organization 132–139\ncommercial alternatives 74–75\nbatch data ingestion 74–75\norchestration layer 75\nstreaming data ingestion and real time \nanalytics 75\ncompatibility rules 249–251\nconfiguration files, metadata layer as 214–217\nconfiguration management 286\nConnectivity Details attribute 207\nconsumers 168\ncontainer storage tier 135\ncontainers 134–139\nfolder naming conventions 135–138\norganizing streaming data 139\ncontinuous integration/continuous deployment \n(CI/CD) 73, 216, 234\ncontract, schema as 233–235\n_corrupt_record field 240\ncost 74\nas platform project stoppers 301–302\ncloud costs considerations 34–35\ncost efficient property 46\nCQL (Cassandra Query Language) 104\nCreated Timestamp attribute 205–208, 210, 248\ncredentials, data security and 286\nD\ndata\ngetting insights from 292–293\npredicting what to do with 293–294\ndata access 268, 288\nbusiness intelligence (BI) tools 283–285\nexternal cloud provider tools 284–285\ntraditional tools and cloud data-platform \nintegration 283–284\nusing Excel as 284\ncloud data warehouses 263–274\nAWS Redshift 264–268\nAzure Synapse 270\nchoosing right data warehouse 273–274\nGoogle BigQuery 270–273\nfor applications 274–278\ncloud key/value data stores 276–277\ncloud relational databases 275–276\nfull text search services 277–278\nin-memory cache 278\nmachine learning (ML) on data platform\n278–283\ncloud collaboration tools 282–283\nmodel lifecycle on cloud data platform 279–282\ntypes of data consumers 262–263\ndata consumers\nAWS 65–66\nAzure 74\nGoogle Cloud 70\nserving layer and 49–52\ntypes of 262–263\ndata encryption 286\ndata governance 299–300\nData Hub 224\ndata ingestion 40–43, 75\ndata lakes 6–7\ndata platform storage\nAWS 63–64\nAzure 72\nGoogle Cloud 68\ndata platforms 17, 36\naccessing data 33–34\nchange from data warehouses to 2–3\ncloud costs considerations 34–35\ncloud data platforms 15\nbuilding blocks of 10–14\nemergence of 9–10\nvariety 14–15\nvelocity 15–16\nvolume 15\ncloud data warehouses vs. 19–24\ncloud data platform architecture \nexample 23–24\ncloud data warehouse-only architecture \nexample 22–23\ndata sources 20–22\ndata lakes 7\ndata warehouses struggling with 3\nall V's at once 6\nvariety 4\nvelocity 5\nvolume 5\n\n\nINDEX\n308\ndata platforms (continued)\nfueling business value with 303\nanalytics maturity 291–297\ndata strategy 290–291\nplatform project stoppers 297–302\ningesting data 24–28\ndirectly into Azure Synapse 25–26\ninto Azure data platform 26\nmanaging changes in upstream data \nsources 26–28\nmachine learning (ML) on 278–283\ncloud collaboration tools 282–283\nmodel lifecycle on cloud data platform\n279–282\nprocessing data 28–32\nin data platform 31–32\nin warehouse 29–31\npublic cloud 7–9\nschema management in 235–241\nreal-time pipelines 241\nschema inference in Apache Spark 237–241\nschema management module 236–237\nData Quality Check IDs attribute 206–208\ndata quality checks 150–152\nin real-time 187–188\nmetadata domains 208–210\nData Quality Checks domain 204\ndata security 285–287\ncredentials and configuration \nmanagement 286\ndata encryption 286\nnetwork boundaries 287\nusers, groups, and roles 285–286\ndata source name 136\ndata sources 20–22, 79–83\nfiles 81–82\nrelational databases (RDBMS) 80\nSaaS data via API 82\nstreams 82–83\ndata strategy 290–291\ndata transformations in real time 178–190\ncombining batch and real-time data 188–190\nconverting message formats in real-time \npipelines 186–187\ndeduplicating data in real-time systems\ncauses of 178–181\nsolutions to 181–186\nreal-time data quality checks 187–188\ndata type conversion 100–102\nData Warehouse Units (DWU) 269\ndata warehouses\nchange to data platforms 2–3\nschema changes in architecture 230\nschema evolution and 255–258\nstruggling with data platforms 3–6\nall V's at once 6\nvariety 4\nvelocity 5\nvolume 5\ndata-transformation pipelines, schema evolution \nscenarios and 251–255\ndeduplicating data 145–150\nin real-time systems\ncauses of 178–181\nsolutions to 181–186\nin Spark 147–150\ndependency graph 55\nDescription attribute 205–206\nDestination object 207\nDestinations attribute 206\ndirect data platform access\nAWS 65\nAzure 72–73\nGoogle Cloud 69\nDISTKEY property 265\nDISTSTYLE property 265\ndomains, metadata 204–208, 213\ndropDuplicates function 148\nDSL (domain-specific language) 210\nduration of ingestion 105, 113\nDWU (Data Warehouse Units) 269\nE\nelastic cost model property 50\nelastic resources 8\nEMR (Elastic MapReduce) 64, 129, 283\nERP (enterprise resource planning) 53, 188\nError Message attribute 212\nETL (extract, transform, load) 4, 22\nETL overlay 56–58\nAWS 65\nAzure 73\nGoogle Cloud 69–70\nEVEN value 265\nEvent Hubs 193\nExcel 284\nextendable property 49\nextensibility property 58\nexternal cloud provider tools 284–285\nExtra attribute 212\nextract, transform, load (ETL) 4, 22\nF\nfailed area 134\nfailed container 138\nfailed storage area 134\n\n\nINDEX\n309\nfast storage\nanatomy of 167–170\nlayer 44–46\nscalability 170–172\nfile format conversion 140–145\nAvro and Parquet file formats 140–143\nusing Spark 143–145\nFile Transfer Protocol (FTP) 107\nfiles 81–82, 107–113\ncapturing metadata 112–113\ntracking data ingestion 109–112\nfolders 134–139\nnaming conventions 135–138\norganizing streaming data 139\nfriends field 238\nFTP (File Transfer Protocol) 107\nfull table ingestion 85–91\nfull text search services 277–278\nG\ngame events 162\nGoogle BigQuery 270–273\nGoogle Cloud\nmapping layers to 66–70\nbatch data ingestion 67\nbatch data processing 68\ncloud warehouse 69\ndata consumers 70\ndata platform storage 68\ndirect data platform access 69\nETL overlay and metadata repository 69–70\norchestration layer 70\nreal-time data processing and analytics 68–69\nstreaming data ingestion 67–68\nreal-time processing services 192–193\nGoogle Cloud Data Catalog 222\ngroups, data security and 285–286\nH\nHASH option 269\nHDFS (Hadoop Distributed File System) 11, 231\nhigh availability property 43, 56\nhighly available property 48\nI\nIaaS (infrastructure as a service) 62\nID attribute 205–207, 210, 247\nin-memory cache 278\nincremental table ingestion 91–94\ninfrastructure as a service (IaaS) 62\ningesting data 84, 91–126\ndata sources 79–83\nfiles 81–82\nrelational databases (RDBMS) 80\nSaaS data via API 82\nstreams 82–83\nfrom data platforms 24–28\ndirectly into Azure Synapse 25–26\ninto Azure data platform 26\nmanaging changes in upstream data \nsources 26–28\nfrom files 107–113\ncapturing metadata 112–113\ntracking 109–112\nfrom relational databases (RDBMS) 83–107\ncapturing metadata for ingestion \npipelines 104–107\nchange data capture (CDC) 94–100\ndata type conversion 100–102\nfrom NoSQL databases 103–104\nfull table ingestion 86–91\nincremental table ingestion 94\nusing SQL interface 85\nfrom SaaS applications 120–124\nfrom streams 114, 119–120\ncapturing metadata 120\ndifferences between batch and streaming \ningestion 117–119\nnetwork and security considerations for 123–124\ningestion layer 10–11, 39\nintegrations property 58\nK\nKEY value 265\nKinesis Data Analytics 190\nKinesis Data Streams 190\nL\nlanding area 133\nlanding container 136\nLast Modified Timestamp attribute\n205, 207–208, 210\nLast Updated Timestamp attribute 248\nLAST_MODIFIED timestamp 92\nlayered architecture 38–40, 58\ndata ingestion layer 43\nETL tools overlay 56–58\nfast and slow storage layer 44–46\nimportance of 59–60\nmapping to tools 60–74\nAWS 62–66\nAzure 70–74\nGoogle Cloud 66–70\n\n\nINDEX\n310\nlayered architecture (continued)\nmetadata implementation options 213–220\nconfiguration files 214–217\nmetadata API 218–220\nmetadata database 217–218\norchestration layer 53–56\nprocessing data as separate layer 129–131\nprocessing layer 46–47\nserving layer and data consumers 49–52\ntechnical metadata layer 47–49\nM\nmaintainable property 56\nManaged Streaming for Apache Kafka (MSK) 63\nmapping layers 68\nto AWS 62–66\nbatch data ingestion 62–63\nbatch data processing 64\ncloud warehouse 64\ndata consumers 65–66\ndata platform storage 63–64\ndirect data platform access 65\nETL overlay and metadata repository 65\norchestration layer 65\nreal-time data processing and analytics 64\nstreaming data ingestion 63\nto Azure 70–74\nbatch data ingestion 71\nbatch data processing 72\ncloud data warehouse 72\ndata consumers 74\ndata platform storage 72\ndirect data platform access 72–73\nETL overlay and metadata repository 73\norchestration layer 73\nreal-time data processing and analytics 72\nstreaming data ingestion 71\nto Google Cloud 66–70\nbatch data ingestion 67\nbatch data processing 68\ncloud warehouse 69\ndata consumers 70\ndata platform storage 68\ndirect data platform access 69\nETL overlay and metadata repository 69–70\norchestration layer 70\nreal-time data processing and analytics 69\nstreaming data ingestion 67–68\nmassively parallel processing (MPP) 64\nMDM (master data management) tools 145\nmessage formats, converting in real-time \npipelines 186–187\nmetadata 227\ncapturing from file ingestion 112–113\ncapturing from streaming pipeline 119–120\nexisting solutions 220–225\nCloud metadata services 221–222\nopen source metadata layer \nimplementations 223–225\nlayer implementation options 213–220\nconfiguration files 214–217\nmetadata API 218–220\nmetadata database 217–218\nmetadata domains 204–213\ndata quality checks 208–210\npipeline activity metadata 210–213\npipeline metadata 204–207\nmetadata model 213\ntypes of 198–199\nbusiness metadata 198–199\ndata platform internal metadata (pipeline \nmetadata) 199–203\nmetadata API 218–220\nmetadata database 217–218\nmetadata layer 39, 246–248\nmetadata repository\nAWS 65\nAzure 73\nGoogle Cloud 69–70\nMicrosoft (MS) SQL SERVER 99–100\nML (machine learning) 278–283\ncloud collaboration tools 282–283\nmodel lifecycle on cloud data platform 279–282\nmodel, metadata 203–213\nMongoDB database 103\nMPP (massively parallel processing) 64\nMS (Microsoft) SQL SERVER 99–100\nMSK (Managed Streaming for Apache Kafka) 63\nMySQL 99\nN\nName attribute 205–207, 210\nNamespaces 136, 204\nnaming conventions 135–138\nnetworks 123–124\nboundries and security 287\nconnecting to cloud data platform 123–124\nNoOps property 50\nNoSQL databases\ncapturing metadata for ingestion pipelines\n104–107\nRDBMS ingesting data from 103–104\nO\nobservability property 43\nonline gaming use case 161–164\n\n\nINDEX\n311\nopen source solutions 74–75\nmetadata layer implementations 223–225\nstreaming data ingestion and real time \nanalytics 75\nOracle 98–99\norchestration layer 53–56\nAWS 65\nAzure 73\nGoogle Cloud 70\nopen source and commercial alternatives 75\norganizing data\ncloud storage 132–139\nfor real-time use 167–177\nanatomy of fast storage 167–170\nfast storage scalability 170–172\nreal-time storage 172–177\noverfitting 281\noverlay layer 39\nP\nPaaS (platform as a service) 8, 14, 22, 60\nParquet file format 140–143\npass-through jobs 134\nperformant property 46\nPipeline Activity domain 204\npipeline activity metadata 210–213\npipeline configuration metadata 210\nPipeline ID attribute 212\npipeline metadata (data platform internal \nmetadata)\nas metadata domain 204–207\ndefined 199\ntaking advantage of 199–203\nPipeline metadata object 205\npipeline name 136\npipelines\nconfigurable for processing data 152–154\nconverting message formats in real-time\n186–187\nplatform as a service (PaaS) 8, 14, 22, 60\nplatform project stoppers 297–302\ncost 301–302\noperating in platform silos 300\ntime 297–298\nuser adoption 298–299\nuser trust and need for data governance\n299–300\nplatform silos 300\npluggable architecture property 43\nPostgreSQL 100\nprinciple of least privilege 286\nprocessing data 28–143, 155\nas separate layer in data platform 129–131\nconfigurable pipelines 152–154\ndata deduplication 145–150\ndata quality checks 150–152\nfile format conversion 140–145\nAvro and Parquet file formats 140–143\nusing Spark to convert file formats 145\nin data platform 31–32\nin warehouse 29–31\nstages 131–132\nprocessing layer 12–13, 46–47\nproducers 168\nproduction area 134\nproduction container 138\nproducts 295–296\nProtobuf (Protocol Buffers) 81\npublic cloud 7–9\nR\nRDBMS (relational databases) 80, 83, 100\ncapturing metadata for ingestion pipelines\n104–107\nchange data capture (CDC) 94–100\ndata type conversion 102\nfrom NoSQL databases 103–104\nfull table ingestion 86–91\nincremental table ingestion 91–94\nusing SQL interface 84–85\nRDDs (Resilient Distributed Datasets) 28\nread function 240\nreal-time data processing and analytics 196\nAWS 64\nAzure 72\ncloud services for 190–194\nAWS 190–192\nAzure 193–194\nGoogle Cloud 192–193\ncommon data transformations in real time\n178–190\ncombining batch and real-time data 188–190\nGoogle Cloud 68–69\norganizing data for real-time use\nanatomy of fast storage 167–170\nfast storage scalability 170–172\nreal-time storage 172–177\nreal-time ingestion vs. 157–164\nknowing when to use 164–167\nonline gaming use case 161–164\nprocessing summary 164\nretail use case 160–161\nwith open source solutions 75\nreal-time ingestion 114, 157–164\nknowing when to use 164–167\nonline gaming use case 161–164\nprocessing summary 164\nretail use case 160–161\n\n\nINDEX\n312\nreal-time pipelines 241\nreal-time storage 172–177\nrecord type 244\nrelational databases see RDBMS (relational \ndatabases)\nreliable property 45, 50\nREPLICATE option 269\nretail use case 160–161\nroles, data security and 285–286\nROUND ROBIN option 269\nrows ingested per table 105\nRows Read attribute 212\nRows Written attribute 212\nRule attribute 210\nS\nSaaS applications 2, 60\ningesting data from 120–124\ningesting data via API 82\nsampleSize option 240\nscalable property 43, 45, 48, 50\nscale property 56\nSchema attribute 248\nschema evolution\ndata warehouses and 255–258\nscenarios 248–255\ncompatibility rules 249–251\ndata-transformation pipelines and 251–255\nSchema ID attribute 207\nschema inference 237\nschema management 260\napproaches to 232–243\nin data platforms 235–241\nmonitoring schema changes 241–243\nschema as contract 233–235\nreasons for 229–232\nschema changes in traditional data warehouse \narchitecture 230\nschema-on-read approach 231–232\nschema evolution 248–255\ncompatibility rules 249–251\ndata warehouses and 255–258\ndata-transformation pipelines and 251–255\nSchema Registry implementation 243–248\nApache Avro schemas 243–245\nas part of metadata layer 246–248\nexisting implementations 245–246\nschema management module 236–237\nSchema Registry implementation 243–248\nApache Avro schemas 243–245\nas part of metadata layer 246–248\nexisting implementations 245–246\nschema-on-read approach 231–232\nsecurity 123–124, 285–287\nconnecting networks to cloud data \nplatform 123–124\ncredentials and configuration \nmanagement 286\ndata encryption 286\nnetwork boundaries 287\nusers, groups, and roles 285–286\nService Level Agreement (SLA) monitoring 105\nserving layer 39\ndata consumers and 49–52\nof cloud data platforms 13–14\nSeverity attribute 210\nSFTP (standard File Transfer Protocol) 107\nsilos, platform 300\nSLA (service level agreement) monitoring 105\nslow storage layer 44–46\nSORT KEY 266\nSource 207\nsource system name 113\nSources attribute 206\nSpark\ndata deduplication 147–150\nfile format conversion 143–145\nSQL interface 84–85\nstages, processing data 131–132\nstaging area 133\nstaging container 137\nstandard File Transfer Protocol (SFTP) 107\nStart Time attribute 212\nStatus attribute 212\nStop Time attribute 212\nstorage layer 11–12, 39\nstore_id 189\nstreaming data ingestion 82–83, 114–120\nAWS 63\nAzure 71\ncapturing metadata 119–120\ndifferences between batch and 117–119\nGoogle Cloud 67–68\norganizing data 139\nwith open source solutions 75\nstructured logging 216\nT\ntags field 238\ntechnical metadata layer 47–49\ntime 297–298\ntoAvroType method 244\ntopics, messages organized as 168\ntracking ingested files 109–112\ntransaction_amount 230\n\n\nINDEX\n313\ntransaction_date 230\ntransaction_total 230\ntransparency property 56\nType attribute 206–208\nU\nUDFs (user defined functions) 31\nULID (Universally Unique Lexicographically \nSortable Identifier) 136\nupstream data sources 26–28\nurl attribute 30\nusers 286\nadoption of users 298–299\ndata security 285\ntrust 299–300\nUUID (Universally Unique Identifier) 136\nV\nvariety\ncloud data platforms 14–15\ndata warehouses 4\nvelocity\ncloud data platforms 15–16\ndata warehouses 5\nVelocity attribute 206\nVersion attribute 248\nvolatility 81\nvolume\ncloud data platforms 15\ndata warehouses 5\nW\nwatermark 92\n\n\n  \nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nA cloud data\nplatform is the\nfoundation for all\nstages of analytics\nmaturity.\n\n\nZburivsky ● Partner\nISBN: 978-1-61729-644-4\nW\nell-designed pipelines, storage systems, and APIs \neliminate the complicated scaling and maintenance \nrequired with on-prem data centers. Once you learn \nthe patterns for designing cloud data platforms, you’ll maxi-\nmize performance no matter which cloud vendor you use.\nIn Designing Cloud Data Platforms, Danil Zburivsky and \nLynda Partner reveal a six-layer approach that increases \nﬂ exibility and reduces costs. Discover patterns for ingesting \ndata from a variety of sources, then learn to harness \npre-built services provided by cloud vendors. \nWhat’s Inside\n● Best practices for structured and unstructured data sets\n● Cloud-ready machine learning tools \n● Metadata and real-time analytics\n● Defensive architecture, access, and security\nFor data professionals familiar with the basics of cloud com-\nputing, and Hadoop or Spark.\nDanil Zburivsky has over 10 years of experience designing \nand supporting large-scale data infrastructure for enterprises \nacross the globe. Lynda Partner is the VP of Analytics-as-a-\nService at Pythian, and has been on the business side of data \nfor over 20 years.\nRegister this print book to get free access to all ebook formats. \nVisit https://www.manning.com/freebook\n$59.99 / Can $79.99  [INCLUDING eBOOK]\nDesigning Cloud Data Platforms\nCLOUD/DATA ENGINEERING\nM A N N I N G\n“\nA great guide to building \ndata platforms from the \nground up!”\n \n—Mike Jensen, Arcadia\n“\nA comprehensive overview \nof cloud data platforms and \n  a valuable resource.”\n \n—Ubaldo Pescatore\nGenerali Business Solutions\n“\nA clear, concise, and useful \nguide…provides a great \nintroduction to architectures \nand tools across the entire \nspectrum of applications \n  and platforms.”\n—Ken Fricklas, Google \n“\nA practical and realistic \nview of the architecture, \nchallenges, and patterns of \n a cloud data platform.”\n \n—Hugo Cruz\nPeople Driven Technology\nSee first page\n",
      "page_number": 312
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "  \nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/Direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\n",
      "content_length": 277,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Designing\n Cloud Data Platforms\nDANIL ZBURIVSKY AND LYNDA PARTNER\nM A N N I N G\nSHELTER ISLAND\n",
      "content_length": 95,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "For online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2021 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end.\nRecognizing also our responsibility to conserve the resources of our planet, Manning books are \nprinted on paper that is at least 15 percent recycled and processed without the use of elemental \nchlorine.\nManning Publications Co.\nDevelopment editor: Susan Ethridge\n20 Baldwin Road\nTechnical development editor: Robert Wenner\nPO Box 761\nReview editor: Mihaela Batinic\nShelter Island, NY 11964\nProduction editor: Deirdre Hiam\nCopy editor: Katie Petito\nProofreader: Katie Tennant\nTechnical proofreader: Borko Djurkovic\nTypesetter: Gordan Salinovic\nCover designer: Marija Tudor\nISBN 9781617296444\nPrinted in the United States of America\n",
      "content_length": 1717,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "iii\nbrief contents\n1\n■\nIntroducing the data platform\n1\n2\n■\nWhy a data platform and not just a data warehouse\n18\n3\n■\nGetting bigger and leveraging the Big 3: Amazon, \nMicrosoft Azure, and Google\n37\n4\n■\nGetting data into the platform\n78\n5\n■\nOrganizing and processing data\n127\n6\n■\nReal-time data processing and analytics\n156\n7\n■\nMetadata layer architecture\n197\n8\n■\nSchema management\n228\n9\n■\nData access and security\n261\n10\n■\nFueling business value with data platforms\n289\n \n \n \n \n \n \n \n \n",
      "content_length": 485,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "v\ncontents\npreface\nxi\nacknowledgments\nxiii\nabout this book\nxv\nabout the authors\nxviii\nabout the cover illustration\nxix\n1 \nIntroducing the data platform\n1\n1.1\nThe trends behind the change from data warehouses to data \nplatforms\n2\n1.2\nData warehouses struggle with data variety, volume, and \nvelocity\n3\nVariety\n4\n■Volume\n5\n■Velocity\n5\n■All the V’s at once\n6\n1.3\nData lakes to the rescue?\n6\n1.4\nAlong came the cloud\n7\n1.5\nCloud, data lakes, and data warehouses: The emergence of \ncloud data platforms\n9\n1.6\nBuilding blocks of a cloud data platform\n10\nIngestion layer\n10\n■Storage layer\n11\n■Processing layer\n12\nServing layer\n13\n",
      "content_length": 623,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "CONTENTS\nvi\n1.7\nHow the cloud data platform deals with the three V’s\n14\nVariety\n14\n■Volume\n15\n■Velocity\n15\n■Two more V’s\n16\n1.8\nCommon use cases\n16\n2 \nWhy a data platform and not just a data warehouse \n18\n2.1\nCloud data platforms and cloud data warehouses: \nThe practical aspects\n19\nA closer look at the data sources\n20\n■An example cloud data \nwarehouse–only architecture\n22\n■An example cloud data platform \narchitecture\n23\n2.2\nIngesting data\n24\nIngesting data directly into Azure Synapse\n25\n■Ingesting data \ninto an Azure data platform\n26\n■Managing changes in upstream \ndata sources\n26\n2.3\nProcessing data\n28\nProcessing data in the warehouse\n29\n■Processing data in the data \nplatform\n31\n2.4\nAccessing data\n33\n2.5\nCloud cost considerations\n34\n2.6\nExercise answers\n36\n3 \nGetting bigger and leveraging the Big 3: Amazon, Microsoft \nAzure, and Google \n37\n3.1\nCloud data platform layered architecture\n38\nData ingestion layer\n40\n■Fast and slow storage\n44\n■Processing \nlayer\n46\n■Technical metadata layer\n47\n■The serving layer and \ndata consumers\n49\n■Orchestration and ETL overlay layers\n53\n3.2\nThe importance of layers in a data platform architecture\n59\n3.3\nMapping cloud data platform layers to specific tools\n60\nAWS\n62\n■Google Cloud\n66\n■Azure\n70\n3.4\nOpen source and commercial alternatives\n74\nBatch data ingestion\n74\n■Streaming data ingestion and real-time \nanalytics\n75\n■Orchestration layer\n75\n3.5\nExercise answers\n77\n",
      "content_length": 1415,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "CONTENTS\nvii\n4 \nGetting data into the platform\n78\n4.1\nDatabases, files, APIs, and streams\n79\nRelational databases\n80\n■Files\n81\n■SaaS data via API\n82\nStreams\n82\n4.2\nIngesting data from relational databases\n83\nIngesting data from RDBMSs using a SQL interface\n84\n■Full-\ntable ingestion\n86\n■Incremental table ingestion\n91\n■Change \ndata capture (CDC)\n94\n■CDC vendors overview\n98\n■Data type \nconversion\n100\n■Ingesting data from NoSQL databases\n103\nCapturing important metadata for RDBMS or NoSQL ingestion \npipelines\n104\n4.3\nIngesting data from files\n107\nTracking ingested files\n109\n■Capturing file ingestion metadata\n112\n4.4\nIngesting data from streams\n114\nDifferences between batch and streaming ingestion\n117\nCapturing streaming pipeline metadata\n119\n4.5\nIngesting data from SaaS applications\n120\nNo standard approach to API design\n121\n■No standard way to \ndeal with full vs. incremental data exports\n122\n■Resulting data is \ntypically highly nested JSON\n122\n4.6\nNetwork and security considerations for data ingestion \ninto the cloud\n123\nConnecting other networks to your cloud data platform\n123\n4.7\nExercise answers\n126\n5 \nOrganizing and processing data\n127\n5.1\nProcessing as a separate layer in the data platform\n129\n5.2\nData processing stages\n131\n5.3\nOrganizing your cloud storage\n132\nCloud storage containers and folders\n134\n5.4\nCommon data processing steps\n140\nFile format conversion\n140\n■Data deduplication\n145\nData quality checks\n150\n5.5\nConfigurable pipelines\n152\n5.6\nExercise answers\n155\n",
      "content_length": 1493,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "CONTENTS\nviii\n6 \nReal-time data processing and analytics\n156\n6.1\nReal-time ingestion vs. real-time processing\n157\n6.2\nUse cases for real-time data processing\n160\nRetail use case: Real-time ingestion\n160\n■Online gaming use case: \nReal-time ingestion and real-time processing\n161\n■Summary of \nreal-time ingestion vs. real-time processing\n164\n6.3\nWhen should you use real-time ingestion and/or real-time \nprocessing?\n164\n6.4\nOrganizing data for real-time use\n167\nThe anatomy of fast storage\n167\n■How does fast storage \nscale?\n170\n■Organizing data in the real-time storage\n172\n6.5\nCommon data transformations in real time\n178\nCauses of duplicates in real-time systems\n178\n■Deduplicating \ndata in real-time systems\n181\n■Converting message formats in \nreal-time pipelines\n186\n■Real-time data quality checks\n187\nCombining batch and real-time data\n188\n6.6\nCloud services for real-time data processing\n190\nAWS real-time processing services\n190\n■Google Cloud real-time \nprocessing services\n192\n■Azure real-time processing services\n193\n6.7\nExercise answers\n195\n7 \nMetadata layer architecture\n197\n7.1\nWhat we mean by metadata\n198\nBusiness metadata\n198\n■Data platform internal metadata or \n“pipeline metadata”\n199\n7.2\nTaking advantage of pipeline metadata\n199\n7.3\nMetadata model\n203\nMetadata domains\n204\n7.4\nMetadata layer implementation options\n213\nMetadata layer as a collection of configuration files\n214\nMetadata database\n217\n■Metadata API\n218\n7.5\nOverview of existing solutions\n220\nCloud metadata services\n221\n■Open source metadata layer \nimplementations\n223\n7.6\nExercise answers\n227\n",
      "content_length": 1576,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "CONTENTS\nix\n8 \nSchema management\n228\n8.1\nWhy schema management\n229\nSchema changes in a traditional data warehouse architecture\n230\nSchema-on-read approach\n231\n8.2\nSchema-management approaches\n232\nSchema as a contract\n233\n■Schema management in the data \nplatform\n235\n■Monitoring schema changes\n241\n8.3\nSchema Registry Implementation\n243\nApache Avro schemas\n243\n■Existing Schema Registry \nimplementations\n245\n■Schema Registry as part of a \nMetadata layer\n246\n8.4\nSchema evolution scenarios\n248\nSchema compatibility rules\n249\n■Schema evolution and data \ntransformation pipelines\n251\n8.5\nSchema evolution and data warehouses\n255\nSchema-management features of cloud data warehouses\n257\n8.6\nExercise answers\n260\n9 \nData access and security\n261\n9.1\nDifferent types of data consumers\n262\n9.2\nCloud data warehouses\n263\nAWS Redshift\n264\n■Azure Synapse\n268\n■Google \nBigQuery\n270\n■Choosing the right data warehouse\n273\n9.3\nApplication data access\n274\nCloud relational databases\n275\n■Cloud key/value data \nstores\n276\n■Full-text search services\n277\n■In-memory \ncache\n278\n9.4\nMachine learning on the data platform\n278\nMachine learning model lifecycle on a cloud data platform\n279\nML cloud collaboration tools\n282\n9.5\nBusiness intelligence and reporting tools\n283\nTraditional BI tools and cloud data platform integration\n283\nUsing Excel as a BI tool\n284\n■BI tools that are external to the \ncloud provider\n284\n",
      "content_length": 1393,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "CONTENTS\nx\n9.6\nData security\n285\nUsers, groups, and roles\n285\n■Credentials and configuration \nmanagement\n286\n■Data encryption\n286\n■Network \nboundaries\n287\n9.7\nExercise Answers\n288\n10 \nFueling business value with data platforms\n289\n10.1\nWhy you need a data strategy\n290\n10.2\nThe analytics maturity journey\n291\nSEE: Getting insights from data\n292\n■PREDICT: Using data to \npredict what to do\n293\n■DO: Making your analytics \nactionable\n294\n■CREATE: Going beyond analytics into \nproducts\n295\n10.3\nThe data platform: The engine that powers analytics \nmaturity\n296\n10.4\nPlatform project stoppers\n297\nTime does indeed kill\n297\n■User adoption\n298\n■User trust and \nthe need for data governance\n299\n■Operating in a platform \nsilo\n300\n■The dollar dance\n301\nindex\n304\n \n",
      "content_length": 757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "xi\npreface\nThis book is a true collaboration, a team effort between two very different people who\nshare a love of data, new technology, and solving customer problems. We (Danil and\nLynda) worked together at a data, analytics, and cloud IT services company for five\nyears, where we partnered up to develop a cloud analytics practice. Danil, with his\nyears of Hadoop experience, brought the technical chops, and Lynda brought the\nbusiness perspective. We realized early on that both were needed to solve real-world\ndata problems, and over time, Danil became more business-oriented and Lynda\nbecame knowledgeable enough about the cloud and data to contribute and even\nsometimes challenge Danil. \n The move from Hadoop as a big data platform to cloud-native platforms for data\nand analytics was an easy one—we both love the promise of cloud and big data. With\nthe support of our employer, we built an internal team and designed and delivered\nnot just awesome technology solutions but solutions that delivered real business out-\ncomes using data and cloud. We did this for dozens of customers, and over time, we\ndeveloped a set of best practices and knowledge. It was this experience and our\nunique mix of technical and business skills that let us believe that we could take a\nreally complicated technical subject and make it understandable for a broader audi-\nence. We started with blog posts and white papers, and when Manning called and\nasked if Danil wanted to write another book (his first was on Hadoop), it seemed right\nand natural to do it together.\n Both of us were active speakers at industry events, so we took advantage of these\nopportunities to frame our ideas for the book and used audience feedback to refine\n",
      "content_length": 1719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "them. We also agreed that we would weave in real customer stories because we both\nbelieve that stories make all learning easier. Once we realized that we were aligned on\nhow to approach the book, there was nothing left but to start typing. It took almost\ntwo years, but we are both really happy with the outcome, and we hope you are too.\n",
      "content_length": 338,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "xiii\nacknowledgments\nWe knew this would be a lot of work—Danil especially, because he had done it\nbefore—but we both agree that it ended up being more work than either of us\nthought. We realized that we are both perfectionists, and we were each happy pushing\nthe other to do better. The end result is a product we are both proud of, but we\nwouldn’t be here without a broader team of people who supported us. We’d like to\nthank a few of them here.\n First and foremost, we thank our spouses for putting up with our absences on\nweekends and holidays while we typed and typed and typed. It is still amazing to us\nthat neither ever complained and both were always there for us.\n Our work communities have been incredibly supportive starting with our employer\nat the time, Pythian—especially founder Paul Vallée, who backed us when we said we\nthought we could develop and sell cloud-native data platforms at a time when people\nwere saying “cloud native what?” Pythian also graciously allowed us to use the dia-\ngrams that appear in chapter 10. The bottom line is that our employers, past and pres-\nent, have encouraged us to keep writing and sharing our knowledge, and we are\ngrateful for that support.\n A big, big thank you goes to the Kick AaaS team we worked with—Kevin Pedersen,\nChristos Soulios, Valentin Nikotin, and Rory Bramwell, who all took a gamble on a\nnew direction and followed us into the unknown—they are the invisible authors of\nthis book. And we will never be able to thank our customers enough, especially the\nfirst few who were patient as we learned how to make better and better designs on\ntheir behalf.\n",
      "content_length": 1619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "ACKNOWLEDGMENTS\nxiv\n Next, we’d like to acknowledge the folks at Manning, especially our editor Susan\nEthridge, who seemed to know just how hard to push to get us to produce our best\nwork but who also knew when we just needed a sympathetic ear. The book is better\nbecause of you, Susan, and you’ve become a friend in the process. We will miss our\nweekly meetings! Deirdre Hiam, our project editor; Katie Petito, our copyeditor; Katie\nTennant, our proofreader; and Mihaela Batinic, our reviewing editor: thank you, all.\n And lastly, we thank the people who reviewed our book as we wrote it and gave great\nconstructive feedback. We’re not going to lie, it was really difficult to send out our opus\nto strangers and ask them for feedback, but we did and we got great suggestions back.\nThank you for taking the time, for being so constructive, and for the nice things you\nsaid about the book—it kept us going through the hard times. Thank you, all the\nreviewers whose suggestions helped make this a better book: Robert Wenner first and\nforemost (“What will Robert say?” became a common refrain in our meetings), Alain\nCouniot, Alex Saez, Borko Djurkovic, Chris Viner, Christopher E. Phillips, Daniel\nBerecz, Dave Corun, David Allen Blubaugh, David Krief, Emanuele Piccinelli, Eros\nPedrini, Gabor Gollnhofer, George Thomas, Hugo Cruz, Jason Rendel, Ken Fricklas,\nMikeJensen, Peter Bishop, Peter Hampton, Richard Vaughan, Sambasiva Andaluri,\nSatej Sahu, Sean Thomas Booker, Simone Sguazza, Ubaldo Pescatore, and Vishwesh\nRavi Shrimali.\n",
      "content_length": 1530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "xv\nabout this book\nDesigning Cloud Data Platforms was written to help guide you in designing a cloud data\nplatform that is both scalable and flexible enough to deal with the inevitable technol-\nogy changes. It begins by explaining what exactly we mean by the term “cloud data\nplatform,” why it matters, and how it is different from a cloud data warehouse. It then\nshifts into following the flow of data into and through the data platform—from inges-\ntion and organization, to processing and managing data. It wraps up with how differ-\nent data consumers use the data in the platform and discusses the most common\nbusiness issues that can impact the success of a cloud data platform project.\nWho should read this book\nThe book is designed for someone who wants to understand what a data platform is\nand how it should be architected to take advantage of the cloud. It is detailed enough\nto get someone with a solid programming background well on their way to architect-\ning a solution and also addresses the connection between technology and business so\nproduct owners and business and data analysts who may never have to design an archi-\ntecture will understand the concepts and rationale behind it. It goes into detail about\nsubjects such as streaming versus batch, schema management, and other key design\nelements, but it is a book about designing, not programming.\n",
      "content_length": 1367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "ABOUT THIS BOOK\nxvi\nHow this book is organized: A roadmap\nChapter 1 introduces the concept of a cloud data platform, describing the trends that\nare driving demand and introducing the key building blocks of a cloud data platform\ndesign.\n Chapter 2 compares and contrasts the differences between cloud data platforms\nand cloud data warehouses.\n Chapter 3 expands on the simple architecture introduced in chapter 1 and maps\nthe layers in the architecture to tools available from each of AWS, Azure, and Google\nCloud.\n Chapter 4 is all about getting data into the data platform—focusing on data com-\ning from relational databases, files, streams, and SaaS systems via an API.\n Chapter 5 explains how to best organize and process the data in your data plat-\nform, introducing the concept of configurable pipelines and common data process-\ning steps.\n Chapter 6 is dedicated to real-time data processing and analytics, the difference\nbetween real-time ingestion versus real-time processing, and how your real-time data\nshould be organized and transformed.\n Chapter 7 introduces the important concept of a technical metadata layer and why\nit is needed, along with options for a technical metadata model, several implementa-\ntion options, and an overview of existing commercial and open source solutions.\n Chapter 8 takes on the long-standing challenges associated with schema manage-\nment, provides several possible approaches, and discusses how schema evolution can\nbe handled in a modern data platform.\n Chapter 9 discusses the different types of data consumers and data access points,\nincluding the data warehouse, application access, machine learning users, and BI and\nreport tools.\n Chapter 10 wraps up the book by describing the way the data platform is used to\ndrive business value and discusses a number of organizational challenges associated\nwith ensuing data platform project success. \nAbout the code \nThis book contains some examples of source code both in numbered listings and in\nline with normal text. In both cases, source code is formatted in a fixed-width font\n'like this' to separate it from ordinary text. In many cases, the original source code\nhas been reformatted; we’ve added line breaks and reworked indentation to accom-\nmodate the available page space in the book. In rare cases, even this was not enough,\nand listings include line-continuation markers (➥). Code annotations accompany\nmany of the listings, highlighting important concepts. \n",
      "content_length": 2461,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "ABOUT THIS BOOK\nxvii\nliveBook discussion forum\nPurchase of Designing Cloud Data Platforms includes free access to a private web forum\nrun by Manning Publications where you can make comments about the book, ask tech-\nnical questions, and receive help from the authors and from other users. To access the\nforum, go to https://livebook.manning.com/book/designing-cloud-data-platforms/\nwelcome/v-8/. You can also learn more about Manning’s forums and the rules of con-\nduct at https://livebook.manning.com/#!/discussion.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the authors can take\nplace. It is not a commitment to any specific amount of participation on the part of\nthe authors, whose contribution to the forum remains voluntary (and unpaid). We\nsuggest you try asking them some challenging questions lest their interest stray! The\nforum and the archives of previous discussions will be accessible from the publisher’s\nwebsite as long as the book is in print.\n",
      "content_length": 1055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "xviii\nabout the authors\nDANIL ZBURIVSKY has spent his entire career designing and supporting large-scale data\ninfrastructure for enterprises across the globe. He started his career over 10 years ago\nat IT services company Pythian, managing open source database systems for a number\nof large-scale internet companies. He was an early champion of Hadoop, and while he\nmanaged a team that designed and implemented large-scale Hadoop analytics infra-\nstructures, he wrote and published a book on Hadoop cluster deployment best prac-\ntices. Anticipating the impact the public cloud would have on data infrastructure, he\nwas an early adopter of cloud data services and has architected and implemented\nmodern cloud-based data platforms on all three public cloud platforms for dozens of\nenterprises across the globe. An avid surfer, Danil lives in Halifax, Nova Scotia, and\nspends his free time riding the waves, 12 months a year.\n LYNDA PARTNER has been on the business side of data for more than 20 years. She\nbecame addicted to data when, as founder of a SaaS company, she used data exten-\nsively to optimize how her customers used her product. When she later became Presi-\ndent of Intouch Insights, she pivoted a traditional market research firm into one of\nthe first mobile data-capture companies, collecting valuable consumer data for major\nautomotive vendors. In her current role as Vice President of Analytics for IT services\ncompany Pythian, she works with companies across a wide range of industries and\ncountries, helping them turn data into insights, predictions, and products. When she\nisn’t working, you’ll find her at her island cottage, where she spends time kayaking\nand writing and plotting new uses for data.\n",
      "content_length": 1720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "xix\nabout the cover illustration\nThe figure on the cover of Designing Cloud Data Platforms is captioned “Femme du\nJapon,” or woman from Japan. The illustration is taken from a collection of dress cos-\ntumes from various countries by Jacques Grasset de Saint-Sauveur (1757-1810), titled\nCostumes de Différents Pays, published in France in 1797. Each illustration is finely\ndrawn and colored by hand. The rich variety of Grasset de Saint-Sauveur’s collection\nreminds us vividly of how culturally apart the world’s towns and regions were just 200\nyears ago. Isolated from each other, people spoke different dialects and languages. In\nthe streets or in the countryside, it was easy to identify where they lived and what their\ntrade or station in life was just by their dress.\n The way we dress has changed since then and the diversity by region, so rich at the\ntime, has faded away. It is now hard to tell apart the inhabitants of different conti-\nnents, let alone different towns, regions, or countries. Perhaps we have traded cultural\ndiversity for a more varied personal life—certainly, for a more varied and fast-paced\ntechnological life.\n At a time when it is hard to tell one computer book from another, Manning cele-\nbrates the inventiveness and initiative of the computer business with book covers\nbased on the rich diversity of regional life of two centuries ago, brought back to life by\nGrasset de Saint-Sauveur’s pictures.\n",
      "content_length": 1430,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "1\nIntroducing\n the data platform\nEvery business, whether it realizes it or not, requires analytics. It’s a fact. There has\nalways been a need to measure important business metrics and make decisions\nbased on these measurements. Questions such as “How many items did we sell last\nmonth?” and “What’s the fastest way to ship a package from A to B?” have evolved\nto “How many new website customers purchased a premium subscription?” and\n“What does my IoT data tell me about customer behavior?” \nThis chapter covers\nDriving change in the world of analytics data\nUnderstanding the growth of data volume, variety, and \nvelocity, and why the traditional data warehouse can’t \nkeep up\nLearning why data lakes alone aren’t the answer\nDiscussing the emergence of the cloud data platform\nStudying the core building blocks of the cloud data \nplatform\nViewing sample use cases for cloud data platforms\n",
      "content_length": 895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "2\nCHAPTER 1\nIntroducing the data platform\n Before computers became ubiquitous, we relied on ledgers, inventory lists, a healthy\ndose of intuition, and other limited, manual means of tracking and analyzing business\nmetrics. The late 1980s ushered in the concept of a data warehouse—a centralized\nrepository of structured data combined from multiple sources—which was typically\nused to produce static reports. Armed with this data warehouse, businesses were increas-\ningly able to shift from intuition-based decision making to an approach based on data.\nHowever, as technology and our needs evolved, we’ve gradually shifted toward a new\ndata management construct: the data platform that increasingly resides in the cloud.\n Simply put, a cloud data platform is “a cloud-native platform capable of cost-\neffectively ingesting, integrating, transforming, and managing an almost unlimited\namount of data of any type data in order to facilitate analytics outcomes.” Cloud data\nplatforms solve or significantly improve many of the fundamental problems and\nshortcomings that plague traditional data warehouses and even modern data lakes—\nproblems that center around data variety, volume, and velocity, or the three V’s.\n In this book, we’ll set the stage by taking a brief look at some of the core constructs\nof the data warehouse and how they lead to the shortcomings outlined in the three\nV’s. Then we’ll consider how data warehouses and data lakes can work together to\nfunction as a data platform. We’ll discuss the key components of an efficient, robust,\nand flexible data platform design and compare the various cloud tools and services\nthat can be used in each layer of your design. We’ll demonstrate the steps involved in\ningesting, organizing, and processing data in the data platform for both batch and\nreal-time/streaming data. After ingesting and processing data in the platform, we will\nmove on to data management with a focus on the creation and use of technical meta-\ndata and schema management. We’ll discuss the various data consumers and ways that\ndata in the platform can be consumed and then end with a discussion about how the\ndata platform supports the business and a list of common nontechnical items that\nshould be taken into consideration to ensure use of the data platform is maximized.\n By the time you’ve finished reading, you’ll be able to\nDesign your own data platform using a modular design\nDesign for the long term to ensure it is manageable, versatile, and scalable \nExplain and justify your design decisions to others \nPick the right cloud tools for each part of your design \nAvoid common pitfalls and mistakes \nAdapt your design to a changing cloud ecosystem\n1.1\nThe trends behind the change from data warehouses \nto data platforms\nData warehouses have, for the most part, stood the test of time and are still used in\nalmost all enterprises. But several recent trends have made their shortcomings pain-\nfully obvious.\n The explosion in popularity of software as a service (SaaS) has resulted in a big\nincrease in the variety and number of sources of data being collected. SaaS and other\n",
      "content_length": 3118,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "3\nData warehouses struggle with data variety, volume, and velocity\nsystems produce a variety of data types beyond the structured data found in traditional\ndata warehouses, including semistructured and unstructured data. These last two data\ntypes are notoriously data warehouse unfriendly, and are also prime contributors to\nthe increasing velocity (the rate at which data arrives into your organization) as real-\ntime streaming starts to supplant daily batch updates and the volume (the total\namount) of data.\n Another and arguably more significant trend, however, is the change of applica-\ntion architecture from monolithic to microservices. Since in the microservices world\nthere is no central operation database from which to pull data, collecting messages\nfrom these microservices becomes one of the most important analytics tasks. To keep\nup with these changes, a traditional data warehouse requires rapid, expensive, and\nongoing investments in hardware and software upgrades. With today’s pricing models,\nthat eventually becomes extremely cost prohibitive.\n There’s also growing pressure from business users and data scientists who use mod-\nern analytics tools that can require access to raw data not typically stored in data ware-\nhouses. This growing demand for self-service access to data also puts stresses on the\nrigid data models associated with traditional data warehouses.\n1.2\nData warehouses struggle with data variety, volume, \nand velocity\nThis section explains why a data warehouse alone just won’t deliver on the growth in\ndata variety, volume, and velocity being experienced today, and how combining a data\nlake with a data warehouse to create a data platform can address the challenges associ-\nated with today’s data: variety, volume, and velocity.\n The following diagram (figure 1.1) illustrates how a relational warehouse typically\nhas an ETL tool or process that delivers data into tables in the data warehouse on a\nschedule. It also has storage, compute (i.e., processing), and SQL services all running\non a single physical machine.\n This single-machine architecture significantly limits flexibility. For example, you\nmay not be able to add more processing capacity to your warehouse without affecting\nstorage. \nData\nsources\nRDBMS data warehouse\nStorage\nSQL\nProcessing\nETL\ntool\nFigure 1.1\nTraditional \ndata warehouse design\n",
      "content_length": 2349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "4\nCHAPTER 1\nIntroducing the data platform\n1.2.1\nVariety\nVariety is indeed the spice of life when it comes to analytics. But traditional data ware-\nhouses are designed to work exclusively with structured data (see figure 1.2). This\nworked well when most ingested data came from other relational data systems, but\nwith the explosion of SaaS, social media, and IoT (Internet of Things), the types of\ndata being demanded by modern analytics are much more varied and now includes\nunstructured data such as text, audio, and video.\n SaaS vendors, under pressure to make data available to their customers, started\nbuilding application APIs using the JSON file format as a popular way to exchange data\nbetween systems. While this format provides a lot of flexibility, it comes with a tendency\nto change schemas often and without warning—making it only semistructured. In addi-\ntion to JSON, there are other formats such as Avro or Protocol Buffers that produce\nsemistructured data, for developers of upstream applications to choose from. Finally,\nthere are binary, image, video, and audio data—truly unstructured data that’s in high\ndemand by data science teams. Data warehouses weren’t designed to deal with any-\nthing but structured data, and even then, they aren’t flexible enough to adapt to the\nfrequent schema changes in structured data that the popularity of SaaS systems has\nmade commonplace.\n Inside a data warehouse, you’re also limited to processing data either in the data\nwarehouse’s built-in SQL engine or a warehouse-specific stored procedure language.\nThis limits your ability to extend the warehouse to support new data formats or\nprocessing scenarios. SQL is a great query language, but it’s not a great programming\nlanguage because it lacks many of the tools today’s software developers take for\ngranted: testing, abstractions, packaging, libraries for common logic, and so on. ETL\n(extract, transform, load) tools often use SQL as a processing language and push all\nprocessing into the warehouse. This, of course, limits the types of data formats you\ncan deal with efficiently. \nData\nsources\nRDBMS data warehouse\nStorage\nSQL\nProcessing\nETL\ntool\nData processing in a\ndata warehouse is limited\nto the data warehouse’s\nbuilt-in SQL engine or a\nwarehouse-specific stored\nprocedure language. \nData warehouses weren’t\ndesigned to deal with anything \nbut structured data. \nFigure 1.2\nHandling of a range of data varieties and processing options are limited in a traditional \ndata warehouse.\n",
      "content_length": 2495,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "5\nData warehouses struggle with data variety, volume, and velocity\n1.2.2\nVolume \nData volume is everyone’s problem. In today’s internet-enabled world, even a small\norganization may need to process and analyze terabytes of data. IT departments are\nregularly being asked to corral more and more data. Clickstreams of user activity from\nwebsites, social media data, third-party data sets, and machine-generated data from\nIoT sensors all produce high-volume data sets that businesses often need to access. \nIn a traditional data warehouse (figure 1.3), storage and processing are coupled\ntogether, significantly limiting scalability and flexibility. To accommodate a surge in\ndata volume in traditional relational data warehouses, bigger servers with more disk,\nRAM, and CPU to process the data must be purchased and installed. This approach is\nslow and very expensive, because you can’t get storage without compute, and buying\nmore servers to increase storage means that you are likely paying for compute that you\nmight not need, or vice versa. Storage appliances evolved as a solution to this problem\nbut did not eliminate the challenges of easily scaling compute and storage at a cost-\neffective ratio. The bottom line is that in a traditional data warehouse design, process-\ning large volumes of data is available only to organizations with significant IT budgets. \n1.2.3\nVelocity\nData velocity, the speed at which data arrives into your data system and is processed,\nmight not be a problem for you today, but with analytics going real-time, it’s just a\nquestion of when, not if. With the increasing proliferation of sensors, streaming data\nis becoming commonplace. In addition to the growing need to ingest and process\nstreaming data, there’s increasing demand to produce analytics in as close to real-time\nas possible.\n Traditional data warehouses are batch-oriented: take nightly data, load it into a\nstaging area, apply business logic, and load your fact and dimension tables. This\nmeans that your data and analytics are delayed until these processes are completed for\nall new data in a batch. Streaming data is available more quickly but forces you to deal\nwith each data point separately as it comes in. This doesn’t work in a data warehouse\nand requires a whole new infrastructure to deliver data over the network, buffer it in\nmemory, provide reliability of computation, etc. \nData\nsources\nRDBMS data warehouse\nStorage\nSQL\nProcessing\nETL\ntool\nFigure 1.3\nIn traditional \ndata warehouses, storage \nand processing are coupled.\n",
      "content_length": 2532,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "6\nCHAPTER 1\nIntroducing the data platform\n1.2.4\nAll the V’s at once\nThe emergence of artificial intelligence and its popular subset, machine learning, cre-\nates a trifecta of V’s. When data scientists become users of your data systems, volume\nand variety challenges come into play all at once. Machine learning models love\ndata—lots and lots of it (i.e., volume). Models developed by data scientists usually\nrequire access not just to the organized, curated data in the data warehouse, but also\nto the raw source-file data of all types that’s typically not brought into the data ware-\nhouse (i.e., variety). Their models are compute intensive, and when run against data\nin a data warehouse, put enormous performance pressure on the system, especially\nwhen they run against data arriving in near-real time (velocity). With current data\nwarehouse architectures, these models often take hours or even days to run. They also\nimpact warehouse performance for all other users while they’re running. Finding a\nway to give data scientists access to high-volume, high-variety data will allow you to cap-\nitalize on the promise of advanced analytics while reducing its impact on other users\nand, if done correctly, it can keep costs lower.\n1.3\nData lakes to the rescue?\nA data lake, as defined by TechTarget’s WhatIs.com is “A storage repository that holds\na vast amount of raw data in its native format until it is needed.” Gartner Research adds\na bit more context in its definition: “A collection of storage instances of various data\nassets additional to the originating data sources. These assets are stored in a near-exact\n(or even exact) copy of the source format. As a result, the data lake is an unintegrated,\nnon-subject-oriented collection of data.” \n The concept of a data lake evolved from these megatrends mentioned previously,\nas organizations desperately needed a way to deal with increasing numbers of data for-\nmats and growing volumes and velocities of data that traditional data warehouses\ncouldn’t handle. The data lake was to be the place where you could bring any data you\nwant, from different sources, structured, unstructured, semistructured, or binary. It\nwas the place where you could store and process all your data in a scalable manner.\n After the introduction of Apache Hadoop in 2006, data lakes became synonymous with\nthe ecosystem of open source software utilities, known simply as “Hadoop,” that provided\na software framework for distributed storage and processing of big data using a network\nof many computers to solve problems involving massive amounts of data and computa-\ntion. While most would argue that Hadoop is more than a data lake, it did address some\nof the variety, velocity, and volume challenges discussed earlier in this chapter:\nVariety—Hadoop’s ability to do schema on read (versus the data warehouse’s\nschema on write) meant that any file in any format could be immediately stored\non the system, and processing could take place later. Unlike data warehouses,\nwhere processing could only be done on the structured data in the data ware-\nhouse, processing in Hadoop could be done on any data type.\n",
      "content_length": 3138,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "7\nAlong came the cloud\nVolume—Unlike the expensive, specialized hardware often required for ware-\nhouses, Hadoop systems took advantage of distributed processing and storage\nacross less expensive commodity hardware that could be added in smaller incre-\nments as needed. This made storage less expensive, and the distributed nature\nof processing made it easier and faster to do processing because the workload\ncould be split among many servers. \nVelocity—When it came to streaming and real-time processing, ingesting and\nstoring streaming data was easy and inexpensive on Hadoop. It was also possi-\nble, with the help of some custom code, to do real-time processing on Hadoop\nusing products such as Hive or MapReduce or, more recently, Spark.\nHadoop’s ability to cost-effectively store and process huge amounts of data in its native\nformat was a step in the right direction towards handing variety, volume, and velocity\nof today’s data estate, and for almost a decade, it was the de facto standard for data\nlakes in the data center. \n But Hadoop did have shortcomings:\nIt is a complex system with many integrated components that run on hardware\nin a data center. This makes it difficult to maintain and requires a team of\nhighly skilled support engineers to keep the system secure and operational.\nIt isn’t easy for users who want to access the data. Its unstructured approach to\nstorage, while more flexible than the very structured and curated data ware-\nhouse, is often too difficult for business users to make sense of.\nFrom a developer perspective, its use of an “open” toolset makes it very flexible,\nbut its lack of cohesiveness makes it challenging to use. For example, you can\ninstall any language, library, or utility onto a Hadoop framework to process\ndata, but you would have to know all those languages and libraries instead of\nusing a generic interface such as SQL.\nStorage and compute are not separate, meaning that while the same hardware\ncan be used for both storage and compute, it can only be deployed effectively in\na static ratio. This limits its flexibility and cost-effectiveness.\nAdding hardware to scale the system often takes months, resulting in a cluster\nthat is either chronically over or underutilized. \nInevitably a better answer came along—one that had the benefits of Hadoop, elimi-\nnated its shortcomings, and brought even more flexibility to designers of data systems.\nAlong came the cloud.\n1.4\nAlong came the cloud\nThe advent of the public cloud, with its on-demand storage, compute resource provi-\nsioning, and pay-per-usage pricing model, allowed data lake design to move beyond\nthe limitations of Hadoop. The public cloud allowed the data lake to include more\nflexibility in design and scalability and be more cost effective while drastically reduc-\ning the amount of support required. \n",
      "content_length": 2832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "8\nCHAPTER 1\nIntroducing the data platform\n Data warehouses and data lakes have moved to the cloud and are increasingly\noffered as a platform as a service (PaaS), defined by Wikipedia as “a category of cloud\ncomputing services that provides a platform allowing customers to develop, run, and\nmanage applications without the complexity of building and maintaining the infra-\nstructure typically associated with developing and launching an app.” Using PaaS\nallows organizations to take advantage of additional flexibility and cost-effective scal-\nability. There’s also a new generation of data processing frameworks available only in\nthe cloud that combine scalability with support for modern programming languages\nand integrate well into the overall cloud paradigm.\n The advent of the public cloud changed everything when it came to analytics data\nsystems. It allowed data lake design to move beyond the limitations of Hadoop and\nallowed for the creation of a combined data lake and data warehouse solution that\nwent far beyond what was available on premises.\n The cloud brought so many things, but topping the list were the following:\nElastic resources —Whether you’re talking storage or compute, you can get either\nfrom your favorite cloud vendor: the amount of that resource is allocated to you\nexactly as you need it; and it grows and shrinks as your needs change—automat-\nically or by request. \nModularity—Storage and compute are separate in a cloud world. No longer do\nyou have to buy both when you need only one, which optimizes your invest-\nment.\nPay per use—Nothing is more irksome than paying for something you aren’t\nusing. In a cloud world, you only pay for what you use so you no longer have to\ninvest in overprovisioned systems in anticipation of future demand. \nCloud turns capital investment, capital budgets, and capital amortization into opera-\ntional expense—This is tied to pay per use. Compute and storage resources are\nnow utilities rather than owned infrastructure.\nManaged services are the norm—In an on-premises world, human resources are\nneeded for the operation, support, and updating of a data system. In a cloud\nworld, much of these functions are done by the cloud provider and are\nincluded in the use of the services.\nInstant availability—Ordering and deploying a new server can take months.\nOrdering and deploying a cloud service takes minutes.\nA new generation of cloud-only processing frameworks—There’s a new generation of\ndata processing frameworks available only in the cloud that combine scalability\nwith support for modern programming languages and integrate well into the\noverall cloud paradigm.\nFaster feature introduction—Data warehouses have moved to the cloud and are\nincreasingly offered as PaaS, allowing organizations to take instant advantage of\nnew features.\nLet’s look at an example: Amazon Web Services (AWS) EMR.\n",
      "content_length": 2870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "9\nCloud, data lakes, and data warehouses: The emergence of cloud data platforms\n AWS EMR is a cloud data platform for processing data using open source tools. It is\noffered as a managed service from AWS and allows you to run Hadoop and Spark jobs\non AWS. All you need to do to create a new cluster is to specify how many virtual\nmachines you need and what type of machines you want. You also need to provide a\nlist of software you want to install on the cluster, and AWS will do the rest for you. In\nseveral minutes you have a fully functional cluster up and running. Compare that to\nmonths of planning, procuring, deploying, and configuring an on-premises Hadoop\ncluster! Additionally, AWS EMR allows you to store data on AWS S3 and process the\ndata on an AWS EMR cluster without permanently storing any data on AWS EMR\nmachines. This unlocks a lot of flexibility in the number of clusters you can run and\ntheir configuration and allows you to create ephemeral clusters that can be disposed\nof once their job is done. \n1.5\nCloud, data lakes, and data warehouses: The emergence \nof cloud data platforms\nThe argument for a data lake is tied to the dramatic increases in variety, volume, and\nvelocity of today’s analytic data, along with the limitations of traditional data ware-\nhouses to accommodate these increases. We’ve described how a data warehouse alone\nstruggles to cost-effectively accommodate the variety of data that IT must make avail-\nable. It’s also more expensive and complicated to store and process these growing vol-\numes and velocities of data in a data warehouse, instead of in a combination of a data\nlake and a data warehouse.\n A data lake easily and cost-effectively handles an almost unlimited variety, volume,\nand velocity of data. The caveat is that it’s not usually organized in a way that’s useful\nto most users—business users in particular. Much of the data in a data lake is also\nungoverned, which presents other challenges. It may be that in the future a modern\ndata lake will completely replace the data warehouse, but for now, based on what we\nsee in all our customer environments, a data lake is almost always coupled with a data\nwarehouse. The data warehouse serves as the primary governed data consumption\npoint for business users, while direct user access to the largely ungoverned data in a\ndata lake is typically reserved for data exploration either by advanced users, such as\ndata scientists, or other systems.\n Until recently, the data warehouse and/or associated ETL tools are where the\nmajority of data processing took place. But today that processing can occur in the data\nlake itself, moving performance-impacting processing from the more expensive data\nwarehouse to the less expensive data lake. This also provides for new forms of process-\ning, such as streaming, as well as the more traditional batch processing supported by\ndata warehouses.\n While the distinction between a data lake and data warehouse continues to blur,\nthey each have distinct roles to play in the design of a modern analytics platform. There\nare many good reasons to consider a data lake in addition to a cloud data warehouse\ninstead of simply choosing one or the other. A data lake can help balance your users’\n",
      "content_length": 3230,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "10\nCHAPTER 1\nIntroducing the data platform\ndesire for immediate access to all the data against the organization’s need to ensure\ndata is properly governed in the warehouse. \n The bottom line is that the combination of new processing technologies available\nin the cloud, a cloud data warehouse, and a cloud data lake enable you to take better\nadvantage of the modularity, flexibility, and elasticity offered in the cloud to meet the\nneeds of the broadest number of use cases. The resulting solution is a modern data\nplatform: cost effective, flexible, and capable of ingesting, integrating, transforming,\nand managing all the V’s to facilitate analytics outcomes. \n The resulting analytics data platform can be far more capable than anything the\ndata center can possibly provide. Designing a cloud data platform to take advantage of\nnew technologies and cloud services to address the needs of the new data consumers\nis the subject of this book.\n1.6\nBuilding blocks of a cloud data platform\nThe purpose of a data platform is to ingest, store, process, and make data available for\nanalysis no matter which type of data comes in—and in the most cost-efficient manner\npossible. To achieve this, well-designed data platforms use a loosely coupled architec-\nture where each layer is responsible for a specific function and interacts with other lay-\ners via their well-defined APIs. The foundational building blocks of a data platform\nare ingestion, storage, processing, and serving layers, as illustrated in figure 1.4.\n1.6.1\nIngestion layer\nThe ingestion layer is all about getting data into the data platform. It’s responsible for\nreaching out to various data sources such as relational or NoSQL databases, file stor-\nage, or internal or third-party APIs, and extracting data from them. With the prolifer-\nation of different data sources that organizations want to feed their analytics, this layer\nmust be very flexible. To this end, the ingestion layer is often implemented using a\nvariety of open source or commercial tools, each specialized to a specific data type. \nIngest\nStorage\nProcessing\nServing\nCloud data\nwarehouse\nAPIs\nData export\nFigure 1.4\nWell-designed data platforms use a loosely coupled architecture where each layer is \nresponsible for a specific function.\n",
      "content_length": 2270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "11\nBuilding blocks of a cloud data platform\n One of the most important characteristics of a data platform’s ingestion layer is\nthat this layer should not modify and transform incoming data in any way. This is to\nmake sure that the raw, unprocessed data is always available in the lake for data lin-\neage tracking and reprocessing.\n1.6.2\nStorage layer\nOnce we’ve acquired the data from the source, it must be stored. This is where data\nlake storage comes into play. An important characteristic of a data lake storage system\nis that it must be scalable and inexpensive, so as to accommodate the vast amounts\nand velocity of data being produced today. The scalability requirement is also driven\nby the need to store all incoming data in its raw format, as well as the results of differ-\nent data transformations or experiments that data lake users apply to the data.\n A standard way to obtain scalable storage in a data center is to use a large disk array\nor Network-Attached Storage. These enterprise-level solutions provide access to large\nvolumes of storage, but have two key drawbacks: they’re usually expensive, and they\ntypically come with a predefined capacity. This means you must buy more devices to\nget more storage.\n Given these factors, it’s not surprising that flexible storage was one of the first ser-\nvices offered by cloud vendors. Cloud storage doesn’t impose any restrictions on the\ntypes of files you can upload—you’ve got free rein to bring in text files like CSV or\nJSON and binary files like Avro, Parquet, images, or video—just about anything can be\nstored in the data lake. This ability to store any file format is an important foundation\nof a data lake because it allows you to store raw, unprocessed data and delay its pro-\ncessing until later.\n For users who have worked with Network-Attached Storage or Hadoop Distributed\nFile System (HDFS), cloud storage may look and feel very similar to one of those sys-\ntems. But there are some important differences:\nCloud storage is fully managed by a cloud provider. This means you don’t need\nto worry about maintenance, software or hardware upgrades, etc.\nCloud storage is elastic. This means cloud vendors will only allocate the amount\nof storage you need, growing or shrinking the volume as requirements dictate.\nYou no longer need to overprovision storage system capacity in anticipation of\nfuture demand.\nYou only pay for the capacity you use.\nThere are no compute resources directly associated with cloud storage. From\nan end-user perspective, there are no virtual machines attached to cloud stor-\nage—this means large volumes of data can be stored without having to take on\nidle compute capacity. When the time comes to process the data, you can easily\nprovision the required compute resources on demand.\nToday, every major cloud provider offers a cloud storage service—and for good rea-\nson. As data flows through the data lake, cloud storage becomes a central component.\n",
      "content_length": 2951,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "12\nCHAPTER 1\nIntroducing the data platform\nRaw data is stored in cloud storage and awaits processing, the processing layer saves\nthe results back to cloud storage, and users access either raw or processed data in an\nad hoc fashion.\n1.6.3\nProcessing layer\nAfter data has been saved to cloud storage in its original form, it can now be processed\nto make it more useful. The processing of data is arguably the most interesting part of\nbuilding a data lake. While the data lake’s design makes it possible to perform analysis\ndirectly on the raw data, this may not be the most productive and efficient method.\nUsually, data is transformed to some degree to make it more user-friendly for analysts,\ndata scientists, and others.\n There are several technologies and frameworks available for implementing a pro-\ncessing layer in the cloud data lake, unlike traditional data warehouses, which typically\nlimited you to a SQL engine provided by your database vendor. However, while SQL is\na great query language, it is not a particularly robust programming language. For exam-\nple, it’s difficult to extract common data-cleaning steps into a separate, reusable\nlibrary in pure SQL, simply because it lacks many of the abstraction and modularity\nfeatures of modern programming languages such as Java, Scala, or Python. SQL also\ndoesn’t support unit or integration testing. It’s very difficult to make iterative data\ntransformations or data-cleaning code without good test coverage. Despite these lim-\nitations, SQL is still widely used in data lakes for analyzing data, and in fact many of\nthe data service components provide a SQL interface. \n Another limitation of SQL—in this case, not the language itself, but its implemen-\ntation in RDBMs—is that all data processing must happen inside the database engine.\nThis limits the amount of computational resources available for data processing tasks\nto how many CPU, RAM, or disks are available in a single database server. Even if\nyou’re not processing extremely large data volumes, you may need to process the\nsame data multiple times to satisfy different data transformation or data governance\nrequirements. Having a data processing framework that can scale to handle any\namount of data, along with cloud compute resources you can tap into anytime, makes\nsolving this problem possible. \n Several data processing frameworks have been developed that combine scalability\nwith support for modern programming languages and integrate well into the overall\ncloud paradigm. Most notable among these are\nApache Spark\nApache Beam\nApache Flink\nThere are other, more specialized frameworks out there, but this book will focus on\nthese three. At a high level, each one allows you to write data transformation, valida-\ntion, or cleaning tasks using one of the modern programming languages (usually Java,\nScala, or Python). These frameworks then read the data from scalable cloud storage,\n",
      "content_length": 2915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "13\nBuilding blocks of a cloud data platform\nsplit it into smaller chunks (if the data volume requires it), and finally process these\nchunks using flexible cloud compute resources. \n It’s also important, when thinking about data processing in the data lake, to keep\nin mind the distinction between batch and stream processing. Figure 1.5 shows that\nthe ingestion layer saves data to cloud storage, with the processing layer reading data\nfrom this storage and saving results back to it. \nThis approach works very well for batch processing because while cloud storage is\ninexpensive and scalable, it’s not particularly fast. Reading and writing data can take\nminutes even for moderate volumes of data. More and more use cases now require sig-\nnificantly lower processing times (seconds or less) and are generally solved with\nstream-based data processing. In this case, also shown in the preceding diagram, the\ningestion layer must bypass cloud storage and send data directly to the processing\nlayer. Cloud storage is then used as an archive where data is periodically dumped but\nisn’t used when processing all that streaming data. \n Processing data in the data platform typically includes several distinct steps includ-\ning schema management, data validation, data cleaning, and the production of data\nproducts. We’ll cover these steps in greater detail in chapter 5.\n1.6.4\nServing layer\nThe goal of the serving layer is to prepare data for consumption by end users, be they\npeople or other systems. The increasing demands from a variety of users in most\norganizations who need faster access to more data is a huge IT challenge in that these\nIngest\nStorage\nProcessing\nServing\nCloud data\nwarehouse\nAPIs\nData export\nStream\nExploration, data science experiments\nIn streaming mode, data ingestion\nbypasses storage, and data is sent\ndirectly to the processing layer. \nIn batch mode, data is\nsaved to storage first,\nthen processed.\nFigure 1.5\nProcessing differs between batch and streaming data.\n",
      "content_length": 1987,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "14\nCHAPTER 1\nIntroducing the data platform\nusers often have different (or even no) technology backgrounds. They also typically\nhave different preferences as to which tools they want to use to access and analyze data.\n Business users often want access to reports and dashboards with rich self-service\ncapabilities. The popularity of this use case is such that when we talk about data plat-\nforms, we almost always design them to include a data warehouse. \n Power users and analysts want to run ad hoc SQL queries and get responses in\nseconds. Data scientists and developers want to use the programming languages\nthey’re most comfortable with to prototype new data transformations or build\nmachine learning models and share the results with other team members. Ultimately,\nyou’ll typically have to use different, specialized technologies for different access tasks.\nBut the good news is that the cloud makes it easy for them to coexist in a single\narchitecture. For example, for fast SQL access, you can load data from the lake into a\ncloud data warehouse. \n To provide data lake access to other applications, you can load data from the lake\ninto a fast key/value or document store and point the application to that. And for\ndata science and engineering teams, a cloud data lake provides an environment where\nthey can work with the data directly in cloud storage by using a processing framework\nsuch as Spark, Beam, or Flink. Some cloud vendors also support managed notebook\nenvironments such as Jupyter Notebook or Apache Zeppelin. Teams can use these\nnotebooks to build a collaborative environment where they can share the results of\ntheir experiments along with performing code reviews and other activities.\n The main benefit of the cloud, in this case, is that several of these technologies are\noffered as platform as a service (PaaS), which shifts the operations and support of these\nfunctions to the cloud provider. Many of these services are also offered through a pay-\nas-you-go pricing model, making them more accessible for organizations of any size.\n1.7\nHow the cloud data platform deals with the three V’s\nThe following sections explain how variety, volume, and velocity work with cloud\nplatforms.\n1.7.1\nVariety\nA cloud data platform is well positioned to adapt to all this data variety because of its\nlayered design. The data platform’s ingestion layer can be implemented as a collec-\ntion of tools, each dealing with a specific source system or data type. Or it can be\nimplemented as a single ingestion application with a plug-and-play design that allows\nyou to add and remove support for different source systems as required. For example,\nKafka Connect and Apache NiFi are examples of plug-and-play ingestion layers that\nadapt to different data types. At the storage layer, cloud storage can accept data in any\nformat because it’s a generic file system—meaning you can store JSON, CSV, video,\naudit data, or any other data type. There are no data type limits associated with cloud\nstorage, which means you can introduce new types of data easily.\n Finally, using a modern data processing framework such as Apache Spark or Beam\nmeans you’re no longer confined by the limitations of the SQL programming language.\n",
      "content_length": 3223,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "15\nHow the cloud data platform deals with the three V’s\nUnlike SQL, in Spark you can easily use existing libraries for parsing and processing\npopular file formats or implement a parser yourself if there’s no support for it today.\n1.7.2\nVolume\nThe cloud provides tools that can store, process, and analyze lots of data without a\nlarge, upfront investment in hardware, software, and support. The separation of stor-\nage and compute and pay-as-you-use pricing in the cloud data platform makes han-\ndling large data volumes in the cloud easier and less expensive. Cloud storage is\nelastic, the amount of storage grows and shrinks as you need it, and the many tiers of\npricing for different types of storage (both hot and cold) means you pay only for what\nyou need in terms of both capacity and accessibility.\n On the compute side, processing large volumes of data is also best done in the\ncloud and outside the data warehouse. You’ll likely need a lot of compute capacity to\nclean and validate all this data, and it’s unlikely you’ll be running these jobs continu-\nously, so you can take advantage of the elasticity of the cloud to provision a required\ncluster on demand and destroy it after processing is complete. By running these jobs\nin the data platform but outside the data warehouse, you also won’t negatively impact\nthe performance of the data warehouse for users, and you might also save a substan-\ntial amount of money because the processing will use data from less expensive storage.\n While cloud storage is almost always the least expensive way to store raw data, pro-\ncessed data in a data warehouse is the de facto standard for business users, and the\nsame elasticity applies to cloud data warehouses offered by Google, AWS, and Micro-\nsoft. Cloud data warehouse services such as Google BigQuery, AWS Redshift, and\nAzure Synapse provide either an easy way to scale warehouse capacity up and down on\ndemand, or, like Google BigQuery, introduce the concept of paying only for the\nresources a particular query has consumed. With cloud data lakes, processing large\nvolumes of data is available to budgets of almost any size. These cloud data ware-\nhouses couple on-demand scaling with an almost endless array of pricing options that\ncan fit any budget.\n1.7.3\nVelocity\nThink about running a predictive model to recommend a next best offer (NBO) to a\nuser on your website. A cloud data lake allows the incorporation of streaming data\ningestion and analytics alongside more traditional business intelligence needs such as\ndashboards and reporting. Most modern data processing frameworks have robust sup-\nport for real-time processing, allowing you to bypass the relatively slow cloud storage\nlayer and have your ingestion layer send streaming data directly to the processing layer.\n With elastic cloud compute resources, there’s no longer any need to share real-time\nworkloads with your batch workloads—you can have dedicated processing clusters for\neach use case, or even for different jobs, if needed. The processing layer can then send\ndata to different destinations: to a fast key/value store to be consumed by an\napplication, to cloud storage for archiving purposes, or to a cloud warehouse for\nreporting and ad hoc analytics.\n",
      "content_length": 3235,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "16\nCHAPTER 1\nIntroducing the data platform\n When data scientists become users of your data systems, volume and variety chal-\nlenges come into play all at once. Machine learning models love data—lots and lots of\nit (i.e., volume). Models developed by data scientists usually require access not just to\nthe organized, curated data in the data warehouse, but also to the raw, source-file data\nof all types that’s typically not brought into the data warehouse (i.e., variety). Their\nmodels are compute intensive, and when run against data in a data warehouse, put\nenormous performance pressure on the system. With current data warehouse archi-\ntectures, these models often take hours or even days to run. They also impact ware-\nhouse performance for all other users while they’re running. Giving data scientists\naccess to the high-volume, high-variety data in the data lake makes everyone happier\nand will likely keep costs lower.\n1.7.4\nTwo more V’s \nVeracity and value are two other V’s that should factor into your choice of a data plat-\nform over just a data warehouse. Turning data into value only happens when your data\nusers, be they people, models, or other systems, get timely access to data and use it\neffectively.\n The beauty of a data lake is that you can now give people access to more data. The\ndownside, though, is that you’re also providing access to data that’s not necessarily as\nclean and organized and well governed as it tends to be in a data warehouse. The\nveracity or correctness of the data is a major consideration of any big data project, and\nwhile data governance is a topic big enough for a book of its own, many big data proj-\nects balance the need for data governance (ensuring veracity) with the need for access\nto more data to drive value. This can be accomplished by using the data platform not\njust as a source of raw data to produce governed data sets for the data warehouse, but\nas an ungoverned or lightly governed data repository where users can explore the\nentirety of their data, knowing that it hasn’t yet been blessed for corporate reporting.\nWe increasingly see data governance as an iterative, more Agile process when data\nlakes are involved—where once the exploratory phase is complete and models appear\nto produce a good output, the data moves into the data warehouse, where it becomes\npart of a governed dataset.\n1.8\nCommon use cases\nUnderstanding the various use cases for data platforms is important when you design\nand plan your own data platform. Without this context, you risk winding up with a\ndata swamp that doesn’t actually deliver real business value.\n One of the most common data platform use cases is driven by the need for a 360-\ndegree view of an organization’s customers. Customers engage with or talk about\norganizations in many ways using many different systems, from social media to\ne-commerce to online chats to call center conversations and more. The data from these\nengagements is both structured and unstructured, originates from many different\nsources, and is of varying degrees of quality, volume, and velocity. Integrating all these\n",
      "content_length": 3100,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "17\nCommon use cases\ntouchpoints into a single view of the customer opens the doors to a plethora of improved\nbusiness outcomes—an improved customer experience as they interact with different\nparts of the business, better personalization in marketing, dynamic pricing, reduced\nchurn, improved cross-selling, and much more.\n A second common use case for a data lake is IoT, where data from machines and\nsensors can be combined to create operational insights and efficiencies ranging from\nproactively predicting equipment failures on a factory floor or in the field, to monitor-\ning skier (a person who moves over snow on skis) performance and location via RFID\ntags. The data from sensors tends to be very high volume and with a high degree of\nuncertainty, which makes it well suited for a data lake. Traditional data warehouses\ndon’t just struggle with this data; the sheer amount of data produced in an IoT use case\nmakes a traditional data warehouse-based project extremely expensive and shifts the\nbalance away from a good return on that investment for all but a small number of cases.\n The emergence of advanced analytics using machine learning and AI has also\ndriven the adoption of data lakes, because these techniques require the processing of\nlarge data sets—often much larger than can be cost-effectively stored or processed in a\ndata warehouse. In this regard, the data lake, with its ability to cost-effectively store\nalmost unlimited amounts of raw data, is a data scientist’s dream come true. And the\nability to process that data without impacting performance for other analytic consum-\ners is another big benefit. \nSummary\nPressure from the business to get more accurate insights faster, cheaper, and\nwith an increasing level of confidence is growing along with the volume, veloc-\nity, and variety of data needed to produce these insights. All of this is putting\nimmense pressure on traditional data warehouses and paving the way for the\nemergence of a new solution.\nTraditional data warehouses or data lakes alone can’t meet today’s rapidly\nchanging data requirements, but when combined with new cloud services and\nprocessing frameworks available only in the cloud, they create a powerful and\nflexible analytics data platform that addresses a wide range of use cases and data\nconsumers.\nData platform design revolves around concepts of flexibility and cost effectiveness.\nPublic cloud, with its on-demand storage, compute resources provisioning, and\npay-per-usage pricing model, fits the data platform design perfectly. \nWell-designed data platforms use a loosely coupled architecture where each\nlayer is responsible for a specific function. The foundational building blocks of\na data platform are individual layers designed for ingestion, storage, processing,\nand serving. \nKey use cases for data platforms include a 360-degree view of an organization’s\ncustomers, IoT, and machine learning.\n",
      "content_length": 2912,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "18\nWhy a data platform\n and not just a data\n warehouse\nWe’ve covered what a data platform is (as a reminder, it is “a cloud-native platform\ncapable of cost-effectively ingesting, integrating, transforming, and managing an\nalmost unlimited amount of data of any type data in order to facilitate analytics\noutcomes”), what drives the need for a data platform, and how changes in data will\nshape your data platform. Now we will explore in more detail why a cloud data\nThis chapter covers\nAnswering “Why a data platform?” and “Why build in \nthe cloud?”\nComparing data platform to data warehouse–only \nsolutions\nProcessing differences in structured and semi-\nstructured data\nComparing cloud costs for data warehouse and data \nplatform\n",
      "content_length": 734,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "19\nCloud data platforms and cloud data warehouses: The practical aspects\nplatform provides more capabilities as opposed to a data warehouse–only architecture.\nIn this chapter, we will equip you with the knowledge necessary to make a solid argument\nfor a data platform and will walk you through several examples demonstrating the\ndifference between the two approaches (data warehouse–only and data platform). \n While designing the best data platform is what we want you to be able to do when\nyou’ve finished this book, we also know from experience that knowing the “why”\nbehind your data platform project will not only help you make better decisions along\nthe way, but you’ll also be ready to justify why it makes sense from a business perspec-\ntive to embark on a cloud data platform project. This chapter will equip you with solid\nbusiness and technical reasons for choosing a data platform so you’ll be ready for that\nmoment when someone asks, “Why are you doing this?”\n We’ll use a simple but common analytics use case to demonstrate how a data ware-\nhouse solution compares to a data platform example, introducing some of the key dif-\nferences between the two options. \n We will start by describing two potential architectures: one that is centered around a\ncloud data warehouse only and another one that uses broader design principles to define\na data platform. Then we will walk through examples showing how to load and work with\nthe data in both solutions. We will specifically focus on what happens to the data platform\npipelines when there are changes to source data structure and look at how data platform\narchitecture can help you analyze semistructured data at scale. Because similar outcomes\ncan be achieved by directly ingesting data into the cloud data warehouse, we’ll also walk\nthrough loading and working with the same data in a data warehouse alone.\n We’ll also explore the main difference between delivering and analyzing data in\nthe traditional warehouse versus a data platform environment. You will see how each\nsolution deals with changes to the source schema and how they work with large vol-\numes of semistructured data, such as JSON. We will also compare the cost and perfor-\nmance characteristics of each.\n By the end of this chapter, you’ll understand how a data platform compares to\nmeeting the same business goals with a data warehouse.\n2.1\nCloud data platforms and cloud data warehouses: \nThe practical aspects\nIn this section, we’ll use an example cloud analytics challenge to illustrate the differ-\nences between data platforms and data warehouse architectures. We’ll also introduce\nAzure as the cloud platform for this example and describe an Azure cloud data ware-\nhouse and an Azure data platform architecture.\n Imagine that we’ve been tasked with building a small reporting solution for our\norganization. The Marketing department in our organization has data from their\nemail marketing campaigns that is stored in a relational database—let’s assume it’s a\nMySQL RDBMS for this scenario. They also have clickstream data that captures all\nwebsite user activity that is then stored in a CSV file that is available to us via an inter-\nnal SFTP server. \n",
      "content_length": 3186,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "20\nCHAPTER 2\nWhy a data platform and not just a data warehouse\n Our main analytical goal is to combine campaign data with the clickstream data to\nidentify users who landed on our website using links from specific email marketing\ncampaigns and the parts of the site they visited. In our example, Marketing wants this\nanalysis done repeatedly, so our data pipelines must bring data into our cloud envi-\nronment on a regular basis and must be resilient to changes in the upstream source.\nWe would also like our solution to be performant and cost efficient. As usual, the\ndeadline for this is “yesterday.”\n To illustrate the differences between a data platform approach to dealing with the\nfirst three V’s of data (volume, variety, and velocity) versus a more traditional ware-\nhouse approach, let’s consider two simplified implementations: (1) a data platform\nwith a data warehouse and (2) a traditional data warehouse. We will use Azure as our\ncloud platform of choice for these examples. We could equally have used similar\nexamples on AWS or Google Cloud Platform, but Azure allows us to easily emulate a\ntraditional warehouse with Azure Synapse. Azure Synapse is a fully managed and scal-\nable warehousing solution from Microsoft, based on a very popular MS SQL Server\ndatabase engine. This way, one of our example architectures will be very close to what\nyou might see in an on-premises data warehouse setup.\n2.1.1\nA closer look at the data sources\nOur simplified email marketing campaign data consists of a single table, as shown in\nfigure 2.1. The table includes the unique identifier of the campaign (campaign_id), a\nlist of target email addresses the campaign was sent to (email), a unique code\nincluded in a link to our website for each specific user (unique_code), and a date\nwhen the campaign was sent (send_date). Real marketing automation systems are, of\ncourse, more complex and include many different tables, but this is good enough for\nour purpose.\nClickstream data with its lack of fixed schema is semistructured data derived from web\napplication logs and includes details about the pages visited, information about the\nvisitor’s browser and operating system, session identifiers, etc. \nNOTE\nIn general terms, semistructured data is any data that doesn’t fit nicely\ninto a relational model. This means that semistructured data cannot be repre-\nsented as a flat table with columns and rows, where each cell contains a value\nof a certain type: integers, dates, strings, etc. JSON documents are a common\nexample of semistructured data.\ncampaign_id\nemail\nunique_code\nsend_date\ncampaign_1\nuser1@example.com\n12342\n2019-08-02\ncampaign_2\nuser2@example.com\n34563\n2019-03-05\nFigure 2.1\nExample marketing campaign table\n",
      "content_length": 2723,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "21\nCloud data platforms and cloud data warehouses: The practical aspects\nThese logs will look different depending on the application that was used to generate\nthem. For our example, we will use a simplified representation of the clickstream log,\nshown in figure 2.2, that only includes the details we need for our use case. \n For our scenario, we will assume that clickstream log is a large (hundreds of GBs) text\nfile in CSV format that includes three columns: a UNIX timestamp of when the event\nhappened (timestamp); a content column containing the details about the page URL,\nunique visitor identifier, and browser info (content_json); and other details (other_\ndetails). One thing that you will notice about this example is that the content_json\ncolumn in our hypothetical CSV file is a JSON document with many nested fields. This\nis a common layout for this type of data and will require extra steps to process. \n Our task is illustrated in figure 2.3. It is to design a cloud data platform capable of\nintegrating these two data sources in a performant and cost-efficient manner and to\nmake this integrated data available to the Marketing team for analysis.\n Our goal here is not only to describe two different cloud architectures, but to high-\nlight the important differences between the two, focusing on what happens to the data\ntimestamp\ncontent_json\nother_details\n1565395200 {\nurl:   \n\"https://example.com/campaigns/landing?co\nde=12342\",\nuser_id: 254,\nbrowser_info: {…}\n}\n...\n1565396381 {\nurl:   \"https://example.com/products\",\nuser_id: 254,\nbrowser_info: {…}\n}\n...\nFigure 2.2\nExample clickstream data\nFigure 2.3\nCloud analytics platform problem statement\nClickstream logs\n(CSV, 100s GBs)\nCloud environment\nMySQL\n?\nWe need a scalable, cost-efficient\nsolution that will allow us to\nperform SQL analytics on the\ncombined data.\nUsers\nreports, dashboards\n",
      "content_length": 1860,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "22\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nplatform pipelines when there are changes to source data structure and how the two\narchitectures analyze semistructured data at scale. In the next section, we will walk you\nthrough the solution to our analytics problem (design a cloud data platform capable of\nintegrating these two data sources in a performant and cost-efficient manner and make\nthis integrated data available to the marketing team for analysis) using a cloud data\nwarehouse architecture and then a cloud data platform architecture. \n2.1.2\nAn example cloud data warehouse–only architecture\nA cloud data warehouse architecture is quite similar to a traditional enterprise data\nwarehouse solution. Figure 2.4 shows how the center of this architecture is a relational\ndata warehouse that is responsible for storing, processing, and serving data to the end\nusers. There is also an extract, transform, load (ETL) process that loads the data from\nthe sources (clickstream data via CSV files and email campaign data from a MySQL\ndatabase) into the warehouse. \nOur example cloud data warehouse–only architecture consists of two PaaS services run-\nning on Azure: Azure Data Factory and Azure SQL Data Warehouse (Azure Synapse).\nNOTE\nIn all of our examples we will use platform-as-a-service (PaaS) offerings\nwhere possible. PaaS enables one of the most powerful promises of the\ncloud—getting platforms up and running in minutes, not days.\nAzure Data Factory is a fully managed PaaS ETL service that allows you to create pipe-\nlines by connecting to various data sources, ingesting data, performing basic transfor-\nmations such as uncompressing files or changing file formats, and loading data into a\ntarget system for processing and serving. In our example cloud data warehouse–only\narchitecture, we will use Data Factory to read email campaign data from a MySQL\nFigure 2.4\nExample cloud data warehouse-only architecture on Azure.\nClickstream logs\n(CSV, 100s GBs)\non SFTP server\nAzure Data\nFactory service\nAzure Data\nFactory service\nAzure Synapse\nMarketing\ncampaigns\ndata\nData Factory\nMySQL\nconnection\nData Factory\nSFTP\nconnection\nData Factory\nAzure Synapse\nsink\nData storage and\nprocessing\nIngest\nStore|Process|Serve\nMySQL\nAzure\n",
      "content_length": 2250,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "23\nCloud data platforms and cloud data warehouses: The practical aspects\ntable as well as to fetch files containing clickstream data from an SFTP server. We will\nalso use Data Factory to load data into Azure Synapse.\n Our example warehouse, Azure Synapse, is a fully managed warehouse service\nbased on MS SQL Server technology. Fully managed in this case means that you don’t\nneed to install, configure, and manage the database server yourself. Instead, you need\nonly choose how much computational and storage capacity you require, and Azure will\ntake care of the rest. While there are certain limitations to fully managed PaaS offerings\nsuch as Azure Synapse and Azure Data Factory, they make it very easy for people who\nare not MS SQL Server experts to implement architectures such as our cloud data\nwarehouse–only architecture and to program relatively complex pipelines quickly. \n In the next section, we will describe an alternative architecture—a cloud data plat-\nform design that provides more flexibility than a data warehouse design. \n2.1.3\nAn example cloud data platform architecture\nA cloud data platform architecture is inspired by the concept of a data lake and is in\nfact a combination of a data lake and a data warehouse created for the age of cloud. A\ncloud data platform consists of several layers, each responsible for a particular aspect\nof the data pipeline: ingestion, storage, processing, and serving. Let’s look at our\nexample cloud data platform architecture, which can be seen in figure 2.5.\nOur cloud data platform architecture consists of these Azure PaaS services:\nAzure Data Factory\nAzure Blob Storage\nAzure Synapse\nAzure Databricks\nFigure 2.5\nExample cloud data platform architecture\nClickstream logs\n(CSV, 100s\nGBs) on SFTP\nserver\nAzure Data\nFactory service\nAzure Data\nFactory service\nIngest\nAzure Blob\nStorage\nStore\nAzure\nDatabricks\nProcess\nAzure \nSynapse\nServe\nMarketing\ncampaigns\ndata\nData Factory\nMySQL\nconnection\nData Factory\nSFTP\nconnection\nData Factory\nBlob Storage\nsink\nRead/write\nWrite\nMySQL\nAzure\n",
      "content_length": 2042,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "24\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nWhile they may look similar (both use Azure Data Factory for ingestion and both use\nAzure Synapse for serving), there are several key differences between a cloud data\nwarehouse–only architecture and cloud data platform architecture. In the cloud data\nplatform, while we are using Azure Data Factory to connect and extract data from the\nsource systems instead of loading it directly into the warehouse, we will save the source\ndata into a landing area on Azure Blob storage (often known as “the lake”). This\nallows us to preserve the original data format and helps with data variety challenges as\nwell as provides other benefits.\n Once the data has landed in Azure Blob Storage, we’ll use Apache Spark running\non the Azure Databricks managed service (PaaS) to process it. As with all PaaS ser-\nvices, we get simple setup and ongoing management, allowing us to create new Spark\nclusters without needing to manually install and configure any software. It also pro-\nvides an easy-to-use notebook environment where you can execute Spark commands\nagainst the data in the lake and see the results right away, without having to compile\nand submit Spark programs to the cluster.\n While Spark and other distributed data processing frameworks can help you pro-\ncess various data formats and almost infinitely large data volumes, these tools aren’t\nwell suited for serving what is sometimes called interactive queries. Interactive in this\ncase means that the query response is usually expected within seconds or less, not\nminutes. For these use cases, a well-designed relational warehouse can typically pro-\nvide a faster query performance than can Spark. Also there are many off-the-shelf\nreporting and BI tools that integrate much better with an RDBMS database than with\na distributed system such as Spark and are easier to use for less technical users.\n2.2\nIngesting data\nThis section covers how to load data into Azure Synapse and an Azure data platform\nusing Azure Data Factory. We’ll also look at what happens to ingestion pipelines where\nthe source schema changes.\n Using a managed service like Azure Data Factory makes creating a pipeline to get\ndata into the data platform or data warehouse a relatively easy task. Data Factory\nExercise 2.1\nIn our examples in this section, which of the following is the main difference between\na cloud data warehouse architecture and a data platform architecture?\n1\nData platform uses only serverless technologies.\n2\nData platform uses Azure functions for data ingestion.\n3\nData warehouses can connect to the data sources directly to perform the\ningestion.\n4\nData platform adds a “data lake” layer to offload data processing from the\ndata warehouse.\n",
      "content_length": 2744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "25\nIngesting data\ncomes with a set of built-in connectors to various sources, allows for basic data trans-\nformations, and supports saving data to the most popular destinations. \n There are, however, fundamental differences between how a data ingestion pipe-\nline works in a cloud data platform versus a cloud data warehouse–only implementa-\ntion. In this section, we will highlight these differences. \n2.2.1\nIngesting data directly into Azure Synapse\nAn Azure Data Factory pipeline consists of several key components: (1) linked ser-\nvices, (2) input and output data sets, and (3) activities. Figure 2.6 shows how these\ncomponents work together to load data from a MySQL table into Azure Synapse.\nA linked service describes a connection to a specific data source (in this case, MySQL)\nor data sink (in this case, Azure Synapse). These services would include the location of\nthe data source, credentials that will be used to connect to it, etc. A data set is a spe-\ncific object in the connected service. It could be a table in the MySQL database that\nwe want to read or a destination table in Azure Synapse. An important property of a\ndata set is a schema. For Data Factory to be able to load data from MySQL into Azure\nSynapse, it needs to know the schema of the source and destination tables. This infor-\nmation is required up front, meaning the schema must be available to the pipeline\nbefore the pipeline can be executed. To be more specific, the schema for the input\ndata source can be inferred by the Data Factory automatically, but the output schema,\nand especially the mapping of the input to output schemas, must be provided. The\nData Factory UI provides a quick way to fetch schema from data sources, but if you are\nbuilding pipeline automation using the Data Factory API to construct pipelines, then\nyou need to provide the schema yourself. In the example in figure 2.6, the MySQL\nMySQL linked \nservice\nCampaigns\ninput data set\nCampaigns \noutput data \nset\nAzure SQL data\nwarehouse\nlinked service\nCopy\nactivity\nInput data set schema\ncampaign_id: varchar\nemail: varchar\nunique_code: integer\nsend_data: date\nOutput data set schema\n1. Data Factory will connect to\n    a MySQL source, fetch table\n    schemas, and create an input\n    data set.\n2. Data Factory will convert an input\n    data set into a corresponding output\n    data set based on a data sink type.\n3. Data Factory\n    creates a new table\n    in Azure SQL DW\n    based on the output\n    data set schema.\ncampaign_id: varchar\nemail: varchar\nunique_code: integer\nsend_data: date\nFigure 2.6\nAzure Data Factory ingestion pipeline for Azure Synapse\n",
      "content_length": 2615,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "26\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nsource schema and the Azure Synapse schema are similar, but in other cases this may\nnot be true because of data types mismatch, etc. \n2.2.2\nIngesting data into an Azure data platform\nOur data platform architecture takes a different approach to data ingestion (figure\n2.7). In a data platform architecture, the primary destination for the data is Azure Blob\nstorage, which, for this use case, can be thought of as an infinitely scalable filesystem. \nThe main difference between this ingestion pipeline and the previous one is that\nAzure Blob Storage Data Factory service doesn’t require a schema to be specified up\nfront. In our use case, each ingestion from MySQL is saved as a text file in Azure Blob\nStorage without concern for the source columns and data types. Our cloud data plat-\nform design has an extra layer of processing data, which will be implemented using\nApache Spark running on the Azure Databricks platform to convert source text files\ninto more efficient binary formats. This way we can combine the flexibility of text files\non Azure Blob Storage with the efficiency of binary formats.\n In the data platform design, the fact that you are no longer required to manually\nprovide output schema and its mapping to the source schema is important for two rea-\nsons: (1) a typical relational database can contain hundreds of tables, which translates\ninto a lot of manual effort and increased chance of errors, and (2) it is highly resilient\nto change, the subject of the next section. \n2.2.3\nManaging changes in upstream data sources\nSource datasets are never static. Developers who support our marketing campaign\nmanagement software will be constantly adding new features. This can result in new\ncolumns added to the source table and/or columns being renamed or deleted. Build-\ning data ingestion and processing pipelines that deal with these types of changes is\none of the most important tasks for a data architect or a data engineer. \nMySQL linked\nservice\nCampaigns\ninput data set\nCampaigns\noutput data\nset\nAzure Blob\nStorage linked\nservice\nCopy\nactivity\ncampaign_id: varchar\nemail: varchar\nunique_code: integer\nsend_data: date\nInput data set schema\n2. Data Factory will convert an input\n    data set into a corresponding\n    output data set based on a data\n    sink type.\n3. Data Factory creates a new text\n    file on Azure Blob Storage. No\n    schema is required for this\n    operation.\n1. Data Factory will connect to a\n    MySQL source, fetch table\n    schemas, and create an input\n    data set.\nFigure 2.7\nAzure Data Factory ingestion pipeline for a cloud data platform\n",
      "content_length": 2652,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "27\nIngesting data\nLet’s imagine that a new version of our email marketing software introduced a new\ncolumn called “country” in the source data set. What will happen to our ingestion\npipeline in our data warehouse–only architecture? Figure 2.8 explains.\n Our data warehouse–only Data Factory pipeline requires both input and output\nschemas, so, without intervention, a change in the source schema means our two sche-\nmas will be out of sync. Data Factory maps input to output columns by position\nbecause it supports column renaming in the output. This means the next time the\ningestion pipeline runs, inserts into Azure Synapse will fail, because it will expect an\ninteger unique_code column where a varchar region column will arrive from source.\nAn operator will need to go and adjust the output data set schema manually and\nrestart the pipeline. We will discuss schema management in great detail in chapter 8. \nNOTE\nData Factory, like any generic ETL overlay tool, will allow you to copy\ndata from a variety of sources into a variety of destinations. Some of these des-\ntinations, such as Azure Blob Storage, don’t care about input schema, but\nothers, such as databases and Azure Synapse, require a strict schema to be\ndefined up front. While you can alter the destination schema and add new\ncolumns, the behavior of this operation will depend on the type of destina-\ntion. Some databases will lock the full table for the duration of the schema\nchange, making it completely unavailable to end users. In other cases, space\ndepending on data size in the table, a schema adjustment operation can take\nhours to run. There is no single way to deal with schema changes in the multi-\ntude of data destinations in a unified way, so Data Factory and other ETL\ntools delegate this responsibility to the platform operators. \nResilience to the upstream schema changes is one of the benefits of the Data Platform\narchitecture over a data warehouse–only approach. As shown in figure 2.9, in a data\nMySQL linked\nservice\nCampaigns\ninput data set\nCampaigns\noutput data\nset\nAzure SQL\nDW linked\nservice\nCopy\nactivity\ncampaign_id: varchar\nemail: varchar\nregion: varchar\nunique_code: integer\nsend_data: date\nInput data set schema\ncampaign_id: varchar\nemail: varchar\nunique_code: integer\nsend_data: date\nOutput data set schema\nA new column was added to the source MySQL\ntable. Input data set schema is adjusted\nautomatically, but output data set is not.\nFigure 2.8\nUpstream schema changes break our data warehouse ingestion pipeline.\n",
      "content_length": 2513,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "28\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nplatform implementation, if the source schema changes, since the output destination\nis a file on Azure Blob Storage, the output schema is not required, and the ingestion\npipeline will simply create a new file with a new schema and continue working. \n While a data platform ingestion pipeline will continue to work as expected if the\nupstream schema changes, there are other problems that come with source schema\nchanges. At some point along the way, a consumer of the data, either an analytics user\nor a scheduled job, will need to work with the changed data, and to work with it they\nwill need to know that a new column was added. We will look into approaches on deal-\ning with schema changes in the data platform in chapter 8.\n2.3\nProcessing data\nNow that we’ve covered some of the key ideas involved with ingestion, let’s consider\nprocessing—specifically, how processing data to answer our analytics problem differs\nwhen using SQL running on a data warehouse versus using Apache Spark on a cloud\ndata platform. We’ll discuss the pros and cons of both approaches.\nMySQL linked\nservice\nCampaigns\ninput data set\nCampaigns\noutput data\nset\nAzure Blob\nStorage linked\nservice\nCopy\nactivity\ncampaign_id: varchar\nemail: varchar\nregion: varchar\nunique_code: integer\nsend_data: date\nInput data set schema (inferred)\nNo output schema means the\npipeline will continue to work\nas expected if the upstream\nschema changes.\nFigure 2.9\nData platform ingestion pipeline is resilient to upstream schema changes.\nExercise 2.2\nOur example data platform architecture is more resilient to the changes in the\nupstream data sources because (choose one)\n1\nIt uses Apache Spark for data processing, which utilizes Resilient Distributed\nDatasets (RDDs).\n2\nIt saves data into Azure Blob Storage first, which doesn’t require strict schema\ndefinition.\n3\nIt uses Azure Blob Storage as a primary data store, which provides extra\nredundancy.\n4\nIt uses Azure Functions for data ingestion, which provides extra flexibility.\n",
      "content_length": 2053,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "29\nProcessing data\n We saw in the previous section that ingesting data into the data warehouse and\ndata platform require different approaches. The differences don’t stop there. In this\nsection, we will explore how processing data to answer analytical questions is different\nin the two systems. \n Let’s recall the two data sources we are using as an example use case. First, we have\nthe marketing campaign table in figure 2.10, with the following columns.\nWe also have clickstream data from our website that comes to us in the following semi-\nstructured format (figure 2.11).\nWhile we have data split into individual columns, we also have a content_json column\nthat contains a complex JSON document with multiple attributes and nested values. \n To demonstrate how the approach to working with such data in the data ware-\nhouse and data platform environments differs, let’s consider the following request for\ninformation from our Marketing team: When users land on our website from cam-\npaign X, what other pages do they visit? Our example data set may be artificial, but\nthis type of request is a common one. \n2.3.1\nProcessing data in the warehouse\nSince we are using Azure Synapse as the destination for our data source in the ware-\nhouse-only design, we will need to work with two relational tables that we have loaded\nusing our ingestion process—one for clickstream data and one for email marketing\ncampaign data. Let’s assume tables in our Azure Synapse are called campaigns and\nclicks for campaigns information and clickstream data, correspondingly. A campaigns\ncampaign_id\nemail\nunique_code\nsend_date\ncampaign_1\nuser1@example.com\n12342\n2019-08-02\ncampaign_2\nuser2@example.com\n34563\n2019-03-05\nFigure 2.10\nExample marketing campaigns table\ntimestamp\ncontent_json\nother_details\n1565395200 {\nurl:   \n\"https://example.com/campaigns/landing?co\nde=12342\",\nuser_id: 254,\nbrowser_info: {…}\n}\n...\n1565396381 {\nurl:   \"https://example.com/products\",\nuser_id: 254,\nbrowser_info: {…}\n}\n...\nFigure 2.11\nExample clickstream data\n",
      "content_length": 2020,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "30\nCHAPTER 2\nWhy a data platform and not just a data warehouse\ntable is a straightforward mapping of our source table to the destination table since\nboth data sources are relational in nature. Azure Synapse will contain the same col-\numns. What about clickstream data and its nested JSON documents? There is cur-\nrently no dedicated data types in Azure Synapse to represent JSON (note that some\ncloud data warehouses such as Google BigQuery have native support for JSON data\nstructures), but you can store JSON documents in standard text columns and use a\nbuilt-in JSON_VALUE function to access specific attributes inside the document. \n The following listing is an example of an Azure Synapse query that can answer our\nMarketing request.\nSELECT\n  DISTINCT SUBSTRING(\n    JSON_VALUE(CL.content_json, '$.url'),\n    1,\n    CHARINDEX('?', JSON_VALUE(CL.content_json, '$.url'))\n  ) as landing_url, \n  SUBSTRING(\n    JSON_VALUE(CL1.content_json, '$.url'),\n    1,\n    CHARINDEX('?', JSON_VALUE(CL1.content_json, '$.url'))\n  ) as follow_url\nFROM\n  clicks CL\n  JOIN campaigns CM ON CM.unique_code = SUBSTRING(\n    JSON_VALUE(CL.content_json, '$.url'),\n    CHARINDEX(\n      'code =',\n      JSON_VALUE(CL.content_json, '$.url') + 5),\n      LEN(JSON_VALUE(CL.content_json, '$.url')\n    )\n  )    \n  LEFT JOIN (\n    SELECT\n      JSON_VALUE(CL_INNER.content_json, '$.user_id') as user_id    \n    FROM\n      clicks CL_INNER\n  ) AS CL1 ON JSON_VALUE(CL.content_json, '$.user_id') = CL1.user_id\nWHERE\n  CM.campaign_id = 'campaign_1'\nThis query joins two tables in the data warehouse using a unique_code column from\nthe campaigns table and a portion of the URL from the clicks table that contains this\nunique code for our landing page. Notice that we need to use a complex string pars-\ning construct in the first join to extract the campaign code from the url attribute of\nthe JSON document. Then we join a subquery that will allow us to find all other pages\nthat the same user has visited using a user_id porting of the url. Again, we need to use\ncomplex string parsing to extract values we want.\nListing 2.1\nAzure Synapse query\nExtracts a URL portion only from a \ncontent_json.url attribute by finding the \n“?” character in the URL and extracting \nall preceding characters\nExtracts campaign code from content_json.url \nattribute and uses it in a JOIN\nExtracts a user_id\nattribute from the\ncontent_json JSON\ndocument\n",
      "content_length": 2400,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "31\nProcessing data\n If you have been working with relational technologies for some time, this query\nmost likely is not the most complex SQL you have seen. What makes it hard to read\nand understand is all the extra logic that is needed to parse and extract value from a\nJSON document. There are ways to make this query simpler—by preparsing the\nincoming data as a part of the ETL pipeline or by implementing custom user-defined\nfunctions (UDFs) to make the query more readable. Both approaches have their lim-\nitations. Preparsing may introduce significant ETL processing times and, subse-\nquently, costs, if your data volumes are large—and clickstream data tends to get very\nbig over time. Implementing custom user-defined functions requires you to have a\nseparate development process to maintain and deploy UDFs to each instance of Azure\nSynapse that you need to work with. \n Besides being not very readable, this SQL also suffers from an issue that is com-\nmon to any SQL-based pipeline—the difficulty of testing it. Our string-parsing logic is\ncomplicated and depends on certain characters in the URL to be in specific places so\nwe can extract parameters like unique_code or user_id. It’s very easy to make a mis-\ntake in one of those expressions or to run into a number of edge cases that will break\nthe logic. Lack of tests means that you will end up relying on your users to find these\nissues for you. Not the best way to build trust with your user community. \n Another challenge with running this type of SQL on Azure Synapse is that you\ncan’t really use a number of performance optimizations that make Azure Synapse a\ngreat warehouse. Azure Synapse uses a columnar storage that allows columns that usu-\nally contain numbers or short text to be compressed and require less reads from disk\nwhile running a query. With JSON values you lose this optimization because you never\nwork with a document as a whole, but rather need to parse it and access individual\nattributes. This negatively impacts query performance, especially as data size grows.\nYou can read more about various cloud data warehouse features in chapter 9. \n2.3.2\nProcessing data in the data platform\nCloud offers multiple ways to process data at any scale outside of the data warehouse\nusing a number of distributed data processing engines. Apache Spark is one of the\nmost popular and widely adopted distributed data processing engines. All cloud ven-\ndors offer some sort of a managed service to run Spark jobs without having to worry\nabout cluster deployment and configuration. Azure offers a managed environment\nfor Spark that is based on Databricks (https://databricks.com/)—a commercial offer-\ning from the team that created Apache Spark.\n In our example data platform implementation, Spark running on an Azure Data-\nbricks platform is responsible for all the data processing. Using Apache Spark gives\nyou a choice of either using SQL to process your data or using a general purpose lan-\nguage such as Python or Scala to write more flexible, readable, and testable programs.\nThis way you can pick the best API that fits your needs. You can use SQL for simple\nreports and ad hoc analytics since it’s quick to write and experiment with. When it\ncomes to code that you are planning to maintain long term or that must be modular\nand testable, then you can use a Python (see listing 2.2) or Scala API. \n",
      "content_length": 3373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "32\nCHAPTER 2\nWhy a data platform and not just a data warehouse\n Our example analytics request can be written in Spark as follows:\nfrom pyspark.sql.functions import from_json, substring_index\ndef get_full_url(json_column):   \n    # extract full URL value from a JSON Column\n    url_full = from_json(json_column, “url STRING”)\n    return url_full\n    \ndef extract_url(json_column):\n    url_full = get_full_url(json_column)\n    url = substring_index(url_full, “?”, 1)  \n    return url\ndef extract_campaign_code(json_column):\n    url_full = get_full_url(json_column)\n    code = substring_index(url_full, “?”, -1)\n    return substring_index(code, “=”, -1)\ncampaigns_df = … # Use either Spark SQL or Spark Python API to get the \n➥ Dataframe\nclicks_df = … # Use either Spark SQL or Spark Python API to get the Dataframe\nresult_df = campaigns_df.join(...)\nWe have omitted some code here that performs the actual join since it’s very similar to\nwhat you have seen in our SQL example and in fact can be written in SQL as well.\nWhat we would like to demonstrate here is that extracting campaign code has been\nfactored out into a separate Python function. This function contains comments that\nmake the code easier to understand. You can also use standard Python testing capabil-\nities to write a comprehensive suite of tests to validate all the edge cases. If you need to\nuse this URL parsing logic in other places in your pipeline, you can easily put it into its\nown library together with other common functions. \n While having more modular and testable code may not sound like a huge advan-\ntage of a data platform over a data warehouse, it is extremely important, in our experi-\nence. Once your platform grows to several dozen use cases with several data engineers\nworking on it, maintaining a code base that is well organized and understood by every-\none is critical to your success. \nListing 2.2\nExample Apache Spark implementation using Python API\nSplits URL parsing logic into small \nfunctions that are easy to test\nExtracts URL portion \n(without parameters) but \ntaking a substring from the first \ncharacter to the “?” character\nExtracts campaign code portion of the \nURL by taking a substring from the \nlast character in the string to the “?” \ncharacter\nReturns the unique code portion of a “code=XYZ” string\nExercise 2.3\nUsing Spark instead of SQL for complex data processing gives you which key advantage?\n1\nSpark allows you to write modular and reusable code.\n2\nSpark is always much faster than SQL.\n3\nSpark uses machine learning to make complex code simpler.\n",
      "content_length": 2560,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "33\nAccessing data\n2.4\nAccessing data\nIn this section, we’ll review the tools that are available for end users to access data in\nthe data warehouse and data platform.\n Our data warehouse–only and data platform architectures offer different ways for\nend users to access the data. In the previous section, we described an example analyti-\ncal use case that can be implemented either as a pure SQL pipeline in Azure Synapse\nor as a Spark job running on the data platform. Both approaches will produce a new\ndata set, containing all pages a particular user visits after landing on a specific market-\ning campaign page. \n This new data set might be of interest in different ways to different users inside\nyour organization. For example:\nMarketing team—Often business users who want to consume the data in a dash-\nboard that lists the top 10 pages visited for each campaign\nData analyst—A power user, who may need to slice and dice data in multiple dif-\nferent ways\nData scientist—An uber power user who may need to categorize users into differ-\nent profiles based on pages visited\nDifferent users will want to consume data in different ways. Reporting and dashboard-\ning tools like Power BI are often preferred by business users and work best with rela-\ntional technologies using a SQL interface. For this use case, Azure Synapse will\nprovide the simplest integration and best performance. \n For many power users or data analysts, SQL is the primary analytical instrument.\nThese users will also benefit from having the resulting data set in Azure Synapse. On\nthe other hand, using a service such as Azure Databricks can allow them to easily run\nSpark SQL even if they aren’t familiar with Spark—bringing them processing perfor-\nmance at scale. \n Data science use cases will require access to raw clickstream data, not just the\nresulting data set described in the previous section. For example, your data science\nteam may be working on a model that categorizes users into several common arche-\ntypes based on their behavior. For this model to work properly, it needs to take into\naccount as many details as possible about how users are browsing your website. The\nmost accurate categorizations are often created using data that might not be the most\nobvious choice to humans, so maximizing the data available to the models used by\ndata scientists is important, and aggregated and cleaned up data that is usually loaded\ninto a warehouse does not contain enough of these details. That’s why tools such as\nSpark and access to files on Azure Blob Storage will be required for data scientists to\ndo their job most efficiently.\n While a data warehouse alone won’t satisfy all these groups of users, a cloud data\nplatform that also includes a data warehouse can. In a cloud data platform, the data\nwarehouse can be used as a destination for your resulting data sets, and your raw data\ncan be made available in storage.\n",
      "content_length": 2907,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "34\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nThis flexibility is demonstrated in the architecture we first looked at in section 2.1.3,\nshown again in figure 2.12. \n We will explore this idea in greater detail in chapter 3. \n2.5\nCloud cost considerations\nThis section covers cloud cost control options for the data warehouse and data\nplatform.\n Our comparison of data warehouse and data platform architecture would not be\ncomplete if we didn’t talk about cloud costs. After all, you are likely to get questions\nabout cloud costs soon after launching your cloud platform—usually after the first\nmonth, when the first bills come in :). \n It is difficult to compare specific costs for different services and implementations\neven within a single cloud provider, not to mention across different providers. You are\nlikely to find that many supporting services, such as cloud storage, are relatively inex-\npensive, and services that require heavy computation will drive the bulk of the costs.\nDetermining which of the services you are considering for your architecture are the\nbiggest cost drivers is a good first step. \n Once you know which service or services are contributing the most cost, you\nshould check whether this service supports truly elastic scaling. The idea of an elastic\nservice is that you can use as much of it as you need for only as long as you need it, and\nthen scale it down when you don’t need it, so your overall cost is optimized. Many\ncloud services would claim that they can do this, but you as an architect for your plat-\nform must be able to validate these claims and understand any trade-offs.\n For our cloud data warehouse–only example, the service that drives most of the cost\nis Azure Synapse. This is not surprising since it does all the processing in this design.\nFigure 2.12\nThe data warehouse becomes just another component in a data platform architecture.\nClickstream logs\n(CSV, 100s GBs)\non SFTP server\nAzure Data\nFactory pipeline\nIngest\nAzure Blob\nStorage\nStore\nAzure\nDatabricks\nProcess\nAzure \nSynapse\nServe\nAzure Data\nFactory pipeline\nData scientists will\nuse raw files on Blob\nStorage and run Spark\njobs on Azure Databricks.\nDashboarding tools\nand power users will\nbenefit from SQL access\nusing Azure Synapse.\nMySQL\nAzure\n",
      "content_length": 2274,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "35\nCloud cost considerations\nWhile Azure Synapse can be scaled up and down in terms of processing capacity using\nthe Azure API, scaling takes time (tens of minutes sometimes) and during that time,\nthe entire warehouse is unavailable. So does Azure Synapse support truly elastic scal-\ning? Not exactly. Scaling Azure Synapse is something you can schedule to do every\nnight if your users don’t require 24/7 access to the data, but it is not something you\nwould be able to do multiple times a day, because that would be very disruptive.\nAnother challenge is that you can’t create multiple instances of Azure Synapse that can\nwork with the same data without having to copy data from one instance to another,\nwhich would take time as well. \n In our cloud data platform design, we have Azure Blob Storage as our primary\nstorage, and we use Spark running on Azure Databricks to do the processing. Data-\nbricks will copy the data you need to process to the virtual machines running Spark\nand will store it in memory. This copy does add some overhead when launching new\njobs, but the benefit is that you can create multiple Spark clusters working with the\nsame data. Those clusters can be of different sizes for different processing needs and\ncan be terminated when not used to save cost. From an elasticity perspective, Azure\nDatabricks provides better cost control options than Azure Synapse. It’s not\nuncommon to see a dedicated cluster be provisioned just to run a heavy SparkSQL\nquery and then be torn down.\n There are, of course, a lot more nuances when it comes to cloud costs, and we will\ntalk more about this important topic in chapter 11.\n To summarize the discussion around data warehouse and data platform design, we\nhave put together table 2.1 to outline use cases where you might want to pick one\ndesign over another. \nTable 2.1\nUse cases for a data warehouse–only vs. a data platform\nData warehouse–only design\nData platform design\nYou only have a relational data source.\nYou have multiple data sources with structured and \nsemistructured data.\nYou have control over your source data and a \nprocess in place to manage schema changes.\nYour want to ingest and use data from multiple data \nsources, i.e., spreadsheets or SaaS products over \nwhich you don’t have full control.\nYour use cases are constrained to BI reports \nand interactive SQL queries.\nYou want to be able to use your data for machine \nlearning and data science use cases in addition to \ntraditional BI and data analytics.\nYou have a limited community of data users.\nYou have more and more users in your organization \nrequiring access to the data to do their job.\nYour data volumes are small enough to justify \nthe cost of storing and processing all data \ninside a cloud data warehouse.\nYou want to optimize your cloud costs by using different \ncloud services for storing and processing data.\n",
      "content_length": 2861,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "36\nCHAPTER 2\nWhy a data platform and not just a data warehouse\nSummary\nPracticing using an RDBMS data source and a data file that contains a JSON\ndocument will give you a good understanding of the challenges associated with\nthe variety of data that is common in a modern analytic platform.\nOne of the most obvious differences between a data platform and a data\nwarehouse–only implementation is how they handle schema changes. In a data\nplatform, unlike with a data warehouse–only design, there is no need to provide\na schema for the incoming data, making it much easier to handle schema\nchanges in a data platform than in a data warehouse. \nA second key difference between a data platform and a data warehouse–only\ndesign is where processing takes place. SQL processing in a data warehouse may\nseem simple, but a lack of available testing frameworks, scalability complexity,\nand restrictions on performance optimization may become problems as data\nvariety and volume increase.\nIn a data platform, distributed data processing engines such as Spark offer huge\nflexibility when dealing with large, semistructured data sets because Spark can\nsplit the file into smaller chunks and use multiple parallel tasks to process each\nchunk. This provides the scalability we are looking for in a big data system.\nData platforms that also include a data warehouse bring maximum flexibility\nfor users, allowing power users access to the data in the lake via Spark jobs as\nwell as SQL-based access to data in the data warehouse, while less technical\nusers can enjoy access using a plethora of commercial tools that can connect via\nSQL to the data warehouse.\nUsing PaaS services in your analytic platform design will minimize ongoing sup-\nport costs and time and make setup much faster.\nThe clear separation between storage and compute in a data platform design\nbrings cost flexibility because storage and compute are charged for separately\nin a cloud data platform architecture, and each one can be optimized for cost\nindependently of the other.\nThe combination of a data warehouse and data platform in the same architec-\nture brings better performance, more options for user access, and lower costs\nthan a data warehouse alone.\n2.6\nExercise answers\nExercise 2.1:\n 4—Data platform adds a “data lake” layer to offload data processing from the\nwarehouse.\nExercise 2.2:\n 2—It saves data into Azure Blob Storage first, which doesn’t require strict schema\ndefinition.\nExercise 2.3:\n 1—Spark allows you to write modular and reusable code.\n",
      "content_length": 2524,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "37\nGetting bigger and\n leveraging the Big 3:\n Amazon, Microsoft\n Azure, and Google\nChapter 2 covered setting up a simple data platform made up of a data lake and a\ndata warehouse in the cloud, with simple batch pipelines to ingest data. It also laid\nout the pros and cons of a data lake versus a data warehouse versus a combination\nof the two to produce the best analysis outcomes.\nThis chapter covers\nDesigning a flexible and scalable six-layer data \nplatform architecture\nUnderstanding how layers support both batch and \nstreaming data\nEnsuring the right foundational components for \neasier management\nImplementing a modern cloud data platform in \nAWS, Google, or Azure\n",
      "content_length": 676,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "38\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n In this chapter, we’ll build on the data platform architecture concepts introduced in\nchapters 1 and 2, and we’ll layer on top of those some of the critical and more advanced\nfunctionality needed for most data platforms today. Without this added layer of sophis-\ntication, your data platform would work, but it wouldn’t scale easily, nor would it meet\nthe growing data velocity challenges discussed in chapter 1. It would also be limited in\nterms of the types of data consumers (people and systems who consume the data from\nthe platform) it supports, as they too are growing in both numbers and variety.\n We will take a deeper dive into a more complex cloud data platform architecture,\nexploring which functional layers exist in modern platform architectures and the\nroles they play. We’ll also introduce the concepts of fast/slow storage, streaming ver-\nsuss batch, metadata management, ETL overlays, and data consumers. \n In a world that has new tools and services being announced almost daily, we’ll also\nlook at some of the tools and services that are available to you to bring into your data\nplatform planning. We’ll map the functional layers to existing tools and services avail-\nable from the three major public cloud providers—Amazon, Google, and Microsoft—\nand then we’ll take a look at some of the tooling that is independent of cloud provid-\ners, including both open source and commercial offerings. \n We know that choosing a cloud provider is a decision that hinges on far more than\nwhat services they offer, and the truth is that you can build and operate a great data\nplatform on any of the three cloud providers discussed here. Our recommendations\non tools are intended to help narrow your search and free up some of your time to\nstart working on your design.\nNOTE\nThere is an ongoing debate about the trade-offs between using cloud\nvendor–specific services versus trying to build a platform that is cloud vendor\nindependent. Going cloud vendor–specific and using PaaS keeps support\ncosts low but makes migrating your platform from one cloud vendor to\nanother challenging. Using non-vendor-specific services, such as open source\nsoftware, brings portability but increases the management burden. There is\nno easy answer, but to date our experience has been that the benefits of going\ncloud vendor–specific and using PaaS outweighs the benefits of being cloud\nvendor independent.\nThat’s a lot in one chapter, but this is where your data platform design gets really\ninteresting. \n3.1\nCloud data platform layered architecture\nWe’ll start by reminding you of the simple high-level data platform architecture we\nintroduced in chapter 1; then we’ll build on that design to introduce you to the six\nlayers of our more complex data platform, which coordinate work across multiple lay-\ners: a data ingestion layer, a data processing layer, a metadata layer, a serving layer, and\ntwo overlay layers (the orchestration and ETL layers).\n We’ll discuss what they do and why, along with some tips that we’ve learned by\nimplementing these in real life. We’ll also talk about why it’s a good idea from an\narchitecture perspective for these layers to be “loosely coupled,” or separate layers\n",
      "content_length": 3280,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "39\nCloud data platform layered architecture\nthat communicate through a well-defined interface and don’t depend on the internal\nimplementation of a specific layer. \n In chapter 1 we introduced a very high-level architecture of a data platform. It had\nfour layers (ingestion, storage, processing, and serving) and looked like figure 3.1.\nAlso in chapter 1, we discussed each of these architecture components and what their\ncore functions are, so if you skipped that bit you might want to go back and review\nthem because, in this chapter, we will expand on this data platform architecture and\nprovide more details about specific functional components. Figure 3.2 shows a more\nsophisticated data platform architecture that builds on our simpler version from chap-\nter 1 and expands it into six layers.\n These layers are as follows:\nIn the ingestion layer (1), we’re showing a distinction between batch and\nstreaming ingest.\nIn our storage layer (2), we’re introducing the concept of slow and fast storage\noptions.\nIn our processing layer (3), we discuss how it will work with batch and stream-\ning data, fast and slow storage.\nWe’ve added a new metadata layer (4) to enhance our processing layer.\nWe’ve expanded the serving layer (5) to go beyond a data warehouse to include\nother data consumers.\nWe’ve added an overlay layer (6) for ETL and/or orchestration.\nA layer is a functional component that performs a specific task in the data platform\nsystem. In practical terms, a layer is either a cloud service, an open source or commer-\ncial tool, or an application component that you have implemented yourself. Very\noften, it’s a combination of several such components. Let’s go through each of these\nlayers in more detail.\nIngest\nStorage\nProcessing\nServing\nCloud data\nwarehouse\nAPIs\nData export\nStream\nExploration, data science experiments\nFigure 3.1\nThe four-layer high-level data platform architecture introduced in chapter 1\n",
      "content_length": 1929,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "40\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n3.1.1\nData ingestion layer\nThe name of this layer is self-explanatory—it is responsible for connecting to the\nsource systems and bringing data into the data platform. There is, of course, much\nmore happening in this layer (see figure 3.3). \nFigure 3.2\nCloud data platform six-layered architecture\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nOperational\nmetadata\nThe ingestion layer connects to\nsource systems and brings data\ninto the data lake, preserving\noriginal data formats.\nBuild your ingestion\nlayer to support\nbatch and stream\ningests as first-class\ncitizens for maximum\nflexibility.\nBatch\ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.3\nThe data ingestion layer connects to the source system and brings data into the data \nplatform.\n",
      "content_length": 1270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "41\nCloud data platform layered architecture\nThe data ingestion layer should be able to perform the following tasks:\nSecurely connect to a wide variety of data sources—in streaming and/or batch\nmodes.\nTransfer data from the source to the data platform without applying significant\nchanges to the data itself or its format. Preserving raw data in the lake is import-\nant for cases where data needs to be reprocessed later, without having to reach\nout to the source again.\nRegister statistics and ingestion status in the metadata repository. For example,\nit’s important to know how much data has been ingested either in a given batch\nor within a specific time frame if it’s a streaming data source. \nYou can see in figure 3.4 that our architecture diagram has both batch and streaming\ningestion coming into the ingestion layer.\nYou may be hearing that the data processing world is moving (or has already moved,\ndepending on who you talk to) to data streaming and real-time solutions. While we\nagree that this is the direction the industry is heading, we also can’t ignore the current\nreality. In our experience, there are many existing data stores today that only support\nbatch data access. This is especially true for third-party data sources that are often\ndelivered as files on FTP in CSV, JSON, or XML format or other systems with batch-\nonly access patterns. \n For the data sources you control, such as your operational RDBMS, it is possible to\nimplement a full streaming solution. It’s significantly harder, if not impossible, to\nachieve the same with third-party sources. And because third-party sources often com-\nprise a large portion of all the data that needs to be brought into the data platform,\nboth batch and streaming ingestion are likely to be part of your data platform design.\nUnlike with a lambda\narchitecture, here\nstreaming and batch\ndata go through\ndifferent pipelines.\nUse streaming\ndata for one-\nevent-at-a-time\ndata access. \nUse batch for CSV,\nJSON, and XML \nfiles on FTP \nservers, and systems \nwith batch-only \naccess patterns. \nBatch\ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.4\nThe ingestion layer should support streaming and batch ingestion.\n",
      "content_length": 2406,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "42\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n We believe that it’s sound architectural thinking to build your data ingestion layer\nto support both batch and streaming ingestion as first-class citizens. This means using\nappropriate tools for different types of data sources. For example, instead of creating\na true streaming capability, you could ingest streaming data as a series of small\nbatches, but you will give up the ability to perform real-time analytics in the future.\nOur design should prevent such technical debt. This way, you will always be able to\ningest any data source, no matter where it comes from. And this is one of the most\nimportant characteristics of a data platform. \nNOTE\nWe often hear from companies that their data platform must be “real-\ntime,” but we’ve learned that it’s important to unpack what real-time means\nwhen it comes to analytics. In our experience, there are often two different\ninterpretations: making data available for analysis as soon as it’s produced at the\nsource (i.e., real-time ingestion) or immediately analyzing and taking action on\ndata that has been ingested in real time (i.e., real-time analytics). Fraud detec-\ntion and real-time recommendation systems are good examples of real-time\nanalytics. Most of our customers actually don’t need real-time analytics, at least\nnot yet. They simply want to ensure that the data they use to produce insights\nis up to date, meaning as current as a few minutes or hours ago, even though\nthe report or dashboard may only be looked at periodically. Given that real-time\nanalytics is much more complex than real-time ingestion, it’s often worth\nexploring user needs in detail to fully understand how to best architect your\ndata platform.\nSome of you may look at the cloud data platform architecture diagram in this chapter,\nsee two ingestion paths, one for batch and one for stream, and ask if this is an exam-\nple of a lambda architecture.\nNOTE\nFor those of you who are not familiar with lambda architecture, we rec-\nommend this great resource on the topic: http://lambda-architecture.net/.\nIn a nutshell, a lambda architecture suggests that, in order to provide accurate analytical\nresults combined with low-latency analytical results, a data platform must support both\nbatch and streaming data processing paths. The difference between lambda architec-\nture and the cloud data platform architecture described here is that in lambda, the same\ndata goes through two different pipelines, and in our data platform architecture, batch\ngoes through one pipeline and streaming through a different one.\n The lambda architecture was conceived in the early days of Hadoop implementations,\nwhen it was impossible to build a fully resilient and accurate real-time pipeline. The fast\npath provided low-latency results, but due to limitations of some of the streaming frame-\nworks available in Hadoop-based platforms, these results were not always 100% accurate.\nTo reconcile the potential differences in results, the same data is pushed through a batch\nlayer and reconciled to produce a completely correct result at a later stage. \n Today with cloud services such as Cloud Dataflow from Google or open source\nsolutions such as Kafka Streams, these limitations have largely been overcome. Other\n",
      "content_length": 3316,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "43\nCloud data platform layered architecture\nframeworks such as Spark Streaming have also made significant improvements to\naccuracy and handling failures. So in our proposed cloud data platform architecture,\nbatch and stream ingestions paths are actually intended for completely different data\nsources. If a data source doesn’t support real-time data access, or the nature of the\ndata is such that it arrives only periodically, it’s easier and more efficient to push this\ndata through the batch path. Other sources that support one-event-at-a-time data\naccess, such as streaming, should go through the real-time layer. \n Delivering data into the lake efficiently and reliably requires a data ingestion layer\nto have four important properties:\nPluggable architecture—New types of data sources are added all the time. It’s\nunrealistic to expect that connectors for every data source will be available in\nthe ingestion tool or service that you choose. Make sure that your data inges-\ntion layer allows you to add new connector types without significant effort.\nScalability—The data ingestion layer should be able to handle large volumes of data\nand scale beyond a single computer capacity. You may not need all this scale today,\nbut you should always plan ahead and choose solutions that will not require you to\ncompletely revamp your data ingestion layer when you need to grow. \nHigh availability—The data ingestion layer should be able to handle the failure\nof individual components, such as disk, network, or full virtual machine failures\nand still be able to deliver data to the data platform. \nObservability—The data ingestion layer should expose critical metrics, like data\nthroughput and latency, to external monitoring tools. Most of these metrics\nshould be stored in the central metadata repository that we will discuss later in\nthis chapter. Some of the more technical metrics, such as memory or CPU or\ndisk utilization, might be exposed to the monitoring tools directly. It’s import-\nant to make sure that the data ingestion layer doesn’t act as a black box if you\nwant visibility into the movement of your data into the data platform. This is\nimportant for monitoring and troubleshooting purposes.\nLater in this chapter we will explore which services and tools, both open source and\ncommercial, are available for a data ingestion layer implementation on all three cloud\nproviders. Not all of the tools will satisfy all of the requirements, and it will be import-\nant to understand the trade-offs a data platform architect will have to accept when\nchoosing a particular solution. \nExercise 3.1\nYou need to plan for both batch and real-time ingestion because (choose one)\n1\nReal-time tools are complex and hard to manage.\n2\nBatch ingestion is the only way to get accurate data.\n3\nSome data sources don’t support real-time ingestion.\n4\nYou need two ingestion mechanisms for redundancy.\n",
      "content_length": 2895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "44\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n3.1.2\nFast and slow storage\nBecause the data ingestion layer usually doesn’t store any data itself, though it may use\ntransient cache, once data is passed through the ingestion layer, it must be stored reli-\nably. The storage layer in the data platform architecture is responsible for persisting\ndata for long-term consumption. It has two types of storage—fast and slow, as shown in\nfigure 3.5.\nWe will be using the terms slow and fast storage throughout this book to differentiate\nbetween cloud storage services that are optimized for larger files (tens of MBs and\nmore) and those optimized for storing smaller bits of data (KBs, typically), but with\nmuch higher performance characteristics. Such systems are also sometimes referred\nto as message buses; distributed logs are queues with persistence. Fast and slow here is\nnot a reference to specific hardware characteristics, such as the difference between\nHDD and SSD drives, but rather the characteristics of the storage software design and\nuse cases it is targeted for. As another example, frameworks that allow you to process\ndata in real-time (Cloud Dataflow, Kafka Streams, and so on) are tied to a specific stor-\nage system. So if you want to do real-time, you will need to work directly with the fast\nstorage layer.\n The storage layer in the data platform must perform the following tasks:\nStore data for both the long term and short term\nMake data available for consumption either in batch or streaming modes\nSlow storage\nis your archive\nand area for\nbatch data. It\nis often used\nfor permanent\ndata retention.\nFast storage is a\nmessage bus for\ndata that is coming\nin streams, message by\nmessage. It typically has\ndata expiry policies.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.5\nThe data storage layer persists data for consumption using fast and slow storage.\n",
      "content_length": 2154,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "45\nCloud data platform layered architecture\nOne of the benefits of cloud is that storage is so inexpensive that storing data for years\nor even decades becomes feasible. Having all that data available gives you so many\noptions with the ability to repurpose it for new analytic use cases, such as machine\nlearning at the top of the list.\n In our cloud data platform architecture, the data storage layer is split into two dis-\ntinct components—slow and fast storage. Slow storage is your main storage for archive\nand persistent data. This is where data will be stored for days, months, and often years\nor even decades. In a cloud environment, this type of storage is available as a service\nfrom the cloud vendors as an object store that allows you to cost effectively store all\nkinds of data and support fast reading of large volumes of data.\n The main benefit of using an object store for long-term storage is that in the cloud\nyou don’t have any compute associated directly with the storage. For example, you\ndon’t need to provision and pay for a new virtual machine if you want to increase the\ncapacity of your object store. Cloud vendors grow and shrink the capacity of your stor-\nage in response to the actual data you upload or delete. This makes this type of stor-\nage very cost efficient. \n The main downside of object stores is that they don’t support low-latency access.\nThis means that for streaming data, which operates on a single message or a single\ndata point at a time, an object store will not provide the necessary response time.\nThere is a difference between uploading a 1TB file with JSON data to the object store\n(batch) or trying to upload the same volume as a billion single JSON documents, one\nat a time (streaming). \n For streaming use cases, a cloud data platform requires a different type of storage.\nWe call it “fast” storage because it can accommodate low-latency read/write opera-\ntions on a single message. Most associate storage of this type with Apache Kafka, but\nthere are also services from cloud vendors that have similar characteristics. We will\nexplore what those are in more detail later in this chapter. \n Fast storage brings the low latency required for streaming data ingestion, but it\nusually means that some compute capacity is associated with the storage itself. For\nexample, in a Kafka cluster, you will need to add new machines with RAM, CPU, and\ndisk if you want to increase your fast storage capacity. This means that the cost of fast\ncloud storage is significantly higher than the cost of slow storage. In practice, you\nwould configure a data retention policy, where your fast storage only stores a certain\namount of data (one day, one week, or one month, depending on your data volumes).\nThe data will then be transferred to a permanent location on the slow storage and\npurged from the fast storage per the policy. \n The storage layer should have the following properties:\nReliable—Both slow and fast storage should be able to persist data in the face of\nvarious failures.\nScalable—You should be able to add extra storage capacity with minimal effort.\n",
      "content_length": 3102,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "46\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nPerformant—You should be able to read large volumes of data with high enough\nthroughput from slow storage or read/write single messages with low latency to\nfast storage. \nCost efficient—You should be able to apply a data retention policy to optimize\nstorage mix to optimize costs.\n3.1.3\nProcessing layer\nThe processing layer, highlighted in figure 3.6, is the heart of the data platform imple-\nmentation. This is where all the required business logic is applied and all the data val-\nidations and data transformations take place. The processing layer also plays an\nimportant role in providing ad hoc access to the data in the data platform.\nThe processing layer should be able to perform the following tasks:\nRead data in batch or streaming modes from storage and apply various types of\nbusiness logic\nExercise 3.2\nWhy do you need two types of storage in your data platform?\n1\nTo provide redundancy in case one storage type fails\n2\nTo support both batch and real-time processing\n3\nTo optimize cloud costs\n4\nTo support both data science and business intelligence use cases\nA distributed data\nprocessing engine\nis a must. You might\nneed separate ones\nfor batch and for\nreal time.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.6\nThe processing layer is where business logic is applied and all data validations \nand data transformations take place, as well as providing ad hoc access to data.\n",
      "content_length": 1719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "47\nCloud data platform layered architecture\nSave data back to storage for data analysts and data scientists to access\nDeliver streaming data outputs to consumers, usually other systems\nThe processing layer is responsible for reading data from storage, applying some cal-\nculations on it, and then saving it back to storage for further consumption. This layer\nshould be able to work with both slow and fast data storage. This means that the ser-\nvices or frameworks that we choose to implement this layer should have support for\nboth batch processing of files stored in slow storage as well as one-message-at-a-time\nprocessing from the fast storage. \n Today, there are open source frameworks and cloud services that allow you to pro-\ncess data from both fast and slow storage at the same time. A good example is an open\nsource Apache Beam project and service from Google called Cloud Dataflow, which\nprovides a managed platform-as-a-service execution environment for Apache Beam\njobs. Apache Beam supports both batch and real-time processing models within the\nsame framework. Generally, a layer in the data platform doesn’t have to be imple-\nmented using a single cloud service or software product. Often, you will find that\nusing specialized solutions for each batch and stream processing will give you better\nresults than using a single multipurpose tool. \n The processing layer should have the following properties:\nScale beyond a single computer. Data processing framework or cloud service\nshould be able to work efficiently with data sizes ranging from megabytes to\nterabytes or petabytes. \nSupport both batch and real-time streaming models. Sometimes it makes sense\nto use two different tools for this. \nSupport most popular programming languages, such as Python, Java, or Scala.\nProvide a SQL interface. This is more a “nice to have” requirement. A lot of\nanalytics, especially in the ad hoc scenario, is done using SQL. Frameworks that\nsupport SQL will significantly increase the productivity of your analysts, data sci-\nentists, or data engineers. \n3.1.4\nTechnical metadata layer\nTechnical metadata, as opposed to business metadata, typically includes but isn’t lim-\nited to schema information from the data sources; status of the ingestion; transforma-\ntion pipelines such as success, failure, error rates, and so on; statistics about ingested\nand processed data, like row counts; and lineage information for data transformation\npipelines. As shown in figure 3.7, the metadata layer is central to the data platform\nand is kept in a metadata store.\n A data platform metadata store performs the following tasks:\nStores information about the activity status of different data platform layers\nProvides an interface for layers to fetch, add, and update metadata in the store\n",
      "content_length": 2787,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "48\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nThis technical metadata is very important for automation, monitoring and alerting,\nand developers’ productivity. Since our data platform design consists of multiple lay-\ners that sometimes don’t communicate directly with other layers, we need to have a\nrepository that stores the state of these layers. This allows, for example, the data pro-\ncessing layer to know which data is now available for processing by checking the meta-\ndata layer instead of trying to communicate with the ingestion layer directly. This\nallows us to decouple different layers from each other, reducing the complexities asso-\nciated with interdependencies. \n Another type of metadata that you might be familiar with is business metadata, which\nis usually represented by a data catalog that stores information about what data actually\nmeans from a business perspective; for example, what this specific column in this data\nsource represents. Business metadata is an important component of an overall data\nstrategy because it allows for easier data discovery and communication. Business meta-\ndata stores and data catalogs are well represented by multiple third-party products and\ncan be plugged into the layered data platform design as another layer. Going into details\nof business metadata solutions is outside of the scope of this book.\n A data platform metadata store should have the following properties:\nScalable—There can be hundreds (or sometimes thousands!) of individual tasks\nrunning in the data platform environment. A metadata layer must be able to\nscale to provide fast responses to all of the tasks.\nHighly available—The metadata layer can become a single point of failure in\nyour data platform pipelines. If the processing layer needs to fetch information\nabout which data is available for processing from the metadata layer and the\nTechnical metadata typically\nincludes schema information,\nstatus of the ingestion and\ntransformation pipelines,\nstatistics about ingested and\nprocessed data, and lineage\ninformation. \nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.7\nThe metadata layer stores information about the status of the data platform layers, \nneeded for automation, monitoring and alerting, and developer productivity.\n",
      "content_length": 2544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "49\nCloud data platform layered architecture\nmetadata service is not responding, the processing pipeline will either fail or\nget stuck. This can trigger cascading failures if there are other pipelines\ndepending on the one that failed.\nExtendable—There are no strict rules on which metadata you should store in this\nlayer. We will explore the most common items like schema and pipeline statis-\ntics in upcoming chapters. You may also find that often you want to store some\nbusiness-specific information in the metadata layer, such as how many rows with\na certain column value we have. Your metadata layer should allow you to easily\nstore this extra information. \nTechnical metadata management in the data platform is a relatively new topic. There\nare few existing solutions that can fulfill the tasks described here. For example, a Con-\nfluent Schema Registry allows you to store, fetch, and update schema definitions, but\nit doesn’t allow you to store any other types of metadata. Some metadata layer roles\ncan be performed by various ETL overlay services such as Amazon Glue. We will talk\nmore about the pros and cons of these types of tools in the later sections of this chap-\nter. As it stands today, you will likely need a combination of different tools and services\nto implement a fully functional technical metadata layer. \n3.1.5\nThe serving layer and data consumers\nThe serving layer delivers the output of analytics processing to the various data consumers.\n As shown in figure 3.8, the serving layer is responsible for:\nServing data to consumers who expect a relational data structure and full SQL\nsupport via a data warehouse\nServing data to consumers who want to access data from storage without going\nthrough a data warehouse\nFigure 3.8\nThe serving layer delivers the output of analytics processing to data consumers.\nConsumers are both people and\nsystems. Some may need to\nconsume streams directly, some\nwant raw data, and others want\norganized, curated data via the\ndata warehouse.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\n",
      "content_length": 2272,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "50\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nIn chapters 1 and 2, we discussed why in most cases a cloud data platform doesn’t\nreplace the need for a data warehouse as a data consumption option. Data warehouses\nprovide a data access point for data consumers that require full SQL support and\nexpect data to be presented in a relational format. Such data consumers may include\nvarious existing dashboarding and business intelligence applications, but also data\nanalysts or power business users familiar with SQL. Figure 3.9 shows the data ware-\nhouse as the access point for these consumers.\nA serving layer will almost always include a data warehouse, which should have the fol-\nlowing properties:\nScalable and reliable—A cloud data warehouse should work efficiently with both\nlarge and small data sets and scale beyond the capacity of a single computer. It\nalso should be able to continue to serve data in the face of inevitable failures or\nindividual components.\nNoOps—Preferably, a cloud warehouse should require as little tuning or opera-\ntional maintenance as possible.\nElastic cost model—Another highly desirable property of the cloud warehouse is\nthe ability to scale both up and down in response to the load. In many tradi-\ntional BI workloads, the data warehouse is used mostly during the business days\nand may experience only a small portion of the load during off hours. A cloud\ncost model should be able to reflect this. \nIn a modern data architecture, while it is very likely you will have data consumers who\nwill want a relational data structure and SQL for data access, increasingly, other data\naccess languages are gaining popularity. \nA cloud data warehouse\nis a destination for\ncurated data sets. It’s\nrole is to provide results\nof the queries back to\nusers—fast.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.9\nA data warehouse is usually included in the serving layer.\n",
      "content_length": 2174,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "51\nCloud data platform layered architecture\nSome data consumers will require direct access to the data in the lake, as shown in fig-\nure 3.10.\n Usually data science, data exploration, and experimentation use cases fall into this\ncategory. Direct access to the data in the lake unlocks the ability to work with the raw,\nunprocessed data. It also moves experimentation workloads outside of a warehouse to\navoid performance impacts. Your data warehouse can be serving critical business\nreports and dashboards, and you don’t want it to suddenly slow down because a data\nscientist decided to read the last 10 years of data from it. There are multiple ways to\nprovide direct access to the data in the lake. Some cloud providers, as discussed in the\nnext sections of this chapter, provide a SQL engine that can run queries directly on\nthe files in cloud storage. In other cases, you may use Spark SQL to achieve the same\ngoal. Finally, it’s not uncommon, especially for data science workloads, to just copy\nrequired files from the data platform into experimentation environments like note-\nbooks or dedicated data science VMs.\nNOTE\nWhile it is possible to have a data platform without a data warehouse, it\nis likely that every business has business users who will want access to the data\nin the lake, and these business users are best served via a data warehouse as\nopposed to direct access to the lake. As such, when we talk about data plat-\nforms, we always assume it will feed at the very least a data warehouse for data\nconsumption.\nData consumers aren’t always humans (see figure 3.11). The results of real-time\nanalytics that are being calculated as the data is received a single message at a time are\nrarely intended to be consumed by a human. No one spends their day watching\nReading raw files from\nstorage is often desired\nby data scientists using\nML applications.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.10\nDirect data lake access allows consumers to bypass the serving layer and work \ndirectly with raw, unprocessed data.\n",
      "content_length": 2269,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "52\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nmetrics on a dashboard change every second. Outputs from a real-time analytics\npipeline are usually consumed by other applications like marketing activation systems,\nsuch as ecommerce recommendation systems that decide which item to recommend\nto a user while they are shopping, or ad bidding systems where the balance between\nad relevance and cost changes in milliseconds. \nSuch programmatic data consumers require a dedicated API to consume data from\nthe lake in real-time, often using built-in APIs that are often available in the real-time\ndata processing engine of your choice. Alternatively, you can implement a separate\nAPI layer to allow multiple programmatic consumers access to real-time data using the\nsame interface. This separate API layer approach scales better when you have multiple\nprogrammatic data consumers, but it also requires significantly more engineering\neffort to implement and maintain. \nOutputs from a real-\ntime analytics pipeline\nare usually consumed\nby other applications,\nnot people.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.11\nConsumers of real-time data are usually other applications, not humans.\nExercise 3.3\nWhy does a data platform need multiple ways to provide access to the data?\n1\nTo support consumers with different needs and requirements\n2\nTo maximize data throughput\n3\nTo provide failover capabilities in case of outages\n4\nTo improve data quality\n",
      "content_length": 1722,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "53\nCloud data platform layered architecture\n3.1.6\nOrchestration and ETL overlay layers\nThere are two components of our cloud data platform architecture that require spe-\ncial consideration. Or rather, they require a slightly different approach to thinking\nabout them. They are the orchestration and ETL overlay layers, highlighted in figure\n3.12. The reason these layers require special treatment is because, in many cloud data\nplatform implementations, the responsibilities of these layers are spread across many\ndifferent tools.\nORCHESTRATION LAYER\nIn a cloud data platform architecture, the orchestration layer (see figure 3.13) is\nresponsible for the following tasks:\nCoordinate multiple data processing jobs according to a dependency graph (a\nlist of dependencies for each data processing job that includes which sources\nare required for each job and whether a job depends on other jobs)\nHandle job failures and retries\nAs we have now seen, a modern cloud data platform architecture includes multiple\nloosely coupled layers that communicate with each other via a metadata layer. The\nmissing piece in this design is a component that can coordinate work across multiple\nlayers. While the metadata layer acts as a repository for various status information and\nstatistics about the data pipelines, the orchestration layer is an action-oriented compo-\nnent. It’s main function is to allow data engineers to construct complex data flows,\nwith multiple interdependencies. \n Imagine the following scenario. Your organization is a retailer that sells goods both\nonline and at brick-and-mortar stores. You want to be able to compare top-selling\nproducts online and at the offline stores. To do that, you need to bring product infor-\nmation data from an enterprise resource planning (ERP) system; sales from the\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.12\nOrchestration and ETL overlay layers usually have responsibilities \nthat are spread across many different tools.\n",
      "content_length": 2207,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "54\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\noffline stores are provided on a regular basis by a third-party point-of-sales (POS) pro-\nvider; and online stores sales are available in real-time as clickstream data. \n To produce a top-sellers comparison report, we need to create two data transfor-\nmation jobs. The first job will combine product information with POS sales data. The\nsecond job will use clickstream data and combine it with the output of the first job to\nproduce a comparison data set (figure 3.14). \n As you can see from this example, jobs 1 and 2 can’t just run independently from\neach other. All three sources can be delivering the latest data at very different sched-\nules. For example, POS and products data can only be available once a day, while\nclickstream is a real-time data source. If we don’t coordinate the two data transforma-\ntion jobs somehow, our final data product may have incorrect or incomplete results.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nThe orchestration layer is\nresponsible for coordinating\nmultiple data processing jobs\nand handling job failures\nand retries.\nFigure 3.13\nThe Orchestration overlay allows data engineers to construct complex data flows \nwith multiple inter-dependencies.\nFigure 3.14\nExample of \njobs and data dependency \ngraph\nProducts\nPOS\nClickstream\nJob\n1\nJob\n2\nFinal data product\nThe first data\ntransformation job\ncombines product\ndata with POS\nsales data.\nThe second job \ncombines the\noutput of the first\njob with online sales.\n",
      "content_length": 1772,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "55\nCloud data platform layered architecture\nThere are several different ways to address this challenge. One approach is to com-\nbine jobs 1 and 2 into one job and schedule it in such a way that it only runs when the\nlatest data becomes available from all three sources (you can use the metadata layer\nfor this!). This approach sounds straightforward, but what if you need to add more\nsteps to this job? Or what happens if job 2 is a common task that should be shared\nacross multiple different jobs? As the complexity of your data pipeline grows, develop-\ning and maintaining monolithic data processing jobs will become a challenge. Com-\nbining the two jobs has all the downside of a monolithic design: hard to make changes\nto specific components, hard to test, and a challenge for different teams to collabo-\nrate on.\n An alternative approach is to coordinate jobs using an external orchestration\nmechanism (figure 3.15).\nAn orchestration layer, shown in figure 3.15, is responsible for coordinating multiple\njobs based on when required input data is available from an external source, or when\nan upstream dependency is met, such as job 1 needs to complete before job 2 can\nstart. In this case, job implementations remain independent of each other. When they\nare independent, they can be developed, tested, and changed separately, and the\norchestration layer maintains what’s called a dependency graph—a list of dependencies\nfor each data processing job that includes which sources are required for each job and\nwhether a job depends on other jobs. A dependency graph need only be changed\nwhen the logical flow of the data is changed, for example, when a new step in process-\ning is introduced. It doesn’t have to be changed when an implementation of a certain\nstep changes. \nProducts\nPOS\nClickstream\nJob\n1\nJob\n2\nFinal data product\nOrchestration layer\nTrigger job 2 when job 1 completes and we have\naccumulated a full day of clickstream data.\nTrigger job 1 when products\nPOS data arrives.\nFigure 3.15\nAn orchestration layer coordinates multiple jobs while allowing job \nimplementations to remain independent of each other.\n",
      "content_length": 2123,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "56\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n In a large data platform implementation, the dependency graph can contain hun-\ndreds and sometimes thousands of dependencies. In such implementations, there are\nusually multiple teams involved in developing and maintaining the data processing\npipelines. The logical separation of the jobs and the dependency graph makes it eas-\nier for these teams to make changes to parts of the system, without having to impact\nthe larger data platform. \n An orchestration layer should have the following properties:\nScalability—It should be able to grow from a handful to thousands of tasks and\nhandle large dependency graphs efficiently.\nHigh availability—If the orchestration layer is down or unresponsive, your data\nprocessing jobs will not run.\nMaintainability—It should have a dependency graph that is easy to describe and\nmaintain.\nTransparency—It should provide visibility into job statuses, history of execution,\nand other observability metrics. This is important for monitoring and debug-\nging purposes.\nThere are several implementations of the orchestration layer available today. One of\nthe most popular is Apache Airflow, an open source job scheduler and orchestration\nmechanism. Airflow satisfies most of the properties listed in this section and is avail-\nable as a cloud service on Google Cloud Platform, called Cloud Composer. There are\nother tools such as Azkaban and Oozie that can serve the same purpose, but both have\nbeen created specifically as job orchestration tools for Hadoop and don’t fit into flexi-\nble cloud environments as well as Airflow does.\n When it comes to native cloud services, different cloud providers approach the\norchestration problem differently. As mentioned, Google adopted Airflow and made\nit available as a managed service, simplifying the operational aspect of managing the\norchestration layer. Amazon and Microsoft include some orchestration features into\ntheir ETL tools overlay products. \nETL TOOLS OVERLAY\nAn ETL tools overlay, highlighted in figure 3.16, is a product or a suite of products\nwhose main purpose is to make the implementation and maintenance of cloud data\npipelines easier. These products absorb some of the responsibilities of the various data\nplatform architecture layers and provide a simplified mechanism to develop and man-\nage specific implementations. Usually these tools have a user interface and allow for\ndata pipelines to be developed and deployed with little or no code. \n ETL overlay tools are usually responsible for\nAdding and configuring data ingestion from multiple sources (ingestion layer)\nCreating data processing pipelines (processing layer)\nStoring some of the metadata about the pipelines (metadata layer)\nCoordinating multiple jobs (orchestration layer)\n",
      "content_length": 2831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "57\nCloud data platform layered architecture\nAs you can see, an ETL overlay tool can implement almost all layers in the cloud data\nplatform architecture. \n Does this mean that you can implement a data platform using a single tool and not\nworry about implementing and managing separate layers yourself? The answer, per-\nhaps unsurprisingly, is that “it depends.” The main question you have to keep in mind\nwhen deciding to fully rely on an ETL overlay service from a cloud vendor (or a simi-\nlar third-party solution) is, “How easy it is to extend it?” Is it possible to add new com-\nponents for data ingestion? Do you have to do data processing using only ETL service\nfacilities, or can you call external data processing components? Finally, it’s important\nto understand whether you can integrate other third-party services or open source\ntools with this ETL service. \n The reason these questions are important is because no system is static. You may\nfind that using an ETL service is a great way to get you started and can provide signifi-\ncant time/cost savings. As your data platform grows, you may find yourself in a situa-\ntion where the ETL service or tool doesn’t allow you to easily implement some needed\nfunctionality. If this ETL service doesn’t provide any options for extending its func-\ntionality or integrating with other solutions, then your only choice is to build a work-\naround that bypasses the ETL layer completely. \n In our experience, at some point these workarounds will become as complex as\nthe initial solution itself and you will end up with what we fondly call a “spaghetti\narchitecture.” We certainly don’t like speaking poorly of one of our favorite meals, but\na spaghetti architecture is the result of different components of the system becoming\nmore and more tangled together, making it harder to maintain. In a spaghetti archi-\ntecture, workarounds exist not because they fit into the overall design but because\nthey have to compensate for ETL service limitations. \nFigure 3.16\nAn ETL tools overlay can be used to implement many of the layer functions in a cloud \ndata platform architecture.\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nThe ETL tools overlay is a\nproduct or a suite of products\nwhose main purpose is to\nmake the implementation and\nmaintenance of data pipelines\neasier.\nOperational\nmetadata\n",
      "content_length": 2546,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "58\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n An ETL overlay should have the following properties:\nExtensibility—It should be possible to add your own components to the system.\nIntegrations—It’s important for modern ETL tools to be able to delegate some\nof the tasks to external systems. For example, if all data processing is done\ninside the ETL service using a black-box engine, then you will not be able to\naddress any of the limitations or idiosyncrasies of this engine.\nAutomation maturity—Many ETL solutions offer a no-code-required, UI-driven\nexperience. This is great for rapid prototyping, but when thinking about a pro-\nduction implementation, consider how this tool will fit into your organization’s\ncontinuous integration/continuous delivery practices. For example, how will a\nchange to a pipeline be tested and validated automatically, before being promoted\nto a production environment? If the ETL tool doesn’t have an API or an easy-to-\nuse configuration language, then your automation options will be very limited.\nCloud architecture fit—The ETL tools market is very mature, but many open\nsource and commercial tools were built in the age of on-premises solutions and\nmonolithic data warehouses. While all of them offer some form of cloud inte-\ngration, you need to carefully evaluate whether this particular solution allows\nyou to utilize cloud capabilities to their fullest extent. For example, some of the\nexisting ETL tools are only capable of processing data using SQL running in\nthe warehouse, while others use their own processing engines, which are less\nscalable and robust than Spark or Beam.\nWhen it comes to existing ETL overlay solutions, there are many available options.\nLater in this chapter, we will take a look at the cloud services AWS Glue, Azure Data\nFactory, and Google Cloud Data Fusion. \n As for ETL solutions that are not part of cloud vendor services, there are many\nthird-party solutions to choose from. Talend is one of the more popular solutions\ntoday. It uses an “open core” model, meaning that the core functionality of Talend is\nopen source and free to use for development and prototyping. When it comes to\nusing Talend for production workloads, a proper commercial license is required.\nInformatica is another ETL tool popular among larger organizations. This book will\nfocus on cloud-native ETL overlay solutions as well as free open source components.\nCommercial or open-core ETL products are outside of the scope of this book, but\nthere is a lot of existing literature available on the topic.\nExercise 3.4\nWhat is the role of the orchestration layer in the data platform architecture?\n1\nTo allow users to find data easily\n2\nTo optimize performance of different data platform components\n3\nTo manage dependencies between different data processing jobs\n4\nTo provide a UI for data engineers to create new data processing jobs\n",
      "content_length": 2920,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "59\nThe importance of layers in a data platform architecture\n3.2\nThe importance of layers in a data platform architecture\nOne of the key conceptual models used in a cloud data platform architecture is the\nidea of a layer. Each layer plays a very specific role, and it’s a good idea from an archi-\ntecture perspective to isolate layers from each other as much as possible. \n Let’s take another look at our data architecture and its layers in figure 3.17.\nYou can see that there is a clear separation between the layers. In the case of ingestion\nand processing, while you could implement both ingestion and processing using a sin-\ngle service, tool, or application, it’s not the best decision as it will cause problems\ndown the road. \n For example, you could use Apache Spark for both data processing and data inges-\ntion, because, while Spark is really good at processing, it also comes with a set of con-\nnectors to external systems such as relational databases and the like. Using Spark for\nboth ingestion and processing combines two functionally different layers into one.\nHere are some of the things to consider if you decide to go this route:\nAre you getting the best possible functionality for data ingestion and data pro-\ncessing using Spark? Yes, Spark can perform data ingestion, but is it functionally\nthe best solution? Is it easy to extend if you will need to bring in a data source,\nfor which there is no existing Spark connector? \nIn larger organizations, different teams or different developers can be\nresponsible for data ingestion and data processing. In fact, you would probably\nwant to encourage a more centralized and controlled data ingestion process,\nbut enable more self-service data transformation practices. This will allow data\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 3.17\nFor maximum flexibility, functional layers in the data platform should be separate but \nloosely coupled.\n",
      "content_length": 2145,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "60\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nconsumers to shape data to their preferences and needs. If you combine\ningestion and processing together, you will lose this flexibility.\nWhat if you decide to replace Apache Spark with a data processing solution that\nsuits your needs better? If your ingestion and processing are combined, you will\nneed to reimplement both of these functions. If they are separate, you can keep\ningestion going and gradually replace processing with another tool.\nIf we translate this idea of separate functional layers into software development terms,\nwe will say that functional layers in the data platform should be loosely coupled. This\nmeans that separate layers must communicate through a well-defined interface but\nshould not depend on the internal implementation of a specific layer. This approach\nbrings significant flexibility to how you can mix and match different cloud services or\ntools to achieve your goals. \n Cloud implementations are notorious for being in a state of constant change:\nthere are always new services being released by cloud vendors or new projects avail-\nable from the open source community. A data platform architecture based on loosely\ncoupled layers allows you to respond to these changes with the least possible impact\non the overall data platform structure. \n3.3\nMapping cloud data platform layers to specific tools \nWhen it comes to selecting services and tools that you can use in your data platform\ndesign planning, you have various options—from PaaS (platform as a service) to\nserverless to open source and SaaS (software as a service) offerings. \n In the next several sections of this chapter, we will map different data platform layers\nto specific services and tools that are available in three major cloud environments: AWS,\nGoogle Cloud, and Azure. There are always multiple options for each layer. There is no\none-size-fits-all solution, and specific implementations will depend on many factors,\nsuch as your organization’s skills, budgets, timelines, and analytical needs. \n When talking about specific implementation details, we will follow the following\npriorities:\n1\nCloud-native platform-as-a-service solutions from AWS, Google, and Microsoft\n2\nServerless solutions\n3\nOpen source solutions\n4\nCommercial and third-party SaaS offerings\nThere are trade-offs among the different options—most often the trade-off between con-\ntrol, flexibility, and effort to support versus portability among cloud and on-premises plat-\nforms. You can see how each option is rated along a trade-off continuum in figure 3.18.\n We will always start with an overview of fully managed solutions from cloud vendors for\neach of the data platform layers, if such a solution exists. While there are pros and cons for\neach type of solution, PaaS solutions usually offer an environment where you don’t need\nto spend time on the mundane tasks associated with managing your own servers, making\nsure that versions of different libraries actually work together, and so on. These solutions\nalso automate many time-consuming tasks such as managing connections to external\n",
      "content_length": 3159,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "61\nMapping cloud data platform layers to specific tools\nsystems or keeping track of what data has been already ingested. This can significantly\nimprove the productivity of the team or a person working on the data platform project.\nPaaS solutions are also a big area of investment from cloud vendors, so there are always new\nfeatures and improvements being released. On the other hand, PaaS solutions are usually\nthe most limiting when it comes to extensibility. It’s often difficult or even completely\nimpossible to introduce your own modules or libraries, add new connectors, and so on.\n Next on our list of recommended solutions are serverless solutions. In a nutshell,\ncloud serverless solutions allow you to execute custom application code without hav-\ning to manage your own servers or worry about scalability and fault tolerance. These\nsolutions offer all the benefits of a managed cloud environment, but are more flexible\nsince you can write your own code. Today several different serverless services exist on\ndifferent clouds and typically include data processing services and short-lived light-\nweight cloud functions. \n We will also provide an overview of existing open source solutions that can be used\nto implement various data platform layers. These solutions usually provide the most\nflexibility, including portability across cloud vendors, but usually require you to provi-\nsion and manage your own cloud infrastructure. This is still a very viable approach if\nthe benefits of the open source solution outweigh the need to provision, monitor, and\nmaintain required VMs. Since cloud makes automation a first-class citizen, it’s possible\nto achieve significant levels of infrastructure stability with a very small team of engi-\nneers using open source solutions. \n When it comes to commercial or third-party SaaS offerings, we will only briefly\nmention existing products if they have a significant market share. There are dozens of\nproducts in the data management and ETL space out there, and the landscape of this\nindustry is changing very rapidly. We recommend commercial SaaS solutions when\nspecial functionality not available as PaaS or services is required or when open source\nalternatives are not mature. SaaS is also a good option when the company has already\nmade a significant investment in a particular product, including the teams using the\nproducts, and wants to protect that investment.\n The reality of a cloud data platform implementation is that you will most likely\nneed to mix and match several solutions. That’s why a loosely coupled layered archi-\ntecture is so important. In the following sections, we will provide an overview of tools\nMore control and flexibility\nMore work to support\nVery portable\nLess control and flexibility\nLess work to support\nNot very portable\nPaaS\nServerless\nThird-party\nSaaS\nOpen\nsource\nFigure 3.18\nKey trade-offs among data platform implementation component options\n",
      "content_length": 2926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "62\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\navailable on AWS, Google Cloud, and Azure and will try to provide an evaluation of\nthese solutions against the desired properties that we have outlined in each section of\nthis chapter.\nNOTE\nToday we are seeing an increasing interest in multicloud solutions,\nwhere an organization decides to utilize components across different cloud\nvendors. Sometimes this is done to reduce the risk of vendor lock-in, but\nmore and more often it is done to utilize best-in-class products that each\ncloud has to offer. For example, we have seen cases where an organization\nwas performing the bulk of their analytics on AWS but decided to implement\ntheir machine learning use cases on Google Cloud. A layered cloud data plat-\nform design allows you to not only mix and match products and services\nwithin one provider, but also build successful multicloud solutions.\n3.3.1\nAWS\nAWS is the oldest player in the relatively new public cloud market and offers a wide selec-\ntion of products both in fully managed platform as a service and flexible infrastructure\nas a service (IaaS) space. In this section, we will go over specific AWS components that\ncan be used to implement various data platform layers, as shown in figure 3.19. \nBATCH DATA INGESTION \nFor batch data ingestion, AWS offers two fully managed services. AWS Glue can be\nused as your ingestion mechanism. Currently, Glue supports ingesting files only from\nAWS S3 storage or reading data from a database using a JDBC connection. External\nAPIs and NoSQL databases are not supported. \nFigure 3.19\nAWS services for cloud data platforms\nBatch \ndata\nStreaming\ndata\nAWS S3 & AWS Athena\nSlow storage/direct data lake access\nAWS Kinesis\nFast storage\nAWS Glue\nData\nCatalog\nOperational metadata\nAWS\nRedshift\ndata\nwarehouse\nAWS Kinesis Data Analytics\nReal-time processing and analytics\nDynamoDB\n+ API\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nAWS Glue\nETL tools overlay\nAWS Glue, AWS Step Functions\nOrchestration overlay\nAWS EMR\nBatch processing and analytics\nAWS Kinesis,\nAWS Kinesis\nFirehose\nIngestion\nAWS Glue,\nAWS DMS,\nAWS Lambda\n",
      "content_length": 2176,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "63\nMapping cloud data platform layers to specific tools\n Another option for batch data ingestion is AWS Database Migration Service. This\nservice allows you to perform historical and ongoing data migration from an on-\npremises relational database into different AWS destinations. While this service is\nprimarily targeted to migrating your operational databases into AWS-managed database\nservices, you can use it to ingest data into your data platform by specifying S3 as a\ndestination. Additionally, AWS DMS supports change data capture (CDC—defined by\nWikipedia as “a set of software design patterns used to determine and track the data that\nhas changed so that action can be taken using the changed data”) for ongoing data\ningestion from MS SQL Server, MySQL, and Oracle. Unless you require CDC, then it’s\npreferable to do all ingestion using Glue. This provides you with a single service where\nyou can monitor ingestion status, configure error handling, alerting, and so on. It also\nallows you to simplify job scheduling and coordination.\n To ingest data from sources that are currently not supported by AWS Glue or DMS,\nyou can implement and run your own ingestion code using a serverless AWS Lambda\nenvironment. This, of course, means that you will need to develop, test, and maintain\nthe ingestion code yourself. \nSTREAMING DATA INGESTION\nFor data sources that produce data one message at a time and require streaming\ningestion, AWS offers the AWS Kinesis service. Kinesis acts as a message bus by storing\nmessages from source systems such as CDC tools, clickstream collection systems such as\nSnowplow, or custom applications and allowing different consumers to read those\nmessages. Kinesis itself is just a fast data transport service. This means that you will need\nto write code that will actually publish messages from your data source into Kinesis\nyourself. There are existing prebuilt Kinesis connectors, but only for a few AWS-specific\ndata sources such as DynamoDB or Redshift. AWS also offers a number of prebuilt\nconsumers and transformations for Kinesis called Kinesis Firehose, which allows you to\nread messages from Kinesis, change the data format (for example, converting JSON\nmessages to Parquet), and save data to various destinations like S3 or Redshift. With\nFirehose, you can quickly configure a data ingestion pipeline that reads JSON messages\nfrom Kinesis, converts them to Parquet (more on this later), and saves them to S3 for\nfurther processing.\n If you need more advanced capabilities for handling incoming streaming data, you\ncan also use the AWS Glue streaming feature. Glue streaming runs on Spark Struc-\ntured Streaming and allows you to ingest and process data from Kinesis and Kafka. \n As an alternative to Kinesis, you can use AWS Managed Streaming for Apache\nKafka (MSK). MSK is a fully managed Kafka cluster that you can use as you would nor-\nmally use a standalone Kafka cluster. MSK provides full compatibility with the existing\nKafka producer and consumer libraries. This option is particularly useful if you are\nmigrating an existing real-time pipeline based on Kafka into AWS.\nDATA PLATFORM STORAGE \nAWS S3 is a great choice for implementing scalable and cost-efficient data platform stor-\nage. It offers unlimited scalability and high data durability guarantees. AWS has different\n",
      "content_length": 3325,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "64\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\ntiers for S3 with different data access latency and cost properties. For example, you can\nuse a slower, less expensive tier for your archival data or data that is not accessed often\nand a lower-latency, faster, more expensive tier when response time is important. \nBATCH DATA PROCESSING\nWe have mentioned before that Apache Spark is one of the most popular distributed\ndata processing frameworks. AWS offers a service called Elastic MapReduce (EMR)\nthat allows you (despite the name!) to execute not only legacy MapReduce jobs, but to\nalso run Apache Spark jobs. \n EMR was originally created to help AWS customers migrate their on-premises\nHadoop workloads into the cloud. It allows you to specify the number of machines\nand types of machines you want in your cluster, and then AWS takes care of provision-\ning and configuring them. EMR can work with both data that is stored locally on clus-\nter machines and data stored on S3. \n For our data platform architecture, we will only use data that is stored on S3. This\nallows us to keep the EMR cluster only for the duration of specific jobs, and then auto-\nmatically destroy the cluster once the job is completed. This type of elastic resource\nusage is one of the primary methods of cloud cost management.\nREAL-TIME DATA PROCESSING AND ANALYTICS\nWhile Apache Spark is great for batch and micro-batch data processing, some modern\napplications and analytics use cases require a more real-time approach. Real-time data\nanalytics means that messages are processed one at a time instead of batching them\nin larger groups. AWS Kinesis Data Analytics allows you to build real-time, data-\nprocessing applications that read data from AWS Kinesis. Kinesis Data Analytics also\nhas support for ad hoc querying of live data streams using SQL, which makes real-time\nanalytics available to people without programming experience. If you are using AWS\nMSK, then you can also use the Kafka Streams libraries to implement your real-time\nprocessing applications.\nCLOUD WAREHOUSE\nOne of the flagship products in the AWS data analytics space is AWS Redshift, the first\nwarehousing solution designed specifically for cloud. Redshift uses a massively parallel\nprocessing (MPP) architecture, meaning data is distributed among multiple nodes in\nthe Redshift cluster. This allows you to add more capacity to the cluster by adding\nmore nodes. Redshift is also tightly integrated with S3 and Kinesis, which makes it easy\nto load processed data in batch or streaming modes. \n Redshift Spectrum allows you to query external tables that are located on S3 with-\nout having to load them into the warehouse first. You still need to create the external\ntables in Redshift and define their schemas, but once that is done, you can query\nthose tables using the Spectrum engine, with most of the processing being done out-\nside of the data warehouse. Keep in mind that Spectrum performance is worse than\nnative Redshift tables because data needs to be moved from S3 into the Spectrum\nengine for processing each time you run a query.\n",
      "content_length": 3131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "65\nMapping cloud data platform layers to specific tools\nDIRECT DATA PLATFORM ACCESS\nTo access data in the data platform directly, AWS has a service called Athena. Athena\nallows you to author a SQL query that will be executed in parallel on multiple\nmachines, reading data in S3 and returning the result back to the client. The main\nbenefit of Athena is that AWS provisions machines required for a specific query on the\nfly, meaning that you don’t need to maintain (and pay for) a fleet of permanent VMs.\nAthena charges per amount of data processed in each query, making it a cost-efficient\nsolution for ad hoc analytics, which tend to be done on less predictable schedules. \nETL OVERLAY AND METADATA REPOSITORY\nAWS Glue can be used not only to ingest data from different sources into your data\nplatform, but also to create and execute data transformation pipelines. Glue is actu-\nally based on Apache Spark and tries to simplify the process of developing Spark jobs.\nIt provides templates for most common data transformation pipelines on AWS and\nhas flexible schema support, tracking which data has been ingested for incremental\nloads. For example, Glue has prebuilt templates that allow you to transform a com-\nplex nested JSON structure into a set of relational tables for loading into AWS Red-\nshift. Glue can simplify the process of creating multiple Spark data pipelines, though\nat a cost—making the pipeline code less portable—since Glue uses Spark add-ons that\nare not available in standard Apache Spark distributions. \n Additionally, Glue maintains a Data Catalog that contains schemas for all the data\nsets that you have on S3 storage. Data Catalog uses an automatic discovery process,\nwhere AWS Glue periodically scans data you have on S3 to keep the catalog up to date.\nGlue also maintains a number of statistics about pipeline execution, such as number\nof rows and bytes processed, and so on. These metrics can be used for pipeline moni-\ntoring and troubleshooting. \nORCHESTRATION LAYER\nAWS Glue supports scheduling ETL jobs and allows you to configure dependencies\nbetween different jobs for more complex workflows. Glue scheduling capabilities are\nlimited to the jobs implemented in the Glue overlay. If you are using multiple services\nfor data ingestion and processing like DMS or lambda functions, then you can use\nAWS Step Functions to build workflows that span multiple services.\n Another AWS orchestration option is Data Pipeline. Data Pipeline is focused on\nscheduling and executing data transfers from one system to another. For example,\nyou can schedule periodic loads of files on S3 into Redshift or run a data transforma-\ntion job on EMR. Data Pipeline supports a limited number of systems it can copy data\nto and from. Data Pipeline somewhat overlaps with Glue in functionality but is\nfocused on predefined out-of-the-box actions that are not extendable.   \nDATA CONSUMERS\nAWS supports different types of data consumers. Applications that support SQL via\nJDBC/ODBC drivers can connect to either Redshift or Athena to execute SQL state-\nments on those systems. This includes tools such as Tableau, Looker, Excel, and other\noff-the-shelf tools. AWS also has web interfaces where data analysts can run ad hoc\nanalysis without having to install drives on their local system.\n",
      "content_length": 3296,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "66\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\n If you need to provide data or the results of real-time analytics to applications that\nrequire low-latency response times, then using JDBC/ODBC connectors to the ware-\nhouse or data platform is not the ideal solution. These systems have inherent latency\nand are designed for large-scale data analysis as opposed to fast data access. For low-\nlatency use cases, you can deliver data from your real-time layer into an AWS\nkey/value store called DynamoDB. DynamoDB offers fast data access (especially when\ncombined with a caching mechanism such as DynamoDB Accelerator). Your applica-\ntions can access DynamoDB directly, or you can build an API layer if you want to con-\ntrol how data is exposed. Other options for fast data access include using managed\nrelational database services from Amazon, such as AWS RDS or AWS Aurora.\n3.3.2\nGoogle Cloud \nGoogle Cloud is a relatively new player in the public cloud space. Google Cloud offers\ncapabilities similar to AWS and Azure infrastructure-as-a-service components, includ-\ning virtual machines, networking, and storage. Google Cloud stands out when it\ncomes to data processing and analytics services. Many Google Cloud tools have been\ndeveloped and used internally at Google for many years. This means they have been\ntested at scale and under high load. On the other hand, these tools were designed to\nsolve Google-specific problems and are only now being retrofitted to match broader\nmarket needs. In this section, we will provide an overview of Google Cloud tools that\nyou can use to implement a cloud data platform as shown in figure 3.20.\nFigure 3.20\nGoogle Cloud services for a cloud data platform\nBatch \ndata\nStreaming\ndata\nGoogle Cloud Storage\nSlow storage/direct data lake access\nCloud Pub/Sub\nFast storage\nData Fusion,\nData Catalog\nOperational metadata\nBigQuery\ndata\nwarehouse\nCloud Dataflow\nReal-time processing and analytics\nCloud Bigtable \n+ API\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nCloud Data Fusion\nETL tools overlay\nGoogle Cloud Composer\nOrchestration overlay\nDataproc, Cloud Dataflow\nBatch processing and analytics\nCloud Pub/Sub,\nDataflow\nIngestion\nCloud Data Fusion,\nCloud\nFunctions,\nBigQuery\nData Transfer\nServices\n",
      "content_length": 2296,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "67\nMapping cloud data platform layers to specific tools\nBATCH DATA INGESTION\nGoogle Cloud offers several services to perform batch data ingestion—Cloud Data\nFusion, Cloud Functions, and BigQuery Data Transfer Service. \n Cloud Data Fusion is an ETL overlay service that allows users to construct data inges-\ntion and data processing pipelines using a UI editor and then execute these pipelines\nusing different data processing engines such as Dataproc and (in the future) Cloud\nDataflow. Cloud Data Fusion supports the ingestion of data from relational databases\nusing JDBC connectors as well as the ingestion of files from Google Cloud Storage.\nCloud Data Fusion also has connectors to ingest files from FTP and AWS S3. Unlike\nother managed ETL services, Cloud Data Fusion is based on an open source project\ncalled CDAP (https://cdap.io/). This means you can implement plugins for various\ndata sources yourself and not be constrained by what’s provided out of the box.\n As with AWS Lambda, Google Cloud also provides a serverless execution environ-\nment for custom code called Cloud Functions. Cloud Functions allow you to imple-\nment ingestions from sources that are not currently supported by Cloud Data Fusion\nor BigQuery Data Transfer Service. As Cloud Functions limit how long each function\ncan run before it’s terminated by Google Cloud, Cloud Functions isn’t well suited to\nlarge data ingestion use cases. At the time of writing, the time limit is 9 minutes. \n BigQuery Data Transfer Service is another viable choice for ingesting data into the\ndata warehouse part of your data platform, in this case, Google’s BigQuery. BigQuery\nData Transfer Service allows you to ingest data directly into BigQuery from selected\nGoogle-owned and -operated SaaS sources like Google Analytics, Google AdWords,\nYouTube statistics, and so on. BigQuery Data Transfer Service also supports ingesting\ndata from hundreds of other SaaS providers through a partnership with the data inte-\ngration SaaS company called Fivetran (https://fivetran.com/). Google Cloud offers\nservice provisioning for Fivetran connectors via the Google Cloud web console and\nunified billing, but the integration service itself is provided by Fivetran. \n The downside of using BigQuery Data Transfer Service is that data goes directly\ninto the warehouse, and as we have discussed previously, this limits you in the ways\ndata can be accessed and processed later. If your data analytics use cases require the\ningestion of data from many different SaaS providers, such as Google Analytics, Sales-\nforce, and others, then the simplicity associated with ingesting data using Transfer Ser-\nvice directly into the warehouse may outweigh the bigger architectural considerations. \n BigQuery Data Transfer Service is also expanding to support ingestion from rela-\ntional databases, similar to AWS’s Database Migration Services. Currently, only Tera-\ndata as a source RDBMS is supported. In this case, BigQuery Data Transfer Service\nactually saves data to Google Cloud Storage first, which makes it better suited for the\ncloud data platform architecture. \nSTREAMING DATA INGESTION\nCloud Pub/Sub service provides a fast message bus for data that needs to be ingested\nin a streaming fashion. Pub/Sub is similar in functionality to AWS Kinesis, but cur-\nrently supports larger message sizes (1 MB in AWS Kinesis and 10 MB in Pub/Sub).\n",
      "content_length": 3380,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "68\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nCloud Pub/Sub is just a message storage and delivery service—it doesn’t offer any pre-\nbuilt connectors or data transformations. You will need to develop the code that will\npublish and consume messages from Pub/Sub. Pub/Sub provides integrations with\nGoogle Cloud Dataflow for real-time data processing and analytics and with Cloud\nFunctions. \nDATA PLATFORM STORAGE\nGoogle Cloud Storage is a primary scalable and cost efficient storage offering on Goo-\ngle Cloud. Google Cloud Storage supports multiple storage tiers that vary in data\naccess speed and cost. Google Cloud Storage also integrates with many of the Google\nCloud data processing services such as Dataproc, Cloud Dataflow, and BigQuery.\nBATCH DATA PROCESSING\nGoogle Cloud offers two different ways to process data at scale in batch mode: Data-\nproc and Cloud Dataflow.\n Dataproc allows you to launch a fully configured Spark/Hadoop cluster and exe-\ncute Apache Spark jobs on it. These clusters don’t need to store any data locally and\nthey can be ephemeral—meaning that if all data is stored on Google Cloud Storage, a\nDataproc cluster is only required for the duration of the data transformation job, sav-\ning you money. \n Another Google Cloud service that can be used to process data from Cloud Stor-\nage is Cloud Dataflow. Cloud Dataflow is a fully managed execution environment for\nthe Apache Beam data processing framework. Cloud Dataflow can automatically\nadjust the compute resources required for your job depending on how much data you\nneed to process. Think of Apache Beam as an alternative to Apache Spark, and, like\nSpark, it’s an open source framework for distributed data processing. \n The main difference between Beam (Cloud Dataflow) and Spark (Dataproc) is\nthat Beam offers the same programming model for both batch and real-time data pro-\ncessing. On the other hand, Spark is a more mature technology that has been tested\nin multiple production environments. \nREAL-TIME DATA PROCESSING AND ANALYTICS\nThe primary cloud-native method of performing real-time data processing or analytics\non Google Cloud is to use Cloud Pub/Sub in conjunction with Apache Beam jobs run-\nning on the Google Cloud Dataflow service. Beam provides robust support for real-\ntime pipelines, including windows, triggers, dealing with late-arriving messages, and\nso on. Cloud Dataflow currently supports Java and Python Beam jobs. There’s no sup-\nport for SQL yet, but expect it to be added in future releases. \n As an alternative to Cloud Dataflow and Apache Beam for real-time processing and\nanalytics is Spark Streaming running on a Dataproc cluster. The Spark Streaming\napproach to real-time data processing is usually called micro-batching. Spark Stream-\ning doesn’t operate on one message at a time; instead, it combines incoming messages\ninto small groups (usually a few seconds long) and processes those micro-batches all at\nonce. Choosing between Apache Beam and Spark Streaming as your real-time data\n",
      "content_length": 3051,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "69\nMapping cloud data platform layers to specific tools\nprocessing engine on Google Cloud usually depends on whether you have existing\ninvestments into Apache Spark. This may include skills that your team has or an existing\ncode base. Google is making significant investments into its Cloud Dataflow + Beam\ncombo, so for new development this may be a better choice long term. Additionally,\nBeam provides richer semantics when it comes to real-time data processing. So if most\nof your pipelines are or will be real-time, then Beam would be a good choice.\nCLOUD WAREHOUSE\nBigQuery is Google’s offering in the managed cloud data warehouse space. It is a dis-\ntributed data warehouse with several unique properties—automatic compute capacity\nmanagement and robust support for complex data types. Where other cloud ware-\nhouses will require you to specify how many nodes you want in your cluster and what\ntypes of nodes you need up front, BigQuery manages the compute capacity for you\nautomatically. For each query you issue, BigQuery will decide how much processing\npower you need and allocate only the resources required. BigQuery offers a per-query\nbilling model, where you only pay for the volume of data each query has to process.\nThis works really well for low-volume analytics workloads or ad hoc data exploration\nuse cases, but it can make it difficult to predict or estimate BigQuery costs. BigQuery\nalso has robust support for complex data types such as arrays and nested data struc-\ntures, making it a great choice if your data sources are JSON-based. \nDIRECT DATA PLATFORM ACCESS\nThere is currently no dedicated service from Google Cloud to directly access data in\nthe lake. BigQuery supports external tables, which allows you to create tables that are\nphysically stored on Google Cloud Storage, without having to load the data into Big-\nQuery first. BigQuery also allows you to create temporary external tables that only\nexist for the duration of your session. Temporary external tables are well suited for ad-\nhoc data exploration on the lake. The limitations of using BigQuery as a data platform\naccess mechanism is that currently you need to provide a schema for each external\ntable. This can be a big barrier for ad hoc analysis, since the schema is often not\nknown at this stage. An alternative way to work with data in Google Cloud Storage\ndirectly is to provision a temporary Dataproc cluster and use Spark SQL to query data\nin the lake. Spark can infer schema for most of the popular file types automatically,\nmaking data discovery easier. \nETL OVERLAY AND METADATA REPOSITORY\nCloud Data Fusion is a managed ETL service available on Google Cloud. Cloud Data\nFusion allows data engineers to construct data processing and analytics pipelines\nusing a UI editor and then have those pipelines translated into one of the data pro-\ncessing frameworks to be executed at scale on Google Cloud. Currently, only Apache\nSpark running on Dataproc is supported, with Apache Beam planned for future\nreleases. The main benefit of using an ETL overlay such as Cloud Data Fusion is that it\nprovides mechanisms for people to search for existing data sets and immediately see\nwhich pipelines and transformations affect them. This allows you to perform a quick\n",
      "content_length": 3256,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "70\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nimpact analysis to understand what data will be affected if a given pipeline is changed.\nCloud Data Fusion also tracks a number of statistics about pipeline execution, such as\nthe number of rows processed, timing of different stages, and so on. This information\ncan be used for monitoring and debugging purposes. \nORCHESTRATION LAYER\nCloud Composer is a fully managed service for complex job orchestration. It is based\non a popular Apache Airflow project and can execute existing Airflow jobs without\nany modifications. Airflow allows you to author jobs that consist of multiple steps. For\nexample, steps could be these: read a file from Google Cloud Storage, launch a Cloud\nDataflow job to process it, and send a notification upon success or failure. Airflow also\nsupports dependencies between jobs and allows you to rerun either separate steps or\nfull jobs on demand. Cloud Composer makes managing an Airflow environment eas-\nier because provisioning the required virtual machines, installing and configuring\nsoftware, and so on, are part of the service.\nDATA CONSUMERS\nBigQuery doesn’t currently have native support for JDBC/ODBC drivers, but these\ndrivers are available for free from a third party called Simba Technologies. BigQuery\nnative data access is all done via a REST API because BigQuery acts more like a global\nSaaS than a typical database. JDBC/ODBC drivers from Simba act as a bridge between\nthe JDBC/ODBC API and the BigQuery REST API. \n As with any translation from one protocol to another, there are limitations, primarily\naround response latency and total throughput. These drivers may not be suitable for\napplications that require low-latency response or that need to extract large (tens of GBs)\nof data from BigQuery. Fortunately, a number of existing BI and reporting tools are\nimplementing native BigQuery support, eliminating the need for a JDBC/ODBC driver.\n You should check that the reporting or BI tools you plan to use with your Google\nCloud data platform offer support for BigQuery. When it comes to data consumers\nwho need real-time data access, Google Cloud offers a fast key/value store called\nCloud Bigtable that can be used as a caching mechanism. As with AWS, you will need\nto implement and maintain application code that will load results from your real-time\npipelines into Cloud Bigtable and then either build a custom API layer on top of\nCloud Bigtable or use the Cloud Bigtable API directly in your applications.  \n3.3.3\nAzure\nAzure is a public cloud service from Microsoft, who have a long and very successful\nhistory of implementing data-related products associated with their flagship MS SQL\nServer RDBMS. Azure offers a range of services based on MS SQL Server, but they also\nhave a set of completely new cloud-native products, including Data Factory, Cosmos\nDB, and others. In this section, we will provide an overview of Azure tools that you can\nuse to implement a cloud data platform as shown in figure 3.21.\n",
      "content_length": 3047,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "71\nMapping cloud data platform layers to specific tools\nBATCH DATA INGESTION\nAzure Data Factory is an ETL overlay service that supports batch ingestion. Currently,\nData Factory supports a wide range of RDBMS products, ingesting flat files from FTP,\nAzure Blob Storage, S3, and Google Cloud Storage; NoSQL databases such as Cassan-\ndra and MongoDB; and a growing number of connectors for external SaaS platforms\nsuch as Salesforce and Marketo. Data Factory also integrates with most Azure data\nproducts like Cosmos DB, Azure SQL Database, and many others. Compared to AWS\nand Google ETL overlays, Data Factory offers the biggest library of connectors and\nsupports some extensibility in the form of a generic HTTP connector. You can use this\nconnector to implement your own ingestion pipelines for REST APIs (internal or\nexternal) that may not be currently supported out of the box. As with other cloud pro-\nviders, Azure has support for a serverless execution environment called Azure Func-\ntions that can be used to implement your own ingestion mechanisms in one of the two\ncurrently supported languages: Java and Python.\nSTREAMING DATA INGESTION\nAzure Event Hubs is a service that allows you to send and receive data from streaming\nsources. It is similar in functionality to AWS Kinesis and Google Cloud Pub/Sub. A\nunique feature of Event Hubs is its compatibility with Apache Kafka API. We will talk\nabout Kafka later in this chapter, but API compatibility means that if you have existing\ninvestments or skills with Kafka, it will be easier to migrate to Event Hubs. Event Hubs\nCapture is a supporting service to Event Hubs that allows you to save messages from\nEvents Hubs into various Azure services, such as Azure Blob Storage or Azure Synapse. \nFigure 3.21\nAzure services for a cloud data platform\nBatch \ndata\nStreaming\ndata\nAzure Data Lake Storage\nSlow storage/direct data lake access\nAzure Event Hubs\nFast storage\nAzure Data Factory,\nAzure Data Catalog\nOperational metadata\nAzure\nSynapse\ndata\nwarehouse\nAzure Stream Analytics\nReal-time processing and analytics\nCosmos\nDB +API\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nSQL, JDBC\nData\nconsumers\nData Factory\nETL tools overlay\nAzure Data Factory\nOrchestration overlay\nAzure Databricks\nBatch processing and analytics\nAzure Event\nHubs\nIngestion\nAzure Data\nFactory,\nAzure\nFunctions\n",
      "content_length": 2332,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "72\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nDATA PLATFORM STORAGE\nSimilar to other major cloud providers, Azure offers a scalable and cost-efficient data\nstorage service called Azure Blob Storage. On top of this service, Azure has imple-\nmented a new service offering called Azure Data Lake Storage. This service provides\nseveral improvements over regular Azure Blob Storage, especially when it comes to\nperformance of large-scale data processing jobs. \nBATCH DATA PROCESSING\nAzure doesn’t offer services like AWS EMR or Google Cloud Dataproc. Instead, they\nhave partnered with a company called Databricks to offer a flexible environment to\nexecute Apache Spark jobs. Databricks was founded by the original creators of Apache\nSpark and currently offers its managed Spark environments on both Azure and AWS.\nAzure Databricks provides seamless integration with the rest of the Azure ecosystem. It\nhas connectors to work with data in Azure Data Lake Storage, read and write data to\nAzure Warehouse, etc. \nREAL-TIME DATA PROCESSING AND ANALYTICS\nAzure Stream Analytics is a service that allows you to perform data transformations\nand analysis on messages from Event Hubs. Stream Analytics uses a SQL-like language\nto do this. You can save the results of the Stream Analytics job to a number of sup-\nported destinations. Currently this includes a new Event Hubs destination, Azure Data\nLake Storage, a SQL database, and Cosmos DB. Since Event Hubs is compatible with\nKafka, you can also implement your own Kafka Streams real-time pipelines or migrate\nexisting ones. \nCLOUD DATA WAREHOUSE\nAzure Synapse is a scalable, cloud-native warehousing service. It’s built on top of\nproven MS SQL Server technology and offers several compelling features. First, Azure\nSynapse separates storage from compute resources. When creating a new Azure Syn-\napse warehouse, you need only specify how much computational capacity you will\nneed, as storage is adjusted automatically based on your needs. This means that you\ncan scale your Azure Synapse data warehouse compute resources up and down on\ndemand while keeping the data on the storage intact. For example, you can switch to\na lower computer tier during off hours or on weekends to reduce cost. The Azure Syn-\napse model is a hybrid between AWS Redshift, where data has to be stored locally on\nthe cluster machines, and BigQuery, where Google Cloud manages both compute and\nstorage capacity for you. In Azure Synapse, you have full control over how much com-\npute you need, and storage is managed for you. Since Azure Synapse is built on top of\nrelational database technology, it offers robust SQL support and is compatible with all\nthe tools that have JDBC/ODBC driver support. With the recent updates, Azure Syn-\napse also supports storing and processing JSON data. \nDIRECT DATA PLATFORM ACCESS\nA recommended way to access and process data stored in Azure Blob Storage is to use\nan Azure Databricks platform. Databricks is a third-party company that provides a data\n",
      "content_length": 3046,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "73\nMapping cloud data platform layers to specific tools\nprocessing platform based on Apache Spark. Databricks significantly simplifies creat-\ning and managing Spark clusters and focuses on providing a collaborative environ-\nment for multiple teams to work on the same data sets. You can use Spark SQL to issue\nSQL queries to work directly on the data in Blob Storage, or you can use native Spark\nAPIs. Databricks allows you to easily create multiple independent clusters that have\naccess to the same data sets. This simplifies resource management and allows fine tun-\ning your processing clusters to fit a specific task. \n What’s unique about the Azure Databricks service is that it’s natively integrated\ninto the Azure platform and looks and feels just like another Azure service. There are\nAzure APIs that allow you to create and manage Databricks workspaces, and Azure\nprovides out-of-the-box integration with other data services like Data Factory, Azure\nSynapse, and others.\nETL OVERLAY AND METADATA REPOSITORY\nAzure Data Factory is an ETL service that you can use to ingest data from sources, pro-\ncess it, and then save data to various destinations. Data Factory provides a UI where\nyou can construct, execute, and monitor your pipeline, but it also has very robust API\nsupport, which makes it possible to automate pipeline creation and deployment. This\nis an important feature that allows your ETL pipeline to be made a part of the contin-\nuous integration/continuous deployment (CI/CD) process. \n Data Factory currently has limited support for data transformations that work out\nof the box—mostly converting files from one format to another. To allow users to per-\nform more complex transformations, Data Factory provides hooks into Azure Data-\nbricks. This way you can use Data Factory to ingest data from the source and then\nexecute a complex Spark transformation job using an Azure Databricks hook. When it\ncomes to pipeline-specific metadata, Azure Data Factory captures and allows you to\nmonitor metrics like pipeline success/failure, duration, and a few others. Currently,\nthe number of available metrics is quite limited and doesn’t allow you to capture\nevents like schema changes, data volume changes, and so on.\n Google Cloud Data Catalog is a separate service that is more focused on business\nmetadata (data that adds business context to other data, providing information\nauthored by business people and/or used by business people) and data discovery\nfunctionality. It integrates with various Azure and external sources and allows you to\ncreate a searchable catalog of existing data assets. Additionally, it can perform basic\ndata profiling (total number of rows, number of distinct or empty values in each col-\numn, and so on) for sources that support SQL access. \nORCHESTRATION LAYER\nAzure Data Factory provides pipeline-scheduling capabilities and also supports com-\nplex pipeline dependencies. For example, you can create a chain of pipelines, where\nas one pipeline finishes successfully, it triggers another pipeline, and so on. \n",
      "content_length": 3053,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "74\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\nDATA CONSUMERS\nAzure Synapse provides full support for SQL consumers, including JDBC/ODBC driv-\ners. This means you can easily connect your favorite reporting client, BI, or SQL to it.\nAzure Databricks also offers JDBC/ODBC connectivity for reporting tools, but to use it,\nyou must ensure that there is an Azure Databricks cluster capable of processing incom-\ning queries that is always on. This approach may be expensive. For application consum-\ners who require more real-time direct access to the data platform, Azure offers a fast\ndocument-oriented database called Cosmos DB. You can save the results of your real-\ntime analytics there and then either connect your applications directly to Cosmos DB\nusing client libraries or build an API layer around it for more controlled data access.\n3.4\nOpen source and commercial alternatives \nThere are scenarios where services provided by public cloud vendors will not meet\nyour specific requirements. Over the years, we have seen the following cases where a\ncloud-native services may not be the best fit:\nFunctionality limitations—Cloud-native services are evolving quickly, but some of\nthem are really new and may not have all the features you need.\nCost—Depending on your data volumes, the cloud vendor pricing model may\nresult in costs that are not reasonable for your use case.\nPortability—There is an increasing demand for organizations to implement\nmulticloud solutions, either to avoid vendor lock-in or to utilize the best ser-\nvices from each provider.\nIn this section, we will give you an overview of some of the open source or commercial\nsoftware alternatives. Not all layers of a cloud data platform have a valid open source\nor commercial software alternative. For example, deploying and managing your own\ndistributed warehouse in the cloud will always be either cost prohibitive or not perfor-\nmant enough (or both), even though open source solutions like Apache Druid exist.\n3.4.1\nBatch data ingestion\nData ingestion is a major component of any data platform, whether it’s a cloud data\nplatform or a traditional on-premises warehouse. That’s why there are lots of tools,\nboth open source and third-party commercial offerings, that can play this role. \n Apache NiFi is one of the popular open source solutions that allows you to connect\nto various data sources and bring data into your cloud data platform. NiFi uses a plug-\ngable architecture that allows you to create new connectors using Java, but also comes\nwith a large library of existing connectors. Talend is another popular ETL solution\nthat you can use to implement a data ingestion layer. Talend uses an open-core model,\nwhere basic functionality is free and open source, but enterprise-level features require\na commercial license. Talend is not just an ingestion tool, and it makes more sense to\nuse its whole ecosystem of solutions that include data profiling, scheduling, and so on.\n",
      "content_length": 2998,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "75\nOpen source and commercial alternatives\n There are also many existing third-party SaaS solutions that specialize in bringing\ndata from various sources into your cloud environment. Alooma (acquired by Google\nin 2019) and Fivetran are two examples of such services. These SaaS services usually\nprovide a very rich set of connectors and additional features such as monitoring or\nlightweight transformation of data. The limitations of using SaaS providers for data\ningestion include having to send your data through a third party, which may not\nalways be acceptable from a security perspective. Also, these tools specialize in writing\ndata directly into the warehouse, making it challenging to integrate them into a flexi-\nble data platform architecture.\n3.4.2\nStreaming data ingestion and real-time analytics\nWe often see fast message store, streaming data ingestion, and real-time data process-\ning implemented using open source solutions instead of cloud-native services. This is\nbecause Apache Kafka is a leading open source solution in this space. Kafka offers a\nfast message bus for your streaming data sources, but also has a Kafka Connect com-\nponent that allows you to easily ingest data from various sources into Kafka. It also\ncomes with support for Kafka Streams—a way for you to implement real-time data pro-\ncessing or analytics applications. The reasons to choose Kafka instead of a cloud\nnative solution are performance, feature set richness, and existing expertise. If you\nhave existing investments into Kafka or have very specific performance requirements,\nyou may consider implementing your own streaming solution in the cloud using this\ntechnology. The downside, as with any open source solution, is that you will need to\ninvest in managing your Kafka cluster.\n3.4.3\nOrchestration layer\nApache Airflow is a popular open source job orchestration tool. It allows you to con-\nstruct complex job dependencies, and provides logging, alerting, and retry mecha-\nnisms. The Google Cloud Composer orchestration service is based on Apache Airflow\ntechnology. The benefit of using Airflow over a cloud service is flexibility because Air-\nflow job configurations are created using the Python programming language, which\nallows you to create dynamic job definitions. For example, you can create an Airflow\njob that can change its behavior based on external configuration or reach out to an\nexternal service to fetch configuration parameters.\nSummary\nA good modern data platform architecture will have six layers—a data ingestion\nlayer, a data processing layer, a metadata layer, a serving layer, and two overlay\nlayers—the orchestration and ETL layers—which coordinate work across multi-\nple layers.\nEach layer in a data platform architecture plays a very specific role, and it’s a\ngood idea from an architecture perspective for these layers to be “loosely cou-\npled,” or separate layers that communicate through a well-defined interface and\n",
      "content_length": 2950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "76\nCHAPTER 3\nGetting bigger and leveraging the Big 3: Amazon, Microsoft Azure, and Google\ndon’t depend on the internal implementation of a specific layer. This will allow\nyou to more easily mix and match different cloud services or tools to achieve\nyour goals and respond to changes in available services while minimizing data\nplatform impact.\nWhile the data processing world is moving to data streaming and real-time solu-\ntions, it’s good architectural thinking to build your data ingestion layer to sup-\nport both batch and streaming ingestion as a first-class citizens. This way, you\nwill always be able to ingest any data source, no matter where it comes from. \nThe data ingestion layer should be able to securely connect to a wide variety of\ndata sources, in streaming and/or batch modes, transfer data from the source\nto the data platform without applying significant changes to the data itself or\nit’s format, and register statistics and ingestion status in the metadata reposi-\ntory. Focus on ensuring your data ingestion layer has a pluggable architecture,\nscalability, high availability, and observability. \nThe storage layer in the data platform must store data for both the long term\nand short term and make data available for consumption either in batch or\nstreaming modes. Focus on designing your storage layer to be reliable, scalable,\nperformant, and cost efficient. \nThe processing layer is the heart of the data platform implementation and\nshould be able to read data in batch or streaming modes from storage, apply\nvarious types of business logic, and provide a way for data analysts and data sci-\nentists to work with data in the data platform in an interactive fashion.\nDesign your data processing layer to also provide direct data platform access.\nConsider using a distributed SQL engine that can work directly on files in the\ndata platform, a distributed data processing engine API, or directly reading files\nfrom the lake. \nA data platform metadata store stores information about the activity status of\ndifferent data platform layers and provides an interface for layers to fetch, add,\nand update metadata in the store. It’s very important for automation, monitor-\ning and alerting, and developer productivity. It also allows us to decouple differ-\nent layers from each other, reducing the complexities associated with\ninterdependencies. \nTechnical metadata management in the data platform is a relatively new topic.\nThere are few existing solutions that can fulfill all the tasks needed, so as it\nstands today, you will likely need a combination of different tools and services to\nimplement a fully functional technical metadata layer. \nThe serving layer delivers the output of analytics processing to the various data\nconsumers. Business users typically use SQL-based tools to access data via the\ndata warehouse. Programmatic data consumers require a dedicated API to con-\nsume data from the lake, usually in real-time.\n",
      "content_length": 2955,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "77\nExercise answers\nThe orchestration layer coordinates multiple data processing jobs according to\nthe dependency graph and handles job failures and retries. It should be scal-\nable, highly available, maintainable, and transparent.\nAn ETL overlay is a product or a suite of products whose main purpose is to\nmake the implementation and maintenance of cloud data pipelines easier.\nThese products absorb some of the responsibilities of the various data platform\narchitecture layers and provide a simplified mechanism to develop and manage\nspecific implementations. \nETL overlay tools work across the various layers as they are responsible for add-\ning and configuring data ingestions from multiple sources (ingestion layer),\ncreating data processing pipelines (processing layer), storing some of the meta-\ndata about the pipelines (metadata layer), and coordinating multiple jobs\n(orchestration layer).\nThe reality of a cloud data platform implementation is that you will most likely\nneed to mix and match several tools, and turning to PaaS services in your lay-\nered architecture will bring you a balance of agility and flexibility with mini-\nmum maintenance. Picking which tools to use requires you to evaluate your\nspecific requirements against features, ease of use, scalability, and more.\nIn an AWS cloud, consider using Glue, DMS, Kinesis, Kinesis Firehose, Lambda,\nGoogle Cloud Data Catalog, Step Functions, S3, EMR, Kinesis Data Analytics,\nRedshift, Athena, and DynamoDB. \nIn a Google cloud, consider using Cloud Pub/Sub, Cloud Dataflow, Cloud Data\nFusion, Cloud Functions, BigQuery, BigQuery Data Transfer Service, Cloud\nComposer, Dataproc, Cloud Catalog, and Cloud Storage.\nIn an Azure cloud, consider using Data Factory, Event Hubs, Azure Functions,\nDatabricks, Data Catalog, Stream Analytics, Data Lake Analytics, Data Lake Stor-\nage, Azure Synapse, and Cosmos DB.\n3.5\nExercise answers\nExercise 3.1:\n 3—Some data sources don’t support real-time ingestion.\nExercise 3.2:\n 2—To support both batch and real-time processing.\nExercise 3.3:\n 1—To support consumers with different needs and requirements.\nExercise 3.4:\n 3—To manage dependencies between different data processing jobs.\n",
      "content_length": 2195,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "78\nGetting data\n into the platform\nIf you’ve read the chapters up to this point, you’re able to architect a good, layered\ndata lake. Now it’s time to start diving into a few of these layers in much greater\ndetail.\nThis chapter covers\nUnderstanding databases, files, APIs, and streams \nIngesting data from RDBMSs using SQL versus \nchange data capture\nParsing and ingesting data from various file formats\nDeveloping strategies to deal with source schema \nchanges\nDesigning an ingestion pipeline to handle the \nchallenges of data streams\nBuilding an ingestion pipeline for SaaS data\nImplementing quality control and monitoring in your \ningestion pipeline\nDiscussing network and security considerations for \ncloud data ingestion\n",
      "content_length": 733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "79\nDatabases, files, APIs, and streams\n In this chapter, we’ll focus on the ingestion layer. Before you can start using your\ncloud data platform to produce outcomes using traditional or advanced analytics or\nreports, you will need to populate it with data. One of the key characteristics of a data\nplatform is its ability to ingest and store data of all types in its native format. This vari-\nety does present challenges, so we’ll walk through the most popular data types—\nRDBMs, files, APIs, and streams—and help you understand how they are different\nfrom the perspective of ingestion. We’ll also touch on the networking and security\nconsiderations that apply regardless of the data source to be ingested.\n By the end of the chapter, you’ll be able to choose the most appropriate ingestion\nmethod for the various types of data sources and use cases and design a robust and\nappropriate ingestion process for each. You’ll also be able to guide developers in iden-\ntifying and dealing with common ingestion challenges, including mapping data types,\nautomation, and data volatility. Finally, you’ll be able to anticipate and work around\nthe most common gotchas when ingesting data.\n4.1\nDatabases, files, APIs, and streams\nThe importance and complexity of establishing reliable data ingestion pipelines are\noften overlooked. \n The complexity comes from the fact that a modern data platform must support\ningestion from a variety of different data sources (remember the three V’s from chap-\nter 1?), often at high velocities and in a consistent, uniform way. In this chapter, we will\ndo an overview of the most common types of data sources we see today and provide\nsome guidance on how to establish a robust ingestion process for each of them. We will\nspecifically focus on databases (relational and NoSQL), files, APIs, and streams. \nNOTE\nIf you are experienced in the use of traditional data warehouses, this\nchapter might cover material you already know, but because data platforms\nare increasingly being designed by data engineers who don’t necessarily come\nfrom a traditional data warehousing world, we felt it was important to include\nthis detail for their benefit.\nEach of these source types have their own unique characteristics. For example, rela-\ntional databases always have data types associated with each column and are organized\ninto tables. Flat CSV files also have a table-like format, but have no data type informa-\ntion attached to columns, so you need to figure out how to deal with this in your plat-\nform. Figure 4.1 illustrates the most common considerations for each data source type.\n This next section will walk through these data sources, exploring the attributes of\neach that will most impact your ability to ingest them into your data platform and\nsome key things to keep in mind as you prepare to ingest data from these sources. In\nthe following sections of this chapter, we will do a deeper dive into each of these data\nsource types and outline specific guidelines and gotchas about how to design and\nimplement a data platform ingestion layer for each of them.\n",
      "content_length": 3085,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "80\nCHAPTER 4\nGetting data into the platform\n4.1.1\nRelational databases\nRelational databases (referred to as RDBMSs) are one of the most common data\nsources to be integrated into a data platform. Today, RDBMSs power most of the\napplications responsible for business operations and contain valuable information\nabout core business transactions such as payments, orders, bookings, etc. Data in\nRDBMSs is organized into tables, each containing one or more columns. Columns\nhave clear data types (such as strings, integers, dates, etc.) associated with them, and\nRDBMSs takes care of making sure that data with the wrong type doesn’t get inserted\ninto the database. Data in relational databases is typically highly normalized, meaning\ndata entities are split into multiple tables that can be joined using a common key at\nquery time. As such, it’s not unusual to have databases with hundreds or even\nthousands of different tables in them. Finally, because of the operational nature of\nthese databases, data in thesm changes all the time: new orders arrive, get changed,\ncanceled, or delivered.\n What do these RDBMS properties mean when it comes to implementing a data\ningestion process into a common data platform? Here are some of the most important\nconsiderations:\nMapping data types—Column data types from the source database must be\nmapped into the destination cloud warehouse. Unfortunately, each RDBMS\nand cloud vendor has their own set of supported data types. While many types\noverlap, such as strings, integers, and dates, there are always data types that are\neither unique to a particular vendor or behave differently in different data-\nbases. For example, TIMESTAMP types can have different precisions (microsec-\nond, nanosecond, etc.), while DATE types can expect dates to be formatted a\ncertain way, etc.\nAutomation—Since RDBMSs are often composed of hundreds of different tables,\nyour ingestion process must be highly configurable and automated. No one will\nCloud data platform\nRDBMS\nFiles\nSaaS\nAPIs\nData type mapping,\nhundreds of tables,\nneed to capture state\nchanges, volatile data\nMultiple file\nformats, no schema\nenforcement, no data\ntype information\nNo single API\nstandard, no data\ntype information,\ncontinuous API\nchanges\nNo single message\nformat, no data\ntype information,\npotential duplicates,\nhigh volume\nFigure 4.1\nCommon data source types\n",
      "content_length": 2360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "81\nDatabases, files, APIs, and streams\nhave time to manually configure ingestions for 600 different tables. Even if you\nhave time to do it manually, you will most likely make mistakes along the way and\nend up with an ingestion process that is brittle and nonreproducible.\nVolatility—Data in RDBMSs is typically highly volatile—businesses are never\nstatic and a lot of the business operations are captured in the database. For\nexample, if you are dealing with a large e-commerce site, you will see hundreds\nor thousands of orders being placed, processed, edited, or cancelled every sec-\nond. This in turn will result in data changes that affect dozens of tables. Your\ningestion process must be able to deal with constantly changing data, capture\ndata state at a given point in time, and deliver it into the data platform, where it\ncan be processed and analyzed further.\n4.1.2\nFiles\nFiles are another very common data source for your data platform. These are usually\ntext or binary files that are delivered to the target system through FTP protocol or\nplaced on cloud storage such as Google Cloud Storage (GCS), S3, or Azure Blob Stor-\nage, where ingestion processes pick them up. It’s worth mentioning that we have seen\nsome more exotic ways of delivering data into a data platform, such as sending files as\nattachments to a dedicated email address. We strongly advise against such an\napproach for security and reliability reasons. \n Despite their simplicity, files are actually really tricky to deal with from an ingestion\nstandpoint. First of all, they come in many different formats. The most popular text\nformats are CSV, JSON, and XML. Binary formats are slightly less common and may\ninclude Avro or Protocol Buffers (protobuf). Text file formats don’t include any col-\numn type information and don’t usually enforce any constraints on the data or file\nstructure itself. This means there is no guarantee that files you receive will follow a\nconsistent structure even for the same source. Your ingestion process must therefore\nbe very resilient to change and must be able to deal with many different edge cases. \n Here are some considerations when building a file-based ingestion process:\nParsing different file formats—You will need to parse different file formats: CSV,\nJSON, XML, Avro, etc. In the case of text file formats, there may be no guaran-\ntee that whoever produced the file followed the same conventions that your\nparser expects.\nDealing with schema changes—Unlike with RDBMSs, adding a new column to a\nCSV file or a new attribute to a JSON document is easy for data producers, and\nas such our experience shows that schema changes cases are very common in\nfile-based data sources. Your ingestion process needs to be able to deal with this. \nSnapshots and multiple files—As opposed to highly volatile RDBMS data, files usu-\nally represent a snapshot of some data set in time. A typical flow for a file-based\ndata source is that data is extracted from some other system, saved as a text file,\nand then delivered to a destination. A single file can either represent a full\nsnapshot of the source system (e.g., all orders) or a data increment (e.g., new\n",
      "content_length": 3170,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "82\nCHAPTER 4\nGetting data into the platform\norders since yesterday). Also any given ingestion batch can either be delivered\nas a single file or multiple files. Your ingestion process must take all of these\noptions into account.\n4.1.3\nSaaS data via API\nIt’s almost impossible to find a business that doesn’t rely on SaaS products for some of\ntheir business operations. SaaS products such as Salesforce or Marketo host some of a\nbusiness’s most important data sets. Combining customer data (Salesforce), and mar-\nketing campaign data (Marketo) with the information about business transactions\n(RDBMS) is a common desired outcome of a unified data platform. Most SaaS compa-\nnies allow you to extract your data using a REST API. Usually this is an HTTP request\nthat returns some data in a JSON format. You may also have an option to download\ndata as a flat CSV file, but since APIs provide more flexibility in the way data can be\naccessed or filtered out, they are a preferred way to extract data from SaaS systems. \n There are multiple challenges associated with using SaaS APIs. Each SaaS provider\ndevelops their own unique way of exposing data to external data consumers. For\nexample, one provider may one have a single API endpoint that provides you with all\nthe data you have in the system. Another provider may have an API endpoint for each\nobject in their system, such as Customer, Contract, Vendor, etc. Some providers allow\nyou to fetch data for a specified time frame, while others may either provide you with\na full snapshot of data or only give you the last couple of days worth of data. This list\ngoes on and on. This lack of standardization for data access and the resulting variety\nof API access methods makes ingesting data from SaaS into a data platform a challeng-\ning task. Here are some of the considerations you need to keep in mind when design-\ning an ingestion process for SaaS:\nIf your organization uses multiple SaaS solutions, then you need to implement\ndifferent data ingestion pipelines for each of those and constantly update them\nto keep up with the provider-introduced changes. \nVery few SaaS provider APIs have any data type information available. From the\nperspective of data types, ingesting data from APIs is similar to dealing with\nJSON files—it’s up to the data pipeline implementation to do the necessary\ndata type and schema validations. \nThere is also no standard for full versus incremental data load when it comes to\nSaaS APIs. You will need to adjust your pipeline for each provider.\n4.1.4\nStreams\nA more recent addition to the list of data source types that a data platform architect\nmust take into account are streams. Data streams usually represent events that happen\nat a given point in time. For example, a popular data stream for web applications is\nclickstream data, where each time a visitor clicks on an object on a web page, this\naction is captured as an event with various different attributes (user IP address,\nbrowser type, etc.). A series of these actions combined with the time of the event form\n",
      "content_length": 3054,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "83\nIngesting data from relational databases\na clickstream, which can represent many different activities and business operations.\nPlacing and removing items in a shopping cart of an e-commerce site can be repre-\nsented as a sequence of corresponding events—or as a stream—as can depositing and\nwithdrawing funds from a bank account. While RDBMSs represent the current state of\nthe system—for example, items in the shopping cart at the time of the checkout or\ncurrent bank account balance at a point in time—event streams represent the\nsequence of actions that occurred prior to arriving at this current state. This informa-\ntion is extremely valuable for analytic purposes because it can provide insights into\nuser motivation, system optimization opportunities, and so on.\n The concept of event streams is not new, but several technological advances now\nmake capturing and storing events scalable, reliable, and cost effective. Apache Kafka\n(an open source stream-processing software platform developed by LinkedIn,\ndonated to Apache Software Foundation, and written in Scala and Java) is the most\npopular open source project created specifically for this purpose. Cloud-native\nstreaming services are available from AWS (Kinesis), Google Cloud Pub/Sub, and\nAzure (Event Hubs) for integrating streaming data from cloud-native applications.\nConsiderations for ingestion pipeline design are the same across both open source\nand cloud service options and include\nIn data streams systems, there is a message format restriction. All messages are\nstored as an array of bytes and can encode data as a JSON document, an Avro\nmessage, or any other format. Your ingestion pipeline must be able to decode\nthe messages back into a consumable format. \nTo deliver data reliably and at scale, streaming data systems allow the same mes-\nsages to be consumed multiple times. This means that the data ingestion pipe-\nline must be able to deal efficiently with duplicate data.\nMessages in streaming data systems are immutable. Once written by the pro-\nducer, they cannot be changed, but a new version of the same message can be\ndelivered later, so your data platform may need to be able to resolve multiple\nversions of the same message.\nStreaming data is typically high-volume data. Your ingestion pipeline must be\nable to scale accordingly. \nIn the following sections of this chapter, we will do a deeper dive into each of these\ndata source types and outline specific guidelines and gotchas about how to design and\nimplement a data platform ingestion layer for each of them.\n4.2\nIngesting data from relational databases\nIn this section, we will discuss different ways to ingest data from RDBMSs. There are\ntwo primary ways to establish an ongoing ingestion process from databases: using a\nSQL interface and using change data capture (CDC) techniques. We will start with\nusing a SQL interface, since it’s a more common method of ingesting data, and we\nwill discuss two methods of ingestion using a SQL interface: full-table versus incre-\nmental table ingestion. \n",
      "content_length": 3047,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "84\nCHAPTER 4\nGetting data into the platform\n4.2.1\nIngesting data from RDBMSs using a SQL interface \nAll relational databases support SQL. The most basic way to extract some data from a\ndatabase is to run a SQL query that looks something like this:\nSELECT * FROM some_table;\nThis query will give you all columns and all rows from a given table. You can then save\nthese results into a flat file (CSV or Avro), for example, in a landing area of your data\nplatform, and there you have a basic RDBMS ingestion process. \n Of course, there is much more involved in making this ingestion process work con-\nsistently and well. To explain, let’s take a look at figure 4.2.\nThere are several important things we need to figure out before we can implement an\ningestion process like this. First of all, we need an application of some sort to actually\nrun those SQL queries against RDBMSs and then save the result to the data platform.\nIn chapter 3 we provided some examples of different cloud-native or open source\ntools that can be used for ingestion. For this specific use case, ingesting data from\nRDBMSs, it’s important that the tool is capable of\nExecuting a SQL statement against different kinds of RDBMSs\nSaving results in one or more formats into a cloud storage\nAllowing specifying filtering conditions and other parameters for the SQL\nquery\nfield1\nfield2\nfield3\nfield1\nfield2\nfield3\nRDBMS Table1\nRDBMS Table2\nIngestion\napplication\nCloud data platform\nSQL\nSQL\nSELECT * FROM Table1\nSample query for data selection \nAn  ingestion application runs SQL\nqueries against the RDBMS and then\nsaves the result to the data platform. \nFigure 4.2\nRDBMS ingestion process\n",
      "content_length": 1658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "85\nIngesting data from relational databases\nIf we look at this list from another angle, we will see a few things. First of all, we need\nsome kind of connector that allows our application to read data from RDBMSs. Luck-\nily for us, RDBMSs have been around for decades, and there are database connectors\n(or drivers) available for just about every RDBMS vendor that can be used with most\npopular programming languages. \n Next, if you look carefully at the diagram you will see that in the RDBMS ingestion\nprocess, data will need to travel through at least three different levels, as shown in fig-\nure 4.3.\nOn level 1, data is stored using native RDBMS types. Then, when a SQL query is exe-\ncuted by an ingestion application, these native RDBMS data types will be converted\ninto a data type supported by the specific programming language in which the inges-\ntion application is implemented (level 2). Finally, when the ingestion application\nsaves data into the landing area of the cloud data platform, data types must be con-\nverted once again into the data types that your chosen file format supports (level 3).\nSince data type mapping is not always one to one, and given that during the ingestion\nprocess from RDBMSs, data types change at least twice, data types mapping is import-\nant. We will talk more about this later in this chapter. \n Finally, if all your ingestion application can do is execute SELECT * FROM\nsome_table, then you are limited to doing only full-table ingestion. In many scenarios,\nthis is not enough, and a more sophisticated way of identifying and ingesting new or\nchanged data is required. \nfield1\nfield2\nfield3\nfield1\nfield2\nfield3\nTable1\nTable2\nIngestion\napplication\nCloud data platform\nLanding\nProcessing\nWarehouse\nSQL\nSQL\nSELECT * FROM Table1\nWhen a SQL query is\nexecuted by an ingestion\napplication, native RDBMS\ndata types are converted\ninto a data type supported\nby the ingestion application\n(level 2).\nWhen the ingestion application\nsaves data into the landing area\nof the cloud data platform, data\ntypes are converted into the\ndata types that your chosen file\nformat supports (level 3).\nData is stored in the\nRDBMS using native\nRDBMS types (level 1). \nFigure 4.3\nDifferent levels of an ingestion process\n",
      "content_length": 2237,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "86\nCHAPTER 4\nGetting data into the platform\n4.2.2\nFull-table ingestion\nTypically, RDBMS power applications are responsible for business operations, and\ndata in the RDBMSs changes all the time: new rows are added, existing rows are\nupdated, and some rows are deleted. It is unusual to see an application that mostly\nworks with static data. When designing a data ingestion pipeline for our cloud data\nplatform, we need to decide how to deal with this constantly changing data.\nNOTE\nThere is a difference between which data is important in an opera-\ntional database and which is important in an analytical data platform. Opera-\ntional databases are usually concerned with the question of “What is the\ncurrent state of some item?” It could be which items are in the shopping cart\nright now, what is the user’s account balance, or how many green gems did a\nplayer collect in the current game? Analytical data platforms are usually con-\ncerned with the question “How did a given item change over time?” In which\norder did the customer add items into the shopping cart? Did they add some\nitems that were later removed? To be able to answer these questions, analyti-\ncal data platforms need to store data differently than operational databases. \nImagine that we are building a cloud data platform for some kind of online service.\nThis service allows users to sign up for a trial account, and after the trial period\nexpires, users can either buy a premium subscription or cancel their account. Our\nchallenge is to design a pipeline that will allow us to capture not only the user status at\na point in time, but also how it evolves over time.\n Let’s dive in. In our scenario, our service uses a relational database as a backend.\nAs new users sign up and existing users change their subscription status, data in our\noperational database may look something like figure 4.4.\n We start with two users, one with a premium subscription and one recently joining\nfor a trial (A). After some time, a new user signs up for a trial, and row with user_id=3\nis added to our table (B). Next, user with user_id=2 decides to switch to a premium\nsubscription (C). Finally, because we don’t have an analytics data platform built yet,\nwe couldn’t identify which incentives to provide for the new trial user and market\nthose incentives to them. So this user decides to cancel their account, and their entry\nis deleted from the table (D). \n These events can be happening in any time frame. All these changes can happen\nwithin hours, minutes, or days. What’s important here is that we started with a table\nwith two rows in it and we ended up with two rows, but the data in these rows is now\ndifferent, and there are important things that happened in between. For example, for\nanalytics purposes, the fact that they canceled their account is important, but in an\nRDBMS, this data would be lost when the entry is deleted from the table. To make\nsure we have all the data we need to do our analysis, we need to design an ingestion\npipeline into our data platform that will allow us to capture not only the data at a\npoint in time, but also how it evolves over time.\n",
      "content_length": 3131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "87\nIngesting data from relational databases\nOne way to address this problem is to create an ingestion pipeline that performs a\nperiodic full-table ingestion from an RDBMS—the simplest though limited approach\nto doing this. Basically, a full-table ingestion pipeline performs the following steps:\n1\nStart the pipeline on a given schedule.\n2\nExecute a SQL query—SELECT * FROM some_table—against the source database.\n3\nSave the results into the cloud data platform storage.\n4\nLoad data into the cloud warehouse.\nWith this full-table ingestion strategy, we read the entire table every time the pipeline\nruns. Let’s assume we run this pipeline once a day for four days. Using our previous\nexample containing users-subscription status, we would end up with four different\nsnapshots of our table in the cloud data platform storage, as shown in figure 4.5.\n Each day we extract all rows from the source database and land this snapshot in a\nfolder in our cloud data platform storage. We will talk more about organizing the data\nin cloud storage in chapter 5, but for now let’s assume we just save the snapshot in the\nfolder that uses the ingestion date as its name. So now we have four snapshots saved in\nUSER_ID\nSTATUS\nJOINED_DATE\n1\nPREMIUM\n2018-03-27\n2\nTRIAL\n2019-05-01\nUSER_ID\nSTATUS\nJOINED_DATE\n1\nPREMIUM\n2018-03-27\n2\nTRIAL\n2019-05-01\n3\nTRIAL\n2019-05-04\nnew row added\nUSER_ID\nSTATUS\nJOINED_DATE\n1\nPREMIUM\n2018-03-27\n2\nPREMIUM\n2019-05-01\n3\nTRIAL\n2019-05-04\nrow updated\nUSER_ID\nSTATUS\nJOINED_DATE\n1\nPREMIUM\n2018-03-27\n2\nPREMIUM\n2019-05-01\nrow deleted\nA. We start with two users,\none with a premium subscription\nand one recently joining for a trial. \nB. A new user signs up for a trial,\nand a row with user_id=3 is\nadded to our table.\nC. A user with user_id=2 decides\nto switch to a premium subscription,\nand a row is updated.\nD. A user decides to cancel their\naccount, and their entry is\ndeleted from the table. \nFigure 4.4\nData is always changing in an operational database.\n",
      "content_length": 1969,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "88\nCHAPTER 4\nGetting data into the platform\ndifferent folders in the storage. In our cloud warehouse, we have two options. We can\nstack these snapshots one on top of the other, as shown in figure 4.5, resulting in one\nlong table. Or we can keep only the latest snapshot in the warehouse. As we discussed\npreviously, keeping only the latest state of the data usually is not acceptable for analyt-\nical use cases, because you will lose a lot of details of how data arrived at this state. \n In our example, if we only keep the latest snapshot in the warehouse, then a data\nanalyst looking into our warehouse for user subscription data will only see what is\nshown in figure 4.6.\nBy looking at this data, we will not be able to see the whole story behind it: users join-\ning, switching to premium or dropping off in between, etc. This is a lot of useful infor-\nmation to lose. A preferred way is to add snapshots on top of one another in the\nwarehouse, as shown in figure 4.7.\n So, basically, we end up with a warehouse table where we have all the rows from\neach snapshot. Note that we have also added a new column in the warehouse that\ndoesn’t exist in the source data: INGEST_DATE. This allows us to distinguish between\ndifferent snapshots, because each of them will have its own ingestion date. There are\nsome other system columns that we recommend adding to each of the ingested tables\nin the cloud data platform, and we will talk more about those in chapter 5.\n \n \n \nA\nB\nC\nD\nA\nB\nC\nD\nSource\ndatabase\nCloud data platform storage folders\nCloud data warehouse\n2019-05-03\n2019-05-04\n2019-05-05\n2019-05-06\nWith full-table ingestion, the entire table from the source database is\nread every time the pipeline runs. Running the pipeline once a day for\nfour days will result in four different snapshots of our table in the cloud \ndata platform storage and data warehouse. \nFigure 4.5\nFull-table snapshots on cloud storage and in the warehouse\nUSER_ID\nSTATUS\nJOINED_DATE\n1\nPREMIUM\n2018-03-27\n2\nPREMIUM\n2019-05-01\nD\nFigure 4.6\nLatest users-\nsubscriptions table snapshot.\n",
      "content_length": 2059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "89\nIngesting data from relational databases\nThis table gives us much more information about how data has changed over time\nthan just storing the latest snapshots, but this data model has its flaws as well. For\nexample, it is easy to write a SQL query that will show you all changes to a particular\nuser, in this case user_id = 2, over time:\nSELECT * FROM subscriptions WHERE user_id = 2 \nThis would give you the history of how the user first signed up as a trial user and then\nswitched to a premium subscription. But if you repeat the same query for user_id = 3,\nthen it would seem that they signed up for a trial and continued with the trial sub-\nscription, when in fact they cancelled their trial and deleted their account. So one of\nthe flaws of this design is that it doesn’t track deleted rows in any way. These rows just\ndisappear from the last full-table snapshot.\n The next issue to deal with is duplication of data. Let’s say you want to count how\nmany premium subscribers you have. If you run a query like this\nSELECT COUNT(*) FROM subscriptions WHERE status=”PREMIUM”\nyour answer would be wrong because you would be counting the number of rows in\neach snapshot. Instead, you need to always remember to limit such queries to a spe-\ncific ingestion date. This way you can get the number of premium subscribers on a\ngiven day. Anyone working with this table in the warehouse must remember that this\ntable is a stack of full source table snapshots taken on different days and then must\nadjust their queries accordingly.\nUSER_ID\nSTATUS\nJOINED_DATE INGEST_DATE\n1\nPREMIUM\n2018-03-27\n2019-05-01\n2\nTRIAL\n2019-05-01\n2019-05-01\n1\nPREMIUM\n2018-03-27\n2019-05-04\n2\nTRIAL\n2019-05-01\n2019-05-04\n3\nTRIAL\n2019-05-04\n2019-05-04\n1\nPREMIUM\n2018-03-27\n2019-05-05\n2\nPREMIUM\n2019-05-01\n2019-05-05\n3\nTRIAL\n2019-05-04\n2019-05-05\n1\nPREMIUM\n2018-03-27\n2019-05-06\n2\nPREMIUM\n2019-05-01\n2019-05-06\nA\nD\nC\nB\nFigure 4.7\nFull-table \nsnapshots appended into \na single warehouse table\n",
      "content_length": 1959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "90\nCHAPTER 4\nGetting data into the platform\n All these issues can be easily overcome by building new data sets in the processing\nlayer that provide the required representation of data. The table structure shown in\nfigure 4.7 is a good fit for data ingestion purposes. It preserves data change history\nand is easy to implement, because we always do a full-table snapshot. You can imple-\nment any number of representations of this data using the processing layer of the\ncloud data platform or by creating a view in the warehouse, depending on the data\nsize and transformation complexity. Here are some examples of derived data sets that\nyou may want to implement based on raw source data:\nLatest version of each row—This is similar to storing only the last snapshot. Some\ndata users may only be interested in how data looked at the last ingestion. You\ncan implement this as a view in the warehouse and still have all the change his-\ntory preserved.\nIdentify deleted rows by comparing the last snapshot with a previous one—You can cre-\nate a transformation that will identify that user_id=3 existed in snapshot C, but\nnot in snapshot D, and mark it as a deleted row. Such a derived data set can be\nvery useful for analytics purposes. \nCompact raw data set—You can create a derived data set that preserves the his-\ntory of data changes but eliminates complete duplicates. For example,\nuser_id=1 hasn’t changed since snapshot A, so you can store only one row for\nthis user. User_id=2 has the same data in snapshots A and B, and after upgrad-\ning to PREMIUM in snapshot C, it doesn’t change in D. This means you need\nonly store two rows for this user. \nYou can see now how data platform layer design allows you to store raw data in a con-\nvenient way and preserve as many details about data changes as you can, while imple-\nmenting multiple derived data sets using the transformation layer. All the previous\nexamples are very typical for any business use case that describes how a certain entity\nchanges over time. All of them can be implemented in SQL, and we leave it as an exer-\ncise for the reader to do that.\n While full-table ingestion is easy to implement (simply execute a full SELECT state-\nment against the required tables on a regular basis) if the source table is large, the\nprocess will become inefficient. Doing a full-table extract of a large table (tens of GBs\nor more) will put an extra load on the source database server. If the source database is\nlocated on premises then, depending on the network bandwidths you have between\nthe data center and specific cloud region, transferring large data volumes into the\ncloud will take time—sometimes significant amounts of time. Second, while the cloud\nprovides almost unlimited scalability for storage, it comes with a cost. If your source\ntable is hundreds of GBs, and you do a full snapshot daily, then in a year this table\nalone will be responsible for about 36 TB of data. To address these issues, you can\ndesign your ingestion pipeline to ingest only data that is new or that has changed\nsince the last ingestion. This process is called incremental ingestion, and we will look into\nit next. \n",
      "content_length": 3155,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "91\nIngesting data from relational databases\n4.2.3\nIncremental table ingestion \nOn a high level, the idea behind incremental ingestion is simple—instead of pulling the\nwhole table on every ingest, we will pull only new rows and rows that have changed since\nthe last ingestion. This can significantly reduce storage needs and data transfer times.\nThere are several challenges with this approach, though. First, how do we know which\nrows are new and which rows have changed at the source? Second, how do we reliably\ntrack which data we have already ingested into our data platform? Let’s unpack this. \n A common way to identify which rows have been added or updated in an RDBMS is\nto use a dedicated column in every table that contains a timestamp of when the row was\nlast changed. For new rows, this column will contain the date and time of when this row\nwas inserted, and for updated rows, it will contain the last time this column was\nmodified. Figure 4.8 shows how this would look in our example subscriptions table.\nExercise 4.1\nThere are two main issues with performing full-table ingestion. What are they?\n1\nIt’s more complex to implement than other ingestion methods.\n2\nIt’s hard to identify rows that were deleted.\n3\nIt’s impossible to see how a given row changed over time.\n4\nIt causes too much duplicate data to be stored in the platform.\nFigure 4.8\nTracking row changes with the LAST_MODIFIED column\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nTRIAL\n2019-05-01\n2019-05-01 17:01:00\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nTRIAL\n2019-05-01\n2019-05-01 17:01:00\n3\nTRIAL\n2019-05-04\n2019-05-04 09:05:39\nNew row added\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nPREMIUM\n2019-05-01\n2019-05-05 08:12:00\n3\nTRIAL\n2019-05-04\n2019-05-04 09:05:39\nRow updated\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2019-03-27 13:57:03\n2\nPREMIUM\n2019-05-01\n2019-05-05 08:12:00\nRow deleted\nA\nD\nC\nB\n",
      "content_length": 2021,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "92\nCHAPTER 4\nGetting data into the platform\nIn our example, ingestion is happening daily, so\nfor each ingestion, we must keep track of the\nmaximum value of the LAST_MODIFIED col-\numn at the time of that ingestion (figure 4.9).\n Now to implement our incremental ingestion\nprocess, we must adjust the SQL query that we run\nto extract data from the source. Instead of pulling\nall rows, it will pull rows that have LAST_\nMODIFIED timestamp in the source table greater\nthan the MAX(LAST_MODIFIED) timestamp in\nthe data platform that we have recorded during\nthe previous ingestion. So during ingestion B, the query we need to run will look like this:\nSELECT * FROM subscriptions WHERE LAST_MODIFIED > “2019-05-01 17:01:00”\nOnly one row satisfies this condition at ingestion B, and only one row will be pulled\ninto our data platform. Now you must record the MAX(LAST_MODIFIED) value\nfrom ingestion B and repeat the process the next day. This will give you a full history\nof changes for each row in your warehouse, without the unnecessary duplication that\nyou would have to deal with if you were implementing a full-table ingestion. \n This approach obviously requires a LAST_MODIFIED column to be present in each\ntable for each incremental ingestion process. Fortunately, many RDBMSs make it easy\nto automatically track change timestamps for each row by providing an ability to specify\nthe default value of the column to be current time. This way, you don’t need to make\nany changes to your application code other than to update the table definition. \nNOTE\nWe strongly recommend using RDBMS capabilities to track last-modified\ntimestamps, if they exist. It’s possible to implement a similar process on the\napplication side, but this approach is error prone. One common issue we see\nwhen using timestamps created by the application is that sometimes bugs are\nintroduced in the application where rows get the last modified timestamp that\nis earlier than the current system time. From the perspective of ingestion, it\nmakes it appear that a row has been modified in the past and therefore no longer\nneeds to be ingested. Last-modified column values must always be increasing\nand never be less than the current MAX value for this column. \nAnother best practice is to track the previous MAX value of the last-modified time-\nstamp column so we can use it as a filtering condition for our ingestion SQL query.\nThe most recent value of the timestamp column is often referred to as the highest\nwatermark in ETL literature. In our layered cloud data platform architecture shown in\nfigure 4.10, we have a dedicated component to store technical metadata about our\npipelines. This is where we recommend storing things like max values of last modified\ntimestamps for incremental ingestion.\nINGESTION MAX( LAST_MODIFIED)\nA\n2019-05-01 17:01:00\nB\nC\nD\n2019-05-04 09:05:39\n2019-05-05 08:12:00\n2019-05-05 08:12:00\nFigure 4.9\nTracking max LAST_ \nMODIFIED values\n",
      "content_length": 2933,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "93\nIngesting data from relational databases\nThe actual implementation of this metadata repository will depend on the cloud ven-\ndor you choose for your data platform. Some of the tools we mentioned in chapter 3,\nsuch as AWS Glue, have a built-in way to track such watermarks. You can always imple-\nment a basic metadata repository by using a managed RDBMS service in the cloud,\nsuch as Cloud SQL on Google Cloud or Azure SQL Database, as your metadata repos-\nitory, implemented as a simple table that tracks max ingested timestamp for each of\nthe source tables, as shown in figure 4.11.\nUsing this metadata table, your ingestion process can then dynamically construct SQL\nqueries to perform the correct incremental injections for each of the tables. Dealing\nwith a simple MySQL or PostgreSQL is easy, but as the number of pipelines grows and\nthe number of different statistics you want to track includes more than high ingestion\nwatermarks, you may want to implement an API layer on top of this metadata reposi-\ntory to provide a consistent way to work with the data in it. \nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 4.10\nUse the cloud data platform operational metadata component to store max values for \nlast-modified timestamps.\nDATABASE\nTABLE_NAME\nWATERMARK_COLUMN\nMAX_INGESTED_VALUE\nmy_service_db\nsubscriptions\nlast_modified\n2019-05-05 08:12:00\nmy_service_db\nusers\nupdated_ts\n2019-05-04 12:23:13\nsales_db\ncontracts\nlast_modified\n2019-05-01 17:02:45\nFigure 4.11\nUsing a relational table as a metadata repository to track highest watermarks for each \nsource table\n",
      "content_length": 1806,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "94\nCHAPTER 4\nGetting data into the platform\n An incremental ingestion process helps address some of the challenges with full-\ntable ingestions—you need only bring in new and changed data, and you can avoid\nlarge data set transfers. However, an incremental ingestion process still doesn’t\naddress some of the fundamental problems with a SQL-based ingestion process. The\nfirst is one we already mentioned in the context of full-table ingestion. If you only read\ndata that currently exists in a source table, you will miss deleted rows. Unless your\napplication is designed specifically to not delete rows and rather mark them as deleted\nusing a special column, then you can only infer deleted rows by comparing previous\nsnapshots and finding missing rows. This is extra development work in the processing\nlayer that you may want to avoid. \n A second issue is that if your data changes often, even the incremental load process\nwill be limited by how many state changes between ingestions it can capture. Let’s say\nyou ingest data incrementally every hour. If rows in the source table changed multiple\ntimes during this hour, you will not get all the history of changes, and you will only see\nthe state of each row at the beginning of each ingestion period. You can ingest more\nfrequently, but you may still not be able to capture every single change of each row,\nbecause in a busy system, rows can be inserted and updated thousands of times per\nsecond. A reasonable maximum frequency of a SQL-based incremental ingestion\nwould be every few minutes because each ingestion has to execute a query against the\nsource database server, introducing extra load. For a busy system, this extra ingestion\nload is usually not acceptable. To see how these challenges can be addressed, we will\nlook into one more way to ingest data from RDBMSs: change data capture.\n4.2.4\nChange data capture (CDC)\nEvery production RDBMS writes row changes into a log. While different vendors\nname them differently—redo logs, transaction logs, binary logs—change data capture\nas an ingestion mechanism involves parsing these logs using a CDC application and\nsending a stream of changes to the target storage system from the log file, as shown in\nfigure 4.12.\n A CDC application parses log files, producing a stream of events: one event for\neach row change, which is sent to a cloud data platform using fast storage as an initial\nExercise 4.2\nWhich of the following is a prerequisite to implement incremental ingestion from an\nRDBMS?\n1\nYou need to migrate your source database into the cloud.\n2\nYou need a database administrator to give your specific permissions.\n3\nYou need to use the latest version of Apache Spark in your data platform.\n4\nYou need to have a LAST_MODIFIED column in each of the source tables.\n",
      "content_length": 2776,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "95\nIngesting data from relational databases\nlanding area. CDC applications are sometimes available from RDBMS vendors—for\nexample, Oracle GoldenGate—or are implemented as third-party applications, such\nas the open source project Debezium. These third-party applications can be either\nexternal to the cloud data platform or can be run as a cloud-native service as a part of\nthe data ingestion layer, such as AWS Database Migration Service. \n Unlike SQL-based ingestion, CDC allows you to capture all changes that happen to\nindividual rows. This includes deleted rows, which makes CDC a more robust way to\ningest data from an RDBMS. It also reduces the load on the source database relative to\nusing SQL-based ingestion, and it allows you to implement real-time analytics use\ncases with an RDBMS source. The trade-offs are that you may add licensing costs for a\nCDC application, and CDC is more complex to implement because it requires a real-\ntime infrastructure. \n Why does CDC ingestion require a real-time infrastructure with fast storage and a\nstreaming ingestion path? RDBMSs usually only retain a certain amount of data in\nthese logs. The retention period can vary from a few days to a few hours, depending\non how much traffic the source database handles. This means we need to transfer the\nrow change events into our cloud data platform, where we have almost unlimited stor-\nage and can retain as much change history as we need, as soon as possible, before this\nchange event is purged from the RDBMS log. \n To understand how a CDC event stream can be represented in the cloud data plat-\nform, let’s take a look at an example. We will use the same subscriptions table example\nwe used in the previous section (figure 4.13).\n We have three row changes in this table: a new row is added, an existing row is\nmodified, and an existing row is deleted. For our SQL-based ingestion example, we\nassumed that data changed only once within a couple of days (see the LAST_MODIFIED\ncolumn), but you can imagine that for a system with lots of users and traffic, multiple\nchanges like this will happen within seconds.\n \nSource\nRDBMS\nInsert\nUpdate\nDelete\nInsert\nUpdate\nDelete\n...\nCDC\napplication\nCloud data platform\nFast storage\n2. A CDC application parses\n    database log files, producing\n    a stream of events, one for\n    each row change.  \n1. A Production\n    RDBMS writes\n    row changes\n    into a log file.\n3. This stream of changes is sent to the\n    cloud data platform using fast storage\n    as an initial landing area.\nFigure 4.12\nCDC ingestion flow\n",
      "content_length": 2551,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "96\nCHAPTER 4\nGetting data into the platform\nThere is no standard on how RDBMSs capture row changes in the logs, and different\nCDC applications parse and format the data differently. We will demonstrate a generic\nway of how changes can be captured by a CDC application and how they can be repre-\nsented in the target data platform. In general, each change is represented by a mes-\nsage that contains information about what the state of the row was before and after\nthe change, with some additional information. \n Figure 4.14 is an example of a change message for a new row. This message is a\nJSON document, which is a common way to represent change message data in differ-\nent CDC applications. The key idea behind CDC is that you not only get the current\nstate of the row, but also the previous version of this row. In our example, these are\nrepresented by before and after attributes of the message. Another important aspect\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nTRIAL\n2019-05-01\n2019-05-01 17:01:00\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nTRIAL\n2019-05-01\n2019-05-01 17:01:00\n3\nTRIAL\n2019-05-04\n2019-05-04 09:05:39\nNew row added\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2018-03-27 13:57:03\n2\nPREMIUM\n2019-05-01\n2019-05-05 08:12:00\n3\nTRIAL\n2019-05-04\n2019-05-04 09:05:39\nRow updated\nUSER_ID\nSTATUS\nJOINED_DATE\nLAST_MODIFIED\n1\nPREMIUM\n2018-03-27\n2019-03-27 13:57:03\n2\nPREMIUM\n2019-05-01\n2019-05-05 08:12:00\nRow deleted\nA\nD\nC\nB\nFigure 4.13\nSubscription table changes example\n{\nmessage: {\nbefore: null,\nafter: {\nuser_id: 3,\nstatus: \"TRIAL\",\njoined_date: \"2019-05-04\",\nlast_modified: \"2019-05-04 09:05:39\"\n},\noperation: \"INSERT\",\ntable_name: \"subscriptions\"\n}\n}\nFigure 4.14\nExample of \na CDC message for a new \nrow\n",
      "content_length": 1823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "97\nIngesting data from relational databases\nof the CDC message is that it usually contains a type of the operation: INSERT, UPDATE,\nor DELETE. In our example, the type of the operation is INSERT and thus, the before\nattribute is empty, because this is a new row and it didn’t exist before. The after attri-\nbute contains all columns for the new row and their values. Typically, CDC messages\ncontain lots of additional information, such as the name of the database server, exact\ntimestamp when a message was written to the RDBMS log, and time when it was\nextracted by the CDC application and other metadata. We have omitted most of this\nextra information in our example for brevity.\n Let’s take a look at how an UPDATE operation is represented in a CDC message (fig-\nure 4.15).\nIn this example, the user with id=2 has switched from a trial subscription to a pre-\nmium one, and once the row for this user has been updated in the source RDBMS\nsubscriptions table, our CDC application captures the state of this row before and\nafter the change. Finally, we will leave constructing a similar message for a DELETE\noperation as an exercise for the reader. \nNOTE\nINSERT, UPDATE, and DELETE operations are not the only ones that\ncan be captured by a CDC process. Many RDBMSs include information\nabout the schema changes in their logs as well. These schema changes (add-\ning new columns, deleting existing ones, etc.) will be included in a CDC\nmessage in a similar before/after pattern, where before represents a table\nschema before the change and after would represent the current status of\nthe schema. We will discuss more about schema management in the data\nplatform in chapter 6.\nNow let’s imagine that a CDC application has delivered data to our cloud data platform.\nThe next question is, what kind of processing do we need to apply to this data so we can\nstart using it in our warehouse? When it comes to representing CDC data in the ware-\nhouse, we end up with a model similar to incremental ingestion described earlier in this\nchapter. You will have a table in the warehouse that represents a full history of changes\n{\nmessage: {\nuser_id: 2,\nstatus: \"TRIAL\",\njoined_date: \"2019-05-01\",\nlast_modified: \"2019-05-01 17:01:00\"\nbefore: {\n},\nafter: {\nuser_id: 2,\nstatus: \"PREMIUM\",\njoined_date: \"2019-05-01\",\nlast_modified: \"2019-05-05 08:12:00\"\n},\noperation: \"UPDATE\",\ntable_name: \"subscriptions\"\n}\n}\nFigure 4.15\nExample \nof a CDC message for \nan UPDATE operation\n",
      "content_length": 2457,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "98\nCHAPTER 4\nGetting data into the platform\nfor each row, but without any gaps that you might see if you use SQL-based incremental\ningestion. There are a number of preprocessing steps that you might need to imple-\nment to get CDC data to this state:\n1\nDepending on the cloud platform of your choice, you might need to unpack\nthe CDC JSON document into a flat structure. Currently, only Google BigQuery\nsupports nested data types, which allows you to store the CDC messages as they\nare. If you are using AWS Redshift or Azure Cloud Warehouse, you will need to\nflatten all nested attributes into a single table.\n2\nEven if your warehouse supports nested data types, the way a CDC message is\nconstructed by a CDC application is not the most efficient and easy to use when\nit comes to warehouses. Since we are always appending new CDC messages to\nthe warehouse table, we are not really interested in the before state of the row,\nbecause we already have it in the warehouse from previous ingestions (it is still\nuseful to store the before attribute in the raw data archive in the cloud stor-\nage). We also usually don’t need any extra metadata associated with the CDC\nmessage in the warehouse. This means that you will need to filter out some of\nthe attributes before loading them into the warehouse. \n3\nSimilar to our incremental ingestion scenario, you might want to have a version\nof the table in the warehouse that only contains the latest version of each row,\nas opposed to having a full history of changes for each row. We recommend\nimplementing this as a view in the warehouse itself or a new transformation that\ncompacts multiple versions, and write them to a separate dedicated table. \n4.2.5\nCDC vendors overview\nAs we mentioned before, different RDBMS vendors implement a CDC process differ-\nently and have different CDC applications available. In this section, we provide a brief\noverview of CDC-specific options for the more popular RDBMSs: Oracle, MySQL, MS\nSQL Server, and PostgreSQL.\nORACLE\nOracle RDBMS captures changes to the rows in its redo logs. These logs are used for\nguaranteeing the reliability of data in the database itself. Oracle also provides a solution\ncalled Oracle GoldenGate that can extract changes from the redo stream and replicate\nit to the destination system. GoldenGate is able to play the role of a CDC application and\nExercise 4.3\nWhat is the main benefit of using CDC over SQL-based data ingestion?\n1\nCDC is easier to implement.\n2\nCDC is designed specifically for cloud.\n3\nCDC includes all changes to a given row.\n4\nCDC makes data warehouse table design simpler.\n",
      "content_length": 2599,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "99\nIngesting data from relational databases\ncan also stream CDC messages to various Big Data platforms via GoldenGate Big Data\nAdapters. GoldenGate is proprietary software and requires the purchase of an addi-\ntional software license.\n An alternative to using GoldenGate for Oracle CDC is Debezium, an open source\nproject that acts as a CDC application and publishes messages to Kafka. While Debe-\nzium itself is free and open source, it does require the Oracle XStream API, which\nrequires a GoldenGate license to be purchased. It should be noted that Kafka does not\nperform transactional consistency because it is essentially a message-queuing system.\n Oracle also comes with a bundled tool called LogMiner that can be used to extract\ndata from redo logs. While primarily designed for data exploration and debugging pur-\nposes, for DBAs there are a number of third-party applications that utilize LogMiner for\nCDC purposes: for example, AWS Database Migration Service uses LogMiner. LogMiner\ngenerally is not considered to be a 100% reliable way to deliver a CDC stream because\nit struggles with data consistency because transaction commits, and more importantly,\nrollbacks may occur after the capture of DML, and this must be accounted for.\n Another possibly lower cost replication product is SharePlex from Quest (https://\nwww.quest.com/products/shareplex/). This tool can be used in a way similar to Ora-\ncle GoldenGate to provide CDC operations.\nMYSQL\nMySQL is an open source RDBMS. MySQL uses what it calls a “binary log,” where it\nrecords all row changes. A binary log is used primarily for replication purposes. You\ncan configure a second MySQL server to continuously replicate changes from the pri-\nmary server via a binary log. Existing CDC tools for MySQL utilize this capability to\ncapture row change events and ship those to other systems. \n MySQL doesn’t come with any built-in tool or a CDC application, but since it is an\nopen source database, there are a number of available third-party CDC applications\nthat work with it. Debezium is one of them, but there are also MySQL CDC plugins\navailable for ETL tools, such as Apache NiFi. \nMS SQL SERVER\nMicrosoft SQL Server has had built-in CDC capabilities since the 2016 version. SQL\nServer, like all other RDBMSs, has a log, called transaction log, where all row changes\nare recorded for reliability purposes. MS SQL Server has a built-in capability that can\nextract change events from this log and populate a special “change table.” A change\ntable is a regular table in the database that contains row change history for a particu-\nlar table. CDC applications can then access this change table using a standard SQL\ninterface and extract changes.\n Since SQL Server exposes it’s change events as a regular database table that can be\naccessed with SQL, implementing a CDC application becomes a relatively simple task.\nA number of existing third-party and cloud applications support SQL Server CDC,\nsuch as AWS Database Migration Service, Debezium, and Apache NiFi.\n There is a downside to using CDC with SQL Server. If you have hundreds of tables\nfor which you want to capture change events, SQL Server will need to create and\n",
      "content_length": 3187,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "100\nCHAPTER 4\nGetting data into the platform\nmaintain a change capture table for each of them. Depending on how busy your pri-\nmary database is, it can introduce additional load on an already overloaded machine.\nYou can always choose to combine which tables require CDC ingestion (ones where\nyou need to have a full history of changes) and which can be implemented using full\nor incremental SQL-based ingestions. \nPOSTGRESQL\nPostgreSQL is another very popular open source RDBMS. Since version 9.4, Postgre-\nSQL has support for an “output plugin,” which can decode row change messages from\na PostgreSQL transaction log and publish them as either Protobuf or JSON messages.\nThis approach is similar to the MySQL approach, but PostgreSQL simplifies CDC\napplications by taking care of reading and parsing the log using the output plugin.\nDebezium and AWS Database Migration Service both support consuming the output\nof this plugin and streaming change events to the target destinations. \n4.2.6\nData type conversion\nLet’s take another look at our RDBMS ingestion diagram, which shows three levels\nwhere each level is a different software system: a relational database engine, an inges-\ntion application that reads data from an RDBMS, and a destination warehouse in a\ncloud data platform. Each of these systems stores and represents data types differently.\nWhen designing an RDBMS ingestion pipeline, you must consider how data types for\nthese three levels will map to each other (figure 4.16). \n First you should take an audit of what data types exist in your source database. Then\nyou should do an analysis of the data types supported by your destination cloud ware-\nhouse and whether there are any types that are supported in your source database that\nfield1\nfield2\nfield3\nfield1\nfield2\nfield3\nTable1\nTable2\nIngestion\napplication\nCloud data platform\nLanding\nProcessing\nWarehouse\nSQL\nSQL\nSELECT * FROM Table1\nWhen a SQL query is\nexecuted by an ingestion\napplication, native RDBMS\ndata types are converted\ninto a data type supported\nby the ingestion application\n(level 2).\nWhen the ingestion application\nsaves data into the landing area\nof the cloud data platform, data\ntypes are converted into the\ndata types that your chosen file\nformat supports (level 3).\nData is stored in the\nRDBMS using native\nRDBMS types (level 1). \nFigure 4.16\nDifferent levels of an RDBMS ingestion process\n",
      "content_length": 2378,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "101\nIngesting data from relational databases\nare not supported in your destination warehouse, or whether some types have different\ncharacteristics (such as TIMESTAMP precision) in the source and destination. \n Most likely you will find some inconsistencies. Typically, relational databases pro-\nvide a wider range of data type support that may not be available in the cloud data\nwarehouse. For example, MySQL supports five different types for integer values:\nTINYINT, SMALLINT, MEDIUMINT, INT, and BIGINT. All these types are represented by dif-\nferent numbers of bytes in the database storage and thus can store numbers up to a\ncertain size. \n If we check which integer types are supported in Google BigQuery (as an exam-\nple), we will see that there is only one—INT64—that is equivalent to BIGINT in MySQL\nin terms of how big of an integer value it can store. Such differences are caused by the\nfact that RDBMS data types need to be optimized for storage and performance. It’s\nbetter to use a small-size data type if your expected range of values fits into it. It will\nsave you disk space and improve performance of your queries. Warehouses, on the\nother end, are optimized for scanning large volumes of data and are less concerned\nwith having granular data type support. \n The important thing to keep in mind when doing your data type analysis is\nwhether there is a data type in your source system that can store a wider range of val-\nues than your destination warehouse. If that’s the case, then there is a chance of data\nloss during ingestion—some values in the source database may be too big for a corre-\nsponding data type in the warehouse. The reverse situation is not a problem. For\nexample, BigQuery doesn’t have support for the SMALLINT data type, but all MySQL\nSMALLINT values (up to 64535) will fit into the INT64 BigQuery data type (up to\n9,223,372,036,854,775,807). \nNOTE Cloud vendors are constantly releasing new versions of their services,\nincluding their cloud warehouse offerings. Check the release notes when a\nnew version is available to see if a new data type support has been added. \nIf that’s the case and you have data types without direct equivalents in the cloud\nwarehouse, then you will need to use some workarounds. Usually you can use TEXT or\nBYTES data types in the warehouse to store some sort of a representation of the source\ndatabase type and that perform required calculations in the data platform processing\nlayer. For example, a point with coordinates (12, 23) can be stored in a MySQL\ndatabase using a native geospatial type, but if your destination warehouse is AWS\nRedshift, then you can store this information as a TEXT type that looks something like\nthis: POINT(12, 23). This allows you to preserve the information, but of course you\nwill need to write custom data processing code in your data platform processing layer\nto perform any calculations on this data, such as checking if the point lies within a\nrectangular area. But where does the conversion from a MySQL native type to a text\nrepresentation happen? The answer is, in the ingestion application (level 2 of fig-\nure 4.16). \n",
      "content_length": 3129,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "102\nCHAPTER 4\nGetting data into the platform\n Ingestion application is software that sits between the source database and the des-\ntination warehouse. Depending on which programming language they are imple-\nmented in and what type RDBMS drivers they use to access the source database,\ningestion applications will have their own set of data types that will be used to map\nsource types to destination types. \n If you are using an off-the-shelf ETL tool for data ingestion, then you are limited\nby whatever data type support it offers. In this case, in addition to doing the source\nversus destination comparison of supported data types, you also need to do an analysis\nof what data types are supported by your ingestion application. \n To summarize this section, here is a sequence of steps that you need to perform to\nevaluate your RDBMS ingestion pipeline from a data type support perspective:\n1\nPrepare a list of data types that your source RDBMS supports. It may be tempt-\ning to limit your analysis to the subset of data types you currently use. We sug-\ngest performing a full analysis on all supported data types to avoid issues in the\nfuture, when your application development team decides to use a new type. \n2\nPrepare a list of data types that your cloud data warehouse supports. Identify\ndifferences between the two.\n3\nIdentify data types that have no direct equivalent in your destination warehouse,\nbut will not case data loss. For example, various sizes of integer data types will\nalways fit into a bigger integer data type size that most warehouses provide.\n4\nIdentify types that do not have equivalents and can cause data to be lost on\ningestion: for example, geospatial types, JSON data types, etc. For each type,\nidentify if a workaround is possible: for example, storing some representation\nof the unsupported data type as text and writing custom data processing code\nto perform analysis. \n5\nIf a workaround is not possible, or if your primary analytics use cases revolve\naround these unsupported data types, then you may consider choosing a differ-\nent cloud warehouse vendor that has the data types you need. For example, if\nyour primary analytics use cases are working with geospatial data, then AWS\nRedshift will not be the best fit for you. \n6\nIf you are using an off-the-shelf ETL tool or a cloud service for data ingestion,\ndo an analysis of which data types are supported there. We always recommend\ndoing several proof of concepts to find out how different data types are sup-\nported, since documentation for these tools may not always be clear.\n7\nIf you are developing an ingestion application yourself, list data types that are\nsupported by the database driver you are planning to use. A popular choice is\nusing a JDBC driver, which has its own set of data types. For JDBC drivers, check\nyour RDBMS vendor documentation to see how source database types map to\nJDBC types. \n",
      "content_length": 2897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "103\nIngesting data from relational databases\n4.2.7\nIngesting data from NoSQL databases\nRDBMSs are still the most popular database backends for most applications today. Over\nthe last several years, we have also seen the significant rise in popularity of NoSQL data-\nbases. NoSQL is a generic term for databases that, as one of their design principles, have\nchosen to sacrifice some of the RDBMS properties (transactions, durability, or others)\nin favor of being able to support a large volume of operations, scale easily by creating\nclusters of machines, or use a more flexible document-oriented data model. \n The challenge with building an ingestion pipeline for a NoSQL database is that\nthere is no single standard on how data can be extracted from them and in which for-\nmat it will be presented to the ingestion applications. The very name NoSQL implies\nthat typically SQL as a data access language is not supported, and every NoSQL data-\nbase vendor has their own set of APIs to access the data. \n We can still outline some of the most common ways to ingest data from NoSQL\ndatabases into our cloud data platform:\nUse an existing commercial or SaaS product for ingestion from a NoSQL data-\nbase. This is the path of least resistance if using such a product fits into your\ntechnology landscape and budget. Vendors who sell data ingestion tools usually\nhave a rich set of various connectors to NoSQL databases.\nImplement a dedicated ingestion application for your NoSQL vendor. You will\nneed to develop an ingestion application that uses client libraries specific to\nyour NoSQL database. This approach gives you the most flexibility because you\ncan use all the features your database has. You can also implement full or incre-\nmental ingestion using guidelines we described earlier in this chapter.\nUse a change data capture plugin, if available. Some of the popular NoSQL ven-\ndors have change data capture plugins available. For example, there is a Debe-\nzium connector for MongoDB that captures all changes done to the database\nand writes them as a stream to Kafka\nUse export tools provided by your NoSQL database. Most of the databases come\nwith tools that allow users to export data into a text format (CSV or JSON, usu-\nally) for backup or migration purposes. You can schedule these tools to run\nperiodically and then build the ingestion pipeline to only work with resulting\ntext files. This approach will simplify your ingestion pipeline, but you might be\nlimited to doing only full exports if your NoSQL database doesn’t support\nincremental data extracts. \nLet’s take a look at which ingestion options are available for some of the most popular\nNoSQL databases that are on the market today.\nMONGODB\nMongoDB is a document-oriented NoSQL database. Document-oriented means that\ninstead of using a table concept with rows and columns, MongoDB stores data in a for-\nmat that is very similar to JSON format. Each document can have nested attributes,\nwhich makes it possible to express dependencies between different entities, such as\n",
      "content_length": 3042,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "104\nCHAPTER 4\nGetting data into the platform\nusers and their orders, for example, in a single document, instead of having two rela-\ntional tables that later need to be joined together. \n MongoDB has client libraries for all popular programming languages that you can\nuse to implement your own ingestion application. There is also a data export\ntool called mongoexport (https://docs.mongodb.com/manual/reference/program/\nmongoexport/) that allows you to export data from MongoDB into CSV or JSON files.\nMongoexport supports a custom query to extract data, which means you can imple-\nment an incremental export process if you have a timestamp of when the document\nhas been last modified in your MongoDB collection. \n There is also a CDC plugin for the open source project Debezium that will capture\nall changes like adding, updating, and deleting a new document as a message stream\nand publish it to Kafka (https://debezium.io/docs/connectors/mongodb/). \nCASSANDRA\nApache Cassandra is an open source, highly scalable database that uses a hybrid\nbetween a key/value and columnar data model. Similar to key/value stores, columns\nin Cassandra do not have specific types, but they can be organized into column fami-\nlies for speeding up data access to columns that are frequently accessed together.\n Cassandra supports CQL—Cassandra Query Language—for accessing data. You\ncan use CQL commands or client libraries for various programming languages to\nimplement a dedicated ingestion application. CQL also has a COPY command that\nallows you to export a table as a CSV file. This only supports full-table exports. \n Cassandra has a built-in CDC feature that saves a log of all changes into a dedi-\ncated directory on disk. While the format of this log is open source and documented,\nthere are currently no widely used CDC applications that can read this log and publish\nchange event messages to Kafka or some other system. \n4.2.8\nCapturing important metadata for RDBMS or NoSQL ingestion pipelines\nIn a production environment, data ingestion pipelines are rarely executed just once;\nthey typically ingest data continuously to keep the data in the data platform current.\nTo make sure your data ingestion pipeline is working correctly and delivering accu-\nrate results, you will need to implement a number of data quality checks and monitor-\ning alerts to know when things don’t go as expected. We will talk more about data\nquality and monitoring in further chapters, but in this section, we will outline some of\nthe important statistics that you should capture in your ingestion pipeline so you can\nimplement quality control and monitoring later. As we described in chapter 3 and is\nshown in figure 4.17, the destination for these metrics is a metadata repository in your\ncloud data platform. \n There is some basic information that you would want to capture for every database\ningestion pipeline, including\nThe source database server name (and IP address, if possible) \nThe database name (or schema name, depending on the database vendor) \nThe source table name \n",
      "content_length": 3056,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "105\nIngesting data from relational databases\nIf you are ingesting from multiple RDBMS and NoSQL databases, it’s a good idea to\nalso store the type of the database as well. This information will help you to perform\nsome basic data lineage operations and is very helpful in debugging and troubleshoot-\ning any ingestion problems. \n When it comes to batch data ingestion from a database (RDBMS or NoSQL), one\nof the most important metrics to capture is the number of rows ingested per table for\neach ingestion. This metric is applicable for both full-table ingests and incremental\ningests, but you should distinguish between the two in your metrics-capturing process.\nThis metric is very important because it allows you to later implement two critical\nmonitoring checks:\nA check that validates that all ingested data made it way into the destination\nwarehouse or other destination system. \nA check that detects anomalies in the ingestion process. For example, a sudden\nincrease or decrease in the number of ingested rows can indicate issues with\neither the data source or the pipeline itself. \nAnother important metric for batch ingestion is the duration of each ingest. If you\ncapture the start and end timestamps for each ingestion, then you can implement a\nservice level agreement (SLA) type of monitoring. SLA monitoring allows you to know\nwhen the whole pipeline, or parts of it, takes either longer than a specified value or\nlonger than an average duration. This way you can detect issues with the pipeline (or\nsource system) as they happen. \n If you are using an existing ETL tool or service to do the data ingestion from the\ndatabase, then you need to carefully evaluate which metrics your tool captures and\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 4.17\nOperational metrics are captured in a metadata repository.\n",
      "content_length": 2060,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "106\nCHAPTER 4\nGetting data into the platform\nwhether these metrics can later be used by external monitoring tools. If you are imple-\nmenting your own pipeline metadata repository, which, as mentioned earlier in this\nchapter, can be a simple database itself, then figure 4.18 is an example of an ingestion\nstatistics table for batch ingestion.\n In this example, we are storing some basic information about the source system, the\nname of the database and table that we are ingesting from, type of operation (full or\nincremental), number of rows ingested, start/end timestamps for this particular ingest,\nas well as the duration of the ingest in seconds. Please note that we would normally rec-\nommend more descriptive column names for statistics tables, such as duration_\nseconds instead of just duration, but we have to be mindful about the page width here. \n For streaming, CDC ingestion the metrics would be slightly different. The basic\ninformation about the source server and table name will be the same, but since data\narrives continuously, we can no longer store metrics about a particular ingest batch.\nInstead, for streaming a CDC ingestion pipeline, we need to store statistics for a spe-\ncific time window. For example, we can calculate how many rows we are ingesting\nevery five minutes. The duration of the window itself will depend on your alerting and\nmonitoring needs. The faster you are expected to react to issues in the pipeline, the\nsmaller the observation window should be. \n In addition to the number of rows ingested in a CDC pipeline, it is a good idea to\nstore the number of inserts, updates, and deletes separately. CDC usually provides this\ninformation out of the box, and having these metrics will allow you to construct more\nprecise data quality and pipeline-monitoring checks. Figure 4.19 shows how a metrics\ntable in the metadata repository might look for a CDC pipeline. (Note that we have\nomitted the common fields like server and IP for brevity.)\nserver\nIP\ndb_type database\ntable\nop_type\nrows\nstart_ts\nend_ts\nduration\nprod1\n10.12.13.4 MySQL\nusers_db\nusers\nfull\n50432\n2019-05-02\n12:03:15\n2019-05-02\n12:15:01\n706\nprod1\n10.12.13.4 MySQL\nusers_db\nsubscrib\nincr\n642\n2019-05-02\n08:27:43\n2019-05-02\n08:28:00\n17\nprod2\n10.12.23.4 MongoDB marketing\ncampaign\nfull\n429\n2019-05-02\n09:48:00\n2019-05-02\n09:48:53\n53\nFigure 4.18\nExample of pipeline metrics for a database batch ingestion pipeline\nFigure 4.19\nExample of pipeline metrics for a database CDC ingestion pipeline\nop_type\nstart_ts\nend_ts\nduration\ninserts\nupdates\ndeletes\ncdc\n2019-05-02\n12:00:00\n2019-05-02\n12:01:00\n60\n10\n129\n2\ncdc\n2019-05-02\n12:01:00\n2019-05-02\n12:02:00\n60\n7\n100\n1\n",
      "content_length": 2657,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "107\nIngesting data from files\nHere, we are splitting a single row count metric into separate metrics for inserts,\nupdates, and delete operations. Also start_ts, end_ts, and duration no longer repre-\nsent how long it took to ingest these rows. Instead, they identify the boundaries and\nthe duration of the observation window we are using to calculate these metrics. \n Finally, we need to mention that another important metric for an ingestion pipeline\nis whether there were any schema changes in the given ingestion batch or an observa-\ntion window for streams. Knowing that the source schema changed allows us to build all\nkinds of alerting and automation for resolving some of the issues that a schema change\ncan cause. We will talk about schema management in great detail in chapter 6.\n4.3\nIngesting data from files\nUsing files to deliver data is probably one of the oldest ETL pipeline types. Various sys-\ntems support exporting data into text files like CSV or JSON, making this type of data\nexchange relatively simple to implement. Files are also a popular way to exchange\ndata with third parties, because they provide a nice separation between the source and\ndestination systems. You can save source data into a file and then load it into a num-\nber of different systems without having to establish direct connectivity between the\ntwo, minimizing security risks and the possibility of a negative performance impact. \n While it looks like files are the simplest way to exchange data, implementing a\nrobust file-based data ingestion pipeline is not straightforward. Text file formats such\nas CSV suffer from a lack of format enforcing—a system that produces a file may use a\ndifferent set of rules to format CSV files than the system consuming the data. File for-\nmats such as JSON have a stricter format but still lack any way to enforce schema or\ndata types. \nNOTE\nThere is a formal specification (https://tools.ietf.org/html/rfc4180)\nto which all CSV tools should adhere, but our experience indicates that\nupstream data producers often choose to implement their own way of writing\nCSV files and don’t always follow the standard. You will need to build your\ningestion pipeline in a defensive way to compensate for this possibility. \nWe will be discussing differences in various file formats in this chapter, in our cloud\ndata platform design; parsing the files into data elements is not the responsibility of\nan ingestion layer. Instead, it is done in the processing layer, and we will talk more\nabout it in chapter 5.\n There are two primary ways to deliver files into a cloud data platform. The first one\nis to use a standard File Transfer Protocol (FTP) or SFTP, a more secure version of\nFTP. FTP is a popular protocol supported by many ETL tools. It requires a dedicated\nserver to host files and to allow various clients to connect to it using username and\npassword as an authentication option.\n As shown in figure 4.20, FTP servers are often hosted on premises while an inges-\ntion application, either an existing ETL tool or cloud ETL service, connects to the\nFTP server and pulls the required files from it, depositing them into cloud storage on\nour data platform.\n",
      "content_length": 3177,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "108\nCHAPTER 4\nGetting data into the platform\nThere are several limitations to this approach. First, you will need to establish secure\nnetwork connectivity between the FTP server and your cloud data platform. Second,\nthe FTP server usually relies on local storage to store the files. This means that you\nneed to plan your FTP storage accordingly if you are planning to ingest large volumes\nof data. \n An alternative approach that we see gradually replacing the traditional FTP file\nexchange method is the use of cloud storage instead of an FTP server. This is shown in\nfigure 4.21.\nFTP server\nLocal storage\nFiles\nIngestion\napplication\nCloud data platform storage\nOn premises\nCloud\nFTP servers are often\nhosted on premises. \nAn ingestion application, either an existing\nETL tool or cloud ETL service, connects to the\nFTP server and pulls the required files, depositing\nthem into cloud storage on our data platform.\nFigure 4.20\nIngesting files from an on-premises FTP server\nCloud storage\nFiles\nIngestion\napplication\nCloud data platform storage\nCloud\nCloud\nFiles are saved to cloud\nstorage instead of FTP. \nAn ingestion application simply copies files\nfrom one cloud storage to another. Storage\nfor incoming files and the cloud data platform\nare often hosted on different clouds. \nFigure 4.21\nIngesting files from cloud storage\n",
      "content_length": 1325,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "109\nIngesting data from files\nHere, files are saved to a cloud storage, and an ingestion app just copies files from one\ncloud storage to another. The benefits of cloud storage over FTP are elastic storage;\nuse of secure file transfer mechanisms provided by cloud vendors that make network\nconfiguration simpler; security options such as temporary access keys that expire after\nsome time; and data access auditing features. \n Storage for incoming files and the cloud data platform are often hosted on differ-\nent clouds. Keep in mind that there is a cost for data transfer between cloud provid-\ners—all cloud vendors charge for copying data out of their environment. The benefits\nof cloud storage instead of FTP are that you get elastic storage and don’t need to\nworry about storage size. Also, using the secure mechanism of file transfer that cloud\nvendors provide makes network configuration much simpler. Usually this involves a\nsecure access key of some kind. Cloud storage also provides other security options\nsuch as creating a temporary access key that expires after some time; as well as data\naccess auditing features. \n4.3.1\nTracking ingested files\nUnlike ingesting data from RDBMSs, with files there are fewer concerns about\nwhether we are dealing with full or incremental ingestion. A file on FTP or a cloud\nstorage is an immutable data set—once the source system finishes writing to it, it\ndoesn’t change. This means that your ingestion pipeline should not be concerned\nwith tracking which specific items for a file have already been ingested and which have\nnot. But what needs to be tracked carefully is which files have already been ingested\ninto the cloud data platform and which have not. \n One of the methods of tracking which files have been ingested is to structure your\nfolders on FTP or cloud storage to have “incoming” and “processed” folders, as shown\nin figure 4.22. \nFigure 4.22\nUsing incoming and processed folders for tracking ingested files\n1. To track which files have been\n    ingested, create “incoming”\n    and “processed” folders on \n    FTP or cloud storage. \nStorage\nfiles\nIngestion\napplication\n3. An ingestion application reads all files from\n    the incoming folder, then saves them to the\n    landing area of the cloud data platform. \n2. The source system saves new\n    files into the incoming folder.\nsales_4\nProcessed\nsales_1\nsales_2\nsales_3\nsales_4\nIncoming\nsales_1\nsales_2\nsales_3\nsales_4\nCloud data platform storage\n4. After the file has been successfully saved to the data platform,\n    the ingestion application copies it into the processed folder on\n    the source system and deletes it from the incoming folder.\n",
      "content_length": 2658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "110\nCHAPTER 4\nGetting data into the platform\nIn this approach, you would create two folders on your source FTP or cloud storage\nsystem—one called “incoming” and one called “processed.” The source system saves\nnew files into the incoming folder. An ingestion application reads all files from the\nincoming folder, then saves them to the landing area of the cloud data platform. After\na file has been successfully saved to the data platform, the ingestion application copies\nit into the processed folder on the source system and deletes it from the incoming\nfolder. You don’t need to store files in the processed folder long term, but we found it\nuseful to keep files there for a few days (depending on your ingestion schedule),\nbecause it is helpful in debugging possible ingestion issues. You can have a scheduled\nprocess that periodically cleans up the processed-folder contents.\n While this may appear to be a complex sequence of steps, there are several major\nbenefits to this approach. First, your ingestion app doesn’t need to actually track\nwhich files have been processed and which have not. It can read all files from the\nincoming folder, knowing that only new files will exist in this folder. Second, it is very\neasy to replay the ingestion of certain files just by copying them from the processed\nfolder into the incoming folder and letting the ingestion app follow the regular\nsequence. Note that for this approach to work correctly, it’s very important to make\nsure the ingestion application properly handles any relevant exceptions. For example,\nif a copy to the landing area of the cloud data platform has failed for some reason,\nfiles should not be copied into the processed folder but rather left in the incoming\nfolder for the next retry. \n Generally, we would recommend this two-folder approach for any file-based inges-\ntion pipelines. Unfortunately, there are cases where it isn’t possible. For example, if\nthe source system already uses a certain folder organization for the files on the FTP or\ncloud storage, or if you don’t have permissions to move files from one folder to\nanother on the source system (common when ingesting files from a third party), then\na different approach is needed. \n Here is a possible different approach. Each file, either stored on FTP or a cloud\nstorage, has a set of metadata stored with it. One of the attributes of a file is a time-\nstamp of when the file was last modified. We can use this timestamp to identify only\nnew files that have been added after a certain time. The approach we use here is very\nsimilar to the incremental ingestion with RDBMSs. The idea is to track the maximum\nlast modified timestamp of all files we have seen so far and only ingest files with the\ntimestamp greater than that high watermark. The ingestion application will need to\nwork together with the data platform metadata repository to store and fetch the high\nwatermark value, as shown in figure 4.23.\n Since, unlike with an RDBMS, FTP and cloud storage don’t support a query lan-\nguage, the first step in the process is to list all available files on the source storage\ntogether with the last modified timestamp value for each file (1). Next, the ingestion\napplication needs to fetch current high watermark value from the metadata repository\n(2). It then can use this value to filter the list of files to only those where the last modified\ntimestamp is greater than the current watermark (3). Finally, the ingestion application\n",
      "content_length": 3465,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "111\nIngesting data from files\nneeds to find the maximum timestamp among the remaining files and save it back into\nthe metadata repository as the new watermark (4).\n This approach is generally more complex than the two-folders approach. It’s worth\nmentioning that many existing ETL tools can implement the process shown in figure\n4.23, so you don’t need to worry about getting all the steps right. For example Apache\nNiFi has ListS3, ListAzureBlobStorage, ListGCSBucket, and ListSFTP processors that\nhandle the high watermark and file-filtering processes. The issue with ETL tools in\nthis case is that they usually store the high watermark in some kind of internal reposi-\ntory that may not be flexible enough for all your pipeline metadata needs. You will\nalso need to make sure that the ETL tool repository is backed up on a regular basis,\nbecause losing the information about the high watermark means you can no longer\ntell which files were already ingested. Another downside of this approach is that if you\nare dealing with thousands of files, then just the operation of listing all files in the\nsource system and filtering them out can become very slow. This is especially true for\ncloud storage. \n A slight variation of the previous approaches to track ingested files is to organize\nfolders on FTP or cloud storage using names that represent the date and time at\nwhich a given file was uploaded. For example:\n/ftp/inventory_data/incoming/2019/05/28/sales_1_081232\nHere we can see that the directory structure on the FTP contains the year, month, and\nday when a file has been uploaded, and the timestamp is included in the file name. Hav-\ning such a directory structure allows you to narrow down listing all files to only listing\nfiles from a folder that represents the most recent date. For example, if your ingestion\nsales_1\n2019-05-28 08:12:31\nsales_2\n2019-05-28 08:13:31\nsales_3\n2019-05-29 14:23:07\nsales_4\n2019-05-29 17:13:43\nMetadata\nrepository\ncurrent high\nwatermark =\n2019-05-28\n08:13:31\nIngestion application\nSource system\n3. Filter files where last\n    modified timestamp >\n    2019-05-28 08:13:31\nsales_3 2019-05-29 14:23:07\nsales_4 2019-05-29 17:13:43\n1. List all available files on the\n    source storage together with\n    last modified timestamp value\n    for each file.\n2. Fetch current high\n    watermark from the\n    metadata repository.\n4. Find the maximum\n    timestamp among\n    the remaining files\n    and save it back\n    into the metadata\n    repository as the\n    new watermark.\nFigure 4.23\nTracking the last modified timestamp of files\n",
      "content_length": 2565,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "112\nCHAPTER 4\nGetting data into the platform\napplication runs on 2019-05-28, then it can only list files from a corresponding directory.\nThis can help improve the performance when the number of files on the source systems\ngrows significantly. \n It’s worth noting that using a timestamp tracking approach makes replaying the\ningestion of a certain file or files more complicated. You will need to adjust the water-\nmark value in the metadata repository to force the process to pick up specific files\nagain, and it may not be possible to reprocess a single file if multiple files have the\nsame timestamp value.\nNOTE\nIf you are dealing with cloud storage as a source system for your files, it’s\nworth checking which file copy tools are available from your specific cloud\nvendor. For example, you can use the gsutil rsync tool on Google Cloud to\nsynchronize files between two Google Cloud Storage destinations, or between\nGoogle Cloud Storage and a local file system, or even between Google Cloud\nStorage and S3. This tool can keep track of which files were added to the\ndestination and only copy new files for you. Similar tools exist for other cloud\nvendors, such as blobxfer (https://github.com/Azure/blobxfer) on Azure\nand s3 sync on AWS. Using these tools is convenient if you are doing a large\ninitial transfer of data into your cloud data platform. \nKeep in mind that these tools don’t really store any information about which\nfiles have been ingested and which not, but rather compare the list and the\nchecksums of files on source and destination to identify new or changed files.\nThis means that you can’t easily reset the high watermark to replay ingestion of\ncertain files, for example. \n4.3.2\nCapturing file ingestion metadata\nAs with an RDBMS ingestion process, it is important to capture statistics and other\nmetadata about the file-based ingestion pipeline. Unlike RDBMSs, though, we will not\nbe capturing the number of rows in each file during the ingestion process. Row count\nis very important for files as well as databases, but instead of doing it during ingestion,\nwe need to do it during processing. \n The main reason for doing it during processing is scalability. When reading data\nfrom RDBMSs, you usually can get the number of rows fetched from a table “for free,”\nmeaning this information is available to you via a client library, or you can send a SQL\nExercise 4.4\nWhen would you choose tracking ingested files in the Metadata layer instead of using\nan “incoming” and “processed” folder structure?\n1\nWhen a source system already has a predefined folder structure\n2\nWhen you are dealing with lots of small files\n3\nWhen you are dealing with lots of large files\n4\nWhen you are concerned about pipeline performance\n",
      "content_length": 2733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "113\nIngesting data from files\nquery to get this number from the database itself. The client (your ingestion applica-\ntion) doesn’t need to do any extra processing to get the row count. \n With files, the situation is different. If you need to count the number of individual\nrows in files, you first of all need to know exactly which file format you are dealing with;\nthen implement a specific parsing functionality in your ingestion application; and,\nfinally, split the files into rows and count the rows in your ingestion application code.\nThis approach won’t scale for large files, because ingestion applications are usually not\ndistributed in nature, meaning they run on a single virtual machine and cannot split\nthe large files into smaller chunks and process each chunk on a separate VM. We have\nseen ETL tools choke trying to parse a CSV file that was tens of GBs in size. \n So we will leave capturing row counts of files to the processing layer, using a distrib-\nuted data processing engine to easily scale to any file size. We will talk more about this\nin chapter 5. \n Some of the statistics and metadata we recommend capturing for the file ingestion\npipeline are similar to what we discussed previously for RDBMSs. It’s important to\nhave some kind of name for the source system that will help you to identify where the\nfile came from. For example, you can give your FTP servers or source cloud storage\nsome descriptive names such as “inventory_ftp” or “demographics_s3.” Knowing and\nstoring the type of the files for this particular source is also useful, since that can later\nbe used by the processing layer to know which parsing library to use to read data from\nthis source. The size of the file and the duration of the ingestion are also useful met-\nrics that you can utilize later to detect any anomalies in file size (files that are too big\nor too small) and/or ingestion time SLAs. \n Finally, as we mentioned before, sometimes there is useful information encoded in\nthe file name itself. It could be the specific time the file was generated, the name of\nthe source system that produced the file, and the like. We have also seen some other\ninformation, such as the name of a state or province, included in the file name, indi-\ncating that we should only inspect data for this geography in the file. Preserving the\nfull file name in the metadata repository can help when debugging data issues. We\nactually recommend storing the full path to the file on the source system, since some-\ntimes directory structure contains useful information as well. \n Figure 4.24 summarizes common metadata that you should consider for a file\ningestion pipeline.\nsource_name\nfile_type\nstart_ts\nend_ts\nduration\nfile_name\nfull_path\nfile_size\ninventory_ftp\nCSV\n2019-05-02\n07:00:00\n2019-05-02\n07:12:00\n720\ninventory_CA.csv\ndata/incoming/inventory/2019/05/01/\n268435456\ndemographics_s3\nJSON\n2019-05-05\n12:00:00\n2019-05-05\n12:02:00\n120\ndem_full.json\ns3://share/demographics/latest\n524288000\nFigure 4.24\nExample metadata for a file ingestion pipeline\n",
      "content_length": 3037,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "114\nCHAPTER 4\nGetting data into the platform\n4.4\nIngesting data from streams\nStreams are becoming an increasingly popular way to exchange data between multiple\nsoftware systems, with Apache Kafka becoming the de facto standard for a message\ndelivery system. This means that as data platform designers we need to account for\nKafka or a similar system as one of the possible data sources. This section will focus\nprimarily on Kafka, but there other message delivery systems that you may see as a\ndata source for your platform. All major cloud vendors have their own services with\nsimilar properties: Google Cloud Pub/Sub, Azure Event Hubs, and AWS Kinesis. The\nchallenges and solutions that we will describe in this section will be generally applica-\nble to all such systems. \n When it comes to ingesting data from a stream, we usually see two main scenarios.\nThe first one is streaming, or real-time, ingestion. In this scenario, what is important\nfrom the end user’s perspective is that data be ingested into the cloud data platform,\nincluding into the warehouse, as fast as possible, but this data is then used for analysis\nin an ad hoc and not necessarily real-time manner. \n The second scenario is when, in addition to ingesting data in real time, you also\nneed to perform some non-trivial computations and analytics on the data as it comes\nin. This is called real-time analytics, as opposed to real-time ingestion. This chapter\nwill focus on the real-time ingestion scenario, and real-time analytics will be covered in\ngreat detail in chapter 6. \n Ingesting data from a streaming system is quite different from dealing with batch\ningestion from an RDBMS or a file. The main difference comes from the fact that\ninstead of receiving multiple data elements together (rows from RDBMS, JSON docu-\nments from a file, etc.), we receive messages one by one from a message bus as they\nare written by an upstream application. One exception is a CDC ingestion pipeline,\nwhich in reality is a streaming ingestion pipeline.\n If we are receiving messages one by one, we can no longer just save them as files to\nour cloud storage landing area—this would mean creating a file for each message!\nStreaming systems usually receive messages at a very high rate, so saving them directly\nto cloud storage wouldn’t work from a performance perspective. This is where the fast\nstorage from our data platform architecture comes into play.\n Let’s imagine that your application development team is already using Kafka as a\nmessage exchange platform for their microservices. This Kafka cluster is either\ndeployed on premises or in the cloud (not necessarily on the same cloud as your data\nplatform). Figure 4.25 shows how your data platform could be architected to do\nstreaming ingestion using Kafka.\n Applications that write data to Kafka are typically called producers (1). We need an\ningestion application that is capable of reading a stream of messages from Kafka and\npublishing it to our data platform fast storage (2). This step of a stream ingestion pro-\ncess can be implemented in a few different ways. Kafka itself comes with a component\ncalled Kafka Connect (https://kafka.apache.org/documentation/#connect) that\nallows you to read data from different sources and publish to Kafka (source) and also\n",
      "content_length": 3284,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "115\nIngesting data from streams\nread from Kafka and publish to a destination (sink). You can use Kafka Connect with\nGoogle Cloud Pub/Sub, AWS Kinesis, or Azure Event Hubs as sinks and in doing so\nestablish replication of messages from your source Kafka to your cloud data platform\nmessage bus (3). This option is the easiest to implement, but it does require changes\nto the Kafka configuration itself. In some cases, you may not be able to do it because\npeople responsible for the existing Kafka cluster might be busy with other work, there\nmight be hardware limitations in your current data center, etc.\n An alternative to this approach is to create a dedicated application that will read\nmessages from Kafka and publish them to your cloud data platform. This is relatively\neasy to implement because Kafka has libraries for most of the popular programming\nlanguages. We recommend finding an out-of-the-box solution to replicate data from\nKafka into the cloud data platform fast storage because, while it easy to implement a\nsimple consumer application for Kafka, you will still need to deal with error handling,\nproper logging strategies, and scaling your application to multiple machines if you are\ndealing with high-volume data. Some of the existing ETL tools support Kafka as a\nsource, so it’s worth checking what is available to you. On Google Cloud Platform you\ncan also use Cloud Dataflow to implement this step in the ingestion process. With\nCloud Dataflow, you can balance having full control of the implementation details,\nwhile getting out-of-the-box scalability, logging, and error handling. \n Once we have data in the fast storage of our cloud data platform, we need to deliver\nit into our cloud data warehouse as well as into our slow storage for archiving and other\nFigure 4.25\nIngesting streaming data into the data platform\nIngestion\napplication\nCloud data\nwarehouse\nFast storage\nSlow storage\nGCP \nPubSub\nAWS \nKinesis\nAzure \nEvent Hubs\nIngestion\napplication\nGoogle\nCloud Storage\nAWS S3\nAzure Blob\nStorage\n2. An ingestion application \n    reads a stream of messages \n    from Kafka and publishes it \n    to data platform fast storage.\n4. Read messages from the fast \n    store and write them to the\n    cloud warehouse and cloud \n    storage using cloud-native \n    services like Azure Stream\n    Analytics, Google Dataflow, \n    or AWS Kinesis Data Firehose.\n1. Producers are applications\n    that write data to Kafka.\n3. Kafka Connect allows you to read data from different sources and publish\n    to Kafka (source) and also read from Kafka and publish to a destination\n    (sink). Use Kafka Connect with Cloud Pub/Sub, AWS Kinesis, or Azure Event\n    Hubs as sinks to replicate messages from your source Kafka to your cloud data\n    platform message bus. \n",
      "content_length": 2775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "116\nCHAPTER 4\nGetting data into the platform\npurposes (4). For this we need another application that will read messages from fast\nstore and write them to the cloud warehouse and cloud storage. Fortunately, this can be\nmostly handled by cloud-native services. On Azure, for example, you can use Azure\nStream Analytics, which can read messages from Azure Event Hubs and write them into\nAzure SQL Warehouse. On Google Cloud Platform, you can use Cloud Dataflow to\nread messages from Cloud Pub/Sub and write them into BigQuery. On AWS, similar\nfunctionality can be implemented with Kinesis Data Firehose. Kinesis Data Firehose\nwill read messages from AWS Kinesis and write them into a Redshift warehouse. While\nit is possible to implement similar applications yourself using existing libraries, we\nstrongly recommend using one of these native services—they provide better integra-\ntion with corresponding message bus systems and various cloud destinations. \nNOTE\nCurrently only Google BigQuery supports actual real-time ingestions\nwhen it comes to cloud warehouses. While both AWS Redshift and Azure SQL\nWarehouse support inserting rows one by one, they still need to batch multi-\nple inserts together to get reasonable performance. This batching is usually\nconfigurable and is easy to tweak using one of the services previously men-\ntioned. We will discuss more about the differences between different ware-\nhouses in chapter 9.\nSome cloud services (Azure Stream Analytics, Google Cloud Dataflow, and AWS Kine-\nsis Data Firehose) can be used to save messages to the regular cloud storage. We need\nto keep in mind that cloud storage is not optimized for dealing with lots of small files,\nso if we were to write each message as a separate file, the performance of our pipeline\nwould be really bad. Instead, a common approach is to batch messages together and\nwrite them to storage as a single large file. We recommend keeping the size of the files\nat several hundreds of MBs or more. This may not always be possible if your ingestion\nstream is low volume. You will need to find a balance between the resulting size of the\nfile and the time it takes to accumulate that volume of messages. \nNOTE\nAs mentioned before, AWS and Azure Cloud Warehouse have to batch\nmessages before inserting them into tables. The batch size doesn’t have to be\nthe same as the batch size for writing files to the cloud storage. Usually you\nwould want to keep the batch size for the warehouse as small as performance\npermits to get the latest data available to the users in near real time. Batch\nsize for the cloud storage would be bigger to get larger resulting files. \nIn some cases, you will not have an existing Kafka service to read messages from in\nreal time. For example, imagine your development team is implementing a brand-new\napplication, and they want to be able to push messages into a cloud data platform for\nfurther analysis and archival. In this case, we can make the fast storage from our cloud\ndata platform available to this application. \n The process is very similar to what we have described previously, except we don’t\nneed an ETL tool or a dedicated ingestion application that will simply transport mes-\nsages from Kafka into our cloud data platform. Figure 4.26 shows the steps involved.\n",
      "content_length": 3282,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "117\nIngesting data from streams\nInstead of an ETL tool or a dedicated ingestion application that transports messages\nfrom Kafka into our cloud data platform, we will expose our fast storage to the appli-\ncations that need to publish messages (1). These applications will need to implement\ncustom code that will write the messages to the fast storage using a client library that is\nprovided by the cloud vendor. \n Once data is published into the fast storage, the process is the same—use one of\nthe cloud services to write data into the cloud warehouse and cloud storage (2). \n4.4.1\nDifferences between batch and streaming ingestion\nThere are certain things that are simpler in the streaming ingestion pipelines than in\nbatch ones and vice versa. We would like to outline some of the key differences in this\nsection. \n One thing that you don’t need to worry about in the streaming ingestion is tracking\nwhich data you have already ingested and processed and which data is new. If you recall\nour discussion in the previous sections, such tracking is critical for RDBMSs and file\ningestion. With streaming pipelines this is less of a concern. Tracking which messages\nhave been consumed and which have not is a common feature for a message bus system\nsuch as Kafka or any of the similar streaming cloud services. Our ingestion application\nwill keep reading messages as they come in, and it will periodically checkpoint the offset\nthat it has last processed back to Kafka itself. The offset is a sequence number for each\nmessage. This allows our ingestion application to easily recover from crashes or\nplanned outages—it will read the latest processed offset from Kafka and will continue\nreading messages from this offset forward. Similar capabilities exist for cloud services\nlike Google Cloud Pub/Sub, Azure Event Hubs, and AWS Kinesis. \n This offset tracking approach has an unfortunate side effect. If our ingestion\napplication needs to write the latest processed offset back to Kafka for every processed\nmessage, we will significantly impact the performance of our pipeline, because we will\nnow need to perform one write operation for every read operation. In real applications,\nFigure 4.26\nIngesting streaming data directly into the cloud data platform\nCloud data\nwarehouse\nFast storage\nSlow storage\nIngestion\napplication\n2. Use GCP PubSub,\n    AWS Kinesis or\n    Azure Event Hubs\n    to write data into\n    the cloud warehouse\n    and cloud storage.\n1. To ingest streaming data\n   directly into the cloud\n   data platform, expose fast\n   storage to the applications\n   that will write messages to\n   fast storage.\nGCP \nPub/Sub\nAWS \nKinesis\nAzure \nEvent Hubs\nGoogle\nCloud Storage\nAWS S3\nAzure Blob\nStorage\n",
      "content_length": 2707,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "118\nCHAPTER 4\nGetting data into the platform\noffsets are usually saved periodically in configurable time intervals. This means that if\nyour ingestion application crashes, or if there is a problem with Kafka itself, then when\nthe ingestion application is back online, it will read some of the messages it has already\nprocessed. This situation happens in production systems more often than you\nmay think. \nNOTE\nFor more information about Kafka, including implementation details\nand various strategies to implement applications using Kafka, we recommend\nhttps://www.manning.com/books/kafka-in-action.\nAs such, your streaming ingestion pipeline needs to be able to deal with duplicate\nmessages. Some streaming data processing systems, such as Google Cloud Dataflow,\nhave built-in functionality that allows you to deduplicate incoming data. In other\ncases, you will need to implement a specific step in the processing layer of your cloud\ndata platform to deduplicate incoming data. Usually this is achieved by having a\nunique identifier included with every message. We will talk more about deduplicating\ndata in chapter 5. One thing to keep in mind is that to implement an efficient dedu-\nplication strategy, each message should have a unique identifier of some sort. \n Another important consideration for building a streaming pipeline is the poten-\ntially large volume of incoming messages. If you are using an off-the-shelf ETL tool or\na custom application to implement step 1 of the streaming ingestion pipeline, which\nis moving messages from Kafka into the data platform fast storage, then you need to\nevaluate how scalable this approach is. If your solution is only capable of processing\nincoming streaming messages on a single virtual machine, then it’s very likely that you\nwill hit scalability limits. To make sure the first step in the pipeline doesn’t become a\nbottleneck, you need to use one of the distributed message processing systems, such as\nKafka Connect, Spark Streaming, or Apache Beam. \n Unlike batch sources, such as files or databases, where data is usually stored for the\nlong term in message bus systems, messages are usually purged on a regular basis. Rea-\nsons for this are usually related to the volume of incoming messages; a reliable mes-\nsage bus needs to replicate messages to multiple machines, so we are potentially\nlooking at lots of storage and compute resources to store these messages over a long\nperiod of time. It’s not uncommon for messages in Kafka to have an expiration period\nof about a week or less. This is, of course, configurable and depends on the volume of\nmessages and the size of the cluster. In cloud systems, such as Cloud Pub/Sub, for\nexample, messages can be purged automatically once a consumer application\nacknowledges receiving it. \n These expiration policies make reprocessing streaming data a more complicated\ntask than reprocessing data from batch sources. To implement a robust reprocessing\npipeline, we will need to utilize archive data that we have saved to cloud storage as one\nof the steps of the ingestion pipeline. But since our downstream transformations\nexpect a stream and not a file, we will need to implement a step that reads a file,\nbreaks it down into individual messages, and pushes those messages through the\n",
      "content_length": 3282,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "119\nIngesting data from streams\nstreaming pipeline again. This way new incoming messages and reprocessed messages\nwill be treated the same. \n4.4.2\nCapturing streaming pipeline metadata\nAs you may have realized by now, streaming data has two representations in our cloud\ndata platform design. The first one is a real-time stream that is handled by our plat-\nform message one at a time and is used for real-time ingestion into the warehouse,\nreal-time analytics, or both. When it comes to capturing pipeline metadata for stream-\ning, these two representations need to be treated differently. \n Important metadata for streaming pipelines is similar to what we have discussed in\nthe CDC section earlier in this chapter. To be able to quickly assess the health of the\npipeline, we need to measure how many messages we processed over a fixed period of\ntime. This time window will depend on your particular scenario and could be anything\nfrom a few minutes to hours. Consider your overall message volume to decide which\ntime window for metrics collection works best. If you only receive a few messages every\nminute, then a larger metrics window, such as an hour, will give you a better baseline\nthat you can implement your monitoring from. For higher-volume streams, you will\nneed to use a smaller window to make sure you don’t miss any spikes or drops that may\nindicate a pipeline issue or an upstream application problem. In fact, we have seen\nexamples in real cloud data platforms where a monitoring check for the volume of mes-\nsages in a streaming pipeline detected an outage of the upstream application, before\nthe application itself detected it—the volume of messages that the pipeline observed\ndropped lower than a baseline, and an alert was triggered immediately. \n In figure 4.27 we have two streaming sources: one is a clickstream from our web\napplication and another some sort of IoT sensor. As you can see, different sources can\nuse different time windows for capturing message throughput. \nAs we have mentioned previously, a streaming pipeline will also flush batches of mes-\nsages to the regular cloud storage periodically. The time window for this operation will\ntypically be bigger than the one used for capturing statistics for the streaming pipe-\nline—we need to choose it in such a way that we don’t end up with lots of small files. \n In figure 4.28 we are writing batches of messages into Google Cloud Storage (GCS)\nand assume that messages arrive in JSON format. We use a larger time window for a\nstream with less volume and a smaller window for a higher throughput stream. We also\nassume message size is big enough to result in files that are at least a hundred MBs. \nsource_name\nstart_ts\nend_ts\nduration\nmessages\nweb_events\n2019-05-02 12:00:00\n2019-05-02 12:05:00\n300\n1038\nsensor1\n2019-05-02 12:00:00\n2019-05-02 12:01:00\n60\n7800\nFigure 4.27\nMetadata for a streaming pipeline\n",
      "content_length": 2892,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "120\nCHAPTER 4\nGetting data into the platform\nNote that we are saving the files into a dedicated archive bucket on Google Cloud\nStorage and using a date-partitioned folder structure. We will talk more about data\norganization on cloud storage in chapter 5.\n4.5\nIngesting data from SaaS applications\nSaaS applications are an increasingly popular data source for all types of analytics\napplications. It’s hard to find a business today that doesn’t use a SaaS CRM or a mar-\nketing management solution. Extracting data from SaaS sources, though, comes with\nits own set of unique challenges. \n Developers of SaaS applications will never give you direct access to the underlying\ndatabases, for security and scalability reasons. This leaves us with two common ways to\nextract data: APIs and file exports. File exports are less and less common these days,\nbecause APIs provide more flexibility for consumers and more control for the SaaS\napplication owners.\n Most of the SaaS applications today provide a REST API over HTTP. This means that\nyou as an API consumer can make a simple call to an HTTP or HTTPS URL, provide\nsome parameters that this API requires, and get a response back from the SaaS appli-\ncation. Responses are usually wrapped in JSON format, but XML is also used sometimes.\n Figure 4.29 shows the steps involved in ingesting data via a REST API over HTTP.\nAs with all other ingestion scenarios described in this chapter, we need to have some\nkind of ingestion application that will be responsible for interacting with the SaaS\napplication and saving results back to our cloud data platform. First of all, this inges-\ntion application needs to authenticate somehow with the SaaS side to prove that it is\nallowed to access the data behind the API(1). Different SaaS providers use different\nauthentication methods: username/password combinations or authentication tokens,\nand today using the OAuth protocol is an increasingly popular authentication mecha-\nnism for web applications (https://oauth.net). You will need to deal with whichever\nmethod the SaaS provider uses.\n Once the ingestion application has proven that it has permission to access the data,\nit can start making calls to the API to fetch data (2). These calls are HTTP calls to spe-\ncific URLs on the SaaS side, sometimes with parameters that specify which specific\nobjects to fetch, etc. In figure 4.29, we are fetching full customer data from some hypo-\nthetical SaaS service. The design of the URLs (or API endpoints) is specific to each\nsource_name\npath\nstart_ts\nend_ts\nduration\nmessages\nweb_events\ngs://archive/web_events/2019/06/08/webe\nvents_01.json\n2019-05-02\n12:00:00\n2019-05-02\n12:15:00\n900\n35231\nsensor1\ngs://archive/sensors/sensor1/2019/06/08/s\nensor1_05.json\n2019-05-02\n12:00:00\n2019-05-02\n12:05:00\n300\n32265\nFigure 4.28\nMetadata for streaming pipeline archive on cloud storage\n",
      "content_length": 2860,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "121\nIngesting data from SaaS applications\nSaaS provider, so you will need to study the API documentation to understand which\nendpoints exist, what data they expose, and which parameters they accept. \n Finally, when the ingestion application receives a response back from the SaaS ser-\nvice, it needs to save the data into the cloud storage for further processing (3). Typi-\ncally, a web API would return data as a JSON document or a collection of documents. \n It may look like building an ingestion pipeline for a SaaS application is simple.\nAfter all, we just need to make a series of HTTP calls and save resulting documents to\ncloud storage. All programming languages have libraries for making parameterized\nHTTP calls, and many of the cloud or third-party ETL tools have components to\naccomplish the same task. In reality, there are many challenges with implementing a\nrobust SaaS ingestion pipeline. \n4.5.1\nNo standard approach to API design\nThere currently are no standards on how SaaS APIs are designed. Each provider\ncomes up with their own set of endpoints and required parameters. For example, in\none SaaS application you may be able to fetch a full list of existing customers, together\nwith all the details on each customer. Another provider may decide to not allow full\ndata exports (to prevent load on their systems) and will give you two endpoints\ninstead: one for fetching only customer unique identifiers and another for fetching\ndetailed information for a given customer identifier. This makes an ingestion pipeline\nfor a specific SaaS application tailored to the design SaaS developers decided to go\nwith. If you need to deal with several SaaS sources, then you will need to build an\ningestion pipeline that can understand dozens of API endpoints. \nIngestion\napplication\nSaaS\napplication\nCloud data platform\nSlow storage\nThe ingestion application is responsible for interacting with the SaaS\napplication and saving results back to the cloud data platform.\n2. Once the ingestion application has proven that it has\n    permission to access the data, it can start making calls to the\n    API to fetch data using HTTP calls to specific URLs on the \n    SaaS side—GET https://some-saas-app/data/customers/all.\n1. The ingestion application authenticates\n    with the SaaS application to prove that\n    it is allowed to access the data behind\n    the API.\n3. When the ingestion application\n    receives a response back from\n    the SaaS service, it will save the\n    data into the cloud storage for\n    further processing.\nFigure 4.29\nExample of an API request to a SaaS application\n",
      "content_length": 2593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "122\nCHAPTER 4\nGetting data into the platform\n4.5.2\nNo standard way to deal with full vs. incremental data exports\nUnlike ingesting data from databases, there is no common way to implement full or\nincremental exports from a SaaS source. Again, you are at the mercy of the SaaS API\ndevelopers. Some SaaS systems we have worked with provide no incremental ingestion\ncapabilities, because there is no API to tell which objects inside SaaS have changed.\nOthers may only provide an incremental data export API. Mature platforms, such as\nSalesforce, would typically give you two options. One is an API endpoint to fetch a list\nof all unique identifiers for a given object and then a second API to fetch details about\na particular object using an identifier as an input parameter. Another option is a dedi-\ncated API to fetch identifiers only for new and updated objects using start and stop\ntimestamps as input parameters. You then call a “fetch details” endpoint with these ids\nand implement an incremental ingestion pipeline. \n4.5.3\nResulting data is typically highly nested JSON\nJSON is a prevalent data exchange format for web applications, so it’s not surprising\nthat most of the SaaS APIs return JSON data. When it comes to performing analytics\ntasks and specifically loading SaaS data into a cloud data warehouse, there are several\nthings to consider. First of all, not all cloud warehouses support storing and querying\nnested data. Currently, only Google BigQuery and Azure Cloud Warehouse have sup-\nport for JSON-like data. If you are planning to use Redshift, then you will need to\nimplement an additional transformation process, which will unnest JSON documents\ninto separate tables. Implementing such a process may be required even if your ware-\nhouse does support nested data structures. From our experience, many analysts find it\nhard to navigate a complex JSON document with several levels of nesting and rather\nprefer these documents to be broken out into multiple “flat tables” that are easier for\na person to reason about. What’s easy for a computer program to generate is not\nalways easy for a human to work with. \n Given the challenges, our recommendation when implementing an ingestion pro-\ncess for a SaaS application is to carefully evaluate the amount of development (and\nfuture maintenance) you will need to incur. If you are only dealing with one SaaS\napplication, which is rare these days, and the API of that application is relatively\nmature, then you may decide to implement your own ingestion application. \n If, on the other hand, you are dealing with multiple SaaS sources and envision the\nnumber of these sources to increase over time, then we would recommend looking at\noff-the-shelf products and services that can help you with integrating SaaS data into\nyour cloud data platform. There are a number of SaaS services that offer extracting\ndata from other applications and saving them to the destination of your choice. One\nof the good examples of such a service is Fivetran (https://fivetran.com/), which sup-\nports most of the popular SaaS applications on the market and can save extracted data\ninto the cloud platform of your choice. For some SaaS services, it also can unpack\nnested JSON structures into a set of relational tables that can be easily queried by data\nanalysts and BI tools. \n",
      "content_length": 3320,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "123\nNetwork and security considerations for data ingestion into the cloud\n When it comes to capturing metadata and statistics for the SaaS data ingestion\npipelines, similar rules as for RDBMS and files apply. Today, most SaaS applications\nonly provide access to data in batch mode. This means that you would want to capture\nat least the following metadata items:\nName of the SaaS source\nName of the specific object in this source (customers, contracts, etc.)\nIngestion start and end times\nNumber of objects fetched\n4.6\nNetwork and security considerations for data ingestion \ninto the cloud\nWhen implementing data ingestion pipelines for your cloud data platform, one of the\nthings you will need to plan for is how your data platform will actually connect to the\ndata source. You may have sources that live on the on-premises network, or you may\nhave sources deployed into a different cloud. While going into the details of the cloud\nnetworking is beyond the scope of this book, we will outline several of the most com-\nmon scenarios that we have seen in real platforms. \n4.6.1\nConnecting other networks to your cloud data platform\nA very common scenario is to have sources that are deployed either on premises or in\nanother cloud. “Another cloud” here could mean a different cloud provider than the\none you are using for your data platform, or a different project with the same cloud\nprovider. For your ingestion pipelines to work, there must be some way for your inges-\ntion application to connect to these data sources. It’s very unlikely that you would\nhave a database that has direct access to the internet. This means that some kind of\nsecure connection between cloud and on-premises resources must be established. All\nthree major cloud providers offer a way for you to deploy your cloud resources into\nwhat’s commonly referred to as a virtual private cloud or VPC. Azure calls it a virtual\nnetwork, but we will use VPC terminology for simplicity. \n What a VPC allows you to do is to restrict access to your cloud resources using a vir-\ntual network construct. If you deploy components of your cloud data platform into a\nVPC, then only resources inside that VPC can communicate with each other. You can\nbreak down your VPC into different subnets to even further limit which resources in\nyour data platform can communicate with each other. For example, it’s a good prac-\ntice to isolate your ingestion layer into its own subnet, because the ingestion layer\nalways needs to deal with external resources. This is an oversimplification, of course,\nbecause there is more to cloud networking configuration than we can describe in this\nbook. For the purpose of this section, let’s assume that your cloud data platform is\ndeployed into its own VPC and has a dedicated subnet for ingestion components, as\nshown in figure 4.30. \n \n \n",
      "content_length": 2832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "124\nCHAPTER 4\nGetting data into the platform\nIn cases when data sources and the data platform itself are deployed on two different\nnetworks, one of the ways to establish connectivity between the two is to use a VPN\ngateway. All cloud vendors have some version of a VPN gateway service that can be\ndeployed into your VPC and connected to a gateway on premises or on the cloud.\nUsing a VPN connection makes it safe to transfer data over the internet, since all data\nis encrypted in-flight. \nNOTE\nIf you are planning to transfer large volumes (hundreds of GBs per\nday) of data from an on-premises location to the cloud, then it’s worth\nexploring if a direct connection can be used. All cloud providers offer a ser-\nvice that allows using a dedicated connection between certain connections\nand their cloud: AWS Direct Connect, Azure ExpressRoute, and Google\nCloud Interconnect. \nIf you are using some of the cloud providers’ PaaS solutions, such as Azure Event\nHubs or Google Cloud Storage as data sources, then you need to understand how\nthose sources are deployed. Often, these services are deployed as global services,\nmeaning they don’t belong to your VPC. In this case, you will need to rely on cloud\nprovider authentication and encryption capabilities to establish a secure connection\nbetween the sources and your data platform. Today, we see a toward for cloud vendors\nmaking these services deployable into a customer VPC to provide additional control\nand security. For example, Azure Blob Storage access can be limited to a certain\nvirtual network. If you need to use such a service as a data source, then the same\npattern we discussed is applicable—you need to establish VPN connectivity between\nthe two networks. \n If you are dealing with SaaS sources, then connectivity will be established over the\ninternet. Today all major SaaS providers provide APIs over HTTPS, meaning that com-\nmunication between the SaaS application and your ingestion application will be\nencrypted. Since SaaS applications are globally available, there is no need to establish\na dedicated network connectivity. \nIngestion\nsubnet\nRest of the\nplatform\nresources\nCloud data platform VPC\nVPN\nGateway\nVPN\nGateway\nRDBMS\nFTP\nOn-premises network\nInternet\nFigure 4.30\nConnecting a cloud data platform to the on-premises network\n",
      "content_length": 2300,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "125\nNetwork and security considerations for data ingestion into the cloud\nSummary\nIngesting data is more complex than many expect as the modern data platform\nmust support ingestion from a variety of different data sources (typically\nRDBMSs, files, SaaS via API, and streams), often at high velocities and in a con-\nsistent, uniform way across different source data types.\nWhen ingesting from an RDBMS, key considerations include the need to map\ncolumn data types from the source database into the cloud data warehouse–\nsupported data types, the need to automate your ingestion process to reduce\nmanual effort and to improve accuracy, and the need to deal with constantly\nchanging source data. \nThere are two primary ways to establish an ongoing ingestion process from data-\nbases: using a SQL interface (full-table ingestion that can be slow and costly, and\nincremental table ingestion that can be more performant and cost effective but\nmay still not be as timely as is needed) or using change data capture techniques,\nwhich is more complicated and may add cost but addresses the challenges associ-\nated with performance at scale. CDC solutions are available from all major RDBMS\nvendors, as open source, and increasingly as cloud services.\nWhen ingesting from files, key considerations include the need to parse different\nfile formats such as CSV, JSON, XML, Avro, etc.; the need to deal with source\nschema changes, which are common in files; and the need to deal with snapshots\nor data increments that can be delivered as either a single file or multiple files.\nYour ingestion process must be very resilient to change and must be able to deal\nwith many different edge cases. \nMost SaaS data can be accessed using a REST API, but a lack of standardization\nfor data access and the resulting variety of API access methods requires these\nkey considerations when building an ingestion pipeline for SaaS data: the need\nto implement and manage different pipelines for each source; the need to do\ndata type validation, as most SaaS APIs have minimal available data type infor-\nmation, and the need to adjust your pipeline for incremental versus full data\nloads, depending on what each provider supports.\nFor data streams, your ingestion pipeline must be able to decode messages into\na consumable format; deal effectively with duplicate data because streaming data\nsystems allow the same messages to be consumed multiple times; resolve multiple\nversions of the same message; and scale to accommodate high-volume data. \nTo implement ingestion pipeline quality control and monitoring, you will want\nto capture at least these important statistics in your metadata layer: the name of\nthe source database server (and IP address if possible); the name of the data-\nbase (or schema name depending on the database vendor) and the name of the\nsource table; the number of rows ingested per table for each ingestion (if batch\ningestion from a database); the duration of each ingest (start and end time-\nstamps); and how many messages processed over a fixed period of time (if\nstreaming data).\n",
      "content_length": 3082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "126\nCHAPTER 4\nGetting data into the platform\n4.7\nExercise answers\nExercise 4.1:\n 2—It’s hard to identify rows that were deleted.\n 4—It causes too much duplicate data to be stored in the platform.\nExercise 4.2:\n 4—You need to have a last_modified column in each of the source tables.\nExercise 4.3:\n 3—CDC includes all changes to a given row.\nExercise 4.4:\n 1—When a source system already has a predefined folder structure.\n",
      "content_length": 422,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "127\nOrganizing\n and processing data\nWe will introduce a number of concepts, such as the difference between common\ndata processing steps (such as file format conversion, deduplication, and schema\nmanagement) versus custom business logic (such as the rules each company\nchooses to apply to transform their data for a unique use case).\nThis chapter covers\nOrganizing and processing data in your cloud data \nplatform\nUnderstanding the different stages of data processing\nDiscussing the rationale for separating storage from \ncompute \nOrganizing data in cloud storage and designing a data flow\nImplementing common data processing patterns \nChoosing the right file formats for archive, staging, and \nproduction\nCreating a single parameter-driven pipeline with common \ndata transformations \n",
      "content_length": 791,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "128\nCHAPTER 5\nOrganizing and processing data\n We will walk through how to organize your data in storage, following the data jour-\nney through landing, archiving, staging, and production areas. We’ll explain the impor-\ntance of using batch identifiers to make it simpler to trace the data journey through the\nstorage areas and the warehouse and make debugging and lineage tracking easier.\n We will talk about the use of different file formats for the different storage areas\nand the importance of standardizing on binary formats in staging and production for\ncompression, performance, and common schemas.\n Last, we’ll explain how we can scale our common data processing by designing\nflexible and configurable pipelines, using orchestration.\n As covered in chapter 3, the processing layer, highlighted in figure 5.1, is the heart\nof the data platform implementation. This is where all the required business logic is\napplied and all the data validations and data transformations take place. The process-\ning layer also plays an important role in providing ad hoc access to the data in the data\nplatform.\nThe processing layer, shown in figure 5.1, is responsible for reading data from storage,\ntransforming it, and then saving it back to storage for further consumption.\nTransformations can include the implementation of common data cleanup steps, such\nas ensuring all date fields follow the same format, or the implementation of specific\nbusiness logic, such as joining two datasets together to produce the data needed for a\nspecific report. This layer should be able to work with both slow and fast data storage.\nThis means that the services or frameworks that we choose to implement in this layer\nshould have support for both batch processing of files stored in the slow storage as well\nas “one message at a time” or streaming processing from fast storage. This chapter will\nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 5.1\nThe processing layer applies business logic to data—transforming and validating it.\n",
      "content_length": 2243,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "129\nProcessing as a separate layer in the data platform\ndescribe processing principles that can be applied to both batch and streaming data,\nbut will focus mostly on batch processing. We will go deep into streaming data in\nchapter 6.\n The processing layer should be able to perform the following tasks: read data in\nbatch or streaming modes from storage, apply various types of business logic, and pro-\nvide a way for data analysts and data scientists to work with data in the data platform in\nan interactive fashion.\n5.1\nProcessing as a separate layer in the data platform\nWe’ve talked at length about the benefits of separating storage from compute in our\ndata platform architecture, so we won’t go over that again, but we will touch on an\nongoing debate we see happening over and over again—whether you should do com-\npute in the lake portion of the data platform or in the data warehouse. While propo-\nnents of using SQL in the data warehouse to apply business logic readily agree that\nthis violates the principles of a layered design, they point to other reasons why they\nthink this is a good idea. We felt it was worth sharing their view because you are likely\nto run into it when you propose your layered design.\n Separating storage from compute is a key tenet of a layered cloud data platform\ndesign. It brings scalability, cost savings, flexibility, and maintainability. But established\nways of operating are slow to change—traditional data warehouse architects may advo-\ncate for processing in the data warehouse, while modern cloud platform design dic-\ntates that processing should happen outside the data warehouse.\n Table 5.1 summarizes the pros and cons of each position—doing processing in the\ndata warehouse using SQL versus doing processing in the lake using a framework like\nSpark.\nNOTE\nSpark is available as a managed service from all three of the public\ncloud vendors. In Microsoft Azure, it is Azure Databricks; in Google Cloud, it\nis Dataproc; and in AWS, it is EMR (Elastic MapReduce).\nTable 5.1\nProcessing data in the warehouse versus in the lake\nProcessing data in the data lake (Spark)\nProcessing data in the data warehouse \n(SQL)\nFlexibility\nProcessing done in the data lake brings \nadditional flexibility because outputs can \nbe used not just for data served in the \ndata warehouse but also for data that can \nbe delivered to or consumed by other \nusers and/or systems.\nOutputs of data processing are typically \nrestricted to use in the data warehouse. \nDeveloper \nproductivity\nOnce trained, developers will appreciate \nthe power and flexibility of Spark with its \nsophisticated testing frameworks and \nlibraries to accelerate code delivery.\nWhile not designed as a programming lan-\nguage, SQL’s popularity means that find-\ning people who know it is relatively easy, \nso using SQL instead of learning Spark \ncan mean a shorter time to value.\n",
      "content_length": 2870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "130\nCHAPTER 5\nOrganizing and processing data\nCreating a modern data platform is a significant change for any organization, and\nthere are times when the need to minimize change, i.e., use the more popular SQL\nfor processing, may outweigh the more “purist view” of using new frameworks such as\nSpark. SQL talent is readily available and is likely already in your organization. But as\nyour platform scales, the challenges with using SQL for processing in the data ware-\nhouse will continue to mount.\n In our experience, doing processing in the data warehouse is one of those things\nthat “seemed like a good idea at the time.” It is likely to get you to a solution fairly\nquickly, and for small platform solutions it might be fine for the mid to long term, but\nif you want to take full advantage of the flexibility of a cloud data platform, it’s not the\nbest solution. You will want to do your processing outside of the data warehouse for all\nthe reasons stated.\nData \ngovernance\nProcessing data as close to the source as \npossible supports more consistent use of \ntransformed data across different sinks \nand reduces the risk of multiple people \ntransforming data in the sink and defining \ndata differently.\nProcessing data in the data warehouse \ncan support a data governance program, \nbut if processing is also done in the data \nlake, conflicts on data definitions may \nemerge.\nCross-platform\nportability\nSpark produces fully portable code that is \nindependent of the cloud vendor. Chang-\ning from one data warehouse to another is \neasier if transformations don’t also need \nto be changed. No migration and minimal \ntesting would be involved.\nTransforms done in ANSI-SQL are sup-\nported by all of the major cloud provider’s \ndata warehouse offerings and are porta-\nble, provided that no extra cloud vendor–\nspecific add-ons have been added. Work \nwill be involved to migrate and test code.\nPerformance\nNo amount of processing will impact data \nwarehouse users when processing is \ndone outside the data warehouse. \nMost modern cloud data warehouses pro-\nvide great performance out of the box, but \nsome may suffer as processing load \nincreases.\nSpeed of \nprocessing\nReal-time analytics is always possible.\nReal-time analytics is possible in some \ncloud data warehouses but involves \nmultiple steps and/or products.\nCost\nWhen cloud data warehouse vendors \ncharge for processing, it will be much less \nexpensive to do processing in the lake.\nDepending on the data warehouse \nselected and the commercial terms asso-\nciated with it, processing in the data ware-\nhouse can be expensive.\nReusability\nReusable functions and modules are read-\nily available in Spark. All processing jobs \nare available, not just to deliver data to \nthe cloud data warehouse but also to \ndeliver processed data to other destina-\ntions, an increasingly popular use of cloud \ndata platforms.\nWhen available in cloud data warehouses, \nstored procedures and functions can pro-\nvide reusable code.\nTable 5.1\nProcessing data in the warehouse versus in the lake (continued)\nProcessing data in the data lake (Spark)\nProcessing data in the data warehouse \n(SQL)\n",
      "content_length": 3125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "131\nData processing stages\n We have seen blended solutions. For example, when the first use case for a data\nplatform is to replace a traditional data warehouse, repurposing existing SQL code\ncan be faster than redoing transforms in Spark. Once the data warehouse has been\nmigrated, the transform jobs can be moved to the data lake.\n For the purposes of this chapter, we will assume you are designing a cloud data\nplatform with storage and compute clearly separated.\n5.2\nData processing stages\nWhen thinking about processing data in a cloud data platform, it’s a good idea to visu-\nalize the data flowing through several stages. At each stage, we apply some data trans-\nformation and validation logic. This increases the “usefulness” of data as it is\ntransformed from raw and unrefined data coming from the data source to well-\ndefined and validated data products that can be used for analysis or made available to\nother data consumers. \n Each stage in the diagram in figure 5.2 consists of two things: a storage component\nwhere data for the stage is stored long term (raw data area, staging area, and produc-\ntion area), and a data processing component that reads data from storage, applies\nsome processing rules, and saves data to the next storage area. This data processing\ncomponent is implemented as a job using a distributed data processing framework,\nsuch as Spark. Different jobs are coordinated together using an orchestration layer in\nour data platform. \nRaw incoming data\nDeduplication\nStaging area: data ready for consumption\nEnterprise-level\nquality check\nEnterprise-level\ndata transformation\nValidation rule 1\nValidation rule 2\nValidation rule 3\nValidation rule 4\nTransformation job 1\nTransformation job 2\nTransformation job 3\nData product 2\nData product 1\nStandardize file formats and resolve schema differences\nProduction area: contains final data products\nCommon data\nprocessing steps\nBusiness\nlogic−specific\nprocessing\nsteps\nFigure 5.2\nData flows through different stages in the platform with processing applied at different \nsteps.\n",
      "content_length": 2050,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "132\nCHAPTER 5\nOrganizing and processing data\nData processing tasks can typically be divided into two broad categories: common data\nprocessing steps and business logic–specific steps. Common data processing steps are\nsteps that you apply to all data coming in from all data sources. Examples would be\nconverting file formats to a single, unified standard and making sure schema differ-\nences between incoming data and existing data are resolved. Common data process-\ning steps can also be jobs that deduplicate data and apply standard quality checks. An\nexample of this could be ensuring that all fields containing zip codes are valid or that\nall dates are formatted using the same pattern. \n In addition to common processing steps, each analytics use case will require its\nown set of transformations and validations that are specific to the use case. For exam-\nple, if you are preparing a data set for a marketing campaign efficiency report, you\nmight want to include only campaigns that produce a certain number of impressions.\nOne of the key benefits of the cloud data platform is that you can implement and exe-\ncute these custom validations and transformations for hundreds of potential reports,\neach running in its own isolated environment. In this new world, there is no need to\nworry about sharing compute or storage resources.\n In this section of the chapter, we will focus on how to plan and design two common\ntransformations steps: file format conversion and data deduplication. We will talk\nmore about schema management in chapter 8. But first, we’ll start with describing\nhow to organize your cloud storage and implement a data flow to support the data\njourney through multiple stages. This chapter focuses on batch data processing, but\nwe will explore real-time processing and analytics and how they differ from the batch\napproach in chapter 6.\n5.3\nOrganizing your cloud storage \nIt may not sound like it’s important, but having a set of consistent, clear principles on\nhow to organize your data in cloud storage is very important. It will allow you to build\nstandardized pipelines that follow the same design with regard to where to read data\nfrom and where to write data to. This standardization will make it much easier for you\nto manage your pipelines at scale. It will also help your data users search for data in\nthe storage and understand exactly where to find the data they need. \n Over the course of having implemented multiple cloud data platforms for compa-\nnies in a wide range of industries, we have arrived at a storage organization pattern\nthat satisfies most of the use cases. Figure 5.3 will walk you through it.\n \n \n \n \n \n \n \n",
      "content_length": 2657,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "133\nOrganizing your cloud storage\n1\nLanding area—When data arrives from the ingestion layer, it is saved into a land-\ning area where all incoming raw data resides until it gets processed. This land-\ning area is a transient area, meaning that data is not stored there for the long\nterm.\n2\nStaging area—Next, raw data goes through a set of common transformations:\nmaking sure it conforms to existing schemas for this data source, converting it\ninto a common Avro binary format, and applying any organization-level data\nquality checks. Once these steps have been successfully applied, data is saved\ninto a staging area. For end users and data processing jobs, data in a staging\narea satisfies some basic quality requirements, there are no major issues with it,\nand it is ready to be used. \n1. Data arriving from the ingestion layer is saved into a landing area where all incoming raw data resides \nuntil it gets processed. Note that the ingestion layer is the only layer that can write to the landing area.\n2. Next, raw data goes through a set of common transformations, and then is saved into a staging area.\n3. Raw data is copied from the landing area into an archive area to be used for reprocessing, debugging \npipelines, and testing any new pipeline code. \n4. Data transformation jobs read data from the staging area, apply necessary business, logic and save data \ninto the production area. \n5. An optional “pass-through” job copies data from staging to production and then into the cloud warehouse \nas an exact replica of the incoming raw data to help debug issues with the business logic of other jobs.\n6. Different jobs read data from the staging area and produce data sets to be used for reporting or other \nanalytical purposes. These derived data sets are saved in a dedicated location in the production area and \nloaded into the cloud warehouse.\n7. Each step of the flow must deal with failures, saving data into a failed area of the storage and allowing \ndata engineers to debug the issues. Once addressed, data can be reprocessed by copying it back into \nthe landing area.  \nLanding\nStaging\nFailed\nArchive\nProduction\nData product 1\nData product 2\nData product 3\nCloud data\nwarehouse\nFigure 5.3\nOrganizing data optimally on storage requires a number of steps\n",
      "content_length": 2268,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "134\nCHAPTER 5\nOrganizing and processing data\n3\nArchive area—After data is processed and saved into a staging area, raw data\nfrom the landing area should be copied into the archive area. This is an import-\nant step, because it will allow us to reprocess any given batch of data by simply\ncopying it from the archive back into the landing area and letting the pipelines\ndo their job. Data in the archive area can also be used for debugging pipeline\nissues and for testing new pipeline code. Data is only copied into the archive\narea once it has successfully made it to the staging area. There is a separate and\nimportant step in this flow to deal with any kind of failures.\n4\nProduction area—Data transformation jobs read data from the staging area,\napply the desired business logic, and save the transformed data into the produc-\ntion area. At this point, we also convert data from Avro format into Parquet for-\nmat, which is more suitable for analytics use cases. We will talk more about file\nformats and their differences later in this chapter. \n5\nPass-through job—Often viewed as a special case, a “pass-through” job copies data\nfrom staging to production (in Parquet format) and then into the cloud data\nwarehouse without applying any business logic. This job is optional, but having\na data set in the data warehouse and production area that is an exact replica of\nthe incoming raw data can be extremely helpful when debugging any issues\nwith the business logic of other jobs.\n6\nCloud data warehouse and production area—You would typically have many differ-\nent jobs that read data from the staging area and produce data sets to be used\nfor reporting or other analytical purposes. These derived data sets should be\nsaved in a dedicated location in the production area and also loaded into the\ncloud warehouse. \n7\nFailed area—Building a robust data pipeline means you need to be able to deal\nwith all kinds of errors and failures. There might be bugs in the pipeline code,\ncloud resources may fail, or incoming data may not satisfy common data-quality\nrules. After data has been saved into the landing area, each step of the flow\nmust deal with failures by saving data into a failed storage area. This will allow\ndata engineers to more easily debug the issues and find the data that caused\nthem. Once an issue is fixed, assuming it’s a code issue and not a data issue,\ndata can be reprocessed by copying it from the failed area into the landing area. \n5.3.1\nCloud storage containers and folders\nIn the data flow described previously, we referred to different stages of the flow as\n“areas.” Containers and folders are important concepts that you will need to under-\nstand to better organize your data and implement these areas in cloud storage. Differ-\nent cloud vendors use different names for these. AWS and Google Cloud refer to\ncontainers as “buckets,” and Azure uses the actual container name. The term folder is\nused universally across these three providers. \n From a hierarchy perspective, you create a cloud storage container and then\nupload files into specific folders in that container. Each container can host multiple\n",
      "content_length": 3131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "135\nOrganizing your cloud storage\nfolders. Containers have different properties that you can configure. While different\nvendors have different configuration options, the two most common properties you\nwill need to set up for your cloud containers are\nAccess and security—Most vendors allow you to control who can access files on\nstorage and what operations they are allowed to perform at the container level. \nContainer storage tier—Cloud vendors offer different storage tiers with different\nprice/performance characteristics. We will refer to those as hot, cold, and\narchive. The hot storage tier provides the fastest read/write operations, but also\nhas the highest cost for storing data long term. Cold and archive tiers are slower\nbut allow you to store large volumes of data long term for a much lower cost\nthan the hot tier. \nIn our data flow, each area—landing, staging, archive, production, and failed—is\nimplemented as a separate container in cloud storage. Container access security and\nstorage tiers can be configured as shown in table 5.2.\nFOLDER-NAMING CONVENTIONS\nUse folders to further organize data inside containers into a logical structure. Differ-\nent containers in our data platform will have slightly different folder structures, but\nbefore we describe them, we need to introduce some of the common elements that\nwill allow you to organize data and data pipelines in the platform in a logical manner: \nTable 5.2\nConfiguring container access security and storage tiers\nContainer\nPermissions\nStorage tier\nLanding\nOnly ingestion-layer applications are allowed to write to \nthis container. Scheduled pipelines can read data, and \ndata engineers supporting the platform have read/ \nwrite access. Data consumers don’t have access.\nHot. Reads and writes are \nhappening frequently. \nStaging\nScheduled pipelines can read/write data, and data engi-\nneers supporting the platform have read/write access. \nSelected data consumers have read-only access.\nHot. Reads and writes are \nhappening frequently.\nProduction\nScheduled pipelines can read/write data, and data \nengineers supporting the platform have read/write \naccess. Consumers of Parquet formatted data will have \nread-only access.\nHot. Reads and writes are \nhappening frequently.\nArchive\nScheduled pipelines can write data, and data engi-\nneers supporting the platform have read/write access. \nA dedicated data reprocessing pipeline has read-only \naccess. Very few selected data consumers have read-\nonly access. \nCold or archive. Depending on \ndata volume, you may store your \nmore recent data in the cold \narchive container and older data \nin the archive container.\nFailed\nScheduled pipelines can write data, and data engi-\nneers supporting the platform have read/write access. \nA dedicated data reprocessing pipeline has read-only \naccess. Data consumers don’t have access.\nHot. Reads and writes are \nhappening frequently. \n",
      "content_length": 2894,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "136\nCHAPTER 5\nOrganizing and processing data\nNamespace—The highest level in our hierarchy, namespaces are used to logically\ngroup multiple pipelines together. In larger organizations that deal with hun-\ndreds of pipelines, a department name or specific initiative can be used as a\nnamespace. For example, you can have a Sales namespace for data and pipe-\nlines used in sales-related reporting or a ProductX namespace that will contain\nall data and pipelines related to a specific product. In smaller organizations, we\nfind that a single namespace with an organization name is enough. It’s worth\nnoting that if you want to provide access to the data in different namespaces to\ndifferent groups of users, then creating a separate storage container per name-\nspace is a better option since it’s easier to assign permissions to containers.\nPipeline name—Each data pipeline should have a name that reflects its purpose\nand is visible in the pipeline logs as well as in storage folders created by the\npipeline. You will have some common pipelines—ones that operate on all data\nin the platform. For example, you will have a pipeline that takes data from the\nlanding area, applies common processing steps, and saves data into the staging\narea. You will also have one for archiving data. Name these pipelines so you can\neasily identify their function. \nData source name—As discussed in the previous chapter, the ingestion layer will\nassign a name to each data source you bring into the platform. This source\nname will be saved in the metadata repository, but it should also be included in\ncloud storage folder names so users and pipelines can easily identify where this\ndata is coming from.\nBatchId—This is a unique identifier for any batch of data that is saved into a\nlanding area. Since the only layer that is allowed to write data to the landing\narea is the ingestion layer, it is the responsibility of the ingestion application to\ngenerate this identifier. A common choice for this type of an identifier is a Uni-\nversally Unique Identifier (UUID). Many existing ETL tools allow you to gener-\nate a UUID that you can then use in your ingestion pipelines. Another good\nchoice for a BatchId is a Universally Unique Lexicographically Sortable Identi-\nfier, or ULID (https://github.com/ulid/spec). A ULID is shorter than a UUID\nand has a great sortability property. If you use a ULID as your BatchId, then\nnewer batches will always be on top of a sorter list, and you can always tell which\nbatch is older just by comparing two ULIDs. \nNow that we have identified all the common elements of a data pipeline, let’s take a\nlook at how to structure our folders in the cloud storage containers. \nLANDING CONTAINER\nA landing container will have the following folder structure:\nlanding/NAMESPACE/PIPELINE/SOURCE_NAME/BATCH_ID/\nHere “landing” is the container name, and the rest of the path is the folder structure.\nItems in bold are variables that will be set by the ingestion layer. The difference between\nPIPELINE and SOURCE_NAME is that a single ingestion pipeline can handle multiple\n",
      "content_length": 3077,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "137\nOrganizing your cloud storage\nsources. For example, when ingesting multiple tables from a single database in a\nRDBMS, PIPELINE could be something similar to my_database_ingest_pipeline, and\nSOURCE could be just the name of the table. We will assume the ingestion layer is using\na ULID as a batch identifier and that a single company-wide namespace is used. We will\ncall this namespace ETL for brevity. \n This shows how folders in the landing container might look for two ingestion\npipelines: \n/landing/ETL/sales_oracle_ingest/customers/01DFTQ028FX89YDFAXREPJTR94\n/landing/ETL/sales_oracle_ingest/contracts/01DFTQB596HG2R2CN2QS6EJGBQ\n/landing/ETL/marketing_ftp_ingest/campaigns/01DFTQCWAYDPW141VYNMCHSE3\nEach container has a folder structure that reflects its namespace, pipeline, and source.\nA ULID is used as the batch identifier.\n Here we can see that there are two pipelines. One pipeline ingests data from an\nOracle sales database and brings in two tables: customers and contracts. Another pipe-\nline brings in marketing data from an FTP server. It’s a good idea to make pipeline\nnames short, but descriptive, so someone looking at a folder in storage can under-\nstand where the data is coming from without referring to documentation or a meta-\ndata repository. In this example, each data source in the pipeline folder has one batch\nof data. BatchId is a folder itself and can contain multiple files that were produced by\nan ingestion application for a single ingestion: a full copy of a table or an incremental\nportion of the table, depending on the type of ingestion you are using. A landing area\ntypically only contains the most recent batches, since the data flow we described previ-\nously will move data into staging for further processing. If you see multiple batches pil-\ning up in the landing area, it could indicate that your downstream processing is not\nworking or is slow.\nSTAGING CONTAINER\nStaging container folder structure is similar to landing, but since we are planning to\nstore data in the staging area long term, data should be organized by time. A common\napproach to organizing by time is known as ingestion time-based partitioning, where we\nput batches into the folders that encode the time when each batch was ingested. It’s\nbetter to see this using the following example.\n This shows how to use time partitioning to organize data by ingestion time in a\nstaging container:\n/staging/ETL/sales_oracle_ingest/customers/year=2019/month=07/day=03/01DFT\n➥ Q028FX89YDFAXREPJTR94\n/staging/ETL/sales_oracle_ingest/contracts/year=2019/month=07/day=03/01DFT\n➥ QB596HG2R2CN2QS6EJGBQ\n/staging/ETL/marketing_ftp_ingest/campaigns/year=2019/month=06/day=01/01D\n➥ FTQCWAYDPW141VYNMCHSE3\nFor each pipeline and source folder, we will introduce three additional folders: year,\nmonth, and day. If multiple batches arrive in a single day, they are placed in the same\n",
      "content_length": 2874,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "138\nCHAPTER 5\nOrganizing and processing data\nfolder. And if you are using a ULID as in the previous example, folders can be sorted,\nand newer batches will always end up on top of the list in a cloud portal web UI or any\nother program you use to access data in storage. The naming convention year=YYYY/\nmonth=MM/day=DD comes from Hadoop and is supported by many distributed\nprocessing engines, including Spark. If you read the whole /staging/ETL/sales_\noracle_ingest/customers/ folder, Spark will recognize the time-partitioning structure\nand will automatically add year, month, and day columns in your data set. This way you\ncan easily filter the data you need. \nNOTE If you are ingesting data frequently, perhaps several times an hour, then\nyou may want to add another folder layer with hour=hh to minimize the num-\nber of batches your jobs need to read if you are only interested in the most\nrecent ones.\nArchived and failed containers will follow the same folder structure as staging.\n Production containers will have the same structure as staging, except here new\npipelines may be introduced; for example, if you have a job that combines data from a\ncontract data source and a campaign data source to produce a marketing report. The\nfollowing example shows that some data sets in the production container can be a\nresult of a data transformation, such as joining two data sets together:\n/production/ETL/sales_oracle_ingest/customers/year=2019/month=07/day=03/01\n➥ DFTQ028FX89YDFAXREPJTR94\n/production/ETL/sales_oracle_ingest/contracts/year=2019/month=07/day=03/01\n➥ DFTQB596HG2R2CN2QS6EJGBQ\n/production/ETL/marketing_ftp_ingest/campaigns/year=2019/month=06/day=01/\n➥ 01DFTQCWAYDPW141VYNMCHSE3\n/production/ETL/marketing_report_job/marketing_report/year=2019/month=7/\n➥ day=3/01DFXA98BGBACGSTH5J63B3ZCZ\nHere we can see a new pipeline called marketing_report_job. It’s a good idea to name\njobs in a way that reflects their origin. In this case, we can see the pipeline is not an\ningestion pipeline, but rather a data transformation pipeline. For data transformation\npipelines, there is rarely a single data source. Most often these types of pipelines read\ndata from multiple sources and produce a new data set. It’s unreasonable to encode\nthe names of all the sources required for the data transformation pipeline in a folder\nname because there could be dozens of them. Instead, a recommended approach is to\ncreate a new “derived” source; in our example, we call it marketing_report. Then you\nregister the information about this derived source in the metadata repository where\nyou can expand on which sources are required to create this derived data set. In our\nexample, that would be contracts and campaigns. Also, note that time partitioning\nhere is not the ingestion time, but rather the time this particular transformation job\nwas executed. \n",
      "content_length": 2847,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "139\nOrganizing your cloud storage\nORGANIZING STREAMING DATA\nIn a data platform architecture, streaming data lives in two different places. For pro-\ncessing that requires real-time response, we have fast storage, and for archiving and\nreprocessing, we have regular cloud storage. Organizing data in fast storage is differ-\nent from what we previously discussed. In message-oriented systems, such as Kafka,\nCloud Pub/Sub, and others, data is usually organized by topics that represent collec-\ntions of individual messages. There are no concepts of containers, folders, or storage\ntiers. We’ll talk more about how data is organized in fast storage in chapter 6. \n When it comes to saving streaming data to regular storage for archiving purposes,\nwe can apply the same storage organization pattern as we used for batch data in the pre-\nvious section. Imagine that we have a clickstream pipeline that we use for real-time\ningestion into our cloud warehouse. We have performed an initial data assessment and\nwe know that we receive about 100 MB of data per minute. We also know (from chapter\n4) that regular cloud storage is optimized for larger files, so we decide to flush click-\nstream data from fast storage to slow storage every 15 minutes. This flushing process will\nsave data to a landing area of our cloud storage, just like any other batch process would,\nand will follow the same folder-naming convention. The following example shows how\nstreaming data is archived into cloud storage by flushing data from the real-time layer\ninto the landing container. Each flush is assigned a unique batch id:\n/landing/ETL/clickstream_ingest/clicks/01DH3XE2MHJBG6ZF4QKK6RF2Q9\n/landing/ETL/clickstream_ingest/clicks/01DH3XFWJVCSK5TDYWATXNDHJ1\n/landing/ETL/clickstream_ingest/clicks/01DH3XG81SKYD30YV8EBP82M0K\nHere we are also using ULID as our unique batch identifier, and you can see that we have\nthree different batches that have been flushed from fast storage into regular storage. \n The remaining journey of this streaming data through different storage areas\nwould be exactly the same as for batch data: data will be converted to a unified file for-\nmat, cleaned up if necessary, and saved to the staging area; and raw data will be saved\nto an archive area. After that, data will be converted into Parquet format and saved to\na production area where it can be used for other batch data processing jobs or for ad\nhoc analysis. The only difference between streaming and batch data in this case is that\nwe will not be loading these batches into our cloud warehouse, because this data\nshould have been loaded by our real-time pipeline already. \nExercise 5.1\nWhy do you need to follow a naming convention for your cloud storage folders?\n1\nThis is how cloud providers expects you to name resources.\n2\nThis keeps the pipeline code consistent.\n3\nThis is a limitation of Apache Spark.\n4\nThis improves pipeline performance.\n",
      "content_length": 2907,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "140\nCHAPTER 5\nOrganizing and processing data\n5.4\nCommon data processing steps\nData processing pipelines in your platform will be divided into common data process-\ning pipelines and custom business logic pipelines. In this section, we will talk about\nwhat data transformations are usually implemented as common processing steps. We\nwill specifically look at\nFile format conversion\nData deduplication\nData quality checks\n5.4.1\nFile format conversion\nAs you learned in chapter 4, depending on the source, data can arrive into the plat-\nform in different formats, including CSV, JSON, XML files, or a custom binary format.\nOne of the core properties of a data lake is its ability to store and provide access to\ndata in different formats, so you might be wondering why we don’t just store data as is\nin our storage layer—a traditional data lake approach. \n Let’s consider what our data transformation and analytics pipelines would look\nlike in a traditional data lake. In a data lake, we’d push the responsibility of dealing\nwith different data formats to each of the pipelines individually. For example, if you\nare building a pipeline to produce a certain report, the first step in your pipeline\nwould be to read a file, figure out which format it is, and which columns and data\ntypes it contains, and only then apply the required business logic. This might be rea-\nsonable if you only have one or two pipelines, but once the number of pipeline\nincreases, this approach won’t scale because you will need to duplicate the file-parsing\nlogic in every single pipeline. If the file format changes or a new column is added, you\nwill need to update and test a lot of code. Leaving the original file formats unchanged\nalso makes data exploration much more complicated. Every person who wants to\naccess data will need to figure out how to read the files first.\n Our modern data platform design suggests a more organized and structured\napproach to this problem. We will still retain data in its original format and save it into\nthe archive area, but one of the first transformations performed on all incoming data\nis to convert it into a single unified file format. Actually, we will be using two different\nfile formats, as described in the previous section. We will use Apache Avro (https://\navro.apache.org/) for our staging area and Apache Parquet (https://parquet\n.apache.org/) in our production area.\nAVRO AND PARQUET FILE FORMATS \nBoth Avro and Parquet are binary file formats. Unlike CSV, JSON, and XML, which\nare text formats, Avro and Parquet are not stored in a human-readable format and\nrequire a special program to decode and encode actual data. While there are many\ndifferent binary file formats that are being used in the data space today, Avro and Par-\nquet are two of the most popular. \n",
      "content_length": 2793,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "141\nCommon data processing steps\n Binary file formats offer several advantages over text-based file formats. First,\nbinary formats take significantly less space on disk because of different optimizations\nthey can apply during data encoding. Both Avro and Parquet include column type\ninformation that allows for much better file compression. We have seen reductions of\nup to 10 times the original data size by going from a text-based file format to a com-\npressed binary format. Smaller file sizes not only reduce your cloud storage costs, but\nalso significantly speed up your data processing pipelines. \n A second advantage of binary file formats is that they enforce the use of a certain\nschema for all files. This means that before saving any data in Avro or Parquet format,\nyou must define which columns and column types exist in your data set. In Avro file\nformat, this schema is actually embedded into every single file, so any program or data\npipeline that reads these files will be automatically aware of all column names and\ntheir types. Schema and file format standardization definitely requires extra develop-\nment and maintenance efforts in comparison with just storing all data as is, but in our\nexperience, this effort pays for itself many times over when you need to deal with more\nthan just a handful of pipelines or have to expose data in the platform to different data\nconsumers. We will talk about schema management in more detail in chapter 8.\n Why do we need both Avro and Parquet formats? To answer this question, we need\nto discuss differences between row-oriented and column-oriented file formats. Most\npeople have experience working with row-oriented file formats, where all information\nfor a single data row is saved into a continuous file block. CSV format is the simplest\nexample of a row-oriented file format: rows are stored one after another, separated by\na newline character, as shown figure 5.4.\nWhen computer programs read files from storage, they don’t really do it byte by byte.\nFor performance reasons, they read a whole block at once. The block size depends on\nthe storage and file system parameters. In row-oriented file formats, values for col-\numns that belong to a single row are written one after another, as shown in figure 5.4.\nTo read the whole block from a file, we would get data for multiple rows. To read the\nwhole file, you will only need to perform M read operations, assuming your file con-\nsists of M blocks. This is quite efficient if your goal is to read all columns for all rows\nColumn 1, Column 2, Column 3,...\nFile\nRow 1\nRow 2\nRow 3\nRow 4\nRow N-1\nRow N\nFile block 1\nFile block M\n...\nColumn 1, Column 2, Column 3,...\nColumn 1, Column 2, Column 3,...\nColumn 1, Column 2, Column 3,...\nColumn 1, Column 2, Column 3,...\nColumn 1, Column 2, Column 3,...\nFigure 5.4\nIn a row-oriented file \nformat layout, information for a \nsingle data row is saved into a \ncontinuous file block.\n",
      "content_length": 2931,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "142\nCHAPTER 5\nOrganizing and processing data\nfrom a file. Row-oriented files are useful when your goal is to read all columns for all\nrows in a file and perform some operations on them. \n For typical analytics workloads, many of the queries are various aggregations on cer-\ntain columns with different grouping and filtering conditions. For example, if we want\nto count how many users with the status “premium” joined last month, then we only\nneed data from the user-status column and the user-joined-date column to answer this\nquestion. If you have dozens of columns in your hypothetical data set, reading all of\nthem just to use two is a waste of resources. This is where column-oriented or columnar\nfile formats come into play. As shown in figure 5.5, in columnar file formats, values for\na single column are stored one after another, even if they belong to different rows.\nBy reading a single block, you can get all values for a single column across all rows. For\nexample, if you want to get a total sum of column 3 from figure 5.5, then you only\nneed to read block 2 from the file and can safely ignore everything else. Columnar file\nformats provide much better performance when it comes to analytical workloads,\nwhere only certain columns are required to answer the question. This, of course, is a\nsimplified representation of a columnar format. In reality, you will have more than\njust a handful of rows in your files, and values for a single column will span multiple\nblocks, but the key idea is that these values will be arranged in continuous blocks\ninside a file. \n Another benefit of a columnar format is that since values for a single column are\nusually of one type (numbers, strings, dates, and the like), you can get a much better\ncompression ratio with columnar formats than with row formats, where values of dif-\nferent types are mixed in a single continuous block. \n Avro is a row-oriented file format. It provides support for primitive and complex\ndata types, including nested types. Additionally, Avro embedded the schema as a part\nof every file, which makes it easy for programs that work with Avro files to quickly get\nall column definitions and their types. Avro also supports schema evolution rules,\nmeaning that if you make schema changes that are backward compatible, then you\ncan always use the latest version of the Avro schema to read all previous Avro files,\neven if the schema changed over time. The simplest example of a schema evolution is\nColumn 1, Column 1, Column 1\nColumn 2, Column 2, Column 2\nFile\nColumn 3, Column 3, Column 3\nColumn 4, Column 4, Column 4\nFile block 1\nFile block 2\n...\nRow 1\nRow 2\nRow 3\nFigure 5.5\nIn columnar file formats, \nvalues for a single column are stored \none after another, even if they belong \nto different rows.\n",
      "content_length": 2778,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "143\nCommon data processing steps\nadding new columns to a data set. We will talk more about schema management and\nAvro features in chapter 8. \n All these properties make Avro a great choice for the staging area, which is primar-\nily used as a source for downstream transformations or ad hoc data exploration use\ncases. Being a row-oriented file format, Avro is not as efficient for analytics use cases as\na columnar file format. That’s why we are using Parquet as a file format for our pro-\nduction area. \n Parquet is a column file format with support for primitive and complex data types.\nIt provides fast access to individual columns in a data set, without having to read the\nwhole data set, which significantly improves performance of analytics queries. Parquet\nalso compresses very well, which helps with reducing storage footprint. All three\nmajor cloud warehouses—AWS Redshift, Google BigQuery, and Azure SQL Data\nWarehouse—have native support for Parquet, which makes loading data from the pro-\nduction area into the warehouse seamless.\nUSING SPARK TO CONVERT FILE FORMATS\nHow do we actually implement converting files from original formats into Avro and\nParquet? Because we are using Apache Spark as our distributed data processing frame-\nwork, this operation is quite simple. \n To work with the Avro file format in Spark (see listing 5.1), you need an external\nAvro library (https://github.com/databricks/spark-avro). Both Google Cloud Datap-\nroc and Azure Databricks services have a preinstalled version of the library available,\nand for the AWS EMR service, you need to explicitly specify external libraries at clus-\nter creation time. \nNOTE\nIf you are using Spark 2.4.0 or later, there is no need for an external\nAvro library, because support for Avro was added to Spark itself. Check which\nversion of Spark your cloud provider supports.\nNOTE\nYou need to make sure your Dataproc cluster has permissions to read\nand write data to the appropriate GCS buckets.\n \nExercise 5.2\nWhat is the benefit of storing data in both Avro and Parquet formats?\n1\nThis reduces the cloud costs.\n2\nThis increases platform reliability.\n3\nThis addresses different data access patterns in the staging and production\nareas.\n4\nThis makes data more portable between different cloud providers.\n",
      "content_length": 2277,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "144\nCHAPTER 5\nOrganizing and processing data\nimport datetime\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder ... # we omit Spark session creation for brevity\nnamespace = “ETL” \npipeline_name = “click_stream_ingest”\nsource_name = “clicks”\nbatch_id = “01DH3XE2MHJBG6ZF4QKK6RF2Q9”\ncurrent_date = datetime.datetime.now()\nin_path = f“gs://landing/{namespace}/{pipeline_name}/{source_name}/{batch_id}/*”\nout_path = f”gs://staging/{namespace}/{pipeline_name}/{source_name}/year=\n➥ {current_date.year}/month={current_date.month}/day={current_date.day}/\n➥ {batch_id}”\nclicks_df = spark.read.json(in_path)   \nclicks_df = spark.write.format(“avro”).save(out_path)\nNOTE\nIn our Spark code examples in this chapter, we assume data is stored\non Google Cloud Storage (GCS). If you are using AWS S3 or Azure Blob Stor-\nage with Spark, your path’s prefixes will be different.\nIn listing 5.1, we assume we want to read one of the incoming clickstream batches in\nJSON format and save them into a staging area in Avro format. First, we predefine\nsome variables that make up the path on GCS to our landing data. Note that we are\nusing a Python datetime library to get the current year, month, and day, so we can use\nit as a part of the staging area path. The actual Spark code is just the last two lines of\nthis snippet. First, we read JSON files from the input path, and then, we save Avro files\nto the output path. \n There are many oversimplifications in listing 5.1. First of all, we omitted the Spark\nsession creation and destruction details. We also didn’t include any error handling\nthat must happen for read/write operations. You can find these details in the Spark\ndocumentation. \n There are several things that are important to highlight about this code example.\nFirst, notice that we have hardcoded things such as namespace, pipeline name, source\nname, and batch id. In a real data processing application, you would make these val-\nues parameters that your pipeline code accepts. This way you can reuse the same pipe-\nline code for many different sources. We will be talking about how to make pipelines\nmore generic later in this chapter. \n Second, we haven’t really mentioned what the source JSON file schema is and how\nAvro knows which columns and types it includes. The reason we didn’t need to do that\nis because of Spark’s feature called schema inference. Spark can understand common\nfile formats and try to figure out which columns have which types automatically. This\nListing 5.1\nReading a JSON file from landing and saving it to staging in the Avro format\nDefines configuration variables \nfor our example pipeline\nInputs and outputs GCS paths\nfollowing our preferred folder\nstructure\nSpark has native methods for reading \nJSON data and inferring its schema.\nSaves data in Avro storage \nusing inferred schema from \nthe previous step\n",
      "content_length": 2846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "145\nCommon data processing steps\nis an extremely helpful feature that simplifies a lot of data transformation code. For\nnow, please keep in mind that just relying on schema inference features is not enough\nfor more complex use cases. We will discuss how to use Spark schema inference\ntogether with schema evolution rules in chapter 8.\n5.4.2\nData deduplication\nData deduplication is a large and important topic. The problems that data deduplica-\ntion deals with can be broadly categorized into the following two challenges:\nAre two similar entries in your data set representing the same logical entities?\nFor example, are the John Smith and Jonathan Smith entries in your customer\ndata referring to the same person or two different persons? To deal with these\nissues, a separate set of techniques and tooling has been created over the years,\ncommonly referred to as master data management (MDM) tools. \nHow can you enforce that certain attributes in your data set are unique; for\nexample, making sure you don’t have two records with the same payment trans-\naction_id in your payments data set?\nDiscussing MDM tools and approaches is outside of the scope of this book, and you\ncan refer to existing books and materials on the topic. In this section, we will focus on\nthe problem of enforcing uniqueness on certain data since this is the problem that\nmost data platform implementations need to deal with. \n If you are familiar with how RDBMSs work, then you might be wondering, what is\nthe big deal about enforcing uniqueness? After all, relational databases hava had sup-\nport for primary and unique keys for decades. There are two main issues with unique-\nness in cloud data platforms:\nUnreliable data sources or ingestion replays.\nLack of uniqueness enforcement in existing cloud warehouses. Because of their\ndistributed nature, existing cloud warehouses don’t support constraints such as\nunique indexes or foreign keys. We will talk more about cloud warehouses fea-\ntures in chapter 9.\nFigure 5.6 demonstrates how duplicate rows can end up in the cloud warehouse even\nif a source like an RDBMS provides uniqueness guarantees.\n In this example, we have a unique key defined for the user_id column in the source\ndatabase. This makes sure that there will be no more than one row with the same\nuser_id. During normal operations, we would get an exact copy of the data from the\nsource database in our data platform, which would make sure the user_id column has\nunique values in the cloud warehouse as well; but normal operations shouldn’t be what\nwe plan for. Working with the cloud or with any complex system means planning for\nvarious types of failures—from cloud resource failures to operator errors and pipeline\ncode bugs. If there is a catastrophic metadata repository failure (catastrophic meaning\nthere are no backups to restore data from, and the like) or a data engineer mistakenly\n",
      "content_length": 2891,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "146\nCHAPTER 5\nOrganizing and processing data\ndecided to re-ingest previously ingested data, then there are no safeguards against\nduplicate data getting into the platform. \nNOTE\nIn chapter 4 we described several ingestion scenarios where capturing\nthe “history of changes” for a single row would result in intentional duplicates\nfor columns that otherwise could be unique in the source data. Since this is\nan intentional outcome, there is no need to deduplicate this data on inges-\ntion. If some reports require certain columns to have a unique value in this\ncase, the jobs producing these reports will be responsible for implementing\nthis logic. \nSimilar data duplication problems can happen when ingesting data from Kafka or\nother similar message busses and even flat files, but for different reasons. When read-\ning data from Kafka, you need to keep in mind that Kafka provides no guarantee that\nthe same message will be read only once by an ingestion application. Kafka rebalanc-\ning operations, individual node failures, or failures on the ingestion side can cause\nthe same message to be read multiple times, resulting in duplicates. \nNOTE\nStarting with Kafka version 0.11, it is possible to configure both appli-\ncations producing messages and applications consuming messages to avoid\nduplication and guarantee “exactly once processing.” This requires changes on\nboth sides of data delivery: producer and consumer. In data platform use cases,\nthis might not always be an option; applications producing data to Kafka may\nbe controlled by different teams or different organizations altogether. \nRDBMS\nCloud data platform\nIngestion\napplication\nMetadata\nrepository\nMetadata\nrepository\nIngest row: user_id=123,\nuser_name=John Smith \nRecord that\nuser_id=123 has\nbeen ingested\nRDBMS\nCloud data platform\nIngesti\nIngestion\napplication\nCloud\nwarehouse\nCloud\nwarehouse\nIngest row: user_id=123,\nuser_name=John Smith \nRow from previous ingest:\nuser_id=123,\nuser_name=John Smith\nDuplicate was added: \nuser_id=123,\nuser_name=John Smith\nHas a\nuniqueness\nconstraint\non user_id\ncolumn\nSome kind of catastrophic failure\nhappens to the metadata\nrepository or a data engineer\ntriggers full ingest by mistake.\nNow has row: \nuser_id=123,\nuser_name=John Smith\nStill has only\none row with\nuser_id=123\nLost the original watermark record\nor was forced to overwrite \nFigure 5.6\nFailures can result in duplicates even from reliable sources.\n",
      "content_length": 2417,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "147\nCommon data processing steps\nIngestion pipelines based on files are also prone to duplicates. While files themselves\nare usually immutable, failures either on the side that delivers files or on the data plat-\nform ingestion side can cause a file to be delivered or consumed multiple times. \nDEDUPLICATING DATA IN SPARK\nIn this section, we will show how to use Apache Spark to deduplicate incoming data to\naddress the issues described. Before we look at the code examples, let’s understand\nthe difference between global and “within the batch” deduplication scenarios as out-\nlined in figure 5.7.\nIn the first scenario, we are only concerned about deduplicating data within a single\nincoming batch that is stored in the landing area. We assume that all data that is\nalready in the platform and is stored in staging, production, and the warehouse is\nduplicate-free. This scenario is common when ingesting data from Kafka in batch\nmode or flat-file ingestion from an unreliable source (usually a third party or an appli-\ncation that cannot provide uniqueness guarantees). \n It is very simple to implement a deduplication code for this scenario using Spark.\nLet’s imagine in the following listing that we are ingesting CSV files containing user\ndata from an unreliable source, meaning each incoming file may contain duplicate\nuser_id values. \nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder ... # we omit Spark session creation for brevity\nnamespace = “ETL”    \npipeline_name = “users_csv_ingest”\nListing 5.2\nUsing Spark dropDuplicates to remove duplicate rows in CSV file\nuser_id\nemail\n5\nuser5@example.com\n6\nuser6@example.com\n5\nuser5@example.com\nuser_id\nemail\n1\nuser1@example.com\n2\nuser2@example.com\n3\nuser3@example.com\n4\nuser4@example.com\nAn incoming batch in the landing area \ncontains duplicate entries for user_id=5.\nData that already exists in the data\nplatform has no duplicates.\nLanding area\nStaging/production area\nFigure 5.7\nIncoming batch deduplication scope assumes duplicates are contained within a single \nbatch.\nDefines configuration variables for our example pipeline\n",
      "content_length": 2101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "148\nCHAPTER 5\nOrganizing and processing data\nsource_name = “users”\nbatch_id = “01DH3XE2MHJBG6ZF4QKK6RF2Q9”\nin_path = f“gs://landing/{namespace}/{pipeline_name}/{source_name}/{batch_id}/*”\nusers_df = spark.read.format(“csv”).load(in_path)  \nusers_deduplicate_df = users_df.dropDuplicates([“user_id”])\nThis Python Spark listing shows that we can read the CSV file from the landing area,\nand use the dropDuplicates function to remove all rows with duplicate user_id values\nfrom the Spark Dataframe. We can then proceed with converting the deduplicated\ndata set into Avro/Parquet, or similar. The dropDuplicates function takes a list of col-\numn names to use for deduplication, so you can use a combination of multiple col-\numns to enforce uniqueness. If you don’t specify column names, dropDuplicates will\nuse all columns for deduplication. \n Deduplication within a single incoming batch is easy to implement and is efficient\nfrom a performance point of view, because it doesn’t require us to join multiple data\nsets to identify duplicates. It is also limited in its ability to prevent duplicates from\nappearing in the data platform. For example, you might not have duplicates in your\ncurrent batch, but adding this batch to existing production data may result in dupli-\ncates, as demonstrated in figure 5.8.\n In this example, we don’t have any duplicate data in the incoming batch, so if we\nonly applied incoming data deduplication, we would miss the fact that user_id=3\nalready exists in the staging and production areas. This scenario is more complicated\nto solve, but it is also a more prevalent one. Our earlier example with a RDBMS inges-\ntion failure or an operator error, as well as flat files that could be sent multiple times\nby mistake, would fall into this category. \nInput and output GCS paths follow\nour preferred folder structure.\nSpark has built-in \nsupport for reading \nCSV files.\nUses dropDuplicates Spark data frame method \nto remove rows with duplicate user_id values\nuser_id\nemail\n5\nuser5@example.com\n6\nuser6@example.com\n3\nuser3@example.com\nuser_id\nemail\n1\nuser1@example.com\n2\nuser2@example.com\n3\nuser3@example.com\n4\nuser4@example.com\nAn incoming batch in the landing \narea doesn’t contain any duplicates.\nCombining incoming data with existing\nproduction data will result in duplicates\nfor user_id=3.\nLanding area\nStaging/production area\nFigure 5.8\nGlobal deduplications scope means we need to look for duplicates in an incoming batch \nas well as in existing data.\n",
      "content_length": 2483,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "149\nCommon data processing steps\nHow can we deduplicate data globally? If you are familiar with SQL and relational\noperations, then you might already see the solution: we need to join incoming data to\nthe existing data and deduplicate the resulting data set. Fortunately, Spark supports\nSQL, so it’s easy to express this logic, as shown in the following listing.\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder ... # we omit Spark session creation for brevity\nnamespace = “ETL”\npipeline_name = “users_csv_ingest”\nsource_name = “users”\nbatch_id = “01DH3XE2MHJBG6ZF4QKK6RF2Q9”\nin_path = f“gs://landing/{namespace}/{pipeline_name}/{source_name}/{batch_id}/*”\nstaging_path = f”gs://staging/{namespace}/{pipeline_name}/{source_name}/*”    \nincoming_users_df = spark.read.format(“csv”).load(in_path)    \nstaging_users_df = spark.read.format(“avro”).load(staging_path)\nincoming_users_df.createOrReplaceTempView(“incomgin_users”) \nstaging.users_df.createOrReplaceTempView(“staging_users”)\nusers_deduplicate_df = \\   \nspark.sql(“SELECT * FROM incoming_users u1 LEFT JOIN staging_users u2 ON \n➥ u1.user_id =  u2.user_id WHERE u2.user_id IS NULL”) \nIn this listing, we are reading both incoming batch and existing Avro data from the\nstaging area into two separate Spark data frames and then using Spark SQL to pro-\nduce a third resulting users_deduplicate_df data frame that will contain only rows\nfrom the incoming data frame that do not already exist in the staging data frame. Our\npipeline then can take this resulting data frame, convert it to Avro, and append it to\nthe existing staging data. \n So when should you use batch-scope deduplication versus global scope deduplica-\ntion? Batch-scope deduplication will only solve for simple use cases, like duplication\nwithin incoming flat files. If you want to avoid duplicates with a 100% guarantee, you\nreally should be doing both. The challenge is that as your data volumes grow, global\nscope deduplication will require more and more computational resources, because\nyou will need to join an ever-growing staging data set. This may or may not present a\nproblem, depending on your data volume and cloud costs that you can afford for the\ndata processing cluster. There are some optimization techniques you can use to make\nglobal deduplication require fewer resources. Note that in listing 5.3, we are reading all\ndata from the staging area. Assuming you followed the year/month/day partitioning\nListing 5.3\nDeduplicating data globally via a join\nWe will be reading ALL data from\nthe staging area for this source.\nReads both \nincoming and \nexisting data in \nstaging\nRegisters \ntemporary tables \nfor SQL operations\nSelects all data from the incoming batch, where a user_id \nfrom the incoming batch doesn’t occur in the existing data set\n",
      "content_length": 2792,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "150\nCHAPTER 5\nOrganizing and processing data\nstructure for your staging area, you can limit the scope of global deduplication to, for\nexample, the current year, month, or week. This will improve performance, but will\nincrease the risk of duplicates: if you receive a duplicate row that you have received a\nyear ago, or similar. You need to carefully evaluate the business logic and the nature of\nthe source data before you can safely limit the global deduplication scope.\n5.4.3\nData quality checks\nTwo of the most common concerns we heard from organizations that have adopted a\nstandard data lake approach is that they can’t always trust the data in the lake and that\ndifferent data sources have very different levels of quality. These concerns are easy to\nunderstand. Since data lake design in itself doesn’t offer any level of control over data\nthat is being ingested, the quality of data completely depends on the source. This is\nnot good enough for most use cases. Data users want reassurance that the data they\nwork with conforms to at least some basic set of standards.\n Interestingly enough, this problem is less prevalent in the traditional relational\nwarehouse. Relational databases, as we know, have a strict schema, which often\nincludes constraints on the length of certain columns, their type, and sometimes even\nadditional business logic, restricting what type of data can be saved to a table. \n In our cloud data platform design, a warehouse is a destination for the processed\ndata and is not used for ingestion. So we can’t use built-in warehouse controls. Also, as\nwe have seen before, existing cloud warehouses often lack column-level constraints\nthat you might find in traditional databases.\n To address this problem, we can implement required quality checks as one of the\nsteps in our data processing pipeline. In the previous section, we described\napproaches to data deduplication. Data deduplication can be considered one of the\nrequired quality checks. Here are some other common checks that we have seen:\nLength of values for certain columns should be within predefined range.\nNumeric values should be within a reasonable range: for example, no negative\nsalary values.\nExercise 5.3\nWhat is the main trade-off between batch-scope deduplication and global deduplication?\n1\nBatch-scope deduplication is much faster than global but will occasionally\nmiss duplicates within the batch.\n2\nBatch-scope deduplication is easier to implement but it doesn’t perform as\nwell as global deduplication.\n3\nGlobal deduplication is easier to implement, but it doesn’t provide full dedu-\nplication guarantees.  \n4\nBatch-scope deduplication is much faster, but it can’t find duplicates outside\nof the incoming batch.\n",
      "content_length": 2721,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "151\nCommon data processing steps\nCertain columns must never contain empty values. The definition of what\n“empty” means can also be different for different columns.\nValues must conform to specific patterns: for example, the email column\nshould contain valid email addresses. \nThese types of checks are really easy to implement in Spark and so are among the steps\nin our data pipeline. We can use the Spark filter function to filter out rows from the\ndata frames that do not satisfy our requirements. We have omitted the initial pipeline\nconfiguration code in this example, because it’s the same as in previous listings:\nusers_df = spark.read.format(“csv”).load(in_path) \nbad_user_rows = users_df.filter(“length(email) > 100 OR username IS NULL”)    \nusers_df = users_df.subtract(bad_user_rows) \nIn this code snippet, you can see how you can use an OR condition to filter out rows\nthat do not satisfy some predefined criteria. In this case, we don’t want rows with email\naddresses that are longer than 100 characters and rows where the username column is\nempty. Note that we save these rows into a bad_user_rows data frame so we can later\nsave them into a failed area in our platform, in case we later want to understand what\nhappened to these rows. \n We also use the Spark subtract function to remove bad rows from our original\ndata set. Our pipeline code can then proceed as usual with the users_df and have a\nseparate functionality for how to deal with bad rows. \nNOTE\nRemoving “bad” rows to improve the overall quality of the data should\nbe done with caution. Lots of data sets come from highly normalized rela-\ntional data sources. For example, you may have separate orders, order items,\nand customers data sets. If you delete an order because it doesn’t conform to\na certain data quality check, you will end up with orphaned order items that\nno longer link to any order. In such cases, you may decide to just alert a data\nengineer about the issue but let the data flow to the platform without\nchanges. You can also implement a more sophisticated data quality check that\ntreats an order and all related items as one unit and either allows all related\nitems through or fails all of them.\nWhile this example is very simple, it does demonstrate the overall approach to imple-\nmenting data quality checks in Spark. Other considerations about data quality flow\nthat you may need to make are the following:\nChecking criticality—From the data user’s perspective, not all data quality issues\nhave the same level of criticality. For example, an empty username column may\nnot break any existing business processes, but it is something that data users may\nUses the Spark built-in method \nto read data from a CSV file\nCreates a new Spark data frame\nobject by filtering out rows with\nemails that are too long or where\nthe username field is empty\nRemoves bad rows from the original data \nframe by using the Spark subtract method\n",
      "content_length": 2921,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "152\nCHAPTER 5\nOrganizing and processing data\nwant to be informed about. On the other hand, a negative value in the salary col-\numn may break an existing report. Such data must never enter the platform.\nAlerting on data quality issues—You may want to send alerts to data engineering\nor selected data consumers in the case of data quality issues. \nRemoving bad rows or failing the whole batch—In some cases, if a certain number of\nrows in the incoming data set don’t pass quality checks, you can decide to not\ningest a batch altogether and move it to the failed directory for further investi-\ngation. This is a common scenario when the incoming batch represents a sum-\nmary of some state, for example, the inventory summary for the previous week.\nProcessing a partial summary could be worse than not processing anything at all. \nNow that we have looked at various steps in our common data processing pipeline, a\nnatural question is, how do we scale this approach to hundreds of different data\nsources? The code examples we showed so far only worked for a single source. Copy-\ning and pasting code snippets for each data source you want to process is obviously\nnot going to work. This brings us to the next section of this chapter, in which we will\ndiscuss how to design a flexible and configurable data pipeline. \n5.5\nConfigurable pipelines\nAs we mentioned earlier in this chapter, shifting from a “free for all” data lake model\nto a more organized data platform approach allows us to unify how data is organized\non storage, how it moves from one stage to another, and what data consumers should\nexpect by looking at data at any stage. It also allows us to standardize some of the com-\nmon transformation steps into a single, highly configurable pipeline. \n We now know that for each data source that we ingest into the incoming area, we\nwould need to at least do file format conversion, deduplication, and some basic data\nquality checks. Because we know the exact folder structure, we can create a single\npipeline that will accept parameters, such as pipeline name and data source name,\nand perform common data transformation steps. This pipeline will be responsible for\nprocessing all incoming data sources, but it will be called with different parameters for\ndifferent sources. Figure 5.9 demonstrates how such a pipeline can be constructed.\n A common data processing pipeline will consist of several modules responsible for\ndifferent aspects of the pipeline. You can implement this pipeline by separating your\nmodules into separate jobs and then using an orchestration layer to execute them one\nafter another, or have a single processing job with several different functions, each\nresponsible for one step in the transformation pipeline. Choosing which one is best\nwill depend on which data processing engine you are using and/or your development\nteam preferences. Regardless of how you choose to implement your pipeline, each\nmodule should accept a configuration telling the module which data source to pro-\ncess, where on storage the source is located, what the source schema is, which col-\numns to use for deduplication, and the like.\n A recommended place to store such configurations is the metadata repository.\nThis way you have a central place with all the configurations and a unified mechanism\n",
      "content_length": 3306,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "153\nConfigurable pipelines\nto fetch a configuration for your data source using the metadata API. If you are deal-\ning with only a handful of pipelines, then you can choose to store configurations in a\ntext file in a dedicated location in cloud storage. \n The only missing piece in this flow is a component that will actually launch the\ndata processing job with a required configuration. This component is the orchestra-\ntion layer in our cloud data platform architecture, highlighted in figure 5.10.\nThe orchestration layer is the glue that holds together the entire pipeline. It needs to\nmonitor the landing area in storage for new data batches. Once a new batch is\ndetected, it will extract the pipeline name, data source name, and other needed\nparameters by using folder-naming conventions that we described earlier. Depending\nIncoming data\nFile format conversion module\nDeduplication module\nData quality checks module\nStaging data\nMetadata API\nPipeline name\nSource name\nSchema\nColumns to deduplicate\nQuality check rules\nQuality check severity\nProvides configuration\nto the pipeline\nExample of\nconfiguration\nparameters\nCommon data transformation pipeline\nFigure 5.9\nAll common \ndata processing steps can be \ncombined into a single pipeline \nthat accepts configuration \nfrom the metadata API. \nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nThe orchestration layer is\nresponsible for coordinating\nmultiple data-processing jobs\nand handling job failures\nand retries.\nFigure 5.10\nThe orchestration layer in a data platform architecture\n",
      "content_length": 1763,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "154\nCHAPTER 5\nOrganizing and processing data\non the orchestration mechanism you choose, you will either need to implement cus-\ntom code that will periodically check for new data to arrive and then use existing trig-\ngers in tools such as Apache Airflow; or, you can use cloud provider built-in\nnotification mechanisms. With these bits of information, the orchestration layer will\nthen fetch a more complete configuration for the data processing job and launch it.\nFigure 5.11 walks you through this process step by step.\nBecause common processing steps for different data sources are typically independent\nof one another, we can run multiple common data transformation jobs in parallel.\nThis will allow us to process potentially hundreds of different data sources in a timely\nmanner. \nSummary\nThe processing layer is the heart of the data platform implementation. This is\nwhere all the required business logic is applied and all the data validations and\ndata transformations take place.\nProcessing can be done in the data warehouse, but if your system is critical to\nthe enterprise, and you want it to scale and be managed and available for the\nlong term, doing your processing in the lake will give you better outcomes.\nIn the processing steps, data flows through several stages, and at each stage data\ntransformation and validation logic is applied, thus increasing the “usefulness”\nof data as it is transformed from raw and unrefined data coming from the data\nsource to well-defined and validated data products that can be used for analysis\nor made available to other data consumers. \nData processing tasks typically can be divided into two broad categories—com-\nmon data processing steps and business logic–specific steps. Common data pro-\ncessing steps are steps that you apply to all data coming in from all data sources\nOrchestration tool\nMetadata\nrepository\nStorage landing area\nCommon data processing job\nAn orchestration tool\nmonitors the storage\nlanding area for\nnew data.\nFetch a pipeline configuration for an\nincoming data for a given data source.\nLaunch a data processing\njob and provide a\nconfiguration to it.\nFigure 5.11\nThe orchestration layer is responsible for providing a configuration \nto the data processing job.\n",
      "content_length": 2239,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "155\nExercise answers\nor jobs that deduplicate data and apply standard quality checks that are\nrequired for all data. In addition to common processing steps, each analytics\nuse case will require its own set of transformations and validations that are spe-\ncific to the use case.\nHaving a set of consistent, clear principles on how to organize your data on\ncloud storage is very important because it will allow you to build pipelines that\nfollow the same design with regard to where to read data from and where to\nwrite data to. It will also help your data users search for data in storage and\nunderstand exactly where to find the data they need. \nA common data processing pipeline will consist of several modules responsible\nfor different aspects of the pipeline. You can implement this pipeline by sepa-\nrating your modules into separate jobs and then using an Orchestration layer to\nexecute them one after another, or you can have a single processing job with\nseveral different functions in it, each responsible for a step in the transforma-\ntion pipeline. \nConverting file formats to binary file formats (AVRO and Parquet) offers sev-\neral advantages over using text-based file formats. First, binary formats take sig-\nnificantly less space on disk because of different optimizations they can apply\nduring data encoding; and second, they enforce the use of a certain schema for\nall files, making scalability much easier.\nWhen it comes to deduplication, a common data processing step, enforcing\nuniqueness is especially important in cloud data platforms with their typically\nunreliable data sources, or ingestion replays and lack of uniqueness enforce-\nment in existing cloud warehouses. \nBecause a data lake design in itself doesn’t offer any level of control over data\nthat is being ingested, and the quality of data depends entirely on the source, a\ngood practice is to implement required quality checks as one of the steps in a\ndata processing pipeline.\n5.6\nExercise answers\nExercise 5.1:\n 2—This keeps the pipeline code consistent.\nExercise 5.2:\n 3—This addresses different data access patterns in the staging and production\nareas.\nExercise 5.3:\n 4—Batch-scope deduplication is much faster, but it can’t find duplicates outside of\nthe incoming batch.\n",
      "content_length": 2262,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "156\nReal-time data\n processing and analytics\nIn this chapter, we’ll help you get a clear understanding of real-time or streaming\ndata—one of the most popular features of a modern data platform.\n We’ll cover the difference between real-time ingestion and real-time processing\nand walk through some examples of when to use one or both, showing different\ndata platform designs.\nThis chapter covers\nDefining real-time processing and real-time \nanalytics\nOrganizing data in fast storage\nUnderstanding typical real-time data \ntransformation scenarios \nOrganizing data for real-time use\nTranslating common data transformations into \nreal-time processing\nComparing real-time processing services \n",
      "content_length": 694,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "157\nReal-time ingestion vs. real-time processing\n We’ll also go deeper into how streaming data is organized—with producers, con-\nsumers, messages, partitions, and offsets. Then we’ll walk through some typical real-\ntime data transformation use cases, with particular attention on dealing with data\ndeduplication, file format conversion, real-time data quality checks, and combining\nbatch and real-time data.\n Last, each cloud vendor provides a pair of related services for real-time process-\ning—one that implements real-time storage and maps to the fast storage layer in our\narchitecture, and another that implements the real-time processing. We will look at\nAWS Kinesis Data Streams and Kinesis Data Analytics, Google Cloud’s Pub/Sub and\nCloud Dataflow, and Azure Event Hubs and Azure Stream Analytics.\n6.1\nReal-time ingestion vs. real-time processing\nAs we discussed in chapter 3, the processing layer, highlighted in figure 6.1, is the\nheart of the data platform implementation. This is where all the required business\nlogic is applied and all the data validations and data transformations take place. The\nprocessing layer also plays an important role in providing ad hoc access to the data in\nthe data platform.\nSo far in this book, we’ve used data processing and analytics scenarios focused on\nbatch data processing. In these scenarios, we assumed that data can be extracted from\nthe source system on regular intervals or that it naturally arrives in the form of files\nthat need to be processed. \nBatch \ndata\nStreaming\ndata\nIngestion\nSlow storage/direct data lake access\nFast storage\nOperational\nmetadata\nData\nwarehouse\nReal-time processing and analytics\nData\nconsumers\nData\nconsumers\nData\nconsumers\nETL tools overlay\nOrchestration overlay\nBatch processing and analytics\nFigure 6.1\nThe processing layer is where business logic is applied and all data validations and data \ntransformations take place, as well as where ad hoc access to data is provided.\n",
      "content_length": 1959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "158\nCHAPTER 6\nReal-time data processing and analytics\n Batch is not the only way data can be delivered and analyzed in our cloud data plat-\nform. You may have already heard the term “real-time data processing,” and in this\nchapter, we will explore this form of processing and its common use cases. Let’s start\nwith some definitions and use cases.\n When people use the terms “real-time” or “streaming” in the context of a data plat-\nform, it can mean different things to different people, and it is relevant in two layers\nof a data platform—the ingestion layer and the processing layer.\n Real-time or streaming ingestion takes place when you have a pipeline that streams\ndata, one message at a time, from a source into a destination such as storage or the\ndata warehouse or both. While the term real-time processing isn’t clearly defined any-\nwhere, much of the available product documentation, blog posts, and books use the\nterm to refer to straightforward data transformations applied to streaming data.\nExamples of these data transformations include converting a date field from one date\nformat to another, or more complex data cleanup use cases, such as making sure all\naddress fields follow the same format. \n The term “real-time data analytics,” on the other hand, is usually reserved for the\napplication of complex computations on streaming data. A good example might be cal-\nculating the probability of a certain event happening based on previous events. While\nthese differences may matter in some cases, going forward we will refer to all real-time\ndata processing and real-time data analytics use cases as “real-time processing.”\n Real-time ingestion can take place without using real-time processing, but real-\ntime processing typically requires real-time ingestion. Whether you need one or both\ndepends on the use case.\n Let’s look at two use cases in table 6.1—one can be satisfied by ingesting data in real\ntime into the data warehouse, and the other will require real-time processing done in\na separate system. The difference comes down to who the final consumer of the data is. \nIn our first use case, the final data consumer is a human analyst looking at a sales\ndashboard produced using data in a data warehouse. They are asking for “real-time”\nbut they aren’t sitting there refreshing the dashboard continuously and acting on\nsecond-by-second changes. It is likely that what they really want is to ensure that the\ndashboard can be refreshed when they want to see it, and that the data they see in the\ndashboard is up to date and reflects the state of things “as of now.” \nTable 6.1\nComparing the “real-time” needs of two different use cases\nUse case\nHuman analyst looking at a dashboard\nGaming application reacting to \nchanges in player behavior\nWhat “real-time” \nmeans to them\nData is refreshed when requested, and data \nreflects the state of things “as of now.”\nData is processed and delivered \nwith sub-second turnaround times.\nWhat they need in \na data platform\nReal-time ingestion to a data warehouse.\nReal-time ingestion and real-time \nprocessing.\n",
      "content_length": 3072,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "159\nReal-time ingestion vs. real-time processing\n To meet this “real-time” requirement, we can develop pipelines that deliver data to\nthe data warehouse in real time. It is important to note that even though the data is\narriving continuously into the data warehouse, data warehouses don’t process data in\nreal time. While dashboards can be updated as often as users want, it can take more\nthan a few seconds for a typical dashboard to refresh. The exact timing of data refresh\nrequired may vary, but as a rule, when the analysis is done by human consumers,\nbringing data into the cloud warehouse in real time and letting the data warehouse\nperform the analysis with a response time of seconds or minutes is usually an accept-\nable trade-off between performance and data platform architecture complexity. \n In this use case, data is delivered in real time (i.e., streaming) into the cloud data\nwarehouse, but it is not being consumed in real time or even close to real time. To a\nuser who is accustomed to seeing their data refresh once a day, a 15-minute refresh\nmight be considered “real time” to them. They may even call this a real-time dash-\nboard. Having the data delivered to the data warehouse in real time may well give\nthem exactly what they want without any real-time processing involved.\n When your business users say they want “real time,” take the time to explore what they\nmean. If the real-time requirement is to make current data available for analysis any\ntime, but the analysis itself happens in an ad hoc manner—i.e., as scheduled reports or\ndashboard refreshes requested by the user—then save yourself some extra work and cost\nand implement real-time ingestion without real-time processing. \n In our second use case, people aren’t involved. In online gaming, for example,\ndata collected from player engagement is to be used to change the game behavior\nitself. Obviously, this has to happen quickly, because you can’t wait seconds to react to\nsomething a player does and then change the game behavior. Unlike people, the\ngame is capable of reacting to ongoing instant changes, so real-time ingestion coupled\nwith real-time processing makes sense. \n So if the end data consumer is an application that needs to perform actions based\non incoming data, it’s a good indicator that ingestion and processing should be imple-\nmented, and you will need a real-time data processing system for this use case.\n Let’s summarize. These two use cases describe two different flavors of real-time\ndata processing. Our dashboard use case is an example of real-time data ingestion\n(sometimes called streaming ingestion or just data streaming) without real-time pro-\ncessing. If the only requirement for real time is that you have to make data available\nfor analysis as fast as possible, but the analysis itself happens in an ad hoc manner,\nthen real-time ingestion is what you should be implementing. If, on the other hand,\nthe requirement is to have the analysis itself done in real time to be passed on to\nanother system for action, then real-time ingestion and real-time processing will be\nrequired.\n Note that in the first scenario, you may still need to perform some data prepara-\ntion before it is made available in the warehouse using real-time processing engines,\nbut we will explore that later in this chapter.\n",
      "content_length": 3320,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "160\nCHAPTER 6\nReal-time data processing and analytics\n6.2\nUse cases for real-time data processing\nIn this section, we will take our two use cases and work through the data platform\ndesign considerations for each one.\n6.2.1\nRetail use case: Real-time ingestion\nIn this first use case, imagine that your company is a retailer that operates brick-and-\nmortar stores as well as an online store. Your old point-of-sale (POS) system in the\nphysical stores is only capable of delivering sales data once a day as a CSV file, while\nyour online store is capable of delivering each sales transaction as it occurs. These\ntransactions are available for analysis within seconds of visitors to the website clicking\nthe “Purchase” button. In this scenario, you would have two dashboards: one for phys-\nical stores with data updated once per day and one for the online store with data\nupdated throughout the day. \n The business users at your company want to visualize daily sales in a dashboard, but\ncombining data from offline and online stores on a single dashboard creates a lot of\nconfusion because the data from the physical stores arrives once a day, while online\nstore data is delivered constantly throughout the day. \n Before the POS upgrade, we had two different pipelines: one batch pipeline that\nprocessed files delivered daily from the POS system and one real-time pipeline that\nprocessed online sales transactions. Both are supported by respective layers in our\ncloud data platform architecture, with the serving layer being a cloud data warehouse\nwhere data can be accessed by reporting tools. \n But, as you can see in figure 6.2, combining two data sources with different data\nrefresh rates into a single dashboard will not give your business a consistent overall\nview of daily sales. \nBefore POS upgrade:\nWeb\nPOS\n1. Data from online sales transactions are streamed\n    in real time into fast storage and delivered in real \n    time into the cloud data warehouse. Real-time \n    data is also archived in slow storage.\n2. Data from the store POS system is\n    ingested into storage once per day,\n    where it is processed and delivered\n    to the cloud data warehouse.\n3. Data from both sources is available in \n    the data warehouse, but the different\n    timing of delivery means that two \n    separate dashboards are required.\nFast storage\nBatch archiving\nprocess\nSlow storage\nReal-time\nprocessing\nBatch\nprocessing\nCloud data\nwarehouse\nOnline sales\ndashboard\nOffline sales\ndashboard\nFigure 6.2\nCombining batch and real-time delivery of data into a data warehouse is possible but may result \nin limitations when displaying the data.\n",
      "content_length": 2633,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "161\nUse cases for real-time data processing\nLuckily, your company has decided to upgrade the old POS system to a newer version\nthat supports sending sales transaction data in real time. Figure 6.3 demonstrates what\nwill change in your cloud data platform when POS data becomes a streaming data\nsource.\n After the POS upgrade, we can deliver both POS data and online data via the real-\ntime layer, eliminating the timing discrepancy. We can now combine two separate\ndashboards into one. Notice that we are still archiving real-time data sets into slow\nstorage, as discussed in chapter 4. \n So in this scenario, our pipeline is delivering data from both data sources in real\ntime into the data warehouse. Dashboards can be updated as often as users want, but\nit can take more than a few seconds for a typical dashboard to refresh—not real time\nby our definition. So while data is being delivered in real time into the cloud data\nwarehouse, it is not being consumed in real time or even close to real time, but users can\nbe confident that the data they are looking at is up to date and reflects the state of the\nthings “as of now.” \n6.2.2\nOnline gaming use case: Real-time ingestion and real-time processing\nNow, let’s take a look at another example. Imagine that after successfully implement-\ning that POS migration to real-time data processing at your retail employer, you\nreceived an offer to join an online gaming company. This company wants to make one\nof its flagship games more sophisticated by adding more interactive elements where\nthe environment and other players can react to actions that are performed in the\ngame. Now that you are equipped with the experience of working with real-time data\nprocessing from your previous job, you think that you can apply the same pattern\nhere. Figure 6.4 shows the first draft of the architecture you have in mind.\n This architecture looks similar to our retail example, with only one data source—\nin this case the game application—which is not part of the data platform, but it does\nAfter POS upgrade:\nWeb\nPOS\n1. Data from both online sales transactions and the store POS system are\n    streamed real time into fast storage and delivered in real time into the\n    cloud data warehouse. Real-time data is also archived in slow storage.\n2. Data from both sources is available in the \n    data warehouse AND can be combined in \n    a single unified sales dashboard as there\n    is no longer any timing discrepancy.\nFast storage\nBatch archiving\nprocess\nSlow storage\nReal-time\nprocessing\nCloud data\nwarehouse\nUnified sales\ndashboard\nFigure 6.3\nWhen data is streamed from different sources, it can be delivered in real time to the data \nwarehouse and combined into a single unified report or dashboard.\n",
      "content_length": 2743,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "162\nCHAPTER 6\nReal-time data processing and analytics\nproduce data in real time. In fact, it plays a dual role, acting as both a data source and\na data consumer.\n Players use their devices (mobile devices, game consoles, or PCs) to interact with\nthe game. These interactions are sent to the game backend application (which in real-\nity is usually composed of many different microservices) as game events. These events\nare just pieces of data with information about what happened, when, where, etc.\nThese events flow through into the data platform in our real-time layer and end up in\nthe cloud data warehouse where, instead of reporting tools connecting to our ware-\nhouse, we’ve got the game backend application running some complex SQL state-\nments to make decisions on how the game environment for this particular player\nneeds to be adjusted. For example, you may need to calculate the probability of a cer-\ntain type of monster appearing in front of the player at a given point in time. \n This design looks good on paper, but as shown in figure 6.5, if you try to implement\nit, you will soon discover a significant limitation. Even though you can deliver data to the\ncloud warehouse in real time (limitations for specific cloud warehouses are discussed in\nchapter 9), there is no guarantee of how quickly a query will produce results. Data\nwarehouses are designed to provide reasonable performance when dealing with very\nBatch archiving\nprocess\nGame\nbackend\napplication\nFast storage\nReal-time\nprocessing\nCloud data\nwarehouse\nPerform an analysis\nof player actions\nusing SQL.\nAdjust the game\nenvironment based\non the analysis.\nGame events\nIn-game\nactions\nPlayer’s device\nSlow storage\nFigure 6.4\nFirst draft of \nthe real-time processing \narchitecture for an online \ngaming use case\nBatch archiving\nprocess\nGame\nbackend\napplication\nFast storage\nReal-time\nprocessing\nCloud data\nwarehouse\nIn today’s cloud data warehouses,\ntypical response times, even for a\nrelatively simple query, are measured\nin seconds and sometimes minutes, \nwhich is not good enough to adjust \nthe game in real time.\nGame events\nPlayer’s device\nIn-game\nactions\nSlow storage\nFigure 6.5\nUsing a \ndata warehouse for \nreal-time processing \nwon’t satisfy a need \nfor sub-second \nprocessing.\n",
      "content_length": 2257,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "163\nUse cases for real-time data processing\nlarge volumes of data, but they are not optimized for fast response. In today’s cloud data\nwarehouses, typical response times, even for a relatively simple query, are measured in\nseconds and sometimes minutes. Unless your online game is a game of chess, response\ntimes of tens of seconds or a couple of minutes won’t be acceptable, especially for an\ninteractive game. We need to look at a different solution. \n Modern cloud-based, real-time processing systems can not only perform basic data\ntransformations such as changing date formats or filtering messages that satisfy a cer-\ntain condition but can also perform complex computations and analytics. Some sup-\nport SQL so you can perform similar types of analysis in the real-time processing\nsystem that you would do in a cloud data warehouse. Later in this chapter, we will\nexplore the different cloud real-time processing systems, but for now, let’s assume that\nthe calculation that we originally planned to take place in the cloud data warehouse\ncan also take place in the real-time system. Figure 6.6 shows the second iteration of\nour architecture.\nIn this architecture, a game backend application submits one or more real-time pro-\ncessing jobs that run constantly in the real-time processing system, adjusting the calcu-\nlations with every new incoming message from the fast storage. One of the most\nimportant differences between submitting a query to the warehouse and submitting a\nquery to a real-time processing job is that a data warehouse starts reading and process-\ning data only after it has received a SQL query from an application and often reads large\nportions of data each time it runs; while a real-time job is always running and doesn’t\nneed to read large portions of data every time a calculation needs to be adjusted. \n It is also important to note that, unlike data warehouses, real-time processing sys-\ntems are not designed as data-serving endpoints. This means that your real-time pro-\ncessing job must save results to another system, one that can provide very low-latency\naccess to data. Usually a key/value NoSQL database or in-memory cache is used for\nBatch archiving\nprocess\nGame\nbackend\napplication\nFast storage\nReal-time\nprocessing\nCloud data\nwarehouse\nSubmit a real-time\nprocessing job to\nperform required\ncalculations.\nWe still need the data\nwarehouse for ad hoc\nanalytics.\nSave result of the\ncalculations into\nfast data store, like\nin-memory cache,\nNoSQL, or an RDBMS.\nFetch results of the analysis\nfrom the fast data store.\nAdjust the game\nenvironment\nbased on the\nanalysis.\nGame events\nPlayer’s\ndevice\nIn-game\nactions\nSlow storage\nFigure 6.6\nTo reduce response latency, real-time calculations should take place in a real-time \nprocessing system with a fast data store used to store the results.\n",
      "content_length": 2827,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "164\nCHAPTER 6\nReal-time data processing and analytics\nthis purpose, but we have also seen relational databases used successfully for storing\nthe results of a real-time calculation. Now our game backend can fetch the result of\nthe real-time data analysis with a latency of a few seconds or less. This is far more\nacceptable for our use case. \n Another common scenario for a real-time processing system is to save the results\nback into fast store, thus creating new streams of data based on incoming streams. We\nwill explore this scenario later in this chapter when we talk about common real-time\ndata transformations. \n6.2.3\nSummary of real-time ingestion vs. real-time processing\nSo, let’s revisit the differences between our retail and online gaming examples when it\ncomes to real-time processing requirements. \n In our retail example, where analysis is done by human consumers, bringing data\ninto the cloud warehouse in real time and letting the data warehouse perform the\nanalysis with a response time of seconds or minutes is an acceptable trade-off between\nperformance and data platform architecture complexity.\n In our online gaming scenario, where decisions are made by other programs and\nthese decisions need to happen fast, we can’t rely on the data warehouse response time\nfor processing. This use case is an example of a complete end-to-end, real-time, data\nprocessing implementation where data needs to be ingested into the warehouse in real\ntime, but we also need to perform some complex data processing and make the results\navailable to an application using a low-latency data store. This scenario requires addi-\ntional infrastructure, monitoring, and ongoing maintenance. You will need to make\nsure that your low-latency data store is highly available and provides optimal perfor-\nmance. Various cloud services definitely make this task easier than implementing a sim-\nilar data store on premises, but you will still need to balance performance and cost of\nyour low-latency data store as well as plan properly for outages and the like. \n6.3\nWhen should you use real-time ingestion and/or \nreal-time processing?\nNow that we know the differences between real-time ingestion and real-time process-\ning, let’s try to answer the question that we have been asked countless times when\nworking on various data platform implementations: can we do this in real time? Usu-\nally this question is asked in relation to either an existing use case or a new use case\nthat business wants to implement. Often there is a report of some sort that runs once\nper day and business users are interested in making it real time. \n It’s your job as a data platform architect to decipher the actual requirement here and\napply the right real-time processing approach. We, of course, will help you with that. \n If the end users are mostly concerned with data “freshness,” then implementing a\nreal-time ingestion process for the required data sources will probably satisfy that\nrequirement. In general, we suggest using real-time data ingestion whenever possible\nand reserve batch layer ingestion only for sources that don’t support real time or when\n",
      "content_length": 3135,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "165\nWhen should you use real-time ingestion and/or real-time processing?\ndata is naturally produced in batch. Real-time ingestion has multiple benefits. For\nexample, real-time ingestion requires less orchestration, such as monitoring if new files\nhave arrived or not. For relational databases, real-time ingestion layers make it possible\nto use a powerful change data capture (CDC) mechanism, which we described in chap-\nter 4. CDC allows you to capture all kinds of changes that happen in the database\n(inserts, updates, and deletes) and does this at a lowest possible granularity, unlike peri-\nodic batch snapshots that will miss changes happening “in between.” Real time is also\nbecoming a standard of data delivery today with more and more source systems making\ndata available as streams of messages. We described some benefits of a real-time change\ndata capture process in chapter 4. Today users expect data to “be there” as soon as a\nchange happens in a source system, so even if you don’t have a formal requirement for\nreal-time ingestion, your users won’t complain. Keep in mind the possibility of confu-\nsion if you mix batch and real-time data in reports and analytics. If your dashboard has\none section that is up to the second, and another where data is only refreshed once a\nday, there is a very real risk that some of the data consumers will not realize this, and the\nnext thing you know, they’ll be reporting that the system is “broken.” \nHINT\nIt’s often better to separate reports where data freshness varies. \nNOTE\nThere are other use cases for combining real-time and batch data,\nsuch as data enrichment, that we will explore later in this chapter.\nAnother benefit of standardizing your cloud data platform around real-time ingestion\nis the fact that today there is no industry standard system that works efficiently with\nboth batch and real-time data. The Google Cloud Dataflow service (using the Apache\nBeam API) is an example of such a system, but it is specific to Google Cloud. Apache\nSpark, which we used as an example in previous chapters, is a great batch processing\nsystem that also supports real-time processing using the Spark Streaming API, but it\ndoes so by using a micro-batching technique. Micro-batching means buffering incom-\ning real-time data into several seconds or longer intervals and then processing all\nbuffered data at once. This approach may not be suitable for use cases where true low-\nlatency response is required. Lack of industry standards here means that if you are\nusing both batch and real-time layers, most likely you will need to use two completely\ndifferent systems. \n In our experience, the vast majority of use cases where end users say they want data\nprocessing in real time can be satisfied by real-time ingestion. Most end users just want\ntimely data delivery into the cloud warehouse, so they can run their queries without\nworrying about data staleness problems. \n There is, of course, a set of problems that can’t be solved by real-time ingestion.\nOur earlier online gaming scenario is a good example of such a problem. But let’s\nlook at a simple use case first. Imagine that there is a report that runs a set of queries\nagainst a data warehouse once a day and then packages the results into an PDF docu-\nment and emails it to end users. Users of this report are asking you whether this\n",
      "content_length": 3349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "166\nCHAPTER 6\nReal-time data processing and analytics\nreport can be “made real time.” Now, real-time ingestion is required here, but it is not\nenough—in the end, the report is still scheduled to run once a day. \n In many cases, what we have seen in our practice is that going from daily to hourly\nreport delivery is “real time” for the end users. Sometimes it is improving from hours\nto every 15 minutes, but the idea is the same. If data is delivered into the platform in\nreal time, then it’s easy to run reports more frequently, because you can get extra\nprocessing capacity for the cloud data warehouse or offload certain reports from the\nwarehouse into the data lake altogether. Remember the “who is the consumer” rule.\nHuman data consumers would rarely make any use of reports that are refreshed\nin seconds. \nHINT\nWhen assessing the needs of business users, don’t simply accept the\nword “real-time” from them. Instead, explore what they really need for data\ntimeliness.\nFinally, there are legitimate use cases where additional real-time processing infrastruc-\nture is required. If the end data consumer is an application that needs to perform\nsome actions based on incoming data, it’s a good indicator that ingestion and process-\ning needs to happen using a real-time data processing system. There are enough\nexamples of such use cases from different industries: various recommendation systems\nthat suggest content to the users on websites and mobile apps need to have a low-\nlatency response time; monitoring and alerting systems that detect anomalies and take\ncorresponding actions based on incoming data; fraud detection in the online pay-\nment systems, etc. As we will see later in this chapter, real-time processing comes with\nsome unique challenges and restrictions. We encourage you to carefully analyze the\nuse case before deciding what type of real-time processing is needed. Table 6.2 lists\nsome factors that might help you.\nAnother question that comes up often is whether it is easy to convert a process from\nbatch to real time. In our experience, the answer is no, because batch in real time today\nrequires different technologies, so conversion means complete code rewrite, bringing\nin new pieces of infrastructure, etc. It is easier to move from batch ingestion to real-\ntime ingestion than to convert a complex batch-processing job to real-time processing.\nChanging the ingestion type only affects a single layer in our architecture and, if your\nTable 6.2\nFactors influencing processing decisions\nUse case\nIngestion\nProcessing\nDashboards\nReal time\nBatch\nIn-game actions\nReal time\nReal time\nRecommendation engines\nReal time\nReal time\nFraud detection\nReal time\nReal time\n",
      "content_length": 2688,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "167\nOrganizing data for real-time use\nsource system can support both batch and real-time delivery, there may be existing\ntools that you can utilize: for example, switching from batch extracts from an RDBMS\nto real-time ingestion using CDC tools. We recommend investing more time into plan-\nning around real-time use cases and choosing the required approach early in the sys-\ntem design to avoid costly rewrites.\n6.4\nOrganizing data for real-time use\nIn chapter 5 we introduced folder and file layouts for cloud storage that can be used\nto organize data for efficient data processing. One of the key benefits of following a\nstandard layout is that you can create a configurable ETL pipeline that can be used to\nexecute data processing steps that are common for all the data sources. A standard lay-\nout also helps users of your data platform, who need direct access to the data lake\nlayer, to easily navigate between different data sets. \n In this section, we will describe how this layout translates from files and folders in\nthe batch scenario to real-time storage and processing. Before we can do that, we need\nto introduce several important real-time storage and processing concepts. \n6.4.1\nThe anatomy of fast storage\nBefore we dive into describing how storage for the real-time systems works, we need to\nnote that we will use Apache Kafka as our main example. Later in this chapter, we will\nlook into cloud-specific services for AWS, Azure, and Google Cloud, but we can’t use\nany of them to describe the common concepts. As these are proprietary systems, we\ndon’t really have significant insights into exactly how those services work or what\nunderlying technologies they use. Because Apache Kafka is the most popular open\nsource system for real-time data ingestion and processing, and many cloud services\nseem to at least adopt similar concepts and terminology, we will base our discussions\nin this section on Kafka. When talking about specific cloud services, we will highlight\nterminology or behavior that is different from one that Kafka uses. \n Batch systems work with files, and files consist of individual rows that contain data.\nReal-time systems operate on the level of an individual row or a message. A message is\nbasically a piece of data that can be written and then read from the real-time storage.\nExercise 6.1\nYou are building a social mobile app for runners, which should be able to notify a user\nabout how their friend performed on a particular part of the route and suggest they\npick up the pace or congratulate them on a job well done. Which data ingestion and\nprocessing should you choose for this use case?\n1\nReal-time data ingestion and batch processing\n2\nReal-time data ingestion and real-time processing\n3\nBatch ingestion and batch processing\n",
      "content_length": 2771,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "168\nCHAPTER 6\nReal-time data processing and analytics\nYou can think of a single row in a relational database as being a single message or a\nsingle row in a text log file. A single JSON document with attribute names and their\nvalues is a good example of a message. JSON also supports arrays of documents, but for\nreal-time systems, a message would typically be a single document from that array. In\nreal-time systems, messages are typically very small in comparison to batch files. We are\ntalking about several KBs to 1MB per message. Messages are organized into topics, which\nare similar to folders on a file system. \n Messages are written into real-time storage by producers and are read and processed\nby consumers. Both producers and consumers in this context are applications of some\nsort. As seen in figure 6.7, producers write messages into topics (1), and consumers\nread from topics (2). \nSo far, this doesn’t sound too different from a batch system where you can have file\nproducers saving data to the storage, and file consumers, such as an ETL pipeline,\nreading those files. But there are some fundamental differences in how producers can\nexchange data with multiple consumers.\n In this real-time processing pipeline, we have a single producer, which is our inges-\ntion application, reading new, updated, and deleted rows from some RDBMS. There is\na single topic in our real-time storage and two different consumer applications. One\n(consumer Y) is a data transformation pipeline that performs common data transfor-\nmations and saves processed messages to a different topic. Consumer Y is acting both\nas a consumer and as a producer, which is not uncommon in real-time processing sys-\ntems. Another consumer, consumer X, is a real-time analytics job of some sort. Maybe\nit’s a machine-learning application, because otherwise, why would it read raw data that\nhasn’t been processed by consumer Y yet? ;)\n \nConsumer Y saves\nprocessed messages\nto another topic.\n1. Producers\n    add new\n    messages to\n    the topic.\n2. Consumers\n    read from\n    topics.\nProducer\nTopic\nMessage 1\nMessage 2\nMessage n\nMessage n+1\n...\nConsumer Y\nConsumer X\nFigure 6.7\nProducers write messages into topics. Consumers, which are applications, read from \ntopics.\n",
      "content_length": 2248,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "169\nOrganizing data for real-time use\n Messages in real-time storage systems are immutable. This means once a producer\nhas written a message to the topic, it cannot change or delete it. As shown in figure\n6.8, when a message is written to the storage, it is assigned an offset (1)—an always-\nincreasing number that plays an important role in real-time processing systems.\nOffsets are used to send acknowledgements back to the producers, confirming that\ntheir message has been successfully saved (2). This way, producers know they can send\na new message. Consumers also track offsets by sending the offset of the latest pro-\ncessed message back to the real-time system (3).\nThe offset/acknowledgement mechanism is used for providing reliability. If a con-\nsumer fails (which it eventually will), it can easily resume processing where it left off\nby checking what was the last message offset it processed because producer offsets\noffer protection from failures in the real-time storage system. If a producer didn’t\nreceive an acknowledgement that a certain message has been successfully saved, it may\ndecide to retry saving the same message again.\n Offsets also provide a way for us to monitor the performance of various consumer\napplications. In our example and shown in figure 6.9, we can tell that consumer Y is\nahead of consumer X because it is processing messages with greater offset numbers. If\nwe notice that consumer Y is falling behind consumer X, this may indicate problems\nwith the consumer application code, network issues, etc. \n Hopefully, by now you will start to realize the benefits of real-time storage and pro-\ncessing systems when compared to file-based storage. In file-based storage, a file is a sin-\ngle unit that your data processing jobs operate on. You usually read a file as a whole and\n3. Consumers acknowledge messages\n    they have processed by saving the\n    offset number back to the storage.\n1. Each message in a topic is\n    assigned a sequential number,\n    called an offset.\nProducer\nTopic\nMessage 1\nMessage 2\nMessage n\nMessage n+1\n...\nConsumer Y\nConsumer X\n2. Offsets are used to provide an\n    acknowledgement to a producer\n    that a message was successfully\n    written.\nFigure 6.8\nReal-time storage systems provide a mechanism for tracking which messages have been \nwritten and which have been consumed by which application using a mechanism of offsets and \nacknowledgements.\n",
      "content_length": 2415,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "170\nCHAPTER 6\nReal-time data processing and analytics\nprocess it in one shot. While there are benefits to this approach (see our deduplication\ndiscussion later in this chapter) some things are more difficult, such as reprocessing\nonly some of the data from a file. Real-time systems offer not only the ability to process\none message at a time with low latency, but they also provide additional mechanisms for\nreliability and message tracking. This makes coordination between different processing\njobs easier, because now you can tell exactly which data the job is processing right now.\nFast storage is not only fast, it is also smart storage, meaning that a real-time system does\nquite a bit more than just saving bytes to disks. \n6.4.2\nHow does fast storage scale?\nTypically, we would not dive deeper into specific storage and processing systems archi-\ntecture, since this book is focused on architecture principles and ideas. For real-time\nsystems though, understanding how these systems work internally is important for\nunderstanding why certain architecture decisions have to be made or what issues you\nshould expect down the road and how to deal with them. \n One related question is, how do real-time storage and processing systems scale?\nData volume and velocity often come hand in hand. Real-time systems that deal with\ncollecting and processing data from sensors of some sort, or processing clickstream\ndata from a website, need to deal with very large data volumes and guarantee low\nlatency processing at the same time. How do they achieve this?\n Obviously, real-time systems (or any modern data processing systems, really) can’t\nrun on just a single physical or virtual machine. They have to be distributed and run\non a cluster of machines to provide the scalability we require today. Figure 6.10 shows\nhow data and processing are distributed to multiple machines in a real-time system.\n \n1. This consumer is our common \n    data transformation pipeline. \n    It is currently ahead of consumer X.\nProducer\nTopic\nMessage 1\nMessage 2\nMessage n\nMessage n+1\n...\nConsumer Y\nConsumer X\n2. Consumer X, in this example, \n    is performing some complex\n    calculations in real time. It is\n    reading a message with an\n    offset.\nFigure 6.9\nOffsets also provide a way for us to monitor the performance of various \nconsumer applications.\n",
      "content_length": 2340,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "171\nOrganizing data for real-time use\nLike many other distributed systems, real-time storage systems split messages in a\ntopic into multiple partitions. Each message in a topic belongs only to a single parti-\ntion. These partitions are physically stored on different machines. Real-time systems\ndecide where to place a partition in order to maximize data availability and perfor-\nmance. Each machine in a cluster can host multiple active partitions and multiple\ncopies of other partitions. In figure 6.10, we show only a single active partition per\nmachine and a copy of the other two partitions. “Active,” in this context, means that\nthis machine processes actual producer and consumer requests. Copies of other par-\ntitions are needed to guarantee data availability. If one of the machines fails, another\nmachine holding a copy of that partition will start serving requests for that partition. \n By splitting data into multiple partitions and placing those partitions on different\nmachines, real-time systems make sure that even a very active topic with potentially\nhundreds of thousands of messages being produced and consumed doesn’t over-\nwhelm a single machine.\nNOTE\nWhy not save each message into a single file on a cloud storage instead\nof using a real-time system? You can achieve an impressive performance writ-\ning and reading a single file on a cloud storage. For example, AWS documen-\ntation says you can read a “small” file from S3 with 100–200 ms latency. On\nthe other hand, Apache Kafka in one of the LinkedIn benchmarks achieved\n14 ms latency for writing and reading a message. While milliseconds latency\nPartition 1\nPartition 2\nProducers and\nconsumers interact\nwith a real-time\nsystem without\nknowing on which\nspecific machines\ndata resides. It’s the\nrole of the system to\nroute them correctly.\nEach partition \nis stored on a\nseparate machine.\nEach topic is split into several\npartitions. Each partition\ncontains only a portion of \nall messages.\nEach machine also stores \ncopies of other partitions, so \nif a single machine fails, data \nis always available elsewhere.\nMachine 3\nMachine 2\nMachine 1\nTopic\nPartition 3\nPartition 1\nCopy of\npartitions 2\nand 3\nPartition 2\nPartition 3\nCopy of\npartitions 1\nand 3\nCopy of\npartitions 1\nand 2\nFigure 6.10\nReal-time systems scale by splitting messages into multiple partitions and distributing those \npartitions across multiple machines. Actual data placement is hidden from producers and consumers.\n",
      "content_length": 2457,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "172\nCHAPTER 6\nReal-time data processing and analytics\nmay not be critical to your workload, things get different when you need to\nwork with a large number of small files on cloud storage. We are talking about\nhundreds of thousands or millions of small files. Cloud storage is not\ndesigned for scanning a large number of files. Latencies will spike when try-\ning to get a list of all files that you need to process. Processing engines such as\nApache Spark will have a hard time working with many small files, because\nthey are designed to deal with fewer, bigger files. We described this problem\nwhen talking about flushing real-time data to cloud storage for archiving in\nchapter 4. \nAs you can see, there are a lot of moving pieces, even in this simplified example. Real-\ntime systems abstract the internal mechanics from the producers and consumers, so\nend users don’t need to worry about how exactly the data is distributed across differ-\nent machines. Understanding these details is important, because it helps to under-\nstand different failure scenarios that will affect real-time data processing. We will look\ninto different failure scenarios and ways they affect your data processing pipelines\nlater in this chapter.\n To conclude our tour of real-time storage system internals, we would like to recom-\nmend additional reading for those who want to dive deeper into details, since we had\nto make a number of simplifications. A great place to start is the seminal blog post “The\nLog: What every software engineer should know about real-time data’s unifying abstrac-\ntion” by Jay Kreps, one of the creators of Apache Kafka, at http://mng.bz/WdVa.\n6.4.3\nOrganizing data in the real-time storage\nIn chapter 5 we talked about the importance of having a standard layout for files and\nfolders in the cloud storage. A standard structure makes it easy for people to navigate\nbetween different data sets and also makes it possible to implement a common data\nprocessing pipeline. The same principles apply to the real-time storage and real-time\nprocessing. Let’s take a look at figure 6.11, which will walk you through the different\nstages of data flow, as introduced in chapter 5.\n \n \n \nExercise 6.2\nWhat is the purpose of offset tracking in a real-time system?\n1\nTo provide a reliable way for consumers to resume processing after a crash or\na restart\n2\nTo improve performance\n3\nTo provide scalability of the real-time system\n4\nAll of the above\n",
      "content_length": 2440,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "173\nOrganizing data for real-time use\n1\nData arriving from the ingestion layer is saved into a landing area where all\nincoming raw data resides until it gets processed. Note that the ingestion layer\nis the only layer that can write to the landing area.\n2\nNext, raw data goes through a set of common transformations, and then is\nsaved into a staging area. \n3\nRaw data is copied from the landing area into the archive area to be used for re-\nprocessing, debugging pipelines, and testing any new pipeline code.\n4\nData transformation jobs read data from the staging area, apply necessary busi-\nness logic, and save data into the production area. \n5\nAn optional “pass-through” job copies data from staging to production and\nthen into the cloud warehouse as an exact replica of the incoming raw data to\nhelp debug issues with the business logic of other jobs.\nFailed\nLanding\nArchive\nStaging\nProduction\nData product 1\nData product 2\nData product 3\nCloud data\nwarehouse\nFast\ndata store\n1. Data arriving from the ingestion layer is saved into a landing area.\n3. Raw data is copied from the landing area into archive area to be used for reprocessing, \ndebugging pipelines, and testing any new pipeline code.\n2. Next, raw data goes through a set of common transformations, and then is saved into a staging area.\n4. Data transformation jobs read data from the staging area, apply necessary business logic, \nand save data into the production area.\n5. An optional “pass-through” job copies data from staging to production and then into the \ncloud warehouse to help debug issues.\n6. Different jobs read data from the staging area and produce data sets to be used for \nreporting or other analytical purposes.\n7. Each step of the flow must deal with failures, saving data into a failed area of the storage.\nFigure 6.11\nDifferent stages that real-time data flows through\n",
      "content_length": 1853,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "174\nCHAPTER 6\nReal-time data processing and analytics\n6\nDifferent jobs read data from the staging area and produce data sets to be used\nfor reporting or other analytical purposes. These derived data sets are saved in a\ndedicated location in the production area and loaded into the cloud ware-\nhouse. In real-time processing, these data sets can also be saved into a fast data\nstore (cache, RDBMS, NoSQL) for low-latency access.\n7\nEach step of the flow must deal with failures, saving data into a failed area of the\nstorage and allowing data engineers to debug the issues. Once addressed, data\ncan be reprocessed by copying it back into the landing area. \nAt this level of detail, the only difference between batch and real time is that real time\ncan be optionally saved into a fast data store if you need low-latency access to the pro-\ncessing results. Other than that, stages are similar to batch processing:\nRaw data is saved as is into the landing area.\nAfter applying common data processing steps, real-time data goes into a staging\narea.\nReal-time data is archived to regular cloud storage.\nVarious data transformation jobs that implement business logic read data from\nthe staging area and save the result back into the production area.\nIn case of failures, either in data processing jobs or data quality checks, data is\ncopied into a failed area for further investigation. \nIn the batch processing scenario, all these different areas are implemented as folders\non cloud storage with files inside those folders. We are using an agreed-upon naming\nconvention for files and folders, which allows us to construct a generic data processing\nand orchestration pipeline. In the real-time processing system, we don’t have a notion\nof folders and files; instead, we have individual messages that contain data and topics\nthat provide logical grouping of these messages.\n We could use topics instead of folders and containers on the regular cloud storage\nand assume that landing, staging, production, and failure are just different topics in\nthe real-time storage system, but details are always more nuanced than that. Let’s use\nour retail example to look further into these details. Let’s assume that after upgrading\nour POS system to a newer version that supports real-time data delivery, we can start\nreceiving individual messages from the three main tables in the RDBMSs that are used\nby the POS: orders, customers, and payments. Maybe our POS vendor has imple-\nmented a nifty change data capture (CDC) connector for Debezium and started send-\ning all the new, updated, and deleted rows from these three tables into our real-time\ningestion system. \n In chapter 4 we showed a couple of examples of what a CDC message looks like.\nFor example, the following listing shows how a new entry in the POS orders table will\nresult in the following (simplified) message.\n \n \n \n",
      "content_length": 2869,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "175\nOrganizing data for real-time use\n{\nmessage: {\n    before: null,\n    after: { \n               order_id: 3,\n               type: “PURCHASE”,\n                      order_total: 37.45,  \n               store_id: 2432,\n               last_modified: “2019-05-04 09:05:39”\n},\noperation: “INSERT”,    \ntable_name: “orders”    \n      }\n}\nFor now we can think of a message as a JSON document with certain attributes. Some\nof these attributes will be present in every CDC message. These common attributes,\nsuch as operation, table_name, before, and after, provide some additional data about\nthe message itself. For example, a table_name attribute contains the name of the source\ntable and will dictate what kinds of attributes will be present in the message body (the\nafter field). These attributes will match the source table schema where each attribute\nwill represent a certain column from the source table. Keep in mind that this is just an\nexample, and different ingestion systems may produce messages of different structures.\n We will have similar messages for the other tables in the POS database: customers\nand payments. Should we save all messages to a single landing topic, or should we cre-\nate separate topics for each of the message types: orders, payments, and customers? \n From a technical point of view, nothing really prevents us from creating a separate\ntopic per message type. We would then have topics such as landing_orders, landing_\npayments, and landing_customers, each containing only a certain message type.\nThere are certain limits that cloud providers impose on the number of topics that you\ncan create. For example, Google Cloud’s Pub/Sub real-time system allows 10,000 top-\nics per project. This is probably enough for most organizations, but is this the optimal\nway to organize the real-time data? \n As illustrated in figure 6.12, the challenge with having a topic for each specific\nsource is that even in a small system, one RDBMS source can have hundreds of tables\nand, thus, we will need a separate landing, staging, production, etc., topic for each of\nthem. This doesn’t help with data discovery and also means that you will need a sepa-\nrate, real-time, data processing job for each topic because, typically, real-time systems\nallow a single consumer to read from a single topic. We learned in chapter 5 that\nthere are great benefits to having a single configurable data transformation pipeline\nthat applies all common data transformations to all incoming data sources. \nNOTE\nSome cloud real-time services, like Google Cloud Pub/Sub, for example,\nallow a single real-time data consumer to read from multiple different topics.\nListing 6.1\nExample of a CDC message\nThis is the body of the \nmessage; it contains \nthe relevant data \nabout the order.\nIn CDC systems like Debezium, \nthe operation attribute of the \nmessage specifies whether this \nis a new entry in the source \ndatabase, an update to an \nexisting entry, or a deletion of \nan existing entry.\nAnother useful attribute of this CDC message is the name of the \nsource table. Knowing the name of the table, we can expect certain \nattributes to be present in the message body.\n",
      "content_length": 3158,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "176\nCHAPTER 6\nReal-time data processing and analytics\nAn alternative to saving data to different landing topics is to use a single topic, but use\na message attribute to identify what type of data this message contains. In this\napproach, we use the fact that for a real-time data processing job to do anything with\nthe data, it needs to look into the message structure itself. Figure 6.13 shows a single-\ntopic approach that uses a common data transformation job.\nPOS\nRDBMS\nIngestion\nlayer\nlanding_orders\nlanding_customers\nlanding_payments\nOrders\ntransformation\njob\nCustomers\ntransformation\njob\nPayments\ntransformation\njob\nstaging_orders\nstaging_customers\nstaging_payments\nThe real-time\ningestion layer\nuses table names\nto save messages\nto specific topic.\nReal-time\nstorage has a\nseparate topic\nper table.\nSeparate topics\nrequire separate\nreal-time data\ntransformation\njobs.\nThis requires separate \ntopics for thestaging\narea, etc.\nFigure 6.12\nOrganizing real-time storage using one topic per each ingestion object is possible but will \nlead to an explosion of topics as your cloud data platform grows.\n{\nmessage: {\n \nbefore: null,\n \nafter: {\n \n \n \norder_id: 3,\n \n \n \ntype:\n \n \n \n\"PURCHASE\",\n \n \n \norder_total: 37.45,\n \n \n \nstore_id: 2432,\n \n \n \nlast_modified: \"2019-05-04\"\n \n \n},\n \noperation: \"INSERT\",\n \ntable_name: \"orders\"\n \n}\n}\nPOS\nRDBMS\nIngestion\nlayer\nLanding\nCommon\ntransformation\njob\nStaging\nThe real-time\ningestion layer\nsaves all messages\nto a single topic.\nReal-time storage\nhas a single topic\nfor all tables.\nWe can use a common\ndata transformation\njob that uses\ntable_name attribute\nto decide which\ntransformations\nto apply.\nThis allows us to\nmaintain fewer\ndownstream\ntopics.\nFigure 6.13\nWe can use a single landing topic and rely on message attributes to apply required data \ntransformation logic without having to run multiple jobs.\n",
      "content_length": 1849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "177\nOrganizing data for real-time use\nThis approach plays well with our idea of a single configurable pipeline for data trans-\nformations. For example, you can have a library of common data transformations for\nspecific message types and use the table_name attribute to call an appropriate library.\nThis reduces the number of active, real-time, data processing jobs and makes the solu-\ntion easier to monitor and maintain. It also reduces the number of downstream topics\nin the staging and production areas. \n Does this mean that a single-topic approach is always the preferred one? As you\nhave probably learned by now, the answer is that it depends on several different fac-\ntors. Generally, you should try and maintain a single landing topic with a common\ndata processing job, but if you are dealing with multiple real-time sources, especially\nthird-party ones, you may find yourself in a situation where message structures are\nvery different; or there are really no common transformations that are applicable to\nall data sources; or some sources may not provide an attribute that you can use in a\ncommon data transformation job to decide which transformations to actually apply. In\nthis case, you will need to use different landing topics, which you can postfix with the\nsource name, for example: landing_pos_rdbms, landing_web_clickstream, etc. \n Another scenario where you may need to use separate topics for different sources\nis if you start running into performance issues related to the cloud service limits and\nquotas. It’s typical for a cloud provider to import various limits and quotas on a ser-\nvice, so a single customer with a heavy load doesn’t impact other customers. All cloud\nvendors impose limits on how much data can be written to and read from a single\ntopic. If one of your sources starts to reach these limits, then your only option is to\nsplit different sources into separate topics, or, in some cases, even split a single source\nif it produces a lot of high-velocity data. \n Finally, sometimes there is a need to split data into multiple topics to implement\ncertain organizational restrictions. For example, in large organizations, certain data\ncan be processed only by a certain team and no one else outside this team can work\nwith a given real-time data set. In this case, as shown in figure 6.14, you can have a sin-\ngle real-time job, which reads from a landing topic and, depending on certain mes-\nsage attributes, saves data to a corresponding staging topic. \n Since none of the existing real-time storage and processing systems allows you to\ncontrol access to individual messages, splitting messages from different sensitive\ndomains into separate topics and controlling access to those topics is the only way to\nachieve a required security posture. \n All existing cloud real-time systems make it very simple to read a message from one\ntopic and save it to another topic, with minimal code. A job that inspects a message’s\nattributes and, based on those attributes, decides in which downstream topic to save\nthis message is a very common pattern in the real-time systems. Another example of\nthis pattern is real-time data quality checks, where you can inspect a message and\ndecide whether it should be saved into a staging topic or into a failed topic. Yet\nanother example is saving the results of a real-time job into an appropriate cloud data\nwarehouse table (if your cloud warehouse supports real-time ingestion) using a table-\nname attribute from the message. \n",
      "content_length": 3496,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "178\nCHAPTER 6\nReal-time data processing and analytics\n6.5\nCommon data transformations in real time\nIn chapter 5 we described some of the common data transformation steps that you will\nlikely need to implement in a batch-processing pipeline. This includes file format con-\nversion, data deduplication, and data quality checks. In this section, we will describe\nhow to implement them in a real-time processing system. We will also look into addi-\ntional transformations that we see often: combining real-time and batch data together.\nReal-time systems make some of these transformations simpler to implement and oth-\ners more complicated, so let’s dig in! \n6.5.1\nCauses of duplicates in real-time systems\nWe will start our discussion about common data processing steps in the real-time sys-\ntem with data deduplication. We talked about data deduplication in batch pipelines in\nchapter 5, and if you recall, it wasn’t that complicated. If you had a column in your\ndata set that could be used to uniquely identify a row, then you could use Spark built-\nin functions to easily deduplicate data.\n Things get more complicated in real-time systems. Our experience shows that\nwhen organizations move from batch pipelines to real time or implement brand-new,\nreal-time pipelines, the increased presence of duplicates in the final data sets puzzles\nmany data engineers and data analysts. The key challenge here is that now we need to\ndeal with two types of duplicates:\nPOS\nRDBMS\nHR\nRDBMS\nIngestion\nlayer\nLanding\nCommon\nrouting job\nAccess to these cloud resources \nis highly restricted.\nOnly members \nof the finance data\nengineering team\nhave access to this\ntopic.\nOnly members of \nthe HR data engineering\nteam have access to\nthis topic.\nWe use a routing job that doesn’t\nchange data, but only saves it to an\nappropriate topic based on certain\nmessage attributes.\nReal-time\ningestion saves\ndata from\nmultiple\nsources into a\nsingle topic.\nlanding_hr\nlanding_sales\nFigure 6.14\nTo control access and permissions to certain data, you need to split it into multiple \ntopics using a routing job.\n",
      "content_length": 2078,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "179\nCommon data transformations in real time\nDuplicate data introduced at the source\nDuplicates introduced as a part of real-time systems’ failure/recovery processes \nDuplicates introduced at the source are basically multiple copies of the same row of\ndata that for one reason or another happened in the source system. It could be a\nhuman operator copying/pasting data in an Excel spreadsheet that was later exported\nas a CSV file and loaded into our data platform. It could also be caused by an issue in\na relational database schema design where unique constraints are not enforced, allow-\ning duplicate data to be added and then ingested into our platform. There are many\ndifferent possible scenarios. What’s common about them is that they all happen out-\nside of our data platform, and we have little to no control over them. \n In batch pipelines, duplicates introduced at source—such as a duplicate row\ninserted into a source table by mistake or the same file delivered multiple times to an\nFTP server—are typically the only types of duplicates that you need to worry about.\nWhen it comes to the real-time system, a new type of data duplication problem is\nintroduced. Real-time systems need to balance many different parameters. They need\nto provide low-latency response, scale with increasing data volume, and provide guar-\nantees that data written to such a system is durably stored and will not disappear if one\nor more components of the real-time system fail. Because satisfying all these require-\nments at once is difficult (if at all possible), all existing real-time processing systems\nhave to compromise on certain characteristics to provide expected behaviors in the\nface of one or more components of the system failing. Losing data in the case of indi-\nvidual machine failures is usually not an option, so real-time systems introduce retries\nthat, in their turn, can cause data duplicates. \n During normal operations, real-time systems send an acknowledgement to the data\nproducers that their messages were successfully written to disk. This way, a producer\nknows that it’s safe to proceed to writing the next message. This flow is shown in fig-\nure 6.15.\n In distributed systems, and especially in the cloud, normal operations are not the\nnorm anymore. Real-time systems run on dozens or hundreds of machines, each with\nits own network, disks, and so on. Any of these components can and will fail all the time.\nActive partition 1\nCopy partition 1\n2. When a message is\n    successfully saved to disk\n    on machine 1, it sends an\n    acknowledgement to the\n    producer. The producer can\n    now move on to writing the\n    next message.\nMachine 2\n3. The message is\n    replicated to\n    machine 2\n    for redundancy.\nMachine 1\n1. A producer writes a \n    message to a partition.\nFigure 6.15\nDuring \nnormal operations, a \nreal-time system sends \nan acknowledgement to \nthe producer when data \nis successfully written \nto disk.\n",
      "content_length": 2942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "180\nCHAPTER 6\nReal-time data processing and analytics\nFor example, imagine that a producer is writing a message, but an underlying machine\nthat hosts that partition is experiencing some sort of a network problem. As shown in\nfigure 6.16, the message is written to the disk on that machine, but the acknowledge-\nment cannot be sent to the producer because the network is not available. \n The real-time system now thinks that that machine is down and tells another\nmachine in the cluster to pick up serving this partition. Our producer doesn’t receive\nan acknowledgement that a message was written successfully and tries to write the\nsame message again. This time it succeeds, but the message is written to a different\nmachine. Now the network issue is resolved, and the machine that originally hosted\nthe partition is back online. We know the data was written to the disk, so we have a sit-\nuation where a single message was written twice to the same partition. There is now a\nduplicate message that we will need to deal with somehow during processing. This sce-\nnario may sound very complicated and unlikely to happen, but even in a small distrib-\nuted system with several machines, these types of failures happen more often that one\nmight anticipate. \n Consumers are also susceptible to these types of issues. Consumers are even more\nlikely to experience different types of failures, just because there are usually more data\nconsumers than there are data producers. As shown in figure 6.17, if a consumer\nreads a message, and then the consumer application crashes, or a machine experi-\nences a network problem similar to that described previously, the offset won’t make it\nback to the storage system. When the consumer restarts, it will reread and reprocess\nthe same message, causing a duplicate. \n3. The producer retries \n    writing the same message  \n    to a machine that it knows\n    has a copy of the partition.\n5. The producer is not\n    aware of the duplication\n    and moves on to the\n    next message to write.\nThe active\npartition for\nthis message\nkey is on\nmachine 1.\n2. A network issue between\n    the real-time system and\n    the producer makes it\n    impossible to send an\n    acknowledgement, but\n    the message is already saved\n    to disk on machine 1.\n4. This time, the\n    acknowledgement\n    goes through\n    without issues,\n    but message 1 is\n    now stored twice.\nActive\npartition on\nmachine 1\nSend\nacknowledgement\nfor message 1\nActive\npartition on\nmachine 2\nSend\nacknowledgement\nfor message 1\nActive\npartition on\nmachine 3\n1. A producer writes\n    a message to a\n    partition.\nReal-time\nsystem\nProducer\nOK\n???\nWrite\nmessage 1\nWrite\nmessage 1\nWrite\nmessage 2\nFigure 6.16\nA network issue between a real-time system and a data producer will prevent an \nacknowledgement being sent. If a producer retries writing the same message, it may result in \nduplicates.\n",
      "content_length": 2885,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "181\nCommon data transformations in real time\nYou may think that a consumer can minimize these types of problems if it acknowl-\nedges every single message it processes and thus minimizes the probability of failures.\nIn reality, most real-time systems acknowledge processed messages in bulk every sev-\neral seconds. Acknowledging every message is very expensive in terms of performance\nand will not scale to the volumes that are expected from such systems today.\nNOTE\nSome real-time cloud services such as AWS Kinesis use an external\ndata store to checkpoint which consumer processed which messages. This\ndata store (DynamoDB, in the case of AWS) is not a part of the real-time sys-\ntem, but it doesn’t eliminate the problem described previously. A consumer\ncan crash before it saves offsets of processed messages to DynamoDB.\n6.5.2\nDeduplicating data in real-time systems\nNow that we know what causes data duplicates in real-time systems, let’s explore what\nsolutions can be implemented to solve this. As with any non-trivial technical problem,\nthere is no silver bullet. In chapter 5 we showed how to deduplicate data in a batch\npipeline using a column that has a unique ID. Can we use a similar deduplication\nmethod for a real-time pipeline? \n What makes deduplication in the batch pipeline relatively simple is the fact that the\nbatch pipeline can “see” all the data that exists at any given point in time. For example,\nit is an easy task to find all duplicate rows in a file, because once a file is written to cloud\nstorage, it doesn’t change. So all we have to do is go row by row, find rows that have the\nsame unique ID value and filter them out, and save the result to a new file. The same\napproach, to some extent, can be applied to relational databases and NoSQL stores. \n In real-time systems, we don’t have the ability to “see” all the data, because data is\nbeing continuously added to the system. And while messages that have been written to\nthe real-time system cannot be changed, new data arrives all the time. Readers that\nhave a relational database background may argue that data is constantly added and\n4. After the \n    consumer restarts, \n    it asks a real-\n    time system \n    what the last\n    checkpoint was.\n5. The consumer \n    reads message 1 \n    again, resulting \n    in duplicate\n    processing.\n3. Before the consumer\n    can acknowledge\n    that it has\n    processed 1 and\n    2, it crashes.\n2. The consumer\n    reads and\n    processes\n    message 2.\n1. The consumer\n    reads and\n    processes\n    message 1.\nReal-time\nsystem\nConsumer\nRead and\nprocess\nmessage 1\nRead and\nprocess\nmessage 1\nRead and\nprocess\nmessage 2\nAcknowledge\nmessages 1\nand 2\nGet last\nprocessed\nmessage\nMessage 1\ndata\nCheckpoint:\nmessage 1\nMessage 1\ndata\nMessage 2\ndata\nCheckpoint\nFigure 6.17\nIf a consumer crashes before it can acknowledge that a certain message has been processed \nback to the real-time system, then after it is restarted, it will process the same message again.\n",
      "content_length": 2983,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "182\nCHAPTER 6\nReal-time data processing and analytics\nchanged in a database as well, but relational databases have the ability to read a cur-\nrent snapshot of the data using a mechanism called transaction isolation. This allows a\ndata reader to operate on data that was “frozen in time” and, from that perspective,\nthe data acts like a static file on cloud storage. \n As illustrated in figure 6.18, real-time systems distributed across potentially hun-\ndreds of machines can’t provide the ability for data consumers to read a snapshot of\nall data at any given point in time. All this makes creating real-time data deduplication\njobs challenging, but it doesn’t mean it’s impossible. \nWe will now take a look at some of the available options. \n One of the important concepts in real-time processing is a time window. Each\nmessage that is written to the real-time system gets assigned a timestamp that indicates\nwhen exactly the message arrived. Using this timestamp—or, if your messages have\nsome sort of event time from the source system, you can use that as well—you can group\nmessages together into various time windows. For example, you can group messages\nthat arrived in the last five minutes, in the last hour, etc. Such grouping is very\nimportant for various real-time analytics applications, for example, calculating an\nhourly total sales amount from an online store. There are different types of time\nwindows that can be used for different types of analytics, such as sliding and tumbling\nwindows. Tumbling windows allow you to slice a data stream into equal non-overlapping\ntime segments. For example, if every 30 seconds you want to know how many users have\nvisited your website, you can use a 30-second tumbling window. Sliding windows also\nsegment data streams, but unlike tumbling windows, sliding windows can overlap.\nThere are other types of windows, but discussing these concepts is outside of the scope\nof this book. \n Since windowing allows us to group messages together using a timestamp, we can\nnow detect duplicates inside the group, as shown in figure 6.19.\nusers.csv\n{user_id:1,name:“Alice”}\nA file has a boundary and\ndoesn’t change. This makes it\neasy to implement a program\nthat will find duplicate rows.\nA data producer is\nabout to send a\nduplicate message.\nReal-time systems don’t provide the capability to read a “snapshot”\nof all existing data. There is also no clear boundary for the data\nbecause new data is added all the time.\nA data consumer works\nwith a single message\nat a time.\n{user_id:3, name:“John”} {user_id:2, name:“Bob”} {user_id:1,name:“Alice”}\nuser_id name\nAlice\nBob\nAlice\n1\n2\n1\nFigure 6.18\nReal-time systems are unbounded and don't provide the capability to read a snapshot of all existing \ndata at once. This makes detecting duplicates a challenging task.\n",
      "content_length": 2802,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "183\nCommon data transformations in real time\nFor example, we can use a 10-minute window to group together messages that were\nadded in the last 10 minutes. Now we have a boundary around the data and can easily\ndetect duplicates inside this window. Hopefully, you are already seeing some of the\nissues with this approach. The key question here is, how big should your window be?\nData is being added to the real-time system continuously, so, looking at our previous\nexample, if a new duplicate message arrives at 10:47, it will not fit into the existing\nwindow and a duplicate will be allowed through. \n Time windows can’t have an infinite size. They essentially cache incoming data\nusing real-time processing system resources and a big enough window can overwhelm\neven a big enough machine. Cloud providers always have some restrictions and limita-\ntions when it comes to using windows. AWS Kinesis Data Analytics, for example, rec-\nommends limiting time windows to a maximum of one hour. Google Cloud Dataflow\nhas built-in mechanisms to detect data duplications, but with the limitation of a 10-\nminute window. \n Using time windows for data deduplication will never provide a 100% guarantee\nthat there will be no duplicates in your real-time data sets. In many cases, this is not\ngood enough. Is there a better way to catch duplicates in an unbounded data stream? \n One common way to implement effective deduplication in a real-time system is to\ncache unique IDs in a fast data store of some sort and then have the data consumer first\ncheck this cache to see if a specific ID has been processed yet. If a message with a given\nID has been processed before, there will be an entry with this ID in the cache and the\ndata consumer can safely ignore this message as a duplicate, as depicted in figure 6.20.\n This idea works very well, but there are some new challenges that it introduces.\nFirst of all, the data store you use needs to be very fast. A lookup time for a given ID\nshould be measured in milliseconds. This way, your real-time processing is not slowed\n{user_id:1,name:“Alice”}\n{user_id:3, name:“John”}\n{user_id:2, name:“Bob”}\n{user_id:1,name:“Alice”}\nA data consumer uses a 10-minute\nwindow to group messages\ntogether. This allows easily finding\nduplicates within a window.\n10:45\n10:40\n10:37\n10:34\n10:32\n{user_id:4,name:“Helen”} {user_id:1,name:“Alice”}\n{user_id:3, name:“John”}\n{user_id:2, name:“Bob”}\n{user_id:1,name:“Alice”}\nFigure 6.19\nUsing time windows, we can find duplicate messages within a window.\n",
      "content_length": 2514,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "184\nCHAPTER 6\nReal-time data processing and analytics\ndown by extra checks. Fortunately, most modern key/value stores and relational data-\nbases have no issues in providing this type of performance, especially when it comes to\nchecking a single ID.\n Depending on the data volumes that you are dealing with in your real-time pro-\ncessing pipeline, you may find that storing all unique IDs requires too many resources\nfrom the data store, and it either becomes too expensive to maintain or doesn’t guar-\nantee required performance. In our experience, you need to be dealing with data vol-\numes comparable to today’s internet giants such as Facebook, Google, or Amazon to\nstart seeing issues with storing all unique IDs. Because you only need to store an ID\nvalue and not the full message, it’s easy to calculate how big the cache data store will\nbe if you have an idea of how many unique IDs your system needs to be able to handle.\nFor example, if you are using 16-byte UUIDs as your unique identifiers, you only need\n~15 GB of storage to store a billion of them. This is far below the capacity that a mod-\nern database can handle. \n One of the biggest challenges that prevents people from adopting this approach is\nthe fact that it does introduce a separate data store. This data store not only needs to\nbe highly performant, but also needs to be highly available and avoid any type of data\nloss. If your ID data store is down, your whole real-time processing pipeline will halt. If\nyou had to implement something like this in a data center, the engineering and oper-\national overhead could be significant. \n Fortunately, we are only concerned with cloud data platforms in this book. Today\nall cloud vendors provide fully managed services for both fast key/value stores (Azure\nCosmos DB, Google Cloud Bigtable, AWS DynamoDB) and various types of managed\nrelational database services. It’s relatively easy to configure these services to be highly\navailable, and they require minimal operational overhead. \n{user_id:1,name:“Alice”}\n{user_id:3, name:“John”}\n{user_id:2, name:“Bob”}\n{user_id:1,name:“Alice”}\nA data consumer first checks if an ID already exists\nin the cache. If it doesn’t, it processes the message\nand saves it’s ID to the cache. If the ID already exists,\nthe message is marked as a duplicate and is not\nprocessed or saved.\n{user_id:4,name:“Helen”} {user_id:1,name:“Alice”}\n{user_id:3, name:“John”}\n{user_id:2, name:“Bob”}\n{user_id:1,name:“Alice”}\nuser_id=1,\nuser_id=2,\nuser_id=3\nA fast data store is like\na key/value store, and\nan RDBMS can be used\nto cache unique IDs.\nFigure 6.20\nIf we can store all unique IDs that we have already processed in a separate data store, then we can \ncheck if a given message has been processed before.\n",
      "content_length": 2748,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "185\nCommon data transformations in real time\n There is another approach to data deduplication in real time. This approach calls\nfor not performing any data deduplication in real time and instead relying on a desti-\nnation data warehouse (which allows for reading a snapshot of data at a given point in\ntime). This may sound like a naive approach to the problem, given the complexities\nwe have just described, but it works well if your primary use case is real-time data\ningestion and not necessarily real-time data analytics. \n If you are only concerned with delivering real-time data into the warehouse as fast\nas possible and then allow end users to perform ad hoc or scheduled analytics on the\nwarehouse, then from the perspective of implementation complexity, it is easier to\nallow duplicates through in the real-time pipeline. Then, produce a deduplicated set\nby either running a batch deduplication job in the data lake layer or performing simi-\nlar operations in the data warehouse itself. \n This approach basically pushes the problem of detecting duplicates from an\nunbounded real-time stream to a bounded system such as a batch pipeline or a cloud\ndata warehouse (table 6.3). The main limitation of this approach, besides the fact that\nit only works for real-time data ingestion, is that if you are dealing with large data sets,\nrunning a batch deduplication job can take a long time or require significant com-\npute resources. In this case, you can do a side-by-side cost comparison of the solution\nthat caches unique IDs versus the batch deduplication approach.  \n \n \nTable 6.3\nComparing ways to eliminate duplicates in real-time streaming\nNeed\nConsider\nPossible negatives\nPerform real-time analytics \nand make sure data is dedu-\nplicated at the beginning of \nthe pipeline.\nUse windowing to group messages \ntogether using a timestamp, and detect \nduplicates inside the group.\nTime windows have to be of a fixed \nsize. Cloud providers always have some \nrestrictions and limitations on the size \nof windows. There is always a \npossibility that a duplicate will arrive \noutside of a given window, so this \napproach doesn’t guarantee full \ndeduplication. \nCache unique IDs in a fast data store and \nhave the data consumer first check this \ncache to see if a specific ID has been pro-\ncessed. If a message with a given ID has \nbeen processed before, there will be an \nentry with this ID in the cache, and the \ndata consumer can safely ignore this \nmessage as a duplicate.\nRequires a very fast data store to avoid \nslowing your real-time processing with \nextra checks.\nStoring all unique IDs may become too \nexpensive to maintain or performance \nmay suffer.\nIntroduces a separate data store.\nDeliver real-time data into \nthe warehouse as fast as \npossible, and then allow end \nusers to perform ad hoc or \nscheduled analytics on the \nwarehouse.\nAllow duplicates to move through the \nreal-time pipeline, and then produce a \ndeduplicated set by either running a \nbatch deduplication job in the data lake \nlayer or performing similar operations in \nthe data warehouse\nRunning a batch deduplication job can \ntake a long time or require significant \ncompute resources.\n",
      "content_length": 3168,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "186\nCHAPTER 6\nReal-time data processing and analytics\n6.5.3\nConverting message formats in real-time pipelines\nIn chapter 5 we described converting files from the original format to a binary format\nsuch as Avro and Parquet. We suggested that Avro be used as a primary file format for\nthe staging area, where multiple data processing jobs may need to access it. Parquet,\nbeing a column-oriented store, is better suited for production storage areas, where\ncomplex analytics and fast performance is required. \n Do the same ideas apply to real-time systems? What would be an equivalent of a file\nformat in this case? Real-time systems don’t operate with files. Instead, they operate\nwith individual messages. Producers write messages to the real-time system and con-\nsumers read those messages. All existing cloud real-time systems are agnostic to the\nmessage content. For a system itself, a message is just a collection of bytes, and it’s up\nto data producers and consumers to agree on how to interpret those bytes into some-\nthing meaningful.\n Making sure that a data producer and a data consumer agree on the given message\nschema is very important in the real-time system. While batch-processing frameworks\nsuch as Spark can scan large volumes of data and try to infer schema, in real-time sys-\ntems, the processing context is typically a single message, and inferring schema can-\nnot be efficiently implemented. We will discuss schema management and ways for\nproducers and consumers to agree on message schemas in detail in chapters 7 and 8. \n In this chapter, we will focus on the performance aspect of different message for-\nmats. We often see developers choosing JSON as a format for their messages. It is not\nsurprising, given the format’s popularity and the fact that it’s easy to implement.\nBesides the fact that JSON doesn’t provide any schema-management capabilities, this\nformat is less than optimal from a performance point of view.\n We saw earlier in this chapter that real-time systems are distributed systems that\nactively send data over the network; data producers and data consumers can be\nlocated on the opposite sides of the globe; and data replication between different\nmachines also happens over the network. To get optimal performance given these fac-\ntors, each individual message size should be as small as possible. JSON as a message\nformat doesn’t make keeping message size small an easy task. The format is very ver-\nbose and has to include attribute names and values in plain text. For large messages\nwith several dozens of attributes or complex nested data structures, this overhead\nExercise 6.3\nWhat is the primary cause of duplicate data in real-time systems?\n1\nReal-time storage is fast, but unreliable. Sometimes data needs to be written\ntwice to ensure persistence.\n2\nAll duplicates are caused by bugs in the producer’s code.\n3\nDuplicates can be caused by consumers’ or producers’ failures as well as\nfailures in the underlying components, such as the network. \n",
      "content_length": 2993,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "187\nCommon data transformations in real time\nbecomes significant. Just to be clear, we are still discussing message sizes in the kilo-\nbytes range, but when multiplied by a number of messages, these sizes will become sig-\nnificant. You can use compression to reduce the size of JSON messages, but\ncompression algorithms perform better when they have a lot of “similar” data to com-\npress. Compressing a single message will require more processing and may not pro-\nvide expected size reduction.\n Using a binary format for real-time messages, such as Apache Avro, provides the\nsame benefits as batch pipelines. Binary formats are smaller in size to corresponding\nplain-text formats. Formats such as Avro also allow you to send an Avro-encoded mes-\nsage without sending a schema alongside them. This minimizes the message size even\nfurther, but requires a metadata layer to store the schema itself, so data consumers can\ndecode the binary data. We will talk about options for implementing this schema\nexchange mechanism in chapter 7. \n Do column-oriented formats such as Parquet provide any benefits in real-time\nsystems? The answer here is no. Column-oriented formats only provide optimizations\nfor workloads where you need to scan several columns from a large data set. Real-time\nsystems mostly operate on one message at a time and won’t benefit from column-\noriented optimizations.\n6.5.4\nReal-time data quality checks\nImplementing data quality checks that inspect a single message at a time in the real-\ntime system is a relatively straightforward task. We need to first define the set of rules\nthat would allow us to determine whether a message is “good” or “bad.” This is actu-\nally the hard part! If you are dealing with many different data sources that are con-\ntrolled by different teams, then coming up with a common set of rules is a challenging\norganizational task. \n Once you have the rules, you can use the message-routing job idea that we\ndescribed earlier in this chapter. You will implement a real-time processing job that\nwill inspect all incoming data from a landing topic, apply the rules and, based on the\nresult, publish the message either into a downstream staging topic or into a failed\ntopic, as shown in figure 6.21. In this example, we have a check for data in a source\nORDERS that says that an order_total value cannot be negative.\n The way you will actually implement the checks will depend on the cloud real-time\nsystem that you are using for your data platform. Many support a SQL-like language to\nanalyze the content of a given message, which makes it easy to implement checks that\noperate on a message-attribute level. We will look into various cloud real-time systems\nand their capabilities later in this chapter. \n Not all data quality checks can be implemented by just looking at a single message\nat a time. For example, in our retail system, we may want to implement a check that\nverifies that no more than 10% of all orders that happened in the last hour have a\nCANCELLED type. Maybe in our case, this usually indicates an issue with POS termi-\nnals or network issues. \n",
      "content_length": 3104,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "188\nCHAPTER 6\nReal-time data processing and analytics\nObviously, we can’t implement something like that just by looking at each individual\nmessage. If you recall our earlier discussion about data deduplication, this is where\ntime-based windows come into play again. You can use windowing functions that your\nreal-time system supports to group all messages from the “orders” source that came in\nthe last hour. Then you can count all orders in that window and compare it to orders\nthat have a CANCELLED type. \n Limitations similar to those in our data deduplication discussion apply. Windows\ncan’t be of an indefinite size, and usually cloud providers impose much stricter limita-\ntions on windows sizes. If your quality check requires looking at the last week, month,\nor year of data, you should consider implementing it in the batch-processing layer by\nusing the data that has been saved to the cloud storage from the real-time system.\n6.5.5\nCombining batch and real-time data\nIt’s unlikely that your organization will only need to deal with real-time data. Existing\nlegacy systems or third-party data sources often only allow you to extract data in batch\nmode in the form of files. A natural question that arises is, what if we need to combine\na batch data source with a real-time stream? \n Going back to our POS example, imagine that we have detailed information about\nall offline stores extracted from an existing Enterprise Resource Planning (ERP) sys-\ntem as a simple CSV file. This data doesn’t change often, and there is little reason to\n{\nsource: \"ORDERS\",\norder_id: 3,\ntype: \"PURCHASE\",\norder_total: -78,\nstore_id: 2435\n}\nMessages that don’t pass the\ncheck are saved to the failed\ntopic.\nA data quality job\nfetches required rules\nusing the message\nsource name.\nAn operational metadata\nlayer stores all data\nquality check rules.\nPOS\nRDBMS\nIngestion\nlayer\nLanding\nData\nquality job\nStaging\nFailed\nOperational\nmetadata\nlayer\nFigure 6.21\nReal-time data quality checks utilize the metadata layer to fetch the right checks for a \ngiven message. Messages that do not pass the check are saved to a failed topic.\n",
      "content_length": 2112,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "189\nCommon data transformations in real time\ningest it as a real-time stream. Our real-time stream from the POS system only includes\nstore_id—a unique number identifying each store. Our business users would like to\nsee full store information alongside POS data, so we need to figure out a way to com-\nbine the two data sets. \n Our store information CSV file is processed by the batch-processing layer and is\nstored in the cloud storage. We can adjust our real-time processing job for the POS\nsales transactions to read this file when the job starts, cache it in memory, and then\nuse it as a lookup dictionary to match store_id with expanded store information. This\nis shown in figure 6.22.\nThe main limitation of this approach is that the batch data set has to be small enough\nto fit into the memory of the machine on which the real-time processing job runs.\nReal-time jobs usually run on many virtual machines in parallel to deal with the scale\nof the incoming data stream. This means that batch data has to be copied into mem-\nory on each of those machines to be available for lookups.\n With existing cloud real-time services, it’s sometimes hard to tell what the exact\ncharacteristics are of the virtual machines your job is running on. Usually, batch data\nsets used for lookups are in the several-MBs size range, which would easily fit into\nThe message that comes out\nof a POS system doesn’t\ncontain any information\nabout the store, except\na store_id.\nA data enrichment\njob can now\ncombine two data\nsets using the store_id\nattribute.\nThe data enrichment job can read and\ncache the store information data set.\nCloud storage\ncontains another\ndata set with\ndetailed store\ninformation that\nis ingested from\nan FTP server\nevery day.\n{\nsource: \"ORDERS\",\norder_id: 3,\ntype: \"PURCHASE\",\norder_total: -78,\nstore_id: 2435\n}\n{store_id:2435, address:\"Barrington st…\", …},\n{store_id:2436, address:\"Agricola st..\", …},\n{store_id:2437, address:\"Lower Water st,..\" ,...}\nCloud storage layer\nData\nenrichment\njob\nProduction\nStaging\n{\nsource: \"ORDERS\",\norder_id: 3,\ntype: \"PURCHASE\",\norder_total: -78,\nstore_id: 2435,\nstore_location:\n\"Barrington st,...\"\n}\nFigure 6.22\nReal-time processing jobs can use batch data from cloud storage as a lookup dictionary to \nenrich the real-time stream.\n",
      "content_length": 2273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "190\nCHAPTER 6\nReal-time data processing and analytics\nmemory in most configurations. You will need to refer to specific real-time cloud ser-\nvice documentation to understand their exact limitations. \n If a batch data set is too big to fit into memory, then you can always fall back to per-\nforming the join of the two data sets either in the data lake using batch-processing\ntools such as Spark or in the cloud warehouse by providing the end users a pre-joined\ntable or a view. This approach, of course, works only if your primary use case is real-\ntime data ingestion. \n6.6\nCloud services for real-time data processing\nSo far we have been describing generic real-time processing concepts that can be\napplied to pretty much any real-time system that exists today. In this section, we give a\nbrief overview of what real-time services are offered by three major public cloud pro-\nviders: AWS, Azure, and Google Cloud. We should warn you that cloud vendors\nrelease new features and make changes to existing systems every six months or so,\nmeaning that some of the behaviors that we will describe in this section may change in\nthe future. We encourage you to always refer to cloud vendor documentation for the\nup-to-date details. \n Each cloud vendor provides a pair of related services for the real-time processing:\none that implements real-time storage and maps to the fast storage layer in our archi-\ntecture and another one that implements the real-time processing. These services are\nusually tightly integrated and, in most cases, it makes sense to use them together (see\ntable 6.4). In some cases it may be possible to use a cloud vendor’s real-time process-\ning services with an existing real-time storage service such as Kafka. \n6.6.1\nAWS real-time processing services\nAWS offers two services in the real-time processing space: Kinesis Data Streams is a\nreal-time storage service, and Kinesis Data Analytics is a real-time processing service.\n Kinesis Data Streams consists of separate Data Streams, which are equivalent to\ntopics, in our terminology. Each Data Stream contains Data Records. These are\nindividual messages that producers write to the stream. Data Stream Applications, or\nconsumers, are applications that read messages from Data Streams. Data Streams\nApplications utilize Kinesis Client Library (currently available for Java, Node.js, .NET,\nPython, and Ruby languages). The client library hides a lot of complexity such as\ntracking sequence numbers of each processed Data Record and storing them in\nTable 6.4\nCloud services for real-time storage and processing\nReal-time storage\nReal-time processing\nAWS\nKinesis Data Streams\nKinesis Data Analytics\nGoogle Cloud\nPub/Sub\nDataflow\nAzure\nEvent Hubs\nStream Analytics\n",
      "content_length": 2730,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "191\nCloud services for real-time data processing\nDynamoDB, an AWS fast key/value store. This protects consumers from reprocessing\nlarge volumes of data in case of a crash, but all the limitations that we have described\nin this chapter still apply—if you save the processed record sequence numbers for each\nrecord, you will quickly run into performance limitations. If you only checkpoint for a\nbatch of records, there is always a possibility of processing the same record twice in case\nof failures. \n AWS also provides a service called Kinesis Data Firehouse, which is a managed\nData Streams Application. Firehouse is used to read data from Data Streams and save\nto a multitude of destination systems, or sinks. It is often used as a data ingestion\nmechanism, and we talked about in chapter 4. \n Kinesis Data Streams has a concept of shards (partitions, in our terminology). You\nneed to specify how many shards you want for a particular Data Stream. Shards are\nthe main unit of scalability for Kinesis Data Streams, with one shard only supporting a\nlimited throughput. Currently, each shard supports 1 MB/sec of data writes to the\nshard and 2 MB/sec of data reads from the shard. Kinesis Data Streams also supports\na resharding operation—you can increase or decrease the number of shards in\na stream.\n Kinesis Data Streams doesn’t impose any limitations on the format of the Data\nRecord, but like many other services, there is a limit on the Data Record size. At the\ntime of this writing, it is 1 MB. As with many other real-time cloud services, Kinesis\nData Streams has limitations on how long a Data Record can be stored in a Data\nStream. The default is a 24-hour retention period. This means that if you haven’t pro-\ncessed a record within this time, it will be purged and no longer available to you. You\ncan increase the retention period to a maximum of seven days, at an additional cost.\nAs you can see, fast storage cannot be used for archival purposes. That is why our\narchitectures suggest you save real-time data to a long-term regular cloud storage. \n Kinesis Data Streams Analytics is a real-time processing engine that allows you to\nsubmit real-time data processing jobs that will read the data from Kinesis Data\nStreams. Data Streams Analytics is a fully managed service—you don’t need to provi-\nsion and configure virtual machines to run your real-time job. All this is taken care of\nby the service itself. \n Kinesis Data Streams Analytics supports two APIs: SQL and Java. SQL allows you to\nuse a familiar SQL syntax to run interactive and ad hoc queries against your real-time\ndata streams. The SQL API only supports messages in CSV or JSON formats, which is a\nsignificant limitation because for performance and schema-management reasons, you\nwant to use a binary format for messages. \n The Java API is based on a popular open source, real-time processing framework\ncalled Apache Flink and provides more flexibility in terms of message formats and\ncontrol over job behavior, such as how often to checkpoint processed messages back\nto Kinesis  and the like.\n Kinesis Data Streams Analytics doesn’t provide any built-in mechanism for data\ndeduplication, and AWS’s recommendation is to cache unique message IDs and look\n",
      "content_length": 3232,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "192\nCHAPTER 6\nReal-time data processing and analytics\nthem up during processing. We described this approach earlier in the chapter.\nAnother deduplication recommendation from AWS is to rely on data destinations to\nperform the deduplication. As we mentioned earlier, this approach works only if your\nprimary use case is real-time data ingestion. \n6.6.2\nGoogle Cloud real-time processing services\nGoogle Cloud’s approach to the real-time processing is slightly different from other\ncloud providers. Google Cloud provides a more managed experience when it comes\nto the real-time storage and more advanced real-time processing capabilities in com-\nparison to what we see in other cloud providers today.\n Google Cloud’s real-time storage service is called Cloud Pub/Sub (publish and\nsubscribe). Cloud Pub/Sub uses the concepts of topics and subscriptions. Topics are\nused as a namespace to group messages together. Topics in Cloud Pub/Sub have less\nimportance than Data Streams in AWS Kinesis or topics in Apache Kafka, because\nCloud Pub/Sub doesn’t explicitly partition data into topics. Most likely, Google Cloud\ndoes partitioning behind the scenes, but from a data producer’s perspective, it is com-\npletely transparent. Producers in Cloud Pub/Sub just write messages to a topic with-\nout having to specify a partitioning key or worry about how many partitions a topic\nshould have.\n Cloud Pub/Sub data consumers need to have a subscription attached to one or\nmore topics to start receiving data. A single topic can have multiple subscriptions if you\nhave multiple different data consumers. You can also have a subscription that combines\ndata from two or more topics. This capability doesn’t exist in other cloud services and\nopens up interesting possibilities when implementing the message-routing job that we\ndescribed previously. \n Subscriptions are also a mechanism for Cloud Pub/Sub to scale. One subscription\nprovides 1 MB/sec data ingestion and 2 MB/sec data reads. Individual message size\nhas a maximum size limit of 10 MB, and maximum data retention is seven days. \n Cloud Pub/Sub doesn’t require data consumers to checkpoint message offsets,\nwhich simplifies consumer code. Since there are no explicit offsets, Cloud Pub/Sub\nhas to introduce different concepts for consumers to be able to replay messages. Cloud\nPub/Sub supports creating snapshots of a subscription, which becomes like a frozen\ncopy of that subscription at a given point in time. Data consumers can replay all mes-\nsages that were not consumed at the time a snapshot was created and all new messages\nthat have arrived since then. There is an extra cost to store snapshots, and also there is\na limit of 5,000 snapshots per Google Cloud project. This limits snapshots’ usability for\nfailure recovery, because you can’t snapshot too frequently. Google Cloud suggests\nusing snapshots mostly for planned outages, like creating a snapshot before you roll\nout an update to your real-time processing code. In addition to snapshots, data con-\nsumers can start consuming messages from a given timestamp, but this method is not\nprecise and most likely will result in the same messages being processed twice. \n",
      "content_length": 3171,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "193\nCloud services for real-time data processing\n Cloud Pub/Sub client libraries are simpler in comparison to AWS, because there\nare fewer configuration options and no need to perform checkpoints. Currently,\nCloud Pub/Sub client libraries are available for C#, Go, Java, Node.js, PHP, Python,\nand Ruby. \n Google Cloud’s real-time data processing service is called Cloud Dataflow. It’s a\nfully managed service that uses the open source Apache Beam API. What’s interesting\nabout Cloud Dataflow is that it supports both batch and real-time processing. This\nmakes it possible to unify two layers in our cloud architecture to use the same process-\ning engine. Cloud Dataflow supports SQL as well as Java and Python. SQL only sup-\nports messages in JSON format and provides less flexibility than Python and Java, but\ncan be useful for ad hoc analytics or simple ETL pipelines.\n Cloud Dataflow has support for various data sources and data sinks (destinations). It\ncan read from Cloud Pub/Sub, files on Google Cloud Storage, and other sources. Data\nflow can write to different Cloud Pub/Sub topics, Google BigQuery, or files on GCS. It\nis possible to implement your own data sources and sinks using the Apache Beam API.\n Cloud Dataflow has built-in capabilities to deduplicate data that comes from Cloud\nPub/Sub. Cloud Dataflow handles any internal Cloud Pub/Sub failures that may\nresult in data duplicates automatically. It can also deal with duplicates that were\ncaused by data producers sending the same message multiple times, if a message has a\nunique ID. The latter deduplication capabilities will only work if a duplicate message\narrives within a 10-minute window. For other scenarios, it is possible to implement dif-\nferent deduplication strategies that we described earlier in this chapter. \n6.6.3\nAzure real-time processing services\nAzure Event Hubs is a real-time storage service with some interesting characteristics.\nWhat’s interesting about Event Hubs is that it supports several protocols for data pro-\nducers: AMQP, Kafka, and custom HTTPS. Both AMQP and Kafka are open industry\nstandards with a number of clients that support these protocols available in open\nsource format. For an end user, this means fewer rewrites of their data producing or\nconsuming applications during cloud migrations if you are already using AMQP or\nKafka. \n The Event Hubs service consists of multiple namespaces that host multiple hubs\n(in the AMQP case) or Kafka topics. Event Hubs uses explicit data partitioning, and\neach topic can have between 2 and 32 (you can get more after talking to Azure sup-\nport) partitions. You need to configure the number of partitions up front and, unlike\nAWS Kinesis, this number cannot be changed later, so some careful planning is\nrequired. Partitions are the main scalability units in Event Hubs. Each partition gets a\nsingle throughput unit that includes 1 MB/sec or 1,000 messages/sec ingest rate and\n2 MB/sec or 4,096 messages/sec data consumption rate. You can prepurchase up to\n20 throughput units per Namespace, which Event Hubs will automatically assign\namong all existing partitions. This way, you can make sure to provide guaranteed per-\nformance for your real-time system. Event Hubs has a message size limit of 1 MB.\n",
      "content_length": 3253,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "194\nCHAPTER 6\nReal-time data processing and analytics\n Like AWS Kinesis or Kafka, Event Hubs exposes an offset number for each message\nin a partition. This number can be used to replay messages from the topic. Keep in\nmind that data retention for Event Hubs is seven days maximum. What’s different in\nEvent Hubs is that data consumers are responsible for storing offsets and managing\nthe checkpoint process themselves. In comparison, AWS does this by using a Dyna-\nmoDB key/value store, and Kafka stores checkpoint offsets in Kafka itself. Event Hubs\nclient libraries come with support for an Azure Blob Storage offset store, but if you\nwant to use a faster store, you need to implement it yourself or look for open source\noptions. As with other real-time storages, the frequency of checkpoints for data con-\nsumers is a trade-off between reliability and performance. Event Hubs client libraries\nare available for .NET and Python, but open source implementations for other lan-\nguages exist as well. \n Event Hubs has a supporting service called Event Hubs Capture that allows you to\nsave data from the real-time storage into Azure Blob Storage on a regular basis. This is\nsimilar to Kinesis Firehose, but only supports Azure Blob Storage currently. \n When it comes to real-time data processing, Azure provides the Azure Stream\nAnalytics service. This is a fully managed service, similar in this sense to what we have\nseen in AWS and Google Cloud. Azure Stream Analytics only supports SQL syntax and\ndoesn’t offer an advanced API in any other language. It is possible to extend SQL\nqueries with user-defined functions in C# and JavaScript. Azure Stream Analytics also\ncomes with lots of SQL extensions geared towards real-time analytics, such as\nwindowing support, window joins, lookups in dictionary data sets, etc. Azure Stream\nAnalytics supports a number of destinations it can write results to, including Azure\nSynapse, Azure SQL Database, and others. \n If you need more controls over your real-time processing jobs or want to use more\nsophisticated code organization patterns than what SQL offers, you can use the\nApache Spark Streaming API (remember the caveat about micro-batching from ear-\nlier in this chapter). The Azure Databricks service provides full integration with Event\nHubs as well and can be used to host your Spark jobs in a managed environment. \n Event Hubs and Azure Stream Analytics don’t provide any built-in mechanisms for\ndata deduplication. Azure’s recommendation is to use a destination, like Azure Syn-\napse, to perform deduplication. \nSummary\nThe processing layer is the heart of the data platform implementation, the layer\nwhere all the required business logic is applied, and the layer where all data val-\nidations and data transformations take place.\nWhen people use the terms “real-time” or “streaming” in the context of a data\nplatform, it can mean different things to different people, and it is relevant in\ntwo layers of a data platform—the ingestion layer and the processing layer. Real-\ntime or streaming ingestion takes place when you have a pipeline that streams\ndata, one message at a time, from a source into a destination such as storage or\n",
      "content_length": 3186,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "195\nExercise answers\nthe data warehouse or both. The terms “real-time data analytics” or “real-time\nprocessing,” on the other hand, are usually reserved for the application of com-\nplex computations on streaming data.\nChoosing between real-time ingestion, real-time processing, or both depends\non the requirements. If the only requirement for real time is that you must\nmake data available for analysis as fast as possible, but the analysis itself happens\nin an ad hoc manner, then real-time ingestion is what you should be imple-\nmenting. If, on the other hand, the requirement is to have the analysis itself\ndone in real time to be passed on to another system for action, then real-time\ningestion and real-time processing will be required.\nWhile batch systems work with files, and files, in their turn, consist of individual\nrows that contain data, real-time systems operate on the level of an individual\nrow or a message. A message is basically a piece of data that can be written and\nthen read from the real-time storage. Messages are organized into topics, which\nare similar to folders on a file system. Messages are written into the real-time\nstorage by producers and are read and processed by consumers. Both producers\nand consumers, in this context, are applications of some sort. Producers write\nmessages into topics, and consumers read from topics.\nReal-time systems (or any modern data processing system, really) can’t just run\non a single physical or virtual machine. They have to be distributed and run on\na cluster of machines to provide the scalability we require today.\nReal-time processing comes with some unique challenges and restrictions. Data\nduplicates are commonplace, and while there are several methods of dealing\nwith them, all of them have negative implications, from additional costs to per-\nformance impacts.\nWhen it comes to common message formats, using a binary format such as\nApache Avro for real-time messages provides the same benefits as batch pipe-\nlines. Binary formats are smaller in size than corresponding plain-text formats.\nFormats such as Avro also allow you to send an Avro-encoded message without\nsending a schema alongside them. This minimizes the message size even fur-\nther but requires a metadata layer to store the schema itself, so data consumers\ncan decode the binary data. \nThe Apache Kafka real-time system is a very popular, open source option for\ntooling, but each of the major public cloud providers also have real-time pro-\ncessing services.\n6.7\nExercise answers\nExercise 6.1:\n 2—Real-time data ingestion and real-time processing. You need to ingest data from\na user’s phone in real time and also have a real-time pipeline that compares users to\ntheir friends. Even a couple of minutes of delay will likely make the data inaccurate.\n",
      "content_length": 2798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "196\nCHAPTER 6\nReal-time data processing and analytics\nExercise 6.2:\n 1—To provide a reliable way for consumers to resume processing after a crash or a\nrestart. \nExercise 6.3:\n 3—Duplicates can be caused by a consumer's or a producer’s failure, as well as fail-\nures in the underlying components, such as the network. Failure of producers can\ncause the same data to be written two (or more) times. Consumer failure can cause\nthe same data to be processed more than once. Network failures between producers\nand consumers can cause issues with saving the offset of a message and cause dupli-\ncate data processing. \n",
      "content_length": 612,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "197\nMetadata\n layer architecture\nIn this chapter, we’ll help you get a clear understanding of what we mean by data plat-\nform internal metadata and why it is important to the operation of a data platform.\n We’ll cover the difference between configuration and activity metadata and how\neach can be used, using examples of a data platform with growing complexity. We\nThis chapter covers\nUnderstanding data platform technical metadata \nvs. business metadata\nLeveraging metadata to simplify data platform \nmanagement\nArchitecting the optimal metadata layer\nDesigning a metadata model with multiple \ndomains\nUnderstanding metadata layer implementation \noptions\nEvaluating commercial and open source \nmetadata options\n",
      "content_length": 718,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "198\nCHAPTER 7\nMetadata layer architecture\nwill show why the metadata layer should become the primary interface for data engi-\nneers and advanced data users.\n We will describe a generic metadata model with four main domains—pipeline\nmetadata, data quality checks, pipeline activity, and schema registry—a model that we\nhave found to work across different organizations, focusing on the aspects of meta-\ndata that we found to be more or less universal. \n After the conversation about how to structure your cloud data platform metadata,\nwhich main metadata entities you should have, and what some of the common attributes\nof these entities are, we will explore how to implement it in a real project, focusing on\nthree metadata layer implementation options of increasing sophistication, starting with\nthe simplest option and providing advice on when each one might be needed.\n Last, we will do a high-level overview of existing open source and cloud services\naround metadata and how they differ from the model we have described.\n7.1\nWhat we mean by metadata\nSimply put, metadata is “a set of data that describes and gives information about other\ndata.” In the case of data platforms and data management, metadata is information\nthat helps us manage our own data better. There are two types of metadata used in\ndata platforms: business metadata and data platform internal metadata, sometimes\ncalled pipeline metadata.\n7.1.1\nBusiness metadata \nTypically, when people talk about metadata in the context of data management they\nusually mean business metadata. Business metadata is information, or “tags” that\ndescribe the organizational source of the data (Sales, HR, Engineering), the owner of\nthe data, the date and time of creation, file size, purpose of the data, rating of the qual-\nity of the data set, etc. Metadata is data that is not in the data itself, and it becomes even\nmore important when files are merged, because context or “assumptions” may be lost\nin the merger. For example, our US sales data may come from an RDBMS, and our\nCanadian sales data may come from a CSV file. These data sets might not have a sepa-\nrate column that describes the country where the sales transaction happened because\nthe applications just assumed that the context for this RDBMS or a CSV file is US or\nCanadian, respectively. But when placed into a single data platform, this context no lon-\nger exists, so we need to add some metadata to be able to distinguish between the two.\n Business metadata’s primary role is to facilitate data discovery for end users. When\na business user wants to find specific data sets or reports, they don’t want to search\nthrough hundreds of existing data sets: they want to search by typing something like\n“Sales Q1 2020 Canada+US quality=high” and get only the data sets and reports that\nhave these tags assigned. \n For many years in the data management domain, business metadata was the only\nmetadata that people were interested in. This is not surprising, because business\nmetadata and the tools that help manage it allow end users to work more efficiently,\n",
      "content_length": 3080,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "199\nTaking advantage of pipeline metadata\nfacilitating data use. With the advent of data platforms, the number of available data\nsets has increased exponentially, and the challenges associated with data discovery\nand data classification continue to this day.\n Addressing the business metadata management challenge is an array of tools\ndesigned to help organizations implement data tagging and data discovery. Alation,\nCollibra, and Informatica are just some of the commercial metadata software products\nthat specifically address these challenges. In addition, many of the ETL overlay tools,\nincluding Talend and Informatica, come with metadata management capabilities,\noften referred to as a Data Catalog. And of course, the public cloud providers also\noffer services in this space, including Google Cloud Data Catalog, Azure Data Catalog,\nand AWS Glue Data Catalog, which is part of the Glue ETL service. \n7.1.2\nData platform internal metadata or “pipeline metadata”\nThere is another type of metadata that is critical to a well-functioning data platform\nbut doesn’t get as much attention as business metadata. This metadata we call data\nplatform metadata or pipeline metadata. It describes the data pipelines themselves,\nhelping us understand which data sources a pipeline connects to and providing\ndetailed information about these data sources. Among other things, it can also tell us\nwhen the pipeline ran successfully and, if it failed, what error was associated with the\nfailure. It can also tell us things like who introduced a new data quality check that sud-\ndenly caused all the data to be marked as bad.\n This metadata is fundamental to automation, monitoring, configuration manage-\nment, and other operations. Without it, any data platform that has more than a cou-\nple of data sources and active pipelines will quickly become a nightmare to manage\nand operate. \n In this chapter and throughout this book we will be focusing on pipeline metadata,\nthough not because it is more important than business metadata. Both play important\nroles, but they do so at different levels. Data platform internal metadata is more\naligned with the needs of data engineering and data operations domains that are\nfocused on making the platform run smoothly. And while there are many existing\ntools and products and sufficient information available for business metadata manage-\nment, there is little available for data platform internal metadata. \n7.2\nTaking advantage of pipeline metadata\nLet’s imagine a simple, but typical, evolution of a data platform. Our example\nplatform starts with just a single batch data source—a file uploaded into an FTP\nserver. Our pipeline is a simple pass-through pipeline that applies some data quality\nchecks and ingests data into a cloud data warehouse. Figure 7.1 shows this simple one-\npipeline platform.\n This example may appear overly simplified, but we have seen many data platforms\nthat start with simple use cases like this. When you have a single pipeline that does\ningestion, some data quality checks, and maybe some basic data formatting, there is\n",
      "content_length": 3085,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "200\nCHAPTER 7\nMetadata layer architecture\nlittle need for pipeline configuration or pipeline metadata. Everything is in one place\nand is contained in a single code base. It’s easy to update a source location or adjust\nthe source or destination data locations—by updating a single ingestion job. \nNOTE\nWhile in the previous example the pipeline code requires little config-\nuration, you still need to configure the platform infrastructure, such as cloud\nstorage account, data warehouse, etc. You could also collect and use infra-\nstructure metadata, but we are excluding it from pipeline metadata because\nthe infrastructure tends to be stable, while pipeline code usually evolves rap-\nidly—driving the need for metadata. Cloud infrastructure configuration can\nalso utilize many infrastructure-as-code options, which are outside of the\nscope of this book.\nThis data platform is also very easy to monitor and operate for all the same reasons. If\nsomething fails and data doesn’t get into the data warehouse, there is only one place\nto look for issues: that one single pipeline. \n Soon, as is typical in real life, we need to extend our example platform by adding\nanother source. Let’s say it’s another file, but this time on AWS S3 storage. Figure 7.2\nshows how our original data platform changes with the addition of a second source.\nCloud data platform\nA single pipeline\nhandles ingestion,\nbasic data cleanup,\nand loading data\ninto the warehouse.\nCloud data\nwarehouse\n1\nFTP\nFigure 7.1\nA simple one-pipeline \nplatform doesn’t require much \nconfiguration because all logic is \nstored in the ingestion pipeline itself.\nCloud data platform\nPipeline that handles ingestion,\nbasic data cleanup, and loading\ndata into the warehouse\nfor the FTP source\nWhen adding a new source, do we duplicate\nthe first pipeline or try to extend the existing\npipeline to support a new data source?\nCloud data\nwarehouse\n2\n1\nAWS S3\nFTP\nFigure 7.2\nAdding a new \ndata source (2) introduces \noptions—to either create a \nnew pipeline to support the \nnew source or to add more \nlogic into the existing \npipeline.\n",
      "content_length": 2081,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "201\nTaking advantage of pipeline metadata\nAdding another data source such as a batch file forces us to make some tough deci-\nsions. If we want to keep the simplicity of having a single ingestion pipeline, then it\nwould suggest we add more logic into the existing pipeline code that would allow it to\nhandle connecting to AWS S3 storage, copying data over, performing data validations\nthat are relevant to this specific data source, etc. This can work, but what happens\nwhen you add more and more data sources? Some of those additional data sources\nmight be very different from the batch files, and they could even be real-time change\ndata capture streams from a database. This will cause your pipeline code to become\ncomplex and harder to maintain. \n Monitoring and operating this still is simple, but an extended data pipeline starts to\nget tricky. If data from AWS didn’t come through in time, what caused the delay? The\ndelay could be an issue with this specific source, or it could be a faulty connection to\nFTP. It’s difficult to tell the root cause of the problem even in this simple case.\n An alternative approach to adding a second data source could be to duplicate the\noriginal ingestion pipeline and adjust it to work with the new AWS S3 source. In this\ncase, you will need to deal with a different set of issues. You will end up with code\nduplication, which will make changes to your pipelines quite challenging. Think\nabout a scenario where you want to adjust one of the data quality checks because a\nbusiness requirement has changed. You will need to make the adjustment in two\nplaces instead of one. \n Throughout this book, we’ve advocated the use of a “configurable pipeline.” This\nmeans using the same pipeline code to handle data ingestion from similar source types\n(files or databases) or performing common data transformation and data quality check\ntasks. Instead of creating separate unique pipelines for these tasks, with names and loca-\ntions of the data sources hardcoded into them, we need to abstract pipeline configura-\ntion from the pipeline code. This approach will help you deal with the almost inevitable\never-increasing complexity, and it is flexible enough to handle change. Instead of cre-\nating a new pipeline or adding extra logic into an existing pipeline, we just need to\nupdate the pipeline configuration without touching the pipeline code itself. \n If we take our example data platform further, and as shown in figure 7.3, add sev-\neral data transformation pipelines that first apply some transformations on each\nsource separately (numbers 3 and 4) and then join the two sources together before\nloading them into the data warehouse (number 5), you can see the need to better\nmanage emerging complexity by separating actual pipeline implementations from\npipeline configurations—or metadata.\n Adding a metadata layer that stores information about what each pipeline actually\ndoes, where it should read data from, where it should save outputs to, etc., and com-\nbining this with configuration files allows you to make changes to the pipeline’s\nbehavior easily. You can now simply update a specific configuration file in the meta-\ndata layer instead of making changes to the pipeline code. This makes it much easier\nto scale your data platform because instead of updating the actual pipeline code, you\ncan simply update a specific configuration in the metadata layer. \n",
      "content_length": 3404,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "202\nCHAPTER 7\nMetadata layer architecture\nNOTE\nKeep in mind that as seen in chapter 4, implementation details of\ningestion pipelines for different types of data sources can vary significantly.\nThis means that you will most likely end up with different ingestion pipelines\nfor different source types: RDBMSs, flat files, real-time streams, etc. The meta-\ndata layer can still be used to store configuration for all these pipelines, pro-\nviding a unified interface for data platform operators.\nStoring pipeline configurations is not the only role that the metadata layer plays. In\nfact, one of the existing implementations for this layer by tech company LinkedIn is\ncalled “LinkedIn Datahub Project.” This gives you a hint of what responsibilities the\nmetadata layer has. It stores information about where data is coming from, where it is\nsaved into, and how it is being processed: how long it takes to process each step,\nwhether there are any issues with processing, and tons of other useful details. \n In our architecture the metadata layer has three major functions:\nIt is a central storage for all pipeline configurations.\nIt serves as a mechanism to track each pipeline’s execution status and various\nstatistics about pipelines (activity).\nIt serves as a schema repository.\nWe will talk in detail about schema repositories in chapter 8, so this chapter will focus\non the first two items and how to maximize your use of metadata. \n Simply put, the metadata layer should become the primary interface for data engi-\nneers and advanced data users to use to interact with the data platform. Following are\nsome examples of how pipeline metadata can be useful to users of a data platform.\nCloud data platform\nCloud data\nwarehouse\n2\n1\nAWS S3\n3\n4\n5\n● Which sources need to be ingested?\n● How do we connect to them?\n● Which quality checks need to be\n applied to which sources?\n● Which transformations require which\n data as inputs?\n● Where do transformations save their\n outputs?\nAs the number of pipeline increases, we\nneed to abstract pipeline configurations\nfrom pipeline implementations.\nIngestion\nand data\nquality\nData\ntransformations\nMetadata layer\nFTP\nFigure 7.3\nAdding more complexity, i.e., more sources and more transformations, drives the \nneed for a new layer to hold pipeline configurations.\n",
      "content_length": 2301,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "203\nMetadata model\n If a data engineer needs to update the location of a source file on an FTP server,\nthey can update a configuration for this source in the metadata layer instead of\ndirectly updating the pipeline code. \n If a data engineer receives an alert saying that a specific transformation pipeline\nfailed (activity), they should not have to dig through dozens of log files located on dif-\nferent machines; they should be able to request the latest pipeline status from the\nmetadata layer, which should have detailed status information. The alerting mecha-\nnism itself can be built on top of this layer. Failures, data ingestion rates, number of\nduplicates in the real-time stream—all this information is recorded in the metadata\nlayer and allows you to build all types of monitoring and alerting. \n Data users who would like to make the most of the data lake capabilities and\nmaybe even construct their own transformation pipelines should not wonder where a\nparticular pipeline gets its inputs from and where it stores its outputs. All this data is in\none place—the metadata layer.\n7.3\nMetadata model\nOne of the key challenges you will face is that there are no industry standards for a\ngood metadata model. If you are designing an operational database using relational\ntechnology, you will find a lot of available information about how to best organize data\nin your tables, pros and cons of different approaches, etc. Nothing like this exists in\nthe metadata domain today. There are works by some of the large internet companies\nsuch as LinkedIn and Lyft that are available in open source today and which we will\ndiscuss later in this chapter, but even those available metadata implementations are\nstrongly tied to the specifics of a particular company’s data engineering approach,\nbusiness domain, etc. None of these are universal enough to be easily applied by many\norganizations. \n In this section, we will describe a generic metadata model, one that we have found\nto work across different organizations. The model is intended to be flexible, which\nmeans we will only describe aspects of metadata that we have found to be more or less\nuniversal. Once you get the general idea about how to organize your data platform\nmetadata, you will need to decide whether you need to extend this model to fit your\nunique organizational needs.\nExercise 7.1\nWhich of the following is not a function of the metadata layer in our design?\n1\nProvide schema repository capabilities for all data sources.\n2\nProvide easy data search capabilities to end users.\n3\nProvide pipeline configuration storage capabilities.\n4\nProvide pipeline activity-tracking capabilities.\n",
      "content_length": 2657,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "204\nCHAPTER 7\nMetadata layer architecture\n7.3.1\nMetadata domains\nWe will group different metadata items into four main domains: Pipeline Metadata\n(configuration), Pipeline Activities (activity), Data Quality Checks (configuration),\nand Schema Registry (configuration). Figure 7.4 shows these domains and the rela-\ntionship among them.\nPipeline Metadata contains information about all existing data sources and data desti-\nnations as well as ingestion and transformation pipelines that are configured in the\nplatform. Data sources and data destinations store their schemas in the Schema Regis-\ntry, which we will cover in detail in chapter 8. Additionally, both ingestion and transfor-\nmation pipelines can apply different data quality checks, and information about these\nchecks is stored in the Data Quality Checks domain. Finally, each pipeline’s execution\nis tracked in the Pipeline Activities domain, including success or failure status, dura-\ntions, and other statistics such as the amount of data read/written, etc. If you think\nabout data platform pipelines as applications, then Pipeline Metadata is your applica-\ntion configuration and Pipeline Activities are your application log files and metrics.\nPIPELINE METADATA\nPipeline metadata is at the core of our architecture. It contains information about\nwhere data is coming from and how exactly it is being processed. It is easier to under-\nstand the key components of the Pipeline Metadata domain if we look at an example. \n Imagine that we are building a data platform to ingest and process data from two\nsources containing sales data (one a file and the other an RDBMS) and one source\ncontaining HR data (an API). We’d like to keep sales and HR data separate in our\nplatform so it is easier to control access permissions to this data.\n Figure 7.5 shows a model for the pipeline metadata associated with our example. At\nthe top level of the pipeline metadata is a Namespace. Namespaces can be used to log-\nically separate pipelines, data sources, and other elements of our data platform. In fig-\nure 7.5 we have two namespaces: Sales and HR. Each namespace has its own data sources\nand pipelines. Namespaces can participate in the naming of the pipelines themselves,\nData Quality\nChecks\nPipeline Activities\nSchema Registry\nStores information\nabout schemas for\nall data sources\nand data\ndestinations\nPipeline\nMetadata\nCaptures information and\nstatistics about pipeline\nexecution\nCentral storage for all pipeline configurations\nFigure 7.4\nMain \nmetadata domains and \ntheir interrelationships\n",
      "content_length": 2549,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "205\nMetadata model\nnaming of the folders in the cloud storage, topics in the real-time systems, etc. This\nallows you to assign permissions to the cloud resources based on the namespace. \n In our metadata data model, a namespace has only a few required attributes:\nID—A unique identifier for the namespace\nName—A short name that can participate in the naming of the cloud resources\nDescription—A more detailed description of what this namespace is about\nCreated Timestamp—Date and time of when the namespace was created\nLast Modified Timestamp—Date and time of when the namespace was last modified\nA Pipeline metadata object describes all your ingestion and transformation pipelines.\nFor example, we can have a “Ingest Sales RDBMS” pipeline or “Sales quarterly report”\npipeline. Pipelines take data sources as an input and produce data destinations as an\noutput. In our example metadata model, we have outputs of the “Ingest sales RDBMS”\npipeline used as an inputs to the “Sales quarterly report” pipeline (figure 7.6).\nNOTE While figure 7.6 shows that the “Sales quarterly report” pipeline uses\nwarehouse tables as an input, it doesn’t necessarily mean the pipeline physi-\ncally will read data from the cloud data warehouse. It’s definitely an option\nyou can use when creating your transformation pipelines, but a data destina-\ntion in the cloud platform usually has at least two representations: a table in\nthe warehouse and a collection of files in the cloud storage. We talked about\npros and cons of using a data warehouse and data lake for data processing in\nchapter 5. \nPipeline: Ingest sales\nRDBMS\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\ntable 4\nNamespace: Sales\nNamespace: HR\nPipeline: Ingest ﬁle\nPipeline: Ingest API\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nFigure 7.5\nPipeline metadata layout for a sample data platform with two domains: \nSales and HR\n",
      "content_length": 2056,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "206\nCHAPTER 7\nMetadata layer architecture\nThe Pipeline metadata object has the following attributes:\nID—Unique identifier for this pipeline.\nName—A short name that can participate in naming of the cloud resources.\nDescription—Provides more information about purpose of this pipeline.\nType—Is this pipeline an ingestion or transformation pipeline? \nVelocity—Is this pipeline batch or a real-time pipeline?\nSources and Destinations—This is a mapping of which data sources should be\nsaved into which data destinations. Sources and Destinations are separate\nobjects in our metadata model, and this attribute can include only Source and\nDestination IDs. For ingestion pipelines, there is usually a one-to-one mapping\nof sources to destinations. For transformation pipelines, usually multiple\nsources are used as inputs and one data output is produced.\nData Quality Checks IDs—A list of Data Quality Checks that need to be applied to\nall sources and destinations in this pipeline. Our model also supports adding\nchecks for a specific Data Source/Destination. Pipelines can combine multiple\nsources together. This allows pipelines to perform data quality checks on a\njoined data set, which is not possible when looking at each individual source\nseparately.\nCreated Timestamp—Date and time of when the pipeline was first created.\nPipeline: Ingest sales\nRDBMS\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\ntable 4\nNamespace: Sales\nNamespace: HR\nPipeline: Ingest ﬁle\nPipeline: Ingest API\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nAn output of \none pipeline (i.e.,\nWarehouse table\n1 from the Ingest\nsales RDBMS\npipeline) (1) ...\n...can be used as \nan input to another\npipeline, i.e., the Sales\nquarterly report\npipeline. (2)\nFigure 7.6\nFrom a pipeline perspective, data sources and data destinations are interchangeable. An \noutput of one pipeline can be used as an input to another pipeline.\n",
      "content_length": 2073,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "207\nMetadata model\nLast Modified Timestamp—Date and time of when the pipeline was last modified.\nThese two timestamps are very useful for troubleshooting because they give you\nan idea of when the pipeline was first created and when it was changed. \nConnectivity Details—Ingestion pipelines need to know how to reach the data\nsources. For example, they need to know the hostname/IP address of the\nsource RDBMS, FTP server, Kafka cluster, etc. \nNOTE\nNever store sensitive information like usernames and passwords for\nthe source systems in your metadata layer. Use dedicated cloud services like\nAzure Key Vault, AWS Secrets Manager, or Google Cloud Secrets Manager.\nA Source is a collection of data that we want to bring into the platform. What usually\nseparates one source from another is it’s schema. For example, a table in an RDBMS is\na single source, a topic in a real-time system is a source, and a collection of CSV files\nwith the same schema on an FTP server is a source. \n A Source metadata object has the following attributes:\nID—Unique identifier for this source.\nName—Can be a table name, a topic name, or a file name.\nSchema ID—A link to the Schema Registry that contains the schema for this spe-\ncific source.\nData Quality Check IDs—A list of IDs of the checks that we want to be applied to\nthis source.\nType—What type of a source this is. This value can be “table”, “file”, “real-time\ntopic”, etc.\nCreated Timestamp—Date and time of when this source was first added.\nLast Modified Timestamp—Date and time of when this source metadata was last\nupdated.\nA pipeline journey concludes with a Destination. In our cloud data platform architec-\nture, this can be a table in the cloud data warehouse, a topic in the real-time system, a\nkey/value store for fast application access, etc. There are some implied destinations as\nwell, which we don’t need to describe explicitly in our metadata layer. For example,\neach pipeline stores data in the cloud storage by design. This allows you to create\nchains of transformation pipelines, where the output of one pipeline serves as an\ninput to another one. In this case, some of the pipelines will not have explicit destina-\ntions attached to them. \n A Destination object is similar to the Source object, except a list of Destination\ntypes will be different:\nID—Unique identifier for this destination.\nName—Can be a table name, a topic name, or a collection in a key/value store.\nSchema ID—A link to the Schema Registry that contains the schema for this spe-\ncific destination. Keep in mind that source and destination schemas may not\nnecessarily be identical. \n",
      "content_length": 2619,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "208\nCHAPTER 7\nMetadata layer architecture\nData Quality Check IDs—A list of IDs of the checks that we want to be applied to\nthis destination.\nType—What type of a destination this is. This value can be “warehouse table”,\n“real-time topic”, etc.\nCreated Timestamp—Date and time of when this destination was first added.\nLast Modified Timestamp—Date and time of when this destination metadata was\nlast updated.\nDATA QUALITY CHECKS\nData quality checks involve the application of business rules to data in the platform.\nTheir purpose is to identify data that doesn’t fit within specified parameters. Figure\n7.7 illustrates how data quality checks can be applied to pipelines (Ingest file and\nIngest API) or data sources (employees API) across multiple namespaces (in this case\nHR and Sales).\nThere are typically two types of data quality checks: proactive and retrospective. Proac-\ntive data quality checks happen in the ingestion pipelines and are used to prevent data\nthat doesn’t pass the test from being added to the data platform. These checks usually\noperate on a column or a single row level. Retrospective checks, on the other hand, ana-\nlyze existing archive data, making sure that data still maintains logical integrity and con-\nsistency. You can think of proactive checks as filters that prevent “bad” data from getting\nPipeline: Ingest sales\nRDBMS\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\ntable 4\nNamespace: Sales\nNamespace: HR\nData\nquality\nchecks\nPipeline: Ingest ﬁle\nPipeline: Ingest API\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nFigure 7.7\nData quality checks involve the application of business rules to pipelines and/or data \nsources and are stored as configurations in metadata.\n",
      "content_length": 1883,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "209\nMetadata model\ninto the platform. Retroactive checks are more like audit reports that make sure that\nnew data combined with existing data doesn’t break any organizational quality rules.\nFigure 7.8 demonstrates differences between proactive and retrospective checks.\n Proactive checks are typically used to make sure data in a certain column follows a\npredefined format or to check that certain values never appear in columns. For exam-\nple, a birth-date column cannot have negative values, and first- and last-name columns\ncannot be empty. Because proactive checks happen during ingestion, they usually\ndon’t have access to data that already exists in the platform and therefore can’t join\nmultiple data sources together. This is not purely a technical limitation. For example,\nin chapter 4 we described batch deduplication pipelines that do require joining\nincoming data to existing data to find duplicates. You would want to limit heavy oper-\nations such as joining two large data sets together during ingestion because this will\nslow down ingestion pipelines significantly. \n Retrospective checks are more flexible, because they can run periodically on their\nown schedule. Retroactive checks are really just data transformations pipelines that\nPipeline: Ingest sales\nRDBMS\nDestination:\nWarehouse\ntable 4\nQA pipeline:\nEmployee count\nNamespace: Sales\nPipeline: Ingest ﬁle\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\nemployees table\nData source:\nEmployees\nAPI\nNamespace: HR\nData\nquality\nchecks\nPipeline: Ingest API\n1. Proactive: Make sure all date fields in this \npipeline follow the same format.\nProactive: Make sure all employees have \nnon-empty first- and last-name fields.\n2. Retrospective: Make sure departments \nwith no employees don’t exist.\nFigure 7.8\nProactive checks (1) happen during ingestion and can be attached to an ingestion pipeline \nor to individual data sources. Retrospective checks (2) are separate pipelines, similar to transformation \npipelines, except that they don’t modify any data.\n",
      "content_length": 2240,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "210\nCHAPTER 7\nMetadata layer architecture\ndon’t change any data. They can produce new data sets, like data quality reports, for\nexample. Retrospective checks can access all historical data and join multiple differ-\nent data sources to make sure that data is still consistent. For example, we can have a\nretrospective check that joins an employees data set with a departments data set to\nmake sure there are no departments without employees in the data. Retrospective\nchecks would typically produce a periodic report that is sent out to the data owners.\nThey can then decide on an appropriate action. This may include cleaning up data at\nthe source or making changes in the data platform itself.\n From the metadata perspective, both proactive and retrospective data checks are\njust collections of different rules that need to be applied to the data. Data quality\nchecks, metadata attributes, and structure will depend on how you decide to imple-\nment quality checks in your pipelines. There are many different options and existing\nproducts that can be used for data quality control. For example, you can implement a\ncheck as an SQL query and store this query together with a description in the Data\nQuality metadata. In some cases, we have seen organizations implement their own\ndomain-specific language (DSL) to configure all kinds of quality checks. They would\nthen store these DSL configurations in the metadata repository and fetch the ones\nthat are required for a given pipeline execution. \n Here are some common attributes that you should include in the Data Quality\nmetadata:\nID—Unique data quality check identifier. \nName—A descriptive name of the data quality check.\nSeverity—Different data quality issues have different criticality. Some data must\nnever enter the data platform, for example, negative numbers in the salary col-\numn of an employees table. Such data may break existing reports or down-\nstream pipelines and must be quarantined for further investigation. Other\nissues may not be as critical and should not prevent data from getting ingested,\nbut a data engineer needs to be notified when such issues happen, so they can\ninvestigate the root cause. We usually see three types of data quality check sever-\nity: “info”, “warning”, and “critical”. “Info” means the issue gets recorded in the\nactivities metadata, but no alert is raised and data is allowed to be ingested.\n“Warning” meaning the data is still allowed in, but an alert is raised, and “criti-\ncal” means data that doesn’t pass this check will not be ingested. \nRule—This attribute contains the logic for the check. As described earlier, it can\nbe an SQL query to run or a more sophisticated DSL configuration. \nCreated Timestamp—Date and time of when this check was added.\nLast Modified Timestamp—Date and time of when this check was last updated.\nPIPELINE ACTIVITY METADATA\nIn the previous section, we described pipeline configuration metadata, which includes\ninformation about how data flows are configured. Unlike pipeline configuration\nmetadata, pipeline activity metadata captures valuable information about the data\nflow as it is being executed. \n",
      "content_length": 3141,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "211\nMetadata model\n Pipeline activity metadata captures data describing successful pipeline execution so\nwe can then use this information to identify what a potentially problematic execution\nlooks like. This matters because pipelines are not static. You typically don’t perform a\ndata ingestion or particular transformation just once. Batch ingestion and transforma-\ntion pipelines are scheduled to run once or multiple times per day. Real-time pipelines\ndon’t have a particular start/stop time—they run continuously all the time. \n Things can change from one pipeline execution to another. A database may\nbecome unavailable for some period of time, or a daily file is misplaced by a third\nparty and doesn’t arrive at its expected location. As shown in figure 7.9, pipeline activ-\nity captures information about every pipeline’s status (successful, failed) as well as var-\nious statistics about pipeline execution, such as number of rows processed, pipeline\nexecution time, etc. This applies for both batch and streaming pipelines.\nWe store this information on an ongoing basis from the first time the pipeline is\ndeployed and typically don’t ever delete it. This archive of historical information\nabout the pipeline behavior has proven to be extremely useful for troubleshooting\nvarious operational issues, and it also gives you a way to analyze the behavior of the\nplatform overall. \n Pipeline activity metadata can capture a lot of different attributes, but here we will\nonly describe some common ones that we have found to be useful for many different\norganizations. When thinking about pipeline activity metadata, it is useful to visualize\nPipeline: Ingest sales\nRDBMS\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\ntable 4\nPipeline\nactivities\nNamespace: Sales\nNamespace: HR\nPipeline: Ingest ﬁle\nPipeline: Ingest API\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nPipeline activity captures\nstatistics and other helpful\ninformation about every single\npipeline run or a periodic\nsnapshot of a pipeline in a\nreal-time scenario.\nFigure 7.9\nPipeline activity captures statistics and other helpful information about every single \npipeline run or a periodic snapshot of a pipeline in a real-time scenario.\n",
      "content_length": 2382,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "212\nCHAPTER 7\nMetadata layer architecture\nit as a log file or table in a database. We will talk about some of the actual implementa-\ntion options later in this chapter.\n Here are some of the attributes that you want to capture about pipeline activities:\nActivity ID—Unique activity identifier. A unique ID must be generated for each\nentry in the activity metadata.\nPipeline ID—ID of the pipeline for which metadata is captured. This allows us to\nget detailed information about the pipeline from the pipeline metadata.\nStart Time—Timestamp of when the pipeline has started execution.\nStop Time—Timestamp of when the pipeline finished.\nStatus—Success or failure or other statuses that you may be interested in.\nError Message—Details of what exactly went wrong if the pipeline has failed. This\nsaves tremendous amounts of time sifting through various log files to find an\nerror. Since this error message is for the whole pipeline, it can include multiple\nerrors in case there were failures caused by individual rows. \nSource and Destination IDs—Since pipelines can read from many different sources\nand write to many different destinations, we need to know exactly which pair of\nSources and Destinations we are recording information for. For the ingestion\npipeline, this would typically be one Source and one Destination. For transfor-\nmations, this attribute can include multiple Sources and a single Destination.\nRows Read—How many rows the pipeline read from the Source.\nRows Written—How many rows the pipeline wrote to the Destination. For inges-\ntion, you typically would want this to match Rows Read, but if you are doing\ndata deduplication or filtering some data out based on data quality checks, this\nmight not be the case.\nBytes Read—Amount of data read from the Source. This is useful when dealing\nwith files to make sure the entire file was processed.\nBytes Written—Amount of data written to the Destination. Usually it will not\nmatch Bytes Read because of file format conversion, but it can be used for extra\nmonitoring, like making sure this value is not 0 if Bytes Read is not 0.\nExtra—This is a family of attributes that can include the batch id and the full\npath on cloud store where a batch pipeline saved data to, or real-time topic data\nwas written to, time windows for real-time pipelines, etc. \nExercise 7.2\nWhich of the following extra attributes should not be included in pipeline activities?\n1\nBatchId for the batch pipelines\n2\nWarnings produced when parsing incoming JSON, CSV, or XML files\n3\nList of all fields in the current data source\n4\nFull cloud storage paths for the input/output data\n",
      "content_length": 2625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "213\nMetadata layer implementation options\nSince pipeline activity metadata is used for both batch and real-time pipelines, some\nattributes will have dual meaning. For example, Start Time and Stop Time attributes\nwork well for batch pipelines because they have clear start and stop times. For real-time\npipelines, on the other hand, you can use a predefined time window to capture these\nstatistics. You can decide to get real-time stats every five minutes, and then the Start\nTime and Stop Time for this pipeline will be a five-minute timeframe. This also applies\nto other time-dependent attributes, such as Rows Read, Rows Written, etc. It’s a good\nidea to align this timeframe with how often you flush real-time data into cloud storage\nfor archival purposes. This way, from the statistics-collection perspective, you are break-\ning down the real-time pipeline into a series of batch pipeline executions. \n To highlight the importance of collecting this information about your pipelines,\nhere are some questions you can ask of this data:\nWhat was the last successful execution of a given pipeline? Here you will need\nto find an entry in the activities metadata with Status being equal both to “suc-\ncess” and latest Start Time. \nWhat is the average duration for a given pipeline execution? This applies to\nbatch pipelines only, because for real-time pipelines, you will have a fixed time\nwindow in which statistics are collected. To answer this question, you will need\nto aggregate the difference between Start Time and Stop Time for a given Pipe-\nline ID where Status is equal to “success”. You want to exclude failed executions\nfrom this calculation.\nWhat is the average number of rows this pipeline processes? This question is\nuseful to ask when you want to establish a baseline of what looks normal for a\ngiven pipeline. This can be used for both batch and real-time pipelines, and to\nanswer this question, you need to aggregate the Rows Read and Rows Written\nmetrics for a given Pipeline ID where Status is equal to “success”.\nHow much data are we ingesting from a given RDBMS table every day? Here you\ncan do similar aggregation of Rows Read, but this time not by Pipeline ID but by\na specific Source ID and only take into account executions that were successful. \nHopefully this gives you some ideas of how pipeline activities can be used for getting\nvisibility into what your pipelines are doing. Not only can this data be used for ad hoc\nexploration and troubleshooting, it is also very easy to use these calculations for your\ncloud data platform monitoring to make sure you get alerted when a certain metric\nbehaves abnormally.\n7.4\nMetadata layer implementation options\nIn the previous sections of this chapter, we described how to structure your cloud data\nplatform metadata, which main metadata entities you should have, and what some of\nthe common attributes of these entities are. Once you know how your metadata\nshould look and what it is used for, the next logical question is, “How do you imple-\nment it in a real project?”\n Some disappointing news first. As mentioned at the beginning of this chapter,\nthere are no industry-standard open source or even commercial products that you can\n",
      "content_length": 3208,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "214\nCHAPTER 7\nMetadata layer architecture\nuse for your metadata layer. There are some solutions that we will review later in this\nchapter that may work for your use case out of the box, but our experience shows that\nthese solutions almost always require significant effort to adjust them to meet the spe-\ncific organization’s needs. \n This means that to have a metadata layer that satisfies your requirements, you will\nneed to build this yourself. This do-it-yourself approach may discourage some of you\nright away. We all know that building a brand-new software component from scratch\nrequires lots of time and effort. The metadata layer is an important component of the\ncloud data platform—without it, pipelines won’t know where to read data from and\nwhere they should store the results, and platform operators will be left clueless about\nwhat’s actually going on in the platform they support. \n All that said, don’t despair! Our experience shows that while most of the really effi-\ncient metadata layer implementations are implemented from scratch, there is a com-\nplexity scale where most simple implementations don’t require a lot of effort and will\nwork just fine for smaller teams or platforms with only a handful of data sources and\npipelines. At the other end of this spectrum are implementations that are more flexi-\nble and sophisticated and are capable of supporting hundreds of data sources, thou-\nsands of pipelines, and multiple data engineering teams. In this section, we will walk\nyou through three metadata layer implementation options of increasing sophistica-\ntion, starting with the simplest option. \n7.4.1\nMetadata layer as a collection of configuration files\nIn the previous section, we talked about two types of metadata. The first is metadata that\ndescribes pipeline configurations. This includes Namespaces, Pipelines, Data Sources,\nDestinations, the Schema Registry, and Data Quality Checks. And then there are Pipeline\nActivities that capture information about what happened during pipeline execution.\n One of the simplest ways to implement pipeline metadata is with configuration\nfiles. In the application development world, configuration files are widely used for\nstoring various application settings. We can adopt the same approach for our data\nplatform. We can represent Namespaces, Pipelines, Data Sources, Destinations, and\nData Quality Checks as separate configuration files, leaving the Schema Registry\nimplementation options to the next chapter. These configuration files can be in any\npopular format, including JSON, YAML, or any other format that your organization is\nmost comfortable with. We will use YAML for examples in this section. \n The following listing is an example of how a Namespace configuration file may look.\n---\nnamespaces: \nsales:   \n  id: 1234 \n  description: This namespace contains data from sales data sources\nListing 7.1\nExample namespace.yaml configuration file\nAll existing Namespaces \nare stored in a single file.\nThis is the name \nattribute.\nAll other namespace \nattributes are nested \nkey/value pairs.\n",
      "content_length": 3068,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "215\nMetadata layer implementation options\n  created_at: 2020-03-10 08:17:52\n  modified_at: 2020-03-15 14:23:05\nhr: \n  id: 1235\n  description: This namespace contains sensitive data from HR data sources.\n  created_at: 2020-03-01 10:08:40\n  modified_at: 2020-03-01 10:08:40\nThis configuration file is easy to read for humans, and it can be easily parsed by the\nactual pipeline code using any of the available YAML libraries for your language of\nchoice. You can now see how all other pipeline metadata domains like Pipelines, Data\nSources, etc., can be represented as configuration files. \n A big benefit of this approach is that configuration files are just text files and in the\nend can be stored together with other code in your code repository. Your code ver-\nsioning tool, such as Git, Mercurial, SVN, or any other, will also allow you to track all\nchanges to these configuration files the same way you track changes to your code. \n Figure 7.10 illustrates how configuration files can be used together with cloud log-\naggregation services to implement the metadata layer. Metadata configuration files are\nstored in the code repository and are versioned as the rest of your code. You can use\nthe same code repository you use for your pipeline code, or you can have a dedicated\nrepository just for the configuration files. Every time a change to the configuration\npipelines:\n - ingest_rdbms:\n  \nid: 1\n  \ntype: ingest\n  \n...\n - sales_report:\n  \nid: 2\n  \ntype: transform\n  \n...\nCode\nrepository\nCI/CD\nCode repository\nstores metadata as\nplain-text\nconfiguration files:\nJSON, YAML, etc.\nExample pipeline\nconfiguration\nfile in YAML\nformat\nConfiguration files\nare saved to the\ncloud storage\nusing an existing \nCI/CD process.\nPipelines save their activity\nmetadata into text log files in a\ndedicated cloud storage container,\nwhich will be ingested by a Cloud\nLog Aggregation service.\nIngestion and\ntransformation\npipelines read\nthese configuration\nfiles from cloud\nstorage.\nTransformation\npipelines\nCloud storage\nfor logs\nCloud storage\nIngestion\npipelines\nCloud Log\nAggregation\nservices\nFigure 7.10\nConfiguration files are stored in the code repository and are pushed to cloud storage \nusing the existing CI/CD process whenever a new configuration change is made. Pipelines use these \nconfiguration files to adjust their behavior. Pipelines use Cloud Log Aggregation services to save \nactivity metadata.\n",
      "content_length": 2396,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "216\nCHAPTER 7\nMetadata layer architecture\nfiles is ready to be released, your continuous integration/continuous delivery (CI/CD)\nprocess will copy the latest version of the files from the code repository into a dedi-\ncated location on cloud storage. Describing a CI/CD pipeline is outside the scope of\nthis book, but in the simplest scenario, this can be a script that a data engineer evokes\nafter making changes to the configuration code, or it can be a more automated\napproach using one of the many existing CI/CD tools like Jenkins, etc.\n The location of the configuration files on the cloud storage is up to you. You can\ncreate a separate storage container for this or just use a folder in the same container\nyou use for actual data.\n Your batch ingestion and transformation pipelines will then read these configura-\ntion files when they start and adjust their behavior accordingly. For real-time pipe-\nlines, you will need to implement periodic polling for the new version of config files\nsince those pipelines run continuously. \n Configuration files work well for the metadata that describes pipeline behavior but\nwhat about Pipeline Activities metadata? Unlike configuration data, activities are not\nstatic. Your pipelines will need to constantly add new entries to the activity metadata.\nAgain, there is an analogy to be made here to application development. Application\nlog files have been around for a long time and are a common way to capture “applica-\ntion activities” as a text file that you append new lines to. Log files are easy to imple-\nment, and most general-purpose programming languages have libraries to make that\nprocess even easier. \n There is one challenge with log files, though. It’s really hard to analyze them with-\nout specialized tools. Imagine that you have several dozen pipelines, each writing\ninformation about their status into a separate log file. You would have a hard time try-\ning to find the bit of information you want in those log files if you were to go through\nthem manually. There are dedicated tools, both commercial and open source, that\nsolve this problem, and we are lucky to be implementing our data platform in the\ncloud, because each cloud provider has a service that solves this problem without you\nhaving to install any additional tools. It’s worth mentioning that it is a good idea to fol-\nlow structured logging principles for your pipeline logs. Logs that follow a certain struc-\nture and clearly delineate between different attributes will be much easier to search\nand analyze in future.\n In figure 7.10 we call these services Cloud Log Aggregation Services, and their pri-\nmary function is to periodically take the log files that your pipelines generate, parse\nthem, and give you a UI to do all types of exploration and analytics. You can search for\nspecific keywords, filter log files for given timeframes, cross-reference, etc. On Azure,\nyou can use Azure Monitor together with Log Analytics; on Google Cloud, you can use\nCloud Logging; and AWS offers an Elasticsearch service for the same purpose. \n Going into details of these services is outside the scope of this book, but at a high\nlevel, the idea is that your pipelines will produce a text log file that includes all the\nattributes that we have described in the Pipeline Activities section. These log files will\nbe stored in a dedicated cloud storage container and then ingested automatically by\n",
      "content_length": 3414,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "217\nMetadata layer implementation options\nthe corresponding Cloud Log Aggregation service. You will then use the cloud ven-\ndor’s specific service UI or tools to run queries against your pipeline logs.\n7.4.2\nMetadata database\nStoring all pipeline metadata as configuration files works for smaller data platforms,\nbut once your cloud data platform grows, it will become more and more challenging\nfor a data engineer to find the information they need in those files. Imagine what will\nhappen once you get to several hundreds of data sources and dozens of pipelines,\nwith a couple of data quality checks assigned to each pipeline. In this scenario, you\nwill have configuration files with potentially thousands of entries. You can generate\nmany parts of those config files automatically using inputs. For example, given a list of\ntable names in a database, you can write a script to generate data source configuration\nfiles in your preferred format. However, daily platform operations will be more\nchallenging.\n Let’s say one of your many data quality checks suddenly starts failing on many dif-\nferent data sources. You will want to find all data sources that have this check attached\nto them. You can’t really run a query against your configuration files, and sifting\nthrough thousands of entries in the files manually is not going to work well either. To\nsolve this problem, you need to load your configuration files into a data store that sup-\nports some kind of query language to make these types of operations possible. Figure\n7.11 introduces a metadata database (1), which is a cloud database that will store your\nconfiguration files from the code repository in a more structured way.\n This metadata layer implementation looks very similar to the previous one, except\nconfiguration files are not stored in the cloud storage but are parsed and loaded into\npipelines:\n - ingest_rdbms:\n  \nid: 1\n  \ntype: ingest\n  \n...\n - sales_report:\n  \nid: 2\n  \ntype: transform\n  \n...\nCode\nrepository\nCI/CD\nMetadata\ndatabase\nConfiguration files are\nparsed and loaded into\ntables in the metadata\ndatabase instead of in\ncloud storage.\nTransformation\npipelines\nCloud storage\nfor logs\nIngestion\npipelines\nCloud Log\nAggregation\nservices\nFigure 7.11\nStoring configuration files in a \ndatabase instead of text files on cloud storage \ngives you the ability to run queries against \nthese configurations.\n",
      "content_length": 2380,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "218\nCHAPTER 7\nMetadata layer architecture\na database. This database can be either a relational database or a document-oriented\nkey/value store. The latter is often a better fit for configuration data, because the\nstructure of your metadata will be evolving, and it is easier to implement schema\nchanges in a document data store than it is in a relational database. Each major cloud\nprovider has fully managed services for relational databases and key/value stores. We\nhave seen successful implementations of this approach using Google Cloud Datastore,\nAzure Cosmos DB, and AWS DynamoDB.\n In figure 7.11 you will see that we still maintain and update text-based configura-\ntion files in our code repository. We do this because we must treat metadata as code\nand make sure that we have a version history of all changes. Some changes to the pipe-\nline code may require making changes to the structure of your metadata, such as\nintroducing new attributes, etc. In this case, you will want to synchronize the release of\nthe new versions of pipelines and configuration updates, and storing both in a code\nrepository (not necessarily the same one) will help. \n In order to load new versions of the configuration data into the metadata database,\nyou will need to implement a tool that can parse the configuration file format that you\nhave and then add, update, or delete specific entries. The database itself will be struc-\ntured to follow your metadata domain design. Each metadata type, such as Pipeline or\nNamespace, will become a separate table in the metadata database. The tool that is\nresponsible for loading configuration files into the database will need to check the\ncurrent data in the file and compare it with what’s in the database and then either add\na new entry, update an existing one, or delete an entry that no longer exists in the\ndatabase. \n Ingestion and transformation pipelines will read directly from the metadata data-\nbase and will require fewer changes, when compared to doing the same with configu-\nration files. Instead of going through hundreds of entries in the configuration file\nwhen they need to troubleshoot an issue, data engineers will now use a query lan-\nguage that the database provides. Usually it’s either SQL or a SQL-like language that\nmost data engineers are familiar with. All major cloud vendors also provide a basic UI\nfor these databases where you can run a query and explore the results, making work-\ning with metadata a much more pleasant experience. \n7.4.3\nMetadata API\nStoring metadata in a dedicated database is an approach that will work for most small\nand medium-size teams, but there are limits to its scalability. If your organization\ngrows beyond a single team developing and maintaining the cloud data platform, you\nwill find it challenging as multiple teams try to work with the metadata database\ndirectly. The problem here is the same one you will see in any large application devel-\nopment project. If you have two or more teams that need to work with the metadata,\nlet’s say building their own pipelines or tools, then those pipelines and tools become\ntoo tightly coupled with the structure of the database itself. If you need to change\nsome of the metadata entities or add new ones, then multiple teams get affected. And\n",
      "content_length": 3278,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "219\nMetadata layer implementation options\nanyone who has had to coordinate a large update of a monolithic application across\nmultiple teams knows it’s not a fun exercise. \n The solution for such large-scale data platforms is to introduce an API layer that\nhides the internals of the metadata database. The most common implementation of\nthe metadata API is REST, which provides teams with a way to make HTTP requests to\nthe service to add new metadata items, retrieve existing ones, and update or delete\nentries. APIs can be versioned, which makes rolling out changes to the underlying\ndatabase structure simpler. Figure 7.12 shows how previous metadata layer implemen-\ntations can be extended to include APIs (2). The metadata API abstracts the internal\nstructure of the metadata database from the pipelines, tools, and end users. This\nallows us to make changes to the database structure without major impact to those\npipelines and tools.\nDiscussing how to implement a cloud-based REST API service is outside the scope of\nthis book. Cloud providers offer different services that allow you to deploy such ser-\nvices in a fully managed environment. For more details on how to implement REST\nAPI services, feel free to check out another Manning publication such as The Design of\nWeb APIs (Arnaud Lauret, 2019).\n From the metadata flow perspective, the only major change is that instead of mak-\ning a connection to the database and writing data directly to it, the CI/CD pipeline or\nany other automation tools that you develop will make an HTTP call to the corre-\nsponding metadata API endpoint to make the necessary changes. Pipelines will also\nneed to switch to using HTTP calls instead of direct database connections. \npipelines:\n - ingest_rdbms:\n  \nid: 1\n  \ntype: ingest\n  \n...\n - sales_report:\n  \nid: 2\n  \ntype: transform\n  \n...\nCode\nrepository\nCI/CD\nMetadata\ndatabase\nMetadata API\nservice\nThis service provides \nAPIs to fetch, add, \nupdate, and delete \nmetadata.\nTransformation\npipelines\nCloud storage\nfor logs\nIngestion\npipelines\nCloud Log\nAggregation\nservices\nFigure 7.12\nThe metadata API \nabstracts the internal structure of \nthe metadata database from the \npipelines, tools, and end users.\n",
      "content_length": 2196,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "220\nCHAPTER 7\nMetadata layer architecture\n It’s very important to carefully assess on which level of maturity and scale your\ncloud data platform implementation is. It will help you to choose the right implemen-\ntation approach. We recommend that you choose the simplest implementation that\nsatisfies your requirements today and evolve the solution as your requirements\nchange. It might be tempting to jump right into the API implementation, which will\ntake significant engineering efforts to implement correctly, and neglect the needs of\nactual data users. \n Table 7.1 provides a summary of which implementation option is a better fit for\ndata platforms and teams of different sizes.\nIn our experience it’s always better to start with a simpler option and then gradually\nevolve your metadata architecture. None of the implementation options that we have\noutlined require you to completely rewrite to move from option 1 to option 2, for\nexample. Adding a database instead of working directly with the configuration files\ndoesn’t require you to change the structure of the configuration files, for example.\nThis makes it possible to move incrementally from one option to the next.\n7.5\nOverview of existing solutions\nAs mentioned before, the topic of collecting and maintaining pipeline metadata is not\nvery widely discussed in the industry today. Cloud vendors focus more on business\nmetadata tools that allow end users to catalog and search various cloud data sets.\nThese are useful services, but not for describing pipeline configuration. Large compa-\nnies with mature data platforms almost always end up implementing some kind of\nTable 7.1\nChoose an appropriate metadata implementation option for the data platform and data\nengineering team sizes.\nMetadata implementation option\nNumber of\ndata sources\nSize of the data\nengineering team\nOption 1, using plain configuration files\n1–5\n1–3\nOption 2, using a database\n5–10\n3–5\nOption 3, using a database with an API layer on top of it\n10 or more\n5 or more, multiple teams\nExercise 7.3\nWhat is the main benefit of using a metadata database with an API layer?\n1\nIt’s the simplest option to implement.\n2\nIt provides a common interface for multiple teams to interact with the \nmetadata.\n3\nIt allows you to store more metadata entities than other options.\n4\nIt provides faster performance than other options.\n",
      "content_length": 2350,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "221\nOverview of existing solutions\nmetadata layer solution that works for them, especially with some of these implemen-\ntations available as open source projects. In this section, we will provide a high-level\noverview of existing cloud services around metadata and how they differ from the\nmodel we have described, and we will also look into some of the existing open source\npipeline metadata projects. \n7.5.1\nCloud metadata services\nAcross AWS, Azure, and Google Cloud, there are several services that at least partially\nfit into the metadata layer model that we have described in this chapter. As much as\nwe would love to see a cloud-native service that is flexible enough to fully solve the\nproblems associated with data platform pipeline metadata, unfortunately, it doesn’t\nexist today. Some of the cloud services are close, but others are too focused on the\ndata discovery part of the problem. \n Among existing cloud services, AWS Glue Data Catalog is most aligned with the\nidea of the metadata layer described in this chapter. Glue Data Catalog stores informa-\ntion about data sources and data destinations as well as information about pipeline\nexecution and other statistics. It has an API that can be used to fetch and modify data\nin the catalog and also plays the Schema Registry role. \n Some of the most interesting features of AWS Glue Data Catalog are crawlers. Crawl-\ners are scheduled processes that connect to your data sources, scan them, and add meta-\ndata about the new tables of files that they discover since the previous run. Crawlers also\nperform schema discovery for each data source, which they store in Data Catalog. This\napproach fits nicely with our Pipeline Metadata/Schema Registry domains.\n AWS Glue Data Catalog also stores metadata about ETL jobs that you run, including\nhigh watermarks for database tables, latest processed files for file-based sources, etc.\nThese are called job bookmarks in Glue terminology. In addition to job bookmarks, Glue\nalso stores various statistics about jobs, such as the number of rows processed, etc.\n One major limitation of Glue Data Catalog is flexibility. AWS Glue Data Catalog is\nnot a standalone service but a component of the AWS Glue ETL service. This means\nthat in order to use Data Catalog properly, you need to author and schedule all your\nETL jobs in Glue. If all your pipelines are batch jobs, and you don’t have data sources\nthat Glue doesn’t support, such as REST APIs, you can implement most of the meta-\ndata and data processing layers in Glue. \n If you need to bring in a wider variety of sources that are currently not supported\nby Glue, you will need to implement a different way to store metadata. This splits your\nmetadata into Glue and non-Glue parts, which becomes a problem from a mainte-\nnance and operations perspective.\n With Azure and Google Cloud, existing offerings are more centered around busi-\nness metadata and data discovery. Azure Data Catalog allows users to register the loca-\ntion of a data source, for example, a table in a database: enter a description and\ndocumentation for each column. This can be either done manually or through an\nAPI. Once most existing data sources are added to the Data Catalog, data users can\n",
      "content_length": 3224,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "222\nCHAPTER 7\nMetadata layer architecture\neasily search for them using the catalog UI. This data discovery aspect of a data plat-\nform is really important if you want to increase the self-service capabilities of your plat-\nform. Essentially, it solves the problem of “only our data engineers know where the\nactual data is” and empowers end users to do more with the data. Azure Data Catalog\ndoesn’t fit the role of the pipeline metadata storage as described in this chapter. It\ndoesn’t capture a lot of the attributes that are required by ETL jobs and generally was\ndesigned to be used by people who consume data rather than those who program data\ningestion and transformation jobs. \n Google Cloud Data Catalog is very similar to Azure’s Data Catalog. It too is\ndesigned to serve as a central hub for data users to search for data they are interested\nin, but it can’t be used as the metadata layer that we have described, because it doesn’t\ncapture a lot of the pipeline-specific metadata attributes. \n To better understand where business-oriented Data Catalog services such as Goo-\ngle Cloud Data Catalog and Azure Data Catalog fit in our data platform design and\nhow they are different from the metadata layer we have been talking about in this\nchapter, let’s use figure 7.13 to look back at the platform layers diagram from chapter\n3, and add a data catalog between the data platform and the data consumers to show\nhow it helps data consumers find and access data.\n Figure 7.13 shows that a Data Catalog (the way it is implemented on Azure and\nGoogle Cloud) is a separate component that sits between the cloud data platform and\ndata consumers. It’s main role is to simplify data access and discovery for human\nusers, not to help with automation and monitoring of the data ingestion and transfor-\nmation pipelines. \nOrchestration overlay\nFast storage\nReal-time analytics\nIngestion\nProcessing\nOperational \nmetadata\nData\nwarehouse\nDirect data lake access\nSlow storage\nETL tools overlay\nData\nCatalog\nData\nconsumers\nData\nconsumers\nStreaming\ndata\nBatch\ndata\nFigure 7.13\nData Catalog sits between the data platform and data consumers and is primarily used to \nallow end users to use the platform in a self-service manner without knowing technical implementation \ndetails of the platform itself.\n",
      "content_length": 2291,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "223\nOverview of existing solutions\n7.5.2\nOpen source metadata layer implementations\nThere are several existing open source projects that we would like to mention here\nthat can, to some extent, be used as a basis for metadata layer implementations.\nBefore we go over some of the projects, it’s worth mentioning that in our experience\nnone of them can be used as a drop-in metadata layer implementation, but depend-\ning on your use case and engineering capabilities, your organization may either find\nthe provided functionality useful or you can use existing code as a foundation for a\nmetadata layer that is tailored to your needs.\n First in our list is Apache Atlas (https://atlas.apache.org/). Atlas combines two\nmain components. It acts as a central metadata storage with an API on top of it, and it\nprovides a UI for end users to do data discovery. So on a high level, Atlas does what\nexisting cloud data catalog services do today, plus it has a flexible metadata system that\ncan be used for storing pipeline information. \n When it comes to pipeline metadata, Atlas provides a very flexible type/entity sys-\ntem. Types in Atlas terminology are metadata items that we have described previously\nin the chapter. For example, a Data Source or Data Destination in our model can be\ndescribed as an Atlas Type. Types are basically collections of attributes. Atlas has a rich\ntype system that allows you to not only define attributes of primitive types like num-\nbers or string, but also arrays of values (useful for things like attaching data quality\nchecks to specific pipelines or sources) and references, which allow you to link one\ntype to another. If you go back to our metadata model, shown again in figure 7.14, you\nwill see that we use references extensively to link metadata items together, such as link-\ning a data source and destination to a specific pipeline. \nPipeline: Ingest sales\nRDBMS\nData source:\nEmployees\nAPI\nDestination:\nWarehouse\ntable 4\nNamespace: Sales\nNamespace: HR\nPipeline: Ingest ﬁle\nPipeline: Ingest API\nDestination:\nWarehouse\ntable 1\nDestination:\nWarehouse\ntable 2\nDestination:\nWarehouse\ntable 3\nPipeline: Sales\nquarterly report\nDestination:\nWarehouse\ntable 5\nData source:\nTable 1\nData source:\nTable 2\nData source:\nJSON ﬁle\nFigure 7.14\nApache Atlas supports linking of one \ntype to another using references. This works well for \nour metadata model that uses references to link data \nsources to pipelines, data quality checks to data \nsources, namespaces to pipelines, etc.\n",
      "content_length": 2499,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "224\nCHAPTER 7\nMetadata layer architecture\nApache Atlas Entity is a concrete representation of a type. Where type describes which\nattributes, let’s say, a pipeline has, an entity describes a specific pipeline. If you are\nfamiliar with object-oriented programming concepts, you will notice that types are\nsimilar to classes and entities are objects. \n Apache Atlas allows you to describe types and specific entities using either a REST\nAPI or a built-in UI. The fact that the type/entity system is flexible and allows you to\ndefine any types and relationships between them that you want means that you can\nuse Atlas to implement the metadata model that we have described in this chapter.\nBecause a second part of Atlas provides capabilities for adding, searching, and propa-\ngating business metadata, it is a two-in-one product, which is useful for both data plat-\nform automation and data discovery/self-service purposes. \n The major limitation of Atlas is that it was created for working with the Hadoop plat-\nform. This means that a lot of its out-of-the-box functionality is tied to specific Hadoop\ncomponents that we don’t really have in the cloud data platform design. For example,\nAtlas has some built-in data sources from which it can import metadata automatically\n(as opposed to creating types and entities using an API or UI). These sources include\nHadoop or Hadoop ecosystem components like Hive, HBase and others.\n Another downside of Apache Atlas is that it requires HBase and Solr components\nto store and index/search metadata, respectively. Both HBase and Solr are open\nsource projects as well, meaning you can download and configure them anywhere,\nincluding in the cloud. Using open source instead of PaaS options goes against our\nprinciple of using as many cloud managed services as possible as it can add a signifi-\ncant operational overhead. If you don’t have a lot of experience in managing an\nHBase installation in production, you probably don’t want to start now. \n To summarize, Apache Atlas offers a very flexible type/entity system that can be\nused to implement any metadata model, including one that we have described in this\nchapter. It also provides business metadata editing and searching capabilities, similar\nto existing cloud data catalog tools. On the other hand, Atlas only provides Hadoop-\nspecific plugins for metadata discovery out of the box and requires some relatively\ncomplex infrastructure to run. We hope that the Apache Atlas community will at some\npoint make this project more cloud native and allow using cloud services instead of\nHBase and Solr. Or maybe it will be your organization that will contribute these\nchanges back to the project. It is open source, after all. :)\n Another open source project in the metadata management space is DataHub\n(https://github.com/linkedin/datahub), developed by LinkedIn’s engineering team.\nAs you can imagine, LinkedIn has a highly mature data platform and metadata\nmanagement design and implementation because of the amount of data they need to\ndeal with and the size of their data engineering, data user community. DataHub is\nsimilar to Apache Atlas because it not only provides capabilities to add information\nabout data sources, their location, etc., but also provides data discovery capabilities for\nend users. \n",
      "content_length": 3299,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "225\nOverview of existing solutions\n DataHub allows you to create your own metadata entities, so you can use it to imple-\nment the metadata model that we have described in this chapter. It’s worth mentioning\nthat DataHub documentation is rather sparse at the moment, which makes it harder to\nquickly adopt this project for your needs. DataHub has drawbacks similar to Apache\nAtlas—it requires some extra infrastructure to be deployed in order to function prop-\nerly. DataHub requires a Kafka cluster in order to collect metadata from various data\nsources. It uses a MySQL database as a primary metadata store and uses Elasticsearch\nand Neo4j graph databases to provide search capabilities to data users. While all these\ntechnologies are open source, it will require significant operational overhead in order\nto deploy and manage all of them in a production environment. \n Finally, we would like to mention the Marquez project (https://github.com/\nMarquezProject/marquez), developed by WeWork engineers. Marquez is different\nfrom Apache Atlas and DataHub because it doesn’t provide the Data Catalog func-\ntionality. It’s main purpose is to collect and track information about data sources and\nthe pipelines that process those data sources. This information is then used to allow\nend users to visualize and search through data lineage information. Data lineage is like\na family tree for data. For each data set in a data platform, data lineage provides infor-\nmation about which pipelines and data sources produced it all the way through to the\noriginal set of data that served as an origin for this data set. \n While data lineage functionality is very useful, it is still mostly used for data discov-\nery purposes. Marquez doesn’t have (at least currently) the functionality to imple-\nment the full metadata model that we have described. For example, you can’t create\nnew metadata entities besides the ones that are already available. Marquez’s metadata\nmodel is very simple and consists only of Data Sets and Job (pipeline) entities. Each\nJob is linked to the actual pipeline code version, and each Data Set is linked to one or\nmore Jobs, allowing you to track data lineage. This functionality can be adopted for\nactivity tracking in our metadata model. Marquez provides libraries for Java and\nPython to allow Jobs to register their progress in a Marquez data store. This can also\nbe used to produce pipeline logs that will be sent to a Cloud Log Aggregation service,\nbut this will require you to make changes to the Marquez code itself. \n From an operations and infrastructure perspective, Marquez is relatively simple\nand only requires a PostgreSQL database as it’s main metadata storage. It should be\npossible to use one of the cloud PostgreSQL managed services to minimize the opera-\ntional impact. \nSummary\nThere are two types of metadata used in data platforms: business metadata and\ndata platform internal metadata, sometimes called pipeline metadata. While\nbusiness metadata is intended for use by business users, data platform metadata\nor pipeline metadata describes the data pipelines themselves. \nThis data platform metadata is fundamental to automation, monitoring, config-\nuration management, and other operations. Without it, any data platform that\n",
      "content_length": 3264,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "226\nCHAPTER 7\nMetadata layer architecture\nhas more than a couple of data sources and active pipelines will quickly become\na nightmare to manage and operate. \nThe metadata layer has three primary functions:\n– It is a central storage for all pipeline configurations.\n– It serves as a mechanism to track each pipeline’s execution status and various\nstatistics about pipelines.\n– It serves as a schema repository.\nThere are two types of pipeline metadata. The first is metadata that describes\npipeline configurations (Configuration) and the other is metadata that cap-\ntures information about what happened during pipeline execution (Activity).\nMetadata items can be grouped into four main domains: \n– Pipeline Metadata (Configuration)—Information about all existing data sources\nand data destinations, as well as ingestion and transformation pipelines\n– Pipeline Activities (Activity)—Information about pipeline success or failure sta-\ntus, durations, and various other statistics, such as the amount of data\nread/written\n– Data Quality Checks (Configuration)—Information about the quality checks\napplied to ingestion and transformation pipelines\n– Schema Registry (Configuration)—To be covered in chapter 8\nData engineers can update a configuration in the metadata layer instead of\ndirectly updating the pipeline code, which makes it easier to scale the platform.\nThey can investigate pipeline failures by requesting the latest pipeline status\nfrom the metadata layer. Data users who would like to make the most of the\ndata platform capabilities can easily see where a particular pipeline gets its\ninputs from and where it stores its outputs.\nThe metadata layer should become the primary unified interface for data engi-\nneers and advanced data users to interact with the data platform.\nThere are different approaches to architecting a metadata layer—from simply\nstoring all configuration files in cloud storage to using a dedicated database,\nwith or without a REST API. We recommend that you choose the simplest\nimplementation that satisfies your requirements today and evolve the solution\nas your requirements change. \nThere are currently no industry standards for a good pipeline metadata model,\nand most available software and cloud services don’t fully address the require-\nments described in this chapter. That said, Amazon, Google, and Azure all offer\nmetadata services. There are also several, also limited, open source metadata\nlayer implementations (Apache Atlas, DataHub, and Marquez, for example)\nthat might help accelerate your development activities. \n",
      "content_length": 2571,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "227\nExercise answers\n7.6\nExercise answers\nExercise 7.1:\n 2—Provide easy data search capabilities to end users.\nExercise 7.2:\n 3—List of all fields in the current data source.\nExercise 7.3:\n 2—It provides a common interface for multiple teams to interact with the\nmetadata.\n",
      "content_length": 273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "228\nSchema management\nIn this chapter, we will tackle the age-old problem of managing schema changes in\na data system introduced when source data changes, exploring how the increase in\nusage of third-party data sources—i.e., SaaS—and the growing use of streaming\ndata add to the challenge. \nThis chapter covers\nManaging schema changes in a cloud data platform\nUnderstanding schema-on-read vs. an active a schema-\nmanagement approach\nEvaluating when to use schema-as-a-contract vs. a smart-\npipeline approach\nUsing Spark to infer schemas in batch mode\nImplementing a Schema Registry as part of a Metadata layer\nUsing operational metadata to manage schema changes \nBuilding resilient data pipelines to manage schema changes \nautomatically\nManaging schema changes with backward and forward \ncompatibility\nManaging schema changes through to the data warehouse \nconsumption layer\n",
      "content_length": 884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "229\nWhy schema management\n We will discuss how our cloud data platform design can be used to address these chal-\nlenges—starting with leveraging the Schema Registry domain in the Metadata layer\nintroduced in chapter 7 and tackling different approaches to updating schemas in the\nRegistry—from “do nothing and wait till something breaks” to schema-as-a-contract and\nsmart pipelines.\n Because our end goal is to be able to maintain potentially hundreds of existing\ndata transformation pipelines and reports and introduce as little disturbance into the\ndownstream data consumers as possible, we will also discuss backward- and forward-\ncompatible schema changes and their potential impacts on different types of schema\nchanges.\n We’ll also discuss how to implement a Schema Registry as part of the cloud data\nplatform. This will include using Avro as a common schema format, inferring schema\nfrom incoming data and where to store the schema. We will also review the option to\nuse PaaS data catalog offerings from AWS, Azure, and Google, and three different\noptions for implementing a Schema Registry using a database with an API layer.\n Last, as the primary way users will consume the data is via a data warehouse, we will\nwalk through what happens when the schema for these data sets changes and which\npart of the data platform should be responsible for keeping data warehouse table\nschemas up to date. \n8.1\nWhy schema management\nDealing with schemas of the input data sources is a problem as old as data warehouses\nthemselves. Traditional data warehouses are based on relational technology, meaning\nthat the structure of the data—it’s column names, their types, and their order—must\nbe known up front before any data gets loaded. Any changes to the data source\nschema, such as adding new columns, must be carefully planned, so the destination\nschemas and ETL pipelines can be adjusted to accommodate this change. \n With traditional relational systems, the process of making a schema change often\ntakes hours to complete, because the data warehouse must reorganize existing data to\nfit the new schema. In an organization that operates proactively, any changes to\nsource data that require a schema change will trigger a “change request” for a data\nwarehouse schema update. It’s not uncommon in large enterprises for the schema\nupdate to take weeks or even months of planning. \n Some, usually smaller, organizations choose a different approach—they do noth-\ning and wait for things to break. In this case, upstream changes to the data sources\nhappen ad hoc, leaving the data engineering team to fix ETL pipelines when they\nbreak because of the schema change. While the proactive option is sometimes oner-\nous, the wait-and-see option can result in significant user dissatisfaction as they are typ-\nically the people who notice and report problems with the data after a schema\nchange. Regardless of how an organization chooses to deal with schema changes, they\ncan’t be ignored, and they require a significant level of manual intervention. The next\nsection will describe the required intervention in more detail.\n",
      "content_length": 3108,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "230\nCHAPTER 8\nSchema management\n8.1.1\nSchema changes in a traditional data warehouse architecture\nLet’s take a look at a simple example in figure 8.1. We have a typical traditional data\nwarehouse architecture with a single file-based data source. In a traditional data ware-\nhouse, data from the source file is always loaded into the landing table first. In data\nwarehouse terminology, a landing table is a table that is only used for ingesting new\ndata from the source, before any data transformations are applied to the data. The\nlanding table mimics the schema of the source data set to make ingestion code simpler. \nAs you can see in the figure, a landing table must have the same column names, such\nas transaction_amount and transaction_date. If we change the schema of the\nsource file and change some of the column names (for example, transaction_\namount changes to transaction_total), the ingestion process will break on the next\nrun, as shown in figure 8.2.\n At this point a data engineer gets involved and fixes the landing table definition,\nand the process is resumed until the next time a schema change happens. \nstore_id\ntransaction_date\ntransaction_amount\nstore_id\ntransaction_date\ntransaction_amount\nData source\nfile\nDW\n1. Data from the source file is\n    loaded into a landing table\n    before any transformations\n    are applied.\nData\nsource file\nschema\n2. The schema of\n    the landing\n    zone must\n    match the\n    schema of the\n    source data.\nFigure 8.1\nTo load data into a traditional data warehouse landing table, it’s schema \nmust match the source file schema.\nstore_id\ntransaction_date\ntransaction_total\nstore_id\ntransaction_date\ntransaction_amount\nData source\nfile\nDW\nIf the landing table schema\ndoesn’t match the data source\nschema, the load will fail.\nUpdated\ndata source\nfile schema\nLanding\ntable\nschema\nFigure 8.2\nIngestion breaks because of the upstream data source schema \nchange.\n",
      "content_length": 1915,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "231\nWhy schema management\n8.1.2\nSchema-on-read approach\nWhen Hadoop came on the scene as a data analytics solution, it introduced the concept\nof “schema-on-read.” The idea here is simple. Hadoop comes with its own file system\ncalled Hadoop Distributed File System, or HDFS. Instead of using landing tables that\nmust have all columns and their types defined up front in the data warehouse, in\nHadoop, you can simplify your ingestion process by saving files as is into HDFS. This pro-\ncess is resilient to the upstream schema changes, because HDFS, much like any other\nfile system, doesn’t care about the internal structure of the file itself. The “on-read” part\ncomes into play when you start processing the data from HDFS. Let’s see how our pre-\nvious example can be implemented on a Hadoop cluster. Figure 8.3 also adds a simple\ndata transformation step in SQL—calculating total sales for all stores for a given day.\nAs you can see, the ingestion process is now independent of the actual file schema.\nBut if you look closely at the SQL statement that represents our simple ETL pipeline,\nyou will see that it still has to reference specific column names. This means that the\nschema-on-read approach just pushes the problem of schema changes further down\nthe pipeline—from ingestion to the data transformation pipeline itself. Figure 8.4\ndemonstrates that while a column rename in the upstream file doesn’t break the\ningestions, it will break the ETL.\n If we start exploring this example even further, then we will arrive at a point where\ndata needs to be loaded into the data warehouse or any other data store to allow end\nusers to access it. These data stores may also need a schema to be defined and will be\naffected by the data source schema changes. At this point, you may be asking, why\ndoes Hadoop have anything to do with our cloud data platform design? It’s because\nthe cloud storage that we use as a landing area in our data platform architecture acts\nlike HDFS—it’s a distributed file storage that doesn’t have any notion of the schema.\nSELECT SUM(transaction_amount),\ntransaction_date\nFROM landing_table\nGROUP BY transation_date\nINSERT INTO result_table ...\nData source\nfile\nHadoop cluster\nHDFS\nSpark\nSQL\n2. Data source files\n    are saved as is\n    in HDFS.\n1. Expected\n   file schema\n3. Note that ETL\n    code references\n    specific column\n    names.\nstore_id\ntransaction_date\ntransaction_amount\nFigure 8.3\nIn Hadoop clusters, incoming data is saved as files on HDFS without the need to check for \nits schema first.\n",
      "content_length": 2530,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "232\nCHAPTER 8\nSchema management\nBecause of this, many data architects adopt a schema-on-read approach in their cloud\ndata platform. \n As you can hopefully see by now, the schema-on-read approach solves the ingestion\npart of the data pipeline, making it easy to land new data into the storage layer of our\ndata platform without worrying about the schema. But it doesn’t solve the schema-\nmanagement problem. Once you’ve ingested your data, you will inevitably need to\nperform data transformations and load data into other systems. That’s when it\nbecomes apparent that you need to know the schema. \n In the next section, we will explore alternatives to the schema-on-read approach. \n8.2\nSchema-management approaches\nIn a cloud data platform, the Metadata layer, introduced in chapter 7, has an import-\nant role to play in managing schema changes more easily than in traditional data\nwarehouse architectures. As shown in figure 8.5, one of the four Metadata layer\ndomains is the Schema Registry. \nstore_id\ntransaction_date\ntransaction_total\nSELECT SUM(transaction_amount),\ntransaction_date\nFROM landing_table\nGROUP BY transation_date\nINSERT INTO result_table ...\nData source\nfile\nHadoop cluster\nHDFS\nSpark\nSQL\n2. When the source\n    schema changes,\n    ingestion still\n    works.\n1. Updated\n   file schema\n3. However, ETL will\n    break as it is\n    expecting a different\n    column name, i.e.,\n    transaction_amount.\nFigure 8.4\nThe schema-on-read approach pushes the problem of schema management further down the \npipeline to the data transformation step.\nExercise 8.1\nWhich of the following best describes the schema-on-read approach?\n1\nSchema-on-read requires an upfront definition of the schema in the data platform.\n2\nSchema-on-read automatically adjusts schema definitions used in the data\nprocessing pipelines.\n3\nSchema-on-read allows you to ingest data with any schema into the data plat-\nform, but you still need to maintain the up-to-date schema for data processing.\n4\nSchema-on-read provides you with a central repository that stores schemas\nfor all your data sources.\n",
      "content_length": 2078,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "233\nSchema-management approaches\nThe Schema Registry is a repository for schemas. It contains all versions of all schemas\nfor all data sources. Data transformation pipelines or people who need to know the\nschema for a particular data source can fetch the latest version from the Registry.\nThey can also explore all previous versions of the schema to understand how a partic-\nular data source has evolved over time. \n But how do the schemas get into the Registry, and who is responsible for updating\nthe schema versions when something changes? \n8.2.1\nSchema as a contract\nAs mentioned in the previous section, if we agree that the “do nothing and wait for things\nto break” approach, often used in conjunction with schema-on-read, is not the best\napproach, there are two alternative methods to proactively handling schema changes:\ntreating schema as a contract and performing schema management in the platform.\n The first approach, treating schema as a contract, is intended to make application\ndevelopers responsible for the schema management of the data that their application\nproduces. This approach says that a schema is a contract between application develop-\ners and data consumers, be it the data platforms, other applications, microservices,\netc. In this method, shown in figure 8.6, application developers publish schemas for\nall the data their application produces into a central repository, the Schema Registry.\nConsumers fetch and use the latest schema version from the same Schema Registry.\n As a part of this contract, only backward-compatible schema changes are allowed.\nBackward compatible means that existing data consumers can use the latest version of\nthe schema to process all existing data, including the data that was produced before\nthe schema change happened. For example, adding a new column to the schema is a\nbackward-compatible change because you can still read the old data using the new\nschema and use default values for recently added columns. On the other hand,\nrenaming a column is not a backward-compatible change. We will discuss schema\ncompatibility in more detail later in this chapter.\nPipeline metadata\nPipeline activity\nData quality\nchecks\nSchema Registry\nThe Schema Registry\ncontains all versions\nof all schemas for all\ndata sources and\ndata destinations.\nFigure 8.5\nThe Schema Registry is a component of the Metadata Layer that is used to store \ninformation about schemas for all data sources and destinations.\n",
      "content_length": 2451,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "234\nCHAPTER 8\nSchema management\nThe approach of treating a schema as a contract between application developers and\nthe downstream data consumers is ideal because it provides a clear separation of\nduties and decouples data producers from data consumers. It’s easy to add new data\nconsumers because we can rely on the schema in the Registry to understand the data\nstructure. But this approach requires two things for a successful implementation: first,\nit requires a high level of maturity in your development processes, and second, it\nrequires a clear data owner for third-party data sources.\n Let’s unpack these two requirements. In order to implement a schema-as-a-contract\napproach, it is the developer’s responsibility to ensure that all schema changes are\ncompatible. As such, you’ll need to make sure that your development practices can\nsupport that. This means not only disciplined developers who actually follow the process\nand publish schema changes to the Registry, but also automated checks and guardrails\nto make sure these changes are backward compatible and won’t break downstream\nconsumers. These assurances require a high level of automation and a mature testing\ninfrastructure. It also requires a high degree of automation when it comes to your\ncontinuous integration/continuous delivery (CI/CD) processes. If developers need to\nperform a time-consuming and complex multistep protocol to deploy their code\nchanges into production, there is a high probability that schema-management steps\nwon’t be followed consistently. \n The second requirement for a schema-as-a-contract approach is to have a data\nowner for all third-party data sources. This includes all of the SaaS solutions that your\norganization is using and any other data you receive from vendors, partners, etc. Since\nyour organization doesn’t own the SaaS application that produces this data, it’s a\ntough sell to make any given development team responsible for managing schemas\nIn a schema-as-a-contract approach,\napplication developers publish schemas\nfor all the data their application\nproduces into a central Registry.\nConsumers fetch the \nlatest versions of schemas \nfrom the Registry.\nData\ntransformation\npipelines\nApplication C\nOther data\nconsumers\nSchema\nRegistry\nRDBMS\nAWS S3\nApplication B\nMetadata layer\nApplication A\nFigure 8.6\nSchema as a contract between application teams producing data and teams consuming \nthe data\n",
      "content_length": 2407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "235\nSchema-management approaches\nfor these data sources. It’s hard to take responsibility for schema changes when you\ndon’t have any control over the application.\n This level of development processes maturity is of course achievable, but in prac-\ntice we found that parts of the organizations that are tasked with designing and imple-\nmenting a data platform usually don’t have any control over the organization’s overall\ndevelopment practices and standards. This means that a data platform implementa-\ntion has to assume that the organization may have different levels of development pro-\ncesses maturity. Another caveat is that a data platform creation is often the first time\nthat an organization attempts to integrate all of its existing data sources. This means\nthat prior to the data platform implementation efforts, schema management was\nnever really a problem. This puts data architects and data engineers in charge of the\nschema-management problem, which often leads to behaviors that we described previ-\nously: rely on schema-on-read for ingestion and fix pipelines as they break because of\nthe schema changes.\n8.2.2\nSchema management in the data platform\nIf we think of the “schema on read/wait for things to break” approach (no control) to\nbe on the one end of a spectrum where we don’t do any schema management up\nfront, controlling the schema at the data sources or schema-as-a-contract (full con-\ntrol) is at the other end.\n In our practice, we have found that an approach to schema management that\nworks well for a wide variety of organizations is somewhere in between the “no con-\ntrol” and “full control” spectrum. In this middle ground, schema management is the\nresponsibility of the data platform owners. There are two reasons why we suggest this:\nSince a data platform is an integration point of internal and third-party data\nsources, it’s the only place where schemas can be managed centrally. \nData transformation pipelines are often the first to break when they encounter\nan incompatible schema change. This makes a data platform a logical place to\nhost a central schema repository and be responsible for maintaining the sche-\nmas up to date. \nNOTE\nThere is also a hybrid approach where schemas for some or all of the\ninternal data sources are managed by the development team, and the data\nplatform takes care of all third-party data sources. This mixed approach is\ncommon when your data sources are real-time. We will talk more about this\nlater in the chapter. \nPerforming schema management in the data platform itself allows you to realize the\nfollowing benefits:\nResilient ETL. For example, you will be able to detect and ideally adjust to\nschema changes before your ETL pipelines fail.\nAn up-to-date schema catalog that contains schema details, which is important\nfor data discovery and self-service use cases. \n",
      "content_length": 2844,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "236\nCHAPTER 8\nSchema management\nA history of schema changes for any given data set not only makes it possible to\nwork with archived data in your data platform because it tracks changes such as\nthe timing of the introduction or deletion of a column. It also simplifies pipe-\nline debugging and troubleshooting because you know exactly how the schema\nchanges over time.\nLet’s take a look at how schema management can be done inside the data platform\nitself. For this we need to take another look at the simplified data platform architec-\nture from chapter 7. Imagine that we have a platform that ingests data from two data\nsources: one is an RDBMS, and the other is flat files from AWS S3. We then combine\nthese two data sources and publish the output into the data warehouse. Figure 8.7\nshows the simplified architecture.\nIn our experience, schema-management steps should be implemented as the first step\nin the common data transformation pipeline. In figure 8.7, step 1 is the data ingestion\nlayer that lands data as is in the platform. Step 2 is a common data transformation\npipeline, and step 3 is a custom data transformation that joins two data sources\ntogether. Step 3 produces a new data set and should also maintain the schema for it.\nIn the next section, we will talk about a common schema-management module that\ncan be used as a part of any data transformation pipeline.\nSCHEMA-MANAGEMENT MODULE\nIn chapter 5 we described the typical steps performed by a common data transforma-\ntion pipeline: data format conversion, deduplication, and data quality checks. Now we\nneed to add one more step to this list: schema management. Figure 8.8 adds a new\nschema-management module to our Configurable Pipeline concept from chapter 5.\nCloud data platform\nMetadata layer\nSchema\nRegistry\nIn each of the transformations \nthat take place in steps 2 and 3, \nthe schema is registered or\nupdated.\nCloud data\nwarehouse\nData\ntransformations\n1\n1\n2\n2\n3\nData Ingestion\nIn step 1, data is ingested\nas is into the platform.\nRDBMS\nAWS S3\nFigure 8.7\nSchema management can be done as a part of the data platform itself.\n",
      "content_length": 2102,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "237\nSchema-management approaches\nThe schema-management module performs the following steps. First, it checks\nwhether the schema for this data source exists in the Schema Registry. If it doesn’t\nexist, this means we haven’t seen this data source before. In this scenario, the schema-\nmanagement module will perform the following steps:\n1\nInfer the schema from the incoming data (more on this later in this chapter).\n2\nRegister Version 1 of the schema in the Registry for this data source.\nIf the schema already exists, then the steps are slightly different:\n1\nFetch the current version of the schema from the Registry.\n2\nInfer the schema from the incoming data.\n3\nCompare the inferred schema to the current schema in the Schema Registry, and\ncreate a new schema version that combines the old and new definitions in a back-\nward-compatible way (more on this later in this chapter).\n4\nPublish the new schema version into the Registry for other pipelines to use.\nOne step that is common in both scenarios is the “infer schema” step. Let’s unpack\nwhat that means. Throughout this book, we use Apache Spark as our framework for\ndata transformations. Spark comes with a feature called schema inference. This means\nthat Spark can read a batch of data and try to automatically come up with a schema\ndefinition that matches this data. This works well on flat CSV files as well as highly\nnested JSON data as well. \nSCHEMA INFERENCE IN APACHE SPARK\nLet’s take a look at an example. The following listing is a sample JSON document that\nwe created using https://www.json-generator.com/.\nTypical steps performed by a\ncommon data transformation\npipeline should also include\nschema management.\nIncoming data\nPipeline name\nSource name\nSchema\nColumns to deduplicate\nQuality check rules\nQuality check severity\nThe schema-\nmanagement\nmodule\nmaintains\nschemas in\nthe Metadata\nlayer.\nMetadata\nlayer\nData format conversion module\nDeduplication module\nData quality checks module\nSchema-management module\nStaging data\nFigure 8.8\nBy adding a schema-management module to our common data transformation pipeline, \nwe can perform all schema-management tasks as a part of initial data transformations.\n",
      "content_length": 2171,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "238\nCHAPTER 8\nSchema management\n[\n  {\n    \"_id\": \"5f084f4ba8de96c3a6df5f1e\",\n    \"index\": 0,\n    \"guid\": \"d776db8c-90a4-4cc7-a136-35e09e8d7fb5\",\n    \"isActive\": false,\n    \"balance\": \"$1,702.05\",\n    \"picture\": \"http://placehold.it/32x32\",\n    \"age\": 23,\n    \"eyeColor\": \"blue\",\n    \"name\": \"Doyle Page\",\n    \"gender\": \"male\",\n    \"company\": \"STELAECOR\",\n    \"email\": \"doylepage@stelaecor.com\",\n    \"phone\": \"+1 (826) 572-2118\",\n    \"address\": \"190 Coventry Road, Riverton, South Dakota, 2701\",\n    \"about\": \"Et Lorem Lorem in aliqua irure nulla nostrud laborum veniam. \nAute cillum occaecat ad non velit eiusmod culpa id. Mollit veniam ut \nmollit consequat dolore Lorem aute voluptate ea aliquip sint anim labore \neu. Aliqua qui cillum proident ad.\\r\\n\",\n    \"registered\": \"2014-11-08T02:38:13 +04:00\",\n    \"latitude\": 60.913309,\n    \"longitude\": -81.07079,\n    \"tags\": [\n      \"velit\",\n      \"duis\",\n      \"et\",\n      \"deserunt\",\n      \"velit\",\n      \"incididunt\",\n      \"Lorem\"\n    ],\n    \"friends\": [\n      {\n        \"id\": 0,\n        \"name\": \"Terrell Donaldson\"\n      },\n      {\n        \"id\": 1,\n        \"name\": \"Freida Brooks\"\n      },\n      {\n        \"id\": 2,\n        \"name\": \"Lisa Cole\"\n      }\n    ],\n    \"favoriteFruit\": \"strawberry\"\n  } ]\nThis represents a profile of some fictitious person and contains a number of nested fields\nsuch as tags and friends. There are 21 fields in this sample document, and describing\nListing 8.1\nA sample JSON document with nested attributes\n",
      "content_length": 1484,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "239\nSchema-management approaches\na schema manually for even this simple example would be rather time consuming. In real\napplications, you might have hundreds of different attributes to deal with. \nNOTE\nWe have formatted the sample JSON document to make it more read-\nable, but for Spark to be able to parse it, each separate item in the document\nmust be a single line in a file. \nLuckily for us, Spark can infer the schema from this document automatically. In the\nfollowing listing, we use Spark Shell, an interactive command-line tool that allows us\nto enter Spark commands and see their outputs right away, without having to compile\nthe full program. Spark commands start with the scala> prompt and are followed by\nthe output. \nscala> val df = spark.read.json(\"/data/sample.json\")    \ndf: org.apache.spark.sql.DataFrame = [_id: string, about: string ... 19 more \n➥ fields]\nscala> df.printSchema    \nroot\n |-- _id: string (nullable = true)\n |-- about: string (nullable = true)\n |-- address: string (nullable = true)\n |-- age: long (nullable = true)    \n |-- balance: string (nullable = true)\n |-- company: string (nullable = true)\n |-- email: string (nullable = true)\n |-- eyeColor: string (nullable = true)\n |-- favoriteFruit: string (nullable = true)\n |-- friends: array (nullable = true)    \n |    |-- element: struct (containsNull = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- guid: string (nullable = true)\n |-- index: long (nullable = true)\n |-- isActive: boolean (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- name: string (nullable = true)\n |-- phone: string (nullable = true)\n |-- picture: string (nullable = true)\n |-- registered: string (nullable = true)\n |-- tags: array (nullable = true)\n |    |-- element: string (containsNull = true)\nNOTE\nIn this example, we are using the Spark Scala API, but it’s Python\nequivalent will only have minor syntactic differences.\nListing 8.2\nUsing the Spark Scala API to read JSON and display the inferred schema\nReads JSON document\nfrom a local file into a\nSpark DataFrame\nShows the \ninferred schema\nSpark identifies the types of the \ncolumns based on the actual data.\nfriends attribute is inferred \ncorrectly as an array.\n",
      "content_length": 2321,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "240\nCHAPTER 8\nSchema management\nAs you can see, Spark did a great job of correctly identifying the column names and\ntheir types from the actual JSON data. Of course, our example only contained one\ndocument, but the same approach works for multiple documents as well. There are\nseveral important things to keep in mind when using Spark’s schema inference:\nSpark uses a sample of all records to infer schema. For example, if you have a\nmillion JSON documents in the incoming batch, by default Spark will only use a\nsample of 1,000 documents to infer schema. This is done to improve inference\nperformance. If your documents have widely varying structures, then there is a\nhigh probability that the inferred schema will not match all of the documents\nin the batch. We recommend either significantly increasing the sample size (by\nsetting a sampleSize option in the read function) or setting your sample size to\nthe whole batch if your batches are small enough or performance is not a con-\ncern. You will need to experiment with your data to identify the sweet spot\nbetween the schema accuracy and performance for each of your data sources.\nFor example, data that comes from an RDBMS will always have the same\nschema for all rows in a given table, so a smaller sample size will work just fine.\nSpark relies on column names from your data files. For a JSON document,\nSpark will use the actual attribute names. For CSVs, you must include a header\nline with column names; otherwise, Spark will name columns like c0_, c1_, etc. \nIf you have an attribute that has different data types in different documents,\nthen Spark will try to use a generalized type that matches all values. For example,\nif in half of your documents an attribute “age” is a number, but in the other half\nit’s a string, then Spark will use a string type, because numbers can be always\nconverted to string, but not vice versa. In some cases, Spark will not be able to\nreconcile types at all if, for example, your age attribute is a number in some doc-\numents and a nested structure in others. In such cases, Spark will place rows it\ncan’t convert into a special field called _corrupt_record, where you can inspect\nthem and work with the data source owner to either split those attributes into\ntwo different ones or use the same data type in all the documents.\nThe schema that we have shown is using Spark internal types and is specific to the\nSpark framework itself. We could take that schema and save it as is in the Registry as is,\nbut because we are designing a data platform that can support a wide variety of tools,\nwe will take the approach of converting the Spark schema into an Avro schema before\nsaving it into the Registry. We have talked about the Avro file format in the previous\nchapters and will further discuss Avro schemas later in this chapter. \n Spark schema inference is a really powerful feature, and Spark itself is widely sup-\nported by the cloud providers. AWS Glue, for example, relies on Spark schema infer-\nences and adds new features on top of it. Azure Databricks and Azure Synapse services\nuse Apache Spark as a primary data transformation framework as well. But there are\ncases where you will not be able to implement schema-management steps entirely\ninside a Spark pipeline. \n",
      "content_length": 3270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "241\nSchema-management approaches\n The first scenario is if your data processing framework of choice doesn’t support\nschema inference. For example, Apache Beam, which is used by Google’s Cloud Data-\nflow service, doesn’t support schema inference from data. This means that you need\nto maintain schemas manually, outside the pipeline. The second scenario is a real-\ntime data pipeline. \nSCHEMA MANAGEMENT IN REAL-TIME PIPELINES\nThe challenge with schema inference in real-time pipelines is that schema inference\nrequires you to look at a statistically significant amount of data to decide which\nschema this data has. Remember that Spark uses a default sample size of 1000 rows to\ninfer schema for a given batch of data. In a real-time data pipeline, our processing is\nconstrained to looking at a single message at a time. We could infer the schema for a\nsingle message, but since there is no guarantee that the next message will have the\nsame schema, we will have to somehow reconcile schemas for each individual mes-\nsage. This is a computationally expensive process and will result in an extremely large\nnumber of schema versions. \n Another challenge with schemas in a real-time pipeline is that in order to achieve\nbetter performance, developers are using binary formats like Protobuf or Avro to min-\nimize the size of each individual message. To even further reduce the size of each mes-\nsage, developers often remove the Avro schema definition from the message itself (in\nmany cases, schema size can be larger than the actual message size). In this case, a\nmessage is just an array of bytes in the real-time storage like Kafka, and the schema\ncannot be inferred from it. In these cases, the schema must be saved into the Registry\nand maintained by the application development teams. \n The good news is that the schema-inference approach and the manual schema-\nmanagement approach can be easily mixed together. Batch data sources can use a\nschema-inference approach, and real-time sources can rely on the manual schema\nmanagement, where development teams are responsible for publishing schemas into\nthe Registry and updating schema versions in the case of changes.\n8.2.3\nMonitoring schema changes\nTaking steps to build a resilient data pipeline that can deal with most of the schema\nchanges automatically is important if you don’t want to spend your days fixing broken\npipelines all the time. But it is also important to have an alerting mechanism in place\nto let you know when schema changes happen. \n The problem with the schema changes is that, while it is possible to build a pipe-\nline that will keep working when the structure of the data changes, certain changes\ncan cause logical errors in the downstream reports or data products. A common sce-\nnario is when a column gets deleted or renamed at the source. It is possible to build\ndata ingestion and data transformation pipelines that will use default values instead of\nmissing columns. We will talk about how this can be achieved in section 8.4, “Schema\nevolution scenarios,” but if we have a report that our business users rely on that\n",
      "content_length": 3098,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "242\nCHAPTER 8\nSchema management\nexpects a certain column to be present, and it starts to receive only default values all of\na sudden, the logic of this report will be broken. \n If we go back to our earlier example where we renamed transaction_amount to\ntransaction_total column and assume that we have built a resilient pipeline that uses a\ndefault value of “0” when it can’t find a transaction_amount column, then at some\npoint our report that calculates total sales per day will start showing zero sales. This is\nclearly a logic error, because the data is there; it’s just in a different column.\n As you can see, there are certain cases where we can’t automate dealing with\nschema changes, and what we need is an alerting mechanism that will let us know that\na schema change can cause issues with the downstream pipelines. If we don’t have this\nmechanism in place, our business users will discover these issues for us, and this will\nsignificantly erode the trust they have in the data platform and the quality of data in it. \n In chapter 7, we discussed the different domains in our Metadata layer. One of\nthese domains is Pipeline Activities. Pipeline Activities captures information about\nwhat happened during a pipeline execution: how much data did the pipeline read,\nwere there any errors, etc. Capturing schema change events as a Pipeline Activity\nbecomes really important if we want to monitor for these events. Figure 8.9 shows how\na common data transformation pipeline can register schema change events in the\nMetadata layer.\n Our common schema-management module can detect the schema changes and\npublish new versions of the schema into the Registry (part of the Metadata layer). It\ncan also record that the change happened into the Pipeline Activities domain of the\nCommon data transformation pipeline\nOther common modules\nSchema-management module\nProvides\nconfiguration to\nthe pipeline\nStaging data\nIncoming data\nMetadata\nlayer\nThe schema-management\nmodule maintains\nschemas in the\nMetadata layer.\nThe schema-management\nmodule saves schema\nchange events into the\nPipeline Activities area\nof the Metadata layer.\nActivities are sent to a cloud Log\nAggregation service, where they\ncan be analyzed and monitored\nfor specific events.\nCloud Log\nAggregation\nservices\nFigure 8.9\nThe schema-management module publishes information about schema change \nevents into the Pipeline Activities area of the Metadata layer, which ends up in a cloud Log \nAggregation service.\n",
      "content_length": 2470,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "243\nSchema Registry Implementation\nMetadata layer. From an implementation perspective, this can be a log file that gets\naggregated into one of the cloud log management solutions. There you can analyze\nthis data and build alerts that will notify the team responsible for managing the data\nplatform about a schema change.\n If your data platform is small and has only a handful of pipelines and reports, you\ncan easily identify the reports that can be affected by the schema changes and warn\nthe end users that the data in the report might not be valid, and you need to do some\nmaintenance on the pipelines. This is a much better user experience than users dis-\ncovering issues on their own and telling your team the data is wrong. Some logical\nissues can be rather subtle and not detected by the end users at all. \n As your data platform grows and you add more and more pipelines and reports,\nmanually figuring out which reports are affected can become time consuming or even\nimpossible. In this case, you can utilize pipeline configurations from the Metadata\nlayer. If you recall from chapter 7, pipeline configurations contain information about\nwhich data sources are used by which data transformation pipelines. If you know\nwhich data sources are affected by the schema change, you can easily identify all\ndownstream transformations and reports that are affected.    \n8.3\nSchema Registry Implementation\nBefore we talk about options to implement a Schema Registry as a part of your cloud\ndata platform, we need to discuss how to actually represent and store schemas. As you\nprobably realized by now, a “Schema Registry” is not a common concept in the data-\nprocessing world. Relational databases use schemas, but each vendor has its own ways\nto describe table schemas using different types, etc. CSV and JSON files only include\nattribute names and don’t include any type information for them, so their schemas\nare only partially represented. To be able to work with data that comes from a multi-\ntude of different sources, we need a schema that includes attribute names, their types,\nand default values. \n8.3.1\nApache Avro schemas\nIn chapter 5 we talked about using Apache Avro (https://avro.apache.org/) as a com-\nmon file format for all the data in our data platform. Avro describes data using its own\nschema, and because we are trying to standardize on a single data format, it makes\nsense for our platform to adopt Avro schemas as a common schema format. Avro sche-\nmas support most common primitive types: strings, integers, float, null, etc. They also\nsupport complex types such as records, arrays, and enums. This makes it possible to\nuse Avro to describe all kinds of data sources—from data that arrived from RDBMSs\nand mostly uses primitive types to various JSON documents that use complex nested\nattributes. \n The following listing is an example of an Avro schema definition for the sample\nJSON document that we used previously.\n \n",
      "content_length": 2941,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "244\nCHAPTER 8\nSchema management\n{\n   \"type\":\"record\",\n   \"name\":\"sampleUserProfile\",\n   \"fields\":[\n      { \"name\":\"_id\", \"type\":[\"string\",\"null\"]},  \n      { \"name\":\"about\", \"type\":[\"string\",\"null\"]},\n      { \"name\":\"address\", \"type\":[ \"string\",\"null\"]},\n      { \"name\":\"age\", \"type\":[\"long\", \"null\"]},\n      … \n      {\n         \"name\":\"friends\",\n         \"type\":[{\"type\":\"array\",\"items\":[ \n                  {\n                     \"type\":\"record\", \n                     \"name\":\"friends\",\n                     \"namespace\":\"sampleFriendsRecord\",\n                     \"fields\":[{\"name\":\"id\",\"type\":[\"long\",\"null\"]},   \n                               {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]\n                  },\n                  \"null\"\n               ]\n            },\n            \"null\"\n         ]\n      },\n      …\nAvro schema definitions are human readable, which means you can create these sche-\nmas manually for some data sources where schema inference can’t be used. \n As we discussed in section 8.2.2, if you are using schema inference in Spark, you\nneed to convert the Spark schema into an Avro schema somehow. Spark provides a\nconvenient method in the Scala API to do that, as shown in the following listing.\nimport org.apache.spark.sql.avro.SchemaConverters\nval df = spark.read.json(\"/data/sample.json\")   \nval avroSchema = SchemaConverters.toAvroType(df.schema, false, \n➥ \"sampleUserProfile\")  \navroSchema: org.apache.avro.Schema = {\"type\":\"record\",\"name\":\n➥ \"sampleUserProfile\",\"fields\":[{\"name\":\"_id\",\"type\":[\"string\",\"null\"]} …\nAs you can see from this example, the output of the toAvroType method is of\norg.apache.avro.Schema type, and it looks exactly like the example Avro schema we\nListing 8.3\nAvro schema definition for our sample JSON document\nListing 8.4\nConverting Spark schema into Avro schema\nAvro schema definitions can \ncontain columns with primitive \ntypes such as string, integers, etc.\nThe “friends” attribute in this \nexample is an array, meaning it \ncan contain multiple values.\nEach item in the\n“friends” array is of\ntype record. This is\nused to describe\nnested values.\nEach item in the “friends” array\ncontains a record with two\nattributes of primitive types.\nImports helper object to \nperform schema conversions\nReads a sample JSON document \ninto a Spark dataFrame\nUses a “toAvroType” method to convert \na Spark schema into an Avro schema\n",
      "content_length": 2371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "245\nSchema Registry Implementation\npreviously showed. We have omitted the full schema output in this listing for brevity, but\nif you try this using a Spark shell yourself, you will see the full Avro schema definition.\n Beside serving as a common format for schema definitions, Avro also supports\nschema evolution. Schemas for the data sources are always changing (otherwise, we\nwouldn’t be having this conversation), so being able to reflect how schema changed\nover time is very helpful. We will talk in detail about schema evolution examples later\nin this chapter. \n Once we have obtained the Avro schema, the next thing to do is to store it some-\nwhere other pipelines, human operators, or monitoring tools can fetch it from. That\nplace is a Schema Registry. A Schema Registry essentially is a database that allows you to\nstore, fetch, and update schema definitions. As we saw in the previous examples, the\nAvro schema definition is just a text describing attributes, their types, etc. Actually, the\nAvro schema definition itself is a valid JSON document. This means any database that\nis capable of storing JSON data would work as a Schema Registry implementation.\n8.3.2\nExisting Schema Registry implementations\nBefore we start talking about how you can implement your own Schema Registry in\nthe cloud, we need to take a look at existing solutions. When talking about ways to\nstore pipeline metadata in chapter 7, we discussed the following cloud services that we\nhave put into a broad category of “data catalogs”:\nAWS Glue Data Catalog\nAzure Data Catalog\nGoogle Cloud Data Catalog\nAll these services offer some capabilities to store the schema of various data sources.\nIn our experience, we have found all of them to be quite limiting when it comes to the\nability for pipeline developers to publish, version, and retrieve schemas. Both Azure\nData Catalog and Google Cloud Data Catalog are focused on automatic discovery of\nexisting data sources to provide end users with a search interface for data discovery.\nFor example, Azure Data Catalog allows you to register not only data sources, but also\nexisting reports, which makes it particularly useful as a data discovery tool. What’s\nmissing from these data catalog solutions is the ability to version schemas and to use a\ncommon schema format such as Avro. AWS, Azure, and Google Cloud do offer APIs to\nupdate schemas, but there is no versioning information, and each data catalog solu-\ntion uses its own way to represent schemas. This means that as a data developer you\nwill need to convert Spark schemas into Avro, which you will still need to do if you\nwant a compact binary format to store files in the cloud storage and then convert Avro\ninto a specific schema representation expected by the data catalog tool.\n Out of these three data catalog solutions, AWS Glue Data Catalog comes the clos-\nest to being able to serve as a Schema Registry in our design. It supports discovering\nschemas from common file formats such as JSON and Avro and updating schemas via\nan API. It doesn’t support (at least in the current version) retrieving old versions of\n",
      "content_length": 3109,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "246\nCHAPTER 8\nSchema management\nthe schemas, though. As mentioned in previous chapters, AWS Glue Data Catalog\nworks well if you decide to implement the whole transformation layer using the AWS\nGlue service. If you want to use just the Data Catalog portion of it and implement the\npipelines yourself using Apache Spark or any other data processing framework, then\nthe limitations become quite apparent. \n If we look into existing open source solutions for Schema Registry we, unfortu-\nnately, won’t find too many. Confluent Schema Registry (http://mng.bz/7V5m) is the\nonly solution that we came across in the past that ticks all the boxes when it comes to\nthe Schema Registry features. It supports versioning and the Avro schema format, has\na proper API, and even supports schema evolution rules to ensure that changes to the\nschema don’t break existing pipelines. \n One big challenge with the Confluent Schema Registry is that it requires Kafka to\nwork. It was developed specifically for real-time processing use cases, and while you\ncan use its API to register schemas for any type of data source, you do need to have a\nKafka cluster up and running. So if you don’t yet have a real-time component in your\ndata platform, or you are using a cloud-specific real-time storage service such as AWS\nKinesis or Google Cloud Pub/Sub, then you will not be able to use this tool. You\nshould be able to use Confluent Schema Registry with the Azure Events Hub because\nit provides an API that is compatible with Kafka.\n It is also important to note that Confluent Schema Registry is distributed under\nConfluent’s own Community License, which is different from most common open\nsource licenses such as Apache v2 or MIT. \n8.3.3\nSchema Registry as part of a Metadata layer\nIn chapter 7 we talked about options for implementing a Metadata layer that will host\npipeline configurations, pipeline activity information, and data source schemas. We\nhave described three solutions that are gradually increasing in complexity and sup-\nported features:\nUse a plain-text pipeline configuration file and a code repository to version it.\nUse a key/value or a relational database to store pipeline configs and other\nmetadata.\nAdd a REST API layer on top of the database to provide a uniform interface for\nall tools in the platform.\nBecause a Schema Registry is a logical part of the Metadata layer, it makes sense for us\nto use the same approach for implementing it as we described in chapter 7. We have\nalready mentioned that a Schema Registry is really just a database for schemas and\ntheir versions. We will not repeat here the same three options that we described here,\nbut we’ll focus on the last one, which has a database and an API layer on top of it. You\ncan still use just a database without an API layer if the number of tools and teams inter-\nacting with the Schema Registry are low. Using text files and a code repository to store\nschemas, similar to the simplest option for pipeline configuration, will not work here,\n",
      "content_length": 3005,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "247\nSchema Registry Implementation\nbecause in our design, schemas are updated automatically by the pipelines themselves.\nFigure 8.10 shows how different tools will interact with the Schema Registry.\n When it comes to the actual database, the same key/value services that we men-\ntioned in chapter 7 will work for the Registry database as well: Azure CosmosDB, Goo-\ngle Cloud Datastore, or AWS DynamoDB. In fact, when implementing a Schema\nRegistry in the past, we have often had the same database for the pipeline metadata\nand the Schema Registry. Sometimes, you may need to use separate instances of Cos-\nmos DB, Datastore, or DynamoDB for the pipeline metadata and the schemas. For\nexample, if you are using a hybrid scenario where some of that data source’s schemas\nare managed by the data platform, and some are managed by the application teams,\nyou may want the application team to only have permissions to access the schema\ndata, but not the pipeline configuration data. Fortunately, the cloud makes it easy to\ncreate multiple instances of these data stores and configure granular access to them. \n Schema Registry operations, whether performed directly on the database or imple-\nmented via the API layer, can be summarized like this:\nFetch the current version for a given data source.\nCreate a new schema for a data source if the current version doesn’t exist. This\nis used to register new data sources.\nAdd a new version of the schema for an existing data source.\nIn chapter 7 we described which attributes different entities in the Metadata layer,\nsuch as Namespaces, Pipelines, Data Sources, etc., should have. Now we can extend\nthis list with attributes that you will need to store in the Schema Registry:\nID—Identifier for the schema. This ID is linked back to the Sources and Desti-\nnation entities in the Metadata layer.\nIngestion pipelines\nregister schemas for\nnew data sources and\nupdate schemas for\nexisting sources via a\ncommon schema-\nmanagement module.\nApplication teams can\nuse the same API to\nregister and maintain\nschemas for data their\napplications produce.\nTransformation\npipelines read\nschemas for the data\nsources they need\nand register schemas\nfor the new datasets\nthey produce.\nMonitoring tools can\nperiodically check for\nnew versions of the\nschema and produce\nalerts or reports.\nIngestion\npipelines\nTransformation\npipelines\nSchema\nRegistry DB\nSchema Registry\nAPI service\nMonitoring tools\nApplication\nteams\nFigure 8.10\nA Schema Registry with an API layer on top of it can be used both by pipelines \nthat are internal to the data platform as well as external teams and tools.\n",
      "content_length": 2608,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "248\nCHAPTER 8\nSchema management\nVersion—A number indicating versions of this schema. Together with the ID\nattribute, this forms a unique schema key.\nSchema—Text attribute that stores the Avro schema definition.\nCreated Timestamp—Date and time of when this schema was first created.\nLast Updated Timestamp—Date and time of when this schema was last updated.\nNote that here ID is not unique, because you can have multiple versions of the same\nschema. As we discussed previously, schemas of the data sources we are dealing with are\nconstantly changing. We need to capture each modification of the schema so we can\nwork with data that was produced with that version of the schema, but also it is import-\nant to know how the schema changed over time for debugging and troubleshooting\npurposes. We don’t assign a new ID to each schema version because then we would\nneed to go and update all Sources and Destinations entities in the Metadata layer.\n8.4\nSchema evolution scenarios\nNow that we know how to infer schema from the incoming data and how to store new\nversions of it in the Schema Registry, we need to discuss what are the most common\nscenarios for schema evolution. Schema evolution is a commonly used term to\ndescribe how programs that process data deal with changes to structure of the data. In\nthe case of the data platform, we need to understand how our data pipelines will work\n(or not work!) when the schema of a certain data source changes. \n Here are the most common examples of schema changes:\nAdding a new column\nDeleting an existing column\nRenaming a column\nChanging the column’s type\nWhen talking about schema evolution, it is important to keep the larger context in\nmind. Our goal is not only to be able to read the data with the new schema, but also to\nmaintain potentially hundreds of existing data transformation pipelines and reports\nand introduce as little disturbance into the downstream data consumers as possible.\n There are two types of schema changes: backward compatible and forward com-\npatible. Backward-compatible schema changes mean that if our data transformation\npipelines use the latest version of the schema from the Registry, they should be able to\nread all data that is already stored in the platform even if that existing data was written\nusing older versions of the schema. This process is illustrated in figure 8.11.\n In this example, we are dealing with a single data source that has an initial schema\n(V1) that consists of two columns. Let’s imagine our ingestion pipelines have worked\nfor some time, and we have already stored some archive data that uses V1 schema.\nNow at some point, a new column was added to this data source and we have created a\nV2 of the schema and started writing data using this version.\n",
      "content_length": 2763,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "249\nSchema evolution scenarios\nIn chapter 5 we introduced two types of data processing pipelines: a common data\nprocessing pipeline and a custom data processing pipeline. As a quick reminder, a\ncommon data processing pipeline is concerned with the following:\nFile format conversion\nData deduplication\nData quality checks\nIn this and the next example we assume that the data processing pipeline is a com-\nmon one, because it needs to be able to deal with different schemas coming out of\ndata sources but doesn’t really need to apply any customer business logic. We will talk\nabout the custom data processing pipelines and how they can deal with the schema\nchanges later in this chapter.\n Let’s also assume that our data processing pipeline automatically picks up the lat-\nest version of the schema for each source from the Registry. What will happen if we\nneed to reprocess data written with the V1 schema using the V2 schema?\n8.4.1\nSchema compatibility rules\nThis is where schema backward-compatibility rules come into play. Avro format\ndefines several rules that make schemas backward compatible. In our example, if a\nnew column that we have added in the V2 of the schema, column_3 has a default value\ndefined, then this schema change is backward compatible. If we read archive data that\nwas written using the V1 schema, that doesn’t have column_3 present, using the V2\nBackward-compatible schema changes mean\nthat if our data transformation pipelines use\nthe latest version of the schema from the\nRegistry (V2), they should be able to read all\ndata that is already stored in the platform\neven if that existing data was written using\nolder versions of the schema.\nThe data ingestion layer\nalways writes data with\nthe latest version because\nit reads it directly from\nthe data source.\nCommon data\nprocessing pipeline\n(using V2 schema)\nSchema V2\nSchema V1\nVersion 1:\ncolumn_1\ncolumn_2\nSchema\nRegistry\nVersion 2:\ncolumn_1\ncolumn_2\ncolumn_3\nData with schema\nV1\nData with schema\nV1\nData with schema\nV2\nData ingestion layer\nFigure 8.11\nBackward-compatible schema changes mean that you can use schema V2 to process \nthe data that was written by schema V1.\n",
      "content_length": 2152,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "250\nCHAPTER 8\nSchema management\nschema, then Avro will automatically use a default value for column_3. Often an empty\nor \"null\" value is used as a default, but it can be any value that matches the column\ntype. If you recall, our example Avro schema for a user profile JSON document, then\nyou can see how default values are used in Avro: { \"name\":\"age\", \"type\":[\"long\",\n\"null\"]}. Here we define a column called \"age\" that is of \"long\" type and has a\ndefault value of \"null\".\n Another case for schema evolution supported by Avro is a forward-compatibility\nscenario. A schema change is forward compatible if you can use an older version of\nthe schema to process data that was written with the newer schema. In our previous\nexample, our data processing pipeline always used the latest version of the schema\nfrom the Registry. Let’s take a look in figure 8.12 at what will happen if our pipeline\nwill use the current version of the schema to process the new incoming data.\nLike in the previous example, we add a new column in the V2 version of the schema\nand have some archive data written with the V1 version, but our pipeline keeps using\nthe V1 schema instead of immediately switching to V2. As you can imagine, the pipe-\nline can reprocess the archive data without any issues because it’s using the same\nschema as the data was written with. What will happen if our pipeline tries to process a\nnew incoming batch that was written with a new column in it? \n In Avro, adding a column to the schema is a forward-compatible change. Our data\nprocessing pipeline that uses the V1 schema will simply ignore any new columns that\nwere added and keep reading the data as if the new columns were not there. This\ncompatibility feature of Avro helps to keep existing pipelines running when faced\nwith schema changes and lets the data engineering team adjust the pipelines to start\nusing new schemas at a later time.\nA schema change is\nforward compatible if you\ncan use an older version\nof the schema to process\ndata that was written\nwith the newer schema.\nCommon data\nprocessing pipeline\n(using V1 schema)\nVersion 1:\ncolumn_1\ncolumn_2\nSchema\nRegistry\nVersion 2:\ncolumn_1\ncolumn_2\ncolumn_3\nData with schema\nV1\nData with schema\nV1\nData with schema\nV2\nData ingestion layer\nFigure 8.12\nForward-compatible schema changes mean that you can use schema V1 to process \nthe data that was written with schema 2.\n",
      "content_length": 2385,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "251\nSchema evolution scenarios\nNOTE\nAs discussed earlier in this chapter, monitoring for schema changes\nusing a Pipeline Activities log is very important. While your pipelines can\nignore new columns and keep working until you make the necessary adjust-\nments, some of the data users might expect the new columns to be added as\nsoon as possible. It’s always a good idea to be one step ahead of your users\nand let them know that you are aware of the change and provide an estimate\non when the new column will become available in the downstream data sets.\nWe have talked about adding and deleting columns so far. Another common type of\nschema change is renaming an existing column. You can probably guess by now that\nrenaming behaves like adding a new column with the new name and deleting a col-\numn with the old name. In Avro, backward- and forward-compatibility rules for\nrenaming would be the same as combined rules for adding and deleting columns. If\nyou rename a column that has a default value, then this change is both forward and\nbackward compatible. If you rename a column that doesn’t have a default value, then\nthis change is neither backward nor forward compatible.\n The last type of schema change is changing the column type. Out of the box, Avro\nsupports “promoting” certain data types to other compatible data types. The key\nrequirement here is not to lose any data. For example, Avro can promote an int type\nto long, float, and double types, but not the other way around. This is because if you\ntry converting a long, which is a 64-bit integer, to an int, which is a 32-bit integer, you\nmay end up with values that don’t fit into an int type. You can find the full list of which\nAvro data types can be automatically promoted to other data types here: http://\nmng.bz/mgRP.\n There are other scenarios for the data types conversions that Avro doesn’t support\nautomatically (meaning without you having to write any code), but that can be rela-\ntively easily implemented in the common schema-management module that we have\ntalked about previously. For example, any number can be represented as a string (the\nreverse is not true) and any single data point can be represented as an array of one\nelement. There are more examples, and depending on your environments and types\nof the schema changes that you see most frequently, you can decide to implement\nadditional data type conversions. In our experience, it’s a good idea to stick to the\nautomatic data type conversions that Avro provides, because implementing custom\ntype conversion will add complexity to downstream data processing pipelines.\n8.4.2\nSchema evolution and data transformation pipelines  \nSchema changes can have a significant impact on the downstream data processing\npipelines and reports or other types of analytics that users perform on your data plat-\nform. Now that we know about Avro schema compatibility rules, we can discuss what\nthis means for the data transformation pipelines.\n Previously in our schema compatibility examples, we have said the pipeline in\nquestion is a common data transformation pipeline, which includes steps that are\ngeneric for all data sources: file format conversion, deduplication, etc. Such pipelines,\n",
      "content_length": 3212,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "252\nCHAPTER 8\nSchema management\nof course, need to be able to read the latest and archive data (in case of reprocessing)\nand need to be able to deal with the schema changes. But they don’t perform any\nbusiness logic that requires specific columns to be present or these columns to have\nspecific types. This means it’s easier to build a resilient common data transformation\npipeline because it is not affected by schema changes as much.\nNOTE\nAn exception here is data deduplication on a particular column. If\nthat column gets deleted, the deduplication process will need to be adjusted.\nThings are more complicated for the data transformation pipelines that actually\nimplement business logic. Let’s modify our previous example and use a data transfor-\nmation pipeline that uses a simple aggregation in figure 8.13.\nIn this example, we have a data transformation pipeline that calculates a sum of\ncolumn_2 values for each unique column_1 value. Let’s say in the V2 of the schema a\nnew column_3 was added, and column_2 was removed. Or column_2 was renamed to\ncolumn_3, which would result in the same schema. What happens to our pipeline then? \n If our data transformation pipeline switches to using the latest version of the\nschema, it will fail. New incoming data batches will not contain column_2, and the\npipeline will produce an error. If the data transformation pipeline sticks to using the\nprevious version of the schema, then results will depend on whether column_2 has a\ndefault value defined. If it does, the pipeline will keep working because it will use the\ndefault value in place of the missing column. If column_2 doesn’t have a default value,\nthe pipeline will fail with an error, because column_2 is declared in the V1 of the\nschema, but is missing from the new data, and there is no default value to use. \nIn this scenario when\ncolumns are deleted\nand a new one is added,\nthe data transformation\npipeline must keep\nusing V1 of the schema\nto keep working.\nThe data ingestion layer always\nwrites data with the latest\nversion because it reads it\ndirectly from the data source.\nSELECT\nSUM(column_2)\nFROM data\nGROUP BY column_1\nVersion 1:\ncolumn_1\ncolumn_2\n(default:0)\nSchema\nRegistry\nVersion 2:\ncolumn_1\ncolumn_2\ncolumn_3\nData with schema\nV1\nData with schema\nV1\nData with schema\nV2\nData ingestion layer\nFigure 8.13\nData transformation pipelines that perform actual calculations on specific columns \nhave to keep using previous versions of the schema to keep them from breaking.\n",
      "content_length": 2487,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "253\nSchema evolution scenarios\n Table 8.1 summarizes the types of the schema changes in relation to the forward\nand backward compatibility. \nAs you can see in the preceding table, adding a new column with a default value is the\nsafest schema change operation. Your existing pipelines will keep working and will\nignore the new column until you make changes to the pipeline’s logic. Deleting a col-\numn that has a default value is also a mostly safe operation. You will need to make sure\nthat your transformation pipeline uses the previous version of the schema though.\nRenaming a column is only safe when a column has a default value. Otherwise, it has\nthe same properties as adding a column without a default value and deleting a col-\numn without a default value. Finally, changing column type, as described earlier,\ndepends on the actual types. \n Should pipelines switch to the latest version of the schema or keep using the previ-\nous version until an engineer makes the switch? Based on table 8.1 and our experi-\nence, it’s better to keep using the previous version of the schema and only switch to\nthe new version once all required changes are made to the pipelines. In this case, your\npipelines will continue working for any schema changes that are forward compatible.\nIf you can negotiate with the owners of the data sources and have them agree to only\nmake changes that are safe (meaning forward compatible), then you will end up with\na resilient pipeline. \nNOTE\nWhen using the schema inference feature of Spark, all columns get a\ndefault value of “null” (empty). This reduces the chances of pipelines break-\ning due to the incompatible schema changes. You may need to update default\nvalues for certain columns manually to better reflect business logic in your\npipelines.\nTable 8.1\nForward and backward compatibility associated with different types of schema changes\nSchema change\nBackward compatibility\nForward compatibility\nSafe for transformations\nAdding a column with \na default value\nYes\nYes\nYes\nAdding a column with-\nout a default value\nNo\nYes\nYes\nDeleting a column \nwith a default value\nYes\nYes\nYes, if you use the previ-\nous schema version\nDeleting a column \nwithout a default value\nYes\nNo\nNo\nRenaming a column \nwith a default value\nYes\nYes\nYes, if you use the previ-\nous schema version\nRenaming a column \nwithout a default value\nNo\nNo\nNo\nChange column type\nSometimes\nSometimes\nSometimes\n",
      "content_length": 2403,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "254\nCHAPTER 8\nSchema management\nLOGICAL ERRORS IN THE DATA TRANSFORMATION PIPELINES DUE TO SCHEMA CHANGES\nIt’s important to understand that when we talk about building pipelines that are resil-\nient to the schema changes, we mean pipelines that keep running and don’t fail with\nerrors when the schema changes. Certain schema changes can impact the business\nlogic in your data transformation pipelines and produce incorrect results. To illustrate\nthis problem, let’s take a look at our previous example, but now instead of abstract\ncolumn_1 and column_2, use a retail scenario. Let’s say we are ingesting a table from\nan RDBMS that contains daily sales for a number of different stores in our retail chain\nempire. We have a data transformation pipeline running that generates a report with\na total amount of sales per each unique store_id. At some point, the application devel-\nopment team decides to change the name of the column that contains daily sales\nnumbers. Figure 8.14 shows a schema change example.\nLet’s say we have implemented all the best practices of building a resilient pipeline: we\nare using Avro as our file format, we have a Schema Registry, and we make sure our\npipelines keep using previous versions of the schema until we update them. In this\ncase, our daily_sales column in the V1 schema has a default value of NULL (empty\nvalue). The schema changes, and all new incoming data has a V2 schema where the\ndaily_sales column is deleted and a total_day_sales column is added. Our simple\ntransformation pipeline will keep working, because when it reads new data it will use a\nNULL value for the daily_sales column. But our report will start showing NULL for\n1. The schema changes, and all new\n    incoming data have a V2 schema where\n    the daily_sales column is deleted and\n    a total_day_sales column is added.\n2. Our simple transformation pipeline will keep working, because when it reads\n    new data it will use a NULL value for the daily_sales column. But our report\n    will start showing NULL for the sum_sales because in SQL if you add a NULL\n    value to a non-NULL value the result is NULL.\nVersion 1:\nstore_id\ndaily_sales\n[default NULL]\nVersion 2:\nstore_id\ndaily_sales\ntotal_day_sales\nSchema\nRegistry\nSELECT\nSUM(daily_sales)as sum_sales\nFROM store_sales\nGROUP BY store_id\n1\n2\n3\nNULL\nNULL\nNULL\nstore_id\nsum_sales\n100\n250\n385\n1\n2\n3\nstore_id\nsum_sales\nFigure 8.14\nRenaming the daily_sales column can cause incorrect or at least unexpected \nreport results.\n",
      "content_length": 2484,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "255\nSchema evolution and data warehouses\nthe sum_sales because in SQL if you add a NULL value to a non-NULL value, the\nresult is NULL. Your business users will be really surprised to see a report that shows\nempty values for total sales. Using “0” as a default value will prevent us from seeing\nNULLs in the sum_sales column, but it will look like no new sales are happening\nbecause existing numbers will be unchanged even as new data arrives. \n As you can see, even if our pipelines keep working, the business logic in them may\nbe broken when the schema changes. Unfortunately, there is no simple solution to\nthis problem. As discussed previously, it is important to have some alerting in place to\nlet you know that schema changes happened so you can review existing pipelines and\nmake adjustments when necessary and inform your users that certain reports may not\nbe correct in the meantime.\n8.5\nSchema evolution and data warehouses\nSo far we have talked about how to handle schema changes in our data transformation\npipelines, but in our data platform architecture, the primary way users will consume\nthe data is via a data warehouse. As we load new incoming data or results of the trans-\nformations into the data warehouse tables, we also need to think about what will hap-\npen when the schema for these datasets changes. \n There are differences between how different cloud data warehouses behave when\nit comes to schema changes, and we will talk about this later, but there are also differ-\nences in approaches to schema management between data transformation pipelines\nand data warehouses. \n Our data transformation pipelines deal mostly with files (unless it’s a real-time\npipeline). When using Avro as a file format, we get the benefit of having the schema\ndefinition that is stored together with each file. This means we can have files with dif-\nferent schema versions stored in our cloud storage and then have our pipeline rely on\nthe compatibility rules to reconcile these different versions of schemas. \n Data warehouses, including the ones that are developed by the cloud vendors,\nwork differently. Data warehouses store all their data in tables, and these tables must\nExercise 8.2\nWhat makes a schema change backward compatible?\n1\nA schema change is backward compatible if it only includes adding new columns.\n2\nA schema change is backward compatible if all schema versions use the\nsame column names.\n3\nA schema change is backward compatible if you can use the latest version of\nthe schema to read the data that was written using the previous versions of\nthe schema.\n4\nA schema change is backward compatible if you can take the oldest schema\nversion and use it to read the data that was written using more recent ver-\nsions of the schema.\n",
      "content_length": 2751,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "256\nCHAPTER 8\nSchema management\nhave a defined schema. There can’t be multiple versions of the schema for the same\ntable. This has two consequences for the schema evolution scenarios:\nAs the schema of the data sources or data products changes, we need to adjust\ncorresponding table schemas in the data warehouse. \nData warehouse table schemas must accumulate changes over time and not\napply irreversible changes.\nExisting data warehouses don’t integrate with external Schema Registries. This means\nthat while you will use schema inference and store new versions of the schemas in the\nRegistry, for your data pipelines to work you will need to make corresponding changes\nto the data warehouse table schemas separately. For example, if a new column is\nadded to some data source and you want this column to be available in the data ware-\nhouse, you will need to add a corresponding column to the data warehouse. \n You also need to treat schema changes to the data warehouse tables differently\nwhen it comes to changes like column deletion or column renames. If a column gets\ndeleted from a source table, you don’t want to delete it from the data warehouse table\nbecause data warehouse tables contain all data, including historical data. If we delete\nthe column from the data warehouse, we lose the data for this column. This means\nthat changes to the data warehouse schema should be accumulative—we can add new\ncolumns or change column types, but we should not delete columns.\nNOTE\nThis is not a problem with data that we store in cloud storage, because\nschema changes only affect new incoming data. We don’t modify existing\narchive data when a column is deleted.\nBut which part of the data platform should be responsible for keeping data warehouse\ntable schemas up to date? Obviously, we don’t want to do it manually. Previously, we\ntalked about creating a schema-management module that will be used as a part of the\ncommon data transformation pipeline. The same module can be used to manage the\ndata warehouse schema changes. \n Because we have previous and current versions of the schema in the Schema Regis-\ntry, we can create an automated way to generate necessary SQL statements to update\nthe data warehouse table definitions. Let’s take another look at our example of a col-\numn rename. Figure 8.15 shows how the schema-management module can be used to\nmaintain data warehouse table definitions.\n In this example, column_2 gets renamed into column_3. In terms of schema\nchange, this can also be represented as deleting column_2 and creating a new\ncolumn_3. Remember that we don’t want to delete existing columns from a data ware-\nhouse table because there might be historical data in that column that is still valuable.\nWe omit the deletion portion and generate a SQL statement that will just add a new\ncolumn_3 to the table. \n Automating schema changes in the data warehouse means that you will need to\nwrite custom code in the schema-management module that will map Avro data types\ninto the data types of the data warehouse of your choice. Schema change commands\n",
      "content_length": 3070,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "257\nSchema evolution and data warehouses\nfor different data warehouses may have different syntax, so you will need to take that\ninto account as well. \n Sometimes you may want to simplify the schema-management process for the\ntables in the data warehouse, and instead of adjusting existing tables on the go, just\ndelete the existing table and create a new one with a new schema and load all the\ndata, including historical data from the cloud storage. This works well for small tables\nthat don’t take a lot of time to rebuild.\n It’s also important to understand how the cloud data warehouse of your choice\nbehaves when you change existing table schemas. Many existing data warehouses will\nmake the table unavailable for any queries while changing the schema. This means\nthat your reports or users who run queries against the data warehouse will have to wait\nfor the change to complete. Depending on the size of the table and complexity of the\nchange, it can take minutes or even hours to run. In the next section, we will take a\nbrief look at schema-management features of AWS Redshift, Google Cloud BigQuery,\nand Azure Synapse. \n8.5.1\nSchema-management features of cloud data warehouses\nImplementations of existing cloud data warehouses differ significantly and offer dif-\nferent ways to deal with schema changes. Unfortunately, cloud vendors rarely publish\nenough details about the internal workings of their data warehouses for us to fully\nunderstand how schema changes are implemented under the covers. We can only\nmake certain assumptions based on the existing documentation. \nSchema Registry\nCommon data transformation pipeline\nVersion 1:\ncolumn_1\ncolumn_2\nVersion 2:\ncolumn_1\ncolumn_2\ncolumn_3\nCommon modules\nThe schema management module\nALTER TABLE some_table\nADD COLUMN column_3...\nCloud\nwarehouse\nCompare V1 and V2 schemas\nand create a SQL statement\nto update the warehouse table.\nFigure 8.15\nThe schema-management module can compare V1 and V2 schemas and \nautomatically generate necessary SQL commands to update data warehouse table definitions.\n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "258\nCHAPTER 8\nSchema management\n Both AWS Redshift and Azure Synapse have roots in traditional relational technol-\nogies. Redshift was originally based on PostgreSQL RDBMS, and Azure Synapse is\nbased on the Parallel Data Warehouse technology from Microsoft. This means that\nwhen it comes to schema management, both AWS Redshift and Azure Synapse have\nproperties similar to what you would expect from relational technologies.\n First of all, you need to define the schema of the tables up front before you can\nload any data to it. AWS Redshift supports loading data directly from Avro files, but it\ndoesn’t offer any automated schema inference features, even though the schema, as\nwe know, is embedded into each Avro file. Azure Synapse at the time of this writing\nonly supports loading data from CSV, ORC, and Parquet files. It also doesn’t offer any\nautomated schema inference tools, and you will need to write some conversion tools\nto map Avro/Parquet schemas into Azure Synapse table schemas. \n Once the initial table is created, you need to keep it up to date based on the prin-\nciples that we have described previously. Both AWS Redshift and Azure Synapse sup-\nport SQL ALTER TABLE commands that allow you to modify existing tables by adding\nnew columns, deleting existing columns, or changing column types. Keep in mind\nthat the ALTER TABLE command locks the table and makes it unavailable for read\nand write queries. This means that if your schema modification takes a long time, the\ntable will be offline for any data consumer who tries to access it. Fortunately, both\nRedshift and Synapse are columnar data warehouses, so adding and removing col-\numns is a fast operation even on large data sets. Changing column types will result in a\ndata conversion operation and can take a long time depending on your data size. \n Google Cloud BigQuery takes a different approach to the data warehouse architec-\nture and schema management. BigQuery is not based on a relational technology, which\nhas some pros and cons. A great feature of BigQuery is the fact that it can automatically\ninfer schema from certain file formats, including Avro, Parquet, and JSON. This means\nthat you don’t need to predefine table schemas before loading data into them.\n When it comes to schema evolution, BigQuery only supports adding new columns\nto existing tables. This is also done automatically based on the schema in existing files\n(but can be done manually via an API or command-line tools that BigQuery provides).\nThis means that if you add a column to a data source and to the new incoming Avro files\nin the data platform, when loading this new data into the data warehouse, BigQuery will\nrecognize the new column and will add it to the table automatically. This makes schema\nchange automation very simple if you are only dealing with adding new columns. \n On the other hand, BigQuery doesn’t support SQL ALTER TABLE commands and\ncan’t rename, delete, or change the type of existing columns. The only workaround\nthat is available for this scenario is basically to create a new table with the desired\nschema and load data from the original table into this new table and then drop the\noriginal table. This approach works fine on small tables, but for large tables it can take\na long time and you can incur significant BigQuery costs based on the size of the table\nthat you are re-creating, because BigQuery charges you based on the volume of data\nthat you read.\n",
      "content_length": 3444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "259\nSchema evolution and data warehouses\nSummary\nTraditional data warehouses are based on relational technology, meaning that\nthe structure of the data, it’s column names, their types, and their order must be\nknown up front before any data gets loaded. Any changes to the data source\nschema, such as adding new columns, must be carefully planned so the destina-\ntion schemas and ETL pipelines can be adjusted to accommodate this change.\nUnplanned changes will result in broken ETL jobs and unhappy data consumers. \nThe most common examples of schema changes include adding a new column,\ndeleting an existing column, renaming a column, and changing the column’s\ndata type.\nManaging schema changes proactively is possible when you have control over\nthe application that is producing the data, but it becomes much more challeng-\ning if you start ingesting and using data outside of your control, such as data\nfrom third-party SaaS companies—a common use case for data platforms. \nYou can also perform schema management in the data platform, which can\nallow you to (1) build resilient ETLs by detecting and ideally adjusting to\nschema changes before your ETL pipelines fail, (2) maintain an up to date\nschema catalog with schema details that can also be used for data discovery and\nself-service, and (3) have a history of schema changes for any given data to work\nwith archived data in your data platform and simplify pipeline debugging and\ntroubleshooting.\nIn a well-architected cloud data platform, the Metadata layer has a Schema Reg-\nistry that contains all versions of all schemas for all data sources. Data transfor-\nmation pipelines or people who need to know the schema for a particular data\nsource can fetch the latest version from the Registry and even explore all previ-\nous versions of the schema to understand how a particular data source has\nevolved over time. \nIn a cloud data platform, we can add schema management to the other typical\nsteps (data format conversion, deduplication, data quality checks) performed\nby a data transformation pipeline. The schema-management module in a pipe-\nline checks if the schema for this data source already exists in the Schema Reg-\nistry. If it doesn’t, the module will infer the schema from the incoming data and\nregister it as Version 1 of the schema in the Registry.\nIf the schema already exists, the module will fetch the current version of the\nschema from the Registry, infer the schema from the incoming data, compare\nthe inferred schema to the current schema in the Schema Registry, and create a\nnew schema version that combines old and new definitions in a backward-\ncompatible way, and publish the new schema version into the Registry for other\npipelines to use.\nWhile schema inference works well in a batch environment, it’s not feasible in a\nreal-time data pipeline because, while we could infer the schema for a single\n",
      "content_length": 2888,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "260\nCHAPTER 8\nSchema management\nmessage, there is no guarantee that the next message will have the same schema.\nReconciling schemas for each individual message would be both computationally\nexpensive and would result in an extremely large number of schema versions. \nBatch data sources can use a schema-inference approach, and real-time sources\ncan rely on the manual schema management where development teams are\nresponsible for publishing schemas into the Registry and updating schema ver-\nsions in case of changes.\nThere are certain cases where we can’t automate dealing with schema changes,\nand what we need is an alerting mechanism that will let us know that a schema\nchange can cause issues with the downstream pipelines. Pipeline configuration\nfrom the Metadata layer contains information about which data sources are\nused by which data transformation pipelines, so if you know which data source\nwas affected by the schema change, you can easily identify all downstream trans-\nformations and reports that are affected. \nExisting solutions for implementing a Schema Registry are available (AWS Glue\nData Catalog, Azure Data Catalog, and Google Cloud Data Catalog) but limited\nin functionality. Other options with gradually increasing in complexity and sup-\nported features include (1) a plain-text pipeline configuration file and a code\nrepository to version it, (2) a key/value or a relational database to store pipe-\nline configs and other metadata, and (3) adding a REST API layer on top of the\ndatabase to provide a uniform interface for all tools in the platform.\n8.6\nExercise answers\nExercise 8.1:\n 3—While schema-on-read allows you to ingest data without considering schema,\nyou still need to make sure your data processing pipelines use relevant schema defini-\ntions.\nExercise 8.2:\n 3—Backward-compatible schemas allow you to always use the most recent versions\nof the schema to read all data that exists in the data platform for a given data source.\n",
      "content_length": 1968,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "261\nData access and security\nIn this chapter we acknowledge that the primary reason for developing a data plat-\nform is to cost effectively and securely make data available to data consumers—at\nscale. While throughout this book we have assumed that your data platform will\ninclude a data warehouse to support users who access data via business intelligence\n(BI) tools or by running SQL queries directly, this isn’t the only way data will be\naccessed.\n Increasingly, raw data in storage is also being accessed by users, especially data\nscientists. And, increasingly, applications want access to data in storage as well. The\nlayered design we’ve discussed throughout this book makes it easy to support a vari-\nety of data consumers.\nThis chapter covers\nAppreciating how data from the data platform is \nconsumed \nComparing cloud-native data warehouse offerings \nUsing cloud-native services for data access \npatterns for applications\nSimplifying the machine learning lifecycle \nUnderstanding the basics of a cloud security model\n",
      "content_length": 1030,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "262\nCHAPTER 9\nData access and security\n Because the data warehouse is the most popular way to access data for reporting or\nad hoc data analysis, we will review existing cloud data warehousing platform-as-a-service\noptions and highlight their key differences and similarities. We will discuss how to\nenable your applications to become data driven by providing them with data access via\nfast data stores such as cloud RDBMSs or key/value services. We will also cover ways to\nenable your data science team with access to the large amounts of data they need to\ndevelop robust machine learning models. \n With the significant proliferation of different types of data consumers and differ-\nent parts of the data platform that they can access, it’s very important to consider how\nto provide this access and in a secure manner. We will cover some of the fundamentals\nof cloud security at the end of this chapter.\n9.1\nDifferent types of data consumers\nData platforms exist to serve data to data consumers. There is little reason to build a\nsophisticated architecture to ingest, process, and store data if no one can access it or if\ngetting access to the data is a cumbersome process. Data warehouse–centric solutions\noffer only one way to access the data—connect to the warehouse with your favorite BI\nor reporting tools, or run SQL queries directly. This process is simple and familiar to\nmany users, but it is also very limited. \n Today, more and different types of data consumers need fast, secure, and reliable\naccess to the data, and a single point of data access via the warehouse can’t satisfy\nthem all. We can generalize different types of data consumers into two categories:\nHuman users—Many people will need to run reports using BI tools or run SQL\nqueries against the data platform, but other users, such as data science teams,\nmay need direct access to the raw data files to run their experiments on.\nApplications—Modern data analytics is not only about helping businesses to\nmake decisions based on data. All types of applications are becoming “data\ndriven,” meaning that applications utilize data analytics approaches to enhance\nthe end-user experience. This includes various machine learning applications\n(ML) like recommending products that we think the customer might be inter-\nested in or predicting when some factory equipment should be maintained\nbefore it develops a problem.\nData warehouses alone can’t satisfy the needs of all these consumers. While data ware-\nhouses remain the primary way to access data for BI tools and direct SQL access, appli-\ncations rarely connect to the warehouse directly. Applications usually need much\nfaster response time than even modern cloud-based data warehouses can provide. You\nalso need to keep the cloud costs in mind all the time. If previously the community of\ndata users was limited to only a few people in your organization, with applications now\nrequiring data access, this community is extended to potentially thousands or millions\nof users all over the world. Many of the cloud services that we will be talking about in\nthis chapter have a consumption billing model, where you will be charged for the vol-\nume of data you push through the system, or in some cases, per individual query. \n",
      "content_length": 3247,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "263\nCloud data warehouses\n Luckily, our cloud data platform design takes that into account. The layered archi-\ntecture utilizes different technologies and different storage types that can address the\nneeds of any data consumer. Figure 9.1 shows how different data consumers can\naccess data from different layers of our platform. \n9.2\nCloud data warehouses\nWe started this book with a comparison of two different architectures: one in which a\ndata warehouse is a center of the data processing and data serving universe, and one\nwhere a data warehouse is just another component in a more flexible layered data\nplatform. Nevertheless, data warehouses remain the most common way to access the\nresults of the data processing pipelines. There are multiple reasons for this. First of\nall, data warehouses have full support of the SQL language standard. SQL remains the\nmost popular data access and data manipulation language out there. The popular BI\ntools are all SQL-based, and for many power data users, writing a SQL query is even\neasier and faster than using a reporting or BI tool. \nOrchestration overlay\nFast storage\nReal-time processing and analytics\nOperational \nmetadata\nBatch processing and analytics\nSlow storage/direct data lake access\nETL tools overlay\nData\nwarehouse\nIngestion\nStreaming\ndata\nBatch\ndata\nRDBMS\nKey/value stores\nCache\nBI tools\nAd hoc SQL\nML libraries\nML collaboration\ntools\nHuman\nusers\nData\nscience\nteams\nApps\nHuman\nusers\nFigure 9.1\nCloud data platform architecture allows different data consumers to use layers that are most \nsuitable for their data access patterns.\nExercise 9.1\nWhy does it matter that there are different types of users of a cloud data platform?\n1\nThey will all be competing for the same data.\n2\nThey are likely to want different ways to access data. \n3\nThere is no difference between any users of a cloud data platform.\n",
      "content_length": 1863,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "264\nCHAPTER 9\nData access and security\nNOTE\nWhile most cloud data warehouses support the SQL ANSI standard,\nthey also introduce multiple extensions to the language, so queries written for\none warehouse may not necessarily work in another.\nMost of the existing cloud warehouses are relational by nature (with the exception of\nGoogle BigQuery). This means that existing BI, reporting, and other tools that sim-\nplify working with data in a traditional data warehouse are easy to use with a cloud\ndata warehouse. This compatibility is very important, because unless you are building\na brand-new data platform and don’t have any existing or legacy reporting in your\norganizations, you will need to give your users tools that they are familiar with.\n In the following sections, we will give a brief overview of the cloud data warehouse\nofferings for the three major cloud providers: AWS, Azure, and Google.\n9.2.1\nAWS Redshift\nAWS Redshift is a distributed relational cloud data warehouse. Let’s unpack what this\nactually means. Distributed means that Redshift can distribute large data sets that you\nhave to multiple machines (nodes) and run queries on those data sets in parallel, uti-\nlizing CPU and memory of multiple computers. Relational means that at its core Red-\nshift is based on a relational technology. Redshift has roots in an open source\nPostgreSQL database, and you might notice some similarities in commands and\nbehavior if you are familiar with PostgreSQL. Finally, cloud data warehouse means\nthat a lot of the management operations on the warehouse are performed by AWS\nand are hidden from the end user. Managing any distributed system yourself is a\ndaunting task since you need to think about how to replicate data between different\nnodes, what to do in case of network issues, etc.\n Figure 9.2 shows a high-level architecture of a Redshift cluster.\nNOTE\nDiagrams in these sections are high level and do not depict the actual\narchitecture of the underlying service. They are based on available documen-\ntation from the cloud vendors, which doesn’t necessarily go into implementa-\ntion details very deeply.\nA Redshift cluster consists of multiple nodes. A leader node is responsible for accept-\ning all connections from clients, parsing queries and distributing queries to the corre-\nsponding nodes. A leader is also responsible for deciding which node will get the new\ndata as it is added into the warehouse. We will talk about different ways Redshift dis-\ntributes data among nodes later in this section. A leader is also responsible for certain\nqueries that can’t be distributed. These include system operations such as getting a list\nof tables, checking user permissions, etc.\n Each compute node gets a portion of the whole data set that is stored in the ware-\nhouse. Our diagram shows a leader node and three compute nodes, but the cluster size\ncan be larger than that. Internally, Redshift actually subdivides each compute node into\n“slices” where each slice gets a portion of node compute and storage capacity. This\narchitecture makes it possible to scale a Redshift cluster by adding new compute nodes.\n",
      "content_length": 3123,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "265\nCloud data warehouses\nAfter a new node is added, Redshift needs to rebalance the data slices and copy some\nof them to the new node. This process runs in the background but will negatively\nimpact query performance. You need to plan scaling operations carefully to avoid\nimpact on existing users.\n You can control how data slices are distributed to different compute nodes when\ncreating a Redshift table. To do this you need to specify a DISTSTYLE property of the\ntable. It can take the following values:\n\nALL—This places a copy of the table into each of the compute nodes. This dis-\ntribution style is useful for small tables that are frequently joined with other,\nlarger tables. Having a copy of the table on a compute node avoids slow net-\nwork transfers.\n\nEVEN—The table is evenly distributed across all compute nodes, so each node\ngets approximately the same number of rows. \n\nKEY—Distributes a table by a given column. You need to specify a column to use\nwith a DISTKEY property. All rows with the same key will be allocated to the\nsame compute nodes. If you have two large tables that are frequently joined\ntogether, distributing them using the same KEY will significantly improve perfor-\nmance because rows with common keys will be on the same compute node and\nwill not have to be transferred over the network. \n\nAUTO—Starts with the ALL distribution style and switches to EVEN automatically\nas the table size increases.\nSQL queries from clients\nA leader node is responsible for\naccepting all connections from\nclients, parsing queries, and\ndistributing queries to the\ncorresponding nodes as well as\ndeciding which node will get the\nnew data as it is added into the\nwarehouse.\nCompute nodes store and \nprocess a portion of the \nwhole data set. Each node\nhas its own storage.\nA Redshift cluster is \nmade of multiple nodes.\nLeader node\nStorage\nStorage\nStorage\nStorage\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nCompute\nnode\nCompute\nnode\nCompute\nnode\nFigure 9.2\nAn AWS Redshift cluster consists of a leader node with multiple compute nodes \nthat are used to distribute data and workload among themselves. Each node has its own \nstorage attached to it.\n",
      "content_length": 2218,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "266\nCHAPTER 9\nData access and security\nSetting a proper table distribution style is the most important performance optimiza-\ntion technique in Redshift.\nNOTE\nPlease refer to AWS documentation for specifics on how to configure\ntable distribution styles and best practices around that.\nAdditionally, you can specify a SORT KEY for a table to force Redshift to physically\norder a table by a given column. This will improve performance for queries that\nrequire data to be sorted in some way, for example, by a date column.\n Redshift is a columnar data warehouse, meaning data on a disk is organized by col-\numns instead of rows. We described the differences between row-oriented and colum-\nnar formats in chapter 5. Columnar storage allows Redshift to use various\ncompression algorithms for different columns. For example, you can specify a “byte-\ndictionary” encoding for a column that has a small number of unique values in a\ntable, such as a column that stores a country name or a status column with only a few\npossible values. This will significantly reduce the data size on disk and improve query\nperformance. Keep in mind that Redshift compute nodes have fixed storage, memory,\nand CPU capacity, and while you can add new nodes to the system, you need to keep\noverall system cost in mind at the same time. Instead, you should try to optimize the\nsize of your data by providing a relevant encoding for specific columns. Redshift sup-\nports a number of different encodings, and you can find detailed explanations of\nwhen to use them in the Redshift documentation.\n Redshift has relational roots, which is reflected in the data types it supports. Red-\nshift only supports what is called “primitive data types” such as integers, strings, dates,\netc. It doesn’t support arrays or nested data structures. This makes working with JSON-\nstyle data challenging because you need to store these values as strings and rely on\nJSON-parsing functions to extract necessary values from them. This approach works for\nsmall data sets, but if your data is mostly JSON documents, then if you are storing it as\na string column in Redshift, you are not utilizing any of the optimizations we have\ntalked about, such as distributing tables by key or using specific encoding for columns. \n Redshift also offers a feature called Spectrum that allows you to query data that is\nstored in AWS S3 storage directly. To use Spectrum, you need to create a Redshift\ntable that is marked as “external” and specify a path to the data on S3. Spectrum que-\nries would typically be slower than queries against tables that are stored on Redshift\ncompute nodes because you don’t benefit from Redshift optimizations that we have\ndescribed previously. In our data platform architecture, you can use Spectrum for data\nexploration or for offloading certain workloads from the data warehouse. Figure 9.3\nshows how Spectrum fits into Redshift’s distributed architecture.\n In our design, we store all the new and archive data in the S3 layer, which makes it\neasy to query it using Spectrum. You can use Spectrum to run queries against this data\nin S3 and even join it to the data that is already stored in Redshift. This is useful if you\nare dealing with a new data set and need to do some data exploration before deciding\nwhether you need to bring it into the data warehouse or what the right Redshift table\nstructure for this data should be.\n",
      "content_length": 3398,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "267\nCloud data warehouses\nImagine that you have an established data platform with a Redshift warehouse. Your\ncompany uses the warehouse to create various reports and dashboards for sales data.\nNow you have obtained new data sources that contain improved demographics data\nfor your customers, and you think it will help your business users to make better deci-\nsions, but you are not a hundred percent sure it is useful. You can load this new data-\nset into Redshift and ask your end users to run some experimental queries against it,\nbut this will consume warehouse storage and compute resources. If your warehouse is\nvery busy, this new data set may even require you to add new nodes to the Redshift\ncluster. Instead, you can create an external table in Redshift and use Spectrum to run\nthese experimental queries outside of Redshift and easily discard the dataset if your\nusers don’t find it helpful. This way, the impact on the warehouse is minimal. \n Keep in mind that using Spectrum still requires you to create external tables in\nRedshift, and while you are not physically copying data to the warehouse at this point,\ntable definitions can pollute your curated warehouse design, especially if you allow\nmultiple people to create external tables. We recommend you group external tables\ninto a dedicated database and periodically clean up table definitions that are no lon-\nger needed. \n You can also use Spectrum to offload some processing from your Redshift data\nwarehouse. For example, if you have a large table that you need to query, loading it\ninto Redshift would require adding more compute nodes, which will increase your\nwarehouse cost. If query performance is not a concern, you can choose to not load\nthis table into Redshift from S3 and instead query it directly using Spectrum.\n In our data platform architecture, Spectrum is definitely an optional feature,\nbecause you can achieve the same results by running Spark jobs or Spark SQL queries\nSpectrum allows you to query\ndata that is stored in AWS S3\nstorage directly.\nAWS Spectrum on-demand compute\ncapacity\nData lake on AWS S3\nQueries to external\ntables are offloaded\nto Spectrum.\nSQL queries from clients\nLeader node\nCompute\nnode\nCompute\nnode\nCompute\nnode\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nData slice\nStorage\nStorage\nStorage\nStorage\nFigure 9.3\nAWS Spectrum can be used to query data that is stored in the data lake without \ncopying it to the warehouse storage first. Spectrum can also offload some processing from your \nwarehouse by using an on-demand compute model.\n",
      "content_length": 2591,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "268\nCHAPTER 9\nData access and security\ndirectly on the data in the S3 storage. It all comes down to whether your end users are\ncomfortable with Spark and Spark-specific tools or whether they prefer to only inter-\nact with the warehouse.\n9.2.2\nAzure Synapse\nAzure Synapse is a distributed cloud data warehouse offering from Microsoft. It is\nbased on the Microsoft Parallel Data Warehouse product and is based on a relational\ntechnology. This makes Azure Synapse, much like AWS Redshift, easily compatible\nwith various existing reporting and BI tools that expect a relational database as their\nsource of data. \n The high-level architecture of Azure Synapse is similar to Redshift, but with a few\nkey distinctions. Figure 9.4 shows Azure Synapse architecture.\nSimilar to AWS Redshift, Synapse clusters consist of a control node and compute\nnodes. The control node accepts connections from clients, parses and validates the\nincoming queries, and sends those queries to compute nodes to execute. One major\ndifference between Synapse and Redshift is that Synapse separates the storage layer\nfrom the compute layer. Synapse splits all your data into 60 data distributions, and each\ndata distribution is attached to a particular compute node, but data is not stored on the\ncompute nodes themselves. This design makes scaling the cluster a simple and fast\noperation. You can add new compute nodes or remove compute nodes, and Synapse\nSQL queries from clients\nControl node\nCompute\nnode\nCompute\nnode\nCompute\nnode\nData\ndistribution 1\nData\ndistribution 2\nData\ndistribution 60\nAzure Storage\nIn Synapse, as in Redshift,\ncontrol nodes accept\nconnections from clients, parse\nand validate the incoming\nqueries, and send those queries\nto compute nodes to execute.\nUnlike with Azure Synapse, \ndata is stored independently\nof compute nodes, which\nmakes adding and removing\nnodes simpler.\nFigure 9.4\nAn Azure Synapse cluster consists of a control node and compute nodes, with data \nstorage being completely separate from the compute layer.\n",
      "content_length": 2018,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "269\nCloud data warehouses\nwill only need to adjust how data distributions are mapped to compute nodes, without\nhaving to actually copy any data around. \n It’s important to note that, currently, scaling a Synapse cluster is an offline opera-\ntion, meaning the cluster will need to complete or cancel any running queries and will\nnot accept any new queries while the scaling operation is underway. This makes Syn-\napse not truly elastically scalable and forces you to plan your scaling operations care-\nfully in order not to impact your users.\n Another property of this design is that you can pause all your computer tasks com-\npletely and resume them later without having to move data anywhere. If your ware-\nhouse doesn’t need to be online 24/7, you can pause it at the end of day and resume\nfirst thing in the morning. Or you can pause it on the weekends. This will save a signif-\nicant amount of your cloud costs because Synapse charges you for compute and stor-\nage separately, and compute is much more expensive than storage.\n Azure Synapse doesn’t allow you to directly specify the type and the number of\ncompute nodes in the cluster. Instead, you need to specify the overall cluster capacity\nusing Data Warehouse Units (DWUs). DWU represents a combination of CPU and\nmemory capacity. In our experience, trying to configure the right number of DWUs is\na bit of a trial-and-error process where you need to run your workload on different-\nsize clusters and arrive at a capacity that gives you the optimal price/performance\ncombination. \n Similar to Redshift, you can specify how a Synapse table will be split into multiple\ndistributions. There are three options that are available:\n\nHASH—This requires a column name to be specified. All rows with the same\nvalue for this column will be placed into the same data distribution. \n\nROUND ROBIN—The table will be split into equal chunks and spread across all\ndata distributions.\n\nREPLICATE—A copy of the table will be placed into each data distribution.\nAs you can see, table distribution options are similar to those in AWS Redshift. Setting\nthe right distribution method for a table is the main performance optimization tech-\nnique, so you will need to spend some time planning this and taking into account the\nmost common ways the end users will query the data in the warehouse. You can also\nforce Synapse to order the data in the table using specific columns during the table\ncreation. This will speed up queries that expect data to be sorted in a particular way.\nNOTE\nPlease refer to Azure documentation for more details and best prac-\ntices around Synapse table design.\nAzure Synapse uses a columnar data store and applies compression to each column\nseparately. Unlike with Redshift, you can’t specify a compression algorithm for each\ncolumn, and you have to rely on Synapse to make the best choice for you. \n Synapse only supports primitive data types (integers, dates, strings, etc.) and\ndoesn’t support arrays or nested data structures. It provides a number of built-in JSON\n",
      "content_length": 3035,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "270\nCHAPTER 9\nData access and security\nparsing functions that you can use to read JSON data from a string column, parse it,\nand access specific attributes inside the JSON document. Similar to Redshift, this\napproach doesn’t utilize many of the columnar optimizations and will not provide you\nwith optimal performance. \n When talking about AWS Redshift, we have discussed a Spectrum feature that\nallows you to query data directly from S3 storage without having to load the data into\nthe warehouse first. Azure Synapse offers similar features by introducing the notion of\npools. Currently there are three types of Azure Synapse pools: SQL pool, SQL on-\ndemand pool, and Spark pool. \n The architecture we described earlier with provisioned capacity, data distributions,\nand compute nodes applies to SQL pool only. This is your traditional cloud data ware-\nhouse module where you need to load data into the warehouse table first before you\ncan query it. With the SQL on-demand pool, you can query Parquet, CSV, or JSON\ndata directly from the Azure Blob Storage in a serverless fashion. When you run a\nquery in the SQL on-demand pool, Azure will provision the required compute node to\nprocess your query and destroy the compute node after your query is finished. This is\nuseful for data exploration use cases or for offloading certain workloads from your\nmain SQL pool. \n In addition to SQL on-demand, Azure Synapse also supports Apache Spark pools.\nThis allows you to run Spark jobs against data in the Blob Storage using the same Syn-\napse interface. Spark pools are not completely ephemeral like SQL on-demand pools\nand require a minimum of three nodes to be available all the time. Spark pools sup-\nport autoscaling, which means extra nodes can be provisioned by Azure to process\nyour job, and then the cluster will be scaled down to its original configuration.\n9.2.3\nGoogle BigQuery\nAmong the three major cloud providers, the Google cloud data warehouse offering\ndefinitely stands out. Unlike Redshift and Synapse, BigQuery is not based on any exist-\ning relational technology and was developed for internal Google purposes before\nGoogle Cloud existed. This heritage provides BigQuery with some unique properties. \n First of all, BigQuery (see figure 9.5) is closer to a fully managed service than Red-\nshift or Synapse. While AWS and Azure take care of the many maintenance and opera-\ntional tasks that are required to make a distributed data warehouse work, you still\nneed to worry about choosing the right type of nodes (in AWS) and planning how\nmuch capacity your cluster will need (both in AWS and Azure). BigQuery doesn’t\nrequire any of that. The cluster capacity is provisioned by Google Cloud on the fly for\nyou and can be different from query to query. BigQuery uses very large pools of hard-\nware resources (tens of thousands of nodes) that are shared between different Google\nCloud customers. This allows BigQuery to allocate required processing power (slots,\nin Google Cloud terminology) on a per-query basis, without having to provision new\nmachines for it (which takes time). For the end user’s perspective, the result is a truly\nelastically scalable data warehouse. \n",
      "content_length": 3182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "271\nCloud data warehouses\nBigQuery uses a tree-like architecture with an additional layer of “intermediate serv-\ners” that sit between the root node and the leaf servers. Intermediate servers are\nresponsible for sending work to specific leaf nodes, collect results from leaf nodes,\nperforming aggregations, etc. Having a three-tier architecture instead of a two-tier\narchitecture like we saw in Redshift and Synapse allows BigQuery to scale to a much\nlarger number of nodes in the cluster.\n Another important property of BigQuery architecture is that in order to be truly\nelastically scalable, it can’t rely on data locality. Data locality in distributed systems\nmeans that a node that processes data should have the fastest access to the data it is\nprocessing as possible. Having local storage such as a hard drive or SSD directly on the\ncompute node is the fastest option, but it is really expensive to move data from one\nnode to another because you need to wait for the data to copy. Another option is to\nattach a network storage of some sort to the compute node so it can read the data and\nthen reattach it to another node if you need to rebalance your cluster. This is faster\nthan a full copy, but still takes time. BigQuery solves this problem by using a Google\ninternal network, which provides high enough throughput and low enough latency\nfor the data locality not to matter as much. This means that leaf nodes in BigQuery\ncan read the data from the storage and not worry much about whether data is local or\ndirectly attached to the node.\n BigQuery is based on an internal Google data processing system called Dremel,\nand Dremel was developed to allow users to analyze very large volumes of various log\nfiles. Log files, as you can imagine, don’t have a relational structure with tables and\nSQL queries/API calls\nfrom clients\nRoot\nserver\nIntermediate\nserver\nIntermediate\nserver\nLeaf\nserver\nLeaf\nserver\nLeaf\nserver\nGoogle’s uber-fast network\neliminates the need to rely\non data locality.\nBigQuery Data Transfer Service \nuses a tree-like architecture with \nan additional layer of “intermediate \nservers” that sit between the root \nnode and the leaf servers and are\nresponsible for sending work to \nspecific leaf nodes, collect results from \nleaf nodes, performing aggregations etc.\nThis three-tier architecture allows \nBigQuery Data Transfer Service\nto scale to a large number of\nnodes in the cluster.\nServers in BigQuery\nData Transfer Service\nare shared across\nmultiple customers.\nData is chunked\ninto shards and\nstored in a\ndistributed file\nstorage.\nFigure 9.5\nBigQuery uses large pools of shared resources to provide elastic scalability. Storage and \ncompute are separate, and the fast internal Google network allows not to worry about data and compute \nnodes being “close.”\n",
      "content_length": 2784,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "272\nCHAPTER 9\nData access and security\nkeys that can be used to join those tables together. Also, log files can store a wide vari-\nety of data, including nested data structures such as JSON documents. BigQuery is the\nonly cloud data warehouse among the three major providers that has a native support\nfor nested data types. BigQuery supports arrays of values as well as JSON data with\nnested attributes. Support is native, meaning that BigQuery actually stores each JSON\nattribute as a separate column and not a full document as a string value, as with Red-\nshift or Synapse. This significantly improves your query performance and makes que-\nries much easier to read and write.\n Finally, BigQuery uses a pay-per-use pricing model where you only pay for the\namount of data your query has actually processed. This is different from other ware-\nhouses where you pay per provisioned capacity, no matter how many queries you run\non the system. BigQuery also charges separately for the storage you use, but those\ncosts are usually negligible in comparison to the compute costs. This pricing model\nhas its pros and cons. For small organizations that don’t have a large volume of analyti-\ncal queries, BigQuery can be extremely cost efficient in comparison with other cloud\nproviders or traditional data warehouses, but as your data and the number of queries\nincreases, it becomes very hard to predict the total cost of BigQuery. To do this accu-\nrately, you need to know every single query that will be executed and how much data\nthis query will read. \nNOTE\nGoogle Cloud offers BigQuery flat-rate pricing for enterprise custom-\ners, which makes costs much more predictable.\nBigQuery also doesn’t offer as many tuning options as Redshift and AWS. You can’t choose\nhow your tables are distributed across the nodes, because, if you recall, in BigQuery, this\nconcept doesn’t exist. You also can’t control the compression algorithms and have to rely\non internal BigQuery columnar storage to optimize the data for you. A couple of import-\nant tuning options for BigQuery are data partitioning and clustering. \n Data partitioning physically splits that data in the BigQuery storage using a particu-\nlar column value. For example, you can partition your table using a month value of a\ndata column. If you do that, then if you only query data for a given month, BigQuery\nwill read only data in that month’s partition. This is much faster and much cheaper\nthan reading a full table and then filtering out data only for a given month. Partition-\ning in BigQuery is important because it is both a performance optimization and a\ncost-control mechanism. \n Clustering of BigQuery tables physically organizes and sorts data in a table or a par-\ntition using one or more columns. This is a performance optimization technique and\nwill speed up queries that often require data to be sorted or aggregated by a certain\ncolumn. You can combine partitioning and clustering together.\n One of the side effects of BigQuery’s non-relational nature is that it is not as seam-\nlessly compatible with existing data visualization and reporting tools as Redshift or Syn-\napse. The primary way of BigQuery accepting requests from clients and sending data\nback is via a REST API. There is an existing third-party implementation of a\nJDBC/ODBC driver for BigQuery, but it still does the translation to REST API calls\n",
      "content_length": 3375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "273\nCloud data warehouses\ninstead of using a native network protocol. This may cause performance issues if you are\ntrying to query large amounts of data from BigQuery into a BI tool. Another compati-\nbility challenge is that many of the existing reporting and BI tools expect data to be in\na relational format with multiple tables and joins between them. These tools may not\nyet understand how to natively work with nested JSON data and arrays. Given the\nincreasing popularity of BigQuery, we expect more and more BI vendors will work out\nthese compatibility issues eventually, but if you have an existing reporting tool that your\norganization uses extensively, it’s a good idea to check its compatibility with BigQuery.\n BigQuery supports external tables, which allows you to query data from Google\nCloud Storage and other data sources such as Cloud Bigtable or Cloud SQL. Unlike\nRedshift and Synapse, BigQuery doesn’t allocate any additional capacity for these que-\nries and uses the same slots available to you for regular BigQuery workloads. The pric-\ning is also the same for external tables as it is for internal BigQuery tables, so while you\ncan use BigQuery to query data directly in your data lake, you need to be cognizant of\nthe cost and other workloads that you run on the data warehouse. \n It is also worth mentioning a recently (2020) introduced feature called BigQuery\nOmni. Omni allows you to deploy BigQuery software on other cloud providers’ VMs,\nsuch as AWS or Azure. You still get all the features of BigQuery, but now you don’t\nneed to copy the data from AWS or Azure into Google Cloud, which can be very slow\nand very expensive. You will still need a Google BigQuery deployment to act as an end-\npoint that accepts queries from clients, provides the UI, etc., but you will be able to\ncreate external tables that actually reside on AWS S3 and Azure Blob Storage.\n This is a new feature, and we haven’t seen it used in real-life scenarios yet. Obvi-\nously, you will not be able to get the same scale of tens of thousands of nodes in Omni\nas you would get natively in BigQuery, but the benefits of not having to move data\nfrom one cloud provider to another may outweigh the limitations for your use case.\n9.2.4\nChoosing the right data warehouse\nAs you can see, different cloud data warehouses offer different sets of features, and a\nnatural question is, which one is the best for your use case? In our experience, the\nchoice of a cloud data warehouse is almost never a separate decision and is always tied\nto choosing a cloud provider. For large organizations, using different cloud vendors\nfor different parts of their infrastructure makes sense, because this protects them\nfrom vendor lock-in and gives them an upper hand in negotiating the best possible\ndeal with each provider. For smaller organizations, a multicloud approach has too\nmuch overhead and extra engineering cost to be truly feasible.\n We have seen use cases in the past where an organization would have its applica-\ntion estate deployed on one cloud provider and their data analytics and machine\nlearning workloads on another provider, but in such cases you need to be cognizant of\nthe data transfer costs between providers. \n In most cases, the choice of the cloud provider will dictate which data warehouse\nyou will use for your cloud data platform. Table 9.1 summarizes some of the key ware-\nhouse features of AWS, Azure, and Google Cloud.\n",
      "content_length": 3430,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "274\nCHAPTER 9\nData access and security\n  \n9.3\nApplication data access\nOver the last several years, there has been an increasing trend for applications to\nbecome more and more data driven. Websites now provide recommendations on\nthings users might like or find useful, provide more personalized experience, etc.\nApplications like these may require access to data that was previously only available in\nthe data warehouse and used exclusively for reporting or ad hoc analysis.\n It is, of course, possible to provide your applications with direct access to the cloud\ndata warehouse, but there are several major challenges with this. First of all, cloud ware-\nhouses are all designed to provide reasonable and consistent query performance over\nlarge data sets, but they were not designed to provide low-latency access. A typical query\nexecution time in a cloud data warehouse is seconds to minutes, while most applica-\ntions require significantly faster response time, usually measured in milliseconds. \nNOTE\nIn this section, we will use “application” to describe a wide spectrum of\ndifferent use cases, but all of these use cases will have the following require-\nments in common: exposed to internet or a large user community inside your\norganizations, are interactive, require fast response time for the data stores,\nand rely on data produced by the data platform. \nAnother challenge with applications is that cloud data warehouses can easily deal with\nhundreds of concurrent queries, which is usually enough for reporting and analytics\nTable 9.1\nComparing key data warehouse features\nAWS Redshift\nAzure Synapse\nGoogle BigQuery\nBased on a relational \ntechnology?\nYes\nYes\nNo\nSupport for nested \ndata structures?\nLimited (via JSON \nparsing functions)\nLimited (via JSON \nparsing functions)\nNative support\nHow does it scale \nup/down? \nManually\nManually\nAutomatically\nPay per use or pay per \nprovisioned capacity?\nPay per provisioned \ncapacity\nPay per provisioned \ncapacity\nPay per use (with option to pay \nper provisioned capacity)\nExercise 9.2\nWhich of AWS Redshift, Azure Synapse, or Google BigQuery are not based on relation-\nship technology?\n1\nRedshift\n2\nSynapse\n3\nBigQuery\n4\nAll of the above\n",
      "content_length": 2190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "275\nApplication data access\nuse cases even at very large organizations. Applications, on the other hand, often are\nexposed to tens or hundreds of thousands of the end users and thus have much\nhigher concurrency requirements. Popular applications connected directly to the\nwarehouse can easily consume all available capacity. \n There is also a security concern with having internet-facing applications connect-\ning to the data warehouse. Data warehouses may contain sensitive data access that\nmust be restricted to internal users only. This often leads to cloud data warehouses\nbeing deployed inside cloud virtual networks to restrict who can access the data on the\nnetwork level. If you expose your data warehouse for application access and applica-\ntion becomes compromised, this potentially can lead to data exfiltration. \n There are several alternatives that can be used as a data store for applications and\nthat don’t have the limitations that we have outlined. Using a dedicated data store for\napplications will not only provide faster data retrieval times and handle concurrent\nrequests but will also allow you to only load data required for a particular application\ninstead of providing access to the data warehouse.\nIn this section, we will do an overview of services that major cloud vendors provide for\napplication data stores. This includes the following:\nRelational databases\nKey/value data stores\nFull-text search systems\nIn-memory caches\n9.3.1\nCloud relational databases\nRelational databases are a tried-and-true way to provide applications with a fast and\nreliable data store. Relational data modelling is well understood and can be very flexi-\nble when done correctly. Cloud-based relational databases are also a great choice if\nyou are migrating an existing application into the cloud. Today, every major cloud\nprovider has at least one service that offers a managed relational database instance.\nThese managed services take care of many of the day-to-day tasks that previously you\nExercise 9.3\nWhich of the following are reasons for not connecting your applications directly to your\ndata warehouse?\n1\nThey have high concurrency requirements and could potentially consume all\navailable capacity.\n2\nThey need lower latency than the data warehouse can provide.\n3\nIf your application becomes compromised, it could potentially lead to data\nexfiltration. \n4\nAll of the above.\n",
      "content_length": 2392,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "276\nCHAPTER 9\nData access and security\nwould have to deal with yourself. This includes backup automation, replication, oper-\nating system patching, updates, etc. Because we are only loading specific data sets that\nhave been preprocessed in the data platform into these data stores, the scalability\nrequirements for application data stores are typically significantly smaller than for the\ndata platform itself. \n AWS offers Relational Database Service (RDS) for PostgreSQL, MySQL, MariaDB,\nOracle, and SQL Server. These would perform well with little tuning and optimiza-\ntions on data sets that are under 1 TB in size. It is possible to scale relational databases\nto much larger data sets, but that would require extra planning and work. For large\ndata sets and applications that need to scale both read and write operations, or server\nusers in multiple geographical regions, AWS offers Aurora, a distributed database ser-\nvice, that is compatible with MySQL and PostgreSQL.\n Google has a service called Google Cloud SQL that offers managed MySQL, Post-\ngreSQL, and SQL Server databases. Google Cloud  Spanner is a large-scale distributed\ndatabase that offers seamless replication into multiple geographical regions. Spanner\nautomatically makes all data available in different geographies without the drawback\nof eventual consistency that are common in key/value data stores.\n Azure’s managed RDBMS offering is an Azure SQL Database that supports\nMySQL, PostgreSQL, and SQL Server. Because SQL Server is a Microsoft product,\nAzure offers multiple options for a cloud version of its flagship database product. You\ncan choose between a fully managed Azure SQL Database or a hybrid Managed\nInstance service. Managed Instance offers more control over the virtual machine on\nwhich the SQL Server runs and also is more compatible with the on-premises versions\nof the SQL Server. For large-scale applications, Azure has a HyperScale version of\nAzure SQL Database. HyperScale is only available for the SQL Server version.\n9.3.2\nCloud key/value data stores\nKey/value or NoSQL data stores became a popular alternative to relational databases\nbecause of their simpler data model and the fact that they are easier to scale. While\ndifferent NoSQL data stores have slightly different data models, the general idea is\nthat you have a unique identifier for the row (the “key”), and you have a number of\ncolumns attached to the key (“values”). Such data stores provide really low latency for\nfetching or writing values for one or more keys. Key/value data stores are popular for\ngreenfield application development because they make it easier to iterate and make\nchanges to the schema. \n AWS’s primary key/value service is DynamoDB. One of its key characteristics is\nconsistent performance at any scale. DynamoDB offers two pricing models: pay-per-\nuser or pay for preprovisioned capacity. The pay-per-use approach is typically cheaper\nfor light or spiky workloads. \n Google Cloud has two services in the key/value category: Datastore and Cloud Big-\ntable. Google Cloud Datastore is similar to DynamoDB in its data model but offers\nonly a pay-per-use model, which can get expensive if you are performing lots of read\n",
      "content_length": 3199,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "277\nApplication data access\nand write operations. Cloud Bigtable offers a simpler data model than Google Cloud\nDatastore; for example, there is no notion of data types for columns in Cloud Big-\ntable, and all columns are represented as arrays of bytes. This makes it possible to\nwrite pretty much any values into Cloud Bigtable, but it can be challenging for appli-\ncation development because you will need to track data types in the application code.\nCloud Bigtable scales to much larger volumes than Google Cloud Datastore, but its\npricing model only has a pay for preprovisioned capacity option. Cloud Bigtable clus-\nters have a minimum-of-three-nodes requirement, which makes it not a cost-efficient\noption for smaller scale applications. One of the popular use cases for Cloud Bigtable\nis migrating existing Apache HBase applications from on-premises Hadoop clusters\ninto Google Cloud. Apache HBase and Cloud Bigtable are compatible on the API\nlevel, which means you can port your existing Apache HBase applications to Cloud\nBigtable with minimal changes. \n Azure’s key/value data store is Cosmos DB. Its unique feature is support for multiple\npopular APIs. Cosmos DB can be configured to support MongoDB, Cassandra, SQL,\nand graph APIs. You need to specify which API you want to use before creating a Cosmos\nDB database. Being able to use MongoDB or Cassandra client libraries with Cosmos DB\nmakes porting existing applications a much simpler process.\n9.3.3\nFull-text search services\nVery often the data that your applications need to deal with is not nicely structured\ntables, with numeric metrics such as sales amount or various counts that you query and\naggregate. If your application needs to deal with text data and provide search function-\nality, then taking a look at full-text search data stores is a good idea. For example, your\napplication may allow users to search through hotel room descriptions or product\ndescriptions and return entries that don’t match the search request 100% but rather\nfind entries very similar to the search request using the natural language semantics.\n Apache Solr and Elasticsearch are two popular, full-text-search data stores. Both\nare based on an open source search library called Apache Lucene. From an applica-\ntion developer perspective, a full-text search data store allows you to load JSON docu-\nments into the data store and then perform search operations on that data. Search\ndata stores can index all attributes in your document or you can specify which attri-\nbutes should be indexed. You get different search options depending on the type of\nthe attribute in your document. For example, for attributes that store numeric values,\nyou can use equals, less than, greater than, etc., search qualifiers in your queries. For\ntext attributes, a search data store will allow you to search on exact text match, find\nsimilar entries, recommend entries for autocompletion, etc. Full-text search data\nstores don’t support joins of different documents, so you need to make sure your data\nis organized in such a way that doesn’t require joins.\n AWS offers CloudSearch, a managed service that provides full-text search capabili-\nties. Azure’s full-text search service is called Azure Search and provides features simi-\nlar to CloudSearch. Google Cloud, surprisingly, doesn’t offer a full-text search\n",
      "content_length": 3342,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "278\nCHAPTER 9\nData access and security\nmanaged service at the time of this writing. There is a product called Google Search,\nbut it is focused on providing companies with capabilities to index and search their\ninternal Word documents, emails, etc., and is not used as a data store for applications.\n9.3.4\nIn-memory cache\nOpen source solutions such as Redis and Memcached are used to provide sub-millisecond\nlatencies for application data access. Redis and Memcached store data in memory and\nthus can provide much faster response times than a relational or key/value database.\nBecause there is no data persistence, these data stores are used as caches, meaning that\na relational or key/value data store still acts as a source of truth, and data in a cache can\nbe reloaded from a persistent database in the case of a crash or any other issues. The key\nidea behind using an in-memory cache is that your application first tries to find the data\nit needs in the cache, and if it’s not there, or if it is stale, applications can reach into a per-\nsistent data store. \n You can use an in-memory cache to provide a very fast access to the data from the\ndata platform to your applications. The data platform in this case will serve as a per-\nsistent layer, and you can create a process that will load and refresh data in the cache\non a regular basis. Caches have a simple data model, similar to key/value stores, and\ndon’t support complex queries and joins. \n AWS provides a managed service for both Memcached and Redis called Elasti-\nCache. Google only supports Memcached in its Memorystore service, and Azure offers\na managed Azure Cache for Redis service.\n As you can see, there are multiple options for data stores that can be used by appli-\ncations. Often, the choice of the particular data store is driven by application develop-\ners, their experience with various solutions, etc. It’s a good idea for you as a data\nplatform designer and architect to understand different access patterns and cloud ser-\nvices that can be used to implement them.\n In the next section, we will look at yet another data access pattern that we see more\nand more often in the real-life projects. We will talk about machine learning on the\ncloud data platform.\n9.4\nMachine learning on the data platform\nMachine learning (ML) workloads have some unique properties and characteristics.\nDeveloping an ML model requires data scientists to understand what kind of data\ntheir model will need to deal with and then run a number of experiments to choose\nthe best suitable algorithm or an existing library that will fit the purpose. Then a\nmodel needs to be trained, which means its parameters are adjusted in multiple itera-\ntions using a training data set until a model accuracy is within acceptable limits. Once\na model is trained, it needs to be validated to make sure that the model produces\ngood results on data other than the training data set. And finally, the model can start\nserving results to the end users by accepting new data, applying calculations, and pro-\nducing results. \n",
      "content_length": 3055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "279\nMachine learning on the data platform\n This process requires data science teams to have access to large volumes of data so\nthey can understand what kind of data they are dealing with and what kind of prob-\nlems can be solved with this data. It also requires access to significant compute capac-\nity to run the training process, which includes rerunning training steps hundreds or\nthousands of times. We also need to have a simple way to split data into training and\nvalidation data sets that doesn’t require a lot of manual effort from the data science\nteam. Communication and collaboration between team members during this process\nis also very important. If each data science team member experiments with data on\ntheir own computers, it can be hard to reconcile or share these results with other\nteam members. \n These requirements make a cloud data platform an ideal place for the machine-\nlearning workloads. The data platform already stores all data that is available to your\norganization, including archives of historical data and access to both raw data that is\ningested from the source systems as is and preprocessed data that has been cleaned up\naccording to organizational standards. \n A cloud data platform also provides multiple different ways to access the data:\nSQL, Apache Spark, direct file access, etc. This is important because it allows you to\nuse the ML tools and libraries that may have different requirements for working with\ndata. If your platform only provides one way to access data (say, SQL queries on the\ndata warehouse), it may limit the data science team in the selection of tools and push\nthem to develop models on their computers where they have more control over the\nenvironment.\n The data processing layer of the cloud data platform offers a scalable compute\nplatform that can be used to train models on much larger datasets than would be pos-\nsible on a personal computer or a dedicated VM. Today, all cloud vendors offer access\nto VMs with powerful graphics processing units (GPUs) that can be used to signifi-\ncantly speed up the ML model training process.\n Cloud vendors also realize the importance of seamless collaboration for the model\ndevelopment process and offer a number of tools that allow data scientists to run,\nshare, and discuss the results of their experiments with the other team members or\nstakeholders. These tools are easy to integrate with the cloud data platform and pro-\nvide a cloud environment that checks all the boxes:\nHas access to all organizations data\nProvides a scalable computational environment\nOffers experimentation and collaboration tools \n9.4.1\nMachine learning model lifecycle on a cloud data platform\nA typical machine learning lifecycle has the following steps:\nIngest and prepare data sets\nTrain/validate model loop\nDeploy the model to production to serve results to the end users\n",
      "content_length": 2866,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "280\nCHAPTER 9\nData access and security\nFirst, we will take a look at how a typical ML process can be implemented without a\ncommon data platform. Figure 9.6 shows what this process might look like.\n The first thing a data scientist does is try to ingest (copy) the data from the existing\ndata sources into their own computer where they can run the rest of the ML process.\nThen they need to perform some data cleanup steps and convert data from multiple\nformats into a single format that their tools of choice will understand. Then a data sci-\nentist needs to run some exploratory analysis on the data sets just to understand what\nkind of data they contain, checking if there are any outliers in the data or any other\nproperties that may impact the model training process. \n According to multiple studies, these first two steps in the ML lifecycle—ingesting\nand cleaning up data—take up to 80% of all the time data scientists spend on the\nmodel development process. This is not surprising, because as we have seen in previ-\nous chapters, ingesting and preparing data for consumption is an involved process. \n Next, data scientists perform the train/validate loop. To train a model of their\nchoice, they need to split the data they now have into two parts: training data set and\nvalidation data set. Then they run the model training process on the training data set.\nThe training process itself is iterative, meaning an ML model will need to read and\nprocess the training data set multiple times, gradually adjusting its parameters to pro-\nduce results with a required accuracy. This process is computationally expensive, espe-\ncially if you are dealing with large data sets. Running it on a single computer can take\na long time.\nNOTE\nWe are making some simplifications when describing the ML lifecycle.\nThere are many more details involved in the process, such as labeling data\nsets for the supervised learning, etc. \nOnce the training process has finished, the data scientist needs to apply the model to\nthe validation data sets to make sure the model still produces accurate results on the\nHow a typical ML process might\nbe implemented without a\ncommon data platform\nData sources\nIngest data\nPersonal\ncomputer\nClean up and\nexplore data sets\nTraining\ndata set\nValidation\ndata set\nTrain/validate\nloop\nShare the\ntrained model\nData scientist\nFigure 9.6\nTypical ML lifecycle without a cloud data platform\n",
      "content_length": 2400,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "281\nMachine learning on the data platform\ndata other than training data. This helps data scientists avoid a problem called overfit-\nting, where the model’s parameters fit the training data set very well but produce inac-\ncurate results on any other data. The way data is split into training and validation data\nsets plays a big role here because you want your validation data set to be as representa-\ntive as possible of data that the model will need to deal with in production. \n Once the training process is complete, the data scientist needs to share the model\nwith their peers or hand it over to the Operations team to actually deploy it into pro-\nduction. One of the challenges with this step is that often code developed on a personal\ncomputer doesn’t integrate very well (if at all) with the production environment. For\nexample, it may not have all the logging, error handling, or monitoring features that\nare required to run an application in production. The fact that data scientists in this\nprocess are isolated from the actual production or testing environments forces the\nOperations or DevOps teams to make significant changes to the model code, for exam-\nple, adding proper logging and error handling, before it can be deployed. \n Now let’s take a look how some of these steps can be simplified by using a cloud\ndata platform as a central place to develop ML models. Figure 9.7 shows how an ML\nlifecycle might look on a shared platform.\n As we mentioned before, data scientists may spend a lot of time doing work that is\nnot directly related to building and training ML models, such as figuring out how to\ningest data from a particular data source or converting all data sets to a common for-\nmat. With a cloud data platform, this part of the work is taken care of by ingestion and\ncommon data transformation steps. \nIn a data platform,\ningestion and data\ntransformation are\nexisting processes.\nData in existing cloud storage or a data warehouse is available for\ndata exploration and training/validating data, existing batch-\nprocessing frameworks can be used to train models, and multiple\npeople can collaborate on the same data sets.\nMachine learning\nprocess steps can\nbe simplified by\nusing a cloud data\nplatform as a\ncentral place to\ndevelop ML models.\nFast storage\nReal-time processing and analytics\nData scientist\nIngestion\nStreaming\ndata\nBatch\ndata\nExplore\nTrain/validate\nShare\nBatch processing and analytics\nSlow storage/direct data lake access\nOperational\nmetadata\nData\nwarehouse\nFigure 9.7\nBy using a cloud data platform as a central place to develop and test ML models, many of the time-\nconsuming tasks, such as data ingestion and cleanup, are significantly simplified.\n",
      "content_length": 2695,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "282\nCHAPTER 9\nData access and security\nData scientists can use existing cloud storage or a data warehouse to browse and run\nexploratory analytics without having to bring data to their own computers. When it\ncomes to the train/validate loop, data scientists can use existing archive data as large-\nscale training data sets and then use more recent incoming data as a validation data\nset. They can also choose to split data in some other way by creating a dedicated area\non the cloud storage and copying required data sets there. Cloud storage is scalable\nand cheap, which allows the data science teams to experiment and slice and dice data\nin any way they want. \n We already mentioned that a model training process is computationally expensive,\nand here data scientists have a choice to either use existing batch-processing frame-\nworks such as Apache Spark (which supports a number of ML models out of the box)\nto train their models in a scalable way or use dedicated cloud virtual machines with\nlarge amounts of memory and GPUs, because some models cannot be trained in a dis-\ntributed way. Using processing in the cloud brings scalability and elasticity, which also\nminimizes any performance impact on other users.\n Because every team member in the data science team is working on the same plat-\nform, it is now possible to not only share final results of the work, but for multiple peo-\nple to collaborate on the same data sets and the same splits of test and validation data\nsets, which significantly shortens the feedback cycle and improves productivity. Being\n“close” to real data also makes it possible to test final models on real data volumes\nbefore publishing it into production. This reduces the number of issues people run\ninto because their locally developed model was not ready for production deployment.\nIn the next section, we will look into some of the ML collaboration and productivity\ntools that cloud providers offer.\n9.4.2\nML cloud collaboration tools\nShared cloud environments, such as a data platform, are perfect places for data scien-\ntists to collaborate and experiment during the model development. All three major\ncloud vendors offer tools that make this collaboration process even simpler and, as\nyou can imagine, integrate with the rest of the cloud components.\nExercise 9.4\nWhich of the following is not a benefit of running your machine learning in a cloud\ndata platform?\n1\nMultiple people can collaborate on the same data sets, which significantly\nshortens the feedback cycle and improves productivity.\n2\nData scientists can use existing cloud storage or a data warehouse to browse\nand run exploratory analytics without having to bring data to their own\ncomputers.\n3\nCloud data platform ingestion and common data transformation steps elimi-\nnate much of the manual effort being done by data scientists.\n4\nRunning models at scale is slow, but it will work in the end.\n",
      "content_length": 2900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "283\nBusiness intelligence and reporting tools\n Azure ML is a service from Microsoft that offers an end-to-end model lifecycle\nfrom exploration up to production deployment. Azure ML provides a visual editor to\ncreate models or allows data scientists to write code in one of the popular program-\nming languages like Python or R. Azure ML integrates with most of the other Azure\ndata services such as Azure Blob Storage and Azure Synapse. Azure ML can also help\nwith operationalizing your models by deploying them as a web service or batch or real-\ntime data processing pipelines. \n AWS has a service called SageMaker, which offers an integrated development envi-\nronment (IDE) for model development and training. It has support for notebooks\nwhere data scientists can run experiments and share results with their peers. Sage-\nMaker notebooks can be connected to an existing Elastic MapReduce (EMR) cluster\nand use existing cloud data platform compute capacity to run the model training and\nvalidation. \n Google Cloud offers an AI Platform that is a collection of services that can simplify\ndevelopment of ML models and their deployment into production. AI Platform sup-\nports many of the popular ML libraries and frameworks like TensorFlow and Keras.\nYou can use notebooks for collaborations and experiments, but currently there is no\nintegration of these notebooks with existing Spark clusters.    \n9.5\nBusiness intelligence and reporting tools\nBI and reporting tools accessing data via the data warehouse are the most common\nways to consume data from the data platform. These traditional analytics use cases are\noften the initial driving force for a data platform to bring together all of the organiza-\ntion’s data, while applications and machine learning uses cases tend to come later. \n9.5.1\nTraditional BI tools and cloud data platform integration\nVarious types of BI tools have been around for as long as traditional data warehousing\ntechnologies, and it is not surprising that all of the existing tools on the market today\ncan easily integrate with relational cloud data warehouses, such as AWS Redshift, Goo-\ngle BigQuery, or Azure Synapse. The relational nature (here meaning that data is\norganized into tables that can be joined together) of the underlying data warehouse is\nimportant to the design of many BI tools because these tools assume the following:\nData has a flat structure (e.g., no nested data types).\nData can be sliced and diced in many different ways; for example, to provide\nusers with a drill-down functionality where you can go from a high-level aggre-\ngated report into lower-level details for a specific metric or a calculation.\nIn the early days of cloud data platforms, many BI tools struggled with BigQuery inte-\ngration because it didn’t use a fully relational model. For example, where a BI tool\nmight expect user data and user address details to live into two separate relational\ntables, BigQuery was better optimized for one table with address data being nested\ninside the user table. Many BI tools didn’t understand how to work with nested data\ntypes and couldn’t process arrays or JSON data. \n",
      "content_length": 3132,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "284\nCHAPTER 9\nData access and security\n This has changed in the recent years, and many major players in the BI space, such\nas Tableau, now have full support for BigQuery. But if your organization is using a\nniche BI solution, and you are planning on adopting Google Cloud for your data plat-\nform, make sure that your reporting solution will work properly with BigQuery out of\nthe box. \n Often people ask us if they can connect their BI tools directly to the data lake layer\nin their data platform. While the answer is “yes, you can,” it is not the solution we usually\nrecommend. Many modern BI and reporting tools today support Apache Spark and its\nSQL API, so technically your BI tools can query data directly from the data lake and pro-\nvide you with visualization and data exploration capabilities. The problem with this\napproach is that users expect BI tools to be interactive and responsive. It’s a frustrating\nexperience for data analysis or business users to wait tens of seconds or minutes for a\nSpark SQL query to finish. Cloud data warehouses are much better suited to provide\nthis interactive experience, and that’s one of the reasons we advocate for data ware-\nhouses to become a standard component in the data platform design. \n9.5.2\nUsing Excel as a BI tool\nOne of the most popular data processing and analytics tools on the planet is Excel,\nwhether you like it or not. A lot of business users are very proficient with it and can\nbuild complex analysis and visualizations in it. So a natural question we get asked a lot\nis whether you can use Excel or a similar tool to connect to the data warehouse. In\nfact, you can definitely do that since Excel has support for JDBC/ODBC connectors\nand can connect to any cloud data warehouse. Also, on Google Cloud, you can use\nGoogle Sheets (an alternative to Excel) to seamlessly connect to BigQuery. \n There are a few things to keep in mind when deciding to use Excel as an analytics\ntool against the data warehouse. First of all, Excel running on user’s computers can only\nhandle a small portion of the data sizes that a BI solution running on a dedicated server\ncan deal with. If your users routinely work with data sets that have millions of rows,\nExcel might not be the best choice from a performance point of view. Another issue\nthat is generally applicable to any tool that runs on the end-user computer versus a cen-\ntralized solution is security. We will talk more about protecting data in your cloud data\nplatform in the next section, but overall, providing multiple users with direct access to\nyour data warehouse from their computers is not a recommended security practice. \n9.5.3\nBI tools that are external to the cloud provider\nSo far we have talked about a cloud data platform as a solution that you build with a\ncombination of a single cloud provider’s tools and services and open source solutions.\nWith BI and reporting tools, where you run them doesn’t matter as much. It’s com-\nmon to see BI tools that are deployed on premises to connect to the cloud data ware-\nhouses, or reporting solutions implemented on one cloud provider connect to the\nwarehouses on another provider. The adoption rate and internal experience with a\nparticular tool is much more important in this case than the technical details. One\n",
      "content_length": 3282,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "285\nData security\nthing to keep in mind if you are deploying a cross-cloud solution is data egress costs.\nAll cloud providers charge you for getting data out of the cloud over the network, and\nthese costs can be substantial if your data volumes are large. Usually this is not a prob-\nlem with the reporting solutions because the queries run inside the warehouse and\nonly return a small result set, but you need to analyze your use cases carefully to avoid\ncloud billing surprises. \n In addition to establishing BI solutions and various SaaS offerings, cloud vendors\nalso provide reporting and visualization tools. Azure Power BI is probably one of the\nmost popular ones, and we often see Power BI being used with data platforms\ndeployed on clouds other than Azure. AWS offers a QuickSight solution, and Google\nhas a DataStudio and Looker BI platform.\n9.6\nData security\nData security and privacy should be on top of the priority list for any organization, no\nmatter whether they have their data platform built on premises or in the cloud. The\ncloud data platform design we have discussed in this book offers a great deal of flexi-\nbility and opens up multiple ways to connect to the platform, from data warehouse to\nmachine learning tools and applications. This proliferation of different types of data\nconsumers and modular platform design that requires multiple layers to communi-\ncate with each other requires a thorough approach to data security. Security is a very\nbroad and complex topic that warrants a book of its own. In this section, we will high-\nlight some of the key items and ideas on how to secure your cloud data platform.\n Cloud providers offer a flexible way to manage access to the cloud resources. You\nneed to design a security model that is easy to manage and maintain over time. One of\nthe common issues with cloud resource security is providing ad hoc access to individ-\nual users as the need arises. This approach makes it hard to get a clear picture of who\nhas access to what at any given point in time. \n9.6.1\nUsers, groups, and roles\nAll cloud providers have a notion of users, which includes human users and applica-\ntions or other cloud services and groups. You can either provide access to the cloud\nresources to individual users or applications, or you can arrange users into logical\ngroups and provide access to resources to the group instead. For example, you can\nhave a group called “Data Platform Operators” which would have permissions to pro-\nvision new resources in the cloud and reconfigure or delete cloud resources. Then\nyou can add the members of your Data Engineering or Operations team to this group.\nYou can also have separate groups for your data scientists and data users, and those\ngroups would have more restricted permissions to only read existing data and write\ndata to a dedicated sandbox area on the cloud storage. Data science teams may also\nneed permissions to provision new compute resources for training ML models. \n It is easier to manage permissions on the group level because you will typically have\nfewer groups than you have users. If someone leaves your organization, or if a new\n",
      "content_length": 3142,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "286\nCHAPTER 9\nData access and security\nmember joins the team, it’s just a matter of adding them to the right group, and they\nwill get access to the cloud resources they need. This significantly simplifies permis-\nsions management.\n It is also a good idea to spend some time and understand what kind of permissions\nusers need to do their jobs. Instead of assigning a very wide set of permissions to cloud\nresources, try to follow the principle of the least privilege. This principle says that you\nonly assign a bare minimum of permissions to a given group of users that is required\nfor them to do their jobs efficiently. While it is easier to give everyone administrative\nprivileges on the cloud resources, this can potentially lead to security issues or cloud\nresource mismanagement. \n9.6.2\nCredentials and configuration management\nLeaking cloud resource credentials such as passwords or secret keys is one of the most\ncommon security problems. Make sure you use cloud-specific key management solu-\ntions to store passwords and other secrets, and never hardcode passwords in your con-\nfiguration files or applications code. Many cloud services today offer a way to\nauthenticate to one another that doesn’t require a password or a secret key. Azure\nActive Directory authentication is a good example. Use cloud-native authentication\nwhenever it’s available. Minimizing the usage of passwords and other shared secrets\nreduces the risk of exposure.\n Another common problem with data security in the cloud is accidentally opening\nup access to your cloud data storage to the public internet. Cloud providers have an\noption to configure storage that doesn’t require any type of authentication and is pub-\nlicly available, for example, to host static websites. Having the same setting acciden-\ntally applied to your data lake storage would have disastrous consequences since\nanyone on the internet will be able to access the data. Use the infrastructure-as-code\napproach to make sure you know exactly which settings are being applied to your\ncloud resources, and audit them periodically. Cloud vendors also offer a notion of a\npolicy that allows you to describe which resources and which configurations are\nallowed for your organization. For example, you can create a policy that prohibits\npublicly available cloud storage in your organization.\n9.6.3\nData encryption\nAll cloud vendors encrypt all the data that is stored in the cloud storage or in cloud\ndata warehouses when it is actually saved into the disks or SSDs in the vendor’s data\ncenters. This encryption, while important, only prevents you from scenarios where\nsomeone got access to a physical infrastructure of your cloud vendor. This is not\nimpossible, but it is a highly unlikely event. In addition to the cloud vendor-provided\nencryption, you may want to encrypt specific sensitive data, such as personally identifi-\nable information, in your data platform. Encrypting specific columns in your data sets\nand then making sure that only a specific group of users have access to the decryption\nkeys is a good way to limit data exposure.\n",
      "content_length": 3091,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "287\nData security\n9.6.4\nNetwork boundaries\nMany cloud services, especially the PaaS solutions, by default are accessible from any\ncomputer on the internet. Accessible here means that they have a public IP address,\nbut it doesn’t mean anyone can actually access them without having a password or\nproviding any other authentication method. You need to work closely with your net-\nwork and security team to understand where your data consumers are located and\nwhich networks they will use to access the data platform. This way you can restrict\naccess to your cloud resources on the network level, which, in addition to other best\npractices covered in this section, will significantly reduce the probability of a security\nincident on your data platform.   \nSummary\nThe primary reason for developing a data platform is to cost effectively and\nsecurely make data available to data consumers—at scale. \nData consumers can be human users running reports or SQL queries against a\ndata warehouse or accessing raw data files for model development. They can\nalso be applications.\nEach public cloud provider offers a data warehouse as a service. Azure has Syn-\napse, Google Cloud has BigQuery, and Amazon has Redshift. They are similar\nin functionality, but there are some differences to be considered.\nWhile data warehouses remain the primary way to access data for BI tools and\ndirect SQL access, applications usually need a much faster response time than\neven modern cloud-based data warehouses can provide. \nUsing a dedicated data store for applications will provide faster data retrieval\ntimes and let you handle concurrent requests, but you should be aware of the\nsecurity risks associated with providing applications access to the data warehouse.\nThe proliferation of different types of data consumers and modular platform\ndesign that needs multiple layers to communicate with each other requires a\nthorough approach to data security. At a minimum, consider the use of users,\ngroups, and roles; the use of credentials and configuration management; data\nencryption; and using network boundaries to restrict access to cloud resources.\nApplication data store options include relational databases, key/value data\nstores, full-text search systems, and in-memory caches. Each cloud vendor offers\nthese as managed services.\nData scientists access and use data differently than do report users or applica-\ntions. They need access to large volumes of data, significant compute capacity\nto run their models, and a simple way to split data into training and validation\ndata sets. Luckily, a well-designed data platform stores all data, including raw\ndata and preprocessed data, and offers multiple different ways to access the\ndata: SQL, Apache Spark, direct file access, etc., to avoid limiting the data sci-\nence team in the selection of tools.\n",
      "content_length": 2838,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "288\nCHAPTER 9\nData access and security\nAll three major cloud vendors offer tools that make collaboration among data\nscientists simpler. Azure provides ML, while AWS and Google Cloud offer Sage-\nMaker and AI Platform, respectively. They also offer BI solutions for reporting\nand visualizing data in the data warehouse Azure has Power BI, AWS has Quick-\nSight, and Google Cloud has Data Studio and the Looker BI platform.\n9.7\nExercise Answers\nExercise 9.1:\n 2—They are likely to want different ways to access data. \nExercise 9.2:\n 3—BigQuery.\nExercise 9.3:\n 4—All of the above.\nExercise 9.4:\n 4—Running models at scale is slow, but it will work in the end.\n",
      "content_length": 656,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "289\nFueling business value\n with data platforms\nWe wanted to wrap up this book with a chapter on how your carefully designed\ncloud data platform will be used in a typical enterprise, and in doing so point out\nwhy we recommended some of the things we did. We’ll also talk about some of the\nnon-technical parts of an analytics project because a data platform doesn’t exist in a\nvacuum; it is only a single ingredient of an organization’s quest to become truly\ndata driven and enabled. Knowing how your platform delivers business value in the\nshort and long term is important because it can impact design, and understanding\nwhat factors other than technology can influence use of a data platform can help\nThis chapter covers\nUnderstanding how the data platform contributes \nto business value\nDeveloping a data strategy for your organization\nAssessing the analytics maturity of your \norganization\nAnticipating and responding to potential “data-\nplatform stoppers”\n",
      "content_length": 964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "290\nCHAPTER 10\nFueling business value with data platforms\nyou ensure that your platform becomes an integral and valued part of your organiza-\ntion’s assets. \nNOTE\nWe want to thank Pythian (www.pythian.com) for the use of the dia-\ngrams and frameworks used in this chapter.\n10.1\nWhy you need a data strategy\nLet’s start with what’s driving the need for a cloud data platform. At the highest level,\nthe business drivers that lead to the need for a data platform are most often a desire to\n(1) gain operational efficiencies, (2) grow revenues, (3) improve the customer experi-\nence, (4) drive innovation, and (5) improve compliance—or some combination of all\nfive. It is incredibly important that an organization understand what they are driving\ntoward as a business, because only then can you develop a data strategy that is focused\non producing business outcomes. At the end of the day, your data platform will be\njudged by how well it contributes to these business outcomes. \n Before you can leap from desired business outcomes to a cloud data platform, you\nneed a data strategy to guide you. Like many terms in our industry, data strategy can\nmean different things to different people, so while there is no commonly accepted\ndefinition of a data strategy, here are a few for consideration:\nAn approach that “aligns and prioritizes data and analytics activities with key\norganizational priorities, goals and objectives.” (Micheline Casey, CDO LLC)\n“A coherent strategy for organizing, governing, analyzing, and deploying an\norganization’s information assets that can be applied across industries and lev-\nels of data maturity.” (DalleMule and Davenport, Harvard Business Review)\n“Intentional action & prioritization plan to harness and integrate data, to create\nand disseminate information/intelligence, to advance a business mission.”\n(Braden J. Hosch, Stony Brook University)\nAs shown in figure 10.1, the purpose of a data strategy is to connect the dots between\nan organization’s data and its business goals, establishing how data and analytics can\nsupport the organization’s business goals and priorities. \nCommon\nbusiness\ngoals\nA data strategy\nconnects data to\nthe business goals.\nGrow\nrevenues\nGain\noperational\nefficiencies\nImprove\ncustomer\nexperiences\nDrive\ninnovation\nImprove\ncompliance\nData strategy\nData\nFigure 10.1\nThe data strategy outlines how data can serve the organization’s business goals.\n",
      "content_length": 2410,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "291\nThe analytics maturity journey\nHere are a few examples of high-level data strategy statements. Of course, a full data\nstrategy includes all of the ways you will use data and technology and change processes\nand engage people to realize the vision, but that would be far beyond the scope of this\nchapter:\nIf an organization wants to empower innovation across the organization, one\nstrategy to do this might be to make self-service analytics an accepted part of the\norganization’s behavior. From this strategy comes the need for a data platform\nthat facilitates access to well-curated data for different types of users. \nIf an organization’s goal is to win more business by delivering better pricing\nproposals faster than their competitors, a data platform that is optimized for\nrapid processing of data supported by machine learning for delivery of recom-\nmended pricing options is important. \nFor a financial services or retail organization whose primary business goal is to\nimprove the customer experience to grow revenues, a data platform that is opti-\nmized to predict customer propensity to buy and deliver data about those seg-\nments to a marketing automation system would make sense. \nIn a different industry, such as with the gaming example in chapter 6, if the\nbusiness goal is to keep users engaged in a game for as long as possible to maxi-\nmize in-game purchases or ad views, a data strategy that supports this would\ninclude a data platform that is optimized to use data in real time to adjust the\ngame to suit the individual playing.\nFor a mining organization whose primary goal is to reduce operational costs, a\ndata platform can support this goal by optimizing for capturing data from\nsensors on trucks, combining it with other data, and then predicting when\nmaintenance is needed before the trucks break down miles away from a service\ncenter.\n10.2\nThe analytics maturity journey\nNobody can wave a magic wand and deliver operational efficiencies, revenue growth,\nimproved customer experiences, or innovation overnight—becoming truly data-\nenabled is a multiyear journey that starts in different places for different organizations\nor even teams within organizations. A typical analytics maturity journey often looks\nlike figure 10.2, which shows an evolution from using data to get insights on “what is\nand was” (SEE) to using data to predict what to do next (PREDICT) to using data to\ndrive other applications (DO) to using analytics to feed application development\n(CREATE). While the picture doesn’t show it, this process is almost always iterative\nwith many loops in each stage. Let’s break down each of these in more detail.\n \n \n \n \n",
      "content_length": 2659,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "292\nCHAPTER 10\nFueling business value with data platforms\n10.2.1 SEE: Getting insights from data\nAs shown in figure 10.3, most businesses start with a desire to “see” their business—\nusing reports and dashboards to gain insights into what is happening today compared\nto what happened in the past. \nIn a traditional data warehouse world, specialized report creators take requests from\nthe business and produce static reports that are delivered to the requestor. This often\ntake time—sometimes weeks can go by before the business user gets the report they\nrequested, and as anyone who’s ever produced a report knows all too well, the user\nthen immediately wants changes. Another drawback, of course, is that traditional data\nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nAn analytics\nmaturity\njourney is\ndefined in the\ndata strategy.\nFigure 10.2\nA typical analytics journey starts with “seeing” data and can evolve into creating \nnew products using data.\nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nA desire to get\ninsights from data\nis often the first\nstep in an analytics\nmaturity journey.\nFigure 10.3\nMost organizations start by bringing data together to allow the business to “see” the \nentirety of the business.\n",
      "content_length": 1507,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "293\nThe analytics maturity journey\nwarehouses are often limited in terms of the data stored in them—while financial data\nis almost always included, data from the growing number of SaaS systems is often\nexcluded. This means that the business user is limited in their analysis to the data in\nthe data warehouse, or they are forced to scrape/export data from different places\nand integrate it themselves into a spreadsheet. Getting a true view of the business\nacross multiple data sources using traditional methods is very difficult, and to many\nbusiness users, it’s an impossible task.\n In a modern data world, these insights are driven by self-service analytics, where the\nbusiness is empowered to use their tools of choice to access any and all data they need\nfor exploration—we call this Bring Your Own Analytics (or BYOA). The argument in\nfavor of BYOA is that different users have different levels of knowledge and comfort in\nusing various tools to analyze data. In some cases, a user may want to only look at a pre-\ncreated dashboard and maybe select a few parameters to drill down into the data\ncontained in the dashboard. In another case, a power user may be comfortable with a\nproduct such as Looker and may want access to a data set or data sets to do their own\nexploration and analytics. Most of these insights are created using off-the-shelf SQL-\nbased tools accessing data in the data warehouse as discussed in the last chapter, but what\nis driving BYOA is the recent introduction of a variety of off-the-shelf options and the\nfact that as people join the organization, they may come with preexisting knowledge of\nthese tools, which means they can be productively analyzing data far more quickly than\nif they have to learn new tools. A data platform that is optimized to allow different people\nto use different tools is one that maximizes the use of insights across the organization\nwhile ensuring compliance with company or government policies for data access.\n And of course, the fact that your data platform is capable of ingesting, integrating,\nand preparing data from an almost unlimited number of data sources means that the\ndata you are giving users is rich and broad and easily consumed from a single location.\nEliminating data silos and manual data integration will result in happy business users.\n10.2.2 PREDICT: Using data to predict what to do \nAs shown in figure 10.4, once business users have visibility into the present and past, they\noften look to more advanced analytics such as machine learning to look at data in a dif-\nferent way, predicting what they should do next to reach their business goals (PREDICT).\n Machine learning models are developed by accessing data in both the data ware-\nhouse and in storage (the lake). The key to effective machine learning models is\ndata—they need lots and lots of data. While cloud data warehouses are increasingly\noffering integrated machine learning features, a well-designed data platform won’t\nstore raw data in the warehouse; it will store it outside the data warehouse where it is\nprocessed and transformed into the clean aggregated data that is aligned with the\nneeds of most business users wishing to produce insights.\n This means that the data science team will want to access data in storage (or the lake\nportion of the data platform). Recognizing that different users have different data needs\nis a core tenet of good platform design—luckily, if you’ve read this book, you’ll have\ndesigned a platform that can support both insight generation and machine learning.\n",
      "content_length": 3543,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "294\nCHAPTER 10\nFueling business value with data platforms\nOur example mining organization, whose primary goal is to reduce operational costs,\nmight use machine learning to predict when maintenance is needed before the trucks\nbreak down miles away from a service center. Our example retail organization, whose\nprimary business goal is to improve the customer experience to grow revenues, might\nuse machine learning to predict which group of customers is most likely to respond to\na particular offer or set of offers so they can target that group, delivering the right\noffer to the right customer, which enhances the customer experience. It should be\nnoted that machine learning is only one form of advanced analytics that could apply\nto these use cases, but of these types of analytics, it is certainly the most talked about\nthese days.\n10.2.3 DO: Making your analytics actionable\nOnce data has been explored, either in the SEE or PREDICT phases, the outputs of\nthat exploration can be automatically delivered to external systems for action (DO),\nshown in figure 10.5. \n This could be a segment of customer data that a business user feels would respond\nbest to a particular marketing message that then gets delivered to a marketing automa-\ntion system, or it can be a machine learning model, such as a recommendation engine,\nwhere the output is integrated into an e-commerce system. When selective data is moved\nfrom the data platform to other systems, we call this orchestration—picture a conductor\nmaking all of the different players in an orchestra work together in harmony.\n Making data actionable, and making those actions automated, really changes the\nnature of your data platform into a mission-critical system. It’s one thing when a sys-\ntem that produces reports and insights responds slowly or is unavailable for periods of\ntime. But a system that is driving recommendations on an e-commerce platform or\ndelivering near-real-time personalized marketing offers, or that is helping optimize\nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nMachine learning\nand other advanced\nanalytics can produce\npredictions, telling\nthe business what\nthey “should” do,\nnot just what is\ntoday.\nFigure 10.4\nUsing advanced analytics can help an organization move from exploring the past and \npresent into predicting what to do in the future.\n",
      "content_length": 2474,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "295\nThe analytics maturity journey\nadvertising spend by deciding what segments should get which ads and when, simply\nmust be performant and available.\n10.2.4 CREATE: Going beyond analytics into products\nIt’s sometimes easy to overlook the fact that all this data you’ve collected has another\nuse not typically associated with analytics. In the CREATE stage (figure 10.6), data that\nmay have been gathered with analytics in mind is made available to developers to\ndrive the creation of new products. That same analytics data can also be a data source\nfor applications, as outlined in chapter 9. \nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nData that moves from \nthe data platform to \nanother system creates \nactionable data.\nFigure 10.5\nWhen data flows from the data platform into other systems automatically, it can drive \nfurther actions across the organization.\nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nData in the data\nplatform can also\nbe used to produce \nproducts.\nFigure 10.6\nEnterprise data platforms can also be a source of data for application development.\n",
      "content_length": 1363,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "296\nCHAPTER 10\nFueling business value with data platforms\nHere are a few examples of how organizations moved to create new products from\ntheir analytics data.\n A bank that started out wanting to improve their customer experience by helping\ntheir agents anticipate and react to customers likely to churn realizes that this same\ndata could be used to enhance their mobile apps. \n A security software company’s service is watching for and reacting to intrusion\nattempts before there is any impact. Their value is preventing attacks, so when they are\nsuccessful, nothing is visible to their clients. While their clients don’t experience any\nintrusions, they also can’t “see” all the prevented attacks, so the software company uses\nthe data from the prevented intrusions to create a dashboard exposing all of the\nthwarted attacks. Their customers can suddenly see the value of the service.\n A postal service is able to take data about parcel deliveries—what type of goods,\ndelivered to what neighborhoods at what frequency—and turn it into a value-added\nrevenue-generating service for other suppliers. This new product shows suppliers data\nthat helps them to understand shopping trends by neighborhood, enabling them to\nfocus their marketing efforts on more lucrative neighborhoods. \n10.3\nThe data platform: The engine that powers analytics maturity\nIn almost every customer engagement, we hear that the data our customers want to\nuse is stuck in silos, a big barrier to making it usable. In some cases with traditional\ndata warehouses, adding new data sources, especially data sources that include\nunstructured data, is challenging, and analytic response times on traditional data\nwarehouses can often be slow. In other cases, analytics are being performed on data\nfrom each silo individually, giving insights that only tell part of the story. In still other\ncases, manual extraction of data is followed by manual integration of data sets, an\nexpensive and cumbersome process that must be repeated each time updated data is\navailable. For more advanced organizations that are developing machine learning\nmodels, most of the work is being done in small data sets with little ability to scale and\nintegrate these models into a production environment.\n The foundation for every modern analytics program is a cloud data platform where\ndata of all types can be ingested and processed at any speed and delivered to users and\nsystems alike in a manageable, scalable, cost-effective way. This can be seen in figure 10.7.\n Taking time to develop a data strategy and truly understand how the business will\nbe using data will help ensure that you build a data platform that can (1) support the\nvariety of data that will be needed and (2) support the expected range of users as well.\nFollowing the guidance of this book will also ensure that your data platform is flexible\nenough to grow as your organization moves along in its maturity journey.\n It’s also important to note that this journey is not always linear—we’ve seen organi-\nzations develop data platforms exclusively to support making predictions, for exam-\nple. What is more common is that different groups in the company are at different\nstages, which means that a strong platform design is even more important as you deal\nsimultaneously with different data consumers who all have different needs.\n",
      "content_length": 3343,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "297\nPlatform project stoppers\nOne thing is certain—without a strong data foundation, none of this is possible.\n10.4\nPlatform project stoppers\nThe success of a data platform project goes well beyond what we’ve talked about in\nthis book. You can design the most flexible, scalable, secure platform in the world, but\nif the business doesn’t buy into its outputs, it will all be in vain. Aside from technology,\nkeys to project success include (1) the ability to deliver value quickly and iteratively,\n(2) managing the change that a data platform introduces to the organization and\nensuring that people actually use the platform, (3) delivering quality data so that users\ntrust the data they are accessing, (4) recognizing and acting on the fact that the plat-\nform impact extends beyond the platform itself—into the hands of the users them-\nselves, and (5) continuously improving the platform from the perspective of evolving\ncloud costs and keeping pace with users’ evolving needs and expectations.\n10.4.1 Time does indeed kill \nGone are the days when the CFO was happy to sign off on a major project investment\neven though everyone agreed it would take years before the organization would be  able\nto derive value from it. More and more we are seeing pressure from the business to\ndeliver value faster—almost always in months, not years. Designing and implementing\nan enterprise data platform is not a simple task, and to do it well is a complex under-\ntaking that will empower new uses of data across the enterprise. This analytics journey\nwill definitely evolve over years, not months, so reconciling the CFO’s or CEO’s need\nfor rapid “payback” against the complexity and importance of an enterprise data plat-\nform isn’t easy. We’ve seen projects die before they even get started because of this.\n In our experience, the best way to address this concern before it becomes a prob-\nlem is to develop your platform with the end goal in mind. We’ve talked at length\nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nA cloud data\nplatform is the\nfoundation for all\nstages of analytics\nmaturity.\nFigure 10.7\nAn analytics journey isn’t possible without a foundation of clean, integrated, organized \ndata.\n",
      "content_length": 2340,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "298\nCHAPTER 10\nFueling business value with data platforms\nabout designing for the long term—this can be coupled with implementing for the\nshort term. What we mean is that you should work with the business users to find a\ncompelling use case, one where the users can articulate what the business benefit of\ntheir use of data would be, and hopefully one that isn’t too complicated. Start with\nthat one use case, work with the business to truly understand what they need from\ndata, and then work backward to the data that is needed to answer their question or\nfuel their analysis. Ingest, transform, and model only the data they need—in our expe-\nrience, that is usually two to three data sources.\n Once the first use case is showing value, move on to the next one and the next one,\nand soon you’ll have exercised your engineering processes to the point where more\nand more use cases can be delivered in parallel. The end result is demonstrable busi-\nness value sooner, which will help you gain support for ongoing investment in your\nplatform.\n The alternative approach is to start at the data source and ingest every possible\ndata source into the platform, even before you know how or even if the data will be\nneeded. Yes, this is the proverbial “data swamp.” It takes more time before you’ll see\nthe data being used, and the effort needed to make sense of the data in the swamp will\nbe much greater,\n10.4.2 User adoption \nFor most organizations, moving from a traditional report-producing operating mode\nto one of user self-service is a major change. And change is always difficult. Most users\ndon’t like change—they are busy, they have their current methods of doing analysis\nand reporting, they hoard their data, they don’t want to take the time to learn some-\nthing new—the excuses are almost infinite.\n Grassroots adoption of new business models, such as those that a data platform\nencourages, is critical to successful adoption. If nobody uses your platform, it can’t\never be considered a success.\n There are some things you can do to help. In the previous section, we talked about\nfinding the early use cases. Inevitably, in the organization you will have the “early\nadopters” (they are the ones asking for more data), the “blockers” (they are the ones\nexpressing skepticism that this will ever work and often seeding doubt in the minds of\nothers), the “chickens” (those who are simply afraid to try anything new) and “the\navoiders” (those who don’t want anything to do with anything new).\n Converting all of these different types of people into avid users and supporters of\nyour data platform can be difficult at times, but we’ve seen a few effective techniques\nthat you might want to consider:\nMake sure the first users are both early adopters and influential employees.\nEarly adopters are most likely to collaborate with you to get results faster, and if\nyou pick a use case that has exposure to senior management, it is more likely\nthat the use case outcomes will make their way into a C-suite presentation. That\nwill help demonstrate the business value of the data platform.\n",
      "content_length": 3086,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "299\nPlatform project stoppers\nIf you are brave enough, shortly after helping an early adopter group, offer to\nsolve a problem being experienced by a blocker. “Why invite pain?” you might\nbe asking. If you can convert a blocker into a fan and get them to start talking\nabout how awesome their data use is, others in the company will know that even\nthe toughest critics have been silenced. And that is great internal marketing.\nThe chickens need lots of attention—lots of lots of training and coaching will be\nrequired to get this group comfortable with using data differently. Encourage\nand support this training and any centers of excellence that might be set up.\nThe avoiders will come along eventually, and it’s OK if they take a bit longer—\nby then there will be lots of “experts” in the company to help them. This last\npoint, encouraging use and expertise across the organization, is important, and\nit will be much easier if you have an executive sponsor—someone in the C-suite\nwho totally buys into the need for organizational transformation using data.\nThis can be the Chief Data Officer, the CIO, the CMO, or even the CEO, but\nwithout this kind of support, adoption will be slower. An effective use of your\nexecutive sponsor might be to create visibility for uses of data both at the C-level\nand across the company. Disney did this by creating a contest where data users\ncould show off their output to the entire company and be rewarded for their\nefforts and the value they created. This showed the company that data was\nimportant, and it also created experts and gave them rewards and additional vis-\nibility. Many of these champions went on to become data coaches, which in turn\naccelerated data and analytics maturity across the organization. \n10.4.3 User trust and the need for data governance\nThe only thing worse than not seeing anyone use your data platform is when you have\nusers but they don’t trust the data it produces.\n Anyone who has found themselves presenting data to a group of people when some-\none points out that the number(s) you are showing are simply not correct, knows that\nit’s the moment you lose the room. The minute anyone sees inaccurate data is the\nmoment that they start mistrusting all future data. From that moment onward, regain-\ning user trust must become the most important thing the extended data team works on.\n At this point, the platform team is likely to point out that they made sure that data\nmade it to the data warehouse, and it was at that point that the business started using\nit, so it’s not the platform’s fault. That may be true, but if you want your platform to be\nused for the foreseeable future, you have to extend your thinking of the data platform\nto include the final destination of your data even if you don’t have control over it. This\nis where data governance comes in. \n While data governance is a broad topic that goes beyond data quality, efforts to\nimprove data quality are an integral part of most governance programs, and measure-\nments of data quality levels are the most widely used data governance metrics.\n Examples of metrics on data quality include percentages of the correct entries in\ndata sets, required data fields that are filled in, and data values that match across\n",
      "content_length": 3253,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "300\nCHAPTER 10\nFueling business value with data platforms\ndifferent systems, plus other measurements of attributes such as data accuracy,\ncompleteness, consistency, and integrity. In chapter 5 we talked about how to\nimplement the rules that are the output of data quality metrics definitions, but we\ndidn’t talk about how these rules came to be. That is usually the work of a group of\npeople from across the organization, in particular, data stewards who are the business\nowners of the data. The designers of the data platform must become a part of the data\ngovernance process—understanding what the business needs from a data quality\nperspective is critical to implementing the necessary changes to the platform.\n Ensuring data quality is more than just applying business rules as data quality checks\non pipelines, it’s also what happens when the data fails the quality check. The goal in\ntaking action is to ensure that (1) the consumers of that “failed” data are aware that the\ndata product is not to be trusted until the issue can be resolved, and (2) that the people\nwho can fix the problems are aware of the need for action, as quickly as possible. And\nyes, that involves going beyond the confines of the data platform.\n10.4.4 Operating in a platform silo\nSilos are a pervasive fact of life in organizations. You know this or you wouldn’t be\nworking so hard to break down the data silos that are stopping your organization from\nfully using their data. But silos are sneaky, and sometimes we see them in the IT orga-\nnization as well.\n Think of the reach of your data platform—it extends from outside your company\nas you reach into SaaS data sources and ingest them, all the way to the dashboard that\nyou didn’t have any part in designing. It can also extend into other systems—into the\nwebsite where the output of a machine learning algorithm is embedded or into the\nmarketing automation system where a segment of users extracted from your data plat-\nform was used as an input to an email marketing campaign. Where exactly does the\ndata platform start and stop?\n It’s too easy for an IT department to take the easy way out—to say that the platform\nstarts when the data comes into the platform and ends when it’s delivered as a data\nset. But we know that this is a short-sighted approach that will lead to many of the\nother problems we’ve identified in this chapter. A data platform is a system with a long\nreach—think of it as having tentacles that reach into many other systems and into all\nparts of the organization.\n Accepting responsibility for the end-to-end system can be the role of IT, but in our\nexperience should be done with the support of the users so that data ownership can\nbe moved to the business, so that the ongoing evolution of data quality rules is driven\nby the business, so that the necessary SLAs for data pipelines are defined mutually by\nthe business and IT. While IT can take ownership of implementation and support, the\nway in which the platform must operate and be measured must be a shared responsi-\nbility. Encourage the creation of multifunctional teams who care about the full system,\nand be an active participant on these teams to share the workload and to create\nshared ownership of a continuously evolving and successful platform. \n",
      "content_length": 3273,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "301\nPlatform project stoppers\n10.4.5 The dollar dance\nA second potential platform project stopper is cost. Understanding the dollar dance is\ncritical to project success. Here’s what we see happening over and over again. The\norganization (rightly) gets sold on the potential of cloud, not just for its agility and\navailability of tools and its opportunity to reduce support costs but also on the promise\nof eliminating large capital expenditures for hardware and especially on the promise\nof only having to pay as the system is used. The data platform project is approved, the\ndesign is done to meet all the requirements, it’s implemented well, and because\nyou’ve also implemented a strong data governance practice, data quality is high and\nusers are consuming data and driving better business outcomes. What could possibly\nbe wrong with that scenario, you ask?\n What happens next, usually within a few months of the solution being in use, is\nthat the CFO starts taking a closer look at the cloud consumption invoices, which\n(understandably) are growing as more data is being ingested, stored, and used. Inevi-\ntably, the CFO will come to the IT team to ask what can be done to slow the increase.\nGetting ahead of this request is important. From an operating perspective, investing\nin FinOps (the ongoing analysis and optimization of cloud consumption costs) is\nimportant. If you can understand the trends and take action to optimize costs before\nthey become excessive, your CFO will thank you, and one can argue that by avoiding\nunnecessary costs, you are creating a form of job security as more money will be avail-\nable for ongoing development. \n The dollar dance has two sides—the top side of the dollar dance is the value the\nplatform brings to the business. The more that can be quantified the better. Perhaps\nyou can encourage the business to share the benefits—how much more revenue came\nin because they were able to do more targeted marketing, or how much was saved\nwhen the algorithm running against all the data in your platform reduced machine\nmaintenance costs by predicting failures in advance. The bottom side of the dollar\ndance is the cost side—how to make sure that you aren’t spending anything that\ndoesn’t have to be spent. While FinOps is often not the responsibility of the data plat-\nform designer, there are times when changes to the platform design can realize big\ncost savings.\n In some cases, there isn’t much to be done—the design may be optimal, users may\nbe consuming data efficiently, and it may even be possible to show how the value that\nis being delivered to the business far exceeds the cloud costs. In other cases, there may\nin fact be something that can be done—but not without a good understanding of how\ncloud consumption costs work coupled with an understanding of what the design\ntrade-offs might be.\n Let’s consider a real-world example. A large telecommunications company built a\ndata platform on Google to capture IoT data from their systems. It was a lot of data\nspanning many years. Best practices suggest that the processing of this data should be\ndone outside of the data warehouse, in this case using Spark. There were many good\nreasons for this design decision, as we discussed in chapter 2, including flexibility,\n",
      "content_length": 3264,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "302\nCHAPTER 10\nFueling business value with data platforms\ndeveloper productivity, data governance, cross-platform portability, performance, and\nspeed. However, it turned out that the design team was unaware that the business had\na plan with Google that gave them unlimited use of BigQuery for a fixed fee per\nmonth. Had this not been the case, doing the processing outside the data warehouse\nwould have been the absolute best decision, but it turned out that by moving the pro-\ncessing into BigQuery, the company could save more than $600K per year. So while it\nmight not have been the perfect design change, the savings were so significant that it\nwas considered to be the right decision.\n Our point with these stories about possible platform stoppers is that the data plat-\nform doesn’t exist in a vacuum—it is a critical part of a much larger ecosystem. The\nneed for platform designers to work with the business can’t be underestimated as the\nmore its designers are connected to the business, the more likely it is that the platform\nproject will be a success—not just technically but also business-wise.\nSummary\nThe business outcomes often tied to a data platform can include a desire to (1)\ngain operational efficiencies, (2) grow revenues, (3) improve the customer\nexperience, (4) drive innovation, and (5) improve compliance—or some com-\nbination of all five. \nThe purpose of a data strategy is to connect the dots between an organization’s\ndata and its business goals, establishing how data and analytics can support the\norganization’s business goals and priorities. \nA typical analytics maturity journey shows an evolution from using data to get\ninsights on “what is and was” (SEE) to using data to predict what to do next\n(PREDICT) to using data to drive other applications (DO) to using analytics to\nfeed application development (CREATE). It is defined in the data strategy. A\ncloud data platform is the foundation for all stages of analytics maturity.\nA desire to get insights from data is often the first step in an analytics maturity\njourney. These insights are driven by self-service analytics, where the business is\nempowered to use their tools of choice to access any and all data they need for\nexploration—we call this Bring Your Own Analytics (or BYOA).\nOnce business users have visibility into the present and past, they often look to\nmore advanced analytics such as machine learning to look at data in a different\nway, predicting what they should do next to reach their business goals.\nData that moves from the data platform to another system creates actionable\ndata and converts your platform into a mission-critical system. Data in the data\nplatform can also be used to produce new products.\nThe success of a data platform project requires not just a good design, but also\nhaving the business buy into its outputs. Aside from technology, keys to project\nsuccess include (1) the ability to deliver value quickly and iteratively, (2) manag-\ning the change that a data platform introduces to the organization and ensur-\ning that people actually use the platform, (3) delivering quality data so that\n",
      "content_length": 3125,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "303\nPlatform project stoppers\nusers trust the data they are accessing, (4) recognizing and acting on the fact\nthat the platform impact extends beyond the platform itself—into the hands of\nthe users themselves, and (5) continuously improving the platform from the\nperspective of ongoing cloud costs.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n",
      "content_length": 373,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "305\nindex\nA\naccess and security property 135\naccessing data 33–288\nbusiness intelligence (BI) tools 283–285\nexternal cloud provider tools 284–285\ntraditional tools and cloud data-platform \nintegration 283–284\nusing Excel as 284\ncloud data warehouses 263–274\nAWS Redshift 264–268\nAzure Synapse 268–270\nchoosing right data warehouse 273–274\nGoogle BigQuery 270–273\nfor applications 274–278\ncloud key/value data stores 276–277\ncloud relational databases 275–276\nfull text search services 277–278\nin-memory cache 278\nmachine learning (ML) on data platform\n278–283\ncloud collaboration tools 282–283\nmodel lifecycle on cloud data platform\n279–282\ntypes of data consumers 262–263\nActivity ID attribute 212\nALL value 265\nAmazon Web Services see AWS (Amazon Web \nServices)\nanalytics maturity 291–296\ndata platform powering 296–297\ngetting insights from data 292–293\ngoing beyond analytics into products 295–296\nmaking analytics actionable 294–295\nusing data to predict what to do 293–294\nApache Atlas 223\nApache Avro schemas 243–245\nApache Cassandra database 104\nApache Spark, schema inference in 237–241\nAPI, ingesting data via 82\napplication data access 274–278\ncloud key/value data stores 276–277\ncloud relational databases 275–276\nfull text search services 277–278\nin-memory cache 278\narchive area 134\narchive container 138\nAUTO value 265\nautomation 80\nautomation maturity property 58\nAvro file format 140–143\nAWS (Amazon Web Services)\nmapping layers to 62–66\nbatch data ingestion 62–63\nbatch data processing 64\ncloud warehouse 64\ndata consumers 65–66\ndata platform storage 63–64\ndirect data platform access 65\nETL overlay and metadata repository 65\norchestration layer 65\nreal-time data processing and analytics 64\nstreaming data ingestion 63\nreal-time processing services 190–192\nAWS Glue Data Catalog 221\nAWS Redshift 264–268\nAzure\ningesting data into 26\nmapping layers to 70–74\nbatch data ingestion 71\nbatch data processing 72\ncloud data warehouse 72\n",
      "content_length": 1950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "INDEX\n306\nAzure (continued)\ndata consumers 74\ndata platform storage 72\ndirect data platform access 72\nETL overlay and metadata repository 73\norchestration layer 73\nreal-time data processing and analytics 72\nstreaming data ingestion 71\nreal-time processing services 193–194\nAzure Data Catalog 221\nAzure Stream Analytics 194\nAzure Synapse\naccessing data, cloud data warehouses 268–270\ningesting data from data platforms 25–26\nB\nbackward-compatible schema 233\nbatch data\ningestion\nAWS 62–63\nAzure 71\ncombining real-time data and 188–190\ndifferences between streaming ingestion \nand 117–119\nGoogle Cloud 67\nopen source and commercial alternatives\n74–75\nprocessing\nAWS 64\nAzure 72\nGoogle Cloud 68\nBatchId 136\nBI (business intelligence) tools 283–285\nexternal cloud provider tools 284–285\ntraditional tools and cloud data-platform \nintegration 283–284\nusing Excel as 284\nBring Your Own Analytics (BYOA) 293\nbusiness metadata 198–199\nbusiness value and data platforms 303\nanalytics maturity 291–296\ndata platform powering 296–297\ngetting insights from data 292\ngoing beyond analytics into products 295–296\nmaking analytics actionable 294–295\nusing data to predict what to do 293–294\ndata strategy 290–291\nplatform project stoppers 297, 301–302\ncost 302\noperating in platform silos 300\ntime 297–298\nuser adoption 298–299\nuser trust and need for data governance\n299–300\nBYOA (Bring Your Own Analytics) 293\nBytes Read attribute 212\nBytes Written attribute 212\nC\nCassandra database 104\nCassandra Query Language (CQL) 104\nCDC (change data capture) 165, 174\nRDBMS ingestion flow 94–98\nvendors 98–100\nMicrosoft (MS) SQL SERVER 99–100\nMySQL 99\nOracle 98–99\nPostgreSQL 100\nCI/CD (continuous integration/continuous \ndeployment) 73, 216, 234\nCloud architecture fit property 58\ncloud costs considerations 34–35\ncloud data platforms 49, 77\narchitecture example 23–24\nemergence of 9–10\nlayered architecture 10–14, 38–58\nETL tools overlay 56–58\nfast and slow storage layer 44–46\nimportance of 59–60\ningestion layer 10–11, 40–43\norchestration layer 53–56\nprocessing layer 12–13, 46–47\nserving layer 13–14, 52\nstorage layer 11–12\ntechnical metadata layer 47–49\nmachine learning (ML)\ncollaboration tools 282–283\nmodel lifecycle 279–282\nmapping layers to tools 60–70, 74\nAWS 62–66\nAzure 70–74\nGoogle Cloud 66\nopen source and commercial alternatives 74–75\nbatch data ingestion 74–75\norchestration layer 75\nvariety 14–15\nvelocity 15–16\nvolume 15\ncloud data warehouses\naccessing data 263–274\nAWS Redshift 264–268\nAzure Synapse 268–270\nchoosing right data warehouse 273–274\nGoogle BigQuery 270–273\narchitecture example 22–23\nAWS 64\nAzure 72\n",
      "content_length": 2611,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "INDEX\n307\ncloud data warehouses (continued)\ndata platforms vs. 19–24\ncloud data platform architecture \nexample 23–24\ncloud data warehouse-only architecture \nexample 22–23\ndata sources 20–22\nGoogle Cloud 69\nschema management features of 257–258\ncloud data-platform integration 283–284\nCloud Dataflow 193\ncloud key/value data stores 276–277\ncloud metadata services 221–222\nCloud Pub/Sub storage service 192\ncloud relational databases 275–276\ncloud services for real-time data processing and \nanalytics 190–194\nAWS 190–192\nAzure 193–194\nGoogle Cloud 192–193\ncloud storage organization 132–139\ncommercial alternatives 74–75\nbatch data ingestion 74–75\norchestration layer 75\nstreaming data ingestion and real time \nanalytics 75\ncompatibility rules 249–251\nconfiguration files, metadata layer as 214–217\nconfiguration management 286\nConnectivity Details attribute 207\nconsumers 168\ncontainer storage tier 135\ncontainers 134–139\nfolder naming conventions 135–138\norganizing streaming data 139\ncontinuous integration/continuous deployment \n(CI/CD) 73, 216, 234\ncontract, schema as 233–235\n_corrupt_record field 240\ncost 74\nas platform project stoppers 301–302\ncloud costs considerations 34–35\ncost efficient property 46\nCQL (Cassandra Query Language) 104\nCreated Timestamp attribute 205–208, 210, 248\ncredentials, data security and 286\nD\ndata\ngetting insights from 292–293\npredicting what to do with 293–294\ndata access 268, 288\nbusiness intelligence (BI) tools 283–285\nexternal cloud provider tools 284–285\ntraditional tools and cloud data-platform \nintegration 283–284\nusing Excel as 284\ncloud data warehouses 263–274\nAWS Redshift 264–268\nAzure Synapse 270\nchoosing right data warehouse 273–274\nGoogle BigQuery 270–273\nfor applications 274–278\ncloud key/value data stores 276–277\ncloud relational databases 275–276\nfull text search services 277–278\nin-memory cache 278\nmachine learning (ML) on data platform\n278–283\ncloud collaboration tools 282–283\nmodel lifecycle on cloud data platform 279–282\ntypes of data consumers 262–263\ndata consumers\nAWS 65–66\nAzure 74\nGoogle Cloud 70\nserving layer and 49–52\ntypes of 262–263\ndata encryption 286\ndata governance 299–300\nData Hub 224\ndata ingestion 40–43, 75\ndata lakes 6–7\ndata platform storage\nAWS 63–64\nAzure 72\nGoogle Cloud 68\ndata platforms 17, 36\naccessing data 33–34\nchange from data warehouses to 2–3\ncloud costs considerations 34–35\ncloud data platforms 15\nbuilding blocks of 10–14\nemergence of 9–10\nvariety 14–15\nvelocity 15–16\nvolume 15\ncloud data warehouses vs. 19–24\ncloud data platform architecture \nexample 23–24\ncloud data warehouse-only architecture \nexample 22–23\ndata sources 20–22\ndata lakes 7\ndata warehouses struggling with 3\nall V's at once 6\nvariety 4\nvelocity 5\nvolume 5\n",
      "content_length": 2733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "INDEX\n308\ndata platforms (continued)\nfueling business value with 303\nanalytics maturity 291–297\ndata strategy 290–291\nplatform project stoppers 297–302\ningesting data 24–28\ndirectly into Azure Synapse 25–26\ninto Azure data platform 26\nmanaging changes in upstream data \nsources 26–28\nmachine learning (ML) on 278–283\ncloud collaboration tools 282–283\nmodel lifecycle on cloud data platform\n279–282\nprocessing data 28–32\nin data platform 31–32\nin warehouse 29–31\npublic cloud 7–9\nschema management in 235–241\nreal-time pipelines 241\nschema inference in Apache Spark 237–241\nschema management module 236–237\nData Quality Check IDs attribute 206–208\ndata quality checks 150–152\nin real-time 187–188\nmetadata domains 208–210\nData Quality Checks domain 204\ndata security 285–287\ncredentials and configuration \nmanagement 286\ndata encryption 286\nnetwork boundaries 287\nusers, groups, and roles 285–286\ndata source name 136\ndata sources 20–22, 79–83\nfiles 81–82\nrelational databases (RDBMS) 80\nSaaS data via API 82\nstreams 82–83\ndata strategy 290–291\ndata transformations in real time 178–190\ncombining batch and real-time data 188–190\nconverting message formats in real-time \npipelines 186–187\ndeduplicating data in real-time systems\ncauses of 178–181\nsolutions to 181–186\nreal-time data quality checks 187–188\ndata type conversion 100–102\nData Warehouse Units (DWU) 269\ndata warehouses\nchange to data platforms 2–3\nschema changes in architecture 230\nschema evolution and 255–258\nstruggling with data platforms 3–6\nall V's at once 6\nvariety 4\nvelocity 5\nvolume 5\ndata-transformation pipelines, schema evolution \nscenarios and 251–255\ndeduplicating data 145–150\nin real-time systems\ncauses of 178–181\nsolutions to 181–186\nin Spark 147–150\ndependency graph 55\nDescription attribute 205–206\nDestination object 207\nDestinations attribute 206\ndirect data platform access\nAWS 65\nAzure 72–73\nGoogle Cloud 69\nDISTKEY property 265\nDISTSTYLE property 265\ndomains, metadata 204–208, 213\ndropDuplicates function 148\nDSL (domain-specific language) 210\nduration of ingestion 105, 113\nDWU (Data Warehouse Units) 269\nE\nelastic cost model property 50\nelastic resources 8\nEMR (Elastic MapReduce) 64, 129, 283\nERP (enterprise resource planning) 53, 188\nError Message attribute 212\nETL (extract, transform, load) 4, 22\nETL overlay 56–58\nAWS 65\nAzure 73\nGoogle Cloud 69–70\nEVEN value 265\nEvent Hubs 193\nExcel 284\nextendable property 49\nextensibility property 58\nexternal cloud provider tools 284–285\nExtra attribute 212\nextract, transform, load (ETL) 4, 22\nF\nfailed area 134\nfailed container 138\nfailed storage area 134\n",
      "content_length": 2593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "INDEX\n309\nfast storage\nanatomy of 167–170\nlayer 44–46\nscalability 170–172\nfile format conversion 140–145\nAvro and Parquet file formats 140–143\nusing Spark 143–145\nFile Transfer Protocol (FTP) 107\nfiles 81–82, 107–113\ncapturing metadata 112–113\ntracking data ingestion 109–112\nfolders 134–139\nnaming conventions 135–138\norganizing streaming data 139\nfriends field 238\nFTP (File Transfer Protocol) 107\nfull table ingestion 85–91\nfull text search services 277–278\nG\ngame events 162\nGoogle BigQuery 270–273\nGoogle Cloud\nmapping layers to 66–70\nbatch data ingestion 67\nbatch data processing 68\ncloud warehouse 69\ndata consumers 70\ndata platform storage 68\ndirect data platform access 69\nETL overlay and metadata repository 69–70\norchestration layer 70\nreal-time data processing and analytics 68–69\nstreaming data ingestion 67–68\nreal-time processing services 192–193\nGoogle Cloud Data Catalog 222\ngroups, data security and 285–286\nH\nHASH option 269\nHDFS (Hadoop Distributed File System) 11, 231\nhigh availability property 43, 56\nhighly available property 48\nI\nIaaS (infrastructure as a service) 62\nID attribute 205–207, 210, 247\nin-memory cache 278\nincremental table ingestion 91–94\ninfrastructure as a service (IaaS) 62\ningesting data 84, 91–126\ndata sources 79–83\nfiles 81–82\nrelational databases (RDBMS) 80\nSaaS data via API 82\nstreams 82–83\nfrom data platforms 24–28\ndirectly into Azure Synapse 25–26\ninto Azure data platform 26\nmanaging changes in upstream data \nsources 26–28\nfrom files 107–113\ncapturing metadata 112–113\ntracking 109–112\nfrom relational databases (RDBMS) 83–107\ncapturing metadata for ingestion \npipelines 104–107\nchange data capture (CDC) 94–100\ndata type conversion 100–102\nfrom NoSQL databases 103–104\nfull table ingestion 86–91\nincremental table ingestion 94\nusing SQL interface 85\nfrom SaaS applications 120–124\nfrom streams 114, 119–120\ncapturing metadata 120\ndifferences between batch and streaming \ningestion 117–119\nnetwork and security considerations for 123–124\ningestion layer 10–11, 39\nintegrations property 58\nK\nKEY value 265\nKinesis Data Analytics 190\nKinesis Data Streams 190\nL\nlanding area 133\nlanding container 136\nLast Modified Timestamp attribute\n205, 207–208, 210\nLast Updated Timestamp attribute 248\nLAST_MODIFIED timestamp 92\nlayered architecture 38–40, 58\ndata ingestion layer 43\nETL tools overlay 56–58\nfast and slow storage layer 44–46\nimportance of 59–60\nmapping to tools 60–74\nAWS 62–66\nAzure 70–74\nGoogle Cloud 66–70\n",
      "content_length": 2465,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "INDEX\n310\nlayered architecture (continued)\nmetadata implementation options 213–220\nconfiguration files 214–217\nmetadata API 218–220\nmetadata database 217–218\norchestration layer 53–56\nprocessing data as separate layer 129–131\nprocessing layer 46–47\nserving layer and data consumers 49–52\ntechnical metadata layer 47–49\nM\nmaintainable property 56\nManaged Streaming for Apache Kafka (MSK) 63\nmapping layers 68\nto AWS 62–66\nbatch data ingestion 62–63\nbatch data processing 64\ncloud warehouse 64\ndata consumers 65–66\ndata platform storage 63–64\ndirect data platform access 65\nETL overlay and metadata repository 65\norchestration layer 65\nreal-time data processing and analytics 64\nstreaming data ingestion 63\nto Azure 70–74\nbatch data ingestion 71\nbatch data processing 72\ncloud data warehouse 72\ndata consumers 74\ndata platform storage 72\ndirect data platform access 72–73\nETL overlay and metadata repository 73\norchestration layer 73\nreal-time data processing and analytics 72\nstreaming data ingestion 71\nto Google Cloud 66–70\nbatch data ingestion 67\nbatch data processing 68\ncloud warehouse 69\ndata consumers 70\ndata platform storage 68\ndirect data platform access 69\nETL overlay and metadata repository 69–70\norchestration layer 70\nreal-time data processing and analytics 69\nstreaming data ingestion 67–68\nmassively parallel processing (MPP) 64\nMDM (master data management) tools 145\nmessage formats, converting in real-time \npipelines 186–187\nmetadata 227\ncapturing from file ingestion 112–113\ncapturing from streaming pipeline 119–120\nexisting solutions 220–225\nCloud metadata services 221–222\nopen source metadata layer \nimplementations 223–225\nlayer implementation options 213–220\nconfiguration files 214–217\nmetadata API 218–220\nmetadata database 217–218\nmetadata domains 204–213\ndata quality checks 208–210\npipeline activity metadata 210–213\npipeline metadata 204–207\nmetadata model 213\ntypes of 198–199\nbusiness metadata 198–199\ndata platform internal metadata (pipeline \nmetadata) 199–203\nmetadata API 218–220\nmetadata database 217–218\nmetadata layer 39, 246–248\nmetadata repository\nAWS 65\nAzure 73\nGoogle Cloud 69–70\nMicrosoft (MS) SQL SERVER 99–100\nML (machine learning) 278–283\ncloud collaboration tools 282–283\nmodel lifecycle on cloud data platform 279–282\nmodel, metadata 203–213\nMongoDB database 103\nMPP (massively parallel processing) 64\nMS (Microsoft) SQL SERVER 99–100\nMSK (Managed Streaming for Apache Kafka) 63\nMySQL 99\nN\nName attribute 205–207, 210\nNamespaces 136, 204\nnaming conventions 135–138\nnetworks 123–124\nboundries and security 287\nconnecting to cloud data platform 123–124\nNoOps property 50\nNoSQL databases\ncapturing metadata for ingestion pipelines\n104–107\nRDBMS ingesting data from 103–104\nO\nobservability property 43\nonline gaming use case 161–164\n",
      "content_length": 2781,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "INDEX\n311\nopen source solutions 74–75\nmetadata layer implementations 223–225\nstreaming data ingestion and real time \nanalytics 75\nOracle 98–99\norchestration layer 53–56\nAWS 65\nAzure 73\nGoogle Cloud 70\nopen source and commercial alternatives 75\norganizing data\ncloud storage 132–139\nfor real-time use 167–177\nanatomy of fast storage 167–170\nfast storage scalability 170–172\nreal-time storage 172–177\noverfitting 281\noverlay layer 39\nP\nPaaS (platform as a service) 8, 14, 22, 60\nParquet file format 140–143\npass-through jobs 134\nperformant property 46\nPipeline Activity domain 204\npipeline activity metadata 210–213\npipeline configuration metadata 210\nPipeline ID attribute 212\npipeline metadata (data platform internal \nmetadata)\nas metadata domain 204–207\ndefined 199\ntaking advantage of 199–203\nPipeline metadata object 205\npipeline name 136\npipelines\nconfigurable for processing data 152–154\nconverting message formats in real-time\n186–187\nplatform as a service (PaaS) 8, 14, 22, 60\nplatform project stoppers 297–302\ncost 301–302\noperating in platform silos 300\ntime 297–298\nuser adoption 298–299\nuser trust and need for data governance\n299–300\nplatform silos 300\npluggable architecture property 43\nPostgreSQL 100\nprinciple of least privilege 286\nprocessing data 28–143, 155\nas separate layer in data platform 129–131\nconfigurable pipelines 152–154\ndata deduplication 145–150\ndata quality checks 150–152\nfile format conversion 140–145\nAvro and Parquet file formats 140–143\nusing Spark to convert file formats 145\nin data platform 31–32\nin warehouse 29–31\nstages 131–132\nprocessing layer 12–13, 46–47\nproducers 168\nproduction area 134\nproduction container 138\nproducts 295–296\nProtobuf (Protocol Buffers) 81\npublic cloud 7–9\nR\nRDBMS (relational databases) 80, 83, 100\ncapturing metadata for ingestion pipelines\n104–107\nchange data capture (CDC) 94–100\ndata type conversion 102\nfrom NoSQL databases 103–104\nfull table ingestion 86–91\nincremental table ingestion 91–94\nusing SQL interface 84–85\nRDDs (Resilient Distributed Datasets) 28\nread function 240\nreal-time data processing and analytics 196\nAWS 64\nAzure 72\ncloud services for 190–194\nAWS 190–192\nAzure 193–194\nGoogle Cloud 192–193\ncommon data transformations in real time\n178–190\ncombining batch and real-time data 188–190\nGoogle Cloud 68–69\norganizing data for real-time use\nanatomy of fast storage 167–170\nfast storage scalability 170–172\nreal-time storage 172–177\nreal-time ingestion vs. 157–164\nknowing when to use 164–167\nonline gaming use case 161–164\nprocessing summary 164\nretail use case 160–161\nwith open source solutions 75\nreal-time ingestion 114, 157–164\nknowing when to use 164–167\nonline gaming use case 161–164\nprocessing summary 164\nretail use case 160–161\n",
      "content_length": 2730,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "INDEX\n312\nreal-time pipelines 241\nreal-time storage 172–177\nrecord type 244\nrelational databases see RDBMS (relational \ndatabases)\nreliable property 45, 50\nREPLICATE option 269\nretail use case 160–161\nroles, data security and 285–286\nROUND ROBIN option 269\nrows ingested per table 105\nRows Read attribute 212\nRows Written attribute 212\nRule attribute 210\nS\nSaaS applications 2, 60\ningesting data from 120–124\ningesting data via API 82\nsampleSize option 240\nscalable property 43, 45, 48, 50\nscale property 56\nSchema attribute 248\nschema evolution\ndata warehouses and 255–258\nscenarios 248–255\ncompatibility rules 249–251\ndata-transformation pipelines and 251–255\nSchema ID attribute 207\nschema inference 237\nschema management 260\napproaches to 232–243\nin data platforms 235–241\nmonitoring schema changes 241–243\nschema as contract 233–235\nreasons for 229–232\nschema changes in traditional data warehouse \narchitecture 230\nschema-on-read approach 231–232\nschema evolution 248–255\ncompatibility rules 249–251\ndata warehouses and 255–258\ndata-transformation pipelines and 251–255\nSchema Registry implementation 243–248\nApache Avro schemas 243–245\nas part of metadata layer 246–248\nexisting implementations 245–246\nschema management module 236–237\nSchema Registry implementation 243–248\nApache Avro schemas 243–245\nas part of metadata layer 246–248\nexisting implementations 245–246\nschema-on-read approach 231–232\nsecurity 123–124, 285–287\nconnecting networks to cloud data \nplatform 123–124\ncredentials and configuration \nmanagement 286\ndata encryption 286\nnetwork boundaries 287\nusers, groups, and roles 285–286\nService Level Agreement (SLA) monitoring 105\nserving layer 39\ndata consumers and 49–52\nof cloud data platforms 13–14\nSeverity attribute 210\nSFTP (standard File Transfer Protocol) 107\nsilos, platform 300\nSLA (service level agreement) monitoring 105\nslow storage layer 44–46\nSORT KEY 266\nSource 207\nsource system name 113\nSources attribute 206\nSpark\ndata deduplication 147–150\nfile format conversion 143–145\nSQL interface 84–85\nstages, processing data 131–132\nstaging area 133\nstaging container 137\nstandard File Transfer Protocol (SFTP) 107\nStart Time attribute 212\nStatus attribute 212\nStop Time attribute 212\nstorage layer 11–12, 39\nstore_id 189\nstreaming data ingestion 82–83, 114–120\nAWS 63\nAzure 71\ncapturing metadata 119–120\ndifferences between batch and 117–119\nGoogle Cloud 67–68\norganizing data 139\nwith open source solutions 75\nstructured logging 216\nT\ntags field 238\ntechnical metadata layer 47–49\ntime 297–298\ntoAvroType method 244\ntopics, messages organized as 168\ntracking ingested files 109–112\ntransaction_amount 230\n",
      "content_length": 2641,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "INDEX\n313\ntransaction_date 230\ntransaction_total 230\ntransparency property 56\nType attribute 206–208\nU\nUDFs (user defined functions) 31\nULID (Universally Unique Lexicographically \nSortable Identifier) 136\nupstream data sources 26–28\nurl attribute 30\nusers 286\nadoption of users 298–299\ndata security 285\ntrust 299–300\nUUID (Universally Unique Identifier) 136\nV\nvariety\ncloud data platforms 14–15\ndata warehouses 4\nvelocity\ncloud data platforms 15–16\ndata warehouses 5\nVelocity attribute 206\nVersion attribute 248\nvolatility 81\nvolume\ncloud data platforms 15\ndata warehouses 5\nW\nwatermark 92\n",
      "content_length": 591,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "  \nData strategy\nSEE\nPREDICT\nDO\nCREATE\nInsights\n(BI and\nanalytics)\nPredictions\n(ML/AI)\nAutomation\n(Orchestration)\nProducts\n(App dev)\nIntegrated, organized, clean data\n(Enterprise data platform)\nA cloud data\nplatform is the\nfoundation for all\nstages of analytics\nmaturity.\n",
      "content_length": 272,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "Zburivsky ● Partner\nISBN: 978-1-61729-644-4\nW\nell-designed pipelines, storage systems, and APIs \neliminate the complicated scaling and maintenance \nrequired with on-prem data centers. Once you learn \nthe patterns for designing cloud data platforms, you’ll maxi-\nmize performance no matter which cloud vendor you use.\nIn Designing Cloud Data Platforms, Danil Zburivsky and \nLynda Partner reveal a six-layer approach that increases \nﬂ exibility and reduces costs. Discover patterns for ingesting \ndata from a variety of sources, then learn to harness \npre-built services provided by cloud vendors. \nWhat’s Inside\n● Best practices for structured and unstructured data sets\n● Cloud-ready machine learning tools \n● Metadata and real-time analytics\n● Defensive architecture, access, and security\nFor data professionals familiar with the basics of cloud com-\nputing, and Hadoop or Spark.\nDanil Zburivsky has over 10 years of experience designing \nand supporting large-scale data infrastructure for enterprises \nacross the globe. Lynda Partner is the VP of Analytics-as-a-\nService at Pythian, and has been on the business side of data \nfor over 20 years.\nRegister this print book to get free access to all ebook formats. \nVisit https://www.manning.com/freebook\n$59.99 / Can $79.99  [INCLUDING eBOOK]\nDesigning Cloud Data Platforms\nCLOUD/DATA ENGINEERING\nM A N N I N G\n“\nA great guide to building \ndata platforms from the \nground up!”\n \n—Mike Jensen, Arcadia\n“\nA comprehensive overview \nof cloud data platforms and \n  a valuable resource.”\n \n—Ubaldo Pescatore\nGenerali Business Solutions\n“\nA clear, concise, and useful \nguide…provides a great \nintroduction to architectures \nand tools across the entire \nspectrum of applications \n  and platforms.”\n—Ken Fricklas, Google \n“\nA practical and realistic \nview of the architecture, \nchallenges, and patterns of \n a cloud data platform.”\n \n—Hugo Cruz\nPeople Driven Technology\nSee first page\n",
      "content_length": 1925,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}