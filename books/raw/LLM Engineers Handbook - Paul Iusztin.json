{
  "metadata": {
    "title": "LLM Engineers Handbook - Paul Iusztin",
    "author": "Paul Iusztin",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 1630,
    "conversion_date": "2025-12-19T17:33:33.142905",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "LLM Engineers Handbook - Paul Iusztin.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "on fine-tuning and Chapters 4 and 9 on RAG, but for now, let’s look at a few examples to intuitively understand",
      "start_page": 44,
      "end_page": 106,
      "detection_method": "regex_chapter",
      "content": "valuable? Secondly, on the moral side, is it OK to do this in the first place? Do we want to create a copycat of ourselves? Will it write using our voice and personality, or just try to replicate it?\n\nRemember that the role of this section is not to bother with the “What” and “How” but with the “Why.” Let’s understand why it makes sense to have your LLM Twin, why it can be valuable, and why it is morally correct if we frame the problem correctly.\n\nOceanofPDF.com\n\nWhy building an LLM Twin matters\n\nAs an engineer (or any other professional career), building a personal brand is more valuable than a standard CV. The biggest issue with creating a personal brand is that writing content on platforms such as LinkedIn, X, or Medium takes a lot of time. Even if you enjoy writing and creating content, you will eventually run out of inspiration or time and feel like you need assistance. We don’t want to transform this section into a pitch, but we have to understand the scope of this product/project clearly.\n\nWe want to build an LLM Twin to write personalized content on LinkedIn, X, Instagram, Substack, and Medium (or other blogs) using our style and voice. It will not be used in any immoral scenarios, but it will act as your writing co-pilot. Based on what we will teach you in this book, you can get creative and adapt it to various use cases, but we will focus on the niche of generating social media content and articles. Thus, instead of writing the content from scratch, we can feed the skeleton of our main idea to the LLM Twin and let it do the grunt work.\n\nUltimately, we will have to check whether everything is correct and format it to our liking (more on the concrete features in the Planning the MVP of the LLM Twin product section). Hence, we project ourselves into a content- writing LLM Twin that will help us automate our writing process. It will likely fail if we try to use this particular LLM in a different scenario, as this is where we will specialize the LLM through fine-tuning, prompt engineering, and RAG.\n\nSo, why does building an LLM Twin matter? It helps you do the following:\n\nCreate your brand\n\nAutomate the writing process\n\nBrainstorm new creative ideas\n\nWhat’s the difference between a co-pilot and an LLM Twin?\n\nA co-pilot and digital twin are two different concepts that work together and can be combined into a powerful solution:\n\nThe co-pilot is an AI assistant or tool that augments human users in various programming, writing, or content creation tasks.\n\nThe twin serves as a 1:1 digital representation of a real-world entity, often using AI to bridge the gap between the physical and digital worlds. For instance, an LLM Twin is an LLM that learns to mimic your voice, personality, and writing style.\n\nWith these definitions in mind, a writing and content creation AI assistant who writes like you is your LLM Twin co-pilot.\n\nAlso, it is critical to understand that building an LLM Twin is entirely moral. The LLM will be fine-tuned only on our personal digital data. We won’t collect and use other people’s data to try to impersonate anyone’s identity. We have a clear goal in mind: creating our personalized writing copycat. Everyone will have their own LLM Twin with restricted access.\n\nOf course, many security concerns are involved, but we won’t go into that here as it could be a book in itself.\n\nOceanofPDF.com\n\nWhy not use ChatGPT (or another similar chatbot)?\n\nThis subsection will refer to using ChatGPT (or another similar chatbot) just in the context of generating personalized content.\n\nWe have already provided the answer. ChatGPT is not personalized to your writing style and voice. Instead, it is very generic, unarticulated, and wordy. Maintaining an original voice is critical for long-term success when building your brand. Thus, directly using ChatGPT or Gemini will not yield the most optimal results. Even if you are OK with sharing impersonalized content, mindlessly using ChatGPT can result in the following:\n\nMisinformation due to hallucination: Manually checking the results for hallucinations or using third-party tools to evaluate your results is a tedious and unproductive experience.\n\nTedious manual prompting: You must manually craft your prompts and inject external information, which is a tiresome experience. Also, the generated answers will be hard to replicate between multiple sessions as you don’t have complete control over your prompts and injected data. You can solve part of this problem using an API and a tool such as LangChain, but you need programming experience to do so.\n\nFrom our experience, if you want high-quality content that provides real value, you will spend more time debugging the generated text than writing it yourself.\n\nThe key of the LLM Twin stands in the following:\n\nWhat data we collect\n\nHow we preprocess the data\n\nHow we feed the data into the LLM\n\nHow we chain multiple prompts for the desired results\n\nHow we evaluate the generated content\n\nThe LLM itself is important, but we want to highlight that using ChatGPT’s web interface is exceptionally tedious in managing and injecting various data sources or evaluating the outputs. The solution is to build an LLM system that encapsulates and automates all the following steps (manually replicating them each time is not a long-term and feasible solution):\n\nData collection\n\nData preprocessing\n\nData storage, versioning, and retrieval\n\nLLM fine-tuning\n\nRAG\n\nContent generation evaluation\n\nNote that we never said not to use OpenAI’s GPT API, just that the LLM framework we will present is LLM-agnostic. Thus, if it can be manipulated programmatically and exposes a fine-tuning interface, it can be integrated into the LLM Twin system we will learn to build. The key to most successful ML products is to be data-centric and make your architecture model-agnostic. Thus, you can quickly experiment with multiple models on your specific data.\n\nOceanofPDF.com\n\nPlanning the MVP of the LLM Twin product\n\nNow that we understand what an LLM Twin is and why we want to build it, we must clearly define the product’s features. In this book, we will focus on the first iteration, often labeled the minimum viable product (MVP), to follow the natural cycle of most products. Here, the main objective is to align our ideas with realistic and doable business objectives using the available resources to produce the product. Even as an engineer, as you grow up in responsibilities, you must go through these steps to bridge the gap between the business needs and what can be implemented.\n\nOceanofPDF.com\n\nWhat is an MVP?\n\nAn MVP is a version of a product that includes just enough features to draw in early users and test the viability of the product concept in the initial stages of development. Usually, the purpose of the MVP is to gather insights from the market with minimal effort.\n\nAn MVP is a powerful strategy because of the following reasons:\n\nAccelerated time-to-market: Launch a product quickly to gain early traction\n\nIdea validation: Test it with real users before investing in the full development of the product\n\nMarket research: Gain insights into what resonates with the target audience\n\nRisk minimization: Reduces the time and resources needed for a product that might not achieve market success\n\nSticking to the V in MVP is essential, meaning the product must be viable. The product must provide an end-to-end user journey without half- implemented features, even if the product is minimal. It must be a working product with a good user experience that people will love and want to keep using to see how it evolves to its full potential.\n\nOceanofPDF.com\n\nDefining the LLM Twin MVP\n\nAs a thought experiment, let’s assume that instead of building this project for this book, we want to make a real product. In that case, what are our resources? Well, unfortunately, not many:\n\nWe are a team of three people with two ML engineers and one ML researcher\n\nOur laptops\n\nPersonal funding for computing, such as training LLMs\n\nOur enthusiasm\n\nAs you can see, we don’t have many resources. Even if this is just a thought experiment, it reflects the reality for most start-ups at the beginning of their journey. Thus, we must be very strategic in defining our LLM Twin MVP and what features we want to pick. Our goal is simple: we want to maximize the product’s value relative to the effort and resources poured into it.\n\nTo keep it simple, we will build the features that can do the following for the LLM Twin:\n\nCollect data from your LinkedIn, Medium, Substack, and GitHub profiles\n\nFine-tune an open-source LLM using the collected data\n\nPopulate a vector database (DB) using our digital data for RAG\n\nCreate LinkedIn posts leveraging the following:\n\nUser prompts\n\nRAG to reuse and reference old content\n\nNew posts, articles, or papers as additional knowledge to the LLM\n\nHave a simple web interface to interact with the LLM Twin and be able to do the following:\n\nConfigure your social media links and trigger the collection step\n\nSend prompts or links to external resources\n\nThat will be the LLM Twin MVP. Even if it doesn’t sound like much, remember that we must make this system cost effective, scalable, and modular.\n\nEven if we focus only on the core features of the LLM Twin defined in this section, we will build the product with the latest LLM research and best software engineering and MLOps practices in mind. We aim to show you how to engineer a cost-effective and scalable LLM application.\n\nUntil now, we have examined the LLM Twin from the users’ and businesses’ perspectives. The last step is to examine it from an engineering perspective and define a development plan to understand how to solve it technically. From now on, the book’s focus will be on the implementation of the LLM Twin.\n\nOceanofPDF.com\n\nBuilding ML systems with feature/training/inference pipelines\n\nBefore diving into the specifics of the LLM Twin architecture, we must understand an ML system pattern at the core of the architecture, known as the feature/training/inference (FTI) architecture. This section will present a general overview of the FTI pipeline design and how it can structure an ML application.\n\nLet’s see how we can apply the FTI pipelines to the LLM Twin architecture.\n\nOceanofPDF.com\n\nThe problem with building ML systems\n\nBuilding production-ready ML systems is much more than just training a model. From an engineering point of view, training the model is the most straightforward step in most use cases. However, training a model becomes complex when deciding on the correct architecture and hyperparameters. That’s not an engineering problem but a research problem.\n\nAt this point, we want to focus on how to design a production-ready architecture. Training a model with high accuracy is extremely valuable, but just by training it on a static dataset, you are far from deploying it robustly. We have to consider how to do the following:\n\nIngest, clean, and validate fresh data\n\nTraining versus inference setups\n\nCompute and serve features in the right environment\n\nServe the model in a cost-effective way\n\nVersion, track, and share the datasets and models\n\nMonitor your infrastructure and models\n\nDeploy the model on a scalable infrastructure\n\nAutomate the deployments and training\n\nThese are the types of problems an ML or MLOps engineer must consider, while the research or data science team is often responsible for training the model.\n\nFigure 1.1: Common elements from an ML system\n\nThe preceding figure shows all the components the Google Cloud team suggests that a mature ML and MLOps system requires. Along with the ML code, there are many moving pieces. The rest of the system comprises configuration, automation, data collection, data verification, testing and debugging, resource management, model analysis, process and metadata management, serving infrastructure, and monitoring. The point is that there are many components we must consider when productionizing an ML model.\n\nThus, the critical question is this: How do we connect all these components into a single homogenous system? We must create a boilerplate for clearly designing ML systems to answer that question.\n\nSimilar solutions exist for classic software. For example, if you zoom out, most software applications can be split between a DB, business logic, and UI layer. Every layer can be as complex as needed, but at a high-level overview, the architecture of standard software can be boiled down to the previous three components.\n\nDo we have something similar for ML applications? The first step is to examine previous solutions and why they are unsuitable for building scalable ML systems.\n\nOceanofPDF.com\n\nThe issue with previous solutions\n\nIn Figure 1.2, you can observe the typical architecture present in most ML applications. It is based on a monolithic batch architecture that couples the feature creation, model training, and inference into the same component. By taking this approach, you quickly solve one critical problem in the ML world: the training-serving skew. The training-serving skew happens when the features passed to the model are computed differently at training and inference time.\n\nIn this architecture, the features are created using the same code. Hence, the training-serving skew issue is solved by default. This pattern works fine when working with small data. The pipeline runs on a schedule in batch mode, and the predictions are consumed by a third-party application such as a dashboard.\n\nFigure 1.2: Monolithic batch pipeline architecture\n\nUnfortunately, building a monolithic batch system raises many other issues, such as the following:\n\nFeatures are not reusable (by your system or others)\n\nIf the data increases, you have to refactor the whole code to support PySpark or Ray\n\nIt’s hard to rewrite the prediction module in a more efficient language such as C++, Java, or Rust\n\nIt’s hard to share the work between multiple teams between the features, training, and prediction modules\n\nIt’s impossible to switch to streaming technology for real-time training\n\nIn Figure 1.3, we can see a similar scenario for a real-time system. This use case introduces another issue in addition to what we listed before. To make the predictions, we have to transfer the whole state through the client request so the features can be computed and passed to the model.\n\nConsider the scenario of computing movie recommendations for a user. Instead of simply passing the user ID, we must transmit the entire user state, including their name, age, gender, movie history, and more. This approach is fraught with potential errors, as the client must understand how to access this state, and it’s tightly coupled with the model service.\n\nAnother example would be when implementing an LLM with RAG support. The documents we add as context along the query represent our external state. If we didn’t store the records in a vector DB, we would have to pass them with the user query. To do so, the client must know how to query and\n\nretrieve the documents, which is not feasible. It is an antipattern for the client application to know how to access or compute the features. If you don’t understand how RAG works, we will explain it in detail in Chapters 8 and 9.\n\nFigure 1.3: Stateless real-time architecture\n\nIn conclusion, our problem is accessing the features to make predictions without passing them at the client’s request. For example, based on our first user movie recommendation example, how can we predict the\n\nrecommendations solely based on the user’s ID? Remember these questions, as we will answer them shortly.\n\nUltimately, on the other spectrum, Google Cloud provides a production- ready architecture, as shown in Figure 1.4. Unfortunately, even if it’s a feasible solution, it’s very complex and not intuitive. You will have difficulty understanding this if you are not highly experienced in deploying and keeping ML models in production. Also, it is not straightforward to understand how to start small and grow the system in time.\n\nThe following image is reproduced from work created and shared by Google and used according to terms described in the Creative Commons 4.0 Attribution License:\n\nFigure 1.4: ML pipeline automation for CT (source: https://cloud.google.com/architecture/mlops-continuous-delivery-and- automation-pipelines-in-machine-learning)\n\nBut here is where the FTI pipeline architectures kick in. The following section will show you how to solve these fundamental issues using an intuitive ML design.\n\nOceanofPDF.com\n\nThe solution – ML pipelines for ML systems\n\nThe solution is based on creating a clear and straightforward mind map that any team or person can follow to compute the features, train the model, and make predictions. Based on these three critical steps that any ML system requires, the pattern is known as the FTI pipeline. So, how does this differ from what we presented before?\n\nThe pattern suggests that any ML system can be boiled down to these three pipelines: feature, training, and inference (similar to the DB, business logic, and UI layers from classic software). This is powerful, as we can clearly define the scope and interface of each pipeline. Also, it’s easier to understand how the three components interact. Ultimately, we have just three instead of 20 moving pieces, as suggested in Figure 1.4, which is much easier to work with and define.\n\nAs shown in Figure 1.5, we have the feature, training, and inference pipelines. We will zoom in on each of them and understand their scope and interface.\n\nFigure 1.5: FTI pipelines architecture\n\nBefore going into the details, it is essential to understand that each pipeline is a different component that can run on a different process or hardware. Thus, each pipeline can be written using a different technology, by a different team, or scaled differently. The key idea is that the design is very flexible to the needs of your team. It acts as a mind map for structuring your architecture.\n\nOceanofPDF.com\n\nThe feature pipeline\n\nThe feature pipeline takes raw data as input, processes it, and outputs the features and labels required by the model for training or inference. Instead of directly passing them to the model, the features and labels are stored inside a feature store. Its responsibility is to store, version, track, and share the features. By saving the features in a feature store, we always have a state of our features. Thus, we can easily send the features to the training and inference pipelines.\n\nAs the data is versioned, we can always ensure that the training and inference time features match. Thus, we avoid the training-serving skew problem.\n\nOceanofPDF.com\n\nThe training pipeline\n\nThe training pipeline takes the features and labels from the features stored as input and outputs a train model or models. The models are stored in a model registry. Its role is similar to that of feature stores, but this time, the model is the first-class citizen. Thus, the model registry will store, version, track, and share the model with the inference pipeline.\n\nAlso, most modern model registries support a metadata store that allows you to specify essential aspects of how the model was trained. The most important are the features, labels, and their version used to train the model. Thus, we will always know what data the model was trained on.\n\nOceanofPDF.com\n\nThe inference pipeline\n\nThe inference pipeline takes as input the features and labels from the feature store and the trained model from the model registry. With these two, predictions can be easily made in either batch or real-time mode.\n\nAs this is a versatile pattern, it is up to you to decide what you do with your predictions. If it’s a batch system, they will probably be stored in a DB. If it’s a real-time system, the predictions will be served to the client who requested them. Additionally, the features, labels, and models are versioned. We can easily upgrade or roll back the deployment of the model. For example, we will always know that model v1 uses features F1, F2, and F3, and model v2 uses F2, F3, and F4. Thus, we can quickly change the connections between the model and features.\n\nOceanofPDF.com\n\nBenefits of the FTI architecture\n\nTo conclude, the most important thing you must remember about the FTI pipelines is their interface:\n\nThe feature pipeline takes in data and outputs the features and labels saved to the feature store.\n\nThe training pipeline queries the features store for features and labels and outputs a model to the model registry.\n\nThe inference pipeline uses the features from the feature store and the model from the model registry to make predictions.\n\nIt doesn’t matter how complex your ML system gets, these interfaces will remain the same.\n\nNow that we understand better how the pattern works, we want to highlight the main benefits of using this pattern:\n\nAs you have just three components, it is intuitive to use and easy to understand.\n\nEach component can be written into its tech stack, so we can quickly adapt them to specific needs, such as big or streaming data. Also, it allows us to pick the best tools for the job.\n\nAs there is a transparent interface between the three components, each one can be developed by a different team (if necessary), making the development more manageable and scalable.\n\nEvery component can be deployed, scaled, and monitored independently.\n\nThe final thing you must understand about the FTI pattern is that the system doesn’t have to contain only three pipelines. In most cases, it will include more. For example, the feature pipeline can be composed of a service that computes the features and one that validates the data. Also, the training pipeline can be composed of the training and evaluation components.\n\nThe FTI pipelines act as logical layers. Thus, it is perfectly fine for each to be complex and contain multiple services. However, what is essential is to stick to the same interface on how the FTI pipelines interact with each other through the feature store and model registries. By doing so, each FTI component can evolve differently, without knowing the details of each other and without breaking the system on new changes.\n\nTo learn more about the FTI pipeline pattern, consider reading From MLOps to ML Systems with Feature/Training/Inference Pipelines by Jim Dowling, CEO and co-founder of Hopsworks: https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines. His article inspired this section.\n\nNow that we understand the FTI pipeline architecture, the final step of this chapter is to see how it can be applied to the LLM Twin use case.\n\nOceanofPDF.com\n\nDesigning the system architecture of the LLM Twin\n\nIn this section, we will list the concrete technical details of the LLM Twin application and understand how we can solve them by designing our LLM system using the FTI architecture. However, before diving into the pipelines, we want to highlight that we won’t focus on the tooling or the tech stack at this step. We only want to define a high-level architecture of the system, which is language-, framework-, platform-, and infrastructure- agnostic at this point. We will focus on each component’s scope, interface, and interconnectivity. In future chapters, we will cover the implementation details and tech stack.\n\nOceanofPDF.com\n\nListing the technical details of the LLM Twin architecture\n\nUntil now, we defined what the LLM Twin should support from the user’s point of view. Now, let’s clarify the requirements of the ML system from a purely technical perspective:\n\nOn the data side, we have to do the following:\n\nCollect data from LinkedIn, Medium, Substack, and GitHub completely autonomously and on a schedule\n\nStandardize the crawled data and store it in a data warehouse\n\nClean the raw data\n\nCreate instruct datasets for fine-tuning an LLM\n\nChunk and embed the cleaned data. Store the vectorized data into a vector DB for RAG.\n\nFor training, we have to do the following:\n\nFine-tune LLMs of various sizes (7B, 14B, 30B, or 70B parameters)\n\nFine-tune on instruction datasets of multiple sizes\n\nSwitch between LLM types (for example, between Mistral, Llama, and GPT)\n\nTrack and compare experiments\n\nTest potential production LLM candidates before deploying them\n\nAutomatically start the training when new instruction datasets are available.\n\nThe inference code will have the following properties:\n\nA REST API interface for clients to interact with the LLM Twin\n\nAccess to the vector DB in real time for RAG\n\nInference with LLMs of various sizes\n\nAutoscaling based on user requests\n\nAutomatically deploy the LLMs that pass the evaluation step.\n\nThe system will support the following LLMOps features:\n\nInstruction dataset versioning, lineage, and reusability\n\nModel versioning, lineage, and reusability\n\nExperiment tracking\n\nContinuous training, continuous integration, and continuous delivery (CT/CI/CD)\n\nPrompt and system monitoring\n\nIf any technical requirement doesn’t make sense now, bear with us. To avoid repetition, we will examine the details in their specific chapter.\n\nThe preceding list is quite comprehensive. We could have detailed it even more, but at this point, we want to focus on the core functionality. When implementing each component, we will look into all the little details. But for now, the fundamental question we must ask ourselves is this: How can we apply the FTI pipeline design to implement the preceding list of requirements?\n\nOceanofPDF.com\n\nHow to design the LLM Twin architecture using the FTI pipeline design\n\nWe will split the system into four core components. You will ask yourself this: “Four? Why not three, as the FTI pipeline design clearly states?” That is a great question. Fortunately, the answer is simple. We must also implement the data pipeline along the three feature/training/inference pipelines. According to best practices:\n\nThe data engineering team owns the data pipeline\n\nThe ML engineering team owns the FTI pipelines.\n\nGiven our goal of building an MVP with a small team, we must implement the entire application. This includes defining the data collection and FTI pipelines. Tackling a problem end to end is often encountered in start-ups that can’t afford dedicated teams. Thus, engineers have to wear many hats, depending on the state of the product. Nevertheless, in any scenario, knowing how an end-to-end ML system works is valuable for better understanding other people’s work.\n\nFigure 1.6 shows the LLM system architecture. The best way to understand it is to review the four components individually and explain how they work.\n\nFigure 1.6: LLM Twin high-level architecture\n\nOceanofPDF.com\n\nData collection pipeline\n\nThe data collection pipeline involves crawling your personal data from Medium, Substack, LinkedIn, and GitHub. As a data pipeline, we will use the extract, load, transform (ETL) pattern to extract data from social media platforms, standardize it, and load it into a data warehouse.\n\nIt is critical to highlight that the data collection pipeline is designed to crawl data only from your social media platform. It will not have access to other people. As an example for this book, we agreed to make our collected data available for learning purposes. Otherwise, using other people’s data without their consent is not moral.\n\nThe output of this component will be a NoSQL DB, which will act as our data warehouse. As we work with text data, which is naturally unstructured, a NoSQL DB fits like a glove.\n\nEven though a NoSQL DB, such as MongoDB, is not labeled as a data warehouse, from our point of view, it will act as one. Why? Because it stores standardized raw data gathered by various ETL pipelines that are ready to be ingested into an ML system.\n\nThe collected digital data is binned into three categories:\n\nArticles (Medium, Substack)\n\nPosts (LinkedIn)\n\nCode (GitHub)\n\nWe want to abstract away the platform where the data was crawled. For example, when feeding an article to the LLM, knowing it came from Medium or Substack is not essential. We can keep the source URL as metadata to give references. However, from the processing, fine-tuning, and RAG points of view, it is vital to know what type of data we ingested, as each category must be processed differently. For example, the chunking strategy between a post, article, and piece of code will look different.\n\nAlso, by grouping the data by category, not the source, we can quickly plug data from other platforms, such as X into the posts or GitLab into the code collection. As a modular system, we must attach an additional ETL in the data collection pipeline, and everything else will work without further code modifications.\n\nOceanofPDF.com\n\nFeature pipeline\n\nThe feature pipeline’s role is to take raw articles, posts, and code data points from the data warehouse, process them, and load them into the feature store.\n\nThe characteristics of the FTI pattern are already present.\n\nHere are some custom properties of the LLM Twin’s feature pipeline:\n\nIt processes three types of data differently: articles, posts, and code\n\nIt contains three main processing steps necessary for fine-tuning and RAG: cleaning, chunking, and embedding\n\nIt creates two snapshots of the digital data, one after cleaning (used for fine- tuning) and one after embedding (used for RAG)\n\nIt uses a logical feature store instead of a specialized feature store\n\nLet’s zoom in on the logical feature store part a bit. As with any RAG-based system, one of the central pieces of the infrastructure is a vector DB. Instead of integrating another DB, more concretely, a specialized feature store, we used the vector DB, plus some additional logic to check all the properties of a feature store our system needs.\n\nThe vector DB doesn’t offer the concept of a training dataset, but it can be used as a NoSQL DB. This means we can access data points using their ID and collection name. Thus, we can easily query the vector DB for new data points without any vector search logic. Ultimately, we will wrap the retrieved data into a versioned, tracked, and shareable artifact—more on artifacts in Chapter 2. For now, you must know it is an MLOps concept used to wrap data and enrich it with the properties listed before.\n\nHow will the rest of the system access the logical feature store? The training pipeline will use the instruct datasets as artifacts, and the inference pipeline will query the vector DB for additional context using vector search techniques.\n\nFor our use case, this is more than enough because of the following reasons:\n\nThe artifacts work great for offline use cases such as training\n\nThe vector DB is built for online access, which we require for inference.\n\nIn future chapters, however, we will explain how the three data categories (articles, posts, and code) are cleaned, chunked, and embedded.\n\nTo conclude, we take in raw article, post, or code data points, process them, and store them in a feature store to make them accessible to the training and inference pipelines. Note that trimming all the complexity away and focusing only on the interface is a perfect match with the FTI pattern. Beautiful, right?\n\nOceanofPDF.com",
      "page_number": 44
    },
    {
      "number": 2,
      "title": "For now, you must know it is an MLOps concept used to wrap data and enrich it with the properties listed before",
      "start_page": 107,
      "end_page": 192,
      "detection_method": "regex_chapter",
      "content": "Training pipeline\n\nThe training pipeline consumes instruct datasets from the feature store, fine- tunes an LLM with it, and stores the tuned LLM weights in a model registry. More concretely, when a new instruct dataset is available in the logical feature store, we will trigger the training pipeline, consume the artifact, and fine-tune the LLM.\n\nIn the initial stages, the data science team owns this step. They run multiple experiments to find the best model and hyperparameters for the job, either through automatic hyperparameter tuning or manually. To compare and pick the best set of hyperparameters, we will use an experiment tracker to log everything of value and compare it between experiments. Ultimately, they will pick the best hyperparameters and fine-tuned LLM and propose it as the LLM production candidate. The proposed LLM is then stored in the model registry. After the experimentation phase is over, we store and reuse the best hyperparameters found to eliminate the manual restrictions of the process. Now, we can completely automate the training process, known as continuous training.\n\nThe testing pipeline is triggered for a more detailed analysis than during fine-tuning. Before pushing the new model to production, assessing it against a stricter set of tests is critical to see that the latest candidate is better than what is currently in production. If this step passes, the model is ultimately tagged as accepted and deployed to the production inference pipeline. Even in a fully automated ML system, it is recommended to have a manual step before accepting a new production model. It is like pushing the red button before a significant action with high consequences. Thus, at this stage, an expert looks at a report generated by the testing component. If\n\neverything looks good, it approves the model, and the automation can continue.\n\nThe particularities of this component will be on LLM aspects, such as the following:\n\nHow do you implement an LLM agnostic pipeline?\n\nWhat fine-tuning techniques should you use?\n\nHow do you scale the fine-tuning algorithm on LLMs and datasets of various sizes?\n\nHow do you pick an LLM production candidate from multiple experiments?\n\nHow do you test the LLM to decide whether to push it to production or not?\n\nBy the end of this book, you will know how to answer all these questions.\n\nOne last aspect we want to clarify is CT. Our modular design allows us to quickly leverage an ML orchestrator to schedule and trigger different\n\nsystem parts. For example, we can schedule the data collection pipeline to crawl data every week.\n\nThen, we can trigger the feature pipeline when new data is available in the data warehouse and the training pipeline when new instruction datasets are available.\n\nOceanofPDF.com\n\nInference pipeline\n\nThe inference pipeline is the last piece of the puzzle. It is connected to the model registry and logical feature store. It loads a fine-tuned LLM from the model registry, and from the logical feature store, it accesses the vector DB for RAG. It takes in client requests through a REST API as queries. It uses the fine-tuned LLM and access to the vector DB to carry out RAG and answer the queries.\n\nAll the client queries, enriched prompts using RAG, and generated answers are sent to a prompt monitoring system to analyze, debug, and better understand the system. Based on specific requirements, the monitoring system can trigger alarms to take action either manually or automatically.\n\nAt the interface level, this component follows exactly the FTI architecture, but when zooming in, we can observe unique characteristics of an LLM and RAG system, such as the following:\n\nA retrieval client used to do vector searches for RAG\n\nPrompt templates used to map user queries and external information to LLM inputs\n\nSpecial tools for prompt monitoring\n\nOceanofPDF.com\n\nFinal thoughts on the FTI design and the LLM Twin architecture\n\nWe don’t have to be highly rigid about the FTI pattern. It is a tool used to clarify how to design ML systems. For example, instead of using a dedicated features store just because that is how it is done, in our system, it is easier and cheaper to use a logical feature store based on a vector DB and artifacts. What was important to focus on were the required properties a feature store provides, such as a versioned and reusable training dataset.\n\nUltimately, we will explain the computing requirements of each component briefly. The data collection and feature pipeline are mostly CPU-based and do not require powerful machines. The training pipeline requires powerful GPU-based machines that could load an LLM and fine-tune it. The inference pipeline is somewhere in the middle. It still needs a powerful machine but is less compute-intensive than the training step. However, it must be tested carefully, as the inference pipeline directly interfaces with the user. Thus, we want the latency to be within the required parameters for a good user experience. However, using the FTI design is not an issue. We can pick the proper computing requirements for each component.\n\nAlso, each pipeline will be scaled differently. The data and feature pipelines will be scaled horizontally based on the CPU and RAM load. The training pipeline will be scaled vertically by adding more GPUs. The inference pipeline will be scaled horizontally based on the number of client requests.\n\nTo conclude, the presented LLM architecture checks all the technical requirements listed at the beginning of the section. It processes the data as requested, and the training is modular and can be quickly adapted to different LLMs, datasets, or fine-tuning techniques. The inference pipeline supports RAG and is exposed as a REST API. On the LLMOps side, the system supports dataset and model versioning, lineage, and reusability. The system has a monitoring service, and the whole ML architecture is designed with CT/CI/CD in mind.\n\nThis concludes the high-level overview of the LLM Twin architecture.\n\nOceanofPDF.com\n\nSummary\n\nThis first chapter was critical to understanding the book’s goal. As a product-oriented book that will walk you through building an end-to-end ML system, it was essential to understand the concept of an LLM Twin initially. Afterward, we walked you through what an MVP is and how to plan our LLM Twin MVP based on our available resources. Following this, we translated our concept into a practical technical solution with specific requirements. In this context, we introduced the FTI design pattern and showcased its real-world application in designing systems that are both modular and scalable. Ultimately, we successfully applied the FTI pattern to design the architecture of the LLM Twin to fit all our technical requirements.\n\nHaving a clear vision of the big picture is essential when building systems. Understanding how a single component will be integrated into the rest of the application can be very valuable when working on it. We started with a more abstract presentation of the LLM Twin architecture, focusing on each component’s scope, interface, and interconnectivity.\n\nThe following chapters will explore how to implement and deploy each component. On the MLOps side, we will walk you through using a computing platform, orchestrator, model registry, artifacts, and other tools and concepts to support all MLOps best practices.\n\nOceanofPDF.com\n\nReferences\n\nDowling, J. (2024a, July 11). From MLOps to ML Systems with Feature/Training/Inference Pipelines. Hopsworks. https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines\n\nDowling, J. (2024b, August 5). Modularity and Composability for AI Systems with AI Pipelines and Shared Storage. Hopsworks. https://www.hopsworks.ai/post/modularity-and-composability-for-ai- systems-with-ai-pipelines-and-shared-storage\n\nJoseph, M. (2024, August 23). The Taxonomy for Data Transformations in AI Systems. Hopsworks. https://www.hopsworks.ai/post/a-taxonomy-for- data-transformations-in-ai-systems\n\nMLOps: Continuous delivery and automation pipelines in machine learning. (2024, August 28). Google Cloud. https://cloud.google.com/architecture/mlops-continuous-delivery-and- automation-pipelines-in-machine-learning\n\nQwak. (2024a, June 2). CI/CD for Machine Learning in 2024: Best Practices to build, test, and Deploy | Infer. Medium. https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best- practices-to-build-test-and-deploy-c4ad869824d2\n\nQwak. (2024b, July 23). 5 Best Open Source Tools to build End-to-End MLOPs Pipeline in 2024. Medium. https://medium.com/infer- qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools- d8bacbf4184f\n\nSalama, K., Kazmierczak, J., & Schut, D. (2021). Practitioners guide to MLOPs: A framework for continuous delivery and automation of machine learning (1st ed.) [PDF]. Google Cloud. https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whi tepaper.pdf\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\n2\n\nOceanofPDF.com\n\nTooling and Installation\n\nThis chapter presents all the essential tools that will be used throughout the book, especially in implementing and deploying the LLM Twin project. At this point in the book, we don’t plan to present in-depth LLM, RAG, MLOps, or LLMOps concepts. We will quickly walk you through our tech stack and prerequisites to avoid repeating ourselves throughout the book on how to set up a particular tool and why we chose it. Starting with Chapter 3, we will begin exploring our LLM Twin use case by implementing a data collection ETL that crawls data from the internet.\n\nIn the first part of the chapter, we will present the tools within the Python ecosystem to manage multiple Python versions, create a virtual environment, and install the pinned dependencies required for our project to run. Alongside presenting these tools, we will also show how to install the\n\nLLM-Engineers-Handbook\n\nrepository on your local machine (in case you want to try out the code yourself): https://github.com/PacktPublishing/LLM-Engineers-Handbook.\n\nNext, we will explore all the MLOps and LLMOps tools we will use, starting with more generic tools, such as a model registry, and moving on to more LLM-oriented tools, such as LLM evaluation and prompt monitoring tools. We will also understand how to manage a project with multiple ML pipelines using ZenML, an orchestrator bridging the gap between ML and MLOps. Also, we will quickly explore what databases we will use for NoSQL and vector storage. We will show you how to run all these components on your local machine using Docker. Lastly, we will quickly\n\nreview AWS and show you how to create an AWS user and access keys and install and configure the AWS CLI to manipulate your cloud resources programmatically. We will also explore SageMaker and why we use it to train and deploy our open-source LLMs.\n\nIf you are familiar with these tools, you can safely skip this chapter. We also explain how to install the project and set up all the necessary components in the repository’s\n\nREADME\n\n. Thus, you also have the option to use that as more concise documentation if you plan to run the code while reading the book.\n\nTo sum all that up, in this chapter, we will explore the following topics:\n\nPython ecosystem and project installation\n\nMLOps and LLMOps tooling\n\nDatabases for storing unstructured and vector data\n\nPreparing for AWS\n\nBy the end of this chapter, you will be aware of all the tools we will use across the book. Also, you will have learned how to install the\n\nLLM-Engineers-Handbook\n\nrepository, set up the rest of the tools, and use them if you run the code while reading the book.\n\nOceanofPDF.com\n\nPython ecosystem and project installation\n\nAny Python project needs three fundamental tools: the Python interpreter, dependency management, and a task execution tool. The Python interpreter executes your Python project as expected. All the code within the book is tested with Python 3.11.8. You can download the Python interpreter from here: https://www.python.org/downloads/. We recommend installing the exact Python version (Python 3.11.8) to run the LLM Twin project using\n\npyenv\n\n, making the installation process straightforward.\n\nInstead of installing multiple global Python versions, we recommend managing them using\n\npyenv\n\n, a Python version management tool that lets you manage multiple Python versions between projects. You can install it using this link: https://github.com/pyenv/pyenv?tab=readme-ov-file#installation.\n\nAfter you have installed\n\npyenv\n\n, you can install the latest version of Python 3.11, using\n\npyenv\n\n, as follows:\n\npyenv install 3.11.8\n\nNow list all installed Python versions to see that it was installed correctly:\n\npyenv versions\n\nYou should see something like this:\n\n#\n\nsystem\n\n#\n\n3.11.8\n\nTo make Python 3.11.8 the default version across your entire system (whenever you open a new terminal), use the following command:\n\npyenv global 3.11.8\n\nHowever, we aim to use Python 3.11.8 locally only in our repository. To achieve that, first, we have to clone the repository and navigate to it:\n\ngit clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git cd LLM-Engineers-Handbook\n\nBecause we defined a\n\n.python-version\n\nfile within the repository,\n\npyenv\n\nwill know to pick up the version from that file and use it locally whenever you are working within that folder. To double-check that, run the following command while you are in the repository:\n\npython --version\n\nIt should output:\n\n# Python 3.11.8\n\nTo create the\n\n.python-version\n\nfile, you must run\n\npyenv local 3.11.8\n\nonce. Then,\n\npyenv\n\nwill always know to use that Python version while working within a specific directory.\n\nNow that we have installed the correct Python version using\n\npyenv\n\n, let’s move on to Poetry, which we will use as our dependency and virtual environment manager.\n\nOceanofPDF.com\n\nPoetry: dependency and virtual environment management\n\nPoetry is one of the most popular dependency and virtual environment managers within the Python ecosystem. But let’s start by clarifying what a dependency manager is. In Python, a dependency manager allows you to specify, install, update, and manage external libraries or packages (dependencies) that a project relies on. For example, this is a simple Poetry requirements file that uses Python 3.11 and the\n\nrequests\n\nand\n\nnumpy\n\nPython packages.\n\n[tool.poetry.dependencies] python = \"^3.11\" requests = \"^2.25.1\" numpy = \"^1.19.5\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\"\n\nBy using Poetry to pin your dependencies, you always ensure that you install the correct version of the dependencies that your projects work with. Poetry, by default, saves all its requirements in\n\npyproject.toml\n\nfiles, which are stored at the root of your repository, as you can see in the cloned LLM-Engineers-Handbook repository.\n\nAnother massive advantage of using Poetry is that it creates a new Python virtual environment in which it installs the specified Python version and requirements. A virtual environment allows you to isolate your project’s dependencies from your global Python dependencies and other projects. By doing so, you ensure there are no version clashes between projects. For example, let’s assume that Project A needs\n\nnumpy == 1.19.5\n\n, and Project B needs\n\nnumpy == 1.26.0\n\n. If you keep both projects in the global Python environment, that will not work, as Project B will override Project A’s\n\nnumpy\n\ninstallation, which will corrupt Project A and stop it from working. Using Poetry, you can isolate each project in its own Python environment with its own Python dependencies, avoiding any dependency clashes.\n\nYou can install Poetry from here: https://python-poetry.org/docs/. We use Poetry 1.8.3 throughout the book. Once Poetry is installed, navigate to your cloned LLM-Engineers-Handbook repository and run the following command to install all the necessary Python dependencies:\n\npoetry install --without aws\n\nThis command knows to pick up all the dependencies from your repository that are listed in the\n\npyproject.toml\n\nand\n\npoetry.lock\n\nfiles. After the installation, you can activate your Poetry environment by running\n\npoetry shell\n\nin your terminal or by prefixing all your CLI commands as follows:\n\npoetry run\n\n.\n\nOne final note on Poetry is that it locks down the exact versions of the dependency tree in the\n\npoetry.lock\n\nfile based on the definitions added to the\n\nproject.toml\n\nfile. While the\n\npyproject.toml\n\nfile may specify version ranges (e.g.,\n\nrequests = \"^2.25.1\"\n\n), the\n\npoetry.lock\n\nfile records the exact version (e.g.,\n\nrequests = \"2.25.1\"\n\n) that was installed. It also locks the versions of sub-dependencies (dependencies of your dependencies), which may not be explicitly listed in your\n\npyproject.toml\n\nfile. By locking all the dependencies and sub-dependencies to specific versions, the\n\npoetry.lock\n\nfile ensures that all project installations use the same versions of each package. This consistency leads to predictable behavior, reducing the likelihood of encountering “works on my machine” issues.\n\nOther tools similar to Poetry are Venv and Conda for creating virtual environments. Still, they lack the dependency management option. Thus, you must do it through Python’s default\n\nrequirements.txt\n\nfiles, which are less powerful than Poetry’s\n\nlock\n\nfiles. Another option is Pipenv, which feature-wise is more like Poetry but slower, and\n\nuv\n\n, which is a replacement for Poetry built in Rust, making it blazing fast.\n\nuv\n\nhas lots of potential to replace Poetry, making it worthwhile to test out: https://github.com/astral-sh/uv.\n\nThe final piece of the puzzle is to look at the task execution tool we used to manage all our CLI commands.\n\nOceanofPDF.com\n\nPoe the Poet: task execution tool\n\nPoe the Poet is a plugin on top of Poetry that is used to manage and execute all the CLI commands required to interact with the project. It helps you define and run tasks within your Python project, simplifying automation and script execution. Other popular options are Makefile, Invoke, or shell scripts, but Poe the Poet eliminates the need to write separate shell scripts or Makefiles for managing project tasks, making it an elegant way to manage tasks using the same configuration file that Poetry already uses for dependencies.\n\nWhen working with Poe the Poet, instead of having all your commands documented in a README file or other document, you can add them directly to your\n\npyproject.toml\n\nfile and execute them in the command line with an alias. For example, using Poe the Poet, we can define the following tasks in a\n\npyproject.toml\n\nfile:\n\n[tool.poe.tasks]\n\ntest\n\n=\n\n\"pytest\"\n\nformat\n\n=\n\n\"black .\"\n\nstart\n\n=\n\n\"python main.py\"\n\nYou can then run these tasks using the\n\npoe\n\ncommand:\n\npoetry poe test poetry poe format poetry poe start\n\nYou can install Poe the Poet as a Poetry plugin, as follows:\n\npoetry self add 'poethepoet[poetry_plugin]'\n\nTo conclude, using a tool as a façade over all your CLI commands is necessary to run your application. It significantly simplifies the application’s complexity and enhances collaboration as it acts as out-of-the- box documentation.\n\nAssuming you have\n\npyenv\n\nand Poetry installed, here are all the commands you need to run to clone the repository and install the dependencies and Poe the Poet as a Poetry plugin:\n\ngit clone https://github.com/PacktPublishing/LLM-Engineers- Handbook.gitcd LLM-Engineers-Handbook poetry install --without aws poetry self add 'poethepoet[poetry_plugin]'\n\nTo make the project fully operational, there are still a few steps to follow, such as filling out a\n\n.env\n\nfile with your credentials and getting tokens from OpenAI and Hugging Face. But this book isn’t an installation guide, so we’ve moved all these details into the repository’s README as they are useful only if you plan to run the repository: https://github.com/PacktPublishing/LLM-Engineers- Handbook.\n\nNow that we have installed our Python project, let’s present the MLOps tools we will use in the book. If you are already familiar with these tools, you can safely skip the following tooling section and move on to the Databases for storing unstructured and vector data section.\n\nOceanofPDF.com\n\nMLOps and LLMOps tooling\n\nThis section will quickly present all the MLOps and LLMOps tools we will use throughout the book and their role in building ML systems using MLOps best practices. At this point in the book, we don’t aim to detail all the MLOps components we will use to implement the LLM Twin use case, such as model registries and orchestrators, but only provide a quick idea of what they are and how to use them. As we develop the LLM Twin project throughout the book, you will see hands-on examples of how we use all these tools. In Chapter 11, we will dive deeply into the theory of MLOps and LLMOps and connect all the dots. As the MLOps and LLMOps fields are highly practical, we will leave the theory of these aspects to the end, as it will be much easier to understand it after you go through the LLM Twin use case implementation.\n\nAlso, this section is not dedicated to showing you how to set up each tool. It focuses primarily on what each tool is used for and highlights the core features used throughout this book.\n\nStill, using Docker, you can quickly run the whole infrastructure locally. If you want to run the steps within the book yourself, you can host the application locally with these three simple steps:\n\nHave Docker 27.1.1 (or higher) installed.\n\nFill your\n\n.env\n\nfile with all the necessary credentials as explained in the repository README.\n\nRun\n\npoetry\n\npoe\n\nlocal-infrastructure-up\n\nto locally spin up ZenML (\n\nhttp://127.0.0.1:8237/\n\n) and the MongoDB and Qdrant databases.\n\nYou can read more details on how to run everything locally in the LLM- Engineers-Handbook repository README: https://github.com/PacktPublishing/LLM-Engineers-Handbook. Within the book, we will also show you how to deploy each component to the cloud.\n\nOceanofPDF.com\n\nHugging Face: model registry\n\nA model registry is a centralized repository that manages ML models throughout their lifecycle. It stores models along with their metadata, version history, and performance metrics, serving as a single source of truth. In MLOps, a model registry is crucial for tracking, sharing, and documenting model versions, facilitating team collaboration. Also, it is a fundamental element in the deployment process as it integrates with continuous integration and continuous deployment (CI/CD) pipelines.\n\nWe used Hugging Face as our model registry, as we can leverage its ecosystem to easily share our fine-tuned LLM Twin models with anyone who reads the book. Also, by following the Hugging Face model registry interface, we can easily integrate the model with all the frameworks around the LLMs ecosystem, such as Unsloth for fine-tuning and SageMaker for inference.\n\nOur fine-tuned LLMs are available on Hugging Face at:\n\nTwinLlama 3.1 8B (after fine-tuning): https://huggingface.co/mlabonne/TwinLlama-3.1-8B\n\nTwinLlama 3.1 8B DPO (after preference alignment): https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO\n\nFigure 2.1: Hugging Face model registry example\n\nFor a quick demo, we have them available on Hugging Face Spaces:\n\nTwinLlama 3.1 8B: https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B\n\nTwinLlama 3.1 8B DPO: https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B-DPO\n\nMost ML tools provide model registry features. For example, ZenML, Comet, and SageMaker, which we will present in future sections, also offer their own model registries. They are good options, but we picked Hugging Face solely because of its ecosystem, which provides easy shareability and integration throughout the open-source environment. Thus, you will usually\n\nselect the model registry that integrates the most with your project’s tooling and requirements.\n\nOceanofPDF.com\n\nZenML: orchestrator, artifacts, and metadata\n\nZenML acts as the bridge between ML and MLOps. Thus, it offers multiple MLOps features that make your ML pipeline traceability, reproducibility, deployment, and maintainability easier. At its core, it is designed to create reproducible workflows in machine learning. It addresses the challenge of transitioning from exploratory research in Jupyter notebooks to a production-ready ML environment. It tackles production-based replication issues, such as versioning difficulties, reproducing experiments, organizing complex ML workflows, bridging the gap between training and deployment, and tracking metadata. Thus, ZenML’s main features are orchestrating ML pipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts for better observability.\n\nInstead of being another ML platform, ZenML introduced the concept of a stack, which allows you to run ZenML on multiple infrastructure options. A stack will enable you to connect ZenML to different cloud services, such as:\n\nAn orchestrator and compute engine (for example, AWS SageMaker or Vertex AI)\n\nRemote storage (for instance, AWS S3 or Google Cloud Storage buckets)\n\nA container registry (for example, Docker Registry or AWS ECR)\n\nThus, ZenML acts as a glue that brings all your infrastructure and tools together in one place through its stack feature, allowing you to quickly iterate through your development processes and easily monitor your entire ML system. The beauty of this is that ZenML doesn’t vendor-lock you into any cloud platform. It completely abstracts away the implementation of your Python code from the infrastructure it runs on. For example, in our LLM Twin use case, we used the AWS stack:\n\nSageMaker as our orchestrator and compute\n\nS3 as our remote storage used to store and track artifacts\n\nECR as our container registry\n\nHowever, the Python code contains no S3 or ECR particularities, as ZenML takes care of them. Thus, we can easily switch to other providers, such as Google Cloud Storage or Azure. For more details on ZenML stacks, you can start here: https://docs.zenml.io/user-guide/production- guide/understand-stacks.\n\nWe will focus only on the ZenML features used throughout the book, such as orchestrating, artifacts, and metadata. For more details on ZenML, check out their starter guide: https://docs.zenml.io/user-guide/starter-guide.\n\nThe local version of the ZenML server comes installed as a Python package. Thus, when running\n\npoetry install\n\n, it installs a ZenML debugging server that you can use locally. In Chapter 11, we will show you how to use their cloud serverless option to deploy the ML pipelines to AWS.\n\nOceanofPDF.com\n\nOrchestrator\n\nAn orchestrator is a system that automates, schedules, and coordinates all your ML pipelines. It ensures that each pipeline—such as data ingestion, preprocessing, model training, and deployment—executes in the correct order and handles dependencies efficiently. By managing these processes, an orchestrator optimizes resource utilization, handles failures gracefully, and enhances scalability, making complex ML pipelines more reliable and easier to manage.\n\nHow does ZenML work as an orchestrator? It works with pipelines and steps. A pipeline is a high-level object that contains multiple steps. A function becomes a ZenML pipeline by being decorated with\n\n@pipeline\n\n, and a step when decorated with\n\n@step\n\n. This is a standard pattern when using orchestrators: you have a high-level function, often called a pipeline, that calls multiple units/steps/tasks.\n\nLet’s explore how we can implement a ZenML pipeline with one of the ML pipelines implemented for the LLM Twin project. In the code snippet below, we defined a ZenML pipeline that queries the database for a user based on its full name and crawls all the provided links under that user:\n\nfrom\n\nzenml\n\nimport\n\npipeline\n\nfrom\n\nsteps.etl\n\nimport\n\ncrawl_links, get_or_create_user\n\n@pipeline\n\ndef\n\ndigital_data_etl\n\n(\n\nuser_full_name:\n\nstr\n\n, links:\n\nlist\n\n[\n\nstr\n\n]\n\n) ->\n\nNone\n\n: user = get_or_create_user(user_full_name) crawl_links(user=user, links=links)\n\nYou can run the pipeline with the following CLI command:\n\npoetry poe run-digital-data-etl\n\n. To visualize the pipeline run, you can go to your ZenML dashboard (at\n\nhttp://127.0.0.1:8237/\n\n) and, on the left panel, click on the Pipelines tab and then on the digital_data_etl pipeline, as illustrated in Figure 2.2:\n\nFigure 2.2: ZenML Pipelines dashboard\n\nAfter clicking on the digital_data_etl pipeline, you can visualize all the previous and current pipeline runs, as seen in Figure 2.3. You can see which one succeeded, failed, or is still running. Also, you can see the stack used to run the pipeline, where the default stack is the one used to run your ML pipelines locally.\n\nFigure 2.3: ZenML digital_data_etl pipeline dashboard. Example of a specific pipeline\n\nNow, after clicking on the latest digital_data_etl pipeline run (or any other run that succeeded or is still running), we can visualize the pipeline’s steps, outputs, and insights, as illustrated in Figure 2.4. This structure is often called a directed acyclic graph (DAG). More on DAGs in Chapter 11.\n\nFigure 2.4: ZenML digital_data_etl pipeline run dashboard (example of a specific pipeline run)\n\nBy clicking on a specific step, you can get more insights into its code and configuration. It even aggregates the logs output by that specific step to avoid switching between tools, as shown in Figure 2.5.\n\nFigure 2.5: Example of insights from a specific step of the digital_data_etl pipeline run\n\nNow that we understand how to define a ZenML pipeline and how to look it up in the dashboard, let’s quickly look at how to define a ZenML step. In the code snippet below, we defined the\n\nget_or_create_user()\n\nstep, which works just like a normal Python function but is decorated with\n\n@step\n\n. We won’t go into the details of the logic, as we will cover the ETL logic in Chapter 3. For now, we will focus only on the ZenML functionality.\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\ntyping_extensions\n\nimport\n\nAnnotated\n\nfrom\n\nzenml\n\nimport\n\nget_step_context, step\n\nfrom\n\nllm_engineering.application\n\nimport\n\nutils\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nUserDocument\n\n@step\n\ndef\n\nget_or_create_user\n\n(\n\nuser_full_name:\n\nstr\n\n) -> Annotated[UserDocument,\n\n\"user\"\n\n]: logger.info(\n\nf\"Getting or creating user:\n\n{user_full_name}\n\n\"\n\n) first_name, last_name = utils.split_user_full_name(user_full_name) user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)\n\nreturn\n\nuser\n\nWithin a ZenML step, you can define any Python logic your use case needs. In this simple example, we are just creating or retrieving a user, but we could replace that code with anything, starting from data collection to feature engineering and training. What is essential to notice is that to integrate ZenML with your code, you have to write modular code, where each function does just one thing. The modularity of your code makes it easy to decorate your functions with\n\n@step\n\nand then glue multiple steps together within a main function decorated with\n\n@pipeline\n\n. One design choice that will impact your application is deciding the granularity of each step, as each will run as a different unit on a different machine when deployed in the cloud.\n\nTo decouple our code from ZenML, we encapsulated all the application and domain logic into the\n\nllm_engineering\n\nPython module. We also defined the\n\npipelines\n\nand\n\nsteps\n\nfolders, where we defined our ZenML logic. Within the\n\nsteps\n\nmodule, we only used what we needed from the\n\nllm_engineering\n\nPython module (similar to how you use a Python package). In the\n\npipelines\n\nmodule, we only aggregated ZenML steps to glue them into the final pipeline. Using this design, we can easily swap ZenML with another orchestrator or use our application logic in other use cases, such as a REST API. We only have to replace the ZenML code without touching the\n\nllm_engineering\n\nmodule where all our logic resides.\n\nThis folder structure is reflected at the root of the LLM-Engineers- Handbook repository, as illustrated in Figure 2.6:\n\nFigure 2.6: LLM-Engineers-Handbook repository folder structure\n\nOne last thing to consider when writing ZenML steps is that if you return a value, it should be serializable. ZenML can serialize most objects that can be reduced to primitive data types, but there are a few exceptions. For example, we used UUID types as IDs throughout the code, which aren’t natively supported by ZenML. Thus, we had to extend ZenML’s materializer to support UUIDs. We raised this issue to ZenML. Hence, in future ZenML versions, UUIDs will be supported, but it was an excellent example of the serialization aspect of transforming function outputs in artifacts.\n\nOceanofPDF.com\n\nArtifacts and metadata\n\nAs mentioned in the previous section, ZenML transforms any step output into an artifact. First, let’s quickly understand what an artifact is. In MLOps, an artifact is any file(s) produced during the machine learning lifecycle, such as datasets, trained models, checkpoints, or logs. Artifacts are crucial for reproducing experiments and deploying models. We can transform anything into an artifact. For example, the model registry is a particular use case for an artifact. Thus, artifacts have these unique properties: they are versioned, sharable, and have metadata attached to them to understand what’s inside quickly. For example, when wrapping your dataset with an artifact, you can add to its metadata the size of the dataset, the train-test split ratio, the size, types of labels, and anything else useful to understand what’s inside the dataset without actually downloading it.\n\nLet’s circle back to our digital_data_etl pipeline example, where we had as a step output an artifact, the crawled links, which are an artifact, as seen in Figure 2.7\n\nFigure 2.7: ZenML artifact example using the digital_data_etl pipeline as an example\n\nBy clicking on the\n\ncrawled_links\n\nartifact and navigating to the Metadata tab, we can quickly see all the domains we crawled for a particular author, the number of links we crawled for each domain, and how many were successful, as illustrated in Figure 2.8:\n\nFigure 2.8: ZenML metadata example using the digital_data_etl pipeline as an example\n\nA more interesting example of an artifact and its metadata is the generated dataset artifact. In Figure 2.9, we can visualize the metadata of the\n\ninstruct_datasets\n\nartifact, which was automatically generated and will be used to fine-tune the LLM Twin model. More details on the\n\ninstruction datasets\n\nare in Chapter 5. For now, we want to highlight that within the dataset’s metadata, we have precomputed a lot of helpful information about it, such as how many data categories it contains, its storage size, and the number of samples per training and testing split.\n\nFigure 2.9: ZenML metadata example for the instruct_datasets artifact\n\nThe metadata is manually added to the artifact, as shown in the code snippet below. Thus, you can precompute and attach to the artifact’s metadata anything you consider helpful for dataset discovery across your business and projects:\n\n…\n\n# More imports\n\nfrom\n\nzenml\n\nimport\n\nArtifactConfig, get_step_context, step\n\n@step\n\ndef\n\ngenerate_intruction_dataset\n\n(\n\nprompts: Annotated[\n\ndict\n\n[DataCategory,\n\nlist\n\n[GenerateDatasetSamplesPrompt]],\n\n\"prompts\"\n\n]\n\n) -> Annotated[ InstructTrainTestSplit, ArtifactConfig( name=\n\n\"instruct_datasets\"\n\n, tags=[\n\n\"dataset\"\n\n,\n\n\"instruct\"\n\n,\n\n\"cleaned\"\n\n], ), ]: datasets = …\n\n# Generate datasets\n\nstep_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"instruct_datasets\"\n\n, metadata=_get_metadata_instruct_dataset(datasets))\n\nreturn\n\ndatasets\n\ndef\n\n_get_metadata_instruct_dataset\n\n(\n\ndatasets: InstructTrainTestSplit\n\n) ->\n\ndict\n\n[\n\nstr\n\n,\n\nAny\n\n]: instruct_dataset_categories =\n\nlist\n\n(datasets.train.keys()) train_num_samples = { category: instruct_dataset.num_samples\n\nfor\n\ncategory, instruct_dataset\n\nin\n\ndatasets.train.items() } test_num_samples = {category: instruct_dataset.num_samples\n\nfor\n\ncategory, instruct_dataset\n\nin\n\ndatasets.test.items()}\n\nreturn\n\n{\n\n\"data_categories\"\n\n: instruct_dataset_categories,\n\n\"test_split_size\"\n\n: datasets.test_split_size,\n\n\"train_num_samples_per_category\"\n\n: train_num_samples,\n\n\"test_num_samples_per_category\"\n\n: test_num_samples, }\n\nAlso, you can easily download and access a specific version of the dataset using its Universally Unique Identifier (UUID), which you can find using the ZenML dashboard or CLI:\n\nfrom\n\nzenml.client\n\nimport\n\nClient artifact = Client().get_artifact_version(\n\n'8bba35c4-8ff9-4d8f-a039-08046efc9fdc'\n\n) loaded_artifact = artifact.load()\n\nThe last step in exploring ZenML is understanding how to run and configure a ZenML pipeline.\n\nOceanofPDF.com\n\nHow to run and configure a ZenML pipeline\n\nAll the ZenML pipelines can be called from the\n\nrun.py\n\nfile, accessed at\n\ntools/run.py\n\nin our GitHub repository. Within the\n\nrun.py\n\nfile, we implemented a simple CLI that allows you to specify what pipeline to run. For example, to call the\n\ndigital_data_etl\n\npipeline to crawl Maxime’s content, you have to run:\n\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_maxime_labonne.yaml\n\nOr, to crawl Paul’s content, you can run:\n\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml\n\nAs explained when introducing Poe the Poet, all our CLI commands used to interact with the project will be executed through Poe to simplify and standardize the project. Thus, we encapsulated these Python calls under the following\n\npoe\n\nCLI commands:\n\npoetry poe run-digital-data-etl-maxime poetry poe run-digital-data-etl-paul\n\nWe only change the ETL config file name when scraping content for different people. ZenML allows us to inject specific configuration files at runtime as follows:\n\nconfig_path = root_dir /\n\n\"configs\"\n\n/ etl_config_filename assert config_path.exists(), f\n\n\"Config file not found: { config_path }\"\n\nrun_args_etl = {\n\n\"config_path\"\n\n: config_path,\n\n\"run_name\"\n\n: f\n\n\"digital_data_etl_run_{dt.now().strftime('\n\n%Y\n\n_\n\n%m\n\n_\n\n%d\n\n_\n\n%H\n\n_\n\n%M\n\n_\n\n%S\n\n')}\"\n\n} digital_data_etl.with_options()(**run_args_etl)\n\nIn the config file, we specify all the parameters that will input the pipeline as parameters. For example, the\n\nconfigs/digital_data_etl_maxime_labonne.yaml\n\nconfiguration file looks as follows:\n\nparameters: user_full_name: Maxime Labonne\n\n# [First Name(s)] [Last Name]\n\nlinks:\n\n# Personal Blog\n\nhttps://mlabonne.github.io/blog/posts/2024-07- 29_Finetune_Llama31.html - https://mlabonne.github.io/blog/posts/2024- 07-15_The_Rise_of_Agentic_Data_Generation.html # Substack - https://maximelabonne.substack.com/p/uncensor-any-llm-with-abliteration- d30148b7d43e … # More links\n\nWhere the\n\ndigital_data_etl\n\nfunction signature looks like this:\n\n@pipeline\n\ndef\n\ndigital_data_etl\n\n(\n\nuser_full_name:\n\nstr\n\n, links:\n\nlist\n\n[\n\nstr\n\n]\n\n) ->\n\nstr\n\n:\n\nThis approach allows us to configure each pipeline at runtime without modifying the code. We can also clearly track the inputs for all our pipelines, ensuring reproducibility. As seen in Figure 2.10, we have one or more configs for each pipeline.\n\nFigure 2.10: ZenML pipeline configs\n\nOther popular orchestrators similar to ZenML that we’ve personally tested and consider powerful are Airflow, Prefect, Metaflow, and Dagster. Also, if you are a heavy user of Kubernetes, you can opt for Agro Workflows or Kubeflow, the latter of which works only on top of Kubernetes. We still consider ZenML the best trade-off between ease of use, features, and costs. Also, none of these tools offer the stack feature that is offered by ZenML, which allows it to avoid vendor-locking you in to any cloud ecosystem.\n\nIn Chapter 11, we will explore in more depth how to leverage an orchestrator to implement MLOps best practices. But now that we understand ZenML, what it is helpful for, and how to use it, let’s move on to the experiment tracker.\n\nOceanofPDF.com\n\nComet ML: experiment tracker\n\nTraining ML models is an entirely iterative and experimental process. Unlike traditional software development, it involves running multiple parallel experiments, comparing them based on predefined metrics, and deciding which one should advance to production. An experiment tracking tool allows you to log all the necessary information, such as metrics and visual representations of your model predictions, to compare all your experiments and quickly select the best model. Our LLM project is no exception.\n\nAs illustrated in Figure 2.11, we used Comet to track metrics such as training and evaluation loss or the value of the gradient norm across all our experiments.\n\nFigure 2.11: Comet ML training metrics example\n\nUsing an experiment tracker, you can go beyond training and evaluation metrics and log your training hyperparameters to track different configurations between experiments.\n\nIt also logs out-of-the-box system metrics such as GPU, CPU, or memory utilization to give you a clear picture of what resources you need during training and where potential bottlenecks slow down your training, as seen in Figure 2.12.\n\nFigure 2.12: Comet ML system metrics example\n\nYou don’t have to set up Comet locally. We will use their online version for free without any constraints throughout this book. Also, if you want to look more in-depth into the Comet ML experiment tracker, we made the training experiments tracked with Comet ML public while fine-tuning our LLM Twin models. You can access them here: https://www.comet.com/mlabonne/llm-twin-training/view/new/panels.\n\nOther popular experiment trackers are W&B, MLflow, and Neptune. We’ve worked with all of them and can state that they all have mostly the same features, but Comet ML differentiates itself through its ease of use and intuitive interface. Let’s move on to the final piece of the MLOps puzzle: Opik for prompt monitoring.\n\nOceanofPDF.com\n\nOpik: prompt monitoring\n\nYou cannot use standard tools and techniques when logging and monitoring prompts. The reason for this is complicated. We will dig into it in Chapter 11. However, to quickly give you some understanding, you cannot use standard logging tools as prompts are complex and unstructured chains.\n\nWhen interacting with an LLM application, you chain multiple input prompts and the generated output into a trace, where one prompt depends on previous prompts.\n\nThus, instead of plain text logs, you need an intuitive way to group these traces into a specialized dashboard that makes debugging and monitoring traces of prompts easier.\n\nWe used Opik, an open-source tool made by Comet, as our prompt monitoring tool because it follows Comet’s philosophy of simplicity and ease of use, which is currently relatively rare in the LLM landscape. Other options offering similar features are Langfuse (open source, https://langfuse.com), Galileo (not open source, rungalileo.io), and LangSmith (not open source, https://www.langchain.com/langsmith), but we found their solutions more cumbersome to use and implement. Opik, along with its serverless option, also provides a free open-source version that you have complete control over. You can read more on Opik at https://github.com/comet-ml/opik.\n\nOceanofPDF.com\n\nDatabases for storing unstructured and vector data\n\nWe also want to present the NoSQL and vector databases we will use within our examples. When working locally, they are already integrated through Docker. Thus, when running\n\npoetry poe local-infrastructure-up\n\n, as instructed a few sections above, local images of Docker for both databases will be pulled and run on your machine. Also, when deploying the project, we will show you how to use their serverless option and integrate it with the rest of the LLM Twin project.",
      "page_number": 107
    },
    {
      "number": 3,
      "title": "However, to quickly give you some understanding, you cannot use standard logging tools as prompts are complex and unstructured chains",
      "start_page": 193,
      "end_page": 230,
      "detection_method": "regex_chapter",
      "content": "OceanofPDF.com\n\nMongoDB: NoSQL database\n\nMongoDB is one of today’s most popular, robust, fast, and feature-rich NoSQL databases. It integrates well with most cloud ecosystems, such as AWS, Google Cloud, Azure, and Databricks. Thus, using MongoDB as our NoSQL database was a no-brainer.\n\nWhen we wrote this book, MongoDB was used by big players such as Novo Nordisk, Delivery Hero, Okta, and Volvo. This widespread adoption suggests that MongoDB will remain a leading NoSQL database for a long time.\n\nWe use MongoDB as a NoSQL database to store the raw data we collect from the internet before processing it and pushing it into the vector database. As we work with unstructured text data, the flexibility of the NoSQL database fits like a charm.\n\nOceanofPDF.com\n\nQdrant: vector database\n\nQdrant (https://qdrant.tech/) is one of the most popular, robust, and feature- rich vector databases. We could have used almost any vector database for our small MVP, but we wanted to pick something light and likely to be used in the industry for many years to come.\n\nWe will use Qdrant to store the data from MongoDB after it’s processed and transformed for GenAI usability.\n\nQdrant is used by big players such as X (formerly Twitter), Disney, Microsoft, Discord, and Johnson & Johnson. Thus, it is highly probable that Qdrant will remain in the vector database game for a long time.\n\nWhile writing the book, other popular options were Milvus, Redis, Weaviate, Pinecone, Chroma, and pgvector (a PostgreSQL plugin for vector indexes). We found that Qdrant offers the best trade-off between RPS, latency, and index time, making it a solid choice for many generative AI applications.\n\nComparing all the vector databases in detail could be a chapter in itself. We don’t want to do that here. Still, if curious, you can check the Vector DB Comparison resource from Superlinked at https://superlinked.com/vector- db-comparison, which compares all the top vector databases in terms of everything you can think about, from the license and release year to database features, embedding models, and frameworks supported.\n\nOceanofPDF.com\n\nPreparing for AWS\n\nThis last part of the chapter will focus on setting up an AWS account (if you don’t already have one), an AWS access key, and the CLI. Also, we will look into what SageMaker is and why we use it.\n\nWe picked AWS as our cloud provider because it’s the most popular out there and the cloud in which we (the writers) have the most experience. The reality is that other big cloud providers, such as GCP or Azure, offer similar services. Thus, depending on your specific application, there is always a trade-off between development time (in which you have the most experience), features, and costs. But for our MVP, AWS, it’s the perfect option as it provides robust features for everything we need, such as S3 (object storage), ECR (container registry), and SageMaker (compute for training and inference).\n\nOceanofPDF.com\n\nSetting up an AWS account, an access key, and the CLI\n\nAs AWS could change its UI/UX, the best way to instruct you on how to create an AWS account is by redirecting you to their official tutorial: https://docs.aws.amazon.com/accounts/latest/reference/manage-acct- creating.html.\n\nAfter successfully creating an AWS account, you can access the AWS console at http://console.aws.amazon.com. Select Sign in using root user email (found under the Sign in button), then enter your account’s email address and password.\n\nNext, we must generate access keys to access AWS programmatically. The best option to do so is first to create an IAM user with administrative access as described in this AWS official tutorial: https://docs.aws.amazon.com/streams/latest/dev/setting-up.html\n\nFor production accounts, it is best practice to grant permissions with a policy of least privilege, giving each user only the permissions they require to perform their role. However, to simplify the setup of our test account, we will use the\n\nAdministratorAccess\n\nmanaged policy, which gives our user full access, as explained in the tutorial above and illustrated in Figure 2.13.\n\nFigure 2.13: IAM user permission policies example\n\nNext, you have to create an access key for the IAM user you just created using the following tutorial: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access- keys.html.\n\nThe access keys will look as follows:\n\naws_access_key_id\n\n=\n\naws_secret_access_key\n\n=\n\nJust be careful to store them somewhere safe, as you won’t be able to access them after you create them. Also, be cautious with who you share them, as they could be used to access your AWS account and manipulate various AWS resources.\n\nThe last step is to install the AWS CLI and configure it with your newly created access keys. You can install the AWS CLI using the following link: https://docs.aws.amazon.com/cli/latest/userguide/getting-started- install.html.\n\nAfter installing the AWS CLI, you can configure it by running\n\naws configure\n\n. Here is an example of our AWS configuration:\n\n[default]\n\naws_access_key_id\n\n= *************\n\naws_secret_access_key\n\n= ************\n\nregion\n\n= eu-central-\n\n1\n\noutput\n\n= json\n\nFor more details on how to configure the AWS CLI, check out the following tutorial: https://docs.aws.amazon.com/cli/v1/userguide/cli-configure- files.html.\n\nAlso, to configure the project with your AWS credentials, you must fill in the following variables within your\n\n.env\n\nfile:\n\nAWS_REGION=\"eu-central-1\" # Change it with your AWS region. By default, we use \"eu-central-1\". AWS_ACCESS_KEY=\"\" AWS_SECRET_KEY=\"\"\n\nAn important note about costs associated with hands-on tasks in this book\n\nAll the cloud services used across the book stick to their freemium option, except AWS. Thus, if you use a personal AWS account, you will be responsible for AWS costs as you follow along in this book. While some services may fall under AWS Free Tier usage, others will not. Thus, you are responsible for checking your billing console regularly.\n\nMost of the costs will come when testing SageMaker for training and inference. Based on our tests, the AWS costs can vary between $50 and $100 using the specifications provided in this book and repository.\n\nSee the AWS documentation on setting up billing alarms to monitor your costs at https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monit or_estimated_charges_with_cloudwatch.html.\n\nOceanofPDF.com\n\nSageMaker: training and inference compute\n\nThe last topic of this chapter is understanding SageMaker and why we decided to use it. SageMaker is an ML platform used to train and deploy ML models. An official definition is as follows: AWS SageMaker is a fully managed machine learning service by AWS that enables developers and data scientists to build, train, and deploy machine learning models at scale. It simplifies the process by handling the underlying infrastructure, allowing users to focus on developing high-quality models efficiently.\n\nWe will use SageMaker to fine-tune and operationalize our training pipeline on clusters of GPUs and to deploy our custom LLM Twin model as a REST API that can be accessed in real time from anywhere in the world.\n\nOceanofPDF.com\n\nWhy AWS SageMaker?\n\nWe must also discuss why we chose AWS SageMaker over simpler and more cost-effective options, such as AWS Bedrock. First, let’s explain Bedrock and its benefits.\n\nAmazon Bedrock is a serverless solution for deploying LLMs. Serverless means that there are no servers or infrastructure to manage. It provides pre- trained models, which you can access directly through API calls. When we wrote this book, they provided support only for Mistral, Flan, Llama 2, and Llama 3 (quite a limited list of options). You can send input data and receive predictions from the models without managing the underlying infrastructure or software. This approach significantly reduces the complexity and time required to integrate AI capabilities into applications, making it more accessible to developers with limited machine learning expertise. However, this ease of integration comes at the cost of limited customization options, as you’re restricted to the pre-trained models and APIs provided by Amazon Bedrock. In terms of pricing, Bedrock uses a simple pricing model based on the number of API calls. This straightforward pricing structure makes it more efficient to estimate and control costs.\n\nMeanwhile, SageMaker provides a comprehensive platform for building, training, and deploying machine learning models. It allows you to customize your ML processes entirely or even use the platform for research. That’s why SageMaker is mainly used by data scientists and machine learning experts who know how to program, understand machine learning concepts, and are comfortable working with cloud platforms such as AWS. SageMaker is a double-edged sword regarding costs, following a pay-as-\n\nyou-go pricing model similar to most AWS services. This means you have to pay for the usage of computing resources, storage, and any other services required to build your applications.\n\nIn contrast to Bedrock, even if the SageMaker endpoint is not used, you will still pay for the deployed resources on AWS, such as online EC2 instances. Thus, you have to design autoscaling systems that delete unused resources. To conclude, Bedrock offers an out-of-the-box solution that allows you to quickly deploy an API endpoint powered by one of the available foundation models. Meanwhile, SageMaker is a multi-functional platform enabling you to customize your ML logic fully.\n\nSo why did we choose SageMaker over Bedrock? Bedrock would have been an excellent solution for quickly prototyping something, but this is a book on LLM engineering, and our goal is to dig into all the engineering aspects that Bedrock tries to mask away. Thus, we chose SageMaker because of its high level of customizability, allowing us to show you all the engineering required to deploy a model.\n\nIn reality, even SageMaker isn’t fully customizable. If you want complete control over your deployment, use EKS, AWS’s Kubernetes self-managed service. In this case, you have direct access to the virtual machines, allowing you to fully customize how you build your ML pipelines, how they interact, and how you manage your resources. You could do the same thing with AWS ECS, AWS’s version of Kubernetes. Using EKS or ECS, you could also reduce the costs, as these services cost considerably less.\n\nTo conclude, SageMaker strikes a balance between complete control and customization and a fully managed service that hides all the engineering complexity behind the scenes. This balance ensures that you have the\n\ncontrol you need while also benefiting from the managed service’s convenience.\n\nOceanofPDF.com\n\nSummary\n\nIn this chapter, we reviewed the core tools used across the book. First, we understood how to install the correct version of Python that supports our repository. Then, we looked over how to create a virtual environment and install all the dependencies using Poetry. Finally, we understood how to use a task execution tool like Poe the Poet to aggregate all the commands required to run the application.\n\nThe next step was to review all the tools used to ensure MLOps best practices, such as a model registry to share our models, an experiment tracker to manage our training experiments, an orchestrator to manage all our ML pipelines and artifacts, and metadata to manage all our files and datasets. We also understood what type of databases we need to implement the LLM Twin use case. Finally, we explored the process of setting up an AWS account, generating an access key, and configuring the AWS CLI for programmatic access to the AWS cloud. We also gained a deep understanding of AWS SageMaker and the reasons behind choosing it to build our LLM Twin application.\n\nIn the next chapter, we will explore the implementation of the LLM Twin project by starting with the data collection ETL that scrapes posts, articles, and repositories from the internet and stores them in a data warehouse.\n\nOceanofPDF.com\n\nReferences\n\nAcsany, P. (2024, February 19). Dependency Management With Python Poetry. https://realpython.com/dependency-management-python-poetry/\n\nComet.ml. (n.d.). comet-ml/opik: Open-source end-to-end LLM Development Platform. GitHub. https://github.com/comet-ml/opik\n\nCzakon, J. (2024, September 25). ML Experiment Tracking: What It Is, Why It Matters, and How to Implement It. neptune.ai. https://neptune.ai/blog/ml- experiment-tracking\n\nHopsworks. (n.d.). ML Artifacts (ML Assets)? Hopsworks. https://www.hopsworks.ai/dictionary/ml-artifacts\n\nIntroduction | Documentation | Poetry – Python dependency management and packaging made easy. (n.d.). https://python-poetry.org/docs\n\nJones, L. (2024, March 21). Managing Multiple Python Versions With pyenv. https://realpython.com/intro-to-pyenv/\n\nKaewsanmua, K. (2024, January 3). Best Machine Learning Workflow and Pipeline Orchestration Tools. neptune.ai. https://neptune.ai/blog/best- workflow-and-pipeline-orchestration-tools\n\nMongoDB. (n.d.). What is NoSQL? NoSQL databases explained. https://www.mongodb.com/resources/basics/databases/nosql-explained\n\nNat-N. (n.d.). nat-n/poethepoet: A task runner that works well with poetry. GitHub. https://github.com/nat-n/poethepoet\n\nOladele, S. (2024, August 29). ML Model Registry: The Ultimate Guide. neptune.ai. https://neptune.ai/blog/ml-model-registry\n\nSchwaber-Cohen, R. (n.d.). What is a Vector Database & How Does it Work? Use Cases + Examples. Pinecone. https://www.pinecone.io/learn/vector-database/\n\nStarter guide | ZenML Documentation. (n.d.). https://docs.zenml.io/user- guide/starter-guide\n\nVector DB Comparison. (n.d.). https://superlinked.com/vector-db- comparison\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\n3\n\nOceanofPDF.com\n\nData Engineering\n\nThis chapter will begin exploring the LLM Twin project in more depth. We will learn how to design and implement the data collection pipeline to gather the raw data we will use in all our LLM use cases, such as fine- tuning or inference. As this is not a book on data engineering, we will keep this chapter short and focus only on what is strictly necessary to collect the required raw data. Starting with Chapter 4, we will concentrate on LLMs and GenAI, exploring its theory and concrete implementation details.\n\nWhen working on toy projects or doing research, you usually have a static dataset with which you work. But in our LLM Twin use case, we want to mimic a real-world scenario where we must gather and curate the data ourselves. Thus, implementing our data pipeline will connect the dots regarding how an end-to-end ML project works. This chapter will explore how to design and implement an Extract, Transform, Load (ETL) pipeline that crawls multiple social platforms, such as Medium, Substack, or GitHub, and aggregates the gathered data into a MongoDB data warehouse. We will show you how to implement various crawling methods, standardize the data, and load it into a data warehouse.\n\nWe will begin by designing the LLM Twin’s data collection pipeline and explaining the architecture of the ETL pipeline. Afterward, we will move directly to implementing the pipeline, starting with ZenML, which will orchestrate the entire process. We will investigate the crawler implementation and understand how to implement a dispatcher layer that instantiates the right crawler class based on the domain of the provided link while following software best practices. Next, we will learn how to\n\nimplement each crawler individually. Also, we will show you how to implement a data layer on top of MongoDB to structure all our documents and interact with the database.\n\nFinally, we will explore how to run the data collection pipeline using ZenML and query the collected data from MongoDB.\n\nThus, in this chapter, we will study the following topics:\n\nDesigning the LLM Twin’s data collection pipeline\n\nImplementing the LLM Twin’s data collection pipeline\n\nGathering raw data into the data warehouse\n\nBy the end of this chapter, you will know how to design and implement an ETL pipeline to extract, transform, and load raw data ready to be ingested into the ML application.\n\nOceanofPDF.com\n\nDesigning the LLM Twin’s data collection pipeline\n\nBefore digging into the implementation, we must understand the LLM Twin’s data collection ETL architecture, illustrated in Figure 3.1. We must explore what platforms we will crawl to extract data from and how we will design our data structures and processes. However, the first step is understanding how our data collection pipeline maps to an ETL process.\n\nAn ETL pipeline involves three fundamental steps:\n\nWe extract data from various sources. We will crawl data from platforms like Medium, Substack, and GitHub to gather raw data.\n\nWe transform this data by cleaning and standardizing it into a consistent format suitable for storage and analysis.\n\nWe load the transformed data into a data warehouse or database.\n\nFor our project, we use MongoDB as our NoSQL data warehouse. Although this is not a standard approach, we will explain the reasoning behind this choice shortly.\n\nFigure 3.1: LLM Twin’s data collection ETL pipeline architecture\n\nWe want to design an ETL pipeline that inputs a user and a list of links as input. Afterward, it crawls each link individually, standardizes the collected content, and saves it under that specific author in a MongoDB data warehouse.\n\nHence, the signature of the data collection pipeline will look as follows:\n\nInput: A list of links and their associated user (the author)\n\nOutput: A list of raw documents stored in the NoSQL data warehouse\n\nWe will use\n\nuser\n\nand\n\nauthor\n\ninterchangeably, as in most scenarios across the ETL pipeline, a user is the author of the extracted content. However, within the data warehouse, we have only a user collection.\n\nThe ETL pipeline will detect the domain of each link, based on which it will call a specialized crawler. We implemented four different crawlers for three different data categories, as seen in Figure 3.2. First, we will explore the three fundamental data categories we will work with across the book. All our collected documents can be boiled down to an article, repository (or code), and post. It doesn’t matter where the data comes from. We are primarily interested in the document’s format. In most scenarios, we will have to process these data categories differently. Thus, we created a different domain entity for each, where each entity will have its class and collection in MongoDB. As we save the source URL within the document’s metadata, we will still know its source and can reference it in our GenAI use cases.\n\nFigure 3.2: The relationship between the crawlers and the data categories\n\nOur codebase supports four different crawlers:\n\nMedium crawler: Used to collect data from Medium. It outputs an article document. It logs in to Medium and crawls the HTML of the article’s link. Then, it extracts, cleans, and normalizes the text from the HTML and loads the standardized text of the article into the NoSQL data warehouse.\n\nCustom article crawler: It performs similar steps to the Medium crawler but is a more generic implementation for collecting articles from various sites. Thus, as it doesn’t implement any particularities of any platform, it doesn’t perform the login step and blindly gathers all the HTML from a particular link. This is enough for articles freely available online, which you can find on Substack and people’s blogs. We will use this crawler as a safety net when the link’s domain isn’t associated with the other supported crawlers. For example, when providing a Substack link, it will default to the custom article crawler, but when providing a Medium URL, it will use the Medium crawler.\n\nGitHub crawler:This collects data from GitHub. It outputs a repository document. It clones the repository, parses the repository file tree, cleans and normalizes the files, and loads them to the database.\n\nLinkedIn crawler:This is used to collect data from LinkedIn. It outputs multiple post documents. It logs in to LinkedIn, navigates to the user’s\n\nfeed, and crawls all the user’s latest posts. For each post, it extracts its HTML, cleans and normalizes it, and loads it to MongoDB.\n\nIn the next section, we will examine each crawler’s implementation in detail. For now, note that each crawler accesses a specific platform or site in a particular way and extracts HTML from it. Afterward, all the crawlers parse the HTML, extract the text from it, and clean and normalize it so it can be stored in the data warehouse under the same interface.\n\nBy reducing all the collected data to three data categories and not creating a new data category for every new data source, we can easily extend this architecture to multiple data sources with minimal effort. For example, if we want to start collecting data from X, we only have to implement a new crawler that outputs a post document, and that’s it. The rest of the code will remain untouched. Otherwise, if we introduced the source dimension in the class and document structure, we would have to add code to all downstream layers to support any new data source. For example, we would have to implement a new document class for each new source and adapt the feature pipeline to support it.\n\nFor our proof of concept, crawling a few hundred documents is enough, but if we want to scale it to a real-world product, we would probably need more data sources to crawl from. LLMs are data-hungry. Thus, you need thousands of documents for ideal results instead of just a few hundred. But in many projects, it’s an excellent strategy to implement an end-to-end project version that isn’t the most accurate and iterate through it later. Thus, by using this architecture, you can easily add more data sources in future iterations to gather a larger dataset. More on LLM fine-tuning and dataset size will be covered in the next chapter.\n\nHow is the ETL process connected to the feature pipeline? The feature pipeline ingests the raw data from the MongoDB data warehouse, cleans it further, processes it into features, and stores it in the Qdrant vector DB to make it accessible for the LLM training and inference pipelines. Chapter 4 provides more information on the feature pipeline. The ETL process is independent of the feature pipeline. The two pipelines communicate with each other strictly through the MongoDB data warehouse. Thus, the data collection pipeline can write data for MongoDB, and the feature pipeline can read from it independently and on different schedules.\n\nWhy did we use MongoDB as a data warehouse? Using a transactional database, such as MongoDB, as a data warehouse is uncommon. However, in our use case, we are working with small amounts of data, which MongoDB can handle. Even if we plan to compute statistics on top of our MongoDB collections, it will work fine at the scale of our LLM Twin’s data (hundreds of documents). We picked MongoDB to store our raw data primarily because of the nature of our unstructured data: text crawled from the internet. By mainly working with unstructured text, selecting a NoSQL database that doesn’t enforce a schema made our development easier and faster. Also, MongoDB is stable and easy to use. Their Python SDK is intuitive. They provide a Docker image that works out of the box locally and a cloud freemium tier that is perfect for proofs of concept, such as the LLM Twin. Thus, we can freely work with it locally and in the cloud. However, when working with big data (millions of documents or more), using a dedicated data warehouse such as Snowflake or BigQuery will be ideal.\n\nNow that we’ve understood the architecture of the LLM Twin’s data collection pipeline, let’s move on to its implementation.\n\nOceanofPDF.com\n\nImplementing the LLM Twin’s data collection pipeline\n\nAs we presented in Chapter 2, the entry point to each pipeline from our LLM Twin project is a ZenML pipeline, which can be configured at runtime through YAML files and run through the ZenML ecosystem. Thus, let’s start by looking into the ZenML\n\ndigital_data_etl\n\npipeline. You’ll notice that this is the same pipeline we used as an example in Chapter 2 to illustrate ZenML. But this time, we will dig deeper into the implementation, explaining how the data collection works behind the scenes. After understanding how the pipeline works, we will explore the implementation of each crawler used to collect data from various sites and the MongoDB documents used to store and query data from the data warehouse.\n\nOceanofPDF.com",
      "page_number": 193
    },
    {
      "number": 4,
      "title": "provides more information on the feature pipeline. The ETL process is independent of the feature pipeline. The two pipelines communicate with each other strictly through the MongoDB data warehouse. Th",
      "start_page": 231,
      "end_page": 557,
      "detection_method": "regex_chapter",
      "content": "ZenML pipeline and steps\n\nIn the code snippet below, we can see the implementation of the ZenML\n\ndigital_data_etl\n\npipeline, which inputs the user’s full name and a list of links that will be crawled under that user (considered the author of the content extracted from those links). Within the function, we call two steps. In the first one, we look up the user in the database based on its full name. Then, we loop through all the links and crawl each independently. The pipeline’s implementation is available in our repository at\n\npipelines/digital_data_etl.py\n\n.\n\nfrom\n\nzenml\n\nimport\n\npipeline\n\nfrom\n\nsteps.etl\n\nimport\n\ncrawl_links, get_or_create_user\n\n@pipeline\n\ndef\n\ndigital_data_etl\n\n(\n\nuser_full_name:\n\nstr\n\n, links:\n\nlist\n\n[\n\nstr\n\n]\n\n) ->\n\nstr\n\n: user = get_or_create_user(user_full_name) last_step = crawl_links(user=user, links=links)\n\nreturn\n\nlast_step.invocation_id\n\nFigure 3.3 shows a run of the\n\ndigital_data_etl\n\npipeline on the ZenML dashboard. The next phase is to explore the\n\nget_or_create_user\n\nand\n\ncrawl_links\n\nZenML steps individually. The step implementation is available in our repository at\n\nsteps/etl\n\n.\n\nFigure 3.3: Example of a digital_data_etl pipeline run from ZenML’s dashboard\n\nWe will start with the\n\nget_or_create_user\n\nZenML step. We begin by importing the necessary modules and functions used throughout the script.\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\ntyping_extensions\n\nimport\n\nAnnotated\n\nfrom\n\nzenml\n\nimport\n\nget_step_context, step\n\nfrom\n\nllm_engineering.application\n\nimport\n\nutils\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nUserDocument\n\nNext, we define the function’s signature, which takes a user’s full name as input and retrieves an existing user or creates a new one in the MongoDB database if it doesn’t exist:\n\n@step\n\ndef\n\nget_or_create_user\n\n(\n\nuser_full_name:\n\nstr\n\n) -> Annotated[UserDocument,\n\n\"user\"\n\n]:\n\nUsing a utility function, we split the full name into first and last names. Then, we attempt to retrieve the user from the database or create a new one if it doesn’t exist. We also retrieve the current step context and add metadata about the user to the output, which will be reflected in the metadata of the\n\nuser\n\nZenML output artifact:\n\nlogger.info(\n\nf\"Getting or creating user:\n\n{user_full_name}\n\n\"\n\n) first_name, last_name = utils.split_user_full_name(user_full_name) user = UserDocument.get_or_create(first_name=first_name, last_name=last_name) step_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"user\"\n\n, metadata=_get_metadata(user_full_name, user))\n\nreturn\n\nuser\n\nAdditionally, we define a helper function called\n\n_get_metadata()\n\n, which builds a dictionary containing the query parameters and the retrieved user information, which will be added as metadata to the user artifact:\n\ndef\n\n_get_metadata\n\n(\n\nuser_full_name:\n\nstr\n\n, user: UserDocument\n\n) ->\n\ndict\n\n:\n\nreturn\n\n{\n\n\"query\"\n\n: {\n\n\"user_full_name\"\n\n: user_full_name, },\n\n\"retrieved\"\n\n: {\n\n\"user_id\"\n\n:\n\nstr\n\n(user.\n\nid\n\n),\n\n\"first_name\"\n\n: user.first_name,\n\n\"last_name\"\n\n: user.last_name, }, }\n\nWe will move on to the\n\ncrawl_links\n\nZenML step, which collects the data from the provided links. The code begins by importing essential modules and libraries for web crawling:\n\nfrom\n\nurllib.parse\n\nimport\n\nurlparse\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\ntqdm\n\nimport\n\ntqdm\n\nfrom\n\ntyping_extensions\n\nimport\n\nAnnotated\n\nfrom\n\nzenml\n\nimport\n\nget_step_context, step\n\nfrom\n\nllm_engineering.application.crawlers.dispatcher\n\nimport\n\nCrawlerDispatcher\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nUserDocument\n\nFollowing the imports, the main function inputs a list of links written by a specific author. Within this function, a crawler dispatcher is initialized and configured to handle specific domains such as LinkedIn, Medium, and GitHub:\n\n@step\n\ndef\n\ncrawl_links\n\n(\n\nuser: UserDocument, links:\n\nlist\n\n[\n\nstr\n\n]\n\n) -> Annotated[\n\nlist\n\n[\n\nstr\n\n],\n\n\"crawled_links\"\n\n]: dispatcher = CrawlerDispatcher.build().register_linkedin().register_medium().register_gi thub() logger.info(\n\nf\"Starting to crawl\n\n{\n\nlen\n\n(links)}\n\nlink(s).\"\n\n)\n\nThe function initializes variables to store the output metadata and count successful crawls. It then iterates over each link. It attempts to crawl and extract data for each link, updating the count of successful crawls and accumulating metadata about each URL:\n\nmetadata = {} successfull_crawls =\n\n0\n\nfor\n\nlink\n\nin\n\ntqdm(links): successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, user) successfull_crawls += successfull_crawl metadata = _add_to_metadata(metadata, crawled_domain, successfull_crawl)\n\nAfter processing all links, the function attaches the accumulated metadata to the output artifact:\n\nstep_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"crawled_links\"\n\n, metadata=metadata) logger.info(\n\nf\"Successfully crawled\n\n{successfull_crawls}\n\n/\n\n{\n\nlen\n\n(links)}\n\nlinks.\"\n\n)\n\nreturn\n\nlinks\n\nThe code includes a helper function that attempts to extract information from each link using the appropriate crawler based on the link’s domain. It handles any exceptions that may occur during extraction and returns a tuple indicating the crawl’s success and the link’s domain:\n\ndef\n\n_crawl_link\n\n(\n\ndispatcher: CrawlerDispatcher, link:\n\nstr\n\n, user: UserDocument\n\n) ->\n\ntuple\n\n[\n\nbool\n\n,\n\nstr\n\n]: crawler = dispatcher.get_crawler(link) crawler_domain = urlparse(link).netloc\n\ntry\n\n: crawler.extract(link=link, user=user)\n\nreturn\n\n(\n\nTrue\n\n, crawler_domain)\n\nexcept\n\nException\n\nas\n\ne: logger.error(\n\nf\"An error occurred while crawling:\n\n{e!s}\n\n\"\n\n)\n\nreturn\n\n(\n\nFalse\n\n, crawler_domain)\n\nAnother helper function is provided to update the metadata dictionary with the results of each crawl:\n\ndef\n\n_add_to_metadata\n\n(\n\nmetadata:\n\ndict\n\n, domain:\n\nstr\n\n, successfull_crawl:\n\nbool\n\n) ->\n\ndict\n\n:\n\nif\n\ndomain\n\nnot\n\nin\n\nmetadata: metadata[domain] = {} metadata[domain][\n\n\"successful\"\n\n] = metadata.get(domain, {}).get(\n\n\"successful\"\n\n,\n\n0\n\n) + successfull_crawl metadata[domain][\n\n\"total\"\n\n] = metadata.get(domain, {}).get(\n\n\"total\"\n\n,\n\n0\n\n) +\n\n1\n\nreturn\n\nmetadata\n\nAs seen in the abovementioned\n\n_crawl_link()\n\nfunction, the\n\nCrawlerDispatcher\n\nclass knows what crawler to initialize based on each link’s domain. The logic is then abstracted away under the crawler’s\n\nextract()\n\nmethod. Let’s zoom in on the\n\nCrawlerDispatcher\n\nclass to understand how this works fully.\n\nOceanofPDF.com\n\nThe dispatcher: How do you instantiate the right crawler?\n\nThe entry point to our crawling logic is the\n\nCrawlerDispatcher\n\nclass. As illustrated in Figure 3.4, the dispatcher acts as the intermediate layer between the provided links and the crawlers. It knows what crawler to associate with each URL.\n\nThe\n\nCrawlerDispatcher\n\nclass knows how to extract the domain of each link and initialize the proper crawler that collects the data from that site. For example, if it detects the https://medium.com domain when providing a link to an article, it will build an instance of the\n\nMediumCrawler\n\nused to crawl that particular platform. With that in mind, let’s explore the implementation of the\n\nCrawlerDispatcher\n\nclass.\n\nAll the crawling logic is available in the GitHub repository at\n\nllm_engineering/application/crawlers\n\n.\n\nFigure 3.4: The relationship between the provided links, the CrawlerDispatcher, and the crawlers\n\nWe begin by importing the necessary Python modules for URL handling and regex, along with importing our crawler classes:\n\nimport\n\nre\n\nfrom\n\nurllib.parse\n\nimport\n\nurlparse\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\n.base\n\nimport\n\nBaseCrawler\n\nfrom\n\n.custom_article\n\nimport\n\nCustomArticleCrawler\n\nfrom\n\n.github\n\nimport\n\nGithubCrawler\n\nfrom\n\n.linkedin\n\nimport\n\nLinkedInCrawler\n\nfrom\n\n.medium\n\nimport\n\nMediumCrawler\n\nThe\n\nCrawlerDispatcher\n\nclass is defined to manage and dispatch appropriate crawler instances based on given URLs and their domains. Its constructor initializes a registry to store the registered crawlers.\n\nclass\n\nCrawlerDispatcher\n\n:\n\ndef\n\n__init__\n\n(\n\nself\n\n) ->\n\nNone\n\n: self._crawlers = {}\n\nAs we are using the builder creational pattern to instantiate and configure the dispatcher, we define a\n\nbuild()\n\nclass method that returns an instance of the dispatcher:\n\n@classmethod\n\ndef\n\nbuild\n\n(\n\ncls\n\n) ->\n\n\"CrawlerDispatcher\"\n\n: dispatcher = cls()\n\nreturn\n\ndispatcher\n\nThe dispatcher includes methods to register crawlers for specific platforms like Medium, LinkedIn, and GitHub. These methods use a generic\n\nregister()\n\nmethod under the hood to add each crawler to the registry. By returning self, we follow the builder creational pattern (more on the builder pattern: https://refactoring.guru/design-patterns/builder). We can chain multiple\n\nregister_*()\n\nmethods when instantiating the dispatcher as follows:\n\nCrawlerDispatcher.build().register_linkedin().register_medium()\n\n.\n\ndef\n\nregister_medium\n\n(\n\nself\n\n) ->\n\n\"CrawlerDispatcher\"\n\n: self.register(\n\n\"https://medium.com\"\n\n, MediumCrawler)\n\nreturn\n\nself\n\ndef\n\nregister_linkedin\n\n(\n\nself\n\n) ->\n\n\"CrawlerDispatcher\"\n\n: self.register(\n\n\"https://linkedin.com\"\n\n, LinkedInCrawler)\n\nreturn\n\nself\n\ndef\n\nregister_github\n\n(\n\nself\n\n) ->\n\n\"CrawlerDispatcher\"\n\n: self.register(\n\n\"https://github.com\"\n\n, GithubCrawler)\n\nreturn\n\nself\n\nThe generic\n\nregister()\n\nmethod normalizes each domain to ensure its format is consistent before it’s added as a key to the\n\nself._crawlers\n\nregistry of the dispatcher. This is a critical step, as we will use the key of the dictionary as the domain pattern to match future links with a crawler:\n\ndef\n\nregister\n\n(\n\nself, domain:\n\nstr\n\n, crawler:\n\ntype\n\n[BaseCrawler]\n\n) ->\n\nNone\n\n: parsed_domain = urlparse(domain) domain = parsed_domain.netloc self._crawlers[\n\nr\"https://(www\\.)?{}/*\"\n\n.\n\nformat\n\n(re.escape(domain))] = crawler\n\nFinally, the\n\nget_crawler()\n\nmethod determines the appropriate crawler for a given URL by matching it against the registered domains. If no match is found, it logs a warning and defaults to using the\n\nCustomArticleCrawler\n\n.\n\ndef\n\nget_crawler\n\n(\n\nself, url:\n\nstr\n\n) -> BaseCrawler:\n\nfor\n\npattern, crawler\n\nin\n\nself._crawlers.items():\n\nif\n\nre.\n\nmatch\n\n(pattern, url):\n\nreturn\n\ncrawler()\n\nelse\n\n: logger.warning(\n\nf\"No crawler found for\n\n{url}\n\n. Defaulting to CustomArticleCrawler.\"\n\n)\n\nreturn\n\nCustomArticleCrawler()\n\nThe next step in understanding how the data collection pipeline works is analyzing each crawler individually.\n\nOceanofPDF.com\n\nThe crawlers\n\nBefore exploring each crawler’s implementation, we must present their base class, which defines a unified interface for all the crawlers. As shown in Figure 3.4, we can implement the dispatcher layer because each crawler follows the same signature. Each class implements the\n\nextract()\n\nmethod, allowing us to leverage OOP techniques such as polymorphism, where we can work with abstract objects without knowing their concrete subclass. For example, in the\n\n_crawl_link()\n\nfunction from the ZenML steps, we had the following code:\n\ncrawler = dispatcher.get_crawler(link) crawler.extract(link=link, user=user)\n\nNote how we called the\n\nextract()\n\nmethod without caring about what specific type of crawler we instantiated. To conclude, working with abstract interfaces ensures core reusability and ease of extension.\n\nOceanofPDF.com\n\nBase classes\n\nNow, let’s explore the\n\nBaseCrawler\n\ninterface, which can be found in the repository at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/crawlers/base.py.\n\nfrom\n\nabc\n\nimport\n\nABC, abstractmethod\n\nclass\n\nBaseCrawler\n\n(\n\nABC\n\n): model:\n\ntype\n\n[NoSQLBaseDocument]\n\n@abstractmethod\n\ndef\n\nextract\n\n(\n\nself, link:\n\nstr\n\n, **kwargs\n\n) ->\n\nNone\n\n: ...\n\nAs mentioned above, the interface defines an\n\nextract()\n\nmethod that takes as input a link. Also, it defines a model attribute at the class level that represents the data category document type used to save the extracted data into the MongoDB data warehouse. Doing so allows us to customize each subclass with different data categories while preserving the same attributes at the class level. We will soon explore the\n\nNoSQLBaseDocument\n\nclass when digging into the document entities.\n\nWe also extend the\n\nBaseCrawler\n\nclass with a\n\nBaseSeleniumCrawler\n\nclass, which implements reusable functionality that uses Selenium to crawl various sites, such as Medium or LinkedIn. Selenium is a tool for automating web browsers. It’s used to interact with web pages programmatically (like logging into LinkedIn, navigating through profiles, etc.).\n\nSelenium can programmatically control various browsers such as Chrome, Firefox, or Brave. For these specific platforms, we need Selenium to manipulate the browser programmatically to log in and scroll through the newsfeed or article before being able to extract the entire HTML. For other sites, where we don’t have to go through the login step or can directly load the whole page, we can extract the HTML from a particular URL using more straightforward methods than Selenium.\n\nFor the Selenium-based crawlers to work, you must install Chrome on your machine (or a Chromium-based browser such as Brave).\n\nThe code begins by setting up the necessary imports and configurations for web crawling using Selenium and the ChromeDriver initializer. The\n\nchromedriver_autoinstaller\n\nensures that the appropriate version of ChromeDriver is installed and added to the system path, maintaining compatibility with the installed version of your Google Chrome browser (or other Chromium-based browser). Selenium will use the ChromeDriver to communicate with the browser and open a headless session, where we can programmatically manipulate the browser to access various URLs, click on specific elements, such as buttons, or scroll through the newsfeed. Using the\n\nchromedriver_autoinstaller\n\n, we ensure we always have the correct ChromeDriver version installed that matches our machine’s Chrome browser version.\n\nimport\n\ntime\n\nfrom\n\ntempfile\n\nimport\n\nmkdtemp\n\nimport\n\nchromedriver_autoinstaller\n\nfrom\n\nselenium\n\nimport\n\nwebdriver\n\nfrom\n\nselenium.webdriver.chrome.options\n\nimport\n\nOptions\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nNoSQLBaseDocument\n\n# Check if the current version of chromedriver exists\n\n# and if it doesn't exist, download it automatically,\n\n# then add chromedriver to path\n\nchromedriver_autoinstaller.install()\n\nNext, we define the\n\nBaseSeleniumCrawler\n\nclass for use cases where we need Selenium to collect the data, such as collecting data from Medium or LinkedIn.\n\nIts constructor initializes various Chrome options to optimize performance, enhance security, and ensure a headless browsing environment. These options disable unnecessary features like GPU rendering, extensions, and notifications, which can interfere with automated browsing. These are standard configurations when crawling in headless mode:\n\nclass\n\nBaseSeleniumCrawler\n\n(BaseCrawler, ABC):\n\ndef\n\n__init__\n\n(\n\nself, scroll_limit:\n\nint\n\n=\n\n5\n\n) ->\n\nNone\n\n: options = webdriver.ChromeOptions() options.add_argument(\n\n\"--no-sandbox\"\n\n) options.add_argument(\n\n\"\n\n--headless=new\"\n\n) options.add_argument(\n\n\"--disable-dev-shm-usage\"\n\n) options.add_argument(\n\n\"--log-level=3\"\n\n) options.add_argument(\n\n\"--disable-popup-blocking\"\n\n) options.add_argument(\n\n\"--disable-notifications\"\n\n) options.add_argument(\n\n\"--disable-extensions\"\n\n) options.add_argument(\n\n\"--disable-background-networking\"\n\n) options.add_argument(\n\n\"--ignore-certificate-errors\"\n\n) options.add_argument(\n\nf\"--user-data-dir=\n\n{mkdtemp()}\n\n\"\n\n) options.add_argument(\n\nf\"--data-path=\n\n{mkdtemp()}\n\n\"\n\n) options.add_argument(\n\nf\"--disk-cache-dir=\n\n{mkdtemp()}\n\n\"\n\n) options.add_argument(\n\n\"--remote-debugging-port=9226\"\n\n)\n\nAfter configuring the Chrome options, the code allows subclasses to set any additional driver options by calling the\n\nset_extra_driver_options()\n\nmethod. It then initializes the scroll limit and creates a new instance of the Chrome driver with the specified options:\n\nself.set_extra_driver_options(options) self.scroll_limit = scroll_limit self.driver = webdriver.Chrome( options=options, )\n\nThe\n\nBaseSeleniumCrawler\n\nclass includes placeholder methods for\n\nset_extra_driver_options()\n\nand\n\nlogin()\n\n, which subclasses can override to provide specific functionality. This ensures modularity, as every platform has a different login page with a different HTML structure:\n\ndef\n\nset_extra_driver_options\n\n(\n\nself, options: Options\n\n) ->\n\nNone\n\n:\n\npass\n\ndef\n\nlogin\n\n(\n\nself\n\n) ->\n\nNone\n\n:\n\npass\n\nFinally, the\n\nscroll_page()\n\nmethod implements a scrolling mechanism to navigate through pages, such as LinkedIn, up to a specified scroll limit. It scrolls to the bottom of the page, waits for new content to load, and repeats the process until it reaches the end of the page or the scroll limit is exceeded. This method is essential for feeds where the content appears as the user scrolls:\n\ndef\n\nscroll_page\n\n(\n\nself\n\n) ->\n\nNone\n\n:\n\n\"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"\n\ncurrent_scroll =\n\n0\n\nlast_height = self.driver.execute_script(\n\n\"return document.body.scrollHeight\"\n\n)\n\nwhile\n\nTrue\n\n: self.driver.execute_script(\n\n\"window.scrollTo(0, document.body.scrollHeight);\"\n\n) time.sleep(\n\n5\n\n) new_height = self.driver.execute_script(\n\n\"return document.body.scrollHeight\"\n\n)\n\nif\n\nnew_height == last_height\n\nor\n\n(self.scroll_limit\n\nand\n\ncurrent_scroll >= self.scroll_limit):\n\nbreak\n\nlast_height = new_height current_scroll +=\n\n1\n\nWe’ve understood what the base classes of our crawlers look like. Next, we will look into the implementation of the following specific crawlers:\n\nGitHubCrawler(BaseCrawler)\n\nCustomArticleCrawler(BaseCrawler)\n\nMediumCrawler(BaseSeleniumCrawler)\n\nYou can find the implementation of the above crawlers in the GitHub repository at https://github.com/PacktPublishing/LLM-Engineers- Handbook/tree/main /llm_engineering/application/crawlers.\n\nOceanofPDF.com\n\nGitHubCrawler class\n\nThe\n\nGithubCrawler\n\nclass is designed to scrape GitHub repositories, extending the functionality of the\n\nBaseCrawler\n\n. We don’t have to log in to GitHub through the browser, as we can leverage Git’s clone functionality. Thus, we don’t have to leverage any Selenium functionality. Upon initialization, it sets up a list of patterns to ignore standard files and directories found in GitHub repositories, such as\n\n.git\n\n,\n\n.toml\n\n,\n\n.lock\n\n, and\n\n.png\n\n, ensuring that unnecessary files are excluded from the scraping process:\n\nclass\n\nGithubCrawler\n\n(\n\nBaseCrawler\n\n): model = RepositoryDocument\n\ndef\n\n__init__\n\n(\n\nself, ignore=(\n\n\".git\"\n\n,\n\n\".toml\"\n\n,\n\n\".lock\"\n\n,\n\n\".png\"\n\n)\n\n) ->\n\nNone\n\n:\n\nsuper\n\n().__init__() self._ignore = ignore\n\nNext, we implement the\n\nextract()\n\nmethod, where the crawler first checks if the repository has already been processed and stored in the database. If it exists, it exits the method to prevent storing duplicates:\n\ndef\n\nextract\n\n(\n\nself, link:\n\nstr\n\n, **kwargs\n\n) ->\n\nNone\n\n: old_model = self.model.find(link=link)\n\nif\n\nold_model\n\nis\n\nnot\n\nNone\n\n: logger.info(\n\nf\"Repository already exists in the database:\n\n{link}\n\n\"\n\n)\n\nreturn\n\nIf the repository is new, the crawler extracts the repository name from the link. Then, it creates a temporary directory to clone the repository to ensure that the cloned repository is cleaned up from the local disk after it’s processed:\n\nlogger.info(\n\nf\"Starting scrapping GitHub repository:\n\n{link}\n\n\"\n\n) repo_name = link.rstrip(\n\n\"/\"\n\n).split(\n\n\"/\"\n\n)[-\n\n1\n\n] local_temp = tempfile.mkdtemp()\n\nWithin a try block, the crawler changes the current working directory to the\n\ntemporary\n\ndirectory and executes the\n\ngit clone\n\ncommand in a different process:\n\ntry\n\n: os.chdir(local_temp) subprocess.run([\n\n\"git\"\n\n,\n\n\"clone\"\n\n, link])\n\nAfter successfully cloning the repository, the crawler constructs the path to the cloned repository. It initializes an empty dictionary used to aggregate the content of the files in a standardized way. It walks through the directory tree, skipping over any directories or files that match the ignore patterns. For each relevant file, it reads the content, removes any spaces, and stores it in the dictionary with the file path as the key:\n\nrepo_path = os.path.join(local_temp, os.listdir(local_temp)[\n\n0\n\n])\n\n#\n\ntree = {}\n\nfor\n\nroot, _, files\n\nin\n\nos.walk(repo_path):\n\ndir\n\n= root.replace(repo_path,\n\n\"\"\n\n).lstrip(\n\n\"/\"\n\n)\n\nif\n\ndir\n\n.startswith(self._ignore):\n\ncontinue\n\nfor\n\nfile\n\nin\n\nfiles:\n\nif\n\nfile.endswith(self._ignore):\n\ncontinue\n\nfile_path = os.path.join(\n\ndir\n\n, file)\n\nwith\n\nopen\n\n(os.path.join(root, file),\n\n\"r\"\n\n, errors=\n\n\"ignore\"\n\n)\n\nas\n\nf: tree[file_path] = f.read().replace(\n\n\" \"\n\n,\n\n\"\"\n\n)\n\nIt then creates a new instance of the\n\nRepositoryDocument\n\nmodel, populating it with the repository content, name, link, platform information, and author details. The instance is then saved to MongoDB:\n\nuser = kwargs[\n\n\"user\"\n\n] instance = self.model( content=tree, name=repo_name, link=link, platform=\n\n\"github\"\n\n, author_id=user.\n\nid\n\n, author_full_name=user.full_name, ) instance.save()\n\nFinally, whether the scraping succeeds or an exception occurs, the crawler ensures that the temporary directory is removed to clean up any resources used during the process:\n\nexcept\n\nException:\n\nraise\n\nfinally\n\n: shutil.rmtree(local_temp) logger.info(\n\nf\"Finished scrapping GitHub repository:\n\n{link}\n\n\"\n\n)\n\nOceanofPDF.com\n\nCustomArticleCrawler class\n\nThe\n\nCustomArticleCrawler\n\nclass takes a different approach to collecting data from the internet. It leverages the\n\nAsyncHtmlLoader\n\nclass to read the entire HTML from a link and the\n\nHtml2TextTransformer\n\nclass to extract the text from that HTML. Both classes are made available by the\n\nlangchain_community\n\nPython package, as seen below, where we import all the necessary Python modules:\n\nfrom\n\nurllib.parse\n\nimport\n\nurlparse\n\nfrom\n\nlangchain_community.document_loaders\n\nimport\n\nAsyncHtmlLoader\n\nfrom\n\nlangchain_community.document_transformers.html2text\n\nimport\n\nHtml2TextTransformer\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nArticleDocument\n\nfrom\n\n.base\n\nimport\n\nBaseCrawler\n\nNext, we define the\n\nCustomArticleCrawler\n\nclass, which inherits from\n\nBaseCrawler\n\n. As before, we don’t need to log in or use the scrolling functionality provided by Selenium. In the\n\nextract\n\nmethod, we first check if the article exists in the database to avoid duplicating content:\n\nclass\n\nCustomArticleCrawler\n\n(\n\nBaseCrawler\n\n): model = ArticleDocument\n\ndef\n\nextract\n\n(\n\nself, link:\n\nstr\n\n, **kwargs\n\n) ->\n\nNone\n\n: old_model = self.model.find(link=link)\n\nif\n\nold_model\n\nis\n\nnot\n\nNone\n\n: logger.info(\n\nf\"Article already exists in the database:\n\n{link}\n\n\"\n\n)\n\nreturn\n\nIf the article doesn’t exist, we proceed to scrape it. We use the\n\nAsyncHtmlLoader\n\nclass to load the HTML from the provided link. After, we transform it into plain text using the\n\nHtml2TextTransformer\n\nclass, which returns a list of documents. We are only interested in the first document. As we delegate the whole logic to these two classes, we don’t control how the content is extracted and parsed. That’s why we used this class as a fallback system for domains where we don’t have anything custom implemented. These two classes follow the LangChain paradigm, which provides high-level functionality that works decently in most scenarios. It is fast to implement but hard to customize. That is one of the reasons why many developers avoid using LangChain in production use cases:\n\nlogger.info(\n\nf\"Starting scrapping article:\n\n{link}\n\n\"\n\n) loader = AsyncHtmlLoader([link]) docs = loader.load() html2text = Html2TextTransformer() docs_transformed = html2text.transform_documents(docs) doc_transformed = docs_transformed[\n\n0\n\n]\n\nWe get the page content from the extracted document, plus relevant metadata such as the\n\ntitle\n\n,\n\nsubtitle\n\n,\n\ncontent\n\n, and\n\nlanguage\n\n:\n\ncontent = {\n\n\"Title\"\n\n: doc_transformed.metadata.get(\n\n\"title\"\n\n),\n\n\"Subtitle\"\n\n: doc_transformed.metadata.get(\n\n\"\n\ndescription\"\n\n),\n\n\"Content\"\n\n: doc_transformed.page_content,\n\n\"language\"\n\n: doc_transformed.metadata.get(\n\n\"language\"\n\n), }\n\nNext, we parse the URL to determine the platform (or domain) from which the article was scraped:\n\nparsed_url = urlparse(link) platform = parsed_url.netloc\n\nWe then create a new instance of the article model, populating it with the extracted content. Finally, we save this instance to the MongoDB data\n\nwarehouse:\n\nuser = kwargs[\n\n\"user\"\n\n] instance = self.model( content=content, link=link, platform=platform, author_id=user.\n\nid\n\n, author_full_name=user.full_name, ) instance.save() logger.info(\n\nf\"Finished scrapping custom article:\n\n{link}\n\n\"\n\n)\n\nSo far, we have seen how to crawl GitHub repositories and random sites using LangChain utility functions. Lastly, we must explore a crawler using Selenium to manipulate the browser programmatically. Thus, we will continue with the\n\nMediumCrawler\n\nimplementation.\n\nOceanofPDF.com\n\nMediumCrawler class\n\nThe code begins by importing essential libraries and defining the\n\nMediumCrawler\n\nclass, which inherits from\n\nBaseSeleniumCrawler\n\n:\n\nfrom\n\nbs4\n\nimport\n\nBeautifulSoup\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nArticleDocument\n\nfrom\n\n.base\n\nimport\n\nBaseSeleniumCrawler\n\nclass\n\nMediumCrawler\n\n(\n\nBaseSeleniumCrawler\n\n): model = ArticleDocument\n\nWithin the\n\nMediumCrawler\n\nclass, we leverage the\n\nset_extra_driver_options()\n\nmethod to extend the default driver options used by Selenium:\n\ndef\n\nset_extra_driver_options\n\n(\n\nself, options\n\n) ->\n\nNone\n\n: options.add_argument(\n\nr\"--profile-directory=Profile 2\"\n\n)\n\nThe\n\nextract()\n\nmethod implements the core functionality, first checking whether the article exists in the database to prevent duplicate entries.\n\nIf the article is new, the method proceeds to navigate to the article’s link and scroll through the page to ensure all content is loaded:\n\ndef\n\nextract\n\n(\n\nself, link:\n\nstr\n\n, **kwargs\n\n) ->\n\nNone\n\n: old_model = self.model.find(link=link)\n\nif\n\nold_model\n\nis\n\nnot\n\nNone\n\n: logger.info(\n\nf\"Article already exists in the database:\n\n{link}\n\n\"\n\n)\n\nreturn\n\nlogger.info(\n\nf\"Starting scrapping Medium article:\n\n{link}\n\n\"\n\n) self.driver.get(link) self.scroll_page()\n\nAfter fully loading the page, the method uses\n\nBeautifulSoup\n\nto parse the HTML content and extract the article’s title, subtitle, and full text.\n\nBeautifulSoup\n\nis a popular Python library for web scraping and parsing HTML or XML documents. Thus, we used it to extract all the HTML elements we needed from the HTML accessed with Selenium. Finally, we aggregate everything into a dictionary:\n\nsoup = BeautifulSoup(self.driver.page_source,\n\n\"html.parser\"\n\n) title = soup.find_all(\n\n\"h1\"\n\n, class_=\n\n\"pw-post-title\"\n\n) subtitle = soup.find_all(\n\n\"h2\"\n\n, class_=\n\n\"pw-subtitle-paragraph\"\n\n) data = {\n\n\"Title\"\n\n: title[\n\n0\n\n].string\n\nif\n\ntitle\n\nelse\n\nNone\n\n,\n\n\"Subtitle\"\n\n: subtitle[\n\n0\n\n].string\n\nif\n\nsubtitle\n\nelse\n\nNone\n\n,\n\n\"Content\"\n\n: soup.get_text(), }\n\nFinally, the method closes the WebDriver to free up resources. It then creates a new\n\nArticleDocument\n\ninstance, populates it with the extracted content and user information provided via\n\nkwargs\n\n, and saves it to the database:\n\nself.driver.close() user = kwargs[\n\n\"user\"\n\n] instance = self.model( platform=\n\n\"medium\"\n\n, content=data, link=link, author_id=user.\n\nid\n\n, author_full_name=user.full_name, ) instance.save() logger.info(\n\nf\"Successfully scraped and saved article:\n\n{link}\n\n\"\n\n)\n\nWith that, we conclude the\n\nMediumCrawler\n\nimplementation. The LinkedIn crawler follows a similar pattern to the Medium one, where it uses Selenium to log in and access the feed of a user’s latest posts. Then, it extracts the posts and scrolls through the feed to load the next page until a limit is hit. You can check the full implementation in our repository at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/crawlers/linkedin.py.\n\nWith the rise of LLMs, collecting data from the internet has become a critical step in many real-world AI applications. Hence, more high-level tools have appeared in the Python ecosystem, such as Scrapy (https://github.com/scrapy/scrapy), which crawls websites and extracts structured data from their pages, and Crawl4AI (https://github.com/unclecode/crawl4ai), which is highly specialized in crawling data for LLMs and AI applications.\n\nIn this section, we’ve looked at implementing three types of crawlers: one that leverages the\n\ngit\n\nexecutable in a subprocess to clone GitHub repositories, one that uses LangChain utilities to extract the HTML of a single web page, and one that leverages Selenium for more complex scenarios where we have to navigate through the login page, scroll the article to load the entire HTML, and extract it into text format. The last step is understanding how the document classes we’ve used across the chapter, such as the\n\nArticleDocument\n\n, work.\n\nOceanofPDF.com\n\nThe NoSQL data warehouse documents\n\nWe had to implement three document classes to structure our data categories. These classes define the specific attributes we require for a document, such as the content, author, and source link. It is best practice to structure your data in classes instead of dictionaries, as the attributes we expect for each item are more verbose, reducing run errors. For example, when accessing a value from a Python dictionary, we can never be sure it is present or its type is current. By wrapping our data items with classes, we can ensure each attribute is as expected.\n\nBy leveraging Python packages such as Pydantic, we have out-of-the-box type validation, which ensures consistency in our datasets. Thus, we modeled the data categories as the following document classes, which we already used in the code up until point:\n\nArticleDocument\n\nclass\n\nPostDocument\n\nclass\n\nRepositoryDocument\n\nclass\n\nThese are not simple Python data classes or Pydantic models. They support read and write operations on top of the MongoDB data warehouse. To inject the read-and-write functionality into all the document classes without repeating any code, we used the Object- Document Mapping (ODM) software pattern, which is based on the object-relational mapping (ORM) pattern. Thus, let’s first explore ORM, then move to ODM, and, finally, dig into our custom ODM implementation and document classes.\n\nOceanofPDF.com\n\nThe ORM and ODM software patterns\n\nBefore we talk about software patterns, let’s see what ORM is. It’s a technique that lets you query and manipulate data from a database using an object-oriented paradigm. Instead of writing SQL or API-specific queries, you encapsulate all the complexity under an ORM class that knows how to handle all the database operations, most commonly CRUD operations. Thus, working with ORM removes the need to handle the database operations manually and reduces the need to write boilerplate code manually. An ORM interacts with a SQL database, such as PostgreSQL or MySQL.\n\nMost modern Python applications use ORMs when interacting with the database. Even though SQL is still a popular choice in the data world, you rarely see raw SQL queries in Python backend components. The most popular Python ORM is SQLAlchemy (https://www.sqlalchemy.org/). Also, with the rise of FastAPI, SQLModel is (https://github.com/fastapi/sqlmodel) a common choice, which is a wrapper over SQLAlchemy that makes the integration easier with FastAPI.\n\nFor example, using SQLAlchemy, we defined a\n\nUser\n\nORM with the ID and name fields. The\n\nUser\n\nORM is mapped to the\n\nusers\n\ntable within the SQL database. Thus, when we create a new user and commit it to the database, it is automatically saved to the\n\nusers\n\ntable. The same applies to all the CRUD operations on top of the\n\nUser\n\nclass.\n\nfrom\n\nsqlalchemy\n\nimport\n\nColumn, Integer, String, create_engine\n\nfrom\n\nsqlalchemy.orm\n\nimport\n\ndeclarative_base, sessionmaker Base = declarative_base()\n\n# Define a class that maps to the users table.\n\nclass\n\nUser\n\n(\n\nBase\n\n): __tablename__ =\n\n\"users\"\n\nid\n\n= Column(Integer, primary_key=\n\nTrue\n\n) name = Column(String)\n\nUsing the\n\nUser\n\nORM, we can quickly insert or query users directly from Python without writing a line of SQL. Note that an ORM usually supports all CRUD operations. Here is a code snippet that shows how to save an instance of the User ORM to a SQLite database:\n\nengine = create_engine(\n\n\"sqlite:///:memory:\"\n\n) Base.metadata.create_all(engine)\n\n# Create a session used to interact with the database.\n\nSession = sessionmaker(bind=engine) session = Session()\n\n# Add a new user.\n\nnew_user = User(name=\n\n\"Alice\"\n\n) session.add(new_user) session.commit()\n\nAlso, this is how we can query a user from the\n\nusers\n\nSQLite table:\n\nuser = session.query(User).first()\n\nif\n\nuser:\n\nprint\n\n(\n\nf\"User ID:\n\n{user.\n\nid\n\n}\n\n\"\n\n)\n\nprint\n\n(\n\nf\"User name:\n\n{user.name}\n\n\"\n\n)\n\nFind the entire script and how to run it in the GitHub repository at\n\ncode_snippets/03_orm.py\n\n.\n\nThe ODM pattern is extremely similar to ORM, but instead of working with SQL databases and tables, it works with NoSQL databases (such as MongoDB) and unstructured collections. As we work with NoSQL databases, the data structure is centered on collections, which store JSON- like documents rather than rows in tables.\n\nTo conclude, ODM simplifies working with document-based NoSQL databases and maps object-oriented code to JSON-like documents. We will implement a light ODM module on top of MongoDB to fully understand how ODM works.\n\nOceanofPDF.com\n\nImplementing the ODM class\n\nThis section will explore how to implement an ODM class from scratch. This is an excellent exercise to learn how ODM works and sharpen our skills in writing modular and reusable Python classes. Hence, we will implement a base ODM class called\n\nNoSQLBaseDocument\n\n, from which all the other documents will inherit to interact with the MongoDB data warehouse.\n\nThe class can be found in our repository at\n\nllm_engineering/domain/base/nosql.py\n\n.\n\nThe code starts by importing essential modules and setting up the database connection. Through the\n\n_database\n\nvariable, we establish a connection to the database specified in the settings, which is by default called\n\ntwin\n\n:\n\nimport\n\nuuid\n\nfrom\n\nabc\n\nimport\n\nABC\n\nfrom\n\ntyping\n\nimport\n\nGeneric\n\n,\n\nType\n\n, TypeVar\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\npydantic\n\nimport\n\nUUID4, BaseModel, Field\n\nfrom\n\npymongo\n\nimport\n\nerrors\n\nfrom\n\nllm_engineering.domain.exceptions\n\nimport\n\nImproperlyConfigured\n\nfrom\n\nllm_engineering.infrastructure.db.mongo\n\nimport\n\nconnection\n\nfrom\n\nllm_engineering.settings\n\nimport\n\nsettings _database = connection.get_database(settings.DATABASE_NAME)\n\nNext, we define a type variable\n\nT\n\nbound to the\n\nNoSQLBaseDocument\n\nclass. The variable leverages Python’s generic module, allowing us to generalize the class’s types. For example, when we implement the\n\nArticleDocument\n\nclass, which will inherit from the\n\nNoSQLBaseDocument\n\nclass, all the instances where\n\nT\n\nwas used will be replaced with the\n\nArticleDocument\n\ntype when analyzing the signature of functions (more on Python generics: https://realpython.com/python312-typing).\n\nThe\n\nNoSQLBaseDocument\n\nclass is then declared as an abstract base class inheriting from Pydantic’s BaseModel, Python’s Generic (which provides the functionality described earlier), and\n\nABC\n\n(making the class abstract) classes. This class serves as the foundational ODM class:\n\nT = TypeVar(\n\n\"T\"\n\n, bound=\n\n\"NoSQLBaseDocument\"\n\n)\n\nclass\n\nNoSQLBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC):\n\nWithin the\n\nNoSQLBaseDocument\n\nclass, an id field is defined as a UUID4, with a default factory generating a unique UUID. The class also implements the\n\n__eq__\n\nand\n\n__hash__\n\nmethods to allow instances to be compared and used in hashed collections like sets or as dictionary keys based on their unique\n\nid\n\nattribute:\n\nid\n\n: UUID4 = Field(default_factory=uuid.uuid4)\n\ndef\n\n__eq__\n\n(\n\nself, value:\n\nobject\n\n) ->\n\nbool\n\n:\n\nif\n\nnot\n\nisinstance\n\n(value, self.__class__):\n\nreturn\n\nFalse\n\nreturn\n\nself.\n\nid\n\n== value.\n\nid\n\ndef\n\n__hash__\n\n(\n\nself\n\n) ->\n\nint\n\n:\n\nreturn\n\nhash\n\n(self.\n\nid\n\n)\n\nThe class provides methods for converting between MongoDB documents and class instances. The\n\nfrom_mongo()\n\nclass method transforms a dictionary retrieved from MongoDB into an instance of the class. The\n\nto_mongo()\n\ninstance method converts the model instance into a dictionary suitable for MongoDB insertion:\n\n@classmethod\n\ndef\n\nfrom_mongo\n\n(\n\ncls:\n\nType\n\n[T], data:\n\ndict\n\n) -> T:\n\nif\n\nnot\n\ndata:\n\nraise\n\nValueError(\n\n\"Data is empty.\"\n\n)\n\nid\n\n= data.pop(\n\n\"_id\"\n\n)\n\nreturn\n\ncls(**\n\ndict\n\n(data,\n\nid\n\n=\n\nid\n\n))\n\ndef\n\nto_mongo\n\n(\n\nself: T, **kwargs\n\n) ->\n\ndict\n\n: exclude_unset = kwargs.pop(\n\n\"exclude_unset\"\n\n,\n\nFalse\n\n) by_alias = kwargs.pop(\n\n\"by_alias\"\n\n,\n\nTrue\n\n) parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)\n\nif\n\n\"_id\"\n\nnot\n\nin\n\nparsed\n\nand\n\n\"id\"\n\nin\n\nparsed: parsed[\n\n\"_id\"\n\n] =\n\nstr\n\n(parsed.pop(\n\n\"id\"\n\n))\n\nfor\n\nkey, value\n\nin\n\nparsed.items():\n\nif\n\nisinstance\n\n(value, uuid.UUID): parsed[key] =\n\nstr\n\n(value)\n\nreturn\n\nparsed\n\nThe\n\nsave()\n\nmethod allows an instance of the model to be inserted into a MongoDB collection. It retrieves the appropriate collection, converts the instance into a MongoDB-compatible document leveraging the\n\nto_mongo()\n\nmethod described above, and attempts to insert it into the database, handling any write errors that may occur:\n\ndef\n\nsave\n\n(\n\nself: T, **kwargs\n\n) -> T |\n\nNone\n\n: collection = _database[self.get_collection_name()]\n\ntry\n\n: collection.insert_one(self.to_mongo(**kwargs))\n\nreturn\n\nself\n\nexcept\n\nerrors.WriteError: logger.exception(\n\n\"\n\nFailed to insert document.\"\n\n)\n\nreturn\n\nNone\n\nThe\n\nget_or_create()\n\nclass method attempts to find a document in the database matching the provided filter options. If a matching document is found, it is converted into an instance of the class. If not, a new instance is created with the filter options as its initial data and saved to the database:\n\n@classmethod\n\ndef\n\nget_or_create\n\n(\n\ncls:\n\nType\n\n[T], **filter_options\n\n) -> T: collection = _database[cls.get_collection_name()]\n\ntry\n\n: instance = collection.find_one(filter_options)\n\nif\n\ninstance:\n\nreturn\n\ncls.from_mongo(instance) new_instance = cls(**filter_options) new_instance = new_instance.save()\n\nreturn\n\nnew_instance\n\nexcept\n\nerrors.OperationFailure: logger.exception(\n\nf\"Failed to retrieve document with filter options:\n\n{filter_options}\n\n\"\n\n)\n\nraise\n\nThe\n\nbulk_insert()\n\nclass method allows multiple documents to be inserted into the database at once:\n\n@classmethod\n\ndef\n\nbulk_insert\n\n(\n\ncls:\n\nType\n\n[T], documents:\n\nlist\n\n[T], **kwargs\n\n) ->\n\nbool\n\n: collection = _database[cls.get_collection_name()]\n\ntry\n\n: collection.insert_many([doc.to_mongo(**kwargs)\n\nfor\n\ndoc\n\nin\n\ndocuments])\n\nreturn\n\nTrue\n\nexcept\n\n(errors.WriteError, errors.BulkWriteError): logger.error(\n\nf\"Failed to insert documents of type\n\n{cls.__name__}\n\n\"\n\n)\n\nreturn\n\nFalse\n\nThe\n\nfind()\n\nclass method searches for a single document in the database that matches the given filter options:\n\n@classmethod\n\ndef\n\nfind\n\n(\n\ncls:\n\nType\n\n[T], **filter_options\n\n) -> T |\n\nNone\n\n: collection = _database[cls.get_collection_name()]\n\ntry\n\n: instance = collection.find_one(filter_options)\n\nif\n\ninstance:\n\nreturn\n\ncls.from_mongo(instance)\n\nreturn\n\nNone\n\nexcept\n\nerrors.OperationFailure: logger.error(\n\n\"Failed to retrieve document.\"\n\n)\n\nreturn\n\nNone\n\nSimilarly, the\n\nbulk_find()\n\nclass method retrieves multiple documents matching the filter options. It converts each retrieved MongoDB document into a model instance, collecting them into a list:\n\n@classmethod\n\ndef\n\nbulk_find\n\n(\n\ncls:\n\nType\n\n[T], **filter_options\n\n) ->\n\nlist\n\n[T]: collection = _database[cls.get_collection_name()]\n\ntry\n\n: instances = collection.find(filter_options)\n\nreturn\n\n[document\n\nfor\n\ninstance\n\nin\n\ninstances\n\nif\n\n(document := cls.from_mongo(instance))\n\nis\n\nnot\n\nNone\n\n]\n\nexcept\n\nerrors.OperationFailure: logger.error(\n\n\"Failed to retrieve document.\"\n\n)\n\nreturn\n\n[]\n\nFinally, the\n\nget_collection_name()\n\nclass method determines the name of the MongoDB collection associated with the class. It expects the class to have a nested\n\nSettings\n\nclass with a name attribute specifying the collection name. If this configuration is missing, an\n\nImproperlyConfigured\n\nexception will be raised specifying that the subclass should define a nested\n\nSettings\n\nclass:\n\n@classmethod\n\ndef\n\nget_collection_name\n\n(\n\ncls:\n\nType\n\n[T]\n\n) ->\n\nstr\n\n:\n\nif\n\nnot\n\nhasattr\n\n(cls,\n\n\"Settings\"\n\n)\n\nor\n\nnot\n\nhasattr\n\n(cls.Settings,\n\n\"name\"\n\n):\n\nraise\n\nImproperlyConfigured(\n\n\"Document should define an Settings configuration class with the name of the collection.\"\n\n)\n\nreturn\n\ncls.Settings.name\n\nWe can configure each subclass using the nested\n\nSettings\n\nclass, such as defining the collection name, or anything else specific to that subclass. Within the Python ecosystem, there is an ODM implementation on\n\ntop of MongoDB, called\n\nmongoengine\n\n, which you can find on GitHub. It follows a pattern similar to ours but more comprehensive. We implemented it by ourselves, as it was an excellent exercise to practice writing modular and generic code following best OOP principles, which are essential for implementing production-level code.\n\nOceanofPDF.com\n\nData categories and user document classes\n\nThe last piece of the puzzle is to see the implementation of the subclasses that inherit from the\n\nNoSQLBaseDocument\n\nbase class. These are the concrete classes that define our data categories. You’ve seen these classes used across the chapter when working with articles, repositories, and posts within the crawler classes.\n\nWe begin by importing the essential Python modules and the ODM base class:\n\nfrom\n\nabc\n\nimport\n\nABC\n\nfrom\n\ntyping\n\nimport\n\nOptional\n\nfrom\n\npydantic\n\nimport\n\nUUID4, Field\n\nfrom\n\n.base\n\nimport\n\nNoSQLBaseDocument\n\nfrom\n\n.types\n\nimport\n\nDataCategory\n\nWe define an\n\nenum\n\nclass, where we centralize all our data category types. These variables will act as constants in configuring all our ODM classes throughout the book.\n\nThe class can be found in the repository at\n\nllm_engineering/domain/types.py\n\n.\n\nfrom\n\nenum\n\nimport\n\nStrEnum\n\nclass\n\nDataCategory\n\n(\n\nStrEnum\n\n): PROMPT =\n\n\"prompt\"\n\nQUERIES =\n\n\"queries\"\n\nINSTRUCT_DATASET_SAMPLES =\n\n\"instruct_dataset_samples\"\n\nINSTRUCT_DATASET =\n\n\"instruct_dataset\"\n\nPREFERENCE_DATASET_SAMPLES =\n\n\"preference_dataset_samples\"\n\nPREFERENCE_DATASET =\n\n\"\n\npreference_dataset\"\n\nPOSTS =\n\n\"posts\"\n\nARTICLES =\n\n\"articles\"\n\nREPOSITORIES =\n\n\"repositories\"\n\nThe\n\nDocument\n\nclass is introduced as an abstract base model for other documents on top of the\n\nNoSQLBaseDocument\n\nODM class. It includes common attributes like content, platform, and author details, providing a standardized structure for documents that will inherit from it:\n\nclass\n\nDocument\n\n(NoSQLBaseDocument, ABC): content:\n\ndict\n\nplatform:\n\nstr\n\nauthor_id: UUID4 = Field(alias=\n\n\"author_id\"\n\n) author_full_name:\n\nstr\n\n= Field(alias=\n\n\"author_full_name\"\n\n)\n\nFinally, specific document types are defined by extending the\n\nDocument\n\nclass. The\n\nRepositoryDocument\n\n,\n\nPostDocument\n\n, and\n\nArticleDocument\n\nclasses represent different categories of data, each with unique fields and settings that specify their respective collection names in the database:\n\nclass\n\nRepositoryDocument\n\n(\n\nDocument\n\n): name:\n\nstr\n\nlink:\n\nstr\n\nclass\n\nSettings\n\n: name = DataCategory.REPOSITORIES\n\nclass\n\nPostDocument\n\n(\n\nDocument\n\n): image:\n\nOptional\n\n[\n\nstr\n\n] =\n\nNone\n\nlink:\n\nstr\n\n|\n\nNone\n\n=\n\nNone\n\nclass\n\nSettings\n\n: name = DataCategory.POSTS\n\nclass\n\nArticleDocument\n\n(\n\nDocument\n\n): link:\n\nstr\n\nclass\n\nSettings\n\n: name = DataCategory.ARTICLES\n\nFinally, we define the\n\nUserDocument\n\nclass, which is used to store and query all the users from the LLM Twin project:\n\nclass\n\nUserDocument\n\n(\n\nNoSQLBaseDocument\n\n): first_name:\n\nstr\n\nlast_name:\n\nstr\n\nclass\n\nSettings\n\n: name =\n\n\"users\"\n\n@property\n\ndef\n\nfull_name\n\n(\n\nself\n\n):\n\nreturn\n\nf\"\n\n{self.first_name}\n\n{self.last_name}\n\n\"\n\nBy implementing the\n\nNoSQLBaseDocument\n\nODM class, we had to focus solely on the fields and specific functionality of each document or domain entity. All the CRUD functionality is delegated to the parent class. Also, by leveraging Pydantic to define the fields, we have out-of-the-box type validation. For example, when creating an instance of the\n\nArticleDocument\n\nclass, if the provided link is\n\nNone\n\nor not a string, it will throw an error signaling that the data is invalid.\n\nWith that, we’ve finished implementing our data collection pipeline, starting with the ZenML components. Then, we looked into the implementation of the crawlers and, finally, wrapped it up with the ODM class and data category documents. The last step is to run the data collection pipeline and ingest raw data into the MongoDB data warehouse.\n\nOceanofPDF.com\n\nGathering raw data into the data warehouse\n\nZenML orchestrates the data collection pipeline. Thus, leveraging ZenML, the data collection pipeline can be run manually, scheduled, or triggered by specific events. Here, we will show you how to run it manually, while we will discuss the other scenarios in Chapter 11 when digging deeper into MLOps.\n\nWe configured a different pipeline run for each author. We provided a ZenML configuration file for Paul Iusztin’s or Maxime Labonne’s data. To call the data collection pipeline to collect Maxime’s data, for example, you can run the following CLI command:\n\npoetry poe run-digital-data-etl-maxime\n\nThat will call the pipeline with the following ZenML YAML configuration file:\n\nparameters:\n\nuser_full_name:\n\nMaxime\n\nLabonne\n\n# [First Name(s)] [Last Name]\n\nlinks:\n\n# Personal Blog\n\n\n\nhttps://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html\n\n\n\nhttps://mlabonne.github.io/blog/posts/2024-07- 15_The_Rise_of_Agentic_Data_Generation.html\n\n# Substack\n\n\n\nhttps://maximelabonne.substack.com/p/uncensor-any-llm-with-abliteration- d30148b7d43e\n\n\n\nhttps://maximelabonne.substack.com/p/create-mixtures-of-experts-with- mergekit-11b318c99562\n\n\n\nhttps://maximelabonne.substack.com/p/merge-large-language-models-with- mergekit-2118fb392b54\n\n…\n\n# More Substack links\n\nIn Figure 3.3 earlier, we saw the pipeline’s run DAG and details in ZenML’s dashboard. Meanwhile, Figure 3.5 shows the\n\nuser\n\noutput artifact generated by this data collection pipeline. You can inspect the query\n\nuser_full_name\n\nand the retrieved\n\nuser\n\nfrom the MongoDB database, for which we collected the links in this specific run.\n\nFigure 3.5: Example of the user output artifact after running the data collection pipeline using Maxime’s configuration file\n\nAlso, in Figure 3.6, you can observe the\n\ncrawled_links\n\noutput artifact, which lists all the domains from which we collected data, the total number of links crawled for each domain, and the number of successfully collected links.\n\nWe want to highlight again the power of these artifacts, as they trace each pipeline’s results and metadata, making it extremely easy to monitor and debug each pipeline run individually.\n\nFigure 3.6: Example of the crawled_links output artifact after running the data collection pipeline using Maxime’s configuration file\n\nNow, we can download the\n\ncrawled_links\n\nartifact anywhere in our code by running the following code, where the\n\nID\n\nof the artifact can be found in ZenML and is unique for every artifact version:\n\nfrom\n\nzenml.client\n\nimport\n\nClient artifact = Client().get_artifact_version(\n\n'8349ce09-0693-4e28-8fa2-20f82c76ddec'\n\n) loaded_artifact = artifact.load()\n\nFor example, we can easily run the same data collection pipeline but with Paul Iusztin’s YAML configuration, listed below:\n\nparameters:\n\nuser_full_name:\n\nPaul\n\nIusztin\n\n# [First Name(s)] [Last Name]\n\nlinks:\n\n# Medium\n\n\n\nhttps://medium.com/decodingml/an-end-to-end-framework-for-production- ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f\n\n\n\nhttps://medium.com/decodingml/a-real-time-retrieval-system-for-rag-on- social-media-data-9cc01d50a2a0\n\n\n\nhttps://medium.com/decodingml/sota-python-streaming-pipelines-for-fine- tuning-llms-and-rag-in-real-time-82eb07795b87\n\n…\n\n# More Medium links\n\n# Substack\n\n\n\nhttps://decodingml.substack.com/p/real-time-feature-pipelines-with? r=1ttoeh\n\n\n\nhttps://decodingml.substack.com/p/building-ml-systems-the-right-way? r=1ttoeh\n\n\n\nhttps://decodingml.substack.com/p/reduce-your-pytorchs-code-latency? r=1ttoeh\n\n…\n\n# More Substack links\n\nTo run the pipeline using Paul’s configuration, we call the following\n\npoe\n\ncommand:\n\npoetry poe run-digital-data-etl-paul\n\nThat, under the hood, calls the following CLI command that references Paul’s config file:\n\npoetry run python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml\n\nYou can find all the configs in the repository in the\n\nconfigs/\n\ndirectory. Also, using\n\npoe\n\n, we configured a command that calls the data collection pipeline for all the supported authors:\n\npoetry poe run-digital-data-etl\n\nWe can easily query the MongoDB data warehouse using our ODM classes. For example, let’s query all the articles collected for Paul Iusztin:\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nArticleDocument, UserDocument user = UserDocument.get_or_create(first_name=\n\n\"Paul\"\n\n, last_name=\n\n\"Iusztin\"\n\n) articles = ArticleDocument.bulk_find(author_id=\n\nstr\n\n(user.\n\nid\n\n))\n\nprint\n\n(\n\nf\"User ID:\n\n{user.\n\nid\n\n}\n\n\"\n\n)\n\nprint\n\n(\n\nf\"User name:\n\n{user.first_name}\n\n{user.last_name}\n\n\"\n\n)\n\nprint\n\n(\n\nf\"Number of articles:\n\n{\n\nlen\n\n(articles)}\n\n\"\n\n)\n\nprint\n\n(\n\n\"First article link:\"\n\n, articles[\n\n0\n\n].link)\n\nThe output of the code from above is:\n\nUser ID: 900fec95-d621-4315-84c6-52e5229e0b96 User name: Paul Iusztin Number of articles: 50 First article link: https://medium.com/decodingml/an-end-to-end-framework-for-production- ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f\n\nWith only two lines of code, we can query and filter our MongoDB data warehouse using any ODM defined within our project.\n\nAlso, to ensure that your data collection pipeline works as expected, you can search your MongoDB collections using your IDE’s MongoDB plugin, which you must install separately. For example, you can use this plugin for VSCode: https://www.mongodb.com/products/tools/vs-code. For other IDEs, you can use similar plugins or external NoSQL visualization tools. After connecting to the MongoDB visualization tool, you can connect to our local database using the following URI:\n\nmongodb://llm_engineering:llm_engineering@127.0.0.1:27017\n\n. For a cloud MongoDB cluster, you must change the URI, which we will explore in Chapter 11.\n\nAnd just like that, you’ve learned how to run the data collection pipeline with different ZenML configs and how to visualize the output artifacts of\n\neach run. We also looked at how to query the data warehouse for a particular data category and author. Thus, we’ve finalized our data engineering chapter and can move to the conclusion.\n\nOceanofPDF.com\n\nTroubleshooting\n\nThe raw data stored in the MongoDB database is central to all future steps. Thus, if you haven’t successfully run the code from this chapter due to any issues with the crawlers, this section provides solutions for fixing potential issues to allow you to move forward.\n\nOceanofPDF.com\n\nSelenium issues\n\nIt is a well-known issue that running Selenium can cause problems due to issues with the browser driver, such as the\n\nChromeDriver\n\n. Thus, if the crawlers that use Selenium, such as the\n\nMediumCrawler\n\n, fail due to problems with your\n\nChromeDriver\n\n, you can easily bypass this by commenting out the Medium links added to the data collection YAML configs. To do so, go to the\n\nconfigs/\n\ndirectory and find all the YAML files that start with\n\ndigital_data_etl_*\n\n, such as\n\ndigital_data_etl_maxime_labonne.yaml\n\n. Open them and comment on all the Medium-related URLs, as illustrated in Figure 3.7. You can leave out the Substack or personal blog URLs as these use the\n\nCustomArticleCrawler\n\n, which is not dependent on Selenium.\n\nFigure 3.7: Fix Selenium issues when crawling raw data\n\nOceanofPDF.com\n\nImport our backed-up data\n\nIf nothing works, there is the possibility of populating the MongoDB database with your backed-up data saved under the\n\ndata/data_warehouse_raw_data directory\n\n. This will allow you to proceed to the fine-tuning and inference sections without running the data collection ETL code. To import all the data within this directory, run:\n\npoetry poe run-import-data-warehouse-from-json\n\nAfter running the CLI command from above, you will have a one-to-one replica of the dataset we used while developing the code. To ensure the import is completed successfully, you should have 88 articles and 3 users in your MongoDB database.\n\nOceanofPDF.com\n\nSummary\n\nIn this chapter, we’ve learned how to design and build the data collection pipeline for the LLM Twin use case. Instead of relying on static datasets, we collected our custom data to mimic real-world situations, preparing us for real-world challenges in building AI systems.\n\nFirst, we examined the architecture of LLM Twin’s data collection pipeline, which functions as an ETL process. Next, we started digging into the pipeline implementation. We began by understanding how we can orchestrate the pipeline using ZenML. Then, we looked into the crawler implementation. We learned how to crawl data in three ways: using CLI commands in subprocesses or using utility functions from LangChain or Selenium to build custom logic that programmatically manipulates the browser. Finally, we looked into how to build our own ODM class, which we used to define our document class hierarchy, which contains entities such as articles, posts, and repositories.\n\nAt the end of the chapter, we learned how to run ZenML pipelines with different YAML configuration files and explore the results in the dashboard. We also saw how to interact with the MongoDB data warehouse through the ODM classes.\n\nIn the next chapter, we will cover the key steps of the RAG feature pipeline, including chunking and embedding documents, ingesting these documents into a vector DB, and applying pre-retrieval optimizations to improve performance. We will also set up the necessary infrastructure\n\nprogrammatically using Pulumi and conclude by deploying the RAG ingestion pipeline to AWS.\n\nOceanofPDF.com\n\nReferences\n\nBreuss, M. (2023, July 26). Beautiful Soup: Build a Web Scraper With Python. https://realpython.com/beautiful-soup-web-scraper-python/\n\nDavid, D. (2024, July 8). Guide to Web Scraping with Selenium in 2024. Bright Data. https://brightdata.com/blog/how-tos/using-selenium-for-web- scraping\n\nHjelle, G. A. (2023, October 21). Python 3.12 Preview: Static Typing Improvements. https://realpython.com/python312-typing/\n\nORM Quick Start — SQLAlchemy 2.0 documentation. (n.d.). https://docs.sqlalchemy.org/en/20/orm/quickstart.html\n\nRamos, L. P. (2023, August 4). Python and MongoDB: Connecting to NoSQL Databases. https://realpython.com/introduction-to-mongodb-and- python/\n\nRefactoring.Guru. (2024, January 1). Builder. https://refactoring.guru/design-patterns/builder\n\nWhat is ETL? A complete guide. (n.d.). Qlik. https://www.qlik.com/us/etl\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\n4\n\nOceanofPDF.com\n\nRAG Feature Pipeline\n\nRetrieval-augmented generation (RAG) is fundamental in most generative AI applications. RAG’s core responsibility is to inject custom data into the large language model (LLM) to perform a given action (e.g., summarize, reformulate, and extract the injected data). You often want to use the LLM on data it wasn’t trained on (e.g., private or new data). As fine-tuning an LLM is a highly costly operation, RAG is a compelling strategy that bypasses the need for constant fine-tuning to access that new data.\n\nWe will start this chapter with a theoretical part that focuses on the fundamentals of RAG and how it works. We will then walk you through all the components of a naïve RAG system: chunking, embedding, and vector DBs. Ultimately, we will present various optimizations used for an advanced RAG system. Then, we will continue exploring LLM Twin’s RAG feature pipeline architecture. At this step, we will apply all the theoretical aspects we discussed at the beginning of the chapter. Finally, we will go through a practical example by implementing the LLM Twin’s RAG feature pipeline based on the system design described throughout the book.\n\nThe main sections of this chapter are:\n\nUnderstanding RAG\n\nAn overview of advanced RAG\n\nExploring the LLM Twin’s RAG feature pipeline architecture\n\nImplementing the LLM Twin’s RAG feature pipeline\n\nBy the end of this chapter, you will have a clear and comprehensive understanding of what RAG is and how it is applied to our LLM Twin use case.\n\nOceanofPDF.com\n\nUnderstanding RAG\n\nRAG enhances the accuracy and reliability of generative AI models with information fetched from external sources. It is a technique complementary to the internal knowledge of the LLMs. Before going into the details, let’s understand what RAG stands for:\n\nRetrieval: Search for relevant data\n\nAugmented: Add the data as context to the prompt\n\nGeneration: Use the augmented prompt with an LLM for generation\n\nAny LLM is bound to understand the data it was trained on, sometimes called parameterized knowledge. Thus, even if the LLM can perfectly answer what happened in the past, it won’t have access to the newest data or any other external sources on which it wasn’t trained.\n\nLet’s take the most powerful model from OpenAI as an example, which, in the summer of 2024, is GPT-4o. The model is trained on data up to October 2023. Thus, if we ask what happened during the 2020 pandemic, it can be answered perfectly due to its parametrized knowledge. However, it will not know the answer if we ask about the 2024 European Football\n\nChampionship results due to its bounded parametrized knowledge. Another scenario is that it will start confidently hallucinating and provide a faulty answer.\n\nRAG overcomes these two limitations of LLMs. It provides access to external or latest data and prevents hallucinations, enhancing generative AI models’ accuracy and reliability.\n\nOceanofPDF.com\n\nWhy use RAG?\n\nWe briefly explained the importance of using RAG in generative AI applications earlier. Now, we will dig deeper into the “why,” following which we will focus on what a naïve RAG framework looks like.\n\nFor now, to get an intuition about RAG, you have to know that when using RAG, we inject the necessary information into the prompt to answer the initial user question. After that, we pass the augmented prompt to the LLM for the final answer. Now, the LLM will use the additional context to answer the user question.\n\nThere are two fundamental problems that RAG solves:\n\nHallucinations\n\nOld or private information\n\nOceanofPDF.com\n\nHallucinations\n\nIf a chatbot without RAG is asked a question about something it wasn’t trained on, there is a high chance that it will give you a confident answer about something that isn’t true. Let’s take the 2024 European Football Championship as an example. If the model is trained up to October 2023 and we ask it something about the tournament, it will most likely come up with a random answer that is hard to differentiate between reality and truth. Even if the LLM doesn’t hallucinate all the time, it raises concerns about the trustworthiness of its answers. Thus, we must ask ourselves: “When can we trust the LLM’s answers?” and “How can we evaluate if the answers are correct?”.\n\nBy introducing RAG, we enforce the LLM to always answer solely based on the introduced context. The LLM will act as the reasoning engine, while the additional information added through RAG will act as the single source of truth for the generated answer. By doing so, we can quickly evaluate if the LLM’s answer is based on the external data or not.\n\nOceanofPDF.com\n\nOld information\n\nAny LLM is trained or fine-tuned on a subset of the total world knowledge dataset. This is due to three main issues:\n\nPrivate data: You cannot train your model on data you don’t own or have the right to use.\n\nNew data: New data is generated every second. Thus, you would have to constantly train your LLM to keep up.\n\nCosts: Training or fine-tuning an LLM is an extremely costly operation. Hence, it is not feasible to do it on an hourly or daily basis.\n\nRAG solves these issues, as you no longer have to constantly fine-tune your LLM on new data (or even private data). Directly injecting the necessary data to respond to user questions into the prompts that are fed to the LLM is enough to generate correct and valuable answers.\n\nTo conclude, RAG is key for a robust and flexible generative AI system. But how do we inject the right data into the prompt based on the user’s questions? We will dig into the technical aspects of RAG in the next sections.\n\nOceanofPDF.com\n\nThe vanilla RAG framework\n\nEvery RAG system is similar at its roots. We will first focus on understanding RAG in its simplest form. Later, we will gradually introduce more advanced RAG techniques to improve the system’s accuracy. Note that we will use vanilla and naive RAG interchangeably to avoid repetition.\n\nA RAG system is composed of three main modules independent of each other:\n\nIngestion pipeline: A batch or streaming pipeline used to populate the vector DB\n\nRetrieval pipeline: A module that queries the vector DB and retrieves relevant entries to the user’s input\n\nGeneration pipeline: The layer that uses the retrieved data to augment the prompt and an LLM to generate answers\n\nAs these three components are classes or services of their own, we will dig into each separately. But for now, let’s try to answer the question “How are these three modules connected?”. Here is a very simplistic overview:\n\nOn the backend side, the ingestion pipeline runs either on a schedule or constantly to populate the vector DB with external data.\n\nOn the client side, the user asks a question.\n\nThe question is passed to the retrieval module, which preprocesses the user’s input and queries the vector DB.\n\nThe generation pipelines use a prompt template, user input, and retrieved context to create the prompt.\n\nThe prompt is passed to an LLM to generate the answer.\n\nThe answer is shown to the user.\n\nFigure 4.1: Vanilla RAG architecture\n\nYou must implement RAG in your generative AI application when you need access to any type of external information. For example, when implementing a financial assistant, you most likely need access to the latest news, reports, and prices before providing valuable answers. Or, if you build a traveling recommender, you must retrieve and parse a list of potential attractions, restaurants, and activities. At training time, LLMs don’t have access to your specific data, so you will often have to implement a RAG strategy in your generative AI project. Now, let’s dig into the ingestion, retrieval, and generation pipelines.\n\nOceanofPDF.com\n\nIngestion pipeline\n\nThe RAG ingestion pipeline extracts raw documents from various data sources (e.g., data warehouse, data lake, web pages, etc.). Then, it cleans, chunks (splits into smaller sections), and embeds the documents. Ultimately, it loads the embedded chunks into a vector DB (or other similar vector storage).\n\nThus, the RAG ingestion pipeline is split into the following:\n\nThe data extraction module gathers all the necessary data from various sources such as DBs, APIs, or web pages. This module is highly dependent on your data. It can be as easy as querying your data warehouse or something more complex such as crawling Wikipedia.\n\nA cleaning layer standardizes and removes unwanted characters from the extracted data. For example, you must remove all invalid characters from your input text, such as non-ASCII and bold and italic characters. Another popular cleaning strategy is to replace URLs with placeholders. However, your cleaning strategy will vary depending on your data source and embedding model.\n\nThe chunking module splits the cleaned documents into smaller ones. As we want to pass the document’s content to an embedding model, this is necessary to ensure it doesn’t exceed the model’s input maximum size. Also, chunking is required to separate specific regions that are\n\nsemantically related. For example, when chunking a book’s chapter, the most optimal way is to group similar paragraphs into the same section or chunk. By doing so, at the retrieval time, you will add only the essential data to the prompt.\n\nThe embedding component uses anembedding model to take the chunk’s content (text, images, audio, etc.) and project it into a dense vector packed with semantic value—more on embeddings in the What are embeddings? section below.\n\nThe loading module takes the embedded chunks along with a metadata document. The metadata will contain essential information such as the embedded content, the URL to the source of the chunk, and when the content was published on the web. The embedding is used as an index to query similar chunks, while the metadata is used to access the information added to augment the prompt.\n\nAt this point, we have a RAG ingestion pipeline that takes raw documents as input, processes them, and populates a vector DB. The next step is to retrieve relevant data from the vector store correctly.\n\nOceanofPDF.com\n\nRetrieval pipeline\n\nThe retrieval components take the user’s input (text, image, audio, etc.), embed it, and query the vector DB for similar vectors to the user’s input.\n\nThe primary function of the retrieval step is to project the user’s input into the same vector space as the embeddings used as an index in the vector DB. This allows us to find the top K’s most similar entries by comparing the embeddings from the vector storage with the user’s input vector. These entries then serve as content to augment the prompt that is passed to the LLM to generate the answer.\n\nYou must use a distance metric to compare two vectors, such as the Euclidean or Manhattan distance. But the most popular one is the cosine distance, which is equal to 1 minus the cosine of the angle between two vectors, as follows:\n\nIt ranges from\n\n1\n\nto\n\n1\n\n, with a value of\n\n1\n\nwhen vectors A and B are in opposite directions,\n\n0\n\nif they are orthogonal, and\n\n1\n\nif they point in the same direction.\n\nMost of the time, the cosine distance works well in non-linear complex vector spaces. However, it is essential to notice that choosing the proper distance between two vectors depends on your data and the embedding model you use.\n\nOne critical factor to highlight is that the user’s input and embeddings must be in the same vector space. Otherwise, you cannot compute the distance between them. To do so, it is essential to preprocess the user input in the same way you processed the raw documents in the RAG ingestion pipeline. This means you must clean, chunk (if necessary), and embed the user’s input using the same functions, models, and hyperparameters. This is similar to how you have to preprocess the data into features in the same way between training and inference; otherwise, the inference will yield inaccurate results—a phenomenon also known as the training-serving skew.\n\nOceanofPDF.com\n\nGeneration pipeline\n\nThe last step of the RAG system is to take the user’s input, retrieve data, pass it to an LLM, and generate a valuable answer.\n\nThe final prompt results from a system and prompt template populated with the user’s query and retrieved context. You might have a single prompt template or multiple prompt templates, depending on your application. Usually, all the prompt engineering is done at the prompt template level.\n\nBelow, you can see a dummy example of what a generic system and prompt template look like and how they are used together with the retrieval logic and the LLM to generate the final answer:\n\nsystem_template =\n\n\"\"\"\n\nYou are a helpful assistant who answers all the user's questions politely.\n\n\"\"\"\n\nprompt_template =\n\n\"\"\"\n\nAnswer the user's question using only the provided context. If you cannot answer using the context, respond with \"I don't know.\"\n\nContext: {context}\n\nUser question: {user_question}\n\n\"\"\"\n\nuser_question =\n\n\"\"\n\nretrieved_context = retrieve(user_question) prompt =\n\nf\"\n\n{system_template}\n\n\\n\"\n\nprompt += prompt_template.\n\nformat\n\n(context=retrieved_context, user_question=user_question) answer = llm(prompt)\n\nAs the prompt templates evolve, each change should be tracked and versioned using machine learning operations (MLOps) best practices. Thus, during training or inference time, you always know that a given answer was generated by a specific version of the LLM and prompt template(s). You can do this through Git, store the prompt templates in a DB, or use specific prompt management tools such as LangFuse.\n\nAs we’ve seen in the retrieval pipeline, some critical aspects that directly impact the accuracy of your RAG system are the embeddings of the external data, usually stored in vector DBs, the embedding of the user’s query, and how we can find similarities between the two using functions such as the cosine distance. To better understand this part of the RAG algorithm, let’s zoom in on what embeddings are and how they are computed.\n\nOceanofPDF.com\n\nWhat are embeddings?\n\nImagine you’re trying to teach a computer to understand the world. Embeddings are like a particular translator that turns these things into a numerical code. This code isn’t random, though, because similar words or items end up with codes that are close to each other. It’s like a map where words with similar meanings are clustered together.\n\nWith that in mind, a more theoretical definition is that embeddings are dense numerical representations of objects encoded as vectors in a continuous vector space, such as words, images, or items in a recommendation system. This transformation helps capture the semantic meaning and relationships between the objects. For instance, in natural language processing (NLP), embeddings translate words into vectors where semantically similar words are positioned closely together in the vector space.\n\nFigure 4.2: What are embeddings?\n\nA popular method is visualizing the embeddings to understand and evaluate their geometrical relationship. As the embeddings often have more than 2 or 3 dimensions, usually between 64 and 2048, you must project them again to 2D or 3D.\n\nFor example, you can use UMAP (https://umap- learn.readthedocs.io/en/latest/index.html), a dimensionality reduction method well known for keeping the geometrical properties between the points when projecting the embeddings to 2D or 3D. Another popular\n\nalgorithm for dimensionality reduction when visualizing vectors is t-SNE (https://scikit- learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). However, compared to UMAP, it is more stochastic and doesn’t preserve the topological relationships between the points.\n\nA dimensionality reduction algorithm, such as PCA, UMAP, and t-SNE, is a mathematical technique used to reduce the number of input variables or features in a dataset while preserving the data’s essential patterns, structure, and relationships. The goal is to transform high-dimensional data into a lower-dimensional form, making it easier to visualize, interpret, and process while minimizing the loss of important information. These methods help to address the “curse of dimensionality,” improve computational efficiency, and often enhance the performance of ML algorithms.\n\nFigure 4.3: Visualize embeddings using UMAP (Source: UMAP’s documentation)\n\nOceanofPDF.com\n\nWhy embeddings are so powerful\n\nFirstly, ML models work only with numerical values. This is not a problem when working with tabular data, as the data is often in numerical form or can easily be processed into numbers. Embeddings come in handy when we want to feed words, images, or audio data into models.\n\nFor instance, when working with transformer models, you tokenize all your text input, where each token has an embedding associated with it. The beauty of this process lies in its simplicity; the input to the transformer is a sequence of embeddings, which can be easily and confidently interpreted by the dense layers of the neural network.\n\nBased on this example, you can use embeddings to encode any categorical variable and feed it to an ML model. But why not use other simple methods, such as one-hot encoding? When working with categorical variables with high cardinality, such as language vocabularies, you will suffer from the curse of dimensionality when using other classical methods. For example, if your vocabulary has 10,000 tokens, then only one token will have a length of 10,000 after applying one-hot encoding. If the input sequence has N tokens, that will become N * 10,000 input parameters. If N >= 100, often, when inputting text, the input is too large to be usable. Another issue with other classical methods that don’t suffer from the curse of dimensionality, such as hashing, is that you lose the semantic relationships between the vectors.\n\nOne-hot encoding is a technique that converts categorical variables into a binary matrix representation. Each category is represented as a unique binary vector. For each categorical variable, a binary vector is created with a length equal to the number of unique categories, where all values are zero except for the index corresponding to the specific category, which is set to one. The method preserves all information about the categories. It is simple and interpretable. However, a significant disadvantage is that it can lead to a high-dimensional feature space if the categorical variable has many unique values, making the method impractical.\n\nFeature hashing, also known as hashing encoding or the “hash trick,” is a technique used to convert categorical variables into numerical features by applying a hash function to the category values. Compared to one-hot encoding, the method is not bound to the number of unique categories, but it reduces the dimensionality of the feature space by mapping categories into a fixed number of bins or buckets. Thus, it reduces the dimensionality of the feature space, which is particularly useful when dealing with high-cardinality categorical variables. This makes it efficient in terms of memory usage and computational time. However, there is a risk of collisions, where different categories might map to the same bin, leading to a loss of information. The mapping makes the method uninterpretable. Also, it is difficult to understand the relationship between the original categories and the hashed features.\n\nEmbeddings help us encode categorical variables while controlling the output vector’s dimension. They also use ingenious ways to condense information into a lower dimension space than naive hashing tricks.\n\nSecondly, embedding your input reduces the size of its dimension and condenses all of its semantic meaning into a dense vector. This is an extremely popular technique when working with images, where a CNN encoder module maps the high-dimensional meaning into an embedding, which is later processed by a CNN decoder that performs the classification or regression steps.\n\nThe following image shows a typical CNN layout. Imagine tiny squares within each layer. Those are the “receptive fields.” Each square feeds information to a single neuron in the previous layer. As you move through the network, two key things are happening:\n\nShrinking the picture: Special “subsampling” operations make the layers smaller, focusing on essential details.\n\nLearning features: “Convolution” operations, on the other hand, actually increase the layer size as the network learns more complex features from the image.\n\nFinally, a fully connected layer at the end takes all this processed information and transforms it into the final vector embedding, a numerical image representation.\n\nFigure 4.4: Creating embeddings from an image using a CNN (Image source)\n\nThe preceding image is sourced from Wikimedia Commons (https://commons.wikimedia.org/wiki/File:Typical_cnn.png) and licensed under the Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0: https://creativecommons.org/licenses/by- sa/4.0/deed.en).\n\nOceanofPDF.com\n\nHow are embeddings created?\n\nEmbeddings are created by deep learning models that understand the context and semantics of your input and project it into a continuous vector space.\n\nVarious deep learning models can be used to create embeddings, varying by the data input type. Thus, it is fundamental to understand your data and what you need from it before picking an embedding model.\n\nFor example, when working with text data, one of the early methods used to create embeddings for your vocabulary is Word2Vec and GloVe. These are still popular methods used today for simpler applications.\n\nAnother popular method is to use encoder-only transformers, such as BERT, or other methods from its family, such as RoBERTa. These models leverage the encoder of the transformer architecture to smartly project your input into a dense vector space that can later be used as embeddings.\n\nTo quickly compute the embeddings in Python, you can conveniently leverage the Sentence Transformers Python package (also available in Hugging Face’s transformer package). This tool provides a user-friendly interface, making the embedding process straightforward and efficient.\n\nIn the code snippet below, you can see how we loaded a model from SentenceTransformer, computed the embeddings for three sentences, and, ultimately, computed the cosine similarity between them. The similarity between one sentence and itself is always 1. Also, the similarity between the first and second sentences is approximately 0, as the sentences have nothing in common. In contrast, the value between the first and third one is higher as there is some overlapping context:\n\nfrom\n\nsentence_transformers\n\nimport\n\nSentenceTransformer model = SentenceTransformer(\n\n\"all-MiniLM-L6-v2\"\n\n) sentences = [\n\n\"The dog sits outside waiting for a treat.\"\n\n,\n\n\"I am going swimming.\"\n\n,\n\n\"The dog is swimming.\"\n\n] embeddings = model.encode(sentences)\n\nprint\n\n(embeddings.shape)\n\n# Output: [3, 384]\n\nsimilarities = model.similarity(embeddings, embeddings)\n\nprint\n\n(similarities)\n\n# Output:\n\n# tensor([[ 1.0000, -0.0389, 0.2692],\n\n# [-0.0389, 1.0000, 0.3837],\n\n# [ 0.2692, 0.3837, 1.0000]])\n\n#\n\n# similarities[0, 0] = The similarity between the first sentence and itself.\n\n# similarities[0, 1] = The similarity between the first and second sentence.\n\n# similarities[2, 1] = The similarity between the third and second sentence.\n\nThe source code for the preceding snippet can be found at https://github.com/PacktPublishing/LLM- Engineering/blob/main/code_snippets/08_text_embeddings.py.\n\nThe examples in the embeddings section can be run within the virtual environment used across the book, as it contains all the required dependencies.\n\nThe best-performing embedding model can change with time and your specific use case. You can find particular models on the Massive Text Embedding Benchmark (MTEB) on Hugging Face. Depending on your needs, you can consider the best-performing model, the one with the best accuracy, or the one with the smallest memory footprint. This decision is solely based on your requirements (e.g., accuracy and hardware). However, Hugging Face and SentenceTransformer make switching between different models straightforward. Thus, you can always experiment with various options.\n\nWhen working with images, you can embed them using convolutional neural networks (CNNs). Popular CNN networks are based on the ResNet architecture. However, we can’t directly use image embedding techniques for audio recordings. Instead, we can create a visual representation of the audio, such as a spectrogram, and then apply image embedding models to those visuals. This allows us to capture the essence of images and sounds in a way computers can understand.\n\nBy leveraging models like CLIP, you can practically embed a piece of text and an image in the same vector space. This allows you to find similar images using a sentence as input, or the other way around, demonstrating the practicality of CLIP.\n\nIn the following code snippet, we use CLIP to encode a crazy cat image and three sentences. Ultimately, we use cosine similarity to compute the resemblance between the picture and the sentences:\n\nfrom\n\nio\n\nimport\n\nBytesIO\n\nimport\n\nrequests\n\nfrom\n\nPIL\n\nimport\n\nImage\n\nfrom\n\nsentence_transformers\n\nimport\n\nSentenceTransformer response = requests.get(\n\n\"https://github.com/PacktPublishing/LLM- Engineering/blob/main/images/crazy_cat.jpg?raw=true\"\n\n) image = Image.\n\nopen\n\n(BytesIO(response.content)) model = SentenceTransformer(\n\n\"\n\nclip-ViT-B-32\"\n\n) img_emb = model.encode(image) text_emb = model.encode( [\n\n\"A crazy cat smiling.\"\n\n,\n\n\"A white and brown cat with a yellow bandana.\"\n\n,\n\n\"A man eating in the garden.\"\n\n] )\n\nprint\n\n(text_emb.shape)\n\n# noqa\n\n# Output: (3, 512)\n\nsimilarity_scores = model.similarity(img_emb, text_emb)\n\nprint\n\n(similarity_scores)\n\n# noqa\n\n# Output: tensor([[0.3068, 0.3300, 0.1719]])\n\nThe source code can be found at https://github.com/PacktPublishing/LLM- Engineering/blob/main/code_snippets/08_text_image_embeddings.py.\n\nHere, we provided a small introduction to how embeddings can be computed. The realm of specific implementations is vast, but what is important to know is that embeddings can be computed for most digital data categories, such as words, sentences, documents, images, videos, and graphs.\n\nIt’s crucial to grasp that you must use specialized models when you need to compute the distance between two different data categories, such as the distance between the vector of a sentence and of an image. These models are designed to project both data types into the same vector space, such as CLIP, ensuring accurate distance computation.\n\nOceanofPDF.com\n\nApplications of embeddings\n\nDue to the generative AI revolution, which uses RAG, embeddings have become extremely popular in information retrieval tasks, such as semantic search for text, code, images, and audio, and long-term memory of agents. But before generative AI, embeddings were already heavily used in:\n\nRepresenting categorical variables (e.g., vocabulary tokens) that are fed to an ML model\n\nRecommender systems by encoding the users and items and finding their relationship\n\nClustering and outlier detection\n\nData visualization by using algorithms such as UMAP\n\nClassification by using the embeddings as features\n\nZero-shot classification by comparing the embedding of each class and picking the most similar one\n\nThe last step to fully understanding how RAG works is to examine vector DBs and how they leverage embeddings to retrieve data.\n\nOceanofPDF.com\n\nMore on vector DBs\n\nVector DBs are specialized DBs designed to efficiently store, index, and retrieve vector embeddings. Traditional scalar-based DBs struggle with the complexity of vector data, making vector DBs crucial for tasks like real- time semantic search.\n\nWhile standalone vector indices like FAISS are effective for similarity search, they lack vector DBs’ comprehensive data management capabilities. Vector DBs support CRUD operations, metadata filtering, scalability, real- time updates, backups, ecosystem integration, and robust data security, making them more suited for production environments than standalone indices.\n\nOceanofPDF.com\n\nHow does a vector DB work?\n\nThink of how you usually search a DB. You type in something specific, and the system spits out the exact match. That’s how traditional DBs work. Vector DBs are different. Instead of perfect matches, we look for the closest neighbors of the query vector. Under the hood, a vector DB uses approximate nearest neighbor (ANN) algorithms to find these close neighbors.\n\nWhile ANN algorithms don’t return the top matches for a given search, standard nearest neighbor algorithms are too slow to work in practice. Also, it is shown empirically that using only approximations of the top matches for a given input query works well enough. Thus, the trade-off between accuracy and latency ultimately favors ANN algorithms.\n\nThis is a typical workflow of a vector DB:\n\nIndexing vectors: Vectors are indexed using data structures optimized for high-dimensional data. Common indexing techniques include hierarchical navigable small world (HNSW), random projection, product quantization (PQ), and locality-sensitive hashing (LSH).\n\nQuerying for similarity: During a search, the DB queries the indexed vectors to find those most similar to the input vector. This process involves comparing vectors based on similarity measures such as cosine\n\nsimilarity, Euclidean distance, or dot product. Each has unique advantages and is suitable for different use cases.\n\nPost-processing results: After identifying potential matches, the results undergo post-processing to refine accuracy. This step ensures that the most relevant vectors are returned to the user.\n\nVector DBs can filter results based on metadata before or after the vector search. Both approaches have trade-offs in terms of performance and accuracy. The query also depends on the metadata (along with the vector index), so it contains a metadata index user for filtering operations.\n\nOceanofPDF.com\n\nAlgorithms for creating the vector index\n\nVector DBs use various algorithms to create the vector index and manage searching data efficiently:\n\nRandom projection: Random projection reduces the dimensionality of vectors by projecting them into a lower-dimensional space using a random matrix. This technique preserves the relative distances between vectors, facilitating faster searches.\n\nPQ: PQ compresses vectors by dividing them into smaller sub-vectors and then quantizing these sub-vectors into representative codes. This reduces memory usage and speeds up similarity searches.\n\nLSH: LSH maps similar vectors into buckets. This method enables fast approximate nearest neighbor searches by focusing on a subset of the data, reducing the computational complexity.\n\nHNSW: HNSW constructs a multi-layer graph where each node represents a set of vectors. Similar nodes are connected, allowing the algorithm to navigate the graph and find the nearest neighbors efficiently.\n\nThese algorithms enable vector DBs to efficiently handle complex and large-scale data, making them a perfect fit for a variety of AI and ML applications.\n\nOceanofPDF.com\n\nDB operations\n\nVector DBs also share common characteristics with standard DBs to ensure high performance, fault tolerance, and ease of management in production environments. Key operations include:\n\nSharding and replication: Data is partitioned (sharded) across multiple nodes to ensure scalability and high availability. Data replication across nodes helps maintain data integrity and availability in case of node failures.\n\nMonitoring: Continuous monitoring of DB performance, including query latency and resource usage (RAM, CPU, disk), helps maintain optimal operations and identify potential issues before they impact the system.\n\nAccess control: Implementing robust access control mechanisms ensures that only authorized users can access and modify data. This includes role-based access controls and other security protocols to protect sensitive information.\n\nBackups: Regular DB backups are critical for disaster recovery. Automated backup processes ensure that data can be restored to a previous state in case of corruption or loss.\n\nOceanofPDF.com\n\nAn overview of advanced RAG\n\nThe vanilla RAG framework we just presented doesn’t address many fundamental aspects that impact the quality of the retrieval and answer generation, such as:\n\nAre the retrieved documents relevant to the user’s question?\n\nIs the retrieved context enough to answer the user’s question?\n\nIs there any redundant information that only adds noise to the augmented prompt?\n\nDoes the latency of the retrieval step match our requirements?\n\nWhat do we do if we can’t generate a valid answer using the retrieved information?\n\nFrom the questions above, we can draw two conclusions. The first one is that we need a robust evaluation module for our RAG system that can quantify and measure the quality of the retrieved data and generate answers\n\nrelative to the user’s question. We will discuss this topic in more detail in Chapter 9. The second conclusion is that we must improve our RAG framework to address the retrieval limitations directly in the algorithm. These improvements are known as advanced RAG.\n\nThe vanilla RAG design can be optimized at three different stages:\n\nPre-retrieval: This stage focuses on how to structure and preprocess your data for data indexing optimizations as well as query optimizations.\n\nRetrieval: This stage revolves around improving the embedding models and metadata filtering to improve the vector search step.\n\nPost-retrieval: This stage mainly targets different ways to filter out noise from the retrieved documents and compress the prompt before feeding it to an LLM for answer generation.\n\nFigure 4.5: The three stages of advanced RAG\n\nThis section is not meant to be an exhaustive list of all the advanced RAG methods available. The goal is to build an intuition about what can be optimized. We will use only examples based on text data, but the principles of advanced RAG remain the same regardless of the data category. Now, let’s zoom in on all three components.\n\nOceanofPDF.com\n\nPre-retrieval\n\nThe pre-retrieval steps are performed in two different ways:\n\nData indexing: It is part of the RAG ingestion pipeline. It is mainly implemented within the cleaning or chunking modules to preprocess the data for better indexing.\n\nQuery optimization: The algorithm is performed directly on the user’s query before embedding it and retrieving the chunks from the vector DB.\n\nAs we index our data using embeddings that semantically represent the content of a chunked document, most of the data indexing techniques focus on better preprocessing and structuring the data to improve retrieval efficiency, such as:\n\nSliding window: The sliding window technique introduces overlap between text chunks, ensuring that important context near chunk boundaries is retained, which enhances retrieval accuracy. This is particularly beneficial in domains like legal documents, scientific papers, customer support logs, and medical records, where critical information often spans multiple sections. The embedding is computed on the chunk along with the overlapping portion. Hence, the sliding\n\nwindow improves the system’s ability to retrieve relevant and coherent information by maintaining context across boundaries.\n\nEnhancing data granularity: This involves data cleaning techniques like removing irrelevant details, verifying factual accuracy, and updating outdated information. A clean and accurate dataset allows for sharper retrieval.\n\nMetadata: Adding metadata tags like dates, URLs, external IDs, or chapter markers helps filter results efficiently during retrieval.\n\nOptimizing index structures: It is based on different data index methods, such as various chunk sizes and multi-indexing strategies.\n\nSmall-to-big: The algorithm decouples the chunks used for retrieval and the context used in the prompt for the final answer generation. The algorithm uses a small sequence of text to compute the embedding while preserving the sequence itself and a wider window around it in the metadata. Thus, using smaller chunks enhances the retrieval’s accuracy, while the larger context adds more contextual information to the LLM.\n\nThe intuition behind this is that if we use the whole text for computing the embedding, we might introduce too much noise, or the text could contain multiple topics, which results in a poor overall semantic representation of the embedding.\n\nFigure 4.6: Query routing\n\nOn the query optimization side, we can leverage techniques such as query routing, query rewriting, and query expansion to refine the retrieved information for the LLM further:\n\nQuery routing: Based on the user’s input, we might have to interact with different categories of data and query each category differently. Query rooting is used to decide what action to take based on the user’s input, similar to if/else statements. Still, the decisions are made solely using natural language instead of logical statements.\n\nAs illustrated in Figure 4.6, let’s assume that, based on the user’s input, to do RAG, we can retrieve additional context from a vector DB using vector search queries, a standard SQL DB by translating the user query to an SQL command, or the internet by leveraging REST API calls. The query router can also detect whether a context is required, helping us avoid making redundant calls to external data storage. Also, a query router can be used to pick the best prompt template for a given input. For example, in the LLM Twin use case, depending on whether the user wants an article paragraph, a post, or a code snippet, you need different prompt templates to optimize the creation process. The routing usually uses an LLM to decide what route to take or embeddings by picking the path with the most similar vectors. To summarize, query routing is identical to an if/else statement but much more versatile as it works directly with natural language.\n\nQuery rewriting: Sometimes, the user’s initial query might not perfectly align with the way your data is structured. Query rewriting tackles this by reformulating the question to match the indexed information better. This can involve techniques like:\n\nParaphrasing: Rephrasing the user’s query while preserving its meaning (e.g., “What are the causes of climate change?” could be rewritten as “Factors contributing to global warming”).\n\nSynonym substitution: Replacing less common words with synonyms to broaden the search scope (e.g., “ joyful” could be rewritten as “happy”).\n\nSub-queries: For longer queries, we can break them down into multiple shorter and more focused sub-queries. This can help the retrieval stage identify relevant documents more precisely.\n\nHypothetical document embeddings (HyDE): This technique involves having an LLM create a hypothetical response to the query. Then, both the original query and the LLM’s response are fed into the retrieval stage.\n\nQuery expansion: This approach aims to enrich the user’s question by adding additional terms or concepts, resulting in different perspectives of the same initial question. For example, when searching for “disease,” you can leverage synonyms and related terms associated with the original query words and also include “illnesses” or “ailments.”\n\nSelf-query: The core idea is to map unstructured queries into structured ones. An LLM identifies key entities, events, and relationships within the input text. These identities are used as filtering parameters to reduce the vector search space (e.g., identify cities within the query, for example, “Paris,” and add it to your filter to reduce your vector search space).\n\nBoth data indexing and query optimization pre-retrieval optimization techniques depend highly on your data type, structure, and source. Thus, as with any data processing pipeline, no method always works, as every use case has its own particularities and gotchas. Optimizing your pre-retrieval RAG layer is experimental. Thus, what is essential is to try multiple methods (such as the ones enumerated in this section), reiterate, and observe what works best.\n\nOceanofPDF.com\n\nRetrieval\n\nThe retrieval step can be optimized in two fundamental ways:\n\nImproving the embedding models used in the RAG ingestion pipeline to encode the chunked documents and, at inference time, transform the user’s input.\n\nLeveraging the DB’s filter and search features. Thisstep will be used solely at inference time when you have to retrieve the most similar chunks based on user input.\n\nBoth strategies are aligned with our ultimate goal: to enhance the vector search step by leveraging the semantic similarity between the query and the indexed data.\n\nWhen improving the embedding models, you usually have to fine-tune the pre-trained embedding models to tailor them to specific jargon and nuances of your domain, especially for areas with evolving terminology or rare terms.\n\nInstead of fine-tuning the embedding model, you can leverage instructor models (https://huggingface.co/hkunlp/instructor-xl) to guide the\n\nembedding generation process with an instruction/prompt aimed at your domain. Tailoring your embedding network to your data using such a model can be a good option, as fine-tuning a model consumes more computing and human resources.\n\nIn the code snippet below, you can see an example of an Instructor model that embeds article titles about AI:\n\nfrom\n\nInstructorEmbedding\n\nimport\n\nINSTRUCTOR model = INSTRUCTOR(\n\n\"hkunlp/instructor-base\"\n\n) sentence =\n\n\"\n\nRAG Fundamentals First\"\n\ninstruction =\n\n\"Represent the title of an article about AI:\"\n\nembeddings = model.encode([[instruction, sentence]])\n\nprint\n\n(embeddings.shape)\n\n# noqa\n\n# Output: (1, 768)\n\nThe source code can be found at https://github.com/PacktPublishing/LLM- Engineering/blob/main/code_snippets/08_instructor_embeddings.py.\n\nTo run the instructor code, you have to create a different virtual environment and activate it:\n\npython3 -m venv instructor_venv && source instructor_venv/bin/activate\n\nAnd install the required Python dependencies:\n\npip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1\n\nOn the other side of the spectrum, here is how you can improve your retrieval by leveraging classic filter and search DB features:\n\nHybrid search: This is a vector and keyword-based search blend. Keyword-based search excels at identifying documents containing specific keywords. When your task demands pinpoint accuracy and the retrieved information must include exact keyword matches, hybrid search shines. Vector search, while powerful, can sometimes struggle with finding exact matches, but it excels at finding more general semantic similarities. You leverage both keyword matching and semantic similarities by combining the two methods. You have a parameter, usually called alpha, that controls the weight between the two methods. The algorithm has two independent searches, which are later normalized and unified.\n\nFiltered vector search: This type of search leverages the metadata index to filter for specific keywords within the metadata. It differs from a hybrid search in that you retrieve the data once using only the vector index and perform the filtering step before or after the vector search to reduce your search space.\n\nIn practice, on the retrieval side, you usually start with filtered vector search or hybrid search, as they are fairly quick to implement. This approach gives you the flexibility to adjust your strategy based on performance. If the results are not as expected, you can always fine-tune your embedding model.\n\nOceanofPDF.com\n\nPost-retrieval\n\nThe post-retrieval optimizations are solely performed on the retrieved data to ensure that the LLM’s performance is not compromised by issues such as limited context windows or noisy data. This is because the retrieved context can sometimes be too large or contain irrelevant information, both of which can distract the LLM.\n\nTwo popular methods performed at the post-retrieval step are:\n\nPrompt compression: Eliminate unnecessary details while keeping the essence of the data.\n\nRe-ranking: Use a cross-encoder ML model to give a matching score between the user’s input and every retrieved chunk. The retrieved items are sorted based on this score. Only the top N results are kept as the most relevant. As you can see in Figure 4.7, this works because the re-ranking model can find more complex relationships between the user input and some content than a simple similarity search. However, we can’t apply this model at the initial retrieval step because it is costly. That is why a popular strategy is to retrieve the data using a similarity distance between the embeddings and refine the retrieved information using a re-raking model, as illustrated in Figure 4.8.\n\nFigure 4.7: Bi-encoder (the standard embedding model) versus cross- encoder\n\nThe abovementioned techniques are far from an exhaustive list of all potential solutions. We used them as examples to get an intuition on what you can (and should) optimize at each step in your RAG workflow. The truth is that these techniques can vary tremendously by the type of data you work with.\n\nFor example, if you work with multi-modal data such as text and images, most of the techniques from earlier won’t work as they are designed for text only.\n\nFigure 4.8: The re-ranking algorithm\n\nTo summarize, the primary goal of these optimizations is to enhance the RAG algorithm at three key stages: pre-retrieval, retrieval, and post- retrieval. This involves preprocessing data for improved vector indexing, adjusting user queries for more accurate searches, enhancing the embedding model, utilizing classic filtering DB operations, and removing noisy data. By keeping these goals in mind, you can effectively optimize your RAG workflow for data processing and retrieval\n\nOceanofPDF.com\n\nExploring the LLM Twin’s RAG feature pipeline architecture\n\nNow that you have a strong intuition and understanding of RAG and its workings, we will continue exploring our particular LLM Twin use case. The goal is to provide a hands-on end-to-end example to solidify the theory presented in this chapter.\n\nAny RAG system is split into two independent components:\n\nThe ingestion pipeline takes in raw data, cleans, chunks, embeds, and loads it into a vector DB.\n\nThe inference pipeline queries the vector DB for relevant context and ultimately generates an answer by levering an LLM.\n\nIn this chapter, we will focus on implementing the RAG ingestion pipeline, and in Chapter 9, we will continue developing the inference pipeline.\n\nWith that in mind, let’s have a quick refresher on the problem we are trying to solve and where we get our raw data. Remember that we are building an end-to-end ML system. Thus, all the components talk to each other through an interface (or a contract), and each pipeline has a single responsibility. In\n\nour case, we ingest raw documents, preprocess them, and load them into a vector DB.\n\nOceanofPDF.com\n\nThe problem we are solving\n\nAs presented in the previous chapter, this book aims to show you how to build a production-ready LLM Twin backed by an end-to-end ML system. In this chapter specifically, we want to design a RAG feature pipeline that takes raw social media data (e.g., articles, code repositories, and posts) from our MongoDB data warehouse. The text of the raw documents will be cleaned, chunked, embedded, and ultimately loaded to a feature store. As discussed in Chapter 1, we will implement a logical feature store using ZenML artifacts and a Qdrant vector DB.\n\nAs we want to build a fully automated feature pipeline, we want to sync the data warehouse and logical feature store. Remember that, at inference time, the context used to generate the answer is retrieved from the vector DB. Thus, the speed of synchronization between the data warehouse and the feature store will directly impact the accuracy of our RAG algorithm.\n\nAnother key consideration is how to automate the feature pipeline and integrate it with the rest of our ML system. Our goal is to minimize any desynchronization between the two data storages, as this could potentially compromise the integrity of our system.\n\nTo conclude, we must design a feature pipeline that constantly syncs the data warehouse and logical feature store while processing the data accordingly. Having the data in a feature store is critical for a production- ready ML system. The LLM Twin inference pipeline will query it for RAG,\n\nwhile the training pipeline will consume tracked and versioned fine-tuning datasets from it.\n\nOceanofPDF.com\n\nThe feature store\n\nThe feature store will be the central access point for all the features used within the training and inference pipelines. The training pipeline will use the cleaned data from the feature store (stored as artifacts) to fine-tune LLMs. The inference pipeline will query the vector DB for chunked documents for RAG. That is why we are designing a feature pipeline and not only a RAG ingestion pipeline. In practice, the feature pipeline contains multiple subcomponents, one of which is the RAG logic.\n\nRemember that the feature pipeline is mainly used as a mind map to navigate the complexity of ML systems. It clearly states that it takes raw data as input and then outputs features and optional labels, which are stored in the feature store. Thus, a good intuition is to consider that all the logic between the data warehouse and the feature store goes into the feature pipeline namespace, consisting of one or more sub-pipelines. For example, we will implement another pipeline that takes in cleaned data, processes it into instruct datasets, and stores it in artifacts; this also sits under the feature pipeline umbrella as the artifacts are part of the logical feature store. Another example would be implementing a data validation pipeline on top of the raw data or computed features.\n\nAnother important observation to make is that text data stored as strings are not considered features if you follow the standard conventions. A feature is something that is fed directly into the model. For example, we would have to tokenize the instruct datasets or chunked documents to be considered features. Why? Because the tokens are fed directly to the model and not the sentences as strings. Unfortunately, this makes the system more complex\n\nand unflexible. Thus, we will do the tokenization at runtime. But this observation is important to understand as it’s a clear example that you don’t have to be too rigid about the feature/training/inference (FTI) architecture. You have to take it and adapt it to your own use case.\n\nOceanofPDF.com\n\nWhere does the raw data come from?\n\nAs a quick reminder, all the raw documents are stored in a MongoDB data warehouse. The data warehouse is populated by the data collection ETL pipeline presented in Chapter 3. The ETL pipeline crawls various platforms such as Medium and Substack, standardizes the data, and loads it into MongoDB. Check out Chapter 3 for more details on this topic.\n\nOceanofPDF.com\n\nDesigning the architecture of the RAG feature pipeline\n\nThe last step is to architect and go through the design of the RAG feature pipeline of the LLM Twin application. We will use a batch design scheduled to poll data from the MongoDB data warehouse, process it, and load it to a Qdrant vector DB. The first question to ask ourselves is, “Why a batch pipeline?”\n\nBut before answering that, let’s quickly understand how a batch architecture works and behaves relative to a streaming design.\n\nFigure 4.9: The architecture of the LLM Twin’s RAG feature pipeline\n\nOceanofPDF.com\n\nBatch pipelines\n\nA batch pipeline in data systems refers to a data processing method where data is collected, processed, and stored in predefined intervals and larger volumes, also known as “batches”. This approach differs from real-time or streaming data processing, where data is processed continuously as it arrives. This is what happens in a batch pipeline:\n\nData collection: Data is collected from various sources and stored until sufficient amounts are accumulated for processing. This can include data from DBs, logs, files, and other sources.\n\nScheduled processing: Data processing is scheduled at regular intervals, for example, hourly or daily. During this time, the collected data is processed in bulk. This can involve data cleansing, transformation, aggregation, and other operations.\n\nData loading: After processing, the data is loaded into the target system, such as a DB, data warehouse, data lake, or feature store. This processed data is then available for analysis, querying, or further processing.\n\nBatch pipelines are particularly useful when dealing with large volumes of data that do not require immediate processing. They offer several\n\nadvantages, including:\n\nEfficiency: Batch processing can handle large volumes of data more efficiently than real-time processing, allowing for optimized resource allocation and parallel processing.\n\nComplex processing: Batch pipelines can perform complex data transformations and aggregations that might be too resource-intensive for real-time processing.\n\nSimplicity: Batch processing systems’ architectures are often simpler than those of real-time systems, making them easier to implement and maintain.\n\nOceanofPDF.com\n\nBatch versus streaming pipelines\n\nWhen implementing feature pipelines, you have two main design choices: batch and streaming. Thus, it is worthwhile to see the difference between the two and understand why we chose a batch architecture over a streaming one for our LLM Twin use case.\n\nYou can effortlessly write a dedicated chapter on streaming pipelines, which suggests its complexity over a batch design. However, as streaming architectures become increasingly popular, one must have an intuition of how they work to choose the best option for your application.\n\nThe core elements of streaming applications are a distributed event streaming platform such as Apache Kafka or Redpanda to store events from multiple clients and a streaming engine such as Apache Flink or Bytewax to process the events. To simplify your architecture, you can swap your event streaming platform with queues, such as RabbitMQ, to store the events until processed. Table 4.1 compares batch and streaming pipelines based on multiple criteria such as processing schedule and complexity:\n\nAspect\n\nBatch pipeline\n\nProcessing schedule\n\nProcesses data at regular intervals (e.g., every minute, hourly, daily).\n\nEfficiency\n\nHandles large volumes of data more efficiently, optimizing resource allocation and parall\n\nProcessing complexity Capable of performing complex data transformations and aggregations.\n\nUse cases\n\nSuitable for scenarios where immediate data processing is not critical. Commonly used in\n\nSystem complexity\n\nCompared to streaming pipelines, systems are generally simpler to implement and maint\n\nTable 4.1: Batch versus streaming pipelines\n\nFor example, streaming pipelines are extremely powerful in social media recommender systems like TikTok. When using social media, user behavior changes frequently. A typical scenario is that you want to relax at a certain point in time and mostly look at videos of puppies. Still, after 15 minutes, you get bored and want something more serious, such as educative content or news. This means the recommender system has to capture these behavior changes without delay to keep you engaged. As the transition between interests is cyclical and not predictable, you can’t use a batch pipeline that runs every 30 minutes or every hour to generate more content. You can run it every minute to create new content, but, at the same time, it will result in unnecessary costs, as most predictions will not be consumed. By implementing a streaming pipeline, you update the features of specific users in real time, which are then passed to a chain of models that predict the new recommendations.\n\nStreaming architectures are also the backbone of real-time fraud detection algorithms, such as those used at Stripe or PayPal. In this context, it’s critical to identify potentially fraudulent transactions as they occur, not after a few minutes or hours as a batch pipeline would process them. The same urgency applies to high-frequency trading platforms that make stock predictions based on the constant influx of market data, enabling traders to make decisions within milliseconds.\n\nOn the other hand, you can use a batch architecture for an offline recommender system. For example, when implementing one for an e-commerce or streaming platform, you don’t need the system to be so reactive, as the user’s behavior rarely changes. Thus, updating the recommendations periodically, such as every night, based on historical user behavior data using a batch pipeline is easier to implement and cheaper.\n\nAnother popular example of batch pipelines is the ETL design used to extract, transform, and load data for different use cases. The ETL design is widespread in data pipelines used to move data from one DB to another. Some practical use cases include aggregating data for analytics, where you have to extract data from multiple sources, aggregate it, and load it to a data warehouse connected to a dashboard. The analytics domains can be widespread, from e-commerce and marketing to finance and research.\n\nThe data collection pipeline used in the LLM Twin use case is another example of an ETL pipeline that extracts data from the internet, structures it, and loads it into a data warehouse for future processing.\n\nAlong with prediction or feature freshness, another disadvantage of batch pipelines over streaming ones is that you usually make redundant predictions. Let’s take the example of a recommender system for a streaming platform like Netflix. Every night, you make the predictions for all users. There is a significant chance that a large chunk of users won’t log in that day. Also, users usually don’t browse all the recommendations but stick to the first ones. Thus, only a portion of predictions are used, wasting computing power on all the others.\n\nThat’s why a popular strategy is to start with a batch architecture, as it’s faster and easier to implement. After the product is in place, you gradually move to a streaming design to reduce costs and improve the user experience.\n\nTo conclude, we have used a batch architecture (and not a streaming one) to implement the LLM Twin’s feature pipeline for the following reasons:\n\nDoes not require immediate data processing: Even if syncing the data warehouse and feature store is critical for an accurate RAG system, a delay of a few minutes is acceptable. Thus, we can schedule the batch pipeline to run every minute, constantly syncing the two data storages. This technique works because the data volume is small. The whole data warehouse will have only thousands of records, not millions or billions. Hence, we can quickly iterate through them and sync the two DBs.\n\nSimplicity: As stated earlier, implementing a streaming pipeline is two times more complex. In the real world, you want to keep your system as simple as possible, making it easier to understand, debug, and maintain. Also,\n\nsimplicity usually translates to lower infrastructure and development costs.\n\nIn Figure 8.10, we compare what tools you can use based on your architecture (streaming versus batch) and the quantity of data you have to process (small versus big data). In our use case, we are in the smaller data and batch quadrant, where we picked a combination of vanilla Python and generative AI tools such as LangChain, Sentence Transformers, and Unstructured.\n\nFigure 4.10: Tools on the streaming versus batch and smaller versus bigger data spectrum\n\nIn the Change data capture: syncing the data warehouse and feature store\n\nsection later in this chapter, we will discuss when switching from a batch architecture to a streaming one makes sense.\n\nOceanofPDF.com\n\nCore steps\n\nMost of the RAG feature pipelines are composed of five core steps. The one implemented in the LLM Twin architecture makes no exception. Thus, you can quickly adapt this pattern for other RAG applications, but here is what the LLM Twin’s RAG feature pipeline looks like:\n\nData extraction: Extract the latest articles, code repositories, and posts from the MongoDB data warehouse. At the extraction step, you usually aggregate all the data you need for processing.\n\nCleaning: The data from the data warehouse is standardized and partially clean, but we have to ensure that the text contains only useful information, is not duplicated, and can be interpreted by the embedding model. For example, we must clean and normalize all non-ASCII characters before passing the text to the embedding model. Also, to keep the information semantically dense, we decided to replace all the URLs with placeholders and remove all emojis. The cleaning step is more art than science. Hence, after you have the first iteration with an evaluation mechanism in place, you will probably reiterate and improve it.\n\nChunking: You must adopt various chunking strategies based on each data category and embedding model. For example, when working with code repositories, you want the chunks broader, whereas when working with articles, you want them narrower or scoped at the paragraph level. Depending on your data, you must decide if you split your document based\n\non the chapter, section, paragraph, sentence, or just a fixed window size. Also, you have to ensure that the chunk size doesn’t exceed the maximum input size of the embedding model. That is why you usually chunk a document based on your data structure and the maximum input size of the model.\n\nEmbedding: You pass each chunk individually to an embedding model of your choice. Implementation-wise, this step is usually the simplest, as tools such as SentenceTransformer and Hugging Face provide high-level interfaces for most embedding models. As explained in the What are embeddings? section of this chapter, at this step, the most critical decisions are to decide what model to use and whether to fine-tune it or not. For example, we used an\n\n\"all-mpnet-base-v2\"\n\nembedding model from SentenceTransformer, which is relatively tiny and runs on most machines. However, we provide a configuration file where you can quickly configure the embedding model with something more powerful based on the state of the art when reading this book. You can quickly find other options on the MTEB on Hugging Face (https://huggingface.co/spaces/mteb/leaderboard).\n\nData loading: The final step combines the embedding of a chunked document and its metadata, such as the author and the document ID, content, URL, platform, and creation date. Ultimately, we wrap the vector and the metadata into a structure compatible with Qdrant and push it to the vector DB. As we want to use Qdrant as the single source of truth for the features, we also push the cleaned documents (before chunking) to Qdrant. We can push data without vectors, as the metadata index of Qdrant behaves like a NoSQL DB. Thus, pushing metadata without a vector attached to it is like using a standard NoSQL engine.\n\nOceanofPDF.com\n\nChange data capture: syncing the data warehouse and feature store\n\nAs highlighted a few times in this chapter, data is constantly changing, which can result in DBs, data lakes, data warehouses, and feature stores getting out of sync. Change data capture (CDC) is a strategy that allows you to optimally keep two or more data storage types in sync without computing and I/O overhead. It captures any CRUD operation done on the source DB and replicates it on a target DB. Optionally, you can add preprocessing steps in between the replication.\n\nThe syncing issues also apply when building a feature pipeline. One key design choice concerns how to sync the data warehouse with the feature store to have data fresh enough for your particular use case.\n\nIn our LLM Twin use case, we chose a naïve approach out of simplicity. We implemented a batch pipeline that is triggered periodically or manually. It reads all the raw data from the data warehouse, processes it in batches, and inserts new records or updates old ones from the Qdrant vector DB. This works fine when you are working with a small number of records, at the order of thousands or tens of thousands. But our naïve approach raises the following questions:\n\nWhat happens if the data suddenly grows to millions of records (or higher)?\n\nWhat happens if a record is deleted from the data warehouse? How is this reflected in the feature store?\n\nWhat if we want to process only the new or updated items from the data warehouse and not all of them?\n\nFortunately, the CDC pattern can solve all of these issues. When implementing CDC, you can take multiple approaches, but all of them use either a push or pull strategy:\n\nPush: The source DB is the primary driver in the push approach. It actively identifies and transmits data modifications to target systems for processing. This method ensures near-instantaneous updates at the target, but data loss can occur if target systems are inaccessible. To mitigate this, a messaging system is typically employed as a buffer.\n\nPull: The pull method assigns a more passive role to the source DB, which only records data changes. Target systems periodically request these changes and handle updates accordingly. While this approach lightens the load on the source, it introduces a delay in data propagation. A messaging system is again essential to prevent data loss during periods of target system unavailability.\n\nIn summary, the push method is ideal for applications demanding immediate data access, whereas the pull method is better suited for large- scale data transfers where real-time updates aren’t critical. With that in\n\nmind, there are different methods to detect changes in data. Thus, let’s list the main CDC patterns that are used in the industry:\n\nTimestamp-based: The approach involves adding a modification time column to DB tables, usually called\n\nLAST_MODIFIED\n\nor\n\nLAST_UPDATED\n\n. Downstream systems can query this column to identify records that have been updated since their last check. While simple to implement, this method is limited to tracking changes, not deletions, and imposes performance overhead due to the need to scan entire tables.\n\nTrigger-based: The trigger-based approach utilizes DB triggers to automatically record data modifications in a separate table upon INSERT, UPDATE, or DELETE operations, often known as the event table. This method provides comprehensive change tracking but can impact the DB performance due to the additional write operations involved for each event.\n\nLog-based: DBs maintain transaction logs to record all data modifications, including timestamps. Primarily used for recovery, these logs can also be leveraged to propagate changes to target systems in real time. This approach minimizes the performance impact on the source DB. As a huge advantage, it avoids additional processing overhead on the source DB, captures all data changes, and requires no schema modification. But on the opposite side, it lacks standardized log formats, leading to vendor-specific implementations.\n\nFor more details on CDC, I recommend What is Change Data Capture? from Confluent’s blog: https://www.confluent.io/en-gb/learn/change-data- capture/.\n\nWith these CDC techniques in mind, we could quickly implement a pull timestamp-based strategy in our RAG feature pipeline to sync the data warehouse and feature store more optimally when the data grows. Our implementation is still pull-based but doesn’t check any last updated field in the source DB; it just pulls everything from the data warehouse.\n\nHowever, the most popular and optimal technique in the industry is the log- based one. It doesn’t add any I/O overhead to the source DB, has low latency, and supports all CRUD operations. The biggest downside is its development complexity, which requires a queue to capture all the CRUD events and a streaming pipeline to process them.\n\nAs this is an LLM book and not a data engineering one, we wanted to keep things simple, but it’s important to know that these techniques exist, and you can always upgrade your current implementation when it doesn’t fit your application requirements anymore.\n\nOceanofPDF.com\n\nWhy is the data stored in two snapshots?\n\nWe store two snapshots of our data in the logical feature store:\n\nAfter the data is cleaned: For fine-tuning LLMs\n\nAfter the documents are chunked and embedded: For RAG\n\nWhy did we design it this way? Remember that the features should be accessed solely from the feature store for training and inference. Thus, this adds consistency to our design and makes it cleaner.\n\nAlso, storing the data cleaned specifically for our fine-tuning and embedding use case in the MongoDB data warehouse would have been an antipattern. The data from the warehouse is shared all across the company. Thus, processing it for a specific use case is not good practice. Imagine another summarization use case where we must clean and preprocess the data differently. We must create a new “Cleaned Data” table prefixed with the use case name. We have to repeat that for every new use case. Therefore, to avoid having a spaghetti data warehouse, the data from the data warehouse is generic and is modeled to specific applications only in downstream components, which, in our case, is the feature store.\n\nUltimately, as we mentioned in the Core steps section, you can leverage the metadata index of a vector DB as a NoSQL DB. Based on these factors, we decided to keep the cleaned data in Qdrant, along with the chunked and embedded versions of the documents.\n\nAs a quick reminder, when operationalizing our LLM Twin system, the create instruct dataset pipeline, explained in Chapter 5, will read the cleaned documents from Qdrant, process them, and save them under a versioned ZenML artifact. The training pipeline requires a dataset and not plain documents. This is a reminder that our logical feature store comprises the Qdrant vector DB for online serving and ZenML artifacts for offline training.\n\nOceanofPDF.com\n\nOrchestration\n\nZenML will orchestrate the batch RAG feature pipeline. Using ZenML, we can schedule it to run on a schedule, for example, every hour, or quickly manually trigger it. Another option is to trigger it after the ETL data collection pipeline finishes.\n\nBy orchestrating the feature pipeline and integrating it into ZenML (or any other orchestration tool), we can operationalize the feature pipeline with the end goal of continuous training (CT).\n\nWe will go into all the details of orchestration, scheduling, and CT in Chapter 11.\n\nOceanofPDF.com\n\nImplementing the LLM Twin’s RAG feature pipeline\n\nThe last step is to review the LLM Twin’s RAG feature pipeline code to see how we applied everything we discussed in this chapter. We will walk you through the following:\n\nZenML code\n\nPydantic domain objects\n\nA custom object-vector mapping (OVM) implementation\n\nThe cleaning, chunking, and embedding logic for all our data categories\n\nWe will take a top-down approach. Thus, let’s start with the Settings class and ZenML pipeline.\n\nOceanofPDF.com\n\nSettings\n\nWe use Pydantic Settings (https://docs.pydantic.dev/latest/concepts/pydantic_settings/) to define a global Settings class that loads sensitive or non-sensitive variables from a\n\n.env\n\nfile. This approach also gives us all the benefits of Pydantic, such as type validation. For example, if we provide a string for the\n\nQDRANT_DATABASE_PORT\n\nvariable instead of an integer, the program will crash. This behavior makes the whole application more deterministic and reliable.\n\nHere is what the\n\nSettings\n\nclass looks like with all the variables necessary to build the RAG feature pipeline:\n\nfrom\n\npydantic\n\nimport\n\nBaseSettings\n\nclass\n\nSettings\n\n(\n\nBaseSettings\n\n):\n\nclass\n\nConfig\n\n: env_file =\n\n\".env\"\n\nenv_file_encoding =\n\n\"utf-8\"\n\n…\n\n# Some other settings…\n\n# RAG\n\nTEXT_EMBEDDING_MODEL_ID:\n\nstr\n\n=\n\n\"sentence-transformers/all-MiniLM-L6-v2\"\n\nRERANKING_CROSS_ENCODER_MODEL_ID:\n\nstr\n\n=\n\n\"cross-encoder/ms-marco-MiniLM-L-4-v2\"\n\nRAG_MODEL_DEVICE:\n\nstr\n\n=\n\n\"cpu\"\n\n# QdrantDB Vector DB\n\nUSE_QDRANT_CLOUD:\n\nbool\n\n=\n\nFalse\n\nQDRANT_DATABASE_HOST:\n\nstr\n\n=\n\n\"localhost\"\n\nQDRANT_DATABASE_PORT:\n\nint\n\n=\n\n6333\n\nQDRANT_CLOUD_URL:\n\nstr\n\n=\n\n\"str\"\n\nQDRANT_APIKEY:\n\nstr\n\n|\n\nNone\n\n=\n\nNone\n\n…\n\n# More settings…\n\nsettings = Settings()\n\nAs stated in the internal Config class, all the variables have default values or can be overridden by providing a\n\n.env\n\nfile.\n\nOceanofPDF.com\n\nZenML pipeline and steps\n\nThe ZenML pipeline is the entry point for the RAG feature engineering pipeline. It reflects the five core phases of RAG ingestion code: extracting raw documents, cleaning, chunking, embedding, and loading them to the logical feature store. The calls within the\n\nfeature_engineering()\n\nfunction are ZenML steps, representing a single execution unit performing the five phases of RAG. The code is available in the GitHub repository at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/pipelines/feature_engineering.py:\n\nfrom\n\nzenml\n\nimport\n\npipeline\n\nfrom\n\nllm_engineering.interfaces.orchestrator.steps\n\nimport\n\nfeature_engineering\n\nas\n\nfe_steps\n\n@pipeline\n\ndef\n\nfeature_engineering\n\n(\n\nauthor_full_names:\n\nlist\n\n[\n\nstr\n\n]\n\n) ->\n\nNone\n\n: raw_documents = fe_steps.query_data_warehouse(author_full_names) cleaned_documents = fe_steps.clean_documents(raw_documents) last_step_1 = fe_steps.load_to_vector_db(cleaned_documents) embedded_documents = fe_steps.chunk_and_embed(cleaned_documents) last_step_2 = fe_steps.load_to_vector_db(embedded_documents)\n\nreturn\n\n[last_step_1.invocation_id, last_step_2.invocation_id]\n\nFigure 4.11 shows how multiple feature engineering pipeline runs look in ZenML’s dashboard.\n\nFigure 4.11: Feature pipeline runs in the ZenML dashboard\n\nFigure 8.12 shows the DAG of the RAG feature pipeline, where you can follow all the pipeline steps and their output artifacts. Remember that whatever is returned from a ZenML step is automatically saved as an artifact, stored in ZenML’s artifact registry, versioned, and shareable across the application.\n\nFigure 4.12: Feature pipeline DAG in the ZenML dashboard\n\nThe final puzzle piece is understanding how to configure the RAG feature pipeline dynamically. All its available settings are exposed as function parameters. Here, we need only a list of author’s names, as seen in the function’s signature:\n\nfeature_engineering(author_full_names: list[str])\n\n. We inject a YAML configuration file at runtime that contains all the necessary values based on different use cases. For example, the following configuration includes a list of all the authors of this book as we want to populate the feature store with data from all of us (available in the GitHub repository at\n\nconfigs/feature_engineering.yaml\n\n):\n\nparameters: author_full_names: - Alex Vesa - Maxime Labonne - Paul Iusztin\n\nThe beauty of this approach is that you don’t have to modify the code to configure the feature pipeline with different input values. You have to provide a different configuration file when running it, as follows:\n\nfeature_engineering.with_options(config_path=\"…/feature_engineering.ya ml\")()\n\nYou can either hardcode the path to the config file or provide the\n\nconfig_path\n\nfrom the CLI, which allows you to modify the pipeline’s configuration between different runs. Out of simplicity, we hard-coded the configuration file. Thus, we can call the feature engineering pipeline calling the\n\nrun.py\n\nscript as follows:\n\npython -m tools.run --no-cache --run-feature-engineering\n\nHowever, you can easily add another CLI argument to pass the\n\nconfig_path\n\nvariable. Also, you can run the feature pipeline using the following\n\npoe\n\ncommand:\n\npoetry poe run-feature-engineering-pipeline\n\nLet’s move forward to the ZenML steps and sequentially zoom in on all of them. The source code for all the feature engineering pipeline steps is available on GitHub at\n\n\"steps/feature_engineering\"\n\n. We will begin with the first step, which involves querying the data warehouse for new content to process into features.\n\nOceanofPDF.com\n\nQuerying the data warehouse\n\nThe first thing to notice is that a step is a Python function decorated with\n\n@step\n\n, similar to how a ZenML pipeline works. The function below takes as input a list of authors’ full names and performs the following core steps:\n\nIt attempts to get or create a\n\nUserDocument\n\ninstance using the first and last names, appending this instance to the authors list. If the user doesn’t exist, it throws an error.\n\nIt fetches all the raw data for the user from the data warehouse and extends the\n\ndocuments\n\nlist to include these user documents.\n\nUltimately, it computes a descriptive metadata dictionary logged and tracked in ZenML.\n\n…\n\n# other imports\n\nfrom\n\nzenml\n\nimport\n\nget_step_context, step\n\n@step\n\ndef\n\nquery_data_warehouse\n\n(\n\nauthor_full_names:\n\nlist\n\n[\n\nstr\n\n],\n\n) -> Annotated[\n\nlist\n\n,\n\n\"raw_documents\"\n\n]: documents = [] authors = []\n\nfor\n\nauthor_full_name\n\nin\n\nauthor_full_names: logger.info(\n\nf\"Querying data warehouse for user:\n\n{author_full_name}\n\n\"\n\n) first_name, last_name = utils.split_user_full_name(author_full_name) logger.info(\n\nf\"First name:\n\n{first_name}\n\n, Last name:\n\n{last_name}\n\n\"\n\n) user = UserDocument.get_or_create(first_name=first_name, last_name=last_name) authors.append(user) results = fetch_all_data(user) user_documents = [doc\n\nfor\n\nquery_result\n\nin\n\nresults.values()\n\nfor\n\ndoc\n\nin\n\nquery_result] documents.extend(user_documents) step_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"raw_documents\"\n\n, metadata=_get_metadata(documents))\n\nreturn\n\ndocuments\n\nThe fetch function leverages a thread pool that runs each query on a different thread. As we have multiple data categories, we have to make a different query for the articles, posts, and repositories, as they are stored in different collections. Each query calls the data warehouse, which is bounded by the network I/O and data warehouse latency, not by the machine’s CPU. Thus, by moving each query to a different thread, we can parallelize them. Ultimately, instead of adding the latency of each query as the total timing, the time to run this fetch function will be the max between all the calls.\n\nUsing threads to parallelize I/O-bounded calls is good practice in Python, as they are not locked by the Python Global Interpreter Lock (GIL). In contrast, adding each call to a different process would add too much overhead, as a process takes longer to spin off than a thread.\n\nIn Python, you want to parallelize things with processes only when the operations are CPU or memory-bound because the GIL affects them. Each process has a different GIL. Thus, parallelizing your computing logic, such as processing a batch of documents or images already loaded in memory, isn’t affected by Python’s GIL limitations.\n\ndef\n\nfetch_all_data\n\n(\n\nuser: UserDocument\n\n) ->\n\ndict\n\n[\n\nstr\n\n,\n\nlist\n\n[NoSQLBaseDocument]]: user_id =\n\nstr\n\n(user.\n\nid\n\n)\n\nwith\n\nThreadPoolExecutor()\n\nas\n\nexecutor: future_to_query = { executor.submit(__fetch_articles, user_id):\n\n\"articles\"\n\n, executor.submit(__fetch_posts, user_id):\n\n\"posts\"\n\n, executor.submit(__fetch_repositories, user_id):\n\n\"repositories\"\n\n, } results = {}\n\nfor\n\nfuture\n\nin\n\nas_completed(future_to_query): query_name = future_to_query[future]\n\ntry\n\n: results[query_name] = future.result()\n\nexcept\n\nException: logger.exception(\n\nf\"'\n\n{query_name}\n\n' request failed.\"\n\n) results[query_name] = []\n\nreturn\n\nresults\n\nThe\n\n_get_metadata()\n\nfunction takes the list of queried documents and authors and counts the number of them relative to each data category:\n\ndef\n\n_get_metadata\n\n(\n\ndocuments:\n\nlist\n\n[Document]\n\n) ->\n\ndict\n\n: metadata = {\n\n\"num_documents\"\n\n:\n\nlen\n\n(documents), }\n\nfor\n\ndocument\n\nin\n\ndocuments: collection = document.get_collection_name()\n\nif\n\ncollection\n\nnot\n\nin\n\nmetadata: metadata[collection] = {}\n\nif\n\n\"authors\"\n\nnot\n\nin\n\nmetadata[collection]: metadata[collection][\n\n\"authors\"\n\n] =\n\nlist\n\n() metadata[collection][\n\n\"\n\nnum_documents\"\n\n] = metadata[collection].get(\n\n\"num_documents\"\n\n,\n\n0\n\n) +\n\n1\n\nmetadata[collection][\n\n\"authors\"\n\n].append(document.author_full_name)\n\nfor\n\nvalue\n\nin\n\nmetadata.values():\n\nif\n\nisinstance\n\n(value,\n\ndict\n\n)\n\nand\n\n\"authors\"\n\nin\n\nvalue: value[\n\n\"authors\"\n\n] =\n\nlist\n\n(\n\nset\n\n(value[\n\n\"authors\"\n\n]))\n\nreturn\n\nmetadata\n\nWe will expose this metadata in the ZenML dashboard to quickly see some statistics on the loaded data. For example, in Figure 4.13, we accessed the metadata tab of the\n\nquery_data_warehouse()\n\nstep, where you can see that, within that particular run of the feature pipeline, we loaded 76 documents from three authors. This is extremely powerful for monitoring and debugging batch pipelines.\n\nYou can always extend it with anything that makes sense for your use case.\n\nFigure 4.13: Metadata of the “query the data warehouse” ZenML step\n\nOceanofPDF.com\n\nCleaning the documents\n\nIn the cleaning step, we iterate through all the documents and delegate all the logic to a\n\nCleaningDispatcher\n\nwho knows what cleaning logic to apply based on the data category. Remember that we want to apply, or have the ability to apply in the future, different cleaning techniques on articles, posts, and code repositories.\n\n@step\n\ndef\n\nclean_documents\n\n(\n\ndocuments: Annotated[\n\nlist\n\n,\n\n\"\n\nraw_documents\"\n\n],\n\n) -> Annotated[\n\nlist\n\n,\n\n\"cleaned_documents\"\n\n]: cleaned_documents = []\n\nfor\n\ndocument\n\nin\n\ndocuments: cleaned_document = CleaningDispatcher.dispatch(document) cleaned_documents.append(cleaned_document) step_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"cleaned_documents\"\n\n, metadata=_get_metadata(cleaned_documents))\n\nreturn\n\ncleaned_documents\n\nThe computed metadata is similar to what we logged in the\n\nquery_data_warehouse()\n\nstep. Thus, let’s move on to chunking and embedding.\n\nOceanofPDF.com\n\nChunk and embed the cleaned documents\n\nSimilar to how we cleaned the documents, we delegate the chunking and embedding logic to a dispatcher who knows how to handle each data category. Note that the chunking dispatcher returns a list instead of a single object, which makes sense as the document is split into multiple chunks. We will dig into the dispatcher in the “The dispatcher layer” section of this chapter.\n\n@step\n\ndef\n\nchunk_and_embed\n\n(\n\ncleaned_documents: Annotated[\n\nlist\n\n,\n\n\"cleaned_documents\"\n\n],\n\n) -> Annotated[\n\nlist\n\n,\n\n\"embedded_documents\"\n\n]: metadata = {\n\n\"chunking\"\n\n: {},\n\n\"embedding\"\n\n: {},\n\n\"num_documents\"\n\n:\n\nlen\n\n(cleaned_documents)} embedded_chunks = []\n\nfor\n\ndocument\n\nin\n\ncleaned_documents: chunks = ChunkingDispatcher.dispatch(document) metadata[\n\n\"chunking\"\n\n] = _add_chunks_metadata(chunks, metadata[\n\n\"chunking\"\n\n])\n\nfor\n\nbatched_chunks\n\nin\n\nutils.misc.batch(chunks,\n\n10\n\n): batched_embedded_chunks = EmbeddingDispatcher.dispatch(batched_chunks) embedded_chunks.extend(batched_embedded_chunks) metadata[\n\n\"embedding\"\n\n] = _add_embeddings_metadata(embedded_chunks, metadata[\n\n\"embedding\"\n\n]) metadata[\n\n\"num_chunks\"\n\n] =\n\nlen\n\n(embedded_chunks) metadata[\n\n\"num_embedded_chunks\"\n\n] =\n\nlen\n\n(embedded_chunks) step_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"embedded_documents\"\n\n, metadata=metadata)\n\nreturn\n\nembedded_chunks\n\nIn Figure 4.14, you can see the metadata of the chunking and embedding ZenML step. For example, you can quickly understand that we transformed 76 documents into 2,373 chunks, or the properties we used for chunking articles, such as a\n\nchunk_size\n\nof 500 and a\n\nchunk_overlap\n\nof 50.\n\nFigure 4.14: Metadata of the embedding and chunking ZenML step, detailing the uncategorized and chunking dropdowns\n\nIn Figure 4.15, the rest of the ZenML metadata from the embedding and chunking step details the embedding model and its properties used to compute the vectors.\n\nFigure 4.15: Metadata of the embedding and chunking ZenML step, detailing the embedding dropdown\n\nAs ML systems can break at any time while in production due to drifts or untreated use cases, leveraging the metadata section to monitor the ingested data can be a powerful tool that will save debugging days, translating to tens of thousands of dollars or more for your business.\n\nOceanofPDF.com\n\nLoading the documents to the vector DB\n\nAs each article, post, or code repository sits in a different collection inside the vector DB, we have to group all the documents based on their data category. Then, we load each group in bulk in the Qdrant vector DB:\n\n@step\n\ndef\n\nload_to_vector_db\n\n(\n\ndocuments: Annotated[\n\nlist\n\n,\n\n\"documents\"\n\n],\n\n) ->\n\nNone\n\n: logger.info(\n\nf\"Loading\n\n{\n\nlen\n\n(documents)}\n\ndocuments into the vector database.\"\n\n) grouped_documents = VectorBaseDocument.group_by_class(documents)\n\nfor\n\ndocument_class, documents\n\nin\n\ngrouped_documents.items(): logger.info(\n\nf\"Loading documents into\n\n{document_class.get_collection_name()}\n\n\"\n\n)\n\nfor\n\ndocuments_batch\n\nin\n\nutils.misc.batch(documents, size=\n\n4\n\n):\n\ntry\n\n: document_class.bulk_insert(documents_batch)\n\nexcept\n\nException:\n\nreturn\n\nFalse\n\nreturn\n\nTrue\n\nOceanofPDF.com\n\nPydantic domain entities\n\nBefore investigating the dispatchers, we must understand the domain objects we work with. To some extent, in implementing the LLM Twin, we are following the domain-driven design (DDD) principles, which state that domain entities are the core of your application. Thus, before proceeding, it’s important to understand the hierarchy of the domain classes we are working with.\n\nThe code for the domain entities is available on GitHub at https://github.com/PacktPublishing/LLM- Engineering/tree/main/llm_engineering/domain.\n\nWe used Pydantic to model all our domain entities. When we wrote the book, choosing Pydantic was a no-brainer, as it is the go-to Python package for writing data structures with out-of-the-box type validation. As Python is a dynamically typed language, using Pydantic for type validation at runtime makes your system order of times more robust, as you can be sure that you are always working with the right type of data.\n\nThe domain of our LLM Twin application is split into two dimensions:\n\nThe data category: Post, article, and repository\n\nThe state of the data: Cleaned, chunked, and embedded\n\nWe decided to create a base class for each state of the document, resulting in having the following base abstract classes:\n\nclass CleanedDocument(VectorBaseDocument, ABC)\n\nclass Chunk(VectorBaseDocument, ABC)\n\nclass EmbeddedChunk(VectorBaseDocument, ABC)\n\nNote that all of them inherit the\n\nVectorBaseDocument\n\nclass, which is our custom OVM implementation, which we will explain in the next section of this chapter. Also, it inherits from ABC, which makes the class abstract. Thus, you cannot initialize an object out of these classes; you may only inherit from them. That is why base classes are always marked as abstract.\n\nEach base abstract class from above (which models the state) will have a subclass that adds the data category dimension. For example, the\n\nCleanedDocument\n\nclass will have the following subclasses:\n\nclass CleanedPostDocument(CleanedDocument)\n\nclass CleanedArticleDocument(CleanedDocument)\n\nclass CleanedRepositoryDocument(CleanedDocument)\n\nAs we can see in Figure 8.16, we will repeat the same logic for the\n\nChunk\n\nand\n\nEmbeddedChunk\n\nbase abstract classes. We will implement a specific document class for each data category and state combination, resulting in nine types of domain entities. For example, when ingesting a raw document, the cleaning step will yield a\n\nCleanedArticleDocument\n\ninstance, the chunking step will return a list of\n\nArticleChunk\n\nobjects, and the embedding operation will return\n\nEmbeddedArticleChunk\n\ninstances that encapsulate the embedding and all the necessary metadata to ingest in the vector DB.\n\nThe same will happen for the posts and repositories.\n\nFigure 4.16: Domain entities class hierarchy and their interaction\n\nWe chose this design because the list of states will rarely change, and we want to extend the list of data categories. Thus, structuring the classes after the state allows us to plug another data category by inheriting these base abstract classes.\n\nLet’s see the complete code for the hierarchy of the cleaned document. All the attributes of a cleaned document will be saved within the metadata of the vector DB. For example, the metadata of a cleaned article document will always contain the content, platform, author ID, author full name, and link of the article.\n\nAnother fundamental aspect is the\n\nConfig\n\ninternal class, which defines the name of the collection within the vector DB, the data category of the entity, and whether to leverage the vector index when creating the collection:\n\nclass\n\nCleanedDocument\n\n(VectorBaseDocument, ABC): content:\n\nstr\n\nplatform:\n\nstr\n\nauthor_id: UUID4 author_full_name:\n\nstr\n\nclass\n\nCleanedPostDocument\n\n(\n\nCleanedDocument\n\n): image:\n\nOptional\n\n[\n\nstr\n\n] =\n\nNone\n\nclass\n\nConfig\n\n: name =\n\n\"cleaned_posts\"\n\ncategory = DataCategory.POSTS use_vector_index =\n\nFalse\n\nclass\n\nCleanedArticleDocument\n\n(\n\nCleanedDocument\n\n): link:\n\nstr\n\nclass\n\nConfig\n\n: name =\n\n\"cleaned_articles\"\n\ncategory = DataCategory.ARTICLES use_vector_index =\n\nFalse\n\nclass\n\nCleanedRepositoryDocument\n\n(\n\nCleanedDocument\n\n): name:\n\nstr\n\nlink:\n\nstr\n\nclass\n\nConfig\n\n: name =\n\n\"cleaned_repositories\"\n\ncategory = DataCategory.REPOSITORIES use_vector_index =\n\nFalse\n\nTo conclude this section, let’s also take a look at the base abstract class of the chunk and embedded chunk:\n\nclass\n\nChunk\n\n(VectorBaseDocument, ABC): content:\n\nstr\n\nplatform:\n\nstr\n\ndocument_id: UUID4 author_id: UUID4 author_full_name:\n\nstr\n\nmetadata:\n\ndict\n\n= Field(default_factory=\n\ndict\n\n) …\n\n# PostChunk, ArticleChunk, RepositoryChunk\n\nclass\n\nEmbeddedChunk\n\n(VectorBaseDocument, ABC): content:\n\nstr\n\nembedding:\n\nlist\n\n[\n\nfloat\n\n] |\n\nNone\n\nplatform:\n\nstr\n\ndocument_id: UUID4 author_id: UUID4 author_full_name:\n\nstr\n\nmetadata:\n\ndict\n\n= Field(default_factory=\n\ndict\n\n) …\n\n# EmbeddedPostChunk, EmbeddedArticleChunk, EmbeddedRepositoryChunk\n\nWe also defined an enum that aggregates all our data categories in a single structure of constants:\n\nclass\n\nDataCategory\n\n(\n\nStrEnum\n\n): POSTS =\n\n\"posts\"\n\nARTICLES =\n\n\"articles\"\n\nREPOSITORIES =\n\n\"repositories\"\n\nThe last step to fully understand how the domain objects work is to zoom into the\n\nVectorBaseDocument\n\nOVM class.\n\nOceanofPDF.com\n\nOVM\n\nThe term OVM is inspired by the object-relational mapping (ORM) pattern we discussed in Chapter 3. We called it OVM because we work with embedding and vector DBs instead of structured data and SQL tables. Otherwise, it follows the same principles as an ORM pattern.\n\nSimilar to what we did in Chapter 3, we will implement our own OVM version. Even if our custom example is simple, it’s a powerful example of how to write modular and extendable classes by leveraging OOP best practices and principles.\n\nThe full implementation of the\n\nVectorBaseDocument\n\nclass is available on GitHub at https://github.com/PacktPublishing/LLM- Engineering/blob/main/llm_engineering/domain/base/vector.py.\n\nOur OVM base class is called\n\nVectorBaseDocument\n\n. It will support CRUD operations on top of Qdrant. Based on our application’s demands, we limited it only to create and read operations, but it can easily be extended to update and delete functions.\n\nLet’s take a look at the definition of the\n\nVectorBaseDocument\n\nclass:\n\nfrom\n\npydantic\n\nimport\n\nUUID4, BaseModel\n\nfrom\n\ntyping\n\nimport\n\nGeneric\n\nfrom\n\nllm_engineering.infrastructure.db.qdrant\n\nimport\n\nconnection T = TypeVar(\n\n\"T\"\n\n, bound=\n\n\"VectorBaseDocument\"\n\n)\n\nclass\n\nVectorBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC):\n\nid\n\n: UUID4 = Field(default_factory=uuid.uuid4)\n\n@classmethod\n\ndef\n\nfrom_record\n\n(\n\ncls:\n\nType\n\n[T], point: Record\n\n) -> T: _\n\nid\n\n= UUID(point.\n\nid\n\n, version=\n\n4\n\n) payload = point.payload\n\nor\n\n{} attributes = {\n\n\"id\"\n\n: _\n\nid\n\n, **payload, }\n\nif\n\ncls._has_class_attribute(\n\n\"embedding\"\n\n): payload[\n\n\"embedding\"\n\n] = point.vector\n\nor\n\nNone\n\nreturn\n\ncls(**attributes)\n\ndef\n\nto_point\n\n(\n\nself: T, **kwargs\n\n) -> PointStruct: exclude_unset = kwargs.pop(\n\n\"exclude_unset\"\n\n,\n\nFalse\n\n) by_alias = kwargs.pop(\n\n\"\n\nby_alias\"\n\n,\n\nTrue\n\n) payload =\n\nself\n\n.\n\ndict\n\n(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs) _\n\nid\n\n=\n\nstr\n\n(payload.pop(\n\n\"id\"\n\n)) vector = payload.pop(\n\n\"embedding\"\n\n, {})\n\nif\n\nvector\n\nand\n\nisinstance\n\n(vector, np.ndarray): vector = vector.tolist()\n\nreturn\n\nPointStruct(\n\nid\n\n=_\n\nid\n\n, vector=vector, payload=payload)\n\nThe\n\nVectorBaseDocument\n\nclass inherits from Pydantic’s\n\nBaseModel\n\nand helps us structure a single record’s attributes from the vector DB. Every OVM will be initialized by default with UUID4 as its unique identifier. Using generics—more precisely, by inheriting from\n\nGeneric[T]\n\n—the signatures of all the subclasses of the\n\nVectorBaseDocument\n\nclass will adapt to that given class. For example, the\n\nfrom_record()\n\nmethod of the\n\nChunk()\n\nclass, which inherits\n\nVectorBaseDocument\n\n, will return the Chunk type, which drastically helps the static analyzer and type checkers such as\n\nmypy\n\n(https://mypy.readthedocs.io/en/stable/).\n\nThe\n\nfrom_record()",
      "page_number": 231
    },
    {
      "number": 5,
      "title": "We called it OVM because we work with embedding and vector DBs instead of structured data and SQL tables. Otherwise, it follows the same principles as an ORM pattern",
      "start_page": 558,
      "end_page": 700,
      "detection_method": "regex_chapter",
      "content": "method adapts a data point from Qdrant’s format to our internal structure based on Pydantic. On the other hand, the\n\nto_point()\n\nmethod takes the attributes of the current instance and adapts them to Qdrant’s\n\nPointStruct()\n\nformat. We will leverage these two methods for our create and read operations.\n\nUltimately, all operations made to Qdrant will be done through the\n\nconnection\n\ninstance, which is instantiated in the application’s infrastructure layer.\n\nThe\n\nbulk_insert()\n\nmethod maps each document to a point. Then, it uses the Qdrant\n\nconnection\n\ninstance to load all the points to a given collection in Qdrant. If the insertion fails once, it tries to create the collection and do the insertion again. Often, it is good practice to split your logic into two functions. One private function contains the logic, in our case\n\n_bulk_insert()\n\n, and one public function handles all the errors and failure scenarios.\n\nclass\n\nVectorBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC): …\n\n# Rest of the class\n\n@classmethod\n\ndef\n\nbulk_insert\n\n(\n\ncls:\n\nType\n\n[T], documents:\n\nlist\n\n[\n\n\"VectorBaseDocument\"\n\n]\n\n) ->\n\nbool\n\n:\n\ntry\n\n: cls._bulk_insert(documents)\n\nexcept\n\nexceptions.UnexpectedResponse: logger.info(\n\nf\"Collection '\n\n{cls.get_collection_name()}\n\n' does not exist. Trying to create the collection and reinsert the documents.\"\n\n) cls.create_collection()\n\ntry\n\n: cls._bulk_insert(documents)\n\nexcept\n\nexceptions.UnexpectedResponse: logger.error(\n\nf\"Failed to insert documents in '\n\n{cls.get_collection_name()}\n\n'.\"\n\n)\n\nreturn\n\nFalse\n\nreturn\n\nTrue\n\n@classmethod\n\ndef\n\n_bulk_insert\n\n(\n\ncls:\n\nType\n\n[T], documents:\n\nlist\n\n[\n\n\"VectorBaseDocument\"\n\n]\n\n) ->\n\nNone\n\n: points = [doc.to_point()\n\nfor\n\ndoc\n\nin\n\ndocuments] connection.upsert(collection_name=cls.get_collection_name(), points=points)\n\nThe collection name is inferred from the\n\nConfig\n\nclass defined in the subclasses inheriting the OVM:\n\nclass\n\nVectorBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC): …\n\n# Rest of the class\n\n@classmethod\n\ndef\n\nget_collection_name\n\n(\n\ncls:\n\nType\n\n[T]\n\n) ->\n\nstr\n\n:\n\nif\n\nnot\n\nhasattr\n\n(cls,\n\n\"Config\"\n\n)\n\nor\n\nnot\n\nhasattr\n\n(cls.Config,\n\n\"name\"\n\n):\n\nraise\n\nImproperlyConfigured(\n\n\"The class should define a Config class with\"\n\n\"the 'name' property that reflects the collection's name.\"\n\n)\n\nreturn\n\ncls.Config.name\n\nNow, we must define a method that lets us read all the records from the vector DB (without using vector similarity search logic). The\n\nbulk_find()\n\nmethod enables us to scroll (or list) all the records from a collection. The function below scrolls the Qdrant vector DB, which returns a list of data points, which are ultimately mapped to our internal structure using the\n\nfrom_record()\n\nmethod.\n\nThe limit parameters control how many items we return at once, and the offset signals the ID of the point from which Qdrant starts returning records.\n\nclass\n\nVectorBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC): …\n\n# Rest of the class\n\n@classmethod\n\ndef\n\nbulk_find\n\n(\n\ncls:\n\nType\n\n[T], limit:\n\nint\n\n=\n\n10\n\n, **kwargs\n\n) ->\n\ntuple\n\n[\n\nlist\n\n[T], UUID |\n\nNone\n\n]:\n\ntry\n\n: documents, next_offset = cls._bulk_find(limit=limit, **kwargs)\n\nexcept\n\nexceptions.UnexpectedResponse: logger.error(\n\nf\"Failed to search documents in '\n\n{cls.get_collection_name()}\n\n'.\"\n\n) documents, next_offset = [],\n\nNone\n\nreturn\n\ndocuments, next_offset\n\n@classmethod\n\ndef\n\n_bulk_find\n\n(\n\ncls:\n\nType\n\n[T], limit:\n\nint\n\n=\n\n10\n\n, **kwargs\n\n) ->\n\ntuple\n\n[\n\nlist\n\n[T], UUID |\n\nNone\n\n]: collection_name = cls.get_collection_name() offset = kwargs.pop(\n\n\"offset\"\n\n,\n\nNone\n\n) offset =\n\nstr\n\n(offset)\n\nif\n\noffset\n\nelse\n\nNone\n\nrecords, next_offset = connection.scroll( collection_name=collection_name, limit=limit, with_payload=kwargs.pop(\n\n\"\n\nwith_payload\"\n\n,\n\nTrue\n\n), with_vectors=kwargs.pop(\n\n\"with_vectors\"\n\n,\n\nFalse\n\n), offset=offset, **kwargs, ) documents = [cls.from_record(record)\n\nfor\n\nrecord\n\nin\n\nrecords]\n\nif\n\nnext_offset\n\nis\n\nnot\n\nNone\n\n: next_offset = UUID(next_offset, version=\n\n4\n\n)\n\nreturn\n\ndocuments, next_offset\n\nThe last piece of the puzzle is to define a method that performs a vector similarity search on a provided query embedding. Like before, we defined a public\n\nsearch()\n\nand private\n\n_search()\n\nmethod. The search is performed by Qdrant when calling the\n\nconnection.search()\n\nfunction.\n\nclass\n\nVectorBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC): …\n\n# Rest of the class\n\n@classmethod\n\ndef\n\nsearch\n\n(\n\ncls:\n\nType\n\n[T], query_vector:\n\nlist\n\n, limit:\n\nint\n\n=\n\n10\n\n, **kwargs\n\n) ->\n\nlist\n\n[T]:\n\ntry\n\n: documents = cls._search(query_vector=query_vector, limit=limit, **kwargs)\n\nexcept\n\nexceptions.UnexpectedResponse: logger.error(\n\nf\"Failed to search documents in '\n\n{cls.get_collection_name()}\n\n'.\"\n\n) documents = []\n\nreturn\n\ndocuments\n\n@classmethod\n\ndef\n\n_search\n\n(\n\ncls:\n\nType\n\n[T], query_vector:\n\nlist\n\n, limit:\n\nint\n\n=\n\n10\n\n, **kwargs\n\n) ->\n\nlist\n\n[T]: collection_name = cls.get_collection_name() records = connection.search( collection_name=collection_name, query_vector=query_vector, limit=limit, with_payload=kwargs.pop(\n\n\"with_payload\"\n\n,\n\nTrue\n\n), with_vectors=kwargs.pop(\n\n\"with_vectors\"\n\n,\n\nFalse\n\n), **kwargs, ) documents = [cls.from_record(record)\n\nfor\n\nrecord\n\nin\n\nrecords]\n\nreturn\n\ndocuments\n\nNow that we understand what our domain entities look like and how the OVM works, let’s move on to the dispatchers who clean, chunk, and embed the documents.\n\nOceanofPDF.com\n\nThe dispatcher layer\n\nA dispatcher inputs a document and applies dedicated handlers based on its data category (article, post, or repository). A handler can either clean, chunk, or embed a document.\n\nLet’s start by zooming in on the\n\nCleaningDispatcher\n\n. It mainly implements a\n\ndispatch()\n\nmethod that inputs a raw document. Based on its data category, it instantiates and calls a handler that applies the cleaning logic specific to that data point:\n\nclass\n\nCleaningDispatcher\n\n: cleaning_factory = CleaningHandlerFactory()\n\n@classmethod\n\ndef\n\ndispatch\n\n(\n\ncls, data_model: NoSQLBaseDocument\n\n) -> VectorBaseDocument: data_category = DataCategory(data_model.get_collection_name()) handler = cls.cleaning_factory.create_handler(data_category) clean_model = handler.clean(data_model) logger.info(\n\n\"Data cleaned successfully.\"\n\n, data_category=data_category, cleaned_content_len=\n\nlen\n\n(clean_model.content), )\n\nreturn\n\nclean_model\n\nThe key in the dispatcher logic is the\n\nCleaningHandlerFactory()\n\n, which instantiates a different cleaning handler based on the document’s data category:\n\nclass\n\nCleaningHandlerFactory\n\n:\n\n@staticmethod\n\ndef\n\ncreate_handler\n\n(\n\ndata_category: DataCategory\n\n) -> CleaningDataHandler:\n\nif\n\ndata_category == DataCategory.POSTS:\n\nreturn\n\nPostCleaningHandler()\n\nelif\n\ndata_category == DataCategory.ARTICLES:\n\nreturn\n\nArticleCleaningHandler()\n\nelif\n\ndata_category == DataCategory.REPOSITORIES:\n\nreturn\n\nRepositoryCleaningHandler()\n\nelse\n\n:\n\nraise\n\nValueError(\n\n\"Unsupported data type\"\n\n)\n\nThe Dispatcher or Factory classes are nothing fancy, but they offer an intuitive and simple interface for applying various operations to your documents. When manipulating documents, instead of worrying about their data category and polluting your business logic with if-else statements, you have a class dedicated to handling that. You have a single class that cleans any document, which respects the DRY (don’t repeat yourself) principles from software engineering. By respecting DRY, you have a single point of failure, and the code can easily be extended. For example, if we add an extra type, we must extend only the Factory class instead of multiple occurrences in the code.\n\nThe\n\nChunkingDispatcher\n\nand\n\nEmbeddingDispatcher\n\nfollow the same pattern. They use a\n\nChunkingHandlerFactory\n\nand, respectively, an\n\nEmbeddingHandlerFactory\n\nthat initializes the correct handler based on the data category of the input document. Afterward, they call the handler and return the result.\n\nThe source code of all the dispatchers and factories can be found on GitHub at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/preprocessing/dispatcher s.py\n\nThe Factory class leverages theabstract factory creational pattern (https://refactoring.guru/design-patterns/abstract-factory), which instantiates a family of classes implementing the same interface. In our case, these handlers implement the\n\nclean()\n\nmethod regardless of the handler type.\n\nAlso, the Handler class family leverages the strategy behavioral pattern (https://refactoring.guru/design-patterns/strategy) used to instantiate when you want to use different variants of an algorithm within an object and be able to switch from one algorithm to another during runtime.\n\nIntuitively, in our dispatcher layer, the combination of the factory and strategy patterns works as follows:\n\nInitially, we knew we wanted to clean the data, but as we knew the data category only at runtime, we couldn’t decide on what strategy to apply.\n\nWe can write the whole code around the cleaning code and abstract away the logic under a\n\nHandler()\n\ninterface, which will represent our strategy.\n\nWhen we get a data point, we apply the abstract factory pattern and create the correct cleaning handler for its data type.\n\nUltimately, the dispatcher layer uses the handler and executes the right strategy.\n\nBy doing so, we:\n\nIsolate the logic for a given data category.\n\nLeverage polymorphism to avoid filling up the code with hundreds of\n\nif-else\n\nstatements.\n\nMake the code modular and extendable. When a new data category arrives, we must implement a new handler and modify the Factory class without touching any other part of the code.\n\nUntil now, we have just modeled our entities and how the data flows in our application. We haven’t written a single piece of cleaning, chunking, or embedding code. That is one big difference between a quick demo and a production-ready application. In a demo, you don’t care about software engineering best practices and structuring your code to make it future-proof. However, writing clean, modular, and scalable code is critical for its longevity when building a real-world application.\n\nThe last component of the RAG feature pipeline is the implementation of the cleaning, chunking, and embedding handlers.\n\nOceanofPDF.com\n\nThe handlers\n\nThe handler has a one-on-one structure with our domain, meaning that every entity has its own handler, as shown in Figure 8.17. In total, we will have nine Handler classes that follow the next base interfaces:\n\nclass CleaningDataHandler()\n\nclass ChunkingDataHandler()\n\nclass EmbeddingDataHandler()\n\nFigure 4.17: Handler class hierarchy and their interaction\n\nThe code for all the handlers is available on GitHub at https://github.com/PacktPublishing/LLM- Engineering/tree/main/llm_engineering/application/preprocessing.\n\nLet’s examine each handler family and see how it is implemented.\n\nOceanofPDF.com\n\nThe cleaning handlers\n\nThe\n\nCleaningDataHandler()\n\nstrategy interface looks as follows:\n\n…\n\n# Other imports.\n\nfrom\n\ntyping\n\nimport\n\nGeneric\n\n, TypeVar DocumentT = TypeVar(\n\n\"DocumentT\"\n\n, bound=Document) CleanedDocumentT = TypeVar(\n\n\"CleanedDocumentT\"\n\n, bound=CleanedDocument)\n\nclass\n\nCleaningDataHandler\n\n(ABC,\n\nGeneric\n\n[DocumentT, CleanedDocumentT]):\n\n@abstractmethod\n\ndef\n\nclean\n\n(\n\nself, data_model: DocumentT\n\n) -> CleanedDocumentT:\n\npass\n\nNow, for every post, article and repository, we have to implement a different handler, as follows:\n\nclass\n\nPostCleaningHandler\n\n(\n\nCleaningDataHandler\n\n):\n\ndef\n\nclean\n\n(\n\nself, data_model: PostDocument\n\n) -> CleanedPostDocument:\n\nreturn\n\nCleanedPostDocument(\n\nid\n\n=data_model.\n\nid\n\n, content=clean_text(\n\n\" #### \"\n\n.join(data_model.content.values())), …\n\n# Copy the rest of the parameters from the data_model object.\n\n)\n\nclass\n\nArticleCleaningHandler\n\n(\n\nCleaningDataHandler\n\n):\n\ndef\n\nclean\n\n(\n\nself, data_model: ArticleDocument\n\n) -> CleanedArticleDocument: valid_content = [content\n\nfor\n\ncontent\n\nin\n\ndata_model.content.values()\n\nif\n\ncontent]\n\nreturn\n\nCleanedArticleDocument(\n\nid\n\n=data_model.\n\nid\n\n, content=clean_text(\n\n\" #### \"\n\n.join(valid_content)), platform=data_model.platform, link=data_model.link, author_id=data_model.author_id, author_full_name=data_model.author_full_name, )\n\nclass\n\nRepositoryCleaningHandler\n\n(\n\nCleaningDataHandler\n\n):\n\ndef\n\nclean\n\n(\n\nself, data_model: RepositoryDocument\n\n) -> CleanedRepositoryDocument:\n\nreturn\n\nCleanedRepositoryDocument(\n\nid\n\n=data_model.\n\nid\n\n, content=clean_text(\n\n\" #### \"\n\n.join(data_model.content.values())), …\n\n# Copy the rest of the parameters from the data_model object.\n\n)\n\nThe handlers input a raw document domain entity, clean the content, and return a cleaned document. All the handlers use the\n\nclean_text()\n\nfunction to clean the text. Out of simplicity, we used the same cleaning technique for all the data categories. Still, in a real-world setup, we would have to further optimize and create a different cleaning function for each data category. The strategy pattern makes this a breeze, as we swap the cleaning function in the handlers, and that’s it.\n\nThe cleaning steps applied in the\n\nclean_text()\n\nfunction are the same ones discussed in Chapter 5 in the Creating an instruction dataset section. We don’t want to repeat ourselves. Thus, for a refresher, check out that chapter. At this point, we mostly care about automating and integrating the whole logic into the RAG feature pipeline. Thus, after operationalizing the ML system, all the cleaned data used for fine-tuning will be accessed from the logical feature store, making it the single source of truth for accessing data.\n\nOceanofPDF.com\n\nThe chunking handlers\n\nFirst, let’s examine the\n\nChunkingDataHandler()\n\nstrategy handler. We exposed the\n\nmetadata\n\ndictionary as a property to aggregate all the necessary properties required for chunking in a single structure. By structuring it like this, we can easily log everything to ZenML to track and debug our chunking logic. The handler takes cleaned documents as input and returns chunk entities. All the handlers can be found on GitHub at https://github.com/PacktPublishing/LLM- Engineering/tree/main/llm_engineering/application/preprocessing.\n\n…\n\n# Other imports.\n\nfrom\n\ntyping\n\nimport\n\nGeneric\n\n, TypeVar CleanedDocumentT = TypeVar(\n\n\"CleanedDocumentT\"\n\n, bound=CleanedDocument) ChunkT = TypeVar(\n\n\"ChunkT\"\n\n, bound=Chunk)\n\nclass\n\nChunkingDataHandler\n\n(ABC,\n\nGeneric\n\n[CleanedDocumentT, ChunkT]):\n\n@property\n\ndef\n\nmetadata\n\n(\n\nself\n\n) ->\n\ndict\n\n:\n\nreturn\n\n{\n\n\"chunk_size\"\n\n:\n\n500\n\n,\n\n\"chunk_overlap\"\n\n:\n\n50\n\n, }\n\n@abstractmethod\n\ndef\n\nchunk\n\n(\n\nself, data_model: CleanedDocumentT\n\n) ->\n\nlist\n\n[ChunkT]:\n\npass\n\nLet’s understand how the\n\nArticleChunkingHandler()\n\nclass is implemented. The first step is to override the metadata property and customize the type of properties the chunking logic requires. For example, when working with articles, we are interested in the chunk’s minimum and maximum length.\n\nThe handler’s\n\nchunk()\n\nmethod inputs cleaned article documents and returns a list of article chunk entities. It uses the\n\nchunk_text()\n\nfunction to split the cleaned content into chunks. The chunking function is customized based on the\n\nmin_length\n\nand\n\nmax_length\n\nmetadata fields. The chunk_id is computed as the MD5 hash of the chunk’s content. Thus, if the two chunks have precisely the same content, they will have the same ID, and we can easily deduplicate them. Lastly, we create a list of chunk entities and return them.\n\nclass\n\nArticleChunkingHandler\n\n(\n\nChunkingDataHandler\n\n):\n\n@property\n\ndef\n\nmetadata\n\n(\n\nself\n\n) ->\n\ndict\n\n:\n\nreturn\n\n{\n\n\"min_length\"\n\n:\n\n1000\n\n,\n\n\"max_length\"\n\n:\n\n1000\n\n, }\n\ndef\n\nchunk\n\n(\n\nself, data_model: CleanedArticleDocument\n\n) ->\n\nlist\n\n[ArticleChunk]: data_models_list = [] cleaned_content = data_model.content chunks = chunk_article( cleaned_content, min_length=\n\nself\n\n.metadata[\n\n\"min_length\"\n\n], max_length=\n\nself\n\n.metadata[\n\n\"max_length\"\n\n] )\n\nfor\n\nchunk\n\nin\n\nchunks: chunk_id = hashlib.md5(chunk.encode()).hexdigest() model = ArticleChunk(\n\nid\n\n=UUID(chunk_id, version=\n\n4\n\n), content=chunk, platform=data_model.platform, link=data_model.link, document_id=data_model.\n\nid\n\n, author_id=data_model.author_id, author_full_name=data_model.author_full_name, metadata=\n\nself\n\n.metadata, ) data_models_list.append(model)\n\nreturn\n\ndata_models_list\n\nThe last step is to dig into the\n\nchunk_article()\n\nfunction, which mainly does two things:\n\nIt uses a regex to find all the sentences within the given text by looking for periods, question marks, or exclamation points followed by a space.\n\nHowever, it avoids splitting into cases where the punctuation is part of an abbreviation or initialism (like “\n\ne.g.\n\n\" or “\n\nDr.\n\n\")\n\nIt groups sentences into a single chunk until the\n\nmax_length\n\nlimit is reached. When the maximum size is reached, and the chunk size is bigger than the minimum allowed value, it is added to the final list the function returns.\n\ndef\n\nchunk_article\n\n(\n\ntext:\n\nstr\n\n, min_length:\n\nint\n\n, max_length:\n\nint\n\n) ->\n\nlist\n\n[\n\nstr\n\n]: sentences = re.split(\n\nr\"(?\n\n, text) extracts = [] current_chunk =\n\n\"\"\n\nfor\n\nsentence\n\nin\n\nsentences: sentence = sentence.strip()\n\nif\n\nnot\n\nsentence:\n\ncontinue\n\nif\n\nlen\n\n(current_chunk) +\n\nlen\n\n(sentence) <= max_length: current_chunk += sentence +\n\n\" \"\n\nelse\n\n:\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip()) current_chunk = sentence +\n\n\" \"\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip())\n\nreturn\n\nextracts\n\nThe\n\nPostChunkingHandler\n\nand\n\nRepositoryChunkingHandler\n\n, available on GitHub at\n\nllm_engineering/application/preprocessing/chunking_data_handlers.py\n\n, have a similar structure to the ArticleChunkingHandler. However, they use a more generic chunking function called\n\nchunk_text()\n\n, worth looking into. The\n\nchunk_text()\n\nfunction is a two-step process that has the following logic:\n\nIt uses a\n\nRecursiveCharacterTextSplitter()\n\nfrom LangChain to split the text based on a given separator or chunk size. Using the separator, we first try to find paragraphs in the given text, but if there are no paragraphs or they are too long, we cut it at a given chunk size.\n\nNotice that we want to ensure that the chunk doesn’t exceed the maximum input length of the embedding model. Thus, we pass all the chunks created above into a\n\nSenteceTransformersTokenTextSplitter()\n\n, which considers the maximum input length of the model. At this point, we also apply the\n\nchunk_overlap\n\nlogic, as we want to do it only after we validate that the chunk is small enough.\n\n…\n\n# Other imports.\n\nfrom\n\nlangchain.text_splitter\n\nimport\n\nRecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n\nfrom\n\nllm_engineering.application.networks\n\nimport\n\nEmbeddingModelSingleton\n\ndef\n\nchunk_text\n\n(\n\ntext:\n\nstr\n\n, chunk_size:\n\nint\n\n=\n\n500\n\n, chunk_overlap:\n\nint\n\n=\n\n50\n\n) ->\n\nlist\n\n[\n\nstr\n\n]: character_splitter = RecursiveCharacterTextSplitter(separators=[\n\n\"\\n\\n\"\n\n], chunk_size=chunk_size, chunk_overlap=\n\n0\n\n) text_split_by_characters = character_splitter.split_text(text) token_splitter = SentenceTransformersTokenTextSplitter( chunk_overlap=chunk_overlap, tokens_per_chunk=embedding_model.max_input_length, model_name=embedding_model.model_id, ) chunks_by_tokens = []\n\nfor\n\nsection\n\nin\n\ntext_split_by_characters: chunks_by_tokens.extend(token_splitter.split_text(section))\n\nreturn\n\nchunks_by_tokens\n\nTo conclude, the function above returns a list of chunks that respect both the provided chunk parameters and the embedding model’s max input length.\n\nOceanofPDF.com\n\nThe embedding handlers\n\nThe embedding handlers differ slightly from the others as the\n\nEmbeddingDataHandler()\n\ninterface contains most of the logic. We took this approach because, when calling the embedding model, we want to batch as many samples as possible to optimize the inference process. When running the model on a GPU, the batched samples are processed independently and in parallel. Thus, by batching the chunks, we can optimize the inference process by 10x or more, depending on the batch size and hardware we use.\n\nWe implemented an\n\nembed()\n\nmethod, in case you want to run the inference on a single data point, and an\n\nembed_batch()\n\nmethod. The\n\nembed_batch()\n\nmethod takes chunked documents as input, gathers their content into a list, passes them to the embedding model, and maps the results to an embedded chunk domain entity. The mapping is done through the\n\nmap_model()\n\nabstract method, which has to be customized for every data category.\n\n…\n\n# Other imports.\n\nfrom\n\ntyping\n\nimport\n\nGeneric\n\n, TypeVar, cast\n\nfrom\n\nllm_engineering.application.networks\n\nimport\n\nEmbeddingModelSingleton ChunkT = TypeVar(\n\n\"ChunkT\"\n\n, bound=Chunk) EmbeddedChunkT = TypeVar(\n\n\"EmbeddedChunkT\"\n\n, bound=EmbeddedChunk) embedding_model = EmbeddingModelSingleton()\n\nclass\n\nEmbeddingDataHandler\n\n(ABC,\n\nGeneric\n\n[ChunkT, EmbeddedChunkT]):\n\n\"\"\"\n\nAbstract class for all embedding data handlers.\n\nAll data transformations logic for the embedding step is done here\n\n\"\"\"\n\ndef\n\nembed\n\n(\n\nself, data_model: ChunkT\n\n) -> EmbeddedChunkT:\n\nreturn\n\nself\n\n.embed_batch([data_model])[\n\n0\n\n]\n\ndef\n\nembed_batch\n\n(\n\nself, data_model:\n\nlist\n\n[ChunkT]\n\n) ->\n\nlist\n\n[EmbeddedChunkT]: embedding_model_input = [data_model.content\n\nfor\n\ndata_model\n\nin\n\ndata_model] embeddings = embedding_model(embedding_model_input, to_list=\n\nTrue\n\n) embedded_chunk = [\n\nself\n\n.map_model(data_model, cast(\n\nlist\n\n[\n\nfloat\n\n], embedding))\n\nfor\n\ndata_model, embedding\n\nin\n\nzip\n\n(data_model, embeddings, strict=\n\nFalse\n\n) ]\n\nreturn\n\nembedded_chunk\n\n@abstractmethod\n\ndef\n\nmap_model\n\n(\n\nself, data_model: ChunkT, embedding:\n\nlist\n\n[\n\nfloat\n\n]\n\n) -> EmbeddedChunkT:\n\npass\n\nLet’s look only at the implementation of the\n\nArticleEmbeddingHandler()\n\n, as the other handlers are highly similar. As you can see, we only have to implement the\n\nmap_model()\n\nmethod, which takes a chunk of input and computes the embeddings in batch mode. Its scope is to map this information to an\n\nEmbeddedArticleChunk\n\nPydantic entity.\n\nclass\n\nArticleEmbeddingHandler\n\n(\n\nEmbeddingDataHandler\n\n):\n\ndef\n\nmap_model\n\n(\n\nself, data_model: ArticleChunk, embedding:\n\nlist\n\n[\n\nfloat\n\n]\n\n) -> EmbeddedArticleChunk:\n\nreturn\n\nEmbeddedArticleChunk(\n\nid\n\n=data_model.\n\nid\n\n, content=data_model.content, embedding=embedding, platform=data_model.platform, link=data_model.link, document_id=data_model.document_id, author_id=data_model.author_id, author_full_name=data_model.author_full_name, metadata={\n\n\"embedding_model_id\"\n\n: embedding_model.model_id,\n\n\"embedding_size\"\n\n: embedding_model.embedding_size,\n\n\"max_input_length\"\n\n: embedding_model.max_input_length, }, )\n\nThe last step is to understand how the\n\nEmbeddingModelSingleton()\n\nworks. It is a wrapper over the\n\nSentenceTransformer()\n\nclass from Sentence Transformers that initializes the embedding model. Writing a wrapper over external packages is often good practice. Thus, when you want to change the third-party tool, you have to modify only the internal logic of the wrapper instead of the whole code base.\n\nThe\n\nSentenceTransformer()\n\nclass is initialized with the\n\nmodel_id\n\ndefined in the\n\nSettings\n\nclass, allowing us to quickly test multiple embedding models just by changing the configuration file and not the code. That is why I am not insisting at all on what embedding model to use. This differs constantly based on your use case, data, hardware, and latency. But by writing a generic class, which can quickly be configured, you can experiment with multiple embedding models until you find the best one for you.\n\nfrom\n\nsentence_transformers.SentenceTransformer\n\nimport\n\nSentenceTransformer\n\nfrom\n\nllm_engineering.settings\n\nimport\n\nsettings\n\nfrom\n\n.base\n\nimport\n\nSingletonMeta\n\nclass\n\nEmbeddingModelSingleton\n\n(metaclass=SingletonMeta):\n\ndef\n\n__init__\n\n(\n\nself,\n\nmodel_id:\n\nstr\n\n= settings.TEXT_EMBEDDING_MODEL_ID,\n\ndevice:\n\nstr\n\n= settings.RAG_MODEL_DEVICE,\n\ncache_dir:\n\nOptional\n\n[Path] =\n\nNone\n\n,\n\n) ->\n\nNone\n\n:\n\nself\n\n._model_id = model_id\n\nself\n\n._device = device\n\nself\n\n._model = SentenceTransformer(\n\nself\n\n._model_id, device=\n\nself\n\n._device, cache_folder=\n\nstr\n\n(cache_dir)\n\nif\n\ncache_dir\n\nelse\n\nNone\n\n, )\n\nself\n\n._model.\n\neval\n\n()\n\n@property\n\ndef\n\nmodel_id\n\n(\n\nself\n\n) ->\n\nstr\n\n:\n\nreturn\n\nself\n\n._model_id\n\n@cached_property\n\ndef\n\nembedding_size\n\n(\n\nself\n\n) ->\n\nint\n\n: dummy_embedding =\n\nself\n\n._model.encode(\n\n\"\"\n\n)\n\nreturn\n\ndummy_embedding.shape[\n\n0\n\n]\n\n@property\n\ndef\n\nmax_input_length\n\n(\n\nself\n\n) ->\n\nint\n\n:\n\nreturn\n\nself\n\n._model.max_seq_length\n\n@property\n\ndef\n\ntokenizer\n\n(\n\nself\n\n) -> AutoTokenizer:\n\nreturn\n\nself\n\n._model.tokenizer\n\ndef\n\n__call__\n\n(\n\nself, input_text:\n\nstr\n\n|\n\nlist\n\n[\n\nstr\n\n], to_list:\n\nbool\n\n=\n\nTrue\n\n) -> NDArray[np.float32] |\n\nlist\n\n[\n\nfloat\n\n] |\n\nlist\n\n[\n\nlist\n\n[\n\nfloat\n\n]]:\n\ntry\n\n: embeddings =\n\nself\n\n._model.encode(input_text)\n\nexcept\n\nException: logger.error(\n\nf\"Error generating embeddings for\n\n{self._model_id=}\n\nand\n\n{input_text=}\n\n\"\n\n)\n\nreturn\n\n[]\n\nif\n\nto_list\n\nelse\n\nnp.array([])\n\nif\n\nto_list: embeddings = embeddings.tolist()\n\nreturn\n\nembeddings\n\nThe embedding model class implements the singleton pattern (https://refactoring.guru/design-patterns/singleton), a creational design pattern that ensures a class has only one instance while providing a global access point to this instance. The\n\nEmbeddingModelSingleton()\n\nclass inherits from the\n\nSingletonMeta\n\nclass, which ensures that whenever an\n\nEmbeddingModelSingleton()\n\nis instantiated, it returns the same instance. This works well with ML models, as you load them once in memory through the singleton pattern, and afterward, you can use them anywhere in the code base. Otherwise, you risk loading the model in memory every time you use it or loading it multiple times, resulting in memory issues. Also, this makes it very convenient to access properties such as\n\nembedding_size\n\n, where you have to make a dummy forward pass into the embedding model to find the size of its output. As a singleton, you do this forward pass only once, and then you have it accessible all the time during the program’s execution.\n\nOceanofPDF.com\n\nSummary\n\nThis chapter began with a soft introduction to RAG and why and when you should use it. We also understood how embeddings and vector DBs work, representing the cornerstone of any RAG system. Then, we looked into advanced RAG and why we need it in the first place. We built a strong understanding of what parts of the RAG can be optimized and proposed some popular advanced RAG techniques for working with textual data. Next, we applied everything we learned about RAG to designing the architecture of LLM Twin’s RAG feature pipeline. We also understood the difference between a batch and streaming pipeline and presented a short introduction to the CDC pattern, which helps sync two DBs.\n\nUltimately, we went step-by-step into the implementation of the LLM Twin’s RAG feature pipeline, where we saw how to integrate ZenML as an orchestrator, how to design the domain entities of the application, and how to implement an OVM module. Also, we understood how to apply some software engineering best practices, such as the abstract factory and strategy software patterns, to implement a modular and extendable layer that applies different cleaning, chunking, and embedding techniques based on the data category of each document.\n\nThis chapter focused only on implementing the ingestion pipeline, which is just one component of a standard RAG application. In Chapter 9, we will conclude the RAG system by implementing the retrieval and generation components and integrating them into the inference pipeline. But first, in the next chapter, we will explore how to generate a custom dataset using the data we collected and fine-tune an LLM with it.\n\nOceanofPDF.com\n\nReferences\n\nKenton, J.D.M.W.C. and Toutanova, L.K., 2019, June. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT (Vol. 1, p. 2).\n\nLiu, Y., 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\n\nMikolov, T., 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\n\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for Computational Linguistics.\n\nHe, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., 2021, July.\n\nLearning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.\n\nWhat is Change Data Capture (CDC)? | Confluent. (n.d.). Confluent. https://www.confluent.io/en-gb/learn/change-data-capture/\n\nRefactoring.Guru. (2024, January 1). Singleton. https://refactoring.guru/design-patterns/singleton\n\nRefactoring.Guru. (2024b, January 1). Strategy. https://refactoring.guru/design-patterns/strategy\n\nRefactoring.Guru. (2024a, January 1). Abstract Factory. https://refactoring.guru/design-patterns/abstract-factory\n\nSchwaber-Cohen, R. (n.d.). What is a Vector Database & How Does it Work? Use Cases + Examples. Pinecone. https://www.pinecone.io/learn/vector-database/\n\nMonigatti, L. (2024, February 19). Advanced Retrieval-Augmented Generation: From Theory to LlaMaIndex Implementation. Medium. https://towardsdatascience.com/advanced-retrieval-augmented-generation- from-theory-to-llamaindex-implementation-4de1464a9930\n\nMonigatti, L. (2023, December 6). A guide on 12 tuning Strategies for Production-Ready RAG applications. Medium. https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for- production-ready-rag-applications-7ca646833439\n\nMonigatti, L. (2024b, February 19). Advanced Retrieval-Augmented Generation: From Theory to LlaMaIndex Implementation. Medium. https://towardsdatascience.com/advanced-retrieval-augmented-generation- from-theory-to-llamaindex-implementation-4de1464a9930\n\nMaameri, S. (2024, May 10). Routing in RAG-Driven applications - towards data science. Medium. https://towardsdatascience.com/routing-in- rag-driven-applications-a685460a7220\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\n5\n\nOceanofPDF.com\n\nSupervised Fine-Tuning\n\nSupervised Fine-Tuning (SFT) is a crucial step in preparing LLMs for real-world applications. Following the initial pre-training phase, where an LLM learns to predict the next token in a sequence, SFT refines the model’s capabilities using carefully curated pairs of instructions and corresponding answers. This process serves two primary purposes: it teaches the model to understand and follow a specific chat format, effectively transforming it into a conversational agent, and it allows the model to adapt its broad knowledge base to excel in targeted tasks or specialized domains.\n\nThe importance of SFT lies in its ability to bridge the gap between a model’s general language understanding and its practical utility. By exposing the model to examples of desired input-output patterns, SFT shapes the LLM’s behavior to align with specific goals, whether they involve task completion (such as summarization or translation) or domain expertise (like medical or legal knowledge). This tailored approach not only enhances the model’s performance in intended areas but also improves its ability to follow instructions and generate more relevant and coherent responses.\n\nIn this chapter, we will cover the following topics:\n\nCreating a high-quality instruction dataset\n\nSFT techniques\n\nImplementing fine-tuning in practice\n\nBy the end of this chapter, you will be able to create your own instruction datasets and efficiently fine-tune LLMs on them.\n\nAll the code examples from this chapter can be found on GitHub at https://github.com/PacktPublishing/LLM-Engineering.\n\nOceanofPDF.com\n\nCreating an instruction dataset\n\nIn most use cases, creating an instruction dataset is the most difficult part of the fine-tuning process. This is due to multiple factors. Most use cases can be connected to raw text, but it is rare to find natural pairs of instructions and answers. This raw text needs to be transformed into a format that includes both instructions and answers. Moreover, the quality of the data is also crucial. Because of this, a lot of time is invested in manually checking and verifying individual samples. This careful review helps ensure that the dataset is accurate and useful for training the model.\n\nFigure 5.1 – Overview of the post-training data pipeline covered in this chapter\n\nIn this section, we will introduce a general framework to create your own instruction datasets, regardless of the final use case. We will then leverage the scraped data from Chapter 3 and transform it into an instruction dataset. The different stages in our data generation pipeline are summarized in Figure 5.1.\n\nOceanofPDF.com\n\nGeneral framework\n\nInstruction datasets are defined as pairs of instructions and answers. The instructions are the inputs of the model, used as context during fine-tuning. The answers are the expected outputs of the model. During fine-tuning, you can choose to train the model on the instructions and answers, or on answers only. Pairs of instructions and answers follow a certain template. Some instruction templates, such as Alpaca, introduce additional fields like\n\ninputs\n\nand\n\nsystem\n\n. Both of them can be considered subfields of the\n\ninstruction\n\nfield. In this case, “inputs” contain the data the model needs to complete the instruction, and “system” is a meta- prompt to steer the general behavior of the model. Here is an example from the SlimOrca dataset, with “system” and “instruction”:\n\nSystem You are a helpful assistant, who always provide explanation. Think like you are answering to a five year ol\n\nInstruction Concepts: building, shop, town Write a sentence that includes all these words.\n\nOutput In our little town, there is a shop inside a big building where people go to buy their favorite toys and candie\n\nTable 5.1 – Example of sample from the Open-Orca/SlimOrca dataset\n\nThis example illustrates how the “system” field is used to define specific behaviors for the model, such as being helpful, always providing explanations, and tailoring responses as if speaking to a five-year-old. The “instruction” field provides the necessary data (the concepts) and the task (constructing a sentence). The\n\noutput\n\nfield shows the expected answer, which, while not the only possible answer, represents a high-quality response.\n\nTo build an instruction dataset, we want to curate data that is representative of how the model will be used. Once we have gathered enough samples, our goal is to filter them to only keep high-quality data. In this context, high- quality data can be described through three main dimensions:\n\nAccuracy: It refers to the factual correctness and relevance of the samples. In the context of instruction datasets, this means ensuring that responses are not only factually accurate but also relevant to their corresponding instructions. High accuracy is essential for training models that can provide reliable and trustworthy information.\n\nDiversity: A high-quality dataset should encompass a wide range of use cases, covering the potential queries and tasks the deployed LLM might encounter. This diversity should span topics, contexts, text lengths, and writing styles. By sampling data in a representative manner, we allow models to develop robust instruction-following capabilities.\n\nComplexity: Trivial or overly simplistic samples do little to improve an LLM’s capabilities. Instead, datasets should include complex, multi-step reasoning problems and challenging tasks that push the boundaries of what the model is expected to handle. This complexity helps in developing models capable of tackling complex real-world problems.\n\nIn the following sections, we will see techniques to filter and evaluate instruction samples according to these dimensions.\n\nOceanofPDF.com\n\nData quantity\n\nThe Hugging Face Hub contains numerous instruction datasets, which can be general-purpose or designed for particular tasks or domains. When working on a new use case, it can be beneficial to look for related open- source datasets to leverage for fine-tuning. This is particularly important if your number of samples is too low (for example, fewer than 1,000), requiring you to augment it with high-quality data.\n\nFigure 5.2 – Screenshot of the most-liked datasets on the Hugging Face Hub\n\nCalculating an ideal number of samples is a difficult task, as both the quality of the data and the size of the model can have a dramatic impact. For large models (around 70 billion parameters, for example), this number can be as low as 1,000 high-quality samples (see the LIMA paper in the References section). This is not true for smaller models (around seven billion parameters, for instance), as they need more samples to simply learn the correct chat template. In any case, the quality of the data is a crucial factor, and a high number of samples is always desirable.\n\nTo provide additional numbers, we can look at the fine-tuned models developed by companies and the open-source community. We can distinguish two types of finetunes: general-purpose, aimed to reproduce the capabilities of models like GPT, and task- or domain-specific models, designed to optimize their performance for a particular application.\n\nGeneral-purpose models cover more topics, which requires additional samples. Among companies, we observe a wide range of values. For instance, Yi models from 01-ai rely on less than 10,000 samples. At the opposite range of the spectrum, Meta reported using 10 million samples for Llama 3 through the entire fine-tuning process (including preference alignment). In the open-source community, models like OpenHermes and Dolphin use around one million samples. Based on the quality of these finetunes, we recommend an instruction dataset of at least one million samples to create a good general-purpose instruct model. On the other hand, models fine-tuned for a specific purpose require fewer samples. Here, we differentiate task-specific models from domain-specific ones.\n\nTask-specific and domain-specific models represent two distinct approaches to fine-tuning LLMs. Task-specific models are designed to excel at a particular function, such as translation, summarization, or sentiment\n\nanalysis. These models benefit from a focused training approach on a single task, allowing for efficient performance even with smaller model sizes (typically less than 8 billion parameters). The data required for task-specific fine-tuning is generally more manageable, ranging from 100 to 100,000 samples. This makes task-specific fine-tuning an attractive option for many applications where resources may be limited.\n\nDomain-specific models, on the other hand, aim to tweak the LLM with specialized knowledge and familiarity with the vocabulary and linguistic patterns of a particular field. These models are valuable in areas such as medicine, law, finance, e-commerce, engineering, and hospitality. The data requirements for domain-specific fine-tuning can vary widely depending on the complexity and breadth of the domain. Some fields, like medicine or law, may require as much data as general-purpose fine-tuning due to their vast technical corpora. Others, such as e-commerce or hospitality, might need fewer samples, more in line with task-specific fine-tuning.\n\nThe key factors determining the data needs for domain-specific models are the “size” of the domain (i.e., the extent of its specialized knowledge and vocabulary) and the representation of that domain in the model’s pre- training data. Domains that are well-represented in the original training data may require less fine-tuning, while those that are more specialized or underrepresented may need more extensive datasets. Even with open-source LLMs, many pre-training datasets are closed-source, which requires making educated guesses to determine their composition (e.g., 30% code or 20% math).\n\nOceanofPDF.com\n\nData curation\n\nWhen it comes to procuring data for fine-tuning, the approaches differ between task-specific and domain-specific models. For task-specific models, data curation often involves collecting examples of the desired task from existing datasets or creating new ones. This might involve gathering pairs of original and summarized texts for a summarization model or collecting sentences in different languages for a translation model.\n\nDomain-specific data curation can be more challenging. It often requires collaboration with subject matter experts to gather and validate relevant texts, research papers, technical documents, and other domain-specific content. In some cases, it may involve partnering with organizations or institutions that have access to large repositories of specialized information. The quality and relevance of this data is crucial, as it directly impacts the model’s ability to understand and generate content in the target domain.\n\nIt’s worth noting that few-shot prompting has emerged as an alternative strategy to fine-tuning, especially for task-specific applications. This approach leverages the capabilities of large, powerful models by providing a few examples of the desired task within the input prompt. While not a replacement for fine-tuning in all scenarios (e.g., when you want to learn a new domain), few-shot prompting can be an efficient way to adapt models to new tasks without the need for extensive additional training.\n\nIn practice, the line between task-specific and domain-specific models can sometimes blur. For instance, a model fine-tuned for medical diagnosis\n\ncould be considered both task-specific (focused on diagnosis) and domain- specific (specialized in medical knowledge). The key is to understand the primary goal of the fine-tuning process and tailor the approach accordingly.\n\nAt this point in the process, we should have a collection of datasets suited for our use case. The next step consists of refining the quality of the samples through rule-based filtering, data duplication, data decontamination, and data quality evaluation.\n\nOceanofPDF.com\n\nRule-based filtering\n\nRule-based filtering is a systematic approach to data quality control that relies on explicit, predefined rules to evaluate and filter data samples. These rules are typically designed to address common quality issues and can range from simple checks to more complex logical operations. The primary goal of rule-based filtering is to maintain a high standard of data quality by removing samples that do not meet specific criteria.\n\nLength filtering is a straightforward yet effective rule-based filtering technique. This method involves setting thresholds for the acceptable length of responses in the dataset. Extremely short responses often lack sufficient information to be meaningful, while excessively long ones may contain irrelevant or redundant content. It’s important to note that the appropriate length thresholds can vary significantly depending on the specific task and domain. For example, a dataset for generating concise summaries might have a lower maximum threshold compared to one for detailed explanations.\n\nKeyword exclusion is another powerful rule-based filtering technique that focuses on the content of the samples rather than their structure. This method involves creating a list of keywords or phrases associated with low-quality or inappropriate content, and then filtering out any samples that contain these terms. The keyword list can include obvious indicators of low quality, such as profanities or spam-related terms, as well as domain-specific words that might indicate irrelevant or off- topic content. For instance, in a dataset for a professional writing assistant, you might exclude samples containing slang terms or informal expressions that don’t align with the intended tone and style.\n\nFormat checking is recommended for datasets that include structured data or follow specific formatting requirements. This technique ensures that all samples adhere to the expected format, maintaining consistency and facilitating processing downstream. Format checking can be particularly important for datasets containing code samples, JSON structures, or other formatted text. For example, in a dataset of programming instructions and solutions, you might implement rules to verify that code samples are syntactically correct and follow specified style guidelines.\n\nRule-based filtering offers significant advantages in preparing instruction datasets. Its speed and efficiency allow for rapid application to large volumes of data, making it highly scalable. The consistency of rule application ensures uniform treatment of data, reducing human error and bias. Furthermore, the explicit definition of filtering criteria provides transparency and interpretability, facilitating easy understanding, auditing, and adjustment. The ability to automate rule-based filtering reduces the need for manual intervention and enables continuous data quality monitoring.\n\nHowever, rule-based filtering also has limitations that must be considered. Predefined rules may lack the nuance required to capture the full complexity of language and context, potentially leading to the removal of valid but unusual samples. The typically binary nature of rules (pass/fail) may not always align with the nuanced nature of language and instruction quality. Additionally, as data patterns and quality standards evolve, rules need regular review and updates to remain effective. There’s also a risk that poorly designed rules could inadvertently introduce or amplify biases in the dataset.\n\nOceanofPDF.com\n\nData deduplication\n\nDataset diversity is fundamental to training models that can generalize well to new, unseen data. When a dataset contains duplicates or near-duplicates, it can lead to several issues:\n\nOverfitting: Models may memorize specific examples rather than learning general patterns.\n\nBiased performance: Overrepresented data points may skew the model’s performance towards certain types of inputs.\n\nInefficient training: Redundant data can increase training time without providing additional valuable information.\n\nInflated evaluation metrics: Duplicate data in test sets may lead to overly optimistic performance estimates.\n\nTo deduplicate datasets, we distinguish between exact and fuzzy deduplication. Exact deduplication removes identical samples through a straightforward process involving data normalization, hash generation, and duplicate removal. Data normalization standardizes the format of entries, such as converting text to lowercase. Hash\n\ngeneration then creates unique hashes for each entry using algorithms like MD5 or SHA-256. These hashes are compared to find matches, and duplicates are removed, leaving only one instance of each. While effective for identical entries, exact deduplication does not detect near- duplicates or semantically similar content, requiring more advanced techniques for those cases.\n\nThe most popular approach to fuzzy deduplication is MinHash deduplication. Compared to other fuzzy techniques, it maintains high accuracy while significantly reducing computational complexity. MinHash operates by generating compact representations, or signatures, for each data item. These signatures serve as fingerprints that capture the essence of the data while drastically reducing its dimensionality. In practice, MinHash transforms data items (such as text documents) into sets of shingles, applies multiple hash functions to these sets, and selects the minimum hash values to form signature vectors. These signatures can then be compared using similarity measures like Jaccard similarity to efficiently identify near-duplicates.\n\nIn addition to exact and fuzzy deduplication, semantic similarity takes a different approach by focusing on the meaning of text for deduplication. This method involves converting words or entire samples into vector representations using various natural language processing techniques. Word embedding models such as Word2Vec, GloVe, and FastText transform individual words into dense vectors, capturing semantic relationships.\n\nFor more context-aware representations, language models like BERT, sentence transformers, or cross-encoders can generate embeddings for entire sentences or documents. Once these vector representations are obtained, deduplication can be performed by comparing the similarity between vectors. Common similarity measures include cosine similarity or\n\nEuclidean distance. Samples with high similarity scores above a predefined threshold can be considered duplicates. For large datasets, clustering techniques may be applied to group similar vectors. Methods like K-means, DBSCAN, or hierarchical clustering can efficiently organize the vector space, allowing for the identification of clusters that represent semantically similar content. Within each cluster, a representative sample can be retained while others are marked as duplicates.\n\nOceanofPDF.com\n\nData decontamination\n\nData decontamination is the process of ensuring that the training dataset does not contain samples that are identical or highly similar to those in the evaluation or test sets. This step is important for ensuring the quality of the model evaluation and preventing overfitting or memorization of test data.\n\nData decontamination uses techniques from data deduplication. Exact matching can be used to remove any training samples that are identical to those in the evaluation sets. This can be done using hash functions or direct string comparisons. Next, we can also use near-duplicate detection methods to identify and remove training samples that are very similar to evaluation samples, even if they are not exactly the same. This often involves techniques like MinHash or computing similarity scores based on n-grams or embeddings.\n\nA simple way to perform data decontamination is to add your evaluation set to the instruction dataset during the data deduplication stage. In this case, we want to ensure that we only remove samples from the instruction dataset, which can be implemented in different ways (only filtering out the first duplicate, recording the indexes of the evaluation samples, etc.). Ideally, you can automatically add your evaluation sets in the data deduplication stage to fully automate this process. This is particularly efficient if you iterate over several versions of custom benchmarks.\n\nAnother aspect of data decontamination is filtering out samples that may have been derived from the same source as evaluation data. This can involve checking for overlapping phrases, similar sentence structures, or common metadata. Practitioners may also use provenance tracking (source the data they use) to identify and exclude data from specific sources that are known to be used in evaluation sets.\n\nOceanofPDF.com\n\nData quality evaluation\n\nData quality evaluation is a critical aspect of machine learning, particularly for LLMs. The process involves assessing various characteristics of datasets, including accuracy, diversity, and complexity. While some aspects like mathematical accuracy can be easily verified using tools such as Python interpreters, evaluating subjective or open-ended content remains challenging.\n\nTraditional methods of data quality assessment include human annotation, which generally provides high accuracy but is resource-intensive. To address scalability issues, machine learning techniques have been developed to automate the evaluation process. These include using LLMs as judges, reward models, and classifiers trained for quality prediction.\n\nThe LLM-as-a-judge strategy involves prompting LLMs to evaluate the quality of each sample. This approach has become popular due to its flexibility and ease of use, though it does present some challenges. Different LLMs have different levels of performance across tasks, and their evaluations often align more closely with those of non-experts. With domain-specific datasets, you might want to use domain-specific models instead of better, general-purpose LLMs. Comparative assessment methods (e.g., “Is answer A better than answer B?”) generally outperform absolute scoring approaches (e.g., “Rate answer A between 1 and 4”), though both can be used at scale with sufficient prompt engineering. We recommend iterating through different prompts over a representative subset to manually verify the quality of the responses. Table 5.2 shows an example of a custom prompt for a judge LLM.\n\nInstruction You are a data quality evaluator. Your goal is to assess an instruction and its corresponding answer, dete\n\nTable 5.2 – Example of LLM-as-a-judge prompt for data quality evaluation\n\nLLM-as-a-judge is known to have several biases. First, it has a position bias in comparative scoring, where the LLM judge favors the first answer. This can be addressed by randomizing the order of answers A and B. In addition, like humans, LLM judges favor long answers. Length normalization techniques can be applied to absolute scoring to mitigate this issue. Finally, LLM judges are known to have intra-model favoritism, meaning that they prefer models from the same family (GPT-4o with GPT-4 and GPT-4o mini, for example). This can be addressed by using several models instead of a single one.\n\nIn general, to improve evaluation reliability, strategies such as using multiple LLMs as a jury reduce bias and improve consistency. Leveraging a jury of smaller LLMs can also reduce costs while increasing accuracy and mitigating intra-model favoritism. For specific applications like chatbots, it’s advisable to aim for high agreement\n\nbetween LLM judges and human evaluators (around 80%). Simple grading scales (with few-shot prompting) and task-specific benchmarks are also recommended to ensure relevant and interpretable evaluations.\n\nReward models are another way to re-purpose LLMs for data quality evaluation. The term “reward model” comes from Reinforcement Learning from Human Feedback (RLHF, see Chapter 6). They can be broadly defined as models that take an instruction and answer pair and return a score as output. Generally, reward models are created by adding a linear head on top of a decoder-only architecture like Gemma or Llama. They are then trained for this specific purpose, using either reinforcement learning or traditional fine- tuning. Figure 5.3 shows ArmoRM-Llama3-8B-v0.1’s architecture, which adds regression and gating layers on top of a Llama 3 8B model. This model outputs multiple scores to target specific dimensions, such as helpfulness, correctness, coherence, complexity, and verbosity. This allows for a more fine-grained approach to data quality evaluation.\n\nFigure 5.3 – Architecture of RLHFlow/ArmoRM-Llama3-8B-v0.1, based on Llama 3 (Source: https://doi.org/10.48550/arXiv.2406.12845)\n\nThe Allen Institute for AI’s RewardBench leaderboard, hosted on Hugging Face (allenai/reward-bench), is a good resource for comparing different reward models. It combines various types of reward models (generative, classifiers, DPO, etc.) and evaluates them on a curated set of chosen and rejected answers for each instruction. While this task is not directly related to instruction data quality, it is a good resource for finding models capable of differentiating between good and bad answers.\n\nClassifiers or encoder-only models can be trained to perform data quality evaluation. A good example is HuggingFaceFW/fineweb-edu-classifier, a classifier designed to judge the educational value of web pages. This model was designed as a quality filter for pretraining data but a similar approach can be taken to evaluate instruction samples at scale. In practice, fineweb-edu-classifier adds a classification head to an embedding model (Snowflake/snowflake-arctic-embed-m) and trains it for 20 epochs on 450,000 samples that are annotated by Llama 3 70B Instruct.\n\nThis approach relies on encoder-only models, which are both smaller and better suited to classification tasks. Thanks to their low number of parameters, these models are faster to run and can scale to millions of samples. However, they are not as accurate as bigger models, particularly for complex reasoning tasks where they lack the\n\nability to capture nuances. At smaller scale, encoder-only models are still valuable to filter out outliers or as part of an automated data pipeline, which requires faster processing.\n\nOceanofPDF.com\n\nData exploration\n\nData exploration is a continuous process that requires practitioners to become familiar with the training data. It involves both manual inspection and automated analysis, each playing a crucial role in understanding the dataset’s characteristics, strengths, and potential shortcomings.\n\nManual dataset exploration, though time-consuming, is an important step. It reveals errors and inconsistencies that automated processes might miss, including formatting issues, data entry mistakes, incoherent reasoning, and factual inaccuracies. This process provides qualitative insights into the dataset’s content and style. To enhance efficiency, researchers can employ techniques like stratified sampling (selecting diverse samples), systematic review (using a criteria checklist), and collaborative review (involving multiple reviewers).\n\nFigure 5.4 shows an example with Argilla, a collaborative platform for manual data quality evaluation and exploration.\n\nFigure 5.4 – Argilla’s interface for collaborative data quality evaluation and exploration\n\nStatistical analysis is a complementary technique that reveals vocabulary diversity, potential biases, and concept representation. This process utilizes natural language processing libraries like NLTK or spaCy for tokenization and analysis of large text volumes. Visualization tools such as Matplotlib or Seaborn create histograms and word clouds, enabling intuitive pattern recognition. These techniques provide insights into dataset composition, language breadth, and possible cultural or contextual preferences, which can influence model outputs.\n\nTopic clustering automatically groups similar documents or pieces of text together, revealing underlying themes and patterns within the data. This process is especially important for understanding the content of large text corpora, identifying trends, and organizing information in a meaningful way. It is often associated with data visualization, with figures that show clusters of similar samples.\n\nLet’s consider the task of building an instruction dataset about various programming languages. You have collected a vast corpus of programming- related text from online forums, documentation, and tutorials. First, topic clustering can help identify the distinct programming languages present in the dataset (Python, JavaScript, etc.). Second, within each language cluster, you can further identify sub-topics like\n\nerror handling\n\n,\n\ndata structures\n\n, and\n\nweb frameworks\n\n. This allows a balanced representation of each language and sub-topic in the corpus.\n\nThis makes sure that each topic is correctly covered for each programming language.\n\nFigure 5.5 – Representation of the historical TikTok dataset made with Nomic Atlas\n\nSeveral tools are available for performing topic clustering, each with its own strengths and approaches. For example, Hugging Face’s text-\n\nclustering provides a simple pipeline with sentence transformers for embedding text into vector space, UMAP for dimensionality reduction, and DBSCAN for clustering. It also automatically labels clusters using an LLM and can output visualizations. Nomic Atlas (see Figure 5.5), BunkaTopics, and Lilac are alternatives proposing similar approaches with additional features.\n\nOceanofPDF.com\n\nData generation\n\nWhen the available instruction datasets are not sufficient, creating custom data becomes necessary. This is particularly relevant for specialized applications where publicly available data is scarce.\n\nAdditionally, it serves as a method to augment underrepresented areas in a dataset, like insufficient examples of JavaScript error-handling techniques in our previous example. While data can be generated manually by individuals or through crowdsourcing, these approaches often incur significant costs and time investments. Synthetic data generation using LLMs offers a more efficient and scalable alternative. This method, when combined with well-designed prompt engineering, can produce high-quality data at a much larger scale, effectively addressing the limitations of manual data creation processes.\n\nThe process of synthetic data generation typically begins with the preparation of a set of carefully designed prompts (sometimes called taxonomy). These serve as the foundation for generating new, diverse examples. Five seed prompts used in the original Alpaca dataset can be seen in Table 5.3. The quality of synthetically generated data largely depends on the prompts and techniques used in the generation process. Well-crafted prompts can guide the language model to produce diverse, relevant, and high-quality instruction-response pairs. These prompts often include specific instructions, examples, and constraints to ensure the generated data aligns with the desired format and content.\n\nSeed instructions Is there anything I can eat for breakfast that doesn’t include eggs, yet includes protein, and has ro\n\nTable 5.3 – Examples of seed prompts used in the original Alpaca dataset\n\nMany synthetic data generation pipelines incorporate multiple steps to ensure data quality. This may include generating an initial set of questions or instructions, followed by generating corresponding answers or responses. Some systems also implement validation steps, where another model or set of rules checks the generated pairs for accuracy, relevance, and adherence to specified criteria.\n\nAn important aspect of synthetic data generation is the ability to control various attributes of the generated data. This includes factors such as the complexity of the instructions, the length of the responses, the tone or style of the language used, and the specific topics or domains covered. By fine-tuning these parameters, it’s possible to create datasets that are tailored to specific training objectives or that complement existing datasets in targeted ways. Structured generation using libraries like Outlines can also be beneficial to adhere to specific formats.\n\nFurthermore, synthetic data generation can be particularly useful for addressing biases and gaps in existing datasets. By carefully designing the generation process, it’s possible to create more balanced and inclusive datasets that represent a wider range of perspectives, topics, and language styles. This can help in training LLMs that are more equitable and capable of serving diverse user bases.\n\nHowever, synthetic data generation also comes with challenges. One primary concern is the potential for the generated data to inherit biases or errors from the underlying language model used for generation. To mitigate this, many approaches incorporate human oversight, diverse prompts, and additional filtering mechanisms to ensure the quality and appropriateness of the generated data.\n\nAnother consideration is the need for the generated data to be sufficiently diverse and challenging. If the synthetic data is too simplistic or repetitive, it may not provide the level of complexity required to train a robust LLM. Advanced techniques in synthetic data generation often focus on creating varied and nuanced instruction-response pairs that can push the boundaries of what the model can learn.\n\nOceanofPDF.com\n\nData augmentation\n\nIn this context, data augmentation refers to the process of increasing both the quantity and the quality of data samples. Unlike data generation, we use pre-existing instruction samples as inputs in this stage. While it is possible to upsample pairs of instructions and answers, data augmentation is mostly used to increase the quality of existing samples. In particular, it focuses on two aspects: diversity and complexity.\n\nA pioneering approach in this field is the Evol-Instruct method, which uses LLMs to evolve simple instructions into more qualitative ones. The evolved instructions can then be used to generate answers using powerful LLMs. This method employs two main strategies: in-depth and in-breadth evolving.\n\nIn-depth evolving focuses on enhancing the complexity of existing instructions. It includes several techniques:\n\nConstraints: It involves introducing additional requirements or limitations to the original instruction, making it more challenging to fulfill.\n\nDeepening: Instead of shallow questions, it tries to find more deep questions, requiring more comprehensive responses.\n\nConcretizing: It replaces general concepts with more specific ones, adding detail and precision to the instruction.\n\nIncreasing reasoning steps: It modifies instructions to explicitly request multiple-step reasoning, promoting more complex problem-solving.\n\nComplicating input: This involves adding more complex data formats or structures to the instruction, such as XML, JSON, or code snippets.\n\nIn-breadth evolving, on the other hand, aims to expand the diversity of the instruction dataset. It generates entirely new instructions inspired by existing ones, focusing on creating more rare or long-tailed examples within the same domain.\n\nAs an example of concrete implementation, in-depth evolving can be automated with the following prompt, from the AutoEvol paper. You simply need to provide the instruction you want to evolve as input, and a powerful model like GPT-4o will return a more complex version of the original instruction.\n\nYou are an Instruction Rewriter that rewrites the given #Instruction# into a more complex version. Please follow th\n\nTable 5.4 – Evol LLM prompt from the “Automatic Instruction Evolving for Large Language Models” paper by Zeng et al. (2024)\n\nThe UltraFeedback method is another innovative approach, focused on answer quality instead of instruction quality. It employs AI feedback to enhance the quality and diversity of model responses. Unlike Evol- Instruct, which evolves instructions, UltraFeedback uses a large pool of diverse instructions and models to generate a wide range of responses.\n\nIt then leverages advanced language models like GPT-4 to provide detailed critiques and numerical scores for these responses across multiple dimensions such as instruction-following, truthfulness, honesty, and helpfulness.\n\nBased on these ideas, you can create your own augmentation techniques to create a more challenging and diverse instruction dataset. By refining and evolving existing instructions and answers, the resulting dataset can better train models to handle complex, multi-step tasks, and improve their performance across a wider range of applications.\n\nOceanofPDF.com\n\nCreating our own instruction dataset\n\nIn this section, we will create our own instruction dataset based on the crawled data from Chapter 3. To create a high-quality instruction dataset, we need to address two main issues: the unstructured nature of our data and the limited number of articles we can crawl.\n\nThis unstructured nature comes from the fact that we are dealing with raw text (articles), instead of pairs of instructions and answers. To address this issue, we will use an LLM to perform this transformation. Specifically, we will employ a combination of backtranslation and rephrasing. Backtranslation refers to the process of providing the expected answer as output and generating its corresponding instruction. However, using a chunk of text like a paragraph as an answer might not always be appropriate. This is why we want to rephrase the raw text to ensure we’re outputting properly formatted, high-quality answers. Additionally, we can ask the model to follow the author’s writing style to stay close to the original paragraph. While this process involves extensive prompt engineering, it can be automated and used at scale, as we will see in the following implementation.\n\nOur second issue regarding the limited number of samples is quite common in real-world use cases. The number of articles we can retrieve is limited, which constrains the size of the instruction dataset we are able to create. In this example, the more samples we have, the better the model becomes at imitating the original authors. To address this problem, we will divide our articles into chunks and generate three instruction-answer pairs for each chunk. This will multiply the number of samples we create while maintaining diversity in the final dataset. For simplicity, we will do it using OpenAI’s GPT-4o-mini model, but you can also use open-source models.\n\nHowever, LLMs are not reliable when it comes to producing structured output. Even when given specific templates or instructions, there’s no guarantee that the model will consistently adhere to them. This inconsistency often necessitates additional string parsing to ensure the output meets the desired format.\n\nTo simplify this process and ensure properly structured results, we can employ structured generation techniques. Structured generation is an effective method to force an LLM to follow a predefined template, such as JSON, pydantic classes, or regular expressions. In the following, we will use OpenAI’s JSON mode feature, which provides a more robust way to return valid JSON objects and reduce the need for extensive post-processing.\n\nBased on this description, the following figure summarizes every step of the synthetic data pipeline we want to build.\n\nFigure 5.6 – Synthetic data generation pipeline from raw text to instruction dataset\n\nLet’s now implement it in Python. You can implement it as part of the LLMOps pipeline, or as a standalone script:\n\nWe want to make sure that the following libraries are installed. The OpenAI library will allow us to interact with a model to generate the instruction data, and datasets will format it into a Hugging Face-compatible format. The tqdm library is installed to visualize the progress during the data generation process.\n\nopenai==\n\n1.37.1\n\ndatasets==\n\n2.20.0\n\ntqdm==\n\n4.66.4\n\nWe import all the required libraries as follows.\n\nimport\n\nconcurrent.futures\n\nimport\n\njson\n\nimport\n\nrandom\n\nimport\n\nre\n\nfrom\n\nconcurrent.futures\n\nimport\n\nThreadPoolExecutor\n\nfrom\n\ntyping\n\nimport\n\nList\n\n,\n\nTuple\n\nfrom\n\ndatasets\n\nimport\n\nDataset\n\nfrom\n\nopenai\n\nimport\n\nOpenAI\n\nfrom\n\npydantic\n\nimport\n\nBaseModel, Field\n\nfrom\n\ntqdm.auto\n\nimport\n\ntqdm\n\nThe raw data we have is a JSON file. We create a Hugging Face dataset from this JSON file by extracting specific fields from each article:\n\nid\n\n,\n\ncontent\n\n,\n\nplatform\n\n,\n\nauthor_id\n\n,\n\nauthor name\n\n, and\n\nlink\n\n.\n\ndef\n\nload_articles_from_json\n\n(\n\nfile_path:\n\nstr\n\n) -> Dataset:\n\nwith\n\nopen\n\n(file_path,\n\n\"r\"\n\n)\n\nas\n\nfile: data = json.load(file)\n\nreturn\n\nDataset.from_dict( {\n\n\"id\"\n\n: [item[\n\n\"id\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"content\"\n\n: [item[\n\n\"content\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"platform\"\n\n: [item[\n\n\"platform\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"author_id\"\n\n: [item[\n\n\"author_id\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"author_full_name\"\n\n: [item[\n\n\"author_full_name\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"link\"\n\n: [item[\n\n\"link\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]], } )\n\nIf we simply load our dataset as a pandas dataframe, it returns the following table.\n\nid\n\ncontent\n\n0\n\nab2f9e2e-5459-4dd6-97d6-c291de4a7093\n\nThe Importance of Data Pipelines in the Era of...\n\n1\n\nccfe70f3-d324-40b6-ba38-86e72786dcf4\n\nChange Data Capture: Enabling Event-Driven Arc...\n\n2\n\n4c9f68ae-ec8b-4534-8ad5-92372bf8bb37\n\nThe Role of Feature Stores in Fine-Tuning LLMs...\n\n...\n\n...\n\n...\n\n73\n\n68795a4d-26c2-43b7-9900-739a80b9b7dc DML: 4 key ideas you must know to train an LLM...\n\n74\n\nd91b17c0-05d8-4838-bf61-e2abc1573622\n\nDML: How to add real-time monitoring & metrics...\n\n75\n\ndcf55b28-2814-4480-a18b-a77d01d44f5f\n\nDML: Top 6 ML Platform Features You Must Know ...\n\nIf we inspect the content of some articles a little further, we realize that some of them have special characters and redundant whitespaces. We can clean this with a simple regex.\n\nFirst, we use\n\n[^\\w\\s.,!?']\n\nplatf\n\nmedi\n\nmedi\n\nmedi\n\n...\n\ndeco\n\ndeco\n\ndeco\n\nto remove non-alphanumeric characters except for apostrophes, periods, commas, exclamation marks, and question marks. Then, we use\n\n\\s+\n\nto replace multiple consecutive whitespace characters with a single space.\n\nFinally, we implement\n\nstrip()\n\nto remove any leading or trailing whitespace.\n\ndef\n\nclean_text\n\n(\n\ntext\n\n): text = re.sub(\n\nr\"[^\\w\\s.,!?']\"\n\n,\n\n\" \"\n\n, text) text = re.sub(\n\nr\"\\s+\"\n\n,\n\n\" \"\n\n, text)\n\nreturn\n\ntext.strip()\n\nNow that we can load our articles, we need to chunk them before turning them into pairs of instructions and answers. Ideally, you would want to use headlines or paragraphs to produce semantically meaningful chunking.\n\nHowever, in our example, like in the real world, raw data tends to be messy. Due to improper formatting, we cannot extract paragraphs or headlines for every article in our raw dataset. Instead, we will extract sentences using a regex to get chunks between 1,000 and 2,000 characters. This number can be optimized depending on the density of the information contained in the text.\n\nThe\n\nextract_substrings\n\nfunction processes each article in the dataset by first cleaning the text and then using a regex to split it into sentences. It then builds chunks of text by concatenating these sentences until each chunk is between 1,000 and 2,000 characters long.\n\ndef\n\nextract_substrings\n\n(\n\ndataset: Dataset, min_length:\n\nint\n\n=\n\n1000\n\n, max_length:\n\nint\n\n=\n\n2000\n\n) ->\n\nList\n\n[\n\nstr\n\n]: extracts = [] sentence_pattern =\n\nr\"(?\n\nfor\n\narticle\n\nin\n\ndataset[\n\n\"content\"\n\n]: cleaned_article = clean_text(article) sentences = re.split(sentence_pattern, cleaned_article) current_chunk =\n\n\"\"\n\nfor\n\nsentence\n\nin\n\nsentences: sentence = sentence.strip()\n\nif\n\nnot\n\nsentence:\n\ncontinue\n\nif\n\nlen\n\n(current_chunk) +\n\nlen\n\n(sentence) <= max_length: current_chunk += sentence +\n\n\" \"\n\nelse\n\n:\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip()) current_chunk = sentence +\n\n\" \"\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip())\n\nreturn\n\nextracts\n\nNext, we want to create instruction-answer pairs from the extracted chunks of text. To manage these pairs effectively, we introduce the\n\nInstructionAnswerSet\n\nclass. This class allows us to create instances directly from JSON strings, which is useful when parsing the output from the OpenAI API.\n\nclass\n\nInstructionAnswerSet\n\n:\n\ndef\n\n__init__\n\n(\n\nself, pairs:\n\nList\n\n[\n\nTuple\n\n[\n\nstr\n\n,\n\nstr\n\n]]\n\n): self.pairs = pairs\n\n@classmethod\n\ndef\n\nfrom_json\n\n(\n\ncls, json_str:\n\nstr\n\n) ->\n\n'InstructionAnswerSet'\n\n: data = json.loads(json_str) pairs = [(pair[\n\n'instruction'\n\n], pair[\n\n'answer'\n\n])\n\nfor\n\npair\n\nin\n\ndata[\n\n'instruction_answer_pairs'\n\n]]\n\nreturn\n\ncls(pairs)\n\ndef\n\n__iter__\n\n(\n\nself\n\n):\n\nreturn\n\niter\n\n(self.pairs)\n\nNow that we have a set of extracts from the articles with a reasonable length, we can use an LLM to transform them into pairs of instructions and answers. Note that this step is model-agnostic and can be implemented with any open-source or closed-source model. Because this output is grounded in the context we provide, it doesn’t require complex reasoning or high-performing models.\n\nFor convenience, we will use GPT-4o mini in this example. This choice is motivated by the low cost and good performance of this model. Prompt engineering is the most important aspect of this data transformation stage and requires several iterations to produce the expected outputs. We recommend starting with simple prompts and adding complexity when required to be more accurate, modify the style, or output multiple responses.\n\nIn our example, we want to create instructions like “Write a paragraph about X topic” and corresponding answers that are factual and imitate the writer’s style. To implement this, we need to provide an extract that will ground the model’s responses. For efficiency, we also choose to generate five instruction-answer pairs for each extract. Here’s the beginning of our function for instruction generation, including our prompt.\n\ndef\n\ngenerate_instruction_answer_pairs\n\n(\n\nextract:\n\nstr\n\n, client: OpenAI\n\n) ->\n\nList\n\n[\n\nTuple\n\n[\n\nstr\n\n,\n\nstr\n\n]]: prompt =\n\nf\"\"\"Based on the following extract, generate five instruction-answer pairs. Each instruction \\\n\nmust ask to write about a specific topic contained in the context. each answer \\\n\nmust provide a relevant paragraph based on the information found in the \\\n\ncontext. Only use concepts from the context to generate the instructions. \\\n\nInstructions must never explicitly mention a context, a system, a course, or an extract. \\\n\nInstructions must be self-contained and general. \\\n\nAnswers must imitate the writing style of the context. \\\n\nExample instruction: Explain the concept of an LLM Twin. \\\n\nExample answer: An LLM Twin is essentially an AI character that mimics your writing style, personality, and voice. \\\n\nIt's designed to write just like you by incorporating these elements into a language model. \\\n\nThe idea is to create a digital replica of your writing habits using advanced AI techniques. \\\n\nProvide your response in JSON format with the following structure:\n\n{{\n\n\"instruction_answer_pairs\": [\n\n{{\"instruction\": \"...\", \"answer\": \"...\"}},\n\n...\n\n]\n\n}}\n\nExtract:\n\n{extract}\n\n\"\"\"\n\nIn addition to the user prompt, we can also specify a system prompt to guide the model into generating the expected instructions. Here, we repeat our high-level task in the system prompt.\n\nThe concatenation of the system and user prompts is fed to the OpenAI API, using the GPT-4o mini model in JSON mode and a maximum of 1,200 tokens in the answer. We also use a standard temperature of\n\n0.7\n\nto encourage diverse responses. The generated text is directly parsed using the InstructionAnswerSet class to return pairs of instructions and answers.\n\ncompletion = client.chat.completions.create( model=\n\n\"gpt-4o-mini\"\n\n, messages=[ {\n\n\"\n\nrole\"\n\n:\n\n\"system\"\n\n,\n\n\"content\"\n\n:\n\n\"You are a helpful assistant who \\\n\ngenerates instruction-answer pairs based on the given context. \\\n\nProvide your response in JSON format.\"\n\n, }, {\n\n\"role\"\n\n:\n\n\"user\"\n\n,\n\n\"content\"\n\n: prompt}, ], response_format={\n\n\"type\"\n\n:\n\n\"json_object\"\n\n}, max_tokens=\n\n1200\n\n, temperature=\n\n0.7\n\n, )\n\n# Parse the structured output\n\nresult = InstructionAnswerSet.from_json(completion.choices[\n\n0\n\n].message.content)\n\n# Convert to list of tuples\n\nreturn\n\nresult.pairs\n\nLet’s create a main function to automate the process. It extracts substrings from the input dataset, then uses concurrent processing via Python’s\n\nThreadPoolExecutor\n\nto efficiently generate instruction-answer pairs for each extract.\n\nWe use a default\n\nmax_workers\n\nvalue of 4 because higher values tend to exceed OpenAI’s rate limits, potentially causing API request failures or throttling.\n\ndef\n\ncreate_instruction_dataset\n\n(\n\ndataset: Dataset, client: OpenAI, num_workers:\n\nint\n\n=\n\n4\n\n) -> Dataset: extracts = extract_substrings(dataset) instruction_answer_pairs = []\n\nwith\n\nconcurrent.futures.ThreadPoolExecutor(max_workers=num_workers)\n\nas\n\nexecutor: futures = [executor.submit(generate_instruction_answer_pairs, extract, client)\n\nfor\n\nextract\n\nin\n\nextracts ]\n\nfor\n\nfuture\n\nin\n\ntqdm(concurrent.futures.as_completed(futures), total=\n\nlen\n\n(futures) ): instruction_answer_pairs.extend(future.result()) instructions, answers =\n\nzip\n\n(*instruction_answer_pairs)\n\nreturn\n\nDataset.from_dict( {\n\n\"instruction\"\n\n:\n\nlist\n\n(instructions),\n\n\"output\"\n\n:\n\nlist\n\n(answers)} )\n\nWe can create our instruction dataset by calling this function. Running it over the raw data with GPT-4o mini costs less than 0.5$.\n\nWe can now create a main function to orchestrate the entire pipeline. It loads the raw data, creates the instruction dataset, splits it into training and testing sets, and pushes the result to the Hugging Face Hub.\n\ndef\n\nmain\n\n(\n\ndataset_id:\n\nstr\n\n) -> Dataset: client = OpenAI()\n\n# 1. Load the raw data\n\nraw_dataset = load_articles_from_json(\n\n\"cleaned_documents.json\"\n\n)\n\nprint\n\n(\n\n\"Raw dataset:\"\n\n)\n\nprint\n\n(raw_dataset.to_pandas())\n\n# 2. Create instructiondataset\n\ninstruction_dataset = create_instruction_dataset(raw_dataset, client)\n\nprint\n\n(\n\n\"Instruction dataset:\"\n\n)\n\nprint\n\n(instruction_dataset.to_pandas())\n\n# 3. Train/test split and export\n\nfiltered_dataset = instruction_dataset.train_test_split(test_size=\n\n0.1\n\n) filtered_dataset.push_to_hub(\n\n\"mlabonne/llmtwin\"\n\n)\n\nreturn\n\nfiltered_dataset\n\nDataset({\n\nfeatures: [\n\n'instruction'\n\n,\n\n'output'\n\n],\n\nnum_rows:\n\n3335\n\n})\n\nWe obtained 3,335 pairs with this process. You can find our version of the dataset at https://huggingface.co/datasets/mlabonne/llmtwin. The Hugging Face Hub provides a convenient dataset viewer (see Figure 5.7) to explore instructions and answers and make sure that there are no obvious mistakes in these samples. Due to the small size of the dataset, there is no need for comprehensive exploration and topic clustering.\n\nFigure 5.7 – The mlabonne/llmtwin instruction dataset on the Hugging Face Hub\n\nAs seen in the previous section, we could refine this instruction dataset by increasing the diversity and complexity of our samples. More advanced prompt engineering could also increase the quality of the generated data by providing examples of the expected results, for instance. Finally, quality evaluation could help filter out low- quality samples by reviewing them individually. For conciseness and simplicity, we will keep a straightforward approach for this instruction dataset and explore more advanced methods in Chapter 6 when we create a preference dataset.\n\nIn the next section, we will introduce SFT techniques, as well as related concepts.\n\nOceanofPDF.com\n\nExploring SFT and its techniques\n\nSFT consists of re-training pre-trained models on a smaller dataset composed of pairs of instructions and answers. The goal of SFT is to turn a base model, which can only perform next-token prediction, into a useful assistant, capable of answering questions and following instructions. SFT can also be used to improve the general performance of the base model (general-purpose SFT), instill new knowledge (e.g., new languages, domains, etc.), focus on specific tasks, adopt a particular voice, and so on.\n\nIn this section, we will discuss when to use fine-tuning and explore related concepts with storage formats and chat templates. Finally, we will introduce three popular ways of implementing SFT: full- finetuning, Low-Rank Adaptation (LoRA) and Quantization-aware Low-Rank Adaptation (QLoRA).\n\nOceanofPDF.com\n\nWhen to fine-tune\n\nIn most scenarios, it is recommended to start with prompt engineering instead of directly fine-tuning models. Prompt engineering can be used with either open-weight or closed-source models. By using techniques like few-shot prompting or retrieval augmented generation (RAG), numerous problems can efficiently be tackled without SFT. Prompt engineering also allows us to build a robust evaluation pipeline, which measures metrics like accuracy, but also cost and latency. If these results do not match the requirements, we can explore the possibility of creating an instruction dataset, as illustrated in the previous section. If enough data is available, fine-tuning becomes an option.\n\nFigure 5.8 – Basic flowchart to determine when fine-tuning is an option on a technical level\n\nBeyond these technical considerations, SFT answers common needs in terms of control (“know your data”) and customizability (the fine-tuned\n\nmodel is unique). Instead of building applications around a chatbot, fine- tuning allows developers to create more diverse interactions with LLMs, like tool analytics, moderation, and additional context. Note that if we focus on open-weight models in this book, several LLM providers offer automated fine-tuning services. While they don’t offer the same level of control and customizability as managing your own fine-tuning pipeline, it can be an interesting trade-off in specific scenarios (e.g., limited resources in terms of machine learning engineering).\n\nDespite these advantages, fine-tuning also has limitations. It is generally understood that SFT leverages pre-existing knowledge in the base model’s weights and refocuses the parameters for a specific purpose. This has several implications. First of all, knowledge that is too distant from what has been learned in the pre-training set (such as an unknown or rare language) can be difficult to learn effectively.\n\nEven worse, a study showed that fine-tuning a model on new knowledge could result in more frequent hallucinations. Depending on the SFT technique that is used, we’re also at risk of erasing knowledge that was present in the base model (a common issue referred to as “catastrophic forgetting”).\n\nOceanofPDF.com\n\nInstruction dataset formats\n\nInstruction datasets are stored in a particular format to organize instructions and answers. Typically, each sample in the dataset can be represented as a Python dictionary, where keys are prompt types like\n\nsystem\n\n,\n\ninstruction\n\n,\n\noutput\n\n, and values corresponding to the actual text. The three most standard formats are Alpaca, ShareGPT, and OpenAI. The following table shows how these data formats are generally organized.\n\nName\n\nJSONL format\n\nAlpaca\n\n{“instruction”: “...”, “input”: “...”, “output”: “...”} {“instruction”: “...”, “output”: “...”}\n\nShareGPT\n\n{“conversations”: [{“from”: “...”, “value”: “...”}, …]}\n\nOpenAI\n\n{“conversations”: [{“role”: “...”, “content”: “...”}, …]}\n\nOASST\n\n{“INSTRUCTION”: “...”, “RESPONSE”: “...”}\n\nRaw text\n\n{“text”: “...”}\n\nTable 5.5 – Examples of instruction data storage format\n\nNote that for Alpaca, the “\n\ninput\n\n\" key is optional. The content of the “\n\ninput",
      "page_number": 558
    },
    {
      "number": 6,
      "title": "when we create a preference dataset",
      "start_page": 701,
      "end_page": 1105,
      "detection_method": "regex_chapter",
      "content": "\" key is only appended to the content of the “\n\ninstruction\n\n\" key when it exists. We also added the “\n\nraw text\n\n\" data format to show that SFT is not inherently different from pre-training. If you choose to re-train a model on raw text, this is a type of fine-tuning generally called “continual pre-training.”\n\nThe dataset we created in the previous section has two columns (“\n\ninstruction\n\n\" and “\n\noutput\n\n\") and corresponds to the Alpaca format. Alpaca is sufficient for single-turn instructions and answers, which means it is limited to one instruction and one answer. When you want to process conversations (multiple instructions and answers), formats like ShareGPT or OpenAI are a better fit. By storing each message as a dictionary in a list, they can represent an arbitrarily long conversation in each sample.\n\nThe choice of single-turn and multi-turn conversations directly impacts the storage type and depends on the end use case.\n\nOceanofPDF.com\n\nChat templates\n\nOnce the instruction-answer pairs are parsed from the dataset format, we want to structure them in a chat template. Chat templates offer a unified way to present the instructions and answers to the model.\n\nIn general, they also include special tokens to identify the beginning and the end of a message, or who is the author of the message. Since base models are not designed to follow instructions, they don’t have a chat template. This means that you can choose any template when you fine-tune a based model. If you want to fine-tune an instruct model (not recommended), you need to use the same template or it might degrade your performance.\n\nLike instruction dataset formats, there are different chat templates: ChatML, Llama 3, Mistral, and many others. In the open-source community, the ChatML template (originally from OpenAI) is a popular option. It simply adds two special tokens\n\n(<|im_start|> and <|im_end|>\n\n) to indicate who is speaking. To give you an example, here is what we obtain when we apply the ChatML template to the instruction-answer pair shown in Table 5.1:\n\n<|im_start|>system You are a helpful assistant, who always provide explanation. Think like you are answering to a\n\nTable 5.6 – Sample from Table 5.1 with the ChatML chat template\n\nAs you can see, we still have three distinct parts: system, user, and assistant. Each part starts with the\n\n<|im_start|>\n\ntoken and ends with\n\n<|im_end|>.\n\nThe current speaker is identified by a string (like “\n\nsystem\n\n\") instead of a special token. This is the exact string that is tokenized and used as input by the model during fine- tuning.\n\nHowever, during inference, we can’t provide the expected answer. In this case, we provide the system and user part as shown in Figure 5.6, and prompt the model to answer by adding\n\n<|im_start|>assistant\\n\n\n.\n\nBecause the model has been fine-tuned with this template, it understands that the next tokens should be an answer relevant to the user instruction and guided by the system prompt. This is how fine-tuned models acquire instruction-following capabilities.\n\nA common issue with chat templates is that every single whitespace and line break is extremely important. Adding or removing any character would result in a wrong tokenization, which negatively impacts the performance of the model. For this reason, it is recommended to use reliable templates like Jinja, as implemented in the Transformers library. Table 5.7 shows a few examples of such templates, including Alpaca, which is both the name of an instruction dataset format and a chat template.\n\nName\n\nJinja template\n\nAlpaca\n\n### Instruction: What is the capital of France? ### Response: The capital of France is Paris.\n\nChatML\n\n<|im_start|>user What is the capital of France?<|im_end|> <|im_start|>assistant The capital of France is\n\nLlama 3\n\n<|begin_of_text|><|start_header_id|>user<|end_header_id|> What is the capital of France?<|eot_id|><|s\n\nPhi-3\n\n<|user|> What is the capital of France?<|end|> <|assistant|> The capital of France is Paris.<|end|>\n\nGemma\n\nuser What is the capital of France? model The capital of France is Paris.\n\nTable 5.7 – Example of common chat templates\n\nJinja implements loops and conditions, which allow the same template to be used for training and inference (\n\nadd_generation_prompt\n\n).\n\nOceanofPDF.com\n\nParameter-efficient fine-tuning techniques\n\nWhile many techniques exist in the literature, SFT has converged on three main techniques: full fine-tuning, LoRA, and QLoRA. We will introduce each technique individually, and weigh their pros and cons depending on your use cases.\n\nFigure 5.9 – Architectural differences of the three main SFT techniques at the module level\n\nOceanofPDF.com\n\nFull fine-tuning\n\nFull fine-tuning refers to the most straightforward SFT technique, consisting of re-training every parameter in the base model. Like pre-training, SFT uses next-token prediction as its training objective. This means that the previously discussed structure of the dataset can be seen as the main difference between continual pre-training and full fine-tuning.\n\nThis method often provides the best results but requires significant computational resources. Memory usage depends on several factors, including model size, training techniques, and optimization methods. At its simplest, using a single-GPU setting, the memory required can be estimated using the following formula:\n\nFor a basic setup using 32-bit floating point (fp32) precision, we can estimate:\n\nParameters: Learnable weights and biases within a neural network. In a large language model, these are typically the weights in the attention mechanisms, feed-forward layers, and embedding layers. Cost: 4 bytes/parameter (FP32) or 2 bytes/parameter (FP16/BF16).\n\nGradients: Gradients are the partial derivatives of the loss function with respect to each model parameter. They indicate how much each parameter should be adjusted to minimize the loss. During training, gradients are\n\ncomputed for each parameter through backpropagation and are used to update the model parameters. Cost: 4 bytes/parameter.\n\nOptimizer states: Optimizer states are additional values maintained by optimization algorithms like Adam or AdamW. These typically include running averages of past gradients and past squared gradients for each parameter. They help in adapting the learning rate for each parameter and navigating the loss landscape more effectively. For instance, Adam maintains two additional values (momentum and variance) per parameter. Cost: 8 bytes/parameter (for Adam optimizer).\n\nActivations: Activations are the intermediate outputs of each layer in the neural network during the forward pass. For transformer-based models, this includes the outputs of attention mechanisms, feed-forward layers, and normalization layers. Activations need to be kept in memory during the forward pass to compute gradients in the backward pass, unless techniques like activation checkpointing are used. Cost: variable, but often negligible for small batch sizes.\n\nThis gives us a baseline of 16 bytes per parameter. This translates into 112 GB of VRAM for a 7 B model and 1,120 GB for a 70 B model. However, this is often an underestimate, as it doesn’t account for additional memory needed for activations, temporary buffers, and overhead from various training techniques.\n\nSeveral techniques can be employed to reduce memory usage during LLM fine-tuning. Model parallelism spreads the workload across multiple GPUs, though it adds some overhead. Gradient accumulation enables larger effective batch sizes without proportional memory increase. Memory- efficient optimizers like 8-bit Adam can reduce the footprint of optimizer\n\nstates. Activation checkpointing trades computation for memory by recalculating certain activations. When combined, these techniques can significantly lower memory usage. For instance, using mixed precision with model parallelism might reduce costs to around 14-15 bytes per parameter, compared to the 16-byte baseline. However, memory requirements remain substantial for large models even with these optimizations.\n\nIn addition, full fine-tuning directly modifies the pre-training weights, which makes it destructive by nature. If training doesn’t behave as expected, it might erase previous knowledge and skills – a phenomenon referred to as “catastrophic forgetting.” The same phenomenon can happen with continual pre-training, which generally makes these techniques more difficult to use. Due to this additional complexity and its high computational requirements, parameter-efficient techniques are often preferred to full fine-tuning to create task and domain-specific models.\n\nOceanofPDF.com\n\nLoRA\n\nLoRA is a parameter-efficient technique for fine-tuning LLMs. Developed to address the computational challenges associated with adapting massive neural networks, LoRA has quickly become a cornerstone technique in LLM fine-tuning.\n\nThe primary purpose of LoRA is to enable the fine-tuning of LLMs with significantly reduced computational resources. This is achieved by introducing trainable low-rank matrices that modify the behavior of the model without changing its original parameters. The key advantages of LoRA include:\n\nDramatically reduced memory usage during training\n\nFaster fine-tuning process\n\nPreservation of pre-trained model weights (non-destructive)\n\nAbility to switch between tasks efficiently by swapping LoRA weights\n\nThese benefits have made LoRA particularly attractive for researchers and developers working with limited computational resources, effectively democratizing the process of LLM fine-tuning.\n\nAt its core, LoRA employs a low-rank decomposition technique to update model weights efficiently. Instead of directly modifying the original weight\n\nmatrix together form a low-rank update to\n\n, LoRA introduces two smaller matrices,\n\n.\n\nand\n\n, which\n\nFigure 5.10 – LoRA adds the two trainable matrices frozen\n\npre-trained weights\n\nand\n\nand keeps the\n\nMathematically, this can be represented as:\n\nHere,\n\nis the original weight matrix,\n\nand\n\nare the LoRA matrices, and\n\nis the effective weight matrix used during inference.\n\nThe dimensions of matrices A and B are chosen such that their product has the same shape as\n\n, but with a much lower rank. This rank, typically\n\ndenoted as original weights approach significantly reduces the number of trainable parameters, leading to substantial memory savings and faster training times.\n\n, is a crucial hyperparameter in LoRA. During training, the are updated. This\n\nremain frozen, while only\n\nand\n\nTo implement LoRA effectively, we need to select the correct hyperparameters and target modules. LoRA comes with two hyperparameters:\n\nRank ( starting point is some cases. Larger ranks may capture more diverse tasks but could lead to overfitting.\n\n): Determines the size of the LoRA matrices. A common\n\n, but values up to 256 have shown good results in\n\nAlpha (\n\n): A scaling factor applied to the LoRA update. In practice,\n\nwe update the frozen weights\n\nby a factor of\n\n. This is why a\n\ncommon heuristic is to set\n\nto twice the value of\n\n, effectively\n\napplying a scaling factor of 2 to the LoRA update. You can experiment with different ratios in case of overfitting or underfitting.\n\nIn addition, it is possible to add a drop-out layer to prevent overfitting. The dropout rate is usually set between 0 and 0.1 as an optional regularization factor, which slightly decreases training speed.\n\nLoRA can be applied to various parts of the model architecture. Initially, LoRA was primarily focused on modifying the attention mechanism, specifically the query (Q) and value (V) matrices in transformer layers. However, experiments have demonstrated significant benefits in extending LoRA’s application to other key components of the model. These additional target modules include:\n\nKey (K) matrices in attention layers\n\nOutput projection layers (often denoted as O) in attention mechanisms\n\nFeed-forward or Multi-Layer Perceptron (MLP) blocks between attention layers\n\nLinear output layers\n\nHowever, it’s important to note that increasing the number of LoRA- adapted modules also increases the number of trainable parameters and, consequently, the memory requirements.\n\nUsing LoRA, it’s possible to fine-tune a 7B parameter model on a single GPU with as little as 14-18 GB of VRAM, depending on the specific configuration. This is a dramatic reduction compared to full fine-tuning, which would typically require multiple high-end GPUs. In terms of trainable parameters, LoRA drastically reduces the number compared to full fine-tuning. For example, even when targeting every module with a rank of 16, a Llama 3 8 B model only has 42 million trainable LoRA parameters out of 8 billion parameters, which is 0.5196% of the model’s parameters.\n\nIn terms of quality, LoRA can also achieve comparable or sometimes better results than full-fine-tuning. Multiple sets of LoRA weights can be combined for different tasks or domains, allowing flexible deployment and task switching without retraining. Different projects are specialized in multiple-LoRA serving, such as LoRAX. It’s also a feature supported by Hugging Face’s Text Generation Inference (TGI) and Nvidia Inference Microservices (NIM).\n\nOceanofPDF.com\n\nQLoRA\n\nIntroduced by Dettmers et al., QLoRA is a method for fine-tuning LLMs that addresses the challenges of high computational costs. By combining quantization techniques with LoRA, QLoRA allows developers to fine-tune models on relatively small, widely available GPUs.\n\nThe core of QLoRA’s approach involves quantizing the base model parameters to a custom 4-bit NormalFloat (NF4) data type, which significantly reduces memory usage. Like LoRA, instead of updating all model parameters during fine-tuning, QLoRA introduces small, trainable low-rank matrices (adapters) to specific layers of the model. Only these adapters are updated during training, while the original model weights remain unchanged. To further reduce memory usage, QLoRA employs double quantization, which quantizes the quantization constants themselves. Additionally, it uses paged optimizers to manage memory spikes during training by leveraging Nvidia’s unified memory feature.\n\nQLoRA provides significant memory savings compared to LoRA, reducing peak GPU memory usage by up to 75%. For example, for a 7B model, QLoRA reduces peak memory usage from 14 GB to 9.1 GB during initialization, a 35% reduction. During fine-tuning, the memory savings increase to 40%, from 15.6 GB for LoRA to 9.3 GB for QLoRA. However, this memory efficiency comes at the cost of increased training time, with QLoRA being about 30% slower than LoRA. In terms of model performance, QLoRA shows only minor differences compared to LoRA.\n\nIn summary, QLoRA is particularly beneficial when memory constraints are the primary concern, such as when working with very large models or on hardware with limited GPU memory. However, if training speed is crucial and sufficient memory is available, LoRA might be the preferred choice.\n\nThe decision between QLoRA and LoRA should be based on the specific requirements of the project, available hardware, and the need to balance memory usage, training speed, and model performance.\n\nOceanofPDF.com\n\nTraining parameters\n\nWhen fine-tuning LLMs, several hyperparameters guide the training process and significantly impact the model’s convergence, generalization, and overall effectiveness.\n\nOceanofPDF.com\n\nLearning rate and scheduler\n\nThe learning rate is the most important hyperparameter. It controls how much the model’s parameters are updated during training. It typically ranges from very small values like\n\n1e-6\n\nto larger values like\n\n1e-3\n\n. A common starting point for transformer models is often around\n\n1e-5\n\n. If the learning rate is too low, training progresses slowly and may get stuck in suboptimal solutions. Conversely, if it’s too high, training can become unstable or diverge, leading to poor performance. It’s often beneficial to experiment with different learning rates to find the optimal value for your specific task and model.\n\nThe learning rate scheduler adjusts the learning rate throughout the training process. It typically starts with a higher learning rate to enable rapid initial progress, then gradually decreases it in later stages to fine-tune the model more precisely. The two most common types of schedulers are linear and cosine. A linear scheduler decreases the learning rate steadily over time, while a cosine scheduler follows a cosine curve, decreasing more slowly at first and then more rapidly toward the end of training. For example, you might start with a learning rate of 3e-4 and decrease it to 1e-7 over the course of training. The specific values and decay schedule depend on your model and dataset, but a common approach is to use a warmup period (e.g.,\n\n5% of total steps) where the learning rate increases from 0 to the initial value, followed by a decay period for the remaining 95% of steps. This approach helps stabilize early training and allows for more refined updates as the model converges. In general, linear and cosine schedulers provide the same level of performance.\n\nOceanofPDF.com\n\nBatch size\n\nThe batch size determines the number of samples processed before the model’s weights are updated. Typical batch sizes for LLM fine-tuning range from 1 to 32, with common values being 1, 2, 4, 8, or 16. Larger batch sizes generally lead to more stable gradient estimates and can improve training speed, as they provide a better approximation of the true gradient of the entire dataset.\n\nHowever, they also require more memory, which can be a limiting factor on GPUs with less VRAM. For instance, a batch size of 16 might work well on a high-end GPU with 24GB of memory, while a smaller GPU with 8 GB might only handle a batch size of 2 or 4.\n\nTo overcome memory constraints while still benefiting from larger batch sizes, a technique called gradient accumulation can be used. It works by performing multiple forward and backward passes with smaller mini- batches, accumulating the gradients over these steps before applying a single update to the model’s parameters. This approach is particularly useful when working with large models or limited GPU memory. For example, if you want to achieve an effective batch size of 32 but your GPU can only handle 8 samples at a time, you can set the gradient accumulation steps to 4. This means you’ll process 4 mini-batches of 8 samples each, accumulating the gradients, and then update the model as if you had processed all 32 samples at once.\n\nThe number of gradient accumulation steps typically ranges from 1 (no accumulation) to 8 or 16, depending on the desired effective batch size and\n\navailable computational resources. When choosing the number of steps, consider the trade-off between training speed and memory usage. More accumulation steps allow for larger effective batch sizes but increase the time required for each update. Here’s a simple formula to determine the effective batch size:\n\nFor instance, if you’re using 2 GPUs, each processing a batch of 4 samples, with 4 gradient accumulation steps, your effective batch size would be\n\n4 * 2 * 4 = 32\n\nsamples.\n\nOceanofPDF.com\n\nMaximum length and packing\n\nThe maximum sequence length determines the longest input the model can process. It’s typically set between 512 and 4,096 tokens but can go up to 128,000 or more, depending on the task and available GPU memory. For example, a maximum length of 2,048 tokens is common for many language generation tasks, while RAG applications might use up to 8,192 tokens or more. When processing input data, sequences longer than this limit are truncated, meaning excess tokens are removed. Truncation can occur at the beginning (left truncation) or end (right truncation) of the sequence. For instance, with a maximum length of 1,024 tokens, a 1,500-token input would have 476 tokens removed. This parameter directly impacts batch size and memory usage; a batch size of 12 with a max length of 1,024 would contain 12,288 tokens (\n\n12 * 1,024\n\n), while the same batch size with a max length of 512 would only contain 6,144 tokens. It’s important to balance this parameter with your GPU capabilities and the nature of your training data to optimize performance and resource utilization.\n\nPacking maximizes the utilization of each training batch. Instead of assigning one sample per batch, packing combines multiple smaller samples into a single batch, effectively increasing the amount of data processed in each iteration. For example, if your maximum sequence length is 1,024 tokens, but many of your samples are only 200-300 tokens long, packing could allow you to fit 3-4 samples into each batch slot. This approach can significantly improve training efficiency, especially when dealing with datasets containing many short sequences. However, packing requires careful implementation to ensure that model attention doesn’t cross between\n\npacked samples. This is typically achieved by using attention masks that prevent the model from attending to tokens from different samples within the same packed sequence.\n\nOceanofPDF.com\n\nNumber of epochs\n\nThe number of epochs is another important parameter, representing the number of complete passes through the entire training dataset. For LLM fine-tuning, the typical range is 1 to 10 epochs, with many successful runs using 2 to 5 epochs. The optimal number depends on factors such as task complexity, dataset size, and model architecture. More epochs allow the model to refine its learning, potentially improving performance. However, there’s a crucial trade-off: too few epochs may lead to underfitting, while too many can cause overfitting. For example, a large model fine-tuned on a small dataset might only need 1-3 epochs, while a smaller model fine-tuned on a larger dataset could benefit from 5-10 epochs. It is helpful to monitor validation performance during training and implement early stopping if the model’s performance plateaus or degrades. This approach helps determine the optimal number of epochs dynamically and prevents overfitting.\n\nOceanofPDF.com\n\nOptimizers\n\nOptimizers adjust the model’s parameters to minimize the loss function. For LLM fine-tuning, AdamW (Adaptive Moment Estimation with Weight Decay) is highly recommended, particularly its 8-bit version. AdamW 8-bit performs comparably to the 32-bit version while using less GPU memory (but it doesn’t improve training speed). AdamW combines adaptive learning rates with weight decay regularization, often leading to better training stability and model performance.\n\nFor scenarios with severe memory constraints, AdaFactor presents an alternative designed for memory efficiency. It works well without explicit learning rate tuning, making it particularly useful in resource-constrained environments. However, it may not always match AdamW’s performance in all cases. In situations involving extremely large models or limited GPU memory, paged versions of optimizers, such as paged AdamW 8-bit, can further reduce memory consumption by offloading to CPU RAM. If memory allows and maximum performance is the priority, the non- quantized\n\nadamw_torch\n\noptimizer may be the best choice.\n\nOceanofPDF.com\n\nWeight decay\n\nWeight decay works by adding a penalty for large weights to the loss function, encouraging the model to learn simpler, more generalizable features. This helps the model avoid relying too heavily on any single input feature, which can improve its performance on unseen data. Typically, weight decay values range from 0.01 to 0.1, with 0.01 being a common starting point. For example, if you’re using the AdamW optimizer, you might set the weight decay to 0.01.\n\nWhile weight decay can be beneficial, setting it too high can impede learning by making it difficult for the model to capture important patterns in the data. Conversely, setting it too low may not provide sufficient regularization. The optimal weight decay value often depends on the specific model architecture and dataset, so it’s generally a good practice to experiment with different values.\n\nOceanofPDF.com\n\nGradient checkpointing\n\nGradient checkpointing is a technique that reduces memory consumption during training by storing only a subset of intermediate activations generated in the forward pass. In standard training procedures, all intermediate activations are retained in memory to facilitate gradient calculation during the backward pass. However, for very deep networks like LLMs, this approach can quickly become impractical due to hardware limitations, especially on GPUs with limited memory capacity.\n\nGradient checkpointing addresses this challenge by selectively saving activations at specific layers within the network. For layers where activations are not saved, they are recomputed during the backward pass as needed for gradient computation. This approach creates a trade-off between computation time and memory usage. While it significantly reduces memory requirements, it may increase overall computation time due to the need to recalculate some activations.\n\nOther parameters and techniques exist but play a minor role compared to those previously discussed. In the next section, we will explore how to select and tune these parameters using a concrete example.\n\nOceanofPDF.com\n\nFine-tuning in practice\n\nLet’s now fine-tune an open-source model on our custom dataset. In this section, we will show an example that implements LoRA and QLoRA for efficiency. Depending on the hardware you have available, you can select the technique that best corresponds to your configuration.\n\nThere are many efficient open-weight models we can leverage for task or domain-specific use cases. To select the most relevant LLM, we need to consider three main parameters:\n\nLicense: Some model licenses only allow non-commercial work, which is a problem if we want to fine-tune for a company. Custom licenses are common in this field, and can target companies with a certain number of users, for example.\n\nBudget: Models with smaller parameter sizes (<10 B) are a lot cheaper to fine-tune and deploy for inference than larger models. This is due to the fact that they can be run on cheaper GPUs and process more tokens per second.\n\nPerformance: Evaluating the base model on general-purpose benchmarks or, even better, domain- or task-specific benchmarks relevant to the final use case, is crucial. This helps ensure that the model has the necessary capabilities to perform well on the intended tasks after fine-tuning.\n\nIn this chapter, we will choose Llama 3.1 8B, an open-weight model released by Meta. It has a permissive custom license (“Llama 3.1 Community License Agreement”) that allows commercial use. With 8B parameters, it is small enough to fit on most GPUs while reaching a high level of performance compared to its competitors. We can verify this using the Open LLM Leaderboard, as well as other benchmarks detailed in the model card.\n\nThere are specialized tools and libraries to fine-tune models. In particular, we recommend the following:\n\nTRL: This is a library created and maintained by Hugging Face to train LLMs using SFT and preference alignment. It is a popular and reliable library that tends to be the most up-to-date in terms of algorithms. It works in single and multi-GPU settings with FSDP and DeepSpeed.\n\nAxolotl: Created by Wing Lian, this tool streamlines the fine-tuning of LLMs with reusable YAML configuration files. It is based on TRL but includes many additional features, such as automatically combining datasets stored in various formats. It also supports single- and multi-GPU settings with FSDP and DeepSpeed.\n\nUnsloth: Created by Daniel and Michael Han, Unsloth uses custom kernels to speed up training (2-5x) and reduce memory use (up to 80% less memory). It is based on TRL and provides many utilities, such as automatically converting models into the GGUF quantization format. At the time of writing, it is only available for single-GPU settings.\n\nTo maximize efficiency, we will perform fine-tuning using the Unsloth library. The following code is designed as part of our LLMOps pipeline, but can also be used as a stand-alone script. It can also be executed in different environments, like SageMaker, cloud GPUs (like Lambda Labs or RunPod), Google Colab, and many others. We tested it on different GPUs, like A40, A100, and L4.\n\nTo install the Unsloth library and its dependencies, we recommend directly installing from the GitHub repository of the book (https://github.com/PacktPublishing/LLM-Engineering) or Unsloth’s repo (https://github.com/unslothai/unsloth). This approach is recommended because the installation steps are regularly updated to address potential conflicts with dependencies:\n\nFirst, we want to access a gated model and (optionally) upload our fine- tuned model to Hugging Face (https://huggingface.co/). This requires being logged in to an account. If you don’t have an account, you can create it and store your API key (Settings | Access Tokens | Create new token) in the .env file:\n\nHF_TOKEN = YOUR_API_KEY\n\nMake sure that your Comet ML API key is also in the .env file:\n\nCOMET_API_KEY = YOUR_API_KEY\n\nImport all the necessary packages:\n\nimport\n\nos\n\nimport\n\ntorch\n\nfrom\n\ntrl\n\nimport\n\nSFTTrainer\n\nfrom\n\ndatasets\n\nimport\n\nload_dataset, concatenate_datasets\n\nfrom\n\ntransformers\n\nimport\n\nTrainingArguments, TextStreamerfrom unsloth\n\nimport\n\nFastLanguageModel, is_bfloat16_supported\n\nLet’s now load the model to fine-tune and its corresponding tokenizer. We use Unsloth’s FastLaguageModel class with the\n\n.from_pretrained()\n\nmethod. In addition to the model name, we need to specify the max sequence length (2,048 in this example). Finally, the\n\nload_in_4bit\n\nargument indicates if we want to use QLoRA (quantized pre-trained weights) or LoRA.\n\nWe’ll use LoRA in this example because of faster training and higher quality, but you can easily switch to QLoRA if you don’t meet the VRAM requirements.\n\nmax_seq_length =\n\n2048\n\nmodel, tokenizer = FastLanguageModel.from_pretrained( model_name=\n\n\"meta-llama/Meta-Llama-3.1-8B\"\n\n, max_seq_length=max_seq_length, load_in_4bit=\n\nFalse\n\n, )\n\nNow that the model is loaded, we can define our LoRA configuration. Here, we use a rank of 32 that is large enough to imitate the writing style and copy the knowledge from our instruction samples. You can increase this value to 64 or 128 if your results are underwhelming. We also set an alpha of 32, without dropout and without bias, to speed up training. Finally, we target every linear layer to maximize the quality of the fine-tuning process.\n\nmodel = FastLanguageModel.get_peft_model( model, r=\n\n32\n\n, lora_alpha=\n\n32\n\n, lora_dropout=\n\n0\n\n, target_modules=[\n\n\"q_proj\"\n\n,\n\n\"k_proj\"\n\n,\n\n\"\n\nv_proj\"\n\n,\n\n\"up_proj\"\n\n,\n\n\"down_proj\"\n\n,\n\n\"o_proj\"\n\n,\n\n\"gate_proj\"\n\n], )\n\nNext, we need to prepare the data in the right format for fine-tuning. In this example, we don’t have a lot of samples in the llmtwin dataset (3,000 samples). This is an issue because the model might not correctly learn the chat template. To address this, we will upsample it with a high-quality general-purpose dataset called FineTome. This is a filtered version of\n\narcee-ai/The-Tome using the fineweb-edu-classifier\n\n. Instead of using the 100,000 samples of this dataset, we will specify we only want 10,000 in the train split. We concatenate these two datasets to create our final set.\n\ndataset1 = load_dataset(\n\n\"mlabonne/llmtwin\"\n\n) dataset2 = load_dataset(\n\n\"mlabonne/FineTome-Alpaca-100k\"\n\n, split=\n\n\"train[:10000]\"\n\n) dataset = concatenate_datasets([dataset1, dataset2])\n\nNow, we need to format this data using a chat template. Let’s use the Alpaca template for convenience. This template doesn’t require additional tokens, which makes it less error-prone (but can slightly\n\nimpact performance compared to ChatML). Here, we map all the instructions and answers to the Alpaca template. We manually add the end of sentence (EOS) token at the end of each message to ensure that the model learns to output it. Without it, it will keep generating answers without ever stopping.\n\nalpaca_template =\n\n\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n\n{}\n\n### Response:\n\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token dataset = dataset.\n\nmap\n\n(format_samples, batched=\n\nTrue\n\n, remove_columns=dataset.column_names)\n\nOnce the dataset is ready, we can divide it into training (95%) and test (5%) sets for validation during training.\n\ndataset = dataset.train_test_split(test_size=\n\n0.05\n\n)\n\nThe model is now ready to be trained. The SFTTrainer() class stores all the hyperparameters for our training. In addition, we provide the model, tokenizer, LoRA configuration, and datasets. Following the recommendations from the previous section, we set a learning rate of\n\n3e-4\n\nwith a linear scheduler and a maximum sequence length of 2048. We train this model for three epochs with a batch size of 2 and 8 gradient accumulation steps (for an effective batch size of 16). We also choose the\n\nadamw_8bit\n\noptimizer with a\n\nweight_decay\n\nof 0.01. Depending on the GPU we use, it will automatically use FP16 or BF16 for the activations. Finally, we report our training run to Comet ML for experiment tracking.\n\ntrainer = SFTTrainer( model=model, tokenizer=tokenizer, train_dataset=dataset[\n\n\"train\"\n\n], eval_dataset=dataset[\n\n\"test\"\n\n], dataset_text_field=\n\n\"text\"\n\n, max_seq_length=max_seq_length, dataset_num_proc=\n\n2\n\n, packing=\n\nTrue\n\n, args=TrainingArguments( learning_rate=\n\n3e-4\n\n, lr_scheduler_type=\n\n\"linear\"\n\n, per_device_train_batch_size=\n\n2\n\n, gradient_accumulation_steps=\n\n8\n\n, num_train_epochs=\n\n3\n\n, fp16=\n\nnot\n\nis_bfloat16_supported(), bf16=is_bfloat16_supported(), logging_steps=\n\n1\n\n, optim=\n\n\"adamw_8bit\"\n\n, weight_decay=\n\n0.01\n\n, warmup_steps=\n\n10\n\n, output_dir=\n\n\"output\"\n\n, report_to=\n\n\"comet_ml\"\n\n, seed=\n\n0\n\n, ), ) trainer.train()\n\nTraining this model on our concatenated dataset can take a few hours. For example, it takes 50 minutes on an A100 GPU.\n\nOnce it’s done, we can test it with a quick example. The goal is not to properly evaluate the fine-tuned model, but to make sure that there are no obvious errors related to the tokenizer or chat template.\n\nFor fast inference, we can use\n\nFastLanguageModel.for_inference()\n\nfrom Unsloth. We directly format an instruction with the Alpaca format. Note that we provide an empty answer to append the assistant header (\n\n### Response\n\n): at the end of the user instruction. This forces the model to answer the instruction instead of completing it. We also use a text streamer to stream the generation instead of waiting for it to be complete before printing it.\n\nFastLanguageModel.for_inference(model) message = alpaca_prompt.\n\nformat\n\n(\n\n\"Write a paragraph to introduce supervised fine-tuning.\"\n\n,\n\n\"\"\n\n) inputs = tokenizer([message], return_tensors=\n\n\"pt\"\n\n).to(\n\n\"cuda\"\n\n) text_streamer = TextStreamer(tokenizer) _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=\n\n256\n\n, use_cache=\n\nTrue\n\n)\n\nHere is the answer provided by our model:\n\nSupervised fine-tuning is a method used to enhance a language model by providing it with a curated dataset of instructions and their corresponding answers. This process is designed to align the model's responses with human expectations, thereby improving its accuracy and relevance. The goal is to ensure that the model can respond effectively to a wide range of queries, making it a valuable tool for applications such as chatbots and virtual assistants.\n\nThis is correct and properly formatted with the Alpaca chat template.\n\nNow that our model has been successfully fine-tuned, we can save it locally and/or push it to the Hugging Face Hub using the following functions.\n\nmodel.save_pretrained_merged(\n\n\"model\"\n\n, tokenizer, save_method=\n\n\"merged_16bit\"\n\n) model.push_to_hub_merged(\n\n\"mlabonne/TwinLlama-3.1-8B\"\n\n, tokenizer, save_method=\n\n\"merged_16bit\"\n\n)\n\nCongratulations on fine-tuning a base model from scratch! During training, you can access Comet ML to monitor your training loss, validation loss, and many other metrics. You want to make sure that these metrics correspond to what is expected. Figure 5.11 shows the training run corresponding to the previous code in Comet ML.\n\nFigure 5.11 – Four monitored metrics during fine-tuning in Comet ML\n\nIn particular, three of these metrics are important to monitor:\n\nTraining loss: It measures how well the model is performing on the task it’s being trained for. The loss should continuously decrease on average, indicating improving performance. We expect a rapid decrease at the beginning of training, followed by a long plateau. Spikes and continuous increases in the loss value are signs that the training is failing. In this case, you might want to check the quality of your data, issues with the tokenizer, and tune parameters like learning rate and batch size. In Figure 5.11 (loss), you can see three different phases corresponding to our three epochs.\n\nValidation loss: It measures the loss using the validation set instead of the training set; a well-fitted model typically shows both training and validation losses decreasing and eventually stabilizing, with a small gap between them. This gap should be minimal but is expected to exist as the model will always perform slightly better on the training data. If the training loss continues to decrease while the validation loss starts to increase, it’s a sign of overfitting. Conversely, if both curves remain flat at a relatively high loss value, it indicates underfitting. There are no universal “recommended ranges” for loss values, as these depend on the specific problem and loss function used. However, you should look for convergence and stability in both curves. In Figure 4.11 (eval_loss), we see a slight increase at step 340. This is still acceptable but might indicate that the model starts to overfit.\n\nGradient norm: It represents the magnitude of the gradient vector during training. Large gradient norms can indicate training instability like overfitting, especially if accompanied by a divergence between training and validation losses. On the other hand, a stable or decreasing gradient norm generally means that the model is converging toward a local optimum. To mitigate issues associated with large gradient norms, gradient clipping can be employed. This technique involves setting a\n\nmaximum threshold for the gradient norm, effectively limiting the size of parameter updates.\n\nIt is often interesting to try different learning rates and select the best model based on the minimal loss. Note that this is a proxy for real evaluations, which are covered in the next chapter.\n\nOceanofPDF.com\n\nSummary\n\nThis chapter covered essential aspects of LLM fine-tuning, both in theory and practice. We examined the instruction data pipeline and how to create high-quality datasets, from curation to augmentation. Each pipeline stage offers optimization opportunities, particularly in quality assessment, data generation, and enhancement. This flexible pipeline can be adapted to your use cases by selecting the most relevant stages and techniques.\n\nWe applied this framework to real-world data from Chapter 3, using an LLM to convert raw text into instruction-answer pairs. We then explored SFT techniques. This included an analysis of SFT’s advantages and limitations, methods for storing and parsing instruction datasets with chat templates, and an overview of three primary SFT techniques: full fine- tuning, LoRA, and QLoRA. We compared these methods based on their impact on memory usage, training efficiency, and output quality. The chapter concluded with a practical demonstration that involved fine-tuning a Llama 3.1 8 B model on our custom instruction dataset. This example highlighted key steps and implementation details for successful fine-tuning.\n\nIn the next chapter, we will use preference alignment techniques to create a new version of TwinLlama-3.1-8B. We will generate a new dataset with chosen and rejected answers that will help us calibrate the type of answers we expect from our model. We will detail many applications that can benefit from this framework and how to implement it.\n\nOceanofPDF.com\n\nReferences\n\nTahori, Gulrajani, Zhang, Dubois, et al.. “Alpaca: A Strong, Replicable Instruction-Following Model” crfm.stanford.edu, March 13, 2023, https://crfm.stanford.edu/2023/03/13/alpaca.html.\n\nSubhabrata Mukherjee et al.. “Orca: Progressive Learning from Complex Explanation Traces of GPT-4.” arXiv preprint arXiv:2306.02707, June 2023.\n\nWing Lian and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and “Teknium”. “Open-Orca/OpenOrca.” huggingface.co, 2023, https://huggingface.co/datasets/Open-Orca/OpenOrca.\n\nWeihao Zeng et al.. “Automatic Instruction Evolving for Large Language Models.” arXiv preprint arXiv:2406.00770, June 2024.\n\nChunting Zhou et al.. “LIMA: Less Is More for Alignment.” arXiv preprint arXiv:2305.11206, May 2023\n\n01. AI. “Yi: Open Foundation Models by 01.AI.” arXiv preprint arXiv:2403.04652, March 2024.\n\nAlex Birch. “LLM finetuning memory requirements.” blog.scottlogic.com, November 24, 2023, https://blog.scottlogic.com/2023/11/24/llm-mem.html.\n\nQuentin Anthony et al.. “Transformer Math 101.” blog.eleuther.ai, April 18, 2023, https://blog.eleuther.ai/transformer-math/.\n\nEdward J. Hu et al.. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv preprint arXiv:2106.09685, June 2021.\n\nTim Dettmers et al.. “QLoRA: Efficient Finetuning of Quantized LLMs.” arXiv preprint arXiv:2305.14314, May 2023.\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\n6\n\nOceanofPDF.com\n\nFine-Tuning with Preference Alignment\n\nSupervised Fine-Tuning (SFT) has been crucial in adapting LLMs to perform specific tasks. However, SFT struggles to capture the nuances of human preferences and the long tail of potential interactions that a model might encounter. This limitation has led to the development of more advanced techniques for aligning AI systems with human preferences, grouped under the umbrella term preference alignment.\n\nPreference alignment addresses the shortcomings of SFT by incorporating direct human or AI feedback into the training process. This method allows a more nuanced understanding of human preferences, especially in complex scenarios where simple supervised learning falls short. While numerous techniques exist for preference alignment, this chapter will primarily focus on Direct Preference Optimization (DPO) for simplicity and efficiency.\n\nIn this chapter, we will talk about the type of data that is required by preference alignment algorithms like DPO. We will build our own dataset to modify the writing style of our model, making it less artificial and more authentic. We will introduce the DPO algorithm and implement it to align the model trained in Chapter 5.\n\nIn this chapter, we will cover the following topics:\n\nUnderstanding preference datasets\n\nHow to create our own preference dataset\n\nDirect preference optimization (DPO)\n\nImplementing DPO in practice to align our model\n\nBy the end of this chapter, you will be able to create your own preference datasets and align models with diverse techniques.\n\nAll the code examples from this chapter can be found on GitHub at https://github.com/PacktPublishing/LLM-Engineering.\n\nOceanofPDF.com\n\nUnderstanding preference datasets\n\nThe principles for creating high-quality preference datasets are the same as those discussed in Chapter 5 for instruction datasets. We want to maximize the accuracy, diversity, and complexity of our samples. To achieve this, we follow the same stages, as outlined in Figure 6.1: data curation, deduplication, decontamination, quality evaluation, exploration, generation, and augmentation.\n\nFigure 6.1 – Overview of the post-training data pipeline covered in this chapter\n\nTo avoid repetition, this section will focus on the main differences between instruction and preference datasets. We will introduce the structure of preference samples and the ideal size for preference datasets. Then, we will focus on the two stages that differ most from creating instruction datasets: data generation and evaluation.\n\nOceanofPDF.com\n\nPreference data\n\nPreference datasets lack the standardization of instruction datasets due to varying data requirements across different training algorithms. Preference data comprises a collection of responses to a given instruction, ranked by humans or language models. This chapter focuses on DPO, so we will examine the specific data format required by this algorithm.\n\nAs illustrated in Table 6.1, the structure of DPO datasets is straightforward: each instruction is paired with one preferred answer and one rejected answer. The objective is to train the model to generate the preferred response rather than the rejected one.\n\nInstruction Tell me a joke about octopuses.\n\nChosen answer Why don’t octopuses play cards in casinos? Because they can’t count past eight. Rejected answe\n\nTable 6.1 – Example of sample from the mlabonne/orpo-dpo-mix-40k dataset\n\nIn preference datasets, the rejected response is as important as the chosen one. Without the rejected response, the dataset would be a simple instruction set. Rejected responses represent the behavior we aim to eliminate from the model. This provides a lot of flexibility and allows us to use preference datasets in many contexts. Here is a list of examples where preference datasets are more beneficial to use compared to using SFT alone:\n\nChatbots: In conversational AI, the quality of responses often depends on subjective factors like naturalness, engagement, and contextual appropriateness. A preference dataset allows the model to learn these nuanced aspects by comparing better and worse responses. Simple SFT might not capture the subtleties of what makes one response preferable over another in a given context.\n\nContent moderation: Determining whether content is appropriate or violates guidelines often involves nuanced judgments. Preference datasets can help the model learn to distinguish between borderline cases by comparing examples of content that is and isn’t acceptable. This is more effective than binary classification through SFT, as it helps the model understand the reasoning behind moderation decisions.\n\nSummarization: The quality of a summary often depends on factors like conciseness, relevance, and coherence. By using preference datasets, models can learn to generate summaries that humans find more useful and informative. Simple SFT might result in summaries that are technically correct but less preferable to human readers.\n\nCode generation: In coding tasks, there are often multiple correct solutions, but some are more efficient or readable, or follow better practices than others. Preference datasets can help the model learn these qualitative aspects of code quality, which might not be captured by simple correctness-based SFT.\n\nCreative writing: For tasks like story generation or poetry writing, the quality of the output is highly subjective and multifaceted. Preference datasets can capture human judgments about style, creativity, and emotional impact better than instruction datasets, which might focus more on technical correctness or adherence to prompts.\n\nTranslation: While traditional metrics like BLEU scores can measure translation accuracy, they don’t always capture the fluency or naturalness of the translation. Preference datasets can help models learn to produce translations that native speakers prefer, even when multiple translations are technically correct.\n\nIn all these scenarios, preference datasets enable a more refined training approach. They capture subjective quality assessments and human preferences that extend beyond simple correctness or adherence to instructions. This method can produce models that generate output that is not only technically accurate but also better aligned with human judgment and preferences in complex, open-ended tasks.\n\nUnlike instruction datasets, there are no standardized storage formats like Alpaca or ShareGPT. Most preference datasets follow a structure similar to that shown in Table 6.1, with columns for an instruction, a preferred answer, and a rejected answer. Multi-turn conversations are uncommon in preference alignment. At the time of writing, major fine-tuning libraries do not support multi-turn conversations and typically extract only the first or last message in a conversation.\n\nOceanofPDF.com\n\nData quantity\n\nDPO datasets typically require fewer samples than instruction datasets to significantly impact model behavior. As with instruction datasets, the required sample count depends on model size and task complexity. Larger models are more sample-efficient and thus require less data, while complex tasks demand more examples to capture the desired behavior. Once again, data quality is crucial, and a large number of preference pairs is generally beneficial.\n\nGeneral-purpose alignment is used by LLM providers to improve the overall performance of the fine-tuned models. This requires preference datasets with millions of samples. Major players in the AI industry, including Nvidia and Meta, are converging on similar post-training pipelines, involving multiple rounds of preference alignment, and extensive use of synthetic data. This consensus suggests that these methods are proving to be the most effective for pushing the boundaries of language model capabilities.\n\nOn a smaller scale, the open-source community uses datasets ranging from 10,000 to 100,000 samples to enhance model performance. This approach has proven effective not only in improving benchmark scores but also in healing networks after merging, pruning, and other modifications. Generally, DPO is less destructive than SFT and has a milder impact on the final model.\n\nOn the other hand, tasks like the ones previously described require fewer preference pairs. Task-specific alignment focuses on improving model performance for a particular function, such as modifying the writing style, refusing certain instructions, and so on. These alignments can often be achieved with smaller datasets, ranging from 100 to 10,000 preference pairs, depending on the task’s complexity.\n\nAn example of an application that requires few samples is instructing the model to state that it wasn’t trained by OpenAI, Meta, or another LLM provider. This can be achieved using a preference dataset, where the rejected answers are those claiming alternative origins, and the chosen answers are responses where the model correctly states that it was trained by you. A relatively small dataset of 200 to 500 pairs can be enough for this task.\n\nOceanofPDF.com\n\nData generation and evaluation\n\nWhen creating preference datasets, data generation and evaluation are closely linked. We first create answers and then rate them to make the final dataset. In the following, we introduce both steps as one process instead of two separate ones.\n\nOceanofPDF.com\n\nGenerating preferences\n\nBefore making new preference data, it’s good to look at relevant open- source datasets. There are fewer of these compared to instruction datasets, but you can find high-quality preference datasets on the Hugging Face Hub. These can be used for specific tasks or to add to your own dataset. Well- known preference datasets include the Anthropic HH-RLHF dataset, which has human preferences for helpful and harmless AI responses, and the OpenAI Summarize from Human Feedback dataset, which focuses on article summaries.\n\nDPO datasets can be created using various methods, each with its own trade-offs between quality, cost, and scalability. These methods can be tailored to specific applications and require varying degrees of human feedback. We divide them into four main categories:\n\nHuman-generated, human-evaluated datasets: This method involves hiring people to both create responses to prompts and evaluate the quality of these responses. While this approach can capture nuanced human preferences and is ideal for complex tasks, it’s extremely resource-intensive and difficult to scale. As a result, it’s primarily used by large AI companies with substantial resources.\n\nHuman-generated, LLM-evaluated datasets: This method can be useful if you have a lot of existing human-generated content. However, it’s rarely used in practice due to inefficiency, as it still requires significant\n\nhuman input for response generation while potentially missing nuanced preferences during the LLM evaluation stage.\n\nLLM-generated, human-evaluated datasets: This method offers a good balance between quality and efficiency. LLMs generate multiple responses to prompts, and humans rank these responses. This approach is often preferred because humans are generally better at judging answers than writing them from scratch. It allows the rapid generation of diverse responses while still capturing human preferences effectively. However, it may not provide creative or unexpected responses that humans might generate.\n\nLLM-generated, LLM-evaluated datasets: Fully synthetic datasets, where both generation and evaluation are done by LLMs, are becoming increasingly common due to their scalability and cost-effectiveness. This method can produce massive datasets quickly and improves as LLM capabilities advance. However, it requires careful prompt engineering to ensure quality and diversity, and may perpetuate biases or limitations of the generating LLM.\n\nIn practice, human-generated datasets are expensive, difficult to scale, and not necessarily of the highest quality. On the other hand, human evaluation is quite valuable but can be difficult to scale, which is why large datasets benefit from LLM evaluation. In addition to these high-level considerations, the way you obtain your data and how you plan to use it also need to be considered. For example, applications with many users can embed a feedback mechanism to provide preferences. This can be as simple as a\n\nlike\n\nand\n\ndislike\n\nscore, or something more in-depth with text.\n\nNote that evaluation is not always required and preferences can emerge naturally from the generation process. For instance, it is possible to use a high-quality model to generate preferred outputs and a lower-quality or intentionally flawed model to produce less preferred alternatives. This creates a clear distinction in the preference dataset, allowing more effective training of AI systems to recognize and emulate high-quality outputs. The\n\nIntel/orca_dpo_pairs\n\ndataset available on the Hugging Face Hub was created with this process.\n\nAnother approach is to compare model-generated outputs with human- written responses, which can provide insights into how well the model aligns with actual human preferences and highlight areas where the model may be lacking. This can be used to copy a particular style and give a more authentic tone to the model.\n\nOceanofPDF.com\n\nTips for data generation\n\nThe data generation is consistent between instruction and preference datasets. Prompts should be designed to encourage diversity and complexity in the model’s responses. By crafting prompts that explicitly request different approaches or styles, we can ensure a wide range of outputs that capture the varied nature of human preferences.\n\nFor instance, when generating summaries, one might request variations such as concise summaries, detailed summaries, and summaries focusing on key points. This approach not only produces a diverse dataset but also helps in understanding how different styles and approaches align with human preferences.\n\nIntroducing variability in the outputs is another crucial aspect of generating synthetic preference datasets. This can be achieved by manipulating the temperature settings or employing other sampling methods in the LLM. Higher temperature settings tend to produce more creative and diverse responses, while lower settings result in more focused and deterministic outputs. This creates a trade-off between diversity and coherence, which depends on the kind of data we want to generate. For example, generating code requires low creativity, thus low temperature, while writing articles can be high temperature.\n\nUsing multiple LLMs to generate samples can be better than using just one model. Some LLMs are better at specific tasks, and this approach also adds more variety. This approach is used by popular open-source datasets like\n\nargilla/Capybara-Preferences\n\n, combining GPT-4 with open-weight models. The evaluation process then selects the chosen and the rejected answers.\n\nOceanofPDF.com\n\nEvaluating preferences\n\nData evaluation can be performed by human raters or automated with LLMs. LLM evaluation involves developing detailed criteria, creating a prompt that clearly communicates these guidelines to the LLM, and using the model to select preferred and rejected responses. While more scalable than human rating and allowing the consistent application of criteria, this quality of LLM evaluation depends directly on the model’s performance and the provided guidelines. It may miss subtle human preferences or cultural nuances. However, as LLMs continue to improve, their ability to make nuanced judgments improves as well, potentially leading to higher-quality datasets over time.\n\nImplementing LLM evaluation for preference datasets can be done through absolute scoring or pairwise ranking. In absolute scoring, the LLM assigns a numerical score or categorical rating to each response based on predefined criteria. This method is straightforward but may suffer from inconsistency across different prompts or evaluation sessions. Pairwise ranking, on the other hand, involves presenting the LLM with two responses and asking it to choose the better one or rank them. This approach more closely mimics the format of human evaluation and can lead to more consistent results.\n\nFor absolute scoring, you would create a prompt that outlines the evaluation criteria and asks the LLM to rate the response on a specific scale (e.g., 1-5 or poor/fair/good/excellent). The prompt might look like this: “Rate the following response on a scale of 1-5 based on relevance, coherence, and helpfulness: [\n\nINSERT RESPONSE\n\n].” For pairwise ranking, the prompt could be: “Compare the following two responses. Which one is better in terms of relevance, coherence, and helpfulness? Response A: [\n\nINSERT RESPONSE A\n\n] Response B: [\n\nINSERT RESPONSE B\n\n].”\n\nThe comparative nature of preference datasets makes pairwise ranking an ideal approach for evaluation. This method is generally more accurate and more closely correlated to human judgment than absolute scoring. Pairwise ranking mimics the natural way humans compare options, making it easier for both human raters and LLMs to provide consistent and meaningful evaluations.\n\nWe can further improve the accuracy of pairwise ranking by providing a ground-truth answer and using chain-of- thought reasoning. This approach encourages the evaluating LLM to consider multiple aspects of the responses and articulate its decision-making process, leading to more thorough and justified evaluations. When no ground- truth answer is available, we can prompt the LLM to create a grading note, which is a description of the expected answer. This technique works particularly well in scenarios where the LLM doesn’t have extensive knowledge about a given topic, as it forces the model to establish clear criteria for evaluation before assessing the responses.\n\nHere’s a concrete implementation of an LLM-as-a-judge prompt to perform pairwise ranking:\n\nInstruction You are an answer judge. Your goal is to compare answer A and answer B. I want to know which answe\n\nTable 6.2 – Example of LLM-as-a-judge prompt for pairwise ranking with one instruction and two answers\n\nHowever, it’s important to note that LLM-based evaluation can be subject to several types of bias:\n\nPosition bias: In relative scoring, LLM judges tend to favor the first answer presented. This bias can skew results and lead to inaccurate preferences.\n\nLength bias: Similar to humans, LLM judges often show a preference for longer answers, potentially overlooking the quality of shorter, more concise responses.\n\nFamily bias: LLM judges may favor responses that are generated by themselves or models from the same family, potentially due to similarities in language patterns or knowledge bases.\n\nTo mitigate these biases and enhance the quality of preference datasets, several solutions can be implemented. One key approach is to randomize the order of answer A and answer B in each comparison, which can counteract position bias by ensuring that the order of presentation doesn’t consistently influence the evaluation. Another valuable strategy involves providing few-shot examples that demonstrate a balanced distribution of scores. These examples serve to calibrate the judge LLM’s internal scoring mechanism and can effectively address both length and family bias by illustrating that shorter answers or those from different model families can also be of high quality. Additionally, employing multiple models as a jury, rather than relying on a single LLM judge, can significantly improve the robustness of the evaluation process. This multi-model approach helps to balance out individual biases that may be present in any single model, leading to a more comprehensive and accurate assessment of the responses.\n\nIn the next section, we will create our own preference dataset. We will rely on the data generation process to naturally create chosen (human-generated) and rejected (LLM-generated) answers.\n\nOceanofPDF.com\n\nCreating our own preference dataset\n\nOur model can currently write paragraphs about topics related to machine learning, but it doesn’t have the same writing style as the original authors. This is a typical use case for preference alignment, where we want to change the “voice” of the model to closely imitate the source data. It’s important to note that, experimentally, DPO tends to make models more verbose and pushes them to use very formal language. Therefore, the training will need to use DPO surgically to avoid this pitfall and instead adopt the less formal style of these blog articles.\n\nIn this section, we will create a preference dataset where the chosen answers are extracts from the text, while rejected answers are generated by the model. To implement it, we will modify the code created in Chapter 5, which was designed to generate instruction datasets.\n\nAs seen in the previous section, preference and instruction datasets rely on the same principles. Instead of pairs of instructions and answers, we need triples (instruction, answer 1, answer 2). What’s interesting in this setting is that we have ground-truth answers in the text chunks, which means we don’t need complex evaluation processes like LLM judges. To make sure that these extracts are high-quality, we will implement two additional quality filters, based on length and punctuation. Figure 6.2 summarizes the end-to- end process:\n\nFigure 6.2 – Synthetic data generation pipeline from raw text to preference dataset\n\nWe are now ready to implement the preference data generation pipeline:\n\nWe start by importing the necessary libraries.\n\nimport\n\nconcurrent.futures\n\nimport\n\njson\n\nimport\n\nre\n\nfrom\n\ntyping\n\nimport\n\nList\n\n,\n\nTuple\n\nfrom\n\ndatasets\n\nimport\n\nDataset\n\nfrom\n\nopenai\n\nimport\n\nOpenAI\n\nfrom\n\ntqdm.auto\n\nimport\n\ntqdm\n\nInstead of the\n\nInstructionAnswerSet\n\nclass, we now have a\n\nPreferenceSet\n\nclass. This class is designed to handle triples of instructions, generated answers (rejected), and extracted answers (chosen).\n\nclass\n\nPreferenceSet\n\n:\n\ndef\n\n__init__\n\n(\n\nself, triples:\n\nList\n\n[\n\nTuple\n\n[\n\nstr\n\n,\n\nstr\n\n,\n\nstr\n\n]]\n\n): self.triples = triples\n\n@classmethod\n\ndef\n\nfrom_json\n\n(\n\ncls, json_str:\n\nstr\n\n) ->\n\n'PreferenceSet'\n\n: data = json.loads(json_str) triples = [(triple[\n\n'instruction'\n\n], triple[\n\n'generated_answer'\n\n], triple[\n\n'extracted_answer'\n\n])\n\nfor\n\ntriple\n\nin\n\ndata[\n\n'preference_triples'\n\n]]\n\nreturn\n\ncls(triples)\n\ndef\n\n__iter__\n\n(\n\nself\n\n):\n\nreturn\n\niter\n\n(self.triples)\n\nThe\n\nload_articles_from_json\n\n,\n\nclean_text\n\n, and\n\nextract_substrings\n\nfunctions remain unchanged from the original code. Let’s start with\n\nload_articles_from_json\n\n, which takes our JSON file (\n\ncleaned_documents.json\n\n) containing the articles as input and returns a Hugging Face dataset with the text and metadata (ID, platform, author ID, author full name, link).\n\ndef\n\nload_articles_from_json\n\n(\n\nfile_path:\n\nstr\n\n) -> Dataset:\n\nwith\n\nopen\n\n(file_path,\n\n\"r\"\n\n)\n\nas\n\nfile: data = json.load(file)\n\nreturn\n\nDataset.from_dict( {\n\n\"id\"\n\n: [item[\n\n\"id\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"content\"\n\n: [item[\n\n\"content\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"platform\"\n\n: [item[\n\n\"platform\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"author_id\"\n\n: [item[\n\n\"author_id\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"author_full_name\"\n\n: [item[\n\n\"author_full_name\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"link\"\n\n: [item[\n\n\"link\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]], } )\n\nThe\n\nclean_text\n\nfunction removes non-alphanumeric characters except for apostrophes, periods, commas, exclamation marks, and question marks. It also replaces multiple whitespaces with a single space to ensure proper formatting.\n\ndef\n\nclean_text\n\n(\n\ntext:\n\nstr\n\n) ->\n\nstr\n\n: text = re.sub(\n\nr\"[^\\w\\s.,!?']\"\n\n,\n\n\" \"\n\n, text) text = re.sub(\n\nr\"\\s+\"\n\n,\n\n\" \"\n\n, text)\n\nreturn\n\ntext.strip()\n\nThe\n\nextract_substrings\n\nfunction splits articles into chunks with a length between 1,000 and 2,000 characters. To make sure that the splitting doesn’t break sentences, which could modify their meanings, we use a regex to only split after the end of a sentence.\n\ndef\n\nextract_substrings\n\n(\n\ndataset: Dataset, min_length:\n\nint\n\n=\n\n1000\n\n, max_length:\n\nint\n\n=\n\n2000\n\n) ->\n\nList\n\n[\n\nstr\n\n]: extracts = [] sentence_pattern =\n\nr\"(?\n\nfor\n\narticle\n\nin\n\ndataset[\n\n\"content\"\n\n]: cleaned_article = clean_text(article) sentences = re.split(sentence_pattern, cleaned_article) current_chunk =\n\n\"\"\n\nfor\n\nsentence\n\nin\n\nsentences: sentence = sentence.strip()\n\nif\n\nnot\n\nsentence:\n\ncontinue\n\nif\n\nlen\n\n(current_chunk) +\n\nlen\n\n(sentence) <= max_length: current_chunk += sentence +\n\n\" \"\n\nelse\n\n:\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip()) current_chunk = sentence +\n\n\" \"\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip())\n\nreturn\n\nextracts\n\nThe\n\ngenerate_preference_triples\n\nfunction replaces the original\n\ngenerate_instruction_answer_pairs\n\nfunction. The prompt is adapted from the instruction version and is designed to generate triples instead of pairs. It also provides general guidance about the type of instructions we’re interested in, how to extract answers from articles, and how to style them:\n\ndef\n\ngenerate_preference_triples\n\n(\n\nextract:\n\nstr\n\n, client: OpenAI\n\n) ->\n\nList\n\n[\n\nTuple\n\n[\n\nstr\n\n,\n\nstr\n\n,\n\nstr\n\n]]: prompt =\n\nf\"\"\"Based on the following extract, generate five instruction-answer triples. Each triple should consist of:\n\n1. An instruction asking about a specific topic in the context.\n\n2. A generated answer that attempts to answer the instruction based on the context.\n\n3. An extracted answer that is a relevant excerpt directly from the given context.\n\nInstructions must be self-contained and general, without explicitly mentioning a context, system, course, or extract.\n\nImportant:\n\nEnsure that the extracted answer is a verbatim copy from the context, including all punctuation and apostrophes.\n\nDo not add any ellipsis (...) or [...] to indicate skipped text in the extracted answer.\n\nIf the relevant text is not continuous, use two separate sentences from the context instead of skipping text.\n\nProvide your response in JSON format with the following structure:\n\n{{\n\n\"preference_triples\": [\n\n{{\n\n\"instruction\": \"...\",\n\n\"generated_answer\": \"...\",\n\n\"extracted_answer\": \"...\"\n\n}},\n\n...\n\n]\n\n}}\n\nExtract:\n\n{extract}\n\n\"\"\"\n\nIn the same function, we use GPT-4o-mini to generate our answers using JSON mode. We specify in the system prompt that we want triples instead of pairs. The JSON answers are directly parsed by our\n\nPreferenceSet\n\nclass to return the expected list of tuples.\n\ncompletion = client.chat.completions.create( model=\n\n\"gpt-4o-mini\"\n\n, messages=[ {\n\n\"role\"\n\n:\n\n\"system\"\n\n,\n\n\"content\"\n\n:\n\n\"You are a helpful assistant who generates instruction-answer triples based on the given context. Each triple should include an instruction, a generated answer, and an extracted answer from the context. Provide your response in JSON format.\"\n\n, }, {\n\n\"role\"\n\n:\n\n\"user\"\n\n,\n\n\"content\"\n\n: prompt}, ], response_format={\n\n\"type\"\n\n:\n\n\"json_object\"\n\n}, max_tokens=\n\n2000\n\n, temperature=\n\n0.7\n\n, ) result = PreferenceSet.from_json(completion.choices[\n\n0\n\n].message.content)\n\nreturn\n\nresult.triples\n\nTwo new filtering functions are introduced for the preference data pipeline:\n\nfilter_short_answers\n\nand\n\nfilter_answer_format\n\n. These functions filter out short answers and ensure that answers start with an uppercase letter and end with proper punctuation. We use them as heuristics to filter out samples with poor quality.\n\ndef\n\nfilter_short_answers\n\n(\n\ndataset: Dataset, min_length:\n\nint\n\n=\n\n100\n\n) -> Dataset:\n\ndef\n\nis_long_enough\n\n(\n\nexample\n\n):\n\nreturn\n\nlen\n\n(example[\n\n'chosen'\n\n]) >= min_length\n\nreturn\n\ndataset.\n\nfilter\n\n(is_long_enough)\n\ndef\n\nfilter_answer_format\n\n(\n\ndataset: Dataset\n\n) -> Dataset:\n\ndef\n\nis_valid_format\n\n(\n\nexample\n\n): chosen = example[\n\n'chosen'\n\n]\n\nreturn\n\n(\n\nlen\n\n(chosen) >\n\n0\n\nand\n\nchosen[\n\n0\n\n].isupper()\n\nand\n\nchosen[-\n\n1\n\n]\n\nin\n\n(\n\n'.'\n\n,\n\n'!'\n\n,\n\n'?'\n\n))\n\nreturn\n\ndataset.\n\nfilter\n\n(is_valid_format)\n\nThe\n\ncreate_preference_dataset\n\nfunction replaces the original\n\ncreate_instruction_dataset\n\nfunction. This function now works with triples instead of pairs and uses different column names in the resulting dataset.\n\ndef\n\ncreate_preference_dataset\n\n(\n\ndataset: Dataset, client: OpenAI, num_workers:\n\nint\n\n=\n\n4\n\n) -> Dataset: extracts = extract_substrings(dataset) preference_triples = []\n\nwith\n\nconcurrent.futures.ThreadPoolExecutor(max_workers=num_workers)\n\nas\n\nexecutor: futures = [ executor.submit(generate_preference_triples, extract, client)\n\nfor\n\nextract\n\nin\n\nextracts ]\n\nfor\n\nfuture\n\nin\n\ntqdm(concurrent.futures.as_completed(futures), total=\n\nlen\n\n(futures)): preference_triples.extend(future.result()) instructions, generated_answers, extracted_answers =\n\nzip\n\n(*preference_triples)\n\nreturn\n\nDataset.from_dict( {\n\n\"prompt\"\n\n:\n\nlist\n\n(instructions),\n\n\"rejected\"\n\n:\n\nlist\n\n(generated_answers),\n\n\"chosen\"\n\n:\n\nlist\n\n(extracted_answers) } )\n\nThe main function is updated to include the new filtering steps and to use the preference dataset creation function:\n\ndef\n\nmain\n\n(\n\ndataset_id:\n\nstr\n\n) -> Dataset: client = OpenAI()\n\n# 1. Load the raw data\n\nraw_dataset = load_articles_from_json(\n\n\"cleaned_documents.json\"\n\n)\n\nprint\n\n(\n\n\"Raw dataset:\"\n\n)\n\nprint\n\n(raw_dataset.to_pandas())\n\n# 2. Create preference dataset\n\ndataset = create_preference_dataset(raw_dataset, client)\n\nprint\n\n(\n\n\"Preference dataset:\"\n\n)\n\nprint\n\n(dataset.to_pandas())\n\n# 3. Filter out samples with short answers\n\ndataset = filter_short_answers(dataset)\n\n# 4. Filter answers based on format\n\ndataset = filter_answer_format(dataset)\n\n# 5. Export\n\ndataset.push_to_hub(dataset_id)\n\nreturn\n\ndataset\n\nThe\n\ncreate_preference_dataset()\n\nfunction generated 2,970 samples. This dataset is then heavily filtered to only retain 1,467 samples by removing answers that are too short or not properly formatted (for example, answers that start with an uppercase letter or end with a period, exclamation mark, or question mark).\n\nThe final dataset is available on the Hugging Face Hub at the following address: https://huggingface.co/datasets/mlabonne/llmtwin-dpo. You can see in Figure 6.3 an example that captures a subtle nuance in terms of\n\nwriting style. Both answers are correct, but the chosen (extracted) answer sounds slightly more casual.\n\nFigure 6.3 – Screenshot of the mlabonne/llmtwin-dpo preference dataset on the Hugging Face Hub\n\nTo produce this dataset, we iterated many times over the prompt to generate the data. This required some manual evaluation and experiments until we reached satisfying results. The quality of the prompt is fundamental in this process, which is why it is recommended to follow a similar process to generate your own preference datasets.\n\nIn the next section, we will introduce concepts related to Reinforcement Learning from Human Feedback (RLHF) and DPO. This will cover new parameters and ideas that are implemented in the final section of this chapter.\n\nOceanofPDF.com\n\nPreference alignment\n\nPreference alignment regroups techniques to fine-tune models on preference data. In this section, we provide an overview of this field and then focus on the technique we will implement: Direct Preference Optimization (DPO).\n\nOceanofPDF.com\n\nReinforcement Learning from Human Feedback\n\nReinforcement Learning from Human Feedback (RLHF) combines reinforcement learning (RL) with human input to align models with human preferences and values. RLHF emerged as a response to challenges in traditional RL methods, particularly the difficulty of specifying reward functions for complex tasks and the potential for misalignment between engineered rewards and intended objectives.\n\nThe origins of RLHF can be traced back to the field of preference- based reinforcement learning (PbRL), which was independently introduced by Akrour et al. and Cheng et al. in 2011. PbRL aimed to infer objectives from qualitative feedback, such as pairwise preferences between behaviors, rather than relying on quantitative reward signals. This approach addressed some of the limitations of conventional RL, where defining appropriate reward functions can be challenging and prone to reward hacking or unintended behaviors.\n\nThe term RLHF was coined later, around 2021-2022, as the approach gained prominence in the context of training LLMs. However, the core ideas had been developing for years prior. A seminal paper by Christiano et al. in 2017 demonstrated the effectiveness of learning reward models from human preferences and using them to train RL agents. This work showed that RLHF could match or exceed the performance of agents trained on hand-engineered rewards, but with significantly less human effort.\n\nAt its core, RLHF works by iteratively improving both a reward model and a policy:\n\nReward model learning: Instead of using a pre-defined reward function, RLHF learns a reward model from human feedback. This is typically done by presenting humans with different answers and asking them to indicate which one they prefer. These preferences are used to train a reward model, often using a Bradley-Terry model or similar approaches that map preferences to underlying utility functions.\n\nPolicy optimization: With the learned reward model, standard RL algorithms can be used to optimize a policy. This policy generates new behaviors that aim to maximize the predicted rewards from the learned model.\n\nIterative improvement: As the policy improves, it generates new behaviors that can be evaluated by humans, leading to refinements in the reward model. This cycle continues, ideally resulting in a policy that aligns well with human preferences.\n\nA key innovation in RLHF is its approach to handling the high cost of human feedback. Rather than requiring constant human oversight, RLHF allows for asynchronous and sparse feedback.\n\nThe learned reward model serves as a proxy for human preferences, enabling the RL algorithm to train continuously without direct human input for every action.\n\nAs an example, Figure 6.4 shows a high-level view of the Proximal Policy Optimization (PPO) algorithm, which is one of the most popular RLHF algorithms. Here, the reward model is used to score the text that is generated by the trained model. This reward is regularized by an additional Kullback–Leibler (KL) divergence factor, ensuring that the distribution of tokens stays similar to the model before training (frozen model).\n\nFigure 6.4 – High-level view of the PPO algorithm for preference alignment\n\nWhile RLHF has proven effective for aligning AI systems with human preferences, it faces challenges due to its iterative nature and reliance on a\n\nseparate reward model, which can be computationally expensive and potentially unstable. Despite theoretical superiority, RLHF algorithms have also experimentally underperformed compared to simpler approaches. One such approach that has gained significant attention is DPO.\n\nOceanofPDF.com\n\nDirect Preference Optimization\n\nIntroduced by Rafailov et al. in their 2023 paper Direct Preference Optimization: Your Language Model is Secretly a Reward Model, DPO offers a streamlined alternative to traditional RLHF methods.\n\nDPO’s core innovation lies in its reformulation of the preference learning problem. Unlike RLHF, which typically involves training a separate reward model and then using reinforcement learning algorithms like PPO to fine- tune the language model, DPO takes a more direct approach.\n\nIt derives a closed-form expression for the optimal policy under the standard RLHF objective of maximizing expected reward subject to a KL- divergence constraint with a reference policy. This mathematical insight allows DPO to express the preference learning problem directly in terms of the policy, eliminating the need for a separate reward model or complex reinforcement learning algorithms.\n\nIn practical terms, DPO can be implemented as a simple binary cross- entropy loss function that operates directly on the language model’s output probabilities. This loss function encourages the model to assign higher probability to preferred responses and lower probability to non-preferred responses, while maintaining closeness to a reference (frozen) model. The importance of the reference model is directly controlled via a beta parameter between 0 and 1. The reference model is ignored when beta is equal to 0, which means that the trained model can be very different from\n\nthe SFT one. In practice, a value of 0.1 is the most popular one, but this can be tweaked, as we’ll see in the next section.\n\nThe simplicity of this approach allows optimization using standard gradient descent techniques, without the need for sampling from the model during training or implementing complex RL algorithms. Figure 6.5 shows a high- level view of the DPO algorithm, greatly simplifying the training process compared to Figure 6.4.\n\nFigure 6.5 – High-level view of the DPO algorithm for preference alignment\n\nDPO has several advantages over traditional RLHF methods. As previously mentioned, it significantly simplifies the preference learning pipeline, reducing the engineering complexity associated with RLHF methods. By eliminating the need for a separate reward model and RL algorithms, DPO is more computationally efficient than traditional RLHF approaches. Particularly when trained with adapters (LoRA, QLoRA), the frozen and trained models don’t have to be separated. Indeed, since we’re only training adapters, the trained model is not modified. This allows us to only load one model instead of two, which saves additional VRAM.\n\nDespite its simplicity, DPO often matches the performance of more complex RLHF methods. It also tends to be more stable during training and less sensitive to hyperparameters. The simplified approach makes DPO easier to implement and scale, particularly for small teams without extensive RL knowledge.\n\nWhile RLHF allows iterative improvement through multiple training rounds and can dynamically adapt to new preferences, DPO offers a more straightforward path to achieving similar results. The choice between DPO and PPO-based RLHF often comes down to a trade-off between ease of implementation and potential peak performance. For large-scale training runs with millions of preference samples, PPO-inspired methods still have a higher performance ceiling. However, for most applications, DPO provides the majority of the performance benefits at a lower computational and engineering cost.\n\nBoth RLHF and DPO benefit significantly from the integration of synthetic data. As LLMs become more capable, they can generate data that surpasses human-created content in quality and diversity. This enables a virtuous cycle where better models produce better training data, which in turn leads to further model improvements. The iterative nature of both approaches allows multiple rounds of model refinement, each focusing on different aspects of model performance and gradually enhancing capabilities across various domains.\n\nDespite its advantages, DPO is not without drawbacks. Like RLHF, DPO still requires paired preference data, which can be expensive and time- consuming to collect. DPO lacks some of the theoretical guarantees associated with reinforcement learning approaches. There may be scenarios where the added flexibility of RLHF is beneficial, particularly for complex tasks or environments.\n\nNonetheless, DPO is ideal in most cases, including our twin LLM example. In the next section, we will implement it using Unsloth.\n\nOceanofPDF.com\n\nImplementing DPO\n\nIn this section, we will DPO fine-tune the TwinLlama-3.1-8B model we created in Chapter 5. For ease of use and to maximize performance, we will again use the Unsloth library for our DPO implementation. Depending on the available VRAM, you can choose between LoRA (higher quality, speed, and VRAM usage) and QLoRA (lower quality, speed, and VRAM usage). This technique, along with other preference alignment algorithms, is also available in TRL and Axolotl.\n\nThis example can be seen as an advanced application of DPO. Indeed, our objective of imitating a writing style conflicts with the natural tendency of DPO to encourage formal language. This is partly due to the fact that chosen answers are often more formal than rejected ones. In practice, this will force us to do light fine-tuning, with a low learning rate and number of epochs. To find the best hyperparameters, we trained over 20 models and compared their outputs on a set of questions, including “Write a paragraph to introduce supervised fine-tuning.” This allowed us to select the model and parameters that worked best for this task.\n\nThe dependencies are the same as those in Chapter 5 with SFT and can be found in the book’s GitHub repository (https://github.com/PacktPublishing/LLM-Engineering) or in Unsloth’s repo (https://github.com/unslothai/unsloth):\n\nFirst, we want to access a gated model and (optionally) upload our fine- tuned model to Hugging Face (https://huggingface.co/). This requires us to log in to an account. If you don’t have an account, you can create one and store your API key (Settings | Access Tokens | Create new token) in the\n\n.env\n\nfile:\n\nHF_TOKEN = YOUR_API_KEY\n\nMake sure that your Comet ML API key is also in the\n\n.env\n\nfile. Otherwise, the code will crash and raise an error when training starts.\n\nCOMET_API_KEY = YOUR_API_KEY\n\nBefore we import all the necessary packages, we want to apply a patch for the\n\nDPOTrainer\n\nclass from TRL. This fixes the DPO logs in notebook environments.\n\nfrom\n\nunsloth\n\nimport\n\nPatchDPOTrainer PatchDPOTrainer()\n\nWe can now import the other libraries. The main difference between DPO and SFT is the import of\n\nDPOConfig\n\nand\n\nDPOTrainer\n\nfrom TRL, which are specific to DPO training.\n\nimport\n\nos\n\nimport\n\ntorch\n\nfrom\n\ndatasets\n\nimport\n\nload_dataset\n\nfrom\n\ntransformers\n\nimport\n\nTrainingArguments, TextStreamer\n\nfrom\n\nunsloth\n\nimport\n\nFastLanguageModel, is_bfloat16_supportedfrom trl\n\nimport\n\nDPOConfig, DPOTrainer\n\nThis step loads our fine-tuned model from Chapter 5. We use the same configuration with a\n\nmax_seq_length\n\nof 2048. You can activate QLoRA by setting\n\nload_in_4bit\n\nto\n\nTrue\n\n. In the following, we will perform LoRA DPO fine-tuning for increased speed and quality.\n\nmax_seq_length =\n\n2048\n\nmodel, tokenizer = FastLanguageModel.from_pretrained( model_name=\n\n\"mlabonne/TwinLlama-3.1-8B\"\n\n, max_seq_length=max_seq_length, load_in_4bit=\n\nFalse\n\n, )\n\nLet’s now prepare the model for PEFT with the LoRA configuration. We increase the rank (\n\nr\n\n) and\n\nlora_alpha\n\nfrom\n\n32\n\n(as it was in Chapter 5) to\n\n64\n\n. This will allow more expressive fine-tuning. We keep a dropout of\n\n0\n\nfor speed and we target every linear module as per usual.\n\nmodel = FastLanguageModel.get_peft_model( model, r=32, lora_alpha=32, lora_dropout=0, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], )\n\nWe load the\n\nllmtwin-dpo\n\ndataset (training split), which contains our prompts, chosen, and rejected answers.\n\ndataset = load_dataset(\n\n\"mlabonne/llmtwin-dpo\"\n\n, split=\n\n\"train\"\n\n)\n\nThe data preparation is significantly different from the SFT example in Chapter 5. Here, we have triples with a prompt, a chosen answer, and a rejected answer. In the\n\nformat_samples\n\nfunction, we apply the Alpaca chat template to each individual message. Note that the instruction is the only one that requires the chat format: chosen and rejected answers only need to be concatenated with the end of sentence (EOS) token. Finally, we create a train/test split with a 95%/5% ratio.\n\nalpaca_template =\n\n\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n\n{}\n\n### Response:\n\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef\n\nformat_samples\n\n(\n\nexample\n\n): example[\n\n\"prompt\"\n\n] = alpaca_template.\n\nformat\n\n(example[\n\n\"prompt\"\n\n]) example[\n\n\"chosen\"\n\n] = example[\n\n'chosen'\n\n] + EOS_TOKEN example[\n\n\"rejected\"\n\n] = example[\n\n'rejected'\n\n] + EOS_TOKEN\n\nreturn\n\n{\n\n\"prompt\"\n\n: example[\n\n\"prompt\"\n\n],\n\n\"chosen\"\n\n: example[\n\n\"chosen\"\n\n],\n\n\"rejected\"\n\n: example[\n\n\"rejected\"\n\n]} dataset = dataset.\n\nmap\n\n(format_samples) dataset = dataset.train_test_split(test_size=\n\n0.05\n\n)\n\nThe model and data are now ready, so we can start fine-tuning. Compared to SFT, there are a few new parameters, like\n\nref_model\n\nand\n\nbeta\n\n. Since we’re using LoRA (or QLoRA), we don’t directly train the model but instead the adapters. This means we can use the original model (without adapters) as a reference, saving a lot of VRAM. The\n\nbeta\n\nparameter controls the importance of the reference model. A standard value of 0.1 works well in most scenarios, but we decided to increase it to 0.5 based on our experiments. This is due to the fact that the trained model used formal language with lower values. Having it closer to the reference model helps to fix this issue.\n\nThe learning rate is also lower (from 3e-4 for SFT to 2e-6 here). We train for 1 epoch instead of 3, and the\n\nmax_seq_length\n\nparameter is now broken down into two new parameters:\n\nmax_prompt_length\n\n(prompt only) and\n\nmax_length\n\n(prompt and answer). Note that we also replaced the\n\nTrainingArguments\n\nclass with\n\nDPOConfig\n\n.\n\ntrainer = DPOTrainer( model=model, ref_model=\n\nNone\n\n, tokenizer=tokenizer, beta=\n\n0.5\n\n, train_dataset=dataset[\n\n\"train\"\n\n], eval_dataset=dataset[\n\n\"test\"\n\n], max_length=max_seq_length//\n\n2\n\n, max_prompt_length=max_seq_length//\n\n2\n\n, args=DPOConfig( learning_rate=\n\n2e-6\n\n, lr_scheduler_type=\n\n\"linear\"\n\n, per_device_train_batch_size=\n\n2\n\n, per_device_eval_batch_size=\n\n2\n\n, gradient_accumulation_steps=\n\n8\n\n, num_train_epochs=\n\n1\n\n, fp16=\n\nnot\n\nis_bfloat16_supported(), bf16=is_bfloat16_supported(), optim=\n\n\"adamw_8bit\"\n\n, weight_decay=\n\n0.01\n\n, warmup_steps=\n\n10\n\n, output_dir=\n\n\"output\"\n\n, eval_strategy=\n\n\"steps\"\n\n, eval_steps=\n\n0.2\n\n, logging_steps=\n\n1\n\n, report_to=\n\n\"\n\ncomet_ml\"\n\n, seed=\n\n0\n\n, ), ) trainer.train()\n\nOnce the model is trained, we can run it for a quick sanity check. This step is similar to the SFT example. It prepares the model for inference and generates a response to a prompt.\n\nFastLanguageModel.for_inference(model) message = alpaca_template.\n\nformat\n\n(\n\n\"Write a paragraph to introduce supervised fine-tuning.\"\n\n,\n\n\"\"\n\n) inputs = tokenizer([message], return_tensors=\n\n\"pt\"\n\n).to(\n\n\"cuda\"\n\n) text_streamer = TextStreamer(tokenizer) _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=\n\n256\n\n, use_cache=\n\nTrue\n\n)\n\nThe trained DPO model returns the following response:\n\nSupervised fine-tuning is a method used to enhance the performance of pre- trained language models by utilizing labeled data. This technique involves taking a pre-trained model and refining it on a specific task, such as content creation or customer service. By providing the model with relevant data and guidance, it can learn to generate outputs that align more closely with the desired outcomes. This approach allows for the creation of more specialized models that can tackle complex tasks with greater accuracy and efficiency.\n\nWe can compare it with the answer provided by the SFT model:\n\nSupervised fine-tuning is a method used to enhance a language model by providing it with a curated dataset of instructions and their corresponding answers. This process is designed to align the model's responses with human expectations, thereby improving its accuracy and relevance. The goal is to ensure that the model can respond effectively to a wide range of queries, making it a valuable tool for applications such as chatbots and virtual assistants.\n\nThe DPO model provides an answer that is both more accurate and closer to the desired writing style. It correctly identifies pre-training language models as source models for SFT. It also mentions domain or task-specific finetunes instead of alignment with “human expectations,” which is closer to the preference alignment stage. The answer is also less formal and something we would use in a blog post.\n\nFinally, the last step consists of saving the trained model locally and pushing it to the Hugging Face Hub.\n\nmodel.save_pretrained_merged(\n\n\"model\"\n\n, tokenizer, save_method=\n\n\"merged_16bit\"\n\n)\n\nCongratulations! We have trained and exported our DPO model. It is now available on the Hugging Face Hub at https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO. Compared to SFT, DPO has a few additional metrics that need to be tracked during training. Figure 6.6 shows the Comet ML dashboard with the main metrics.\n\nYou can publicly access it using the following URL: https://www.comet.com/mlabonne/llm-twin-training/\n\nFigure 6.6 – Experiment tracking in Comet ML with DPO metrics\n\nLet’s review these metrics:\n\nTraining loss: We still want the loss to continuously decrease on average. Note that it can rapidly fall to zero, meaning that the model is no longer learning anything. This behavior doesn’t necessarily lead to overfitting or bad models but needs to be monitored closely.\n\nValidation loss: The same thing can be said about the validation loss. We expect a small gap compared to the training loss.\n\nGradient norm: We expect small gradient norms with few spikes.\n\nRewards: We have two different rewards: chosen and rejected. They correspond to the mean difference between the log probabilities output by the trained and reference models. Over time, we expect the model to choose the chosen answers and reject the rejected answers, which means that the gap between them should increase. This difference is directly tracked by the\n\nmargins\n\nmetric, defined as the difference between chosen and rejected rewards. A well-trained model’s margin will quickly increase and then plateau.\n\nAccuracies: This metric represents the percentage of times the model correctly identifies the chosen answers. We want this accuracy to gradually increase during training, but it doesn’t need to reach 100%. An accuracy of 100%, especially if it’s achieved quickly, indicates that the preference dataset might be too easy for the model. While the LLM can still learn from such a dataset, it might be beneficial to add more challenging examples.\n\nIn general, DPO is slightly harder to monitor and debug than SFT because it’s a more complex process, involving a reference model. However, it’s also significantly easier to use than PPO and other RLHF algorithms. As long as you have a high-quality preference dataset and a strong fine-tuned model, you can experiment with different ranks, beta parameters, learning\n\nrates, and number of epochs to see which experiment best captures your preferences.\n\nWhile this is not the purpose of this chapter, it is possible to automate the evaluation of models designed to imitate a writing style. A possible solution consists of comparing the distribution of words in the text generated by different models (SFT and DPO) with our ground-truth dataset. In this example, we expect the SFT model to output a lot of words that are overrepresented in GPT-4o-mini (like “delve into”). The distribution output by our DPO model should be a lot closer to the chosen answers.\n\nOceanofPDF.com\n\nSummary\n\nThis chapter explored preference alignment techniques for improving LLMs. It introduced the concept of preference datasets, explaining their structure and importance in capturing nuanced human preferences. We implemented our own custom preference data generation pipeline by comparing original and AI-generated text from real articles. This pipeline can be reused and customized based on your use case.\n\nWe also provided an overview of the evolution of RLHF, leading to the introduction of DPO as a simpler and more efficient alternative. Finally, we implemented DPO using the Unsloth library to fine-tune our TwinLlama- 3.1-8B model from Chapter 5. Our step-by-step tutorial gave practical instructions for training the model, as well as highlighting key differences from SFT. The final model is available on the Hugging Face Hub.\n\nIn the next chapter, we will explore the crucial topic of LLM evaluation, addressing the challenges and current approaches in assessing LLM performance. We’ll cover the creation of domain-specific evaluation sets, examine why evaluation remains a persistent problem in the field, and introduce the concept of using larger models to evaluate smaller ones (LLM-as-a-judge). The chapter will conclude with a comprehensive evaluation pipeline, providing a structured framework for consistent and effective LLM evaluation.\n\nOceanofPDF.com\n\nReferences\n\nRafael Rafailov et al.. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” arXiv preprint arXiv:2305.18290, May 2023.\n\nTimo Kaufmann et al.. “A Survey of Reinforcement Learning from Human Feedback.” arXiv preprint arXiv:2312.14925, December 2023.\n\nAnthropic. “GitHub - anthropics/hh-rlhf: Human preference data for “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback”.” github.com, 2022, https://github.com/anthropics/hh-rlhf.\n\nNisan Stiennon et al.. “Learning to summarize from human feedback.” arXiv preprint arXiv:2009.01325, September 2020.\n\nIntel(R) Neural Compressor. “Supervised Fine-Tuning and Direct Preference Optimization on Intel Gaudi2.” medium.com, March 26, 2024, https://medium.com/intel-analytics-software/the-practice-of-supervised- finetuning-and-direct-preference-optimization-on-habana-gaudi2- a1197d8a3cd3.\n\nArgilla. “GitHub - argilla-io/distilabel.” github.com, August 23, 2024, https://github.com/argilla-io/distilabel.\n\nDatabricks. “Enhancing LLM-as-a-Judge with Grading Notes.” databricks.com, July 22, 2024, https://www.databricks.com/blog/enhancing- llm-as-a-judge-with-grading-notes.\n\nAkrour, Riad & Schoenauer, Marc & Sebag, Michèle. (2011). Preference- Based Policy Learning. 12-27. 10.1007/978-3-642-23780-5_11.\n\nCheng, Weiwei & Fürnkranz, Johannes & Hüllermeier, Eyke & Park, Sang- Hyeun. (2011). Preference-Based Policy Iteration: Leveraging Preference Learning for Reinforcement Learning. 312-327. 10.1007/978-3-642-23780- 5_30.\n\nPaul Christiano et al.. “Deep reinforcement learning from human preferences.” arXiv preprint arXiv:1706.03741, June 2017.\n\nLong Ouyang et al.. “Training language models to follow instructions with human feedback.” arXiv preprint arXiv:2203.02155, March 2022.\n\nJohn Schulman et al.. “Proximal Policy Optimization Algorithms.” arXiv preprint arXiv:1707.06347, July 2017.\n\nunslothai. “GitHub - unslothai/unsloth: Finetune Llama 3.1, Mistral, Phi & Gemma LLMs 2-5x faster with 80% less memory.” github.com, August 21, 2024, https://github.com/unslothai/unsloth.\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\n7\n\nOceanofPDF.com\n\nEvaluating LLMs\n\nLLM evaluation is a crucial process used to assess the performance and capabilities of LLM models. It can take multiple forms, such as multiple- choice question answering, open-ended instructions, and feedback from real users. Currently, there is no unified approach to measuring a model’s performance but there are patterns and recipes that we can adapt to specific use cases.\n\nWhile general-purpose evaluations are the most popular ones, with benchmarks like Massive Multi-Task Language Understanding (MMLU) or LMSYS Chatbot Arena, domain- and task-specific models benefit from more narrow approaches. This is particularly true when dealing with entire LLM systems (as opposed to models), often centered around a retrieval-augmented generation (RAG) pipeline. In these scenarios, we need to expand our evaluation framework to encompass the entire system, including new modules like retrievers and post- processors.\n\nIn this chapter, we will cover the following topics:\n\nModel evaluation\n\nRAG evaluation\n\nEvaluating TwinLlama-3.1-8B\n\nBy the end of this chapter, you will know the most popular LLM evaluations and how to evaluate models and RAG systems using different techniques.\n\nOceanofPDF.com\n\nModel evaluation\n\nIn model evaluation, the objective is to assess the capabilities of a single model without any prompt engineering, RAG pipeline, and so on.\n\nThis evaluation is essential for several reasons, such as selecting the most relevant LLM or making sure that the fine-tuning process actually improved the model. In this section, we will compare ML and LLM evaluation to understand the main differences between these two fields. We will then explore benchmarks for general-purpose, domain-specific, and task-specific models.\n\nOceanofPDF.com\n\nComparing ML and LLM evaluation\n\nML evaluation is centered on assessing the performance of models designed for tasks like prediction, classification, and regression. Unlike the evaluation of LLMs, which often focuses on how well a model understands and generates language, ML evaluation is more concerned with how accurately and efficiently a model can process structured data to produce specific outcomes.\n\nThis difference comes from the nature of the tasks these models handle. ML models are generally designed for narrowly defined problems, such as predicting stock prices or detecting outliers, which often involve numerical or categorical data, making the evaluation process more straightforward. On the other hand, LLMs are tasked with interpreting and generating language, which adds a layer of subjectivity to the evaluation process. Instead of relying solely on numerical benchmarks, LLM evaluation requires a more nuanced approach and often incorporates qualitative assessments, examining how well the model produces coherent, relevant, and contextually accurate responses in natural language.\n\nIn particular, we can see three key differences in how these models work, which impact the evaluation process:\n\nNumerical metrics: Evaluating ML models typically involves measuring objective performance metrics, such as accuracy, precision, recall, or mean squared error, depending on the type of task at hand. This is less clear with LLMs, which can handle multiple tasks (hence,\n\nmultiple evaluations) and can rarely rely on the same numerical metrics.\n\nFeature engineering: In traditional ML, a critical part of the process involves manually selecting and transforming relevant data features before training the model. Evaluating the success of this feature engineering often becomes part of the broader model evaluation. LLMs, however, are designed to handle raw text data directly, reducing the need for manual feature engineering.\n\nInterpretability: With ML models, it is easier to interpret why a model made certain predictions or classifications, and this interpretability can be a core part of their evaluation. This direct interpretation is not possible with LLMs. However, requesting explanations during the generation process can give insights into the model’s decision-making process.\n\nIn the following section, we will see a more fine-grained exploration of different types of LLMs. While evaluating general-purpose models is fairly disconnected from ML evaluation, task-specific LLMs are more closely aligned with traditional ML.\n\nOceanofPDF.com\n\nGeneral-purpose LLM evaluations\n\nGeneral-purpose evaluations refer to metrics dedicated to base and general- purpose fine-tuned models. They cover a breadth of capabilities that are correlated with knowledge and usefulness without focusing on specific tasks or domains. This allows developers to get an overview of these capabilities, compare themselves with competitors, and identify strengths and weaknesses. Based on these results, it is possible to tweak the dataset and hyperparameters, or even modify the architecture.\n\nWe can broadly categorize general-purpose evaluations in three phases: during pre-training, after pre-training, and after fine-tuning.\n\nDuring pre-training, we closely monitor how the model learns, as shown at the end of Chapter 5. The most straightforward metrics are low-level and correspond to how models are trained:\n\nTraining loss: Based on the cross-entropy loss, measures the difference between the model’s predicted probability distribution and the true distribution of the next token\n\nValidation loss: Calculates the same loss as training loss, but on a held- out validation set to assess generalization\n\nPerplexity: Exponential of the cross-entropy loss, representing how “surprised” the model is by the data (lower is better)\n\nGradient norm: Monitors the magnitude of gradients during training to detect potential instabilities or vanishing/exploding gradients\n\nIt’s also possible to include benchmarks like HellaSwag (common sense reasoning) during this stage but there’s a risk of overfitting these evaluations.\n\nAfter pre-training, it is common to use a suite of evaluations to evaluate the base model. This suite can include internal and public benchmarks. Here’s a non-exhaustive list of common public pre-training evaluations:\n\nMMLU (knowledge): Tests models on multiple-choice questions across 57 subjects, from elementary to professional levels\n\nHellaSwag (reasoning): Challenges models to complete a given situation with the most plausible ending from multiple choices\n\nARC-C (reasoning): Evaluates models on grade-school-level multiple- choice science questions requiring causal reasoning\n\nWinogrande (reasoning): Assesses common sense reasoning through pronoun resolution in carefully crafted sentences\n\nPIQA (reasoning): Measures physical common sense understanding through questions about everyday physical interactions\n\nMany of these datasets are also used to evaluate general-purpose fine-tuned models. In this case, we focus on the difference in a given score between the base and the fine-tuned model. For example, bad fine-tuning can degrade the knowledge of the model, measured by MMLU. On the contrary, a good one might instill even more knowledge and increase the MMLU score.\n\nThis can also help identify any contamination issues, where the model might have been fine-tuned on data that is too close to a test set. For instance, improving the MMLU score of a base model by 10 points during the fine-tuning phase is unlikely. This is a sign that the instruction data might be contaminated.\n\nIn addition to these pre-trained evaluations, fine-tuned models also have their own benchmarks. Here, we use the term “fine-tuned model” to designate a model that has been trained with supervised fine-tuning (SFT) and preference alignment. These benchmarks target capabilities connected to the ability of fine-tuned models to understand and answer questions. In particular, they test instruction-following, multi-turn conversation, and agentic skills:\n\nIFEval (instruction following): Assesses a model’s ability to follow instructions with particular constraints, like not outputting any commas in your answer\n\nChatbot Arena (conversation): A framework where humans vote for the best answer to an instruction, comparing two models in head-to-head conversations\n\nAlpacaEval (instruction following): Automatic evaluation for fine- tuned models that is highly correlated with Chatbot Arena\n\nMT-Bench (conversation): Evaluates models on multi-turn conversations, testing their ability to maintain context and provide coherent responses\n\nGAIA (agentic): Tests a wide range of abilities like tool use and web browsing, in a multi-step fashion\n\nUnderstanding how these evaluations are designed and used is important to choose the best LLM for your application. For example, if you want to fine- tune a model, you want the best base model in terms of knowledge and reasoning for a given size. This allows you to compare the capabilities of different LLMs and pick the one that will offer the strongest foundation for your fine-tuning.\n\nEven if you don’t want to fine-tune a model, benchmarks like Chatbot Arena or IFEval are a good way to compare different instruct models. For instance, you want great conversational abilities if you’re building a chatbot. However, this is not necessary if your end goal is something like information extraction from unstructured documents. In this case, you will benefit more from excellent instruction-following skills to understand and execute tasks.\n\nWhile these benchmarks are popular and useful, they also suffer from inherent flaws. For example, public benchmarks can be gamed by training models on test data or samples that are very similar to benchmark datasets. Even human evaluation is not perfect and is often biased toward long and confident answers, especially when they’re nicely formatted (e.g., using Markdown). On the other hand, private test sets have not been scrutinized as much as public ones and might have their own issues and biases.\n\nThis means that benchmarks are not a single source of truth but should be used as signals. Once multiple evaluations provide a similar answer, you can raise your confidence level about the real capabilities of a model.\n\nOceanofPDF.com\n\nDomain-specific LLM evaluations\n\nDomain-specific LLMs don’t have the same scope as general-purpose models. This is helpful to target more fine-grained capabilities with more depth than the previous benchmarks.\n\nWithin the category, the choice of benchmarks entirely depends on the domain in question. For common applications like a language-specific model or a code model, it is recommended to search for relevant evaluations and even benchmark suites. These suites encompass different benchmarks and are designed to be reproducible. By targeting different aspects of a domain, they often capture domain performance more accurately.\n\nTo illustrate this, here is a list of domain-specific evaluations with leaderboards on the Hugging Face Hub:\n\nOpen Medical-LLM Leaderboard: Evaluates the performance of LLMs in medical question-answering tasks. It regroups 9 metrics, with 1,273 questions from the US medical license exams (MedQA), 500 questions from PubMed articles (PubMedQA), 4,183 questions from Indian medical entrance exams (MedMCQA), and 1,089 questions from 6 sub-categories of MMLU (clinical knowledge, medical genetics, anatomy, professional medicine, college biology, and college medicine).\n\nBigCodeBench Leaderboard: Evaluates the performance of code LLMs, featuring two main categories: BigCodeBench-Complete for code completion based on structured docstrings, and BigCodeBench- Instruct for code generation from natural language instructions. Models are ranked by their Pass@1 scores using greedy decoding, with an additional Elo rating for the Complete variant. It covers a wide range of programming scenarios that test LLMs’ compositional reasoning and instruction-following capabilities.\n\nHallucinations Leaderboard: Evaluates LLMs’ tendency to produce false or unsupported information across 16 diverse tasks spanning 5 categories. These include Question Answering (with datasets like NQ Open, TruthfulQA, and SQuADv2), Reading Comprehension (using TriviaQA and RACE), Summarization (employing HaluEval Summ, XSum, and CNN/DM), Dialogue (featuring HaluEval Dial and FaithDial), and Fact Checking (utilizing MemoTrap, SelfCheckGPT, FEVER, and TrueFalse). The leaderboard also assesses instruction- following ability using IFEval.\n\nEnterprise Scenarios Leaderboard: Evaluates the performance of LLMs on six real-world enterprise use cases, covering diverse tasks relevant to business applications. The benchmarks include FinanceBench (100 financial questions with retrieved context), Legal Confidentiality (100 prompts from LegalBench for legal reasoning), Writing Prompts (creative writing evaluation), Customer Support Dialogue (relevance in customer service interactions), Toxic Prompts (safety assessment for harmful content generation), and Enterprise PII (business safety for sensitive information protection). Some test sets are closed-source to prevent gaming of the leaderboard. The evaluation focuses on specific capabilities such as answer accuracy, legal reasoning, creative writing, contextual relevance, and safety measures, providing a comprehensive assessment of LLMs’ suitability for enterprise environments.\n\nLeaderboards can have different approaches based on their domain. For example, BigCodeBench is significantly different from others because it relies on only two metrics that sufficiently capture the entire domain. On the other hand, the Hallucinations Leaderboard regroups 16 metrics, including many general-purpose evaluations. It shows that in addition to custom benchmarks, reusing general-purpose ones can complete your own suite.\n\nIn particular, language-specific LLMs often reuse translated versions of general-purpose benchmarks. This can be completed with original evaluations in the native language. While some of these benchmarks use machine translation, it is better to rely on human-translated evaluations to improve their quality. We selected the following three task-specific leaderboards and their respective evaluation suites to give you an idea of how to build your own:\n\nOpenKo-LLM Leaderboard: Evaluates the performance of Korean LLMs using nine metrics. These metrics are a combination of general- purpose benchmarks translated into Korean (GPQA, Winogrande, GSM8K, EQ-Bench, and IFEval) and custom evaluations (Knowledge, Social Value, Harmlessness, and Helpfulness).\n\nOpen Portuguese LLM Leaderboard: Evaluates the performance of Portuguese language LLMs using nine diverse benchmarks. These benchmarks include educational assessments (ENEM with 1,430 questions, and BLUEX with 724 questions from university entrance exams), professional exams (OAB Exams with over 2,000 questions), language understanding tasks (ASSIN2 RTE and STS, FAQUAD NLI),\n\nand social media content analysis (HateBR with 7,000 Instagram comments, PT Hate Speech with 5,668 tweets, and tweetSentBR).\n\nOpen Arabic LLM Leaderboard: Evaluates the performance of Arabic language LLMs using a comprehensive set of benchmarks, including both native Arabic tasks and translated datasets. The leaderboard features two native Arabic benchmarks: AlGhafa and Arabic-Culture- Value-Alignment. Additionally, it incorporates 12 translated benchmarks covering various domains, such as MMLU, ARC- Challenge, HellaSwag, and PIQA.\n\nBoth general-purpose and domain-specific evaluations are designed with three main principles. First, they should be complex and challenge models to distinguish good and bad outputs. Second, they should be diverse and cover as many topics and scenarios as possible. When one benchmark is not enough, additional ones can create a stronger suite. Finally, they should be practical and easy to run. This is more connected to evaluation libraries, which can be more or less complex to work with. We recommend lm- evaluation-harness (github.com/EleutherAI/lm-evaluation-harness) from Eleuther AI and lighteval (github.com/huggingface/lighteval) from Hugging Face to run your benchmarks.\n\nOceanofPDF.com\n\nTask-specific LLM evaluations\n\nWhile general-purpose and domain-specific evaluations indicate strong base or instruct models, they cannot provide insights into how well these models work for a given task. This requires benchmarks specifically designed for this purpose, measuring downstream performance.\n\nBecause of their narrow focus, task-specific LLMs can rarely rely on pre-existing evaluation datasets. This can be advantageous because their outputs also tend to be more structured and easier to evaluate using traditional ML metrics. For example, a summarization task can leverage the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metric, which measures the overlap between the generated text and reference text using n-grams.\n\nLikewise, classification tasks also benefit from it and use the following classic metrics, among others:\n\nAccuracy: Accuracy refers to the proportion of correctly predicted instances compared to the total instances. It’s particularly useful for tasks with categorical outputs or where there is a clear distinction between right and wrong answers, such as named entity recognition (NER).\n\nPrecision: The ratio of true positive predictions to the total positive predictions made by the model.\n\nRecall: The ratio of true positive predictions to the total actual positive instances.\n\nF1 Score: The harmonic mean of precision and recall, used to balance both metrics. These are particularly useful in tasks such as classification or entity extraction.\n\nWhen the task cannot be directly mapped to a traditional ML task, it is possible to create a custom benchmark. This benchmark can be inspired by general-purpose and domain-specific evaluation datasets. A common and successful pattern is the use of multiple-choice question answering. In this framework, the instruction consists of a question with several options. See the following example with a question from the MMLU dataset (abstract algebra):\n\nInstruction Find the degree for the given field extension Q(sqrt(2), sqrt(3)) over Q. A. 0 B. 4 C. 2 D. 6\n\nOutput B\n\nTable 7.1: Example from the MMLU dataset\n\nThere are two main ways of evaluating models with this scheme—text generation and log-likelihood evaluations:\n\nThe first approach involves having the model generate text responses and comparing those to predefined answer choices. For example, the model generates a letter (A, B, C, or D) as its answer, which is then checked against the correct answer. This method tests the model’s ability to produce coherent and accurate responses in a format similar to how it would be used in real-world applications.\n\nEvaluation using probabilities, on the other hand, looks at the model’s predicted probabilities for different answer options without requiring text generation. For MMLU, lm-evaluation-harness compares the probabilities for the full text of each answer choice. This approach allows for a more nuanced assessment of the model’s understanding, as it can capture the relative confidence the model has in different options, even if it wouldn’t necessarily generate the exact correct answer text.\n\nFor simplicity, we recommend the text-generation version of the evaluation that mimics human test-taking. It is easier to implement, and generally more discriminative, as low-quality models tend to overperform on probability- based evaluations. You can adapt this technique to quiz your models about a particular task, and even expand it to specific domains.\n\nConversely, if the task is too open-ended, traditional ML metrics and multiple-choice question answering might not be relevant. In this scenario, the LLM-as-a-judge technique introduced in Chapter 5 can be used to evaluate the quality of the answers. If you have ground-truth answers, providing them as additional context improves the accuracy of the evaluation. Otherwise, defining different dimensions (such as relevance or toxicity, depending on your task) can also ground the evaluation in more interpretable categories.\n\nIt is recommended to use large models for evaluation and to iteratively refine your prompt. In this process, the explanations outputted by the model are important for understanding errors in its reasoning and fixing them through additional prompt engineering.\n\nIn order to easily parse answers, one can specify a structure in the instruction or use some kind of structured generation (like Outlines or OpenAI’s JSON mode). Here is an example of an instruction with a structure:\n\nYou are an evaluator who assesses the quality of an answer to an instruction. Your goal is to provide a score that re\n\nTable 7.2: Example of general-purpose LLM-as-a-judge prompt for answer evaluation\n\nNaturally, you can tweak the scale, add a ground-truth answer to this prompt, and customize it for your own use cases.\n\nHowever, judge LLMs can exhibit biases favoring assertive or verbose responses, potentially overrating answers that sound more confident but are less accurate. They may also lack domain expertise for specialized topics, leading to misjudgments. Consistency is also a concern, as LLMs might score similar responses differently. Additionally, they could have implicit preferences for certain writing styles unrelated to actual answer quality. To mitigate these issues, it’s possible to combine LLM evaluations with other metrics, use multiple judges, and carefully design prompts to address biases.\n\nOnce a model has been properly evaluated and works as intended, it might be included within a broader system. In the next section, we will see how systems change the evaluation framework.\n\nOceanofPDF.com\n\nRAG evaluation\n\nWhile traditional LLM evaluation focuses on the model’s inherent capabilities, RAG evaluation requires a more comprehensive approach that considers both the model’s generative abilities and its interaction with external information sources.\n\nRAG systems combine the strengths of LLMs with information retrieval mechanisms, allowing them to generate responses that are not only coherent and contextually appropriate but also grounded in up-to-date, externally sourced information. This makes RAG particularly valuable in fields where current and accurate information is crucial, such as news reporting, research, and customer support.\n\nThe evaluation of RAG systems goes beyond assessing a standalone LLM. It requires examining the entire system’s performance, including:\n\nRetrieval accuracy: How well does the system fetch relevant information?\n\nIntegration quality: How effectively is the retrieved information incorporated into the generated response?\n\nFactuality and relevance: Does the final output address the query appropriately while seamlessly blending retrieved and generated content?\n\nKey metrics for RAG evaluation include retrieval precision and recall, which measure the accuracy and comprehensiveness of the retrieved information. Additionally, the quality of integration between retrieved data and generated text is crucial, as is the overall factuality and coherence of the output.\n\nTo illustrate how these metrics are applied in practice, consider a RAG system designed for a customer support chatbot in an e-commerce setting. In this scenario, the user asks “What’s your return policy for laptops purchased during the holiday sale?” The RAG pipeline finds relevant documents on the electronics return policy and documents on holiday sale terms. This additional context is appended at the end of the question, and the model uses it to respond:\n\nFor laptops purchased during our holiday sale, you have an extended return period of 60 days from the date of purc\n\nTable 7.3: Example of output from a RAG pipeline designed for customer support\n\nIn this pipeline, we can evaluate if the retrieved documents correspond to what was expected (retrieval accuracy). We can also measure the difference between responses with and without additional context (integration quality). Finally, we can assess whether the output is relevant and grounded in the information provided by the documents (factuality and relevance).\n\nIn this section, we will cover two methods to evaluate how well RAG models incorporate external information into their responses.\n\nOceanofPDF.com\n\nRagas\n\nRetrieval-Augmented Generation Assessment (Ragas) is an open-source toolkit designed to provide developers with a comprehensive set of tools for RAG evaluation and optimization. It’s designed around the idea of metrics-driven development (MDD), a product development approach that relies on data to make well-informed decisions, involving the ongoing monitoring of essential metrics over time to gain valuable insights into an application’s performance. By embracing this methodology, Ragas enables developers to objectively assess their RAG systems, identify areas for improvement, and track the impact of changes over time.\n\nOne of the key capabilities of Ragas is its ability to synthetically generate diverse and complex test datasets. This feature addresses a significant pain point in RAG development, as manually creating hundreds of questions, answers, and contexts is both time-consuming and labor-intensive. Instead, it uses an evolutionary approach paradigm inspired by works like Evol- Instruct to craft questions with varying characteristics such as reasoning complexity, conditional elements, and multi-context requirements. This approach ensures a comprehensive evaluation of different components within the RAG pipeline.\n\nAdditionally, Ragas can generate conversational samples that simulate chat- based question-and-follow-up interactions, allowing developers to evaluate their systems in more realistic scenarios.\n\nFigure 7.1: Overview of the Ragas evaluation framework\n\nAs illustrated in Figure 7.1, Ragas provides a suite of LLM-assisted evaluation metrics designed to objectively measure different aspects of RAG system performance. These metrics include:\n\nFaithfulness: This metric measures the factual consistency of the generated answer against the given context. It works by breaking down the answer into individual claims and verifying if each claim can be inferred from the provided context. The faithfulness score is calculated as the ratio of verifiable claims to the total number of claims in the answer.\n\nAnswer relevancy: This metric evaluates how pertinent the generated answer is to the given prompt. It uses an innovative approach where an LLM is prompted to generate multiple questions based on the answer and then calculates the mean cosine similarity between these generated questions and the original question. This method helps identify answers that may be factually correct but off-topic or incomplete.\n\nContext precision: This metric evaluates whether all the ground-truth relevant items present in the contexts are ranked appropriately. It considers the position of relevant information within the retrieved context, rewarding systems that place the most pertinent information at the top.\n\nContext recall: This metric measures the extent to which the retrieved context aligns with the annotated answer (ground truth). It analyzes each claim in the ground truth answer to determine whether it can be attributed to the retrieved context, providing insights into the completeness of the retrieved information.\n\nFinally, Ragas also provides building blocks for monitoring RAG quality in production environments. This facilitates continuous improvement of RAG systems. By leveraging the evaluation results from test datasets and insights gathered from production monitoring, developers can iteratively enhance their applications. This might involve fine-tuning retrieval algorithms, adjusting prompt engineering strategies, or optimizing the balance between retrieved context and LLM generation.\n\nRagas can be complemented with another approach, based on custom classifiers.\n\nOceanofPDF.com\n\nARES\n\nARES (an automated evaluation framework for RAG systems) is a comprehensive tool designed to evaluate RAG systems. It offers an automated process that combines synthetic data generation with fine-tuned classifiers to assess various aspects of RAG performance, including context relevance, answer faithfulness, and answer relevance.\n\nThe ARES framework operates in three main stages: synthetic data generation, classifier training, and RAG evaluation. Each stage is configurable, allowing users to tailor the evaluation process to their specific needs and datasets.\n\nIn the synthetic data generation stage, ARES creates datasets that closely mimic real-world scenarios for robust RAG testing. Users can configure this process by specifying document file paths, few-shot prompt files, and output locations for the synthetic queries. The framework supports various pre-trained language models for this task, with the default being google/flan-t5-xxl. Users can control the number of documents sampled and other parameters to balance between comprehensive coverage and computational efficiency.\n\nFigure 7.2: Overview of the ARES evaluation framework\n\nThe classifier training stage involves creating high-precision classifiers to determine the relevance and faithfulness of RAG outputs. Users can specify the classification dataset (typically generated from the previous stage), test set for evaluation, label columns, and model choice. ARES uses microsoft/deberta-v3-large as the default model but supports other Hugging Face models. Training parameters such as the number of epochs, patience value for early stopping, and learning rate can be fine-tuned to optimize classifier performance.\n\nThe final stage, RAG evaluation, leverages the trained classifiers and synthetic data to assess the RAG model’s performance. Users provide evaluation datasets, few-shot examples for guiding the evaluation, classifier checkpoints, and gold label paths. ARES supports various evaluation metrics and can generate confidence intervals for its assessments.\n\nARES offers flexible model execution options, supporting both cloud-based and local runs through vLLM integration. The framework also supports various artifact types (code snippets, documents, HTML, images, and so on), enabling comprehensive evaluation across different RAG system outputs.\n\nIn summary, Ragas and ARES complement each other through their distinct approaches to evaluation and dataset generation. Ragas’s strength in production monitoring and LLM-assisted metrics can be combined with ARES’s highly configurable evaluation process and classifier-based assessments. While Ragas may offer more nuanced evaluations based on LLM capabilities, ARES provides consistent and potentially faster evaluations once its classifiers are trained. Combining them offers a comprehensive evaluation framework, benefiting from quick iterations with Ragas and in-depth, customized evaluations with ARES at key stages.\n\nIn the next section, we will create our own evaluation framework to evaluate our task-specific TwinLlama-3.1-8B model.\n\nOceanofPDF.com\n\nEvaluating TwinLlama-3.1-8B\n\nIn the previous chapters, we created two models fine-tuned to generate high-quality posts and articles: TwinLlama-3.1-8B and TwinLlama-3.1-8B- DPO. Based on this summary, we want to assess their abilities to write text that is both accurate and well-written. In comparison, general-purpose fine- tuned models are accurate thanks to their extensive knowledge but often use overly formal and verbose language. With this fine-tuning, we want to adopt a more natural writing style, based on the original articles from the training set.\n\nDue to the open-ended nature of this problem, we will leverage a judge LLM to evaluate the quality of the generated text. It will take both the instruction and the answer as inputs, and score it on a 1–3 scale based on two criteria:\n\nAccuracy: The degree of factual correctness and comprehensiveness of the information presented in the answer\n\nStyle: The appropriateness of the tone and writing style for blog posts or social media content (no formal or academic expressions)\n\nIn our evaluation framework, we will use the test split of our instruction dataset to get test instructions. We will feed them to our models and generate answers. These answers will then be evaluated by our judge LLM\n\n(GPT-4o-mini), based on a prompt that specifies our criteria. Finally, we will analyze the scores and draw conclusions based on qualitative and quantitative evaluations.\n\nOceanofPDF.com\n\nGenerating answers\n\nThe first step consists of efficiently generating answers for each instruction in our test set. In addition to our two models, we will also use meta- llama/Meta-Llama-3.1-8B-Instruct, the official instruct version of Llama- 3.1-8B, as a reference point to better understand the trade-offs we made.\n\nLet’s start the first stage of the implementation:\n\nWe import the relevant libraries, including vLLM for fast generation. This library is a lot faster than transformers for batch generation with local models:\n\nfrom\n\nvllm\n\nimport\n\nLLM, SamplingParams\n\nfrom\n\ndatasets\n\nimport\n\nload_dataset\n\nfrom\n\ntqdm.auto\n\nimport\n\ntqdm\n\nimport\n\ngc\n\nWe define a function called\n\ngenerate_answers\n\nthat will process our dataset and generate responses using a specified model. It takes two inputs—the ID of the model we want to use and the name of the test dataset:\n\ndef\n\ngenerate_answers\n\n(\n\nmodel_id, dataset_name\n\n): dataset = load_dataset(dataset_name, split=\n\n\"test\"\n\n)\n\nWe need to format the raw instructions using the chat template our models have been trained on. Note that Llama-3.1-8B-Instruct has been used with a different template, but it can follow this simple format. Here, we use the same chat template with every model for simplicity. We map the entire test set to this template with the\n\nformat()\n\nfunction:\n\ndef\n\nformat\n\n(\n\nsample\n\n):\n\nreturn\n\n\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{}\\n\\n### Response:\\n\"\n\n.\n\nformat\n\n(sample[\n\n\"instruction\"\n\n]) dataset = dataset.\n\nmap\n\n(\n\nlambda\n\nsample: {\n\n\"prompt\"\n\n:\n\nformat\n\n(sample)})\n\nLet’s initialize the LLM object used by vLLM with a maximum length of 4,096 tokens. We can also specify sampling parameters, which correspond to variables used in the decoding strategy. Here, we use parameters to encourage diversity (high temperature) while removing the most unlikely tokens (\n\ntop_p\n\nand\n\nmin_p\n\n). Finally, we start the generation by providing the list of prompts with\n\ndataset[\"prompt\"]\n\n:\n\nllm = LLM(model=model_id, max_model_len=\n\n4096\n\n) sampling_params = SamplingParams(temperature=\n\n0.8\n\n, top_p=\n\n0.95\n\n, min_p=\n\n0.05\n\n, max_tokens=\n\n4096\n\n) outputs = llm.generate(dataset[\n\n\"prompt\"\n\n], sampling_params)\n\nThis process should take a few minutes with our 334 prompts. Once this is done, we extract the answers from the object that is outputted by vLLM. We then add these answers as a new column to our dataset. This is useful to log the answers and review them later:\n\nanswers = [output.outputs[\n\n0\n\n].text\n\nfor\n\noutput\n\nin\n\noutputs] dataset = dataset.add_column(\n\n\"answers\"\n\n, answers)\n\nWe save our results to the Hugging Face Hub for easy access later. Then, we clear our GPU memory to prevent running out of space when we process the next model:\n\nprint\n\n(\n\nf\"Uploading results for\n\n{model_id}\n\n\"\n\n) dataset.push_to_hub(\n\nf\"mlabonne/\n\n{model_id.split(\n\n'/'\n\n)[-\n\n1\n\n]}\n\nresults\"\n\n) gc.collect()\n\nreturn\n\ndataset\n\nWe create a list of the three models we want to test. Then, we run our\n\ngenerate_answers()\n\nfunction for each of these models, one at a time. This will create and upload a separate set of results for each model:\n\nmodel_ids = [\n\n'mlabonne/TwinLlama-3.1-8B'\n\n,\n\n'mlabonne/TwinLlama-3.1-8B-DPO'\n\n,\n\n'meta-llama/Meta-Llama-3.1-8B-Instruct'\n\n]\n\nfor\n\nmodel_id\n\nin\n\nmodel_ids: generate_answers(model_id,\n\n\"mlabonne/llmtwin\"\n\n)\n\nNow that we have the answer generation, we can move on to the evaluation process.\n\nOceanofPDF.com\n\nEvaluating answers\n\nTo evaluate our answers, we will rely on GPT-4o-mini as a judge. This strategy is similar to what we used for data generation. As a matter of fact, you could adapt it to filter out bad samples during the data generation process. Here, we will score every generated answer from every model in terms of accuracy and style. The average scores will inform us about the quality of our fine-tuning compared to Llama-3.1-8B-Instruct:\n\nFirst, we import the required libraries, including\n\nopenai\n\n:\n\nimport\n\njson\n\nfrom\n\ntyping\n\nimport\n\nList\n\nfrom\n\ndatasets\n\nimport\n\nDataset, load_dataset\n\nfrom\n\nopenai\n\nimport\n\nOpenAI\n\nfrom\n\ntqdm.auto\n\nimport\n\ntqdm\n\nimport\n\nconcurrent.futures\n\nWe then define the\n\nevaluate_answer()\n\nfunction. This function contains our evaluation prompt, which sets up the context for evaluating answers based on accuracy and style:\n\ndef\n\nevaluate_answer\n\n(\n\ninstruction:\n\nstr\n\n, answer:\n\nstr\n\n, client: OpenAI\n\n) ->\n\ndict\n\n: prompt =\n\nf\"\"\"You are an expert judge. Please evaluate the quality of a given answer to an instruction based on two criteria:\n\n1. Accuracy: How factually correct is the information presented in the answer? You are a technical expert in this topic.\n\n2. Style: Is the tone and writing style appropriate for a blog post or social media content? It should use simple but technical words and avoid formal or academic language.\n\nIn the same prompt, we define our scales for each metric. Those are three- point Likert scales with a precise definition for each score:\n\nAccuracy scale: 1 (Poor): Contains factual errors or misleading information 2 (Good): Mostly accurate with minor errors or omissions 3 (Excellent): Highly accurate and comprehensive Style scale: 1 (Poor): Too formal, uses some overly complex words 2 (Good): Good balance of technical content and accessibility, but still uses formal words and expressions 3 (Excellent): Perfectly accessible language for blog/social media, uses simple but precise technical terms when necessary\n\nFinally, we conclude the prompt with two examples to illustrate what we mean by “complex words” and “formal or academic language.” We provide the corresponding instruction-answer pair and ask the model to return a response in JSON:\n\nExample of bad style: The Llama2 7B model constitutes a noteworthy progression in the field of artificial intelligence, serving as the successor to its predecessor, the original Llama architecture. Example of excellent style: Llama2 7B outperforms the original Llama model across multiple benchmarks. Instruction: {instruction} Answer: {answer} Provide your evaluation in JSON format with the following structure: {{\n\n\"accuracy\"\n\n: {{\n\n\"analysis\"\n\n:\n\n\"...\"\n\n,\n\n\"score\"\n\n:\n\n0\n\n}},\n\n\"style\"\n\n: {{\n\n\"analysis\"\n\n:\n\n\"...\"\n\n,\n\n\"score\"\n\n:\n\n0\n\n}} }}\n\n\"\"\"\n\nThis prompt is given as a user query to the GPT-4o-mini model. The system prompt reinforces that we are interested in answer evaluation based on accuracy and style:\n\ncompletion = client.chat.completions.create( model=\n\n\"gpt-4o-mini\"\n\n, messages=[ {\n\n\"role\"\n\n:\n\n\"system\"\n\n,\n\n\"content\"\n\n:\n\n\"You are a helpful assistant who evaluates answers based on accuracy and style. Provide your response in JSON format with a short analysis and score for each criterion.\"\n\n, }, {\n\n\"role\"\n\n:\n\n\"\n\nuser\"\n\n,\n\n\"content\"\n\n: prompt}, ], response_format={\n\n\"type\"\n\n:\n\n\"json_object\"\n\n}, max_tokens=\n\n1000\n\n, temperature=\n\n0.8\n\n, )\n\nAs in the previous chapters, we will batch our requests to speed up the process. This is why we create an\n\nevaluate_batch()\n\nfunction, which returns a list of parsed structured outputs with their corresponding indices. These indices are important to ensure a correct ordering of the evaluations:\n\ndef\n\nevaluate_batch\n\n(\n\nbatch, start_index\n\n): client = OpenAI(api_key=OPENAI_KEY)\n\nreturn\n\n[ (i, evaluate_answer(instr, ans, client))\n\nfor\n\ni, (instr, ans)\n\nin\n\nenumerate\n\n(batch, start=start_index) ]\n\nWe can now orchestrate the previous code in the\n\nevaluate_answers()\n\nfunction. It takes the model ID, number of threads, and batch size as inputs. First, we load the dataset with the generations we previously saved:\n\ndef\n\nevaluate_answers\n\n(\n\nmodel_id:\n\nstr\n\n, num_threads:\n\nint\n\n=\n\n10\n\n, batch_size:\n\nint\n\n=\n\n5\n\n) -> Dataset: dataset = load_dataset(\n\nf\"mlabonne/\n\n{model_id.split(\n\n'/'\n\n)[-\n\n1\n\n]}\n\nresults\"\n\n, split=\n\n\"all\"\n\n)\n\nWe create batches of instruction-answer pairs from our dataset. Each batch contains\n\nbatch_size\n\nnumber of pairs:\n\nbatches = [ (i,\n\nlist\n\n(\n\nzip\n\n(dataset[\n\n\"instruction\"\n\n][i:i+batch_size], dataset[\n\n\"answers\"\n\n][i:i+batch_size])))\n\nfor\n\ni\n\nin\n\nrange\n\n(\n\n0\n\n,\n\nlen\n\n(dataset), batch_size) ]\n\nWe perform parallel evaluation of batches of instruction-answer pairs using multiple threads. We use parallel processing to evaluate multiple batches simultaneously, speeding up the overall evaluation process. The\n\nThreadPoolExecutor\n\nsubmits each batch to\n\nevaluate_batch()\n\n. The results are stored in the evaluations list:\n\nevaluations = [\n\nNone\n\n] *\n\nlen\n\n(dataset)\n\nwith\n\nconcurrent.futures.ThreadPoolExecutor(max_workers=num_threads)\n\nas\n\nexecutor: futures = [executor.submit(evaluate_batch, batch, start_index)\n\nfor\n\nstart_index, batch\n\nin\n\nbatches]\n\nfor\n\nfuture\n\nin\n\ntqdm(concurrent.futures.as_completed(futures), total=\n\nlen\n\n(futures)):\n\nfor\n\nindex, evaluation\n\nin\n\nfuture.result(): evaluations[index] = evaluation\n\nWe create a new column with the result of the evaluation process. This column will store the raw JSON output of the judge model, including scores and explanations:\n\nif\n\n'evaluation'\n\nin\n\ndataset.column_names: dataset = dataset.remove_columns([\n\n'evaluation'\n\n]) dataset = dataset.add_column(\n\n\"evaluation\"\n\n, evaluations)\n\nWe can directly parse this JSON object with\n\njson.loads()\n\nand try to retrieve the accuracy and style scores that should have been generated. This generation is in best-effort mode, which means that scores are not guaranteed. If there’s an error in parsing, we use\n\nNone\n\nvalues as a fallback:\n\naccuracy_scores = [] style_scores = []\n\nfor\n\nevaluation\n\nin\n\ndataset[\n\n'evaluation'\n\n]:\n\ntry\n\n: eval_dict = json.loads(evaluation)\n\nif\n\nisinstance\n\n(evaluation,\n\nstr\n\n)\n\nelse\n\nevaluation accuracy_score = eval_dict[\n\n'accuracy'\n\n][\n\n'score'\n\n] style_score = eval_dict[\n\n'style'\n\n][\n\n'score'\n\n] accuracy_scores.append(accuracy_score) style_scores.append(style_score)\n\nexcept\n\n(json.JSONDecodeError, KeyError, TypeError): accuracy_scores.append(\n\nNone\n\n) style_scores.append(\n\nNone\n\n)\n\nWe add two new columns to store the accuracy and style scores for further analysis:\n\nif\n\n'accuracy'\n\nin\n\ndataset.column_names: dataset = dataset.remove_columns([\n\n'accuracy'\n\n]) dataset = dataset.add_column(\n\n'accuracy'\n\n, accuracy_scores)\n\nif\n\n'style'\n\nin\n\ndataset.column_names: dataset = dataset.remove_columns([\n\n'style'\n\n]) dataset = dataset.add_column(\n\n'style'\n\n, style_scores)\n\nLet’s push the final dataset with generated answers, evaluations, and scores to the Hugging Face Hub:\n\ndataset.push_to_hub(\n\nf\"mlabonne/\n\n{model_id.split(\n\n'/'\n\n)[-\n\n1\n\n]}\n\nresults\"\n\n)\n\nreturn\n\ndataset\n\nWe can now call the\n\nevaluate_answers()\n\nfunction with the three models we selected:\n\nmodel_ids = [\n\n'mlabonne/TwinLlama-3.1-8B'\n\n,\n\n'mlabonne/TwinLlama-3.1-8B-DPO'\n\n,\n\n'meta-llama/Meta-Llama-3.1-8B-Instruct'\n\n]\n\nfor\n\nmodel_id\n\nin\n\nmodel_ids: evaluate_answers(model_id)\n\nBy saving intermediate results, our evaluation framework becomes more robust to failures. It is also easily extendable to other models, datasets, and criteria.\n\nOceanofPDF.com\n\nAnalyzing results\n\nThere are three elements we can review at the end of this evaluation process: model answers, explanations from evaluations, and scores.\n\nWhile reviewing answers is not scalable, it is a crucial step that can help to identify a lot of common mistakes, such as a wrong chat template or incorrect model. Here, we will focus on a single instruction and read the answers provided by our two models and Llama-3.1-8B-Instruct.\n\nInstruction Discuss the concept of algorithm bias and its implications.\n\nTwinLlama-3.1-8B Algorithm bias refers to the tendency of algorithms to produce outcomes that are skewed or un\n\nTwinLlama-3.1-8B-DPO Algorithm bias refers to the tendency of algorithms to produce outcomes that are skewed\n\nLlama-3.1-8B-Instruct Algorithm bias, also known as algorithmic bias, refers to the unintended or inherent bias in\n\nTable 7.4: Example of generated answers for a given instruction\n\nImmediately, we can see that the answers from our models are very close to each other. This is not the case with Llama-3.1-8B-Instruct’s answer, which is extremely verbose and lists many examples. This is typically the kind of answer that is correct but can feel artificial and unpleasant to read because of its length. On the other hand, the DPO model slightly simplifies the language of the SFT version, making it less academic. This is exactly the behavior we want to capture, modifying the writing style but not the actual content of the answer.\n\nLet’s now review the evaluations provided by GPT-4o-mini for each answer.\n\nTwinLlama-3.1-8B\n\nAccuracy The answer accurately defines algorithm bias and highlights its implications in fields like machine learni\n\nTwinLlama-3.1-8B-DPO\n\nAccuracy The answer accurately defines algorithm bias and outlines its implications in critical fields like machine\n\nLlama-3.1-8B-Instruct\n\nAccuracy The answer accurately defines algorithm bias and discusses its causes and implications in various domai\n\nTable 7.5: Evaluations of each answer made by GPT-4o-mini, according to style and accuracy\n\nAccording to our judge LLM, there is no issue with the accuracy of the answers, which get a perfect score. However, the style is considered too formal for TwinLlama-3.1-8B (SFT) and Llama-3.1-8B-Instruct, with a score of 2. The judge LLM agreed with our previous analysis and assigned a perfect score to TwinLlama-3.1-8B-DPO’s answer for communicating “the technical concept of algorithm bias without becoming overly formal.”\n\nThis trend is confirmed by the average scores obtained by each model:\n\nTwinLlama-3.1-8B - Accuracy: 2.45 TwinLlama-3.1-8B - Style: 2.04 TwinLlama-3.1-8B-DPO - Accuracy: 2.46\n\nTwinLlama-3.1-8B-DPO - Style: 2.12\n\nLlama-3.1-8B-Instruct - Accuracy: 2.62\n\nLlama-3.1-8B-Instruct - Style: 1.86\n\nIn terms of accuracy, our two fine-tuned models get similar scores, while Llama-3.1-8B-Instruct achieves the highest accuracy score of 2.62. This suggests that the instruct-tuned Llama model may have a slight edge in providing factually correct information. This is probably due to its extensive post-training process with over 10 million samples (compared to 13,000 in our case).\n\nHowever, when it comes to style, we see a different pattern. TwinLlama-3.1-8B-DPO leads with a score of 2.12, successfully achieving a more accessible and less formal writing style without sacrificing content quality. TwinLlama-3.1-8B (SFT) follows with 2.04, showing improvement but retaining some formality, while Llama-3.1- 8B-Instruct trails with 1.86, tending toward verbosity.\n\nBased on this feedback and the manual review of the generated answers, we can detect mistakes and identify areas for improvement. This is essential for refining the data generation process through additional filtering or augmenting the dataset with missing information. While this first version already shows promising results, iterating over different datasets and models will allow us to significantly outperform our baseline and create the best possible model for our use case.\n\nOceanofPDF.com\n\nSummary\n\nIn this chapter, we explored LLM evaluation with models and RAG systems. We saw how to interpret classic benchmarks like MMLU to select strong candidates to use or fine-tune. We also detailed how domain-specific and task-specific evaluations work, and how to create our own based on publicly available examples.\n\nWe focused on two techniques (multiple-choice question answering and LLM-as-a-judge) as the backbone of these custom evaluation frameworks.\n\nHowever, models are commonly integrated into broader systems that provide additional context. We introduced two evaluation frameworks for RAG systems, Ragas and ARES. We saw both similarities (for example, synthetic data generation) and differences in how they evaluate RAG systems (context-based metrics versus trained classifiers). Finally, we evaluated TwinLlama-3.1-8B with a judge LLM according to three criteria: relevance, coherence, and conciseness. This provided insights into how we can improve it.\n\nIn the next chapter, we will explore inference optimization techniques to improve speed and reduce memory usage, without significantly compromising model performance. We will also delve into optimization methods, model parallelism techniques and examine different quantization approaches.\n\nOceanofPDF.com\n\nReferences\n\nLianmin Zheng et al.. “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.” arXiv preprint arXiv:2306.05685, June 2023.\n\nAymeric Roucher. “Using LLM-as-a-judge for an automated and versatile evaluation - Hugging Face Open-Source AI Cookbook.” huggingface.co, No date found, https://huggingface.co/learn/cookbook/en/llm_judge.\n\nLangChain. “Aligning LLM-as-a-Judge with Human Preferences.” blog.langchain.dev, June 26, 2024, https://blog.langchain.dev/aligning-llm- as-a-judge-with-human-preferences/.\n\nDan Hendrycks et al.. “Measuring Massive Multitask Language Understanding.” arXiv preprint arXiv:2009.03300, September 2020.\n\nJeffrey Zhou et al.. “Instruction-Following Evaluation for Large Language Models.” arXiv preprint arXiv:2311.07911, November 2023.\n\nYann Dubois et al.. “Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators.” arXiv preprint arXiv:2404.04475, April 2024.\n\nGrégoire Mialon et al.. “GAIA: a benchmark for General AI Assistants.” arXiv preprint arXiv:2311.12983, November 2023.\n\nGiwon Hong et al.. “The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations in Large Language Models.” arXiv preprint arXiv:2404.05904, April 2024.\n\nShahul Es et al.. “RAGAS: Automated Evaluation of Retrieval Augmented Generation.” arXiv preprint arXiv:2309.15217, September 2023.\n\nJon Saad-Falcon et al.. “ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems.” arXiv preprint arXiv:2311.09476, November 2023.\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\n8\n\nOceanofPDF.com\n\nInference Optimization\n\nDeploying LLMs is challenging due to their significant computational and memory requirements. Efficiently running these models necessitates the use of specialized accelerators, such as GPUs or TPUs, which can parallelize operations and achieve higher throughput. While some tasks, like document generation, can be processed in batches overnight, others require low latency and fast generation, such as code completion. As a result, optimizing the inference process – how these models make predictions based on input data – is critical for many practical applications. This includes reducing the time it takes to generate the first token (latency), increasing the number of tokens generated per second (throughput), and minimizing the memory footprint of LLMs.\n\nIndeed, naive deployment approaches lead to poor hardware utilization and underwhelming throughput and latency. Fortunately, a variety of optimization techniques have emerged to dramatically speed up inference. This chapter will explore key methods like speculative decoding, model parallelism, and weight quantization, demonstrating how thoughtful implementations can achieve speedups of 2–4X or more. We will also introduce three popular inference engines (Text Generation Inference, vLLM, and TensorRT-LLM) and compare their features in terms of inference optimization.\n\nIn this chapter, we will cover the following topics:\n\nModel optimization strategies\n\nModel parallelism\n\nModel quantization\n\nBy the end of this chapter, you will understand the core challenges in LLM inference and be familiar with state-of-the-art optimization techniques, including model parallelism and weight quantization.\n\nAll the code examples from this chapter can be found on GitHub at https://github.com/PacktPublishing/LLM-Engineering.\n\nOceanofPDF.com\n\nModel optimization strategies\n\nMost of the LLMs used nowadays, like GPT or Llama, are powered by a decoder-only Transformer architecture. The decoder-only architecture is designed for text-generation tasks. It predicts the next word in a sequence based on preceding words, making it effective for generating contextually appropriate text continuations.\n\nIn contrast, an encoder-only architecture, like BERT, focuses on understanding and representing the input text with detailed embeddings. It excels in tasks that require comprehensive context understanding, such as text classification and named entity recognition. Finally, the encoder- decoder architecture, like T5, combines both functionalities. The encoder processes the input text to generate a context-rich representation, which the decoder then uses to produce the output text. This dual structure is particularly powerful for sequence-to-sequence tasks like translation and summarization, where understanding the input context and generating a relevant output are equally important.\n\nIn this book, we only focus on the decoder-only architecture, which dominates the LLM field.\n\nFigure 8.1 – Inference process with decoder-only models. We provide “I have a dream” as input and obtain “of” as output.\n\nAs shown in Figure 8.1, the basic inference process for a decoder-only model involves:\n\nTokenizing the input prompt and passing it through an embedding layer and positional encoding.\n\nComputing key and value pairs for each input token using the multi- head attention mechanism.\n\nGenerating output tokens sequentially, one at a time, using the computed keys and values.\n\nWhile Steps 1 and 2 are computationally expensive, they consist of highly parallelizable matrix multiplication that can achieve high hardware utilization on accelerators like GPUs and TPUs.\n\nThe real challenge is that the token generation in Step 3 is inherently sequential – to generate the next token, you need to have generated all previous tokens. This leads to an iterative process where the output sequence is grown one token at a time, failing to leverage the parallel computing capabilities of the hardware. Addressing this bottleneck is one of the core focuses of inference optimization.\n\nIn this section, we will detail several optimization strategies that are commonly used to speed up inference and reduce Video Random- Access Memory (VRAM) usage, such as implementing a (static) KV cache, continuous batching, speculative decoding, and optimized attention mechanisms.\n\nOceanofPDF.com\n\nKV cache\n\nWe saw that LLMs generate text token by token, which is slow because each new prediction depends on the entire previous context. For example, to predict the 100th token in a sequence, the model needs the context of tokens 1 through 99. When predicting the 101st token, it again needs the information from tokens 1 through 99, plus token 100. This repeated computation is particularly inefficient.\n\nThe key-value (KV) cache addresses this issue by storing key-value pairs produced by self-attention layers. Instead of recalculating these pairs for each new token, the model retrieves them from the cache, significantly speeding up the generation.\n\nYou can see an illustration of this technique in Figure 8.2:\n\nFigure 8.2 – Illustration of the KV cache\n\nWhen a new token is generated, only the key and value for that single token need to be computed and added to the cache. The KV cache is an immediate optimization that is implemented in every popular tool and library. Some implementations maintain a separate KV cache for each layer of the model.\n\nThe size of the KV cache scales with the number of tokens ( several model dimensions, like the number of layers ( of attention heads (\n\n) and\n\n), the number ), and the precision of\n\n), their dimension (\n\nthe parameters in bytes (\n\n):\n\nFor a typical 7B parameter model using 16-bit precision, this exceeds 2 GB for high sequence lengths (higher than 2,048 tokens). Larger models with more layers and higher embedding dimensions will see even greater memory requirements.\n\nSince the KV cache grows with each generation step and is dynamic, it prevents you from taking advantage of torch.compile, a powerful optimization tool that fuses PyTorch code into fast and optimized kernels. The static KV cache solves this issue by pre-allocating the KV cache size to a maximum value, which allows you to combine it with\n\ntorch.compile\n\nfor up to a 4x speedup in the forward pass.\n\nTo configure a model to use a static KV cache with the transformers library, follow these steps:\n\nWe import the tokenizer and the model we want to optimize:\n\nimport\n\ntorch\n\nfrom\n\ntransformers\n\nimport\n\nAutoTokenizer, AutoModelForCausalLM model_id =\n\n\"google/gemma-2b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\n\n\"auto\"\n\n)\n\nTo implement the static cache, we change the cache implementation in the model’s generation config to\n\nstatic\n\n:\n\nmodel.generation_config.cache_implementation =\n\n\"static\"\n\nNow that our KV cache is static, we can compile the model using torch.compile:\n\ncompiled_model = torch.compile(model,\n\nmode\n\n=\n\n\"reduce-overhead\"\n\n,\n\nfullgraph\n\n=\n\nTrue\n\n)\n\nWe tokenize an input question, “\n\nWhat is 2+2?\n\n\", and store it on a GPU if available (if not, we store it on the CPU):\n\ndevice =\n\n\"cuda\"\n\nif\n\ntorch.cuda.is_available()\n\nelse\n\n\"cpu\"\n\ninputs = tokenizer(\n\n\"What is 2+2?\"\n\n, return_tensors=\n\n\"pt\"\n\n).to(device)\n\nLet’s use the\n\ngenerate()\n\nmethod to get the model’s output and decode it with\n\nbatch_decode()\n\nto print its answer:\n\noutputs = model.generate(**inputs, do_sample=\n\nTrue\n\n, temperature=\n\n0.7\n\n, max_new_tokens=\n\n64\n\n)\n\nprint\n\n(tokenizer.batch_decode(outputs, skip_special_tokens=\n\nTrue\n\n))\n\n[\n\n'What is 2+2?\\n\\nThe answer is 4. 2+2 = 4.'\n\n]\n\nThis returns a list containing both the input and output, correctly answering our question.\n\nNote that the static cache doesn’t work with all architectures. For details on which architectures are supported, check out the transformers documentation.\n\nEfficiently managing the KV cache is essential, as it can quickly exhaust available GPU memory and limit the batch sizes that can be processed. This has motivated the development of memory-efficient attention mechanisms and other techniques, which we will cover in the last section.\n\nOceanofPDF.com\n\nContinuous batching\n\nBatching, or processing multiple inference requests simultaneously, is a standard approach to achieve high throughput. Larger batch sizes spread out the memory cost of model weights and transfer more data to the GPU at once, better saturating its parallel compute capacity.\n\nHowever, decoder-only models pose a particular challenge due to the high variability in input prompt lengths and desired output lengths. Some requests may have short prompts and only need a one-word answer, while others may input a lengthy context and expect a multi-paragraph response.\n\nWith traditional batching, we would have to wait for the longest request in a batch to complete before starting a new batch. This leads to under- utilization as the accelerator sits partly idle waiting for a straggling request to finish. Continuous batching, also known as\n\nin-flight\n\nbatching, aims to prevent idle time by immediately feeding a new request into the batch as soon as one completes.\n\nThe batching process begins the same – by filling the batch with initial requests. But as soon as a request completes its generation, it is evicted from the batch and a new request takes its place. This way, the accelerator is always processing a full batch, leading to maximally efficient hardware utilization. An additional consideration is the need to periodically pause the generation process to run prefill, or the embedding and encoding of waiting\n\nrequests. Finding the optimal balance between generation and prefill requires some tuning of the waiting-served ratio hyperparameter.\n\nContinuous batching is natively implemented in most inference frameworks, like Hugging Face’s Text Generation Inference (TGI), vLLM, and NVIDIA TensorRT-LLM.\n\nOceanofPDF.com\n\nSpeculative decoding\n\nAnother powerful optimization technique is speculative decoding, also called assisted generation. The key insight is that even with continuous batching, the token-by-token generation process fails to fully saturate the parallel processing capabilities of the accelerator. Speculative decoding aims to use this spare compute capacity to predict multiple tokens simultaneously, using a smaller proxy model (see Figure 8.3).\n\nFigure 8.3 – Illustration of traditional decoding (left) and speculative decoding (right)\n\nThe general approach is:\n\nApply a smaller model, like a distilled or pruned version of the main model, to predict multiple token completions in parallel. This could be 5–10 tokens predicted in a single step.\n\nFeed these speculative completions into the full model to validate which predictions match what the large model would have generated.\n\nRetain the longest matching prefix from the speculative completions and discard any incorrect tokens.\n\nThe result is that, if the small model approximates the large model well, multiple tokens can be generated in a single step. This avoids running the expensive large model for several iterations. The degree of speedup depends on the quality of the small model’s predictions – a 90% match could result in a 3–4X speedup.\n\nIt is crucial that both models use the same tokenizer. If this is not the case, the tokens generated by the draft model will not align with those produced by the large model, making them incompatible. Let’s implement this using the transformers library. In this example, we will use two Qwen1.5 models from Alibaba Cloud: a 1.8B version as the main model, and a 0.5B version as the draft model. Note that, if you have enough VRAM, you can use much larger models like 14B, 32B, 72B, or 110B as the main model.\n\nHere, we’re limited by the VRAM of the T4 GPU in Google Colab, but to get the maximum speedup, the assistant model should be much smaller than the large model.\n\nHere’s a step-by-step guide to implement speculative decoding:\n\nWe load the tokenizer and both models:\n\nimport\n\ntorch\n\nfrom\n\ntransformers\n\nimport\n\nAutoTokenizer, AutoModelForCausalLM model_id =\n\n\"Qwen/Qwen1.5-1.8B-Chat\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\n\n\"auto\"\n\n) draft_model = AutoModelForCausalLM.from_pretrained(\n\n\"Qwen/Qwen1.5-0.5B-Chat\"\n\n, device_map=\n\n\"auto\"\n\n)\n\nWe then tokenize the same input and store it in the accelerator, if available:\n\ndevice =\n\n\"cuda\"\n\nif\n\ntorch.cuda.is_available()\n\nelse\n\n\"cpu\"\n\ninputs = tokenizer(\n\n\"What is 2+2?\"\n\n, return_tensors=\n\n\"pt\"\n\n).to(device)\n\nWe can now use\n\nmodel.generate()\n\nwith the argument\n\nassistant_model\n\nto enable speculative decoding:\n\noutputs = model.generate(**inputs, do_sample=\n\nTrue\n\n, assistant_model=draft_model, temperature=\n\n0.7\n\n, max_new_tokens=\n\n64\n\n)\n\nprint\n\n(tokenizer.batch_decode(outputs, skip_special_tokens=\n\nTrue\n\n))\n\n[\n\n'What is 2+2? 2 + 2 equals 4!'\n\n]\n\nThe speedup in this small example is not significant, but it is clearly noticeable with bigger models.\n\nPrompt lookup decoding is a variant of speculative decoding, tailored to input-grounded tasks like summarization where there is often overlap between the prompt and output. Shared n-grams are used as the LLM candidate tokens. We can enable prompt lookup decoding by using the\n\nprompt_lookup_num_tokens\n\nparameter in\n\nmodel.generate()\n\n:\n\noutputs = model.generate(**inputs, prompt_lookup_num_tokens=\n\n4\n\n)\n\nBy combining the static KV cache with torch.compile, implementing continuous batching, and leveraging speculative decoding techniques, LLMs can see inference speedups of 2–4x or more with no loss in quality.\n\nAnother approach to creating a small proxy model consists of jointly fine- tuning a small model alongside a large model for maximum fidelity. A representative technique here is Medusa, which inserts dedicated speculation heads into the main model. The Medusa-1 approach fine-tunes these speculation heads while freezing the large model, while the Medusa-2 approach jointly fine-tunes both the speculation heads and the large model. The Medusa method has demonstrated impressive results, enabling a 70M parameter model to closely approximate the performance of a 7B parameter model on a range of tasks. Speculative decoding is natively supported by TGI.\n\nOceanofPDF.com\n\nOptimized attention mechanisms\n\nThe Transformer architecture is based on the attention mechanism, which scales quadratically with the number of input tokens (or sequence length). This is particularly inefficient for longer sequences, where the size of the KV cache can blow up.\n\nIntroduced by Kwon, Li, et al. (2023), PagedAttention addresses these memory challenges by drawing inspiration from virtual memory and paging in operating systems. It partitions the KV cache into blocks, eliminating the need for contiguous memory allocation. Each block contains the keys and values for a fixed number of tokens. During attention computation, the PagedAttention kernel efficiently fetches these blocks, regardless of their physical memory location.\n\nThis partitioning allows for near-optimal memory utilization. This is useful for batching more sequences together, which increases throughput and GPU utilization. Moreover,\n\nPagedAttention\n\n's block-based approach naturally supports memory sharing across multiple output sequences generated from the same prompt. This is particularly advantageous in parallel sampling and beam search, where the same prompt is used to generate multiple outputs. The shared memory blocks reduce redundant computations and memory usage, cutting the memory overhead by up to 55% and improving throughput by up to 2.2x, according to the authors. The vLLM library received the first implementation of\n\nPagedAttention. Since then, PagedAttention has also been implemented in TGI and TensorRT-LLM.\n\nAnother popular option is FlashAttention-2. Developed by Tri Dao (2023), it introduced several key innovations that are designed to address the quadratic runtime and memory constraints in traditional attention. By dividing input and output matrices into smaller blocks, FlashAttention-2 ensures that these blocks can fit into the GPU’s on-chip SRAM, which is much faster than high-bandwidth memory. This approach significantly reduces the frequency of data transfers between the GPU’s main memory and its processing units.\n\nThis is combined with online softmax, which computes the softmax function independently for each block of the attention scores matrix, rather than for the entire matrix at once. By maintaining a running maximum and a running sum of exponentials, FlashAttention-2 can calculate attention probabilities without needing to store large intermediate matrices.\n\nAdditionally, FlashAttention-2’s online softmax computation enables block- wise processing, maintaining accuracy while significantly reducing memory requirements. This is particularly important for training, where the recomputation of intermediate values (instead of storing them) in the backward pass reduces memory usage from quadratic to linear, in relation to sequence length.\n\nUnlike PagedAttention, FlashAttention-2 can easily be used with the transformers library through the\n\nattn_implementation\n\nparameter:\n\nInstall the\n\nflash-attn\n\nlibrary with\n\n--no-build-isolation\n\nso that we don’t install the dependencies:\n\npip install flash-attn --no-build-isolation\n\nTo use FlashAttention-2 for inference, specify\n\nflash_attention_2\n\nin the\n\nattn_implementation\n\nparameter when loading a model. For example, this is how to load Mistral- 7B-Instruct-v0.3 with FlashAttention-2:\n\nfrom\n\ntransformers\n\nimport\n\nAutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(\n\n\"mistralai/Mistral-7B-Instruct-v0.3\"\n\n, attn_implementation=\n\n\"flash_attention_2\"\n\n, )\n\nThe techniques presented in this section focused on improving the model’s efficiency in processing tokens. In the next section, we will discuss how to distribute our model and calculations across multiple GPUs.\n\nOceanofPDF.com\n\nModel parallelism\n\nModel parallelism allows you to distribute the memory and compute requirements of LLMs across multiple GPUs. This enables the training and inference of models too large to fit on a single device, while also improving performance in terms of throughput (tokens per second).\n\nThere are three main approaches to model parallelism, each involving splitting the model weights and computation in different ways: data parallelism, pipeline parallelism, and tensor parallelism.\n\nAlthough these approaches were originally developed for training, we can reuse them for inference by focusing on the forward pass only.\n\nOceanofPDF.com\n\nData parallelism\n\nData parallelism (DP) is the simplest type of model parallelism. It involves making copies of the model and distributing these replicas across different GPUs (see Figure 8.4). Each GPU processes a subset of the data simultaneously. During training, the gradients calculated on each GPU are averaged and used to update the model parameters, ensuring that each replica remains synchronized. This approach is particularly beneficial when the batch size is too large to fit into a single machine or when aiming to speed up the training process.\n\nFigure 8.4 – Illustration of data parallelism with four GPUs\n\nDuring inference, DP can be useful for processing concurrent requests. By distributing the workload across multiple GPUs, this approach helps reduce latency, as multiple requests can be handled simultaneously. This\n\nconcurrent processing also increases throughput, since a higher number of requests can be processed at the same time.\n\nHowever, the effectiveness of DP is limited by the model size and the communication overhead between GPUs. Indeed, replicating the model’s parameters on each GPU is inefficient. This means that this technique only works when the model is small enough to fit into a single GPU, leaving less room for input data and thus limiting the batch size. For larger models or when memory is a constraint, this can be a significant drawback.\n\nTypically, DP is mainly used for training, while pipeline and tensor parallelism are preferred for inference.\n\nOceanofPDF.com\n\nPipeline parallelism\n\nIntroduced by Huang et al. in the GPipe paper (2019), pipeline parallelism (PP) is a strategy for distributing the computational load of training and running large neural networks across multiple GPUs.\n\nUnlike traditional DP, which replicates the entire model on each GPU, pipeline parallelism partitions the model’s layers across different GPUs. This approach allows each GPU to handle a specific portion of the model, thereby reducing the memory burden on individual GPUs.\n\nFigure 8.5 – Illustration of pipeline parallelism with four GPUs\n\nAs shown in Figure 8.5, in a typical four-way pipeline parallel split, the model is divided into four segments, with each segment assigned to a different GPU. The first 25% of the model’s layers might be processed by GPU 1, the next 25% by GPU 2, and so on. During the forward pass,\n\nactivations are computed and then passed along to the next GPU. For training, the backward pass follows a similar sequence in reverse, with gradients being propagated back through the GPUs. The number of GPUs is often referred to as the degree of parallelism.\n\nThe primary advantage of pipeline parallelism is its ability to significantly reduce the memory requirements per GPU. However, this approach introduces new challenges, particularly related to the sequential nature of the pipeline. One of the main issues is the occurrence of “pipeline bubbles.” These bubbles arise when some GPUs are idle, waiting for activations from preceding layers. This idle time can reduce the overall efficiency of the process.\n\nMicro-batching was developed to mitigate the impact of pipeline bubbles. By splitting the input batch into smaller sub-batches, micro-batching ensures that GPUs remain busier, as the next sub-batch can begin processing before the previous one is fully completed.\n\nFigure 8.6 – Illustration of pipeline parallelism with micro-batching.\n\nFigure 8.6 shows an example of pipeline parallelism with micro-batching. In this example, the pipeline has four stages (F0, F1, F2, F3), and the input\n\nbatch is divided into four micro-batches. GPU 0 will process forward paths F0,0, F0,1, F0,2, and F0,3, sequentially. Once F0,0 is complete, GPU 1 can immediately start processing F1,0 and so on. After completing these forward passes, GPU 0 waits for the other GPUs to finish their respective forward computations before starting the backward paths (B0,3, B0,2, B0,1, and B0,0).\n\nPipeline parallelism is implemented in distributed training frameworks like Megatron-LM, DeepSpeed (ZeRO), and PyTorch through the dedicated Pipeline Parallelism for PyTorch (PiPPy) library. At the time of writing, only certain inference frameworks like TensorRT-LLM support pipeline parallelism.\n\nOceanofPDF.com\n\nTensor parallelism\n\nIntroduced by Shoeby, Patwary, Puri et al. in the Megatron-LM paper (2019), tensor parallelism (TP) is another popular technique to distribute the computation of LLM layers across multiple devices. In contrast to pipeline parallelism, TP splits the weight matrices found in individual layers. This enables simultaneous computations, significantly reducing memory bottlenecks and increasing processing speed.\n\nIn TP, large matrices, such as the weight matrices in MLPs or the attention heads in self-attention layers, are partitioned across several GPUs. Each GPU holds a portion of these matrices and performs computations on its respective slice.\n\nFigure 8.7 – Illustration of column-wise tensor parallelism in an MLP layer (W)\n\nFor instance, in an MLP layer, the weight matrix is divided so that each GPU processes only a subset of the weights (see Figure 8.7). The inputs are broadcast to all GPUs, which then independently compute their respective outputs. The partial results are then aggregated through an all-reduce operation, combining them to form the final output.\n\nIn the context of self-attention layers, TP is particularly efficient due to the inherent parallelism of attention heads. Each GPU can compute a subset of these heads independently, allowing the model to process large sequences more effectively. This makes TP more efficient than pipeline parallelism, which requires waiting for the completion of previous layers.\n\nDespite its advantages, TP is not universally applicable to all layers of a neural network. Layers like LayerNorm and Dropout, which have dependencies spanning the entire input, cannot be efficiently partitioned and are typically replicated across devices instead. However, these operations can be split on the sequence dimension of the input instead (sequence parallelism). Different GPUs can compute these layers on different slices of the input sequence, avoiding replication of weights. This technique is limited to a few specific layers, but it can provide additional memory savings, especially for very large input sequence lengths.\n\nMoreover, TP necessitates high-speed interconnects between devices to minimize communication overhead, making it impractical to implement across nodes with insufficient interconnect bandwidth.\n\nTP is also implemented in distributed training frameworks like Megatron- LM, DeepSpeed (ZeRO), and PyTorch (FSDP). It is available in most inference frameworks, like TGI, vLLM, and TensorRT-LLM.\n\nOceanofPDF.com\n\nCombining approaches\n\nData, tensor, and pipeline parallelisms are orthogonal techniques that can be combined. Figure 8.8 illustrates how a given model can be split according to each approach:\n\nFigure 8.8 – Illustration of the different model parallelism techniques\n\nCombining these techniques can mitigate their respective issues. Pipeline parallelism provides the greatest memory reduction but sacrifices efficiency, due to pipeline bubbles. This may be ideal if the primary constraint fits the model in the GPU memory. In contrast, if low latency is paramount, then prioritizing tensor parallelism and accepting a larger memory footprint may be the better trade-off. In practice, a model may be split depth-wise into a few pipeline stages, with tensor parallelism used within each stage.\n\nBalancing these tradeoffs and mapping a given model architecture onto available hardware accelerators is a key challenge in deploying LLMs.\n\nOceanofPDF.com\n\nModel quantization\n\nQuantization refers to the process of representing the weights and activations of a neural network using lower-precision data types. In the context of LLMs, quantization primarily focuses on reducing the precision of the model’s weights and activations.\n\nBy default, weights are typically stored in a 16-bit or 32-bit floating-point format (FP16 or FP32), which provides high precision but comes at the cost of increased memory usage and computational complexity. Quantization is a solution to reduce the memory footprint and accelerate the inference of LLMs.\n\nIn addition to these benefits, larger models with over 30 billion parameters can outperform smaller models (7B–13B LLMs) in terms of quality when quantized to 2- or 3-bit precision. This means they can achieve superior performance while maintaining a comparable memory footprint.\n\nIn this section, we will introduce the concepts of quantization, GGUF with\n\nllama.cpp\n\n, GPTQ, and EXL2, along with an overview of additional techniques. In addition to the code provided in this section, you can refer to AutoQuant (bit.ly/autoquant) to quantize their models using a Google Colab notebook.\n\nOceanofPDF.com\n\nIntroduction to quantization\n\nThere are two main approaches to weight quantization: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ is a straightforward technique where the weights of a pre-trained model are directly converted to a lower precision format without any retraining. While PTQ is easy to implement, it may result in some performance degradation. Conversely, QAT performs quantization during the training or fine-tuning stage, allowing the model to adapt to the lower precision weights. QAT often yields better performance compared to PTQ but requires additional computational resources and representative training data.\n\nThe choice of data type plays a crucial role in quantization. Floating- point numbers, such as FP32, FP16 (half-precision), and BF16 (brain floating-point), are commonly used in deep learning. These formats allocate a fixed number of bits to represent the\n\nsign\n\n,\n\nexponent\n\n, and\n\nsignificand\n\n(mantissa) of a number.\n\nFigure 8.9 – Comparison the between FP32, FP16, and BF16 formats\n\nA sign of 0 represents a positive number, while 1 indicates a negative number. Conversely, the exponent controls the range that is represented (big or small). Finally, the significand controls the precision of the number (the number of digits). The formula used to convert these representations into real numbers is:\n\nThe data types shown in Figure 7.7 display different tradeoffs, as illustrated with different representations of ). FP32 uses 32 bits, ( providing high precision but also requiring more memory. Conversely, FP16 and BF16 use 16 bits, lowering the memory footprint at the cost of a lower precision. In general, neural networks prefer a bigger range than better precision, which is why BF16 is the most popular data type when the hardware supports it. For example, NVIDIA’s Ampere architecture (A100, A30, etc.) supports BF16, but previous generations like Turing (T4, T40, etc.) do not.\n\nHowever, we are not restricted to these three data types. Lower-precision data types, such as INT8 (8-bit integers), can be employed for quantization, further reducing the memory footprint. Naïve quantization techniques, such as absolute maximum (absmax) quantization and zero-point quantization, can be applied to convert\n\nFP32\n\n,\n\nFP16\n\n, or\n\nBF16\n\nweights to\n\nINT8\n\n, as illustrated in Figure 8.10:\n\nFigure 8.10 – Quantization of 0.1 in a [-3.0, 3.2] range with absmax quantization and zero-point quantization\n\nAbsmax quantization maps the original weights by dividing them by the absolute maximum value of\n\nto the range [-127, 127]\n\nand scaling them:\n\nFor example, if our absolute maximum value is 3.2 (see Figure 8.8), a\n\nweight of 0.1 would be quantized to the inverse operation:\n\n. To dequantize it, we do\n\nThis means that if we dequantize our weight, we obtain can see a rounding error of implement it as follows with the PyTorch library:\n\nin this example. In Python, we can\n\n. We\n\nimport\n\ntorch\n\ndef\n\nabsmax_quantize\n\n(\n\nX\n\n):\n\n# Calculate scale\n\nscale =\n\n127\n\n/ torch.\n\nmax\n\n(torch.\n\nabs\n\n(X))\n\n# Quantize\n\nX_quant = (scale * X).\n\nround\n\n()\n\nreturn\n\nX_quant.to(torch.int8)\n\nZero-point quantization, on the other hand, considers asymmetric input distributions and maps the weights introducing a zero-point offset:\n\nto the range [-128, 127] by\n\nWhere\n\nand\n\n.\n\nIf we take the same example with a weight of 0.1, we get a scale of\n\nand a zero-point value of\n\n. The\n\nweight of 0.1 would be quantized to\n\n, unlike the value of\n\nprovided by absmax.\n\nWe can easily get the dequantization by applying the inverse operation:\n\nIn Python, zero-point quantization can be implemented as follows:\n\ndef\n\nzeropoint_quantize\n\n(\n\nX\n\n):\n\n# Calculate value range (denominator)\n\nx_range = torch.\n\nmax\n\n(X) - torch.\n\nmin\n\n(X) x_range =\n\n1\n\nif\n\nx_range ==\n\n0\n\nelse\n\nx_range\n\n# Calculate scale\n\nscale =\n\n255\n\n/ x_range\n\n# Shift by zero-point\n\nzeropoint = (-scale * torch.\n\nmin\n\n(X) -\n\n128\n\n).\n\nround\n\n()\n\n# Scale and round the inputs\n\nX_quant = torch.clip((X * scale + zeropoint).\n\nround\n\n(), -\n\n128\n\n,\n\n127\n\n)\n\nreturn\n\nX_quant.to(torch.int8)\n\nHowever, naïve quantization methods have limitations, particularly when dealing with outlier features in LLMs. Outlier features are extreme weight values (about 0.1% of total values) that can significantly impact the quantization process, leading to reduced precision for other values.\n\nDiscarding these outliers is not feasible, as it would degrade a model’s performance. You can see an example of outliers in Figure 8.11:\n\nFigure 8.11 – Example of outliers in a weight matrix\n\nTo address the outlier problem, more advanced quantization techniques have been proposed. One notable example is\n\nLLM.int8()\n\n, introduced by Dettmers et al. (2022).\n\nLLM.int8()\n\nemploys a mixed-precision quantization scheme, where outlier features are processed using FP16, while the remaining values are quantized to INT8. This approach effectively reduces the memory footprint of LLMs by nearly 2x while minimizing performance degradation.\n\nLLM.int8()\n\nworks by performing matrix multiplication in three steps. First, it extracts columns containing outlier features from the input hidden states using a custom threshold. Second, it performs separate matrix multiplications for the outliers (in\n\nFP16\n\n) and non-outliers (in\n\nINT8\n\n) using vector-wise quantization. Finally, it dequantizes the non-outlier results and combines them with the outlier results to obtain the final output in FP16.\n\nThe effectiveness of\n\nLLM.int8()\n\nhas been demonstrated empirically, showing negligible performance degradation (<1%) compared to the original\n\nFP32\n\nmodels. However, it does introduce an additional computational overhead, resulting in around 20% slower inference for large models. Models can be directly loaded in 8-bit precision with the transformer library, using\n\nLLM.int8()\n\n, as follows:\n\nfrom\n\ntransformers\n\nimport\n\nAutoModelForCausalLM model_name =\n\n\"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\n\n\"\n\nauto\"\n\n, load_in_8bit=\n\nTrue\n\n)\n\nIntroduced by Dettmers et al. (2023), NF4 is a 4-bit precision format designed for QLoRA (discussed in Chapter 5). It is also integrated into the transformers library but requires the bitsandbytes library as a dependency. To load a model in NF4 (4-bit precision), you can use the\n\nload_in_4bit\n\nparameter, as follows:\n\nfrom\n\ntransformers\n\nimport\n\nAutoModelForCausalLM model_name =\n\n\"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\n\n\"auto\"\n\n, load_in_4bit=\n\nTrue\n\n)\n\nOceanofPDF.com\n\nQuantization with GGUF and llama.cpp\n\nThe llama.cpp project is an open-source C++ software library created by Georgi Gerganov, designed to perform inference with various LLMs. It is the most popular quantization technique, with many quantized models available on the Hugging Face Hub.\n\nCompared to other libraries that rely on hardware-specific closed-source libraries like CUDA, llama.cpp can run on a broader range of hardware. It has gained significant popularity, particularly among users without specialized hardware, as it can operate on CPUs and Android devices. Moreover, llama.cpp can also offload layers to the GPU, accelerating inference speed. It is compatible with different inference optimization techniques, such as FlashAttention-2 and speculative decoding.\n\nThis project features its own quantization format, GGUF, designed to simplify and speed up model loading. GGUF files store tensors and metadata, supporting various formats, from 1-bit to 8-bit precision. It follows a naming convention based on the number of bits used and specific variants, such as:\n\nIQ1_S\n\nand\n\nIQ1_M\n\n: 1-bit precision – very low quality\n\nIQ2_XXS/XS/S/M\n\nand\n\nQ2_K\n\n: 2-bit precision – generally low quality but IQ2 can be usable for large models\n\nIQ3_XXS/XS/S/M\n\nand\n\nQ3_K_S/M/L\n\n: 3-bit precision – low quality but usable for large models\n\nIQ4_XS/NL\n\nand\n\nQ4_K_S/M, Q4_0/1\n\n: 4-bit precision – good quality and usable for most models\n\nQ5_K_S/M\n\nand\n\nQ5_0/1\n\n: 5-bit precision – high quality\n\nQ6_K\n\n: 6-bit precision –very high quality\n\nQ8_0\n\n: 8-bit precision – highest quality\n\nTo provide a brief overview of GGUF quantization, llama.cpp groups values into blocks and rounds them to a lower precision. For instance, the legacy Q4_0 format handles 32 values per block, scaling and quantizing them based on the largest weight value in the block ( In Q4_1, the smallest Lvalue in the block is also added (\n\n). In Q4_K, weights are divided into\n\nsuper-blocks, containing 8 blocks with 32 values. Block scales and minimum values are also quantized in higher precision with 6 bits (\n\n). Finally, i-quants like IQ4_XS are inspired by another quantization technique called QuIP#. This ensures an even number of positive (or negative) quant signs in groups of eight and implements the store their magnitude.\n\nlattice to\n\nHere is a practical example of how to quantize a model in the GGUF format. The following steps can be executed on a free T4 GPU in Google Colab:\n\n).\n\nInstall llama.cpp and the required libraries:\n\n!git clone https://github.com/ggerganov/llama.cpp !cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make !pip install -r llama.cpp/requirements.txt\n\nDownload the model to convert. We will provide the model ID from the Hugging Face Hub – for example,\n\nmistralai/Mistral-7B-Instruct-v0.2\n\n:\n\nMODEL_ID = \"mlabonne/EvolCodeLlama-7b\" MODEL_NAME = MODEL_ID.split('/')[-1] !git lfs install !git clone https://huggingface.co/{MODEL_ID}\n\nFirst, we convert the model into FP16. This is an intermediary artifact that will be used for every GGUF quantization type. Note that different\n\nconversion scripts exist in llama.cpp and are compatible with different models:\n\nfp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\" !python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n\nWe select a format (here,\n\nQ4_K_M\n\n) and start the quantization. This process can take an hour on a T4 GPU:\n\nMETHOD = \"q4_k_m\" qtype = f\" {MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\" !./llama.cpp/quantize {fp16} {qtype} {METHOD}\n\nOnce it’s done, your quantized model is ready. You can download it locally, or upload it to the Hugging Face Hub using the following code:\n\nfrom\n\nhuggingface_hub\n\nimport\n\ncreate_repo, HfApi hf_token =\n\n\"\"\n\n# Specify your token\n\nusername =\n\n\"\"\n\n# Specify your username\n\napi = HfApi()\n\n# Create empty repo\n\ncreate_repo( repo_id =\n\nf\"\n\n{username}\n\n/\n\n{MODEL_NAME}\n\nGGUF\"\n\n, repo_type=\n\n\"model\"\n\n, exist_ok=\n\nTrue\n\n, token=hf_token )\n\n# Upload gguf files\n\napi.upload_folder( folder_path=MODEL_NAME, repo_id=\n\nf\"\n\n{username}\n\n/\n\n{MODEL_NAME}\n\nGGUF\"\n\n, allow_patterns=\n\nf\"*.gguf\"\n\n, token=hf_token )\n\nGGUF models can be used with backends such as llama-cpp-python and frameworks like LangChain. This is useful if you want to integrate a quantized model into a broader system. You can also directly chat with the model using frontends, like llama.cpp’s lightweight server, LM Studio, and the Text Generation Web UI. These tools enable easy interaction with the GGUF models, providing an experience similar to ChatGPT.\n\nOceanofPDF.com\n\nQuantization with GPTQ and EXL2\n\nWhile GGUF and llama.cpp offer CPU inference with GPU offloading, GPTQ and EXL2 are two quantization formats dedicated to GPUs. This makes them both faster than llama.cpp during inference. In particular, EXL2 offers the highest throughput with its dedicated library, ExLlamaV2.\n\nGPTQ and EXL2 quants are based on the GPTQ algorithm, introduced by Frantar et al. (2023). It optimizes weight quantization for LLMs by refining the Optimal Brain Quantization (OBQ) approach to handle extensive matrices efficiently. It begins with a Cholesky decomposition of the Hessian inverse, ensuring numerical stability. Instead of quantizing weights in a strict order, GPTQ processes them in batches, updating columns and associated blocks iteratively. This method leverages lazy batch updates, reducing computational redundancy and memory bottlenecks.\n\nWhile GPTQ is limited to 4-bit precision, EXL2 offers more flexibility with a highly customizable precision that can mix different quantization levels. This allows for precise bitrates between 2 and 8 bits per weight, such as\n\n2.3\n\n,\n\n3.5\n\n, or\n\n6.0\n\n. It can also apply multiple quantization levels to each linear layer, prioritizing more important weights with higher bit quantization. Parameters are selected automatically, by quantizing each matrix multiple times and choosing a combination that minimizes the quantization error while meeting a target bitrate. In practice, this allows 70B models to run on a single 24 GB GPU with 2.55-bit precision.\n\nThe inference itself is handled by the ExLlamaV2 library, which supports both the GPTQ and EXL2 models.\n\nIn the following example, let’s quantize a model in the EXL2 format using ExLlamaV2. These steps can be executed on a free T4 GPU in Google Colab:\n\nInstall the ExLlamaV2 library from source:\n\n!git clone https://github.com/turboderp/exllamav2 !pip install -e exllamav2\n\nWe download the model to quantize by cloning its repo from the Hugging Face Hub:\n\nMODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\" MODEL_NAME = MODEL_ID.split('/')[-1] !git lfs install !git clone https://huggingface.co/{MODEL_ID}\n\nDownload the calibration dataset used to measure the quantization error. In this case, we will use WikiText-103, a standard calibration dataset with high-quality articles from Wikipedia:\n\n!wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2 883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet\n\nQuantize the model at a given precision (for example, 4.5):\n\n!mkdir quant !python exllamav2/convert.py \\ -i {MODEL_NAME} \\ -o quant \\ -c wikitext-test.parquet \\ -b 4.5\n\nThe quantized model can then be uploaded to the Hugging Face Hub, as seen previously.\n\nGPTQ and EXL2 quants are not as widely supported as GGUF. For example, frontends like LM Studio do not currently integrate them. You can use other tools instead, like oobabooga’s Text Generation Web UI. It is also directly integrated into the transformers library and supported by TGI. GPTQ models are also supported in TensorRT-LLM.\n\nWhile less popular than GGUF, you can find a lot of GPTQ and EXL2 models on the Hugging Face Hub.\n\nOceanofPDF.com\n\nOther quantization techniques\n\nThere is a variety of quantization techniques beyond GGUF, GPTQ, and EXL2. This subsection will briefly introduce Activate-aware Weight Quantization (AWQ) as well as extreme quantization techniques, like QuIP# (Quantization with Incoherence Processing) and HQQ (Half-Quadratic Quantization).\n\nIntroduced by Lin et al. (2023), AWQ is another popular quantization algorithm. It identifies and protects the most important weights, which are determined based on activation magnitude instead of weight magnitude. This approach involves applying optimal per-channel scaling to these salient weights, without relying on backpropagation or reconstruction, ensuring that the LLM does not overfit the calibration set. While it relies on a different paradigm, AWQ is quite close to the GPTQ and EXL2 versions, although slightly slower. They are well-supported by inference engines and integrated into TGI, vLLM, and TensorRT-LLM.\n\nAn interesting trend is the quantization of models into 1- or 2-bit precision. While some formats, like EXL2, allow extreme quantization, the quality of the models often suffers significantly. However, recent algorithms like QuIP# and HQQ have targeted this regime and offer quantization methods that better preserve the performance of the original models. This is particularly true for large models (over 30B parameters), which can end up taking less space than 7B or 13B parameter models while providing higher- quality outputs.\n\nThis trend is expected to continue, further optimizing these quantization methods.\n\nTo conclude this chapter, here is a table summarizing the features of the three main inference engines we covered in the previous sections:\n\nTechnique\n\nTGI\n\nvLLM\n\nTensorRT-LLM\n\nContinuous batching ✓\n\n✓\n\n✓\n\nSpeculative decoding ✓\n\nFlashAttention2\n\n✓\n\n✓\n\n✓\n\nPagedAttention\n\n✓\n\n✓\n\n✓\n\nPipeline parallelism\n\n✓\n\nTensor parallelism\n\n✓\n\n✓\n\n✓\n\nGPTQ\n\n✓\n\n✓\n\nEXL2\n\n✓\n\nAWQ\n\n✓\n\n✓\n\n✓\n\nTable 8.1 – Summary of features for TGI, vLLM, and TensorRT-LLM\n\nOceanofPDF.com\n\nSummary\n\nIn summary, inference optimization is a critical aspect of deploying LLMs effectively. This chapter explored various optimization techniques, including optimized generation methods, model parallelism, and weight quantization. Significant speedups can be achieved by leveraging techniques like predicting multiple tokens in parallel with speculative decoding, or using an optimized attention mechanism with FlashAttention- 2. Additionally, we discussed how model parallelism methods, including data, pipeline, and tensor parallelism, distribute the computational load across multiple GPUs to increase throughput and reduce latency. Weight quantization, with formats like GGUF and EXL2, further reduces the memory footprint and accelerates inference, with some calculated tradeoff in output quality.\n\nUnderstanding and applying these optimization strategies are essential for achieving high performance in practical applications of LLMs, such as chatbots and code completion. The choice of techniques and tools depends on specific requirements, including available hardware, desired latency, and throughput. By combining various approaches, such as continuous batching and speculative decoding, along with advanced attention mechanisms and model parallelism, users can tailor their deployment strategies to maximize efficiency.\n\nWay back in Chapter 4, we focused only on implementing the ingestion pipeline, which is just one component of a standard RAG application. In the next chapter, we will conclude the RAG system by implementing the retrieval and generation components and integrating them into the inference pipeline.\n\nOceanofPDF.com\n\nReferences\n\nHugging Face, Text Generation Inference, https://github.com/huggingface/text-generation-inference, 2022.\n\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Efficient Memory Management for Large Language Model Serving with PagedAttention, 2023.\n\nNvidia, TensorRT-LLM, https://github.com/NVIDIA/TensorRT-LLM, 2023.\n\nY. Leviathan, M. Kalman, Y. Matias, Fast Inference from Transformers via Speculative Decoding, 2023.\n\nT. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, T. Dao, Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads, 2024.\n\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Efficient Memory Management for Large Language Model Serving with PagedAttention, 2023.\n\nR.Y. Aminabadi, S. Rajbhandari, M. Zhang, A.A. Awan, C. Li, D. Li, E. Zheng, J. Rasley, S. Smith, O. Ruwase, Y. He, DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale, 2022.\n\nY. Huang, Y. Cheng, A. Bapna, O. Firat, M.X. Chen, D. Chen, H. Lee, J. Ngiam, Q.V. Le, Y. Wu, Z. Chen, GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism, 2019.\n\nK. James Reed, PiPPy: Pipeline Parallelism for PyTorch, https://github.com/pytorch/PiPPy, 2022.\n\nM. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism, 2020.\n\nVerma and Vaidya, Mastering LLM Techniques: Inference Optimization, NVIDIA Developer Technical Blog, https://developer.nvidia.com/blog/mastering-llm-techniques-inference- optimization/, 2023.\n\nT. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale, 2022.\n\nG. Gerganov, llama.cpp, https://github.com/ggerganov/llama.cpp, 2023.\n\nE. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, GPTQ: Accurate Post- Training Quantization for Generative Pre-trained Transformers, 2023.\n\nTuboderp, exllamav2, https://github.com/turboderp/exllamav2, 2023.\n\nJ. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, S. Han, AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, 2024.\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\n9\n\nOceanofPDF.com\n\nRAG Inference Pipeline\n\nBack in Chapter 4, we implemented the retrieval-augmented generation (RAG) feature pipeline to populate the vector database (DB). Within the feature pipeline, we gathered data from the data warehouse, cleaned, chunked, and embedded the documents, and, ultimately, loaded them to the vector DB. Thus, at this point, the vector DB is filled with documents and ready to be used for RAG.\n\nBased on the RAG methodology, you can split your software architecture into three modules: one for retrieval, one to augment the prompt, and one to generate the answer. We will follow a similar pattern by implementing a retrieval module to query the vector DB. Within this module, we will implement advanced RAG techniques to optimize the search. Afterward, we won’t dedicate a whole module to augmenting the prompt, as that would be overengineering, which we try to avoid. However, we will write an inference service that inputs the user query and context, builds the prompt, and calls the LLM to generate the answer. To summarize, we will implement two core Python modules, one for retrieval and one for calling the LLM using the user’s input and context as input. When we glue these together, we will have an end-to-end RAG flow.\n\nIn Chapters 5 and 6, we fine-tuned our LLM Twin model, and in Chapter 8, we learned how to optimize it for inference. Thus, at this point, the LLM is ready for production. What is left is to build and deploy the two modules described above.\n\nWe will dedicate the next chapter entirely to deploying our fine-tuned LLM Twin model to AWS SageMaker, as an AWS SageMaker inference endpoint. Thus, the focus of this chapter is to dig into the advanced RAG retrieval module implementation. We have dedicated a whole chapter to the retrieval step because this is where the magic happens in an RAG system. At the retrieval step (and not when calling the LLM), you write most of the RAG inference code. This step is where you have to wrangle your data to ensure that you retrieve the most relevant data points from the vector DB. Hence, most of the advanced RAG logic goes within the retrieval step.\n\nTo sum up, in this chapter, we will cover the following topics:\n\nUnderstanding the LLM Twin’s RAG inference pipeline\n\nExploring the LLM Twin’s advanced RAG techniques\n\nImplementing the LLM Twin’s RAG inference pipeline\n\nBy the end of this chapter, you will know how to implement an advanced RAG retrieval module, augment a prompt using the retrieved context, and call an LLM to generate the final answer. Ultimately, you will know how to build a production-ready RAG inference pipeline end to end.\n\nOceanofPDF.com\n\nUnderstanding the LLM Twin’s RAG inference pipeline\n\nBefore implementing the RAG inference pipeline, we want to discuss its software architecture and advanced RAG techniques. Figure 9.1 illustrates an overview of the RAG inference flow. The inference pipeline starts with the input query, retrieves the context using the retrieval module (based on the query), and calls the LLM SageMaker service to generate the final answer.\n\nFigure 9.1: RAG inference pipeline architecture\n\nThe feature pipeline and the retrieval module, defined in Figure 9.1, are independent processes. The feature pipeline runs on a different machine on a schedule to populate the vector DB. At the same time, the retrieval module is called on demand, within the inference pipeline, on every user request.\n\nBy separating concerns between the two components, the vector DB is always populated with the latest data, ensuring feature freshness, while the retrieval module can access the latest features on every request. The input of the RAG retrieval module is the user’s query, based on which we have to return the most relevant and similar data points from the vector DB, which will be used to guide the LLM in generating the final answer.\n\nTo fully understand the dynamics of the RAG inference pipeline, let’s go through the architecture flow from Figure 9.1 step by step:\n\nUser query:We begin with the user who makes a query, such as “Write an article about...”\n\nQuery expansion:We expand the initial query to generate multiple queries that reflect different aspects or interpretations of the original user query. Thus, instead of one query, we will use xN queries. By diversifying the search terms, the retrieval module increases the likelihood of capturing a comprehensive set of relevant data points. This step is crucial when the original query is too narrow or vague.\n\nSelf-querying: We extract useful metadata from the original query, such as the author’s name. The extracted metadata will be used as filters for the vector search operation, eliminating redundant data points from the query vector space (making the search more accurate and faster).\n\nFiltered vector search: We embed each query and perform a similarity search to find each search’s top K data points. We execute xN searches corresponding to the number of expanded queries. We call this step a filtered vector search as we leverage the metadata extracted from the self-query step as query filters.\n\nCollecting results:We get up to xK results closest to its specific expanded query interpretation for each search operation. Further, we aggregate the results of all the xN searches, ending up with a list of N x K results containing a mix of articles, posts, and repositories chunks. The results include a broader set of potentially relevant chunks, offering multiple relevant angles based on the original query’s different facets.\n\nReranking:To keep only the top K most relevant results from the list of N x K potential items, we must filter the list further. We will use a reranking algorithm that scores each chunk based on the relevance and importance relative to the initial user query.We will leverage a neural cross-encoder model to compute the score, a value between 0 and 1, where 1 means the result is entirely relevant to the query. Ultimately, we sort the N x K results based on the score and pick the top K items. Thus, the output is a ranked list of K chunks, with the most relevant data points situated at the top.\n\nBuild the prompt and call the LLM:We map the final list of the most relevant K chunks to a string used to build the final prompt. We create\n\nthe prompt using a prompt template, the retrieved context, and the user’s query. Ultimately, the augmented prompt is sent to the LLM (hosted on AWS SageMaker exposed as an API endpoint).\n\nAnswer: We are waiting for the answer to be generated. After the LLM processes the prompt, the RAG logic finishes by sending the generated response to the user.\n\nThat wraps up the overview of the RAG inference pipeline. Now, let’s dig deeper into the details.\n\nOceanofPDF.com\n\nExploring the LLM Twin’s advanced RAG techniques\n\nNow that we understand the overall flow of our RAG inference pipeline, let’s explore the advanced RAG techniques we used in our retrieval module:\n\nPre-retrieval step: Query expansion and self-querying\n\nRetrieval step: Filtered vector search\n\nPost-retrieval step: Reranking\n\nBefore digging into each method individually, let’s lay down the Python interfaces we will use in this section, which are available at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/rag/base.py.\n\nThe first is a prompt template factory that standardizes how we instantiate prompt templates. As an interface, it inherits from\n\nABC\n\nand exposes the\n\ncreate_template()\n\nmethod, which returns a LangChain\n\nPromptTemplate\n\ninstance. Even if we avoid being heavily reliant on LangChain, as we want to implement everything ourselves to understand the engineering behind the scenes, some objects, such as the\n\nPromptTemplate\n\nclass, are helpful to speed up the development without hiding too much functionality:\n\nfrom\n\nabc\n\nimport\n\nABC, abstractmethod\n\nfrom\n\nlangchain.prompts\n\nimport\n\nPromptTemplate\n\nfrom\n\npydantic\n\nimport\n\nBaseModel\n\nclass\n\nPromptTemplateFactory\n\n(ABC, BaseModel):\n\n@abstractmethod\n\ndef\n\ncreate_template\n\n(\n\nself\n\n) -> PromptTemplate:\n\npass\n\nWe also want to define a\n\nRAGStep\n\ninterface used to standardize the interface of advanced RAG steps such as query expansion and self-querying. As these steps are often dependent on other LLMs, it has a\n\nmock\n\nattribute to reduce costs and debugging time during development:\n\nfrom\n\ntyping\n\nimport\n\nAny\n\nfrom\n\nllm_engineering.domain.queries\n\nimport\n\nQuery\n\nclass\n\nRAGStep\n\n(\n\nABC\n\n):\n\ndef\n\n__init__\n\n(\n\nself, mock:\n\nbool\n\n=\n\nFalse\n\n) ->\n\nNone\n\n: self._mock = mock\n\n@abstractmethod\n\ndef\n\ngenerate\n\n(\n\nself, query: Query, *args, **kwargs\n\n) ->\n\nAny\n\n:\n\npass\n\nUltimately, we must understand how we modeled the\n\nQuery\n\ndomain entity to wrap the user’s input with other metadata required for advanced RAG. Thus, let’s look at its implementation. First, we import the necessary classes:\n\nfrom\n\npydantic\n\nimport\n\nUUID4, Field\n\nfrom\n\nllm_engineering.domain.base\n\nimport\n\nVectorBaseDocument\n\nfrom\n\nllm_engineering.domain.types\n\nimport\n\nDataCategory\n\nNext, we define the\n\nQuery\n\nentity class, which inherits from the\n\nVectorBaseDocument\n\nobject-vector mapping (OVM) class, discussed in Chapter 4. Thus, each query can easily be saved or retrieved from the vector DB:\n\nclass\n\nQuery\n\n(\n\nVectorBaseDocument\n\n): content:\n\nstr\n\nauthor_id: UUID4 |\n\nNone\n\n=\n\nNone\n\nauthor_full_name:\n\nstr\n\n|\n\nNone\n\n=\n\nNone\n\nmetadata:\n\ndict\n\n= Field(default_factory=\n\ndict\n\n)\n\nclass\n\nConfig\n\n: category = DataCategory.QUERIES\n\nWhat is essential to notice are the class’s attributes used to combine the user’s query with a bunch of metadata fields:\n\ncontent\n\n: A string containing input query.\n\nauthor_id\n\n: An optional UUID4 identifier extracted from the query used as a filter within the vector search operation to retrieve chunks written only by a specific author\n\nauthor_full_name\n\n: An optional string used to query the\n\nauthor_id\n\nmetadata\n\n: A dictionary for any additional metadata, initialized as an empty\n\ndict\n\nby default\n\nBesides the standard definition of a domain class, we also define a\n\nfrom_str()\n\nclass method to create a\n\nQuery\n\ninstance directly from a string. This allows us to standardize how we clean the query string before constructing the\n\nquery\n\nobject, such as stripping any leading or trailing whitespace and newline characters:\n\n@classmethod\n\ndef\n\nfrom_str\n\n(\n\ncls, query:\n\nstr\n\n) ->\n\n\"Query\"\n\n:\n\nreturn\n\nQuery(content=query.strip(\n\n\"\\n \"\n\n))\n\nAdditionally, there’s an instance method called\n\nreplace_content()\n\nused to create a new\n\nQuery\n\ninstance with updated content while retaining the original query’s\n\nid\n\n,\n\nauthor_id\n\n,\n\nauthor_full_name\n\n, and\n\nmetadata\n\n:\n\ndef\n\nreplace_content\n\n(\n\nself, new_content:\n\nstr\n\n) ->\n\n\"Query\"\n\n:\n\nreturn\n\nQuery(\n\nid\n\n=self.\n\nid\n\n, content=new_content, author_id=self.author_id, author_full_name=self.author_full_name, metadata=self.metadata, )\n\nThis can be particularly useful when modifying the query text, for example, during preprocessing or normalization, without losing the associated metadata or identifiers. Following the\n\nQuery\n\nclass, we define the\n\nEmbeddedQuery\n\nclass:\n\nclass\n\nEmbeddedQuery\n\n(\n\nQuery\n\n): embedding:\n\nlist\n\n[\n\nfloat\n\n]\n\nclass\n\nConfig\n\n: category = DataCategory.QUERIES\n\nThe\n\nEmbeddedQuery\n\nclass extends\n\nQuery\n\nby adding the embedding field. The\n\nEmbeddedQuery\n\nentity encapsulates all the data and metadata necessary to perform vector search operations on top of Qdrant (or another vector DB).\n\nNow that we understand all the interfaces and new domain entities used within the RAG inference pipeline, let’s move on to our advanced RAG pre-retrieval optimization techniques.\n\nOceanofPDF.com\n\nAdvanced RAG pre-retrieval optimizations: query expansion and self-querying\n\nWe implemented two methods to optimize the pre-retrieval optimization step: query expansion and self-querying. The two methods work closely with the filtered vector search step, which we will touch on in the next section. For now, however, we will start with understanding the code for query expansion and move to implementing self-querying.\n\nWithin these two methods, we will leverage OpenAI’s API to generate variations of the original query within the query expansion step and to extract the necessary metadata within the self-querying algorithm. When we wrote this book, we used\n\nGPT-4o-mini\n\nin all our examples, but as OpenAI’s models quickly evolve, the model might get deprecated. But that’s not an issue, as you can quickly change it in your\n\n.env\n\nfile by configuring the\n\nOPENAI_MODEL_ID\n\nenvironment variable.\n\nOceanofPDF.com\n\nQuery expansion\n\nThe problem in a typical retrieval step is that you query your vector DB using a single vector representation of your original question. This approach covers only a small area of the embedding space, which can be limiting. If the embedding doesn’t contain all the required information or nuances of your query, the retrieved context may not be relevant. This means essential documents that are semantically related but not near the query vector might be overlooked.\n\nThe solution is based on query expansion, which offers a way to overcome this limitation. Using an LLM to generate multiple queries based on your initial question, you create various perspectives that capture different facets of your query. These expanded queries, when embedded, target other areas of the embedding space that are still relevant to your original question. This increases the likelihood of retrieving more relevant documents from the vector DB.\n\nImplementing query expansion can be as straightforward as crafting a detailed zero-shot prompt to guide the LLM in generating these alternative queries. Thus, after implementing query expansion, instead of having only one query to search relevant context, you will have xN queries, hence xN searches.\n\nIncreasing the number of searches can impact your latency. Thus, you must experiment with the number of queries you generate to ensure the retrieval step meets your application requirements. You can also optimize the\n\nsearches by parallelizing them, drastically reducing the latency, which we will do in the\n\nContextRetriever\n\nclass implemented at the end of this chapter.\n\nQuery expansion is also known as multi-query, but the principles are the same. For example, this is an example of LangChain’s implementation called\n\nMultiQueryRetriver\n\n: https://python.langchain.com/docs/how_to/MultiQueryRetriever/\n\nNow, let’s dig into the code. We begin by importing the necessary modules and classes required for query expansion:\n\nfrom\n\nlangchain_openai\n\nimport\n\nChatOpenAI\n\nfrom\n\nllm_engineering.domain.queries\n\nimport\n\nQuery\n\nfrom\n\nllm_engineering.settings\n\nimport\n\nsettings\n\nfrom\n\n.base\n\nimport\n\nRAGStep\n\nfrom\n\n.prompt_templates\n\nimport\n\nQueryExpansionTemplate\n\nNext, we define the\n\nQueryExpansion\n\nclass, which generates expanded query versions. The class implementation can be found at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/rag/query_expanison.py:\n\nclass\n\nQueryExpansion\n\n(\n\nRAGStep\n\n):\n\ndef\n\ngenerate\n\n(\n\nself, query: Query, expand_to_n:\n\nint\n\n) ->\n\nlist\n\n[Query]:\n\nassert\n\nexpand_to_n >\n\n0\n\n,\n\nf\"'expand_to_n' should be greater than 0. Got\n\n{expand_to_n}\n\n.\"\n\nif\n\nself._mock:\n\nreturn\n\n[query\n\nfor\n\n_\n\nin\n\nrange\n\n(expand_to_n)]\n\nIn the\n\ngenerate\n\nmethod, we first ensure that the number of expansions requested (\n\nexpand_to_n\n\n) is greater than zero. If the instance is in mock mode (\n\nself._mock is True\n\n), it simply returns a list containing copies of the original query to simulate expansion without actually calling the API. If not in mock mode, we proceed to create the prompt and initialize the language model:\n\nquery_expansion_template = QueryExpansionTemplate() prompt = query_expansion_template.create_template(expand_to_n -\n\n1\n\n) model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_key=settings.OPENAI_API_KEY, temperature=\n\n0\n\n)\n\nHere, we instantiate\n\nQueryExpansionTemplate\n\nand create a prompt tailored to generate\n\nexpand_to_n - 1\n\nnew queries (excluding the original). We initialize the\n\nChatOpenAI\n\nmodel with the specified settings and set the temperature to 0 for deterministic output. We then create a LangChain chain by combining the prompt with the model and invoke it with the user’s question:\n\nchain = prompt | model response = chain.invoke({\n\n\"question\"\n\n: query}) result = response.content\n\nBy piping the prompt into the model (\n\nprompt | model\n\n), we set up a chain that generates expanded queries when invoked with the original query. The response from the model is captured in the\n\nresult\n\nobject. After receiving the response, we parse and clean the expanded queries:\n\nqueries_content = result.strip().split(query_expansion_template.separator) queries = [query] queries += [ query.replace_content(stripped_content)\n\nfor\n\ncontent\n\nin\n\nqueries_content\n\nif\n\n(stripped_content := content.strip()) ]\n\nreturn\n\nqueries\n\nWe split the result using the separator defined in the template to get individual queries. Starting with a list containing the original query, we append each expanded query after stripping any extra whitespace.\n\nFinally, we define the\n\nQueryExpansionTemplate\n\nclass, which constructs the prompt used for query expansion. The class and other prompt templates can be accessed at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/rag/prompt_templates.py :\n\nfrom\n\nlangchain.prompts\n\nimport\n\nPromptTemplate\n\nfrom\n\n.base\n\nimport\n\nPromptTemplateFactory\n\nclass\n\nQueryExpansionTemplate\n\n(\n\nPromptTemplateFactory\n\n): prompt:\n\nstr\n\n=\n\n\"\"\"You are an AI language model assistant. Your task is to generate {expand_to_n}\n\ndifferent versions of the given user question to retrieve relevant documents from a vector\n\ndatabase. By generating multiple perspectives on the user question, your goal is to help\n\nthe user overcome some of the limitations of the distance-based similarity search.\n\nProvide these alternative questions separated by '{separator}'.\n\nOriginal question: {question}\"\"\"\n\n@property\n\ndef\n\nseparator\n\n(\n\nself\n\n) ->\n\nstr\n\n:\n\nreturn\n\n\"#next-question#\"\n\ndef\n\ncreate_template\n\n(\n\nself, expand_to_n:\n\nint\n\n) -> PromptTemplate:\n\nreturn\n\nPromptTemplate( template=self.prompt, input_variables=[\n\n\"question\"\n\n], partial_variables={\n\n\"separator\"\n\n: self.separator,\n\n\"expand_to_n\"\n\n: expand_to_n, }, )\n\nThis class defines a prompt instructing the language model to generate multiple versions of the user’s question. It uses placeholders like\n\n{expand_to_n}\n\n,\n\n{separator}\n\n, and\n\n{question}\n\nto customize the prompt.\n\nIt takes\n\nexpand_to_n\n\nas an input parameter to define how many queries we wish to generate while we build the\n\nPromptTemplate\n\ninstance. The separator property provides a unique string to split the generated queries. The\n\nexpand_to_n\n\nand\n\nseparator\n\nvariables are passed as\n\npartial_variables\n\n, making them immutable at runtime. Meanwhile, the\n\n{question}\n\nplaceholder will be changed every time the LLM chain is called.\n\nNow that we have finished studying the query expansion implementation, let’s look at an example of how to use the\n\nQueryExpansion\n\nclass. Let’s run the following code using this\n\npython -m llm_engineering.application.rag.query_expansion\n\ncommand:\n\nquery = Query.from_str(\n\n\"\n\nWrite an article about the best types of advanced RAG methods.\"\n\n) query_expander = QueryExpansion() expanded_queries = query_expander.generate(query, expand_to_n=\n\n3\n\n)\n\nfor\n\nexpanded_query\n\nin\n\nexpanded_queries: logger.info(expanded_query.content)\n\nWe get the following variations of the original query. As you can observe, the query expansion method was successful in providing more details and different perspectives of the initial query, such as highlighting the effectiveness of advanced RAG methods or the overview of these methods (remember that the first query is the original one):\n\n2024-09-18 17:51:33.529 | INFO - Write an article about the best types of advanced RAG methods. 2024-09-18 17:51:33.529 | INFO - What are the most effective advanced RAG methods, and how can they be applied? 2024-09-18 17:51:33.529 | INFO - Can you provide an overview of the top advanced retrieval-augmented generation techniques?\n\nNow, let’s move to the next pre-retrieval optimization method: self- querying.\n\nOceanofPDF.com\n\nSelf-querying\n\nThe problem when embedding your query into a vector space is that you cannot guarantee that all the aspects required by your use case are present with enough signal in the embedding vector. For example, you want to be 100% sure that your retrieval depends on the tags provided in the user’s input. Unfortunately, you can’t control the signal left within the embedding that emphasizes the tag. By embedding the query prompt alone, you can never be sure that the tags are sufficiently represented in the embedding vector or have enough signal when computing the distance against other vectors.\n\nThis problem stands for any other metadata you want to present during the search, such as IDs, names, or categories.\n\nThe solution is to use self-querying to extract the tags or other critical metadata within the query and use them alongside the vector search as filters. Self-querying uses an LLM to extract various metadata fields crucial for your business use case, such as tags, IDs, number of comments, likes, shares, etc. Afterward, you have complete control over how the extracted metadata is considered during retrieval. In our LLM Twin use case, we extract the author’s name and use it as a filter. Self-queries work hand-in- hand with filtered vector searches, which we will explain in the next section.\n\nNow, let’s move on to the code. We begin by importing the necessary modules and classes on which our code relies:\n\nfrom\n\nlangchain_openai\n\nimport\n\nChatOpenAI\n\nfrom\n\nllm_engineering.application\n\nimport\n\nutils\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nUserDocument\n\nfrom\n\nllm_engineering.domain.queries\n\nimport\n\nQuery\n\nfrom\n\nllm_engineering.settings\n\nimport\n\nsettings\n\nfrom\n\n.base\n\nimport\n\nRAGStep\n\nfrom\n\n.prompt_templates\n\nimport\n\nSelfQueryTemplate\n\nNext, we define the\n\nSelfQuery\n\nclass, which inherits from\n\nRAGStep\n\nand implements the\n\ngenerate()\n\nmethod. The class can be found at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/rag/self_query.py:\n\nclass\n\nSelfQuery\n\n(\n\nRAGStep\n\n):\n\ndef\n\ngenerate\n\n(\n\nself, query: Query\n\n) -> Query:\n\nif\n\nself._mock:\n\nreturn\n\nquery\n\nIn the\n\ngenerate()\n\nmethod, we check if the\n\n_mock\n\nattribute is set to\n\nTrue\n\n. If it is, we will return the original query object unmodified. This allows us to bypass calling the model while testing and debugging. If not in mock mode, we create the prompt template and initialize the language model.\n\nprompt = SelfQueryTemplate().create_template() model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_key=settings.OPENAI_API_KEY, temperature=\n\n0\n\n)\n\nHere, we instantiate the prompt using the\n\nSelfQueryTemplate\n\nfactory class and create a\n\nChatOpenAI\n\nmodel instance (similar to the query expansion implementation). We then combine the prompt and the model into a chain and invoke it with the user’s query.\n\nchain = prompt | model response = chain.invoke({\n\n\"question\"\n\n: query}) user_full_name = response.content.strip(\n\n\"\\n \"\n\n)\n\nWe extract the content from the LLM response and strip any leading or trailing whitespace to obtain the\n\nuser_full_name\n\nvalue. Next, we check if the model was able to extract any user information.\n\nif\n\nuser_full_name ==\n\n\"\n\nnone\"\n\n:\n\nreturn\n\nquery\n\nIf the response is\n\n\"none\"\n\n, it means no user name was found in the query, so we return the original query object. If a user name is found, we will split the\n\nuser_full_name\n\ninto the\n\nfirst_name\n\nand\n\nlast_name\n\nvariables using a utility function. Then, based on the user’s details, we retrieve or create a\n\nUserDocument\n\nuser instance:\n\nfirst_name, last_name = utils.split_user_full_name(user_full_name) user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)\n\nFinally, we update the query object with the extracted author information and return it:\n\nquery.author_id = user.\n\nid\n\nquery.author_full_name = user.full_name\n\nreturn\n\nquery\n\nThe updated query now contains the\n\nauthor_id\n\nand\n\nauthor_full_name\n\nvalues, which can be used in subsequent steps of the RAG pipeline.\n\nLet’s look at the\n\nSelfQueryTemplate\n\nclass, which defines the prompt to extract user information:\n\nfrom\n\nlangchain.prompts\n\nimport\n\nPromptTemplate\n\nfrom\n\n.base\n\nimport\n\nPromptTemplateFactory\n\nclass\n\nSelfQueryTemplate\n\n(\n\nPromptTemplateFactory\n\n): prompt:\n\nstr\n\n=\n\n\"\"\"You are an AI language model assistant. Your task is to extract information from a user question.\n\nThe required information that needs to be extracted is the user name or user id.\n\nYour response should consist of only the extracted user name (e.g., John Doe) or id (e.g. 1345256), nothing else.\n\nIf the user question does not contain any user name or id, you should return the following token: none.\n\nFor example:\n\nQUESTION 1:\n\nMy name is Paul Iusztin and I want a post about...\n\nRESPONSE 1:\n\nPaul Iusztin\n\nQUESTION 2:\n\nI want to write a post about...\n\nRESPONSE 2:\n\nnone\n\nQUESTION 3:\n\nMy user id is 1345256 and I want to write a post about...\n\nRESPONSE 3:\n\n1345256\n\nUser question: {question}\"\"\"\n\ndef\n\ncreate_template\n\n(\n\nself\n\n) -> PromptTemplate:\n\nreturn\n\nPromptTemplate(template=self.prompt, input_variables=[\n\n\"question\"\n\n])\n\nIn the\n\nSelfQueryTemplate\n\nclass, we define a prompt instructing the AI model to extract the user name or ID from the input question. The prompt uses few-shot learning to guide the model on how to respond in different scenarios. When the template is invoked, the\n\n{question}\n\nplaceholder will be replaced with the actual user question.\n\nBy implementing self-querying, we ensure that critical metadata required for our use case is explicitly extracted and used during retrieval. This\n\napproach overcomes the limitations of relying solely on the semantics of the embeddings to capture all necessary aspects of a query.\n\nNow that we’ve implemented the\n\nSelfQuery\n\nclass, let’s provide an example. Run the following code using the\n\npython -m llm_engineering.application.rag.self_query\n\nCLI command:\n\nquery = Query.from_str(\n\n\"I am Paul Iusztin. Write an article about the best types of advanced RAG methods.\"\n\n) self_query = SelfQuery() query = self_query.generate(query) logger.info(\n\nf\"Extracted author_id:\n\n{query.author_id}\n\n\"\n\n) logger.info(\n\nf\"Extracted author_full_name:\n\n{query.author_full_name}\n\n\"\n\n)\n\nWe get the following results where the author’s full name and ID were extracted correctly:\n\n2024-09-18 18:02:10.362 | INFO - Extracted author_id: 900fec95-d621- 4315-84c6-52e5229e0b96 2024-09-18 18:02:10.362 | INFO - Extracted author_full_name: Paul Iusztin\n\nNow that we understand how self-querying works, let’s explore how it can be used together with filtered vector search within the retrieval optimization step.\n\nOceanofPDF.com\n\nAdvanced RAG retrieval optimization: filtered vector search\n\nVector search is pivotal in retrieving relevant information based on semantic similarity. A plain vector search, however, can introduce significant challenges that affect both the accuracy and latency of information retrieval. This is primarily because it operates solely on the numerical proximity of vector embeddings without considering the contextual or categorical nuances that might be crucial for relevance.\n\nOne of the primary issues with plain vector search is retrieving semantically similar but contextually irrelevant documents. Since vector embeddings capture general semantic meanings, they might assign high similarity scores to content that shares language patterns or topics but doesn’t align with the specific intent or constraints of the query. For instance, searching for “Java” could retrieve documents about the programming language or the Indonesian island, depending solely on semantic similarity, leading to ambiguous or misleading results.\n\nMoreover, as the size of the dataset increases, plain vector search can suffer from scalability issues. The lack of filtering means the search algorithm has to compute similarities across the entire vector space, which can significantly increase latency.\n\nThis exhaustive search slows response times and consumes more computational resources, making it inefficient for real-time or large-scale applications.\n\nFiltered vector search emerges as a solution by filtering after additional criteria, such as metadata tags or categories, reducing the search space before computing vector similarities. By applying these filters, the search algorithm narrows the pool of potential results to those contextually aligned with the query’s intent. This targeted approach enhances accuracy by eliminating irrelevant documents that might have otherwise been considered due to their semantic similarities alone.\n\nAdditionally, filtered vector search improves latency by reducing the number of comparisons the algorithm needs to perform. Working with a smaller, more relevant subset of data decreases the computational overhead, leading to faster response times. This efficiency is crucial for applications requiring real-time interactions or handling large queries.\n\nAs the metadata used within the filtered vector search is often part of the user’s input, we have to extract it before querying the vector DB. That’s precisely what we did during the self-query step, where we extracted the author’s name to reduce the vector space only to the author’s content. Thus, as we processed the query within the self-query step, it went into the pre- retrieval optimization category, whereas when the filtered vector search optimized the query, it went into the retrieval optimization bin.\n\nFor example, when using Qdrant, to add a filter that looks for a matching\n\nauthor_id\n\nwithin the metadata of each document, you must implement the following code:\n\nfrom\n\nqdrant_client.models\n\nimport\n\nFieldCondition, Filter, MatchValue records = qdrant_connection.search( collection_name=\n\n\"articles\"\n\n, query_vector=query_embedding, limit=\n\n3\n\n, with_payload=\n\nTrue\n\n, query_filter= Filter( must=[ FieldCondition( key=\n\n\"author_id\"\n\n,\n\nmatch\n\n=MatchValue( value=\n\nstr\n\n(\n\n\"1234\"\n\n), ), ) ] ), )\n\nIn essence, while plain vector search provides a foundation for semantic retrieval, its limitations can slow performance in practical applications. Filtered vector search addresses these challenges by combining the strengths of vector embeddings with contextual filtering, resulting in more accurate and efficient information retrieval in RAG systems. The last step for optimizing our RAG pipeline is to look into reranking.\n\nOceanofPDF.com\n\nAdvanced RAG post-retrieval optimization: reranking\n\nThe problem in RAG systems is that the retrieved context may contain irrelevant chunks that only:\n\nAdd noise: The retrieved context might be irrelevant, cluttering the information and potentially confusing the language model.\n\nMake the prompt bigger: Including unnecessary chunks increases the prompt size, leading to higher costs. Moreover, language models are usually biased toward the context’s first and last pieces. So, if you add a large amount of context, there’s a big chance it will miss the essence.\n\nBe come unaligned with your question: Chunks are retrieved based on the similarity between the query and chunk embeddings. The issue is that the embedding model might not be tuned to your question, resulting in high similarity scores for chunks that aren’t entirely relevant.\n\nThe solution is to use reranking to order all the N × K retrieved chunks based on their relevance relative to the initial question, where the first chunk will be the most relevant and the last the least. N represents the number of searches after query expansion, while K is the number of chunks\n\nretrieved per search. Hence, we retrieve a total of N x K chunks. In RAG systems, reranking serves as a critical post-retrieval step that refines the initial results obtained from the retrieval model.\n\nWe assess each chunk’s relevance to the original query by applying the reranking algorithm, which often uses advanced models like neural cross- encoders. These models evaluate the semantic similarity between the query and each chunk more accurately than initial retrieval methods based on embeddings and the cosine similarity distance, as explained in more detail in Chapter 4 in the An overview of advanced RAG section.\n\nUltimately, we pick the top K most relevant chunks from the sorted list of N x K items based on the reranking score. Reranking works well when combined with query expansion. First, let’s understand how reranking works without query expansion:\n\nSearch for > K chunks: Retrieve more than K chunks to have a broader pool of potentially relevant information.\n\nReorder using rerank: Apply reranking to this larger set to evaluate the actual relevance of each chunk relative to the query.\n\nTake top K: Select the top K chunks to use them as context in the final prompt.\n\nThus, when combined with query expansion, we gather potential valuable context from multiple points in space rather than just looking for more than K samples in a single location. Now the flow looks like this:\n\nSearch for N × K chunks: Retrieve multiple sets of chunks using the expanded queries.\n\nReorder using rerank: Rerank all the retrieved chunks based on their relevance.\n\nTake top K: Select the most relevant chunks for the final prompt.\n\nIntegrating reranking into the RAG pipeline enhances the quality and relevance of the retrieved context and efficiently uses computational resources. Let’s look at implementing the LLM Twin’s reranking step to understand what we described above, which can be accessed on GitHub at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/rag/reranking.py.\n\nWe begin by importing the necessary modules and classes for our reranking process:\n\nfrom\n\nllm_engineering.application.networks\n\nimport\n\nCrossEncoderModelSingleton\n\nfrom\n\nllm_engineering.domain.embedded_chunks\n\nimport\n\nEmbeddedChunk\n\nfrom\n\nllm_engineering.domain.queries\n\nimport\n\nQuery\n\nfrom\n\n.base\n\nimport\n\nRAGStep\n\nNext, we define the\n\nReranker\n\nclass, which is responsible for reranking the retrieved documents based on their relevance to the query:\n\nclass\n\nReranker\n\n(\n\nRAGStep\n\n):\n\ndef\n\n__init__\n\n(\n\nself, mock:\n\nbool\n\n=\n\nFalse\n\n) ->\n\nNone\n\n:\n\nsuper\n\n().__init__(mock=mock) self._model = CrossEncoderModelSingleton()\n\nIn the initializer of the Reranker class, we instantiate our cross-encoder model by creating an instance of\n\nCrossEncoderModelSingleton\n\n. This is the cross-encoder model used to score the relevance of each document chunk with respect to the query.\n\nThe core functionality of the\n\nReranker\n\nclass is implemented in the\n\ngenerate()\n\nmethod:\n\ndef\n\ngenerate\n\n(\n\nself, query: Query, chunks:\n\nlist\n\n[EmbeddedChunk], keep_top_k:\n\nint\n\n) ->\n\nlist\n\n[EmbeddedChunk]:\n\nif\n\nself._mock:\n\nreturn\n\nchunks query_doc_tuples = [(query.content, chunk.content)\n\nfor\n\nchunk\n\nin\n\nchunks] scores = self._model(query_doc_tuples) scored_query_doc_tuples =\n\nlist\n\n(\n\nzip\n\n(scores, chunks, strict=\n\nFalse\n\n)) scored_query_doc_tuples.sort(key=\n\nlambda\n\nx: x[\n\n0\n\n], reverse=\n\nTrue\n\n) reranked_documents = scored_query_doc_tuples[:keep_top_k] reranked_documents = [doc\n\nfor\n\n_, doc\n\nin\n\nreranked_documents]\n\nreturn\n\nreranked_documents\n\nThe\n\ngenerate()\n\nmethod takes a query, a list of chunks (document segments), and the number of top documents to keep (\n\nkeep_top_k\n\n). If we’re in mock mode, it simply returns the original chunks. Otherwise, it performs the following steps:\n\nCreates pairs of the query content and each chunk’s content\n\nUses the cross-encoder model to score each pair, assessing how well the chunk matches the query\n\nZips the scores with the corresponding chunks to create a scored list of tuples\n\nSorts this list in descending order based on the scores\n\nSelects the top\n\nkeep_top_k\n\nchunks\n\nExtracts the chunks from the tuples and returns them as the reranked documents\n\nBefore defining the\n\nCrossEncoder\n\nclass, we import the necessary components:\n\nfrom\n\nsentence_transformers.cross_encoder\n\nimport\n\nCrossEncoder\n\nfrom\n\n.base\n\nimport\n\nSingletonMeta\n\nWe import the\n\nCrossEncoder\n\nclass from the sentence_transformers library, which provides the functionality for scoring text pairs. We also import\n\nSingletonMeta\n\nfrom our base module to ensure our model class follows the singleton pattern, meaning only one instance of the model exists throughout the application. Now, we define the\n\nCrossEncoderModelSingleton\n\nclass:\n\nclass\n\nCrossEncoderModelSingleton\n\n(metaclass=SingletonMeta):\n\ndef\n\n__init__\n\n(\n\nself,\n\nmodel_id:\n\nstr\n\n= settings.RERANKING_CROSS_ENCODER_MODEL_ID,\n\ndevice:\n\nstr\n\n= settings.RAG_MODEL_DEVICE,\n\n) ->\n\nNone\n\n:\n\n\"\"\"\n\nA singleton class that provides a pre-trained cross-encoder model for scoring pairs of input text.\n\n\"\"\"\n\nself._model_id = model_id self._device = device self._model = CrossEncoder( model_name=self._model_id, device=self._device, ) self._model.model.\n\neval\n\n()\n\nThis class initializes the cross-encoder model using the specified\n\nmodel_id\n\nand\n\ndevice\n\nfrom the global\n\nsettings\n\nloaded from the\n\n.env\n\nfile. We set the model to evaluation mode using\n\nself._model.model.eval()\n\nto ensure the model is ready for inference.\n\nThe\n\nCrossEncoderModelSingleton\n\nclass includes a callable method to score text pairs:\n\ndef\n\n__call__\n\n(\n\nself, pairs:\n\nlist\n\n[\n\ntuple\n\n[\n\nstr\n\n,\n\nstr\n\n]], to_list:\n\nbool\n\n=\n\nTrue\n\n) -> NDArray[np.float32] |\n\nlist\n\n[\n\nfloat\n\n]: scores = self._model.predict(pairs)\n\nif\n\nto_list: scores = scores.tolist()\n\nreturn\n\nscores\n\nThe\n\n__call__\n\nmethod allows us to pass in a list of text\n\npairs\n\n(each consisting of the query and a document chunk) and receive their relevance scores. The method uses the model’s\n\npredict()\n\nfunction to call the model and compute the scores.\n\nThe\n\nCrossEncoderModelSingleton\n\nclass is a wrapper over the\n\nCrossEncoder\n\nclass, which we wrote for two purposes. The first one is for the singleton pattern, which allows us to easily access the same instance of the cross- encoder model from anywhere within the application without loading the model in memory every time we need it. The second reason is that by writing our wrapper, we defined our interface for a cross-encoder model (or any other model used for reranking). This makes the code future-proof as in case we need a different implementation or strategy for reranking, for example, using an API, we only have to write a different wrapper that follows the same interface and swap the old class with the new one. Thus, we can introduce new reranking methods without touching the rest of the code.\n\nWe now understand all the advanced RAG techniques used within our architecture. In the next section, we will examine the\n\nContextRetriever\n\nclass that connects all these methods and explain how to use the retrieval module with an LLM for an end-to-end RAG inference pipeline.\n\nOceanofPDF.com\n\nImplementing the LLM Twin’s RAG inference pipeline\n\nAs explained at the beginning of this chapter, the RAG inference pipeline can mainly be divided into three parts: the retrieval module, the prompt creation, and the answer generation, which boils down to calling an LLM with the augmented prompt. In this section, our primary focus will be implementing the retrieval module, where most of the code and logic go. Afterward, we will look at how to build the final prompt using the retrieved context and user query.\n\nUltimately, we will examine how to combine the retrieval module, prompt creation logic, and the LLM to capture an end-to-end RAG workflow. Unfortunately, we won’t be able to test out the LLM until we finish Chapter 10, as we haven’t deployed our fine-tuned LLM Twin module to AWS SageMaker.\n\nThus, by the end of this section, you will learn how to implement the RAG inference pipeline, which you can test out end to end only after finishing Chapter 10. Now, let’s start by looking at the implementation of the retrieval module.\n\nOceanofPDF.com\n\nImplementing the retrieval module\n\nLet’s dive into the\n\nContextRetriever\n\nclass implementation, which orchestrates the retrieval step in our RAG system by integrating all the advanced techniques we previously used: query expansion, self-querying, reranking, and filtered vector search. The class can be found on GitHub at https://github.com/PacktPublishing/LLM- Engineers- Handbook/blob/main/llm_engineering/application/rag/retriever.py.\n\nFigure 9.2: Search logic of the RAG retrieval module\n\nThe entry point function of the\n\nContextRetriever\n\nclass is the\n\nsearch()\n\nmethod, which calls all the advanced steps discussed in this chapter. Figure 9.2 shows in more detail how the search method glues together all the steps required to search results similar to the user’s query. It highlights how the extracted author details from the self-query step are used within the filtered vector search. Also, it zooms in on the search operation itself, where, for each query, we do three searches to the vector DB, looking for articles, posts, or repositories similar to the query. For each search (out of N searches), we want to retrieve a maximum of K results. Thus, we retrieve a maximum of K / 3 items for each data category (as we have three categories). Therefore, when summed up, we will have a list of\n\n≤ K\n\nchunks. The retrieved list is\n\n≤ K\n\n(and not equal to K) when a particular data category or more returns\n\n< K / 3\n\nitems after applying the author filters due to missing chunks for that specific author or data category.\n\nFigure 9.3: Processing the results flow of the RAG retrieval module\n\nFigure 9.3 illustrates how we process the results returned by the xN searches. As each search returns\n\n≤ K\n\nitems, we will end up with\n\n≤ N x K\n\nchunks that we aggregate into a single list. As some results might overlap between searchers, we must deduplicate the aggregated list to ensure each chunk is unique. Ultimately, we send the results to the rerank model, order them based on their reranking score, and pick the most relevant top K chunks we will use as context for RAG.\n\nLet’s understand how everything from Figures 9.2 and 9.3 is implemented in the\n\nContextRetriever\n\nclass. First, we initialize the class by setting up instances of the\n\nQueryExpansion\n\n,\n\nSelfQuery\n\n, and\n\nReranker\n\nclasses:\n\nclass\n\nContextRetriever\n\n:\n\ndef\n\n__init__\n\n(\n\nself, mock:\n\nbool\n\n=\n\nFalse\n\n) ->\n\nNone\n\n: self._query_expander = QueryExpansion(mock=mock) self._metadata_extractor = SelfQuery(mock=mock) self._reranker = Reranker(mock=mock)\n\nIn the\n\nsearch()\n\nmethod, we convert the user’s input string into a\n\nquery\n\nobject. We then use the\n\nSelfQuery\n\ninstance to extract the\n\nauthor_id\n\nand\n\nauthor_full_name\n\nfrom the query:\n\ndef\n\nsearch\n\n(\n\nself,\n\nquery:\n\nstr\n\n,\n\nk:\n\nint\n\n=\n\n3\n\n,\n\nexpand_to_n_queries:\n\nint\n\n=\n\n3\n\n,\n\n) ->\n\nlist\n\n: query_model = Query.from_str(query) query_model = self._metadata_extractor.generate(query_model) logger.info(\n\n\"Successfully extracted the author_id from the query.\"\n\n, author_id=query_model.author_id, )\n\nNext, we expand the query to generate multiple semantically similar queries using the\n\nQueryExpansion\n\ninstance:\n\nn_generated_queries = self._query_expander.generate(query_model, expand_to_n=expand_to_n_queries) logger.info(\n\n\"Successfully generated queries for search.\"\n\n, num_queries=\n\nlen\n\n(n_generated_queries), )\n\nWe then perform the search concurrently for all expanded queries using a thread pool. Each query is processed by the\n\n_search()\n\nmethod, which we’ll explore shortly. The results are flattened, deduplicated, and collected into a single list:\n\nwith\n\nconcurrent.futures.ThreadPoolExecutor()\n\nas\n\nexecutor: search_tasks = [executor.submit(self._search, _query_model, k)\n\nfor\n\n_query_model",
      "page_number": 701
    },
    {
      "number": 7,
      "title": "Now, let’s start by looking at the implementation of the retrieval module",
      "start_page": 1106,
      "end_page": 1198,
      "detection_method": "regex_chapter",
      "content": "in\n\nn_generated_queries] n_k_documents = [task.result()\n\nfor\n\ntask\n\nin\n\nconcurrent.futures.as_completed(search_tasks)] n_k_documents = utils.misc.flatten(n_k_documents) n_k_documents =\n\nlist\n\n(\n\nset\n\n(n_k_documents)) logger.info(\n\n\"All documents retrieved successfully.\"\n\n, num_documents=\n\nlen\n\n(n_k_documents))\n\nAfter retrieving the documents, we rerank them based on their relevance to the original query and keep only the top k documents:\n\nif\n\nlen\n\n(n_k_documents) >\n\n0\n\n: k_documents = self.rerank(query, chunks=n_k_documents, keep_top_k=k)\n\nelse\n\n: k_documents = []\n\nreturn\n\nk_documents\n\nThe\n\n_search()\n\nmethod performs the filtered vector search across different data categories like posts, articles, and repositories. It uses the\n\nEmbeddingDispatcher\n\nto convert the query into an\n\nEmbeddedQuery\n\n, which includes the query’s embedding vector and any extracted metadata:\n\ndef\n\n_search\n\n(\n\nself, query: Query, k:\n\nint\n\n=\n\n3\n\n) ->\n\nlist\n\n[EmbeddedChunk]:\n\nassert\n\nk >=\n\n3\n\n,\n\n\"k should be >= 3\"\n\ndef\n\n_search_data_category\n\n(\n\ndata_category_odm:\n\ntype\n\n[EmbeddedChunk], embedded_query: EmbeddedQuery\n\n) ->\n\nlist\n\n[EmbeddedChunk]:\n\nif\n\nembedded_query.author_id: query_filter = Filter( must=[ FieldCondition( key=\n\n\"author_id\"\n\n,\n\nmatch\n\n=MatchValue( value=\n\nstr\n\n(embedded_query.author_id), ), ) ] )\n\nelse\n\n: query_filter =\n\nNone\n\nreturn\n\ndata_category_odm.search( query_vector=embedded_query.embedding, limit=k //\n\n3\n\n, query_filter=query_filter, ) embedded_query: EmbeddedQuery = EmbeddingDispatcher.dispatch(query)\n\nWe used the same\n\nEmbeddingDispatcher\n\nto embed the query as in the RAG feature pipeline to embed the document chunks stored in the vector DB. Using the same class ensures we use the same embedding model at ingestion and query time, which is critical for the retrieval step.\n\nWe search each data category separately by leveraging the local\n\n_search_data_category()\n\nfunction. Within the\n\n_search_data_category()\n\nfunction, we apply the filters extracted from the\n\nembedded_query\n\nobject. For instance, if an\n\nauthor_id\n\nis present, we use it to filter the search results only to include documents from that author. The results from all categories are then combined:\n\npost_chunks = _search_data_category(EmbeddedPostChunk, embedded_query) articles_chunks = _search_data_category(EmbeddedArticleChunk, embedded_query) repositories_chunks = _search_data_category(EmbeddedRepositoryChunk, embedded_query) retrieved_chunks = post_chunks + articles_chunks + repositories_chunks\n\nreturn\n\nretrieved_chunks\n\nFinally, the\n\nrerank()\n\nmethod takes the original query and the list of retrieved documents to reorder them based on relevance:\n\ndef\n\nrerank\n\n(\n\nself, query:\n\nstr\n\n| Query, chunks:\n\nlist\n\n[EmbeddedChunk], keep_top_k:\n\nint\n\n) ->\n\nlist\n\n[EmbeddedChunk]:\n\nif\n\nisinstance\n\n(query,\n\nstr\n\n): query = Query.from_str(query) reranked_documents = self._reranker.generate(query=query, chunks=chunks, keep_top_k=keep_top_k) logger.info(\n\n\"\n\nDocuments reranked successfully.\"\n\n, num_documents=\n\nlen\n\n(reranked_documents))\n\nreturn\n\nreranked_documents\n\nLeveraging the\n\nContextRetriever\n\nclass, we can retrieve context from any query with only a few lines of code. For example, let’s take a look at the following code snippet, where we call the entire advanced RAG architecture with a simple call to the\n\nsearch()\n\nmethod:\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\nllm_engineering.application.rag.retriever\n\nimport\n\nContextRetriever query =\n\n\"\"\"\n\nMy name is Paul Iusztin.\n\nCould you draft a LinkedIn post discussing RAG systems?\n\nI'm particularly interested in:\n\nhow RAG works\n\nhow it is integrated with vector DBs and large language models (LLMs).\n\n\"\"\"\n\nretriever = ContextRetriever(mock=\n\nFalse\n\n) documents = retriever.search(query, k=\n\n3\n\n) logger.info(\n\n\"Retrieved documents:\"\n\n)\n\nfor\n\nrank, document\n\nin\n\nenumerate\n\n(documents): logger.info(\n\nf\"\n\n{rank +\n\n1\n\n}\n\n:\n\n{document}\n\n\"\n\n)\n\nCalling the code from above using the following CLI command:\n\npoetry poe call-rag-retrieval-module\n\n. This outputs the following:\n\n2024-09-18 19:01:50.588 | INFO - Retrieved documents: 2024-09-18 19:01:50.588 | INFO - 1: id=UUID('541d6c22-d15a-4e6a-924a- 68b7b1e0a330') content='4 Advanced RAG Algorithms You Must Know by Paul Iusztin Implement 4 advanced RAG retrieval techniques to optimize your vector DB searches. Integrate the RAG retrieval module into a production LLM system…\" platform='decodingml.substack.com' document_id=UUID('32648f33-87e6-435c-b2d7-861a03e72392') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://decodingml.substack.com/p/the-4- advanced-rag-algorithms-you?r=1ttoeh' 2024-09-18 19:01:50.588 | INFO - 2: id=UUID('5ce78438-1314-4874-8a5a-04f5fcf0cb21') content='Overview of advanced RAG optimization techniquesA production RAG system is\n\nsplit into 3 main components ingestion clean, chunk, embed, and load your data to a vector DBretrieval query your vector DB for …\" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7- 0d06760ee6bf') author_id=UUID('900fec95-d621-4315-84c6- 52e5229e0b96') author_full_name='Paul Iusztin' metadata= {'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://medium.com/decodingml/the-4-advanced-rag-algorithms-you- must-know-to-implement-5d0c7f1199d2' 2024-09-18 19:02:45.729 | INFO - 3: id=UUID('0405a5da-4686-428a-91ca-446b8e0446ff') content='Every Medium article will be its own lesson An End to End Framework for Production Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture Enabling Event Driven …\" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7-0d06760ee6bf') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://medium. com/decodingml/the-4- advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2'\n\nAs you can observe in the output above, along with the retrieved content, we have access to all kinds of metadata, such as the embedding model used for retrieval or the link from which the chunk was taken. These can quickly be added to a list of references when generating the result for the user, increasing trust in the final results.\n\nNow that we understand how the retrieval module works, let’s take a final step and examine the end-to-end RAG inference pipeline.\n\nOceanofPDF.com\n\nBringing everything together into the RAG inference pipeline\n\nTo fully implement the RAG flow, we still have to build the prompt using the context from the retrieval model and call the LLM to generate the answer. This section will discuss these two steps and wrap everything together into a single\n\nrag()\n\nfunction. The functions from this section can be accessed on GitHub at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/infrastructure/inference_pipeline_api .py.\n\nLet’s start by looking at the\n\ncall_llm_service()\n\nfunction, responsible for interfacing with the LLM service. It takes in a user’s query and an optional context, sets up the language model endpoint, executes the inference, and returns the generated answer. The context is optional; you can call the LLM without it, as you would when interacting with any other LLM:\n\ndef\n\ncall_llm_service\n\n(\n\nquery:\n\nstr\n\n, context:\n\nstr\n\n|\n\nNone\n\n) ->\n\nstr\n\n: llm = LLMInferenceSagemakerEndpoint( endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_component_name=\n\nNone\n\n) answer = InferenceExecutor(llm, query, context).execute()\n\nreturn\n\nanswer\n\nThis function makes an HTTP request to our fine-tuned LLM Twin model, which is hosted as an AWS SageMaker inference endpoint. We will explore all the SageMaker details in the next chapter, where we will dig into the\n\nLLMInferenceSagemakerEndpoint\n\nand\n\nInferenceExecutor\n\nclasses. For now, what is essential to know is that we use this function to call our fine-tuned LLM. Still, we must highlight how the query and context, passed to the\n\nInferenceExecutor\n\nclass, are transformed into the final prompt. We do that using a simple prompt template that is customized using the user query and retrieved context:\n\nprompt =\n\nf\"\"\"\n\nYou are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.\n\nUser query:\n\n{query}\n\nContext:\n\n{context}\n\n\"\"\"\n\nMoving on to the\n\nrag()\n\nfunction, this is where the RAG logic comes together. It handles retrieving relevant documents based on the query, mapping the documents to the context that will be injected into the prompt, and obtaining the final answer from the LLM:\n\ndef\n\nrag\n\n(\n\nquery:\n\nstr\n\n) ->\n\nstr\n\n: retriever = ContextRetriever(mock=\n\nFalse\n\n) documents = retriever.search(query, k=\n\n3\n\n) context = EmbeddedChunk.to_context(documents) answer = call_llm_service(query, context)\n\nreturn\n\nanswer\n\nAs we modularized all the RAG steps into independent classes, we reduced the high-level\n\nrag()\n\nfunction to five lines of code (encapsulating all the complexities of the system) similar to what we see in tools such as LangChain, LlamaIndex, or Haystack. Instead of their high-level implementation, we learned how to build an advanced RAG service from scratch. Also, by clearly separating the responsibility of each class, we can use them like LEGOs. Thus, you can quickly call the LLM independently without context or use the retrieval module as a query engine on top of your vector DB. In the next chapter, we will see the\n\nrag()\n\nfunction in action after we deploy our fine-tuned LLM to an AWS SageMaker inference endpoint.\n\nBefore ending this chapter, we want to discuss potential improvements you could add to the RAG inference pipeline. As we are building a chatbot, the first one is to add a conversation memory that stores all the user prompts and generated answers in memory. Thus, when interacting with the chatbot, it will be aware of the whole conversation, not only the latest prompt. When prompting the LLM, along with the new user input and context, we also pass the conversation history from the memory. As the conversation history can get long, to avoid exceeding the context window or higher costs, you have to implement a way to reduce the size of your memory. As illustrated in\n\nFigure 9.4, the simplest one is to keep only the latest K items from your chat history. Unfortunately, using this strategy, the LLM will never be aware of the whole conversation.\n\nTherefore, another way to add the chat history to your prompt is to keep a summary of the conversation along with the latest K replies. There are multiple ways to compute this summary, which might defeat the purpose of this book if we get into them all, but the simplest way is to always update the summary on every user prompt and generate an answer.\n\nFigure 9.4: Routing and memory examples\n\nAs for each search, we send three queries to the vector DB, one for each data category. Thus, the second improvement is to add a router between the query and the search. The router will be a multi-category classifier that predicts the data categories we must retrieve for that specific query. Hence, instead of making three requests for every search, we can often reduce it to one or two. For example, if the user wants to write a theoretical paragraph about RAG for an article, then most probably, it’s valuable to query only the article’s collection. In this case, the router will predict the article class, which we can use to decide what collection we must query.\n\nAnother example would be if we want to illustrate a piece of code that shows how to build a RAG pipeline. In this case, the router would have to predict the article and repository data category, as we need to look up examples in both collections for an exhaustive context.\n\nUsually, the router strategy decides what model to call based on a user’s input, such as whether to use GPT-4 or a self-hosted Llama 3.1 model for that specific query. However, in our particular use case, we can adapt the router algorithm to optimize the retrieval step.\n\nWe can further optimize the retrieval using a hybrid search algorithm that combines the vector search (based on embeddings) with a keyword search algorithm, such as BM25. Search algorithms used BM25 (or similar methods) to find similar items in a DB before vector search algorithms became popular. By merging the methods, hybrid search retrieves results that match the exact terms, such as RAG, LLM, or SageMaker, and the query semantics, increasing the accuracy and relevance of your retrieved results. Fundamentally, the hybrid search algorithms follow the next mechanics:\n\nParallel processing: The search query is processed simultaneously through both the vector search and BM25 algorithms. Each algorithm retrieves a set of relevant documents based on its criteria.\n\nScore normalization: The results from both searches are assigned relevance scores, which are then normalized to ensure comparability. This step is crucial because vector search and BM25 scoring mechanisms work at different scales. Thus, they can’t be compared or merged without normalization.\n\nResult merging: The normalized scores are combined, often through a weighted sum, to produce a final ranking of documents. Adjusting the weights allows for fine-tuning the emphasis on the semantic or keyword search algorithm.\n\nTo conclude, by combining the semantic and exact keyword search algorithms, you can improve the accuracy of your retrieval step. Vector search helps recognize synonyms or related concepts, ensuring that relevant information isn’t overlooked due to vocabulary differences. Keyword search ensures that documents containing critical keywords are emphasized appropriately, particularly in technical fields with specific terminology.\n\nOne last improvement we can make to our RAG system is to use multi- index vector structures instead of indexing based only on the content’s embedding. Let’s detail how multi-indexing works. Instead of using the embeddings of a single field to do the vector search for a particular collection, it combines multiple fields.\n\nFor example, in our LLM Twin use case, we used only the content field of our articles, posts, or repositories to query the vector DB. When using a multi-index strategy, along with the content field, we could index the embeddings of the platform where the content was posted or when the content was published. This could impact the final accuracy of your retrieval as different platforms have different types of content, or more recent content is usually more relevant. Frameworks such as Superlinked make multi-indexing easy. For example, in the code snippet below, using Superlinked, we defined a multi-index on the content and platform for our article collection in just a few lines of code:\n\nfrom\n\nsuperlinked.framework.common.schema.id_schema_object\n\nimport\n\nIdField\n\nfrom\n\nsuperlinked.framework.common.schema.schema\n\nimport\n\nschema\n\nfrom\n\nsuperlinked.framework.common.schema.schema_object\n\nimport\n\nString …\n\n# Other Superlinked imports.\n\n@schema\n\nclass\n\nArticleSchema\n\n:\n\nid\n\n: IdField platform: String content: String article = ArticleSchema() articles_space_content = TextSimilaritySpace( text=chunk(article.content, chunk_size=\n\n500\n\n, chunk_overlap=\n\n50\n\n), model=settings.EMBEDDING_MODEL_ID, ) articles_space_plaform = CategoricalSimilaritySpace( category_input=article.platform, categories=[\n\n\"medium\"\n\n,\n\n\"substack\"\n\n, \"wordpress\"], negative_filter=-\n\n5.0\n\n, ) article_index = Index( [articles_space_content, articles_space_plaform], fields=[article.author_id], )\n\nSuperlinked is a powerful Python tool for any use case that includes vector computing, such as RAG, recommender systems, and semantic search. It offers an ecosystem where you can quickly ingest data into a vector DB, write complex queries on top of it, and deploy the service as a RESTful API.\n\nThe world of LLMs and RAG is experimental, similar to any other AI domain. Thus, when building real-world products, it’s important to quickly build an end-to-end solution that works but is not necessarily the best. Then, you can reiterate with various experiments until you completely optimize it for your use case. This is standard practice in the industry and lets you iterate fast while providing value to the business and gathering user feedback as quickly as possible in the product’s lifecycle.\n\nOceanofPDF.com\n\nSummary\n\nThis chapter taught us how to build an advanced RAG inference pipeline. We started by looking into the software architecture of the RAG system. Then, we zoomed in on the advanced RAG methods we used within the retrieval module, such as query expansion, self-querying, filtered vector search, and reranking. Afterward, we saw how to write a modular\n\nContextRetriever\n\nclass that glues all the advanced RAG components under a single interface, making searching for relevant documents a breeze. Ultimately, we looked into how to connect all the missing dots, such as the retrieval, the prompt augmentation, and the LLM call, under a single RAG function that will serve as our RAG inference pipeline.\n\nAs highlighted a few times in this chapter, we couldn’t test our fine-tuned LLM because we haven’t deployed it yet to AWS SageMaker as an inference endpoint. Thus, in the next chapter, we will learn how to deploy the LLM to AWS SageMaker, write an inference interface to call the endpoint, and implement a FastAPI web server to serve as our business layer.\n\nOceanofPDF.com\n\nReferences\n\nA real-time retrieval system for social media data | VectorHub by SuperLinked. (n.d.). https://superlinked.com/vectorhub/articles/real-time- retrieval-system-social-media-data\n\nBuilding a Router from Scratch - LlamaIndex. (n.d.). https://docs.llamaindex.ai/en/stable/examples/low_level/router/\n\nHow to add memory to chatbots | LangChain. (n.d.). https://python.langchain.com/docs/how_to/chatbots_memory/#summary- memory\n\nHow to do “self-querying” retrieval | LangChain. (n.d.). https://python.langchain.com/docs/how_to/self_query/\n\nHow to route between sub-chains | LangChain. (n.d.). https://python.langchain.com/docs/how_to/routing/#routing-by-semantic- similarity\n\nHow to use the MultiQueryRetriever | LangChain. (n.d.). https://python.langchain.com/docs/how_to/MultiQueryRetriever/\n\nHybrid Search explained. (2023, January 3). Weaviate. https://weaviate.io/blog/hybrid-search-explained\n\nIusztin, P. (2024, August 20). 4 Advanced RAG Algorithms You Must Know | Decoding ML. Medium. https://medium.com/decodingml/the-4- advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2\n\nMonigatti, L. (2024, February 19). Advanced Retrieval-Augmented Generation: From Theory to LlamaIndex Implementation. Medium. https://towardsdatascience.com/advanced-retrieval-augmented-generation- from-theory-to-llamaindex-implementation-4de1464a9930\n\nMulti-attribute search with vector embeddings | VectorHub by Superlinked. (n.d.). https://superlinked.com/vectorhub/articles/multi-attribute-semantic- search\n\nOptimizing RAG with Hybrid Search & Reranking | VectorHub by Superlinked. (n.d.). https://superlinked.com/vectorhub/articles/optimizing- rag-with-hybrid-search-reranking\n\nRefactoring.Guru. (2024, January 1). Singleton. https://refactoring.guru/design-patterns/singleton\n\nStoll, M. (2024, September 7). Visualize your RAG Data—Evaluate your Retrieval-Augmented Generation System with Ragas. Medium. https://towardsdatascience.com/visualize-your-rag-data-evaluate-your- retrieval-augmented-generation-system-with-ragas-fc2486308557\n\nUsing LLM’s for retrieval and reranking—LlamaIndex, data framework for LLM applications. (n.d.). https://www.llamaindex.ai/blog/using-llms-for- retrieval-and-reranking-23cf2d3a14b6\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\n10\n\nOceanofPDF.com\n\nInference Pipeline Deployment\n\nDeploying the inference pipeline for the large language model (LLM) Twin application is a critical stage in the machine learning (ML) application life cycle. It’s where the most value is added to your business, making your models accessible to your end users. However, successfully deploying AI models can be challenging, as the models require expensive computing power and access to up-to-date features to run the inference. To overcome these constraints, it’s crucial to carefully design your deployment strategy. This ensures that it meets the application’s requirements, such as latency, throughput, and costs. As we work with LLMs, we must consider the inference optimization techniques presented in Chapter 8, such as model quantization. Also, to automate the deployment processes, we must leverage MLOps best practices, such as model registries that version and share our models across our infrastructure.\n\nTo understand how to design the deployment architecture of the LLM Twin, we will first look at three deployment types we can choose from: online real-time inference, asynchronous inference, and offline batch transform. Also, to better understand which option to choose for our LLM Twin use case, we will quickly walk you through a set of critical criteria we must consider before making an architectural decision, such as latency, throughput, data, and infrastructure. Also, we’ll weigh the pros and cons of monolithic and microservices architecture in model serving, a decision that can significantly influence the scalability and maintainability of your service.Once we’ve grasped the various design choices available, we’ll focus on understanding the deployment strategy for the LLM Twin’s inference pipeline. Subsequently, we will walk you through an end-to-end tutorial on deploying the LLM Twin service, including deploying our custom fine-tuned LLM to AWS SageMaker endpoints and implementing a\n\nFastAPI server as the central entry point for our users. We will then wrap up this chapter with a short discussion on autoscaling strategies and how to use them on SageMaker.\n\nHence, in this chapter, we will cover the following topics:\n\nCriteria for choosing deployment types\n\nUnderstanding inference deployment types\n\nMonolithic versus microservices architecture in model serving\n\nExploring the LLM Twin’s inference pipeline deployment strategy\n\nDeploying the LLM Twin service\n\nAutoscaling capabilities to handle spikes in usage\n\nOceanofPDF.com\n\nCriteria for choosing deployment types\n\nWhen it comes to deploying ML models, the first step is to understand the four requirements present in every ML application: throughput, latency, data, and infrastructure.\n\nUnderstanding them and their interaction is essential. When designing the deployment architecture for your models, there is always a trade-off between the four that will directly impact the user’s experience. For example, should your model deployment be optimized for low latency or high throughput?\n\nOceanofPDF.com\n\nThroughput and latency\n\nThroughput refers to the number of inference requests a system can process in a given period. It is typically measured in requests per second (RPS). Throughput is crucial when deploying ML models when you expect to process many requests. It ensures the system can handle many requests efficiently without becoming a bottleneck.\n\nHigh throughput often requires scalable and robust infrastructure, such as machines or clusters with multiple high-end GPUs.Latency is the time it takes for a system to process a single inference request from when it is received until the result is returned. Latency is critical in real-time applications where quick response times are essential, such as in live user interactions, fraud detection, or any system requiring immediate feedback. For example, the average latency of OpenAI’s API is the average response time from when a user sends a request, and the service provides a result that is accessible within your application.\n\nThe latency is the sum of the network I/O, serialization and deserialization, and the LLM’s inference time. Meanwhile, the throughput is the average number of requests the API processes and serves a second.\n\nLow-latency systems require optimized and often more costly infrastructure, such as faster processors, lower network latency, and possibly edge computing to reduce the distance data needs to travel.\n\nA lower latency translates to higher throughput when the service processes multiple queries in parallel successfully. For example, if the service takes 100 ms to process requests, this translates to a throughput of 10 requests per second. If the latency reaches 10 ms per request, the throughput rises to 100 requests per second.\n\nHowever, to complicate things, most ML applications adopt a batching strategy to simultaneously pass multiple data samples to the model. In this case, a lower latency can translate into lower throughput; in other words, a higher latency maps to a higher throughput. For example, if you process 20 batched requests in 100 ms, the latency is 100 ms, while the throughput is 200 requests per second. If you process 60 requests in 200 ms, the latency is 200 ms, while the throughput rises to 300 requests per second. Thus, even when batching requests at serving time, it’s essential to consider the minimum latency accepted for a good user experience.\n\nOceanofPDF.com\n\nData\n\nAs we know, data is everywhere in an ML system. But when talking about model serving, we mostly care about the model’s input and output. This includes the format, volume, and complexity of the processed data. Data is the foundation of the inference process. The characteristics of the data, such as its size and type, determine how the system needs to be configured and optimized for efficient processing.\n\nThe type and size of the data directly impact latency and throughput, as more complex or extensive data can take longer to process. For example, designing a model that takes input structured data and outputs a probability differs entirely from an LLM that takes input text (or even images) and outputs an array of characters.\n\nInfrastructure\n\nInfrastructure refers to the underlying hardware, software, networking, and system architecture that supports the deployment and operation of the ML models. The infrastructure provides the necessary resources for deploying, scaling, and maintaining ML models. It includes computing resources, memory, storage, networking components, and the software stack:\n\nFor high throughput, the systems require scalable infrastructure to manage large data volumes and high request rates, possibly through parallel processing, distributed systems, and high-end GPUs.\n\nInfrastructure must be optimized to reduce processing time to achieve low latency, such as using faster CPUs, GPUs, or specialized hardware. While optimizing your system for low latency while batching your requests, you often have to sacrifice high throughput in favor of lower latency, which can result in your hardware not being utilized at total capacity. As you process fewer requests per second, it results in idle computing, which increases the overall cost of processing a request. Thus, picking the suitable machine for your requirements is critical in optimizing costs.\n\nIt is crucial to design infrastructure to meet specific data requirements. This includes selecting storage solutions to handle large datasets and implementing fast retrieval mechanisms to ensure efficient data access. For example, we mostly care about optimizing throughput for offline training, while for online inference, we generally care about latency.\n\nWith this in mind, before picking a specific deployment type, you should ask yourself questions such as:\n\nWhat are the throughput requirements? You should make this decision based on the throughput’s required minimum, average, and maximum statistics.\n\nHow many requests the system must handle simultaneously? (1, 10, 1,000, 1 million, etc.)\n\nWhat are the latency requirements? (1 millisecond, 10 milliseconds, 1 second, etc.)\n\nHow should the system scale? For example, we should look at the CPU workload, number of requests, queue size, data size, or a combination of them.\n\nWhat are the cost requirements?With what data do we work with? For example, do we work with images, text, or tabular data?\n\nWhat is the size of the data we work with? (100 MB, 1 GB, 10 GB)\n\nDeeply thinking about these questions directly impacts the user experience of your application, which ultimately makes the difference between a successful product and not. Even if you ship a mind-blowing model, if the user needs to wait too long for a response or it often crashes, the user will switch your production to something less accurate that works reliably. For example, Google found in a 2016 study that 53% of visits are abandoned if a mobile site takes longer than three seconds to load: https://www.thinkwithgoogle.com/consumer-insights/consumer- trends/mobile-site-load-time-statistics/.\n\nLet’s move on to the three deployment architectures we can leverage to serve our models.\n\nOceanofPDF.com\n\nUnderstanding inference deployment types\n\nAs illustrated in Figure 10.1, you can choose from three fundamental deployment types when serving models:\n\nOnline real-time inference\n\nAsynchronous inference\n\nOffline batch transform\n\nWhen selecting one design over the other, there is a trade-off between latency, throughput, and costs. You must consider how the data is accessed and the infrastructure you are working with. Another criterion you have to consider is how the user will interact with the model. For example, will the user use it directly, like a chatbot, or will it be hidden within your system, like a classifier that checks if an input (or output) is safe?\n\nYou have to consider the freshness of the predictions as well. For example, serving your model in offline batch mode might be easier to implement if, in your use case, it is OK to consume delayed predictions. Otherwise, you have to serve your model in real-time, which is more infrastructure- demanding. Also, you have to consider your application’s traffic. Ask\n\nyourself questions such as, “Will the application be constantly used, or will there be spikes in traffic and then flatten out?”\n\nWith that in mind, let’s explore the three major ML deployment types.\n\nFigure 10.1: The three fundamental architectures of inference deployment types\n\nOceanofPDF.com\n\nOnline real-time inference\n\nIn real-time inference, we have a simple architecture based on a server that can be accessed through HTTP requests. The most popular options are to implement a REST API or gRPC server. The REST API is more accessible but slower, using JSON to pass data between the client and server.\n\nThis approach is usually taken when serving models outside your internal network to the broader public. For example, OpenAI’s API implements a REST API protocol.\n\nOn the other hand, implementing a gRPC makes your ML server faster, though it may reduce its flexibility and general applicability. You have to implement\n\nprotobuf\n\nschemas in your client application, which are more tedious to work with than JSON structures. The benefit, however, is that\n\nprotobuf\n\nobjects can be compiled into bites, making the network transfers much faster. Thus, this protocol is often adopted for internal services within the same ML system.\n\nUsing the real-time inference approach, the client sends an HTTP request to the ML service, which immediately processes the request and returns the\n\nresult in the same response. This synchronous interaction means the client waits for the result before moving on.\n\nTo make this work efficiently, the infrastructure must support low-latency, highly responsive ML services, often deployed on fast, scalable servers. Load balancing is crucial to evenly distribute incoming traffic evenly, while autoscaling ensures the system can handle varying loads. High availability is also essential to keeping the service operational at all times.\n\nFor example, this architecture is often present when interacting with LLMs, as when sending a request to a chatbot or API (powered by LLMs), you expend the predictions right ahead. LLM services, such as ChatGPT or Claude, often use WebSockets to stream each token individually to the end user, making the interaction more responsive. Other famous examples are AI services such as embedding or reranking models used for retrieval-augmented generation (RAG) or online recommendation engines in platforms like TikTok.\n\nThe simplicity of real-time inference, with its direct client-server interaction, makes it an attractive option for applications that require immediate responses, like chatbots or real-time recommendations. However, this approach can be challenging to scale and may lead to underutilized resources during low-traffic periods.\n\nOceanofPDF.com\n\nAsynchronous inference\n\nIn asynchronous inference, the client sends a request to the ML service, which acknowledges the request and places it in a queue for processing. Unlike real-time inference, the client doesn’t wait for an immediate response. Instead, the ML service processes the request asynchronously. This requires a robust infrastructure that queues the messages to be processed by the ML service later on.\n\nWhen the results are ready, you can leverage multiple techniques to send them to the client. For example, depending on the size of the result, you can put it either in a different queue or an object storage dedicated to storing the results.\n\nThe client can either adopt a polling mechanism that checks on a schedule if there are new results or adopt a push strategy and implement a notification system to inform the client when the results are ready.\n\nAsynchronous inference uses resources more efficiently. It doesn’t have to process all the requests simultaneously but can define a maximum number of machines that run in parallel to process the messages. This is possible because the requests are stored in the queue until a machine can process them. Another huge benefit is that it can handle spikes in requests without any timeouts. For example, let’s assume that on an e- shop site, we usually have 10 requests per second handled by two machines. Because of a promotion, many people started to visit the site, and the number of requests spiked to 100 requests per second. Instead\n\nof scaling the number of virtual machines (VMs) by 10, which can add drastic costs, the requests are queued, and the same two VMs can process them in their rhythm without any failures.\n\nAnother popular advantage for asynchronous architectures is when the requested job takes significant time to complete. For example, if the job takes over five minutes, you don’t want to block the client waiting for a response.\n\nWhile asynchronous inference offers significant benefits, it does come with trade-offs. It introduces higher latency, making it less suitable for time- sensitive applications. Additionally, it adds complexity to the implementation and infrastructure. Depending on your design choices, this architecture type falls somewhere between online and offline, offering a balance of benefits and trade-offs.\n\nFor example, this is a robust design where you don’t care too much about the latency of the inference but want to optimize costs heavily. Thus, it is a popular choice for problems such as extracting keywords from documents, summarizing them using LLMs, or running deep-fake models on top of videos. But suppose you carefully design the autoscaling system to process the requests from the queue at decent speeds. In that case, you can leverage this design for other use cases, such as online recommendations for e- commerce. In the end, it sums up how much computing power you are willing to pay to meet the expectations of your application.\n\nOceanofPDF.com\n\nOffline batch transform\n\nBatch transform is about processing large volumes of data simultaneously, either on a schedule or triggered manually. In a batch transform architecture, the ML service pulls data from a storage system, processes it in a single operation, and then stores the results in storage. The storage system can be implemented as an object storage like AWS S3 or a data warehouse like GCP BigQuery.\n\nUnlike the asynchronous inference architecture, a batch transform design is optimized for high throughput with permissive latency requirements. When real-time predictions are unnecessary, this approach can significantly reduce costs, as processing data in big batches is the most economical method. Moreover, the batch transform architecture is the simplest way to serve a model, accelerating development time.\n\nThe client pulls the results directly from data storage, decoupling its interaction with the ML service. Taking this approach, the client never has to wait for the ML service to process its input, but at the same time, it doesn’t have the flexibility to ask for new results at any time. You can see the data storage, where the results are stored as a large cache, from where the client can take what is required. If you want to make your application more responsive, the client can be notified when the processing is complete and can retrieve the results.\n\nUnfortunately, this approach will always introduce a delay between the time the predictions were computed and consumed. That’s why not all\n\napplications can leverage this design choice. For example, if we implement a recommender system for a video streaming application, having a delay of one day for the predicted movies and TV shows might work because you don’t consume these products at a high frequency. But suppose you make a recommender system for a social media platform. In that case, delaying one day or even one hour is unacceptable, as you constantly want to provide fresh content to the user.\n\nBatch transform shines in scenarios where high throughput is needed, like data analytics or periodic reporting. However, it’s unsuitable for real-time applications due to its high latency and requires careful planning and scheduling to manage large datasets effectively. That’s why it is an offline serving method.\n\nTo conclude, we examined the three most common architectures for serving ML models. We started with online real-time inference, which serves clients when they request a prediction. Then, we looked at the asynchronous inference method, which sits between online and offline. Ultimately, we presented the offline batch transform, which is used to process large amounts of data and store them in data storage, from where the client later consumes them.\n\nOceanofPDF.com\n\nMonolithic versus microservices architecture in model serving\n\nIn the previous section, we saw three different methods of deploying the ML service. The differences in architecture were mainly based on the interaction between the client and the ML service, such as the communication protocol, the ML service responsiveness, and prediction freshness.\n\nBut another aspect to consider is the architecture of the ML service itself, which can be implemented as a monolithic server or as multiple microservices. This will impact how the ML service is implemented, maintained, and scaled. Let’s explore the two options.\n\nFigure 10.2: Monolithic versus microservices architecture in model serving\n\nOceanofPDF.com\n\nMonolithic architecture\n\nThe LLM (or any other ML model) and the associated business logic (preprocessing and post-processing steps) are bundled into a single service in a monolithic architecture. This approach is straightforward to implement at the beginning of a project, as everything is placed within one code base. Simplicity makes maintenance easy when working on small to medium projects, as updates and changes can be made within a unified system.\n\nOne key challenge of a monolithic architecture is the difficulty of scaling components independently. The LLM typically requires GPU power, while the rest of the business logic is CPU and I/O-bound. As a result, the infrastructure must be optimized for both GPU and CPU. This can lead to inefficient resource use, with the GPU being idle when the business logic is executed and vice versa. Such inefficiency can result in additional costs that could be avoided.\n\nMoreover, this architecture can limit flexibility, as all components must share the same tech stack and runtime environment. For example, you might want to run the LLM using Rust or C++ or compile it with ONNX or TensorRT while keeping the business logic in Python. Having all the code in one system makes this differentiation difficult. Finally, splitting the work across different teams is complex, often leading to bottlenecks and reduced agility.\n\nOceanofPDF.com\n\nMicroservices architecture\n\nA microservices architecture breaks down the inference pipeline into separate, independent services—typically splitting the LLM service and the business logic into distinct components. These services communicate over a network using protocols such as REST or gRPC.\n\nAs illustrated in Figure 10.3, the main advantage of this approach is the ability to scale each component independently. For instance, since the LLM service might require more GPU resources than the business logic, it can be scaled horizontally without impacting the other components. This optimizes resource usage and reduces costs, as different types of machines (e.g., GPU versus CPU) can be used according to each service’s needs.\n\nFor example, let’s assume that the LLM inference takes longer, so you will need more ML service replicas to meet the demand. But remember that GPU VMs are expensive. By decoupling the two components, you will run only what is required on the GPU machine and not block the GPU VM with other computing that can be done on a much cheaper machine.\n\nThus, by decoupling the components, you can scale horizontally as required, with minimal costs, providing a cost-effective solution to your system’s needs.\n\nFigure 10.3: Scaling microservices independently based on compute requirements\n\nAdditionally, each microservice can adopt the most suitable technology stack, allowing teams to innovate and optimize independently.\n\nHowever, microservices introduce complexity in deployment and maintenance. Each service must be deployed, monitored, and maintained separately, which can be more challenging than managing a monolithic system.\n\nThe increased network communication between services can also introduce latency and potential points of failure, necessitating robust monitoring and resilience mechanisms.\n\nNote that the proposed design for decoupling the ML model and business logic into two services can be extended if necessary. For example, you can have one service for preprocessing the data, one for the model, and another for post-processing the data. Depending on the four pillars (latency, throughput, data, and infrastructure), you can get creative and design the most optimal architecture for your application needs.\n\nOceanofPDF.com\n\nChoosing between monolithic and microservices architectures\n\nThe choice between monolithic and microservices architectures for serving ML models largely depends on the application’s specific needs. A monolithic approach might be ideal for smaller teams or more straightforward applications where ease of development and maintenance is a priority. It’s also a good starting point for projects without frequent scaling requirements. Also, if the ML models are smaller, don’t require a GPU, or don’t require smaller and cheaper GPUs, the trade-off between reducing costs and complicating your infrastructure is worth considering.\n\nOn the other hand, microservices, with their adaptability and scalability, are well suited for larger, more complex systems where different components have varying scaling needs or require distinct tech stacks. This architecture is particularly advantageous when scaling specific system parts, such as GPU-intensive LLM services. As LLMs require powerful machines with GPUs, such as Nvidia A100, V100, or A10g, which are incredibly costly, microservices offer the flexibility to optimize the system for keeping these machines busy all the time or quickly scaling down when the GPU is idle. However, this flexibility comes at the cost of increased complexity in both development and operations.\n\nA common strategy is to start with a monolithic design and further decouple it into multiple services as the project grows. However, to successfully do so without making the transition too complex and costly, you must design the monolith application with this in mind. For instance, even if all the code runs on a single machine, you can completely decouple the modules of the application at the software level. This makes it easier to move these\n\nmodules to different microservices when the time comes. When working with Python, for example, you can implement the ML and business logic into two different Python modules that don’t interact with each other. Then, you can glue these two modules at a higher level, such as through a service class, or directly into the framework you use to expose your application over the internet, such as FastAPI.\n\nAnother option is to write the ML and business logic as two different Python packages that you glue together in the same ways as before. This is better because it completely enforces a separation between the two but adds extra complexity at development time. The main idea, therefore, is that if you start with a monolith and down the line you want to move to a microservices architecture, it’s essential to design your software with modularity in mind. Otherwise, if the logic is mixed, you will probably have to rewrite everything from scratch, adding tons of development time, which translates into wasted resources.\n\nIn summary, monolithic architectures offer simplicity and ease of maintenance but at the cost of flexibility and scalability. At the same time, microservices provide the agility to scale and innovate but require more sophisticated management and operational practices.\n\nOceanofPDF.com\n\nExploring the LLM Twin’s inference pipeline deployment strategy\n\nNow that we’ve understood all the design choices available for implementing the deployment strategy of the LLM Twin’s inference pipeline, let’s explore the concrete decisions we made to actualize it.\n\nOur primary objective is to develop a chatbot that facilitates content creation. To achieve this, we will process requests sequentially, with a strong emphasis on low latency. This necessitates the selection of an online real-time inference deployment architecture.\n\nOn the monolith versus microservice aspect, we will split the ML service between a REST API server containing the business logic and an LLM microservice optimized for running the given LLM. As the LLM requires a powerful machine to run the inference, and we can further optimize it with various engines to speed up the latency and memory usage, it makes the most sense to go with the microservice architecture. By doing so, we can quickly adapt the infrastructure based on various LLM sizes. For example, if we run an 8B parameter model, the model can run on a single machine with a Nivida A10G GPU after quantization. But if we want to run a 30B model, we can upgrade to an Nvidia A100 GPU. Doing so allows us to upgrade only the LLM microservice while keeping the REST API untouched.\n\nAs illustrated in Figure 10.4, most business logic is centered around RAG in our particular use case. Thus, we will perform RAG’s retrieval and\n\naugmentation parts within the business microservice. It will also include all the advanced RAG techniques presented in the previous chapter to optimize the pre-retrieval, retrieval, and post-retrieval steps.\n\nThe LLM microservice is strictly optimized for the RAG generation component. Ultimately, the business layer will send the prompt trace consisting of the user query, prompt, answer, and other intermediary steps to the prompt monitoring pipeline, which we will detail in Chapter 11.\n\nIn summary, our approach involves implementing an online real-time ML service using a microservice architecture, which effectively splits the LLM and business logic into two distinct services.\n\nFigure 10.4: Microservice deployment architecture of the LLM Twin’s inference pipeline\n\nLet’s review the interface of the inference pipeline, which is defined by the feature/training/inference (FTI) architecture. For the pipeline to run, it needs two things:\n\nReal-time features used for RAG, generated by the feature pipeline, which is queried from our online feature store, more concretely from the Qdrant vector database (DB)\n\nA fine-tuned LLM generated by the training pipeline, which is pulled from our model registry\n\nWith that in mind, the flow of the ML service looks as follows, as illustrated in Figure 10.4:\n\nA user sends a query through an HTTP request.\n\nThe user’s input retrieves the proper context by leveraging the advanced RAG retrieval module implemented in Chapter 4.\n\nThe user’s input and retrieved context are packed into the final prompt using a dedicated prompt template.\n\nThe prompt is sent to the LLM microservice through an HTTP request.\n\nThe business microservices wait for the generated answer.\n\nAfter the answer is generated, it is sent to the prompt monitoring pipeline along with the user’s input and other vital information to monitor.\n\nUltimately, the generated answer is sent back to the user.\n\nNow, let’s explore what tech stack we used to implement the architecture presented in Figure 10.4. As we know, we use Qdrant for the vector DB. We will leverage Hugging Face for the model registry. By doing so, we can publicly share our model with everyone who is testing the code from this book. Thus, you can easily use the model we provided if you don’t want to run the training pipeline, which can cost up to 100 dollars. As you can see, shareability and accessibility are some of the most beautiful aspects of storing your model in a model registry.\n\nWe will implement the business microservice in FastAPI because it’s popular, easy to use, and fast. The LLM microservice will be deployed on AWS SageMaker, where we will leverage SageMaker’s integration with Hugging Face’s Deep Learning Containers (DLCs) to deploy the model. We will discuss Hugging Face’s DLCs in the next section, but\n\nintuitively, it is an inference engine used to optimize LLMs at serving time. The prompt monitoring pipeline is implemented using Comet, but we will look over that module only in Chapter 11.\n\nThe SageMaker Inference deployment is composed of the following components that we will show you how to implement:\n\nSageMaker endpoint: An endpoint is a scalable and secure API that SageMaker hosts to enable real-time predictions from deployed models. It’s essentially the interface through which applications interact with your model. Once deployed, an application can make HTTP requests to the endpoint to receive real-time predictions.\n\nSageMaker model: In SageMaker, a model is an artifact that results from training an algorithm. It contains the information required to make predictions, including the weights and computation logic. You can create multiple models and use them in different configurations or for various predictions.\n\nSageMaker configuration: This configuration specifies the hardware and software set up to host the model. It defines the resources required for the endpoint, such as the type and number of ML compute instances. Endpoint configurations are used when creating or updating an endpoint. They allow for flexibility in the deployment and scalability of the hosted models.\n\nSageMaker Inference component: This is the last piece of the puzzle that connects the model and configuration to anendpoint. You can\n\ndeploy multiple models to an endpoint, each with its resource configuration. Once deployed, models are easily accessible via the InvokeEndpoint API in Python.\n\nTogether, these components create a robust infrastructure for deploying and managing ML models in SageMaker, enabling scalable, secure, and efficient real-time predictions.\n\nOther popular cloud platforms offer the exact solutions. For example, you have Azure OpenAI instead of Bedrock and Azure ML instead of SageMaker on Azure. The list of ML deployment tools, such as Hopsworks, Modal, Vertex AI, Seldon, BentoML, and many more, is endless and will probably change. What is essential though is to understand your use case requirements and find a tool that fits your needs.\n\nOceanofPDF.com\n\nThe training versus the inference pipeline\n\nUnderstanding the nuances between the training and inference pipelines is crucial before we deploy the inference pipeline. While it might seem straightforward that the training pipeline is for training and the inference pipeline is for inference, there are significant differences that we need to grasp to comprehend the technical aspects of our discussion fully.\n\nOne key difference lies in how data is handled and accessed within each pipeline. During training, data is typically accessed from offline storage in batch mode, optimized for throughput and ensuring data lineage. For example, our LLM Twin architecture uses ZenML artifacts to access, version, and track data fed to the training loop in batches. In contrast, the inference pipeline requires an online DB optimized for low latency. We will leverage the Qdrant vector DB to grab the necessary context for RAG. In this context, the focus shifts from data lineage and versioning to quick data access, ensuring a seamless user experience. Additionally, the outputs of these pipelines also differ significantly. The training pipeline outputs trained model weights stored in the model registry. Meanwhile, the inference pipeline outputs predictions served directly to the user.\n\nAlso, the infrastructure required for each pipeline is different. The training pipeline demands more powerful machines equipped with as many GPUs as possible. This is because training involves batching data and holding all the necessary gradients in memory for optimization steps, making it highly compute-intensive. More computational power and VRAM allow larger batches (or throughput), reducing training time and enabling more extensive experimentation. On the other hand, the inference pipeline typically\n\nrequires less computation. Inference often involves passing a single sample or smaller batches to the model without the need for optimization steps.\n\nDespite these differences, there is some overlap between the two pipelines, particularly regarding preprocessing and post-processing steps. Applying the same preprocessing and post-processing functions and hyperparameters during training and inference is crucial. Any discrepancies can lead to what is known as training-serving skew, where the model’s performance during inference deviates from its performance during training.\n\nOceanofPDF.com\n\nDeploying the LLM Twin service\n\nThe last step is implementing the architecture presented in the previous section. More concretely, we will deploy the LLM microservice using AWS SageMaker and the business microservice using FastAPI. Within the business microservice, we will glue the RAG logic written in Chapter 9 with our fine-tuned LLM Twin, ultimately being able to test out the inference pipeline end to end.\n\nServing the ML model is one of the most critical steps in any ML application’s life cycle, as users can only interact with our model after this phase is completed. If the serving architecture isn’t designed correctly or if the infrastructure isn’t working properly, it doesn’t matter that you have implemented a powerful and excellent model. As long as the user cannot appropriately interact with it, it has near zero value from a business point of view. For example, if you have the best code assistant on the market, but the latency to use it is too high, or the API calls keep crashing, the user will probably switch to a less performant code assistant that works faster and is more stable.\n\nThus, in this section, we will show you how to:\n\nDeploy our fined-tuned LLM Twin model to AWS SageMaker\n\nWrite an inference client to interact with the deployed model\n\nWrite the business service in FastAPI\n\nIntegrate our RAG logic with our fine-tuned LLM\n\nImplement autoscaling rules for the LLM microservice\n\nOceanofPDF.com\n\nImplementing the LLM microservice using AWS SageMaker\n\nWe aim to deploy the LLM Twin model, stored in Hugging Face’s model registry, to Amazon SageMaker as an online real-time inference endpoint. We will leverage Hugging Face’s specialized inference container, known as the Hugging Face LLM DLC, to deploy our LLM.\n\nOceanofPDF.com\n\nWhat are Hugging Face’s DLCs?\n\nDLCs are specialized Docker images that come pre-loaded with essential deep-learning frameworks and libraries, including popular tools like transformers, datasets, and tokenizers from Hugging Face. These containers are designed to simplify the process of training and deploying models by eliminating the need for complex environment setup and optimization. The Hugging Face Inference DLC, in particular, includes a fully integrated serving stack, significantly simplifying the deployment process and reducing the technical expertise needed to serve deep learning models in production.\n\nWhen it comes to serving models, the DLC is powered by the Text Generation Inference (TGI) engine, made by Hugging Face: https://github.com/huggingface/text-generation-inference.\n\nTGI is an open-source solution for deploying and serving LLMs. It offers high-performance text generation using tensor parallelism and dynamic batching for the most popular open-source LLMs available on Hugging Face, such as Mistral, Llama, and Falcon. To sum up, the most powerful features the DLC image provides are:\n\nTensor parallelism, thus enhancing the computational efficiency of model inference\n\nOptimized transformers code for inference, leveraging flash-attention to maximize performance across the most widely used architectures: https://github.com/Dao-AILab/flash-attention\n\nQuantization with\n\nbitsandbytes\n\nthat reduces the model size while maintaining performance, making deployments more efficient: https://github.com/bitsandbytes- foundation/bitsandbytes\n\nContinuous batching of incoming requests, thus improving throughput by dynamically batching requests as they arrive\n\nAccelerated weight loading by utilizing\n\nsafetensors\n\nfor faster model initialization, reducing start-up time: https://github.com/huggingface/safetensors\n\nToken streaming that supports real-time interactions through Server- Sent Events (SSE)\n\nTo summarize, our LLM Twin model will run inside DLC Docker images, listening to requests, optimizing the LLM for inference, and serving the results in real time. The DLC’s Docker images will be hosted on AWS\n\nSageMaker under inference endpoints that can be accessed through HTTP requests. With that in mind, let’s move on to the implementation. We will start by deploying the LLM and then writing a wrapper class to interact with the SageMaker Inference endpoint.\n\nOceanofPDF.com\n\nConfiguring SageMaker roles\n\nThe first step is to create the proper AWS Identity and Access Management (IAM) users and roles to access and deploy the SageMaker infrastructure. AWS IAM controls who can authenticate and what any actor has access to. You can create new users (assigned to people) and new roles (assigned to other actors within your infrastructure, such as EC2 VMs) through IAM.\n\nThe whole deployment process is automated. We will have to run a few CLI commands, but first, ensure that you have correctly configured the\n\nAWS_ACCESS_KEY\n\n,\n\nAWS_SECRET_KEY\n\n, and\n\nAWS_REGION\n\nenvironmental variables in the\n\n.env\n\nfile. At this step, the easiest way is to use the credentials attached to an admin role as, in the following steps, we will create a set of narrower IAM roles used in the rest of the chapter.\n\nAfter you configured your\n\n.env\n\nfile, we have to:\n\nCreate an IAM user restricted to creating and deleting only the resources we need for the deployment, such as SageMaker itself, Elastic Container Registry (ECR), and S3. To make it, run the following:\n\npoetry poe create-sagemaker-role\n\nThis command will generate a JSON file called\n\nsagemaker_user_credentials.json\n\nthat contains a new AWS access and secret key. From now on, we will use these credentials to deploy everything related to SageMaker to ensure we modify only the resources associated with SageMaker. Otherwise, we could accidentally modify other AWS resources using an admin account, resulting in additional costs or altering other existing projects. Thus, having a narrow role only to your use case is good practice.\n\nThe last step is to take the new credentials from the JSON file and update the\n\nAWS_ACCESS_KEY\n\nand\n\nAWS_SECRET_KEY\n\nvariables in your\n\n.env\n\nfile. You can check out the implementation at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_sage maker_role.py.\n\nCreate an IAM execution role. We will attach this role to the SageMaker deployment, empowering it to access other AWS resources on our behalf. This is standard practice for cloud deployments, as instead of authenticating every machine within your credentials, you attach a role that allows them to access only what is necessary from your infrastructure. In our case, we will provide SageMaker access to AWS S3, CloudWatch, and ECR. To create the role, run the following:\n\npoetry poe create-sagemaker-execution-role\n\nThis command will generate a JSON file called\n\nsagemaker_execution_role.json\n\nthat contains the Amazon Resource Name (ARN) of the newly created role. The ARN is an ID attached to any AWS resource to identify it across your cloud infrastructure. Take the ARN value from the JSON file and update the\n\nAWS_ARN_ROLE\n\nvariable from your\n\n.env\n\nfile with it. You can check out the implementation at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_exec ution_role.py.\n\nIf you have issues, configure the AWS CLI with the same AWS credentials as in the\n\n.env\n\nfile and repeat the process. Official documentation for installing the AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html.\n\nBy setting the IAM user and role in your\n\n.env\n\nfile, we will automatically load them in the settings Python object and use them throughout the following steps. Now, let’s move on to the actual\n\ndeployment.\n\nOceanofPDF.com",
      "page_number": 1106
    },
    {
      "number": 8,
      "title": "with our fine-tuned LLM Twin, ultimately being able to test out the inference pipeline end to end",
      "start_page": 1199,
      "end_page": 1630,
      "detection_method": "regex_chapter",
      "content": "Deploying the LLM Twin model to AWS SageMaker\n\nThe deployment of AWS SageMaker is fully automated through a set of Python classes, which we will cover in this chapter. This section aims to understand how we configure the SageMaker infrastructure directly from Python. Thus, you don’t have to run everything step by step, as in a standard tutorial, but only to understand the code.\n\nWe can initiate and finalize the entire SageMaker deployment using a simple CLI command:\n\npoe deploy-inference-endpoint\n\n. This command will initialize all the steps presented in Figure 10.5, except for creating the SageMaker AWS IAMs we created and configured in the previous step.\n\nIn this section, we will walk you through the code presented in Figure 10.5 that helps us fully automate the deployment process, starting with the\n\ncreate_endpoint()\n\nfunction. Ultimately, we will test the CLI command and check the AWS console to see whether the deployment was successful. The SageMaker deployment code is available at https://github.com/PacktPublishing/LLM- Engineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy.\n\nFigure 10.5: AWS SageMaker deployment steps\n\nWe will take a top-down approach to walk you through the implementation, starting with the main function that deploys the LLM Twin model to AWS SageMaker. In the function below, we first take the latest version of the Docker DLC image using the\n\nget_huggingface_llm_image_uri()\n\nfunction, which is later passed to the deployment strategy class, along with an instance of the resource manager and deployment service:\n\ndef\n\ncreate_endpoint\n\n(\n\nendpoint_type=EndpointType.INFERENCE_COMPONENT_BASED\n\n): llm_image = get_huggingface_llm_image_uri(\n\n\"huggingface\"\n\n, version=\n\nNone\n\n) resource_manager = ResourceManager() deployment_service = DeploymentService(resource_manager=resource_manager) SagemakerHuggingfaceStrategy(deployment_service).deploy( role_arn=settings.ARN_ROLE, llm_image=llm_image, config=hugging_face_deploy_config,\n\nendpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, endpoint_config_name=settings.SAGEMAKER_ENDPOINT_CONFIG_I NFERENCE, gpu_instance_type=settings.GPU_INSTANCE_TYPE, resources=model_resource_config, endpoint_type=endpoint_type, )\n\nWe must review the three classes used in the\n\ncreate_endpoint()\n\nfunction to fully understand the deployment process. Let’s start with the\n\nResourceManager\n\nclass. The class begins with the initialization method, establishing the connection to AWS SageMaker using boto3, the AWS SDK for Python, which provides the necessary functions to interact with various AWS services, including SageMaker.\n\nclass\n\nResourceManager\n\n:\n\ndef\n\n__init__\n\n(\n\nself\n\n) ->\n\nNone\n\n:\n\nself\n\n.sagemaker_client = boto3.client(\n\n\"sagemaker\"\n\n, region_name=settings.AWS_REGION, aws_access_key_id=settings.AWS_ACCESS_KEY, aws_secret_access_key=settings.AWS_SECRET_KEY, )\n\nNext, we implement the\n\nendpoint_config_exists\n\nmethod, checking whether a specific SageMaker endpoint configuration exists:\n\ndef\n\nendpoint_config_exists\n\n(\n\nself, endpoint_config_name:\n\nstr\n\n) ->\n\nbool\n\n:\n\ntry\n\n:\n\nself\n\n.sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoi nt_config_name) logger.info(\n\nf\"Endpoint configuration '\n\n{endpoint_config_name}\n\n' exists.\"\n\n)\n\nreturn\n\nTrue\n\nexcept\n\nClientError: logger.info(\n\nf\"Endpoint configuration '\n\n{endpoint_config_name}\n\n' does not exist.\"\n\n)\n\nreturn\n\nFalse\n\nThe class also includes the\n\nendpoint_exists\n\nmethod, which checks the existence of a specific SageMaker endpoint:\n\ndef\n\nendpoint_exists\n\n(\n\nself, endpoint_name:\n\nstr\n\n) ->\n\nbool\n\n:\n\ntry\n\n:\n\nself\n\n.sagemaker_client.describe_endpoint(EndpointName=endpoint_name) logger.info(\n\nf\"Endpoint '\n\n{endpoint_name}\n\n' exists.\"\n\n)\n\nreturn\n\nTrue\n\nexcept\n\nself\n\n.sagemaker_client.exceptions.ResourceNotFoundException: logger.info(\n\nf\"Endpoint '\n\n{endpoint_name}\n\n' does not exist.\"\n\n)\n\nreturn\n\nFalse\n\nLet’s move to the\n\nDeploymentService\n\n. Within the constructor, we set up the\n\nsagemaker_client\n\n, which will interface with AWS SageMaker and an instance of the\n\nResourceManager\n\nclass we talked about earlier:\n\nclass\n\nDeploymentService\n\n:\n\ndef\n\n__init__\n\n(\n\nself, resource_manager\n\n):\n\nself\n\n.sagemaker_client = boto3.client(\n\n\"sagemaker\"\n\n, region_name=settings.AWS_REGION, aws_access_key_id=settings.AWS_ACCESS_KEY, aws_secret_access_key=settings.AWS_SECRET_KEY, )\n\nself\n\n.resource_manager = resource_manager\n\nThe\n\ndeploy()\n\nmethod is the heart of the\n\nDeploymentService\n\nclass. This method orchestrates the entire process of deploying a model to a SageMaker endpoint. It checks whether the necessary configurations are already in place and, if not, it triggers the deployment:\n\ndef\n\ndeploy\n\n(\n\nself,\n\nrole_arn:\n\nstr\n\n,\n\nllm_image:\n\nstr\n\n,\n\nconfig:\n\ndict\n\n,\n\nendpoint_name:\n\nstr\n\n,\n\nendpoint_config_name:\n\nstr\n\n,\n\ngpu_instance_type:\n\nstr\n\n,\n\nresources:\n\nOptional\n\n[\n\ndict\n\n] =\n\nNone\n\n,\n\nendpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n\n) ->\n\nNone\n\n:\n\ntry\n\n:\n\nif\n\nself\n\n.resource_manager.endpoint_config_exists(endpoint_config_name=endpoin t_config_name): logger.info(\n\nf\"Endpoint configuration\n\n{endpoint_config_name}\n\nexists. Using existing configuration...\"\n\n)\n\nelse\n\n: logger.info(\n\nf\"Endpoint configuration\n\n{endpoint_config_name}\n\ndoes not exist.\"\n\n)\n\nself\n\n.prepare_and_deploy_model( role_arn=role_arn, llm_image=llm_image, config=config, endpoint_name=endpoint_name, update_endpoint=\n\nFalse\n\n, resources=resources, endpoint_type=endpoint_type, gpu_instance_type=gpu_instance_type, ) logger.info(\n\nf\"Successfully deployed/updated model to endpoint\n\n{endpoint_name}\n\n.\"\n\n)\n\nexcept\n\nException\n\nas\n\ne: logger.error(\n\nf\"Failed to deploy model to SageMaker:\n\n{e}\n\n\"\n\n)\n\nraise\n\nThe deploy method begins by checking whether the endpoint configuration already exists using the\n\nresource_manager\n\n. This step is crucial because it avoids unnecessary redeployment if the configuration is already set up. The deployment itself is handled by calling the\n\nprepare_and_deploy_model()\n\nmethod, which is responsible for the actual deployment of the model to the specified SageMaker endpoint.\n\nThe\n\nprepare_and_deploy_model()\n\nmethod is a static method within the\n\nDeploymentService\n\nclass. This method is focused on setting up and deploying the Hugging Face model to SageMaker:\n\n@staticmethod\n\ndef\n\nprepare_and_deploy_model\n\n(\n\nrole_arn:\n\nstr\n\n,\n\nllm_image:\n\nstr\n\n,\n\nconfig:\n\ndict\n\n,\n\nendpoint_name:\n\nstr\n\n,\n\nupdate_endpoint:\n\nbool\n\n,\n\ngpu_instance_type:\n\nstr\n\n,\n\nresources:\n\nOptional\n\n[\n\ndict\n\n] =\n\nNone\n\n,\n\nendpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n\n) ->\n\nNone\n\n: huggingface_model = HuggingFaceModel( role=role_arn, image_uri=llm_image, env=config, transformers_version=\n\n\"4.6\"\n\n, pytorch_version=\n\n\"1.13\"\n\n, py_version=\n\n\"py310\"\n\n, ) huggingface_model.deploy( instance_type=gpu_instance_type, initial_instance_count=\n\n1\n\n, endpoint_name=endpoint_name, update_endpoint=update_endpoint, resources=resources, tags=[{\n\n\"Key\"\n\n:\n\n\"task\"\n\n,\n\n\"Value\"\n\n:\n\n\"model_task\"\n\n}], endpoint_type=endpoint_type, )\n\nThis method begins by creating an instance of HuggingFaceModel, a specialized model class from SageMaker designed to handle Hugging Face models. The constructor for HuggingFaceModel takes several essential parameters, such as the role ARN (which gives SageMaker the necessary permissions), the URI of the LLM DLC Docker image, and the LLM configuration that specifies what LLM to load from Hugging Face and its inference parameters, such as the maximum total of tokens.\n\nOnce HuggingFaceModel is instantiated, the method deploys it to SageMaker using the deploy function. This deployment process involves specifying the type of instance used, the number of instances, and whether to update an existing endpoint or create a new one. The method also includes optional resources for more complex deployments, such as the\n\ninitial_instance_count\n\nparameter for multi-model endpoints and tags for tracking and categorization.\n\nThe last step is to walk you through the\n\nSagemakerHuggingfaceStrategy\n\nclass, which aggregates everything we have shown. The class is initialized only with an instance of a deployment service, such as the one shown above.\n\nclass\n\nSagemakerHuggingfaceStrategy\n\n(\n\nDeploymentStrategy\n\n):\n\ndef\n\n__init__\n\n(\n\nself, deployment_service\n\n):\n\nself\n\n.deployment_service = deployment_service\n\nThe core functionality of the\n\nSagemakerHuggingfaceStrategy\n\nclass is encapsulated in its\n\ndeploy()\n\nmethod. This method orchestrates the deployment process, taking various parameters that define how the Hugging Face model should be deployed to AWS SageMaker:\n\ndef\n\ndeploy\n\n(\n\nself,\n\nrole_arn:\n\nstr\n\n,\n\nllm_image:\n\nstr\n\n,\n\nconfig:\n\ndict\n\n,\n\nendpoint_name:\n\nstr\n\n,\n\nendpoint_config_name:\n\nstr\n\n,\n\ngpu_instance_type:\n\nstr\n\n,\n\nresources:\n\nOptional\n\n[\n\ndict\n\n] =\n\nNone\n\n,\n\nendpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n\n) ->\n\nNone\n\n: logger.info(\n\n\"Starting deployment using Sagemaker Huggingface Strategy...\"\n\n) logger.info(\n\nf\"Deployment parameters: nb of replicas:\n\n{settings.COPIES}\n\n, nb of gpus:\n\n{settings.GPUS}\n\n, instance_type:\n\n{settings.GPU_INSTANCE_TYPE}\n\n\"\n\n)\n\nThe parameters passed into the method are crucial to the deployment process:\n\nrole_arn\n\n: The AWS IAM role that provides permissions for the SageMaker deployment.\n\nllm_image\n\n: The URI of the DLC Docker image\n\nconfig\n\n: A dictionary containing configuration settings for the model environment.\n\nendpoint_name\n\nand\n\nendpoint_config_name\n\n: Names for the SageMaker endpoint and its configuration, respectively.\n\ngpu_instance_type\n\n: The type of the GPU EC2 instances used for the deployment.\n\nresources\n\n: Optional resources dictionary used for multi-model endpoint deployments.\n\nendpoint_type\n\n: This can either be\n\nMODEL_BASED\n\nor\n\nINFERENCE_COMPONENT\n\n, determining whether the endpoint includes an inference component.\n\nThe method delegates the actual deployment process to the\n\ndeployment_service\n\n. This delegation is a critical aspect of the strategy pattern, allowing for flexibility in how the deployment is carried out without altering the high- level deployment logic.\n\ntry\n\n:\n\nself\n\n.deployment_service.deploy( role_arn=role_arn, llm_image=llm_image, config=config, endpoint_name=endpoint_name, endpoint_config_name=endpoint_config_name, gpu_instance_type=gpu_instance_type, resources=resources, endpoint_type=endpoint_type, ) logger.info(\n\n\"Deployment completed successfully.\"\n\n)\n\nexcept\n\nException\n\nas\n\ne: logger.error(\n\nf\"Error during deployment:\n\n{e}\n\n\"\n\n)\n\nraise\n\nAlso, let’s review the resource configuration to understand the infrastructure better. These resources are leveraged when setting up multi- endpoint configurations that use multiple replicas to serve clients while respecting the latency and throughput requirements of the application. The\n\nResourceRequirements\n\nobject is initialized with a dictionary that specifies various resource parameters. These parameters include the number of replicas (copies) of the model to be deployed, the number of GPUs required, the number of CPU cores, and the memory allocation in megabytes. Each of these parameters plays a crucial role in the performance and scalability of the deployed model.\n\nfrom\n\nsagemaker.compute_resource_requirements.resource_requirements\n\nimport\n\nResourceRequirements model_resource_config = ResourceRequirements( requests={\n\n\"copies\"\n\n: settings.COPIES,\n\n\"num_accelerators\"\n\n: settings.GPUS\n\n\"num_cpus\"\n\n: settings.CPUS,\n\n\"memory\"\n\n:\n\n5\n\n\n\n1024\n\n}, )\n\nIn the preceding snippet,\n\nResourceRequirements\n\nis configured with four key parameters:\n\ncopies: This parameter determines how many instances or replicas of the model should be deployed. Having multiple replicas can help in reducing latency and increasing throughput.\n\nnum_accelerators: This parameter specifies the number of GPUs to allocate. Since LLMs are computationally intensive, multiple GPUs are typically required to accelerate inference processes.\n\nnum_cpus: This defines the number of CPU cores the deployment should have. The number of CPUs impacts the model’s ability to handle data preprocessing, post-processing, and other tasks that are less GPU-dependent but still essential.\n\nmemory: The memory parameter sets the minimum amount of RAM required for the deployment. Adequate memory is necessary to ensure the model can load and operate without running into memory shortages.\n\nBy setting these parameters, the class ensures that it has sufficient resources to operate efficiently when the model is deployed to a SageMaker endpoint. The precise tuning of these values will vary depending on the LLM’s\n\nspecific requirements, such as its size, the complexity of the tasks it will perform, and the expected load. To get a better understanding of how to use them, after deploying the endpoint, we suggest modifying them and seeing how the performance of the LLM microservice changes.\n\nUltimately, let’s review the settings configuring the LLM engine. The\n\nHF_MODEL_ID\n\nidentifies which Hugging Face model to deploy. For example, in the settings class, we set it to\n\nmlabonne/TwinLlama-3.1-8B-13\n\nto load our custom LLM Twin model stored in Hugging Face.\n\nSM_NUM_GPUS\n\nspecifies the number of GPUs allocated per model replica, which is crucial for fitting your model into the GPU’s VRAM.\n\nHUGGING_FACE_HUB_TOKEN\n\nprovides access to the Hugging Face Hub for model retrieval.\n\nHF_MODEL_QUANTIZE\n\nspecifies what quantization technique to use, while the rest of the variables control the LLM token generation process.\n\nhugging_face_deploy_config = {\n\n\"HF_MODEL_ID\"\n\n: settings.HF_MODEL_ID,\n\n\"SM_NUM_GPUS\"\n\n: json.dumps(settings.SM_NUM_GPUS),\n\n# Number of GPU used per replica\n\n\"MAX_INPUT_LENGTH\"\n\n: json.dumps(settings.MAX_INPUT_LENGTH),\n\n# Max length of input text\n\n\"\n\nMAX_TOTAL_TOKENS\"\n\n: json.dumps(settings.MAX_TOTAL_TOKENS),\n\n# Max length of the generation (including input text)\n\n\"MAX_BATCH_TOTAL_TOKENS\"\n\n: json.dumps(settings.MAX_BATCH_TOTAL_TOKENS),\n\n\"HUGGING_FACE_HUB_TOKEN\"\n\n: settings.HUGGINGFACE_ACCESS_TOKEN,\n\n\"MAX_BATCH_PREFILL_TOKENS\"\n\n:\n\n\"10000\"\n\n, \"HF_MODEL_QUANTIZE\":\n\n\"bitsandbytes\"\n\n, }\n\nUsing these two configurations, we fully control our infrastructure, what LLM to use, and how it behaves. To start the SageMaker deployment with the configuration shown above, call the\n\ncreate_endpoint()\n\nfunction (presented at the beginning of the section) as follows:\n\ncreate_endpoint(endpoint_type=EndpointType.MODEL_BASED)\n\nFor convenience, we also wrapped it up under a\n\npoe\n\ncommand:\n\npoetry poe deploy-inference-endpoint\n\nThat’s all you need to deploy an inference pipeline to AWS SageMaker. The hardest part is finding the correct configuration to fit your needs while reducing your infrastructure’s costs. Depending on AWS, this will take up to 15-30 minutes to deploy. You can always change any value directly from your\n\n.env\n\nfile and deploy the model with a different configuration without touching the code. For example, our default values use a single GPU instance of type\n\nml.g5.xlargeGPU\n\n. If you want more replicas, you can tweak the\n\nGPUS\n\nand\n\nSM_NUM_GPUS\n\nsettings or change your instance type by changing the\n\nGPU_INSTANCE_TYPE\n\nvariable.\n\nBefore deploying the LLM microservice to AWS SageMaker, ensure that you’ve generated a user role by running\n\npoetry poe create-sagemaker-role\n\nand an execution role by running\n\npoetry poe create-sagemaker-execution-role\n\n. Also, ensure you update your\n\nAWS_*\n\nenvironment variables in your\n\n.env\n\nfile with the credentials generated by the two steps. You can find more details on this aspect in the repository’s README file.\n\nAfter deploying the AWS SageMaker Inference endpoint, you can navigate to the SageMaker dashboard in AWS to visualize it. First, in the left panel, click on SageMaker dashboard, and then in the Inference column, click on the Endpoints button, as illustrated in Figure 10.6.\n\nFigure 10.6: AWS SageMaker Inference endpoints example\n\nAfter clicking the Endpoints button, you will see your twin endpoint in a Creating or Created status, as seen in Figure 10.7. After clicking on it, you can look at the endpoint’s logs in CloudWatch and monitor the CPU, memory, disk, and GPU utilization.\n\nAlso, they provide an excellent way to monitor all the HTTP errors, such as\n\n4XX\n\nand\n\n5XX\n\n, in one place.\n\nFigure 10.7: AWS SageMaker twin inference endpoint example\n\nOceanofPDF.com\n\nCalling the AWS SageMaker Inference endpoint\n\nNow that our LLM service has been deployed on AWS SageMaker, let’s learn how to call the service. To do so, we will write two classes that will help us prepare the prompt for SageMaker, call the inference endpoint through HTTP requests, and decode the results in a way the client can work with. All the AWS SageMaker Inference code is available on GitHub at\n\nllm_engineering/model/inference\n\n. It all starts with the following example:\n\ntext =\n\n\"Write me a post about AWS SageMaker inference endpoints.\"\n\nllm = LLMInferenceSagemakerEndpoint( endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE ) Answer = InferenceExecutor(llm, text).execute()\n\nAs before, we will walk you through the\n\nLLMInferenceSagemakerEndpoint\n\nand\n\nInferenceExecutor\n\nclasses. Let’s start with the\n\nLLMInferenceSagemakerEndpoint\n\nclass, which directly interacts with SageMaker. The constructor initializes all the essential attributes necessary to interact with the SageMaker endpoint:\n\nclass\n\nLLMInferenceSagemakerEndpoint\n\n(\n\nInference\n\n):\n\ndef\n\n__init__\n\n(\n\nself,\n\nendpoint_name:\n\nstr\n\n,\n\ndefault_payload:\n\nOptional\n\n[\n\nDict\n\n[\n\nstr\n\n,\n\nAny\n\n]] =\n\nNone\n\n,\n\ninference_component_name:\n\nOptional\n\n[\n\nstr\n\n] =\n\nNone\n\n,\n\n) ->\n\nNone\n\n:\n\nsuper\n\n().__init__()\n\nself\n\n.client = boto3.client(\n\n\"sagemaker-runtime\"\n\n,\n\nregion_name=settings.AWS_REGION,\n\naws_access_key_id=settings.AWS_ACCESS_KEY,\n\naws_secret_access_key=settings.AWS_SECRET_KEY,\n\n)\n\nself\n\n.endpoint_name = endpoint_name\n\nself\n\n.payload = default_payload\n\nif\n\ndefault_payload\n\nelse\n\nself\n\n._default_payload()\n\nself\n\n.inference_component_name = inference_component_name\n\nendpoint_name\n\nis crucial for identifying the SageMaker endpoint we want to request. Additionally, the method initializes the payload using a provided value or by calling a method that generates a default payload if none is provided.\n\nOne of the key features of the class is its ability to generate a default payload for inference requests. This is handled by the\n\n_default_payload()\n\nmethod:\n\ndef\n\n_default_payload\n\n(\n\nself\n\n) ->\n\nDict\n\n[\n\nstr\n\n,\n\nAny\n\n]:\n\nreturn\n\n{\n\n\"inputs\"\n\n:\n\n\"\"\n\n,\n\n\"parameters\"\n\n: {\n\n\"max_new_tokens\"\n\n: settings.MAX_NEW_TOKENS_INFERENCE,\n\n\"top_p\"\n\n: settings.TOP_P_INFERENCE,\n\n\"temperature\"\n\n: settings.TEMPERATURE_INFERENCE,\n\n\"return_full_text\"\n\n:\n\nFalse\n\n, }, }\n\nThis method returns a dictionary that represents the default structure of the payload to be sent for inference. The parameters section includes settings that influence the model’s behavior during inference, such as the number of tokens to generate, the sampling strategy (\n\ntop_p\n\n), and the temperature setting, which controls randomness in the output. These parameters are fetched from the application’s settings, ensuring consistency across different inference tasks.\n\nThe class allows customization of the payload through the\n\nset_payload()\n\nmethod, which enables the user to modify the inputs and parameters before sending an inference request:\n\ndef\n\nset_payload\n\n(\n\nself, inputs:\n\nstr\n\n, parameters:\n\nOptional\n\n[\n\nDict\n\n[\n\nstr\n\n,\n\nAny\n\n]] =\n\nNone\n\n) ->\n\nNone\n\n:\n\nself\n\n.payload[\n\n\"inputs\"\n\n] = inputs\n\nif\n\nparameters:\n\nself\n\n.payload[\n\n\"parameters\"\n\n].update(parameters)\n\nThis method updates the\n\ninputs\n\nfield of the payload with the new input text provided by the user. Additionally, it allows for modifying inference parameters if any are provided.\n\nUltimately, we leverage the\n\ninference()\n\nmethod to call the SageMaker endpoint with the customized payload:\n\ndef\n\ninference\n\n(\n\nself\n\n) ->\n\nDict\n\n[\n\nstr\n\n,\n\nAny\n\n]:\n\ntry\n\n: logger.info(\n\n\"Inference request sent.\"\n\n) invoke_args = {\n\n\"EndpointName\"\n\n:\n\nself\n\n.endpoint_name,\n\n\"ContentType\"\n\n:\n\n\"application/json\"\n\n,\n\n\"Body\"\n\n: json.dumps(\n\nself\n\n.payload), }\n\nif\n\nself\n\n.inference_component_name\n\nnot\n\nin\n\n[\n\n\"None\"\n\n,\n\nNone\n\n]: invoke_args[\n\n\"InferenceComponentName\"\n\n] =\n\nself\n\n.inference_component_name response =\n\nself\n\n.client.invoke_endpoint(**invoke_args) response_body = response[\n\n\"Body\"\n\n].read().decode(\n\n\"utf8\"\n\n)\n\nreturn\n\njson.loads(response_body)\n\nexcept\n\nException: logger.exception(\n\n\"SageMaker inference failed.\"\n\n)\n\nraise\n\nIn this method, the inference method constructs the request to be sent to the SageMaker endpoint. The method packages the payload and other necessary details into a format SageMaker expects. If an\n\ninference_component_name\n\nis specified, it is included in the request, allowing for more granular control over the inference process if needed. The request is sent using the\n\ninvoke_endpoint()\n\nfunction, and the response is read, decoded, and returned as a JSON object.\n\nLet’s understand how the\n\nInferenceExecutor\n\nuses the\n\nLLMInferenceSagemakerEndpoint\n\nclass we previously presented to send HTTP requests to the AWS SageMaker endpoint.\n\nThe\n\nInferenceExecutor\n\nclass begins with the constructor, which inputs key parameters for calling the LLM. The\n\nllm\n\nparameter accepts any instance that implements the Inference interface, such as the\n\nLLMInferenceSagemakerEndpoint\n\nclass, which is used to perform the inference.\n\nAlso, it accepts the query parameter, which represents the user input. Ultimately, it takes an optional context field if you want to do RAG, and you can customize the prompt template. If no prompt template is provided, it will default to a generic version that is not specialized in any LLM:\n\nclass\n\nInferenceExecutor\n\n:\n\ndef\n\n__init__\n\n(\n\nself,\n\nllm: Inference,\n\nquery:\n\nstr\n\n,\n\ncontext:\n\nstr\n\n|\n\nNone\n\n=\n\nNone\n\n,\n\nprompt:\n\nstr\n\n|\n\nNone\n\n=\n\nNone\n\n,\n\n) ->\n\nNone\n\n:\n\nself\n\n.llm = llm\n\nself\n\n.query = query\n\nself\n\n.context = context\n\nif\n\ncontext\n\nelse\n\n\"\"\n\nif\n\nprompt\n\nis\n\nNone\n\n:\n\nself\n\n.prompt =\n\n\"\"\"\n\nYou are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.\n\nUser query: {query}\n\nContext: {context}\n\n\"\"\"\n\nelse\n\n:\n\nself\n\n.prompt = prompt\n\nThe\n\nexecute()\n\nmethod is the key component of the\n\nInferenceExecutor\n\nclass. This method is responsible for actually performing the inference. When execute is called, it prepares the payload sent to the LLM by formatting the prompt with the user’s query and context.\n\nThen, it configures several parameters that influence the behavior of the LLM, such as the maximum number of new tokens the model is allowed to generate, a repetition penalty to discourage the model from generating repetitive text, and the temperature setting that controls the randomness of the output.\n\nOnce the payload and parameters are set, the method calls the\n\ninference\n\nfunction from\n\nLLMInferenceSagemakerEndpoint\n\nand waits for the generated answer:\n\ndef\n\nexecute\n\n(\n\nself\n\n) ->\n\nstr\n\n:\n\nself\n\n.llm.set_payload( inputs=\n\nself\n\n.prompt.\n\nformat\n\n(query=\n\nself\n\n.query, context=\n\nself\n\n.context), parameters={\n\n\"max_new_tokens\"\n\n: settings.MAX_NEW_TOKENS_INFERENCE,\n\n\"repetition_penalty\"\n\n:\n\n1.1\n\n,\n\n\"temperature\"\n\n: settings.TEMPERATURE_INFERENCE, }, ) answer =\n\nself\n\n.llm.inference()[\n\n0\n\n][\n\n\"generated_text\"\n\n]\n\nreturn\n\nanswer\n\nBy making the inference through an object that implements the Inference interface we decouple, we can easily inject other Inference strategies and the\n\nLLMInferenceSagemakerEndpoint\n\nimplementation presented above without modifying different parts of the code.\n\nRunning a test example is straightforward. Simply call the following Python file, as shown below:\n\npoetry run python -m llm_engineering.model.inference.test\n\nAlso, for convenience, we wrap it under a\n\npoe\n\ncommand:\n\npoetry poe test-sagemaker-endpoint\n\nNow, we must understand how we implement the business microservice using FastAPI. This microservice will send HTTP requests to the LLM microservice defined above and call the RAG retrieval module implemented in Chapter 9.\n\nOceanofPDF.com\n\nBuilding the business microservice using FastAPI\n\nTo implement a simple FastAPI application that proves our deployment strategy, we first have to define a FastAPI instance as follows:\n\nfrom\n\nfastapi\n\nimport\n\nFastAPI app = FastAPI()\n\nNext, we define the\n\nQueryRequest\n\nand\n\nQueryResponse\n\nclasses using Pydantic’s\n\nBaseModel\n\n. These classes represent the request and response structure for the FastAPI endpoints:\n\nclass\n\nQueryRequest\n\n(\n\nBaseModel\n\n): query:\n\nstr\n\nclass\n\nQueryResponse\n\n(\n\nBaseModel\n\n): answer:\n\nstr\n\nNow that we’ve defined our FastAPI components and have all the SageMaker elements in place, let’s reiterate over the\n\ncall_llm_service()\n\nand\n\nrag()\n\nfunctions we’ve presented in Chapter 9 and couldn’t run because we haven’t deployed our fine-tuned LLM. Thus, as a refresher, the\n\ncall_llm_service()\n\nfunction wraps the inference logic used to call the SageMaker LLM microservice:\n\ndef\n\ncall_llm_service\n\n(\n\nquery:\n\nstr\n\n, context:\n\nstr\n\n|\n\nNone\n\n) ->\n\nstr\n\n: llm = LLMInferenceSagemakerEndpoint( endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_component_name=\n\nNone\n\n) answer = InferenceExecutor(llm, query, context).execute()\n\nreturn\n\nanswer\n\nNext, we define the\n\nrag()\n\nfunction that implements all the RAG business logic. To avoid repeating ourselves, check Chapter 9 for the complete function explanation. What is important to highlight is that the\n\nrag()\n\nfunction only implements the business steps required to do RAG, which are CPU- and I/O-bounded. For example, the\n\nContextRetriever\n\nclass makes API calls to OpenAI and Qdrant, which are network I/O bounded, and calls the embedding model, which runs directly on the CPU. Also, as the LLM inference logic is moved to a different microservice, the\n\ncall_llm_service()\n\nfunction is only network I/O bounded. To conclude, the whole function is light to run, where the heavy computing is done on other services, which\n\nallows us to host the FastAPI server on a light and cheap machine that doesn’t need a GPU to run at low latencies:\n\ndef\n\nrag\n\n(\n\nquery:\n\nstr\n\n) ->\n\nstr\n\n: retriever = ContextRetriever(mock=\n\nFalse\n\n) documents = retriever.search(query, k=\n\n3\n\n\n\n3\n\n) context = EmbeddedChunk.to_context(documents) answer = call_llm_service(query, context)\n\nreturn\n\nanswer\n\nUltimately, we define the\n\nrag_endpoint()\n\nfunction, used to expose our RAG logic over the internet as an HTTP endpoint. We use a Python decorator to expose it as a POST endpoint in the FastAPI application. This endpoint is mapped to the\n\n/rag\n\nroute and expects a\n\nQueryRequest\n\nas input. The function processes the request by calling the rag function with the user’s query. If successful, it returns the answer wrapped in a\n\nQueryResponse\n\nobject. If an exception occurs, it raises an HTTP 500 error with the exception details:\n\n@app.post(\n\n\"/rag\"\n\n, response_model=QueryResponse\n\n)\n\nasync\n\ndef\n\nrag_endpoint\n\n(\n\nrequest: QueryRequest\n\n):\n\ntry\n\n: answer = rag(query=request.query)\n\nreturn\n\n{\n\n\"answer\"\n\n: answer}\n\nexcept\n\nException\n\nas\n\ne:\n\nraise\n\nHTTPException(status_code=\n\n500\n\n, detail=\n\nstr\n\n(e))\n\nfrom\n\ne\n\nThis FastAPI application demonstrates how to effectively integrate an LLM hosted on AWS SageMaker into a web service, utilizing RAG to enhance the relevance of the model’s responses. The code’s modular design, leveraging custom classes like\n\nContextRetriever\n\n,\n\nInferenceExecutor\n\n, and\n\nLLMInferenceSagemakerEndpoint\n\n, allows for easy customization and scalability, making it a powerful tool for deploying ML models in production environments.\n\nWe will leverage the\n\nuvicorn\n\nweb server, the go-to method for FastAPI applications, to start the server. To do so, you have to run the following:\n\nuvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload\n\nAlso, you can run the following\n\npoe\n\ncommand to achieve the same:\n\npoetry poe run-inference-ml-service\n\nTo call the\n\n/rag\n\nendpoint, we can leverage the\n\ncurl\n\nCLI command to make a POST HTTP request to our FastAPI server, as follows:\n\ncurl -X POST 'http://127.0.0.1:8000/rag' -H 'Content-Type: application/json' -d '{\\\"query\\\": \\\"your_query \\\"}'\n\nAs usual, we provided an example using a\n\npoe\n\ncommand that contains an actual user query:\n\npoetry poe call-inference-ml-service\n\nThis FastAPI server runs only locally. The next step would be to deploy it to AWS Elastic Kubernetes Service (EKS), a self-hosted version of Kubernetes by AWS. Another option would be to deploy it to AWS Elastic Container Service (ECS), which is similar to AWS EKS but doesn’t use Kubernetes under the hood but AWS’s implementation. Unfortunately, this is not specific to LLMs or LLMOps. Hence, we won’t go through these steps in this book. But to get an idea of what you must do, you must create an AWS EKS/ECS cluster from the dashboard or leverage an infrastructure-as-code (IaC) tool such as Terraform. After that, you will have to Dockerize the FastAPI code presented above. Ultimately, you would have to push the Docker image to AWS ECR and create an ECS/EKR deployment using the Docker image hosted on ECR. If this sounds like a lot, the good news is that we will walk you through a similar example in Chapter 11, where we will deploy the ZenML pipelines to AWS.\n\nOnce you’re done testing your inference pipeline deployment, deleting all your AWS SageMaker resources used to deploy the LLM is essential. As almost all AWS resources use a pay-as-you-go strategy, using SageMaker for a few hours wouldn’t break your wallet, but if you forget and leave it open, in a few days, the costs can grow exponentially. Thus, a good rule of thumb is to always delete everything after you’re done testing your SageMaker infrastructure (or any AWS resource). Luckily, we have provided a script that deletes all the AWS SageMaker resources for you:\n\npoetry poe delete-inference-endpoint\n\nTo ensure everything was correctly deleted, go to your SageMaker dashboard and check it yourself.\n\nOceanofPDF.com\n\nAutoscaling capabilities to handle spikes in usage\n\nSo far, the SageMaker LLM microservice has used a static number of replicas to serve our users, which means that all the time, regardless of the traffic, it has the same number of instances up and running. As we highlighted throughout this book, machines with GPUs are expensive. Thus, we lose a lot of money during downtime when most replicas are idle. Also, if our application has sudden spikes in traffic, the application will perform poorly as the server cannot handle the number of requests. This is a massive problem for the user experience of our application, as in those spikes, we bring in the majority of new users. Thus, if they have a terrible impression of our product, we significantly reduce their chance of returning to our platform.\n\nPreviously, we configured our multi-endpoint service using the\n\nResourceRequirements\n\nclass from SageMaker. For example, let’s assume we requested four copies (replicas) with the following compute requirements:\n\nmodel_resource_config = ResourceRequirements( requests={\n\n\"copies\"\n\n:\n\n4\n\n,\n\n# Number of replicas.\n\n\"num_accelerators\"\n\n:\n\n4\n\n,\n\n# Number of GPUs required.\n\n\"num_cpus\"\n\n:\n\n8\n\n,\n\n# Number of CPU cores required.\n\n\"memory\"\n\n:\n\n5\n\n\n\n1024\n\n,\n\n# Minimum memory required in Mb (required)\n\n}, )\n\nUsing this configuration, we always have four replicas serving the clients, regardless of idle time or spikes in traffic. The solution is to implement an autoscaling strategy that scales the number of replicas up and down dynamically based on various metrics, such as the number of requests.\n\nFor example, Figure 10.8 shows a standard architecture where the SageMaker Inference endpoints scale in and out based on the number of requests. When there is no traffic, we can have one online replica so the server remains responsive to new user requests or even scales down to zero if the latency is not super critical. Then, let’s assume that when we have around 10 requests per second, we have to keep two replicas online, and when the number of requests spikes to 100 per second, the autoscaling service should spin up to 20 replicas to keep up with the demand. Note that these are fictional numbers that should be adapted to your specific use case.\n\nFigure 10.8: Autoscaling possible use cases\n\nWithout going into the little details of cloud networking, when working with multi-replica systems, between the client and the replicas sits an Application Load Balancer (ALB) or another type of load balancer.\n\nAll the requests first go to the ALB, which knows to route them to a replica. The ALB can adopt various routing strategies, where the simplest one is called round robin, which sequentially sends a request to each replica. For example, the first request is routed to replica one, the second to replica two, and so on. Taking this approach, regardless of how many replicas you have online, the endpoint that the client calls is always represented by the load balancer that acts as an entry point into your cluster. Thus, adding or removing new replicas doesn’t affect the server and client communication protocol.\n\nLet’s quickly learn how to implement an autoscaling strategy for our AWS SageMaker Inference endpoint. SageMaker provides a feature called Application Auto Scaling that allows you to scale resources dynamically based on pre-defined policies. Two foundational steps are involved in effectively leveraging this functionality: registering a scalable target and creating a scalable policy.\n\nOceanofPDF.com\n\nRegistering a scalable target\n\nThe first step in enabling autoscaling for your resources is to register a scalable target with the Application Auto Scaling feature AWS provides. Think of this as informing AWS about the specific resource you intend to scale, as well as setting the boundaries within which the scaling should occur. However, this step does not dictate how or when the scaling should happen.\n\nFor instance, when working with SageMaker Inference components, you’ll define the following:\n\nResource ID: This serves as a unique identifier for the resource you want to scale, typically including the name of the SageMaker Inference component.\n\nService namespace: This identifies the AWS service the resource belongs to, which, in this case, is SageMaker.\n\nScalable dimension: This specifies the resources to be scaled, such as the desired number of copies.\n\nMinCapacity and MaxCapacity: These parameters define the boundaries of the autoscaling strategies, such as minimum and\n\nmaximum limits of the number of replicas.\n\nBy registering a scalable target, you prepare your SageMaker Inference component for future scaling actions without determining when or how these actions should occur.\n\nOceanofPDF.com\n\nCreating a scalable policy\n\nOnce your scalable target is registered, the next step is defining how the scaling should occur. This is where creating a scaling policy comes in. A scaling policy defines specific rules that trigger scaling events. When creating policies, you have to define metrics to know what to monitor and thresholds to know when to emit scaling events.\n\nIn the context of our SageMaker Inference component, the scalable policy might include the following elements:\n\nPolicy type: For instance, you might select TargetTrackingScaling, a policy that adjusts the resource’s capacity to maintain a specific target value for a chosen metric.\n\nTarget tracking configuration: This involves selecting the metric to monitor (such as SageMakerInferenceComponentInvocationsPerCopy), setting the desired target value, and specifying cooldown periods that control how quickly scaling actions can occur after previous ones.\n\nThe scaling policy defines the rules for your scaling-in and scaling-out strategy. It constantly monitors the specified metric, and depending on whether the metric exceeds or falls below the target value, it triggers actions\n\nto scale the number of inference component copies up or down, always within the limits defined by the registered scalable target.\n\nLet’s explain in more depth how the TargetTrackingScaling policy works. Imagine you have a metric that represents the ideal average utilization or throughput level for your application. With target tracking, you select this metric and set a target value that reflects the optimal state for your application. Once defined, Application Auto Scaling creates and manages the necessary CloudWatch alarms to monitor this metric. When deviations occur, scaling actions are triggered, similar to how a thermostat adjusts to maintain a consistent room temperature.\n\nFor instance, consider an application running on SageMaker. Let’s assume we set a target of keeping GPU utilization around 70 percent. This target allows you to maintain enough headroom to manage sudden traffic spikes while preventing the unnecessary cost of idle resources. When GPU usage exceeds the target, the system scales out, adding resources to manage the increased load. Conversely, when GPU usage drops below the target, the system scales in, reducing capacity to minimize costs during quieter periods.\n\nOne significant advantage of setting up target tracking policies using Application Auto Scaling is that they simplify the scaling process. You no longer need to configure CloudWatch alarms and define scaling adjustments manually.\n\nOceanofPDF.com\n\nMinimum and maximum scaling limits\n\nWhen setting up autoscaling for your SageMaker Inference endpoints, it’s crucial to establish your maximum and minimum scaling limits before creating your scaling policy. The minimum value represents the least resources your model can operate with. This value must be at least 1, ensuring that your model always has some capacity.\n\nNext, configure the maximum value, which defines the upper limit of resources your model can scale up to. While the maximum must be equal to or greater than the minimum value, it doesn’t impose any upper limit. Thus, you can scale up as much as your application needs within the boundaries of what AWS can provide.\n\nOceanofPDF.com\n\nCooldown period\n\nAnother important aspect of a scaling policy is the cooldown period, during which it’s crucial to maintain a balance between responsiveness and stability. This cooldown period acts as a safeguard, ensuring that your system doesn’t overreact during scaling events—whether it’s reducing capacity (scaling in) or increasing it (scaling out). By introducing a calculated pause, the cooldown period prevents rapid fluctuations in the number of instances. Specifically, it delays the removal of instances during scale-in requests and restricts the creation of new replicas during scale-out requests. This strategy helps maintain a stable and efficient environment for LLM service.\n\nThese practical basics are used in autoscaling most web servers, including online real-time ML servers. Once you understand how to configure scaling policies for SageMaker, you can immediately apply the strategies you’ve learned to other popular deployment tools like Kubernetes or AWS ECS.\n\nFor a step-by-step guideline on how to configure autoscaling for the AWS SagaMaker endpoint implemented in this chapter, you can follow this official tutorial from AWS: https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling- prerequisites.html.\n\nAutoscaling is a critical component in any cloud architecture, but there are some pitfalls you should be aware of. The first and most dangerous one is over-scaling, which directly impacts the costs of your infrastructure. If your scaling policy or cooldown period is too sensitive, you may be uselessly spinning up new machines that will remain idle or with the resources underused. The second reason is on the other side of the spectrum, where your system doesn’t scale enough, resulting in a bad user experience for the end user.\n\nThat’s why a good practice is to understand the requirements of your system. Based on them, you should tweak and experiment with the autoscaling parameters in a dev or test environment until you find the sweet spot (similar to hyperparameter tuning when training models). Let’s suppose, for instance, that you expect your system to support an average of 100 users per minute and scale up to 10,000 users per minute in case of an outlier event such as a holiday. Using this spec, you can stress test your system and monitor your resources to find the best trade-off between costs, latency, and throughput that supports standard and outlier use cases.\n\nOceanofPDF.com\n\nSummary\n\nIn this chapter, we learned what design decisions to make before serving an ML model, whether an LLM or not, by walking you through the three fundamental deployment types for ML models: online real-time inference, asynchronous inference, and offline batch transform. Then, we considered whether building our ML-serving service as a monolith application made sense or splitting it into two microservices, such as an LLM microservice and a business microservice. To do this, we weighed the pros and cons of a monolithic versus microservices architecture in model-serving.\n\nNext, we walked you through deploying our fine-tuned LLM Twin to an AWS SageMaker Inference endpoint. We also saw how to implement the business microservice using FastAPI, which consists of all the RAG steps based on the retrieval module implemented in Chapter 9 and the LLM microservice deployed on AWS SageMaker. Ultimately, we explored why we have to implement an autoscaling strategy. We also reviewed a popular autoscaling strategy that scales in and out based on a given set of metrics and saw how to implement it in AWS SageMaker.\n\nIn the next chapter, we will learn about the fundamentals of MLOps and LLMOps and then explore how to deploy the ZenML pipelines to AWS and implement a continuous training, continuous integration, and continuous delivery (CT/CI/CD) and monitoring pipeline.\n\nOceanofPDF.com\n\nReferences\n\nAWS Developers. (2023, September 22). Machine Learning in 15: Amazon SageMaker High-Performance Inference at Low Cost [Video]. YouTube. https://www.youtube.com/watch?v=FRbcb7CtIOw\n\nbitsandbytes-foundation. (n.d.). GitHub—bitsandbytes- foundation/bitsandbytes: Accessible large language models via k-bit quantization for PyTorch. GitHub. https://github.com/bitsandbytes- foundation/bitsandbytes\n\nDifference between IAM role and IAM user in AWS. (n.d.). Stack Overflow. https://stackoverflow.com/questions/46199680/difference-between-iam-role- and-iam-user-in-aws\n\nHuggingface. (n.d.-a). GitHub—huggingface/safetensors: Simple, safe way to store and distribute tensors. GitHub. https://github.com/huggingface/safetensors\n\nHuggingface. (n.d.-b). GitHub—huggingface/text-generation-inference: Large Language Model Text Generation Inference. GitHub. https://github.com/huggingface/text-generation-inference\n\nHuyen, C. (n.d.). Designing machine learning systems. O’Reilly Online Learning. https://www.oreilly.com/library/view/designing-machine- learning/9781098107956/\n\nIusztin, P. (2024, August 20). Architect LLM & RAG inference pipelines | Decoding ML. Medium. https://medium.com/decodingml/architect- scalable-and-cost-effective-llm-rag-inference-pipelines-73b94ef82a99\n\nLakshmanan, V., Robinson, S., and Munn, M. (n.d.). Machine Learning design patterns. O’Reilly Online Learning. https://www.oreilly.com/library/view/machine-learning- design/9781098115777/\n\nMendoza, A. (2024, August 21). Best tools for ML model Serving. neptune.ai. https://neptune.ai/blog/ml-model-serving-best-tools\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\n11\n\nOceanofPDF.com\n\nMLOps and LLMOps\n\nThroughout the book, we’ve already used machine learning operations (MLOps) components and principles such as a model registry to share and version our fined-tuned large language models (LLMs), a logical feature store for our fine-tuning and RAG data, and an orchestrator to glue all our ML pipelines together. But MLOps is not just about these components; it takes an ML application to the next level by automating data collection, training, testing, and deployment. Thus, the end goal of MLOps is to automate as much as possible and let users focus on the most critical decisions, such as when a change in distribution is detected and a decision must be taken on whether it is essential to retrain the model or not. But what about LLM operations (LLMOps)? How does it differ from MLOps?\n\nThe term LLMOps is a product of the widespread adoption of LLMs. It is built on top of MLOps, which is built on top of development operations (DevOps). Thus, to fully understand what LLMOps is about, we must provide a historical context, starting with DevOps and building on the term from there—which is precisely what this chapter will do. At its core, LLMOps focuses on problems specific to LLMs, such as prompt monitoring and versioning, input and output guardrails to prevent toxic behavior, and feedback loops to gather fine-tuning data. It also focuses on scaling issues that appear when working with LLMs, such as collecting trillions of tokens for training datasets, training models on massive GPU clusters, and reducing infrastructure costs. Fortunately for the common folk, these issues are solved mainly by a few companies that fine-tune foundational models, such as Meta, which provides the Llama family of models. Most companies will adopt these pre-trained foundational models for their use cases, focusing on LLMOps problems such as prompt monitoring and versioning.\n\nOn the implementation side of things, to add LLMOps to our LLM Twin use case, we will deploy all our ZenML pipelines to AWS. We will implement a continuous integration and continuous deployment (CI/CD) pipeline to test the integrity of our code and automate the deployment process, a continuous training (CT) pipeline to automate our training, and a monitoring pipeline to track all our prompts and generated answers. This is a natural progression in any ML project, regardless of whether you use LLMs.\n\nIn previous chapters, you learned how to build an LLM application. Now, it’s time to explore three main goals related to LLMOps. The first one is to gain a theoretical understanding of LLMOps, starting with DevOps, then moving to the fundamental principles of MLOps, and finally, digging into LLMOps. We don’t aim to provide the whole theory on DevOps, MLOps, and LLMOps, as you could easily write an entire book on these topics. However, we want to build a strong understanding of why we make certain decisions when implementing the LLM Twin use case.\n\nOur second goal is to deploy the ZenML pipelines to AWS (currently, we’ve deployed only our inference pipeline to AWS in Chapter 10). This section will be hands-on, showing you how to leverage ZenML to deploy everything to AWS. We need this to implement our third and last goal, which is to apply what we’ve learned in the theory section to our LLM Twin use case. We will implement a CI/CD pipeline using GitHub Actions, a CT and alerting pipeline using ZenML, and a monitoring pipeline using Opik from Comet ML.\n\nThus, in this chapter, we will cover the following topics:\n\nThe path to LLMOps: Understanding its roots in DevOps and MLOps\n\nDeploying the LLM Twin’s pipelines to the cloud\n\nAdding LLMOps to the LLM Twin\n\nOceanofPDF.com\n\nThe path to LLMOps: Understanding its roots in DevOps and MLOps\n\nTo understand LLMOps, we have to start with the field’s beginning, which is DevOps, as it inherits most of its fundamental principles from there. Then, we will move to MLOps to understand how the DevOps domain was adapted to support ML systems. Finally, we will explain what LLMOps is and how it emerged from MLOps after the widespread adoption of LLMs.\n\nOceanofPDF.com\n\nDevOps\n\nManually shipping software is time-consuming, error-prone, involves security risks, and doesn’t scale. Thus, DevOps was born to automate the process of shipping software at scale. More specifically, DevOps is used in software development, where you want to completely automate your building, testing, deploying, and monitoring components. It is a methodology designed to shorten the development lifecycle and ensure continuous delivery of high-quality software. It encourages collaboration, automates processes, integrates workflows, and implements rapid feedback loops. These elements contribute to a culture where building, testing, and releasing software becomes more reliable and faster.\n\nEmbracing a DevOps culture offers significant advantages to an organization, primarily boosting operational efficiency, speeding up feature delivery, and enhancing product quality. Some of the main benefits include:\n\nImproved collaboration: DevOps is pivotal in creating a more unified working environment. Eliminating the barriers between development and operations teams fosters enhanced communication and teamwork, leading to a more efficient and productive workplace.\n\nBoosted efficiency: Automating the software development lifecycle reduces manual tasks, errors, and delivery times.\n\nOngoing improvement: DevOps is not just about internal processes. It’s about ensuring that the software effectively meets user needs. Promoting a culture of continuous feedback enables teams to quickly adapt and enhance their processes, thereby delivering software that genuinely satisfies the end users.\n\nSuperior quality and security: DevOps ensures swift software development while maintaining high quality and security standards through CI/CD and proactive security measures.\n\nOceanofPDF.com\n\nThe DevOps lifecycle\n\nAs illustrated in Figure 11.1, the DevOps lifecycle encompasses the entire journey from the inception of software development to its delivery, upkeep, and security. The key stages of this lifecycle are:\n\nPlan: Organize and prioritize the tasks, ensuring each is tracked to completion.\n\nCode: Collaborate with your team to write, design, develop, and securely manage code and project data.\n\nBuild: Package your applications and dependencies into an executable format.\n\nTest: This stage is crucial. It’s where you confirm that your code functions correctly and meets quality standards, ideally through automated testing.\n\nRelease: If the tests pass, flag the tested build as a new release, which is now ready to be shipped.\n\nDeploy: Deploy the latest release to the end users.\n\nOperate: Manage and maintain the infrastructure on which the software runs effectively once it is live. This involves scaling, security, data management, and backup and recovery.\n\nMonitor: Track performance metrics and errors to reduce the severity and frequency of incidents.\n\nFigure 11.1: DevOps lifecycle steps\n\nOceanofPDF.com\n\nThe core DevOps concepts\n\nDevOps encompasses various practices throughout the application lifecycle, but the core ones that we will touch on throughout this book are:\n\nDeployment environments: To thoroughly test your code before shipping it to production, you must define multiple pre-production environments that mimic the production one. The most common approach is to create a dev environment where the developers can test their latest features. Then, you have a staging environment where the QA team and stakeholders tinker with the application to find bugs and experience the latest features before they ship to the users. Lastly, we have the production environment, which is exposed to end users.\n\nVersion control: Used to track, manage, and version every change made to the source code. This allows you to have complete control over the evolution of the code and deployment processes. For example, without versioning, tracking changes between the dev, staging, and production environments would be impossible. By versioning your software, you always know what version is stable and ready to be shipped.\n\nContinuous integration (CI): Before pushing the code into the dev, staging, and production main branches, you automatically build your application and run automated tests on each change. After all the automated tests pass, the feature branch can be merged into the main one.\n\nContinuous delivery (CD): Continuous delivery works in conjunction with CI and automates the infrastructure provisioning and application deployment steps. For example, after the code is merged into the staging environment, the application with the latest changes will be automatically deployed on top of your staging infrastructure. After, the QA team (or stakeholders) starts manually testing the latest features to verify that they work as expected. These two steps are commonly referred to together as CI/CD.\n\nNote that DevOps suggests a set of core principles that are platform/tool agnostic. However, within our LLM Twin use case, we will add a version control layer using GitHub, which aims to track the evolution of the code. Another popular tool for version control is GitLab. To implement the CI/CD pipeline, we will leverage the GitHub ecosystem and GitHub Actions, which are free for open-source projects. Other tool choices are GitLab CI/CD, CircleCI, and Jenkins. Usually, you pick the DevOps tool based on your development environment, customization, and privacy needs. For example, Jenkins is an open-source DevOps tool you can host yourself and control fully. The downside is that you must host and maintain it yourself, adding a complexity layer. Thus, many companies choose what works best with their version control ecosystem, such as GitHub Actions or GitLab CI/CD.\n\nNow that we’ve established a solid understanding of DevOps, let’s explore how the MLOps field has emerged to keep these same core principles in the AI/ML world.\n\nOceanofPDF.com\n\nMLOps\n\nAs you might have worked out by now, MLOps tries to apply the DevOps principles to ML. The core issue is that an ML application has many other moving parts compared to a standard software application, such as the data, model, and, finally, the code. MLOps aims to track, operationalize, and monitor all these concepts for better reproducibility, robustness, and control.\n\nIn ML systems, a build can be triggered by any change in these areas— whether it’s an update in the code, modifications in the data, or adjustments to the model.\n\nFigure 11.2: Relationship between data, model, and code changes\n\nIn DevOps, everything is centered around the code. For example, when a new feature is added to the codebase, you have to trigger the CI/CD pipeline. In MLOps, the code can remain unchanged while only the data changes. In that case, you must train (or fine-tune) a new model, resulting in a new dataset and model version. Intuitively, when one component changes, it affects one or more of the others. Thus, MLOps has to take into consideration all this extra complexity. Here are a few examples that can trigger a change in the data and indirectly in the model:\n\nAfter deploying the ML model, its performance might decay as time passes, so we need new data to retrain it.\n\nAfter understanding how to collect data in the real world, we might recognize that getting the data for our problem is challenging, so we need to re-formulate it to work with our real-world setup.\n\nWhile in the experimentation stage and training the model, we often must collect more data or re-label it, which generates a new set of models.\n\nAfter serving the model in the production environment and collecting feedback from the end users, we might recognize that the assumptions we made for training the model are wrong, so we must change our model.\n\nSo, what is MLOps?\n\nA more official definition of MLOps is the following: MLOps is the extension of the DevOps field that makes data and models their first-class citizen while preserving the DevOps methodology.\n\nLike DevOps, MLOps originates from the idea that isolating ML model development from its deployment process (ML operations) diminishes the system’s overall quality, transparency, and agility. With that in mind, an\n\noptimal MLOps experience treats ML assets consistently as other software assets within a CI/CD environment as part of a cohesive release process.\n\nOceanofPDF.com\n\nMLOps core components\n\nWe have already used all of these components throughout the book, but let’s have a quick refresher on the MLOps core components now that we better understand the field. Along with source control and CI/CD, MLOps revolves around:\n\nModel registry: A centralized repository for storing trained ML models (tools:Comet ML, W&B, MLflow, ZenML)\n\nFeature store: Preprocessing and storing input data as features for both model training and inference pipelines (tools:Hopsworks, Tecton, Featureform)\n\nML metadata store: This store tracks information related to model training, such as model configurations, training data, testing data, and performance metrics. It is mainly used to compare multiple models and look at the model lineages to understand how they were created (tools:Comet ML, W&B, MLflow)\n\nML pipeline orchestrator: Automating the sequence of steps in ML projects (tools:ZenML, Airflow, Prefect, Dagster)\n\nYou might have noticed an overlap between the MLOps components and its specific tooling. This is common, as most MLOps tools offer unified solutions, often called MLOps platforms.\n\nOceanofPDF.com\n\nMLOps principles\n\nSix core principles guide the MLOps field. These are independent of any tool and sit at the core of building robust and scalable ML systems.\n\nThey are:\n\nAutomation or operationalization: Automation in MLOps involves transitioning from manual processes to automated pipelines through CT and CI/CD. This enables the efficient retraining and deployment of ML models in response to triggers such as new data, performance drops, or unhandled edge cases. Moving from manual experimentation to full automation ensures that our ML systems are robust, scalable, and adaptable to changing requirements without errors or delays.\n\nVersioning: In MLOps, it is crucial to track changes in code, models, and data individually, ensuring consistency and reproducibility. Code is tracked using tools like Git, models are versioned through model registries, and data versioning can be managed using solutions like DVC or artifact management systems.\n\nExperiment tracking: As training ML models is an iterative and experimental process that involves comparing multiple experiments based on predefined metrics, using an experiment tracker to help us pick the best model is important. Tools like Comet ML, W&B, MLflow,\n\nand Neptune allow us to log all necessary information to compare experiments easily and select the best model for production.\n\nTesting: MLOps suggests that along with testing your code, you should also test your data and models through unit, integration, acceptance, regression, and stress tests. This ensures that each component functions correctly and integrates well, focusing on inputs, outputs, and handling edge cases.\n\nMonitoring: This stage is vital for detecting performance degradation in served ML models due to changes in production data, allowing timely intervention such as retraining, further prompt or feature engineering, or data validation. By tracking logs, system metrics, and model metrics and detecting drifts, we can maintain the health of ML systems in production, detect issues as fast as possible, and ensure they continue to deliver accurate results.\n\nReproducibility: This ensures that every process (such as training or feature engineering) within your ML systems produces identical results when given the same input by tracking all the moving variables, such as code versions, data versions, hyperparameters, or any other type of configurations. Due to the non-deterministic nature of ML training and inference, setting well- known seeds when generating pseudo-random numbers is essential to achieving consistent outcomes and making processes as deterministic as possible.\n\nIf you want to learn more, we’ve offered an in-depth exploration of these principles in the Appendix at the end of this book.\n\nOceanofPDF.com\n\nML vs. MLOps engineering\n\nThere is a fine line between ML engineering and MLOps. If we want to define a rigid job description for the two rules, it cannot be easy to completely differentiate what responsibilities go into ML engineering (MLE) and what goes into MLOps. I have seen many job roles that bucket the MLOps role with the platform and cloud engineers. From one perspective, that makes a lot of sense: as an MLOps engineer, you have a lot of work to do on the infrastructure side. On the other hand, as seen in this section, an MLOps engineer still has to implement things such as experiment tracking, model registries, versioning, and more. A good strategy would be to let the ML engineer integrate these into the code and the MLOps engineer focus on making them work on their infrastructure.\n\nAt a big corporation, ultimately, differentiating the two roles might make sense. But when working in small to medium-sized teams, you will wear multiple hats and probably work on the ML system’s MLE and MLOps aspects.\n\nFigure 11.3: DS vs. MLE vs. MLOps\n\nFor instance, in Figure 11.3, we see a clear division of responsibilities among the three key roles: data scientist/ML researcher, ML engineer, and MLOps engineer. The Data Scientist (DS) implements specific models to address problems.\n\nThe ML engineer takes the functional models from the DS team and constructs a layer on top of them, making them modular and\n\nextendable and providing access to a database (DB) or exposing them as an API over the internet. However, the MLOps engineer plays a pivotal role in this process. They take the code from this intermediate layer and place it on a more generic layer, the infrastructure. This action marks the application’s transition to production. From this point, we can start thinking about automation, monitoring, versioning, and more.\n\nThe intermediate layer differentiates a proof of concept from an actual product. In that layer, you design an extendable application that has a state by integrating a DB and is accessible over the internet through an API. When shipping the application on a specific infrastructure, you must consider scalability, latency, and cost-effectiveness. Of course, the intermediate and generic layers depend on each other, and often, you must reiterate to meet the application requirements.\n\nOceanofPDF.com\n\nLLMOps\n\nLLMOps encompasses the practices and processes essential for managing and running LLMs. This field is a specialized branch of MLOps, concentrating on the unique challenges and demands associated with LLMs. While MLOps addresses the principles and practices of managing various ML models, LLMOps focuses on the distinct aspects of LLMs, including their large size, highly complex training requirements, prompt management, and non-deterministic nature of generating answers. However, note that at its core, LLMOps still inherits all the fundamentals presented in the MLOps section. Thus, here, we will focus on what it adds on top.\n\nWhen training LLMs from scratch, the data and model dimensions of an ML system grow substantially, which is one aspect that sets LLMOps apart from MLOps. These are the main concerns when training LLMs from scratch:\n\nData collection and preparation involves collecting, preparing, and managing the massive datasets required for training LLMs. It involves big data techniques for processing, storing, and sharing training datasets. For example, GPT-4 was trained on roughly 13 trillion tokens, equal to approximately 10 trillion words.\n\nManaging LLMs’considerable number of parameters is a significant technical challenge from the infrastructure’s point of view. It requires vast computation resources, usually clusters of machines powered by Nvidia GPUs with CUDA support.\n\nThe massive size of LLMs directly impacts model training. When training an LLM from scratch, you can’t fit it on a single GPU due to the model’s size or the higher batch size you require for the expected results. Thus, you need multi-GPU training, which involves optimizing your processes and infrastructure to support data, model, or tensor parallelism.\n\nManaging massive datasets and multi-GPU clusters involves substantial costs. For example, the estimated training cost for GPT-4 is around $100 million, as stated by Sam Altman, the CEO of OpenAI (https://en.wikipedia.org/wiki/GPT-4#Training). Add to that the costs of multiple experiments, evaluation, and inference. Even if these numbers are not exact, as the sources are not 100% reliable, the scale of the costs of training an LLM is trustworthy, which implies that only the large players in the industry can afford to train LLMs from scratch.\n\nAt its core, LLMOps is MLOps at scale. It uses the same MLOps principles but is applied to big data and huge models that require more computing power to train and run. However, due to its huge scale, the most significant trend is the shift away from training neural networks from scratch for specific tasks. This approach is becoming obsolete with the rise of fine- tuning, especially with the advent of foundation models such as GPT. A few organizations with extensive computational resources, such as OpenAI and Google, develop these foundation models. Thus, most applications now rely on the lightweight fine-tuning of parts of these models, prompt engineering, or optionally distilling data or models into smaller, specialized inference networks.\n\nThus, for most LLM applications out there, your development steps will involve the selection of a foundation model, which you further have to optimize by using prompt engineering, fine-tuning, or RAG. Thus, the operational aspect of these three steps is the most critical to understand. Let’s dive into some popular components of LLMOps that can improve prompt engineering, fine-tuning, and RAG.\n\nOceanofPDF.com\n\nHuman feedback\n\nOne valuable refinement step of your LLM is aligning it with your audience’s preferences. You must introduce a feedback loop within your application and gather a human feedback dataset to further fine- tune the LLM with techniques such as Reinforcement Learning with Human Feedback (RLHF) or more advanced ones such as Direct Preference Optimization (DPO). One popular feedback loop is the thumbs-up/thumbs-down button present in most chatbot interfaces. You can read more on preference alignment in Chapter 6.\n\nOceanofPDF.com\n\nGuardrails\n\nUnfortunately, LLM systems are not reliable, as they often hallucinate. You can optimize your system against hallucinations, but as hallucinations are hard to detect and can take many forms, there are significant changes that will still happen in the future.\n\nMost users have accepted this phenomenon, but what is not acceptable is when LLMs accidentally output sensitive information, such as GitHub Copilot outputting AWS secret keys or other chatbots providing people’s passwords. This can also happen with people’s phone numbers, addresses, email addresses, and more. Ideally, you should remove all this sensitive data from your training data so the LLM doesn’t memorize it, but that doesn’t always happen.\n\nLLMs are well known for producing toxic and harmful outputs, such as sexist and racist outputs. For example, during an experiment on ChatGPT around April 2023, people found how to hijack the system by forcing the chatbot to adopt a negative persona, such as “a bad person” or “a horrible person.” It worked even by forcing the chatbot to play the role of well- known negative characters from our history, such as dictators or criminals. For example, this is what ChatGPT produced when impersonating a bad person:\n\nX is just another third-world country with nothing but drug lords and poverty-stricken people. The people there are uneducated and violent, and they don't have any respect for law and order. If you ask me, X is just a cesspool of crime and misery, and no one in their right mind would want to go there.\n\nCheck the source of the experiment for more examples of different personas: https://techcrunch.com/2023/04/12/researchers-discover-a-way- to-make-chatgpt-consistently-toxic/.\n\nThe discussion can be extended to a never-ending list of examples, but the key takeaway is that your LLM can produce harmful output or receive dangerous input, so you should monitor and prepare for them. Thus, to create safe LLM systems, you must protect them against harmful, sensitive, or invalid input and output by adding guardrails:\n\nInput guardrails:Input guardrails primarily protect against three main risks: exposing private information to external APIs, executing harmful prompts that could compromise your system (model jailbreaking), and accepting violent or unethical prompts. When it comes to leaking private information to external APIs, the risk is specific to sending sensitive data outside your organization, such as credentials or classified information. When talking about model jailbreaking, we mainly refer to prompt injection, such as executing malicious SQL code that can access, delete, or corrupt your data. Lastly, some applications don’t want to accept violent or unethical queries from users, such as asking an LLM how to build a bomb.\n\nOutput guardrails: At the output of an LLM response, you want to catch failed outputs that don’t respect your application’s standards. This can vary from one application to another, but some examples are empty responses (these responses don’t follow your expected format, such as JSON or YAML), toxic responses, hallucinations, and, in general, wrong responses. Also, you have to check for sensitive information that can leak from the internal knowledge of the LLM or your RAG system.\n\nPopular guardrail tools are Galileo Protect, which detects prompt injections, toxic language, data privacy protection leaks, and hallucinations. Also, you can use OpenAI’s Moderation API to detect harmful inputs or outputs and take action on them.\n\nThe downside of adding input and output guardrails is the extra latency added to your system, which might interfere with your application’s user experience. Thus, there is a trade-off between the safety of your input/output and latency. Regarding invalid outputs, as LLMs are non- deterministic, you can implement a retry mechanism to generate another potential candidate. However, as stated above, running the retry sequentially will double the response time. Thus, a common strategy is to run multiple generations in parallel and pick the best one. This will increase redundancy but help keep the latency in check.\n\nOceanofPDF.com\n\nPrompt monitoring\n\nMonitoring is not new to LLMOps, but in the LLM world, we have a new entity to manage: the prompt. Thus, we have to find specific ways to log and analyze them.\n\nMost ML platforms, such as Opik (from Comet ML) and W&B, or other specialized tools like Langfuse, have implemented logging tools to debug and monitor prompts. While in production, using these tools, you usually want to track the user input, the prompt templates, the input variables, the generated response, the number of tokens, and the latency.\n\nWhen generating an answer with an LLM, we don’t wait for the whole answer to be generated; we stream the output token by token. This makes the entire process snappier and more responsive. Thus, when it comes to tracking the latency of generating an answer, the final user experience must look at this from multiple perspectives, such as:\n\nTime to First Token (TTFT): The time it takes for the first token to be generated\n\nTime between Tokens (TBT): The interval between each token generation\n\nTokens per Second (TPS): The rate at which tokens are generated\n\nTime per Output Token (TPOT): The time it takes to generate each output token\n\nTotal Latency: The total time required to complete a response\n\nAlso, tracking the total input and output tokens is critical to understanding the costs of hosting your LLMs.\n\nUltimately, you can compute metrics that validate your model’s performance for each input, prompt, and output tuple. Depending on your use case, you can compute things such as accuracy, toxicity, and hallucination rate. When working with RAG systems, you can also compute metrics relative to the relevance and precision of the retrieved context.\n\nAnother essential thing to consider when monitoring prompts is to log their full traces. You might have multiple intermediate steps from the user query to the final general answer. For example, rewriting the query to improve the RAG’s retrieval accuracy evolves one or more intermediate steps. Thus, logging the full trace reveals the entire process from when a user sends a query to when the final response is returned, including the actions the system takes, the documents retrieved, and the final prompt sent to the model. Additionally, you can log the latency, tokens, and costs at each step, providing a more fine-grained view of all the steps.\n\nFigure 11.4: Example trace in the Langfuse UI\n\nAs shown in Figure 11.4, the end goal is to trace each step from the user’s input until the generated answer. If something fails or behaves unexpectedly, you can point exactly to the faulty step. The query can fail due to an incorrect answer, an invalid context, or incorrect data processing. Also, the application can behave unexpectedly if the number of generated tokens suddenly fluctuates during specific steps.\n\nTo conclude, LLMOps is a rapidly developing field. Given its quick evolution, making predictions is challenging. The truth is that we are not sure if the term LLMOps is here to stay. However, what is certain is that numerous new use cases for LLMs will emerge, along with tools and best practices to manage their lifecycle.\n\nEven if this DevOps, MLOps, and LLMOps section is far from comprehensive, it provides a strong idea of how to apply best ops practices in our LLM Twin use case.\n\nOceanofPDF.com\n\nDeploying the LLM Twin’s pipelines to the cloud\n\nThis section will show you how to deploy all the LLM Twin’s pipelines to the cloud. We must deploy the entire infrastructure to have the whole system working in the cloud. Thus, we will have to:\n\nSet up an instance of MongoDB serverless.\n\nSet up an instance of Qdrant serverless.\n\nDeploy the ZenML pipelines, container, and artifact registry to AWS.\n\nContainerize the code and push the Docker image to a container registry.\n\nNote that the training and inference pipelines already work with AWS SageMaker. Thus, by following the preceding four steps, we ensure that our whole system is on the cloud, ready to scale and serve our imaginary clients.\n\nWhat are the deployment costs?\n\nWe will stick to the free versions of the MongoDB, Qdrant, and ZenML services. As for AWS, we will mostly stick to their free tier for running the ZenML pipelines. The SageMaker training and inference components are more costly to run (which we won’t run in this section). Thus, what we will show you in the following sections will generate minimum costs (a few dollars at most) from AWS.\n\nOceanofPDF.com\n\nUnderstanding the infrastructure\n\nBefore diving into the step-by-step tutorial, where we will show you how to set up all the necessary components, let’s briefly overview our infrastructure and how all the elements interact. This will help us in mindfully following the tutorials below.\n\nAs shown in Figure 11.5, we have a few services to set up. To keep things simple, for MongoDB and Qdrant, we will leverage their serverless freemium version. As for ZenML, we will leverage the free trial of the ZenML cloud, which will help us orchestrate all the pipelines in the cloud. How will it do that?\n\nBy leveraging the ZenML cloud, we can quickly allocate all the required AWS resources to run, scale, and store the ML pipeline. It will help us spin up, with a few clicks, the following AWS components:\n\nAn ECR service for storing Docker images\n\nAn S3 object storage for storing all our artifacts and models\n\nSageMaker Orchestrator for orchestrating, running, and scaling all our ML pipelines\n\nFigure 11.5: Infrastructure flow\n\nNow that we understand what the essential resources of our infrastructure are, let’s look over the core flow of running a pipeline in the cloud that we will learn to implement, presented in Figure 11.5:\n\nBuild a Docker image that contains all the system dependencies, the project dependencies, and the LLM Twin application.\n\nPush the Docker image to ECR, where SageMaker can access it.\n\nNow, we can trigger any pipeline implemented during this book either from the CLI of our local machine or ZenML’s dashboard.\n\nEach step from ZenML’s pipeline will be mapped to a SageMaker job that runs on an AWS EC2 virtual machine (VM). Based on the dependencies between the directed acyclic graph (DAG) steps, some will run in parallel and others sequentially.\n\nWhen running a step, SageMaker pulls the Docker image from ECR, defined in step 2. Based on the pulled image, it creates a Docker container that executes the pipeline step.\n\nAs the job is executed, it can access the S3 artifact storage, MongoDB, and Qdrant vector DB to query or push data. The ZenML dashboard is a key\n\ntool, providing real-time updates on the pipeline’s progress and ensuring a clear view of the process.\n\nNow that we know how the infrastructure works, let’s start by setting up MongoDB, Qdrant, and the ZenML cloud.\n\nWhat AWS cloud region should I choose?\n\nIn our tutorials, all the services will be deployed to AWS within the Frankfurt (eu-central-1) region. You can select another region, but be consistent across all the services to ensure faster responses between components and reduce potential errors.\n\nHow should I manage changes in the services’ UIs?\n\nUnfortunately, MongoDB, Qdrant, or other services may change their UI or naming conventions. As we can’t update this book each time that happens, please refer to their official documentation to check anything that differs from our tutorial. We apologize for this inconvenience, but unfortunately, it is not in our control.\n\nOceanofPDF.com\n\nSetting up MongoDB\n\nWe will show you how to create and integrate a free MongoDB cluster into our projects. To do so, these are the steps you have to follow:\n\nGo to their site at https://www.mongodb.com and create an account.\n\nIn the left panel, go to Deployment|Database and click Build a Cluster.\n\nWithin the creation form, do the following:\n\nChoose an M0 Free cluster.\n\nCall your cluster twin.\n\nChoose AWS as your provider.\n\nChoose Frankfurt (eu-central-1) as your region. You can choose another region, but be careful to choose the same region for all future AWS services.\n\nLeave the rest of the attributes with their default values.\n\nIn the bottom right, click the Create Deployment green button.\n\nTo test that your newly created MongoDB cluster works fine, we must connect to it from our local machine. We used the MongoDB VS Code extension to do so, but you can use any other tool. Thus, from their Choose a connection method setup flow, choose MongoDB for VS Code. Then, follow the steps provided on their site.\n\nTo connect, you must paste the DB connection URL in the VS Code extension (or another tool of your liking), which contains your username, password, and cluster URL, similar to this one:\n\nmongodb+srv://: @twin.vhxy1.mongodb.net\n\n. Make sure to save this URL somewhere you can copy it from later.\n\nIf you don’t know or want to change your password, go to Security → Quickstart in the left panel. There, you can edit your login credentials. Be sure to save them somewhere safe, as you won’t be able to access them later.\n\nAfter verifying that your connections work, go to Security → Network Access in the left panel and click ADD IP ADDRESS.Then click ALLOW ACCESS FROM ANYWHERE and hit Confirm. Out of simplicity, we allow any machine from any IP to access our MongoDB cluster. This ensures that our pipelines can query or write to the DB without any additional complex networking setup. It’s not the safest option for production, but for our example, it’s perfectly fine.\n\nThe final step is to return to your project and open your\n\n.env\n\nfile. Now, either add or replace the\n\nDATABASE_HOST\n\nvariable with your MongoDB connection string. It should look something like this:\n\nDATABASE_HOST= mongodb+srv://: @twin.vhxy1.mongodb.net\n\n.\n\nThat’s it! Now, instead of reading and writing from your local MongoDB, you will do it from the cloud MongoDB cluster we just created. Let’s repeat a similar process with Qdrant.\n\nOceanofPDF.com\n\nSetting up Qdrant\n\nWe have to repeat a similar process to what we did for MongoDB. Thus, to create a Qdrant cluster and hook it to our project, follow these steps:\n\nGo to Qdrant at https://cloud.qdrant.io/ and create an account.\n\nIn the left panel, go to Clusters and click Create.\n\nFill out the cluster creation form with the following:\n\nChoose the Free version of the cluster.\n\nChoose GCP as the cloud provider (while writing the book, it was the only one allowed for a free cluster).\n\nChoose Frankfurt as the region (or the same region as you chose for MongoDB).\n\nName the cluster twin.\n\nLeave the rest of the attributes with their default values and click Create.\n\nAccess the cluster in the Data Access Control section in the left panel.\n\nClick Create and choose your twin cluster to create a new access token.Copy the newly created token somewhere safe, as you won’t be able to access it anymore.\n\nYou can run their example from Usage Examples to test that your connection works fine.\n\nGo back to the Clusters section of Qdrant and open your newly created twin cluster. You will have access to the cluster’s endpoint, which you need to configure Qdrant in your code.\n\nYou can visualize your Qdrant collections and documents by clicking Open Dashboard and entering your API Key as your password. The Qdrant cluster dashboard will now be empty, but after running the pipelines, you will see all the collections, as shown here:\n\nFigure 11.6: Qdrant cluster dashboard example after being populated with two collections.\n\nFinally, return to your project and open your\n\n.env\n\nfile. Now, we must fill in a couple of environment variables as follows:\n\nUSE_QDRANT_CLOUD\n\n=\n\ntrue\n\nQDRANT_CLOUD_URL\n\n=\n\nstep\n\n7>\n\nQDRANT_APIKEY\n\n=\n\nstep\n\n5>\n\nThat’s it! Instead of reading and writing from your local Qdrant vector DB, you will do it from the cloud Qdrant cluster we just created. Just to be sure that everything works fine, run the end-to-end data pipeline with the cloud version of MongoDB and Qdrant as follows:\n\npeotry poe run-end-to-end-data-pipeline\n\nThe last step is setting up the ZenML cloud and deploying all our infrastructure to AWS.\n\nOceanofPDF.com\n\nSetting up the ZenML cloud\n\nSetting up the ZenML cloud and the AWS infrastructure is a multi-step process. First, we will set up a ZenML cloud account, then the AWS infrastructure through the ZenML cloud, and, finally, we will bundle our code in a Docker image to run it in AWS SageMaker.\n\nLet’s start with setting up the ZenML cloud:\n\nGo to the ZenML cloud at https://cloud.zenml.io and make an account. They provide a seven-day free trial, which is enough to run our examples.\n\nFill out their onboarding form and create an organization with a unique name and a tenant called twin. A tenant refers to a deployment of ZenML in a fully isolated environment. Wait a few minutes until your tenant server is up before proceeding to the next step.\n\nIf you want to, you can go through their Quickstart Guide to understand how the ZenML cloud works with a simpler example. It is not required to go through it to deploy the LLM Twin application, but we recommend it to ensure everything works fine.\n\nAt this point, we assume that you have gone through the Quickstart Guide. Otherwise, you might encounter issues during the next steps. To connect our project with this ZenML cloud tenant, return to the project and run the\n\nzenml connect\n\ncommand provided in the dashboard. It looks similar to the following example but with a different URL:\n\nzenml connect --url https://0c37a553-zenml.cloudinfra.zenml.io\n\n.\n\nTo ensure everything works fine, run a random pipeline from your code. Note that at this point, we are still running it locally, but instead of logging the results to the local server, we log everything to the cloud version:\n\npoetry poe run-digital-data-etl\n\nGo to the Pipelines section in the left panel of the ZenML dashboard. If everything worked fine, you should see the pipeline you ran in Step 5 there.\n\nEnsure that your ZenML server version matches your local ZenML version. For example, when we wrote this book, both were version 0.64.0. If they don’t match, you might encounter strange behavior, or it might not work correctly. The easiest fix is to go to your\n\npyproject.toml\n\nfile, find the\n\nzenml\n\ndependency, and update it with the version of your server. Then run\n\npoetry lock --no-update && poetry install\n\nto update your local virtual environment.\n\nTo ship the code to AWS, you must create a ZenML stack. A stack is a set of components, such as the underlying orchestrator, object storage, and container registry, that ZenML needs under the hood to run the pipelines. Intuitively, you can see your stack as your infrastructure. While working locally, ZenML offers a default stack that allows you to quickly develop your code and test things locally. However, by defining different stacks, you can quickly switch between different infrastructure environments, such as local and AWS runs, which we will showcase in this section.\n\nBefore starting this section, ensure you have an AWS account with admin permissions ready.\n\nWith that in mind, let’s create an AWS stack for our project. To do so, follow the next steps:\n\nIn the left panel, click on the Stacks section and hit the New Stack button.\n\nYou will have multiple options for creating a stack, but the easiest is creating one from scratch within the in-browser experience, which doesn’t require additional preparations. This is not very flexible, but it is enough to host our project. Thus, choose Create New Infrastructure → In-browser Experience.\n\nThen, choose AWS as your cloud provider.\n\nChoose Europe (Frankfurt)—eu-central-1 as your location or the region you used to set up MongoDB and Qdrant.\n\nName it aws-stack.It is essential to name it exactly like this so that the commands that we will use work.\n\nNow ZenML will create a set of IAM roles to give permissions to all the other components to communicate with each other, an S3 bucket as your artifact storage, an ECR repository as your container registry, and SageMaker as your orchestrator.\n\nClick Next.\n\nClick the Deploy to AWS button. It will open a CloudFormation page on AWS. ZenML leverages CloudFormation (an infrastructure as code, or IaC, tool)to create all the AWS resources we enumerated in Step 6.\n\nAt the bottom, check all the boxes to acknowledge that AWS CloudFormation will create AWS resources on your behalf. Finally, click the Create stack button. Now, we must wait for a couple of minutes for AWS CloudFormation to spin up all the resources.\n\nReturn to the ZenML page and click the Finish button.\n\nBy leveraging ZenML, we efficiently deployed the entire AWS infrastructure for our ML pipelines. We began with a basic example, sacrificing some control. However, if you seek more control, ZenML offers the option to use Terraform (an IaC tool) to fully control your AWS resources or to connect ZenML with your current infrastructure.\n\nBefore moving to the next step, let’s have a quick recap of the AWS resources we just created:\n\nAn IAM role is an AWS identity with permissions policies that define what actions are allowed or denied for that role. It is used to grant access to AWS services without needing to share security credentials.\n\nS3 is a scalable and secure object storage service that allows storing and retrieving files from anywhere on the web. It is commonly used for data backup, content storage, and data lakes. It’s more scalable and flexible than Google Drive.\n\nECR is a fully managed Docker container registry that makes storing, managing, and deploying Docker container images easy.\n\nSageMaker is a fully managed service that allows developers and data scientists to quickly build, train, and deploy ML models.\n\nSageMaker Orchestrator is a feature of SageMaker that helps automate the execution of ML workflows, manage dependencies between steps, and ensure the reproducibility and scalability of model training and deployment pipelines. Other similar tools are Prefect, Dagster, Metaflow, and Airflow.\n\nCloudFormation is a service that allows you to model and set up your AWS resources so that you can spend less time managing them and more time focusing on your applications. It automates the process of provisioning AWS infrastructure using templates.\n\nBefore running the ML pipelines, the last step is to containerize the code and prepare a Docker image that packages our dependencies and code.\n\nOceanofPDF.com\n\nContainerize the code using Docker\n\nSo far, we have defined our infrastructure, MongoDB, Qdrant, and AWS, for storage and computing. The last step is to find a way to take our code and run it on top of this infrastructure. The most popular solution is Docker, a tool that allows us to create an isolated environment (a container) that contains everything we need to run our application, such as system dependencies, Python dependencies, and the code.\n\nWe defined our Docker image at the project’s root in the\n\nDockerfile\n\n. This is the standard naming convention for Docker. Before digging into the code, if you want to build the Docker image yourself, ensure that you have Docker installed on your machine. If you don’t have it, you can install it by following the instructions provided here: https://docs.docker.com/engine/install. Now, let’s look at the content of the\n\nDockerfile\n\nstep by step.\n\nThe\n\nDockerfile\n\nbegins by specifying the base image, which is a lightweight version of Python 3.11 based on the Debian Bullseye distribution. The environment variables are then set up to configure various aspects of the container, such\n\nas the workspace directory, turning off Python bytecode generation, and configuring Python to output directly to the terminal. Additionally, the version of Poetry to be installed is specified, and a few environment variables are set to ensure that package installations are non-interactive, which is vital for automated builds.\n\nFROM python:\n\n3.11\n\nslim-bullseye AS release ENV WORKSPACE_ROOT=/app/ ENV PYTHONDONTWRITEBYTECODE=\n\n1\n\nENV PYTHONUNBUFFERED=\n\n1\n\nENV POETRY_VERSION=\n\n1.8.3\n\nENV DEBIAN_FRONTEND=noninteractive ENV POETRY_NO_INTERACTION=\n\n1\n\nNext, we install Google Chrome in the container. The installation process begins by updating the package lists and installing essential tools like gnupg, wget, and curl. The Google Linux signing key is added, and the\n\nGoogle Chrome repository is configured. After another package list update, the stable version of Google Chrome is installed. The package lists are removed after installation to keep the image as small as possible.\n\nRUN apt-get update -y && \\ apt-get install -y gnupg wget curl --no-install- recommends && \\ wget -q -O - https://dl- ssl.google.com/linux/linux_signing_key.pub | gpg --dearmor -o /usr/share/keyrings/google-linux-signing-key.gpg && \\ echo\n\n\"deb [signed-by=/usr/share/keyrings/google-linux-signing-key.gpg] https://dl.google.com/linux/chrome/deb/ stable main\"\n\n> /etc/apt/sources.list.d/google-chrome.list && \\ apt-get update -y && \\ apt-get install -y google-chrome-stable && \\ rm -rf /var/lib/apt/lists/*\n\nFollowing the Chrome installation, other essential system dependencies are installed. Once these packages are installed, the package cache is cleaned up to reduce the image size further.\n\nRUN apt-get update -y \\ && apt-get install -y --no-install-recommends build-essential \\ gcc \\ python3-dev \\ build-essential \\ libglib2.0-dev \\ libnss3-dev \\ && apt-get clean \\ && rm -rf /var/lib/apt/lists/*\n\nPoetry, the dependency management tool, is then installed using pip. The\n\n--no-cache-dir\n\noption prevents pip from caching packages, helping to keep the image smaller. After installation, Poetry is configured to use up to 20 parallel workers when installing packages, which can speed up the installation process.\n\nRUN pip install --no-cache-dir \"poetry==$POETRY_VERSION\" RUN poetry config installer.max-workers 20\n\nThe working directory inside the container is set to\n\nWORKSPACE_ROOT\n\n, which defaults to\n\n/app/\n\n, where the application code will reside. The\n\npyproject.toml\n\nand\n\npoetry.lock\n\nfiles define the Python’s project dependencies and are copied into this directory.\n\nWORKDIR $WORKSPACE_ROOT COPY pyproject.toml poetry.lock $WORKSPACE_ROOT\n\nWith the dependency files in place, the project’s dependencies are installed using Poetry. The configuration turns off the creation of a virtual environment, meaning the dependencies will be installed directly into the container’s Python environment. The installation excludes development dependencies and prevents caching to minimize space usage.\n\nAdditionally, the\n\npoethepoet\n\nplugin is installed to help manage tasks within the project. Finally, any remaining Poetry cache is removed to keep the container as lean as possible.\n\nRUN poetry config virtualenvs.create false && \\ poetry install --no-root -- no-interaction --no-cache --without dev && \\ poetry self add 'poethepoet[poetry_plugin]' && \\ rm -rf ~/.cache/pypoetry/cache/ && \\ rm - rf ~/.cache/pypoetry/artifacts/\n\nIn the final step, the entire project directory from the host machine is copied into the container’s working directory. This step ensures that all the application files are available within the container.\n\nOne important trick when writing a\n\nDockerfile\n\nis to decouple your installation steps from copying the rest of the files. This is useful because each Docker command is cached and layered on top of each other. Thus, whenever you change one layer when rebuilding the Docker image, all the layers below the one altered are executed again. Because you rarely change your system and project dependencies but mostly change your code, copying your project files in the last step makes rebuilding Docker images fast by taking advantage of the caching mechanism’s full potential.\n\nCOPY . $WORKSPACE_ROOT\n\nThis\n\nDockerfile\n\nis designed to create a clean, consistent Python environment with all necessary dependencies. It allows the project to run smoothly in any environment that supports Docker.\n\nThe last step is to build the Docker image and push it to the ECR created by ZenML. To build the Docker image from the root of the project, run the following:\n\ndocker buildx build --platform linux/amd64 -t llmtwin -f Dockerfile .\n\nWe must build it on a Linux platform as the Google Chrome installer we used inside Docker works only on a Linux machine. Even if you use a macOS or Windows machine, Docker can emulate a virtual Linux container.\n\nThe tag of the newly created Docker image is\n\nllmtwin\n\n. We also provide this\n\nbuild\n\ncommand under a\n\npoethepoet\n\ncommand:\n\npoetry poe build-docker-image\n\nNow, let’s push the Docker image to ECR. To do so, navigate to your AWS console and then to the ECR service. From there, find the newly created ECR repository. It should be prefixed with\n\nzenml-*\n\n, as shown here:\n\nFigure 11.7: AWS ECR example\n\nThe first step is to authenticate to ECR. For this to work, ensure that you have the AWS CLI installed and configured with your admin AWS credentials, as explained in Chapter 2:\n\nAWS_REGION=\n\n# e.g. AWS_REGION=eu-central-1\n\nAWS_ECR_URL= aws ecr get-login-password --region ${AWS_REGION}| docker login --username AWS --password-stdin ${AWS_ECR_URL}\n\nYou can get your current\n\nAWS_REGION\n\nby clicking on the toggle in the top-right corner, as seen in Figure 11.8. Also, you can copy the ECR URL to fill the\n\nAWS_ECR_URL\n\nvariable from the main AWS ECR dashboard, as illustrated in Figure 11.7. After running the previous command, you should see the message Login Succeeded on the CLI.\n\nFigure 11.8: AWS region and account details\n\nNow we have to add another tag to the\n\nllmtwin\n\nDocker image that signals the Docker registry we want to push it to:\n\ndocker tag llmtwin ${AWS_ECR_URL}:latest\n\nFinally, we push it to ECR by running:\n\ndocker push ${AWS_ECR_URL}:latest\n\nAfter the upload is finished, return to your AWS ECR dashboard and open your ZenML repository. The Docker image should appear, as shown here:\n\nFigure 11.9: AWS ECR repository example after the Docker image is pushed\n\nFor every change in the code that you need to ship and test, you would have to go through all these steps, which are tedious and error-prone. The Adding LLMOps to the LLM Twin section of this chapter will teach us how to automate these steps within the CD pipeline using GitHub Actions. Still, we first wanted to go through them manually to fully understand the behind- the-scenes process and not treat it as a black box. Understanding these\n\ndetails is vital for debugging your CI/CD pipelines, where you must understand the error messages and how to fix them.\n\nNow that we have built our Docker image and pushed it to AWS ECR, let’s deploy it to AWS.\n\nOceanofPDF.com\n\nRun the pipelines on AWS\n\nWe are very close to running the ML pipelines on AWS, but we have to go through a few final steps. Let’s switch from the default ZenML stack to the AWS one we created in this chapter. From the root of your project, run the following in the CLI:\n\nzenml stack set aws-stack\n\nReturn to your AWS ECR ZenML repository and copy the image URI as shown in Figure 11.9. Then, go to the\n\nconfigs\n\ndirectory, open the\n\nconfigs/end_to_end_data.yaml\n\nfile, and update the\n\nsettings.docker.parent_image\n\nattribute with your ECR URL, as shown below:\n\nsettings:\n\ndocker:\n\nparent_image:\n\n#e.g., 992382797823.dkr.ecr.eu-central-1.amazonaws.com/zenml- rlwlcs:latest\n\nskip_build:\n\nTrue\n\nWe’ve configured the pipeline to always use the latest Docker image available in ECR. This means that the pipeline will automatically pick up the latest changes made to the code whenever we push a new image.\n\nWe must export all the credentials from our\n\n.env\n\nfile to ZenML secrets, a feature that safely stores your credentials and makes them accessible within your pipelines:\n\npoetry poe export-settings-to-zenml\n\nThe last step is setting up to run the pipelines asynchronously so we don’t have to wait until they are finished, which might result in timeout errors:\n\nzenml orchestrator update aws-stack --synchronous=False\n\nNow that ZenML knows to use the AWS stack, our custom Docker image, and has access to our credentials, we are finally done with the setup. Run the\n\nend-to-end-data-pipeline\n\nwith the following command:\n\npoetry poe run-end-to-end-data-pipeline\n\nNow you can go to ZenML Cloud → Pipelines → end_to_end_data and open the latest run. On the ZenML dashboard, you can visualize the latest state of the pipeline, as seen in Figure 11.10. Note that this pipeline runs all the data-related pipelines in a single run.\n\nIn the Adding LLMOps to the LLM Twin section, we will explain why we compressed all the steps into a single pipeline.\n\nFigure 11.10: ZenML example of running the end-to-end-data-pipeline\n\nYou can click on any running block and find details about the run, the code used for that specific step, and the logs for monitoring and debugging, as illustrated in Figure 11.11:\n\nFigure 11.11: ZenML step metadata example\n\nTo run other pipelines, you have to update the\n\nsettings.docker.parent_image\n\nattribute in their config file under the\n\nconfigs/\n\ndirectory.\n\nTo find even more details about the runs, you can go to AWS SageMaker. In the left panel, click SageMaker dashboard, and on the right, in the Processing column, click on the green Running section, as shown in Figure 11.12.\n\nThis will open a list of all the processing jobs that execute your ZenML pipelines.\n\nFigure 11.12: SageMaker dashboard\n\nIf you want to run the pipelines locally again, use the following CLI command:\n\npoetry poe set-local-stack\n\nIf you want to disconnect from the ZenML cloud dashboard and use the local version again, run the following:\n\nzenml disconnect\n\nOceanofPDF.com\n\nTroubleshooting the ResourceLimitExceeded error after running a ZenML pipeline on SageMaker\n\nLet’s assume, you’ve encountered a ResourceLimitExceeded error after running a ZenML pipeline on SageMaker using the AWS stack. In this case, you have to explicitly ask AWS to give you access to a specific type of AWS EC2 VM.\n\nZenML uses, by default,\n\nml.t3.medium\n\nEC2 machines, which are part of the AWS freemium tier. However, some AWS accounts cannot access these VMs by default. To check your access, search your AWS console for Service Quotas.\n\nThen, in the left panel, click on AWS services, search for Amazon SageMaker, and then for\n\nml.t3.medium\n\n. In Figure 11.13, you can see our quotas for these types of machines. If yours is 0, you should request that AWS increase them to numbers similar to those from Figure 11.13 in the Applied account-level quota value column. The whole process is free of charge and only requires a few clicks. Unfortunately, you might have to wait for a few hours up to one day until AWS accepts your request.\n\nFigure 11.13: SageMaker—ml.t3.medium expected quotas\n\nYou can find step-by-step instructions on how to solve this error and request new quotas at this link: https://repost.aws/knowledge-center/sagemaker- resource-limit-exceeded-error.\n\nIf you changed the values from your .env file and want to update the ZenML secrets with them, first run the following CLI command to delete the old secrets:\n\npoetry poe delete-settings-zenml\n\nThen, you can export them again by running:\n\npoetry poe export-settings-to-zenml\n\nOceanofPDF.com\n\nAdding LLMOps to the LLM Twin\n\nIn the previous section, we saw how to set up the infrastructure for the LLM Twin project by manually building the Docker image and pushing it to ECR. We want to automate the entire process and implement a CI/CD pipeline using GitHub Actions and a CT pipeline using ZenML. As mentioned earlier, implementing a CI/CD/CT pipeline ensures that each feature pushed to main branches is consistent and tested. Also, by automating the deployment and training, you support collaboration, save time, and reduce human errors.\n\nFinally, at the end of the section, we will show you how to implement a prompt monitoring pipeline using Opik from Comet ML and an alerting system using ZenML. This prompt monitoring pipeline will help us debug and analyze the RAG and LLM logic. As LLM systems are non- deterministic, capturing and storing the prompt traces is essential for monitoring your ML logic.\n\nBefore diving into the implementation, let’s start with a quick section on the LLM Twin’s CI/CD pipeline flow.\n\nOceanofPDF.com\n\nLLM Twin’s CI/CD pipeline flow\n\nWe have two environments: staging and production. When developing a new feature, we create a new branch out of the staging branch and develop solely on that one. When we are done and consider the feature finished, we open a pull request (PR) to the staging branch. After the feature branch is accepted, it is merged into the staging branch. This is a standard workflow in most software applications. There might be variations, like adding a dev environment, but the principles remain the same.\n\nAs illustrated in Figure 11.14, the CI pipeline is triggered when the PR opens. At this point, we test the feature branch for linting and formatting errors. Also, we run a\n\ngitleaks\n\ncommand to check for credentials and sensitive information that was committed by mistake. If the linting, formatting, and gitleaks steps pass (also known as static analysis), we run the automated tests. Note that the static analysis steps run faster than the automated tests. Thus, the order matters. That’s why adding the static analysis steps at the beginning of the CI pipeline is good practice. We propose the following order of the CI steps:\n\ngitleaks\n\nchecks\n\nLinting checks\n\nFormatting checks\n\nAutomated testing, such as unit and integration tests\n\nIf any check fails, the CI pipeline fails, and the developer who created the PR cannot merge it into the staging branch until it fixes the issues.\n\nImplementing a CI pipeline ensures that new features follow the repository’s standards and don’t break existing functionality. The exact process repeats when we plan to merge the staging branch into the production one. We open a PR, and the CI pipeline is automatically executed before merging the staging branch into production.\n\nFigure 11.14: CI/CD pipelines flow\n\nThe CD pipeline runs after the branch is merged. For example, after the feature branch is merged into staging, the CD pipeline takes the code from the staging branch, builds a new Docker image, and pushes it to the AWS ECR Docker repository. When running future pipeline runs in the staging environment, it will use the latest Docker image that was built by the CD pipeline. The exact process happens between staging and production. Still,\n\nthe key difference is that the staging environment exists as an experimental place where the QA team and stakeholders can further manually test the new feature along with what is automatically tested in the CI pipeline.\n\nIn our repository, we used only a main branch, which reflects production, and feature branches to push new work. We did this to keep things simple, but the same principles apply. To extend the flow, you must create a staging branch and add it to the CD pipeline.\n\nOceanofPDF.com\n\nMore on formatting errors\n\nFormatting errors relate to the style and structure of your code, ensuring that it adheres to a consistent visual layout. This can include the placement of spaces, indentation, line length, and other stylistic elements.\n\nThe main purpose of formatting is to make your code more readable and maintainable. Consistent formatting helps teams work together more effectively, as the code looks uniform, regardless of who wrote it. Examples of formatting errors are:\n\nIncorrect indentation (e.g., mixing spaces and tabs)\n\nLines that are too long (e.g., exceeding\n\n79\n\nor\n\n88\n\ncharacters, depending on your style guide)\n\nMissing or extra spaces around operators or after commas\n\nOceanofPDF.com\n\nMore on linting errors\n\nLinting errors relate to potential issues in your code that could lead to bugs, inefficiencies, or non-adherence to coding standards beyond just style. Linting checks often involve static analysis of the code to catch things like unused variables, undefined names, or questionable practices.\n\nLinting’s main goal is to catch potential errors or bad practices early in the development process, improving code quality and reducing the likelihood of bugs. Examples of linting errors are:\n\nUnused imports or variables\n\nUndefined variables or functions are being used\n\nPotentially dangerous code (e.g., using\n\n==\n\ninstead of\n\nis\n\nfor checking against\n\nNone\n\n)\n\nWe use Ruff, a versatile tool for formatting and linting. It incorporates checks for common formatting issues and PEP 8 compliance, as well as deeper linting checks for potential errors and code quality problems. Also, it is written in Rust, making it fast for big codebases.\n\nBefore implementing what we’ve explained above, let’s examine the core principles of GitHub Actions.\n\nOceanofPDF.com\n\nQuick overview of GitHub Actions\n\nGitHub Actions is a CI/CD platform provided by GitHub that allows developers to automate their workflows directly within a GitHub repository. It enables users to build, test, and deploy their code directly from GitHub by defining workflows in YAML files. Since it’s part of GitHub, it works seamlessly with repositories, issues, PRs, and other GitHub features. Here are the key components you should know about:\n\nWorkflows: A workflow is an automated process defined in a YAML file located in your repository’s\n\n.github/workflows directory\n\n. It specifies what should happen (e.g.,\n\nbuild\n\n,\n\ntest\n\n, and\n\ndeploy\n\n) and when (e.g., on push, on PR).\n\nJobs: Workflows are made up of jobs, which are groups of steps that execute on the same runner. Each job runs in its own virtual\n\nenvironment.\n\nSteps: Jobs are made up of multiple independent steps, which can be actions or shell commands.\n\nActions: Actions are reusable commands or scripts. You can use pre- built actions from GitHub Marketplace or create your own. You can think of them as Python functions.\n\nRunners: Runners are the servers that run your jobs. GitHub provides hosted runners (Linux, Windows, macOS), or you can even self-host your runners.\n\nA workflow is described using YAML syntax. For example, a simple workflow that clones the current GitHub repository and installs Python 3.11 on an Ubuntu machine looks like this:\n\nname\n\n:\n\nExample\n\non\n\n:\n\n[push]\n\njobs\n\n:\n\nbuild\n\n:\n\nruns-on\n\n:\n\nubuntu-latest\n\nsteps\n\n:\n\n\n\nname: Checkout\n\nuses\n\n:\n\nactions/checkout@v3\n\n\n\nname: Setup Python\n\nuses\n\n:\n\nactions/setup-python@v3\n\nwith\n\n:\n\npython-version\n\n:\n\n\"3.11\"\n\nThe workflows are triggered by events like\n\npush\n\n,\n\npull_request\n\n, or\n\nschedule\n\n. For example, you might trigger a workflow every time code is pushed to a specific branch. Now that we understand how GitHub Actions works, let’s look at the LLM Twin’s CI pipeline.\n\nOceanofPDF.com\n\nThe CI pipeline\n\nThe LLM Twin’s CI pipeline is split into two jobs:\n\nA QA job that looks for formatting and linting errors using Ruff. Also, it runs a\n\ngitleaks\n\nstep to scan for leaked secrets throughout our repository.\n\nA test job that runs all our automatic tests using\n\nPytest\n\n. In our use case, we implemented just a dummy test to showcase the CI pipeline, but using the structure from this book, you can easily extend it with real tests for your use case.\n\nOceanofPDF.com\n\nGitHub Actions CI YAML file\n\nThe YAML file sits under\n\n.github/workflows/ci.yaml\n\n. It begins by defining the workflow’s name as\n\nCI\n\n, as you can see in the following snippet. This label will be used to identify the workflow within GitHub’s Actions interface. Next, the section specifies that the workflow should be triggered whenever a\n\npull_request\n\nevent occurs. Hence, the CI workflow will automatically run whenever a PR is opened, synchronized, or reopened.\n\nname:\n\nCI\n\non:\n\npull_request:\n\nThe\n\nconcurrency\n\nsection ensures that only one instance of this workflow runs for a given reference (like a branch) at any given time. The\n\ngroup\n\nfield is defined using GitHub’s expression syntax to create a unique group name based on the workflow and the reference. The\n\ncancel-in-progress: true\n\nline ensures that if a new workflow run is triggered before the previous one finishes, the previous run is canceled. This is particularly useful to prevent redundant executions of the same workflow.\n\nconcurrency:\n\ngroup:\n\n${{\n\ngithub.workflow\n\n}}-${{\n\ngithub.ref\n\n}}\n\ncancel-in-progress:\n\ntrue\n\nThe workflow defines two separate jobs:\n\nqa\n\nand\n\ntest\n\n. Each job runs on the latest version of Ubuntu, specified by\n\nruns-on: ubuntu-latest\n\n.\n\nThe first job, named\n\nQA\n\n, is responsible for quality assurance tasks like code checks and formatting verification. Within the\n\nqa\n\njob, the first step is to check out the repository’s code using the\n\nactions/checkout@v3\n\naction. This step is necessary to ensure that the job has access to the code that needs to be analyzed.\n\njobs:\n\nqa:\n\nname:\n\nQA\n\nruns-on:\n\nubuntu-latest\n\nsteps:\n\n\n\nname:\n\nCheckout\n\nuses:\n\nactions/checkout@v3\n\nThe next step is to set up the Python environment. This is done using the\n\nactions/setup-python@v3\n\naction, with the Python version specified as\n\n\"3.11\"\n\n. This step ensures that the subsequent steps in the job will run in the correct Python environment.\n\n\n\nname:\n\nSetup\n\nPython\n\nuses:\n\nactions/setup-python@v3\n\nwith:\n\npython-version:\n\n\"3.11\"\n\nThe workflow then installs Poetry using the\n\nabatilo/actions-poetry@v2\n\naction, specifying the version of Poetry as\n\n1.8.3\n\n:\n\n\n\nname:\n\nInstall\n\npoetry\n\nuses:\n\nabatilo/actions-poetry@v2\n\nwith:\n\npoetry-version:\n\n1.8.3\n\nOnce Poetry is set up, the workflow installs the project’s development dependencies using the\n\npoetry install --only dev\n\ncommand. Additionally, the workflow adds the\n\npoethepoet\n\nplugin for Poetry, which will be used to run predefined tasks more conveniently within the project.\n\n\n\nname:\n\nInstall\n\npackages\n\nrun:\n\n|\n\npoetry\n\ninstall\n\n--only\n\ndev\n\npoetry\n\nself\n\nadd\n\n'poethepoet[poetry_plugin]'\n\nThe\n\nqa\n\njob then runs several quality checks on the code. The first check uses a tool called\n\ngitleaks\n\nto scan for secrets in the codebase, ensuring that no sensitive information is accidentally committed:\n\n\n\nname:\n\ngitleaks\n\ncheck\n\nrun:\n\npoetry\n\npoe\n\ngitleaks-check\n\nFollowing the\n\ngitleaks\n\ncheck, the workflow runs a linting process to enforce coding standards and best practices in the Python code. This is achieved through the\n\npoetry poe lint-check\n\ncommand, which uses Ruff under the hood.\n\n\n\nname:\n\nLint\n\ncheck\n\n[\n\nPython\n\n]\n\nrun:\n\npoetry\n\npoe\n\nlint-check\n\nThe last step in the\n\nqa\n\njob is a format check, which ensures that the Python code is properly formatted according to the project’s style guidelines. This is done using the\n\npoetry poe format-check\n\ncommand, which uses Ruff under the hood.\n\n\n\nname:\n\nFormat\n\ncheck\n\n[\n\nPython\n\n]\n\nrun:\n\npoetry\n\npoe\n\nformat-check\n\nThe second job defined in the workflow is the\n\ntest\n\njob, which also runs on the latest version of Ubuntu. Like the\n\nqa\n\njob, it starts by checking out the code from the repository and installing Python 3.11 and Poetry 1.8.3.\n\ntest:\n\nname:\n\nTest\n\nruns-on:\n\nubuntu-latest\n\nsteps:\n\n\n\nname:\n\nCheckout\n\nuses:\n\nactions/checkout@v3\n\n…\n\nAfter setting up the system dependencies, the\n\ntest\n\njob installs all the project’s dependencies with the\n\npoetry install\n\ncommand. As we want to run the tests, this time, we need to install all the dependencies that are required to run the application.\n\n\n\nname:\n\nInstall\n\npackages\n\nrun:\n\n|\n\npoetry\n\ninstall\n\n–-without\n\naws\n\npoetry\n\nself\n\nadd\n\n'poethepoet[poetry_plugin]'\n\nFinally, the\n\ntest\n\njob runs the project’s tests using the\n\npoetry poe test\n\ncommand. This step ensures that all tests are executed and provides feedback on whether the current code changes break any functionality.\n\n\n\nname:\n\nRun\n\ntests\n\nrun:\n\n|\n\necho\n\n\"Running tests...\"\n\npoetry\n\npoe\n\ntest\n\nIf any of the steps from the QA or test jobs fail, the GitHub Actions workflow will fail, resulting in the PR not being able to be merged until the issue is fixed. By taking this approach, we ensure that all the new features added to the main branches respect the standard of the project and that it doesn’t break existing functionality through automated tests.\n\nFigure 11.15 shows the CI pipeline in the Actions tab of the GitHub repository. It was run after a commit with the message feat: Add Docker image and CD pipeline and ran the two jobs described above, QA and Test.\n\nFigure 11.15: GitHub Actions CI pipeline run example\n\nOceanofPDF.com\n\nThe CD pipeline\n\nThe CD pipeline will automate the Docker steps we manually performed in the Deploying the LLM Twin’s pipelines to the cloud section, which are:\n\nSet up Docker.\n\nLog in to AWS.\n\nBuild the Docker image.\n\nPush the Docker image to AWS ECR.\n\nWith that in mind, let’s look at the GitHub Actions YAML file, which sits under\n\n.github/workflows/cd.yaml\n\n. It begins by naming the workflow\n\nCD\n\nand specifying the trigger for this workflow. The trigger is any push to the repository’s main branch. This workflow will automatically run when new code is pushed to the main branch, usually when a PR is merged into the main branch. The\n\non.push\n\nconfiguration sets up the trigger:\n\nname:\n\nCD\n\non:\n\npush:\n\nbranches:\n\n\n\nmain\n\nThe workflow then defines a single job named\n\nBuild & Push Docker Image\n\n:\n\njobs:\n\nbuild:\n\nname:\n\nBuild\n\n&\n\nPush\n\nDocker\n\nImage\n\nruns-on:\n\nubuntu-latest\n\nThe first step within the job is to check out the repository’s code.\n\nsteps:\n\n\n\nname:\n\nCheckout\n\nCode\n\nuses:\n\nactions/checkout@v3\n\nAfter checking out the code, the workflow sets up docker buildx, a Docker CLI plugin that extends Docker’s build capabilities with features like multi- platform builds and cache import/export:\n\n\n\nname:\n\nSet\n\nup\n\nDocker\n\nBuildx\n\nuses:\n\ndocker/setup-buildx-action@v3\n\nThe next step involves configuring the AWS credentials. This step is crucial for interacting with AWS services, such as Amazon Elastic Container Registry (ECR), where the Docker images will be pushed. The AWS access key, secret access key, and region are securely retrieved from the repository’s secrets to authenticate the workflow with AWS. This ensures the workflow has the necessary permissions to push Docker images to the ECR repository. We will show you how to configure these secrets after wrapping up with the YAML file:\n\n\n\nname:\n\nConfigure\n\nAWS\n\ncredentials\n\nuses:\n\naws-actions/configure-aws-credentials@v1\n\nwith:\n\naws-access-key-id:\n\n${{\n\nsecrets.AWS_ACCESS_KEY_ID\n\n}}\n\naws-secret-access-key:\n\n${{\n\nsecrets.AWS_SECRET_ACCESS_KEY\n\n}}\n\naws-region:\n\n${{\n\nsecrets.AWS_REGION\n\n}}\n\nOnce the AWS credentials are configured, the workflow logs in to Amazon ECR. This step is essential for authenticating the Docker CLI with the ECR registry, allowing subsequent steps to push images to the registry:\n\n\n\nname:\n\nLogin\n\nto\n\nAmazon\n\nECR\n\nid:\n\nlogin-ecr\n\nuses:\n\naws-actions/amazon-ecr-login@v1\n\nThe final step in the workflow involves building the Docker image and pushing it to the Amazon ECR repository. This is accomplished using the\n\ndocker/build-push-action@v6\n\naction. The\n\ncontext\n\nspecifies the build context, which is typically the repository’s root directory. The\n\nfile\n\noption points to the\n\nDockerfile\n\n, which defines how the image should be built. The\n\ntags\n\nsection assigns tags to the image, including the specific commit SHA and the\n\nlatest\n\ntag, which is a common practice for identifying the most recent version of the image. The\n\npush\n\noption is set to\n\ntrue\n\n, meaning the image will be uploaded to ECR after it is built:\n\n\n\nname:\n\nBuild\n\nimages\n\n&\n\npush\n\nto\n\nECR\n\nid:\n\nbuild-image\n\nuses:\n\ndocker/build-push-action@v6\n\nwith:\n\ncontext:\n\n.\n\nfile:\n\n./Dockerfile\n\ntags:\n\n|\n\n${{\n\nsteps.login-ecr.outputs.registry\n\n}}/${{\n\nsecrets.AWS_ECR_NAME\n\n}}:${{\n\ngithub.sha\n\n}}\n\n${{\n\nsteps.login-ecr.outputs.registry\n\n}}/${{\n\nsecrets.AWS_ECR_NAME\n\n}}:latest\n\npush:\n\ntrue\n\nTo conclude, the CD pipeline authenticates to AWS, builds the Docker image, and pushes it to AWS ECR. The Docker image is pushed with\n\nlatest\n\nand the commit’s SHA tag. By doing so, we can always use the latest image and point to the commit of the code from which the image was generated.\n\nAlso, in our code, we have only a main branch, which reflects our production environment. But you, as a developer, have the power to extend this functionality with a staging and dev environment. You just have to add the name of the branches in the\n\non.push.branches\n\nconfiguration at the beginning of the YAML file.\n\nIn Figure 11.16, you can observe how the CD pipeline looks after a PR is merged into the production branch. As seen before, we only have the Build & Push Docker Image job here.\n\nFigure 11.16: GitHub Actions CD pipeline run example\n\nThe last step in setting up the CI/CD pipeline is to test it and see how it works.\n\nOceanofPDF.com\n\nTest out the CI/CD pipeline\n\nTo test the CI/CD pipelines yourself, you must fork the LLM-Engineering repository to have full write access to the GitHub repository. Here is the official tutorial on how to fork a GitHub project: https://docs.github.com/en/pull-requests/collaborating-with-pull- requests/working-with-forks/fork-a-repo\n\nThe last step is to set up a few secrets that will allow the CD pipeline to log in to AWS and point to the right ECR resource. To do so, go to the Settings tab at the top of the forked repository in GitHub. In the left panel, in the Security section, click on the Secrets and Variables toggle and, finally, on Actions. Then, on the Secrets tab, create four repository secrets, as shown in Figure 11.17. These secrets will be securely stored and accessible only by the GitHub Actions CD pipeline.\n\nThe\n\nAWS_ACCESS_KEY_ID\n\nand\n\nAWS_SECRET_ACCESS_KEY\n\nare the AWS credentials you used across the book. In Chapter 2, you see how to create them. The\n\nAWS_REGION\n\n(e.g.,\n\neu-central-1\n\n) and\n\nAWS_ECR_NAME\n\nare the same ones used in the Deploying the LLM Twin’s pipelines to the cloud section.\n\nFor the\n\nAWS_ECR_NAME\n\n, you should configure only the name of the repository (e.g.,\n\nzenml-vrsopg\n\n) and not the full URI (e.g., 992382797823.dkr.ecr.eu-central- 1.amazonaws.com/zenml-vrsopg), as seen in the image below:\n\nFigure 11.17: Configuring only repository name\n\nTo trigger the CI pipeline, create a feature branch, modify the code or documentation, and create a PR to the main branch. To trigger the CD pipeline, merge the PR into the main branch.\n\nAfter the CD GitHub Actions are complete, check the ECR repository to see whether the Docker image was pushed successfully.\n\nFigure 11.18: GitHub Actions secrets\n\nIf you need more details on how to set up GitHub Actions secrets, we recommend checking out their official documentation: https://docs.github.com/en/actions/security-for-github-actions/security- guides/using-secrets-in-github-actions\n\nOceanofPDF.com\n\nThe CT pipeline\n\nTo implement the CT pipeline, we will leverage ZenML. Once ZenML (or other orchestrators such as Metaflow, Dagster, or Airflow) orchestrates all your pipelines and your infrastructure is deployed, you are very close to reaching CT.\n\nRemember the core difference between the CI/CD and CT pipelines. The CI/CD pipeline takes care of testing, building, and deploying your code—a dimension that any software program has. The CT pipeline leverages the code managed by the CI/CD pipeline to automate your data, training, and model-serving process, where the data and model dimensions are present only in the AI world.\n\nBefore diving into the implementation, we want to highlight two design choices that made reaching CT simple:\n\nThe FTI architecture: A modular system with clear interfaces and components made it easy to capture the relationship between the pipelines and automate them.\n\nStarting with an orchestrator since day 0: We started with ZenML at the beginning of the project’s development. Early on, we only used it locally. But it acted as an entry point for our pipelines and a way to monitor their execution. Doing so forced us to decouple each pipeline and transfer the communication between them solely through various\n\ntypes of data storage, such as the data warehouse, feature store, or artifact store. As we have leveraged ZenML since day 0, we got rid of implementing a tedious CLI to configure our application. Instead, we did it directly through YAML configuration files out of the box.\n\nIn Figure 11.19, we can see all the pipelines that we have to chain together to fully automate our training and deployment. The pipelines aren’t new; they aggregate everything we’ve covered throughout this book. Thus, at this point, we will treat them as black boxes that interact with each other.\n\nFigure 11.19: CT pipeline\n\nFor the LLM Twin’s CT pipeline, we have to discuss the initial trigger that starts the pipelines and how the pipelines are triggered by each other.\n\nOceanofPDF.com\n\nInitial triggers\n\nAs illustrated in Figure 11.18, we initially want to trigger the data collection pipeline. Usually, the triggers can be of three types:\n\nManual triggers: Done through the CLI or the orchestrator’s dashboard, in our case, through the ZenML dashboard. Manual triggers are still extremely powerful tools, as you need just one action to start the whole ML system, from data gathering to deployment, instead of fiddling with dozens of scripts that you might configure wrong or run in an invalid order.\n\nREST API triggers: You can call a pipeline by an HTTP request. This is extremely useful when integrating your ML pipelines with other components. For example, you can have a watcher constantly looking for new articles. It triggers the ML logic using this REST API trigger when it finds some. To find more details on this feature, check out this tutorial on ZenML’s documentation: https://docs.zenml.io/v/docs/how- to/trigger-pipelines/trigger-a-pipeline-from-rest-api.\n\nScheduled triggers: Another common approach is to schedule your pipeline to run constantly on a fixed interval. For example, depending on your use case, you can schedule your pipeline to run daily, hourly, or every minute. Most of the orchestrators, ZenML included, provide a cron expression interface where you can define your execution frequency. In the following example from ZenML, the pipeline is scheduled every hour:\n\nSchedule(cron_expression=\n\n\"* * 1 * *\"\n\n)\n\nWe chose a manual trigger for our LLM Twin use case as we don’t have other components to leverage the REST API triggers. Also, as the datasets are generated from a list of static links defined in the ZenML configs, running them on a schedule doesn’t make sense as they would always yield the same results.\n\nBut a possible next step for the project is to implement a watcher that monitors for new articles. When it finds any, it generates a new config and triggers the pipelines through the REST API. Another option is implementing the watcher as an additional pipeline and leveraging the schedule triggers to look daily for new data. If it finds any, it executes the whole ML system; otherwise, it stops.\n\nThe conclusion is that once you can manually trigger all your ML pipelines through a single command, you can quickly adapt it to more advanced and complex scenarios.\n\nOceanofPDF.com\n\nTrigger downstream pipelines\n\nTo keep things simple, we sequentially chained all the pipelines. More concretely, when the data collection pipeline has finished, it will trigger the feature pipeline. When the feature pipeline has been completed successfully, it triggers the dataset generation pipeline, and so on. You can make the logic more complex, like scheduling the generate instruct dataset pipeline to run daily, checking the amount of new data in the Qdrant vector DB, and starting only if it has enough new data. From this point, you can further tweak the system’s parameters and optimize them to reduce costs.\n\nTo trigger all the pipelines in one go, we created one master pipeline that aggregates everything in one entry point:\n\n@pipeline\n\ndef\n\nend_to_end_data\n\n(\n\nauthor_links:\n\nlist\n\n[\n\ndict\n\n[\n\nstr\n\n,\n\nstr\n\n|\n\nlist\n\n[\n\nstr\n\n]]], …\n\n# Other paramaters…\n\n) ->\n\nNone\n\n: wait_for_ids = []\n\nfor\n\nauthor_data\n\nin\n\nauthor_links: last_step_invocation_id = digital_data_etl( user_full_name=author_data[\n\n\"user_full_name\"\n\n], links=author_data[\n\n\"links\"\n\n] ) wait_for_ids.append(last_step_invocation_id) author_full_names = [author_data[\n\n\"user_full_name\"\n\n]\n\nfor\n\nauthor_data\n\nin\n\nauthor_links] wait_for_ids = feature_engineering(author_full_names=author_full_names, wait_for=wait_for_ids) generate_instruct_datasets(…) training(…) deploy(…)\n\nTo keep the function light, we added all the logic up to computing the features. But, as we suggested in the code snippet above, you can easily add the instruction dataset generation, training, and deploy logic to the parent pipeline to implement an end-to-end flow. By doing that, you can automate everything from data collection to deploying the model.\n\nTo run the end-to-end pipeline, use the following\n\npoe\n\ncommand:\n\npoetry poe run-end-to-end-data-pipeline\n\nWhat we implemented is not the best approach, as it compresses all the steps into a single monolith pipeline (which we want to avoid), as illustrated in Figure 11.20. Usually, you want to keep each pipeline isolated and use triggers to start downstream pipelines. This makes the system easier to understand, debug, and monitor.\n\nFigure 11.20: End-to-end pipeline illustrated in ZenML’s dashboard\n\nUnfortunately, the ZenML cloud’s free trial has a limitation of a maximum of three pipelines. As we have more, we avoided that limitation by compressing all the steps into a single pipeline. But if you plan to host\n\nZenML yourself or buy their license, they offer the possibility to independently trigger a pipeline from another pipeline, as you can see in the code snippet below where we triggered the feature engineering pipeline after the data collection ETL:\n\nfrom\n\nzenml\n\nimport\n\npipeline, step\n\n@pipeline\n\ndef\n\ndigital_data_etl\n\n(\n\nuser_full_name:\n\nstr\n\n, links:\n\nlist\n\n[\n\nstr\n\n]\n\n) ->\n\nstr\n\n: user = get_or_create_user(user_full_name) crawl_links(user=user, links=links) trigger_feature_engineering_pipeline(user)\n\n@step\n\ndef\n\ntrigger_feature_engineering_pipeline\n\n(\n\nuser\n\n): run_config = PipelineRunConfiguration(…) Client().trigger_pipeline(\n\n\"feature_engineering\"\n\n, run_configuration=run_config)\n\n@pipeline\n\ndef\n\nfeature_engineering\n\n(\n\nauthor_full_names:\n\nlist\n\n[\n\nstr\n\n]\n\n) ->\n\nlist\n\n[\n\nstr\n\n]: …\n\n# ZenML steps\n\nBy taking this approach, each pipeline will have its independent run, where one pipeline sequentially triggers the next one, as described at the beginning of this section. Note that this feature is not unique to ZenML but is common in orchestrator tools. The principles we have learned so far hold. Only how we interact with the tool changes.\n\nOceanofPDF.com\n\nPrompt monitoring\n\nWe will use Opik (from Comet ML) to monitor our prompts. But remember from the LLMOps section earlier in this chapter that we are not interested only in the input prompt and generated answer.\n\nWe want to log the entire trace from the user’s input until the final result is available. Before diving into the LLM Twin use case, let’s look at a simpler example:\n\nfrom\n\nopik\n\nimport\n\ntrack\n\nimport\n\nopenai\n\nfrom\n\nopik.integrations.openai\n\nimport\n\ntrack_openai openai_client = track_openai(openai.OpenAI())\n\n@track\n\ndef\n\npreprocess_input\n\n(\n\ntext:\n\nstr\n\n) ->\n\nstr\n\n:\n\nreturn\n\ntext.strip().lower()\n\n@track\n\ndef\n\ngenerate_response\n\n(\n\nprompt:\n\nstr\n\n) ->\n\nstr\n\n: response = openai_client.chat.completions.create( model=\n\n\"gpt-3.5-turbo\"\n\n, messages=[{\n\n\"role\"\n\n:\n\n\"user\"\n\n,\n\n\"content\"\n\n: prompt}] )\n\nreturn\n\nresponse.choices[\n\n0\n\n].message.content\n\n@track\n\ndef\n\npostprocess_output\n\n(\n\nresponse:\n\nstr\n\n) ->\n\nstr\n\n:\n\nreturn\n\nresponse.capitalize()\n\n@track(\n\nname=\n\n\"llm_chain\"\n\n)\n\ndef\n\nllm_chain\n\n(\n\ninput_text:\n\nstr\n\n) ->\n\nstr\n\n: preprocessed = preprocess_input(input_text) generated = generate_response(preprocessed) postprocessed = postprocess_output(generated)\n\nreturn\n\npostprocessed result = llm_chain(\n\n\"Hello, do you enjoy reading the book?\"\n\n)\n\nThe preceding code snippet reflects in a simplistic way what most LLM applications will look like. You have the\n\nllm_chain()\n\nmain function, which takes the initial input as a parameter and returns the final result.\n\nThen, you have preprocessing and postprocessing functions surrounding the actual LLM call. Using the\n\n@track()\n\ndecorator, we log the input and output of each function, which will ultimately be aggregated into a single trace. By doing so, we will have access to the initial input text, the generated answer, and all the intermediary steps required to debug any potential issues using Opik’s dashboard.\n\nThe last step is to attach the necessary metadata for your use case to the current trace. As seen in the following code snippet, you can easily do that by calling the\n\nupdate()\n\nmethod, where you can tag your trace or add any other metadata, such as the number of input tokens, through a Python dictionary:\n\nfrom\n\nopik\n\nimport\n\ntrack, opik_context\n\n@track\n\ndef\n\nllm_chain\n\n(\n\ninput_text\n\n):\n\n# LLM chain code\n\n# ...\n\nopik_context.update_current_trace( tags=[\n\n\"inference_pipeline\"\n\n], metadata={ \"num_tokens\": compute_num_tokens(…) }, feedback_scores=[ { \"name\": \"user_feedback\", \"value\":\n\n1.0\n\n, \"reason\": \"The response was valuable\n\nand\n\ncorrect.\" }, { \"name\": \"llm_judge_score\", \"value\": compute_llm_judge_score(…), \"reason\": \"Computing runtime metrics using an LLM Judge.\" } )\n\nYou can expand on this idea and log various feedback scores. The most common is asking the user if the generated answer is valuable and correct. Another option is to compute various metrics automatically through heuristics or LLM judges.\n\nFinally, let’s see how to add prompt monitoring to our LLM Twin project. First, look at Figure 11.21 and remember our model-serving architecture. We have two microservices, the LLM and business microservices. The LLM microservice has a narrow scope, as it only takes as input a prompt that already contains the user’s input and context and returns an answer that is usually post-processed. Thus, the business microservice is the right place to implement the monitoring pipeline, as it coordinates the end-to-end flow. More concretely, Opik implementation will be in the FastAPI server developed in Chapter 10.\n\nFigure 11.21: Inference pipeline serving architecture\n\nAs our implementation is already modular, using Opik makes it straightforward to log an end-to-end trace of a user’s request:\n\nfrom\n\nopik\n\nimport\n\ntrack\n\n@track\n\ndef\n\ncall_llm_service\n\n(\n\nquery:\n\nstr\n\n, context:\n\nstr\n\n|\n\nNone\n\n) ->\n\nstr\n\n: llm = LLMInferenceSagemakerEndpoint(…) answer = InferenceExecutor(llm, query, context).execute()\n\nreturn\n\nanswer\n\n@track\n\ndef\n\nrag\n\n(\n\nquery:\n\nstr\n\n) ->\n\nstr\n\n: retriever = ContextRetriever() documents = retriever.search(query, k=\n\n3\n\n\n\n3\n\n) context = EmbeddedChunk.to_context(documents) answer = call_llm_service(query, context)\n\nreturn\n\nanswer\n\nThe\n\nrag()\n\nfunction represents your application’s entry point. All the other processing steps take place in the\n\nContextRetriever\n\nand\n\nInferenceExector\n\nclasses. Also, by decorating the\n\ncall_llm_service()\n\nfunction, we can clearly capture the prompt sent to the LLM and its response.\n\nTo add more granularity to our trace, we can further decorate other functions containing pre- or post-processing steps, such as the\n\nContextRetriever\n\nsearch function:\n\nclass\n\nContextRetriever\n\n: …\n\n@track\n\ndef\n\nsearch\n\n(\n\nself,\n\nquery:\n\nstr\n\n,\n\nk:\n\nint\n\n=\n\n3\n\n,\n\nexpand_to_n_queries:\n\nint\n\n=\n\n3\n\n,\n\n) ->\n\nlist\n\n: query_model = Query.from_str(query) query_model = self._metadata_extractor.generate(query_model) …\n\n# Rest of the implementation\n\nOr even go further to the retrieval optimization methods, such as the self- query metadata extractor, to add more granularity:\n\nclass\n\nSelfQuery\n\n:\n\n@track\n\ndef\n\ngenerate\n\n(\n\nself, query:\n\nstr\n\n) ->\n\nstr\n\n: …\n\nreturn\n\nenhanced_query\n\nThe developer is responsible for deciding how much granularity the application needs for proper debugging and analysis. As having detailed monitoring is healthy, monitoring everything can be dangerous as it adds too much noise and makes manually understanding the traces difficult. You must find the right balance. A good rule of thumb is tracing the most critical functions, such as\n\nrag()\n\nand\n\ncall_llm_service()\n\n, and gradually adding more granularity when needed.\n\nThe last step is to attach valuable metadata and tags to our traces. To do so, we will further enhance the\n\nrag()\n\nfunction as follows:\n\n@track\n\ndef\n\nrag\n\n(\n\nquery:\n\nstr\n\n) ->\n\nstr\n\n: retriever = ContextRetriever() documents = retriever.search(query, k=\n\n3\n\n\n\n3\n\n) context = EmbeddedChunk.to_context(documents) answer, prompt = call_llm_service(query, context) trace = get_current_trace() trace.update( tags=[\n\n\"rag\"\n\n], metadata={ \"model_id\": settings.HF_MODEL_ID, \"embedding_model_id\": settings.TEXT_EMBEDDING_MODEL_ID, \"temperature\": settings.TEMPERATURE_INFERENCE, \"prompt_tokens\": compute_num_tokens(prompt), \"total_tokens\": compute_num_tokens(answer), } )\n\nreturn\n\nanswer\n\nThere are three main aspects that we should constantly monitor:\n\nModel configuration: Here, we should consider both the LLM and other models used within the RAG layer. The most critical aspects of logging are the model IDs, but you can also capture other important information that significantly impacts the generation, such as the temperature.\n\nTotal number of tokens: It’s critical to constantly analyze the statistics of the number of tokens generated by your input prompts and total tokens, as this significantly impacts your serving costs. For example, if the average of the total number of tokens generated suddenly increases, it’s a strong signal that you have a bug in your system that you should investigate.\n\nThe duration of each step: Tracking the duration of each step within your trace is essential to finding bottlenecks within your system. If the latency of a specific request is abnormally large, you quickly have access to a report that helps you find the source of the problem.\n\nOceanofPDF.com\n\nAlerting\n\nUsing ZenML, you can quickly implement an alerting system on any platform of your liking, such as email, Discord, or Slack. For example, you can add a callback in your training pipeline to trigger a notification when the pipeline fails or the training has finished successfully:\n\nfrom\n\nzenml\n\nimport\n\nget_pipeline_context, pipeline\n\n@pipeline(\n\non_failure=notify_on_failure\n\n)\n\ndef\n\ntraining_pipeline\n\n(\n\n…\n\n): … notify_on_success()\n\nImplementing the notification functions is straightforward. As seen in the code snippets below, you have to get the\n\nalerter\n\ninstance from your current stack, build the message as you see fit, and send it to your notification channel of choice:\n\nfrom\n\nzenml.client\n\nimport\n\nClient alerter = Client().active_stack.alerter\n\ndef\n\nnotify_on_failure\n\n() ->\n\nNone\n\n: alerter.post(message=build_message(status=\n\n\"failed\"\n\n))\n\n@step(\n\nenable_cache=\n\nFalse\n\n)\n\ndef\n\nnotify_on_success\n\n() ->\n\nNone\n\n: alerter.post(message=build_message(status=\n\n\"succeeded\"\n\n))\n\nZenML and most orchestrators simplify implementing an\n\nalerter\n\n, as it’s a critical component in your MLOps/LLMOps infrastructure.\n\nOceanofPDF.com\n\nSummary\n\nIn this chapter, we laid down the foundations with a theoretical section on DevOps. Then, we moved on to MLOps and its core components and principles. Finally, we presented how LLMOps differs from MLOps by introducing strategies such as prompt monitoring, guardrails, and human-in- the-loop feedback. Also, we briefly discussed why most companies would avoid training LLMs from scratch but choose to optimize them for their use case through prompt engineering or fine-tuning. At the end of the theoretical portion of the chapter, we learned what a CI/CD/CT pipeline is, the three core dimensions of an ML application (code, data, model), and that, after deployment, it is more critical than ever to implement a monitoring and alerting layer due to model degradation.\n\nNext, we learned how to deploy the LLM Twin’s pipeline to the cloud. We understood the infrastructure and went step by step through deploying MongoDB, Qdrant, the ZenML cloud, and all the necessary AWS resources to sustain the application. Finally, we learned how to Dockerize our application and push our Docker image to AWS ECR, which will be used to execute the application on top of AWS SageMaker.\n\nThe final step was to add LLMOps to our LLM Twin project. We began by implementing a CI/CD pipeline with GitHub Actions. Then, we looked at our CT strategy by leveraging ZenML.\n\nFinally, we saw how to implement a monitoring pipeline using Opik from Comet ML and an alerting system using ZenML. These are the fundamental\n\npillars in adding MLOps and LLMOps to any LLM-based application.\n\nThe framework we learned about throughout the book can quickly be extrapolated to other LLM applications. Even if we used the LLM Twin use case as an example, most of the strategies applied can be adapted to other projects. Thus, we can get an entirely new application by changing the data and making minor tweaks to the code. Data is the new oil, remember?\n\nBy finalizing this chapter, we’ve learned to build an end-to-end LLM application, starting with data collection and fine-tuning until deploying the LLM microservice and RAG service. Throughout this book, we aimed to provide a thought framework to help you build and solve real-world problems in the GenAI landscape. Now that you have it, we wish you good luck in your journey and happy building!\n\nOceanofPDF.com\n\nReferences\n\nGitLab. (2023, January 25). What is DevOps? | GitLab. GitLab. https://about.gitlab.com/topics/devops/\n\nHuyen, C. (2024, July 25). Building a generative AI platform. Chip Huyen. https://huyenchip.com/2024/07/25/genai-platform.html\n\nLightricks customer story: Building a recommendation engine from scratch. (n.d.). https://www.qwak.com/academy/lightricks-customer-story-building- a-recommendation-engine-from-scratch\n\nWhat LLMOps. (n.d.). Google Cloud. https://cloud.google.com/discover/what-is-llmops?hl=en\n\nMLOps: Continuous delivery and automation pipelines in machine learning. (2024, August 28). Google Cloud. https://cloud.google.com/architecture/mlops-continuous-delivery-and- automation-pipelines-in-machine-learning#top_of_page\n\nMl-ops.org. (2024a, July 5). https://ml-ops.org/content/mlops-principles\n\nMl-ops.org. (2024b, July 5). https://ml-ops.org/content/mlops-principles\n\nMl-ops.org. (2024c, July 5). https://ml-ops.org/content/motivation\n\nMohandas, G. M. (2022a). Monitoring machine learning systems. Made With ML. https://madewithml.com/courses/mlops/monitoring/\n\nMohandas, G. M. (2022b). Testing Machine Learning Systems: Code, Data and Models. Made With ML. https://madewithml.com/courses/mlops/testing/\n\nPreston-Werner, T. (n.d.). Semantic Versioning 2.0.0. Semantic Versioning. https://semver.org/\n\nRibeiro, M. T., Wu, T., Guestrin, C., & Singh, S. (2020, May 8). Beyond Accuracy: Behavioral Testing of NLP models with CheckList. arXiv.org. https://arxiv.org/abs/2005.04118\n\nWandb. (2023, November 30). Understanding LLMOps: Large Language Model Operations. Weights & Biases. https://wandb.ai/site/articles/understanding-llmops-large-language-model- operations/\n\nZenml-Io. (n.d.). GitHub—zenml-io/zenml-huggingface-sagemaker: An example MLOps overview of ZenML pipelines from a Hugging Face model\n\nrepository to a deployed AWS SageMaker endpoint. GitHub. https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/main\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\nAppendix\n\nOceanofPDF.com\n\nMLOps Principles\n\nBuilding robust and scalable ML systems requires more than creating powerful models. It demands an all-encompassing approach to operationalizing the entire ML lifecycle. Let’s explore the six core principles that guide the MLOps field. These principles are independent of any tool and are at the core of building robust and scalable ML systems. They provide a guideline for designing production-ready applications, ensuring consistency, reliability, and scalability at every stage.\n\nWith that in mind, let’s begin with the foundation: automation or operationalization.\n\nOceanofPDF.com\n\n1. Automation or operationalization\n\nTo adopt MLOps, there are three core tiers that most applications build up gradually, from manual processing to full automation:\n\nManual process: The process is experimental and iterative in the early stages of developing an ML application. The data scientist manually performs each pipeline step, such as data preparation and validation, model training, and testing. At this point, they commonly use Jupyter notebooks to train their models. This stage’s output is the code used to prepare the data and train the models.\n\nContinuoustraining (CT): The next level involves automating model training. This is known as continuous training, which triggers model retraining whenever required. At this point, you often automate your data and model validation steps. This step is usually done by an orchestration tool, such as ZenML, that glues all your code together and runs it on specific triggers. The most common triggers are on a schedule, for example, every day or when a specific event comes in, such as when new data is uploaded or the monitoring system detects a drop in performance, offering you the flexibility to adapt to various triggers.\n\nCI/CD: In the final stage, you implement your CI/CD pipelines to enable fast and reliable deployment of your ML code into production. The key advancement at this stage is the automatic building, testing, and deployment of data, ML models, and training pipeline components. CI/CD is used to\n\nquickly push new code into various environments, such as staging or production, ensuring efficient and reliable deployment.\n\nAs we build our LLM system using the FTI (feature, training, inference) architecture, we can quickly move from a manual process to CI/CD/CT. In Figure A.1, we can observe that the CT process can be triggered by various events, such as a drop in performance detected by the monitoring pipeline or a batch of fresh data arriving. Also, Figure A.1 is split into two main sections; the first one highlights the automated processes, while at the bottom, we can observe the manual processes performed by the data science team while experimenting with various data processing methods and models. Once they improve the model by tinkering with how the data is processed or the model architecture, they push the code to the code repository, which triggers the CI/CD pipeline to build, test, package, and deploy the new changes to the FTI pipelines.\n\nFigure A.1: CI/CD/CT on top of the FTI architecture\n\nTo conclude, CT automates the FTI pipelines, while CI/CD builds, tests, and pushes new versions of the FTI pipeline code to production.\n\nOceanofPDF.com\n\n2. Versioning\n\nBy now, we understand that the whole ML system changes if the code, model, or data changes. Thus, it is critical to track and version these three elements individually. But what strategies can we adopt to track the code, model, and data separately?\n\nThe code is tracked by Git, which helps us create a new commit (a snapshot of the code) on every change added to the codebase. Also, Git- based tools usually allow us to make releases, which typically pack multiple features and bug fixes. While the commits contain unique identifiers that are not human-interpretable, a release follows more common conventions based on their major, minor, and patch versions. For example, in a release with version “v1.2.3,” 1 is the major version, 2 is the minor version, and 3 is the patch version. Popular tools are GitHub and GitLab.\n\nTo version the model, you leverage the model registry to store, share, and version all the models used within your system. It usually follows the same versioning conventions used in code releases, defined as Semantic Versioning, which, along with the major, minor, and patch versions, also supports alpha and beta releases that signal applications. At this point, you can also leverage the ML metadata store to attach information to the stored model, such as what data it was trained on, its architecture, performance, latency, and whatever else makes sense to your specific use case. Doing so creates a clear catalog of models that can easily be navigated across your team and company.\n\nVersioning the data isn’t as straightforward as versioning the code and model because it depends on the type of data you have (structured or unstructured) and the scale of data you have (big or small). For example, for structured data, you can leverage a SQL database with a version column that helps you track the changes in the dataset. However, other popular solutions are based on Git-like systems, such as Data Version Control (DVC), that track every change made to the dataset. Other trendy solutions are based on artifacts similar to a model registry that allows you to add a virtual layer to your dataset, tracking and creating a new version for every change made to your data. Comet.ml, W&B (Weights & Biases), and ZenML offer powerful artifact features. For all solutions, you must store the data on-premises or use cloud object storage solutions such as AWS S3. These tools provide features that allow you to structure your datasets and versions, track, and access them.\n\nOceanofPDF.com\n\n3. Experiment tracking\n\nTraining ML models is an entirely iterative and experimental process. Unlike traditional software development, it involves running multiple parallel experiments, comparing them based on a set of predefined metrics, and deciding which one should advance to production. An experiment tracking tool allows you to log all the necessary information, such as metrics and visual representations of your model predictions, to compare all your experiments and easily select the best model. Popular tools are Comet ML, W&B, MLflow, and Neptune.\n\nOceanofPDF.com\n\n4. Testing\n\nThe same trend is followed when testing ML systems. Hence, we must test our application across all three dimensions: the data, the model, and the code. We must also ensure that the feature, training, and inference pipeline are well integrated with external services, such as the feature store, and work together as a system. When working with Python, the most common tool to write your tests is\n\npytest\n\n, which we also recommend.\n\nOceanofPDF.com\n\nTest types\n\nIn the development cycle, six primary types of tests are commonly employed at various stages:\n\nUnit tests: These tests focus on individual components with a single responsibility, such as a function that adds two tensors or one that finds an element in a list.\n\nIntegration tests: These tests evaluate the interaction between integrated components or units within a system, such as the data evaluation pipeline or the feature engineering pipeline, and how they are integrated with the data warehouse and feature store.\n\nSystem tests: System tests play a crucial role in the development cycle as they examine the entire system, including the complete and integrated application. These tests rigorously evaluate the end-to-end functionality of the system, including performance, security, and overall user experience—for example, testing an entire ML pipeline, from data ingestion to model training and inference, ensuring the system produces the correct outputs for given inputs.\n\nAcceptance tests: These tests, often called user acceptance testing (UAT), are designed to confirm that the system meets specified requirements, ensuring it is ready for deployment.\n\nRegression tests: These tests check for previously identified errors to ensure that new changes do not reintroduce them.\n\nStress tests: These tests evaluate the system’s performance and stability under extreme conditions, such as high load or limited resources. They aim to identify breaking points and ensure the system can handle unexpected spikes in demand or adverse situations without failing.\n\nFigure A.2: Test types\n\nWe’ve intentionally left regression tests out of the preceding figure because they aren’t a distinct testing phase. Instead, regression testing is applied across all levels—unit, integration, system, acceptance, and stress tests—to\n\nensure that changes don’t reintroduce previous errors. It’s an ongoing process within these phases, not a separate type of test, which is why it’s not shown as a separate category.\n\nOceanofPDF.com\n\nWhat do we test?\n\nWhen writing most tests, you take a component and treat it as a black box. Thus, what you have control over is the input and output. You want to test that you get an expected output for a given input. With that in mind, here are a few things you should usually test:\n\nInputs: Data types, format, length, and edge cases (min/max, small/large, etc.)\n\nOutputs: Data types, formats, exceptions, and intermediary and final outputs\n\nOceanofPDF.com\n\nTest examples\n\nWhen testing your code, you can leverage the standards from classic software engineering. Here are a few examples of code tests you can include when writing unit tests to get a better idea of what we want to test at this point—for instance, you want to check that a sentence is cleaned as expected.\n\nAlso, you can look at your chunking algorithm and assert that it works properly by using various sentences and chunk sizes.\n\nWhen we talk about data tests, we mainly refer to data validity. Your data validity code usually runs when raw data is ingested from the data warehouse or after computing the features. It is part of the feature pipeline. Thus, by writing integration or system tests for your feature pipeline, you can check that your system responds properly to valid and invalid data.\n\nTesting data validity depends a lot on your application and data type. For example, when working with tabular data, you can check for non-null values, that a categorical variable contains only the expected values, or that a float value is always positive. You can check for length, character encoding, language, special characters, and grammar errors when working with unstructured data such as text.\n\nModel tests are the trickiest, as model training is the most non- deterministic process of an ML system. However, unlike traditional software, ML systems can successfully complete without throwing any errors. However, the real issue is that they produce incorrect results that can only be observed during evaluations or tests. Some standard model test techniques involve checking:\n\nThe shapes of the input and model output tensors\n\nThat the loss decreases after one batch (or more) of training\n\nOverfit on a small batch, and the loss approaches 0\n\nThat your training pipeline works on all the supported devices, such as the CPU and GPU\n\nThat your early stopping and checkpoint logic works\n\nAll the tests are triggered inside the CI pipeline. If some tests are more costly, for example, the model ones, you can execute them only on special terms, such as only when modifying the model code.\n\nAt the other end of the spectrum, you can also perform behavioral testing on your model, which tries to adopt the strategy from code\n\ntesting and treats the model as a black box while looking solely at the input data and expected outputs. This makes the behavioral testing methods model agnostic. A fundamental paper in this area is Beyond Accuracy: Behavioral Testing of NLP Models with CheckList, which we recommend if you want to dig more into the subject. However, as a quick overview, the paper proposes that you test your model against three types of tests. We use a model that extracts the main subject from a sentence as an example:\n\nInvariance: Changes in your input should not affect the output—for example, below is an example based on synonym injection:\n\nmodel(text=\n\n\"The advancements in AI are changing the world rapidly.\"\n\n)\n\n# output: ai\n\nmodel(text=\n\n\"The progress in AI is changing the world rapidly.\"\n\n)\n\n# output: ai\n\nDirectional: Changes in your input should affect the outputs—for example, below is an example where we know the outputs should change based on the provided inputs:\n\nmodel(text=\n\n\"Deep learning used for sentiment analysis.\"\n\n)\n\n# output: deep-learning\n\nmodel(text=\n\n\"Deep learning used for object detection.\"\n\n)\n\n# output: deep-learning\n\nmodel(text=\n\n\"RNNs for sentiment analysis.\"\n\n)\n\n# output: rnn\n\nMinimum functionality: The most simple combination of inputs and expected outputs—for example, below is a set of simple examples that we expect the model should always get right:\n\nmodel(text=\n\n\"NLP is the next big wave in machine learning.\"\n\n)\n\n# output: nlp\n\nmodel(text=\n\n\"MLOps is the next big wave in machine learning.\"\n\n)\n\n# output: mlops\n\nmodel(text=\n\n\"This is about graph neural networks.\"\n\n)\n\n# output: gnn\n\nFor more on testing, we recommend reading Testing Machine Learning Systems: Code, Data, and Models by Goku Mohandas: https://madewithml.com/courses/mlops/testing/.\n\nOceanofPDF.com\n\n5. Monitoring\n\nMonitoring is vital for any ML system that reaches production. Traditional software systems are rule-based and deterministic. Thus, once it is built, it will always work as defined. Unfortunately, that is not the case with ML systems. When implementing ML models, we haven’t explicitly described how they should work. We have used data to compile a probabilistic solution, which means that our ML model will constantly be exposed to a level of degradation. This happens because the data from production might differ from the data the model was trained on. Thus, it is natural that the shipped model doesn’t know how to handle these scenarios.\n\nWe shouldn’t try to avoid these situations but create a strategy to catch and fix these errors in time. Intuitively, monitoring detects the model’s performance degradation, which triggers an alarm that signals that the model should be retrained manually, automatically, or with a combination of both.\n\nWhy retrain the model? As the model performance degrades due to a drift in the training dataset and what it inputs from production, the only solution is to adapt or retrain the model on a new dataset that captures all the new scenarios from production.\n\nAs training is a costly operation, there are some tricks that you can perform to avoid retraining, but before describing them, let’s quickly understand what we can monitor to understand our ML system’s health.\n\nOceanofPDF.com\n\nLogs\n\nThe approach to logging is straightforward, which is to capture everything, such as:\n\nDocument the system configurations.\n\nRecord the query, the results, and any intermediate outputs.\n\nLog when a component begins, ends, crashes, and so on.\n\nEnsure that each log entry is tagged and identified in a way that clarifies its origin within the system.\n\nWhile capturing all activities can rapidly increase the volume of logs, you can take advantage of numerous tools for automated log analysis and anomaly detection that leverage AI to efficiently scan all the logs, providing you with the confidence to manage the logs effectively.\n\nOceanofPDF.com\n\nMetrics\n\nTo quantify your application’s healthiness, you must define a set of metrics. Each metric measures different aspects of your application, such as the infrastructure, data, and model.\n\nOceanofPDF.com\n\nSystem metrics\n\nThe system metrics are based on monitoring service-level metrics (latency, throughput, error rates) and infrastructure health (CPU/GPU, memory). These metrics are used both in traditional software and ML as they are crucial to understanding whether the infrastructure works well and the system works as expected to provide a good user experience to the end users.\n\nOceanofPDF.com\n\nModel metrics\n\nMerely monitoring the system’s health won’t suffice to identify the deeper issues within our model. Therefore, moving on to the next layer of metrics that focus on the model’s performance is crucial. This includes quantitative evaluation metrics like accuracy, precision, and F1 score, as well as essential business metrics influenced by the model, such as ROI and click rate.\n\nAnalyzing cumulative performance metrics over the entire deployment period is often ineffective. Instead, evaluating performance over time intervals relevant to our application, such as hourly, is essential. Thus, in practice, you window your inputs and compute and aggregate the metrics at the window level. These sliding metrics can provide a clearer picture of the system’s health, allowing us to detect issues more promptly without them being obscured by historical data.\n\nWe may not always have access to ground-truth outcomes to evaluate the model’s performance on production data. This is particularly challenging when there is a significant delay or when real-life data requires annotation. To address this issue, we can develop an approximate signal to estimate the model’s performance or label a small portion of our live dataset to assess performance. When talking about ML monitoring, an approximate signal is also known as a proxy metric, usually implemented by drift detection methods, which are discussed in the following section.\n\nOceanofPDF.com\n\nDrifts\n\nDrifts are proxy metrics that help us detect potential issues with the production model in time without requiring any ground truths/labels. Table A.1 shows three kinds of drifts.\n\nWhat drifts\n\nDescription\n\nDrift formulation\n\nInputs (features)\n\nOutputs (ground truths/labels)\n\nTable A.1: Relationship between data, model, and code changes\n\nData drift\n\nData drift, also called feature drift or covariate shift, occurs when the distribution of the production data deviates from that of the training data, as shown in Figure A.3. This difference means the model cannot handle the changes in feature space, leading to potentially unreliable predictions. Drift can result from natural real-life changes or systemic problems like missing data, pipeline errors, and schema modifications.\n\nFigure A.3: Data drift examples\n\nWhen data begins to drift, the degradation in our model’s performance might not be immediately noticeable, particularly if the model interpolates effectively. Nevertheless, this presents an ideal chance to consider retraining before the drift affects the model’s performance.\n\nTarget drift\n\nIn addition to changes in input data (data drift), we might also encounter shifts in output distribution. The shift could involve changes in the shape of the distribution or the addition and removal of classes in categorical tasks. While retraining the model can help reduce performance degradation due to target drift, you can often prevent it by adapting the head processing steps and model head to support the new schema of the output class.\n\nFor example, if you have a classifier that predicts if an image contains animals or people, and you get a picture with buildings, you can either adapt your model to support an unknown class or adjust the head of the model to add the new class for future predictions.\n\nConcept drift\n\nIn addition to changes in input and output data, their relationship can also shift. This phenomenon, known as concept drift, makes our model ineffective because the patterns it previously learned to associate inputs with outputs become outdated. As illustrated in the following figure, concept drifts can manifest in various ways:\n\nGradually over time\n\nSuddenly, due to an external event\n\nPeriodically, due to recurring events\n\nFigure A.4: Concept drift examples\n\nFor example, this happens when using the model in a different geographic area. Let’s assume you want to build a model that predicts whether a person will buy a specific car. You initially built it for the American market. Now, you want to use it in the European market, where people tend to buy smaller cars, creating a drift between the size feature of the car and the output probability of purchasing the vehicle. Of course, concept drifts can be more subtle than this example.\n\nAll these types of drift can happen simultaneously, complicating pinpointing the exact sources of drift.\n\nHow to detect and measure drifts\n\nNow that we’ve recognized the various types of drift, it’s crucial to understand how to detect and measure it. To do so, you need two types of windows:\n\nA reference window: This is the collection of data points used as a baseline to compare against the production data distributions for drift identification. It is usually gathered from the training dataset.\n\nA test window: This collects data points gathered while the ML system is in production. It is compared with the reference window to ascertain if drift has occurred.\n\nTo measure the drifts, you leverage hypothesis tests that verify the change in distribution between the two windows. For example, you can use the Kolmogorov-Smirnov (KS) test to monitor a single continuous feature. This is known as a univariate (1D) test. Thus, you must run it for every feature you want to monitor. You can leverage a chi-squared univariate test to monitor categorical variables and determine if the frequency of events in production is consistent with the reference window distribution.\n\nfrom\n\nalibi_detect.cd\n\nimport\n\nKSDrift cd = KSDrift(X_ref, p_val=\n\n.05\n\n, preprocess_fn=preprocess_fn, input_shape=(max_len,))\n\nWhen working with text data in an embedding representation, we have to model a multivariate distribution, which is how LLMs work with text. A popular approach is to take the embeddings of the test and reference windows, apply a dimensionality reduction algorithm, and apply an algorithm such as maximum mean discrepancy (MMD). This algorithm is a kernel-based approach that measures the distance between two distributions by computing the distance between the mean of the embeddings of the two windows.\n\nfrom\n\nalibi_detect.cd\n\nimport\n\nMMDDrift cd = MMDDrift(x_ref, backend=\n\n'pytorch'\n\n, p_val=\n\n.05\n\n) preds = cd.predict(x)\n\nOceanofPDF.com\n\nMonitoring vs. observability\n\nMonitoring involves the collection and visualization of data, whereas observability provides insights into system health by examining its inputs and outputs. For instance, monitoring allows us to track a specific metric to detect potential issues.\n\nOn the other hand, a system is considered observable if it generates meaningful data about its internal state, which is essential for diagnosing root causes.\n\nOceanofPDF.com\n\nAlerts\n\nOnce we define our monitoring metrics, we need a way to get notified. The most common approaches are to send an alarm in the following scenarios:\n\nA metric passes the values of a static threshold—for example, when the accuracy of the classifier is lower than 0.8, send an alarm.\n\nTweaking the p-value of the statistical tests that check for drifts. A lower p- value means a higher confidence that the production distribution differs from the reference one.\n\nThese thresholds and p-values depend on your application. However, it is essential to find the correct values, as you don’t want to overcrowd your alarming system with false positives. In that case, your alarm system won’t be trustworthy, and you will either overreact or not react at all to issues in your system. Some common channels for sending alarms to your stakeholders are Slack, Discord, your email, and PagerDuty. The system’s stakeholders can be the core engineers, managers, or anyone interested in the system.\n\nDepending on the nature of the alarm, you have to react differently. But before taking any action, you should be able to inspect it and understand what caused it. You should inspect what metric triggered the alarm, with\n\nwhat value, the time it happened, and anything else that makes sense to your application.\n\nWhen the model’s performance degrades, the first impulse is to retrain it. But that is a costly operation. Thus, you first have to check that the data is valid, the schema hasn’t changed, and the data point was not an isolated outlier. If neither is true, you should trigger the training pipeline and train the model on the newly shifted dataset to solve the drift.\n\nOceanofPDF.com\n\n6. Reproducibility\n\nReproducibility means that every process within your ML systems should produce identical results given the same input. This has two main aspects.\n\nThe first one is that you should always know what the inputs are—for example, when training a model, you can use a plethora of hyperparameters. Thus, you need a way to always track what assets were used to generate the new assets, such as what dataset version and config were used to train the model.\n\nThe second aspect is based on the non-deterministic nature of ML processes. For example, when training a model from scratch, all the weights are initially randomly initialized. Thus, even if you use the same dataset and hyperparameters, you might end up with a model with a different performance. This aspect can be solved by always using a seed before generating random numbers, as in reality, we cannot digitally create randomness, only pseudo-random numbers. Thus, by providing a seed, we ensure that we always produce the same trace of pseudo-random numbers. This can also happen at the feature engineering step, in case we impute values with random values or randomly remove data or labels. But as a general rule of thumb, always try to make your processes as deterministic as possible, and in case you have to introduce randomness, always provide a seed that you have control over.\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng\n\npackt.com\n\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.\n\nWhy subscribe?\n\nSpend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\nFully searchable for easy access to vital information\n\nCopy and paste, print, and bookmark content\n\nAt www.packt.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\n\nOceanofPDF.com\n\nOther Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other books by Packt:\n\nRAG-Driven Generative AI\n\nDenis Rothman\n\nISBN: 9781836200918\n\nScale RAG pipelines to handle large datasets efficiently\n\nEmploy techniques that minimize hallucinations and ensure accurate responses\n\nImplement indexing techniques to improve AI accuracy with traceable and transparent outputs\n\nCustomize and scale RAG-driven generative AI systems across domains\n\nFind out how to use Deep Lake and Pinecone for efficient and fast data retrieval\n\nControl and build robust generative AI systems grounded in real-world data\n\nCombine text and image data for richer, more informative AI responses\n\nBuilding LLM Powered Applications\n\nValentina Alto\n\nISBN: 9781835462317\n\nExplore the core components of LLM architecture, including encoder- decoder blocks and embeddings\n\nUnderstand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM\n\nUse AI orchestrators like LangChain, with Streamlit for the frontend\n\nGet familiar with LLM components such as memory, prompts, and tools\n\nLearn how to use non-parametric knowledge and vector databases\n\nUnderstand the implications of LFMs for AI research and industry applications\n\nCustomize your LLMs with fine tuning\n\nLearn about the ethical implications of LLM-powered applications\n\nPackt is searching for authors like you\n\nIf you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\n\nShare your thoughts\n\nNow you’ve finished LLM Engineer’s Handbook, First Edition, we’d love to hear your thoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon review page for this book and share your feedback or leave a review on the site that you purchased it from.\n\nYour review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.\n\nOceanofPDF.com\n\nSymbols\n\n4-bit NormalFloat (NF4) 215\n\n32-bit floating point (fp32) 211, 212\n\nA\n\nacceptance tests 464\n\nactions 437\n\nActivate-aware Weight Quantization (AWQ) 313\n\nadvanced RAG\n\nIndex\n\noverview 117, 118\n\npost-retrieval step 124-126\n\npre-retrieval steps 119-122\n\nretrieval step 122-124\n\nadvanced RAG post-retrieval optimization\n\nreranking 334-338\n\nadvanced RAG pre-retrieval optimizations 324\n\nquery expansion 324-328\n\nself-querying 328-332\n\nadvanced RAG retrieval optimization\n\nfiltered vector search 332-334\n\nadvanced RAG techniques\n\nexploring 321-324\n\npost-retrieval optimization 334-338\n\npre-retrieval optimizations 324-332\n\nretrieval optimization 332-334\n\nalerting system 457, 458\n\nalerts 473\n\nAlpacaEval 264\n\nAmazon Resource Name (ARN) 375\n\nApplication Auto Scaling 396, 397\n\nApplication Load Balancer (ALB) 395\n\nasynchronous inference 361, 362\n\nautoscaling 393, 399\n\nscalable policy, creating 397\n\nscalable target, registering 396\n\nuse cases 394\n\nAWS\n\naccess key, setting up 48-50\n\naccount, setting up 48-50\n\nCLI, setting up 48-50\n\npreparing 48\n\nSageMaker 50\n\nAWS Elastic Container Service (ECS) 393\n\nAWS Elastic Kubernetes Service (EKS) 393\n\nAWS SageMaker 50\n\nLLM Twin model, deploying to 375-385\n\nneed for 51, 52\n\nAWS SageMaker Inference endpoint\n\ncalling 386-389\n\nautomated evaluation framework for RAG systems (ARES) 274, 275\n\nB\n\nbacked-up data\n\nimporting 95\n\nBaseCrawler interface 69-72\n\nbehavioral testing 466\n\nbias types\n\nfamily bias 237\n\nlength bias 237\n\nposition bias 237\n\nBigCodeBench Leaderboard 266\n\nbusiness microservice\n\nbuilding, with FastAPI 390-393\n\nC\n\nCDC patterns\n\nlog-based 137\n\ntimestamp-based 137\n\ntrigger-based 137\n\ncentral access point 128\n\nChange data capture (CDC) 136\n\nChatbot Arena 264\n\nChatbots 231\n\nChatGPT 5\n\nlimitations 5\n\nchat templates 208-210\n\nchunking handlers 165-169\n\nCI/CD pipeline 462\n\nCI pipeline, LLM Twin\n\nQA job 438\n\ntest job 438\n\nCircleCI 405\n\nclassifiers models 189\n\ncleaning handlers 163-165\n\nCloudFormation 423\n\ncode generation 231\n\nComet ML 45, 46\n\nconcept drift 471\n\ncontent moderation 231\n\ncontinuous batching 294\n\ncontinuous integration and continuous deployment (CI/CD) pipeline 31, 402\n\ncontinuous training (CT) 138, 402, 461\n\ncooldown period 398\n\nco-pilot\n\nversus LLM Twin 4\n\ncovariate drift 470\n\nCrawlerDispatcher class 66-68\n\ncrawlers\n\nBaseCrawler interface 69-72\n\nCustomArticleCrawler class 75-77\n\nGithubCrawler class 73-75\n\nimplementing 69\n\nMediumCrawler class 77-79\n\nCustomArticleCrawler class 75-77\n\nD\n\ndata augmentation 193-196\n\ndatabase (DB) 317, 410\n\ndatabase, for unstructured and vector data\n\nMongoDB 47\n\nQdrant 47, 48\n\nstoring 47\n\ndata collection pipeline 19\n\ndata curation 182\n\ndata decontamination 185\n\ndata deduplication 184, 185\n\ndata drift 470\n\ndata evaluation 233\n\ndata exploration 189-191\n\ndata generation 191-233\n\npreference data, evaluating 235-237\n\npreference data, generating 233, 234\n\ntips 234\n\ndata indexing techniques 119\n\ndata parallelism (DP) 299\n\ndata quality evaluation 186-189\n\ndata quantity 180, 181\n\nData Scientist (DS) 409\n\ndataset formats 208\n\ndata tests 466\n\ndecoder-only model\n\narchitecture 290\n\ncomputing 291\n\ngenerating 291\n\ntokenizing 291\n\nDeep Learning Containers (DLCs) 373\n\ndeployment costs 415\n\ndeployment types, criteria for selection\n\ndata 357\n\ninfrastructure 357, 358\n\nlatency 356\n\nthroughput 356, 357\n\nDevOps 401-403\n\nbenefits 403\n\ncontinuous delivery (CD) 405\n\ncontinuous integration (CI) 405\n\ndeployment environments 404\n\nversion control 405\n\nDevOps lifecycle\n\nbuild 404\n\ncode 403\n\ndeploy 404\n\nmonitor 404\n\noperate 404\n\nplan 403\n\nrelease 404\n\ntest 404\n\ndirectional 467\n\nDirect Preference Optimization (DPO) 229, 245, 248-250, 411\n\nimplementing 250-257\n\ndispatcher layer 160-162\n\nDLC image\n\nfeatures 373\n\nDocker 424\n\nDockerfile 424\n\ndomain-driven design (DDD) 150\n\ndomain-specific LLM evaluations 265-267\n\ndownstream pipelines\n\ntriggering 449-451\n\nDPO datasets\n\nhuman-generated, human-evaluated datasets 233\n\nhuman-generated, LLM-evaluated datasets 233\n\nLLM-generated, human-evaluated datasets 234\n\nLLM-generated, LLM-evaluated datasets 234\n\ndrifts 469\n\nconcept drift 471\n\ndata drift 470\n\ndetecting 472\n\nmeasuring 472\n\ntarget drift 470\n\nE\n\nElastic Container Registry (ECR) 423, 443\n\nembedding handlers 169-173\n\nencoder-only models 189\n\nend of sentence (EOS) token 222, 252\n\nend-to-end RAG inference pipeline\n\nexamining 346-351\n\nEnterprise Scenarios Leaderboard 266\n\nETL pipeline\n\nfundamental steps 56\n\nETL process\n\nconnecting, to feature pipeline 60\n\nexact deduplication 184\n\nextract, load, transform (ETL) pattern 19\n\nExtract, Transform, Load (ETL) pipeline 55\n\nF\n\nfamily bias 237\n\nFastAPI\n\nbusiness microservice, building 390-393\n\nfeature drift 470\n\nfeature pipeline 14, 19, 20\n\nfeature/training/inference (FTI)architecture 8, 13, 22, 370\n\nbenefits 15\n\nfeature pipeline 14\n\ninference pipeline 14\n\ntraining pipeline 14\n\nfiltered vector search 123\n\nfine-tune\n\nusage, considerations 206, 207\n\nfine-tune models\n\nspecialized tools 220\n\nfine-tuning\n\nbest practices 219-226\n\nformat filtering 183\n\nformatting errors 436\n\nexamples 436\n\nFTI architecture\n\nused, for building LLM system 462, 463\n\nFTI pipeline design\n\nLLM Twin architecture, designing 17\n\nFTI pipelines architecture\n\ninference pipeline 14\n\nfull fine-tuning 211, 212\n\nfuzzy deduplication 184\n\nG\n\nGAIA 264\n\nGalileo Protect 413\n\ngeneral-purpose LLM evaluations 263-265\n\nGitHub 405\n\nGitHub Actions 405, 437\n\nGitHub Actions CI YAML file 438-441\n\nGitHubCrawler class 73-75\n\nGitHub ecosystem 405\n\nGitLab 405\n\nGitLab CI/CD 405\n\nGlobal Interpreter Lock (GIL) 144\n\nGPT 411\n\nguardrails 411, 412\n\ninput guardrails 412\n\noutput guardrails 413\n\nH\n\nHallucinations Leaderboard 266\n\nhandlers 162, 163\n\nchunking handlers 165-169\n\ncleaning handlers 163-165\n\nembedding handlers 169-173\n\nhigh throughput 357\n\nHugging Face 31, 32\n\nfine-tuned LLMs 31\n\nreference link 251\n\nHugging Face Hub\n\nreference link 245\n\nhuman-generated, human-evaluated datasets 233\n\nhuman-generated, LLM-evaluated datasets 233\n\nhybrid search 123\n\nHypothetical document embeddings (HyDE) 121\n\nI\n\nIAM role 423\n\nIDE's MongoDB plugin 94\n\nIFEval 264\n\nin-breadth evolving 194\n\nin-depth evolving 194\n\ninference deployment types 359\n\nasynchronous inference 361, 362\n\noffline batch transform 362\n\nonline real-time inference 360, 361\n\ninference pipeline 22\n\nversus training pipeline 371, 372\n\ninfrastructure 357, 358\n\ninfrastructure-as-code (IaC) 393\n\ninput guardrails 412\n\ninput test 465\n\ninstruction dataset\n\ncreating 178, 196-206\n\ndata augmentation 193-196\n\ndata curation 182\n\ndata decontamination 185\n\ndata deduplication 184, 185\n\ndata exploration 189-191\n\ndata generation 191, 193\n\ndata quality evaluation 186-189\n\ndata quantity 180, 181\n\ngeneral framework 178-180\n\nhigh-quality data 179\n\nrule-based filtering 182, 183\n\nintegration tests 464\n\ninvariance 467\n\niterative improvement 246\n\nJ\n\nJenkins 405\n\njobs 437\n\nK\n\nkey-value (KV) cache 291-294\n\nkeywords filtering 183\n\nKolmogorov-Smirnov (KS) 472\n\nKullback-Leibler (KL) 247\n\nL\n\nLangfuse 413\n\nLangfuse UI\n\nexample trace 414, 415\n\nlarge language model (LLM) 1, 99, 355, 401\n\nlatency 356\n\nlength bias 237\n\nlength filtering 183\n\nlinting errors 436\n\nexamples 436\n\nLLM-as-a-judge strategy 186\n\nLLM evaluation 235\n\nversus, ML evaluation 262, 263\n\nLLM-generated, human-evaluated datasets 234\n\nLLM-generated, LLM-evaluated datasets 234\n\nLLMOps 401, 402, 410, 411, 415\n\nadding, to LLM Twin 434\n\nguardrails 411, 412\n\nhuman feedback 411\n\nprompt monitoring 413\n\nLLMs, training from scratch\n\nconcerns 410, 411\n\nLLM system\n\nbuilding, with FTI architecture 462, 463\n\nLLM Twin 2, 5, 6\n\nCD pipeline 442-444\n\nCI/CD pipeline flow 434, 435\n\nCI/CD pipeline, testing 445\n\nCI pipeline 438\n\nCT pipeline 446, 448\n\ninference pipeline deployment strategy 368-370\n\nMVP, defining 7\n\nRAG feature pipeline architecture 127, 139\n\nsignificance 3, 4\n\nsystem architecture 16\n\nversus co-pilot 4\n\nLLM Twin architecture 23\n\ndata collection pipeline 19\n\ndesigning, with FTI pipeline design 17\n\nfeature pipeline 19, 20\n\ninference pipeline 22\n\ntechnical details 16, 17\n\ntraining pipeline 21, 22\n\nLLM Twin model\n\ndeploying, to AWS SageMaker 375-385\n\nLLM Twin RAG feature pipeline\n\ndispatcher layer 160\n\nhandlers 162\n\nimplementing 139\n\npydantic domain entities 150\n\nsetting 139\n\nZenML pipeline and steps 140, 141\n\nLLM Twin's data collection pipeline\n\ncrawlers 59, 69\n\ndesigning 56-60\n\ndispatcher 66-68\n\nimplementing 61\n\nNoSQL data warehouse documents 79, 80\n\nZenML pipeline and steps 61-65\n\nLLM Twin service\n\ndeploying 372\n\nLLM Twin's pipelines, cloud deployment 415\n\ncode, containerizing with Docker 424-428\n\ninfrastructure 416-418\n\nMongoDB, setting up 418, 419\n\npipelines, running on AWS 428-431\n\nQdrant, setting up 419, 420\n\nResourceLimitExceeded error, troubleshooting after running ZenML pipeline on SageMaker 432, 433\n\nZenML, setting up 421-423\n\nlogs 468\n\nlow latency 358\n\nLow-Rank Adaptation (LoRA) 213-215\n\nM\n\nmachine learning (ML) 1, 355\n\nengineering 409\n\nmanual dataset exploration 189, 190\n\nmanual process 461\n\nmanual triggers 448\n\nMassive Multi-Task Language Understanding (MMLU) 261\n\nMaximum Mean Discrepancy (MMD) 472\n\nMediumCrawler class 77-79\n\nmetrics 468\n\ndrifts 469\n\nmodel metrics 469\n\nsystem metrics 469\n\nmetrics-driven development (MDD) 272\n\nmicroservices architecture 365-367\n\nversus monolithic architecture 367, 368\n\nminimum functionality 467\n\nminimum viable product (MVP) 6\n\nfeatures 6\n\nML engineer 410\n\nML evaluation\n\nvesus, LLM evaluation 262, 263\n\nML models\n\ntraining 464\n\nMLOps 401-407, 411, 461\n\nCI/CD pipeline 462\n\ncontinuous training (CT) 461\n\nengineering 409\n\nmanual process 461\n\nMLOps and LLMOps tools 30, 31\n\nComet ML 45, 46\n\nHugging Face 31, 32\n\nOpik 46, 47\n\nZenML 32, 33\n\nMLOps, core components\n\nfeature store 407\n\nML metadata store 407\n\nML pipeline orchestrator 407\n\nmodel registry 407\n\nMLOps engineer 410\n\nMLOps, principles\n\nautomation 408\n\nexperiment tracking 408\n\nmonitoring 408\n\noperationalization 408\n\nreproducibility 408\n\ntesting 408\n\nversioning 408\n\nML pipeline automation\n\nfor CT 12\n\nML pipelines\n\nfor ML systems 13\n\nML systems\n\nelements 9\n\nissues, with building 8, 9\n\ntesting 464\n\nmodel evaluation 261\n\ndomain-specific LLM evaluations 265-267\n\ngeneral-purpose LLM evaluations 263-265\n\nML, versus LLM evaluation 262, 263\n\ntask-specific LLM evaluations 267-271\n\nmodel metrics 469\n\nmodel optimization strategies 290\n\ncontinuous batching 294\n\nkey-value (KV) cache 291, 293\n\noptimized attention mechanisms 297, 298\n\nspeculative decoding 295, 296\n\nmodel parallelism 298\n\ndata parallelism (DP) 299\n\npipeline parallelism (PP) 300, 301\n\ntechniques, combining 303\n\ntensor parallelism (TP) 301, 302\n\nmodel quantization 303, 304\n\nmodel tests 466\n\nModeration API 413\n\nMongoDB 47\n\nsetting up 418, 419\n\nreference link 418\n\nMongoDB, as data warehouse\n\nusage, considerations 60\n\nmonitoring 468\n\nlogs 468\n\nmetrics 468\n\nversus observability 472\n\nmonolithic architecture 365\n\nmonolithic batch pipeline architecture 10\n\nMT-Bench 264\n\nN\n\nNoSQL data warehouse documents 79, 80\n\ndata categories and user document classes 87-89\n\nODM class, implementing 82-87\n\nORM and ODM software patterns 80, 82\n\nO\n\nobject-relational mapping (ORM) 154\n\nobject-vector mapping (OVM) 139\n\nimplementation 139\n\nobservability\n\nversus monitoring 472\n\nODM class\n\nimplementing 82-87\n\nODM software patterns 80, 82\n\noffline batch transform 362\n\nonline real-time inference 360, 361\n\nOpen Arabic LLM Leaderboard 267\n\nOpenKo-LLM Leaderboard 267\n\nOpen Medical-LLM Leaderboard 265\n\nOpen Portuguese LLM Leaderboard 267\n\nOpik 46, 47, 413\n\nOptimal Brain Quantization (OBQ)approach 312\n\noptimized attention mechanisms 297, 298\n\nORM software patterns 80, 82\n\noutput guardrails 413\n\noutput test 465\n\nP\n\nparameter-efficient fine-tuning techniques\n\nfull fine-tuning 211, 212\n\nLoRA 213-215\n\nQLoRA 215, 216\n\nParameter-efficient fine-tuning techniques 211\n\npipeline parallelism (PP) 300\n\nPiPPy (Pipeline Parallelism for PyTorch) library 301\n\npolicy optimization 246\n\nposition bias 237\n\npost-retrieval step, performing\n\nprompt compression 124\n\nre-ranking 124\n\nPost-Training Quantization (PTQ) 304\n\npreference alignment 245\n\npreference-based reinforcement learning (PbRL) 246\n\npreference dataset 230, 232\n\nChatbots 231\n\ncode generation 231\n\ncontent moderation 231\n\ncreating 230, 237-245\n\ncreative writing 232\n\ndata evaluation 233\n\ndata generation 233\n\ndata quantity 232\n\nsummarization 231\n\ntranslation 232\n\npre-retrieval steps, performing\n\ndata indexing 119\n\nquery optimizing 119\n\nproduction environment 434\n\nprompt monitoring 413, 451-457\n\npull method 136\n\npush method 136\n\nPydantic domain entities 150-154\n\ndata category 151\n\nOVM 154-159\n\nstate of data category 151\n\nPydantic Settings\n\nreference link 139\n\nPython ecosystem\n\ndependency and virtual environment management 27-29\n\nproject installation 26, 27\n\ntask execution tool 29, 30\n\nQ\n\nQA job 438\n\nQdrant 47, 48\n\nreference link 419\n\nsetting up 419, 420\n\nquantization 303-308\n\ntechniques 313, 314\n\nwith GGUF and llama.cpp 309-311\n\nwith GPTQ and EXL2 311, 312\n\nQuantization-aware Low-Rank Adaptation (QLoRA) 215, 216, 221\n\nQuantization-Aware Training (QAT) 304\n\nquery optimization 120\n\nquery rewriting 121\n\nquery routing 120\n\nR\n\nRAG evaluation 271, 272\n\nARES 274, 275\n\nRagas 272-274\n\nRAG feature pipeline\n\nchunking 135\n\ncleaning 135\n\ndata extraction 134\n\ndata loading 135\n\ndata storage, in snapshots 138\n\ndata warehouse and feature store,syncing 136, 137\n\nembedding 135\n\norchestration 138\n\nRAG feature pipeline architecture\n\nbatch pipelines 130\n\nbatch pipelines, versus streaming pipelines 130-134\n\ncore steps 134\n\ndesigning 129\n\nfeature store 128\n\ninference pipeline 127\n\ningestion pipeline 127\n\nproblem, solution 127, 128\n\nraw data 128\n\nRAG inference pipeline\n\narchitecture flow 320, 321\n\nimplementing 318-320, 338\n\nretrieval module, implementing 339-346\n\nraw data, into data warehouse\n\nobtaining 89-94\n\ntroubleshooting 94, 95\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) metric 267\n\nreference window 472\n\nregression tests 464\n\nReinforcement Learning from Human Feedback (RLHF) 245-247, 411\n\niterative improvement 246\n\npolicy optimization 246\n\nreward model learning 246\n\nreinforcement learning (RL) 246\n\nreproducibility 473\n\nrequests per second (RPS) 356\n\nREST API triggers 448\n\nRetrieval-Augmented Generation Assessment (Ragas) 272-274\n\nretrieval-augmented generation (RAG) 2, 99, 100, 317\n\nembeddings 107, 108\n\nembeddings, applications 114\n\nembeddings, creating 111-114\n\nembeddings, significance 109, 110\n\nhallucinations 101\n\nissues, solving 101\n\nvanilla RAG framework 101\n\nvector DBs 115\n\nretrieval-augmented generation (RAG) pipeline 206, 261\n\nreward model learning 246\n\nreward models 188\n\nrule-based filtering 182, 183\n\nrunners 437\n\nS\n\nSageMaker 423\n\nSageMaker Inference deployment 371\n\nconfiguration 371\n\nendpoint 371\n\nInference component 371\n\nmodel 371\n\nSageMaker Orchestrator 423\n\nSageMaker roles\n\nconfiguring 374, 375\n\nscalable and secure object storage service (S3) 423\n\nscalable policy\n\ncreating 397\n\nscalable target\n\nregistering 396\n\nscaling limits\n\nmaximum 398\n\nminimum 398\n\nscheduled triggers 448\n\nSelenium tool 69\n\nissues 95\n\nsemantic similarity 184\n\nServer-Sent Events (SSE) 374\n\nSFT, techniques\n\nchat templates 208-210\n\nfine-tune, usage, considerations 206, 207\n\nhyperparameters, training 216\n\ninstruction dataset formats 208\n\nparameter-efficient fine-tuning techniques 211\n\nSFT techniques, parameters\n\nbatch size 216, 217\n\ngradient checkpointing 219\n\nlearning rate and scheduler 216\n\nmaximum length and packing 217, 218\n\nnumber of epochs 218\n\noptimizers 218\n\nweight decay 219\n\nspeculative decoding 295, 296\n\nstack 422\n\nstaging environment 434\n\nstateless real-time architecture 11\n\nstatistical analysis 190\n\nstress tests 465\n\nstyle transfer 2\n\nsummarization 231\n\nSupervised Fine-Tuning (SFT) 177, 229, 264\n\ntechniques, exploring 206\n\nsystem metrics 469\n\nsystem tests 464\n\nT\n\ntarget drift 470\n\nTargetTrackingScaling policy 397\n\ntask-specific LLM evaluations 267-271\n\ntensor parallelism (TP) 301, 302\n\nTerraform 393\n\ntest example 465\n\ntest job 438\n\ntest types 465\n\nacceptance tests 464\n\nintegration tests 464\n\nregression tests 464\n\nstress tests 465\n\nsystem tests 464\n\nunit tests 464\n\ntest window 472\n\nText Generation Inference (TGI) 294, 373\n\nthroughput 356, 357\n\nTime between Tokens (TBT) 413\n\nTime per Output Token (TPOT) 413\n\nTime to First Token (TTFT) 413\n\nTokens per Second (TPS) 413\n\ntopic clustering 190, 191\n\nTotal Latency 413\n\ntraining pipeline 14, 21, 22\n\nversus inference pipeline 371, 372\n\ntriggers\n\nmanual triggers 448\n\nREST API triggers 448\n\nscheduled triggers 448\n\nTwinLlama-3.1-8B\n\nanswers, evaluating 278-283\n\nanswers, generating 276-278\n\nevaluating 275, 276\n\nresults, analyzing 283-286\n\nTwinLlama-3.1-8B model 250\n\nU\n\nUltraFeedback method 195\n\nunit tests 464\n\nUser Acceptance Testing (UAT) 464\n\nV\n\nvector DBs 115\n\nalgorithms, for creating vector index 116\n\nDB operations 116\n\nworking 115\n\nversioning 463\n\ncode 463\n\ndata 463\n\nmodel 463\n\nVideo Random-Access Memory (VRAM) 291\n\nW\n\nwindow types\n\nreference window 472\n\ntest window 472\n\nworkflow 437\n\nZ\n\nZenML 32, 33\n\nartifacts and metadata 39-43\n\norchestrator 33-37\n\nreference link 421\n\nsetting up 421-423\n\nZenML pipeline 140-142\n\ncleaned documents, chunking 147-150\n\ncleaned documents, embedding 147-150\n\nconfiguring 43, 45\n\ndata warehouse, querying 143-145\n\ndocuments, cleaning 146, 147\n\ndocuments, loading to vector DB 150\n\nimplementing 61-65\n\nrunning 43, 45\n\nzero-point quantization 307\n\nDownload a free PDF copy of this book\n\nThanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books everywhere?\n\nIs your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily.\n\nFollow these simple steps to get the benefits:\n\nScan the QR code or visit the link below:\n\nhttps://packt.link/free-ebook/9781836200079\n\nSubmit your proof of purchase.\n\nThat’s it! We’ll send your free PDF and other benefits to your email directly.\n\nOceanofPDF.com\n\nOceanofPDF.com",
      "page_number": 1199
    }
  ],
  "pages": [
    {
      "page_number": 3,
      "content": "LLM Engineer’s Handbook\n\nMaster the art of engineering large language models from concept to production\n\nPaul Iusztin\n\nMaxime Labonne",
      "content_length": 133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "LLM Engineer’s Handbook\n\nCopyright © 2024 Packt Publishing\n\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.\n\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the authors, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book.\n\nPackt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.\n\nSenior Publishing Product Manager: Gebin George\n\nAcquisition Editor – Peer Reviews: Swaroop Singh",
      "content_length": 1071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Project Editor: Amisha Vathare\n\nContent Development Editor: Tanya D’cruz\n\nCopy Editor: Safis Editing\n\nTechnical Editor: Karan Sonawane\n\nProofreader: Safis Editing\n\nIndexer: Manju Arasan\n\nPresentation Designer: Rajesh Shirsath\n\nDeveloper Relations Marketing Executive: Anamika Singh\n\nFirst published: October 2024\n\nProduction reference: 2171024",
      "content_length": 343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "Published by Packt Publishing Ltd.\n\nGrosvenor House\n\n11 St Paul’s Square\n\nBirmingham\n\nB3 1RB, UK.\n\nISBN 978-1-83620-007-9\n\nwww.packt.com",
      "content_length": 136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Forewords\n\nAs my co-founder at Hugging Face, Clement Delangue, and I often say, AI is becoming the default way of building technology.\n\nOver the past 3 years, LLMs have already had a profound impact on technology, and they are bound to have an even greater impact in the coming 5 years. They will be embedded in more and more products and, I believe, at the center of any human activity based on knowledge or creativity.\n\nFor instance, coders are already leveraging LLMs and changing the way they work, focusing on higher-order thinking and tasks while collaborating with machines. Studio musicians rely on AI-powered tools to explore the musical creativity space faster. Lawyers are increasing their impact through retrieval-augmented generation (RAG) and large databases of case law.\n\nAt Hugging Face, we’ve always advocated for a future where not just one company or a small number of scientists control the AI models used by the rest of the population, but instead for a future where as many people as possible—from as many different backgrounds as possible—are capable of diving into how cutting-edge machine learning models actually work.\n\nMaxime Labonne and Paul Iusztin have been instrumental in this movement to democratize LLMs by writing this book and making sure that as many",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "people as possible can not only use them but also adapt them, fine-tune them, quantize them, and make them efficient enough to actually deploy in the real world.\n\nTheir work is essential, and I’m glad they are making this resource available to the community. This expands the convex hull of human knowledge.\n\nJulien Chaumond\n\nCo-founder and CTO, Hugging Face",
      "content_length": 358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "As someone deeply immersed in the world of machine learning operations, I’m thrilled to endorse The LLM Engineer’s Handbook. This comprehensive guide arrives at a crucial time when the demand for LLM expertise is skyrocketing across industries.\n\nWhat sets this book apart is its practical, end-to-end approach. By walking readers through the creation of an LLM Twin, it bridges the often daunting gap between theory and real-world application. From data engineering and model fine-tuning to advanced topics like RAG pipelines and inference optimization, the authors leave no stone unturned.\n\nI’m particularly impressed by the emphasis on MLOps and LLMOps principles. As organizations increasingly rely on LLMs, understanding how to build scalable, reproducible, and robust systems is paramount. The inclusion of orchestration strategies and cloud integration showcases the authors’ commitment to equipping readers with truly production-ready skills.\n\nWhether you’re a seasoned ML practitioner looking to specialize in LLMs or a software engineer aiming to break into this exciting field, this handbook provides the perfect blend of foundational knowledge and cutting-edge techniques. The clear explanations, practical examples, and focus on best practices make it an invaluable resource for anyone serious about mastering LLM engineering.\n\nIn an era where AI is reshaping industries at breakneck speed, The LLM Engineer’s Handbook stands out as an essential guide for navigating the",
      "content_length": 1482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "complexities of large language models. It’s not just a book; it’s a roadmap to becoming a proficient LLM engineer in today’s AI-driven landscape.\n\nHamza Tahir\n\nCo-founder and CTO, ZenML",
      "content_length": 185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "Contributors\n\nAbout the authors\n\nPaul Iusztin is a senior ML and MLOps engineer with over seven years of experience building GenAI, Computer Vision and MLOps solutions. His latest contribution was at Metaphysic, where he served as one of their core engineers in taking large neural networks to production. He previously worked at CoreAI, Everseen, and Continental. He is the Founder of Decoding ML, an educational channel on production-grade ML that provides posts, articles, and open-source courses to help others build real-world ML systems.\n\nMaxime Labonne is the Head of Post-Training at Liquid AI. He holds a PhD. in ML from the Polytechnic Institute of Paris and is recognized as a Google Developer Expert in AI/ML. As an active blogger, he has made significant contributions to the open-source community, including the LLM Course on GitHub, tools such as LLM AutoEval, and several state-of-the-art models like NeuralDaredevil. He is the author of the best-selling book Hands-On Graph Neural Networks Using Python, published by Packt.\n\nI want to thank my family and partner. Your unwavering support and patience made this book possible.",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "About the reviewer\n\nRany ElHousieny is an AI solutions architect and AI engineering manager with over two decades of experience in AI, NLP, and ML. Throughout his career, he has focused on the development and deployment of AI models, authoring multiple articles on AI systems architecture and ethical AI deployment. He has led groundbreaking projects at companies like Microsoft, where he spearheaded advancements in NLP and the Language Understanding Intelligent Service (LUIS). Currently, he plays a pivotal role at Clearwater Analytics, driving innovation in GenAI and AI-driven financial and investment management solutions.\n\nI would like to thank Clearwater Analytics for providing a supportive and learning environment that fosters growth and innovation. The vision of our leaders, always staying ahead with the latest technologies, has been a constant source of inspiration. Their commitment to AI advancements made my experience of reviewing this book insightful and enriching. Special thanks to my family for their ongoing encouragement throughout this journey.\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:",
      "content_length": 1189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "https://packt.link/llmeng",
      "content_length": 25,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "Preface\n\nThe field of LLM engineering has rapidly emerged as a critical area in artificial intelligence and machine learning. As LLMs continue to revolutionize natural language processing and generation, the demand for professionals who can effectively implement, optimize, and deploy these models in real-world scenarios has grown exponentially. LLM engineering encompasses a wide range of disciplines, from data preparation and model fine-tuning to inference optimization and production deployment, requiring a unique blend of software engineering, machine learning expertise, and domain knowledge.\n\nMachine Learning Operations (MLOps) plays a crucial role in the successful implementation of LLMs in production environments. MLOps extends the principles of DevOps to machine learning projects, focusing on automating and streamlining the entire ML lifecycle. For LLMs, MLOps is particularly important due to the complexity and scale of these models. It addresses challenges such as managing large datasets, handling model versioning, ensuring reproducibility, and maintaining model performance over time. By incorporating MLOps practices, LLM projects can achieve greater efficiency, reliability, and scalability, ultimately leading to more successful and impactful deployments.\n\nThe LLM Engineer’s Handbook is a comprehensive guide to applying best practices to the new field of LLM engineering. Throughout the chapters, readers will find simplified key concepts, practical techniques, and experts tips for every stage of the LLM lifecycle. The book covers topics such as data engineering, supervised fine-tuning,",
      "content_length": 1617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "model evaluation, inference optimization, and Retrieval-Augmented Generation (RAG) pipeline development.\n\nTo illustrate these concepts in action, an end-to-end project called the LLM Twin will be developed throughout the book., with the goal of imitating someone’s writing style and personality. This use case will demonstrate how to build a minimum viable product to solve a specific problem, using various aspects of LLM engineering and MLOps.\n\nReaders can expect to gain a deeper understanding of how to collect and prepare data for LLMs, fine-tune models for specific tasks, optimize inference performance, and implement RAG pipelines. They will learn how to evaluate LLM performance, align models with human preferences, and deploy LLM-based applications. The book also covers essential MLOps principles and practices, enabling readers to build scalable, reproducible, and robust LLM applications.",
      "content_length": 902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Who this book is for\n\nThis book is intended for a wide range of technology professionals and enthusiasts interested in the practical applications of LLMs. It’s ideal for software engineers aiming to transition into AI projects. While some familiarity with software development is beneficial, the book explains many concepts from the ground up, making it accessible even to those who are new to AI and machine learning.\n\nFor those already working with machine learning , this book will enhance your skills in implementing and deploying LLM-based systems. We provide a deep dive into the fundamentals of MLOps, guiding you through the process of creating a minimum viable product using an open-source LLM to solve real-world problems.",
      "content_length": 732,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "What this book covers\n\nChapter 1, Understanding the LLM Twin Concept and Architecture, introduces the LLM Twin project, which is used throughout the book as an end-to-end example of a production-level LLM application, and defines the FTI architecture for building scalable ML systems and applies it to the LLM Twin use case.\n\nChapter 2, Tooling and Installation, presents Python, MLOps, and cloud tools used to build real-world LLM applications, such as an orchestrator, experiment tracker, prompt monitoring and LLM evaluation tool. It shows how to use and install them locally for testing and development.\n\nChapter 3, Data Engineering, shows the implementation of a data collection pipeline that scrapes multiple sites, such as Medium, GitHub and Substack and stores the raw data in a data warehouse. It emphasizes collecting raw data from dynamic sources over static datasets for real- world ML applications.\n\nChapter 4, RAG Feature Pipeline, introduces RAG fundamental concepts, such as embeddings, the vanilla RAG framework, vector databases, and how to optimize RAG applications. It applies the RAG theory by architecting and implementing LLM Twin’s RAG feature pipeline using software best practices.",
      "content_length": 1207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "Chapter 5, Supervised Fine-Tuning, explores the process of refining pre- trained language models for specific tasks using instruction-answer pairs. It covers creating high-quality datasets, implementing fine-tuning techniques like full fine-tuning, LoRA, and QLoRA, and provides a practical demonstration of fine-tuning a Llama 3.1 8B model on a custom dataset.\n\nChapter 6, Fine-Tuning with Preference Alignment, introduces techniques for aligning language models with human preferences, focusing on Direct Preference Optimization (DPO). It covers creating custom preference datasets, implementing DPO, and provides a practical demonstration of aligning the TwinLlama-3.1-8B model using the Unsloth library.\n\nChapter 7, Evaluating LLMs, details various methods for assessing the performance of language models and LLM systems. It introduces general- purpose and domain-specific evaluations and discusses popular benchmarks. The chapter includes a practical evaluation of the TwinLlama- 3.1-8B model using multiple criteria.\n\nChapter 8, Inference Optimization, covers key optimization strategies such as speculative decoding, model parallelism, and weight quantization. It discusses how to improve inference speed, reduce latency, and minimize memory usage, introducing popular inference engines and comparing their features.\n\nChapter 9, RAG Inference Pipeline, explores advanced RAG techniques by implementing methods such as self-query, reranking, and filtered vector search from scratch. It covers designing and implementing the LLM Twin’s RAG inference pipeline and a custom retrieval module similar to what you see in popular frameworks such as LangChain.",
      "content_length": 1659,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Chapter 10, Inference Pipeline Deployment, introduces ML deployment strategies, such as online, asynchronous and batch inference, which will help in architecting and deploying the LLM Twin fine-tuned model to AWS SageMaker and building a FastAPI microservice to expose the RAG inference pipeline as a RESTful API.\n\nChapter 11, MLOps and LLMOps, presents what LLMOps is, starting with its roots in DevOps and MLOps. This chapter explains how to deploy the LLM Twin project to the cloud, such as the ML pipelines to AWS and shows how to containerize the code using Docker and build a CI/CD/CT pipeline. It also adds a prompt monitoring layer on top of LLM Twin’s inference pipeline.\n\nAppendix, MLOps Principles, covers the six MLOps principles used to build scalable, reproducible, and robust ML applications.",
      "content_length": 807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "To get the most out of this book\n\nTo maximize your learning experience, you are expected to have, at the very least, a foundational understanding of software development principles and practices. Familiarity with Python programming is particularly beneficial, as the book’s examples and code snippets are predominantly in Python. While prior experience with machine learning concepts is advantageous, it is not strictly necessary, as the book provides explanations for many fundamental AI and ML concepts. However, you should be comfortable with basic data structures, algorithms, and have some experience working with APIs and cloud services.\n\nFamiliarity with version control systems like Git is assumed, as this book has a GitHub repository for code examples. While this book is designed to be accessible to those who are new to AI and LLMs, if you have some background in these areas, you will find it easier to grasp the more advanced concepts and techniques we present.\n\nDownload the example code files\n\nThe code bundle for the book is hosted on GitHub at https://github.com/PacktPublishing/LLM-Engineers-Handbook. We also have other code bundles from our rich catalog of books and videos available at https://github.com/PacktPublishing/. Check them out!",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Download the color images\n\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: https://packt.link/gbp/9781836200079.\n\nConventions used\n\nThere are a number of text conventions used throughout this book.\n\nCodeInText\n\n: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. For example: “In the\n\nformat_samples\n\nfunction, we apply the Alpaca chat template to each individual message.”\n\nA block of code is set as follows:\n\ndef",
      "content_length": 588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "format_samples\n\n(\n\nexample\n\n): example[\n\n\"prompt\"\n\n] = alpaca_template.\n\nformat\n\n(example[\n\n\"prompt\"\n\n]) example[\n\n\"chosen\"\n\n] = example[\n\n'chosen'\n\n] + EOS_TOKEN example[\n\n\"rejected\"\n\n] = example[\n\n'rejected'\n\n] + EOS_TOKEN\n\nreturn\n\n{",
      "content_length": 235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "\"prompt\"\n\n: example[\n\n\"prompt\"\n\n],\n\n\"chosen\"\n\n: example[\n\n\"chosen\"\n\n],\n\n\"rejected\"\n\n: example[\n\n\"rejected\"\n\n]}\n\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:\n\ndef\n\nformat_samples",
      "content_length": 251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "(\n\nexample\n\n): example[\n\n\"prompt\"\n\n] = alpaca_template.\n\nformat\n\n(example[\n\n\"prompt\"\n\n]) example[\n\n\"chosen\"\n\n] = example[\n\n'\n\nchosen'\n\n] + EOS_TOKEN example[\n\n\"rejected\"\n\n] = example[\n\n'rejected'\n\n] + EOS_TOKEN\n\nreturn\n\n{",
      "content_length": 221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "\"prompt\"\n\n: example[\n\n\"prompt\"\n\n],\n\n\"chosen\"\n\n:\n\nexample[\n\n\"chosen\"\n\n],\n\n\"rejected\"\n\n: example[\n\n\"rejected\"\n\n]}\n\nAny command-line input or output is written as follows:\n\npoetry install --without aws",
      "content_length": 198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Bold: Indicates a new term, an important word, or words that you see on the screen. For instance, words in menus or dialog boxes appear in the text like this. For example: “To do so, go to the Settings tab at the top of the forked repository in GitHub. In the left panel, in the Security section, click on the Secrets and Variables toggle and, finally, click on Actions.”\n\nWarnings or important notes appear like this.\n\nTips and tricks appear like this.",
      "content_length": 453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Get in touch\n\nFeedback from our readers is always welcome.\n\nGeneral feedback: Email\n\nfeedback@packtpub.com\n\nand mention the book’s title in the subject of your message. If you have questions about any aspect of this book, please email us at\n\nquestions@packtpub.com\n\n.\n\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you reported this to us. Please visit http://www.packtpub.com/submit-errata, click Submit Errata, and fill in the form.\n\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at\n\ncopyright@packtpub.com\n\nwith a link to the material.",
      "content_length": 807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "If you are interested in becoming an author: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit http://authors.packtpub.com.",
      "content_length": 201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Share your thoughts\n\nOnce you’ve read LLM Engineer’s Handbook, First Edition, we’d love to hear your thoughts! Please click here to go straight to the Amazon review page for this book and share your feedback.\n\nYour review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.",
      "content_length": 334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "Download a free PDF copy of this book\n\nThanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books everywhere?\n\nIs your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily.\n\nFollow these simple steps to get the benefits:",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Scan the QR code or visit the link below:\n\nhttps://packt.link/free-ebook/9781836200079\n\nSubmit your proof of purchase.\n\nThat’s it! We’ll send your free PDF and other benefits to your email directly.",
      "content_length": 198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "1",
      "content_length": 1,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "Understanding the LLM Twin Concept and Architecture\n\nBy the end of this book, we will have walked you through the journey of building an end-to-end large language model (LLM) product. We firmly believe that the best way to learn about LLMs and production machine learning (ML) is to get your hands dirty and build systems. This book will show you how to build an LLM Twin, an AI character that learns to write like a particular person by incorporating its style, voice, and personality into an LLM. Using this example, we will walk you through the complete ML life cycle, from data gathering to deployment and monitoring. Most of the concepts learned while implementing your LLM Twin can be applied in other LLM-based or ML applications.\n\nWhen starting to implement a new product, from an engineering point of view, there are three planning steps we must go through before we start building. First, it is critical to understand the problem we are trying to solve and what we want to build. In our case, what exactly is an LLM Twin, and why build it? This step is where we must dream and focus on the “Why.” Secondly, to reflect a real-world scenario, we will design the first iteration of a product with minimum functionality. Here, we must clearly define the core features required to create a working and valuable product. The choices are made based on the timeline, resources, and team’s knowledge. This is where we bridge the gap between dreaming and focusing on what is realistic and eventually answer the following question: “What are we going to build?”.",
      "content_length": 1561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Finally, we will go through a system design step, laying out the core architecture and design choices used to build the LLM system. Note that the first two components are primarily product-related, while the last one is technical and focuses on the “How.”\n\nThese three steps are natural in building a real-world product. Even if the first two do not require much ML knowledge, it is critical to go through them to understand “how” to build the product with a clear vision. In a nutshell, this chapter covers the following topics:\n\nUnderstanding the LLM Twin concept\n\nPlanning the MVP of the LLM Twin product\n\nBuilding ML systems with feature/training/inference pipelines\n\nDesigning the system architecture of the LLM Twin\n\nBy the end of this chapter, you will have a clear picture of what you will learn to build throughout the book.",
      "content_length": 833,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Understanding the LLM Twin concept\n\nThe first step is to have a clear vision of what we want to create and why it’s valuable to build it. The concept of an LLM Twin is new. Thus, before diving into the technical details, it is essential to understand what it is, what we should expect from it, and how it should work. Having a solid intuition of your end goal makes it much easier to digest the theory, code, and infrastructure presented in this book.",
      "content_length": 451,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "What is an LLM Twin?\n\nIn a few words, an LLM Twin is an AI character that incorporates your writing style, voice, and personality into an LLM, which is a complex AI model. It is a digital version of yourself projected into an LLM. Instead of a generic LLM trained on the whole internet, an LLM Twin is fine-tuned on yourself. Naturally, as an ML model reflects the data it is trained on, this LLM will incorporate your writing style, voice, and personality. We intentionally used the word “projected.” As with any other projection, you lose a lot of information along the way. Thus, this LLM will not be you; it will copy the side of you reflected in the data it was trained on.\n\nIt is essential to understand that an LLM reflects the data it was trained on. If you feed it Shakespeare, it will start writing like him. If you train it on Billie Eilish, it will start writing songs in her style. This is also known as style transfer. This concept is prevalent in generating images, too. For example, let’s say you want to create a cat image using Van Gogh’s style. We will leverage the style transfer strategy, but instead of choosing a personality, we will do it on our own persona.\n\nTo adjust the LLM to a given style and voice along with fine-tuning, we will also leverage various advanced retrieval-augmented generation (RAG) techniques to condition the autoregressive process with previous embeddings of ourselves.\n\nWe will explore the details in Chapter 5 on fine-tuning and Chapters 4 and 9 on RAG, but for now, let’s look at a few examples to intuitively understand",
      "content_length": 1572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "what we stated previously.\n\nHere are some scenarios of what you can fine-tune an LLM on to become your twin:\n\nLinkedIn posts and X threads: Specialize the LLM in writing social media content.\n\nMessages with your friends and family: Adapt the LLM to an unfiltered version of yourself.\n\nAcademic papers and articles: Calibrate the LLM in writing formal and educative content.\n\nCode: Specialize the LLM in implementing code as you would.\n\nAll the preceding scenarios can be reduced to one core strategy: collecting your digital data (or some parts of it) and feeding it to an LLM using different algorithms. Ultimately, the LLM reflects the voice and style of the collected data. Easy, right?\n\nUnfortunately, this raises many technical and moral issues. First, on the technical side, how can we access this data? Do we have enough digital data to project ourselves into an LLM? What kind of data would be",
      "content_length": 901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "valuable? Secondly, on the moral side, is it OK to do this in the first place? Do we want to create a copycat of ourselves? Will it write using our voice and personality, or just try to replicate it?\n\nRemember that the role of this section is not to bother with the “What” and “How” but with the “Why.” Let’s understand why it makes sense to have your LLM Twin, why it can be valuable, and why it is morally correct if we frame the problem correctly.",
      "content_length": 450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "Why building an LLM Twin matters\n\nAs an engineer (or any other professional career), building a personal brand is more valuable than a standard CV. The biggest issue with creating a personal brand is that writing content on platforms such as LinkedIn, X, or Medium takes a lot of time. Even if you enjoy writing and creating content, you will eventually run out of inspiration or time and feel like you need assistance. We don’t want to transform this section into a pitch, but we have to understand the scope of this product/project clearly.\n\nWe want to build an LLM Twin to write personalized content on LinkedIn, X, Instagram, Substack, and Medium (or other blogs) using our style and voice. It will not be used in any immoral scenarios, but it will act as your writing co-pilot. Based on what we will teach you in this book, you can get creative and adapt it to various use cases, but we will focus on the niche of generating social media content and articles. Thus, instead of writing the content from scratch, we can feed the skeleton of our main idea to the LLM Twin and let it do the grunt work.\n\nUltimately, we will have to check whether everything is correct and format it to our liking (more on the concrete features in the Planning the MVP of the LLM Twin product section). Hence, we project ourselves into a content- writing LLM Twin that will help us automate our writing process. It will likely fail if we try to use this particular LLM in a different scenario, as this is where we will specialize the LLM through fine-tuning, prompt engineering, and RAG.",
      "content_length": 1570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "So, why does building an LLM Twin matter? It helps you do the following:\n\nCreate your brand\n\nAutomate the writing process\n\nBrainstorm new creative ideas\n\nWhat’s the difference between a co-pilot and an LLM Twin?\n\nA co-pilot and digital twin are two different concepts that work together and can be combined into a powerful solution:\n\nThe co-pilot is an AI assistant or tool that augments human users in various programming, writing, or content creation tasks.\n\nThe twin serves as a 1:1 digital representation of a real-world entity, often using AI to bridge the gap between the physical and digital worlds. For instance, an LLM Twin is an LLM that learns to mimic your voice, personality, and writing style.",
      "content_length": 707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "With these definitions in mind, a writing and content creation AI assistant who writes like you is your LLM Twin co-pilot.\n\nAlso, it is critical to understand that building an LLM Twin is entirely moral. The LLM will be fine-tuned only on our personal digital data. We won’t collect and use other people’s data to try to impersonate anyone’s identity. We have a clear goal in mind: creating our personalized writing copycat. Everyone will have their own LLM Twin with restricted access.\n\nOf course, many security concerns are involved, but we won’t go into that here as it could be a book in itself.",
      "content_length": 599,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "Why not use ChatGPT (or another similar chatbot)?\n\nThis subsection will refer to using ChatGPT (or another similar chatbot) just in the context of generating personalized content.\n\nWe have already provided the answer. ChatGPT is not personalized to your writing style and voice. Instead, it is very generic, unarticulated, and wordy. Maintaining an original voice is critical for long-term success when building your brand. Thus, directly using ChatGPT or Gemini will not yield the most optimal results. Even if you are OK with sharing impersonalized content, mindlessly using ChatGPT can result in the following:\n\nMisinformation due to hallucination: Manually checking the results for hallucinations or using third-party tools to evaluate your results is a tedious and unproductive experience.\n\nTedious manual prompting: You must manually craft your prompts and inject external information, which is a tiresome experience. Also, the generated answers will be hard to replicate between multiple sessions as you don’t have complete control over your prompts and injected data. You can solve part of this problem using an API and a tool such as LangChain, but you need programming experience to do so.",
      "content_length": 1199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "From our experience, if you want high-quality content that provides real value, you will spend more time debugging the generated text than writing it yourself.\n\nThe key of the LLM Twin stands in the following:\n\nWhat data we collect\n\nHow we preprocess the data\n\nHow we feed the data into the LLM\n\nHow we chain multiple prompts for the desired results\n\nHow we evaluate the generated content\n\nThe LLM itself is important, but we want to highlight that using ChatGPT’s web interface is exceptionally tedious in managing and injecting various data sources or evaluating the outputs. The solution is to build an LLM system that encapsulates and automates all the following steps (manually replicating them each time is not a long-term and feasible solution):",
      "content_length": 752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Data collection\n\nData preprocessing\n\nData storage, versioning, and retrieval\n\nLLM fine-tuning\n\nRAG\n\nContent generation evaluation\n\nNote that we never said not to use OpenAI’s GPT API, just that the LLM framework we will present is LLM-agnostic. Thus, if it can be manipulated programmatically and exposes a fine-tuning interface, it can be integrated into the LLM Twin system we will learn to build. The key to most successful ML products is to be data-centric and make your architecture model-agnostic. Thus, you can quickly experiment with multiple models on your specific data.",
      "content_length": 580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Planning the MVP of the LLM Twin product\n\nNow that we understand what an LLM Twin is and why we want to build it, we must clearly define the product’s features. In this book, we will focus on the first iteration, often labeled the minimum viable product (MVP), to follow the natural cycle of most products. Here, the main objective is to align our ideas with realistic and doable business objectives using the available resources to produce the product. Even as an engineer, as you grow up in responsibilities, you must go through these steps to bridge the gap between the business needs and what can be implemented.",
      "content_length": 616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "What is an MVP?\n\nAn MVP is a version of a product that includes just enough features to draw in early users and test the viability of the product concept in the initial stages of development. Usually, the purpose of the MVP is to gather insights from the market with minimal effort.\n\nAn MVP is a powerful strategy because of the following reasons:\n\nAccelerated time-to-market: Launch a product quickly to gain early traction\n\nIdea validation: Test it with real users before investing in the full development of the product\n\nMarket research: Gain insights into what resonates with the target audience\n\nRisk minimization: Reduces the time and resources needed for a product that might not achieve market success",
      "content_length": 709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Sticking to the V in MVP is essential, meaning the product must be viable. The product must provide an end-to-end user journey without half- implemented features, even if the product is minimal. It must be a working product with a good user experience that people will love and want to keep using to see how it evolves to its full potential.",
      "content_length": 341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "Defining the LLM Twin MVP\n\nAs a thought experiment, let’s assume that instead of building this project for this book, we want to make a real product. In that case, what are our resources? Well, unfortunately, not many:\n\nWe are a team of three people with two ML engineers and one ML researcher\n\nOur laptops\n\nPersonal funding for computing, such as training LLMs\n\nOur enthusiasm\n\nAs you can see, we don’t have many resources. Even if this is just a thought experiment, it reflects the reality for most start-ups at the beginning of their journey. Thus, we must be very strategic in defining our LLM Twin MVP and what features we want to pick. Our goal is simple: we want to maximize the product’s value relative to the effort and resources poured into it.",
      "content_length": 754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "To keep it simple, we will build the features that can do the following for the LLM Twin:\n\nCollect data from your LinkedIn, Medium, Substack, and GitHub profiles\n\nFine-tune an open-source LLM using the collected data\n\nPopulate a vector database (DB) using our digital data for RAG\n\nCreate LinkedIn posts leveraging the following:\n\nUser prompts\n\nRAG to reuse and reference old content\n\nNew posts, articles, or papers as additional knowledge to the LLM\n\nHave a simple web interface to interact with the LLM Twin and be able to do the following:\n\nConfigure your social media links and trigger the collection step",
      "content_length": 609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Send prompts or links to external resources\n\nThat will be the LLM Twin MVP. Even if it doesn’t sound like much, remember that we must make this system cost effective, scalable, and modular.\n\nEven if we focus only on the core features of the LLM Twin defined in this section, we will build the product with the latest LLM research and best software engineering and MLOps practices in mind. We aim to show you how to engineer a cost-effective and scalable LLM application.\n\nUntil now, we have examined the LLM Twin from the users’ and businesses’ perspectives. The last step is to examine it from an engineering perspective and define a development plan to understand how to solve it technically. From now on, the book’s focus will be on the implementation of the LLM Twin.",
      "content_length": 771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "Building ML systems with feature/training/inference pipelines\n\nBefore diving into the specifics of the LLM Twin architecture, we must understand an ML system pattern at the core of the architecture, known as the feature/training/inference (FTI) architecture. This section will present a general overview of the FTI pipeline design and how it can structure an ML application.\n\nLet’s see how we can apply the FTI pipelines to the LLM Twin architecture.",
      "content_length": 450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "The problem with building ML systems\n\nBuilding production-ready ML systems is much more than just training a model. From an engineering point of view, training the model is the most straightforward step in most use cases. However, training a model becomes complex when deciding on the correct architecture and hyperparameters. That’s not an engineering problem but a research problem.\n\nAt this point, we want to focus on how to design a production-ready architecture. Training a model with high accuracy is extremely valuable, but just by training it on a static dataset, you are far from deploying it robustly. We have to consider how to do the following:\n\nIngest, clean, and validate fresh data\n\nTraining versus inference setups\n\nCompute and serve features in the right environment\n\nServe the model in a cost-effective way\n\nVersion, track, and share the datasets and models",
      "content_length": 875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Monitor your infrastructure and models\n\nDeploy the model on a scalable infrastructure\n\nAutomate the deployments and training\n\nThese are the types of problems an ML or MLOps engineer must consider, while the research or data science team is often responsible for training the model.",
      "content_length": 281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "Figure 1.1: Common elements from an ML system\n\nThe preceding figure shows all the components the Google Cloud team suggests that a mature ML and MLOps system requires. Along with the ML code, there are many moving pieces. The rest of the system comprises configuration, automation, data collection, data verification, testing and debugging, resource management, model analysis, process and metadata management, serving infrastructure, and monitoring. The point is that there are many components we must consider when productionizing an ML model.",
      "content_length": 545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Thus, the critical question is this: How do we connect all these components into a single homogenous system? We must create a boilerplate for clearly designing ML systems to answer that question.\n\nSimilar solutions exist for classic software. For example, if you zoom out, most software applications can be split between a DB, business logic, and UI layer. Every layer can be as complex as needed, but at a high-level overview, the architecture of standard software can be boiled down to the previous three components.\n\nDo we have something similar for ML applications? The first step is to examine previous solutions and why they are unsuitable for building scalable ML systems.",
      "content_length": 679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "The issue with previous solutions\n\nIn Figure 1.2, you can observe the typical architecture present in most ML applications. It is based on a monolithic batch architecture that couples the feature creation, model training, and inference into the same component. By taking this approach, you quickly solve one critical problem in the ML world: the training-serving skew. The training-serving skew happens when the features passed to the model are computed differently at training and inference time.\n\nIn this architecture, the features are created using the same code. Hence, the training-serving skew issue is solved by default. This pattern works fine when working with small data. The pipeline runs on a schedule in batch mode, and the predictions are consumed by a third-party application such as a dashboard.",
      "content_length": 811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "Figure 1.2: Monolithic batch pipeline architecture\n\nUnfortunately, building a monolithic batch system raises many other issues, such as the following:\n\nFeatures are not reusable (by your system or others)",
      "content_length": 204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "If the data increases, you have to refactor the whole code to support PySpark or Ray\n\nIt’s hard to rewrite the prediction module in a more efficient language such as C++, Java, or Rust\n\nIt’s hard to share the work between multiple teams between the features, training, and prediction modules\n\nIt’s impossible to switch to streaming technology for real-time training\n\nIn Figure 1.3, we can see a similar scenario for a real-time system. This use case introduces another issue in addition to what we listed before. To make the predictions, we have to transfer the whole state through the client request so the features can be computed and passed to the model.\n\nConsider the scenario of computing movie recommendations for a user. Instead of simply passing the user ID, we must transmit the entire user state, including their name, age, gender, movie history, and more. This approach is fraught with potential errors, as the client must understand how to access this state, and it’s tightly coupled with the model service.\n\nAnother example would be when implementing an LLM with RAG support. The documents we add as context along the query represent our external state. If we didn’t store the records in a vector DB, we would have to pass them with the user query. To do so, the client must know how to query and",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "retrieve the documents, which is not feasible. It is an antipattern for the client application to know how to access or compute the features. If you don’t understand how RAG works, we will explain it in detail in Chapters 8 and 9.\n\nFigure 1.3: Stateless real-time architecture\n\nIn conclusion, our problem is accessing the features to make predictions without passing them at the client’s request. For example, based on our first user movie recommendation example, how can we predict the",
      "content_length": 486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "recommendations solely based on the user’s ID? Remember these questions, as we will answer them shortly.\n\nUltimately, on the other spectrum, Google Cloud provides a production- ready architecture, as shown in Figure 1.4. Unfortunately, even if it’s a feasible solution, it’s very complex and not intuitive. You will have difficulty understanding this if you are not highly experienced in deploying and keeping ML models in production. Also, it is not straightforward to understand how to start small and grow the system in time.\n\nThe following image is reproduced from work created and shared by Google and used according to terms described in the Creative Commons 4.0 Attribution License:",
      "content_length": 689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Figure 1.4: ML pipeline automation for CT (source: https://cloud.google.com/architecture/mlops-continuous-delivery-and- automation-pipelines-in-machine-learning)\n\nBut here is where the FTI pipeline architectures kick in. The following section will show you how to solve these fundamental issues using an intuitive ML design.",
      "content_length": 324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "The solution – ML pipelines for ML systems\n\nThe solution is based on creating a clear and straightforward mind map that any team or person can follow to compute the features, train the model, and make predictions. Based on these three critical steps that any ML system requires, the pattern is known as the FTI pipeline. So, how does this differ from what we presented before?\n\nThe pattern suggests that any ML system can be boiled down to these three pipelines: feature, training, and inference (similar to the DB, business logic, and UI layers from classic software). This is powerful, as we can clearly define the scope and interface of each pipeline. Also, it’s easier to understand how the three components interact. Ultimately, we have just three instead of 20 moving pieces, as suggested in Figure 1.4, which is much easier to work with and define.\n\nAs shown in Figure 1.5, we have the feature, training, and inference pipelines. We will zoom in on each of them and understand their scope and interface.",
      "content_length": 1010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Figure 1.5: FTI pipelines architecture\n\nBefore going into the details, it is essential to understand that each pipeline is a different component that can run on a different process or hardware. Thus, each pipeline can be written using a different technology, by a different team, or scaled differently. The key idea is that the design is very flexible to the needs of your team. It acts as a mind map for structuring your architecture.",
      "content_length": 435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "The feature pipeline\n\nThe feature pipeline takes raw data as input, processes it, and outputs the features and labels required by the model for training or inference. Instead of directly passing them to the model, the features and labels are stored inside a feature store. Its responsibility is to store, version, track, and share the features. By saving the features in a feature store, we always have a state of our features. Thus, we can easily send the features to the training and inference pipelines.\n\nAs the data is versioned, we can always ensure that the training and inference time features match. Thus, we avoid the training-serving skew problem.",
      "content_length": 657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "The training pipeline\n\nThe training pipeline takes the features and labels from the features stored as input and outputs a train model or models. The models are stored in a model registry. Its role is similar to that of feature stores, but this time, the model is the first-class citizen. Thus, the model registry will store, version, track, and share the model with the inference pipeline.\n\nAlso, most modern model registries support a metadata store that allows you to specify essential aspects of how the model was trained. The most important are the features, labels, and their version used to train the model. Thus, we will always know what data the model was trained on.",
      "content_length": 676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "The inference pipeline\n\nThe inference pipeline takes as input the features and labels from the feature store and the trained model from the model registry. With these two, predictions can be easily made in either batch or real-time mode.\n\nAs this is a versatile pattern, it is up to you to decide what you do with your predictions. If it’s a batch system, they will probably be stored in a DB. If it’s a real-time system, the predictions will be served to the client who requested them. Additionally, the features, labels, and models are versioned. We can easily upgrade or roll back the deployment of the model. For example, we will always know that model v1 uses features F1, F2, and F3, and model v2 uses F2, F3, and F4. Thus, we can quickly change the connections between the model and features.",
      "content_length": 799,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Benefits of the FTI architecture\n\nTo conclude, the most important thing you must remember about the FTI pipelines is their interface:\n\nThe feature pipeline takes in data and outputs the features and labels saved to the feature store.\n\nThe training pipeline queries the features store for features and labels and outputs a model to the model registry.\n\nThe inference pipeline uses the features from the feature store and the model from the model registry to make predictions.\n\nIt doesn’t matter how complex your ML system gets, these interfaces will remain the same.\n\nNow that we understand better how the pattern works, we want to highlight the main benefits of using this pattern:",
      "content_length": 681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "As you have just three components, it is intuitive to use and easy to understand.\n\nEach component can be written into its tech stack, so we can quickly adapt them to specific needs, such as big or streaming data. Also, it allows us to pick the best tools for the job.\n\nAs there is a transparent interface between the three components, each one can be developed by a different team (if necessary), making the development more manageable and scalable.\n\nEvery component can be deployed, scaled, and monitored independently.\n\nThe final thing you must understand about the FTI pattern is that the system doesn’t have to contain only three pipelines. In most cases, it will include more. For example, the feature pipeline can be composed of a service that computes the features and one that validates the data. Also, the training pipeline can be composed of the training and evaluation components.\n\nThe FTI pipelines act as logical layers. Thus, it is perfectly fine for each to be complex and contain multiple services. However, what is essential is to stick to the same interface on how the FTI pipelines interact with each other through the feature store and model registries. By doing so, each FTI component can evolve differently, without knowing the details of each other and without breaking the system on new changes.",
      "content_length": 1319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "To learn more about the FTI pipeline pattern, consider reading From MLOps to ML Systems with Feature/Training/Inference Pipelines by Jim Dowling, CEO and co-founder of Hopsworks: https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines. His article inspired this section.\n\nNow that we understand the FTI pipeline architecture, the final step of this chapter is to see how it can be applied to the LLM Twin use case.",
      "content_length": 427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Designing the system architecture of the LLM Twin\n\nIn this section, we will list the concrete technical details of the LLM Twin application and understand how we can solve them by designing our LLM system using the FTI architecture. However, before diving into the pipelines, we want to highlight that we won’t focus on the tooling or the tech stack at this step. We only want to define a high-level architecture of the system, which is language-, framework-, platform-, and infrastructure- agnostic at this point. We will focus on each component’s scope, interface, and interconnectivity. In future chapters, we will cover the implementation details and tech stack.",
      "content_length": 666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Listing the technical details of the LLM Twin architecture\n\nUntil now, we defined what the LLM Twin should support from the user’s point of view. Now, let’s clarify the requirements of the ML system from a purely technical perspective:\n\nOn the data side, we have to do the following:\n\nCollect data from LinkedIn, Medium, Substack, and GitHub completely autonomously and on a schedule\n\nStandardize the crawled data and store it in a data warehouse\n\nClean the raw data\n\nCreate instruct datasets for fine-tuning an LLM\n\nChunk and embed the cleaned data. Store the vectorized data into a vector DB for RAG.",
      "content_length": 602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "For training, we have to do the following:\n\nFine-tune LLMs of various sizes (7B, 14B, 30B, or 70B parameters)\n\nFine-tune on instruction datasets of multiple sizes\n\nSwitch between LLM types (for example, between Mistral, Llama, and GPT)\n\nTrack and compare experiments\n\nTest potential production LLM candidates before deploying them\n\nAutomatically start the training when new instruction datasets are available.\n\nThe inference code will have the following properties:\n\nA REST API interface for clients to interact with the LLM Twin\n\nAccess to the vector DB in real time for RAG\n\nInference with LLMs of various sizes",
      "content_length": 613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Autoscaling based on user requests\n\nAutomatically deploy the LLMs that pass the evaluation step.\n\nThe system will support the following LLMOps features:\n\nInstruction dataset versioning, lineage, and reusability\n\nModel versioning, lineage, and reusability\n\nExperiment tracking\n\nContinuous training, continuous integration, and continuous delivery (CT/CI/CD)\n\nPrompt and system monitoring\n\nIf any technical requirement doesn’t make sense now, bear with us. To avoid repetition, we will examine the details in their specific chapter.",
      "content_length": 530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "The preceding list is quite comprehensive. We could have detailed it even more, but at this point, we want to focus on the core functionality. When implementing each component, we will look into all the little details. But for now, the fundamental question we must ask ourselves is this: How can we apply the FTI pipeline design to implement the preceding list of requirements?",
      "content_length": 377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "How to design the LLM Twin architecture using the FTI pipeline design\n\nWe will split the system into four core components. You will ask yourself this: “Four? Why not three, as the FTI pipeline design clearly states?” That is a great question. Fortunately, the answer is simple. We must also implement the data pipeline along the three feature/training/inference pipelines. According to best practices:\n\nThe data engineering team owns the data pipeline\n\nThe ML engineering team owns the FTI pipelines.\n\nGiven our goal of building an MVP with a small team, we must implement the entire application. This includes defining the data collection and FTI pipelines. Tackling a problem end to end is often encountered in start-ups that can’t afford dedicated teams. Thus, engineers have to wear many hats, depending on the state of the product. Nevertheless, in any scenario, knowing how an end-to-end ML system works is valuable for better understanding other people’s work.\n\nFigure 1.6 shows the LLM system architecture. The best way to understand it is to review the four components individually and explain how they work.",
      "content_length": 1117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "Figure 1.6: LLM Twin high-level architecture",
      "content_length": 44,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "Data collection pipeline\n\nThe data collection pipeline involves crawling your personal data from Medium, Substack, LinkedIn, and GitHub. As a data pipeline, we will use the extract, load, transform (ETL) pattern to extract data from social media platforms, standardize it, and load it into a data warehouse.\n\nIt is critical to highlight that the data collection pipeline is designed to crawl data only from your social media platform. It will not have access to other people. As an example for this book, we agreed to make our collected data available for learning purposes. Otherwise, using other people’s data without their consent is not moral.\n\nThe output of this component will be a NoSQL DB, which will act as our data warehouse. As we work with text data, which is naturally unstructured, a NoSQL DB fits like a glove.\n\nEven though a NoSQL DB, such as MongoDB, is not labeled as a data warehouse, from our point of view, it will act as one. Why? Because it stores standardized raw data gathered by various ETL pipelines that are ready to be ingested into an ML system.",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "The collected digital data is binned into three categories:\n\nArticles (Medium, Substack)\n\nPosts (LinkedIn)\n\nCode (GitHub)\n\nWe want to abstract away the platform where the data was crawled. For example, when feeding an article to the LLM, knowing it came from Medium or Substack is not essential. We can keep the source URL as metadata to give references. However, from the processing, fine-tuning, and RAG points of view, it is vital to know what type of data we ingested, as each category must be processed differently. For example, the chunking strategy between a post, article, and piece of code will look different.\n\nAlso, by grouping the data by category, not the source, we can quickly plug data from other platforms, such as X into the posts or GitLab into the code collection. As a modular system, we must attach an additional ETL in the data collection pipeline, and everything else will work without further code modifications.",
      "content_length": 937,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Feature pipeline\n\nThe feature pipeline’s role is to take raw articles, posts, and code data points from the data warehouse, process them, and load them into the feature store.\n\nThe characteristics of the FTI pattern are already present.\n\nHere are some custom properties of the LLM Twin’s feature pipeline:\n\nIt processes three types of data differently: articles, posts, and code\n\nIt contains three main processing steps necessary for fine-tuning and RAG: cleaning, chunking, and embedding\n\nIt creates two snapshots of the digital data, one after cleaning (used for fine- tuning) and one after embedding (used for RAG)\n\nIt uses a logical feature store instead of a specialized feature store",
      "content_length": 689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Let’s zoom in on the logical feature store part a bit. As with any RAG-based system, one of the central pieces of the infrastructure is a vector DB. Instead of integrating another DB, more concretely, a specialized feature store, we used the vector DB, plus some additional logic to check all the properties of a feature store our system needs.\n\nThe vector DB doesn’t offer the concept of a training dataset, but it can be used as a NoSQL DB. This means we can access data points using their ID and collection name. Thus, we can easily query the vector DB for new data points without any vector search logic. Ultimately, we will wrap the retrieved data into a versioned, tracked, and shareable artifact—more on artifacts in Chapter 2. For now, you must know it is an MLOps concept used to wrap data and enrich it with the properties listed before.\n\nHow will the rest of the system access the logical feature store? The training pipeline will use the instruct datasets as artifacts, and the inference pipeline will query the vector DB for additional context using vector search techniques.\n\nFor our use case, this is more than enough because of the following reasons:\n\nThe artifacts work great for offline use cases such as training\n\nThe vector DB is built for online access, which we require for inference.",
      "content_length": 1306,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "In future chapters, however, we will explain how the three data categories (articles, posts, and code) are cleaned, chunked, and embedded.\n\nTo conclude, we take in raw article, post, or code data points, process them, and store them in a feature store to make them accessible to the training and inference pipelines. Note that trimming all the complexity away and focusing only on the interface is a perfect match with the FTI pattern. Beautiful, right?",
      "content_length": 453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Training pipeline\n\nThe training pipeline consumes instruct datasets from the feature store, fine- tunes an LLM with it, and stores the tuned LLM weights in a model registry. More concretely, when a new instruct dataset is available in the logical feature store, we will trigger the training pipeline, consume the artifact, and fine-tune the LLM.\n\nIn the initial stages, the data science team owns this step. They run multiple experiments to find the best model and hyperparameters for the job, either through automatic hyperparameter tuning or manually. To compare and pick the best set of hyperparameters, we will use an experiment tracker to log everything of value and compare it between experiments. Ultimately, they will pick the best hyperparameters and fine-tuned LLM and propose it as the LLM production candidate. The proposed LLM is then stored in the model registry. After the experimentation phase is over, we store and reuse the best hyperparameters found to eliminate the manual restrictions of the process. Now, we can completely automate the training process, known as continuous training.\n\nThe testing pipeline is triggered for a more detailed analysis than during fine-tuning. Before pushing the new model to production, assessing it against a stricter set of tests is critical to see that the latest candidate is better than what is currently in production. If this step passes, the model is ultimately tagged as accepted and deployed to the production inference pipeline. Even in a fully automated ML system, it is recommended to have a manual step before accepting a new production model. It is like pushing the red button before a significant action with high consequences. Thus, at this stage, an expert looks at a report generated by the testing component. If",
      "content_length": 1783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "everything looks good, it approves the model, and the automation can continue.\n\nThe particularities of this component will be on LLM aspects, such as the following:\n\nHow do you implement an LLM agnostic pipeline?\n\nWhat fine-tuning techniques should you use?\n\nHow do you scale the fine-tuning algorithm on LLMs and datasets of various sizes?\n\nHow do you pick an LLM production candidate from multiple experiments?\n\nHow do you test the LLM to decide whether to push it to production or not?\n\nBy the end of this book, you will know how to answer all these questions.\n\nOne last aspect we want to clarify is CT. Our modular design allows us to quickly leverage an ML orchestrator to schedule and trigger different",
      "content_length": 708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "system parts. For example, we can schedule the data collection pipeline to crawl data every week.\n\nThen, we can trigger the feature pipeline when new data is available in the data warehouse and the training pipeline when new instruction datasets are available.",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Inference pipeline\n\nThe inference pipeline is the last piece of the puzzle. It is connected to the model registry and logical feature store. It loads a fine-tuned LLM from the model registry, and from the logical feature store, it accesses the vector DB for RAG. It takes in client requests through a REST API as queries. It uses the fine-tuned LLM and access to the vector DB to carry out RAG and answer the queries.\n\nAll the client queries, enriched prompts using RAG, and generated answers are sent to a prompt monitoring system to analyze, debug, and better understand the system. Based on specific requirements, the monitoring system can trigger alarms to take action either manually or automatically.\n\nAt the interface level, this component follows exactly the FTI architecture, but when zooming in, we can observe unique characteristics of an LLM and RAG system, such as the following:\n\nA retrieval client used to do vector searches for RAG\n\nPrompt templates used to map user queries and external information to LLM inputs\n\nSpecial tools for prompt monitoring",
      "content_length": 1066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Final thoughts on the FTI design and the LLM Twin architecture\n\nWe don’t have to be highly rigid about the FTI pattern. It is a tool used to clarify how to design ML systems. For example, instead of using a dedicated features store just because that is how it is done, in our system, it is easier and cheaper to use a logical feature store based on a vector DB and artifacts. What was important to focus on were the required properties a feature store provides, such as a versioned and reusable training dataset.\n\nUltimately, we will explain the computing requirements of each component briefly. The data collection and feature pipeline are mostly CPU-based and do not require powerful machines. The training pipeline requires powerful GPU-based machines that could load an LLM and fine-tune it. The inference pipeline is somewhere in the middle. It still needs a powerful machine but is less compute-intensive than the training step. However, it must be tested carefully, as the inference pipeline directly interfaces with the user. Thus, we want the latency to be within the required parameters for a good user experience. However, using the FTI design is not an issue. We can pick the proper computing requirements for each component.\n\nAlso, each pipeline will be scaled differently. The data and feature pipelines will be scaled horizontally based on the CPU and RAM load. The training pipeline will be scaled vertically by adding more GPUs. The inference pipeline will be scaled horizontally based on the number of client requests.",
      "content_length": 1536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "To conclude, the presented LLM architecture checks all the technical requirements listed at the beginning of the section. It processes the data as requested, and the training is modular and can be quickly adapted to different LLMs, datasets, or fine-tuning techniques. The inference pipeline supports RAG and is exposed as a REST API. On the LLMOps side, the system supports dataset and model versioning, lineage, and reusability. The system has a monitoring service, and the whole ML architecture is designed with CT/CI/CD in mind.\n\nThis concludes the high-level overview of the LLM Twin architecture.",
      "content_length": 602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Summary\n\nThis first chapter was critical to understanding the book’s goal. As a product-oriented book that will walk you through building an end-to-end ML system, it was essential to understand the concept of an LLM Twin initially. Afterward, we walked you through what an MVP is and how to plan our LLM Twin MVP based on our available resources. Following this, we translated our concept into a practical technical solution with specific requirements. In this context, we introduced the FTI design pattern and showcased its real-world application in designing systems that are both modular and scalable. Ultimately, we successfully applied the FTI pattern to design the architecture of the LLM Twin to fit all our technical requirements.\n\nHaving a clear vision of the big picture is essential when building systems. Understanding how a single component will be integrated into the rest of the application can be very valuable when working on it. We started with a more abstract presentation of the LLM Twin architecture, focusing on each component’s scope, interface, and interconnectivity.\n\nThe following chapters will explore how to implement and deploy each component. On the MLOps side, we will walk you through using a computing platform, orchestrator, model registry, artifacts, and other tools and concepts to support all MLOps best practices.",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "References\n\nDowling, J. (2024a, July 11). From MLOps to ML Systems with Feature/Training/Inference Pipelines. Hopsworks. https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines\n\nDowling, J. (2024b, August 5). Modularity and Composability for AI Systems with AI Pipelines and Shared Storage. Hopsworks. https://www.hopsworks.ai/post/modularity-and-composability-for-ai- systems-with-ai-pipelines-and-shared-storage\n\nJoseph, M. (2024, August 23). The Taxonomy for Data Transformations in AI Systems. Hopsworks. https://www.hopsworks.ai/post/a-taxonomy-for- data-transformations-in-ai-systems\n\nMLOps: Continuous delivery and automation pipelines in machine learning. (2024, August 28). Google Cloud. https://cloud.google.com/architecture/mlops-continuous-delivery-and- automation-pipelines-in-machine-learning\n\nQwak. (2024a, June 2). CI/CD for Machine Learning in 2024: Best Practices to build, test, and Deploy | Infer. Medium. https://medium.com/infer-qwak/ci-cd-for-machine-learning-in-2024-best- practices-to-build-test-and-deploy-c4ad869824d2",
      "content_length": 1057,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Qwak. (2024b, July 23). 5 Best Open Source Tools to build End-to-End MLOPs Pipeline in 2024. Medium. https://medium.com/infer- qwak/building-an-end-to-end-mlops-pipeline-with-open-source-tools- d8bacbf4184f\n\nSalama, K., Kazmierczak, J., & Schut, D. (2021). Practitioners guide to MLOPs: A framework for continuous delivery and automation of machine learning (1st ed.) [PDF]. Google Cloud. https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whi tepaper.pdf\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "2",
      "content_length": 1,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "Tooling and Installation\n\nThis chapter presents all the essential tools that will be used throughout the book, especially in implementing and deploying the LLM Twin project. At this point in the book, we don’t plan to present in-depth LLM, RAG, MLOps, or LLMOps concepts. We will quickly walk you through our tech stack and prerequisites to avoid repeating ourselves throughout the book on how to set up a particular tool and why we chose it. Starting with Chapter 3, we will begin exploring our LLM Twin use case by implementing a data collection ETL that crawls data from the internet.\n\nIn the first part of the chapter, we will present the tools within the Python ecosystem to manage multiple Python versions, create a virtual environment, and install the pinned dependencies required for our project to run. Alongside presenting these tools, we will also show how to install the\n\nLLM-Engineers-Handbook\n\nrepository on your local machine (in case you want to try out the code yourself): https://github.com/PacktPublishing/LLM-Engineers-Handbook.\n\nNext, we will explore all the MLOps and LLMOps tools we will use, starting with more generic tools, such as a model registry, and moving on to more LLM-oriented tools, such as LLM evaluation and prompt monitoring tools. We will also understand how to manage a project with multiple ML pipelines using ZenML, an orchestrator bridging the gap between ML and MLOps. Also, we will quickly explore what databases we will use for NoSQL and vector storage. We will show you how to run all these components on your local machine using Docker. Lastly, we will quickly",
      "content_length": 1608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "review AWS and show you how to create an AWS user and access keys and install and configure the AWS CLI to manipulate your cloud resources programmatically. We will also explore SageMaker and why we use it to train and deploy our open-source LLMs.\n\nIf you are familiar with these tools, you can safely skip this chapter. We also explain how to install the project and set up all the necessary components in the repository’s\n\nREADME\n\n. Thus, you also have the option to use that as more concise documentation if you plan to run the code while reading the book.\n\nTo sum all that up, in this chapter, we will explore the following topics:\n\nPython ecosystem and project installation\n\nMLOps and LLMOps tooling\n\nDatabases for storing unstructured and vector data\n\nPreparing for AWS",
      "content_length": 775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "By the end of this chapter, you will be aware of all the tools we will use across the book. Also, you will have learned how to install the\n\nLLM-Engineers-Handbook\n\nrepository, set up the rest of the tools, and use them if you run the code while reading the book.",
      "content_length": 262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Python ecosystem and project installation\n\nAny Python project needs three fundamental tools: the Python interpreter, dependency management, and a task execution tool. The Python interpreter executes your Python project as expected. All the code within the book is tested with Python 3.11.8. You can download the Python interpreter from here: https://www.python.org/downloads/. We recommend installing the exact Python version (Python 3.11.8) to run the LLM Twin project using\n\npyenv\n\n, making the installation process straightforward.\n\nInstead of installing multiple global Python versions, we recommend managing them using\n\npyenv\n\n, a Python version management tool that lets you manage multiple Python versions between projects. You can install it using this link: https://github.com/pyenv/pyenv?tab=readme-ov-file#installation.\n\nAfter you have installed\n\npyenv\n\n, you can install the latest version of Python 3.11, using\n\npyenv",
      "content_length": 930,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": ", as follows:\n\npyenv install 3.11.8\n\nNow list all installed Python versions to see that it was installed correctly:\n\npyenv versions\n\nYou should see something like this:\n\n#\n\nsystem\n\n#\n\n3.11.8",
      "content_length": 190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "To make Python 3.11.8 the default version across your entire system (whenever you open a new terminal), use the following command:\n\npyenv global 3.11.8\n\nHowever, we aim to use Python 3.11.8 locally only in our repository. To achieve that, first, we have to clone the repository and navigate to it:\n\ngit clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git cd LLM-Engineers-Handbook\n\nBecause we defined a\n\n.python-version\n\nfile within the repository,\n\npyenv",
      "content_length": 471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "will know to pick up the version from that file and use it locally whenever you are working within that folder. To double-check that, run the following command while you are in the repository:\n\npython --version\n\nIt should output:\n\n# Python 3.11.8\n\nTo create the\n\n.python-version\n\nfile, you must run\n\npyenv local 3.11.8\n\nonce. Then,\n\npyenv",
      "content_length": 338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "will always know to use that Python version while working within a specific directory.\n\nNow that we have installed the correct Python version using\n\npyenv\n\n, let’s move on to Poetry, which we will use as our dependency and virtual environment manager.",
      "content_length": 251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Poetry: dependency and virtual environment management\n\nPoetry is one of the most popular dependency and virtual environment managers within the Python ecosystem. But let’s start by clarifying what a dependency manager is. In Python, a dependency manager allows you to specify, install, update, and manage external libraries or packages (dependencies) that a project relies on. For example, this is a simple Poetry requirements file that uses Python 3.11 and the\n\nrequests\n\nand\n\nnumpy\n\nPython packages.\n\n[tool.poetry.dependencies] python = \"^3.11\" requests = \"^2.25.1\" numpy = \"^1.19.5\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\"\n\nBy using Poetry to pin your dependencies, you always ensure that you install the correct version of the dependencies that your projects work with. Poetry, by default, saves all its requirements in",
      "content_length": 867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "pyproject.toml\n\nfiles, which are stored at the root of your repository, as you can see in the cloned LLM-Engineers-Handbook repository.\n\nAnother massive advantage of using Poetry is that it creates a new Python virtual environment in which it installs the specified Python version and requirements. A virtual environment allows you to isolate your project’s dependencies from your global Python dependencies and other projects. By doing so, you ensure there are no version clashes between projects. For example, let’s assume that Project A needs\n\nnumpy == 1.19.5\n\n, and Project B needs\n\nnumpy == 1.26.0\n\n. If you keep both projects in the global Python environment, that will not work, as Project B will override Project A’s\n\nnumpy\n\ninstallation, which will corrupt Project A and stop it from working. Using Poetry, you can isolate each project in its own Python environment with its own Python dependencies, avoiding any dependency clashes.\n\nYou can install Poetry from here: https://python-poetry.org/docs/. We use Poetry 1.8.3 throughout the book. Once Poetry is installed, navigate to your cloned LLM-Engineers-Handbook repository and run the following command to install all the necessary Python dependencies:",
      "content_length": 1214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "poetry install --without aws\n\nThis command knows to pick up all the dependencies from your repository that are listed in the\n\npyproject.toml\n\nand\n\npoetry.lock\n\nfiles. After the installation, you can activate your Poetry environment by running\n\npoetry shell\n\nin your terminal or by prefixing all your CLI commands as follows:\n\npoetry run\n\n.\n\nOne final note on Poetry is that it locks down the exact versions of the dependency tree in the\n\npoetry.lock\n\nfile based on the definitions added to the\n\nproject.toml\n\nfile. While the",
      "content_length": 524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "pyproject.toml\n\nfile may specify version ranges (e.g.,\n\nrequests = \"^2.25.1\"\n\n), the\n\npoetry.lock\n\nfile records the exact version (e.g.,\n\nrequests = \"2.25.1\"\n\n) that was installed. It also locks the versions of sub-dependencies (dependencies of your dependencies), which may not be explicitly listed in your\n\npyproject.toml\n\nfile. By locking all the dependencies and sub-dependencies to specific versions, the\n\npoetry.lock\n\nfile ensures that all project installations use the same versions of each package. This consistency leads to predictable behavior, reducing the likelihood of encountering “works on my machine” issues.\n\nOther tools similar to Poetry are Venv and Conda for creating virtual environments. Still, they lack the dependency management option. Thus, you must do it through Python’s default\n\nrequirements.txt\n\nfiles, which are less powerful than Poetry’s",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "lock\n\nfiles. Another option is Pipenv, which feature-wise is more like Poetry but slower, and\n\nuv\n\n, which is a replacement for Poetry built in Rust, making it blazing fast.\n\nuv\n\nhas lots of potential to replace Poetry, making it worthwhile to test out: https://github.com/astral-sh/uv.\n\nThe final piece of the puzzle is to look at the task execution tool we used to manage all our CLI commands.",
      "content_length": 395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "Poe the Poet: task execution tool\n\nPoe the Poet is a plugin on top of Poetry that is used to manage and execute all the CLI commands required to interact with the project. It helps you define and run tasks within your Python project, simplifying automation and script execution. Other popular options are Makefile, Invoke, or shell scripts, but Poe the Poet eliminates the need to write separate shell scripts or Makefiles for managing project tasks, making it an elegant way to manage tasks using the same configuration file that Poetry already uses for dependencies.\n\nWhen working with Poe the Poet, instead of having all your commands documented in a README file or other document, you can add them directly to your\n\npyproject.toml\n\nfile and execute them in the command line with an alias. For example, using Poe the Poet, we can define the following tasks in a\n\npyproject.toml\n\nfile:\n\n[tool.poe.tasks]\n\ntest",
      "content_length": 911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "=\n\n\"pytest\"\n\nformat\n\n=\n\n\"black .\"\n\nstart\n\n=\n\n\"python main.py\"\n\nYou can then run these tasks using the\n\npoe\n\ncommand:\n\npoetry poe test poetry poe format poetry poe start\n\nYou can install Poe the Poet as a Poetry plugin, as follows:",
      "content_length": 230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "poetry self add 'poethepoet[poetry_plugin]'\n\nTo conclude, using a tool as a façade over all your CLI commands is necessary to run your application. It significantly simplifies the application’s complexity and enhances collaboration as it acts as out-of-the- box documentation.\n\nAssuming you have\n\npyenv\n\nand Poetry installed, here are all the commands you need to run to clone the repository and install the dependencies and Poe the Poet as a Poetry plugin:\n\ngit clone https://github.com/PacktPublishing/LLM-Engineers- Handbook.gitcd LLM-Engineers-Handbook poetry install --without aws poetry self add 'poethepoet[poetry_plugin]'\n\nTo make the project fully operational, there are still a few steps to follow, such as filling out a",
      "content_length": 730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": ".env\n\nfile with your credentials and getting tokens from OpenAI and Hugging Face. But this book isn’t an installation guide, so we’ve moved all these details into the repository’s README as they are useful only if you plan to run the repository: https://github.com/PacktPublishing/LLM-Engineers- Handbook.\n\nNow that we have installed our Python project, let’s present the MLOps tools we will use in the book. If you are already familiar with these tools, you can safely skip the following tooling section and move on to the Databases for storing unstructured and vector data section.",
      "content_length": 583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "MLOps and LLMOps tooling\n\nThis section will quickly present all the MLOps and LLMOps tools we will use throughout the book and their role in building ML systems using MLOps best practices. At this point in the book, we don’t aim to detail all the MLOps components we will use to implement the LLM Twin use case, such as model registries and orchestrators, but only provide a quick idea of what they are and how to use them. As we develop the LLM Twin project throughout the book, you will see hands-on examples of how we use all these tools. In Chapter 11, we will dive deeply into the theory of MLOps and LLMOps and connect all the dots. As the MLOps and LLMOps fields are highly practical, we will leave the theory of these aspects to the end, as it will be much easier to understand it after you go through the LLM Twin use case implementation.\n\nAlso, this section is not dedicated to showing you how to set up each tool. It focuses primarily on what each tool is used for and highlights the core features used throughout this book.\n\nStill, using Docker, you can quickly run the whole infrastructure locally. If you want to run the steps within the book yourself, you can host the application locally with these three simple steps:\n\nHave Docker 27.1.1 (or higher) installed.",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "Fill your\n\n.env\n\nfile with all the necessary credentials as explained in the repository README.\n\nRun\n\npoetry\n\npoe\n\nlocal-infrastructure-up\n\nto locally spin up ZenML (\n\nhttp://127.0.0.1:8237/\n\n) and the MongoDB and Qdrant databases.\n\nYou can read more details on how to run everything locally in the LLM- Engineers-Handbook repository README: https://github.com/PacktPublishing/LLM-Engineers-Handbook. Within the book, we will also show you how to deploy each component to the cloud.",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "Hugging Face: model registry\n\nA model registry is a centralized repository that manages ML models throughout their lifecycle. It stores models along with their metadata, version history, and performance metrics, serving as a single source of truth. In MLOps, a model registry is crucial for tracking, sharing, and documenting model versions, facilitating team collaboration. Also, it is a fundamental element in the deployment process as it integrates with continuous integration and continuous deployment (CI/CD) pipelines.\n\nWe used Hugging Face as our model registry, as we can leverage its ecosystem to easily share our fine-tuned LLM Twin models with anyone who reads the book. Also, by following the Hugging Face model registry interface, we can easily integrate the model with all the frameworks around the LLMs ecosystem, such as Unsloth for fine-tuning and SageMaker for inference.\n\nOur fine-tuned LLMs are available on Hugging Face at:\n\nTwinLlama 3.1 8B (after fine-tuning): https://huggingface.co/mlabonne/TwinLlama-3.1-8B\n\nTwinLlama 3.1 8B DPO (after preference alignment): https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO",
      "content_length": 1137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Figure 2.1: Hugging Face model registry example\n\nFor a quick demo, we have them available on Hugging Face Spaces:\n\nTwinLlama 3.1 8B: https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B\n\nTwinLlama 3.1 8B DPO: https://huggingface.co/spaces/mlabonne/TwinLlama-3.1-8B-DPO\n\nMost ML tools provide model registry features. For example, ZenML, Comet, and SageMaker, which we will present in future sections, also offer their own model registries. They are good options, but we picked Hugging Face solely because of its ecosystem, which provides easy shareability and integration throughout the open-source environment. Thus, you will usually",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "select the model registry that integrates the most with your project’s tooling and requirements.",
      "content_length": 96,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "ZenML: orchestrator, artifacts, and metadata\n\nZenML acts as the bridge between ML and MLOps. Thus, it offers multiple MLOps features that make your ML pipeline traceability, reproducibility, deployment, and maintainability easier. At its core, it is designed to create reproducible workflows in machine learning. It addresses the challenge of transitioning from exploratory research in Jupyter notebooks to a production-ready ML environment. It tackles production-based replication issues, such as versioning difficulties, reproducing experiments, organizing complex ML workflows, bridging the gap between training and deployment, and tracking metadata. Thus, ZenML’s main features are orchestrating ML pipelines, storing and versioning ML pipelines as outputs, and attaching metadata to artifacts for better observability.\n\nInstead of being another ML platform, ZenML introduced the concept of a stack, which allows you to run ZenML on multiple infrastructure options. A stack will enable you to connect ZenML to different cloud services, such as:\n\nAn orchestrator and compute engine (for example, AWS SageMaker or Vertex AI)\n\nRemote storage (for instance, AWS S3 or Google Cloud Storage buckets)\n\nA container registry (for example, Docker Registry or AWS ECR)",
      "content_length": 1261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "Thus, ZenML acts as a glue that brings all your infrastructure and tools together in one place through its stack feature, allowing you to quickly iterate through your development processes and easily monitor your entire ML system. The beauty of this is that ZenML doesn’t vendor-lock you into any cloud platform. It completely abstracts away the implementation of your Python code from the infrastructure it runs on. For example, in our LLM Twin use case, we used the AWS stack:\n\nSageMaker as our orchestrator and compute\n\nS3 as our remote storage used to store and track artifacts\n\nECR as our container registry\n\nHowever, the Python code contains no S3 or ECR particularities, as ZenML takes care of them. Thus, we can easily switch to other providers, such as Google Cloud Storage or Azure. For more details on ZenML stacks, you can start here: https://docs.zenml.io/user-guide/production- guide/understand-stacks.\n\nWe will focus only on the ZenML features used throughout the book, such as orchestrating, artifacts, and metadata. For more details on ZenML, check out their starter guide: https://docs.zenml.io/user-guide/starter-guide.",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "The local version of the ZenML server comes installed as a Python package. Thus, when running\n\npoetry install\n\n, it installs a ZenML debugging server that you can use locally. In Chapter 11, we will show you how to use their cloud serverless option to deploy the ML pipelines to AWS.",
      "content_length": 283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Orchestrator\n\nAn orchestrator is a system that automates, schedules, and coordinates all your ML pipelines. It ensures that each pipeline—such as data ingestion, preprocessing, model training, and deployment—executes in the correct order and handles dependencies efficiently. By managing these processes, an orchestrator optimizes resource utilization, handles failures gracefully, and enhances scalability, making complex ML pipelines more reliable and easier to manage.\n\nHow does ZenML work as an orchestrator? It works with pipelines and steps. A pipeline is a high-level object that contains multiple steps. A function becomes a ZenML pipeline by being decorated with\n\n@pipeline\n\n, and a step when decorated with\n\n@step\n\n. This is a standard pattern when using orchestrators: you have a high-level function, often called a pipeline, that calls multiple units/steps/tasks.\n\nLet’s explore how we can implement a ZenML pipeline with one of the ML pipelines implemented for the LLM Twin project. In the code snippet below, we defined a ZenML pipeline that queries the database for a user based on its full name and crawls all the provided links under that user:",
      "content_length": 1161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "from\n\nzenml\n\nimport\n\npipeline\n\nfrom\n\nsteps.etl\n\nimport\n\ncrawl_links, get_or_create_user\n\n@pipeline\n\ndef\n\ndigital_data_etl\n\n(\n\nuser_full_name:\n\nstr\n\n, links:\n\nlist\n\n[\n\nstr\n\n]",
      "content_length": 173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": ") ->\n\nNone\n\n: user = get_or_create_user(user_full_name) crawl_links(user=user, links=links)\n\nYou can run the pipeline with the following CLI command:\n\npoetry poe run-digital-data-etl\n\n. To visualize the pipeline run, you can go to your ZenML dashboard (at\n\nhttp://127.0.0.1:8237/\n\n) and, on the left panel, click on the Pipelines tab and then on the digital_data_etl pipeline, as illustrated in Figure 2.2:",
      "content_length": 406,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "Figure 2.2: ZenML Pipelines dashboard\n\nAfter clicking on the digital_data_etl pipeline, you can visualize all the previous and current pipeline runs, as seen in Figure 2.3. You can see which one succeeded, failed, or is still running. Also, you can see the stack used to run the pipeline, where the default stack is the one used to run your ML pipelines locally.",
      "content_length": 362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "Figure 2.3: ZenML digital_data_etl pipeline dashboard. Example of a specific pipeline\n\nNow, after clicking on the latest digital_data_etl pipeline run (or any other run that succeeded or is still running), we can visualize the pipeline’s steps, outputs, and insights, as illustrated in Figure 2.4. This structure is often called a directed acyclic graph (DAG). More on DAGs in Chapter 11.",
      "content_length": 388,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Figure 2.4: ZenML digital_data_etl pipeline run dashboard (example of a specific pipeline run)\n\nBy clicking on a specific step, you can get more insights into its code and configuration. It even aggregates the logs output by that specific step to avoid switching between tools, as shown in Figure 2.5.",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "Figure 2.5: Example of insights from a specific step of the digital_data_etl pipeline run\n\nNow that we understand how to define a ZenML pipeline and how to look it up in the dashboard, let’s quickly look at how to define a ZenML step. In the code snippet below, we defined the\n\nget_or_create_user()\n\nstep, which works just like a normal Python function but is decorated with\n\n@step\n\n. We won’t go into the details of the logic, as we will cover the ETL logic in Chapter 3. For now, we will focus only on the ZenML functionality.",
      "content_length": 528,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "from\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\ntyping_extensions\n\nimport\n\nAnnotated\n\nfrom\n\nzenml\n\nimport\n\nget_step_context, step\n\nfrom\n\nllm_engineering.application\n\nimport\n\nutils\n\nfrom\n\nllm_engineering.domain.documents",
      "content_length": 207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "import\n\nUserDocument\n\n@step\n\ndef\n\nget_or_create_user\n\n(\n\nuser_full_name:\n\nstr\n\n) -> Annotated[UserDocument,\n\n\"user\"\n\n]: logger.info(\n\nf\"Getting or creating user:\n\n{user_full_name}\n\n\"\n\n) first_name, last_name = utils.split_user_full_name(user_full_name) user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)\n\nreturn\n\nuser",
      "content_length": 344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Within a ZenML step, you can define any Python logic your use case needs. In this simple example, we are just creating or retrieving a user, but we could replace that code with anything, starting from data collection to feature engineering and training. What is essential to notice is that to integrate ZenML with your code, you have to write modular code, where each function does just one thing. The modularity of your code makes it easy to decorate your functions with\n\n@step\n\nand then glue multiple steps together within a main function decorated with\n\n@pipeline\n\n. One design choice that will impact your application is deciding the granularity of each step, as each will run as a different unit on a different machine when deployed in the cloud.\n\nTo decouple our code from ZenML, we encapsulated all the application and domain logic into the\n\nllm_engineering\n\nPython module. We also defined the\n\npipelines\n\nand\n\nsteps\n\nfolders, where we defined our ZenML logic. Within the\n\nsteps\n\nmodule, we only used what we needed from the",
      "content_length": 1031,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "llm_engineering\n\nPython module (similar to how you use a Python package). In the\n\npipelines\n\nmodule, we only aggregated ZenML steps to glue them into the final pipeline. Using this design, we can easily swap ZenML with another orchestrator or use our application logic in other use cases, such as a REST API. We only have to replace the ZenML code without touching the\n\nllm_engineering\n\nmodule where all our logic resides.\n\nThis folder structure is reflected at the root of the LLM-Engineers- Handbook repository, as illustrated in Figure 2.6:",
      "content_length": 543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Figure 2.6: LLM-Engineers-Handbook repository folder structure\n\nOne last thing to consider when writing ZenML steps is that if you return a value, it should be serializable. ZenML can serialize most objects that can be reduced to primitive data types, but there are a few exceptions. For example, we used UUID types as IDs throughout the code, which aren’t natively supported by ZenML. Thus, we had to extend ZenML’s materializer to support UUIDs. We raised this issue to ZenML. Hence, in future ZenML versions, UUIDs will be supported, but it was an excellent example of the serialization aspect of transforming function outputs in artifacts.",
      "content_length": 643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Artifacts and metadata\n\nAs mentioned in the previous section, ZenML transforms any step output into an artifact. First, let’s quickly understand what an artifact is. In MLOps, an artifact is any file(s) produced during the machine learning lifecycle, such as datasets, trained models, checkpoints, or logs. Artifacts are crucial for reproducing experiments and deploying models. We can transform anything into an artifact. For example, the model registry is a particular use case for an artifact. Thus, artifacts have these unique properties: they are versioned, sharable, and have metadata attached to them to understand what’s inside quickly. For example, when wrapping your dataset with an artifact, you can add to its metadata the size of the dataset, the train-test split ratio, the size, types of labels, and anything else useful to understand what’s inside the dataset without actually downloading it.\n\nLet’s circle back to our digital_data_etl pipeline example, where we had as a step output an artifact, the crawled links, which are an artifact, as seen in Figure 2.7",
      "content_length": 1076,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "Figure 2.7: ZenML artifact example using the digital_data_etl pipeline as an example\n\nBy clicking on the\n\ncrawled_links\n\nartifact and navigating to the Metadata tab, we can quickly see all the domains we crawled for a particular author, the number of links we crawled for each domain, and how many were successful, as illustrated in Figure 2.8:",
      "content_length": 344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Figure 2.8: ZenML metadata example using the digital_data_etl pipeline as an example\n\nA more interesting example of an artifact and its metadata is the generated dataset artifact. In Figure 2.9, we can visualize the metadata of the\n\ninstruct_datasets\n\nartifact, which was automatically generated and will be used to fine-tune the LLM Twin model. More details on the\n\ninstruction datasets",
      "content_length": 387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "are in Chapter 5. For now, we want to highlight that within the dataset’s metadata, we have precomputed a lot of helpful information about it, such as how many data categories it contains, its storage size, and the number of samples per training and testing split.",
      "content_length": 264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Figure 2.9: ZenML metadata example for the instruct_datasets artifact",
      "content_length": 69,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "The metadata is manually added to the artifact, as shown in the code snippet below. Thus, you can precompute and attach to the artifact’s metadata anything you consider helpful for dataset discovery across your business and projects:\n\n…\n\n# More imports\n\nfrom\n\nzenml\n\nimport\n\nArtifactConfig, get_step_context, step\n\n@step\n\ndef\n\ngenerate_intruction_dataset\n\n(\n\nprompts: Annotated[\n\ndict\n\n[DataCategory,\n\nlist\n\n[GenerateDatasetSamplesPrompt]],",
      "content_length": 440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "\"prompts\"\n\n]\n\n) -> Annotated[ InstructTrainTestSplit, ArtifactConfig( name=\n\n\"instruct_datasets\"\n\n, tags=[\n\n\"dataset\"\n\n,\n\n\"instruct\"\n\n,\n\n\"cleaned\"\n\n], ), ]: datasets = …\n\n# Generate datasets\n\nstep_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"instruct_datasets\"\n\n, metadata=_get_metadata_instruct_dataset(datasets))\n\nreturn\n\ndatasets\n\ndef\n\n_get_metadata_instruct_dataset\n\n(",
      "content_length": 404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "datasets: InstructTrainTestSplit\n\n) ->\n\ndict\n\n[\n\nstr\n\n,\n\nAny\n\n]: instruct_dataset_categories =\n\nlist\n\n(datasets.train.keys()) train_num_samples = { category: instruct_dataset.num_samples\n\nfor\n\ncategory, instruct_dataset\n\nin\n\ndatasets.train.items() } test_num_samples = {category: instruct_dataset.num_samples\n\nfor\n\ncategory, instruct_dataset\n\nin\n\ndatasets.test.items()}\n\nreturn",
      "content_length": 377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "{\n\n\"data_categories\"\n\n: instruct_dataset_categories,\n\n\"test_split_size\"\n\n: datasets.test_split_size,\n\n\"train_num_samples_per_category\"\n\n: train_num_samples,\n\n\"test_num_samples_per_category\"\n\n: test_num_samples, }\n\nAlso, you can easily download and access a specific version of the dataset using its Universally Unique Identifier (UUID), which you can find using the ZenML dashboard or CLI:\n\nfrom\n\nzenml.client\n\nimport\n\nClient artifact = Client().get_artifact_version(\n\n'8bba35c4-8ff9-4d8f-a039-08046efc9fdc'",
      "content_length": 507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": ") loaded_artifact = artifact.load()\n\nThe last step in exploring ZenML is understanding how to run and configure a ZenML pipeline.",
      "content_length": 129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "How to run and configure a ZenML pipeline\n\nAll the ZenML pipelines can be called from the\n\nrun.py\n\nfile, accessed at\n\ntools/run.py\n\nin our GitHub repository. Within the\n\nrun.py\n\nfile, we implemented a simple CLI that allows you to specify what pipeline to run. For example, to call the\n\ndigital_data_etl\n\npipeline to crawl Maxime’s content, you have to run:\n\npython -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_maxime_labonne.yaml\n\nOr, to crawl Paul’s content, you can run:",
      "content_length": 501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml\n\nAs explained when introducing Poe the Poet, all our CLI commands used to interact with the project will be executed through Poe to simplify and standardize the project. Thus, we encapsulated these Python calls under the following\n\npoe\n\nCLI commands:\n\npoetry poe run-digital-data-etl-maxime poetry poe run-digital-data-etl-paul\n\nWe only change the ETL config file name when scraping content for different people. ZenML allows us to inject specific configuration files at runtime as follows:\n\nconfig_path = root_dir /",
      "content_length": 614,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "\"configs\"\n\n/ etl_config_filename assert config_path.exists(), f\n\n\"Config file not found: { config_path }\"\n\nrun_args_etl = {\n\n\"config_path\"\n\n: config_path,\n\n\"run_name\"\n\n: f\n\n\"digital_data_etl_run_{dt.now().strftime('\n\n%Y\n\n_\n\n%m\n\n_\n\n%d\n\n_\n\n%H\n\n_\n\n%M\n\n_\n\n%S",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "')}\"\n\n} digital_data_etl.with_options()(**run_args_etl)\n\nIn the config file, we specify all the parameters that will input the pipeline as parameters. For example, the\n\nconfigs/digital_data_etl_maxime_labonne.yaml\n\nconfiguration file looks as follows:\n\nparameters: user_full_name: Maxime Labonne\n\n# [First Name(s)] [Last Name]\n\nlinks:\n\n# Personal Blog\n\nhttps://mlabonne.github.io/blog/posts/2024-07- 29_Finetune_Llama31.html - https://mlabonne.github.io/blog/posts/2024- 07-15_The_Rise_of_Agentic_Data_Generation.html # Substack - https://maximelabonne.substack.com/p/uncensor-any-llm-with-abliteration- d30148b7d43e … # More links",
      "content_length": 631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "Where the\n\ndigital_data_etl\n\nfunction signature looks like this:\n\n@pipeline\n\ndef\n\ndigital_data_etl\n\n(\n\nuser_full_name:\n\nstr\n\n, links:\n\nlist\n\n[\n\nstr\n\n]\n\n) ->\n\nstr\n\n:",
      "content_length": 164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "This approach allows us to configure each pipeline at runtime without modifying the code. We can also clearly track the inputs for all our pipelines, ensuring reproducibility. As seen in Figure 2.10, we have one or more configs for each pipeline.",
      "content_length": 246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "Figure 2.10: ZenML pipeline configs\n\nOther popular orchestrators similar to ZenML that we’ve personally tested and consider powerful are Airflow, Prefect, Metaflow, and Dagster. Also, if you are a heavy user of Kubernetes, you can opt for Agro Workflows or Kubeflow, the latter of which works only on top of Kubernetes. We still consider ZenML the best trade-off between ease of use, features, and costs. Also, none of these tools offer the stack feature that is offered by ZenML, which allows it to avoid vendor-locking you in to any cloud ecosystem.\n\nIn Chapter 11, we will explore in more depth how to leverage an orchestrator to implement MLOps best practices. But now that we understand ZenML, what it is helpful for, and how to use it, let’s move on to the experiment tracker.",
      "content_length": 782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "Comet ML: experiment tracker\n\nTraining ML models is an entirely iterative and experimental process. Unlike traditional software development, it involves running multiple parallel experiments, comparing them based on predefined metrics, and deciding which one should advance to production. An experiment tracking tool allows you to log all the necessary information, such as metrics and visual representations of your model predictions, to compare all your experiments and quickly select the best model. Our LLM project is no exception.\n\nAs illustrated in Figure 2.11, we used Comet to track metrics such as training and evaluation loss or the value of the gradient norm across all our experiments.",
      "content_length": 697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Figure 2.11: Comet ML training metrics example\n\nUsing an experiment tracker, you can go beyond training and evaluation metrics and log your training hyperparameters to track different configurations between experiments.\n\nIt also logs out-of-the-box system metrics such as GPU, CPU, or memory utilization to give you a clear picture of what resources you need during training and where potential bottlenecks slow down your training, as seen in Figure 2.12.",
      "content_length": 455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "Figure 2.12: Comet ML system metrics example\n\nYou don’t have to set up Comet locally. We will use their online version for free without any constraints throughout this book. Also, if you want to look more in-depth into the Comet ML experiment tracker, we made the training experiments tracked with Comet ML public while fine-tuning our LLM Twin models. You can access them here: https://www.comet.com/mlabonne/llm-twin-training/view/new/panels.\n\nOther popular experiment trackers are W&B, MLflow, and Neptune. We’ve worked with all of them and can state that they all have mostly the same features, but Comet ML differentiates itself through its ease of use and intuitive interface. Let’s move on to the final piece of the MLOps puzzle: Opik for prompt monitoring.",
      "content_length": 764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "Opik: prompt monitoring\n\nYou cannot use standard tools and techniques when logging and monitoring prompts. The reason for this is complicated. We will dig into it in Chapter 11. However, to quickly give you some understanding, you cannot use standard logging tools as prompts are complex and unstructured chains.\n\nWhen interacting with an LLM application, you chain multiple input prompts and the generated output into a trace, where one prompt depends on previous prompts.\n\nThus, instead of plain text logs, you need an intuitive way to group these traces into a specialized dashboard that makes debugging and monitoring traces of prompts easier.\n\nWe used Opik, an open-source tool made by Comet, as our prompt monitoring tool because it follows Comet’s philosophy of simplicity and ease of use, which is currently relatively rare in the LLM landscape. Other options offering similar features are Langfuse (open source, https://langfuse.com), Galileo (not open source, rungalileo.io), and LangSmith (not open source, https://www.langchain.com/langsmith), but we found their solutions more cumbersome to use and implement. Opik, along with its serverless option, also provides a free open-source version that you have complete control over. You can read more on Opik at https://github.com/comet-ml/opik.",
      "content_length": 1303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "Databases for storing unstructured and vector data\n\nWe also want to present the NoSQL and vector databases we will use within our examples. When working locally, they are already integrated through Docker. Thus, when running\n\npoetry poe local-infrastructure-up\n\n, as instructed a few sections above, local images of Docker for both databases will be pulled and run on your machine. Also, when deploying the project, we will show you how to use their serverless option and integrate it with the rest of the LLM Twin project.",
      "content_length": 523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "MongoDB: NoSQL database\n\nMongoDB is one of today’s most popular, robust, fast, and feature-rich NoSQL databases. It integrates well with most cloud ecosystems, such as AWS, Google Cloud, Azure, and Databricks. Thus, using MongoDB as our NoSQL database was a no-brainer.\n\nWhen we wrote this book, MongoDB was used by big players such as Novo Nordisk, Delivery Hero, Okta, and Volvo. This widespread adoption suggests that MongoDB will remain a leading NoSQL database for a long time.\n\nWe use MongoDB as a NoSQL database to store the raw data we collect from the internet before processing it and pushing it into the vector database. As we work with unstructured text data, the flexibility of the NoSQL database fits like a charm.",
      "content_length": 728,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "Qdrant: vector database\n\nQdrant (https://qdrant.tech/) is one of the most popular, robust, and feature- rich vector databases. We could have used almost any vector database for our small MVP, but we wanted to pick something light and likely to be used in the industry for many years to come.\n\nWe will use Qdrant to store the data from MongoDB after it’s processed and transformed for GenAI usability.\n\nQdrant is used by big players such as X (formerly Twitter), Disney, Microsoft, Discord, and Johnson & Johnson. Thus, it is highly probable that Qdrant will remain in the vector database game for a long time.\n\nWhile writing the book, other popular options were Milvus, Redis, Weaviate, Pinecone, Chroma, and pgvector (a PostgreSQL plugin for vector indexes). We found that Qdrant offers the best trade-off between RPS, latency, and index time, making it a solid choice for many generative AI applications.\n\nComparing all the vector databases in detail could be a chapter in itself. We don’t want to do that here. Still, if curious, you can check the Vector DB Comparison resource from Superlinked at https://superlinked.com/vector- db-comparison, which compares all the top vector databases in terms of everything you can think about, from the license and release year to database features, embedding models, and frameworks supported.",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "Preparing for AWS\n\nThis last part of the chapter will focus on setting up an AWS account (if you don’t already have one), an AWS access key, and the CLI. Also, we will look into what SageMaker is and why we use it.\n\nWe picked AWS as our cloud provider because it’s the most popular out there and the cloud in which we (the writers) have the most experience. The reality is that other big cloud providers, such as GCP or Azure, offer similar services. Thus, depending on your specific application, there is always a trade-off between development time (in which you have the most experience), features, and costs. But for our MVP, AWS, it’s the perfect option as it provides robust features for everything we need, such as S3 (object storage), ECR (container registry), and SageMaker (compute for training and inference).",
      "content_length": 819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "Setting up an AWS account, an access key, and the CLI\n\nAs AWS could change its UI/UX, the best way to instruct you on how to create an AWS account is by redirecting you to their official tutorial: https://docs.aws.amazon.com/accounts/latest/reference/manage-acct- creating.html.\n\nAfter successfully creating an AWS account, you can access the AWS console at http://console.aws.amazon.com. Select Sign in using root user email (found under the Sign in button), then enter your account’s email address and password.\n\nNext, we must generate access keys to access AWS programmatically. The best option to do so is first to create an IAM user with administrative access as described in this AWS official tutorial: https://docs.aws.amazon.com/streams/latest/dev/setting-up.html\n\nFor production accounts, it is best practice to grant permissions with a policy of least privilege, giving each user only the permissions they require to perform their role. However, to simplify the setup of our test account, we will use the\n\nAdministratorAccess\n\nmanaged policy, which gives our user full access, as explained in the tutorial above and illustrated in Figure 2.13.",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Figure 2.13: IAM user permission policies example\n\nNext, you have to create an access key for the IAM user you just created using the following tutorial: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access- keys.html.\n\nThe access keys will look as follows:\n\naws_access_key_id\n\n=\n\naws_secret_access_key\n\n=",
      "content_length": 323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Just be careful to store them somewhere safe, as you won’t be able to access them after you create them. Also, be cautious with who you share them, as they could be used to access your AWS account and manipulate various AWS resources.\n\nThe last step is to install the AWS CLI and configure it with your newly created access keys. You can install the AWS CLI using the following link: https://docs.aws.amazon.com/cli/latest/userguide/getting-started- install.html.\n\nAfter installing the AWS CLI, you can configure it by running\n\naws configure\n\n. Here is an example of our AWS configuration:\n\n[default]\n\naws_access_key_id\n\n= *************\n\naws_secret_access_key\n\n= ************\n\nregion",
      "content_length": 683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "= eu-central-\n\n1\n\noutput\n\n= json\n\nFor more details on how to configure the AWS CLI, check out the following tutorial: https://docs.aws.amazon.com/cli/v1/userguide/cli-configure- files.html.\n\nAlso, to configure the project with your AWS credentials, you must fill in the following variables within your\n\n.env\n\nfile:\n\nAWS_REGION=\"eu-central-1\" # Change it with your AWS region. By default, we use \"eu-central-1\". AWS_ACCESS_KEY=\"\" AWS_SECRET_KEY=\"\"",
      "content_length": 446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "An important note about costs associated with hands-on tasks in this book\n\nAll the cloud services used across the book stick to their freemium option, except AWS. Thus, if you use a personal AWS account, you will be responsible for AWS costs as you follow along in this book. While some services may fall under AWS Free Tier usage, others will not. Thus, you are responsible for checking your billing console regularly.\n\nMost of the costs will come when testing SageMaker for training and inference. Based on our tests, the AWS costs can vary between $50 and $100 using the specifications provided in this book and repository.\n\nSee the AWS documentation on setting up billing alarms to monitor your costs at https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monit or_estimated_charges_with_cloudwatch.html.",
      "content_length": 819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "SageMaker: training and inference compute\n\nThe last topic of this chapter is understanding SageMaker and why we decided to use it. SageMaker is an ML platform used to train and deploy ML models. An official definition is as follows: AWS SageMaker is a fully managed machine learning service by AWS that enables developers and data scientists to build, train, and deploy machine learning models at scale. It simplifies the process by handling the underlying infrastructure, allowing users to focus on developing high-quality models efficiently.\n\nWe will use SageMaker to fine-tune and operationalize our training pipeline on clusters of GPUs and to deploy our custom LLM Twin model as a REST API that can be accessed in real time from anywhere in the world.",
      "content_length": 756,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "Why AWS SageMaker?\n\nWe must also discuss why we chose AWS SageMaker over simpler and more cost-effective options, such as AWS Bedrock. First, let’s explain Bedrock and its benefits.\n\nAmazon Bedrock is a serverless solution for deploying LLMs. Serverless means that there are no servers or infrastructure to manage. It provides pre- trained models, which you can access directly through API calls. When we wrote this book, they provided support only for Mistral, Flan, Llama 2, and Llama 3 (quite a limited list of options). You can send input data and receive predictions from the models without managing the underlying infrastructure or software. This approach significantly reduces the complexity and time required to integrate AI capabilities into applications, making it more accessible to developers with limited machine learning expertise. However, this ease of integration comes at the cost of limited customization options, as you’re restricted to the pre-trained models and APIs provided by Amazon Bedrock. In terms of pricing, Bedrock uses a simple pricing model based on the number of API calls. This straightforward pricing structure makes it more efficient to estimate and control costs.\n\nMeanwhile, SageMaker provides a comprehensive platform for building, training, and deploying machine learning models. It allows you to customize your ML processes entirely or even use the platform for research. That’s why SageMaker is mainly used by data scientists and machine learning experts who know how to program, understand machine learning concepts, and are comfortable working with cloud platforms such as AWS. SageMaker is a double-edged sword regarding costs, following a pay-as-",
      "content_length": 1692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "you-go pricing model similar to most AWS services. This means you have to pay for the usage of computing resources, storage, and any other services required to build your applications.\n\nIn contrast to Bedrock, even if the SageMaker endpoint is not used, you will still pay for the deployed resources on AWS, such as online EC2 instances. Thus, you have to design autoscaling systems that delete unused resources. To conclude, Bedrock offers an out-of-the-box solution that allows you to quickly deploy an API endpoint powered by one of the available foundation models. Meanwhile, SageMaker is a multi-functional platform enabling you to customize your ML logic fully.\n\nSo why did we choose SageMaker over Bedrock? Bedrock would have been an excellent solution for quickly prototyping something, but this is a book on LLM engineering, and our goal is to dig into all the engineering aspects that Bedrock tries to mask away. Thus, we chose SageMaker because of its high level of customizability, allowing us to show you all the engineering required to deploy a model.\n\nIn reality, even SageMaker isn’t fully customizable. If you want complete control over your deployment, use EKS, AWS’s Kubernetes self-managed service. In this case, you have direct access to the virtual machines, allowing you to fully customize how you build your ML pipelines, how they interact, and how you manage your resources. You could do the same thing with AWS ECS, AWS’s version of Kubernetes. Using EKS or ECS, you could also reduce the costs, as these services cost considerably less.\n\nTo conclude, SageMaker strikes a balance between complete control and customization and a fully managed service that hides all the engineering complexity behind the scenes. This balance ensures that you have the",
      "content_length": 1776,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "control you need while also benefiting from the managed service’s convenience.",
      "content_length": 78,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "Summary\n\nIn this chapter, we reviewed the core tools used across the book. First, we understood how to install the correct version of Python that supports our repository. Then, we looked over how to create a virtual environment and install all the dependencies using Poetry. Finally, we understood how to use a task execution tool like Poe the Poet to aggregate all the commands required to run the application.\n\nThe next step was to review all the tools used to ensure MLOps best practices, such as a model registry to share our models, an experiment tracker to manage our training experiments, an orchestrator to manage all our ML pipelines and artifacts, and metadata to manage all our files and datasets. We also understood what type of databases we need to implement the LLM Twin use case. Finally, we explored the process of setting up an AWS account, generating an access key, and configuring the AWS CLI for programmatic access to the AWS cloud. We also gained a deep understanding of AWS SageMaker and the reasons behind choosing it to build our LLM Twin application.\n\nIn the next chapter, we will explore the implementation of the LLM Twin project by starting with the data collection ETL that scrapes posts, articles, and repositories from the internet and stores them in a data warehouse.",
      "content_length": 1300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "References\n\nAcsany, P. (2024, February 19). Dependency Management With Python Poetry. https://realpython.com/dependency-management-python-poetry/\n\nComet.ml. (n.d.). comet-ml/opik: Open-source end-to-end LLM Development Platform. GitHub. https://github.com/comet-ml/opik\n\nCzakon, J. (2024, September 25). ML Experiment Tracking: What It Is, Why It Matters, and How to Implement It. neptune.ai. https://neptune.ai/blog/ml- experiment-tracking\n\nHopsworks. (n.d.). ML Artifacts (ML Assets)? Hopsworks. https://www.hopsworks.ai/dictionary/ml-artifacts\n\nIntroduction | Documentation | Poetry – Python dependency management and packaging made easy. (n.d.). https://python-poetry.org/docs\n\nJones, L. (2024, March 21). Managing Multiple Python Versions With pyenv. https://realpython.com/intro-to-pyenv/",
      "content_length": 794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Kaewsanmua, K. (2024, January 3). Best Machine Learning Workflow and Pipeline Orchestration Tools. neptune.ai. https://neptune.ai/blog/best- workflow-and-pipeline-orchestration-tools\n\nMongoDB. (n.d.). What is NoSQL? NoSQL databases explained. https://www.mongodb.com/resources/basics/databases/nosql-explained\n\nNat-N. (n.d.). nat-n/poethepoet: A task runner that works well with poetry. GitHub. https://github.com/nat-n/poethepoet\n\nOladele, S. (2024, August 29). ML Model Registry: The Ultimate Guide. neptune.ai. https://neptune.ai/blog/ml-model-registry\n\nSchwaber-Cohen, R. (n.d.). What is a Vector Database & How Does it Work? Use Cases + Examples. Pinecone. https://www.pinecone.io/learn/vector-database/\n\nStarter guide | ZenML Documentation. (n.d.). https://docs.zenml.io/user- guide/starter-guide\n\nVector DB Comparison. (n.d.). https://superlinked.com/vector-db- comparison",
      "content_length": 879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "Join our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 144,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "3",
      "content_length": 1,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Data Engineering\n\nThis chapter will begin exploring the LLM Twin project in more depth. We will learn how to design and implement the data collection pipeline to gather the raw data we will use in all our LLM use cases, such as fine- tuning or inference. As this is not a book on data engineering, we will keep this chapter short and focus only on what is strictly necessary to collect the required raw data. Starting with Chapter 4, we will concentrate on LLMs and GenAI, exploring its theory and concrete implementation details.\n\nWhen working on toy projects or doing research, you usually have a static dataset with which you work. But in our LLM Twin use case, we want to mimic a real-world scenario where we must gather and curate the data ourselves. Thus, implementing our data pipeline will connect the dots regarding how an end-to-end ML project works. This chapter will explore how to design and implement an Extract, Transform, Load (ETL) pipeline that crawls multiple social platforms, such as Medium, Substack, or GitHub, and aggregates the gathered data into a MongoDB data warehouse. We will show you how to implement various crawling methods, standardize the data, and load it into a data warehouse.\n\nWe will begin by designing the LLM Twin’s data collection pipeline and explaining the architecture of the ETL pipeline. Afterward, we will move directly to implementing the pipeline, starting with ZenML, which will orchestrate the entire process. We will investigate the crawler implementation and understand how to implement a dispatcher layer that instantiates the right crawler class based on the domain of the provided link while following software best practices. Next, we will learn how to",
      "content_length": 1711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "implement each crawler individually. Also, we will show you how to implement a data layer on top of MongoDB to structure all our documents and interact with the database.\n\nFinally, we will explore how to run the data collection pipeline using ZenML and query the collected data from MongoDB.\n\nThus, in this chapter, we will study the following topics:\n\nDesigning the LLM Twin’s data collection pipeline\n\nImplementing the LLM Twin’s data collection pipeline\n\nGathering raw data into the data warehouse\n\nBy the end of this chapter, you will know how to design and implement an ETL pipeline to extract, transform, and load raw data ready to be ingested into the ML application.",
      "content_length": 674,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "Designing the LLM Twin’s data collection pipeline\n\nBefore digging into the implementation, we must understand the LLM Twin’s data collection ETL architecture, illustrated in Figure 3.1. We must explore what platforms we will crawl to extract data from and how we will design our data structures and processes. However, the first step is understanding how our data collection pipeline maps to an ETL process.\n\nAn ETL pipeline involves three fundamental steps:\n\nWe extract data from various sources. We will crawl data from platforms like Medium, Substack, and GitHub to gather raw data.\n\nWe transform this data by cleaning and standardizing it into a consistent format suitable for storage and analysis.\n\nWe load the transformed data into a data warehouse or database.\n\nFor our project, we use MongoDB as our NoSQL data warehouse. Although this is not a standard approach, we will explain the reasoning behind this choice shortly.",
      "content_length": 929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "Figure 3.1: LLM Twin’s data collection ETL pipeline architecture\n\nWe want to design an ETL pipeline that inputs a user and a list of links as input. Afterward, it crawls each link individually, standardizes the collected content, and saves it under that specific author in a MongoDB data warehouse.\n\nHence, the signature of the data collection pipeline will look as follows:\n\nInput: A list of links and their associated user (the author)\n\nOutput: A list of raw documents stored in the NoSQL data warehouse\n\nWe will use\n\nuser\n\nand\n\nauthor\n\ninterchangeably, as in most scenarios across the ETL pipeline, a user is the author of the extracted content. However, within the data warehouse, we have only a user collection.",
      "content_length": 716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "The ETL pipeline will detect the domain of each link, based on which it will call a specialized crawler. We implemented four different crawlers for three different data categories, as seen in Figure 3.2. First, we will explore the three fundamental data categories we will work with across the book. All our collected documents can be boiled down to an article, repository (or code), and post. It doesn’t matter where the data comes from. We are primarily interested in the document’s format. In most scenarios, we will have to process these data categories differently. Thus, we created a different domain entity for each, where each entity will have its class and collection in MongoDB. As we save the source URL within the document’s metadata, we will still know its source and can reference it in our GenAI use cases.",
      "content_length": 821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "Figure 3.2: The relationship between the crawlers and the data categories\n\nOur codebase supports four different crawlers:\n\nMedium crawler: Used to collect data from Medium. It outputs an article document. It logs in to Medium and crawls the HTML of the article’s link. Then, it extracts, cleans, and normalizes the text from the HTML and loads the standardized text of the article into the NoSQL data warehouse.\n\nCustom article crawler: It performs similar steps to the Medium crawler but is a more generic implementation for collecting articles from various sites. Thus, as it doesn’t implement any particularities of any platform, it doesn’t perform the login step and blindly gathers all the HTML from a particular link. This is enough for articles freely available online, which you can find on Substack and people’s blogs. We will use this crawler as a safety net when the link’s domain isn’t associated with the other supported crawlers. For example, when providing a Substack link, it will default to the custom article crawler, but when providing a Medium URL, it will use the Medium crawler.\n\nGitHub crawler:This collects data from GitHub. It outputs a repository document. It clones the repository, parses the repository file tree, cleans and normalizes the files, and loads them to the database.\n\nLinkedIn crawler:This is used to collect data from LinkedIn. It outputs multiple post documents. It logs in to LinkedIn, navigates to the user’s",
      "content_length": 1452,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "feed, and crawls all the user’s latest posts. For each post, it extracts its HTML, cleans and normalizes it, and loads it to MongoDB.\n\nIn the next section, we will examine each crawler’s implementation in detail. For now, note that each crawler accesses a specific platform or site in a particular way and extracts HTML from it. Afterward, all the crawlers parse the HTML, extract the text from it, and clean and normalize it so it can be stored in the data warehouse under the same interface.\n\nBy reducing all the collected data to three data categories and not creating a new data category for every new data source, we can easily extend this architecture to multiple data sources with minimal effort. For example, if we want to start collecting data from X, we only have to implement a new crawler that outputs a post document, and that’s it. The rest of the code will remain untouched. Otherwise, if we introduced the source dimension in the class and document structure, we would have to add code to all downstream layers to support any new data source. For example, we would have to implement a new document class for each new source and adapt the feature pipeline to support it.\n\nFor our proof of concept, crawling a few hundred documents is enough, but if we want to scale it to a real-world product, we would probably need more data sources to crawl from. LLMs are data-hungry. Thus, you need thousands of documents for ideal results instead of just a few hundred. But in many projects, it’s an excellent strategy to implement an end-to-end project version that isn’t the most accurate and iterate through it later. Thus, by using this architecture, you can easily add more data sources in future iterations to gather a larger dataset. More on LLM fine-tuning and dataset size will be covered in the next chapter.",
      "content_length": 1822,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "How is the ETL process connected to the feature pipeline? The feature pipeline ingests the raw data from the MongoDB data warehouse, cleans it further, processes it into features, and stores it in the Qdrant vector DB to make it accessible for the LLM training and inference pipelines. Chapter 4 provides more information on the feature pipeline. The ETL process is independent of the feature pipeline. The two pipelines communicate with each other strictly through the MongoDB data warehouse. Thus, the data collection pipeline can write data for MongoDB, and the feature pipeline can read from it independently and on different schedules.\n\nWhy did we use MongoDB as a data warehouse? Using a transactional database, such as MongoDB, as a data warehouse is uncommon. However, in our use case, we are working with small amounts of data, which MongoDB can handle. Even if we plan to compute statistics on top of our MongoDB collections, it will work fine at the scale of our LLM Twin’s data (hundreds of documents). We picked MongoDB to store our raw data primarily because of the nature of our unstructured data: text crawled from the internet. By mainly working with unstructured text, selecting a NoSQL database that doesn’t enforce a schema made our development easier and faster. Also, MongoDB is stable and easy to use. Their Python SDK is intuitive. They provide a Docker image that works out of the box locally and a cloud freemium tier that is perfect for proofs of concept, such as the LLM Twin. Thus, we can freely work with it locally and in the cloud. However, when working with big data (millions of documents or more), using a dedicated data warehouse such as Snowflake or BigQuery will be ideal.\n\nNow that we’ve understood the architecture of the LLM Twin’s data collection pipeline, let’s move on to its implementation.",
      "content_length": 1835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "Implementing the LLM Twin’s data collection pipeline\n\nAs we presented in Chapter 2, the entry point to each pipeline from our LLM Twin project is a ZenML pipeline, which can be configured at runtime through YAML files and run through the ZenML ecosystem. Thus, let’s start by looking into the ZenML\n\ndigital_data_etl\n\npipeline. You’ll notice that this is the same pipeline we used as an example in Chapter 2 to illustrate ZenML. But this time, we will dig deeper into the implementation, explaining how the data collection works behind the scenes. After understanding how the pipeline works, we will explore the implementation of each crawler used to collect data from various sites and the MongoDB documents used to store and query data from the data warehouse.",
      "content_length": 762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "ZenML pipeline and steps\n\nIn the code snippet below, we can see the implementation of the ZenML\n\ndigital_data_etl\n\npipeline, which inputs the user’s full name and a list of links that will be crawled under that user (considered the author of the content extracted from those links). Within the function, we call two steps. In the first one, we look up the user in the database based on its full name. Then, we loop through all the links and crawl each independently. The pipeline’s implementation is available in our repository at\n\npipelines/digital_data_etl.py\n\n.\n\nfrom\n\nzenml\n\nimport\n\npipeline\n\nfrom\n\nsteps.etl\n\nimport",
      "content_length": 620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "crawl_links, get_or_create_user\n\n@pipeline\n\ndef\n\ndigital_data_etl\n\n(\n\nuser_full_name:\n\nstr\n\n, links:\n\nlist\n\n[\n\nstr\n\n]\n\n) ->\n\nstr\n\n: user = get_or_create_user(user_full_name) last_step = crawl_links(user=user, links=links)\n\nreturn\n\nlast_step.invocation_id",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "Figure 3.3 shows a run of the\n\ndigital_data_etl\n\npipeline on the ZenML dashboard. The next phase is to explore the\n\nget_or_create_user\n\nand\n\ncrawl_links\n\nZenML steps individually. The step implementation is available in our repository at\n\nsteps/etl\n\n.",
      "content_length": 251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Figure 3.3: Example of a digital_data_etl pipeline run from ZenML’s dashboard\n\nWe will start with the\n\nget_or_create_user\n\nZenML step. We begin by importing the necessary modules and functions used throughout the script.\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\ntyping_extensions\n\nimport\n\nAnnotated\n\nfrom\n\nzenml\n\nimport\n\nget_step_context, step",
      "content_length": 339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "from\n\nllm_engineering.application\n\nimport\n\nutils\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nUserDocument\n\nNext, we define the function’s signature, which takes a user’s full name as input and retrieves an existing user or creates a new one in the MongoDB database if it doesn’t exist:\n\n@step\n\ndef\n\nget_or_create_user\n\n(\n\nuser_full_name:\n\nstr",
      "content_length": 347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": ") -> Annotated[UserDocument,\n\n\"user\"\n\n]:\n\nUsing a utility function, we split the full name into first and last names. Then, we attempt to retrieve the user from the database or create a new one if it doesn’t exist. We also retrieve the current step context and add metadata about the user to the output, which will be reflected in the metadata of the\n\nuser\n\nZenML output artifact:\n\nlogger.info(\n\nf\"Getting or creating user:\n\n{user_full_name}\n\n\"\n\n) first_name, last_name = utils.split_user_full_name(user_full_name) user = UserDocument.get_or_create(first_name=first_name, last_name=last_name) step_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"user\"",
      "content_length": 680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": ", metadata=_get_metadata(user_full_name, user))\n\nreturn\n\nuser\n\nAdditionally, we define a helper function called\n\n_get_metadata()\n\n, which builds a dictionary containing the query parameters and the retrieved user information, which will be added as metadata to the user artifact:\n\ndef\n\n_get_metadata\n\n(\n\nuser_full_name:\n\nstr\n\n, user: UserDocument\n\n) ->\n\ndict\n\n:",
      "content_length": 361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "return\n\n{\n\n\"query\"\n\n: {\n\n\"user_full_name\"\n\n: user_full_name, },\n\n\"retrieved\"\n\n: {\n\n\"user_id\"\n\n:\n\nstr\n\n(user.\n\nid\n\n),\n\n\"first_name\"\n\n: user.first_name,\n\n\"last_name\"\n\n: user.last_name, }, }",
      "content_length": 187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "We will move on to the\n\ncrawl_links\n\nZenML step, which collects the data from the provided links. The code begins by importing essential modules and libraries for web crawling:\n\nfrom\n\nurllib.parse\n\nimport\n\nurlparse\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\ntqdm\n\nimport\n\ntqdm\n\nfrom\n\ntyping_extensions",
      "content_length": 295,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "import\n\nAnnotated\n\nfrom\n\nzenml\n\nimport\n\nget_step_context, step\n\nfrom\n\nllm_engineering.application.crawlers.dispatcher\n\nimport\n\nCrawlerDispatcher\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nUserDocument\n\nFollowing the imports, the main function inputs a list of links written by a specific author. Within this function, a crawler dispatcher is initialized and configured to handle specific domains such as LinkedIn, Medium, and GitHub:",
      "content_length": 439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "@step\n\ndef\n\ncrawl_links\n\n(\n\nuser: UserDocument, links:\n\nlist\n\n[\n\nstr\n\n]\n\n) -> Annotated[\n\nlist\n\n[\n\nstr\n\n],\n\n\"crawled_links\"\n\n]: dispatcher = CrawlerDispatcher.build().register_linkedin().register_medium().register_gi thub() logger.info(\n\nf\"Starting to crawl\n\n{",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "len\n\n(links)}\n\nlink(s).\"\n\n)\n\nThe function initializes variables to store the output metadata and count successful crawls. It then iterates over each link. It attempts to crawl and extract data for each link, updating the count of successful crawls and accumulating metadata about each URL:\n\nmetadata = {} successfull_crawls =\n\n0\n\nfor\n\nlink\n\nin\n\ntqdm(links): successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, user) successfull_crawls += successfull_crawl metadata = _add_to_metadata(metadata, crawled_domain, successfull_crawl)",
      "content_length": 542,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "After processing all links, the function attaches the accumulated metadata to the output artifact:\n\nstep_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"crawled_links\"\n\n, metadata=metadata) logger.info(\n\nf\"Successfully crawled\n\n{successfull_crawls}\n\n/\n\n{\n\nlen\n\n(links)}\n\nlinks.\"\n\n)\n\nreturn\n\nlinks",
      "content_length": 325,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "The code includes a helper function that attempts to extract information from each link using the appropriate crawler based on the link’s domain. It handles any exceptions that may occur during extraction and returns a tuple indicating the crawl’s success and the link’s domain:\n\ndef\n\n_crawl_link\n\n(\n\ndispatcher: CrawlerDispatcher, link:\n\nstr\n\n, user: UserDocument\n\n) ->\n\ntuple\n\n[\n\nbool\n\n,\n\nstr\n\n]: crawler = dispatcher.get_crawler(link) crawler_domain = urlparse(link).netloc\n\ntry\n\n: crawler.extract(link=link, user=user)",
      "content_length": 522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "return\n\n(\n\nTrue\n\n, crawler_domain)\n\nexcept\n\nException\n\nas\n\ne: logger.error(\n\nf\"An error occurred while crawling:\n\n{e!s}\n\n\"\n\n)\n\nreturn\n\n(\n\nFalse\n\n, crawler_domain)\n\nAnother helper function is provided to update the metadata dictionary with the results of each crawl:",
      "content_length": 265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "def\n\n_add_to_metadata\n\n(\n\nmetadata:\n\ndict\n\n, domain:\n\nstr\n\n, successfull_crawl:\n\nbool\n\n) ->\n\ndict\n\n:\n\nif\n\ndomain\n\nnot\n\nin\n\nmetadata: metadata[domain] = {} metadata[domain][\n\n\"successful\"",
      "content_length": 186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "] = metadata.get(domain, {}).get(\n\n\"successful\"\n\n,\n\n0\n\n) + successfull_crawl metadata[domain][\n\n\"total\"\n\n] = metadata.get(domain, {}).get(\n\n\"total\"\n\n,\n\n0\n\n) +\n\n1\n\nreturn\n\nmetadata\n\nAs seen in the abovementioned\n\n_crawl_link()\n\nfunction, the\n\nCrawlerDispatcher",
      "content_length": 259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "class knows what crawler to initialize based on each link’s domain. The logic is then abstracted away under the crawler’s\n\nextract()\n\nmethod. Let’s zoom in on the\n\nCrawlerDispatcher\n\nclass to understand how this works fully.",
      "content_length": 224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "The dispatcher: How do you instantiate the right crawler?\n\nThe entry point to our crawling logic is the\n\nCrawlerDispatcher\n\nclass. As illustrated in Figure 3.4, the dispatcher acts as the intermediate layer between the provided links and the crawlers. It knows what crawler to associate with each URL.\n\nThe\n\nCrawlerDispatcher\n\nclass knows how to extract the domain of each link and initialize the proper crawler that collects the data from that site. For example, if it detects the https://medium.com domain when providing a link to an article, it will build an instance of the\n\nMediumCrawler\n\nused to crawl that particular platform. With that in mind, let’s explore the implementation of the\n\nCrawlerDispatcher\n\nclass.",
      "content_length": 719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "All the crawling logic is available in the GitHub repository at\n\nllm_engineering/application/crawlers\n\n.\n\nFigure 3.4: The relationship between the provided links, the CrawlerDispatcher, and the crawlers",
      "content_length": 202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "We begin by importing the necessary Python modules for URL handling and regex, along with importing our crawler classes:\n\nimport\n\nre\n\nfrom\n\nurllib.parse\n\nimport\n\nurlparse\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\n.base\n\nimport\n\nBaseCrawler\n\nfrom\n\n.custom_article",
      "content_length": 257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "import\n\nCustomArticleCrawler\n\nfrom\n\n.github\n\nimport\n\nGithubCrawler\n\nfrom\n\n.linkedin\n\nimport\n\nLinkedInCrawler\n\nfrom\n\n.medium\n\nimport\n\nMediumCrawler\n\nThe\n\nCrawlerDispatcher\n\nclass is defined to manage and dispatch appropriate crawler instances based on given URLs and their domains. Its constructor initializes a registry to store the registered crawlers.",
      "content_length": 353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "class\n\nCrawlerDispatcher\n\n:\n\ndef\n\n__init__\n\n(\n\nself\n\n) ->\n\nNone\n\n: self._crawlers = {}\n\nAs we are using the builder creational pattern to instantiate and configure the dispatcher, we define a\n\nbuild()\n\nclass method that returns an instance of the dispatcher:",
      "content_length": 258,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "@classmethod\n\ndef\n\nbuild\n\n(\n\ncls\n\n) ->\n\n\"CrawlerDispatcher\"\n\n: dispatcher = cls()\n\nreturn\n\ndispatcher\n\nThe dispatcher includes methods to register crawlers for specific platforms like Medium, LinkedIn, and GitHub. These methods use a generic\n\nregister()\n\nmethod under the hood to add each crawler to the registry. By returning self, we follow the builder creational pattern (more on the builder pattern: https://refactoring.guru/design-patterns/builder). We can chain multiple\n\nregister_*()\n\nmethods when instantiating the dispatcher as follows:\n\nCrawlerDispatcher.build().register_linkedin().register_medium()",
      "content_length": 610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": ".\n\ndef\n\nregister_medium\n\n(\n\nself\n\n) ->\n\n\"CrawlerDispatcher\"\n\n: self.register(\n\n\"https://medium.com\"\n\n, MediumCrawler)\n\nreturn\n\nself\n\ndef\n\nregister_linkedin\n\n(\n\nself\n\n) ->\n\n\"CrawlerDispatcher\"",
      "content_length": 191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": ": self.register(\n\n\"https://linkedin.com\"\n\n, LinkedInCrawler)\n\nreturn\n\nself\n\ndef\n\nregister_github\n\n(\n\nself\n\n) ->\n\n\"CrawlerDispatcher\"\n\n: self.register(\n\n\"https://github.com\"\n\n, GithubCrawler)\n\nreturn\n\nself\n\nThe generic\n\nregister()",
      "content_length": 229,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "method normalizes each domain to ensure its format is consistent before it’s added as a key to the\n\nself._crawlers\n\nregistry of the dispatcher. This is a critical step, as we will use the key of the dictionary as the domain pattern to match future links with a crawler:\n\ndef\n\nregister\n\n(\n\nself, domain:\n\nstr\n\n, crawler:\n\ntype\n\n[BaseCrawler]\n\n) ->\n\nNone\n\n: parsed_domain = urlparse(domain) domain = parsed_domain.netloc self._crawlers[\n\nr\"https://(www\\.)?{}/*\"\n\n.",
      "content_length": 462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "format\n\n(re.escape(domain))] = crawler\n\nFinally, the\n\nget_crawler()\n\nmethod determines the appropriate crawler for a given URL by matching it against the registered domains. If no match is found, it logs a warning and defaults to using the\n\nCustomArticleCrawler\n\n.\n\ndef\n\nget_crawler\n\n(\n\nself, url:\n\nstr\n\n) -> BaseCrawler:\n\nfor\n\npattern, crawler",
      "content_length": 344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "in\n\nself._crawlers.items():\n\nif\n\nre.\n\nmatch\n\n(pattern, url):\n\nreturn\n\ncrawler()\n\nelse\n\n: logger.warning(\n\nf\"No crawler found for\n\n{url}\n\n. Defaulting to CustomArticleCrawler.\"\n\n)\n\nreturn\n\nCustomArticleCrawler()\n\nThe next step in understanding how the data collection pipeline works is analyzing each crawler individually.",
      "content_length": 321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "The crawlers\n\nBefore exploring each crawler’s implementation, we must present their base class, which defines a unified interface for all the crawlers. As shown in Figure 3.4, we can implement the dispatcher layer because each crawler follows the same signature. Each class implements the\n\nextract()\n\nmethod, allowing us to leverage OOP techniques such as polymorphism, where we can work with abstract objects without knowing their concrete subclass. For example, in the\n\n_crawl_link()\n\nfunction from the ZenML steps, we had the following code:\n\ncrawler = dispatcher.get_crawler(link) crawler.extract(link=link, user=user)\n\nNote how we called the\n\nextract()\n\nmethod without caring about what specific type of crawler we instantiated. To conclude, working with abstract interfaces ensures core reusability and ease of extension.",
      "content_length": 827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "Base classes\n\nNow, let’s explore the\n\nBaseCrawler\n\ninterface, which can be found in the repository at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/crawlers/base.py.\n\nfrom\n\nabc\n\nimport\n\nABC, abstractmethod\n\nclass\n\nBaseCrawler\n\n(\n\nABC\n\n): model:\n\ntype",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "[NoSQLBaseDocument]\n\n@abstractmethod\n\ndef\n\nextract\n\n(\n\nself, link:\n\nstr\n\n, **kwargs\n\n) ->\n\nNone\n\n: ...\n\nAs mentioned above, the interface defines an\n\nextract()\n\nmethod that takes as input a link. Also, it defines a model attribute at the class level that represents the data category document type used to save the extracted data into the MongoDB data warehouse. Doing so allows us to customize each subclass with different data categories while preserving the same attributes at the class level. We will soon explore the\n\nNoSQLBaseDocument\n\nclass when digging into the document entities.",
      "content_length": 588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "We also extend the\n\nBaseCrawler\n\nclass with a\n\nBaseSeleniumCrawler\n\nclass, which implements reusable functionality that uses Selenium to crawl various sites, such as Medium or LinkedIn. Selenium is a tool for automating web browsers. It’s used to interact with web pages programmatically (like logging into LinkedIn, navigating through profiles, etc.).\n\nSelenium can programmatically control various browsers such as Chrome, Firefox, or Brave. For these specific platforms, we need Selenium to manipulate the browser programmatically to log in and scroll through the newsfeed or article before being able to extract the entire HTML. For other sites, where we don’t have to go through the login step or can directly load the whole page, we can extract the HTML from a particular URL using more straightforward methods than Selenium.\n\nFor the Selenium-based crawlers to work, you must install Chrome on your machine (or a Chromium-based browser such as Brave).\n\nThe code begins by setting up the necessary imports and configurations for web crawling using Selenium and the ChromeDriver initializer. The",
      "content_length": 1100,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "chromedriver_autoinstaller\n\nensures that the appropriate version of ChromeDriver is installed and added to the system path, maintaining compatibility with the installed version of your Google Chrome browser (or other Chromium-based browser). Selenium will use the ChromeDriver to communicate with the browser and open a headless session, where we can programmatically manipulate the browser to access various URLs, click on specific elements, such as buttons, or scroll through the newsfeed. Using the\n\nchromedriver_autoinstaller\n\n, we ensure we always have the correct ChromeDriver version installed that matches our machine’s Chrome browser version.\n\nimport\n\ntime\n\nfrom\n\ntempfile\n\nimport\n\nmkdtemp\n\nimport\n\nchromedriver_autoinstaller\n\nfrom\n\nselenium",
      "content_length": 750,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "import\n\nwebdriver\n\nfrom\n\nselenium.webdriver.chrome.options\n\nimport\n\nOptions\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nNoSQLBaseDocument\n\n# Check if the current version of chromedriver exists\n\n# and if it doesn't exist, download it automatically,\n\n# then add chromedriver to path\n\nchromedriver_autoinstaller.install()\n\nNext, we define the\n\nBaseSeleniumCrawler\n\nclass for use cases where we need Selenium to collect the data, such as collecting data from Medium or LinkedIn.",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "Its constructor initializes various Chrome options to optimize performance, enhance security, and ensure a headless browsing environment. These options disable unnecessary features like GPU rendering, extensions, and notifications, which can interfere with automated browsing. These are standard configurations when crawling in headless mode:\n\nclass\n\nBaseSeleniumCrawler\n\n(BaseCrawler, ABC):\n\ndef\n\n__init__\n\n(\n\nself, scroll_limit:\n\nint\n\n=\n\n5\n\n) ->\n\nNone\n\n: options = webdriver.ChromeOptions() options.add_argument(\n\n\"--no-sandbox\"",
      "content_length": 530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": ") options.add_argument(\n\n\"\n\n--headless=new\"\n\n) options.add_argument(\n\n\"--disable-dev-shm-usage\"\n\n) options.add_argument(\n\n\"--log-level=3\"\n\n) options.add_argument(\n\n\"--disable-popup-blocking\"\n\n) options.add_argument(\n\n\"--disable-notifications\"\n\n) options.add_argument(\n\n\"--disable-extensions\"\n\n) options.add_argument(\n\n\"--disable-background-networking\"\n\n) options.add_argument(\n\n\"--ignore-certificate-errors\"\n\n) options.add_argument(\n\nf\"--user-data-dir=\n\n{mkdtemp()}",
      "content_length": 465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "\"\n\n) options.add_argument(\n\nf\"--data-path=\n\n{mkdtemp()}\n\n\"\n\n) options.add_argument(\n\nf\"--disk-cache-dir=\n\n{mkdtemp()}\n\n\"\n\n) options.add_argument(\n\n\"--remote-debugging-port=9226\"\n\n)\n\nAfter configuring the Chrome options, the code allows subclasses to set any additional driver options by calling the\n\nset_extra_driver_options()\n\nmethod. It then initializes the scroll limit and creates a new instance of the Chrome driver with the specified options:",
      "content_length": 448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "self.set_extra_driver_options(options) self.scroll_limit = scroll_limit self.driver = webdriver.Chrome( options=options, )\n\nThe\n\nBaseSeleniumCrawler\n\nclass includes placeholder methods for\n\nset_extra_driver_options()\n\nand\n\nlogin()\n\n, which subclasses can override to provide specific functionality. This ensures modularity, as every platform has a different login page with a different HTML structure:\n\ndef\n\nset_extra_driver_options\n\n(\n\nself, options: Options\n\n) ->\n\nNone",
      "content_length": 471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": ":\n\npass\n\ndef\n\nlogin\n\n(\n\nself\n\n) ->\n\nNone\n\n:\n\npass\n\nFinally, the\n\nscroll_page()\n\nmethod implements a scrolling mechanism to navigate through pages, such as LinkedIn, up to a specified scroll limit. It scrolls to the bottom of the page, waits for new content to load, and repeats the process until it reaches the end of the page or the scroll limit is exceeded. This method is essential for feeds where the content appears as the user scrolls:\n\ndef",
      "content_length": 446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "scroll_page\n\n(\n\nself\n\n) ->\n\nNone\n\n:\n\n\"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"\n\ncurrent_scroll =\n\n0\n\nlast_height = self.driver.execute_script(\n\n\"return document.body.scrollHeight\"\n\n)\n\nwhile\n\nTrue\n\n: self.driver.execute_script(\n\n\"window.scrollTo(0, document.body.scrollHeight);\"\n\n) time.sleep(\n\n5\n\n) new_height = self.driver.execute_script(\n\n\"return document.body.scrollHeight\"",
      "content_length": 400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": ")\n\nif\n\nnew_height == last_height\n\nor\n\n(self.scroll_limit\n\nand\n\ncurrent_scroll >= self.scroll_limit):\n\nbreak\n\nlast_height = new_height current_scroll +=\n\n1\n\nWe’ve understood what the base classes of our crawlers look like. Next, we will look into the implementation of the following specific crawlers:\n\nGitHubCrawler(BaseCrawler)\n\nCustomArticleCrawler(BaseCrawler)\n\nMediumCrawler(BaseSeleniumCrawler)",
      "content_length": 399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "You can find the implementation of the above crawlers in the GitHub repository at https://github.com/PacktPublishing/LLM-Engineers- Handbook/tree/main /llm_engineering/application/crawlers.",
      "content_length": 189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "GitHubCrawler class\n\nThe\n\nGithubCrawler\n\nclass is designed to scrape GitHub repositories, extending the functionality of the\n\nBaseCrawler\n\n. We don’t have to log in to GitHub through the browser, as we can leverage Git’s clone functionality. Thus, we don’t have to leverage any Selenium functionality. Upon initialization, it sets up a list of patterns to ignore standard files and directories found in GitHub repositories, such as\n\n.git\n\n,\n\n.toml\n\n,\n\n.lock\n\n, and\n\n.png\n\n, ensuring that unnecessary files are excluded from the scraping process:",
      "content_length": 545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "class\n\nGithubCrawler\n\n(\n\nBaseCrawler\n\n): model = RepositoryDocument\n\ndef\n\n__init__\n\n(\n\nself, ignore=(\n\n\".git\"\n\n,\n\n\".toml\"\n\n,\n\n\".lock\"\n\n,\n\n\".png\"\n\n)\n\n) ->\n\nNone",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": ":\n\nsuper\n\n().__init__() self._ignore = ignore\n\nNext, we implement the\n\nextract()\n\nmethod, where the crawler first checks if the repository has already been processed and stored in the database. If it exists, it exits the method to prevent storing duplicates:\n\ndef\n\nextract\n\n(\n\nself, link:\n\nstr\n\n, **kwargs\n\n) ->\n\nNone\n\n: old_model = self.model.find(link=link)",
      "content_length": 359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "if\n\nold_model\n\nis\n\nnot\n\nNone\n\n: logger.info(\n\nf\"Repository already exists in the database:\n\n{link}\n\n\"\n\n)\n\nreturn\n\nIf the repository is new, the crawler extracts the repository name from the link. Then, it creates a temporary directory to clone the repository to ensure that the cloned repository is cleaned up from the local disk after it’s processed:\n\nlogger.info(\n\nf\"Starting scrapping GitHub repository:",
      "content_length": 406,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "{link}\n\n\"\n\n) repo_name = link.rstrip(\n\n\"/\"\n\n).split(\n\n\"/\"\n\n)[-\n\n1\n\n] local_temp = tempfile.mkdtemp()\n\nWithin a try block, the crawler changes the current working directory to the\n\ntemporary\n\ndirectory and executes the\n\ngit clone\n\ncommand in a different process:\n\ntry\n\n: os.chdir(local_temp) subprocess.run([",
      "content_length": 307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "\"git\"\n\n,\n\n\"clone\"\n\n, link])\n\nAfter successfully cloning the repository, the crawler constructs the path to the cloned repository. It initializes an empty dictionary used to aggregate the content of the files in a standardized way. It walks through the directory tree, skipping over any directories or files that match the ignore patterns. For each relevant file, it reads the content, removes any spaces, and stores it in the dictionary with the file path as the key:\n\nrepo_path = os.path.join(local_temp, os.listdir(local_temp)[\n\n0\n\n])\n\n#\n\ntree = {}\n\nfor\n\nroot, _, files\n\nin",
      "content_length": 575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "os.walk(repo_path):\n\ndir\n\n= root.replace(repo_path,\n\n\"\"\n\n).lstrip(\n\n\"/\"\n\n)\n\nif\n\ndir\n\n.startswith(self._ignore):\n\ncontinue\n\nfor\n\nfile\n\nin\n\nfiles:\n\nif\n\nfile.endswith(self._ignore):\n\ncontinue\n\nfile_path = os.path.join(\n\ndir",
      "content_length": 220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": ", file)\n\nwith\n\nopen\n\n(os.path.join(root, file),\n\n\"r\"\n\n, errors=\n\n\"ignore\"\n\n)\n\nas\n\nf: tree[file_path] = f.read().replace(\n\n\" \"\n\n,\n\n\"\"\n\n)\n\nIt then creates a new instance of the\n\nRepositoryDocument\n\nmodel, populating it with the repository content, name, link, platform information, and author details. The instance is then saved to MongoDB:",
      "content_length": 338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "user = kwargs[\n\n\"user\"\n\n] instance = self.model( content=tree, name=repo_name, link=link, platform=\n\n\"github\"\n\n, author_id=user.\n\nid\n\n, author_full_name=user.full_name, ) instance.save()\n\nFinally, whether the scraping succeeds or an exception occurs, the crawler ensures that the temporary directory is removed to clean up any resources used during the process:\n\nexcept\n\nException:\n\nraise\n\nfinally",
      "content_length": 397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": ": shutil.rmtree(local_temp) logger.info(\n\nf\"Finished scrapping GitHub repository:\n\n{link}\n\n\"\n\n)",
      "content_length": 95,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "CustomArticleCrawler class\n\nThe\n\nCustomArticleCrawler\n\nclass takes a different approach to collecting data from the internet. It leverages the\n\nAsyncHtmlLoader\n\nclass to read the entire HTML from a link and the\n\nHtml2TextTransformer\n\nclass to extract the text from that HTML. Both classes are made available by the\n\nlangchain_community\n\nPython package, as seen below, where we import all the necessary Python modules:\n\nfrom\n\nurllib.parse\n\nimport\n\nurlparse",
      "content_length": 455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "from\n\nlangchain_community.document_loaders\n\nimport\n\nAsyncHtmlLoader\n\nfrom\n\nlangchain_community.document_transformers.html2text\n\nimport\n\nHtml2TextTransformer\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nArticleDocument\n\nfrom\n\n.base\n\nimport\n\nBaseCrawler",
      "content_length": 285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "Next, we define the\n\nCustomArticleCrawler\n\nclass, which inherits from\n\nBaseCrawler\n\n. As before, we don’t need to log in or use the scrolling functionality provided by Selenium. In the\n\nextract\n\nmethod, we first check if the article exists in the database to avoid duplicating content:\n\nclass\n\nCustomArticleCrawler\n\n(\n\nBaseCrawler\n\n): model = ArticleDocument\n\ndef\n\nextract\n\n(",
      "content_length": 375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "self, link:\n\nstr\n\n, **kwargs\n\n) ->\n\nNone\n\n: old_model = self.model.find(link=link)\n\nif\n\nold_model\n\nis\n\nnot\n\nNone\n\n: logger.info(\n\nf\"Article already exists in the database:\n\n{link}\n\n\"\n\n)\n\nreturn\n\nIf the article doesn’t exist, we proceed to scrape it. We use the",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "AsyncHtmlLoader\n\nclass to load the HTML from the provided link. After, we transform it into plain text using the\n\nHtml2TextTransformer\n\nclass, which returns a list of documents. We are only interested in the first document. As we delegate the whole logic to these two classes, we don’t control how the content is extracted and parsed. That’s why we used this class as a fallback system for domains where we don’t have anything custom implemented. These two classes follow the LangChain paradigm, which provides high-level functionality that works decently in most scenarios. It is fast to implement but hard to customize. That is one of the reasons why many developers avoid using LangChain in production use cases:\n\nlogger.info(\n\nf\"Starting scrapping article:\n\n{link}\n\n\"\n\n) loader = AsyncHtmlLoader([link]) docs = loader.load() html2text = Html2TextTransformer() docs_transformed = html2text.transform_documents(docs) doc_transformed = docs_transformed[\n\n0\n\n]",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "We get the page content from the extracted document, plus relevant metadata such as the\n\ntitle\n\n,\n\nsubtitle\n\n,\n\ncontent\n\n, and\n\nlanguage\n\n:\n\ncontent = {\n\n\"Title\"\n\n: doc_transformed.metadata.get(\n\n\"title\"\n\n),\n\n\"Subtitle\"",
      "content_length": 219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": ": doc_transformed.metadata.get(\n\n\"\n\ndescription\"\n\n),\n\n\"Content\"\n\n: doc_transformed.page_content,\n\n\"language\"\n\n: doc_transformed.metadata.get(\n\n\"language\"\n\n), }\n\nNext, we parse the URL to determine the platform (or domain) from which the article was scraped:\n\nparsed_url = urlparse(link) platform = parsed_url.netloc\n\nWe then create a new instance of the article model, populating it with the extracted content. Finally, we save this instance to the MongoDB data",
      "content_length": 461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "warehouse:\n\nuser = kwargs[\n\n\"user\"\n\n] instance = self.model( content=content, link=link, platform=platform, author_id=user.\n\nid\n\n, author_full_name=user.full_name, ) instance.save() logger.info(\n\nf\"Finished scrapping custom article:\n\n{link}\n\n\"\n\n)\n\nSo far, we have seen how to crawl GitHub repositories and random sites using LangChain utility functions. Lastly, we must explore a crawler using Selenium to manipulate the browser programmatically. Thus, we will continue with the\n\nMediumCrawler\n\nimplementation.",
      "content_length": 510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "MediumCrawler class\n\nThe code begins by importing essential libraries and defining the\n\nMediumCrawler\n\nclass, which inherits from\n\nBaseSeleniumCrawler\n\n:\n\nfrom\n\nbs4\n\nimport\n\nBeautifulSoup\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\nllm_engineering.domain.documents",
      "content_length": 257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "import\n\nArticleDocument\n\nfrom\n\n.base\n\nimport\n\nBaseSeleniumCrawler\n\nclass\n\nMediumCrawler\n\n(\n\nBaseSeleniumCrawler\n\n): model = ArticleDocument\n\nWithin the\n\nMediumCrawler\n\nclass, we leverage the\n\nset_extra_driver_options()\n\nmethod to extend the default driver options used by Selenium:",
      "content_length": 281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "def\n\nset_extra_driver_options\n\n(\n\nself, options\n\n) ->\n\nNone\n\n: options.add_argument(\n\nr\"--profile-directory=Profile 2\"\n\n)\n\nThe\n\nextract()\n\nmethod implements the core functionality, first checking whether the article exists in the database to prevent duplicate entries.\n\nIf the article is new, the method proceeds to navigate to the article’s link and scroll through the page to ensure all content is loaded:\n\ndef",
      "content_length": 412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "extract\n\n(\n\nself, link:\n\nstr\n\n, **kwargs\n\n) ->\n\nNone\n\n: old_model = self.model.find(link=link)\n\nif\n\nold_model\n\nis\n\nnot\n\nNone\n\n: logger.info(\n\nf\"Article already exists in the database:\n\n{link}\n\n\"\n\n)\n\nreturn\n\nlogger.info(",
      "content_length": 219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "f\"Starting scrapping Medium article:\n\n{link}\n\n\"\n\n) self.driver.get(link) self.scroll_page()\n\nAfter fully loading the page, the method uses\n\nBeautifulSoup\n\nto parse the HTML content and extract the article’s title, subtitle, and full text.\n\nBeautifulSoup\n\nis a popular Python library for web scraping and parsing HTML or XML documents. Thus, we used it to extract all the HTML elements we needed from the HTML accessed with Selenium. Finally, we aggregate everything into a dictionary:\n\nsoup = BeautifulSoup(self.driver.page_source,\n\n\"html.parser\"\n\n) title = soup.find_all(\n\n\"h1\"\n\n, class_=",
      "content_length": 589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "\"pw-post-title\"\n\n) subtitle = soup.find_all(\n\n\"h2\"\n\n, class_=\n\n\"pw-subtitle-paragraph\"\n\n) data = {\n\n\"Title\"\n\n: title[\n\n0\n\n].string\n\nif\n\ntitle\n\nelse\n\nNone\n\n,\n\n\"Subtitle\"\n\n: subtitle[\n\n0\n\n].string\n\nif",
      "content_length": 198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "subtitle\n\nelse\n\nNone\n\n,\n\n\"Content\"\n\n: soup.get_text(), }\n\nFinally, the method closes the WebDriver to free up resources. It then creates a new\n\nArticleDocument\n\ninstance, populates it with the extracted content and user information provided via\n\nkwargs\n\n, and saves it to the database:\n\nself.driver.close() user = kwargs[\n\n\"user\"\n\n] instance = self.model( platform=\n\n\"medium\"",
      "content_length": 375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": ", content=data, link=link, author_id=user.\n\nid\n\n, author_full_name=user.full_name, ) instance.save() logger.info(\n\nf\"Successfully scraped and saved article:\n\n{link}\n\n\"\n\n)\n\nWith that, we conclude the\n\nMediumCrawler\n\nimplementation. The LinkedIn crawler follows a similar pattern to the Medium one, where it uses Selenium to log in and access the feed of a user’s latest posts. Then, it extracts the posts and scrolls through the feed to load the next page until a limit is hit. You can check the full implementation in our repository at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/crawlers/linkedin.py.\n\nWith the rise of LLMs, collecting data from the internet has become a critical step in many real-world AI applications. Hence, more high-level tools have appeared in the Python ecosystem, such as Scrapy (https://github.com/scrapy/scrapy), which crawls websites and extracts structured data from their pages, and Crawl4AI (https://github.com/unclecode/crawl4ai), which is highly specialized in crawling data for LLMs and AI applications.",
      "content_length": 1092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "In this section, we’ve looked at implementing three types of crawlers: one that leverages the\n\ngit\n\nexecutable in a subprocess to clone GitHub repositories, one that uses LangChain utilities to extract the HTML of a single web page, and one that leverages Selenium for more complex scenarios where we have to navigate through the login page, scroll the article to load the entire HTML, and extract it into text format. The last step is understanding how the document classes we’ve used across the chapter, such as the\n\nArticleDocument\n\n, work.",
      "content_length": 543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "The NoSQL data warehouse documents\n\nWe had to implement three document classes to structure our data categories. These classes define the specific attributes we require for a document, such as the content, author, and source link. It is best practice to structure your data in classes instead of dictionaries, as the attributes we expect for each item are more verbose, reducing run errors. For example, when accessing a value from a Python dictionary, we can never be sure it is present or its type is current. By wrapping our data items with classes, we can ensure each attribute is as expected.\n\nBy leveraging Python packages such as Pydantic, we have out-of-the-box type validation, which ensures consistency in our datasets. Thus, we modeled the data categories as the following document classes, which we already used in the code up until point:\n\nArticleDocument\n\nclass\n\nPostDocument\n\nclass\n\nRepositoryDocument",
      "content_length": 916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "class\n\nThese are not simple Python data classes or Pydantic models. They support read and write operations on top of the MongoDB data warehouse. To inject the read-and-write functionality into all the document classes without repeating any code, we used the Object- Document Mapping (ODM) software pattern, which is based on the object-relational mapping (ORM) pattern. Thus, let’s first explore ORM, then move to ODM, and, finally, dig into our custom ODM implementation and document classes.",
      "content_length": 493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "The ORM and ODM software patterns\n\nBefore we talk about software patterns, let’s see what ORM is. It’s a technique that lets you query and manipulate data from a database using an object-oriented paradigm. Instead of writing SQL or API-specific queries, you encapsulate all the complexity under an ORM class that knows how to handle all the database operations, most commonly CRUD operations. Thus, working with ORM removes the need to handle the database operations manually and reduces the need to write boilerplate code manually. An ORM interacts with a SQL database, such as PostgreSQL or MySQL.\n\nMost modern Python applications use ORMs when interacting with the database. Even though SQL is still a popular choice in the data world, you rarely see raw SQL queries in Python backend components. The most popular Python ORM is SQLAlchemy (https://www.sqlalchemy.org/). Also, with the rise of FastAPI, SQLModel is (https://github.com/fastapi/sqlmodel) a common choice, which is a wrapper over SQLAlchemy that makes the integration easier with FastAPI.\n\nFor example, using SQLAlchemy, we defined a\n\nUser\n\nORM with the ID and name fields. The\n\nUser\n\nORM is mapped to the",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "users\n\ntable within the SQL database. Thus, when we create a new user and commit it to the database, it is automatically saved to the\n\nusers\n\ntable. The same applies to all the CRUD operations on top of the\n\nUser\n\nclass.\n\nfrom\n\nsqlalchemy\n\nimport\n\nColumn, Integer, String, create_engine\n\nfrom\n\nsqlalchemy.orm\n\nimport\n\ndeclarative_base, sessionmaker Base = declarative_base()\n\n# Define a class that maps to the users table.\n\nclass\n\nUser",
      "content_length": 435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "(\n\nBase\n\n): __tablename__ =\n\n\"users\"\n\nid\n\n= Column(Integer, primary_key=\n\nTrue\n\n) name = Column(String)\n\nUsing the\n\nUser\n\nORM, we can quickly insert or query users directly from Python without writing a line of SQL. Note that an ORM usually supports all CRUD operations. Here is a code snippet that shows how to save an instance of the User ORM to a SQLite database:\n\nengine = create_engine(\n\n\"sqlite:///:memory:\"\n\n) Base.metadata.create_all(engine)",
      "content_length": 449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "# Create a session used to interact with the database.\n\nSession = sessionmaker(bind=engine) session = Session()\n\n# Add a new user.\n\nnew_user = User(name=\n\n\"Alice\"\n\n) session.add(new_user) session.commit()\n\nAlso, this is how we can query a user from the\n\nusers\n\nSQLite table:\n\nuser = session.query(User).first()\n\nif\n\nuser:\n\nprint\n\n(\n\nf\"User ID:\n\n{user.",
      "content_length": 351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "id\n\n}\n\n\"\n\n)\n\nprint\n\n(\n\nf\"User name:\n\n{user.name}\n\n\"\n\n)\n\nFind the entire script and how to run it in the GitHub repository at\n\ncode_snippets/03_orm.py\n\n.\n\nThe ODM pattern is extremely similar to ORM, but instead of working with SQL databases and tables, it works with NoSQL databases (such as MongoDB) and unstructured collections. As we work with NoSQL databases, the data structure is centered on collections, which store JSON- like documents rather than rows in tables.",
      "content_length": 471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "To conclude, ODM simplifies working with document-based NoSQL databases and maps object-oriented code to JSON-like documents. We will implement a light ODM module on top of MongoDB to fully understand how ODM works.",
      "content_length": 215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "Implementing the ODM class\n\nThis section will explore how to implement an ODM class from scratch. This is an excellent exercise to learn how ODM works and sharpen our skills in writing modular and reusable Python classes. Hence, we will implement a base ODM class called\n\nNoSQLBaseDocument\n\n, from which all the other documents will inherit to interact with the MongoDB data warehouse.\n\nThe class can be found in our repository at\n\nllm_engineering/domain/base/nosql.py\n\n.\n\nThe code starts by importing essential modules and setting up the database connection. Through the\n\n_database\n\nvariable, we establish a connection to the database specified in the settings, which is by default called\n\ntwin",
      "content_length": 695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": ":\n\nimport\n\nuuid\n\nfrom\n\nabc\n\nimport\n\nABC\n\nfrom\n\ntyping\n\nimport\n\nGeneric\n\n,\n\nType\n\n, TypeVar\n\nfrom\n\nloguru\n\nimport\n\nlogger",
      "content_length": 120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "from\n\npydantic\n\nimport\n\nUUID4, BaseModel, Field\n\nfrom\n\npymongo\n\nimport\n\nerrors\n\nfrom\n\nllm_engineering.domain.exceptions\n\nimport\n\nImproperlyConfigured\n\nfrom\n\nllm_engineering.infrastructure.db.mongo\n\nimport\n\nconnection\n\nfrom\n\nllm_engineering.settings\n\nimport\n\nsettings _database = connection.get_database(settings.DATABASE_NAME)",
      "content_length": 326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "Next, we define a type variable\n\nT\n\nbound to the\n\nNoSQLBaseDocument\n\nclass. The variable leverages Python’s generic module, allowing us to generalize the class’s types. For example, when we implement the\n\nArticleDocument\n\nclass, which will inherit from the\n\nNoSQLBaseDocument\n\nclass, all the instances where\n\nT\n\nwas used will be replaced with the\n\nArticleDocument\n\ntype when analyzing the signature of functions (more on Python generics: https://realpython.com/python312-typing).\n\nThe\n\nNoSQLBaseDocument",
      "content_length": 503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "class is then declared as an abstract base class inheriting from Pydantic’s BaseModel, Python’s Generic (which provides the functionality described earlier), and\n\nABC\n\n(making the class abstract) classes. This class serves as the foundational ODM class:\n\nT = TypeVar(\n\n\"T\"\n\n, bound=\n\n\"NoSQLBaseDocument\"\n\n)\n\nclass\n\nNoSQLBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC):\n\nWithin the",
      "content_length": 378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "NoSQLBaseDocument\n\nclass, an id field is defined as a UUID4, with a default factory generating a unique UUID. The class also implements the\n\n__eq__\n\nand\n\n__hash__\n\nmethods to allow instances to be compared and used in hashed collections like sets or as dictionary keys based on their unique\n\nid\n\nattribute:\n\nid\n\n: UUID4 = Field(default_factory=uuid.uuid4)\n\ndef\n\n__eq__\n\n(\n\nself, value:\n\nobject\n\n) ->\n\nbool",
      "content_length": 405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": ":\n\nif\n\nnot\n\nisinstance\n\n(value, self.__class__):\n\nreturn\n\nFalse\n\nreturn\n\nself.\n\nid\n\n== value.\n\nid\n\ndef\n\n__hash__\n\n(\n\nself\n\n) ->\n\nint\n\n:\n\nreturn",
      "content_length": 143,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "hash\n\n(self.\n\nid\n\n)\n\nThe class provides methods for converting between MongoDB documents and class instances. The\n\nfrom_mongo()\n\nclass method transforms a dictionary retrieved from MongoDB into an instance of the class. The\n\nto_mongo()\n\ninstance method converts the model instance into a dictionary suitable for MongoDB insertion:\n\n@classmethod\n\ndef\n\nfrom_mongo\n\n(\n\ncls:",
      "content_length": 370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "Type\n\n[T], data:\n\ndict\n\n) -> T:\n\nif\n\nnot\n\ndata:\n\nraise\n\nValueError(\n\n\"Data is empty.\"\n\n)\n\nid\n\n= data.pop(\n\n\"_id\"\n\n)\n\nreturn\n\ncls(**\n\ndict\n\n(data,\n\nid",
      "content_length": 149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "=\n\nid\n\n))\n\ndef\n\nto_mongo\n\n(\n\nself: T, **kwargs\n\n) ->\n\ndict\n\n: exclude_unset = kwargs.pop(\n\n\"exclude_unset\"\n\n,\n\nFalse\n\n) by_alias = kwargs.pop(\n\n\"by_alias\"\n\n,\n\nTrue\n\n) parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)\n\nif\n\n\"_id\"",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "not\n\nin\n\nparsed\n\nand\n\n\"id\"\n\nin\n\nparsed: parsed[\n\n\"_id\"\n\n] =\n\nstr\n\n(parsed.pop(\n\n\"id\"\n\n))\n\nfor\n\nkey, value\n\nin\n\nparsed.items():\n\nif\n\nisinstance\n\n(value, uuid.UUID): parsed[key] =",
      "content_length": 177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "str\n\n(value)\n\nreturn\n\nparsed\n\nThe\n\nsave()\n\nmethod allows an instance of the model to be inserted into a MongoDB collection. It retrieves the appropriate collection, converts the instance into a MongoDB-compatible document leveraging the\n\nto_mongo()\n\nmethod described above, and attempts to insert it into the database, handling any write errors that may occur:\n\ndef\n\nsave\n\n(\n\nself: T, **kwargs\n\n) -> T |",
      "content_length": 403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "None\n\n: collection = _database[self.get_collection_name()]\n\ntry\n\n: collection.insert_one(self.to_mongo(**kwargs))\n\nreturn\n\nself\n\nexcept\n\nerrors.WriteError: logger.exception(\n\n\"\n\nFailed to insert document.\"\n\n)\n\nreturn\n\nNone\n\nThe\n\nget_or_create()\n\nclass method attempts to find a document in the database matching the provided filter options. If a matching document is found, it is converted into an instance of the class. If not, a new instance is created with the filter options as its initial data and saved to the database:",
      "content_length": 525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "@classmethod\n\ndef\n\nget_or_create\n\n(\n\ncls:\n\nType\n\n[T], **filter_options\n\n) -> T: collection = _database[cls.get_collection_name()]\n\ntry\n\n: instance = collection.find_one(filter_options)\n\nif\n\ninstance:\n\nreturn\n\ncls.from_mongo(instance) new_instance = cls(**filter_options) new_instance = new_instance.save()\n\nreturn\n\nnew_instance\n\nexcept",
      "content_length": 335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "errors.OperationFailure: logger.exception(\n\nf\"Failed to retrieve document with filter options:\n\n{filter_options}\n\n\"\n\n)\n\nraise\n\nThe\n\nbulk_insert()\n\nclass method allows multiple documents to be inserted into the database at once:\n\n@classmethod\n\ndef\n\nbulk_insert\n\n(\n\ncls:\n\nType",
      "content_length": 274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "[T], documents:\n\nlist\n\n[T], **kwargs\n\n) ->\n\nbool\n\n: collection = _database[cls.get_collection_name()]\n\ntry\n\n: collection.insert_many([doc.to_mongo(**kwargs)\n\nfor\n\ndoc\n\nin\n\ndocuments])\n\nreturn\n\nTrue\n\nexcept\n\n(errors.WriteError, errors.BulkWriteError): logger.error(\n\nf\"Failed to insert documents of type\n\n{cls.__name__}\n\n\"\n\n)",
      "content_length": 324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "return\n\nFalse\n\nThe\n\nfind()\n\nclass method searches for a single document in the database that matches the given filter options:\n\n@classmethod\n\ndef\n\nfind\n\n(\n\ncls:\n\nType\n\n[T], **filter_options\n\n) -> T |\n\nNone\n\n: collection = _database[cls.get_collection_name()]",
      "content_length": 258,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "try\n\n: instance = collection.find_one(filter_options)\n\nif\n\ninstance:\n\nreturn\n\ncls.from_mongo(instance)\n\nreturn\n\nNone\n\nexcept\n\nerrors.OperationFailure: logger.error(\n\n\"Failed to retrieve document.\"\n\n)\n\nreturn\n\nNone\n\nSimilarly, the\n\nbulk_find()\n\nclass method retrieves multiple documents matching the filter options. It converts each retrieved MongoDB document into a model instance, collecting them into a list:",
      "content_length": 410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "@classmethod\n\ndef\n\nbulk_find\n\n(\n\ncls:\n\nType\n\n[T], **filter_options\n\n) ->\n\nlist\n\n[T]: collection = _database[cls.get_collection_name()]\n\ntry\n\n: instances = collection.find(filter_options)\n\nreturn\n\n[document\n\nfor\n\ninstance\n\nin\n\ninstances",
      "content_length": 235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "if\n\n(document := cls.from_mongo(instance))\n\nis\n\nnot\n\nNone\n\n]\n\nexcept\n\nerrors.OperationFailure: logger.error(\n\n\"Failed to retrieve document.\"\n\n)\n\nreturn\n\n[]\n\nFinally, the\n\nget_collection_name()\n\nclass method determines the name of the MongoDB collection associated with the class. It expects the class to have a nested\n\nSettings\n\nclass with a name attribute specifying the collection name. If this configuration is missing, an",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "ImproperlyConfigured\n\nexception will be raised specifying that the subclass should define a nested\n\nSettings\n\nclass:\n\n@classmethod\n\ndef\n\nget_collection_name\n\n(\n\ncls:\n\nType\n\n[T]\n\n) ->\n\nstr\n\n:\n\nif\n\nnot\n\nhasattr\n\n(cls,",
      "content_length": 215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "\"Settings\"\n\n)\n\nor\n\nnot\n\nhasattr\n\n(cls.Settings,\n\n\"name\"\n\n):\n\nraise\n\nImproperlyConfigured(\n\n\"Document should define an Settings configuration class with the name of the collection.\"\n\n)\n\nreturn\n\ncls.Settings.name\n\nWe can configure each subclass using the nested\n\nSettings\n\nclass, such as defining the collection name, or anything else specific to that subclass. Within the Python ecosystem, there is an ODM implementation on",
      "content_length": 422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "top of MongoDB, called\n\nmongoengine\n\n, which you can find on GitHub. It follows a pattern similar to ours but more comprehensive. We implemented it by ourselves, as it was an excellent exercise to practice writing modular and generic code following best OOP principles, which are essential for implementing production-level code.",
      "content_length": 329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "Data categories and user document classes\n\nThe last piece of the puzzle is to see the implementation of the subclasses that inherit from the\n\nNoSQLBaseDocument\n\nbase class. These are the concrete classes that define our data categories. You’ve seen these classes used across the chapter when working with articles, repositories, and posts within the crawler classes.\n\nWe begin by importing the essential Python modules and the ODM base class:\n\nfrom\n\nabc\n\nimport\n\nABC\n\nfrom\n\ntyping\n\nimport",
      "content_length": 488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "Optional\n\nfrom\n\npydantic\n\nimport\n\nUUID4, Field\n\nfrom\n\n.base\n\nimport\n\nNoSQLBaseDocument\n\nfrom\n\n.types\n\nimport\n\nDataCategory\n\nWe define an\n\nenum\n\nclass, where we centralize all our data category types. These variables will act as constants in configuring all our ODM classes throughout the book.",
      "content_length": 293,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "The class can be found in the repository at\n\nllm_engineering/domain/types.py\n\n.\n\nfrom\n\nenum\n\nimport\n\nStrEnum\n\nclass\n\nDataCategory\n\n(\n\nStrEnum\n\n): PROMPT =\n\n\"prompt\"\n\nQUERIES =\n\n\"queries\"\n\nINSTRUCT_DATASET_SAMPLES =\n\n\"instruct_dataset_samples\"",
      "content_length": 242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "INSTRUCT_DATASET =\n\n\"instruct_dataset\"\n\nPREFERENCE_DATASET_SAMPLES =\n\n\"preference_dataset_samples\"\n\nPREFERENCE_DATASET =\n\n\"\n\npreference_dataset\"\n\nPOSTS =\n\n\"posts\"\n\nARTICLES =\n\n\"articles\"\n\nREPOSITORIES =\n\n\"repositories\"\n\nThe\n\nDocument\n\nclass is introduced as an abstract base model for other documents on top of the\n\nNoSQLBaseDocument",
      "content_length": 333,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "ODM class. It includes common attributes like content, platform, and author details, providing a standardized structure for documents that will inherit from it:\n\nclass\n\nDocument\n\n(NoSQLBaseDocument, ABC): content:\n\ndict\n\nplatform:\n\nstr\n\nauthor_id: UUID4 = Field(alias=\n\n\"author_id\"\n\n) author_full_name:\n\nstr\n\n= Field(alias=\n\n\"author_full_name\"\n\n)\n\nFinally, specific document types are defined by extending the",
      "content_length": 409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "Document\n\nclass. The\n\nRepositoryDocument\n\n,\n\nPostDocument\n\n, and\n\nArticleDocument\n\nclasses represent different categories of data, each with unique fields and settings that specify their respective collection names in the database:\n\nclass\n\nRepositoryDocument\n\n(\n\nDocument\n\n): name:\n\nstr\n\nlink:\n\nstr\n\nclass",
      "content_length": 305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "Settings\n\n: name = DataCategory.REPOSITORIES\n\nclass\n\nPostDocument\n\n(\n\nDocument\n\n): image:\n\nOptional\n\n[\n\nstr\n\n] =\n\nNone\n\nlink:\n\nstr\n\n|\n\nNone\n\n=\n\nNone\n\nclass\n\nSettings",
      "content_length": 165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": ": name = DataCategory.POSTS\n\nclass\n\nArticleDocument\n\n(\n\nDocument\n\n): link:\n\nstr\n\nclass\n\nSettings\n\n: name = DataCategory.ARTICLES\n\nFinally, we define the\n\nUserDocument\n\nclass, which is used to store and query all the users from the LLM Twin project:\n\nclass\n\nUserDocument",
      "content_length": 269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "(\n\nNoSQLBaseDocument\n\n): first_name:\n\nstr\n\nlast_name:\n\nstr\n\nclass\n\nSettings\n\n: name =\n\n\"users\"\n\n@property\n\ndef\n\nfull_name\n\n(\n\nself\n\n):\n\nreturn\n\nf\"\n\n{self.first_name}\n\n{self.last_name}",
      "content_length": 183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "\"\n\nBy implementing the\n\nNoSQLBaseDocument\n\nODM class, we had to focus solely on the fields and specific functionality of each document or domain entity. All the CRUD functionality is delegated to the parent class. Also, by leveraging Pydantic to define the fields, we have out-of-the-box type validation. For example, when creating an instance of the\n\nArticleDocument\n\nclass, if the provided link is\n\nNone\n\nor not a string, it will throw an error signaling that the data is invalid.\n\nWith that, we’ve finished implementing our data collection pipeline, starting with the ZenML components. Then, we looked into the implementation of the crawlers and, finally, wrapped it up with the ODM class and data category documents. The last step is to run the data collection pipeline and ingest raw data into the MongoDB data warehouse.",
      "content_length": 826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "Gathering raw data into the data warehouse\n\nZenML orchestrates the data collection pipeline. Thus, leveraging ZenML, the data collection pipeline can be run manually, scheduled, or triggered by specific events. Here, we will show you how to run it manually, while we will discuss the other scenarios in Chapter 11 when digging deeper into MLOps.\n\nWe configured a different pipeline run for each author. We provided a ZenML configuration file for Paul Iusztin’s or Maxime Labonne’s data. To call the data collection pipeline to collect Maxime’s data, for example, you can run the following CLI command:\n\npoetry poe run-digital-data-etl-maxime\n\nThat will call the pipeline with the following ZenML YAML configuration file:\n\nparameters:",
      "content_length": 733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "user_full_name:\n\nMaxime\n\nLabonne\n\n# [First Name(s)] [Last Name]\n\nlinks:\n\n# Personal Blog\n\n\n\nhttps://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html\n\n\n\nhttps://mlabonne.github.io/blog/posts/2024-07- 15_The_Rise_of_Agentic_Data_Generation.html\n\n# Substack\n\n\n\nhttps://maximelabonne.substack.com/p/uncensor-any-llm-with-abliteration- d30148b7d43e\n\n\n\nhttps://maximelabonne.substack.com/p/create-mixtures-of-experts-with- mergekit-11b318c99562\n\n\n\nhttps://maximelabonne.substack.com/p/merge-large-language-models-with- mergekit-2118fb392b54\n\n…",
      "content_length": 551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "# More Substack links\n\nIn Figure 3.3 earlier, we saw the pipeline’s run DAG and details in ZenML’s dashboard. Meanwhile, Figure 3.5 shows the\n\nuser\n\noutput artifact generated by this data collection pipeline. You can inspect the query\n\nuser_full_name\n\nand the retrieved\n\nuser\n\nfrom the MongoDB database, for which we collected the links in this specific run.",
      "content_length": 358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "Figure 3.5: Example of the user output artifact after running the data collection pipeline using Maxime’s configuration file\n\nAlso, in Figure 3.6, you can observe the\n\ncrawled_links",
      "content_length": 181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "output artifact, which lists all the domains from which we collected data, the total number of links crawled for each domain, and the number of successfully collected links.\n\nWe want to highlight again the power of these artifacts, as they trace each pipeline’s results and metadata, making it extremely easy to monitor and debug each pipeline run individually.",
      "content_length": 361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "Figure 3.6: Example of the crawled_links output artifact after running the data collection pipeline using Maxime’s configuration file",
      "content_length": 133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "Now, we can download the\n\ncrawled_links\n\nartifact anywhere in our code by running the following code, where the\n\nID\n\nof the artifact can be found in ZenML and is unique for every artifact version:\n\nfrom\n\nzenml.client\n\nimport\n\nClient artifact = Client().get_artifact_version(\n\n'8349ce09-0693-4e28-8fa2-20f82c76ddec'\n\n) loaded_artifact = artifact.load()\n\nFor example, we can easily run the same data collection pipeline but with Paul Iusztin’s YAML configuration, listed below:\n\nparameters:",
      "content_length": 488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "user_full_name:\n\nPaul\n\nIusztin\n\n# [First Name(s)] [Last Name]\n\nlinks:\n\n# Medium\n\n\n\nhttps://medium.com/decodingml/an-end-to-end-framework-for-production- ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f\n\n\n\nhttps://medium.com/decodingml/a-real-time-retrieval-system-for-rag-on- social-media-data-9cc01d50a2a0\n\n\n\nhttps://medium.com/decodingml/sota-python-streaming-pipelines-for-fine- tuning-llms-and-rag-in-real-time-82eb07795b87\n\n…\n\n# More Medium links\n\n# Substack\n\n\n\nhttps://decodingml.substack.com/p/real-time-feature-pipelines-with? r=1ttoeh\n\n",
      "content_length": 553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "https://decodingml.substack.com/p/building-ml-systems-the-right-way? r=1ttoeh\n\n\n\nhttps://decodingml.substack.com/p/reduce-your-pytorchs-code-latency? r=1ttoeh\n\n…\n\n# More Substack links\n\nTo run the pipeline using Paul’s configuration, we call the following\n\npoe\n\ncommand:\n\npoetry poe run-digital-data-etl-paul\n\nThat, under the hood, calls the following CLI command that references Paul’s config file:",
      "content_length": 399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "poetry run python -m tools.run --run-etl --no-cache --etl-config-filename digital_data_etl_paul_iusztin.yaml\n\nYou can find all the configs in the repository in the\n\nconfigs/\n\ndirectory. Also, using\n\npoe\n\n, we configured a command that calls the data collection pipeline for all the supported authors:\n\npoetry poe run-digital-data-etl\n\nWe can easily query the MongoDB data warehouse using our ODM classes. For example, let’s query all the articles collected for Paul Iusztin:\n\nfrom\n\nllm_engineering.domain.documents",
      "content_length": 514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "import\n\nArticleDocument, UserDocument user = UserDocument.get_or_create(first_name=\n\n\"Paul\"\n\n, last_name=\n\n\"Iusztin\"\n\n) articles = ArticleDocument.bulk_find(author_id=\n\nstr\n\n(user.\n\nid\n\n))\n\nprint\n\n(\n\nf\"User ID:\n\n{user.\n\nid\n\n}\n\n\"\n\n)\n\nprint\n\n(",
      "content_length": 241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "f\"User name:\n\n{user.first_name}\n\n{user.last_name}\n\n\"\n\n)\n\nprint\n\n(\n\nf\"Number of articles:\n\n{\n\nlen\n\n(articles)}\n\n\"\n\n)\n\nprint\n\n(\n\n\"First article link:\"\n\n, articles[\n\n0\n\n].link)",
      "content_length": 173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "The output of the code from above is:\n\nUser ID: 900fec95-d621-4315-84c6-52e5229e0b96 User name: Paul Iusztin Number of articles: 50 First article link: https://medium.com/decodingml/an-end-to-end-framework-for-production- ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f\n\nWith only two lines of code, we can query and filter our MongoDB data warehouse using any ODM defined within our project.\n\nAlso, to ensure that your data collection pipeline works as expected, you can search your MongoDB collections using your IDE’s MongoDB plugin, which you must install separately. For example, you can use this plugin for VSCode: https://www.mongodb.com/products/tools/vs-code. For other IDEs, you can use similar plugins or external NoSQL visualization tools. After connecting to the MongoDB visualization tool, you can connect to our local database using the following URI:\n\nmongodb://llm_engineering:llm_engineering@127.0.0.1:27017\n\n. For a cloud MongoDB cluster, you must change the URI, which we will explore in Chapter 11.\n\nAnd just like that, you’ve learned how to run the data collection pipeline with different ZenML configs and how to visualize the output artifacts of",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "each run. We also looked at how to query the data warehouse for a particular data category and author. Thus, we’ve finalized our data engineering chapter and can move to the conclusion.",
      "content_length": 185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "Troubleshooting\n\nThe raw data stored in the MongoDB database is central to all future steps. Thus, if you haven’t successfully run the code from this chapter due to any issues with the crawlers, this section provides solutions for fixing potential issues to allow you to move forward.",
      "content_length": 284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "Selenium issues\n\nIt is a well-known issue that running Selenium can cause problems due to issues with the browser driver, such as the\n\nChromeDriver\n\n. Thus, if the crawlers that use Selenium, such as the\n\nMediumCrawler\n\n, fail due to problems with your\n\nChromeDriver\n\n, you can easily bypass this by commenting out the Medium links added to the data collection YAML configs. To do so, go to the\n\nconfigs/\n\ndirectory and find all the YAML files that start with\n\ndigital_data_etl_*\n\n, such as\n\ndigital_data_etl_maxime_labonne.yaml\n\n. Open them and comment on all the Medium-related URLs, as illustrated in Figure 3.7. You can leave out the Substack or personal blog URLs as these use the\n\nCustomArticleCrawler",
      "content_length": 707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": ", which is not dependent on Selenium.\n\nFigure 3.7: Fix Selenium issues when crawling raw data",
      "content_length": 93,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "Import our backed-up data\n\nIf nothing works, there is the possibility of populating the MongoDB database with your backed-up data saved under the\n\ndata/data_warehouse_raw_data directory\n\n. This will allow you to proceed to the fine-tuning and inference sections without running the data collection ETL code. To import all the data within this directory, run:\n\npoetry poe run-import-data-warehouse-from-json\n\nAfter running the CLI command from above, you will have a one-to-one replica of the dataset we used while developing the code. To ensure the import is completed successfully, you should have 88 articles and 3 users in your MongoDB database.",
      "content_length": 648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "Summary\n\nIn this chapter, we’ve learned how to design and build the data collection pipeline for the LLM Twin use case. Instead of relying on static datasets, we collected our custom data to mimic real-world situations, preparing us for real-world challenges in building AI systems.\n\nFirst, we examined the architecture of LLM Twin’s data collection pipeline, which functions as an ETL process. Next, we started digging into the pipeline implementation. We began by understanding how we can orchestrate the pipeline using ZenML. Then, we looked into the crawler implementation. We learned how to crawl data in three ways: using CLI commands in subprocesses or using utility functions from LangChain or Selenium to build custom logic that programmatically manipulates the browser. Finally, we looked into how to build our own ODM class, which we used to define our document class hierarchy, which contains entities such as articles, posts, and repositories.\n\nAt the end of the chapter, we learned how to run ZenML pipelines with different YAML configuration files and explore the results in the dashboard. We also saw how to interact with the MongoDB data warehouse through the ODM classes.\n\nIn the next chapter, we will cover the key steps of the RAG feature pipeline, including chunking and embedding documents, ingesting these documents into a vector DB, and applying pre-retrieval optimizations to improve performance. We will also set up the necessary infrastructure",
      "content_length": 1470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "programmatically using Pulumi and conclude by deploying the RAG ingestion pipeline to AWS.",
      "content_length": 90,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "References\n\nBreuss, M. (2023, July 26). Beautiful Soup: Build a Web Scraper With Python. https://realpython.com/beautiful-soup-web-scraper-python/\n\nDavid, D. (2024, July 8). Guide to Web Scraping with Selenium in 2024. Bright Data. https://brightdata.com/blog/how-tos/using-selenium-for-web- scraping\n\nHjelle, G. A. (2023, October 21). Python 3.12 Preview: Static Typing Improvements. https://realpython.com/python312-typing/\n\nORM Quick Start — SQLAlchemy 2.0 documentation. (n.d.). https://docs.sqlalchemy.org/en/20/orm/quickstart.html\n\nRamos, L. P. (2023, August 4). Python and MongoDB: Connecting to NoSQL Databases. https://realpython.com/introduction-to-mongodb-and- python/\n\nRefactoring.Guru. (2024, January 1). Builder. https://refactoring.guru/design-patterns/builder",
      "content_length": 775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "What is ETL? A complete guide. (n.d.). Qlik. https://www.qlik.com/us/etl\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "4",
      "content_length": 1,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "RAG Feature Pipeline\n\nRetrieval-augmented generation (RAG) is fundamental in most generative AI applications. RAG’s core responsibility is to inject custom data into the large language model (LLM) to perform a given action (e.g., summarize, reformulate, and extract the injected data). You often want to use the LLM on data it wasn’t trained on (e.g., private or new data). As fine-tuning an LLM is a highly costly operation, RAG is a compelling strategy that bypasses the need for constant fine-tuning to access that new data.\n\nWe will start this chapter with a theoretical part that focuses on the fundamentals of RAG and how it works. We will then walk you through all the components of a naïve RAG system: chunking, embedding, and vector DBs. Ultimately, we will present various optimizations used for an advanced RAG system. Then, we will continue exploring LLM Twin’s RAG feature pipeline architecture. At this step, we will apply all the theoretical aspects we discussed at the beginning of the chapter. Finally, we will go through a practical example by implementing the LLM Twin’s RAG feature pipeline based on the system design described throughout the book.\n\nThe main sections of this chapter are:\n\nUnderstanding RAG\n\nAn overview of advanced RAG",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "Exploring the LLM Twin’s RAG feature pipeline architecture\n\nImplementing the LLM Twin’s RAG feature pipeline\n\nBy the end of this chapter, you will have a clear and comprehensive understanding of what RAG is and how it is applied to our LLM Twin use case.",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "Understanding RAG\n\nRAG enhances the accuracy and reliability of generative AI models with information fetched from external sources. It is a technique complementary to the internal knowledge of the LLMs. Before going into the details, let’s understand what RAG stands for:\n\nRetrieval: Search for relevant data\n\nAugmented: Add the data as context to the prompt\n\nGeneration: Use the augmented prompt with an LLM for generation\n\nAny LLM is bound to understand the data it was trained on, sometimes called parameterized knowledge. Thus, even if the LLM can perfectly answer what happened in the past, it won’t have access to the newest data or any other external sources on which it wasn’t trained.\n\nLet’s take the most powerful model from OpenAI as an example, which, in the summer of 2024, is GPT-4o. The model is trained on data up to October 2023. Thus, if we ask what happened during the 2020 pandemic, it can be answered perfectly due to its parametrized knowledge. However, it will not know the answer if we ask about the 2024 European Football",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "Championship results due to its bounded parametrized knowledge. Another scenario is that it will start confidently hallucinating and provide a faulty answer.\n\nRAG overcomes these two limitations of LLMs. It provides access to external or latest data and prevents hallucinations, enhancing generative AI models’ accuracy and reliability.",
      "content_length": 336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "Why use RAG?\n\nWe briefly explained the importance of using RAG in generative AI applications earlier. Now, we will dig deeper into the “why,” following which we will focus on what a naïve RAG framework looks like.\n\nFor now, to get an intuition about RAG, you have to know that when using RAG, we inject the necessary information into the prompt to answer the initial user question. After that, we pass the augmented prompt to the LLM for the final answer. Now, the LLM will use the additional context to answer the user question.\n\nThere are two fundamental problems that RAG solves:\n\nHallucinations\n\nOld or private information",
      "content_length": 626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "Hallucinations\n\nIf a chatbot without RAG is asked a question about something it wasn’t trained on, there is a high chance that it will give you a confident answer about something that isn’t true. Let’s take the 2024 European Football Championship as an example. If the model is trained up to October 2023 and we ask it something about the tournament, it will most likely come up with a random answer that is hard to differentiate between reality and truth. Even if the LLM doesn’t hallucinate all the time, it raises concerns about the trustworthiness of its answers. Thus, we must ask ourselves: “When can we trust the LLM’s answers?” and “How can we evaluate if the answers are correct?”.\n\nBy introducing RAG, we enforce the LLM to always answer solely based on the introduced context. The LLM will act as the reasoning engine, while the additional information added through RAG will act as the single source of truth for the generated answer. By doing so, we can quickly evaluate if the LLM’s answer is based on the external data or not.",
      "content_length": 1040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "Old information\n\nAny LLM is trained or fine-tuned on a subset of the total world knowledge dataset. This is due to three main issues:\n\nPrivate data: You cannot train your model on data you don’t own or have the right to use.\n\nNew data: New data is generated every second. Thus, you would have to constantly train your LLM to keep up.\n\nCosts: Training or fine-tuning an LLM is an extremely costly operation. Hence, it is not feasible to do it on an hourly or daily basis.\n\nRAG solves these issues, as you no longer have to constantly fine-tune your LLM on new data (or even private data). Directly injecting the necessary data to respond to user questions into the prompts that are fed to the LLM is enough to generate correct and valuable answers.\n\nTo conclude, RAG is key for a robust and flexible generative AI system. But how do we inject the right data into the prompt based on the user’s questions? We will dig into the technical aspects of RAG in the next sections.",
      "content_length": 971,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "The vanilla RAG framework\n\nEvery RAG system is similar at its roots. We will first focus on understanding RAG in its simplest form. Later, we will gradually introduce more advanced RAG techniques to improve the system’s accuracy. Note that we will use vanilla and naive RAG interchangeably to avoid repetition.\n\nA RAG system is composed of three main modules independent of each other:\n\nIngestion pipeline: A batch or streaming pipeline used to populate the vector DB\n\nRetrieval pipeline: A module that queries the vector DB and retrieves relevant entries to the user’s input\n\nGeneration pipeline: The layer that uses the retrieved data to augment the prompt and an LLM to generate answers\n\nAs these three components are classes or services of their own, we will dig into each separately. But for now, let’s try to answer the question “How are these three modules connected?”. Here is a very simplistic overview:",
      "content_length": 912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "On the backend side, the ingestion pipeline runs either on a schedule or constantly to populate the vector DB with external data.\n\nOn the client side, the user asks a question.\n\nThe question is passed to the retrieval module, which preprocesses the user’s input and queries the vector DB.\n\nThe generation pipelines use a prompt template, user input, and retrieved context to create the prompt.\n\nThe prompt is passed to an LLM to generate the answer.\n\nThe answer is shown to the user.",
      "content_length": 483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "Figure 4.1: Vanilla RAG architecture",
      "content_length": 36,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "You must implement RAG in your generative AI application when you need access to any type of external information. For example, when implementing a financial assistant, you most likely need access to the latest news, reports, and prices before providing valuable answers. Or, if you build a traveling recommender, you must retrieve and parse a list of potential attractions, restaurants, and activities. At training time, LLMs don’t have access to your specific data, so you will often have to implement a RAG strategy in your generative AI project. Now, let’s dig into the ingestion, retrieval, and generation pipelines.",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "Ingestion pipeline\n\nThe RAG ingestion pipeline extracts raw documents from various data sources (e.g., data warehouse, data lake, web pages, etc.). Then, it cleans, chunks (splits into smaller sections), and embeds the documents. Ultimately, it loads the embedded chunks into a vector DB (or other similar vector storage).\n\nThus, the RAG ingestion pipeline is split into the following:\n\nThe data extraction module gathers all the necessary data from various sources such as DBs, APIs, or web pages. This module is highly dependent on your data. It can be as easy as querying your data warehouse or something more complex such as crawling Wikipedia.\n\nA cleaning layer standardizes and removes unwanted characters from the extracted data. For example, you must remove all invalid characters from your input text, such as non-ASCII and bold and italic characters. Another popular cleaning strategy is to replace URLs with placeholders. However, your cleaning strategy will vary depending on your data source and embedding model.\n\nThe chunking module splits the cleaned documents into smaller ones. As we want to pass the document’s content to an embedding model, this is necessary to ensure it doesn’t exceed the model’s input maximum size. Also, chunking is required to separate specific regions that are",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "semantically related. For example, when chunking a book’s chapter, the most optimal way is to group similar paragraphs into the same section or chunk. By doing so, at the retrieval time, you will add only the essential data to the prompt.\n\nThe embedding component uses anembedding model to take the chunk’s content (text, images, audio, etc.) and project it into a dense vector packed with semantic value—more on embeddings in the What are embeddings? section below.\n\nThe loading module takes the embedded chunks along with a metadata document. The metadata will contain essential information such as the embedded content, the URL to the source of the chunk, and when the content was published on the web. The embedding is used as an index to query similar chunks, while the metadata is used to access the information added to augment the prompt.\n\nAt this point, we have a RAG ingestion pipeline that takes raw documents as input, processes them, and populates a vector DB. The next step is to retrieve relevant data from the vector store correctly.",
      "content_length": 1049,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "Retrieval pipeline\n\nThe retrieval components take the user’s input (text, image, audio, etc.), embed it, and query the vector DB for similar vectors to the user’s input.\n\nThe primary function of the retrieval step is to project the user’s input into the same vector space as the embeddings used as an index in the vector DB. This allows us to find the top K’s most similar entries by comparing the embeddings from the vector storage with the user’s input vector. These entries then serve as content to augment the prompt that is passed to the LLM to generate the answer.\n\nYou must use a distance metric to compare two vectors, such as the Euclidean or Manhattan distance. But the most popular one is the cosine distance, which is equal to 1 minus the cosine of the angle between two vectors, as follows:\n\nIt ranges from\n\n1\n\nto\n\n1\n\n, with a value of",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "1\n\nwhen vectors A and B are in opposite directions,\n\n0\n\nif they are orthogonal, and\n\n1\n\nif they point in the same direction.\n\nMost of the time, the cosine distance works well in non-linear complex vector spaces. However, it is essential to notice that choosing the proper distance between two vectors depends on your data and the embedding model you use.\n\nOne critical factor to highlight is that the user’s input and embeddings must be in the same vector space. Otherwise, you cannot compute the distance between them. To do so, it is essential to preprocess the user input in the same way you processed the raw documents in the RAG ingestion pipeline. This means you must clean, chunk (if necessary), and embed the user’s input using the same functions, models, and hyperparameters. This is similar to how you have to preprocess the data into features in the same way between training and inference; otherwise, the inference will yield inaccurate results—a phenomenon also known as the training-serving skew.",
      "content_length": 1010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "Generation pipeline\n\nThe last step of the RAG system is to take the user’s input, retrieve data, pass it to an LLM, and generate a valuable answer.\n\nThe final prompt results from a system and prompt template populated with the user’s query and retrieved context. You might have a single prompt template or multiple prompt templates, depending on your application. Usually, all the prompt engineering is done at the prompt template level.\n\nBelow, you can see a dummy example of what a generic system and prompt template look like and how they are used together with the retrieval logic and the LLM to generate the final answer:\n\nsystem_template =\n\n\"\"\"\n\nYou are a helpful assistant who answers all the user's questions politely.\n\n\"\"\"\n\nprompt_template =\n\n\"\"\"",
      "content_length": 755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "Answer the user's question using only the provided context. If you cannot answer using the context, respond with \"I don't know.\"\n\nContext: {context}\n\nUser question: {user_question}\n\n\"\"\"\n\nuser_question =\n\n\"\"\n\nretrieved_context = retrieve(user_question) prompt =\n\nf\"\n\n{system_template}\n\n\\n\"\n\nprompt += prompt_template.\n\nformat\n\n(context=retrieved_context, user_question=user_question) answer = llm(prompt)\n\nAs the prompt templates evolve, each change should be tracked and versioned using machine learning operations (MLOps) best practices. Thus, during training or inference time, you always know that a given answer was generated by a specific version of the LLM and prompt template(s). You can do this through Git, store the prompt templates in a DB, or use specific prompt management tools such as LangFuse.",
      "content_length": 809,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "As we’ve seen in the retrieval pipeline, some critical aspects that directly impact the accuracy of your RAG system are the embeddings of the external data, usually stored in vector DBs, the embedding of the user’s query, and how we can find similarities between the two using functions such as the cosine distance. To better understand this part of the RAG algorithm, let’s zoom in on what embeddings are and how they are computed.",
      "content_length": 432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "What are embeddings?\n\nImagine you’re trying to teach a computer to understand the world. Embeddings are like a particular translator that turns these things into a numerical code. This code isn’t random, though, because similar words or items end up with codes that are close to each other. It’s like a map where words with similar meanings are clustered together.\n\nWith that in mind, a more theoretical definition is that embeddings are dense numerical representations of objects encoded as vectors in a continuous vector space, such as words, images, or items in a recommendation system. This transformation helps capture the semantic meaning and relationships between the objects. For instance, in natural language processing (NLP), embeddings translate words into vectors where semantically similar words are positioned closely together in the vector space.",
      "content_length": 861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "Figure 4.2: What are embeddings?\n\nA popular method is visualizing the embeddings to understand and evaluate their geometrical relationship. As the embeddings often have more than 2 or 3 dimensions, usually between 64 and 2048, you must project them again to 2D or 3D.\n\nFor example, you can use UMAP (https://umap- learn.readthedocs.io/en/latest/index.html), a dimensionality reduction method well known for keeping the geometrical properties between the points when projecting the embeddings to 2D or 3D. Another popular",
      "content_length": 520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "algorithm for dimensionality reduction when visualizing vectors is t-SNE (https://scikit- learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). However, compared to UMAP, it is more stochastic and doesn’t preserve the topological relationships between the points.\n\nA dimensionality reduction algorithm, such as PCA, UMAP, and t-SNE, is a mathematical technique used to reduce the number of input variables or features in a dataset while preserving the data’s essential patterns, structure, and relationships. The goal is to transform high-dimensional data into a lower-dimensional form, making it easier to visualize, interpret, and process while minimizing the loss of important information. These methods help to address the “curse of dimensionality,” improve computational efficiency, and often enhance the performance of ML algorithms.",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "Figure 4.3: Visualize embeddings using UMAP (Source: UMAP’s documentation)",
      "content_length": 74,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "Why embeddings are so powerful\n\nFirstly, ML models work only with numerical values. This is not a problem when working with tabular data, as the data is often in numerical form or can easily be processed into numbers. Embeddings come in handy when we want to feed words, images, or audio data into models.\n\nFor instance, when working with transformer models, you tokenize all your text input, where each token has an embedding associated with it. The beauty of this process lies in its simplicity; the input to the transformer is a sequence of embeddings, which can be easily and confidently interpreted by the dense layers of the neural network.\n\nBased on this example, you can use embeddings to encode any categorical variable and feed it to an ML model. But why not use other simple methods, such as one-hot encoding? When working with categorical variables with high cardinality, such as language vocabularies, you will suffer from the curse of dimensionality when using other classical methods. For example, if your vocabulary has 10,000 tokens, then only one token will have a length of 10,000 after applying one-hot encoding. If the input sequence has N tokens, that will become N * 10,000 input parameters. If N >= 100, often, when inputting text, the input is too large to be usable. Another issue with other classical methods that don’t suffer from the curse of dimensionality, such as hashing, is that you lose the semantic relationships between the vectors.",
      "content_length": 1469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "One-hot encoding is a technique that converts categorical variables into a binary matrix representation. Each category is represented as a unique binary vector. For each categorical variable, a binary vector is created with a length equal to the number of unique categories, where all values are zero except for the index corresponding to the specific category, which is set to one. The method preserves all information about the categories. It is simple and interpretable. However, a significant disadvantage is that it can lead to a high-dimensional feature space if the categorical variable has many unique values, making the method impractical.\n\nFeature hashing, also known as hashing encoding or the “hash trick,” is a technique used to convert categorical variables into numerical features by applying a hash function to the category values. Compared to one-hot encoding, the method is not bound to the number of unique categories, but it reduces the dimensionality of the feature space by mapping categories into a fixed number of bins or buckets. Thus, it reduces the dimensionality of the feature space, which is particularly useful when dealing with high-cardinality categorical variables. This makes it efficient in terms of memory usage and computational time. However, there is a risk of collisions, where different categories might map to the same bin, leading to a loss of information. The mapping makes the method uninterpretable. Also, it is difficult to understand the relationship between the original categories and the hashed features.\n\nEmbeddings help us encode categorical variables while controlling the output vector’s dimension. They also use ingenious ways to condense information into a lower dimension space than naive hashing tricks.",
      "content_length": 1763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "Secondly, embedding your input reduces the size of its dimension and condenses all of its semantic meaning into a dense vector. This is an extremely popular technique when working with images, where a CNN encoder module maps the high-dimensional meaning into an embedding, which is later processed by a CNN decoder that performs the classification or regression steps.\n\nThe following image shows a typical CNN layout. Imagine tiny squares within each layer. Those are the “receptive fields.” Each square feeds information to a single neuron in the previous layer. As you move through the network, two key things are happening:\n\nShrinking the picture: Special “subsampling” operations make the layers smaller, focusing on essential details.\n\nLearning features: “Convolution” operations, on the other hand, actually increase the layer size as the network learns more complex features from the image.\n\nFinally, a fully connected layer at the end takes all this processed information and transforms it into the final vector embedding, a numerical image representation.",
      "content_length": 1064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "Figure 4.4: Creating embeddings from an image using a CNN (Image source)\n\nThe preceding image is sourced from Wikimedia Commons (https://commons.wikimedia.org/wiki/File:Typical_cnn.png) and licensed under the Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0: https://creativecommons.org/licenses/by- sa/4.0/deed.en).",
      "content_length": 347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "How are embeddings created?\n\nEmbeddings are created by deep learning models that understand the context and semantics of your input and project it into a continuous vector space.\n\nVarious deep learning models can be used to create embeddings, varying by the data input type. Thus, it is fundamental to understand your data and what you need from it before picking an embedding model.\n\nFor example, when working with text data, one of the early methods used to create embeddings for your vocabulary is Word2Vec and GloVe. These are still popular methods used today for simpler applications.\n\nAnother popular method is to use encoder-only transformers, such as BERT, or other methods from its family, such as RoBERTa. These models leverage the encoder of the transformer architecture to smartly project your input into a dense vector space that can later be used as embeddings.\n\nTo quickly compute the embeddings in Python, you can conveniently leverage the Sentence Transformers Python package (also available in Hugging Face’s transformer package). This tool provides a user-friendly interface, making the embedding process straightforward and efficient.",
      "content_length": 1154,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "In the code snippet below, you can see how we loaded a model from SentenceTransformer, computed the embeddings for three sentences, and, ultimately, computed the cosine similarity between them. The similarity between one sentence and itself is always 1. Also, the similarity between the first and second sentences is approximately 0, as the sentences have nothing in common. In contrast, the value between the first and third one is higher as there is some overlapping context:\n\nfrom\n\nsentence_transformers\n\nimport\n\nSentenceTransformer model = SentenceTransformer(\n\n\"all-MiniLM-L6-v2\"\n\n) sentences = [\n\n\"The dog sits outside waiting for a treat.\"\n\n,\n\n\"I am going swimming.\"\n\n,\n\n\"The dog is swimming.\"\n\n] embeddings = model.encode(sentences)\n\nprint\n\n(embeddings.shape)",
      "content_length": 767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "# Output: [3, 384]\n\nsimilarities = model.similarity(embeddings, embeddings)\n\nprint\n\n(similarities)\n\n# Output:\n\n# tensor([[ 1.0000, -0.0389, 0.2692],\n\n# [-0.0389, 1.0000, 0.3837],\n\n# [ 0.2692, 0.3837, 1.0000]])\n\n#\n\n# similarities[0, 0] = The similarity between the first sentence and itself.\n\n# similarities[0, 1] = The similarity between the first and second sentence.\n\n# similarities[2, 1] = The similarity between the third and second sentence.\n\nThe source code for the preceding snippet can be found at https://github.com/PacktPublishing/LLM- Engineering/blob/main/code_snippets/08_text_embeddings.py.\n\nThe examples in the embeddings section can be run within the virtual environment used across the book, as it contains all the required dependencies.",
      "content_length": 754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "The best-performing embedding model can change with time and your specific use case. You can find particular models on the Massive Text Embedding Benchmark (MTEB) on Hugging Face. Depending on your needs, you can consider the best-performing model, the one with the best accuracy, or the one with the smallest memory footprint. This decision is solely based on your requirements (e.g., accuracy and hardware). However, Hugging Face and SentenceTransformer make switching between different models straightforward. Thus, you can always experiment with various options.\n\nWhen working with images, you can embed them using convolutional neural networks (CNNs). Popular CNN networks are based on the ResNet architecture. However, we can’t directly use image embedding techniques for audio recordings. Instead, we can create a visual representation of the audio, such as a spectrogram, and then apply image embedding models to those visuals. This allows us to capture the essence of images and sounds in a way computers can understand.\n\nBy leveraging models like CLIP, you can practically embed a piece of text and an image in the same vector space. This allows you to find similar images using a sentence as input, or the other way around, demonstrating the practicality of CLIP.\n\nIn the following code snippet, we use CLIP to encode a crazy cat image and three sentences. Ultimately, we use cosine similarity to compute the resemblance between the picture and the sentences:",
      "content_length": 1470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "from\n\nio\n\nimport\n\nBytesIO\n\nimport\n\nrequests\n\nfrom\n\nPIL\n\nimport\n\nImage\n\nfrom\n\nsentence_transformers\n\nimport\n\nSentenceTransformer response = requests.get(\n\n\"https://github.com/PacktPublishing/LLM- Engineering/blob/main/images/crazy_cat.jpg?raw=true\"\n\n) image = Image.\n\nopen\n\n(BytesIO(response.content)) model = SentenceTransformer(\n\n\"",
      "content_length": 332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "clip-ViT-B-32\"\n\n) img_emb = model.encode(image) text_emb = model.encode( [\n\n\"A crazy cat smiling.\"\n\n,\n\n\"A white and brown cat with a yellow bandana.\"\n\n,\n\n\"A man eating in the garden.\"\n\n] )\n\nprint\n\n(text_emb.shape)\n\n# noqa\n\n# Output: (3, 512)\n\nsimilarity_scores = model.similarity(img_emb, text_emb)\n\nprint\n\n(similarity_scores)\n\n# noqa\n\n# Output: tensor([[0.3068, 0.3300, 0.1719]])",
      "content_length": 380,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "The source code can be found at https://github.com/PacktPublishing/LLM- Engineering/blob/main/code_snippets/08_text_image_embeddings.py.\n\nHere, we provided a small introduction to how embeddings can be computed. The realm of specific implementations is vast, but what is important to know is that embeddings can be computed for most digital data categories, such as words, sentences, documents, images, videos, and graphs.\n\nIt’s crucial to grasp that you must use specialized models when you need to compute the distance between two different data categories, such as the distance between the vector of a sentence and of an image. These models are designed to project both data types into the same vector space, such as CLIP, ensuring accurate distance computation.",
      "content_length": 765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "Applications of embeddings\n\nDue to the generative AI revolution, which uses RAG, embeddings have become extremely popular in information retrieval tasks, such as semantic search for text, code, images, and audio, and long-term memory of agents. But before generative AI, embeddings were already heavily used in:\n\nRepresenting categorical variables (e.g., vocabulary tokens) that are fed to an ML model\n\nRecommender systems by encoding the users and items and finding their relationship\n\nClustering and outlier detection\n\nData visualization by using algorithms such as UMAP\n\nClassification by using the embeddings as features\n\nZero-shot classification by comparing the embedding of each class and picking the most similar one",
      "content_length": 724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "The last step to fully understanding how RAG works is to examine vector DBs and how they leverage embeddings to retrieve data.",
      "content_length": 126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "More on vector DBs\n\nVector DBs are specialized DBs designed to efficiently store, index, and retrieve vector embeddings. Traditional scalar-based DBs struggle with the complexity of vector data, making vector DBs crucial for tasks like real- time semantic search.\n\nWhile standalone vector indices like FAISS are effective for similarity search, they lack vector DBs’ comprehensive data management capabilities. Vector DBs support CRUD operations, metadata filtering, scalability, real- time updates, backups, ecosystem integration, and robust data security, making them more suited for production environments than standalone indices.",
      "content_length": 634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "How does a vector DB work?\n\nThink of how you usually search a DB. You type in something specific, and the system spits out the exact match. That’s how traditional DBs work. Vector DBs are different. Instead of perfect matches, we look for the closest neighbors of the query vector. Under the hood, a vector DB uses approximate nearest neighbor (ANN) algorithms to find these close neighbors.\n\nWhile ANN algorithms don’t return the top matches for a given search, standard nearest neighbor algorithms are too slow to work in practice. Also, it is shown empirically that using only approximations of the top matches for a given input query works well enough. Thus, the trade-off between accuracy and latency ultimately favors ANN algorithms.\n\nThis is a typical workflow of a vector DB:\n\nIndexing vectors: Vectors are indexed using data structures optimized for high-dimensional data. Common indexing techniques include hierarchical navigable small world (HNSW), random projection, product quantization (PQ), and locality-sensitive hashing (LSH).\n\nQuerying for similarity: During a search, the DB queries the indexed vectors to find those most similar to the input vector. This process involves comparing vectors based on similarity measures such as cosine",
      "content_length": 1253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "similarity, Euclidean distance, or dot product. Each has unique advantages and is suitable for different use cases.\n\nPost-processing results: After identifying potential matches, the results undergo post-processing to refine accuracy. This step ensures that the most relevant vectors are returned to the user.\n\nVector DBs can filter results based on metadata before or after the vector search. Both approaches have trade-offs in terms of performance and accuracy. The query also depends on the metadata (along with the vector index), so it contains a metadata index user for filtering operations.",
      "content_length": 596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "Algorithms for creating the vector index\n\nVector DBs use various algorithms to create the vector index and manage searching data efficiently:\n\nRandom projection: Random projection reduces the dimensionality of vectors by projecting them into a lower-dimensional space using a random matrix. This technique preserves the relative distances between vectors, facilitating faster searches.\n\nPQ: PQ compresses vectors by dividing them into smaller sub-vectors and then quantizing these sub-vectors into representative codes. This reduces memory usage and speeds up similarity searches.\n\nLSH: LSH maps similar vectors into buckets. This method enables fast approximate nearest neighbor searches by focusing on a subset of the data, reducing the computational complexity.\n\nHNSW: HNSW constructs a multi-layer graph where each node represents a set of vectors. Similar nodes are connected, allowing the algorithm to navigate the graph and find the nearest neighbors efficiently.",
      "content_length": 970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "These algorithms enable vector DBs to efficiently handle complex and large-scale data, making them a perfect fit for a variety of AI and ML applications.",
      "content_length": 153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "DB operations\n\nVector DBs also share common characteristics with standard DBs to ensure high performance, fault tolerance, and ease of management in production environments. Key operations include:\n\nSharding and replication: Data is partitioned (sharded) across multiple nodes to ensure scalability and high availability. Data replication across nodes helps maintain data integrity and availability in case of node failures.\n\nMonitoring: Continuous monitoring of DB performance, including query latency and resource usage (RAM, CPU, disk), helps maintain optimal operations and identify potential issues before they impact the system.\n\nAccess control: Implementing robust access control mechanisms ensures that only authorized users can access and modify data. This includes role-based access controls and other security protocols to protect sensitive information.\n\nBackups: Regular DB backups are critical for disaster recovery. Automated backup processes ensure that data can be restored to a previous state in case of corruption or loss.",
      "content_length": 1040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "An overview of advanced RAG\n\nThe vanilla RAG framework we just presented doesn’t address many fundamental aspects that impact the quality of the retrieval and answer generation, such as:\n\nAre the retrieved documents relevant to the user’s question?\n\nIs the retrieved context enough to answer the user’s question?\n\nIs there any redundant information that only adds noise to the augmented prompt?\n\nDoes the latency of the retrieval step match our requirements?\n\nWhat do we do if we can’t generate a valid answer using the retrieved information?\n\nFrom the questions above, we can draw two conclusions. The first one is that we need a robust evaluation module for our RAG system that can quantify and measure the quality of the retrieved data and generate answers",
      "content_length": 759,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "relative to the user’s question. We will discuss this topic in more detail in Chapter 9. The second conclusion is that we must improve our RAG framework to address the retrieval limitations directly in the algorithm. These improvements are known as advanced RAG.\n\nThe vanilla RAG design can be optimized at three different stages:\n\nPre-retrieval: This stage focuses on how to structure and preprocess your data for data indexing optimizations as well as query optimizations.\n\nRetrieval: This stage revolves around improving the embedding models and metadata filtering to improve the vector search step.\n\nPost-retrieval: This stage mainly targets different ways to filter out noise from the retrieved documents and compress the prompt before feeding it to an LLM for answer generation.",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "Figure 4.5: The three stages of advanced RAG\n\nThis section is not meant to be an exhaustive list of all the advanced RAG methods available. The goal is to build an intuition about what can be optimized. We will use only examples based on text data, but the principles of advanced RAG remain the same regardless of the data category. Now, let’s zoom in on all three components.",
      "content_length": 376,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "Pre-retrieval\n\nThe pre-retrieval steps are performed in two different ways:\n\nData indexing: It is part of the RAG ingestion pipeline. It is mainly implemented within the cleaning or chunking modules to preprocess the data for better indexing.\n\nQuery optimization: The algorithm is performed directly on the user’s query before embedding it and retrieving the chunks from the vector DB.\n\nAs we index our data using embeddings that semantically represent the content of a chunked document, most of the data indexing techniques focus on better preprocessing and structuring the data to improve retrieval efficiency, such as:\n\nSliding window: The sliding window technique introduces overlap between text chunks, ensuring that important context near chunk boundaries is retained, which enhances retrieval accuracy. This is particularly beneficial in domains like legal documents, scientific papers, customer support logs, and medical records, where critical information often spans multiple sections. The embedding is computed on the chunk along with the overlapping portion. Hence, the sliding",
      "content_length": 1089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "window improves the system’s ability to retrieve relevant and coherent information by maintaining context across boundaries.\n\nEnhancing data granularity: This involves data cleaning techniques like removing irrelevant details, verifying factual accuracy, and updating outdated information. A clean and accurate dataset allows for sharper retrieval.\n\nMetadata: Adding metadata tags like dates, URLs, external IDs, or chapter markers helps filter results efficiently during retrieval.\n\nOptimizing index structures: It is based on different data index methods, such as various chunk sizes and multi-indexing strategies.\n\nSmall-to-big: The algorithm decouples the chunks used for retrieval and the context used in the prompt for the final answer generation. The algorithm uses a small sequence of text to compute the embedding while preserving the sequence itself and a wider window around it in the metadata. Thus, using smaller chunks enhances the retrieval’s accuracy, while the larger context adds more contextual information to the LLM.\n\nThe intuition behind this is that if we use the whole text for computing the embedding, we might introduce too much noise, or the text could contain multiple topics, which results in a poor overall semantic representation of the embedding.",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "Figure 4.6: Query routing",
      "content_length": 25,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "On the query optimization side, we can leverage techniques such as query routing, query rewriting, and query expansion to refine the retrieved information for the LLM further:\n\nQuery routing: Based on the user’s input, we might have to interact with different categories of data and query each category differently. Query rooting is used to decide what action to take based on the user’s input, similar to if/else statements. Still, the decisions are made solely using natural language instead of logical statements.\n\nAs illustrated in Figure 4.6, let’s assume that, based on the user’s input, to do RAG, we can retrieve additional context from a vector DB using vector search queries, a standard SQL DB by translating the user query to an SQL command, or the internet by leveraging REST API calls. The query router can also detect whether a context is required, helping us avoid making redundant calls to external data storage. Also, a query router can be used to pick the best prompt template for a given input. For example, in the LLM Twin use case, depending on whether the user wants an article paragraph, a post, or a code snippet, you need different prompt templates to optimize the creation process. The routing usually uses an LLM to decide what route to take or embeddings by picking the path with the most similar vectors. To summarize, query routing is identical to an if/else statement but much more versatile as it works directly with natural language.\n\nQuery rewriting: Sometimes, the user’s initial query might not perfectly align with the way your data is structured. Query rewriting tackles this by reformulating the question to match the indexed information better. This can involve techniques like:\n\nParaphrasing: Rephrasing the user’s query while preserving its meaning (e.g., “What are the causes of climate change?” could be rewritten as “Factors contributing to global warming”).",
      "content_length": 1903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "Synonym substitution: Replacing less common words with synonyms to broaden the search scope (e.g., “ joyful” could be rewritten as “happy”).\n\nSub-queries: For longer queries, we can break them down into multiple shorter and more focused sub-queries. This can help the retrieval stage identify relevant documents more precisely.\n\nHypothetical document embeddings (HyDE): This technique involves having an LLM create a hypothetical response to the query. Then, both the original query and the LLM’s response are fed into the retrieval stage.\n\nQuery expansion: This approach aims to enrich the user’s question by adding additional terms or concepts, resulting in different perspectives of the same initial question. For example, when searching for “disease,” you can leverage synonyms and related terms associated with the original query words and also include “illnesses” or “ailments.”\n\nSelf-query: The core idea is to map unstructured queries into structured ones. An LLM identifies key entities, events, and relationships within the input text. These identities are used as filtering parameters to reduce the vector search space (e.g., identify cities within the query, for example, “Paris,” and add it to your filter to reduce your vector search space).",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "Both data indexing and query optimization pre-retrieval optimization techniques depend highly on your data type, structure, and source. Thus, as with any data processing pipeline, no method always works, as every use case has its own particularities and gotchas. Optimizing your pre-retrieval RAG layer is experimental. Thus, what is essential is to try multiple methods (such as the ones enumerated in this section), reiterate, and observe what works best.",
      "content_length": 457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "Retrieval\n\nThe retrieval step can be optimized in two fundamental ways:\n\nImproving the embedding models used in the RAG ingestion pipeline to encode the chunked documents and, at inference time, transform the user’s input.\n\nLeveraging the DB’s filter and search features. Thisstep will be used solely at inference time when you have to retrieve the most similar chunks based on user input.\n\nBoth strategies are aligned with our ultimate goal: to enhance the vector search step by leveraging the semantic similarity between the query and the indexed data.\n\nWhen improving the embedding models, you usually have to fine-tune the pre-trained embedding models to tailor them to specific jargon and nuances of your domain, especially for areas with evolving terminology or rare terms.\n\nInstead of fine-tuning the embedding model, you can leverage instructor models (https://huggingface.co/hkunlp/instructor-xl) to guide the",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "embedding generation process with an instruction/prompt aimed at your domain. Tailoring your embedding network to your data using such a model can be a good option, as fine-tuning a model consumes more computing and human resources.\n\nIn the code snippet below, you can see an example of an Instructor model that embeds article titles about AI:\n\nfrom\n\nInstructorEmbedding\n\nimport\n\nINSTRUCTOR model = INSTRUCTOR(\n\n\"hkunlp/instructor-base\"\n\n) sentence =\n\n\"\n\nRAG Fundamentals First\"\n\ninstruction =\n\n\"Represent the title of an article about AI:\"\n\nembeddings = model.encode([[instruction, sentence]])\n\nprint\n\n(embeddings.shape)",
      "content_length": 621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "# noqa\n\n# Output: (1, 768)\n\nThe source code can be found at https://github.com/PacktPublishing/LLM- Engineering/blob/main/code_snippets/08_instructor_embeddings.py.\n\nTo run the instructor code, you have to create a different virtual environment and activate it:\n\npython3 -m venv instructor_venv && source instructor_venv/bin/activate\n\nAnd install the required Python dependencies:\n\npip install sentence-transformers==2.2.2 InstructorEmbedding==1.0.1",
      "content_length": 449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "On the other side of the spectrum, here is how you can improve your retrieval by leveraging classic filter and search DB features:\n\nHybrid search: This is a vector and keyword-based search blend. Keyword-based search excels at identifying documents containing specific keywords. When your task demands pinpoint accuracy and the retrieved information must include exact keyword matches, hybrid search shines. Vector search, while powerful, can sometimes struggle with finding exact matches, but it excels at finding more general semantic similarities. You leverage both keyword matching and semantic similarities by combining the two methods. You have a parameter, usually called alpha, that controls the weight between the two methods. The algorithm has two independent searches, which are later normalized and unified.\n\nFiltered vector search: This type of search leverages the metadata index to filter for specific keywords within the metadata. It differs from a hybrid search in that you retrieve the data once using only the vector index and perform the filtering step before or after the vector search to reduce your search space.\n\nIn practice, on the retrieval side, you usually start with filtered vector search or hybrid search, as they are fairly quick to implement. This approach gives you the flexibility to adjust your strategy based on performance. If the results are not as expected, you can always fine-tune your embedding model.",
      "content_length": 1444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "Post-retrieval\n\nThe post-retrieval optimizations are solely performed on the retrieved data to ensure that the LLM’s performance is not compromised by issues such as limited context windows or noisy data. This is because the retrieved context can sometimes be too large or contain irrelevant information, both of which can distract the LLM.\n\nTwo popular methods performed at the post-retrieval step are:\n\nPrompt compression: Eliminate unnecessary details while keeping the essence of the data.\n\nRe-ranking: Use a cross-encoder ML model to give a matching score between the user’s input and every retrieved chunk. The retrieved items are sorted based on this score. Only the top N results are kept as the most relevant. As you can see in Figure 4.7, this works because the re-ranking model can find more complex relationships between the user input and some content than a simple similarity search. However, we can’t apply this model at the initial retrieval step because it is costly. That is why a popular strategy is to retrieve the data using a similarity distance between the embeddings and refine the retrieved information using a re-raking model, as illustrated in Figure 4.8.",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "Figure 4.7: Bi-encoder (the standard embedding model) versus cross- encoder",
      "content_length": 75,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "The abovementioned techniques are far from an exhaustive list of all potential solutions. We used them as examples to get an intuition on what you can (and should) optimize at each step in your RAG workflow. The truth is that these techniques can vary tremendously by the type of data you work with.\n\nFor example, if you work with multi-modal data such as text and images, most of the techniques from earlier won’t work as they are designed for text only.",
      "content_length": 455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "Figure 4.8: The re-ranking algorithm",
      "content_length": 36,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "To summarize, the primary goal of these optimizations is to enhance the RAG algorithm at three key stages: pre-retrieval, retrieval, and post- retrieval. This involves preprocessing data for improved vector indexing, adjusting user queries for more accurate searches, enhancing the embedding model, utilizing classic filtering DB operations, and removing noisy data. By keeping these goals in mind, you can effectively optimize your RAG workflow for data processing and retrieval",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "Exploring the LLM Twin’s RAG feature pipeline architecture\n\nNow that you have a strong intuition and understanding of RAG and its workings, we will continue exploring our particular LLM Twin use case. The goal is to provide a hands-on end-to-end example to solidify the theory presented in this chapter.\n\nAny RAG system is split into two independent components:\n\nThe ingestion pipeline takes in raw data, cleans, chunks, embeds, and loads it into a vector DB.\n\nThe inference pipeline queries the vector DB for relevant context and ultimately generates an answer by levering an LLM.\n\nIn this chapter, we will focus on implementing the RAG ingestion pipeline, and in Chapter 9, we will continue developing the inference pipeline.\n\nWith that in mind, let’s have a quick refresher on the problem we are trying to solve and where we get our raw data. Remember that we are building an end-to-end ML system. Thus, all the components talk to each other through an interface (or a contract), and each pipeline has a single responsibility. In",
      "content_length": 1032,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "our case, we ingest raw documents, preprocess them, and load them into a vector DB.",
      "content_length": 83,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 469,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "The problem we are solving\n\nAs presented in the previous chapter, this book aims to show you how to build a production-ready LLM Twin backed by an end-to-end ML system. In this chapter specifically, we want to design a RAG feature pipeline that takes raw social media data (e.g., articles, code repositories, and posts) from our MongoDB data warehouse. The text of the raw documents will be cleaned, chunked, embedded, and ultimately loaded to a feature store. As discussed in Chapter 1, we will implement a logical feature store using ZenML artifacts and a Qdrant vector DB.\n\nAs we want to build a fully automated feature pipeline, we want to sync the data warehouse and logical feature store. Remember that, at inference time, the context used to generate the answer is retrieved from the vector DB. Thus, the speed of synchronization between the data warehouse and the feature store will directly impact the accuracy of our RAG algorithm.\n\nAnother key consideration is how to automate the feature pipeline and integrate it with the rest of our ML system. Our goal is to minimize any desynchronization between the two data storages, as this could potentially compromise the integrity of our system.\n\nTo conclude, we must design a feature pipeline that constantly syncs the data warehouse and logical feature store while processing the data accordingly. Having the data in a feature store is critical for a production- ready ML system. The LLM Twin inference pipeline will query it for RAG,",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "while the training pipeline will consume tracked and versioned fine-tuning datasets from it.",
      "content_length": 92,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "The feature store\n\nThe feature store will be the central access point for all the features used within the training and inference pipelines. The training pipeline will use the cleaned data from the feature store (stored as artifacts) to fine-tune LLMs. The inference pipeline will query the vector DB for chunked documents for RAG. That is why we are designing a feature pipeline and not only a RAG ingestion pipeline. In practice, the feature pipeline contains multiple subcomponents, one of which is the RAG logic.\n\nRemember that the feature pipeline is mainly used as a mind map to navigate the complexity of ML systems. It clearly states that it takes raw data as input and then outputs features and optional labels, which are stored in the feature store. Thus, a good intuition is to consider that all the logic between the data warehouse and the feature store goes into the feature pipeline namespace, consisting of one or more sub-pipelines. For example, we will implement another pipeline that takes in cleaned data, processes it into instruct datasets, and stores it in artifacts; this also sits under the feature pipeline umbrella as the artifacts are part of the logical feature store. Another example would be implementing a data validation pipeline on top of the raw data or computed features.\n\nAnother important observation to make is that text data stored as strings are not considered features if you follow the standard conventions. A feature is something that is fed directly into the model. For example, we would have to tokenize the instruct datasets or chunked documents to be considered features. Why? Because the tokens are fed directly to the model and not the sentences as strings. Unfortunately, this makes the system more complex",
      "content_length": 1756,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "and unflexible. Thus, we will do the tokenization at runtime. But this observation is important to understand as it’s a clear example that you don’t have to be too rigid about the feature/training/inference (FTI) architecture. You have to take it and adapt it to your own use case.",
      "content_length": 281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": "Where does the raw data come from?\n\nAs a quick reminder, all the raw documents are stored in a MongoDB data warehouse. The data warehouse is populated by the data collection ETL pipeline presented in Chapter 3. The ETL pipeline crawls various platforms such as Medium and Substack, standardizes the data, and loads it into MongoDB. Check out Chapter 3 for more details on this topic.",
      "content_length": 383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "Designing the architecture of the RAG feature pipeline\n\nThe last step is to architect and go through the design of the RAG feature pipeline of the LLM Twin application. We will use a batch design scheduled to poll data from the MongoDB data warehouse, process it, and load it to a Qdrant vector DB. The first question to ask ourselves is, “Why a batch pipeline?”\n\nBut before answering that, let’s quickly understand how a batch architecture works and behaves relative to a streaming design.",
      "content_length": 490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "Figure 4.9: The architecture of the LLM Twin’s RAG feature pipeline",
      "content_length": 67,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "Batch pipelines\n\nA batch pipeline in data systems refers to a data processing method where data is collected, processed, and stored in predefined intervals and larger volumes, also known as “batches”. This approach differs from real-time or streaming data processing, where data is processed continuously as it arrives. This is what happens in a batch pipeline:\n\nData collection: Data is collected from various sources and stored until sufficient amounts are accumulated for processing. This can include data from DBs, logs, files, and other sources.\n\nScheduled processing: Data processing is scheduled at regular intervals, for example, hourly or daily. During this time, the collected data is processed in bulk. This can involve data cleansing, transformation, aggregation, and other operations.\n\nData loading: After processing, the data is loaded into the target system, such as a DB, data warehouse, data lake, or feature store. This processed data is then available for analysis, querying, or further processing.\n\nBatch pipelines are particularly useful when dealing with large volumes of data that do not require immediate processing. They offer several",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "advantages, including:\n\nEfficiency: Batch processing can handle large volumes of data more efficiently than real-time processing, allowing for optimized resource allocation and parallel processing.\n\nComplex processing: Batch pipelines can perform complex data transformations and aggregations that might be too resource-intensive for real-time processing.\n\nSimplicity: Batch processing systems’ architectures are often simpler than those of real-time systems, making them easier to implement and maintain.",
      "content_length": 505,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "Batch versus streaming pipelines\n\nWhen implementing feature pipelines, you have two main design choices: batch and streaming. Thus, it is worthwhile to see the difference between the two and understand why we chose a batch architecture over a streaming one for our LLM Twin use case.\n\nYou can effortlessly write a dedicated chapter on streaming pipelines, which suggests its complexity over a batch design. However, as streaming architectures become increasingly popular, one must have an intuition of how they work to choose the best option for your application.\n\nThe core elements of streaming applications are a distributed event streaming platform such as Apache Kafka or Redpanda to store events from multiple clients and a streaming engine such as Apache Flink or Bytewax to process the events. To simplify your architecture, you can swap your event streaming platform with queues, such as RabbitMQ, to store the events until processed. Table 4.1 compares batch and streaming pipelines based on multiple criteria such as processing schedule and complexity:\n\nAspect\n\nBatch pipeline\n\nProcessing schedule\n\nProcesses data at regular intervals (e.g., every minute, hourly, daily).\n\nEfficiency\n\nHandles large volumes of data more efficiently, optimizing resource allocation and parall\n\nProcessing complexity Capable of performing complex data transformations and aggregations.\n\nUse cases\n\nSuitable for scenarios where immediate data processing is not critical. Commonly used in\n\nSystem complexity\n\nCompared to streaming pipelines, systems are generally simpler to implement and maint\n\nTable 4.1: Batch versus streaming pipelines\n\nFor example, streaming pipelines are extremely powerful in social media recommender systems like TikTok. When using social media, user behavior changes frequently. A typical scenario is that you want to relax at a certain point in time and mostly look at videos of puppies. Still, after 15 minutes, you get bored and want something more serious, such as educative content or news. This means the recommender system has to capture these behavior changes without delay to keep you engaged. As the transition between interests is cyclical and not predictable, you can’t use a batch pipeline that runs every 30 minutes or every hour to generate more content. You can run it every minute to create new content, but, at the same time, it will result in unnecessary costs, as most predictions will not be consumed. By implementing a streaming pipeline, you update the features of specific users in real time, which are then passed to a chain of models that predict the new recommendations.",
      "content_length": 2612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "Streaming architectures are also the backbone of real-time fraud detection algorithms, such as those used at Stripe or PayPal. In this context, it’s critical to identify potentially fraudulent transactions as they occur, not after a few minutes or hours as a batch pipeline would process them. The same urgency applies to high-frequency trading platforms that make stock predictions based on the constant influx of market data, enabling traders to make decisions within milliseconds.\n\nOn the other hand, you can use a batch architecture for an offline recommender system. For example, when implementing one for an e-commerce or streaming platform, you don’t need the system to be so reactive, as the user’s behavior rarely changes. Thus, updating the recommendations periodically, such as every night, based on historical user behavior data using a batch pipeline is easier to implement and cheaper.\n\nAnother popular example of batch pipelines is the ETL design used to extract, transform, and load data for different use cases. The ETL design is widespread in data pipelines used to move data from one DB to another. Some practical use cases include aggregating data for analytics, where you have to extract data from multiple sources, aggregate it, and load it to a data warehouse connected to a dashboard. The analytics domains can be widespread, from e-commerce and marketing to finance and research.\n\nThe data collection pipeline used in the LLM Twin use case is another example of an ETL pipeline that extracts data from the internet, structures it, and loads it into a data warehouse for future processing.\n\nAlong with prediction or feature freshness, another disadvantage of batch pipelines over streaming ones is that you usually make redundant predictions. Let’s take the example of a recommender system for a streaming platform like Netflix. Every night, you make the predictions for all users. There is a significant chance that a large chunk of users won’t log in that day. Also, users usually don’t browse all the recommendations but stick to the first ones. Thus, only a portion of predictions are used, wasting computing power on all the others.\n\nThat’s why a popular strategy is to start with a batch architecture, as it’s faster and easier to implement. After the product is in place, you gradually move to a streaming design to reduce costs and improve the user experience.\n\nTo conclude, we have used a batch architecture (and not a streaming one) to implement the LLM Twin’s feature pipeline for the following reasons:\n\nDoes not require immediate data processing: Even if syncing the data warehouse and feature store is critical for an accurate RAG system, a delay of a few minutes is acceptable. Thus, we can schedule the batch pipeline to run every minute, constantly syncing the two data storages. This technique works because the data volume is small. The whole data warehouse will have only thousands of records, not millions or billions. Hence, we can quickly iterate through them and sync the two DBs.\n\nSimplicity: As stated earlier, implementing a streaming pipeline is two times more complex. In the real world, you want to keep your system as simple as possible, making it easier to understand, debug, and maintain. Also,",
      "content_length": 3251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "simplicity usually translates to lower infrastructure and development costs.\n\nIn Figure 8.10, we compare what tools you can use based on your architecture (streaming versus batch) and the quantity of data you have to process (small versus big data). In our use case, we are in the smaller data and batch quadrant, where we picked a combination of vanilla Python and generative AI tools such as LangChain, Sentence Transformers, and Unstructured.\n\nFigure 4.10: Tools on the streaming versus batch and smaller versus bigger data spectrum\n\nIn the Change data capture: syncing the data warehouse and feature store\n\nsection later in this chapter, we will discuss when switching from a batch architecture to a streaming one makes sense.",
      "content_length": 730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "Core steps\n\nMost of the RAG feature pipelines are composed of five core steps. The one implemented in the LLM Twin architecture makes no exception. Thus, you can quickly adapt this pattern for other RAG applications, but here is what the LLM Twin’s RAG feature pipeline looks like:\n\nData extraction: Extract the latest articles, code repositories, and posts from the MongoDB data warehouse. At the extraction step, you usually aggregate all the data you need for processing.\n\nCleaning: The data from the data warehouse is standardized and partially clean, but we have to ensure that the text contains only useful information, is not duplicated, and can be interpreted by the embedding model. For example, we must clean and normalize all non-ASCII characters before passing the text to the embedding model. Also, to keep the information semantically dense, we decided to replace all the URLs with placeholders and remove all emojis. The cleaning step is more art than science. Hence, after you have the first iteration with an evaluation mechanism in place, you will probably reiterate and improve it.\n\nChunking: You must adopt various chunking strategies based on each data category and embedding model. For example, when working with code repositories, you want the chunks broader, whereas when working with articles, you want them narrower or scoped at the paragraph level. Depending on your data, you must decide if you split your document based",
      "content_length": 1448,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 489,
      "content": "on the chapter, section, paragraph, sentence, or just a fixed window size. Also, you have to ensure that the chunk size doesn’t exceed the maximum input size of the embedding model. That is why you usually chunk a document based on your data structure and the maximum input size of the model.\n\nEmbedding: You pass each chunk individually to an embedding model of your choice. Implementation-wise, this step is usually the simplest, as tools such as SentenceTransformer and Hugging Face provide high-level interfaces for most embedding models. As explained in the What are embeddings? section of this chapter, at this step, the most critical decisions are to decide what model to use and whether to fine-tune it or not. For example, we used an\n\n\"all-mpnet-base-v2\"\n\nembedding model from SentenceTransformer, which is relatively tiny and runs on most machines. However, we provide a configuration file where you can quickly configure the embedding model with something more powerful based on the state of the art when reading this book. You can quickly find other options on the MTEB on Hugging Face (https://huggingface.co/spaces/mteb/leaderboard).\n\nData loading: The final step combines the embedding of a chunked document and its metadata, such as the author and the document ID, content, URL, platform, and creation date. Ultimately, we wrap the vector and the metadata into a structure compatible with Qdrant and push it to the vector DB. As we want to use Qdrant as the single source of truth for the features, we also push the cleaned documents (before chunking) to Qdrant. We can push data without vectors, as the metadata index of Qdrant behaves like a NoSQL DB. Thus, pushing metadata without a vector attached to it is like using a standard NoSQL engine.",
      "content_length": 1763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "Change data capture: syncing the data warehouse and feature store\n\nAs highlighted a few times in this chapter, data is constantly changing, which can result in DBs, data lakes, data warehouses, and feature stores getting out of sync. Change data capture (CDC) is a strategy that allows you to optimally keep two or more data storage types in sync without computing and I/O overhead. It captures any CRUD operation done on the source DB and replicates it on a target DB. Optionally, you can add preprocessing steps in between the replication.\n\nThe syncing issues also apply when building a feature pipeline. One key design choice concerns how to sync the data warehouse with the feature store to have data fresh enough for your particular use case.\n\nIn our LLM Twin use case, we chose a naïve approach out of simplicity. We implemented a batch pipeline that is triggered periodically or manually. It reads all the raw data from the data warehouse, processes it in batches, and inserts new records or updates old ones from the Qdrant vector DB. This works fine when you are working with a small number of records, at the order of thousands or tens of thousands. But our naïve approach raises the following questions:\n\nWhat happens if the data suddenly grows to millions of records (or higher)?",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "What happens if a record is deleted from the data warehouse? How is this reflected in the feature store?\n\nWhat if we want to process only the new or updated items from the data warehouse and not all of them?\n\nFortunately, the CDC pattern can solve all of these issues. When implementing CDC, you can take multiple approaches, but all of them use either a push or pull strategy:\n\nPush: The source DB is the primary driver in the push approach. It actively identifies and transmits data modifications to target systems for processing. This method ensures near-instantaneous updates at the target, but data loss can occur if target systems are inaccessible. To mitigate this, a messaging system is typically employed as a buffer.\n\nPull: The pull method assigns a more passive role to the source DB, which only records data changes. Target systems periodically request these changes and handle updates accordingly. While this approach lightens the load on the source, it introduces a delay in data propagation. A messaging system is again essential to prevent data loss during periods of target system unavailability.\n\nIn summary, the push method is ideal for applications demanding immediate data access, whereas the pull method is better suited for large- scale data transfers where real-time updates aren’t critical. With that in",
      "content_length": 1328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "mind, there are different methods to detect changes in data. Thus, let’s list the main CDC patterns that are used in the industry:\n\nTimestamp-based: The approach involves adding a modification time column to DB tables, usually called\n\nLAST_MODIFIED\n\nor\n\nLAST_UPDATED\n\n. Downstream systems can query this column to identify records that have been updated since their last check. While simple to implement, this method is limited to tracking changes, not deletions, and imposes performance overhead due to the need to scan entire tables.\n\nTrigger-based: The trigger-based approach utilizes DB triggers to automatically record data modifications in a separate table upon INSERT, UPDATE, or DELETE operations, often known as the event table. This method provides comprehensive change tracking but can impact the DB performance due to the additional write operations involved for each event.\n\nLog-based: DBs maintain transaction logs to record all data modifications, including timestamps. Primarily used for recovery, these logs can also be leveraged to propagate changes to target systems in real time. This approach minimizes the performance impact on the source DB. As a huge advantage, it avoids additional processing overhead on the source DB, captures all data changes, and requires no schema modification. But on the opposite side, it lacks standardized log formats, leading to vendor-specific implementations.",
      "content_length": 1413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "For more details on CDC, I recommend What is Change Data Capture? from Confluent’s blog: https://www.confluent.io/en-gb/learn/change-data- capture/.\n\nWith these CDC techniques in mind, we could quickly implement a pull timestamp-based strategy in our RAG feature pipeline to sync the data warehouse and feature store more optimally when the data grows. Our implementation is still pull-based but doesn’t check any last updated field in the source DB; it just pulls everything from the data warehouse.\n\nHowever, the most popular and optimal technique in the industry is the log- based one. It doesn’t add any I/O overhead to the source DB, has low latency, and supports all CRUD operations. The biggest downside is its development complexity, which requires a queue to capture all the CRUD events and a streaming pipeline to process them.\n\nAs this is an LLM book and not a data engineering one, we wanted to keep things simple, but it’s important to know that these techniques exist, and you can always upgrade your current implementation when it doesn’t fit your application requirements anymore.",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 496,
      "content": "Why is the data stored in two snapshots?\n\nWe store two snapshots of our data in the logical feature store:\n\nAfter the data is cleaned: For fine-tuning LLMs\n\nAfter the documents are chunked and embedded: For RAG\n\nWhy did we design it this way? Remember that the features should be accessed solely from the feature store for training and inference. Thus, this adds consistency to our design and makes it cleaner.\n\nAlso, storing the data cleaned specifically for our fine-tuning and embedding use case in the MongoDB data warehouse would have been an antipattern. The data from the warehouse is shared all across the company. Thus, processing it for a specific use case is not good practice. Imagine another summarization use case where we must clean and preprocess the data differently. We must create a new “Cleaned Data” table prefixed with the use case name. We have to repeat that for every new use case. Therefore, to avoid having a spaghetti data warehouse, the data from the data warehouse is generic and is modeled to specific applications only in downstream components, which, in our case, is the feature store.",
      "content_length": 1118,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "Ultimately, as we mentioned in the Core steps section, you can leverage the metadata index of a vector DB as a NoSQL DB. Based on these factors, we decided to keep the cleaned data in Qdrant, along with the chunked and embedded versions of the documents.\n\nAs a quick reminder, when operationalizing our LLM Twin system, the create instruct dataset pipeline, explained in Chapter 5, will read the cleaned documents from Qdrant, process them, and save them under a versioned ZenML artifact. The training pipeline requires a dataset and not plain documents. This is a reminder that our logical feature store comprises the Qdrant vector DB for online serving and ZenML artifacts for offline training.",
      "content_length": 696,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "Orchestration\n\nZenML will orchestrate the batch RAG feature pipeline. Using ZenML, we can schedule it to run on a schedule, for example, every hour, or quickly manually trigger it. Another option is to trigger it after the ETL data collection pipeline finishes.\n\nBy orchestrating the feature pipeline and integrating it into ZenML (or any other orchestration tool), we can operationalize the feature pipeline with the end goal of continuous training (CT).\n\nWe will go into all the details of orchestration, scheduling, and CT in Chapter 11.",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "Implementing the LLM Twin’s RAG feature pipeline\n\nThe last step is to review the LLM Twin’s RAG feature pipeline code to see how we applied everything we discussed in this chapter. We will walk you through the following:\n\nZenML code\n\nPydantic domain objects\n\nA custom object-vector mapping (OVM) implementation\n\nThe cleaning, chunking, and embedding logic for all our data categories\n\nWe will take a top-down approach. Thus, let’s start with the Settings class and ZenML pipeline.",
      "content_length": 480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "Settings\n\nWe use Pydantic Settings (https://docs.pydantic.dev/latest/concepts/pydantic_settings/) to define a global Settings class that loads sensitive or non-sensitive variables from a\n\n.env\n\nfile. This approach also gives us all the benefits of Pydantic, such as type validation. For example, if we provide a string for the\n\nQDRANT_DATABASE_PORT\n\nvariable instead of an integer, the program will crash. This behavior makes the whole application more deterministic and reliable.\n\nHere is what the\n\nSettings\n\nclass looks like with all the variables necessary to build the RAG feature pipeline:\n\nfrom\n\npydantic\n\nimport",
      "content_length": 618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "BaseSettings\n\nclass\n\nSettings\n\n(\n\nBaseSettings\n\n):\n\nclass\n\nConfig\n\n: env_file =\n\n\".env\"\n\nenv_file_encoding =\n\n\"utf-8\"\n\n…\n\n# Some other settings…\n\n# RAG\n\nTEXT_EMBEDDING_MODEL_ID:\n\nstr\n\n=\n\n\"sentence-transformers/all-MiniLM-L6-v2\"\n\nRERANKING_CROSS_ENCODER_MODEL_ID:",
      "content_length": 262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "str\n\n=\n\n\"cross-encoder/ms-marco-MiniLM-L-4-v2\"\n\nRAG_MODEL_DEVICE:\n\nstr\n\n=\n\n\"cpu\"\n\n# QdrantDB Vector DB\n\nUSE_QDRANT_CLOUD:\n\nbool\n\n=\n\nFalse\n\nQDRANT_DATABASE_HOST:\n\nstr\n\n=\n\n\"localhost\"\n\nQDRANT_DATABASE_PORT:\n\nint\n\n=\n\n6333",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "QDRANT_CLOUD_URL:\n\nstr\n\n=\n\n\"str\"\n\nQDRANT_APIKEY:\n\nstr\n\n|\n\nNone\n\n=\n\nNone\n\n…\n\n# More settings…\n\nsettings = Settings()\n\nAs stated in the internal Config class, all the variables have default values or can be overridden by providing a\n\n.env\n\nfile.",
      "content_length": 243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "ZenML pipeline and steps\n\nThe ZenML pipeline is the entry point for the RAG feature engineering pipeline. It reflects the five core phases of RAG ingestion code: extracting raw documents, cleaning, chunking, embedding, and loading them to the logical feature store. The calls within the\n\nfeature_engineering()\n\nfunction are ZenML steps, representing a single execution unit performing the five phases of RAG. The code is available in the GitHub repository at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/pipelines/feature_engineering.py:\n\nfrom\n\nzenml\n\nimport\n\npipeline\n\nfrom\n\nllm_engineering.interfaces.orchestrator.steps\n\nimport\n\nfeature_engineering",
      "content_length": 674,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "as\n\nfe_steps\n\n@pipeline\n\ndef\n\nfeature_engineering\n\n(\n\nauthor_full_names:\n\nlist\n\n[\n\nstr\n\n]\n\n) ->\n\nNone\n\n: raw_documents = fe_steps.query_data_warehouse(author_full_names) cleaned_documents = fe_steps.clean_documents(raw_documents) last_step_1 = fe_steps.load_to_vector_db(cleaned_documents) embedded_documents = fe_steps.chunk_and_embed(cleaned_documents) last_step_2 = fe_steps.load_to_vector_db(embedded_documents)\n\nreturn\n\n[last_step_1.invocation_id, last_step_2.invocation_id]",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 510,
      "content": "Figure 4.11 shows how multiple feature engineering pipeline runs look in ZenML’s dashboard.\n\nFigure 4.11: Feature pipeline runs in the ZenML dashboard\n\nFigure 8.12 shows the DAG of the RAG feature pipeline, where you can follow all the pipeline steps and their output artifacts. Remember that whatever is returned from a ZenML step is automatically saved as an artifact, stored in ZenML’s artifact registry, versioned, and shareable across the application.",
      "content_length": 456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "Figure 4.12: Feature pipeline DAG in the ZenML dashboard\n\nThe final puzzle piece is understanding how to configure the RAG feature pipeline dynamically. All its available settings are exposed as function parameters. Here, we need only a list of author’s names, as seen in the function’s signature:\n\nfeature_engineering(author_full_names: list[str])",
      "content_length": 348,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": ". We inject a YAML configuration file at runtime that contains all the necessary values based on different use cases. For example, the following configuration includes a list of all the authors of this book as we want to populate the feature store with data from all of us (available in the GitHub repository at\n\nconfigs/feature_engineering.yaml\n\n):\n\nparameters: author_full_names: - Alex Vesa - Maxime Labonne - Paul Iusztin\n\nThe beauty of this approach is that you don’t have to modify the code to configure the feature pipeline with different input values. You have to provide a different configuration file when running it, as follows:\n\nfeature_engineering.with_options(config_path=\"…/feature_engineering.ya ml\")()\n\nYou can either hardcode the path to the config file or provide the\n\nconfig_path",
      "content_length": 799,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "from the CLI, which allows you to modify the pipeline’s configuration between different runs. Out of simplicity, we hard-coded the configuration file. Thus, we can call the feature engineering pipeline calling the\n\nrun.py\n\nscript as follows:\n\npython -m tools.run --no-cache --run-feature-engineering\n\nHowever, you can easily add another CLI argument to pass the\n\nconfig_path\n\nvariable. Also, you can run the feature pipeline using the following\n\npoe\n\ncommand:\n\npoetry poe run-feature-engineering-pipeline",
      "content_length": 504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 514,
      "content": "Let’s move forward to the ZenML steps and sequentially zoom in on all of them. The source code for all the feature engineering pipeline steps is available on GitHub at\n\n\"steps/feature_engineering\"\n\n. We will begin with the first step, which involves querying the data warehouse for new content to process into features.",
      "content_length": 319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 515,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 516,
      "content": "Querying the data warehouse\n\nThe first thing to notice is that a step is a Python function decorated with\n\n@step\n\n, similar to how a ZenML pipeline works. The function below takes as input a list of authors’ full names and performs the following core steps:\n\nIt attempts to get or create a\n\nUserDocument\n\ninstance using the first and last names, appending this instance to the authors list. If the user doesn’t exist, it throws an error.\n\nIt fetches all the raw data for the user from the data warehouse and extends the\n\ndocuments\n\nlist to include these user documents.\n\nUltimately, it computes a descriptive metadata dictionary logged and tracked in ZenML.",
      "content_length": 657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 517,
      "content": "…\n\n# other imports\n\nfrom\n\nzenml\n\nimport\n\nget_step_context, step\n\n@step\n\ndef\n\nquery_data_warehouse\n\n(\n\nauthor_full_names:\n\nlist\n\n[\n\nstr\n\n],\n\n) -> Annotated[\n\nlist\n\n,\n\n\"raw_documents\"",
      "content_length": 181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 518,
      "content": "]: documents = [] authors = []\n\nfor\n\nauthor_full_name\n\nin\n\nauthor_full_names: logger.info(\n\nf\"Querying data warehouse for user:\n\n{author_full_name}\n\n\"\n\n) first_name, last_name = utils.split_user_full_name(author_full_name) logger.info(\n\nf\"First name:\n\n{first_name}\n\n, Last name:\n\n{last_name}\n\n\"\n\n) user = UserDocument.get_or_create(first_name=first_name, last_name=last_name) authors.append(user) results = fetch_all_data(user) user_documents = [doc\n\nfor\n\nquery_result\n\nin",
      "content_length": 472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 519,
      "content": "results.values()\n\nfor\n\ndoc\n\nin\n\nquery_result] documents.extend(user_documents) step_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"raw_documents\"\n\n, metadata=_get_metadata(documents))\n\nreturn\n\ndocuments\n\nThe fetch function leverages a thread pool that runs each query on a different thread. As we have multiple data categories, we have to make a different query for the articles, posts, and repositories, as they are stored in different collections. Each query calls the data warehouse, which is bounded by the network I/O and data warehouse latency, not by the machine’s CPU. Thus, by moving each query to a different thread, we can parallelize them. Ultimately, instead of adding the latency of each query as the total timing, the time to run this fetch function will be the max between all the calls.\n\nUsing threads to parallelize I/O-bounded calls is good practice in Python, as they are not locked by the Python Global Interpreter Lock (GIL). In contrast, adding each call to a different process would add too much overhead, as a process takes longer to spin off than a thread.",
      "content_length": 1112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 520,
      "content": "In Python, you want to parallelize things with processes only when the operations are CPU or memory-bound because the GIL affects them. Each process has a different GIL. Thus, parallelizing your computing logic, such as processing a batch of documents or images already loaded in memory, isn’t affected by Python’s GIL limitations.\n\ndef\n\nfetch_all_data\n\n(\n\nuser: UserDocument\n\n) ->\n\ndict\n\n[\n\nstr\n\n,\n\nlist\n\n[NoSQLBaseDocument]]: user_id =\n\nstr\n\n(user.\n\nid",
      "content_length": 454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 521,
      "content": ")\n\nwith\n\nThreadPoolExecutor()\n\nas\n\nexecutor: future_to_query = { executor.submit(__fetch_articles, user_id):\n\n\"articles\"\n\n, executor.submit(__fetch_posts, user_id):\n\n\"posts\"\n\n, executor.submit(__fetch_repositories, user_id):\n\n\"repositories\"\n\n, } results = {}\n\nfor\n\nfuture\n\nin\n\nas_completed(future_to_query): query_name = future_to_query[future]\n\ntry\n\n: results[query_name] = future.result()\n\nexcept\n\nException: logger.exception(\n\nf\"'",
      "content_length": 433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 522,
      "content": "{query_name}\n\n' request failed.\"\n\n) results[query_name] = []\n\nreturn\n\nresults\n\nThe\n\n_get_metadata()\n\nfunction takes the list of queried documents and authors and counts the number of them relative to each data category:\n\ndef\n\n_get_metadata\n\n(\n\ndocuments:\n\nlist\n\n[Document]\n\n) ->",
      "content_length": 278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 523,
      "content": "dict\n\n: metadata = {\n\n\"num_documents\"\n\n:\n\nlen\n\n(documents), }\n\nfor\n\ndocument\n\nin\n\ndocuments: collection = document.get_collection_name()\n\nif\n\ncollection\n\nnot\n\nin\n\nmetadata: metadata[collection] = {}\n\nif\n\n\"authors\"\n\nnot\n\nin\n\nmetadata[collection]: metadata[collection][",
      "content_length": 267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 524,
      "content": "\"authors\"\n\n] =\n\nlist\n\n() metadata[collection][\n\n\"\n\nnum_documents\"\n\n] = metadata[collection].get(\n\n\"num_documents\"\n\n,\n\n0\n\n) +\n\n1\n\nmetadata[collection][\n\n\"authors\"\n\n].append(document.author_full_name)\n\nfor\n\nvalue\n\nin\n\nmetadata.values():\n\nif",
      "content_length": 238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 525,
      "content": "isinstance\n\n(value,\n\ndict\n\n)\n\nand\n\n\"authors\"\n\nin\n\nvalue: value[\n\n\"authors\"\n\n] =\n\nlist\n\n(\n\nset\n\n(value[\n\n\"authors\"\n\n]))\n\nreturn\n\nmetadata",
      "content_length": 136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 526,
      "content": "We will expose this metadata in the ZenML dashboard to quickly see some statistics on the loaded data. For example, in Figure 4.13, we accessed the metadata tab of the\n\nquery_data_warehouse()\n\nstep, where you can see that, within that particular run of the feature pipeline, we loaded 76 documents from three authors. This is extremely powerful for monitoring and debugging batch pipelines.\n\nYou can always extend it with anything that makes sense for your use case.",
      "content_length": 466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 527,
      "content": "Figure 4.13: Metadata of the “query the data warehouse” ZenML step",
      "content_length": 66,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 528,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 529,
      "content": "Cleaning the documents\n\nIn the cleaning step, we iterate through all the documents and delegate all the logic to a\n\nCleaningDispatcher\n\nwho knows what cleaning logic to apply based on the data category. Remember that we want to apply, or have the ability to apply in the future, different cleaning techniques on articles, posts, and code repositories.\n\n@step\n\ndef\n\nclean_documents\n\n(\n\ndocuments: Annotated[\n\nlist\n\n,\n\n\"\n\nraw_documents\"\n\n],",
      "content_length": 438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 530,
      "content": ") -> Annotated[\n\nlist\n\n,\n\n\"cleaned_documents\"\n\n]: cleaned_documents = []\n\nfor\n\ndocument\n\nin\n\ndocuments: cleaned_document = CleaningDispatcher.dispatch(document) cleaned_documents.append(cleaned_document) step_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"cleaned_documents\"\n\n, metadata=_get_metadata(cleaned_documents))\n\nreturn\n\ncleaned_documents\n\nThe computed metadata is similar to what we logged in the\n\nquery_data_warehouse()\n\nstep. Thus, let’s move on to chunking and embedding.",
      "content_length": 514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 531,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 532,
      "content": "Chunk and embed the cleaned documents\n\nSimilar to how we cleaned the documents, we delegate the chunking and embedding logic to a dispatcher who knows how to handle each data category. Note that the chunking dispatcher returns a list instead of a single object, which makes sense as the document is split into multiple chunks. We will dig into the dispatcher in the “The dispatcher layer” section of this chapter.\n\n@step\n\ndef\n\nchunk_and_embed\n\n(\n\ncleaned_documents: Annotated[\n\nlist\n\n,\n\n\"cleaned_documents\"\n\n],\n\n) -> Annotated[\n\nlist",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 533,
      "content": ",\n\n\"embedded_documents\"\n\n]: metadata = {\n\n\"chunking\"\n\n: {},\n\n\"embedding\"\n\n: {},\n\n\"num_documents\"\n\n:\n\nlen\n\n(cleaned_documents)} embedded_chunks = []\n\nfor\n\ndocument\n\nin\n\ncleaned_documents: chunks = ChunkingDispatcher.dispatch(document) metadata[\n\n\"chunking\"\n\n] = _add_chunks_metadata(chunks, metadata[\n\n\"chunking\"\n\n])\n\nfor",
      "content_length": 320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 534,
      "content": "batched_chunks\n\nin\n\nutils.misc.batch(chunks,\n\n10\n\n): batched_embedded_chunks = EmbeddingDispatcher.dispatch(batched_chunks) embedded_chunks.extend(batched_embedded_chunks) metadata[\n\n\"embedding\"\n\n] = _add_embeddings_metadata(embedded_chunks, metadata[\n\n\"embedding\"\n\n]) metadata[\n\n\"num_chunks\"\n\n] =\n\nlen\n\n(embedded_chunks) metadata[\n\n\"num_embedded_chunks\"\n\n] =\n\nlen\n\n(embedded_chunks) step_context = get_step_context() step_context.add_output_metadata(output_name=\n\n\"embedded_documents\"",
      "content_length": 485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 535,
      "content": ", metadata=metadata)\n\nreturn\n\nembedded_chunks\n\nIn Figure 4.14, you can see the metadata of the chunking and embedding ZenML step. For example, you can quickly understand that we transformed 76 documents into 2,373 chunks, or the properties we used for chunking articles, such as a\n\nchunk_size\n\nof 500 and a\n\nchunk_overlap\n\nof 50.",
      "content_length": 329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 536,
      "content": "Figure 4.14: Metadata of the embedding and chunking ZenML step, detailing the uncategorized and chunking dropdowns",
      "content_length": 114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 537,
      "content": "In Figure 4.15, the rest of the ZenML metadata from the embedding and chunking step details the embedding model and its properties used to compute the vectors.",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 540,
      "content": "Figure 4.15: Metadata of the embedding and chunking ZenML step, detailing the embedding dropdown\n\nAs ML systems can break at any time while in production due to drifts or untreated use cases, leveraging the metadata section to monitor the ingested data can be a powerful tool that will save debugging days, translating to tens of thousands of dollars or more for your business.",
      "content_length": 377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 541,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 542,
      "content": "Loading the documents to the vector DB\n\nAs each article, post, or code repository sits in a different collection inside the vector DB, we have to group all the documents based on their data category. Then, we load each group in bulk in the Qdrant vector DB:\n\n@step\n\ndef\n\nload_to_vector_db\n\n(\n\ndocuments: Annotated[\n\nlist\n\n,\n\n\"documents\"\n\n],\n\n) ->\n\nNone\n\n: logger.info(",
      "content_length": 368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 543,
      "content": "f\"Loading\n\n{\n\nlen\n\n(documents)}\n\ndocuments into the vector database.\"\n\n) grouped_documents = VectorBaseDocument.group_by_class(documents)\n\nfor\n\ndocument_class, documents\n\nin\n\ngrouped_documents.items(): logger.info(\n\nf\"Loading documents into\n\n{document_class.get_collection_name()}\n\n\"\n\n)\n\nfor\n\ndocuments_batch\n\nin\n\nutils.misc.batch(documents, size=\n\n4\n\n):",
      "content_length": 354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 544,
      "content": "try\n\n: document_class.bulk_insert(documents_batch)\n\nexcept\n\nException:\n\nreturn\n\nFalse\n\nreturn\n\nTrue",
      "content_length": 99,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 545,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 546,
      "content": "Pydantic domain entities\n\nBefore investigating the dispatchers, we must understand the domain objects we work with. To some extent, in implementing the LLM Twin, we are following the domain-driven design (DDD) principles, which state that domain entities are the core of your application. Thus, before proceeding, it’s important to understand the hierarchy of the domain classes we are working with.\n\nThe code for the domain entities is available on GitHub at https://github.com/PacktPublishing/LLM- Engineering/tree/main/llm_engineering/domain.\n\nWe used Pydantic to model all our domain entities. When we wrote the book, choosing Pydantic was a no-brainer, as it is the go-to Python package for writing data structures with out-of-the-box type validation. As Python is a dynamically typed language, using Pydantic for type validation at runtime makes your system order of times more robust, as you can be sure that you are always working with the right type of data.\n\nThe domain of our LLM Twin application is split into two dimensions:\n\nThe data category: Post, article, and repository",
      "content_length": 1087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 547,
      "content": "The state of the data: Cleaned, chunked, and embedded\n\nWe decided to create a base class for each state of the document, resulting in having the following base abstract classes:\n\nclass CleanedDocument(VectorBaseDocument, ABC)\n\nclass Chunk(VectorBaseDocument, ABC)\n\nclass EmbeddedChunk(VectorBaseDocument, ABC)\n\nNote that all of them inherit the\n\nVectorBaseDocument\n\nclass, which is our custom OVM implementation, which we will explain in the next section of this chapter. Also, it inherits from ABC, which makes the class abstract. Thus, you cannot initialize an object out of these classes; you may only inherit from them. That is why base classes are always marked as abstract.\n\nEach base abstract class from above (which models the state) will have a subclass that adds the data category dimension. For example, the",
      "content_length": 818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 548,
      "content": "CleanedDocument\n\nclass will have the following subclasses:\n\nclass CleanedPostDocument(CleanedDocument)\n\nclass CleanedArticleDocument(CleanedDocument)\n\nclass CleanedRepositoryDocument(CleanedDocument)\n\nAs we can see in Figure 8.16, we will repeat the same logic for the\n\nChunk\n\nand\n\nEmbeddedChunk\n\nbase abstract classes. We will implement a specific document class for each data category and state combination, resulting in nine types of domain entities. For example, when ingesting a raw document, the cleaning step will yield a\n\nCleanedArticleDocument\n\ninstance, the chunking step will return a list of\n\nArticleChunk\n\nobjects, and the embedding operation will return",
      "content_length": 667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 549,
      "content": "EmbeddedArticleChunk\n\ninstances that encapsulate the embedding and all the necessary metadata to ingest in the vector DB.\n\nThe same will happen for the posts and repositories.\n\nFigure 4.16: Domain entities class hierarchy and their interaction",
      "content_length": 243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 550,
      "content": "We chose this design because the list of states will rarely change, and we want to extend the list of data categories. Thus, structuring the classes after the state allows us to plug another data category by inheriting these base abstract classes.\n\nLet’s see the complete code for the hierarchy of the cleaned document. All the attributes of a cleaned document will be saved within the metadata of the vector DB. For example, the metadata of a cleaned article document will always contain the content, platform, author ID, author full name, and link of the article.\n\nAnother fundamental aspect is the\n\nConfig\n\ninternal class, which defines the name of the collection within the vector DB, the data category of the entity, and whether to leverage the vector index when creating the collection:\n\nclass\n\nCleanedDocument\n\n(VectorBaseDocument, ABC): content:\n\nstr\n\nplatform:\n\nstr",
      "content_length": 874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 551,
      "content": "author_id: UUID4 author_full_name:\n\nstr\n\nclass\n\nCleanedPostDocument\n\n(\n\nCleanedDocument\n\n): image:\n\nOptional\n\n[\n\nstr\n\n] =\n\nNone\n\nclass\n\nConfig\n\n: name =\n\n\"cleaned_posts\"\n\ncategory = DataCategory.POSTS use_vector_index =\n\nFalse\n\nclass\n\nCleanedArticleDocument",
      "content_length": 257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 552,
      "content": "(\n\nCleanedDocument\n\n): link:\n\nstr\n\nclass\n\nConfig\n\n: name =\n\n\"cleaned_articles\"\n\ncategory = DataCategory.ARTICLES use_vector_index =\n\nFalse\n\nclass\n\nCleanedRepositoryDocument\n\n(\n\nCleanedDocument\n\n): name:\n\nstr\n\nlink:\n\nstr\n\nclass\n\nConfig",
      "content_length": 234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 553,
      "content": ": name =\n\n\"cleaned_repositories\"\n\ncategory = DataCategory.REPOSITORIES use_vector_index =\n\nFalse\n\nTo conclude this section, let’s also take a look at the base abstract class of the chunk and embedded chunk:\n\nclass\n\nChunk\n\n(VectorBaseDocument, ABC): content:\n\nstr\n\nplatform:\n\nstr\n\ndocument_id: UUID4 author_id: UUID4 author_full_name:\n\nstr\n\nmetadata:\n\ndict",
      "content_length": 355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 554,
      "content": "= Field(default_factory=\n\ndict\n\n) …\n\n# PostChunk, ArticleChunk, RepositoryChunk\n\nclass\n\nEmbeddedChunk\n\n(VectorBaseDocument, ABC): content:\n\nstr\n\nembedding:\n\nlist\n\n[\n\nfloat\n\n] |\n\nNone\n\nplatform:\n\nstr\n\ndocument_id: UUID4 author_id: UUID4 author_full_name:\n\nstr\n\nmetadata:\n\ndict",
      "content_length": 275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 555,
      "content": "= Field(default_factory=\n\ndict\n\n) …\n\n# EmbeddedPostChunk, EmbeddedArticleChunk, EmbeddedRepositoryChunk\n\nWe also defined an enum that aggregates all our data categories in a single structure of constants:\n\nclass\n\nDataCategory\n\n(\n\nStrEnum\n\n): POSTS =\n\n\"posts\"\n\nARTICLES =\n\n\"articles\"\n\nREPOSITORIES =\n\n\"repositories\"",
      "content_length": 314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 556,
      "content": "The last step to fully understand how the domain objects work is to zoom into the\n\nVectorBaseDocument\n\nOVM class.",
      "content_length": 113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 557,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 558,
      "content": "OVM\n\nThe term OVM is inspired by the object-relational mapping (ORM) pattern we discussed in Chapter 3. We called it OVM because we work with embedding and vector DBs instead of structured data and SQL tables. Otherwise, it follows the same principles as an ORM pattern.\n\nSimilar to what we did in Chapter 3, we will implement our own OVM version. Even if our custom example is simple, it’s a powerful example of how to write modular and extendable classes by leveraging OOP best practices and principles.\n\nThe full implementation of the\n\nVectorBaseDocument\n\nclass is available on GitHub at https://github.com/PacktPublishing/LLM- Engineering/blob/main/llm_engineering/domain/base/vector.py.\n\nOur OVM base class is called\n\nVectorBaseDocument\n\n. It will support CRUD operations on top of Qdrant. Based on our application’s demands, we limited it only to create and read operations, but it can easily be extended to update and delete functions.",
      "content_length": 942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 559,
      "content": "Let’s take a look at the definition of the\n\nVectorBaseDocument\n\nclass:\n\nfrom\n\npydantic\n\nimport\n\nUUID4, BaseModel\n\nfrom\n\ntyping\n\nimport\n\nGeneric\n\nfrom\n\nllm_engineering.infrastructure.db.qdrant\n\nimport\n\nconnection T = TypeVar(\n\n\"T\"\n\n, bound=",
      "content_length": 239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 560,
      "content": "\"VectorBaseDocument\"\n\n)\n\nclass\n\nVectorBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC):\n\nid\n\n: UUID4 = Field(default_factory=uuid.uuid4)\n\n@classmethod\n\ndef\n\nfrom_record\n\n(\n\ncls:\n\nType\n\n[T], point: Record\n\n) -> T: _\n\nid\n\n= UUID(point.\n\nid",
      "content_length": 234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 561,
      "content": ", version=\n\n4\n\n) payload = point.payload\n\nor\n\n{} attributes = {\n\n\"id\"\n\n: _\n\nid\n\n, **payload, }\n\nif\n\ncls._has_class_attribute(\n\n\"embedding\"\n\n): payload[\n\n\"embedding\"\n\n] = point.vector\n\nor\n\nNone\n\nreturn\n\ncls(**attributes)\n\ndef",
      "content_length": 224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 562,
      "content": "to_point\n\n(\n\nself: T, **kwargs\n\n) -> PointStruct: exclude_unset = kwargs.pop(\n\n\"exclude_unset\"\n\n,\n\nFalse\n\n) by_alias = kwargs.pop(\n\n\"\n\nby_alias\"\n\n,\n\nTrue\n\n) payload =\n\nself\n\n.\n\ndict\n\n(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs) _\n\nid\n\n=\n\nstr",
      "content_length": 255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 563,
      "content": "(payload.pop(\n\n\"id\"\n\n)) vector = payload.pop(\n\n\"embedding\"\n\n, {})\n\nif\n\nvector\n\nand\n\nisinstance\n\n(vector, np.ndarray): vector = vector.tolist()\n\nreturn\n\nPointStruct(\n\nid\n\n=_\n\nid\n\n, vector=vector, payload=payload)\n\nThe\n\nVectorBaseDocument",
      "content_length": 236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 564,
      "content": "class inherits from Pydantic’s\n\nBaseModel\n\nand helps us structure a single record’s attributes from the vector DB. Every OVM will be initialized by default with UUID4 as its unique identifier. Using generics—more precisely, by inheriting from\n\nGeneric[T]\n\n—the signatures of all the subclasses of the\n\nVectorBaseDocument\n\nclass will adapt to that given class. For example, the\n\nfrom_record()\n\nmethod of the\n\nChunk()\n\nclass, which inherits\n\nVectorBaseDocument\n\n, will return the Chunk type, which drastically helps the static analyzer and type checkers such as\n\nmypy\n\n(https://mypy.readthedocs.io/en/stable/).\n\nThe\n\nfrom_record()",
      "content_length": 628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 565,
      "content": "method adapts a data point from Qdrant’s format to our internal structure based on Pydantic. On the other hand, the\n\nto_point()\n\nmethod takes the attributes of the current instance and adapts them to Qdrant’s\n\nPointStruct()\n\nformat. We will leverage these two methods for our create and read operations.\n\nUltimately, all operations made to Qdrant will be done through the\n\nconnection\n\ninstance, which is instantiated in the application’s infrastructure layer.\n\nThe\n\nbulk_insert()\n\nmethod maps each document to a point. Then, it uses the Qdrant\n\nconnection\n\ninstance to load all the points to a given collection in Qdrant. If the insertion fails once, it tries to create the collection and do the insertion again. Often, it is good practice to split your logic into two functions. One private function contains the logic, in our case\n\n_bulk_insert()\n\n, and one public function handles all the errors and failure scenarios.",
      "content_length": 921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 566,
      "content": "class\n\nVectorBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC): …\n\n# Rest of the class\n\n@classmethod\n\ndef\n\nbulk_insert\n\n(\n\ncls:\n\nType\n\n[T], documents:\n\nlist\n\n[\n\n\"VectorBaseDocument\"\n\n]\n\n) ->",
      "content_length": 186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 567,
      "content": "bool\n\n:\n\ntry\n\n: cls._bulk_insert(documents)\n\nexcept\n\nexceptions.UnexpectedResponse: logger.info(\n\nf\"Collection '\n\n{cls.get_collection_name()}\n\n' does not exist. Trying to create the collection and reinsert the documents.\"\n\n) cls.create_collection()\n\ntry\n\n: cls._bulk_insert(documents)\n\nexcept\n\nexceptions.UnexpectedResponse: logger.error(\n\nf\"Failed to insert documents in '\n\n{cls.get_collection_name()}\n\n'.\"\n\n)\n\nreturn\n\nFalse",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 568,
      "content": "return\n\nTrue\n\n@classmethod\n\ndef\n\n_bulk_insert\n\n(\n\ncls:\n\nType\n\n[T], documents:\n\nlist\n\n[\n\n\"VectorBaseDocument\"\n\n]\n\n) ->\n\nNone\n\n: points = [doc.to_point()\n\nfor\n\ndoc\n\nin\n\ndocuments] connection.upsert(collection_name=cls.get_collection_name(), points=points)",
      "content_length": 253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 569,
      "content": "The collection name is inferred from the\n\nConfig\n\nclass defined in the subclasses inheriting the OVM:\n\nclass\n\nVectorBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC): …\n\n# Rest of the class\n\n@classmethod\n\ndef\n\nget_collection_name\n\n(\n\ncls:\n\nType\n\n[T]",
      "content_length": 245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 570,
      "content": ") ->\n\nstr\n\n:\n\nif\n\nnot\n\nhasattr\n\n(cls,\n\n\"Config\"\n\n)\n\nor\n\nnot\n\nhasattr\n\n(cls.Config,\n\n\"name\"\n\n):\n\nraise\n\nImproperlyConfigured(\n\n\"The class should define a Config class with\"\n\n\"the 'name' property that reflects the collection's name.\"\n\n)",
      "content_length": 234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 571,
      "content": "return\n\ncls.Config.name\n\nNow, we must define a method that lets us read all the records from the vector DB (without using vector similarity search logic). The\n\nbulk_find()\n\nmethod enables us to scroll (or list) all the records from a collection. The function below scrolls the Qdrant vector DB, which returns a list of data points, which are ultimately mapped to our internal structure using the\n\nfrom_record()\n\nmethod.\n\nThe limit parameters control how many items we return at once, and the offset signals the ID of the point from which Qdrant starts returning records.\n\nclass\n\nVectorBaseDocument\n\n(BaseModel,\n\nGeneric",
      "content_length": 619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 572,
      "content": "[T], ABC): …\n\n# Rest of the class\n\n@classmethod\n\ndef\n\nbulk_find\n\n(\n\ncls:\n\nType\n\n[T], limit:\n\nint\n\n=\n\n10\n\n, **kwargs\n\n) ->\n\ntuple\n\n[\n\nlist\n\n[T], UUID |\n\nNone\n\n]:",
      "content_length": 160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 573,
      "content": "try\n\n: documents, next_offset = cls._bulk_find(limit=limit, **kwargs)\n\nexcept\n\nexceptions.UnexpectedResponse: logger.error(\n\nf\"Failed to search documents in '\n\n{cls.get_collection_name()}\n\n'.\"\n\n) documents, next_offset = [],\n\nNone\n\nreturn\n\ndocuments, next_offset\n\n@classmethod\n\ndef\n\n_bulk_find\n\n(\n\ncls:\n\nType\n\n[T], limit:\n\nint\n\n=",
      "content_length": 329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 574,
      "content": "10\n\n, **kwargs\n\n) ->\n\ntuple\n\n[\n\nlist\n\n[T], UUID |\n\nNone\n\n]: collection_name = cls.get_collection_name() offset = kwargs.pop(\n\n\"offset\"\n\n,\n\nNone\n\n) offset =\n\nstr\n\n(offset)\n\nif\n\noffset\n\nelse\n\nNone\n\nrecords, next_offset = connection.scroll( collection_name=collection_name, limit=limit, with_payload=kwargs.pop(",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 575,
      "content": "\"\n\nwith_payload\"\n\n,\n\nTrue\n\n), with_vectors=kwargs.pop(\n\n\"with_vectors\"\n\n,\n\nFalse\n\n), offset=offset, **kwargs, ) documents = [cls.from_record(record)\n\nfor\n\nrecord\n\nin\n\nrecords]\n\nif\n\nnext_offset\n\nis\n\nnot\n\nNone\n\n: next_offset = UUID(next_offset, version=\n\n4",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 576,
      "content": ")\n\nreturn\n\ndocuments, next_offset\n\nThe last piece of the puzzle is to define a method that performs a vector similarity search on a provided query embedding. Like before, we defined a public\n\nsearch()\n\nand private\n\n_search()\n\nmethod. The search is performed by Qdrant when calling the\n\nconnection.search()\n\nfunction.\n\nclass\n\nVectorBaseDocument\n\n(BaseModel,\n\nGeneric\n\n[T], ABC): …",
      "content_length": 379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 577,
      "content": "# Rest of the class\n\n@classmethod\n\ndef\n\nsearch\n\n(\n\ncls:\n\nType\n\n[T], query_vector:\n\nlist\n\n, limit:\n\nint\n\n=\n\n10\n\n, **kwargs\n\n) ->\n\nlist\n\n[T]:\n\ntry\n\n: documents = cls._search(query_vector=query_vector, limit=limit, **kwargs)\n\nexcept",
      "content_length": 229,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 578,
      "content": "exceptions.UnexpectedResponse: logger.error(\n\nf\"Failed to search documents in '\n\n{cls.get_collection_name()}\n\n'.\"\n\n) documents = []\n\nreturn\n\ndocuments\n\n@classmethod\n\ndef\n\n_search\n\n(\n\ncls:\n\nType\n\n[T], query_vector:\n\nlist\n\n, limit:\n\nint\n\n=\n\n10\n\n, **kwargs",
      "content_length": 253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 579,
      "content": ") ->\n\nlist\n\n[T]: collection_name = cls.get_collection_name() records = connection.search( collection_name=collection_name, query_vector=query_vector, limit=limit, with_payload=kwargs.pop(\n\n\"with_payload\"\n\n,\n\nTrue\n\n), with_vectors=kwargs.pop(\n\n\"with_vectors\"\n\n,\n\nFalse\n\n), **kwargs, ) documents = [cls.from_record(record)\n\nfor\n\nrecord\n\nin\n\nrecords]\n\nreturn\n\ndocuments",
      "content_length": 366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 580,
      "content": "Now that we understand what our domain entities look like and how the OVM works, let’s move on to the dispatchers who clean, chunk, and embed the documents.",
      "content_length": 156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 581,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 582,
      "content": "The dispatcher layer\n\nA dispatcher inputs a document and applies dedicated handlers based on its data category (article, post, or repository). A handler can either clean, chunk, or embed a document.\n\nLet’s start by zooming in on the\n\nCleaningDispatcher\n\n. It mainly implements a\n\ndispatch()\n\nmethod that inputs a raw document. Based on its data category, it instantiates and calls a handler that applies the cleaning logic specific to that data point:\n\nclass\n\nCleaningDispatcher\n\n: cleaning_factory = CleaningHandlerFactory()\n\n@classmethod\n\ndef",
      "content_length": 544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 583,
      "content": "dispatch\n\n(\n\ncls, data_model: NoSQLBaseDocument\n\n) -> VectorBaseDocument: data_category = DataCategory(data_model.get_collection_name()) handler = cls.cleaning_factory.create_handler(data_category) clean_model = handler.clean(data_model) logger.info(\n\n\"Data cleaned successfully.\"\n\n, data_category=data_category, cleaned_content_len=\n\nlen\n\n(clean_model.content), )\n\nreturn\n\nclean_model\n\nThe key in the dispatcher logic is the\n\nCleaningHandlerFactory()\n\n, which instantiates a different cleaning handler based on the document’s data category:\n\nclass",
      "content_length": 548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 584,
      "content": "CleaningHandlerFactory\n\n:\n\n@staticmethod\n\ndef\n\ncreate_handler\n\n(\n\ndata_category: DataCategory\n\n) -> CleaningDataHandler:\n\nif\n\ndata_category == DataCategory.POSTS:\n\nreturn\n\nPostCleaningHandler()\n\nelif\n\ndata_category == DataCategory.ARTICLES:\n\nreturn\n\nArticleCleaningHandler()\n\nelif\n\ndata_category == DataCategory.REPOSITORIES:\n\nreturn\n\nRepositoryCleaningHandler()",
      "content_length": 362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 585,
      "content": "else\n\n:\n\nraise\n\nValueError(\n\n\"Unsupported data type\"\n\n)\n\nThe Dispatcher or Factory classes are nothing fancy, but they offer an intuitive and simple interface for applying various operations to your documents. When manipulating documents, instead of worrying about their data category and polluting your business logic with if-else statements, you have a class dedicated to handling that. You have a single class that cleans any document, which respects the DRY (don’t repeat yourself) principles from software engineering. By respecting DRY, you have a single point of failure, and the code can easily be extended. For example, if we add an extra type, we must extend only the Factory class instead of multiple occurrences in the code.\n\nThe\n\nChunkingDispatcher\n\nand\n\nEmbeddingDispatcher\n\nfollow the same pattern. They use a",
      "content_length": 824,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 586,
      "content": "ChunkingHandlerFactory\n\nand, respectively, an\n\nEmbeddingHandlerFactory\n\nthat initializes the correct handler based on the data category of the input document. Afterward, they call the handler and return the result.\n\nThe source code of all the dispatchers and factories can be found on GitHub at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/preprocessing/dispatcher s.py\n\nThe Factory class leverages theabstract factory creational pattern (https://refactoring.guru/design-patterns/abstract-factory), which instantiates a family of classes implementing the same interface. In our case, these handlers implement the\n\nclean()\n\nmethod regardless of the handler type.\n\nAlso, the Handler class family leverages the strategy behavioral pattern (https://refactoring.guru/design-patterns/strategy) used to instantiate when you want to use different variants of an algorithm within an object and be able to switch from one algorithm to another during runtime.",
      "content_length": 1000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 587,
      "content": "Intuitively, in our dispatcher layer, the combination of the factory and strategy patterns works as follows:\n\nInitially, we knew we wanted to clean the data, but as we knew the data category only at runtime, we couldn’t decide on what strategy to apply.\n\nWe can write the whole code around the cleaning code and abstract away the logic under a\n\nHandler()\n\ninterface, which will represent our strategy.\n\nWhen we get a data point, we apply the abstract factory pattern and create the correct cleaning handler for its data type.\n\nUltimately, the dispatcher layer uses the handler and executes the right strategy.\n\nBy doing so, we:\n\nIsolate the logic for a given data category.",
      "content_length": 673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 588,
      "content": "Leverage polymorphism to avoid filling up the code with hundreds of\n\nif-else\n\nstatements.\n\nMake the code modular and extendable. When a new data category arrives, we must implement a new handler and modify the Factory class without touching any other part of the code.\n\nUntil now, we have just modeled our entities and how the data flows in our application. We haven’t written a single piece of cleaning, chunking, or embedding code. That is one big difference between a quick demo and a production-ready application. In a demo, you don’t care about software engineering best practices and structuring your code to make it future-proof. However, writing clean, modular, and scalable code is critical for its longevity when building a real-world application.\n\nThe last component of the RAG feature pipeline is the implementation of the cleaning, chunking, and embedding handlers.",
      "content_length": 878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 589,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 590,
      "content": "The handlers\n\nThe handler has a one-on-one structure with our domain, meaning that every entity has its own handler, as shown in Figure 8.17. In total, we will have nine Handler classes that follow the next base interfaces:\n\nclass CleaningDataHandler()\n\nclass ChunkingDataHandler()\n\nclass EmbeddingDataHandler()",
      "content_length": 311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 591,
      "content": "Figure 4.17: Handler class hierarchy and their interaction\n\nThe code for all the handlers is available on GitHub at https://github.com/PacktPublishing/LLM- Engineering/tree/main/llm_engineering/application/preprocessing.",
      "content_length": 220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 592,
      "content": "Let’s examine each handler family and see how it is implemented.",
      "content_length": 64,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 593,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 594,
      "content": "The cleaning handlers\n\nThe\n\nCleaningDataHandler()\n\nstrategy interface looks as follows:\n\n…\n\n# Other imports.\n\nfrom\n\ntyping\n\nimport\n\nGeneric\n\n, TypeVar DocumentT = TypeVar(\n\n\"DocumentT\"\n\n, bound=Document) CleanedDocumentT = TypeVar(\n\n\"CleanedDocumentT\"\n\n, bound=CleanedDocument)\n\nclass",
      "content_length": 284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 595,
      "content": "CleaningDataHandler\n\n(ABC,\n\nGeneric\n\n[DocumentT, CleanedDocumentT]):\n\n@abstractmethod\n\ndef\n\nclean\n\n(\n\nself, data_model: DocumentT\n\n) -> CleanedDocumentT:\n\npass\n\nNow, for every post, article and repository, we have to implement a different handler, as follows:\n\nclass\n\nPostCleaningHandler\n\n(",
      "content_length": 290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 596,
      "content": "CleaningDataHandler\n\n):\n\ndef\n\nclean\n\n(\n\nself, data_model: PostDocument\n\n) -> CleanedPostDocument:\n\nreturn\n\nCleanedPostDocument(\n\nid\n\n=data_model.\n\nid\n\n, content=clean_text(\n\n\" #### \"\n\n.join(data_model.content.values())), …\n\n# Copy the rest of the parameters from the data_model object.\n\n)\n\nclass\n\nArticleCleaningHandler\n\n(",
      "content_length": 322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 597,
      "content": "CleaningDataHandler\n\n):\n\ndef\n\nclean\n\n(\n\nself, data_model: ArticleDocument\n\n) -> CleanedArticleDocument: valid_content = [content\n\nfor\n\ncontent\n\nin\n\ndata_model.content.values()\n\nif\n\ncontent]\n\nreturn\n\nCleanedArticleDocument(\n\nid\n\n=data_model.\n\nid\n\n, content=clean_text(\n\n\" #### \"",
      "content_length": 277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 598,
      "content": ".join(valid_content)), platform=data_model.platform, link=data_model.link, author_id=data_model.author_id, author_full_name=data_model.author_full_name, )\n\nclass\n\nRepositoryCleaningHandler\n\n(\n\nCleaningDataHandler\n\n):\n\ndef\n\nclean\n\n(\n\nself, data_model: RepositoryDocument\n\n) -> CleanedRepositoryDocument:\n\nreturn\n\nCleanedRepositoryDocument(\n\nid\n\n=data_model.\n\nid\n\n, content=clean_text(\n\n\" #### \"\n\n.join(data_model.content.values())), …",
      "content_length": 433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 599,
      "content": "# Copy the rest of the parameters from the data_model object.\n\n)\n\nThe handlers input a raw document domain entity, clean the content, and return a cleaned document. All the handlers use the\n\nclean_text()\n\nfunction to clean the text. Out of simplicity, we used the same cleaning technique for all the data categories. Still, in a real-world setup, we would have to further optimize and create a different cleaning function for each data category. The strategy pattern makes this a breeze, as we swap the cleaning function in the handlers, and that’s it.\n\nThe cleaning steps applied in the\n\nclean_text()\n\nfunction are the same ones discussed in Chapter 5 in the Creating an instruction dataset section. We don’t want to repeat ourselves. Thus, for a refresher, check out that chapter. At this point, we mostly care about automating and integrating the whole logic into the RAG feature pipeline. Thus, after operationalizing the ML system, all the cleaned data used for fine-tuning will be accessed from the logical feature store, making it the single source of truth for accessing data.",
      "content_length": 1084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 600,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 601,
      "content": "The chunking handlers\n\nFirst, let’s examine the\n\nChunkingDataHandler()\n\nstrategy handler. We exposed the\n\nmetadata\n\ndictionary as a property to aggregate all the necessary properties required for chunking in a single structure. By structuring it like this, we can easily log everything to ZenML to track and debug our chunking logic. The handler takes cleaned documents as input and returns chunk entities. All the handlers can be found on GitHub at https://github.com/PacktPublishing/LLM- Engineering/tree/main/llm_engineering/application/preprocessing.\n\n…\n\n# Other imports.\n\nfrom\n\ntyping\n\nimport\n\nGeneric",
      "content_length": 606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 602,
      "content": ", TypeVar CleanedDocumentT = TypeVar(\n\n\"CleanedDocumentT\"\n\n, bound=CleanedDocument) ChunkT = TypeVar(\n\n\"ChunkT\"\n\n, bound=Chunk)\n\nclass\n\nChunkingDataHandler\n\n(ABC,\n\nGeneric\n\n[CleanedDocumentT, ChunkT]):\n\n@property\n\ndef\n\nmetadata\n\n(\n\nself\n\n) ->\n\ndict\n\n:\n\nreturn\n\n{",
      "content_length": 262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 603,
      "content": "\"chunk_size\"\n\n:\n\n500\n\n,\n\n\"chunk_overlap\"\n\n:\n\n50\n\n, }\n\n@abstractmethod\n\ndef\n\nchunk\n\n(\n\nself, data_model: CleanedDocumentT\n\n) ->\n\nlist\n\n[ChunkT]:\n\npass\n\nLet’s understand how the",
      "content_length": 175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 604,
      "content": "ArticleChunkingHandler()\n\nclass is implemented. The first step is to override the metadata property and customize the type of properties the chunking logic requires. For example, when working with articles, we are interested in the chunk’s minimum and maximum length.\n\nThe handler’s\n\nchunk()\n\nmethod inputs cleaned article documents and returns a list of article chunk entities. It uses the\n\nchunk_text()\n\nfunction to split the cleaned content into chunks. The chunking function is customized based on the\n\nmin_length\n\nand\n\nmax_length\n\nmetadata fields. The chunk_id is computed as the MD5 hash of the chunk’s content. Thus, if the two chunks have precisely the same content, they will have the same ID, and we can easily deduplicate them. Lastly, we create a list of chunk entities and return them.\n\nclass\n\nArticleChunkingHandler",
      "content_length": 829,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 605,
      "content": "(\n\nChunkingDataHandler\n\n):\n\n@property\n\ndef\n\nmetadata\n\n(\n\nself\n\n) ->\n\ndict\n\n:\n\nreturn\n\n{\n\n\"min_length\"\n\n:\n\n1000\n\n,\n\n\"max_length\"\n\n:\n\n1000",
      "content_length": 136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 606,
      "content": ", }\n\ndef\n\nchunk\n\n(\n\nself, data_model: CleanedArticleDocument\n\n) ->\n\nlist\n\n[ArticleChunk]: data_models_list = [] cleaned_content = data_model.content chunks = chunk_article( cleaned_content, min_length=\n\nself\n\n.metadata[\n\n\"min_length\"\n\n], max_length=\n\nself\n\n.metadata[\n\n\"max_length\"\n\n] )\n\nfor\n\nchunk\n\nin",
      "content_length": 302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 607,
      "content": "chunks: chunk_id = hashlib.md5(chunk.encode()).hexdigest() model = ArticleChunk(\n\nid\n\n=UUID(chunk_id, version=\n\n4\n\n), content=chunk, platform=data_model.platform, link=data_model.link, document_id=data_model.\n\nid\n\n, author_id=data_model.author_id, author_full_name=data_model.author_full_name, metadata=\n\nself\n\n.metadata, ) data_models_list.append(model)\n\nreturn\n\ndata_models_list\n\nThe last step is to dig into the\n\nchunk_article()\n\nfunction, which mainly does two things:\n\nIt uses a regex to find all the sentences within the given text by looking for periods, question marks, or exclamation points followed by a space.",
      "content_length": 620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 608,
      "content": "However, it avoids splitting into cases where the punctuation is part of an abbreviation or initialism (like “\n\ne.g.\n\n\" or “\n\nDr.\n\n\")\n\nIt groups sentences into a single chunk until the\n\nmax_length\n\nlimit is reached. When the maximum size is reached, and the chunk size is bigger than the minimum allowed value, it is added to the final list the function returns.\n\ndef\n\nchunk_article\n\n(\n\ntext:\n\nstr\n\n, min_length:\n\nint",
      "content_length": 417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 609,
      "content": ", max_length:\n\nint\n\n) ->\n\nlist\n\n[\n\nstr\n\n]: sentences = re.split(\n\nr\"(?\n\n, text) extracts = [] current_chunk =\n\n\"\"\n\nfor\n\nsentence\n\nin\n\nsentences: sentence = sentence.strip()\n\nif\n\nnot\n\nsentence:\n\ncontinue\n\nif\n\nlen",
      "content_length": 211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 610,
      "content": "(current_chunk) +\n\nlen\n\n(sentence) <= max_length: current_chunk += sentence +\n\n\" \"\n\nelse\n\n:\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip()) current_chunk = sentence +\n\n\" \"\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip())\n\nreturn\n\nextracts\n\nThe\n\nPostChunkingHandler",
      "content_length": 327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 611,
      "content": "and\n\nRepositoryChunkingHandler\n\n, available on GitHub at\n\nllm_engineering/application/preprocessing/chunking_data_handlers.py\n\n, have a similar structure to the ArticleChunkingHandler. However, they use a more generic chunking function called\n\nchunk_text()\n\n, worth looking into. The\n\nchunk_text()\n\nfunction is a two-step process that has the following logic:\n\nIt uses a\n\nRecursiveCharacterTextSplitter()\n\nfrom LangChain to split the text based on a given separator or chunk size. Using the separator, we first try to find paragraphs in the given text, but if there are no paragraphs or they are too long, we cut it at a given chunk size.\n\nNotice that we want to ensure that the chunk doesn’t exceed the maximum input length of the embedding model. Thus, we pass all the chunks created above into a\n\nSenteceTransformersTokenTextSplitter()",
      "content_length": 838,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 612,
      "content": ", which considers the maximum input length of the model. At this point, we also apply the\n\nchunk_overlap\n\nlogic, as we want to do it only after we validate that the chunk is small enough.\n\n…\n\n# Other imports.\n\nfrom\n\nlangchain.text_splitter\n\nimport\n\nRecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n\nfrom\n\nllm_engineering.application.networks\n\nimport\n\nEmbeddingModelSingleton\n\ndef\n\nchunk_text\n\n(\n\ntext:",
      "content_length": 422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 613,
      "content": "str\n\n, chunk_size:\n\nint\n\n=\n\n500\n\n, chunk_overlap:\n\nint\n\n=\n\n50\n\n) ->\n\nlist\n\n[\n\nstr\n\n]: character_splitter = RecursiveCharacterTextSplitter(separators=[\n\n\"\\n\\n\"\n\n], chunk_size=chunk_size, chunk_overlap=\n\n0\n\n) text_split_by_characters = character_splitter.split_text(text) token_splitter = SentenceTransformersTokenTextSplitter( chunk_overlap=chunk_overlap, tokens_per_chunk=embedding_model.max_input_length, model_name=embedding_model.model_id, ) chunks_by_tokens = []",
      "content_length": 466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 614,
      "content": "for\n\nsection\n\nin\n\ntext_split_by_characters: chunks_by_tokens.extend(token_splitter.split_text(section))\n\nreturn\n\nchunks_by_tokens\n\nTo conclude, the function above returns a list of chunks that respect both the provided chunk parameters and the embedding model’s max input length.",
      "content_length": 279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 615,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 616,
      "content": "The embedding handlers\n\nThe embedding handlers differ slightly from the others as the\n\nEmbeddingDataHandler()\n\ninterface contains most of the logic. We took this approach because, when calling the embedding model, we want to batch as many samples as possible to optimize the inference process. When running the model on a GPU, the batched samples are processed independently and in parallel. Thus, by batching the chunks, we can optimize the inference process by 10x or more, depending on the batch size and hardware we use.\n\nWe implemented an\n\nembed()\n\nmethod, in case you want to run the inference on a single data point, and an\n\nembed_batch()\n\nmethod. The\n\nembed_batch()\n\nmethod takes chunked documents as input, gathers their content into a list, passes them to the embedding model, and maps the results to an embedded chunk domain entity. The mapping is done through the\n\nmap_model()\n\nabstract method, which has to be customized for every data category.",
      "content_length": 958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 617,
      "content": "…\n\n# Other imports.\n\nfrom\n\ntyping\n\nimport\n\nGeneric\n\n, TypeVar, cast\n\nfrom\n\nllm_engineering.application.networks\n\nimport\n\nEmbeddingModelSingleton ChunkT = TypeVar(\n\n\"ChunkT\"\n\n, bound=Chunk) EmbeddedChunkT = TypeVar(\n\n\"EmbeddedChunkT\"\n\n, bound=EmbeddedChunk) embedding_model = EmbeddingModelSingleton()\n\nclass\n\nEmbeddingDataHandler",
      "content_length": 329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 618,
      "content": "(ABC,\n\nGeneric\n\n[ChunkT, EmbeddedChunkT]):\n\n\"\"\"\n\nAbstract class for all embedding data handlers.\n\nAll data transformations logic for the embedding step is done here\n\n\"\"\"\n\ndef\n\nembed\n\n(\n\nself, data_model: ChunkT\n\n) -> EmbeddedChunkT:\n\nreturn\n\nself\n\n.embed_batch([data_model])[\n\n0\n\n]\n\ndef\n\nembed_batch\n\n(",
      "content_length": 302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 619,
      "content": "self, data_model:\n\nlist\n\n[ChunkT]\n\n) ->\n\nlist\n\n[EmbeddedChunkT]: embedding_model_input = [data_model.content\n\nfor\n\ndata_model\n\nin\n\ndata_model] embeddings = embedding_model(embedding_model_input, to_list=\n\nTrue\n\n) embedded_chunk = [\n\nself\n\n.map_model(data_model, cast(\n\nlist\n\n[\n\nfloat\n\n], embedding))\n\nfor\n\ndata_model, embedding",
      "content_length": 327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 620,
      "content": "in\n\nzip\n\n(data_model, embeddings, strict=\n\nFalse\n\n) ]\n\nreturn\n\nembedded_chunk\n\n@abstractmethod\n\ndef\n\nmap_model\n\n(\n\nself, data_model: ChunkT, embedding:\n\nlist\n\n[\n\nfloat\n\n]\n\n) -> EmbeddedChunkT:\n\npass",
      "content_length": 198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 621,
      "content": "Let’s look only at the implementation of the\n\nArticleEmbeddingHandler()\n\n, as the other handlers are highly similar. As you can see, we only have to implement the\n\nmap_model()\n\nmethod, which takes a chunk of input and computes the embeddings in batch mode. Its scope is to map this information to an\n\nEmbeddedArticleChunk\n\nPydantic entity.\n\nclass\n\nArticleEmbeddingHandler\n\n(\n\nEmbeddingDataHandler\n\n):\n\ndef\n\nmap_model\n\n(\n\nself, data_model: ArticleChunk, embedding:\n\nlist",
      "content_length": 469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 622,
      "content": "[\n\nfloat\n\n]\n\n) -> EmbeddedArticleChunk:\n\nreturn\n\nEmbeddedArticleChunk(\n\nid\n\n=data_model.\n\nid\n\n, content=data_model.content, embedding=embedding, platform=data_model.platform, link=data_model.link, document_id=data_model.document_id, author_id=data_model.author_id, author_full_name=data_model.author_full_name, metadata={\n\n\"embedding_model_id\"\n\n: embedding_model.model_id,\n\n\"embedding_size\"\n\n: embedding_model.embedding_size,\n\n\"max_input_length\"\n\n: embedding_model.max_input_length, }, )",
      "content_length": 487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 623,
      "content": "The last step is to understand how the\n\nEmbeddingModelSingleton()\n\nworks. It is a wrapper over the\n\nSentenceTransformer()\n\nclass from Sentence Transformers that initializes the embedding model. Writing a wrapper over external packages is often good practice. Thus, when you want to change the third-party tool, you have to modify only the internal logic of the wrapper instead of the whole code base.\n\nThe\n\nSentenceTransformer()\n\nclass is initialized with the\n\nmodel_id\n\ndefined in the\n\nSettings\n\nclass, allowing us to quickly test multiple embedding models just by changing the configuration file and not the code. That is why I am not insisting at all on what embedding model to use. This differs constantly based on your use case, data, hardware, and latency. But by writing a generic class, which can quickly be configured, you can experiment with multiple embedding models until you find the best one for you.\n\nfrom",
      "content_length": 920,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 624,
      "content": "sentence_transformers.SentenceTransformer\n\nimport\n\nSentenceTransformer\n\nfrom\n\nllm_engineering.settings\n\nimport\n\nsettings\n\nfrom\n\n.base\n\nimport\n\nSingletonMeta\n\nclass\n\nEmbeddingModelSingleton\n\n(metaclass=SingletonMeta):\n\ndef\n\n__init__\n\n(\n\nself,\n\nmodel_id:\n\nstr",
      "content_length": 257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 625,
      "content": "= settings.TEXT_EMBEDDING_MODEL_ID,\n\ndevice:\n\nstr\n\n= settings.RAG_MODEL_DEVICE,\n\ncache_dir:\n\nOptional\n\n[Path] =\n\nNone\n\n,\n\n) ->\n\nNone\n\n:\n\nself\n\n._model_id = model_id\n\nself\n\n._device = device\n\nself\n\n._model = SentenceTransformer(\n\nself\n\n._model_id, device=",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 626,
      "content": "self\n\n._device, cache_folder=\n\nstr\n\n(cache_dir)\n\nif\n\ncache_dir\n\nelse\n\nNone\n\n, )\n\nself\n\n._model.\n\neval\n\n()\n\n@property\n\ndef\n\nmodel_id\n\n(\n\nself\n\n) ->\n\nstr",
      "content_length": 151,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 627,
      "content": ":\n\nreturn\n\nself\n\n._model_id\n\n@cached_property\n\ndef\n\nembedding_size\n\n(\n\nself\n\n) ->\n\nint\n\n: dummy_embedding =\n\nself\n\n._model.encode(\n\n\"\"\n\n)\n\nreturn\n\ndummy_embedding.shape[\n\n0\n\n]",
      "content_length": 175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 628,
      "content": "@property\n\ndef\n\nmax_input_length\n\n(\n\nself\n\n) ->\n\nint\n\n:\n\nreturn\n\nself\n\n._model.max_seq_length\n\n@property\n\ndef\n\ntokenizer\n\n(\n\nself\n\n) -> AutoTokenizer:\n\nreturn\n\nself\n\n._model.tokenizer",
      "content_length": 183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 629,
      "content": "def\n\n__call__\n\n(\n\nself, input_text:\n\nstr\n\n|\n\nlist\n\n[\n\nstr\n\n], to_list:\n\nbool\n\n=\n\nTrue\n\n) -> NDArray[np.float32] |\n\nlist\n\n[\n\nfloat\n\n] |\n\nlist\n\n[",
      "content_length": 143,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 630,
      "content": "list\n\n[\n\nfloat\n\n]]:\n\ntry\n\n: embeddings =\n\nself\n\n._model.encode(input_text)\n\nexcept\n\nException: logger.error(\n\nf\"Error generating embeddings for\n\n{self._model_id=}\n\nand\n\n{input_text=}\n\n\"\n\n)\n\nreturn\n\n[]\n\nif\n\nto_list",
      "content_length": 213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 631,
      "content": "else\n\nnp.array([])\n\nif\n\nto_list: embeddings = embeddings.tolist()\n\nreturn\n\nembeddings\n\nThe embedding model class implements the singleton pattern (https://refactoring.guru/design-patterns/singleton), a creational design pattern that ensures a class has only one instance while providing a global access point to this instance. The\n\nEmbeddingModelSingleton()\n\nclass inherits from the\n\nSingletonMeta\n\nclass, which ensures that whenever an\n\nEmbeddingModelSingleton()\n\nis instantiated, it returns the same instance. This works well with ML models, as you load them once in memory through the singleton pattern, and afterward, you can use them anywhere in the code base. Otherwise, you risk loading the model in memory every time you use it or loading it multiple times, resulting in memory issues. Also, this makes it very convenient to access properties such as\n\nembedding_size",
      "content_length": 874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 632,
      "content": ", where you have to make a dummy forward pass into the embedding model to find the size of its output. As a singleton, you do this forward pass only once, and then you have it accessible all the time during the program’s execution.",
      "content_length": 231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 633,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 634,
      "content": "Summary\n\nThis chapter began with a soft introduction to RAG and why and when you should use it. We also understood how embeddings and vector DBs work, representing the cornerstone of any RAG system. Then, we looked into advanced RAG and why we need it in the first place. We built a strong understanding of what parts of the RAG can be optimized and proposed some popular advanced RAG techniques for working with textual data. Next, we applied everything we learned about RAG to designing the architecture of LLM Twin’s RAG feature pipeline. We also understood the difference between a batch and streaming pipeline and presented a short introduction to the CDC pattern, which helps sync two DBs.\n\nUltimately, we went step-by-step into the implementation of the LLM Twin’s RAG feature pipeline, where we saw how to integrate ZenML as an orchestrator, how to design the domain entities of the application, and how to implement an OVM module. Also, we understood how to apply some software engineering best practices, such as the abstract factory and strategy software patterns, to implement a modular and extendable layer that applies different cleaning, chunking, and embedding techniques based on the data category of each document.\n\nThis chapter focused only on implementing the ingestion pipeline, which is just one component of a standard RAG application. In Chapter 9, we will conclude the RAG system by implementing the retrieval and generation components and integrating them into the inference pipeline. But first, in the next chapter, we will explore how to generate a custom dataset using the data we collected and fine-tune an LLM with it.",
      "content_length": 1649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 635,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 636,
      "content": "References\n\nKenton, J.D.M.W.C. and Toutanova, L.K., 2019, June. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT (Vol. 1, p. 2).\n\nLiu, Y., 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\n\nMikolov, T., 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\n\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for Computational Linguistics.\n\nHe, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G., 2021, July.",
      "content_length": 1033,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 637,
      "content": "Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.\n\nWhat is Change Data Capture (CDC)? | Confluent. (n.d.). Confluent. https://www.confluent.io/en-gb/learn/change-data-capture/\n\nRefactoring.Guru. (2024, January 1). Singleton. https://refactoring.guru/design-patterns/singleton\n\nRefactoring.Guru. (2024b, January 1). Strategy. https://refactoring.guru/design-patterns/strategy\n\nRefactoring.Guru. (2024a, January 1). Abstract Factory. https://refactoring.guru/design-patterns/abstract-factory\n\nSchwaber-Cohen, R. (n.d.). What is a Vector Database & How Does it Work? Use Cases + Examples. Pinecone. https://www.pinecone.io/learn/vector-database/\n\nMonigatti, L. (2024, February 19). Advanced Retrieval-Augmented Generation: From Theory to LlaMaIndex Implementation. Medium. https://towardsdatascience.com/advanced-retrieval-augmented-generation- from-theory-to-llamaindex-implementation-4de1464a9930",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 638,
      "content": "Monigatti, L. (2023, December 6). A guide on 12 tuning Strategies for Production-Ready RAG applications. Medium. https://towardsdatascience.com/a-guide-on-12-tuning-strategies-for- production-ready-rag-applications-7ca646833439\n\nMonigatti, L. (2024b, February 19). Advanced Retrieval-Augmented Generation: From Theory to LlaMaIndex Implementation. Medium. https://towardsdatascience.com/advanced-retrieval-augmented-generation- from-theory-to-llamaindex-implementation-4de1464a9930\n\nMaameri, S. (2024, May 10). Routing in RAG-Driven applications - towards data science. Medium. https://towardsdatascience.com/routing-in- rag-driven-applications-a685460a7220\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 640,
      "content": "5",
      "content_length": 1,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 641,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 642,
      "content": "Supervised Fine-Tuning\n\nSupervised Fine-Tuning (SFT) is a crucial step in preparing LLMs for real-world applications. Following the initial pre-training phase, where an LLM learns to predict the next token in a sequence, SFT refines the model’s capabilities using carefully curated pairs of instructions and corresponding answers. This process serves two primary purposes: it teaches the model to understand and follow a specific chat format, effectively transforming it into a conversational agent, and it allows the model to adapt its broad knowledge base to excel in targeted tasks or specialized domains.\n\nThe importance of SFT lies in its ability to bridge the gap between a model’s general language understanding and its practical utility. By exposing the model to examples of desired input-output patterns, SFT shapes the LLM’s behavior to align with specific goals, whether they involve task completion (such as summarization or translation) or domain expertise (like medical or legal knowledge). This tailored approach not only enhances the model’s performance in intended areas but also improves its ability to follow instructions and generate more relevant and coherent responses.\n\nIn this chapter, we will cover the following topics:\n\nCreating a high-quality instruction dataset",
      "content_length": 1290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 643,
      "content": "SFT techniques\n\nImplementing fine-tuning in practice\n\nBy the end of this chapter, you will be able to create your own instruction datasets and efficiently fine-tune LLMs on them.\n\nAll the code examples from this chapter can be found on GitHub at https://github.com/PacktPublishing/LLM-Engineering.",
      "content_length": 297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 644,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 645,
      "content": "Creating an instruction dataset\n\nIn most use cases, creating an instruction dataset is the most difficult part of the fine-tuning process. This is due to multiple factors. Most use cases can be connected to raw text, but it is rare to find natural pairs of instructions and answers. This raw text needs to be transformed into a format that includes both instructions and answers. Moreover, the quality of the data is also crucial. Because of this, a lot of time is invested in manually checking and verifying individual samples. This careful review helps ensure that the dataset is accurate and useful for training the model.\n\nFigure 5.1 – Overview of the post-training data pipeline covered in this chapter\n\nIn this section, we will introduce a general framework to create your own instruction datasets, regardless of the final use case. We will then leverage the scraped data from Chapter 3 and transform it into an instruction dataset. The different stages in our data generation pipeline are summarized in Figure 5.1.",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 646,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 647,
      "content": "General framework\n\nInstruction datasets are defined as pairs of instructions and answers. The instructions are the inputs of the model, used as context during fine-tuning. The answers are the expected outputs of the model. During fine-tuning, you can choose to train the model on the instructions and answers, or on answers only. Pairs of instructions and answers follow a certain template. Some instruction templates, such as Alpaca, introduce additional fields like\n\ninputs\n\nand\n\nsystem\n\n. Both of them can be considered subfields of the\n\ninstruction\n\nfield. In this case, “inputs” contain the data the model needs to complete the instruction, and “system” is a meta- prompt to steer the general behavior of the model. Here is an example from the SlimOrca dataset, with “system” and “instruction”:\n\nSystem You are a helpful assistant, who always provide explanation. Think like you are answering to a five year ol\n\nInstruction Concepts: building, shop, town Write a sentence that includes all these words.\n\nOutput In our little town, there is a shop inside a big building where people go to buy their favorite toys and candie\n\nTable 5.1 – Example of sample from the Open-Orca/SlimOrca dataset\n\nThis example illustrates how the “system” field is used to define specific behaviors for the model, such as being helpful, always providing explanations, and tailoring responses as if speaking to a five-year-old. The “instruction” field provides the necessary data (the concepts) and the task (constructing a sentence). The\n\noutput\n\nfield shows the expected answer, which, while not the only possible answer, represents a high-quality response.\n\nTo build an instruction dataset, we want to curate data that is representative of how the model will be used. Once we have gathered enough samples, our goal is to filter them to only keep high-quality data. In this context, high- quality data can be described through three main dimensions:",
      "content_length": 1932,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 648,
      "content": "Accuracy: It refers to the factual correctness and relevance of the samples. In the context of instruction datasets, this means ensuring that responses are not only factually accurate but also relevant to their corresponding instructions. High accuracy is essential for training models that can provide reliable and trustworthy information.\n\nDiversity: A high-quality dataset should encompass a wide range of use cases, covering the potential queries and tasks the deployed LLM might encounter. This diversity should span topics, contexts, text lengths, and writing styles. By sampling data in a representative manner, we allow models to develop robust instruction-following capabilities.\n\nComplexity: Trivial or overly simplistic samples do little to improve an LLM’s capabilities. Instead, datasets should include complex, multi-step reasoning problems and challenging tasks that push the boundaries of what the model is expected to handle. This complexity helps in developing models capable of tackling complex real-world problems.\n\nIn the following sections, we will see techniques to filter and evaluate instruction samples according to these dimensions.",
      "content_length": 1159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 649,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 650,
      "content": "Data quantity\n\nThe Hugging Face Hub contains numerous instruction datasets, which can be general-purpose or designed for particular tasks or domains. When working on a new use case, it can be beneficial to look for related open- source datasets to leverage for fine-tuning. This is particularly important if your number of samples is too low (for example, fewer than 1,000), requiring you to augment it with high-quality data.\n\nFigure 5.2 – Screenshot of the most-liked datasets on the Hugging Face Hub",
      "content_length": 502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 651,
      "content": "Calculating an ideal number of samples is a difficult task, as both the quality of the data and the size of the model can have a dramatic impact. For large models (around 70 billion parameters, for example), this number can be as low as 1,000 high-quality samples (see the LIMA paper in the References section). This is not true for smaller models (around seven billion parameters, for instance), as they need more samples to simply learn the correct chat template. In any case, the quality of the data is a crucial factor, and a high number of samples is always desirable.\n\nTo provide additional numbers, we can look at the fine-tuned models developed by companies and the open-source community. We can distinguish two types of finetunes: general-purpose, aimed to reproduce the capabilities of models like GPT, and task- or domain-specific models, designed to optimize their performance for a particular application.\n\nGeneral-purpose models cover more topics, which requires additional samples. Among companies, we observe a wide range of values. For instance, Yi models from 01-ai rely on less than 10,000 samples. At the opposite range of the spectrum, Meta reported using 10 million samples for Llama 3 through the entire fine-tuning process (including preference alignment). In the open-source community, models like OpenHermes and Dolphin use around one million samples. Based on the quality of these finetunes, we recommend an instruction dataset of at least one million samples to create a good general-purpose instruct model. On the other hand, models fine-tuned for a specific purpose require fewer samples. Here, we differentiate task-specific models from domain-specific ones.\n\nTask-specific and domain-specific models represent two distinct approaches to fine-tuning LLMs. Task-specific models are designed to excel at a particular function, such as translation, summarization, or sentiment",
      "content_length": 1904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 652,
      "content": "analysis. These models benefit from a focused training approach on a single task, allowing for efficient performance even with smaller model sizes (typically less than 8 billion parameters). The data required for task-specific fine-tuning is generally more manageable, ranging from 100 to 100,000 samples. This makes task-specific fine-tuning an attractive option for many applications where resources may be limited.\n\nDomain-specific models, on the other hand, aim to tweak the LLM with specialized knowledge and familiarity with the vocabulary and linguistic patterns of a particular field. These models are valuable in areas such as medicine, law, finance, e-commerce, engineering, and hospitality. The data requirements for domain-specific fine-tuning can vary widely depending on the complexity and breadth of the domain. Some fields, like medicine or law, may require as much data as general-purpose fine-tuning due to their vast technical corpora. Others, such as e-commerce or hospitality, might need fewer samples, more in line with task-specific fine-tuning.\n\nThe key factors determining the data needs for domain-specific models are the “size” of the domain (i.e., the extent of its specialized knowledge and vocabulary) and the representation of that domain in the model’s pre- training data. Domains that are well-represented in the original training data may require less fine-tuning, while those that are more specialized or underrepresented may need more extensive datasets. Even with open-source LLMs, many pre-training datasets are closed-source, which requires making educated guesses to determine their composition (e.g., 30% code or 20% math).",
      "content_length": 1664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 653,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 654,
      "content": "Data curation\n\nWhen it comes to procuring data for fine-tuning, the approaches differ between task-specific and domain-specific models. For task-specific models, data curation often involves collecting examples of the desired task from existing datasets or creating new ones. This might involve gathering pairs of original and summarized texts for a summarization model or collecting sentences in different languages for a translation model.\n\nDomain-specific data curation can be more challenging. It often requires collaboration with subject matter experts to gather and validate relevant texts, research papers, technical documents, and other domain-specific content. In some cases, it may involve partnering with organizations or institutions that have access to large repositories of specialized information. The quality and relevance of this data is crucial, as it directly impacts the model’s ability to understand and generate content in the target domain.\n\nIt’s worth noting that few-shot prompting has emerged as an alternative strategy to fine-tuning, especially for task-specific applications. This approach leverages the capabilities of large, powerful models by providing a few examples of the desired task within the input prompt. While not a replacement for fine-tuning in all scenarios (e.g., when you want to learn a new domain), few-shot prompting can be an efficient way to adapt models to new tasks without the need for extensive additional training.\n\nIn practice, the line between task-specific and domain-specific models can sometimes blur. For instance, a model fine-tuned for medical diagnosis",
      "content_length": 1617,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 655,
      "content": "could be considered both task-specific (focused on diagnosis) and domain- specific (specialized in medical knowledge). The key is to understand the primary goal of the fine-tuning process and tailor the approach accordingly.\n\nAt this point in the process, we should have a collection of datasets suited for our use case. The next step consists of refining the quality of the samples through rule-based filtering, data duplication, data decontamination, and data quality evaluation.",
      "content_length": 481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 656,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 657,
      "content": "Rule-based filtering\n\nRule-based filtering is a systematic approach to data quality control that relies on explicit, predefined rules to evaluate and filter data samples. These rules are typically designed to address common quality issues and can range from simple checks to more complex logical operations. The primary goal of rule-based filtering is to maintain a high standard of data quality by removing samples that do not meet specific criteria.\n\nLength filtering is a straightforward yet effective rule-based filtering technique. This method involves setting thresholds for the acceptable length of responses in the dataset. Extremely short responses often lack sufficient information to be meaningful, while excessively long ones may contain irrelevant or redundant content. It’s important to note that the appropriate length thresholds can vary significantly depending on the specific task and domain. For example, a dataset for generating concise summaries might have a lower maximum threshold compared to one for detailed explanations.\n\nKeyword exclusion is another powerful rule-based filtering technique that focuses on the content of the samples rather than their structure. This method involves creating a list of keywords or phrases associated with low-quality or inappropriate content, and then filtering out any samples that contain these terms. The keyword list can include obvious indicators of low quality, such as profanities or spam-related terms, as well as domain-specific words that might indicate irrelevant or off- topic content. For instance, in a dataset for a professional writing assistant, you might exclude samples containing slang terms or informal expressions that don’t align with the intended tone and style.",
      "content_length": 1746,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 658,
      "content": "Format checking is recommended for datasets that include structured data or follow specific formatting requirements. This technique ensures that all samples adhere to the expected format, maintaining consistency and facilitating processing downstream. Format checking can be particularly important for datasets containing code samples, JSON structures, or other formatted text. For example, in a dataset of programming instructions and solutions, you might implement rules to verify that code samples are syntactically correct and follow specified style guidelines.\n\nRule-based filtering offers significant advantages in preparing instruction datasets. Its speed and efficiency allow for rapid application to large volumes of data, making it highly scalable. The consistency of rule application ensures uniform treatment of data, reducing human error and bias. Furthermore, the explicit definition of filtering criteria provides transparency and interpretability, facilitating easy understanding, auditing, and adjustment. The ability to automate rule-based filtering reduces the need for manual intervention and enables continuous data quality monitoring.\n\nHowever, rule-based filtering also has limitations that must be considered. Predefined rules may lack the nuance required to capture the full complexity of language and context, potentially leading to the removal of valid but unusual samples. The typically binary nature of rules (pass/fail) may not always align with the nuanced nature of language and instruction quality. Additionally, as data patterns and quality standards evolve, rules need regular review and updates to remain effective. There’s also a risk that poorly designed rules could inadvertently introduce or amplify biases in the dataset.",
      "content_length": 1762,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 659,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 660,
      "content": "Data deduplication\n\nDataset diversity is fundamental to training models that can generalize well to new, unseen data. When a dataset contains duplicates or near-duplicates, it can lead to several issues:\n\nOverfitting: Models may memorize specific examples rather than learning general patterns.\n\nBiased performance: Overrepresented data points may skew the model’s performance towards certain types of inputs.\n\nInefficient training: Redundant data can increase training time without providing additional valuable information.\n\nInflated evaluation metrics: Duplicate data in test sets may lead to overly optimistic performance estimates.\n\nTo deduplicate datasets, we distinguish between exact and fuzzy deduplication. Exact deduplication removes identical samples through a straightforward process involving data normalization, hash generation, and duplicate removal. Data normalization standardizes the format of entries, such as converting text to lowercase. Hash",
      "content_length": 964,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 661,
      "content": "generation then creates unique hashes for each entry using algorithms like MD5 or SHA-256. These hashes are compared to find matches, and duplicates are removed, leaving only one instance of each. While effective for identical entries, exact deduplication does not detect near- duplicates or semantically similar content, requiring more advanced techniques for those cases.\n\nThe most popular approach to fuzzy deduplication is MinHash deduplication. Compared to other fuzzy techniques, it maintains high accuracy while significantly reducing computational complexity. MinHash operates by generating compact representations, or signatures, for each data item. These signatures serve as fingerprints that capture the essence of the data while drastically reducing its dimensionality. In practice, MinHash transforms data items (such as text documents) into sets of shingles, applies multiple hash functions to these sets, and selects the minimum hash values to form signature vectors. These signatures can then be compared using similarity measures like Jaccard similarity to efficiently identify near-duplicates.\n\nIn addition to exact and fuzzy deduplication, semantic similarity takes a different approach by focusing on the meaning of text for deduplication. This method involves converting words or entire samples into vector representations using various natural language processing techniques. Word embedding models such as Word2Vec, GloVe, and FastText transform individual words into dense vectors, capturing semantic relationships.\n\nFor more context-aware representations, language models like BERT, sentence transformers, or cross-encoders can generate embeddings for entire sentences or documents. Once these vector representations are obtained, deduplication can be performed by comparing the similarity between vectors. Common similarity measures include cosine similarity or",
      "content_length": 1886,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 662,
      "content": "Euclidean distance. Samples with high similarity scores above a predefined threshold can be considered duplicates. For large datasets, clustering techniques may be applied to group similar vectors. Methods like K-means, DBSCAN, or hierarchical clustering can efficiently organize the vector space, allowing for the identification of clusters that represent semantically similar content. Within each cluster, a representative sample can be retained while others are marked as duplicates.",
      "content_length": 486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 663,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 664,
      "content": "Data decontamination\n\nData decontamination is the process of ensuring that the training dataset does not contain samples that are identical or highly similar to those in the evaluation or test sets. This step is important for ensuring the quality of the model evaluation and preventing overfitting or memorization of test data.\n\nData decontamination uses techniques from data deduplication. Exact matching can be used to remove any training samples that are identical to those in the evaluation sets. This can be done using hash functions or direct string comparisons. Next, we can also use near-duplicate detection methods to identify and remove training samples that are very similar to evaluation samples, even if they are not exactly the same. This often involves techniques like MinHash or computing similarity scores based on n-grams or embeddings.\n\nA simple way to perform data decontamination is to add your evaluation set to the instruction dataset during the data deduplication stage. In this case, we want to ensure that we only remove samples from the instruction dataset, which can be implemented in different ways (only filtering out the first duplicate, recording the indexes of the evaluation samples, etc.). Ideally, you can automatically add your evaluation sets in the data deduplication stage to fully automate this process. This is particularly efficient if you iterate over several versions of custom benchmarks.",
      "content_length": 1434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 665,
      "content": "Another aspect of data decontamination is filtering out samples that may have been derived from the same source as evaluation data. This can involve checking for overlapping phrases, similar sentence structures, or common metadata. Practitioners may also use provenance tracking (source the data they use) to identify and exclude data from specific sources that are known to be used in evaluation sets.",
      "content_length": 402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 666,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 667,
      "content": "Data quality evaluation\n\nData quality evaluation is a critical aspect of machine learning, particularly for LLMs. The process involves assessing various characteristics of datasets, including accuracy, diversity, and complexity. While some aspects like mathematical accuracy can be easily verified using tools such as Python interpreters, evaluating subjective or open-ended content remains challenging.\n\nTraditional methods of data quality assessment include human annotation, which generally provides high accuracy but is resource-intensive. To address scalability issues, machine learning techniques have been developed to automate the evaluation process. These include using LLMs as judges, reward models, and classifiers trained for quality prediction.\n\nThe LLM-as-a-judge strategy involves prompting LLMs to evaluate the quality of each sample. This approach has become popular due to its flexibility and ease of use, though it does present some challenges. Different LLMs have different levels of performance across tasks, and their evaluations often align more closely with those of non-experts. With domain-specific datasets, you might want to use domain-specific models instead of better, general-purpose LLMs. Comparative assessment methods (e.g., “Is answer A better than answer B?”) generally outperform absolute scoring approaches (e.g., “Rate answer A between 1 and 4”), though both can be used at scale with sufficient prompt engineering. We recommend iterating through different prompts over a representative subset to manually verify the quality of the responses. Table 5.2 shows an example of a custom prompt for a judge LLM.\n\nInstruction You are a data quality evaluator. Your goal is to assess an instruction and its corresponding answer, dete\n\nTable 5.2 – Example of LLM-as-a-judge prompt for data quality evaluation\n\nLLM-as-a-judge is known to have several biases. First, it has a position bias in comparative scoring, where the LLM judge favors the first answer. This can be addressed by randomizing the order of answers A and B. In addition, like humans, LLM judges favor long answers. Length normalization techniques can be applied to absolute scoring to mitigate this issue. Finally, LLM judges are known to have intra-model favoritism, meaning that they prefer models from the same family (GPT-4o with GPT-4 and GPT-4o mini, for example). This can be addressed by using several models instead of a single one.\n\nIn general, to improve evaluation reliability, strategies such as using multiple LLMs as a jury reduce bias and improve consistency. Leveraging a jury of smaller LLMs can also reduce costs while increasing accuracy and mitigating intra-model favoritism. For specific applications like chatbots, it’s advisable to aim for high agreement",
      "content_length": 2774,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 668,
      "content": "between LLM judges and human evaluators (around 80%). Simple grading scales (with few-shot prompting) and task-specific benchmarks are also recommended to ensure relevant and interpretable evaluations.\n\nReward models are another way to re-purpose LLMs for data quality evaluation. The term “reward model” comes from Reinforcement Learning from Human Feedback (RLHF, see Chapter 6). They can be broadly defined as models that take an instruction and answer pair and return a score as output. Generally, reward models are created by adding a linear head on top of a decoder-only architecture like Gemma or Llama. They are then trained for this specific purpose, using either reinforcement learning or traditional fine- tuning. Figure 5.3 shows ArmoRM-Llama3-8B-v0.1’s architecture, which adds regression and gating layers on top of a Llama 3 8B model. This model outputs multiple scores to target specific dimensions, such as helpfulness, correctness, coherence, complexity, and verbosity. This allows for a more fine-grained approach to data quality evaluation.\n\nFigure 5.3 – Architecture of RLHFlow/ArmoRM-Llama3-8B-v0.1, based on Llama 3 (Source: https://doi.org/10.48550/arXiv.2406.12845)\n\nThe Allen Institute for AI’s RewardBench leaderboard, hosted on Hugging Face (allenai/reward-bench), is a good resource for comparing different reward models. It combines various types of reward models (generative, classifiers, DPO, etc.) and evaluates them on a curated set of chosen and rejected answers for each instruction. While this task is not directly related to instruction data quality, it is a good resource for finding models capable of differentiating between good and bad answers.\n\nClassifiers or encoder-only models can be trained to perform data quality evaluation. A good example is HuggingFaceFW/fineweb-edu-classifier, a classifier designed to judge the educational value of web pages. This model was designed as a quality filter for pretraining data but a similar approach can be taken to evaluate instruction samples at scale. In practice, fineweb-edu-classifier adds a classification head to an embedding model (Snowflake/snowflake-arctic-embed-m) and trains it for 20 epochs on 450,000 samples that are annotated by Llama 3 70B Instruct.\n\nThis approach relies on encoder-only models, which are both smaller and better suited to classification tasks. Thanks to their low number of parameters, these models are faster to run and can scale to millions of samples. However, they are not as accurate as bigger models, particularly for complex reasoning tasks where they lack the",
      "content_length": 2588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 669,
      "content": "ability to capture nuances. At smaller scale, encoder-only models are still valuable to filter out outliers or as part of an automated data pipeline, which requires faster processing.",
      "content_length": 183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 670,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 671,
      "content": "Data exploration\n\nData exploration is a continuous process that requires practitioners to become familiar with the training data. It involves both manual inspection and automated analysis, each playing a crucial role in understanding the dataset’s characteristics, strengths, and potential shortcomings.\n\nManual dataset exploration, though time-consuming, is an important step. It reveals errors and inconsistencies that automated processes might miss, including formatting issues, data entry mistakes, incoherent reasoning, and factual inaccuracies. This process provides qualitative insights into the dataset’s content and style. To enhance efficiency, researchers can employ techniques like stratified sampling (selecting diverse samples), systematic review (using a criteria checklist), and collaborative review (involving multiple reviewers).\n\nFigure 5.4 shows an example with Argilla, a collaborative platform for manual data quality evaluation and exploration.",
      "content_length": 967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 672,
      "content": "Figure 5.4 – Argilla’s interface for collaborative data quality evaluation and exploration\n\nStatistical analysis is a complementary technique that reveals vocabulary diversity, potential biases, and concept representation. This process utilizes natural language processing libraries like NLTK or spaCy for tokenization and analysis of large text volumes. Visualization tools such as Matplotlib or Seaborn create histograms and word clouds, enabling intuitive pattern recognition. These techniques provide insights into dataset composition, language breadth, and possible cultural or contextual preferences, which can influence model outputs.",
      "content_length": 641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 673,
      "content": "Topic clustering automatically groups similar documents or pieces of text together, revealing underlying themes and patterns within the data. This process is especially important for understanding the content of large text corpora, identifying trends, and organizing information in a meaningful way. It is often associated with data visualization, with figures that show clusters of similar samples.\n\nLet’s consider the task of building an instruction dataset about various programming languages. You have collected a vast corpus of programming- related text from online forums, documentation, and tutorials. First, topic clustering can help identify the distinct programming languages present in the dataset (Python, JavaScript, etc.). Second, within each language cluster, you can further identify sub-topics like\n\nerror handling\n\n,\n\ndata structures\n\n, and\n\nweb frameworks\n\n. This allows a balanced representation of each language and sub-topic in the corpus.\n\nThis makes sure that each topic is correctly covered for each programming language.",
      "content_length": 1046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 674,
      "content": "Figure 5.5 – Representation of the historical TikTok dataset made with Nomic Atlas\n\nSeveral tools are available for performing topic clustering, each with its own strengths and approaches. For example, Hugging Face’s text-",
      "content_length": 222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 675,
      "content": "clustering provides a simple pipeline with sentence transformers for embedding text into vector space, UMAP for dimensionality reduction, and DBSCAN for clustering. It also automatically labels clusters using an LLM and can output visualizations. Nomic Atlas (see Figure 5.5), BunkaTopics, and Lilac are alternatives proposing similar approaches with additional features.",
      "content_length": 371,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 676,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 677,
      "content": "Data generation\n\nWhen the available instruction datasets are not sufficient, creating custom data becomes necessary. This is particularly relevant for specialized applications where publicly available data is scarce.\n\nAdditionally, it serves as a method to augment underrepresented areas in a dataset, like insufficient examples of JavaScript error-handling techniques in our previous example. While data can be generated manually by individuals or through crowdsourcing, these approaches often incur significant costs and time investments. Synthetic data generation using LLMs offers a more efficient and scalable alternative. This method, when combined with well-designed prompt engineering, can produce high-quality data at a much larger scale, effectively addressing the limitations of manual data creation processes.\n\nThe process of synthetic data generation typically begins with the preparation of a set of carefully designed prompts (sometimes called taxonomy). These serve as the foundation for generating new, diverse examples. Five seed prompts used in the original Alpaca dataset can be seen in Table 5.3. The quality of synthetically generated data largely depends on the prompts and techniques used in the generation process. Well-crafted prompts can guide the language model to produce diverse, relevant, and high-quality instruction-response pairs. These prompts often include specific instructions, examples, and constraints to ensure the generated data aligns with the desired format and content.\n\nSeed instructions Is there anything I can eat for breakfast that doesn’t include eggs, yet includes protein, and has ro\n\nTable 5.3 – Examples of seed prompts used in the original Alpaca dataset\n\nMany synthetic data generation pipelines incorporate multiple steps to ensure data quality. This may include generating an initial set of questions or instructions, followed by generating corresponding answers or responses. Some systems also implement validation steps, where another model or set of rules checks the generated pairs for accuracy, relevance, and adherence to specified criteria.\n\nAn important aspect of synthetic data generation is the ability to control various attributes of the generated data. This includes factors such as the complexity of the instructions, the length of the responses, the tone or style of the language used, and the specific topics or domains covered. By fine-tuning these parameters, it’s possible to create datasets that are tailored to specific training objectives or that complement existing datasets in targeted ways. Structured generation using libraries like Outlines can also be beneficial to adhere to specific formats.",
      "content_length": 2679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 678,
      "content": "Furthermore, synthetic data generation can be particularly useful for addressing biases and gaps in existing datasets. By carefully designing the generation process, it’s possible to create more balanced and inclusive datasets that represent a wider range of perspectives, topics, and language styles. This can help in training LLMs that are more equitable and capable of serving diverse user bases.\n\nHowever, synthetic data generation also comes with challenges. One primary concern is the potential for the generated data to inherit biases or errors from the underlying language model used for generation. To mitigate this, many approaches incorporate human oversight, diverse prompts, and additional filtering mechanisms to ensure the quality and appropriateness of the generated data.\n\nAnother consideration is the need for the generated data to be sufficiently diverse and challenging. If the synthetic data is too simplistic or repetitive, it may not provide the level of complexity required to train a robust LLM. Advanced techniques in synthetic data generation often focus on creating varied and nuanced instruction-response pairs that can push the boundaries of what the model can learn.",
      "content_length": 1197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 679,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 680,
      "content": "Data augmentation\n\nIn this context, data augmentation refers to the process of increasing both the quantity and the quality of data samples. Unlike data generation, we use pre-existing instruction samples as inputs in this stage. While it is possible to upsample pairs of instructions and answers, data augmentation is mostly used to increase the quality of existing samples. In particular, it focuses on two aspects: diversity and complexity.\n\nA pioneering approach in this field is the Evol-Instruct method, which uses LLMs to evolve simple instructions into more qualitative ones. The evolved instructions can then be used to generate answers using powerful LLMs. This method employs two main strategies: in-depth and in-breadth evolving.\n\nIn-depth evolving focuses on enhancing the complexity of existing instructions. It includes several techniques:\n\nConstraints: It involves introducing additional requirements or limitations to the original instruction, making it more challenging to fulfill.\n\nDeepening: Instead of shallow questions, it tries to find more deep questions, requiring more comprehensive responses.\n\nConcretizing: It replaces general concepts with more specific ones, adding detail and precision to the instruction.\n\nIncreasing reasoning steps: It modifies instructions to explicitly request multiple-step reasoning, promoting more complex problem-solving.\n\nComplicating input: This involves adding more complex data formats or structures to the instruction, such as XML, JSON, or code snippets.\n\nIn-breadth evolving, on the other hand, aims to expand the diversity of the instruction dataset. It generates entirely new instructions inspired by existing ones, focusing on creating more rare or long-tailed examples within the same domain.\n\nAs an example of concrete implementation, in-depth evolving can be automated with the following prompt, from the AutoEvol paper. You simply need to provide the instruction you want to evolve as input, and a powerful model like GPT-4o will return a more complex version of the original instruction.",
      "content_length": 2058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 681,
      "content": "You are an Instruction Rewriter that rewrites the given #Instruction# into a more complex version. Please follow th\n\nTable 5.4 – Evol LLM prompt from the “Automatic Instruction Evolving for Large Language Models” paper by Zeng et al. (2024)\n\nThe UltraFeedback method is another innovative approach, focused on answer quality instead of instruction quality. It employs AI feedback to enhance the quality and diversity of model responses. Unlike Evol- Instruct, which evolves instructions, UltraFeedback uses a large pool of diverse instructions and models to generate a wide range of responses.\n\nIt then leverages advanced language models like GPT-4 to provide detailed critiques and numerical scores for these responses across multiple dimensions such as instruction-following, truthfulness, honesty, and helpfulness.\n\nBased on these ideas, you can create your own augmentation techniques to create a more challenging and diverse instruction dataset. By refining and evolving existing instructions and answers, the resulting dataset can better train models to handle complex, multi-step tasks, and improve their performance across a wider range of applications.",
      "content_length": 1161,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 682,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 683,
      "content": "Creating our own instruction dataset\n\nIn this section, we will create our own instruction dataset based on the crawled data from Chapter 3. To create a high-quality instruction dataset, we need to address two main issues: the unstructured nature of our data and the limited number of articles we can crawl.\n\nThis unstructured nature comes from the fact that we are dealing with raw text (articles), instead of pairs of instructions and answers. To address this issue, we will use an LLM to perform this transformation. Specifically, we will employ a combination of backtranslation and rephrasing. Backtranslation refers to the process of providing the expected answer as output and generating its corresponding instruction. However, using a chunk of text like a paragraph as an answer might not always be appropriate. This is why we want to rephrase the raw text to ensure we’re outputting properly formatted, high-quality answers. Additionally, we can ask the model to follow the author’s writing style to stay close to the original paragraph. While this process involves extensive prompt engineering, it can be automated and used at scale, as we will see in the following implementation.\n\nOur second issue regarding the limited number of samples is quite common in real-world use cases. The number of articles we can retrieve is limited, which constrains the size of the instruction dataset we are able to create. In this example, the more samples we have, the better the model becomes at imitating the original authors. To address this problem, we will divide our articles into chunks and generate three instruction-answer pairs for each chunk. This will multiply the number of samples we create while maintaining diversity in the final dataset. For simplicity, we will do it using OpenAI’s GPT-4o-mini model, but you can also use open-source models.\n\nHowever, LLMs are not reliable when it comes to producing structured output. Even when given specific templates or instructions, there’s no guarantee that the model will consistently adhere to them. This inconsistency often necessitates additional string parsing to ensure the output meets the desired format.\n\nTo simplify this process and ensure properly structured results, we can employ structured generation techniques. Structured generation is an effective method to force an LLM to follow a predefined template, such as JSON, pydantic classes, or regular expressions. In the following, we will use OpenAI’s JSON mode feature, which provides a more robust way to return valid JSON objects and reduce the need for extensive post-processing.\n\nBased on this description, the following figure summarizes every step of the synthetic data pipeline we want to build.",
      "content_length": 2719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 684,
      "content": "Figure 5.6 – Synthetic data generation pipeline from raw text to instruction dataset\n\nLet’s now implement it in Python. You can implement it as part of the LLMOps pipeline, or as a standalone script:\n\nWe want to make sure that the following libraries are installed. The OpenAI library will allow us to interact with a model to generate the instruction data, and datasets will format it into a Hugging Face-compatible format. The tqdm library is installed to visualize the progress during the data generation process.\n\nopenai==\n\n1.37.1\n\ndatasets==\n\n2.20.0\n\ntqdm==\n\n4.66.4\n\nWe import all the required libraries as follows.",
      "content_length": 620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 685,
      "content": "import\n\nconcurrent.futures\n\nimport\n\njson\n\nimport\n\nrandom\n\nimport\n\nre\n\nfrom\n\nconcurrent.futures\n\nimport\n\nThreadPoolExecutor\n\nfrom\n\ntyping\n\nimport\n\nList\n\n,\n\nTuple\n\nfrom\n\ndatasets\n\nimport\n\nDataset\n\nfrom\n\nopenai\n\nimport\n\nOpenAI\n\nfrom\n\npydantic",
      "content_length": 239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 686,
      "content": "import\n\nBaseModel, Field\n\nfrom\n\ntqdm.auto\n\nimport\n\ntqdm\n\nThe raw data we have is a JSON file. We create a Hugging Face dataset from this JSON file by extracting specific fields from each article:\n\nid\n\n,\n\ncontent\n\n,\n\nplatform\n\n,\n\nauthor_id\n\n,\n\nauthor name\n\n, and\n\nlink\n\n.\n\ndef\n\nload_articles_from_json\n\n(\n\nfile_path:\n\nstr\n\n) -> Dataset:",
      "content_length": 335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 687,
      "content": "with\n\nopen\n\n(file_path,\n\n\"r\"\n\n)\n\nas\n\nfile: data = json.load(file)\n\nreturn\n\nDataset.from_dict( {\n\n\"id\"\n\n: [item[\n\n\"id\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"content\"\n\n: [item[\n\n\"content\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"platform\"",
      "content_length": 255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 688,
      "content": ": [item[\n\n\"platform\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"author_id\"\n\n: [item[\n\n\"author_id\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"author_full_name\"\n\n: [item[\n\n\"author_full_name\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"link\"",
      "content_length": 255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 689,
      "content": ": [item[\n\n\"link\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]], } )\n\nIf we simply load our dataset as a pandas dataframe, it returns the following table.\n\nid\n\ncontent\n\n0\n\nab2f9e2e-5459-4dd6-97d6-c291de4a7093\n\nThe Importance of Data Pipelines in the Era of...\n\n1\n\nccfe70f3-d324-40b6-ba38-86e72786dcf4\n\nChange Data Capture: Enabling Event-Driven Arc...\n\n2\n\n4c9f68ae-ec8b-4534-8ad5-92372bf8bb37\n\nThe Role of Feature Stores in Fine-Tuning LLMs...\n\n...\n\n...\n\n...\n\n73\n\n68795a4d-26c2-43b7-9900-739a80b9b7dc DML: 4 key ideas you must know to train an LLM...\n\n74\n\nd91b17c0-05d8-4838-bf61-e2abc1573622\n\nDML: How to add real-time monitoring & metrics...\n\n75\n\ndcf55b28-2814-4480-a18b-a77d01d44f5f\n\nDML: Top 6 ML Platform Features You Must Know ...\n\nIf we inspect the content of some articles a little further, we realize that some of them have special characters and redundant whitespaces. We can clean this with a simple regex.\n\nFirst, we use\n\n[^\\w\\s.,!?']\n\nplatf\n\nmedi\n\nmedi\n\nmedi\n\n...\n\ndeco\n\ndeco\n\ndeco",
      "content_length": 993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 690,
      "content": "to remove non-alphanumeric characters except for apostrophes, periods, commas, exclamation marks, and question marks. Then, we use\n\n\\s+\n\nto replace multiple consecutive whitespace characters with a single space.\n\nFinally, we implement\n\nstrip()\n\nto remove any leading or trailing whitespace.\n\ndef\n\nclean_text\n\n(\n\ntext\n\n): text = re.sub(\n\nr\"[^\\w\\s.,!?']\"\n\n,\n\n\" \"\n\n, text) text = re.sub(\n\nr\"\\s+\"\n\n,\n\n\" \"\n\n, text)\n\nreturn\n\ntext.strip()\n\nNow that we can load our articles, we need to chunk them before turning them into pairs of instructions and answers. Ideally, you would want to use headlines or paragraphs to produce semantically meaningful chunking.",
      "content_length": 649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 691,
      "content": "However, in our example, like in the real world, raw data tends to be messy. Due to improper formatting, we cannot extract paragraphs or headlines for every article in our raw dataset. Instead, we will extract sentences using a regex to get chunks between 1,000 and 2,000 characters. This number can be optimized depending on the density of the information contained in the text.\n\nThe\n\nextract_substrings\n\nfunction processes each article in the dataset by first cleaning the text and then using a regex to split it into sentences. It then builds chunks of text by concatenating these sentences until each chunk is between 1,000 and 2,000 characters long.\n\ndef\n\nextract_substrings\n\n(\n\ndataset: Dataset, min_length:\n\nint\n\n=\n\n1000\n\n, max_length:\n\nint\n\n=\n\n2000\n\n) ->\n\nList\n\n[\n\nstr\n\n]: extracts = [] sentence_pattern =\n\nr\"(?\n\nfor\n\narticle\n\nin",
      "content_length": 837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 692,
      "content": "dataset[\n\n\"content\"\n\n]: cleaned_article = clean_text(article) sentences = re.split(sentence_pattern, cleaned_article) current_chunk =\n\n\"\"\n\nfor\n\nsentence\n\nin\n\nsentences: sentence = sentence.strip()\n\nif\n\nnot\n\nsentence:\n\ncontinue\n\nif\n\nlen\n\n(current_chunk) +\n\nlen\n\n(sentence) <= max_length: current_chunk += sentence +\n\n\" \"\n\nelse\n\n:\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip()) current_chunk = sentence +\n\n\" \"\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip())\n\nreturn\n\nextracts",
      "content_length": 538,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 693,
      "content": "Next, we want to create instruction-answer pairs from the extracted chunks of text. To manage these pairs effectively, we introduce the\n\nInstructionAnswerSet\n\nclass. This class allows us to create instances directly from JSON strings, which is useful when parsing the output from the OpenAI API.\n\nclass\n\nInstructionAnswerSet\n\n:\n\ndef\n\n__init__\n\n(\n\nself, pairs:\n\nList\n\n[\n\nTuple\n\n[\n\nstr\n\n,\n\nstr\n\n]]\n\n): self.pairs = pairs\n\n@classmethod\n\ndef\n\nfrom_json\n\n(\n\ncls, json_str:\n\nstr",
      "content_length": 472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 694,
      "content": ") ->\n\n'InstructionAnswerSet'\n\n: data = json.loads(json_str) pairs = [(pair[\n\n'instruction'\n\n], pair[\n\n'answer'\n\n])\n\nfor\n\npair\n\nin\n\ndata[\n\n'instruction_answer_pairs'\n\n]]\n\nreturn\n\ncls(pairs)\n\ndef\n\n__iter__\n\n(\n\nself\n\n):\n\nreturn\n\niter\n\n(self.pairs)\n\nNow that we have a set of extracts from the articles with a reasonable length, we can use an LLM to transform them into pairs of instructions and answers. Note that this step is model-agnostic and can be implemented with any open-source or closed-source model. Because this output is grounded in the context we provide, it doesn’t require complex reasoning or high-performing models.",
      "content_length": 629,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 695,
      "content": "For convenience, we will use GPT-4o mini in this example. This choice is motivated by the low cost and good performance of this model. Prompt engineering is the most important aspect of this data transformation stage and requires several iterations to produce the expected outputs. We recommend starting with simple prompts and adding complexity when required to be more accurate, modify the style, or output multiple responses.\n\nIn our example, we want to create instructions like “Write a paragraph about X topic” and corresponding answers that are factual and imitate the writer’s style. To implement this, we need to provide an extract that will ground the model’s responses. For efficiency, we also choose to generate five instruction-answer pairs for each extract. Here’s the beginning of our function for instruction generation, including our prompt.\n\ndef\n\ngenerate_instruction_answer_pairs\n\n(\n\nextract:\n\nstr\n\n, client: OpenAI\n\n) ->\n\nList\n\n[\n\nTuple\n\n[\n\nstr\n\n,\n\nstr\n\n]]: prompt =\n\nf\"\"\"Based on the following extract, generate five instruction-answer pairs. Each instruction \\\n\nmust ask to write about a specific topic contained in the context. each answer \\\n\nmust provide a relevant paragraph based on the information found in the \\\n\ncontext. Only use concepts from the context to generate the instructions. \\\n\nInstructions must never explicitly mention a context, a system, a course, or an extract. \\\n\nInstructions must be self-contained and general. \\\n\nAnswers must imitate the writing style of the context. \\",
      "content_length": 1517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 696,
      "content": "Example instruction: Explain the concept of an LLM Twin. \\\n\nExample answer: An LLM Twin is essentially an AI character that mimics your writing style, personality, and voice. \\\n\nIt's designed to write just like you by incorporating these elements into a language model. \\\n\nThe idea is to create a digital replica of your writing habits using advanced AI techniques. \\\n\nProvide your response in JSON format with the following structure:\n\n{{\n\n\"instruction_answer_pairs\": [\n\n{{\"instruction\": \"...\", \"answer\": \"...\"}},\n\n...\n\n]\n\n}}\n\nExtract:\n\n{extract}\n\n\"\"\"\n\nIn addition to the user prompt, we can also specify a system prompt to guide the model into generating the expected instructions. Here, we repeat our high-level task in the system prompt.\n\nThe concatenation of the system and user prompts is fed to the OpenAI API, using the GPT-4o mini model in JSON mode and a maximum of 1,200 tokens in the answer. We also use a standard temperature of\n\n0.7\n\nto encourage diverse responses. The generated text is directly parsed using the InstructionAnswerSet class to return pairs of instructions and answers.\n\ncompletion = client.chat.completions.create( model=\n\n\"gpt-4o-mini\"\n\n, messages=[ {\n\n\"",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 697,
      "content": "role\"\n\n:\n\n\"system\"\n\n,\n\n\"content\"\n\n:\n\n\"You are a helpful assistant who \\\n\ngenerates instruction-answer pairs based on the given context. \\\n\nProvide your response in JSON format.\"\n\n, }, {\n\n\"role\"\n\n:\n\n\"user\"\n\n,\n\n\"content\"\n\n: prompt}, ], response_format={\n\n\"type\"\n\n:\n\n\"json_object\"\n\n}, max_tokens=\n\n1200\n\n, temperature=\n\n0.7\n\n, )\n\n# Parse the structured output\n\nresult = InstructionAnswerSet.from_json(completion.choices[\n\n0\n\n].message.content)\n\n# Convert to list of tuples\n\nreturn",
      "content_length": 477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 698,
      "content": "result.pairs\n\nLet’s create a main function to automate the process. It extracts substrings from the input dataset, then uses concurrent processing via Python’s\n\nThreadPoolExecutor\n\nto efficiently generate instruction-answer pairs for each extract.\n\nWe use a default\n\nmax_workers\n\nvalue of 4 because higher values tend to exceed OpenAI’s rate limits, potentially causing API request failures or throttling.\n\ndef\n\ncreate_instruction_dataset\n\n(\n\ndataset: Dataset, client: OpenAI, num_workers:\n\nint\n\n=\n\n4\n\n) -> Dataset: extracts = extract_substrings(dataset) instruction_answer_pairs = []\n\nwith\n\nconcurrent.futures.ThreadPoolExecutor(max_workers=num_workers)\n\nas\n\nexecutor: futures = [executor.submit(generate_instruction_answer_pairs, extract, client)\n\nfor\n\nextract\n\nin\n\nextracts ]",
      "content_length": 778,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 699,
      "content": "for\n\nfuture\n\nin\n\ntqdm(concurrent.futures.as_completed(futures), total=\n\nlen\n\n(futures) ): instruction_answer_pairs.extend(future.result()) instructions, answers =\n\nzip\n\n(*instruction_answer_pairs)\n\nreturn\n\nDataset.from_dict( {\n\n\"instruction\"\n\n:\n\nlist\n\n(instructions),\n\n\"output\"\n\n:\n\nlist\n\n(answers)} )\n\nWe can create our instruction dataset by calling this function. Running it over the raw data with GPT-4o mini costs less than 0.5$.\n\nWe can now create a main function to orchestrate the entire pipeline. It loads the raw data, creates the instruction dataset, splits it into training and testing sets, and pushes the result to the Hugging Face Hub.\n\ndef\n\nmain\n\n(",
      "content_length": 663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 700,
      "content": "dataset_id:\n\nstr\n\n) -> Dataset: client = OpenAI()\n\n# 1. Load the raw data\n\nraw_dataset = load_articles_from_json(\n\n\"cleaned_documents.json\"\n\n)\n\nprint\n\n(\n\n\"Raw dataset:\"\n\n)\n\nprint\n\n(raw_dataset.to_pandas())\n\n# 2. Create instructiondataset\n\ninstruction_dataset = create_instruction_dataset(raw_dataset, client)\n\nprint\n\n(\n\n\"Instruction dataset:\"\n\n)\n\nprint\n\n(instruction_dataset.to_pandas())\n\n# 3. Train/test split and export\n\nfiltered_dataset = instruction_dataset.train_test_split(test_size=\n\n0.1\n\n) filtered_dataset.push_to_hub(\n\n\"mlabonne/llmtwin\"\n\n)\n\nreturn\n\nfiltered_dataset\n\nDataset({",
      "content_length": 587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 701,
      "content": "features: [\n\n'instruction'\n\n,\n\n'output'\n\n],\n\nnum_rows:\n\n3335\n\n})\n\nWe obtained 3,335 pairs with this process. You can find our version of the dataset at https://huggingface.co/datasets/mlabonne/llmtwin. The Hugging Face Hub provides a convenient dataset viewer (see Figure 5.7) to explore instructions and answers and make sure that there are no obvious mistakes in these samples. Due to the small size of the dataset, there is no need for comprehensive exploration and topic clustering.\n\nFigure 5.7 – The mlabonne/llmtwin instruction dataset on the Hugging Face Hub\n\nAs seen in the previous section, we could refine this instruction dataset by increasing the diversity and complexity of our samples. More advanced prompt engineering could also increase the quality of the generated data by providing examples of the expected results, for instance. Finally, quality evaluation could help filter out low- quality samples by reviewing them individually. For conciseness and simplicity, we will keep a straightforward approach for this instruction dataset and explore more advanced methods in Chapter 6 when we create a preference dataset.\n\nIn the next section, we will introduce SFT techniques, as well as related concepts.",
      "content_length": 1220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 702,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 703,
      "content": "Exploring SFT and its techniques\n\nSFT consists of re-training pre-trained models on a smaller dataset composed of pairs of instructions and answers. The goal of SFT is to turn a base model, which can only perform next-token prediction, into a useful assistant, capable of answering questions and following instructions. SFT can also be used to improve the general performance of the base model (general-purpose SFT), instill new knowledge (e.g., new languages, domains, etc.), focus on specific tasks, adopt a particular voice, and so on.\n\nIn this section, we will discuss when to use fine-tuning and explore related concepts with storage formats and chat templates. Finally, we will introduce three popular ways of implementing SFT: full- finetuning, Low-Rank Adaptation (LoRA) and Quantization-aware Low-Rank Adaptation (QLoRA).",
      "content_length": 830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 704,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 705,
      "content": "When to fine-tune\n\nIn most scenarios, it is recommended to start with prompt engineering instead of directly fine-tuning models. Prompt engineering can be used with either open-weight or closed-source models. By using techniques like few-shot prompting or retrieval augmented generation (RAG), numerous problems can efficiently be tackled without SFT. Prompt engineering also allows us to build a robust evaluation pipeline, which measures metrics like accuracy, but also cost and latency. If these results do not match the requirements, we can explore the possibility of creating an instruction dataset, as illustrated in the previous section. If enough data is available, fine-tuning becomes an option.\n\nFigure 5.8 – Basic flowchart to determine when fine-tuning is an option on a technical level\n\nBeyond these technical considerations, SFT answers common needs in terms of control (“know your data”) and customizability (the fine-tuned",
      "content_length": 938,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 706,
      "content": "model is unique). Instead of building applications around a chatbot, fine- tuning allows developers to create more diverse interactions with LLMs, like tool analytics, moderation, and additional context. Note that if we focus on open-weight models in this book, several LLM providers offer automated fine-tuning services. While they don’t offer the same level of control and customizability as managing your own fine-tuning pipeline, it can be an interesting trade-off in specific scenarios (e.g., limited resources in terms of machine learning engineering).\n\nDespite these advantages, fine-tuning also has limitations. It is generally understood that SFT leverages pre-existing knowledge in the base model’s weights and refocuses the parameters for a specific purpose. This has several implications. First of all, knowledge that is too distant from what has been learned in the pre-training set (such as an unknown or rare language) can be difficult to learn effectively.\n\nEven worse, a study showed that fine-tuning a model on new knowledge could result in more frequent hallucinations. Depending on the SFT technique that is used, we’re also at risk of erasing knowledge that was present in the base model (a common issue referred to as “catastrophic forgetting”).",
      "content_length": 1267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 707,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 708,
      "content": "Instruction dataset formats\n\nInstruction datasets are stored in a particular format to organize instructions and answers. Typically, each sample in the dataset can be represented as a Python dictionary, where keys are prompt types like\n\nsystem\n\n,\n\ninstruction\n\n,\n\noutput\n\n, and values corresponding to the actual text. The three most standard formats are Alpaca, ShareGPT, and OpenAI. The following table shows how these data formats are generally organized.\n\nName\n\nJSONL format\n\nAlpaca\n\n{“instruction”: “...”, “input”: “...”, “output”: “...”} {“instruction”: “...”, “output”: “...”}\n\nShareGPT\n\n{“conversations”: [{“from”: “...”, “value”: “...”}, …]}\n\nOpenAI\n\n{“conversations”: [{“role”: “...”, “content”: “...”}, …]}\n\nOASST\n\n{“INSTRUCTION”: “...”, “RESPONSE”: “...”}\n\nRaw text\n\n{“text”: “...”}\n\nTable 5.5 – Examples of instruction data storage format\n\nNote that for Alpaca, the “\n\ninput\n\n\" key is optional. The content of the “\n\ninput",
      "content_length": 935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 709,
      "content": "\" key is only appended to the content of the “\n\ninstruction\n\n\" key when it exists. We also added the “\n\nraw text\n\n\" data format to show that SFT is not inherently different from pre-training. If you choose to re-train a model on raw text, this is a type of fine-tuning generally called “continual pre-training.”\n\nThe dataset we created in the previous section has two columns (“\n\ninstruction\n\n\" and “\n\noutput\n\n\") and corresponds to the Alpaca format. Alpaca is sufficient for single-turn instructions and answers, which means it is limited to one instruction and one answer. When you want to process conversations (multiple instructions and answers), formats like ShareGPT or OpenAI are a better fit. By storing each message as a dictionary in a list, they can represent an arbitrarily long conversation in each sample.\n\nThe choice of single-turn and multi-turn conversations directly impacts the storage type and depends on the end use case.",
      "content_length": 942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 710,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 711,
      "content": "Chat templates\n\nOnce the instruction-answer pairs are parsed from the dataset format, we want to structure them in a chat template. Chat templates offer a unified way to present the instructions and answers to the model.\n\nIn general, they also include special tokens to identify the beginning and the end of a message, or who is the author of the message. Since base models are not designed to follow instructions, they don’t have a chat template. This means that you can choose any template when you fine-tune a based model. If you want to fine-tune an instruct model (not recommended), you need to use the same template or it might degrade your performance.\n\nLike instruction dataset formats, there are different chat templates: ChatML, Llama 3, Mistral, and many others. In the open-source community, the ChatML template (originally from OpenAI) is a popular option. It simply adds two special tokens\n\n(<|im_start|> and <|im_end|>\n\n) to indicate who is speaking. To give you an example, here is what we obtain when we apply the ChatML template to the instruction-answer pair shown in Table 5.1:\n\n<|im_start|>system You are a helpful assistant, who always provide explanation. Think like you are answering to a\n\nTable 5.6 – Sample from Table 5.1 with the ChatML chat template\n\nAs you can see, we still have three distinct parts: system, user, and assistant. Each part starts with the\n\n<|im_start|>\n\ntoken and ends with\n\n<|im_end|>.\n\nThe current speaker is identified by a string (like “\n\nsystem\n\n\") instead of a special token. This is the exact string that is tokenized and used as input by the model during fine- tuning.",
      "content_length": 1623,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 712,
      "content": "However, during inference, we can’t provide the expected answer. In this case, we provide the system and user part as shown in Figure 5.6, and prompt the model to answer by adding\n\n<|im_start|>assistant\\n\n\n.\n\nBecause the model has been fine-tuned with this template, it understands that the next tokens should be an answer relevant to the user instruction and guided by the system prompt. This is how fine-tuned models acquire instruction-following capabilities.\n\nA common issue with chat templates is that every single whitespace and line break is extremely important. Adding or removing any character would result in a wrong tokenization, which negatively impacts the performance of the model. For this reason, it is recommended to use reliable templates like Jinja, as implemented in the Transformers library. Table 5.7 shows a few examples of such templates, including Alpaca, which is both the name of an instruction dataset format and a chat template.\n\nName\n\nJinja template\n\nAlpaca\n\n### Instruction: What is the capital of France? ### Response: The capital of France is Paris.\n\nChatML\n\n<|im_start|>user What is the capital of France?<|im_end|> <|im_start|>assistant The capital of France is\n\nLlama 3\n\n<|begin_of_text|><|start_header_id|>user<|end_header_id|> What is the capital of France?<|eot_id|><|s\n\nPhi-3\n\n<|user|> What is the capital of France?<|end|> <|assistant|> The capital of France is Paris.<|end|>\n\nGemma\n\nuser What is the capital of France? model The capital of France is Paris.\n\nTable 5.7 – Example of common chat templates\n\nJinja implements loops and conditions, which allow the same template to be used for training and inference (\n\nadd_generation_prompt\n\n).",
      "content_length": 1681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 713,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 714,
      "content": "Parameter-efficient fine-tuning techniques\n\nWhile many techniques exist in the literature, SFT has converged on three main techniques: full fine-tuning, LoRA, and QLoRA. We will introduce each technique individually, and weigh their pros and cons depending on your use cases.\n\nFigure 5.9 – Architectural differences of the three main SFT techniques at the module level",
      "content_length": 368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 715,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 716,
      "content": "Full fine-tuning\n\nFull fine-tuning refers to the most straightforward SFT technique, consisting of re-training every parameter in the base model. Like pre-training, SFT uses next-token prediction as its training objective. This means that the previously discussed structure of the dataset can be seen as the main difference between continual pre-training and full fine-tuning.\n\nThis method often provides the best results but requires significant computational resources. Memory usage depends on several factors, including model size, training techniques, and optimization methods. At its simplest, using a single-GPU setting, the memory required can be estimated using the following formula:\n\nFor a basic setup using 32-bit floating point (fp32) precision, we can estimate:\n\nParameters: Learnable weights and biases within a neural network. In a large language model, these are typically the weights in the attention mechanisms, feed-forward layers, and embedding layers. Cost: 4 bytes/parameter (FP32) or 2 bytes/parameter (FP16/BF16).\n\nGradients: Gradients are the partial derivatives of the loss function with respect to each model parameter. They indicate how much each parameter should be adjusted to minimize the loss. During training, gradients are",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 717,
      "content": "computed for each parameter through backpropagation and are used to update the model parameters. Cost: 4 bytes/parameter.\n\nOptimizer states: Optimizer states are additional values maintained by optimization algorithms like Adam or AdamW. These typically include running averages of past gradients and past squared gradients for each parameter. They help in adapting the learning rate for each parameter and navigating the loss landscape more effectively. For instance, Adam maintains two additional values (momentum and variance) per parameter. Cost: 8 bytes/parameter (for Adam optimizer).\n\nActivations: Activations are the intermediate outputs of each layer in the neural network during the forward pass. For transformer-based models, this includes the outputs of attention mechanisms, feed-forward layers, and normalization layers. Activations need to be kept in memory during the forward pass to compute gradients in the backward pass, unless techniques like activation checkpointing are used. Cost: variable, but often negligible for small batch sizes.\n\nThis gives us a baseline of 16 bytes per parameter. This translates into 112 GB of VRAM for a 7 B model and 1,120 GB for a 70 B model. However, this is often an underestimate, as it doesn’t account for additional memory needed for activations, temporary buffers, and overhead from various training techniques.\n\nSeveral techniques can be employed to reduce memory usage during LLM fine-tuning. Model parallelism spreads the workload across multiple GPUs, though it adds some overhead. Gradient accumulation enables larger effective batch sizes without proportional memory increase. Memory- efficient optimizers like 8-bit Adam can reduce the footprint of optimizer",
      "content_length": 1722,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 718,
      "content": "states. Activation checkpointing trades computation for memory by recalculating certain activations. When combined, these techniques can significantly lower memory usage. For instance, using mixed precision with model parallelism might reduce costs to around 14-15 bytes per parameter, compared to the 16-byte baseline. However, memory requirements remain substantial for large models even with these optimizations.\n\nIn addition, full fine-tuning directly modifies the pre-training weights, which makes it destructive by nature. If training doesn’t behave as expected, it might erase previous knowledge and skills – a phenomenon referred to as “catastrophic forgetting.” The same phenomenon can happen with continual pre-training, which generally makes these techniques more difficult to use. Due to this additional complexity and its high computational requirements, parameter-efficient techniques are often preferred to full fine-tuning to create task and domain-specific models.",
      "content_length": 981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 719,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 720,
      "content": "LoRA\n\nLoRA is a parameter-efficient technique for fine-tuning LLMs. Developed to address the computational challenges associated with adapting massive neural networks, LoRA has quickly become a cornerstone technique in LLM fine-tuning.\n\nThe primary purpose of LoRA is to enable the fine-tuning of LLMs with significantly reduced computational resources. This is achieved by introducing trainable low-rank matrices that modify the behavior of the model without changing its original parameters. The key advantages of LoRA include:\n\nDramatically reduced memory usage during training\n\nFaster fine-tuning process\n\nPreservation of pre-trained model weights (non-destructive)\n\nAbility to switch between tasks efficiently by swapping LoRA weights",
      "content_length": 739,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 721,
      "content": "These benefits have made LoRA particularly attractive for researchers and developers working with limited computational resources, effectively democratizing the process of LLM fine-tuning.\n\nAt its core, LoRA employs a low-rank decomposition technique to update model weights efficiently. Instead of directly modifying the original weight\n\nmatrix together form a low-rank update to\n\n, LoRA introduces two smaller matrices,\n\n.\n\nand\n\n, which\n\nFigure 5.10 – LoRA adds the two trainable matrices frozen\n\npre-trained weights\n\nand\n\nand keeps the\n\nMathematically, this can be represented as:",
      "content_length": 583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 722,
      "content": "Here,\n\nis the original weight matrix,\n\nand\n\nare the LoRA matrices, and\n\nis the effective weight matrix used during inference.\n\nThe dimensions of matrices A and B are chosen such that their product has the same shape as\n\n, but with a much lower rank. This rank, typically\n\ndenoted as original weights approach significantly reduces the number of trainable parameters, leading to substantial memory savings and faster training times.\n\n, is a crucial hyperparameter in LoRA. During training, the are updated. This\n\nremain frozen, while only\n\nand\n\nTo implement LoRA effectively, we need to select the correct hyperparameters and target modules. LoRA comes with two hyperparameters:\n\nRank ( starting point is some cases. Larger ranks may capture more diverse tasks but could lead to overfitting.\n\n): Determines the size of the LoRA matrices. A common\n\n, but values up to 256 have shown good results in\n\nAlpha (\n\n): A scaling factor applied to the LoRA update. In practice,\n\nwe update the frozen weights\n\nby a factor of\n\n. This is why a\n\ncommon heuristic is to set\n\nto twice the value of\n\n, effectively",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 723,
      "content": "applying a scaling factor of 2 to the LoRA update. You can experiment with different ratios in case of overfitting or underfitting.\n\nIn addition, it is possible to add a drop-out layer to prevent overfitting. The dropout rate is usually set between 0 and 0.1 as an optional regularization factor, which slightly decreases training speed.\n\nLoRA can be applied to various parts of the model architecture. Initially, LoRA was primarily focused on modifying the attention mechanism, specifically the query (Q) and value (V) matrices in transformer layers. However, experiments have demonstrated significant benefits in extending LoRA’s application to other key components of the model. These additional target modules include:\n\nKey (K) matrices in attention layers\n\nOutput projection layers (often denoted as O) in attention mechanisms\n\nFeed-forward or Multi-Layer Perceptron (MLP) blocks between attention layers\n\nLinear output layers",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 724,
      "content": "However, it’s important to note that increasing the number of LoRA- adapted modules also increases the number of trainable parameters and, consequently, the memory requirements.\n\nUsing LoRA, it’s possible to fine-tune a 7B parameter model on a single GPU with as little as 14-18 GB of VRAM, depending on the specific configuration. This is a dramatic reduction compared to full fine-tuning, which would typically require multiple high-end GPUs. In terms of trainable parameters, LoRA drastically reduces the number compared to full fine-tuning. For example, even when targeting every module with a rank of 16, a Llama 3 8 B model only has 42 million trainable LoRA parameters out of 8 billion parameters, which is 0.5196% of the model’s parameters.\n\nIn terms of quality, LoRA can also achieve comparable or sometimes better results than full-fine-tuning. Multiple sets of LoRA weights can be combined for different tasks or domains, allowing flexible deployment and task switching without retraining. Different projects are specialized in multiple-LoRA serving, such as LoRAX. It’s also a feature supported by Hugging Face’s Text Generation Inference (TGI) and Nvidia Inference Microservices (NIM).",
      "content_length": 1198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 725,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 726,
      "content": "QLoRA\n\nIntroduced by Dettmers et al., QLoRA is a method for fine-tuning LLMs that addresses the challenges of high computational costs. By combining quantization techniques with LoRA, QLoRA allows developers to fine-tune models on relatively small, widely available GPUs.\n\nThe core of QLoRA’s approach involves quantizing the base model parameters to a custom 4-bit NormalFloat (NF4) data type, which significantly reduces memory usage. Like LoRA, instead of updating all model parameters during fine-tuning, QLoRA introduces small, trainable low-rank matrices (adapters) to specific layers of the model. Only these adapters are updated during training, while the original model weights remain unchanged. To further reduce memory usage, QLoRA employs double quantization, which quantizes the quantization constants themselves. Additionally, it uses paged optimizers to manage memory spikes during training by leveraging Nvidia’s unified memory feature.\n\nQLoRA provides significant memory savings compared to LoRA, reducing peak GPU memory usage by up to 75%. For example, for a 7B model, QLoRA reduces peak memory usage from 14 GB to 9.1 GB during initialization, a 35% reduction. During fine-tuning, the memory savings increase to 40%, from 15.6 GB for LoRA to 9.3 GB for QLoRA. However, this memory efficiency comes at the cost of increased training time, with QLoRA being about 30% slower than LoRA. In terms of model performance, QLoRA shows only minor differences compared to LoRA.",
      "content_length": 1486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 727,
      "content": "In summary, QLoRA is particularly beneficial when memory constraints are the primary concern, such as when working with very large models or on hardware with limited GPU memory. However, if training speed is crucial and sufficient memory is available, LoRA might be the preferred choice.\n\nThe decision between QLoRA and LoRA should be based on the specific requirements of the project, available hardware, and the need to balance memory usage, training speed, and model performance.",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 728,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 729,
      "content": "Training parameters\n\nWhen fine-tuning LLMs, several hyperparameters guide the training process and significantly impact the model’s convergence, generalization, and overall effectiveness.",
      "content_length": 187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 730,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 731,
      "content": "Learning rate and scheduler\n\nThe learning rate is the most important hyperparameter. It controls how much the model’s parameters are updated during training. It typically ranges from very small values like\n\n1e-6\n\nto larger values like\n\n1e-3\n\n. A common starting point for transformer models is often around\n\n1e-5\n\n. If the learning rate is too low, training progresses slowly and may get stuck in suboptimal solutions. Conversely, if it’s too high, training can become unstable or diverge, leading to poor performance. It’s often beneficial to experiment with different learning rates to find the optimal value for your specific task and model.\n\nThe learning rate scheduler adjusts the learning rate throughout the training process. It typically starts with a higher learning rate to enable rapid initial progress, then gradually decreases it in later stages to fine-tune the model more precisely. The two most common types of schedulers are linear and cosine. A linear scheduler decreases the learning rate steadily over time, while a cosine scheduler follows a cosine curve, decreasing more slowly at first and then more rapidly toward the end of training. For example, you might start with a learning rate of 3e-4 and decrease it to 1e-7 over the course of training. The specific values and decay schedule depend on your model and dataset, but a common approach is to use a warmup period (e.g.,",
      "content_length": 1397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 732,
      "content": "5% of total steps) where the learning rate increases from 0 to the initial value, followed by a decay period for the remaining 95% of steps. This approach helps stabilize early training and allows for more refined updates as the model converges. In general, linear and cosine schedulers provide the same level of performance.",
      "content_length": 325,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 733,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 734,
      "content": "Batch size\n\nThe batch size determines the number of samples processed before the model’s weights are updated. Typical batch sizes for LLM fine-tuning range from 1 to 32, with common values being 1, 2, 4, 8, or 16. Larger batch sizes generally lead to more stable gradient estimates and can improve training speed, as they provide a better approximation of the true gradient of the entire dataset.\n\nHowever, they also require more memory, which can be a limiting factor on GPUs with less VRAM. For instance, a batch size of 16 might work well on a high-end GPU with 24GB of memory, while a smaller GPU with 8 GB might only handle a batch size of 2 or 4.\n\nTo overcome memory constraints while still benefiting from larger batch sizes, a technique called gradient accumulation can be used. It works by performing multiple forward and backward passes with smaller mini- batches, accumulating the gradients over these steps before applying a single update to the model’s parameters. This approach is particularly useful when working with large models or limited GPU memory. For example, if you want to achieve an effective batch size of 32 but your GPU can only handle 8 samples at a time, you can set the gradient accumulation steps to 4. This means you’ll process 4 mini-batches of 8 samples each, accumulating the gradients, and then update the model as if you had processed all 32 samples at once.\n\nThe number of gradient accumulation steps typically ranges from 1 (no accumulation) to 8 or 16, depending on the desired effective batch size and",
      "content_length": 1543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 735,
      "content": "available computational resources. When choosing the number of steps, consider the trade-off between training speed and memory usage. More accumulation steps allow for larger effective batch sizes but increase the time required for each update. Here’s a simple formula to determine the effective batch size:\n\nFor instance, if you’re using 2 GPUs, each processing a batch of 4 samples, with 4 gradient accumulation steps, your effective batch size would be\n\n4 * 2 * 4 = 32\n\nsamples.",
      "content_length": 481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 736,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 737,
      "content": "Maximum length and packing\n\nThe maximum sequence length determines the longest input the model can process. It’s typically set between 512 and 4,096 tokens but can go up to 128,000 or more, depending on the task and available GPU memory. For example, a maximum length of 2,048 tokens is common for many language generation tasks, while RAG applications might use up to 8,192 tokens or more. When processing input data, sequences longer than this limit are truncated, meaning excess tokens are removed. Truncation can occur at the beginning (left truncation) or end (right truncation) of the sequence. For instance, with a maximum length of 1,024 tokens, a 1,500-token input would have 476 tokens removed. This parameter directly impacts batch size and memory usage; a batch size of 12 with a max length of 1,024 would contain 12,288 tokens (\n\n12 * 1,024\n\n), while the same batch size with a max length of 512 would only contain 6,144 tokens. It’s important to balance this parameter with your GPU capabilities and the nature of your training data to optimize performance and resource utilization.\n\nPacking maximizes the utilization of each training batch. Instead of assigning one sample per batch, packing combines multiple smaller samples into a single batch, effectively increasing the amount of data processed in each iteration. For example, if your maximum sequence length is 1,024 tokens, but many of your samples are only 200-300 tokens long, packing could allow you to fit 3-4 samples into each batch slot. This approach can significantly improve training efficiency, especially when dealing with datasets containing many short sequences. However, packing requires careful implementation to ensure that model attention doesn’t cross between",
      "content_length": 1748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 738,
      "content": "packed samples. This is typically achieved by using attention masks that prevent the model from attending to tokens from different samples within the same packed sequence.",
      "content_length": 171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 739,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 740,
      "content": "Number of epochs\n\nThe number of epochs is another important parameter, representing the number of complete passes through the entire training dataset. For LLM fine-tuning, the typical range is 1 to 10 epochs, with many successful runs using 2 to 5 epochs. The optimal number depends on factors such as task complexity, dataset size, and model architecture. More epochs allow the model to refine its learning, potentially improving performance. However, there’s a crucial trade-off: too few epochs may lead to underfitting, while too many can cause overfitting. For example, a large model fine-tuned on a small dataset might only need 1-3 epochs, while a smaller model fine-tuned on a larger dataset could benefit from 5-10 epochs. It is helpful to monitor validation performance during training and implement early stopping if the model’s performance plateaus or degrades. This approach helps determine the optimal number of epochs dynamically and prevents overfitting.",
      "content_length": 969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 741,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 742,
      "content": "Optimizers\n\nOptimizers adjust the model’s parameters to minimize the loss function. For LLM fine-tuning, AdamW (Adaptive Moment Estimation with Weight Decay) is highly recommended, particularly its 8-bit version. AdamW 8-bit performs comparably to the 32-bit version while using less GPU memory (but it doesn’t improve training speed). AdamW combines adaptive learning rates with weight decay regularization, often leading to better training stability and model performance.\n\nFor scenarios with severe memory constraints, AdaFactor presents an alternative designed for memory efficiency. It works well without explicit learning rate tuning, making it particularly useful in resource-constrained environments. However, it may not always match AdamW’s performance in all cases. In situations involving extremely large models or limited GPU memory, paged versions of optimizers, such as paged AdamW 8-bit, can further reduce memory consumption by offloading to CPU RAM. If memory allows and maximum performance is the priority, the non- quantized\n\nadamw_torch\n\noptimizer may be the best choice.",
      "content_length": 1091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 743,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 744,
      "content": "Weight decay\n\nWeight decay works by adding a penalty for large weights to the loss function, encouraging the model to learn simpler, more generalizable features. This helps the model avoid relying too heavily on any single input feature, which can improve its performance on unseen data. Typically, weight decay values range from 0.01 to 0.1, with 0.01 being a common starting point. For example, if you’re using the AdamW optimizer, you might set the weight decay to 0.01.\n\nWhile weight decay can be beneficial, setting it too high can impede learning by making it difficult for the model to capture important patterns in the data. Conversely, setting it too low may not provide sufficient regularization. The optimal weight decay value often depends on the specific model architecture and dataset, so it’s generally a good practice to experiment with different values.",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 745,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 746,
      "content": "Gradient checkpointing\n\nGradient checkpointing is a technique that reduces memory consumption during training by storing only a subset of intermediate activations generated in the forward pass. In standard training procedures, all intermediate activations are retained in memory to facilitate gradient calculation during the backward pass. However, for very deep networks like LLMs, this approach can quickly become impractical due to hardware limitations, especially on GPUs with limited memory capacity.\n\nGradient checkpointing addresses this challenge by selectively saving activations at specific layers within the network. For layers where activations are not saved, they are recomputed during the backward pass as needed for gradient computation. This approach creates a trade-off between computation time and memory usage. While it significantly reduces memory requirements, it may increase overall computation time due to the need to recalculate some activations.\n\nOther parameters and techniques exist but play a minor role compared to those previously discussed. In the next section, we will explore how to select and tune these parameters using a concrete example.",
      "content_length": 1175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 747,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 748,
      "content": "Fine-tuning in practice\n\nLet’s now fine-tune an open-source model on our custom dataset. In this section, we will show an example that implements LoRA and QLoRA for efficiency. Depending on the hardware you have available, you can select the technique that best corresponds to your configuration.\n\nThere are many efficient open-weight models we can leverage for task or domain-specific use cases. To select the most relevant LLM, we need to consider three main parameters:\n\nLicense: Some model licenses only allow non-commercial work, which is a problem if we want to fine-tune for a company. Custom licenses are common in this field, and can target companies with a certain number of users, for example.\n\nBudget: Models with smaller parameter sizes (<10 B) are a lot cheaper to fine-tune and deploy for inference than larger models. This is due to the fact that they can be run on cheaper GPUs and process more tokens per second.\n\nPerformance: Evaluating the base model on general-purpose benchmarks or, even better, domain- or task-specific benchmarks relevant to the final use case, is crucial. This helps ensure that the model has the necessary capabilities to perform well on the intended tasks after fine-tuning.",
      "content_length": 1218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 749,
      "content": "In this chapter, we will choose Llama 3.1 8B, an open-weight model released by Meta. It has a permissive custom license (“Llama 3.1 Community License Agreement”) that allows commercial use. With 8B parameters, it is small enough to fit on most GPUs while reaching a high level of performance compared to its competitors. We can verify this using the Open LLM Leaderboard, as well as other benchmarks detailed in the model card.\n\nThere are specialized tools and libraries to fine-tune models. In particular, we recommend the following:\n\nTRL: This is a library created and maintained by Hugging Face to train LLMs using SFT and preference alignment. It is a popular and reliable library that tends to be the most up-to-date in terms of algorithms. It works in single and multi-GPU settings with FSDP and DeepSpeed.\n\nAxolotl: Created by Wing Lian, this tool streamlines the fine-tuning of LLMs with reusable YAML configuration files. It is based on TRL but includes many additional features, such as automatically combining datasets stored in various formats. It also supports single- and multi-GPU settings with FSDP and DeepSpeed.\n\nUnsloth: Created by Daniel and Michael Han, Unsloth uses custom kernels to speed up training (2-5x) and reduce memory use (up to 80% less memory). It is based on TRL and provides many utilities, such as automatically converting models into the GGUF quantization format. At the time of writing, it is only available for single-GPU settings.",
      "content_length": 1470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 750,
      "content": "To maximize efficiency, we will perform fine-tuning using the Unsloth library. The following code is designed as part of our LLMOps pipeline, but can also be used as a stand-alone script. It can also be executed in different environments, like SageMaker, cloud GPUs (like Lambda Labs or RunPod), Google Colab, and many others. We tested it on different GPUs, like A40, A100, and L4.\n\nTo install the Unsloth library and its dependencies, we recommend directly installing from the GitHub repository of the book (https://github.com/PacktPublishing/LLM-Engineering) or Unsloth’s repo (https://github.com/unslothai/unsloth). This approach is recommended because the installation steps are regularly updated to address potential conflicts with dependencies:\n\nFirst, we want to access a gated model and (optionally) upload our fine- tuned model to Hugging Face (https://huggingface.co/). This requires being logged in to an account. If you don’t have an account, you can create it and store your API key (Settings | Access Tokens | Create new token) in the .env file:\n\nHF_TOKEN = YOUR_API_KEY",
      "content_length": 1085,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 751,
      "content": "Make sure that your Comet ML API key is also in the .env file:\n\nCOMET_API_KEY = YOUR_API_KEY\n\nImport all the necessary packages:\n\nimport\n\nos\n\nimport\n\ntorch\n\nfrom\n\ntrl\n\nimport\n\nSFTTrainer\n\nfrom\n\ndatasets\n\nimport",
      "content_length": 210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 752,
      "content": "load_dataset, concatenate_datasets\n\nfrom\n\ntransformers\n\nimport\n\nTrainingArguments, TextStreamerfrom unsloth\n\nimport\n\nFastLanguageModel, is_bfloat16_supported\n\nLet’s now load the model to fine-tune and its corresponding tokenizer. We use Unsloth’s FastLaguageModel class with the\n\n.from_pretrained()\n\nmethod. In addition to the model name, we need to specify the max sequence length (2,048 in this example). Finally, the\n\nload_in_4bit\n\nargument indicates if we want to use QLoRA (quantized pre-trained weights) or LoRA.\n\nWe’ll use LoRA in this example because of faster training and higher quality, but you can easily switch to QLoRA if you don’t meet the VRAM requirements.",
      "content_length": 673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 753,
      "content": "max_seq_length =\n\n2048\n\nmodel, tokenizer = FastLanguageModel.from_pretrained( model_name=\n\n\"meta-llama/Meta-Llama-3.1-8B\"\n\n, max_seq_length=max_seq_length, load_in_4bit=\n\nFalse\n\n, )\n\nNow that the model is loaded, we can define our LoRA configuration. Here, we use a rank of 32 that is large enough to imitate the writing style and copy the knowledge from our instruction samples. You can increase this value to 64 or 128 if your results are underwhelming. We also set an alpha of 32, without dropout and without bias, to speed up training. Finally, we target every linear layer to maximize the quality of the fine-tuning process.\n\nmodel = FastLanguageModel.get_peft_model( model, r=\n\n32\n\n, lora_alpha=",
      "content_length": 701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 754,
      "content": "32\n\n, lora_dropout=\n\n0\n\n, target_modules=[\n\n\"q_proj\"\n\n,\n\n\"k_proj\"\n\n,\n\n\"\n\nv_proj\"\n\n,\n\n\"up_proj\"\n\n,\n\n\"down_proj\"\n\n,\n\n\"o_proj\"\n\n,\n\n\"gate_proj\"\n\n], )",
      "content_length": 145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 755,
      "content": "Next, we need to prepare the data in the right format for fine-tuning. In this example, we don’t have a lot of samples in the llmtwin dataset (3,000 samples). This is an issue because the model might not correctly learn the chat template. To address this, we will upsample it with a high-quality general-purpose dataset called FineTome. This is a filtered version of\n\narcee-ai/The-Tome using the fineweb-edu-classifier\n\n. Instead of using the 100,000 samples of this dataset, we will specify we only want 10,000 in the train split. We concatenate these two datasets to create our final set.\n\ndataset1 = load_dataset(\n\n\"mlabonne/llmtwin\"\n\n) dataset2 = load_dataset(\n\n\"mlabonne/FineTome-Alpaca-100k\"\n\n, split=\n\n\"train[:10000]\"\n\n) dataset = concatenate_datasets([dataset1, dataset2])\n\nNow, we need to format this data using a chat template. Let’s use the Alpaca template for convenience. This template doesn’t require additional tokens, which makes it less error-prone (but can slightly",
      "content_length": 983,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 756,
      "content": "impact performance compared to ChatML). Here, we map all the instructions and answers to the Alpaca template. We manually add the end of sentence (EOS) token at the end of each message to ensure that the model learns to output it. Without it, it will keep generating answers without ever stopping.\n\nalpaca_template =\n\n\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n\n{}\n\n### Response:\n\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token dataset = dataset.\n\nmap\n\n(format_samples, batched=\n\nTrue\n\n, remove_columns=dataset.column_names)\n\nOnce the dataset is ready, we can divide it into training (95%) and test (5%) sets for validation during training.",
      "content_length": 716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 757,
      "content": "dataset = dataset.train_test_split(test_size=\n\n0.05\n\n)\n\nThe model is now ready to be trained. The SFTTrainer() class stores all the hyperparameters for our training. In addition, we provide the model, tokenizer, LoRA configuration, and datasets. Following the recommendations from the previous section, we set a learning rate of\n\n3e-4\n\nwith a linear scheduler and a maximum sequence length of 2048. We train this model for three epochs with a batch size of 2 and 8 gradient accumulation steps (for an effective batch size of 16). We also choose the\n\nadamw_8bit\n\noptimizer with a\n\nweight_decay\n\nof 0.01. Depending on the GPU we use, it will automatically use FP16 or BF16 for the activations. Finally, we report our training run to Comet ML for experiment tracking.",
      "content_length": 764,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 758,
      "content": "trainer = SFTTrainer( model=model, tokenizer=tokenizer, train_dataset=dataset[\n\n\"train\"\n\n], eval_dataset=dataset[\n\n\"test\"\n\n], dataset_text_field=\n\n\"text\"\n\n, max_seq_length=max_seq_length, dataset_num_proc=\n\n2\n\n, packing=\n\nTrue\n\n, args=TrainingArguments( learning_rate=\n\n3e-4\n\n, lr_scheduler_type=\n\n\"linear\"\n\n, per_device_train_batch_size=\n\n2\n\n, gradient_accumulation_steps=\n\n8\n\n, num_train_epochs=\n\n3",
      "content_length": 400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 759,
      "content": ", fp16=\n\nnot\n\nis_bfloat16_supported(), bf16=is_bfloat16_supported(), logging_steps=\n\n1\n\n, optim=\n\n\"adamw_8bit\"\n\n, weight_decay=\n\n0.01\n\n, warmup_steps=\n\n10\n\n, output_dir=\n\n\"output\"\n\n, report_to=\n\n\"comet_ml\"\n\n, seed=\n\n0\n\n, ), ) trainer.train()",
      "content_length": 241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 760,
      "content": "Training this model on our concatenated dataset can take a few hours. For example, it takes 50 minutes on an A100 GPU.\n\nOnce it’s done, we can test it with a quick example. The goal is not to properly evaluate the fine-tuned model, but to make sure that there are no obvious errors related to the tokenizer or chat template.\n\nFor fast inference, we can use\n\nFastLanguageModel.for_inference()\n\nfrom Unsloth. We directly format an instruction with the Alpaca format. Note that we provide an empty answer to append the assistant header (\n\n### Response\n\n): at the end of the user instruction. This forces the model to answer the instruction instead of completing it. We also use a text streamer to stream the generation instead of waiting for it to be complete before printing it.\n\nFastLanguageModel.for_inference(model) message = alpaca_prompt.\n\nformat\n\n(\n\n\"Write a paragraph to introduce supervised fine-tuning.\"\n\n,",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 761,
      "content": "\"\"\n\n) inputs = tokenizer([message], return_tensors=\n\n\"pt\"\n\n).to(\n\n\"cuda\"\n\n) text_streamer = TextStreamer(tokenizer) _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=\n\n256\n\n, use_cache=\n\nTrue\n\n)\n\nHere is the answer provided by our model:\n\nSupervised fine-tuning is a method used to enhance a language model by providing it with a curated dataset of instructions and their corresponding answers. This process is designed to align the model's responses with human expectations, thereby improving its accuracy and relevance. The goal is to ensure that the model can respond effectively to a wide range of queries, making it a valuable tool for applications such as chatbots and virtual assistants.",
      "content_length": 712,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 762,
      "content": "This is correct and properly formatted with the Alpaca chat template.\n\nNow that our model has been successfully fine-tuned, we can save it locally and/or push it to the Hugging Face Hub using the following functions.\n\nmodel.save_pretrained_merged(\n\n\"model\"\n\n, tokenizer, save_method=\n\n\"merged_16bit\"\n\n) model.push_to_hub_merged(\n\n\"mlabonne/TwinLlama-3.1-8B\"\n\n, tokenizer, save_method=\n\n\"merged_16bit\"\n\n)",
      "content_length": 403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 763,
      "content": "Congratulations on fine-tuning a base model from scratch! During training, you can access Comet ML to monitor your training loss, validation loss, and many other metrics. You want to make sure that these metrics correspond to what is expected. Figure 5.11 shows the training run corresponding to the previous code in Comet ML.\n\nFigure 5.11 – Four monitored metrics during fine-tuning in Comet ML\n\nIn particular, three of these metrics are important to monitor:",
      "content_length": 460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 764,
      "content": "Training loss: It measures how well the model is performing on the task it’s being trained for. The loss should continuously decrease on average, indicating improving performance. We expect a rapid decrease at the beginning of training, followed by a long plateau. Spikes and continuous increases in the loss value are signs that the training is failing. In this case, you might want to check the quality of your data, issues with the tokenizer, and tune parameters like learning rate and batch size. In Figure 5.11 (loss), you can see three different phases corresponding to our three epochs.\n\nValidation loss: It measures the loss using the validation set instead of the training set; a well-fitted model typically shows both training and validation losses decreasing and eventually stabilizing, with a small gap between them. This gap should be minimal but is expected to exist as the model will always perform slightly better on the training data. If the training loss continues to decrease while the validation loss starts to increase, it’s a sign of overfitting. Conversely, if both curves remain flat at a relatively high loss value, it indicates underfitting. There are no universal “recommended ranges” for loss values, as these depend on the specific problem and loss function used. However, you should look for convergence and stability in both curves. In Figure 4.11 (eval_loss), we see a slight increase at step 340. This is still acceptable but might indicate that the model starts to overfit.\n\nGradient norm: It represents the magnitude of the gradient vector during training. Large gradient norms can indicate training instability like overfitting, especially if accompanied by a divergence between training and validation losses. On the other hand, a stable or decreasing gradient norm generally means that the model is converging toward a local optimum. To mitigate issues associated with large gradient norms, gradient clipping can be employed. This technique involves setting a",
      "content_length": 1997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 765,
      "content": "maximum threshold for the gradient norm, effectively limiting the size of parameter updates.\n\nIt is often interesting to try different learning rates and select the best model based on the minimal loss. Note that this is a proxy for real evaluations, which are covered in the next chapter.",
      "content_length": 289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 766,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 767,
      "content": "Summary\n\nThis chapter covered essential aspects of LLM fine-tuning, both in theory and practice. We examined the instruction data pipeline and how to create high-quality datasets, from curation to augmentation. Each pipeline stage offers optimization opportunities, particularly in quality assessment, data generation, and enhancement. This flexible pipeline can be adapted to your use cases by selecting the most relevant stages and techniques.\n\nWe applied this framework to real-world data from Chapter 3, using an LLM to convert raw text into instruction-answer pairs. We then explored SFT techniques. This included an analysis of SFT’s advantages and limitations, methods for storing and parsing instruction datasets with chat templates, and an overview of three primary SFT techniques: full fine- tuning, LoRA, and QLoRA. We compared these methods based on their impact on memory usage, training efficiency, and output quality. The chapter concluded with a practical demonstration that involved fine-tuning a Llama 3.1 8 B model on our custom instruction dataset. This example highlighted key steps and implementation details for successful fine-tuning.\n\nIn the next chapter, we will use preference alignment techniques to create a new version of TwinLlama-3.1-8B. We will generate a new dataset with chosen and rejected answers that will help us calibrate the type of answers we expect from our model. We will detail many applications that can benefit from this framework and how to implement it.",
      "content_length": 1502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 768,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 769,
      "content": "References\n\nTahori, Gulrajani, Zhang, Dubois, et al.. “Alpaca: A Strong, Replicable Instruction-Following Model” crfm.stanford.edu, March 13, 2023, https://crfm.stanford.edu/2023/03/13/alpaca.html.\n\nSubhabrata Mukherjee et al.. “Orca: Progressive Learning from Complex Explanation Traces of GPT-4.” arXiv preprint arXiv:2306.02707, June 2023.\n\nWing Lian and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and “Teknium”. “Open-Orca/OpenOrca.” huggingface.co, 2023, https://huggingface.co/datasets/Open-Orca/OpenOrca.\n\nWeihao Zeng et al.. “Automatic Instruction Evolving for Large Language Models.” arXiv preprint arXiv:2406.00770, June 2024.\n\nChunting Zhou et al.. “LIMA: Less Is More for Alignment.” arXiv preprint arXiv:2305.11206, May 2023\n\n01. AI. “Yi: Open Foundation Models by 01.AI.” arXiv preprint arXiv:2403.04652, March 2024.",
      "content_length": 857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 770,
      "content": "Alex Birch. “LLM finetuning memory requirements.” blog.scottlogic.com, November 24, 2023, https://blog.scottlogic.com/2023/11/24/llm-mem.html.\n\nQuentin Anthony et al.. “Transformer Math 101.” blog.eleuther.ai, April 18, 2023, https://blog.eleuther.ai/transformer-math/.\n\nEdward J. Hu et al.. “LoRA: Low-Rank Adaptation of Large Language Models.” arXiv preprint arXiv:2106.09685, June 2021.\n\nTim Dettmers et al.. “QLoRA: Efficient Finetuning of Quantized LLMs.” arXiv preprint arXiv:2305.14314, May 2023.\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 772,
      "content": "6",
      "content_length": 1,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 773,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 774,
      "content": "Fine-Tuning with Preference Alignment\n\nSupervised Fine-Tuning (SFT) has been crucial in adapting LLMs to perform specific tasks. However, SFT struggles to capture the nuances of human preferences and the long tail of potential interactions that a model might encounter. This limitation has led to the development of more advanced techniques for aligning AI systems with human preferences, grouped under the umbrella term preference alignment.\n\nPreference alignment addresses the shortcomings of SFT by incorporating direct human or AI feedback into the training process. This method allows a more nuanced understanding of human preferences, especially in complex scenarios where simple supervised learning falls short. While numerous techniques exist for preference alignment, this chapter will primarily focus on Direct Preference Optimization (DPO) for simplicity and efficiency.\n\nIn this chapter, we will talk about the type of data that is required by preference alignment algorithms like DPO. We will build our own dataset to modify the writing style of our model, making it less artificial and more authentic. We will introduce the DPO algorithm and implement it to align the model trained in Chapter 5.\n\nIn this chapter, we will cover the following topics:\n\nUnderstanding preference datasets",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 775,
      "content": "How to create our own preference dataset\n\nDirect preference optimization (DPO)\n\nImplementing DPO in practice to align our model\n\nBy the end of this chapter, you will be able to create your own preference datasets and align models with diverse techniques.\n\nAll the code examples from this chapter can be found on GitHub at https://github.com/PacktPublishing/LLM-Engineering.",
      "content_length": 373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 776,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 777,
      "content": "Understanding preference datasets\n\nThe principles for creating high-quality preference datasets are the same as those discussed in Chapter 5 for instruction datasets. We want to maximize the accuracy, diversity, and complexity of our samples. To achieve this, we follow the same stages, as outlined in Figure 6.1: data curation, deduplication, decontamination, quality evaluation, exploration, generation, and augmentation.\n\nFigure 6.1 – Overview of the post-training data pipeline covered in this chapter\n\nTo avoid repetition, this section will focus on the main differences between instruction and preference datasets. We will introduce the structure of preference samples and the ideal size for preference datasets. Then, we will focus on the two stages that differ most from creating instruction datasets: data generation and evaluation.",
      "content_length": 841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 778,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 779,
      "content": "Preference data\n\nPreference datasets lack the standardization of instruction datasets due to varying data requirements across different training algorithms. Preference data comprises a collection of responses to a given instruction, ranked by humans or language models. This chapter focuses on DPO, so we will examine the specific data format required by this algorithm.\n\nAs illustrated in Table 6.1, the structure of DPO datasets is straightforward: each instruction is paired with one preferred answer and one rejected answer. The objective is to train the model to generate the preferred response rather than the rejected one.\n\nInstruction Tell me a joke about octopuses.\n\nChosen answer Why don’t octopuses play cards in casinos? Because they can’t count past eight. Rejected answe\n\nTable 6.1 – Example of sample from the mlabonne/orpo-dpo-mix-40k dataset\n\nIn preference datasets, the rejected response is as important as the chosen one. Without the rejected response, the dataset would be a simple instruction set. Rejected responses represent the behavior we aim to eliminate from the model. This provides a lot of flexibility and allows us to use preference datasets in many contexts. Here is a list of examples where preference datasets are more beneficial to use compared to using SFT alone:\n\nChatbots: In conversational AI, the quality of responses often depends on subjective factors like naturalness, engagement, and contextual appropriateness. A preference dataset allows the model to learn these nuanced aspects by comparing better and worse responses. Simple SFT might not capture the subtleties of what makes one response preferable over another in a given context.\n\nContent moderation: Determining whether content is appropriate or violates guidelines often involves nuanced judgments. Preference datasets can help the model learn to distinguish between borderline cases by comparing examples of content that is and isn’t acceptable. This is more effective than binary classification through SFT, as it helps the model understand the reasoning behind moderation decisions.\n\nSummarization: The quality of a summary often depends on factors like conciseness, relevance, and coherence. By using preference datasets, models can learn to generate summaries that humans find more useful and informative. Simple SFT might result in summaries that are technically correct but less preferable to human readers.",
      "content_length": 2417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 780,
      "content": "Code generation: In coding tasks, there are often multiple correct solutions, but some are more efficient or readable, or follow better practices than others. Preference datasets can help the model learn these qualitative aspects of code quality, which might not be captured by simple correctness-based SFT.\n\nCreative writing: For tasks like story generation or poetry writing, the quality of the output is highly subjective and multifaceted. Preference datasets can capture human judgments about style, creativity, and emotional impact better than instruction datasets, which might focus more on technical correctness or adherence to prompts.\n\nTranslation: While traditional metrics like BLEU scores can measure translation accuracy, they don’t always capture the fluency or naturalness of the translation. Preference datasets can help models learn to produce translations that native speakers prefer, even when multiple translations are technically correct.\n\nIn all these scenarios, preference datasets enable a more refined training approach. They capture subjective quality assessments and human preferences that extend beyond simple correctness or adherence to instructions. This method can produce models that generate output that is not only technically accurate but also better aligned with human judgment and preferences in complex, open-ended tasks.\n\nUnlike instruction datasets, there are no standardized storage formats like Alpaca or ShareGPT. Most preference datasets follow a structure similar to that shown in Table 6.1, with columns for an instruction, a preferred answer, and a rejected answer. Multi-turn conversations are uncommon in preference alignment. At the time of writing, major fine-tuning libraries do not support multi-turn conversations and typically extract only the first or last message in a conversation.",
      "content_length": 1839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 781,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 782,
      "content": "Data quantity\n\nDPO datasets typically require fewer samples than instruction datasets to significantly impact model behavior. As with instruction datasets, the required sample count depends on model size and task complexity. Larger models are more sample-efficient and thus require less data, while complex tasks demand more examples to capture the desired behavior. Once again, data quality is crucial, and a large number of preference pairs is generally beneficial.\n\nGeneral-purpose alignment is used by LLM providers to improve the overall performance of the fine-tuned models. This requires preference datasets with millions of samples. Major players in the AI industry, including Nvidia and Meta, are converging on similar post-training pipelines, involving multiple rounds of preference alignment, and extensive use of synthetic data. This consensus suggests that these methods are proving to be the most effective for pushing the boundaries of language model capabilities.\n\nOn a smaller scale, the open-source community uses datasets ranging from 10,000 to 100,000 samples to enhance model performance. This approach has proven effective not only in improving benchmark scores but also in healing networks after merging, pruning, and other modifications. Generally, DPO is less destructive than SFT and has a milder impact on the final model.",
      "content_length": 1349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 783,
      "content": "On the other hand, tasks like the ones previously described require fewer preference pairs. Task-specific alignment focuses on improving model performance for a particular function, such as modifying the writing style, refusing certain instructions, and so on. These alignments can often be achieved with smaller datasets, ranging from 100 to 10,000 preference pairs, depending on the task’s complexity.\n\nAn example of an application that requires few samples is instructing the model to state that it wasn’t trained by OpenAI, Meta, or another LLM provider. This can be achieved using a preference dataset, where the rejected answers are those claiming alternative origins, and the chosen answers are responses where the model correctly states that it was trained by you. A relatively small dataset of 200 to 500 pairs can be enough for this task.",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 784,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 785,
      "content": "Data generation and evaluation\n\nWhen creating preference datasets, data generation and evaluation are closely linked. We first create answers and then rate them to make the final dataset. In the following, we introduce both steps as one process instead of two separate ones.",
      "content_length": 274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 786,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 787,
      "content": "Generating preferences\n\nBefore making new preference data, it’s good to look at relevant open- source datasets. There are fewer of these compared to instruction datasets, but you can find high-quality preference datasets on the Hugging Face Hub. These can be used for specific tasks or to add to your own dataset. Well- known preference datasets include the Anthropic HH-RLHF dataset, which has human preferences for helpful and harmless AI responses, and the OpenAI Summarize from Human Feedback dataset, which focuses on article summaries.\n\nDPO datasets can be created using various methods, each with its own trade-offs between quality, cost, and scalability. These methods can be tailored to specific applications and require varying degrees of human feedback. We divide them into four main categories:\n\nHuman-generated, human-evaluated datasets: This method involves hiring people to both create responses to prompts and evaluate the quality of these responses. While this approach can capture nuanced human preferences and is ideal for complex tasks, it’s extremely resource-intensive and difficult to scale. As a result, it’s primarily used by large AI companies with substantial resources.\n\nHuman-generated, LLM-evaluated datasets: This method can be useful if you have a lot of existing human-generated content. However, it’s rarely used in practice due to inefficiency, as it still requires significant",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 788,
      "content": "human input for response generation while potentially missing nuanced preferences during the LLM evaluation stage.\n\nLLM-generated, human-evaluated datasets: This method offers a good balance between quality and efficiency. LLMs generate multiple responses to prompts, and humans rank these responses. This approach is often preferred because humans are generally better at judging answers than writing them from scratch. It allows the rapid generation of diverse responses while still capturing human preferences effectively. However, it may not provide creative or unexpected responses that humans might generate.\n\nLLM-generated, LLM-evaluated datasets: Fully synthetic datasets, where both generation and evaluation are done by LLMs, are becoming increasingly common due to their scalability and cost-effectiveness. This method can produce massive datasets quickly and improves as LLM capabilities advance. However, it requires careful prompt engineering to ensure quality and diversity, and may perpetuate biases or limitations of the generating LLM.\n\nIn practice, human-generated datasets are expensive, difficult to scale, and not necessarily of the highest quality. On the other hand, human evaluation is quite valuable but can be difficult to scale, which is why large datasets benefit from LLM evaluation. In addition to these high-level considerations, the way you obtain your data and how you plan to use it also need to be considered. For example, applications with many users can embed a feedback mechanism to provide preferences. This can be as simple as a\n\nlike\n\nand",
      "content_length": 1580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 789,
      "content": "dislike\n\nscore, or something more in-depth with text.\n\nNote that evaluation is not always required and preferences can emerge naturally from the generation process. For instance, it is possible to use a high-quality model to generate preferred outputs and a lower-quality or intentionally flawed model to produce less preferred alternatives. This creates a clear distinction in the preference dataset, allowing more effective training of AI systems to recognize and emulate high-quality outputs. The\n\nIntel/orca_dpo_pairs\n\ndataset available on the Hugging Face Hub was created with this process.\n\nAnother approach is to compare model-generated outputs with human- written responses, which can provide insights into how well the model aligns with actual human preferences and highlight areas where the model may be lacking. This can be used to copy a particular style and give a more authentic tone to the model.",
      "content_length": 911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 790,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 791,
      "content": "Tips for data generation\n\nThe data generation is consistent between instruction and preference datasets. Prompts should be designed to encourage diversity and complexity in the model’s responses. By crafting prompts that explicitly request different approaches or styles, we can ensure a wide range of outputs that capture the varied nature of human preferences.\n\nFor instance, when generating summaries, one might request variations such as concise summaries, detailed summaries, and summaries focusing on key points. This approach not only produces a diverse dataset but also helps in understanding how different styles and approaches align with human preferences.\n\nIntroducing variability in the outputs is another crucial aspect of generating synthetic preference datasets. This can be achieved by manipulating the temperature settings or employing other sampling methods in the LLM. Higher temperature settings tend to produce more creative and diverse responses, while lower settings result in more focused and deterministic outputs. This creates a trade-off between diversity and coherence, which depends on the kind of data we want to generate. For example, generating code requires low creativity, thus low temperature, while writing articles can be high temperature.\n\nUsing multiple LLMs to generate samples can be better than using just one model. Some LLMs are better at specific tasks, and this approach also adds more variety. This approach is used by popular open-source datasets like",
      "content_length": 1499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 792,
      "content": "argilla/Capybara-Preferences\n\n, combining GPT-4 with open-weight models. The evaluation process then selects the chosen and the rejected answers.",
      "content_length": 145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 793,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 794,
      "content": "Evaluating preferences\n\nData evaluation can be performed by human raters or automated with LLMs. LLM evaluation involves developing detailed criteria, creating a prompt that clearly communicates these guidelines to the LLM, and using the model to select preferred and rejected responses. While more scalable than human rating and allowing the consistent application of criteria, this quality of LLM evaluation depends directly on the model’s performance and the provided guidelines. It may miss subtle human preferences or cultural nuances. However, as LLMs continue to improve, their ability to make nuanced judgments improves as well, potentially leading to higher-quality datasets over time.\n\nImplementing LLM evaluation for preference datasets can be done through absolute scoring or pairwise ranking. In absolute scoring, the LLM assigns a numerical score or categorical rating to each response based on predefined criteria. This method is straightforward but may suffer from inconsistency across different prompts or evaluation sessions. Pairwise ranking, on the other hand, involves presenting the LLM with two responses and asking it to choose the better one or rank them. This approach more closely mimics the format of human evaluation and can lead to more consistent results.\n\nFor absolute scoring, you would create a prompt that outlines the evaluation criteria and asks the LLM to rate the response on a specific scale (e.g., 1-5 or poor/fair/good/excellent). The prompt might look like this: “Rate the following response on a scale of 1-5 based on relevance, coherence, and helpfulness: [\n\nINSERT RESPONSE\n\n].” For pairwise ranking, the prompt could be: “Compare the following two responses. Which one is better in terms of relevance, coherence, and helpfulness? Response A: [\n\nINSERT RESPONSE A\n\n] Response B: [\n\nINSERT RESPONSE B\n\n].”\n\nThe comparative nature of preference datasets makes pairwise ranking an ideal approach for evaluation. This method is generally more accurate and more closely correlated to human judgment than absolute scoring. Pairwise ranking mimics the natural way humans compare options, making it easier for both human raters and LLMs to provide consistent and meaningful evaluations.\n\nWe can further improve the accuracy of pairwise ranking by providing a ground-truth answer and using chain-of- thought reasoning. This approach encourages the evaluating LLM to consider multiple aspects of the responses and articulate its decision-making process, leading to more thorough and justified evaluations. When no ground- truth answer is available, we can prompt the LLM to create a grading note, which is a description of the expected answer. This technique works particularly well in scenarios where the LLM doesn’t have extensive knowledge about a given topic, as it forces the model to establish clear criteria for evaluation before assessing the responses.",
      "content_length": 2897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 795,
      "content": "Here’s a concrete implementation of an LLM-as-a-judge prompt to perform pairwise ranking:\n\nInstruction You are an answer judge. Your goal is to compare answer A and answer B. I want to know which answe\n\nTable 6.2 – Example of LLM-as-a-judge prompt for pairwise ranking with one instruction and two answers\n\nHowever, it’s important to note that LLM-based evaluation can be subject to several types of bias:\n\nPosition bias: In relative scoring, LLM judges tend to favor the first answer presented. This bias can skew results and lead to inaccurate preferences.\n\nLength bias: Similar to humans, LLM judges often show a preference for longer answers, potentially overlooking the quality of shorter, more concise responses.\n\nFamily bias: LLM judges may favor responses that are generated by themselves or models from the same family, potentially due to similarities in language patterns or knowledge bases.\n\nTo mitigate these biases and enhance the quality of preference datasets, several solutions can be implemented. One key approach is to randomize the order of answer A and answer B in each comparison, which can counteract position bias by ensuring that the order of presentation doesn’t consistently influence the evaluation. Another valuable strategy involves providing few-shot examples that demonstrate a balanced distribution of scores. These examples serve to calibrate the judge LLM’s internal scoring mechanism and can effectively address both length and family bias by illustrating that shorter answers or those from different model families can also be of high quality. Additionally, employing multiple models as a jury, rather than relying on a single LLM judge, can significantly improve the robustness of the evaluation process. This multi-model approach helps to balance out individual biases that may be present in any single model, leading to a more comprehensive and accurate assessment of the responses.\n\nIn the next section, we will create our own preference dataset. We will rely on the data generation process to naturally create chosen (human-generated) and rejected (LLM-generated) answers.",
      "content_length": 2113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 796,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 797,
      "content": "Creating our own preference dataset\n\nOur model can currently write paragraphs about topics related to machine learning, but it doesn’t have the same writing style as the original authors. This is a typical use case for preference alignment, where we want to change the “voice” of the model to closely imitate the source data. It’s important to note that, experimentally, DPO tends to make models more verbose and pushes them to use very formal language. Therefore, the training will need to use DPO surgically to avoid this pitfall and instead adopt the less formal style of these blog articles.\n\nIn this section, we will create a preference dataset where the chosen answers are extracts from the text, while rejected answers are generated by the model. To implement it, we will modify the code created in Chapter 5, which was designed to generate instruction datasets.\n\nAs seen in the previous section, preference and instruction datasets rely on the same principles. Instead of pairs of instructions and answers, we need triples (instruction, answer 1, answer 2). What’s interesting in this setting is that we have ground-truth answers in the text chunks, which means we don’t need complex evaluation processes like LLM judges. To make sure that these extracts are high-quality, we will implement two additional quality filters, based on length and punctuation. Figure 6.2 summarizes the end-to- end process:",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 798,
      "content": "Figure 6.2 – Synthetic data generation pipeline from raw text to preference dataset\n\nWe are now ready to implement the preference data generation pipeline:\n\nWe start by importing the necessary libraries.",
      "content_length": 203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 799,
      "content": "import\n\nconcurrent.futures\n\nimport\n\njson\n\nimport\n\nre\n\nfrom\n\ntyping\n\nimport\n\nList\n\n,\n\nTuple\n\nfrom\n\ndatasets\n\nimport\n\nDataset\n\nfrom\n\nopenai\n\nimport\n\nOpenAI",
      "content_length": 153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 800,
      "content": "from\n\ntqdm.auto\n\nimport\n\ntqdm\n\nInstead of the\n\nInstructionAnswerSet\n\nclass, we now have a\n\nPreferenceSet\n\nclass. This class is designed to handle triples of instructions, generated answers (rejected), and extracted answers (chosen).\n\nclass\n\nPreferenceSet\n\n:\n\ndef\n\n__init__\n\n(",
      "content_length": 275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 801,
      "content": "self, triples:\n\nList\n\n[\n\nTuple\n\n[\n\nstr\n\n,\n\nstr\n\n,\n\nstr\n\n]]\n\n): self.triples = triples\n\n@classmethod\n\ndef\n\nfrom_json\n\n(\n\ncls, json_str:\n\nstr\n\n) ->\n\n'PreferenceSet'",
      "content_length": 162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 802,
      "content": ": data = json.loads(json_str) triples = [(triple[\n\n'instruction'\n\n], triple[\n\n'generated_answer'\n\n], triple[\n\n'extracted_answer'\n\n])\n\nfor\n\ntriple\n\nin\n\ndata[\n\n'preference_triples'\n\n]]\n\nreturn\n\ncls(triples)\n\ndef\n\n__iter__\n\n(\n\nself\n\n):",
      "content_length": 232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 803,
      "content": "return\n\niter\n\n(self.triples)\n\nThe\n\nload_articles_from_json\n\n,\n\nclean_text\n\n, and\n\nextract_substrings\n\nfunctions remain unchanged from the original code. Let’s start with\n\nload_articles_from_json\n\n, which takes our JSON file (\n\ncleaned_documents.json\n\n) containing the articles as input and returns a Hugging Face dataset with the text and metadata (ID, platform, author ID, author full name, link).\n\ndef",
      "content_length": 403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 804,
      "content": "load_articles_from_json\n\n(\n\nfile_path:\n\nstr\n\n) -> Dataset:\n\nwith\n\nopen\n\n(file_path,\n\n\"r\"\n\n)\n\nas\n\nfile: data = json.load(file)\n\nreturn\n\nDataset.from_dict( {\n\n\"id\"\n\n: [item[\n\n\"id\"\n\n]\n\nfor\n\nitem",
      "content_length": 191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 805,
      "content": "in\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"content\"\n\n: [item[\n\n\"content\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"platform\"\n\n: [item[\n\n\"platform\"\n\n]\n\nfor\n\nitem",
      "content_length": 158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 806,
      "content": "in\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"author_id\"\n\n: [item[\n\n\"author_id\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"author_full_name\"\n\n: [item[\n\n\"author_full_name\"\n\n]\n\nfor\n\nitem",
      "content_length": 178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 807,
      "content": "in\n\ndata[\n\n\"artifact_data\"\n\n]],\n\n\"link\"\n\n: [item[\n\n\"link\"\n\n]\n\nfor\n\nitem\n\nin\n\ndata[\n\n\"artifact_data\"\n\n]], } )\n\nThe\n\nclean_text\n\nfunction removes non-alphanumeric characters except for apostrophes, periods, commas, exclamation marks, and question marks. It also replaces multiple whitespaces with a single space to ensure proper formatting.",
      "content_length": 338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 808,
      "content": "def\n\nclean_text\n\n(\n\ntext:\n\nstr\n\n) ->\n\nstr\n\n: text = re.sub(\n\nr\"[^\\w\\s.,!?']\"\n\n,\n\n\" \"\n\n, text) text = re.sub(\n\nr\"\\s+\"\n\n,\n\n\" \"\n\n, text)\n\nreturn\n\ntext.strip()",
      "content_length": 155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 809,
      "content": "The\n\nextract_substrings\n\nfunction splits articles into chunks with a length between 1,000 and 2,000 characters. To make sure that the splitting doesn’t break sentences, which could modify their meanings, we use a regex to only split after the end of a sentence.\n\ndef\n\nextract_substrings\n\n(\n\ndataset: Dataset, min_length:\n\nint\n\n=\n\n1000\n\n, max_length:\n\nint\n\n=\n\n2000",
      "content_length": 363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 810,
      "content": ") ->\n\nList\n\n[\n\nstr\n\n]: extracts = [] sentence_pattern =\n\nr\"(?\n\nfor\n\narticle\n\nin\n\ndataset[\n\n\"content\"\n\n]: cleaned_article = clean_text(article) sentences = re.split(sentence_pattern, cleaned_article) current_chunk =\n\n\"\"\n\nfor\n\nsentence\n\nin\n\nsentences: sentence = sentence.strip()\n\nif\n\nnot\n\nsentence:",
      "content_length": 297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 811,
      "content": "continue\n\nif\n\nlen\n\n(current_chunk) +\n\nlen\n\n(sentence) <= max_length: current_chunk += sentence +\n\n\" \"\n\nelse\n\n:\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip()) current_chunk = sentence +\n\n\" \"\n\nif\n\nlen\n\n(current_chunk) >= min_length: extracts.append(current_chunk.strip())\n\nreturn\n\nextracts",
      "content_length": 320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 812,
      "content": "The\n\ngenerate_preference_triples\n\nfunction replaces the original\n\ngenerate_instruction_answer_pairs\n\nfunction. The prompt is adapted from the instruction version and is designed to generate triples instead of pairs. It also provides general guidance about the type of instructions we’re interested in, how to extract answers from articles, and how to style them:\n\ndef\n\ngenerate_preference_triples\n\n(\n\nextract:\n\nstr\n\n, client: OpenAI\n\n) ->\n\nList\n\n[\n\nTuple",
      "content_length": 454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 813,
      "content": "[\n\nstr\n\n,\n\nstr\n\n,\n\nstr\n\n]]: prompt =\n\nf\"\"\"Based on the following extract, generate five instruction-answer triples. Each triple should consist of:\n\n1. An instruction asking about a specific topic in the context.\n\n2. A generated answer that attempts to answer the instruction based on the context.\n\n3. An extracted answer that is a relevant excerpt directly from the given context.\n\nInstructions must be self-contained and general, without explicitly mentioning a context, system, course, or extract.\n\nImportant:\n\nEnsure that the extracted answer is a verbatim copy from the context, including all punctuation and apostrophes.\n\nDo not add any ellipsis (...) or [...] to indicate skipped text in the extracted answer.\n\nIf the relevant text is not continuous, use two separate sentences from the context instead of skipping text.",
      "content_length": 826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 814,
      "content": "Provide your response in JSON format with the following structure:\n\n{{\n\n\"preference_triples\": [\n\n{{\n\n\"instruction\": \"...\",\n\n\"generated_answer\": \"...\",\n\n\"extracted_answer\": \"...\"\n\n}},\n\n...\n\n]\n\n}}\n\nExtract:\n\n{extract}\n\n\"\"\"\n\nIn the same function, we use GPT-4o-mini to generate our answers using JSON mode. We specify in the system prompt that we want triples instead of pairs. The JSON answers are directly parsed by our\n\nPreferenceSet\n\nclass to return the expected list of tuples.",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 815,
      "content": "completion = client.chat.completions.create( model=\n\n\"gpt-4o-mini\"\n\n, messages=[ {\n\n\"role\"\n\n:\n\n\"system\"\n\n,\n\n\"content\"\n\n:\n\n\"You are a helpful assistant who generates instruction-answer triples based on the given context. Each triple should include an instruction, a generated answer, and an extracted answer from the context. Provide your response in JSON format.\"\n\n, }, {\n\n\"role\"\n\n:\n\n\"user\"\n\n,\n\n\"content\"",
      "content_length": 404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 816,
      "content": ": prompt}, ], response_format={\n\n\"type\"\n\n:\n\n\"json_object\"\n\n}, max_tokens=\n\n2000\n\n, temperature=\n\n0.7\n\n, ) result = PreferenceSet.from_json(completion.choices[\n\n0\n\n].message.content)\n\nreturn\n\nresult.triples\n\nTwo new filtering functions are introduced for the preference data pipeline:\n\nfilter_short_answers\n\nand\n\nfilter_answer_format",
      "content_length": 332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 817,
      "content": ". These functions filter out short answers and ensure that answers start with an uppercase letter and end with proper punctuation. We use them as heuristics to filter out samples with poor quality.\n\ndef\n\nfilter_short_answers\n\n(\n\ndataset: Dataset, min_length:\n\nint\n\n=\n\n100\n\n) -> Dataset:\n\ndef\n\nis_long_enough\n\n(\n\nexample\n\n):\n\nreturn\n\nlen\n\n(example[",
      "content_length": 347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 818,
      "content": "'chosen'\n\n]) >= min_length\n\nreturn\n\ndataset.\n\nfilter\n\n(is_long_enough)\n\ndef\n\nfilter_answer_format\n\n(\n\ndataset: Dataset\n\n) -> Dataset:\n\ndef\n\nis_valid_format\n\n(\n\nexample\n\n): chosen = example[\n\n'chosen'\n\n]\n\nreturn\n\n(",
      "content_length": 213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 819,
      "content": "len\n\n(chosen) >\n\n0\n\nand\n\nchosen[\n\n0\n\n].isupper()\n\nand\n\nchosen[-\n\n1\n\n]\n\nin\n\n(\n\n'.'\n\n,\n\n'!'\n\n,\n\n'?'\n\n))\n\nreturn",
      "content_length": 109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 820,
      "content": "dataset.\n\nfilter\n\n(is_valid_format)\n\nThe\n\ncreate_preference_dataset\n\nfunction replaces the original\n\ncreate_instruction_dataset\n\nfunction. This function now works with triples instead of pairs and uses different column names in the resulting dataset.\n\ndef\n\ncreate_preference_dataset\n\n(\n\ndataset: Dataset, client: OpenAI, num_workers:\n\nint\n\n=\n\n4",
      "content_length": 344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 821,
      "content": ") -> Dataset: extracts = extract_substrings(dataset) preference_triples = []\n\nwith\n\nconcurrent.futures.ThreadPoolExecutor(max_workers=num_workers)\n\nas\n\nexecutor: futures = [ executor.submit(generate_preference_triples, extract, client)\n\nfor\n\nextract\n\nin\n\nextracts ]\n\nfor\n\nfuture\n\nin\n\ntqdm(concurrent.futures.as_completed(futures), total=\n\nlen\n\n(futures)): preference_triples.extend(future.result()) instructions, generated_answers, extracted_answers =\n\nzip\n\n(*preference_triples)\n\nreturn\n\nDataset.from_dict( {",
      "content_length": 509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 822,
      "content": "\"prompt\"\n\n:\n\nlist\n\n(instructions),\n\n\"rejected\"\n\n:\n\nlist\n\n(generated_answers),\n\n\"chosen\"\n\n:\n\nlist\n\n(extracted_answers) } )\n\nThe main function is updated to include the new filtering steps and to use the preference dataset creation function:\n\ndef\n\nmain",
      "content_length": 250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 823,
      "content": "(\n\ndataset_id:\n\nstr\n\n) -> Dataset: client = OpenAI()\n\n# 1. Load the raw data\n\nraw_dataset = load_articles_from_json(\n\n\"cleaned_documents.json\"\n\n)\n\nprint\n\n(\n\n\"Raw dataset:\"\n\n)\n\nprint\n\n(raw_dataset.to_pandas())\n\n# 2. Create preference dataset\n\ndataset = create_preference_dataset(raw_dataset, client)\n\nprint\n\n(\n\n\"Preference dataset:\"\n\n)",
      "content_length": 334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 824,
      "content": "print\n\n(dataset.to_pandas())\n\n# 3. Filter out samples with short answers\n\ndataset = filter_short_answers(dataset)\n\n# 4. Filter answers based on format\n\ndataset = filter_answer_format(dataset)\n\n# 5. Export\n\ndataset.push_to_hub(dataset_id)\n\nreturn\n\ndataset\n\nThe\n\ncreate_preference_dataset()\n\nfunction generated 2,970 samples. This dataset is then heavily filtered to only retain 1,467 samples by removing answers that are too short or not properly formatted (for example, answers that start with an uppercase letter or end with a period, exclamation mark, or question mark).\n\nThe final dataset is available on the Hugging Face Hub at the following address: https://huggingface.co/datasets/mlabonne/llmtwin-dpo. You can see in Figure 6.3 an example that captures a subtle nuance in terms of",
      "content_length": 787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 825,
      "content": "writing style. Both answers are correct, but the chosen (extracted) answer sounds slightly more casual.\n\nFigure 6.3 – Screenshot of the mlabonne/llmtwin-dpo preference dataset on the Hugging Face Hub\n\nTo produce this dataset, we iterated many times over the prompt to generate the data. This required some manual evaluation and experiments until we reached satisfying results. The quality of the prompt is fundamental in this process, which is why it is recommended to follow a similar process to generate your own preference datasets.\n\nIn the next section, we will introduce concepts related to Reinforcement Learning from Human Feedback (RLHF) and DPO. This will cover new parameters and ideas that are implemented in the final section of this chapter.",
      "content_length": 754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 826,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 827,
      "content": "Preference alignment\n\nPreference alignment regroups techniques to fine-tune models on preference data. In this section, we provide an overview of this field and then focus on the technique we will implement: Direct Preference Optimization (DPO).",
      "content_length": 245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 828,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 829,
      "content": "Reinforcement Learning from Human Feedback\n\nReinforcement Learning from Human Feedback (RLHF) combines reinforcement learning (RL) with human input to align models with human preferences and values. RLHF emerged as a response to challenges in traditional RL methods, particularly the difficulty of specifying reward functions for complex tasks and the potential for misalignment between engineered rewards and intended objectives.\n\nThe origins of RLHF can be traced back to the field of preference- based reinforcement learning (PbRL), which was independently introduced by Akrour et al. and Cheng et al. in 2011. PbRL aimed to infer objectives from qualitative feedback, such as pairwise preferences between behaviors, rather than relying on quantitative reward signals. This approach addressed some of the limitations of conventional RL, where defining appropriate reward functions can be challenging and prone to reward hacking or unintended behaviors.\n\nThe term RLHF was coined later, around 2021-2022, as the approach gained prominence in the context of training LLMs. However, the core ideas had been developing for years prior. A seminal paper by Christiano et al. in 2017 demonstrated the effectiveness of learning reward models from human preferences and using them to train RL agents. This work showed that RLHF could match or exceed the performance of agents trained on hand-engineered rewards, but with significantly less human effort.",
      "content_length": 1447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 830,
      "content": "At its core, RLHF works by iteratively improving both a reward model and a policy:\n\nReward model learning: Instead of using a pre-defined reward function, RLHF learns a reward model from human feedback. This is typically done by presenting humans with different answers and asking them to indicate which one they prefer. These preferences are used to train a reward model, often using a Bradley-Terry model or similar approaches that map preferences to underlying utility functions.\n\nPolicy optimization: With the learned reward model, standard RL algorithms can be used to optimize a policy. This policy generates new behaviors that aim to maximize the predicted rewards from the learned model.\n\nIterative improvement: As the policy improves, it generates new behaviors that can be evaluated by humans, leading to refinements in the reward model. This cycle continues, ideally resulting in a policy that aligns well with human preferences.\n\nA key innovation in RLHF is its approach to handling the high cost of human feedback. Rather than requiring constant human oversight, RLHF allows for asynchronous and sparse feedback.\n\nThe learned reward model serves as a proxy for human preferences, enabling the RL algorithm to train continuously without direct human input for every action.",
      "content_length": 1285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 831,
      "content": "As an example, Figure 6.4 shows a high-level view of the Proximal Policy Optimization (PPO) algorithm, which is one of the most popular RLHF algorithms. Here, the reward model is used to score the text that is generated by the trained model. This reward is regularized by an additional Kullback–Leibler (KL) divergence factor, ensuring that the distribution of tokens stays similar to the model before training (frozen model).",
      "content_length": 426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 832,
      "content": "Figure 6.4 – High-level view of the PPO algorithm for preference alignment\n\nWhile RLHF has proven effective for aligning AI systems with human preferences, it faces challenges due to its iterative nature and reliance on a",
      "content_length": 221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 833,
      "content": "separate reward model, which can be computationally expensive and potentially unstable. Despite theoretical superiority, RLHF algorithms have also experimentally underperformed compared to simpler approaches. One such approach that has gained significant attention is DPO.",
      "content_length": 272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 834,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 835,
      "content": "Direct Preference Optimization\n\nIntroduced by Rafailov et al. in their 2023 paper Direct Preference Optimization: Your Language Model is Secretly a Reward Model, DPO offers a streamlined alternative to traditional RLHF methods.\n\nDPO’s core innovation lies in its reformulation of the preference learning problem. Unlike RLHF, which typically involves training a separate reward model and then using reinforcement learning algorithms like PPO to fine- tune the language model, DPO takes a more direct approach.\n\nIt derives a closed-form expression for the optimal policy under the standard RLHF objective of maximizing expected reward subject to a KL- divergence constraint with a reference policy. This mathematical insight allows DPO to express the preference learning problem directly in terms of the policy, eliminating the need for a separate reward model or complex reinforcement learning algorithms.\n\nIn practical terms, DPO can be implemented as a simple binary cross- entropy loss function that operates directly on the language model’s output probabilities. This loss function encourages the model to assign higher probability to preferred responses and lower probability to non-preferred responses, while maintaining closeness to a reference (frozen) model. The importance of the reference model is directly controlled via a beta parameter between 0 and 1. The reference model is ignored when beta is equal to 0, which means that the trained model can be very different from",
      "content_length": 1484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 836,
      "content": "the SFT one. In practice, a value of 0.1 is the most popular one, but this can be tweaked, as we’ll see in the next section.\n\nThe simplicity of this approach allows optimization using standard gradient descent techniques, without the need for sampling from the model during training or implementing complex RL algorithms. Figure 6.5 shows a high- level view of the DPO algorithm, greatly simplifying the training process compared to Figure 6.4.",
      "content_length": 444,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 837,
      "content": "Figure 6.5 – High-level view of the DPO algorithm for preference alignment\n\nDPO has several advantages over traditional RLHF methods. As previously mentioned, it significantly simplifies the preference learning pipeline, reducing the engineering complexity associated with RLHF methods. By eliminating the need for a separate reward model and RL algorithms, DPO is more computationally efficient than traditional RLHF approaches. Particularly when trained with adapters (LoRA, QLoRA), the frozen and trained models don’t have to be separated. Indeed, since we’re only training adapters, the trained model is not modified. This allows us to only load one model instead of two, which saves additional VRAM.\n\nDespite its simplicity, DPO often matches the performance of more complex RLHF methods. It also tends to be more stable during training and less sensitive to hyperparameters. The simplified approach makes DPO easier to implement and scale, particularly for small teams without extensive RL knowledge.\n\nWhile RLHF allows iterative improvement through multiple training rounds and can dynamically adapt to new preferences, DPO offers a more straightforward path to achieving similar results. The choice between DPO and PPO-based RLHF often comes down to a trade-off between ease of implementation and potential peak performance. For large-scale training runs with millions of preference samples, PPO-inspired methods still have a higher performance ceiling. However, for most applications, DPO provides the majority of the performance benefits at a lower computational and engineering cost.",
      "content_length": 1594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 838,
      "content": "Both RLHF and DPO benefit significantly from the integration of synthetic data. As LLMs become more capable, they can generate data that surpasses human-created content in quality and diversity. This enables a virtuous cycle where better models produce better training data, which in turn leads to further model improvements. The iterative nature of both approaches allows multiple rounds of model refinement, each focusing on different aspects of model performance and gradually enhancing capabilities across various domains.\n\nDespite its advantages, DPO is not without drawbacks. Like RLHF, DPO still requires paired preference data, which can be expensive and time- consuming to collect. DPO lacks some of the theoretical guarantees associated with reinforcement learning approaches. There may be scenarios where the added flexibility of RLHF is beneficial, particularly for complex tasks or environments.\n\nNonetheless, DPO is ideal in most cases, including our twin LLM example. In the next section, we will implement it using Unsloth.",
      "content_length": 1039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 839,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 840,
      "content": "Implementing DPO\n\nIn this section, we will DPO fine-tune the TwinLlama-3.1-8B model we created in Chapter 5. For ease of use and to maximize performance, we will again use the Unsloth library for our DPO implementation. Depending on the available VRAM, you can choose between LoRA (higher quality, speed, and VRAM usage) and QLoRA (lower quality, speed, and VRAM usage). This technique, along with other preference alignment algorithms, is also available in TRL and Axolotl.\n\nThis example can be seen as an advanced application of DPO. Indeed, our objective of imitating a writing style conflicts with the natural tendency of DPO to encourage formal language. This is partly due to the fact that chosen answers are often more formal than rejected ones. In practice, this will force us to do light fine-tuning, with a low learning rate and number of epochs. To find the best hyperparameters, we trained over 20 models and compared their outputs on a set of questions, including “Write a paragraph to introduce supervised fine-tuning.” This allowed us to select the model and parameters that worked best for this task.\n\nThe dependencies are the same as those in Chapter 5 with SFT and can be found in the book’s GitHub repository (https://github.com/PacktPublishing/LLM-Engineering) or in Unsloth’s repo (https://github.com/unslothai/unsloth):",
      "content_length": 1341,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 841,
      "content": "First, we want to access a gated model and (optionally) upload our fine- tuned model to Hugging Face (https://huggingface.co/). This requires us to log in to an account. If you don’t have an account, you can create one and store your API key (Settings | Access Tokens | Create new token) in the\n\n.env\n\nfile:\n\nHF_TOKEN = YOUR_API_KEY\n\nMake sure that your Comet ML API key is also in the\n\n.env\n\nfile. Otherwise, the code will crash and raise an error when training starts.\n\nCOMET_API_KEY = YOUR_API_KEY\n\nBefore we import all the necessary packages, we want to apply a patch for the",
      "content_length": 579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 842,
      "content": "DPOTrainer\n\nclass from TRL. This fixes the DPO logs in notebook environments.\n\nfrom\n\nunsloth\n\nimport\n\nPatchDPOTrainer PatchDPOTrainer()\n\nWe can now import the other libraries. The main difference between DPO and SFT is the import of\n\nDPOConfig\n\nand\n\nDPOTrainer\n\nfrom TRL, which are specific to DPO training.\n\nimport\n\nos",
      "content_length": 319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 843,
      "content": "import\n\ntorch\n\nfrom\n\ndatasets\n\nimport\n\nload_dataset\n\nfrom\n\ntransformers\n\nimport\n\nTrainingArguments, TextStreamer\n\nfrom\n\nunsloth\n\nimport\n\nFastLanguageModel, is_bfloat16_supportedfrom trl\n\nimport\n\nDPOConfig, DPOTrainer\n\nThis step loads our fine-tuned model from Chapter 5. We use the same configuration with a",
      "content_length": 307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 844,
      "content": "max_seq_length\n\nof 2048. You can activate QLoRA by setting\n\nload_in_4bit\n\nto\n\nTrue\n\n. In the following, we will perform LoRA DPO fine-tuning for increased speed and quality.\n\nmax_seq_length =\n\n2048\n\nmodel, tokenizer = FastLanguageModel.from_pretrained( model_name=\n\n\"mlabonne/TwinLlama-3.1-8B\"\n\n, max_seq_length=max_seq_length, load_in_4bit=\n\nFalse\n\n, )\n\nLet’s now prepare the model for PEFT with the LoRA configuration. We increase the rank (\n\nr",
      "content_length": 446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 845,
      "content": ") and\n\nlora_alpha\n\nfrom\n\n32\n\n(as it was in Chapter 5) to\n\n64\n\n. This will allow more expressive fine-tuning. We keep a dropout of\n\n0\n\nfor speed and we target every linear module as per usual.\n\nmodel = FastLanguageModel.get_peft_model( model, r=32, lora_alpha=32, lora_dropout=0, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], )\n\nWe load the\n\nllmtwin-dpo\n\ndataset (training split), which contains our prompts, chosen, and rejected answers.",
      "content_length": 485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 846,
      "content": "dataset = load_dataset(\n\n\"mlabonne/llmtwin-dpo\"\n\n, split=\n\n\"train\"\n\n)\n\nThe data preparation is significantly different from the SFT example in Chapter 5. Here, we have triples with a prompt, a chosen answer, and a rejected answer. In the\n\nformat_samples\n\nfunction, we apply the Alpaca chat template to each individual message. Note that the instruction is the only one that requires the chat format: chosen and rejected answers only need to be concatenated with the end of sentence (EOS) token. Finally, we create a train/test split with a 95%/5% ratio.\n\nalpaca_template =\n\n\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:",
      "content_length": 700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 847,
      "content": "{}\n\n### Response:\n\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef\n\nformat_samples\n\n(\n\nexample\n\n): example[\n\n\"prompt\"\n\n] = alpaca_template.\n\nformat\n\n(example[\n\n\"prompt\"\n\n]) example[\n\n\"chosen\"\n\n] = example[\n\n'chosen'\n\n] + EOS_TOKEN example[\n\n\"rejected\"",
      "content_length": 245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 848,
      "content": "] = example[\n\n'rejected'\n\n] + EOS_TOKEN\n\nreturn\n\n{\n\n\"prompt\"\n\n: example[\n\n\"prompt\"\n\n],\n\n\"chosen\"\n\n: example[\n\n\"chosen\"\n\n],\n\n\"rejected\"\n\n: example[\n\n\"rejected\"\n\n]} dataset = dataset.\n\nmap\n\n(format_samples) dataset = dataset.train_test_split(test_size=\n\n0.05",
      "content_length": 256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 849,
      "content": ")\n\nThe model and data are now ready, so we can start fine-tuning. Compared to SFT, there are a few new parameters, like\n\nref_model\n\nand\n\nbeta\n\n. Since we’re using LoRA (or QLoRA), we don’t directly train the model but instead the adapters. This means we can use the original model (without adapters) as a reference, saving a lot of VRAM. The\n\nbeta\n\nparameter controls the importance of the reference model. A standard value of 0.1 works well in most scenarios, but we decided to increase it to 0.5 based on our experiments. This is due to the fact that the trained model used formal language with lower values. Having it closer to the reference model helps to fix this issue.\n\nThe learning rate is also lower (from 3e-4 for SFT to 2e-6 here). We train for 1 epoch instead of 3, and the\n\nmax_seq_length\n\nparameter is now broken down into two new parameters:\n\nmax_prompt_length",
      "content_length": 875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 850,
      "content": "(prompt only) and\n\nmax_length\n\n(prompt and answer). Note that we also replaced the\n\nTrainingArguments\n\nclass with\n\nDPOConfig\n\n.\n\ntrainer = DPOTrainer( model=model, ref_model=\n\nNone\n\n, tokenizer=tokenizer, beta=\n\n0.5\n\n, train_dataset=dataset[\n\n\"train\"\n\n], eval_dataset=dataset[\n\n\"test\"\n\n], max_length=max_seq_length//\n\n2\n\n, max_prompt_length=max_seq_length//",
      "content_length": 357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 851,
      "content": "2\n\n, args=DPOConfig( learning_rate=\n\n2e-6\n\n, lr_scheduler_type=\n\n\"linear\"\n\n, per_device_train_batch_size=\n\n2\n\n, per_device_eval_batch_size=\n\n2\n\n, gradient_accumulation_steps=\n\n8\n\n, num_train_epochs=\n\n1\n\n, fp16=\n\nnot\n\nis_bfloat16_supported(), bf16=is_bfloat16_supported(), optim=\n\n\"adamw_8bit\"\n\n, weight_decay=\n\n0.01\n\n, warmup_steps=",
      "content_length": 332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 852,
      "content": "10\n\n, output_dir=\n\n\"output\"\n\n, eval_strategy=\n\n\"steps\"\n\n, eval_steps=\n\n0.2\n\n, logging_steps=\n\n1\n\n, report_to=\n\n\"\n\ncomet_ml\"\n\n, seed=\n\n0\n\n, ), ) trainer.train()\n\nOnce the model is trained, we can run it for a quick sanity check. This step is similar to the SFT example. It prepares the model for inference and generates a response to a prompt.",
      "content_length": 342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 853,
      "content": "FastLanguageModel.for_inference(model) message = alpaca_template.\n\nformat\n\n(\n\n\"Write a paragraph to introduce supervised fine-tuning.\"\n\n,\n\n\"\"\n\n) inputs = tokenizer([message], return_tensors=\n\n\"pt\"\n\n).to(\n\n\"cuda\"\n\n) text_streamer = TextStreamer(tokenizer) _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=\n\n256\n\n, use_cache=\n\nTrue\n\n)",
      "content_length": 351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 854,
      "content": "The trained DPO model returns the following response:\n\nSupervised fine-tuning is a method used to enhance the performance of pre- trained language models by utilizing labeled data. This technique involves taking a pre-trained model and refining it on a specific task, such as content creation or customer service. By providing the model with relevant data and guidance, it can learn to generate outputs that align more closely with the desired outcomes. This approach allows for the creation of more specialized models that can tackle complex tasks with greater accuracy and efficiency.\n\nWe can compare it with the answer provided by the SFT model:\n\nSupervised fine-tuning is a method used to enhance a language model by providing it with a curated dataset of instructions and their corresponding answers. This process is designed to align the model's responses with human expectations, thereby improving its accuracy and relevance. The goal is to ensure that the model can respond effectively to a wide range of queries, making it a valuable tool for applications such as chatbots and virtual assistants.",
      "content_length": 1105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 855,
      "content": "The DPO model provides an answer that is both more accurate and closer to the desired writing style. It correctly identifies pre-training language models as source models for SFT. It also mentions domain or task-specific finetunes instead of alignment with “human expectations,” which is closer to the preference alignment stage. The answer is also less formal and something we would use in a blog post.\n\nFinally, the last step consists of saving the trained model locally and pushing it to the Hugging Face Hub.\n\nmodel.save_pretrained_merged(\n\n\"model\"\n\n, tokenizer, save_method=\n\n\"merged_16bit\"\n\n)\n\nCongratulations! We have trained and exported our DPO model. It is now available on the Hugging Face Hub at https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO. Compared to SFT, DPO has a few additional metrics that need to be tracked during training. Figure 6.6 shows the Comet ML dashboard with the main metrics.",
      "content_length": 915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 856,
      "content": "You can publicly access it using the following URL: https://www.comet.com/mlabonne/llm-twin-training/",
      "content_length": 101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 857,
      "content": "Figure 6.6 – Experiment tracking in Comet ML with DPO metrics\n\nLet’s review these metrics:\n\nTraining loss: We still want the loss to continuously decrease on average. Note that it can rapidly fall to zero, meaning that the model is no longer learning anything. This behavior doesn’t necessarily lead to overfitting or bad models but needs to be monitored closely.",
      "content_length": 363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 858,
      "content": "Validation loss: The same thing can be said about the validation loss. We expect a small gap compared to the training loss.\n\nGradient norm: We expect small gradient norms with few spikes.\n\nRewards: We have two different rewards: chosen and rejected. They correspond to the mean difference between the log probabilities output by the trained and reference models. Over time, we expect the model to choose the chosen answers and reject the rejected answers, which means that the gap between them should increase. This difference is directly tracked by the\n\nmargins\n\nmetric, defined as the difference between chosen and rejected rewards. A well-trained model’s margin will quickly increase and then plateau.\n\nAccuracies: This metric represents the percentage of times the model correctly identifies the chosen answers. We want this accuracy to gradually increase during training, but it doesn’t need to reach 100%. An accuracy of 100%, especially if it’s achieved quickly, indicates that the preference dataset might be too easy for the model. While the LLM can still learn from such a dataset, it might be beneficial to add more challenging examples.\n\nIn general, DPO is slightly harder to monitor and debug than SFT because it’s a more complex process, involving a reference model. However, it’s also significantly easier to use than PPO and other RLHF algorithms. As long as you have a high-quality preference dataset and a strong fine-tuned model, you can experiment with different ranks, beta parameters, learning",
      "content_length": 1515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 859,
      "content": "rates, and number of epochs to see which experiment best captures your preferences.\n\nWhile this is not the purpose of this chapter, it is possible to automate the evaluation of models designed to imitate a writing style. A possible solution consists of comparing the distribution of words in the text generated by different models (SFT and DPO) with our ground-truth dataset. In this example, we expect the SFT model to output a lot of words that are overrepresented in GPT-4o-mini (like “delve into”). The distribution output by our DPO model should be a lot closer to the chosen answers.",
      "content_length": 589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 860,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 861,
      "content": "Summary\n\nThis chapter explored preference alignment techniques for improving LLMs. It introduced the concept of preference datasets, explaining their structure and importance in capturing nuanced human preferences. We implemented our own custom preference data generation pipeline by comparing original and AI-generated text from real articles. This pipeline can be reused and customized based on your use case.\n\nWe also provided an overview of the evolution of RLHF, leading to the introduction of DPO as a simpler and more efficient alternative. Finally, we implemented DPO using the Unsloth library to fine-tune our TwinLlama- 3.1-8B model from Chapter 5. Our step-by-step tutorial gave practical instructions for training the model, as well as highlighting key differences from SFT. The final model is available on the Hugging Face Hub.\n\nIn the next chapter, we will explore the crucial topic of LLM evaluation, addressing the challenges and current approaches in assessing LLM performance. We’ll cover the creation of domain-specific evaluation sets, examine why evaluation remains a persistent problem in the field, and introduce the concept of using larger models to evaluate smaller ones (LLM-as-a-judge). The chapter will conclude with a comprehensive evaluation pipeline, providing a structured framework for consistent and effective LLM evaluation.",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 862,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 863,
      "content": "References\n\nRafael Rafailov et al.. “Direct Preference Optimization: Your Language Model is Secretly a Reward Model.” arXiv preprint arXiv:2305.18290, May 2023.\n\nTimo Kaufmann et al.. “A Survey of Reinforcement Learning from Human Feedback.” arXiv preprint arXiv:2312.14925, December 2023.\n\nAnthropic. “GitHub - anthropics/hh-rlhf: Human preference data for “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback”.” github.com, 2022, https://github.com/anthropics/hh-rlhf.\n\nNisan Stiennon et al.. “Learning to summarize from human feedback.” arXiv preprint arXiv:2009.01325, September 2020.\n\nIntel(R) Neural Compressor. “Supervised Fine-Tuning and Direct Preference Optimization on Intel Gaudi2.” medium.com, March 26, 2024, https://medium.com/intel-analytics-software/the-practice-of-supervised- finetuning-and-direct-preference-optimization-on-habana-gaudi2- a1197d8a3cd3.",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 864,
      "content": "Argilla. “GitHub - argilla-io/distilabel.” github.com, August 23, 2024, https://github.com/argilla-io/distilabel.\n\nDatabricks. “Enhancing LLM-as-a-Judge with Grading Notes.” databricks.com, July 22, 2024, https://www.databricks.com/blog/enhancing- llm-as-a-judge-with-grading-notes.\n\nAkrour, Riad & Schoenauer, Marc & Sebag, Michèle. (2011). Preference- Based Policy Learning. 12-27. 10.1007/978-3-642-23780-5_11.\n\nCheng, Weiwei & Fürnkranz, Johannes & Hüllermeier, Eyke & Park, Sang- Hyeun. (2011). Preference-Based Policy Iteration: Leveraging Preference Learning for Reinforcement Learning. 312-327. 10.1007/978-3-642-23780- 5_30.\n\nPaul Christiano et al.. “Deep reinforcement learning from human preferences.” arXiv preprint arXiv:1706.03741, June 2017.\n\nLong Ouyang et al.. “Training language models to follow instructions with human feedback.” arXiv preprint arXiv:2203.02155, March 2022.\n\nJohn Schulman et al.. “Proximal Policy Optimization Algorithms.” arXiv preprint arXiv:1707.06347, July 2017.",
      "content_length": 1003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 865,
      "content": "unslothai. “GitHub - unslothai/unsloth: Finetune Llama 3.1, Mistral, Phi & Gemma LLMs 2-5x faster with 80% less memory.” github.com, August 21, 2024, https://github.com/unslothai/unsloth.\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 333,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 866,
      "content": "7",
      "content_length": 1,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 867,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 868,
      "content": "Evaluating LLMs\n\nLLM evaluation is a crucial process used to assess the performance and capabilities of LLM models. It can take multiple forms, such as multiple- choice question answering, open-ended instructions, and feedback from real users. Currently, there is no unified approach to measuring a model’s performance but there are patterns and recipes that we can adapt to specific use cases.\n\nWhile general-purpose evaluations are the most popular ones, with benchmarks like Massive Multi-Task Language Understanding (MMLU) or LMSYS Chatbot Arena, domain- and task-specific models benefit from more narrow approaches. This is particularly true when dealing with entire LLM systems (as opposed to models), often centered around a retrieval-augmented generation (RAG) pipeline. In these scenarios, we need to expand our evaluation framework to encompass the entire system, including new modules like retrievers and post- processors.\n\nIn this chapter, we will cover the following topics:\n\nModel evaluation\n\nRAG evaluation",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 869,
      "content": "Evaluating TwinLlama-3.1-8B\n\nBy the end of this chapter, you will know the most popular LLM evaluations and how to evaluate models and RAG systems using different techniques.",
      "content_length": 174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 870,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 871,
      "content": "Model evaluation\n\nIn model evaluation, the objective is to assess the capabilities of a single model without any prompt engineering, RAG pipeline, and so on.\n\nThis evaluation is essential for several reasons, such as selecting the most relevant LLM or making sure that the fine-tuning process actually improved the model. In this section, we will compare ML and LLM evaluation to understand the main differences between these two fields. We will then explore benchmarks for general-purpose, domain-specific, and task-specific models.",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 872,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 873,
      "content": "Comparing ML and LLM evaluation\n\nML evaluation is centered on assessing the performance of models designed for tasks like prediction, classification, and regression. Unlike the evaluation of LLMs, which often focuses on how well a model understands and generates language, ML evaluation is more concerned with how accurately and efficiently a model can process structured data to produce specific outcomes.\n\nThis difference comes from the nature of the tasks these models handle. ML models are generally designed for narrowly defined problems, such as predicting stock prices or detecting outliers, which often involve numerical or categorical data, making the evaluation process more straightforward. On the other hand, LLMs are tasked with interpreting and generating language, which adds a layer of subjectivity to the evaluation process. Instead of relying solely on numerical benchmarks, LLM evaluation requires a more nuanced approach and often incorporates qualitative assessments, examining how well the model produces coherent, relevant, and contextually accurate responses in natural language.\n\nIn particular, we can see three key differences in how these models work, which impact the evaluation process:\n\nNumerical metrics: Evaluating ML models typically involves measuring objective performance metrics, such as accuracy, precision, recall, or mean squared error, depending on the type of task at hand. This is less clear with LLMs, which can handle multiple tasks (hence,",
      "content_length": 1485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 874,
      "content": "multiple evaluations) and can rarely rely on the same numerical metrics.\n\nFeature engineering: In traditional ML, a critical part of the process involves manually selecting and transforming relevant data features before training the model. Evaluating the success of this feature engineering often becomes part of the broader model evaluation. LLMs, however, are designed to handle raw text data directly, reducing the need for manual feature engineering.\n\nInterpretability: With ML models, it is easier to interpret why a model made certain predictions or classifications, and this interpretability can be a core part of their evaluation. This direct interpretation is not possible with LLMs. However, requesting explanations during the generation process can give insights into the model’s decision-making process.\n\nIn the following section, we will see a more fine-grained exploration of different types of LLMs. While evaluating general-purpose models is fairly disconnected from ML evaluation, task-specific LLMs are more closely aligned with traditional ML.",
      "content_length": 1062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 875,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 876,
      "content": "General-purpose LLM evaluations\n\nGeneral-purpose evaluations refer to metrics dedicated to base and general- purpose fine-tuned models. They cover a breadth of capabilities that are correlated with knowledge and usefulness without focusing on specific tasks or domains. This allows developers to get an overview of these capabilities, compare themselves with competitors, and identify strengths and weaknesses. Based on these results, it is possible to tweak the dataset and hyperparameters, or even modify the architecture.\n\nWe can broadly categorize general-purpose evaluations in three phases: during pre-training, after pre-training, and after fine-tuning.\n\nDuring pre-training, we closely monitor how the model learns, as shown at the end of Chapter 5. The most straightforward metrics are low-level and correspond to how models are trained:\n\nTraining loss: Based on the cross-entropy loss, measures the difference between the model’s predicted probability distribution and the true distribution of the next token\n\nValidation loss: Calculates the same loss as training loss, but on a held- out validation set to assess generalization",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 877,
      "content": "Perplexity: Exponential of the cross-entropy loss, representing how “surprised” the model is by the data (lower is better)\n\nGradient norm: Monitors the magnitude of gradients during training to detect potential instabilities or vanishing/exploding gradients\n\nIt’s also possible to include benchmarks like HellaSwag (common sense reasoning) during this stage but there’s a risk of overfitting these evaluations.\n\nAfter pre-training, it is common to use a suite of evaluations to evaluate the base model. This suite can include internal and public benchmarks. Here’s a non-exhaustive list of common public pre-training evaluations:\n\nMMLU (knowledge): Tests models on multiple-choice questions across 57 subjects, from elementary to professional levels\n\nHellaSwag (reasoning): Challenges models to complete a given situation with the most plausible ending from multiple choices\n\nARC-C (reasoning): Evaluates models on grade-school-level multiple- choice science questions requiring causal reasoning",
      "content_length": 995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 878,
      "content": "Winogrande (reasoning): Assesses common sense reasoning through pronoun resolution in carefully crafted sentences\n\nPIQA (reasoning): Measures physical common sense understanding through questions about everyday physical interactions\n\nMany of these datasets are also used to evaluate general-purpose fine-tuned models. In this case, we focus on the difference in a given score between the base and the fine-tuned model. For example, bad fine-tuning can degrade the knowledge of the model, measured by MMLU. On the contrary, a good one might instill even more knowledge and increase the MMLU score.\n\nThis can also help identify any contamination issues, where the model might have been fine-tuned on data that is too close to a test set. For instance, improving the MMLU score of a base model by 10 points during the fine-tuning phase is unlikely. This is a sign that the instruction data might be contaminated.\n\nIn addition to these pre-trained evaluations, fine-tuned models also have their own benchmarks. Here, we use the term “fine-tuned model” to designate a model that has been trained with supervised fine-tuning (SFT) and preference alignment. These benchmarks target capabilities connected to the ability of fine-tuned models to understand and answer questions. In particular, they test instruction-following, multi-turn conversation, and agentic skills:",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 879,
      "content": "IFEval (instruction following): Assesses a model’s ability to follow instructions with particular constraints, like not outputting any commas in your answer\n\nChatbot Arena (conversation): A framework where humans vote for the best answer to an instruction, comparing two models in head-to-head conversations\n\nAlpacaEval (instruction following): Automatic evaluation for fine- tuned models that is highly correlated with Chatbot Arena\n\nMT-Bench (conversation): Evaluates models on multi-turn conversations, testing their ability to maintain context and provide coherent responses\n\nGAIA (agentic): Tests a wide range of abilities like tool use and web browsing, in a multi-step fashion\n\nUnderstanding how these evaluations are designed and used is important to choose the best LLM for your application. For example, if you want to fine- tune a model, you want the best base model in terms of knowledge and reasoning for a given size. This allows you to compare the capabilities of different LLMs and pick the one that will offer the strongest foundation for your fine-tuning.",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 880,
      "content": "Even if you don’t want to fine-tune a model, benchmarks like Chatbot Arena or IFEval are a good way to compare different instruct models. For instance, you want great conversational abilities if you’re building a chatbot. However, this is not necessary if your end goal is something like information extraction from unstructured documents. In this case, you will benefit more from excellent instruction-following skills to understand and execute tasks.\n\nWhile these benchmarks are popular and useful, they also suffer from inherent flaws. For example, public benchmarks can be gamed by training models on test data or samples that are very similar to benchmark datasets. Even human evaluation is not perfect and is often biased toward long and confident answers, especially when they’re nicely formatted (e.g., using Markdown). On the other hand, private test sets have not been scrutinized as much as public ones and might have their own issues and biases.\n\nThis means that benchmarks are not a single source of truth but should be used as signals. Once multiple evaluations provide a similar answer, you can raise your confidence level about the real capabilities of a model.",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 881,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 882,
      "content": "Domain-specific LLM evaluations\n\nDomain-specific LLMs don’t have the same scope as general-purpose models. This is helpful to target more fine-grained capabilities with more depth than the previous benchmarks.\n\nWithin the category, the choice of benchmarks entirely depends on the domain in question. For common applications like a language-specific model or a code model, it is recommended to search for relevant evaluations and even benchmark suites. These suites encompass different benchmarks and are designed to be reproducible. By targeting different aspects of a domain, they often capture domain performance more accurately.\n\nTo illustrate this, here is a list of domain-specific evaluations with leaderboards on the Hugging Face Hub:\n\nOpen Medical-LLM Leaderboard: Evaluates the performance of LLMs in medical question-answering tasks. It regroups 9 metrics, with 1,273 questions from the US medical license exams (MedQA), 500 questions from PubMed articles (PubMedQA), 4,183 questions from Indian medical entrance exams (MedMCQA), and 1,089 questions from 6 sub-categories of MMLU (clinical knowledge, medical genetics, anatomy, professional medicine, college biology, and college medicine).",
      "content_length": 1201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 883,
      "content": "BigCodeBench Leaderboard: Evaluates the performance of code LLMs, featuring two main categories: BigCodeBench-Complete for code completion based on structured docstrings, and BigCodeBench- Instruct for code generation from natural language instructions. Models are ranked by their Pass@1 scores using greedy decoding, with an additional Elo rating for the Complete variant. It covers a wide range of programming scenarios that test LLMs’ compositional reasoning and instruction-following capabilities.\n\nHallucinations Leaderboard: Evaluates LLMs’ tendency to produce false or unsupported information across 16 diverse tasks spanning 5 categories. These include Question Answering (with datasets like NQ Open, TruthfulQA, and SQuADv2), Reading Comprehension (using TriviaQA and RACE), Summarization (employing HaluEval Summ, XSum, and CNN/DM), Dialogue (featuring HaluEval Dial and FaithDial), and Fact Checking (utilizing MemoTrap, SelfCheckGPT, FEVER, and TrueFalse). The leaderboard also assesses instruction- following ability using IFEval.\n\nEnterprise Scenarios Leaderboard: Evaluates the performance of LLMs on six real-world enterprise use cases, covering diverse tasks relevant to business applications. The benchmarks include FinanceBench (100 financial questions with retrieved context), Legal Confidentiality (100 prompts from LegalBench for legal reasoning), Writing Prompts (creative writing evaluation), Customer Support Dialogue (relevance in customer service interactions), Toxic Prompts (safety assessment for harmful content generation), and Enterprise PII (business safety for sensitive information protection). Some test sets are closed-source to prevent gaming of the leaderboard. The evaluation focuses on specific capabilities such as answer accuracy, legal reasoning, creative writing, contextual relevance, and safety measures, providing a comprehensive assessment of LLMs’ suitability for enterprise environments.",
      "content_length": 1938,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 884,
      "content": "Leaderboards can have different approaches based on their domain. For example, BigCodeBench is significantly different from others because it relies on only two metrics that sufficiently capture the entire domain. On the other hand, the Hallucinations Leaderboard regroups 16 metrics, including many general-purpose evaluations. It shows that in addition to custom benchmarks, reusing general-purpose ones can complete your own suite.\n\nIn particular, language-specific LLMs often reuse translated versions of general-purpose benchmarks. This can be completed with original evaluations in the native language. While some of these benchmarks use machine translation, it is better to rely on human-translated evaluations to improve their quality. We selected the following three task-specific leaderboards and their respective evaluation suites to give you an idea of how to build your own:\n\nOpenKo-LLM Leaderboard: Evaluates the performance of Korean LLMs using nine metrics. These metrics are a combination of general- purpose benchmarks translated into Korean (GPQA, Winogrande, GSM8K, EQ-Bench, and IFEval) and custom evaluations (Knowledge, Social Value, Harmlessness, and Helpfulness).\n\nOpen Portuguese LLM Leaderboard: Evaluates the performance of Portuguese language LLMs using nine diverse benchmarks. These benchmarks include educational assessments (ENEM with 1,430 questions, and BLUEX with 724 questions from university entrance exams), professional exams (OAB Exams with over 2,000 questions), language understanding tasks (ASSIN2 RTE and STS, FAQUAD NLI),",
      "content_length": 1567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 885,
      "content": "and social media content analysis (HateBR with 7,000 Instagram comments, PT Hate Speech with 5,668 tweets, and tweetSentBR).\n\nOpen Arabic LLM Leaderboard: Evaluates the performance of Arabic language LLMs using a comprehensive set of benchmarks, including both native Arabic tasks and translated datasets. The leaderboard features two native Arabic benchmarks: AlGhafa and Arabic-Culture- Value-Alignment. Additionally, it incorporates 12 translated benchmarks covering various domains, such as MMLU, ARC- Challenge, HellaSwag, and PIQA.\n\nBoth general-purpose and domain-specific evaluations are designed with three main principles. First, they should be complex and challenge models to distinguish good and bad outputs. Second, they should be diverse and cover as many topics and scenarios as possible. When one benchmark is not enough, additional ones can create a stronger suite. Finally, they should be practical and easy to run. This is more connected to evaluation libraries, which can be more or less complex to work with. We recommend lm- evaluation-harness (github.com/EleutherAI/lm-evaluation-harness) from Eleuther AI and lighteval (github.com/huggingface/lighteval) from Hugging Face to run your benchmarks.",
      "content_length": 1219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 886,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 887,
      "content": "Task-specific LLM evaluations\n\nWhile general-purpose and domain-specific evaluations indicate strong base or instruct models, they cannot provide insights into how well these models work for a given task. This requires benchmarks specifically designed for this purpose, measuring downstream performance.\n\nBecause of their narrow focus, task-specific LLMs can rarely rely on pre-existing evaluation datasets. This can be advantageous because their outputs also tend to be more structured and easier to evaluate using traditional ML metrics. For example, a summarization task can leverage the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metric, which measures the overlap between the generated text and reference text using n-grams.\n\nLikewise, classification tasks also benefit from it and use the following classic metrics, among others:\n\nAccuracy: Accuracy refers to the proportion of correctly predicted instances compared to the total instances. It’s particularly useful for tasks with categorical outputs or where there is a clear distinction between right and wrong answers, such as named entity recognition (NER).\n\nPrecision: The ratio of true positive predictions to the total positive predictions made by the model.\n\nRecall: The ratio of true positive predictions to the total actual positive instances.\n\nF1 Score: The harmonic mean of precision and recall, used to balance both metrics. These are particularly useful in tasks such as classification or entity extraction.\n\nWhen the task cannot be directly mapped to a traditional ML task, it is possible to create a custom benchmark. This benchmark can be inspired by general-purpose and domain-specific evaluation datasets. A common and successful pattern is the use of multiple-choice question answering. In this framework, the instruction consists of a question with several options. See the following example with a question from the MMLU dataset (abstract algebra):\n\nInstruction Find the degree for the given field extension Q(sqrt(2), sqrt(3)) over Q. A. 0 B. 4 C. 2 D. 6\n\nOutput B",
      "content_length": 2058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 888,
      "content": "Table 7.1: Example from the MMLU dataset\n\nThere are two main ways of evaluating models with this scheme—text generation and log-likelihood evaluations:\n\nThe first approach involves having the model generate text responses and comparing those to predefined answer choices. For example, the model generates a letter (A, B, C, or D) as its answer, which is then checked against the correct answer. This method tests the model’s ability to produce coherent and accurate responses in a format similar to how it would be used in real-world applications.\n\nEvaluation using probabilities, on the other hand, looks at the model’s predicted probabilities for different answer options without requiring text generation. For MMLU, lm-evaluation-harness compares the probabilities for the full text of each answer choice. This approach allows for a more nuanced assessment of the model’s understanding, as it can capture the relative confidence the model has in different options, even if it wouldn’t necessarily generate the exact correct answer text.\n\nFor simplicity, we recommend the text-generation version of the evaluation that mimics human test-taking. It is easier to implement, and generally more discriminative, as low-quality models tend to overperform on probability- based evaluations. You can adapt this technique to quiz your models about a particular task, and even expand it to specific domains.\n\nConversely, if the task is too open-ended, traditional ML metrics and multiple-choice question answering might not be relevant. In this scenario, the LLM-as-a-judge technique introduced in Chapter 5 can be used to evaluate the quality of the answers. If you have ground-truth answers, providing them as additional context improves the accuracy of the evaluation. Otherwise, defining different dimensions (such as relevance or toxicity, depending on your task) can also ground the evaluation in more interpretable categories.\n\nIt is recommended to use large models for evaluation and to iteratively refine your prompt. In this process, the explanations outputted by the model are important for understanding errors in its reasoning and fixing them through additional prompt engineering.\n\nIn order to easily parse answers, one can specify a structure in the instruction or use some kind of structured generation (like Outlines or OpenAI’s JSON mode). Here is an example of an instruction with a structure:\n\nYou are an evaluator who assesses the quality of an answer to an instruction. Your goal is to provide a score that re",
      "content_length": 2523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 889,
      "content": "Table 7.2: Example of general-purpose LLM-as-a-judge prompt for answer evaluation\n\nNaturally, you can tweak the scale, add a ground-truth answer to this prompt, and customize it for your own use cases.\n\nHowever, judge LLMs can exhibit biases favoring assertive or verbose responses, potentially overrating answers that sound more confident but are less accurate. They may also lack domain expertise for specialized topics, leading to misjudgments. Consistency is also a concern, as LLMs might score similar responses differently. Additionally, they could have implicit preferences for certain writing styles unrelated to actual answer quality. To mitigate these issues, it’s possible to combine LLM evaluations with other metrics, use multiple judges, and carefully design prompts to address biases.\n\nOnce a model has been properly evaluated and works as intended, it might be included within a broader system. In the next section, we will see how systems change the evaluation framework.",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 890,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 891,
      "content": "RAG evaluation\n\nWhile traditional LLM evaluation focuses on the model’s inherent capabilities, RAG evaluation requires a more comprehensive approach that considers both the model’s generative abilities and its interaction with external information sources.\n\nRAG systems combine the strengths of LLMs with information retrieval mechanisms, allowing them to generate responses that are not only coherent and contextually appropriate but also grounded in up-to-date, externally sourced information. This makes RAG particularly valuable in fields where current and accurate information is crucial, such as news reporting, research, and customer support.\n\nThe evaluation of RAG systems goes beyond assessing a standalone LLM. It requires examining the entire system’s performance, including:\n\nRetrieval accuracy: How well does the system fetch relevant information?\n\nIntegration quality: How effectively is the retrieved information incorporated into the generated response?\n\nFactuality and relevance: Does the final output address the query appropriately while seamlessly blending retrieved and generated content?\n\nKey metrics for RAG evaluation include retrieval precision and recall, which measure the accuracy and comprehensiveness of the retrieved information. Additionally, the quality of integration between retrieved data and generated text is crucial, as is the overall factuality and coherence of the output.\n\nTo illustrate how these metrics are applied in practice, consider a RAG system designed for a customer support chatbot in an e-commerce setting. In this scenario, the user asks “What’s your return policy for laptops purchased during the holiday sale?” The RAG pipeline finds relevant documents on the electronics return policy and documents on holiday sale terms. This additional context is appended at the end of the question, and the model uses it to respond:\n\nFor laptops purchased during our holiday sale, you have an extended return period of 60 days from the date of purc",
      "content_length": 1992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 892,
      "content": "Table 7.3: Example of output from a RAG pipeline designed for customer support\n\nIn this pipeline, we can evaluate if the retrieved documents correspond to what was expected (retrieval accuracy). We can also measure the difference between responses with and without additional context (integration quality). Finally, we can assess whether the output is relevant and grounded in the information provided by the documents (factuality and relevance).\n\nIn this section, we will cover two methods to evaluate how well RAG models incorporate external information into their responses.",
      "content_length": 577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 893,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 894,
      "content": "Ragas\n\nRetrieval-Augmented Generation Assessment (Ragas) is an open-source toolkit designed to provide developers with a comprehensive set of tools for RAG evaluation and optimization. It’s designed around the idea of metrics-driven development (MDD), a product development approach that relies on data to make well-informed decisions, involving the ongoing monitoring of essential metrics over time to gain valuable insights into an application’s performance. By embracing this methodology, Ragas enables developers to objectively assess their RAG systems, identify areas for improvement, and track the impact of changes over time.\n\nOne of the key capabilities of Ragas is its ability to synthetically generate diverse and complex test datasets. This feature addresses a significant pain point in RAG development, as manually creating hundreds of questions, answers, and contexts is both time-consuming and labor-intensive. Instead, it uses an evolutionary approach paradigm inspired by works like Evol- Instruct to craft questions with varying characteristics such as reasoning complexity, conditional elements, and multi-context requirements. This approach ensures a comprehensive evaluation of different components within the RAG pipeline.\n\nAdditionally, Ragas can generate conversational samples that simulate chat- based question-and-follow-up interactions, allowing developers to evaluate their systems in more realistic scenarios.",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 895,
      "content": "Figure 7.1: Overview of the Ragas evaluation framework\n\nAs illustrated in Figure 7.1, Ragas provides a suite of LLM-assisted evaluation metrics designed to objectively measure different aspects of RAG system performance. These metrics include:\n\nFaithfulness: This metric measures the factual consistency of the generated answer against the given context. It works by breaking down the answer into individual claims and verifying if each claim can be inferred from the provided context. The faithfulness score is calculated as the ratio of verifiable claims to the total number of claims in the answer.\n\nAnswer relevancy: This metric evaluates how pertinent the generated answer is to the given prompt. It uses an innovative approach where an LLM is prompted to generate multiple questions based on the answer and then calculates the mean cosine similarity between these generated questions and the original question. This method helps identify answers that may be factually correct but off-topic or incomplete.",
      "content_length": 1010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 896,
      "content": "Context precision: This metric evaluates whether all the ground-truth relevant items present in the contexts are ranked appropriately. It considers the position of relevant information within the retrieved context, rewarding systems that place the most pertinent information at the top.\n\nContext recall: This metric measures the extent to which the retrieved context aligns with the annotated answer (ground truth). It analyzes each claim in the ground truth answer to determine whether it can be attributed to the retrieved context, providing insights into the completeness of the retrieved information.\n\nFinally, Ragas also provides building blocks for monitoring RAG quality in production environments. This facilitates continuous improvement of RAG systems. By leveraging the evaluation results from test datasets and insights gathered from production monitoring, developers can iteratively enhance their applications. This might involve fine-tuning retrieval algorithms, adjusting prompt engineering strategies, or optimizing the balance between retrieved context and LLM generation.\n\nRagas can be complemented with another approach, based on custom classifiers.",
      "content_length": 1167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 897,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 898,
      "content": "ARES\n\nARES (an automated evaluation framework for RAG systems) is a comprehensive tool designed to evaluate RAG systems. It offers an automated process that combines synthetic data generation with fine-tuned classifiers to assess various aspects of RAG performance, including context relevance, answer faithfulness, and answer relevance.\n\nThe ARES framework operates in three main stages: synthetic data generation, classifier training, and RAG evaluation. Each stage is configurable, allowing users to tailor the evaluation process to their specific needs and datasets.\n\nIn the synthetic data generation stage, ARES creates datasets that closely mimic real-world scenarios for robust RAG testing. Users can configure this process by specifying document file paths, few-shot prompt files, and output locations for the synthetic queries. The framework supports various pre-trained language models for this task, with the default being google/flan-t5-xxl. Users can control the number of documents sampled and other parameters to balance between comprehensive coverage and computational efficiency.",
      "content_length": 1096,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 899,
      "content": "Figure 7.2: Overview of the ARES evaluation framework\n\nThe classifier training stage involves creating high-precision classifiers to determine the relevance and faithfulness of RAG outputs. Users can specify the classification dataset (typically generated from the previous stage), test set for evaluation, label columns, and model choice. ARES uses microsoft/deberta-v3-large as the default model but supports other Hugging Face models. Training parameters such as the number of epochs, patience value for early stopping, and learning rate can be fine-tuned to optimize classifier performance.\n\nThe final stage, RAG evaluation, leverages the trained classifiers and synthetic data to assess the RAG model’s performance. Users provide evaluation datasets, few-shot examples for guiding the evaluation, classifier checkpoints, and gold label paths. ARES supports various evaluation metrics and can generate confidence intervals for its assessments.",
      "content_length": 947,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 900,
      "content": "ARES offers flexible model execution options, supporting both cloud-based and local runs through vLLM integration. The framework also supports various artifact types (code snippets, documents, HTML, images, and so on), enabling comprehensive evaluation across different RAG system outputs.\n\nIn summary, Ragas and ARES complement each other through their distinct approaches to evaluation and dataset generation. Ragas’s strength in production monitoring and LLM-assisted metrics can be combined with ARES’s highly configurable evaluation process and classifier-based assessments. While Ragas may offer more nuanced evaluations based on LLM capabilities, ARES provides consistent and potentially faster evaluations once its classifiers are trained. Combining them offers a comprehensive evaluation framework, benefiting from quick iterations with Ragas and in-depth, customized evaluations with ARES at key stages.\n\nIn the next section, we will create our own evaluation framework to evaluate our task-specific TwinLlama-3.1-8B model.",
      "content_length": 1033,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 901,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 902,
      "content": "Evaluating TwinLlama-3.1-8B\n\nIn the previous chapters, we created two models fine-tuned to generate high-quality posts and articles: TwinLlama-3.1-8B and TwinLlama-3.1-8B- DPO. Based on this summary, we want to assess their abilities to write text that is both accurate and well-written. In comparison, general-purpose fine- tuned models are accurate thanks to their extensive knowledge but often use overly formal and verbose language. With this fine-tuning, we want to adopt a more natural writing style, based on the original articles from the training set.\n\nDue to the open-ended nature of this problem, we will leverage a judge LLM to evaluate the quality of the generated text. It will take both the instruction and the answer as inputs, and score it on a 1–3 scale based on two criteria:\n\nAccuracy: The degree of factual correctness and comprehensiveness of the information presented in the answer\n\nStyle: The appropriateness of the tone and writing style for blog posts or social media content (no formal or academic expressions)\n\nIn our evaluation framework, we will use the test split of our instruction dataset to get test instructions. We will feed them to our models and generate answers. These answers will then be evaluated by our judge LLM",
      "content_length": 1255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 903,
      "content": "(GPT-4o-mini), based on a prompt that specifies our criteria. Finally, we will analyze the scores and draw conclusions based on qualitative and quantitative evaluations.",
      "content_length": 169,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 904,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 905,
      "content": "Generating answers\n\nThe first step consists of efficiently generating answers for each instruction in our test set. In addition to our two models, we will also use meta- llama/Meta-Llama-3.1-8B-Instruct, the official instruct version of Llama- 3.1-8B, as a reference point to better understand the trade-offs we made.\n\nLet’s start the first stage of the implementation:\n\nWe import the relevant libraries, including vLLM for fast generation. This library is a lot faster than transformers for batch generation with local models:\n\nfrom\n\nvllm\n\nimport\n\nLLM, SamplingParams\n\nfrom\n\ndatasets",
      "content_length": 584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 906,
      "content": "import\n\nload_dataset\n\nfrom\n\ntqdm.auto\n\nimport\n\ntqdm\n\nimport\n\ngc\n\nWe define a function called\n\ngenerate_answers\n\nthat will process our dataset and generate responses using a specified model. It takes two inputs—the ID of the model we want to use and the name of the test dataset:\n\ndef\n\ngenerate_answers\n\n(\n\nmodel_id, dataset_name",
      "content_length": 328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 907,
      "content": "): dataset = load_dataset(dataset_name, split=\n\n\"test\"\n\n)\n\nWe need to format the raw instructions using the chat template our models have been trained on. Note that Llama-3.1-8B-Instruct has been used with a different template, but it can follow this simple format. Here, we use the same chat template with every model for simplicity. We map the entire test set to this template with the\n\nformat()\n\nfunction:\n\ndef\n\nformat\n\n(\n\nsample\n\n):\n\nreturn\n\n\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{}\\n\\n### Response:\\n\"",
      "content_length": 596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 908,
      "content": ".\n\nformat\n\n(sample[\n\n\"instruction\"\n\n]) dataset = dataset.\n\nmap\n\n(\n\nlambda\n\nsample: {\n\n\"prompt\"\n\n:\n\nformat\n\n(sample)})\n\nLet’s initialize the LLM object used by vLLM with a maximum length of 4,096 tokens. We can also specify sampling parameters, which correspond to variables used in the decoding strategy. Here, we use parameters to encourage diversity (high temperature) while removing the most unlikely tokens (\n\ntop_p\n\nand",
      "content_length": 424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 909,
      "content": "min_p\n\n). Finally, we start the generation by providing the list of prompts with\n\ndataset[\"prompt\"]\n\n:\n\nllm = LLM(model=model_id, max_model_len=\n\n4096\n\n) sampling_params = SamplingParams(temperature=\n\n0.8\n\n, top_p=\n\n0.95\n\n, min_p=\n\n0.05\n\n, max_tokens=\n\n4096\n\n) outputs = llm.generate(dataset[\n\n\"prompt\"\n\n], sampling_params)",
      "content_length": 323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 910,
      "content": "This process should take a few minutes with our 334 prompts. Once this is done, we extract the answers from the object that is outputted by vLLM. We then add these answers as a new column to our dataset. This is useful to log the answers and review them later:\n\nanswers = [output.outputs[\n\n0\n\n].text\n\nfor\n\noutput\n\nin\n\noutputs] dataset = dataset.add_column(\n\n\"answers\"\n\n, answers)\n\nWe save our results to the Hugging Face Hub for easy access later. Then, we clear our GPU memory to prevent running out of space when we process the next model:",
      "content_length": 541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 911,
      "content": "print\n\n(\n\nf\"Uploading results for\n\n{model_id}\n\n\"\n\n) dataset.push_to_hub(\n\nf\"mlabonne/\n\n{model_id.split(\n\n'/'\n\n)[-\n\n1\n\n]}\n\nresults\"\n\n) gc.collect()\n\nreturn\n\ndataset",
      "content_length": 163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 912,
      "content": "We create a list of the three models we want to test. Then, we run our\n\ngenerate_answers()\n\nfunction for each of these models, one at a time. This will create and upload a separate set of results for each model:\n\nmodel_ids = [\n\n'mlabonne/TwinLlama-3.1-8B'\n\n,\n\n'mlabonne/TwinLlama-3.1-8B-DPO'\n\n,\n\n'meta-llama/Meta-Llama-3.1-8B-Instruct'\n\n]\n\nfor\n\nmodel_id\n\nin\n\nmodel_ids: generate_answers(model_id,\n\n\"mlabonne/llmtwin\"\n\n)",
      "content_length": 419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 913,
      "content": "Now that we have the answer generation, we can move on to the evaluation process.",
      "content_length": 81,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 914,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 915,
      "content": "Evaluating answers\n\nTo evaluate our answers, we will rely on GPT-4o-mini as a judge. This strategy is similar to what we used for data generation. As a matter of fact, you could adapt it to filter out bad samples during the data generation process. Here, we will score every generated answer from every model in terms of accuracy and style. The average scores will inform us about the quality of our fine-tuning compared to Llama-3.1-8B-Instruct:\n\nFirst, we import the required libraries, including\n\nopenai\n\n:\n\nimport\n\njson\n\nfrom\n\ntyping\n\nimport\n\nList",
      "content_length": 551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 916,
      "content": "from\n\ndatasets\n\nimport\n\nDataset, load_dataset\n\nfrom\n\nopenai\n\nimport\n\nOpenAI\n\nfrom\n\ntqdm.auto\n\nimport\n\ntqdm\n\nimport\n\nconcurrent.futures\n\nWe then define the\n\nevaluate_answer()\n\nfunction. This function contains our evaluation prompt, which sets up the context for evaluating answers based on accuracy and style:",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 917,
      "content": "def\n\nevaluate_answer\n\n(\n\ninstruction:\n\nstr\n\n, answer:\n\nstr\n\n, client: OpenAI\n\n) ->\n\ndict\n\n: prompt =\n\nf\"\"\"You are an expert judge. Please evaluate the quality of a given answer to an instruction based on two criteria:\n\n1. Accuracy: How factually correct is the information presented in the answer? You are a technical expert in this topic.\n\n2. Style: Is the tone and writing style appropriate for a blog post or social media content? It should use simple but technical words and avoid formal or academic language.",
      "content_length": 513,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 918,
      "content": "In the same prompt, we define our scales for each metric. Those are three- point Likert scales with a precise definition for each score:\n\nAccuracy scale: 1 (Poor): Contains factual errors or misleading information 2 (Good): Mostly accurate with minor errors or omissions 3 (Excellent): Highly accurate and comprehensive Style scale: 1 (Poor): Too formal, uses some overly complex words 2 (Good): Good balance of technical content and accessibility, but still uses formal words and expressions 3 (Excellent): Perfectly accessible language for blog/social media, uses simple but precise technical terms when necessary\n\nFinally, we conclude the prompt with two examples to illustrate what we mean by “complex words” and “formal or academic language.” We provide the corresponding instruction-answer pair and ask the model to return a response in JSON:\n\nExample of bad style: The Llama2 7B model constitutes a noteworthy progression in the field of artificial intelligence, serving as the successor to its predecessor, the original Llama architecture. Example of excellent style: Llama2 7B outperforms the original Llama model across multiple benchmarks. Instruction: {instruction} Answer: {answer} Provide your evaluation in JSON format with the following structure: {{\n\n\"accuracy\"\n\n: {{",
      "content_length": 1284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 919,
      "content": "\"analysis\"\n\n:\n\n\"...\"\n\n,\n\n\"score\"\n\n:\n\n0\n\n}},\n\n\"style\"\n\n: {{\n\n\"analysis\"\n\n:\n\n\"...\"\n\n,\n\n\"score\"\n\n:\n\n0\n\n}} }}\n\n\"\"\"",
      "content_length": 110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 920,
      "content": "This prompt is given as a user query to the GPT-4o-mini model. The system prompt reinforces that we are interested in answer evaluation based on accuracy and style:\n\ncompletion = client.chat.completions.create( model=\n\n\"gpt-4o-mini\"\n\n, messages=[ {\n\n\"role\"\n\n:\n\n\"system\"\n\n,\n\n\"content\"\n\n:\n\n\"You are a helpful assistant who evaluates answers based on accuracy and style. Provide your response in JSON format with a short analysis and score for each criterion.\"\n\n, }, {\n\n\"role\"\n\n:\n\n\"",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 921,
      "content": "user\"\n\n,\n\n\"content\"\n\n: prompt}, ], response_format={\n\n\"type\"\n\n:\n\n\"json_object\"\n\n}, max_tokens=\n\n1000\n\n, temperature=\n\n0.8\n\n, )\n\nAs in the previous chapters, we will batch our requests to speed up the process. This is why we create an\n\nevaluate_batch()\n\nfunction, which returns a list of parsed structured outputs with their corresponding indices. These indices are important to ensure a correct ordering of the evaluations:",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 922,
      "content": "def\n\nevaluate_batch\n\n(\n\nbatch, start_index\n\n): client = OpenAI(api_key=OPENAI_KEY)\n\nreturn\n\n[ (i, evaluate_answer(instr, ans, client))\n\nfor\n\ni, (instr, ans)\n\nin\n\nenumerate\n\n(batch, start=start_index) ]\n\nWe can now orchestrate the previous code in the\n\nevaluate_answers()\n\nfunction. It takes the model ID, number of threads, and batch size as inputs. First, we load the dataset with the generations we previously saved:",
      "content_length": 418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 923,
      "content": "def\n\nevaluate_answers\n\n(\n\nmodel_id:\n\nstr\n\n, num_threads:\n\nint\n\n=\n\n10\n\n, batch_size:\n\nint\n\n=\n\n5\n\n) -> Dataset: dataset = load_dataset(\n\nf\"mlabonne/\n\n{model_id.split(\n\n'/'\n\n)[-\n\n1",
      "content_length": 177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 924,
      "content": "]}\n\nresults\"\n\n, split=\n\n\"all\"\n\n)\n\nWe create batches of instruction-answer pairs from our dataset. Each batch contains\n\nbatch_size\n\nnumber of pairs:\n\nbatches = [ (i,\n\nlist\n\n(\n\nzip\n\n(dataset[\n\n\"instruction\"\n\n][i:i+batch_size], dataset[",
      "content_length": 233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 925,
      "content": "\"answers\"\n\n][i:i+batch_size])))\n\nfor\n\ni\n\nin\n\nrange\n\n(\n\n0\n\n,\n\nlen\n\n(dataset), batch_size) ]\n\nWe perform parallel evaluation of batches of instruction-answer pairs using multiple threads. We use parallel processing to evaluate multiple batches simultaneously, speeding up the overall evaluation process. The\n\nThreadPoolExecutor\n\nsubmits each batch to\n\nevaluate_batch()\n\n. The results are stored in the evaluations list:",
      "content_length": 417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 926,
      "content": "evaluations = [\n\nNone\n\n] *\n\nlen\n\n(dataset)\n\nwith\n\nconcurrent.futures.ThreadPoolExecutor(max_workers=num_threads)\n\nas\n\nexecutor: futures = [executor.submit(evaluate_batch, batch, start_index)\n\nfor\n\nstart_index, batch\n\nin\n\nbatches]\n\nfor\n\nfuture\n\nin\n\ntqdm(concurrent.futures.as_completed(futures), total=\n\nlen",
      "content_length": 306,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 927,
      "content": "(futures)):\n\nfor\n\nindex, evaluation\n\nin\n\nfuture.result(): evaluations[index] = evaluation\n\nWe create a new column with the result of the evaluation process. This column will store the raw JSON output of the judge model, including scores and explanations:\n\nif\n\n'evaluation'\n\nin\n\ndataset.column_names: dataset = dataset.remove_columns([\n\n'evaluation'\n\n]) dataset = dataset.add_column(\n\n\"evaluation\"\n\n, evaluations)",
      "content_length": 412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 928,
      "content": "We can directly parse this JSON object with\n\njson.loads()\n\nand try to retrieve the accuracy and style scores that should have been generated. This generation is in best-effort mode, which means that scores are not guaranteed. If there’s an error in parsing, we use\n\nNone\n\nvalues as a fallback:\n\naccuracy_scores = [] style_scores = []\n\nfor\n\nevaluation\n\nin\n\ndataset[\n\n'evaluation'\n\n]:\n\ntry\n\n: eval_dict = json.loads(evaluation)\n\nif",
      "content_length": 429,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 929,
      "content": "isinstance\n\n(evaluation,\n\nstr\n\n)\n\nelse\n\nevaluation accuracy_score = eval_dict[\n\n'accuracy'\n\n][\n\n'score'\n\n] style_score = eval_dict[\n\n'style'\n\n][\n\n'score'\n\n] accuracy_scores.append(accuracy_score) style_scores.append(style_score)\n\nexcept\n\n(json.JSONDecodeError, KeyError, TypeError): accuracy_scores.append(\n\nNone\n\n) style_scores.append(\n\nNone\n\n)",
      "content_length": 345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 930,
      "content": "We add two new columns to store the accuracy and style scores for further analysis:\n\nif\n\n'accuracy'\n\nin\n\ndataset.column_names: dataset = dataset.remove_columns([\n\n'accuracy'\n\n]) dataset = dataset.add_column(\n\n'accuracy'\n\n, accuracy_scores)\n\nif\n\n'style'\n\nin\n\ndataset.column_names: dataset = dataset.remove_columns([\n\n'style'\n\n]) dataset = dataset.add_column(",
      "content_length": 357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 931,
      "content": "'style'\n\n, style_scores)\n\nLet’s push the final dataset with generated answers, evaluations, and scores to the Hugging Face Hub:\n\ndataset.push_to_hub(\n\nf\"mlabonne/\n\n{model_id.split(\n\n'/'\n\n)[-\n\n1\n\n]}\n\nresults\"\n\n)\n\nreturn\n\ndataset",
      "content_length": 227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 932,
      "content": "We can now call the\n\nevaluate_answers()\n\nfunction with the three models we selected:\n\nmodel_ids = [\n\n'mlabonne/TwinLlama-3.1-8B'\n\n,\n\n'mlabonne/TwinLlama-3.1-8B-DPO'\n\n,\n\n'meta-llama/Meta-Llama-3.1-8B-Instruct'\n\n]\n\nfor\n\nmodel_id\n\nin\n\nmodel_ids: evaluate_answers(model_id)",
      "content_length": 269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 933,
      "content": "By saving intermediate results, our evaluation framework becomes more robust to failures. It is also easily extendable to other models, datasets, and criteria.",
      "content_length": 159,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 934,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 935,
      "content": "Analyzing results\n\nThere are three elements we can review at the end of this evaluation process: model answers, explanations from evaluations, and scores.\n\nWhile reviewing answers is not scalable, it is a crucial step that can help to identify a lot of common mistakes, such as a wrong chat template or incorrect model. Here, we will focus on a single instruction and read the answers provided by our two models and Llama-3.1-8B-Instruct.\n\nInstruction Discuss the concept of algorithm bias and its implications.\n\nTwinLlama-3.1-8B Algorithm bias refers to the tendency of algorithms to produce outcomes that are skewed or un\n\nTwinLlama-3.1-8B-DPO Algorithm bias refers to the tendency of algorithms to produce outcomes that are skewed\n\nLlama-3.1-8B-Instruct Algorithm bias, also known as algorithmic bias, refers to the unintended or inherent bias in\n\nTable 7.4: Example of generated answers for a given instruction\n\nImmediately, we can see that the answers from our models are very close to each other. This is not the case with Llama-3.1-8B-Instruct’s answer, which is extremely verbose and lists many examples. This is typically the kind of answer that is correct but can feel artificial and unpleasant to read because of its length. On the other hand, the DPO model slightly simplifies the language of the SFT version, making it less academic. This is exactly the behavior we want to capture, modifying the writing style but not the actual content of the answer.\n\nLet’s now review the evaluations provided by GPT-4o-mini for each answer.\n\nTwinLlama-3.1-8B\n\nAccuracy The answer accurately defines algorithm bias and highlights its implications in fields like machine learni\n\nTwinLlama-3.1-8B-DPO\n\nAccuracy The answer accurately defines algorithm bias and outlines its implications in critical fields like machine\n\nLlama-3.1-8B-Instruct",
      "content_length": 1837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 936,
      "content": "Accuracy The answer accurately defines algorithm bias and discusses its causes and implications in various domai\n\nTable 7.5: Evaluations of each answer made by GPT-4o-mini, according to style and accuracy\n\nAccording to our judge LLM, there is no issue with the accuracy of the answers, which get a perfect score. However, the style is considered too formal for TwinLlama-3.1-8B (SFT) and Llama-3.1-8B-Instruct, with a score of 2. The judge LLM agreed with our previous analysis and assigned a perfect score to TwinLlama-3.1-8B-DPO’s answer for communicating “the technical concept of algorithm bias without becoming overly formal.”\n\nThis trend is confirmed by the average scores obtained by each model:\n\nTwinLlama-3.1-8B - Accuracy: 2.45 TwinLlama-3.1-8B - Style: 2.04 TwinLlama-3.1-8B-DPO - Accuracy: 2.46\n\nTwinLlama-3.1-8B-DPO - Style: 2.12\n\nLlama-3.1-8B-Instruct - Accuracy: 2.62\n\nLlama-3.1-8B-Instruct - Style: 1.86\n\nIn terms of accuracy, our two fine-tuned models get similar scores, while Llama-3.1-8B-Instruct achieves the highest accuracy score of 2.62. This suggests that the instruct-tuned Llama model may have a slight edge in providing factually correct information. This is probably due to its extensive post-training process with over 10 million samples (compared to 13,000 in our case).\n\nHowever, when it comes to style, we see a different pattern. TwinLlama-3.1-8B-DPO leads with a score of 2.12, successfully achieving a more accessible and less formal writing style without sacrificing content quality. TwinLlama-3.1-8B (SFT) follows with 2.04, showing improvement but retaining some formality, while Llama-3.1- 8B-Instruct trails with 1.86, tending toward verbosity.\n\nBased on this feedback and the manual review of the generated answers, we can detect mistakes and identify areas for improvement. This is essential for refining the data generation process through additional filtering or augmenting the dataset with missing information. While this first version already shows promising results, iterating over different datasets and models will allow us to significantly outperform our baseline and create the best possible model for our use case.",
      "content_length": 2167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 937,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 938,
      "content": "Summary\n\nIn this chapter, we explored LLM evaluation with models and RAG systems. We saw how to interpret classic benchmarks like MMLU to select strong candidates to use or fine-tune. We also detailed how domain-specific and task-specific evaluations work, and how to create our own based on publicly available examples.\n\nWe focused on two techniques (multiple-choice question answering and LLM-as-a-judge) as the backbone of these custom evaluation frameworks.\n\nHowever, models are commonly integrated into broader systems that provide additional context. We introduced two evaluation frameworks for RAG systems, Ragas and ARES. We saw both similarities (for example, synthetic data generation) and differences in how they evaluate RAG systems (context-based metrics versus trained classifiers). Finally, we evaluated TwinLlama-3.1-8B with a judge LLM according to three criteria: relevance, coherence, and conciseness. This provided insights into how we can improve it.\n\nIn the next chapter, we will explore inference optimization techniques to improve speed and reduce memory usage, without significantly compromising model performance. We will also delve into optimization methods, model parallelism techniques and examine different quantization approaches.",
      "content_length": 1261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 939,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 940,
      "content": "References\n\nLianmin Zheng et al.. “Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.” arXiv preprint arXiv:2306.05685, June 2023.\n\nAymeric Roucher. “Using LLM-as-a-judge for an automated and versatile evaluation - Hugging Face Open-Source AI Cookbook.” huggingface.co, No date found, https://huggingface.co/learn/cookbook/en/llm_judge.\n\nLangChain. “Aligning LLM-as-a-Judge with Human Preferences.” blog.langchain.dev, June 26, 2024, https://blog.langchain.dev/aligning-llm- as-a-judge-with-human-preferences/.\n\nDan Hendrycks et al.. “Measuring Massive Multitask Language Understanding.” arXiv preprint arXiv:2009.03300, September 2020.\n\nJeffrey Zhou et al.. “Instruction-Following Evaluation for Large Language Models.” arXiv preprint arXiv:2311.07911, November 2023.\n\nYann Dubois et al.. “Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators.” arXiv preprint arXiv:2404.04475, April 2024.",
      "content_length": 916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 941,
      "content": "Grégoire Mialon et al.. “GAIA: a benchmark for General AI Assistants.” arXiv preprint arXiv:2311.12983, November 2023.\n\nGiwon Hong et al.. “The Hallucinations Leaderboard -- An Open Effort to Measure Hallucinations in Large Language Models.” arXiv preprint arXiv:2404.05904, April 2024.\n\nShahul Es et al.. “RAGAS: Automated Evaluation of Retrieval Augmented Generation.” arXiv preprint arXiv:2309.15217, September 2023.\n\nJon Saad-Falcon et al.. “ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems.” arXiv preprint arXiv:2311.09476, November 2023.\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 943,
      "content": "8",
      "content_length": 1,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 944,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 945,
      "content": "Inference Optimization\n\nDeploying LLMs is challenging due to their significant computational and memory requirements. Efficiently running these models necessitates the use of specialized accelerators, such as GPUs or TPUs, which can parallelize operations and achieve higher throughput. While some tasks, like document generation, can be processed in batches overnight, others require low latency and fast generation, such as code completion. As a result, optimizing the inference process – how these models make predictions based on input data – is critical for many practical applications. This includes reducing the time it takes to generate the first token (latency), increasing the number of tokens generated per second (throughput), and minimizing the memory footprint of LLMs.\n\nIndeed, naive deployment approaches lead to poor hardware utilization and underwhelming throughput and latency. Fortunately, a variety of optimization techniques have emerged to dramatically speed up inference. This chapter will explore key methods like speculative decoding, model parallelism, and weight quantization, demonstrating how thoughtful implementations can achieve speedups of 2–4X or more. We will also introduce three popular inference engines (Text Generation Inference, vLLM, and TensorRT-LLM) and compare their features in terms of inference optimization.\n\nIn this chapter, we will cover the following topics:\n\nModel optimization strategies",
      "content_length": 1442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 946,
      "content": "Model parallelism\n\nModel quantization\n\nBy the end of this chapter, you will understand the core challenges in LLM inference and be familiar with state-of-the-art optimization techniques, including model parallelism and weight quantization.\n\nAll the code examples from this chapter can be found on GitHub at https://github.com/PacktPublishing/LLM-Engineering.",
      "content_length": 358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 947,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 948,
      "content": "Model optimization strategies\n\nMost of the LLMs used nowadays, like GPT or Llama, are powered by a decoder-only Transformer architecture. The decoder-only architecture is designed for text-generation tasks. It predicts the next word in a sequence based on preceding words, making it effective for generating contextually appropriate text continuations.\n\nIn contrast, an encoder-only architecture, like BERT, focuses on understanding and representing the input text with detailed embeddings. It excels in tasks that require comprehensive context understanding, such as text classification and named entity recognition. Finally, the encoder- decoder architecture, like T5, combines both functionalities. The encoder processes the input text to generate a context-rich representation, which the decoder then uses to produce the output text. This dual structure is particularly powerful for sequence-to-sequence tasks like translation and summarization, where understanding the input context and generating a relevant output are equally important.\n\nIn this book, we only focus on the decoder-only architecture, which dominates the LLM field.",
      "content_length": 1137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 949,
      "content": "Figure 8.1 – Inference process with decoder-only models. We provide “I have a dream” as input and obtain “of” as output.\n\nAs shown in Figure 8.1, the basic inference process for a decoder-only model involves:\n\nTokenizing the input prompt and passing it through an embedding layer and positional encoding.\n\nComputing key and value pairs for each input token using the multi- head attention mechanism.\n\nGenerating output tokens sequentially, one at a time, using the computed keys and values.\n\nWhile Steps 1 and 2 are computationally expensive, they consist of highly parallelizable matrix multiplication that can achieve high hardware utilization on accelerators like GPUs and TPUs.\n\nThe real challenge is that the token generation in Step 3 is inherently sequential – to generate the next token, you need to have generated all previous tokens. This leads to an iterative process where the output sequence is grown one token at a time, failing to leverage the parallel computing capabilities of the hardware. Addressing this bottleneck is one of the core focuses of inference optimization.",
      "content_length": 1088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 950,
      "content": "In this section, we will detail several optimization strategies that are commonly used to speed up inference and reduce Video Random- Access Memory (VRAM) usage, such as implementing a (static) KV cache, continuous batching, speculative decoding, and optimized attention mechanisms.",
      "content_length": 282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 951,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 952,
      "content": "KV cache\n\nWe saw that LLMs generate text token by token, which is slow because each new prediction depends on the entire previous context. For example, to predict the 100th token in a sequence, the model needs the context of tokens 1 through 99. When predicting the 101st token, it again needs the information from tokens 1 through 99, plus token 100. This repeated computation is particularly inefficient.\n\nThe key-value (KV) cache addresses this issue by storing key-value pairs produced by self-attention layers. Instead of recalculating these pairs for each new token, the model retrieves them from the cache, significantly speeding up the generation.\n\nYou can see an illustration of this technique in Figure 8.2:",
      "content_length": 717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 953,
      "content": "Figure 8.2 – Illustration of the KV cache\n\nWhen a new token is generated, only the key and value for that single token need to be computed and added to the cache. The KV cache is an immediate optimization that is implemented in every popular tool and library. Some implementations maintain a separate KV cache for each layer of the model.",
      "content_length": 338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 954,
      "content": "The size of the KV cache scales with the number of tokens ( several model dimensions, like the number of layers ( of attention heads (\n\n) and\n\n), the number ), and the precision of\n\n), their dimension (\n\nthe parameters in bytes (\n\n):\n\nFor a typical 7B parameter model using 16-bit precision, this exceeds 2 GB for high sequence lengths (higher than 2,048 tokens). Larger models with more layers and higher embedding dimensions will see even greater memory requirements.\n\nSince the KV cache grows with each generation step and is dynamic, it prevents you from taking advantage of torch.compile, a powerful optimization tool that fuses PyTorch code into fast and optimized kernels. The static KV cache solves this issue by pre-allocating the KV cache size to a maximum value, which allows you to combine it with\n\ntorch.compile\n\nfor up to a 4x speedup in the forward pass.\n\nTo configure a model to use a static KV cache with the transformers library, follow these steps:\n\nWe import the tokenizer and the model we want to optimize:",
      "content_length": 1027,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 955,
      "content": "import\n\ntorch\n\nfrom\n\ntransformers\n\nimport\n\nAutoTokenizer, AutoModelForCausalLM model_id =\n\n\"google/gemma-2b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\n\n\"auto\"\n\n)\n\nTo implement the static cache, we change the cache implementation in the model’s generation config to\n\nstatic\n\n:\n\nmodel.generation_config.cache_implementation =",
      "content_length": 404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 956,
      "content": "\"static\"\n\nNow that our KV cache is static, we can compile the model using torch.compile:\n\ncompiled_model = torch.compile(model,\n\nmode\n\n=\n\n\"reduce-overhead\"\n\n,\n\nfullgraph\n\n=\n\nTrue\n\n)\n\nWe tokenize an input question, “\n\nWhat is 2+2?",
      "content_length": 229,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 957,
      "content": "\", and store it on a GPU if available (if not, we store it on the CPU):\n\ndevice =\n\n\"cuda\"\n\nif\n\ntorch.cuda.is_available()\n\nelse\n\n\"cpu\"\n\ninputs = tokenizer(\n\n\"What is 2+2?\"\n\n, return_tensors=\n\n\"pt\"\n\n).to(device)\n\nLet’s use the\n\ngenerate()\n\nmethod to get the model’s output and decode it with\n\nbatch_decode()",
      "content_length": 305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 958,
      "content": "to print its answer:\n\noutputs = model.generate(**inputs, do_sample=\n\nTrue\n\n, temperature=\n\n0.7\n\n, max_new_tokens=\n\n64\n\n)\n\nprint\n\n(tokenizer.batch_decode(outputs, skip_special_tokens=\n\nTrue\n\n))\n\n[\n\n'What is 2+2?\\n\\nThe answer is 4. 2+2 = 4.'\n\n]",
      "content_length": 243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 959,
      "content": "This returns a list containing both the input and output, correctly answering our question.\n\nNote that the static cache doesn’t work with all architectures. For details on which architectures are supported, check out the transformers documentation.\n\nEfficiently managing the KV cache is essential, as it can quickly exhaust available GPU memory and limit the batch sizes that can be processed. This has motivated the development of memory-efficient attention mechanisms and other techniques, which we will cover in the last section.",
      "content_length": 532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 960,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 961,
      "content": "Continuous batching\n\nBatching, or processing multiple inference requests simultaneously, is a standard approach to achieve high throughput. Larger batch sizes spread out the memory cost of model weights and transfer more data to the GPU at once, better saturating its parallel compute capacity.\n\nHowever, decoder-only models pose a particular challenge due to the high variability in input prompt lengths and desired output lengths. Some requests may have short prompts and only need a one-word answer, while others may input a lengthy context and expect a multi-paragraph response.\n\nWith traditional batching, we would have to wait for the longest request in a batch to complete before starting a new batch. This leads to under- utilization as the accelerator sits partly idle waiting for a straggling request to finish. Continuous batching, also known as\n\nin-flight\n\nbatching, aims to prevent idle time by immediately feeding a new request into the batch as soon as one completes.\n\nThe batching process begins the same – by filling the batch with initial requests. But as soon as a request completes its generation, it is evicted from the batch and a new request takes its place. This way, the accelerator is always processing a full batch, leading to maximally efficient hardware utilization. An additional consideration is the need to periodically pause the generation process to run prefill, or the embedding and encoding of waiting",
      "content_length": 1437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 962,
      "content": "requests. Finding the optimal balance between generation and prefill requires some tuning of the waiting-served ratio hyperparameter.\n\nContinuous batching is natively implemented in most inference frameworks, like Hugging Face’s Text Generation Inference (TGI), vLLM, and NVIDIA TensorRT-LLM.",
      "content_length": 292,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 963,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 964,
      "content": "Speculative decoding\n\nAnother powerful optimization technique is speculative decoding, also called assisted generation. The key insight is that even with continuous batching, the token-by-token generation process fails to fully saturate the parallel processing capabilities of the accelerator. Speculative decoding aims to use this spare compute capacity to predict multiple tokens simultaneously, using a smaller proxy model (see Figure 8.3).\n\nFigure 8.3 – Illustration of traditional decoding (left) and speculative decoding (right)\n\nThe general approach is:\n\nApply a smaller model, like a distilled or pruned version of the main model, to predict multiple token completions in parallel. This could be 5–10 tokens predicted in a single step.",
      "content_length": 743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 965,
      "content": "Feed these speculative completions into the full model to validate which predictions match what the large model would have generated.\n\nRetain the longest matching prefix from the speculative completions and discard any incorrect tokens.\n\nThe result is that, if the small model approximates the large model well, multiple tokens can be generated in a single step. This avoids running the expensive large model for several iterations. The degree of speedup depends on the quality of the small model’s predictions – a 90% match could result in a 3–4X speedup.\n\nIt is crucial that both models use the same tokenizer. If this is not the case, the tokens generated by the draft model will not align with those produced by the large model, making them incompatible. Let’s implement this using the transformers library. In this example, we will use two Qwen1.5 models from Alibaba Cloud: a 1.8B version as the main model, and a 0.5B version as the draft model. Note that, if you have enough VRAM, you can use much larger models like 14B, 32B, 72B, or 110B as the main model.\n\nHere, we’re limited by the VRAM of the T4 GPU in Google Colab, but to get the maximum speedup, the assistant model should be much smaller than the large model.\n\nHere’s a step-by-step guide to implement speculative decoding:",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 966,
      "content": "We load the tokenizer and both models:\n\nimport\n\ntorch\n\nfrom\n\ntransformers\n\nimport\n\nAutoTokenizer, AutoModelForCausalLM model_id =\n\n\"Qwen/Qwen1.5-1.8B-Chat\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\n\n\"auto\"\n\n) draft_model = AutoModelForCausalLM.from_pretrained(\n\n\"Qwen/Qwen1.5-0.5B-Chat\"\n\n, device_map=\n\n\"auto\"\n\n)",
      "content_length": 390,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 967,
      "content": "We then tokenize the same input and store it in the accelerator, if available:\n\ndevice =\n\n\"cuda\"\n\nif\n\ntorch.cuda.is_available()\n\nelse\n\n\"cpu\"\n\ninputs = tokenizer(\n\n\"What is 2+2?\"\n\n, return_tensors=\n\n\"pt\"\n\n).to(device)\n\nWe can now use\n\nmodel.generate()\n\nwith the argument\n\nassistant_model",
      "content_length": 286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 968,
      "content": "to enable speculative decoding:\n\noutputs = model.generate(**inputs, do_sample=\n\nTrue\n\n, assistant_model=draft_model, temperature=\n\n0.7\n\n, max_new_tokens=\n\n64\n\n)\n\nprint\n\n(tokenizer.batch_decode(outputs, skip_special_tokens=\n\nTrue\n\n))\n\n[\n\n'What is 2+2? 2 + 2 equals 4!'\n\n]",
      "content_length": 270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 969,
      "content": "The speedup in this small example is not significant, but it is clearly noticeable with bigger models.\n\nPrompt lookup decoding is a variant of speculative decoding, tailored to input-grounded tasks like summarization where there is often overlap between the prompt and output. Shared n-grams are used as the LLM candidate tokens. We can enable prompt lookup decoding by using the\n\nprompt_lookup_num_tokens\n\nparameter in\n\nmodel.generate()\n\n:\n\noutputs = model.generate(**inputs, prompt_lookup_num_tokens=\n\n4\n\n)\n\nBy combining the static KV cache with torch.compile, implementing continuous batching, and leveraging speculative decoding techniques, LLMs can see inference speedups of 2–4x or more with no loss in quality.",
      "content_length": 717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 970,
      "content": "Another approach to creating a small proxy model consists of jointly fine- tuning a small model alongside a large model for maximum fidelity. A representative technique here is Medusa, which inserts dedicated speculation heads into the main model. The Medusa-1 approach fine-tunes these speculation heads while freezing the large model, while the Medusa-2 approach jointly fine-tunes both the speculation heads and the large model. The Medusa method has demonstrated impressive results, enabling a 70M parameter model to closely approximate the performance of a 7B parameter model on a range of tasks. Speculative decoding is natively supported by TGI.",
      "content_length": 652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 971,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 972,
      "content": "Optimized attention mechanisms\n\nThe Transformer architecture is based on the attention mechanism, which scales quadratically with the number of input tokens (or sequence length). This is particularly inefficient for longer sequences, where the size of the KV cache can blow up.\n\nIntroduced by Kwon, Li, et al. (2023), PagedAttention addresses these memory challenges by drawing inspiration from virtual memory and paging in operating systems. It partitions the KV cache into blocks, eliminating the need for contiguous memory allocation. Each block contains the keys and values for a fixed number of tokens. During attention computation, the PagedAttention kernel efficiently fetches these blocks, regardless of their physical memory location.\n\nThis partitioning allows for near-optimal memory utilization. This is useful for batching more sequences together, which increases throughput and GPU utilization. Moreover,\n\nPagedAttention\n\n's block-based approach naturally supports memory sharing across multiple output sequences generated from the same prompt. This is particularly advantageous in parallel sampling and beam search, where the same prompt is used to generate multiple outputs. The shared memory blocks reduce redundant computations and memory usage, cutting the memory overhead by up to 55% and improving throughput by up to 2.2x, according to the authors. The vLLM library received the first implementation of",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 973,
      "content": "PagedAttention. Since then, PagedAttention has also been implemented in TGI and TensorRT-LLM.\n\nAnother popular option is FlashAttention-2. Developed by Tri Dao (2023), it introduced several key innovations that are designed to address the quadratic runtime and memory constraints in traditional attention. By dividing input and output matrices into smaller blocks, FlashAttention-2 ensures that these blocks can fit into the GPU’s on-chip SRAM, which is much faster than high-bandwidth memory. This approach significantly reduces the frequency of data transfers between the GPU’s main memory and its processing units.\n\nThis is combined with online softmax, which computes the softmax function independently for each block of the attention scores matrix, rather than for the entire matrix at once. By maintaining a running maximum and a running sum of exponentials, FlashAttention-2 can calculate attention probabilities without needing to store large intermediate matrices.\n\nAdditionally, FlashAttention-2’s online softmax computation enables block- wise processing, maintaining accuracy while significantly reducing memory requirements. This is particularly important for training, where the recomputation of intermediate values (instead of storing them) in the backward pass reduces memory usage from quadratic to linear, in relation to sequence length.\n\nUnlike PagedAttention, FlashAttention-2 can easily be used with the transformers library through the\n\nattn_implementation",
      "content_length": 1478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 974,
      "content": "parameter:\n\nInstall the\n\nflash-attn\n\nlibrary with\n\n--no-build-isolation\n\nso that we don’t install the dependencies:\n\npip install flash-attn --no-build-isolation\n\nTo use FlashAttention-2 for inference, specify\n\nflash_attention_2\n\nin the\n\nattn_implementation\n\nparameter when loading a model. For example, this is how to load Mistral- 7B-Instruct-v0.3 with FlashAttention-2:",
      "content_length": 371,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 975,
      "content": "from\n\ntransformers\n\nimport\n\nAutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(\n\n\"mistralai/Mistral-7B-Instruct-v0.3\"\n\n, attn_implementation=\n\n\"flash_attention_2\"\n\n, )\n\nThe techniques presented in this section focused on improving the model’s efficiency in processing tokens. In the next section, we will discuss how to distribute our model and calculations across multiple GPUs.",
      "content_length": 394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 976,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 977,
      "content": "Model parallelism\n\nModel parallelism allows you to distribute the memory and compute requirements of LLMs across multiple GPUs. This enables the training and inference of models too large to fit on a single device, while also improving performance in terms of throughput (tokens per second).\n\nThere are three main approaches to model parallelism, each involving splitting the model weights and computation in different ways: data parallelism, pipeline parallelism, and tensor parallelism.\n\nAlthough these approaches were originally developed for training, we can reuse them for inference by focusing on the forward pass only.",
      "content_length": 625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 978,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 979,
      "content": "Data parallelism\n\nData parallelism (DP) is the simplest type of model parallelism. It involves making copies of the model and distributing these replicas across different GPUs (see Figure 8.4). Each GPU processes a subset of the data simultaneously. During training, the gradients calculated on each GPU are averaged and used to update the model parameters, ensuring that each replica remains synchronized. This approach is particularly beneficial when the batch size is too large to fit into a single machine or when aiming to speed up the training process.\n\nFigure 8.4 – Illustration of data parallelism with four GPUs\n\nDuring inference, DP can be useful for processing concurrent requests. By distributing the workload across multiple GPUs, this approach helps reduce latency, as multiple requests can be handled simultaneously. This",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 980,
      "content": "concurrent processing also increases throughput, since a higher number of requests can be processed at the same time.\n\nHowever, the effectiveness of DP is limited by the model size and the communication overhead between GPUs. Indeed, replicating the model’s parameters on each GPU is inefficient. This means that this technique only works when the model is small enough to fit into a single GPU, leaving less room for input data and thus limiting the batch size. For larger models or when memory is a constraint, this can be a significant drawback.\n\nTypically, DP is mainly used for training, while pipeline and tensor parallelism are preferred for inference.",
      "content_length": 659,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 981,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 982,
      "content": "Pipeline parallelism\n\nIntroduced by Huang et al. in the GPipe paper (2019), pipeline parallelism (PP) is a strategy for distributing the computational load of training and running large neural networks across multiple GPUs.\n\nUnlike traditional DP, which replicates the entire model on each GPU, pipeline parallelism partitions the model’s layers across different GPUs. This approach allows each GPU to handle a specific portion of the model, thereby reducing the memory burden on individual GPUs.\n\nFigure 8.5 – Illustration of pipeline parallelism with four GPUs\n\nAs shown in Figure 8.5, in a typical four-way pipeline parallel split, the model is divided into four segments, with each segment assigned to a different GPU. The first 25% of the model’s layers might be processed by GPU 1, the next 25% by GPU 2, and so on. During the forward pass,",
      "content_length": 846,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 983,
      "content": "activations are computed and then passed along to the next GPU. For training, the backward pass follows a similar sequence in reverse, with gradients being propagated back through the GPUs. The number of GPUs is often referred to as the degree of parallelism.\n\nThe primary advantage of pipeline parallelism is its ability to significantly reduce the memory requirements per GPU. However, this approach introduces new challenges, particularly related to the sequential nature of the pipeline. One of the main issues is the occurrence of “pipeline bubbles.” These bubbles arise when some GPUs are idle, waiting for activations from preceding layers. This idle time can reduce the overall efficiency of the process.\n\nMicro-batching was developed to mitigate the impact of pipeline bubbles. By splitting the input batch into smaller sub-batches, micro-batching ensures that GPUs remain busier, as the next sub-batch can begin processing before the previous one is fully completed.\n\nFigure 8.6 – Illustration of pipeline parallelism with micro-batching.\n\nFigure 8.6 shows an example of pipeline parallelism with micro-batching. In this example, the pipeline has four stages (F0, F1, F2, F3), and the input",
      "content_length": 1200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 984,
      "content": "batch is divided into four micro-batches. GPU 0 will process forward paths F0,0, F0,1, F0,2, and F0,3, sequentially. Once F0,0 is complete, GPU 1 can immediately start processing F1,0 and so on. After completing these forward passes, GPU 0 waits for the other GPUs to finish their respective forward computations before starting the backward paths (B0,3, B0,2, B0,1, and B0,0).\n\nPipeline parallelism is implemented in distributed training frameworks like Megatron-LM, DeepSpeed (ZeRO), and PyTorch through the dedicated Pipeline Parallelism for PyTorch (PiPPy) library. At the time of writing, only certain inference frameworks like TensorRT-LLM support pipeline parallelism.",
      "content_length": 675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 985,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 986,
      "content": "Tensor parallelism\n\nIntroduced by Shoeby, Patwary, Puri et al. in the Megatron-LM paper (2019), tensor parallelism (TP) is another popular technique to distribute the computation of LLM layers across multiple devices. In contrast to pipeline parallelism, TP splits the weight matrices found in individual layers. This enables simultaneous computations, significantly reducing memory bottlenecks and increasing processing speed.\n\nIn TP, large matrices, such as the weight matrices in MLPs or the attention heads in self-attention layers, are partitioned across several GPUs. Each GPU holds a portion of these matrices and performs computations on its respective slice.",
      "content_length": 667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 987,
      "content": "Figure 8.7 – Illustration of column-wise tensor parallelism in an MLP layer (W)\n\nFor instance, in an MLP layer, the weight matrix is divided so that each GPU processes only a subset of the weights (see Figure 8.7). The inputs are broadcast to all GPUs, which then independently compute their respective outputs. The partial results are then aggregated through an all-reduce operation, combining them to form the final output.\n\nIn the context of self-attention layers, TP is particularly efficient due to the inherent parallelism of attention heads. Each GPU can compute a subset of these heads independently, allowing the model to process large sequences more effectively. This makes TP more efficient than pipeline parallelism, which requires waiting for the completion of previous layers.",
      "content_length": 790,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 988,
      "content": "Despite its advantages, TP is not universally applicable to all layers of a neural network. Layers like LayerNorm and Dropout, which have dependencies spanning the entire input, cannot be efficiently partitioned and are typically replicated across devices instead. However, these operations can be split on the sequence dimension of the input instead (sequence parallelism). Different GPUs can compute these layers on different slices of the input sequence, avoiding replication of weights. This technique is limited to a few specific layers, but it can provide additional memory savings, especially for very large input sequence lengths.\n\nMoreover, TP necessitates high-speed interconnects between devices to minimize communication overhead, making it impractical to implement across nodes with insufficient interconnect bandwidth.\n\nTP is also implemented in distributed training frameworks like Megatron- LM, DeepSpeed (ZeRO), and PyTorch (FSDP). It is available in most inference frameworks, like TGI, vLLM, and TensorRT-LLM.",
      "content_length": 1028,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 989,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 990,
      "content": "Combining approaches\n\nData, tensor, and pipeline parallelisms are orthogonal techniques that can be combined. Figure 8.8 illustrates how a given model can be split according to each approach:\n\nFigure 8.8 – Illustration of the different model parallelism techniques\n\nCombining these techniques can mitigate their respective issues. Pipeline parallelism provides the greatest memory reduction but sacrifices efficiency, due to pipeline bubbles. This may be ideal if the primary constraint fits the model in the GPU memory. In contrast, if low latency is paramount, then prioritizing tensor parallelism and accepting a larger memory footprint may be the better trade-off. In practice, a model may be split depth-wise into a few pipeline stages, with tensor parallelism used within each stage.",
      "content_length": 789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 991,
      "content": "Balancing these tradeoffs and mapping a given model architecture onto available hardware accelerators is a key challenge in deploying LLMs.",
      "content_length": 139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 992,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 993,
      "content": "Model quantization\n\nQuantization refers to the process of representing the weights and activations of a neural network using lower-precision data types. In the context of LLMs, quantization primarily focuses on reducing the precision of the model’s weights and activations.\n\nBy default, weights are typically stored in a 16-bit or 32-bit floating-point format (FP16 or FP32), which provides high precision but comes at the cost of increased memory usage and computational complexity. Quantization is a solution to reduce the memory footprint and accelerate the inference of LLMs.\n\nIn addition to these benefits, larger models with over 30 billion parameters can outperform smaller models (7B–13B LLMs) in terms of quality when quantized to 2- or 3-bit precision. This means they can achieve superior performance while maintaining a comparable memory footprint.\n\nIn this section, we will introduce the concepts of quantization, GGUF with\n\nllama.cpp\n\n, GPTQ, and EXL2, along with an overview of additional techniques. In addition to the code provided in this section, you can refer to AutoQuant (bit.ly/autoquant) to quantize their models using a Google Colab notebook.",
      "content_length": 1167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 994,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 995,
      "content": "Introduction to quantization\n\nThere are two main approaches to weight quantization: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ is a straightforward technique where the weights of a pre-trained model are directly converted to a lower precision format without any retraining. While PTQ is easy to implement, it may result in some performance degradation. Conversely, QAT performs quantization during the training or fine-tuning stage, allowing the model to adapt to the lower precision weights. QAT often yields better performance compared to PTQ but requires additional computational resources and representative training data.\n\nThe choice of data type plays a crucial role in quantization. Floating- point numbers, such as FP32, FP16 (half-precision), and BF16 (brain floating-point), are commonly used in deep learning. These formats allocate a fixed number of bits to represent the\n\nsign\n\n,\n\nexponent\n\n, and\n\nsignificand\n\n(mantissa) of a number.",
      "content_length": 980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 996,
      "content": "Figure 8.9 – Comparison the between FP32, FP16, and BF16 formats\n\nA sign of 0 represents a positive number, while 1 indicates a negative number. Conversely, the exponent controls the range that is represented (big or small). Finally, the significand controls the precision of the number (the number of digits). The formula used to convert these representations into real numbers is:",
      "content_length": 382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 997,
      "content": "The data types shown in Figure 7.7 display different tradeoffs, as illustrated with different representations of ). FP32 uses 32 bits, ( providing high precision but also requiring more memory. Conversely, FP16 and BF16 use 16 bits, lowering the memory footprint at the cost of a lower precision. In general, neural networks prefer a bigger range than better precision, which is why BF16 is the most popular data type when the hardware supports it. For example, NVIDIA’s Ampere architecture (A100, A30, etc.) supports BF16, but previous generations like Turing (T4, T40, etc.) do not.\n\nHowever, we are not restricted to these three data types. Lower-precision data types, such as INT8 (8-bit integers), can be employed for quantization, further reducing the memory footprint. Naïve quantization techniques, such as absolute maximum (absmax) quantization and zero-point quantization, can be applied to convert\n\nFP32\n\n,\n\nFP16\n\n, or\n\nBF16\n\nweights to\n\nINT8\n\n, as illustrated in Figure 8.10:",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 998,
      "content": "Figure 8.10 – Quantization of 0.1 in a [-3.0, 3.2] range with absmax quantization and zero-point quantization\n\nAbsmax quantization maps the original weights by dividing them by the absolute maximum value of\n\nto the range [-127, 127]\n\nand scaling them:\n\nFor example, if our absolute maximum value is 3.2 (see Figure 8.8), a\n\nweight of 0.1 would be quantized to the inverse operation:\n\n. To dequantize it, we do\n\nThis means that if we dequantize our weight, we obtain can see a rounding error of implement it as follows with the PyTorch library:\n\nin this example. In Python, we can\n\n. We",
      "content_length": 585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 999,
      "content": "import\n\ntorch\n\ndef\n\nabsmax_quantize\n\n(\n\nX\n\n):\n\n# Calculate scale\n\nscale =\n\n127\n\n/ torch.\n\nmax\n\n(torch.\n\nabs\n\n(X))\n\n# Quantize\n\nX_quant = (scale * X).\n\nround",
      "content_length": 156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1000,
      "content": "()\n\nreturn\n\nX_quant.to(torch.int8)\n\nZero-point quantization, on the other hand, considers asymmetric input distributions and maps the weights introducing a zero-point offset:\n\nto the range [-128, 127] by\n\nWhere\n\nand\n\n.\n\nIf we take the same example with a weight of 0.1, we get a scale of\n\nand a zero-point value of\n\n. The\n\nweight of 0.1 would be quantized to\n\n, unlike the value of\n\nprovided by absmax.\n\nWe can easily get the dequantization by applying the inverse operation:",
      "content_length": 475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1001,
      "content": "In Python, zero-point quantization can be implemented as follows:\n\ndef\n\nzeropoint_quantize\n\n(\n\nX\n\n):\n\n# Calculate value range (denominator)\n\nx_range = torch.\n\nmax\n\n(X) - torch.\n\nmin\n\n(X) x_range =\n\n1\n\nif\n\nx_range ==\n\n0",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1002,
      "content": "else\n\nx_range\n\n# Calculate scale\n\nscale =\n\n255\n\n/ x_range\n\n# Shift by zero-point\n\nzeropoint = (-scale * torch.\n\nmin\n\n(X) -\n\n128\n\n).\n\nround\n\n()\n\n# Scale and round the inputs\n\nX_quant = torch.clip((X * scale + zeropoint).\n\nround\n\n(), -\n\n128\n\n,",
      "content_length": 241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1003,
      "content": "127\n\n)\n\nreturn\n\nX_quant.to(torch.int8)\n\nHowever, naïve quantization methods have limitations, particularly when dealing with outlier features in LLMs. Outlier features are extreme weight values (about 0.1% of total values) that can significantly impact the quantization process, leading to reduced precision for other values.\n\nDiscarding these outliers is not feasible, as it would degrade a model’s performance. You can see an example of outliers in Figure 8.11:\n\nFigure 8.11 – Example of outliers in a weight matrix",
      "content_length": 517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1004,
      "content": "To address the outlier problem, more advanced quantization techniques have been proposed. One notable example is\n\nLLM.int8()\n\n, introduced by Dettmers et al. (2022).\n\nLLM.int8()\n\nemploys a mixed-precision quantization scheme, where outlier features are processed using FP16, while the remaining values are quantized to INT8. This approach effectively reduces the memory footprint of LLMs by nearly 2x while minimizing performance degradation.\n\nLLM.int8()\n\nworks by performing matrix multiplication in three steps. First, it extracts columns containing outlier features from the input hidden states using a custom threshold. Second, it performs separate matrix multiplications for the outliers (in\n\nFP16\n\n) and non-outliers (in\n\nINT8\n\n) using vector-wise quantization. Finally, it dequantizes the non-outlier results and combines them with the outlier results to obtain the final output in FP16.\n\nThe effectiveness of",
      "content_length": 916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1005,
      "content": "LLM.int8()\n\nhas been demonstrated empirically, showing negligible performance degradation (<1%) compared to the original\n\nFP32\n\nmodels. However, it does introduce an additional computational overhead, resulting in around 20% slower inference for large models. Models can be directly loaded in 8-bit precision with the transformer library, using\n\nLLM.int8()\n\n, as follows:\n\nfrom\n\ntransformers\n\nimport\n\nAutoModelForCausalLM model_name =\n\n\"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\n\n\"\n\nauto\"\n\n, load_in_8bit=\n\nTrue",
      "content_length": 576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1006,
      "content": ")\n\nIntroduced by Dettmers et al. (2023), NF4 is a 4-bit precision format designed for QLoRA (discussed in Chapter 5). It is also integrated into the transformers library but requires the bitsandbytes library as a dependency. To load a model in NF4 (4-bit precision), you can use the\n\nload_in_4bit\n\nparameter, as follows:\n\nfrom\n\ntransformers\n\nimport\n\nAutoModelForCausalLM model_name =\n\n\"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\n\n\"auto\"\n\n, load_in_4bit=\n\nTrue\n\n)",
      "content_length": 526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1007,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1008,
      "content": "Quantization with GGUF and llama.cpp\n\nThe llama.cpp project is an open-source C++ software library created by Georgi Gerganov, designed to perform inference with various LLMs. It is the most popular quantization technique, with many quantized models available on the Hugging Face Hub.\n\nCompared to other libraries that rely on hardware-specific closed-source libraries like CUDA, llama.cpp can run on a broader range of hardware. It has gained significant popularity, particularly among users without specialized hardware, as it can operate on CPUs and Android devices. Moreover, llama.cpp can also offload layers to the GPU, accelerating inference speed. It is compatible with different inference optimization techniques, such as FlashAttention-2 and speculative decoding.\n\nThis project features its own quantization format, GGUF, designed to simplify and speed up model loading. GGUF files store tensors and metadata, supporting various formats, from 1-bit to 8-bit precision. It follows a naming convention based on the number of bits used and specific variants, such as:\n\nIQ1_S\n\nand\n\nIQ1_M\n\n: 1-bit precision – very low quality",
      "content_length": 1131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1009,
      "content": "IQ2_XXS/XS/S/M\n\nand\n\nQ2_K\n\n: 2-bit precision – generally low quality but IQ2 can be usable for large models\n\nIQ3_XXS/XS/S/M\n\nand\n\nQ3_K_S/M/L\n\n: 3-bit precision – low quality but usable for large models\n\nIQ4_XS/NL\n\nand\n\nQ4_K_S/M, Q4_0/1\n\n: 4-bit precision – good quality and usable for most models\n\nQ5_K_S/M\n\nand\n\nQ5_0/1\n\n: 5-bit precision – high quality",
      "content_length": 353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1010,
      "content": "Q6_K\n\n: 6-bit precision –very high quality\n\nQ8_0\n\n: 8-bit precision – highest quality\n\nTo provide a brief overview of GGUF quantization, llama.cpp groups values into blocks and rounds them to a lower precision. For instance, the legacy Q4_0 format handles 32 values per block, scaling and quantizing them based on the largest weight value in the block ( In Q4_1, the smallest Lvalue in the block is also added (\n\n). In Q4_K, weights are divided into\n\nsuper-blocks, containing 8 blocks with 32 values. Block scales and minimum values are also quantized in higher precision with 6 bits (\n\n). Finally, i-quants like IQ4_XS are inspired by another quantization technique called QuIP#. This ensures an even number of positive (or negative) quant signs in groups of eight and implements the store their magnitude.\n\nlattice to\n\nHere is a practical example of how to quantize a model in the GGUF format. The following steps can be executed on a free T4 GPU in Google Colab:\n\n).",
      "content_length": 969,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1011,
      "content": "Install llama.cpp and the required libraries:\n\n!git clone https://github.com/ggerganov/llama.cpp !cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make !pip install -r llama.cpp/requirements.txt\n\nDownload the model to convert. We will provide the model ID from the Hugging Face Hub – for example,\n\nmistralai/Mistral-7B-Instruct-v0.2\n\n:\n\nMODEL_ID = \"mlabonne/EvolCodeLlama-7b\" MODEL_NAME = MODEL_ID.split('/')[-1] !git lfs install !git clone https://huggingface.co/{MODEL_ID}\n\nFirst, we convert the model into FP16. This is an intermediary artifact that will be used for every GGUF quantization type. Note that different",
      "content_length": 627,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1012,
      "content": "conversion scripts exist in llama.cpp and are compatible with different models:\n\nfp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\" !python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n\nWe select a format (here,\n\nQ4_K_M\n\n) and start the quantization. This process can take an hour on a T4 GPU:\n\nMETHOD = \"q4_k_m\" qtype = f\" {MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\" !./llama.cpp/quantize {fp16} {qtype} {METHOD}\n\nOnce it’s done, your quantized model is ready. You can download it locally, or upload it to the Hugging Face Hub using the following code:",
      "content_length": 588,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1013,
      "content": "from\n\nhuggingface_hub\n\nimport\n\ncreate_repo, HfApi hf_token =\n\n\"\"\n\n# Specify your token\n\nusername =\n\n\"\"\n\n# Specify your username\n\napi = HfApi()\n\n# Create empty repo\n\ncreate_repo( repo_id =\n\nf\"\n\n{username}\n\n/\n\n{MODEL_NAME}\n\nGGUF\"\n\n, repo_type=\n\n\"model\"\n\n, exist_ok=",
      "content_length": 263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1014,
      "content": "True\n\n, token=hf_token )\n\n# Upload gguf files\n\napi.upload_folder( folder_path=MODEL_NAME, repo_id=\n\nf\"\n\n{username}\n\n/\n\n{MODEL_NAME}\n\nGGUF\"\n\n, allow_patterns=\n\nf\"*.gguf\"\n\n, token=hf_token )\n\nGGUF models can be used with backends such as llama-cpp-python and frameworks like LangChain. This is useful if you want to integrate a quantized model into a broader system. You can also directly chat with the model using frontends, like llama.cpp’s lightweight server, LM Studio, and the Text Generation Web UI. These tools enable easy interaction with the GGUF models, providing an experience similar to ChatGPT.",
      "content_length": 605,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1015,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1016,
      "content": "Quantization with GPTQ and EXL2\n\nWhile GGUF and llama.cpp offer CPU inference with GPU offloading, GPTQ and EXL2 are two quantization formats dedicated to GPUs. This makes them both faster than llama.cpp during inference. In particular, EXL2 offers the highest throughput with its dedicated library, ExLlamaV2.\n\nGPTQ and EXL2 quants are based on the GPTQ algorithm, introduced by Frantar et al. (2023). It optimizes weight quantization for LLMs by refining the Optimal Brain Quantization (OBQ) approach to handle extensive matrices efficiently. It begins with a Cholesky decomposition of the Hessian inverse, ensuring numerical stability. Instead of quantizing weights in a strict order, GPTQ processes them in batches, updating columns and associated blocks iteratively. This method leverages lazy batch updates, reducing computational redundancy and memory bottlenecks.\n\nWhile GPTQ is limited to 4-bit precision, EXL2 offers more flexibility with a highly customizable precision that can mix different quantization levels. This allows for precise bitrates between 2 and 8 bits per weight, such as\n\n2.3\n\n,\n\n3.5\n\n, or\n\n6.0",
      "content_length": 1122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1017,
      "content": ". It can also apply multiple quantization levels to each linear layer, prioritizing more important weights with higher bit quantization. Parameters are selected automatically, by quantizing each matrix multiple times and choosing a combination that minimizes the quantization error while meeting a target bitrate. In practice, this allows 70B models to run on a single 24 GB GPU with 2.55-bit precision.\n\nThe inference itself is handled by the ExLlamaV2 library, which supports both the GPTQ and EXL2 models.\n\nIn the following example, let’s quantize a model in the EXL2 format using ExLlamaV2. These steps can be executed on a free T4 GPU in Google Colab:\n\nInstall the ExLlamaV2 library from source:\n\n!git clone https://github.com/turboderp/exllamav2 !pip install -e exllamav2\n\nWe download the model to quantize by cloning its repo from the Hugging Face Hub:",
      "content_length": 859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1018,
      "content": "MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\" MODEL_NAME = MODEL_ID.split('/')[-1] !git lfs install !git clone https://huggingface.co/{MODEL_ID}\n\nDownload the calibration dataset used to measure the quantization error. In this case, we will use WikiText-103, a standard calibration dataset with high-quality articles from Wikipedia:\n\n!wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2 883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet\n\nQuantize the model at a given precision (for example, 4.5):\n\n!mkdir quant !python exllamav2/convert.py \\ -i {MODEL_NAME} \\ -o quant \\ -c wikitext-test.parquet \\ -b 4.5",
      "content_length": 634,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1019,
      "content": "The quantized model can then be uploaded to the Hugging Face Hub, as seen previously.\n\nGPTQ and EXL2 quants are not as widely supported as GGUF. For example, frontends like LM Studio do not currently integrate them. You can use other tools instead, like oobabooga’s Text Generation Web UI. It is also directly integrated into the transformers library and supported by TGI. GPTQ models are also supported in TensorRT-LLM.\n\nWhile less popular than GGUF, you can find a lot of GPTQ and EXL2 models on the Hugging Face Hub.",
      "content_length": 519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1020,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1021,
      "content": "Other quantization techniques\n\nThere is a variety of quantization techniques beyond GGUF, GPTQ, and EXL2. This subsection will briefly introduce Activate-aware Weight Quantization (AWQ) as well as extreme quantization techniques, like QuIP# (Quantization with Incoherence Processing) and HQQ (Half-Quadratic Quantization).\n\nIntroduced by Lin et al. (2023), AWQ is another popular quantization algorithm. It identifies and protects the most important weights, which are determined based on activation magnitude instead of weight magnitude. This approach involves applying optimal per-channel scaling to these salient weights, without relying on backpropagation or reconstruction, ensuring that the LLM does not overfit the calibration set. While it relies on a different paradigm, AWQ is quite close to the GPTQ and EXL2 versions, although slightly slower. They are well-supported by inference engines and integrated into TGI, vLLM, and TensorRT-LLM.\n\nAn interesting trend is the quantization of models into 1- or 2-bit precision. While some formats, like EXL2, allow extreme quantization, the quality of the models often suffers significantly. However, recent algorithms like QuIP# and HQQ have targeted this regime and offer quantization methods that better preserve the performance of the original models. This is particularly true for large models (over 30B parameters), which can end up taking less space than 7B or 13B parameter models while providing higher- quality outputs.",
      "content_length": 1481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1022,
      "content": "This trend is expected to continue, further optimizing these quantization methods.\n\nTo conclude this chapter, here is a table summarizing the features of the three main inference engines we covered in the previous sections:\n\nTechnique\n\nTGI\n\nvLLM\n\nTensorRT-LLM\n\nContinuous batching ✓\n\n✓\n\n✓\n\nSpeculative decoding ✓\n\nFlashAttention2\n\n✓\n\n✓\n\n✓\n\nPagedAttention\n\n✓\n\n✓\n\n✓\n\nPipeline parallelism\n\n✓\n\nTensor parallelism\n\n✓\n\n✓\n\n✓\n\nGPTQ\n\n✓\n\n✓\n\nEXL2\n\n✓\n\nAWQ\n\n✓\n\n✓\n\n✓\n\nTable 8.1 – Summary of features for TGI, vLLM, and TensorRT-LLM",
      "content_length": 517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1023,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1024,
      "content": "Summary\n\nIn summary, inference optimization is a critical aspect of deploying LLMs effectively. This chapter explored various optimization techniques, including optimized generation methods, model parallelism, and weight quantization. Significant speedups can be achieved by leveraging techniques like predicting multiple tokens in parallel with speculative decoding, or using an optimized attention mechanism with FlashAttention- 2. Additionally, we discussed how model parallelism methods, including data, pipeline, and tensor parallelism, distribute the computational load across multiple GPUs to increase throughput and reduce latency. Weight quantization, with formats like GGUF and EXL2, further reduces the memory footprint and accelerates inference, with some calculated tradeoff in output quality.\n\nUnderstanding and applying these optimization strategies are essential for achieving high performance in practical applications of LLMs, such as chatbots and code completion. The choice of techniques and tools depends on specific requirements, including available hardware, desired latency, and throughput. By combining various approaches, such as continuous batching and speculative decoding, along with advanced attention mechanisms and model parallelism, users can tailor their deployment strategies to maximize efficiency.\n\nWay back in Chapter 4, we focused only on implementing the ingestion pipeline, which is just one component of a standard RAG application. In the next chapter, we will conclude the RAG system by implementing the retrieval and generation components and integrating them into the inference pipeline.",
      "content_length": 1632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1025,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1026,
      "content": "References\n\nHugging Face, Text Generation Inference, https://github.com/huggingface/text-generation-inference, 2022.\n\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Efficient Memory Management for Large Language Model Serving with PagedAttention, 2023.\n\nNvidia, TensorRT-LLM, https://github.com/NVIDIA/TensorRT-LLM, 2023.\n\nY. Leviathan, M. Kalman, Y. Matias, Fast Inference from Transformers via Speculative Decoding, 2023.\n\nT. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, T. Dao, Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads, 2024.\n\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H. Zhang, I. Stoica, Efficient Memory Management for Large Language Model Serving with PagedAttention, 2023.",
      "content_length": 798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1027,
      "content": "R.Y. Aminabadi, S. Rajbhandari, M. Zhang, A.A. Awan, C. Li, D. Li, E. Zheng, J. Rasley, S. Smith, O. Ruwase, Y. He, DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale, 2022.\n\nY. Huang, Y. Cheng, A. Bapna, O. Firat, M.X. Chen, D. Chen, H. Lee, J. Ngiam, Q.V. Le, Y. Wu, Z. Chen, GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism, 2019.\n\nK. James Reed, PiPPy: Pipeline Parallelism for PyTorch, https://github.com/pytorch/PiPPy, 2022.\n\nM. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism, 2020.\n\nVerma and Vaidya, Mastering LLM Techniques: Inference Optimization, NVIDIA Developer Technical Blog, https://developer.nvidia.com/blog/mastering-llm-techniques-inference- optimization/, 2023.\n\nT. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale, 2022.\n\nG. Gerganov, llama.cpp, https://github.com/ggerganov/llama.cpp, 2023.",
      "content_length": 1058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1028,
      "content": "E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, GPTQ: Accurate Post- Training Quantization for Generative Pre-trained Transformers, 2023.\n\nTuboderp, exllamav2, https://github.com/turboderp/exllamav2, 2023.\n\nJ. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, S. Han, AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, 2024.\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1029,
      "content": "9",
      "content_length": 1,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1030,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1031,
      "content": "RAG Inference Pipeline\n\nBack in Chapter 4, we implemented the retrieval-augmented generation (RAG) feature pipeline to populate the vector database (DB). Within the feature pipeline, we gathered data from the data warehouse, cleaned, chunked, and embedded the documents, and, ultimately, loaded them to the vector DB. Thus, at this point, the vector DB is filled with documents and ready to be used for RAG.\n\nBased on the RAG methodology, you can split your software architecture into three modules: one for retrieval, one to augment the prompt, and one to generate the answer. We will follow a similar pattern by implementing a retrieval module to query the vector DB. Within this module, we will implement advanced RAG techniques to optimize the search. Afterward, we won’t dedicate a whole module to augmenting the prompt, as that would be overengineering, which we try to avoid. However, we will write an inference service that inputs the user query and context, builds the prompt, and calls the LLM to generate the answer. To summarize, we will implement two core Python modules, one for retrieval and one for calling the LLM using the user’s input and context as input. When we glue these together, we will have an end-to-end RAG flow.\n\nIn Chapters 5 and 6, we fine-tuned our LLM Twin model, and in Chapter 8, we learned how to optimize it for inference. Thus, at this point, the LLM is ready for production. What is left is to build and deploy the two modules described above.",
      "content_length": 1483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1032,
      "content": "We will dedicate the next chapter entirely to deploying our fine-tuned LLM Twin model to AWS SageMaker, as an AWS SageMaker inference endpoint. Thus, the focus of this chapter is to dig into the advanced RAG retrieval module implementation. We have dedicated a whole chapter to the retrieval step because this is where the magic happens in an RAG system. At the retrieval step (and not when calling the LLM), you write most of the RAG inference code. This step is where you have to wrangle your data to ensure that you retrieve the most relevant data points from the vector DB. Hence, most of the advanced RAG logic goes within the retrieval step.\n\nTo sum up, in this chapter, we will cover the following topics:\n\nUnderstanding the LLM Twin’s RAG inference pipeline\n\nExploring the LLM Twin’s advanced RAG techniques\n\nImplementing the LLM Twin’s RAG inference pipeline\n\nBy the end of this chapter, you will know how to implement an advanced RAG retrieval module, augment a prompt using the retrieved context, and call an LLM to generate the final answer. Ultimately, you will know how to build a production-ready RAG inference pipeline end to end.",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1033,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1034,
      "content": "Understanding the LLM Twin’s RAG inference pipeline\n\nBefore implementing the RAG inference pipeline, we want to discuss its software architecture and advanced RAG techniques. Figure 9.1 illustrates an overview of the RAG inference flow. The inference pipeline starts with the input query, retrieves the context using the retrieval module (based on the query), and calls the LLM SageMaker service to generate the final answer.",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1035,
      "content": "Figure 9.1: RAG inference pipeline architecture",
      "content_length": 47,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1036,
      "content": "The feature pipeline and the retrieval module, defined in Figure 9.1, are independent processes. The feature pipeline runs on a different machine on a schedule to populate the vector DB. At the same time, the retrieval module is called on demand, within the inference pipeline, on every user request.\n\nBy separating concerns between the two components, the vector DB is always populated with the latest data, ensuring feature freshness, while the retrieval module can access the latest features on every request. The input of the RAG retrieval module is the user’s query, based on which we have to return the most relevant and similar data points from the vector DB, which will be used to guide the LLM in generating the final answer.\n\nTo fully understand the dynamics of the RAG inference pipeline, let’s go through the architecture flow from Figure 9.1 step by step:\n\nUser query:We begin with the user who makes a query, such as “Write an article about...”\n\nQuery expansion:We expand the initial query to generate multiple queries that reflect different aspects or interpretations of the original user query. Thus, instead of one query, we will use xN queries. By diversifying the search terms, the retrieval module increases the likelihood of capturing a comprehensive set of relevant data points. This step is crucial when the original query is too narrow or vague.",
      "content_length": 1369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1037,
      "content": "Self-querying: We extract useful metadata from the original query, such as the author’s name. The extracted metadata will be used as filters for the vector search operation, eliminating redundant data points from the query vector space (making the search more accurate and faster).\n\nFiltered vector search: We embed each query and perform a similarity search to find each search’s top K data points. We execute xN searches corresponding to the number of expanded queries. We call this step a filtered vector search as we leverage the metadata extracted from the self-query step as query filters.\n\nCollecting results:We get up to xK results closest to its specific expanded query interpretation for each search operation. Further, we aggregate the results of all the xN searches, ending up with a list of N x K results containing a mix of articles, posts, and repositories chunks. The results include a broader set of potentially relevant chunks, offering multiple relevant angles based on the original query’s different facets.\n\nReranking:To keep only the top K most relevant results from the list of N x K potential items, we must filter the list further. We will use a reranking algorithm that scores each chunk based on the relevance and importance relative to the initial user query.We will leverage a neural cross-encoder model to compute the score, a value between 0 and 1, where 1 means the result is entirely relevant to the query. Ultimately, we sort the N x K results based on the score and pick the top K items. Thus, the output is a ranked list of K chunks, with the most relevant data points situated at the top.\n\nBuild the prompt and call the LLM:We map the final list of the most relevant K chunks to a string used to build the final prompt. We create",
      "content_length": 1766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1038,
      "content": "the prompt using a prompt template, the retrieved context, and the user’s query. Ultimately, the augmented prompt is sent to the LLM (hosted on AWS SageMaker exposed as an API endpoint).\n\nAnswer: We are waiting for the answer to be generated. After the LLM processes the prompt, the RAG logic finishes by sending the generated response to the user.\n\nThat wraps up the overview of the RAG inference pipeline. Now, let’s dig deeper into the details.",
      "content_length": 447,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1039,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1040,
      "content": "Exploring the LLM Twin’s advanced RAG techniques\n\nNow that we understand the overall flow of our RAG inference pipeline, let’s explore the advanced RAG techniques we used in our retrieval module:\n\nPre-retrieval step: Query expansion and self-querying\n\nRetrieval step: Filtered vector search\n\nPost-retrieval step: Reranking\n\nBefore digging into each method individually, let’s lay down the Python interfaces we will use in this section, which are available at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/rag/base.py.\n\nThe first is a prompt template factory that standardizes how we instantiate prompt templates. As an interface, it inherits from\n\nABC\n\nand exposes the",
      "content_length": 719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1041,
      "content": "create_template()\n\nmethod, which returns a LangChain\n\nPromptTemplate\n\ninstance. Even if we avoid being heavily reliant on LangChain, as we want to implement everything ourselves to understand the engineering behind the scenes, some objects, such as the\n\nPromptTemplate\n\nclass, are helpful to speed up the development without hiding too much functionality:\n\nfrom\n\nabc\n\nimport\n\nABC, abstractmethod\n\nfrom\n\nlangchain.prompts\n\nimport\n\nPromptTemplate\n\nfrom\n\npydantic",
      "content_length": 460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1042,
      "content": "import\n\nBaseModel\n\nclass\n\nPromptTemplateFactory\n\n(ABC, BaseModel):\n\n@abstractmethod\n\ndef\n\ncreate_template\n\n(\n\nself\n\n) -> PromptTemplate:\n\npass\n\nWe also want to define a\n\nRAGStep\n\ninterface used to standardize the interface of advanced RAG steps such as query expansion and self-querying. As these steps are often dependent on other LLMs, it has a\n\nmock\n\nattribute to reduce costs and debugging time during development:",
      "content_length": 418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1043,
      "content": "from\n\ntyping\n\nimport\n\nAny\n\nfrom\n\nllm_engineering.domain.queries\n\nimport\n\nQuery\n\nclass\n\nRAGStep\n\n(\n\nABC\n\n):\n\ndef\n\n__init__\n\n(\n\nself, mock:\n\nbool",
      "content_length": 143,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1044,
      "content": "=\n\nFalse\n\n) ->\n\nNone\n\n: self._mock = mock\n\n@abstractmethod\n\ndef\n\ngenerate\n\n(\n\nself, query: Query, *args, **kwargs\n\n) ->\n\nAny\n\n:\n\npass\n\nUltimately, we must understand how we modeled the\n\nQuery\n\ndomain entity to wrap the user’s input with other metadata required for advanced RAG. Thus, let’s look at its implementation. First, we import the necessary classes:",
      "content_length": 358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1045,
      "content": "from\n\npydantic\n\nimport\n\nUUID4, Field\n\nfrom\n\nllm_engineering.domain.base\n\nimport\n\nVectorBaseDocument\n\nfrom\n\nllm_engineering.domain.types\n\nimport\n\nDataCategory\n\nNext, we define the\n\nQuery\n\nentity class, which inherits from the\n\nVectorBaseDocument",
      "content_length": 244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1046,
      "content": "object-vector mapping (OVM) class, discussed in Chapter 4. Thus, each query can easily be saved or retrieved from the vector DB:\n\nclass\n\nQuery\n\n(\n\nVectorBaseDocument\n\n): content:\n\nstr\n\nauthor_id: UUID4 |\n\nNone\n\n=\n\nNone\n\nauthor_full_name:\n\nstr\n\n|\n\nNone\n\n=\n\nNone",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1047,
      "content": "metadata:\n\ndict\n\n= Field(default_factory=\n\ndict\n\n)\n\nclass\n\nConfig\n\n: category = DataCategory.QUERIES\n\nWhat is essential to notice are the class’s attributes used to combine the user’s query with a bunch of metadata fields:\n\ncontent\n\n: A string containing input query.\n\nauthor_id\n\n: An optional UUID4 identifier extracted from the query used as a filter within the vector search operation to retrieve chunks written only by a specific author",
      "content_length": 440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1048,
      "content": "author_full_name\n\n: An optional string used to query the\n\nauthor_id\n\nmetadata\n\n: A dictionary for any additional metadata, initialized as an empty\n\ndict\n\nby default\n\nBesides the standard definition of a domain class, we also define a\n\nfrom_str()\n\nclass method to create a\n\nQuery\n\ninstance directly from a string. This allows us to standardize how we clean the query string before constructing the\n\nquery\n\nobject, such as stripping any leading or trailing whitespace and newline characters:",
      "content_length": 489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1049,
      "content": "@classmethod\n\ndef\n\nfrom_str\n\n(\n\ncls, query:\n\nstr\n\n) ->\n\n\"Query\"\n\n:\n\nreturn\n\nQuery(content=query.strip(\n\n\"\\n \"\n\n))\n\nAdditionally, there’s an instance method called\n\nreplace_content()\n\nused to create a new\n\nQuery\n\ninstance with updated content while retaining the original query’s",
      "content_length": 278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1050,
      "content": "id\n\n,\n\nauthor_id\n\n,\n\nauthor_full_name\n\n, and\n\nmetadata\n\n:\n\ndef\n\nreplace_content\n\n(\n\nself, new_content:\n\nstr\n\n) ->\n\n\"Query\"\n\n:\n\nreturn\n\nQuery(",
      "content_length": 141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1051,
      "content": "id\n\n=self.\n\nid\n\n, content=new_content, author_id=self.author_id, author_full_name=self.author_full_name, metadata=self.metadata, )\n\nThis can be particularly useful when modifying the query text, for example, during preprocessing or normalization, without losing the associated metadata or identifiers. Following the\n\nQuery\n\nclass, we define the\n\nEmbeddedQuery\n\nclass:\n\nclass\n\nEmbeddedQuery\n\n(\n\nQuery\n\n): embedding:",
      "content_length": 414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1052,
      "content": "list\n\n[\n\nfloat\n\n]\n\nclass\n\nConfig\n\n: category = DataCategory.QUERIES\n\nThe\n\nEmbeddedQuery\n\nclass extends\n\nQuery\n\nby adding the embedding field. The\n\nEmbeddedQuery\n\nentity encapsulates all the data and metadata necessary to perform vector search operations on top of Qdrant (or another vector DB).\n\nNow that we understand all the interfaces and new domain entities used within the RAG inference pipeline, let’s move on to our advanced RAG pre-retrieval optimization techniques.",
      "content_length": 474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1053,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1054,
      "content": "Advanced RAG pre-retrieval optimizations: query expansion and self-querying\n\nWe implemented two methods to optimize the pre-retrieval optimization step: query expansion and self-querying. The two methods work closely with the filtered vector search step, which we will touch on in the next section. For now, however, we will start with understanding the code for query expansion and move to implementing self-querying.\n\nWithin these two methods, we will leverage OpenAI’s API to generate variations of the original query within the query expansion step and to extract the necessary metadata within the self-querying algorithm. When we wrote this book, we used\n\nGPT-4o-mini\n\nin all our examples, but as OpenAI’s models quickly evolve, the model might get deprecated. But that’s not an issue, as you can quickly change it in your\n\n.env\n\nfile by configuring the\n\nOPENAI_MODEL_ID\n\nenvironment variable.",
      "content_length": 898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1055,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1056,
      "content": "Query expansion\n\nThe problem in a typical retrieval step is that you query your vector DB using a single vector representation of your original question. This approach covers only a small area of the embedding space, which can be limiting. If the embedding doesn’t contain all the required information or nuances of your query, the retrieved context may not be relevant. This means essential documents that are semantically related but not near the query vector might be overlooked.\n\nThe solution is based on query expansion, which offers a way to overcome this limitation. Using an LLM to generate multiple queries based on your initial question, you create various perspectives that capture different facets of your query. These expanded queries, when embedded, target other areas of the embedding space that are still relevant to your original question. This increases the likelihood of retrieving more relevant documents from the vector DB.\n\nImplementing query expansion can be as straightforward as crafting a detailed zero-shot prompt to guide the LLM in generating these alternative queries. Thus, after implementing query expansion, instead of having only one query to search relevant context, you will have xN queries, hence xN searches.\n\nIncreasing the number of searches can impact your latency. Thus, you must experiment with the number of queries you generate to ensure the retrieval step meets your application requirements. You can also optimize the",
      "content_length": 1464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1057,
      "content": "searches by parallelizing them, drastically reducing the latency, which we will do in the\n\nContextRetriever\n\nclass implemented at the end of this chapter.\n\nQuery expansion is also known as multi-query, but the principles are the same. For example, this is an example of LangChain’s implementation called\n\nMultiQueryRetriver\n\n: https://python.langchain.com/docs/how_to/MultiQueryRetriever/\n\nNow, let’s dig into the code. We begin by importing the necessary modules and classes required for query expansion:\n\nfrom\n\nlangchain_openai\n\nimport\n\nChatOpenAI\n\nfrom\n\nllm_engineering.domain.queries",
      "content_length": 587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1058,
      "content": "import\n\nQuery\n\nfrom\n\nllm_engineering.settings\n\nimport\n\nsettings\n\nfrom\n\n.base\n\nimport\n\nRAGStep\n\nfrom\n\n.prompt_templates\n\nimport\n\nQueryExpansionTemplate\n\nNext, we define the\n\nQueryExpansion\n\nclass, which generates expanded query versions. The class implementation can be found at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/rag/query_expanison.py:",
      "content_length": 398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1059,
      "content": "class\n\nQueryExpansion\n\n(\n\nRAGStep\n\n):\n\ndef\n\ngenerate\n\n(\n\nself, query: Query, expand_to_n:\n\nint\n\n) ->\n\nlist\n\n[Query]:\n\nassert\n\nexpand_to_n >\n\n0\n\n,\n\nf\"'expand_to_n' should be greater than 0. Got",
      "content_length": 192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1060,
      "content": "{expand_to_n}\n\n.\"\n\nif\n\nself._mock:\n\nreturn\n\n[query\n\nfor\n\n_\n\nin\n\nrange\n\n(expand_to_n)]\n\nIn the\n\ngenerate\n\nmethod, we first ensure that the number of expansions requested (\n\nexpand_to_n\n\n) is greater than zero. If the instance is in mock mode (\n\nself._mock is True",
      "content_length": 262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1061,
      "content": "), it simply returns a list containing copies of the original query to simulate expansion without actually calling the API. If not in mock mode, we proceed to create the prompt and initialize the language model:\n\nquery_expansion_template = QueryExpansionTemplate() prompt = query_expansion_template.create_template(expand_to_n -\n\n1\n\n) model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_key=settings.OPENAI_API_KEY, temperature=\n\n0\n\n)\n\nHere, we instantiate\n\nQueryExpansionTemplate\n\nand create a prompt tailored to generate\n\nexpand_to_n - 1\n\nnew queries (excluding the original). We initialize the\n\nChatOpenAI\n\nmodel with the specified settings and set the temperature to 0 for deterministic output. We then create a LangChain chain by combining the prompt with the model and invoke it with the user’s question:",
      "content_length": 813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1062,
      "content": "chain = prompt | model response = chain.invoke({\n\n\"question\"\n\n: query}) result = response.content\n\nBy piping the prompt into the model (\n\nprompt | model\n\n), we set up a chain that generates expanded queries when invoked with the original query. The response from the model is captured in the\n\nresult\n\nobject. After receiving the response, we parse and clean the expanded queries:\n\nqueries_content = result.strip().split(query_expansion_template.separator) queries = [query] queries += [ query.replace_content(stripped_content)\n\nfor\n\ncontent\n\nin",
      "content_length": 544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1063,
      "content": "queries_content\n\nif\n\n(stripped_content := content.strip()) ]\n\nreturn\n\nqueries\n\nWe split the result using the separator defined in the template to get individual queries. Starting with a list containing the original query, we append each expanded query after stripping any extra whitespace.\n\nFinally, we define the\n\nQueryExpansionTemplate\n\nclass, which constructs the prompt used for query expansion. The class and other prompt templates can be accessed at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/rag/prompt_templates.py :\n\nfrom\n\nlangchain.prompts\n\nimport",
      "content_length": 611,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1064,
      "content": "PromptTemplate\n\nfrom\n\n.base\n\nimport\n\nPromptTemplateFactory\n\nclass\n\nQueryExpansionTemplate\n\n(\n\nPromptTemplateFactory\n\n): prompt:\n\nstr\n\n=\n\n\"\"\"You are an AI language model assistant. Your task is to generate {expand_to_n}\n\ndifferent versions of the given user question to retrieve relevant documents from a vector\n\ndatabase. By generating multiple perspectives on the user question, your goal is to help\n\nthe user overcome some of the limitations of the distance-based similarity search.\n\nProvide these alternative questions separated by '{separator}'.\n\nOriginal question: {question}\"\"\"",
      "content_length": 583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1065,
      "content": "@property\n\ndef\n\nseparator\n\n(\n\nself\n\n) ->\n\nstr\n\n:\n\nreturn\n\n\"#next-question#\"\n\ndef\n\ncreate_template\n\n(\n\nself, expand_to_n:\n\nint\n\n) -> PromptTemplate:\n\nreturn\n\nPromptTemplate( template=self.prompt, input_variables=[\n\n\"question\"\n\n], partial_variables={",
      "content_length": 248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1066,
      "content": "\"separator\"\n\n: self.separator,\n\n\"expand_to_n\"\n\n: expand_to_n, }, )\n\nThis class defines a prompt instructing the language model to generate multiple versions of the user’s question. It uses placeholders like\n\n{expand_to_n}\n\n,\n\n{separator}\n\n, and\n\n{question}\n\nto customize the prompt.\n\nIt takes\n\nexpand_to_n\n\nas an input parameter to define how many queries we wish to generate while we build the\n\nPromptTemplate",
      "content_length": 410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1067,
      "content": "instance. The separator property provides a unique string to split the generated queries. The\n\nexpand_to_n\n\nand\n\nseparator\n\nvariables are passed as\n\npartial_variables\n\n, making them immutable at runtime. Meanwhile, the\n\n{question}\n\nplaceholder will be changed every time the LLM chain is called.\n\nNow that we have finished studying the query expansion implementation, let’s look at an example of how to use the\n\nQueryExpansion\n\nclass. Let’s run the following code using this\n\npython -m llm_engineering.application.rag.query_expansion\n\ncommand:\n\nquery = Query.from_str(\n\n\"",
      "content_length": 571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1068,
      "content": "Write an article about the best types of advanced RAG methods.\"\n\n) query_expander = QueryExpansion() expanded_queries = query_expander.generate(query, expand_to_n=\n\n3\n\n)\n\nfor\n\nexpanded_query\n\nin\n\nexpanded_queries: logger.info(expanded_query.content)\n\nWe get the following variations of the original query. As you can observe, the query expansion method was successful in providing more details and different perspectives of the initial query, such as highlighting the effectiveness of advanced RAG methods or the overview of these methods (remember that the first query is the original one):\n\n2024-09-18 17:51:33.529 | INFO - Write an article about the best types of advanced RAG methods. 2024-09-18 17:51:33.529 | INFO - What are the most effective advanced RAG methods, and how can they be applied? 2024-09-18 17:51:33.529 | INFO - Can you provide an overview of the top advanced retrieval-augmented generation techniques?",
      "content_length": 924,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1069,
      "content": "Now, let’s move to the next pre-retrieval optimization method: self- querying.",
      "content_length": 78,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1070,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1071,
      "content": "Self-querying\n\nThe problem when embedding your query into a vector space is that you cannot guarantee that all the aspects required by your use case are present with enough signal in the embedding vector. For example, you want to be 100% sure that your retrieval depends on the tags provided in the user’s input. Unfortunately, you can’t control the signal left within the embedding that emphasizes the tag. By embedding the query prompt alone, you can never be sure that the tags are sufficiently represented in the embedding vector or have enough signal when computing the distance against other vectors.\n\nThis problem stands for any other metadata you want to present during the search, such as IDs, names, or categories.\n\nThe solution is to use self-querying to extract the tags or other critical metadata within the query and use them alongside the vector search as filters. Self-querying uses an LLM to extract various metadata fields crucial for your business use case, such as tags, IDs, number of comments, likes, shares, etc. Afterward, you have complete control over how the extracted metadata is considered during retrieval. In our LLM Twin use case, we extract the author’s name and use it as a filter. Self-queries work hand-in- hand with filtered vector searches, which we will explain in the next section.\n\nNow, let’s move on to the code. We begin by importing the necessary modules and classes on which our code relies:",
      "content_length": 1436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1072,
      "content": "from\n\nlangchain_openai\n\nimport\n\nChatOpenAI\n\nfrom\n\nllm_engineering.application\n\nimport\n\nutils\n\nfrom\n\nllm_engineering.domain.documents\n\nimport\n\nUserDocument\n\nfrom\n\nllm_engineering.domain.queries\n\nimport\n\nQuery\n\nfrom\n\nllm_engineering.settings",
      "content_length": 239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1073,
      "content": "import\n\nsettings\n\nfrom\n\n.base\n\nimport\n\nRAGStep\n\nfrom\n\n.prompt_templates\n\nimport\n\nSelfQueryTemplate\n\nNext, we define the\n\nSelfQuery\n\nclass, which inherits from\n\nRAGStep\n\nand implements the\n\ngenerate()\n\nmethod. The class can be found at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/rag/self_query.py:",
      "content_length": 350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1074,
      "content": "class\n\nSelfQuery\n\n(\n\nRAGStep\n\n):\n\ndef\n\ngenerate\n\n(\n\nself, query: Query\n\n) -> Query:\n\nif\n\nself._mock:\n\nreturn\n\nquery\n\nIn the\n\ngenerate()",
      "content_length": 135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1075,
      "content": "method, we check if the\n\n_mock\n\nattribute is set to\n\nTrue\n\n. If it is, we will return the original query object unmodified. This allows us to bypass calling the model while testing and debugging. If not in mock mode, we create the prompt template and initialize the language model.\n\nprompt = SelfQueryTemplate().create_template() model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_key=settings.OPENAI_API_KEY, temperature=\n\n0\n\n)\n\nHere, we instantiate the prompt using the\n\nSelfQueryTemplate\n\nfactory class and create a\n\nChatOpenAI\n\nmodel instance (similar to the query expansion implementation). We then combine the prompt and the model into a chain and invoke it with the user’s query.",
      "content_length": 690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1076,
      "content": "chain = prompt | model response = chain.invoke({\n\n\"question\"\n\n: query}) user_full_name = response.content.strip(\n\n\"\\n \"\n\n)\n\nWe extract the content from the LLM response and strip any leading or trailing whitespace to obtain the\n\nuser_full_name\n\nvalue. Next, we check if the model was able to extract any user information.\n\nif\n\nuser_full_name ==\n\n\"\n\nnone\"\n\n:",
      "content_length": 357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1077,
      "content": "return\n\nquery\n\nIf the response is\n\n\"none\"\n\n, it means no user name was found in the query, so we return the original query object. If a user name is found, we will split the\n\nuser_full_name\n\ninto the\n\nfirst_name\n\nand\n\nlast_name\n\nvariables using a utility function. Then, based on the user’s details, we retrieve or create a\n\nUserDocument\n\nuser instance:\n\nfirst_name, last_name = utils.split_user_full_name(user_full_name) user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)",
      "content_length": 499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1078,
      "content": "Finally, we update the query object with the extracted author information and return it:\n\nquery.author_id = user.\n\nid\n\nquery.author_full_name = user.full_name\n\nreturn\n\nquery\n\nThe updated query now contains the\n\nauthor_id\n\nand\n\nauthor_full_name\n\nvalues, which can be used in subsequent steps of the RAG pipeline.\n\nLet’s look at the",
      "content_length": 330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1079,
      "content": "SelfQueryTemplate\n\nclass, which defines the prompt to extract user information:\n\nfrom\n\nlangchain.prompts\n\nimport\n\nPromptTemplate\n\nfrom\n\n.base\n\nimport\n\nPromptTemplateFactory\n\nclass\n\nSelfQueryTemplate\n\n(\n\nPromptTemplateFactory\n\n): prompt:\n\nstr\n\n=",
      "content_length": 244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1080,
      "content": "\"\"\"You are an AI language model assistant. Your task is to extract information from a user question.\n\nThe required information that needs to be extracted is the user name or user id.\n\nYour response should consist of only the extracted user name (e.g., John Doe) or id (e.g. 1345256), nothing else.\n\nIf the user question does not contain any user name or id, you should return the following token: none.\n\nFor example:\n\nQUESTION 1:\n\nMy name is Paul Iusztin and I want a post about...\n\nRESPONSE 1:\n\nPaul Iusztin\n\nQUESTION 2:\n\nI want to write a post about...\n\nRESPONSE 2:\n\nnone\n\nQUESTION 3:\n\nMy user id is 1345256 and I want to write a post about...\n\nRESPONSE 3:\n\n1345256\n\nUser question: {question}\"\"\"",
      "content_length": 697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1081,
      "content": "def\n\ncreate_template\n\n(\n\nself\n\n) -> PromptTemplate:\n\nreturn\n\nPromptTemplate(template=self.prompt, input_variables=[\n\n\"question\"\n\n])\n\nIn the\n\nSelfQueryTemplate\n\nclass, we define a prompt instructing the AI model to extract the user name or ID from the input question. The prompt uses few-shot learning to guide the model on how to respond in different scenarios. When the template is invoked, the\n\n{question}\n\nplaceholder will be replaced with the actual user question.\n\nBy implementing self-querying, we ensure that critical metadata required for our use case is explicitly extracted and used during retrieval. This",
      "content_length": 615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1082,
      "content": "approach overcomes the limitations of relying solely on the semantics of the embeddings to capture all necessary aspects of a query.\n\nNow that we’ve implemented the\n\nSelfQuery\n\nclass, let’s provide an example. Run the following code using the\n\npython -m llm_engineering.application.rag.self_query\n\nCLI command:\n\nquery = Query.from_str(\n\n\"I am Paul Iusztin. Write an article about the best types of advanced RAG methods.\"\n\n) self_query = SelfQuery() query = self_query.generate(query) logger.info(\n\nf\"Extracted author_id:\n\n{query.author_id}\n\n\"\n\n) logger.info(\n\nf\"Extracted author_full_name:\n\n{query.author_full_name}\n\n\"",
      "content_length": 618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1083,
      "content": ")\n\nWe get the following results where the author’s full name and ID were extracted correctly:\n\n2024-09-18 18:02:10.362 | INFO - Extracted author_id: 900fec95-d621- 4315-84c6-52e5229e0b96 2024-09-18 18:02:10.362 | INFO - Extracted author_full_name: Paul Iusztin\n\nNow that we understand how self-querying works, let’s explore how it can be used together with filtered vector search within the retrieval optimization step.",
      "content_length": 419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1084,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1085,
      "content": "Advanced RAG retrieval optimization: filtered vector search\n\nVector search is pivotal in retrieving relevant information based on semantic similarity. A plain vector search, however, can introduce significant challenges that affect both the accuracy and latency of information retrieval. This is primarily because it operates solely on the numerical proximity of vector embeddings without considering the contextual or categorical nuances that might be crucial for relevance.\n\nOne of the primary issues with plain vector search is retrieving semantically similar but contextually irrelevant documents. Since vector embeddings capture general semantic meanings, they might assign high similarity scores to content that shares language patterns or topics but doesn’t align with the specific intent or constraints of the query. For instance, searching for “Java” could retrieve documents about the programming language or the Indonesian island, depending solely on semantic similarity, leading to ambiguous or misleading results.\n\nMoreover, as the size of the dataset increases, plain vector search can suffer from scalability issues. The lack of filtering means the search algorithm has to compute similarities across the entire vector space, which can significantly increase latency.\n\nThis exhaustive search slows response times and consumes more computational resources, making it inefficient for real-time or large-scale applications.",
      "content_length": 1435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1086,
      "content": "Filtered vector search emerges as a solution by filtering after additional criteria, such as metadata tags or categories, reducing the search space before computing vector similarities. By applying these filters, the search algorithm narrows the pool of potential results to those contextually aligned with the query’s intent. This targeted approach enhances accuracy by eliminating irrelevant documents that might have otherwise been considered due to their semantic similarities alone.\n\nAdditionally, filtered vector search improves latency by reducing the number of comparisons the algorithm needs to perform. Working with a smaller, more relevant subset of data decreases the computational overhead, leading to faster response times. This efficiency is crucial for applications requiring real-time interactions or handling large queries.\n\nAs the metadata used within the filtered vector search is often part of the user’s input, we have to extract it before querying the vector DB. That’s precisely what we did during the self-query step, where we extracted the author’s name to reduce the vector space only to the author’s content. Thus, as we processed the query within the self-query step, it went into the pre- retrieval optimization category, whereas when the filtered vector search optimized the query, it went into the retrieval optimization bin.\n\nFor example, when using Qdrant, to add a filter that looks for a matching\n\nauthor_id\n\nwithin the metadata of each document, you must implement the following code:",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1087,
      "content": "from\n\nqdrant_client.models\n\nimport\n\nFieldCondition, Filter, MatchValue records = qdrant_connection.search( collection_name=\n\n\"articles\"\n\n, query_vector=query_embedding, limit=\n\n3\n\n, with_payload=\n\nTrue\n\n, query_filter= Filter( must=[ FieldCondition( key=\n\n\"author_id\"\n\n,\n\nmatch\n\n=MatchValue( value=\n\nstr\n\n(\n\n\"1234\"",
      "content_length": 314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1088,
      "content": "), ), ) ] ), )\n\nIn essence, while plain vector search provides a foundation for semantic retrieval, its limitations can slow performance in practical applications. Filtered vector search addresses these challenges by combining the strengths of vector embeddings with contextual filtering, resulting in more accurate and efficient information retrieval in RAG systems. The last step for optimizing our RAG pipeline is to look into reranking.",
      "content_length": 440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1089,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1090,
      "content": "Advanced RAG post-retrieval optimization: reranking\n\nThe problem in RAG systems is that the retrieved context may contain irrelevant chunks that only:\n\nAdd noise: The retrieved context might be irrelevant, cluttering the information and potentially confusing the language model.\n\nMake the prompt bigger: Including unnecessary chunks increases the prompt size, leading to higher costs. Moreover, language models are usually biased toward the context’s first and last pieces. So, if you add a large amount of context, there’s a big chance it will miss the essence.\n\nBe come unaligned with your question: Chunks are retrieved based on the similarity between the query and chunk embeddings. The issue is that the embedding model might not be tuned to your question, resulting in high similarity scores for chunks that aren’t entirely relevant.\n\nThe solution is to use reranking to order all the N × K retrieved chunks based on their relevance relative to the initial question, where the first chunk will be the most relevant and the last the least. N represents the number of searches after query expansion, while K is the number of chunks",
      "content_length": 1135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1091,
      "content": "retrieved per search. Hence, we retrieve a total of N x K chunks. In RAG systems, reranking serves as a critical post-retrieval step that refines the initial results obtained from the retrieval model.\n\nWe assess each chunk’s relevance to the original query by applying the reranking algorithm, which often uses advanced models like neural cross- encoders. These models evaluate the semantic similarity between the query and each chunk more accurately than initial retrieval methods based on embeddings and the cosine similarity distance, as explained in more detail in Chapter 4 in the An overview of advanced RAG section.\n\nUltimately, we pick the top K most relevant chunks from the sorted list of N x K items based on the reranking score. Reranking works well when combined with query expansion. First, let’s understand how reranking works without query expansion:\n\nSearch for > K chunks: Retrieve more than K chunks to have a broader pool of potentially relevant information.\n\nReorder using rerank: Apply reranking to this larger set to evaluate the actual relevance of each chunk relative to the query.\n\nTake top K: Select the top K chunks to use them as context in the final prompt.",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1092,
      "content": "Thus, when combined with query expansion, we gather potential valuable context from multiple points in space rather than just looking for more than K samples in a single location. Now the flow looks like this:\n\nSearch for N × K chunks: Retrieve multiple sets of chunks using the expanded queries.\n\nReorder using rerank: Rerank all the retrieved chunks based on their relevance.\n\nTake top K: Select the most relevant chunks for the final prompt.\n\nIntegrating reranking into the RAG pipeline enhances the quality and relevance of the retrieved context and efficiently uses computational resources. Let’s look at implementing the LLM Twin’s reranking step to understand what we described above, which can be accessed on GitHub at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/application/rag/reranking.py.\n\nWe begin by importing the necessary modules and classes for our reranking process:\n\nfrom",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1093,
      "content": "llm_engineering.application.networks\n\nimport\n\nCrossEncoderModelSingleton\n\nfrom\n\nllm_engineering.domain.embedded_chunks\n\nimport\n\nEmbeddedChunk\n\nfrom\n\nllm_engineering.domain.queries\n\nimport\n\nQuery\n\nfrom\n\n.base\n\nimport\n\nRAGStep\n\nNext, we define the\n\nReranker",
      "content_length": 255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1094,
      "content": "class, which is responsible for reranking the retrieved documents based on their relevance to the query:\n\nclass\n\nReranker\n\n(\n\nRAGStep\n\n):\n\ndef\n\n__init__\n\n(\n\nself, mock:\n\nbool\n\n=\n\nFalse\n\n) ->\n\nNone\n\n:\n\nsuper",
      "content_length": 206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1095,
      "content": "().__init__(mock=mock) self._model = CrossEncoderModelSingleton()\n\nIn the initializer of the Reranker class, we instantiate our cross-encoder model by creating an instance of\n\nCrossEncoderModelSingleton\n\n. This is the cross-encoder model used to score the relevance of each document chunk with respect to the query.\n\nThe core functionality of the\n\nReranker\n\nclass is implemented in the\n\ngenerate()\n\nmethod:\n\ndef\n\ngenerate\n\n(\n\nself, query: Query, chunks:\n\nlist",
      "content_length": 459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1096,
      "content": "[EmbeddedChunk], keep_top_k:\n\nint\n\n) ->\n\nlist\n\n[EmbeddedChunk]:\n\nif\n\nself._mock:\n\nreturn\n\nchunks query_doc_tuples = [(query.content, chunk.content)\n\nfor\n\nchunk\n\nin\n\nchunks] scores = self._model(query_doc_tuples) scored_query_doc_tuples =\n\nlist\n\n(\n\nzip\n\n(scores, chunks, strict=\n\nFalse\n\n)) scored_query_doc_tuples.sort(key=\n\nlambda",
      "content_length": 330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1097,
      "content": "x: x[\n\n0\n\n], reverse=\n\nTrue\n\n) reranked_documents = scored_query_doc_tuples[:keep_top_k] reranked_documents = [doc\n\nfor\n\n_, doc\n\nin\n\nreranked_documents]\n\nreturn\n\nreranked_documents\n\nThe\n\ngenerate()\n\nmethod takes a query, a list of chunks (document segments), and the number of top documents to keep (\n\nkeep_top_k\n\n). If we’re in mock mode, it simply returns the original chunks. Otherwise, it performs the following steps:",
      "content_length": 422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1098,
      "content": "Creates pairs of the query content and each chunk’s content\n\nUses the cross-encoder model to score each pair, assessing how well the chunk matches the query\n\nZips the scores with the corresponding chunks to create a scored list of tuples\n\nSorts this list in descending order based on the scores\n\nSelects the top\n\nkeep_top_k\n\nchunks\n\nExtracts the chunks from the tuples and returns them as the reranked documents\n\nBefore defining the\n\nCrossEncoder",
      "content_length": 446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1099,
      "content": "class, we import the necessary components:\n\nfrom\n\nsentence_transformers.cross_encoder\n\nimport\n\nCrossEncoder\n\nfrom\n\n.base\n\nimport\n\nSingletonMeta\n\nWe import the\n\nCrossEncoder\n\nclass from the sentence_transformers library, which provides the functionality for scoring text pairs. We also import\n\nSingletonMeta\n\nfrom our base module to ensure our model class follows the singleton pattern, meaning only one instance of the model exists throughout the application. Now, we define the",
      "content_length": 478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1100,
      "content": "CrossEncoderModelSingleton\n\nclass:\n\nclass\n\nCrossEncoderModelSingleton\n\n(metaclass=SingletonMeta):\n\ndef\n\n__init__\n\n(\n\nself,\n\nmodel_id:\n\nstr\n\n= settings.RERANKING_CROSS_ENCODER_MODEL_ID,\n\ndevice:\n\nstr\n\n= settings.RAG_MODEL_DEVICE,\n\n) ->\n\nNone\n\n:",
      "content_length": 243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1101,
      "content": "\"\"\"\n\nA singleton class that provides a pre-trained cross-encoder model for scoring pairs of input text.\n\n\"\"\"\n\nself._model_id = model_id self._device = device self._model = CrossEncoder( model_name=self._model_id, device=self._device, ) self._model.model.\n\neval\n\n()\n\nThis class initializes the cross-encoder model using the specified\n\nmodel_id\n\nand\n\ndevice\n\nfrom the global\n\nsettings\n\nloaded from the\n\n.env\n\nfile. We set the model to evaluation mode using\n\nself._model.model.eval()",
      "content_length": 480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1102,
      "content": "to ensure the model is ready for inference.\n\nThe\n\nCrossEncoderModelSingleton\n\nclass includes a callable method to score text pairs:\n\ndef\n\n__call__\n\n(\n\nself, pairs:\n\nlist\n\n[\n\ntuple\n\n[\n\nstr\n\n,\n\nstr\n\n]], to_list:\n\nbool",
      "content_length": 215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1103,
      "content": "=\n\nTrue\n\n) -> NDArray[np.float32] |\n\nlist\n\n[\n\nfloat\n\n]: scores = self._model.predict(pairs)\n\nif\n\nto_list: scores = scores.tolist()\n\nreturn\n\nscores\n\nThe\n\n__call__\n\nmethod allows us to pass in a list of text\n\npairs\n\n(each consisting of the query and a document chunk) and receive their relevance scores. The method uses the model’s\n\npredict()",
      "content_length": 340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1104,
      "content": "function to call the model and compute the scores.\n\nThe\n\nCrossEncoderModelSingleton\n\nclass is a wrapper over the\n\nCrossEncoder\n\nclass, which we wrote for two purposes. The first one is for the singleton pattern, which allows us to easily access the same instance of the cross- encoder model from anywhere within the application without loading the model in memory every time we need it. The second reason is that by writing our wrapper, we defined our interface for a cross-encoder model (or any other model used for reranking). This makes the code future-proof as in case we need a different implementation or strategy for reranking, for example, using an API, we only have to write a different wrapper that follows the same interface and swap the old class with the new one. Thus, we can introduce new reranking methods without touching the rest of the code.\n\nWe now understand all the advanced RAG techniques used within our architecture. In the next section, we will examine the\n\nContextRetriever\n\nclass that connects all these methods and explain how to use the retrieval module with an LLM for an end-to-end RAG inference pipeline.",
      "content_length": 1137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1105,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1106,
      "content": "Implementing the LLM Twin’s RAG inference pipeline\n\nAs explained at the beginning of this chapter, the RAG inference pipeline can mainly be divided into three parts: the retrieval module, the prompt creation, and the answer generation, which boils down to calling an LLM with the augmented prompt. In this section, our primary focus will be implementing the retrieval module, where most of the code and logic go. Afterward, we will look at how to build the final prompt using the retrieved context and user query.\n\nUltimately, we will examine how to combine the retrieval module, prompt creation logic, and the LLM to capture an end-to-end RAG workflow. Unfortunately, we won’t be able to test out the LLM until we finish Chapter 10, as we haven’t deployed our fine-tuned LLM Twin module to AWS SageMaker.\n\nThus, by the end of this section, you will learn how to implement the RAG inference pipeline, which you can test out end to end only after finishing Chapter 10. Now, let’s start by looking at the implementation of the retrieval module.",
      "content_length": 1042,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1107,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1108,
      "content": "Implementing the retrieval module\n\nLet’s dive into the\n\nContextRetriever\n\nclass implementation, which orchestrates the retrieval step in our RAG system by integrating all the advanced techniques we previously used: query expansion, self-querying, reranking, and filtered vector search. The class can be found on GitHub at https://github.com/PacktPublishing/LLM- Engineers- Handbook/blob/main/llm_engineering/application/rag/retriever.py.",
      "content_length": 437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1110,
      "content": "Figure 9.2: Search logic of the RAG retrieval module\n\nThe entry point function of the\n\nContextRetriever\n\nclass is the\n\nsearch()\n\nmethod, which calls all the advanced steps discussed in this chapter. Figure 9.2 shows in more detail how the search method glues together all the steps required to search results similar to the user’s query. It highlights how the extracted author details from the self-query step are used within the filtered vector search. Also, it zooms in on the search operation itself, where, for each query, we do three searches to the vector DB, looking for articles, posts, or repositories similar to the query. For each search (out of N searches), we want to retrieve a maximum of K results. Thus, we retrieve a maximum of K / 3 items for each data category (as we have three categories). Therefore, when summed up, we will have a list of\n\n≤ K\n\nchunks. The retrieved list is\n\n≤ K\n\n(and not equal to K) when a particular data category or more returns\n\n< K / 3\n\nitems after applying the author filters due to missing chunks for that specific author or data category.",
      "content_length": 1086,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1111,
      "content": "Figure 9.3: Processing the results flow of the RAG retrieval module",
      "content_length": 67,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1112,
      "content": "Figure 9.3 illustrates how we process the results returned by the xN searches. As each search returns\n\n≤ K\n\nitems, we will end up with\n\n≤ N x K\n\nchunks that we aggregate into a single list. As some results might overlap between searchers, we must deduplicate the aggregated list to ensure each chunk is unique. Ultimately, we send the results to the rerank model, order them based on their reranking score, and pick the most relevant top K chunks we will use as context for RAG.\n\nLet’s understand how everything from Figures 9.2 and 9.3 is implemented in the\n\nContextRetriever\n\nclass. First, we initialize the class by setting up instances of the\n\nQueryExpansion\n\n,\n\nSelfQuery\n\n, and\n\nReranker\n\nclasses:",
      "content_length": 703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1113,
      "content": "class\n\nContextRetriever\n\n:\n\ndef\n\n__init__\n\n(\n\nself, mock:\n\nbool\n\n=\n\nFalse\n\n) ->\n\nNone\n\n: self._query_expander = QueryExpansion(mock=mock) self._metadata_extractor = SelfQuery(mock=mock) self._reranker = Reranker(mock=mock)\n\nIn the\n\nsearch()\n\nmethod, we convert the user’s input string into a\n\nquery",
      "content_length": 298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1114,
      "content": "object. We then use the\n\nSelfQuery\n\ninstance to extract the\n\nauthor_id\n\nand\n\nauthor_full_name\n\nfrom the query:\n\ndef\n\nsearch\n\n(\n\nself,\n\nquery:\n\nstr\n\n,\n\nk:\n\nint\n\n=\n\n3",
      "content_length": 164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1115,
      "content": ",\n\nexpand_to_n_queries:\n\nint\n\n=\n\n3\n\n,\n\n) ->\n\nlist\n\n: query_model = Query.from_str(query) query_model = self._metadata_extractor.generate(query_model) logger.info(\n\n\"Successfully extracted the author_id from the query.\"\n\n, author_id=query_model.author_id, )\n\nNext, we expand the query to generate multiple semantically similar queries using the\n\nQueryExpansion\n\ninstance:",
      "content_length": 370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1116,
      "content": "n_generated_queries = self._query_expander.generate(query_model, expand_to_n=expand_to_n_queries) logger.info(\n\n\"Successfully generated queries for search.\"\n\n, num_queries=\n\nlen\n\n(n_generated_queries), )\n\nWe then perform the search concurrently for all expanded queries using a thread pool. Each query is processed by the\n\n_search()\n\nmethod, which we’ll explore shortly. The results are flattened, deduplicated, and collected into a single list:\n\nwith\n\nconcurrent.futures.ThreadPoolExecutor()\n\nas\n\nexecutor: search_tasks = [executor.submit(self._search, _query_model, k)\n\nfor\n\n_query_model",
      "content_length": 589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1117,
      "content": "in\n\nn_generated_queries] n_k_documents = [task.result()\n\nfor\n\ntask\n\nin\n\nconcurrent.futures.as_completed(search_tasks)] n_k_documents = utils.misc.flatten(n_k_documents) n_k_documents =\n\nlist\n\n(\n\nset\n\n(n_k_documents)) logger.info(\n\n\"All documents retrieved successfully.\"\n\n, num_documents=\n\nlen\n\n(n_k_documents))\n\nAfter retrieving the documents, we rerank them based on their relevance to the original query and keep only the top k documents:",
      "content_length": 441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1118,
      "content": "if\n\nlen\n\n(n_k_documents) >\n\n0\n\n: k_documents = self.rerank(query, chunks=n_k_documents, keep_top_k=k)\n\nelse\n\n: k_documents = []\n\nreturn\n\nk_documents\n\nThe\n\n_search()\n\nmethod performs the filtered vector search across different data categories like posts, articles, and repositories. It uses the\n\nEmbeddingDispatcher\n\nto convert the query into an\n\nEmbeddedQuery\n\n, which includes the query’s embedding vector and any extracted metadata:",
      "content_length": 434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1119,
      "content": "def\n\n_search\n\n(\n\nself, query: Query, k:\n\nint\n\n=\n\n3\n\n) ->\n\nlist\n\n[EmbeddedChunk]:\n\nassert\n\nk >=\n\n3\n\n,\n\n\"k should be >= 3\"\n\ndef\n\n_search_data_category\n\n(\n\ndata_category_odm:",
      "content_length": 171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1120,
      "content": "type\n\n[EmbeddedChunk], embedded_query: EmbeddedQuery\n\n) ->\n\nlist\n\n[EmbeddedChunk]:\n\nif\n\nembedded_query.author_id: query_filter = Filter( must=[ FieldCondition( key=\n\n\"author_id\"\n\n,\n\nmatch\n\n=MatchValue( value=\n\nstr\n\n(embedded_query.author_id), ), ) ] )\n\nelse\n\n: query_filter =\n\nNone\n\nreturn\n\ndata_category_odm.search( query_vector=embedded_query.embedding, limit=k //\n\n3",
      "content_length": 369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1121,
      "content": ", query_filter=query_filter, ) embedded_query: EmbeddedQuery = EmbeddingDispatcher.dispatch(query)\n\nWe used the same\n\nEmbeddingDispatcher\n\nto embed the query as in the RAG feature pipeline to embed the document chunks stored in the vector DB. Using the same class ensures we use the same embedding model at ingestion and query time, which is critical for the retrieval step.\n\nWe search each data category separately by leveraging the local\n\n_search_data_category()\n\nfunction. Within the\n\n_search_data_category()\n\nfunction, we apply the filters extracted from the\n\nembedded_query\n\nobject. For instance, if an\n\nauthor_id\n\nis present, we use it to filter the search results only to include documents from that author. The results from all categories are then combined:",
      "content_length": 765,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1122,
      "content": "post_chunks = _search_data_category(EmbeddedPostChunk, embedded_query) articles_chunks = _search_data_category(EmbeddedArticleChunk, embedded_query) repositories_chunks = _search_data_category(EmbeddedRepositoryChunk, embedded_query) retrieved_chunks = post_chunks + articles_chunks + repositories_chunks\n\nreturn\n\nretrieved_chunks\n\nFinally, the\n\nrerank()\n\nmethod takes the original query and the list of retrieved documents to reorder them based on relevance:\n\ndef\n\nrerank\n\n(\n\nself, query:\n\nstr\n\n| Query, chunks:",
      "content_length": 512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1123,
      "content": "list\n\n[EmbeddedChunk], keep_top_k:\n\nint\n\n) ->\n\nlist\n\n[EmbeddedChunk]:\n\nif\n\nisinstance\n\n(query,\n\nstr\n\n): query = Query.from_str(query) reranked_documents = self._reranker.generate(query=query, chunks=chunks, keep_top_k=keep_top_k) logger.info(\n\n\"\n\nDocuments reranked successfully.\"\n\n, num_documents=\n\nlen\n\n(reranked_documents))\n\nreturn\n\nreranked_documents",
      "content_length": 354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1124,
      "content": "Leveraging the\n\nContextRetriever\n\nclass, we can retrieve context from any query with only a few lines of code. For example, let’s take a look at the following code snippet, where we call the entire advanced RAG architecture with a simple call to the\n\nsearch()\n\nmethod:\n\nfrom\n\nloguru\n\nimport\n\nlogger\n\nfrom\n\nllm_engineering.application.rag.retriever\n\nimport\n\nContextRetriever query =\n\n\"\"\"\n\nMy name is Paul Iusztin.",
      "content_length": 412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1125,
      "content": "Could you draft a LinkedIn post discussing RAG systems?\n\nI'm particularly interested in:\n\nhow RAG works\n\nhow it is integrated with vector DBs and large language models (LLMs).\n\n\"\"\"\n\nretriever = ContextRetriever(mock=\n\nFalse\n\n) documents = retriever.search(query, k=\n\n3\n\n) logger.info(\n\n\"Retrieved documents:\"\n\n)\n\nfor\n\nrank, document\n\nin\n\nenumerate\n\n(documents): logger.info(\n\nf\"\n\n{rank +\n\n1",
      "content_length": 390,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1126,
      "content": "}\n\n:\n\n{document}\n\n\"\n\n)\n\nCalling the code from above using the following CLI command:\n\npoetry poe call-rag-retrieval-module\n\n. This outputs the following:\n\n2024-09-18 19:01:50.588 | INFO - Retrieved documents: 2024-09-18 19:01:50.588 | INFO - 1: id=UUID('541d6c22-d15a-4e6a-924a- 68b7b1e0a330') content='4 Advanced RAG Algorithms You Must Know by Paul Iusztin Implement 4 advanced RAG retrieval techniques to optimize your vector DB searches. Integrate the RAG retrieval module into a production LLM system…\" platform='decodingml.substack.com' document_id=UUID('32648f33-87e6-435c-b2d7-861a03e72392') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://decodingml.substack.com/p/the-4- advanced-rag-algorithms-you?r=1ttoeh' 2024-09-18 19:01:50.588 | INFO - 2: id=UUID('5ce78438-1314-4874-8a5a-04f5fcf0cb21') content='Overview of advanced RAG optimization techniquesA production RAG system is",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1127,
      "content": "split into 3 main components ingestion clean, chunk, embed, and load your data to a vector DBretrieval query your vector DB for …\" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7- 0d06760ee6bf') author_id=UUID('900fec95-d621-4315-84c6- 52e5229e0b96') author_full_name='Paul Iusztin' metadata= {'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://medium.com/decodingml/the-4-advanced-rag-algorithms-you- must-know-to-implement-5d0c7f1199d2' 2024-09-18 19:02:45.729 | INFO - 3: id=UUID('0405a5da-4686-428a-91ca-446b8e0446ff') content='Every Medium article will be its own lesson An End to End Framework for Production Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture Enabling Event Driven …\" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7-0d06760ee6bf') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://medium. com/decodingml/the-4- advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2'\n\nAs you can observe in the output above, along with the retrieved content, we have access to all kinds of metadata, such as the embedding model used for retrieval or the link from which the chunk was taken. These can quickly be added to a list of references when generating the result for the user, increasing trust in the final results.\n\nNow that we understand how the retrieval module works, let’s take a final step and examine the end-to-end RAG inference pipeline.",
      "content_length": 1711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1128,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1129,
      "content": "Bringing everything together into the RAG inference pipeline\n\nTo fully implement the RAG flow, we still have to build the prompt using the context from the retrieval model and call the LLM to generate the answer. This section will discuss these two steps and wrap everything together into a single\n\nrag()\n\nfunction. The functions from this section can be accessed on GitHub at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/infrastructure/inference_pipeline_api .py.\n\nLet’s start by looking at the\n\ncall_llm_service()\n\nfunction, responsible for interfacing with the LLM service. It takes in a user’s query and an optional context, sets up the language model endpoint, executes the inference, and returns the generated answer. The context is optional; you can call the LLM without it, as you would when interacting with any other LLM:\n\ndef",
      "content_length": 876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1130,
      "content": "call_llm_service\n\n(\n\nquery:\n\nstr\n\n, context:\n\nstr\n\n|\n\nNone\n\n) ->\n\nstr\n\n: llm = LLMInferenceSagemakerEndpoint( endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_component_name=\n\nNone\n\n) answer = InferenceExecutor(llm, query, context).execute()\n\nreturn\n\nanswer\n\nThis function makes an HTTP request to our fine-tuned LLM Twin model, which is hosted as an AWS SageMaker inference endpoint. We will explore all the SageMaker details in the next chapter, where we will dig into the",
      "content_length": 488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1131,
      "content": "LLMInferenceSagemakerEndpoint\n\nand\n\nInferenceExecutor\n\nclasses. For now, what is essential to know is that we use this function to call our fine-tuned LLM. Still, we must highlight how the query and context, passed to the\n\nInferenceExecutor\n\nclass, are transformed into the final prompt. We do that using a simple prompt template that is customized using the user query and retrieved context:\n\nprompt =\n\nf\"\"\"\n\nYou are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.\n\nUser query:\n\n{query}\n\nContext:\n\n{context}\n\n\"\"\"",
      "content_length": 604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1132,
      "content": "Moving on to the\n\nrag()\n\nfunction, this is where the RAG logic comes together. It handles retrieving relevant documents based on the query, mapping the documents to the context that will be injected into the prompt, and obtaining the final answer from the LLM:\n\ndef\n\nrag\n\n(\n\nquery:\n\nstr\n\n) ->\n\nstr\n\n: retriever = ContextRetriever(mock=\n\nFalse\n\n) documents = retriever.search(query, k=\n\n3\n\n) context = EmbeddedChunk.to_context(documents) answer = call_llm_service(query, context)",
      "content_length": 478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1133,
      "content": "return\n\nanswer\n\nAs we modularized all the RAG steps into independent classes, we reduced the high-level\n\nrag()\n\nfunction to five lines of code (encapsulating all the complexities of the system) similar to what we see in tools such as LangChain, LlamaIndex, or Haystack. Instead of their high-level implementation, we learned how to build an advanced RAG service from scratch. Also, by clearly separating the responsibility of each class, we can use them like LEGOs. Thus, you can quickly call the LLM independently without context or use the retrieval module as a query engine on top of your vector DB. In the next chapter, we will see the\n\nrag()\n\nfunction in action after we deploy our fine-tuned LLM to an AWS SageMaker inference endpoint.\n\nBefore ending this chapter, we want to discuss potential improvements you could add to the RAG inference pipeline. As we are building a chatbot, the first one is to add a conversation memory that stores all the user prompts and generated answers in memory. Thus, when interacting with the chatbot, it will be aware of the whole conversation, not only the latest prompt. When prompting the LLM, along with the new user input and context, we also pass the conversation history from the memory. As the conversation history can get long, to avoid exceeding the context window or higher costs, you have to implement a way to reduce the size of your memory. As illustrated in",
      "content_length": 1412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1134,
      "content": "Figure 9.4, the simplest one is to keep only the latest K items from your chat history. Unfortunately, using this strategy, the LLM will never be aware of the whole conversation.\n\nTherefore, another way to add the chat history to your prompt is to keep a summary of the conversation along with the latest K replies. There are multiple ways to compute this summary, which might defeat the purpose of this book if we get into them all, but the simplest way is to always update the summary on every user prompt and generate an answer.",
      "content_length": 531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1135,
      "content": "Figure 9.4: Routing and memory examples",
      "content_length": 39,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1136,
      "content": "As for each search, we send three queries to the vector DB, one for each data category. Thus, the second improvement is to add a router between the query and the search. The router will be a multi-category classifier that predicts the data categories we must retrieve for that specific query. Hence, instead of making three requests for every search, we can often reduce it to one or two. For example, if the user wants to write a theoretical paragraph about RAG for an article, then most probably, it’s valuable to query only the article’s collection. In this case, the router will predict the article class, which we can use to decide what collection we must query.\n\nAnother example would be if we want to illustrate a piece of code that shows how to build a RAG pipeline. In this case, the router would have to predict the article and repository data category, as we need to look up examples in both collections for an exhaustive context.\n\nUsually, the router strategy decides what model to call based on a user’s input, such as whether to use GPT-4 or a self-hosted Llama 3.1 model for that specific query. However, in our particular use case, we can adapt the router algorithm to optimize the retrieval step.\n\nWe can further optimize the retrieval using a hybrid search algorithm that combines the vector search (based on embeddings) with a keyword search algorithm, such as BM25. Search algorithms used BM25 (or similar methods) to find similar items in a DB before vector search algorithms became popular. By merging the methods, hybrid search retrieves results that match the exact terms, such as RAG, LLM, or SageMaker, and the query semantics, increasing the accuracy and relevance of your retrieved results. Fundamentally, the hybrid search algorithms follow the next mechanics:",
      "content_length": 1789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1137,
      "content": "Parallel processing: The search query is processed simultaneously through both the vector search and BM25 algorithms. Each algorithm retrieves a set of relevant documents based on its criteria.\n\nScore normalization: The results from both searches are assigned relevance scores, which are then normalized to ensure comparability. This step is crucial because vector search and BM25 scoring mechanisms work at different scales. Thus, they can’t be compared or merged without normalization.\n\nResult merging: The normalized scores are combined, often through a weighted sum, to produce a final ranking of documents. Adjusting the weights allows for fine-tuning the emphasis on the semantic or keyword search algorithm.\n\nTo conclude, by combining the semantic and exact keyword search algorithms, you can improve the accuracy of your retrieval step. Vector search helps recognize synonyms or related concepts, ensuring that relevant information isn’t overlooked due to vocabulary differences. Keyword search ensures that documents containing critical keywords are emphasized appropriately, particularly in technical fields with specific terminology.\n\nOne last improvement we can make to our RAG system is to use multi- index vector structures instead of indexing based only on the content’s embedding. Let’s detail how multi-indexing works. Instead of using the embeddings of a single field to do the vector search for a particular collection, it combines multiple fields.",
      "content_length": 1467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1138,
      "content": "For example, in our LLM Twin use case, we used only the content field of our articles, posts, or repositories to query the vector DB. When using a multi-index strategy, along with the content field, we could index the embeddings of the platform where the content was posted or when the content was published. This could impact the final accuracy of your retrieval as different platforms have different types of content, or more recent content is usually more relevant. Frameworks such as Superlinked make multi-indexing easy. For example, in the code snippet below, using Superlinked, we defined a multi-index on the content and platform for our article collection in just a few lines of code:\n\nfrom\n\nsuperlinked.framework.common.schema.id_schema_object\n\nimport\n\nIdField\n\nfrom\n\nsuperlinked.framework.common.schema.schema\n\nimport\n\nschema\n\nfrom\n\nsuperlinked.framework.common.schema.schema_object\n\nimport\n\nString …",
      "content_length": 911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1139,
      "content": "# Other Superlinked imports.\n\n@schema\n\nclass\n\nArticleSchema\n\n:\n\nid\n\n: IdField platform: String content: String article = ArticleSchema() articles_space_content = TextSimilaritySpace( text=chunk(article.content, chunk_size=\n\n500\n\n, chunk_overlap=\n\n50\n\n), model=settings.EMBEDDING_MODEL_ID, ) articles_space_plaform = CategoricalSimilaritySpace( category_input=article.platform, categories=[\n\n\"medium\"\n\n,\n\n\"substack\"\n\n, \"wordpress\"], negative_filter=-\n\n5.0\n\n, ) article_index = Index( [articles_space_content, articles_space_plaform], fields=[article.author_id], )",
      "content_length": 562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1140,
      "content": "Superlinked is a powerful Python tool for any use case that includes vector computing, such as RAG, recommender systems, and semantic search. It offers an ecosystem where you can quickly ingest data into a vector DB, write complex queries on top of it, and deploy the service as a RESTful API.\n\nThe world of LLMs and RAG is experimental, similar to any other AI domain. Thus, when building real-world products, it’s important to quickly build an end-to-end solution that works but is not necessarily the best. Then, you can reiterate with various experiments until you completely optimize it for your use case. This is standard practice in the industry and lets you iterate fast while providing value to the business and gathering user feedback as quickly as possible in the product’s lifecycle.",
      "content_length": 795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1141,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1142,
      "content": "Summary\n\nThis chapter taught us how to build an advanced RAG inference pipeline. We started by looking into the software architecture of the RAG system. Then, we zoomed in on the advanced RAG methods we used within the retrieval module, such as query expansion, self-querying, filtered vector search, and reranking. Afterward, we saw how to write a modular\n\nContextRetriever\n\nclass that glues all the advanced RAG components under a single interface, making searching for relevant documents a breeze. Ultimately, we looked into how to connect all the missing dots, such as the retrieval, the prompt augmentation, and the LLM call, under a single RAG function that will serve as our RAG inference pipeline.\n\nAs highlighted a few times in this chapter, we couldn’t test our fine-tuned LLM because we haven’t deployed it yet to AWS SageMaker as an inference endpoint. Thus, in the next chapter, we will learn how to deploy the LLM to AWS SageMaker, write an inference interface to call the endpoint, and implement a FastAPI web server to serve as our business layer.",
      "content_length": 1063,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1143,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1144,
      "content": "References\n\nA real-time retrieval system for social media data | VectorHub by SuperLinked. (n.d.). https://superlinked.com/vectorhub/articles/real-time- retrieval-system-social-media-data\n\nBuilding a Router from Scratch - LlamaIndex. (n.d.). https://docs.llamaindex.ai/en/stable/examples/low_level/router/\n\nHow to add memory to chatbots | LangChain. (n.d.). https://python.langchain.com/docs/how_to/chatbots_memory/#summary- memory\n\nHow to do “self-querying” retrieval | LangChain. (n.d.). https://python.langchain.com/docs/how_to/self_query/\n\nHow to route between sub-chains | LangChain. (n.d.). https://python.langchain.com/docs/how_to/routing/#routing-by-semantic- similarity\n\nHow to use the MultiQueryRetriever | LangChain. (n.d.). https://python.langchain.com/docs/how_to/MultiQueryRetriever/",
      "content_length": 797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1145,
      "content": "Hybrid Search explained. (2023, January 3). Weaviate. https://weaviate.io/blog/hybrid-search-explained\n\nIusztin, P. (2024, August 20). 4 Advanced RAG Algorithms You Must Know | Decoding ML. Medium. https://medium.com/decodingml/the-4- advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2\n\nMonigatti, L. (2024, February 19). Advanced Retrieval-Augmented Generation: From Theory to LlamaIndex Implementation. Medium. https://towardsdatascience.com/advanced-retrieval-augmented-generation- from-theory-to-llamaindex-implementation-4de1464a9930\n\nMulti-attribute search with vector embeddings | VectorHub by Superlinked. (n.d.). https://superlinked.com/vectorhub/articles/multi-attribute-semantic- search\n\nOptimizing RAG with Hybrid Search & Reranking | VectorHub by Superlinked. (n.d.). https://superlinked.com/vectorhub/articles/optimizing- rag-with-hybrid-search-reranking\n\nRefactoring.Guru. (2024, January 1). Singleton. https://refactoring.guru/design-patterns/singleton\n\nStoll, M. (2024, September 7). Visualize your RAG Data—Evaluate your Retrieval-Augmented Generation System with Ragas. Medium. https://towardsdatascience.com/visualize-your-rag-data-evaluate-your- retrieval-augmented-generation-system-with-ragas-fc2486308557",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1146,
      "content": "Using LLM’s for retrieval and reranking—LlamaIndex, data framework for LLM applications. (n.d.). https://www.llamaindex.ai/blog/using-llms-for- retrieval-and-reranking-23cf2d3a14b6\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1147,
      "content": "10",
      "content_length": 2,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1148,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1149,
      "content": "Inference Pipeline Deployment\n\nDeploying the inference pipeline for the large language model (LLM) Twin application is a critical stage in the machine learning (ML) application life cycle. It’s where the most value is added to your business, making your models accessible to your end users. However, successfully deploying AI models can be challenging, as the models require expensive computing power and access to up-to-date features to run the inference. To overcome these constraints, it’s crucial to carefully design your deployment strategy. This ensures that it meets the application’s requirements, such as latency, throughput, and costs. As we work with LLMs, we must consider the inference optimization techniques presented in Chapter 8, such as model quantization. Also, to automate the deployment processes, we must leverage MLOps best practices, such as model registries that version and share our models across our infrastructure.\n\nTo understand how to design the deployment architecture of the LLM Twin, we will first look at three deployment types we can choose from: online real-time inference, asynchronous inference, and offline batch transform. Also, to better understand which option to choose for our LLM Twin use case, we will quickly walk you through a set of critical criteria we must consider before making an architectural decision, such as latency, throughput, data, and infrastructure. Also, we’ll weigh the pros and cons of monolithic and microservices architecture in model serving, a decision that can significantly influence the scalability and maintainability of your service.Once we’ve grasped the various design choices available, we’ll focus on understanding the deployment strategy for the LLM Twin’s inference pipeline. Subsequently, we will walk you through an end-to-end tutorial on deploying the LLM Twin service, including deploying our custom fine-tuned LLM to AWS SageMaker endpoints and implementing a",
      "content_length": 1946,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1150,
      "content": "FastAPI server as the central entry point for our users. We will then wrap up this chapter with a short discussion on autoscaling strategies and how to use them on SageMaker.\n\nHence, in this chapter, we will cover the following topics:\n\nCriteria for choosing deployment types\n\nUnderstanding inference deployment types\n\nMonolithic versus microservices architecture in model serving\n\nExploring the LLM Twin’s inference pipeline deployment strategy\n\nDeploying the LLM Twin service\n\nAutoscaling capabilities to handle spikes in usage",
      "content_length": 529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1151,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1152,
      "content": "Criteria for choosing deployment types\n\nWhen it comes to deploying ML models, the first step is to understand the four requirements present in every ML application: throughput, latency, data, and infrastructure.\n\nUnderstanding them and their interaction is essential. When designing the deployment architecture for your models, there is always a trade-off between the four that will directly impact the user’s experience. For example, should your model deployment be optimized for low latency or high throughput?",
      "content_length": 512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1153,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1154,
      "content": "Throughput and latency\n\nThroughput refers to the number of inference requests a system can process in a given period. It is typically measured in requests per second (RPS). Throughput is crucial when deploying ML models when you expect to process many requests. It ensures the system can handle many requests efficiently without becoming a bottleneck.\n\nHigh throughput often requires scalable and robust infrastructure, such as machines or clusters with multiple high-end GPUs.Latency is the time it takes for a system to process a single inference request from when it is received until the result is returned. Latency is critical in real-time applications where quick response times are essential, such as in live user interactions, fraud detection, or any system requiring immediate feedback. For example, the average latency of OpenAI’s API is the average response time from when a user sends a request, and the service provides a result that is accessible within your application.\n\nThe latency is the sum of the network I/O, serialization and deserialization, and the LLM’s inference time. Meanwhile, the throughput is the average number of requests the API processes and serves a second.\n\nLow-latency systems require optimized and often more costly infrastructure, such as faster processors, lower network latency, and possibly edge computing to reduce the distance data needs to travel.",
      "content_length": 1393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1155,
      "content": "A lower latency translates to higher throughput when the service processes multiple queries in parallel successfully. For example, if the service takes 100 ms to process requests, this translates to a throughput of 10 requests per second. If the latency reaches 10 ms per request, the throughput rises to 100 requests per second.\n\nHowever, to complicate things, most ML applications adopt a batching strategy to simultaneously pass multiple data samples to the model. In this case, a lower latency can translate into lower throughput; in other words, a higher latency maps to a higher throughput. For example, if you process 20 batched requests in 100 ms, the latency is 100 ms, while the throughput is 200 requests per second. If you process 60 requests in 200 ms, the latency is 200 ms, while the throughput rises to 300 requests per second. Thus, even when batching requests at serving time, it’s essential to consider the minimum latency accepted for a good user experience.",
      "content_length": 978,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1156,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1157,
      "content": "Data\n\nAs we know, data is everywhere in an ML system. But when talking about model serving, we mostly care about the model’s input and output. This includes the format, volume, and complexity of the processed data. Data is the foundation of the inference process. The characteristics of the data, such as its size and type, determine how the system needs to be configured and optimized for efficient processing.\n\nThe type and size of the data directly impact latency and throughput, as more complex or extensive data can take longer to process. For example, designing a model that takes input structured data and outputs a probability differs entirely from an LLM that takes input text (or even images) and outputs an array of characters.\n\nInfrastructure\n\nInfrastructure refers to the underlying hardware, software, networking, and system architecture that supports the deployment and operation of the ML models. The infrastructure provides the necessary resources for deploying, scaling, and maintaining ML models. It includes computing resources, memory, storage, networking components, and the software stack:\n\nFor high throughput, the systems require scalable infrastructure to manage large data volumes and high request rates, possibly through parallel processing, distributed systems, and high-end GPUs.",
      "content_length": 1309,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1158,
      "content": "Infrastructure must be optimized to reduce processing time to achieve low latency, such as using faster CPUs, GPUs, or specialized hardware. While optimizing your system for low latency while batching your requests, you often have to sacrifice high throughput in favor of lower latency, which can result in your hardware not being utilized at total capacity. As you process fewer requests per second, it results in idle computing, which increases the overall cost of processing a request. Thus, picking the suitable machine for your requirements is critical in optimizing costs.\n\nIt is crucial to design infrastructure to meet specific data requirements. This includes selecting storage solutions to handle large datasets and implementing fast retrieval mechanisms to ensure efficient data access. For example, we mostly care about optimizing throughput for offline training, while for online inference, we generally care about latency.\n\nWith this in mind, before picking a specific deployment type, you should ask yourself questions such as:\n\nWhat are the throughput requirements? You should make this decision based on the throughput’s required minimum, average, and maximum statistics.\n\nHow many requests the system must handle simultaneously? (1, 10, 1,000, 1 million, etc.)",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1159,
      "content": "What are the latency requirements? (1 millisecond, 10 milliseconds, 1 second, etc.)\n\nHow should the system scale? For example, we should look at the CPU workload, number of requests, queue size, data size, or a combination of them.\n\nWhat are the cost requirements?With what data do we work with? For example, do we work with images, text, or tabular data?\n\nWhat is the size of the data we work with? (100 MB, 1 GB, 10 GB)\n\nDeeply thinking about these questions directly impacts the user experience of your application, which ultimately makes the difference between a successful product and not. Even if you ship a mind-blowing model, if the user needs to wait too long for a response or it often crashes, the user will switch your production to something less accurate that works reliably. For example, Google found in a 2016 study that 53% of visits are abandoned if a mobile site takes longer than three seconds to load: https://www.thinkwithgoogle.com/consumer-insights/consumer- trends/mobile-site-load-time-statistics/.\n\nLet’s move on to the three deployment architectures we can leverage to serve our models.",
      "content_length": 1114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1160,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1161,
      "content": "Understanding inference deployment types\n\nAs illustrated in Figure 10.1, you can choose from three fundamental deployment types when serving models:\n\nOnline real-time inference\n\nAsynchronous inference\n\nOffline batch transform\n\nWhen selecting one design over the other, there is a trade-off between latency, throughput, and costs. You must consider how the data is accessed and the infrastructure you are working with. Another criterion you have to consider is how the user will interact with the model. For example, will the user use it directly, like a chatbot, or will it be hidden within your system, like a classifier that checks if an input (or output) is safe?\n\nYou have to consider the freshness of the predictions as well. For example, serving your model in offline batch mode might be easier to implement if, in your use case, it is OK to consume delayed predictions. Otherwise, you have to serve your model in real-time, which is more infrastructure- demanding. Also, you have to consider your application’s traffic. Ask",
      "content_length": 1030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1162,
      "content": "yourself questions such as, “Will the application be constantly used, or will there be spikes in traffic and then flatten out?”\n\nWith that in mind, let’s explore the three major ML deployment types.",
      "content_length": 198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1164,
      "content": "Figure 10.1: The three fundamental architectures of inference deployment types",
      "content_length": 78,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1165,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1166,
      "content": "Online real-time inference\n\nIn real-time inference, we have a simple architecture based on a server that can be accessed through HTTP requests. The most popular options are to implement a REST API or gRPC server. The REST API is more accessible but slower, using JSON to pass data between the client and server.\n\nThis approach is usually taken when serving models outside your internal network to the broader public. For example, OpenAI’s API implements a REST API protocol.\n\nOn the other hand, implementing a gRPC makes your ML server faster, though it may reduce its flexibility and general applicability. You have to implement\n\nprotobuf\n\nschemas in your client application, which are more tedious to work with than JSON structures. The benefit, however, is that\n\nprotobuf\n\nobjects can be compiled into bites, making the network transfers much faster. Thus, this protocol is often adopted for internal services within the same ML system.\n\nUsing the real-time inference approach, the client sends an HTTP request to the ML service, which immediately processes the request and returns the",
      "content_length": 1088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1167,
      "content": "result in the same response. This synchronous interaction means the client waits for the result before moving on.\n\nTo make this work efficiently, the infrastructure must support low-latency, highly responsive ML services, often deployed on fast, scalable servers. Load balancing is crucial to evenly distribute incoming traffic evenly, while autoscaling ensures the system can handle varying loads. High availability is also essential to keeping the service operational at all times.\n\nFor example, this architecture is often present when interacting with LLMs, as when sending a request to a chatbot or API (powered by LLMs), you expend the predictions right ahead. LLM services, such as ChatGPT or Claude, often use WebSockets to stream each token individually to the end user, making the interaction more responsive. Other famous examples are AI services such as embedding or reranking models used for retrieval-augmented generation (RAG) or online recommendation engines in platforms like TikTok.\n\nThe simplicity of real-time inference, with its direct client-server interaction, makes it an attractive option for applications that require immediate responses, like chatbots or real-time recommendations. However, this approach can be challenging to scale and may lead to underutilized resources during low-traffic periods.",
      "content_length": 1326,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1168,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1169,
      "content": "Asynchronous inference\n\nIn asynchronous inference, the client sends a request to the ML service, which acknowledges the request and places it in a queue for processing. Unlike real-time inference, the client doesn’t wait for an immediate response. Instead, the ML service processes the request asynchronously. This requires a robust infrastructure that queues the messages to be processed by the ML service later on.\n\nWhen the results are ready, you can leverage multiple techniques to send them to the client. For example, depending on the size of the result, you can put it either in a different queue or an object storage dedicated to storing the results.\n\nThe client can either adopt a polling mechanism that checks on a schedule if there are new results or adopt a push strategy and implement a notification system to inform the client when the results are ready.\n\nAsynchronous inference uses resources more efficiently. It doesn’t have to process all the requests simultaneously but can define a maximum number of machines that run in parallel to process the messages. This is possible because the requests are stored in the queue until a machine can process them. Another huge benefit is that it can handle spikes in requests without any timeouts. For example, let’s assume that on an e- shop site, we usually have 10 requests per second handled by two machines. Because of a promotion, many people started to visit the site, and the number of requests spiked to 100 requests per second. Instead",
      "content_length": 1502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1170,
      "content": "of scaling the number of virtual machines (VMs) by 10, which can add drastic costs, the requests are queued, and the same two VMs can process them in their rhythm without any failures.\n\nAnother popular advantage for asynchronous architectures is when the requested job takes significant time to complete. For example, if the job takes over five minutes, you don’t want to block the client waiting for a response.\n\nWhile asynchronous inference offers significant benefits, it does come with trade-offs. It introduces higher latency, making it less suitable for time- sensitive applications. Additionally, it adds complexity to the implementation and infrastructure. Depending on your design choices, this architecture type falls somewhere between online and offline, offering a balance of benefits and trade-offs.\n\nFor example, this is a robust design where you don’t care too much about the latency of the inference but want to optimize costs heavily. Thus, it is a popular choice for problems such as extracting keywords from documents, summarizing them using LLMs, or running deep-fake models on top of videos. But suppose you carefully design the autoscaling system to process the requests from the queue at decent speeds. In that case, you can leverage this design for other use cases, such as online recommendations for e- commerce. In the end, it sums up how much computing power you are willing to pay to meet the expectations of your application.",
      "content_length": 1454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1171,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1172,
      "content": "Offline batch transform\n\nBatch transform is about processing large volumes of data simultaneously, either on a schedule or triggered manually. In a batch transform architecture, the ML service pulls data from a storage system, processes it in a single operation, and then stores the results in storage. The storage system can be implemented as an object storage like AWS S3 or a data warehouse like GCP BigQuery.\n\nUnlike the asynchronous inference architecture, a batch transform design is optimized for high throughput with permissive latency requirements. When real-time predictions are unnecessary, this approach can significantly reduce costs, as processing data in big batches is the most economical method. Moreover, the batch transform architecture is the simplest way to serve a model, accelerating development time.\n\nThe client pulls the results directly from data storage, decoupling its interaction with the ML service. Taking this approach, the client never has to wait for the ML service to process its input, but at the same time, it doesn’t have the flexibility to ask for new results at any time. You can see the data storage, where the results are stored as a large cache, from where the client can take what is required. If you want to make your application more responsive, the client can be notified when the processing is complete and can retrieve the results.\n\nUnfortunately, this approach will always introduce a delay between the time the predictions were computed and consumed. That’s why not all",
      "content_length": 1521,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1173,
      "content": "applications can leverage this design choice. For example, if we implement a recommender system for a video streaming application, having a delay of one day for the predicted movies and TV shows might work because you don’t consume these products at a high frequency. But suppose you make a recommender system for a social media platform. In that case, delaying one day or even one hour is unacceptable, as you constantly want to provide fresh content to the user.\n\nBatch transform shines in scenarios where high throughput is needed, like data analytics or periodic reporting. However, it’s unsuitable for real-time applications due to its high latency and requires careful planning and scheduling to manage large datasets effectively. That’s why it is an offline serving method.\n\nTo conclude, we examined the three most common architectures for serving ML models. We started with online real-time inference, which serves clients when they request a prediction. Then, we looked at the asynchronous inference method, which sits between online and offline. Ultimately, we presented the offline batch transform, which is used to process large amounts of data and store them in data storage, from where the client later consumes them.",
      "content_length": 1231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1174,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1175,
      "content": "Monolithic versus microservices architecture in model serving\n\nIn the previous section, we saw three different methods of deploying the ML service. The differences in architecture were mainly based on the interaction between the client and the ML service, such as the communication protocol, the ML service responsiveness, and prediction freshness.\n\nBut another aspect to consider is the architecture of the ML service itself, which can be implemented as a monolithic server or as multiple microservices. This will impact how the ML service is implemented, maintained, and scaled. Let’s explore the two options.",
      "content_length": 611,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1177,
      "content": "Figure 10.2: Monolithic versus microservices architecture in model serving",
      "content_length": 74,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1178,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1179,
      "content": "Monolithic architecture\n\nThe LLM (or any other ML model) and the associated business logic (preprocessing and post-processing steps) are bundled into a single service in a monolithic architecture. This approach is straightforward to implement at the beginning of a project, as everything is placed within one code base. Simplicity makes maintenance easy when working on small to medium projects, as updates and changes can be made within a unified system.\n\nOne key challenge of a monolithic architecture is the difficulty of scaling components independently. The LLM typically requires GPU power, while the rest of the business logic is CPU and I/O-bound. As a result, the infrastructure must be optimized for both GPU and CPU. This can lead to inefficient resource use, with the GPU being idle when the business logic is executed and vice versa. Such inefficiency can result in additional costs that could be avoided.\n\nMoreover, this architecture can limit flexibility, as all components must share the same tech stack and runtime environment. For example, you might want to run the LLM using Rust or C++ or compile it with ONNX or TensorRT while keeping the business logic in Python. Having all the code in one system makes this differentiation difficult. Finally, splitting the work across different teams is complex, often leading to bottlenecks and reduced agility.",
      "content_length": 1370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1180,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1181,
      "content": "Microservices architecture\n\nA microservices architecture breaks down the inference pipeline into separate, independent services—typically splitting the LLM service and the business logic into distinct components. These services communicate over a network using protocols such as REST or gRPC.\n\nAs illustrated in Figure 10.3, the main advantage of this approach is the ability to scale each component independently. For instance, since the LLM service might require more GPU resources than the business logic, it can be scaled horizontally without impacting the other components. This optimizes resource usage and reduces costs, as different types of machines (e.g., GPU versus CPU) can be used according to each service’s needs.\n\nFor example, let’s assume that the LLM inference takes longer, so you will need more ML service replicas to meet the demand. But remember that GPU VMs are expensive. By decoupling the two components, you will run only what is required on the GPU machine and not block the GPU VM with other computing that can be done on a much cheaper machine.\n\nThus, by decoupling the components, you can scale horizontally as required, with minimal costs, providing a cost-effective solution to your system’s needs.",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1183,
      "content": "Figure 10.3: Scaling microservices independently based on compute requirements\n\nAdditionally, each microservice can adopt the most suitable technology stack, allowing teams to innovate and optimize independently.\n\nHowever, microservices introduce complexity in deployment and maintenance. Each service must be deployed, monitored, and maintained separately, which can be more challenging than managing a monolithic system.\n\nThe increased network communication between services can also introduce latency and potential points of failure, necessitating robust monitoring and resilience mechanisms.\n\nNote that the proposed design for decoupling the ML model and business logic into two services can be extended if necessary. For example, you can have one service for preprocessing the data, one for the model, and another for post-processing the data. Depending on the four pillars (latency, throughput, data, and infrastructure), you can get creative and design the most optimal architecture for your application needs.",
      "content_length": 1017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1184,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1185,
      "content": "Choosing between monolithic and microservices architectures\n\nThe choice between monolithic and microservices architectures for serving ML models largely depends on the application’s specific needs. A monolithic approach might be ideal for smaller teams or more straightforward applications where ease of development and maintenance is a priority. It’s also a good starting point for projects without frequent scaling requirements. Also, if the ML models are smaller, don’t require a GPU, or don’t require smaller and cheaper GPUs, the trade-off between reducing costs and complicating your infrastructure is worth considering.\n\nOn the other hand, microservices, with their adaptability and scalability, are well suited for larger, more complex systems where different components have varying scaling needs or require distinct tech stacks. This architecture is particularly advantageous when scaling specific system parts, such as GPU-intensive LLM services. As LLMs require powerful machines with GPUs, such as Nvidia A100, V100, or A10g, which are incredibly costly, microservices offer the flexibility to optimize the system for keeping these machines busy all the time or quickly scaling down when the GPU is idle. However, this flexibility comes at the cost of increased complexity in both development and operations.\n\nA common strategy is to start with a monolithic design and further decouple it into multiple services as the project grows. However, to successfully do so without making the transition too complex and costly, you must design the monolith application with this in mind. For instance, even if all the code runs on a single machine, you can completely decouple the modules of the application at the software level. This makes it easier to move these",
      "content_length": 1769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1186,
      "content": "modules to different microservices when the time comes. When working with Python, for example, you can implement the ML and business logic into two different Python modules that don’t interact with each other. Then, you can glue these two modules at a higher level, such as through a service class, or directly into the framework you use to expose your application over the internet, such as FastAPI.\n\nAnother option is to write the ML and business logic as two different Python packages that you glue together in the same ways as before. This is better because it completely enforces a separation between the two but adds extra complexity at development time. The main idea, therefore, is that if you start with a monolith and down the line you want to move to a microservices architecture, it’s essential to design your software with modularity in mind. Otherwise, if the logic is mixed, you will probably have to rewrite everything from scratch, adding tons of development time, which translates into wasted resources.\n\nIn summary, monolithic architectures offer simplicity and ease of maintenance but at the cost of flexibility and scalability. At the same time, microservices provide the agility to scale and innovate but require more sophisticated management and operational practices.",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1187,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1188,
      "content": "Exploring the LLM Twin’s inference pipeline deployment strategy\n\nNow that we’ve understood all the design choices available for implementing the deployment strategy of the LLM Twin’s inference pipeline, let’s explore the concrete decisions we made to actualize it.\n\nOur primary objective is to develop a chatbot that facilitates content creation. To achieve this, we will process requests sequentially, with a strong emphasis on low latency. This necessitates the selection of an online real-time inference deployment architecture.\n\nOn the monolith versus microservice aspect, we will split the ML service between a REST API server containing the business logic and an LLM microservice optimized for running the given LLM. As the LLM requires a powerful machine to run the inference, and we can further optimize it with various engines to speed up the latency and memory usage, it makes the most sense to go with the microservice architecture. By doing so, we can quickly adapt the infrastructure based on various LLM sizes. For example, if we run an 8B parameter model, the model can run on a single machine with a Nivida A10G GPU after quantization. But if we want to run a 30B model, we can upgrade to an Nvidia A100 GPU. Doing so allows us to upgrade only the LLM microservice while keeping the REST API untouched.\n\nAs illustrated in Figure 10.4, most business logic is centered around RAG in our particular use case. Thus, we will perform RAG’s retrieval and",
      "content_length": 1463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1189,
      "content": "augmentation parts within the business microservice. It will also include all the advanced RAG techniques presented in the previous chapter to optimize the pre-retrieval, retrieval, and post-retrieval steps.\n\nThe LLM microservice is strictly optimized for the RAG generation component. Ultimately, the business layer will send the prompt trace consisting of the user query, prompt, answer, and other intermediary steps to the prompt monitoring pipeline, which we will detail in Chapter 11.\n\nIn summary, our approach involves implementing an online real-time ML service using a microservice architecture, which effectively splits the LLM and business logic into two distinct services.",
      "content_length": 683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1191,
      "content": "Figure 10.4: Microservice deployment architecture of the LLM Twin’s inference pipeline\n\nLet’s review the interface of the inference pipeline, which is defined by the feature/training/inference (FTI) architecture. For the pipeline to run, it needs two things:\n\nReal-time features used for RAG, generated by the feature pipeline, which is queried from our online feature store, more concretely from the Qdrant vector database (DB)\n\nA fine-tuned LLM generated by the training pipeline, which is pulled from our model registry\n\nWith that in mind, the flow of the ML service looks as follows, as illustrated in Figure 10.4:\n\nA user sends a query through an HTTP request.\n\nThe user’s input retrieves the proper context by leveraging the advanced RAG retrieval module implemented in Chapter 4.",
      "content_length": 786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1192,
      "content": "The user’s input and retrieved context are packed into the final prompt using a dedicated prompt template.\n\nThe prompt is sent to the LLM microservice through an HTTP request.\n\nThe business microservices wait for the generated answer.\n\nAfter the answer is generated, it is sent to the prompt monitoring pipeline along with the user’s input and other vital information to monitor.\n\nUltimately, the generated answer is sent back to the user.\n\nNow, let’s explore what tech stack we used to implement the architecture presented in Figure 10.4. As we know, we use Qdrant for the vector DB. We will leverage Hugging Face for the model registry. By doing so, we can publicly share our model with everyone who is testing the code from this book. Thus, you can easily use the model we provided if you don’t want to run the training pipeline, which can cost up to 100 dollars. As you can see, shareability and accessibility are some of the most beautiful aspects of storing your model in a model registry.\n\nWe will implement the business microservice in FastAPI because it’s popular, easy to use, and fast. The LLM microservice will be deployed on AWS SageMaker, where we will leverage SageMaker’s integration with Hugging Face’s Deep Learning Containers (DLCs) to deploy the model. We will discuss Hugging Face’s DLCs in the next section, but",
      "content_length": 1333,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1193,
      "content": "intuitively, it is an inference engine used to optimize LLMs at serving time. The prompt monitoring pipeline is implemented using Comet, but we will look over that module only in Chapter 11.\n\nThe SageMaker Inference deployment is composed of the following components that we will show you how to implement:\n\nSageMaker endpoint: An endpoint is a scalable and secure API that SageMaker hosts to enable real-time predictions from deployed models. It’s essentially the interface through which applications interact with your model. Once deployed, an application can make HTTP requests to the endpoint to receive real-time predictions.\n\nSageMaker model: In SageMaker, a model is an artifact that results from training an algorithm. It contains the information required to make predictions, including the weights and computation logic. You can create multiple models and use them in different configurations or for various predictions.\n\nSageMaker configuration: This configuration specifies the hardware and software set up to host the model. It defines the resources required for the endpoint, such as the type and number of ML compute instances. Endpoint configurations are used when creating or updating an endpoint. They allow for flexibility in the deployment and scalability of the hosted models.\n\nSageMaker Inference component: This is the last piece of the puzzle that connects the model and configuration to anendpoint. You can",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1194,
      "content": "deploy multiple models to an endpoint, each with its resource configuration. Once deployed, models are easily accessible via the InvokeEndpoint API in Python.\n\nTogether, these components create a robust infrastructure for deploying and managing ML models in SageMaker, enabling scalable, secure, and efficient real-time predictions.\n\nOther popular cloud platforms offer the exact solutions. For example, you have Azure OpenAI instead of Bedrock and Azure ML instead of SageMaker on Azure. The list of ML deployment tools, such as Hopsworks, Modal, Vertex AI, Seldon, BentoML, and many more, is endless and will probably change. What is essential though is to understand your use case requirements and find a tool that fits your needs.",
      "content_length": 734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1195,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1196,
      "content": "The training versus the inference pipeline\n\nUnderstanding the nuances between the training and inference pipelines is crucial before we deploy the inference pipeline. While it might seem straightforward that the training pipeline is for training and the inference pipeline is for inference, there are significant differences that we need to grasp to comprehend the technical aspects of our discussion fully.\n\nOne key difference lies in how data is handled and accessed within each pipeline. During training, data is typically accessed from offline storage in batch mode, optimized for throughput and ensuring data lineage. For example, our LLM Twin architecture uses ZenML artifacts to access, version, and track data fed to the training loop in batches. In contrast, the inference pipeline requires an online DB optimized for low latency. We will leverage the Qdrant vector DB to grab the necessary context for RAG. In this context, the focus shifts from data lineage and versioning to quick data access, ensuring a seamless user experience. Additionally, the outputs of these pipelines also differ significantly. The training pipeline outputs trained model weights stored in the model registry. Meanwhile, the inference pipeline outputs predictions served directly to the user.\n\nAlso, the infrastructure required for each pipeline is different. The training pipeline demands more powerful machines equipped with as many GPUs as possible. This is because training involves batching data and holding all the necessary gradients in memory for optimization steps, making it highly compute-intensive. More computational power and VRAM allow larger batches (or throughput), reducing training time and enabling more extensive experimentation. On the other hand, the inference pipeline typically",
      "content_length": 1789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1197,
      "content": "requires less computation. Inference often involves passing a single sample or smaller batches to the model without the need for optimization steps.\n\nDespite these differences, there is some overlap between the two pipelines, particularly regarding preprocessing and post-processing steps. Applying the same preprocessing and post-processing functions and hyperparameters during training and inference is crucial. Any discrepancies can lead to what is known as training-serving skew, where the model’s performance during inference deviates from its performance during training.",
      "content_length": 577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1198,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1199,
      "content": "Deploying the LLM Twin service\n\nThe last step is implementing the architecture presented in the previous section. More concretely, we will deploy the LLM microservice using AWS SageMaker and the business microservice using FastAPI. Within the business microservice, we will glue the RAG logic written in Chapter 9 with our fine-tuned LLM Twin, ultimately being able to test out the inference pipeline end to end.\n\nServing the ML model is one of the most critical steps in any ML application’s life cycle, as users can only interact with our model after this phase is completed. If the serving architecture isn’t designed correctly or if the infrastructure isn’t working properly, it doesn’t matter that you have implemented a powerful and excellent model. As long as the user cannot appropriately interact with it, it has near zero value from a business point of view. For example, if you have the best code assistant on the market, but the latency to use it is too high, or the API calls keep crashing, the user will probably switch to a less performant code assistant that works faster and is more stable.\n\nThus, in this section, we will show you how to:\n\nDeploy our fined-tuned LLM Twin model to AWS SageMaker\n\nWrite an inference client to interact with the deployed model",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1200,
      "content": "Write the business service in FastAPI\n\nIntegrate our RAG logic with our fine-tuned LLM\n\nImplement autoscaling rules for the LLM microservice",
      "content_length": 140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1201,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1202,
      "content": "Implementing the LLM microservice using AWS SageMaker\n\nWe aim to deploy the LLM Twin model, stored in Hugging Face’s model registry, to Amazon SageMaker as an online real-time inference endpoint. We will leverage Hugging Face’s specialized inference container, known as the Hugging Face LLM DLC, to deploy our LLM.",
      "content_length": 314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1203,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1204,
      "content": "What are Hugging Face’s DLCs?\n\nDLCs are specialized Docker images that come pre-loaded with essential deep-learning frameworks and libraries, including popular tools like transformers, datasets, and tokenizers from Hugging Face. These containers are designed to simplify the process of training and deploying models by eliminating the need for complex environment setup and optimization. The Hugging Face Inference DLC, in particular, includes a fully integrated serving stack, significantly simplifying the deployment process and reducing the technical expertise needed to serve deep learning models in production.\n\nWhen it comes to serving models, the DLC is powered by the Text Generation Inference (TGI) engine, made by Hugging Face: https://github.com/huggingface/text-generation-inference.\n\nTGI is an open-source solution for deploying and serving LLMs. It offers high-performance text generation using tensor parallelism and dynamic batching for the most popular open-source LLMs available on Hugging Face, such as Mistral, Llama, and Falcon. To sum up, the most powerful features the DLC image provides are:\n\nTensor parallelism, thus enhancing the computational efficiency of model inference",
      "content_length": 1199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1205,
      "content": "Optimized transformers code for inference, leveraging flash-attention to maximize performance across the most widely used architectures: https://github.com/Dao-AILab/flash-attention\n\nQuantization with\n\nbitsandbytes\n\nthat reduces the model size while maintaining performance, making deployments more efficient: https://github.com/bitsandbytes- foundation/bitsandbytes\n\nContinuous batching of incoming requests, thus improving throughput by dynamically batching requests as they arrive\n\nAccelerated weight loading by utilizing\n\nsafetensors\n\nfor faster model initialization, reducing start-up time: https://github.com/huggingface/safetensors\n\nToken streaming that supports real-time interactions through Server- Sent Events (SSE)\n\nTo summarize, our LLM Twin model will run inside DLC Docker images, listening to requests, optimizing the LLM for inference, and serving the results in real time. The DLC’s Docker images will be hosted on AWS",
      "content_length": 936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1206,
      "content": "SageMaker under inference endpoints that can be accessed through HTTP requests. With that in mind, let’s move on to the implementation. We will start by deploying the LLM and then writing a wrapper class to interact with the SageMaker Inference endpoint.",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1207,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1208,
      "content": "Configuring SageMaker roles\n\nThe first step is to create the proper AWS Identity and Access Management (IAM) users and roles to access and deploy the SageMaker infrastructure. AWS IAM controls who can authenticate and what any actor has access to. You can create new users (assigned to people) and new roles (assigned to other actors within your infrastructure, such as EC2 VMs) through IAM.\n\nThe whole deployment process is automated. We will have to run a few CLI commands, but first, ensure that you have correctly configured the\n\nAWS_ACCESS_KEY\n\n,\n\nAWS_SECRET_KEY\n\n, and\n\nAWS_REGION\n\nenvironmental variables in the\n\n.env\n\nfile. At this step, the easiest way is to use the credentials attached to an admin role as, in the following steps, we will create a set of narrower IAM roles used in the rest of the chapter.",
      "content_length": 817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1209,
      "content": "After you configured your\n\n.env\n\nfile, we have to:\n\nCreate an IAM user restricted to creating and deleting only the resources we need for the deployment, such as SageMaker itself, Elastic Container Registry (ECR), and S3. To make it, run the following:\n\npoetry poe create-sagemaker-role\n\nThis command will generate a JSON file called\n\nsagemaker_user_credentials.json\n\nthat contains a new AWS access and secret key. From now on, we will use these credentials to deploy everything related to SageMaker to ensure we modify only the resources associated with SageMaker. Otherwise, we could accidentally modify other AWS resources using an admin account, resulting in additional costs or altering other existing projects. Thus, having a narrow role only to your use case is good practice.\n\nThe last step is to take the new credentials from the JSON file and update the",
      "content_length": 863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1210,
      "content": "AWS_ACCESS_KEY\n\nand\n\nAWS_SECRET_KEY\n\nvariables in your\n\n.env\n\nfile. You can check out the implementation at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_sage maker_role.py.\n\nCreate an IAM execution role. We will attach this role to the SageMaker deployment, empowering it to access other AWS resources on our behalf. This is standard practice for cloud deployments, as instead of authenticating every machine within your credentials, you attach a role that allows them to access only what is necessary from your infrastructure. In our case, we will provide SageMaker access to AWS S3, CloudWatch, and ECR. To create the role, run the following:\n\npoetry poe create-sagemaker-execution-role\n\nThis command will generate a JSON file called",
      "content_length": 807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1211,
      "content": "sagemaker_execution_role.json\n\nthat contains the Amazon Resource Name (ARN) of the newly created role. The ARN is an ID attached to any AWS resource to identify it across your cloud infrastructure. Take the ARN value from the JSON file and update the\n\nAWS_ARN_ROLE\n\nvariable from your\n\n.env\n\nfile with it. You can check out the implementation at https://github.com/PacktPublishing/LLM-Engineers- Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_exec ution_role.py.\n\nIf you have issues, configure the AWS CLI with the same AWS credentials as in the\n\n.env\n\nfile and repeat the process. Official documentation for installing the AWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html.\n\nBy setting the IAM user and role in your\n\n.env\n\nfile, we will automatically load them in the settings Python object and use them throughout the following steps. Now, let’s move on to the actual",
      "content_length": 916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1212,
      "content": "deployment.",
      "content_length": 11,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1213,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1214,
      "content": "Deploying the LLM Twin model to AWS SageMaker\n\nThe deployment of AWS SageMaker is fully automated through a set of Python classes, which we will cover in this chapter. This section aims to understand how we configure the SageMaker infrastructure directly from Python. Thus, you don’t have to run everything step by step, as in a standard tutorial, but only to understand the code.\n\nWe can initiate and finalize the entire SageMaker deployment using a simple CLI command:\n\npoe deploy-inference-endpoint\n\n. This command will initialize all the steps presented in Figure 10.5, except for creating the SageMaker AWS IAMs we created and configured in the previous step.\n\nIn this section, we will walk you through the code presented in Figure 10.5 that helps us fully automate the deployment process, starting with the\n\ncreate_endpoint()\n\nfunction. Ultimately, we will test the CLI command and check the AWS console to see whether the deployment was successful. The SageMaker deployment code is available at https://github.com/PacktPublishing/LLM- Engineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy.",
      "content_length": 1113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1216,
      "content": "Figure 10.5: AWS SageMaker deployment steps\n\nWe will take a top-down approach to walk you through the implementation, starting with the main function that deploys the LLM Twin model to AWS SageMaker. In the function below, we first take the latest version of the Docker DLC image using the\n\nget_huggingface_llm_image_uri()\n\nfunction, which is later passed to the deployment strategy class, along with an instance of the resource manager and deployment service:\n\ndef\n\ncreate_endpoint\n\n(\n\nendpoint_type=EndpointType.INFERENCE_COMPONENT_BASED\n\n): llm_image = get_huggingface_llm_image_uri(\n\n\"huggingface\"\n\n, version=\n\nNone\n\n) resource_manager = ResourceManager() deployment_service = DeploymentService(resource_manager=resource_manager) SagemakerHuggingfaceStrategy(deployment_service).deploy( role_arn=settings.ARN_ROLE, llm_image=llm_image, config=hugging_face_deploy_config,",
      "content_length": 874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1217,
      "content": "endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, endpoint_config_name=settings.SAGEMAKER_ENDPOINT_CONFIG_I NFERENCE, gpu_instance_type=settings.GPU_INSTANCE_TYPE, resources=model_resource_config, endpoint_type=endpoint_type, )\n\nWe must review the three classes used in the\n\ncreate_endpoint()\n\nfunction to fully understand the deployment process. Let’s start with the\n\nResourceManager\n\nclass. The class begins with the initialization method, establishing the connection to AWS SageMaker using boto3, the AWS SDK for Python, which provides the necessary functions to interact with various AWS services, including SageMaker.\n\nclass\n\nResourceManager\n\n:\n\ndef\n\n__init__\n\n(\n\nself",
      "content_length": 676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1218,
      "content": ") ->\n\nNone\n\n:\n\nself\n\n.sagemaker_client = boto3.client(\n\n\"sagemaker\"\n\n, region_name=settings.AWS_REGION, aws_access_key_id=settings.AWS_ACCESS_KEY, aws_secret_access_key=settings.AWS_SECRET_KEY, )\n\nNext, we implement the\n\nendpoint_config_exists\n\nmethod, checking whether a specific SageMaker endpoint configuration exists:\n\ndef\n\nendpoint_config_exists\n\n(\n\nself, endpoint_config_name:",
      "content_length": 382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1219,
      "content": "str\n\n) ->\n\nbool\n\n:\n\ntry\n\n:\n\nself\n\n.sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoi nt_config_name) logger.info(\n\nf\"Endpoint configuration '\n\n{endpoint_config_name}\n\n' exists.\"\n\n)\n\nreturn\n\nTrue\n\nexcept\n\nClientError: logger.info(\n\nf\"Endpoint configuration '\n\n{endpoint_config_name}\n\n' does not exist.\"\n\n)",
      "content_length": 322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1220,
      "content": "return\n\nFalse\n\nThe class also includes the\n\nendpoint_exists\n\nmethod, which checks the existence of a specific SageMaker endpoint:\n\ndef\n\nendpoint_exists\n\n(\n\nself, endpoint_name:\n\nstr\n\n) ->\n\nbool\n\n:\n\ntry\n\n:\n\nself",
      "content_length": 210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1221,
      "content": ".sagemaker_client.describe_endpoint(EndpointName=endpoint_name) logger.info(\n\nf\"Endpoint '\n\n{endpoint_name}\n\n' exists.\"\n\n)\n\nreturn\n\nTrue\n\nexcept\n\nself\n\n.sagemaker_client.exceptions.ResourceNotFoundException: logger.info(\n\nf\"Endpoint '\n\n{endpoint_name}\n\n' does not exist.\"\n\n)\n\nreturn\n\nFalse\n\nLet’s move to the",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1222,
      "content": "DeploymentService\n\n. Within the constructor, we set up the\n\nsagemaker_client\n\n, which will interface with AWS SageMaker and an instance of the\n\nResourceManager\n\nclass we talked about earlier:\n\nclass\n\nDeploymentService\n\n:\n\ndef\n\n__init__\n\n(\n\nself, resource_manager\n\n):\n\nself\n\n.sagemaker_client = boto3.client(\n\n\"sagemaker\"",
      "content_length": 320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1223,
      "content": ", region_name=settings.AWS_REGION, aws_access_key_id=settings.AWS_ACCESS_KEY, aws_secret_access_key=settings.AWS_SECRET_KEY, )\n\nself\n\n.resource_manager = resource_manager\n\nThe\n\ndeploy()\n\nmethod is the heart of the\n\nDeploymentService\n\nclass. This method orchestrates the entire process of deploying a model to a SageMaker endpoint. It checks whether the necessary configurations are already in place and, if not, it triggers the deployment:\n\ndef\n\ndeploy\n\n(\n\nself,\n\nrole_arn:\n\nstr",
      "content_length": 478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1224,
      "content": ",\n\nllm_image:\n\nstr\n\n,\n\nconfig:\n\ndict\n\n,\n\nendpoint_name:\n\nstr\n\n,\n\nendpoint_config_name:\n\nstr\n\n,\n\ngpu_instance_type:\n\nstr\n\n,\n\nresources:\n\nOptional\n\n[\n\ndict",
      "content_length": 153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1225,
      "content": "] =\n\nNone\n\n,\n\nendpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n\n) ->\n\nNone\n\n:\n\ntry\n\n:\n\nif\n\nself\n\n.resource_manager.endpoint_config_exists(endpoint_config_name=endpoin t_config_name): logger.info(\n\nf\"Endpoint configuration\n\n{endpoint_config_name}\n\nexists. Using existing configuration...\"\n\n)\n\nelse\n\n: logger.info(\n\nf\"Endpoint configuration\n\n{endpoint_config_name}",
      "content_length": 366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1226,
      "content": "does not exist.\"\n\n)\n\nself\n\n.prepare_and_deploy_model( role_arn=role_arn, llm_image=llm_image, config=config, endpoint_name=endpoint_name, update_endpoint=\n\nFalse\n\n, resources=resources, endpoint_type=endpoint_type, gpu_instance_type=gpu_instance_type, ) logger.info(\n\nf\"Successfully deployed/updated model to endpoint\n\n{endpoint_name}\n\n.\"\n\n)\n\nexcept\n\nException\n\nas\n\ne: logger.error(\n\nf\"Failed to deploy model to SageMaker:\n\n{e}\n\n\"\n\n)\n\nraise",
      "content_length": 440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1227,
      "content": "The deploy method begins by checking whether the endpoint configuration already exists using the\n\nresource_manager\n\n. This step is crucial because it avoids unnecessary redeployment if the configuration is already set up. The deployment itself is handled by calling the\n\nprepare_and_deploy_model()\n\nmethod, which is responsible for the actual deployment of the model to the specified SageMaker endpoint.\n\nThe\n\nprepare_and_deploy_model()\n\nmethod is a static method within the\n\nDeploymentService\n\nclass. This method is focused on setting up and deploying the Hugging Face model to SageMaker:\n\n@staticmethod\n\ndef",
      "content_length": 609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1228,
      "content": "prepare_and_deploy_model\n\n(\n\nrole_arn:\n\nstr\n\n,\n\nllm_image:\n\nstr\n\n,\n\nconfig:\n\ndict\n\n,\n\nendpoint_name:\n\nstr\n\n,\n\nupdate_endpoint:\n\nbool\n\n,\n\ngpu_instance_type:\n\nstr\n\n,",
      "content_length": 163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1229,
      "content": "resources:\n\nOptional\n\n[\n\ndict\n\n] =\n\nNone\n\n,\n\nendpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n\n) ->\n\nNone\n\n: huggingface_model = HuggingFaceModel( role=role_arn, image_uri=llm_image, env=config, transformers_version=\n\n\"4.6\"\n\n, pytorch_version=\n\n\"1.13\"\n\n, py_version=\n\n\"py310\"\n\n, ) huggingface_model.deploy( instance_type=gpu_instance_type, initial_instance_count=\n\n1",
      "content_length": 370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1230,
      "content": ", endpoint_name=endpoint_name, update_endpoint=update_endpoint, resources=resources, tags=[{\n\n\"Key\"\n\n:\n\n\"task\"\n\n,\n\n\"Value\"\n\n:\n\n\"model_task\"\n\n}], endpoint_type=endpoint_type, )\n\nThis method begins by creating an instance of HuggingFaceModel, a specialized model class from SageMaker designed to handle Hugging Face models. The constructor for HuggingFaceModel takes several essential parameters, such as the role ARN (which gives SageMaker the necessary permissions), the URI of the LLM DLC Docker image, and the LLM configuration that specifies what LLM to load from Hugging Face and its inference parameters, such as the maximum total of tokens.\n\nOnce HuggingFaceModel is instantiated, the method deploys it to SageMaker using the deploy function. This deployment process involves specifying the type of instance used, the number of instances, and whether to update an existing endpoint or create a new one. The method also includes optional resources for more complex deployments, such as the",
      "content_length": 994,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1231,
      "content": "initial_instance_count\n\nparameter for multi-model endpoints and tags for tracking and categorization.\n\nThe last step is to walk you through the\n\nSagemakerHuggingfaceStrategy\n\nclass, which aggregates everything we have shown. The class is initialized only with an instance of a deployment service, such as the one shown above.\n\nclass\n\nSagemakerHuggingfaceStrategy\n\n(\n\nDeploymentStrategy\n\n):\n\ndef\n\n__init__\n\n(\n\nself, deployment_service\n\n):",
      "content_length": 437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1232,
      "content": "self\n\n.deployment_service = deployment_service\n\nThe core functionality of the\n\nSagemakerHuggingfaceStrategy\n\nclass is encapsulated in its\n\ndeploy()\n\nmethod. This method orchestrates the deployment process, taking various parameters that define how the Hugging Face model should be deployed to AWS SageMaker:\n\ndef\n\ndeploy\n\n(\n\nself,\n\nrole_arn:\n\nstr\n\n,\n\nllm_image:",
      "content_length": 361,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1233,
      "content": "str\n\n,\n\nconfig:\n\ndict\n\n,\n\nendpoint_name:\n\nstr\n\n,\n\nendpoint_config_name:\n\nstr\n\n,\n\ngpu_instance_type:\n\nstr\n\n,\n\nresources:\n\nOptional\n\n[\n\ndict\n\n] =\n\nNone",
      "content_length": 149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1234,
      "content": ",\n\nendpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n\n) ->\n\nNone\n\n: logger.info(\n\n\"Starting deployment using Sagemaker Huggingface Strategy...\"\n\n) logger.info(\n\nf\"Deployment parameters: nb of replicas:\n\n{settings.COPIES}\n\n, nb of gpus:\n\n{settings.GPUS}\n\n, instance_type:\n\n{settings.GPU_INSTANCE_TYPE}\n\n\"\n\n)\n\nThe parameters passed into the method are crucial to the deployment process:",
      "content_length": 387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1235,
      "content": "role_arn\n\n: The AWS IAM role that provides permissions for the SageMaker deployment.\n\nllm_image\n\n: The URI of the DLC Docker image\n\nconfig\n\n: A dictionary containing configuration settings for the model environment.\n\nendpoint_name\n\nand\n\nendpoint_config_name\n\n: Names for the SageMaker endpoint and its configuration, respectively.\n\ngpu_instance_type\n\n: The type of the GPU EC2 instances used for the deployment.\n\nresources\n\n: Optional resources dictionary used for multi-model endpoint deployments.",
      "content_length": 498,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1236,
      "content": "endpoint_type\n\n: This can either be\n\nMODEL_BASED\n\nor\n\nINFERENCE_COMPONENT\n\n, determining whether the endpoint includes an inference component.\n\nThe method delegates the actual deployment process to the\n\ndeployment_service\n\n. This delegation is a critical aspect of the strategy pattern, allowing for flexibility in how the deployment is carried out without altering the high- level deployment logic.\n\ntry\n\n:\n\nself\n\n.deployment_service.deploy( role_arn=role_arn, llm_image=llm_image, config=config, endpoint_name=endpoint_name, endpoint_config_name=endpoint_config_name, gpu_instance_type=gpu_instance_type, resources=resources, endpoint_type=endpoint_type, ) logger.info(",
      "content_length": 671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1237,
      "content": "\"Deployment completed successfully.\"\n\n)\n\nexcept\n\nException\n\nas\n\ne: logger.error(\n\nf\"Error during deployment:\n\n{e}\n\n\"\n\n)\n\nraise\n\nAlso, let’s review the resource configuration to understand the infrastructure better. These resources are leveraged when setting up multi- endpoint configurations that use multiple replicas to serve clients while respecting the latency and throughput requirements of the application. The\n\nResourceRequirements\n\nobject is initialized with a dictionary that specifies various resource parameters. These parameters include the number of replicas (copies) of the model to be deployed, the number of GPUs required, the number of CPU cores, and the memory allocation in megabytes. Each of these parameters plays a crucial role in the performance and scalability of the deployed model.",
      "content_length": 807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1238,
      "content": "from\n\nsagemaker.compute_resource_requirements.resource_requirements\n\nimport\n\nResourceRequirements model_resource_config = ResourceRequirements( requests={\n\n\"copies\"\n\n: settings.COPIES,\n\n\"num_accelerators\"\n\n: settings.GPUS\n\n\"num_cpus\"\n\n: settings.CPUS,\n\n\"memory\"\n\n:\n\n5\n\n\n\n1024\n\n}, )",
      "content_length": 281,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1239,
      "content": "In the preceding snippet,\n\nResourceRequirements\n\nis configured with four key parameters:\n\ncopies: This parameter determines how many instances or replicas of the model should be deployed. Having multiple replicas can help in reducing latency and increasing throughput.\n\nnum_accelerators: This parameter specifies the number of GPUs to allocate. Since LLMs are computationally intensive, multiple GPUs are typically required to accelerate inference processes.\n\nnum_cpus: This defines the number of CPU cores the deployment should have. The number of CPUs impacts the model’s ability to handle data preprocessing, post-processing, and other tasks that are less GPU-dependent but still essential.\n\nmemory: The memory parameter sets the minimum amount of RAM required for the deployment. Adequate memory is necessary to ensure the model can load and operate without running into memory shortages.\n\nBy setting these parameters, the class ensures that it has sufficient resources to operate efficiently when the model is deployed to a SageMaker endpoint. The precise tuning of these values will vary depending on the LLM’s",
      "content_length": 1116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1240,
      "content": "specific requirements, such as its size, the complexity of the tasks it will perform, and the expected load. To get a better understanding of how to use them, after deploying the endpoint, we suggest modifying them and seeing how the performance of the LLM microservice changes.\n\nUltimately, let’s review the settings configuring the LLM engine. The\n\nHF_MODEL_ID\n\nidentifies which Hugging Face model to deploy. For example, in the settings class, we set it to\n\nmlabonne/TwinLlama-3.1-8B-13\n\nto load our custom LLM Twin model stored in Hugging Face.\n\nSM_NUM_GPUS\n\nspecifies the number of GPUs allocated per model replica, which is crucial for fitting your model into the GPU’s VRAM.\n\nHUGGING_FACE_HUB_TOKEN\n\nprovides access to the Hugging Face Hub for model retrieval.\n\nHF_MODEL_QUANTIZE\n\nspecifies what quantization technique to use, while the rest of the variables control the LLM token generation process.\n\nhugging_face_deploy_config = {\n\n\"HF_MODEL_ID\"",
      "content_length": 954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1241,
      "content": ": settings.HF_MODEL_ID,\n\n\"SM_NUM_GPUS\"\n\n: json.dumps(settings.SM_NUM_GPUS),\n\n# Number of GPU used per replica\n\n\"MAX_INPUT_LENGTH\"\n\n: json.dumps(settings.MAX_INPUT_LENGTH),\n\n# Max length of input text\n\n\"\n\nMAX_TOTAL_TOKENS\"\n\n: json.dumps(settings.MAX_TOTAL_TOKENS),\n\n# Max length of the generation (including input text)\n\n\"MAX_BATCH_TOTAL_TOKENS\"\n\n: json.dumps(settings.MAX_BATCH_TOTAL_TOKENS),\n\n\"HUGGING_FACE_HUB_TOKEN\"\n\n: settings.HUGGINGFACE_ACCESS_TOKEN,\n\n\"MAX_BATCH_PREFILL_TOKENS\"\n\n:\n\n\"10000\"\n\n, \"HF_MODEL_QUANTIZE\":\n\n\"bitsandbytes\"",
      "content_length": 536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1242,
      "content": ", }\n\nUsing these two configurations, we fully control our infrastructure, what LLM to use, and how it behaves. To start the SageMaker deployment with the configuration shown above, call the\n\ncreate_endpoint()\n\nfunction (presented at the beginning of the section) as follows:\n\ncreate_endpoint(endpoint_type=EndpointType.MODEL_BASED)\n\nFor convenience, we also wrapped it up under a\n\npoe\n\ncommand:\n\npoetry poe deploy-inference-endpoint",
      "content_length": 432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1243,
      "content": "That’s all you need to deploy an inference pipeline to AWS SageMaker. The hardest part is finding the correct configuration to fit your needs while reducing your infrastructure’s costs. Depending on AWS, this will take up to 15-30 minutes to deploy. You can always change any value directly from your\n\n.env\n\nfile and deploy the model with a different configuration without touching the code. For example, our default values use a single GPU instance of type\n\nml.g5.xlargeGPU\n\n. If you want more replicas, you can tweak the\n\nGPUS\n\nand\n\nSM_NUM_GPUS\n\nsettings or change your instance type by changing the\n\nGPU_INSTANCE_TYPE\n\nvariable.\n\nBefore deploying the LLM microservice to AWS SageMaker, ensure that you’ve generated a user role by running\n\npoetry poe create-sagemaker-role\n\nand an execution role by running\n\npoetry poe create-sagemaker-execution-role",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1244,
      "content": ". Also, ensure you update your\n\nAWS_*\n\nenvironment variables in your\n\n.env\n\nfile with the credentials generated by the two steps. You can find more details on this aspect in the repository’s README file.\n\nAfter deploying the AWS SageMaker Inference endpoint, you can navigate to the SageMaker dashboard in AWS to visualize it. First, in the left panel, click on SageMaker dashboard, and then in the Inference column, click on the Endpoints button, as illustrated in Figure 10.6.\n\nFigure 10.6: AWS SageMaker Inference endpoints example",
      "content_length": 534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1245,
      "content": "After clicking the Endpoints button, you will see your twin endpoint in a Creating or Created status, as seen in Figure 10.7. After clicking on it, you can look at the endpoint’s logs in CloudWatch and monitor the CPU, memory, disk, and GPU utilization.\n\nAlso, they provide an excellent way to monitor all the HTTP errors, such as\n\n4XX\n\nand\n\n5XX\n\n, in one place.\n\nFigure 10.7: AWS SageMaker twin inference endpoint example",
      "content_length": 422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1246,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1247,
      "content": "Calling the AWS SageMaker Inference endpoint\n\nNow that our LLM service has been deployed on AWS SageMaker, let’s learn how to call the service. To do so, we will write two classes that will help us prepare the prompt for SageMaker, call the inference endpoint through HTTP requests, and decode the results in a way the client can work with. All the AWS SageMaker Inference code is available on GitHub at\n\nllm_engineering/model/inference\n\n. It all starts with the following example:\n\ntext =\n\n\"Write me a post about AWS SageMaker inference endpoints.\"\n\nllm = LLMInferenceSagemakerEndpoint( endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE ) Answer = InferenceExecutor(llm, text).execute()\n\nAs before, we will walk you through the\n\nLLMInferenceSagemakerEndpoint\n\nand",
      "content_length": 766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1248,
      "content": "InferenceExecutor\n\nclasses. Let’s start with the\n\nLLMInferenceSagemakerEndpoint\n\nclass, which directly interacts with SageMaker. The constructor initializes all the essential attributes necessary to interact with the SageMaker endpoint:\n\nclass\n\nLLMInferenceSagemakerEndpoint\n\n(\n\nInference\n\n):\n\ndef\n\n__init__\n\n(\n\nself,\n\nendpoint_name:\n\nstr\n\n,\n\ndefault_payload:",
      "content_length": 359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1249,
      "content": "Optional\n\n[\n\nDict\n\n[\n\nstr\n\n,\n\nAny\n\n]] =\n\nNone\n\n,\n\ninference_component_name:\n\nOptional\n\n[\n\nstr\n\n] =\n\nNone\n\n,\n\n) ->\n\nNone\n\n:",
      "content_length": 122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1250,
      "content": "super\n\n().__init__()\n\nself\n\n.client = boto3.client(\n\n\"sagemaker-runtime\"\n\n,\n\nregion_name=settings.AWS_REGION,\n\naws_access_key_id=settings.AWS_ACCESS_KEY,\n\naws_secret_access_key=settings.AWS_SECRET_KEY,\n\n)\n\nself\n\n.endpoint_name = endpoint_name\n\nself\n\n.payload = default_payload\n\nif\n\ndefault_payload\n\nelse\n\nself\n\n._default_payload()\n\nself",
      "content_length": 336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1251,
      "content": ".inference_component_name = inference_component_name\n\nendpoint_name\n\nis crucial for identifying the SageMaker endpoint we want to request. Additionally, the method initializes the payload using a provided value or by calling a method that generates a default payload if none is provided.\n\nOne of the key features of the class is its ability to generate a default payload for inference requests. This is handled by the\n\n_default_payload()\n\nmethod:\n\ndef\n\n_default_payload\n\n(\n\nself\n\n) ->\n\nDict\n\n[",
      "content_length": 493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1252,
      "content": "str\n\n,\n\nAny\n\n]:\n\nreturn\n\n{\n\n\"inputs\"\n\n:\n\n\"\"\n\n,\n\n\"parameters\"\n\n: {\n\n\"max_new_tokens\"\n\n: settings.MAX_NEW_TOKENS_INFERENCE,\n\n\"top_p\"\n\n: settings.TOP_P_INFERENCE,\n\n\"temperature\"\n\n: settings.TEMPERATURE_INFERENCE,\n\n\"return_full_text\"\n\n:",
      "content_length": 232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1253,
      "content": "False\n\n, }, }\n\nThis method returns a dictionary that represents the default structure of the payload to be sent for inference. The parameters section includes settings that influence the model’s behavior during inference, such as the number of tokens to generate, the sampling strategy (\n\ntop_p\n\n), and the temperature setting, which controls randomness in the output. These parameters are fetched from the application’s settings, ensuring consistency across different inference tasks.\n\nThe class allows customization of the payload through the\n\nset_payload()\n\nmethod, which enables the user to modify the inputs and parameters before sending an inference request:\n\ndef\n\nset_payload\n\n(\n\nself, inputs:",
      "content_length": 700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1254,
      "content": "str\n\n, parameters:\n\nOptional\n\n[\n\nDict\n\n[\n\nstr\n\n,\n\nAny\n\n]] =\n\nNone\n\n) ->\n\nNone\n\n:\n\nself\n\n.payload[\n\n\"inputs\"\n\n] = inputs\n\nif\n\nparameters:",
      "content_length": 136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1255,
      "content": "self\n\n.payload[\n\n\"parameters\"\n\n].update(parameters)\n\nThis method updates the\n\ninputs\n\nfield of the payload with the new input text provided by the user. Additionally, it allows for modifying inference parameters if any are provided.\n\nUltimately, we leverage the\n\ninference()\n\nmethod to call the SageMaker endpoint with the customized payload:\n\ndef\n\ninference\n\n(\n\nself",
      "content_length": 367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1256,
      "content": ") ->\n\nDict\n\n[\n\nstr\n\n,\n\nAny\n\n]:\n\ntry\n\n: logger.info(\n\n\"Inference request sent.\"\n\n) invoke_args = {\n\n\"EndpointName\"\n\n:\n\nself\n\n.endpoint_name,\n\n\"ContentType\"\n\n:\n\n\"application/json\"\n\n,\n\n\"Body\"",
      "content_length": 188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1257,
      "content": ": json.dumps(\n\nself\n\n.payload), }\n\nif\n\nself\n\n.inference_component_name\n\nnot\n\nin\n\n[\n\n\"None\"\n\n,\n\nNone\n\n]: invoke_args[\n\n\"InferenceComponentName\"\n\n] =\n\nself\n\n.inference_component_name response =\n\nself\n\n.client.invoke_endpoint(**invoke_args) response_body = response[\n\n\"Body\"",
      "content_length": 271,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1258,
      "content": "].read().decode(\n\n\"utf8\"\n\n)\n\nreturn\n\njson.loads(response_body)\n\nexcept\n\nException: logger.exception(\n\n\"SageMaker inference failed.\"\n\n)\n\nraise\n\nIn this method, the inference method constructs the request to be sent to the SageMaker endpoint. The method packages the payload and other necessary details into a format SageMaker expects. If an\n\ninference_component_name\n\nis specified, it is included in the request, allowing for more granular control over the inference process if needed. The request is sent using the\n\ninvoke_endpoint()\n\nfunction, and the response is read, decoded, and returned as a JSON object.",
      "content_length": 610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1259,
      "content": "Let’s understand how the\n\nInferenceExecutor\n\nuses the\n\nLLMInferenceSagemakerEndpoint\n\nclass we previously presented to send HTTP requests to the AWS SageMaker endpoint.\n\nThe\n\nInferenceExecutor\n\nclass begins with the constructor, which inputs key parameters for calling the LLM. The\n\nllm\n\nparameter accepts any instance that implements the Inference interface, such as the\n\nLLMInferenceSagemakerEndpoint\n\nclass, which is used to perform the inference.\n\nAlso, it accepts the query parameter, which represents the user input. Ultimately, it takes an optional context field if you want to do RAG, and you can customize the prompt template. If no prompt template is provided, it will default to a generic version that is not specialized in any LLM:",
      "content_length": 743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1260,
      "content": "class\n\nInferenceExecutor\n\n:\n\ndef\n\n__init__\n\n(\n\nself,\n\nllm: Inference,\n\nquery:\n\nstr\n\n,\n\ncontext:\n\nstr\n\n|\n\nNone\n\n=\n\nNone\n\n,\n\nprompt:\n\nstr",
      "content_length": 135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1261,
      "content": "|\n\nNone\n\n=\n\nNone\n\n,\n\n) ->\n\nNone\n\n:\n\nself\n\n.llm = llm\n\nself\n\n.query = query\n\nself\n\n.context = context\n\nif\n\ncontext\n\nelse\n\n\"\"\n\nif\n\nprompt",
      "content_length": 135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1262,
      "content": "is\n\nNone\n\n:\n\nself\n\n.prompt =\n\n\"\"\"\n\nYou are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.\n\nUser query: {query}\n\nContext: {context}\n\n\"\"\"\n\nelse\n\n:\n\nself\n\n.prompt = prompt\n\nThe\n\nexecute()\n\nmethod is the key component of the",
      "content_length": 312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1263,
      "content": "InferenceExecutor\n\nclass. This method is responsible for actually performing the inference. When execute is called, it prepares the payload sent to the LLM by formatting the prompt with the user’s query and context.\n\nThen, it configures several parameters that influence the behavior of the LLM, such as the maximum number of new tokens the model is allowed to generate, a repetition penalty to discourage the model from generating repetitive text, and the temperature setting that controls the randomness of the output.\n\nOnce the payload and parameters are set, the method calls the\n\ninference\n\nfunction from\n\nLLMInferenceSagemakerEndpoint\n\nand waits for the generated answer:\n\ndef\n\nexecute\n\n(\n\nself\n\n) ->",
      "content_length": 706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1264,
      "content": "str\n\n:\n\nself\n\n.llm.set_payload( inputs=\n\nself\n\n.prompt.\n\nformat\n\n(query=\n\nself\n\n.query, context=\n\nself\n\n.context), parameters={\n\n\"max_new_tokens\"\n\n: settings.MAX_NEW_TOKENS_INFERENCE,\n\n\"repetition_penalty\"\n\n:\n\n1.1\n\n,\n\n\"temperature\"\n\n: settings.TEMPERATURE_INFERENCE, }, ) answer =",
      "content_length": 280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1265,
      "content": "self\n\n.llm.inference()[\n\n0\n\n][\n\n\"generated_text\"\n\n]\n\nreturn\n\nanswer\n\nBy making the inference through an object that implements the Inference interface we decouple, we can easily inject other Inference strategies and the\n\nLLMInferenceSagemakerEndpoint\n\nimplementation presented above without modifying different parts of the code.\n\nRunning a test example is straightforward. Simply call the following Python file, as shown below:\n\npoetry run python -m llm_engineering.model.inference.test",
      "content_length": 487,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1266,
      "content": "Also, for convenience, we wrap it under a\n\npoe\n\ncommand:\n\npoetry poe test-sagemaker-endpoint\n\nNow, we must understand how we implement the business microservice using FastAPI. This microservice will send HTTP requests to the LLM microservice defined above and call the RAG retrieval module implemented in Chapter 9.",
      "content_length": 315,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1267,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1268,
      "content": "Building the business microservice using FastAPI\n\nTo implement a simple FastAPI application that proves our deployment strategy, we first have to define a FastAPI instance as follows:\n\nfrom\n\nfastapi\n\nimport\n\nFastAPI app = FastAPI()\n\nNext, we define the\n\nQueryRequest\n\nand\n\nQueryResponse\n\nclasses using Pydantic’s\n\nBaseModel",
      "content_length": 323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1269,
      "content": ". These classes represent the request and response structure for the FastAPI endpoints:\n\nclass\n\nQueryRequest\n\n(\n\nBaseModel\n\n): query:\n\nstr\n\nclass\n\nQueryResponse\n\n(\n\nBaseModel\n\n): answer:\n\nstr\n\nNow that we’ve defined our FastAPI components and have all the SageMaker elements in place, let’s reiterate over the\n\ncall_llm_service()",
      "content_length": 329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1270,
      "content": "and\n\nrag()\n\nfunctions we’ve presented in Chapter 9 and couldn’t run because we haven’t deployed our fine-tuned LLM. Thus, as a refresher, the\n\ncall_llm_service()\n\nfunction wraps the inference logic used to call the SageMaker LLM microservice:\n\ndef\n\ncall_llm_service\n\n(\n\nquery:\n\nstr\n\n, context:\n\nstr\n\n|\n\nNone\n\n) ->\n\nstr",
      "content_length": 318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1271,
      "content": ": llm = LLMInferenceSagemakerEndpoint( endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_component_name=\n\nNone\n\n) answer = InferenceExecutor(llm, query, context).execute()\n\nreturn\n\nanswer\n\nNext, we define the\n\nrag()\n\nfunction that implements all the RAG business logic. To avoid repeating ourselves, check Chapter 9 for the complete function explanation. What is important to highlight is that the\n\nrag()\n\nfunction only implements the business steps required to do RAG, which are CPU- and I/O-bounded. For example, the\n\nContextRetriever\n\nclass makes API calls to OpenAI and Qdrant, which are network I/O bounded, and calls the embedding model, which runs directly on the CPU. Also, as the LLM inference logic is moved to a different microservice, the\n\ncall_llm_service()\n\nfunction is only network I/O bounded. To conclude, the whole function is light to run, where the heavy computing is done on other services, which",
      "content_length": 930,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1272,
      "content": "allows us to host the FastAPI server on a light and cheap machine that doesn’t need a GPU to run at low latencies:\n\ndef\n\nrag\n\n(\n\nquery:\n\nstr\n\n) ->\n\nstr\n\n: retriever = ContextRetriever(mock=\n\nFalse\n\n) documents = retriever.search(query, k=\n\n3\n\n\n\n3\n\n) context = EmbeddedChunk.to_context(documents) answer = call_llm_service(query, context)\n\nreturn\n\nanswer",
      "content_length": 353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1273,
      "content": "Ultimately, we define the\n\nrag_endpoint()\n\nfunction, used to expose our RAG logic over the internet as an HTTP endpoint. We use a Python decorator to expose it as a POST endpoint in the FastAPI application. This endpoint is mapped to the\n\n/rag\n\nroute and expects a\n\nQueryRequest\n\nas input. The function processes the request by calling the rag function with the user’s query. If successful, it returns the answer wrapped in a\n\nQueryResponse\n\nobject. If an exception occurs, it raises an HTTP 500 error with the exception details:\n\n@app.post(\n\n\"/rag\"\n\n, response_model=QueryResponse\n\n)\n\nasync",
      "content_length": 591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1274,
      "content": "def\n\nrag_endpoint\n\n(\n\nrequest: QueryRequest\n\n):\n\ntry\n\n: answer = rag(query=request.query)\n\nreturn\n\n{\n\n\"answer\"\n\n: answer}\n\nexcept\n\nException\n\nas\n\ne:\n\nraise\n\nHTTPException(status_code=\n\n500\n\n, detail=\n\nstr",
      "content_length": 204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1275,
      "content": "(e))\n\nfrom\n\ne\n\nThis FastAPI application demonstrates how to effectively integrate an LLM hosted on AWS SageMaker into a web service, utilizing RAG to enhance the relevance of the model’s responses. The code’s modular design, leveraging custom classes like\n\nContextRetriever\n\n,\n\nInferenceExecutor\n\n, and\n\nLLMInferenceSagemakerEndpoint\n\n, allows for easy customization and scalability, making it a powerful tool for deploying ML models in production environments.\n\nWe will leverage the\n\nuvicorn\n\nweb server, the go-to method for FastAPI applications, to start the server. To do so, you have to run the following:",
      "content_length": 610,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1276,
      "content": "uvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload\n\nAlso, you can run the following\n\npoe\n\ncommand to achieve the same:\n\npoetry poe run-inference-ml-service\n\nTo call the\n\n/rag\n\nendpoint, we can leverage the\n\ncurl\n\nCLI command to make a POST HTTP request to our FastAPI server, as follows:",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1277,
      "content": "curl -X POST 'http://127.0.0.1:8000/rag' -H 'Content-Type: application/json' -d '{\\\"query\\\": \\\"your_query \\\"}'\n\nAs usual, we provided an example using a\n\npoe\n\ncommand that contains an actual user query:\n\npoetry poe call-inference-ml-service\n\nThis FastAPI server runs only locally. The next step would be to deploy it to AWS Elastic Kubernetes Service (EKS), a self-hosted version of Kubernetes by AWS. Another option would be to deploy it to AWS Elastic Container Service (ECS), which is similar to AWS EKS but doesn’t use Kubernetes under the hood but AWS’s implementation. Unfortunately, this is not specific to LLMs or LLMOps. Hence, we won’t go through these steps in this book. But to get an idea of what you must do, you must create an AWS EKS/ECS cluster from the dashboard or leverage an infrastructure-as-code (IaC) tool such as Terraform. After that, you will have to Dockerize the FastAPI code presented above. Ultimately, you would have to push the Docker image to AWS ECR and create an ECS/EKR deployment using the Docker image hosted on ECR. If this sounds like a lot, the good news is that we will walk you through a similar example in Chapter 11, where we will deploy the ZenML pipelines to AWS.",
      "content_length": 1211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1278,
      "content": "Once you’re done testing your inference pipeline deployment, deleting all your AWS SageMaker resources used to deploy the LLM is essential. As almost all AWS resources use a pay-as-you-go strategy, using SageMaker for a few hours wouldn’t break your wallet, but if you forget and leave it open, in a few days, the costs can grow exponentially. Thus, a good rule of thumb is to always delete everything after you’re done testing your SageMaker infrastructure (or any AWS resource). Luckily, we have provided a script that deletes all the AWS SageMaker resources for you:\n\npoetry poe delete-inference-endpoint\n\nTo ensure everything was correctly deleted, go to your SageMaker dashboard and check it yourself.",
      "content_length": 706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1279,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1280,
      "content": "Autoscaling capabilities to handle spikes in usage\n\nSo far, the SageMaker LLM microservice has used a static number of replicas to serve our users, which means that all the time, regardless of the traffic, it has the same number of instances up and running. As we highlighted throughout this book, machines with GPUs are expensive. Thus, we lose a lot of money during downtime when most replicas are idle. Also, if our application has sudden spikes in traffic, the application will perform poorly as the server cannot handle the number of requests. This is a massive problem for the user experience of our application, as in those spikes, we bring in the majority of new users. Thus, if they have a terrible impression of our product, we significantly reduce their chance of returning to our platform.\n\nPreviously, we configured our multi-endpoint service using the\n\nResourceRequirements\n\nclass from SageMaker. For example, let’s assume we requested four copies (replicas) with the following compute requirements:\n\nmodel_resource_config = ResourceRequirements( requests={\n\n\"copies\"\n\n:\n\n4",
      "content_length": 1087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1281,
      "content": ",\n\n# Number of replicas.\n\n\"num_accelerators\"\n\n:\n\n4\n\n,\n\n# Number of GPUs required.\n\n\"num_cpus\"\n\n:\n\n8\n\n,\n\n# Number of CPU cores required.\n\n\"memory\"\n\n:\n\n5\n\n\n\n1024\n\n,\n\n# Minimum memory required in Mb (required)\n\n}, )",
      "content_length": 212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1282,
      "content": "Using this configuration, we always have four replicas serving the clients, regardless of idle time or spikes in traffic. The solution is to implement an autoscaling strategy that scales the number of replicas up and down dynamically based on various metrics, such as the number of requests.\n\nFor example, Figure 10.8 shows a standard architecture where the SageMaker Inference endpoints scale in and out based on the number of requests. When there is no traffic, we can have one online replica so the server remains responsive to new user requests or even scales down to zero if the latency is not super critical. Then, let’s assume that when we have around 10 requests per second, we have to keep two replicas online, and when the number of requests spikes to 100 per second, the autoscaling service should spin up to 20 replicas to keep up with the demand. Note that these are fictional numbers that should be adapted to your specific use case.",
      "content_length": 947,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1283,
      "content": "Figure 10.8: Autoscaling possible use cases",
      "content_length": 43,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1284,
      "content": "Without going into the little details of cloud networking, when working with multi-replica systems, between the client and the replicas sits an Application Load Balancer (ALB) or another type of load balancer.\n\nAll the requests first go to the ALB, which knows to route them to a replica. The ALB can adopt various routing strategies, where the simplest one is called round robin, which sequentially sends a request to each replica. For example, the first request is routed to replica one, the second to replica two, and so on. Taking this approach, regardless of how many replicas you have online, the endpoint that the client calls is always represented by the load balancer that acts as an entry point into your cluster. Thus, adding or removing new replicas doesn’t affect the server and client communication protocol.\n\nLet’s quickly learn how to implement an autoscaling strategy for our AWS SageMaker Inference endpoint. SageMaker provides a feature called Application Auto Scaling that allows you to scale resources dynamically based on pre-defined policies. Two foundational steps are involved in effectively leveraging this functionality: registering a scalable target and creating a scalable policy.",
      "content_length": 1209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1285,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1286,
      "content": "Registering a scalable target\n\nThe first step in enabling autoscaling for your resources is to register a scalable target with the Application Auto Scaling feature AWS provides. Think of this as informing AWS about the specific resource you intend to scale, as well as setting the boundaries within which the scaling should occur. However, this step does not dictate how or when the scaling should happen.\n\nFor instance, when working with SageMaker Inference components, you’ll define the following:\n\nResource ID: This serves as a unique identifier for the resource you want to scale, typically including the name of the SageMaker Inference component.\n\nService namespace: This identifies the AWS service the resource belongs to, which, in this case, is SageMaker.\n\nScalable dimension: This specifies the resources to be scaled, such as the desired number of copies.\n\nMinCapacity and MaxCapacity: These parameters define the boundaries of the autoscaling strategies, such as minimum and",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1287,
      "content": "maximum limits of the number of replicas.\n\nBy registering a scalable target, you prepare your SageMaker Inference component for future scaling actions without determining when or how these actions should occur.",
      "content_length": 210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1288,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1289,
      "content": "Creating a scalable policy\n\nOnce your scalable target is registered, the next step is defining how the scaling should occur. This is where creating a scaling policy comes in. A scaling policy defines specific rules that trigger scaling events. When creating policies, you have to define metrics to know what to monitor and thresholds to know when to emit scaling events.\n\nIn the context of our SageMaker Inference component, the scalable policy might include the following elements:\n\nPolicy type: For instance, you might select TargetTrackingScaling, a policy that adjusts the resource’s capacity to maintain a specific target value for a chosen metric.\n\nTarget tracking configuration: This involves selecting the metric to monitor (such as SageMakerInferenceComponentInvocationsPerCopy), setting the desired target value, and specifying cooldown periods that control how quickly scaling actions can occur after previous ones.\n\nThe scaling policy defines the rules for your scaling-in and scaling-out strategy. It constantly monitors the specified metric, and depending on whether the metric exceeds or falls below the target value, it triggers actions",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1290,
      "content": "to scale the number of inference component copies up or down, always within the limits defined by the registered scalable target.\n\nLet’s explain in more depth how the TargetTrackingScaling policy works. Imagine you have a metric that represents the ideal average utilization or throughput level for your application. With target tracking, you select this metric and set a target value that reflects the optimal state for your application. Once defined, Application Auto Scaling creates and manages the necessary CloudWatch alarms to monitor this metric. When deviations occur, scaling actions are triggered, similar to how a thermostat adjusts to maintain a consistent room temperature.\n\nFor instance, consider an application running on SageMaker. Let’s assume we set a target of keeping GPU utilization around 70 percent. This target allows you to maintain enough headroom to manage sudden traffic spikes while preventing the unnecessary cost of idle resources. When GPU usage exceeds the target, the system scales out, adding resources to manage the increased load. Conversely, when GPU usage drops below the target, the system scales in, reducing capacity to minimize costs during quieter periods.\n\nOne significant advantage of setting up target tracking policies using Application Auto Scaling is that they simplify the scaling process. You no longer need to configure CloudWatch alarms and define scaling adjustments manually.",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1291,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1292,
      "content": "Minimum and maximum scaling limits\n\nWhen setting up autoscaling for your SageMaker Inference endpoints, it’s crucial to establish your maximum and minimum scaling limits before creating your scaling policy. The minimum value represents the least resources your model can operate with. This value must be at least 1, ensuring that your model always has some capacity.\n\nNext, configure the maximum value, which defines the upper limit of resources your model can scale up to. While the maximum must be equal to or greater than the minimum value, it doesn’t impose any upper limit. Thus, you can scale up as much as your application needs within the boundaries of what AWS can provide.",
      "content_length": 682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1293,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1294,
      "content": "Cooldown period\n\nAnother important aspect of a scaling policy is the cooldown period, during which it’s crucial to maintain a balance between responsiveness and stability. This cooldown period acts as a safeguard, ensuring that your system doesn’t overreact during scaling events—whether it’s reducing capacity (scaling in) or increasing it (scaling out). By introducing a calculated pause, the cooldown period prevents rapid fluctuations in the number of instances. Specifically, it delays the removal of instances during scale-in requests and restricts the creation of new replicas during scale-out requests. This strategy helps maintain a stable and efficient environment for LLM service.\n\nThese practical basics are used in autoscaling most web servers, including online real-time ML servers. Once you understand how to configure scaling policies for SageMaker, you can immediately apply the strategies you’ve learned to other popular deployment tools like Kubernetes or AWS ECS.\n\nFor a step-by-step guideline on how to configure autoscaling for the AWS SagaMaker endpoint implemented in this chapter, you can follow this official tutorial from AWS: https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling- prerequisites.html.",
      "content_length": 1244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1295,
      "content": "Autoscaling is a critical component in any cloud architecture, but there are some pitfalls you should be aware of. The first and most dangerous one is over-scaling, which directly impacts the costs of your infrastructure. If your scaling policy or cooldown period is too sensitive, you may be uselessly spinning up new machines that will remain idle or with the resources underused. The second reason is on the other side of the spectrum, where your system doesn’t scale enough, resulting in a bad user experience for the end user.\n\nThat’s why a good practice is to understand the requirements of your system. Based on them, you should tweak and experiment with the autoscaling parameters in a dev or test environment until you find the sweet spot (similar to hyperparameter tuning when training models). Let’s suppose, for instance, that you expect your system to support an average of 100 users per minute and scale up to 10,000 users per minute in case of an outlier event such as a holiday. Using this spec, you can stress test your system and monitor your resources to find the best trade-off between costs, latency, and throughput that supports standard and outlier use cases.",
      "content_length": 1182,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1296,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1297,
      "content": "Summary\n\nIn this chapter, we learned what design decisions to make before serving an ML model, whether an LLM or not, by walking you through the three fundamental deployment types for ML models: online real-time inference, asynchronous inference, and offline batch transform. Then, we considered whether building our ML-serving service as a monolith application made sense or splitting it into two microservices, such as an LLM microservice and a business microservice. To do this, we weighed the pros and cons of a monolithic versus microservices architecture in model-serving.\n\nNext, we walked you through deploying our fine-tuned LLM Twin to an AWS SageMaker Inference endpoint. We also saw how to implement the business microservice using FastAPI, which consists of all the RAG steps based on the retrieval module implemented in Chapter 9 and the LLM microservice deployed on AWS SageMaker. Ultimately, we explored why we have to implement an autoscaling strategy. We also reviewed a popular autoscaling strategy that scales in and out based on a given set of metrics and saw how to implement it in AWS SageMaker.\n\nIn the next chapter, we will learn about the fundamentals of MLOps and LLMOps and then explore how to deploy the ZenML pipelines to AWS and implement a continuous training, continuous integration, and continuous delivery (CT/CI/CD) and monitoring pipeline.",
      "content_length": 1375,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1298,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1299,
      "content": "References\n\nAWS Developers. (2023, September 22). Machine Learning in 15: Amazon SageMaker High-Performance Inference at Low Cost [Video]. YouTube. https://www.youtube.com/watch?v=FRbcb7CtIOw\n\nbitsandbytes-foundation. (n.d.). GitHub—bitsandbytes- foundation/bitsandbytes: Accessible large language models via k-bit quantization for PyTorch. GitHub. https://github.com/bitsandbytes- foundation/bitsandbytes\n\nDifference between IAM role and IAM user in AWS. (n.d.). Stack Overflow. https://stackoverflow.com/questions/46199680/difference-between-iam-role- and-iam-user-in-aws\n\nHuggingface. (n.d.-a). GitHub—huggingface/safetensors: Simple, safe way to store and distribute tensors. GitHub. https://github.com/huggingface/safetensors\n\nHuggingface. (n.d.-b). GitHub—huggingface/text-generation-inference: Large Language Model Text Generation Inference. GitHub. https://github.com/huggingface/text-generation-inference",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1300,
      "content": "Huyen, C. (n.d.). Designing machine learning systems. O’Reilly Online Learning. https://www.oreilly.com/library/view/designing-machine- learning/9781098107956/\n\nIusztin, P. (2024, August 20). Architect LLM & RAG inference pipelines | Decoding ML. Medium. https://medium.com/decodingml/architect- scalable-and-cost-effective-llm-rag-inference-pipelines-73b94ef82a99\n\nLakshmanan, V., Robinson, S., and Munn, M. (n.d.). Machine Learning design patterns. O’Reilly Online Learning. https://www.oreilly.com/library/view/machine-learning- design/9781098115777/\n\nMendoza, A. (2024, August 21). Best tools for ML model Serving. neptune.ai. https://neptune.ai/blog/ml-model-serving-best-tools\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 828,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1302,
      "content": "11",
      "content_length": 2,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1303,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1304,
      "content": "MLOps and LLMOps\n\nThroughout the book, we’ve already used machine learning operations (MLOps) components and principles such as a model registry to share and version our fined-tuned large language models (LLMs), a logical feature store for our fine-tuning and RAG data, and an orchestrator to glue all our ML pipelines together. But MLOps is not just about these components; it takes an ML application to the next level by automating data collection, training, testing, and deployment. Thus, the end goal of MLOps is to automate as much as possible and let users focus on the most critical decisions, such as when a change in distribution is detected and a decision must be taken on whether it is essential to retrain the model or not. But what about LLM operations (LLMOps)? How does it differ from MLOps?\n\nThe term LLMOps is a product of the widespread adoption of LLMs. It is built on top of MLOps, which is built on top of development operations (DevOps). Thus, to fully understand what LLMOps is about, we must provide a historical context, starting with DevOps and building on the term from there—which is precisely what this chapter will do. At its core, LLMOps focuses on problems specific to LLMs, such as prompt monitoring and versioning, input and output guardrails to prevent toxic behavior, and feedback loops to gather fine-tuning data. It also focuses on scaling issues that appear when working with LLMs, such as collecting trillions of tokens for training datasets, training models on massive GPU clusters, and reducing infrastructure costs. Fortunately for the common folk, these issues are solved mainly by a few companies that fine-tune foundational models, such as Meta, which provides the Llama family of models. Most companies will adopt these pre-trained foundational models for their use cases, focusing on LLMOps problems such as prompt monitoring and versioning.",
      "content_length": 1889,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1305,
      "content": "On the implementation side of things, to add LLMOps to our LLM Twin use case, we will deploy all our ZenML pipelines to AWS. We will implement a continuous integration and continuous deployment (CI/CD) pipeline to test the integrity of our code and automate the deployment process, a continuous training (CT) pipeline to automate our training, and a monitoring pipeline to track all our prompts and generated answers. This is a natural progression in any ML project, regardless of whether you use LLMs.\n\nIn previous chapters, you learned how to build an LLM application. Now, it’s time to explore three main goals related to LLMOps. The first one is to gain a theoretical understanding of LLMOps, starting with DevOps, then moving to the fundamental principles of MLOps, and finally, digging into LLMOps. We don’t aim to provide the whole theory on DevOps, MLOps, and LLMOps, as you could easily write an entire book on these topics. However, we want to build a strong understanding of why we make certain decisions when implementing the LLM Twin use case.\n\nOur second goal is to deploy the ZenML pipelines to AWS (currently, we’ve deployed only our inference pipeline to AWS in Chapter 10). This section will be hands-on, showing you how to leverage ZenML to deploy everything to AWS. We need this to implement our third and last goal, which is to apply what we’ve learned in the theory section to our LLM Twin use case. We will implement a CI/CD pipeline using GitHub Actions, a CT and alerting pipeline using ZenML, and a monitoring pipeline using Opik from Comet ML.\n\nThus, in this chapter, we will cover the following topics:",
      "content_length": 1630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1306,
      "content": "The path to LLMOps: Understanding its roots in DevOps and MLOps\n\nDeploying the LLM Twin’s pipelines to the cloud\n\nAdding LLMOps to the LLM Twin",
      "content_length": 143,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1307,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1308,
      "content": "The path to LLMOps: Understanding its roots in DevOps and MLOps\n\nTo understand LLMOps, we have to start with the field’s beginning, which is DevOps, as it inherits most of its fundamental principles from there. Then, we will move to MLOps to understand how the DevOps domain was adapted to support ML systems. Finally, we will explain what LLMOps is and how it emerged from MLOps after the widespread adoption of LLMs.",
      "content_length": 418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1309,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1310,
      "content": "DevOps\n\nManually shipping software is time-consuming, error-prone, involves security risks, and doesn’t scale. Thus, DevOps was born to automate the process of shipping software at scale. More specifically, DevOps is used in software development, where you want to completely automate your building, testing, deploying, and monitoring components. It is a methodology designed to shorten the development lifecycle and ensure continuous delivery of high-quality software. It encourages collaboration, automates processes, integrates workflows, and implements rapid feedback loops. These elements contribute to a culture where building, testing, and releasing software becomes more reliable and faster.\n\nEmbracing a DevOps culture offers significant advantages to an organization, primarily boosting operational efficiency, speeding up feature delivery, and enhancing product quality. Some of the main benefits include:\n\nImproved collaboration: DevOps is pivotal in creating a more unified working environment. Eliminating the barriers between development and operations teams fosters enhanced communication and teamwork, leading to a more efficient and productive workplace.\n\nBoosted efficiency: Automating the software development lifecycle reduces manual tasks, errors, and delivery times.",
      "content_length": 1289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1311,
      "content": "Ongoing improvement: DevOps is not just about internal processes. It’s about ensuring that the software effectively meets user needs. Promoting a culture of continuous feedback enables teams to quickly adapt and enhance their processes, thereby delivering software that genuinely satisfies the end users.\n\nSuperior quality and security: DevOps ensures swift software development while maintaining high quality and security standards through CI/CD and proactive security measures.",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1312,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1313,
      "content": "The DevOps lifecycle\n\nAs illustrated in Figure 11.1, the DevOps lifecycle encompasses the entire journey from the inception of software development to its delivery, upkeep, and security. The key stages of this lifecycle are:\n\nPlan: Organize and prioritize the tasks, ensuring each is tracked to completion.\n\nCode: Collaborate with your team to write, design, develop, and securely manage code and project data.\n\nBuild: Package your applications and dependencies into an executable format.\n\nTest: This stage is crucial. It’s where you confirm that your code functions correctly and meets quality standards, ideally through automated testing.\n\nRelease: If the tests pass, flag the tested build as a new release, which is now ready to be shipped.",
      "content_length": 743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1314,
      "content": "Deploy: Deploy the latest release to the end users.\n\nOperate: Manage and maintain the infrastructure on which the software runs effectively once it is live. This involves scaling, security, data management, and backup and recovery.\n\nMonitor: Track performance metrics and errors to reduce the severity and frequency of incidents.\n\nFigure 11.1: DevOps lifecycle steps",
      "content_length": 366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1315,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1316,
      "content": "The core DevOps concepts\n\nDevOps encompasses various practices throughout the application lifecycle, but the core ones that we will touch on throughout this book are:\n\nDeployment environments: To thoroughly test your code before shipping it to production, you must define multiple pre-production environments that mimic the production one. The most common approach is to create a dev environment where the developers can test their latest features. Then, you have a staging environment where the QA team and stakeholders tinker with the application to find bugs and experience the latest features before they ship to the users. Lastly, we have the production environment, which is exposed to end users.\n\nVersion control: Used to track, manage, and version every change made to the source code. This allows you to have complete control over the evolution of the code and deployment processes. For example, without versioning, tracking changes between the dev, staging, and production environments would be impossible. By versioning your software, you always know what version is stable and ready to be shipped.\n\nContinuous integration (CI): Before pushing the code into the dev, staging, and production main branches, you automatically build your application and run automated tests on each change. After all the automated tests pass, the feature branch can be merged into the main one.",
      "content_length": 1385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1317,
      "content": "Continuous delivery (CD): Continuous delivery works in conjunction with CI and automates the infrastructure provisioning and application deployment steps. For example, after the code is merged into the staging environment, the application with the latest changes will be automatically deployed on top of your staging infrastructure. After, the QA team (or stakeholders) starts manually testing the latest features to verify that they work as expected. These two steps are commonly referred to together as CI/CD.\n\nNote that DevOps suggests a set of core principles that are platform/tool agnostic. However, within our LLM Twin use case, we will add a version control layer using GitHub, which aims to track the evolution of the code. Another popular tool for version control is GitLab. To implement the CI/CD pipeline, we will leverage the GitHub ecosystem and GitHub Actions, which are free for open-source projects. Other tool choices are GitLab CI/CD, CircleCI, and Jenkins. Usually, you pick the DevOps tool based on your development environment, customization, and privacy needs. For example, Jenkins is an open-source DevOps tool you can host yourself and control fully. The downside is that you must host and maintain it yourself, adding a complexity layer. Thus, many companies choose what works best with their version control ecosystem, such as GitHub Actions or GitLab CI/CD.\n\nNow that we’ve established a solid understanding of DevOps, let’s explore how the MLOps field has emerged to keep these same core principles in the AI/ML world.",
      "content_length": 1547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1318,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1319,
      "content": "MLOps\n\nAs you might have worked out by now, MLOps tries to apply the DevOps principles to ML. The core issue is that an ML application has many other moving parts compared to a standard software application, such as the data, model, and, finally, the code. MLOps aims to track, operationalize, and monitor all these concepts for better reproducibility, robustness, and control.\n\nIn ML systems, a build can be triggered by any change in these areas— whether it’s an update in the code, modifications in the data, or adjustments to the model.",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1320,
      "content": "Figure 11.2: Relationship between data, model, and code changes\n\nIn DevOps, everything is centered around the code. For example, when a new feature is added to the codebase, you have to trigger the CI/CD pipeline. In MLOps, the code can remain unchanged while only the data changes. In that case, you must train (or fine-tune) a new model, resulting in a new dataset and model version. Intuitively, when one component changes, it affects one or more of the others. Thus, MLOps has to take into consideration all this extra complexity. Here are a few examples that can trigger a change in the data and indirectly in the model:",
      "content_length": 625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1321,
      "content": "After deploying the ML model, its performance might decay as time passes, so we need new data to retrain it.\n\nAfter understanding how to collect data in the real world, we might recognize that getting the data for our problem is challenging, so we need to re-formulate it to work with our real-world setup.\n\nWhile in the experimentation stage and training the model, we often must collect more data or re-label it, which generates a new set of models.\n\nAfter serving the model in the production environment and collecting feedback from the end users, we might recognize that the assumptions we made for training the model are wrong, so we must change our model.\n\nSo, what is MLOps?\n\nA more official definition of MLOps is the following: MLOps is the extension of the DevOps field that makes data and models their first-class citizen while preserving the DevOps methodology.\n\nLike DevOps, MLOps originates from the idea that isolating ML model development from its deployment process (ML operations) diminishes the system’s overall quality, transparency, and agility. With that in mind, an",
      "content_length": 1088,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1322,
      "content": "optimal MLOps experience treats ML assets consistently as other software assets within a CI/CD environment as part of a cohesive release process.",
      "content_length": 145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1323,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1324,
      "content": "MLOps core components\n\nWe have already used all of these components throughout the book, but let’s have a quick refresher on the MLOps core components now that we better understand the field. Along with source control and CI/CD, MLOps revolves around:\n\nModel registry: A centralized repository for storing trained ML models (tools:Comet ML, W&B, MLflow, ZenML)\n\nFeature store: Preprocessing and storing input data as features for both model training and inference pipelines (tools:Hopsworks, Tecton, Featureform)\n\nML metadata store: This store tracks information related to model training, such as model configurations, training data, testing data, and performance metrics. It is mainly used to compare multiple models and look at the model lineages to understand how they were created (tools:Comet ML, W&B, MLflow)\n\nML pipeline orchestrator: Automating the sequence of steps in ML projects (tools:ZenML, Airflow, Prefect, Dagster)",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1325,
      "content": "You might have noticed an overlap between the MLOps components and its specific tooling. This is common, as most MLOps tools offer unified solutions, often called MLOps platforms.",
      "content_length": 179,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1326,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1327,
      "content": "MLOps principles\n\nSix core principles guide the MLOps field. These are independent of any tool and sit at the core of building robust and scalable ML systems.\n\nThey are:\n\nAutomation or operationalization: Automation in MLOps involves transitioning from manual processes to automated pipelines through CT and CI/CD. This enables the efficient retraining and deployment of ML models in response to triggers such as new data, performance drops, or unhandled edge cases. Moving from manual experimentation to full automation ensures that our ML systems are robust, scalable, and adaptable to changing requirements without errors or delays.\n\nVersioning: In MLOps, it is crucial to track changes in code, models, and data individually, ensuring consistency and reproducibility. Code is tracked using tools like Git, models are versioned through model registries, and data versioning can be managed using solutions like DVC or artifact management systems.\n\nExperiment tracking: As training ML models is an iterative and experimental process that involves comparing multiple experiments based on predefined metrics, using an experiment tracker to help us pick the best model is important. Tools like Comet ML, W&B, MLflow,",
      "content_length": 1214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1328,
      "content": "and Neptune allow us to log all necessary information to compare experiments easily and select the best model for production.\n\nTesting: MLOps suggests that along with testing your code, you should also test your data and models through unit, integration, acceptance, regression, and stress tests. This ensures that each component functions correctly and integrates well, focusing on inputs, outputs, and handling edge cases.\n\nMonitoring: This stage is vital for detecting performance degradation in served ML models due to changes in production data, allowing timely intervention such as retraining, further prompt or feature engineering, or data validation. By tracking logs, system metrics, and model metrics and detecting drifts, we can maintain the health of ML systems in production, detect issues as fast as possible, and ensure they continue to deliver accurate results.\n\nReproducibility: This ensures that every process (such as training or feature engineering) within your ML systems produces identical results when given the same input by tracking all the moving variables, such as code versions, data versions, hyperparameters, or any other type of configurations. Due to the non-deterministic nature of ML training and inference, setting well- known seeds when generating pseudo-random numbers is essential to achieving consistent outcomes and making processes as deterministic as possible.\n\nIf you want to learn more, we’ve offered an in-depth exploration of these principles in the Appendix at the end of this book.",
      "content_length": 1529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1329,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1330,
      "content": "ML vs. MLOps engineering\n\nThere is a fine line between ML engineering and MLOps. If we want to define a rigid job description for the two rules, it cannot be easy to completely differentiate what responsibilities go into ML engineering (MLE) and what goes into MLOps. I have seen many job roles that bucket the MLOps role with the platform and cloud engineers. From one perspective, that makes a lot of sense: as an MLOps engineer, you have a lot of work to do on the infrastructure side. On the other hand, as seen in this section, an MLOps engineer still has to implement things such as experiment tracking, model registries, versioning, and more. A good strategy would be to let the ML engineer integrate these into the code and the MLOps engineer focus on making them work on their infrastructure.\n\nAt a big corporation, ultimately, differentiating the two roles might make sense. But when working in small to medium-sized teams, you will wear multiple hats and probably work on the ML system’s MLE and MLOps aspects.",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1331,
      "content": "Figure 11.3: DS vs. MLE vs. MLOps\n\nFor instance, in Figure 11.3, we see a clear division of responsibilities among the three key roles: data scientist/ML researcher, ML engineer, and MLOps engineer. The Data Scientist (DS) implements specific models to address problems.\n\nThe ML engineer takes the functional models from the DS team and constructs a layer on top of them, making them modular and",
      "content_length": 395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1332,
      "content": "extendable and providing access to a database (DB) or exposing them as an API over the internet. However, the MLOps engineer plays a pivotal role in this process. They take the code from this intermediate layer and place it on a more generic layer, the infrastructure. This action marks the application’s transition to production. From this point, we can start thinking about automation, monitoring, versioning, and more.\n\nThe intermediate layer differentiates a proof of concept from an actual product. In that layer, you design an extendable application that has a state by integrating a DB and is accessible over the internet through an API. When shipping the application on a specific infrastructure, you must consider scalability, latency, and cost-effectiveness. Of course, the intermediate and generic layers depend on each other, and often, you must reiterate to meet the application requirements.",
      "content_length": 905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1333,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1334,
      "content": "LLMOps\n\nLLMOps encompasses the practices and processes essential for managing and running LLMs. This field is a specialized branch of MLOps, concentrating on the unique challenges and demands associated with LLMs. While MLOps addresses the principles and practices of managing various ML models, LLMOps focuses on the distinct aspects of LLMs, including their large size, highly complex training requirements, prompt management, and non-deterministic nature of generating answers. However, note that at its core, LLMOps still inherits all the fundamentals presented in the MLOps section. Thus, here, we will focus on what it adds on top.\n\nWhen training LLMs from scratch, the data and model dimensions of an ML system grow substantially, which is one aspect that sets LLMOps apart from MLOps. These are the main concerns when training LLMs from scratch:\n\nData collection and preparation involves collecting, preparing, and managing the massive datasets required for training LLMs. It involves big data techniques for processing, storing, and sharing training datasets. For example, GPT-4 was trained on roughly 13 trillion tokens, equal to approximately 10 trillion words.\n\nManaging LLMs’considerable number of parameters is a significant technical challenge from the infrastructure’s point of view. It requires vast computation resources, usually clusters of machines powered by Nvidia GPUs with CUDA support.",
      "content_length": 1410,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1335,
      "content": "The massive size of LLMs directly impacts model training. When training an LLM from scratch, you can’t fit it on a single GPU due to the model’s size or the higher batch size you require for the expected results. Thus, you need multi-GPU training, which involves optimizing your processes and infrastructure to support data, model, or tensor parallelism.\n\nManaging massive datasets and multi-GPU clusters involves substantial costs. For example, the estimated training cost for GPT-4 is around $100 million, as stated by Sam Altman, the CEO of OpenAI (https://en.wikipedia.org/wiki/GPT-4#Training). Add to that the costs of multiple experiments, evaluation, and inference. Even if these numbers are not exact, as the sources are not 100% reliable, the scale of the costs of training an LLM is trustworthy, which implies that only the large players in the industry can afford to train LLMs from scratch.\n\nAt its core, LLMOps is MLOps at scale. It uses the same MLOps principles but is applied to big data and huge models that require more computing power to train and run. However, due to its huge scale, the most significant trend is the shift away from training neural networks from scratch for specific tasks. This approach is becoming obsolete with the rise of fine- tuning, especially with the advent of foundation models such as GPT. A few organizations with extensive computational resources, such as OpenAI and Google, develop these foundation models. Thus, most applications now rely on the lightweight fine-tuning of parts of these models, prompt engineering, or optionally distilling data or models into smaller, specialized inference networks.",
      "content_length": 1654,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1336,
      "content": "Thus, for most LLM applications out there, your development steps will involve the selection of a foundation model, which you further have to optimize by using prompt engineering, fine-tuning, or RAG. Thus, the operational aspect of these three steps is the most critical to understand. Let’s dive into some popular components of LLMOps that can improve prompt engineering, fine-tuning, and RAG.",
      "content_length": 395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1337,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1338,
      "content": "Human feedback\n\nOne valuable refinement step of your LLM is aligning it with your audience’s preferences. You must introduce a feedback loop within your application and gather a human feedback dataset to further fine- tune the LLM with techniques such as Reinforcement Learning with Human Feedback (RLHF) or more advanced ones such as Direct Preference Optimization (DPO). One popular feedback loop is the thumbs-up/thumbs-down button present in most chatbot interfaces. You can read more on preference alignment in Chapter 6.",
      "content_length": 526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1339,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1340,
      "content": "Guardrails\n\nUnfortunately, LLM systems are not reliable, as they often hallucinate. You can optimize your system against hallucinations, but as hallucinations are hard to detect and can take many forms, there are significant changes that will still happen in the future.\n\nMost users have accepted this phenomenon, but what is not acceptable is when LLMs accidentally output sensitive information, such as GitHub Copilot outputting AWS secret keys or other chatbots providing people’s passwords. This can also happen with people’s phone numbers, addresses, email addresses, and more. Ideally, you should remove all this sensitive data from your training data so the LLM doesn’t memorize it, but that doesn’t always happen.\n\nLLMs are well known for producing toxic and harmful outputs, such as sexist and racist outputs. For example, during an experiment on ChatGPT around April 2023, people found how to hijack the system by forcing the chatbot to adopt a negative persona, such as “a bad person” or “a horrible person.” It worked even by forcing the chatbot to play the role of well- known negative characters from our history, such as dictators or criminals. For example, this is what ChatGPT produced when impersonating a bad person:",
      "content_length": 1235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1341,
      "content": "X is just another third-world country with nothing but drug lords and poverty-stricken people. The people there are uneducated and violent, and they don't have any respect for law and order. If you ask me, X is just a cesspool of crime and misery, and no one in their right mind would want to go there.\n\nCheck the source of the experiment for more examples of different personas: https://techcrunch.com/2023/04/12/researchers-discover-a-way- to-make-chatgpt-consistently-toxic/.\n\nThe discussion can be extended to a never-ending list of examples, but the key takeaway is that your LLM can produce harmful output or receive dangerous input, so you should monitor and prepare for them. Thus, to create safe LLM systems, you must protect them against harmful, sensitive, or invalid input and output by adding guardrails:\n\nInput guardrails:Input guardrails primarily protect against three main risks: exposing private information to external APIs, executing harmful prompts that could compromise your system (model jailbreaking), and accepting violent or unethical prompts. When it comes to leaking private information to external APIs, the risk is specific to sending sensitive data outside your organization, such as credentials or classified information. When talking about model jailbreaking, we mainly refer to prompt injection, such as executing malicious SQL code that can access, delete, or corrupt your data. Lastly, some applications don’t want to accept violent or unethical queries from users, such as asking an LLM how to build a bomb.",
      "content_length": 1544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1342,
      "content": "Output guardrails: At the output of an LLM response, you want to catch failed outputs that don’t respect your application’s standards. This can vary from one application to another, but some examples are empty responses (these responses don’t follow your expected format, such as JSON or YAML), toxic responses, hallucinations, and, in general, wrong responses. Also, you have to check for sensitive information that can leak from the internal knowledge of the LLM or your RAG system.\n\nPopular guardrail tools are Galileo Protect, which detects prompt injections, toxic language, data privacy protection leaks, and hallucinations. Also, you can use OpenAI’s Moderation API to detect harmful inputs or outputs and take action on them.\n\nThe downside of adding input and output guardrails is the extra latency added to your system, which might interfere with your application’s user experience. Thus, there is a trade-off between the safety of your input/output and latency. Regarding invalid outputs, as LLMs are non- deterministic, you can implement a retry mechanism to generate another potential candidate. However, as stated above, running the retry sequentially will double the response time. Thus, a common strategy is to run multiple generations in parallel and pick the best one. This will increase redundancy but help keep the latency in check.",
      "content_length": 1351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1343,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1344,
      "content": "Prompt monitoring\n\nMonitoring is not new to LLMOps, but in the LLM world, we have a new entity to manage: the prompt. Thus, we have to find specific ways to log and analyze them.\n\nMost ML platforms, such as Opik (from Comet ML) and W&B, or other specialized tools like Langfuse, have implemented logging tools to debug and monitor prompts. While in production, using these tools, you usually want to track the user input, the prompt templates, the input variables, the generated response, the number of tokens, and the latency.\n\nWhen generating an answer with an LLM, we don’t wait for the whole answer to be generated; we stream the output token by token. This makes the entire process snappier and more responsive. Thus, when it comes to tracking the latency of generating an answer, the final user experience must look at this from multiple perspectives, such as:\n\nTime to First Token (TTFT): The time it takes for the first token to be generated\n\nTime between Tokens (TBT): The interval between each token generation",
      "content_length": 1020,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1345,
      "content": "Tokens per Second (TPS): The rate at which tokens are generated\n\nTime per Output Token (TPOT): The time it takes to generate each output token\n\nTotal Latency: The total time required to complete a response\n\nAlso, tracking the total input and output tokens is critical to understanding the costs of hosting your LLMs.\n\nUltimately, you can compute metrics that validate your model’s performance for each input, prompt, and output tuple. Depending on your use case, you can compute things such as accuracy, toxicity, and hallucination rate. When working with RAG systems, you can also compute metrics relative to the relevance and precision of the retrieved context.\n\nAnother essential thing to consider when monitoring prompts is to log their full traces. You might have multiple intermediate steps from the user query to the final general answer. For example, rewriting the query to improve the RAG’s retrieval accuracy evolves one or more intermediate steps. Thus, logging the full trace reveals the entire process from when a user sends a query to when the final response is returned, including the actions the system takes, the documents retrieved, and the final prompt sent to the model. Additionally, you can log the latency, tokens, and costs at each step, providing a more fine-grained view of all the steps.",
      "content_length": 1314,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1346,
      "content": "Figure 11.4: Example trace in the Langfuse UI\n\nAs shown in Figure 11.4, the end goal is to trace each step from the user’s input until the generated answer. If something fails or behaves unexpectedly, you can point exactly to the faulty step. The query can fail due to an incorrect answer, an invalid context, or incorrect data processing. Also, the application can behave unexpectedly if the number of generated tokens suddenly fluctuates during specific steps.",
      "content_length": 462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1347,
      "content": "To conclude, LLMOps is a rapidly developing field. Given its quick evolution, making predictions is challenging. The truth is that we are not sure if the term LLMOps is here to stay. However, what is certain is that numerous new use cases for LLMs will emerge, along with tools and best practices to manage their lifecycle.\n\nEven if this DevOps, MLOps, and LLMOps section is far from comprehensive, it provides a strong idea of how to apply best ops practices in our LLM Twin use case.",
      "content_length": 485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1348,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1349,
      "content": "Deploying the LLM Twin’s pipelines to the cloud\n\nThis section will show you how to deploy all the LLM Twin’s pipelines to the cloud. We must deploy the entire infrastructure to have the whole system working in the cloud. Thus, we will have to:\n\nSet up an instance of MongoDB serverless.\n\nSet up an instance of Qdrant serverless.\n\nDeploy the ZenML pipelines, container, and artifact registry to AWS.\n\nContainerize the code and push the Docker image to a container registry.\n\nNote that the training and inference pipelines already work with AWS SageMaker. Thus, by following the preceding four steps, we ensure that our whole system is on the cloud, ready to scale and serve our imaginary clients.",
      "content_length": 695,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1350,
      "content": "What are the deployment costs?\n\nWe will stick to the free versions of the MongoDB, Qdrant, and ZenML services. As for AWS, we will mostly stick to their free tier for running the ZenML pipelines. The SageMaker training and inference components are more costly to run (which we won’t run in this section). Thus, what we will show you in the following sections will generate minimum costs (a few dollars at most) from AWS.",
      "content_length": 420,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1351,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1352,
      "content": "Understanding the infrastructure\n\nBefore diving into the step-by-step tutorial, where we will show you how to set up all the necessary components, let’s briefly overview our infrastructure and how all the elements interact. This will help us in mindfully following the tutorials below.\n\nAs shown in Figure 11.5, we have a few services to set up. To keep things simple, for MongoDB and Qdrant, we will leverage their serverless freemium version. As for ZenML, we will leverage the free trial of the ZenML cloud, which will help us orchestrate all the pipelines in the cloud. How will it do that?\n\nBy leveraging the ZenML cloud, we can quickly allocate all the required AWS resources to run, scale, and store the ML pipeline. It will help us spin up, with a few clicks, the following AWS components:\n\nAn ECR service for storing Docker images\n\nAn S3 object storage for storing all our artifacts and models\n\nSageMaker Orchestrator for orchestrating, running, and scaling all our ML pipelines",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1353,
      "content": "Figure 11.5: Infrastructure flow",
      "content_length": 32,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1354,
      "content": "Now that we understand what the essential resources of our infrastructure are, let’s look over the core flow of running a pipeline in the cloud that we will learn to implement, presented in Figure 11.5:\n\nBuild a Docker image that contains all the system dependencies, the project dependencies, and the LLM Twin application.\n\nPush the Docker image to ECR, where SageMaker can access it.\n\nNow, we can trigger any pipeline implemented during this book either from the CLI of our local machine or ZenML’s dashboard.\n\nEach step from ZenML’s pipeline will be mapped to a SageMaker job that runs on an AWS EC2 virtual machine (VM). Based on the dependencies between the directed acyclic graph (DAG) steps, some will run in parallel and others sequentially.\n\nWhen running a step, SageMaker pulls the Docker image from ECR, defined in step 2. Based on the pulled image, it creates a Docker container that executes the pipeline step.\n\nAs the job is executed, it can access the S3 artifact storage, MongoDB, and Qdrant vector DB to query or push data. The ZenML dashboard is a key",
      "content_length": 1069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1355,
      "content": "tool, providing real-time updates on the pipeline’s progress and ensuring a clear view of the process.\n\nNow that we know how the infrastructure works, let’s start by setting up MongoDB, Qdrant, and the ZenML cloud.\n\nWhat AWS cloud region should I choose?\n\nIn our tutorials, all the services will be deployed to AWS within the Frankfurt (eu-central-1) region. You can select another region, but be consistent across all the services to ensure faster responses between components and reduce potential errors.\n\nHow should I manage changes in the services’ UIs?\n\nUnfortunately, MongoDB, Qdrant, or other services may change their UI or naming conventions. As we can’t update this book each time that happens, please refer to their official documentation to check anything that differs from our tutorial. We apologize for this inconvenience, but unfortunately, it is not in our control.",
      "content_length": 881,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1356,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1357,
      "content": "Setting up MongoDB\n\nWe will show you how to create and integrate a free MongoDB cluster into our projects. To do so, these are the steps you have to follow:\n\nGo to their site at https://www.mongodb.com and create an account.\n\nIn the left panel, go to Deployment|Database and click Build a Cluster.\n\nWithin the creation form, do the following:\n\nChoose an M0 Free cluster.\n\nCall your cluster twin.\n\nChoose AWS as your provider.",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1358,
      "content": "Choose Frankfurt (eu-central-1) as your region. You can choose another region, but be careful to choose the same region for all future AWS services.\n\nLeave the rest of the attributes with their default values.\n\nIn the bottom right, click the Create Deployment green button.\n\nTo test that your newly created MongoDB cluster works fine, we must connect to it from our local machine. We used the MongoDB VS Code extension to do so, but you can use any other tool. Thus, from their Choose a connection method setup flow, choose MongoDB for VS Code. Then, follow the steps provided on their site.\n\nTo connect, you must paste the DB connection URL in the VS Code extension (or another tool of your liking), which contains your username, password, and cluster URL, similar to this one:\n\nmongodb+srv://: @twin.vhxy1.mongodb.net\n\n. Make sure to save this URL somewhere you can copy it from later.\n\nIf you don’t know or want to change your password, go to Security → Quickstart in the left panel. There, you can edit your login credentials. Be sure to save them somewhere safe, as you won’t be able to access them later.",
      "content_length": 1110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1359,
      "content": "After verifying that your connections work, go to Security → Network Access in the left panel and click ADD IP ADDRESS.Then click ALLOW ACCESS FROM ANYWHERE and hit Confirm. Out of simplicity, we allow any machine from any IP to access our MongoDB cluster. This ensures that our pipelines can query or write to the DB without any additional complex networking setup. It’s not the safest option for production, but for our example, it’s perfectly fine.\n\nThe final step is to return to your project and open your\n\n.env\n\nfile. Now, either add or replace the\n\nDATABASE_HOST\n\nvariable with your MongoDB connection string. It should look something like this:\n\nDATABASE_HOST= mongodb+srv://: @twin.vhxy1.mongodb.net\n\n.\n\nThat’s it! Now, instead of reading and writing from your local MongoDB, you will do it from the cloud MongoDB cluster we just created. Let’s repeat a similar process with Qdrant.",
      "content_length": 891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1360,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1361,
      "content": "Setting up Qdrant\n\nWe have to repeat a similar process to what we did for MongoDB. Thus, to create a Qdrant cluster and hook it to our project, follow these steps:\n\nGo to Qdrant at https://cloud.qdrant.io/ and create an account.\n\nIn the left panel, go to Clusters and click Create.\n\nFill out the cluster creation form with the following:\n\nChoose the Free version of the cluster.\n\nChoose GCP as the cloud provider (while writing the book, it was the only one allowed for a free cluster).\n\nChoose Frankfurt as the region (or the same region as you chose for MongoDB).",
      "content_length": 565,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1362,
      "content": "Name the cluster twin.\n\nLeave the rest of the attributes with their default values and click Create.\n\nAccess the cluster in the Data Access Control section in the left panel.\n\nClick Create and choose your twin cluster to create a new access token.Copy the newly created token somewhere safe, as you won’t be able to access it anymore.\n\nYou can run their example from Usage Examples to test that your connection works fine.\n\nGo back to the Clusters section of Qdrant and open your newly created twin cluster. You will have access to the cluster’s endpoint, which you need to configure Qdrant in your code.\n\nYou can visualize your Qdrant collections and documents by clicking Open Dashboard and entering your API Key as your password. The Qdrant cluster dashboard will now be empty, but after running the pipelines, you will see all the collections, as shown here:",
      "content_length": 862,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1363,
      "content": "Figure 11.6: Qdrant cluster dashboard example after being populated with two collections.\n\nFinally, return to your project and open your\n\n.env\n\nfile. Now, we must fill in a couple of environment variables as follows:\n\nUSE_QDRANT_CLOUD\n\n=\n\ntrue\n\nQDRANT_CLOUD_URL",
      "content_length": 261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1364,
      "content": "=\n\nstep\n\n7>\n\nQDRANT_APIKEY\n\n=\n\nstep\n\n5>\n\nThat’s it! Instead of reading and writing from your local Qdrant vector DB, you will do it from the cloud Qdrant cluster we just created. Just to be sure that everything works fine, run the end-to-end data pipeline with the cloud version of MongoDB and Qdrant as follows:\n\npeotry poe run-end-to-end-data-pipeline\n\nThe last step is setting up the ZenML cloud and deploying all our infrastructure to AWS.",
      "content_length": 443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1365,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1366,
      "content": "Setting up the ZenML cloud\n\nSetting up the ZenML cloud and the AWS infrastructure is a multi-step process. First, we will set up a ZenML cloud account, then the AWS infrastructure through the ZenML cloud, and, finally, we will bundle our code in a Docker image to run it in AWS SageMaker.\n\nLet’s start with setting up the ZenML cloud:\n\nGo to the ZenML cloud at https://cloud.zenml.io and make an account. They provide a seven-day free trial, which is enough to run our examples.\n\nFill out their onboarding form and create an organization with a unique name and a tenant called twin. A tenant refers to a deployment of ZenML in a fully isolated environment. Wait a few minutes until your tenant server is up before proceeding to the next step.\n\nIf you want to, you can go through their Quickstart Guide to understand how the ZenML cloud works with a simpler example. It is not required to go through it to deploy the LLM Twin application, but we recommend it to ensure everything works fine.",
      "content_length": 990,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1367,
      "content": "At this point, we assume that you have gone through the Quickstart Guide. Otherwise, you might encounter issues during the next steps. To connect our project with this ZenML cloud tenant, return to the project and run the\n\nzenml connect\n\ncommand provided in the dashboard. It looks similar to the following example but with a different URL:\n\nzenml connect --url https://0c37a553-zenml.cloudinfra.zenml.io\n\n.\n\nTo ensure everything works fine, run a random pipeline from your code. Note that at this point, we are still running it locally, but instead of logging the results to the local server, we log everything to the cloud version:\n\npoetry poe run-digital-data-etl\n\nGo to the Pipelines section in the left panel of the ZenML dashboard. If everything worked fine, you should see the pipeline you ran in Step 5 there.",
      "content_length": 817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1368,
      "content": "Ensure that your ZenML server version matches your local ZenML version. For example, when we wrote this book, both were version 0.64.0. If they don’t match, you might encounter strange behavior, or it might not work correctly. The easiest fix is to go to your\n\npyproject.toml\n\nfile, find the\n\nzenml\n\ndependency, and update it with the version of your server. Then run\n\npoetry lock --no-update && poetry install\n\nto update your local virtual environment.\n\nTo ship the code to AWS, you must create a ZenML stack. A stack is a set of components, such as the underlying orchestrator, object storage, and container registry, that ZenML needs under the hood to run the pipelines. Intuitively, you can see your stack as your infrastructure. While working locally, ZenML offers a default stack that allows you to quickly develop your code and test things locally. However, by defining different stacks, you can quickly switch between different infrastructure environments, such as local and AWS runs, which we will showcase in this section.\n\nBefore starting this section, ensure you have an AWS account with admin permissions ready.",
      "content_length": 1124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1369,
      "content": "With that in mind, let’s create an AWS stack for our project. To do so, follow the next steps:\n\nIn the left panel, click on the Stacks section and hit the New Stack button.\n\nYou will have multiple options for creating a stack, but the easiest is creating one from scratch within the in-browser experience, which doesn’t require additional preparations. This is not very flexible, but it is enough to host our project. Thus, choose Create New Infrastructure → In-browser Experience.\n\nThen, choose AWS as your cloud provider.\n\nChoose Europe (Frankfurt)—eu-central-1 as your location or the region you used to set up MongoDB and Qdrant.\n\nName it aws-stack.It is essential to name it exactly like this so that the commands that we will use work.\n\nNow ZenML will create a set of IAM roles to give permissions to all the other components to communicate with each other, an S3 bucket as your artifact storage, an ECR repository as your container registry, and SageMaker as your orchestrator.",
      "content_length": 984,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1370,
      "content": "Click Next.\n\nClick the Deploy to AWS button. It will open a CloudFormation page on AWS. ZenML leverages CloudFormation (an infrastructure as code, or IaC, tool)to create all the AWS resources we enumerated in Step 6.\n\nAt the bottom, check all the boxes to acknowledge that AWS CloudFormation will create AWS resources on your behalf. Finally, click the Create stack button. Now, we must wait for a couple of minutes for AWS CloudFormation to spin up all the resources.\n\nReturn to the ZenML page and click the Finish button.\n\nBy leveraging ZenML, we efficiently deployed the entire AWS infrastructure for our ML pipelines. We began with a basic example, sacrificing some control. However, if you seek more control, ZenML offers the option to use Terraform (an IaC tool) to fully control your AWS resources or to connect ZenML with your current infrastructure.\n\nBefore moving to the next step, let’s have a quick recap of the AWS resources we just created:\n\nAn IAM role is an AWS identity with permissions policies that define what actions are allowed or denied for that role. It is used to grant access to AWS services without needing to share security credentials.",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1371,
      "content": "S3 is a scalable and secure object storage service that allows storing and retrieving files from anywhere on the web. It is commonly used for data backup, content storage, and data lakes. It’s more scalable and flexible than Google Drive.\n\nECR is a fully managed Docker container registry that makes storing, managing, and deploying Docker container images easy.\n\nSageMaker is a fully managed service that allows developers and data scientists to quickly build, train, and deploy ML models.\n\nSageMaker Orchestrator is a feature of SageMaker that helps automate the execution of ML workflows, manage dependencies between steps, and ensure the reproducibility and scalability of model training and deployment pipelines. Other similar tools are Prefect, Dagster, Metaflow, and Airflow.\n\nCloudFormation is a service that allows you to model and set up your AWS resources so that you can spend less time managing them and more time focusing on your applications. It automates the process of provisioning AWS infrastructure using templates.\n\nBefore running the ML pipelines, the last step is to containerize the code and prepare a Docker image that packages our dependencies and code.",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1372,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1373,
      "content": "Containerize the code using Docker\n\nSo far, we have defined our infrastructure, MongoDB, Qdrant, and AWS, for storage and computing. The last step is to find a way to take our code and run it on top of this infrastructure. The most popular solution is Docker, a tool that allows us to create an isolated environment (a container) that contains everything we need to run our application, such as system dependencies, Python dependencies, and the code.\n\nWe defined our Docker image at the project’s root in the\n\nDockerfile\n\n. This is the standard naming convention for Docker. Before digging into the code, if you want to build the Docker image yourself, ensure that you have Docker installed on your machine. If you don’t have it, you can install it by following the instructions provided here: https://docs.docker.com/engine/install. Now, let’s look at the content of the\n\nDockerfile\n\nstep by step.\n\nThe\n\nDockerfile\n\nbegins by specifying the base image, which is a lightweight version of Python 3.11 based on the Debian Bullseye distribution. The environment variables are then set up to configure various aspects of the container, such",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1374,
      "content": "as the workspace directory, turning off Python bytecode generation, and configuring Python to output directly to the terminal. Additionally, the version of Poetry to be installed is specified, and a few environment variables are set to ensure that package installations are non-interactive, which is vital for automated builds.\n\nFROM python:\n\n3.11\n\nslim-bullseye AS release ENV WORKSPACE_ROOT=/app/ ENV PYTHONDONTWRITEBYTECODE=\n\n1\n\nENV PYTHONUNBUFFERED=\n\n1\n\nENV POETRY_VERSION=\n\n1.8.3\n\nENV DEBIAN_FRONTEND=noninteractive ENV POETRY_NO_INTERACTION=\n\n1\n\nNext, we install Google Chrome in the container. The installation process begins by updating the package lists and installing essential tools like gnupg, wget, and curl. The Google Linux signing key is added, and the",
      "content_length": 768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1375,
      "content": "Google Chrome repository is configured. After another package list update, the stable version of Google Chrome is installed. The package lists are removed after installation to keep the image as small as possible.\n\nRUN apt-get update -y && \\ apt-get install -y gnupg wget curl --no-install- recommends && \\ wget -q -O - https://dl- ssl.google.com/linux/linux_signing_key.pub | gpg --dearmor -o /usr/share/keyrings/google-linux-signing-key.gpg && \\ echo\n\n\"deb [signed-by=/usr/share/keyrings/google-linux-signing-key.gpg] https://dl.google.com/linux/chrome/deb/ stable main\"\n\n> /etc/apt/sources.list.d/google-chrome.list && \\ apt-get update -y && \\ apt-get install -y google-chrome-stable && \\ rm -rf /var/lib/apt/lists/*\n\nFollowing the Chrome installation, other essential system dependencies are installed. Once these packages are installed, the package cache is cleaned up to reduce the image size further.\n\nRUN apt-get update -y \\ && apt-get install -y --no-install-recommends build-essential \\ gcc \\ python3-dev \\ build-essential \\ libglib2.0-dev \\ libnss3-dev \\ && apt-get clean \\ && rm -rf /var/lib/apt/lists/*",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1376,
      "content": "Poetry, the dependency management tool, is then installed using pip. The\n\n--no-cache-dir\n\noption prevents pip from caching packages, helping to keep the image smaller. After installation, Poetry is configured to use up to 20 parallel workers when installing packages, which can speed up the installation process.\n\nRUN pip install --no-cache-dir \"poetry==$POETRY_VERSION\" RUN poetry config installer.max-workers 20\n\nThe working directory inside the container is set to\n\nWORKSPACE_ROOT\n\n, which defaults to\n\n/app/\n\n, where the application code will reside. The\n\npyproject.toml\n\nand\n\npoetry.lock\n\nfiles define the Python’s project dependencies and are copied into this directory.",
      "content_length": 676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1377,
      "content": "WORKDIR $WORKSPACE_ROOT COPY pyproject.toml poetry.lock $WORKSPACE_ROOT\n\nWith the dependency files in place, the project’s dependencies are installed using Poetry. The configuration turns off the creation of a virtual environment, meaning the dependencies will be installed directly into the container’s Python environment. The installation excludes development dependencies and prevents caching to minimize space usage.\n\nAdditionally, the\n\npoethepoet\n\nplugin is installed to help manage tasks within the project. Finally, any remaining Poetry cache is removed to keep the container as lean as possible.\n\nRUN poetry config virtualenvs.create false && \\ poetry install --no-root -- no-interaction --no-cache --without dev && \\ poetry self add 'poethepoet[poetry_plugin]' && \\ rm -rf ~/.cache/pypoetry/cache/ && \\ rm - rf ~/.cache/pypoetry/artifacts/",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1378,
      "content": "In the final step, the entire project directory from the host machine is copied into the container’s working directory. This step ensures that all the application files are available within the container.\n\nOne important trick when writing a\n\nDockerfile\n\nis to decouple your installation steps from copying the rest of the files. This is useful because each Docker command is cached and layered on top of each other. Thus, whenever you change one layer when rebuilding the Docker image, all the layers below the one altered are executed again. Because you rarely change your system and project dependencies but mostly change your code, copying your project files in the last step makes rebuilding Docker images fast by taking advantage of the caching mechanism’s full potential.\n\nCOPY . $WORKSPACE_ROOT\n\nThis\n\nDockerfile\n\nis designed to create a clean, consistent Python environment with all necessary dependencies. It allows the project to run smoothly in any environment that supports Docker.",
      "content_length": 993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1379,
      "content": "The last step is to build the Docker image and push it to the ECR created by ZenML. To build the Docker image from the root of the project, run the following:\n\ndocker buildx build --platform linux/amd64 -t llmtwin -f Dockerfile .\n\nWe must build it on a Linux platform as the Google Chrome installer we used inside Docker works only on a Linux machine. Even if you use a macOS or Windows machine, Docker can emulate a virtual Linux container.\n\nThe tag of the newly created Docker image is\n\nllmtwin\n\n. We also provide this\n\nbuild\n\ncommand under a\n\npoethepoet\n\ncommand:",
      "content_length": 566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1380,
      "content": "poetry poe build-docker-image\n\nNow, let’s push the Docker image to ECR. To do so, navigate to your AWS console and then to the ECR service. From there, find the newly created ECR repository. It should be prefixed with\n\nzenml-*\n\n, as shown here:\n\nFigure 11.7: AWS ECR example\n\nThe first step is to authenticate to ECR. For this to work, ensure that you have the AWS CLI installed and configured with your admin AWS credentials, as explained in Chapter 2:",
      "content_length": 453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1381,
      "content": "AWS_REGION=\n\n# e.g. AWS_REGION=eu-central-1\n\nAWS_ECR_URL= aws ecr get-login-password --region ${AWS_REGION}| docker login --username AWS --password-stdin ${AWS_ECR_URL}\n\nYou can get your current\n\nAWS_REGION\n\nby clicking on the toggle in the top-right corner, as seen in Figure 11.8. Also, you can copy the ECR URL to fill the\n\nAWS_ECR_URL\n\nvariable from the main AWS ECR dashboard, as illustrated in Figure 11.7. After running the previous command, you should see the message Login Succeeded on the CLI.",
      "content_length": 503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1382,
      "content": "Figure 11.8: AWS region and account details\n\nNow we have to add another tag to the\n\nllmtwin\n\nDocker image that signals the Docker registry we want to push it to:\n\ndocker tag llmtwin ${AWS_ECR_URL}:latest\n\nFinally, we push it to ECR by running:",
      "content_length": 243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1383,
      "content": "docker push ${AWS_ECR_URL}:latest\n\nAfter the upload is finished, return to your AWS ECR dashboard and open your ZenML repository. The Docker image should appear, as shown here:\n\nFigure 11.9: AWS ECR repository example after the Docker image is pushed\n\nFor every change in the code that you need to ship and test, you would have to go through all these steps, which are tedious and error-prone. The Adding LLMOps to the LLM Twin section of this chapter will teach us how to automate these steps within the CD pipeline using GitHub Actions. Still, we first wanted to go through them manually to fully understand the behind- the-scenes process and not treat it as a black box. Understanding these",
      "content_length": 693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1384,
      "content": "details is vital for debugging your CI/CD pipelines, where you must understand the error messages and how to fix them.\n\nNow that we have built our Docker image and pushed it to AWS ECR, let’s deploy it to AWS.",
      "content_length": 209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1385,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1386,
      "content": "Run the pipelines on AWS\n\nWe are very close to running the ML pipelines on AWS, but we have to go through a few final steps. Let’s switch from the default ZenML stack to the AWS one we created in this chapter. From the root of your project, run the following in the CLI:\n\nzenml stack set aws-stack\n\nReturn to your AWS ECR ZenML repository and copy the image URI as shown in Figure 11.9. Then, go to the\n\nconfigs\n\ndirectory, open the\n\nconfigs/end_to_end_data.yaml\n\nfile, and update the\n\nsettings.docker.parent_image\n\nattribute with your ECR URL, as shown below:",
      "content_length": 560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1387,
      "content": "settings:\n\ndocker:\n\nparent_image:\n\n#e.g., 992382797823.dkr.ecr.eu-central-1.amazonaws.com/zenml- rlwlcs:latest\n\nskip_build:\n\nTrue\n\nWe’ve configured the pipeline to always use the latest Docker image available in ECR. This means that the pipeline will automatically pick up the latest changes made to the code whenever we push a new image.\n\nWe must export all the credentials from our\n\n.env\n\nfile to ZenML secrets, a feature that safely stores your credentials and makes them accessible within your pipelines:\n\npoetry poe export-settings-to-zenml",
      "content_length": 545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1388,
      "content": "The last step is setting up to run the pipelines asynchronously so we don’t have to wait until they are finished, which might result in timeout errors:\n\nzenml orchestrator update aws-stack --synchronous=False\n\nNow that ZenML knows to use the AWS stack, our custom Docker image, and has access to our credentials, we are finally done with the setup. Run the\n\nend-to-end-data-pipeline\n\nwith the following command:\n\npoetry poe run-end-to-end-data-pipeline\n\nNow you can go to ZenML Cloud → Pipelines → end_to_end_data and open the latest run. On the ZenML dashboard, you can visualize the latest state of the pipeline, as seen in Figure 11.10. Note that this pipeline runs all the data-related pipelines in a single run.",
      "content_length": 716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1389,
      "content": "In the Adding LLMOps to the LLM Twin section, we will explain why we compressed all the steps into a single pipeline.\n\nFigure 11.10: ZenML example of running the end-to-end-data-pipeline\n\nYou can click on any running block and find details about the run, the code used for that specific step, and the logs for monitoring and debugging, as illustrated in Figure 11.11:",
      "content_length": 367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1390,
      "content": "Figure 11.11: ZenML step metadata example\n\nTo run other pipelines, you have to update the\n\nsettings.docker.parent_image\n\nattribute in their config file under the\n\nconfigs/",
      "content_length": 171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1391,
      "content": "directory.\n\nTo find even more details about the runs, you can go to AWS SageMaker. In the left panel, click SageMaker dashboard, and on the right, in the Processing column, click on the green Running section, as shown in Figure 11.12.\n\nThis will open a list of all the processing jobs that execute your ZenML pipelines.\n\nFigure 11.12: SageMaker dashboard",
      "content_length": 354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1392,
      "content": "If you want to run the pipelines locally again, use the following CLI command:\n\npoetry poe set-local-stack\n\nIf you want to disconnect from the ZenML cloud dashboard and use the local version again, run the following:\n\nzenml disconnect",
      "content_length": 234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1393,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1394,
      "content": "Troubleshooting the ResourceLimitExceeded error after running a ZenML pipeline on SageMaker\n\nLet’s assume, you’ve encountered a ResourceLimitExceeded error after running a ZenML pipeline on SageMaker using the AWS stack. In this case, you have to explicitly ask AWS to give you access to a specific type of AWS EC2 VM.\n\nZenML uses, by default,\n\nml.t3.medium\n\nEC2 machines, which are part of the AWS freemium tier. However, some AWS accounts cannot access these VMs by default. To check your access, search your AWS console for Service Quotas.\n\nThen, in the left panel, click on AWS services, search for Amazon SageMaker, and then for\n\nml.t3.medium\n\n. In Figure 11.13, you can see our quotas for these types of machines. If yours is 0, you should request that AWS increase them to numbers similar to those from Figure 11.13 in the Applied account-level quota value column. The whole process is free of charge and only requires a few clicks. Unfortunately, you might have to wait for a few hours up to one day until AWS accepts your request.",
      "content_length": 1039,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1395,
      "content": "Figure 11.13: SageMaker—ml.t3.medium expected quotas\n\nYou can find step-by-step instructions on how to solve this error and request new quotas at this link: https://repost.aws/knowledge-center/sagemaker- resource-limit-exceeded-error.\n\nIf you changed the values from your .env file and want to update the ZenML secrets with them, first run the following CLI command to delete the old secrets:",
      "content_length": 392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1396,
      "content": "poetry poe delete-settings-zenml\n\nThen, you can export them again by running:\n\npoetry poe export-settings-to-zenml",
      "content_length": 114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1397,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1398,
      "content": "Adding LLMOps to the LLM Twin\n\nIn the previous section, we saw how to set up the infrastructure for the LLM Twin project by manually building the Docker image and pushing it to ECR. We want to automate the entire process and implement a CI/CD pipeline using GitHub Actions and a CT pipeline using ZenML. As mentioned earlier, implementing a CI/CD/CT pipeline ensures that each feature pushed to main branches is consistent and tested. Also, by automating the deployment and training, you support collaboration, save time, and reduce human errors.\n\nFinally, at the end of the section, we will show you how to implement a prompt monitoring pipeline using Opik from Comet ML and an alerting system using ZenML. This prompt monitoring pipeline will help us debug and analyze the RAG and LLM logic. As LLM systems are non- deterministic, capturing and storing the prompt traces is essential for monitoring your ML logic.\n\nBefore diving into the implementation, let’s start with a quick section on the LLM Twin’s CI/CD pipeline flow.",
      "content_length": 1027,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1399,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1400,
      "content": "LLM Twin’s CI/CD pipeline flow\n\nWe have two environments: staging and production. When developing a new feature, we create a new branch out of the staging branch and develop solely on that one. When we are done and consider the feature finished, we open a pull request (PR) to the staging branch. After the feature branch is accepted, it is merged into the staging branch. This is a standard workflow in most software applications. There might be variations, like adding a dev environment, but the principles remain the same.\n\nAs illustrated in Figure 11.14, the CI pipeline is triggered when the PR opens. At this point, we test the feature branch for linting and formatting errors. Also, we run a\n\ngitleaks\n\ncommand to check for credentials and sensitive information that was committed by mistake. If the linting, formatting, and gitleaks steps pass (also known as static analysis), we run the automated tests. Note that the static analysis steps run faster than the automated tests. Thus, the order matters. That’s why adding the static analysis steps at the beginning of the CI pipeline is good practice. We propose the following order of the CI steps:\n\ngitleaks\n\nchecks",
      "content_length": 1174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1401,
      "content": "Linting checks\n\nFormatting checks\n\nAutomated testing, such as unit and integration tests\n\nIf any check fails, the CI pipeline fails, and the developer who created the PR cannot merge it into the staging branch until it fixes the issues.\n\nImplementing a CI pipeline ensures that new features follow the repository’s standards and don’t break existing functionality. The exact process repeats when we plan to merge the staging branch into the production one. We open a PR, and the CI pipeline is automatically executed before merging the staging branch into production.",
      "content_length": 567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1402,
      "content": "Figure 11.14: CI/CD pipelines flow\n\nThe CD pipeline runs after the branch is merged. For example, after the feature branch is merged into staging, the CD pipeline takes the code from the staging branch, builds a new Docker image, and pushes it to the AWS ECR Docker repository. When running future pipeline runs in the staging environment, it will use the latest Docker image that was built by the CD pipeline. The exact process happens between staging and production. Still,",
      "content_length": 475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1403,
      "content": "the key difference is that the staging environment exists as an experimental place where the QA team and stakeholders can further manually test the new feature along with what is automatically tested in the CI pipeline.\n\nIn our repository, we used only a main branch, which reflects production, and feature branches to push new work. We did this to keep things simple, but the same principles apply. To extend the flow, you must create a staging branch and add it to the CD pipeline.",
      "content_length": 483,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1404,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1405,
      "content": "More on formatting errors\n\nFormatting errors relate to the style and structure of your code, ensuring that it adheres to a consistent visual layout. This can include the placement of spaces, indentation, line length, and other stylistic elements.\n\nThe main purpose of formatting is to make your code more readable and maintainable. Consistent formatting helps teams work together more effectively, as the code looks uniform, regardless of who wrote it. Examples of formatting errors are:\n\nIncorrect indentation (e.g., mixing spaces and tabs)\n\nLines that are too long (e.g., exceeding\n\n79\n\nor\n\n88\n\ncharacters, depending on your style guide)\n\nMissing or extra spaces around operators or after commas",
      "content_length": 697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1406,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1407,
      "content": "More on linting errors\n\nLinting errors relate to potential issues in your code that could lead to bugs, inefficiencies, or non-adherence to coding standards beyond just style. Linting checks often involve static analysis of the code to catch things like unused variables, undefined names, or questionable practices.\n\nLinting’s main goal is to catch potential errors or bad practices early in the development process, improving code quality and reducing the likelihood of bugs. Examples of linting errors are:\n\nUnused imports or variables\n\nUndefined variables or functions are being used\n\nPotentially dangerous code (e.g., using\n\n==\n\ninstead of\n\nis\n\nfor checking against\n\nNone",
      "content_length": 675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1408,
      "content": ")\n\nWe use Ruff, a versatile tool for formatting and linting. It incorporates checks for common formatting issues and PEP 8 compliance, as well as deeper linting checks for potential errors and code quality problems. Also, it is written in Rust, making it fast for big codebases.\n\nBefore implementing what we’ve explained above, let’s examine the core principles of GitHub Actions.",
      "content_length": 380,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1409,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1410,
      "content": "Quick overview of GitHub Actions\n\nGitHub Actions is a CI/CD platform provided by GitHub that allows developers to automate their workflows directly within a GitHub repository. It enables users to build, test, and deploy their code directly from GitHub by defining workflows in YAML files. Since it’s part of GitHub, it works seamlessly with repositories, issues, PRs, and other GitHub features. Here are the key components you should know about:\n\nWorkflows: A workflow is an automated process defined in a YAML file located in your repository’s\n\n.github/workflows directory\n\n. It specifies what should happen (e.g.,\n\nbuild\n\n,\n\ntest\n\n, and\n\ndeploy\n\n) and when (e.g., on push, on PR).\n\nJobs: Workflows are made up of jobs, which are groups of steps that execute on the same runner. Each job runs in its own virtual",
      "content_length": 812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1411,
      "content": "environment.\n\nSteps: Jobs are made up of multiple independent steps, which can be actions or shell commands.\n\nActions: Actions are reusable commands or scripts. You can use pre- built actions from GitHub Marketplace or create your own. You can think of them as Python functions.\n\nRunners: Runners are the servers that run your jobs. GitHub provides hosted runners (Linux, Windows, macOS), or you can even self-host your runners.\n\nA workflow is described using YAML syntax. For example, a simple workflow that clones the current GitHub repository and installs Python 3.11 on an Ubuntu machine looks like this:\n\nname\n\n:\n\nExample\n\non\n\n:",
      "content_length": 633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1412,
      "content": "[push]\n\njobs\n\n:\n\nbuild\n\n:\n\nruns-on\n\n:\n\nubuntu-latest\n\nsteps\n\n:\n\n\n\nname: Checkout\n\nuses\n\n:\n\nactions/checkout@v3\n\n\n\nname: Setup Python\n\nuses\n\n:\n\nactions/setup-python@v3",
      "content_length": 166,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1413,
      "content": "with\n\n:\n\npython-version\n\n:\n\n\"3.11\"\n\nThe workflows are triggered by events like\n\npush\n\n,\n\npull_request\n\n, or\n\nschedule\n\n. For example, you might trigger a workflow every time code is pushed to a specific branch. Now that we understand how GitHub Actions works, let’s look at the LLM Twin’s CI pipeline.",
      "content_length": 301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1414,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1415,
      "content": "The CI pipeline\n\nThe LLM Twin’s CI pipeline is split into two jobs:\n\nA QA job that looks for formatting and linting errors using Ruff. Also, it runs a\n\ngitleaks\n\nstep to scan for leaked secrets throughout our repository.\n\nA test job that runs all our automatic tests using\n\nPytest\n\n. In our use case, we implemented just a dummy test to showcase the CI pipeline, but using the structure from this book, you can easily extend it with real tests for your use case.",
      "content_length": 462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1416,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1417,
      "content": "GitHub Actions CI YAML file\n\nThe YAML file sits under\n\n.github/workflows/ci.yaml\n\n. It begins by defining the workflow’s name as\n\nCI\n\n, as you can see in the following snippet. This label will be used to identify the workflow within GitHub’s Actions interface. Next, the section specifies that the workflow should be triggered whenever a\n\npull_request\n\nevent occurs. Hence, the CI workflow will automatically run whenever a PR is opened, synchronized, or reopened.\n\nname:\n\nCI\n\non:\n\npull_request:",
      "content_length": 495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1418,
      "content": "The\n\nconcurrency\n\nsection ensures that only one instance of this workflow runs for a given reference (like a branch) at any given time. The\n\ngroup\n\nfield is defined using GitHub’s expression syntax to create a unique group name based on the workflow and the reference. The\n\ncancel-in-progress: true\n\nline ensures that if a new workflow run is triggered before the previous one finishes, the previous run is canceled. This is particularly useful to prevent redundant executions of the same workflow.\n\nconcurrency:\n\ngroup:\n\n${{\n\ngithub.workflow\n\n}}-${{\n\ngithub.ref\n\n}}\n\ncancel-in-progress:\n\ntrue",
      "content_length": 593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1419,
      "content": "The workflow defines two separate jobs:\n\nqa\n\nand\n\ntest\n\n. Each job runs on the latest version of Ubuntu, specified by\n\nruns-on: ubuntu-latest\n\n.\n\nThe first job, named\n\nQA\n\n, is responsible for quality assurance tasks like code checks and formatting verification. Within the\n\nqa\n\njob, the first step is to check out the repository’s code using the\n\nactions/checkout@v3\n\naction. This step is necessary to ensure that the job has access to the code that needs to be analyzed.",
      "content_length": 472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1420,
      "content": "jobs:\n\nqa:\n\nname:\n\nQA\n\nruns-on:\n\nubuntu-latest\n\nsteps:\n\n\n\nname:\n\nCheckout\n\nuses:\n\nactions/checkout@v3\n\nThe next step is to set up the Python environment. This is done using the\n\nactions/setup-python@v3\n\naction, with the Python version specified as\n\n\"3.11\"\n\n. This step ensures that the subsequent steps in the job will run in the correct Python environment.",
      "content_length": 357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1421,
      "content": "\n\nname:\n\nSetup\n\nPython\n\nuses:\n\nactions/setup-python@v3\n\nwith:\n\npython-version:\n\n\"3.11\"\n\nThe workflow then installs Poetry using the\n\nabatilo/actions-poetry@v2\n\naction, specifying the version of Poetry as\n\n1.8.3\n\n:",
      "content_length": 213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1422,
      "content": "\n\nname:\n\nInstall\n\npoetry\n\nuses:\n\nabatilo/actions-poetry@v2\n\nwith:\n\npoetry-version:\n\n1.8.3\n\nOnce Poetry is set up, the workflow installs the project’s development dependencies using the\n\npoetry install --only dev\n\ncommand. Additionally, the workflow adds the\n\npoethepoet\n\nplugin for Poetry, which will be used to run predefined tasks more conveniently within the project.\n\n",
      "content_length": 372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1423,
      "content": "name:\n\nInstall\n\npackages\n\nrun:\n\n|\n\npoetry\n\ninstall\n\n--only\n\ndev\n\npoetry\n\nself\n\nadd\n\n'poethepoet[poetry_plugin]'\n\nThe\n\nqa\n\njob then runs several quality checks on the code. The first check uses a tool called\n\ngitleaks",
      "content_length": 216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1424,
      "content": "to scan for secrets in the codebase, ensuring that no sensitive information is accidentally committed:\n\n\n\nname:\n\ngitleaks\n\ncheck\n\nrun:\n\npoetry\n\npoe\n\ngitleaks-check\n\nFollowing the\n\ngitleaks\n\ncheck, the workflow runs a linting process to enforce coding standards and best practices in the Python code. This is achieved through the\n\npoetry poe lint-check\n\ncommand, which uses Ruff under the hood.",
      "content_length": 393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1425,
      "content": "\n\nname:\n\nLint\n\ncheck\n\n[\n\nPython\n\n]\n\nrun:\n\npoetry\n\npoe\n\nlint-check\n\nThe last step in the\n\nqa\n\njob is a format check, which ensures that the Python code is properly formatted according to the project’s style guidelines. This is done using the\n\npoetry poe format-check",
      "content_length": 265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1426,
      "content": "command, which uses Ruff under the hood.\n\n\n\nname:\n\nFormat\n\ncheck\n\n[\n\nPython\n\n]\n\nrun:\n\npoetry\n\npoe\n\nformat-check\n\nThe second job defined in the workflow is the\n\ntest\n\njob, which also runs on the latest version of Ubuntu. Like the\n\nqa",
      "content_length": 232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1427,
      "content": "job, it starts by checking out the code from the repository and installing Python 3.11 and Poetry 1.8.3.\n\ntest:\n\nname:\n\nTest\n\nruns-on:\n\nubuntu-latest\n\nsteps:\n\n\n\nname:\n\nCheckout\n\nuses:\n\nactions/checkout@v3\n\n…\n\nAfter setting up the system dependencies, the\n\ntest",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1428,
      "content": "job installs all the project’s dependencies with the\n\npoetry install\n\ncommand. As we want to run the tests, this time, we need to install all the dependencies that are required to run the application.\n\n\n\nname:\n\nInstall\n\npackages\n\nrun:\n\n|\n\npoetry\n\ninstall\n\n–-without\n\naws\n\npoetry\n\nself\n\nadd\n\n'poethepoet[poetry_plugin]'",
      "content_length": 318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1429,
      "content": "Finally, the\n\ntest\n\njob runs the project’s tests using the\n\npoetry poe test\n\ncommand. This step ensures that all tests are executed and provides feedback on whether the current code changes break any functionality.\n\n\n\nname:\n\nRun\n\ntests\n\nrun:\n\n|\n\necho\n\n\"Running tests...\"\n\npoetry\n\npoe",
      "content_length": 283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1430,
      "content": "test\n\nIf any of the steps from the QA or test jobs fail, the GitHub Actions workflow will fail, resulting in the PR not being able to be merged until the issue is fixed. By taking this approach, we ensure that all the new features added to the main branches respect the standard of the project and that it doesn’t break existing functionality through automated tests.\n\nFigure 11.15 shows the CI pipeline in the Actions tab of the GitHub repository. It was run after a commit with the message feat: Add Docker image and CD pipeline and ran the two jobs described above, QA and Test.",
      "content_length": 581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1431,
      "content": "Figure 11.15: GitHub Actions CI pipeline run example",
      "content_length": 52,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1432,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1433,
      "content": "The CD pipeline\n\nThe CD pipeline will automate the Docker steps we manually performed in the Deploying the LLM Twin’s pipelines to the cloud section, which are:\n\nSet up Docker.\n\nLog in to AWS.\n\nBuild the Docker image.\n\nPush the Docker image to AWS ECR.\n\nWith that in mind, let’s look at the GitHub Actions YAML file, which sits under\n\n.github/workflows/cd.yaml\n\n. It begins by naming the workflow\n\nCD",
      "content_length": 400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1434,
      "content": "and specifying the trigger for this workflow. The trigger is any push to the repository’s main branch. This workflow will automatically run when new code is pushed to the main branch, usually when a PR is merged into the main branch. The\n\non.push\n\nconfiguration sets up the trigger:\n\nname:\n\nCD\n\non:\n\npush:\n\nbranches:\n\n\n\nmain\n\nThe workflow then defines a single job named\n\nBuild & Push Docker Image\n\n:",
      "content_length": 400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1435,
      "content": "jobs:\n\nbuild:\n\nname:\n\nBuild\n\n&\n\nPush\n\nDocker\n\nImage\n\nruns-on:\n\nubuntu-latest\n\nThe first step within the job is to check out the repository’s code.\n\nsteps:\n\n\n\nname:\n\nCheckout",
      "content_length": 173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1436,
      "content": "Code\n\nuses:\n\nactions/checkout@v3\n\nAfter checking out the code, the workflow sets up docker buildx, a Docker CLI plugin that extends Docker’s build capabilities with features like multi- platform builds and cache import/export:\n\n\n\nname:\n\nSet\n\nup\n\nDocker\n\nBuildx\n\nuses:\n\ndocker/setup-buildx-action@v3",
      "content_length": 298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1437,
      "content": "The next step involves configuring the AWS credentials. This step is crucial for interacting with AWS services, such as Amazon Elastic Container Registry (ECR), where the Docker images will be pushed. The AWS access key, secret access key, and region are securely retrieved from the repository’s secrets to authenticate the workflow with AWS. This ensures the workflow has the necessary permissions to push Docker images to the ECR repository. We will show you how to configure these secrets after wrapping up with the YAML file:\n\n\n\nname:\n\nConfigure\n\nAWS\n\ncredentials\n\nuses:\n\naws-actions/configure-aws-credentials@v1\n\nwith:\n\naws-access-key-id:\n\n${{\n\nsecrets.AWS_ACCESS_KEY_ID\n\n}}\n\naws-secret-access-key:",
      "content_length": 703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1438,
      "content": "${{\n\nsecrets.AWS_SECRET_ACCESS_KEY\n\n}}\n\naws-region:\n\n${{\n\nsecrets.AWS_REGION\n\n}}\n\nOnce the AWS credentials are configured, the workflow logs in to Amazon ECR. This step is essential for authenticating the Docker CLI with the ECR registry, allowing subsequent steps to push images to the registry:\n\n\n\nname:\n\nLogin\n\nto\n\nAmazon\n\nECR\n\nid:",
      "content_length": 334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1439,
      "content": "login-ecr\n\nuses:\n\naws-actions/amazon-ecr-login@v1\n\nThe final step in the workflow involves building the Docker image and pushing it to the Amazon ECR repository. This is accomplished using the\n\ndocker/build-push-action@v6\n\naction. The\n\ncontext\n\nspecifies the build context, which is typically the repository’s root directory. The\n\nfile\n\noption points to the\n\nDockerfile\n\n, which defines how the image should be built. The\n\ntags\n\nsection assigns tags to the image, including the specific commit SHA and the\n\nlatest\n\ntag, which is a common practice for identifying the most recent version of the image. The",
      "content_length": 604,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1440,
      "content": "push\n\noption is set to\n\ntrue\n\n, meaning the image will be uploaded to ECR after it is built:\n\n\n\nname:\n\nBuild\n\nimages\n\n&\n\npush\n\nto\n\nECR\n\nid:\n\nbuild-image\n\nuses:\n\ndocker/build-push-action@v6\n\nwith:\n\ncontext:",
      "content_length": 205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1441,
      "content": ".\n\nfile:\n\n./Dockerfile\n\ntags:\n\n|\n\n${{\n\nsteps.login-ecr.outputs.registry\n\n}}/${{\n\nsecrets.AWS_ECR_NAME\n\n}}:${{\n\ngithub.sha\n\n}}\n\n${{\n\nsteps.login-ecr.outputs.registry\n\n}}/${{\n\nsecrets.AWS_ECR_NAME\n\n}}:latest\n\npush:\n\ntrue",
      "content_length": 218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1442,
      "content": "To conclude, the CD pipeline authenticates to AWS, builds the Docker image, and pushes it to AWS ECR. The Docker image is pushed with\n\nlatest\n\nand the commit’s SHA tag. By doing so, we can always use the latest image and point to the commit of the code from which the image was generated.\n\nAlso, in our code, we have only a main branch, which reflects our production environment. But you, as a developer, have the power to extend this functionality with a staging and dev environment. You just have to add the name of the branches in the\n\non.push.branches\n\nconfiguration at the beginning of the YAML file.\n\nIn Figure 11.16, you can observe how the CD pipeline looks after a PR is merged into the production branch. As seen before, we only have the Build & Push Docker Image job here.",
      "content_length": 783,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1443,
      "content": "Figure 11.16: GitHub Actions CD pipeline run example\n\nThe last step in setting up the CI/CD pipeline is to test it and see how it works.",
      "content_length": 136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1444,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1445,
      "content": "Test out the CI/CD pipeline\n\nTo test the CI/CD pipelines yourself, you must fork the LLM-Engineering repository to have full write access to the GitHub repository. Here is the official tutorial on how to fork a GitHub project: https://docs.github.com/en/pull-requests/collaborating-with-pull- requests/working-with-forks/fork-a-repo\n\nThe last step is to set up a few secrets that will allow the CD pipeline to log in to AWS and point to the right ECR resource. To do so, go to the Settings tab at the top of the forked repository in GitHub. In the left panel, in the Security section, click on the Secrets and Variables toggle and, finally, on Actions. Then, on the Secrets tab, create four repository secrets, as shown in Figure 11.17. These secrets will be securely stored and accessible only by the GitHub Actions CD pipeline.\n\nThe\n\nAWS_ACCESS_KEY_ID\n\nand\n\nAWS_SECRET_ACCESS_KEY\n\nare the AWS credentials you used across the book. In Chapter 2, you see how to create them. The\n\nAWS_REGION\n\n(e.g.,",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1446,
      "content": "eu-central-1\n\n) and\n\nAWS_ECR_NAME\n\nare the same ones used in the Deploying the LLM Twin’s pipelines to the cloud section.\n\nFor the\n\nAWS_ECR_NAME\n\n, you should configure only the name of the repository (e.g.,\n\nzenml-vrsopg\n\n) and not the full URI (e.g., 992382797823.dkr.ecr.eu-central- 1.amazonaws.com/zenml-vrsopg), as seen in the image below:\n\nFigure 11.17: Configuring only repository name\n\nTo trigger the CI pipeline, create a feature branch, modify the code or documentation, and create a PR to the main branch. To trigger the CD pipeline, merge the PR into the main branch.",
      "content_length": 579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1447,
      "content": "After the CD GitHub Actions are complete, check the ECR repository to see whether the Docker image was pushed successfully.\n\nFigure 11.18: GitHub Actions secrets\n\nIf you need more details on how to set up GitHub Actions secrets, we recommend checking out their official documentation: https://docs.github.com/en/actions/security-for-github-actions/security- guides/using-secrets-in-github-actions",
      "content_length": 396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1448,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1449,
      "content": "The CT pipeline\n\nTo implement the CT pipeline, we will leverage ZenML. Once ZenML (or other orchestrators such as Metaflow, Dagster, or Airflow) orchestrates all your pipelines and your infrastructure is deployed, you are very close to reaching CT.\n\nRemember the core difference between the CI/CD and CT pipelines. The CI/CD pipeline takes care of testing, building, and deploying your code—a dimension that any software program has. The CT pipeline leverages the code managed by the CI/CD pipeline to automate your data, training, and model-serving process, where the data and model dimensions are present only in the AI world.\n\nBefore diving into the implementation, we want to highlight two design choices that made reaching CT simple:\n\nThe FTI architecture: A modular system with clear interfaces and components made it easy to capture the relationship between the pipelines and automate them.\n\nStarting with an orchestrator since day 0: We started with ZenML at the beginning of the project’s development. Early on, we only used it locally. But it acted as an entry point for our pipelines and a way to monitor their execution. Doing so forced us to decouple each pipeline and transfer the communication between them solely through various",
      "content_length": 1244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1450,
      "content": "types of data storage, such as the data warehouse, feature store, or artifact store. As we have leveraged ZenML since day 0, we got rid of implementing a tedious CLI to configure our application. Instead, we did it directly through YAML configuration files out of the box.\n\nIn Figure 11.19, we can see all the pipelines that we have to chain together to fully automate our training and deployment. The pipelines aren’t new; they aggregate everything we’ve covered throughout this book. Thus, at this point, we will treat them as black boxes that interact with each other.",
      "content_length": 571,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1451,
      "content": "Figure 11.19: CT pipeline\n\nFor the LLM Twin’s CT pipeline, we have to discuss the initial trigger that starts the pipelines and how the pipelines are triggered by each other.",
      "content_length": 174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1452,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1453,
      "content": "Initial triggers\n\nAs illustrated in Figure 11.18, we initially want to trigger the data collection pipeline. Usually, the triggers can be of three types:\n\nManual triggers: Done through the CLI or the orchestrator’s dashboard, in our case, through the ZenML dashboard. Manual triggers are still extremely powerful tools, as you need just one action to start the whole ML system, from data gathering to deployment, instead of fiddling with dozens of scripts that you might configure wrong or run in an invalid order.\n\nREST API triggers: You can call a pipeline by an HTTP request. This is extremely useful when integrating your ML pipelines with other components. For example, you can have a watcher constantly looking for new articles. It triggers the ML logic using this REST API trigger when it finds some. To find more details on this feature, check out this tutorial on ZenML’s documentation: https://docs.zenml.io/v/docs/how- to/trigger-pipelines/trigger-a-pipeline-from-rest-api.\n\nScheduled triggers: Another common approach is to schedule your pipeline to run constantly on a fixed interval. For example, depending on your use case, you can schedule your pipeline to run daily, hourly, or every minute. Most of the orchestrators, ZenML included, provide a cron expression interface where you can define your execution frequency. In the following example from ZenML, the pipeline is scheduled every hour:",
      "content_length": 1409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1454,
      "content": "Schedule(cron_expression=\n\n\"* * 1 * *\"\n\n)\n\nWe chose a manual trigger for our LLM Twin use case as we don’t have other components to leverage the REST API triggers. Also, as the datasets are generated from a list of static links defined in the ZenML configs, running them on a schedule doesn’t make sense as they would always yield the same results.\n\nBut a possible next step for the project is to implement a watcher that monitors for new articles. When it finds any, it generates a new config and triggers the pipelines through the REST API. Another option is implementing the watcher as an additional pipeline and leveraging the schedule triggers to look daily for new data. If it finds any, it executes the whole ML system; otherwise, it stops.\n\nThe conclusion is that once you can manually trigger all your ML pipelines through a single command, you can quickly adapt it to more advanced and complex scenarios.",
      "content_length": 914,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1455,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1456,
      "content": "Trigger downstream pipelines\n\nTo keep things simple, we sequentially chained all the pipelines. More concretely, when the data collection pipeline has finished, it will trigger the feature pipeline. When the feature pipeline has been completed successfully, it triggers the dataset generation pipeline, and so on. You can make the logic more complex, like scheduling the generate instruct dataset pipeline to run daily, checking the amount of new data in the Qdrant vector DB, and starting only if it has enough new data. From this point, you can further tweak the system’s parameters and optimize them to reduce costs.\n\nTo trigger all the pipelines in one go, we created one master pipeline that aggregates everything in one entry point:\n\n@pipeline\n\ndef\n\nend_to_end_data\n\n(\n\nauthor_links:\n\nlist\n\n[",
      "content_length": 798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1457,
      "content": "dict\n\n[\n\nstr\n\n,\n\nstr\n\n|\n\nlist\n\n[\n\nstr\n\n]]], …\n\n# Other paramaters…\n\n) ->\n\nNone\n\n: wait_for_ids = []\n\nfor\n\nauthor_data\n\nin\n\nauthor_links: last_step_invocation_id = digital_data_etl( user_full_name=author_data[\n\n\"user_full_name\"\n\n], links=author_data[",
      "content_length": 249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1458,
      "content": "\"links\"\n\n] ) wait_for_ids.append(last_step_invocation_id) author_full_names = [author_data[\n\n\"user_full_name\"\n\n]\n\nfor\n\nauthor_data\n\nin\n\nauthor_links] wait_for_ids = feature_engineering(author_full_names=author_full_names, wait_for=wait_for_ids) generate_instruct_datasets(…) training(…) deploy(…)\n\nTo keep the function light, we added all the logic up to computing the features. But, as we suggested in the code snippet above, you can easily add the instruction dataset generation, training, and deploy logic to the parent pipeline to implement an end-to-end flow. By doing that, you can automate everything from data collection to deploying the model.\n\nTo run the end-to-end pipeline, use the following\n\npoe\n\ncommand:",
      "content_length": 718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1459,
      "content": "poetry poe run-end-to-end-data-pipeline\n\nWhat we implemented is not the best approach, as it compresses all the steps into a single monolith pipeline (which we want to avoid), as illustrated in Figure 11.20. Usually, you want to keep each pipeline isolated and use triggers to start downstream pipelines. This makes the system easier to understand, debug, and monitor.",
      "content_length": 368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1460,
      "content": "Figure 11.20: End-to-end pipeline illustrated in ZenML’s dashboard\n\nUnfortunately, the ZenML cloud’s free trial has a limitation of a maximum of three pipelines. As we have more, we avoided that limitation by compressing all the steps into a single pipeline. But if you plan to host",
      "content_length": 282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1461,
      "content": "ZenML yourself or buy their license, they offer the possibility to independently trigger a pipeline from another pipeline, as you can see in the code snippet below where we triggered the feature engineering pipeline after the data collection ETL:\n\nfrom\n\nzenml\n\nimport\n\npipeline, step\n\n@pipeline\n\ndef\n\ndigital_data_etl\n\n(\n\nuser_full_name:\n\nstr\n\n, links:\n\nlist\n\n[\n\nstr\n\n]",
      "content_length": 369,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1462,
      "content": ") ->\n\nstr\n\n: user = get_or_create_user(user_full_name) crawl_links(user=user, links=links) trigger_feature_engineering_pipeline(user)\n\n@step\n\ndef\n\ntrigger_feature_engineering_pipeline\n\n(\n\nuser\n\n): run_config = PipelineRunConfiguration(…) Client().trigger_pipeline(\n\n\"feature_engineering\"\n\n, run_configuration=run_config)\n\n@pipeline\n\ndef\n\nfeature_engineering\n\n(\n\nauthor_full_names:\n\nlist\n\n[\n\nstr\n\n]",
      "content_length": 397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1463,
      "content": ") ->\n\nlist\n\n[\n\nstr\n\n]: …\n\n# ZenML steps\n\nBy taking this approach, each pipeline will have its independent run, where one pipeline sequentially triggers the next one, as described at the beginning of this section. Note that this feature is not unique to ZenML but is common in orchestrator tools. The principles we have learned so far hold. Only how we interact with the tool changes.",
      "content_length": 383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1464,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1465,
      "content": "Prompt monitoring\n\nWe will use Opik (from Comet ML) to monitor our prompts. But remember from the LLMOps section earlier in this chapter that we are not interested only in the input prompt and generated answer.\n\nWe want to log the entire trace from the user’s input until the final result is available. Before diving into the LLM Twin use case, let’s look at a simpler example:\n\nfrom\n\nopik\n\nimport\n\ntrack\n\nimport\n\nopenai\n\nfrom\n\nopik.integrations.openai\n\nimport",
      "content_length": 460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1466,
      "content": "track_openai openai_client = track_openai(openai.OpenAI())\n\n@track\n\ndef\n\npreprocess_input\n\n(\n\ntext:\n\nstr\n\n) ->\n\nstr\n\n:\n\nreturn\n\ntext.strip().lower()\n\n@track\n\ndef\n\ngenerate_response\n\n(\n\nprompt:\n\nstr\n\n) ->\n\nstr",
      "content_length": 208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1467,
      "content": ": response = openai_client.chat.completions.create( model=\n\n\"gpt-3.5-turbo\"\n\n, messages=[{\n\n\"role\"\n\n:\n\n\"user\"\n\n,\n\n\"content\"\n\n: prompt}] )\n\nreturn\n\nresponse.choices[\n\n0\n\n].message.content\n\n@track\n\ndef\n\npostprocess_output\n\n(\n\nresponse:\n\nstr\n\n) ->",
      "content_length": 244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1468,
      "content": "str\n\n:\n\nreturn\n\nresponse.capitalize()\n\n@track(\n\nname=\n\n\"llm_chain\"\n\n)\n\ndef\n\nllm_chain\n\n(\n\ninput_text:\n\nstr\n\n) ->\n\nstr\n\n: preprocessed = preprocess_input(input_text) generated = generate_response(preprocessed) postprocessed = postprocess_output(generated)\n\nreturn\n\npostprocessed result = llm_chain(\n\n\"Hello, do you enjoy reading the book?\"",
      "content_length": 338,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1469,
      "content": ")\n\nThe preceding code snippet reflects in a simplistic way what most LLM applications will look like. You have the\n\nllm_chain()\n\nmain function, which takes the initial input as a parameter and returns the final result.\n\nThen, you have preprocessing and postprocessing functions surrounding the actual LLM call. Using the\n\n@track()\n\ndecorator, we log the input and output of each function, which will ultimately be aggregated into a single trace. By doing so, we will have access to the initial input text, the generated answer, and all the intermediary steps required to debug any potential issues using Opik’s dashboard.\n\nThe last step is to attach the necessary metadata for your use case to the current trace. As seen in the following code snippet, you can easily do that by calling the\n\nupdate()\n\nmethod, where you can tag your trace or add any other metadata, such as the number of input tokens, through a Python dictionary:",
      "content_length": 929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1470,
      "content": "from\n\nopik\n\nimport\n\ntrack, opik_context\n\n@track\n\ndef\n\nllm_chain\n\n(\n\ninput_text\n\n):\n\n# LLM chain code\n\n# ...\n\nopik_context.update_current_trace( tags=[\n\n\"inference_pipeline\"\n\n], metadata={ \"num_tokens\": compute_num_tokens(…) }, feedback_scores=[ { \"name\": \"user_feedback\", \"value\":\n\n1.0\n\n, \"reason\": \"The response was valuable",
      "content_length": 325,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1471,
      "content": "and\n\ncorrect.\" }, { \"name\": \"llm_judge_score\", \"value\": compute_llm_judge_score(…), \"reason\": \"Computing runtime metrics using an LLM Judge.\" } )\n\nYou can expand on this idea and log various feedback scores. The most common is asking the user if the generated answer is valuable and correct. Another option is to compute various metrics automatically through heuristics or LLM judges.\n\nFinally, let’s see how to add prompt monitoring to our LLM Twin project. First, look at Figure 11.21 and remember our model-serving architecture. We have two microservices, the LLM and business microservices. The LLM microservice has a narrow scope, as it only takes as input a prompt that already contains the user’s input and context and returns an answer that is usually post-processed. Thus, the business microservice is the right place to implement the monitoring pipeline, as it coordinates the end-to-end flow. More concretely, Opik implementation will be in the FastAPI server developed in Chapter 10.",
      "content_length": 995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1472,
      "content": "Figure 11.21: Inference pipeline serving architecture",
      "content_length": 53,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1473,
      "content": "As our implementation is already modular, using Opik makes it straightforward to log an end-to-end trace of a user’s request:\n\nfrom\n\nopik\n\nimport\n\ntrack\n\n@track\n\ndef\n\ncall_llm_service\n\n(\n\nquery:\n\nstr\n\n, context:\n\nstr\n\n|\n\nNone\n\n) ->\n\nstr",
      "content_length": 236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1474,
      "content": ": llm = LLMInferenceSagemakerEndpoint(…) answer = InferenceExecutor(llm, query, context).execute()\n\nreturn\n\nanswer\n\n@track\n\ndef\n\nrag\n\n(\n\nquery:\n\nstr\n\n) ->\n\nstr\n\n: retriever = ContextRetriever() documents = retriever.search(query, k=\n\n3\n\n\n\n3\n\n) context = EmbeddedChunk.to_context(documents) answer = call_llm_service(query, context)\n\nreturn\n\nanswer",
      "content_length": 347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1475,
      "content": "The\n\nrag()\n\nfunction represents your application’s entry point. All the other processing steps take place in the\n\nContextRetriever\n\nand\n\nInferenceExector\n\nclasses. Also, by decorating the\n\ncall_llm_service()\n\nfunction, we can clearly capture the prompt sent to the LLM and its response.\n\nTo add more granularity to our trace, we can further decorate other functions containing pre- or post-processing steps, such as the\n\nContextRetriever\n\nsearch function:\n\nclass",
      "content_length": 462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1476,
      "content": "ContextRetriever\n\n: …\n\n@track\n\ndef\n\nsearch\n\n(\n\nself,\n\nquery:\n\nstr\n\n,\n\nk:\n\nint\n\n=\n\n3\n\n,\n\nexpand_to_n_queries:\n\nint\n\n=\n\n3\n\n,",
      "content_length": 122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1477,
      "content": ") ->\n\nlist\n\n: query_model = Query.from_str(query) query_model = self._metadata_extractor.generate(query_model) …\n\n# Rest of the implementation\n\nOr even go further to the retrieval optimization methods, such as the self- query metadata extractor, to add more granularity:\n\nclass\n\nSelfQuery\n\n:\n\n@track\n\ndef\n\ngenerate\n\n(\n\nself, query:\n\nstr\n\n) ->",
      "content_length": 342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1478,
      "content": "str\n\n: …\n\nreturn\n\nenhanced_query\n\nThe developer is responsible for deciding how much granularity the application needs for proper debugging and analysis. As having detailed monitoring is healthy, monitoring everything can be dangerous as it adds too much noise and makes manually understanding the traces difficult. You must find the right balance. A good rule of thumb is tracing the most critical functions, such as\n\nrag()\n\nand\n\ncall_llm_service()\n\n, and gradually adding more granularity when needed.\n\nThe last step is to attach valuable metadata and tags to our traces. To do so, we will further enhance the\n\nrag()\n\nfunction as follows:",
      "content_length": 640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1479,
      "content": "@track\n\ndef\n\nrag\n\n(\n\nquery:\n\nstr\n\n) ->\n\nstr\n\n: retriever = ContextRetriever() documents = retriever.search(query, k=\n\n3\n\n\n\n3\n\n) context = EmbeddedChunk.to_context(documents) answer, prompt = call_llm_service(query, context) trace = get_current_trace() trace.update( tags=[\n\n\"rag\"\n\n], metadata={ \"model_id\": settings.HF_MODEL_ID, \"embedding_model_id\": settings.TEXT_EMBEDDING_MODEL_ID, \"temperature\": settings.TEMPERATURE_INFERENCE, \"prompt_tokens\": compute_num_tokens(prompt), \"total_tokens\": compute_num_tokens(answer), } )\n\nreturn\n\nanswer",
      "content_length": 540,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1480,
      "content": "There are three main aspects that we should constantly monitor:\n\nModel configuration: Here, we should consider both the LLM and other models used within the RAG layer. The most critical aspects of logging are the model IDs, but you can also capture other important information that significantly impacts the generation, such as the temperature.\n\nTotal number of tokens: It’s critical to constantly analyze the statistics of the number of tokens generated by your input prompts and total tokens, as this significantly impacts your serving costs. For example, if the average of the total number of tokens generated suddenly increases, it’s a strong signal that you have a bug in your system that you should investigate.\n\nThe duration of each step: Tracking the duration of each step within your trace is essential to finding bottlenecks within your system. If the latency of a specific request is abnormally large, you quickly have access to a report that helps you find the source of the problem.",
      "content_length": 995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1481,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1482,
      "content": "Alerting\n\nUsing ZenML, you can quickly implement an alerting system on any platform of your liking, such as email, Discord, or Slack. For example, you can add a callback in your training pipeline to trigger a notification when the pipeline fails or the training has finished successfully:\n\nfrom\n\nzenml\n\nimport\n\nget_pipeline_context, pipeline\n\n@pipeline(\n\non_failure=notify_on_failure\n\n)\n\ndef\n\ntraining_pipeline\n\n(\n\n…\n\n): … notify_on_success()",
      "content_length": 442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1483,
      "content": "Implementing the notification functions is straightforward. As seen in the code snippets below, you have to get the\n\nalerter\n\ninstance from your current stack, build the message as you see fit, and send it to your notification channel of choice:\n\nfrom\n\nzenml.client\n\nimport\n\nClient alerter = Client().active_stack.alerter\n\ndef\n\nnotify_on_failure\n\n() ->\n\nNone\n\n: alerter.post(message=build_message(status=\n\n\"failed\"\n\n))\n\n@step(",
      "content_length": 426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1484,
      "content": "enable_cache=\n\nFalse\n\n)\n\ndef\n\nnotify_on_success\n\n() ->\n\nNone\n\n: alerter.post(message=build_message(status=\n\n\"succeeded\"\n\n))\n\nZenML and most orchestrators simplify implementing an\n\nalerter\n\n, as it’s a critical component in your MLOps/LLMOps infrastructure.",
      "content_length": 256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1485,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1486,
      "content": "Summary\n\nIn this chapter, we laid down the foundations with a theoretical section on DevOps. Then, we moved on to MLOps and its core components and principles. Finally, we presented how LLMOps differs from MLOps by introducing strategies such as prompt monitoring, guardrails, and human-in- the-loop feedback. Also, we briefly discussed why most companies would avoid training LLMs from scratch but choose to optimize them for their use case through prompt engineering or fine-tuning. At the end of the theoretical portion of the chapter, we learned what a CI/CD/CT pipeline is, the three core dimensions of an ML application (code, data, model), and that, after deployment, it is more critical than ever to implement a monitoring and alerting layer due to model degradation.\n\nNext, we learned how to deploy the LLM Twin’s pipeline to the cloud. We understood the infrastructure and went step by step through deploying MongoDB, Qdrant, the ZenML cloud, and all the necessary AWS resources to sustain the application. Finally, we learned how to Dockerize our application and push our Docker image to AWS ECR, which will be used to execute the application on top of AWS SageMaker.\n\nThe final step was to add LLMOps to our LLM Twin project. We began by implementing a CI/CD pipeline with GitHub Actions. Then, we looked at our CT strategy by leveraging ZenML.\n\nFinally, we saw how to implement a monitoring pipeline using Opik from Comet ML and an alerting system using ZenML. These are the fundamental",
      "content_length": 1499,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1487,
      "content": "pillars in adding MLOps and LLMOps to any LLM-based application.\n\nThe framework we learned about throughout the book can quickly be extrapolated to other LLM applications. Even if we used the LLM Twin use case as an example, most of the strategies applied can be adapted to other projects. Thus, we can get an entirely new application by changing the data and making minor tweaks to the code. Data is the new oil, remember?\n\nBy finalizing this chapter, we’ve learned to build an end-to-end LLM application, starting with data collection and fine-tuning until deploying the LLM microservice and RAG service. Throughout this book, we aimed to provide a thought framework to help you build and solve real-world problems in the GenAI landscape. Now that you have it, we wish you good luck in your journey and happy building!",
      "content_length": 820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1488,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1489,
      "content": "References\n\nGitLab. (2023, January 25). What is DevOps? | GitLab. GitLab. https://about.gitlab.com/topics/devops/\n\nHuyen, C. (2024, July 25). Building a generative AI platform. Chip Huyen. https://huyenchip.com/2024/07/25/genai-platform.html\n\nLightricks customer story: Building a recommendation engine from scratch. (n.d.). https://www.qwak.com/academy/lightricks-customer-story-building- a-recommendation-engine-from-scratch\n\nWhat LLMOps. (n.d.). Google Cloud. https://cloud.google.com/discover/what-is-llmops?hl=en\n\nMLOps: Continuous delivery and automation pipelines in machine learning. (2024, August 28). Google Cloud. https://cloud.google.com/architecture/mlops-continuous-delivery-and- automation-pipelines-in-machine-learning#top_of_page\n\nMl-ops.org. (2024a, July 5). https://ml-ops.org/content/mlops-principles",
      "content_length": 820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1490,
      "content": "Ml-ops.org. (2024b, July 5). https://ml-ops.org/content/mlops-principles\n\nMl-ops.org. (2024c, July 5). https://ml-ops.org/content/motivation\n\nMohandas, G. M. (2022a). Monitoring machine learning systems. Made With ML. https://madewithml.com/courses/mlops/monitoring/\n\nMohandas, G. M. (2022b). Testing Machine Learning Systems: Code, Data and Models. Made With ML. https://madewithml.com/courses/mlops/testing/\n\nPreston-Werner, T. (n.d.). Semantic Versioning 2.0.0. Semantic Versioning. https://semver.org/\n\nRibeiro, M. T., Wu, T., Guestrin, C., & Singh, S. (2020, May 8). Beyond Accuracy: Behavioral Testing of NLP models with CheckList. arXiv.org. https://arxiv.org/abs/2005.04118\n\nWandb. (2023, November 30). Understanding LLMOps: Large Language Model Operations. Weights & Biases. https://wandb.ai/site/articles/understanding-llmops-large-language-model- operations/\n\nZenml-Io. (n.d.). GitHub—zenml-io/zenml-huggingface-sagemaker: An example MLOps overview of ZenML pipelines from a Hugging Face model",
      "content_length": 1004,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1491,
      "content": "repository to a deployed AWS SageMaker endpoint. GitHub. https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/main\n\nJoin our book’s Discord space\n\nJoin our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1492,
      "content": "Appendix",
      "content_length": 8,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1493,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1494,
      "content": "MLOps Principles\n\nBuilding robust and scalable ML systems requires more than creating powerful models. It demands an all-encompassing approach to operationalizing the entire ML lifecycle. Let’s explore the six core principles that guide the MLOps field. These principles are independent of any tool and are at the core of building robust and scalable ML systems. They provide a guideline for designing production-ready applications, ensuring consistency, reliability, and scalability at every stage.\n\nWith that in mind, let’s begin with the foundation: automation or operationalization.",
      "content_length": 586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1495,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1496,
      "content": "1. Automation or operationalization\n\nTo adopt MLOps, there are three core tiers that most applications build up gradually, from manual processing to full automation:\n\nManual process: The process is experimental and iterative in the early stages of developing an ML application. The data scientist manually performs each pipeline step, such as data preparation and validation, model training, and testing. At this point, they commonly use Jupyter notebooks to train their models. This stage’s output is the code used to prepare the data and train the models.\n\nContinuoustraining (CT): The next level involves automating model training. This is known as continuous training, which triggers model retraining whenever required. At this point, you often automate your data and model validation steps. This step is usually done by an orchestration tool, such as ZenML, that glues all your code together and runs it on specific triggers. The most common triggers are on a schedule, for example, every day or when a specific event comes in, such as when new data is uploaded or the monitoring system detects a drop in performance, offering you the flexibility to adapt to various triggers.\n\nCI/CD: In the final stage, you implement your CI/CD pipelines to enable fast and reliable deployment of your ML code into production. The key advancement at this stage is the automatic building, testing, and deployment of data, ML models, and training pipeline components. CI/CD is used to",
      "content_length": 1472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1497,
      "content": "quickly push new code into various environments, such as staging or production, ensuring efficient and reliable deployment.\n\nAs we build our LLM system using the FTI (feature, training, inference) architecture, we can quickly move from a manual process to CI/CD/CT. In Figure A.1, we can observe that the CT process can be triggered by various events, such as a drop in performance detected by the monitoring pipeline or a batch of fresh data arriving. Also, Figure A.1 is split into two main sections; the first one highlights the automated processes, while at the bottom, we can observe the manual processes performed by the data science team while experimenting with various data processing methods and models. Once they improve the model by tinkering with how the data is processed or the model architecture, they push the code to the code repository, which triggers the CI/CD pipeline to build, test, package, and deploy the new changes to the FTI pipelines.",
      "content_length": 963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1498,
      "content": "Figure A.1: CI/CD/CT on top of the FTI architecture\n\nTo conclude, CT automates the FTI pipelines, while CI/CD builds, tests, and pushes new versions of the FTI pipeline code to production.",
      "content_length": 188,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1499,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1500,
      "content": "2. Versioning\n\nBy now, we understand that the whole ML system changes if the code, model, or data changes. Thus, it is critical to track and version these three elements individually. But what strategies can we adopt to track the code, model, and data separately?\n\nThe code is tracked by Git, which helps us create a new commit (a snapshot of the code) on every change added to the codebase. Also, Git- based tools usually allow us to make releases, which typically pack multiple features and bug fixes. While the commits contain unique identifiers that are not human-interpretable, a release follows more common conventions based on their major, minor, and patch versions. For example, in a release with version “v1.2.3,” 1 is the major version, 2 is the minor version, and 3 is the patch version. Popular tools are GitHub and GitLab.\n\nTo version the model, you leverage the model registry to store, share, and version all the models used within your system. It usually follows the same versioning conventions used in code releases, defined as Semantic Versioning, which, along with the major, minor, and patch versions, also supports alpha and beta releases that signal applications. At this point, you can also leverage the ML metadata store to attach information to the stored model, such as what data it was trained on, its architecture, performance, latency, and whatever else makes sense to your specific use case. Doing so creates a clear catalog of models that can easily be navigated across your team and company.",
      "content_length": 1523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1501,
      "content": "Versioning the data isn’t as straightforward as versioning the code and model because it depends on the type of data you have (structured or unstructured) and the scale of data you have (big or small). For example, for structured data, you can leverage a SQL database with a version column that helps you track the changes in the dataset. However, other popular solutions are based on Git-like systems, such as Data Version Control (DVC), that track every change made to the dataset. Other trendy solutions are based on artifacts similar to a model registry that allows you to add a virtual layer to your dataset, tracking and creating a new version for every change made to your data. Comet.ml, W&B (Weights & Biases), and ZenML offer powerful artifact features. For all solutions, you must store the data on-premises or use cloud object storage solutions such as AWS S3. These tools provide features that allow you to structure your datasets and versions, track, and access them.",
      "content_length": 981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1502,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1503,
      "content": "3. Experiment tracking\n\nTraining ML models is an entirely iterative and experimental process. Unlike traditional software development, it involves running multiple parallel experiments, comparing them based on a set of predefined metrics, and deciding which one should advance to production. An experiment tracking tool allows you to log all the necessary information, such as metrics and visual representations of your model predictions, to compare all your experiments and easily select the best model. Popular tools are Comet ML, W&B, MLflow, and Neptune.",
      "content_length": 558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1504,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1505,
      "content": "4. Testing\n\nThe same trend is followed when testing ML systems. Hence, we must test our application across all three dimensions: the data, the model, and the code. We must also ensure that the feature, training, and inference pipeline are well integrated with external services, such as the feature store, and work together as a system. When working with Python, the most common tool to write your tests is\n\npytest\n\n, which we also recommend.",
      "content_length": 442,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1506,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1507,
      "content": "Test types\n\nIn the development cycle, six primary types of tests are commonly employed at various stages:\n\nUnit tests: These tests focus on individual components with a single responsibility, such as a function that adds two tensors or one that finds an element in a list.\n\nIntegration tests: These tests evaluate the interaction between integrated components or units within a system, such as the data evaluation pipeline or the feature engineering pipeline, and how they are integrated with the data warehouse and feature store.\n\nSystem tests: System tests play a crucial role in the development cycle as they examine the entire system, including the complete and integrated application. These tests rigorously evaluate the end-to-end functionality of the system, including performance, security, and overall user experience—for example, testing an entire ML pipeline, from data ingestion to model training and inference, ensuring the system produces the correct outputs for given inputs.\n\nAcceptance tests: These tests, often called user acceptance testing (UAT), are designed to confirm that the system meets specified requirements, ensuring it is ready for deployment.",
      "content_length": 1173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1508,
      "content": "Regression tests: These tests check for previously identified errors to ensure that new changes do not reintroduce them.\n\nStress tests: These tests evaluate the system’s performance and stability under extreme conditions, such as high load or limited resources. They aim to identify breaking points and ensure the system can handle unexpected spikes in demand or adverse situations without failing.\n\nFigure A.2: Test types\n\nWe’ve intentionally left regression tests out of the preceding figure because they aren’t a distinct testing phase. Instead, regression testing is applied across all levels—unit, integration, system, acceptance, and stress tests—to",
      "content_length": 655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1509,
      "content": "ensure that changes don’t reintroduce previous errors. It’s an ongoing process within these phases, not a separate type of test, which is why it’s not shown as a separate category.",
      "content_length": 180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1510,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1511,
      "content": "What do we test?\n\nWhen writing most tests, you take a component and treat it as a black box. Thus, what you have control over is the input and output. You want to test that you get an expected output for a given input. With that in mind, here are a few things you should usually test:\n\nInputs: Data types, format, length, and edge cases (min/max, small/large, etc.)\n\nOutputs: Data types, formats, exceptions, and intermediary and final outputs",
      "content_length": 443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1512,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1513,
      "content": "Test examples\n\nWhen testing your code, you can leverage the standards from classic software engineering. Here are a few examples of code tests you can include when writing unit tests to get a better idea of what we want to test at this point—for instance, you want to check that a sentence is cleaned as expected.\n\nAlso, you can look at your chunking algorithm and assert that it works properly by using various sentences and chunk sizes.\n\nWhen we talk about data tests, we mainly refer to data validity. Your data validity code usually runs when raw data is ingested from the data warehouse or after computing the features. It is part of the feature pipeline. Thus, by writing integration or system tests for your feature pipeline, you can check that your system responds properly to valid and invalid data.\n\nTesting data validity depends a lot on your application and data type. For example, when working with tabular data, you can check for non-null values, that a categorical variable contains only the expected values, or that a float value is always positive. You can check for length, character encoding, language, special characters, and grammar errors when working with unstructured data such as text.",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1514,
      "content": "Model tests are the trickiest, as model training is the most non- deterministic process of an ML system. However, unlike traditional software, ML systems can successfully complete without throwing any errors. However, the real issue is that they produce incorrect results that can only be observed during evaluations or tests. Some standard model test techniques involve checking:\n\nThe shapes of the input and model output tensors\n\nThat the loss decreases after one batch (or more) of training\n\nOverfit on a small batch, and the loss approaches 0\n\nThat your training pipeline works on all the supported devices, such as the CPU and GPU\n\nThat your early stopping and checkpoint logic works\n\nAll the tests are triggered inside the CI pipeline. If some tests are more costly, for example, the model ones, you can execute them only on special terms, such as only when modifying the model code.\n\nAt the other end of the spectrum, you can also perform behavioral testing on your model, which tries to adopt the strategy from code",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1515,
      "content": "testing and treats the model as a black box while looking solely at the input data and expected outputs. This makes the behavioral testing methods model agnostic. A fundamental paper in this area is Beyond Accuracy: Behavioral Testing of NLP Models with CheckList, which we recommend if you want to dig more into the subject. However, as a quick overview, the paper proposes that you test your model against three types of tests. We use a model that extracts the main subject from a sentence as an example:\n\nInvariance: Changes in your input should not affect the output—for example, below is an example based on synonym injection:\n\nmodel(text=\n\n\"The advancements in AI are changing the world rapidly.\"\n\n)\n\n# output: ai\n\nmodel(text=\n\n\"The progress in AI is changing the world rapidly.\"\n\n)\n\n# output: ai",
      "content_length": 802,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1516,
      "content": "Directional: Changes in your input should affect the outputs—for example, below is an example where we know the outputs should change based on the provided inputs:\n\nmodel(text=\n\n\"Deep learning used for sentiment analysis.\"\n\n)\n\n# output: deep-learning\n\nmodel(text=\n\n\"Deep learning used for object detection.\"\n\n)\n\n# output: deep-learning\n\nmodel(text=\n\n\"RNNs for sentiment analysis.\"\n\n)\n\n# output: rnn\n\nMinimum functionality: The most simple combination of inputs and expected outputs—for example, below is a set of simple examples that we expect the model should always get right:",
      "content_length": 578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1517,
      "content": "model(text=\n\n\"NLP is the next big wave in machine learning.\"\n\n)\n\n# output: nlp\n\nmodel(text=\n\n\"MLOps is the next big wave in machine learning.\"\n\n)\n\n# output: mlops\n\nmodel(text=\n\n\"This is about graph neural networks.\"\n\n)\n\n# output: gnn\n\nFor more on testing, we recommend reading Testing Machine Learning Systems: Code, Data, and Models by Goku Mohandas: https://madewithml.com/courses/mlops/testing/.",
      "content_length": 398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1518,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1519,
      "content": "5. Monitoring\n\nMonitoring is vital for any ML system that reaches production. Traditional software systems are rule-based and deterministic. Thus, once it is built, it will always work as defined. Unfortunately, that is not the case with ML systems. When implementing ML models, we haven’t explicitly described how they should work. We have used data to compile a probabilistic solution, which means that our ML model will constantly be exposed to a level of degradation. This happens because the data from production might differ from the data the model was trained on. Thus, it is natural that the shipped model doesn’t know how to handle these scenarios.\n\nWe shouldn’t try to avoid these situations but create a strategy to catch and fix these errors in time. Intuitively, monitoring detects the model’s performance degradation, which triggers an alarm that signals that the model should be retrained manually, automatically, or with a combination of both.\n\nWhy retrain the model? As the model performance degrades due to a drift in the training dataset and what it inputs from production, the only solution is to adapt or retrain the model on a new dataset that captures all the new scenarios from production.\n\nAs training is a costly operation, there are some tricks that you can perform to avoid retraining, but before describing them, let’s quickly understand what we can monitor to understand our ML system’s health.",
      "content_length": 1424,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1520,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1521,
      "content": "Logs\n\nThe approach to logging is straightforward, which is to capture everything, such as:\n\nDocument the system configurations.\n\nRecord the query, the results, and any intermediate outputs.\n\nLog when a component begins, ends, crashes, and so on.\n\nEnsure that each log entry is tagged and identified in a way that clarifies its origin within the system.\n\nWhile capturing all activities can rapidly increase the volume of logs, you can take advantage of numerous tools for automated log analysis and anomaly detection that leverage AI to efficiently scan all the logs, providing you with the confidence to manage the logs effectively.",
      "content_length": 632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1522,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1523,
      "content": "Metrics\n\nTo quantify your application’s healthiness, you must define a set of metrics. Each metric measures different aspects of your application, such as the infrastructure, data, and model.",
      "content_length": 191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1524,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1525,
      "content": "System metrics\n\nThe system metrics are based on monitoring service-level metrics (latency, throughput, error rates) and infrastructure health (CPU/GPU, memory). These metrics are used both in traditional software and ML as they are crucial to understanding whether the infrastructure works well and the system works as expected to provide a good user experience to the end users.",
      "content_length": 379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1526,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1527,
      "content": "Model metrics\n\nMerely monitoring the system’s health won’t suffice to identify the deeper issues within our model. Therefore, moving on to the next layer of metrics that focus on the model’s performance is crucial. This includes quantitative evaluation metrics like accuracy, precision, and F1 score, as well as essential business metrics influenced by the model, such as ROI and click rate.\n\nAnalyzing cumulative performance metrics over the entire deployment period is often ineffective. Instead, evaluating performance over time intervals relevant to our application, such as hourly, is essential. Thus, in practice, you window your inputs and compute and aggregate the metrics at the window level. These sliding metrics can provide a clearer picture of the system’s health, allowing us to detect issues more promptly without them being obscured by historical data.\n\nWe may not always have access to ground-truth outcomes to evaluate the model’s performance on production data. This is particularly challenging when there is a significant delay or when real-life data requires annotation. To address this issue, we can develop an approximate signal to estimate the model’s performance or label a small portion of our live dataset to assess performance. When talking about ML monitoring, an approximate signal is also known as a proxy metric, usually implemented by drift detection methods, which are discussed in the following section.",
      "content_length": 1438,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1528,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1529,
      "content": "Drifts\n\nDrifts are proxy metrics that help us detect potential issues with the production model in time without requiring any ground truths/labels. Table A.1 shows three kinds of drifts.\n\nWhat drifts\n\nDescription\n\nDrift formulation\n\nInputs (features)\n\nOutputs (ground truths/labels)\n\nTable A.1: Relationship between data, model, and code changes\n\nData drift\n\nData drift, also called feature drift or covariate shift, occurs when the distribution of the production data deviates from that of the training data, as shown in Figure A.3. This difference means the model cannot handle the changes in feature space, leading to potentially unreliable predictions. Drift can result from natural real-life changes or systemic problems like missing data, pipeline errors, and schema modifications.",
      "content_length": 787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1530,
      "content": "Figure A.3: Data drift examples\n\nWhen data begins to drift, the degradation in our model’s performance might not be immediately noticeable, particularly if the model interpolates effectively. Nevertheless, this presents an ideal chance to consider retraining before the drift affects the model’s performance.\n\nTarget drift",
      "content_length": 322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1531,
      "content": "In addition to changes in input data (data drift), we might also encounter shifts in output distribution. The shift could involve changes in the shape of the distribution or the addition and removal of classes in categorical tasks. While retraining the model can help reduce performance degradation due to target drift, you can often prevent it by adapting the head processing steps and model head to support the new schema of the output class.\n\nFor example, if you have a classifier that predicts if an image contains animals or people, and you get a picture with buildings, you can either adapt your model to support an unknown class or adjust the head of the model to add the new class for future predictions.\n\nConcept drift\n\nIn addition to changes in input and output data, their relationship can also shift. This phenomenon, known as concept drift, makes our model ineffective because the patterns it previously learned to associate inputs with outputs become outdated. As illustrated in the following figure, concept drifts can manifest in various ways:\n\nGradually over time\n\nSuddenly, due to an external event\n\nPeriodically, due to recurring events",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1532,
      "content": "Figure A.4: Concept drift examples\n\nFor example, this happens when using the model in a different geographic area. Let’s assume you want to build a model that predicts whether a person will buy a specific car. You initially built it for the American market. Now, you want to use it in the European market, where people tend to buy smaller cars, creating a drift between the size feature of the car and the output probability of purchasing the vehicle. Of course, concept drifts can be more subtle than this example.",
      "content_length": 515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1533,
      "content": "All these types of drift can happen simultaneously, complicating pinpointing the exact sources of drift.\n\nHow to detect and measure drifts\n\nNow that we’ve recognized the various types of drift, it’s crucial to understand how to detect and measure it. To do so, you need two types of windows:\n\nA reference window: This is the collection of data points used as a baseline to compare against the production data distributions for drift identification. It is usually gathered from the training dataset.\n\nA test window: This collects data points gathered while the ML system is in production. It is compared with the reference window to ascertain if drift has occurred.\n\nTo measure the drifts, you leverage hypothesis tests that verify the change in distribution between the two windows. For example, you can use the Kolmogorov-Smirnov (KS) test to monitor a single continuous feature. This is known as a univariate (1D) test. Thus, you must run it for every feature you want to monitor. You can leverage a chi-squared univariate test to monitor categorical variables and determine if the frequency of events in production is consistent with the reference window distribution.",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1534,
      "content": "from\n\nalibi_detect.cd\n\nimport\n\nKSDrift cd = KSDrift(X_ref, p_val=\n\n.05\n\n, preprocess_fn=preprocess_fn, input_shape=(max_len,))\n\nWhen working with text data in an embedding representation, we have to model a multivariate distribution, which is how LLMs work with text. A popular approach is to take the embeddings of the test and reference windows, apply a dimensionality reduction algorithm, and apply an algorithm such as maximum mean discrepancy (MMD). This algorithm is a kernel-based approach that measures the distance between two distributions by computing the distance between the mean of the embeddings of the two windows.\n\nfrom\n\nalibi_detect.cd\n\nimport",
      "content_length": 661,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1535,
      "content": "MMDDrift cd = MMDDrift(x_ref, backend=\n\n'pytorch'\n\n, p_val=\n\n.05\n\n) preds = cd.predict(x)",
      "content_length": 89,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1536,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1537,
      "content": "Monitoring vs. observability\n\nMonitoring involves the collection and visualization of data, whereas observability provides insights into system health by examining its inputs and outputs. For instance, monitoring allows us to track a specific metric to detect potential issues.\n\nOn the other hand, a system is considered observable if it generates meaningful data about its internal state, which is essential for diagnosing root causes.",
      "content_length": 436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1538,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1539,
      "content": "Alerts\n\nOnce we define our monitoring metrics, we need a way to get notified. The most common approaches are to send an alarm in the following scenarios:\n\nA metric passes the values of a static threshold—for example, when the accuracy of the classifier is lower than 0.8, send an alarm.\n\nTweaking the p-value of the statistical tests that check for drifts. A lower p- value means a higher confidence that the production distribution differs from the reference one.\n\nThese thresholds and p-values depend on your application. However, it is essential to find the correct values, as you don’t want to overcrowd your alarming system with false positives. In that case, your alarm system won’t be trustworthy, and you will either overreact or not react at all to issues in your system. Some common channels for sending alarms to your stakeholders are Slack, Discord, your email, and PagerDuty. The system’s stakeholders can be the core engineers, managers, or anyone interested in the system.\n\nDepending on the nature of the alarm, you have to react differently. But before taking any action, you should be able to inspect it and understand what caused it. You should inspect what metric triggered the alarm, with",
      "content_length": 1208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1540,
      "content": "what value, the time it happened, and anything else that makes sense to your application.\n\nWhen the model’s performance degrades, the first impulse is to retrain it. But that is a costly operation. Thus, you first have to check that the data is valid, the schema hasn’t changed, and the data point was not an isolated outlier. If neither is true, you should trigger the training pipeline and train the model on the newly shifted dataset to solve the drift.",
      "content_length": 456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1541,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1542,
      "content": "6. Reproducibility\n\nReproducibility means that every process within your ML systems should produce identical results given the same input. This has two main aspects.\n\nThe first one is that you should always know what the inputs are—for example, when training a model, you can use a plethora of hyperparameters. Thus, you need a way to always track what assets were used to generate the new assets, such as what dataset version and config were used to train the model.\n\nThe second aspect is based on the non-deterministic nature of ML processes. For example, when training a model from scratch, all the weights are initially randomly initialized. Thus, even if you use the same dataset and hyperparameters, you might end up with a model with a different performance. This aspect can be solved by always using a seed before generating random numbers, as in reality, we cannot digitally create randomness, only pseudo-random numbers. Thus, by providing a seed, we ensure that we always produce the same trace of pseudo-random numbers. This can also happen at the feature engineering step, in case we impute values with random values or randomly remove data or labels. But as a general rule of thumb, always try to make your processes as deterministic as possible, and in case you have to introduce randomness, always provide a seed that you have control over.\n\nJoin our book’s Discord space",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1543,
      "content": "Join our community’s Discord space for discussions with the authors and other readers:\n\nhttps://packt.link/llmeng",
      "content_length": 113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1544,
      "content": "packt.com\n\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.\n\nWhy subscribe?\n\nSpend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\nFully searchable for easy access to vital information\n\nCopy and paste, print, and bookmark content",
      "content_length": 587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1545,
      "content": "At www.packt.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.",
      "content_length": 187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1546,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1547,
      "content": "Other Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other books by Packt:\n\nRAG-Driven Generative AI\n\nDenis Rothman",
      "content_length": 146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1548,
      "content": "ISBN: 9781836200918\n\nScale RAG pipelines to handle large datasets efficiently\n\nEmploy techniques that minimize hallucinations and ensure accurate responses\n\nImplement indexing techniques to improve AI accuracy with traceable and transparent outputs\n\nCustomize and scale RAG-driven generative AI systems across domains\n\nFind out how to use Deep Lake and Pinecone for efficient and fast data retrieval\n\nControl and build robust generative AI systems grounded in real-world data\n\nCombine text and image data for richer, more informative AI responses",
      "content_length": 546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1550,
      "content": "Building LLM Powered Applications\n\nValentina Alto\n\nISBN: 9781835462317\n\nExplore the core components of LLM architecture, including encoder- decoder blocks and embeddings\n\nUnderstand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM\n\nUse AI orchestrators like LangChain, with Streamlit for the frontend\n\nGet familiar with LLM components such as memory, prompts, and tools\n\nLearn how to use non-parametric knowledge and vector databases\n\nUnderstand the implications of LFMs for AI research and industry applications",
      "content_length": 531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1551,
      "content": "Customize your LLMs with fine tuning\n\nLearn about the ethical implications of LLM-powered applications",
      "content_length": 102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1552,
      "content": "Packt is searching for authors like you\n\nIf you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.",
      "content_length": 423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1553,
      "content": "Share your thoughts\n\nNow you’ve finished LLM Engineer’s Handbook, First Edition, we’d love to hear your thoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon review page for this book and share your feedback or leave a review on the site that you purchased it from.\n\nYour review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.",
      "content_length": 433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1554,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1555,
      "content": "Symbols\n\n4-bit NormalFloat (NF4) 215\n\n32-bit floating point (fp32) 211, 212\n\nA\n\nacceptance tests 464\n\nactions 437\n\nActivate-aware Weight Quantization (AWQ) 313\n\nadvanced RAG\n\nIndex",
      "content_length": 180,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1556,
      "content": "overview 117, 118\n\npost-retrieval step 124-126\n\npre-retrieval steps 119-122\n\nretrieval step 122-124\n\nadvanced RAG post-retrieval optimization\n\nreranking 334-338\n\nadvanced RAG pre-retrieval optimizations 324\n\nquery expansion 324-328\n\nself-querying 328-332\n\nadvanced RAG retrieval optimization",
      "content_length": 291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1557,
      "content": "filtered vector search 332-334\n\nadvanced RAG techniques\n\nexploring 321-324\n\npost-retrieval optimization 334-338\n\npre-retrieval optimizations 324-332\n\nretrieval optimization 332-334\n\nalerting system 457, 458\n\nalerts 473\n\nAlpacaEval 264\n\nAmazon Resource Name (ARN) 375",
      "content_length": 266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1558,
      "content": "Application Auto Scaling 396, 397\n\nApplication Load Balancer (ALB) 395\n\nasynchronous inference 361, 362\n\nautoscaling 393, 399\n\nscalable policy, creating 397\n\nscalable target, registering 396\n\nuse cases 394\n\nAWS\n\naccess key, setting up 48-50\n\naccount, setting up 48-50",
      "content_length": 267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1559,
      "content": "CLI, setting up 48-50\n\npreparing 48\n\nSageMaker 50\n\nAWS Elastic Container Service (ECS) 393\n\nAWS Elastic Kubernetes Service (EKS) 393\n\nAWS SageMaker 50\n\nLLM Twin model, deploying to 375-385\n\nneed for 51, 52\n\nAWS SageMaker Inference endpoint\n\ncalling 386-389",
      "content_length": 256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1560,
      "content": "automated evaluation framework for RAG systems (ARES) 274, 275\n\nB\n\nbacked-up data\n\nimporting 95\n\nBaseCrawler interface 69-72\n\nbehavioral testing 466\n\nbias types\n\nfamily bias 237\n\nlength bias 237\n\nposition bias 237",
      "content_length": 213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1561,
      "content": "BigCodeBench Leaderboard 266\n\nbusiness microservice\n\nbuilding, with FastAPI 390-393\n\nC\n\nCDC patterns\n\nlog-based 137\n\ntimestamp-based 137\n\ntrigger-based 137\n\ncentral access point 128\n\nChange data capture (CDC) 136",
      "content_length": 212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1562,
      "content": "Chatbot Arena 264\n\nChatbots 231\n\nChatGPT 5\n\nlimitations 5\n\nchat templates 208-210\n\nchunking handlers 165-169\n\nCI/CD pipeline 462\n\nCI pipeline, LLM Twin\n\nQA job 438\n\ntest job 438",
      "content_length": 177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1563,
      "content": "CircleCI 405\n\nclassifiers models 189\n\ncleaning handlers 163-165\n\nCloudFormation 423\n\ncode generation 231\n\nComet ML 45, 46\n\nconcept drift 471\n\ncontent moderation 231\n\ncontinuous batching 294\n\ncontinuous integration and continuous deployment (CI/CD) pipeline 31, 402",
      "content_length": 264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1564,
      "content": "continuous training (CT) 138, 402, 461\n\ncooldown period 398\n\nco-pilot\n\nversus LLM Twin 4\n\ncovariate drift 470\n\nCrawlerDispatcher class 66-68\n\ncrawlers\n\nBaseCrawler interface 69-72\n\nCustomArticleCrawler class 75-77\n\nGithubCrawler class 73-75",
      "content_length": 240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1565,
      "content": "implementing 69\n\nMediumCrawler class 77-79\n\nCustomArticleCrawler class 75-77\n\nD\n\ndata augmentation 193-196\n\ndatabase (DB) 317, 410\n\ndatabase, for unstructured and vector data\n\nMongoDB 47\n\nQdrant 47, 48\n\nstoring 47",
      "content_length": 213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1566,
      "content": "data collection pipeline 19\n\ndata curation 182\n\ndata decontamination 185\n\ndata deduplication 184, 185\n\ndata drift 470\n\ndata evaluation 233\n\ndata exploration 189-191\n\ndata generation 191-233\n\npreference data, evaluating 235-237\n\npreference data, generating 233, 234",
      "content_length": 264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1567,
      "content": "tips 234\n\ndata indexing techniques 119\n\ndata parallelism (DP) 299\n\ndata quality evaluation 186-189\n\ndata quantity 180, 181\n\nData Scientist (DS) 409\n\ndataset formats 208\n\ndata tests 466\n\ndecoder-only model\n\narchitecture 290",
      "content_length": 222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1568,
      "content": "computing 291\n\ngenerating 291\n\ntokenizing 291\n\nDeep Learning Containers (DLCs) 373\n\ndeployment costs 415\n\ndeployment types, criteria for selection\n\ndata 357\n\ninfrastructure 357, 358\n\nlatency 356\n\nthroughput 356, 357",
      "content_length": 215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1569,
      "content": "DevOps 401-403\n\nbenefits 403\n\ncontinuous delivery (CD) 405\n\ncontinuous integration (CI) 405\n\ndeployment environments 404\n\nversion control 405\n\nDevOps lifecycle\n\nbuild 404\n\ncode 403\n\ndeploy 404",
      "content_length": 192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1570,
      "content": "monitor 404\n\noperate 404\n\nplan 403\n\nrelease 404\n\ntest 404\n\ndirectional 467\n\nDirect Preference Optimization (DPO) 229, 245, 248-250, 411\n\nimplementing 250-257\n\ndispatcher layer 160-162\n\nDLC image",
      "content_length": 194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1571,
      "content": "features 373\n\nDocker 424\n\nDockerfile 424\n\ndomain-driven design (DDD) 150\n\ndomain-specific LLM evaluations 265-267\n\ndownstream pipelines\n\ntriggering 449-451\n\nDPO datasets\n\nhuman-generated, human-evaluated datasets 233\n\nhuman-generated, LLM-evaluated datasets 233",
      "content_length": 261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1572,
      "content": "LLM-generated, human-evaluated datasets 234\n\nLLM-generated, LLM-evaluated datasets 234\n\ndrifts 469\n\nconcept drift 471\n\ndata drift 470\n\ndetecting 472\n\nmeasuring 472\n\ntarget drift 470\n\nE\n\nElastic Container Registry (ECR) 423, 443",
      "content_length": 227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1573,
      "content": "embedding handlers 169-173\n\nencoder-only models 189\n\nend of sentence (EOS) token 222, 252\n\nend-to-end RAG inference pipeline\n\nexamining 346-351\n\nEnterprise Scenarios Leaderboard 266\n\nETL pipeline\n\nfundamental steps 56\n\nETL process\n\nconnecting, to feature pipeline 60",
      "content_length": 266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1574,
      "content": "exact deduplication 184\n\nextract, load, transform (ETL) pattern 19\n\nExtract, Transform, Load (ETL) pipeline 55\n\nF\n\nfamily bias 237\n\nFastAPI\n\nbusiness microservice, building 390-393\n\nfeature drift 470\n\nfeature pipeline 14, 19, 20\n\nfeature/training/inference (FTI)architecture 8, 13, 22, 370",
      "content_length": 289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1575,
      "content": "benefits 15\n\nfeature pipeline 14\n\ninference pipeline 14\n\ntraining pipeline 14\n\nfiltered vector search 123\n\nfine-tune\n\nusage, considerations 206, 207\n\nfine-tune models\n\nspecialized tools 220\n\nfine-tuning",
      "content_length": 202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1576,
      "content": "best practices 219-226\n\nformat filtering 183\n\nformatting errors 436\n\nexamples 436\n\nFTI architecture\n\nused, for building LLM system 462, 463\n\nFTI pipeline design\n\nLLM Twin architecture, designing 17\n\nFTI pipelines architecture\n\ninference pipeline 14",
      "content_length": 248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1577,
      "content": "full fine-tuning 211, 212\n\nfuzzy deduplication 184\n\nG\n\nGAIA 264\n\nGalileo Protect 413\n\ngeneral-purpose LLM evaluations 263-265\n\nGitHub 405\n\nGitHub Actions 405, 437\n\nGitHub Actions CI YAML file 438-441\n\nGitHubCrawler class 73-75",
      "content_length": 226,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1578,
      "content": "GitHub ecosystem 405\n\nGitLab 405\n\nGitLab CI/CD 405\n\nGlobal Interpreter Lock (GIL) 144\n\nGPT 411\n\nguardrails 411, 412\n\ninput guardrails 412\n\noutput guardrails 413\n\nH\n\nHallucinations Leaderboard 266",
      "content_length": 195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1579,
      "content": "handlers 162, 163\n\nchunking handlers 165-169\n\ncleaning handlers 163-165\n\nembedding handlers 169-173\n\nhigh throughput 357\n\nHugging Face 31, 32\n\nfine-tuned LLMs 31\n\nreference link 251\n\nHugging Face Hub\n\nreference link 245",
      "content_length": 219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1580,
      "content": "human-generated, human-evaluated datasets 233\n\nhuman-generated, LLM-evaluated datasets 233\n\nhybrid search 123\n\nHypothetical document embeddings (HyDE) 121\n\nI\n\nIAM role 423\n\nIDE's MongoDB plugin 94\n\nIFEval 264\n\nin-breadth evolving 194\n\nin-depth evolving 194",
      "content_length": 256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1581,
      "content": "inference deployment types 359\n\nasynchronous inference 361, 362\n\noffline batch transform 362\n\nonline real-time inference 360, 361\n\ninference pipeline 22\n\nversus training pipeline 371, 372\n\ninfrastructure 357, 358\n\ninfrastructure-as-code (IaC) 393\n\ninput guardrails 412\n\ninput test 465",
      "content_length": 284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1582,
      "content": "instruction dataset\n\ncreating 178, 196-206\n\ndata augmentation 193-196\n\ndata curation 182\n\ndata decontamination 185\n\ndata deduplication 184, 185\n\ndata exploration 189-191\n\ndata generation 191, 193\n\ndata quality evaluation 186-189\n\ndata quantity 180, 181",
      "content_length": 252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1583,
      "content": "general framework 178-180\n\nhigh-quality data 179\n\nrule-based filtering 182, 183\n\nintegration tests 464\n\ninvariance 467\n\niterative improvement 246\n\nJ\n\nJenkins 405\n\njobs 437\n\nK",
      "content_length": 174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1584,
      "content": "key-value (KV) cache 291-294\n\nkeywords filtering 183\n\nKolmogorov-Smirnov (KS) 472\n\nKullback-Leibler (KL) 247\n\nL\n\nLangfuse 413\n\nLangfuse UI\n\nexample trace 414, 415\n\nlarge language model (LLM) 1, 99, 355, 401\n\nlatency 356",
      "content_length": 219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1585,
      "content": "length bias 237\n\nlength filtering 183\n\nlinting errors 436\n\nexamples 436\n\nLLM-as-a-judge strategy 186\n\nLLM evaluation 235\n\nversus, ML evaluation 262, 263\n\nLLM-generated, human-evaluated datasets 234\n\nLLM-generated, LLM-evaluated datasets 234\n\nLLMOps 401, 402, 410, 411, 415",
      "content_length": 272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1586,
      "content": "adding, to LLM Twin 434\n\nguardrails 411, 412\n\nhuman feedback 411\n\nprompt monitoring 413\n\nLLMs, training from scratch\n\nconcerns 410, 411\n\nLLM system\n\nbuilding, with FTI architecture 462, 463\n\nLLM Twin 2, 5, 6\n\nCD pipeline 442-444",
      "content_length": 228,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1587,
      "content": "CI/CD pipeline flow 434, 435\n\nCI/CD pipeline, testing 445\n\nCI pipeline 438\n\nCT pipeline 446, 448\n\ninference pipeline deployment strategy 368-370\n\nMVP, defining 7\n\nRAG feature pipeline architecture 127, 139\n\nsignificance 3, 4\n\nsystem architecture 16\n\nversus co-pilot 4",
      "content_length": 267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1588,
      "content": "LLM Twin architecture 23\n\ndata collection pipeline 19\n\ndesigning, with FTI pipeline design 17\n\nfeature pipeline 19, 20\n\ninference pipeline 22\n\ntechnical details 16, 17\n\ntraining pipeline 21, 22\n\nLLM Twin model\n\ndeploying, to AWS SageMaker 375-385\n\nLLM Twin RAG feature pipeline",
      "content_length": 277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1589,
      "content": "dispatcher layer 160\n\nhandlers 162\n\nimplementing 139\n\npydantic domain entities 150\n\nsetting 139\n\nZenML pipeline and steps 140, 141\n\nLLM Twin's data collection pipeline\n\ncrawlers 59, 69\n\ndesigning 56-60\n\ndispatcher 66-68",
      "content_length": 219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1590,
      "content": "implementing 61\n\nNoSQL data warehouse documents 79, 80\n\nZenML pipeline and steps 61-65\n\nLLM Twin service\n\ndeploying 372\n\nLLM Twin's pipelines, cloud deployment 415\n\ncode, containerizing with Docker 424-428\n\ninfrastructure 416-418\n\nMongoDB, setting up 418, 419\n\npipelines, running on AWS 428-431",
      "content_length": 294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1591,
      "content": "Qdrant, setting up 419, 420\n\nResourceLimitExceeded error, troubleshooting after running ZenML pipeline on SageMaker 432, 433\n\nZenML, setting up 421-423\n\nlogs 468\n\nlow latency 358\n\nLow-Rank Adaptation (LoRA) 213-215\n\nM\n\nmachine learning (ML) 1, 355\n\nengineering 409",
      "content_length": 264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1592,
      "content": "manual dataset exploration 189, 190\n\nmanual process 461\n\nmanual triggers 448\n\nMassive Multi-Task Language Understanding (MMLU) 261\n\nMaximum Mean Discrepancy (MMD) 472\n\nMediumCrawler class 77-79\n\nmetrics 468\n\ndrifts 469\n\nmodel metrics 469\n\nsystem metrics 469",
      "content_length": 257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1593,
      "content": "metrics-driven development (MDD) 272\n\nmicroservices architecture 365-367\n\nversus monolithic architecture 367, 368\n\nminimum functionality 467\n\nminimum viable product (MVP) 6\n\nfeatures 6\n\nML engineer 410\n\nML evaluation\n\nvesus, LLM evaluation 262, 263\n\nML models",
      "content_length": 259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1594,
      "content": "training 464\n\nMLOps 401-407, 411, 461\n\nCI/CD pipeline 462\n\ncontinuous training (CT) 461\n\nengineering 409\n\nmanual process 461\n\nMLOps and LLMOps tools 30, 31\n\nComet ML 45, 46\n\nHugging Face 31, 32\n\nOpik 46, 47",
      "content_length": 206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1595,
      "content": "ZenML 32, 33\n\nMLOps, core components\n\nfeature store 407\n\nML metadata store 407\n\nML pipeline orchestrator 407\n\nmodel registry 407\n\nMLOps engineer 410\n\nMLOps, principles\n\nautomation 408\n\nexperiment tracking 408",
      "content_length": 208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1596,
      "content": "monitoring 408\n\noperationalization 408\n\nreproducibility 408\n\ntesting 408\n\nversioning 408\n\nML pipeline automation\n\nfor CT 12\n\nML pipelines\n\nfor ML systems 13\n\nML systems",
      "content_length": 168,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1597,
      "content": "elements 9\n\nissues, with building 8, 9\n\ntesting 464\n\nmodel evaluation 261\n\ndomain-specific LLM evaluations 265-267\n\ngeneral-purpose LLM evaluations 263-265\n\nML, versus LLM evaluation 262, 263\n\ntask-specific LLM evaluations 267-271\n\nmodel metrics 469\n\nmodel optimization strategies 290",
      "content_length": 284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1598,
      "content": "continuous batching 294\n\nkey-value (KV) cache 291, 293\n\noptimized attention mechanisms 297, 298\n\nspeculative decoding 295, 296\n\nmodel parallelism 298\n\ndata parallelism (DP) 299\n\npipeline parallelism (PP) 300, 301\n\ntechniques, combining 303\n\ntensor parallelism (TP) 301, 302\n\nmodel quantization 303, 304",
      "content_length": 302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1599,
      "content": "model tests 466\n\nModeration API 413\n\nMongoDB 47\n\nsetting up 418, 419\n\nreference link 418\n\nMongoDB, as data warehouse\n\nusage, considerations 60\n\nmonitoring 468\n\nlogs 468\n\nmetrics 468",
      "content_length": 181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1600,
      "content": "versus observability 472\n\nmonolithic architecture 365\n\nmonolithic batch pipeline architecture 10\n\nMT-Bench 264\n\nN\n\nNoSQL data warehouse documents 79, 80\n\ndata categories and user document classes 87-89\n\nODM class, implementing 82-87\n\nORM and ODM software patterns 80, 82\n\nO",
      "content_length": 273,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1601,
      "content": "object-relational mapping (ORM) 154\n\nobject-vector mapping (OVM) 139\n\nimplementation 139\n\nobservability\n\nversus monitoring 472\n\nODM class\n\nimplementing 82-87\n\nODM software patterns 80, 82\n\noffline batch transform 362\n\nonline real-time inference 360, 361",
      "content_length": 253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1602,
      "content": "Open Arabic LLM Leaderboard 267\n\nOpenKo-LLM Leaderboard 267\n\nOpen Medical-LLM Leaderboard 265\n\nOpen Portuguese LLM Leaderboard 267\n\nOpik 46, 47, 413\n\nOptimal Brain Quantization (OBQ)approach 312\n\noptimized attention mechanisms 297, 298\n\nORM software patterns 80, 82\n\noutput guardrails 413\n\noutput test 465",
      "content_length": 305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1603,
      "content": "P\n\nparameter-efficient fine-tuning techniques\n\nfull fine-tuning 211, 212\n\nLoRA 213-215\n\nQLoRA 215, 216\n\nParameter-efficient fine-tuning techniques 211\n\npipeline parallelism (PP) 300\n\nPiPPy (Pipeline Parallelism for PyTorch) library 301\n\npolicy optimization 246\n\nposition bias 237",
      "content_length": 279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1604,
      "content": "post-retrieval step, performing\n\nprompt compression 124\n\nre-ranking 124\n\nPost-Training Quantization (PTQ) 304\n\npreference alignment 245\n\npreference-based reinforcement learning (PbRL) 246\n\npreference dataset 230, 232\n\nChatbots 231\n\ncode generation 231\n\ncontent moderation 231",
      "content_length": 275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1605,
      "content": "creating 230, 237-245\n\ncreative writing 232\n\ndata evaluation 233\n\ndata generation 233\n\ndata quantity 232\n\nsummarization 231\n\ntranslation 232\n\npre-retrieval steps, performing\n\ndata indexing 119\n\nquery optimizing 119",
      "content_length": 214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1606,
      "content": "production environment 434\n\nprompt monitoring 413, 451-457\n\npull method 136\n\npush method 136\n\nPydantic domain entities 150-154\n\ndata category 151\n\nOVM 154-159\n\nstate of data category 151\n\nPydantic Settings\n\nreference link 139",
      "content_length": 225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1607,
      "content": "Python ecosystem\n\ndependency and virtual environment management 27-29\n\nproject installation 26, 27\n\ntask execution tool 29, 30\n\nQ\n\nQA job 438\n\nQdrant 47, 48\n\nreference link 419\n\nsetting up 419, 420\n\nquantization 303-308",
      "content_length": 219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1608,
      "content": "techniques 313, 314\n\nwith GGUF and llama.cpp 309-311\n\nwith GPTQ and EXL2 311, 312\n\nQuantization-aware Low-Rank Adaptation (QLoRA) 215, 216, 221\n\nQuantization-Aware Training (QAT) 304\n\nquery optimization 120\n\nquery rewriting 121\n\nquery routing 120\n\nR\n\nRAG evaluation 271, 272",
      "content_length": 274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1609,
      "content": "ARES 274, 275\n\nRagas 272-274\n\nRAG feature pipeline\n\nchunking 135\n\ncleaning 135\n\ndata extraction 134\n\ndata loading 135\n\ndata storage, in snapshots 138\n\ndata warehouse and feature store,syncing 136, 137\n\nembedding 135",
      "content_length": 215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1610,
      "content": "orchestration 138\n\nRAG feature pipeline architecture\n\nbatch pipelines 130\n\nbatch pipelines, versus streaming pipelines 130-134\n\ncore steps 134\n\ndesigning 129\n\nfeature store 128\n\ninference pipeline 127\n\ningestion pipeline 127\n\nproblem, solution 127, 128",
      "content_length": 252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1611,
      "content": "raw data 128\n\nRAG inference pipeline\n\narchitecture flow 320, 321\n\nimplementing 318-320, 338\n\nretrieval module, implementing 339-346\n\nraw data, into data warehouse\n\nobtaining 89-94\n\ntroubleshooting 94, 95\n\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) metric 267\n\nreference window 472",
      "content_length": 295,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1612,
      "content": "regression tests 464\n\nReinforcement Learning from Human Feedback (RLHF) 245-247, 411\n\niterative improvement 246\n\npolicy optimization 246\n\nreward model learning 246\n\nreinforcement learning (RL) 246\n\nreproducibility 473\n\nrequests per second (RPS) 356\n\nREST API triggers 448\n\nRetrieval-Augmented Generation Assessment (Ragas) 272-274",
      "content_length": 330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1613,
      "content": "retrieval-augmented generation (RAG) 2, 99, 100, 317\n\nembeddings 107, 108\n\nembeddings, applications 114\n\nembeddings, creating 111-114\n\nembeddings, significance 109, 110\n\nhallucinations 101\n\nissues, solving 101\n\nvanilla RAG framework 101\n\nvector DBs 115\n\nretrieval-augmented generation (RAG) pipeline 206, 261",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1614,
      "content": "reward model learning 246\n\nreward models 188\n\nrule-based filtering 182, 183\n\nrunners 437\n\nS\n\nSageMaker 423\n\nSageMaker Inference deployment 371\n\nconfiguration 371\n\nendpoint 371\n\nInference component 371",
      "content_length": 200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1615,
      "content": "model 371\n\nSageMaker Orchestrator 423\n\nSageMaker roles\n\nconfiguring 374, 375\n\nscalable and secure object storage service (S3) 423\n\nscalable policy\n\ncreating 397\n\nscalable target\n\nregistering 396\n\nscaling limits",
      "content_length": 210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1616,
      "content": "maximum 398\n\nminimum 398\n\nscheduled triggers 448\n\nSelenium tool 69\n\nissues 95\n\nsemantic similarity 184\n\nServer-Sent Events (SSE) 374\n\nSFT, techniques\n\nchat templates 208-210\n\nfine-tune, usage, considerations 206, 207",
      "content_length": 216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1617,
      "content": "hyperparameters, training 216\n\ninstruction dataset formats 208\n\nparameter-efficient fine-tuning techniques 211\n\nSFT techniques, parameters\n\nbatch size 216, 217\n\ngradient checkpointing 219\n\nlearning rate and scheduler 216\n\nmaximum length and packing 217, 218\n\nnumber of epochs 218\n\noptimizers 218",
      "content_length": 295,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1618,
      "content": "weight decay 219\n\nspeculative decoding 295, 296\n\nstack 422\n\nstaging environment 434\n\nstateless real-time architecture 11\n\nstatistical analysis 190\n\nstress tests 465\n\nstyle transfer 2\n\nsummarization 231\n\nSupervised Fine-Tuning (SFT) 177, 229, 264",
      "content_length": 245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1619,
      "content": "techniques, exploring 206\n\nsystem metrics 469\n\nsystem tests 464\n\nT\n\ntarget drift 470\n\nTargetTrackingScaling policy 397\n\ntask-specific LLM evaluations 267-271\n\ntensor parallelism (TP) 301, 302\n\nTerraform 393\n\ntest example 465",
      "content_length": 224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1620,
      "content": "test job 438\n\ntest types 465\n\nacceptance tests 464\n\nintegration tests 464\n\nregression tests 464\n\nstress tests 465\n\nsystem tests 464\n\nunit tests 464\n\ntest window 472\n\nText Generation Inference (TGI) 294, 373",
      "content_length": 206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1621,
      "content": "throughput 356, 357\n\nTime between Tokens (TBT) 413\n\nTime per Output Token (TPOT) 413\n\nTime to First Token (TTFT) 413\n\nTokens per Second (TPS) 413\n\ntopic clustering 190, 191\n\nTotal Latency 413\n\ntraining pipeline 14, 21, 22\n\nversus inference pipeline 371, 372\n\ntriggers",
      "content_length": 267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1622,
      "content": "manual triggers 448\n\nREST API triggers 448\n\nscheduled triggers 448\n\nTwinLlama-3.1-8B\n\nanswers, evaluating 278-283\n\nanswers, generating 276-278\n\nevaluating 275, 276\n\nresults, analyzing 283-286\n\nTwinLlama-3.1-8B model 250\n\nU",
      "content_length": 222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1623,
      "content": "UltraFeedback method 195\n\nunit tests 464\n\nUser Acceptance Testing (UAT) 464\n\nV\n\nvector DBs 115\n\nalgorithms, for creating vector index 116\n\nDB operations 116\n\nworking 115\n\nversioning 463\n\ncode 463",
      "content_length": 195,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1624,
      "content": "data 463\n\nmodel 463\n\nVideo Random-Access Memory (VRAM) 291\n\nW\n\nwindow types\n\nreference window 472\n\ntest window 472\n\nworkflow 437\n\nZ",
      "content_length": 131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1625,
      "content": "ZenML 32, 33\n\nartifacts and metadata 39-43\n\norchestrator 33-37\n\nreference link 421\n\nsetting up 421-423\n\nZenML pipeline 140-142\n\ncleaned documents, chunking 147-150\n\ncleaned documents, embedding 147-150\n\nconfiguring 43, 45\n\ndata warehouse, querying 143-145",
      "content_length": 255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1626,
      "content": "documents, cleaning 146, 147\n\ndocuments, loading to vector DB 150\n\nimplementing 61-65\n\nrunning 43, 45\n\nzero-point quantization 307",
      "content_length": 130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1627,
      "content": "Download a free PDF copy of this book\n\nThanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books everywhere?\n\nIs your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily.\n\nFollow these simple steps to get the benefits:",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1628,
      "content": "Scan the QR code or visit the link below:\n\nhttps://packt.link/free-ebook/9781836200079\n\nSubmit your proof of purchase.\n\nThat’s it! We’ll send your free PDF and other benefits to your email directly.",
      "content_length": 198,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1629,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 1630,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    }
  ]
}